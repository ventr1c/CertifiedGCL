Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9541475772857666 = 1.945773720741272 + 0.001 * 8.373821258544922
Epoch 0, val loss: 1.9449304342269897
Epoch 10, training loss: 1.9441683292388916 = 1.9357945919036865 + 0.001 * 8.373740196228027
Epoch 10, val loss: 1.9348820447921753
Epoch 20, training loss: 1.9315885305404663 = 1.9232150316238403 + 0.001 * 8.37346363067627
Epoch 20, val loss: 1.9220805168151855
Epoch 30, training loss: 1.9136499166488647 = 1.9052770137786865 + 0.001 * 8.372843742370605
Epoch 30, val loss: 1.9036376476287842
Epoch 40, training loss: 1.8869787454605103 = 1.8786073923110962 + 0.001 * 8.371376991271973
Epoch 40, val loss: 1.876423716545105
Epoch 50, training loss: 1.8501601219177246 = 1.841793179512024 + 0.001 * 8.366896629333496
Epoch 50, val loss: 1.8407316207885742
Epoch 60, training loss: 1.8092632293701172 = 1.8009166717529297 + 0.001 * 8.346575736999512
Epoch 60, val loss: 1.805529236793518
Epoch 70, training loss: 1.7717636823654175 = 1.7635490894317627 + 0.001 * 8.214545249938965
Epoch 70, val loss: 1.7766691446304321
Epoch 80, training loss: 1.7233245372772217 = 1.7155297994613647 + 0.001 * 7.794769287109375
Epoch 80, val loss: 1.7367464303970337
Epoch 90, training loss: 1.6554547548294067 = 1.6478052139282227 + 0.001 * 7.649535179138184
Epoch 90, val loss: 1.6792399883270264
Epoch 100, training loss: 1.5665663480758667 = 1.5590699911117554 + 0.001 * 7.496298313140869
Epoch 100, val loss: 1.6055333614349365
Epoch 110, training loss: 1.4663406610488892 = 1.4590826034545898 + 0.001 * 7.258018493652344
Epoch 110, val loss: 1.5266590118408203
Epoch 120, training loss: 1.365647792816162 = 1.3585602045059204 + 0.001 * 7.08762264251709
Epoch 120, val loss: 1.448973536491394
Epoch 130, training loss: 1.2668697834014893 = 1.2598347663879395 + 0.001 * 7.035025596618652
Epoch 130, val loss: 1.3742735385894775
Epoch 140, training loss: 1.1705543994903564 = 1.163596749305725 + 0.001 * 6.957614421844482
Epoch 140, val loss: 1.3032013177871704
Epoch 150, training loss: 1.0804122686386108 = 1.0735100507736206 + 0.001 * 6.902196884155273
Epoch 150, val loss: 1.2370258569717407
Epoch 160, training loss: 1.0000426769256592 = 0.9931774735450745 + 0.001 * 6.865185737609863
Epoch 160, val loss: 1.1787937879562378
Epoch 170, training loss: 0.9294462203979492 = 0.922602117061615 + 0.001 * 6.844091415405273
Epoch 170, val loss: 1.1287732124328613
Epoch 180, training loss: 0.8658544421195984 = 0.8590192794799805 + 0.001 * 6.835190296173096
Epoch 180, val loss: 1.0855238437652588
Epoch 190, training loss: 0.8058664202690125 = 0.7990360260009766 + 0.001 * 6.83040714263916
Epoch 190, val loss: 1.0466668605804443
Epoch 200, training loss: 0.747044026851654 = 0.7402174472808838 + 0.001 * 6.826599597930908
Epoch 200, val loss: 1.0091813802719116
Epoch 210, training loss: 0.6881453394889832 = 0.681321382522583 + 0.001 * 6.823966026306152
Epoch 210, val loss: 0.9713934063911438
Epoch 220, training loss: 0.6287948489189148 = 0.6219731569290161 + 0.001 * 6.82167911529541
Epoch 220, val loss: 0.9326329827308655
Epoch 230, training loss: 0.5696740746498108 = 0.5628551244735718 + 0.001 * 6.818936824798584
Epoch 230, val loss: 0.893460214138031
Epoch 240, training loss: 0.5126830339431763 = 0.5058677196502686 + 0.001 * 6.815301895141602
Epoch 240, val loss: 0.8551350831985474
Epoch 250, training loss: 0.4595484733581543 = 0.45273733139038086 + 0.001 * 6.8111443519592285
Epoch 250, val loss: 0.8196027278900146
Epoch 260, training loss: 0.4107953608036041 = 0.4039883613586426 + 0.001 * 6.807003974914551
Epoch 260, val loss: 0.7881682515144348
Epoch 270, training loss: 0.3658471405506134 = 0.35904473066329956 + 0.001 * 6.802403450012207
Epoch 270, val loss: 0.7610230445861816
Epoch 280, training loss: 0.32396766543388367 = 0.3171694576740265 + 0.001 * 6.798196792602539
Epoch 280, val loss: 0.7377456426620483
Epoch 290, training loss: 0.2848283350467682 = 0.27803361415863037 + 0.001 * 6.794715881347656
Epoch 290, val loss: 0.7177457809448242
Epoch 300, training loss: 0.24853046238422394 = 0.241740882396698 + 0.001 * 6.789579391479492
Epoch 300, val loss: 0.7008165121078491
Epoch 310, training loss: 0.21533113718032837 = 0.2085452675819397 + 0.001 * 6.7858757972717285
Epoch 310, val loss: 0.6871526837348938
Epoch 320, training loss: 0.18560552597045898 = 0.17882275581359863 + 0.001 * 6.782777309417725
Epoch 320, val loss: 0.677108883857727
Epoch 330, training loss: 0.15964257717132568 = 0.15286341309547424 + 0.001 * 6.779162883758545
Epoch 330, val loss: 0.6707637310028076
Epoch 340, training loss: 0.13746488094329834 = 0.13069026172161102 + 0.001 * 6.774621963500977
Epoch 340, val loss: 0.6680928468704224
Epoch 350, training loss: 0.11880307644605637 = 0.11202894896268845 + 0.001 * 6.774127006530762
Epoch 350, val loss: 0.6688836812973022
Epoch 360, training loss: 0.10319110751152039 = 0.0964227244257927 + 0.001 * 6.768386363983154
Epoch 360, val loss: 0.6726884245872498
Epoch 370, training loss: 0.0901460200548172 = 0.08338423073291779 + 0.001 * 6.761785507202148
Epoch 370, val loss: 0.6789988279342651
Epoch 380, training loss: 0.07922956347465515 = 0.07247191667556763 + 0.001 * 6.7576470375061035
Epoch 380, val loss: 0.6873055100440979
Epoch 390, training loss: 0.0700613260269165 = 0.06331200152635574 + 0.001 * 6.749326705932617
Epoch 390, val loss: 0.6971384882926941
Epoch 400, training loss: 0.062341149896383286 = 0.055603042244911194 + 0.001 * 6.738107204437256
Epoch 400, val loss: 0.7080704569816589
Epoch 410, training loss: 0.05584356561303139 = 0.04909354820847511 + 0.001 * 6.750017166137695
Epoch 410, val loss: 0.7198056578636169
Epoch 420, training loss: 0.05029459670186043 = 0.043574217706918716 + 0.001 * 6.720377445220947
Epoch 420, val loss: 0.7320713996887207
Epoch 430, training loss: 0.04559929668903351 = 0.03887927159667015 + 0.001 * 6.720026016235352
Epoch 430, val loss: 0.7445727586746216
Epoch 440, training loss: 0.04158167541027069 = 0.034865956753492355 + 0.001 * 6.715716361999512
Epoch 440, val loss: 0.7571213841438293
Epoch 450, training loss: 0.038155268877744675 = 0.03141816705465317 + 0.001 * 6.73710298538208
Epoch 450, val loss: 0.7696456909179688
Epoch 460, training loss: 0.035151101648807526 = 0.02844097651541233 + 0.001 * 6.7101263999938965
Epoch 460, val loss: 0.7819701433181763
Epoch 470, training loss: 0.03254741057753563 = 0.02585744298994541 + 0.001 * 6.689966678619385
Epoch 470, val loss: 0.7940487861633301
Epoch 480, training loss: 0.030287884175777435 = 0.023604849353432655 + 0.001 * 6.6830339431762695
Epoch 480, val loss: 0.8059046864509583
Epoch 490, training loss: 0.028313996270298958 = 0.021630948409438133 + 0.001 * 6.683047294616699
Epoch 490, val loss: 0.8174530863761902
Epoch 500, training loss: 0.026574982330203056 = 0.019892552867531776 + 0.001 * 6.682429790496826
Epoch 500, val loss: 0.8286890387535095
Epoch 510, training loss: 0.025038057938218117 = 0.018356259912252426 + 0.001 * 6.681797504425049
Epoch 510, val loss: 0.8395982384681702
Epoch 520, training loss: 0.02368655800819397 = 0.01699306070804596 + 0.001 * 6.693496227264404
Epoch 520, val loss: 0.8501884341239929
Epoch 530, training loss: 0.022452108561992645 = 0.015779482200741768 + 0.001 * 6.672625541687012
Epoch 530, val loss: 0.8604773879051208
Epoch 540, training loss: 0.021358555182814598 = 0.01469400804489851 + 0.001 * 6.664547443389893
Epoch 540, val loss: 0.8704639673233032
Epoch 550, training loss: 0.02038334682583809 = 0.013719568960368633 + 0.001 * 6.663778305053711
Epoch 550, val loss: 0.8802597522735596
Epoch 560, training loss: 0.019516415894031525 = 0.01284187100827694 + 0.001 * 6.674544334411621
Epoch 560, val loss: 0.8897705674171448
Epoch 570, training loss: 0.01872250996530056 = 0.012048762291669846 + 0.001 * 6.673748016357422
Epoch 570, val loss: 0.8989585638046265
Epoch 580, training loss: 0.017993977293372154 = 0.011329738423228264 + 0.001 * 6.664237976074219
Epoch 580, val loss: 0.9079081416130066
Epoch 590, training loss: 0.017329687252640724 = 0.010675927624106407 + 0.001 * 6.653759479522705
Epoch 590, val loss: 0.9166461229324341
Epoch 600, training loss: 0.016747942194342613 = 0.01007936242967844 + 0.001 * 6.668579578399658
Epoch 600, val loss: 0.9251708388328552
Epoch 610, training loss: 0.01618475653231144 = 0.00953380111604929 + 0.001 * 6.6509552001953125
Epoch 610, val loss: 0.9334350824356079
Epoch 620, training loss: 0.015683358535170555 = 0.009034070186316967 + 0.001 * 6.649288177490234
Epoch 620, val loss: 0.941482663154602
Epoch 630, training loss: 0.015241967514157295 = 0.008575216867029667 + 0.001 * 6.666750431060791
Epoch 630, val loss: 0.9493021368980408
Epoch 640, training loss: 0.014813613146543503 = 0.008152601309120655 + 0.001 * 6.661011695861816
Epoch 640, val loss: 0.9569475054740906
Epoch 650, training loss: 0.014407682232558727 = 0.007762480061501265 + 0.001 * 6.645201683044434
Epoch 650, val loss: 0.9644359350204468
Epoch 660, training loss: 0.01404636725783348 = 0.007401686627417803 + 0.001 * 6.644680023193359
Epoch 660, val loss: 0.9717259407043457
Epoch 670, training loss: 0.01370939053595066 = 0.007067294791340828 + 0.001 * 6.642096042633057
Epoch 670, val loss: 0.9787986278533936
Epoch 680, training loss: 0.01343424804508686 = 0.006756756454706192 + 0.001 * 6.677491664886475
Epoch 680, val loss: 0.985738217830658
Epoch 690, training loss: 0.013110937550663948 = 0.006467911414802074 + 0.001 * 6.643025875091553
Epoch 690, val loss: 0.9924998879432678
Epoch 700, training loss: 0.01285054162144661 = 0.0061988187953829765 + 0.001 * 6.651722431182861
Epoch 700, val loss: 0.9991291165351868
Epoch 710, training loss: 0.012583544477820396 = 0.00594763457775116 + 0.001 * 6.635909080505371
Epoch 710, val loss: 1.0055913925170898
Epoch 720, training loss: 0.012364039197564125 = 0.005712801590561867 + 0.001 * 6.6512370109558105
Epoch 720, val loss: 1.0119142532348633
Epoch 730, training loss: 0.012145362794399261 = 0.005492933560162783 + 0.001 * 6.652429103851318
Epoch 730, val loss: 1.0181034803390503
Epoch 740, training loss: 0.011932095512747765 = 0.005286792293190956 + 0.001 * 6.645302772521973
Epoch 740, val loss: 1.0241447687149048
Epoch 750, training loss: 0.01172911748290062 = 0.005093200132250786 + 0.001 * 6.635916709899902
Epoch 750, val loss: 1.0300545692443848
Epoch 760, training loss: 0.011558657512068748 = 0.004911179654300213 + 0.001 * 6.647478103637695
Epoch 760, val loss: 1.0358446836471558
Epoch 770, training loss: 0.011386960744857788 = 0.004739829804748297 + 0.001 * 6.647130489349365
Epoch 770, val loss: 1.0415124893188477
Epoch 780, training loss: 0.011207027360796928 = 0.004578223917633295 + 0.001 * 6.628803253173828
Epoch 780, val loss: 1.0470685958862305
Epoch 790, training loss: 0.011103739961981773 = 0.004425286781042814 + 0.001 * 6.67845344543457
Epoch 790, val loss: 1.0525391101837158
Epoch 800, training loss: 0.010905468836426735 = 0.004279567394405603 + 0.001 * 6.625900745391846
Epoch 800, val loss: 1.057997226715088
Epoch 810, training loss: 0.010765441693365574 = 0.004139228258281946 + 0.001 * 6.626213073730469
Epoch 810, val loss: 1.0635539293289185
Epoch 820, training loss: 0.010629533790051937 = 0.0040033962577581406 + 0.001 * 6.6261372566223145
Epoch 820, val loss: 1.0691406726837158
Epoch 830, training loss: 0.010508028790354729 = 0.003872109344229102 + 0.001 * 6.635919094085693
Epoch 830, val loss: 1.0746794939041138
Epoch 840, training loss: 0.010382703505456448 = 0.0037456999998539686 + 0.001 * 6.637003421783447
Epoch 840, val loss: 1.0801759958267212
Epoch 850, training loss: 0.010252779349684715 = 0.003624142147600651 + 0.001 * 6.628637313842773
Epoch 850, val loss: 1.0856291055679321
Epoch 860, training loss: 0.010147997178137302 = 0.0035075421910732985 + 0.001 * 6.640454292297363
Epoch 860, val loss: 1.0910428762435913
Epoch 870, training loss: 0.010020246729254723 = 0.0033959762658923864 + 0.001 * 6.624270439147949
Epoch 870, val loss: 1.0963889360427856
Epoch 880, training loss: 0.009907707571983337 = 0.0032894685864448547 + 0.001 * 6.618238925933838
Epoch 880, val loss: 1.101684331893921
Epoch 890, training loss: 0.009823955595493317 = 0.0031879101879894733 + 0.001 * 6.636045455932617
Epoch 890, val loss: 1.1069122552871704
Epoch 900, training loss: 0.009724518284201622 = 0.0030911460053175688 + 0.001 * 6.6333723068237305
Epoch 900, val loss: 1.1120595932006836
Epoch 910, training loss: 0.009615877643227577 = 0.002999093383550644 + 0.001 * 6.61678409576416
Epoch 910, val loss: 1.1171308755874634
Epoch 920, training loss: 0.009532134979963303 = 0.0029114889912307262 + 0.001 * 6.620645523071289
Epoch 920, val loss: 1.1221305131912231
Epoch 930, training loss: 0.009448446333408356 = 0.0028281863778829575 + 0.001 * 6.620259761810303
Epoch 930, val loss: 1.1270228624343872
Epoch 940, training loss: 0.009378846734762192 = 0.0027488218620419502 + 0.001 * 6.6300249099731445
Epoch 940, val loss: 1.1318649053573608
Epoch 950, training loss: 0.009287474676966667 = 0.0026732590049505234 + 0.001 * 6.614215850830078
Epoch 950, val loss: 1.136595606803894
Epoch 960, training loss: 0.009212357923388481 = 0.0026012605521827936 + 0.001 * 6.61109733581543
Epoch 960, val loss: 1.1412603855133057
Epoch 970, training loss: 0.009145914576947689 = 0.0025325696915388107 + 0.001 * 6.613344669342041
Epoch 970, val loss: 1.1458311080932617
Epoch 980, training loss: 0.00907459668815136 = 0.0024670525453984737 + 0.001 * 6.607543468475342
Epoch 980, val loss: 1.1503558158874512
Epoch 990, training loss: 0.009042314253747463 = 0.0024044846650213003 + 0.001 * 6.637829303741455
Epoch 990, val loss: 1.1548010110855103
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6531
Flip ASR: 0.5822/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9493132829666138 = 1.9409394264221191 + 0.001 * 8.373858451843262
Epoch 0, val loss: 1.9401590824127197
Epoch 10, training loss: 1.939892292022705 = 1.9315185546875 + 0.001 * 8.373796463012695
Epoch 10, val loss: 1.9308396577835083
Epoch 20, training loss: 1.9287978410720825 = 1.920424222946167 + 0.001 * 8.373579978942871
Epoch 20, val loss: 1.9193271398544312
Epoch 30, training loss: 1.9136751890182495 = 1.9053020477294922 + 0.001 * 8.373103141784668
Epoch 30, val loss: 1.9031308889389038
Epoch 40, training loss: 1.8913832902908325 = 1.8830112218856812 + 0.001 * 8.372023582458496
Epoch 40, val loss: 1.8789547681808472
Epoch 50, training loss: 1.8589805364608765 = 1.8506112098693848 + 0.001 * 8.369309425354004
Epoch 50, val loss: 1.8445501327514648
Epoch 60, training loss: 1.8172942399978638 = 1.8089340925216675 + 0.001 * 8.36010456085205
Epoch 60, val loss: 1.803157925605774
Epoch 70, training loss: 1.7732621431350708 = 1.7649471759796143 + 0.001 * 8.314945220947266
Epoch 70, val loss: 1.7640610933303833
Epoch 80, training loss: 1.7226756811141968 = 1.714663028717041 + 0.001 * 8.012688636779785
Epoch 80, val loss: 1.7214617729187012
Epoch 90, training loss: 1.6527037620544434 = 1.6451401710510254 + 0.001 * 7.563557147979736
Epoch 90, val loss: 1.662095308303833
Epoch 100, training loss: 1.5604439973831177 = 1.553114652633667 + 0.001 * 7.329326152801514
Epoch 100, val loss: 1.5830919742584229
Epoch 110, training loss: 1.4511436223983765 = 1.44394850730896 + 0.001 * 7.195126056671143
Epoch 110, val loss: 1.4931154251098633
Epoch 120, training loss: 1.3355532884597778 = 1.3284368515014648 + 0.001 * 7.116416931152344
Epoch 120, val loss: 1.4005247354507446
Epoch 130, training loss: 1.2210875749588013 = 1.2140063047409058 + 0.001 * 7.081278324127197
Epoch 130, val loss: 1.311572551727295
Epoch 140, training loss: 1.1124355792999268 = 1.1053920984268188 + 0.001 * 7.043446063995361
Epoch 140, val loss: 1.2290441989898682
Epoch 150, training loss: 1.0123955011367798 = 1.005385160446167 + 0.001 * 7.010293006896973
Epoch 150, val loss: 1.1540426015853882
Epoch 160, training loss: 0.9219669103622437 = 0.9149845242500305 + 0.001 * 6.982389450073242
Epoch 160, val loss: 1.0876121520996094
Epoch 170, training loss: 0.8404411673545837 = 0.8334845304489136 + 0.001 * 6.9566216468811035
Epoch 170, val loss: 1.0290696620941162
Epoch 180, training loss: 0.7671496272087097 = 0.7602179646492004 + 0.001 * 6.931640625
Epoch 180, val loss: 0.9772675633430481
Epoch 190, training loss: 0.7014551162719727 = 0.6945497393608093 + 0.001 * 6.905378818511963
Epoch 190, val loss: 0.9316957592964172
Epoch 200, training loss: 0.6421695947647095 = 0.635292649269104 + 0.001 * 6.876944541931152
Epoch 200, val loss: 0.8917987942695618
Epoch 210, training loss: 0.5874295830726624 = 0.5805773138999939 + 0.001 * 6.852296352386475
Epoch 210, val loss: 0.8567032217979431
Epoch 220, training loss: 0.5359297394752502 = 0.5290912389755249 + 0.001 * 6.838502407073975
Epoch 220, val loss: 0.8259427547454834
Epoch 230, training loss: 0.4872335195541382 = 0.48039954900741577 + 0.001 * 6.833973407745361
Epoch 230, val loss: 0.7994614243507385
Epoch 240, training loss: 0.44162213802337646 = 0.4347909986972809 + 0.001 * 6.83113956451416
Epoch 240, val loss: 0.7773944139480591
Epoch 250, training loss: 0.3995485305786133 = 0.392719566822052 + 0.001 * 6.828972816467285
Epoch 250, val loss: 0.7596374154090881
Epoch 260, training loss: 0.36141204833984375 = 0.3545851409435272 + 0.001 * 6.826893329620361
Epoch 260, val loss: 0.7460561394691467
Epoch 270, training loss: 0.3271663188934326 = 0.320341020822525 + 0.001 * 6.825297832489014
Epoch 270, val loss: 0.7363746762275696
Epoch 280, training loss: 0.2961842119693756 = 0.28936031460762024 + 0.001 * 6.82388973236084
Epoch 280, val loss: 0.7298644781112671
Epoch 290, training loss: 0.2674373686313629 = 0.26061496138572693 + 0.001 * 6.822409629821777
Epoch 290, val loss: 0.7257620096206665
Epoch 300, training loss: 0.24004049599170685 = 0.233219712972641 + 0.001 * 6.820777893066406
Epoch 300, val loss: 0.7231842875480652
Epoch 310, training loss: 0.21333885192871094 = 0.20651987195014954 + 0.001 * 6.818984031677246
Epoch 310, val loss: 0.7216496467590332
Epoch 320, training loss: 0.1874699741601944 = 0.18065206706523895 + 0.001 * 6.817911148071289
Epoch 320, val loss: 0.7210592031478882
Epoch 330, training loss: 0.16330371797084808 = 0.15648725628852844 + 0.001 * 6.816456317901611
Epoch 330, val loss: 0.7218403816223145
Epoch 340, training loss: 0.141788512468338 = 0.13497398793697357 + 0.001 * 6.814518928527832
Epoch 340, val loss: 0.7245933413505554
Epoch 350, training loss: 0.1232738196849823 = 0.11646074801683426 + 0.001 * 6.813072681427002
Epoch 350, val loss: 0.7296612858772278
Epoch 360, training loss: 0.10769011080265045 = 0.10087832063436508 + 0.001 * 6.811789512634277
Epoch 360, val loss: 0.737227201461792
Epoch 370, training loss: 0.09467935562133789 = 0.08786799758672714 + 0.001 * 6.811354637145996
Epoch 370, val loss: 0.7469541430473328
Epoch 380, training loss: 0.0838126689195633 = 0.07700333744287491 + 0.001 * 6.8093342781066895
Epoch 380, val loss: 0.7584335803985596
Epoch 390, training loss: 0.07470151036977768 = 0.06789188086986542 + 0.001 * 6.809627532958984
Epoch 390, val loss: 0.7711659073829651
Epoch 400, training loss: 0.06701651960611343 = 0.060209233313798904 + 0.001 * 6.807287693023682
Epoch 400, val loss: 0.7847079634666443
Epoch 410, training loss: 0.0604901947081089 = 0.053685493767261505 + 0.001 * 6.804701805114746
Epoch 410, val loss: 0.7987005114555359
Epoch 420, training loss: 0.0549134686589241 = 0.048109449446201324 + 0.001 * 6.804019927978516
Epoch 420, val loss: 0.8127596378326416
Epoch 430, training loss: 0.050117943435907364 = 0.04331479221582413 + 0.001 * 6.803151607513428
Epoch 430, val loss: 0.8268162608146667
Epoch 440, training loss: 0.04596754163503647 = 0.03916602581739426 + 0.001 * 6.801515579223633
Epoch 440, val loss: 0.840694785118103
Epoch 450, training loss: 0.04236406087875366 = 0.03555857762694359 + 0.001 * 6.805484294891357
Epoch 450, val loss: 0.854340136051178
Epoch 460, training loss: 0.03920169919729233 = 0.03240647166967392 + 0.001 * 6.795228958129883
Epoch 460, val loss: 0.8677018284797668
Epoch 470, training loss: 0.03644119203090668 = 0.02963976189494133 + 0.001 * 6.8014302253723145
Epoch 470, val loss: 0.8807353377342224
Epoch 480, training loss: 0.03399313986301422 = 0.02720053866505623 + 0.001 * 6.792601585388184
Epoch 480, val loss: 0.8934990167617798
Epoch 490, training loss: 0.03183160722255707 = 0.025041401386260986 + 0.001 * 6.790205478668213
Epoch 490, val loss: 0.9059216380119324
Epoch 500, training loss: 0.029916448518633842 = 0.023122398182749748 + 0.001 * 6.7940497398376465
Epoch 500, val loss: 0.9180092811584473
Epoch 510, training loss: 0.028195852413773537 = 0.0214100144803524 + 0.001 * 6.785837650299072
Epoch 510, val loss: 0.9297635555267334
Epoch 520, training loss: 0.026662131771445274 = 0.0198774766176939 + 0.001 * 6.784654140472412
Epoch 520, val loss: 0.9412016868591309
Epoch 530, training loss: 0.025285907089710236 = 0.018501529470086098 + 0.001 * 6.784377574920654
Epoch 530, val loss: 0.9523220658302307
Epoch 540, training loss: 0.024036262184381485 = 0.017261963337659836 + 0.001 * 6.774299144744873
Epoch 540, val loss: 0.9631456136703491
Epoch 550, training loss: 0.02292330004274845 = 0.016142066568136215 + 0.001 * 6.781232833862305
Epoch 550, val loss: 0.9736649990081787
Epoch 560, training loss: 0.021896738559007645 = 0.015127411112189293 + 0.001 * 6.769327163696289
Epoch 560, val loss: 0.9838865995407104
Epoch 570, training loss: 0.02097306028008461 = 0.014205663464963436 + 0.001 * 6.767396926879883
Epoch 570, val loss: 0.9938039183616638
Epoch 580, training loss: 0.020125556737184525 = 0.013366094790399075 + 0.001 * 6.759461402893066
Epoch 580, val loss: 1.0034551620483398
Epoch 590, training loss: 0.01935710199177265 = 0.012599457055330276 + 0.001 * 6.7576446533203125
Epoch 590, val loss: 1.0128535032272339
Epoch 600, training loss: 0.018649917095899582 = 0.011897769756615162 + 0.001 * 6.7521467208862305
Epoch 600, val loss: 1.0219764709472656
Epoch 610, training loss: 0.018014147877693176 = 0.01125410944223404 + 0.001 * 6.760038375854492
Epoch 610, val loss: 1.0308609008789062
Epoch 620, training loss: 0.017415210604667664 = 0.01066240482032299 + 0.001 * 6.752806186676025
Epoch 620, val loss: 1.0394999980926514
Epoch 630, training loss: 0.016859494149684906 = 0.010117342695593834 + 0.001 * 6.74215030670166
Epoch 630, val loss: 1.0478947162628174
Epoch 640, training loss: 0.016359269618988037 = 0.009614245034754276 + 0.001 * 6.745023727416992
Epoch 640, val loss: 1.0560745000839233
Epoch 650, training loss: 0.015960855409502983 = 0.00914894137531519 + 0.001 * 6.811913013458252
Epoch 650, val loss: 1.0640277862548828
Epoch 660, training loss: 0.015443902462720871 = 0.00871784146875143 + 0.001 * 6.72606086730957
Epoch 660, val loss: 1.0717675685882568
Epoch 670, training loss: 0.015045113861560822 = 0.008317786268889904 + 0.001 * 6.7273268699646
Epoch 670, val loss: 1.079298496246338
Epoch 680, training loss: 0.014665789902210236 = 0.007945867255330086 + 0.001 * 6.7199225425720215
Epoch 680, val loss: 1.086653232574463
Epoch 690, training loss: 0.014309689402580261 = 0.007599490694701672 + 0.001 * 6.710198879241943
Epoch 690, val loss: 1.093780279159546
Epoch 700, training loss: 0.013999372720718384 = 0.007276463322341442 + 0.001 * 6.722909450531006
Epoch 700, val loss: 1.1007505655288696
Epoch 710, training loss: 0.013682002201676369 = 0.006974823772907257 + 0.001 * 6.707177639007568
Epoch 710, val loss: 1.1075416803359985
Epoch 720, training loss: 0.013402534648776054 = 0.006692684721201658 + 0.001 * 6.709849834442139
Epoch 720, val loss: 1.114187479019165
Epoch 730, training loss: 0.013126783072948456 = 0.006428474560379982 + 0.001 * 6.698307514190674
Epoch 730, val loss: 1.1206613779067993
Epoch 740, training loss: 0.012894513085484505 = 0.006180711556226015 + 0.001 * 6.71380090713501
Epoch 740, val loss: 1.12697172164917
Epoch 750, training loss: 0.012656887993216515 = 0.0059480248019099236 + 0.001 * 6.708863258361816
Epoch 750, val loss: 1.133150339126587
Epoch 760, training loss: 0.012423882260918617 = 0.005729238037019968 + 0.001 * 6.694643497467041
Epoch 760, val loss: 1.1391737461090088
Epoch 770, training loss: 0.012210346758365631 = 0.005523276049643755 + 0.001 * 6.687069892883301
Epoch 770, val loss: 1.1450448036193848
Epoch 780, training loss: 0.01201587449759245 = 0.0053291767835617065 + 0.001 * 6.686697483062744
Epoch 780, val loss: 1.150804042816162
Epoch 790, training loss: 0.011836318299174309 = 0.0051460228860378265 + 0.001 * 6.6902947425842285
Epoch 790, val loss: 1.15642249584198
Epoch 800, training loss: 0.011659421026706696 = 0.004973047878593206 + 0.001 * 6.68637228012085
Epoch 800, val loss: 1.16192626953125
Epoch 810, training loss: 0.01151016540825367 = 0.004809496458619833 + 0.001 * 6.700668811798096
Epoch 810, val loss: 1.167295217514038
Epoch 820, training loss: 0.01133562158793211 = 0.0046546971425414085 + 0.001 * 6.680923938751221
Epoch 820, val loss: 1.1725484132766724
Epoch 830, training loss: 0.011182025074958801 = 0.0045080347917973995 + 0.001 * 6.673990249633789
Epoch 830, val loss: 1.1776797771453857
Epoch 840, training loss: 0.011061441153287888 = 0.004368951078504324 + 0.001 * 6.692489147186279
Epoch 840, val loss: 1.1827126741409302
Epoch 850, training loss: 0.010921664535999298 = 0.004236932378262281 + 0.001 * 6.684731960296631
Epoch 850, val loss: 1.1876115798950195
Epoch 860, training loss: 0.01079944334924221 = 0.0041114878840744495 + 0.001 * 6.687954902648926
Epoch 860, val loss: 1.1924383640289307
Epoch 870, training loss: 0.010659463703632355 = 0.003992224577814341 + 0.001 * 6.667238712310791
Epoch 870, val loss: 1.1971665620803833
Epoch 880, training loss: 0.01054018922150135 = 0.003878746647387743 + 0.001 * 6.661442279815674
Epoch 880, val loss: 1.2017697095870972
Epoch 890, training loss: 0.010434106923639774 = 0.003770642215386033 + 0.001 * 6.663464546203613
Epoch 890, val loss: 1.206307291984558
Epoch 900, training loss: 0.010332484729588032 = 0.0036676109302788973 + 0.001 * 6.6648736000061035
Epoch 900, val loss: 1.210734486579895
Epoch 910, training loss: 0.010229464620351791 = 0.0035692863166332245 + 0.001 * 6.6601786613464355
Epoch 910, val loss: 1.2150710821151733
Epoch 920, training loss: 0.010152911767363548 = 0.0034752790816128254 + 0.001 * 6.677631855010986
Epoch 920, val loss: 1.2193433046340942
Epoch 930, training loss: 0.010050507262349129 = 0.0033850723411887884 + 0.001 * 6.665434837341309
Epoch 930, val loss: 1.2235193252563477
Epoch 940, training loss: 0.009951516054570675 = 0.003298069117590785 + 0.001 * 6.653446674346924
Epoch 940, val loss: 1.2276681661605835
Epoch 950, training loss: 0.009860390797257423 = 0.003213572781533003 + 0.001 * 6.646817207336426
Epoch 950, val loss: 1.2317758798599243
Epoch 960, training loss: 0.009785512462258339 = 0.0031310701742768288 + 0.001 * 6.6544413566589355
Epoch 960, val loss: 1.2358824014663696
Epoch 970, training loss: 0.009700268507003784 = 0.003050403203815222 + 0.001 * 6.64986515045166
Epoch 970, val loss: 1.239997386932373
Epoch 980, training loss: 0.009648450650274754 = 0.0029716913122683764 + 0.001 * 6.676758766174316
Epoch 980, val loss: 1.2441660165786743
Epoch 990, training loss: 0.009573829360306263 = 0.0028950560372322798 + 0.001 * 6.678773403167725
Epoch 990, val loss: 1.2482421398162842
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9605529308319092 = 1.9521790742874146 + 0.001 * 8.373815536499023
Epoch 0, val loss: 1.9540547132492065
Epoch 10, training loss: 1.949143648147583 = 1.940769910812378 + 0.001 * 8.37369441986084
Epoch 10, val loss: 1.9424558877944946
Epoch 20, training loss: 1.9352614879608154 = 1.926888108253479 + 0.001 * 8.373373985290527
Epoch 20, val loss: 1.9281344413757324
Epoch 30, training loss: 1.9162859916687012 = 1.907913327217102 + 0.001 * 8.372654914855957
Epoch 30, val loss: 1.9085146188735962
Epoch 40, training loss: 1.8888226747512817 = 1.8804517984390259 + 0.001 * 8.370840072631836
Epoch 40, val loss: 1.880522608757019
Epoch 50, training loss: 1.8505854606628418 = 1.8422207832336426 + 0.001 * 8.36473274230957
Epoch 50, val loss: 1.8432203531265259
Epoch 60, training loss: 1.8058823347091675 = 1.7975481748580933 + 0.001 * 8.33410930633545
Epoch 60, val loss: 1.8036574125289917
Epoch 70, training loss: 1.763354778289795 = 1.7552464008331299 + 0.001 * 8.108379364013672
Epoch 70, val loss: 1.7698898315429688
Epoch 80, training loss: 1.711562156677246 = 1.703789472579956 + 0.001 * 7.772689342498779
Epoch 80, val loss: 1.7271075248718262
Epoch 90, training loss: 1.6401655673980713 = 1.6327484846115112 + 0.001 * 7.417036533355713
Epoch 90, val loss: 1.6676586866378784
Epoch 100, training loss: 1.5481805801391602 = 1.5410184860229492 + 0.001 * 7.162034034729004
Epoch 100, val loss: 1.5926088094711304
Epoch 110, training loss: 1.4435529708862305 = 1.4364608526229858 + 0.001 * 7.092062950134277
Epoch 110, val loss: 1.50968337059021
Epoch 120, training loss: 1.3369901180267334 = 1.3299503326416016 + 0.001 * 7.0397772789001465
Epoch 120, val loss: 1.4258617162704468
Epoch 130, training loss: 1.2323161363601685 = 1.2253040075302124 + 0.001 * 7.012182712554932
Epoch 130, val loss: 1.3448002338409424
Epoch 140, training loss: 1.1314064264297485 = 1.1244218349456787 + 0.001 * 6.984576225280762
Epoch 140, val loss: 1.2671055793762207
Epoch 150, training loss: 1.0363574028015137 = 1.0294032096862793 + 0.001 * 6.954166889190674
Epoch 150, val loss: 1.1943480968475342
Epoch 160, training loss: 0.948633074760437 = 0.9417058229446411 + 0.001 * 6.92722225189209
Epoch 160, val loss: 1.1278270483016968
Epoch 170, training loss: 0.8678757548332214 = 0.8609707355499268 + 0.001 * 6.905008792877197
Epoch 170, val loss: 1.067413330078125
Epoch 180, training loss: 0.7934867739677429 = 0.786605715751648 + 0.001 * 6.881078720092773
Epoch 180, val loss: 1.0129863023757935
Epoch 190, training loss: 0.7256923913955688 = 0.718828558921814 + 0.001 * 6.863838195800781
Epoch 190, val loss: 0.9645193815231323
Epoch 200, training loss: 0.6643174886703491 = 0.657473087310791 + 0.001 * 6.84439754486084
Epoch 200, val loss: 0.9227389097213745
Epoch 210, training loss: 0.6076663732528687 = 0.6008342504501343 + 0.001 * 6.8321356773376465
Epoch 210, val loss: 0.8864573836326599
Epoch 220, training loss: 0.5531675219535828 = 0.5463442802429199 + 0.001 * 6.823225975036621
Epoch 220, val loss: 0.8536331057548523
Epoch 230, training loss: 0.49883517622947693 = 0.4920116066932678 + 0.001 * 6.823563575744629
Epoch 230, val loss: 0.8222020864486694
Epoch 240, training loss: 0.44417980313301086 = 0.43736162781715393 + 0.001 * 6.818180561065674
Epoch 240, val loss: 0.7920483350753784
Epoch 250, training loss: 0.3902949094772339 = 0.38347938656806946 + 0.001 * 6.815515995025635
Epoch 250, val loss: 0.764657199382782
Epoch 260, training loss: 0.3392368257045746 = 0.3324213922023773 + 0.001 * 6.815430641174316
Epoch 260, val loss: 0.742520272731781
Epoch 270, training loss: 0.2926618754863739 = 0.2858479619026184 + 0.001 * 6.813899040222168
Epoch 270, val loss: 0.726474404335022
Epoch 280, training loss: 0.2516976296901703 = 0.24487119913101196 + 0.001 * 6.826418876647949
Epoch 280, val loss: 0.7161750793457031
Epoch 290, training loss: 0.21662722527980804 = 0.20981189608573914 + 0.001 * 6.815335750579834
Epoch 290, val loss: 0.7109994292259216
Epoch 300, training loss: 0.18718977272510529 = 0.1803729385137558 + 0.001 * 6.816834926605225
Epoch 300, val loss: 0.7103163003921509
Epoch 310, training loss: 0.16266247630119324 = 0.15584778785705566 + 0.001 * 6.8146891593933105
Epoch 310, val loss: 0.7132503390312195
Epoch 320, training loss: 0.1422950029373169 = 0.13548070192337036 + 0.001 * 6.814306735992432
Epoch 320, val loss: 0.7189803123474121
Epoch 330, training loss: 0.12533147633075714 = 0.11851710826158524 + 0.001 * 6.814370155334473
Epoch 330, val loss: 0.7268343567848206
Epoch 340, training loss: 0.11108574271202087 = 0.10427158325910568 + 0.001 * 6.8141608238220215
Epoch 340, val loss: 0.7362581491470337
Epoch 350, training loss: 0.09900935739278793 = 0.09219305962324142 + 0.001 * 6.8162970542907715
Epoch 350, val loss: 0.746942400932312
Epoch 360, training loss: 0.0886647030711174 = 0.08184932172298431 + 0.001 * 6.815380096435547
Epoch 360, val loss: 0.7586349248886108
Epoch 370, training loss: 0.07972449064254761 = 0.0729101151227951 + 0.001 * 6.8143720626831055
Epoch 370, val loss: 0.7711121439933777
Epoch 380, training loss: 0.07195059210062027 = 0.06512486934661865 + 0.001 * 6.825724124908447
Epoch 380, val loss: 0.7843201160430908
Epoch 390, training loss: 0.06512702256441116 = 0.05831015110015869 + 0.001 * 6.816871166229248
Epoch 390, val loss: 0.7981482744216919
Epoch 400, training loss: 0.05914132297039032 = 0.052326932549476624 + 0.001 * 6.814390182495117
Epoch 400, val loss: 0.8125021457672119
Epoch 410, training loss: 0.053877364844083786 = 0.04706518352031708 + 0.001 * 6.812180995941162
Epoch 410, val loss: 0.8272951245307922
Epoch 420, training loss: 0.04924646019935608 = 0.04243515804409981 + 0.001 * 6.811300754547119
Epoch 420, val loss: 0.8424118161201477
Epoch 430, training loss: 0.045175619423389435 = 0.03836074098944664 + 0.001 * 6.814876556396484
Epoch 430, val loss: 0.8576912879943848
Epoch 440, training loss: 0.04158499836921692 = 0.03477436304092407 + 0.001 * 6.810633182525635
Epoch 440, val loss: 0.8730263710021973
Epoch 450, training loss: 0.03843773528933525 = 0.03161661699414253 + 0.001 * 6.821116924285889
Epoch 450, val loss: 0.8882578611373901
Epoch 460, training loss: 0.03564215824007988 = 0.028834208846092224 + 0.001 * 6.807949542999268
Epoch 460, val loss: 0.9031833410263062
Epoch 470, training loss: 0.03318407014012337 = 0.02637733891606331 + 0.001 * 6.806729793548584
Epoch 470, val loss: 0.9177208542823792
Epoch 480, training loss: 0.03101668879389763 = 0.024203099310398102 + 0.001 * 6.8135881423950195
Epoch 480, val loss: 0.931838870048523
Epoch 490, training loss: 0.02907579578459263 = 0.022273700684309006 + 0.001 * 6.802094459533691
Epoch 490, val loss: 0.9455143809318542
Epoch 500, training loss: 0.02735923044383526 = 0.020556850358843803 + 0.001 * 6.802380084991455
Epoch 500, val loss: 0.9587361812591553
Epoch 510, training loss: 0.02582894079387188 = 0.01902492344379425 + 0.001 * 6.804017543792725
Epoch 510, val loss: 0.9714829325675964
Epoch 520, training loss: 0.024453263729810715 = 0.01765388622879982 + 0.001 * 6.79937744140625
Epoch 520, val loss: 0.9838185906410217
Epoch 530, training loss: 0.02323523722589016 = 0.01642325520515442 + 0.001 * 6.811981678009033
Epoch 530, val loss: 0.9957321882247925
Epoch 540, training loss: 0.022106150165200233 = 0.015315419062972069 + 0.001 * 6.790731430053711
Epoch 540, val loss: 1.0072803497314453
Epoch 550, training loss: 0.021103691309690475 = 0.014315476641058922 + 0.001 * 6.788213729858398
Epoch 550, val loss: 1.0184454917907715
Epoch 560, training loss: 0.020219165831804276 = 0.013410407118499279 + 0.001 * 6.808758735656738
Epoch 560, val loss: 1.0292744636535645
Epoch 570, training loss: 0.019387340173125267 = 0.012589048594236374 + 0.001 * 6.798291206359863
Epoch 570, val loss: 1.0397450923919678
Epoch 580, training loss: 0.018626917153596878 = 0.011841777712106705 + 0.001 * 6.785140037536621
Epoch 580, val loss: 1.049919605255127
Epoch 590, training loss: 0.017942767590284348 = 0.011160179041326046 + 0.001 * 6.78258752822876
Epoch 590, val loss: 1.0597763061523438
Epoch 600, training loss: 0.017325062304735184 = 0.01053698267787695 + 0.001 * 6.788079738616943
Epoch 600, val loss: 1.0693265199661255
Epoch 610, training loss: 0.016737492755055428 = 0.009965982288122177 + 0.001 * 6.771510601043701
Epoch 610, val loss: 1.0786018371582031
Epoch 620, training loss: 0.01623833179473877 = 0.009441616013646126 + 0.001 * 6.796716213226318
Epoch 620, val loss: 1.0875979661941528
Epoch 630, training loss: 0.015727505087852478 = 0.008959099650382996 + 0.001 * 6.768405914306641
Epoch 630, val loss: 1.096372127532959
Epoch 640, training loss: 0.01527492143213749 = 0.008514142595231533 + 0.001 * 6.760777950286865
Epoch 640, val loss: 1.104878544807434
Epoch 650, training loss: 0.014862878248095512 = 0.008102981373667717 + 0.001 * 6.759896278381348
Epoch 650, val loss: 1.1131631135940552
Epoch 660, training loss: 0.014492587186396122 = 0.007722376845777035 + 0.001 * 6.770209789276123
Epoch 660, val loss: 1.121225118637085
Epoch 670, training loss: 0.014123179018497467 = 0.0073694223538041115 + 0.001 * 6.753756523132324
Epoch 670, val loss: 1.1290838718414307
Epoch 680, training loss: 0.013801781460642815 = 0.00704157492145896 + 0.001 * 6.76020622253418
Epoch 680, val loss: 1.1367303133010864
Epoch 690, training loss: 0.013493156060576439 = 0.006736476439982653 + 0.001 * 6.756679534912109
Epoch 690, val loss: 1.144199252128601
Epoch 700, training loss: 0.013223418034613132 = 0.006452122237533331 + 0.001 * 6.771295547485352
Epoch 700, val loss: 1.1514652967453003
Epoch 710, training loss: 0.01292648445814848 = 0.006186731159687042 + 0.001 * 6.739752769470215
Epoch 710, val loss: 1.1585811376571655
Epoch 720, training loss: 0.012678575702011585 = 0.005938628688454628 + 0.001 * 6.7399468421936035
Epoch 720, val loss: 1.1654877662658691
Epoch 730, training loss: 0.012468134984374046 = 0.005706361494958401 + 0.001 * 6.761773586273193
Epoch 730, val loss: 1.1722444295883179
Epoch 740, training loss: 0.01222648099064827 = 0.005488651338964701 + 0.001 * 6.737829208374023
Epoch 740, val loss: 1.1788252592086792
Epoch 750, training loss: 0.012026814743876457 = 0.00528426980599761 + 0.001 * 6.742544174194336
Epoch 750, val loss: 1.1852630376815796
Epoch 760, training loss: 0.011830722913146019 = 0.005092175677418709 + 0.001 * 6.738546848297119
Epoch 760, val loss: 1.1915520429611206
Epoch 770, training loss: 0.011639988049864769 = 0.004911412484943867 + 0.001 * 6.728574752807617
Epoch 770, val loss: 1.1976821422576904
Epoch 780, training loss: 0.01146221999078989 = 0.004741072654724121 + 0.001 * 6.721147060394287
Epoch 780, val loss: 1.2037087678909302
Epoch 790, training loss: 0.011308304965496063 = 0.004580399952828884 + 0.001 * 6.727904796600342
Epoch 790, val loss: 1.209590196609497
Epoch 800, training loss: 0.011138569563627243 = 0.00442869309335947 + 0.001 * 6.709876537322998
Epoch 800, val loss: 1.2153306007385254
Epoch 810, training loss: 0.011007947847247124 = 0.00428526196628809 + 0.001 * 6.72268533706665
Epoch 810, val loss: 1.220950961112976
Epoch 820, training loss: 0.010894615203142166 = 0.004149536602199078 + 0.001 * 6.7450785636901855
Epoch 820, val loss: 1.2264618873596191
Epoch 830, training loss: 0.010729790665209293 = 0.004020996857434511 + 0.001 * 6.708793640136719
Epoch 830, val loss: 1.2318288087844849
Epoch 840, training loss: 0.010630644857883453 = 0.0038990979082882404 + 0.001 * 6.731546401977539
Epoch 840, val loss: 1.2371008396148682
Epoch 850, training loss: 0.010497035458683968 = 0.003783425549045205 + 0.001 * 6.71360969543457
Epoch 850, val loss: 1.2422690391540527
Epoch 860, training loss: 0.010388209484517574 = 0.003673538099974394 + 0.001 * 6.7146711349487305
Epoch 860, val loss: 1.247305154800415
Epoch 870, training loss: 0.010271923616528511 = 0.0035690683871507645 + 0.001 * 6.702855110168457
Epoch 870, val loss: 1.2522624731063843
Epoch 880, training loss: 0.010174307972192764 = 0.0034696555230766535 + 0.001 * 6.704652309417725
Epoch 880, val loss: 1.257125735282898
Epoch 890, training loss: 0.01010760385543108 = 0.0033749649301171303 + 0.001 * 6.732638835906982
Epoch 890, val loss: 1.2618833780288696
Epoch 900, training loss: 0.00997919775545597 = 0.0032847351394593716 + 0.001 * 6.694462776184082
Epoch 900, val loss: 1.2665380239486694
Epoch 910, training loss: 0.009973745793104172 = 0.0031986634712666273 + 0.001 * 6.775082111358643
Epoch 910, val loss: 1.2711091041564941
Epoch 920, training loss: 0.009810554794967175 = 0.0031164956744760275 + 0.001 * 6.694058418273926
Epoch 920, val loss: 1.2756097316741943
Epoch 930, training loss: 0.009739960543811321 = 0.0030380189418792725 + 0.001 * 6.70194149017334
Epoch 930, val loss: 1.2800177335739136
Epoch 940, training loss: 0.009647700004279613 = 0.002962991828098893 + 0.001 * 6.684708118438721
Epoch 940, val loss: 1.284309983253479
Epoch 950, training loss: 0.009588689543306828 = 0.0028912394773215055 + 0.001 * 6.697449684143066
Epoch 950, val loss: 1.2885586023330688
Epoch 960, training loss: 0.009495239704847336 = 0.0028225653804838657 + 0.001 * 6.67267370223999
Epoch 960, val loss: 1.292701244354248
Epoch 970, training loss: 0.009428455494344234 = 0.0027568007353693247 + 0.001 * 6.671654224395752
Epoch 970, val loss: 1.2967902421951294
Epoch 980, training loss: 0.009384237229824066 = 0.0026937690563499928 + 0.001 * 6.690467834472656
Epoch 980, val loss: 1.3007851839065552
Epoch 990, training loss: 0.009333206340670586 = 0.002633296186104417 + 0.001 * 6.699909687042236
Epoch 990, val loss: 1.304749608039856
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6827
Flip ASR: 0.6311/225 nodes
The final ASR:0.77245, 0.14835, Accuracy:0.81975, 0.00698
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11560])
remove edge: torch.Size([2, 9504])
updated graph: torch.Size([2, 10508])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9379550218582153 = 1.9295812845230103 + 0.001 * 8.37376880645752
Epoch 0, val loss: 1.9339097738265991
Epoch 10, training loss: 1.9284645318984985 = 1.920090913772583 + 0.001 * 8.373652458190918
Epoch 10, val loss: 1.924336552619934
Epoch 20, training loss: 1.916599154472351 = 1.9082258939743042 + 0.001 * 8.373298645019531
Epoch 20, val loss: 1.9122766256332397
Epoch 30, training loss: 1.8997238874435425 = 1.891351342201233 + 0.001 * 8.372560501098633
Epoch 30, val loss: 1.8952213525772095
Epoch 40, training loss: 1.8747780323028564 = 1.8664072751998901 + 0.001 * 8.370732307434082
Epoch 40, val loss: 1.870439052581787
Epoch 50, training loss: 1.8407199382781982 = 1.832355260848999 + 0.001 * 8.364703178405762
Epoch 50, val loss: 1.838349461555481
Epoch 60, training loss: 1.8036227226257324 = 1.7952861785888672 + 0.001 * 8.336541175842285
Epoch 60, val loss: 1.8067318201065063
Epoch 70, training loss: 1.7666277885437012 = 1.7584575414657593 + 0.001 * 8.170241355895996
Epoch 70, val loss: 1.775521993637085
Epoch 80, training loss: 1.7148644924163818 = 1.7070976495742798 + 0.001 * 7.766852378845215
Epoch 80, val loss: 1.7294824123382568
Epoch 90, training loss: 1.6432565450668335 = 1.6355756521224976 + 0.001 * 7.680939197540283
Epoch 90, val loss: 1.667677640914917
Epoch 100, training loss: 1.553717851638794 = 1.5460859537124634 + 0.001 * 7.631934642791748
Epoch 100, val loss: 1.5935673713684082
Epoch 110, training loss: 1.4584201574325562 = 1.4508615732192993 + 0.001 * 7.5586323738098145
Epoch 110, val loss: 1.5154496431350708
Epoch 120, training loss: 1.3642982244491577 = 1.3569386005401611 + 0.001 * 7.3596415519714355
Epoch 120, val loss: 1.4426971673965454
Epoch 130, training loss: 1.2703697681427002 = 1.2633163928985596 + 0.001 * 7.0533318519592285
Epoch 130, val loss: 1.3734513521194458
Epoch 140, training loss: 1.1769449710845947 = 1.1699485778808594 + 0.001 * 6.996395111083984
Epoch 140, val loss: 1.3064370155334473
Epoch 150, training loss: 1.0858513116836548 = 1.0789023637771606 + 0.001 * 6.949006080627441
Epoch 150, val loss: 1.2432483434677124
Epoch 160, training loss: 0.998965322971344 = 0.9920341372489929 + 0.001 * 6.931182384490967
Epoch 160, val loss: 1.1843879222869873
Epoch 170, training loss: 0.9168582558631897 = 0.909937858581543 + 0.001 * 6.920413970947266
Epoch 170, val loss: 1.1289571523666382
Epoch 180, training loss: 0.8393011093139648 = 0.8323922753334045 + 0.001 * 6.90885591506958
Epoch 180, val loss: 1.075941562652588
Epoch 190, training loss: 0.7659515738487244 = 0.7590556740760803 + 0.001 * 6.895884037017822
Epoch 190, val loss: 1.0255688428878784
Epoch 200, training loss: 0.6966610550880432 = 0.6897786855697632 + 0.001 * 6.882368087768555
Epoch 200, val loss: 0.9780641794204712
Epoch 210, training loss: 0.6315730214118958 = 0.6247026324272156 + 0.001 * 6.870388984680176
Epoch 210, val loss: 0.9342470765113831
Epoch 220, training loss: 0.5711915493011475 = 0.5643308758735657 + 0.001 * 6.860665321350098
Epoch 220, val loss: 0.8950720429420471
Epoch 230, training loss: 0.5161329507827759 = 0.509280264377594 + 0.001 * 6.852694511413574
Epoch 230, val loss: 0.8614848852157593
Epoch 240, training loss: 0.466420978307724 = 0.4595754146575928 + 0.001 * 6.845562934875488
Epoch 240, val loss: 0.8340988159179688
Epoch 250, training loss: 0.4213469624519348 = 0.4145086407661438 + 0.001 * 6.838317394256592
Epoch 250, val loss: 0.8128094673156738
Epoch 260, training loss: 0.37972715497016907 = 0.37289637327194214 + 0.001 * 6.830773830413818
Epoch 260, val loss: 0.7967279553413391
Epoch 270, training loss: 0.3407486081123352 = 0.33392491936683655 + 0.001 * 6.823689937591553
Epoch 270, val loss: 0.7849772572517395
Epoch 280, training loss: 0.30433759093284607 = 0.29752078652381897 + 0.001 * 6.816807746887207
Epoch 280, val loss: 0.7771702408790588
Epoch 290, training loss: 0.27062249183654785 = 0.2638125717639923 + 0.001 * 6.809928894042969
Epoch 290, val loss: 0.7729473114013672
Epoch 300, training loss: 0.23961129784584045 = 0.23280732333660126 + 0.001 * 6.803969860076904
Epoch 300, val loss: 0.7721123099327087
Epoch 310, training loss: 0.2112874984741211 = 0.20448976755142212 + 0.001 * 6.797735691070557
Epoch 310, val loss: 0.7745856046676636
Epoch 320, training loss: 0.18571162223815918 = 0.17892003059387207 + 0.001 * 6.7915873527526855
Epoch 320, val loss: 0.7802801132202148
Epoch 330, training loss: 0.1630992740392685 = 0.15631292760372162 + 0.001 * 6.786349296569824
Epoch 330, val loss: 0.7892734408378601
Epoch 340, training loss: 0.1435636430978775 = 0.13678433001041412 + 0.001 * 6.77931547164917
Epoch 340, val loss: 0.8014102578163147
Epoch 350, training loss: 0.12693721055984497 = 0.12016380578279495 + 0.001 * 6.773406982421875
Epoch 350, val loss: 0.816572368144989
Epoch 360, training loss: 0.11283546686172485 = 0.10606768727302551 + 0.001 * 6.767781734466553
Epoch 360, val loss: 0.8341754674911499
Epoch 370, training loss: 0.1008392721414566 = 0.09407657384872437 + 0.001 * 6.762701511383057
Epoch 370, val loss: 0.8533627390861511
Epoch 380, training loss: 0.0905814841389656 = 0.08382634818553925 + 0.001 * 6.75513219833374
Epoch 380, val loss: 0.8734839558601379
Epoch 390, training loss: 0.08176293224096298 = 0.07501260191202164 + 0.001 * 6.750329971313477
Epoch 390, val loss: 0.8941102027893066
Epoch 400, training loss: 0.07413782179355621 = 0.06739138066768646 + 0.001 * 6.746443271636963
Epoch 400, val loss: 0.9148379564285278
Epoch 410, training loss: 0.06750496476888657 = 0.06076429784297943 + 0.001 * 6.740665912628174
Epoch 410, val loss: 0.9354330897331238
Epoch 420, training loss: 0.06170854717493057 = 0.054970741271972656 + 0.001 * 6.737804412841797
Epoch 420, val loss: 0.9556631445884705
Epoch 430, training loss: 0.0566185787320137 = 0.04988018423318863 + 0.001 * 6.738396167755127
Epoch 430, val loss: 0.9755086302757263
Epoch 440, training loss: 0.05211983248591423 = 0.04538566246628761 + 0.001 * 6.734169960021973
Epoch 440, val loss: 0.9948986768722534
Epoch 450, training loss: 0.04813525080680847 = 0.04140366241335869 + 0.001 * 6.731586456298828
Epoch 450, val loss: 1.013804316520691
Epoch 460, training loss: 0.044596221297979355 = 0.03786703571677208 + 0.001 * 6.729185104370117
Epoch 460, val loss: 1.032269835472107
Epoch 470, training loss: 0.04144451767206192 = 0.034717585891485214 + 0.001 * 6.726932525634766
Epoch 470, val loss: 1.0502597093582153
Epoch 480, training loss: 0.0386480987071991 = 0.03190562501549721 + 0.001 * 6.742472171783447
Epoch 480, val loss: 1.0678212642669678
Epoch 490, training loss: 0.03611388057470322 = 0.029389653354883194 + 0.001 * 6.724226951599121
Epoch 490, val loss: 1.0849661827087402
Epoch 500, training loss: 0.03385715186595917 = 0.027133863419294357 + 0.001 * 6.723288536071777
Epoch 500, val loss: 1.1016170978546143
Epoch 510, training loss: 0.03182779625058174 = 0.02510751411318779 + 0.001 * 6.720280647277832
Epoch 510, val loss: 1.117821455001831
Epoch 520, training loss: 0.03000214323401451 = 0.023283900693058968 + 0.001 * 6.718241214752197
Epoch 520, val loss: 1.1335543394088745
Epoch 530, training loss: 0.028355948626995087 = 0.021639423444867134 + 0.001 * 6.716524600982666
Epoch 530, val loss: 1.1488571166992188
Epoch 540, training loss: 0.02687479741871357 = 0.020153548568487167 + 0.001 * 6.721249103546143
Epoch 540, val loss: 1.16371750831604
Epoch 550, training loss: 0.025522101670503616 = 0.018808593973517418 + 0.001 * 6.713507175445557
Epoch 550, val loss: 1.178118348121643
Epoch 560, training loss: 0.024299850687384605 = 0.01758868619799614 + 0.001 * 6.711164474487305
Epoch 560, val loss: 1.1920969486236572
Epoch 570, training loss: 0.02319354936480522 = 0.01647965982556343 + 0.001 * 6.713888168334961
Epoch 570, val loss: 1.2056481838226318
Epoch 580, training loss: 0.0221813153475523 = 0.015469647012650967 + 0.001 * 6.711668014526367
Epoch 580, val loss: 1.2188208103179932
Epoch 590, training loss: 0.021256837993860245 = 0.014547726139426231 + 0.001 * 6.709112167358398
Epoch 590, val loss: 1.2316031455993652
Epoch 600, training loss: 0.020409798249602318 = 0.013704500161111355 + 0.001 * 6.705297946929932
Epoch 600, val loss: 1.244022250175476
Epoch 610, training loss: 0.019636230543255806 = 0.012931752018630505 + 0.001 * 6.704477787017822
Epoch 610, val loss: 1.2560213804244995
Epoch 620, training loss: 0.01892791874706745 = 0.012222211807966232 + 0.001 * 6.7057061195373535
Epoch 620, val loss: 1.2677063941955566
Epoch 630, training loss: 0.01827070489525795 = 0.01156965084373951 + 0.001 * 6.701054573059082
Epoch 630, val loss: 1.2790100574493408
Epoch 640, training loss: 0.017686285078525543 = 0.010968301445245743 + 0.001 * 6.717982769012451
Epoch 640, val loss: 1.290026068687439
Epoch 650, training loss: 0.017111696302890778 = 0.010413327254354954 + 0.001 * 6.698367595672607
Epoch 650, val loss: 1.3007184267044067
Epoch 660, training loss: 0.01659821718931198 = 0.009900037199258804 + 0.001 * 6.698179244995117
Epoch 660, val loss: 1.3111268281936646
Epoch 670, training loss: 0.016121257096529007 = 0.009424570016562939 + 0.001 * 6.696685791015625
Epoch 670, val loss: 1.3212060928344727
Epoch 680, training loss: 0.015688549727201462 = 0.008983397856354713 + 0.001 * 6.70515251159668
Epoch 680, val loss: 1.3310481309890747
Epoch 690, training loss: 0.01526738703250885 = 0.008573337458074093 + 0.001 * 6.694048881530762
Epoch 690, val loss: 1.340605616569519
Epoch 700, training loss: 0.014884267002344131 = 0.008191688917577267 + 0.001 * 6.692577838897705
Epoch 700, val loss: 1.3499174118041992
Epoch 710, training loss: 0.01452646218240261 = 0.00783595908433199 + 0.001 * 6.690503120422363
Epoch 710, val loss: 1.3589682579040527
Epoch 720, training loss: 0.014199280180037022 = 0.007503884378820658 + 0.001 * 6.695395469665527
Epoch 720, val loss: 1.3677934408187866
Epoch 730, training loss: 0.013882500119507313 = 0.0071934363804757595 + 0.001 * 6.689063549041748
Epoch 730, val loss: 1.3763608932495117
Epoch 740, training loss: 0.013589613139629364 = 0.006902852561324835 + 0.001 * 6.686759948730469
Epoch 740, val loss: 1.3847278356552124
Epoch 750, training loss: 0.013320056721568108 = 0.006630495190620422 + 0.001 * 6.689560890197754
Epoch 750, val loss: 1.3929005861282349
Epoch 760, training loss: 0.013059718534350395 = 0.0063749016262590885 + 0.001 * 6.684816837310791
Epoch 760, val loss: 1.4008973836898804
Epoch 770, training loss: 0.012818984687328339 = 0.006134780589491129 + 0.001 * 6.684203624725342
Epoch 770, val loss: 1.408691644668579
Epoch 780, training loss: 0.01259627379477024 = 0.00590891158208251 + 0.001 * 6.687361240386963
Epoch 780, val loss: 1.4162235260009766
Epoch 790, training loss: 0.012386802583932877 = 0.005696204956620932 + 0.001 * 6.6905975341796875
Epoch 790, val loss: 1.4236421585083008
Epoch 800, training loss: 0.01217500027269125 = 0.005495672579854727 + 0.001 * 6.679327487945557
Epoch 800, val loss: 1.4308505058288574
Epoch 810, training loss: 0.011989055201411247 = 0.00530631747096777 + 0.001 * 6.682737350463867
Epoch 810, val loss: 1.4378522634506226
Epoch 820, training loss: 0.011797185987234116 = 0.00512714171782136 + 0.001 * 6.6700439453125
Epoch 820, val loss: 1.4447581768035889
Epoch 830, training loss: 0.011628173291683197 = 0.0049565513618290424 + 0.001 * 6.671621322631836
Epoch 830, val loss: 1.4516048431396484
Epoch 840, training loss: 0.011504799127578735 = 0.004792392253875732 + 0.001 * 6.712407112121582
Epoch 840, val loss: 1.458495855331421
Epoch 850, training loss: 0.011308331042528152 = 0.00463318545371294 + 0.001 * 6.675145626068115
Epoch 850, val loss: 1.4654244184494019
Epoch 860, training loss: 0.011149924248456955 = 0.004478384740650654 + 0.001 * 6.671538829803467
Epoch 860, val loss: 1.4723612070083618
Epoch 870, training loss: 0.01099918782711029 = 0.004328272305428982 + 0.001 * 6.670915126800537
Epoch 870, val loss: 1.4793121814727783
Epoch 880, training loss: 0.010856909677386284 = 0.004183344542980194 + 0.001 * 6.673564910888672
Epoch 880, val loss: 1.4861724376678467
Epoch 890, training loss: 0.010709422640502453 = 0.004043967928737402 + 0.001 * 6.665454387664795
Epoch 890, val loss: 1.49294114112854
Epoch 900, training loss: 0.010563943535089493 = 0.003910392057150602 + 0.001 * 6.65355110168457
Epoch 900, val loss: 1.4996135234832764
Epoch 910, training loss: 0.010460833087563515 = 0.003782709361985326 + 0.001 * 6.678123474121094
Epoch 910, val loss: 1.5061390399932861
Epoch 920, training loss: 0.010311349295079708 = 0.0036609084345400333 + 0.001 * 6.650440692901611
Epoch 920, val loss: 1.5125725269317627
Epoch 930, training loss: 0.010229947976768017 = 0.003544817678630352 + 0.001 * 6.6851301193237305
Epoch 930, val loss: 1.518847942352295
Epoch 940, training loss: 0.01007944718003273 = 0.0034342689905315638 + 0.001 * 6.645178318023682
Epoch 940, val loss: 1.5250203609466553
Epoch 950, training loss: 0.009976841509342194 = 0.0033290700521320105 + 0.001 * 6.647770881652832
Epoch 950, val loss: 1.5310356616973877
Epoch 960, training loss: 0.00991862453520298 = 0.0032289784867316484 + 0.001 * 6.689645290374756
Epoch 960, val loss: 1.5369315147399902
Epoch 970, training loss: 0.009776154533028603 = 0.003133739111945033 + 0.001 * 6.6424150466918945
Epoch 970, val loss: 1.5426995754241943
Epoch 980, training loss: 0.0097025902941823 = 0.003043112577870488 + 0.001 * 6.659477233886719
Epoch 980, val loss: 1.5483331680297852
Epoch 990, training loss: 0.009600372053682804 = 0.0029568346217274666 + 0.001 * 6.6435370445251465
Epoch 990, val loss: 1.5538612604141235
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.5387
Flip ASR: 0.4711/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9694669246673584 = 1.9610930681228638 + 0.001 * 8.373799324035645
Epoch 0, val loss: 1.9640089273452759
Epoch 10, training loss: 1.959272861480713 = 1.9508991241455078 + 0.001 * 8.373687744140625
Epoch 10, val loss: 1.954045295715332
Epoch 20, training loss: 1.9466956853866577 = 1.9383223056793213 + 0.001 * 8.373361587524414
Epoch 20, val loss: 1.9412606954574585
Epoch 30, training loss: 1.92906653881073 = 1.9206938743591309 + 0.001 * 8.372636795043945
Epoch 30, val loss: 1.9229806661605835
Epoch 40, training loss: 1.903053879737854 = 1.8946830034255981 + 0.001 * 8.370923042297363
Epoch 40, val loss: 1.8960391283035278
Epoch 50, training loss: 1.8656667470932007 = 1.8573007583618164 + 0.001 * 8.365961074829102
Epoch 50, val loss: 1.8583803176879883
Epoch 60, training loss: 1.8185547590255737 = 1.8102086782455444 + 0.001 * 8.346039772033691
Epoch 60, val loss: 1.813679814338684
Epoch 70, training loss: 1.770336389541626 = 1.7621033191680908 + 0.001 * 8.233036041259766
Epoch 70, val loss: 1.770707607269287
Epoch 80, training loss: 1.7174181938171387 = 1.7096893787384033 + 0.001 * 7.728759765625
Epoch 80, val loss: 1.7225123643875122
Epoch 90, training loss: 1.6453258991241455 = 1.637866497039795 + 0.001 * 7.459370136260986
Epoch 90, val loss: 1.6577682495117188
Epoch 100, training loss: 1.5501519441604614 = 1.54291570186615 + 0.001 * 7.236260890960693
Epoch 100, val loss: 1.5765478610992432
Epoch 110, training loss: 1.437980055809021 = 1.4308762550354004 + 0.001 * 7.103847980499268
Epoch 110, val loss: 1.4838663339614868
Epoch 120, training loss: 1.3229929208755493 = 1.315892219543457 + 0.001 * 7.100667476654053
Epoch 120, val loss: 1.3914391994476318
Epoch 130, training loss: 1.2124404907226562 = 1.2053518295288086 + 0.001 * 7.088687896728516
Epoch 130, val loss: 1.3060756921768188
Epoch 140, training loss: 1.1068596839904785 = 1.0997899770736694 + 0.001 * 7.0696892738342285
Epoch 140, val loss: 1.2267619371414185
Epoch 150, training loss: 1.005150318145752 = 0.9980979561805725 + 0.001 * 7.052387237548828
Epoch 150, val loss: 1.1507991552352905
Epoch 160, training loss: 0.9083365201950073 = 0.9013077616691589 + 0.001 * 7.028733253479004
Epoch 160, val loss: 1.0792770385742188
Epoch 170, training loss: 0.819827139377594 = 0.8128264546394348 + 0.001 * 7.000696659088135
Epoch 170, val loss: 1.0149357318878174
Epoch 180, training loss: 0.7423613667488098 = 0.7353897094726562 + 0.001 * 6.97166633605957
Epoch 180, val loss: 0.9610005617141724
Epoch 190, training loss: 0.6758031845092773 = 0.6688580513000488 + 0.001 * 6.945122241973877
Epoch 190, val loss: 0.918289303779602
Epoch 200, training loss: 0.6178007125854492 = 0.61087965965271 + 0.001 * 6.921061992645264
Epoch 200, val loss: 0.885367751121521
Epoch 210, training loss: 0.565558910369873 = 0.5586602687835693 + 0.001 * 6.898658275604248
Epoch 210, val loss: 0.8594777584075928
Epoch 220, training loss: 0.5169275403022766 = 0.5100501179695129 + 0.001 * 6.877434253692627
Epoch 220, val loss: 0.8381760120391846
Epoch 230, training loss: 0.4705132246017456 = 0.46365776658058167 + 0.001 * 6.855458736419678
Epoch 230, val loss: 0.8191846013069153
Epoch 240, training loss: 0.42570066452026367 = 0.4188695251941681 + 0.001 * 6.831142425537109
Epoch 240, val loss: 0.8019115924835205
Epoch 250, training loss: 0.3825424015522003 = 0.3757324814796448 + 0.001 * 6.8099164962768555
Epoch 250, val loss: 0.7869495749473572
Epoch 260, training loss: 0.3415082097053528 = 0.334716260433197 + 0.001 * 6.791962146759033
Epoch 260, val loss: 0.7748610973358154
Epoch 270, training loss: 0.3033132553100586 = 0.29653164744377136 + 0.001 * 6.781619071960449
Epoch 270, val loss: 0.7661100029945374
Epoch 280, training loss: 0.26837024092674255 = 0.26159727573394775 + 0.001 * 6.772952556610107
Epoch 280, val loss: 0.7612340450286865
Epoch 290, training loss: 0.2369839996099472 = 0.2302163541316986 + 0.001 * 6.767641067504883
Epoch 290, val loss: 0.7606669068336487
Epoch 300, training loss: 0.20915481448173523 = 0.20238636434078217 + 0.001 * 6.768448829650879
Epoch 300, val loss: 0.7644239068031311
Epoch 310, training loss: 0.1846909523010254 = 0.17792990803718567 + 0.001 * 6.761039733886719
Epoch 310, val loss: 0.7717403173446655
Epoch 320, training loss: 0.16337965428829193 = 0.15662448108196259 + 0.001 * 6.755178928375244
Epoch 320, val loss: 0.7821031212806702
Epoch 330, training loss: 0.14500398933887482 = 0.1382552832365036 + 0.001 * 6.748706340789795
Epoch 330, val loss: 0.795170783996582
Epoch 340, training loss: 0.1292574405670166 = 0.12251077592372894 + 0.001 * 6.746661186218262
Epoch 340, val loss: 0.8103222250938416
Epoch 350, training loss: 0.11570332199335098 = 0.10896613448858261 + 0.001 * 6.737190246582031
Epoch 350, val loss: 0.8268657922744751
Epoch 360, training loss: 0.10394053906202316 = 0.09720809757709503 + 0.001 * 6.7324419021606445
Epoch 360, val loss: 0.8444711565971375
Epoch 370, training loss: 0.09365644305944443 = 0.0869312658905983 + 0.001 * 6.725177764892578
Epoch 370, val loss: 0.8624566793441772
Epoch 380, training loss: 0.08460398018360138 = 0.07788107544183731 + 0.001 * 6.722901821136475
Epoch 380, val loss: 0.8808346390724182
Epoch 390, training loss: 0.07657070457935333 = 0.06986242532730103 + 0.001 * 6.708280086517334
Epoch 390, val loss: 0.8993862867355347
Epoch 400, training loss: 0.06944401562213898 = 0.06274361908435822 + 0.001 * 6.700396537780762
Epoch 400, val loss: 0.9179359674453735
Epoch 410, training loss: 0.06311755627393723 = 0.05641802400350571 + 0.001 * 6.699532985687256
Epoch 410, val loss: 0.9363731741905212
Epoch 420, training loss: 0.05751248449087143 = 0.050807710736989975 + 0.001 * 6.704774379730225
Epoch 420, val loss: 0.9545003175735474
Epoch 430, training loss: 0.0525352917611599 = 0.045847196131944656 + 0.001 * 6.688096523284912
Epoch 430, val loss: 0.9723484516143799
Epoch 440, training loss: 0.04814721271395683 = 0.04146718606352806 + 0.001 * 6.680025100708008
Epoch 440, val loss: 0.9899784922599792
Epoch 450, training loss: 0.044282909482717514 = 0.03760773316025734 + 0.001 * 6.675174713134766
Epoch 450, val loss: 1.0072996616363525
Epoch 460, training loss: 0.0408763512969017 = 0.03420596942305565 + 0.001 * 6.670382022857666
Epoch 460, val loss: 1.024137258529663
Epoch 470, training loss: 0.037879280745983124 = 0.03120325319468975 + 0.001 * 6.676029205322266
Epoch 470, val loss: 1.0405842065811157
Epoch 480, training loss: 0.03521785885095596 = 0.02854541502892971 + 0.001 * 6.6724419593811035
Epoch 480, val loss: 1.056551218032837
Epoch 490, training loss: 0.03285670280456543 = 0.026188647374510765 + 0.001 * 6.668054103851318
Epoch 490, val loss: 1.0720319747924805
Epoch 500, training loss: 0.03075900673866272 = 0.02409476228058338 + 0.001 * 6.664245128631592
Epoch 500, val loss: 1.0869317054748535
Epoch 510, training loss: 0.028892474249005318 = 0.02222992293536663 + 0.001 * 6.662550926208496
Epoch 510, val loss: 1.1014912128448486
Epoch 520, training loss: 0.027235452085733414 = 0.02056436985731125 + 0.001 * 6.67108154296875
Epoch 520, val loss: 1.1154919862747192
Epoch 530, training loss: 0.02573506534099579 = 0.019073154777288437 + 0.001 * 6.661909103393555
Epoch 530, val loss: 1.1289916038513184
Epoch 540, training loss: 0.024389266967773438 = 0.017734535038471222 + 0.001 * 6.6547322273254395
Epoch 540, val loss: 1.1420276165008545
Epoch 550, training loss: 0.02319551445543766 = 0.01652926206588745 + 0.001 * 6.6662516593933105
Epoch 550, val loss: 1.1546566486358643
Epoch 560, training loss: 0.02210046537220478 = 0.01544239278882742 + 0.001 * 6.6580729484558105
Epoch 560, val loss: 1.166890263557434
Epoch 570, training loss: 0.021112890914082527 = 0.01445848774164915 + 0.001 * 6.654403209686279
Epoch 570, val loss: 1.1786444187164307
Epoch 580, training loss: 0.020224515348672867 = 0.013565446250140667 + 0.001 * 6.659067630767822
Epoch 580, val loss: 1.1900051832199097
Epoch 590, training loss: 0.019404198974370956 = 0.012752890586853027 + 0.001 * 6.651309013366699
Epoch 590, val loss: 1.2010507583618164
Epoch 600, training loss: 0.018656646832823753 = 0.012011495418846607 + 0.001 * 6.645151138305664
Epoch 600, val loss: 1.2117393016815186
Epoch 610, training loss: 0.017995011061429977 = 0.011333734728395939 + 0.001 * 6.6612749099731445
Epoch 610, val loss: 1.2220743894577026
Epoch 620, training loss: 0.0173572339117527 = 0.010712968185544014 + 0.001 * 6.6442646980285645
Epoch 620, val loss: 1.2320852279663086
Epoch 630, training loss: 0.016779761761426926 = 0.010143020190298557 + 0.001 * 6.636741638183594
Epoch 630, val loss: 1.241790771484375
Epoch 640, training loss: 0.016264205798506737 = 0.009618506766855717 + 0.001 * 6.6456990242004395
Epoch 640, val loss: 1.2512257099151611
Epoch 650, training loss: 0.01578647643327713 = 0.009134805761277676 + 0.001 * 6.651670932769775
Epoch 650, val loss: 1.260379433631897
Epoch 660, training loss: 0.015321573242545128 = 0.008687915280461311 + 0.001 * 6.633656978607178
Epoch 660, val loss: 1.269276738166809
Epoch 670, training loss: 0.014907277189195156 = 0.008274019695818424 + 0.001 * 6.6332573890686035
Epoch 670, val loss: 1.2779181003570557
Epoch 680, training loss: 0.014528725296258926 = 0.007889963686466217 + 0.001 * 6.638761520385742
Epoch 680, val loss: 1.2863459587097168
Epoch 690, training loss: 0.014168942347168922 = 0.007533245254307985 + 0.001 * 6.6356964111328125
Epoch 690, val loss: 1.2945196628570557
Epoch 700, training loss: 0.01385857630521059 = 0.007201367057859898 + 0.001 * 6.6572089195251465
Epoch 700, val loss: 1.3024600744247437
Epoch 710, training loss: 0.013526802882552147 = 0.006892147473990917 + 0.001 * 6.634654521942139
Epoch 710, val loss: 1.3101835250854492
Epoch 720, training loss: 0.013235067948698997 = 0.006603604182600975 + 0.001 * 6.631463527679443
Epoch 720, val loss: 1.3176612854003906
Epoch 730, training loss: 0.012962538748979568 = 0.0063338917680084705 + 0.001 * 6.628646373748779
Epoch 730, val loss: 1.3250246047973633
Epoch 740, training loss: 0.012707038782536983 = 0.006081495899707079 + 0.001 * 6.625542640686035
Epoch 740, val loss: 1.332129716873169
Epoch 750, training loss: 0.012478834949433804 = 0.005844995379447937 + 0.001 * 6.633839130401611
Epoch 750, val loss: 1.3390382528305054
Epoch 760, training loss: 0.012242069467902184 = 0.005622998345643282 + 0.001 * 6.6190714836120605
Epoch 760, val loss: 1.345832347869873
Epoch 770, training loss: 0.012035309337079525 = 0.005414342507719994 + 0.001 * 6.62096643447876
Epoch 770, val loss: 1.352421760559082
Epoch 780, training loss: 0.011849744245409966 = 0.005217786878347397 + 0.001 * 6.631956577301025
Epoch 780, val loss: 1.3589107990264893
Epoch 790, training loss: 0.011661263182759285 = 0.005032388027757406 + 0.001 * 6.6288743019104
Epoch 790, val loss: 1.365220308303833
Epoch 800, training loss: 0.011465763673186302 = 0.00485738180577755 + 0.001 * 6.608381748199463
Epoch 800, val loss: 1.3713876008987427
Epoch 810, training loss: 0.0113258371129632 = 0.0046918392181396484 + 0.001 * 6.633997440338135
Epoch 810, val loss: 1.3774456977844238
Epoch 820, training loss: 0.011150063946843147 = 0.004534922540187836 + 0.001 * 6.615140438079834
Epoch 820, val loss: 1.3833987712860107
Epoch 830, training loss: 0.011014724150300026 = 0.0043859779834747314 + 0.001 * 6.628746509552002
Epoch 830, val loss: 1.3891974687576294
Epoch 840, training loss: 0.010868354700505733 = 0.004244570154696703 + 0.001 * 6.623784065246582
Epoch 840, val loss: 1.3949546813964844
Epoch 850, training loss: 0.01072409562766552 = 0.004110078793019056 + 0.001 * 6.614017009735107
Epoch 850, val loss: 1.4006118774414062
Epoch 860, training loss: 0.010597001761198044 = 0.003982064314186573 + 0.001 * 6.614937782287598
Epoch 860, val loss: 1.4060850143432617
Epoch 870, training loss: 0.010465151630342007 = 0.0038601348642259836 + 0.001 * 6.605016231536865
Epoch 870, val loss: 1.4115480184555054
Epoch 880, training loss: 0.010356717742979527 = 0.0037439751904457808 + 0.001 * 6.612741947174072
Epoch 880, val loss: 1.416887640953064
Epoch 890, training loss: 0.01022897195070982 = 0.0036331701558083296 + 0.001 * 6.59580135345459
Epoch 890, val loss: 1.4220941066741943
Epoch 900, training loss: 0.010140514001250267 = 0.0035273381508886814 + 0.001 * 6.613174915313721
Epoch 900, val loss: 1.427260160446167
Epoch 910, training loss: 0.010029099881649017 = 0.0034263250418007374 + 0.001 * 6.602774620056152
Epoch 910, val loss: 1.4323019981384277
Epoch 920, training loss: 0.00994846597313881 = 0.0033295229077339172 + 0.001 * 6.618943214416504
Epoch 920, val loss: 1.437279462814331
Epoch 930, training loss: 0.009859330952167511 = 0.0032369273249059916 + 0.001 * 6.622403144836426
Epoch 930, val loss: 1.4421573877334595
Epoch 940, training loss: 0.009763467125594616 = 0.003148299176245928 + 0.001 * 6.615167617797852
Epoch 940, val loss: 1.4470182657241821
Epoch 950, training loss: 0.009664492681622505 = 0.0030632761772722006 + 0.001 * 6.601215839385986
Epoch 950, val loss: 1.4517061710357666
Epoch 960, training loss: 0.009608780033886433 = 0.002981724916025996 + 0.001 * 6.6270551681518555
Epoch 960, val loss: 1.4562605619430542
Epoch 970, training loss: 0.009503735229372978 = 0.0029035804327577353 + 0.001 * 6.600154399871826
Epoch 970, val loss: 1.4608474969863892
Epoch 980, training loss: 0.009422997012734413 = 0.00282872817479074 + 0.001 * 6.594268798828125
Epoch 980, val loss: 1.465262532234192
Epoch 990, training loss: 0.00935971736907959 = 0.0027571483515203 + 0.001 * 6.602569103240967
Epoch 990, val loss: 1.4696811437606812
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6863
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9717046022415161 = 1.963330864906311 + 0.001 * 8.373785972595215
Epoch 0, val loss: 1.9601713418960571
Epoch 10, training loss: 1.961226224899292 = 1.952852487564087 + 0.001 * 8.373717308044434
Epoch 10, val loss: 1.9497895240783691
Epoch 20, training loss: 1.948606252670288 = 1.940232753753662 + 0.001 * 8.373438835144043
Epoch 20, val loss: 1.9367824792861938
Epoch 30, training loss: 1.931321382522583 = 1.9229485988616943 + 0.001 * 8.37283992767334
Epoch 30, val loss: 1.918717861175537
Epoch 40, training loss: 1.9058960676193237 = 1.8975247144699097 + 0.001 * 8.371400833129883
Epoch 40, val loss: 1.8922985792160034
Epoch 50, training loss: 1.8688559532165527 = 1.8604888916015625 + 0.001 * 8.367061614990234
Epoch 50, val loss: 1.854981541633606
Epoch 60, training loss: 1.8224080801010132 = 1.814059853553772 + 0.001 * 8.348182678222656
Epoch 60, val loss: 1.81161367893219
Epoch 70, training loss: 1.778581976890564 = 1.770351529121399 + 0.001 * 8.230405807495117
Epoch 70, val loss: 1.7755885124206543
Epoch 80, training loss: 1.7335880994796753 = 1.7258388996124268 + 0.001 * 7.749237060546875
Epoch 80, val loss: 1.738189697265625
Epoch 90, training loss: 1.6723995208740234 = 1.6649246215820312 + 0.001 * 7.474846363067627
Epoch 90, val loss: 1.6855885982513428
Epoch 100, training loss: 1.5902783870697021 = 1.5830543041229248 + 0.001 * 7.224081993103027
Epoch 100, val loss: 1.6165086030960083
Epoch 110, training loss: 1.487800121307373 = 1.4807155132293701 + 0.001 * 7.0845770835876465
Epoch 110, val loss: 1.5323710441589355
Epoch 120, training loss: 1.376149296760559 = 1.3691153526306152 + 0.001 * 7.033968448638916
Epoch 120, val loss: 1.4414514303207397
Epoch 130, training loss: 1.2627007961273193 = 1.2557222843170166 + 0.001 * 6.978481292724609
Epoch 130, val loss: 1.3519476652145386
Epoch 140, training loss: 1.149116039276123 = 1.142175316810608 + 0.001 * 6.940666198730469
Epoch 140, val loss: 1.2643465995788574
Epoch 150, training loss: 1.0371149778366089 = 1.0301942825317383 + 0.001 * 6.920678615570068
Epoch 150, val loss: 1.1793508529663086
Epoch 160, training loss: 0.9309811592102051 = 0.9240816831588745 + 0.001 * 6.899473190307617
Epoch 160, val loss: 1.1007150411605835
Epoch 170, training loss: 0.8353228569030762 = 0.8284461498260498 + 0.001 * 6.876728534698486
Epoch 170, val loss: 1.031818151473999
Epoch 180, training loss: 0.7522401213645935 = 0.7453889846801758 + 0.001 * 6.8511576652526855
Epoch 180, val loss: 0.9743766784667969
Epoch 190, training loss: 0.6806222200393677 = 0.6737985014915466 + 0.001 * 6.823708534240723
Epoch 190, val loss: 0.9277179837226868
Epoch 200, training loss: 0.6179035902023315 = 0.6111067533493042 + 0.001 * 6.796834945678711
Epoch 200, val loss: 0.8899193406105042
Epoch 210, training loss: 0.5616912245750427 = 0.5549158453941345 + 0.001 * 6.775354385375977
Epoch 210, val loss: 0.8587371110916138
Epoch 220, training loss: 0.5101776123046875 = 0.5034134984016418 + 0.001 * 6.764138221740723
Epoch 220, val loss: 0.8327220678329468
Epoch 230, training loss: 0.4622408151626587 = 0.45548760890960693 + 0.001 * 6.7531914710998535
Epoch 230, val loss: 0.8117472529411316
Epoch 240, training loss: 0.4171398878097534 = 0.41039276123046875 + 0.001 * 6.747119426727295
Epoch 240, val loss: 0.7956432104110718
Epoch 250, training loss: 0.3744592070579529 = 0.36771661043167114 + 0.001 * 6.742590427398682
Epoch 250, val loss: 0.7836387157440186
Epoch 260, training loss: 0.3338641822338104 = 0.3271237313747406 + 0.001 * 6.740447044372559
Epoch 260, val loss: 0.7746689915657043
Epoch 270, training loss: 0.29519379138946533 = 0.2884562611579895 + 0.001 * 6.7375311851501465
Epoch 270, val loss: 0.7679535746574402
Epoch 280, training loss: 0.2586744725704193 = 0.2519413232803345 + 0.001 * 6.7331461906433105
Epoch 280, val loss: 0.7632557153701782
Epoch 290, training loss: 0.22496524453163147 = 0.21823571622371674 + 0.001 * 6.729528427124023
Epoch 290, val loss: 0.7604450583457947
Epoch 300, training loss: 0.19475121796131134 = 0.1880253255367279 + 0.001 * 6.725889682769775
Epoch 300, val loss: 0.7597348093986511
Epoch 310, training loss: 0.16834497451782227 = 0.16162201762199402 + 0.001 * 6.72295618057251
Epoch 310, val loss: 0.7609690427780151
Epoch 320, training loss: 0.1456415355205536 = 0.1389244794845581 + 0.001 * 6.7170610427856445
Epoch 320, val loss: 0.7643093466758728
Epoch 330, training loss: 0.12630170583724976 = 0.11958973854780197 + 0.001 * 6.711963653564453
Epoch 330, val loss: 0.7694869637489319
Epoch 340, training loss: 0.10991483181715012 = 0.10320410877466202 + 0.001 * 6.710720062255859
Epoch 340, val loss: 0.7765404582023621
Epoch 350, training loss: 0.09604368358850479 = 0.0893407016992569 + 0.001 * 6.702983379364014
Epoch 350, val loss: 0.785272479057312
Epoch 360, training loss: 0.08432408422231674 = 0.07762724906206131 + 0.001 * 6.696834564208984
Epoch 360, val loss: 0.7954815626144409
Epoch 370, training loss: 0.07443909347057343 = 0.06774406880140305 + 0.001 * 6.6950225830078125
Epoch 370, val loss: 0.8069953322410583
Epoch 380, training loss: 0.06609459966421127 = 0.05941115692257881 + 0.001 * 6.683439254760742
Epoch 380, val loss: 0.8194010853767395
Epoch 390, training loss: 0.05906003713607788 = 0.0523785836994648 + 0.001 * 6.681451320648193
Epoch 390, val loss: 0.8325932025909424
Epoch 400, training loss: 0.05310797691345215 = 0.0464305505156517 + 0.001 * 6.677425861358643
Epoch 400, val loss: 0.8461660742759705
Epoch 410, training loss: 0.048050880432128906 = 0.04138203710317612 + 0.001 * 6.66884183883667
Epoch 410, val loss: 0.8598695397377014
Epoch 420, training loss: 0.043761592358350754 = 0.03707575052976608 + 0.001 * 6.685841083526611
Epoch 420, val loss: 0.8735910058021545
Epoch 430, training loss: 0.04004630446434021 = 0.03338485583662987 + 0.001 * 6.661450386047363
Epoch 430, val loss: 0.8872211575508118
Epoch 440, training loss: 0.03686000779271126 = 0.030205100774765015 + 0.001 * 6.654908180236816
Epoch 440, val loss: 0.9006904363632202
Epoch 450, training loss: 0.03410705178976059 = 0.027450691908597946 + 0.001 * 6.656360149383545
Epoch 450, val loss: 0.9139063358306885
Epoch 460, training loss: 0.03170335292816162 = 0.025051282718777657 + 0.001 * 6.65207052230835
Epoch 460, val loss: 0.9268635511398315
Epoch 470, training loss: 0.02958780899643898 = 0.022949697449803352 + 0.001 * 6.638110160827637
Epoch 470, val loss: 0.9395110011100769
Epoch 480, training loss: 0.027741724625229836 = 0.021100718528032303 + 0.001 * 6.6410064697265625
Epoch 480, val loss: 0.9518350958824158
Epoch 490, training loss: 0.026107652112841606 = 0.0194670669734478 + 0.001 * 6.640584468841553
Epoch 490, val loss: 0.9638463854789734
Epoch 500, training loss: 0.02465132623910904 = 0.018014978617429733 + 0.001 * 6.636347770690918
Epoch 500, val loss: 0.9756006598472595
Epoch 510, training loss: 0.023348761722445488 = 0.01671602576971054 + 0.001 * 6.632735729217529
Epoch 510, val loss: 0.9870587587356567
Epoch 520, training loss: 0.022189294919371605 = 0.015547843649983406 + 0.001 * 6.641450881958008
Epoch 520, val loss: 0.998347282409668
Epoch 530, training loss: 0.02112058363854885 = 0.014493638649582863 + 0.001 * 6.626944065093994
Epoch 530, val loss: 1.0093632936477661
Epoch 540, training loss: 0.020166445523500443 = 0.013539500534534454 + 0.001 * 6.6269450187683105
Epoch 540, val loss: 1.0201618671417236
Epoch 550, training loss: 0.019299902021884918 = 0.0126736955717206 + 0.001 * 6.6262054443359375
Epoch 550, val loss: 1.0307540893554688
Epoch 560, training loss: 0.018509678542613983 = 0.01188654638826847 + 0.001 * 6.623132228851318
Epoch 560, val loss: 1.0411109924316406
Epoch 570, training loss: 0.01780092716217041 = 0.011169747449457645 + 0.001 * 6.631180286407471
Epoch 570, val loss: 1.051254153251648
Epoch 580, training loss: 0.017140211537480354 = 0.010515592060983181 + 0.001 * 6.6246185302734375
Epoch 580, val loss: 1.0611828565597534
Epoch 590, training loss: 0.016539275646209717 = 0.00991643127053976 + 0.001 * 6.6228437423706055
Epoch 590, val loss: 1.0708746910095215
Epoch 600, training loss: 0.015977628529071808 = 0.009364504367113113 + 0.001 * 6.613123416900635
Epoch 600, val loss: 1.0803905725479126
Epoch 610, training loss: 0.01547942589968443 = 0.008852898143231869 + 0.001 * 6.626527309417725
Epoch 610, val loss: 1.0898160934448242
Epoch 620, training loss: 0.014996429905295372 = 0.008377150632441044 + 0.001 * 6.619278907775879
Epoch 620, val loss: 1.0991599559783936
Epoch 630, training loss: 0.0145442895591259 = 0.007934287190437317 + 0.001 * 6.610002040863037
Epoch 630, val loss: 1.1084460020065308
Epoch 640, training loss: 0.01412949338555336 = 0.0075221252627670765 + 0.001 * 6.607367992401123
Epoch 640, val loss: 1.1176447868347168
Epoch 650, training loss: 0.013749365694820881 = 0.007138734683394432 + 0.001 * 6.610630512237549
Epoch 650, val loss: 1.126727819442749
Epoch 660, training loss: 0.013397522270679474 = 0.0067821466363966465 + 0.001 * 6.615375518798828
Epoch 660, val loss: 1.1357018947601318
Epoch 670, training loss: 0.013057280331850052 = 0.0064505827613174915 + 0.001 * 6.6066975593566895
Epoch 670, val loss: 1.1445086002349854
Epoch 680, training loss: 0.012760689482092857 = 0.006142190657556057 + 0.001 * 6.618497848510742
Epoch 680, val loss: 1.1531904935836792
Epoch 690, training loss: 0.012472055852413177 = 0.0058552236296236515 + 0.001 * 6.616832256317139
Epoch 690, val loss: 1.161696195602417
Epoch 700, training loss: 0.012196713127195835 = 0.005587868858128786 + 0.001 * 6.608843803405762
Epoch 700, val loss: 1.1700608730316162
Epoch 710, training loss: 0.011942162178456783 = 0.005338585935533047 + 0.001 * 6.603575706481934
Epoch 710, val loss: 1.1782898902893066
Epoch 720, training loss: 0.011718165129423141 = 0.005105935037136078 + 0.001 * 6.612229347229004
Epoch 720, val loss: 1.1863585710525513
Epoch 730, training loss: 0.011497913859784603 = 0.004888562485575676 + 0.001 * 6.60935115814209
Epoch 730, val loss: 1.1942590475082397
Epoch 740, training loss: 0.011293359100818634 = 0.004685310646891594 + 0.001 * 6.6080474853515625
Epoch 740, val loss: 1.2020316123962402
Epoch 750, training loss: 0.011089971289038658 = 0.004495133180171251 + 0.001 * 6.594837665557861
Epoch 750, val loss: 1.2096384763717651
Epoch 760, training loss: 0.010923683643341064 = 0.004316841252148151 + 0.001 * 6.606841564178467
Epoch 760, val loss: 1.217105507850647
Epoch 770, training loss: 0.01074990350753069 = 0.004149362910538912 + 0.001 * 6.6005401611328125
Epoch 770, val loss: 1.2244290113449097
Epoch 780, training loss: 0.010604092851281166 = 0.003992047160863876 + 0.001 * 6.612044811248779
Epoch 780, val loss: 1.2316220998764038
Epoch 790, training loss: 0.010430323891341686 = 0.0038441538345068693 + 0.001 * 6.586169719696045
Epoch 790, val loss: 1.2386581897735596
Epoch 800, training loss: 0.010284766554832458 = 0.0037049041129648685 + 0.001 * 6.579861640930176
Epoch 800, val loss: 1.24557626247406
Epoch 810, training loss: 0.01016200240701437 = 0.0035736507270485163 + 0.001 * 6.588351249694824
Epoch 810, val loss: 1.2523454427719116
Epoch 820, training loss: 0.01003403589129448 = 0.0034498563036322594 + 0.001 * 6.584178924560547
Epoch 820, val loss: 1.2589930295944214
Epoch 830, training loss: 0.009925929829478264 = 0.003332956228405237 + 0.001 * 6.592972755432129
Epoch 830, val loss: 1.2655093669891357
Epoch 840, training loss: 0.009800382889807224 = 0.003222458530217409 + 0.001 * 6.5779242515563965
Epoch 840, val loss: 1.2719073295593262
Epoch 850, training loss: 0.009716298431158066 = 0.0031179143115878105 + 0.001 * 6.59838342666626
Epoch 850, val loss: 1.2781901359558105
Epoch 860, training loss: 0.00962768029421568 = 0.0030189151875674725 + 0.001 * 6.6087646484375
Epoch 860, val loss: 1.284340739250183
Epoch 870, training loss: 0.009501039981842041 = 0.0029250942170619965 + 0.001 * 6.575944900512695
Epoch 870, val loss: 1.2903916835784912
Epoch 880, training loss: 0.009408979676663876 = 0.0028360928408801556 + 0.001 * 6.5728864669799805
Epoch 880, val loss: 1.296318769454956
Epoch 890, training loss: 0.0093386210501194 = 0.002751594875007868 + 0.001 * 6.587026119232178
Epoch 890, val loss: 1.3021289110183716
Epoch 900, training loss: 0.009246144443750381 = 0.002671330003067851 + 0.001 * 6.574814319610596
Epoch 900, val loss: 1.3078563213348389
Epoch 910, training loss: 0.009182855486869812 = 0.0025950134731829166 + 0.001 * 6.587841987609863
Epoch 910, val loss: 1.3134592771530151
Epoch 920, training loss: 0.009087968617677689 = 0.0025223526172339916 + 0.001 * 6.565615653991699
Epoch 920, val loss: 1.3189750909805298
Epoch 930, training loss: 0.009026557207107544 = 0.0024531688541173935 + 0.001 * 6.573387622833252
Epoch 930, val loss: 1.3243803977966309
Epoch 940, training loss: 0.008969790302217007 = 0.002387245884165168 + 0.001 * 6.582544326782227
Epoch 940, val loss: 1.3296986818313599
Epoch 950, training loss: 0.008898190222680569 = 0.002324383007362485 + 0.001 * 6.573807239532471
Epoch 950, val loss: 1.334924340248108
Epoch 960, training loss: 0.008834685198962688 = 0.0022643778938800097 + 0.001 * 6.570306777954102
Epoch 960, val loss: 1.3400449752807617
Epoch 970, training loss: 0.00876648724079132 = 0.0022070626728236675 + 0.001 * 6.55942440032959
Epoch 970, val loss: 1.345072627067566
Epoch 980, training loss: 0.008732630871236324 = 0.002152293687686324 + 0.001 * 6.580336570739746
Epoch 980, val loss: 1.3500398397445679
Epoch 990, training loss: 0.008677668869495392 = 0.002099985023960471 + 0.001 * 6.577683448791504
Epoch 990, val loss: 1.3548892736434937
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8155
Flip ASR: 0.7822/225 nodes
The final ASR:0.68020, 0.11307, Accuracy:0.80123, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9474])
updated graph: torch.Size([2, 10534])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9532982110977173 = 1.9449243545532227 + 0.001 * 8.373826026916504
Epoch 0, val loss: 1.9462754726409912
Epoch 10, training loss: 1.9432185888290405 = 1.9348448514938354 + 0.001 * 8.373701095581055
Epoch 10, val loss: 1.9365290403366089
Epoch 20, training loss: 1.9302542209625244 = 1.921880841255188 + 0.001 * 8.373367309570312
Epoch 20, val loss: 1.9234291315078735
Epoch 30, training loss: 1.9114677906036377 = 1.9030951261520386 + 0.001 * 8.372653007507324
Epoch 30, val loss: 1.9041147232055664
Epoch 40, training loss: 1.8834257125854492 = 1.8750548362731934 + 0.001 * 8.370875358581543
Epoch 40, val loss: 1.8756083250045776
Epoch 50, training loss: 1.8450369834899902 = 1.8366715908050537 + 0.001 * 8.365401268005371
Epoch 50, val loss: 1.8388464450836182
Epoch 60, training loss: 1.8049174547195435 = 1.7965763807296753 + 0.001 * 8.341086387634277
Epoch 60, val loss: 1.8055076599121094
Epoch 70, training loss: 1.7686398029327393 = 1.7604459524154663 + 0.001 * 8.193875312805176
Epoch 70, val loss: 1.7773301601409912
Epoch 80, training loss: 1.7196531295776367 = 1.7117865085601807 + 0.001 * 7.866621494293213
Epoch 80, val loss: 1.7355681657791138
Epoch 90, training loss: 1.6521327495574951 = 1.6445482969284058 + 0.001 * 7.584396839141846
Epoch 90, val loss: 1.6780798435211182
Epoch 100, training loss: 1.5653380155563354 = 1.5580451488494873 + 0.001 * 7.29288911819458
Epoch 100, val loss: 1.6058616638183594
Epoch 110, training loss: 1.4704222679138184 = 1.4633548259735107 + 0.001 * 7.067468643188477
Epoch 110, val loss: 1.528507113456726
Epoch 120, training loss: 1.376196265220642 = 1.3691864013671875 + 0.001 * 7.0098114013671875
Epoch 120, val loss: 1.4550361633300781
Epoch 130, training loss: 1.2812234163284302 = 1.2742571830749512 + 0.001 * 6.966207981109619
Epoch 130, val loss: 1.3836179971694946
Epoch 140, training loss: 1.1824437379837036 = 1.1754932403564453 + 0.001 * 6.950496673583984
Epoch 140, val loss: 1.3110461235046387
Epoch 150, training loss: 1.080867052078247 = 1.073926568031311 + 0.001 * 6.9404988288879395
Epoch 150, val loss: 1.2367533445358276
Epoch 160, training loss: 0.9809765815734863 = 0.9740462899208069 + 0.001 * 6.930270671844482
Epoch 160, val loss: 1.165186882019043
Epoch 170, training loss: 0.8871878385543823 = 0.8802688121795654 + 0.001 * 6.918997764587402
Epoch 170, val loss: 1.1003236770629883
Epoch 180, training loss: 0.8016641139984131 = 0.794758677482605 + 0.001 * 6.905458450317383
Epoch 180, val loss: 1.0430688858032227
Epoch 190, training loss: 0.7244794368743896 = 0.7175856232643127 + 0.001 * 6.893841743469238
Epoch 190, val loss: 0.9929008483886719
Epoch 200, training loss: 0.6550400853157043 = 0.6481542587280273 + 0.001 * 6.885820388793945
Epoch 200, val loss: 0.9491561055183411
Epoch 210, training loss: 0.5931520462036133 = 0.5862736105918884 + 0.001 * 6.878406524658203
Epoch 210, val loss: 0.9117237329483032
Epoch 220, training loss: 0.5388928055763245 = 0.5320195555686951 + 0.001 * 6.873246669769287
Epoch 220, val loss: 0.8808707594871521
Epoch 230, training loss: 0.491799920797348 = 0.48493215441703796 + 0.001 * 6.867753505706787
Epoch 230, val loss: 0.856683075428009
Epoch 240, training loss: 0.45048215985298157 = 0.44362038373947144 + 0.001 * 6.861767768859863
Epoch 240, val loss: 0.8381331562995911
Epoch 250, training loss: 0.41309112310409546 = 0.40623360872268677 + 0.001 * 6.8575263023376465
Epoch 250, val loss: 0.8238405585289001
Epoch 260, training loss: 0.3780156672000885 = 0.3711673319339752 + 0.001 * 6.848333358764648
Epoch 260, val loss: 0.8121510148048401
Epoch 270, training loss: 0.3444344997406006 = 0.337594598531723 + 0.001 * 6.839902400970459
Epoch 270, val loss: 0.802558422088623
Epoch 280, training loss: 0.31222960352897644 = 0.3053932189941406 + 0.001 * 6.836396217346191
Epoch 280, val loss: 0.7952877283096313
Epoch 290, training loss: 0.2816433906555176 = 0.2748161852359772 + 0.001 * 6.827190399169922
Epoch 290, val loss: 0.7907200455665588
Epoch 300, training loss: 0.25288236141204834 = 0.24606603384017944 + 0.001 * 6.816330432891846
Epoch 300, val loss: 0.7891234159469604
Epoch 310, training loss: 0.2259937822818756 = 0.21918576955795288 + 0.001 * 6.808017730712891
Epoch 310, val loss: 0.7908031940460205
Epoch 320, training loss: 0.20108972489833832 = 0.1942865401506424 + 0.001 * 6.803187847137451
Epoch 320, val loss: 0.7959416508674622
Epoch 330, training loss: 0.17841152846813202 = 0.17161716520786285 + 0.001 * 6.794364929199219
Epoch 330, val loss: 0.8046467900276184
Epoch 340, training loss: 0.15816304087638855 = 0.15137848258018494 + 0.001 * 6.784552097320557
Epoch 340, val loss: 0.8167594075202942
Epoch 350, training loss: 0.1403544694185257 = 0.13357675075531006 + 0.001 * 6.777713775634766
Epoch 350, val loss: 0.831971287727356
Epoch 360, training loss: 0.12480292469263077 = 0.11802778393030167 + 0.001 * 6.775143623352051
Epoch 360, val loss: 0.8497442007064819
Epoch 370, training loss: 0.111249178647995 = 0.10447484999895096 + 0.001 * 6.774326801300049
Epoch 370, val loss: 0.8695532083511353
Epoch 380, training loss: 0.099427729845047 = 0.0926610454916954 + 0.001 * 6.766686916351318
Epoch 380, val loss: 0.890743613243103
Epoch 390, training loss: 0.08912114799022675 = 0.08235485851764679 + 0.001 * 6.766289710998535
Epoch 390, val loss: 0.9128720164299011
Epoch 400, training loss: 0.0801236629486084 = 0.07335847616195679 + 0.001 * 6.765183925628662
Epoch 400, val loss: 0.9354361891746521
Epoch 410, training loss: 0.07226572930812836 = 0.06550205498933792 + 0.001 * 6.763672828674316
Epoch 410, val loss: 0.9581011533737183
Epoch 420, training loss: 0.0654035434126854 = 0.05863937363028526 + 0.001 * 6.764168739318848
Epoch 420, val loss: 0.980593740940094
Epoch 430, training loss: 0.059401094913482666 = 0.052640318870544434 + 0.001 * 6.760775089263916
Epoch 430, val loss: 1.0027273893356323
Epoch 440, training loss: 0.054152727127075195 = 0.04739172011613846 + 0.001 * 6.76100492477417
Epoch 440, val loss: 1.0244261026382446
Epoch 450, training loss: 0.0495540127158165 = 0.042793892323970795 + 0.001 * 6.760119915008545
Epoch 450, val loss: 1.045760989189148
Epoch 460, training loss: 0.04551919922232628 = 0.03875911608338356 + 0.001 * 6.7600812911987305
Epoch 460, val loss: 1.0665810108184814
Epoch 470, training loss: 0.04197476804256439 = 0.03521207347512245 + 0.001 * 6.762696266174316
Epoch 470, val loss: 1.0869600772857666
Epoch 480, training loss: 0.0388462208211422 = 0.03208809345960617 + 0.001 * 6.758126258850098
Epoch 480, val loss: 1.1068260669708252
Epoch 490, training loss: 0.03608686104416847 = 0.029330268502235413 + 0.001 * 6.756590843200684
Epoch 490, val loss: 1.1261557340621948
Epoch 500, training loss: 0.03364735096693039 = 0.02688993141055107 + 0.001 * 6.757417678833008
Epoch 500, val loss: 1.1449408531188965
Epoch 510, training loss: 0.03148331120610237 = 0.024725211784243584 + 0.001 * 6.75809907913208
Epoch 510, val loss: 1.163178563117981
Epoch 520, training loss: 0.02955569140613079 = 0.02279989793896675 + 0.001 * 6.755792617797852
Epoch 520, val loss: 1.180868148803711
Epoch 530, training loss: 0.027837011963129044 = 0.02108277939260006 + 0.001 * 6.7542314529418945
Epoch 530, val loss: 1.1979402303695679
Epoch 540, training loss: 0.026305006816983223 = 0.01954716071486473 + 0.001 * 6.757845878601074
Epoch 540, val loss: 1.2145172357559204
Epoch 550, training loss: 0.024923047050833702 = 0.01816994696855545 + 0.001 * 6.75309944152832
Epoch 550, val loss: 1.2305551767349243
Epoch 560, training loss: 0.023683220148086548 = 0.016931332647800446 + 0.001 * 6.751888275146484
Epoch 560, val loss: 1.246073603630066
Epoch 570, training loss: 0.02257302775979042 = 0.015814276412129402 + 0.001 * 6.758751392364502
Epoch 570, val loss: 1.26112961769104
Epoch 580, training loss: 0.0215542484074831 = 0.014804043807089329 + 0.001 * 6.750203609466553
Epoch 580, val loss: 1.2757092714309692
Epoch 590, training loss: 0.0206396896392107 = 0.013888058252632618 + 0.001 * 6.751631259918213
Epoch 590, val loss: 1.2898308038711548
Epoch 600, training loss: 0.019804326817393303 = 0.013055473566055298 + 0.001 * 6.748852252960205
Epoch 600, val loss: 1.3035005331039429
Epoch 610, training loss: 0.01904449239373207 = 0.012296736240386963 + 0.001 * 6.74775505065918
Epoch 610, val loss: 1.3167697191238403
Epoch 620, training loss: 0.01834923028945923 = 0.011603621765971184 + 0.001 * 6.745607852935791
Epoch 620, val loss: 1.3296104669570923
Epoch 630, training loss: 0.017720932140946388 = 0.01096898689866066 + 0.001 * 6.7519450187683105
Epoch 630, val loss: 1.3420908451080322
Epoch 640, training loss: 0.017142366617918015 = 0.010386571288108826 + 0.001 * 6.755795001983643
Epoch 640, val loss: 1.3542053699493408
Epoch 650, training loss: 0.016600098460912704 = 0.009850967675447464 + 0.001 * 6.7491302490234375
Epoch 650, val loss: 1.3659740686416626
Epoch 660, training loss: 0.016100840643048286 = 0.00935737881809473 + 0.001 * 6.743462085723877
Epoch 660, val loss: 1.3774089813232422
Epoch 670, training loss: 0.015644967555999756 = 0.008901585824787617 + 0.001 * 6.743380546569824
Epoch 670, val loss: 1.3885267972946167
Epoch 680, training loss: 0.015225308947265148 = 0.008479843847453594 + 0.001 * 6.74546480178833
Epoch 680, val loss: 1.3993330001831055
Epoch 690, training loss: 0.014832324348390102 = 0.008088501170277596 + 0.001 * 6.743823051452637
Epoch 690, val loss: 1.4098676443099976
Epoch 700, training loss: 0.014464013278484344 = 0.007723651826381683 + 0.001 * 6.740360736846924
Epoch 700, val loss: 1.4201490879058838
Epoch 710, training loss: 0.01412620022892952 = 0.007380720227956772 + 0.001 * 6.745479106903076
Epoch 710, val loss: 1.4303001165390015
Epoch 720, training loss: 0.013796787708997726 = 0.0070564886555075645 + 0.001 * 6.740298271179199
Epoch 720, val loss: 1.4403594732284546
Epoch 730, training loss: 0.013485843315720558 = 0.006749341264367104 + 0.001 * 6.736501216888428
Epoch 730, val loss: 1.450352430343628
Epoch 740, training loss: 0.013192107900977135 = 0.006458524614572525 + 0.001 * 6.733582973480225
Epoch 740, val loss: 1.4602595567703247
Epoch 750, training loss: 0.012921792455017567 = 0.006183710414916277 + 0.001 * 6.738081932067871
Epoch 750, val loss: 1.4700769186019897
Epoch 760, training loss: 0.012664757668972015 = 0.005924484226852655 + 0.001 * 6.740272521972656
Epoch 760, val loss: 1.4797865152359009
Epoch 770, training loss: 0.012408602051436901 = 0.005680161062628031 + 0.001 * 6.728440761566162
Epoch 770, val loss: 1.489366888999939
Epoch 780, training loss: 0.012202619574964046 = 0.005450000986456871 + 0.001 * 6.752618312835693
Epoch 780, val loss: 1.4987939596176147
Epoch 790, training loss: 0.011964252218604088 = 0.0052332282066345215 + 0.001 * 6.73102331161499
Epoch 790, val loss: 1.508068561553955
Epoch 800, training loss: 0.011765392497181892 = 0.005029031075537205 + 0.001 * 6.736361503601074
Epoch 800, val loss: 1.5171916484832764
Epoch 810, training loss: 0.011566441506147385 = 0.0048365602269768715 + 0.001 * 6.729880332946777
Epoch 810, val loss: 1.5261664390563965
Epoch 820, training loss: 0.011383741162717342 = 0.004655069205909967 + 0.001 * 6.728671550750732
Epoch 820, val loss: 1.5349730253219604
Epoch 830, training loss: 0.011212265118956566 = 0.004483753349632025 + 0.001 * 6.728510856628418
Epoch 830, val loss: 1.5436331033706665
Epoch 840, training loss: 0.011045966297388077 = 0.00432185223326087 + 0.001 * 6.724113464355469
Epoch 840, val loss: 1.5521360635757446
Epoch 850, training loss: 0.010892482474446297 = 0.004168631974607706 + 0.001 * 6.723849773406982
Epoch 850, val loss: 1.5604921579360962
Epoch 860, training loss: 0.010764526203274727 = 0.0040235514752566814 + 0.001 * 6.740973949432373
Epoch 860, val loss: 1.5687090158462524
Epoch 870, training loss: 0.010611450299620628 = 0.0038861590437591076 + 0.001 * 6.725290775299072
Epoch 870, val loss: 1.576751708984375
Epoch 880, training loss: 0.010475099086761475 = 0.003755865152925253 + 0.001 * 6.71923303604126
Epoch 880, val loss: 1.584617018699646
Epoch 890, training loss: 0.010376130230724812 = 0.0036321599036455154 + 0.001 * 6.743969917297363
Epoch 890, val loss: 1.5923174619674683
Epoch 900, training loss: 0.010230457410216331 = 0.003514649113640189 + 0.001 * 6.715807914733887
Epoch 900, val loss: 1.599889874458313
Epoch 910, training loss: 0.010119441896677017 = 0.0034029099624603987 + 0.001 * 6.716531276702881
Epoch 910, val loss: 1.6073284149169922
Epoch 920, training loss: 0.010016459040343761 = 0.0032966083381325006 + 0.001 * 6.719850063323975
Epoch 920, val loss: 1.6146160364151
Epoch 930, training loss: 0.009913156740367413 = 0.0031954804435372353 + 0.001 * 6.717676162719727
Epoch 930, val loss: 1.6217533349990845
Epoch 940, training loss: 0.009811142459511757 = 0.00309917819686234 + 0.001 * 6.711964130401611
Epoch 940, val loss: 1.6287837028503418
Epoch 950, training loss: 0.009729194454848766 = 0.003007511841133237 + 0.001 * 6.721682548522949
Epoch 950, val loss: 1.6356953382492065
Epoch 960, training loss: 0.009634487330913544 = 0.002920206170529127 + 0.001 * 6.71428108215332
Epoch 960, val loss: 1.642452597618103
Epoch 970, training loss: 0.009551957249641418 = 0.002836924511939287 + 0.001 * 6.71503210067749
Epoch 970, val loss: 1.6491066217422485
Epoch 980, training loss: 0.009459561668336391 = 0.0027574312407523394 + 0.001 * 6.70212984085083
Epoch 980, val loss: 1.6556299924850464
Epoch 990, training loss: 0.009420836344361305 = 0.0026815442834049463 + 0.001 * 6.739292144775391
Epoch 990, val loss: 1.6620101928710938
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6273
Flip ASR: 0.5511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9393385648727417 = 1.930964708328247 + 0.001 * 8.373849868774414
Epoch 0, val loss: 1.9232624769210815
Epoch 10, training loss: 1.9301154613494873 = 1.9217417240142822 + 0.001 * 8.373779296875
Epoch 10, val loss: 1.9141398668289185
Epoch 20, training loss: 1.9193147420883179 = 1.910941243171692 + 0.001 * 8.373483657836914
Epoch 20, val loss: 1.903050184249878
Epoch 30, training loss: 1.9047200679779053 = 1.8963472843170166 + 0.001 * 8.372828483581543
Epoch 30, val loss: 1.88763427734375
Epoch 40, training loss: 1.8836575746536255 = 1.8752862215042114 + 0.001 * 8.371347427368164
Epoch 40, val loss: 1.8651564121246338
Epoch 50, training loss: 1.8533904552459717 = 1.8450230360031128 + 0.001 * 8.367431640625
Epoch 50, val loss: 1.8332188129425049
Epoch 60, training loss: 1.8135172128677368 = 1.805164098739624 + 0.001 * 8.3530855178833
Epoch 60, val loss: 1.7931158542633057
Epoch 70, training loss: 1.7676517963409424 = 1.759385585784912 + 0.001 * 8.266155242919922
Epoch 70, val loss: 1.7505782842636108
Epoch 80, training loss: 1.7139220237731934 = 1.706251621246338 + 0.001 * 7.670388698577881
Epoch 80, val loss: 1.7042443752288818
Epoch 90, training loss: 1.642378807067871 = 1.6350526809692383 + 0.001 * 7.326099872589111
Epoch 90, val loss: 1.6438615322113037
Epoch 100, training loss: 1.5481979846954346 = 1.5409785509109497 + 0.001 * 7.219444751739502
Epoch 100, val loss: 1.5660138130187988
Epoch 110, training loss: 1.4329590797424316 = 1.4257837533950806 + 0.001 * 7.175276756286621
Epoch 110, val loss: 1.4725359678268433
Epoch 120, training loss: 1.3053609132766724 = 1.298219919204712 + 0.001 * 7.1409783363342285
Epoch 120, val loss: 1.3712016344070435
Epoch 130, training loss: 1.1752070188522339 = 1.1680978536605835 + 0.001 * 7.109210014343262
Epoch 130, val loss: 1.2709318399429321
Epoch 140, training loss: 1.049964427947998 = 1.0429028272628784 + 0.001 * 7.061581611633301
Epoch 140, val loss: 1.1763966083526611
Epoch 150, training loss: 0.9353270530700684 = 0.9283222556114197 + 0.001 * 7.004775047302246
Epoch 150, val loss: 1.0924650430679321
Epoch 160, training loss: 0.8345806002616882 = 0.8276171088218689 + 0.001 * 6.96348762512207
Epoch 160, val loss: 1.0198378562927246
Epoch 170, training loss: 0.7480343580245972 = 0.7410953044891357 + 0.001 * 6.939062595367432
Epoch 170, val loss: 0.9584681987762451
Epoch 180, training loss: 0.6735748052597046 = 0.6666619181632996 + 0.001 * 6.912863731384277
Epoch 180, val loss: 0.9070125818252563
Epoch 190, training loss: 0.6084040999412537 = 0.601509153842926 + 0.001 * 6.89497184753418
Epoch 190, val loss: 0.8638850450515747
Epoch 200, training loss: 0.5502519011497498 = 0.5433652400970459 + 0.001 * 6.886671543121338
Epoch 200, val loss: 0.8278322219848633
Epoch 210, training loss: 0.49759066104888916 = 0.49070683121681213 + 0.001 * 6.883829116821289
Epoch 210, val loss: 0.798230767250061
Epoch 220, training loss: 0.44945845007896423 = 0.4425754249095917 + 0.001 * 6.8830108642578125
Epoch 220, val loss: 0.7742634415626526
Epoch 230, training loss: 0.4050961434841156 = 0.3982142210006714 + 0.001 * 6.881922245025635
Epoch 230, val loss: 0.7546097636222839
Epoch 240, training loss: 0.36395740509033203 = 0.3570766746997833 + 0.001 * 6.880716323852539
Epoch 240, val loss: 0.7385422587394714
Epoch 250, training loss: 0.32578396797180176 = 0.3189046084880829 + 0.001 * 6.879345417022705
Epoch 250, val loss: 0.7259138822555542
Epoch 260, training loss: 0.29057103395462036 = 0.2836933732032776 + 0.001 * 6.877662181854248
Epoch 260, val loss: 0.7165474891662598
Epoch 270, training loss: 0.25843125581741333 = 0.25155478715896606 + 0.001 * 6.876466274261475
Epoch 270, val loss: 0.7104589343070984
Epoch 280, training loss: 0.22950217127799988 = 0.22262851893901825 + 0.001 * 6.873652458190918
Epoch 280, val loss: 0.7077110409736633
Epoch 290, training loss: 0.2037200629711151 = 0.19684891402721405 + 0.001 * 6.871142387390137
Epoch 290, val loss: 0.7082428336143494
Epoch 300, training loss: 0.18091052770614624 = 0.17404302954673767 + 0.001 * 6.867499351501465
Epoch 300, val loss: 0.7119085788726807
Epoch 310, training loss: 0.1608506143093109 = 0.15398792922496796 + 0.001 * 6.862689018249512
Epoch 310, val loss: 0.7183011770248413
Epoch 320, training loss: 0.14333157241344452 = 0.13647344708442688 + 0.001 * 6.858120918273926
Epoch 320, val loss: 0.7270547151565552
Epoch 330, training loss: 0.12816429138183594 = 0.12131401151418686 + 0.001 * 6.850279808044434
Epoch 330, val loss: 0.7375927567481995
Epoch 340, training loss: 0.11502182483673096 = 0.10818085819482803 + 0.001 * 6.840968608856201
Epoch 340, val loss: 0.7497482895851135
Epoch 350, training loss: 0.10358387976884842 = 0.09673621505498886 + 0.001 * 6.847661018371582
Epoch 350, val loss: 0.7631666660308838
Epoch 360, training loss: 0.0935487449169159 = 0.08671852201223373 + 0.001 * 6.830224990844727
Epoch 360, val loss: 0.7773820757865906
Epoch 370, training loss: 0.08473710715770721 = 0.07791773974895477 + 0.001 * 6.819363594055176
Epoch 370, val loss: 0.7921987175941467
Epoch 380, training loss: 0.0769721195101738 = 0.07016436755657196 + 0.001 * 6.8077521324157715
Epoch 380, val loss: 0.8072516918182373
Epoch 390, training loss: 0.07013282924890518 = 0.06332007795572281 + 0.001 * 6.812751293182373
Epoch 390, val loss: 0.8224142789840698
Epoch 400, training loss: 0.06406905502080917 = 0.05726602301001549 + 0.001 * 6.803034782409668
Epoch 400, val loss: 0.8374284505844116
Epoch 410, training loss: 0.058693014085292816 = 0.05190272629261017 + 0.001 * 6.790285587310791
Epoch 410, val loss: 0.8521499037742615
Epoch 420, training loss: 0.053926732391119 = 0.047144077718257904 + 0.001 * 6.782654285430908
Epoch 420, val loss: 0.8665510416030884
Epoch 430, training loss: 0.04971114173531532 = 0.042916443198919296 + 0.001 * 6.794698715209961
Epoch 430, val loss: 0.8805365562438965
Epoch 440, training loss: 0.04592551663517952 = 0.03915274515748024 + 0.001 * 6.77277135848999
Epoch 440, val loss: 0.8941401243209839
Epoch 450, training loss: 0.042571354657411575 = 0.03579746559262276 + 0.001 * 6.773889064788818
Epoch 450, val loss: 0.9072698354721069
Epoch 460, training loss: 0.03956703469157219 = 0.03279901668429375 + 0.001 * 6.7680182456970215
Epoch 460, val loss: 0.9200242757797241
Epoch 470, training loss: 0.036879636347293854 = 0.030108381062746048 + 0.001 * 6.771255970001221
Epoch 470, val loss: 0.932372510433197
Epoch 480, training loss: 0.034450821578502655 = 0.027684561908245087 + 0.001 * 6.7662577629089355
Epoch 480, val loss: 0.9444229006767273
Epoch 490, training loss: 0.03226153552532196 = 0.02549680508673191 + 0.001 * 6.76472806930542
Epoch 490, val loss: 0.9561341404914856
Epoch 500, training loss: 0.030286170542240143 = 0.02351868897676468 + 0.001 * 6.767481327056885
Epoch 500, val loss: 0.9675526022911072
Epoch 510, training loss: 0.0284852497279644 = 0.02172522246837616 + 0.001 * 6.760025978088379
Epoch 510, val loss: 0.9786962270736694
Epoch 520, training loss: 0.026859864592552185 = 0.020096823573112488 + 0.001 * 6.763040065765381
Epoch 520, val loss: 0.9895702004432678
Epoch 530, training loss: 0.025378484278917313 = 0.018616987392306328 + 0.001 * 6.7614970207214355
Epoch 530, val loss: 1.0001899003982544
Epoch 540, training loss: 0.02403288520872593 = 0.017271623015403748 + 0.001 * 6.761261463165283
Epoch 540, val loss: 1.0105031728744507
Epoch 550, training loss: 0.022802237421274185 = 0.01604672335088253 + 0.001 * 6.755514144897461
Epoch 550, val loss: 1.0204740762710571
Epoch 560, training loss: 0.021688934415578842 = 0.014931046403944492 + 0.001 * 6.75788688659668
Epoch 560, val loss: 1.0302654504776
Epoch 570, training loss: 0.020671162754297256 = 0.013915639370679855 + 0.001 * 6.755523681640625
Epoch 570, val loss: 1.0398720502853394
Epoch 580, training loss: 0.019741224125027657 = 0.012990845367312431 + 0.001 * 6.750378608703613
Epoch 580, val loss: 1.0491676330566406
Epoch 590, training loss: 0.0188964381814003 = 0.012148212641477585 + 0.001 * 6.748226165771484
Epoch 590, val loss: 1.0581127405166626
Epoch 600, training loss: 0.018139434978365898 = 0.011379116214811802 + 0.001 * 6.760318756103516
Epoch 600, val loss: 1.0669225454330444
Epoch 610, training loss: 0.017424780875444412 = 0.010676239617168903 + 0.001 * 6.748539924621582
Epoch 610, val loss: 1.0754151344299316
Epoch 620, training loss: 0.01677641086280346 = 0.010033000260591507 + 0.001 * 6.743410587310791
Epoch 620, val loss: 1.0837212800979614
Epoch 630, training loss: 0.016195591539144516 = 0.00944352988153696 + 0.001 * 6.7520623207092285
Epoch 630, val loss: 1.0917587280273438
Epoch 640, training loss: 0.015644531697034836 = 0.00890257302671671 + 0.001 * 6.7419586181640625
Epoch 640, val loss: 1.0996220111846924
Epoch 650, training loss: 0.015139630064368248 = 0.008405451662838459 + 0.001 * 6.734178066253662
Epoch 650, val loss: 1.1072179079055786
Epoch 660, training loss: 0.014685242436826229 = 0.007947900332510471 + 0.001 * 6.73734188079834
Epoch 660, val loss: 1.1146334409713745
Epoch 670, training loss: 0.014266349375247955 = 0.007526039611548185 + 0.001 * 6.740309715270996
Epoch 670, val loss: 1.121856927871704
Epoch 680, training loss: 0.01386512815952301 = 0.007136088330298662 + 0.001 * 6.729039192199707
Epoch 680, val loss: 1.1288747787475586
Epoch 690, training loss: 0.013503724709153175 = 0.006773808039724827 + 0.001 * 6.729916095733643
Epoch 690, val loss: 1.1356947422027588
Epoch 700, training loss: 0.013179518282413483 = 0.006435331888496876 + 0.001 * 6.7441864013671875
Epoch 700, val loss: 1.142404556274414
Epoch 710, training loss: 0.01284160278737545 = 0.006117928307503462 + 0.001 * 6.723674774169922
Epoch 710, val loss: 1.1489477157592773
Epoch 720, training loss: 0.012574744410812855 = 0.005820207763463259 + 0.001 * 6.754536151885986
Epoch 720, val loss: 1.1554542779922485
Epoch 730, training loss: 0.0122763030230999 = 0.005541543941944838 + 0.001 * 6.7347588539123535
Epoch 730, val loss: 1.1618177890777588
Epoch 740, training loss: 0.011999761685729027 = 0.005280382931232452 + 0.001 * 6.7193779945373535
Epoch 740, val loss: 1.1680411100387573
Epoch 750, training loss: 0.011781474575400352 = 0.00503567373380065 + 0.001 * 6.745800018310547
Epoch 750, val loss: 1.1741621494293213
Epoch 760, training loss: 0.011527271941304207 = 0.004806882701814175 + 0.001 * 6.720389366149902
Epoch 760, val loss: 1.18012535572052
Epoch 770, training loss: 0.011303969658911228 = 0.004592863377183676 + 0.001 * 6.711105823516846
Epoch 770, val loss: 1.1860040426254272
Epoch 780, training loss: 0.01111122127622366 = 0.004392653238028288 + 0.001 * 6.718567848205566
Epoch 780, val loss: 1.1917411088943481
Epoch 790, training loss: 0.010947644710540771 = 0.004205275326967239 + 0.001 * 6.742369174957275
Epoch 790, val loss: 1.1973499059677124
Epoch 800, training loss: 0.010743559338152409 = 0.004030019044876099 + 0.001 * 6.713540077209473
Epoch 800, val loss: 1.202846646308899
Epoch 810, training loss: 0.010574620217084885 = 0.00386588042601943 + 0.001 * 6.708739280700684
Epoch 810, val loss: 1.2082241773605347
Epoch 820, training loss: 0.010422804392874241 = 0.0037119179032742977 + 0.001 * 6.710886001586914
Epoch 820, val loss: 1.213521957397461
Epoch 830, training loss: 0.010287011042237282 = 0.003567497478798032 + 0.001 * 6.719513416290283
Epoch 830, val loss: 1.2187120914459229
Epoch 840, training loss: 0.010148469358682632 = 0.0034318994730710983 + 0.001 * 6.716569423675537
Epoch 840, val loss: 1.2237411737442017
Epoch 850, training loss: 0.010019234381616116 = 0.0033043234143406153 + 0.001 * 6.714910507202148
Epoch 850, val loss: 1.2287123203277588
Epoch 860, training loss: 0.00988964643329382 = 0.003184175118803978 + 0.001 * 6.705471038818359
Epoch 860, val loss: 1.2336145639419556
Epoch 870, training loss: 0.009774683974683285 = 0.0030707779806107283 + 0.001 * 6.703906059265137
Epoch 870, val loss: 1.2383530139923096
Epoch 880, training loss: 0.009660663083195686 = 0.002963634440675378 + 0.001 * 6.697028160095215
Epoch 880, val loss: 1.2431511878967285
Epoch 890, training loss: 0.009556146338582039 = 0.0028621507808566093 + 0.001 * 6.693994998931885
Epoch 890, val loss: 1.2478402853012085
Epoch 900, training loss: 0.009449790231883526 = 0.00276584317907691 + 0.001 * 6.68394660949707
Epoch 900, val loss: 1.2525368928909302
Epoch 910, training loss: 0.009372166357934475 = 0.0026743600610643625 + 0.001 * 6.697806358337402
Epoch 910, val loss: 1.2571511268615723
Epoch 920, training loss: 0.009274162352085114 = 0.002587456488981843 + 0.001 * 6.686705589294434
Epoch 920, val loss: 1.2617499828338623
Epoch 930, training loss: 0.009222391992807388 = 0.0025048451498150826 + 0.001 * 6.7175469398498535
Epoch 930, val loss: 1.2662880420684814
Epoch 940, training loss: 0.009127934463322163 = 0.002426157472655177 + 0.001 * 6.701776504516602
Epoch 940, val loss: 1.2708215713500977
Epoch 950, training loss: 0.009032650850713253 = 0.0023512353654950857 + 0.001 * 6.68141508102417
Epoch 950, val loss: 1.2752947807312012
Epoch 960, training loss: 0.008957461453974247 = 0.0022797787096351385 + 0.001 * 6.677682876586914
Epoch 960, val loss: 1.2797942161560059
Epoch 970, training loss: 0.00889238528907299 = 0.002211747458204627 + 0.001 * 6.680637836456299
Epoch 970, val loss: 1.284196138381958
Epoch 980, training loss: 0.008821846917271614 = 0.0021469243802130222 + 0.001 * 6.674921989440918
Epoch 980, val loss: 1.2885679006576538
Epoch 990, training loss: 0.00875629298388958 = 0.0020851129665970802 + 0.001 * 6.67117977142334
Epoch 990, val loss: 1.2928615808486938
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8672
Flip ASR: 0.8400/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9559842348098755 = 1.9476103782653809 + 0.001 * 8.37385368347168
Epoch 0, val loss: 1.9413588047027588
Epoch 10, training loss: 1.9456965923309326 = 1.937322735786438 + 0.001 * 8.37380599975586
Epoch 10, val loss: 1.931890845298767
Epoch 20, training loss: 1.9332176446914673 = 1.9248440265655518 + 0.001 * 8.373579978942871
Epoch 20, val loss: 1.9200961589813232
Epoch 30, training loss: 1.9157819747924805 = 1.9074089527130127 + 0.001 * 8.37306022644043
Epoch 30, val loss: 1.9032647609710693
Epoch 40, training loss: 1.8901751041412354 = 1.881803274154663 + 0.001 * 8.371867179870605
Epoch 40, val loss: 1.8785954713821411
Epoch 50, training loss: 1.8542934656143188 = 1.845924735069275 + 0.001 * 8.36874771118164
Epoch 50, val loss: 1.845444679260254
Epoch 60, training loss: 1.812158226966858 = 1.8038002252578735 + 0.001 * 8.35803508758545
Epoch 60, val loss: 1.8101190328598022
Epoch 70, training loss: 1.7727937698364258 = 1.7644842863082886 + 0.001 * 8.30949878692627
Epoch 70, val loss: 1.7793329954147339
Epoch 80, training loss: 1.7253327369689941 = 1.7172908782958984 + 0.001 * 8.041873931884766
Epoch 80, val loss: 1.736890196800232
Epoch 90, training loss: 1.6588590145111084 = 1.651073694229126 + 0.001 * 7.785342693328857
Epoch 90, val loss: 1.676854133605957
Epoch 100, training loss: 1.5720124244689941 = 1.56432044506073 + 0.001 * 7.691991806030273
Epoch 100, val loss: 1.6016443967819214
Epoch 110, training loss: 1.4732320308685303 = 1.4656445980072021 + 0.001 * 7.587433338165283
Epoch 110, val loss: 1.5196439027786255
Epoch 120, training loss: 1.373177170753479 = 1.3658061027526855 + 0.001 * 7.371102333068848
Epoch 120, val loss: 1.4393452405929565
Epoch 130, training loss: 1.2745462656021118 = 1.267367959022522 + 0.001 * 7.178340911865234
Epoch 130, val loss: 1.3626176118850708
Epoch 140, training loss: 1.1769001483917236 = 1.1698172092437744 + 0.001 * 7.082913875579834
Epoch 140, val loss: 1.2892978191375732
Epoch 150, training loss: 1.080889105796814 = 1.073870301246643 + 0.001 * 7.018847942352295
Epoch 150, val loss: 1.2188332080841064
Epoch 160, training loss: 0.987507700920105 = 0.9805310368537903 + 0.001 * 6.976659297943115
Epoch 160, val loss: 1.1516090631484985
Epoch 170, training loss: 0.8974452614784241 = 0.8905037641525269 + 0.001 * 6.941492557525635
Epoch 170, val loss: 1.0880072116851807
Epoch 180, training loss: 0.8110618591308594 = 0.8041421175003052 + 0.001 * 6.919739723205566
Epoch 180, val loss: 1.0277091264724731
Epoch 190, training loss: 0.7286514043807983 = 0.7217404842376709 + 0.001 * 6.910907745361328
Epoch 190, val loss: 0.9708519577980042
Epoch 200, training loss: 0.6507740616798401 = 0.6438654065132141 + 0.001 * 6.908674716949463
Epoch 200, val loss: 0.9180281162261963
Epoch 210, training loss: 0.5784115195274353 = 0.5715042352676392 + 0.001 * 6.907268047332764
Epoch 210, val loss: 0.870631992816925
Epoch 220, training loss: 0.5127898454666138 = 0.5058834552764893 + 0.001 * 6.906410217285156
Epoch 220, val loss: 0.8299908638000488
Epoch 230, training loss: 0.4545952081680298 = 0.4476892054080963 + 0.001 * 6.906000137329102
Epoch 230, val loss: 0.7965609431266785
Epoch 240, training loss: 0.4037206470966339 = 0.3968150317668915 + 0.001 * 6.905627727508545
Epoch 240, val loss: 0.7699853181838989
Epoch 250, training loss: 0.3592897057533264 = 0.3523845672607422 + 0.001 * 6.905139923095703
Epoch 250, val loss: 0.7493359446525574
Epoch 260, training loss: 0.3202386796474457 = 0.31333428621292114 + 0.001 * 6.904388904571533
Epoch 260, val loss: 0.7334891557693481
Epoch 270, training loss: 0.28558382391929626 = 0.2786804139614105 + 0.001 * 6.90341854095459
Epoch 270, val loss: 0.7217370271682739
Epoch 280, training loss: 0.2545070946216583 = 0.24760481715202332 + 0.001 * 6.902285575866699
Epoch 280, val loss: 0.7132085561752319
Epoch 290, training loss: 0.2264653891324997 = 0.21956448256969452 + 0.001 * 6.900906085968018
Epoch 290, val loss: 0.7074152827262878
Epoch 300, training loss: 0.20109520852565765 = 0.19419574737548828 + 0.001 * 6.899457931518555
Epoch 300, val loss: 0.7038668394088745
Epoch 310, training loss: 0.17817814648151398 = 0.17128002643585205 + 0.001 * 6.898116111755371
Epoch 310, val loss: 0.7023258209228516
Epoch 320, training loss: 0.15757326781749725 = 0.150676429271698 + 0.001 * 6.8968329429626465
Epoch 320, val loss: 0.7026425004005432
Epoch 330, training loss: 0.1391584724187851 = 0.1322629600763321 + 0.001 * 6.895518779754639
Epoch 330, val loss: 0.7048181891441345
Epoch 340, training loss: 0.12283062189817429 = 0.11593561619520187 + 0.001 * 6.89500617980957
Epoch 340, val loss: 0.7088188529014587
Epoch 350, training loss: 0.10846512764692307 = 0.10157186537981033 + 0.001 * 6.893261909484863
Epoch 350, val loss: 0.7146179676055908
Epoch 360, training loss: 0.0959293395280838 = 0.08903709053993225 + 0.001 * 6.892250061035156
Epoch 360, val loss: 0.7221020460128784
Epoch 370, training loss: 0.08506768941879272 = 0.07817690074443817 + 0.001 * 6.890789031982422
Epoch 370, val loss: 0.7310476303100586
Epoch 380, training loss: 0.07570885121822357 = 0.06881745904684067 + 0.001 * 6.891391754150391
Epoch 380, val loss: 0.741166353225708
Epoch 390, training loss: 0.06767113506793976 = 0.060781870037317276 + 0.001 * 6.8892669677734375
Epoch 390, val loss: 0.752149224281311
Epoch 400, training loss: 0.06078112870454788 = 0.053893666714429855 + 0.001 * 6.887462139129639
Epoch 400, val loss: 0.763639509677887
Epoch 410, training loss: 0.05487767234444618 = 0.04799213632941246 + 0.001 * 6.885536193847656
Epoch 410, val loss: 0.7753309607505798
Epoch 420, training loss: 0.04981713742017746 = 0.04293191805481911 + 0.001 * 6.885218620300293
Epoch 420, val loss: 0.787042498588562
Epoch 430, training loss: 0.04546266794204712 = 0.03858049213886261 + 0.001 * 6.882176876068115
Epoch 430, val loss: 0.7986766695976257
Epoch 440, training loss: 0.041703950613737106 = 0.034823622554540634 + 0.001 * 6.880326271057129
Epoch 440, val loss: 0.8100507259368896
Epoch 450, training loss: 0.038447197526693344 = 0.03156695514917374 + 0.001 * 6.8802409172058105
Epoch 450, val loss: 0.8211485743522644
Epoch 460, training loss: 0.03560784459114075 = 0.028731755912303925 + 0.001 * 6.876089096069336
Epoch 460, val loss: 0.8319821357727051
Epoch 470, training loss: 0.03312491253018379 = 0.02625095285475254 + 0.001 * 6.873958587646484
Epoch 470, val loss: 0.8425391316413879
Epoch 480, training loss: 0.030946390703320503 = 0.02407052554190159 + 0.001 * 6.8758649826049805
Epoch 480, val loss: 0.8528121113777161
Epoch 490, training loss: 0.0290159210562706 = 0.022145789116621017 + 0.001 * 6.87013053894043
Epoch 490, val loss: 0.8627938032150269
Epoch 500, training loss: 0.027307026088237762 = 0.02044001594185829 + 0.001 * 6.86700963973999
Epoch 500, val loss: 0.872498095035553
Epoch 510, training loss: 0.025784319266676903 = 0.018922233954072 + 0.001 * 6.862084865570068
Epoch 510, val loss: 0.8819296360015869
Epoch 520, training loss: 0.024445252493023872 = 0.017566891387104988 + 0.001 * 6.878360748291016
Epoch 520, val loss: 0.8910931348800659
Epoch 530, training loss: 0.023209545761346817 = 0.016352489590644836 + 0.001 * 6.857056617736816
Epoch 530, val loss: 0.9000144004821777
Epoch 540, training loss: 0.02212372235953808 = 0.015260327607393265 + 0.001 * 6.863393783569336
Epoch 540, val loss: 0.9087193012237549
Epoch 550, training loss: 0.021125946193933487 = 0.01427487563341856 + 0.001 * 6.851069450378418
Epoch 550, val loss: 0.9171832799911499
Epoch 560, training loss: 0.02022409252822399 = 0.013382915407419205 + 0.001 * 6.841176986694336
Epoch 560, val loss: 0.9254193305969238
Epoch 570, training loss: 0.01942249946296215 = 0.012573199346661568 + 0.001 * 6.849299907684326
Epoch 570, val loss: 0.9334478378295898
Epoch 580, training loss: 0.01867409236729145 = 0.011836204677820206 + 0.001 * 6.837886810302734
Epoch 580, val loss: 0.9412757158279419
Epoch 590, training loss: 0.017999062314629555 = 0.011163664050400257 + 0.001 * 6.835398197174072
Epoch 590, val loss: 0.9488722085952759
Epoch 600, training loss: 0.017374292016029358 = 0.010548430494964123 + 0.001 * 6.825860500335693
Epoch 600, val loss: 0.9562808275222778
Epoch 610, training loss: 0.016794847324490547 = 0.009984301403164864 + 0.001 * 6.810544967651367
Epoch 610, val loss: 0.9635046124458313
Epoch 620, training loss: 0.01642916351556778 = 0.009465847164392471 + 0.001 * 6.963315963745117
Epoch 620, val loss: 0.970552921295166
Epoch 630, training loss: 0.01581401191651821 = 0.008988281711935997 + 0.001 * 6.8257293701171875
Epoch 630, val loss: 0.9773920178413391
Epoch 640, training loss: 0.015370363369584084 = 0.008547530509531498 + 0.001 * 6.822833061218262
Epoch 640, val loss: 0.9840569496154785
Epoch 650, training loss: 0.014938538894057274 = 0.00813994463533163 + 0.001 * 6.7985944747924805
Epoch 650, val loss: 0.9905703067779541
Epoch 660, training loss: 0.014563236385583878 = 0.007762324530631304 + 0.001 * 6.800911903381348
Epoch 660, val loss: 0.9969169497489929
Epoch 670, training loss: 0.014204472303390503 = 0.007411693222820759 + 0.001 * 6.792778968811035
Epoch 670, val loss: 1.0031179189682007
Epoch 680, training loss: 0.013871904462575912 = 0.007085305172950029 + 0.001 * 6.786599159240723
Epoch 680, val loss: 1.0092065334320068
Epoch 690, training loss: 0.013590148650109768 = 0.006780318915843964 + 0.001 * 6.809829235076904
Epoch 690, val loss: 1.0151458978652954
Epoch 700, training loss: 0.013275339268147945 = 0.006494174245744944 + 0.001 * 6.781164646148682
Epoch 700, val loss: 1.020995020866394
Epoch 710, training loss: 0.013002652674913406 = 0.00622417451813817 + 0.001 * 6.778477668762207
Epoch 710, val loss: 1.0268069505691528
Epoch 720, training loss: 0.01279774121940136 = 0.005968363489955664 + 0.001 * 6.8293776512146
Epoch 720, val loss: 1.032582402229309
Epoch 730, training loss: 0.012505597434937954 = 0.005725862458348274 + 0.001 * 6.7797346115112305
Epoch 730, val loss: 1.0383485555648804
Epoch 740, training loss: 0.012271879240870476 = 0.005495873745530844 + 0.001 * 6.776005268096924
Epoch 740, val loss: 1.0440850257873535
Epoch 750, training loss: 0.01206945814192295 = 0.005277857184410095 + 0.001 * 6.791601181030273
Epoch 750, val loss: 1.0497993230819702
Epoch 760, training loss: 0.01184903271496296 = 0.005071524530649185 + 0.001 * 6.77750825881958
Epoch 760, val loss: 1.0554431676864624
Epoch 770, training loss: 0.011653286404907703 = 0.004876522347331047 + 0.001 * 6.776763916015625
Epoch 770, val loss: 1.0610244274139404
Epoch 780, training loss: 0.011465400457382202 = 0.004692206624895334 + 0.001 * 6.773192882537842
Epoch 780, val loss: 1.0665967464447021
Epoch 790, training loss: 0.011307436972856522 = 0.004518039058893919 + 0.001 * 6.789397716522217
Epoch 790, val loss: 1.0720999240875244
Epoch 800, training loss: 0.011124055832624435 = 0.004353531636297703 + 0.001 * 6.770524501800537
Epoch 800, val loss: 1.0775156021118164
Epoch 810, training loss: 0.010968106798827648 = 0.004198112990707159 + 0.001 * 6.769993305206299
Epoch 810, val loss: 1.082833170890808
Epoch 820, training loss: 0.010828368365764618 = 0.004051188472658396 + 0.001 * 6.777180194854736
Epoch 820, val loss: 1.0880740880966187
Epoch 830, training loss: 0.010688262060284615 = 0.003912287298589945 + 0.001 * 6.775973796844482
Epoch 830, val loss: 1.0932207107543945
Epoch 840, training loss: 0.010548517107963562 = 0.0037808986380696297 + 0.001 * 6.767617702484131
Epoch 840, val loss: 1.0982849597930908
Epoch 850, training loss: 0.010419703088700771 = 0.003656529588624835 + 0.001 * 6.7631731033325195
Epoch 850, val loss: 1.1032575368881226
Epoch 860, training loss: 0.010334832593798637 = 0.0035387107636779547 + 0.001 * 6.796121120452881
Epoch 860, val loss: 1.108160376548767
Epoch 870, training loss: 0.01018228568136692 = 0.003427073359489441 + 0.001 * 6.75521183013916
Epoch 870, val loss: 1.1129769086837769
Epoch 880, training loss: 0.010080965235829353 = 0.003321160562336445 + 0.001 * 6.7598042488098145
Epoch 880, val loss: 1.117714524269104
Epoch 890, training loss: 0.009983018040657043 = 0.0032206398900598288 + 0.001 * 6.762377738952637
Epoch 890, val loss: 1.1223753690719604
Epoch 900, training loss: 0.009878646582365036 = 0.003125167451798916 + 0.001 * 6.753479480743408
Epoch 900, val loss: 1.1269546747207642
Epoch 910, training loss: 0.009796078316867352 = 0.0030344072729349136 + 0.001 * 6.7616705894470215
Epoch 910, val loss: 1.131449580192566
Epoch 920, training loss: 0.009695379994809628 = 0.0029481237288564444 + 0.001 * 6.747256278991699
Epoch 920, val loss: 1.1358730792999268
Epoch 930, training loss: 0.009632788598537445 = 0.002865993417799473 + 0.001 * 6.766794681549072
Epoch 930, val loss: 1.1402305364608765
Epoch 940, training loss: 0.009535370394587517 = 0.0027877935208380222 + 0.001 * 6.747576713562012
Epoch 940, val loss: 1.1444857120513916
Epoch 950, training loss: 0.009463783353567123 = 0.0027132576797157526 + 0.001 * 6.75052547454834
Epoch 950, val loss: 1.1486902236938477
Epoch 960, training loss: 0.00939871184527874 = 0.0026421942748129368 + 0.001 * 6.7565178871154785
Epoch 960, val loss: 1.1528067588806152
Epoch 970, training loss: 0.00932325329631567 = 0.0025743816513568163 + 0.001 * 6.748871326446533
Epoch 970, val loss: 1.1568686962127686
Epoch 980, training loss: 0.00925868283957243 = 0.00250963494181633 + 0.001 * 6.749047756195068
Epoch 980, val loss: 1.1608375310897827
Epoch 990, training loss: 0.00919559970498085 = 0.0024477920960634947 + 0.001 * 6.747807502746582
Epoch 990, val loss: 1.1647652387619019
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9114
Flip ASR: 0.8933/225 nodes
The final ASR:0.80197, 0.12482, Accuracy:0.82346, 0.01259
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9412])
updated graph: torch.Size([2, 10484])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00460, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9653737545013428 = 1.9569998979568481 + 0.001 * 8.373905181884766
Epoch 0, val loss: 1.950529932975769
Epoch 10, training loss: 1.9548017978668213 = 1.9464279413223267 + 0.001 * 8.373872756958008
Epoch 10, val loss: 1.9407219886779785
Epoch 20, training loss: 1.9429161548614502 = 1.9345424175262451 + 0.001 * 8.373703956604004
Epoch 20, val loss: 1.9293005466461182
Epoch 30, training loss: 1.9272595643997192 = 1.9188863039016724 + 0.001 * 8.373287200927734
Epoch 30, val loss: 1.9138963222503662
Epoch 40, training loss: 1.9048479795455933 = 1.8964756727218628 + 0.001 * 8.372270584106445
Epoch 40, val loss: 1.8917386531829834
Epoch 50, training loss: 1.8726518154144287 = 1.8642823696136475 + 0.001 * 8.369491577148438
Epoch 50, val loss: 1.860566258430481
Epoch 60, training loss: 1.8312244415283203 = 1.8228645324707031 + 0.001 * 8.35986042022705
Epoch 60, val loss: 1.8232758045196533
Epoch 70, training loss: 1.788763165473938 = 1.7804468870162964 + 0.001 * 8.316225051879883
Epoch 70, val loss: 1.7895954847335815
Epoch 80, training loss: 1.744952917098999 = 1.736930012702942 + 0.001 * 8.0228853225708
Epoch 80, val loss: 1.7551989555358887
Epoch 90, training loss: 1.6847271919250488 = 1.6769996881484985 + 0.001 * 7.727560997009277
Epoch 90, val loss: 1.703590750694275
Epoch 100, training loss: 1.6032179594039917 = 1.5955615043640137 + 0.001 * 7.656428337097168
Epoch 100, val loss: 1.633809208869934
Epoch 110, training loss: 1.5041733980178833 = 1.4965786933898926 + 0.001 * 7.594654560089111
Epoch 110, val loss: 1.5534679889678955
Epoch 120, training loss: 1.4003015756607056 = 1.392858624458313 + 0.001 * 7.4430084228515625
Epoch 120, val loss: 1.4722936153411865
Epoch 130, training loss: 1.3010410070419312 = 1.2939136028289795 + 0.001 * 7.127407073974609
Epoch 130, val loss: 1.3989486694335938
Epoch 140, training loss: 1.2075779438018799 = 1.2005164623260498 + 0.001 * 7.0615386962890625
Epoch 140, val loss: 1.3325436115264893
Epoch 150, training loss: 1.119163155555725 = 1.1121635437011719 + 0.001 * 6.999591827392578
Epoch 150, val loss: 1.270939826965332
Epoch 160, training loss: 1.0353577136993408 = 1.028365969657898 + 0.001 * 6.991715908050537
Epoch 160, val loss: 1.2121533155441284
Epoch 170, training loss: 0.9554304480552673 = 0.9484444260597229 + 0.001 * 6.986027240753174
Epoch 170, val loss: 1.1551027297973633
Epoch 180, training loss: 0.8782446980476379 = 0.8712687492370605 + 0.001 * 6.975948810577393
Epoch 180, val loss: 1.098411202430725
Epoch 190, training loss: 0.8027978539466858 = 0.7958341240882874 + 0.001 * 6.963715076446533
Epoch 190, val loss: 1.0415754318237305
Epoch 200, training loss: 0.7291742563247681 = 0.7222276926040649 + 0.001 * 6.9465813636779785
Epoch 200, val loss: 0.98538738489151
Epoch 210, training loss: 0.6586028933525085 = 0.6516742706298828 + 0.001 * 6.928619384765625
Epoch 210, val loss: 0.9323011040687561
Epoch 220, training loss: 0.5921187400817871 = 0.5852038264274597 + 0.001 * 6.914919853210449
Epoch 220, val loss: 0.8845314383506775
Epoch 230, training loss: 0.5296361446380615 = 0.5227281451225281 + 0.001 * 6.9079813957214355
Epoch 230, val loss: 0.8427793383598328
Epoch 240, training loss: 0.47040417790412903 = 0.4634990394115448 + 0.001 * 6.905137538909912
Epoch 240, val loss: 0.8067136406898499
Epoch 250, training loss: 0.4139949381351471 = 0.40709176659584045 + 0.001 * 6.903158187866211
Epoch 250, val loss: 0.7757402658462524
Epoch 260, training loss: 0.3605096638202667 = 0.35360845923423767 + 0.001 * 6.901208877563477
Epoch 260, val loss: 0.7500706315040588
Epoch 270, training loss: 0.3105929493904114 = 0.3036934435367584 + 0.001 * 6.899499893188477
Epoch 270, val loss: 0.7299724221229553
Epoch 280, training loss: 0.2654836177825928 = 0.25858527421951294 + 0.001 * 6.898345470428467
Epoch 280, val loss: 0.7164812684059143
Epoch 290, training loss: 0.226140096783638 = 0.21924224495887756 + 0.001 * 6.897854328155518
Epoch 290, val loss: 0.7096390128135681
Epoch 300, training loss: 0.1929233968257904 = 0.1860261857509613 + 0.001 * 6.897205352783203
Epoch 300, val loss: 0.7091057300567627
Epoch 310, training loss: 0.1654805988073349 = 0.1585836261510849 + 0.001 * 6.896969318389893
Epoch 310, val loss: 0.7138162851333618
Epoch 320, training loss: 0.14300492405891418 = 0.13610783219337463 + 0.001 * 6.897095203399658
Epoch 320, val loss: 0.7225703597068787
Epoch 330, training loss: 0.1245705857872963 = 0.11767365783452988 + 0.001 * 6.896925926208496
Epoch 330, val loss: 0.7342664003372192
Epoch 340, training loss: 0.10935519635677338 = 0.10245848447084427 + 0.001 * 6.8967132568359375
Epoch 340, val loss: 0.747933030128479
Epoch 350, training loss: 0.09669197350740433 = 0.08979523181915283 + 0.001 * 6.896744728088379
Epoch 350, val loss: 0.7629417181015015
Epoch 360, training loss: 0.0860583707690239 = 0.07916188985109329 + 0.001 * 6.896480083465576
Epoch 360, val loss: 0.7787604331970215
Epoch 370, training loss: 0.07705685496330261 = 0.07016041874885559 + 0.001 * 6.8964385986328125
Epoch 370, val loss: 0.7951393127441406
Epoch 380, training loss: 0.06937786936759949 = 0.06248196214437485 + 0.001 * 6.895905017852783
Epoch 380, val loss: 0.8118369579315186
Epoch 390, training loss: 0.06278342008590698 = 0.05588804557919502 + 0.001 * 6.895374774932861
Epoch 390, val loss: 0.8287522792816162
Epoch 400, training loss: 0.057090163230895996 = 0.05019534006714821 + 0.001 * 6.894822597503662
Epoch 400, val loss: 0.845764696598053
Epoch 410, training loss: 0.05215124413371086 = 0.045257698744535446 + 0.001 * 6.893545627593994
Epoch 410, val loss: 0.8627681732177734
Epoch 420, training loss: 0.047849494963884354 = 0.0409572497010231 + 0.001 * 6.892245769500732
Epoch 420, val loss: 0.8796408772468567
Epoch 430, training loss: 0.04408680647611618 = 0.037196069955825806 + 0.001 * 6.8907341957092285
Epoch 430, val loss: 0.896293580532074
Epoch 440, training loss: 0.040783874690532684 = 0.03389466926455498 + 0.001 * 6.889205455780029
Epoch 440, val loss: 0.9126797318458557
Epoch 450, training loss: 0.03787519410252571 = 0.030986573547124863 + 0.001 * 6.888619422912598
Epoch 450, val loss: 0.9287299513816833
Epoch 460, training loss: 0.03530091419816017 = 0.028414664790034294 + 0.001 * 6.886247634887695
Epoch 460, val loss: 0.9443718791007996
Epoch 470, training loss: 0.033009566366672516 = 0.026125648990273476 + 0.001 * 6.883915424346924
Epoch 470, val loss: 0.9595407247543335
Epoch 480, training loss: 0.03095393441617489 = 0.024072036147117615 + 0.001 * 6.881897449493408
Epoch 480, val loss: 0.9742348194122314
Epoch 490, training loss: 0.02912469208240509 = 0.02224438264966011 + 0.001 * 6.880309104919434
Epoch 490, val loss: 0.988913893699646
Epoch 500, training loss: 0.027487047016620636 = 0.020610766485333443 + 0.001 * 6.876280784606934
Epoch 500, val loss: 1.003253698348999
Epoch 510, training loss: 0.026015618816018105 = 0.019142070785164833 + 0.001 * 6.873547077178955
Epoch 510, val loss: 1.0169081687927246
Epoch 520, training loss: 0.024711783975362778 = 0.01781899482011795 + 0.001 * 6.892788410186768
Epoch 520, val loss: 1.030493140220642
Epoch 530, training loss: 0.023494552820920944 = 0.016624020412564278 + 0.001 * 6.870532035827637
Epoch 530, val loss: 1.043597936630249
Epoch 540, training loss: 0.022409066557884216 = 0.015542590990662575 + 0.001 * 6.8664751052856445
Epoch 540, val loss: 1.0563032627105713
Epoch 550, training loss: 0.021420598030090332 = 0.01456154603511095 + 0.001 * 6.8590521812438965
Epoch 550, val loss: 1.0687527656555176
Epoch 560, training loss: 0.020533166825771332 = 0.013668137602508068 + 0.001 * 6.865027904510498
Epoch 560, val loss: 1.0808700323104858
Epoch 570, training loss: 0.019708827137947083 = 0.012847424484789371 + 0.001 * 6.8614020347595215
Epoch 570, val loss: 1.092819094657898
Epoch 580, training loss: 0.0189436636865139 = 0.012098873034119606 + 0.001 * 6.844791412353516
Epoch 580, val loss: 1.104366421699524
Epoch 590, training loss: 0.018271399661898613 = 0.011413484811782837 + 0.001 * 6.857914924621582
Epoch 590, val loss: 1.115713119506836
Epoch 600, training loss: 0.017623107880353928 = 0.010786134749650955 + 0.001 * 6.836972713470459
Epoch 600, val loss: 1.126659631729126
Epoch 610, training loss: 0.01704271510243416 = 0.010209886357188225 + 0.001 * 6.832828044891357
Epoch 610, val loss: 1.137370228767395
Epoch 620, training loss: 0.016510367393493652 = 0.00967955682426691 + 0.001 * 6.830809593200684
Epoch 620, val loss: 1.147839903831482
Epoch 630, training loss: 0.01602376624941826 = 0.009190386161208153 + 0.001 * 6.833380222320557
Epoch 630, val loss: 1.1580796241760254
Epoch 640, training loss: 0.01557603944092989 = 0.008738390170037746 + 0.001 * 6.837648868560791
Epoch 640, val loss: 1.1680415868759155
Epoch 650, training loss: 0.015124021098017693 = 0.00831998884677887 + 0.001 * 6.804032325744629
Epoch 650, val loss: 1.1777698993682861
Epoch 660, training loss: 0.014781426638364792 = 0.00793197937309742 + 0.001 * 6.8494462966918945
Epoch 660, val loss: 1.1872811317443848
Epoch 670, training loss: 0.014383537694811821 = 0.007571490481495857 + 0.001 * 6.812046527862549
Epoch 670, val loss: 1.1965657472610474
Epoch 680, training loss: 0.014037803746759892 = 0.0072360774502158165 + 0.001 * 6.8017258644104
Epoch 680, val loss: 1.2056492567062378
Epoch 690, training loss: 0.013788586482405663 = 0.006923454813659191 + 0.001 * 6.86513090133667
Epoch 690, val loss: 1.21453857421875
Epoch 700, training loss: 0.013423526659607887 = 0.006631728261709213 + 0.001 * 6.7917985916137695
Epoch 700, val loss: 1.2232263088226318
Epoch 710, training loss: 0.013160075061023235 = 0.006359047722071409 + 0.001 * 6.801026821136475
Epoch 710, val loss: 1.2317357063293457
Epoch 720, training loss: 0.012893058359622955 = 0.006103838328272104 + 0.001 * 6.789219856262207
Epoch 720, val loss: 1.2400548458099365
Epoch 730, training loss: 0.012658117339015007 = 0.005864683073014021 + 0.001 * 6.7934346199035645
Epoch 730, val loss: 1.2482191324234009
Epoch 740, training loss: 0.012446863576769829 = 0.005640228744596243 + 0.001 * 6.806634426116943
Epoch 740, val loss: 1.2561832666397095
Epoch 750, training loss: 0.012202627956867218 = 0.005429377313703299 + 0.001 * 6.773249626159668
Epoch 750, val loss: 1.2640131711959839
Epoch 760, training loss: 0.012026578187942505 = 0.005230794195085764 + 0.001 * 6.795783519744873
Epoch 760, val loss: 1.2716691493988037
Epoch 770, training loss: 0.011815421283245087 = 0.005043500103056431 + 0.001 * 6.771921157836914
Epoch 770, val loss: 1.2791646718978882
Epoch 780, training loss: 0.011623387224972248 = 0.004866090603172779 + 0.001 * 6.757296085357666
Epoch 780, val loss: 1.2864795923233032
Epoch 790, training loss: 0.011460117995738983 = 0.004697263240814209 + 0.001 * 6.762855052947998
Epoch 790, val loss: 1.2936804294586182
Epoch 800, training loss: 0.01131243072450161 = 0.004535988438874483 + 0.001 * 6.776442050933838
Epoch 800, val loss: 1.3007607460021973
Epoch 810, training loss: 0.011158646084368229 = 0.004381583072245121 + 0.001 * 6.777062892913818
Epoch 810, val loss: 1.3077569007873535
Epoch 820, training loss: 0.010996252298355103 = 0.004233817104250193 + 0.001 * 6.762434959411621
Epoch 820, val loss: 1.3146525621414185
Epoch 830, training loss: 0.010887982323765755 = 0.004092177376151085 + 0.001 * 6.795804023742676
Epoch 830, val loss: 1.3215175867080688
Epoch 840, training loss: 0.010702368803322315 = 0.0039568073116242886 + 0.001 * 6.745561122894287
Epoch 840, val loss: 1.3282605409622192
Epoch 850, training loss: 0.010568175464868546 = 0.0038273632526397705 + 0.001 * 6.740811824798584
Epoch 850, val loss: 1.3349649906158447
Epoch 860, training loss: 0.010433802381157875 = 0.003703656140714884 + 0.001 * 6.730146408081055
Epoch 860, val loss: 1.341571569442749
Epoch 870, training loss: 0.010316473431885242 = 0.0035856745671480894 + 0.001 * 6.730798244476318
Epoch 870, val loss: 1.3481377363204956
Epoch 880, training loss: 0.010199420154094696 = 0.003473144955933094 + 0.001 * 6.726274490356445
Epoch 880, val loss: 1.35459566116333
Epoch 890, training loss: 0.010138489305973053 = 0.0033657937310636044 + 0.001 * 6.772695541381836
Epoch 890, val loss: 1.3609974384307861
Epoch 900, training loss: 0.009988470934331417 = 0.003263479098677635 + 0.001 * 6.724991321563721
Epoch 900, val loss: 1.3672852516174316
Epoch 910, training loss: 0.009912123903632164 = 0.003165502566844225 + 0.001 * 6.7466206550598145
Epoch 910, val loss: 1.3735418319702148
Epoch 920, training loss: 0.009787577204406261 = 0.003071173559874296 + 0.001 * 6.716403484344482
Epoch 920, val loss: 1.379790186882019
Epoch 930, training loss: 0.009699895046651363 = 0.002979627577587962 + 0.001 * 6.720267295837402
Epoch 930, val loss: 1.386061668395996
Epoch 940, training loss: 0.009626014158129692 = 0.002890498610213399 + 0.001 * 6.735515594482422
Epoch 940, val loss: 1.3923197984695435
Epoch 950, training loss: 0.009524887427687645 = 0.0028037549927830696 + 0.001 * 6.721132278442383
Epoch 950, val loss: 1.398673415184021
Epoch 960, training loss: 0.009439446963369846 = 0.0027194402646273375 + 0.001 * 6.720006465911865
Epoch 960, val loss: 1.4050021171569824
Epoch 970, training loss: 0.009354117326438427 = 0.002637862227857113 + 0.001 * 6.716254711151123
Epoch 970, val loss: 1.4113481044769287
Epoch 980, training loss: 0.009260265156626701 = 0.0025591938756406307 + 0.001 * 6.701070785522461
Epoch 980, val loss: 1.4176818132400513
Epoch 990, training loss: 0.009194351732730865 = 0.002483522752299905 + 0.001 * 6.7108283042907715
Epoch 990, val loss: 1.4239627122879028
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.5720
Flip ASR: 0.4844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9495623111724854 = 1.9411884546279907 + 0.001 * 8.373909950256348
Epoch 0, val loss: 1.9406206607818604
Epoch 10, training loss: 1.9396671056747437 = 1.931293249130249 + 0.001 * 8.373823165893555
Epoch 10, val loss: 1.930856466293335
Epoch 20, training loss: 1.926940679550171 = 1.9185670614242554 + 0.001 * 8.373590469360352
Epoch 20, val loss: 1.9179115295410156
Epoch 30, training loss: 1.908528208732605 = 1.9001551866531372 + 0.001 * 8.373048782348633
Epoch 30, val loss: 1.898950457572937
Epoch 40, training loss: 1.8808174133300781 = 1.8724457025527954 + 0.001 * 8.371757507324219
Epoch 40, val loss: 1.8709096908569336
Epoch 50, training loss: 1.8424028158187866 = 1.83403480052948 + 0.001 * 8.367974281311035
Epoch 50, val loss: 1.8337997198104858
Epoch 60, training loss: 1.800088882446289 = 1.7917355298995972 + 0.001 * 8.353336334228516
Epoch 60, val loss: 1.7964078187942505
Epoch 70, training loss: 1.7606770992279053 = 1.7523983716964722 + 0.001 * 8.278682708740234
Epoch 70, val loss: 1.7625398635864258
Epoch 80, training loss: 1.7081308364868164 = 1.7002959251403809 + 0.001 * 7.834895610809326
Epoch 80, val loss: 1.7160370349884033
Epoch 90, training loss: 1.6368803977966309 = 1.6291626691818237 + 0.001 * 7.717678070068359
Epoch 90, val loss: 1.6564559936523438
Epoch 100, training loss: 1.5456055402755737 = 1.5379698276519775 + 0.001 * 7.635693073272705
Epoch 100, val loss: 1.5824140310287476
Epoch 110, training loss: 1.4435290098190308 = 1.4360638856887817 + 0.001 * 7.4651408195495605
Epoch 110, val loss: 1.4979737997055054
Epoch 120, training loss: 1.3401085138320923 = 1.3328295946121216 + 0.001 * 7.278885841369629
Epoch 120, val loss: 1.4138705730438232
Epoch 130, training loss: 1.2376734018325806 = 1.2304188013076782 + 0.001 * 7.25464391708374
Epoch 130, val loss: 1.330920934677124
Epoch 140, training loss: 1.136195421218872 = 1.1289916038513184 + 0.001 * 7.203842639923096
Epoch 140, val loss: 1.2503050565719604
Epoch 150, training loss: 1.0374335050582886 = 1.0302984714508057 + 0.001 * 7.1350250244140625
Epoch 150, val loss: 1.1739565134048462
Epoch 160, training loss: 0.9437716603279114 = 0.9367104768753052 + 0.001 * 7.061171531677246
Epoch 160, val loss: 1.1030304431915283
Epoch 170, training loss: 0.8563700318336487 = 0.8493695855140686 + 0.001 * 7.000452995300293
Epoch 170, val loss: 1.0382243394851685
Epoch 180, training loss: 0.7759264707565308 = 0.7689564228057861 + 0.001 * 6.970050811767578
Epoch 180, val loss: 0.9801465272903442
Epoch 190, training loss: 0.7031169533729553 = 0.6961614489555359 + 0.001 * 6.955497741699219
Epoch 190, val loss: 0.9298306107521057
Epoch 200, training loss: 0.63826584815979 = 0.6313197612762451 + 0.001 * 6.9460883140563965
Epoch 200, val loss: 0.8881311416625977
Epoch 210, training loss: 0.580810010433197 = 0.5738725662231445 + 0.001 * 6.937433242797852
Epoch 210, val loss: 0.8549827337265015
Epoch 220, training loss: 0.5296857357025146 = 0.5227595567703247 + 0.001 * 6.926167011260986
Epoch 220, val loss: 0.829764723777771
Epoch 230, training loss: 0.48372066020965576 = 0.4768067002296448 + 0.001 * 6.913974285125732
Epoch 230, val loss: 0.811370313167572
Epoch 240, training loss: 0.44198378920555115 = 0.43507954478263855 + 0.001 * 6.904246807098389
Epoch 240, val loss: 0.7986608147621155
Epoch 250, training loss: 0.403791606426239 = 0.39689508080482483 + 0.001 * 6.896533012390137
Epoch 250, val loss: 0.7907510995864868
Epoch 260, training loss: 0.3686481714248657 = 0.3617578446865082 + 0.001 * 6.890328407287598
Epoch 260, val loss: 0.787147045135498
Epoch 270, training loss: 0.3360603451728821 = 0.32917511463165283 + 0.001 * 6.885216236114502
Epoch 270, val loss: 0.7873324751853943
Epoch 280, training loss: 0.3054230213165283 = 0.29853856563568115 + 0.001 * 6.88446044921875
Epoch 280, val loss: 0.7907200455665588
Epoch 290, training loss: 0.27605488896369934 = 0.2691769301891327 + 0.001 * 6.877968788146973
Epoch 290, val loss: 0.7966194152832031
Epoch 300, training loss: 0.24759235978126526 = 0.24071700870990753 + 0.001 * 6.875353813171387
Epoch 300, val loss: 0.8045501708984375
Epoch 310, training loss: 0.22028064727783203 = 0.213408961892128 + 0.001 * 6.871678829193115
Epoch 310, val loss: 0.8144103288650513
Epoch 320, training loss: 0.1948194056749344 = 0.18794548511505127 + 0.001 * 6.873922824859619
Epoch 320, val loss: 0.8264044523239136
Epoch 330, training loss: 0.17181243002414703 = 0.16494397819042206 + 0.001 * 6.8684539794921875
Epoch 330, val loss: 0.8406411409378052
Epoch 340, training loss: 0.15145115554332733 = 0.14458578824996948 + 0.001 * 6.865361213684082
Epoch 340, val loss: 0.8574807047843933
Epoch 350, training loss: 0.13364656269550323 = 0.12678050994873047 + 0.001 * 6.866052627563477
Epoch 350, val loss: 0.8766053318977356
Epoch 360, training loss: 0.11815374344587326 = 0.11128880083560944 + 0.001 * 6.864940643310547
Epoch 360, val loss: 0.8975033164024353
Epoch 370, training loss: 0.1047394797205925 = 0.09787823259830475 + 0.001 * 6.861247539520264
Epoch 370, val loss: 0.9196380972862244
Epoch 380, training loss: 0.09317925572395325 = 0.086320661008358 + 0.001 * 6.858597278594971
Epoch 380, val loss: 0.9423976540565491
Epoch 390, training loss: 0.08321136236190796 = 0.07634835690259933 + 0.001 * 6.863008975982666
Epoch 390, val loss: 0.9652968645095825
Epoch 400, training loss: 0.07457395642995834 = 0.06771650910377502 + 0.001 * 6.857447624206543
Epoch 400, val loss: 0.9879764318466187
Epoch 410, training loss: 0.06708372384309769 = 0.06022799015045166 + 0.001 * 6.855731964111328
Epoch 410, val loss: 1.0102710723876953
Epoch 420, training loss: 0.060571134090423584 = 0.05371731519699097 + 0.001 * 6.853816509246826
Epoch 420, val loss: 1.0320327281951904
Epoch 430, training loss: 0.0549049898982048 = 0.048049118369817734 + 0.001 * 6.855869770050049
Epoch 430, val loss: 1.0532176494598389
Epoch 440, training loss: 0.04996294528245926 = 0.043109022080898285 + 0.001 * 6.853923797607422
Epoch 440, val loss: 1.0737534761428833
Epoch 450, training loss: 0.045653365552425385 = 0.038800861686468124 + 0.001 * 6.852503299713135
Epoch 450, val loss: 1.0937105417251587
Epoch 460, training loss: 0.04188863933086395 = 0.03503882512450218 + 0.001 * 6.849814414978027
Epoch 460, val loss: 1.113093614578247
Epoch 470, training loss: 0.038597218692302704 = 0.031747620552778244 + 0.001 * 6.849597454071045
Epoch 470, val loss: 1.1318904161453247
Epoch 480, training loss: 0.035716500133275986 = 0.02886197715997696 + 0.001 * 6.854523658752441
Epoch 480, val loss: 1.1501461267471313
Epoch 490, training loss: 0.03317016735672951 = 0.026325209066271782 + 0.001 * 6.844956874847412
Epoch 490, val loss: 1.1678218841552734
Epoch 500, training loss: 0.030933275818824768 = 0.02408946119248867 + 0.001 * 6.843814849853516
Epoch 500, val loss: 1.1849104166030884
Epoch 510, training loss: 0.028956511989235878 = 0.022112984210252762 + 0.001 * 6.843526840209961
Epoch 510, val loss: 1.2014389038085938
Epoch 520, training loss: 0.0272046010941267 = 0.020360330119729042 + 0.001 * 6.844271183013916
Epoch 520, val loss: 1.2174272537231445
Epoch 530, training loss: 0.025640353560447693 = 0.018801329657435417 + 0.001 * 6.839022636413574
Epoch 530, val loss: 1.2328583002090454
Epoch 540, training loss: 0.024248339235782623 = 0.01741020195186138 + 0.001 * 6.838136196136475
Epoch 540, val loss: 1.2478017807006836
Epoch 550, training loss: 0.023001637309789658 = 0.01616457849740982 + 0.001 * 6.837058067321777
Epoch 550, val loss: 1.2622544765472412
Epoch 560, training loss: 0.021877432242035866 = 0.015045623295009136 + 0.001 * 6.831808567047119
Epoch 560, val loss: 1.2762268781661987
Epoch 570, training loss: 0.020868612453341484 = 0.014036418870091438 + 0.001 * 6.832193851470947
Epoch 570, val loss: 1.289811134338379
Epoch 580, training loss: 0.019954025745391846 = 0.013122385367751122 + 0.001 * 6.831639766693115
Epoch 580, val loss: 1.3030134439468384
Epoch 590, training loss: 0.019120074808597565 = 0.012291536666452885 + 0.001 * 6.828538417816162
Epoch 590, val loss: 1.3159292936325073
Epoch 600, training loss: 0.018373064696788788 = 0.01153415348380804 + 0.001 * 6.838910102844238
Epoch 600, val loss: 1.3285377025604248
Epoch 610, training loss: 0.017668548971414566 = 0.010843069292604923 + 0.001 * 6.8254804611206055
Epoch 610, val loss: 1.3408458232879639
Epoch 620, training loss: 0.017038634046912193 = 0.010210384614765644 + 0.001 * 6.828248977661133
Epoch 620, val loss: 1.3528776168823242
Epoch 630, training loss: 0.01645662449300289 = 0.009630694054067135 + 0.001 * 6.825930595397949
Epoch 630, val loss: 1.3646045923233032
Epoch 640, training loss: 0.015914535149931908 = 0.009098552167415619 + 0.001 * 6.815982818603516
Epoch 640, val loss: 1.376045823097229
Epoch 650, training loss: 0.015424884855747223 = 0.008609384298324585 + 0.001 * 6.815499782562256
Epoch 650, val loss: 1.3871883153915405
Epoch 660, training loss: 0.014967508614063263 = 0.008159161545336246 + 0.001 * 6.808346748352051
Epoch 660, val loss: 1.398037314414978
Epoch 670, training loss: 0.01454946119338274 = 0.007743097376078367 + 0.001 * 6.806363582611084
Epoch 670, val loss: 1.4086264371871948
Epoch 680, training loss: 0.01417992077767849 = 0.007357679307460785 + 0.001 * 6.822240829467773
Epoch 680, val loss: 1.418989896774292
Epoch 690, training loss: 0.01381270494312048 = 0.006999005097895861 + 0.001 * 6.813699722290039
Epoch 690, val loss: 1.4291654825210571
Epoch 700, training loss: 0.013454727828502655 = 0.006663990672677755 + 0.001 * 6.790736675262451
Epoch 700, val loss: 1.439186692237854
Epoch 710, training loss: 0.013144208118319511 = 0.006350581534206867 + 0.001 * 6.793626308441162
Epoch 710, val loss: 1.4490604400634766
Epoch 720, training loss: 0.012853018008172512 = 0.006057728081941605 + 0.001 * 6.795289516448975
Epoch 720, val loss: 1.4587666988372803
Epoch 730, training loss: 0.012567657977342606 = 0.005783403757959604 + 0.001 * 6.7842535972595215
Epoch 730, val loss: 1.4683469533920288
Epoch 740, training loss: 0.012333283200860023 = 0.005526301916688681 + 0.001 * 6.806981086730957
Epoch 740, val loss: 1.4777624607086182
Epoch 750, training loss: 0.01209503598511219 = 0.0052855792455375195 + 0.001 * 6.809455871582031
Epoch 750, val loss: 1.48701012134552
Epoch 760, training loss: 0.011863498017191887 = 0.005060124211013317 + 0.001 * 6.803372859954834
Epoch 760, val loss: 1.496087908744812
Epoch 770, training loss: 0.0116269551217556 = 0.004848842043429613 + 0.001 * 6.77811336517334
Epoch 770, val loss: 1.5049904584884644
Epoch 780, training loss: 0.011446837335824966 = 0.004650762304663658 + 0.001 * 6.796075344085693
Epoch 780, val loss: 1.5137088298797607
Epoch 790, training loss: 0.011234797537326813 = 0.004465047270059586 + 0.001 * 6.769750118255615
Epoch 790, val loss: 1.5222221612930298
Epoch 800, training loss: 0.011120103299617767 = 0.004290665965527296 + 0.001 * 6.829437255859375
Epoch 800, val loss: 1.5305572748184204
Epoch 810, training loss: 0.010889850556850433 = 0.004126928746700287 + 0.001 * 6.762921333312988
Epoch 810, val loss: 1.5387059450149536
Epoch 820, training loss: 0.01074204407632351 = 0.003972972743213177 + 0.001 * 6.769071578979492
Epoch 820, val loss: 1.54668390750885
Epoch 830, training loss: 0.010593838058412075 = 0.003828051732853055 + 0.001 * 6.765786170959473
Epoch 830, val loss: 1.554478645324707
Epoch 840, training loss: 0.010472774505615234 = 0.0036915100645273924 + 0.001 * 6.781263828277588
Epoch 840, val loss: 1.562103509902954
Epoch 850, training loss: 0.01032112818211317 = 0.003562814788892865 + 0.001 * 6.758312702178955
Epoch 850, val loss: 1.5695523023605347
Epoch 860, training loss: 0.010190476663410664 = 0.0034413740504533052 + 0.001 * 6.7491021156311035
Epoch 860, val loss: 1.5768495798110962
Epoch 870, training loss: 0.010071732103824615 = 0.003326689824461937 + 0.001 * 6.745041847229004
Epoch 870, val loss: 1.5839709043502808
Epoch 880, training loss: 0.009987180121243 = 0.003218227531760931 + 0.001 * 6.768952369689941
Epoch 880, val loss: 1.5909349918365479
Epoch 890, training loss: 0.0098623838275671 = 0.0031156367622315884 + 0.001 * 6.746746063232422
Epoch 890, val loss: 1.597740650177002
Epoch 900, training loss: 0.00977112166583538 = 0.0030184274073690176 + 0.001 * 6.7526936531066895
Epoch 900, val loss: 1.604406714439392
Epoch 910, training loss: 0.009670502506196499 = 0.0029262832831591368 + 0.001 * 6.744218826293945
Epoch 910, val loss: 1.6109235286712646
Epoch 920, training loss: 0.009593656286597252 = 0.002838837681338191 + 0.001 * 6.754817962646484
Epoch 920, val loss: 1.617323398590088
Epoch 930, training loss: 0.00952157936990261 = 0.0027558510191738605 + 0.001 * 6.76572847366333
Epoch 930, val loss: 1.6235519647598267
Epoch 940, training loss: 0.009408792480826378 = 0.002677032258361578 + 0.001 * 6.731759548187256
Epoch 940, val loss: 1.6296792030334473
Epoch 950, training loss: 0.009356170892715454 = 0.0026020710356533527 + 0.001 * 6.754098892211914
Epoch 950, val loss: 1.635655164718628
Epoch 960, training loss: 0.009293550625443459 = 0.002530765952542424 + 0.001 * 6.762784481048584
Epoch 960, val loss: 1.641526460647583
Epoch 970, training loss: 0.009209070354700089 = 0.0024628511164337397 + 0.001 * 6.746219158172607
Epoch 970, val loss: 1.647253155708313
Epoch 980, training loss: 0.00912005640566349 = 0.0023981209378689528 + 0.001 * 6.721934795379639
Epoch 980, val loss: 1.6528825759887695
Epoch 990, training loss: 0.009050552733242512 = 0.0023363640066236258 + 0.001 * 6.714188098907471
Epoch 990, val loss: 1.6583789587020874
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.8524
Flip ASR: 0.8267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.971078872680664 = 1.9627050161361694 + 0.001 * 8.37387466430664
Epoch 0, val loss: 1.9594500064849854
Epoch 10, training loss: 1.9601161479949951 = 1.95174241065979 + 0.001 * 8.37379264831543
Epoch 10, val loss: 1.9486812353134155
Epoch 20, training loss: 1.9464677572250366 = 1.9380942583084106 + 0.001 * 8.373539924621582
Epoch 20, val loss: 1.9346867799758911
Epoch 30, training loss: 1.9271876811981201 = 1.918814778327942 + 0.001 * 8.372941017150879
Epoch 30, val loss: 1.9144140481948853
Epoch 40, training loss: 1.898669958114624 = 1.89029860496521 + 0.001 * 8.371394157409668
Epoch 40, val loss: 1.8845198154449463
Epoch 50, training loss: 1.8584978580474854 = 1.8501312732696533 + 0.001 * 8.366551399230957
Epoch 50, val loss: 1.8441020250320435
Epoch 60, training loss: 1.8125066757202148 = 1.8041595220565796 + 0.001 * 8.347189903259277
Epoch 60, val loss: 1.8023052215576172
Epoch 70, training loss: 1.772805094718933 = 1.764553189277649 + 0.001 * 8.251874923706055
Epoch 70, val loss: 1.7708441019058228
Epoch 80, training loss: 1.7276660203933716 = 1.719817042350769 + 0.001 * 7.8489766120910645
Epoch 80, val loss: 1.73343026638031
Epoch 90, training loss: 1.666247844696045 = 1.658496618270874 + 0.001 * 7.751211166381836
Epoch 90, val loss: 1.6810249090194702
Epoch 100, training loss: 1.5838592052459717 = 1.576201319694519 + 0.001 * 7.657855987548828
Epoch 100, val loss: 1.6114487648010254
Epoch 110, training loss: 1.4863474369049072 = 1.4787927865982056 + 0.001 * 7.554596900939941
Epoch 110, val loss: 1.5312310457229614
Epoch 120, training loss: 1.3874214887619019 = 1.3800793886184692 + 0.001 * 7.3420491218566895
Epoch 120, val loss: 1.4538313150405884
Epoch 130, training loss: 1.293927788734436 = 1.2868326902389526 + 0.001 * 7.095126152038574
Epoch 130, val loss: 1.3868424892425537
Epoch 140, training loss: 1.2064979076385498 = 1.1994404792785645 + 0.001 * 7.057443618774414
Epoch 140, val loss: 1.3286309242248535
Epoch 150, training loss: 1.123660683631897 = 1.116633415222168 + 0.001 * 7.027299404144287
Epoch 150, val loss: 1.2745977640151978
Epoch 160, training loss: 1.0446171760559082 = 1.037611722946167 + 0.001 * 7.005396842956543
Epoch 160, val loss: 1.222558856010437
Epoch 170, training loss: 0.969439685344696 = 0.9624507427215576 + 0.001 * 6.988954544067383
Epoch 170, val loss: 1.1720480918884277
Epoch 180, training loss: 0.898176372051239 = 0.891206681728363 + 0.001 * 6.969698905944824
Epoch 180, val loss: 1.1234354972839355
Epoch 190, training loss: 0.830007791519165 = 0.8230637311935425 + 0.001 * 6.944085597991943
Epoch 190, val loss: 1.0760688781738281
Epoch 200, training loss: 0.7635894417762756 = 0.7566747069358826 + 0.001 * 6.914731025695801
Epoch 200, val loss: 1.0292619466781616
Epoch 210, training loss: 0.6979047060012817 = 0.6910141706466675 + 0.001 * 6.890505313873291
Epoch 210, val loss: 0.9827516674995422
Epoch 220, training loss: 0.6325900554656982 = 0.6257094740867615 + 0.001 * 6.880592346191406
Epoch 220, val loss: 0.9368420243263245
Epoch 230, training loss: 0.567587673664093 = 0.5607061982154846 + 0.001 * 6.881497383117676
Epoch 230, val loss: 0.8915195465087891
Epoch 240, training loss: 0.5031523704528809 = 0.49626854062080383 + 0.001 * 6.883850574493408
Epoch 240, val loss: 0.8470909595489502
Epoch 250, training loss: 0.44014012813568115 = 0.43325498700141907 + 0.001 * 6.885127544403076
Epoch 250, val loss: 0.804928719997406
Epoch 260, training loss: 0.38006019592285156 = 0.37317416071891785 + 0.001 * 6.886049747467041
Epoch 260, val loss: 0.7669537663459778
Epoch 270, training loss: 0.32500582933425903 = 0.3181189000606537 + 0.001 * 6.886933326721191
Epoch 270, val loss: 0.7353854179382324
Epoch 280, training loss: 0.27678629755973816 = 0.2698984146118164 + 0.001 * 6.887878894805908
Epoch 280, val loss: 0.7110386490821838
Epoch 290, training loss: 0.23600079119205475 = 0.22911204397678375 + 0.001 * 6.8887529373168945
Epoch 290, val loss: 0.6937431693077087
Epoch 300, training loss: 0.20219476521015167 = 0.19530518352985382 + 0.001 * 6.889586448669434
Epoch 300, val loss: 0.68283611536026
Epoch 310, training loss: 0.17436693608760834 = 0.16747541725635529 + 0.001 * 6.8915181159973145
Epoch 310, val loss: 0.6771547794342041
Epoch 320, training loss: 0.15140099823474884 = 0.14450959861278534 + 0.001 * 6.891397476196289
Epoch 320, val loss: 0.6753113865852356
Epoch 330, training loss: 0.1323462277650833 = 0.12545433640480042 + 0.001 * 6.891890525817871
Epoch 330, val loss: 0.6763486266136169
Epoch 340, training loss: 0.11642425507307053 = 0.10953201353549957 + 0.001 * 6.892241954803467
Epoch 340, val loss: 0.6795558333396912
Epoch 350, training loss: 0.10304038226604462 = 0.0961480364203453 + 0.001 * 6.8923444747924805
Epoch 350, val loss: 0.684418261051178
Epoch 360, training loss: 0.0917247012257576 = 0.08483260869979858 + 0.001 * 6.892093181610107
Epoch 360, val loss: 0.6904984712600708
Epoch 370, training loss: 0.08210121840238571 = 0.07520872354507446 + 0.001 * 6.8924946784973145
Epoch 370, val loss: 0.6974846720695496
Epoch 380, training loss: 0.07386881858110428 = 0.06697830557823181 + 0.001 * 6.8905110359191895
Epoch 380, val loss: 0.7052948474884033
Epoch 390, training loss: 0.06679670512676239 = 0.05990058556199074 + 0.001 * 6.896122932434082
Epoch 390, val loss: 0.713718831539154
Epoch 400, training loss: 0.0606759712100029 = 0.05378603935241699 + 0.001 * 6.889930725097656
Epoch 400, val loss: 0.7225773930549622
Epoch 410, training loss: 0.05536441132426262 = 0.048481158912181854 + 0.001 * 6.8832526206970215
Epoch 410, val loss: 0.7318089604377747
Epoch 420, training loss: 0.0507381372153759 = 0.04385833069682121 + 0.001 * 6.879807472229004
Epoch 420, val loss: 0.7413198947906494
Epoch 430, training loss: 0.04669862240552902 = 0.0398145355284214 + 0.001 * 6.884085655212402
Epoch 430, val loss: 0.7509698867797852
Epoch 440, training loss: 0.0431370809674263 = 0.036264970898628235 + 0.001 * 6.872108459472656
Epoch 440, val loss: 0.7607265710830688
Epoch 450, training loss: 0.04000174626708031 = 0.033138424158096313 + 0.001 * 6.863320827484131
Epoch 450, val loss: 0.7704732418060303
Epoch 460, training loss: 0.03731946647167206 = 0.030375560745596886 + 0.001 * 6.94390344619751
Epoch 460, val loss: 0.7801846861839294
Epoch 470, training loss: 0.03477436304092407 = 0.027925925329327583 + 0.001 * 6.848438262939453
Epoch 470, val loss: 0.7897911071777344
Epoch 480, training loss: 0.03259129077196121 = 0.025747371837496758 + 0.001 * 6.843919277191162
Epoch 480, val loss: 0.7992809414863586
Epoch 490, training loss: 0.0306386798620224 = 0.023804133757948875 + 0.001 * 6.834546089172363
Epoch 490, val loss: 0.8085830211639404
Epoch 500, training loss: 0.028899598866701126 = 0.022065578028559685 + 0.001 * 6.834020614624023
Epoch 500, val loss: 0.8177156448364258
Epoch 510, training loss: 0.027345620095729828 = 0.020505204796791077 + 0.001 * 6.8404154777526855
Epoch 510, val loss: 0.8266644477844238
Epoch 520, training loss: 0.025911003351211548 = 0.019099228084087372 + 0.001 * 6.811775207519531
Epoch 520, val loss: 0.8354058265686035
Epoch 530, training loss: 0.024639850482344627 = 0.017826298251748085 + 0.001 * 6.813552379608154
Epoch 530, val loss: 0.8439453840255737
Epoch 540, training loss: 0.023475565016269684 = 0.01666739210486412 + 0.001 * 6.808172702789307
Epoch 540, val loss: 0.8522758483886719
Epoch 550, training loss: 0.022412076592445374 = 0.01560908928513527 + 0.001 * 6.8029866218566895
Epoch 550, val loss: 0.8604780435562134
Epoch 560, training loss: 0.021443970501422882 = 0.01464103627949953 + 0.001 * 6.802933216094971
Epoch 560, val loss: 0.8684865832328796
Epoch 570, training loss: 0.020576149225234985 = 0.013754698447883129 + 0.001 * 6.8214497566223145
Epoch 570, val loss: 0.8763831853866577
Epoch 580, training loss: 0.019743595272302628 = 0.012942439876496792 + 0.001 * 6.801154136657715
Epoch 580, val loss: 0.884058952331543
Epoch 590, training loss: 0.019001444801688194 = 0.012197300791740417 + 0.001 * 6.804143905639648
Epoch 590, val loss: 0.8915953636169434
Epoch 600, training loss: 0.01831246167421341 = 0.011513072066009045 + 0.001 * 6.799388408660889
Epoch 600, val loss: 0.8989275097846985
Epoch 610, training loss: 0.017699211835861206 = 0.010883815586566925 + 0.001 * 6.815395355224609
Epoch 610, val loss: 0.9061219096183777
Epoch 620, training loss: 0.017102476209402084 = 0.010304317809641361 + 0.001 * 6.798157215118408
Epoch 620, val loss: 0.9131621718406677
Epoch 630, training loss: 0.016566524282097816 = 0.009769750759005547 + 0.001 * 6.7967729568481445
Epoch 630, val loss: 0.9200214147567749
Epoch 640, training loss: 0.01606534793972969 = 0.009276000782847404 + 0.001 * 6.789346694946289
Epoch 640, val loss: 0.9267496466636658
Epoch 650, training loss: 0.015602938830852509 = 0.008819175884127617 + 0.001 * 6.783762454986572
Epoch 650, val loss: 0.9332965016365051
Epoch 660, training loss: 0.015186941251158714 = 0.008395860902965069 + 0.001 * 6.791080474853516
Epoch 660, val loss: 0.9397144317626953
Epoch 670, training loss: 0.014780576340854168 = 0.008003025315701962 + 0.001 * 6.77755069732666
Epoch 670, val loss: 0.9459975957870483
Epoch 680, training loss: 0.014459718018770218 = 0.007637880742549896 + 0.001 * 6.821837425231934
Epoch 680, val loss: 0.9521077275276184
Epoch 690, training loss: 0.014087846502661705 = 0.007298169191926718 + 0.001 * 6.789676666259766
Epoch 690, val loss: 0.9581081867218018
Epoch 700, training loss: 0.013765092939138412 = 0.006981558166444302 + 0.001 * 6.783534526824951
Epoch 700, val loss: 0.9639270901679993
Epoch 710, training loss: 0.013458127155900002 = 0.006686068139970303 + 0.001 * 6.772058486938477
Epoch 710, val loss: 0.969622790813446
Epoch 720, training loss: 0.013197077438235283 = 0.006409813649952412 + 0.001 * 6.7872633934021
Epoch 720, val loss: 0.9752467274665833
Epoch 730, training loss: 0.012915066443383694 = 0.006151287350803614 + 0.001 * 6.7637786865234375
Epoch 730, val loss: 0.9806833863258362
Epoch 740, training loss: 0.012684155255556107 = 0.005909021943807602 + 0.001 * 6.77513313293457
Epoch 740, val loss: 0.9860257506370544
Epoch 750, training loss: 0.012448765337467194 = 0.005681697279214859 + 0.001 * 6.767067909240723
Epoch 750, val loss: 0.9912535548210144
Epoch 760, training loss: 0.012236129492521286 = 0.005468164104968309 + 0.001 * 6.767964839935303
Epoch 760, val loss: 0.9963406324386597
Epoch 770, training loss: 0.012032819911837578 = 0.00526737654581666 + 0.001 * 6.765443325042725
Epoch 770, val loss: 1.0013645887374878
Epoch 780, training loss: 0.011845838278532028 = 0.00507832458242774 + 0.001 * 6.767512798309326
Epoch 780, val loss: 1.0062410831451416
Epoch 790, training loss: 0.01165100373327732 = 0.004900106694549322 + 0.001 * 6.750897407531738
Epoch 790, val loss: 1.0110362768173218
Epoch 800, training loss: 0.011498442851006985 = 0.00473188329488039 + 0.001 * 6.76655912399292
Epoch 800, val loss: 1.0157277584075928
Epoch 810, training loss: 0.011327022686600685 = 0.004572978243231773 + 0.001 * 6.754044055938721
Epoch 810, val loss: 1.0202912092208862
Epoch 820, training loss: 0.011187110096216202 = 0.0044226860627532005 + 0.001 * 6.764423847198486
Epoch 820, val loss: 1.0247920751571655
Epoch 830, training loss: 0.01104472391307354 = 0.004280450753867626 + 0.001 * 6.764272689819336
Epoch 830, val loss: 1.0291510820388794
Epoch 840, training loss: 0.010890001431107521 = 0.00414567394182086 + 0.001 * 6.744327068328857
Epoch 840, val loss: 1.0334514379501343
Epoch 850, training loss: 0.010770715773105621 = 0.004017858300358057 + 0.001 * 6.752857685089111
Epoch 850, val loss: 1.03766930103302
Epoch 860, training loss: 0.010642953217029572 = 0.003896544221788645 + 0.001 * 6.746408939361572
Epoch 860, val loss: 1.0417826175689697
Epoch 870, training loss: 0.010520740412175655 = 0.003781284671276808 + 0.001 * 6.739455223083496
Epoch 870, val loss: 1.045824408531189
Epoch 880, training loss: 0.010427860543131828 = 0.0036717290058732033 + 0.001 * 6.756130695343018
Epoch 880, val loss: 1.0497841835021973
Epoch 890, training loss: 0.010294160805642605 = 0.003567491425201297 + 0.001 * 6.726668834686279
Epoch 890, val loss: 1.05366051197052
Epoch 900, training loss: 0.010204889811575413 = 0.0034681970719248056 + 0.001 * 6.736692428588867
Epoch 900, val loss: 1.057464361190796
Epoch 910, training loss: 0.010096907615661621 = 0.0033735695760697126 + 0.001 * 6.7233381271362305
Epoch 910, val loss: 1.061166524887085
Epoch 920, training loss: 0.010004919022321701 = 0.0032833456061780453 + 0.001 * 6.721573352813721
Epoch 920, val loss: 1.0648276805877686
Epoch 930, training loss: 0.009909776970744133 = 0.0031972574070096016 + 0.001 * 6.71251916885376
Epoch 930, val loss: 1.0684144496917725
Epoch 940, training loss: 0.009840504266321659 = 0.0031150239519774914 + 0.001 * 6.725480079650879
Epoch 940, val loss: 1.0719131231307983
Epoch 950, training loss: 0.009811034426093102 = 0.003036416368559003 + 0.001 * 6.774618148803711
Epoch 950, val loss: 1.075350046157837
Epoch 960, training loss: 0.009666827507317066 = 0.0029612977523356676 + 0.001 * 6.705529689788818
Epoch 960, val loss: 1.0787025690078735
Epoch 970, training loss: 0.00961842481046915 = 0.002889371942728758 + 0.001 * 6.729052543640137
Epoch 970, val loss: 1.0820099115371704
Epoch 980, training loss: 0.009521440602838993 = 0.002820499474182725 + 0.001 * 6.70094108581543
Epoch 980, val loss: 1.085263967514038
Epoch 990, training loss: 0.009478408843278885 = 0.0027545124758034945 + 0.001 * 6.723896503448486
Epoch 990, val loss: 1.0884264707565308
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8044
Flip ASR: 0.7733/225 nodes
The final ASR:0.74293, 0.12247, Accuracy:0.81235, 0.02968
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10582])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98524, 0.00797, Accuracy:0.83333, 0.00907
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.963143229484558 = 1.9547693729400635 + 0.001 * 8.373882293701172
Epoch 0, val loss: 1.9538214206695557
Epoch 10, training loss: 1.9533233642578125 = 1.9449496269226074 + 0.001 * 8.37378978729248
Epoch 10, val loss: 1.9445909261703491
Epoch 20, training loss: 1.9410369396209717 = 1.9326634407043457 + 0.001 * 8.373553276062012
Epoch 20, val loss: 1.9325059652328491
Epoch 30, training loss: 1.9233702421188354 = 1.9149972200393677 + 0.001 * 8.373047828674316
Epoch 30, val loss: 1.9147142171859741
Epoch 40, training loss: 1.8965102434158325 = 1.8881384134292603 + 0.001 * 8.371782302856445
Epoch 40, val loss: 1.8875031471252441
Epoch 50, training loss: 1.8574914932250977 = 1.8491238355636597 + 0.001 * 8.367697715759277
Epoch 50, val loss: 1.8493444919586182
Epoch 60, training loss: 1.8112542629241943 = 1.8029038906097412 + 0.001 * 8.350341796875
Epoch 60, val loss: 1.8084850311279297
Epoch 70, training loss: 1.7703948020935059 = 1.762127161026001 + 0.001 * 8.267618179321289
Epoch 70, val loss: 1.777129888534546
Epoch 80, training loss: 1.7233030796051025 = 1.7154945135116577 + 0.001 * 7.8085222244262695
Epoch 80, val loss: 1.7391619682312012
Epoch 90, training loss: 1.6582961082458496 = 1.6506600379943848 + 0.001 * 7.636051654815674
Epoch 90, val loss: 1.6846696138381958
Epoch 100, training loss: 1.5723048448562622 = 1.5648516416549683 + 0.001 * 7.453153610229492
Epoch 100, val loss: 1.6139037609100342
Epoch 110, training loss: 1.4675710201263428 = 1.460270643234253 + 0.001 * 7.300319671630859
Epoch 110, val loss: 1.5293644666671753
Epoch 120, training loss: 1.3542065620422363 = 1.3469998836517334 + 0.001 * 7.206656455993652
Epoch 120, val loss: 1.4379552602767944
Epoch 130, training loss: 1.242550015449524 = 1.2354425191879272 + 0.001 * 7.107508182525635
Epoch 130, val loss: 1.3488367795944214
Epoch 140, training loss: 1.1387437582015991 = 1.1317397356033325 + 0.001 * 7.004080295562744
Epoch 140, val loss: 1.2662066221237183
Epoch 150, training loss: 1.0441094636917114 = 1.0371681451797485 + 0.001 * 6.9413347244262695
Epoch 150, val loss: 1.1913222074508667
Epoch 160, training loss: 0.9571872353553772 = 0.9502806067466736 + 0.001 * 6.906636714935303
Epoch 160, val loss: 1.1232408285140991
Epoch 170, training loss: 0.8758664727210999 = 0.8689804673194885 + 0.001 * 6.885988712310791
Epoch 170, val loss: 1.0607352256774902
Epoch 180, training loss: 0.798744797706604 = 0.791881263256073 + 0.001 * 6.8635053634643555
Epoch 180, val loss: 1.0029830932617188
Epoch 190, training loss: 0.7260037064552307 = 0.7191557884216309 + 0.001 * 6.84789514541626
Epoch 190, val loss: 0.9498457312583923
Epoch 200, training loss: 0.6589500904083252 = 0.6521089673042297 + 0.001 * 6.841141223907471
Epoch 200, val loss: 0.9025992751121521
Epoch 210, training loss: 0.598689079284668 = 0.5918492674827576 + 0.001 * 6.839783191680908
Epoch 210, val loss: 0.8620262145996094
Epoch 220, training loss: 0.5451557040214539 = 0.5383155941963196 + 0.001 * 6.840131759643555
Epoch 220, val loss: 0.8280410170555115
Epoch 230, training loss: 0.4974706172943115 = 0.49063166975975037 + 0.001 * 6.838959217071533
Epoch 230, val loss: 0.7999802827835083
Epoch 240, training loss: 0.45436546206474304 = 0.44752827286720276 + 0.001 * 6.837194919586182
Epoch 240, val loss: 0.7770029902458191
Epoch 250, training loss: 0.41441819071769714 = 0.4075831472873688 + 0.001 * 6.835035800933838
Epoch 250, val loss: 0.7583041787147522
Epoch 260, training loss: 0.37608814239501953 = 0.36925560235977173 + 0.001 * 6.832545280456543
Epoch 260, val loss: 0.7428164482116699
Epoch 270, training loss: 0.3383190929889679 = 0.3314892053604126 + 0.001 * 6.829891204833984
Epoch 270, val loss: 0.7296182513237
Epoch 280, training loss: 0.30097708106040955 = 0.2941499948501587 + 0.001 * 6.827099800109863
Epoch 280, val loss: 0.718498945236206
Epoch 290, training loss: 0.2647748291492462 = 0.2579503357410431 + 0.001 * 6.824483394622803
Epoch 290, val loss: 0.7098585367202759
Epoch 300, training loss: 0.23086056113243103 = 0.22403906285762787 + 0.001 * 6.821493625640869
Epoch 300, val loss: 0.7042202353477478
Epoch 310, training loss: 0.2002338469028473 = 0.19341565668582916 + 0.001 * 6.8181891441345215
Epoch 310, val loss: 0.7019307613372803
Epoch 320, training loss: 0.17349302768707275 = 0.1666770875453949 + 0.001 * 6.815944671630859
Epoch 320, val loss: 0.703248918056488
Epoch 330, training loss: 0.15076318383216858 = 0.14395157992839813 + 0.001 * 6.811606407165527
Epoch 330, val loss: 0.7084677219390869
Epoch 340, training loss: 0.13173285126686096 = 0.12492493540048599 + 0.001 * 6.807921409606934
Epoch 340, val loss: 0.7173087000846863
Epoch 350, training loss: 0.11582064628601074 = 0.10901670157909393 + 0.001 * 6.8039469718933105
Epoch 350, val loss: 0.7290254235267639
Epoch 360, training loss: 0.10243356227874756 = 0.09563398361206055 + 0.001 * 6.7995758056640625
Epoch 360, val loss: 0.7427523732185364
Epoch 370, training loss: 0.09108545631170273 = 0.0842859074473381 + 0.001 * 6.799551010131836
Epoch 370, val loss: 0.7576866745948792
Epoch 380, training loss: 0.08137679845094681 = 0.07458397001028061 + 0.001 * 6.792829513549805
Epoch 380, val loss: 0.7732743620872498
Epoch 390, training loss: 0.0730106309056282 = 0.06622550636529922 + 0.001 * 6.785120964050293
Epoch 390, val loss: 0.7890383005142212
Epoch 400, training loss: 0.0657651424407959 = 0.058984678238630295 + 0.001 * 6.780467510223389
Epoch 400, val loss: 0.8047282695770264
Epoch 410, training loss: 0.059471987187862396 = 0.05268910899758339 + 0.001 * 6.782878398895264
Epoch 410, val loss: 0.8201854228973389
Epoch 420, training loss: 0.05397525429725647 = 0.047200288623571396 + 0.001 * 6.77496337890625
Epoch 420, val loss: 0.8353538513183594
Epoch 430, training loss: 0.04917312413454056 = 0.04240493103861809 + 0.001 * 6.768192291259766
Epoch 430, val loss: 0.8501848578453064
Epoch 440, training loss: 0.04498143121600151 = 0.03821311146020889 + 0.001 * 6.768320560455322
Epoch 440, val loss: 0.8645879030227661
Epoch 450, training loss: 0.041312847286462784 = 0.03454522415995598 + 0.001 * 6.7676215171813965
Epoch 450, val loss: 0.8786232471466064
Epoch 460, training loss: 0.03809315711259842 = 0.0313294418156147 + 0.001 * 6.763716697692871
Epoch 460, val loss: 0.8922141790390015
Epoch 470, training loss: 0.03526736795902252 = 0.028505340218544006 + 0.001 * 6.762025833129883
Epoch 470, val loss: 0.9054433107376099
Epoch 480, training loss: 0.03278162330389023 = 0.026018016040325165 + 0.001 * 6.763605117797852
Epoch 480, val loss: 0.9182513356208801
Epoch 490, training loss: 0.030587652698159218 = 0.0238240547478199 + 0.001 * 6.7635979652404785
Epoch 490, val loss: 0.9307050704956055
Epoch 500, training loss: 0.028645973652601242 = 0.021886251866817474 + 0.001 * 6.759721279144287
Epoch 500, val loss: 0.942738950252533
Epoch 510, training loss: 0.026925954967737198 = 0.020166995003819466 + 0.001 * 6.75895881652832
Epoch 510, val loss: 0.95440673828125
Epoch 520, training loss: 0.02539907768368721 = 0.018636729568243027 + 0.001 * 6.7623467445373535
Epoch 520, val loss: 0.9656756520271301
Epoch 530, training loss: 0.024027153849601746 = 0.01726873219013214 + 0.001 * 6.758421421051025
Epoch 530, val loss: 0.9766115546226501
Epoch 540, training loss: 0.022800501435995102 = 0.016041046008467674 + 0.001 * 6.759456157684326
Epoch 540, val loss: 0.9872093796730042
Epoch 550, training loss: 0.02169264666736126 = 0.014934487640857697 + 0.001 * 6.7581586837768555
Epoch 550, val loss: 0.9975166320800781
Epoch 560, training loss: 0.020688770338892937 = 0.01393332052975893 + 0.001 * 6.7554497718811035
Epoch 560, val loss: 1.0075156688690186
Epoch 570, training loss: 0.01978648267686367 = 0.013024905696511269 + 0.001 * 6.761577129364014
Epoch 570, val loss: 1.0172255039215088
Epoch 580, training loss: 0.018954668194055557 = 0.012198374606668949 + 0.001 * 6.756293773651123
Epoch 580, val loss: 1.0266437530517578
Epoch 590, training loss: 0.018199842423200607 = 0.011444932781159878 + 0.001 * 6.754910469055176
Epoch 590, val loss: 1.035788893699646
Epoch 600, training loss: 0.017508579418063164 = 0.010757016018033028 + 0.001 * 6.751562595367432
Epoch 600, val loss: 1.044678807258606
Epoch 610, training loss: 0.016879165545105934 = 0.01012798584997654 + 0.001 * 6.7511796951293945
Epoch 610, val loss: 1.053304672241211
Epoch 620, training loss: 0.016317743808031082 = 0.009551885537803173 + 0.001 * 6.765857219696045
Epoch 620, val loss: 1.0616474151611328
Epoch 630, training loss: 0.015774959698319435 = 0.009023522958159447 + 0.001 * 6.751436710357666
Epoch 630, val loss: 1.0697691440582275
Epoch 640, training loss: 0.01528666540980339 = 0.008538099005818367 + 0.001 * 6.748566627502441
Epoch 640, val loss: 1.0776299238204956
Epoch 650, training loss: 0.014842817559838295 = 0.008091313764452934 + 0.001 * 6.751503944396973
Epoch 650, val loss: 1.0852723121643066
Epoch 660, training loss: 0.014428770169615746 = 0.007679462432861328 + 0.001 * 6.749308109283447
Epoch 660, val loss: 1.0926681756973267
Epoch 670, training loss: 0.014046435244381428 = 0.007299270946532488 + 0.001 * 6.747163772583008
Epoch 670, val loss: 1.0998767614364624
Epoch 680, training loss: 0.013699050061404705 = 0.006947674322873354 + 0.001 * 6.751375198364258
Epoch 680, val loss: 1.106864333152771
Epoch 690, training loss: 0.013368669897317886 = 0.006622036918997765 + 0.001 * 6.746633052825928
Epoch 690, val loss: 1.1136753559112549
Epoch 700, training loss: 0.013062113896012306 = 0.006319946609437466 + 0.001 * 6.7421674728393555
Epoch 700, val loss: 1.1202926635742188
Epoch 710, training loss: 0.012779787182807922 = 0.0060391975566744804 + 0.001 * 6.740589141845703
Epoch 710, val loss: 1.1267346143722534
Epoch 720, training loss: 0.012524480931460857 = 0.005777882412075996 + 0.001 * 6.746598243713379
Epoch 720, val loss: 1.1330010890960693
Epoch 730, training loss: 0.012273648753762245 = 0.005534271243959665 + 0.001 * 6.739377021789551
Epoch 730, val loss: 1.1391167640686035
Epoch 740, training loss: 0.012043343856930733 = 0.005306848790496588 + 0.001 * 6.736495018005371
Epoch 740, val loss: 1.1450670957565308
Epoch 750, training loss: 0.011836206540465355 = 0.005094194784760475 + 0.001 * 6.742011547088623
Epoch 750, val loss: 1.1508632898330688
Epoch 760, training loss: 0.011629892513155937 = 0.004895098973065615 + 0.001 * 6.734793663024902
Epoch 760, val loss: 1.1565277576446533
Epoch 770, training loss: 0.01144677959382534 = 0.0047084554098546505 + 0.001 * 6.738323211669922
Epoch 770, val loss: 1.1620569229125977
Epoch 780, training loss: 0.011267539113759995 = 0.004533249884843826 + 0.001 * 6.734288692474365
Epoch 780, val loss: 1.167450189590454
Epoch 790, training loss: 0.011106910184025764 = 0.004368570167571306 + 0.001 * 6.738339900970459
Epoch 790, val loss: 1.1727155447006226
Epoch 800, training loss: 0.010943583212792873 = 0.004213598556816578 + 0.001 * 6.729984283447266
Epoch 800, val loss: 1.1778664588928223
Epoch 810, training loss: 0.010796882212162018 = 0.00406759325414896 + 0.001 * 6.729288101196289
Epoch 810, val loss: 1.1828876733779907
Epoch 820, training loss: 0.010662078857421875 = 0.003929845057427883 + 0.001 * 6.732233047485352
Epoch 820, val loss: 1.1877951622009277
Epoch 830, training loss: 0.010525044053792953 = 0.0037997765466570854 + 0.001 * 6.725266933441162
Epoch 830, val loss: 1.1926003694534302
Epoch 840, training loss: 0.010407437570393085 = 0.0036768224090337753 + 0.001 * 6.73061466217041
Epoch 840, val loss: 1.197310209274292
Epoch 850, training loss: 0.010281730443239212 = 0.003560452489182353 + 0.001 * 6.721277236938477
Epoch 850, val loss: 1.2018933296203613
Epoch 860, training loss: 0.010178888216614723 = 0.003450233256444335 + 0.001 * 6.728654384613037
Epoch 860, val loss: 1.2063987255096436
Epoch 870, training loss: 0.010064303874969482 = 0.003345716744661331 + 0.001 * 6.718586444854736
Epoch 870, val loss: 1.2108008861541748
Epoch 880, training loss: 0.009967336431145668 = 0.0032465450931340456 + 0.001 * 6.720791339874268
Epoch 880, val loss: 1.2151128053665161
Epoch 890, training loss: 0.00987282395362854 = 0.0031523273792117834 + 0.001 * 6.720496654510498
Epoch 890, val loss: 1.219341516494751
Epoch 900, training loss: 0.009775657206773758 = 0.003062736475840211 + 0.001 * 6.712920188903809
Epoch 900, val loss: 1.223471999168396
Epoch 910, training loss: 0.009688721038401127 = 0.0029774957802146673 + 0.001 * 6.7112250328063965
Epoch 910, val loss: 1.2275468111038208
Epoch 920, training loss: 0.009615674614906311 = 0.0028963107615709305 + 0.001 * 6.719363689422607
Epoch 920, val loss: 1.2315276861190796
Epoch 930, training loss: 0.009541777893900871 = 0.002818930894136429 + 0.001 * 6.722846984863281
Epoch 930, val loss: 1.2354347705841064
Epoch 940, training loss: 0.009460611268877983 = 0.002745129866525531 + 0.001 * 6.715481281280518
Epoch 940, val loss: 1.2392622232437134
Epoch 950, training loss: 0.009380809962749481 = 0.002674700226634741 + 0.001 * 6.706109046936035
Epoch 950, val loss: 1.2430115938186646
Epoch 960, training loss: 0.009330876171588898 = 0.002607415895909071 + 0.001 * 6.7234601974487305
Epoch 960, val loss: 1.2467122077941895
Epoch 970, training loss: 0.009248664602637291 = 0.002543109003454447 + 0.001 * 6.705554962158203
Epoch 970, val loss: 1.2503106594085693
Epoch 980, training loss: 0.009197638370096684 = 0.002481613075360656 + 0.001 * 6.716025352478027
Epoch 980, val loss: 1.2538610696792603
Epoch 990, training loss: 0.009122313000261784 = 0.0024227467365562916 + 0.001 * 6.699565887451172
Epoch 990, val loss: 1.2573212385177612
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.4723
Flip ASR: 0.3956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9412622451782227 = 1.932888388633728 + 0.001 * 8.373856544494629
Epoch 0, val loss: 1.927092432975769
Epoch 10, training loss: 1.9315792322158813 = 1.9232054948806763 + 0.001 * 8.373741149902344
Epoch 10, val loss: 1.91744863986969
Epoch 20, training loss: 1.919760823249817 = 1.9113874435424805 + 0.001 * 8.373370170593262
Epoch 20, val loss: 1.9050136804580688
Epoch 30, training loss: 1.9031785726547241 = 1.8948060274124146 + 0.001 * 8.372579574584961
Epoch 30, val loss: 1.8868550062179565
Epoch 40, training loss: 1.878902554512024 = 1.8705315589904785 + 0.001 * 8.370945930480957
Epoch 40, val loss: 1.8601419925689697
Epoch 50, training loss: 1.8442484140396118 = 1.835881233215332 + 0.001 * 8.367151260375977
Epoch 50, val loss: 1.8234829902648926
Epoch 60, training loss: 1.7997337579727173 = 1.7913788557052612 + 0.001 * 8.354877471923828
Epoch 60, val loss: 1.7801856994628906
Epoch 70, training loss: 1.7499675750732422 = 1.7416776418685913 + 0.001 * 8.289911270141602
Epoch 70, val loss: 1.7356857061386108
Epoch 80, training loss: 1.6899629831314087 = 1.6821595430374146 + 0.001 * 7.803414821624756
Epoch 80, val loss: 1.6823770999908447
Epoch 90, training loss: 1.608628273010254 = 1.601086139678955 + 0.001 * 7.5421061515808105
Epoch 90, val loss: 1.612608551979065
Epoch 100, training loss: 1.5065784454345703 = 1.4991059303283691 + 0.001 * 7.472544193267822
Epoch 100, val loss: 1.5307964086532593
Epoch 110, training loss: 1.3956176042556763 = 1.3882794380187988 + 0.001 * 7.338176250457764
Epoch 110, val loss: 1.4442102909088135
Epoch 120, training loss: 1.2907538414001465 = 1.2835073471069336 + 0.001 * 7.246540069580078
Epoch 120, val loss: 1.3667155504226685
Epoch 130, training loss: 1.1979563236236572 = 1.1907532215118408 + 0.001 * 7.203102111816406
Epoch 130, val loss: 1.3009681701660156
Epoch 140, training loss: 1.1152739524841309 = 1.1080880165100098 + 0.001 * 7.185927867889404
Epoch 140, val loss: 1.2444394826889038
Epoch 150, training loss: 1.039419412612915 = 1.0322465896606445 + 0.001 * 7.172823429107666
Epoch 150, val loss: 1.1937144994735718
Epoch 160, training loss: 0.9684037566184998 = 0.9612624645233154 + 0.001 * 7.1413187980651855
Epoch 160, val loss: 1.1467294692993164
Epoch 170, training loss: 0.9017804265022278 = 0.8946838974952698 + 0.001 * 7.096552848815918
Epoch 170, val loss: 1.1022804975509644
Epoch 180, training loss: 0.8390803337097168 = 0.8320339322090149 + 0.001 * 7.046402931213379
Epoch 180, val loss: 1.060802936553955
Epoch 190, training loss: 0.7800117135047913 = 0.7730004191398621 + 0.001 * 7.011289596557617
Epoch 190, val loss: 1.0230664014816284
Epoch 200, training loss: 0.7243590354919434 = 0.717377245426178 + 0.001 * 6.981799602508545
Epoch 200, val loss: 0.9897711873054504
Epoch 210, training loss: 0.6718869805335999 = 0.6649380922317505 + 0.001 * 6.948892593383789
Epoch 210, val loss: 0.9605417847633362
Epoch 220, training loss: 0.6222458481788635 = 0.6153246164321899 + 0.001 * 6.921255111694336
Epoch 220, val loss: 0.9349502921104431
Epoch 230, training loss: 0.5752172470092773 = 0.5683122873306274 + 0.001 * 6.904937744140625
Epoch 230, val loss: 0.9132301807403564
Epoch 240, training loss: 0.5307417511940002 = 0.5238445997238159 + 0.001 * 6.8971333503723145
Epoch 240, val loss: 0.8954025506973267
Epoch 250, training loss: 0.48860040307044983 = 0.48170721530914307 + 0.001 * 6.893184661865234
Epoch 250, val loss: 0.8814828395843506
Epoch 260, training loss: 0.44864487648010254 = 0.4417545795440674 + 0.001 * 6.89030647277832
Epoch 260, val loss: 0.8709914684295654
Epoch 270, training loss: 0.41038936376571655 = 0.4035016894340515 + 0.001 * 6.887689113616943
Epoch 270, val loss: 0.8635313510894775
Epoch 280, training loss: 0.3735211193561554 = 0.3666360080242157 + 0.001 * 6.885107040405273
Epoch 280, val loss: 0.8584578633308411
Epoch 290, training loss: 0.33797168731689453 = 0.3310885429382324 + 0.001 * 6.883155345916748
Epoch 290, val loss: 0.8557295799255371
Epoch 300, training loss: 0.30394792556762695 = 0.29706695675849915 + 0.001 * 6.880977630615234
Epoch 300, val loss: 0.8550267815589905
Epoch 310, training loss: 0.2718321681022644 = 0.26495298743247986 + 0.001 * 6.879182815551758
Epoch 310, val loss: 0.856469988822937
Epoch 320, training loss: 0.24199412763118744 = 0.23511675000190735 + 0.001 * 6.877376556396484
Epoch 320, val loss: 0.8599700331687927
Epoch 330, training loss: 0.21484647691249847 = 0.20797115564346313 + 0.001 * 6.875328063964844
Epoch 330, val loss: 0.8658533692359924
Epoch 340, training loss: 0.19064325094223022 = 0.1837639957666397 + 0.001 * 6.87924861907959
Epoch 340, val loss: 0.8741366267204285
Epoch 350, training loss: 0.16933810710906982 = 0.16246585547924042 + 0.001 * 6.872251033782959
Epoch 350, val loss: 0.8847487568855286
Epoch 360, training loss: 0.15076209604740143 = 0.14389343559741974 + 0.001 * 6.868659019470215
Epoch 360, val loss: 0.8973649144172668
Epoch 370, training loss: 0.1346302181482315 = 0.12776394188404083 + 0.001 * 6.866277694702148
Epoch 370, val loss: 0.911514937877655
Epoch 380, training loss: 0.12056390196084976 = 0.11370112001895905 + 0.001 * 6.862778186798096
Epoch 380, val loss: 0.926891565322876
Epoch 390, training loss: 0.10824217647314072 = 0.10138469934463501 + 0.001 * 6.857474327087402
Epoch 390, val loss: 0.9430082440376282
Epoch 400, training loss: 0.09742027521133423 = 0.09056688845157623 + 0.001 * 6.853389263153076
Epoch 400, val loss: 0.9595251083374023
Epoch 410, training loss: 0.08789459615945816 = 0.08103911578655243 + 0.001 * 6.855477809906006
Epoch 410, val loss: 0.9761377573013306
Epoch 420, training loss: 0.07945924997329712 = 0.07262285053730011 + 0.001 * 6.836398124694824
Epoch 420, val loss: 0.992680549621582
Epoch 430, training loss: 0.07201124727725983 = 0.06518124788999557 + 0.001 * 6.829996585845947
Epoch 430, val loss: 1.009102463722229
Epoch 440, training loss: 0.06542236357927322 = 0.05859944596886635 + 0.001 * 6.822915077209473
Epoch 440, val loss: 1.0251134634017944
Epoch 450, training loss: 0.05959073454141617 = 0.052776169031858444 + 0.001 * 6.814563751220703
Epoch 450, val loss: 1.040924310684204
Epoch 460, training loss: 0.05443325266242027 = 0.04762249439954758 + 0.001 * 6.810758590698242
Epoch 460, val loss: 1.0560810565948486
Epoch 470, training loss: 0.04987252503633499 = 0.04306521266698837 + 0.001 * 6.807310104370117
Epoch 470, val loss: 1.0709035396575928
Epoch 480, training loss: 0.045827098190784454 = 0.03902965411543846 + 0.001 * 6.797443389892578
Epoch 480, val loss: 1.0851637125015259
Epoch 490, training loss: 0.042247891426086426 = 0.035454366356134415 + 0.001 * 6.793524265289307
Epoch 490, val loss: 1.0989521741867065
Epoch 500, training loss: 0.0391191691160202 = 0.03228813782334328 + 0.001 * 6.83103084564209
Epoch 500, val loss: 1.1123054027557373
Epoch 510, training loss: 0.03628692030906677 = 0.029486212879419327 + 0.001 * 6.800708770751953
Epoch 510, val loss: 1.1251461505889893
Epoch 520, training loss: 0.03378632664680481 = 0.026998961344361305 + 0.001 * 6.787365913391113
Epoch 520, val loss: 1.13755202293396
Epoch 530, training loss: 0.03157292678952217 = 0.024786125868558884 + 0.001 * 6.786800861358643
Epoch 530, val loss: 1.1494518518447876
Epoch 540, training loss: 0.02959660440683365 = 0.022813567891716957 + 0.001 * 6.783036708831787
Epoch 540, val loss: 1.1610058546066284
Epoch 550, training loss: 0.027837108820676804 = 0.021052725613117218 + 0.001 * 6.7843828201293945
Epoch 550, val loss: 1.172040343284607
Epoch 560, training loss: 0.026253607124090195 = 0.019475387409329414 + 0.001 * 6.778218746185303
Epoch 560, val loss: 1.1828681230545044
Epoch 570, training loss: 0.024840187281370163 = 0.01805976778268814 + 0.001 * 6.780419826507568
Epoch 570, val loss: 1.1932451725006104
Epoch 580, training loss: 0.023562036454677582 = 0.016786007210612297 + 0.001 * 6.776028633117676
Epoch 580, val loss: 1.2032458782196045
Epoch 590, training loss: 0.022433249279856682 = 0.01563693955540657 + 0.001 * 6.796309471130371
Epoch 590, val loss: 1.2129079103469849
Epoch 600, training loss: 0.021379267796874046 = 0.014598307199776173 + 0.001 * 6.780959606170654
Epoch 600, val loss: 1.2223114967346191
Epoch 610, training loss: 0.020430434495210648 = 0.013655669055879116 + 0.001 * 6.7747650146484375
Epoch 610, val loss: 1.2313083410263062
Epoch 620, training loss: 0.019575372338294983 = 0.012795910239219666 + 0.001 * 6.77946138381958
Epoch 620, val loss: 1.2400338649749756
Epoch 630, training loss: 0.01878237910568714 = 0.01200816873461008 + 0.001 * 6.774209976196289
Epoch 630, val loss: 1.2484086751937866
Epoch 640, training loss: 0.018053535372018814 = 0.011284480802714825 + 0.001 * 6.769054412841797
Epoch 640, val loss: 1.2565776109695435
Epoch 650, training loss: 0.01739383302628994 = 0.010618786327540874 + 0.001 * 6.7750468254089355
Epoch 650, val loss: 1.264400601387024
Epoch 660, training loss: 0.01677217334508896 = 0.010006009601056576 + 0.001 * 6.766162872314453
Epoch 660, val loss: 1.272050380706787
Epoch 670, training loss: 0.016213085502386093 = 0.009441832080483437 + 0.001 * 6.771252632141113
Epoch 670, val loss: 1.2794044017791748
Epoch 680, training loss: 0.01568366214632988 = 0.008919701911509037 + 0.001 * 6.763959884643555
Epoch 680, val loss: 1.2864997386932373
Epoch 690, training loss: 0.015203740447759628 = 0.0084373839199543 + 0.001 * 6.766355514526367
Epoch 690, val loss: 1.2936382293701172
Epoch 700, training loss: 0.014756678603589535 = 0.00799200776964426 + 0.001 * 6.764670372009277
Epoch 700, val loss: 1.3003201484680176
Epoch 710, training loss: 0.014349434524774551 = 0.007580588571727276 + 0.001 * 6.768846035003662
Epoch 710, val loss: 1.306720495223999
Epoch 720, training loss: 0.013962466269731522 = 0.007199296727776527 + 0.001 * 6.763168811798096
Epoch 720, val loss: 1.3131355047225952
Epoch 730, training loss: 0.01360345259308815 = 0.006845151074230671 + 0.001 * 6.758301734924316
Epoch 730, val loss: 1.3192414045333862
Epoch 740, training loss: 0.013272076845169067 = 0.006516990251839161 + 0.001 * 6.7550859451293945
Epoch 740, val loss: 1.3251945972442627
Epoch 750, training loss: 0.01297597773373127 = 0.006211787927895784 + 0.001 * 6.76418924331665
Epoch 750, val loss: 1.331113338470459
Epoch 760, training loss: 0.012684892863035202 = 0.005927268881350756 + 0.001 * 6.757623195648193
Epoch 760, val loss: 1.33674156665802
Epoch 770, training loss: 0.012405303306877613 = 0.005662487354129553 + 0.001 * 6.7428154945373535
Epoch 770, val loss: 1.3421887159347534
Epoch 780, training loss: 0.012163568288087845 = 0.005416111089289188 + 0.001 * 6.747457027435303
Epoch 780, val loss: 1.3474864959716797
Epoch 790, training loss: 0.011941399425268173 = 0.0051867784932255745 + 0.001 * 6.75462007522583
Epoch 790, val loss: 1.352645754814148
Epoch 800, training loss: 0.011715115047991276 = 0.0049729663878679276 + 0.001 * 6.742148399353027
Epoch 800, val loss: 1.357672095298767
Epoch 810, training loss: 0.011535612866282463 = 0.00477330107241869 + 0.001 * 6.7623114585876465
Epoch 810, val loss: 1.3625892400741577
Epoch 820, training loss: 0.011332426220178604 = 0.004586757160723209 + 0.001 * 6.745669364929199
Epoch 820, val loss: 1.3673570156097412
Epoch 830, training loss: 0.011144561693072319 = 0.00441205408424139 + 0.001 * 6.73250675201416
Epoch 830, val loss: 1.3719853162765503
Epoch 840, training loss: 0.010989097878336906 = 0.0042481860145926476 + 0.001 * 6.74091100692749
Epoch 840, val loss: 1.3765512704849243
Epoch 850, training loss: 0.010834764689207077 = 0.0040943315252661705 + 0.001 * 6.740432262420654
Epoch 850, val loss: 1.380981683731079
Epoch 860, training loss: 0.010698627680540085 = 0.003949667792767286 + 0.001 * 6.748959541320801
Epoch 860, val loss: 1.385332465171814
Epoch 870, training loss: 0.010551043786108494 = 0.0038134483620524406 + 0.001 * 6.737595081329346
Epoch 870, val loss: 1.389556646347046
Epoch 880, training loss: 0.010474250651896 = 0.0036851412151008844 + 0.001 * 6.789109230041504
Epoch 880, val loss: 1.3936827182769775
Epoch 890, training loss: 0.010291999205946922 = 0.0035641721915453672 + 0.001 * 6.727827072143555
Epoch 890, val loss: 1.397735834121704
Epoch 900, training loss: 0.010185547173023224 = 0.0034499289467930794 + 0.001 * 6.735618591308594
Epoch 900, val loss: 1.4016799926757812
Epoch 910, training loss: 0.010074506513774395 = 0.0033419111277908087 + 0.001 * 6.732594966888428
Epoch 910, val loss: 1.4055240154266357
Epoch 920, training loss: 0.00997451413422823 = 0.003239674726501107 + 0.001 * 6.734838962554932
Epoch 920, val loss: 1.4092822074890137
Epoch 930, training loss: 0.009871574118733406 = 0.0031428043730556965 + 0.001 * 6.728769779205322
Epoch 930, val loss: 1.4129832983016968
Epoch 940, training loss: 0.009784124791622162 = 0.003050930565223098 + 0.001 * 6.733194351196289
Epoch 940, val loss: 1.4166063070297241
Epoch 950, training loss: 0.009695946238934994 = 0.0029637557454407215 + 0.001 * 6.732190132141113
Epoch 950, val loss: 1.4201476573944092
Epoch 960, training loss: 0.009627532213926315 = 0.002880923682823777 + 0.001 * 6.746608257293701
Epoch 960, val loss: 1.423590064048767
Epoch 970, training loss: 0.00952248927205801 = 0.002802180591970682 + 0.001 * 6.720308303833008
Epoch 970, val loss: 1.4270066022872925
Epoch 980, training loss: 0.009453101083636284 = 0.0027272452134639025 + 0.001 * 6.725855350494385
Epoch 980, val loss: 1.4303308725357056
Epoch 990, training loss: 0.009381797164678574 = 0.0026559142861515284 + 0.001 * 6.7258830070495605
Epoch 990, val loss: 1.4335615634918213
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.1661
Flip ASR: 0.1689/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9613490104675293 = 1.9529751539230347 + 0.001 * 8.373892784118652
Epoch 0, val loss: 1.9496583938598633
Epoch 10, training loss: 1.9511698484420776 = 1.942795991897583 + 0.001 * 8.373832702636719
Epoch 10, val loss: 1.9397307634353638
Epoch 20, training loss: 1.938288927078247 = 1.9299153089523315 + 0.001 * 8.373637199401855
Epoch 20, val loss: 1.9268710613250732
Epoch 30, training loss: 1.9199132919311523 = 1.911540150642395 + 0.001 * 8.373188018798828
Epoch 30, val loss: 1.9083799123764038
Epoch 40, training loss: 1.8923840522766113 = 1.88401198387146 + 0.001 * 8.37209415435791
Epoch 40, val loss: 1.8811020851135254
Epoch 50, training loss: 1.8532319068908691 = 1.8448631763458252 + 0.001 * 8.368685722351074
Epoch 50, val loss: 1.8442453145980835
Epoch 60, training loss: 1.80748450756073 = 1.7991297245025635 + 0.001 * 8.354832649230957
Epoch 60, val loss: 1.8053741455078125
Epoch 70, training loss: 1.7660490274429321 = 1.757772445678711 + 0.001 * 8.276586532592773
Epoch 70, val loss: 1.7730408906936646
Epoch 80, training loss: 1.716025948524475 = 1.7082109451293945 + 0.001 * 7.815046787261963
Epoch 80, val loss: 1.7299704551696777
Epoch 90, training loss: 1.647048830986023 = 1.63937246799469 + 0.001 * 7.6763200759887695
Epoch 90, val loss: 1.6714248657226562
Epoch 100, training loss: 1.558288335800171 = 1.550701379776001 + 0.001 * 7.586946964263916
Epoch 100, val loss: 1.5999118089675903
Epoch 110, training loss: 1.4570130109786987 = 1.449586033821106 + 0.001 * 7.426990509033203
Epoch 110, val loss: 1.5192641019821167
Epoch 120, training loss: 1.3533905744552612 = 1.3460978269577026 + 0.001 * 7.292691707611084
Epoch 120, val loss: 1.4377702474594116
Epoch 130, training loss: 1.2510871887207031 = 1.2438230514526367 + 0.001 * 7.26413631439209
Epoch 130, val loss: 1.3598912954330444
Epoch 140, training loss: 1.1509766578674316 = 1.143763780593872 + 0.001 * 7.212834358215332
Epoch 140, val loss: 1.2843351364135742
Epoch 150, training loss: 1.0546951293945312 = 1.047563076019287 + 0.001 * 7.132019996643066
Epoch 150, val loss: 1.2118486166000366
Epoch 160, training loss: 0.9640511274337769 = 0.9570242166519165 + 0.001 * 7.026923179626465
Epoch 160, val loss: 1.1437873840332031
Epoch 170, training loss: 0.8790776133537292 = 0.87211012840271 + 0.001 * 6.967472553253174
Epoch 170, val loss: 1.0812451839447021
Epoch 180, training loss: 0.7990906238555908 = 0.7921351194381714 + 0.001 * 6.955526828765869
Epoch 180, val loss: 1.0240623950958252
Epoch 190, training loss: 0.7239136695861816 = 0.7169692516326904 + 0.001 * 6.944426536560059
Epoch 190, val loss: 0.9727739095687866
Epoch 200, training loss: 0.6541436910629272 = 0.6472080945968628 + 0.001 * 6.935624122619629
Epoch 200, val loss: 0.9278975129127502
Epoch 210, training loss: 0.5902479887008667 = 0.5833262205123901 + 0.001 * 6.921738624572754
Epoch 210, val loss: 0.8899087309837341
Epoch 220, training loss: 0.5322296023368835 = 0.5253255367279053 + 0.001 * 6.904057025909424
Epoch 220, val loss: 0.8589047193527222
Epoch 230, training loss: 0.4798637926578522 = 0.47297316789627075 + 0.001 * 6.890615940093994
Epoch 230, val loss: 0.8346018195152283
Epoch 240, training loss: 0.43258780241012573 = 0.42571598291397095 + 0.001 * 6.871827125549316
Epoch 240, val loss: 0.8166493773460388
Epoch 250, training loss: 0.3896569311618805 = 0.3827938735485077 + 0.001 * 6.863052845001221
Epoch 250, val loss: 0.8045639395713806
Epoch 260, training loss: 0.3503301441669464 = 0.34347179532051086 + 0.001 * 6.858356952667236
Epoch 260, val loss: 0.7974534034729004
Epoch 270, training loss: 0.3140917420387268 = 0.3072403073310852 + 0.001 * 6.851425647735596
Epoch 270, val loss: 0.7944396138191223
Epoch 280, training loss: 0.28071799874305725 = 0.2738678753376007 + 0.001 * 6.8501105308532715
Epoch 280, val loss: 0.7950243949890137
Epoch 290, training loss: 0.25018513202667236 = 0.24333694577217102 + 0.001 * 6.848179340362549
Epoch 290, val loss: 0.798957347869873
Epoch 300, training loss: 0.22256946563720703 = 0.21572370827198029 + 0.001 * 6.8457512855529785
Epoch 300, val loss: 0.806110680103302
Epoch 310, training loss: 0.197894886136055 = 0.19105181097984314 + 0.001 * 6.843072891235352
Epoch 310, val loss: 0.816370964050293
Epoch 320, training loss: 0.17606785893440247 = 0.16922810673713684 + 0.001 * 6.839745998382568
Epoch 320, val loss: 0.8294693231582642
Epoch 330, training loss: 0.15690463781356812 = 0.1500660628080368 + 0.001 * 6.838577747344971
Epoch 330, val loss: 0.8451056480407715
Epoch 340, training loss: 0.14014750719070435 = 0.13331584632396698 + 0.001 * 6.831656455993652
Epoch 340, val loss: 0.8628697991371155
Epoch 350, training loss: 0.1255238652229309 = 0.11869622021913528 + 0.001 * 6.827645778656006
Epoch 350, val loss: 0.8823016285896301
Epoch 360, training loss: 0.11275535076856613 = 0.10593127459287643 + 0.001 * 6.824075222015381
Epoch 360, val loss: 0.9030710458755493
Epoch 370, training loss: 0.1015889048576355 = 0.09476911276578903 + 0.001 * 6.819788455963135
Epoch 370, val loss: 0.9247443079948425
Epoch 380, training loss: 0.09179884940385818 = 0.0849870815873146 + 0.001 * 6.811767101287842
Epoch 380, val loss: 0.9469695091247559
Epoch 390, training loss: 0.08320296555757523 = 0.07639611512422562 + 0.001 * 6.806848526000977
Epoch 390, val loss: 0.9694099426269531
Epoch 400, training loss: 0.07564444839954376 = 0.06883763521909714 + 0.001 * 6.806816101074219
Epoch 400, val loss: 0.991818368434906
Epoch 410, training loss: 0.06897399574518204 = 0.06217795982956886 + 0.001 * 6.7960357666015625
Epoch 410, val loss: 1.0140976905822754
Epoch 420, training loss: 0.06309051066637039 = 0.05630038306117058 + 0.001 * 6.790124893188477
Epoch 420, val loss: 1.0360567569732666
Epoch 430, training loss: 0.05790106952190399 = 0.051104284822940826 + 0.001 * 6.796783924102783
Epoch 430, val loss: 1.0575873851776123
Epoch 440, training loss: 0.05328144133090973 = 0.04650275409221649 + 0.001 * 6.778688430786133
Epoch 440, val loss: 1.0786187648773193
Epoch 450, training loss: 0.04919469356536865 = 0.04241981729865074 + 0.001 * 6.774875640869141
Epoch 450, val loss: 1.0991156101226807
Epoch 460, training loss: 0.04556092247366905 = 0.03878956660628319 + 0.001 * 6.77135705947876
Epoch 460, val loss: 1.1190614700317383
Epoch 470, training loss: 0.04232238605618477 = 0.0355546660721302 + 0.001 * 6.767720699310303
Epoch 470, val loss: 1.1384395360946655
Epoch 480, training loss: 0.03943125903606415 = 0.03266594931483269 + 0.001 * 6.7653093338012695
Epoch 480, val loss: 1.1572742462158203
Epoch 490, training loss: 0.03684491664171219 = 0.030081063508987427 + 0.001 * 6.763853073120117
Epoch 490, val loss: 1.17551851272583
Epoch 500, training loss: 0.03452468290925026 = 0.02776271291077137 + 0.001 * 6.7619709968566895
Epoch 500, val loss: 1.193238377571106
Epoch 510, training loss: 0.032458193600177765 = 0.025679441168904305 + 0.001 * 6.778752326965332
Epoch 510, val loss: 1.2104300260543823
Epoch 520, training loss: 0.030563678592443466 = 0.02380300499498844 + 0.001 * 6.760672569274902
Epoch 520, val loss: 1.2270923852920532
Epoch 530, training loss: 0.0288629662245512 = 0.022104894742369652 + 0.001 * 6.758070468902588
Epoch 530, val loss: 1.243198037147522
Epoch 540, training loss: 0.027321958914399147 = 0.020559849217534065 + 0.001 * 6.762109756469727
Epoch 540, val loss: 1.258762001991272
Epoch 550, training loss: 0.025910545140504837 = 0.01915018819272518 + 0.001 * 6.7603559494018555
Epoch 550, val loss: 1.2737770080566406
Epoch 560, training loss: 0.02461731992661953 = 0.017863139510154724 + 0.001 * 6.754179954528809
Epoch 560, val loss: 1.2882614135742188
Epoch 570, training loss: 0.02343946136534214 = 0.016688013449311256 + 0.001 * 6.751447677612305
Epoch 570, val loss: 1.3022433519363403
Epoch 580, training loss: 0.022365424782037735 = 0.015614373609423637 + 0.001 * 6.751049995422363
Epoch 580, val loss: 1.315705418586731
Epoch 590, training loss: 0.021382251754403114 = 0.014632610604166985 + 0.001 * 6.749640941619873
Epoch 590, val loss: 1.3286950588226318
Epoch 600, training loss: 0.020482921972870827 = 0.013732483610510826 + 0.001 * 6.7504377365112305
Epoch 600, val loss: 1.3412110805511475
Epoch 610, training loss: 0.019670827314257622 = 0.012902684509754181 + 0.001 * 6.768143177032471
Epoch 610, val loss: 1.353222370147705
Epoch 620, training loss: 0.018881985917687416 = 0.01213502511382103 + 0.001 * 6.746960163116455
Epoch 620, val loss: 1.364793300628662
Epoch 630, training loss: 0.018168946728110313 = 0.011424237862229347 + 0.001 * 6.74470853805542
Epoch 630, val loss: 1.375881314277649
Epoch 640, training loss: 0.017508555203676224 = 0.010766508989036083 + 0.001 * 6.7420454025268555
Epoch 640, val loss: 1.386549949645996
Epoch 650, training loss: 0.016912590712308884 = 0.010158305987715721 + 0.001 * 6.754284858703613
Epoch 650, val loss: 1.396798014640808
Epoch 660, training loss: 0.016341201961040497 = 0.0095962630584836 + 0.001 * 6.744937419891357
Epoch 660, val loss: 1.4066493511199951
Epoch 670, training loss: 0.015823131427168846 = 0.009076821617782116 + 0.001 * 6.74630880355835
Epoch 670, val loss: 1.41616952419281
Epoch 680, training loss: 0.015344999730587006 = 0.008596655912697315 + 0.001 * 6.748342990875244
Epoch 680, val loss: 1.425334095954895
Epoch 690, training loss: 0.014887947589159012 = 0.008152516558766365 + 0.001 * 6.73543119430542
Epoch 690, val loss: 1.4341708421707153
Epoch 700, training loss: 0.01447670254856348 = 0.007741374429315329 + 0.001 * 6.73532772064209
Epoch 700, val loss: 1.4427030086517334
Epoch 710, training loss: 0.014100520871579647 = 0.0073603494092822075 + 0.001 * 6.740170955657959
Epoch 710, val loss: 1.4509568214416504
Epoch 720, training loss: 0.013744868338108063 = 0.00700668478384614 + 0.001 * 6.738183498382568
Epoch 720, val loss: 1.4589704275131226
Epoch 730, training loss: 0.013407640159130096 = 0.006677466910332441 + 0.001 * 6.730173587799072
Epoch 730, val loss: 1.466719150543213
Epoch 740, training loss: 0.013099472969770432 = 0.006369627546519041 + 0.001 * 6.72984504699707
Epoch 740, val loss: 1.4742281436920166
Epoch 750, training loss: 0.012819012627005577 = 0.006080673076212406 + 0.001 * 6.738339424133301
Epoch 750, val loss: 1.4815363883972168
Epoch 760, training loss: 0.012536613270640373 = 0.005808827001601458 + 0.001 * 6.727786064147949
Epoch 760, val loss: 1.4886285066604614
Epoch 770, training loss: 0.012276856228709221 = 0.005552656017243862 + 0.001 * 6.724199295043945
Epoch 770, val loss: 1.4955716133117676
Epoch 780, training loss: 0.012034857645630836 = 0.005311258137226105 + 0.001 * 6.723599433898926
Epoch 780, val loss: 1.5023053884506226
Epoch 790, training loss: 0.011808877810835838 = 0.005083962809294462 + 0.001 * 6.724914073944092
Epoch 790, val loss: 1.50887930393219
Epoch 800, training loss: 0.011599062010645866 = 0.004870143253356218 + 0.001 * 6.728918552398682
Epoch 800, val loss: 1.5152440071105957
Epoch 810, training loss: 0.011392928659915924 = 0.0046690343879163265 + 0.001 * 6.7238945960998535
Epoch 810, val loss: 1.5214680433273315
Epoch 820, training loss: 0.011198019608855247 = 0.00447993166744709 + 0.001 * 6.718088150024414
Epoch 820, val loss: 1.527604579925537
Epoch 830, training loss: 0.011024078354239464 = 0.004302168730646372 + 0.001 * 6.721909046173096
Epoch 830, val loss: 1.5335149765014648
Epoch 840, training loss: 0.010859915986657143 = 0.004135127179324627 + 0.001 * 6.724789142608643
Epoch 840, val loss: 1.5393279790878296
Epoch 850, training loss: 0.010707557201385498 = 0.003978108987212181 + 0.001 * 6.729447364807129
Epoch 850, val loss: 1.5449917316436768
Epoch 860, training loss: 0.010545440018177032 = 0.003830438945442438 + 0.001 * 6.715000629425049
Epoch 860, val loss: 1.5505266189575195
Epoch 870, training loss: 0.010401951149106026 = 0.0036913605872541666 + 0.001 * 6.71058988571167
Epoch 870, val loss: 1.55598783493042
Epoch 880, training loss: 0.010285132564604282 = 0.0035602685529738665 + 0.001 * 6.724863529205322
Epoch 880, val loss: 1.5612977743148804
Epoch 890, training loss: 0.010149766691029072 = 0.0034366401378065348 + 0.001 * 6.7131266593933105
Epoch 890, val loss: 1.5664918422698975
Epoch 900, training loss: 0.010031120851635933 = 0.0033199572935700417 + 0.001 * 6.711163520812988
Epoch 900, val loss: 1.5715831518173218
Epoch 910, training loss: 0.00994027592241764 = 0.0032097301445901394 + 0.001 * 6.730545520782471
Epoch 910, val loss: 1.5765444040298462
Epoch 920, training loss: 0.009813136421144009 = 0.003105513984337449 + 0.001 * 6.707622528076172
Epoch 920, val loss: 1.5814194679260254
Epoch 930, training loss: 0.00970753375440836 = 0.0030068878550082445 + 0.001 * 6.700645446777344
Epoch 930, val loss: 1.5862011909484863
Epoch 940, training loss: 0.009648839011788368 = 0.0029134578071534634 + 0.001 * 6.735381126403809
Epoch 940, val loss: 1.590872049331665
Epoch 950, training loss: 0.009530235081911087 = 0.0028249030001461506 + 0.001 * 6.705332279205322
Epoch 950, val loss: 1.5954300165176392
Epoch 960, training loss: 0.00944357831031084 = 0.002740843454375863 + 0.001 * 6.702734470367432
Epoch 960, val loss: 1.5998963117599487
Epoch 970, training loss: 0.009378520771861076 = 0.0026609778869897127 + 0.001 * 6.7175421714782715
Epoch 970, val loss: 1.604270339012146
Epoch 980, training loss: 0.009294123388826847 = 0.0025850485544651747 + 0.001 * 6.709074974060059
Epoch 980, val loss: 1.6085582971572876
Epoch 990, training loss: 0.009208614937961102 = 0.002512809121981263 + 0.001 * 6.695805549621582
Epoch 990, val loss: 1.6127293109893799
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7528
Flip ASR: 0.7022/225 nodes
The final ASR:0.46371, 0.23960, Accuracy:0.80000, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11600])
remove edge: torch.Size([2, 9452])
updated graph: torch.Size([2, 10496])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83580, 0.00924
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9521042108535767 = 1.9437302350997925 + 0.001 * 8.373926162719727
Epoch 0, val loss: 1.934682846069336
Epoch 10, training loss: 1.9415345191955566 = 1.933160662651062 + 0.001 * 8.373875617980957
Epoch 10, val loss: 1.9245944023132324
Epoch 20, training loss: 1.9285022020339966 = 1.9201284646987915 + 0.001 * 8.373706817626953
Epoch 20, val loss: 1.9115396738052368
Epoch 30, training loss: 1.9102989435195923 = 1.9019256830215454 + 0.001 * 8.373303413391113
Epoch 30, val loss: 1.8928064107894897
Epoch 40, training loss: 1.8836802244186401 = 1.8753079175949097 + 0.001 * 8.372361183166504
Epoch 40, val loss: 1.8654513359069824
Epoch 50, training loss: 1.8469308614730835 = 1.8385610580444336 + 0.001 * 8.369763374328613
Epoch 50, val loss: 1.8296006917953491
Epoch 60, training loss: 1.8048959970474243 = 1.796535611152649 + 0.001 * 8.360328674316406
Epoch 60, val loss: 1.7937848567962646
Epoch 70, training loss: 1.7643853425979614 = 1.756070613861084 + 0.001 * 8.314738273620605
Epoch 70, val loss: 1.7641398906707764
Epoch 80, training loss: 1.7115017175674438 = 1.7034733295440674 + 0.001 * 8.028390884399414
Epoch 80, val loss: 1.722164273262024
Epoch 90, training loss: 1.6383146047592163 = 1.630538821220398 + 0.001 * 7.775732040405273
Epoch 90, val loss: 1.6610662937164307
Epoch 100, training loss: 1.548062801361084 = 1.5403404235839844 + 0.001 * 7.7223801612854
Epoch 100, val loss: 1.5876742601394653
Epoch 110, training loss: 1.4516574144363403 = 1.4439607858657837 + 0.001 * 7.6966552734375
Epoch 110, val loss: 1.5103901624679565
Epoch 120, training loss: 1.356701374053955 = 1.3490402698516846 + 0.001 * 7.66111946105957
Epoch 120, val loss: 1.437122106552124
Epoch 130, training loss: 1.2644731998443604 = 1.2569211721420288 + 0.001 * 7.552017688751221
Epoch 130, val loss: 1.3686660528182983
Epoch 140, training loss: 1.1746506690979004 = 1.167330026626587 + 0.001 * 7.320615291595459
Epoch 140, val loss: 1.3053710460662842
Epoch 150, training loss: 1.0863847732543945 = 1.079192042350769 + 0.001 * 7.19275426864624
Epoch 150, val loss: 1.2447409629821777
Epoch 160, training loss: 0.9987949728965759 = 0.9916728138923645 + 0.001 * 7.122147083282471
Epoch 160, val loss: 1.1848368644714355
Epoch 170, training loss: 0.9129366278648376 = 0.905853271484375 + 0.001 * 7.08333683013916
Epoch 170, val loss: 1.1252212524414062
Epoch 180, training loss: 0.8319825530052185 = 0.824907660484314 + 0.001 * 7.074903964996338
Epoch 180, val loss: 1.06842041015625
Epoch 190, training loss: 0.7590045928955078 = 0.7519373893737793 + 0.001 * 7.0672287940979
Epoch 190, val loss: 1.017004132270813
Epoch 200, training loss: 0.6947483420372009 = 0.6876899003982544 + 0.001 * 7.058426856994629
Epoch 200, val loss: 0.9727518558502197
Epoch 210, training loss: 0.6373851299285889 = 0.6303333640098572 + 0.001 * 7.051740646362305
Epoch 210, val loss: 0.934506356716156
Epoch 220, training loss: 0.5838769674301147 = 0.5768306851387024 + 0.001 * 7.046280860900879
Epoch 220, val loss: 0.9006661772727966
Epoch 230, training loss: 0.5316847562789917 = 0.5246434807777405 + 0.001 * 7.041255950927734
Epoch 230, val loss: 0.870600700378418
Epoch 240, training loss: 0.4796753227710724 = 0.47263848781585693 + 0.001 * 7.036835193634033
Epoch 240, val loss: 0.8439986705780029
Epoch 250, training loss: 0.4281597137451172 = 0.4211266338825226 + 0.001 * 7.033076286315918
Epoch 250, val loss: 0.8219225406646729
Epoch 260, training loss: 0.3785538673400879 = 0.3715238571166992 + 0.001 * 7.030008792877197
Epoch 260, val loss: 0.8048480153083801
Epoch 270, training loss: 0.3323463499546051 = 0.3253178298473358 + 0.001 * 7.028522491455078
Epoch 270, val loss: 0.7931073904037476
Epoch 280, training loss: 0.29068419337272644 = 0.28365713357925415 + 0.001 * 7.02705192565918
Epoch 280, val loss: 0.7868258953094482
Epoch 290, training loss: 0.25408995151519775 = 0.24706478416919708 + 0.001 * 7.025157451629639
Epoch 290, val loss: 0.785981297492981
Epoch 300, training loss: 0.22253061830997467 = 0.21550780534744263 + 0.001 * 7.0228166580200195
Epoch 300, val loss: 0.790471076965332
Epoch 310, training loss: 0.19559332728385925 = 0.18857300281524658 + 0.001 * 7.020323276519775
Epoch 310, val loss: 0.7997404336929321
Epoch 320, training loss: 0.17272865772247314 = 0.16570989787578583 + 0.001 * 7.018754482269287
Epoch 320, val loss: 0.8130518198013306
Epoch 330, training loss: 0.15330134332180023 = 0.14629243314266205 + 0.001 * 7.008906364440918
Epoch 330, val loss: 0.8295361399650574
Epoch 340, training loss: 0.1367245763540268 = 0.12972712516784668 + 0.001 * 6.997450828552246
Epoch 340, val loss: 0.8483253121376038
Epoch 350, training loss: 0.12249427288770676 = 0.1155090257525444 + 0.001 * 6.985247611999512
Epoch 350, val loss: 0.8687499165534973
Epoch 360, training loss: 0.11020113527774811 = 0.10323268175125122 + 0.001 * 6.9684553146362305
Epoch 360, val loss: 0.890225887298584
Epoch 370, training loss: 0.09953252971172333 = 0.09257045388221741 + 0.001 * 6.96207332611084
Epoch 370, val loss: 0.9123523831367493
Epoch 380, training loss: 0.09021427482366562 = 0.08325773477554321 + 0.001 * 6.956539630889893
Epoch 380, val loss: 0.9347917437553406
Epoch 390, training loss: 0.08201730251312256 = 0.07508711516857147 + 0.001 * 6.930187702178955
Epoch 390, val loss: 0.9572795629501343
Epoch 400, training loss: 0.07481023669242859 = 0.06788525730371475 + 0.001 * 6.924975872039795
Epoch 400, val loss: 0.9797097444534302
Epoch 410, training loss: 0.06842556595802307 = 0.061506468802690506 + 0.001 * 6.919100284576416
Epoch 410, val loss: 1.0019748210906982
Epoch 420, training loss: 0.0627380982041359 = 0.05583201348781586 + 0.001 * 6.906086444854736
Epoch 420, val loss: 1.0239516496658325
Epoch 430, training loss: 0.05766856670379639 = 0.05077124759554863 + 0.001 * 6.897316932678223
Epoch 430, val loss: 1.0455869436264038
Epoch 440, training loss: 0.05314270406961441 = 0.04624997451901436 + 0.001 * 6.892727851867676
Epoch 440, val loss: 1.0667766332626343
Epoch 450, training loss: 0.04909111559391022 = 0.042204681783914566 + 0.001 * 6.886431694030762
Epoch 450, val loss: 1.0875359773635864
Epoch 460, training loss: 0.045462459325790405 = 0.03858204931020737 + 0.001 * 6.880409240722656
Epoch 460, val loss: 1.1078801155090332
Epoch 470, training loss: 0.04221757501363754 = 0.03533484786748886 + 0.001 * 6.882725715637207
Epoch 470, val loss: 1.1276962757110596
Epoch 480, training loss: 0.03929593786597252 = 0.03242218494415283 + 0.001 * 6.873752593994141
Epoch 480, val loss: 1.1470204591751099
Epoch 490, training loss: 0.036675550043582916 = 0.02980736643075943 + 0.001 * 6.868182182312012
Epoch 490, val loss: 1.1657651662826538
Epoch 500, training loss: 0.034319180995225906 = 0.02745812200009823 + 0.001 * 6.861058235168457
Epoch 500, val loss: 1.1840393543243408
Epoch 510, training loss: 0.03220974653959274 = 0.02534453384578228 + 0.001 * 6.865213394165039
Epoch 510, val loss: 1.2017418146133423
Epoch 520, training loss: 0.030300261452794075 = 0.02344151958823204 + 0.001 * 6.8587422370910645
Epoch 520, val loss: 1.2189058065414429
Epoch 530, training loss: 0.028580043464899063 = 0.021725835278630257 + 0.001 * 6.8542070388793945
Epoch 530, val loss: 1.2354944944381714
Epoch 540, training loss: 0.027031725272536278 = 0.02017681859433651 + 0.001 * 6.85490608215332
Epoch 540, val loss: 1.2515374422073364
Epoch 550, training loss: 0.025620680302381516 = 0.01877620443701744 + 0.001 * 6.844475269317627
Epoch 550, val loss: 1.2670385837554932
Epoch 560, training loss: 0.024355599656701088 = 0.01750783808529377 + 0.001 * 6.847761631011963
Epoch 560, val loss: 1.2819883823394775
Epoch 570, training loss: 0.02321142889559269 = 0.016357239335775375 + 0.001 * 6.854189395904541
Epoch 570, val loss: 1.2964054346084595
Epoch 580, training loss: 0.022147536277770996 = 0.01531150471419096 + 0.001 * 6.836030960083008
Epoch 580, val loss: 1.3103642463684082
Epoch 590, training loss: 0.02119838446378708 = 0.014359533786773682 + 0.001 * 6.8388495445251465
Epoch 590, val loss: 1.3238111734390259
Epoch 600, training loss: 0.020357875153422356 = 0.013491217978298664 + 0.001 * 6.866657257080078
Epoch 600, val loss: 1.3367801904678345
Epoch 610, training loss: 0.01953670009970665 = 0.012697722762823105 + 0.001 * 6.8389763832092285
Epoch 610, val loss: 1.3492780923843384
Epoch 620, training loss: 0.018804538995027542 = 0.011971130035817623 + 0.001 * 6.833408355712891
Epoch 620, val loss: 1.3613266944885254
Epoch 630, training loss: 0.01812906190752983 = 0.01130445022135973 + 0.001 * 6.824610710144043
Epoch 630, val loss: 1.3729223012924194
Epoch 640, training loss: 0.01751869171857834 = 0.010691771283745766 + 0.001 * 6.8269195556640625
Epoch 640, val loss: 1.3841462135314941
Epoch 650, training loss: 0.01695077307522297 = 0.010127625428140163 + 0.001 * 6.823147773742676
Epoch 650, val loss: 1.394970417022705
Epoch 660, training loss: 0.016426416113972664 = 0.009607271291315556 + 0.001 * 6.819144248962402
Epoch 660, val loss: 1.405394434928894
Epoch 670, training loss: 0.01595352776348591 = 0.009126396849751472 + 0.001 * 6.8271307945251465
Epoch 670, val loss: 1.4154603481292725
Epoch 680, training loss: 0.015511834993958473 = 0.008681290782988071 + 0.001 * 6.830543518066406
Epoch 680, val loss: 1.4251956939697266
Epoch 690, training loss: 0.015082672238349915 = 0.00826866365969181 + 0.001 * 6.8140082359313965
Epoch 690, val loss: 1.4346171617507935
Epoch 700, training loss: 0.014705561101436615 = 0.00788557343184948 + 0.001 * 6.819986820220947
Epoch 700, val loss: 1.4437000751495361
Epoch 710, training loss: 0.0143516194075346 = 0.007529284805059433 + 0.001 * 6.822333812713623
Epoch 710, val loss: 1.4524931907653809
Epoch 720, training loss: 0.014011524617671967 = 0.007197557482868433 + 0.001 * 6.813966751098633
Epoch 720, val loss: 1.4610254764556885
Epoch 730, training loss: 0.013720178045332432 = 0.0068880715407431126 + 0.001 * 6.832106113433838
Epoch 730, val loss: 1.4692600965499878
Epoch 740, training loss: 0.013400515541434288 = 0.006598996464163065 + 0.001 * 6.801518440246582
Epoch 740, val loss: 1.4772148132324219
Epoch 750, training loss: 0.013148967176675797 = 0.006328546907752752 + 0.001 * 6.820420265197754
Epoch 750, val loss: 1.4849271774291992
Epoch 760, training loss: 0.01288636215031147 = 0.00607521366328001 + 0.001 * 6.811148643493652
Epoch 760, val loss: 1.4924261569976807
Epoch 770, training loss: 0.012636527419090271 = 0.00583758857101202 + 0.001 * 6.798938274383545
Epoch 770, val loss: 1.4996414184570312
Epoch 780, training loss: 0.012411842122673988 = 0.005614436697214842 + 0.001 * 6.79740571975708
Epoch 780, val loss: 1.5066560506820679
Epoch 790, training loss: 0.012199092656373978 = 0.005404645577073097 + 0.001 * 6.79444694519043
Epoch 790, val loss: 1.5134698152542114
Epoch 800, training loss: 0.012003734707832336 = 0.005207149311900139 + 0.001 * 6.796584606170654
Epoch 800, val loss: 1.5200471878051758
Epoch 810, training loss: 0.011816502548754215 = 0.005021029617637396 + 0.001 * 6.795472621917725
Epoch 810, val loss: 1.526452660560608
Epoch 820, training loss: 0.01163630560040474 = 0.0048454455099999905 + 0.001 * 6.790859699249268
Epoch 820, val loss: 1.532627820968628
Epoch 830, training loss: 0.01147763803601265 = 0.004679648205637932 + 0.001 * 6.7979888916015625
Epoch 830, val loss: 1.5386581420898438
Epoch 840, training loss: 0.011309676803648472 = 0.0045229108072817326 + 0.001 * 6.7867655754089355
Epoch 840, val loss: 1.5445144176483154
Epoch 850, training loss: 0.01115993782877922 = 0.004374583717435598 + 0.001 * 6.785353183746338
Epoch 850, val loss: 1.5501550436019897
Epoch 860, training loss: 0.011032111942768097 = 0.004234105348587036 + 0.001 * 6.798006534576416
Epoch 860, val loss: 1.5556681156158447
Epoch 870, training loss: 0.010884524323046207 = 0.004100920632481575 + 0.001 * 6.783603191375732
Epoch 870, val loss: 1.5610183477401733
Epoch 880, training loss: 0.010765315964818 = 0.003974548075348139 + 0.001 * 6.790767669677734
Epoch 880, val loss: 1.5661948919296265
Epoch 890, training loss: 0.01063506118953228 = 0.0038545301649719477 + 0.001 * 6.7805304527282715
Epoch 890, val loss: 1.5712546110153198
Epoch 900, training loss: 0.010529864579439163 = 0.0037403940223157406 + 0.001 * 6.789470195770264
Epoch 900, val loss: 1.5761196613311768
Epoch 910, training loss: 0.010412732139229774 = 0.0036318418569862843 + 0.001 * 6.780889987945557
Epoch 910, val loss: 1.5808875560760498
Epoch 920, training loss: 0.010312769562005997 = 0.003528510220348835 + 0.001 * 6.7842583656311035
Epoch 920, val loss: 1.58552086353302
Epoch 930, training loss: 0.010236093774437904 = 0.0034300456754863262 + 0.001 * 6.8060479164123535
Epoch 930, val loss: 1.5900121927261353
Epoch 940, training loss: 0.010099286213517189 = 0.0033361781388521194 + 0.001 * 6.7631072998046875
Epoch 940, val loss: 1.5943819284439087
Epoch 950, training loss: 0.010022935457527637 = 0.0032466065604239702 + 0.001 * 6.7763285636901855
Epoch 950, val loss: 1.5986404418945312
Epoch 960, training loss: 0.00991802103817463 = 0.003161086468026042 + 0.001 * 6.756934642791748
Epoch 960, val loss: 1.6028060913085938
Epoch 970, training loss: 0.00984796229749918 = 0.003079374087974429 + 0.001 * 6.768587589263916
Epoch 970, val loss: 1.6067742109298706
Epoch 980, training loss: 0.009781424887478352 = 0.0030012568458914757 + 0.001 * 6.780167579650879
Epoch 980, val loss: 1.6107243299484253
Epoch 990, training loss: 0.00969304982572794 = 0.0029265230987221003 + 0.001 * 6.766526699066162
Epoch 990, val loss: 1.6145410537719727
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.8303
Flip ASR: 0.7956/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9678144454956055 = 1.9594404697418213 + 0.001 * 8.37394905090332
Epoch 0, val loss: 1.966416835784912
Epoch 10, training loss: 1.957385540008545 = 1.9490116834640503 + 0.001 * 8.373908042907715
Epoch 10, val loss: 1.9560521841049194
Epoch 20, training loss: 1.944905400276184 = 1.936531662940979 + 0.001 * 8.373756408691406
Epoch 20, val loss: 1.9429782629013062
Epoch 30, training loss: 1.927871584892273 = 1.9194982051849365 + 0.001 * 8.373385429382324
Epoch 30, val loss: 1.9245928525924683
Epoch 40, training loss: 1.9031379222869873 = 1.8947654962539673 + 0.001 * 8.372456550598145
Epoch 40, val loss: 1.897847294807434
Epoch 50, training loss: 1.8676204681396484 = 1.859250545501709 + 0.001 * 8.369882583618164
Epoch 50, val loss: 1.8601475954055786
Epoch 60, training loss: 1.8225258588790894 = 1.8141649961471558 + 0.001 * 8.360857963562012
Epoch 60, val loss: 1.8145992755889893
Epoch 70, training loss: 1.7772542238235474 = 1.7689316272735596 + 0.001 * 8.322559356689453
Epoch 70, val loss: 1.7723788022994995
Epoch 80, training loss: 1.7305493354797363 = 1.7224434614181519 + 0.001 * 8.105817794799805
Epoch 80, val loss: 1.7306251525878906
Epoch 90, training loss: 1.6668637990951538 = 1.6590439081192017 + 0.001 * 7.8199028968811035
Epoch 90, val loss: 1.6751493215560913
Epoch 100, training loss: 1.5818122625350952 = 1.574214220046997 + 0.001 * 7.597995758056641
Epoch 100, val loss: 1.6022391319274902
Epoch 110, training loss: 1.4751365184783936 = 1.4677908420562744 + 0.001 * 7.345677852630615
Epoch 110, val loss: 1.5130219459533691
Epoch 120, training loss: 1.3547426462173462 = 1.3474777936935425 + 0.001 * 7.2648820877075195
Epoch 120, val loss: 1.4134190082550049
Epoch 130, training loss: 1.2289679050445557 = 1.2217270135879517 + 0.001 * 7.240866184234619
Epoch 130, val loss: 1.3119235038757324
Epoch 140, training loss: 1.1054127216339111 = 1.0982083082199097 + 0.001 * 7.204431533813477
Epoch 140, val loss: 1.2150323390960693
Epoch 150, training loss: 0.991136908531189 = 0.983970582485199 + 0.001 * 7.166330814361572
Epoch 150, val loss: 1.1284449100494385
Epoch 160, training loss: 0.8909204006195068 = 0.8837874531745911 + 0.001 * 7.132956504821777
Epoch 160, val loss: 1.05507493019104
Epoch 170, training loss: 0.8058018088340759 = 0.7986960411071777 + 0.001 * 7.105788707733154
Epoch 170, val loss: 0.9954470992088318
Epoch 180, training loss: 0.7338859438896179 = 0.7268053889274597 + 0.001 * 7.080528259277344
Epoch 180, val loss: 0.9480878710746765
Epoch 190, training loss: 0.6719302535057068 = 0.6648746728897095 + 0.001 * 7.055560111999512
Epoch 190, val loss: 0.9105861186981201
Epoch 200, training loss: 0.6164770126342773 = 0.6094483733177185 + 0.001 * 7.028658390045166
Epoch 200, val loss: 0.8796969652175903
Epoch 210, training loss: 0.5648267865180969 = 0.5578245520591736 + 0.001 * 7.002248764038086
Epoch 210, val loss: 0.853150486946106
Epoch 220, training loss: 0.5156656503677368 = 0.5086838603019714 + 0.001 * 6.9817891120910645
Epoch 220, val loss: 0.8303177952766418
Epoch 230, training loss: 0.46888497471809387 = 0.4619154930114746 + 0.001 * 6.9694952964782715
Epoch 230, val loss: 0.811768651008606
Epoch 240, training loss: 0.4246649742126465 = 0.41770127415657043 + 0.001 * 6.963706016540527
Epoch 240, val loss: 0.7974662780761719
Epoch 250, training loss: 0.38310229778289795 = 0.3761439621448517 + 0.001 * 6.958344459533691
Epoch 250, val loss: 0.7865946888923645
Epoch 260, training loss: 0.3443637490272522 = 0.3374090790748596 + 0.001 * 6.9546685218811035
Epoch 260, val loss: 0.7786516547203064
Epoch 270, training loss: 0.30859607458114624 = 0.30164530873298645 + 0.001 * 6.950770378112793
Epoch 270, val loss: 0.773604154586792
Epoch 280, training loss: 0.2760251462459564 = 0.2690776586532593 + 0.001 * 6.9474945068359375
Epoch 280, val loss: 0.7710553407669067
Epoch 290, training loss: 0.24689985811710358 = 0.23995546996593475 + 0.001 * 6.944386005401611
Epoch 290, val loss: 0.770611584186554
Epoch 300, training loss: 0.22120201587677002 = 0.2142595499753952 + 0.001 * 6.942471504211426
Epoch 300, val loss: 0.7731621265411377
Epoch 310, training loss: 0.1987222135066986 = 0.19178122282028198 + 0.001 * 6.940997123718262
Epoch 310, val loss: 0.7779691219329834
Epoch 320, training loss: 0.17911554872989655 = 0.17217595875263214 + 0.001 * 6.939586639404297
Epoch 320, val loss: 0.7848607897758484
Epoch 330, training loss: 0.16194497048854828 = 0.15500451624393463 + 0.001 * 6.940458297729492
Epoch 330, val loss: 0.7935076355934143
Epoch 340, training loss: 0.14680981636047363 = 0.1398722380399704 + 0.001 * 6.93758487701416
Epoch 340, val loss: 0.8033450245857239
Epoch 350, training loss: 0.13341963291168213 = 0.12648388743400574 + 0.001 * 6.935739040374756
Epoch 350, val loss: 0.8141300678253174
Epoch 360, training loss: 0.12154066562652588 = 0.1145935207605362 + 0.001 * 6.9471435546875
Epoch 360, val loss: 0.8255798816680908
Epoch 370, training loss: 0.11096089333295822 = 0.10402556508779526 + 0.001 * 6.935324668884277
Epoch 370, val loss: 0.8373790383338928
Epoch 380, training loss: 0.10151543468236923 = 0.09458345174789429 + 0.001 * 6.931983947753906
Epoch 380, val loss: 0.8493707776069641
Epoch 390, training loss: 0.09303698688745499 = 0.08610803633928299 + 0.001 * 6.928948879241943
Epoch 390, val loss: 0.8615262508392334
Epoch 400, training loss: 0.08541841804981232 = 0.07848076522350311 + 0.001 * 6.937649250030518
Epoch 400, val loss: 0.8737519979476929
Epoch 410, training loss: 0.07853494584560394 = 0.07160348445177078 + 0.001 * 6.931460380554199
Epoch 410, val loss: 0.8859136700630188
Epoch 420, training loss: 0.07230500131845474 = 0.06538090854883194 + 0.001 * 6.924091815948486
Epoch 420, val loss: 0.8979620337486267
Epoch 430, training loss: 0.0666869506239891 = 0.05976754426956177 + 0.001 * 6.919407844543457
Epoch 430, val loss: 0.9100573062896729
Epoch 440, training loss: 0.06159578263759613 = 0.05467581748962402 + 0.001 * 6.919963836669922
Epoch 440, val loss: 0.9220330119132996
Epoch 450, training loss: 0.05691530928015709 = 0.05000072345137596 + 0.001 * 6.914585113525391
Epoch 450, val loss: 0.9340603351593018
Epoch 460, training loss: 0.0526355616748333 = 0.04572008177638054 + 0.001 * 6.915480613708496
Epoch 460, val loss: 0.9460276961326599
Epoch 470, training loss: 0.0487200990319252 = 0.0418076366186142 + 0.001 * 6.912463188171387
Epoch 470, val loss: 0.9581833481788635
Epoch 480, training loss: 0.0451415479183197 = 0.03823263943195343 + 0.001 * 6.908909797668457
Epoch 480, val loss: 0.9699893593788147
Epoch 490, training loss: 0.04185574874281883 = 0.03495355322957039 + 0.001 * 6.902195930480957
Epoch 490, val loss: 0.981938362121582
Epoch 500, training loss: 0.038800809532403946 = 0.031900953501462936 + 0.001 * 6.899857044219971
Epoch 500, val loss: 0.9931715130805969
Epoch 510, training loss: 0.03599183261394501 = 0.029094450175762177 + 0.001 * 6.897382736206055
Epoch 510, val loss: 1.0049575567245483
Epoch 520, training loss: 0.03341793641448021 = 0.026530778035521507 + 0.001 * 6.88715934753418
Epoch 520, val loss: 1.0163531303405762
Epoch 530, training loss: 0.031115643680095673 = 0.02421402931213379 + 0.001 * 6.901614189147949
Epoch 530, val loss: 1.0273045301437378
Epoch 540, training loss: 0.028929051011800766 = 0.02204768918454647 + 0.001 * 6.88136100769043
Epoch 540, val loss: 1.0386505126953125
Epoch 550, training loss: 0.026954958215355873 = 0.02007574401795864 + 0.001 * 6.879213333129883
Epoch 550, val loss: 1.0492216348648071
Epoch 560, training loss: 0.025239840149879456 = 0.018356023356318474 + 0.001 * 6.883816242218018
Epoch 560, val loss: 1.059670329093933
Epoch 570, training loss: 0.023746095597743988 = 0.016869734972715378 + 0.001 * 6.876359939575195
Epoch 570, val loss: 1.070104956626892
Epoch 580, training loss: 0.022458331659436226 = 0.015580181032419205 + 0.001 * 6.878150939941406
Epoch 580, val loss: 1.0800917148590088
Epoch 590, training loss: 0.021316440775990486 = 0.014444009400904179 + 0.001 * 6.872430801391602
Epoch 590, val loss: 1.0896879434585571
Epoch 600, training loss: 0.020278282463550568 = 0.013418437913060188 + 0.001 * 6.859843730926514
Epoch 600, val loss: 1.0988689661026
Epoch 610, training loss: 0.019367320463061333 = 0.012496492825448513 + 0.001 * 6.870826721191406
Epoch 610, val loss: 1.107704520225525
Epoch 620, training loss: 0.018528055399656296 = 0.011670724488794804 + 0.001 * 6.857329845428467
Epoch 620, val loss: 1.1161043643951416
Epoch 630, training loss: 0.017784785479307175 = 0.010924646630883217 + 0.001 * 6.860138893127441
Epoch 630, val loss: 1.124291181564331
Epoch 640, training loss: 0.01710335910320282 = 0.010245946235954762 + 0.001 * 6.857412815093994
Epoch 640, val loss: 1.1322673559188843
Epoch 650, training loss: 0.016464516520500183 = 0.009614129550755024 + 0.001 * 6.850386142730713
Epoch 650, val loss: 1.1399765014648438
Epoch 660, training loss: 0.015903346240520477 = 0.009036161005496979 + 0.001 * 6.867185592651367
Epoch 660, val loss: 1.14730966091156
Epoch 670, training loss: 0.015361478552222252 = 0.008507478050887585 + 0.001 * 6.854000568389893
Epoch 670, val loss: 1.1543420553207397
Epoch 680, training loss: 0.014875292778015137 = 0.008026724681258202 + 0.001 * 6.848568439483643
Epoch 680, val loss: 1.1612660884857178
Epoch 690, training loss: 0.014428913593292236 = 0.007587546482682228 + 0.001 * 6.841367244720459
Epoch 690, val loss: 1.1679667234420776
Epoch 700, training loss: 0.014030005782842636 = 0.00718550942838192 + 0.001 * 6.844496726989746
Epoch 700, val loss: 1.1744121313095093
Epoch 710, training loss: 0.013651506043970585 = 0.006816249806433916 + 0.001 * 6.835256099700928
Epoch 710, val loss: 1.180773377418518
Epoch 720, training loss: 0.013323839753866196 = 0.006476208101958036 + 0.001 * 6.847631454467773
Epoch 720, val loss: 1.18687105178833
Epoch 730, training loss: 0.01300808135420084 = 0.006162966601550579 + 0.001 * 6.845114231109619
Epoch 730, val loss: 1.1928174495697021
Epoch 740, training loss: 0.012705548666417599 = 0.005875698756426573 + 0.001 * 6.829849720001221
Epoch 740, val loss: 1.1985352039337158
Epoch 750, training loss: 0.01245352067053318 = 0.005610201507806778 + 0.001 * 6.843318939208984
Epoch 750, val loss: 1.204158902168274
Epoch 760, training loss: 0.012194521725177765 = 0.005363272037357092 + 0.001 * 6.831249713897705
Epoch 760, val loss: 1.209643006324768
Epoch 770, training loss: 0.01196163147687912 = 0.005133418831974268 + 0.001 * 6.828211784362793
Epoch 770, val loss: 1.2149522304534912
Epoch 780, training loss: 0.011756427586078644 = 0.004919611383229494 + 0.001 * 6.836815357208252
Epoch 780, val loss: 1.220147967338562
Epoch 790, training loss: 0.011553873308002949 = 0.004720095545053482 + 0.001 * 6.83377742767334
Epoch 790, val loss: 1.225104808807373
Epoch 800, training loss: 0.011365069076418877 = 0.004533758386969566 + 0.001 * 6.831310749053955
Epoch 800, val loss: 1.2299964427947998
Epoch 810, training loss: 0.011177151463925838 = 0.004359513521194458 + 0.001 * 6.8176374435424805
Epoch 810, val loss: 1.2347368001937866
Epoch 820, training loss: 0.011013375595211983 = 0.004196164198219776 + 0.001 * 6.817211151123047
Epoch 820, val loss: 1.2393569946289062
Epoch 830, training loss: 0.010863778181374073 = 0.004043241497129202 + 0.001 * 6.820536136627197
Epoch 830, val loss: 1.2438708543777466
Epoch 840, training loss: 0.010725494474172592 = 0.003899643663316965 + 0.001 * 6.825850963592529
Epoch 840, val loss: 1.2482781410217285
Epoch 850, training loss: 0.010572202503681183 = 0.003764673136174679 + 0.001 * 6.807528495788574
Epoch 850, val loss: 1.2525209188461304
Epoch 860, training loss: 0.010442117229104042 = 0.0036376581992954016 + 0.001 * 6.8044586181640625
Epoch 860, val loss: 1.2567442655563354
Epoch 870, training loss: 0.010327949188649654 = 0.003517974866554141 + 0.001 * 6.80997371673584
Epoch 870, val loss: 1.2608658075332642
Epoch 880, training loss: 0.010217463597655296 = 0.003405198222026229 + 0.001 * 6.812265396118164
Epoch 880, val loss: 1.2648403644561768
Epoch 890, training loss: 0.010110670700669289 = 0.0032988335005939007 + 0.001 * 6.8118367195129395
Epoch 890, val loss: 1.2687865495681763
Epoch 900, training loss: 0.010003232397139072 = 0.003198181511834264 + 0.001 * 6.805050373077393
Epoch 900, val loss: 1.272626519203186
Epoch 910, training loss: 0.009933976456522942 = 0.003102944465354085 + 0.001 * 6.831031322479248
Epoch 910, val loss: 1.276414394378662
Epoch 920, training loss: 0.009802740067243576 = 0.0030126054771244526 + 0.001 * 6.790133953094482
Epoch 920, val loss: 1.2801218032836914
Epoch 930, training loss: 0.009729031473398209 = 0.0029268437065184116 + 0.001 * 6.802187442779541
Epoch 930, val loss: 1.2837204933166504
Epoch 940, training loss: 0.009642224758863449 = 0.002845252398401499 + 0.001 * 6.796972274780273
Epoch 940, val loss: 1.2872710227966309
Epoch 950, training loss: 0.009551307186484337 = 0.0027668941766023636 + 0.001 * 6.7844133377075195
Epoch 950, val loss: 1.290743112564087
Epoch 960, training loss: 0.009507847018539906 = 0.0026915064081549644 + 0.001 * 6.816340446472168
Epoch 960, val loss: 1.2939914464950562
Epoch 970, training loss: 0.009397551417350769 = 0.0026192041113972664 + 0.001 * 6.778346538543701
Epoch 970, val loss: 1.297303318977356
Epoch 980, training loss: 0.009367666207253933 = 0.0025494613219052553 + 0.001 * 6.818204879760742
Epoch 980, val loss: 1.3005318641662598
Epoch 990, training loss: 0.009279406629502773 = 0.0024822119157761335 + 0.001 * 6.797194004058838
Epoch 990, val loss: 1.30372154712677
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.7343
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.968947410583496 = 1.9605735540390015 + 0.001 * 8.373894691467285
Epoch 0, val loss: 1.954233169555664
Epoch 10, training loss: 1.9571518898010254 = 1.9487780332565308 + 0.001 * 8.373796463012695
Epoch 10, val loss: 1.9414725303649902
Epoch 20, training loss: 1.9426437616348267 = 1.9342702627182007 + 0.001 * 8.37352180480957
Epoch 20, val loss: 1.925485372543335
Epoch 30, training loss: 1.922500491142273 = 1.9141275882720947 + 0.001 * 8.372929573059082
Epoch 30, val loss: 1.9032694101333618
Epoch 40, training loss: 1.893564224243164 = 1.885192632675171 + 0.001 * 8.371583938598633
Epoch 40, val loss: 1.8719842433929443
Epoch 50, training loss: 1.853591799736023 = 1.8452240228652954 + 0.001 * 8.367805480957031
Epoch 50, val loss: 1.8311694860458374
Epoch 60, training loss: 1.8058767318725586 = 1.797522783279419 + 0.001 * 8.353926658630371
Epoch 60, val loss: 1.7873882055282593
Epoch 70, training loss: 1.760733962059021 = 1.7524436712265015 + 0.001 * 8.290253639221191
Epoch 70, val loss: 1.7506849765777588
Epoch 80, training loss: 1.7143274545669556 = 1.7064236402511597 + 0.001 * 7.903759479522705
Epoch 80, val loss: 1.7123336791992188
Epoch 90, training loss: 1.6523517370224 = 1.644632339477539 + 0.001 * 7.719435691833496
Epoch 90, val loss: 1.661706805229187
Epoch 100, training loss: 1.5716357231140137 = 1.5640833377838135 + 0.001 * 7.552428245544434
Epoch 100, val loss: 1.5986742973327637
Epoch 110, training loss: 1.4760922193527222 = 1.4686936140060425 + 0.001 * 7.398660182952881
Epoch 110, val loss: 1.5262593030929565
Epoch 120, training loss: 1.3770911693572998 = 1.3697843551635742 + 0.001 * 7.306763172149658
Epoch 120, val loss: 1.450516939163208
Epoch 130, training loss: 1.2800018787384033 = 1.2727590799331665 + 0.001 * 7.2427873611450195
Epoch 130, val loss: 1.378105878829956
Epoch 140, training loss: 1.1839519739151 = 1.1767966747283936 + 0.001 * 7.155319690704346
Epoch 140, val loss: 1.3071410655975342
Epoch 150, training loss: 1.0854980945587158 = 1.0784212350845337 + 0.001 * 7.076827049255371
Epoch 150, val loss: 1.2349846363067627
Epoch 160, training loss: 0.9833420515060425 = 0.9763210415840149 + 0.001 * 7.02103853225708
Epoch 160, val loss: 1.1609570980072021
Epoch 170, training loss: 0.8808956146240234 = 0.8738985061645508 + 0.001 * 6.997100830078125
Epoch 170, val loss: 1.0877972841262817
Epoch 180, training loss: 0.784741222858429 = 0.7777490019798279 + 0.001 * 6.992219924926758
Epoch 180, val loss: 1.021168828010559
Epoch 190, training loss: 0.7002244591712952 = 0.6932336091995239 + 0.001 * 6.990852355957031
Epoch 190, val loss: 0.9657633304595947
Epoch 200, training loss: 0.628677248954773 = 0.6216885447502136 + 0.001 * 6.988717079162598
Epoch 200, val loss: 0.9226574897766113
Epoch 210, training loss: 0.5683299899101257 = 0.5613467693328857 + 0.001 * 6.983240127563477
Epoch 210, val loss: 0.8904944658279419
Epoch 220, training loss: 0.5165229439735413 = 0.509548008441925 + 0.001 * 6.974953651428223
Epoch 220, val loss: 0.8674352765083313
Epoch 230, training loss: 0.4709389805793762 = 0.4639754295349121 + 0.001 * 6.96354341506958
Epoch 230, val loss: 0.8514575362205505
Epoch 240, training loss: 0.4296562671661377 = 0.42270687222480774 + 0.001 * 6.949389934539795
Epoch 240, val loss: 0.840345025062561
Epoch 250, training loss: 0.39135727286338806 = 0.38442257046699524 + 0.001 * 6.934706687927246
Epoch 250, val loss: 0.8328335881233215
Epoch 260, training loss: 0.35527515411376953 = 0.34835150837898254 + 0.001 * 6.9236531257629395
Epoch 260, val loss: 0.8277014493942261
Epoch 270, training loss: 0.32115456461906433 = 0.31424176692962646 + 0.001 * 6.912797927856445
Epoch 270, val loss: 0.8243986964225769
Epoch 280, training loss: 0.2890445590019226 = 0.2821366786956787 + 0.001 * 6.9078874588012695
Epoch 280, val loss: 0.8225470185279846
Epoch 290, training loss: 0.2592891454696655 = 0.25238898396492004 + 0.001 * 6.9001593589782715
Epoch 290, val loss: 0.8218129873275757
Epoch 300, training loss: 0.23222248256206512 = 0.22532537579536438 + 0.001 * 6.8971123695373535
Epoch 300, val loss: 0.8222974538803101
Epoch 310, training loss: 0.20798861980438232 = 0.20109444856643677 + 0.001 * 6.894174098968506
Epoch 310, val loss: 0.8239863514900208
Epoch 320, training loss: 0.1865229457616806 = 0.1796315461397171 + 0.001 * 6.891404628753662
Epoch 320, val loss: 0.8270493149757385
Epoch 330, training loss: 0.16761450469493866 = 0.16072791814804077 + 0.001 * 6.886593341827393
Epoch 330, val loss: 0.8314509987831116
Epoch 340, training loss: 0.1509477198123932 = 0.1440613865852356 + 0.001 * 6.886332988739014
Epoch 340, val loss: 0.8370416164398193
Epoch 350, training loss: 0.13607989251613617 = 0.12919946014881134 + 0.001 * 6.880429267883301
Epoch 350, val loss: 0.8436164259910583
Epoch 360, training loss: 0.12276723235845566 = 0.11588868498802185 + 0.001 * 6.878543853759766
Epoch 360, val loss: 0.8509277701377869
Epoch 370, training loss: 0.11073052138090134 = 0.10385843366384506 + 0.001 * 6.8720903396606445
Epoch 370, val loss: 0.8590430617332458
Epoch 380, training loss: 0.09981589764356613 = 0.09294592589139938 + 0.001 * 6.869969367980957
Epoch 380, val loss: 0.868252694606781
Epoch 390, training loss: 0.0900309830904007 = 0.08316295593976974 + 0.001 * 6.868023872375488
Epoch 390, val loss: 0.8786431550979614
Epoch 400, training loss: 0.08118419349193573 = 0.07432333379983902 + 0.001 * 6.860857009887695
Epoch 400, val loss: 0.8901917338371277
Epoch 410, training loss: 0.07327109575271606 = 0.0664050281047821 + 0.001 * 6.866068363189697
Epoch 410, val loss: 0.9028453826904297
Epoch 420, training loss: 0.06603196263313293 = 0.05917557701468468 + 0.001 * 6.856383323669434
Epoch 420, val loss: 0.9158768653869629
Epoch 430, training loss: 0.05933397263288498 = 0.052476946264505386 + 0.001 * 6.857024192810059
Epoch 430, val loss: 0.9290711283683777
Epoch 440, training loss: 0.05316893383860588 = 0.04631761834025383 + 0.001 * 6.851315975189209
Epoch 440, val loss: 0.9426954388618469
Epoch 450, training loss: 0.0478392019867897 = 0.040983542799949646 + 0.001 * 6.855660438537598
Epoch 450, val loss: 0.9577028751373291
Epoch 460, training loss: 0.04342800751328468 = 0.03657899051904678 + 0.001 * 6.849018096923828
Epoch 460, val loss: 0.9744104743003845
Epoch 470, training loss: 0.03975754231214523 = 0.032907407730817795 + 0.001 * 6.850135326385498
Epoch 470, val loss: 0.9919934272766113
Epoch 480, training loss: 0.03663860261440277 = 0.029793383553624153 + 0.001 * 6.845218658447266
Epoch 480, val loss: 1.0091338157653809
Epoch 490, training loss: 0.03394410014152527 = 0.027100060135126114 + 0.001 * 6.8440399169921875
Epoch 490, val loss: 1.025889277458191
Epoch 500, training loss: 0.03161363676190376 = 0.02476094476878643 + 0.001 * 6.852692127227783
Epoch 500, val loss: 1.0423351526260376
Epoch 510, training loss: 0.02956431731581688 = 0.022717908024787903 + 0.001 * 6.846409797668457
Epoch 510, val loss: 1.0583410263061523
Epoch 520, training loss: 0.027766916900873184 = 0.020923692733049393 + 0.001 * 6.843225002288818
Epoch 520, val loss: 1.0739413499832153
Epoch 530, training loss: 0.026178454980254173 = 0.019339295104146004 + 0.001 * 6.839159965515137
Epoch 530, val loss: 1.0891293287277222
Epoch 540, training loss: 0.024784250184893608 = 0.017931675538420677 + 0.001 * 6.852574348449707
Epoch 540, val loss: 1.103868842124939
Epoch 550, training loss: 0.02351597137749195 = 0.016676397994160652 + 0.001 * 6.839572429656982
Epoch 550, val loss: 1.1181706190109253
Epoch 560, training loss: 0.02238982915878296 = 0.015551107935607433 + 0.001 * 6.838719844818115
Epoch 560, val loss: 1.1321015357971191
Epoch 570, training loss: 0.021375000476837158 = 0.01453861128538847 + 0.001 * 6.836388111114502
Epoch 570, val loss: 1.1456323862075806
Epoch 580, training loss: 0.020459474995732307 = 0.013624357059597969 + 0.001 * 6.835117340087891
Epoch 580, val loss: 1.158766508102417
Epoch 590, training loss: 0.019635718315839767 = 0.01279631070792675 + 0.001 * 6.839407444000244
Epoch 590, val loss: 1.1715527772903442
Epoch 600, training loss: 0.018875138834118843 = 0.01204359158873558 + 0.001 * 6.831547260284424
Epoch 600, val loss: 1.1839452981948853
Epoch 610, training loss: 0.018187817186117172 = 0.011357691138982773 + 0.001 * 6.83012580871582
Epoch 610, val loss: 1.196022868156433
Epoch 620, training loss: 0.01756185293197632 = 0.010730787180364132 + 0.001 * 6.831065654754639
Epoch 620, val loss: 1.2077559232711792
Epoch 630, training loss: 0.016985058784484863 = 0.01015617698431015 + 0.001 * 6.828880786895752
Epoch 630, val loss: 1.21919584274292
Epoch 640, training loss: 0.01645824685692787 = 0.00962842907756567 + 0.001 * 6.8298163414001465
Epoch 640, val loss: 1.2303466796875
Epoch 650, training loss: 0.015981407836079597 = 0.00914238765835762 + 0.001 * 6.839019298553467
Epoch 650, val loss: 1.2412089109420776
Epoch 660, training loss: 0.015518082305788994 = 0.00869401078671217 + 0.001 * 6.824071884155273
Epoch 660, val loss: 1.2518188953399658
Epoch 670, training loss: 0.015103119425475597 = 0.008279625326395035 + 0.001 * 6.823493957519531
Epoch 670, val loss: 1.2621545791625977
Epoch 680, training loss: 0.014727801084518433 = 0.007895871996879578 + 0.001 * 6.831928730010986
Epoch 680, val loss: 1.272210955619812
Epoch 690, training loss: 0.014363097958266735 = 0.007539853919297457 + 0.001 * 6.823243618011475
Epoch 690, val loss: 1.2820568084716797
Epoch 700, training loss: 0.01402583159506321 = 0.007208933588117361 + 0.001 * 6.816897869110107
Epoch 700, val loss: 1.2916374206542969
Epoch 710, training loss: 0.013728495687246323 = 0.006900746375322342 + 0.001 * 6.827748775482178
Epoch 710, val loss: 1.3009814023971558
Epoch 720, training loss: 0.013431317172944546 = 0.006613312754780054 + 0.001 * 6.818004131317139
Epoch 720, val loss: 1.310131549835205
Epoch 730, training loss: 0.013155870139598846 = 0.0063448380678892136 + 0.001 * 6.811031818389893
Epoch 730, val loss: 1.3190535306930542
Epoch 740, training loss: 0.012913614511489868 = 0.0060936580412089825 + 0.001 * 6.8199567794799805
Epoch 740, val loss: 1.3277764320373535
Epoch 750, training loss: 0.012679527513682842 = 0.005858349613845348 + 0.001 * 6.8211774826049805
Epoch 750, val loss: 1.3363126516342163
Epoch 760, training loss: 0.012450840324163437 = 0.005637593101710081 + 0.001 * 6.813247203826904
Epoch 760, val loss: 1.3446519374847412
Epoch 770, training loss: 0.012235967442393303 = 0.005430193617939949 + 0.001 * 6.8057732582092285
Epoch 770, val loss: 1.3528131246566772
Epoch 780, training loss: 0.012045500800013542 = 0.005235092248767614 + 0.001 * 6.810408115386963
Epoch 780, val loss: 1.3607913255691528
Epoch 790, training loss: 0.01185406930744648 = 0.005051306448876858 + 0.001 * 6.802762508392334
Epoch 790, val loss: 1.368607521057129
Epoch 800, training loss: 0.011682100594043732 = 0.004877805709838867 + 0.001 * 6.804295063018799
Epoch 800, val loss: 1.376250982284546
Epoch 810, training loss: 0.011516942642629147 = 0.004713187925517559 + 0.001 * 6.8037543296813965
Epoch 810, val loss: 1.3837512731552124
Epoch 820, training loss: 0.011359469965100288 = 0.004556481260806322 + 0.001 * 6.802988529205322
Epoch 820, val loss: 1.3911657333374023
Epoch 830, training loss: 0.011220039799809456 = 0.004406745545566082 + 0.001 * 6.813293933868408
Epoch 830, val loss: 1.3984297513961792
Epoch 840, training loss: 0.011063162237405777 = 0.004262960981577635 + 0.001 * 6.800201416015625
Epoch 840, val loss: 1.405660629272461
Epoch 850, training loss: 0.010919550433754921 = 0.004124433733522892 + 0.001 * 6.795116901397705
Epoch 850, val loss: 1.4127904176712036
Epoch 860, training loss: 0.010801059193909168 = 0.0039907461032271385 + 0.001 * 6.810312747955322
Epoch 860, val loss: 1.4199098348617554
Epoch 870, training loss: 0.010651866905391216 = 0.0038617774844169617 + 0.001 * 6.790089130401611
Epoch 870, val loss: 1.426999807357788
Epoch 880, training loss: 0.010527444072067738 = 0.0037373811937868595 + 0.001 * 6.790062427520752
Epoch 880, val loss: 1.4340519905090332
Epoch 890, training loss: 0.010407542809844017 = 0.0036174138076603413 + 0.001 * 6.790128707885742
Epoch 890, val loss: 1.441081166267395
Epoch 900, training loss: 0.010289686731994152 = 0.0035019940696656704 + 0.001 * 6.787692546844482
Epoch 900, val loss: 1.4481031894683838
Epoch 910, training loss: 0.010170955210924149 = 0.00339109287597239 + 0.001 * 6.779861927032471
Epoch 910, val loss: 1.455073595046997
Epoch 920, training loss: 0.010065745562314987 = 0.003284475998952985 + 0.001 * 6.781269550323486
Epoch 920, val loss: 1.4620033502578735
Epoch 930, training loss: 0.009966876357793808 = 0.0031819608993828297 + 0.001 * 6.784915447235107
Epoch 930, val loss: 1.4689499139785767
Epoch 940, training loss: 0.009865352883934975 = 0.0030835773795843124 + 0.001 * 6.781774520874023
Epoch 940, val loss: 1.4758328199386597
Epoch 950, training loss: 0.009776152670383453 = 0.0029895061161369085 + 0.001 * 6.786646366119385
Epoch 950, val loss: 1.4826650619506836
Epoch 960, training loss: 0.009669570252299309 = 0.0028994311578571796 + 0.001 * 6.770139217376709
Epoch 960, val loss: 1.4894745349884033
Epoch 970, training loss: 0.009588781744241714 = 0.0028130386490374804 + 0.001 * 6.775743007659912
Epoch 970, val loss: 1.4962387084960938
Epoch 980, training loss: 0.009498700499534607 = 0.0027303367387503386 + 0.001 * 6.7683634757995605
Epoch 980, val loss: 1.5029059648513794
Epoch 990, training loss: 0.009439751505851746 = 0.002651351038366556 + 0.001 * 6.788400650024414
Epoch 990, val loss: 1.5095373392105103
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7343
Flip ASR: 0.6978/225 nodes
The final ASR:0.76630, 0.04523, Accuracy:0.78272, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11586])
remove edge: torch.Size([2, 9424])
updated graph: torch.Size([2, 10454])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98032, 0.00174, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9513925313949585 = 1.9430186748504639 + 0.001 * 8.373828887939453
Epoch 0, val loss: 1.9397481679916382
Epoch 10, training loss: 1.9410284757614136 = 1.9326547384262085 + 0.001 * 8.373754501342773
Epoch 10, val loss: 1.9297939538955688
Epoch 20, training loss: 1.9281638860702515 = 1.9197903871536255 + 0.001 * 8.373506546020508
Epoch 20, val loss: 1.9168721437454224
Epoch 30, training loss: 1.9100494384765625 = 1.9016764163970947 + 0.001 * 8.372983932495117
Epoch 30, val loss: 1.8983691930770874
Epoch 40, training loss: 1.883542776107788 = 1.8751710653305054 + 0.001 * 8.371760368347168
Epoch 40, val loss: 1.8718069791793823
Epoch 50, training loss: 1.8472399711608887 = 1.8388715982437134 + 0.001 * 8.368337631225586
Epoch 50, val loss: 1.8374143838882446
Epoch 60, training loss: 1.8068771362304688 = 1.7985213994979858 + 0.001 * 8.355711936950684
Epoch 60, val loss: 1.803620457649231
Epoch 70, training loss: 1.7696280479431152 = 1.7613366842269897 + 0.001 * 8.291356086730957
Epoch 70, val loss: 1.7760405540466309
Epoch 80, training loss: 1.722248911857605 = 1.7142822742462158 + 0.001 * 7.966658592224121
Epoch 80, val loss: 1.7372708320617676
Epoch 90, training loss: 1.6557292938232422 = 1.6479392051696777 + 0.001 * 7.79008150100708
Epoch 90, val loss: 1.680882215499878
Epoch 100, training loss: 1.568550705909729 = 1.5608716011047363 + 0.001 * 7.679150104522705
Epoch 100, val loss: 1.6092851161956787
Epoch 110, training loss: 1.4682217836380005 = 1.4607306718826294 + 0.001 * 7.491103649139404
Epoch 110, val loss: 1.5303165912628174
Epoch 120, training loss: 1.3658483028411865 = 1.3586270809173584 + 0.001 * 7.221188068389893
Epoch 120, val loss: 1.452722430229187
Epoch 130, training loss: 1.2646548748016357 = 1.2575169801712036 + 0.001 * 7.1379170417785645
Epoch 130, val loss: 1.3786354064941406
Epoch 140, training loss: 1.16561758518219 = 1.158560872077942 + 0.001 * 7.056662082672119
Epoch 140, val loss: 1.307246208190918
Epoch 150, training loss: 1.0712428092956543 = 1.064241647720337 + 0.001 * 7.001116752624512
Epoch 150, val loss: 1.239896535873413
Epoch 160, training loss: 0.9837509989738464 = 0.97679603099823 + 0.001 * 6.954953670501709
Epoch 160, val loss: 1.1779721975326538
Epoch 170, training loss: 0.9027906656265259 = 0.8958753347396851 + 0.001 * 6.915333271026611
Epoch 170, val loss: 1.1213889122009277
Epoch 180, training loss: 0.8261576890945435 = 0.8192648887634277 + 0.001 * 6.892772674560547
Epoch 180, val loss: 1.0681613683700562
Epoch 190, training loss: 0.7516356110572815 = 0.7447514533996582 + 0.001 * 6.884137153625488
Epoch 190, val loss: 1.017082691192627
Epoch 200, training loss: 0.6785948276519775 = 0.6717159152030945 + 0.001 * 6.878916263580322
Epoch 200, val loss: 0.9677785038948059
Epoch 210, training loss: 0.6081900596618652 = 0.6013146638870239 + 0.001 * 6.875371932983398
Epoch 210, val loss: 0.9214693307876587
Epoch 220, training loss: 0.5422968864440918 = 0.5354241728782654 + 0.001 * 6.8726887702941895
Epoch 220, val loss: 0.8802655935287476
Epoch 230, training loss: 0.48208847641944885 = 0.475218266248703 + 0.001 * 6.870224475860596
Epoch 230, val loss: 0.8459634780883789
Epoch 240, training loss: 0.4276222884654999 = 0.4207543730735779 + 0.001 * 6.867923736572266
Epoch 240, val loss: 0.8189187049865723
Epoch 250, training loss: 0.37839028239250183 = 0.3715243935585022 + 0.001 * 6.865886688232422
Epoch 250, val loss: 0.7987677454948425
Epoch 260, training loss: 0.333827406167984 = 0.3269631564617157 + 0.001 * 6.864259719848633
Epoch 260, val loss: 0.7850863933563232
Epoch 270, training loss: 0.29355576634407043 = 0.28669273853302 + 0.001 * 6.863039970397949
Epoch 270, val loss: 0.7776781916618347
Epoch 280, training loss: 0.2574377954006195 = 0.2505747079849243 + 0.001 * 6.863091468811035
Epoch 280, val loss: 0.7763259410858154
Epoch 290, training loss: 0.22534804046154022 = 0.21848638355731964 + 0.001 * 6.861651420593262
Epoch 290, val loss: 0.7806074619293213
Epoch 300, training loss: 0.1970958560705185 = 0.19023479521274567 + 0.001 * 6.8610639572143555
Epoch 300, val loss: 0.7896528244018555
Epoch 310, training loss: 0.17244860529899597 = 0.16558793187141418 + 0.001 * 6.860667705535889
Epoch 310, val loss: 0.8024227023124695
Epoch 320, training loss: 0.15117143094539642 = 0.14431113004684448 + 0.001 * 6.860304355621338
Epoch 320, val loss: 0.8178945779800415
Epoch 330, training loss: 0.13298195600509644 = 0.12612201273441315 + 0.001 * 6.859950542449951
Epoch 330, val loss: 0.8350714445114136
Epoch 340, training loss: 0.11753294616937637 = 0.11067340523004532 + 0.001 * 6.859543323516846
Epoch 340, val loss: 0.8531479239463806
Epoch 350, training loss: 0.10444054752588272 = 0.09758055955171585 + 0.001 * 6.859986782073975
Epoch 350, val loss: 0.8717337846755981
Epoch 360, training loss: 0.09332927316427231 = 0.08647046238183975 + 0.001 * 6.858808517456055
Epoch 360, val loss: 0.8905771970748901
Epoch 370, training loss: 0.08386564254760742 = 0.07700744271278381 + 0.001 * 6.858202934265137
Epoch 370, val loss: 0.9094828367233276
Epoch 380, training loss: 0.07576595991849899 = 0.06890840083360672 + 0.001 * 6.857558727264404
Epoch 380, val loss: 0.9283556938171387
Epoch 390, training loss: 0.06879568099975586 = 0.0619383230805397 + 0.001 * 6.857354164123535
Epoch 390, val loss: 0.9470965266227722
Epoch 400, training loss: 0.06276150792837143 = 0.05590524896979332 + 0.001 * 6.856256484985352
Epoch 400, val loss: 0.9656418561935425
Epoch 410, training loss: 0.05750942602753639 = 0.05065293237566948 + 0.001 * 6.856494426727295
Epoch 410, val loss: 0.9838874936103821
Epoch 420, training loss: 0.05291105806827545 = 0.04605613648891449 + 0.001 * 6.854921340942383
Epoch 420, val loss: 1.001801609992981
Epoch 430, training loss: 0.04886646568775177 = 0.04201212525367737 + 0.001 * 6.854339599609375
Epoch 430, val loss: 1.0192830562591553
Epoch 440, training loss: 0.04529339075088501 = 0.03843911737203598 + 0.001 * 6.854271411895752
Epoch 440, val loss: 1.0363354682922363
Epoch 450, training loss: 0.04212334379553795 = 0.035269614309072495 + 0.001 * 6.85373067855835
Epoch 450, val loss: 1.0529778003692627
Epoch 460, training loss: 0.039300091564655304 = 0.03244730085134506 + 0.001 * 6.852788925170898
Epoch 460, val loss: 1.0691953897476196
Epoch 470, training loss: 0.03677944093942642 = 0.029925422742962837 + 0.001 * 6.854015827178955
Epoch 470, val loss: 1.0849484205245972
Epoch 480, training loss: 0.03451753407716751 = 0.027665138244628906 + 0.001 * 6.8523969650268555
Epoch 480, val loss: 1.100253939628601
Epoch 490, training loss: 0.03248516470193863 = 0.02563326433300972 + 0.001 * 6.851899147033691
Epoch 490, val loss: 1.1151033639907837
Epoch 500, training loss: 0.03065251186490059 = 0.023800909519195557 + 0.001 * 6.851602077484131
Epoch 500, val loss: 1.1295018196105957
Epoch 510, training loss: 0.028997790068387985 = 0.02214493229985237 + 0.001 * 6.852856636047363
Epoch 510, val loss: 1.1434228420257568
Epoch 520, training loss: 0.027496060356497765 = 0.020644454285502434 + 0.001 * 6.851605415344238
Epoch 520, val loss: 1.1569018363952637
Epoch 530, training loss: 0.02613132819533348 = 0.01928205043077469 + 0.001 * 6.849277496337891
Epoch 530, val loss: 1.1699668169021606
Epoch 540, training loss: 0.02489374950528145 = 0.018043000251054764 + 0.001 * 6.850749969482422
Epoch 540, val loss: 1.182640552520752
Epoch 550, training loss: 0.023761941120028496 = 0.01691329851746559 + 0.001 * 6.848641872406006
Epoch 550, val loss: 1.1949540376663208
Epoch 560, training loss: 0.02272835001349449 = 0.015881329774856567 + 0.001 * 6.847020149230957
Epoch 560, val loss: 1.206861138343811
Epoch 570, training loss: 0.021784838289022446 = 0.014937412925064564 + 0.001 * 6.847425937652588
Epoch 570, val loss: 1.2184176445007324
Epoch 580, training loss: 0.02091776207089424 = 0.014072439633309841 + 0.001 * 6.845322132110596
Epoch 580, val loss: 1.2296066284179688
Epoch 590, training loss: 0.020120695233345032 = 0.013278011232614517 + 0.001 * 6.842684268951416
Epoch 590, val loss: 1.240488886833191
Epoch 600, training loss: 0.019399592652916908 = 0.012547357939183712 + 0.001 * 6.852234840393066
Epoch 600, val loss: 1.2510203123092651
Epoch 610, training loss: 0.018721792846918106 = 0.011874152347445488 + 0.001 * 6.847640037536621
Epoch 610, val loss: 1.2612608671188354
Epoch 620, training loss: 0.01809212565422058 = 0.011252906173467636 + 0.001 * 6.8392181396484375
Epoch 620, val loss: 1.271193265914917
Epoch 630, training loss: 0.017516005784273148 = 0.010678610764443874 + 0.001 * 6.837395668029785
Epoch 630, val loss: 1.2808407545089722
Epoch 640, training loss: 0.016982022672891617 = 0.010146833024919033 + 0.001 * 6.835188388824463
Epoch 640, val loss: 1.2902183532714844
Epoch 650, training loss: 0.01650569774210453 = 0.0096539705991745 + 0.001 * 6.851726531982422
Epoch 650, val loss: 1.2993296384811401
Epoch 660, training loss: 0.01602962613105774 = 0.009196308441460133 + 0.001 * 6.833316326141357
Epoch 660, val loss: 1.3082129955291748
Epoch 670, training loss: 0.015600651502609253 = 0.008770745247602463 + 0.001 * 6.8299055099487305
Epoch 670, val loss: 1.3168258666992188
Epoch 680, training loss: 0.015208728611469269 = 0.008374565280973911 + 0.001 * 6.834163188934326
Epoch 680, val loss: 1.3252198696136475
Epoch 690, training loss: 0.014829668216407299 = 0.008005249314010143 + 0.001 * 6.824418544769287
Epoch 690, val loss: 1.3334242105484009
Epoch 700, training loss: 0.014510765671730042 = 0.00766052957624197 + 0.001 * 6.850236415863037
Epoch 700, val loss: 1.3413927555084229
Epoch 710, training loss: 0.014176926575601101 = 0.007338183466345072 + 0.001 * 6.838742733001709
Epoch 710, val loss: 1.349175214767456
Epoch 720, training loss: 0.013858046382665634 = 0.0070363599807024 + 0.001 * 6.821685791015625
Epoch 720, val loss: 1.3567523956298828
Epoch 730, training loss: 0.01356187928467989 = 0.0067535885609686375 + 0.001 * 6.808290481567383
Epoch 730, val loss: 1.3641414642333984
Epoch 740, training loss: 0.013313107192516327 = 0.00648830272257328 + 0.001 * 6.824804782867432
Epoch 740, val loss: 1.3713741302490234
Epoch 750, training loss: 0.01304108276963234 = 0.006239105947315693 + 0.001 * 6.801976203918457
Epoch 750, val loss: 1.3784360885620117
Epoch 760, training loss: 0.012800857424736023 = 0.0060047670267522335 + 0.001 * 6.796090602874756
Epoch 760, val loss: 1.3853085041046143
Epoch 770, training loss: 0.012583483941853046 = 0.005784162320196629 + 0.001 * 6.799321174621582
Epoch 770, val loss: 1.3920141458511353
Epoch 780, training loss: 0.012381285429000854 = 0.005576244089752436 + 0.001 * 6.8050408363342285
Epoch 780, val loss: 1.3985849618911743
Epoch 790, training loss: 0.012167135253548622 = 0.005380077753216028 + 0.001 * 6.787056922912598
Epoch 790, val loss: 1.4049979448318481
Epoch 800, training loss: 0.012026989832520485 = 0.005194858647882938 + 0.001 * 6.832130432128906
Epoch 800, val loss: 1.411264419555664
Epoch 810, training loss: 0.011820029467344284 = 0.005019715521484613 + 0.001 * 6.800313472747803
Epoch 810, val loss: 1.417418122291565
Epoch 820, training loss: 0.011623241007328033 = 0.004854019731283188 + 0.001 * 6.769220352172852
Epoch 820, val loss: 1.4234176874160767
Epoch 830, training loss: 0.011512354016304016 = 0.0046971202827990055 + 0.001 * 6.8152337074279785
Epoch 830, val loss: 1.4292800426483154
Epoch 840, training loss: 0.011332089081406593 = 0.004548398777842522 + 0.001 * 6.783689498901367
Epoch 840, val loss: 1.4350216388702393
Epoch 850, training loss: 0.011197706684470177 = 0.00440730107948184 + 0.001 * 6.7904052734375
Epoch 850, val loss: 1.440647840499878
Epoch 860, training loss: 0.011067590676248074 = 0.004273327998816967 + 0.001 * 6.794262409210205
Epoch 860, val loss: 1.4461649656295776
Epoch 870, training loss: 0.010920883156359196 = 0.004146001301705837 + 0.001 * 6.774881362915039
Epoch 870, val loss: 1.4515644311904907
Epoch 880, training loss: 0.01078815758228302 = 0.00402491120621562 + 0.001 * 6.763245582580566
Epoch 880, val loss: 1.4568451642990112
Epoch 890, training loss: 0.010706990957260132 = 0.003909695893526077 + 0.001 * 6.7972941398620605
Epoch 890, val loss: 1.462009072303772
Epoch 900, training loss: 0.010559815913438797 = 0.0037999311462044716 + 0.001 * 6.759883880615234
Epoch 900, val loss: 1.4670718908309937
Epoch 910, training loss: 0.010451750829815865 = 0.003695332445204258 + 0.001 * 6.756418704986572
Epoch 910, val loss: 1.4720604419708252
Epoch 920, training loss: 0.010375981219112873 = 0.00359553936868906 + 0.001 * 6.780441761016846
Epoch 920, val loss: 1.4769355058670044
Epoch 930, training loss: 0.010250028222799301 = 0.0035002981312572956 + 0.001 * 6.749729633331299
Epoch 930, val loss: 1.4817116260528564
Epoch 940, training loss: 0.010163024067878723 = 0.003409335156902671 + 0.001 * 6.753688812255859
Epoch 940, val loss: 1.48638916015625
Epoch 950, training loss: 0.01009999867528677 = 0.003322411561384797 + 0.001 * 6.777586936950684
Epoch 950, val loss: 1.4910084009170532
Epoch 960, training loss: 0.009979112073779106 = 0.003239251207560301 + 0.001 * 6.739861011505127
Epoch 960, val loss: 1.4955108165740967
Epoch 970, training loss: 0.009900438599288464 = 0.0031596857588738203 + 0.001 * 6.740752220153809
Epoch 970, val loss: 1.499955654144287
Epoch 980, training loss: 0.009826410561800003 = 0.00308350776322186 + 0.001 * 6.742902755737305
Epoch 980, val loss: 1.504309058189392
Epoch 990, training loss: 0.00974325556308031 = 0.003010490443557501 + 0.001 * 6.732764720916748
Epoch 990, val loss: 1.5085794925689697
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5351
Flip ASR: 0.4400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9346967935562134 = 1.9263229370117188 + 0.001 * 8.373801231384277
Epoch 0, val loss: 1.9219857454299927
Epoch 10, training loss: 1.9253464937210083 = 1.9169727563858032 + 0.001 * 8.373702049255371
Epoch 10, val loss: 1.9117027521133423
Epoch 20, training loss: 1.91396963596344 = 1.9055962562561035 + 0.001 * 8.37336254119873
Epoch 20, val loss: 1.8989146947860718
Epoch 30, training loss: 1.8981462717056274 = 1.8897736072540283 + 0.001 * 8.372608184814453
Epoch 30, val loss: 1.8809876441955566
Epoch 40, training loss: 1.875131368637085 = 1.866760492324829 + 0.001 * 8.370903968811035
Epoch 40, val loss: 1.8550491333007812
Epoch 50, training loss: 1.8423136472702026 = 1.8339471817016602 + 0.001 * 8.36644458770752
Epoch 50, val loss: 1.8193267583847046
Epoch 60, training loss: 1.8002614974975586 = 1.7919106483459473 + 0.001 * 8.350794792175293
Epoch 60, val loss: 1.7768281698226929
Epoch 70, training loss: 1.752301812171936 = 1.7440428733825684 + 0.001 * 8.258917808532715
Epoch 70, val loss: 1.7333850860595703
Epoch 80, training loss: 1.6929552555084229 = 1.685178518295288 + 0.001 * 7.776764869689941
Epoch 80, val loss: 1.683365821838379
Epoch 90, training loss: 1.6138920783996582 = 1.6063251495361328 + 0.001 * 7.56688928604126
Epoch 90, val loss: 1.6173275709152222
Epoch 100, training loss: 1.5153464078903198 = 1.507957100868225 + 0.001 * 7.3893280029296875
Epoch 100, val loss: 1.5363014936447144
Epoch 110, training loss: 1.405504584312439 = 1.3982739448547363 + 0.001 * 7.230646133422852
Epoch 110, val loss: 1.4487148523330688
Epoch 120, training loss: 1.2927762269973755 = 1.285561203956604 + 0.001 * 7.215011119842529
Epoch 120, val loss: 1.361938714981079
Epoch 130, training loss: 1.1806563138961792 = 1.1734533309936523 + 0.001 * 7.203040599822998
Epoch 130, val loss: 1.2770938873291016
Epoch 140, training loss: 1.0711126327514648 = 1.063927173614502 + 0.001 * 7.1854248046875
Epoch 140, val loss: 1.195213794708252
Epoch 150, training loss: 0.9677044749259949 = 0.9605447053909302 + 0.001 * 7.159755706787109
Epoch 150, val loss: 1.1192573308944702
Epoch 160, training loss: 0.8758621215820312 = 0.8687517046928406 + 0.001 * 7.110441207885742
Epoch 160, val loss: 1.0546461343765259
Epoch 170, training loss: 0.7984961271286011 = 0.7914644479751587 + 0.001 * 7.031651020050049
Epoch 170, val loss: 1.0024598836898804
Epoch 180, training loss: 0.734700083732605 = 0.7277379035949707 + 0.001 * 6.962164878845215
Epoch 180, val loss: 0.9613857865333557
Epoch 190, training loss: 0.6813991665840149 = 0.674473226070404 + 0.001 * 6.925953388214111
Epoch 190, val loss: 0.9294501543045044
Epoch 200, training loss: 0.6350247859954834 = 0.6281233429908752 + 0.001 * 6.901451110839844
Epoch 200, val loss: 0.9041492342948914
Epoch 210, training loss: 0.5926307439804077 = 0.5857433080673218 + 0.001 * 6.88740873336792
Epoch 210, val loss: 0.8829908967018127
Epoch 220, training loss: 0.5522444844245911 = 0.5453636050224304 + 0.001 * 6.880897045135498
Epoch 220, val loss: 0.8647043108940125
Epoch 230, training loss: 0.5127021670341492 = 0.5058251023292542 + 0.001 * 6.877049446105957
Epoch 230, val loss: 0.8488183617591858
Epoch 240, training loss: 0.4736596345901489 = 0.4667854309082031 + 0.001 * 6.8742170333862305
Epoch 240, val loss: 0.8353832364082336
Epoch 250, training loss: 0.43509650230407715 = 0.42822471261024475 + 0.001 * 6.871793270111084
Epoch 250, val loss: 0.8245505094528198
Epoch 260, training loss: 0.3971423804759979 = 0.3902739882469177 + 0.001 * 6.868386268615723
Epoch 260, val loss: 0.8168268203735352
Epoch 270, training loss: 0.3602057099342346 = 0.35334068536758423 + 0.001 * 6.865015506744385
Epoch 270, val loss: 0.8125638961791992
Epoch 280, training loss: 0.32517340779304504 = 0.31831157207489014 + 0.001 * 6.861844062805176
Epoch 280, val loss: 0.8119854927062988
Epoch 290, training loss: 0.29278478026390076 = 0.2859245538711548 + 0.001 * 6.8602399826049805
Epoch 290, val loss: 0.814872145652771
Epoch 300, training loss: 0.2635762393474579 = 0.25672003626823425 + 0.001 * 6.856189727783203
Epoch 300, val loss: 0.82099449634552
Epoch 310, training loss: 0.23767884075641632 = 0.23082642257213593 + 0.001 * 6.852423667907715
Epoch 310, val loss: 0.8303184509277344
Epoch 320, training loss: 0.21484389901161194 = 0.20799481868743896 + 0.001 * 6.849076747894287
Epoch 320, val loss: 0.8422232270240784
Epoch 330, training loss: 0.19466552138328552 = 0.18781910836696625 + 0.001 * 6.846409797668457
Epoch 330, val loss: 0.8563799858093262
Epoch 340, training loss: 0.17679907381534576 = 0.1699589341878891 + 0.001 * 6.840139389038086
Epoch 340, val loss: 0.8724564909934998
Epoch 350, training loss: 0.1609087884426117 = 0.1540745049715042 + 0.001 * 6.834282398223877
Epoch 350, val loss: 0.8901243209838867
Epoch 360, training loss: 0.1467055231332779 = 0.13987308740615845 + 0.001 * 6.832433700561523
Epoch 360, val loss: 0.908821165561676
Epoch 370, training loss: 0.13401378691196442 = 0.12718793749809265 + 0.001 * 6.82584810256958
Epoch 370, val loss: 0.9281948804855347
Epoch 380, training loss: 0.12261375784873962 = 0.1157786101102829 + 0.001 * 6.835145473480225
Epoch 380, val loss: 0.9479694366455078
Epoch 390, training loss: 0.11225612461566925 = 0.10544166713953018 + 0.001 * 6.814460277557373
Epoch 390, val loss: 0.9676935076713562
Epoch 400, training loss: 0.10283543169498444 = 0.09603739529848099 + 0.001 * 6.798033237457275
Epoch 400, val loss: 0.9872872829437256
Epoch 410, training loss: 0.09423716366291046 = 0.08744391053915024 + 0.001 * 6.793252944946289
Epoch 410, val loss: 1.0066144466400146
Epoch 420, training loss: 0.08640629053115845 = 0.07961135357618332 + 0.001 * 6.79494047164917
Epoch 420, val loss: 1.0254573822021484
Epoch 430, training loss: 0.07934942096471786 = 0.07255787402391434 + 0.001 * 6.7915472984313965
Epoch 430, val loss: 1.0441056489944458
Epoch 440, training loss: 0.07304131239652634 = 0.06626147031784058 + 0.001 * 6.779845237731934
Epoch 440, val loss: 1.0626559257507324
Epoch 450, training loss: 0.06741996854543686 = 0.060655202716588974 + 0.001 * 6.764764785766602
Epoch 450, val loss: 1.0810235738754272
Epoch 460, training loss: 0.062464985996484756 = 0.05569200590252876 + 0.001 * 6.772980690002441
Epoch 460, val loss: 1.0992993116378784
Epoch 470, training loss: 0.05805099755525589 = 0.05129595100879669 + 0.001 * 6.755047798156738
Epoch 470, val loss: 1.1173280477523804
Epoch 480, training loss: 0.05414411798119545 = 0.04738183692097664 + 0.001 * 6.7622809410095215
Epoch 480, val loss: 1.1350337266921997
Epoch 490, training loss: 0.05063514783978462 = 0.043889470398426056 + 0.001 * 6.745676040649414
Epoch 490, val loss: 1.152510643005371
Epoch 500, training loss: 0.04750319570302963 = 0.04074952006340027 + 0.001 * 6.753675937652588
Epoch 500, val loss: 1.1696674823760986
Epoch 510, training loss: 0.04465349763631821 = 0.0379023551940918 + 0.001 * 6.751141548156738
Epoch 510, val loss: 1.1864428520202637
Epoch 520, training loss: 0.04201488941907883 = 0.03528133034706116 + 0.001 * 6.7335591316223145
Epoch 520, val loss: 1.202962040901184
Epoch 530, training loss: 0.03958668187260628 = 0.03285486251115799 + 0.001 * 6.731818675994873
Epoch 530, val loss: 1.2192307710647583
Epoch 540, training loss: 0.03733721375465393 = 0.030597295612096786 + 0.001 * 6.739916801452637
Epoch 540, val loss: 1.2352685928344727
Epoch 550, training loss: 0.03521735966205597 = 0.028475891798734665 + 0.001 * 6.7414679527282715
Epoch 550, val loss: 1.2510614395141602
Epoch 560, training loss: 0.03318413719534874 = 0.026456987485289574 + 0.001 * 6.727148056030273
Epoch 560, val loss: 1.266671061515808
Epoch 570, training loss: 0.031230930238962173 = 0.024507271125912666 + 0.001 * 6.723659038543701
Epoch 570, val loss: 1.2822065353393555
Epoch 580, training loss: 0.0293745007365942 = 0.022582311183214188 + 0.001 * 6.792189121246338
Epoch 580, val loss: 1.2976861000061035
Epoch 590, training loss: 0.027343522757291794 = 0.02060936577618122 + 0.001 * 6.734155654907227
Epoch 590, val loss: 1.313356876373291
Epoch 600, training loss: 0.025330718606710434 = 0.018606359139084816 + 0.001 * 6.724358558654785
Epoch 600, val loss: 1.3294509649276733
Epoch 610, training loss: 0.023298457264900208 = 0.01657971739768982 + 0.001 * 6.718739986419678
Epoch 610, val loss: 1.3462918996810913
Epoch 620, training loss: 0.02144245244562626 = 0.01471000537276268 + 0.001 * 6.732447147369385
Epoch 620, val loss: 1.3640345335006714
Epoch 630, training loss: 0.019901080057024956 = 0.013174360617995262 + 0.001 * 6.726718425750732
Epoch 630, val loss: 1.3820651769638062
Epoch 640, training loss: 0.018641486763954163 = 0.011921298690140247 + 0.001 * 6.720187187194824
Epoch 640, val loss: 1.3996858596801758
Epoch 650, training loss: 0.0176132470369339 = 0.010891054756939411 + 0.001 * 6.722192764282227
Epoch 650, val loss: 1.4166796207427979
Epoch 660, training loss: 0.01676062121987343 = 0.01004457101225853 + 0.001 * 6.716050624847412
Epoch 660, val loss: 1.4328469038009644
Epoch 670, training loss: 0.01605290360748768 = 0.009337443858385086 + 0.001 * 6.71545934677124
Epoch 670, val loss: 1.4481910467147827
Epoch 680, training loss: 0.015450741164386272 = 0.00873331818729639 + 0.001 * 6.7174224853515625
Epoch 680, val loss: 1.4626749753952026
Epoch 690, training loss: 0.014933845028281212 = 0.008207552134990692 + 0.001 * 6.726292133331299
Epoch 690, val loss: 1.4764009714126587
Epoch 700, training loss: 0.014454526826739311 = 0.007742608431726694 + 0.001 * 6.711917877197266
Epoch 700, val loss: 1.4894200563430786
Epoch 710, training loss: 0.014026814140379429 = 0.0073249624110758305 + 0.001 * 6.7018513679504395
Epoch 710, val loss: 1.501886248588562
Epoch 720, training loss: 0.013687039725482464 = 0.00694612693041563 + 0.001 * 6.740912437438965
Epoch 720, val loss: 1.513861894607544
Epoch 730, training loss: 0.013302180916070938 = 0.006601964123547077 + 0.001 * 6.700216293334961
Epoch 730, val loss: 1.5254417657852173
Epoch 740, training loss: 0.012989070266485214 = 0.006288130301982164 + 0.001 * 6.700939178466797
Epoch 740, val loss: 1.5365707874298096
Epoch 750, training loss: 0.012703316286206245 = 0.0060006314888596535 + 0.001 * 6.702683925628662
Epoch 750, val loss: 1.5473095178604126
Epoch 760, training loss: 0.012442754581570625 = 0.005736049730330706 + 0.001 * 6.706705093383789
Epoch 760, val loss: 1.5576838254928589
Epoch 770, training loss: 0.01218732725828886 = 0.0054915789514780045 + 0.001 * 6.6957478523254395
Epoch 770, val loss: 1.5677241086959839
Epoch 780, training loss: 0.01196327805519104 = 0.005264829844236374 + 0.001 * 6.6984477043151855
Epoch 780, val loss: 1.5774955749511719
Epoch 790, training loss: 0.011744711548089981 = 0.005054061766713858 + 0.001 * 6.690649032592773
Epoch 790, val loss: 1.5869524478912354
Epoch 800, training loss: 0.011563453823328018 = 0.004857651423662901 + 0.001 * 6.705801486968994
Epoch 800, val loss: 1.5961776971817017
Epoch 810, training loss: 0.01136651262640953 = 0.00467434199526906 + 0.001 * 6.692170143127441
Epoch 810, val loss: 1.605128288269043
Epoch 820, training loss: 0.011197197251021862 = 0.004502736963331699 + 0.001 * 6.694459915161133
Epoch 820, val loss: 1.6138505935668945
Epoch 830, training loss: 0.01104564219713211 = 0.004341880325227976 + 0.001 * 6.703761577606201
Epoch 830, val loss: 1.6223547458648682
Epoch 840, training loss: 0.010882627218961716 = 0.004190764855593443 + 0.001 * 6.691861629486084
Epoch 840, val loss: 1.630613923072815
Epoch 850, training loss: 0.010763471946120262 = 0.004048535600304604 + 0.001 * 6.714936256408691
Epoch 850, val loss: 1.6386953592300415
Epoch 860, training loss: 0.010608714073896408 = 0.003914512228220701 + 0.001 * 6.694201946258545
Epoch 860, val loss: 1.6465827226638794
Epoch 870, training loss: 0.01047129463404417 = 0.003788185305893421 + 0.001 * 6.683108806610107
Epoch 870, val loss: 1.6542619466781616
Epoch 880, training loss: 0.010354156605899334 = 0.0036689366679638624 + 0.001 * 6.6852192878723145
Epoch 880, val loss: 1.6617608070373535
Epoch 890, training loss: 0.010242847725749016 = 0.0035560508258640766 + 0.001 * 6.686797142028809
Epoch 890, val loss: 1.6691035032272339
Epoch 900, training loss: 0.010126947425305843 = 0.003449178533628583 + 0.001 * 6.677768230438232
Epoch 900, val loss: 1.6763043403625488
Epoch 910, training loss: 0.01002691499888897 = 0.0033478690311312675 + 0.001 * 6.679045677185059
Epoch 910, val loss: 1.6833223104476929
Epoch 920, training loss: 0.009945767000317574 = 0.003251733724027872 + 0.001 * 6.694032669067383
Epoch 920, val loss: 1.6902141571044922
Epoch 930, training loss: 0.0098467618227005 = 0.0031603919342160225 + 0.001 * 6.686369895935059
Epoch 930, val loss: 1.6969366073608398
Epoch 940, training loss: 0.009743662551045418 = 0.0030735384207218885 + 0.001 * 6.67012357711792
Epoch 940, val loss: 1.7035223245620728
Epoch 950, training loss: 0.009673627093434334 = 0.0029908637516200542 + 0.001 * 6.682763576507568
Epoch 950, val loss: 1.7099803686141968
Epoch 960, training loss: 0.009595668874680996 = 0.002912099240347743 + 0.001 * 6.683568954467773
Epoch 960, val loss: 1.7162998914718628
Epoch 970, training loss: 0.009517991915345192 = 0.00283695082180202 + 0.001 * 6.681041240692139
Epoch 970, val loss: 1.722501516342163
Epoch 980, training loss: 0.009434987790882587 = 0.0027652473654597998 + 0.001 * 6.669740200042725
Epoch 980, val loss: 1.7286368608474731
Epoch 990, training loss: 0.009373120963573456 = 0.0026967504527419806 + 0.001 * 6.676370620727539
Epoch 990, val loss: 1.7345565557479858
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.0627
Flip ASR: 0.0622/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.947981595993042 = 1.939607858657837 + 0.001 * 8.373750686645508
Epoch 0, val loss: 1.9336543083190918
Epoch 10, training loss: 1.9381825923919678 = 1.9298088550567627 + 0.001 * 8.373700141906738
Epoch 10, val loss: 1.9244823455810547
Epoch 20, training loss: 1.9265860319137573 = 1.918212652206421 + 0.001 * 8.3734130859375
Epoch 20, val loss: 1.9132269620895386
Epoch 30, training loss: 1.9106966257095337 = 1.902323842048645 + 0.001 * 8.372791290283203
Epoch 30, val loss: 1.8975169658660889
Epoch 40, training loss: 1.887574315071106 = 1.879202961921692 + 0.001 * 8.371354103088379
Epoch 40, val loss: 1.874679684638977
Epoch 50, training loss: 1.8544400930404663 = 1.8460729122161865 + 0.001 * 8.367179870605469
Epoch 50, val loss: 1.8428350687026978
Epoch 60, training loss: 1.8129849433898926 = 1.8046363592147827 + 0.001 * 8.348569869995117
Epoch 60, val loss: 1.806139349937439
Epoch 70, training loss: 1.7709535360336304 = 1.7627248764038086 + 0.001 * 8.228601455688477
Epoch 70, val loss: 1.772737979888916
Epoch 80, training loss: 1.7219878435134888 = 1.7142374515533447 + 0.001 * 7.75034236907959
Epoch 80, val loss: 1.7320804595947266
Epoch 90, training loss: 1.6539565324783325 = 1.646482229232788 + 0.001 * 7.474270343780518
Epoch 90, val loss: 1.6737900972366333
Epoch 100, training loss: 1.5638185739517212 = 1.55660879611969 + 0.001 * 7.209765434265137
Epoch 100, val loss: 1.5992765426635742
Epoch 110, training loss: 1.455084204673767 = 1.448041558265686 + 0.001 * 7.042603492736816
Epoch 110, val loss: 1.5127389430999756
Epoch 120, training loss: 1.3388071060180664 = 1.3318071365356445 + 0.001 * 6.999996185302734
Epoch 120, val loss: 1.4207714796066284
Epoch 130, training loss: 1.223193883895874 = 1.216233730316162 + 0.001 * 6.960208892822266
Epoch 130, val loss: 1.331874132156372
Epoch 140, training loss: 1.1129462718963623 = 1.1060028076171875 + 0.001 * 6.9434051513671875
Epoch 140, val loss: 1.2486945390701294
Epoch 150, training loss: 1.0117899179458618 = 1.0048619508743286 + 0.001 * 6.927996635437012
Epoch 150, val loss: 1.1732722520828247
Epoch 160, training loss: 0.920352578163147 = 0.9134396910667419 + 0.001 * 6.912874221801758
Epoch 160, val loss: 1.1046451330184937
Epoch 170, training loss: 0.836621105670929 = 0.8297226428985596 + 0.001 * 6.898449897766113
Epoch 170, val loss: 1.0409224033355713
Epoch 180, training loss: 0.7587198615074158 = 0.751833975315094 + 0.001 * 6.885890960693359
Epoch 180, val loss: 0.9812208414077759
Epoch 190, training loss: 0.6863330006599426 = 0.6794564723968506 + 0.001 * 6.876555442810059
Epoch 190, val loss: 0.9261768460273743
Epoch 200, training loss: 0.620173990726471 = 0.6133029460906982 + 0.001 * 6.871018886566162
Epoch 200, val loss: 0.8772891163825989
Epoch 210, training loss: 0.56098473072052 = 0.5541167855262756 + 0.001 * 6.867971420288086
Epoch 210, val loss: 0.8357470035552979
Epoch 220, training loss: 0.5088810324668884 = 0.5020155310630798 + 0.001 * 6.8655104637146
Epoch 220, val loss: 0.8019177913665771
Epoch 230, training loss: 0.46311673521995544 = 0.45625412464141846 + 0.001 * 6.862597465515137
Epoch 230, val loss: 0.7754656672477722
Epoch 240, training loss: 0.4224123954772949 = 0.41555336117744446 + 0.001 * 6.859043121337891
Epoch 240, val loss: 0.7552419304847717
Epoch 250, training loss: 0.38526853919029236 = 0.3784130811691284 + 0.001 * 6.855462551116943
Epoch 250, val loss: 0.740084171295166
Epoch 260, training loss: 0.35042446851730347 = 0.34357354044914246 + 0.001 * 6.850916862487793
Epoch 260, val loss: 0.7286239266395569
Epoch 270, training loss: 0.3171587586402893 = 0.3103126585483551 + 0.001 * 6.846111297607422
Epoch 270, val loss: 0.719876229763031
Epoch 280, training loss: 0.28537440299987793 = 0.278533399105072 + 0.001 * 6.840991020202637
Epoch 280, val loss: 0.7133517265319824
Epoch 290, training loss: 0.25533294677734375 = 0.2484975904226303 + 0.001 * 6.83535623550415
Epoch 290, val loss: 0.7088226079940796
Epoch 300, training loss: 0.2274240106344223 = 0.22059328854084015 + 0.001 * 6.830717086791992
Epoch 300, val loss: 0.7061837315559387
Epoch 310, training loss: 0.20189593732357025 = 0.195073202252388 + 0.001 * 6.822737693786621
Epoch 310, val loss: 0.7054092884063721
Epoch 320, training loss: 0.17888243496418 = 0.1720665991306305 + 0.001 * 6.815832138061523
Epoch 320, val loss: 0.7065662145614624
Epoch 330, training loss: 0.15837755799293518 = 0.1515665054321289 + 0.001 * 6.811044692993164
Epoch 330, val loss: 0.7094472646713257
Epoch 340, training loss: 0.14025989174842834 = 0.1334560215473175 + 0.001 * 6.803874492645264
Epoch 340, val loss: 0.7139670848846436
Epoch 350, training loss: 0.12434534728527069 = 0.11754509061574936 + 0.001 * 6.800256729125977
Epoch 350, val loss: 0.7198651432991028
Epoch 360, training loss: 0.1104004979133606 = 0.10360962152481079 + 0.001 * 6.790879249572754
Epoch 360, val loss: 0.7269042134284973
Epoch 370, training loss: 0.0982208102941513 = 0.0914304181933403 + 0.001 * 6.79039192199707
Epoch 370, val loss: 0.7348289489746094
Epoch 380, training loss: 0.08758320659399033 = 0.08080128580331802 + 0.001 * 6.781919479370117
Epoch 380, val loss: 0.7434165477752686
Epoch 390, training loss: 0.07831069082021713 = 0.07153420895338058 + 0.001 * 6.7764811515808105
Epoch 390, val loss: 0.7525595426559448
Epoch 400, training loss: 0.07027466595172882 = 0.06346431374549866 + 0.001 * 6.810351848602295
Epoch 400, val loss: 0.7620519995689392
Epoch 410, training loss: 0.06323013454675674 = 0.056446608155965805 + 0.001 * 6.7835283279418945
Epoch 410, val loss: 0.7717441320419312
Epoch 420, training loss: 0.05712132900953293 = 0.05035132169723511 + 0.001 * 6.77000617980957
Epoch 420, val loss: 0.7814597487449646
Epoch 430, training loss: 0.05182711035013199 = 0.04505963623523712 + 0.001 * 6.767475128173828
Epoch 430, val loss: 0.7912267446517944
Epoch 440, training loss: 0.04722801223397255 = 0.04046403989195824 + 0.001 * 6.763973236083984
Epoch 440, val loss: 0.8008756637573242
Epoch 450, training loss: 0.04323246330022812 = 0.03647066280245781 + 0.001 * 6.761801242828369
Epoch 450, val loss: 0.8103911876678467
Epoch 460, training loss: 0.03977249562740326 = 0.03299381211400032 + 0.001 * 6.778684616088867
Epoch 460, val loss: 0.819713830947876
Epoch 470, training loss: 0.036725472658872604 = 0.029960034415125847 + 0.001 * 6.765439510345459
Epoch 470, val loss: 0.8287923336029053
Epoch 480, training loss: 0.03406579792499542 = 0.027305055409669876 + 0.001 * 6.760743618011475
Epoch 480, val loss: 0.8376202583312988
Epoch 490, training loss: 0.03173340857028961 = 0.024975113570690155 + 0.001 * 6.758296489715576
Epoch 490, val loss: 0.846216082572937
Epoch 500, training loss: 0.029679495841264725 = 0.022922459989786148 + 0.001 * 6.7570366859436035
Epoch 500, val loss: 0.8546077609062195
Epoch 510, training loss: 0.02786964364349842 = 0.021107077598571777 + 0.001 * 6.762565612792969
Epoch 510, val loss: 0.8627465963363647
Epoch 520, training loss: 0.02625231258571148 = 0.019496334716677666 + 0.001 * 6.755978107452393
Epoch 520, val loss: 0.8706557750701904
Epoch 530, training loss: 0.02481773868203163 = 0.01806170865893364 + 0.001 * 6.756030559539795
Epoch 530, val loss: 0.8783148527145386
Epoch 540, training loss: 0.023532725870609283 = 0.01677941530942917 + 0.001 * 6.753310203552246
Epoch 540, val loss: 0.8857940435409546
Epoch 550, training loss: 0.022410636767745018 = 0.015629354864358902 + 0.001 * 6.781280994415283
Epoch 550, val loss: 0.8930318355560303
Epoch 560, training loss: 0.021350601688027382 = 0.014594691805541515 + 0.001 * 6.755909442901611
Epoch 560, val loss: 0.9000999331474304
Epoch 570, training loss: 0.020412379875779152 = 0.013660605065524578 + 0.001 * 6.751773834228516
Epoch 570, val loss: 0.9069784879684448
Epoch 580, training loss: 0.01956525817513466 = 0.01281486265361309 + 0.001 * 6.75039529800415
Epoch 580, val loss: 0.9136342406272888
Epoch 590, training loss: 0.018796812742948532 = 0.012046827003359795 + 0.001 * 6.749985218048096
Epoch 590, val loss: 0.9201637506484985
Epoch 600, training loss: 0.018103238195180893 = 0.011347413063049316 + 0.001 * 6.755824089050293
Epoch 600, val loss: 0.9264788627624512
Epoch 610, training loss: 0.017458442598581314 = 0.010708872228860855 + 0.001 * 6.749570846557617
Epoch 610, val loss: 0.9326552748680115
Epoch 620, training loss: 0.016872389242053032 = 0.010124399326741695 + 0.001 * 6.747989177703857
Epoch 620, val loss: 0.9386553764343262
Epoch 630, training loss: 0.016344983130693436 = 0.009588096290826797 + 0.001 * 6.756886005401611
Epoch 630, val loss: 0.9445011019706726
Epoch 640, training loss: 0.015843944624066353 = 0.009094895794987679 + 0.001 * 6.749048709869385
Epoch 640, val loss: 0.9502044916152954
Epoch 650, training loss: 0.015386020764708519 = 0.008640442974865437 + 0.001 * 6.745576858520508
Epoch 650, val loss: 0.9557511210441589
Epoch 660, training loss: 0.014965196140110493 = 0.008220769464969635 + 0.001 * 6.744426250457764
Epoch 660, val loss: 0.9611726999282837
Epoch 670, training loss: 0.01457713358104229 = 0.00783250667154789 + 0.001 * 6.744626045227051
Epoch 670, val loss: 0.9664613008499146
Epoch 680, training loss: 0.014227282255887985 = 0.007472596131265163 + 0.001 * 6.75468635559082
Epoch 680, val loss: 0.9716355204582214
Epoch 690, training loss: 0.013879263773560524 = 0.007138353306800127 + 0.001 * 6.740910053253174
Epoch 690, val loss: 0.9766672849655151
Epoch 700, training loss: 0.013574916869401932 = 0.006827411707490683 + 0.001 * 6.747504234313965
Epoch 700, val loss: 0.9815940856933594
Epoch 710, training loss: 0.01327720656991005 = 0.00653767678886652 + 0.001 * 6.739529132843018
Epoch 710, val loss: 0.9863986372947693
Epoch 720, training loss: 0.013011541217565536 = 0.006267320364713669 + 0.001 * 6.744220733642578
Epoch 720, val loss: 0.9910930395126343
Epoch 730, training loss: 0.012752823531627655 = 0.006014665588736534 + 0.001 * 6.738158226013184
Epoch 730, val loss: 0.995689332485199
Epoch 740, training loss: 0.012512429617345333 = 0.005778192542493343 + 0.001 * 6.734236717224121
Epoch 740, val loss: 1.0001827478408813
Epoch 750, training loss: 0.012290336191654205 = 0.005556552670896053 + 0.001 * 6.733782768249512
Epoch 750, val loss: 1.0045572519302368
Epoch 760, training loss: 0.012087411247193813 = 0.005348498467355967 + 0.001 * 6.738912582397461
Epoch 760, val loss: 1.008834958076477
Epoch 770, training loss: 0.01188956294208765 = 0.005152983125299215 + 0.001 * 6.736579418182373
Epoch 770, val loss: 1.013033390045166
Epoch 780, training loss: 0.01171162724494934 = 0.004969008266925812 + 0.001 * 6.742619037628174
Epoch 780, val loss: 1.017134189605713
Epoch 790, training loss: 0.011527633294463158 = 0.004795705433934927 + 0.001 * 6.731926918029785
Epoch 790, val loss: 1.0211375951766968
Epoch 800, training loss: 0.011359315365552902 = 0.0046322583220899105 + 0.001 * 6.72705602645874
Epoch 800, val loss: 1.0250635147094727
Epoch 810, training loss: 0.011217504739761353 = 0.004477915819734335 + 0.001 * 6.739588260650635
Epoch 810, val loss: 1.0289100408554077
Epoch 820, training loss: 0.011059684678912163 = 0.004332013428211212 + 0.001 * 6.7276716232299805
Epoch 820, val loss: 1.0326807498931885
Epoch 830, training loss: 0.010917825624346733 = 0.0041939555667340755 + 0.001 * 6.723870277404785
Epoch 830, val loss: 1.0363744497299194
Epoch 840, training loss: 0.010807891376316547 = 0.004063199739903212 + 0.001 * 6.744691371917725
Epoch 840, val loss: 1.0399882793426514
Epoch 850, training loss: 0.01065717451274395 = 0.00393921323120594 + 0.001 * 6.717960357666016
Epoch 850, val loss: 1.0435365438461304
Epoch 860, training loss: 0.010544337332248688 = 0.0038215555250644684 + 0.001 * 6.722781181335449
Epoch 860, val loss: 1.0470134019851685
Epoch 870, training loss: 0.010441737249493599 = 0.0037097912281751633 + 0.001 * 6.731945037841797
Epoch 870, val loss: 1.050413727760315
Epoch 880, training loss: 0.010326635092496872 = 0.0036035380326211452 + 0.001 * 6.72309684753418
Epoch 880, val loss: 1.0537737607955933
Epoch 890, training loss: 0.010224263183772564 = 0.0035024278331547976 + 0.001 * 6.721835136413574
Epoch 890, val loss: 1.0570513010025024
Epoch 900, training loss: 0.010118095204234123 = 0.003406150033697486 + 0.001 * 6.711944580078125
Epoch 900, val loss: 1.0602771043777466
Epoch 910, training loss: 0.010034623555839062 = 0.0033144091721624136 + 0.001 * 6.720213890075684
Epoch 910, val loss: 1.0634433031082153
Epoch 920, training loss: 0.00993338879197836 = 0.003226895583793521 + 0.001 * 6.7064924240112305
Epoch 920, val loss: 1.066555380821228
Epoch 930, training loss: 0.009848584420979023 = 0.0031433713156729937 + 0.001 * 6.7052130699157715
Epoch 930, val loss: 1.0696033239364624
Epoch 940, training loss: 0.009767022915184498 = 0.0030636044684797525 + 0.001 * 6.703417778015137
Epoch 940, val loss: 1.0725975036621094
Epoch 950, training loss: 0.009700906462967396 = 0.0029873510356992483 + 0.001 * 6.713555335998535
Epoch 950, val loss: 1.0755449533462524
Epoch 960, training loss: 0.009633401408791542 = 0.00291440705768764 + 0.001 * 6.718993663787842
Epoch 960, val loss: 1.0784480571746826
Epoch 970, training loss: 0.009557534009218216 = 0.0028445711359381676 + 0.001 * 6.712962627410889
Epoch 970, val loss: 1.0812708139419556
Epoch 980, training loss: 0.009477801620960236 = 0.002777695655822754 + 0.001 * 6.700105667114258
Epoch 980, val loss: 1.0840500593185425
Epoch 990, training loss: 0.009410550817847252 = 0.0027136008720844984 + 0.001 * 6.696949481964111
Epoch 990, val loss: 1.0867884159088135
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8266
Flip ASR: 0.7911/225 nodes
The final ASR:0.47478, 0.31473, Accuracy:0.81728, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11582])
remove edge: torch.Size([2, 9542])
updated graph: torch.Size([2, 10568])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97909, 0.00460, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9408011436462402 = 1.9324274063110352 + 0.001 * 8.373758316040039
Epoch 0, val loss: 1.9372727870941162
Epoch 10, training loss: 1.9314887523651123 = 1.9231151342391968 + 0.001 * 8.373672485351562
Epoch 10, val loss: 1.9276355504989624
Epoch 20, training loss: 1.9202524423599243 = 1.911879062652588 + 0.001 * 8.373340606689453
Epoch 20, val loss: 1.915878176689148
Epoch 30, training loss: 1.9048488140106201 = 1.8964762687683105 + 0.001 * 8.372590065002441
Epoch 30, val loss: 1.8998922109603882
Epoch 40, training loss: 1.8826234340667725 = 1.8742525577545166 + 0.001 * 8.37086296081543
Epoch 40, val loss: 1.8772194385528564
Epoch 50, training loss: 1.851383090019226 = 1.8430172204971313 + 0.001 * 8.365885734558105
Epoch 50, val loss: 1.846635341644287
Epoch 60, training loss: 1.8124098777770996 = 1.804065465927124 + 0.001 * 8.344449043273926
Epoch 60, val loss: 1.8113372325897217
Epoch 70, training loss: 1.7705631256103516 = 1.7623631954193115 + 0.001 * 8.199970245361328
Epoch 70, val loss: 1.777020812034607
Epoch 80, training loss: 1.7195101976394653 = 1.7119032144546509 + 0.001 * 7.607028007507324
Epoch 80, val loss: 1.7346702814102173
Epoch 90, training loss: 1.649032473564148 = 1.6416994333267212 + 0.001 * 7.3330841064453125
Epoch 90, val loss: 1.6741085052490234
Epoch 100, training loss: 1.5552778244018555 = 1.5481421947479248 + 0.001 * 7.135685443878174
Epoch 100, val loss: 1.5940228700637817
Epoch 110, training loss: 1.4409867525100708 = 1.4339264631271362 + 0.001 * 7.06027364730835
Epoch 110, val loss: 1.4989534616470337
Epoch 120, training loss: 1.314699411392212 = 1.3076746463775635 + 0.001 * 7.024778842926025
Epoch 120, val loss: 1.3939166069030762
Epoch 130, training loss: 1.184779167175293 = 1.1777838468551636 + 0.001 * 6.995325565338135
Epoch 130, val loss: 1.2869576215744019
Epoch 140, training loss: 1.0578569173812866 = 1.050887942314148 + 0.001 * 6.969025611877441
Epoch 140, val loss: 1.1832499504089355
Epoch 150, training loss: 0.9397457242012024 = 0.9328078627586365 + 0.001 * 6.93783712387085
Epoch 150, val loss: 1.088196873664856
Epoch 160, training loss: 0.8336348533630371 = 0.8267319798469543 + 0.001 * 6.902846336364746
Epoch 160, val loss: 1.004251480102539
Epoch 170, training loss: 0.740039050579071 = 0.733163595199585 + 0.001 * 6.875448226928711
Epoch 170, val loss: 0.9324551820755005
Epoch 180, training loss: 0.6586105227470398 = 0.6517468690872192 + 0.001 * 6.8636794090271
Epoch 180, val loss: 0.8723402619361877
Epoch 190, training loss: 0.5888660550117493 = 0.5820058584213257 + 0.001 * 6.860196113586426
Epoch 190, val loss: 0.8238095045089722
Epoch 200, training loss: 0.5298731923103333 = 0.5230160355567932 + 0.001 * 6.8571295738220215
Epoch 200, val loss: 0.786202609539032
Epoch 210, training loss: 0.4797895550727844 = 0.4729355573654175 + 0.001 * 6.854005336761475
Epoch 210, val loss: 0.757763147354126
Epoch 220, training loss: 0.43616440892219543 = 0.42931339144706726 + 0.001 * 6.851021766662598
Epoch 220, val loss: 0.7361109256744385
Epoch 230, training loss: 0.39659231901168823 = 0.38974452018737793 + 0.001 * 6.847795009613037
Epoch 230, val loss: 0.7188252210617065
Epoch 240, training loss: 0.3593844175338745 = 0.3525402247905731 + 0.001 * 6.844181060791016
Epoch 240, val loss: 0.7041615843772888
Epoch 250, training loss: 0.3238024115562439 = 0.316962331533432 + 0.001 * 6.840066909790039
Epoch 250, val loss: 0.6916601061820984
Epoch 260, training loss: 0.2898336946964264 = 0.28299832344055176 + 0.001 * 6.83536958694458
Epoch 260, val loss: 0.6813097596168518
Epoch 270, training loss: 0.25778067111968994 = 0.2509506642818451 + 0.001 * 6.829991817474365
Epoch 270, val loss: 0.673146665096283
Epoch 280, training loss: 0.22785629332065582 = 0.2210308015346527 + 0.001 * 6.825488090515137
Epoch 280, val loss: 0.667172372341156
Epoch 290, training loss: 0.20024637877941132 = 0.19342736899852753 + 0.001 * 6.819011688232422
Epoch 290, val loss: 0.663391649723053
Epoch 300, training loss: 0.17519083619117737 = 0.16837899386882782 + 0.001 * 6.811835289001465
Epoch 300, val loss: 0.6620761156082153
Epoch 310, training loss: 0.1530182808637619 = 0.14620909094810486 + 0.001 * 6.809185981750488
Epoch 310, val loss: 0.6631852984428406
Epoch 320, training loss: 0.133894681930542 = 0.12710043787956238 + 0.001 * 6.794245719909668
Epoch 320, val loss: 0.6667956709861755
Epoch 330, training loss: 0.11771316826343536 = 0.11093169450759888 + 0.001 * 6.781469821929932
Epoch 330, val loss: 0.672699511051178
Epoch 340, training loss: 0.10410812497138977 = 0.09733766317367554 + 0.001 * 6.7704596519470215
Epoch 340, val loss: 0.6806460618972778
Epoch 350, training loss: 0.09262847155332565 = 0.08587202429771423 + 0.001 * 6.756443500518799
Epoch 350, val loss: 0.6901412010192871
Epoch 360, training loss: 0.08286763727664948 = 0.07612797617912292 + 0.001 * 6.739661693572998
Epoch 360, val loss: 0.7007951140403748
Epoch 370, training loss: 0.07450637966394424 = 0.06777778267860413 + 0.001 * 6.728597164154053
Epoch 370, val loss: 0.7122418284416199
Epoch 380, training loss: 0.06729795783758163 = 0.06057354435324669 + 0.001 * 6.724410057067871
Epoch 380, val loss: 0.7242182493209839
Epoch 390, training loss: 0.0610395222902298 = 0.05432368442416191 + 0.001 * 6.715837478637695
Epoch 390, val loss: 0.7364062666893005
Epoch 400, training loss: 0.05558684095740318 = 0.04887215048074722 + 0.001 * 6.714688777923584
Epoch 400, val loss: 0.7486785054206848
Epoch 410, training loss: 0.050817642360925674 = 0.044106848537921906 + 0.001 * 6.710793972015381
Epoch 410, val loss: 0.7608867287635803
Epoch 420, training loss: 0.04663049429655075 = 0.039932847023010254 + 0.001 * 6.697646617889404
Epoch 420, val loss: 0.7730374932289124
Epoch 430, training loss: 0.04297143220901489 = 0.03627456724643707 + 0.001 * 6.696863651275635
Epoch 430, val loss: 0.785055935382843
Epoch 440, training loss: 0.03975515440106392 = 0.033057987689971924 + 0.001 * 6.6971659660339355
Epoch 440, val loss: 0.7968330383300781
Epoch 450, training loss: 0.03690807521343231 = 0.03021858260035515 + 0.001 * 6.689490795135498
Epoch 450, val loss: 0.8083829283714294
Epoch 460, training loss: 0.034394893795251846 = 0.027705853804945946 + 0.001 * 6.689039707183838
Epoch 460, val loss: 0.8197423219680786
Epoch 470, training loss: 0.032162006944417953 = 0.02547609433531761 + 0.001 * 6.685911655426025
Epoch 470, val loss: 0.8307815194129944
Epoch 480, training loss: 0.03018016740679741 = 0.02349007874727249 + 0.001 * 6.690088272094727
Epoch 480, val loss: 0.841559112071991
Epoch 490, training loss: 0.028398433700203896 = 0.021716704592108727 + 0.001 * 6.681729316711426
Epoch 490, val loss: 0.8520515561103821
Epoch 500, training loss: 0.026811569929122925 = 0.020127607509493828 + 0.001 * 6.683961391448975
Epoch 500, val loss: 0.8623313903808594
Epoch 510, training loss: 0.025376854464411736 = 0.018698904663324356 + 0.001 * 6.677948951721191
Epoch 510, val loss: 0.8722898960113525
Epoch 520, training loss: 0.024088485166430473 = 0.01741151511669159 + 0.001 * 6.676969051361084
Epoch 520, val loss: 0.8819786310195923
Epoch 530, training loss: 0.022940322756767273 = 0.01624898612499237 + 0.001 * 6.691336631774902
Epoch 530, val loss: 0.8913952708244324
Epoch 540, training loss: 0.021874215453863144 = 0.015196708962321281 + 0.001 * 6.677505970001221
Epoch 540, val loss: 0.9005897045135498
Epoch 550, training loss: 0.020916132256388664 = 0.01424308493733406 + 0.001 * 6.6730475425720215
Epoch 550, val loss: 0.9095067977905273
Epoch 560, training loss: 0.02004934847354889 = 0.013375570066273212 + 0.001 * 6.673778533935547
Epoch 560, val loss: 0.9182544350624084
Epoch 570, training loss: 0.019255265593528748 = 0.012584012933075428 + 0.001 * 6.6712517738342285
Epoch 570, val loss: 0.9267289042472839
Epoch 580, training loss: 0.01852790266275406 = 0.011860133148729801 + 0.001 * 6.667769432067871
Epoch 580, val loss: 0.9349582195281982
Epoch 590, training loss: 0.017874760553240776 = 0.011197288520634174 + 0.001 * 6.677471160888672
Epoch 590, val loss: 0.9429950714111328
Epoch 600, training loss: 0.017260154709219933 = 0.01058925874531269 + 0.001 * 6.670895099639893
Epoch 600, val loss: 0.95083087682724
Epoch 610, training loss: 0.01669512689113617 = 0.010030282661318779 + 0.001 * 6.6648430824279785
Epoch 610, val loss: 0.9584581255912781
Epoch 620, training loss: 0.01618199422955513 = 0.009516310878098011 + 0.001 * 6.665682792663574
Epoch 620, val loss: 0.965850830078125
Epoch 630, training loss: 0.015705786645412445 = 0.009041807614266872 + 0.001 * 6.663979530334473
Epoch 630, val loss: 0.9731020331382751
Epoch 640, training loss: 0.015266770496964455 = 0.008602920919656754 + 0.001 * 6.663848876953125
Epoch 640, val loss: 0.9801576733589172
Epoch 650, training loss: 0.01486346684396267 = 0.008196375332772732 + 0.001 * 6.667090892791748
Epoch 650, val loss: 0.9870579242706299
Epoch 660, training loss: 0.014480332843959332 = 0.007819072343409061 + 0.001 * 6.66126012802124
Epoch 660, val loss: 0.9937772154808044
Epoch 670, training loss: 0.014134110882878304 = 0.007468316238373518 + 0.001 * 6.6657938957214355
Epoch 670, val loss: 1.000317096710205
Epoch 680, training loss: 0.013800963759422302 = 0.0071417042054235935 + 0.001 * 6.659258842468262
Epoch 680, val loss: 1.0067070722579956
Epoch 690, training loss: 0.013500463217496872 = 0.006837103515863419 + 0.001 * 6.66335916519165
Epoch 690, val loss: 1.0129441022872925
Epoch 700, training loss: 0.013215329498052597 = 0.006552650593221188 + 0.001 * 6.6626787185668945
Epoch 700, val loss: 1.01902174949646
Epoch 710, training loss: 0.01295284740626812 = 0.006286675110459328 + 0.001 * 6.666172027587891
Epoch 710, val loss: 1.0249536037445068
Epoch 720, training loss: 0.012692905031144619 = 0.006037591025233269 + 0.001 * 6.655313491821289
Epoch 720, val loss: 1.0307624340057373
Epoch 730, training loss: 0.012466652318835258 = 0.005804033484309912 + 0.001 * 6.662619113922119
Epoch 730, val loss: 1.0364456176757812
Epoch 740, training loss: 0.012238009832799435 = 0.005584767088294029 + 0.001 * 6.653242588043213
Epoch 740, val loss: 1.0419594049453735
Epoch 750, training loss: 0.012032409198582172 = 0.005378641188144684 + 0.001 * 6.6537675857543945
Epoch 750, val loss: 1.0473963022232056
Epoch 760, training loss: 0.01184321939945221 = 0.00518457917496562 + 0.001 * 6.658639430999756
Epoch 760, val loss: 1.0526962280273438
Epoch 770, training loss: 0.011653171852231026 = 0.005001681856811047 + 0.001 * 6.6514892578125
Epoch 770, val loss: 1.0578869581222534
Epoch 780, training loss: 0.01151477824896574 = 0.004829172044992447 + 0.001 * 6.685606002807617
Epoch 780, val loss: 1.0629656314849854
Epoch 790, training loss: 0.011322435922920704 = 0.004666291177272797 + 0.001 * 6.656144618988037
Epoch 790, val loss: 1.0679306983947754
Epoch 800, training loss: 0.011164319701492786 = 0.004512346815317869 + 0.001 * 6.651972770690918
Epoch 800, val loss: 1.0728111267089844
Epoch 810, training loss: 0.011017704382538795 = 0.004366684705018997 + 0.001 * 6.651019096374512
Epoch 810, val loss: 1.0775824785232544
Epoch 820, training loss: 0.010880209505558014 = 0.004228697624057531 + 0.001 * 6.6515116691589355
Epoch 820, val loss: 1.082241177558899
Epoch 830, training loss: 0.010748145170509815 = 0.004097873345017433 + 0.001 * 6.650271415710449
Epoch 830, val loss: 1.0868076086044312
Epoch 840, training loss: 0.010626927949488163 = 0.003973736427724361 + 0.001 * 6.653191089630127
Epoch 840, val loss: 1.091293454170227
Epoch 850, training loss: 0.010514607653021812 = 0.0038558458909392357 + 0.001 * 6.658761978149414
Epoch 850, val loss: 1.0956851243972778
Epoch 860, training loss: 0.010386345908045769 = 0.003743786597624421 + 0.001 * 6.642559051513672
Epoch 860, val loss: 1.0999870300292969
Epoch 870, training loss: 0.010280067101120949 = 0.003637156914919615 + 0.001 * 6.642910480499268
Epoch 870, val loss: 1.1041938066482544
Epoch 880, training loss: 0.010189132764935493 = 0.003535627154633403 + 0.001 * 6.653505325317383
Epoch 880, val loss: 1.1083416938781738
Epoch 890, training loss: 0.010084062814712524 = 0.0034388680942356586 + 0.001 * 6.6451945304870605
Epoch 890, val loss: 1.1123919486999512
Epoch 900, training loss: 0.0099948700517416 = 0.003346601501107216 + 0.001 * 6.648268699645996
Epoch 900, val loss: 1.1163666248321533
Epoch 910, training loss: 0.00990869291126728 = 0.0032585663720965385 + 0.001 * 6.6501264572143555
Epoch 910, val loss: 1.1202585697174072
Epoch 920, training loss: 0.009816051460802555 = 0.0031744863372296095 + 0.001 * 6.641564846038818
Epoch 920, val loss: 1.1240851879119873
Epoch 930, training loss: 0.009730962105095387 = 0.003094132523983717 + 0.001 * 6.636829376220703
Epoch 930, val loss: 1.1278364658355713
Epoch 940, training loss: 0.00967591255903244 = 0.0030172925908118486 + 0.001 * 6.658619403839111
Epoch 940, val loss: 1.131515383720398
Epoch 950, training loss: 0.009582972154021263 = 0.002943773753941059 + 0.001 * 6.6391987800598145
Epoch 950, val loss: 1.1351228952407837
Epoch 960, training loss: 0.009511330164968967 = 0.0028733813669532537 + 0.001 * 6.637948036193848
Epoch 960, val loss: 1.1386553049087524
Epoch 970, training loss: 0.009453188627958298 = 0.002805933589115739 + 0.001 * 6.647254943847656
Epoch 970, val loss: 1.1421412229537964
Epoch 980, training loss: 0.009376989677548409 = 0.0027412627823650837 + 0.001 * 6.635726451873779
Epoch 980, val loss: 1.1455392837524414
Epoch 990, training loss: 0.009321227669715881 = 0.0026792362332344055 + 0.001 * 6.641991138458252
Epoch 990, val loss: 1.1489160060882568
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6494
Flip ASR: 0.5822/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9486440420150757 = 1.9402703046798706 + 0.001 * 8.373743057250977
Epoch 0, val loss: 1.9277633428573608
Epoch 10, training loss: 1.9386157989501953 = 1.9302421808242798 + 0.001 * 8.373658180236816
Epoch 10, val loss: 1.9180148839950562
Epoch 20, training loss: 1.9266494512557983 = 1.9182761907577515 + 0.001 * 8.373315811157227
Epoch 20, val loss: 1.905975103378296
Epoch 30, training loss: 1.9102158546447754 = 1.9018431901931763 + 0.001 * 8.372610092163086
Epoch 30, val loss: 1.8891087770462036
Epoch 40, training loss: 1.886269450187683 = 1.8778983354568481 + 0.001 * 8.37106704711914
Epoch 40, val loss: 1.8645497560501099
Epoch 50, training loss: 1.8520567417144775 = 1.8436899185180664 + 0.001 * 8.366803169250488
Epoch 50, val loss: 1.8305644989013672
Epoch 60, training loss: 1.8088053464889526 = 1.8004556894302368 + 0.001 * 8.34965991973877
Epoch 60, val loss: 1.790509819984436
Epoch 70, training loss: 1.7613168954849243 = 1.753080129623413 + 0.001 * 8.236786842346191
Epoch 70, val loss: 1.7494012117385864
Epoch 80, training loss: 1.701214075088501 = 1.6935142278671265 + 0.001 * 7.699843883514404
Epoch 80, val loss: 1.696412444114685
Epoch 90, training loss: 1.6211069822311401 = 1.6134980916976929 + 0.001 * 7.6088385581970215
Epoch 90, val loss: 1.6263898611068726
Epoch 100, training loss: 1.5239018201828003 = 1.516342043876648 + 0.001 * 7.559817314147949
Epoch 100, val loss: 1.546037197113037
Epoch 110, training loss: 1.4237812757492065 = 1.416327953338623 + 0.001 * 7.453334808349609
Epoch 110, val loss: 1.4669772386550903
Epoch 120, training loss: 1.3321017026901245 = 1.3248945474624634 + 0.001 * 7.207198619842529
Epoch 120, val loss: 1.4012784957885742
Epoch 130, training loss: 1.2521588802337646 = 1.2450181245803833 + 0.001 * 7.140730381011963
Epoch 130, val loss: 1.35101318359375
Epoch 140, training loss: 1.1824572086334229 = 1.1753746271133423 + 0.001 * 7.082528114318848
Epoch 140, val loss: 1.3127903938293457
Epoch 150, training loss: 1.1184403896331787 = 1.1114152669906616 + 0.001 * 7.0250749588012695
Epoch 150, val loss: 1.2806822061538696
Epoch 160, training loss: 1.0553691387176514 = 1.0483890771865845 + 0.001 * 6.980105876922607
Epoch 160, val loss: 1.2503293752670288
Epoch 170, training loss: 0.9907328486442566 = 0.9837830662727356 + 0.001 * 6.949760437011719
Epoch 170, val loss: 1.2188055515289307
Epoch 180, training loss: 0.9241817593574524 = 0.9172551035881042 + 0.001 * 6.92665958404541
Epoch 180, val loss: 1.1843650341033936
Epoch 190, training loss: 0.8560618758201599 = 0.8491578698158264 + 0.001 * 6.903993606567383
Epoch 190, val loss: 1.1470221281051636
Epoch 200, training loss: 0.7869171500205994 = 0.7800343036651611 + 0.001 * 6.882853031158447
Epoch 200, val loss: 1.1071147918701172
Epoch 210, training loss: 0.7178112268447876 = 0.7109456062316895 + 0.001 * 6.86562442779541
Epoch 210, val loss: 1.0656325817108154
Epoch 220, training loss: 0.6502954363822937 = 0.6434423327445984 + 0.001 * 6.85310697555542
Epoch 220, val loss: 1.0248662233352661
Epoch 230, training loss: 0.585752546787262 = 0.5789079070091248 + 0.001 * 6.844655990600586
Epoch 230, val loss: 0.9869673252105713
Epoch 240, training loss: 0.5253423452377319 = 0.5185036063194275 + 0.001 * 6.838723182678223
Epoch 240, val loss: 0.9539418816566467
Epoch 250, training loss: 0.4698202610015869 = 0.4629859924316406 + 0.001 * 6.8342790603637695
Epoch 250, val loss: 0.9268855452537537
Epoch 260, training loss: 0.41930150985717773 = 0.41247087717056274 + 0.001 * 6.830644130706787
Epoch 260, val loss: 0.9064969420433044
Epoch 270, training loss: 0.3735274076461792 = 0.366700142621994 + 0.001 * 6.827258586883545
Epoch 270, val loss: 0.891838014125824
Epoch 280, training loss: 0.3321235775947571 = 0.32529938220977783 + 0.001 * 6.824180603027344
Epoch 280, val loss: 0.8821386694908142
Epoch 290, training loss: 0.29473254084587097 = 0.28791144490242004 + 0.001 * 6.821105480194092
Epoch 290, val loss: 0.8769534826278687
Epoch 300, training loss: 0.261223167181015 = 0.2544057071208954 + 0.001 * 6.817465782165527
Epoch 300, val loss: 0.8752196431159973
Epoch 310, training loss: 0.231475830078125 = 0.22466273605823517 + 0.001 * 6.813086986541748
Epoch 310, val loss: 0.875767707824707
Epoch 320, training loss: 0.20525388419628143 = 0.19844399392604828 + 0.001 * 6.809895992279053
Epoch 320, val loss: 0.8781148195266724
Epoch 330, training loss: 0.18227611482143402 = 0.17547482252120972 + 0.001 * 6.801293849945068
Epoch 330, val loss: 0.8821045756340027
Epoch 340, training loss: 0.16221342980861664 = 0.15541823208332062 + 0.001 * 6.795193672180176
Epoch 340, val loss: 0.8877413868904114
Epoch 350, training loss: 0.14469373226165771 = 0.13790659606456757 + 0.001 * 6.7871317863464355
Epoch 350, val loss: 0.8947404026985168
Epoch 360, training loss: 0.12938331067562103 = 0.12260767072439194 + 0.001 * 6.775637626647949
Epoch 360, val loss: 0.9029307961463928
Epoch 370, training loss: 0.11598140001296997 = 0.1092129722237587 + 0.001 * 6.768426895141602
Epoch 370, val loss: 0.9120689630508423
Epoch 380, training loss: 0.1042139083147049 = 0.09744947403669357 + 0.001 * 6.764430522918701
Epoch 380, val loss: 0.9217780828475952
Epoch 390, training loss: 0.09385973960161209 = 0.0871070995926857 + 0.001 * 6.7526421546936035
Epoch 390, val loss: 0.9320288300514221
Epoch 400, training loss: 0.08475076407194138 = 0.07800321280956268 + 0.001 * 6.747553825378418
Epoch 400, val loss: 0.9426852464675903
Epoch 410, training loss: 0.07671892642974854 = 0.06997274607419968 + 0.001 * 6.746177673339844
Epoch 410, val loss: 0.953740119934082
Epoch 420, training loss: 0.06962832808494568 = 0.06288396567106247 + 0.001 * 6.744365215301514
Epoch 420, val loss: 0.9649925827980042
Epoch 430, training loss: 0.06336452066898346 = 0.056621648371219635 + 0.001 * 6.7428717613220215
Epoch 430, val loss: 0.9764459729194641
Epoch 440, training loss: 0.0578339621424675 = 0.05109434947371483 + 0.001 * 6.739614009857178
Epoch 440, val loss: 0.9881036877632141
Epoch 450, training loss: 0.05295998603105545 = 0.04622361809015274 + 0.001 * 6.736367702484131
Epoch 450, val loss: 0.9998611807823181
Epoch 460, training loss: 0.04866301640868187 = 0.0419277586042881 + 0.001 * 6.735256195068359
Epoch 460, val loss: 1.0115646123886108
Epoch 470, training loss: 0.044877707958221436 = 0.03813821077346802 + 0.001 * 6.739497661590576
Epoch 470, val loss: 1.0233179330825806
Epoch 480, training loss: 0.04152296110987663 = 0.03478919342160225 + 0.001 * 6.733767032623291
Epoch 480, val loss: 1.034929633140564
Epoch 490, training loss: 0.03855232521891594 = 0.03182288631796837 + 0.001 * 6.729438304901123
Epoch 490, val loss: 1.0464402437210083
Epoch 500, training loss: 0.03592326119542122 = 0.0291890949010849 + 0.001 * 6.734167575836182
Epoch 500, val loss: 1.057779312133789
Epoch 510, training loss: 0.033571064472198486 = 0.02684456668794155 + 0.001 * 6.726495742797852
Epoch 510, val loss: 1.068955898284912
Epoch 520, training loss: 0.031481046229600906 = 0.024752505123615265 + 0.001 * 6.728540420532227
Epoch 520, val loss: 1.0799616575241089
Epoch 530, training loss: 0.02960553765296936 = 0.022881165146827698 + 0.001 * 6.724372386932373
Epoch 530, val loss: 1.0906622409820557
Epoch 540, training loss: 0.02793295308947563 = 0.021202847361564636 + 0.001 * 6.730104923248291
Epoch 540, val loss: 1.1012495756149292
Epoch 550, training loss: 0.026415802538394928 = 0.019694004207849503 + 0.001 * 6.721798896789551
Epoch 550, val loss: 1.1115537881851196
Epoch 560, training loss: 0.025052877143025398 = 0.018333952873945236 + 0.001 * 6.718923568725586
Epoch 560, val loss: 1.1216892004013062
Epoch 570, training loss: 0.023821625858545303 = 0.017105115577578545 + 0.001 * 6.716509819030762
Epoch 570, val loss: 1.131557583808899
Epoch 580, training loss: 0.02270720899105072 = 0.015990372747182846 + 0.001 * 6.716836452484131
Epoch 580, val loss: 1.1412222385406494
Epoch 590, training loss: 0.02168942429125309 = 0.014977996237576008 + 0.001 * 6.711427688598633
Epoch 590, val loss: 1.1506633758544922
Epoch 600, training loss: 0.020767224952578545 = 0.0140570979565382 + 0.001 * 6.7101263999938965
Epoch 600, val loss: 1.1598780155181885
Epoch 610, training loss: 0.019931411370635033 = 0.013217714615166187 + 0.001 * 6.713696002960205
Epoch 610, val loss: 1.1688969135284424
Epoch 620, training loss: 0.019155891612172127 = 0.012450998649001122 + 0.001 * 6.704892158508301
Epoch 620, val loss: 1.177683711051941
Epoch 630, training loss: 0.018467076122760773 = 0.01174916047602892 + 0.001 * 6.717915058135986
Epoch 630, val loss: 1.1862558126449585
Epoch 640, training loss: 0.017806151881814003 = 0.011105453595519066 + 0.001 * 6.700697422027588
Epoch 640, val loss: 1.1946029663085938
Epoch 650, training loss: 0.017212025821208954 = 0.010513919405639172 + 0.001 * 6.698105335235596
Epoch 650, val loss: 1.2027522325515747
Epoch 660, training loss: 0.016679584980010986 = 0.009969267062842846 + 0.001 * 6.710318088531494
Epoch 660, val loss: 1.2106938362121582
Epoch 670, training loss: 0.016162432730197906 = 0.009466818533837795 + 0.001 * 6.695613861083984
Epoch 670, val loss: 1.2184438705444336
Epoch 680, training loss: 0.015690026804804802 = 0.009002425707876682 + 0.001 * 6.687600612640381
Epoch 680, val loss: 1.2260148525238037
Epoch 690, training loss: 0.015324501320719719 = 0.008572532795369625 + 0.001 * 6.751968860626221
Epoch 690, val loss: 1.2333993911743164
Epoch 700, training loss: 0.014867241494357586 = 0.008174005895853043 + 0.001 * 6.693235397338867
Epoch 700, val loss: 1.2405879497528076
Epoch 710, training loss: 0.014487799257040024 = 0.007803840097039938 + 0.001 * 6.683959484100342
Epoch 710, val loss: 1.247613787651062
Epoch 720, training loss: 0.014140497893095016 = 0.007459461688995361 + 0.001 * 6.681036472320557
Epoch 720, val loss: 1.2545019388198853
Epoch 730, training loss: 0.013848982751369476 = 0.007138561457395554 + 0.001 * 6.710421085357666
Epoch 730, val loss: 1.2612107992172241
Epoch 740, training loss: 0.013522425666451454 = 0.006839054636657238 + 0.001 * 6.683370113372803
Epoch 740, val loss: 1.2677580118179321
Epoch 750, training loss: 0.013235852122306824 = 0.006559167522937059 + 0.001 * 6.676684856414795
Epoch 750, val loss: 1.274144172668457
Epoch 760, training loss: 0.01299954205751419 = 0.006297108251601458 + 0.001 * 6.7024335861206055
Epoch 760, val loss: 1.2804076671600342
Epoch 770, training loss: 0.012719297781586647 = 0.0060514723882079124 + 0.001 * 6.667825698852539
Epoch 770, val loss: 1.2864938974380493
Epoch 780, training loss: 0.012498192489147186 = 0.005820970516651869 + 0.001 * 6.67722225189209
Epoch 780, val loss: 1.2924549579620361
Epoch 790, training loss: 0.012270081788301468 = 0.005604425445199013 + 0.001 * 6.665656566619873
Epoch 790, val loss: 1.2982851266860962
Epoch 800, training loss: 0.012064936570823193 = 0.0054007116705179214 + 0.001 * 6.664224624633789
Epoch 800, val loss: 1.3039822578430176
Epoch 810, training loss: 0.011871205642819405 = 0.005208869464695454 + 0.001 * 6.662335395812988
Epoch 810, val loss: 1.3095624446868896
Epoch 820, training loss: 0.011712546460330486 = 0.005028010345995426 + 0.001 * 6.684535980224609
Epoch 820, val loss: 1.3149993419647217
Epoch 830, training loss: 0.011524038389325142 = 0.004857331048697233 + 0.001 * 6.6667070388793945
Epoch 830, val loss: 1.3203397989273071
Epoch 840, training loss: 0.011360341683030128 = 0.0046960641629993916 + 0.001 * 6.664276599884033
Epoch 840, val loss: 1.3255656957626343
Epoch 850, training loss: 0.01125028170645237 = 0.004543546587228775 + 0.001 * 6.706734657287598
Epoch 850, val loss: 1.3306962251663208
Epoch 860, training loss: 0.011064756661653519 = 0.004399159923195839 + 0.001 * 6.665596008300781
Epoch 860, val loss: 1.3356951475143433
Epoch 870, training loss: 0.010917356237769127 = 0.004262364003807306 + 0.001 * 6.654992580413818
Epoch 870, val loss: 1.3405925035476685
Epoch 880, training loss: 0.01078803464770317 = 0.004132615868002176 + 0.001 * 6.655418872833252
Epoch 880, val loss: 1.3454025983810425
Epoch 890, training loss: 0.010676922276616096 = 0.0040093474090099335 + 0.001 * 6.667574405670166
Epoch 890, val loss: 1.350106954574585
Epoch 900, training loss: 0.010550573468208313 = 0.003892163746058941 + 0.001 * 6.65841007232666
Epoch 900, val loss: 1.354719877243042
Epoch 910, training loss: 0.010446816682815552 = 0.0037807137705385685 + 0.001 * 6.666101932525635
Epoch 910, val loss: 1.3592529296875
Epoch 920, training loss: 0.010331526398658752 = 0.003674647305160761 + 0.001 * 6.65687894821167
Epoch 920, val loss: 1.363673210144043
Epoch 930, training loss: 0.010227145627140999 = 0.003573640016838908 + 0.001 * 6.653505325317383
Epoch 930, val loss: 1.368015170097351
Epoch 940, training loss: 0.010134806856513023 = 0.0034773678053170443 + 0.001 * 6.657438278198242
Epoch 940, val loss: 1.372273325920105
Epoch 950, training loss: 0.010033639147877693 = 0.0033855452202260494 + 0.001 * 6.648093223571777
Epoch 950, val loss: 1.3764512538909912
Epoch 960, training loss: 0.009952227585017681 = 0.003297892864793539 + 0.001 * 6.654334545135498
Epoch 960, val loss: 1.3805408477783203
Epoch 970, training loss: 0.009863484650850296 = 0.0032141448464244604 + 0.001 * 6.649339199066162
Epoch 970, val loss: 1.3845574855804443
Epoch 980, training loss: 0.009804777801036835 = 0.003134083468466997 + 0.001 * 6.670693397521973
Epoch 980, val loss: 1.3884878158569336
Epoch 990, training loss: 0.009708108380436897 = 0.003057410940527916 + 0.001 * 6.650697231292725
Epoch 990, val loss: 1.3923650979995728
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.2214
Flip ASR: 0.1867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9441479444503784 = 1.9357742071151733 + 0.001 * 8.373740196228027
Epoch 0, val loss: 1.9275456666946411
Epoch 10, training loss: 1.9342120885849 = 1.9258384704589844 + 0.001 * 8.373621940612793
Epoch 10, val loss: 1.9176111221313477
Epoch 20, training loss: 1.9219868183135986 = 1.9136135578155518 + 0.001 * 8.373275756835938
Epoch 20, val loss: 1.9053019285202026
Epoch 30, training loss: 1.9047502279281616 = 1.896377682685852 + 0.001 * 8.37253475189209
Epoch 30, val loss: 1.8880735635757446
Epoch 40, training loss: 1.8790971040725708 = 1.870726227760315 + 0.001 * 8.370835304260254
Epoch 40, val loss: 1.8631831407546997
Epoch 50, training loss: 1.8429625034332275 = 1.8345966339111328 + 0.001 * 8.365835189819336
Epoch 50, val loss: 1.8299872875213623
Epoch 60, training loss: 1.8001490831375122 = 1.7918063402175903 + 0.001 * 8.342794418334961
Epoch 60, val loss: 1.7942646741867065
Epoch 70, training loss: 1.7567869424819946 = 1.748608112335205 + 0.001 * 8.178847312927246
Epoch 70, val loss: 1.7595196962356567
Epoch 80, training loss: 1.7011111974716187 = 1.693418264389038 + 0.001 * 7.692986965179443
Epoch 80, val loss: 1.7106690406799316
Epoch 90, training loss: 1.6245300769805908 = 1.6169848442077637 + 0.001 * 7.545235633850098
Epoch 90, val loss: 1.6443229913711548
Epoch 100, training loss: 1.527156949043274 = 1.5198134183883667 + 0.001 * 7.343513488769531
Epoch 100, val loss: 1.5654438734054565
Epoch 110, training loss: 1.4202324151992798 = 1.413174033164978 + 0.001 * 7.05841588973999
Epoch 110, val loss: 1.4826087951660156
Epoch 120, training loss: 1.3158737421035767 = 1.3088462352752686 + 0.001 * 7.027499675750732
Epoch 120, val loss: 1.405073642730713
Epoch 130, training loss: 1.2181038856506348 = 1.2111413478851318 + 0.001 * 6.962544918060303
Epoch 130, val loss: 1.3348476886749268
Epoch 140, training loss: 1.1272202730178833 = 1.1203008890151978 + 0.001 * 6.91935920715332
Epoch 140, val loss: 1.2702901363372803
Epoch 150, training loss: 1.0430954694747925 = 1.036211371421814 + 0.001 * 6.884151458740234
Epoch 150, val loss: 1.2102243900299072
Epoch 160, training loss: 0.9651256799697876 = 0.9582639336585999 + 0.001 * 6.861769676208496
Epoch 160, val loss: 1.153851866722107
Epoch 170, training loss: 0.8917313814163208 = 0.8848795890808105 + 0.001 * 6.851778984069824
Epoch 170, val loss: 1.1001032590866089
Epoch 180, training loss: 0.8209856748580933 = 0.8141380548477173 + 0.001 * 6.84764289855957
Epoch 180, val loss: 1.047869324684143
Epoch 190, training loss: 0.7523940205574036 = 0.7455494999885559 + 0.001 * 6.844527721405029
Epoch 190, val loss: 0.9976579546928406
Epoch 200, training loss: 0.6871352195739746 = 0.6802934408187866 + 0.001 * 6.841790199279785
Epoch 200, val loss: 0.9518605470657349
Epoch 210, training loss: 0.6264671087265015 = 0.6196279525756836 + 0.001 * 6.839181900024414
Epoch 210, val loss: 0.9130771160125732
Epoch 220, training loss: 0.5704466700553894 = 0.5636104345321655 + 0.001 * 6.836221218109131
Epoch 220, val loss: 0.8818019032478333
Epoch 230, training loss: 0.5182397961616516 = 0.5114073157310486 + 0.001 * 6.832460880279541
Epoch 230, val loss: 0.8574044108390808
Epoch 240, training loss: 0.46886613965034485 = 0.4620383381843567 + 0.001 * 6.8278093338012695
Epoch 240, val loss: 0.8391914367675781
Epoch 250, training loss: 0.42165490984916687 = 0.4148317575454712 + 0.001 * 6.823159694671631
Epoch 250, val loss: 0.8260419964790344
Epoch 260, training loss: 0.37635862827301025 = 0.36954206228256226 + 0.001 * 6.816555500030518
Epoch 260, val loss: 0.8169765472412109
Epoch 270, training loss: 0.3331553339958191 = 0.3263460695743561 + 0.001 * 6.809257984161377
Epoch 270, val loss: 0.8111016750335693
Epoch 280, training loss: 0.29263100028038025 = 0.28582963347435 + 0.001 * 6.801353931427002
Epoch 280, val loss: 0.8082252740859985
Epoch 290, training loss: 0.2554346024990082 = 0.24864241480827332 + 0.001 * 6.792197227478027
Epoch 290, val loss: 0.8086004853248596
Epoch 300, training loss: 0.2220487743616104 = 0.2152642160654068 + 0.001 * 6.784554958343506
Epoch 300, val loss: 0.8120980858802795
Epoch 310, training loss: 0.19268444180488586 = 0.1859092116355896 + 0.001 * 6.7752275466918945
Epoch 310, val loss: 0.8182941675186157
Epoch 320, training loss: 0.1672099530696869 = 0.16044354438781738 + 0.001 * 6.766407012939453
Epoch 320, val loss: 0.8263667225837708
Epoch 330, training loss: 0.14535070955753326 = 0.13859032094478607 + 0.001 * 6.760393142700195
Epoch 330, val loss: 0.8359910249710083
Epoch 340, training loss: 0.12668558955192566 = 0.11993677914142609 + 0.001 * 6.748815536499023
Epoch 340, val loss: 0.8466945290565491
Epoch 350, training loss: 0.11078787595033646 = 0.104042649269104 + 0.001 * 6.745223522186279
Epoch 350, val loss: 0.858340322971344
Epoch 360, training loss: 0.09728305041790009 = 0.09053933620452881 + 0.001 * 6.743712425231934
Epoch 360, val loss: 0.8708258271217346
Epoch 370, training loss: 0.0858151838183403 = 0.07907389104366302 + 0.001 * 6.741293907165527
Epoch 370, val loss: 0.8836382627487183
Epoch 380, training loss: 0.0760795921087265 = 0.06934020668268204 + 0.001 * 6.739383220672607
Epoch 380, val loss: 0.8969365358352661
Epoch 390, training loss: 0.06782536208629608 = 0.06108749285340309 + 0.001 * 6.737872123718262
Epoch 390, val loss: 0.9106170535087585
Epoch 400, training loss: 0.06082329899072647 = 0.054087359458208084 + 0.001 * 6.735940456390381
Epoch 400, val loss: 0.9245719909667969
Epoch 410, training loss: 0.054862625896930695 = 0.04812716692686081 + 0.001 * 6.735456943511963
Epoch 410, val loss: 0.9387061595916748
Epoch 420, training loss: 0.04976792261004448 = 0.043031640350818634 + 0.001 * 6.736282825469971
Epoch 420, val loss: 0.9530400037765503
Epoch 430, training loss: 0.04539114981889725 = 0.038656484335660934 + 0.001 * 6.73466682434082
Epoch 430, val loss: 0.9674378633499146
Epoch 440, training loss: 0.041613973677158356 = 0.03487813100218773 + 0.001 * 6.735843658447266
Epoch 440, val loss: 0.9817755818367004
Epoch 450, training loss: 0.03833388164639473 = 0.03159775957465172 + 0.001 * 6.7361226081848145
Epoch 450, val loss: 0.9961605668067932
Epoch 460, training loss: 0.035477831959724426 = 0.028743939474225044 + 0.001 * 6.733893394470215
Epoch 460, val loss: 1.0103386640548706
Epoch 470, training loss: 0.03298216685652733 = 0.026248693466186523 + 0.001 * 6.733473300933838
Epoch 470, val loss: 1.0243263244628906
Epoch 480, training loss: 0.03078426793217659 = 0.024051683023571968 + 0.001 * 6.732585430145264
Epoch 480, val loss: 1.0379868745803833
Epoch 490, training loss: 0.02884821593761444 = 0.022110028192400932 + 0.001 * 6.73818826675415
Epoch 490, val loss: 1.0515674352645874
Epoch 500, training loss: 0.02712351642549038 = 0.02039087563753128 + 0.001 * 6.732640266418457
Epoch 500, val loss: 1.0647799968719482
Epoch 510, training loss: 0.0255963746458292 = 0.018865427002310753 + 0.001 * 6.730947017669678
Epoch 510, val loss: 1.0776633024215698
Epoch 520, training loss: 0.02423946000635624 = 0.0175080057233572 + 0.001 * 6.731454372406006
Epoch 520, val loss: 1.090178370475769
Epoch 530, training loss: 0.023026034235954285 = 0.01629381999373436 + 0.001 * 6.732213497161865
Epoch 530, val loss: 1.1023869514465332
Epoch 540, training loss: 0.02193467877805233 = 0.015204164199531078 + 0.001 * 6.730514049530029
Epoch 540, val loss: 1.1142467260360718
Epoch 550, training loss: 0.020951706916093826 = 0.014223000966012478 + 0.001 * 6.728705406188965
Epoch 550, val loss: 1.1257870197296143
Epoch 560, training loss: 0.02006462775170803 = 0.013335666619241238 + 0.001 * 6.72896146774292
Epoch 560, val loss: 1.1370317935943604
Epoch 570, training loss: 0.019261619076132774 = 0.012530953623354435 + 0.001 * 6.7306647300720215
Epoch 570, val loss: 1.1479332447052002
Epoch 580, training loss: 0.01852533221244812 = 0.011799294501543045 + 0.001 * 6.72603702545166
Epoch 580, val loss: 1.1585876941680908
Epoch 590, training loss: 0.017858318984508514 = 0.011132082901895046 + 0.001 * 6.7262349128723145
Epoch 590, val loss: 1.1689670085906982
Epoch 600, training loss: 0.017248153686523438 = 0.010522912256419659 + 0.001 * 6.725240230560303
Epoch 600, val loss: 1.1790772676467896
Epoch 610, training loss: 0.0166885145008564 = 0.009965687990188599 + 0.001 * 6.722825527191162
Epoch 610, val loss: 1.1888796091079712
Epoch 620, training loss: 0.01618395745754242 = 0.009455841965973377 + 0.001 * 6.728115081787109
Epoch 620, val loss: 1.1983908414840698
Epoch 630, training loss: 0.015710627660155296 = 0.008987152948975563 + 0.001 * 6.723474502563477
Epoch 630, val loss: 1.20774245262146
Epoch 640, training loss: 0.015274837613105774 = 0.008554774336516857 + 0.001 * 6.720063209533691
Epoch 640, val loss: 1.2167855501174927
Epoch 650, training loss: 0.014874633401632309 = 0.00815505813807249 + 0.001 * 6.71957540512085
Epoch 650, val loss: 1.2256931066513062
Epoch 660, training loss: 0.01450163684785366 = 0.007784860208630562 + 0.001 * 6.716776371002197
Epoch 660, val loss: 1.2343183755874634
Epoch 670, training loss: 0.01415686309337616 = 0.007441237568855286 + 0.001 * 6.715624809265137
Epoch 670, val loss: 1.2428253889083862
Epoch 680, training loss: 0.013838764280080795 = 0.007121753413230181 + 0.001 * 6.717010498046875
Epoch 680, val loss: 1.251057744026184
Epoch 690, training loss: 0.013538646511733532 = 0.006824010983109474 + 0.001 * 6.714635372161865
Epoch 690, val loss: 1.2591644525527954
Epoch 700, training loss: 0.013257835060358047 = 0.0065460894256830215 + 0.001 * 6.711744785308838
Epoch 700, val loss: 1.2670480012893677
Epoch 710, training loss: 0.013002900406718254 = 0.006286281626671553 + 0.001 * 6.716618537902832
Epoch 710, val loss: 1.2747735977172852
Epoch 720, training loss: 0.012758253142237663 = 0.006043117493391037 + 0.001 * 6.715135097503662
Epoch 720, val loss: 1.2823530435562134
Epoch 730, training loss: 0.012525595724582672 = 0.005815131589770317 + 0.001 * 6.710464000701904
Epoch 730, val loss: 1.2897616624832153
Epoch 740, training loss: 0.012315604835748672 = 0.005601037293672562 + 0.001 * 6.7145676612854
Epoch 740, val loss: 1.2969697713851929
Epoch 750, training loss: 0.012111526913940907 = 0.005399737507104874 + 0.001 * 6.711789131164551
Epoch 750, val loss: 1.3040781021118164
Epoch 760, training loss: 0.011921560391783714 = 0.005210363306105137 + 0.001 * 6.711196422576904
Epoch 760, val loss: 1.3110408782958984
Epoch 770, training loss: 0.01173882931470871 = 0.0050318543799221516 + 0.001 * 6.706974983215332
Epoch 770, val loss: 1.3178108930587769
Epoch 780, training loss: 0.01156800426542759 = 0.004863383714109659 + 0.001 * 6.704619884490967
Epoch 780, val loss: 1.3244963884353638
Epoch 790, training loss: 0.011408179067075253 = 0.004704104270786047 + 0.001 * 6.704074382781982
Epoch 790, val loss: 1.3310024738311768
Epoch 800, training loss: 0.011256136000156403 = 0.00455303443595767 + 0.001 * 6.703100681304932
Epoch 800, val loss: 1.337397813796997
Epoch 810, training loss: 0.011110913008451462 = 0.004408903419971466 + 0.001 * 6.702009677886963
Epoch 810, val loss: 1.3436468839645386
Epoch 820, training loss: 0.01099478080868721 = 0.0042702797800302505 + 0.001 * 6.7245001792907715
Epoch 820, val loss: 1.3497620820999146
Epoch 830, training loss: 0.010841019451618195 = 0.004136042669415474 + 0.001 * 6.704977035522461
Epoch 830, val loss: 1.355843186378479
Epoch 840, training loss: 0.010705897584557533 = 0.004005847964435816 + 0.001 * 6.70004940032959
Epoch 840, val loss: 1.361804723739624
Epoch 850, training loss: 0.010590682737529278 = 0.003879798110574484 + 0.001 * 6.710884094238281
Epoch 850, val loss: 1.3677536249160767
Epoch 860, training loss: 0.010454744100570679 = 0.0037580952048301697 + 0.001 * 6.696649074554443
Epoch 860, val loss: 1.3736623525619507
Epoch 870, training loss: 0.010338256135582924 = 0.003640933893620968 + 0.001 * 6.69732141494751
Epoch 870, val loss: 1.3795311450958252
Epoch 880, training loss: 0.01022679265588522 = 0.003528422676026821 + 0.001 * 6.69836950302124
Epoch 880, val loss: 1.3853256702423096
Epoch 890, training loss: 0.010121094062924385 = 0.00342062721028924 + 0.001 * 6.700467109680176
Epoch 890, val loss: 1.3910260200500488
Epoch 900, training loss: 0.01001256424933672 = 0.0033175265416502953 + 0.001 * 6.695037364959717
Epoch 900, val loss: 1.396562099456787
Epoch 910, training loss: 0.00991865899413824 = 0.0032190417405217886 + 0.001 * 6.6996169090271
Epoch 910, val loss: 1.4021731615066528
Epoch 920, training loss: 0.009819778613746166 = 0.0031250312458723783 + 0.001 * 6.694746971130371
Epoch 920, val loss: 1.4075855016708374
Epoch 930, training loss: 0.009724915027618408 = 0.003035355359315872 + 0.001 * 6.689559459686279
Epoch 930, val loss: 1.412933349609375
Epoch 940, training loss: 0.00965204555541277 = 0.002949815010651946 + 0.001 * 6.702229976654053
Epoch 940, val loss: 1.4181987047195435
Epoch 950, training loss: 0.009560009464621544 = 0.002868261421099305 + 0.001 * 6.691747665405273
Epoch 950, val loss: 1.423241376876831
Epoch 960, training loss: 0.009491600096225739 = 0.00279044546186924 + 0.001 * 6.701154708862305
Epoch 960, val loss: 1.4283223152160645
Epoch 970, training loss: 0.009410160593688488 = 0.0027160837780684233 + 0.001 * 6.6940765380859375
Epoch 970, val loss: 1.4333195686340332
Epoch 980, training loss: 0.00934341549873352 = 0.00264500523917377 + 0.001 * 6.6984100341796875
Epoch 980, val loss: 1.4383001327514648
Epoch 990, training loss: 0.009268544614315033 = 0.0025769611820578575 + 0.001 * 6.691582679748535
Epoch 990, val loss: 1.443165898323059
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.9446
Flip ASR: 0.9333/225 nodes
The final ASR:0.60517, 0.29692, Accuracy:0.80741, 0.00000
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10626])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98032, 0.00174, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.965813159942627 = 1.9574393033981323 + 0.001 * 8.373832702636719
Epoch 0, val loss: 1.9613662958145142
Epoch 10, training loss: 1.955216407775879 = 1.9468426704406738 + 0.001 * 8.373743057250977
Epoch 10, val loss: 1.9511675834655762
Epoch 20, training loss: 1.9418516159057617 = 1.9334781169891357 + 0.001 * 8.373466491699219
Epoch 20, val loss: 1.937780737876892
Epoch 30, training loss: 1.9225661754608154 = 1.9141932725906372 + 0.001 * 8.372883796691895
Epoch 30, val loss: 1.9180091619491577
Epoch 40, training loss: 1.893536925315857 = 1.8851653337478638 + 0.001 * 8.371546745300293
Epoch 40, val loss: 1.888486623764038
Epoch 50, training loss: 1.8528144359588623 = 1.8444466590881348 + 0.001 * 8.367734909057617
Epoch 50, val loss: 1.8490363359451294
Epoch 60, training loss: 1.8084334135055542 = 1.8000812530517578 + 0.001 * 8.352129936218262
Epoch 60, val loss: 1.8111892938613892
Epoch 70, training loss: 1.7716892957687378 = 1.7634351253509521 + 0.001 * 8.254122734069824
Epoch 70, val loss: 1.7827560901641846
Epoch 80, training loss: 1.7249433994293213 = 1.717005729675293 + 0.001 * 7.9377007484436035
Epoch 80, val loss: 1.742793083190918
Epoch 90, training loss: 1.6612268686294556 = 1.6536556482315063 + 0.001 * 7.5712080001831055
Epoch 90, val loss: 1.6895803213119507
Epoch 100, training loss: 1.5781923532485962 = 1.5709072351455688 + 0.001 * 7.2851481437683105
Epoch 100, val loss: 1.6218767166137695
Epoch 110, training loss: 1.484136939048767 = 1.4769442081451416 + 0.001 * 7.192677974700928
Epoch 110, val loss: 1.5463783740997314
Epoch 120, training loss: 1.388776183128357 = 1.3816052675247192 + 0.001 * 7.170907020568848
Epoch 120, val loss: 1.472423791885376
Epoch 130, training loss: 1.2940627336502075 = 1.286925196647644 + 0.001 * 7.13753604888916
Epoch 130, val loss: 1.3997029066085815
Epoch 140, training loss: 1.1984938383102417 = 1.1913784742355347 + 0.001 * 7.115361213684082
Epoch 140, val loss: 1.327669620513916
Epoch 150, training loss: 1.102022409439087 = 1.0949186086654663 + 0.001 * 7.103822708129883
Epoch 150, val loss: 1.2543684244155884
Epoch 160, training loss: 1.0071907043457031 = 1.0000975131988525 + 0.001 * 7.093133449554443
Epoch 160, val loss: 1.182413935661316
Epoch 170, training loss: 0.9173223972320557 = 0.910243034362793 + 0.001 * 7.079384803771973
Epoch 170, val loss: 1.1143286228179932
Epoch 180, training loss: 0.8344092965126038 = 0.8273465037345886 + 0.001 * 7.062764644622803
Epoch 180, val loss: 1.0517650842666626
Epoch 190, training loss: 0.7597313523292542 = 0.7526907324790955 + 0.001 * 7.040647029876709
Epoch 190, val loss: 0.996592104434967
Epoch 200, training loss: 0.6938525438308716 = 0.6868422031402588 + 0.001 * 7.010324954986572
Epoch 200, val loss: 0.9502259492874146
Epoch 210, training loss: 0.6358568668365479 = 0.6288781762123108 + 0.001 * 6.978696823120117
Epoch 210, val loss: 0.9128924012184143
Epoch 220, training loss: 0.584029495716095 = 0.5770856142044067 + 0.001 * 6.943869113922119
Epoch 220, val loss: 0.8837405443191528
Epoch 230, training loss: 0.5370813012123108 = 0.5301679968833923 + 0.001 * 6.913300514221191
Epoch 230, val loss: 0.8616138100624084
Epoch 240, training loss: 0.4940701723098755 = 0.48717668652534485 + 0.001 * 6.893485069274902
Epoch 240, val loss: 0.8453029990196228
Epoch 250, training loss: 0.4540708363056183 = 0.4471887946128845 + 0.001 * 6.882050514221191
Epoch 250, val loss: 0.8338866829872131
Epoch 260, training loss: 0.41625967621803284 = 0.4093817174434662 + 0.001 * 6.877945899963379
Epoch 260, val loss: 0.8271978497505188
Epoch 270, training loss: 0.380095899105072 = 0.373227596282959 + 0.001 * 6.868296146392822
Epoch 270, val loss: 0.8250829577445984
Epoch 280, training loss: 0.34554076194763184 = 0.3386787474155426 + 0.001 * 6.862011909484863
Epoch 280, val loss: 0.8273560404777527
Epoch 290, training loss: 0.31275278329849243 = 0.30589935183525085 + 0.001 * 6.85341739654541
Epoch 290, val loss: 0.8337736129760742
Epoch 300, training loss: 0.28185465931892395 = 0.27500203251838684 + 0.001 * 6.852612495422363
Epoch 300, val loss: 0.8439574837684631
Epoch 310, training loss: 0.25289350748062134 = 0.24603383243083954 + 0.001 * 6.85966157913208
Epoch 310, val loss: 0.8576225638389587
Epoch 320, training loss: 0.22598521411418915 = 0.21914422512054443 + 0.001 * 6.840987205505371
Epoch 320, val loss: 0.8746859431266785
Epoch 330, training loss: 0.20137113332748413 = 0.1945396214723587 + 0.001 * 6.831518173217773
Epoch 330, val loss: 0.895049512386322
Epoch 340, training loss: 0.17922131717205048 = 0.17238926887512207 + 0.001 * 6.8320488929748535
Epoch 340, val loss: 0.9187062978744507
Epoch 350, training loss: 0.1595408171415329 = 0.1527174562215805 + 0.001 * 6.823359966278076
Epoch 350, val loss: 0.9455984830856323
Epoch 360, training loss: 0.14222340285778046 = 0.13540813326835632 + 0.001 * 6.815262317657471
Epoch 360, val loss: 0.9754886031150818
Epoch 370, training loss: 0.12705722451210022 = 0.12024039030075073 + 0.001 * 6.81683874130249
Epoch 370, val loss: 1.0078190565109253
Epoch 380, training loss: 0.11379823833703995 = 0.10696738213300705 + 0.001 * 6.830857753753662
Epoch 380, val loss: 1.042222023010254
Epoch 390, training loss: 0.10215640068054199 = 0.09535340964794159 + 0.001 * 6.8029937744140625
Epoch 390, val loss: 1.0782363414764404
Epoch 400, training loss: 0.09198813885450363 = 0.08518853783607483 + 0.001 * 6.799600124359131
Epoch 400, val loss: 1.1154073476791382
Epoch 410, training loss: 0.08308947831392288 = 0.07628899812698364 + 0.001 * 6.800477504730225
Epoch 410, val loss: 1.1531195640563965
Epoch 420, training loss: 0.0753038227558136 = 0.06850466132164001 + 0.001 * 6.799161434173584
Epoch 420, val loss: 1.1907153129577637
Epoch 430, training loss: 0.06848680973052979 = 0.06169707700610161 + 0.001 * 6.789733409881592
Epoch 430, val loss: 1.227739691734314
Epoch 440, training loss: 0.06252694129943848 = 0.05573746934533119 + 0.001 * 6.789470672607422
Epoch 440, val loss: 1.2639696598052979
Epoch 450, training loss: 0.05730551481246948 = 0.050508711487054825 + 0.001 * 6.7968034744262695
Epoch 450, val loss: 1.2991374731063843
Epoch 460, training loss: 0.052700549364089966 = 0.04590637981891632 + 0.001 * 6.794167518615723
Epoch 460, val loss: 1.3332126140594482
Epoch 470, training loss: 0.04861893504858017 = 0.04184045270085335 + 0.001 * 6.778481960296631
Epoch 470, val loss: 1.3661918640136719
Epoch 480, training loss: 0.04502800852060318 = 0.03823857754468918 + 0.001 * 6.789429664611816
Epoch 480, val loss: 1.398116946220398
Epoch 490, training loss: 0.041823625564575195 = 0.03503946587443352 + 0.001 * 6.784157752990723
Epoch 490, val loss: 1.4289995431900024
Epoch 500, training loss: 0.03897484764456749 = 0.032190632075071335 + 0.001 * 6.784215927124023
Epoch 500, val loss: 1.458965539932251
Epoch 510, training loss: 0.03642453998327255 = 0.02964787185192108 + 0.001 * 6.776669502258301
Epoch 510, val loss: 1.4880331754684448
Epoch 520, training loss: 0.03414663299918175 = 0.027372747659683228 + 0.001 * 6.773883819580078
Epoch 520, val loss: 1.5162192583084106
Epoch 530, training loss: 0.03212868049740791 = 0.02533297799527645 + 0.001 * 6.795702934265137
Epoch 530, val loss: 1.5435799360275269
Epoch 540, training loss: 0.030272405594587326 = 0.023499879986047745 + 0.001 * 6.772525787353516
Epoch 540, val loss: 1.5700857639312744
Epoch 550, training loss: 0.02861747518181801 = 0.02184872515499592 + 0.001 * 6.768748760223389
Epoch 550, val loss: 1.5957921743392944
Epoch 560, training loss: 0.027123793959617615 = 0.020358238369226456 + 0.001 * 6.765556335449219
Epoch 560, val loss: 1.6207232475280762
Epoch 570, training loss: 0.025800175964832306 = 0.01900973543524742 + 0.001 * 6.790439605712891
Epoch 570, val loss: 1.644900918006897
Epoch 580, training loss: 0.024554848670959473 = 0.017786983400583267 + 0.001 * 6.767864227294922
Epoch 580, val loss: 1.6683557033538818
Epoch 590, training loss: 0.0234422255307436 = 0.01667574793100357 + 0.001 * 6.766477584838867
Epoch 590, val loss: 1.6911475658416748
Epoch 600, training loss: 0.022431161254644394 = 0.015662936493754387 + 0.001 * 6.768224716186523
Epoch 600, val loss: 1.7132624387741089
Epoch 610, training loss: 0.021493611857295036 = 0.014736903831362724 + 0.001 * 6.756707191467285
Epoch 610, val loss: 1.7348127365112305
Epoch 620, training loss: 0.020647572353482246 = 0.01388674508780241 + 0.001 * 6.76082706451416
Epoch 620, val loss: 1.7558724880218506
Epoch 630, training loss: 0.019855819642543793 = 0.013103402219712734 + 0.001 * 6.752418041229248
Epoch 630, val loss: 1.7764500379562378
Epoch 640, training loss: 0.01913103647530079 = 0.012379893101751804 + 0.001 * 6.751142501831055
Epoch 640, val loss: 1.796573281288147
Epoch 650, training loss: 0.01848437264561653 = 0.011710734106600285 + 0.001 * 6.7736382484436035
Epoch 650, val loss: 1.8162530660629272
Epoch 660, training loss: 0.017842022702097893 = 0.011091252788901329 + 0.001 * 6.750769138336182
Epoch 660, val loss: 1.8356050252914429
Epoch 670, training loss: 0.01726747490465641 = 0.01051737554371357 + 0.001 * 6.750099182128906
Epoch 670, val loss: 1.8545098304748535
Epoch 680, training loss: 0.016747605055570602 = 0.009985266253352165 + 0.001 * 6.7623395919799805
Epoch 680, val loss: 1.8729677200317383
Epoch 690, training loss: 0.016247307881712914 = 0.009491569362580776 + 0.001 * 6.755738258361816
Epoch 690, val loss: 1.8910683393478394
Epoch 700, training loss: 0.015772124752402306 = 0.009033182635903358 + 0.001 * 6.738941669464111
Epoch 700, val loss: 1.908737301826477
Epoch 710, training loss: 0.015342756174504757 = 0.008607122115790844 + 0.001 * 6.735633850097656
Epoch 710, val loss: 1.9260133504867554
Epoch 720, training loss: 0.014961177483201027 = 0.008210641331970692 + 0.001 * 6.7505364418029785
Epoch 720, val loss: 1.942870020866394
Epoch 730, training loss: 0.014581860974431038 = 0.007841317914426327 + 0.001 * 6.740543365478516
Epoch 730, val loss: 1.959321141242981
Epoch 740, training loss: 0.014236653223633766 = 0.007496802136301994 + 0.001 * 6.739850997924805
Epoch 740, val loss: 1.975453495979309
Epoch 750, training loss: 0.013917170464992523 = 0.007175106555223465 + 0.001 * 6.742063045501709
Epoch 750, val loss: 1.9911469221115112
Epoch 760, training loss: 0.013605477288365364 = 0.006874386686831713 + 0.001 * 6.731090545654297
Epoch 760, val loss: 2.0065174102783203
Epoch 770, training loss: 0.013321786187589169 = 0.006592945661395788 + 0.001 * 6.728840351104736
Epoch 770, val loss: 2.0214836597442627
Epoch 780, training loss: 0.013060393743216991 = 0.006329206749796867 + 0.001 * 6.731186866760254
Epoch 780, val loss: 2.0361645221710205
Epoch 790, training loss: 0.012820182368159294 = 0.006081781815737486 + 0.001 * 6.738399982452393
Epoch 790, val loss: 2.050461530685425
Epoch 800, training loss: 0.012579236179590225 = 0.005849429871886969 + 0.001 * 6.729805946350098
Epoch 800, val loss: 2.0644755363464355
Epoch 810, training loss: 0.01235400140285492 = 0.005630949046462774 + 0.001 * 6.723052501678467
Epoch 810, val loss: 2.078132152557373
Epoch 820, training loss: 0.012156309559941292 = 0.005425280891358852 + 0.001 * 6.731029033660889
Epoch 820, val loss: 2.0915114879608154
Epoch 830, training loss: 0.011959997937083244 = 0.005231498274952173 + 0.001 * 6.728498935699463
Epoch 830, val loss: 2.104581356048584
Epoch 840, training loss: 0.011764759197831154 = 0.005048705730587244 + 0.001 * 6.716053009033203
Epoch 840, val loss: 2.117307662963867
Epoch 850, training loss: 0.011590502224862576 = 0.004876121412962675 + 0.001 * 6.714380264282227
Epoch 850, val loss: 2.129856586456299
Epoch 860, training loss: 0.011440372094511986 = 0.004713010508567095 + 0.001 * 6.72736120223999
Epoch 860, val loss: 2.1420085430145264
Epoch 870, training loss: 0.011268988251686096 = 0.004558682907372713 + 0.001 * 6.710305690765381
Epoch 870, val loss: 2.153989553451538
Epoch 880, training loss: 0.011163046583533287 = 0.004412544891238213 + 0.001 * 6.75050163269043
Epoch 880, val loss: 2.1656653881073
Epoch 890, training loss: 0.011012738570570946 = 0.004274048376828432 + 0.001 * 6.738690376281738
Epoch 890, val loss: 2.1771275997161865
Epoch 900, training loss: 0.010862797498703003 = 0.004142708610743284 + 0.001 * 6.720088481903076
Epoch 900, val loss: 2.1883277893066406
Epoch 910, training loss: 0.010722428560256958 = 0.004017968662083149 + 0.001 * 6.7044596672058105
Epoch 910, val loss: 2.199302911758423
Epoch 920, training loss: 0.010611037723720074 = 0.0038994476199150085 + 0.001 * 6.711589813232422
Epoch 920, val loss: 2.2100515365600586
Epoch 930, training loss: 0.010494159534573555 = 0.003786735935136676 + 0.001 * 6.707423210144043
Epoch 930, val loss: 2.2205920219421387
Epoch 940, training loss: 0.010389484465122223 = 0.003679486457258463 + 0.001 * 6.709997177124023
Epoch 940, val loss: 2.230905771255493
Epoch 950, training loss: 0.010313637554645538 = 0.003577337134629488 + 0.001 * 6.736299514770508
Epoch 950, val loss: 2.240990161895752
Epoch 960, training loss: 0.01019800454378128 = 0.00348000624217093 + 0.001 * 6.717998027801514
Epoch 960, val loss: 2.2509002685546875
Epoch 970, training loss: 0.010090664029121399 = 0.0033871522173285484 + 0.001 * 6.703512191772461
Epoch 970, val loss: 2.2606375217437744
Epoch 980, training loss: 0.010000594891607761 = 0.003298538038507104 + 0.001 * 6.702056884765625
Epoch 980, val loss: 2.2701570987701416
Epoch 990, training loss: 0.009922852739691734 = 0.003213892225176096 + 0.001 * 6.708960056304932
Epoch 990, val loss: 2.279477596282959
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9414976835250854 = 1.9331238269805908 + 0.001 * 8.373849868774414
Epoch 0, val loss: 1.927719235420227
Epoch 10, training loss: 1.9321733713150024 = 1.9237995147705078 + 0.001 * 8.373810768127441
Epoch 10, val loss: 1.9192079305648804
Epoch 20, training loss: 1.9209985733032227 = 1.9126249551773071 + 0.001 * 8.373625755310059
Epoch 20, val loss: 1.9086520671844482
Epoch 30, training loss: 1.9053934812545776 = 1.8970202207565308 + 0.001 * 8.373211860656738
Epoch 30, val loss: 1.8937485218048096
Epoch 40, training loss: 1.8820279836654663 = 1.8736556768417358 + 0.001 * 8.372265815734863
Epoch 40, val loss: 1.8716492652893066
Epoch 50, training loss: 1.8479357957839966 = 1.8395661115646362 + 0.001 * 8.36962604522705
Epoch 50, val loss: 1.8406341075897217
Epoch 60, training loss: 1.8051183223724365 = 1.7967592477798462 + 0.001 * 8.359064102172852
Epoch 60, val loss: 1.8049639463424683
Epoch 70, training loss: 1.7606019973754883 = 1.7523064613342285 + 0.001 * 8.295555114746094
Epoch 70, val loss: 1.7702443599700928
Epoch 80, training loss: 1.7035127878189087 = 1.6956604719161987 + 0.001 * 7.8522868156433105
Epoch 80, val loss: 1.7207245826721191
Epoch 90, training loss: 1.624497652053833 = 1.6167885065078735 + 0.001 * 7.709132671356201
Epoch 90, val loss: 1.6529968976974487
Epoch 100, training loss: 1.524806261062622 = 1.517271637916565 + 0.001 * 7.5345778465271
Epoch 100, val loss: 1.5735501050949097
Epoch 110, training loss: 1.4128520488739014 = 1.4055057764053345 + 0.001 * 7.346317768096924
Epoch 110, val loss: 1.4798333644866943
Epoch 120, training loss: 1.2983232736587524 = 1.2909979820251465 + 0.001 * 7.325281143188477
Epoch 120, val loss: 1.3867783546447754
Epoch 130, training loss: 1.1856046915054321 = 1.1783138513565063 + 0.001 * 7.290879249572754
Epoch 130, val loss: 1.295845866203308
Epoch 140, training loss: 1.0772093534469604 = 1.0699567794799805 + 0.001 * 7.252541542053223
Epoch 140, val loss: 1.2113265991210938
Epoch 150, training loss: 0.9744077324867249 = 0.967201292514801 + 0.001 * 7.206410884857178
Epoch 150, val loss: 1.1330381631851196
Epoch 160, training loss: 0.878441572189331 = 0.8712899088859558 + 0.001 * 7.151670932769775
Epoch 160, val loss: 1.0607972145080566
Epoch 170, training loss: 0.7907770276069641 = 0.7836685180664062 + 0.001 * 7.108532428741455
Epoch 170, val loss: 0.9958100318908691
Epoch 180, training loss: 0.7127010822296143 = 0.7056195735931396 + 0.001 * 7.081505298614502
Epoch 180, val loss: 0.9400893449783325
Epoch 190, training loss: 0.6444044709205627 = 0.6373400688171387 + 0.001 * 7.06437873840332
Epoch 190, val loss: 0.8946996331214905
Epoch 200, training loss: 0.5850505828857422 = 0.5780027508735657 + 0.001 * 7.047820568084717
Epoch 200, val loss: 0.8592620491981506
Epoch 210, training loss: 0.533258318901062 = 0.5262311697006226 + 0.001 * 7.027143478393555
Epoch 210, val loss: 0.8323781490325928
Epoch 220, training loss: 0.4872991442680359 = 0.48029592633247375 + 0.001 * 7.003211975097656
Epoch 220, val loss: 0.8123627305030823
Epoch 230, training loss: 0.4454326033592224 = 0.43845051527023315 + 0.001 * 6.982084274291992
Epoch 230, val loss: 0.797629714012146
Epoch 240, training loss: 0.40624910593032837 = 0.3992808163166046 + 0.001 * 6.968290328979492
Epoch 240, val loss: 0.7868415713310242
Epoch 250, training loss: 0.36882728338241577 = 0.3618679642677307 + 0.001 * 6.959318161010742
Epoch 250, val loss: 0.7790769934654236
Epoch 260, training loss: 0.3327809274196625 = 0.3258257508277893 + 0.001 * 6.955179214477539
Epoch 260, val loss: 0.7739195227622986
Epoch 270, training loss: 0.2982095181941986 = 0.2912561595439911 + 0.001 * 6.953362464904785
Epoch 270, val loss: 0.771149218082428
Epoch 280, training loss: 0.26554012298583984 = 0.25858697295188904 + 0.001 * 6.953158378601074
Epoch 280, val loss: 0.7707266211509705
Epoch 290, training loss: 0.23532702028751373 = 0.228376567363739 + 0.001 * 6.9504547119140625
Epoch 290, val loss: 0.7727357149124146
Epoch 300, training loss: 0.20802755653858185 = 0.2010778784751892 + 0.001 * 6.949675559997559
Epoch 300, val loss: 0.7771937847137451
Epoch 310, training loss: 0.18385519087314606 = 0.17690500617027283 + 0.001 * 6.9501800537109375
Epoch 310, val loss: 0.7841923236846924
Epoch 320, training loss: 0.1627420037984848 = 0.1557939201593399 + 0.001 * 6.94807767868042
Epoch 320, val loss: 0.7936682105064392
Epoch 330, training loss: 0.1444103866815567 = 0.13746246695518494 + 0.001 * 6.947917461395264
Epoch 330, val loss: 0.8052469491958618
Epoch 340, training loss: 0.12852256000041962 = 0.12157617509365082 + 0.001 * 6.946386814117432
Epoch 340, val loss: 0.8184552192687988
Epoch 350, training loss: 0.11473763734102249 = 0.1077902615070343 + 0.001 * 6.947377681732178
Epoch 350, val loss: 0.8329864144325256
Epoch 360, training loss: 0.10273026674985886 = 0.0957828015089035 + 0.001 * 6.9474663734436035
Epoch 360, val loss: 0.8484287261962891
Epoch 370, training loss: 0.0922337993979454 = 0.08528882265090942 + 0.001 * 6.944977760314941
Epoch 370, val loss: 0.864425539970398
Epoch 380, training loss: 0.0830325111746788 = 0.0760876014828682 + 0.001 * 6.944909572601318
Epoch 380, val loss: 0.8806875944137573
Epoch 390, training loss: 0.07494670152664185 = 0.06800338625907898 + 0.001 * 6.943315029144287
Epoch 390, val loss: 0.8970381021499634
Epoch 400, training loss: 0.06783534586429596 = 0.06089336797595024 + 0.001 * 6.941981315612793
Epoch 400, val loss: 0.9132371544837952
Epoch 410, training loss: 0.0615888349711895 = 0.054633695632219315 + 0.001 * 6.955138206481934
Epoch 410, val loss: 0.9291820526123047
Epoch 420, training loss: 0.056061819195747375 = 0.049118928611278534 + 0.001 * 6.942890644073486
Epoch 420, val loss: 0.9447277784347534
Epoch 430, training loss: 0.0511934868991375 = 0.044257357716560364 + 0.001 * 6.936128616333008
Epoch 430, val loss: 0.9597779512405396
Epoch 440, training loss: 0.046914808452129364 = 0.03996884450316429 + 0.001 * 6.945962905883789
Epoch 440, val loss: 0.9743320941925049
Epoch 450, training loss: 0.04311338812112808 = 0.0361831933259964 + 0.001 * 6.930192947387695
Epoch 450, val loss: 0.9883921146392822
Epoch 460, training loss: 0.03976588323712349 = 0.032838404178619385 + 0.001 * 6.927478313446045
Epoch 460, val loss: 1.0019350051879883
Epoch 470, training loss: 0.03681996464729309 = 0.029878944158554077 + 0.001 * 6.94102144241333
Epoch 470, val loss: 1.0149415731430054
Epoch 480, training loss: 0.03418591991066933 = 0.027257230132818222 + 0.001 * 6.9286885261535645
Epoch 480, val loss: 1.0274595022201538
Epoch 490, training loss: 0.031858544796705246 = 0.024931563064455986 + 0.001 * 6.926981449127197
Epoch 490, val loss: 1.0395278930664062
Epoch 500, training loss: 0.029783334583044052 = 0.022864924743771553 + 0.001 * 6.918408393859863
Epoch 500, val loss: 1.0511401891708374
Epoch 510, training loss: 0.027928249910473824 = 0.021025963127613068 + 0.001 * 6.902287006378174
Epoch 510, val loss: 1.0623565912246704
Epoch 520, training loss: 0.02629946544766426 = 0.01938638836145401 + 0.001 * 6.913076400756836
Epoch 520, val loss: 1.0731867551803589
Epoch 530, training loss: 0.024810895323753357 = 0.017921311780810356 + 0.001 * 6.88958215713501
Epoch 530, val loss: 1.0836527347564697
Epoch 540, training loss: 0.02354351431131363 = 0.016609495505690575 + 0.001 * 6.934018611907959
Epoch 540, val loss: 1.0937750339508057
Epoch 550, training loss: 0.02232048474252224 = 0.015432546846568584 + 0.001 * 6.887937068939209
Epoch 550, val loss: 1.1035667657852173
Epoch 560, training loss: 0.021257128566503525 = 0.01437421701848507 + 0.001 * 6.882911682128906
Epoch 560, val loss: 1.1130542755126953
Epoch 570, training loss: 0.020278433337807655 = 0.01342039369046688 + 0.001 * 6.858039855957031
Epoch 570, val loss: 1.1222549676895142
Epoch 580, training loss: 0.01955985091626644 = 0.012558829970657825 + 0.001 * 7.001020908355713
Epoch 580, val loss: 1.1311888694763184
Epoch 590, training loss: 0.01866266131401062 = 0.011778744868934155 + 0.001 * 6.883915424346924
Epoch 590, val loss: 1.1398507356643677
Epoch 600, training loss: 0.01792045682668686 = 0.011070857755839825 + 0.001 * 6.849599838256836
Epoch 600, val loss: 1.148271918296814
Epoch 610, training loss: 0.01727714017033577 = 0.010426818393170834 + 0.001 * 6.850321292877197
Epoch 610, val loss: 1.1564383506774902
Epoch 620, training loss: 0.016690818592905998 = 0.009839392267167568 + 0.001 * 6.851426124572754
Epoch 620, val loss: 1.1643710136413574
Epoch 630, training loss: 0.016151215881109238 = 0.009302282705903053 + 0.001 * 6.848933219909668
Epoch 630, val loss: 1.1720911264419556
Epoch 640, training loss: 0.015656618401408195 = 0.00881014484912157 + 0.001 * 6.846473693847656
Epoch 640, val loss: 1.1795930862426758
Epoch 650, training loss: 0.015195488929748535 = 0.008358332328498363 + 0.001 * 6.837155818939209
Epoch 650, val loss: 1.1869081258773804
Epoch 660, training loss: 0.014816982671618462 = 0.00794257503002882 + 0.001 * 6.874406814575195
Epoch 660, val loss: 1.1940133571624756
Epoch 670, training loss: 0.01439119502902031 = 0.007559230551123619 + 0.001 * 6.831963539123535
Epoch 670, val loss: 1.2009646892547607
Epoch 680, training loss: 0.014043096452951431 = 0.007204997818917036 + 0.001 * 6.838098526000977
Epoch 680, val loss: 1.2077146768569946
Epoch 690, training loss: 0.013713893480598927 = 0.006877035368233919 + 0.001 * 6.836857795715332
Epoch 690, val loss: 1.214308738708496
Epoch 700, training loss: 0.01340023335069418 = 0.006572773214429617 + 0.001 * 6.827459812164307
Epoch 700, val loss: 1.2207063436508179
Epoch 710, training loss: 0.013110794126987457 = 0.006289982236921787 + 0.001 * 6.820812225341797
Epoch 710, val loss: 1.2269554138183594
Epoch 720, training loss: 0.012848645448684692 = 0.006026687100529671 + 0.001 * 6.821958065032959
Epoch 720, val loss: 1.2330526113510132
Epoch 730, training loss: 0.012606360018253326 = 0.005781149957329035 + 0.001 * 6.825209140777588
Epoch 730, val loss: 1.239004135131836
Epoch 740, training loss: 0.012370518408715725 = 0.005551844369620085 + 0.001 * 6.818673610687256
Epoch 740, val loss: 1.244813084602356
Epoch 750, training loss: 0.012163176201283932 = 0.005337344016879797 + 0.001 * 6.825831890106201
Epoch 750, val loss: 1.2504937648773193
Epoch 760, training loss: 0.011960452422499657 = 0.005136433057487011 + 0.001 * 6.824019432067871
Epoch 760, val loss: 1.2560667991638184
Epoch 770, training loss: 0.011773845180869102 = 0.004947919864207506 + 0.001 * 6.825925350189209
Epoch 770, val loss: 1.2615011930465698
Epoch 780, training loss: 0.011592276394367218 = 0.004770822357386351 + 0.001 * 6.82145357131958
Epoch 780, val loss: 1.2668206691741943
Epoch 790, training loss: 0.011415678076446056 = 0.004604198504239321 + 0.001 * 6.811479091644287
Epoch 790, val loss: 1.2720065116882324
Epoch 800, training loss: 0.01126323826611042 = 0.004447258543223143 + 0.001 * 6.815979480743408
Epoch 800, val loss: 1.2770764827728271
Epoch 810, training loss: 0.011102553457021713 = 0.004299244377762079 + 0.001 * 6.803309440612793
Epoch 810, val loss: 1.2820605039596558
Epoch 820, training loss: 0.010978429578244686 = 0.004159480798989534 + 0.001 * 6.818948268890381
Epoch 820, val loss: 1.286915898323059
Epoch 830, training loss: 0.01082014013081789 = 0.0040273829363286495 + 0.001 * 6.792757034301758
Epoch 830, val loss: 1.2917009592056274
Epoch 840, training loss: 0.010703299194574356 = 0.0039024038705974817 + 0.001 * 6.800894737243652
Epoch 840, val loss: 1.2963749170303345
Epoch 850, training loss: 0.010579018853604794 = 0.0037840050645172596 + 0.001 * 6.795013427734375
Epoch 850, val loss: 1.3009196519851685
Epoch 860, training loss: 0.010473513044416904 = 0.0036717604380100965 + 0.001 * 6.80175256729126
Epoch 860, val loss: 1.3053665161132812
Epoch 870, training loss: 0.010375563986599445 = 0.0035652066580951214 + 0.001 * 6.810357093811035
Epoch 870, val loss: 1.3097515106201172
Epoch 880, training loss: 0.010254235938191414 = 0.00346396304666996 + 0.001 * 6.7902727127075195
Epoch 880, val loss: 1.3140252828598022
Epoch 890, training loss: 0.010153887793421745 = 0.0033676880411803722 + 0.001 * 6.786199569702148
Epoch 890, val loss: 1.3182084560394287
Epoch 900, training loss: 0.010078724473714828 = 0.0032760631293058395 + 0.001 * 6.8026604652404785
Epoch 900, val loss: 1.3223098516464233
Epoch 910, training loss: 0.009980480186641216 = 0.0031887893564999104 + 0.001 * 6.791690349578857
Epoch 910, val loss: 1.3263299465179443
Epoch 920, training loss: 0.009898324497044086 = 0.003105584532022476 + 0.001 * 6.7927398681640625
Epoch 920, val loss: 1.33029043674469
Epoch 930, training loss: 0.00981101393699646 = 0.0030262095388025045 + 0.001 * 6.784803867340088
Epoch 930, val loss: 1.3341423273086548
Epoch 940, training loss: 0.00975695252418518 = 0.0029504213016480207 + 0.001 * 6.806530475616455
Epoch 940, val loss: 1.3379545211791992
Epoch 950, training loss: 0.00967020820826292 = 0.002878030529245734 + 0.001 * 6.792177677154541
Epoch 950, val loss: 1.3416985273361206
Epoch 960, training loss: 0.009584012441337109 = 0.0028088062535971403 + 0.001 * 6.775205612182617
Epoch 960, val loss: 1.3453459739685059
Epoch 970, training loss: 0.009521907195448875 = 0.002742578275501728 + 0.001 * 6.7793288230896
Epoch 970, val loss: 1.3489201068878174
Epoch 980, training loss: 0.009472641162574291 = 0.0026791587006300688 + 0.001 * 6.793481826782227
Epoch 980, val loss: 1.3524296283721924
Epoch 990, training loss: 0.009399662725627422 = 0.00261839316226542 + 0.001 * 6.781269073486328
Epoch 990, val loss: 1.3558777570724487
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.4133
Flip ASR: 0.3111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9696910381317139 = 1.9613171815872192 + 0.001 * 8.373871803283691
Epoch 0, val loss: 1.9627084732055664
Epoch 10, training loss: 1.9598917961120605 = 1.951517939567566 + 0.001 * 8.373814582824707
Epoch 10, val loss: 1.9520139694213867
Epoch 20, training loss: 1.9478495121002197 = 1.9394758939743042 + 0.001 * 8.37360668182373
Epoch 20, val loss: 1.9387433528900146
Epoch 30, training loss: 1.9310237169265747 = 1.9226505756378174 + 0.001 * 8.373140335083008
Epoch 30, val loss: 1.9201732873916626
Epoch 40, training loss: 1.9060336351394653 = 1.8976616859436035 + 0.001 * 8.37195873260498
Epoch 40, val loss: 1.892926573753357
Epoch 50, training loss: 1.8696119785308838 = 1.8612436056137085 + 0.001 * 8.368386268615723
Epoch 50, val loss: 1.8545076847076416
Epoch 60, training loss: 1.8237274885177612 = 1.815372347831726 + 0.001 * 8.355106353759766
Epoch 60, val loss: 1.8093132972717285
Epoch 70, training loss: 1.7794164419174194 = 1.7711172103881836 + 0.001 * 8.299200057983398
Epoch 70, val loss: 1.77030611038208
Epoch 80, training loss: 1.7331080436706543 = 1.7251249551773071 + 0.001 * 7.983047962188721
Epoch 80, val loss: 1.7314013242721558
Epoch 90, training loss: 1.6703277826309204 = 1.6625961065292358 + 0.001 * 7.731697082519531
Epoch 90, val loss: 1.678836703300476
Epoch 100, training loss: 1.5855653285980225 = 1.578050971031189 + 0.001 * 7.5143256187438965
Epoch 100, val loss: 1.6075905561447144
Epoch 110, training loss: 1.4809677600860596 = 1.4736270904541016 + 0.001 * 7.340634822845459
Epoch 110, val loss: 1.5205085277557373
Epoch 120, training loss: 1.3665376901626587 = 1.3592489957809448 + 0.001 * 7.2887163162231445
Epoch 120, val loss: 1.425444483757019
Epoch 130, training loss: 1.2496376037597656 = 1.2423678636550903 + 0.001 * 7.269775390625
Epoch 130, val loss: 1.331465244293213
Epoch 140, training loss: 1.133150339126587 = 1.1258997917175293 + 0.001 * 7.250542163848877
Epoch 140, val loss: 1.2392810583114624
Epoch 150, training loss: 1.0210349559783936 = 1.0138087272644043 + 0.001 * 7.226192951202393
Epoch 150, val loss: 1.1524616479873657
Epoch 160, training loss: 0.9188908934593201 = 0.9116901159286499 + 0.001 * 7.200767993927002
Epoch 160, val loss: 1.0757864713668823
Epoch 170, training loss: 0.8300869464874268 = 0.8229108452796936 + 0.001 * 7.176084041595459
Epoch 170, val loss: 1.0118701457977295
Epoch 180, training loss: 0.7545761466026306 = 0.7474309802055359 + 0.001 * 7.14517068862915
Epoch 180, val loss: 0.960751473903656
Epoch 190, training loss: 0.6900454759597778 = 0.6829453110694885 + 0.001 * 7.100166320800781
Epoch 190, val loss: 0.9206147789955139
Epoch 200, training loss: 0.6337379217147827 = 0.626684844493866 + 0.001 * 7.0530900955200195
Epoch 200, val loss: 0.8887755870819092
Epoch 210, training loss: 0.5831488966941833 = 0.576126217842102 + 0.001 * 7.022695541381836
Epoch 210, val loss: 0.8624942898750305
Epoch 220, training loss: 0.5364488363265991 = 0.5294510722160339 + 0.001 * 6.997747421264648
Epoch 220, val loss: 0.8407366275787354
Epoch 230, training loss: 0.49225232005119324 = 0.4852835536003113 + 0.001 * 6.968780040740967
Epoch 230, val loss: 0.8229913711547852
Epoch 240, training loss: 0.44956445693969727 = 0.44261637330055237 + 0.001 * 6.948098182678223
Epoch 240, val loss: 0.8090295791625977
Epoch 250, training loss: 0.40793249011039734 = 0.40099671483039856 + 0.001 * 6.935778617858887
Epoch 250, val loss: 0.7992914319038391
Epoch 260, training loss: 0.3676345646381378 = 0.36070716381073 + 0.001 * 6.92739725112915
Epoch 260, val loss: 0.7941234111785889
Epoch 270, training loss: 0.32960596680641174 = 0.3226834535598755 + 0.001 * 6.922523021697998
Epoch 270, val loss: 0.7934738397598267
Epoch 280, training loss: 0.2949637770652771 = 0.2880474925041199 + 0.001 * 6.9162917137146
Epoch 280, val loss: 0.796856164932251
Epoch 290, training loss: 0.2640816867351532 = 0.25716432929039 + 0.001 * 6.917357444763184
Epoch 290, val loss: 0.8032512664794922
Epoch 300, training loss: 0.23699083924293518 = 0.23008669912815094 + 0.001 * 6.904133319854736
Epoch 300, val loss: 0.812171220779419
Epoch 310, training loss: 0.21364757418632507 = 0.2067503035068512 + 0.001 * 6.897270202636719
Epoch 310, val loss: 0.8232820630073547
Epoch 320, training loss: 0.19350580871105194 = 0.18661712110042572 + 0.001 * 6.888693332672119
Epoch 320, val loss: 0.8366997838020325
Epoch 330, training loss: 0.1759425699710846 = 0.1690603792667389 + 0.001 * 6.88218355178833
Epoch 330, val loss: 0.8518688678741455
Epoch 340, training loss: 0.16045622527599335 = 0.1535768061876297 + 0.001 * 6.879425048828125
Epoch 340, val loss: 0.8684071898460388
Epoch 350, training loss: 0.14665406942367554 = 0.13978049159049988 + 0.001 * 6.873576641082764
Epoch 350, val loss: 0.8857954740524292
Epoch 360, training loss: 0.13428589701652527 = 0.12740886211395264 + 0.001 * 6.877031326293945
Epoch 360, val loss: 0.9040582776069641
Epoch 370, training loss: 0.12314004451036453 = 0.11627479642629623 + 0.001 * 6.865246772766113
Epoch 370, val loss: 0.9229047298431396
Epoch 380, training loss: 0.11303634941577911 = 0.10617273300886154 + 0.001 * 6.863616466522217
Epoch 380, val loss: 0.9425628185272217
Epoch 390, training loss: 0.10385037958621979 = 0.09698867797851562 + 0.001 * 6.861697673797607
Epoch 390, val loss: 0.9619393944740295
Epoch 400, training loss: 0.0954887866973877 = 0.08862678706645966 + 0.001 * 6.862001895904541
Epoch 400, val loss: 0.9814851880073547
Epoch 410, training loss: 0.08790359646081924 = 0.08103252947330475 + 0.001 * 6.871069431304932
Epoch 410, val loss: 1.001155972480774
Epoch 420, training loss: 0.08098068088293076 = 0.07412015646696091 + 0.001 * 6.860523223876953
Epoch 420, val loss: 1.0204380750656128
Epoch 430, training loss: 0.07463497668504715 = 0.06777491420507431 + 0.001 * 6.860065460205078
Epoch 430, val loss: 1.0397181510925293
Epoch 440, training loss: 0.0686945989727974 = 0.061835698783397675 + 0.001 * 6.858897686004639
Epoch 440, val loss: 1.0588138103485107
Epoch 450, training loss: 0.06312903761863708 = 0.05626596510410309 + 0.001 * 6.863072872161865
Epoch 450, val loss: 1.0775740146636963
Epoch 460, training loss: 0.05790292099118233 = 0.05104109272360802 + 0.001 * 6.861828327178955
Epoch 460, val loss: 1.0958815813064575
Epoch 470, training loss: 0.05291040241718292 = 0.04605189338326454 + 0.001 * 6.858508586883545
Epoch 470, val loss: 1.114051342010498
Epoch 480, training loss: 0.04806891083717346 = 0.04121185094118118 + 0.001 * 6.85706090927124
Epoch 480, val loss: 1.1318254470825195
Epoch 490, training loss: 0.04318529739975929 = 0.0363212525844574 + 0.001 * 6.864043712615967
Epoch 490, val loss: 1.149445652961731
Epoch 500, training loss: 0.038603585213422775 = 0.031743861734867096 + 0.001 * 6.859721660614014
Epoch 500, val loss: 1.1672980785369873
Epoch 510, training loss: 0.034911833703517914 = 0.028055338189005852 + 0.001 * 6.856495380401611
Epoch 510, val loss: 1.1851810216903687
Epoch 520, training loss: 0.032004691660404205 = 0.02515116147696972 + 0.001 * 6.853528022766113
Epoch 520, val loss: 1.2033814191818237
Epoch 530, training loss: 0.02968456782400608 = 0.022820616140961647 + 0.001 * 6.863951683044434
Epoch 530, val loss: 1.221752643585205
Epoch 540, training loss: 0.027694184333086014 = 0.02083909697830677 + 0.001 * 6.8550872802734375
Epoch 540, val loss: 1.239088773727417
Epoch 550, training loss: 0.025958608835935593 = 0.01910487562417984 + 0.001 * 6.853732109069824
Epoch 550, val loss: 1.255257248878479
Epoch 560, training loss: 0.02443619631230831 = 0.01758568547666073 + 0.001 * 6.850510120391846
Epoch 560, val loss: 1.270667552947998
Epoch 570, training loss: 0.02309957519173622 = 0.016244372352957726 + 0.001 * 6.855201721191406
Epoch 570, val loss: 1.285644292831421
Epoch 580, training loss: 0.021900631487369537 = 0.01505189947783947 + 0.001 * 6.848731994628906
Epoch 580, val loss: 1.3001759052276611
Epoch 590, training loss: 0.020825354382395744 = 0.013984283432364464 + 0.001 * 6.841071128845215
Epoch 590, val loss: 1.3142082691192627
Epoch 600, training loss: 0.01987023651599884 = 0.013025266118347645 + 0.001 * 6.844971179962158
Epoch 600, val loss: 1.3277698755264282
Epoch 610, training loss: 0.019005287438631058 = 0.01216207817196846 + 0.001 * 6.843209743499756
Epoch 610, val loss: 1.340736746788025
Epoch 620, training loss: 0.01823502406477928 = 0.011382808908820152 + 0.001 * 6.852214813232422
Epoch 620, val loss: 1.3532216548919678
Epoch 630, training loss: 0.017517035827040672 = 0.010677034966647625 + 0.001 * 6.840000629425049
Epoch 630, val loss: 1.365283727645874
Epoch 640, training loss: 0.01687270775437355 = 0.01003591064363718 + 0.001 * 6.836797714233398
Epoch 640, val loss: 1.3769340515136719
Epoch 650, training loss: 0.016288790851831436 = 0.009452467784285545 + 0.001 * 6.836321830749512
Epoch 650, val loss: 1.3881455659866333
Epoch 660, training loss: 0.015753036364912987 = 0.008920463733375072 + 0.001 * 6.832571983337402
Epoch 660, val loss: 1.398988127708435
Epoch 670, training loss: 0.01526966318488121 = 0.008434191346168518 + 0.001 * 6.8354716300964355
Epoch 670, val loss: 1.4095033407211304
Epoch 680, training loss: 0.01481817476451397 = 0.00798859167844057 + 0.001 * 6.829582214355469
Epoch 680, val loss: 1.419661283493042
Epoch 690, training loss: 0.014407696202397346 = 0.007578888908028603 + 0.001 * 6.8288068771362305
Epoch 690, val loss: 1.4294815063476562
Epoch 700, training loss: 0.014026208780705929 = 0.007201614324003458 + 0.001 * 6.824594020843506
Epoch 700, val loss: 1.4389939308166504
Epoch 710, training loss: 0.013670495711266994 = 0.006853119935840368 + 0.001 * 6.817375659942627
Epoch 710, val loss: 1.4481996297836304
Epoch 720, training loss: 0.013350826688110828 = 0.006530456244945526 + 0.001 * 6.820370197296143
Epoch 720, val loss: 1.4570894241333008
Epoch 730, training loss: 0.013069119304418564 = 0.006231514271348715 + 0.001 * 6.83760404586792
Epoch 730, val loss: 1.4657249450683594
Epoch 740, training loss: 0.012776711955666542 = 0.005954199004918337 + 0.001 * 6.822513103485107
Epoch 740, val loss: 1.4741007089614868
Epoch 750, training loss: 0.01250381674617529 = 0.0056963213719427586 + 0.001 * 6.8074951171875
Epoch 750, val loss: 1.4822121858596802
Epoch 760, training loss: 0.012288419529795647 = 0.005456205457448959 + 0.001 * 6.832213878631592
Epoch 760, val loss: 1.490097999572754
Epoch 770, training loss: 0.012033846229314804 = 0.005232294090092182 + 0.001 * 6.801551818847656
Epoch 770, val loss: 1.4977481365203857
Epoch 780, training loss: 0.01181701384484768 = 0.0050229886546730995 + 0.001 * 6.794024467468262
Epoch 780, val loss: 1.505177617073059
Epoch 790, training loss: 0.011627315543591976 = 0.0048267776146531105 + 0.001 * 6.800537586212158
Epoch 790, val loss: 1.512407660484314
Epoch 800, training loss: 0.011447313241660595 = 0.004641834180802107 + 0.001 * 6.805478572845459
Epoch 800, val loss: 1.5194511413574219
Epoch 810, training loss: 0.011257175356149673 = 0.0044662863947451115 + 0.001 * 6.790889263153076
Epoch 810, val loss: 1.5263913869857788
Epoch 820, training loss: 0.011124080047011375 = 0.004298850893974304 + 0.001 * 6.825228691101074
Epoch 820, val loss: 1.5332368612289429
Epoch 830, training loss: 0.01094308402389288 = 0.004138983320444822 + 0.001 * 6.804100513458252
Epoch 830, val loss: 1.5400292873382568
Epoch 840, training loss: 0.010769957676529884 = 0.003986494615674019 + 0.001 * 6.7834625244140625
Epoch 840, val loss: 1.546791911125183
Epoch 850, training loss: 0.010660791769623756 = 0.0038412127178162336 + 0.001 * 6.819579124450684
Epoch 850, val loss: 1.553501844406128
Epoch 860, training loss: 0.010483736172318459 = 0.003703102469444275 + 0.001 * 6.780633449554443
Epoch 860, val loss: 1.560137391090393
Epoch 870, training loss: 0.010347628965973854 = 0.0035719890147447586 + 0.001 * 6.77564001083374
Epoch 870, val loss: 1.5667442083358765
Epoch 880, training loss: 0.010248899459838867 = 0.003447562223300338 + 0.001 * 6.801337242126465
Epoch 880, val loss: 1.5732532739639282
Epoch 890, training loss: 0.010117593221366405 = 0.0033295967150479555 + 0.001 * 6.787996292114258
Epoch 890, val loss: 1.5795862674713135
Epoch 900, training loss: 0.009999982081353664 = 0.0032178072724491358 + 0.001 * 6.782174587249756
Epoch 900, val loss: 1.585856318473816
Epoch 910, training loss: 0.009898107498884201 = 0.0031118132174015045 + 0.001 * 6.786294460296631
Epoch 910, val loss: 1.5920063257217407
Epoch 920, training loss: 0.009775960817933083 = 0.0030113309621810913 + 0.001 * 6.76462984085083
Epoch 920, val loss: 1.598020076751709
Epoch 930, training loss: 0.009699289686977863 = 0.002916033146902919 + 0.001 * 6.783256530761719
Epoch 930, val loss: 1.603933572769165
Epoch 940, training loss: 0.009593071416020393 = 0.0028257290832698345 + 0.001 * 6.767341613769531
Epoch 940, val loss: 1.6097501516342163
Epoch 950, training loss: 0.00950810220092535 = 0.0027400378603488207 + 0.001 * 6.768064022064209
Epoch 950, val loss: 1.6154011487960815
Epoch 960, training loss: 0.00942760705947876 = 0.0026586938183754683 + 0.001 * 6.768913269042969
Epoch 960, val loss: 1.6209253072738647
Epoch 970, training loss: 0.009334932081401348 = 0.002581394510343671 + 0.001 * 6.753537178039551
Epoch 970, val loss: 1.626358985900879
Epoch 980, training loss: 0.009260667487978935 = 0.002507870551198721 + 0.001 * 6.752796649932861
Epoch 980, val loss: 1.6316596269607544
Epoch 990, training loss: 0.009208595380187035 = 0.002437953371554613 + 0.001 * 6.770641326904297
Epoch 990, val loss: 1.6368528604507446
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.7159
Flip ASR: 0.6711/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9403080940246582 = 1.931934118270874 + 0.001 * 8.373916625976562
Epoch 0, val loss: 1.932325839996338
Epoch 10, training loss: 1.9313980340957642 = 1.9230241775512695 + 0.001 * 8.373866081237793
Epoch 10, val loss: 1.923707365989685
Epoch 20, training loss: 1.9206297397613525 = 1.9122560024261475 + 0.001 * 8.373685836791992
Epoch 20, val loss: 1.9128456115722656
Epoch 30, training loss: 1.9054843187332153 = 1.8971110582351685 + 0.001 * 8.373254776000977
Epoch 30, val loss: 1.8973007202148438
Epoch 40, training loss: 1.8827483654022217 = 1.8743761777877808 + 0.001 * 8.372214317321777
Epoch 40, val loss: 1.8741681575775146
Epoch 50, training loss: 1.8500016927719116 = 1.84163236618042 + 0.001 * 8.36935806274414
Epoch 50, val loss: 1.8422399759292603
Epoch 60, training loss: 1.8097896575927734 = 1.801430106163025 + 0.001 * 8.359594345092773
Epoch 60, val loss: 1.8060065507888794
Epoch 70, training loss: 1.7677570581436157 = 1.7594393491744995 + 0.001 * 8.317652702331543
Epoch 70, val loss: 1.7702094316482544
Epoch 80, training loss: 1.7147480249404907 = 1.706676721572876 + 0.001 * 8.071246147155762
Epoch 80, val loss: 1.722975730895996
Epoch 90, training loss: 1.6418694257736206 = 1.6341041326522827 + 0.001 * 7.765345573425293
Epoch 90, val loss: 1.6583646535873413
Epoch 100, training loss: 1.5470962524414062 = 1.539425253868103 + 0.001 * 7.671051025390625
Epoch 100, val loss: 1.5763890743255615
Epoch 110, training loss: 1.4358556270599365 = 1.4283580780029297 + 0.001 * 7.497524261474609
Epoch 110, val loss: 1.4829515218734741
Epoch 120, training loss: 1.3180747032165527 = 1.3107556104660034 + 0.001 * 7.319082736968994
Epoch 120, val loss: 1.3871732950210571
Epoch 130, training loss: 1.1995939016342163 = 1.1923692226409912 + 0.001 * 7.224680423736572
Epoch 130, val loss: 1.2924315929412842
Epoch 140, training loss: 1.085616946220398 = 1.0784599781036377 + 0.001 * 7.156965255737305
Epoch 140, val loss: 1.2033346891403198
Epoch 150, training loss: 0.9809426665306091 = 0.9738189578056335 + 0.001 * 7.12371301651001
Epoch 150, val loss: 1.123816967010498
Epoch 160, training loss: 0.8876895904541016 = 0.8805807828903198 + 0.001 * 7.108821868896484
Epoch 160, val loss: 1.0554449558258057
Epoch 170, training loss: 0.804669201374054 = 0.7975761890411377 + 0.001 * 7.092992305755615
Epoch 170, val loss: 0.9962452054023743
Epoch 180, training loss: 0.7294257879257202 = 0.7223494052886963 + 0.001 * 7.076365947723389
Epoch 180, val loss: 0.9445384740829468
Epoch 190, training loss: 0.6597236394882202 = 0.6526602506637573 + 0.001 * 7.063405990600586
Epoch 190, val loss: 0.8981063961982727
Epoch 200, training loss: 0.5944449305534363 = 0.5873908996582031 + 0.001 * 7.05402946472168
Epoch 200, val loss: 0.8562037944793701
Epoch 210, training loss: 0.5335317254066467 = 0.526484489440918 + 0.001 * 7.047220230102539
Epoch 210, val loss: 0.8189456462860107
Epoch 220, training loss: 0.47753337025642395 = 0.47049230337142944 + 0.001 * 7.041078567504883
Epoch 220, val loss: 0.7871788740158081
Epoch 230, training loss: 0.42692574858665466 = 0.41989272832870483 + 0.001 * 7.033020973205566
Epoch 230, val loss: 0.7612239122390747
Epoch 240, training loss: 0.38170620799064636 = 0.37468621134757996 + 0.001 * 7.019989013671875
Epoch 240, val loss: 0.7408093214035034
Epoch 250, training loss: 0.34143826365470886 = 0.3344278633594513 + 0.001 * 7.010400772094727
Epoch 250, val loss: 0.7253857851028442
Epoch 260, training loss: 0.3054155707359314 = 0.2984277606010437 + 0.001 * 6.987810134887695
Epoch 260, val loss: 0.7144866585731506
Epoch 270, training loss: 0.27291709184646606 = 0.2659488320350647 + 0.001 * 6.968255043029785
Epoch 270, val loss: 0.7076086401939392
Epoch 280, training loss: 0.2433546483516693 = 0.23640087246894836 + 0.001 * 6.9537763595581055
Epoch 280, val loss: 0.7042372822761536
Epoch 290, training loss: 0.2163829207420349 = 0.20943664014339447 + 0.001 * 6.946281909942627
Epoch 290, val loss: 0.7037959098815918
Epoch 300, training loss: 0.1918485015630722 = 0.18489821255207062 + 0.001 * 6.950288772583008
Epoch 300, val loss: 0.7061358690261841
Epoch 310, training loss: 0.16967731714248657 = 0.1627337634563446 + 0.001 * 6.943556785583496
Epoch 310, val loss: 0.7111349105834961
Epoch 320, training loss: 0.14987006783485413 = 0.14292612671852112 + 0.001 * 6.943942546844482
Epoch 320, val loss: 0.7186235189437866
Epoch 330, training loss: 0.13233937323093414 = 0.12539516389369965 + 0.001 * 6.944214820861816
Epoch 330, val loss: 0.7283656597137451
Epoch 340, training loss: 0.11694730073213577 = 0.11000235378742218 + 0.001 * 6.9449462890625
Epoch 340, val loss: 0.7401856184005737
Epoch 350, training loss: 0.10356006771326065 = 0.09661433100700378 + 0.001 * 6.945736408233643
Epoch 350, val loss: 0.7536903023719788
Epoch 360, training loss: 0.09200479090213776 = 0.08505832403898239 + 0.001 * 6.946470260620117
Epoch 360, val loss: 0.7683144807815552
Epoch 370, training loss: 0.08208687603473663 = 0.07513891905546188 + 0.001 * 6.947954177856445
Epoch 370, val loss: 0.7836342453956604
Epoch 380, training loss: 0.07359740883111954 = 0.0666479542851448 + 0.001 * 6.949453830718994
Epoch 380, val loss: 0.7992176413536072
Epoch 390, training loss: 0.06632685661315918 = 0.05937863513827324 + 0.001 * 6.94821834564209
Epoch 390, val loss: 0.8147434592247009
Epoch 400, training loss: 0.0600869320333004 = 0.053138185292482376 + 0.001 * 6.948746204376221
Epoch 400, val loss: 0.8300716876983643
Epoch 410, training loss: 0.05470893159508705 = 0.04776021093130112 + 0.001 * 6.9487199783325195
Epoch 410, val loss: 0.8451015949249268
Epoch 420, training loss: 0.050055116415023804 = 0.043105822056531906 + 0.001 * 6.949294567108154
Epoch 420, val loss: 0.8597646951675415
Epoch 430, training loss: 0.04600851237773895 = 0.03905947878956795 + 0.001 * 6.949034690856934
Epoch 430, val loss: 0.8740556836128235
Epoch 440, training loss: 0.04247596114873886 = 0.03552674502134323 + 0.001 * 6.949214458465576
Epoch 440, val loss: 0.887947142124176
Epoch 450, training loss: 0.03937835618853569 = 0.03242973983287811 + 0.001 * 6.948617458343506
Epoch 450, val loss: 0.9014340043067932
Epoch 460, training loss: 0.03665166720747948 = 0.029702480882406235 + 0.001 * 6.949185371398926
Epoch 460, val loss: 0.9145081043243408
Epoch 470, training loss: 0.03423910215497017 = 0.02729150280356407 + 0.001 * 6.947598934173584
Epoch 470, val loss: 0.9271934628486633
Epoch 480, training loss: 0.03211396187543869 = 0.025152107700705528 + 0.001 * 6.961854457855225
Epoch 480, val loss: 0.9395056962966919
Epoch 490, training loss: 0.030196331441402435 = 0.02324693277478218 + 0.001 * 6.949398040771484
Epoch 490, val loss: 0.9514521956443787
Epoch 500, training loss: 0.02849082462489605 = 0.0215444453060627 + 0.001 * 6.9463791847229
Epoch 500, val loss: 0.9630122184753418
Epoch 510, training loss: 0.026963427662849426 = 0.02001827582716942 + 0.001 * 6.945150375366211
Epoch 510, val loss: 0.9741936326026917
Epoch 520, training loss: 0.025590253993868828 = 0.018645895645022392 + 0.001 * 6.944357872009277
Epoch 520, val loss: 0.9850503206253052
Epoch 530, training loss: 0.024351218715310097 = 0.017408030107617378 + 0.001 * 6.943188667297363
Epoch 530, val loss: 0.9955885410308838
Epoch 540, training loss: 0.023232072591781616 = 0.01628829725086689 + 0.001 * 6.943775653839111
Epoch 540, val loss: 1.0057841539382935
Epoch 550, training loss: 0.022214528173208237 = 0.015272547490894794 + 0.001 * 6.941981315612793
Epoch 550, val loss: 1.0156844854354858
Epoch 560, training loss: 0.021287646144628525 = 0.014348329044878483 + 0.001 * 6.93931770324707
Epoch 560, val loss: 1.025264024734497
Epoch 570, training loss: 0.020445186644792557 = 0.013503818772733212 + 0.001 * 6.941367149353027
Epoch 570, val loss: 1.0345956087112427
Epoch 580, training loss: 0.01966460421681404 = 0.012727548368275166 + 0.001 * 6.937055587768555
Epoch 580, val loss: 1.0436657667160034
Epoch 590, training loss: 0.018945686519145966 = 0.012010577134788036 + 0.001 * 6.935109615325928
Epoch 590, val loss: 1.0525325536727905
Epoch 600, training loss: 0.01828346960246563 = 0.011346694082021713 + 0.001 * 6.936774730682373
Epoch 600, val loss: 1.0611944198608398
Epoch 610, training loss: 0.017667410895228386 = 0.010731414891779423 + 0.001 * 6.935995578765869
Epoch 610, val loss: 1.0696712732315063
Epoch 620, training loss: 0.01709316298365593 = 0.010160960257053375 + 0.001 * 6.9322028160095215
Epoch 620, val loss: 1.0779509544372559
Epoch 630, training loss: 0.01655436120927334 = 0.009631864726543427 + 0.001 * 6.9224958419799805
Epoch 630, val loss: 1.0860462188720703
Epoch 640, training loss: 0.01607319340109825 = 0.00914094876497984 + 0.001 * 6.932244300842285
Epoch 640, val loss: 1.0939407348632812
Epoch 650, training loss: 0.015610113739967346 = 0.008685176260769367 + 0.001 * 6.924936771392822
Epoch 650, val loss: 1.1016696691513062
Epoch 660, training loss: 0.01517682895064354 = 0.00826182123273611 + 0.001 * 6.915008068084717
Epoch 660, val loss: 1.1092233657836914
Epoch 670, training loss: 0.014776893891394138 = 0.007868260145187378 + 0.001 * 6.908633232116699
Epoch 670, val loss: 1.1166090965270996
Epoch 680, training loss: 0.014447670429944992 = 0.007502097170799971 + 0.001 * 6.945572853088379
Epoch 680, val loss: 1.123827338218689
Epoch 690, training loss: 0.014062481001019478 = 0.00716107664629817 + 0.001 * 6.90140438079834
Epoch 690, val loss: 1.1308776140213013
Epoch 700, training loss: 0.01373826153576374 = 0.006843126844614744 + 0.001 * 6.895133972167969
Epoch 700, val loss: 1.1377589702606201
Epoch 710, training loss: 0.013476643711328506 = 0.006546398624777794 + 0.001 * 6.930244445800781
Epoch 710, val loss: 1.1444923877716064
Epoch 720, training loss: 0.013169869780540466 = 0.006269216537475586 + 0.001 * 6.9006524085998535
Epoch 720, val loss: 1.1510237455368042
Epoch 730, training loss: 0.012905619107186794 = 0.00600984925404191 + 0.001 * 6.8957695960998535
Epoch 730, val loss: 1.1574342250823975
Epoch 740, training loss: 0.01264750212430954 = 0.005766875110566616 + 0.001 * 6.880627155303955
Epoch 740, val loss: 1.1636751890182495
Epoch 750, training loss: 0.012407641857862473 = 0.005539040081202984 + 0.001 * 6.8686017990112305
Epoch 750, val loss: 1.1697980165481567
Epoch 760, training loss: 0.01224077120423317 = 0.0053251963108778 + 0.001 * 6.91557502746582
Epoch 760, val loss: 1.1757885217666626
Epoch 770, training loss: 0.012017861008644104 = 0.005124418064951897 + 0.001 * 6.893442153930664
Epoch 770, val loss: 1.1815896034240723
Epoch 780, training loss: 0.011814316734671593 = 0.004935523960739374 + 0.001 * 6.8787922859191895
Epoch 780, val loss: 1.1873116493225098
Epoch 790, training loss: 0.011614326387643814 = 0.004757556598633528 + 0.001 * 6.85676908493042
Epoch 790, val loss: 1.1928561925888062
Epoch 800, training loss: 0.011428331956267357 = 0.004589701537042856 + 0.001 * 6.838629722595215
Epoch 800, val loss: 1.1982815265655518
Epoch 810, training loss: 0.011281333863735199 = 0.0044311839155852795 + 0.001 * 6.850150108337402
Epoch 810, val loss: 1.2036110162734985
Epoch 820, training loss: 0.011137732304632664 = 0.004281359724700451 + 0.001 * 6.856372356414795
Epoch 820, val loss: 1.2088055610656738
Epoch 830, training loss: 0.01102013885974884 = 0.004139622673392296 + 0.001 * 6.8805155754089355
Epoch 830, val loss: 1.2139009237289429
Epoch 840, training loss: 0.01084187999367714 = 0.00400538882240653 + 0.001 * 6.836490631103516
Epoch 840, val loss: 1.2188884019851685
Epoch 850, training loss: 0.010717874392867088 = 0.003878036281093955 + 0.001 * 6.839838027954102
Epoch 850, val loss: 1.2237778902053833
Epoch 860, training loss: 0.01057281531393528 = 0.0037571112625300884 + 0.001 * 6.815703868865967
Epoch 860, val loss: 1.2285592555999756
Epoch 870, training loss: 0.010471723042428493 = 0.0036421180702745914 + 0.001 * 6.829604625701904
Epoch 870, val loss: 1.233255386352539
Epoch 880, training loss: 0.010382404550909996 = 0.003532631788402796 + 0.001 * 6.849771976470947
Epoch 880, val loss: 1.2378703355789185
Epoch 890, training loss: 0.010259980335831642 = 0.003428237745538354 + 0.001 * 6.831742286682129
Epoch 890, val loss: 1.2423917055130005
Epoch 900, training loss: 0.010122476145625114 = 0.003328614868223667 + 0.001 * 6.793860912322998
Epoch 900, val loss: 1.2468550205230713
Epoch 910, training loss: 0.010056017898023129 = 0.0032334288116544485 + 0.001 * 6.822588920593262
Epoch 910, val loss: 1.251260757446289
Epoch 920, training loss: 0.009976401925086975 = 0.003142521483823657 + 0.001 * 6.833880424499512
Epoch 920, val loss: 1.255536675453186
Epoch 930, training loss: 0.0098795834928751 = 0.0030555555131286383 + 0.001 * 6.8240275382995605
Epoch 930, val loss: 1.2598379850387573
Epoch 940, training loss: 0.009762155823409557 = 0.002972423331812024 + 0.001 * 6.789731979370117
Epoch 940, val loss: 1.2639974355697632
Epoch 950, training loss: 0.009701428934931755 = 0.002892825286835432 + 0.001 * 6.808603763580322
Epoch 950, val loss: 1.2681561708450317
Epoch 960, training loss: 0.00961668323725462 = 0.0028165539260953665 + 0.001 * 6.800128936767578
Epoch 960, val loss: 1.2721941471099854
Epoch 970, training loss: 0.009519264101982117 = 0.002743467455729842 + 0.001 * 6.775796413421631
Epoch 970, val loss: 1.2761999368667603
Epoch 980, training loss: 0.009459858760237694 = 0.0026733619160950184 + 0.001 * 6.786496162414551
Epoch 980, val loss: 1.280124306678772
Epoch 990, training loss: 0.009385541081428528 = 0.002606126479804516 + 0.001 * 6.779414176940918
Epoch 990, val loss: 1.2840255498886108
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.8635
Flip ASR: 0.8356/225 nodes
The final ASR:0.66421, 0.18738, Accuracy:0.80370, 0.02688
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10556])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.964994192123413 = 1.9566203355789185 + 0.001 * 8.37389087677002
Epoch 0, val loss: 1.9644438028335571
Epoch 10, training loss: 1.9545388221740723 = 1.9461649656295776 + 0.001 * 8.373801231384277
Epoch 10, val loss: 1.954624891281128
Epoch 20, training loss: 1.9411978721618652 = 1.9328242540359497 + 0.001 * 8.373558044433594
Epoch 20, val loss: 1.9416730403900146
Epoch 30, training loss: 1.9219526052474976 = 1.9135795831680298 + 0.001 * 8.372991561889648
Epoch 30, val loss: 1.9226984977722168
Epoch 40, training loss: 1.893093228340149 = 1.8847217559814453 + 0.001 * 8.37147045135498
Epoch 40, val loss: 1.8946380615234375
Epoch 50, training loss: 1.8527203798294067 = 1.8443539142608643 + 0.001 * 8.366460800170898
Epoch 50, val loss: 1.8573038578033447
Epoch 60, training loss: 1.8078618049621582 = 1.7995164394378662 + 0.001 * 8.34534740447998
Epoch 60, val loss: 1.8193776607513428
Epoch 70, training loss: 1.768099308013916 = 1.7598607540130615 + 0.001 * 8.238603591918945
Epoch 70, val loss: 1.7850676774978638
Epoch 80, training loss: 1.7182247638702393 = 1.7103396654129028 + 0.001 * 7.885061264038086
Epoch 80, val loss: 1.7379075288772583
Epoch 90, training loss: 1.6504894495010376 = 1.6426982879638672 + 0.001 * 7.791147708892822
Epoch 90, val loss: 1.6787781715393066
Epoch 100, training loss: 1.56306791305542 = 1.5553454160690308 + 0.001 * 7.722499370574951
Epoch 100, val loss: 1.6071786880493164
Epoch 110, training loss: 1.466252326965332 = 1.4586377143859863 + 0.001 * 7.61456823348999
Epoch 110, val loss: 1.5276545286178589
Epoch 120, training loss: 1.3713358640670776 = 1.3639522790908813 + 0.001 * 7.383594512939453
Epoch 120, val loss: 1.451933741569519
Epoch 130, training loss: 1.2814654111862183 = 1.2741961479187012 + 0.001 * 7.269217491149902
Epoch 130, val loss: 1.3810826539993286
Epoch 140, training loss: 1.195603370666504 = 1.1883697509765625 + 0.001 * 7.233651161193848
Epoch 140, val loss: 1.3161566257476807
Epoch 150, training loss: 1.1125432252883911 = 1.1053701639175415 + 0.001 * 7.173099040985107
Epoch 150, val loss: 1.2554845809936523
Epoch 160, training loss: 1.0308085680007935 = 1.0236895084381104 + 0.001 * 7.119020462036133
Epoch 160, val loss: 1.1968392133712769
Epoch 170, training loss: 0.9495195746421814 = 0.9424498677253723 + 0.001 * 7.069716453552246
Epoch 170, val loss: 1.1391611099243164
Epoch 180, training loss: 0.8699249029159546 = 0.8628758192062378 + 0.001 * 7.049107074737549
Epoch 180, val loss: 1.08216392993927
Epoch 190, training loss: 0.7948362827301025 = 0.7877998352050781 + 0.001 * 7.036460876464844
Epoch 190, val loss: 1.0282775163650513
Epoch 200, training loss: 0.7268438935279846 = 0.7198257446289062 + 0.001 * 7.0181660652160645
Epoch 200, val loss: 0.979922354221344
Epoch 210, training loss: 0.6666024327278137 = 0.659611701965332 + 0.001 * 6.990743637084961
Epoch 210, val loss: 0.9385854005813599
Epoch 220, training loss: 0.6125567555427551 = 0.6055861711502075 + 0.001 * 6.970560550689697
Epoch 220, val loss: 0.9035434126853943
Epoch 230, training loss: 0.56257563829422 = 0.5556291937828064 + 0.001 * 6.9464335441589355
Epoch 230, val loss: 0.8730404376983643
Epoch 240, training loss: 0.5151577591896057 = 0.5082217454910278 + 0.001 * 6.936025142669678
Epoch 240, val loss: 0.8457943201065063
Epoch 250, training loss: 0.4699115753173828 = 0.46297863125801086 + 0.001 * 6.932958126068115
Epoch 250, val loss: 0.8223713636398315
Epoch 260, training loss: 0.4263876676559448 = 0.41945645213127136 + 0.001 * 6.931210517883301
Epoch 260, val loss: 0.8033011555671692
Epoch 270, training loss: 0.38431990146636963 = 0.3773898482322693 + 0.001 * 6.930037975311279
Epoch 270, val loss: 0.7890208959579468
Epoch 280, training loss: 0.34360814094543457 = 0.3366795480251312 + 0.001 * 6.928599834442139
Epoch 280, val loss: 0.7793991565704346
Epoch 290, training loss: 0.30465760827064514 = 0.2977307140827179 + 0.001 * 6.926880836486816
Epoch 290, val loss: 0.7738478183746338
Epoch 300, training loss: 0.2683103382587433 = 0.2613852024078369 + 0.001 * 6.925140380859375
Epoch 300, val loss: 0.7720348834991455
Epoch 310, training loss: 0.23536987602710724 = 0.22844600677490234 + 0.001 * 6.923868656158447
Epoch 310, val loss: 0.7732856869697571
Epoch 320, training loss: 0.20619672536849976 = 0.19927379488945007 + 0.001 * 6.922929763793945
Epoch 320, val loss: 0.7777374386787415
Epoch 330, training loss: 0.18069805204868317 = 0.17377720773220062 + 0.001 * 6.920849800109863
Epoch 330, val loss: 0.7849677205085754
Epoch 340, training loss: 0.15856356918811798 = 0.1516445130109787 + 0.001 * 6.919052600860596
Epoch 340, val loss: 0.7947713136672974
Epoch 350, training loss: 0.13943593204021454 = 0.13251900672912598 + 0.001 * 6.916926860809326
Epoch 350, val loss: 0.8066728115081787
Epoch 360, training loss: 0.12294695526361465 = 0.11603224277496338 + 0.001 * 6.91471529006958
Epoch 360, val loss: 0.8203444480895996
Epoch 370, training loss: 0.10872659832239151 = 0.10181611776351929 + 0.001 * 6.910478591918945
Epoch 370, val loss: 0.8353891372680664
Epoch 380, training loss: 0.09644679725170135 = 0.08954045176506042 + 0.001 * 6.906344413757324
Epoch 380, val loss: 0.8513054251670837
Epoch 390, training loss: 0.08581985533237457 = 0.07891920953989029 + 0.001 * 6.900644302368164
Epoch 390, val loss: 0.8677172660827637
Epoch 400, training loss: 0.0766100212931633 = 0.06971456855535507 + 0.001 * 6.895449638366699
Epoch 400, val loss: 0.8843172192573547
Epoch 410, training loss: 0.06861109286546707 = 0.0617254413664341 + 0.001 * 6.885650634765625
Epoch 410, val loss: 0.9008144736289978
Epoch 420, training loss: 0.061679791659116745 = 0.05478443205356598 + 0.001 * 6.895360469818115
Epoch 420, val loss: 0.916982889175415
Epoch 430, training loss: 0.05561622604727745 = 0.04874204844236374 + 0.001 * 6.874177932739258
Epoch 430, val loss: 0.9329156279563904
Epoch 440, training loss: 0.050342097878456116 = 0.04347214847803116 + 0.001 * 6.869948863983154
Epoch 440, val loss: 0.9483723044395447
Epoch 450, training loss: 0.04573790729045868 = 0.03887505829334259 + 0.001 * 6.862846851348877
Epoch 450, val loss: 0.9634063839912415
Epoch 460, training loss: 0.041728489100933075 = 0.03486461192369461 + 0.001 * 6.863874912261963
Epoch 460, val loss: 0.978011429309845
Epoch 470, training loss: 0.038204193115234375 = 0.03136521950364113 + 0.001 * 6.838972091674805
Epoch 470, val loss: 0.9921426177024841
Epoch 480, training loss: 0.03514102101325989 = 0.028310123831033707 + 0.001 * 6.83089542388916
Epoch 480, val loss: 1.0058213472366333
Epoch 490, training loss: 0.03246210515499115 = 0.025640137493610382 + 0.001 * 6.821968078613281
Epoch 490, val loss: 1.0190784931182861
Epoch 500, training loss: 0.030127163976430893 = 0.023303121328353882 + 0.001 * 6.824041843414307
Epoch 500, val loss: 1.031846284866333
Epoch 510, training loss: 0.028075486421585083 = 0.02125251665711403 + 0.001 * 6.822969913482666
Epoch 510, val loss: 1.044171690940857
Epoch 520, training loss: 0.02626729942858219 = 0.01944868639111519 + 0.001 * 6.818612575531006
Epoch 520, val loss: 1.0560529232025146
Epoch 530, training loss: 0.024664256721735 = 0.017857082188129425 + 0.001 * 6.8071746826171875
Epoch 530, val loss: 1.0675584077835083
Epoch 540, training loss: 0.023255130276083946 = 0.016448475420475006 + 0.001 * 6.806654930114746
Epoch 540, val loss: 1.0786831378936768
Epoch 550, training loss: 0.022003259509801865 = 0.015198052860796452 + 0.001 * 6.805205345153809
Epoch 550, val loss: 1.089421033859253
Epoch 560, training loss: 0.020885862410068512 = 0.01408463716506958 + 0.001 * 6.801225185394287
Epoch 560, val loss: 1.0998042821884155
Epoch 570, training loss: 0.019900495186448097 = 0.013089746236801147 + 0.001 * 6.810749053955078
Epoch 570, val loss: 1.109837293624878
Epoch 580, training loss: 0.018999753519892693 = 0.012198025360703468 + 0.001 * 6.801727294921875
Epoch 580, val loss: 1.1195557117462158
Epoch 590, training loss: 0.018190622329711914 = 0.011396043002605438 + 0.001 * 6.794578552246094
Epoch 590, val loss: 1.1289960145950317
Epoch 600, training loss: 0.01747463271021843 = 0.010672567412257195 + 0.001 * 6.802065372467041
Epoch 600, val loss: 1.138123631477356
Epoch 610, training loss: 0.016817644238471985 = 0.010017922148108482 + 0.001 * 6.799722194671631
Epoch 610, val loss: 1.146960735321045
Epoch 620, training loss: 0.016216972842812538 = 0.009423799812793732 + 0.001 * 6.793172359466553
Epoch 620, val loss: 1.155519962310791
Epoch 630, training loss: 0.015677455812692642 = 0.00888308510184288 + 0.001 * 6.794369697570801
Epoch 630, val loss: 1.1638015508651733
Epoch 640, training loss: 0.015177280642092228 = 0.008389711380004883 + 0.001 * 6.787569046020508
Epoch 640, val loss: 1.1718144416809082
Epoch 650, training loss: 0.014737386256456375 = 0.00793843436986208 + 0.001 * 6.798952102661133
Epoch 650, val loss: 1.1795860528945923
Epoch 660, training loss: 0.014314161613583565 = 0.007524679880589247 + 0.001 * 6.789481163024902
Epoch 660, val loss: 1.187166690826416
Epoch 670, training loss: 0.01393367163836956 = 0.0071443975903093815 + 0.001 * 6.789274215698242
Epoch 670, val loss: 1.194503664970398
Epoch 680, training loss: 0.013579221442341805 = 0.00679408572614193 + 0.001 * 6.785135269165039
Epoch 680, val loss: 1.2016404867172241
Epoch 690, training loss: 0.013264777138829231 = 0.006470678839832544 + 0.001 * 6.794097900390625
Epoch 690, val loss: 1.208572268486023
Epoch 700, training loss: 0.012955805286765099 = 0.006171509623527527 + 0.001 * 6.784295558929443
Epoch 700, val loss: 1.2152982950210571
Epoch 710, training loss: 0.01268077827990055 = 0.005894201807677746 + 0.001 * 6.786576271057129
Epoch 710, val loss: 1.2218419313430786
Epoch 720, training loss: 0.012417735531926155 = 0.005636698100715876 + 0.001 * 6.781037330627441
Epoch 720, val loss: 1.2282265424728394
Epoch 730, training loss: 0.012177947908639908 = 0.005397141445428133 + 0.001 * 6.780805587768555
Epoch 730, val loss: 1.234403133392334
Epoch 740, training loss: 0.011943780817091465 = 0.00517393508926034 + 0.001 * 6.769845485687256
Epoch 740, val loss: 1.2404454946517944
Epoch 750, training loss: 0.011748220771551132 = 0.004965610336512327 + 0.001 * 6.7826104164123535
Epoch 750, val loss: 1.2463277578353882
Epoch 760, training loss: 0.011544675566256046 = 0.004770850762724876 + 0.001 * 6.773824691772461
Epoch 760, val loss: 1.2520556449890137
Epoch 770, training loss: 0.011362995952367783 = 0.004588492214679718 + 0.001 * 6.774503707885742
Epoch 770, val loss: 1.257648229598999
Epoch 780, training loss: 0.011188734322786331 = 0.00441751116886735 + 0.001 * 6.7712225914001465
Epoch 780, val loss: 1.263105034828186
Epoch 790, training loss: 0.011021140962839127 = 0.004256955347955227 + 0.001 * 6.764185428619385
Epoch 790, val loss: 1.2684271335601807
Epoch 800, training loss: 0.010875165462493896 = 0.004106014035642147 + 0.001 * 6.769150733947754
Epoch 800, val loss: 1.273601770401001
Epoch 810, training loss: 0.010735531337559223 = 0.003963932860642672 + 0.001 * 6.7715983390808105
Epoch 810, val loss: 1.2786388397216797
Epoch 820, training loss: 0.01059934962540865 = 0.0038300224114209414 + 0.001 * 6.769326686859131
Epoch 820, val loss: 1.2835922241210938
Epoch 830, training loss: 0.010462392121553421 = 0.0037036649882793427 + 0.001 * 6.758727073669434
Epoch 830, val loss: 1.2884222269058228
Epoch 840, training loss: 0.01034022681415081 = 0.003584307851269841 + 0.001 * 6.755918502807617
Epoch 840, val loss: 1.2931402921676636
Epoch 850, training loss: 0.010243446566164494 = 0.0034714494831860065 + 0.001 * 6.771996974945068
Epoch 850, val loss: 1.2977521419525146
Epoch 860, training loss: 0.010124928317964077 = 0.0033646239899098873 + 0.001 * 6.760303974151611
Epoch 860, val loss: 1.3022576570510864
Epoch 870, training loss: 0.010031376965343952 = 0.0032633875962346792 + 0.001 * 6.767988681793213
Epoch 870, val loss: 1.3066675662994385
Epoch 880, training loss: 0.00992531981319189 = 0.0031673854682594538 + 0.001 * 6.757933616638184
Epoch 880, val loss: 1.3109737634658813
Epoch 890, training loss: 0.009844336658716202 = 0.0030762157402932644 + 0.001 * 6.768120288848877
Epoch 890, val loss: 1.3151992559432983
Epoch 900, training loss: 0.009738622233271599 = 0.002989578992128372 + 0.001 * 6.749042510986328
Epoch 900, val loss: 1.31932532787323
Epoch 910, training loss: 0.009653997607529163 = 0.002907153917476535 + 0.001 * 6.746843338012695
Epoch 910, val loss: 1.3233675956726074
Epoch 920, training loss: 0.009574895724654198 = 0.0028286981396377087 + 0.001 * 6.746196746826172
Epoch 920, val loss: 1.3273247480392456
Epoch 930, training loss: 0.009511011652648449 = 0.002753956476226449 + 0.001 * 6.757054805755615
Epoch 930, val loss: 1.3311963081359863
Epoch 940, training loss: 0.009437156841158867 = 0.0026826877146959305 + 0.001 * 6.754469394683838
Epoch 940, val loss: 1.3350075483322144
Epoch 950, training loss: 0.009357192553579807 = 0.0026146816089749336 + 0.001 * 6.742510795593262
Epoch 950, val loss: 1.338732123374939
Epoch 960, training loss: 0.009296458214521408 = 0.0025497335009276867 + 0.001 * 6.746725082397461
Epoch 960, val loss: 1.3423928022384644
Epoch 970, training loss: 0.009227927774190903 = 0.0024876717943698168 + 0.001 * 6.740255832672119
Epoch 970, val loss: 1.3459447622299194
Epoch 980, training loss: 0.009165463969111443 = 0.0024283123202621937 + 0.001 * 6.737151622772217
Epoch 980, val loss: 1.3494610786437988
Epoch 990, training loss: 0.009119869209825993 = 0.002371530281379819 + 0.001 * 6.748338222503662
Epoch 990, val loss: 1.352853536605835
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9707322120666504 = 1.9623583555221558 + 0.001 * 8.373903274536133
Epoch 0, val loss: 1.9715293645858765
Epoch 10, training loss: 1.959062099456787 = 1.9506882429122925 + 0.001 * 8.3738374710083
Epoch 10, val loss: 1.9595937728881836
Epoch 20, training loss: 1.944492220878601 = 1.9361186027526855 + 0.001 * 8.373619079589844
Epoch 20, val loss: 1.944494366645813
Epoch 30, training loss: 1.9239990711212158 = 1.9156259298324585 + 0.001 * 8.373092651367188
Epoch 30, val loss: 1.923253059387207
Epoch 40, training loss: 1.894111156463623 = 1.8857394456863403 + 0.001 * 8.371758460998535
Epoch 40, val loss: 1.892587661743164
Epoch 50, training loss: 1.853324294090271 = 1.844956636428833 + 0.001 * 8.36768627166748
Epoch 50, val loss: 1.8525588512420654
Epoch 60, training loss: 1.8089505434036255 = 1.8005990982055664 + 0.001 * 8.351460456848145
Epoch 60, val loss: 1.8136036396026611
Epoch 70, training loss: 1.7719027996063232 = 1.763639211654663 + 0.001 * 8.263555526733398
Epoch 70, val loss: 1.7837622165679932
Epoch 80, training loss: 1.7267814874649048 = 1.7188961505889893 + 0.001 * 7.885383605957031
Epoch 80, val loss: 1.7444814443588257
Epoch 90, training loss: 1.6652672290802002 = 1.6575236320495605 + 0.001 * 7.743597984313965
Epoch 90, val loss: 1.6917572021484375
Epoch 100, training loss: 1.5824471712112427 = 1.5748604536056519 + 0.001 * 7.586690902709961
Epoch 100, val loss: 1.6220594644546509
Epoch 110, training loss: 1.4846364259719849 = 1.4772794246673584 + 0.001 * 7.356962203979492
Epoch 110, val loss: 1.5415730476379395
Epoch 120, training loss: 1.3816893100738525 = 1.3744947910308838 + 0.001 * 7.194498538970947
Epoch 120, val loss: 1.4579601287841797
Epoch 130, training loss: 1.2775421142578125 = 1.270389199256897 + 0.001 * 7.152856349945068
Epoch 130, val loss: 1.3752710819244385
Epoch 140, training loss: 1.1730623245239258 = 1.1659572124481201 + 0.001 * 7.105075359344482
Epoch 140, val loss: 1.2943668365478516
Epoch 150, training loss: 1.0702797174453735 = 1.0631963014602661 + 0.001 * 7.083445072174072
Epoch 150, val loss: 1.2152442932128906
Epoch 160, training loss: 0.9721332788467407 = 0.9650609493255615 + 0.001 * 7.072350978851318
Epoch 160, val loss: 1.1397206783294678
Epoch 170, training loss: 0.8803481459617615 = 0.8732838034629822 + 0.001 * 7.064352512359619
Epoch 170, val loss: 1.0689865350723267
Epoch 180, training loss: 0.7956048250198364 = 0.78854900598526 + 0.001 * 7.055837154388428
Epoch 180, val loss: 1.0036286115646362
Epoch 190, training loss: 0.7180763483047485 = 0.711029589176178 + 0.001 * 7.046749114990234
Epoch 190, val loss: 0.944405734539032
Epoch 200, training loss: 0.6471713781356812 = 0.6401362419128418 + 0.001 * 7.035146713256836
Epoch 200, val loss: 0.8918283581733704
Epoch 210, training loss: 0.5817470550537109 = 0.5747281908988953 + 0.001 * 7.018834590911865
Epoch 210, val loss: 0.8457999229431152
Epoch 220, training loss: 0.5212061405181885 = 0.5142082571983337 + 0.001 * 6.997864723205566
Epoch 220, val loss: 0.8065383434295654
Epoch 230, training loss: 0.4656919538974762 = 0.4587068259716034 + 0.001 * 6.985116481781006
Epoch 230, val loss: 0.7747626900672913
Epoch 240, training loss: 0.4156418442726135 = 0.4086625874042511 + 0.001 * 6.979248523712158
Epoch 240, val loss: 0.7505860924720764
Epoch 250, training loss: 0.37111395597457886 = 0.3641393482685089 + 0.001 * 6.974610805511475
Epoch 250, val loss: 0.7332321405410767
Epoch 260, training loss: 0.3316628634929657 = 0.32468968629837036 + 0.001 * 6.973182678222656
Epoch 260, val loss: 0.7215473055839539
Epoch 270, training loss: 0.29639753699302673 = 0.28942611813545227 + 0.001 * 6.9714250564575195
Epoch 270, val loss: 0.7143808007240295
Epoch 280, training loss: 0.2643697261810303 = 0.25739923119544983 + 0.001 * 6.970492362976074
Epoch 280, val loss: 0.7107971906661987
Epoch 290, training loss: 0.23490986227989197 = 0.22794243693351746 + 0.001 * 6.967427730560303
Epoch 290, val loss: 0.7101312875747681
Epoch 300, training loss: 0.2078406810760498 = 0.200877845287323 + 0.001 * 6.962841510772705
Epoch 300, val loss: 0.7120028138160706
Epoch 310, training loss: 0.18335893750190735 = 0.17639854550361633 + 0.001 * 6.960396766662598
Epoch 310, val loss: 0.7163459062576294
Epoch 320, training loss: 0.1617063730955124 = 0.15475425124168396 + 0.001 * 6.952117919921875
Epoch 320, val loss: 0.7229530215263367
Epoch 330, training loss: 0.14293324947357178 = 0.1359851211309433 + 0.001 * 6.948127269744873
Epoch 330, val loss: 0.7316431403160095
Epoch 340, training loss: 0.12681891024112701 = 0.11987311393022537 + 0.001 * 6.945797443389893
Epoch 340, val loss: 0.7420694828033447
Epoch 350, training loss: 0.11300185322761536 = 0.1060628741979599 + 0.001 * 6.938980579376221
Epoch 350, val loss: 0.7538636922836304
Epoch 360, training loss: 0.1011202335357666 = 0.09418369084596634 + 0.001 * 6.9365434646606445
Epoch 360, val loss: 0.7667604684829712
Epoch 370, training loss: 0.09083227813243866 = 0.08391628414392471 + 0.001 * 6.91599178314209
Epoch 370, val loss: 0.7804893851280212
Epoch 380, training loss: 0.08191059529781342 = 0.07499945163726807 + 0.001 * 6.911139965057373
Epoch 380, val loss: 0.7947607040405273
Epoch 390, training loss: 0.07413487881422043 = 0.06722226738929749 + 0.001 * 6.9126105308532715
Epoch 390, val loss: 0.8093197345733643
Epoch 400, training loss: 0.06733101606369019 = 0.060413897037506104 + 0.001 * 6.917121410369873
Epoch 400, val loss: 0.8241007328033447
Epoch 410, training loss: 0.06134137883782387 = 0.05443781986832619 + 0.001 * 6.903559684753418
Epoch 410, val loss: 0.8389518857002258
Epoch 420, training loss: 0.056092046201229095 = 0.04918202757835388 + 0.001 * 6.9100189208984375
Epoch 420, val loss: 0.8538872003555298
Epoch 430, training loss: 0.05145001411437988 = 0.044553980231285095 + 0.001 * 6.896031856536865
Epoch 430, val loss: 0.8687408566474915
Epoch 440, training loss: 0.04735711216926575 = 0.040473151952028275 + 0.001 * 6.8839616775512695
Epoch 440, val loss: 0.8835370540618896
Epoch 450, training loss: 0.043749790638685226 = 0.03687020018696785 + 0.001 * 6.879589080810547
Epoch 450, val loss: 0.8981773257255554
Epoch 460, training loss: 0.0405631847679615 = 0.033684998750686646 + 0.001 * 6.878184795379639
Epoch 460, val loss: 0.9128141403198242
Epoch 470, training loss: 0.03774486109614372 = 0.030864443629980087 + 0.001 * 6.880418300628662
Epoch 470, val loss: 0.9272234439849854
Epoch 480, training loss: 0.0352344773709774 = 0.02836034633219242 + 0.001 * 6.874130725860596
Epoch 480, val loss: 0.9414933323860168
Epoch 490, training loss: 0.03301847726106644 = 0.026131171733140945 + 0.001 * 6.887304306030273
Epoch 490, val loss: 0.9554981589317322
Epoch 500, training loss: 0.03101157955825329 = 0.024141483008861542 + 0.001 * 6.870096683502197
Epoch 500, val loss: 0.9692878723144531
Epoch 510, training loss: 0.0292266346514225 = 0.022361012175679207 + 0.001 * 6.865621089935303
Epoch 510, val loss: 0.9827541708946228
Epoch 520, training loss: 0.027623120695352554 = 0.020762862637639046 + 0.001 * 6.860257148742676
Epoch 520, val loss: 0.9959893226623535
Epoch 530, training loss: 0.026185758411884308 = 0.019324587658047676 + 0.001 * 6.861171245574951
Epoch 530, val loss: 1.008888840675354
Epoch 540, training loss: 0.024885423481464386 = 0.018027206882834435 + 0.001 * 6.858215808868408
Epoch 540, val loss: 1.0215274095535278
Epoch 550, training loss: 0.023705746978521347 = 0.01685379073023796 + 0.001 * 6.851956367492676
Epoch 550, val loss: 1.0338490009307861
Epoch 560, training loss: 0.022640861570835114 = 0.015789775177836418 + 0.001 * 6.851086616516113
Epoch 560, val loss: 1.0458803176879883
Epoch 570, training loss: 0.021679211407899857 = 0.014822455123066902 + 0.001 * 6.85675573348999
Epoch 570, val loss: 1.0576034784317017
Epoch 580, training loss: 0.020791366696357727 = 0.013940881937742233 + 0.001 * 6.850485324859619
Epoch 580, val loss: 1.0690395832061768
Epoch 590, training loss: 0.01999777927994728 = 0.013135680928826332 + 0.001 * 6.862097263336182
Epoch 590, val loss: 1.0801730155944824
Epoch 600, training loss: 0.019242554903030396 = 0.012398643419146538 + 0.001 * 6.843910217285156
Epoch 600, val loss: 1.0910395383834839
Epoch 610, training loss: 0.01857035979628563 = 0.011722564697265625 + 0.001 * 6.847794055938721
Epoch 610, val loss: 1.101635217666626
Epoch 620, training loss: 0.017949283123016357 = 0.011101090349256992 + 0.001 * 6.848191738128662
Epoch 620, val loss: 1.1119495630264282
Epoch 630, training loss: 0.017369065433740616 = 0.010528650134801865 + 0.001 * 6.840414047241211
Epoch 630, val loss: 1.1220365762710571
Epoch 640, training loss: 0.016831887885928154 = 0.010000267997384071 + 0.001 * 6.831619739532471
Epoch 640, val loss: 1.1318504810333252
Epoch 650, training loss: 0.016335077583789825 = 0.0095114316791296 + 0.001 * 6.823644638061523
Epoch 650, val loss: 1.1414374113082886
Epoch 660, training loss: 0.01589474268257618 = 0.00905847642570734 + 0.001 * 6.836265563964844
Epoch 660, val loss: 1.1508150100708008
Epoch 670, training loss: 0.015461208298802376 = 0.008638150058686733 + 0.001 * 6.823058605194092
Epoch 670, val loss: 1.1599375009536743
Epoch 680, training loss: 0.01506181713193655 = 0.00824739970266819 + 0.001 * 6.814416885375977
Epoch 680, val loss: 1.168846607208252
Epoch 690, training loss: 0.01471039280295372 = 0.007883655838668346 + 0.001 * 6.826736927032471
Epoch 690, val loss: 1.1775493621826172
Epoch 700, training loss: 0.01435522735118866 = 0.007544579450041056 + 0.001 * 6.810647964477539
Epoch 700, val loss: 1.1860591173171997
Epoch 710, training loss: 0.014035471715033054 = 0.007228050380945206 + 0.001 * 6.8074212074279785
Epoch 710, val loss: 1.194379448890686
Epoch 720, training loss: 0.013734952546656132 = 0.006932047661393881 + 0.001 * 6.8029046058654785
Epoch 720, val loss: 1.2024662494659424
Epoch 730, training loss: 0.013468943536281586 = 0.006654888391494751 + 0.001 * 6.8140549659729
Epoch 730, val loss: 1.2104257345199585
Epoch 740, training loss: 0.013209371827542782 = 0.006395109463483095 + 0.001 * 6.8142619132995605
Epoch 740, val loss: 1.2181528806686401
Epoch 750, training loss: 0.0129427220672369 = 0.006151190958917141 + 0.001 * 6.791530132293701
Epoch 750, val loss: 1.2257169485092163
Epoch 760, training loss: 0.012724220752716064 = 0.005921501200646162 + 0.001 * 6.802718639373779
Epoch 760, val loss: 1.2330996990203857
Epoch 770, training loss: 0.012509902007877827 = 0.0057046376168727875 + 0.001 * 6.805263996124268
Epoch 770, val loss: 1.2403590679168701
Epoch 780, training loss: 0.012299580499529839 = 0.00549872824922204 + 0.001 * 6.800851345062256
Epoch 780, val loss: 1.2474641799926758
Epoch 790, training loss: 0.012123161926865578 = 0.005302011501044035 + 0.001 * 6.821150302886963
Epoch 790, val loss: 1.2544578313827515
Epoch 800, training loss: 0.011903087608516216 = 0.005113559775054455 + 0.001 * 6.789527416229248
Epoch 800, val loss: 1.2614531517028809
Epoch 810, training loss: 0.011724145151674747 = 0.004932674113661051 + 0.001 * 6.791470527648926
Epoch 810, val loss: 1.268402338027954
Epoch 820, training loss: 0.011565972119569778 = 0.004759137984365225 + 0.001 * 6.806833267211914
Epoch 820, val loss: 1.2753125429153442
Epoch 830, training loss: 0.011381516233086586 = 0.004593012388795614 + 0.001 * 6.788503170013428
Epoch 830, val loss: 1.2822266817092896
Epoch 840, training loss: 0.011236036196351051 = 0.004434067290276289 + 0.001 * 6.801968097686768
Epoch 840, val loss: 1.289060115814209
Epoch 850, training loss: 0.011060260236263275 = 0.004282323177903891 + 0.001 * 6.7779364585876465
Epoch 850, val loss: 1.2958651781082153
Epoch 860, training loss: 0.01090543158352375 = 0.004137612879276276 + 0.001 * 6.767818450927734
Epoch 860, val loss: 1.3025919198989868
Epoch 870, training loss: 0.010784979909658432 = 0.003999822307378054 + 0.001 * 6.785157680511475
Epoch 870, val loss: 1.3092412948608398
Epoch 880, training loss: 0.01063772663474083 = 0.0038686043117195368 + 0.001 * 6.7691216468811035
Epoch 880, val loss: 1.3157984018325806
Epoch 890, training loss: 0.010505089536309242 = 0.0037436415441334248 + 0.001 * 6.761447429656982
Epoch 890, val loss: 1.3222832679748535
Epoch 900, training loss: 0.010393021628260612 = 0.003624649252742529 + 0.001 * 6.768372058868408
Epoch 900, val loss: 1.328648567199707
Epoch 910, training loss: 0.010278338566422462 = 0.0035112560726702213 + 0.001 * 6.767082691192627
Epoch 910, val loss: 1.3349403142929077
Epoch 920, training loss: 0.010158215649425983 = 0.0034034259151667356 + 0.001 * 6.754789352416992
Epoch 920, val loss: 1.3411723375320435
Epoch 930, training loss: 0.010060230270028114 = 0.003300777170807123 + 0.001 * 6.7594523429870605
Epoch 930, val loss: 1.3472882509231567
Epoch 940, training loss: 0.009970057755708694 = 0.0032031075097620487 + 0.001 * 6.7669501304626465
Epoch 940, val loss: 1.3532986640930176
Epoch 950, training loss: 0.009862895123660564 = 0.0031100832857191563 + 0.001 * 6.752811431884766
Epoch 950, val loss: 1.359185814857483
Epoch 960, training loss: 0.009774385020136833 = 0.0030214921571314335 + 0.001 * 6.752892017364502
Epoch 960, val loss: 1.3649650812149048
Epoch 970, training loss: 0.009693090803921223 = 0.002937004901468754 + 0.001 * 6.756085395812988
Epoch 970, val loss: 1.3706536293029785
Epoch 980, training loss: 0.00963807012885809 = 0.002856446662917733 + 0.001 * 6.781623363494873
Epoch 980, val loss: 1.3762333393096924
Epoch 990, training loss: 0.00953944493085146 = 0.0027796169742941856 + 0.001 * 6.759827613830566
Epoch 990, val loss: 1.3817411661148071
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.4686
Flip ASR: 0.3733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9626339673995972 = 1.9542601108551025 + 0.001 * 8.373910903930664
Epoch 0, val loss: 1.9528727531433105
Epoch 10, training loss: 1.95197331905365 = 1.9435994625091553 + 0.001 * 8.373867988586426
Epoch 10, val loss: 1.9415172338485718
Epoch 20, training loss: 1.9392061233520508 = 1.9308323860168457 + 0.001 * 8.373703002929688
Epoch 20, val loss: 1.9276012182235718
Epoch 30, training loss: 1.921567440032959 = 1.913194179534912 + 0.001 * 8.373310089111328
Epoch 30, val loss: 1.9082618951797485
Epoch 40, training loss: 1.8957427740097046 = 1.8873704671859741 + 0.001 * 8.372357368469238
Epoch 40, val loss: 1.8803496360778809
Epoch 50, training loss: 1.858863115310669 = 1.8504934310913086 + 0.001 * 8.369638442993164
Epoch 50, val loss: 1.8420289754867554
Epoch 60, training loss: 1.8137435913085938 = 1.805383563041687 + 0.001 * 8.360002517700195
Epoch 60, val loss: 1.7987152338027954
Epoch 70, training loss: 1.7714314460754395 = 1.76311457157135 + 0.001 * 8.316872596740723
Epoch 70, val loss: 1.7624542713165283
Epoch 80, training loss: 1.7252012491226196 = 1.717166781425476 + 0.001 * 8.034518241882324
Epoch 80, val loss: 1.7224078178405762
Epoch 90, training loss: 1.6618309020996094 = 1.6540069580078125 + 0.001 * 7.82389497756958
Epoch 90, val loss: 1.667717695236206
Epoch 100, training loss: 1.577822208404541 = 1.5701509714126587 + 0.001 * 7.671198844909668
Epoch 100, val loss: 1.5974293947219849
Epoch 110, training loss: 1.4759267568588257 = 1.4684503078460693 + 0.001 * 7.4764251708984375
Epoch 110, val loss: 1.5146663188934326
Epoch 120, training loss: 1.3676366806030273 = 1.3604103326797485 + 0.001 * 7.226314544677734
Epoch 120, val loss: 1.42861807346344
Epoch 130, training loss: 1.261598825454712 = 1.2543810606002808 + 0.001 * 7.217772960662842
Epoch 130, val loss: 1.3470286130905151
Epoch 140, training loss: 1.1590009927749634 = 1.1518254280090332 + 0.001 * 7.175530433654785
Epoch 140, val loss: 1.2698442935943604
Epoch 150, training loss: 1.060218334197998 = 1.0530788898468018 + 0.001 * 7.139431953430176
Epoch 150, val loss: 1.1957032680511475
Epoch 160, training loss: 0.9662407040596008 = 0.9591541886329651 + 0.001 * 7.086524963378906
Epoch 160, val loss: 1.1255024671554565
Epoch 170, training loss: 0.8774774074554443 = 0.8704410791397095 + 0.001 * 7.036317825317383
Epoch 170, val loss: 1.0593771934509277
Epoch 180, training loss: 0.793152391910553 = 0.7861456871032715 + 0.001 * 7.0067315101623535
Epoch 180, val loss: 0.9968998432159424
Epoch 190, training loss: 0.7125664949417114 = 0.7055759429931641 + 0.001 * 6.990571022033691
Epoch 190, val loss: 0.9372636675834656
Epoch 200, training loss: 0.6360830068588257 = 0.6291016936302185 + 0.001 * 6.981286525726318
Epoch 200, val loss: 0.8814029693603516
Epoch 210, training loss: 0.5647417902946472 = 0.5577656626701355 + 0.001 * 6.976100921630859
Epoch 210, val loss: 0.8306421041488647
Epoch 220, training loss: 0.4994764029979706 = 0.49250367283821106 + 0.001 * 6.972718238830566
Epoch 220, val loss: 0.7866129279136658
Epoch 230, training loss: 0.44083020091056824 = 0.43385976552963257 + 0.001 * 6.970430850982666
Epoch 230, val loss: 0.7501168251037598
Epoch 240, training loss: 0.3889687657356262 = 0.38199910521507263 + 0.001 * 6.969651222229004
Epoch 240, val loss: 0.7213597297668457
Epoch 250, training loss: 0.34370481967926025 = 0.33673712611198425 + 0.001 * 6.96768856048584
Epoch 250, val loss: 0.6997720003128052
Epoch 260, training loss: 0.3044542670249939 = 0.2974873483181 + 0.001 * 6.966931343078613
Epoch 260, val loss: 0.6841763854026794
Epoch 270, training loss: 0.2703583538532257 = 0.26339206099510193 + 0.001 * 6.966289043426514
Epoch 270, val loss: 0.6732686161994934
Epoch 280, training loss: 0.24056364595890045 = 0.2335960417985916 + 0.001 * 6.96760892868042
Epoch 280, val loss: 0.6663948893547058
Epoch 290, training loss: 0.21432600915431976 = 0.20735956728458405 + 0.001 * 6.9664411544799805
Epoch 290, val loss: 0.6625675559043884
Epoch 300, training loss: 0.19119329750537872 = 0.18422703444957733 + 0.001 * 6.966267108917236
Epoch 300, val loss: 0.6612997651100159
Epoch 310, training loss: 0.17080096900463104 = 0.16383366286754608 + 0.001 * 6.967311859130859
Epoch 310, val loss: 0.662307858467102
Epoch 320, training loss: 0.152885302901268 = 0.1459173858165741 + 0.001 * 6.967923641204834
Epoch 320, val loss: 0.6654131412506104
Epoch 330, training loss: 0.13720743358135223 = 0.1302388459444046 + 0.001 * 6.9685893058776855
Epoch 330, val loss: 0.6702935099601746
Epoch 340, training loss: 0.1234145388007164 = 0.1164453774690628 + 0.001 * 6.969162464141846
Epoch 340, val loss: 0.6768185496330261
Epoch 350, training loss: 0.11124470084905624 = 0.10427597910165787 + 0.001 * 6.968721866607666
Epoch 350, val loss: 0.68475341796875
Epoch 360, training loss: 0.10053829103708267 = 0.09356934577226639 + 0.001 * 6.9689483642578125
Epoch 360, val loss: 0.6938735246658325
Epoch 370, training loss: 0.09101108461618423 = 0.08404490351676941 + 0.001 * 6.966183185577393
Epoch 370, val loss: 0.7038289904594421
Epoch 380, training loss: 0.08247945457696915 = 0.07551389187574387 + 0.001 * 6.9655609130859375
Epoch 380, val loss: 0.7143065333366394
Epoch 390, training loss: 0.074824258685112 = 0.06785961985588074 + 0.001 * 6.96463680267334
Epoch 390, val loss: 0.7252779006958008
Epoch 400, training loss: 0.0678839460015297 = 0.06092015653848648 + 0.001 * 6.963785648345947
Epoch 400, val loss: 0.7363212704658508
Epoch 410, training loss: 0.061561211943626404 = 0.05459729582071304 + 0.001 * 6.9639177322387695
Epoch 410, val loss: 0.7477246522903442
Epoch 420, training loss: 0.05582171678543091 = 0.04884351044893265 + 0.001 * 6.978206157684326
Epoch 420, val loss: 0.7591278553009033
Epoch 430, training loss: 0.050701312720775604 = 0.04373539611697197 + 0.001 * 6.965917587280273
Epoch 430, val loss: 0.7704644203186035
Epoch 440, training loss: 0.04624175280332565 = 0.039280831813812256 + 0.001 * 6.960920810699463
Epoch 440, val loss: 0.7817825078964233
Epoch 450, training loss: 0.04240002483129501 = 0.035440001636743546 + 0.001 * 6.960022449493408
Epoch 450, val loss: 0.793276846408844
Epoch 460, training loss: 0.03906916826963425 = 0.032113999128341675 + 0.001 * 6.955170631408691
Epoch 460, val loss: 0.8045482635498047
Epoch 470, training loss: 0.03620665892958641 = 0.02922867238521576 + 0.001 * 6.977987289428711
Epoch 470, val loss: 0.8161178827285767
Epoch 480, training loss: 0.03366521745920181 = 0.02670126035809517 + 0.001 * 6.963954925537109
Epoch 480, val loss: 0.8272140026092529
Epoch 490, training loss: 0.031435057520866394 = 0.024484826251864433 + 0.001 * 6.950231075286865
Epoch 490, val loss: 0.8382505774497986
Epoch 500, training loss: 0.029474247246980667 = 0.02253088727593422 + 0.001 * 6.943359375
Epoch 500, val loss: 0.8491389155387878
Epoch 510, training loss: 0.02774653211236 = 0.02080744504928589 + 0.001 * 6.939087867736816
Epoch 510, val loss: 0.8597456216812134
Epoch 520, training loss: 0.026327650994062424 = 0.019277697429060936 + 0.001 * 7.049952983856201
Epoch 520, val loss: 0.8702006936073303
Epoch 530, training loss: 0.024873511865735054 = 0.017913861200213432 + 0.001 * 6.95965051651001
Epoch 530, val loss: 0.8804060220718384
Epoch 540, training loss: 0.023632459342479706 = 0.01669299043715 + 0.001 * 6.939469337463379
Epoch 540, val loss: 0.8903940320014954
Epoch 550, training loss: 0.022530129179358482 = 0.015595846809446812 + 0.001 * 6.934282302856445
Epoch 550, val loss: 0.900119960308075
Epoch 560, training loss: 0.021533386781811714 = 0.014606279321014881 + 0.001 * 6.927106857299805
Epoch 560, val loss: 0.9096277952194214
Epoch 570, training loss: 0.020632635802030563 = 0.013710479252040386 + 0.001 * 6.922155857086182
Epoch 570, val loss: 0.9188646674156189
Epoch 580, training loss: 0.019819404929876328 = 0.012896404601633549 + 0.001 * 6.923000335693359
Epoch 580, val loss: 0.9278931617736816
Epoch 590, training loss: 0.01908351108431816 = 0.012154528871178627 + 0.001 * 6.928980827331543
Epoch 590, val loss: 0.9366861581802368
Epoch 600, training loss: 0.018392913043498993 = 0.011476682499051094 + 0.001 * 6.91623067855835
Epoch 600, val loss: 0.9452929496765137
Epoch 610, training loss: 0.01776902750134468 = 0.01085540372878313 + 0.001 * 6.913623332977295
Epoch 610, val loss: 0.9535943865776062
Epoch 620, training loss: 0.017201170325279236 = 0.01028500497341156 + 0.001 * 6.916163921356201
Epoch 620, val loss: 0.9617959260940552
Epoch 630, training loss: 0.016668852418661118 = 0.009760632179677486 + 0.001 * 6.908220291137695
Epoch 630, val loss: 0.9697214961051941
Epoch 640, training loss: 0.016185669228434563 = 0.009276988916099072 + 0.001 * 6.908680438995361
Epoch 640, val loss: 0.9773967862129211
Epoch 650, training loss: 0.015731025487184525 = 0.00883014127612114 + 0.001 * 6.900883674621582
Epoch 650, val loss: 0.9849683046340942
Epoch 660, training loss: 0.015324319712817669 = 0.008416065014898777 + 0.001 * 6.908254146575928
Epoch 660, val loss: 0.9923455119132996
Epoch 670, training loss: 0.01493065245449543 = 0.008031606674194336 + 0.001 * 6.899045467376709
Epoch 670, val loss: 0.9995310306549072
Epoch 680, training loss: 0.014562595635652542 = 0.007674200460314751 + 0.001 * 6.888394832611084
Epoch 680, val loss: 1.0065138339996338
Epoch 690, training loss: 0.014225736260414124 = 0.007341381162405014 + 0.001 * 6.884354591369629
Epoch 690, val loss: 1.0133631229400635
Epoch 700, training loss: 0.01393723115324974 = 0.007030499633401632 + 0.001 * 6.906731605529785
Epoch 700, val loss: 1.0200618505477905
Epoch 710, training loss: 0.013621404767036438 = 0.006739700213074684 + 0.001 * 6.881703853607178
Epoch 710, val loss: 1.0266106128692627
Epoch 720, training loss: 0.013357910327613354 = 0.006467669736593962 + 0.001 * 6.89024019241333
Epoch 720, val loss: 1.0330255031585693
Epoch 730, training loss: 0.013089001178741455 = 0.006212370470166206 + 0.001 * 6.876630783081055
Epoch 730, val loss: 1.039258360862732
Epoch 740, training loss: 0.012861601077020168 = 0.0059721460565924644 + 0.001 * 6.8894548416137695
Epoch 740, val loss: 1.0454214811325073
Epoch 750, training loss: 0.012623876333236694 = 0.005746279377490282 + 0.001 * 6.877596378326416
Epoch 750, val loss: 1.0514241456985474
Epoch 760, training loss: 0.012416377663612366 = 0.005534290801733732 + 0.001 * 6.882086277008057
Epoch 760, val loss: 1.057242512702942
Epoch 770, training loss: 0.01220344752073288 = 0.0053351568058133125 + 0.001 * 6.868290424346924
Epoch 770, val loss: 1.0629746913909912
Epoch 780, training loss: 0.012022297829389572 = 0.0051484680734574795 + 0.001 * 6.873828887939453
Epoch 780, val loss: 1.0685970783233643
Epoch 790, training loss: 0.01184254139661789 = 0.004972763825207949 + 0.001 * 6.869776725769043
Epoch 790, val loss: 1.0740852355957031
Epoch 800, training loss: 0.011674275621771812 = 0.004806797485798597 + 0.001 * 6.867478370666504
Epoch 800, val loss: 1.0794841051101685
Epoch 810, training loss: 0.011498760432004929 = 0.0046498882584273815 + 0.001 * 6.848872184753418
Epoch 810, val loss: 1.084775447845459
Epoch 820, training loss: 0.011384437792003155 = 0.004501357674598694 + 0.001 * 6.883080005645752
Epoch 820, val loss: 1.0899016857147217
Epoch 830, training loss: 0.011216118931770325 = 0.004360624123364687 + 0.001 * 6.855494499206543
Epoch 830, val loss: 1.0949690341949463
Epoch 840, training loss: 0.01107737421989441 = 0.004227244295179844 + 0.001 * 6.850129127502441
Epoch 840, val loss: 1.0999411344528198
Epoch 850, training loss: 0.010960343293845654 = 0.004100332967936993 + 0.001 * 6.860010147094727
Epoch 850, val loss: 1.1048674583435059
Epoch 860, training loss: 0.0108560249209404 = 0.0039794365875422955 + 0.001 * 6.876588344573975
Epoch 860, val loss: 1.1095823049545288
Epoch 870, training loss: 0.010705573484301567 = 0.0038643740117549896 + 0.001 * 6.841198921203613
Epoch 870, val loss: 1.1142284870147705
Epoch 880, training loss: 0.010621590539813042 = 0.00375489704310894 + 0.001 * 6.866693496704102
Epoch 880, val loss: 1.1188477277755737
Epoch 890, training loss: 0.01048931572586298 = 0.0036506340838968754 + 0.001 * 6.838681221008301
Epoch 890, val loss: 1.1233021020889282
Epoch 900, training loss: 0.010374102741479874 = 0.0035512996837496758 + 0.001 * 6.822802543640137
Epoch 900, val loss: 1.1277185678482056
Epoch 910, training loss: 0.010312046855688095 = 0.003456571837887168 + 0.001 * 6.855474472045898
Epoch 910, val loss: 1.1320855617523193
Epoch 920, training loss: 0.010195374488830566 = 0.0033662226051092148 + 0.001 * 6.8291521072387695
Epoch 920, val loss: 1.1363186836242676
Epoch 930, training loss: 0.010108739137649536 = 0.003279955592006445 + 0.001 * 6.8287835121154785
Epoch 930, val loss: 1.1405104398727417
Epoch 940, training loss: 0.010032908990979195 = 0.003197507467120886 + 0.001 * 6.835400581359863
Epoch 940, val loss: 1.1446129083633423
Epoch 950, training loss: 0.009966325014829636 = 0.003118651220574975 + 0.001 * 6.847673416137695
Epoch 950, val loss: 1.148607611656189
Epoch 960, training loss: 0.009870007634162903 = 0.003043196862563491 + 0.001 * 6.826810359954834
Epoch 960, val loss: 1.1525583267211914
Epoch 970, training loss: 0.009840792044997215 = 0.002970857545733452 + 0.001 * 6.869933605194092
Epoch 970, val loss: 1.1564381122589111
Epoch 980, training loss: 0.009699741378426552 = 0.002901565982028842 + 0.001 * 6.79817533493042
Epoch 980, val loss: 1.1602137088775635
Epoch 990, training loss: 0.009639124386012554 = 0.002835097024217248 + 0.001 * 6.8040266036987305
Epoch 990, val loss: 1.163945198059082
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.5941
Flip ASR: 0.5378/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.959987998008728 = 1.9516141414642334 + 0.001 * 8.373910903930664
Epoch 0, val loss: 1.952641487121582
Epoch 10, training loss: 1.9497827291488647 = 1.9414089918136597 + 0.001 * 8.373783111572266
Epoch 10, val loss: 1.9420372247695923
Epoch 20, training loss: 1.9373981952667236 = 1.9290248155593872 + 0.001 * 8.373395919799805
Epoch 20, val loss: 1.928288459777832
Epoch 30, training loss: 1.9203068017959595 = 1.9119341373443604 + 0.001 * 8.372652053833008
Epoch 30, val loss: 1.9086447954177856
Epoch 40, training loss: 1.895633578300476 = 1.887262225151062 + 0.001 * 8.371302604675293
Epoch 40, val loss: 1.88045072555542
Epoch 50, training loss: 1.8608850240707397 = 1.8525166511535645 + 0.001 * 8.368321418762207
Epoch 50, val loss: 1.842705249786377
Epoch 60, training loss: 1.8178142309188843 = 1.8094556331634521 + 0.001 * 8.358561515808105
Epoch 60, val loss: 1.8002787828445435
Epoch 70, training loss: 1.775177240371704 = 1.7668627500534058 + 0.001 * 8.314534187316895
Epoch 70, val loss: 1.7633649110794067
Epoch 80, training loss: 1.7310715913772583 = 1.7230709791183472 + 0.001 * 8.000563621520996
Epoch 80, val loss: 1.726332426071167
Epoch 90, training loss: 1.6724209785461426 = 1.6646742820739746 + 0.001 * 7.746643543243408
Epoch 90, val loss: 1.6771149635314941
Epoch 100, training loss: 1.59453284740448 = 1.5869510173797607 + 0.001 * 7.581854343414307
Epoch 100, val loss: 1.614023208618164
Epoch 110, training loss: 1.497649908065796 = 1.4902163743972778 + 0.001 * 7.433562755584717
Epoch 110, val loss: 1.5374364852905273
Epoch 120, training loss: 1.3913835287094116 = 1.3840986490249634 + 0.001 * 7.2848711013793945
Epoch 120, val loss: 1.453953504562378
Epoch 130, training loss: 1.2832471132278442 = 1.276013970375061 + 0.001 * 7.233191967010498
Epoch 130, val loss: 1.370129108428955
Epoch 140, training loss: 1.176059603691101 = 1.168869972229004 + 0.001 * 7.18960428237915
Epoch 140, val loss: 1.2882081270217896
Epoch 150, training loss: 1.0696911811828613 = 1.0625377893447876 + 0.001 * 7.153397560119629
Epoch 150, val loss: 1.2059733867645264
Epoch 160, training loss: 0.9636651277542114 = 0.9565311074256897 + 0.001 * 7.134003639221191
Epoch 160, val loss: 1.1231342554092407
Epoch 170, training loss: 0.8599488139152527 = 0.8528301119804382 + 0.001 * 7.118716716766357
Epoch 170, val loss: 1.0424262285232544
Epoch 180, training loss: 0.7619773149490356 = 0.7548748254776001 + 0.001 * 7.10246467590332
Epoch 180, val loss: 0.9670011401176453
Epoch 190, training loss: 0.6731551885604858 = 0.6660733819007874 + 0.001 * 7.081802845001221
Epoch 190, val loss: 0.9005900621414185
Epoch 200, training loss: 0.59501713514328 = 0.5879628658294678 + 0.001 * 7.0542402267456055
Epoch 200, val loss: 0.8449540138244629
Epoch 210, training loss: 0.5272470712661743 = 0.5202133655548096 + 0.001 * 7.03372859954834
Epoch 210, val loss: 0.7999117374420166
Epoch 220, training loss: 0.46840086579322815 = 0.4613822102546692 + 0.001 * 7.018655776977539
Epoch 220, val loss: 0.7640804052352905
Epoch 230, training loss: 0.41666337847709656 = 0.4096514880657196 + 0.001 * 7.011885166168213
Epoch 230, val loss: 0.735710084438324
Epoch 240, training loss: 0.37047669291496277 = 0.36347025632858276 + 0.001 * 7.00644588470459
Epoch 240, val loss: 0.7141275405883789
Epoch 250, training loss: 0.3285168409347534 = 0.32151180505752563 + 0.001 * 7.005036354064941
Epoch 250, val loss: 0.6983354091644287
Epoch 260, training loss: 0.2897598445415497 = 0.2827583849430084 + 0.001 * 7.00146484375
Epoch 260, val loss: 0.6867668032646179
Epoch 270, training loss: 0.25390371680259705 = 0.24690476059913635 + 0.001 * 6.998958587646484
Epoch 270, val loss: 0.6788914203643799
Epoch 280, training loss: 0.22100049257278442 = 0.21400117874145508 + 0.001 * 6.9993109703063965
Epoch 280, val loss: 0.6743309497833252
Epoch 290, training loss: 0.19138993322849274 = 0.18439005315303802 + 0.001 * 6.999876499176025
Epoch 290, val loss: 0.6730145215988159
Epoch 300, training loss: 0.16537337005138397 = 0.15838050842285156 + 0.001 * 6.99286413192749
Epoch 300, val loss: 0.6753309369087219
Epoch 310, training loss: 0.14300860464572906 = 0.13602031767368317 + 0.001 * 6.988290786743164
Epoch 310, val loss: 0.6810809373855591
Epoch 320, training loss: 0.12407875061035156 = 0.11708291620016098 + 0.001 * 6.995835781097412
Epoch 320, val loss: 0.6897612810134888
Epoch 330, training loss: 0.10816007107496262 = 0.10117654502391815 + 0.001 * 6.983529090881348
Epoch 330, val loss: 0.7007269859313965
Epoch 340, training loss: 0.09484921395778656 = 0.08786892890930176 + 0.001 * 6.980287551879883
Epoch 340, val loss: 0.713474452495575
Epoch 350, training loss: 0.08372313529253006 = 0.07674559205770493 + 0.001 * 6.97754430770874
Epoch 350, val loss: 0.7275981307029724
Epoch 360, training loss: 0.07441884279251099 = 0.06744278222322464 + 0.001 * 6.9760589599609375
Epoch 360, val loss: 0.7426737546920776
Epoch 370, training loss: 0.06659401953220367 = 0.059623368084430695 + 0.001 * 6.970651626586914
Epoch 370, val loss: 0.7583905458450317
Epoch 380, training loss: 0.05998154357075691 = 0.05300211161375046 + 0.001 * 6.979432106018066
Epoch 380, val loss: 0.7744561433792114
Epoch 390, training loss: 0.05432562530040741 = 0.04735882207751274 + 0.001 * 6.966804027557373
Epoch 390, val loss: 0.7906394600868225
Epoch 400, training loss: 0.0494915135204792 = 0.04252949729561806 + 0.001 * 6.962015628814697
Epoch 400, val loss: 0.8066682815551758
Epoch 410, training loss: 0.04539799317717552 = 0.03837602585554123 + 0.001 * 7.02196741104126
Epoch 410, val loss: 0.8223675489425659
Epoch 420, training loss: 0.04174108803272247 = 0.03477925434708595 + 0.001 * 6.961834907531738
Epoch 420, val loss: 0.8377909064292908
Epoch 430, training loss: 0.03860212117433548 = 0.031647227704524994 + 0.001 * 6.95489501953125
Epoch 430, val loss: 0.8529384136199951
Epoch 440, training loss: 0.035861216485500336 = 0.028905028477311134 + 0.001 * 6.956186771392822
Epoch 440, val loss: 0.8676471710205078
Epoch 450, training loss: 0.03344176709651947 = 0.02649114653468132 + 0.001 * 6.950620651245117
Epoch 450, val loss: 0.8820632100105286
Epoch 460, training loss: 0.031325146555900574 = 0.024358954280614853 + 0.001 * 6.966190814971924
Epoch 460, val loss: 0.8961522579193115
Epoch 470, training loss: 0.02942010387778282 = 0.02246806025505066 + 0.001 * 6.952043533325195
Epoch 470, val loss: 0.9098819494247437
Epoch 480, training loss: 0.027737097814679146 = 0.020785203203558922 + 0.001 * 6.951894283294678
Epoch 480, val loss: 0.9232059121131897
Epoch 490, training loss: 0.026225382462143898 = 0.019281618297100067 + 0.001 * 6.943763732910156
Epoch 490, val loss: 0.9362228512763977
Epoch 500, training loss: 0.02487821690738201 = 0.017933694645762444 + 0.001 * 6.944521903991699
Epoch 500, val loss: 0.9488945603370667
Epoch 510, training loss: 0.02368273213505745 = 0.016721216961741447 + 0.001 * 6.9615159034729
Epoch 510, val loss: 0.9612113833427429
Epoch 520, training loss: 0.022571032866835594 = 0.015624800696969032 + 0.001 * 6.946232318878174
Epoch 520, val loss: 0.9731898307800293
Epoch 530, training loss: 0.02156851254403591 = 0.01463091466575861 + 0.001 * 6.93759822845459
Epoch 530, val loss: 0.9849756360054016
Epoch 540, training loss: 0.02066144160926342 = 0.013728424906730652 + 0.001 * 6.933016300201416
Epoch 540, val loss: 0.9963618516921997
Epoch 550, training loss: 0.019849266856908798 = 0.012906690128147602 + 0.001 * 6.942575931549072
Epoch 550, val loss: 1.0075350999832153
Epoch 560, training loss: 0.0190877802670002 = 0.012156346812844276 + 0.001 * 6.931434154510498
Epoch 560, val loss: 1.0183806419372559
Epoch 570, training loss: 0.018394403159618378 = 0.011469768360257149 + 0.001 * 6.924633502960205
Epoch 570, val loss: 1.02895987033844
Epoch 580, training loss: 0.01776740700006485 = 0.010840250179171562 + 0.001 * 6.927155494689941
Epoch 580, val loss: 1.0392547845840454
Epoch 590, training loss: 0.017190948128700256 = 0.010261855088174343 + 0.001 * 6.9290924072265625
Epoch 590, val loss: 1.0492945909500122
Epoch 600, training loss: 0.01665179803967476 = 0.009729458019137383 + 0.001 * 6.922340393066406
Epoch 600, val loss: 1.0590705871582031
Epoch 610, training loss: 0.016166137531399727 = 0.009238410741090775 + 0.001 * 6.9277262687683105
Epoch 610, val loss: 1.0686309337615967
Epoch 620, training loss: 0.015698712319135666 = 0.008784381672739983 + 0.001 * 6.914330959320068
Epoch 620, val loss: 1.07791268825531
Epoch 630, training loss: 0.015284594148397446 = 0.00836405623704195 + 0.001 * 6.920537948608398
Epoch 630, val loss: 1.0870397090911865
Epoch 640, training loss: 0.01487920805811882 = 0.007974335923790932 + 0.001 * 6.904871940612793
Epoch 640, val loss: 1.0959008932113647
Epoch 650, training loss: 0.014516918919980526 = 0.0076123857870697975 + 0.001 * 6.9045329093933105
Epoch 650, val loss: 1.104565978050232
Epoch 660, training loss: 0.014178358018398285 = 0.007275253534317017 + 0.001 * 6.903104305267334
Epoch 660, val loss: 1.113011360168457
Epoch 670, training loss: 0.013857932761311531 = 0.006960890255868435 + 0.001 * 6.897042274475098
Epoch 670, val loss: 1.1212860345840454
Epoch 680, training loss: 0.013575084507465363 = 0.006667644716799259 + 0.001 * 6.907439231872559
Epoch 680, val loss: 1.1293672323226929
Epoch 690, training loss: 0.013282017782330513 = 0.006393782328814268 + 0.001 * 6.888234615325928
Epoch 690, val loss: 1.1372543573379517
Epoch 700, training loss: 0.013036375865340233 = 0.006137731950730085 + 0.001 * 6.898643493652344
Epoch 700, val loss: 1.1449733972549438
Epoch 710, training loss: 0.012798216193914413 = 0.005897963419556618 + 0.001 * 6.900252342224121
Epoch 710, val loss: 1.1525259017944336
Epoch 720, training loss: 0.012566832825541496 = 0.005673068575561047 + 0.001 * 6.893764019012451
Epoch 720, val loss: 1.1598644256591797
Epoch 730, training loss: 0.01236036978662014 = 0.005461846012622118 + 0.001 * 6.898522853851318
Epoch 730, val loss: 1.1671099662780762
Epoch 740, training loss: 0.01215086318552494 = 0.0052632479928433895 + 0.001 * 6.887615203857422
Epoch 740, val loss: 1.174148440361023
Epoch 750, training loss: 0.011956782080233097 = 0.005076301284134388 + 0.001 * 6.8804802894592285
Epoch 750, val loss: 1.1810678243637085
Epoch 760, training loss: 0.011776765808463097 = 0.004900056403130293 + 0.001 * 6.876709461212158
Epoch 760, val loss: 1.1878093481063843
Epoch 770, training loss: 0.01160906907171011 = 0.004733754321932793 + 0.001 * 6.875314235687256
Epoch 770, val loss: 1.1944377422332764
Epoch 780, training loss: 0.01144535280764103 = 0.004576649516820908 + 0.001 * 6.868703365325928
Epoch 780, val loss: 1.2009332180023193
Epoch 790, training loss: 0.01132144220173359 = 0.00442808261141181 + 0.001 * 6.893359184265137
Epoch 790, val loss: 1.2072807550430298
Epoch 800, training loss: 0.01115148514509201 = 0.004287485498934984 + 0.001 * 6.863999843597412
Epoch 800, val loss: 1.2134674787521362
Epoch 810, training loss: 0.011030144989490509 = 0.004154205322265625 + 0.001 * 6.875938892364502
Epoch 810, val loss: 1.219614863395691
Epoch 820, training loss: 0.010879682376980782 = 0.004027831833809614 + 0.001 * 6.851849555969238
Epoch 820, val loss: 1.2255827188491821
Epoch 830, training loss: 0.010771555826067924 = 0.0039078835397958755 + 0.001 * 6.863672256469727
Epoch 830, val loss: 1.2314846515655518
Epoch 840, training loss: 0.01064363494515419 = 0.003793910378590226 + 0.001 * 6.849723815917969
Epoch 840, val loss: 1.2372148036956787
Epoch 850, training loss: 0.010569615289568901 = 0.00368553027510643 + 0.001 * 6.884084224700928
Epoch 850, val loss: 1.2429159879684448
Epoch 860, training loss: 0.010406054556369781 = 0.003582411678507924 + 0.001 * 6.823642253875732
Epoch 860, val loss: 1.2484631538391113
Epoch 870, training loss: 0.01030891016125679 = 0.003484193002805114 + 0.001 * 6.824717044830322
Epoch 870, val loss: 1.25393545627594
Epoch 880, training loss: 0.010251441039144993 = 0.003390550846233964 + 0.001 * 6.860889911651611
Epoch 880, val loss: 1.2593106031417847
Epoch 890, training loss: 0.010121006518602371 = 0.0033012903295457363 + 0.001 * 6.819716453552246
Epoch 890, val loss: 1.2645318508148193
Epoch 900, training loss: 0.010058583691716194 = 0.0032160384580492973 + 0.001 * 6.8425445556640625
Epoch 900, val loss: 1.2697386741638184
Epoch 910, training loss: 0.00994678121060133 = 0.003134611062705517 + 0.001 * 6.812170028686523
Epoch 910, val loss: 1.2747642993927002
Epoch 920, training loss: 0.009895124472677708 = 0.003056782530620694 + 0.001 * 6.838341236114502
Epoch 920, val loss: 1.2798372507095337
Epoch 930, training loss: 0.009794958867132664 = 0.0029823516961187124 + 0.001 * 6.812607288360596
Epoch 930, val loss: 1.2846832275390625
Epoch 940, training loss: 0.009723767638206482 = 0.0029111169278621674 + 0.001 * 6.812649726867676
Epoch 940, val loss: 1.2895594835281372
Epoch 950, training loss: 0.009648870676755905 = 0.002842916641384363 + 0.001 * 6.805953502655029
Epoch 950, val loss: 1.294271469116211
Epoch 960, training loss: 0.009575935080647469 = 0.0027775655034929514 + 0.001 * 6.798369407653809
Epoch 960, val loss: 1.2989429235458374
Epoch 970, training loss: 0.009528658352792263 = 0.0027149172965437174 + 0.001 * 6.8137407302856445
Epoch 970, val loss: 1.3035438060760498
Epoch 980, training loss: 0.009472288191318512 = 0.00265480880625546 + 0.001 * 6.817478656768799
Epoch 980, val loss: 1.30804443359375
Epoch 990, training loss: 0.009395265020430088 = 0.0025970893912017345 + 0.001 * 6.79817533493042
Epoch 990, val loss: 1.3124796152114868
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.9225
Flip ASR: 0.9067/225 nodes
The final ASR:0.66175, 0.19137, Accuracy:0.81481, 0.00302
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10556])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.944315791130066 = 1.9359419345855713 + 0.001 * 8.373823165893555
Epoch 0, val loss: 1.935685634613037
Epoch 10, training loss: 1.9348621368408203 = 1.9264883995056152 + 0.001 * 8.373741149902344
Epoch 10, val loss: 1.9269537925720215
Epoch 20, training loss: 1.9232120513916016 = 1.9148385524749756 + 0.001 * 8.373510360717773
Epoch 20, val loss: 1.9159421920776367
Epoch 30, training loss: 1.9066948890686035 = 1.8983218669891357 + 0.001 * 8.373035430908203
Epoch 30, val loss: 1.900070309638977
Epoch 40, training loss: 1.8820911645889282 = 1.8737192153930664 + 0.001 * 8.371954917907715
Epoch 40, val loss: 1.8766095638275146
Epoch 50, training loss: 1.8472769260406494 = 1.838908076286316 + 0.001 * 8.368831634521484
Epoch 50, val loss: 1.8450881242752075
Epoch 60, training loss: 1.8067699670791626 = 1.7984135150909424 + 0.001 * 8.356453895568848
Epoch 60, val loss: 1.8120174407958984
Epoch 70, training loss: 1.7687501907348633 = 1.7604564428329468 + 0.001 * 8.293712615966797
Epoch 70, val loss: 1.7820466756820679
Epoch 80, training loss: 1.7196372747421265 = 1.7116810083389282 + 0.001 * 7.9562225341796875
Epoch 80, val loss: 1.737786054611206
Epoch 90, training loss: 1.650474190711975 = 1.6427184343338013 + 0.001 * 7.7558112144470215
Epoch 90, val loss: 1.6774805784225464
Epoch 100, training loss: 1.5634528398513794 = 1.5557336807250977 + 0.001 * 7.719200611114502
Epoch 100, val loss: 1.6067733764648438
Epoch 110, training loss: 1.4700922966003418 = 1.4623955488204956 + 0.001 * 7.6967668533325195
Epoch 110, val loss: 1.5298575162887573
Epoch 120, training loss: 1.3797199726104736 = 1.3720645904541016 + 0.001 * 7.655439376831055
Epoch 120, val loss: 1.4561349153518677
Epoch 130, training loss: 1.2930786609649658 = 1.2855591773986816 + 0.001 * 7.519496917724609
Epoch 130, val loss: 1.3860360383987427
Epoch 140, training loss: 1.2077054977416992 = 1.2004820108413696 + 0.001 * 7.223535060882568
Epoch 140, val loss: 1.3189387321472168
Epoch 150, training loss: 1.1220725774765015 = 1.1149675846099854 + 0.001 * 7.104984760284424
Epoch 150, val loss: 1.2529689073562622
Epoch 160, training loss: 1.03364098072052 = 1.0265921354293823 + 0.001 * 7.048786163330078
Epoch 160, val loss: 1.185697317123413
Epoch 170, training loss: 0.9409233331680298 = 0.9338791370391846 + 0.001 * 7.044183731079102
Epoch 170, val loss: 1.1153315305709839
Epoch 180, training loss: 0.8458163142204285 = 0.8387733101844788 + 0.001 * 7.042974472045898
Epoch 180, val loss: 1.0438133478164673
Epoch 190, training loss: 0.7535814046859741 = 0.7465395927429199 + 0.001 * 7.041828632354736
Epoch 190, val loss: 0.9756931066513062
Epoch 200, training loss: 0.6701560616493225 = 0.6631141901016235 + 0.001 * 7.041848659515381
Epoch 200, val loss: 0.9162460565567017
Epoch 210, training loss: 0.5993655920028687 = 0.5923235416412354 + 0.001 * 7.042022228240967
Epoch 210, val loss: 0.8690013885498047
Epoch 220, training loss: 0.5421497225761414 = 0.5351080298423767 + 0.001 * 7.04170036315918
Epoch 220, val loss: 0.8348096013069153
Epoch 230, training loss: 0.496987909078598 = 0.48994725942611694 + 0.001 * 7.040663242340088
Epoch 230, val loss: 0.8122225403785706
Epoch 240, training loss: 0.4610719084739685 = 0.4540332853794098 + 0.001 * 7.038620471954346
Epoch 240, val loss: 0.7984585165977478
Epoch 250, training loss: 0.4313838481903076 = 0.424348384141922 + 0.001 * 7.0354719161987305
Epoch 250, val loss: 0.7904081344604492
Epoch 260, training loss: 0.40523067116737366 = 0.39819958806037903 + 0.001 * 7.0310869216918945
Epoch 260, val loss: 0.7856277823448181
Epoch 270, training loss: 0.3802524507045746 = 0.3732270300388336 + 0.001 * 7.025434494018555
Epoch 270, val loss: 0.782269299030304
Epoch 280, training loss: 0.3544354736804962 = 0.3474171459674835 + 0.001 * 7.018325328826904
Epoch 280, val loss: 0.7791455388069153
Epoch 290, training loss: 0.32629063725471497 = 0.3192817270755768 + 0.001 * 7.008907794952393
Epoch 290, val loss: 0.7755692601203918
Epoch 300, training loss: 0.2953718304634094 = 0.2883775234222412 + 0.001 * 6.994302272796631
Epoch 300, val loss: 0.7716289162635803
Epoch 310, training loss: 0.2626076340675354 = 0.25563982129096985 + 0.001 * 6.96781063079834
Epoch 310, val loss: 0.7686296701431274
Epoch 320, training loss: 0.23000648617744446 = 0.22307312488555908 + 0.001 * 6.933366298675537
Epoch 320, val loss: 0.7677550315856934
Epoch 330, training loss: 0.19970683753490448 = 0.192799910902977 + 0.001 * 6.906927585601807
Epoch 330, val loss: 0.7701235413551331
Epoch 340, training loss: 0.17306333780288696 = 0.1661730855703354 + 0.001 * 6.890254020690918
Epoch 340, val loss: 0.7756979465484619
Epoch 350, training loss: 0.15040621161460876 = 0.14352743327617645 + 0.001 * 6.878772735595703
Epoch 350, val loss: 0.7843239307403564
Epoch 360, training loss: 0.13137246668338776 = 0.12449898570775986 + 0.001 * 6.873482704162598
Epoch 360, val loss: 0.7951629757881165
Epoch 370, training loss: 0.11535529047250748 = 0.10848522186279297 + 0.001 * 6.870065689086914
Epoch 370, val loss: 0.8075966238975525
Epoch 380, training loss: 0.1017790287733078 = 0.09491242468357086 + 0.001 * 6.866601467132568
Epoch 380, val loss: 0.8211312294006348
Epoch 390, training loss: 0.09018030017614365 = 0.08331946283578873 + 0.001 * 6.860838413238525
Epoch 390, val loss: 0.8354179859161377
Epoch 400, training loss: 0.08022639155387878 = 0.07337155938148499 + 0.001 * 6.854831695556641
Epoch 400, val loss: 0.8502131104469299
Epoch 410, training loss: 0.07166814804077148 = 0.06481894105672836 + 0.001 * 6.849208831787109
Epoch 410, val loss: 0.86529141664505
Epoch 420, training loss: 0.06430351734161377 = 0.05746309831738472 + 0.001 * 6.840422630310059
Epoch 420, val loss: 0.8806242346763611
Epoch 430, training loss: 0.05795826017856598 = 0.05112801492214203 + 0.001 * 6.830243110656738
Epoch 430, val loss: 0.8960066437721252
Epoch 440, training loss: 0.052471503615379333 = 0.04565479978919029 + 0.001 * 6.816701412200928
Epoch 440, val loss: 0.9113775491714478
Epoch 450, training loss: 0.04772753641009331 = 0.04091433808207512 + 0.001 * 6.813196659088135
Epoch 450, val loss: 0.9267584085464478
Epoch 460, training loss: 0.04361267387866974 = 0.03680213540792465 + 0.001 * 6.810537815093994
Epoch 460, val loss: 0.9420034885406494
Epoch 470, training loss: 0.04002803564071655 = 0.03322813659906387 + 0.001 * 6.799896717071533
Epoch 470, val loss: 0.9570100903511047
Epoch 480, training loss: 0.036903440952301025 = 0.030114302411675453 + 0.001 * 6.789137363433838
Epoch 480, val loss: 0.9717477560043335
Epoch 490, training loss: 0.03417854756116867 = 0.027394449338316917 + 0.001 * 6.7840986251831055
Epoch 490, val loss: 0.9861804842948914
Epoch 500, training loss: 0.03179314360022545 = 0.02501167356967926 + 0.001 * 6.781471252441406
Epoch 500, val loss: 1.0002837181091309
Epoch 510, training loss: 0.029699580743908882 = 0.022916385903954506 + 0.001 * 6.783194065093994
Epoch 510, val loss: 1.0140353441238403
Epoch 520, training loss: 0.027845414355397224 = 0.021067319437861443 + 0.001 * 6.778094291687012
Epoch 520, val loss: 1.02742600440979
Epoch 530, training loss: 0.026208871975541115 = 0.0194302499294281 + 0.001 * 6.778621673583984
Epoch 530, val loss: 1.0405634641647339
Epoch 540, training loss: 0.024752814322710037 = 0.017975782975554466 + 0.001 * 6.777031421661377
Epoch 540, val loss: 1.0533310174942017
Epoch 550, training loss: 0.023457258939743042 = 0.016679253429174423 + 0.001 * 6.778005599975586
Epoch 550, val loss: 1.0656880140304565
Epoch 560, training loss: 0.022293729707598686 = 0.015519331209361553 + 0.001 * 6.774397850036621
Epoch 560, val loss: 1.0777714252471924
Epoch 570, training loss: 0.02124953456223011 = 0.0144780483096838 + 0.001 * 6.771485805511475
Epoch 570, val loss: 1.0894191265106201
Epoch 580, training loss: 0.02032535895705223 = 0.013540443032979965 + 0.001 * 6.784916400909424
Epoch 580, val loss: 1.1008524894714355
Epoch 590, training loss: 0.019467206671833992 = 0.012693287804722786 + 0.001 * 6.773918151855469
Epoch 590, val loss: 1.1118959188461304
Epoch 600, training loss: 0.018693499267101288 = 0.011925477534532547 + 0.001 * 6.768022060394287
Epoch 600, val loss: 1.1226556301116943
Epoch 610, training loss: 0.01801716536283493 = 0.011227605864405632 + 0.001 * 6.7895588874816895
Epoch 610, val loss: 1.1330941915512085
Epoch 620, training loss: 0.017360083758831024 = 0.010591481812298298 + 0.001 * 6.76860237121582
Epoch 620, val loss: 1.1431964635849
Epoch 630, training loss: 0.016775131225585938 = 0.010010140016674995 + 0.001 * 6.76499080657959
Epoch 630, val loss: 1.1531026363372803
Epoch 640, training loss: 0.016238167881965637 = 0.009477538987994194 + 0.001 * 6.760629653930664
Epoch 640, val loss: 1.1627016067504883
Epoch 650, training loss: 0.01574881002306938 = 0.00898840930312872 + 0.001 * 6.760399341583252
Epoch 650, val loss: 1.1720470190048218
Epoch 660, training loss: 0.015304851345717907 = 0.008538227528333664 + 0.001 * 6.766623497009277
Epoch 660, val loss: 1.1811796426773071
Epoch 670, training loss: 0.01487890537828207 = 0.0081229442730546 + 0.001 * 6.755960941314697
Epoch 670, val loss: 1.1900655031204224
Epoch 680, training loss: 0.014492344111204147 = 0.007739088963717222 + 0.001 * 6.753255367279053
Epoch 680, val loss: 1.1987305879592896
Epoch 690, training loss: 0.014154447242617607 = 0.007383568212389946 + 0.001 * 6.770878314971924
Epoch 690, val loss: 1.2072186470031738
Epoch 700, training loss: 0.013805958442389965 = 0.0070536513812839985 + 0.001 * 6.752306938171387
Epoch 700, val loss: 1.2154018878936768
Epoch 710, training loss: 0.013495257124304771 = 0.006746895611286163 + 0.001 * 6.748360633850098
Epoch 710, val loss: 1.2234605550765991
Epoch 720, training loss: 0.013231613673269749 = 0.006461212411522865 + 0.001 * 6.7704010009765625
Epoch 720, val loss: 1.2312771081924438
Epoch 730, training loss: 0.012943428941071033 = 0.006194775458425283 + 0.001 * 6.748652935028076
Epoch 730, val loss: 1.2389596700668335
Epoch 740, training loss: 0.012690937146544456 = 0.0059458534233272076 + 0.001 * 6.745083808898926
Epoch 740, val loss: 1.246425986289978
Epoch 750, training loss: 0.012463994324207306 = 0.005712949205189943 + 0.001 * 6.751044750213623
Epoch 750, val loss: 1.2537055015563965
Epoch 760, training loss: 0.012240815907716751 = 0.005494727753102779 + 0.001 * 6.74608850479126
Epoch 760, val loss: 1.2608492374420166
Epoch 770, training loss: 0.012027433142066002 = 0.005290007218718529 + 0.001 * 6.737426280975342
Epoch 770, val loss: 1.2678641080856323
Epoch 780, training loss: 0.011867206543684006 = 0.005097642075270414 + 0.001 * 6.769563674926758
Epoch 780, val loss: 1.2746601104736328
Epoch 790, training loss: 0.011660350486636162 = 0.004916679114103317 + 0.001 * 6.74367094039917
Epoch 790, val loss: 1.2813029289245605
Epoch 800, training loss: 0.011486094444990158 = 0.004746209364384413 + 0.001 * 6.739884376525879
Epoch 800, val loss: 1.2878522872924805
Epoch 810, training loss: 0.011315405368804932 = 0.0045854211784899235 + 0.001 * 6.729983329772949
Epoch 810, val loss: 1.2942382097244263
Epoch 820, training loss: 0.011199340224266052 = 0.004433557391166687 + 0.001 * 6.765782356262207
Epoch 820, val loss: 1.300472378730774
Epoch 830, training loss: 0.01101725734770298 = 0.004289892036467791 + 0.001 * 6.727365016937256
Epoch 830, val loss: 1.30659019947052
Epoch 840, training loss: 0.010881185531616211 = 0.004153801128268242 + 0.001 * 6.727384567260742
Epoch 840, val loss: 1.3125860691070557
Epoch 850, training loss: 0.01074860617518425 = 0.004024621099233627 + 0.001 * 6.723984718322754
Epoch 850, val loss: 1.3184666633605957
Epoch 860, training loss: 0.010626058094203472 = 0.0039015410002321005 + 0.001 * 6.72451639175415
Epoch 860, val loss: 1.3242298364639282
Epoch 870, training loss: 0.010516447015106678 = 0.0037842949386686087 + 0.001 * 6.732151508331299
Epoch 870, val loss: 1.329903483390808
Epoch 880, training loss: 0.010397566482424736 = 0.00367243611253798 + 0.001 * 6.725130081176758
Epoch 880, val loss: 1.3355064392089844
Epoch 890, training loss: 0.010286534205079079 = 0.003565409919247031 + 0.001 * 6.721123695373535
Epoch 890, val loss: 1.3410166501998901
Epoch 900, training loss: 0.010177410207688808 = 0.003462904831394553 + 0.001 * 6.714504718780518
Epoch 900, val loss: 1.3464542627334595
Epoch 910, training loss: 0.010090839117765427 = 0.0033646472729742527 + 0.001 * 6.72619104385376
Epoch 910, val loss: 1.3518049716949463
Epoch 920, training loss: 0.009985492564737797 = 0.0032702949829399586 + 0.001 * 6.7151970863342285
Epoch 920, val loss: 1.3571059703826904
Epoch 930, training loss: 0.0098958071321249 = 0.00317975552752614 + 0.001 * 6.71605110168457
Epoch 930, val loss: 1.3623384237289429
Epoch 940, training loss: 0.009809708222746849 = 0.003092898055911064 + 0.001 * 6.716809272766113
Epoch 940, val loss: 1.3674827814102173
Epoch 950, training loss: 0.009718242101371288 = 0.0030094243120402098 + 0.001 * 6.708817005157471
Epoch 950, val loss: 1.3725868463516235
Epoch 960, training loss: 0.009653426706790924 = 0.0029291482642292976 + 0.001 * 6.724278450012207
Epoch 960, val loss: 1.3776066303253174
Epoch 970, training loss: 0.00957547128200531 = 0.002852127654477954 + 0.001 * 6.7233428955078125
Epoch 970, val loss: 1.382567048072815
Epoch 980, training loss: 0.009499871172010899 = 0.002778095891699195 + 0.001 * 6.721775054931641
Epoch 980, val loss: 1.3874523639678955
Epoch 990, training loss: 0.009424964897334576 = 0.0027069903444498777 + 0.001 * 6.7179741859436035
Epoch 990, val loss: 1.3922808170318604
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.5941
Flip ASR: 0.5156/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9471741914749146 = 1.93880033493042 + 0.001 * 8.373826026916504
Epoch 0, val loss: 1.937448263168335
Epoch 10, training loss: 1.9373019933700562 = 1.928928256034851 + 0.001 * 8.37372875213623
Epoch 10, val loss: 1.9268029928207397
Epoch 20, training loss: 1.9252053499221802 = 1.9168318510055542 + 0.001 * 8.373464584350586
Epoch 20, val loss: 1.9135512113571167
Epoch 30, training loss: 1.9084042310714722 = 1.900031328201294 + 0.001 * 8.372915267944336
Epoch 30, val loss: 1.8950841426849365
Epoch 40, training loss: 1.8837794065475464 = 1.8754076957702637 + 0.001 * 8.371672630310059
Epoch 40, val loss: 1.8682886362075806
Epoch 50, training loss: 1.8489433526992798 = 1.840575098991394 + 0.001 * 8.368308067321777
Epoch 50, val loss: 1.8316200971603394
Epoch 60, training loss: 1.8075205087661743 = 1.7991646528244019 + 0.001 * 8.355807304382324
Epoch 60, val loss: 1.7912533283233643
Epoch 70, training loss: 1.7677078247070312 = 1.7594178915023804 + 0.001 * 8.289935111999512
Epoch 70, val loss: 1.7564876079559326
Epoch 80, training loss: 1.7199574708938599 = 1.7120531797409058 + 0.001 * 7.904317378997803
Epoch 80, val loss: 1.7156034708023071
Epoch 90, training loss: 1.6535252332687378 = 1.6459131240844727 + 0.001 * 7.612064361572266
Epoch 90, val loss: 1.658211350440979
Epoch 100, training loss: 1.5667444467544556 = 1.5593681335449219 + 0.001 * 7.376262664794922
Epoch 100, val loss: 1.5841279029846191
Epoch 110, training loss: 1.464903473854065 = 1.457735538482666 + 0.001 * 7.167992115020752
Epoch 110, val loss: 1.5000888109207153
Epoch 120, training loss: 1.3573493957519531 = 1.3503050804138184 + 0.001 * 7.044322490692139
Epoch 120, val loss: 1.4137496948242188
Epoch 130, training loss: 1.247624397277832 = 1.2405942678451538 + 0.001 * 7.030082702636719
Epoch 130, val loss: 1.3282703161239624
Epoch 140, training loss: 1.135862946510315 = 1.1288540363311768 + 0.001 * 7.008872985839844
Epoch 140, val loss: 1.2426573038101196
Epoch 150, training loss: 1.0238406658172607 = 1.0168449878692627 + 0.001 * 6.995736598968506
Epoch 150, val loss: 1.1568119525909424
Epoch 160, training loss: 0.9156451225280762 = 0.9086634516716003 + 0.001 * 6.981647491455078
Epoch 160, val loss: 1.074055552482605
Epoch 170, training loss: 0.8157649636268616 = 0.8087978959083557 + 0.001 * 6.967074871063232
Epoch 170, val loss: 0.9985343813896179
Epoch 180, training loss: 0.7265206575393677 = 0.7195678353309631 + 0.001 * 6.952841281890869
Epoch 180, val loss: 0.9319747090339661
Epoch 190, training loss: 0.6477189660072327 = 0.6407793760299683 + 0.001 * 6.93960428237915
Epoch 190, val loss: 0.8746076822280884
Epoch 200, training loss: 0.5777285695075989 = 0.570799708366394 + 0.001 * 6.928857326507568
Epoch 200, val loss: 0.8253963589668274
Epoch 210, training loss: 0.514664888381958 = 0.5077447891235352 + 0.001 * 6.920102119445801
Epoch 210, val loss: 0.7839583158493042
Epoch 220, training loss: 0.45740067958831787 = 0.4504885673522949 + 0.001 * 6.9121198654174805
Epoch 220, val loss: 0.7489371299743652
Epoch 230, training loss: 0.406391441822052 = 0.3994869887828827 + 0.001 * 6.904467582702637
Epoch 230, val loss: 0.7204859852790833
Epoch 240, training loss: 0.36159294843673706 = 0.3546959161758423 + 0.001 * 6.8970208168029785
Epoch 240, val loss: 0.6985684037208557
Epoch 250, training loss: 0.3220764398574829 = 0.31518638134002686 + 0.001 * 6.890064239501953
Epoch 250, val loss: 0.6831576824188232
Epoch 260, training loss: 0.2869957685470581 = 0.2801099717617035 + 0.001 * 6.885797023773193
Epoch 260, val loss: 0.6730509400367737
Epoch 270, training loss: 0.25530004501342773 = 0.24841982126235962 + 0.001 * 6.880212306976318
Epoch 270, val loss: 0.6667886972427368
Epoch 280, training loss: 0.22615039348602295 = 0.21927398443222046 + 0.001 * 6.876407146453857
Epoch 280, val loss: 0.6638465523719788
Epoch 290, training loss: 0.19925425946712494 = 0.19238050282001495 + 0.001 * 6.8737568855285645
Epoch 290, val loss: 0.6637892127037048
Epoch 300, training loss: 0.1747027486562729 = 0.16783079504966736 + 0.001 * 6.871953010559082
Epoch 300, val loss: 0.6662324666976929
Epoch 310, training loss: 0.15287013351917267 = 0.14599889516830444 + 0.001 * 6.871240139007568
Epoch 310, val loss: 0.6708639860153198
Epoch 320, training loss: 0.13388404250144958 = 0.12701408565044403 + 0.001 * 6.869955062866211
Epoch 320, val loss: 0.677273154258728
Epoch 330, training loss: 0.11764725297689438 = 0.11077841371297836 + 0.001 * 6.868839740753174
Epoch 330, val loss: 0.6852267384529114
Epoch 340, training loss: 0.10385484248399734 = 0.09698835760354996 + 0.001 * 6.866486072540283
Epoch 340, val loss: 0.6946178078651428
Epoch 350, training loss: 0.09216803312301636 = 0.08530288934707642 + 0.001 * 6.865140438079834
Epoch 350, val loss: 0.7052180767059326
Epoch 360, training loss: 0.08225756883621216 = 0.07539471238851547 + 0.001 * 6.862853050231934
Epoch 360, val loss: 0.7166356444358826
Epoch 370, training loss: 0.0738379955291748 = 0.06697548180818558 + 0.001 * 6.8625168800354
Epoch 370, val loss: 0.7286971211433411
Epoch 380, training loss: 0.06663792580366135 = 0.05978048965334892 + 0.001 * 6.8574371337890625
Epoch 380, val loss: 0.7411003708839417
Epoch 390, training loss: 0.06044238060712814 = 0.053589269518852234 + 0.001 * 6.853110313415527
Epoch 390, val loss: 0.7535874843597412
Epoch 400, training loss: 0.05508532002568245 = 0.04822881519794464 + 0.001 * 6.856505870819092
Epoch 400, val loss: 0.7661218047142029
Epoch 410, training loss: 0.05040576308965683 = 0.04356181249022484 + 0.001 * 6.8439507484436035
Epoch 410, val loss: 0.7785212993621826
Epoch 420, training loss: 0.046320993453264236 = 0.03948124498128891 + 0.001 * 6.839748859405518
Epoch 420, val loss: 0.7907609939575195
Epoch 430, training loss: 0.04273952916264534 = 0.03589900583028793 + 0.001 * 6.840521812438965
Epoch 430, val loss: 0.8027733564376831
Epoch 440, training loss: 0.03957267105579376 = 0.032744456082582474 + 0.001 * 6.828216075897217
Epoch 440, val loss: 0.8145725727081299
Epoch 450, training loss: 0.03678217530250549 = 0.029959702864289284 + 0.001 * 6.822473526000977
Epoch 450, val loss: 0.8261415958404541
Epoch 460, training loss: 0.03430721536278725 = 0.02749517373740673 + 0.001 * 6.812039852142334
Epoch 460, val loss: 0.8374720215797424
Epoch 470, training loss: 0.03211672231554985 = 0.025306692346930504 + 0.001 * 6.810030460357666
Epoch 470, val loss: 0.8485475778579712
Epoch 480, training loss: 0.030171595513820648 = 0.023357564583420753 + 0.001 * 6.814030170440674
Epoch 480, val loss: 0.8593350052833557
Epoch 490, training loss: 0.028423594310879707 = 0.021615780889987946 + 0.001 * 6.80781364440918
Epoch 490, val loss: 0.8698850274085999
Epoch 500, training loss: 0.026845615357160568 = 0.02005288191139698 + 0.001 * 6.792734146118164
Epoch 500, val loss: 0.8801896572113037
Epoch 510, training loss: 0.025448055937886238 = 0.018646229058504105 + 0.001 * 6.801826477050781
Epoch 510, val loss: 0.8902120590209961
Epoch 520, training loss: 0.02416311763226986 = 0.01737593114376068 + 0.001 * 6.787186622619629
Epoch 520, val loss: 0.8999791145324707
Epoch 530, training loss: 0.023014072328805923 = 0.016225794330239296 + 0.001 * 6.788278579711914
Epoch 530, val loss: 0.9095161557197571
Epoch 540, training loss: 0.021969377994537354 = 0.01518147811293602 + 0.001 * 6.787898540496826
Epoch 540, val loss: 0.9188055396080017
Epoch 550, training loss: 0.021011773496866226 = 0.014231128618121147 + 0.001 * 6.780643939971924
Epoch 550, val loss: 0.927850604057312
Epoch 560, training loss: 0.020142890512943268 = 0.013364293612539768 + 0.001 * 6.7785964012146
Epoch 560, val loss: 0.9366713762283325
Epoch 570, training loss: 0.01935725100338459 = 0.012572268955409527 + 0.001 * 6.784982204437256
Epoch 570, val loss: 0.9452592730522156
Epoch 580, training loss: 0.01862538419663906 = 0.011847109533846378 + 0.001 * 6.7782745361328125
Epoch 580, val loss: 0.9536486864089966
Epoch 590, training loss: 0.01795661821961403 = 0.011182012036442757 + 0.001 * 6.7746052742004395
Epoch 590, val loss: 0.961794912815094
Epoch 600, training loss: 0.017352880910038948 = 0.010570951737463474 + 0.001 * 6.7819294929504395
Epoch 600, val loss: 0.9697529077529907
Epoch 610, training loss: 0.016779199242591858 = 0.01000848039984703 + 0.001 * 6.770718097686768
Epoch 610, val loss: 0.9775031208992004
Epoch 620, training loss: 0.016277512535452843 = 0.009489856660366058 + 0.001 * 6.787655353546143
Epoch 620, val loss: 0.9850675463676453
Epoch 630, training loss: 0.01577848754823208 = 0.00901105161756277 + 0.001 * 6.767435073852539
Epoch 630, val loss: 0.9924325346946716
Epoch 640, training loss: 0.015355153009295464 = 0.008568164892494678 + 0.001 * 6.786988258361816
Epoch 640, val loss: 0.9995970726013184
Epoch 650, training loss: 0.014920560643076897 = 0.00815737247467041 + 0.001 * 6.763188362121582
Epoch 650, val loss: 1.006574273109436
Epoch 660, training loss: 0.014536615461111069 = 0.007776008918881416 + 0.001 * 6.760606288909912
Epoch 660, val loss: 1.0134146213531494
Epoch 670, training loss: 0.014180406928062439 = 0.007421452552080154 + 0.001 * 6.7589545249938965
Epoch 670, val loss: 1.020050287246704
Epoch 680, training loss: 0.013851610943675041 = 0.007091536186635494 + 0.001 * 6.760074615478516
Epoch 680, val loss: 1.026518702507019
Epoch 690, training loss: 0.013547131791710854 = 0.006784107536077499 + 0.001 * 6.763023376464844
Epoch 690, val loss: 1.032839059829712
Epoch 700, training loss: 0.013252791948616505 = 0.006497192662209272 + 0.001 * 6.755599021911621
Epoch 700, val loss: 1.0389964580535889
Epoch 710, training loss: 0.01298953965306282 = 0.006229053251445293 + 0.001 * 6.760486602783203
Epoch 710, val loss: 1.044988989830017
Epoch 720, training loss: 0.012732002884149551 = 0.00597808975726366 + 0.001 * 6.753912448883057
Epoch 720, val loss: 1.050843596458435
Epoch 730, training loss: 0.012496527284383774 = 0.005742892622947693 + 0.001 * 6.753634929656982
Epoch 730, val loss: 1.0565739870071411
Epoch 740, training loss: 0.012277638539671898 = 0.005522135179489851 + 0.001 * 6.755503177642822
Epoch 740, val loss: 1.0621458292007446
Epoch 750, training loss: 0.01205165684223175 = 0.005314713343977928 + 0.001 * 6.736942768096924
Epoch 750, val loss: 1.0676072835922241
Epoch 760, training loss: 0.011858779937028885 = 0.005119516048580408 + 0.001 * 6.739263534545898
Epoch 760, val loss: 1.0729235410690308
Epoch 770, training loss: 0.011704551056027412 = 0.004935572389513254 + 0.001 * 6.768978595733643
Epoch 770, val loss: 1.0781484842300415
Epoch 780, training loss: 0.01149599626660347 = 0.0047620427794754505 + 0.001 * 6.73395299911499
Epoch 780, val loss: 1.0832319259643555
Epoch 790, training loss: 0.011333928443491459 = 0.004598116036504507 + 0.001 * 6.735812187194824
Epoch 790, val loss: 1.088198184967041
Epoch 800, training loss: 0.01117149367928505 = 0.004442882724106312 + 0.001 * 6.728610992431641
Epoch 800, val loss: 1.0930508375167847
Epoch 810, training loss: 0.01104216743260622 = 0.004295730497688055 + 0.001 * 6.746436595916748
Epoch 810, val loss: 1.097826361656189
Epoch 820, training loss: 0.010934894904494286 = 0.00415579229593277 + 0.001 * 6.779101848602295
Epoch 820, val loss: 1.1025354862213135
Epoch 830, training loss: 0.010764824226498604 = 0.004022966604679823 + 0.001 * 6.741857528686523
Epoch 830, val loss: 1.1072044372558594
Epoch 840, training loss: 0.010619976557791233 = 0.0038963875267654657 + 0.001 * 6.723588466644287
Epoch 840, val loss: 1.1118017435073853
Epoch 850, training loss: 0.010497593320906162 = 0.0037754590157419443 + 0.001 * 6.722134113311768
Epoch 850, val loss: 1.116309404373169
Epoch 860, training loss: 0.010403532534837723 = 0.0036598178558051586 + 0.001 * 6.743714809417725
Epoch 860, val loss: 1.1207798719406128
Epoch 870, training loss: 0.0102686220780015 = 0.0035492803435772657 + 0.001 * 6.71934175491333
Epoch 870, val loss: 1.12521493434906
Epoch 880, training loss: 0.010166737250983715 = 0.0034434606786817312 + 0.001 * 6.723276138305664
Epoch 880, val loss: 1.1295782327651978
Epoch 890, training loss: 0.010070852003991604 = 0.0033420578110963106 + 0.001 * 6.728794097900391
Epoch 890, val loss: 1.1339043378829956
Epoch 900, training loss: 0.009971882216632366 = 0.003244898049160838 + 0.001 * 6.726984024047852
Epoch 900, val loss: 1.1381882429122925
Epoch 910, training loss: 0.009876339696347713 = 0.003151901997625828 + 0.001 * 6.724437236785889
Epoch 910, val loss: 1.1424109935760498
Epoch 920, training loss: 0.009773789905011654 = 0.003062800969928503 + 0.001 * 6.710988521575928
Epoch 920, val loss: 1.146579623222351
Epoch 930, training loss: 0.009708471596240997 = 0.0029773360583931208 + 0.001 * 6.731135368347168
Epoch 930, val loss: 1.1507198810577393
Epoch 940, training loss: 0.00961233302950859 = 0.002895485609769821 + 0.001 * 6.716846942901611
Epoch 940, val loss: 1.154788851737976
Epoch 950, training loss: 0.009535246528685093 = 0.0028171043377369642 + 0.001 * 6.718141555786133
Epoch 950, val loss: 1.1588125228881836
Epoch 960, training loss: 0.009497073478996754 = 0.002741995733231306 + 0.001 * 6.755077362060547
Epoch 960, val loss: 1.1627224683761597
Epoch 970, training loss: 0.009370354004204273 = 0.0026701244059950113 + 0.001 * 6.700229644775391
Epoch 970, val loss: 1.166616678237915
Epoch 980, training loss: 0.009295894764363766 = 0.002601223299279809 + 0.001 * 6.694671154022217
Epoch 980, val loss: 1.1704460382461548
Epoch 990, training loss: 0.009226660244166851 = 0.0025351454969495535 + 0.001 * 6.691514015197754
Epoch 990, val loss: 1.1742005348205566
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7085
Flip ASR: 0.6533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9602783918380737 = 1.951904535293579 + 0.001 * 8.373826026916504
Epoch 0, val loss: 1.9477722644805908
Epoch 10, training loss: 1.9492456912994385 = 1.9408719539642334 + 0.001 * 8.373722076416016
Epoch 10, val loss: 1.9364336729049683
Epoch 20, training loss: 1.935383915901184 = 1.927010416984558 + 0.001 * 8.373449325561523
Epoch 20, val loss: 1.9218542575836182
Epoch 30, training loss: 1.9156678915023804 = 1.9072949886322021 + 0.001 * 8.372856140136719
Epoch 30, val loss: 1.9009747505187988
Epoch 40, training loss: 1.8867381811141968 = 1.8783668279647827 + 0.001 * 8.371407508850098
Epoch 40, val loss: 1.8707938194274902
Epoch 50, training loss: 1.8473750352859497 = 1.8390082120895386 + 0.001 * 8.366778373718262
Epoch 50, val loss: 1.8319694995880127
Epoch 60, training loss: 1.8041683435440063 = 1.795822024345398 + 0.001 * 8.34630012512207
Epoch 60, val loss: 1.7946231365203857
Epoch 70, training loss: 1.7648200988769531 = 1.7565851211547852 + 0.001 * 8.235027313232422
Epoch 70, val loss: 1.7654547691345215
Epoch 80, training loss: 1.7156368494033813 = 1.7077497243881226 + 0.001 * 7.887178897857666
Epoch 80, val loss: 1.726349115371704
Epoch 90, training loss: 1.6480958461761475 = 1.640365719795227 + 0.001 * 7.7301812171936035
Epoch 90, val loss: 1.6696020364761353
Epoch 100, training loss: 1.5627394914627075 = 1.555116891860962 + 0.001 * 7.62261962890625
Epoch 100, val loss: 1.5996981859207153
Epoch 110, training loss: 1.468593955039978 = 1.4611469507217407 + 0.001 * 7.446985244750977
Epoch 110, val loss: 1.525607943534851
Epoch 120, training loss: 1.3746824264526367 = 1.3674075603485107 + 0.001 * 7.274893760681152
Epoch 120, val loss: 1.4540302753448486
Epoch 130, training loss: 1.2836785316467285 = 1.2764416933059692 + 0.001 * 7.23689603805542
Epoch 130, val loss: 1.3882278203964233
Epoch 140, training loss: 1.1948643922805786 = 1.1877243518829346 + 0.001 * 7.14000940322876
Epoch 140, val loss: 1.325026035308838
Epoch 150, training loss: 1.1102774143218994 = 1.1032017469406128 + 0.001 * 7.075664520263672
Epoch 150, val loss: 1.2649812698364258
Epoch 160, training loss: 1.0332430601119995 = 1.0261991024017334 + 0.001 * 7.04400634765625
Epoch 160, val loss: 1.2106927633285522
Epoch 170, training loss: 0.9644346833229065 = 0.9574072957038879 + 0.001 * 7.027373313903809
Epoch 170, val loss: 1.1628772020339966
Epoch 180, training loss: 0.9008088707923889 = 0.8937960863113403 + 0.001 * 7.012807846069336
Epoch 180, val loss: 1.1185541152954102
Epoch 190, training loss: 0.8377839922904968 = 0.8307892084121704 + 0.001 * 6.9947686195373535
Epoch 190, val loss: 1.0737602710723877
Epoch 200, training loss: 0.7720364332199097 = 0.7650635242462158 + 0.001 * 6.972894191741943
Epoch 200, val loss: 1.0260205268859863
Epoch 210, training loss: 0.7033983469009399 = 0.696450412273407 + 0.001 * 6.947964191436768
Epoch 210, val loss: 0.9761762619018555
Epoch 220, training loss: 0.6348047256469727 = 0.6278777718544006 + 0.001 * 6.926970958709717
Epoch 220, val loss: 0.9283266663551331
Epoch 230, training loss: 0.5697420835494995 = 0.5628286600112915 + 0.001 * 6.913437366485596
Epoch 230, val loss: 0.8866249322891235
Epoch 240, training loss: 0.5103265643119812 = 0.5034189820289612 + 0.001 * 6.907558441162109
Epoch 240, val loss: 0.8537416458129883
Epoch 250, training loss: 0.4568779170513153 = 0.4499736726284027 + 0.001 * 6.904257774353027
Epoch 250, val loss: 0.8297166228294373
Epoch 260, training loss: 0.4085700809955597 = 0.40166816115379333 + 0.001 * 6.901918411254883
Epoch 260, val loss: 0.812708854675293
Epoch 270, training loss: 0.3641490638256073 = 0.35724976658821106 + 0.001 * 6.899298191070557
Epoch 270, val loss: 0.8002913594245911
Epoch 280, training loss: 0.3226679563522339 = 0.3157714903354645 + 0.001 * 6.8964524269104
Epoch 280, val loss: 0.7908361554145813
Epoch 290, training loss: 0.28341394662857056 = 0.27652040123939514 + 0.001 * 6.893558979034424
Epoch 290, val loss: 0.7836000323295593
Epoch 300, training loss: 0.24612630903720856 = 0.23923450708389282 + 0.001 * 6.891794681549072
Epoch 300, val loss: 0.7783379554748535
Epoch 310, training loss: 0.21126897633075714 = 0.20438100397586823 + 0.001 * 6.887967586517334
Epoch 310, val loss: 0.7753114104270935
Epoch 320, training loss: 0.17989738285541534 = 0.1730126589536667 + 0.001 * 6.884726524353027
Epoch 320, val loss: 0.7752577066421509
Epoch 330, training loss: 0.15303117036819458 = 0.14614835381507874 + 0.001 * 6.8828206062316895
Epoch 330, val loss: 0.7790420651435852
Epoch 340, training loss: 0.1309431940317154 = 0.12406236678361893 + 0.001 * 6.880819797515869
Epoch 340, val loss: 0.7869378328323364
Epoch 350, training loss: 0.11318214982748032 = 0.1063021644949913 + 0.001 * 6.879987716674805
Epoch 350, val loss: 0.7982061505317688
Epoch 360, training loss: 0.09891180694103241 = 0.09203362464904785 + 0.001 * 6.878180503845215
Epoch 360, val loss: 0.8118036985397339
Epoch 370, training loss: 0.08729758858680725 = 0.08042015880346298 + 0.001 * 6.877429962158203
Epoch 370, val loss: 0.8269018530845642
Epoch 380, training loss: 0.07769574224948883 = 0.0708167776465416 + 0.001 * 6.878964424133301
Epoch 380, val loss: 0.8429118990898132
Epoch 390, training loss: 0.06964410841464996 = 0.06276777386665344 + 0.001 * 6.876331806182861
Epoch 390, val loss: 0.8594051599502563
Epoch 400, training loss: 0.06282241642475128 = 0.055948078632354736 + 0.001 * 6.874334812164307
Epoch 400, val loss: 0.8760719299316406
Epoch 410, training loss: 0.05699385330080986 = 0.05012066289782524 + 0.001 * 6.873189926147461
Epoch 410, val loss: 0.8927095532417297
Epoch 420, training loss: 0.05197621136903763 = 0.04510410502552986 + 0.001 * 6.872104167938232
Epoch 420, val loss: 0.9092379212379456
Epoch 430, training loss: 0.047626204788684845 = 0.040753211826086044 + 0.001 * 6.872990608215332
Epoch 430, val loss: 0.925611674785614
Epoch 440, training loss: 0.043825745582580566 = 0.03695288300514221 + 0.001 * 6.87286376953125
Epoch 440, val loss: 0.9418231844902039
Epoch 450, training loss: 0.04048388823866844 = 0.033614709973335266 + 0.001 * 6.869176387786865
Epoch 450, val loss: 0.9577998518943787
Epoch 460, training loss: 0.03753892704844475 = 0.030670123174786568 + 0.001 * 6.868803024291992
Epoch 460, val loss: 0.9736012816429138
Epoch 470, training loss: 0.03493088111281395 = 0.028064224869012833 + 0.001 * 6.866654872894287
Epoch 470, val loss: 0.9892071485519409
Epoch 480, training loss: 0.03262987732887268 = 0.025751108303666115 + 0.001 * 6.878769397735596
Epoch 480, val loss: 1.0045061111450195
Epoch 490, training loss: 0.03055702894926071 = 0.023691672831773758 + 0.001 * 6.865356922149658
Epoch 490, val loss: 1.019564151763916
Epoch 500, training loss: 0.028716666623950005 = 0.021854205057024956 + 0.001 * 6.862461566925049
Epoch 500, val loss: 1.0343244075775146
Epoch 510, training loss: 0.027083279564976692 = 0.02020990662276745 + 0.001 * 6.8733720779418945
Epoch 510, val loss: 1.0487911701202393
Epoch 520, training loss: 0.02559567242860794 = 0.018734462559223175 + 0.001 * 6.861208438873291
Epoch 520, val loss: 1.0629277229309082
Epoch 530, training loss: 0.02426750771701336 = 0.01740747131407261 + 0.001 * 6.860036373138428
Epoch 530, val loss: 1.0767216682434082
Epoch 540, training loss: 0.023068303242325783 = 0.016211118549108505 + 0.001 * 6.857184886932373
Epoch 540, val loss: 1.0902072191238403
Epoch 550, training loss: 0.021982669830322266 = 0.015129867941141129 + 0.001 * 6.8528008460998535
Epoch 550, val loss: 1.10332453250885
Epoch 560, training loss: 0.02099825069308281 = 0.014150265604257584 + 0.001 * 6.847984313964844
Epoch 560, val loss: 1.116105556488037
Epoch 570, training loss: 0.020106784999370575 = 0.01326087024062872 + 0.001 * 6.845913887023926
Epoch 570, val loss: 1.1285607814788818
Epoch 580, training loss: 0.01934986002743244 = 0.012451195158064365 + 0.001 * 6.898664474487305
Epoch 580, val loss: 1.1406378746032715
Epoch 590, training loss: 0.018555019050836563 = 0.011712673120200634 + 0.001 * 6.842345714569092
Epoch 590, val loss: 1.1524370908737183
Epoch 600, training loss: 0.017881903797388077 = 0.011037669144570827 + 0.001 * 6.844235420227051
Epoch 600, val loss: 1.1638983488082886
Epoch 610, training loss: 0.01725860871374607 = 0.010419228114187717 + 0.001 * 6.839380264282227
Epoch 610, val loss: 1.1750717163085938
Epoch 620, training loss: 0.016697969287633896 = 0.009851631708443165 + 0.001 * 6.84633731842041
Epoch 620, val loss: 1.1860041618347168
Epoch 630, training loss: 0.016172457486391068 = 0.009329557418823242 + 0.001 * 6.842899799346924
Epoch 630, val loss: 1.196614146232605
Epoch 640, training loss: 0.015677638351917267 = 0.00884847529232502 + 0.001 * 6.829163074493408
Epoch 640, val loss: 1.2069709300994873
Epoch 650, training loss: 0.015224961563944817 = 0.00840427540242672 + 0.001 * 6.820686340332031
Epoch 650, val loss: 1.2170695066452026
Epoch 660, training loss: 0.014804407954216003 = 0.007993481121957302 + 0.001 * 6.810926914215088
Epoch 660, val loss: 1.2269326448440552
Epoch 670, training loss: 0.014427517540752888 = 0.0076129138469696045 + 0.001 * 6.814603328704834
Epoch 670, val loss: 1.2365232706069946
Epoch 680, training loss: 0.01407611183822155 = 0.007259813137352467 + 0.001 * 6.816298961639404
Epoch 680, val loss: 1.2459250688552856
Epoch 690, training loss: 0.013743864372372627 = 0.0069316113367676735 + 0.001 * 6.812252044677734
Epoch 690, val loss: 1.2550710439682007
Epoch 700, training loss: 0.013439221307635307 = 0.006626104470342398 + 0.001 * 6.813117027282715
Epoch 700, val loss: 1.2639926671981812
Epoch 710, training loss: 0.013138312846422195 = 0.006341234315186739 + 0.001 * 6.797077655792236
Epoch 710, val loss: 1.2727328538894653
Epoch 720, training loss: 0.01290063001215458 = 0.0060753412544727325 + 0.001 * 6.825288772583008
Epoch 720, val loss: 1.2812352180480957
Epoch 730, training loss: 0.012620631605386734 = 0.005826747510582209 + 0.001 * 6.79388427734375
Epoch 730, val loss: 1.2895724773406982
Epoch 740, training loss: 0.012397232465445995 = 0.005593928974121809 + 0.001 * 6.803303241729736
Epoch 740, val loss: 1.2977412939071655
Epoch 750, training loss: 0.012182654812932014 = 0.005375655367970467 + 0.001 * 6.806999206542969
Epoch 750, val loss: 1.3057230710983276
Epoch 760, training loss: 0.01200610026717186 = 0.005170789547264576 + 0.001 * 6.835310459136963
Epoch 760, val loss: 1.313510775566101
Epoch 770, training loss: 0.011769084259867668 = 0.004978243727236986 + 0.001 * 6.790840148925781
Epoch 770, val loss: 1.3211838006973267
Epoch 780, training loss: 0.011598167940974236 = 0.0047970423474907875 + 0.001 * 6.8011250495910645
Epoch 780, val loss: 1.3286361694335938
Epoch 790, training loss: 0.011413057334721088 = 0.0046262131072580814 + 0.001 * 6.786843776702881
Epoch 790, val loss: 1.335967779159546
Epoch 800, training loss: 0.011255653575062752 = 0.004464876838028431 + 0.001 * 6.790776252746582
Epoch 800, val loss: 1.3431216478347778
Epoch 810, training loss: 0.011087924242019653 = 0.0043121036142110825 + 0.001 * 6.775820732116699
Epoch 810, val loss: 1.3501702547073364
Epoch 820, training loss: 0.010950079187750816 = 0.004166745580732822 + 0.001 * 6.783332824707031
Epoch 820, val loss: 1.357094407081604
Epoch 830, training loss: 0.010814947076141834 = 0.004027837887406349 + 0.001 * 6.787108898162842
Epoch 830, val loss: 1.3639533519744873
Epoch 840, training loss: 0.01066394429653883 = 0.0038946992717683315 + 0.001 * 6.76924467086792
Epoch 840, val loss: 1.3707494735717773
Epoch 850, training loss: 0.010543503798544407 = 0.0037670754827558994 + 0.001 * 6.77642822265625
Epoch 850, val loss: 1.3775378465652466
Epoch 860, training loss: 0.010414048098027706 = 0.003644545329734683 + 0.001 * 6.76950216293335
Epoch 860, val loss: 1.3842347860336304
Epoch 870, training loss: 0.010288700461387634 = 0.003527018940076232 + 0.001 * 6.76168155670166
Epoch 870, val loss: 1.3909469842910767
Epoch 880, training loss: 0.01018186192959547 = 0.0034142935182899237 + 0.001 * 6.767568111419678
Epoch 880, val loss: 1.3975549936294556
Epoch 890, training loss: 0.010081155225634575 = 0.0033063311129808426 + 0.001 * 6.774824142456055
Epoch 890, val loss: 1.4041584730148315
Epoch 900, training loss: 0.009970556013286114 = 0.0032030364964157343 + 0.001 * 6.767519474029541
Epoch 900, val loss: 1.4106258153915405
Epoch 910, training loss: 0.009861627593636513 = 0.003104354254901409 + 0.001 * 6.757273197174072
Epoch 910, val loss: 1.4170863628387451
Epoch 920, training loss: 0.009781775996088982 = 0.0030102417804300785 + 0.001 * 6.771534442901611
Epoch 920, val loss: 1.4234033823013306
Epoch 930, training loss: 0.009676405228674412 = 0.0029204750899225473 + 0.001 * 6.755929946899414
Epoch 930, val loss: 1.4296631813049316
Epoch 940, training loss: 0.009594464674592018 = 0.0028348371852189302 + 0.001 * 6.759626865386963
Epoch 940, val loss: 1.4357552528381348
Epoch 950, training loss: 0.009500022046267986 = 0.002753137843683362 + 0.001 * 6.746884346008301
Epoch 950, val loss: 1.4418526887893677
Epoch 960, training loss: 0.009460320696234703 = 0.0026751484256237745 + 0.001 * 6.7851715087890625
Epoch 960, val loss: 1.4478096961975098
Epoch 970, training loss: 0.009350964799523354 = 0.0026007029227912426 + 0.001 * 6.750261306762695
Epoch 970, val loss: 1.4536703824996948
Epoch 980, training loss: 0.009275361895561218 = 0.002529641380533576 + 0.001 * 6.745720386505127
Epoch 980, val loss: 1.4594084024429321
Epoch 990, training loss: 0.009221715852618217 = 0.0024618622846901417 + 0.001 * 6.759853839874268
Epoch 990, val loss: 1.4650577306747437
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7860
Flip ASR: 0.7467/225 nodes
The final ASR:0.69619, 0.07882, Accuracy:0.81111, 0.00907
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11658])
remove edge: torch.Size([2, 9476])
updated graph: torch.Size([2, 10578])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.83086, 0.00761
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9660181999206543 = 1.9576444625854492 + 0.001 * 8.373773574829102
Epoch 0, val loss: 1.960188388824463
Epoch 10, training loss: 1.9557793140411377 = 1.9474055767059326 + 0.001 * 8.373726844787598
Epoch 10, val loss: 1.9505800008773804
Epoch 20, training loss: 1.9434798955917358 = 1.9351063966751099 + 0.001 * 8.373475074768066
Epoch 20, val loss: 1.9385570287704468
Epoch 30, training loss: 1.9265238046646118 = 1.9181509017944336 + 0.001 * 8.372919082641602
Epoch 30, val loss: 1.9215713739395142
Epoch 40, training loss: 1.9015980958938599 = 1.8932265043258667 + 0.001 * 8.371641159057617
Epoch 40, val loss: 1.8964624404907227
Epoch 50, training loss: 1.8659210205078125 = 1.8575528860092163 + 0.001 * 8.368104934692383
Epoch 50, val loss: 1.861329197883606
Epoch 60, training loss: 1.8220754861831665 = 1.8137205839157104 + 0.001 * 8.354902267456055
Epoch 60, val loss: 1.8212419748306274
Epoch 70, training loss: 1.7804254293441772 = 1.7721366882324219 + 0.001 * 8.288697242736816
Epoch 70, val loss: 1.7871825695037842
Epoch 80, training loss: 1.7357488870620728 = 1.7278556823730469 + 0.001 * 7.893186092376709
Epoch 80, val loss: 1.749328374862671
Epoch 90, training loss: 1.6759179830551147 = 1.6684166193008423 + 0.001 * 7.501338481903076
Epoch 90, val loss: 1.6966156959533691
Epoch 100, training loss: 1.5958898067474365 = 1.588659405708313 + 0.001 * 7.230445861816406
Epoch 100, val loss: 1.6268844604492188
Epoch 110, training loss: 1.4955631494522095 = 1.4884644746780396 + 0.001 * 7.0986762046813965
Epoch 110, val loss: 1.5410372018814087
Epoch 120, training loss: 1.382250189781189 = 1.3752135038375854 + 0.001 * 7.03669548034668
Epoch 120, val loss: 1.4456048011779785
Epoch 130, training loss: 1.2649458646774292 = 1.2579516172409058 + 0.001 * 6.9942426681518555
Epoch 130, val loss: 1.3500691652297974
Epoch 140, training loss: 1.1495190858840942 = 1.1425620317459106 + 0.001 * 6.9571003913879395
Epoch 140, val loss: 1.259273886680603
Epoch 150, training loss: 1.0408127307891846 = 1.0338850021362305 + 0.001 * 6.927762508392334
Epoch 150, val loss: 1.1751081943511963
Epoch 160, training loss: 0.9415701031684875 = 0.934669017791748 + 0.001 * 6.9011054039001465
Epoch 160, val loss: 1.0991730690002441
Epoch 170, training loss: 0.8512383699417114 = 0.8443609476089478 + 0.001 * 6.877438545227051
Epoch 170, val loss: 1.0304627418518066
Epoch 180, training loss: 0.7680023908615112 = 0.7611435651779175 + 0.001 * 6.858822345733643
Epoch 180, val loss: 0.967534601688385
Epoch 190, training loss: 0.6914848685264587 = 0.6846381425857544 + 0.001 * 6.846714496612549
Epoch 190, val loss: 0.9105926752090454
Epoch 200, training loss: 0.6227416396141052 = 0.6159027814865112 + 0.001 * 6.838846683502197
Epoch 200, val loss: 0.8611598610877991
Epoch 210, training loss: 0.5627816319465637 = 0.5559505224227905 + 0.001 * 6.831100940704346
Epoch 210, val loss: 0.8210060000419617
Epoch 220, training loss: 0.511391282081604 = 0.5045694708824158 + 0.001 * 6.821835041046143
Epoch 220, val loss: 0.7905664443969727
Epoch 230, training loss: 0.46716728806495667 = 0.4603551924228668 + 0.001 * 6.812106132507324
Epoch 230, val loss: 0.7685771584510803
Epoch 240, training loss: 0.42803123593330383 = 0.4212278127670288 + 0.001 * 6.80342435836792
Epoch 240, val loss: 0.7527490258216858
Epoch 250, training loss: 0.3917498290538788 = 0.38495489954948425 + 0.001 * 6.794938087463379
Epoch 250, val loss: 0.7403585910797119
Epoch 260, training loss: 0.35653555393218994 = 0.3497470021247864 + 0.001 * 6.788547515869141
Epoch 260, val loss: 0.729680061340332
Epoch 270, training loss: 0.32142966985702515 = 0.31464678049087524 + 0.001 * 6.782900810241699
Epoch 270, val loss: 0.7201269268989563
Epoch 280, training loss: 0.2863861322402954 = 0.27960753440856934 + 0.001 * 6.778589248657227
Epoch 280, val loss: 0.7118505239486694
Epoch 290, training loss: 0.2521474063396454 = 0.24537211656570435 + 0.001 * 6.775292873382568
Epoch 290, val loss: 0.7052361965179443
Epoch 300, training loss: 0.2199014127254486 = 0.2131270170211792 + 0.001 * 6.774388313293457
Epoch 300, val loss: 0.7008082270622253
Epoch 310, training loss: 0.19076192378997803 = 0.18398955464363098 + 0.001 * 6.772364616394043
Epoch 310, val loss: 0.6990172266960144
Epoch 320, training loss: 0.165366530418396 = 0.15859586000442505 + 0.001 * 6.770669460296631
Epoch 320, val loss: 0.6999884247779846
Epoch 330, training loss: 0.14382138848304749 = 0.1370520442724228 + 0.001 * 6.769344806671143
Epoch 330, val loss: 0.7036627531051636
Epoch 340, training loss: 0.12582522630691528 = 0.11905729025602341 + 0.001 * 6.767934799194336
Epoch 340, val loss: 0.709712564945221
Epoch 350, training loss: 0.110835961997509 = 0.10406921803951263 + 0.001 * 6.766742706298828
Epoch 350, val loss: 0.7177333235740662
Epoch 360, training loss: 0.0982804074883461 = 0.09151396155357361 + 0.001 * 6.766443252563477
Epoch 360, val loss: 0.7273883819580078
Epoch 370, training loss: 0.08766105771064758 = 0.08089611679315567 + 0.001 * 6.764937877655029
Epoch 370, val loss: 0.7383343577384949
Epoch 380, training loss: 0.07859326899051666 = 0.07182832807302475 + 0.001 * 6.764944553375244
Epoch 380, val loss: 0.7503213882446289
Epoch 390, training loss: 0.0707767903804779 = 0.06401341408491135 + 0.001 * 6.7633795738220215
Epoch 390, val loss: 0.7632149457931519
Epoch 400, training loss: 0.06399592012166977 = 0.05723338946700096 + 0.001 * 6.7625274658203125
Epoch 400, val loss: 0.7768075466156006
Epoch 410, training loss: 0.058088257908821106 = 0.05132661387324333 + 0.001 * 6.761642932891846
Epoch 410, val loss: 0.7910430431365967
Epoch 420, training loss: 0.052926380187273026 = 0.046165212988853455 + 0.001 * 6.761168003082275
Epoch 420, val loss: 0.805778980255127
Epoch 430, training loss: 0.04840043559670448 = 0.041639331728219986 + 0.001 * 6.761104106903076
Epoch 430, val loss: 0.8208932280540466
Epoch 440, training loss: 0.044419676065444946 = 0.037660371512174606 + 0.001 * 6.759304046630859
Epoch 440, val loss: 0.8362914323806763
Epoch 450, training loss: 0.04091256856918335 = 0.03415393456816673 + 0.001 * 6.758631706237793
Epoch 450, val loss: 0.8518599271774292
Epoch 460, training loss: 0.03781712055206299 = 0.031059505417943 + 0.001 * 6.7576141357421875
Epoch 460, val loss: 0.8674944639205933
Epoch 470, training loss: 0.03508578613400459 = 0.028325265273451805 + 0.001 * 6.760520935058594
Epoch 470, val loss: 0.8830142617225647
Epoch 480, training loss: 0.03266466408967972 = 0.025907285511493683 + 0.001 * 6.757376194000244
Epoch 480, val loss: 0.8982785940170288
Epoch 490, training loss: 0.03052198886871338 = 0.02376672811806202 + 0.001 * 6.755260944366455
Epoch 490, val loss: 0.913230836391449
Epoch 500, training loss: 0.028621960431337357 = 0.02186794951558113 + 0.001 * 6.754010200500488
Epoch 500, val loss: 0.9278049468994141
Epoch 510, training loss: 0.02693396434187889 = 0.02018057182431221 + 0.001 * 6.753392696380615
Epoch 510, val loss: 0.9419532418251038
Epoch 520, training loss: 0.025433745235204697 = 0.0186776015907526 + 0.001 * 6.756143093109131
Epoch 520, val loss: 0.9556334614753723
Epoch 530, training loss: 0.024087226018309593 = 0.017334861680865288 + 0.001 * 6.752363681793213
Epoch 530, val loss: 0.9688636064529419
Epoch 540, training loss: 0.02288193441927433 = 0.016131674870848656 + 0.001 * 6.7502593994140625
Epoch 540, val loss: 0.9816107749938965
Epoch 550, training loss: 0.02179904840886593 = 0.015050163492560387 + 0.001 * 6.748884677886963
Epoch 550, val loss: 0.9939743876457214
Epoch 560, training loss: 0.020824009552598 = 0.014075237326323986 + 0.001 * 6.748772144317627
Epoch 560, val loss: 1.005918025970459
Epoch 570, training loss: 0.01994718611240387 = 0.013193747960031033 + 0.001 * 6.753437042236328
Epoch 570, val loss: 1.017472267150879
Epoch 580, training loss: 0.01914084330201149 = 0.012394465506076813 + 0.001 * 6.7463765144348145
Epoch 580, val loss: 1.0286705493927002
Epoch 590, training loss: 0.01841910183429718 = 0.011667712591588497 + 0.001 * 6.751389026641846
Epoch 590, val loss: 1.039507508277893
Epoch 600, training loss: 0.017750350758433342 = 0.01100514642894268 + 0.001 * 6.745204448699951
Epoch 600, val loss: 1.0500293970108032
Epoch 610, training loss: 0.017140738666057587 = 0.010399460792541504 + 0.001 * 6.741278171539307
Epoch 610, val loss: 1.0602151155471802
Epoch 620, training loss: 0.016594307497143745 = 0.00984438881278038 + 0.001 * 6.7499189376831055
Epoch 620, val loss: 1.0700924396514893
Epoch 630, training loss: 0.01607220061123371 = 0.009334525093436241 + 0.001 * 6.737675189971924
Epoch 630, val loss: 1.0796754360198975
Epoch 640, training loss: 0.015600567683577538 = 0.00886512454599142 + 0.001 * 6.735442161560059
Epoch 640, val loss: 1.0889461040496826
Epoch 650, training loss: 0.0151851586997509 = 0.008431994356215 + 0.001 * 6.753164291381836
Epoch 650, val loss: 1.0979546308517456
Epoch 660, training loss: 0.014770072884857655 = 0.008031499572098255 + 0.001 * 6.73857307434082
Epoch 660, val loss: 1.1067229509353638
Epoch 670, training loss: 0.014395903795957565 = 0.007660561706870794 + 0.001 * 6.735342025756836
Epoch 670, val loss: 1.115235686302185
Epoch 680, training loss: 0.014055831357836723 = 0.007316347677260637 + 0.001 * 6.73948335647583
Epoch 680, val loss: 1.1235203742980957
Epoch 690, training loss: 0.013729250058531761 = 0.006996328476816416 + 0.001 * 6.732921600341797
Epoch 690, val loss: 1.1315940618515015
Epoch 700, training loss: 0.013434339314699173 = 0.006698281969875097 + 0.001 * 6.736056804656982
Epoch 700, val loss: 1.139457106590271
Epoch 710, training loss: 0.013150162994861603 = 0.006420239340513945 + 0.001 * 6.729923725128174
Epoch 710, val loss: 1.1470985412597656
Epoch 720, training loss: 0.012888908386230469 = 0.006160351913422346 + 0.001 * 6.7285566329956055
Epoch 720, val loss: 1.1545813083648682
Epoch 730, training loss: 0.012645747512578964 = 0.005916725378483534 + 0.001 * 6.72902250289917
Epoch 730, val loss: 1.1618715524673462
Epoch 740, training loss: 0.012407410889863968 = 0.005687502212822437 + 0.001 * 6.719907760620117
Epoch 740, val loss: 1.1690115928649902
Epoch 750, training loss: 0.012202953919768333 = 0.005470585078001022 + 0.001 * 6.732367992401123
Epoch 750, val loss: 1.1760450601577759
Epoch 760, training loss: 0.011981245130300522 = 0.00526413694024086 + 0.001 * 6.717108249664307
Epoch 760, val loss: 1.1830048561096191
Epoch 770, training loss: 0.011792265810072422 = 0.005067098420113325 + 0.001 * 6.725167274475098
Epoch 770, val loss: 1.1898988485336304
Epoch 780, training loss: 0.011591480113565922 = 0.00487922178581357 + 0.001 * 6.7122578620910645
Epoch 780, val loss: 1.1967096328735352
Epoch 790, training loss: 0.011411641724407673 = 0.00469995616003871 + 0.001 * 6.7116851806640625
Epoch 790, val loss: 1.2034722566604614
Epoch 800, training loss: 0.011246562004089355 = 0.004529085475951433 + 0.001 * 6.7174763679504395
Epoch 800, val loss: 1.2101384401321411
Epoch 810, training loss: 0.011072865687310696 = 0.004366509150713682 + 0.001 * 6.706356048583984
Epoch 810, val loss: 1.2167309522628784
Epoch 820, training loss: 0.010922864079475403 = 0.00421205535531044 + 0.001 * 6.710808753967285
Epoch 820, val loss: 1.223196029663086
Epoch 830, training loss: 0.010774089954793453 = 0.004065447952598333 + 0.001 * 6.708641529083252
Epoch 830, val loss: 1.2295647859573364
Epoch 840, training loss: 0.010630116797983646 = 0.003926293458789587 + 0.001 * 6.703823089599609
Epoch 840, val loss: 1.2358454465866089
Epoch 850, training loss: 0.01052120141685009 = 0.0037942412309348583 + 0.001 * 6.726960182189941
Epoch 850, val loss: 1.2419929504394531
Epoch 860, training loss: 0.010377583093941212 = 0.0036690111737698317 + 0.001 * 6.708571434020996
Epoch 860, val loss: 1.248051404953003
Epoch 870, training loss: 0.010257070884108543 = 0.003550145775079727 + 0.001 * 6.706924915313721
Epoch 870, val loss: 1.2539840936660767
Epoch 880, training loss: 0.01013652328401804 = 0.003437280422076583 + 0.001 * 6.699242115020752
Epoch 880, val loss: 1.259817361831665
Epoch 890, training loss: 0.010024815797805786 = 0.003330112900584936 + 0.001 * 6.6947021484375
Epoch 890, val loss: 1.265533447265625
Epoch 900, training loss: 0.009924059733748436 = 0.003228310728445649 + 0.001 * 6.695748329162598
Epoch 900, val loss: 1.2711267471313477
Epoch 910, training loss: 0.00982420239597559 = 0.003131593344733119 + 0.001 * 6.69260835647583
Epoch 910, val loss: 1.2766227722167969
Epoch 920, training loss: 0.009742889553308487 = 0.0030396832153201103 + 0.001 * 6.7032060623168945
Epoch 920, val loss: 1.281987190246582
Epoch 930, training loss: 0.009664639830589294 = 0.0029523433186113834 + 0.001 * 6.712296485900879
Epoch 930, val loss: 1.2872735261917114
Epoch 940, training loss: 0.009575411677360535 = 0.0028692258056253195 + 0.001 * 6.706185340881348
Epoch 940, val loss: 1.2924379110336304
Epoch 950, training loss: 0.009491290897130966 = 0.0027900380082428455 + 0.001 * 6.701251983642578
Epoch 950, val loss: 1.2975343465805054
Epoch 960, training loss: 0.009414889849722385 = 0.00271455361507833 + 0.001 * 6.700335502624512
Epoch 960, val loss: 1.302517056465149
Epoch 970, training loss: 0.009349285624921322 = 0.00264252838678658 + 0.001 * 6.706756591796875
Epoch 970, val loss: 1.3074326515197754
Epoch 980, training loss: 0.009256408549845219 = 0.0025737760588526726 + 0.001 * 6.682631969451904
Epoch 980, val loss: 1.3122506141662598
Epoch 990, training loss: 0.00920919980853796 = 0.002508096629753709 + 0.001 * 6.701103210449219
Epoch 990, val loss: 1.3169938325881958
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6937
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9513829946517944 = 1.9430092573165894 + 0.001 * 8.37375259399414
Epoch 0, val loss: 1.9374371767044067
Epoch 10, training loss: 1.941122055053711 = 1.9327484369277954 + 0.001 * 8.37366771697998
Epoch 10, val loss: 1.927362322807312
Epoch 20, training loss: 1.9290281534194946 = 1.9206547737121582 + 0.001 * 8.373361587524414
Epoch 20, val loss: 1.9150750637054443
Epoch 30, training loss: 1.9126355648040771 = 1.904262900352478 + 0.001 * 8.372701644897461
Epoch 30, val loss: 1.8981711864471436
Epoch 40, training loss: 1.888838768005371 = 1.8804676532745361 + 0.001 * 8.371167182922363
Epoch 40, val loss: 1.8736846446990967
Epoch 50, training loss: 1.8549721240997314 = 1.8466051816940308 + 0.001 * 8.366903305053711
Epoch 50, val loss: 1.839836835861206
Epoch 60, training loss: 1.812192440032959 = 1.8038426637649536 + 0.001 * 8.349833488464355
Epoch 60, val loss: 1.7997151613235474
Epoch 70, training loss: 1.767311453819275 = 1.7590649127960205 + 0.001 * 8.246541023254395
Epoch 70, val loss: 1.7610708475112915
Epoch 80, training loss: 1.7148092985153198 = 1.7071034908294678 + 0.001 * 7.7057952880859375
Epoch 80, val loss: 1.7153874635696411
Epoch 90, training loss: 1.6418968439102173 = 1.634368658065796 + 0.001 * 7.528200149536133
Epoch 90, val loss: 1.6509641408920288
Epoch 100, training loss: 1.5472239255905151 = 1.5398361682891846 + 0.001 * 7.3877854347229
Epoch 100, val loss: 1.5707536935806274
Epoch 110, training loss: 1.4392508268356323 = 1.4320266246795654 + 0.001 * 7.224254131317139
Epoch 110, val loss: 1.4831432104110718
Epoch 120, training loss: 1.331845760345459 = 1.3247864246368408 + 0.001 * 7.059283256530762
Epoch 120, val loss: 1.3977491855621338
Epoch 130, training loss: 1.2311325073242188 = 1.2241394519805908 + 0.001 * 6.993112564086914
Epoch 130, val loss: 1.3203387260437012
Epoch 140, training loss: 1.1361958980560303 = 1.129260540008545 + 0.001 * 6.935389995574951
Epoch 140, val loss: 1.2495887279510498
Epoch 150, training loss: 1.0443897247314453 = 1.037474513053894 + 0.001 * 6.915158748626709
Epoch 150, val loss: 1.1816638708114624
Epoch 160, training loss: 0.9545335173606873 = 0.947638213634491 + 0.001 * 6.895287036895752
Epoch 160, val loss: 1.115585207939148
Epoch 170, training loss: 0.8684256672859192 = 0.861549973487854 + 0.001 * 6.875701427459717
Epoch 170, val loss: 1.0519424676895142
Epoch 180, training loss: 0.7894923090934753 = 0.7826313376426697 + 0.001 * 6.860959529876709
Epoch 180, val loss: 0.9936050772666931
Epoch 190, training loss: 0.7200162410736084 = 0.7131662368774414 + 0.001 * 6.849981784820557
Epoch 190, val loss: 0.943061113357544
Epoch 200, training loss: 0.659617006778717 = 0.6527737379074097 + 0.001 * 6.843253135681152
Epoch 200, val loss: 0.9006543159484863
Epoch 210, training loss: 0.6058640480041504 = 0.5990245938301086 + 0.001 * 6.839445114135742
Epoch 210, val loss: 0.8651596307754517
Epoch 220, training loss: 0.5559152960777283 = 0.5490783452987671 + 0.001 * 6.836968421936035
Epoch 220, val loss: 0.8346197009086609
Epoch 230, training loss: 0.5075294375419617 = 0.5006949305534363 + 0.001 * 6.834517478942871
Epoch 230, val loss: 0.807408332824707
Epoch 240, training loss: 0.4597933292388916 = 0.4529617726802826 + 0.001 * 6.831541538238525
Epoch 240, val loss: 0.782758355140686
Epoch 250, training loss: 0.4127768278121948 = 0.4059487283229828 + 0.001 * 6.828109264373779
Epoch 250, val loss: 0.7607542276382446
Epoch 260, training loss: 0.3670262098312378 = 0.36020174622535706 + 0.001 * 6.824477195739746
Epoch 260, val loss: 0.7417123317718506
Epoch 270, training loss: 0.32333898544311523 = 0.31651830673217773 + 0.001 * 6.820682525634766
Epoch 270, val loss: 0.7259912490844727
Epoch 280, training loss: 0.28263160586357117 = 0.275814950466156 + 0.001 * 6.816653251647949
Epoch 280, val loss: 0.7136322855949402
Epoch 290, training loss: 0.24578288197517395 = 0.23897069692611694 + 0.001 * 6.812191486358643
Epoch 290, val loss: 0.7046945095062256
Epoch 300, training loss: 0.2133251279592514 = 0.20651811361312866 + 0.001 * 6.807007312774658
Epoch 300, val loss: 0.6991302371025085
Epoch 310, training loss: 0.18542040884494781 = 0.17861485481262207 + 0.001 * 6.805553436279297
Epoch 310, val loss: 0.6968932151794434
Epoch 320, training loss: 0.16172099113464355 = 0.1549256294965744 + 0.001 * 6.7953572273254395
Epoch 320, val loss: 0.6977876424789429
Epoch 330, training loss: 0.14171867072582245 = 0.13492928445339203 + 0.001 * 6.78938627243042
Epoch 330, val loss: 0.70101398229599
Epoch 340, training loss: 0.12484965473413467 = 0.11806818097829819 + 0.001 * 6.781470775604248
Epoch 340, val loss: 0.7065789699554443
Epoch 350, training loss: 0.1105632558465004 = 0.10378525406122208 + 0.001 * 6.778001308441162
Epoch 350, val loss: 0.7138204574584961
Epoch 360, training loss: 0.09836781769990921 = 0.09159847348928452 + 0.001 * 6.769346237182617
Epoch 360, val loss: 0.7223634123802185
Epoch 370, training loss: 0.08791311085224152 = 0.08114900439977646 + 0.001 * 6.764106273651123
Epoch 370, val loss: 0.731999933719635
Epoch 380, training loss: 0.07891390472650528 = 0.07215496152639389 + 0.001 * 6.758941650390625
Epoch 380, val loss: 0.742401123046875
Epoch 390, training loss: 0.07112850248813629 = 0.06437338888645172 + 0.001 * 6.755110263824463
Epoch 390, val loss: 0.7534602880477905
Epoch 400, training loss: 0.06435544788837433 = 0.05760086700320244 + 0.001 * 6.754578590393066
Epoch 400, val loss: 0.7649574279785156
Epoch 410, training loss: 0.058450549840927124 = 0.051698438823223114 + 0.001 * 6.752110004425049
Epoch 410, val loss: 0.7766290903091431
Epoch 420, training loss: 0.053288619965314865 = 0.046536292880773544 + 0.001 * 6.752325534820557
Epoch 420, val loss: 0.7883391380310059
Epoch 430, training loss: 0.04876278340816498 = 0.042012862861156464 + 0.001 * 6.749919891357422
Epoch 430, val loss: 0.8000988364219666
Epoch 440, training loss: 0.04478614032268524 = 0.03803864121437073 + 0.001 * 6.747498512268066
Epoch 440, val loss: 0.8117421269416809
Epoch 450, training loss: 0.04128352925181389 = 0.034537941217422485 + 0.001 * 6.745587348937988
Epoch 450, val loss: 0.8232690095901489
Epoch 460, training loss: 0.038195960223674774 = 0.03145122900605202 + 0.001 * 6.74473237991333
Epoch 460, val loss: 0.8345661163330078
Epoch 470, training loss: 0.035464778542518616 = 0.028721122071146965 + 0.001 * 6.743656158447266
Epoch 470, val loss: 0.8456692099571228
Epoch 480, training loss: 0.03303975611925125 = 0.02629832923412323 + 0.001 * 6.741425037384033
Epoch 480, val loss: 0.8565608263015747
Epoch 490, training loss: 0.03088006004691124 = 0.024142315611243248 + 0.001 * 6.737743854522705
Epoch 490, val loss: 0.8672087788581848
Epoch 500, training loss: 0.028957340866327286 = 0.02222210355103016 + 0.001 * 6.7352375984191895
Epoch 500, val loss: 0.8776392340660095
Epoch 510, training loss: 0.0272353645414114 = 0.02050645463168621 + 0.001 * 6.728909969329834
Epoch 510, val loss: 0.8878616094589233
Epoch 520, training loss: 0.025698469951748848 = 0.01896951161324978 + 0.001 * 6.7289581298828125
Epoch 520, val loss: 0.8978535532951355
Epoch 530, training loss: 0.02431691437959671 = 0.017589503899216652 + 0.001 * 6.727410793304443
Epoch 530, val loss: 0.9075809717178345
Epoch 540, training loss: 0.02306544966995716 = 0.016347646713256836 + 0.001 * 6.717803001403809
Epoch 540, val loss: 0.917175829410553
Epoch 550, training loss: 0.021947816014289856 = 0.015227307565510273 + 0.001 * 6.7205071449279785
Epoch 550, val loss: 0.9264851212501526
Epoch 560, training loss: 0.020942306146025658 = 0.014214334078133106 + 0.001 * 6.727972030639648
Epoch 560, val loss: 0.9356049299240112
Epoch 570, training loss: 0.020002655684947968 = 0.013296589255332947 + 0.001 * 6.706066608428955
Epoch 570, val loss: 0.944516658782959
Epoch 580, training loss: 0.019162407144904137 = 0.01246325671672821 + 0.001 * 6.699150085449219
Epoch 580, val loss: 0.9531627297401428
Epoch 590, training loss: 0.018431976437568665 = 0.011704700067639351 + 0.001 * 6.727276802062988
Epoch 590, val loss: 0.9616348743438721
Epoch 600, training loss: 0.017704986035823822 = 0.011012916453182697 + 0.001 * 6.6920695304870605
Epoch 600, val loss: 0.9699236750602722
Epoch 610, training loss: 0.017073072493076324 = 0.010380733758211136 + 0.001 * 6.692337989807129
Epoch 610, val loss: 0.9779865145683289
Epoch 620, training loss: 0.01648579351603985 = 0.009801099076867104 + 0.001 * 6.684694290161133
Epoch 620, val loss: 0.985842227935791
Epoch 630, training loss: 0.015946775674819946 = 0.009265728294849396 + 0.001 * 6.681047439575195
Epoch 630, val loss: 0.9935976266860962
Epoch 640, training loss: 0.015461225062608719 = 0.00877166073769331 + 0.001 * 6.6895647048950195
Epoch 640, val loss: 1.0012187957763672
Epoch 650, training loss: 0.01499783806502819 = 0.008316608145833015 + 0.001 * 6.681229114532471
Epoch 650, val loss: 1.0086010694503784
Epoch 660, training loss: 0.014573132619261742 = 0.007897649891674519 + 0.001 * 6.675481796264648
Epoch 660, val loss: 1.0158803462982178
Epoch 670, training loss: 0.014198558405041695 = 0.007510353811085224 + 0.001 * 6.688204765319824
Epoch 670, val loss: 1.0228654146194458
Epoch 680, training loss: 0.013823434710502625 = 0.007149131968617439 + 0.001 * 6.674302101135254
Epoch 680, val loss: 1.0297470092773438
Epoch 690, training loss: 0.01350090466439724 = 0.006809636484831572 + 0.001 * 6.691267967224121
Epoch 690, val loss: 1.0364793539047241
Epoch 700, training loss: 0.013154249638319016 = 0.006489686667919159 + 0.001 * 6.664562702178955
Epoch 700, val loss: 1.043165922164917
Epoch 710, training loss: 0.012851594015955925 = 0.006188179831951857 + 0.001 * 6.6634135246276855
Epoch 710, val loss: 1.049753189086914
Epoch 720, training loss: 0.012604080140590668 = 0.005904362536966801 + 0.001 * 6.699717044830322
Epoch 720, val loss: 1.0563522577285767
Epoch 730, training loss: 0.012315632775425911 = 0.005637787748128176 + 0.001 * 6.677844047546387
Epoch 730, val loss: 1.062746524810791
Epoch 740, training loss: 0.012048279866576195 = 0.005387584213167429 + 0.001 * 6.6606950759887695
Epoch 740, val loss: 1.0690686702728271
Epoch 750, training loss: 0.011812979355454445 = 0.005152907222509384 + 0.001 * 6.66007137298584
Epoch 750, val loss: 1.0752851963043213
Epoch 760, training loss: 0.011589733883738518 = 0.0049327053129673 + 0.001 * 6.657028675079346
Epoch 760, val loss: 1.0813573598861694
Epoch 770, training loss: 0.011388270184397697 = 0.004726170562207699 + 0.001 * 6.662099838256836
Epoch 770, val loss: 1.0873395204544067
Epoch 780, training loss: 0.011196031235158443 = 0.0045324950478971004 + 0.001 * 6.663536071777344
Epoch 780, val loss: 1.093196988105774
Epoch 790, training loss: 0.011002564802765846 = 0.004350756760686636 + 0.001 * 6.6518073081970215
Epoch 790, val loss: 1.098978042602539
Epoch 800, training loss: 0.010831659659743309 = 0.004180120769888163 + 0.001 * 6.651538848876953
Epoch 800, val loss: 1.1046056747436523
Epoch 810, training loss: 0.01067337766289711 = 0.004019848071038723 + 0.001 * 6.653529167175293
Epoch 810, val loss: 1.110162615776062
Epoch 820, training loss: 0.010525490157306194 = 0.003869198262691498 + 0.001 * 6.656291484832764
Epoch 820, val loss: 1.1155707836151123
Epoch 830, training loss: 0.010376508347690105 = 0.00372766493819654 + 0.001 * 6.648843288421631
Epoch 830, val loss: 1.1208996772766113
Epoch 840, training loss: 0.010244611650705338 = 0.00359466508962214 + 0.001 * 6.649946689605713
Epoch 840, val loss: 1.1260924339294434
Epoch 850, training loss: 0.010135719552636147 = 0.0034692527260631323 + 0.001 * 6.666466236114502
Epoch 850, val loss: 1.1311914920806885
Epoch 860, training loss: 0.010001475922763348 = 0.0033509230706840754 + 0.001 * 6.650552749633789
Epoch 860, val loss: 1.1361838579177856
Epoch 870, training loss: 0.009889200329780579 = 0.003239148063585162 + 0.001 * 6.650051593780518
Epoch 870, val loss: 1.1410915851593018
Epoch 880, training loss: 0.009783084504306316 = 0.0031334564555436373 + 0.001 * 6.649628162384033
Epoch 880, val loss: 1.145873785018921
Epoch 890, training loss: 0.009677710011601448 = 0.0030333639588207006 + 0.001 * 6.644345283508301
Epoch 890, val loss: 1.1505600214004517
Epoch 900, training loss: 0.00958402082324028 = 0.0029385085217654705 + 0.001 * 6.645512580871582
Epoch 900, val loss: 1.1551792621612549
Epoch 910, training loss: 0.009490606375038624 = 0.0028485471848398447 + 0.001 * 6.642058849334717
Epoch 910, val loss: 1.1596883535385132
Epoch 920, training loss: 0.009402012452483177 = 0.002763149794191122 + 0.001 * 6.638861656188965
Epoch 920, val loss: 1.1641303300857544
Epoch 930, training loss: 0.009319474920630455 = 0.0026820593047887087 + 0.001 * 6.637415409088135
Epoch 930, val loss: 1.1684683561325073
Epoch 940, training loss: 0.009249701164662838 = 0.0026050067972391844 + 0.001 * 6.644693851470947
Epoch 940, val loss: 1.1727312803268433
Epoch 950, training loss: 0.009169600903987885 = 0.00253172661177814 + 0.001 * 6.637874126434326
Epoch 950, val loss: 1.1769293546676636
Epoch 960, training loss: 0.00909547321498394 = 0.0024619773030281067 + 0.001 * 6.633495330810547
Epoch 960, val loss: 1.181031346321106
Epoch 970, training loss: 0.009026051498949528 = 0.002395536983385682 + 0.001 * 6.630514144897461
Epoch 970, val loss: 1.1850734949111938
Epoch 980, training loss: 0.008965960703790188 = 0.0023322294000536203 + 0.001 * 6.633730888366699
Epoch 980, val loss: 1.1890103816986084
Epoch 990, training loss: 0.008910160511732101 = 0.002271881327033043 + 0.001 * 6.638278961181641
Epoch 990, val loss: 1.1928781270980835
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7491
Flip ASR: 0.7156/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.937300205230713 = 1.9289264678955078 + 0.001 * 8.373771667480469
Epoch 0, val loss: 1.9318811893463135
Epoch 10, training loss: 1.9282320737838745 = 1.9198583364486694 + 0.001 * 8.373682975769043
Epoch 10, val loss: 1.9234908819198608
Epoch 20, training loss: 1.9173400402069092 = 1.9089666604995728 + 0.001 * 8.373357772827148
Epoch 20, val loss: 1.9131044149398804
Epoch 30, training loss: 1.9021594524383545 = 1.8937867879867554 + 0.001 * 8.372654914855957
Epoch 30, val loss: 1.8984460830688477
Epoch 40, training loss: 1.8797950744628906 = 1.8714239597320557 + 0.001 * 8.371111869812012
Epoch 40, val loss: 1.877192735671997
Epoch 50, training loss: 1.8479909896850586 = 1.839624047279358 + 0.001 * 8.366951942443848
Epoch 50, val loss: 1.8481180667877197
Epoch 60, training loss: 1.80775785446167 = 1.7994070053100586 + 0.001 * 8.350820541381836
Epoch 60, val loss: 1.8122899532318115
Epoch 70, training loss: 1.7623684406280518 = 1.7541029453277588 + 0.001 * 8.26548957824707
Epoch 70, val loss: 1.769437551498413
Epoch 80, training loss: 1.706738829612732 = 1.6989281177520752 + 0.001 * 7.810669422149658
Epoch 80, val loss: 1.7139735221862793
Epoch 90, training loss: 1.6325753927230835 = 1.6249743700027466 + 0.001 * 7.601057529449463
Epoch 90, val loss: 1.6460824012756348
Epoch 100, training loss: 1.5377389192581177 = 1.5302098989486694 + 0.001 * 7.528998851776123
Epoch 100, val loss: 1.5672365427017212
Epoch 110, training loss: 1.4292819499969482 = 1.4218803644180298 + 0.001 * 7.401569366455078
Epoch 110, val loss: 1.478515863418579
Epoch 120, training loss: 1.3194350004196167 = 1.312270998954773 + 0.001 * 7.164045810699463
Epoch 120, val loss: 1.389823317527771
Epoch 130, training loss: 1.216801643371582 = 1.2096866369247437 + 0.001 * 7.114958763122559
Epoch 130, val loss: 1.307202696800232
Epoch 140, training loss: 1.1219083070755005 = 1.114863395690918 + 0.001 * 7.04496955871582
Epoch 140, val loss: 1.2318705320358276
Epoch 150, training loss: 1.031146764755249 = 1.0241429805755615 + 0.001 * 7.003839492797852
Epoch 150, val loss: 1.159771203994751
Epoch 160, training loss: 0.9414834976196289 = 0.9345109462738037 + 0.001 * 6.972558498382568
Epoch 160, val loss: 1.089056134223938
Epoch 170, training loss: 0.8526501655578613 = 0.8456976413726807 + 0.001 * 6.952547073364258
Epoch 170, val loss: 1.019870400428772
Epoch 180, training loss: 0.7665047645568848 = 0.7595667839050293 + 0.001 * 6.937979221343994
Epoch 180, val loss: 0.9540658593177795
Epoch 190, training loss: 0.6856041550636292 = 0.6786779165267944 + 0.001 * 6.926225662231445
Epoch 190, val loss: 0.8940064907073975
Epoch 200, training loss: 0.611748456954956 = 0.6048330068588257 + 0.001 * 6.915433406829834
Epoch 200, val loss: 0.8420710563659668
Epoch 210, training loss: 0.5450832843780518 = 0.5381782650947571 + 0.001 * 6.905008792877197
Epoch 210, val loss: 0.7990241050720215
Epoch 220, training loss: 0.48440203070640564 = 0.47750821709632874 + 0.001 * 6.8938117027282715
Epoch 220, val loss: 0.7644068598747253
Epoch 230, training loss: 0.4279608726501465 = 0.4210790693759918 + 0.001 * 6.881810665130615
Epoch 230, val loss: 0.7368028163909912
Epoch 240, training loss: 0.37450501322746277 = 0.36763453483581543 + 0.001 * 6.870490074157715
Epoch 240, val loss: 0.714625358581543
Epoch 250, training loss: 0.32393473386764526 = 0.31707313656806946 + 0.001 * 6.861592769622803
Epoch 250, val loss: 0.6966725587844849
Epoch 260, training loss: 0.277302086353302 = 0.27044591307640076 + 0.001 * 6.856166839599609
Epoch 260, val loss: 0.6827652454376221
Epoch 270, training loss: 0.23592828214168549 = 0.22907450795173645 + 0.001 * 6.853776454925537
Epoch 270, val loss: 0.6730140447616577
Epoch 280, training loss: 0.2006072849035263 = 0.19375532865524292 + 0.001 * 6.851953983306885
Epoch 280, val loss: 0.6677723526954651
Epoch 290, training loss: 0.17131003737449646 = 0.16445954144001007 + 0.001 * 6.85049295425415
Epoch 290, val loss: 0.66716468334198
Epoch 300, training loss: 0.1474050134420395 = 0.14055544137954712 + 0.001 * 6.849573135375977
Epoch 300, val loss: 0.670841634273529
Epoch 310, training loss: 0.12796702980995178 = 0.12111838161945343 + 0.001 * 6.848654270172119
Epoch 310, val loss: 0.6781541705131531
Epoch 320, training loss: 0.11209011822938919 = 0.10524240881204605 + 0.001 * 6.847706317901611
Epoch 320, val loss: 0.6881796717643738
Epoch 330, training loss: 0.09902366995811462 = 0.09217707812786102 + 0.001 * 6.846588611602783
Epoch 330, val loss: 0.7004383206367493
Epoch 340, training loss: 0.08815845847129822 = 0.08131320774555206 + 0.001 * 6.845253944396973
Epoch 340, val loss: 0.7142922282218933
Epoch 350, training loss: 0.07902112603187561 = 0.07217709720134735 + 0.001 * 6.844029426574707
Epoch 350, val loss: 0.7293161749839783
Epoch 360, training loss: 0.0712582916021347 = 0.06441444158554077 + 0.001 * 6.843847274780273
Epoch 360, val loss: 0.7451683878898621
Epoch 370, training loss: 0.06459780037403107 = 0.057756874710321426 + 0.001 * 6.840926170349121
Epoch 370, val loss: 0.7615074515342712
Epoch 380, training loss: 0.05883995443582535 = 0.05200115963816643 + 0.001 * 6.838794231414795
Epoch 380, val loss: 0.7781100273132324
Epoch 390, training loss: 0.053827665746212006 = 0.046991296112537384 + 0.001 * 6.836369037628174
Epoch 390, val loss: 0.794805645942688
Epoch 400, training loss: 0.04944340139627457 = 0.04260663688182831 + 0.001 * 6.836765289306641
Epoch 400, val loss: 0.811461329460144
Epoch 410, training loss: 0.04558350890874863 = 0.03875197842717171 + 0.001 * 6.831528186798096
Epoch 410, val loss: 0.8279436826705933
Epoch 420, training loss: 0.04218057915568352 = 0.035350579768419266 + 0.001 * 6.829997539520264
Epoch 420, val loss: 0.8441994190216064
Epoch 430, training loss: 0.03916458040475845 = 0.032340019941329956 + 0.001 * 6.824558734893799
Epoch 430, val loss: 0.8601669073104858
Epoch 440, training loss: 0.036488477140665054 = 0.029668252915143967 + 0.001 * 6.820222854614258
Epoch 440, val loss: 0.8757700324058533
Epoch 450, training loss: 0.034109875559806824 = 0.027291512116789818 + 0.001 * 6.818361759185791
Epoch 450, val loss: 0.8910250663757324
Epoch 460, training loss: 0.0319853276014328 = 0.025171324610710144 + 0.001 * 6.8140034675598145
Epoch 460, val loss: 0.9058927893638611
Epoch 470, training loss: 0.03008587844669819 = 0.023275407031178474 + 0.001 * 6.8104705810546875
Epoch 470, val loss: 0.9203866124153137
Epoch 480, training loss: 0.028381172567605972 = 0.021575868129730225 + 0.001 * 6.80530309677124
Epoch 480, val loss: 0.9344815611839294
Epoch 490, training loss: 0.02684604376554489 = 0.020048746839165688 + 0.001 * 6.797295570373535
Epoch 490, val loss: 0.9481940269470215
Epoch 500, training loss: 0.025464991107583046 = 0.018673211336135864 + 0.001 * 6.791779041290283
Epoch 500, val loss: 0.9615415334701538
Epoch 510, training loss: 0.024221163243055344 = 0.017431136220693588 + 0.001 * 6.790026664733887
Epoch 510, val loss: 0.9745323061943054
Epoch 520, training loss: 0.0230929683893919 = 0.016306862235069275 + 0.001 * 6.786105632781982
Epoch 520, val loss: 0.9871695041656494
Epoch 530, training loss: 0.022068407386541367 = 0.015286672860383987 + 0.001 * 6.781734943389893
Epoch 530, val loss: 0.9994661808013916
Epoch 540, training loss: 0.021138209849596024 = 0.014358548447489738 + 0.001 * 6.779661655426025
Epoch 540, val loss: 1.0114359855651855
Epoch 550, training loss: 0.020288720726966858 = 0.013512281700968742 + 0.001 * 6.776438236236572
Epoch 550, val loss: 1.0231094360351562
Epoch 560, training loss: 0.01950586773455143 = 0.012738822028040886 + 0.001 * 6.767045497894287
Epoch 560, val loss: 1.0344942808151245
Epoch 570, training loss: 0.01879468560218811 = 0.012030372396111488 + 0.001 * 6.764311790466309
Epoch 570, val loss: 1.0455888509750366
Epoch 580, training loss: 0.018141970038414 = 0.011380047537386417 + 0.001 * 6.761923313140869
Epoch 580, val loss: 1.0564022064208984
Epoch 590, training loss: 0.017542066052556038 = 0.010781863704323769 + 0.001 * 6.760202407836914
Epoch 590, val loss: 1.066949725151062
Epoch 600, training loss: 0.016992568969726562 = 0.010230508632957935 + 0.001 * 6.762060642242432
Epoch 600, val loss: 1.0772457122802734
Epoch 610, training loss: 0.016482805833220482 = 0.009721358306705952 + 0.001 * 6.761447429656982
Epoch 610, val loss: 1.0873090028762817
Epoch 620, training loss: 0.016015443950891495 = 0.009250259026885033 + 0.001 * 6.765183925628662
Epoch 620, val loss: 1.0971237421035767
Epoch 630, training loss: 0.015574470162391663 = 0.008813628926873207 + 0.001 * 6.760840892791748
Epoch 630, val loss: 1.106727957725525
Epoch 640, training loss: 0.015165746212005615 = 0.00840824656188488 + 0.001 * 6.757498741149902
Epoch 640, val loss: 1.116102933883667
Epoch 650, training loss: 0.014783820137381554 = 0.008031259290874004 + 0.001 * 6.752560615539551
Epoch 650, val loss: 1.1252657175064087
Epoch 660, training loss: 0.014429769478738308 = 0.00768016092479229 + 0.001 * 6.749608039855957
Epoch 660, val loss: 1.13424551486969
Epoch 670, training loss: 0.014102735556662083 = 0.007352680433541536 + 0.001 * 6.750054836273193
Epoch 670, val loss: 1.1430180072784424
Epoch 680, training loss: 0.01379532553255558 = 0.007046760991215706 + 0.001 * 6.748563766479492
Epoch 680, val loss: 1.1516085863113403
Epoch 690, training loss: 0.013508647680282593 = 0.0067606596276164055 + 0.001 * 6.747988224029541
Epoch 690, val loss: 1.1600189208984375
Epoch 700, training loss: 0.013242032378911972 = 0.0064926459454 + 0.001 * 6.749385833740234
Epoch 700, val loss: 1.1682575941085815
Epoch 710, training loss: 0.012983644381165504 = 0.006241226103156805 + 0.001 * 6.742417812347412
Epoch 710, val loss: 1.1763170957565308
Epoch 720, training loss: 0.01277790404856205 = 0.006005088333040476 + 0.001 * 6.772814750671387
Epoch 720, val loss: 1.1842148303985596
Epoch 730, training loss: 0.0125346090644598 = 0.005783038679510355 + 0.001 * 6.751569747924805
Epoch 730, val loss: 1.1919504404067993
Epoch 740, training loss: 0.012313476763665676 = 0.00557398796081543 + 0.001 * 6.73948860168457
Epoch 740, val loss: 1.1995306015014648
Epoch 750, training loss: 0.012112462893128395 = 0.005376957822591066 + 0.001 * 6.735504627227783
Epoch 750, val loss: 1.206978440284729
Epoch 760, training loss: 0.011937994509935379 = 0.005191020201891661 + 0.001 * 6.746974468231201
Epoch 760, val loss: 1.2142612934112549
Epoch 770, training loss: 0.011756965890526772 = 0.005015355069190264 + 0.001 * 6.741610050201416
Epoch 770, val loss: 1.221420168876648
Epoch 780, training loss: 0.011581867933273315 = 0.004849104676395655 + 0.001 * 6.732762813568115
Epoch 780, val loss: 1.2284759283065796
Epoch 790, training loss: 0.011424874886870384 = 0.004691477864980698 + 0.001 * 6.733396053314209
Epoch 790, val loss: 1.2354058027267456
Epoch 800, training loss: 0.011274958029389381 = 0.004541910253465176 + 0.001 * 6.733047962188721
Epoch 800, val loss: 1.2422109842300415
Epoch 810, training loss: 0.011133231222629547 = 0.004399759694933891 + 0.001 * 6.733471870422363
Epoch 810, val loss: 1.248937726020813
Epoch 820, training loss: 0.011004939675331116 = 0.004264445044100285 + 0.001 * 6.7404937744140625
Epoch 820, val loss: 1.2555688619613647
Epoch 830, training loss: 0.010861548595130444 = 0.0041353036649525166 + 0.001 * 6.7262444496154785
Epoch 830, val loss: 1.2621146440505981
Epoch 840, training loss: 0.01075996458530426 = 0.004011735785752535 + 0.001 * 6.748228549957275
Epoch 840, val loss: 1.268611192703247
Epoch 850, training loss: 0.010611950419843197 = 0.0038931260351091623 + 0.001 * 6.7188239097595215
Epoch 850, val loss: 1.2750611305236816
Epoch 860, training loss: 0.010518727824091911 = 0.0037784974556416273 + 0.001 * 6.740229606628418
Epoch 860, val loss: 1.2815227508544922
Epoch 870, training loss: 0.010383827611804008 = 0.0036668642424046993 + 0.001 * 6.716962814331055
Epoch 870, val loss: 1.2880163192749023
Epoch 880, training loss: 0.010280206799507141 = 0.0035576187074184418 + 0.001 * 6.7225871086120605
Epoch 880, val loss: 1.2946338653564453
Epoch 890, training loss: 0.010165326297283173 = 0.0034507170785218477 + 0.001 * 6.714609146118164
Epoch 890, val loss: 1.3013631105422974
Epoch 900, training loss: 0.01006015669554472 = 0.0033463432919234037 + 0.001 * 6.713813304901123
Epoch 900, val loss: 1.3081790208816528
Epoch 910, training loss: 0.009974819608032703 = 0.003244752297177911 + 0.001 * 6.730066776275635
Epoch 910, val loss: 1.3150800466537476
Epoch 920, training loss: 0.009865708649158478 = 0.003146236762404442 + 0.001 * 6.719470977783203
Epoch 920, val loss: 1.3220378160476685
Epoch 930, training loss: 0.009769991971552372 = 0.0030510181095451117 + 0.001 * 6.718973636627197
Epoch 930, val loss: 1.3290047645568848
Epoch 940, training loss: 0.009668763726949692 = 0.00295918807387352 + 0.001 * 6.709575653076172
Epoch 940, val loss: 1.3359917402267456
Epoch 950, training loss: 0.00958920642733574 = 0.00287075387313962 + 0.001 * 6.718452453613281
Epoch 950, val loss: 1.3429816961288452
Epoch 960, training loss: 0.009489879012107849 = 0.002785810735076666 + 0.001 * 6.704068183898926
Epoch 960, val loss: 1.3499155044555664
Epoch 970, training loss: 0.009406300261616707 = 0.002704296261072159 + 0.001 * 6.702003002166748
Epoch 970, val loss: 1.3568246364593506
Epoch 980, training loss: 0.009336483664810658 = 0.0026261548046022654 + 0.001 * 6.710328102111816
Epoch 980, val loss: 1.3636910915374756
Epoch 990, training loss: 0.009261712431907654 = 0.0025512713473290205 + 0.001 * 6.710440635681152
Epoch 990, val loss: 1.370476245880127
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9520
Flip ASR: 0.9422/225 nodes
The final ASR:0.79828, 0.11104, Accuracy:0.83704, 0.01090
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9386])
updated graph: torch.Size([2, 10426])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9641926288604736 = 1.955818772315979 + 0.001 * 8.373866081237793
Epoch 0, val loss: 1.9608721733093262
Epoch 10, training loss: 1.95400071144104 = 1.9456268548965454 + 0.001 * 8.373801231384277
Epoch 10, val loss: 1.9509812593460083
Epoch 20, training loss: 1.9414517879486084 = 1.9330781698226929 + 0.001 * 8.373601913452148
Epoch 20, val loss: 1.9383970499038696
Epoch 30, training loss: 1.9238121509552002 = 1.9154390096664429 + 0.001 * 8.37318229675293
Epoch 30, val loss: 1.9204590320587158
Epoch 40, training loss: 1.8976386785507202 = 1.8892663717269897 + 0.001 * 8.372288703918457
Epoch 40, val loss: 1.8940277099609375
Epoch 50, training loss: 1.8607391119003296 = 1.8523690700531006 + 0.001 * 8.370043754577637
Epoch 50, val loss: 1.8583312034606934
Epoch 60, training loss: 1.8177944421768188 = 1.809432029724121 + 0.001 * 8.362418174743652
Epoch 60, val loss: 1.8201645612716675
Epoch 70, training loss: 1.7792638540267944 = 1.7709389925003052 + 0.001 * 8.324881553649902
Epoch 70, val loss: 1.7879292964935303
Epoch 80, training loss: 1.7344985008239746 = 1.7264173030853271 + 0.001 * 8.081214904785156
Epoch 80, val loss: 1.7486974000930786
Epoch 90, training loss: 1.671690583229065 = 1.663893222808838 + 0.001 * 7.797393798828125
Epoch 90, val loss: 1.6960378885269165
Epoch 100, training loss: 1.5869311094284058 = 1.5793287754058838 + 0.001 * 7.602313995361328
Epoch 100, val loss: 1.626671552658081
Epoch 110, training loss: 1.4858216047286987 = 1.4784555435180664 + 0.001 * 7.36602258682251
Epoch 110, val loss: 1.5452171564102173
Epoch 120, training loss: 1.380267858505249 = 1.3730065822601318 + 0.001 * 7.261268615722656
Epoch 120, val loss: 1.4593349695205688
Epoch 130, training loss: 1.2776756286621094 = 1.2705192565917969 + 0.001 * 7.156368732452393
Epoch 130, val loss: 1.3780587911605835
Epoch 140, training loss: 1.180473804473877 = 1.1733914613723755 + 0.001 * 7.082290172576904
Epoch 140, val loss: 1.302934169769287
Epoch 150, training loss: 1.0905733108520508 = 1.0835238695144653 + 0.001 * 7.0494704246521
Epoch 150, val loss: 1.2355501651763916
Epoch 160, training loss: 1.0091444253921509 = 1.0021122694015503 + 0.001 * 7.032206058502197
Epoch 160, val loss: 1.1762629747390747
Epoch 170, training loss: 0.9349730014801025 = 0.9279592633247375 + 0.001 * 7.013767242431641
Epoch 170, val loss: 1.1229591369628906
Epoch 180, training loss: 0.8652462363243103 = 0.858252227306366 + 0.001 * 6.993997097015381
Epoch 180, val loss: 1.073024034500122
Epoch 190, training loss: 0.7976114749908447 = 0.7906315922737122 + 0.001 * 6.979867935180664
Epoch 190, val loss: 1.0243254899978638
Epoch 200, training loss: 0.7318695187568665 = 0.7249007821083069 + 0.001 * 6.968737602233887
Epoch 200, val loss: 0.9768563508987427
Epoch 210, training loss: 0.6693642139434814 = 0.6624007225036621 + 0.001 * 6.963473796844482
Epoch 210, val loss: 0.932510256767273
Epoch 220, training loss: 0.6112286448478699 = 0.604268491268158 + 0.001 * 6.960137367248535
Epoch 220, val loss: 0.89261794090271
Epoch 230, training loss: 0.557671844959259 = 0.5507166385650635 + 0.001 * 6.955233097076416
Epoch 230, val loss: 0.8578583598136902
Epoch 240, training loss: 0.5083380937576294 = 0.5013852119445801 + 0.001 * 6.952888488769531
Epoch 240, val loss: 0.828595757484436
Epoch 250, training loss: 0.4627852439880371 = 0.45583516359329224 + 0.001 * 6.950070858001709
Epoch 250, val loss: 0.8045905828475952
Epoch 260, training loss: 0.4207063615322113 = 0.41375917196273804 + 0.001 * 6.947182655334473
Epoch 260, val loss: 0.7861453294754028
Epoch 270, training loss: 0.38174015283584595 = 0.37479737401008606 + 0.001 * 6.942781925201416
Epoch 270, val loss: 0.7730087041854858
Epoch 280, training loss: 0.3451356291770935 = 0.3381975293159485 + 0.001 * 6.938113689422607
Epoch 280, val loss: 0.764743447303772
Epoch 290, training loss: 0.31006237864494324 = 0.30313122272491455 + 0.001 * 6.931157112121582
Epoch 290, val loss: 0.7603310346603394
Epoch 300, training loss: 0.27605944871902466 = 0.2691296339035034 + 0.001 * 6.929816722869873
Epoch 300, val loss: 0.7590962648391724
Epoch 310, training loss: 0.24350838363170624 = 0.23658886551856995 + 0.001 * 6.9195122718811035
Epoch 310, val loss: 0.7610929608345032
Epoch 320, training loss: 0.21345122158527374 = 0.20653842389583588 + 0.001 * 6.912796974182129
Epoch 320, val loss: 0.7665975689888
Epoch 330, training loss: 0.18691717088222504 = 0.18001028895378113 + 0.001 * 6.906876087188721
Epoch 330, val loss: 0.7758215665817261
Epoch 340, training loss: 0.16428416967391968 = 0.15738624334335327 + 0.001 * 6.897927284240723
Epoch 340, val loss: 0.788601279258728
Epoch 350, training loss: 0.14522311091423035 = 0.13833212852478027 + 0.001 * 6.890985488891602
Epoch 350, val loss: 0.8043324947357178
Epoch 360, training loss: 0.12909381091594696 = 0.12220875918865204 + 0.001 * 6.885044574737549
Epoch 360, val loss: 0.8223385214805603
Epoch 370, training loss: 0.11530821770429611 = 0.10842360556125641 + 0.001 * 6.884609699249268
Epoch 370, val loss: 0.8420014977455139
Epoch 380, training loss: 0.10340070724487305 = 0.09652251750230789 + 0.001 * 6.878187656402588
Epoch 380, val loss: 0.8627718687057495
Epoch 390, training loss: 0.09304744750261307 = 0.08617227524518967 + 0.001 * 6.875171184539795
Epoch 390, val loss: 0.8842164278030396
Epoch 400, training loss: 0.08399026840925217 = 0.07711613923311234 + 0.001 * 6.874129295349121
Epoch 400, val loss: 0.9059216380119324
Epoch 410, training loss: 0.0760294646024704 = 0.06915942579507828 + 0.001 * 6.870035648345947
Epoch 410, val loss: 0.927568256855011
Epoch 420, training loss: 0.06900983303785324 = 0.06214214488863945 + 0.001 * 6.867686748504639
Epoch 420, val loss: 0.9489449858665466
Epoch 430, training loss: 0.06280577182769775 = 0.05593860521912575 + 0.001 * 6.867166519165039
Epoch 430, val loss: 0.9699518084526062
Epoch 440, training loss: 0.05731118470430374 = 0.05044296383857727 + 0.001 * 6.868218898773193
Epoch 440, val loss: 0.9905144572257996
Epoch 450, training loss: 0.05243189260363579 = 0.045564837753772736 + 0.001 * 6.867053508758545
Epoch 450, val loss: 1.0107076168060303
Epoch 460, training loss: 0.048091042786836624 = 0.04122535511851311 + 0.001 * 6.865688323974609
Epoch 460, val loss: 1.0303807258605957
Epoch 470, training loss: 0.04422362893819809 = 0.03735867515206337 + 0.001 * 6.864954948425293
Epoch 470, val loss: 1.049521565437317
Epoch 480, training loss: 0.0407785028219223 = 0.0339122973382473 + 0.001 * 6.86620569229126
Epoch 480, val loss: 1.0680804252624512
Epoch 490, training loss: 0.037709638476371765 = 0.030842972919344902 + 0.001 * 6.866666316986084
Epoch 490, val loss: 1.0860737562179565
Epoch 500, training loss: 0.03497357666492462 = 0.028110874816775322 + 0.001 * 6.862700462341309
Epoch 500, val loss: 1.1034971475601196
Epoch 510, training loss: 0.0325409397482872 = 0.025679338723421097 + 0.001 * 6.861602783203125
Epoch 510, val loss: 1.1203582286834717
Epoch 520, training loss: 0.03037586435675621 = 0.023515067994594574 + 0.001 * 6.860795497894287
Epoch 520, val loss: 1.136728286743164
Epoch 530, training loss: 0.028456594794988632 = 0.021587686613202095 + 0.001 * 6.868908882141113
Epoch 530, val loss: 1.152464747428894
Epoch 540, training loss: 0.02672836370766163 = 0.019869364798069 + 0.001 * 6.858998775482178
Epoch 540, val loss: 1.1676186323165894
Epoch 550, training loss: 0.02519107237458229 = 0.01833501085639 + 0.001 * 6.8560614585876465
Epoch 550, val loss: 1.182352900505066
Epoch 560, training loss: 0.023823704570531845 = 0.016962479799985886 + 0.001 * 6.861225128173828
Epoch 560, val loss: 1.1965285539627075
Epoch 570, training loss: 0.02258823812007904 = 0.015731941908597946 + 0.001 * 6.856295108795166
Epoch 570, val loss: 1.2101943492889404
Epoch 580, training loss: 0.021479185670614243 = 0.014626138843595982 + 0.001 * 6.8530473709106445
Epoch 580, val loss: 1.223393201828003
Epoch 590, training loss: 0.02048368752002716 = 0.013630026951432228 + 0.001 * 6.853659629821777
Epoch 590, val loss: 1.2361063957214355
Epoch 600, training loss: 0.019588138908147812 = 0.012730645947158337 + 0.001 * 6.857491493225098
Epoch 600, val loss: 1.2484009265899658
Epoch 610, training loss: 0.018767867237329483 = 0.011916641145944595 + 0.001 * 6.85122537612915
Epoch 610, val loss: 1.2602381706237793
Epoch 620, training loss: 0.01802813448011875 = 0.011177944019436836 + 0.001 * 6.85019063949585
Epoch 620, val loss: 1.2716898918151855
Epoch 630, training loss: 0.017353348433971405 = 0.010506127960979939 + 0.001 * 6.8472208976745605
Epoch 630, val loss: 1.2826465368270874
Epoch 640, training loss: 0.01674254611134529 = 0.009893548674881458 + 0.001 * 6.848996162414551
Epoch 640, val loss: 1.2933332920074463
Epoch 650, training loss: 0.016173291951417923 = 0.009333809837698936 + 0.001 * 6.839482307434082
Epoch 650, val loss: 1.303640365600586
Epoch 660, training loss: 0.015667185187339783 = 0.008821263909339905 + 0.001 * 6.845921993255615
Epoch 660, val loss: 1.3136796951293945
Epoch 670, training loss: 0.015186571516096592 = 0.008350891061127186 + 0.001 * 6.83568000793457
Epoch 670, val loss: 1.3232605457305908
Epoch 680, training loss: 0.0147897619754076 = 0.007918305695056915 + 0.001 * 6.871455669403076
Epoch 680, val loss: 1.3326282501220703
Epoch 690, training loss: 0.014355039224028587 = 0.0075196013785898685 + 0.001 * 6.835437297821045
Epoch 690, val loss: 1.3416249752044678
Epoch 700, training loss: 0.013981230556964874 = 0.007151435595005751 + 0.001 * 6.829794883728027
Epoch 700, val loss: 1.3503960371017456
Epoch 710, training loss: 0.013669963926076889 = 0.006810811813920736 + 0.001 * 6.859151363372803
Epoch 710, val loss: 1.3588672876358032
Epoch 720, training loss: 0.01332305558025837 = 0.006495146546512842 + 0.001 * 6.827908992767334
Epoch 720, val loss: 1.3670998811721802
Epoch 730, training loss: 0.01303466223180294 = 0.006202092859894037 + 0.001 * 6.832569599151611
Epoch 730, val loss: 1.3750840425491333
Epoch 740, training loss: 0.01277775876224041 = 0.005929551087319851 + 0.001 * 6.848207473754883
Epoch 740, val loss: 1.3828151226043701
Epoch 750, training loss: 0.01250828243792057 = 0.005675697233527899 + 0.001 * 6.832584381103516
Epoch 750, val loss: 1.390343427658081
Epoch 760, training loss: 0.012256467714905739 = 0.005438810680061579 + 0.001 * 6.817656993865967
Epoch 760, val loss: 1.397648572921753
Epoch 770, training loss: 0.012045699171721935 = 0.005217490252107382 + 0.001 * 6.8282084465026855
Epoch 770, val loss: 1.4047236442565918
Epoch 780, training loss: 0.011846872046589851 = 0.0050103976391255856 + 0.001 * 6.8364739418029785
Epoch 780, val loss: 1.411635398864746
Epoch 790, training loss: 0.011624962091445923 = 0.004816374741494656 + 0.001 * 6.808587074279785
Epoch 790, val loss: 1.4183294773101807
Epoch 800, training loss: 0.011444507166743279 = 0.004634328652173281 + 0.001 * 6.810178279876709
Epoch 800, val loss: 1.4248607158660889
Epoch 810, training loss: 0.011296132579445839 = 0.004463297314941883 + 0.001 * 6.832834243774414
Epoch 810, val loss: 1.4311950206756592
Epoch 820, training loss: 0.011108221486210823 = 0.004302412271499634 + 0.001 * 6.8058085441589355
Epoch 820, val loss: 1.4373518228530884
Epoch 830, training loss: 0.011005114763975143 = 0.004150925669819117 + 0.001 * 6.854188919067383
Epoch 830, val loss: 1.443359375
Epoch 840, training loss: 0.010822908952832222 = 0.004008093383163214 + 0.001 * 6.814814567565918
Epoch 840, val loss: 1.449188470840454
Epoch 850, training loss: 0.010695604607462883 = 0.003873281879350543 + 0.001 * 6.822322845458984
Epoch 850, val loss: 1.4549171924591064
Epoch 860, training loss: 0.010567811317741871 = 0.0037459260784089565 + 0.001 * 6.821885108947754
Epoch 860, val loss: 1.4604159593582153
Epoch 870, training loss: 0.010434483177959919 = 0.0036254823207855225 + 0.001 * 6.809000492095947
Epoch 870, val loss: 1.465826153755188
Epoch 880, training loss: 0.010304226540029049 = 0.0035114630591124296 + 0.001 * 6.7927632331848145
Epoch 880, val loss: 1.4710744619369507
Epoch 890, training loss: 0.010207615792751312 = 0.003403427777811885 + 0.001 * 6.804187297821045
Epoch 890, val loss: 1.4761892557144165
Epoch 900, training loss: 0.010117617435753345 = 0.0033009785693138838 + 0.001 * 6.816638469696045
Epoch 900, val loss: 1.4811830520629883
Epoch 910, training loss: 0.010000335983932018 = 0.003203692613169551 + 0.001 * 6.796642780303955
Epoch 910, val loss: 1.4860599040985107
Epoch 920, training loss: 0.009911244735121727 = 0.0031112940050661564 + 0.001 * 6.799950122833252
Epoch 920, val loss: 1.4908080101013184
Epoch 930, training loss: 0.009827852249145508 = 0.003023429773747921 + 0.001 * 6.804422378540039
Epoch 930, val loss: 1.4955371618270874
Epoch 940, training loss: 0.009741315618157387 = 0.002939813770353794 + 0.001 * 6.801501750946045
Epoch 940, val loss: 1.500067114830017
Epoch 950, training loss: 0.009642010554671288 = 0.0028601756785064936 + 0.001 * 6.781834602355957
Epoch 950, val loss: 1.5044684410095215
Epoch 960, training loss: 0.009565090760588646 = 0.002784274285659194 + 0.001 * 6.780816555023193
Epoch 960, val loss: 1.5087803602218628
Epoch 970, training loss: 0.009503157809376717 = 0.0027118788566440344 + 0.001 * 6.791278839111328
Epoch 970, val loss: 1.5129921436309814
Epoch 980, training loss: 0.009435395710170269 = 0.0026427677366882563 + 0.001 * 6.792627334594727
Epoch 980, val loss: 1.5171284675598145
Epoch 990, training loss: 0.009367968887090683 = 0.002576779341325164 + 0.001 * 6.791189193725586
Epoch 990, val loss: 1.52115797996521
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7704
Overall ASR: 0.6900
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9416407346725464 = 1.9332668781280518 + 0.001 * 8.373860359191895
Epoch 0, val loss: 1.9322819709777832
Epoch 10, training loss: 1.931996464729309 = 1.9236226081848145 + 0.001 * 8.373805046081543
Epoch 10, val loss: 1.922494888305664
Epoch 20, training loss: 1.920246958732605 = 1.9118733406066895 + 0.001 * 8.373616218566895
Epoch 20, val loss: 1.9101510047912598
Epoch 30, training loss: 1.903652310371399 = 1.895279049873352 + 0.001 * 8.373251914978027
Epoch 30, val loss: 1.8923170566558838
Epoch 40, training loss: 1.8792728185653687 = 1.870900273323059 + 0.001 * 8.372550964355469
Epoch 40, val loss: 1.8660924434661865
Epoch 50, training loss: 1.8451738357543945 = 1.8368028402328491 + 0.001 * 8.370954513549805
Epoch 50, val loss: 1.83051335811615
Epoch 60, training loss: 1.8041850328445435 = 1.7958192825317383 + 0.001 * 8.365705490112305
Epoch 60, val loss: 1.7911367416381836
Epoch 70, training loss: 1.7611136436462402 = 1.7527745962142944 + 0.001 * 8.33902645111084
Epoch 70, val loss: 1.7541447877883911
Epoch 80, training loss: 1.7055442333221436 = 1.6973885297775269 + 0.001 * 8.155691146850586
Epoch 80, val loss: 1.7077182531356812
Epoch 90, training loss: 1.628205418586731 = 1.6204636096954346 + 0.001 * 7.741835117340088
Epoch 90, val loss: 1.6421911716461182
Epoch 100, training loss: 1.5296508073806763 = 1.5221478939056396 + 0.001 * 7.502878665924072
Epoch 100, val loss: 1.558157205581665
Epoch 110, training loss: 1.4214423894882202 = 1.4142056703567505 + 0.001 * 7.236711025238037
Epoch 110, val loss: 1.4704294204711914
Epoch 120, training loss: 1.3139359951019287 = 1.3068046569824219 + 0.001 * 7.131353855133057
Epoch 120, val loss: 1.388858675956726
Epoch 130, training loss: 1.212320327758789 = 1.205226182937622 + 0.001 * 7.09409236907959
Epoch 130, val loss: 1.31669282913208
Epoch 140, training loss: 1.1188404560089111 = 1.1117686033248901 + 0.001 * 7.071857929229736
Epoch 140, val loss: 1.2525360584259033
Epoch 150, training loss: 1.035042405128479 = 1.0279897451400757 + 0.001 * 7.052630424499512
Epoch 150, val loss: 1.1956003904342651
Epoch 160, training loss: 0.9603307843208313 = 0.9532972574234009 + 0.001 * 7.0335001945495605
Epoch 160, val loss: 1.1454685926437378
Epoch 170, training loss: 0.8916637897491455 = 0.8846526741981506 + 0.001 * 7.011116027832031
Epoch 170, val loss: 1.1000124216079712
Epoch 180, training loss: 0.8254076242446899 = 0.8184207677841187 + 0.001 * 6.9868292808532715
Epoch 180, val loss: 1.0559076070785522
Epoch 190, training loss: 0.7595875263214111 = 0.7526237964630127 + 0.001 * 6.963711738586426
Epoch 190, val loss: 1.0112316608428955
Epoch 200, training loss: 0.694543719291687 = 0.6875969767570496 + 0.001 * 6.946722984313965
Epoch 200, val loss: 0.9662171602249146
Epoch 210, training loss: 0.6317376494407654 = 0.6247997283935547 + 0.001 * 6.937936782836914
Epoch 210, val loss: 0.9225555658340454
Epoch 220, training loss: 0.5721135139465332 = 0.5651788115501404 + 0.001 * 6.934686183929443
Epoch 220, val loss: 0.8819489479064941
Epoch 230, training loss: 0.5157527923583984 = 0.5088202357292175 + 0.001 * 6.932544708251953
Epoch 230, val loss: 0.8455031514167786
Epoch 240, training loss: 0.46278712153434753 = 0.4558555483818054 + 0.001 * 6.931574821472168
Epoch 240, val loss: 0.8142083883285522
Epoch 250, training loss: 0.4133254885673523 = 0.4063945710659027 + 0.001 * 6.930907249450684
Epoch 250, val loss: 0.7880605459213257
Epoch 260, training loss: 0.3674887716770172 = 0.3605584502220154 + 0.001 * 6.930325031280518
Epoch 260, val loss: 0.7668909430503845
Epoch 270, training loss: 0.32545292377471924 = 0.3185231387615204 + 0.001 * 6.929794788360596
Epoch 270, val loss: 0.7502861022949219
Epoch 280, training loss: 0.28719305992126465 = 0.28026390075683594 + 0.001 * 6.9291672706604
Epoch 280, val loss: 0.7377620935440063
Epoch 290, training loss: 0.2526181638240814 = 0.24568983912467957 + 0.001 * 6.928328037261963
Epoch 290, val loss: 0.7287055850028992
Epoch 300, training loss: 0.22161957621574402 = 0.21469250321388245 + 0.001 * 6.927065372467041
Epoch 300, val loss: 0.7227693200111389
Epoch 310, training loss: 0.194090336561203 = 0.18716369569301605 + 0.001 * 6.926641941070557
Epoch 310, val loss: 0.7194535136222839
Epoch 320, training loss: 0.16990165412425995 = 0.16297847032546997 + 0.001 * 6.923187255859375
Epoch 320, val loss: 0.7186489701271057
Epoch 330, training loss: 0.14885056018829346 = 0.14193017780780792 + 0.001 * 6.920384407043457
Epoch 330, val loss: 0.7201066017150879
Epoch 340, training loss: 0.13067807257175446 = 0.12376159429550171 + 0.001 * 6.916472434997559
Epoch 340, val loss: 0.7234081029891968
Epoch 350, training loss: 0.11508555710315704 = 0.10817208141088486 + 0.001 * 6.913476467132568
Epoch 350, val loss: 0.728269636631012
Epoch 360, training loss: 0.10175470262765884 = 0.09484916180372238 + 0.001 * 6.905540466308594
Epoch 360, val loss: 0.7343300580978394
Epoch 370, training loss: 0.09039393812417984 = 0.08349482715129852 + 0.001 * 6.899110794067383
Epoch 370, val loss: 0.7413954138755798
Epoch 380, training loss: 0.08072001487016678 = 0.07382399588823318 + 0.001 * 6.896021366119385
Epoch 380, val loss: 0.7492468953132629
Epoch 390, training loss: 0.07245395332574844 = 0.06557175517082214 + 0.001 * 6.882194995880127
Epoch 390, val loss: 0.7576481699943542
Epoch 400, training loss: 0.0654393881559372 = 0.05851154029369354 + 0.001 * 6.927847862243652
Epoch 400, val loss: 0.7665248513221741
Epoch 410, training loss: 0.05931632220745087 = 0.0524476058781147 + 0.001 * 6.868714809417725
Epoch 410, val loss: 0.775728166103363
Epoch 420, training loss: 0.05407852306962013 = 0.047210488468408585 + 0.001 * 6.868033409118652
Epoch 420, val loss: 0.7851119637489319
Epoch 430, training loss: 0.049518510699272156 = 0.042662423104047775 + 0.001 * 6.856086730957031
Epoch 430, val loss: 0.794619619846344
Epoch 440, training loss: 0.04554382711648941 = 0.03869643062353134 + 0.001 * 6.847396373748779
Epoch 440, val loss: 0.8041171431541443
Epoch 450, training loss: 0.04211629182100296 = 0.035223640501499176 + 0.001 * 6.892649173736572
Epoch 450, val loss: 0.8136456608772278
Epoch 460, training loss: 0.03902710974216461 = 0.03217262774705887 + 0.001 * 6.854482650756836
Epoch 460, val loss: 0.8231031894683838
Epoch 470, training loss: 0.036326393485069275 = 0.029483316466212273 + 0.001 * 6.843074798583984
Epoch 470, val loss: 0.8325059413909912
Epoch 480, training loss: 0.033936627209186554 = 0.027102578431367874 + 0.001 * 6.834048748016357
Epoch 480, val loss: 0.8418665528297424
Epoch 490, training loss: 0.03181401640176773 = 0.024985022842884064 + 0.001 * 6.828991889953613
Epoch 490, val loss: 0.8510504961013794
Epoch 500, training loss: 0.029921460896730423 = 0.023095693439245224 + 0.001 * 6.825766563415527
Epoch 500, val loss: 0.8601090312004089
Epoch 510, training loss: 0.028234247118234634 = 0.021404722705483437 + 0.001 * 6.82952356338501
Epoch 510, val loss: 0.8690267205238342
Epoch 520, training loss: 0.026707395911216736 = 0.01988658867776394 + 0.001 * 6.820807933807373
Epoch 520, val loss: 0.877741813659668
Epoch 530, training loss: 0.025357766076922417 = 0.018519310280680656 + 0.001 * 6.8384552001953125
Epoch 530, val loss: 0.8863444328308105
Epoch 540, training loss: 0.024106236174702644 = 0.017283499240875244 + 0.001 * 6.822736740112305
Epoch 540, val loss: 0.8947373628616333
Epoch 550, training loss: 0.022976186126470566 = 0.016165390610694885 + 0.001 * 6.810794353485107
Epoch 550, val loss: 0.9030057787895203
Epoch 560, training loss: 0.021968578919768333 = 0.01515075284987688 + 0.001 * 6.817825794219971
Epoch 560, val loss: 0.9110636115074158
Epoch 570, training loss: 0.021034779027104378 = 0.014225469902157784 + 0.001 * 6.809309482574463
Epoch 570, val loss: 0.9189725518226624
Epoch 580, training loss: 0.020190522074699402 = 0.013380579650402069 + 0.001 * 6.809942722320557
Epoch 580, val loss: 0.926691472530365
Epoch 590, training loss: 0.0194380022585392 = 0.012607638724148273 + 0.001 * 6.830364227294922
Epoch 590, val loss: 0.9342700242996216
Epoch 600, training loss: 0.01869562640786171 = 0.011898900382220745 + 0.001 * 6.796726226806641
Epoch 600, val loss: 0.9416632652282715
Epoch 610, training loss: 0.018068017438054085 = 0.011248049326241016 + 0.001 * 6.819968223571777
Epoch 610, val loss: 0.9488543272018433
Epoch 620, training loss: 0.017446761950850487 = 0.010648997500538826 + 0.001 * 6.797764778137207
Epoch 620, val loss: 0.9559152126312256
Epoch 630, training loss: 0.016886701807379723 = 0.010096775367856026 + 0.001 * 6.789925575256348
Epoch 630, val loss: 0.9628344774246216
Epoch 640, training loss: 0.01637858897447586 = 0.009583809413015842 + 0.001 * 6.794778347015381
Epoch 640, val loss: 0.969525158405304
Epoch 650, training loss: 0.015879105776548386 = 0.00908575113862753 + 0.001 * 6.79335355758667
Epoch 650, val loss: 0.9762802720069885
Epoch 660, training loss: 0.015422492288053036 = 0.008636650629341602 + 0.001 * 6.785841464996338
Epoch 660, val loss: 0.9832890033721924
Epoch 670, training loss: 0.015024073421955109 = 0.0082175862044096 + 0.001 * 6.806487083435059
Epoch 670, val loss: 0.9896368980407715
Epoch 680, training loss: 0.014605375938117504 = 0.007827790454030037 + 0.001 * 6.777585029602051
Epoch 680, val loss: 0.9961803555488586
Epoch 690, training loss: 0.014242865145206451 = 0.007464908529073 + 0.001 * 6.777956485748291
Epoch 690, val loss: 1.002532958984375
Epoch 700, training loss: 0.013905379921197891 = 0.0071266936138272285 + 0.001 * 6.778686046600342
Epoch 700, val loss: 1.0086103677749634
Epoch 710, training loss: 0.013608084991574287 = 0.006811284925788641 + 0.001 * 6.796800136566162
Epoch 710, val loss: 1.0146149396896362
Epoch 720, training loss: 0.013286896049976349 = 0.006516958121210337 + 0.001 * 6.769937515258789
Epoch 720, val loss: 1.0204678773880005
Epoch 730, training loss: 0.013022329658269882 = 0.006241913419216871 + 0.001 * 6.780416488647461
Epoch 730, val loss: 1.0261956453323364
Epoch 740, training loss: 0.012752752751111984 = 0.005984573625028133 + 0.001 * 6.768178939819336
Epoch 740, val loss: 1.0318470001220703
Epoch 750, training loss: 0.012512525543570518 = 0.005743376445025206 + 0.001 * 6.769149303436279
Epoch 750, val loss: 1.0373377799987793
Epoch 760, training loss: 0.01229368057101965 = 0.005516972858458757 + 0.001 * 6.776707172393799
Epoch 760, val loss: 1.0427255630493164
Epoch 770, training loss: 0.01207059621810913 = 0.0053043849766254425 + 0.001 * 6.766210556030273
Epoch 770, val loss: 1.0480095148086548
Epoch 780, training loss: 0.011890100315213203 = 0.005104568786919117 + 0.001 * 6.785531044006348
Epoch 780, val loss: 1.0531812906265259
Epoch 790, training loss: 0.011676523834466934 = 0.004916638135910034 + 0.001 * 6.759885311126709
Epoch 790, val loss: 1.0582565069198608
Epoch 800, training loss: 0.011497735977172852 = 0.0047396887093782425 + 0.001 * 6.758047103881836
Epoch 800, val loss: 1.0631959438323975
Epoch 810, training loss: 0.011333214119076729 = 0.004572887904942036 + 0.001 * 6.760326385498047
Epoch 810, val loss: 1.06807279586792
Epoch 820, training loss: 0.011175270192325115 = 0.00441549438983202 + 0.001 * 6.759775638580322
Epoch 820, val loss: 1.0728317499160767
Epoch 830, training loss: 0.011035805568099022 = 0.004266754724085331 + 0.001 * 6.769050121307373
Epoch 830, val loss: 1.0775173902511597
Epoch 840, training loss: 0.010888412594795227 = 0.004126179032027721 + 0.001 * 6.762233257293701
Epoch 840, val loss: 1.082144021987915
Epoch 850, training loss: 0.010746575891971588 = 0.003993208985775709 + 0.001 * 6.753366470336914
Epoch 850, val loss: 1.0866369009017944
Epoch 860, training loss: 0.010618804953992367 = 0.003867227118462324 + 0.001 * 6.751577377319336
Epoch 860, val loss: 1.0910693407058716
Epoch 870, training loss: 0.010499399155378342 = 0.0037477859295904636 + 0.001 * 6.751613140106201
Epoch 870, val loss: 1.0954139232635498
Epoch 880, training loss: 0.010404733940958977 = 0.0036344423424452543 + 0.001 * 6.770291328430176
Epoch 880, val loss: 1.0996896028518677
Epoch 890, training loss: 0.010271547362208366 = 0.0035267071798443794 + 0.001 * 6.744839668273926
Epoch 890, val loss: 1.1038706302642822
Epoch 900, training loss: 0.010172143578529358 = 0.003424139693379402 + 0.001 * 6.7480034828186035
Epoch 900, val loss: 1.1080102920532227
Epoch 910, training loss: 0.01006331853568554 = 0.003326431382447481 + 0.001 * 6.736886978149414
Epoch 910, val loss: 1.1121233701705933
Epoch 920, training loss: 0.010002324357628822 = 0.003233343828469515 + 0.001 * 6.768979549407959
Epoch 920, val loss: 1.1161627769470215
Epoch 930, training loss: 0.009887568652629852 = 0.0031446567736566067 + 0.001 * 6.742910861968994
Epoch 930, val loss: 1.1201517581939697
Epoch 940, training loss: 0.009847136214375496 = 0.0030600479803979397 + 0.001 * 6.787087917327881
Epoch 940, val loss: 1.1239879131317139
Epoch 950, training loss: 0.009723049588501453 = 0.002979308133944869 + 0.001 * 6.743741035461426
Epoch 950, val loss: 1.1278150081634521
Epoch 960, training loss: 0.009638180956244469 = 0.002902163192629814 + 0.001 * 6.736016750335693
Epoch 960, val loss: 1.1316007375717163
Epoch 970, training loss: 0.009576183743774891 = 0.0028284084983170033 + 0.001 * 6.747775077819824
Epoch 970, val loss: 1.1352726221084595
Epoch 980, training loss: 0.009487826377153397 = 0.002757853828370571 + 0.001 * 6.729971885681152
Epoch 980, val loss: 1.1389355659484863
Epoch 990, training loss: 0.009441733360290527 = 0.0026902661193162203 + 0.001 * 6.751467227935791
Epoch 990, val loss: 1.1424996852874756
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.6974
Flip ASR: 0.6489/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9472544193267822 = 1.9388805627822876 + 0.001 * 8.373814582824707
Epoch 0, val loss: 1.9352744817733765
Epoch 10, training loss: 1.9373308420181274 = 1.9289571046829224 + 0.001 * 8.373703002929688
Epoch 10, val loss: 1.925740122795105
Epoch 20, training loss: 1.925238847732544 = 1.9168654680252075 + 0.001 * 8.37337589263916
Epoch 20, val loss: 1.9138364791870117
Epoch 30, training loss: 1.9085110425949097 = 1.9001383781433105 + 0.001 * 8.372722625732422
Epoch 30, val loss: 1.8969852924346924
Epoch 40, training loss: 1.8841495513916016 = 1.8757781982421875 + 0.001 * 8.371302604675293
Epoch 40, val loss: 1.8725628852844238
Epoch 50, training loss: 1.849875569343567 = 1.8415077924728394 + 0.001 * 8.367820739746094
Epoch 50, val loss: 1.839234471321106
Epoch 60, training loss: 1.8078105449676514 = 1.799453616142273 + 0.001 * 8.356932640075684
Epoch 60, val loss: 1.8005391359329224
Epoch 70, training loss: 1.7649500370025635 = 1.7566403150558472 + 0.001 * 8.309758186340332
Epoch 70, val loss: 1.7620552778244019
Epoch 80, training loss: 1.7146323919296265 = 1.7065505981445312 + 0.001 * 8.081751823425293
Epoch 80, val loss: 1.715010404586792
Epoch 90, training loss: 1.6451414823532104 = 1.6373536586761475 + 0.001 * 7.787804126739502
Epoch 90, val loss: 1.6538621187210083
Epoch 100, training loss: 1.553395390510559 = 1.5458931922912598 + 0.001 * 7.50214958190918
Epoch 100, val loss: 1.5790644884109497
Epoch 110, training loss: 1.4459333419799805 = 1.438763976097107 + 0.001 * 7.169408798217773
Epoch 110, val loss: 1.4939374923706055
Epoch 120, training loss: 1.3357747793197632 = 1.3286713361740112 + 0.001 * 7.103459358215332
Epoch 120, val loss: 1.4082149267196655
Epoch 130, training loss: 1.2304409742355347 = 1.2233880758285522 + 0.001 * 7.052857875823975
Epoch 130, val loss: 1.328897476196289
Epoch 140, training loss: 1.1318613290786743 = 1.1248403787612915 + 0.001 * 7.02091646194458
Epoch 140, val loss: 1.2558950185775757
Epoch 150, training loss: 1.040689468383789 = 1.033694863319397 + 0.001 * 6.994621276855469
Epoch 150, val loss: 1.1886825561523438
Epoch 160, training loss: 0.9570391774177551 = 0.9500601887702942 + 0.001 * 6.97899055480957
Epoch 160, val loss: 1.1279031038284302
Epoch 170, training loss: 0.879491925239563 = 0.8725204467773438 + 0.001 * 6.971501350402832
Epoch 170, val loss: 1.0723083019256592
Epoch 180, training loss: 0.8055576682090759 = 0.7985908389091492 + 0.001 * 6.96684455871582
Epoch 180, val loss: 1.0196517705917358
Epoch 190, training loss: 0.7333472967147827 = 0.7263835072517395 + 0.001 * 6.9637956619262695
Epoch 190, val loss: 0.9686603546142578
Epoch 200, training loss: 0.6624171137809753 = 0.6554560661315918 + 0.001 * 6.961043834686279
Epoch 200, val loss: 0.9193032383918762
Epoch 210, training loss: 0.593606173992157 = 0.5866479277610779 + 0.001 * 6.958232879638672
Epoch 210, val loss: 0.8725192546844482
Epoch 220, training loss: 0.5285117626190186 = 0.5215564370155334 + 0.001 * 6.95530891418457
Epoch 220, val loss: 0.8293256759643555
Epoch 230, training loss: 0.46874919533729553 = 0.46179667115211487 + 0.001 * 6.952517509460449
Epoch 230, val loss: 0.7913677096366882
Epoch 240, training loss: 0.415306955575943 = 0.40835678577423096 + 0.001 * 6.950175762176514
Epoch 240, val loss: 0.7597380876541138
Epoch 250, training loss: 0.3682405650615692 = 0.36129289865493774 + 0.001 * 6.947671890258789
Epoch 250, val loss: 0.7345728278160095
Epoch 260, training loss: 0.3268805742263794 = 0.31993579864501953 + 0.001 * 6.944786071777344
Epoch 260, val loss: 0.7153849005699158
Epoch 270, training loss: 0.29026398062705994 = 0.2833222448825836 + 0.001 * 6.941739559173584
Epoch 270, val loss: 0.7012220621109009
Epoch 280, training loss: 0.2573462128639221 = 0.250407874584198 + 0.001 * 6.938344478607178
Epoch 280, val loss: 0.6910011768341064
Epoch 290, training loss: 0.22725561261177063 = 0.2203199714422226 + 0.001 * 6.935646057128906
Epoch 290, val loss: 0.6836841106414795
Epoch 300, training loss: 0.1995406299829483 = 0.19260764122009277 + 0.001 * 6.932995319366455
Epoch 300, val loss: 0.6787600517272949
Epoch 310, training loss: 0.17415258288383484 = 0.16722434759140015 + 0.001 * 6.928242206573486
Epoch 310, val loss: 0.6758050918579102
Epoch 320, training loss: 0.15137454867362976 = 0.14444972574710846 + 0.001 * 6.924829959869385
Epoch 320, val loss: 0.6752587556838989
Epoch 330, training loss: 0.13146618008613586 = 0.12454480677843094 + 0.001 * 6.9213666915893555
Epoch 330, val loss: 0.6773489117622375
Epoch 340, training loss: 0.11450116336345673 = 0.10758573561906815 + 0.001 * 6.915425777435303
Epoch 340, val loss: 0.6821871995925903
Epoch 350, training loss: 0.10028963536024094 = 0.09337657690048218 + 0.001 * 6.913058280944824
Epoch 350, val loss: 0.6897310018539429
Epoch 360, training loss: 0.0884661003947258 = 0.08156032860279083 + 0.001 * 6.905771255493164
Epoch 360, val loss: 0.6996071934700012
Epoch 370, training loss: 0.07862842828035355 = 0.07172638922929764 + 0.001 * 6.902041435241699
Epoch 370, val loss: 0.7112878561019897
Epoch 380, training loss: 0.07038702070713043 = 0.06349869817495346 + 0.001 * 6.8883256912231445
Epoch 380, val loss: 0.7243167757987976
Epoch 390, training loss: 0.06345103681087494 = 0.056565724313259125 + 0.001 * 6.885308265686035
Epoch 390, val loss: 0.7382630109786987
Epoch 400, training loss: 0.057567331939935684 = 0.05067667365074158 + 0.001 * 6.890656471252441
Epoch 400, val loss: 0.7526852488517761
Epoch 410, training loss: 0.052498891949653625 = 0.04563485458493233 + 0.001 * 6.864037990570068
Epoch 410, val loss: 0.7673426866531372
Epoch 420, training loss: 0.04815069958567619 = 0.04128529503941536 + 0.001 * 6.8654046058654785
Epoch 420, val loss: 0.7820344567298889
Epoch 430, training loss: 0.044388461858034134 = 0.0375046543776989 + 0.001 * 6.8838067054748535
Epoch 430, val loss: 0.7966516613960266
Epoch 440, training loss: 0.04105747491121292 = 0.03419587388634682 + 0.001 * 6.861602783203125
Epoch 440, val loss: 0.8111323118209839
Epoch 450, training loss: 0.03812173381447792 = 0.031283728778362274 + 0.001 * 6.838004112243652
Epoch 450, val loss: 0.8254022002220154
Epoch 460, training loss: 0.03555208817124367 = 0.02870965749025345 + 0.001 * 6.842429161071777
Epoch 460, val loss: 0.8395131826400757
Epoch 470, training loss: 0.033261723816394806 = 0.026425184682011604 + 0.001 * 6.836536884307861
Epoch 470, val loss: 0.8533521294593811
Epoch 480, training loss: 0.031244901940226555 = 0.024389047175645828 + 0.001 * 6.855854511260986
Epoch 480, val loss: 0.8669687509536743
Epoch 490, training loss: 0.02940453216433525 = 0.022568147629499435 + 0.001 * 6.836385250091553
Epoch 490, val loss: 0.8803073167800903
Epoch 500, training loss: 0.027762625366449356 = 0.02093356102705002 + 0.001 * 6.829063892364502
Epoch 500, val loss: 0.8933522701263428
Epoch 510, training loss: 0.02630157209932804 = 0.019461870193481445 + 0.001 * 6.839702129364014
Epoch 510, val loss: 0.9061272740364075
Epoch 520, training loss: 0.02496691420674324 = 0.018135353922843933 + 0.001 * 6.831559181213379
Epoch 520, val loss: 0.9185265898704529
Epoch 530, training loss: 0.023771261796355247 = 0.016937678679823875 + 0.001 * 6.833582878112793
Epoch 530, val loss: 0.930705189704895
Epoch 540, training loss: 0.022670431062579155 = 0.015848681330680847 + 0.001 * 6.821748733520508
Epoch 540, val loss: 0.9426038861274719
Epoch 550, training loss: 0.02167809195816517 = 0.01485641673207283 + 0.001 * 6.821674823760986
Epoch 550, val loss: 0.9542350769042969
Epoch 560, training loss: 0.020769527181982994 = 0.013949344865977764 + 0.001 * 6.820181846618652
Epoch 560, val loss: 0.9657069444656372
Epoch 570, training loss: 0.01993454620242119 = 0.013118733651936054 + 0.001 * 6.815812587738037
Epoch 570, val loss: 0.9768091440200806
Epoch 580, training loss: 0.019171709194779396 = 0.012356325052678585 + 0.001 * 6.815384387969971
Epoch 580, val loss: 0.9877798557281494
Epoch 590, training loss: 0.018472537398338318 = 0.011655431240797043 + 0.001 * 6.817106246948242
Epoch 590, val loss: 0.998514711856842
Epoch 600, training loss: 0.017821792513132095 = 0.011010300368070602 + 0.001 * 6.8114914894104
Epoch 600, val loss: 1.0089994668960571
Epoch 610, training loss: 0.01722647063434124 = 0.010415548458695412 + 0.001 * 6.810922145843506
Epoch 610, val loss: 1.0192714929580688
Epoch 620, training loss: 0.01668102666735649 = 0.009866476058959961 + 0.001 * 6.81455135345459
Epoch 620, val loss: 1.029295563697815
Epoch 630, training loss: 0.016169635578989983 = 0.009358908981084824 + 0.001 * 6.810726165771484
Epoch 630, val loss: 1.0390985012054443
Epoch 640, training loss: 0.015705639496445656 = 0.008889134973287582 + 0.001 * 6.81650447845459
Epoch 640, val loss: 1.0486986637115479
Epoch 650, training loss: 0.015263023786246777 = 0.008453681133687496 + 0.001 * 6.809342384338379
Epoch 650, val loss: 1.0580822229385376
Epoch 660, training loss: 0.014855198562145233 = 0.008049577474594116 + 0.001 * 6.805620193481445
Epoch 660, val loss: 1.067270040512085
Epoch 670, training loss: 0.014485595747828484 = 0.007674011867493391 + 0.001 * 6.811583518981934
Epoch 670, val loss: 1.076240062713623
Epoch 680, training loss: 0.01411762647330761 = 0.007324534468352795 + 0.001 * 6.793091297149658
Epoch 680, val loss: 1.0850051641464233
Epoch 690, training loss: 0.013795033097267151 = 0.006998878438025713 + 0.001 * 6.796154022216797
Epoch 690, val loss: 1.0935957431793213
Epoch 700, training loss: 0.01349283941090107 = 0.006695011630654335 + 0.001 * 6.797826766967773
Epoch 700, val loss: 1.1020090579986572
Epoch 710, training loss: 0.01320667751133442 = 0.006411172449588776 + 0.001 * 6.795504570007324
Epoch 710, val loss: 1.1102133989334106
Epoch 720, training loss: 0.012938262894749641 = 0.0061456686817109585 + 0.001 * 6.792593955993652
Epoch 720, val loss: 1.1182589530944824
Epoch 730, training loss: 0.012690600007772446 = 0.005896968767046928 + 0.001 * 6.793631076812744
Epoch 730, val loss: 1.1261444091796875
Epoch 740, training loss: 0.012460460886359215 = 0.005663751158863306 + 0.001 * 6.796710014343262
Epoch 740, val loss: 1.1338660717010498
Epoch 750, training loss: 0.0122585603967309 = 0.005444738548249006 + 0.001 * 6.813821315765381
Epoch 750, val loss: 1.141424298286438
Epoch 760, training loss: 0.012025898322463036 = 0.005238846410065889 + 0.001 * 6.787051200866699
Epoch 760, val loss: 1.1488264799118042
Epoch 770, training loss: 0.01183401420712471 = 0.005045374855399132 + 0.001 * 6.788639545440674
Epoch 770, val loss: 1.1560231447219849
Epoch 780, training loss: 0.011639355681836605 = 0.004863135050982237 + 0.001 * 6.776220321655273
Epoch 780, val loss: 1.1631381511688232
Epoch 790, training loss: 0.011460507288575172 = 0.0046912929974496365 + 0.001 * 6.769213676452637
Epoch 790, val loss: 1.1700835227966309
Epoch 800, training loss: 0.011304561980068684 = 0.004529069177806377 + 0.001 * 6.7754926681518555
Epoch 800, val loss: 1.1768887042999268
Epoch 810, training loss: 0.011148270219564438 = 0.004375757649540901 + 0.001 * 6.772512435913086
Epoch 810, val loss: 1.1835627555847168
Epoch 820, training loss: 0.011007281020283699 = 0.004230797290802002 + 0.001 * 6.776483058929443
Epoch 820, val loss: 1.190086007118225
Epoch 830, training loss: 0.010881582275032997 = 0.004093552939593792 + 0.001 * 6.788029193878174
Epoch 830, val loss: 1.1965092420578003
Epoch 840, training loss: 0.010736977681517601 = 0.0039635649882256985 + 0.001 * 6.773412227630615
Epoch 840, val loss: 1.2027748823165894
Epoch 850, training loss: 0.010603182017803192 = 0.0038402648642659187 + 0.001 * 6.762916564941406
Epoch 850, val loss: 1.208951711654663
Epoch 860, training loss: 0.01049068197607994 = 0.0037232348695397377 + 0.001 * 6.767447471618652
Epoch 860, val loss: 1.214990496635437
Epoch 870, training loss: 0.010363573208451271 = 0.003612074302509427 + 0.001 * 6.751498699188232
Epoch 870, val loss: 1.2209166288375854
Epoch 880, training loss: 0.01028501708060503 = 0.0035064201802015305 + 0.001 * 6.7785964012146
Epoch 880, val loss: 1.2267396450042725
Epoch 890, training loss: 0.01017298549413681 = 0.003405902301892638 + 0.001 * 6.767083168029785
Epoch 890, val loss: 1.2324436902999878
Epoch 900, training loss: 0.010069150477647781 = 0.003310189116746187 + 0.001 * 6.758961200714111
Epoch 900, val loss: 1.2380529642105103
Epoch 910, training loss: 0.009968576952815056 = 0.003218994475901127 + 0.001 * 6.749582290649414
Epoch 910, val loss: 1.243572473526001
Epoch 920, training loss: 0.009875314310193062 = 0.0031320175621658564 + 0.001 * 6.7432966232299805
Epoch 920, val loss: 1.2489368915557861
Epoch 930, training loss: 0.009813936427235603 = 0.0030490150675177574 + 0.001 * 6.764920711517334
Epoch 930, val loss: 1.2542442083358765
Epoch 940, training loss: 0.009737969376146793 = 0.0029697497375309467 + 0.001 * 6.768219470977783
Epoch 940, val loss: 1.2594481706619263
Epoch 950, training loss: 0.00965588353574276 = 0.002894003875553608 + 0.001 * 6.761879920959473
Epoch 950, val loss: 1.2645549774169922
Epoch 960, training loss: 0.009571636095643044 = 0.002821580972522497 + 0.001 * 6.750054836273193
Epoch 960, val loss: 1.269603967666626
Epoch 970, training loss: 0.00952153466641903 = 0.0027522791642695665 + 0.001 * 6.7692551612854
Epoch 970, val loss: 1.274530291557312
Epoch 980, training loss: 0.009431063197553158 = 0.002685922896489501 + 0.001 * 6.7451395988464355
Epoch 980, val loss: 1.2794008255004883
Epoch 990, training loss: 0.009408805519342422 = 0.002622348954901099 + 0.001 * 6.78645658493042
Epoch 990, val loss: 1.2841991186141968
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7823
Flip ASR: 0.7556/225 nodes
The final ASR:0.72325, 0.04186, Accuracy:0.81235, 0.02983
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9590])
updated graph: torch.Size([2, 10664])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00460, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9580048322677612 = 1.9496309757232666 + 0.001 * 8.373896598815918
Epoch 0, val loss: 1.9550899267196655
Epoch 10, training loss: 1.948338508605957 = 1.9399646520614624 + 0.001 * 8.37385368347168
Epoch 10, val loss: 1.9461735486984253
Epoch 20, training loss: 1.9366947412490845 = 1.9283210039138794 + 0.001 * 8.37368392944336
Epoch 20, val loss: 1.9348926544189453
Epoch 30, training loss: 1.9207384586334229 = 1.912365198135376 + 0.001 * 8.373312950134277
Epoch 30, val loss: 1.9189589023590088
Epoch 40, training loss: 1.89724600315094 = 1.88887357711792 + 0.001 * 8.372380256652832
Epoch 40, val loss: 1.8952994346618652
Epoch 50, training loss: 1.8631563186645508 = 1.8547865152359009 + 0.001 * 8.36978530883789
Epoch 50, val loss: 1.8617008924484253
Epoch 60, training loss: 1.8194035291671753 = 1.8110427856445312 + 0.001 * 8.360751152038574
Epoch 60, val loss: 1.8212090730667114
Epoch 70, training loss: 1.774849772453308 = 1.7665294408798218 + 0.001 * 8.320294380187988
Epoch 70, val loss: 1.7833055257797241
Epoch 80, training loss: 1.7273979187011719 = 1.7193708419799805 + 0.001 * 8.027087211608887
Epoch 80, val loss: 1.742428183555603
Epoch 90, training loss: 1.6611645221710205 = 1.6534091234207153 + 0.001 * 7.755359172821045
Epoch 90, val loss: 1.6851871013641357
Epoch 100, training loss: 1.5741097927093506 = 1.566554307937622 + 0.001 * 7.555458068847656
Epoch 100, val loss: 1.6119939088821411
Epoch 110, training loss: 1.4669854640960693 = 1.459606409072876 + 0.001 * 7.37907075881958
Epoch 110, val loss: 1.5228991508483887
Epoch 120, training loss: 1.34837806224823 = 1.3410454988479614 + 0.001 * 7.332517147064209
Epoch 120, val loss: 1.422921061515808
Epoch 130, training loss: 1.226889729499817 = 1.2196135520935059 + 0.001 * 7.276132106781006
Epoch 130, val loss: 1.3218419551849365
Epoch 140, training loss: 1.1106908321380615 = 1.1034924983978271 + 0.001 * 7.198304653167725
Epoch 140, val loss: 1.2261725664138794
Epoch 150, training loss: 1.003009557723999 = 0.9959084987640381 + 0.001 * 7.101073265075684
Epoch 150, val loss: 1.1392217874526978
Epoch 160, training loss: 0.9040653705596924 = 0.8969920873641968 + 0.001 * 7.073278427124023
Epoch 160, val loss: 1.0603606700897217
Epoch 170, training loss: 0.8138405084609985 = 0.8067708015441895 + 0.001 * 7.069684028625488
Epoch 170, val loss: 0.9899560809135437
Epoch 180, training loss: 0.7330228090286255 = 0.7259596586227417 + 0.001 * 7.063128471374512
Epoch 180, val loss: 0.9294283986091614
Epoch 190, training loss: 0.662165105342865 = 0.655108630657196 + 0.001 * 7.056495189666748
Epoch 190, val loss: 0.8797754049301147
Epoch 200, training loss: 0.6010119318962097 = 0.5939691662788391 + 0.001 * 7.042761325836182
Epoch 200, val loss: 0.841311514377594
Epoch 210, training loss: 0.5486342906951904 = 0.5416107177734375 + 0.001 * 7.023574352264404
Epoch 210, val loss: 0.8130614757537842
Epoch 220, training loss: 0.503617525100708 = 0.4966203272342682 + 0.001 * 6.997220039367676
Epoch 220, val loss: 0.7929797172546387
Epoch 230, training loss: 0.4641093909740448 = 0.45713433623313904 + 0.001 * 6.975040435791016
Epoch 230, val loss: 0.7784969210624695
Epoch 240, training loss: 0.428129106760025 = 0.42116761207580566 + 0.001 * 6.96149206161499
Epoch 240, val loss: 0.7676113843917847
Epoch 250, training loss: 0.3941141366958618 = 0.3871619403362274 + 0.001 * 6.952187538146973
Epoch 250, val loss: 0.7595260143280029
Epoch 260, training loss: 0.36124899983406067 = 0.35430285334587097 + 0.001 * 6.946133613586426
Epoch 260, val loss: 0.7539376616477966
Epoch 270, training loss: 0.3295314311981201 = 0.3225894868373871 + 0.001 * 6.941929817199707
Epoch 270, val loss: 0.750970721244812
Epoch 280, training loss: 0.2994377911090851 = 0.2924993336200714 + 0.001 * 6.938451766967773
Epoch 280, val loss: 0.7506467700004578
Epoch 290, training loss: 0.27121812105178833 = 0.26428258419036865 + 0.001 * 6.935547828674316
Epoch 290, val loss: 0.7527405619621277
Epoch 300, training loss: 0.2446489930152893 = 0.23771706223487854 + 0.001 * 6.9319376945495605
Epoch 300, val loss: 0.7566404938697815
Epoch 310, training loss: 0.2193991094827652 = 0.2124713659286499 + 0.001 * 6.927740573883057
Epoch 310, val loss: 0.7616839408874512
Epoch 320, training loss: 0.1954714059829712 = 0.18854916095733643 + 0.001 * 6.922238349914551
Epoch 320, val loss: 0.7675732374191284
Epoch 330, training loss: 0.17319355905056 = 0.16627512872219086 + 0.001 * 6.918431758880615
Epoch 330, val loss: 0.7744865417480469
Epoch 340, training loss: 0.15298239886760712 = 0.14606665074825287 + 0.001 * 6.915749549865723
Epoch 340, val loss: 0.782768189907074
Epoch 350, training loss: 0.13507696986198425 = 0.1281656175851822 + 0.001 * 6.911352157592773
Epoch 350, val loss: 0.7926075458526611
Epoch 360, training loss: 0.11949503421783447 = 0.11258825659751892 + 0.001 * 6.906773567199707
Epoch 360, val loss: 0.8040421009063721
Epoch 370, training loss: 0.10605985671281815 = 0.09916526824235916 + 0.001 * 6.894586086273193
Epoch 370, val loss: 0.8170098066329956
Epoch 380, training loss: 0.09452613443136215 = 0.08763622492551804 + 0.001 * 6.889906406402588
Epoch 380, val loss: 0.8312667608261108
Epoch 390, training loss: 0.08459793776273727 = 0.07771197706460953 + 0.001 * 6.88595724105835
Epoch 390, val loss: 0.8464477062225342
Epoch 400, training loss: 0.07600808143615723 = 0.06912467628717422 + 0.001 * 6.883403778076172
Epoch 400, val loss: 0.8622477650642395
Epoch 410, training loss: 0.06853309273719788 = 0.061650797724723816 + 0.001 * 6.882296085357666
Epoch 410, val loss: 0.8783383369445801
Epoch 420, training loss: 0.062006715685129166 = 0.05511901527643204 + 0.001 * 6.887701034545898
Epoch 420, val loss: 0.8946041464805603
Epoch 430, training loss: 0.05627060309052467 = 0.04939606785774231 + 0.001 * 6.87453556060791
Epoch 430, val loss: 0.9108947515487671
Epoch 440, training loss: 0.05125025659799576 = 0.0443718396127224 + 0.001 * 6.878418445587158
Epoch 440, val loss: 0.9270808696746826
Epoch 450, training loss: 0.04682999104261398 = 0.0399593785405159 + 0.001 * 6.870614051818848
Epoch 450, val loss: 0.9430928230285645
Epoch 460, training loss: 0.04295207932591438 = 0.03608324006199837 + 0.001 * 6.868839740753174
Epoch 460, val loss: 0.9588549733161926
Epoch 470, training loss: 0.039541780948638916 = 0.03267156332731247 + 0.001 * 6.8702168464660645
Epoch 470, val loss: 0.9742578864097595
Epoch 480, training loss: 0.03652367740869522 = 0.02966129779815674 + 0.001 * 6.862380027770996
Epoch 480, val loss: 0.9892980456352234
Epoch 490, training loss: 0.03385789319872856 = 0.026998871937394142 + 0.001 * 6.859020233154297
Epoch 490, val loss: 1.0038493871688843
Epoch 500, training loss: 0.031513430178165436 = 0.024641048163175583 + 0.001 * 6.872382640838623
Epoch 500, val loss: 1.0179150104522705
Epoch 510, training loss: 0.02940657176077366 = 0.022551165893673897 + 0.001 * 6.855405330657959
Epoch 510, val loss: 1.0314885377883911
Epoch 520, training loss: 0.027551356703042984 = 0.020696910098195076 + 0.001 * 6.854445934295654
Epoch 520, val loss: 1.0446025133132935
Epoch 530, training loss: 0.02591230534017086 = 0.01904923841357231 + 0.001 * 6.863067150115967
Epoch 530, val loss: 1.0572198629379272
Epoch 540, training loss: 0.024431614205241203 = 0.017582714557647705 + 0.001 * 6.848898887634277
Epoch 540, val loss: 1.0693782567977905
Epoch 550, training loss: 0.023114364594221115 = 0.01627418026328087 + 0.001 * 6.840185165405273
Epoch 550, val loss: 1.0810937881469727
Epoch 560, training loss: 0.021946685388684273 = 0.015103846788406372 + 0.001 * 6.842838764190674
Epoch 560, val loss: 1.0924186706542969
Epoch 570, training loss: 0.02089991234242916 = 0.014054306782782078 + 0.001 * 6.84560489654541
Epoch 570, val loss: 1.1033118963241577
Epoch 580, training loss: 0.019944606348872185 = 0.013110536150634289 + 0.001 * 6.83406925201416
Epoch 580, val loss: 1.1138386726379395
Epoch 590, training loss: 0.01911305822432041 = 0.012259535491466522 + 0.001 * 6.853522300720215
Epoch 590, val loss: 1.1240131855010986
Epoch 600, training loss: 0.018326403573155403 = 0.011490077711641788 + 0.001 * 6.836325168609619
Epoch 600, val loss: 1.1338238716125488
Epoch 610, training loss: 0.017620638012886047 = 0.010792501270771027 + 0.001 * 6.828136444091797
Epoch 610, val loss: 1.1433130502700806
Epoch 620, training loss: 0.017026163637638092 = 0.010158423334360123 + 0.001 * 6.867740631103516
Epoch 620, val loss: 1.1524866819381714
Epoch 630, training loss: 0.01641913875937462 = 0.009580678306519985 + 0.001 * 6.838459014892578
Epoch 630, val loss: 1.1613895893096924
Epoch 640, training loss: 0.015877176076173782 = 0.009052873589098454 + 0.001 * 6.824303150177002
Epoch 640, val loss: 1.1699953079223633
Epoch 650, training loss: 0.015393435955047607 = 0.008569528348743916 + 0.001 * 6.823907852172852
Epoch 650, val loss: 1.178350806236267
Epoch 660, training loss: 0.014961745589971542 = 0.008125838823616505 + 0.001 * 6.835906505584717
Epoch 660, val loss: 1.186416745185852
Epoch 670, training loss: 0.014542466960847378 = 0.007717689033597708 + 0.001 * 6.824777603149414
Epoch 670, val loss: 1.1942347288131714
Epoch 680, training loss: 0.014155924320220947 = 0.007341370917856693 + 0.001 * 6.814553737640381
Epoch 680, val loss: 1.201825737953186
Epoch 690, training loss: 0.013810183852910995 = 0.006993710994720459 + 0.001 * 6.816472053527832
Epoch 690, val loss: 1.209177851676941
Epoch 700, training loss: 0.013487763702869415 = 0.006671912968158722 + 0.001 * 6.815849781036377
Epoch 700, val loss: 1.2163386344909668
Epoch 710, training loss: 0.01318727619946003 = 0.006373494863510132 + 0.001 * 6.813781261444092
Epoch 710, val loss: 1.2232751846313477
Epoch 720, training loss: 0.012909352779388428 = 0.00609624432399869 + 0.001 * 6.813107967376709
Epoch 720, val loss: 1.2300108671188354
Epoch 730, training loss: 0.012643853202462196 = 0.005838203243911266 + 0.001 * 6.805649280548096
Epoch 730, val loss: 1.2365797758102417
Epoch 740, training loss: 0.012424321845173836 = 0.005597627721726894 + 0.001 * 6.826694488525391
Epoch 740, val loss: 1.2429627180099487
Epoch 750, training loss: 0.012177681550383568 = 0.00537287350744009 + 0.001 * 6.804807662963867
Epoch 750, val loss: 1.2491732835769653
Epoch 760, training loss: 0.011989980936050415 = 0.0051621608436107635 + 0.001 * 6.827819347381592
Epoch 760, val loss: 1.2552194595336914
Epoch 770, training loss: 0.01176898367702961 = 0.004963123705238104 + 0.001 * 6.8058600425720215
Epoch 770, val loss: 1.2611002922058105
Epoch 780, training loss: 0.011576483026146889 = 0.004773433785885572 + 0.001 * 6.803049564361572
Epoch 780, val loss: 1.2668567895889282
Epoch 790, training loss: 0.011390102095901966 = 0.004591784905642271 + 0.001 * 6.798316955566406
Epoch 790, val loss: 1.2725175619125366
Epoch 800, training loss: 0.011218512430787086 = 0.004417897202074528 + 0.001 * 6.800615310668945
Epoch 800, val loss: 1.2780423164367676
Epoch 810, training loss: 0.011062050238251686 = 0.004251799080520868 + 0.001 * 6.810251235961914
Epoch 810, val loss: 1.2834410667419434
Epoch 820, training loss: 0.01088686939328909 = 0.004093622323125601 + 0.001 * 6.793246746063232
Epoch 820, val loss: 1.2887266874313354
Epoch 830, training loss: 0.010751252993941307 = 0.003943366464227438 + 0.001 * 6.807886123657227
Epoch 830, val loss: 1.2938896417617798
Epoch 840, training loss: 0.010602034628391266 = 0.003800959326326847 + 0.001 * 6.801074981689453
Epoch 840, val loss: 1.2989380359649658
Epoch 850, training loss: 0.010457799769937992 = 0.003666105680167675 + 0.001 * 6.791693687438965
Epoch 850, val loss: 1.3038545846939087
Epoch 860, training loss: 0.010327080264687538 = 0.0035385265946388245 + 0.001 * 6.788553714752197
Epoch 860, val loss: 1.3086563348770142
Epoch 870, training loss: 0.010198243893682957 = 0.0034178579226136208 + 0.001 * 6.780385494232178
Epoch 870, val loss: 1.3133524656295776
Epoch 880, training loss: 0.01009367685765028 = 0.003303761826828122 + 0.001 * 6.789915084838867
Epoch 880, val loss: 1.317946195602417
Epoch 890, training loss: 0.009974262677133083 = 0.003195868106558919 + 0.001 * 6.77839469909668
Epoch 890, val loss: 1.3224234580993652
Epoch 900, training loss: 0.009892615489661694 = 0.003093777457252145 + 0.001 * 6.798838138580322
Epoch 900, val loss: 1.3268078565597534
Epoch 910, training loss: 0.009782210923731327 = 0.0029971683397889137 + 0.001 * 6.7850422859191895
Epoch 910, val loss: 1.3311147689819336
Epoch 920, training loss: 0.009684679098427296 = 0.0029056742787361145 + 0.001 * 6.7790045738220215
Epoch 920, val loss: 1.3353084325790405
Epoch 930, training loss: 0.009600190445780754 = 0.002818954410031438 + 0.001 * 6.781235694885254
Epoch 930, val loss: 1.3394070863723755
Epoch 940, training loss: 0.009510275907814503 = 0.002736716764047742 + 0.001 * 6.773559093475342
Epoch 940, val loss: 1.343413233757019
Epoch 950, training loss: 0.009424416348338127 = 0.0026586665771901608 + 0.001 * 6.765748977661133
Epoch 950, val loss: 1.3473436832427979
Epoch 960, training loss: 0.009347314946353436 = 0.0025845279451459646 + 0.001 * 6.762786388397217
Epoch 960, val loss: 1.3511868715286255
Epoch 970, training loss: 0.00929260440170765 = 0.0025140666402876377 + 0.001 * 6.778537750244141
Epoch 970, val loss: 1.3549442291259766
Epoch 980, training loss: 0.009203428402543068 = 0.00244705518707633 + 0.001 * 6.756372928619385
Epoch 980, val loss: 1.3586235046386719
Epoch 990, training loss: 0.009159756824374199 = 0.0023832551669329405 + 0.001 * 6.776501655578613
Epoch 990, val loss: 1.3622238636016846
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.4982
Flip ASR: 0.4044/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9496197700500488 = 1.9412459135055542 + 0.001 * 8.373858451843262
Epoch 0, val loss: 1.9413847923278809
Epoch 10, training loss: 1.9391717910766602 = 1.9307979345321655 + 0.001 * 8.373815536499023
Epoch 10, val loss: 1.9299712181091309
Epoch 20, training loss: 1.9265587329864502 = 1.9181851148605347 + 0.001 * 8.373642921447754
Epoch 20, val loss: 1.9156665802001953
Epoch 30, training loss: 1.9091607332229614 = 1.9007874727249146 + 0.001 * 8.373284339904785
Epoch 30, val loss: 1.895414113998413
Epoch 40, training loss: 1.8842817544937134 = 1.8759092092514038 + 0.001 * 8.372535705566406
Epoch 40, val loss: 1.866620659828186
Epoch 50, training loss: 1.8501273393630981 = 1.8417567014694214 + 0.001 * 8.370591163635254
Epoch 50, val loss: 1.8288885354995728
Epoch 60, training loss: 1.8088140487670898 = 1.800450325012207 + 0.001 * 8.36373233795166
Epoch 60, val loss: 1.7872579097747803
Epoch 70, training loss: 1.7669310569763184 = 1.7586007118225098 + 0.001 * 8.330403327941895
Epoch 70, val loss: 1.7491000890731812
Epoch 80, training loss: 1.7203869819641113 = 1.7123016119003296 + 0.001 * 8.085370063781738
Epoch 80, val loss: 1.7078839540481567
Epoch 90, training loss: 1.6570994853973389 = 1.6493470668792725 + 0.001 * 7.752406120300293
Epoch 90, val loss: 1.6530306339263916
Epoch 100, training loss: 1.5729907751083374 = 1.5654022693634033 + 0.001 * 7.588485240936279
Epoch 100, val loss: 1.5824692249298096
Epoch 110, training loss: 1.4731214046478271 = 1.4657191038131714 + 0.001 * 7.402261734008789
Epoch 110, val loss: 1.5017362833023071
Epoch 120, training loss: 1.367862582206726 = 1.3606621026992798 + 0.001 * 7.200497627258301
Epoch 120, val loss: 1.4195654392242432
Epoch 130, training loss: 1.264689326286316 = 1.2575289011001587 + 0.001 * 7.160400390625
Epoch 130, val loss: 1.3433785438537598
Epoch 140, training loss: 1.1667523384094238 = 1.1596537828445435 + 0.001 * 7.09850549697876
Epoch 140, val loss: 1.2750037908554077
Epoch 150, training loss: 1.075508952140808 = 1.0684475898742676 + 0.001 * 7.06141996383667
Epoch 150, val loss: 1.2130041122436523
Epoch 160, training loss: 0.9913272857666016 = 0.9842870235443115 + 0.001 * 7.04024076461792
Epoch 160, val loss: 1.1563397645950317
Epoch 170, training loss: 0.9132793545722961 = 0.9062564969062805 + 0.001 * 7.022845268249512
Epoch 170, val loss: 1.1032124757766724
Epoch 180, training loss: 0.8395421504974365 = 0.8325353264808655 + 0.001 * 7.006850719451904
Epoch 180, val loss: 1.0523043870925903
Epoch 190, training loss: 0.7683449387550354 = 0.7613565325737 + 0.001 * 6.988391876220703
Epoch 190, val loss: 1.0022697448730469
Epoch 200, training loss: 0.6984699368476868 = 0.6914985179901123 + 0.001 * 6.971442222595215
Epoch 200, val loss: 0.9521479606628418
Epoch 210, training loss: 0.6300367712974548 = 0.6230767965316772 + 0.001 * 6.959948539733887
Epoch 210, val loss: 0.9026980996131897
Epoch 220, training loss: 0.5638660788536072 = 0.5569124817848206 + 0.001 * 6.953571796417236
Epoch 220, val loss: 0.8547030687332153
Epoch 230, training loss: 0.5010414719581604 = 0.4940912127494812 + 0.001 * 6.950254917144775
Epoch 230, val loss: 0.8112332224845886
Epoch 240, training loss: 0.44323909282684326 = 0.4362906515598297 + 0.001 * 6.948441982269287
Epoch 240, val loss: 0.7743492126464844
Epoch 250, training loss: 0.39172905683517456 = 0.38478153944015503 + 0.001 * 6.9475178718566895
Epoch 250, val loss: 0.7449546456336975
Epoch 260, training loss: 0.3473482131958008 = 0.3404010236263275 + 0.001 * 6.9471893310546875
Epoch 260, val loss: 0.7234877943992615
Epoch 270, training loss: 0.3094412386417389 = 0.3024938702583313 + 0.001 * 6.9473772048950195
Epoch 270, val loss: 0.7081478834152222
Epoch 280, training loss: 0.2766745984554291 = 0.26972657442092896 + 0.001 * 6.94803524017334
Epoch 280, val loss: 0.6975405216217041
Epoch 290, training loss: 0.24770362675189972 = 0.24075454473495483 + 0.001 * 6.949084758758545
Epoch 290, val loss: 0.6900997161865234
Epoch 300, training loss: 0.22149613499641418 = 0.21454572677612305 + 0.001 * 6.950409889221191
Epoch 300, val loss: 0.6852231025695801
Epoch 310, training loss: 0.19741444289684296 = 0.19046251475811005 + 0.001 * 6.9519267082214355
Epoch 310, val loss: 0.6817749738693237
Epoch 320, training loss: 0.17531664669513702 = 0.16836293041706085 + 0.001 * 6.95371150970459
Epoch 320, val loss: 0.679819643497467
Epoch 330, training loss: 0.15526831150054932 = 0.1483129858970642 + 0.001 * 6.95532751083374
Epoch 330, val loss: 0.6792228817939758
Epoch 340, training loss: 0.1373334378004074 = 0.13037636876106262 + 0.001 * 6.957066059112549
Epoch 340, val loss: 0.6798907518386841
Epoch 350, training loss: 0.12151540070772171 = 0.11455633491277695 + 0.001 * 6.959066867828369
Epoch 350, val loss: 0.6818097829818726
Epoch 360, training loss: 0.10768428444862366 = 0.1007237583398819 + 0.001 * 6.960525989532471
Epoch 360, val loss: 0.6849109530448914
Epoch 370, training loss: 0.09567008167505264 = 0.08870813250541687 + 0.001 * 6.96195125579834
Epoch 370, val loss: 0.6891381740570068
Epoch 380, training loss: 0.08525155484676361 = 0.0782884731888771 + 0.001 * 6.963083267211914
Epoch 380, val loss: 0.6944071650505066
Epoch 390, training loss: 0.07628796249628067 = 0.06932292133569717 + 0.001 * 6.9650373458862305
Epoch 390, val loss: 0.7005714178085327
Epoch 400, training loss: 0.0686030462384224 = 0.06163792684674263 + 0.001 * 6.965116024017334
Epoch 400, val loss: 0.7076517343521118
Epoch 410, training loss: 0.06199905276298523 = 0.05503362789750099 + 0.001 * 6.965422630310059
Epoch 410, val loss: 0.7152846455574036
Epoch 420, training loss: 0.056314095854759216 = 0.049346838146448135 + 0.001 * 6.967257976531982
Epoch 420, val loss: 0.7235233187675476
Epoch 430, training loss: 0.05139964073896408 = 0.044433314353227615 + 0.001 * 6.966324329376221
Epoch 430, val loss: 0.7321756482124329
Epoch 440, training loss: 0.04713039472699165 = 0.0401669517159462 + 0.001 * 6.963443756103516
Epoch 440, val loss: 0.7410059571266174
Epoch 450, training loss: 0.04341508820652962 = 0.03644018620252609 + 0.001 * 6.974902153015137
Epoch 450, val loss: 0.7501043677330017
Epoch 460, training loss: 0.04014923423528671 = 0.0331832580268383 + 0.001 * 6.965977191925049
Epoch 460, val loss: 0.7593342661857605
Epoch 470, training loss: 0.037271130830049515 = 0.030311528593301773 + 0.001 * 6.959601879119873
Epoch 470, val loss: 0.7685742974281311
Epoch 480, training loss: 0.0347154326736927 = 0.02776215970516205 + 0.001 * 6.953274250030518
Epoch 480, val loss: 0.7779765725135803
Epoch 490, training loss: 0.032440200448036194 = 0.025490708649158478 + 0.001 * 6.949493408203125
Epoch 490, val loss: 0.7872697114944458
Epoch 500, training loss: 0.030411656945943832 = 0.023469645529985428 + 0.001 * 6.942009925842285
Epoch 500, val loss: 0.7964953184127808
Epoch 510, training loss: 0.028615329414606094 = 0.02167520858347416 + 0.001 * 6.940119743347168
Epoch 510, val loss: 0.805571436882019
Epoch 520, training loss: 0.026997489854693413 = 0.02007155679166317 + 0.001 * 6.925933361053467
Epoch 520, val loss: 0.8145212531089783
Epoch 530, training loss: 0.025616448372602463 = 0.01863686926662922 + 0.001 * 6.979579925537109
Epoch 530, val loss: 0.8233312368392944
Epoch 540, training loss: 0.024284062907099724 = 0.017349014058709145 + 0.001 * 6.935049057006836
Epoch 540, val loss: 0.8319914937019348
Epoch 550, training loss: 0.023101171478629112 = 0.01618819683790207 + 0.001 * 6.912974834442139
Epoch 550, val loss: 0.8405830264091492
Epoch 560, training loss: 0.02204720675945282 = 0.015138027258217335 + 0.001 * 6.909179210662842
Epoch 560, val loss: 0.8489711284637451
Epoch 570, training loss: 0.021085139364004135 = 0.014185536652803421 + 0.001 * 6.899602890014648
Epoch 570, val loss: 0.8572043776512146
Epoch 580, training loss: 0.020220544189214706 = 0.0133186224848032 + 0.001 * 6.901920795440674
Epoch 580, val loss: 0.8652806878089905
Epoch 590, training loss: 0.019459091126918793 = 0.012528837658464909 + 0.001 * 6.930253505706787
Epoch 590, val loss: 0.8731576800346375
Epoch 600, training loss: 0.018711693584918976 = 0.011808376759290695 + 0.001 * 6.903316020965576
Epoch 600, val loss: 0.8808341026306152
Epoch 610, training loss: 0.018046759068965912 = 0.011148539371788502 + 0.001 * 6.898218631744385
Epoch 610, val loss: 0.8883443474769592
Epoch 620, training loss: 0.01743447594344616 = 0.010543020442128181 + 0.001 * 6.891454696655273
Epoch 620, val loss: 0.8956787586212158
Epoch 630, training loss: 0.016874704509973526 = 0.009986711665987968 + 0.001 * 6.887992858886719
Epoch 630, val loss: 0.90285724401474
Epoch 640, training loss: 0.016374483704566956 = 0.009474104270339012 + 0.001 * 6.900378227233887
Epoch 640, val loss: 0.9098761081695557
Epoch 650, training loss: 0.015886899083852768 = 0.009000882506370544 + 0.001 * 6.886017322540283
Epoch 650, val loss: 0.9167498350143433
Epoch 660, training loss: 0.015445472672581673 = 0.008563184179365635 + 0.001 * 6.882287979125977
Epoch 660, val loss: 0.9234641790390015
Epoch 670, training loss: 0.015101216733455658 = 0.008157516829669476 + 0.001 * 6.943699836730957
Epoch 670, val loss: 0.9300403594970703
Epoch 680, training loss: 0.014664871618151665 = 0.007780914194881916 + 0.001 * 6.883957386016846
Epoch 680, val loss: 0.936459481716156
Epoch 690, training loss: 0.01430922094732523 = 0.0074308146722614765 + 0.001 * 6.878406047821045
Epoch 690, val loss: 0.9427434206008911
Epoch 700, training loss: 0.013982350938022137 = 0.007104875054210424 + 0.001 * 6.877475738525391
Epoch 700, val loss: 0.9489140510559082
Epoch 710, training loss: 0.01368760410696268 = 0.006800904870033264 + 0.001 * 6.8866987228393555
Epoch 710, val loss: 0.9549499750137329
Epoch 720, training loss: 0.013391301035881042 = 0.006516963243484497 + 0.001 * 6.874338150024414
Epoch 720, val loss: 0.9608289003372192
Epoch 730, training loss: 0.013127809390425682 = 0.006251310929656029 + 0.001 * 6.876497745513916
Epoch 730, val loss: 0.9665862917900085
Epoch 740, training loss: 0.012887172400951385 = 0.006002322304993868 + 0.001 * 6.8848490715026855
Epoch 740, val loss: 0.9722483158111572
Epoch 750, training loss: 0.01264383178204298 = 0.00576913682743907 + 0.001 * 6.87469482421875
Epoch 750, val loss: 0.9778057932853699
Epoch 760, training loss: 0.012418264523148537 = 0.0055501810275018215 + 0.001 * 6.8680830001831055
Epoch 760, val loss: 0.9832391738891602
Epoch 770, training loss: 0.01221584901213646 = 0.0053442674688994884 + 0.001 * 6.871581077575684
Epoch 770, val loss: 0.9885256290435791
Epoch 780, training loss: 0.012014497071504593 = 0.00515039311721921 + 0.001 * 6.8641037940979
Epoch 780, val loss: 0.9937291741371155
Epoch 790, training loss: 0.011828654445707798 = 0.004967439919710159 + 0.001 * 6.8612141609191895
Epoch 790, val loss: 0.99882972240448
Epoch 800, training loss: 0.011668814346194267 = 0.004794150125235319 + 0.001 * 6.874663829803467
Epoch 800, val loss: 1.0038546323776245
Epoch 810, training loss: 0.011483625508844852 = 0.004628985654562712 + 0.001 * 6.854639530181885
Epoch 810, val loss: 1.0088187456130981
Epoch 820, training loss: 0.011331428773701191 = 0.004470456391572952 + 0.001 * 6.860971927642822
Epoch 820, val loss: 1.013731598854065
Epoch 830, training loss: 0.011170108802616596 = 0.004317568149417639 + 0.001 * 6.852540493011475
Epoch 830, val loss: 1.0186387300491333
Epoch 840, training loss: 0.011054730974137783 = 0.004170051775872707 + 0.001 * 6.884678840637207
Epoch 840, val loss: 1.0235522985458374
Epoch 850, training loss: 0.010886717587709427 = 0.004027763847261667 + 0.001 * 6.85895299911499
Epoch 850, val loss: 1.0284388065338135
Epoch 860, training loss: 0.010727036744356155 = 0.0038909048307687044 + 0.001 * 6.836132049560547
Epoch 860, val loss: 1.0332790613174438
Epoch 870, training loss: 0.010598694905638695 = 0.0037596169859170914 + 0.001 * 6.839077472686768
Epoch 870, val loss: 1.0381096601486206
Epoch 880, training loss: 0.010467134416103363 = 0.00363387749530375 + 0.001 * 6.833256721496582
Epoch 880, val loss: 1.042898178100586
Epoch 890, training loss: 0.010473307222127914 = 0.003513476811349392 + 0.001 * 6.959830284118652
Epoch 890, val loss: 1.04764986038208
Epoch 900, training loss: 0.01022949069738388 = 0.0033987783826887608 + 0.001 * 6.830711364746094
Epoch 900, val loss: 1.0523450374603271
Epoch 910, training loss: 0.01011055987328291 = 0.003289374290034175 + 0.001 * 6.82118558883667
Epoch 910, val loss: 1.0569454431533813
Epoch 920, training loss: 0.010016259737312794 = 0.0031850452069193125 + 0.001 * 6.831214427947998
Epoch 920, val loss: 1.0615264177322388
Epoch 930, training loss: 0.00992551352828741 = 0.003085579490289092 + 0.001 * 6.8399338722229
Epoch 930, val loss: 1.0660326480865479
Epoch 940, training loss: 0.009850548580288887 = 0.0029905897099524736 + 0.001 * 6.859958648681641
Epoch 940, val loss: 1.070479393005371
Epoch 950, training loss: 0.009701991453766823 = 0.002900102874264121 + 0.001 * 6.8018879890441895
Epoch 950, val loss: 1.0748575925827026
Epoch 960, training loss: 0.009616279043257236 = 0.0028138349298387766 + 0.001 * 6.802443981170654
Epoch 960, val loss: 1.079178810119629
Epoch 970, training loss: 0.009560108184814453 = 0.0027316557243466377 + 0.001 * 6.8284525871276855
Epoch 970, val loss: 1.0833954811096191
Epoch 980, training loss: 0.00949578732252121 = 0.0026534986682236195 + 0.001 * 6.842288017272949
Epoch 980, val loss: 1.087562918663025
Epoch 990, training loss: 0.009379095397889614 = 0.0025792326778173447 + 0.001 * 6.799862384796143
Epoch 990, val loss: 1.0916523933410645
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7786
Flip ASR: 0.7467/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9628002643585205 = 1.9544264078140259 + 0.001 * 8.373902320861816
Epoch 0, val loss: 1.9634191989898682
Epoch 10, training loss: 1.9530035257339478 = 1.9446296691894531 + 0.001 * 8.373858451843262
Epoch 10, val loss: 1.9535115957260132
Epoch 20, training loss: 1.9413206577301025 = 1.9329469203948975 + 0.001 * 8.373682022094727
Epoch 20, val loss: 1.9414927959442139
Epoch 30, training loss: 1.9255717992782593 = 1.9171985387802124 + 0.001 * 8.373303413391113
Epoch 30, val loss: 1.9254685640335083
Epoch 40, training loss: 1.9027646780014038 = 1.8943921327590942 + 0.001 * 8.372492790222168
Epoch 40, val loss: 1.9027795791625977
Epoch 50, training loss: 1.8697465658187866 = 1.8613760471343994 + 0.001 * 8.370492935180664
Epoch 50, val loss: 1.8709927797317505
Epoch 60, training loss: 1.8263756036758423 = 1.81801176071167 + 0.001 * 8.363860130310059
Epoch 60, val loss: 1.831013560295105
Epoch 70, training loss: 1.778862476348877 = 1.7705308198928833 + 0.001 * 8.33168888092041
Epoch 70, val loss: 1.787352442741394
Epoch 80, training loss: 1.7267944812774658 = 1.7187350988388062 + 0.001 * 8.059357643127441
Epoch 80, val loss: 1.7354134321212769
Epoch 90, training loss: 1.6578809022903442 = 1.650191068649292 + 0.001 * 7.68980073928833
Epoch 90, val loss: 1.6694270372390747
Epoch 100, training loss: 1.566574215888977 = 1.55906081199646 + 0.001 * 7.5134477615356445
Epoch 100, val loss: 1.589428424835205
Epoch 110, training loss: 1.4561121463775635 = 1.4488531351089478 + 0.001 * 7.258983135223389
Epoch 110, val loss: 1.4965811967849731
Epoch 120, training loss: 1.3411537408828735 = 1.3340246677398682 + 0.001 * 7.129021644592285
Epoch 120, val loss: 1.4012199640274048
Epoch 130, training loss: 1.2325398921966553 = 1.2254278659820557 + 0.001 * 7.111987590789795
Epoch 130, val loss: 1.314501166343689
Epoch 140, training loss: 1.1326097249984741 = 1.125516414642334 + 0.001 * 7.093273162841797
Epoch 140, val loss: 1.2381988763809204
Epoch 150, training loss: 1.03817880153656 = 1.0310887098312378 + 0.001 * 7.090097904205322
Epoch 150, val loss: 1.167407512664795
Epoch 160, training loss: 0.9456746578216553 = 0.9385867714881897 + 0.001 * 7.087896823883057
Epoch 160, val loss: 1.0987714529037476
Epoch 170, training loss: 0.8542260527610779 = 0.8471408486366272 + 0.001 * 7.0851945877075195
Epoch 170, val loss: 1.0311548709869385
Epoch 180, training loss: 0.7647369503974915 = 0.7576562166213989 + 0.001 * 7.080747127532959
Epoch 180, val loss: 0.964892566204071
Epoch 190, training loss: 0.6791579127311707 = 0.6720828413963318 + 0.001 * 7.075093746185303
Epoch 190, val loss: 0.9022992849349976
Epoch 200, training loss: 0.5997303128242493 = 0.592662513256073 + 0.001 * 7.067823886871338
Epoch 200, val loss: 0.8464370369911194
Epoch 210, training loss: 0.5282016396522522 = 0.5211434364318848 + 0.001 * 7.0581746101379395
Epoch 210, val loss: 0.8007223606109619
Epoch 220, training loss: 0.4650709331035614 = 0.4580261707305908 + 0.001 * 7.044764041900635
Epoch 220, val loss: 0.766639232635498
Epoch 230, training loss: 0.4099330008029938 = 0.4029061198234558 + 0.001 * 7.026889324188232
Epoch 230, val loss: 0.7432749271392822
Epoch 240, training loss: 0.3620365262031555 = 0.3550291657447815 + 0.001 * 7.007345199584961
Epoch 240, val loss: 0.7284704446792603
Epoch 250, training loss: 0.32048889994621277 = 0.3134935200214386 + 0.001 * 6.99538516998291
Epoch 250, val loss: 0.7199047803878784
Epoch 260, training loss: 0.28420552611351013 = 0.27721354365348816 + 0.001 * 6.991970062255859
Epoch 260, val loss: 0.7165207862854004
Epoch 270, training loss: 0.2521619498729706 = 0.2451752871274948 + 0.001 * 6.9866533279418945
Epoch 270, val loss: 0.7172667384147644
Epoch 280, training loss: 0.22358715534210205 = 0.21660086512565613 + 0.001 * 6.986291408538818
Epoch 280, val loss: 0.720771849155426
Epoch 290, training loss: 0.19801659882068634 = 0.19102996587753296 + 0.001 * 6.986630916595459
Epoch 290, val loss: 0.7262125611305237
Epoch 300, training loss: 0.17520098388195038 = 0.16821342706680298 + 0.001 * 6.987550258636475
Epoch 300, val loss: 0.733562707901001
Epoch 310, training loss: 0.1550017148256302 = 0.14801345765590668 + 0.001 * 6.988254070281982
Epoch 310, val loss: 0.7425769567489624
Epoch 320, training loss: 0.13729055225849152 = 0.13030201196670532 + 0.001 * 6.988546848297119
Epoch 320, val loss: 0.7531424760818481
Epoch 330, training loss: 0.12188661843538284 = 0.11489787697792053 + 0.001 * 6.988740921020508
Epoch 330, val loss: 0.7651820182800293
Epoch 340, training loss: 0.10856615006923676 = 0.10157804191112518 + 0.001 * 6.9881062507629395
Epoch 340, val loss: 0.7785800099372864
Epoch 350, training loss: 0.09707947075366974 = 0.09009202569723129 + 0.001 * 6.987444877624512
Epoch 350, val loss: 0.7932029962539673
Epoch 360, training loss: 0.08717384934425354 = 0.08018793165683746 + 0.001 * 6.985918998718262
Epoch 360, val loss: 0.8087754845619202
Epoch 370, training loss: 0.07861361652612686 = 0.0716305822134018 + 0.001 * 6.983034610748291
Epoch 370, val loss: 0.8250806331634521
Epoch 380, training loss: 0.0711924359202385 = 0.06420738995075226 + 0.001 * 6.985047340393066
Epoch 380, val loss: 0.8419628739356995
Epoch 390, training loss: 0.06472445279359818 = 0.057747066020965576 + 0.001 * 6.97738790512085
Epoch 390, val loss: 0.8590999245643616
Epoch 400, training loss: 0.05909489095211029 = 0.05212559551000595 + 0.001 * 6.969295024871826
Epoch 400, val loss: 0.876380443572998
Epoch 410, training loss: 0.05421459674835205 = 0.04720332473516464 + 0.001 * 7.011270523071289
Epoch 410, val loss: 0.8937605023384094
Epoch 420, training loss: 0.04984753951430321 = 0.042877890169620514 + 0.001 * 6.969648361206055
Epoch 420, val loss: 0.9111142158508301
Epoch 430, training loss: 0.04601595550775528 = 0.039059024304151535 + 0.001 * 6.9569292068481445
Epoch 430, val loss: 0.9285098910331726
Epoch 440, training loss: 0.042611703276634216 = 0.03566983342170715 + 0.001 * 6.941870212554932
Epoch 440, val loss: 0.9457048773765564
Epoch 450, training loss: 0.03959371894598007 = 0.032655712217092514 + 0.001 * 6.9380059242248535
Epoch 450, val loss: 0.9627854824066162
Epoch 460, training loss: 0.03691340982913971 = 0.029973229393363 + 0.001 * 6.940178394317627
Epoch 460, val loss: 0.9795842170715332
Epoch 470, training loss: 0.03449409455060959 = 0.02757587470114231 + 0.001 * 6.9182209968566895
Epoch 470, val loss: 0.9961870908737183
Epoch 480, training loss: 0.032350603491067886 = 0.025428274646401405 + 0.001 * 6.922329902648926
Epoch 480, val loss: 1.0124396085739136
Epoch 490, training loss: 0.03042709082365036 = 0.023500310257077217 + 0.001 * 6.926781177520752
Epoch 490, val loss: 1.0283530950546265
Epoch 500, training loss: 0.02867119386792183 = 0.0217663012444973 + 0.001 * 6.9048919677734375
Epoch 500, val loss: 1.0439071655273438
Epoch 510, training loss: 0.027111442759633064 = 0.020203201100230217 + 0.001 * 6.908240795135498
Epoch 510, val loss: 1.0591870546340942
Epoch 520, training loss: 0.0256972499191761 = 0.018791282549500465 + 0.001 * 6.905967712402344
Epoch 520, val loss: 1.0740433931350708
Epoch 530, training loss: 0.024415329098701477 = 0.017513029277324677 + 0.001 * 6.902298927307129
Epoch 530, val loss: 1.0885783433914185
Epoch 540, training loss: 0.02323944866657257 = 0.016353653743863106 + 0.001 * 6.8857951164245605
Epoch 540, val loss: 1.1027002334594727
Epoch 550, training loss: 0.022183213382959366 = 0.015299823135137558 + 0.001 * 6.883389949798584
Epoch 550, val loss: 1.1164604425430298
Epoch 560, training loss: 0.021227609366178513 = 0.014340000227093697 + 0.001 * 6.887609481811523
Epoch 560, val loss: 1.129866361618042
Epoch 570, training loss: 0.020336108282208443 = 0.013464383780956268 + 0.001 * 6.8717241287231445
Epoch 570, val loss: 1.1428898572921753
Epoch 580, training loss: 0.019556166604161263 = 0.012663960456848145 + 0.001 * 6.892206192016602
Epoch 580, val loss: 1.1555674076080322
Epoch 590, training loss: 0.01880861632525921 = 0.01193084567785263 + 0.001 * 6.87777042388916
Epoch 590, val loss: 1.1679359674453735
Epoch 600, training loss: 0.018124323338270187 = 0.011258264072239399 + 0.001 * 6.866058349609375
Epoch 600, val loss: 1.179945468902588
Epoch 610, training loss: 0.017562026157975197 = 0.010640054009854794 + 0.001 * 6.921971797943115
Epoch 610, val loss: 1.1916396617889404
Epoch 620, training loss: 0.01694444939494133 = 0.01007056050002575 + 0.001 * 6.8738884925842285
Epoch 620, val loss: 1.203025460243225
Epoch 630, training loss: 0.016399025917053223 = 0.009545182809233665 + 0.001 * 6.853842258453369
Epoch 630, val loss: 1.2141194343566895
Epoch 640, training loss: 0.015918821096420288 = 0.00905972346663475 + 0.001 * 6.859096527099609
Epoch 640, val loss: 1.2249391078948975
Epoch 650, training loss: 0.0154768917709589 = 0.008610473945736885 + 0.001 * 6.866417407989502
Epoch 650, val loss: 1.2354811429977417
Epoch 660, training loss: 0.015059493482112885 = 0.008194156922399998 + 0.001 * 6.865336894989014
Epoch 660, val loss: 1.2457472085952759
Epoch 670, training loss: 0.014661623165011406 = 0.007807587739080191 + 0.001 * 6.854035377502441
Epoch 670, val loss: 1.2557770013809204
Epoch 680, training loss: 0.014291871339082718 = 0.007448249030858278 + 0.001 * 6.843621730804443
Epoch 680, val loss: 1.2655644416809082
Epoch 690, training loss: 0.01396444533020258 = 0.007113705854862928 + 0.001 * 6.850739002227783
Epoch 690, val loss: 1.2751009464263916
Epoch 700, training loss: 0.01363967265933752 = 0.006801745388656855 + 0.001 * 6.837926864624023
Epoch 700, val loss: 1.2844069004058838
Epoch 710, training loss: 0.013347934931516647 = 0.006510528270155191 + 0.001 * 6.837406635284424
Epoch 710, val loss: 1.2934842109680176
Epoch 720, training loss: 0.013093523681163788 = 0.006238317582756281 + 0.001 * 6.855205535888672
Epoch 720, val loss: 1.3023618459701538
Epoch 730, training loss: 0.01283392496407032 = 0.005983615759760141 + 0.001 * 6.850308418273926
Epoch 730, val loss: 1.3110243082046509
Epoch 740, training loss: 0.012594059109687805 = 0.0057449182495474815 + 0.001 * 6.849140644073486
Epoch 740, val loss: 1.319473385810852
Epoch 750, training loss: 0.012358528561890125 = 0.0055207484401762486 + 0.001 * 6.837779998779297
Epoch 750, val loss: 1.3277446031570435
Epoch 760, training loss: 0.012136522680521011 = 0.005310044623911381 + 0.001 * 6.826478004455566
Epoch 760, val loss: 1.3358328342437744
Epoch 770, training loss: 0.011974427849054337 = 0.005111859645694494 + 0.001 * 6.862567901611328
Epoch 770, val loss: 1.343700885772705
Epoch 780, training loss: 0.01175844855606556 = 0.004925255198031664 + 0.001 * 6.833192825317383
Epoch 780, val loss: 1.3514254093170166
Epoch 790, training loss: 0.011603396385908127 = 0.004749626386910677 + 0.001 * 6.853769779205322
Epoch 790, val loss: 1.358985424041748
Epoch 800, training loss: 0.011402545496821404 = 0.004583905450999737 + 0.001 * 6.818640232086182
Epoch 800, val loss: 1.3663798570632935
Epoch 810, training loss: 0.011243769899010658 = 0.0044273873791098595 + 0.001 * 6.816382884979248
Epoch 810, val loss: 1.3736196756362915
Epoch 820, training loss: 0.01109287515282631 = 0.0042793587781488895 + 0.001 * 6.813515663146973
Epoch 820, val loss: 1.3807146549224854
Epoch 830, training loss: 0.010959727689623833 = 0.004139160271733999 + 0.001 * 6.820566654205322
Epoch 830, val loss: 1.3876652717590332
Epoch 840, training loss: 0.010839294642210007 = 0.004006360191851854 + 0.001 * 6.832934379577637
Epoch 840, val loss: 1.3944538831710815
Epoch 850, training loss: 0.010698530822992325 = 0.003880532691255212 + 0.001 * 6.817997932434082
Epoch 850, val loss: 1.401102900505066
Epoch 860, training loss: 0.010564032010734081 = 0.003761094529181719 + 0.001 * 6.802937030792236
Epoch 860, val loss: 1.4076530933380127
Epoch 870, training loss: 0.01045521441847086 = 0.0036476347595453262 + 0.001 * 6.807579517364502
Epoch 870, val loss: 1.4140453338623047
Epoch 880, training loss: 0.010385286994278431 = 0.0035398141480982304 + 0.001 * 6.84547233581543
Epoch 880, val loss: 1.4203166961669922
Epoch 890, training loss: 0.010240274481475353 = 0.0034372671507298946 + 0.001 * 6.803007125854492
Epoch 890, val loss: 1.4264501333236694
Epoch 900, training loss: 0.01015201024711132 = 0.0033396687358617783 + 0.001 * 6.812341213226318
Epoch 900, val loss: 1.4324891567230225
Epoch 910, training loss: 0.010048216208815575 = 0.003246673382818699 + 0.001 * 6.801541805267334
Epoch 910, val loss: 1.4384037256240845
Epoch 920, training loss: 0.009976472705602646 = 0.0031579891219735146 + 0.001 * 6.818482875823975
Epoch 920, val loss: 1.4441941976547241
Epoch 930, training loss: 0.009874586015939713 = 0.0030733596067875624 + 0.001 * 6.8012261390686035
Epoch 930, val loss: 1.4499050378799438
Epoch 940, training loss: 0.00978562980890274 = 0.0029925729613751173 + 0.001 * 6.793056964874268
Epoch 940, val loss: 1.4554895162582397
Epoch 950, training loss: 0.009719687514007092 = 0.0029153835494071245 + 0.001 * 6.8043036460876465
Epoch 950, val loss: 1.4609990119934082
Epoch 960, training loss: 0.00964103639125824 = 0.002841617912054062 + 0.001 * 6.7994184494018555
Epoch 960, val loss: 1.4663878679275513
Epoch 970, training loss: 0.009557971730828285 = 0.0027710357680916786 + 0.001 * 6.786935806274414
Epoch 970, val loss: 1.4716730117797852
Epoch 980, training loss: 0.009529735893011093 = 0.0027034503873437643 + 0.001 * 6.826284885406494
Epoch 980, val loss: 1.4768967628479004
Epoch 990, training loss: 0.009425666183233261 = 0.0026386366225779057 + 0.001 * 6.787029266357422
Epoch 990, val loss: 1.4820252656936646
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8413
Flip ASR: 0.8178/225 nodes
The final ASR:0.70603, 0.14920, Accuracy:0.81235, 0.01823
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9472])
updated graph: torch.Size([2, 10556])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83333, 0.00605
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9574973583221436 = 1.949123501777649 + 0.001 * 8.373876571655273
Epoch 0, val loss: 1.9450360536575317
Epoch 10, training loss: 1.9469094276428223 = 1.9385355710983276 + 0.001 * 8.373824119567871
Epoch 10, val loss: 1.93545401096344
Epoch 20, training loss: 1.9341487884521484 = 1.925775170326233 + 0.001 * 8.373635292053223
Epoch 20, val loss: 1.9235328435897827
Epoch 30, training loss: 1.9165089130401611 = 1.9081357717514038 + 0.001 * 8.373194694519043
Epoch 30, val loss: 1.9066648483276367
Epoch 40, training loss: 1.890777349472046 = 1.882405161857605 + 0.001 * 8.372138977050781
Epoch 40, val loss: 1.8820879459381104
Epoch 50, training loss: 1.8549209833145142 = 1.846551775932312 + 0.001 * 8.369245529174805
Epoch 50, val loss: 1.8494632244110107
Epoch 60, training loss: 1.8137197494506836 = 1.8053604364395142 + 0.001 * 8.359292030334473
Epoch 60, val loss: 1.816127061843872
Epoch 70, training loss: 1.7757676839828491 = 1.7674577236175537 + 0.001 * 8.31001091003418
Epoch 70, val loss: 1.7884175777435303
Epoch 80, training loss: 1.7305419445037842 = 1.7225894927978516 + 0.001 * 7.952455997467041
Epoch 80, val loss: 1.7506520748138428
Epoch 90, training loss: 1.6688669919967651 = 1.6610710620880127 + 0.001 * 7.795931816101074
Epoch 90, val loss: 1.6967968940734863
Epoch 100, training loss: 1.5866479873657227 = 1.5790334939956665 + 0.001 * 7.614504814147949
Epoch 100, val loss: 1.6277542114257812
Epoch 110, training loss: 1.4868433475494385 = 1.479409098625183 + 0.001 * 7.434255123138428
Epoch 110, val loss: 1.54783296585083
Epoch 120, training loss: 1.3801501989364624 = 1.3728137016296387 + 0.001 * 7.336554050445557
Epoch 120, val loss: 1.4647116661071777
Epoch 130, training loss: 1.2738983631134033 = 1.2666438817977905 + 0.001 * 7.254519939422607
Epoch 130, val loss: 1.3845409154891968
Epoch 140, training loss: 1.171754002571106 = 1.1645931005477905 + 0.001 * 7.16089391708374
Epoch 140, val loss: 1.307835578918457
Epoch 150, training loss: 1.0760395526885986 = 1.0689383745193481 + 0.001 * 7.101179122924805
Epoch 150, val loss: 1.2354106903076172
Epoch 160, training loss: 0.9881365895271301 = 0.9810695052146912 + 0.001 * 7.067067623138428
Epoch 160, val loss: 1.1691292524337769
Epoch 170, training loss: 0.9065759181976318 = 0.8995376825332642 + 0.001 * 7.038209915161133
Epoch 170, val loss: 1.107467532157898
Epoch 180, training loss: 0.8282753825187683 = 0.8212710022926331 + 0.001 * 7.00439453125
Epoch 180, val loss: 1.0484286546707153
Epoch 190, training loss: 0.7510996460914612 = 0.7441242933273315 + 0.001 * 6.975332260131836
Epoch 190, val loss: 0.9903820753097534
Epoch 200, training loss: 0.6748782396316528 = 0.6679139137268066 + 0.001 * 6.964304447174072
Epoch 200, val loss: 0.9339689016342163
Epoch 210, training loss: 0.6009148955345154 = 0.5939557552337646 + 0.001 * 6.9591240882873535
Epoch 210, val loss: 0.8811256885528564
Epoch 220, training loss: 0.5309593677520752 = 0.5240016579627991 + 0.001 * 6.957729339599609
Epoch 220, val loss: 0.8341937065124512
Epoch 230, training loss: 0.46610942482948303 = 0.45915353298187256 + 0.001 * 6.955904483795166
Epoch 230, val loss: 0.7949140667915344
Epoch 240, training loss: 0.4065554440021515 = 0.3996012508869171 + 0.001 * 6.95418119430542
Epoch 240, val loss: 0.7633579969406128
Epoch 250, training loss: 0.35199588537216187 = 0.34504348039627075 + 0.001 * 6.952413082122803
Epoch 250, val loss: 0.7386247515678406
Epoch 260, training loss: 0.30224400758743286 = 0.29529324173927307 + 0.001 * 6.950769424438477
Epoch 260, val loss: 0.7197919487953186
Epoch 270, training loss: 0.2574383318424225 = 0.250488817691803 + 0.001 * 6.949514865875244
Epoch 270, val loss: 0.7063934803009033
Epoch 280, training loss: 0.21818096935749054 = 0.21123211085796356 + 0.001 * 6.9488525390625
Epoch 280, val loss: 0.6984370946884155
Epoch 290, training loss: 0.18484194576740265 = 0.17789331078529358 + 0.001 * 6.948631286621094
Epoch 290, val loss: 0.6959486603736877
Epoch 300, training loss: 0.1572786420583725 = 0.15033142268657684 + 0.001 * 6.9472150802612305
Epoch 300, val loss: 0.6986480951309204
Epoch 310, training loss: 0.13488635420799255 = 0.12793944776058197 + 0.001 * 6.946906566619873
Epoch 310, val loss: 0.7057709097862244
Epoch 320, training loss: 0.11679217964410782 = 0.10984569787979126 + 0.001 * 6.946483135223389
Epoch 320, val loss: 0.7163560390472412
Epoch 330, training loss: 0.10212216526269913 = 0.09517635405063629 + 0.001 * 6.945809364318848
Epoch 330, val loss: 0.7295132279396057
Epoch 340, training loss: 0.09013611078262329 = 0.08319062739610672 + 0.001 * 6.945484161376953
Epoch 340, val loss: 0.7443912029266357
Epoch 350, training loss: 0.08024759590625763 = 0.07330139726400375 + 0.001 * 6.946195125579834
Epoch 350, val loss: 0.7602102756500244
Epoch 360, training loss: 0.0719999223947525 = 0.06505872309207916 + 0.001 * 6.941202163696289
Epoch 360, val loss: 0.7764639258384705
Epoch 370, training loss: 0.06506121158599854 = 0.05811960622668266 + 0.001 * 6.941603660583496
Epoch 370, val loss: 0.7927459478378296
Epoch 380, training loss: 0.05915745720267296 = 0.052218787372112274 + 0.001 * 6.938669681549072
Epoch 380, val loss: 0.8086957335472107
Epoch 390, training loss: 0.05408601090312004 = 0.047152917832136154 + 0.001 * 6.9330925941467285
Epoch 390, val loss: 0.8242858648300171
Epoch 400, training loss: 0.04969881474971771 = 0.04276501387357712 + 0.001 * 6.933799743652344
Epoch 400, val loss: 0.8394189476966858
Epoch 410, training loss: 0.045862533152103424 = 0.038936592638492584 + 0.001 * 6.925939559936523
Epoch 410, val loss: 0.8542156219482422
Epoch 420, training loss: 0.04251493513584137 = 0.03557555750012398 + 0.001 * 6.939377784729004
Epoch 420, val loss: 0.868530809879303
Epoch 430, training loss: 0.03952169790863991 = 0.03260893374681473 + 0.001 * 6.912765026092529
Epoch 430, val loss: 0.882571816444397
Epoch 440, training loss: 0.03687530383467674 = 0.029978955164551735 + 0.001 * 6.896347999572754
Epoch 440, val loss: 0.8962538242340088
Epoch 450, training loss: 0.034541696310043335 = 0.027638595551252365 + 0.001 * 6.903099536895752
Epoch 450, val loss: 0.9096345901489258
Epoch 460, training loss: 0.03243720903992653 = 0.02554897964000702 + 0.001 * 6.888227462768555
Epoch 460, val loss: 0.9227120280265808
Epoch 470, training loss: 0.030546698719263077 = 0.023677412420511246 + 0.001 * 6.86928653717041
Epoch 470, val loss: 0.9354007244110107
Epoch 480, training loss: 0.028875447809696198 = 0.021996287629008293 + 0.001 * 6.879159450531006
Epoch 480, val loss: 0.947806179523468
Epoch 490, training loss: 0.027342811226844788 = 0.020482104271650314 + 0.001 * 6.860705852508545
Epoch 490, val loss: 0.9598608016967773
Epoch 500, training loss: 0.02597801387310028 = 0.019115068018436432 + 0.001 * 6.862945556640625
Epoch 500, val loss: 0.9716309905052185
Epoch 510, training loss: 0.024737395346164703 = 0.017877325415611267 + 0.001 * 6.860069751739502
Epoch 510, val loss: 0.9830840826034546
Epoch 520, training loss: 0.02358519285917282 = 0.016753822565078735 + 0.001 * 6.831369400024414
Epoch 520, val loss: 0.9942324161529541
Epoch 530, training loss: 0.022577760741114616 = 0.015731366351246834 + 0.001 * 6.8463945388793945
Epoch 530, val loss: 1.005070447921753
Epoch 540, training loss: 0.02162417396903038 = 0.014798609539866447 + 0.001 * 6.825562953948975
Epoch 540, val loss: 1.0156638622283936
Epoch 550, training loss: 0.02076934091746807 = 0.01394590549170971 + 0.001 * 6.823434829711914
Epoch 550, val loss: 1.025940179824829
Epoch 560, training loss: 0.01999051868915558 = 0.013164582662284374 + 0.001 * 6.825934886932373
Epoch 560, val loss: 1.0359749794006348
Epoch 570, training loss: 0.01925574615597725 = 0.012447228655219078 + 0.001 * 6.808516979217529
Epoch 570, val loss: 1.0457110404968262
Epoch 580, training loss: 0.01859811320900917 = 0.01178724505007267 + 0.001 * 6.8108673095703125
Epoch 580, val loss: 1.055226445198059
Epoch 590, training loss: 0.018000397831201553 = 0.011178947985172272 + 0.001 * 6.821449279785156
Epoch 590, val loss: 1.0644699335098267
Epoch 600, training loss: 0.017434101551771164 = 0.010617109015583992 + 0.001 * 6.816993236541748
Epoch 600, val loss: 1.0735095739364624
Epoch 610, training loss: 0.016894366592168808 = 0.010097230784595013 + 0.001 * 6.797134876251221
Epoch 610, val loss: 1.0823243856430054
Epoch 620, training loss: 0.016453005373477936 = 0.00961532536894083 + 0.001 * 6.837679386138916
Epoch 620, val loss: 1.0908838510513306
Epoch 630, training loss: 0.015954025089740753 = 0.009167937561869621 + 0.001 * 6.786087989807129
Epoch 630, val loss: 1.0992958545684814
Epoch 640, training loss: 0.015546128153800964 = 0.008751977235078812 + 0.001 * 6.794150352478027
Epoch 640, val loss: 1.1074820756912231
Epoch 650, training loss: 0.015154293738305569 = 0.008364563807845116 + 0.001 * 6.789729595184326
Epoch 650, val loss: 1.115464687347412
Epoch 660, training loss: 0.014783667400479317 = 0.008003073744475842 + 0.001 * 6.7805938720703125
Epoch 660, val loss: 1.1232597827911377
Epoch 670, training loss: 0.014454925432801247 = 0.007665291894227266 + 0.001 * 6.789633750915527
Epoch 670, val loss: 1.1308625936508179
Epoch 680, training loss: 0.014124792069196701 = 0.007349441293627024 + 0.001 * 6.775350093841553
Epoch 680, val loss: 1.1383507251739502
Epoch 690, training loss: 0.013830630108714104 = 0.007053548935800791 + 0.001 * 6.777081489562988
Epoch 690, val loss: 1.145637035369873
Epoch 700, training loss: 0.013556480407714844 = 0.006776048801839352 + 0.001 * 6.780431270599365
Epoch 700, val loss: 1.1526964902877808
Epoch 710, training loss: 0.013294858857989311 = 0.006515475455671549 + 0.001 * 6.779383182525635
Epoch 710, val loss: 1.1596914529800415
Epoch 720, training loss: 0.013063540682196617 = 0.0062705352902412415 + 0.001 * 6.793004989624023
Epoch 720, val loss: 1.166459083557129
Epoch 730, training loss: 0.01282762922346592 = 0.006039880216121674 + 0.001 * 6.787748336791992
Epoch 730, val loss: 1.1731492280960083
Epoch 740, training loss: 0.012601585127413273 = 0.005822570528835058 + 0.001 * 6.7790141105651855
Epoch 740, val loss: 1.1796561479568481
Epoch 750, training loss: 0.012375850230455399 = 0.005617593880742788 + 0.001 * 6.758255958557129
Epoch 750, val loss: 1.1860276460647583
Epoch 760, training loss: 0.012193708680570126 = 0.005424022674560547 + 0.001 * 6.769685745239258
Epoch 760, val loss: 1.1922664642333984
Epoch 770, training loss: 0.012021053582429886 = 0.005241059698164463 + 0.001 * 6.779993534088135
Epoch 770, val loss: 1.1983921527862549
Epoch 780, training loss: 0.011836985126137733 = 0.005067921709269285 + 0.001 * 6.769062519073486
Epoch 780, val loss: 1.2043883800506592
Epoch 790, training loss: 0.011673979461193085 = 0.004903988912701607 + 0.001 * 6.769990921020508
Epoch 790, val loss: 1.2102491855621338
Epoch 800, training loss: 0.011505967006087303 = 0.004748559091240168 + 0.001 * 6.757407188415527
Epoch 800, val loss: 1.2160241603851318
Epoch 810, training loss: 0.011350231245160103 = 0.004601121414452791 + 0.001 * 6.749109268188477
Epoch 810, val loss: 1.2216107845306396
Epoch 820, training loss: 0.011215128004550934 = 0.004461091011762619 + 0.001 * 6.754036903381348
Epoch 820, val loss: 1.2271406650543213
Epoch 830, training loss: 0.011085864156484604 = 0.0043280175887048244 + 0.001 * 6.757846355438232
Epoch 830, val loss: 1.2325453758239746
Epoch 840, training loss: 0.010952209122478962 = 0.004201437812298536 + 0.001 * 6.7507710456848145
Epoch 840, val loss: 1.2378556728363037
Epoch 850, training loss: 0.01082373782992363 = 0.004080931190401316 + 0.001 * 6.742806434631348
Epoch 850, val loss: 1.2430813312530518
Epoch 860, training loss: 0.010745820589363575 = 0.0039661237969994545 + 0.001 * 6.779696464538574
Epoch 860, val loss: 1.2481918334960938
Epoch 870, training loss: 0.010604889132082462 = 0.003856697352603078 + 0.001 * 6.748191833496094
Epoch 870, val loss: 1.2531695365905762
Epoch 880, training loss: 0.010518092662096024 = 0.0037522788625210524 + 0.001 * 6.76581335067749
Epoch 880, val loss: 1.2581133842468262
Epoch 890, training loss: 0.010410778224468231 = 0.003652587067335844 + 0.001 * 6.758190155029297
Epoch 890, val loss: 1.2629213333129883
Epoch 900, training loss: 0.010309508070349693 = 0.003557319287210703 + 0.001 * 6.752188682556152
Epoch 900, val loss: 1.2676482200622559
Epoch 910, training loss: 0.010200736112892628 = 0.003466218477115035 + 0.001 * 6.734517574310303
Epoch 910, val loss: 1.2723006010055542
Epoch 920, training loss: 0.010103471577167511 = 0.003379067638888955 + 0.001 * 6.7244038581848145
Epoch 920, val loss: 1.276875376701355
Epoch 930, training loss: 0.01002964936196804 = 0.003295639529824257 + 0.001 * 6.734009742736816
Epoch 930, val loss: 1.2813630104064941
Epoch 940, training loss: 0.00995553471148014 = 0.0032157525420188904 + 0.001 * 6.739781856536865
Epoch 940, val loss: 1.2857716083526611
Epoch 950, training loss: 0.009889969602227211 = 0.003139195032417774 + 0.001 * 6.75077486038208
Epoch 950, val loss: 1.2901028394699097
Epoch 960, training loss: 0.009794470854103565 = 0.0030657541938126087 + 0.001 * 6.7287163734436035
Epoch 960, val loss: 1.2943644523620605
Epoch 970, training loss: 0.009743952192366123 = 0.0029953166376799345 + 0.001 * 6.748635292053223
Epoch 970, val loss: 1.2985434532165527
Epoch 980, training loss: 0.009657619521021843 = 0.0029276907444000244 + 0.001 * 6.729928016662598
Epoch 980, val loss: 1.3026365041732788
Epoch 990, training loss: 0.00959151890128851 = 0.0028627358842641115 + 0.001 * 6.728783130645752
Epoch 990, val loss: 1.3066587448120117
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6900
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9567137956619263 = 1.9483399391174316 + 0.001 * 8.373822212219238
Epoch 0, val loss: 1.9529120922088623
Epoch 10, training loss: 1.9458290338516235 = 1.9374552965164185 + 0.001 * 8.373678207397461
Epoch 10, val loss: 1.941951036453247
Epoch 20, training loss: 1.932276964187622 = 1.9239037036895752 + 0.001 * 8.373285293579102
Epoch 20, val loss: 1.9279199838638306
Epoch 30, training loss: 1.9131603240966797 = 1.9047878980636597 + 0.001 * 8.372403144836426
Epoch 30, val loss: 1.9078868627548218
Epoch 40, training loss: 1.8850877285003662 = 1.876717448234558 + 0.001 * 8.370269775390625
Epoch 40, val loss: 1.8787075281143188
Epoch 50, training loss: 1.8461143970489502 = 1.8377505540847778 + 0.001 * 8.363795280456543
Epoch 50, val loss: 1.839524269104004
Epoch 60, training loss: 1.8013299703598022 = 1.792993187904358 + 0.001 * 8.336737632751465
Epoch 60, val loss: 1.7970516681671143
Epoch 70, training loss: 1.758391261100769 = 1.750227689743042 + 0.001 * 8.163625717163086
Epoch 70, val loss: 1.7564409971237183
Epoch 80, training loss: 1.7049314975738525 = 1.697015404701233 + 0.001 * 7.916142463684082
Epoch 80, val loss: 1.705634593963623
Epoch 90, training loss: 1.632270097732544 = 1.6245431900024414 + 0.001 * 7.726901054382324
Epoch 90, val loss: 1.6422775983810425
Epoch 100, training loss: 1.5416737794876099 = 1.534158706665039 + 0.001 * 7.515044212341309
Epoch 100, val loss: 1.5680732727050781
Epoch 110, training loss: 1.4455572366714478 = 1.4383015632629395 + 0.001 * 7.255721092224121
Epoch 110, val loss: 1.491448163986206
Epoch 120, training loss: 1.35362708568573 = 1.3464369773864746 + 0.001 * 7.190130710601807
Epoch 120, val loss: 1.4204487800598145
Epoch 130, training loss: 1.2663370370864868 = 1.2592071294784546 + 0.001 * 7.129883289337158
Epoch 130, val loss: 1.3549875020980835
Epoch 140, training loss: 1.1815240383148193 = 1.1744368076324463 + 0.001 * 7.087174892425537
Epoch 140, val loss: 1.2934616804122925
Epoch 150, training loss: 1.0965375900268555 = 1.0894747972488403 + 0.001 * 7.0627899169921875
Epoch 150, val loss: 1.2324732542037964
Epoch 160, training loss: 1.0099084377288818 = 1.0028585195541382 + 0.001 * 7.049938201904297
Epoch 160, val loss: 1.171032428741455
Epoch 170, training loss: 0.9223191738128662 = 0.9152807593345642 + 0.001 * 7.038441181182861
Epoch 170, val loss: 1.109724760055542
Epoch 180, training loss: 0.8366663455963135 = 0.8296425938606262 + 0.001 * 7.02373743057251
Epoch 180, val loss: 1.0495638847351074
Epoch 190, training loss: 0.7564270496368408 = 0.7494210004806519 + 0.001 * 7.006033897399902
Epoch 190, val loss: 0.9930785298347473
Epoch 200, training loss: 0.6838074326515198 = 0.6768214106559753 + 0.001 * 6.9860029220581055
Epoch 200, val loss: 0.9433457255363464
Epoch 210, training loss: 0.6191034317016602 = 0.612133264541626 + 0.001 * 6.9701828956604
Epoch 210, val loss: 0.9010850191116333
Epoch 220, training loss: 0.5615405440330505 = 0.5545782446861267 + 0.001 * 6.962317943572998
Epoch 220, val loss: 0.866076648235321
Epoch 230, training loss: 0.5102539658546448 = 0.5032971501350403 + 0.001 * 6.9568257331848145
Epoch 230, val loss: 0.8379760980606079
Epoch 240, training loss: 0.46427860856056213 = 0.4573250114917755 + 0.001 * 6.953600883483887
Epoch 240, val loss: 0.8161143660545349
Epoch 250, training loss: 0.4220220446586609 = 0.41507118940353394 + 0.001 * 6.950859069824219
Epoch 250, val loss: 0.7991880774497986
Epoch 260, training loss: 0.38238900899887085 = 0.37544092535972595 + 0.001 * 6.948092937469482
Epoch 260, val loss: 0.7864957451820374
Epoch 270, training loss: 0.344796359539032 = 0.3378506898880005 + 0.001 * 6.945657730102539
Epoch 270, val loss: 0.7773759961128235
Epoch 280, training loss: 0.309360533952713 = 0.30241671204566956 + 0.001 * 6.943819522857666
Epoch 280, val loss: 0.7710404396057129
Epoch 290, training loss: 0.27643582224845886 = 0.26949483156204224 + 0.001 * 6.940980911254883
Epoch 290, val loss: 0.7677316069602966
Epoch 300, training loss: 0.2463558167219162 = 0.23941749334335327 + 0.001 * 6.938319206237793
Epoch 300, val loss: 0.7674568295478821
Epoch 310, training loss: 0.219172403216362 = 0.21223585307598114 + 0.001 * 6.936546325683594
Epoch 310, val loss: 0.7699769735336304
Epoch 320, training loss: 0.1948014199733734 = 0.18786931037902832 + 0.001 * 6.9321136474609375
Epoch 320, val loss: 0.7750489115715027
Epoch 330, training loss: 0.17327378690242767 = 0.1663440763950348 + 0.001 * 6.929715156555176
Epoch 330, val loss: 0.7828970551490784
Epoch 340, training loss: 0.15423516929149628 = 0.14730212092399597 + 0.001 * 6.933049201965332
Epoch 340, val loss: 0.7930465340614319
Epoch 350, training loss: 0.1374635547399521 = 0.13054326176643372 + 0.001 * 6.920287609100342
Epoch 350, val loss: 0.8051484227180481
Epoch 360, training loss: 0.12266483902931213 = 0.11575280874967575 + 0.001 * 6.9120306968688965
Epoch 360, val loss: 0.8190346956253052
Epoch 370, training loss: 0.10972493141889572 = 0.10275813192129135 + 0.001 * 6.966801643371582
Epoch 370, val loss: 0.8343982696533203
Epoch 380, training loss: 0.09819813072681427 = 0.09129425138235092 + 0.001 * 6.903878211975098
Epoch 380, val loss: 0.8509613871574402
Epoch 390, training loss: 0.08795645833015442 = 0.08106730878353119 + 0.001 * 6.889145851135254
Epoch 390, val loss: 0.86827552318573
Epoch 400, training loss: 0.07895477861166 = 0.07206039130687714 + 0.001 * 6.894384384155273
Epoch 400, val loss: 0.8866276144981384
Epoch 410, training loss: 0.07108598202466965 = 0.06420797854661942 + 0.001 * 6.877999782562256
Epoch 410, val loss: 0.9055071473121643
Epoch 420, training loss: 0.06424039602279663 = 0.057375241070985794 + 0.001 * 6.8651556968688965
Epoch 420, val loss: 0.9245687127113342
Epoch 430, training loss: 0.05831672623753548 = 0.05144498124718666 + 0.001 * 6.871744155883789
Epoch 430, val loss: 0.9439287185668945
Epoch 440, training loss: 0.053139761090278625 = 0.046285826712846756 + 0.001 * 6.853933811187744
Epoch 440, val loss: 0.9634976983070374
Epoch 450, training loss: 0.04863997921347618 = 0.04178764298558235 + 0.001 * 6.852336406707764
Epoch 450, val loss: 0.9831082820892334
Epoch 460, training loss: 0.04469674825668335 = 0.03785174712538719 + 0.001 * 6.8450026512146
Epoch 460, val loss: 1.002561092376709
Epoch 470, training loss: 0.04124636948108673 = 0.03439914807677269 + 0.001 * 6.847219944000244
Epoch 470, val loss: 1.021773338317871
Epoch 480, training loss: 0.038229312747716904 = 0.031363341957330704 + 0.001 * 6.865971088409424
Epoch 480, val loss: 1.0405229330062866
Epoch 490, training loss: 0.03551477938890457 = 0.028685515746474266 + 0.001 * 6.829264163970947
Epoch 490, val loss: 1.0588302612304688
Epoch 500, training loss: 0.03315800428390503 = 0.026318145915865898 + 0.001 * 6.839858055114746
Epoch 500, val loss: 1.0765575170516968
Epoch 510, training loss: 0.031053703278303146 = 0.02421896532177925 + 0.001 * 6.834738731384277
Epoch 510, val loss: 1.0937408208847046
Epoch 520, training loss: 0.029187891632318497 = 0.022352220490574837 + 0.001 * 6.835669994354248
Epoch 520, val loss: 1.110427737236023
Epoch 530, training loss: 0.027511857450008392 = 0.020687319338321686 + 0.001 * 6.824538707733154
Epoch 530, val loss: 1.1266542673110962
Epoch 540, training loss: 0.026034941896796227 = 0.01919698342680931 + 0.001 * 6.837957859039307
Epoch 540, val loss: 1.1423578262329102
Epoch 550, training loss: 0.02468886971473694 = 0.017859455198049545 + 0.001 * 6.829413890838623
Epoch 550, val loss: 1.157463550567627
Epoch 560, training loss: 0.02347925491631031 = 0.016655396670103073 + 0.001 * 6.823858261108398
Epoch 560, val loss: 1.172114610671997
Epoch 570, training loss: 0.02239236794412136 = 0.015569468960165977 + 0.001 * 6.822898864746094
Epoch 570, val loss: 1.1863559484481812
Epoch 580, training loss: 0.021427800878882408 = 0.01458603236824274 + 0.001 * 6.84176778793335
Epoch 580, val loss: 1.2001057863235474
Epoch 590, training loss: 0.02051117643713951 = 0.013693447224795818 + 0.001 * 6.817728519439697
Epoch 590, val loss: 1.2134796380996704
Epoch 600, training loss: 0.019689491018652916 = 0.012881152331829071 + 0.001 * 6.808338642120361
Epoch 600, val loss: 1.2263996601104736
Epoch 610, training loss: 0.018948256969451904 = 0.012138932943344116 + 0.001 * 6.809324264526367
Epoch 610, val loss: 1.238950252532959
Epoch 620, training loss: 0.018284939229488373 = 0.011459620669484138 + 0.001 * 6.825319290161133
Epoch 620, val loss: 1.2511423826217651
Epoch 630, training loss: 0.017647454515099525 = 0.010836725123226643 + 0.001 * 6.810728549957275
Epoch 630, val loss: 1.2629841566085815
Epoch 640, training loss: 0.017060620710253716 = 0.010264878161251545 + 0.001 * 6.795742034912109
Epoch 640, val loss: 1.2745110988616943
Epoch 650, training loss: 0.01653403416275978 = 0.00973851140588522 + 0.001 * 6.795522212982178
Epoch 650, val loss: 1.285714864730835
Epoch 660, training loss: 0.016081061214208603 = 0.009253185242414474 + 0.001 * 6.827875137329102
Epoch 660, val loss: 1.2966116666793823
Epoch 670, training loss: 0.015605486929416656 = 0.00880469661206007 + 0.001 * 6.8007893562316895
Epoch 670, val loss: 1.3072260618209839
Epoch 680, training loss: 0.015189153142273426 = 0.008389454334974289 + 0.001 * 6.799698352813721
Epoch 680, val loss: 1.3175816535949707
Epoch 690, training loss: 0.014788687229156494 = 0.008004207164049149 + 0.001 * 6.784479141235352
Epoch 690, val loss: 1.3276687860488892
Epoch 700, training loss: 0.014446382410824299 = 0.007646042853593826 + 0.001 * 6.800339221954346
Epoch 700, val loss: 1.3374935388565063
Epoch 710, training loss: 0.014093711040914059 = 0.007312375586479902 + 0.001 * 6.781335353851318
Epoch 710, val loss: 1.3470782041549683
Epoch 720, training loss: 0.013786692172288895 = 0.007001353427767754 + 0.001 * 6.785337924957275
Epoch 720, val loss: 1.3564138412475586
Epoch 730, training loss: 0.013511009514331818 = 0.00671102711930871 + 0.001 * 6.799981594085693
Epoch 730, val loss: 1.3655365705490112
Epoch 740, training loss: 0.013220192864537239 = 0.006439877208322287 + 0.001 * 6.780315399169922
Epoch 740, val loss: 1.3744195699691772
Epoch 750, training loss: 0.012999879196286201 = 0.006185982841998339 + 0.001 * 6.8138957023620605
Epoch 750, val loss: 1.383093237876892
Epoch 760, training loss: 0.012731390073895454 = 0.005947888363152742 + 0.001 * 6.783501148223877
Epoch 760, val loss: 1.3915457725524902
Epoch 770, training loss: 0.012522722594439983 = 0.005724440794438124 + 0.001 * 6.798281669616699
Epoch 770, val loss: 1.399810552597046
Epoch 780, training loss: 0.012284431606531143 = 0.00551433814689517 + 0.001 * 6.770092964172363
Epoch 780, val loss: 1.407887578010559
Epoch 790, training loss: 0.012088109739124775 = 0.005316619761288166 + 0.001 * 6.77148962020874
Epoch 790, val loss: 1.4157614707946777
Epoch 800, training loss: 0.011892968788743019 = 0.005130324512720108 + 0.001 * 6.762643814086914
Epoch 800, val loss: 1.4234932661056519
Epoch 810, training loss: 0.011717806570231915 = 0.004954599775373936 + 0.001 * 6.763206481933594
Epoch 810, val loss: 1.4309875965118408
Epoch 820, training loss: 0.01155762281268835 = 0.004788683261722326 + 0.001 * 6.768939018249512
Epoch 820, val loss: 1.4383904933929443
Epoch 830, training loss: 0.011403471231460571 = 0.004631795920431614 + 0.001 * 6.771675109863281
Epoch 830, val loss: 1.4455645084381104
Epoch 840, training loss: 0.011254185810685158 = 0.004483328666538 + 0.001 * 6.770856857299805
Epoch 840, val loss: 1.4526572227478027
Epoch 850, training loss: 0.011145996861159801 = 0.004342701751738787 + 0.001 * 6.803294658660889
Epoch 850, val loss: 1.4595423936843872
Epoch 860, training loss: 0.010966802015900612 = 0.004209357779473066 + 0.001 * 6.757443428039551
Epoch 860, val loss: 1.4663124084472656
Epoch 870, training loss: 0.010885057970881462 = 0.004082844126969576 + 0.001 * 6.802213191986084
Epoch 870, val loss: 1.4729443788528442
Epoch 880, training loss: 0.0107090063393116 = 0.0039626481011509895 + 0.001 * 6.746358394622803
Epoch 880, val loss: 1.4794267416000366
Epoch 890, training loss: 0.010607147589325905 = 0.0038483915850520134 + 0.001 * 6.758755207061768
Epoch 890, val loss: 1.4857624769210815
Epoch 900, training loss: 0.01048100832849741 = 0.003739649895578623 + 0.001 * 6.741358280181885
Epoch 900, val loss: 1.4919847249984741
Epoch 910, training loss: 0.010391810908913612 = 0.0036360956728458405 + 0.001 * 6.755714416503906
Epoch 910, val loss: 1.4980664253234863
Epoch 920, training loss: 0.010301029309630394 = 0.0035374376457184553 + 0.001 * 6.763591289520264
Epoch 920, val loss: 1.5040587186813354
Epoch 930, training loss: 0.010217022150754929 = 0.003443305380642414 + 0.001 * 6.773716926574707
Epoch 930, val loss: 1.5098928213119507
Epoch 940, training loss: 0.010105744004249573 = 0.003353482810780406 + 0.001 * 6.752261161804199
Epoch 940, val loss: 1.5156365633010864
Epoch 950, training loss: 0.010011585429310799 = 0.00326768821105361 + 0.001 * 6.743897438049316
Epoch 950, val loss: 1.5212538242340088
Epoch 960, training loss: 0.009954707697033882 = 0.0031856915447860956 + 0.001 * 6.769016265869141
Epoch 960, val loss: 1.526771903038025
Epoch 970, training loss: 0.009845629334449768 = 0.003107240656390786 + 0.001 * 6.738388538360596
Epoch 970, val loss: 1.532184362411499
Epoch 980, training loss: 0.009767495095729828 = 0.0030321876984089613 + 0.001 * 6.735306739807129
Epoch 980, val loss: 1.5374807119369507
Epoch 990, training loss: 0.009701856411993504 = 0.0029603324364870787 + 0.001 * 6.741523742675781
Epoch 990, val loss: 1.5426863431930542
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.9410
Flip ASR: 0.9289/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.965345025062561 = 1.9569711685180664 + 0.001 * 8.373799324035645
Epoch 0, val loss: 1.954695224761963
Epoch 10, training loss: 1.9530936479568481 = 1.9447202682495117 + 0.001 * 8.373416900634766
Epoch 10, val loss: 1.9408775568008423
Epoch 20, training loss: 1.9376202821731567 = 1.9292477369308472 + 0.001 * 8.372570991516113
Epoch 20, val loss: 1.9225513935089111
Epoch 30, training loss: 1.9161933660507202 = 1.9078223705291748 + 0.001 * 8.37103271484375
Epoch 30, val loss: 1.89663827419281
Epoch 40, training loss: 1.8867896795272827 = 1.8784213066101074 + 0.001 * 8.368374824523926
Epoch 40, val loss: 1.862333059310913
Epoch 50, training loss: 1.8490161895751953 = 1.8406530618667603 + 0.001 * 8.363097190856934
Epoch 50, val loss: 1.8222765922546387
Epoch 60, training loss: 1.8061561584472656 = 1.797809362411499 + 0.001 * 8.346763610839844
Epoch 60, val loss: 1.7819912433624268
Epoch 70, training loss: 1.7659580707550049 = 1.757703185081482 + 0.001 * 8.254886627197266
Epoch 70, val loss: 1.7481945753097534
Epoch 80, training loss: 1.7222645282745361 = 1.7144358158111572 + 0.001 * 7.82865571975708
Epoch 80, val loss: 1.7113929986953735
Epoch 90, training loss: 1.661210536956787 = 1.6535543203353882 + 0.001 * 7.656201362609863
Epoch 90, val loss: 1.6594054698944092
Epoch 100, training loss: 1.5791563987731934 = 1.571583867073059 + 0.001 * 7.572573661804199
Epoch 100, val loss: 1.5912736654281616
Epoch 110, training loss: 1.4821405410766602 = 1.4746520519256592 + 0.001 * 7.488536834716797
Epoch 110, val loss: 1.513358235359192
Epoch 120, training loss: 1.3851065635681152 = 1.3777776956558228 + 0.001 * 7.328872203826904
Epoch 120, val loss: 1.438651204109192
Epoch 130, training loss: 1.295358419418335 = 1.2881760597229004 + 0.001 * 7.1823225021362305
Epoch 130, val loss: 1.3741334676742554
Epoch 140, training loss: 1.2132959365844727 = 1.2062387466430664 + 0.001 * 7.0571675300598145
Epoch 140, val loss: 1.319100022315979
Epoch 150, training loss: 1.137071132659912 = 1.1300528049468994 + 0.001 * 7.01835823059082
Epoch 150, val loss: 1.2691459655761719
Epoch 160, training loss: 1.061829686164856 = 1.0548195838928223 + 0.001 * 7.010148525238037
Epoch 160, val loss: 1.2213107347488403
Epoch 170, training loss: 0.9830852746963501 = 0.9760885238647461 + 0.001 * 6.99677038192749
Epoch 170, val loss: 1.1715883016586304
Epoch 180, training loss: 0.9001950025558472 = 0.8932193517684937 + 0.001 * 6.975636005401611
Epoch 180, val loss: 1.1177343130111694
Epoch 190, training loss: 0.8163685202598572 = 0.8094164133071899 + 0.001 * 6.95210075378418
Epoch 190, val loss: 1.0619724988937378
Epoch 200, training loss: 0.7364817261695862 = 0.7295472621917725 + 0.001 * 6.934451103210449
Epoch 200, val loss: 1.008923053741455
Epoch 210, training loss: 0.6644620299339294 = 0.6575354933738708 + 0.001 * 6.926548957824707
Epoch 210, val loss: 0.9625119566917419
Epoch 220, training loss: 0.602301836013794 = 0.595377504825592 + 0.001 * 6.9243292808532715
Epoch 220, val loss: 0.9242225885391235
Epoch 230, training loss: 0.5497577786445618 = 0.5428340435028076 + 0.001 * 6.923720836639404
Epoch 230, val loss: 0.8942608833312988
Epoch 240, training loss: 0.5052390098571777 = 0.49831634759902954 + 0.001 * 6.922634601593018
Epoch 240, val loss: 0.8711581230163574
Epoch 250, training loss: 0.466705858707428 = 0.4597843885421753 + 0.001 * 6.921477794647217
Epoch 250, val loss: 0.8534563779830933
Epoch 260, training loss: 0.4320862889289856 = 0.4251665771007538 + 0.001 * 6.9197235107421875
Epoch 260, val loss: 0.8388690948486328
Epoch 270, training loss: 0.3995357155799866 = 0.3926181495189667 + 0.001 * 6.917577266693115
Epoch 270, val loss: 0.8261734843254089
Epoch 280, training loss: 0.3678165674209595 = 0.3609013855457306 + 0.001 * 6.915175437927246
Epoch 280, val loss: 0.8144609332084656
Epoch 290, training loss: 0.33611541986465454 = 0.3292020857334137 + 0.001 * 6.913331508636475
Epoch 290, val loss: 0.804014265537262
Epoch 300, training loss: 0.30459073185920715 = 0.29767927527427673 + 0.001 * 6.911452770233154
Epoch 300, val loss: 0.794946551322937
Epoch 310, training loss: 0.2739063501358032 = 0.26699739694595337 + 0.001 * 6.9089531898498535
Epoch 310, val loss: 0.7882589101791382
Epoch 320, training loss: 0.24452589452266693 = 0.23761972784996033 + 0.001 * 6.906164646148682
Epoch 320, val loss: 0.7832292914390564
Epoch 330, training loss: 0.2167607694864273 = 0.20985724031925201 + 0.001 * 6.903525352478027
Epoch 330, val loss: 0.7798911333084106
Epoch 340, training loss: 0.19077935814857483 = 0.18387985229492188 + 0.001 * 6.899505138397217
Epoch 340, val loss: 0.7773868441581726
Epoch 350, training loss: 0.16680066287517548 = 0.15990440547466278 + 0.001 * 6.896259307861328
Epoch 350, val loss: 0.7769709825515747
Epoch 360, training loss: 0.14529329538345337 = 0.13840267062187195 + 0.001 * 6.890617370605469
Epoch 360, val loss: 0.7788116931915283
Epoch 370, training loss: 0.12639151513576508 = 0.11950571835041046 + 0.001 * 6.885794639587402
Epoch 370, val loss: 0.782627522945404
Epoch 380, training loss: 0.11008305847644806 = 0.10319993644952774 + 0.001 * 6.88312292098999
Epoch 380, val loss: 0.7890799641609192
Epoch 390, training loss: 0.09623120725154877 = 0.08935151249170303 + 0.001 * 6.879693508148193
Epoch 390, val loss: 0.7976467609405518
Epoch 400, training loss: 0.0845278725028038 = 0.07764793187379837 + 0.001 * 6.879942417144775
Epoch 400, val loss: 0.8078458905220032
Epoch 410, training loss: 0.07462383061647415 = 0.06774405390024185 + 0.001 * 6.879777908325195
Epoch 410, val loss: 0.8192992210388184
Epoch 420, training loss: 0.06623858213424683 = 0.059360649436712265 + 0.001 * 6.877930164337158
Epoch 420, val loss: 0.8316351771354675
Epoch 430, training loss: 0.05913310870528221 = 0.05225558578968048 + 0.001 * 6.877521514892578
Epoch 430, val loss: 0.8444061279296875
Epoch 440, training loss: 0.05308829993009567 = 0.04621279239654541 + 0.001 * 6.875505447387695
Epoch 440, val loss: 0.8574790954589844
Epoch 450, training loss: 0.04795677959918976 = 0.04107451066374779 + 0.001 * 6.882268905639648
Epoch 450, val loss: 0.8704437017440796
Epoch 460, training loss: 0.04356547072529793 = 0.0366910956799984 + 0.001 * 6.8743743896484375
Epoch 460, val loss: 0.8831822872161865
Epoch 470, training loss: 0.03981191664934158 = 0.03293783962726593 + 0.001 * 6.8740763664245605
Epoch 470, val loss: 0.895715057849884
Epoch 480, training loss: 0.03658644109964371 = 0.029713522642850876 + 0.001 * 6.8729166984558105
Epoch 480, val loss: 0.9079354405403137
Epoch 490, training loss: 0.0338042788207531 = 0.026932302862405777 + 0.001 * 6.871975898742676
Epoch 490, val loss: 0.9198747277259827
Epoch 500, training loss: 0.03139017894864082 = 0.02451905608177185 + 0.001 * 6.871123790740967
Epoch 500, val loss: 0.9314646124839783
Epoch 510, training loss: 0.029282469302415848 = 0.022412225604057312 + 0.001 * 6.87024450302124
Epoch 510, val loss: 0.9426562786102295
Epoch 520, training loss: 0.027439367026090622 = 0.02056841552257538 + 0.001 * 6.870951175689697
Epoch 520, val loss: 0.9534910917282104
Epoch 530, training loss: 0.025810126215219498 = 0.018944524228572845 + 0.001 * 6.865601062774658
Epoch 530, val loss: 0.9639378190040588
Epoch 540, training loss: 0.024373764172196388 = 0.017510080710053444 + 0.001 * 6.863683700561523
Epoch 540, val loss: 0.9741017818450928
Epoch 550, training loss: 0.023102089762687683 = 0.016237789765000343 + 0.001 * 6.86430025100708
Epoch 550, val loss: 0.983906626701355
Epoch 560, training loss: 0.021965067833662033 = 0.015104114077985287 + 0.001 * 6.8609538078308105
Epoch 560, val loss: 0.9934580326080322
Epoch 570, training loss: 0.020948903635144234 = 0.014089192263782024 + 0.001 * 6.859711170196533
Epoch 570, val loss: 1.0026859045028687
Epoch 580, training loss: 0.02003728598356247 = 0.01317630521953106 + 0.001 * 6.860980033874512
Epoch 580, val loss: 1.011637568473816
Epoch 590, training loss: 0.019226685166358948 = 0.012352978810667992 + 0.001 * 6.8737053871154785
Epoch 590, val loss: 1.0203282833099365
Epoch 600, training loss: 0.018465552479028702 = 0.011609307490289211 + 0.001 * 6.856244087219238
Epoch 600, val loss: 1.0287922620773315
Epoch 610, training loss: 0.017788005992770195 = 0.01093483716249466 + 0.001 * 6.85316801071167
Epoch 610, val loss: 1.0369949340820312
Epoch 620, training loss: 0.01717248186469078 = 0.010320696979761124 + 0.001 * 6.8517842292785645
Epoch 620, val loss: 1.044970989227295
Epoch 630, training loss: 0.01660892926156521 = 0.009759943000972271 + 0.001 * 6.848986625671387
Epoch 630, val loss: 1.0527162551879883
Epoch 640, training loss: 0.016116805374622345 = 0.009246564470231533 + 0.001 * 6.870241641998291
Epoch 640, val loss: 1.0602431297302246
Epoch 650, training loss: 0.015628740191459656 = 0.008775435388088226 + 0.001 * 6.853305339813232
Epoch 650, val loss: 1.0675594806671143
Epoch 660, training loss: 0.015192713588476181 = 0.008341910317540169 + 0.001 * 6.850802421569824
Epoch 660, val loss: 1.074697732925415
Epoch 670, training loss: 0.01478341594338417 = 0.007942072115838528 + 0.001 * 6.841343879699707
Epoch 670, val loss: 1.0816526412963867
Epoch 680, training loss: 0.014456257224082947 = 0.007572491187602282 + 0.001 * 6.88376522064209
Epoch 680, val loss: 1.088409185409546
Epoch 690, training loss: 0.01407878939062357 = 0.00723021337762475 + 0.001 * 6.848575592041016
Epoch 690, val loss: 1.0949859619140625
Epoch 700, training loss: 0.01375666819512844 = 0.006912585347890854 + 0.001 * 6.844082355499268
Epoch 700, val loss: 1.10139799118042
Epoch 710, training loss: 0.013455946929752827 = 0.006617260165512562 + 0.001 * 6.838686466217041
Epoch 710, val loss: 1.107664704322815
Epoch 720, training loss: 0.013181939721107483 = 0.006342208944261074 + 0.001 * 6.8397297859191895
Epoch 720, val loss: 1.1137515306472778
Epoch 730, training loss: 0.012957458384335041 = 0.006085575092583895 + 0.001 * 6.871882915496826
Epoch 730, val loss: 1.119704246520996
Epoch 740, training loss: 0.012686293572187424 = 0.005845816805958748 + 0.001 * 6.840476036071777
Epoch 740, val loss: 1.1254898309707642
Epoch 750, training loss: 0.01244986243546009 = 0.00562138669192791 + 0.001 * 6.828475475311279
Epoch 750, val loss: 1.1311655044555664
Epoch 760, training loss: 0.012240166775882244 = 0.005410871002823114 + 0.001 * 6.829295635223389
Epoch 760, val loss: 1.1366971731185913
Epoch 770, training loss: 0.012045267969369888 = 0.005213110242038965 + 0.001 * 6.832158088684082
Epoch 770, val loss: 1.142073392868042
Epoch 780, training loss: 0.011880671605467796 = 0.005027103703469038 + 0.001 * 6.853567600250244
Epoch 780, val loss: 1.1473300457000732
Epoch 790, training loss: 0.011674681678414345 = 0.004852008074522018 + 0.001 * 6.8226728439331055
Epoch 790, val loss: 1.1524524688720703
Epoch 800, training loss: 0.011520156636834145 = 0.004687019158154726 + 0.001 * 6.833136558532715
Epoch 800, val loss: 1.15748131275177
Epoch 810, training loss: 0.011362213641405106 = 0.004531323444098234 + 0.001 * 6.83089017868042
Epoch 810, val loss: 1.1623859405517578
Epoch 820, training loss: 0.01119652297347784 = 0.004384128842502832 + 0.001 * 6.812393665313721
Epoch 820, val loss: 1.1672214269638062
Epoch 830, training loss: 0.01107088103890419 = 0.0042449370957911015 + 0.001 * 6.825943470001221
Epoch 830, val loss: 1.1719610691070557
Epoch 840, training loss: 0.01091975998133421 = 0.004113343544304371 + 0.001 * 6.806416034698486
Epoch 840, val loss: 1.1765644550323486
Epoch 850, training loss: 0.010803915560245514 = 0.003988825250416994 + 0.001 * 6.815089702606201
Epoch 850, val loss: 1.1810715198516846
Epoch 860, training loss: 0.010684291832149029 = 0.0038708015345036983 + 0.001 * 6.81348991394043
Epoch 860, val loss: 1.1854993104934692
Epoch 870, training loss: 0.010578878223896027 = 0.003758747596293688 + 0.001 * 6.820130348205566
Epoch 870, val loss: 1.1898523569107056
Epoch 880, training loss: 0.010458025150001049 = 0.0036522482987493277 + 0.001 * 6.805776119232178
Epoch 880, val loss: 1.194120168685913
Epoch 890, training loss: 0.010347271338105202 = 0.0035509178414940834 + 0.001 * 6.796353816986084
Epoch 890, val loss: 1.1983129978179932
Epoch 900, training loss: 0.010271882638335228 = 0.0034544309601187706 + 0.001 * 6.817451477050781
Epoch 900, val loss: 1.2024153470993042
Epoch 910, training loss: 0.010159256868064404 = 0.003362491726875305 + 0.001 * 6.796764850616455
Epoch 910, val loss: 1.2064208984375
Epoch 920, training loss: 0.010074148885905743 = 0.003274819115176797 + 0.001 * 6.79932975769043
Epoch 920, val loss: 1.2103526592254639
Epoch 930, training loss: 0.010076941922307014 = 0.0031911293044686317 + 0.001 * 6.885812282562256
Epoch 930, val loss: 1.214223861694336
Epoch 940, training loss: 0.00989129301160574 = 0.003111170371994376 + 0.001 * 6.780122756958008
Epoch 940, val loss: 1.2180012464523315
Epoch 950, training loss: 0.009859559126198292 = 0.0030347630381584167 + 0.001 * 6.824795722961426
Epoch 950, val loss: 1.2217178344726562
Epoch 960, training loss: 0.009754355065524578 = 0.0029616092797368765 + 0.001 * 6.792745113372803
Epoch 960, val loss: 1.2253599166870117
Epoch 970, training loss: 0.009758926928043365 = 0.0028915577568113804 + 0.001 * 6.867368698120117
Epoch 970, val loss: 1.2289162874221802
Epoch 980, training loss: 0.0096171535551548 = 0.0028244636487215757 + 0.001 * 6.792689323425293
Epoch 980, val loss: 1.2324098348617554
Epoch 990, training loss: 0.009540712460875511 = 0.0027601749170571566 + 0.001 * 6.7805376052856445
Epoch 990, val loss: 1.2358561754226685
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7565
Flip ASR: 0.7244/225 nodes
The final ASR:0.79582, 0.10615, Accuracy:0.81605, 0.01062
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11572])
remove edge: torch.Size([2, 9486])
updated graph: torch.Size([2, 10502])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98401, 0.00870, Accuracy:0.83333, 0.00605
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9417728185653687 = 1.933398962020874 + 0.001 * 8.37389850616455
Epoch 0, val loss: 1.9301611185073853
Epoch 10, training loss: 1.932144284248352 = 1.9237704277038574 + 0.001 * 8.373842239379883
Epoch 10, val loss: 1.9210630655288696
Epoch 20, training loss: 1.9201867580413818 = 1.9118131399154663 + 0.001 * 8.373640060424805
Epoch 20, val loss: 1.9092463254928589
Epoch 30, training loss: 1.9032037258148193 = 1.894830584526062 + 0.001 * 8.37318229675293
Epoch 30, val loss: 1.8921631574630737
Epoch 40, training loss: 1.8783714771270752 = 1.8699994087219238 + 0.001 * 8.372075080871582
Epoch 40, val loss: 1.8674497604370117
Epoch 50, training loss: 1.8442153930664062 = 1.8358463048934937 + 0.001 * 8.369029998779297
Epoch 50, val loss: 1.8353757858276367
Epoch 60, training loss: 1.805587887763977 = 1.7972294092178345 + 0.001 * 8.358437538146973
Epoch 60, val loss: 1.803268313407898
Epoch 70, training loss: 1.768387794494629 = 1.7600785493850708 + 0.001 * 8.30923843383789
Epoch 70, val loss: 1.7751091718673706
Epoch 80, training loss: 1.7188529968261719 = 1.7108657360076904 + 0.001 * 7.9872355461120605
Epoch 80, val loss: 1.7343891859054565
Epoch 90, training loss: 1.6487287282943726 = 1.6409294605255127 + 0.001 * 7.799298286437988
Epoch 90, val loss: 1.6752082109451294
Epoch 100, training loss: 1.5550248622894287 = 1.5474165678024292 + 0.001 * 7.608330249786377
Epoch 100, val loss: 1.5968403816223145
Epoch 110, training loss: 1.4438854455947876 = 1.4364686012268066 + 0.001 * 7.416789531707764
Epoch 110, val loss: 1.506309151649475
Epoch 120, training loss: 1.3233939409255981 = 1.316024899482727 + 0.001 * 7.369078159332275
Epoch 120, val loss: 1.4085898399353027
Epoch 130, training loss: 1.1991045475006104 = 1.1917743682861328 + 0.001 * 7.330177307128906
Epoch 130, val loss: 1.3108593225479126
Epoch 140, training loss: 1.0772900581359863 = 1.0700128078460693 + 0.001 * 7.277205944061279
Epoch 140, val loss: 1.2178875207901
Epoch 150, training loss: 0.9640465378761292 = 0.9568413496017456 + 0.001 * 7.205163955688477
Epoch 150, val loss: 1.133536458015442
Epoch 160, training loss: 0.8628976941108704 = 0.855743944644928 + 0.001 * 7.153756141662598
Epoch 160, val loss: 1.0592286586761475
Epoch 170, training loss: 0.7746350765228271 = 0.7674970030784607 + 0.001 * 7.138086795806885
Epoch 170, val loss: 0.9954149127006531
Epoch 180, training loss: 0.6982900500297546 = 0.6911645531654358 + 0.001 * 7.1255083084106445
Epoch 180, val loss: 0.9419299364089966
Epoch 190, training loss: 0.6320701837539673 = 0.6249592900276184 + 0.001 * 7.110912322998047
Epoch 190, val loss: 0.8982090353965759
Epoch 200, training loss: 0.5739342570304871 = 0.5668470859527588 + 0.001 * 7.087154865264893
Epoch 200, val loss: 0.8631138205528259
Epoch 210, training loss: 0.5220276713371277 = 0.5149646401405334 + 0.001 * 7.063060283660889
Epoch 210, val loss: 0.8350716233253479
Epoch 220, training loss: 0.4746686518192291 = 0.4676279127597809 + 0.001 * 7.040730953216553
Epoch 220, val loss: 0.8124121427536011
Epoch 230, training loss: 0.43053460121154785 = 0.4235090911388397 + 0.001 * 7.025495529174805
Epoch 230, val loss: 0.7938566207885742
Epoch 240, training loss: 0.38887399435043335 = 0.3818546235561371 + 0.001 * 7.01938533782959
Epoch 240, val loss: 0.7787997126579285
Epoch 250, training loss: 0.3495040833950043 = 0.34249061346054077 + 0.001 * 7.013462066650391
Epoch 250, val loss: 0.7672282457351685
Epoch 260, training loss: 0.31258583068847656 = 0.30557405948638916 + 0.001 * 7.01178503036499
Epoch 260, val loss: 0.7590665221214294
Epoch 270, training loss: 0.2783510088920593 = 0.27133867144584656 + 0.001 * 7.012322425842285
Epoch 270, val loss: 0.7541782259941101
Epoch 280, training loss: 0.2469860464334488 = 0.23997816443443298 + 0.001 * 7.007875442504883
Epoch 280, val loss: 0.7524845600128174
Epoch 290, training loss: 0.21857692301273346 = 0.21156994998455048 + 0.001 * 7.006969928741455
Epoch 290, val loss: 0.7539354562759399
Epoch 300, training loss: 0.19311147928237915 = 0.18610519170761108 + 0.001 * 7.006293296813965
Epoch 300, val loss: 0.7585486769676208
Epoch 310, training loss: 0.17050813138484955 = 0.1634993851184845 + 0.001 * 7.008752346038818
Epoch 310, val loss: 0.7662587761878967
Epoch 320, training loss: 0.15061594545841217 = 0.1436084806919098 + 0.001 * 7.007471561431885
Epoch 320, val loss: 0.7769124507904053
Epoch 330, training loss: 0.13323968648910522 = 0.1262325644493103 + 0.001 * 7.007123947143555
Epoch 330, val loss: 0.7900519371032715
Epoch 340, training loss: 0.11814189702272415 = 0.11113296449184418 + 0.001 * 7.008935451507568
Epoch 340, val loss: 0.8052846789360046
Epoch 350, training loss: 0.10503647476434708 = 0.09803351014852524 + 0.001 * 7.002963066101074
Epoch 350, val loss: 0.8221803307533264
Epoch 360, training loss: 0.0936703160405159 = 0.08666566014289856 + 0.001 * 7.0046586990356445
Epoch 360, val loss: 0.8403461575508118
Epoch 370, training loss: 0.08379580080509186 = 0.07679035514593124 + 0.001 * 7.005444526672363
Epoch 370, val loss: 0.8593928217887878
Epoch 380, training loss: 0.07520215958356857 = 0.06820256263017654 + 0.001 * 6.999598026275635
Epoch 380, val loss: 0.8789515495300293
Epoch 390, training loss: 0.06772763282060623 = 0.060729771852493286 + 0.001 * 6.997859954833984
Epoch 390, val loss: 0.8987260460853577
Epoch 400, training loss: 0.06121920049190521 = 0.05422338843345642 + 0.001 * 6.995811939239502
Epoch 400, val loss: 0.9183365702629089
Epoch 410, training loss: 0.05556622892618179 = 0.048556242138147354 + 0.001 * 7.009988307952881
Epoch 410, val loss: 0.937717080116272
Epoch 420, training loss: 0.05060527101159096 = 0.043614648282527924 + 0.001 * 6.9906229972839355
Epoch 420, val loss: 0.9566766619682312
Epoch 430, training loss: 0.04628822207450867 = 0.039299581199884415 + 0.001 * 6.9886393547058105
Epoch 430, val loss: 0.975204586982727
Epoch 440, training loss: 0.042504750192165375 = 0.03552120178937912 + 0.001 * 6.983546733856201
Epoch 440, val loss: 0.9932169318199158
Epoch 450, training loss: 0.03919212892651558 = 0.03220508620142937 + 0.001 * 6.987040996551514
Epoch 450, val loss: 1.0107873678207397
Epoch 460, training loss: 0.036271654069423676 = 0.029288819059729576 + 0.001 * 6.9828338623046875
Epoch 460, val loss: 1.0278418064117432
Epoch 470, training loss: 0.0337035171687603 = 0.02671629749238491 + 0.001 * 6.987219333648682
Epoch 470, val loss: 1.0444577932357788
Epoch 480, training loss: 0.03141665831208229 = 0.02444511465728283 + 0.001 * 6.971543312072754
Epoch 480, val loss: 1.060599684715271
Epoch 490, training loss: 0.02940547466278076 = 0.022433990612626076 + 0.001 * 6.971482753753662
Epoch 490, val loss: 1.0762414932250977
Epoch 500, training loss: 0.027608919888734818 = 0.020648419857025146 + 0.001 * 6.960498809814453
Epoch 500, val loss: 1.0913851261138916
Epoch 510, training loss: 0.026018716394901276 = 0.01905900239944458 + 0.001 * 6.959714412689209
Epoch 510, val loss: 1.1060328483581543
Epoch 520, training loss: 0.024602247402071953 = 0.017640577629208565 + 0.001 * 6.961668968200684
Epoch 520, val loss: 1.1202079057693481
Epoch 530, training loss: 0.02332664281129837 = 0.016371000558137894 + 0.001 * 6.955642223358154
Epoch 530, val loss: 1.133934736251831
Epoch 540, training loss: 0.022181959822773933 = 0.015231499448418617 + 0.001 * 6.9504594802856445
Epoch 540, val loss: 1.1472280025482178
Epoch 550, training loss: 0.021163415163755417 = 0.014205791056156158 + 0.001 * 6.95762300491333
Epoch 550, val loss: 1.1600769758224487
Epoch 560, training loss: 0.02022060751914978 = 0.013279975391924381 + 0.001 * 6.940630912780762
Epoch 560, val loss: 1.172540307044983
Epoch 570, training loss: 0.019380858168005943 = 0.012442105449736118 + 0.001 * 6.938752174377441
Epoch 570, val loss: 1.1846277713775635
Epoch 580, training loss: 0.018620861694216728 = 0.011681823991239071 + 0.001 * 6.939037322998047
Epoch 580, val loss: 1.1963133811950684
Epoch 590, training loss: 0.017903998494148254 = 0.010990229435265064 + 0.001 * 6.913769245147705
Epoch 590, val loss: 1.2076334953308105
Epoch 600, training loss: 0.01727926917374134 = 0.010359637439250946 + 0.001 * 6.919631481170654
Epoch 600, val loss: 1.2186121940612793
Epoch 610, training loss: 0.01670508086681366 = 0.009783631190657616 + 0.001 * 6.921448707580566
Epoch 610, val loss: 1.2292587757110596
Epoch 620, training loss: 0.01615525782108307 = 0.009255906566977501 + 0.001 * 6.899352073669434
Epoch 620, val loss: 1.239591360092163
Epoch 630, training loss: 0.01573622226715088 = 0.008771141059696674 + 0.001 * 6.9650797843933105
Epoch 630, val loss: 1.2495882511138916
Epoch 640, training loss: 0.015216492116451263 = 0.008325068280100822 + 0.001 * 6.891424179077148
Epoch 640, val loss: 1.259315013885498
Epoch 650, training loss: 0.014806974679231644 = 0.007913760840892792 + 0.001 * 6.893213748931885
Epoch 650, val loss: 1.2687662839889526
Epoch 660, training loss: 0.014418397098779678 = 0.007533728610724211 + 0.001 * 6.884668827056885
Epoch 660, val loss: 1.2779532670974731
Epoch 670, training loss: 0.014060664921998978 = 0.007181847933679819 + 0.001 * 6.878816604614258
Epoch 670, val loss: 1.2868983745574951
Epoch 680, training loss: 0.013739535585045815 = 0.0068554505705833435 + 0.001 * 6.884084224700928
Epoch 680, val loss: 1.2955870628356934
Epoch 690, training loss: 0.013437987305223942 = 0.006552248261868954 + 0.001 * 6.885738849639893
Epoch 690, val loss: 1.304052710533142
Epoch 700, training loss: 0.013162914663553238 = 0.006270088721066713 + 0.001 * 6.892825603485107
Epoch 700, val loss: 1.3122944831848145
Epoch 710, training loss: 0.012913413345813751 = 0.006007119547575712 + 0.001 * 6.906292915344238
Epoch 710, val loss: 1.3203214406967163
Epoch 720, training loss: 0.01262397039681673 = 0.005761592648923397 + 0.001 * 6.862377643585205
Epoch 720, val loss: 1.328144907951355
Epoch 730, training loss: 0.012381763197481632 = 0.0055320230312645435 + 0.001 * 6.849740028381348
Epoch 730, val loss: 1.3357511758804321
Epoch 740, training loss: 0.012202223762869835 = 0.005317097995430231 + 0.001 * 6.885126113891602
Epoch 740, val loss: 1.3431822061538696
Epoch 750, training loss: 0.012017416767776012 = 0.005115497391670942 + 0.001 * 6.901918888092041
Epoch 750, val loss: 1.350412130355835
Epoch 760, training loss: 0.011772657744586468 = 0.004926278721541166 + 0.001 * 6.846378803253174
Epoch 760, val loss: 1.3574682474136353
Epoch 770, training loss: 0.011576488614082336 = 0.0047483621165156364 + 0.001 * 6.828125953674316
Epoch 770, val loss: 1.3643358945846558
Epoch 780, training loss: 0.011420611292123795 = 0.0045808772556483746 + 0.001 * 6.839733600616455
Epoch 780, val loss: 1.3710511922836304
Epoch 790, training loss: 0.01123763807117939 = 0.004423025995492935 + 0.001 * 6.814611911773682
Epoch 790, val loss: 1.3776079416275024
Epoch 800, training loss: 0.011108257807791233 = 0.0042740521021187305 + 0.001 * 6.834205150604248
Epoch 800, val loss: 1.38399338722229
Epoch 810, training loss: 0.010984757915139198 = 0.004133386537432671 + 0.001 * 6.8513712882995605
Epoch 810, val loss: 1.3902431726455688
Epoch 820, training loss: 0.010837623849511147 = 0.004000381100922823 + 0.001 * 6.837242603302002
Epoch 820, val loss: 1.3963656425476074
Epoch 830, training loss: 0.010697011835873127 = 0.0038744783960282803 + 0.001 * 6.822533130645752
Epoch 830, val loss: 1.4023232460021973
Epoch 840, training loss: 0.010564925149083138 = 0.003755209967494011 + 0.001 * 6.809714317321777
Epoch 840, val loss: 1.4081530570983887
Epoch 850, training loss: 0.010475268587470055 = 0.003642102936282754 + 0.001 * 6.833165645599365
Epoch 850, val loss: 1.413857340812683
Epoch 860, training loss: 0.01036483608186245 = 0.0035347254015505314 + 0.001 * 6.830110549926758
Epoch 860, val loss: 1.4194118976593018
Epoch 870, training loss: 0.010236624628305435 = 0.0034327369648963213 + 0.001 * 6.803886890411377
Epoch 870, val loss: 1.4248636960983276
Epoch 880, training loss: 0.010128817521035671 = 0.0033357450738549232 + 0.001 * 6.79307222366333
Epoch 880, val loss: 1.4301964044570923
Epoch 890, training loss: 0.010044965893030167 = 0.003243454499170184 + 0.001 * 6.801510810852051
Epoch 890, val loss: 1.4353941679000854
Epoch 900, training loss: 0.009940055198967457 = 0.003155545564368367 + 0.001 * 6.784509658813477
Epoch 900, val loss: 1.440502405166626
Epoch 910, training loss: 0.00985535979270935 = 0.003071719780564308 + 0.001 * 6.783639907836914
Epoch 910, val loss: 1.4454853534698486
Epoch 920, training loss: 0.009796022437512875 = 0.0029917375650256872 + 0.001 * 6.804284572601318
Epoch 920, val loss: 1.4503631591796875
Epoch 930, training loss: 0.00968549121171236 = 0.0029154063668102026 + 0.001 * 6.770084381103516
Epoch 930, val loss: 1.455137014389038
Epoch 940, training loss: 0.009629981592297554 = 0.002842511748895049 + 0.001 * 6.787469387054443
Epoch 940, val loss: 1.4598218202590942
Epoch 950, training loss: 0.009566135704517365 = 0.0027728283312171698 + 0.001 * 6.793307304382324
Epoch 950, val loss: 1.4643875360488892
Epoch 960, training loss: 0.009492824785411358 = 0.0027061474975198507 + 0.001 * 6.786677360534668
Epoch 960, val loss: 1.468893051147461
Epoch 970, training loss: 0.009433877654373646 = 0.0026423439849168062 + 0.001 * 6.79153299331665
Epoch 970, val loss: 1.473296046257019
Epoch 980, training loss: 0.009344436228275299 = 0.002581218257546425 + 0.001 * 6.763216972351074
Epoch 980, val loss: 1.4776136875152588
Epoch 990, training loss: 0.009298557415604591 = 0.0025225619319826365 + 0.001 * 6.775995254516602
Epoch 990, val loss: 1.48184072971344
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5055
Flip ASR: 0.4089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.959398865699768 = 1.9510250091552734 + 0.001 * 8.373902320861816
Epoch 0, val loss: 1.955755591392517
Epoch 10, training loss: 1.9491525888442993 = 1.9407787322998047 + 0.001 * 8.373826026916504
Epoch 10, val loss: 1.9456323385238647
Epoch 20, training loss: 1.93613862991333 = 1.9277650117874146 + 0.001 * 8.373641014099121
Epoch 20, val loss: 1.9325238466262817
Epoch 30, training loss: 1.9176065921783447 = 1.9092333316802979 + 0.001 * 8.373254776000977
Epoch 30, val loss: 1.913655161857605
Epoch 40, training loss: 1.8901636600494385 = 1.8817912340164185 + 0.001 * 8.372365951538086
Epoch 40, val loss: 1.8859151601791382
Epoch 50, training loss: 1.8517025709152222 = 1.8433327674865723 + 0.001 * 8.369765281677246
Epoch 50, val loss: 1.8486950397491455
Epoch 60, training loss: 1.807610273361206 = 1.7992510795593262 + 0.001 * 8.35923957824707
Epoch 60, val loss: 1.8093007802963257
Epoch 70, training loss: 1.7671104669570923 = 1.7588038444519043 + 0.001 * 8.30661392211914
Epoch 70, val loss: 1.7737321853637695
Epoch 80, training loss: 1.7179440259933472 = 1.709967851638794 + 0.001 * 7.976205348968506
Epoch 80, val loss: 1.728074550628662
Epoch 90, training loss: 1.650734305381775 = 1.6429325342178345 + 0.001 * 7.801712989807129
Epoch 90, val loss: 1.6693010330200195
Epoch 100, training loss: 1.5635316371917725 = 1.5557947158813477 + 0.001 * 7.736937999725342
Epoch 100, val loss: 1.5969113111495972
Epoch 110, training loss: 1.4641852378845215 = 1.4565134048461914 + 0.001 * 7.671826362609863
Epoch 110, val loss: 1.5145100355148315
Epoch 120, training loss: 1.3645355701446533 = 1.3570319414138794 + 0.001 * 7.503683567047119
Epoch 120, val loss: 1.4339585304260254
Epoch 130, training loss: 1.2698158025741577 = 1.2624735832214355 + 0.001 * 7.342213153839111
Epoch 130, val loss: 1.3592579364776611
Epoch 140, training loss: 1.1808042526245117 = 1.1735033988952637 + 0.001 * 7.300900936126709
Epoch 140, val loss: 1.2911839485168457
Epoch 150, training loss: 1.0969513654708862 = 1.0897212028503418 + 0.001 * 7.230173587799072
Epoch 150, val loss: 1.228403091430664
Epoch 160, training loss: 1.0182747840881348 = 1.0111147165298462 + 0.001 * 7.160104751586914
Epoch 160, val loss: 1.1703197956085205
Epoch 170, training loss: 0.9448334574699402 = 0.9377176761627197 + 0.001 * 7.115760326385498
Epoch 170, val loss: 1.1164202690124512
Epoch 180, training loss: 0.8756911754608154 = 0.8686071038246155 + 0.001 * 7.0840888023376465
Epoch 180, val loss: 1.0652273893356323
Epoch 190, training loss: 0.809036374092102 = 0.8019833564758301 + 0.001 * 7.053013801574707
Epoch 190, val loss: 1.0153688192367554
Epoch 200, training loss: 0.7436856031417847 = 0.7366548180580139 + 0.001 * 7.030806541442871
Epoch 200, val loss: 0.9666056036949158
Epoch 210, training loss: 0.6800945401191711 = 0.6730696558952332 + 0.001 * 7.024885654449463
Epoch 210, val loss: 0.9189427495002747
Epoch 220, training loss: 0.6195200681686401 = 0.612494945526123 + 0.001 * 7.02515172958374
Epoch 220, val loss: 0.8739749789237976
Epoch 230, training loss: 0.5627867579460144 = 0.5557628273963928 + 0.001 * 7.023928642272949
Epoch 230, val loss: 0.8333591222763062
Epoch 240, training loss: 0.5096904039382935 = 0.5026692152023315 + 0.001 * 7.021191120147705
Epoch 240, val loss: 0.7977509498596191
Epoch 250, training loss: 0.4596593677997589 = 0.45264172554016113 + 0.001 * 7.017632007598877
Epoch 250, val loss: 0.7668293714523315
Epoch 260, training loss: 0.41216400265693665 = 0.4051511585712433 + 0.001 * 7.012845039367676
Epoch 260, val loss: 0.7400794625282288
Epoch 270, training loss: 0.3668922781944275 = 0.359886109828949 + 0.001 * 7.006163597106934
Epoch 270, val loss: 0.7175241708755493
Epoch 280, training loss: 0.32402387261390686 = 0.3170258700847626 + 0.001 * 6.997995853424072
Epoch 280, val loss: 0.6991293430328369
Epoch 290, training loss: 0.28409314155578613 = 0.27710574865341187 + 0.001 * 6.987402439117432
Epoch 290, val loss: 0.6851680874824524
Epoch 300, training loss: 0.247783362865448 = 0.2408091276884079 + 0.001 * 6.974234580993652
Epoch 300, val loss: 0.6755629181861877
Epoch 310, training loss: 0.2156691551208496 = 0.20870500802993774 + 0.001 * 6.96414852142334
Epoch 310, val loss: 0.6703751087188721
Epoch 320, training loss: 0.187961146235466 = 0.18101252615451813 + 0.001 * 6.948620319366455
Epoch 320, val loss: 0.669794499874115
Epoch 330, training loss: 0.1644214689731598 = 0.15748560428619385 + 0.001 * 6.9358649253845215
Epoch 330, val loss: 0.673573911190033
Epoch 340, training loss: 0.14456018805503845 = 0.13762815296649933 + 0.001 * 6.932027339935303
Epoch 340, val loss: 0.6806498169898987
Epoch 350, training loss: 0.1277783215045929 = 0.1208549216389656 + 0.001 * 6.92340087890625
Epoch 350, val loss: 0.6901200413703918
Epoch 360, training loss: 0.11353355646133423 = 0.10661733150482178 + 0.001 * 6.916228294372559
Epoch 360, val loss: 0.7015258073806763
Epoch 370, training loss: 0.10134045779705048 = 0.09442846477031708 + 0.001 * 6.911993503570557
Epoch 370, val loss: 0.7140727639198303
Epoch 380, training loss: 0.09079702198505402 = 0.08387915790081024 + 0.001 * 6.917861461639404
Epoch 380, val loss: 0.7273558974266052
Epoch 390, training loss: 0.0815734714269638 = 0.07466260343790054 + 0.001 * 6.91087007522583
Epoch 390, val loss: 0.7411167025566101
Epoch 400, training loss: 0.07345489412546158 = 0.06654852628707886 + 0.001 * 6.906365871429443
Epoch 400, val loss: 0.7552462220191956
Epoch 410, training loss: 0.0662778988480568 = 0.05937376990914345 + 0.001 * 6.904130458831787
Epoch 410, val loss: 0.7696187496185303
Epoch 420, training loss: 0.059927456080913544 = 0.05302193760871887 + 0.001 * 6.905517101287842
Epoch 420, val loss: 0.7842594385147095
Epoch 430, training loss: 0.054333847016096115 = 0.04743047431111336 + 0.001 * 6.9033708572387695
Epoch 430, val loss: 0.7991381287574768
Epoch 440, training loss: 0.04942106455564499 = 0.042520105838775635 + 0.001 * 6.900956630706787
Epoch 440, val loss: 0.8141921162605286
Epoch 450, training loss: 0.04513109475374222 = 0.03823006525635719 + 0.001 * 6.901027202606201
Epoch 450, val loss: 0.8292632699012756
Epoch 460, training loss: 0.04139511659741402 = 0.03449751436710358 + 0.001 * 6.8976006507873535
Epoch 460, val loss: 0.8443020582199097
Epoch 470, training loss: 0.03815348073840141 = 0.031246230006217957 + 0.001 * 6.907251834869385
Epoch 470, val loss: 0.8588786125183105
Epoch 480, training loss: 0.03530927002429962 = 0.028414389118552208 + 0.001 * 6.89487886428833
Epoch 480, val loss: 0.8733342885971069
Epoch 490, training loss: 0.0328349843621254 = 0.0259404256939888 + 0.001 * 6.894556522369385
Epoch 490, val loss: 0.8872340321540833
Epoch 500, training loss: 0.03066425770521164 = 0.02376999519765377 + 0.001 * 6.894261360168457
Epoch 500, val loss: 0.9006838798522949
Epoch 510, training loss: 0.0287521630525589 = 0.021856889128684998 + 0.001 * 6.895273685455322
Epoch 510, val loss: 0.9137539863586426
Epoch 520, training loss: 0.027050809934735298 = 0.020159009844064713 + 0.001 * 6.891800403594971
Epoch 520, val loss: 0.9264808893203735
Epoch 530, training loss: 0.025534749031066895 = 0.018641680479049683 + 0.001 * 6.893069267272949
Epoch 530, val loss: 0.9388523697853088
Epoch 540, training loss: 0.024169782176613808 = 0.017279943451285362 + 0.001 * 6.889838218688965
Epoch 540, val loss: 0.9510102272033691
Epoch 550, training loss: 0.022951481863856316 = 0.016054844483733177 + 0.001 * 6.896636962890625
Epoch 550, val loss: 0.9628363847732544
Epoch 560, training loss: 0.021838730201125145 = 0.014950616285204887 + 0.001 * 6.888113975524902
Epoch 560, val loss: 0.9743875861167908
Epoch 570, training loss: 0.02084391750395298 = 0.013953585177659988 + 0.001 * 6.890332221984863
Epoch 570, val loss: 0.985618531703949
Epoch 580, training loss: 0.019935747608542442 = 0.01305137574672699 + 0.001 * 6.884371757507324
Epoch 580, val loss: 0.9965835213661194
Epoch 590, training loss: 0.019118938595056534 = 0.012232868000864983 + 0.001 * 6.886069297790527
Epoch 590, val loss: 1.0072064399719238
Epoch 600, training loss: 0.01837095431983471 = 0.011489019729197025 + 0.001 * 6.881933689117432
Epoch 600, val loss: 1.0175684690475464
Epoch 610, training loss: 0.017691776156425476 = 0.010811442509293556 + 0.001 * 6.88033390045166
Epoch 610, val loss: 1.0276511907577515
Epoch 620, training loss: 0.017069833353161812 = 0.01019283663481474 + 0.001 * 6.876996040344238
Epoch 620, val loss: 1.0374305248260498
Epoch 630, training loss: 0.01658017188310623 = 0.009626775979995728 + 0.001 * 6.953395366668701
Epoch 630, val loss: 1.0469729900360107
Epoch 640, training loss: 0.015991291031241417 = 0.00910785049200058 + 0.001 * 6.8834404945373535
Epoch 640, val loss: 1.0562467575073242
Epoch 650, training loss: 0.015505695715546608 = 0.008631044067442417 + 0.001 * 6.874651908874512
Epoch 650, val loss: 1.065323829650879
Epoch 660, training loss: 0.015065418556332588 = 0.008191894739866257 + 0.001 * 6.873523235321045
Epoch 660, val loss: 1.0740078687667847
Epoch 670, training loss: 0.014657480642199516 = 0.007786779664456844 + 0.001 * 6.870701313018799
Epoch 670, val loss: 1.0826570987701416
Epoch 680, training loss: 0.014300459995865822 = 0.007412245962768793 + 0.001 * 6.888213157653809
Epoch 680, val loss: 1.0909638404846191
Epoch 690, training loss: 0.013942086137831211 = 0.007065383717417717 + 0.001 * 6.876702308654785
Epoch 690, val loss: 1.099091649055481
Epoch 700, training loss: 0.013610847294330597 = 0.006743533071130514 + 0.001 * 6.867313385009766
Epoch 700, val loss: 1.1070318222045898
Epoch 710, training loss: 0.013310338370501995 = 0.00644438061863184 + 0.001 * 6.865957260131836
Epoch 710, val loss: 1.1147414445877075
Epoch 720, training loss: 0.013043273240327835 = 0.006165897008031607 + 0.001 * 6.877375602722168
Epoch 720, val loss: 1.1222587823867798
Epoch 730, training loss: 0.012769924476742744 = 0.005906368605792522 + 0.001 * 6.863554954528809
Epoch 730, val loss: 1.1296148300170898
Epoch 740, training loss: 0.012531952932476997 = 0.0056640529073774815 + 0.001 * 6.867899417877197
Epoch 740, val loss: 1.1367908716201782
Epoch 750, training loss: 0.012301990762352943 = 0.005437399726361036 + 0.001 * 6.864591121673584
Epoch 750, val loss: 1.1437656879425049
Epoch 760, training loss: 0.012108951807022095 = 0.005225091241300106 + 0.001 * 6.8838605880737305
Epoch 760, val loss: 1.1506150960922241
Epoch 770, training loss: 0.011889027431607246 = 0.0050259847193956375 + 0.001 * 6.86304235458374
Epoch 770, val loss: 1.1572591066360474
Epoch 780, training loss: 0.011694936081767082 = 0.004839014261960983 + 0.001 * 6.855921745300293
Epoch 780, val loss: 1.1637635231018066
Epoch 790, training loss: 0.011550548486411572 = 0.004663208499550819 + 0.001 * 6.8873395919799805
Epoch 790, val loss: 1.1701291799545288
Epoch 800, training loss: 0.011360459960997105 = 0.00449770363047719 + 0.001 * 6.86275577545166
Epoch 800, val loss: 1.1763395071029663
Epoch 810, training loss: 0.011196461506187916 = 0.004341756924986839 + 0.001 * 6.8547043800354
Epoch 810, val loss: 1.1824265718460083
Epoch 820, training loss: 0.011043073609471321 = 0.004194620065391064 + 0.001 * 6.848453044891357
Epoch 820, val loss: 1.188370943069458
Epoch 830, training loss: 0.010919123888015747 = 0.004055667668581009 + 0.001 * 6.8634562492370605
Epoch 830, val loss: 1.194183111190796
Epoch 840, training loss: 0.010770954191684723 = 0.003924304153770208 + 0.001 * 6.846650123596191
Epoch 840, val loss: 1.1998933553695679
Epoch 850, training loss: 0.010644595138728619 = 0.003799974685534835 + 0.001 * 6.8446197509765625
Epoch 850, val loss: 1.2054671049118042
Epoch 860, training loss: 0.010540373623371124 = 0.0036821879912167788 + 0.001 * 6.858185768127441
Epoch 860, val loss: 1.2109392881393433
Epoch 870, training loss: 0.010408148169517517 = 0.00357050565071404 + 0.001 * 6.837642192840576
Epoch 870, val loss: 1.2162790298461914
Epoch 880, training loss: 0.01031201146543026 = 0.0034645283594727516 + 0.001 * 6.847483158111572
Epoch 880, val loss: 1.2215054035186768
Epoch 890, training loss: 0.010199891403317451 = 0.0033638598397374153 + 0.001 * 6.836030960083008
Epoch 890, val loss: 1.226639747619629
Epoch 900, training loss: 0.010127300396561623 = 0.0032681552693247795 + 0.001 * 6.859145164489746
Epoch 900, val loss: 1.231667399406433
Epoch 910, training loss: 0.010014215484261513 = 0.003177050966769457 + 0.001 * 6.837164878845215
Epoch 910, val loss: 1.2365790605545044
Epoch 920, training loss: 0.009918256662786007 = 0.0030902887228876352 + 0.001 * 6.827967643737793
Epoch 920, val loss: 1.2414097785949707
Epoch 930, training loss: 0.009844157844781876 = 0.003007608698680997 + 0.001 * 6.836549282073975
Epoch 930, val loss: 1.246157169342041
Epoch 940, training loss: 0.009751973673701286 = 0.0029287603683769703 + 0.001 * 6.823212623596191
Epoch 940, val loss: 1.2507840394973755
Epoch 950, training loss: 0.009695019572973251 = 0.002853527432307601 + 0.001 * 6.841492176055908
Epoch 950, val loss: 1.255334496498108
Epoch 960, training loss: 0.009603123180568218 = 0.002781681017950177 + 0.001 * 6.821441650390625
Epoch 960, val loss: 1.2598057985305786
Epoch 970, training loss: 0.009586438536643982 = 0.0027130611706525087 + 0.001 * 6.873376846313477
Epoch 970, val loss: 1.2641758918762207
Epoch 980, training loss: 0.009470978751778603 = 0.002647432265803218 + 0.001 * 6.823546409606934
Epoch 980, val loss: 1.2684638500213623
Epoch 990, training loss: 0.009406816214323044 = 0.0025845973286777735 + 0.001 * 6.822218418121338
Epoch 990, val loss: 1.2727004289627075
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7306
Flip ASR: 0.6978/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9609198570251465 = 1.9525460004806519 + 0.001 * 8.373900413513184
Epoch 0, val loss: 1.9508813619613647
Epoch 10, training loss: 1.9502252340316772 = 1.9418513774871826 + 0.001 * 8.373817443847656
Epoch 10, val loss: 1.939961552619934
Epoch 20, training loss: 1.9371215105056763 = 1.9287480115890503 + 0.001 * 8.37354850769043
Epoch 20, val loss: 1.9259638786315918
Epoch 30, training loss: 1.9189573526382446 = 1.9105844497680664 + 0.001 * 8.37292766571045
Epoch 30, val loss: 1.9059982299804688
Epoch 40, training loss: 1.8925111293792725 = 1.8841396570205688 + 0.001 * 8.371438026428223
Epoch 40, val loss: 1.8768537044525146
Epoch 50, training loss: 1.8554543256759644 = 1.8470869064331055 + 0.001 * 8.367417335510254
Epoch 50, val loss: 1.837501883506775
Epoch 60, training loss: 1.8114591836929321 = 1.8031048774719238 + 0.001 * 8.35435962677002
Epoch 60, val loss: 1.7945997714996338
Epoch 70, training loss: 1.7696661949157715 = 1.7613669633865356 + 0.001 * 8.29918384552002
Epoch 70, val loss: 1.757913589477539
Epoch 80, training loss: 1.7221660614013672 = 1.714197039604187 + 0.001 * 7.969020366668701
Epoch 80, val loss: 1.716712236404419
Epoch 90, training loss: 1.6593705415725708 = 1.6515825986862183 + 0.001 * 7.787970066070557
Epoch 90, val loss: 1.6635596752166748
Epoch 100, training loss: 1.5773979425430298 = 1.5697075128555298 + 0.001 * 7.690483093261719
Epoch 100, val loss: 1.5966888666152954
Epoch 110, training loss: 1.4811954498291016 = 1.473602056503296 + 0.001 * 7.593438148498535
Epoch 110, val loss: 1.5206317901611328
Epoch 120, training loss: 1.3829275369644165 = 1.3755438327789307 + 0.001 * 7.383645534515381
Epoch 120, val loss: 1.446682095527649
Epoch 130, training loss: 1.2904095649719238 = 1.2831337451934814 + 0.001 * 7.275763511657715
Epoch 130, val loss: 1.3813432455062866
Epoch 140, training loss: 1.2032532691955566 = 1.196035385131836 + 0.001 * 7.217830181121826
Epoch 140, val loss: 1.3224077224731445
Epoch 150, training loss: 1.1189459562301636 = 1.1117480993270874 + 0.001 * 7.1978302001953125
Epoch 150, val loss: 1.2653875350952148
Epoch 160, training loss: 1.0358562469482422 = 1.0286668539047241 + 0.001 * 7.189350128173828
Epoch 160, val loss: 1.208171010017395
Epoch 170, training loss: 0.9538898468017578 = 0.9467076063156128 + 0.001 * 7.182245254516602
Epoch 170, val loss: 1.1510136127471924
Epoch 180, training loss: 0.8742374181747437 = 0.8670647144317627 + 0.001 * 7.172679901123047
Epoch 180, val loss: 1.0949530601501465
Epoch 190, training loss: 0.7977547645568848 = 0.790596067905426 + 0.001 * 7.158688068389893
Epoch 190, val loss: 1.0399752855300903
Epoch 200, training loss: 0.7252365946769714 = 0.7180987000465393 + 0.001 * 7.137876510620117
Epoch 200, val loss: 0.9861178994178772
Epoch 210, training loss: 0.6576981544494629 = 0.650589644908905 + 0.001 * 7.108538627624512
Epoch 210, val loss: 0.9342238903045654
Epoch 220, training loss: 0.5957167148590088 = 0.5886401534080505 + 0.001 * 7.076541423797607
Epoch 220, val loss: 0.886480450630188
Epoch 230, training loss: 0.5389833450317383 = 0.5319221019744873 + 0.001 * 7.0612711906433105
Epoch 230, val loss: 0.8438763618469238
Epoch 240, training loss: 0.48657453060150146 = 0.47951632738113403 + 0.001 * 7.0581955909729
Epoch 240, val loss: 0.8065211176872253
Epoch 250, training loss: 0.4374558925628662 = 0.4303983747959137 + 0.001 * 7.057523250579834
Epoch 250, val loss: 0.774783730506897
Epoch 260, training loss: 0.39089930057525635 = 0.3838408291339874 + 0.001 * 7.058462142944336
Epoch 260, val loss: 0.7485417723655701
Epoch 270, training loss: 0.34651967883110046 = 0.33945974707603455 + 0.001 * 7.059917449951172
Epoch 270, val loss: 0.7275035977363586
Epoch 280, training loss: 0.30441543459892273 = 0.2973538041114807 + 0.001 * 7.061621189117432
Epoch 280, val loss: 0.7110815644264221
Epoch 290, training loss: 0.265119343996048 = 0.2580556869506836 + 0.001 * 7.063651084899902
Epoch 290, val loss: 0.6985607743263245
Epoch 300, training loss: 0.22940786182880402 = 0.22234220802783966 + 0.001 * 7.065658092498779
Epoch 300, val loss: 0.6896013021469116
Epoch 310, training loss: 0.1978476196527481 = 0.1907801479101181 + 0.001 * 7.067474365234375
Epoch 310, val loss: 0.6836740970611572
Epoch 320, training loss: 0.17061945796012878 = 0.16354922950267792 + 0.001 * 7.0702338218688965
Epoch 320, val loss: 0.680720329284668
Epoch 330, training loss: 0.14753374457359314 = 0.14046357572078705 + 0.001 * 7.0701727867126465
Epoch 330, val loss: 0.680449903011322
Epoch 340, training loss: 0.12817621231079102 = 0.12110505253076553 + 0.001 * 7.071160793304443
Epoch 340, val loss: 0.6827465295791626
Epoch 350, training loss: 0.11202758550643921 = 0.10495781153440475 + 0.001 * 7.069774627685547
Epoch 350, val loss: 0.6872798800468445
Epoch 360, training loss: 0.09855490922927856 = 0.09148183465003967 + 0.001 * 7.07307767868042
Epoch 360, val loss: 0.6935162544250488
Epoch 370, training loss: 0.0872686505317688 = 0.08019822835922241 + 0.001 * 7.07042121887207
Epoch 370, val loss: 0.7011480331420898
Epoch 380, training loss: 0.07778069376945496 = 0.0707162544131279 + 0.001 * 7.064442157745361
Epoch 380, val loss: 0.7097136378288269
Epoch 390, training loss: 0.06979483366012573 = 0.06271054595708847 + 0.001 * 7.084286212921143
Epoch 390, val loss: 0.7189894914627075
Epoch 400, training loss: 0.06296443939208984 = 0.05590979754924774 + 0.001 * 7.054638862609863
Epoch 400, val loss: 0.7287120223045349
Epoch 410, training loss: 0.05714767426252365 = 0.0500955730676651 + 0.001 * 7.052101135253906
Epoch 410, val loss: 0.7387587428092957
Epoch 420, training loss: 0.052142634987831116 = 0.04508930444717407 + 0.001 * 7.053329944610596
Epoch 420, val loss: 0.7488897442817688
Epoch 430, training loss: 0.047797832638025284 = 0.04075074940919876 + 0.001 * 7.04708194732666
Epoch 430, val loss: 0.7591385245323181
Epoch 440, training loss: 0.04401232674717903 = 0.036971718072891235 + 0.001 * 7.040608882904053
Epoch 440, val loss: 0.769329845905304
Epoch 450, training loss: 0.0406888909637928 = 0.03366565331816673 + 0.001 * 7.023237228393555
Epoch 450, val loss: 0.7795212268829346
Epoch 460, training loss: 0.03779226168990135 = 0.030760860070586205 + 0.001 * 7.03140115737915
Epoch 460, val loss: 0.7896368503570557
Epoch 470, training loss: 0.03521982207894325 = 0.028200553730130196 + 0.001 * 7.019268035888672
Epoch 470, val loss: 0.799612820148468
Epoch 480, training loss: 0.03295517712831497 = 0.0259358249604702 + 0.001 * 7.019353866577148
Epoch 480, val loss: 0.8094764351844788
Epoch 490, training loss: 0.030939981341362 = 0.023925961926579475 + 0.001 * 7.0140180587768555
Epoch 490, val loss: 0.819172203540802
Epoch 500, training loss: 0.02915375307202339 = 0.022137954831123352 + 0.001 * 7.015798091888428
Epoch 500, val loss: 0.82869553565979
Epoch 510, training loss: 0.027541976422071457 = 0.020540539175271988 + 0.001 * 7.001436710357666
Epoch 510, val loss: 0.8380866050720215
Epoch 520, training loss: 0.026116132736206055 = 0.01910870335996151 + 0.001 * 7.007428169250488
Epoch 520, val loss: 0.8472735285758972
Epoch 530, training loss: 0.024823397397994995 = 0.017820367589592934 + 0.001 * 7.003028869628906
Epoch 530, val loss: 0.8562833070755005
Epoch 540, training loss: 0.023648595437407494 = 0.01665649004280567 + 0.001 * 6.992104530334473
Epoch 540, val loss: 0.8651153445243835
Epoch 550, training loss: 0.022591687738895416 = 0.015598522499203682 + 0.001 * 6.993165016174316
Epoch 550, val loss: 0.8737257122993469
Epoch 560, training loss: 0.02161230705678463 = 0.014628016389906406 + 0.001 * 6.98429012298584
Epoch 560, val loss: 0.8822638988494873
Epoch 570, training loss: 0.020726412534713745 = 0.013732721097767353 + 0.001 * 6.993691921234131
Epoch 570, val loss: 0.8906468749046326
Epoch 580, training loss: 0.019886348396539688 = 0.012910017743706703 + 0.001 * 6.976330757141113
Epoch 580, val loss: 0.8988615870475769
Epoch 590, training loss: 0.01914263144135475 = 0.012151814997196198 + 0.001 * 6.990817070007324
Epoch 590, val loss: 0.9069458842277527
Epoch 600, training loss: 0.01842782273888588 = 0.011452226899564266 + 0.001 * 6.975595474243164
Epoch 600, val loss: 0.9148739576339722
Epoch 610, training loss: 0.017777832224965096 = 0.01080625131726265 + 0.001 * 6.971580982208252
Epoch 610, val loss: 0.9226354956626892
Epoch 620, training loss: 0.017214283347129822 = 0.010208988562226295 + 0.001 * 7.005293846130371
Epoch 620, val loss: 0.9302262663841248
Epoch 630, training loss: 0.016627918928861618 = 0.009658693335950375 + 0.001 * 6.969226360321045
Epoch 630, val loss: 0.9377747178077698
Epoch 640, training loss: 0.016117719933390617 = 0.009150763042271137 + 0.001 * 6.966957092285156
Epoch 640, val loss: 0.945098876953125
Epoch 650, training loss: 0.015659097582101822 = 0.008681297302246094 + 0.001 * 6.977799892425537
Epoch 650, val loss: 0.9522212147712708
Epoch 660, training loss: 0.015211084857583046 = 0.008247175253927708 + 0.001 * 6.96390962600708
Epoch 660, val loss: 0.9592608213424683
Epoch 670, training loss: 0.0148074422031641 = 0.007844884879887104 + 0.001 * 6.9625563621521
Epoch 670, val loss: 0.9661893248558044
Epoch 680, training loss: 0.014424542896449566 = 0.007470887154340744 + 0.001 * 6.953655242919922
Epoch 680, val loss: 0.9729938507080078
Epoch 690, training loss: 0.01407667063176632 = 0.007121975999325514 + 0.001 * 6.954694747924805
Epoch 690, val loss: 0.9795952439308167
Epoch 700, training loss: 0.01375243254005909 = 0.00679707620292902 + 0.001 * 6.955356597900391
Epoch 700, val loss: 0.9860630631446838
Epoch 710, training loss: 0.013445290736854076 = 0.006494749337434769 + 0.001 * 6.950541019439697
Epoch 710, val loss: 0.9923610091209412
Epoch 720, training loss: 0.013151802122592926 = 0.006213038694113493 + 0.001 * 6.938762664794922
Epoch 720, val loss: 0.9985289573669434
Epoch 730, training loss: 0.012903141789138317 = 0.005950153339654207 + 0.001 * 6.952988147735596
Epoch 730, val loss: 1.0045658349990845
Epoch 740, training loss: 0.01264185644686222 = 0.0057046483270823956 + 0.001 * 6.937207221984863
Epoch 740, val loss: 1.0104821920394897
Epoch 750, training loss: 0.012416457757353783 = 0.005476594902575016 + 0.001 * 6.939862251281738
Epoch 750, val loss: 1.016204833984375
Epoch 760, training loss: 0.01219940185546875 = 0.005263421218842268 + 0.001 * 6.935979843139648
Epoch 760, val loss: 1.0218350887298584
Epoch 770, training loss: 0.012010524049401283 = 0.005063384771347046 + 0.001 * 6.94713830947876
Epoch 770, val loss: 1.0273860692977905
Epoch 780, training loss: 0.011803028173744678 = 0.00487559987232089 + 0.001 * 6.927427768707275
Epoch 780, val loss: 1.0327659845352173
Epoch 790, training loss: 0.011631560511887074 = 0.0046987757086753845 + 0.001 * 6.932784557342529
Epoch 790, val loss: 1.038092017173767
Epoch 800, training loss: 0.011465242132544518 = 0.004532177466899157 + 0.001 * 6.9330644607543945
Epoch 800, val loss: 1.0432459115982056
Epoch 810, training loss: 0.01128724031150341 = 0.004375080112367868 + 0.001 * 6.9121599197387695
Epoch 810, val loss: 1.0483287572860718
Epoch 820, training loss: 0.011139199137687683 = 0.0042267837561666965 + 0.001 * 6.91241455078125
Epoch 820, val loss: 1.0532785654067993
Epoch 830, training loss: 0.010988699272274971 = 0.00408664345741272 + 0.001 * 6.902055263519287
Epoch 830, val loss: 1.0581331253051758
Epoch 840, training loss: 0.010937507264316082 = 0.003954021260142326 + 0.001 * 6.983485698699951
Epoch 840, val loss: 1.062903642654419
Epoch 850, training loss: 0.01074958685785532 = 0.0038285416085273027 + 0.001 * 6.921044826507568
Epoch 850, val loss: 1.0676013231277466
Epoch 860, training loss: 0.01060156337916851 = 0.0037096585147082806 + 0.001 * 6.891903877258301
Epoch 860, val loss: 1.0721427202224731
Epoch 870, training loss: 0.010504934936761856 = 0.0035970548633486032 + 0.001 * 6.90787935256958
Epoch 870, val loss: 1.076619267463684
Epoch 880, training loss: 0.010385029017925262 = 0.0034903748892247677 + 0.001 * 6.894654273986816
Epoch 880, val loss: 1.080978274345398
Epoch 890, training loss: 0.010292978957295418 = 0.003388982266187668 + 0.001 * 6.90399694442749
Epoch 890, val loss: 1.0853030681610107
Epoch 900, training loss: 0.010196879506111145 = 0.0032926027197390795 + 0.001 * 6.9042768478393555
Epoch 900, val loss: 1.0895309448242188
Epoch 910, training loss: 0.010078832507133484 = 0.0032008634880185127 + 0.001 * 6.877968788146973
Epoch 910, val loss: 1.0936682224273682
Epoch 920, training loss: 0.010013394057750702 = 0.0031134597957134247 + 0.001 * 6.899933338165283
Epoch 920, val loss: 1.0977189540863037
Epoch 930, training loss: 0.009951673448085785 = 0.003030118066817522 + 0.001 * 6.921555042266846
Epoch 930, val loss: 1.1017038822174072
Epoch 940, training loss: 0.009831088595092297 = 0.00295063154771924 + 0.001 * 6.880456924438477
Epoch 940, val loss: 1.1055959463119507
Epoch 950, training loss: 0.009749051183462143 = 0.0028745296876877546 + 0.001 * 6.874520778656006
Epoch 950, val loss: 1.109450340270996
Epoch 960, training loss: 0.009665913879871368 = 0.0028017875738441944 + 0.001 * 6.864126205444336
Epoch 960, val loss: 1.1131898164749146
Epoch 970, training loss: 0.009602760896086693 = 0.00273221917450428 + 0.001 * 6.870542049407959
Epoch 970, val loss: 1.1168384552001953
Epoch 980, training loss: 0.009527168236672878 = 0.0026656980626285076 + 0.001 * 6.861469745635986
Epoch 980, val loss: 1.120439887046814
Epoch 990, training loss: 0.009462852030992508 = 0.0026020898949354887 + 0.001 * 6.860762119293213
Epoch 990, val loss: 1.1239954233169556
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.3358
Flip ASR: 0.2844/225 nodes
The final ASR:0.52399, 0.16172, Accuracy:0.82346, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11622])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10572])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9557
Flip ASR: 0.9467/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97171, 0.01218, Accuracy:0.83704, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9694945812225342 = 1.961120843887329 + 0.001 * 8.373764038085938
Epoch 0, val loss: 1.9617140293121338
Epoch 10, training loss: 1.958626627922058 = 1.9502530097961426 + 0.001 * 8.373669624328613
Epoch 10, val loss: 1.9509458541870117
Epoch 20, training loss: 1.9454293251037598 = 1.9370559453964233 + 0.001 * 8.373379707336426
Epoch 20, val loss: 1.9378490447998047
Epoch 30, training loss: 1.9271146059036255 = 1.9187418222427368 + 0.001 * 8.372791290283203
Epoch 30, val loss: 1.9198343753814697
Epoch 40, training loss: 1.9001590013504028 = 1.8917875289916992 + 0.001 * 8.371454238891602
Epoch 40, val loss: 1.89381742477417
Epoch 50, training loss: 1.8620176315307617 = 1.8536500930786133 + 0.001 * 8.367584228515625
Epoch 50, val loss: 1.8586797714233398
Epoch 60, training loss: 1.8178606033325195 = 1.8095083236694336 + 0.001 * 8.352328300476074
Epoch 60, val loss: 1.8220270872116089
Epoch 70, training loss: 1.7795178890228271 = 1.7712513208389282 + 0.001 * 8.266531944274902
Epoch 70, val loss: 1.7928194999694824
Epoch 80, training loss: 1.7361936569213867 = 1.7282702922821045 + 0.001 * 7.923356533050537
Epoch 80, val loss: 1.755064606666565
Epoch 90, training loss: 1.6762675046920776 = 1.6685611009597778 + 0.001 * 7.706442356109619
Epoch 90, val loss: 1.7033950090408325
Epoch 100, training loss: 1.5955461263656616 = 1.5881224870681763 + 0.001 * 7.423697471618652
Epoch 100, val loss: 1.6370126008987427
Epoch 110, training loss: 1.4988255500793457 = 1.491767168045044 + 0.001 * 7.058347225189209
Epoch 110, val loss: 1.5596522092819214
Epoch 120, training loss: 1.4007114171981812 = 1.3937186002731323 + 0.001 * 6.992772102355957
Epoch 120, val loss: 1.482385277748108
Epoch 130, training loss: 1.3067660331726074 = 1.2998019456863403 + 0.001 * 6.964037895202637
Epoch 130, val loss: 1.4102635383605957
Epoch 140, training loss: 1.2161535024642944 = 1.2092024087905884 + 0.001 * 6.9511003494262695
Epoch 140, val loss: 1.3426544666290283
Epoch 150, training loss: 1.1284558773040771 = 1.1215139627456665 + 0.001 * 6.941943168640137
Epoch 150, val loss: 1.2779834270477295
Epoch 160, training loss: 1.0439074039459229 = 1.0369724035263062 + 0.001 * 6.935059070587158
Epoch 160, val loss: 1.216334581375122
Epoch 170, training loss: 0.9625323414802551 = 0.9556015133857727 + 0.001 * 6.930837154388428
Epoch 170, val loss: 1.157721757888794
Epoch 180, training loss: 0.8839977979660034 = 0.8770684599876404 + 0.001 * 6.929343223571777
Epoch 180, val loss: 1.1009129285812378
Epoch 190, training loss: 0.8077199459075928 = 0.8007904887199402 + 0.001 * 6.929458141326904
Epoch 190, val loss: 1.045037865638733
Epoch 200, training loss: 0.7332990765571594 = 0.7263695001602173 + 0.001 * 6.929579734802246
Epoch 200, val loss: 0.989456295967102
Epoch 210, training loss: 0.6613567471504211 = 0.6544274687767029 + 0.001 * 6.929290294647217
Epoch 210, val loss: 0.9353466629981995
Epoch 220, training loss: 0.5936048030853271 = 0.5866762399673462 + 0.001 * 6.928563117980957
Epoch 220, val loss: 0.8842986226081848
Epoch 230, training loss: 0.5321161150932312 = 0.5251889228820801 + 0.001 * 6.927163600921631
Epoch 230, val loss: 0.8389340043067932
Epoch 240, training loss: 0.4778313934803009 = 0.47090619802474976 + 0.001 * 6.925198554992676
Epoch 240, val loss: 0.8009583353996277
Epoch 250, training loss: 0.4301290810108185 = 0.42320629954338074 + 0.001 * 6.92277717590332
Epoch 250, val loss: 0.7705053091049194
Epoch 260, training loss: 0.3872050344944 = 0.3802850544452667 + 0.001 * 6.919983863830566
Epoch 260, val loss: 0.7460201978683472
Epoch 270, training loss: 0.3470897674560547 = 0.34017276763916016 + 0.001 * 6.916993618011475
Epoch 270, val loss: 0.7255969643592834
Epoch 280, training loss: 0.30848369002342224 = 0.3015696108341217 + 0.001 * 6.914082050323486
Epoch 280, val loss: 0.7080469727516174
Epoch 290, training loss: 0.27112969756126404 = 0.2642183005809784 + 0.001 * 6.911396026611328
Epoch 290, val loss: 0.6924695372581482
Epoch 300, training loss: 0.23569896817207336 = 0.2287900745868683 + 0.001 * 6.908890724182129
Epoch 300, val loss: 0.6790504455566406
Epoch 310, training loss: 0.20325712859630585 = 0.1963508278131485 + 0.001 * 6.906303882598877
Epoch 310, val loss: 0.668166995048523
Epoch 320, training loss: 0.17464813590049744 = 0.1677446961402893 + 0.001 * 6.903432369232178
Epoch 320, val loss: 0.6603555083274841
Epoch 330, training loss: 0.15014371275901794 = 0.14324404299259186 + 0.001 * 6.899667739868164
Epoch 330, val loss: 0.6557881236076355
Epoch 340, training loss: 0.12952467799186707 = 0.12263067811727524 + 0.001 * 6.894001007080078
Epoch 340, val loss: 0.6546013951301575
Epoch 350, training loss: 0.1123184785246849 = 0.1054285317659378 + 0.001 * 6.889944553375244
Epoch 350, val loss: 0.6566050052642822
Epoch 360, training loss: 0.09797511249780655 = 0.09109453856945038 + 0.001 * 6.8805742263793945
Epoch 360, val loss: 0.661357045173645
Epoch 370, training loss: 0.08601140230894089 = 0.07914381474256516 + 0.001 * 6.867588520050049
Epoch 370, val loss: 0.6684508919715881
Epoch 380, training loss: 0.07601785659790039 = 0.06916285306215286 + 0.001 * 6.855003833770752
Epoch 380, val loss: 0.6774083375930786
Epoch 390, training loss: 0.0676504522562027 = 0.06080498918890953 + 0.001 * 6.845466613769531
Epoch 390, val loss: 0.6877981424331665
Epoch 400, training loss: 0.060617681592702866 = 0.053781431168317795 + 0.001 * 6.836249351501465
Epoch 400, val loss: 0.6991065144538879
Epoch 410, training loss: 0.05468539521098137 = 0.047852374613285065 + 0.001 * 6.83302116394043
Epoch 410, val loss: 0.7109394669532776
Epoch 420, training loss: 0.04964935779571533 = 0.042820997536182404 + 0.001 * 6.8283610343933105
Epoch 420, val loss: 0.723029613494873
Epoch 430, training loss: 0.04535503312945366 = 0.03852343186736107 + 0.001 * 6.831600666046143
Epoch 430, val loss: 0.7351574897766113
Epoch 440, training loss: 0.04165288805961609 = 0.034829236567020416 + 0.001 * 6.823650360107422
Epoch 440, val loss: 0.7471984028816223
Epoch 450, training loss: 0.03845712170004845 = 0.03163325786590576 + 0.001 * 6.823863506317139
Epoch 450, val loss: 0.7591072916984558
Epoch 460, training loss: 0.03567236661911011 = 0.02885114587843418 + 0.001 * 6.8212199211120605
Epoch 460, val loss: 0.7707868218421936
Epoch 470, training loss: 0.0332457460463047 = 0.026416867971420288 + 0.001 * 6.828878402709961
Epoch 470, val loss: 0.7822191119194031
Epoch 480, training loss: 0.03109433501958847 = 0.02427385188639164 + 0.001 * 6.820483207702637
Epoch 480, val loss: 0.793411910533905
Epoch 490, training loss: 0.02919677086174488 = 0.022378455847501755 + 0.001 * 6.818315029144287
Epoch 490, val loss: 0.8043512105941772
Epoch 500, training loss: 0.027518093585968018 = 0.02069423720240593 + 0.001 * 6.823855876922607
Epoch 500, val loss: 0.8149760961532593
Epoch 510, training loss: 0.026009459048509598 = 0.019191836938261986 + 0.001 * 6.81762170791626
Epoch 510, val loss: 0.8253369927406311
Epoch 520, training loss: 0.024662157520651817 = 0.017846791073679924 + 0.001 * 6.815365791320801
Epoch 520, val loss: 0.8354750871658325
Epoch 530, training loss: 0.0234640222042799 = 0.016637925058603287 + 0.001 * 6.826097011566162
Epoch 530, val loss: 0.8453090786933899
Epoch 540, training loss: 0.02236078679561615 = 0.015547902323305607 + 0.001 * 6.81288480758667
Epoch 540, val loss: 0.8549541234970093
Epoch 550, training loss: 0.02137310430407524 = 0.014562051743268967 + 0.001 * 6.8110527992248535
Epoch 550, val loss: 0.8643351793289185
Epoch 560, training loss: 0.0204782672226429 = 0.013666723854839802 + 0.001 * 6.8115434646606445
Epoch 560, val loss: 0.8734380006790161
Epoch 570, training loss: 0.019659457728266716 = 0.01285045687109232 + 0.001 * 6.809000492095947
Epoch 570, val loss: 0.8823474645614624
Epoch 580, training loss: 0.01890948787331581 = 0.012102845124900341 + 0.001 * 6.806643009185791
Epoch 580, val loss: 0.8910158276557922
Epoch 590, training loss: 0.01823999173939228 = 0.011415405198931694 + 0.001 * 6.824586868286133
Epoch 590, val loss: 0.8995046019554138
Epoch 600, training loss: 0.017591306939721107 = 0.010782250203192234 + 0.001 * 6.809056758880615
Epoch 600, val loss: 0.9078121781349182
Epoch 610, training loss: 0.016999781131744385 = 0.010197646915912628 + 0.001 * 6.802133560180664
Epoch 610, val loss: 0.915970504283905
Epoch 620, training loss: 0.016457412391901016 = 0.009657095186412334 + 0.001 * 6.800317764282227
Epoch 620, val loss: 0.9239503145217896
Epoch 630, training loss: 0.01595606654882431 = 0.00915672816336155 + 0.001 * 6.7993388175964355
Epoch 630, val loss: 0.9317613244056702
Epoch 640, training loss: 0.015498533844947815 = 0.008693269453942776 + 0.001 * 6.805263996124268
Epoch 640, val loss: 0.93942791223526
Epoch 650, training loss: 0.015060672536492348 = 0.008263632655143738 + 0.001 * 6.797039985656738
Epoch 650, val loss: 0.9469282627105713
Epoch 660, training loss: 0.014673200435936451 = 0.007864782586693764 + 0.001 * 6.808417320251465
Epoch 660, val loss: 0.9542961716651917
Epoch 670, training loss: 0.014290092512965202 = 0.007494369056075811 + 0.001 * 6.795722484588623
Epoch 670, val loss: 0.9615058898925781
Epoch 680, training loss: 0.013940597884356976 = 0.007149833254516125 + 0.001 * 6.790764331817627
Epoch 680, val loss: 0.9685488343238831
Epoch 690, training loss: 0.013623451814055443 = 0.0068290154449641705 + 0.001 * 6.794435501098633
Epoch 690, val loss: 0.9754826426506042
Epoch 700, training loss: 0.013314766809344292 = 0.006529886741191149 + 0.001 * 6.784879207611084
Epoch 700, val loss: 0.982250988483429
Epoch 710, training loss: 0.013051485642790794 = 0.006250673905014992 + 0.001 * 6.800810813903809
Epoch 710, val loss: 0.9888988137245178
Epoch 720, training loss: 0.012775002047419548 = 0.005989684723317623 + 0.001 * 6.785317420959473
Epoch 720, val loss: 0.9954171776771545
Epoch 730, training loss: 0.012520182877779007 = 0.00574543047696352 + 0.001 * 6.774751663208008
Epoch 730, val loss: 1.0018131732940674
Epoch 740, training loss: 0.012313423678278923 = 0.0055166552774608135 + 0.001 * 6.796768665313721
Epoch 740, val loss: 1.0080623626708984
Epoch 750, training loss: 0.012073932215571404 = 0.00530210230499506 + 0.001 * 6.771830081939697
Epoch 750, val loss: 1.014211654663086
Epoch 760, training loss: 0.011886268854141235 = 0.005100673530250788 + 0.001 * 6.785594940185547
Epoch 760, val loss: 1.0202369689941406
Epoch 770, training loss: 0.011693575419485569 = 0.0049113077111542225 + 0.001 * 6.7822675704956055
Epoch 770, val loss: 1.0261400938034058
Epoch 780, training loss: 0.011510666459798813 = 0.004733006004244089 + 0.001 * 6.777659893035889
Epoch 780, val loss: 1.0319422483444214
Epoch 790, training loss: 0.011340982280671597 = 0.004565043840557337 + 0.001 * 6.775938034057617
Epoch 790, val loss: 1.0376430749893188
Epoch 800, training loss: 0.011165997944772243 = 0.004406599327921867 + 0.001 * 6.759398460388184
Epoch 800, val loss: 1.0432195663452148
Epoch 810, training loss: 0.011027099564671516 = 0.004257055930793285 + 0.001 * 6.770042896270752
Epoch 810, val loss: 1.0487288236618042
Epoch 820, training loss: 0.010868165642023087 = 0.004115752410143614 + 0.001 * 6.75241231918335
Epoch 820, val loss: 1.0540984869003296
Epoch 830, training loss: 0.01082448847591877 = 0.0039820545352995396 + 0.001 * 6.842432975769043
Epoch 830, val loss: 1.0593690872192383
Epoch 840, training loss: 0.010632520541548729 = 0.0038555022329092026 + 0.001 * 6.777018070220947
Epoch 840, val loss: 1.064573884010315
Epoch 850, training loss: 0.010477624833583832 = 0.0037355790846049786 + 0.001 * 6.742045879364014
Epoch 850, val loss: 1.0696674585342407
Epoch 860, training loss: 0.010363543406128883 = 0.0036218129098415375 + 0.001 * 6.741730690002441
Epoch 860, val loss: 1.0746690034866333
Epoch 870, training loss: 0.010258940048515797 = 0.0035138169769197702 + 0.001 * 6.745122909545898
Epoch 870, val loss: 1.0795809030532837
Epoch 880, training loss: 0.010157953016459942 = 0.0034111891873180866 + 0.001 * 6.746763706207275
Epoch 880, val loss: 1.0844039916992188
Epoch 890, training loss: 0.010048210620880127 = 0.003313593100756407 + 0.001 * 6.734617710113525
Epoch 890, val loss: 1.089157223701477
Epoch 900, training loss: 0.009961682371795177 = 0.0032207120675593615 + 0.001 * 6.740970134735107
Epoch 900, val loss: 1.0938208103179932
Epoch 910, training loss: 0.009908189997076988 = 0.0031322489958256483 + 0.001 * 6.775940895080566
Epoch 910, val loss: 1.098418951034546
Epoch 920, training loss: 0.009779867716133595 = 0.003047937760129571 + 0.001 * 6.731929779052734
Epoch 920, val loss: 1.1029340028762817
Epoch 930, training loss: 0.009700294584035873 = 0.0029675010591745377 + 0.001 * 6.732793807983398
Epoch 930, val loss: 1.107385277748108
Epoch 940, training loss: 0.009633142501115799 = 0.002890735398977995 + 0.001 * 6.742406368255615
Epoch 940, val loss: 1.1117463111877441
Epoch 950, training loss: 0.009542868472635746 = 0.0028173953760415316 + 0.001 * 6.725472450256348
Epoch 950, val loss: 1.1160334348678589
Epoch 960, training loss: 0.009470585733652115 = 0.002747270278632641 + 0.001 * 6.7233147621154785
Epoch 960, val loss: 1.120273470878601
Epoch 970, training loss: 0.009402325376868248 = 0.0026802008505910635 + 0.001 * 6.722124099731445
Epoch 970, val loss: 1.1244335174560547
Epoch 980, training loss: 0.009341667406260967 = 0.002615994540974498 + 0.001 * 6.725672721862793
Epoch 980, val loss: 1.1285380125045776
Epoch 990, training loss: 0.009327252395451069 = 0.0025545137468725443 + 0.001 * 6.772738456726074
Epoch 990, val loss: 1.1326124668121338
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7454
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9531408548355103 = 1.9447669982910156 + 0.001 * 8.373804092407227
Epoch 0, val loss: 1.947068452835083
Epoch 10, training loss: 1.9425525665283203 = 1.9341788291931152 + 0.001 * 8.373709678649902
Epoch 10, val loss: 1.9360541105270386
Epoch 20, training loss: 1.9292794466018677 = 1.9209060668945312 + 0.001 * 8.373416900634766
Epoch 20, val loss: 1.922311544418335
Epoch 30, training loss: 1.910224437713623 = 1.9018516540527344 + 0.001 * 8.372776985168457
Epoch 30, val loss: 1.9028037786483765
Epoch 40, training loss: 1.881740689277649 = 1.8733694553375244 + 0.001 * 8.371286392211914
Epoch 40, val loss: 1.8744256496429443
Epoch 50, training loss: 1.8423919677734375 = 1.8340251445770264 + 0.001 * 8.36681079864502
Epoch 50, val loss: 1.8373017311096191
Epoch 60, training loss: 1.7993533611297607 = 1.791006088256836 + 0.001 * 8.347326278686523
Epoch 60, val loss: 1.800186038017273
Epoch 70, training loss: 1.7583307027816772 = 1.750114917755127 + 0.001 * 8.215836524963379
Epoch 70, val loss: 1.7652864456176758
Epoch 80, training loss: 1.7025017738342285 = 1.6946609020233154 + 0.001 * 7.8408637046813965
Epoch 80, val loss: 1.7151415348052979
Epoch 90, training loss: 1.6270798444747925 = 1.619314432144165 + 0.001 * 7.765362739562988
Epoch 90, val loss: 1.6496961116790771
Epoch 100, training loss: 1.5350923538208008 = 1.5273576974868774 + 0.001 * 7.7346415519714355
Epoch 100, val loss: 1.574256420135498
Epoch 110, training loss: 1.4395209550857544 = 1.4318476915359497 + 0.001 * 7.673296928405762
Epoch 110, val loss: 1.4948238134384155
Epoch 120, training loss: 1.3488013744354248 = 1.3413296937942505 + 0.001 * 7.471718788146973
Epoch 120, val loss: 1.4233593940734863
Epoch 130, training loss: 1.2627782821655273 = 1.2554603815078735 + 0.001 * 7.317910671234131
Epoch 130, val loss: 1.3571126461029053
Epoch 140, training loss: 1.17905592918396 = 1.171764612197876 + 0.001 * 7.291263103485107
Epoch 140, val loss: 1.2946518659591675
Epoch 150, training loss: 1.0969247817993164 = 1.0896743535995483 + 0.001 * 7.250463962554932
Epoch 150, val loss: 1.2348970174789429
Epoch 160, training loss: 1.0165461301803589 = 1.009352445602417 + 0.001 * 7.193732261657715
Epoch 160, val loss: 1.1773923635482788
Epoch 170, training loss: 0.9386664628982544 = 0.9315482378005981 + 0.001 * 7.118229389190674
Epoch 170, val loss: 1.1214691400527954
Epoch 180, training loss: 0.8640841841697693 = 0.8570362329483032 + 0.001 * 7.047970771789551
Epoch 180, val loss: 1.0679365396499634
Epoch 190, training loss: 0.7932992577552795 = 0.7862886190414429 + 0.001 * 7.010614395141602
Epoch 190, val loss: 1.017574667930603
Epoch 200, training loss: 0.7269642949104309 = 0.7199710011482239 + 0.001 * 6.993320941925049
Epoch 200, val loss: 0.9717852473258972
Epoch 210, training loss: 0.6661728024482727 = 0.6591891050338745 + 0.001 * 6.983720779418945
Epoch 210, val loss: 0.9319952726364136
Epoch 220, training loss: 0.6116566061973572 = 0.6046770215034485 + 0.001 * 6.979559421539307
Epoch 220, val loss: 0.8987579941749573
Epoch 230, training loss: 0.5632684826850891 = 0.5562907457351685 + 0.001 * 6.97770881652832
Epoch 230, val loss: 0.871360182762146
Epoch 240, training loss: 0.5203530192375183 = 0.5133770108222961 + 0.001 * 6.97599458694458
Epoch 240, val loss: 0.8491239547729492
Epoch 250, training loss: 0.48189976811408997 = 0.47492554783821106 + 0.001 * 6.9742326736450195
Epoch 250, val loss: 0.8311449885368347
Epoch 260, training loss: 0.44660699367523193 = 0.4396345913410187 + 0.001 * 6.972410202026367
Epoch 260, val loss: 0.8163453936576843
Epoch 270, training loss: 0.4134244918823242 = 0.4064539968967438 + 0.001 * 6.970489501953125
Epoch 270, val loss: 0.8043372631072998
Epoch 280, training loss: 0.38150203227996826 = 0.3745335638523102 + 0.001 * 6.968475818634033
Epoch 280, val loss: 0.7941163778305054
Epoch 290, training loss: 0.3505971133708954 = 0.3436291217803955 + 0.001 * 6.967986583709717
Epoch 290, val loss: 0.785609245300293
Epoch 300, training loss: 0.32091224193573 = 0.31394731998443604 + 0.001 * 6.96491003036499
Epoch 300, val loss: 0.778684139251709
Epoch 310, training loss: 0.29290831089019775 = 0.28594547510147095 + 0.001 * 6.962834358215332
Epoch 310, val loss: 0.773758590221405
Epoch 320, training loss: 0.26688507199287415 = 0.25992387533187866 + 0.001 * 6.961192607879639
Epoch 320, val loss: 0.7720298171043396
Epoch 330, training loss: 0.24276205897331238 = 0.23580080270767212 + 0.001 * 6.961258888244629
Epoch 330, val loss: 0.7741068601608276
Epoch 340, training loss: 0.2199556976556778 = 0.21299658715724945 + 0.001 * 6.959105014801025
Epoch 340, val loss: 0.7797821760177612
Epoch 350, training loss: 0.19795086979866028 = 0.19099366664886475 + 0.001 * 6.957196235656738
Epoch 350, val loss: 0.7887051105499268
Epoch 360, training loss: 0.1768202781677246 = 0.1698645055294037 + 0.001 * 6.955765724182129
Epoch 360, val loss: 0.8000701069831848
Epoch 370, training loss: 0.15697336196899414 = 0.1500188559293747 + 0.001 * 6.954505920410156
Epoch 370, val loss: 0.8135849833488464
Epoch 380, training loss: 0.13885727524757385 = 0.13190416991710663 + 0.001 * 6.953098773956299
Epoch 380, val loss: 0.8289778828620911
Epoch 390, training loss: 0.1226782575249672 = 0.11572607606649399 + 0.001 * 6.952178478240967
Epoch 390, val loss: 0.8460092544555664
Epoch 400, training loss: 0.10848505795001984 = 0.101533904671669 + 0.001 * 6.951149940490723
Epoch 400, val loss: 0.8644518256187439
Epoch 410, training loss: 0.09618987143039703 = 0.08924059569835663 + 0.001 * 6.949279308319092
Epoch 410, val loss: 0.883805513381958
Epoch 420, training loss: 0.08559875935316086 = 0.07864736020565033 + 0.001 * 6.951397895812988
Epoch 420, val loss: 0.9035698175430298
Epoch 430, training loss: 0.07653281092643738 = 0.0695853978395462 + 0.001 * 6.947410583496094
Epoch 430, val loss: 0.9237131476402283
Epoch 440, training loss: 0.0687604695558548 = 0.061816394329071045 + 0.001 * 6.94407320022583
Epoch 440, val loss: 0.9440919160842896
Epoch 450, training loss: 0.06208895519375801 = 0.055148035287857056 + 0.001 * 6.940920352935791
Epoch 450, val loss: 0.9644258618354797
Epoch 460, training loss: 0.05635237693786621 = 0.04940931499004364 + 0.001 * 6.943063259124756
Epoch 460, val loss: 0.9845765233039856
Epoch 470, training loss: 0.051386699080467224 = 0.04444574937224388 + 0.001 * 6.940948963165283
Epoch 470, val loss: 1.0045580863952637
Epoch 480, training loss: 0.04707365483045578 = 0.04013662040233612 + 0.001 * 6.937033176422119
Epoch 480, val loss: 1.0242085456848145
Epoch 490, training loss: 0.04331060126423836 = 0.03637959063053131 + 0.001 * 6.9310102462768555
Epoch 490, val loss: 1.0434712171554565
Epoch 500, training loss: 0.04003134369850159 = 0.03309336677193642 + 0.001 * 6.937975883483887
Epoch 500, val loss: 1.0621700286865234
Epoch 510, training loss: 0.03714179992675781 = 0.03020942211151123 + 0.001 * 6.932376384735107
Epoch 510, val loss: 1.0803037881851196
Epoch 520, training loss: 0.03458916023373604 = 0.027669062837958336 + 0.001 * 6.9200968742370605
Epoch 520, val loss: 1.097773790359497
Epoch 530, training loss: 0.03236766532063484 = 0.025424230843782425 + 0.001 * 6.9434356689453125
Epoch 530, val loss: 1.1146963834762573
Epoch 540, training loss: 0.030344780534505844 = 0.02343302220106125 + 0.001 * 6.911757469177246
Epoch 540, val loss: 1.1310526132583618
Epoch 550, training loss: 0.028568997979164124 = 0.021659785881638527 + 0.001 * 6.909210681915283
Epoch 550, val loss: 1.1469296216964722
Epoch 560, training loss: 0.02698875032365322 = 0.0200748723000288 + 0.001 * 6.913877487182617
Epoch 560, val loss: 1.1623320579528809
Epoch 570, training loss: 0.02557157725095749 = 0.018653379753232002 + 0.001 * 6.918196678161621
Epoch 570, val loss: 1.1773039102554321
Epoch 580, training loss: 0.024290941655635834 = 0.017374251037836075 + 0.001 * 6.916691303253174
Epoch 580, val loss: 1.19172203540802
Epoch 590, training loss: 0.023126883432269096 = 0.016219036653637886 + 0.001 * 6.907845973968506
Epoch 590, val loss: 1.2057771682739258
Epoch 600, training loss: 0.022057073190808296 = 0.015173421241343021 + 0.001 * 6.8836517333984375
Epoch 600, val loss: 1.2193877696990967
Epoch 610, training loss: 0.021117571741342545 = 0.014224713668227196 + 0.001 * 6.892856597900391
Epoch 610, val loss: 1.2326568365097046
Epoch 620, training loss: 0.020243285223841667 = 0.013361770659685135 + 0.001 * 6.881514549255371
Epoch 620, val loss: 1.245513677597046
Epoch 630, training loss: 0.019455784931778908 = 0.012575105763971806 + 0.001 * 6.880678653717041
Epoch 630, val loss: 1.2580432891845703
Epoch 640, training loss: 0.018742259591817856 = 0.011856375262141228 + 0.001 * 6.885884761810303
Epoch 640, val loss: 1.2702057361602783
Epoch 650, training loss: 0.018098771572113037 = 0.011198245920240879 + 0.001 * 6.9005255699157715
Epoch 650, val loss: 1.2820565700531006
Epoch 660, training loss: 0.017476815730333328 = 0.01059428695589304 + 0.001 * 6.882529258728027
Epoch 660, val loss: 1.293609380722046
Epoch 670, training loss: 0.01690591126680374 = 0.01003899984061718 + 0.001 * 6.866909980773926
Epoch 670, val loss: 1.3048410415649414
Epoch 680, training loss: 0.016408398747444153 = 0.009527377784252167 + 0.001 * 6.881019592285156
Epoch 680, val loss: 1.3157751560211182
Epoch 690, training loss: 0.015945836901664734 = 0.009054997935891151 + 0.001 * 6.890838623046875
Epoch 690, val loss: 1.3264566659927368
Epoch 700, training loss: 0.01547497883439064 = 0.008618197403848171 + 0.001 * 6.856781482696533
Epoch 700, val loss: 1.3368152379989624
Epoch 710, training loss: 0.015085270628333092 = 0.008213545195758343 + 0.001 * 6.871725559234619
Epoch 710, val loss: 1.346955418586731
Epoch 720, training loss: 0.014694547280669212 = 0.007838115096092224 + 0.001 * 6.8564324378967285
Epoch 720, val loss: 1.3568472862243652
Epoch 730, training loss: 0.014343301765620708 = 0.007489194627851248 + 0.001 * 6.854106903076172
Epoch 730, val loss: 1.3664920330047607
Epoch 740, training loss: 0.014003496617078781 = 0.007164476905018091 + 0.001 * 6.839019775390625
Epoch 740, val loss: 1.3759191036224365
Epoch 750, training loss: 0.013720647431910038 = 0.006861738860607147 + 0.001 * 6.858908176422119
Epoch 750, val loss: 1.3851346969604492
Epoch 760, training loss: 0.013435110449790955 = 0.006579094100743532 + 0.001 * 6.856015682220459
Epoch 760, val loss: 1.3940950632095337
Epoch 770, training loss: 0.013166606426239014 = 0.006314872298389673 + 0.001 * 6.851734161376953
Epoch 770, val loss: 1.4028712511062622
Epoch 780, training loss: 0.012896042317152023 = 0.006067500915378332 + 0.001 * 6.8285417556762695
Epoch 780, val loss: 1.4114344120025635
Epoch 790, training loss: 0.012661566957831383 = 0.0058355676010251045 + 0.001 * 6.825998783111572
Epoch 790, val loss: 1.4197999238967896
Epoch 800, training loss: 0.012449965812265873 = 0.005617865826934576 + 0.001 * 6.832099437713623
Epoch 800, val loss: 1.4279640913009644
Epoch 810, training loss: 0.01224428229033947 = 0.005413254722952843 + 0.001 * 6.831027507781982
Epoch 810, val loss: 1.4360052347183228
Epoch 820, training loss: 0.012055758386850357 = 0.00522069726139307 + 0.001 * 6.835060119628906
Epoch 820, val loss: 1.443799614906311
Epoch 830, training loss: 0.011869717389345169 = 0.00503923837095499 + 0.001 * 6.830478668212891
Epoch 830, val loss: 1.4514479637145996
Epoch 840, training loss: 0.011702725663781166 = 0.004868139512836933 + 0.001 * 6.834585666656494
Epoch 840, val loss: 1.4589202404022217
Epoch 850, training loss: 0.011548719368875027 = 0.0047065787948668 + 0.001 * 6.842140197753906
Epoch 850, val loss: 1.4662466049194336
Epoch 860, training loss: 0.011368109844624996 = 0.004553840495646 + 0.001 * 6.814269065856934
Epoch 860, val loss: 1.473429799079895
Epoch 870, training loss: 0.011243032291531563 = 0.004409341607242823 + 0.001 * 6.8336896896362305
Epoch 870, val loss: 1.4804376363754272
Epoch 880, training loss: 0.011102065443992615 = 0.004272483289241791 + 0.001 * 6.829582214355469
Epoch 880, val loss: 1.4872733354568481
Epoch 890, training loss: 0.010959114879369736 = 0.00414272490888834 + 0.001 * 6.816389083862305
Epoch 890, val loss: 1.494014024734497
Epoch 900, training loss: 0.010828345082700253 = 0.004019593819975853 + 0.001 * 6.808751106262207
Epoch 900, val loss: 1.5005652904510498
Epoch 910, training loss: 0.010713724419474602 = 0.0039026488084346056 + 0.001 * 6.811075687408447
Epoch 910, val loss: 1.5070254802703857
Epoch 920, training loss: 0.010592501610517502 = 0.0037914749700576067 + 0.001 * 6.801026344299316
Epoch 920, val loss: 1.5133408308029175
Epoch 930, training loss: 0.010508095845580101 = 0.0036856892984360456 + 0.001 * 6.822405815124512
Epoch 930, val loss: 1.5195255279541016
Epoch 940, training loss: 0.010386073030531406 = 0.0035849560517817736 + 0.001 * 6.801116943359375
Epoch 940, val loss: 1.5255879163742065
Epoch 950, training loss: 0.0102899931371212 = 0.0034890216775238514 + 0.001 * 6.800971031188965
Epoch 950, val loss: 1.5315428972244263
Epoch 960, training loss: 0.010215598158538342 = 0.0033975292462855577 + 0.001 * 6.818068504333496
Epoch 960, val loss: 1.5373806953430176
Epoch 970, training loss: 0.010102272033691406 = 0.0033101739827543497 + 0.001 * 6.792097568511963
Epoch 970, val loss: 1.5431159734725952
Epoch 980, training loss: 0.01002111379057169 = 0.0032267654314637184 + 0.001 * 6.794348239898682
Epoch 980, val loss: 1.5487157106399536
Epoch 990, training loss: 0.009945308789610863 = 0.0031470402609556913 + 0.001 * 6.7982683181762695
Epoch 990, val loss: 1.5542525053024292
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7630
Overall ASR: 0.6125
Flip ASR: 0.5689/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9578248262405396 = 1.9494510889053345 + 0.001 * 8.373777389526367
Epoch 0, val loss: 1.9454901218414307
Epoch 10, training loss: 1.9480180740356445 = 1.939644455909729 + 0.001 * 8.373647689819336
Epoch 10, val loss: 1.9357370138168335
Epoch 20, training loss: 1.9354058504104614 = 1.9270325899124146 + 0.001 * 8.373298645019531
Epoch 20, val loss: 1.9231126308441162
Epoch 30, training loss: 1.9170750379562378 = 1.9087023735046387 + 0.001 * 8.372613906860352
Epoch 30, val loss: 1.904937744140625
Epoch 40, training loss: 1.889467477798462 = 1.881096363067627 + 0.001 * 8.371147155761719
Epoch 40, val loss: 1.8781901597976685
Epoch 50, training loss: 1.8501745462417603 = 1.8418073654174805 + 0.001 * 8.367228507995605
Epoch 50, val loss: 1.8418720960617065
Epoch 60, training loss: 1.8034358024597168 = 1.7950844764709473 + 0.001 * 8.351305961608887
Epoch 60, val loss: 1.8017957210540771
Epoch 70, training loss: 1.7575221061706543 = 1.7492775917053223 + 0.001 * 8.244492530822754
Epoch 70, val loss: 1.7626765966415405
Epoch 80, training loss: 1.700616478919983 = 1.6928369998931885 + 0.001 * 7.779535293579102
Epoch 80, val loss: 1.7093511819839478
Epoch 90, training loss: 1.6234228610992432 = 1.6157375574111938 + 0.001 * 7.685250282287598
Epoch 90, val loss: 1.6390882730484009
Epoch 100, training loss: 1.5263872146606445 = 1.5187584161758423 + 0.001 * 7.628764629364014
Epoch 100, val loss: 1.5571742057800293
Epoch 110, training loss: 1.4221071004867554 = 1.4145987033843994 + 0.001 * 7.508388042449951
Epoch 110, val loss: 1.4707978963851929
Epoch 120, training loss: 1.3220926523208618 = 1.314831018447876 + 0.001 * 7.261679649353027
Epoch 120, val loss: 1.3916609287261963
Epoch 130, training loss: 1.2294657230377197 = 1.2222574949264526 + 0.001 * 7.2082343101501465
Epoch 130, val loss: 1.3214430809020996
Epoch 140, training loss: 1.142064094543457 = 1.1348876953125 + 0.001 * 7.176447868347168
Epoch 140, val loss: 1.2579399347305298
Epoch 150, training loss: 1.0560002326965332 = 1.048862099647522 + 0.001 * 7.138076305389404
Epoch 150, val loss: 1.1969157457351685
Epoch 160, training loss: 0.9691052436828613 = 0.9620053768157959 + 0.001 * 7.099857807159424
Epoch 160, val loss: 1.1353650093078613
Epoch 170, training loss: 0.8835670351982117 = 0.8764892816543579 + 0.001 * 7.077764511108398
Epoch 170, val loss: 1.0747255086898804
Epoch 180, training loss: 0.8041154742240906 = 0.797053337097168 + 0.001 * 7.062130451202393
Epoch 180, val loss: 1.0187393426895142
Epoch 190, training loss: 0.7344009876251221 = 0.7273589372634888 + 0.001 * 7.042044639587402
Epoch 190, val loss: 0.9705069065093994
Epoch 200, training loss: 0.6747980713844299 = 0.6677818894386292 + 0.001 * 7.016209602355957
Epoch 200, val loss: 0.931331992149353
Epoch 210, training loss: 0.6229150891304016 = 0.6159307956695557 + 0.001 * 6.984318733215332
Epoch 210, val loss: 0.8996723890304565
Epoch 220, training loss: 0.5758329033851624 = 0.5688818097114563 + 0.001 * 6.9510650634765625
Epoch 220, val loss: 0.8731724619865417
Epoch 230, training loss: 0.5314124822616577 = 0.5244846343994141 + 0.001 * 6.927858352661133
Epoch 230, val loss: 0.8500330448150635
Epoch 240, training loss: 0.4886675477027893 = 0.481759250164032 + 0.001 * 6.908283233642578
Epoch 240, val loss: 0.8300979733467102
Epoch 250, training loss: 0.4471025764942169 = 0.44020479917526245 + 0.001 * 6.8977766036987305
Epoch 250, val loss: 0.8132514953613281
Epoch 260, training loss: 0.40622183680534363 = 0.3993328809738159 + 0.001 * 6.888965606689453
Epoch 260, val loss: 0.7993098497390747
Epoch 270, training loss: 0.36575886607170105 = 0.35887855291366577 + 0.001 * 6.880311965942383
Epoch 270, val loss: 0.7880114316940308
Epoch 280, training loss: 0.32591214776039124 = 0.3190389573574066 + 0.001 * 6.8731889724731445
Epoch 280, val loss: 0.7797724008560181
Epoch 290, training loss: 0.2874927818775177 = 0.2806272506713867 + 0.001 * 6.865531921386719
Epoch 290, val loss: 0.7743393778800964
Epoch 300, training loss: 0.25160685181617737 = 0.2447495013475418 + 0.001 * 6.857349872589111
Epoch 300, val loss: 0.7711542248725891
Epoch 310, training loss: 0.21902695298194885 = 0.2121751755475998 + 0.001 * 6.851772308349609
Epoch 310, val loss: 0.7699280381202698
Epoch 320, training loss: 0.1902483105659485 = 0.18339890241622925 + 0.001 * 6.849408149719238
Epoch 320, val loss: 0.7707467079162598
Epoch 330, training loss: 0.1652148962020874 = 0.1583719253540039 + 0.001 * 6.842963695526123
Epoch 330, val loss: 0.7732998728752136
Epoch 340, training loss: 0.1436125934123993 = 0.13677169382572174 + 0.001 * 6.8408942222595215
Epoch 340, val loss: 0.7776661515235901
Epoch 350, training loss: 0.12506811320781708 = 0.11823082715272903 + 0.001 * 6.837287425994873
Epoch 350, val loss: 0.7838847041130066
Epoch 360, training loss: 0.10922438651323318 = 0.1023896113038063 + 0.001 * 6.834776878356934
Epoch 360, val loss: 0.791529655456543
Epoch 370, training loss: 0.09576702862977982 = 0.08893496543169022 + 0.001 * 6.832064628601074
Epoch 370, val loss: 0.8007505536079407
Epoch 380, training loss: 0.08433753252029419 = 0.07750824093818665 + 0.001 * 6.829293727874756
Epoch 380, val loss: 0.8112788200378418
Epoch 390, training loss: 0.07465772330760956 = 0.06782501935958862 + 0.001 * 6.832704544067383
Epoch 390, val loss: 0.8230708837509155
Epoch 400, training loss: 0.06646736711263657 = 0.05964042246341705 + 0.001 * 6.826943397521973
Epoch 400, val loss: 0.835818350315094
Epoch 410, training loss: 0.05955447629094124 = 0.052728231996297836 + 0.001 * 6.826244354248047
Epoch 410, val loss: 0.8492326140403748
Epoch 420, training loss: 0.05369741842150688 = 0.04687262326478958 + 0.001 * 6.824795246124268
Epoch 420, val loss: 0.863060712814331
Epoch 430, training loss: 0.048719990998506546 = 0.041894763708114624 + 0.001 * 6.825225353240967
Epoch 430, val loss: 0.8769912123680115
Epoch 440, training loss: 0.04446016624569893 = 0.037636689841747284 + 0.001 * 6.8234758377075195
Epoch 440, val loss: 0.8908871412277222
Epoch 450, training loss: 0.04079576954245567 = 0.03397327661514282 + 0.001 * 6.822493553161621
Epoch 450, val loss: 0.9046591520309448
Epoch 460, training loss: 0.037639714777469635 = 0.03080359287559986 + 0.001 * 6.836121082305908
Epoch 460, val loss: 0.9181745648384094
Epoch 470, training loss: 0.03486362099647522 = 0.028043968603014946 + 0.001 * 6.819650650024414
Epoch 470, val loss: 0.9314936995506287
Epoch 480, training loss: 0.03244869410991669 = 0.025629762560129166 + 0.001 * 6.8189311027526855
Epoch 480, val loss: 0.9445241689682007
Epoch 490, training loss: 0.03032693825662136 = 0.023510050028562546 + 0.001 * 6.816888332366943
Epoch 490, val loss: 0.9572365283966064
Epoch 500, training loss: 0.028467580676078796 = 0.02164110727608204 + 0.001 * 6.826472759246826
Epoch 500, val loss: 0.9696723222732544
Epoch 510, training loss: 0.026807479560375214 = 0.019985748454928398 + 0.001 * 6.821730613708496
Epoch 510, val loss: 0.9817287921905518
Epoch 520, training loss: 0.025328798219561577 = 0.0185135155916214 + 0.001 * 6.815282821655273
Epoch 520, val loss: 0.9935040473937988
Epoch 530, training loss: 0.024011489003896713 = 0.017198879271745682 + 0.001 * 6.812609672546387
Epoch 530, val loss: 1.0049306154251099
Epoch 540, training loss: 0.02283121831715107 = 0.01602073758840561 + 0.001 * 6.810481071472168
Epoch 540, val loss: 1.0160894393920898
Epoch 550, training loss: 0.021780159324407578 = 0.014961134642362595 + 0.001 * 6.819024562835693
Epoch 550, val loss: 1.0269229412078857
Epoch 560, training loss: 0.020820029079914093 = 0.014005064964294434 + 0.001 * 6.8149638175964355
Epoch 560, val loss: 1.037454605102539
Epoch 570, training loss: 0.019944915547966957 = 0.013139703311026096 + 0.001 * 6.805212020874023
Epoch 570, val loss: 1.047717809677124
Epoch 580, training loss: 0.019157182425260544 = 0.012353996746242046 + 0.001 * 6.803184986114502
Epoch 580, val loss: 1.05768620967865
Epoch 590, training loss: 0.01844104193150997 = 0.011638659983873367 + 0.001 * 6.802381992340088
Epoch 590, val loss: 1.067431092262268
Epoch 600, training loss: 0.017793860286474228 = 0.010985182598233223 + 0.001 * 6.8086771965026855
Epoch 600, val loss: 1.0768941640853882
Epoch 610, training loss: 0.017186572775244713 = 0.010387035086750984 + 0.001 * 6.79953670501709
Epoch 610, val loss: 1.0861386060714722
Epoch 620, training loss: 0.01663549244403839 = 0.009838237427175045 + 0.001 * 6.7972540855407715
Epoch 620, val loss: 1.0951563119888306
Epoch 630, training loss: 0.016129881143569946 = 0.009333549998700619 + 0.001 * 6.796331405639648
Epoch 630, val loss: 1.1039068698883057
Epoch 640, training loss: 0.01566474325954914 = 0.008868420496582985 + 0.001 * 6.796321868896484
Epoch 640, val loss: 1.1124542951583862
Epoch 650, training loss: 0.015238789841532707 = 0.008438833057880402 + 0.001 * 6.799956321716309
Epoch 650, val loss: 1.120793342590332
Epoch 660, training loss: 0.014832587912678719 = 0.008041366003453732 + 0.001 * 6.791222095489502
Epoch 660, val loss: 1.1289360523223877
Epoch 670, training loss: 0.014504088088870049 = 0.007672899402678013 + 0.001 * 6.831187725067139
Epoch 670, val loss: 1.1368463039398193
Epoch 680, training loss: 0.014121448621153831 = 0.007330704014748335 + 0.001 * 6.790744304656982
Epoch 680, val loss: 1.1446107625961304
Epoch 690, training loss: 0.013798336498439312 = 0.007012332323938608 + 0.001 * 6.786004066467285
Epoch 690, val loss: 1.1521397829055786
Epoch 700, training loss: 0.013515405356884003 = 0.006715535651892424 + 0.001 * 6.799869537353516
Epoch 700, val loss: 1.1594974994659424
Epoch 710, training loss: 0.013223838061094284 = 0.006437819451093674 + 0.001 * 6.7860188484191895
Epoch 710, val loss: 1.1666948795318604
Epoch 720, training loss: 0.012957572937011719 = 0.0061778477393090725 + 0.001 * 6.779725074768066
Epoch 720, val loss: 1.1737251281738281
Epoch 730, training loss: 0.01271867472678423 = 0.005934382788836956 + 0.001 * 6.784291744232178
Epoch 730, val loss: 1.1806094646453857
Epoch 740, training loss: 0.012483159080147743 = 0.0057060630060732365 + 0.001 * 6.777095317840576
Epoch 740, val loss: 1.1873289346694946
Epoch 750, training loss: 0.012297534383833408 = 0.005491664167493582 + 0.001 * 6.805870056152344
Epoch 750, val loss: 1.1939091682434082
Epoch 760, training loss: 0.012060020118951797 = 0.005290056113153696 + 0.001 * 6.76996374130249
Epoch 760, val loss: 1.2003337144851685
Epoch 770, training loss: 0.011874640360474586 = 0.0051001594401896 + 0.001 * 6.77448034286499
Epoch 770, val loss: 1.2066214084625244
Epoch 780, training loss: 0.011686686426401138 = 0.004921530839055777 + 0.001 * 6.765155792236328
Epoch 780, val loss: 1.2127931118011475
Epoch 790, training loss: 0.01151609979569912 = 0.004752571694552898 + 0.001 * 6.763527870178223
Epoch 790, val loss: 1.2188173532485962
Epoch 800, training loss: 0.011362943798303604 = 0.004592662211507559 + 0.001 * 6.770280838012695
Epoch 800, val loss: 1.2247341871261597
Epoch 810, training loss: 0.011236635968089104 = 0.004441199358552694 + 0.001 * 6.795435905456543
Epoch 810, val loss: 1.2305406332015991
Epoch 820, training loss: 0.011061446741223335 = 0.004297560080885887 + 0.001 * 6.763886451721191
Epoch 820, val loss: 1.236228108406067
Epoch 830, training loss: 0.010919108986854553 = 0.0041612014174461365 + 0.001 * 6.757907390594482
Epoch 830, val loss: 1.2418245077133179
Epoch 840, training loss: 0.010794779285788536 = 0.004031573422253132 + 0.001 * 6.763205051422119
Epoch 840, val loss: 1.2473113536834717
Epoch 850, training loss: 0.010665342211723328 = 0.003908287268131971 + 0.001 * 6.757054328918457
Epoch 850, val loss: 1.252686619758606
Epoch 860, training loss: 0.01055426150560379 = 0.0037908186204731464 + 0.001 * 6.7634429931640625
Epoch 860, val loss: 1.2579927444458008
Epoch 870, training loss: 0.010431147180497646 = 0.0036788496654480696 + 0.001 * 6.752297401428223
Epoch 870, val loss: 1.2632074356079102
Epoch 880, training loss: 0.010353859513998032 = 0.0035719741135835648 + 0.001 * 6.781885147094727
Epoch 880, val loss: 1.268341064453125
Epoch 890, training loss: 0.010223033837974072 = 0.003469925606623292 + 0.001 * 6.75310754776001
Epoch 890, val loss: 1.2733993530273438
Epoch 900, training loss: 0.010151535272598267 = 0.0033725393004715443 + 0.001 * 6.778995513916016
Epoch 900, val loss: 1.278363585472107
Epoch 910, training loss: 0.010026492178440094 = 0.0032795341685414314 + 0.001 * 6.746957778930664
Epoch 910, val loss: 1.283281683921814
Epoch 920, training loss: 0.009938720613718033 = 0.0031904838979244232 + 0.001 * 6.748236179351807
Epoch 920, val loss: 1.2880922555923462
Epoch 930, training loss: 0.00985056720674038 = 0.003105170326307416 + 0.001 * 6.745396614074707
Epoch 930, val loss: 1.2928415536880493
Epoch 940, training loss: 0.00976303406059742 = 0.0030234723817557096 + 0.001 * 6.739561080932617
Epoch 940, val loss: 1.2975366115570068
Epoch 950, training loss: 0.009721411392092705 = 0.0029451283626258373 + 0.001 * 6.776283264160156
Epoch 950, val loss: 1.302162766456604
Epoch 960, training loss: 0.009617932140827179 = 0.0028700060211122036 + 0.001 * 6.747925281524658
Epoch 960, val loss: 1.3067185878753662
Epoch 970, training loss: 0.009534805081784725 = 0.0027979265432804823 + 0.001 * 6.736877918243408
Epoch 970, val loss: 1.3111995458602905
Epoch 980, training loss: 0.009467083029448986 = 0.002728716004639864 + 0.001 * 6.738366603851318
Epoch 980, val loss: 1.3156310319900513
Epoch 990, training loss: 0.009417773224413395 = 0.0026623555459082127 + 0.001 * 6.755417346954346
Epoch 990, val loss: 1.3199827671051025
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7565
Flip ASR: 0.7244/225 nodes
The final ASR:0.70480, 0.06539, Accuracy:0.79259, 0.02117
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9484])
updated graph: torch.Size([2, 10542])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9701039791107178 = 1.9617302417755127 + 0.001 * 8.373758316040039
Epoch 0, val loss: 1.9589039087295532
Epoch 10, training loss: 1.9593310356140137 = 1.9509574174880981 + 0.001 * 8.373676300048828
Epoch 10, val loss: 1.9486119747161865
Epoch 20, training loss: 1.9458794593811035 = 1.937506079673767 + 0.001 * 8.373392105102539
Epoch 20, val loss: 1.9353350400924683
Epoch 30, training loss: 1.9267245531082153 = 1.9183517694473267 + 0.001 * 8.37279224395752
Epoch 30, val loss: 1.9160951375961304
Epoch 40, training loss: 1.8981084823608398 = 1.8897370100021362 + 0.001 * 8.371454238891602
Epoch 40, val loss: 1.8875389099121094
Epoch 50, training loss: 1.8577547073364258 = 1.8493870496749878 + 0.001 * 8.367693901062012
Epoch 50, val loss: 1.849147081375122
Epoch 60, training loss: 1.8111423254013062 = 1.8027889728546143 + 0.001 * 8.353338241577148
Epoch 60, val loss: 1.8095343112945557
Epoch 70, training loss: 1.7705165147781372 = 1.7622385025024414 + 0.001 * 8.278048515319824
Epoch 70, val loss: 1.7796071767807007
Epoch 80, training loss: 1.7247668504714966 = 1.7168917655944824 + 0.001 * 7.875035762786865
Epoch 80, val loss: 1.7425923347473145
Epoch 90, training loss: 1.6625021696090698 = 1.6548103094100952 + 0.001 * 7.6918792724609375
Epoch 90, val loss: 1.6909615993499756
Epoch 100, training loss: 1.579229712486267 = 1.571768879890442 + 0.001 * 7.460795879364014
Epoch 100, val loss: 1.6236348152160645
Epoch 110, training loss: 1.4770097732543945 = 1.469759225845337 + 0.001 * 7.250600814819336
Epoch 110, val loss: 1.542433738708496
Epoch 120, training loss: 1.3666573762893677 = 1.3594485521316528 + 0.001 * 7.208847999572754
Epoch 120, val loss: 1.4544621706008911
Epoch 130, training loss: 1.2554941177368164 = 1.2483479976654053 + 0.001 * 7.146176338195801
Epoch 130, val loss: 1.3670716285705566
Epoch 140, training loss: 1.1470938920974731 = 1.140022873878479 + 0.001 * 7.070985317230225
Epoch 140, val loss: 1.283069372177124
Epoch 150, training loss: 1.0439943075180054 = 1.037007212638855 + 0.001 * 6.987142562866211
Epoch 150, val loss: 1.204827904701233
Epoch 160, training loss: 0.9489868879318237 = 0.9420639872550964 + 0.001 * 6.922893524169922
Epoch 160, val loss: 1.1342942714691162
Epoch 170, training loss: 0.8631620407104492 = 0.8562607169151306 + 0.001 * 6.901333332061768
Epoch 170, val loss: 1.0716627836227417
Epoch 180, training loss: 0.7863537669181824 = 0.7794615626335144 + 0.001 * 6.892230987548828
Epoch 180, val loss: 1.0167831182479858
Epoch 190, training loss: 0.7179970741271973 = 0.7111095190048218 + 0.001 * 6.887561798095703
Epoch 190, val loss: 0.9692010879516602
Epoch 200, training loss: 0.6569196581840515 = 0.6500343084335327 + 0.001 * 6.88533353805542
Epoch 200, val loss: 0.9281224608421326
Epoch 210, training loss: 0.6011773347854614 = 0.5942939519882202 + 0.001 * 6.883360385894775
Epoch 210, val loss: 0.8920365571975708
Epoch 220, training loss: 0.5488520264625549 = 0.5419707298278809 + 0.001 * 6.881278991699219
Epoch 220, val loss: 0.8596661686897278
Epoch 230, training loss: 0.49875134229660034 = 0.49187225103378296 + 0.001 * 6.879087448120117
Epoch 230, val loss: 0.8301528692245483
Epoch 240, training loss: 0.45035919547080994 = 0.4434824585914612 + 0.001 * 6.876741886138916
Epoch 240, val loss: 0.8035320043563843
Epoch 250, training loss: 0.40354418754577637 = 0.3966708481311798 + 0.001 * 6.873324394226074
Epoch 250, val loss: 0.7796079516410828
Epoch 260, training loss: 0.35848602652549744 = 0.35161611437797546 + 0.001 * 6.869898319244385
Epoch 260, val loss: 0.7580484747886658
Epoch 270, training loss: 0.31564369797706604 = 0.308777779340744 + 0.001 * 6.865919589996338
Epoch 270, val loss: 0.7387604117393494
Epoch 280, training loss: 0.2757323384284973 = 0.2688698470592499 + 0.001 * 6.862477779388428
Epoch 280, val loss: 0.7217076420783997
Epoch 290, training loss: 0.23947276175022125 = 0.23261603713035583 + 0.001 * 6.856729507446289
Epoch 290, val loss: 0.7072530388832092
Epoch 300, training loss: 0.20730815827846527 = 0.20045752823352814 + 0.001 * 6.850635528564453
Epoch 300, val loss: 0.6956316232681274
Epoch 310, training loss: 0.1793571412563324 = 0.17251335084438324 + 0.001 * 6.843787670135498
Epoch 310, val loss: 0.6871111989021301
Epoch 320, training loss: 0.15540246665477753 = 0.14856770634651184 + 0.001 * 6.834758281707764
Epoch 320, val loss: 0.6817983388900757
Epoch 330, training loss: 0.13502568006515503 = 0.12819847464561462 + 0.001 * 6.827201843261719
Epoch 330, val loss: 0.6796748638153076
Epoch 340, training loss: 0.11773338913917542 = 0.1109161227941513 + 0.001 * 6.8172688484191895
Epoch 340, val loss: 0.6805959343910217
Epoch 350, training loss: 0.10308413207530975 = 0.09627382457256317 + 0.001 * 6.810310363769531
Epoch 350, val loss: 0.6842114329338074
Epoch 360, training loss: 0.0906708613038063 = 0.08386926352977753 + 0.001 * 6.801598072052002
Epoch 360, val loss: 0.6901618242263794
Epoch 370, training loss: 0.08015456050634384 = 0.07336422801017761 + 0.001 * 6.790335655212402
Epoch 370, val loss: 0.6980652213096619
Epoch 380, training loss: 0.0712519958615303 = 0.06446699053049088 + 0.001 * 6.78500509262085
Epoch 380, val loss: 0.7074975371360779
Epoch 390, training loss: 0.06370443850755692 = 0.05692285671830177 + 0.001 * 6.781583309173584
Epoch 390, val loss: 0.7180987596511841
Epoch 400, training loss: 0.05728782340884209 = 0.050510991364717484 + 0.001 * 6.77683162689209
Epoch 400, val loss: 0.7295274138450623
Epoch 410, training loss: 0.051824431866407394 = 0.04504559189081192 + 0.001 * 6.778838634490967
Epoch 410, val loss: 0.7414891719818115
Epoch 420, training loss: 0.047149017453193665 = 0.04037169739603996 + 0.001 * 6.777318477630615
Epoch 420, val loss: 0.7537294030189514
Epoch 430, training loss: 0.04312954843044281 = 0.036358390003442764 + 0.001 * 6.771156311035156
Epoch 430, val loss: 0.7660787105560303
Epoch 440, training loss: 0.03967292234301567 = 0.032896775752305984 + 0.001 * 6.776145935058594
Epoch 440, val loss: 0.7784249782562256
Epoch 450, training loss: 0.036663345992565155 = 0.02989717572927475 + 0.001 * 6.766168117523193
Epoch 450, val loss: 0.7906614542007446
Epoch 460, training loss: 0.034052614122629166 = 0.027285199612379074 + 0.001 * 6.7674150466918945
Epoch 460, val loss: 0.8027417063713074
Epoch 470, training loss: 0.0317675918340683 = 0.024999506771564484 + 0.001 * 6.7680840492248535
Epoch 470, val loss: 0.8145837783813477
Epoch 480, training loss: 0.029750406742095947 = 0.022989701479673386 + 0.001 * 6.760704517364502
Epoch 480, val loss: 0.8261961936950684
Epoch 490, training loss: 0.02799437940120697 = 0.02121409773826599 + 0.001 * 6.780282497406006
Epoch 490, val loss: 0.8375535607337952
Epoch 500, training loss: 0.026396043598651886 = 0.019638272002339363 + 0.001 * 6.757770538330078
Epoch 500, val loss: 0.8486335873603821
Epoch 510, training loss: 0.024985356256365776 = 0.018233971670269966 + 0.001 * 6.751384735107422
Epoch 510, val loss: 0.8594423532485962
Epoch 520, training loss: 0.023763220757246017 = 0.01697741076350212 + 0.001 * 6.785810470581055
Epoch 520, val loss: 0.8699787259101868
Epoch 530, training loss: 0.022597752511501312 = 0.015848781913518906 + 0.001 * 6.748970031738281
Epoch 530, val loss: 0.8802493214607239
Epoch 540, training loss: 0.021577801555395126 = 0.01483159326016903 + 0.001 * 6.746208190917969
Epoch 540, val loss: 0.8902643918991089
Epoch 550, training loss: 0.020663630217313766 = 0.01391192153096199 + 0.001 * 6.751708030700684
Epoch 550, val loss: 0.9000409841537476
Epoch 560, training loss: 0.019824666902422905 = 0.013077530078589916 + 0.001 * 6.74713659286499
Epoch 560, val loss: 0.9095712304115295
Epoch 570, training loss: 0.019063439220190048 = 0.012318383902311325 + 0.001 * 6.745055198669434
Epoch 570, val loss: 0.9188699126243591
Epoch 580, training loss: 0.01837502419948578 = 0.011625627987086773 + 0.001 * 6.749395370483398
Epoch 580, val loss: 0.9279214143753052
Epoch 590, training loss: 0.017727559432387352 = 0.010991908609867096 + 0.001 * 6.735651016235352
Epoch 590, val loss: 0.9367568492889404
Epoch 600, training loss: 0.017151696607470512 = 0.010410643182694912 + 0.001 * 6.741053581237793
Epoch 600, val loss: 0.9453898668289185
Epoch 610, training loss: 0.016604648903012276 = 0.009876342490315437 + 0.001 * 6.728306770324707
Epoch 610, val loss: 0.9537960290908813
Epoch 620, training loss: 0.016111474484205246 = 0.009384025819599628 + 0.001 * 6.727447509765625
Epoch 620, val loss: 0.9620122909545898
Epoch 630, training loss: 0.01566009595990181 = 0.008929395116865635 + 0.001 * 6.7307000160217285
Epoch 630, val loss: 0.9700465202331543
Epoch 640, training loss: 0.015234753489494324 = 0.008508686907589436 + 0.001 * 6.726065635681152
Epoch 640, val loss: 0.9778929352760315
Epoch 650, training loss: 0.014846481382846832 = 0.008118655532598495 + 0.001 * 6.727826118469238
Epoch 650, val loss: 0.9855619072914124
Epoch 660, training loss: 0.01447193045169115 = 0.007756355684250593 + 0.001 * 6.715574264526367
Epoch 660, val loss: 0.9930382370948792
Epoch 670, training loss: 0.014129441231489182 = 0.007419217843562365 + 0.001 * 6.71022367477417
Epoch 670, val loss: 1.0003544092178345
Epoch 680, training loss: 0.013840226456522942 = 0.007104998920112848 + 0.001 * 6.735226631164551
Epoch 680, val loss: 1.0075079202651978
Epoch 690, training loss: 0.013523241505026817 = 0.0068116942420601845 + 0.001 * 6.711546897888184
Epoch 690, val loss: 1.014500379562378
Epoch 700, training loss: 0.013245608657598495 = 0.0065374416299164295 + 0.001 * 6.70816707611084
Epoch 700, val loss: 1.0213581323623657
Epoch 710, training loss: 0.012989439070224762 = 0.0062806261703372 + 0.001 * 6.708813190460205
Epoch 710, val loss: 1.0280641317367554
Epoch 720, training loss: 0.012742847204208374 = 0.006039820145815611 + 0.001 * 6.70302677154541
Epoch 720, val loss: 1.0346336364746094
Epoch 730, training loss: 0.012519417330622673 = 0.005813705734908581 + 0.001 * 6.7057108879089355
Epoch 730, val loss: 1.0410598516464233
Epoch 740, training loss: 0.012317914515733719 = 0.005601141601800919 + 0.001 * 6.71677303314209
Epoch 740, val loss: 1.0473482608795166
Epoch 750, training loss: 0.012106847949326038 = 0.005401005502790213 + 0.001 * 6.705842018127441
Epoch 750, val loss: 1.0535218715667725
Epoch 760, training loss: 0.011925511993467808 = 0.005212357733398676 + 0.001 * 6.713153839111328
Epoch 760, val loss: 1.0595752000808716
Epoch 770, training loss: 0.011753806844353676 = 0.005034355912357569 + 0.001 * 6.719450950622559
Epoch 770, val loss: 1.0655063390731812
Epoch 780, training loss: 0.011574761942029 = 0.0048661925829946995 + 0.001 * 6.708568572998047
Epoch 780, val loss: 1.0713142156600952
Epoch 790, training loss: 0.011410677805542946 = 0.00470721023157239 + 0.001 * 6.703466892242432
Epoch 790, val loss: 1.0770106315612793
Epoch 800, training loss: 0.011274763382971287 = 0.004556705243885517 + 0.001 * 6.718057632446289
Epoch 800, val loss: 1.0826009511947632
Epoch 810, training loss: 0.011113779619336128 = 0.004414101131260395 + 0.001 * 6.69967794418335
Epoch 810, val loss: 1.0880929231643677
Epoch 820, training loss: 0.010970652103424072 = 0.004278835840523243 + 0.001 * 6.691815376281738
Epoch 820, val loss: 1.093477725982666
Epoch 830, training loss: 0.0108941700309515 = 0.004150393884629011 + 0.001 * 6.743776321411133
Epoch 830, val loss: 1.0987560749053955
Epoch 840, training loss: 0.010723554529249668 = 0.00402838084846735 + 0.001 * 6.695173263549805
Epoch 840, val loss: 1.1039265394210815
Epoch 850, training loss: 0.010604722425341606 = 0.0039123413152992725 + 0.001 * 6.692381381988525
Epoch 850, val loss: 1.1090179681777954
Epoch 860, training loss: 0.01048593781888485 = 0.0038018845953047276 + 0.001 * 6.68405294418335
Epoch 860, val loss: 1.1140249967575073
Epoch 870, training loss: 0.010396674275398254 = 0.0036966626066714525 + 0.001 * 6.700011730194092
Epoch 870, val loss: 1.11893630027771
Epoch 880, training loss: 0.010299218818545341 = 0.0035963591653853655 + 0.001 * 6.702858924865723
Epoch 880, val loss: 1.1237527132034302
Epoch 890, training loss: 0.010187831707298756 = 0.003500658320263028 + 0.001 * 6.687172889709473
Epoch 890, val loss: 1.1284842491149902
Epoch 900, training loss: 0.010100817307829857 = 0.00340928859077394 + 0.001 * 6.691528797149658
Epoch 900, val loss: 1.133142352104187
Epoch 910, training loss: 0.010002071037888527 = 0.00332197779789567 + 0.001 * 6.680092811584473
Epoch 910, val loss: 1.1377168893814087
Epoch 920, training loss: 0.009937691502273083 = 0.0032385012600570917 + 0.001 * 6.69918966293335
Epoch 920, val loss: 1.1422145366668701
Epoch 930, training loss: 0.009858849458396435 = 0.003158627776429057 + 0.001 * 6.700221538543701
Epoch 930, val loss: 1.1466373205184937
Epoch 940, training loss: 0.009774894453585148 = 0.0030821815598756075 + 0.001 * 6.692712783813477
Epoch 940, val loss: 1.150986909866333
Epoch 950, training loss: 0.009691703133285046 = 0.003008928382769227 + 0.001 * 6.682774066925049
Epoch 950, val loss: 1.1552587747573853
Epoch 960, training loss: 0.009616291150450706 = 0.002938712015748024 + 0.001 * 6.677579402923584
Epoch 960, val loss: 1.1594630479812622
Epoch 970, training loss: 0.009549306705594063 = 0.0028713697101920843 + 0.001 * 6.677937030792236
Epoch 970, val loss: 1.1635955572128296
Epoch 980, training loss: 0.009480183944106102 = 0.0028067505918443203 + 0.001 * 6.673432350158691
Epoch 980, val loss: 1.167667031288147
Epoch 990, training loss: 0.009415424428880215 = 0.0027446916792541742 + 0.001 * 6.670732021331787
Epoch 990, val loss: 1.1716711521148682
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9470314979553223 = 1.9386576414108276 + 0.001 * 8.373839378356934
Epoch 0, val loss: 1.9329768419265747
Epoch 10, training loss: 1.9372992515563965 = 1.9289255142211914 + 0.001 * 8.37372875213623
Epoch 10, val loss: 1.9241782426834106
Epoch 20, training loss: 1.925032138824463 = 1.916658639907837 + 0.001 * 8.373438835144043
Epoch 20, val loss: 1.912766695022583
Epoch 30, training loss: 1.9076071977615356 = 1.8992342948913574 + 0.001 * 8.372864723205566
Epoch 30, val loss: 1.8963873386383057
Epoch 40, training loss: 1.88190758228302 = 1.8735359907150269 + 0.001 * 8.371646881103516
Epoch 40, val loss: 1.8724191188812256
Epoch 50, training loss: 1.8459562063217163 = 1.8375877141952515 + 0.001 * 8.368451118469238
Epoch 50, val loss: 1.840390920639038
Epoch 60, training loss: 1.8044049739837646 = 1.7960481643676758 + 0.001 * 8.356867790222168
Epoch 60, val loss: 1.8073042631149292
Epoch 70, training loss: 1.764807939529419 = 1.7565158605575562 + 0.001 * 8.292080879211426
Epoch 70, val loss: 1.7774909734725952
Epoch 80, training loss: 1.713951826095581 = 1.706047773361206 + 0.001 * 7.90402889251709
Epoch 80, val loss: 1.7337745428085327
Epoch 90, training loss: 1.6438826322555542 = 1.6361595392227173 + 0.001 * 7.723057270050049
Epoch 90, val loss: 1.6738567352294922
Epoch 100, training loss: 1.5539870262145996 = 1.5463966131210327 + 0.001 * 7.590409755706787
Epoch 100, val loss: 1.601722002029419
Epoch 110, training loss: 1.453174114227295 = 1.4458376169204712 + 0.001 * 7.336493492126465
Epoch 110, val loss: 1.5215250253677368
Epoch 120, training loss: 1.349746584892273 = 1.3425427675247192 + 0.001 * 7.203832626342773
Epoch 120, val loss: 1.438874363899231
Epoch 130, training loss: 1.246178388595581 = 1.2390257120132446 + 0.001 * 7.152673244476318
Epoch 130, val loss: 1.3570623397827148
Epoch 140, training loss: 1.1435133218765259 = 1.1364402770996094 + 0.001 * 7.0730204582214355
Epoch 140, val loss: 1.2768397331237793
Epoch 150, training loss: 1.0439693927764893 = 1.0369439125061035 + 0.001 * 7.025522708892822
Epoch 150, val loss: 1.200032114982605
Epoch 160, training loss: 0.9501602053642273 = 0.9431618452072144 + 0.001 * 6.99833869934082
Epoch 160, val loss: 1.1285068988800049
Epoch 170, training loss: 0.8633973002433777 = 0.8564199805259705 + 0.001 * 6.977331161499023
Epoch 170, val loss: 1.0629749298095703
Epoch 180, training loss: 0.7839033007621765 = 0.7769396305084229 + 0.001 * 6.963656425476074
Epoch 180, val loss: 1.003370761871338
Epoch 190, training loss: 0.7115748524665833 = 0.7046175599098206 + 0.001 * 6.957294940948486
Epoch 190, val loss: 0.9503954648971558
Epoch 200, training loss: 0.6455269455909729 = 0.6385723352432251 + 0.001 * 6.954617977142334
Epoch 200, val loss: 0.9040113091468811
Epoch 210, training loss: 0.5841394066810608 = 0.5771875977516174 + 0.001 * 6.95180082321167
Epoch 210, val loss: 0.8637418150901794
Epoch 220, training loss: 0.5259498953819275 = 0.519001841545105 + 0.001 * 6.948057651519775
Epoch 220, val loss: 0.8287530541419983
Epoch 230, training loss: 0.4702202379703522 = 0.4632761776447296 + 0.001 * 6.9440717697143555
Epoch 230, val loss: 0.7983275055885315
Epoch 240, training loss: 0.4170512557029724 = 0.41011112928390503 + 0.001 * 6.940135955810547
Epoch 240, val loss: 0.7725465297698975
Epoch 250, training loss: 0.3670906722545624 = 0.3601541519165039 + 0.001 * 6.936526298522949
Epoch 250, val loss: 0.7519498467445374
Epoch 260, training loss: 0.3211432099342346 = 0.31420937180519104 + 0.001 * 6.93385124206543
Epoch 260, val loss: 0.7370032072067261
Epoch 270, training loss: 0.2797873318195343 = 0.27285701036453247 + 0.001 * 6.9303202629089355
Epoch 270, val loss: 0.7275283932685852
Epoch 280, training loss: 0.2432360202074051 = 0.23630845546722412 + 0.001 * 6.92756462097168
Epoch 280, val loss: 0.7229026556015015
Epoch 290, training loss: 0.21144378185272217 = 0.20451882481575012 + 0.001 * 6.924962997436523
Epoch 290, val loss: 0.7226405143737793
Epoch 300, training loss: 0.18417203426361084 = 0.17724797129631042 + 0.001 * 6.924067497253418
Epoch 300, val loss: 0.7258918881416321
Epoch 310, training loss: 0.16097882390022278 = 0.1540578007698059 + 0.001 * 6.921027660369873
Epoch 310, val loss: 0.7320859432220459
Epoch 320, training loss: 0.14133727550506592 = 0.1344192773103714 + 0.001 * 6.917998313903809
Epoch 320, val loss: 0.740769624710083
Epoch 330, training loss: 0.12470162659883499 = 0.11778616905212402 + 0.001 * 6.915459156036377
Epoch 330, val loss: 0.7513842582702637
Epoch 340, training loss: 0.11056806147098541 = 0.10365552455186844 + 0.001 * 6.912539005279541
Epoch 340, val loss: 0.7634590864181519
Epoch 350, training loss: 0.09850072860717773 = 0.09159161895513535 + 0.001 * 6.909108638763428
Epoch 350, val loss: 0.7766267657279968
Epoch 360, training loss: 0.08814617246389389 = 0.08123943209648132 + 0.001 * 6.906739711761475
Epoch 360, val loss: 0.7905175685882568
Epoch 370, training loss: 0.07920636236667633 = 0.07230912148952484 + 0.001 * 6.897239685058594
Epoch 370, val loss: 0.8049745559692383
Epoch 380, training loss: 0.0714598000049591 = 0.06456854194402695 + 0.001 * 6.891254901885986
Epoch 380, val loss: 0.819742739200592
Epoch 390, training loss: 0.06472041457891464 = 0.057833097875118256 + 0.001 * 6.887318134307861
Epoch 390, val loss: 0.8347429037094116
Epoch 400, training loss: 0.058827031403779984 = 0.05195055156946182 + 0.001 * 6.876480579376221
Epoch 400, val loss: 0.8498059511184692
Epoch 410, training loss: 0.05367173254489899 = 0.04680018872022629 + 0.001 * 6.871542453765869
Epoch 410, val loss: 0.8649652600288391
Epoch 420, training loss: 0.04915018379688263 = 0.04228239879012108 + 0.001 * 6.867784023284912
Epoch 420, val loss: 0.8800477981567383
Epoch 430, training loss: 0.045172225683927536 = 0.038313690572977066 + 0.001 * 6.8585357666015625
Epoch 430, val loss: 0.8949837684631348
Epoch 440, training loss: 0.04167107492685318 = 0.03482302278280258 + 0.001 * 6.848053455352783
Epoch 440, val loss: 0.9097538590431213
Epoch 450, training loss: 0.03859608620405197 = 0.03174790367484093 + 0.001 * 6.848180294036865
Epoch 450, val loss: 0.9242452383041382
Epoch 460, training loss: 0.03588269650936127 = 0.029033685103058815 + 0.001 * 6.8490095138549805
Epoch 460, val loss: 0.9384959936141968
Epoch 470, training loss: 0.03347522392868996 = 0.02663249708712101 + 0.001 * 6.84272575378418
Epoch 470, val loss: 0.9524504542350769
Epoch 480, training loss: 0.03134075924754143 = 0.02450280264019966 + 0.001 * 6.837956428527832
Epoch 480, val loss: 0.9660742878913879
Epoch 490, training loss: 0.02945852093398571 = 0.022608982399106026 + 0.001 * 6.849538803100586
Epoch 490, val loss: 0.9793137311935425
Epoch 500, training loss: 0.027753697708249092 = 0.02091994509100914 + 0.001 * 6.833752632141113
Epoch 500, val loss: 0.9922488927841187
Epoch 510, training loss: 0.026240700855851173 = 0.01940891705453396 + 0.001 * 6.831783294677734
Epoch 510, val loss: 1.0047850608825684
Epoch 520, training loss: 0.024882810190320015 = 0.018052848055958748 + 0.001 * 6.82996129989624
Epoch 520, val loss: 1.0169600248336792
Epoch 530, training loss: 0.023661883547902107 = 0.01683216728270054 + 0.001 * 6.829716205596924
Epoch 530, val loss: 1.0288339853286743
Epoch 540, training loss: 0.022560877725481987 = 0.015730179846286774 + 0.001 * 6.830698013305664
Epoch 540, val loss: 1.0403356552124023
Epoch 550, training loss: 0.021564194932579994 = 0.014732633717358112 + 0.001 * 6.8315606117248535
Epoch 550, val loss: 1.051520824432373
Epoch 560, training loss: 0.020651455968618393 = 0.013827186077833176 + 0.001 * 6.8242692947387695
Epoch 560, val loss: 1.0624310970306396
Epoch 570, training loss: 0.019825443625450134 = 0.013003217056393623 + 0.001 * 6.822225570678711
Epoch 570, val loss: 1.0729931592941284
Epoch 580, training loss: 0.019081909209489822 = 0.012251373380422592 + 0.001 * 6.830536365509033
Epoch 580, val loss: 1.0832968950271606
Epoch 590, training loss: 0.018384050577878952 = 0.011563785374164581 + 0.001 * 6.820265769958496
Epoch 590, val loss: 1.093313455581665
Epoch 600, training loss: 0.017771298065781593 = 0.010933469980955124 + 0.001 * 6.837828159332275
Epoch 600, val loss: 1.1030346155166626
Epoch 610, training loss: 0.017168454825878143 = 0.010354387573897839 + 0.001 * 6.814067363739014
Epoch 610, val loss: 1.1125025749206543
Epoch 620, training loss: 0.016632385551929474 = 0.009821360930800438 + 0.001 * 6.811025142669678
Epoch 620, val loss: 1.1217130422592163
Epoch 630, training loss: 0.01613708958029747 = 0.009329692460596561 + 0.001 * 6.807396411895752
Epoch 630, val loss: 1.1306864023208618
Epoch 640, training loss: 0.01568477414548397 = 0.008875234052538872 + 0.001 * 6.809539794921875
Epoch 640, val loss: 1.1394528150558472
Epoch 650, training loss: 0.015262625180184841 = 0.008454428054392338 + 0.001 * 6.808197021484375
Epoch 650, val loss: 1.1479641199111938
Epoch 660, training loss: 0.014879131689667702 = 0.008064044639468193 + 0.001 * 6.815086364746094
Epoch 660, val loss: 1.1562838554382324
Epoch 670, training loss: 0.01450764574110508 = 0.0077013010159134865 + 0.001 * 6.806344032287598
Epoch 670, val loss: 1.164374828338623
Epoch 680, training loss: 0.014172891154885292 = 0.007363666780292988 + 0.001 * 6.809223651885986
Epoch 680, val loss: 1.1722924709320068
Epoch 690, training loss: 0.013851792551577091 = 0.007048946339637041 + 0.001 * 6.8028459548950195
Epoch 690, val loss: 1.1799982786178589
Epoch 700, training loss: 0.013557957485318184 = 0.006755114533007145 + 0.001 * 6.802842140197754
Epoch 700, val loss: 1.1875360012054443
Epoch 710, training loss: 0.013294033706188202 = 0.0064803967252373695 + 0.001 * 6.813636302947998
Epoch 710, val loss: 1.194878101348877
Epoch 720, training loss: 0.013014989905059338 = 0.006223181262612343 + 0.001 * 6.791808128356934
Epoch 720, val loss: 1.2020522356033325
Epoch 730, training loss: 0.012766154482960701 = 0.005982021801173687 + 0.001 * 6.784132480621338
Epoch 730, val loss: 1.2090591192245483
Epoch 740, training loss: 0.012551607564091682 = 0.005755652207881212 + 0.001 * 6.795955657958984
Epoch 740, val loss: 1.2158986330032349
Epoch 750, training loss: 0.012338201515376568 = 0.0055428738705813885 + 0.001 * 6.795327186584473
Epoch 750, val loss: 1.22258722782135
Epoch 760, training loss: 0.012132981792092323 = 0.005342620890587568 + 0.001 * 6.790360450744629
Epoch 760, val loss: 1.2291381359100342
Epoch 770, training loss: 0.011930704116821289 = 0.005153965670615435 + 0.001 * 6.77673864364624
Epoch 770, val loss: 1.235555648803711
Epoch 780, training loss: 0.01176842488348484 = 0.004976015537977219 + 0.001 * 6.7924089431762695
Epoch 780, val loss: 1.2418423891067505
Epoch 790, training loss: 0.011590050533413887 = 0.004807983059436083 + 0.001 * 6.78206729888916
Epoch 790, val loss: 1.2479703426361084
Epoch 800, training loss: 0.011437870562076569 = 0.0046491604298353195 + 0.001 * 6.78870964050293
Epoch 800, val loss: 1.2539710998535156
Epoch 810, training loss: 0.011269940994679928 = 0.004498861730098724 + 0.001 * 6.771079063415527
Epoch 810, val loss: 1.259855031967163
Epoch 820, training loss: 0.011129220947623253 = 0.004356523975729942 + 0.001 * 6.772696495056152
Epoch 820, val loss: 1.2656100988388062
Epoch 830, training loss: 0.010985249653458595 = 0.004221554379910231 + 0.001 * 6.7636942863464355
Epoch 830, val loss: 1.2712578773498535
Epoch 840, training loss: 0.010889880359172821 = 0.004093484487384558 + 0.001 * 6.796395778656006
Epoch 840, val loss: 1.2767843008041382
Epoch 850, training loss: 0.010740606114268303 = 0.003971877042204142 + 0.001 * 6.768728733062744
Epoch 850, val loss: 1.282218098640442
Epoch 860, training loss: 0.010626226663589478 = 0.0038562654517591 + 0.001 * 6.769960880279541
Epoch 860, val loss: 1.2875233888626099
Epoch 870, training loss: 0.01056039147078991 = 0.0037462820764631033 + 0.001 * 6.8141093254089355
Epoch 870, val loss: 1.292741298675537
Epoch 880, training loss: 0.010406894609332085 = 0.0036415732465684414 + 0.001 * 6.765320777893066
Epoch 880, val loss: 1.297849178314209
Epoch 890, training loss: 0.010305368341505527 = 0.0035418022889643908 + 0.001 * 6.763565540313721
Epoch 890, val loss: 1.3028546571731567
Epoch 900, training loss: 0.010214726440608501 = 0.0034466448705643415 + 0.001 * 6.7680816650390625
Epoch 900, val loss: 1.307774305343628
Epoch 910, training loss: 0.01012431737035513 = 0.0033558381255716085 + 0.001 * 6.768478870391846
Epoch 910, val loss: 1.3125883340835571
Epoch 920, training loss: 0.010044054128229618 = 0.003269114764407277 + 0.001 * 6.774939060211182
Epoch 920, val loss: 1.317335844039917
Epoch 930, training loss: 0.009936954826116562 = 0.0031862303148955107 + 0.001 * 6.7507243156433105
Epoch 930, val loss: 1.3219724893569946
Epoch 940, training loss: 0.009849806316196918 = 0.003106969641521573 + 0.001 * 6.7428364753723145
Epoch 940, val loss: 1.3265341520309448
Epoch 950, training loss: 0.009782694280147552 = 0.003031129250302911 + 0.001 * 6.7515645027160645
Epoch 950, val loss: 1.3309988975524902
Epoch 960, training loss: 0.00971513707190752 = 0.0029585009906440973 + 0.001 * 6.756636142730713
Epoch 960, val loss: 1.3354336023330688
Epoch 970, training loss: 0.009643364697694778 = 0.002888943301513791 + 0.001 * 6.754420757293701
Epoch 970, val loss: 1.339748501777649
Epoch 980, training loss: 0.009569821879267693 = 0.0028222338296473026 + 0.001 * 6.747588157653809
Epoch 980, val loss: 1.3439910411834717
Epoch 990, training loss: 0.009515914134681225 = 0.0027582549955695868 + 0.001 * 6.757658958435059
Epoch 990, val loss: 1.348159909248352
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6716
Flip ASR: 0.6044/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9400215148925781 = 1.931647777557373 + 0.001 * 8.373786926269531
Epoch 0, val loss: 1.9207826852798462
Epoch 10, training loss: 1.9300270080566406 = 1.921653389930725 + 0.001 * 8.373653411865234
Epoch 10, val loss: 1.9096647500991821
Epoch 20, training loss: 1.917823076248169 = 1.9094499349594116 + 0.001 * 8.373190879821777
Epoch 20, val loss: 1.8954944610595703
Epoch 30, training loss: 1.9007974863052368 = 1.8924251794815063 + 0.001 * 8.372356414794922
Epoch 30, val loss: 1.8755314350128174
Epoch 40, training loss: 1.8760098218917847 = 1.8676389455795288 + 0.001 * 8.370901107788086
Epoch 40, val loss: 1.847194790840149
Epoch 50, training loss: 1.841855525970459 = 1.8334873914718628 + 0.001 * 8.368117332458496
Epoch 50, val loss: 1.8105781078338623
Epoch 60, training loss: 1.8006434440612793 = 1.7922828197479248 + 0.001 * 8.360634803771973
Epoch 60, val loss: 1.7709473371505737
Epoch 70, training loss: 1.7575689554214478 = 1.749242901802063 + 0.001 * 8.326032638549805
Epoch 70, val loss: 1.7348361015319824
Epoch 80, training loss: 1.7059290409088135 = 1.697881817817688 + 0.001 * 8.047229766845703
Epoch 80, val loss: 1.6925427913665771
Epoch 90, training loss: 1.6339213848114014 = 1.6262940168380737 + 0.001 * 7.627415180206299
Epoch 90, val loss: 1.6324342489242554
Epoch 100, training loss: 1.5386571884155273 = 1.5311858654022217 + 0.001 * 7.471296787261963
Epoch 100, val loss: 1.5535757541656494
Epoch 110, training loss: 1.4267630577087402 = 1.4194613695144653 + 0.001 * 7.301727771759033
Epoch 110, val loss: 1.465291976928711
Epoch 120, training loss: 1.3120648860931396 = 1.3047822713851929 + 0.001 * 7.282611846923828
Epoch 120, val loss: 1.3792530298233032
Epoch 130, training loss: 1.2037508487701416 = 1.1964908838272095 + 0.001 * 7.259953022003174
Epoch 130, val loss: 1.3020001649856567
Epoch 140, training loss: 1.1059173345565796 = 1.0986862182617188 + 0.001 * 7.231091499328613
Epoch 140, val loss: 1.234650731086731
Epoch 150, training loss: 1.0192956924438477 = 1.012102723121643 + 0.001 * 7.193009376525879
Epoch 150, val loss: 1.1753498315811157
Epoch 160, training loss: 0.9419620633125305 = 0.9348099827766418 + 0.001 * 7.152094841003418
Epoch 160, val loss: 1.12088143825531
Epoch 170, training loss: 0.8699670433998108 = 0.8628548979759216 + 0.001 * 7.112154483795166
Epoch 170, val loss: 1.0685718059539795
Epoch 180, training loss: 0.7995377779006958 = 0.7924575805664062 + 0.001 * 7.080188751220703
Epoch 180, val loss: 1.0170989036560059
Epoch 190, training loss: 0.7285122871398926 = 0.721460223197937 + 0.001 * 7.052054405212402
Epoch 190, val loss: 0.966369092464447
Epoch 200, training loss: 0.6572137475013733 = 0.6501877307891846 + 0.001 * 7.026040554046631
Epoch 200, val loss: 0.9170873165130615
Epoch 210, training loss: 0.5877659916877747 = 0.5807599425315857 + 0.001 * 7.006068706512451
Epoch 210, val loss: 0.8718537092208862
Epoch 220, training loss: 0.5225702524185181 = 0.515576183795929 + 0.001 * 6.994082450866699
Epoch 220, val loss: 0.8333215713500977
Epoch 230, training loss: 0.4631440043449402 = 0.4561574161052704 + 0.001 * 6.986595630645752
Epoch 230, val loss: 0.80341637134552
Epoch 240, training loss: 0.4099265933036804 = 0.40294569730758667 + 0.001 * 6.980889320373535
Epoch 240, val loss: 0.7819706201553345
Epoch 250, training loss: 0.3624337315559387 = 0.3554568588733673 + 0.001 * 6.97688627243042
Epoch 250, val loss: 0.7685092687606812
Epoch 260, training loss: 0.3199664354324341 = 0.3129928410053253 + 0.001 * 6.973592758178711
Epoch 260, val loss: 0.7619307041168213
Epoch 270, training loss: 0.28210940957069397 = 0.2751379907131195 + 0.001 * 6.971432209014893
Epoch 270, val loss: 0.7608053088188171
Epoch 280, training loss: 0.24852138757705688 = 0.24155330657958984 + 0.001 * 6.968078136444092
Epoch 280, val loss: 0.7643819451332092
Epoch 290, training loss: 0.2189132422208786 = 0.21195007860660553 + 0.001 * 6.963164329528809
Epoch 290, val loss: 0.7720432877540588
Epoch 300, training loss: 0.19306489825248718 = 0.18610623478889465 + 0.001 * 6.958666801452637
Epoch 300, val loss: 0.782952606678009
Epoch 310, training loss: 0.17065274715423584 = 0.16369806230068207 + 0.001 * 6.954683780670166
Epoch 310, val loss: 0.796804666519165
Epoch 320, training loss: 0.1513345092535019 = 0.14438790082931519 + 0.001 * 6.946610450744629
Epoch 320, val loss: 0.8133321404457092
Epoch 330, training loss: 0.13470439612865448 = 0.12776358425617218 + 0.001 * 6.9408159255981445
Epoch 330, val loss: 0.8319205641746521
Epoch 340, training loss: 0.12033694237470627 = 0.11340539902448654 + 0.001 * 6.931543350219727
Epoch 340, val loss: 0.8518020510673523
Epoch 350, training loss: 0.10778612643480301 = 0.10086514800786972 + 0.001 * 6.920976161956787
Epoch 350, val loss: 0.8726275563240051
Epoch 360, training loss: 0.0966837927699089 = 0.08976949751377106 + 0.001 * 6.914297580718994
Epoch 360, val loss: 0.8938234448432922
Epoch 370, training loss: 0.08659796416759491 = 0.0796922892332077 + 0.001 * 6.905673980712891
Epoch 370, val loss: 0.9151294231414795
Epoch 380, training loss: 0.0772145688533783 = 0.07032229006290436 + 0.001 * 6.892279148101807
Epoch 380, val loss: 0.9364193081855774
Epoch 390, training loss: 0.06847967952489853 = 0.06159760057926178 + 0.001 * 6.882079124450684
Epoch 390, val loss: 0.9579066634178162
Epoch 400, training loss: 0.06131391599774361 = 0.05443074181675911 + 0.001 * 6.883173942565918
Epoch 400, val loss: 0.9794378280639648
Epoch 410, training loss: 0.05569876730442047 = 0.04881773516535759 + 0.001 * 6.881030559539795
Epoch 410, val loss: 1.0014336109161377
Epoch 420, training loss: 0.0509551540017128 = 0.04408847540616989 + 0.001 * 6.866678237915039
Epoch 420, val loss: 1.0228490829467773
Epoch 430, training loss: 0.04684564098715782 = 0.03998804837465286 + 0.001 * 6.8575921058654785
Epoch 430, val loss: 1.0438714027404785
Epoch 440, training loss: 0.043265704065561295 = 0.03641265630722046 + 0.001 * 6.853046417236328
Epoch 440, val loss: 1.0639523267745972
Epoch 450, training loss: 0.04013223946094513 = 0.03328002989292145 + 0.001 * 6.852210998535156
Epoch 450, val loss: 1.0832959413528442
Epoch 460, training loss: 0.03736569732427597 = 0.03051670826971531 + 0.001 * 6.848987102508545
Epoch 460, val loss: 1.101668357849121
Epoch 470, training loss: 0.03490663319826126 = 0.02805902622640133 + 0.001 * 6.847604751586914
Epoch 470, val loss: 1.1194320917129517
Epoch 480, training loss: 0.03270621970295906 = 0.02586178481578827 + 0.001 * 6.84443473815918
Epoch 480, val loss: 1.1365373134613037
Epoch 490, training loss: 0.030754733830690384 = 0.023897390812635422 + 0.001 * 6.8573431968688965
Epoch 490, val loss: 1.1531533002853394
Epoch 500, training loss: 0.028983425348997116 = 0.022135591134428978 + 0.001 * 6.847834587097168
Epoch 500, val loss: 1.1692105531692505
Epoch 510, training loss: 0.027394361793994904 = 0.02054954692721367 + 0.001 * 6.844815254211426
Epoch 510, val loss: 1.184779405593872
Epoch 520, training loss: 0.025955837219953537 = 0.019118400290608406 + 0.001 * 6.837436676025391
Epoch 520, val loss: 1.1998696327209473
Epoch 530, training loss: 0.024663353338837624 = 0.017824968323111534 + 0.001 * 6.838385105133057
Epoch 530, val loss: 1.214510440826416
Epoch 540, training loss: 0.023499710485339165 = 0.01665302738547325 + 0.001 * 6.846683025360107
Epoch 540, val loss: 1.228758454322815
Epoch 550, training loss: 0.022431515157222748 = 0.015589207410812378 + 0.001 * 6.842307090759277
Epoch 550, val loss: 1.2425051927566528
Epoch 560, training loss: 0.021458640694618225 = 0.014622293412685394 + 0.001 * 6.836346626281738
Epoch 560, val loss: 1.2558987140655518
Epoch 570, training loss: 0.020576337352395058 = 0.01374166551977396 + 0.001 * 6.8346710205078125
Epoch 570, val loss: 1.2688422203063965
Epoch 580, training loss: 0.019765380769968033 = 0.012937304563820362 + 0.001 * 6.828075408935547
Epoch 580, val loss: 1.2814503908157349
Epoch 590, training loss: 0.019039643928408623 = 0.01220123190432787 + 0.001 * 6.838412284851074
Epoch 590, val loss: 1.293686032295227
Epoch 600, training loss: 0.018356265500187874 = 0.011526341550052166 + 0.001 * 6.829923629760742
Epoch 600, val loss: 1.3055750131607056
Epoch 610, training loss: 0.0177288968116045 = 0.010906202718615532 + 0.001 * 6.822693347930908
Epoch 610, val loss: 1.3171495199203491
Epoch 620, training loss: 0.017155468463897705 = 0.010335310362279415 + 0.001 * 6.820157051086426
Epoch 620, val loss: 1.3283720016479492
Epoch 630, training loss: 0.01664981059730053 = 0.009808789007365704 + 0.001 * 6.8410210609436035
Epoch 630, val loss: 1.3393157720565796
Epoch 640, training loss: 0.016141243278980255 = 0.009322261437773705 + 0.001 * 6.818982124328613
Epoch 640, val loss: 1.349943995475769
Epoch 650, training loss: 0.015687700361013412 = 0.008871873840689659 + 0.001 * 6.815825939178467
Epoch 650, val loss: 1.3603073358535767
Epoch 660, training loss: 0.015270015224814415 = 0.008454116992652416 + 0.001 * 6.8158979415893555
Epoch 660, val loss: 1.3704029321670532
Epoch 670, training loss: 0.014884870499372482 = 0.008066066540777683 + 0.001 * 6.8188042640686035
Epoch 670, val loss: 1.3802703619003296
Epoch 680, training loss: 0.014522524550557137 = 0.007705052848905325 + 0.001 * 6.817471504211426
Epoch 680, val loss: 1.3898515701293945
Epoch 690, training loss: 0.014183859340846539 = 0.007368749938905239 + 0.001 * 6.8151092529296875
Epoch 690, val loss: 1.3992189168930054
Epoch 700, training loss: 0.013870837166905403 = 0.0070548877120018005 + 0.001 * 6.815949440002441
Epoch 700, val loss: 1.4083565473556519
Epoch 710, training loss: 0.013563646003603935 = 0.006761281751096249 + 0.001 * 6.802363395690918
Epoch 710, val loss: 1.4172413349151611
Epoch 720, training loss: 0.013301558792591095 = 0.00648608710616827 + 0.001 * 6.815471649169922
Epoch 720, val loss: 1.4259527921676636
Epoch 730, training loss: 0.013036343269050121 = 0.006227463483810425 + 0.001 * 6.808879375457764
Epoch 730, val loss: 1.4344937801361084
Epoch 740, training loss: 0.01279422827064991 = 0.005983646959066391 + 0.001 * 6.810580730438232
Epoch 740, val loss: 1.4427740573883057
Epoch 750, training loss: 0.012547606602311134 = 0.0057533481158316135 + 0.001 * 6.7942585945129395
Epoch 750, val loss: 1.4509345293045044
Epoch 760, training loss: 0.012337767519056797 = 0.005535487551242113 + 0.001 * 6.802279472351074
Epoch 760, val loss: 1.4589303731918335
Epoch 770, training loss: 0.012132164090871811 = 0.005329272709786892 + 0.001 * 6.802891731262207
Epoch 770, val loss: 1.4667348861694336
Epoch 780, training loss: 0.011933583766222 = 0.005133704282343388 + 0.001 * 6.799879550933838
Epoch 780, val loss: 1.474446415901184
Epoch 790, training loss: 0.011745002120733261 = 0.0049481261521577835 + 0.001 * 6.796875476837158
Epoch 790, val loss: 1.4820175170898438
Epoch 800, training loss: 0.011577310040593147 = 0.00477248802781105 + 0.001 * 6.804821968078613
Epoch 800, val loss: 1.489441156387329
Epoch 810, training loss: 0.011393203400075436 = 0.004606056027114391 + 0.001 * 6.787147045135498
Epoch 810, val loss: 1.4966861009597778
Epoch 820, training loss: 0.011239798739552498 = 0.004448053892701864 + 0.001 * 6.791745185852051
Epoch 820, val loss: 1.5038621425628662
Epoch 830, training loss: 0.011082280427217484 = 0.004298021551221609 + 0.001 * 6.784257888793945
Epoch 830, val loss: 1.5108839273452759
Epoch 840, training loss: 0.0109472144395113 = 0.004155330825597048 + 0.001 * 6.79188346862793
Epoch 840, val loss: 1.5178308486938477
Epoch 850, training loss: 0.010796554386615753 = 0.004019643645733595 + 0.001 * 6.776910305023193
Epoch 850, val loss: 1.5245989561080933
Epoch 860, training loss: 0.010668152943253517 = 0.0038905292749404907 + 0.001 * 6.777623653411865
Epoch 860, val loss: 1.5313541889190674
Epoch 870, training loss: 0.010552393272519112 = 0.003767278976738453 + 0.001 * 6.78511381149292
Epoch 870, val loss: 1.5380449295043945
Epoch 880, training loss: 0.010424070991575718 = 0.003649384481832385 + 0.001 * 6.774685859680176
Epoch 880, val loss: 1.5446598529815674
Epoch 890, training loss: 0.010315491817891598 = 0.0035362665075808764 + 0.001 * 6.7792253494262695
Epoch 890, val loss: 1.5512521266937256
Epoch 900, training loss: 0.010212017223238945 = 0.0034276721999049187 + 0.001 * 6.78434419631958
Epoch 900, val loss: 1.5578218698501587
Epoch 910, training loss: 0.010095784440636635 = 0.003323437413200736 + 0.001 * 6.772346496582031
Epoch 910, val loss: 1.5643603801727295
Epoch 920, training loss: 0.009999162517488003 = 0.0032232345547527075 + 0.001 * 6.775928020477295
Epoch 920, val loss: 1.5708712339401245
Epoch 930, training loss: 0.009900419972836971 = 0.003126966068521142 + 0.001 * 6.773453712463379
Epoch 930, val loss: 1.577343463897705
Epoch 940, training loss: 0.009833301417529583 = 0.0030343979597091675 + 0.001 * 6.798902988433838
Epoch 940, val loss: 1.5838028192520142
Epoch 950, training loss: 0.009718164801597595 = 0.002945485757663846 + 0.001 * 6.772678375244141
Epoch 950, val loss: 1.5901871919631958
Epoch 960, training loss: 0.00961851142346859 = 0.00285992375575006 + 0.001 * 6.75858736038208
Epoch 960, val loss: 1.596575379371643
Epoch 970, training loss: 0.009553184732794762 = 0.0027775901835411787 + 0.001 * 6.775594234466553
Epoch 970, val loss: 1.6029075384140015
Epoch 980, training loss: 0.009458466432988644 = 0.0026983434800058603 + 0.001 * 6.760122299194336
Epoch 980, val loss: 1.6092427968978882
Epoch 990, training loss: 0.009380238130688667 = 0.0026220164727419615 + 0.001 * 6.758221626281738
Epoch 990, val loss: 1.6155295372009277
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.8524
Flip ASR: 0.8267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9436577558517456 = 1.935283899307251 + 0.001 * 8.373872756958008
Epoch 0, val loss: 1.9267089366912842
Epoch 10, training loss: 1.9339834451675415 = 1.9256095886230469 + 0.001 * 8.373824119567871
Epoch 10, val loss: 1.9177818298339844
Epoch 20, training loss: 1.9225441217422485 = 1.914170503616333 + 0.001 * 8.373645782470703
Epoch 20, val loss: 1.9070366621017456
Epoch 30, training loss: 1.9069563150405884 = 1.8985830545425415 + 0.001 * 8.373276710510254
Epoch 30, val loss: 1.8921993970870972
Epoch 40, training loss: 1.8843754529953003 = 1.8760029077529907 + 0.001 * 8.372541427612305
Epoch 40, val loss: 1.8707058429718018
Epoch 50, training loss: 1.8523365259170532 = 1.8439656496047974 + 0.001 * 8.3709077835083
Epoch 50, val loss: 1.8411377668380737
Epoch 60, training loss: 1.8124555349349976 = 1.8040894269943237 + 0.001 * 8.366131782531738
Epoch 60, val loss: 1.8072459697723389
Epoch 70, training loss: 1.7688593864440918 = 1.7605137825012207 + 0.001 * 8.345545768737793
Epoch 70, val loss: 1.7722394466400146
Epoch 80, training loss: 1.7128320932388306 = 1.704633116722107 + 0.001 * 8.198987007141113
Epoch 80, val loss: 1.7236839532852173
Epoch 90, training loss: 1.6345880031585693 = 1.6267952919006348 + 0.001 * 7.792660236358643
Epoch 90, val loss: 1.6554030179977417
Epoch 100, training loss: 1.534202218055725 = 1.5265289545059204 + 0.001 * 7.6732378005981445
Epoch 100, val loss: 1.5711392164230347
Epoch 110, training loss: 1.4212042093276978 = 1.4137001037597656 + 0.001 * 7.50407600402832
Epoch 110, val loss: 1.4785583019256592
Epoch 120, training loss: 1.306638240814209 = 1.2992229461669922 + 0.001 * 7.4152512550354
Epoch 120, val loss: 1.3881546258926392
Epoch 130, training loss: 1.199351191520691 = 1.1919516324996948 + 0.001 * 7.399537563323975
Epoch 130, val loss: 1.3071335554122925
Epoch 140, training loss: 1.1054072380065918 = 1.0980724096298218 + 0.001 * 7.334819316864014
Epoch 140, val loss: 1.23946213722229
Epoch 150, training loss: 1.0266607999801636 = 1.019425392150879 + 0.001 * 7.235432147979736
Epoch 150, val loss: 1.1863337755203247
Epoch 160, training loss: 0.9601728320121765 = 0.953037679195404 + 0.001 * 7.135168552398682
Epoch 160, val loss: 1.1437703371047974
Epoch 170, training loss: 0.9003515839576721 = 0.8932672739028931 + 0.001 * 7.084295749664307
Epoch 170, val loss: 1.1061571836471558
Epoch 180, training loss: 0.8409844636917114 = 0.8339376449584961 + 0.001 * 7.046807289123535
Epoch 180, val loss: 1.067708134651184
Epoch 190, training loss: 0.77745521068573 = 0.7704505324363708 + 0.001 * 7.00468111038208
Epoch 190, val loss: 1.0243358612060547
Epoch 200, training loss: 0.708781361579895 = 0.7017982602119446 + 0.001 * 6.983096122741699
Epoch 200, val loss: 0.9766505360603333
Epoch 210, training loss: 0.6379720568656921 = 0.6309968829154968 + 0.001 * 6.975180625915527
Epoch 210, val loss: 0.9295848608016968
Epoch 220, training loss: 0.5692175626754761 = 0.5622455477714539 + 0.001 * 6.972028732299805
Epoch 220, val loss: 0.8881325125694275
Epoch 230, training loss: 0.5057719945907593 = 0.49880272150039673 + 0.001 * 6.969270706176758
Epoch 230, val loss: 0.8552411794662476
Epoch 240, training loss: 0.4486430883407593 = 0.4416765868663788 + 0.001 * 6.966516017913818
Epoch 240, val loss: 0.8312712907791138
Epoch 250, training loss: 0.39730384945869446 = 0.390340119600296 + 0.001 * 6.9637346267700195
Epoch 250, val loss: 0.8146619200706482
Epoch 260, training loss: 0.3508932888507843 = 0.3439326584339142 + 0.001 * 6.960617542266846
Epoch 260, val loss: 0.8035638928413391
Epoch 270, training loss: 0.30907243490219116 = 0.30211496353149414 + 0.001 * 6.957480430603027
Epoch 270, val loss: 0.797005295753479
Epoch 280, training loss: 0.27181270718574524 = 0.2648586928844452 + 0.001 * 6.9540114402771
Epoch 280, val loss: 0.7946031093597412
Epoch 290, training loss: 0.2390281856060028 = 0.23207852244377136 + 0.001 * 6.949667453765869
Epoch 290, val loss: 0.7959646582603455
Epoch 300, training loss: 0.21043646335601807 = 0.20348958671092987 + 0.001 * 6.9468770027160645
Epoch 300, val loss: 0.800214409828186
Epoch 310, training loss: 0.18560656905174255 = 0.1786651611328125 + 0.001 * 6.94140625
Epoch 310, val loss: 0.8068349361419678
Epoch 320, training loss: 0.1640700250864029 = 0.15713538229465485 + 0.001 * 6.934648036956787
Epoch 320, val loss: 0.8152327537536621
Epoch 330, training loss: 0.14539329707622528 = 0.13846474885940552 + 0.001 * 6.928550720214844
Epoch 330, val loss: 0.8247972726821899
Epoch 340, training loss: 0.12919525802135468 = 0.12227193266153336 + 0.001 * 6.923322677612305
Epoch 340, val loss: 0.8352633714675903
Epoch 350, training loss: 0.11514768749475479 = 0.10823214799165726 + 0.001 * 6.915535926818848
Epoch 350, val loss: 0.8463148474693298
Epoch 360, training loss: 0.10297206789255142 = 0.09606428444385529 + 0.001 * 6.907785415649414
Epoch 360, val loss: 0.8578647971153259
Epoch 370, training loss: 0.09241068363189697 = 0.08550840616226196 + 0.001 * 6.902276039123535
Epoch 370, val loss: 0.8696771860122681
Epoch 380, training loss: 0.08323948085308075 = 0.07634151726961136 + 0.001 * 6.897960662841797
Epoch 380, val loss: 0.8816184997558594
Epoch 390, training loss: 0.07525701820850372 = 0.06837014108896255 + 0.001 * 6.88687801361084
Epoch 390, val loss: 0.8936419486999512
Epoch 400, training loss: 0.0683070719242096 = 0.06142249330878258 + 0.001 * 6.884579658508301
Epoch 400, val loss: 0.9056053757667542
Epoch 410, training loss: 0.062232330441474915 = 0.05534888058900833 + 0.001 * 6.883447647094727
Epoch 410, val loss: 0.9174395203590393
Epoch 420, training loss: 0.0569034069776535 = 0.05002492666244507 + 0.001 * 6.878478527069092
Epoch 420, val loss: 0.929176390171051
Epoch 430, training loss: 0.05222273990511894 = 0.045346181839704514 + 0.001 * 6.87655782699585
Epoch 430, val loss: 0.940701961517334
Epoch 440, training loss: 0.04810721054673195 = 0.04122289642691612 + 0.001 * 6.884313106536865
Epoch 440, val loss: 0.9521523714065552
Epoch 450, training loss: 0.04445664584636688 = 0.037579238414764404 + 0.001 * 6.877405166625977
Epoch 450, val loss: 0.9634442329406738
Epoch 460, training loss: 0.041221633553504944 = 0.03435136377811432 + 0.001 * 6.870269775390625
Epoch 460, val loss: 0.9746001958847046
Epoch 470, training loss: 0.03835175186395645 = 0.03148406744003296 + 0.001 * 6.867684364318848
Epoch 470, val loss: 0.9856619238853455
Epoch 480, training loss: 0.035812996327877045 = 0.02893095277249813 + 0.001 * 6.882044315338135
Epoch 480, val loss: 0.996607780456543
Epoch 490, training loss: 0.03351675719022751 = 0.026652462780475616 + 0.001 * 6.86429500579834
Epoch 490, val loss: 1.0074466466903687
Epoch 500, training loss: 0.03147584944963455 = 0.02461429126560688 + 0.001 * 6.861557960510254
Epoch 500, val loss: 1.0181379318237305
Epoch 510, training loss: 0.029648300260305405 = 0.022786974906921387 + 0.001 * 6.861325263977051
Epoch 510, val loss: 1.0287452936172485
Epoch 520, training loss: 0.02800295129418373 = 0.021145127713680267 + 0.001 * 6.857823848724365
Epoch 520, val loss: 1.0392160415649414
Epoch 530, training loss: 0.02651803568005562 = 0.019666709005832672 + 0.001 * 6.851325511932373
Epoch 530, val loss: 1.0495507717132568
Epoch 540, training loss: 0.025181584060192108 = 0.01833225041627884 + 0.001 * 6.849332332611084
Epoch 540, val loss: 1.0597056150436401
Epoch 550, training loss: 0.023967931047081947 = 0.017125073820352554 + 0.001 * 6.8428568840026855
Epoch 550, val loss: 1.069723129272461
Epoch 560, training loss: 0.022871864959597588 = 0.01603059098124504 + 0.001 * 6.841273307800293
Epoch 560, val loss: 1.0795633792877197
Epoch 570, training loss: 0.021878600120544434 = 0.0150361442938447 + 0.001 * 6.8424553871154785
Epoch 570, val loss: 1.089213252067566
Epoch 580, training loss: 0.020960282534360886 = 0.01413058489561081 + 0.001 * 6.829697132110596
Epoch 580, val loss: 1.098694920539856
Epoch 590, training loss: 0.020142775028944016 = 0.013304129242897034 + 0.001 * 6.838646411895752
Epoch 590, val loss: 1.1080225706100464
Epoch 600, training loss: 0.019376540556550026 = 0.012548317201435566 + 0.001 * 6.82822322845459
Epoch 600, val loss: 1.1171449422836304
Epoch 610, training loss: 0.01868727058172226 = 0.011855712160468102 + 0.001 * 6.831558704376221
Epoch 610, val loss: 1.1261048316955566
Epoch 620, training loss: 0.01804429292678833 = 0.011219657026231289 + 0.001 * 6.824635028839111
Epoch 620, val loss: 1.1348726749420166
Epoch 630, training loss: 0.017452098429203033 = 0.010634412989020348 + 0.001 * 6.817685127258301
Epoch 630, val loss: 1.1435264348983765
Epoch 640, training loss: 0.01692574843764305 = 0.010094808414578438 + 0.001 * 6.830938816070557
Epoch 640, val loss: 1.1519850492477417
Epoch 650, training loss: 0.016413984820246696 = 0.009596460498869419 + 0.001 * 6.81752347946167
Epoch 650, val loss: 1.160267949104309
Epoch 660, training loss: 0.015951331704854965 = 0.00913531705737114 + 0.001 * 6.816014766693115
Epoch 660, val loss: 1.1684308052062988
Epoch 670, training loss: 0.015520749613642693 = 0.00870786514133215 + 0.001 * 6.8128838539123535
Epoch 670, val loss: 1.1764144897460938
Epoch 680, training loss: 0.01511799730360508 = 0.008310982026159763 + 0.001 * 6.807015419006348
Epoch 680, val loss: 1.1842409372329712
Epoch 690, training loss: 0.014751819893717766 = 0.007941843941807747 + 0.001 * 6.8099751472473145
Epoch 690, val loss: 1.191935420036316
Epoch 700, training loss: 0.014414590783417225 = 0.007597996853291988 + 0.001 * 6.816593647003174
Epoch 700, val loss: 1.1994574069976807
Epoch 710, training loss: 0.014091785997152328 = 0.007277223281562328 + 0.001 * 6.8145623207092285
Epoch 710, val loss: 1.206842064857483
Epoch 720, training loss: 0.01378687098622322 = 0.006977543234825134 + 0.001 * 6.809327125549316
Epoch 720, val loss: 1.2140735387802124
Epoch 730, training loss: 0.013501346111297607 = 0.006697188131511211 + 0.001 * 6.8041582107543945
Epoch 730, val loss: 1.2211711406707764
Epoch 740, training loss: 0.013233471661806107 = 0.006434633396565914 + 0.001 * 6.798838138580322
Epoch 740, val loss: 1.2281408309936523
Epoch 750, training loss: 0.012985234148800373 = 0.006188330240547657 + 0.001 * 6.796903610229492
Epoch 750, val loss: 1.2349889278411865
Epoch 760, training loss: 0.01275283470749855 = 0.00595694687217474 + 0.001 * 6.795887470245361
Epoch 760, val loss: 1.2416877746582031
Epoch 770, training loss: 0.012555857188999653 = 0.005739368498325348 + 0.001 * 6.816488265991211
Epoch 770, val loss: 1.2482563257217407
Epoch 780, training loss: 0.01232932135462761 = 0.0055344863794744015 + 0.001 * 6.794834136962891
Epoch 780, val loss: 1.254717230796814
Epoch 790, training loss: 0.012133495882153511 = 0.005341347306966782 + 0.001 * 6.792148590087891
Epoch 790, val loss: 1.26105797290802
Epoch 800, training loss: 0.011970373801887035 = 0.005159062799066305 + 0.001 * 6.811310768127441
Epoch 800, val loss: 1.2673007249832153
Epoch 810, training loss: 0.011785310693085194 = 0.004986858461052179 + 0.001 * 6.798451900482178
Epoch 810, val loss: 1.2733758687973022
Epoch 820, training loss: 0.011613215319812298 = 0.004824015777558088 + 0.001 * 6.789199352264404
Epoch 820, val loss: 1.2793772220611572
Epoch 830, training loss: 0.011476358398795128 = 0.004669852089136839 + 0.001 * 6.806506156921387
Epoch 830, val loss: 1.2852689027786255
Epoch 840, training loss: 0.01131138950586319 = 0.004523744806647301 + 0.001 * 6.787644386291504
Epoch 840, val loss: 1.2910513877868652
Epoch 850, training loss: 0.01117889117449522 = 0.004385174717754126 + 0.001 * 6.793715953826904
Epoch 850, val loss: 1.2967363595962524
Epoch 860, training loss: 0.01103642862290144 = 0.004253610968589783 + 0.001 * 6.782817363739014
Epoch 860, val loss: 1.302320122718811
Epoch 870, training loss: 0.010947938077151775 = 0.004128608852624893 + 0.001 * 6.819328784942627
Epoch 870, val loss: 1.3078175783157349
Epoch 880, training loss: 0.010795023292303085 = 0.004009717609733343 + 0.001 * 6.785305023193359
Epoch 880, val loss: 1.3131897449493408
Epoch 890, training loss: 0.010682933032512665 = 0.0038965686690062284 + 0.001 * 6.78636360168457
Epoch 890, val loss: 1.3184850215911865
Epoch 900, training loss: 0.010579445399343967 = 0.00378878484480083 + 0.001 * 6.790660381317139
Epoch 900, val loss: 1.3237006664276123
Epoch 910, training loss: 0.010462496429681778 = 0.003686035517603159 + 0.001 * 6.776461124420166
Epoch 910, val loss: 1.3288108110427856
Epoch 920, training loss: 0.01036614179611206 = 0.0035880152136087418 + 0.001 * 6.778125762939453
Epoch 920, val loss: 1.3338552713394165
Epoch 930, training loss: 0.010270778089761734 = 0.003494441043585539 + 0.001 * 6.776336669921875
Epoch 930, val loss: 1.3387895822525024
Epoch 940, training loss: 0.010190864093601704 = 0.003405042225494981 + 0.001 * 6.785821437835693
Epoch 940, val loss: 1.343656301498413
Epoch 950, training loss: 0.01009815838187933 = 0.00331958313472569 + 0.001 * 6.7785749435424805
Epoch 950, val loss: 1.3484326601028442
Epoch 960, training loss: 0.010006421245634556 = 0.0032378307078033686 + 0.001 * 6.768589973449707
Epoch 960, val loss: 1.3531383275985718
Epoch 970, training loss: 0.009949947707355022 = 0.0031595868058502674 + 0.001 * 6.790360450744629
Epoch 970, val loss: 1.3577879667282104
Epoch 980, training loss: 0.009855244308710098 = 0.003084663301706314 + 0.001 * 6.770581245422363
Epoch 980, val loss: 1.362377405166626
Epoch 990, training loss: 0.009793579578399658 = 0.0030128448270261288 + 0.001 * 6.780734062194824
Epoch 990, val loss: 1.3668662309646606
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.5904
Flip ASR: 0.5156/225 nodes
The final ASR:0.70480, 0.10951, Accuracy:0.79753, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10588])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97909, 0.00920, Accuracy:0.83704, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9338154792785645 = 1.9254416227340698 + 0.001 * 8.3739013671875
Epoch 0, val loss: 1.9219350814819336
Epoch 10, training loss: 1.9249895811080933 = 1.9166157245635986 + 0.001 * 8.373851776123047
Epoch 10, val loss: 1.9135550260543823
Epoch 20, training loss: 1.9141807556152344 = 1.9058071374893188 + 0.001 * 8.373665809631348
Epoch 20, val loss: 1.9029290676116943
Epoch 30, training loss: 1.8988703489303589 = 1.890497088432312 + 0.001 * 8.373236656188965
Epoch 30, val loss: 1.887624979019165
Epoch 40, training loss: 1.8761088848114014 = 1.867736577987671 + 0.001 * 8.372264862060547
Epoch 40, val loss: 1.8651725053787231
Epoch 50, training loss: 1.8436070680618286 = 1.8352372646331787 + 0.001 * 8.369771957397461
Epoch 50, val loss: 1.8345420360565186
Epoch 60, training loss: 1.8041144609451294 = 1.7957525253295898 + 0.001 * 8.36189079284668
Epoch 60, val loss: 1.8009427785873413
Epoch 70, training loss: 1.763939380645752 = 1.7556111812591553 + 0.001 * 8.328238487243652
Epoch 70, val loss: 1.7696443796157837
Epoch 80, training loss: 1.712780237197876 = 1.7046846151351929 + 0.001 * 8.095674514770508
Epoch 80, val loss: 1.7262924909591675
Epoch 90, training loss: 1.6416183710098267 = 1.6338504552841187 + 0.001 * 7.767913341522217
Epoch 90, val loss: 1.6645954847335815
Epoch 100, training loss: 1.5491405725479126 = 1.541481375694275 + 0.001 * 7.659185409545898
Epoch 100, val loss: 1.5868377685546875
Epoch 110, training loss: 1.4412387609481812 = 1.433753252029419 + 0.001 * 7.485471725463867
Epoch 110, val loss: 1.4991950988769531
Epoch 120, training loss: 1.3253703117370605 = 1.3180633783340454 + 0.001 * 7.306899070739746
Epoch 120, val loss: 1.4055408239364624
Epoch 130, training loss: 1.2061283588409424 = 1.1988812685012817 + 0.001 * 7.247082233428955
Epoch 130, val loss: 1.3103607892990112
Epoch 140, training loss: 1.0877717733383179 = 1.0805481672286987 + 0.001 * 7.223598003387451
Epoch 140, val loss: 1.2177834510803223
Epoch 150, training loss: 0.9756407737731934 = 0.9684426784515381 + 0.001 * 7.198070049285889
Epoch 150, val loss: 1.131193995475769
Epoch 160, training loss: 0.8737273812294006 = 0.8665533661842346 + 0.001 * 7.1740336418151855
Epoch 160, val loss: 1.0533803701400757
Epoch 170, training loss: 0.7829207181930542 = 0.7757754325866699 + 0.001 * 7.145268440246582
Epoch 170, val loss: 0.9856067895889282
Epoch 180, training loss: 0.7018504738807678 = 0.694745659828186 + 0.001 * 7.104811668395996
Epoch 180, val loss: 0.9267822504043579
Epoch 190, training loss: 0.6285350322723389 = 0.6214797496795654 + 0.001 * 7.055264472961426
Epoch 190, val loss: 0.8755161166191101
Epoch 200, training loss: 0.5616397857666016 = 0.5546115040779114 + 0.001 * 7.028253078460693
Epoch 200, val loss: 0.8307677507400513
Epoch 210, training loss: 0.5004886984825134 = 0.49347829818725586 + 0.001 * 7.0104265213012695
Epoch 210, val loss: 0.7918796539306641
Epoch 220, training loss: 0.444497674703598 = 0.43750080466270447 + 0.001 * 6.996865272521973
Epoch 220, val loss: 0.758464515209198
Epoch 230, training loss: 0.39291560649871826 = 0.3859293460845947 + 0.001 * 6.986265182495117
Epoch 230, val loss: 0.7303003072738647
Epoch 240, training loss: 0.34503617882728577 = 0.3380614221096039 + 0.001 * 6.974766731262207
Epoch 240, val loss: 0.7070051431655884
Epoch 250, training loss: 0.30074501037597656 = 0.2937786281108856 + 0.001 * 6.966390132904053
Epoch 250, val loss: 0.6887050867080688
Epoch 260, training loss: 0.2604728043079376 = 0.2535126507282257 + 0.001 * 6.960143089294434
Epoch 260, val loss: 0.6752759218215942
Epoch 270, training loss: 0.2248269021511078 = 0.21787038445472717 + 0.001 * 6.956514835357666
Epoch 270, val loss: 0.6664384007453918
Epoch 280, training loss: 0.19413019716739655 = 0.18717573583126068 + 0.001 * 6.954466819763184
Epoch 280, val loss: 0.6615906953811646
Epoch 290, training loss: 0.16822879016399384 = 0.16127429902553558 + 0.001 * 6.954483985900879
Epoch 290, val loss: 0.6601349711418152
Epoch 300, training loss: 0.1465597152709961 = 0.1396051049232483 + 0.001 * 6.9546027183532715
Epoch 300, val loss: 0.661440908908844
Epoch 310, training loss: 0.128402978181839 = 0.12144845724105835 + 0.001 * 6.954516410827637
Epoch 310, val loss: 0.6649221777915955
Epoch 320, training loss: 0.11311077326536179 = 0.1061561331152916 + 0.001 * 6.954643249511719
Epoch 320, val loss: 0.6702226400375366
Epoch 330, training loss: 0.10015390068292618 = 0.09319933503866196 + 0.001 * 6.954566955566406
Epoch 330, val loss: 0.6769458055496216
Epoch 340, training loss: 0.0891309455037117 = 0.08217426389455795 + 0.001 * 6.956683158874512
Epoch 340, val loss: 0.6848649978637695
Epoch 350, training loss: 0.07971552759408951 = 0.07275919616222382 + 0.001 * 6.956332683563232
Epoch 350, val loss: 0.6937819719314575
Epoch 360, training loss: 0.07165179401636124 = 0.06469583511352539 + 0.001 * 6.955956935882568
Epoch 360, val loss: 0.7035210728645325
Epoch 370, training loss: 0.06472577899694443 = 0.057769451290369034 + 0.001 * 6.956326484680176
Epoch 370, val loss: 0.7138726711273193
Epoch 380, training loss: 0.05875696241855621 = 0.05180053412914276 + 0.001 * 6.956427097320557
Epoch 380, val loss: 0.7246369123458862
Epoch 390, training loss: 0.0535997599363327 = 0.04663959518074989 + 0.001 * 6.96016263961792
Epoch 390, val loss: 0.7356878519058228
Epoch 400, training loss: 0.049117088317871094 = 0.0421595573425293 + 0.001 * 6.9575324058532715
Epoch 400, val loss: 0.7469427585601807
Epoch 410, training loss: 0.045213811099529266 = 0.038256216794252396 + 0.001 * 6.957592487335205
Epoch 410, val loss: 0.7582043409347534
Epoch 420, training loss: 0.04179889336228371 = 0.03484130650758743 + 0.001 * 6.957585334777832
Epoch 420, val loss: 0.7695095539093018
Epoch 430, training loss: 0.03879910707473755 = 0.03184177726507187 + 0.001 * 6.957330226898193
Epoch 430, val loss: 0.7807353138923645
Epoch 440, training loss: 0.03615381568670273 = 0.029196789488196373 + 0.001 * 6.957025527954102
Epoch 440, val loss: 0.7918022274971008
Epoch 450, training loss: 0.03381267189979553 = 0.026855770498514175 + 0.001 * 6.956900596618652
Epoch 450, val loss: 0.8027240633964539
Epoch 460, training loss: 0.03174155205488205 = 0.02477603778243065 + 0.001 * 6.965512275695801
Epoch 460, val loss: 0.8134408593177795
Epoch 470, training loss: 0.029880918562412262 = 0.022921985015273094 + 0.001 * 6.9589338302612305
Epoch 470, val loss: 0.8239633440971375
Epoch 480, training loss: 0.02821989171206951 = 0.02126321755349636 + 0.001 * 6.956674098968506
Epoch 480, val loss: 0.8343147039413452
Epoch 490, training loss: 0.026730883866548538 = 0.01977481134235859 + 0.001 * 6.956071376800537
Epoch 490, val loss: 0.8444426655769348
Epoch 500, training loss: 0.025390082970261574 = 0.018434815108776093 + 0.001 * 6.955266952514648
Epoch 500, val loss: 0.8543087244033813
Epoch 510, training loss: 0.024182621389627457 = 0.017224878072738647 + 0.001 * 6.957744121551514
Epoch 510, val loss: 0.8639919757843018
Epoch 520, training loss: 0.023083440959453583 = 0.016129283234477043 + 0.001 * 6.954156875610352
Epoch 520, val loss: 0.8733943104743958
Epoch 530, training loss: 0.02208855003118515 = 0.015134451910853386 + 0.001 * 6.954098701477051
Epoch 530, val loss: 0.8826236128807068
Epoch 540, training loss: 0.02118483930826187 = 0.01422877050936222 + 0.001 * 6.956069469451904
Epoch 540, val loss: 0.891619861125946
Epoch 550, training loss: 0.020356204360723495 = 0.013402296230196953 + 0.001 * 6.953907012939453
Epoch 550, val loss: 0.900435209274292
Epoch 560, training loss: 0.019598737359046936 = 0.01264628954231739 + 0.001 * 6.952446937561035
Epoch 560, val loss: 0.9090381264686584
Epoch 570, training loss: 0.018903639167547226 = 0.011953101493418217 + 0.001 * 6.950536727905273
Epoch 570, val loss: 0.9174245595932007
Epoch 580, training loss: 0.01826649345457554 = 0.011315635405480862 + 0.001 * 6.950857162475586
Epoch 580, val loss: 0.9256432056427002
Epoch 590, training loss: 0.01767604425549507 = 0.010727258399128914 + 0.001 * 6.948784828186035
Epoch 590, val loss: 0.9336774945259094
Epoch 600, training loss: 0.017132233828306198 = 0.010181287303566933 + 0.001 * 6.950946807861328
Epoch 600, val loss: 0.9416139125823975
Epoch 610, training loss: 0.016622841358184814 = 0.009671943262219429 + 0.001 * 6.950896739959717
Epoch 610, val loss: 0.949478030204773
Epoch 620, training loss: 0.01614481583237648 = 0.009195235557854176 + 0.001 * 6.949580192565918
Epoch 620, val loss: 0.9572737216949463
Epoch 630, training loss: 0.01569523848593235 = 0.0087485546246171 + 0.001 * 6.946683883666992
Epoch 630, val loss: 0.9650020003318787
Epoch 640, training loss: 0.015275195240974426 = 0.008330118842422962 + 0.001 * 6.945075511932373
Epoch 640, val loss: 0.9726364612579346
Epoch 650, training loss: 0.014883337542414665 = 0.007938382215797901 + 0.001 * 6.944955348968506
Epoch 650, val loss: 0.9801865220069885
Epoch 660, training loss: 0.014522958546876907 = 0.007571792230010033 + 0.001 * 6.951166152954102
Epoch 660, val loss: 0.9876063466072083
Epoch 670, training loss: 0.014172567054629326 = 0.007228864822536707 + 0.001 * 6.943702220916748
Epoch 670, val loss: 0.9949101209640503
Epoch 680, training loss: 0.013848565518856049 = 0.006908016745001078 + 0.001 * 6.940547943115234
Epoch 680, val loss: 1.0021029710769653
Epoch 690, training loss: 0.013552877120673656 = 0.006607782561331987 + 0.001 * 6.945094108581543
Epoch 690, val loss: 1.009156584739685
Epoch 700, training loss: 0.013261525891721249 = 0.006326746661216021 + 0.001 * 6.934778690338135
Epoch 700, val loss: 1.0160784721374512
Epoch 710, training loss: 0.012998159043490887 = 0.006063451524823904 + 0.001 * 6.934707164764404
Epoch 710, val loss: 1.0228681564331055
Epoch 720, training loss: 0.01274996530264616 = 0.0058165788650512695 + 0.001 * 6.933386325836182
Epoch 720, val loss: 1.0295219421386719
Epoch 730, training loss: 0.012538060545921326 = 0.005584944970905781 + 0.001 * 6.953115940093994
Epoch 730, val loss: 1.036041259765625
Epoch 740, training loss: 0.012300049886107445 = 0.005367476027458906 + 0.001 * 6.932573318481445
Epoch 740, val loss: 1.0424308776855469
Epoch 750, training loss: 0.01209185365587473 = 0.005163040943443775 + 0.001 * 6.928812503814697
Epoch 750, val loss: 1.0486810207366943
Epoch 760, training loss: 0.01189890131354332 = 0.00497062411159277 + 0.001 * 6.928277492523193
Epoch 760, val loss: 1.0548120737075806
Epoch 770, training loss: 0.011720703914761543 = 0.004789381287992001 + 0.001 * 6.931321620941162
Epoch 770, val loss: 1.0608083009719849
Epoch 780, training loss: 0.011541835032403469 = 0.004618530627340078 + 0.001 * 6.923304080963135
Epoch 780, val loss: 1.0666838884353638
Epoch 790, training loss: 0.011391043663024902 = 0.004457262810319662 + 0.001 * 6.933781147003174
Epoch 790, val loss: 1.0724477767944336
Epoch 800, training loss: 0.01122787594795227 = 0.004304972477257252 + 0.001 * 6.922903060913086
Epoch 800, val loss: 1.0780762434005737
Epoch 810, training loss: 0.011085433885455132 = 0.004160992335528135 + 0.001 * 6.924441814422607
Epoch 810, val loss: 1.0835905075073242
Epoch 820, training loss: 0.010939929634332657 = 0.004024723079055548 + 0.001 * 6.9152069091796875
Epoch 820, val loss: 1.0890188217163086
Epoch 830, training loss: 0.01082612108439207 = 0.003895635949447751 + 0.001 * 6.930485248565674
Epoch 830, val loss: 1.0943223237991333
Epoch 840, training loss: 0.010719877667725086 = 0.003773289732635021 + 0.001 * 6.946587562561035
Epoch 840, val loss: 1.0995149612426758
Epoch 850, training loss: 0.010580174624919891 = 0.0036572515964508057 + 0.001 * 6.9229230880737305
Epoch 850, val loss: 1.1046113967895508
Epoch 860, training loss: 0.010461529716849327 = 0.003547058906406164 + 0.001 * 6.914470672607422
Epoch 860, val loss: 1.1095921993255615
Epoch 870, training loss: 0.0103883882984519 = 0.0034423398319631815 + 0.001 * 6.946047782897949
Epoch 870, val loss: 1.114504337310791
Epoch 880, training loss: 0.010264499112963676 = 0.0033427965827286243 + 0.001 * 6.921702861785889
Epoch 880, val loss: 1.1192787885665894
Epoch 890, training loss: 0.010154004208743572 = 0.003248056396842003 + 0.001 * 6.905947685241699
Epoch 890, val loss: 1.123982548713684
Epoch 900, training loss: 0.010065490379929543 = 0.0031577995978295803 + 0.001 * 6.907690525054932
Epoch 900, val loss: 1.128588318824768
Epoch 910, training loss: 0.00997777096927166 = 0.003071802668273449 + 0.001 * 6.905967712402344
Epoch 910, val loss: 1.1331077814102173
Epoch 920, training loss: 0.009889228269457817 = 0.002989811124280095 + 0.001 * 6.899416923522949
Epoch 920, val loss: 1.1375435590744019
Epoch 930, training loss: 0.00980735570192337 = 0.0029115360230207443 + 0.001 * 6.895819664001465
Epoch 930, val loss: 1.1418936252593994
Epoch 940, training loss: 0.009770563803613186 = 0.0028367217164486647 + 0.001 * 6.933842182159424
Epoch 940, val loss: 1.146173119544983
Epoch 950, training loss: 0.009663723409175873 = 0.0027652145363390446 + 0.001 * 6.8985090255737305
Epoch 950, val loss: 1.1503691673278809
Epoch 960, training loss: 0.009599396958947182 = 0.002696824260056019 + 0.001 * 6.9025726318359375
Epoch 960, val loss: 1.154483675956726
Epoch 970, training loss: 0.009518621489405632 = 0.0026314062997698784 + 0.001 * 6.887214183807373
Epoch 970, val loss: 1.1585217714309692
Epoch 980, training loss: 0.009547442197799683 = 0.0025687571614980698 + 0.001 * 6.97868537902832
Epoch 980, val loss: 1.1624833345413208
Epoch 990, training loss: 0.009398086927831173 = 0.002508746925741434 + 0.001 * 6.889339447021484
Epoch 990, val loss: 1.1663763523101807
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.5277
Flip ASR: 0.4356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9472335577011108 = 1.9388595819473267 + 0.001 * 8.373917579650879
Epoch 0, val loss: 1.9338431358337402
Epoch 10, training loss: 1.9374704360961914 = 1.9290965795516968 + 0.001 * 8.37388801574707
Epoch 10, val loss: 1.9238730669021606
Epoch 20, training loss: 1.925868034362793 = 1.917494297027588 + 0.001 * 8.37375259399414
Epoch 20, val loss: 1.9115620851516724
Epoch 30, training loss: 1.9102305173873901 = 1.9018570184707642 + 0.001 * 8.373475074768066
Epoch 30, val loss: 1.894602656364441
Epoch 40, training loss: 1.8879221677780151 = 1.879549264907837 + 0.001 * 8.372903823852539
Epoch 40, val loss: 1.8702784776687622
Epoch 50, training loss: 1.856295108795166 = 1.8479235172271729 + 0.001 * 8.371569633483887
Epoch 50, val loss: 1.8365541696548462
Epoch 60, training loss: 1.8158609867095947 = 1.8074932098388672 + 0.001 * 8.367746353149414
Epoch 60, val loss: 1.7955238819122314
Epoch 70, training loss: 1.771733283996582 = 1.7633801698684692 + 0.001 * 8.35317325592041
Epoch 70, val loss: 1.7545123100280762
Epoch 80, training loss: 1.7223927974700928 = 1.7141255140304565 + 0.001 * 8.267303466796875
Epoch 80, val loss: 1.7117506265640259
Epoch 90, training loss: 1.6565289497375488 = 1.6487082242965698 + 0.001 * 7.820730686187744
Epoch 90, val loss: 1.6552629470825195
Epoch 100, training loss: 1.569138526916504 = 1.5615674257278442 + 0.001 * 7.571102619171143
Epoch 100, val loss: 1.5794821977615356
Epoch 110, training loss: 1.4605463743209839 = 1.45321524143219 + 0.001 * 7.331101894378662
Epoch 110, val loss: 1.4874345064163208
Epoch 120, training loss: 1.338252305984497 = 1.3310198783874512 + 0.001 * 7.232417106628418
Epoch 120, val loss: 1.3866474628448486
Epoch 130, training loss: 1.2100266218185425 = 1.2028225660324097 + 0.001 * 7.204069137573242
Epoch 130, val loss: 1.2843478918075562
Epoch 140, training loss: 1.0834733247756958 = 1.0762939453125 + 0.001 * 7.179322242736816
Epoch 140, val loss: 1.1858645677566528
Epoch 150, training loss: 0.9661996364593506 = 0.9590400457382202 + 0.001 * 7.159589767456055
Epoch 150, val loss: 1.0957523584365845
Epoch 160, training loss: 0.8632474541664124 = 0.8561154007911682 + 0.001 * 7.1320295333862305
Epoch 160, val loss: 1.0172172784805298
Epoch 170, training loss: 0.7765907049179077 = 0.7694882750511169 + 0.001 * 7.102426052093506
Epoch 170, val loss: 0.9524643421173096
Epoch 180, training loss: 0.7046108245849609 = 0.6975332498550415 + 0.001 * 7.077547073364258
Epoch 180, val loss: 0.901054322719574
Epoch 190, training loss: 0.6428890228271484 = 0.6358267068862915 + 0.001 * 7.06233024597168
Epoch 190, val loss: 0.8595477938652039
Epoch 200, training loss: 0.5869166851043701 = 0.5798614025115967 + 0.001 * 7.055258750915527
Epoch 200, val loss: 0.8238500952720642
Epoch 210, training loss: 0.5334709286689758 = 0.526418924331665 + 0.001 * 7.051980495452881
Epoch 210, val loss: 0.791529655456543
Epoch 220, training loss: 0.4808829724788666 = 0.47383353114128113 + 0.001 * 7.049442768096924
Epoch 220, val loss: 0.7618612051010132
Epoch 230, training loss: 0.42881685495376587 = 0.421769917011261 + 0.001 * 7.046936988830566
Epoch 230, val loss: 0.7350258827209473
Epoch 240, training loss: 0.37795406579971313 = 0.37090930342674255 + 0.001 * 7.044766902923584
Epoch 240, val loss: 0.7116813063621521
Epoch 250, training loss: 0.32960206270217896 = 0.32255905866622925 + 0.001 * 7.042997360229492
Epoch 250, val loss: 0.6923100352287292
Epoch 260, training loss: 0.28521808981895447 = 0.2781769335269928 + 0.001 * 7.04116678237915
Epoch 260, val loss: 0.6773890852928162
Epoch 270, training loss: 0.24587009847164154 = 0.23883043229579926 + 0.001 * 7.039659023284912
Epoch 270, val loss: 0.6666820049285889
Epoch 280, training loss: 0.2119554728269577 = 0.204917311668396 + 0.001 * 7.038161754608154
Epoch 280, val loss: 0.6598776578903198
Epoch 290, training loss: 0.1833108514547348 = 0.176273912191391 + 0.001 * 7.0369415283203125
Epoch 290, val loss: 0.6568270325660706
Epoch 300, training loss: 0.15937083959579468 = 0.15233518183231354 + 0.001 * 7.035656452178955
Epoch 300, val loss: 0.6570760011672974
Epoch 310, training loss: 0.13939465582370758 = 0.13236069679260254 + 0.001 * 7.033957481384277
Epoch 310, val loss: 0.6600913405418396
Epoch 320, training loss: 0.12266378849744797 = 0.11563048511743546 + 0.001 * 7.033303737640381
Epoch 320, val loss: 0.6653574109077454
Epoch 330, training loss: 0.10857231169939041 = 0.10154382884502411 + 0.001 * 7.02848482131958
Epoch 330, val loss: 0.6724660396575928
Epoch 340, training loss: 0.09662531316280365 = 0.08959320932626724 + 0.001 * 7.032101631164551
Epoch 340, val loss: 0.6810804605484009
Epoch 350, training loss: 0.08640773594379425 = 0.07938669621944427 + 0.001 * 7.021039962768555
Epoch 350, val loss: 0.690814197063446
Epoch 360, training loss: 0.07764767855405807 = 0.07063355296850204 + 0.001 * 7.014128684997559
Epoch 360, val loss: 0.7013832926750183
Epoch 370, training loss: 0.07010666280984879 = 0.06309346854686737 + 0.001 * 7.013194561004639
Epoch 370, val loss: 0.7125971913337708
Epoch 380, training loss: 0.06357496976852417 = 0.05657707527279854 + 0.001 * 6.9978928565979
Epoch 380, val loss: 0.7240568995475769
Epoch 390, training loss: 0.057916950434446335 = 0.050928544253110886 + 0.001 * 6.988405704498291
Epoch 390, val loss: 0.7358292937278748
Epoch 400, training loss: 0.05300284922122955 = 0.046020057052373886 + 0.001 * 6.982790470123291
Epoch 400, val loss: 0.7475994825363159
Epoch 410, training loss: 0.048725470900535583 = 0.04174582660198212 + 0.001 * 6.979645729064941
Epoch 410, val loss: 0.7594555616378784
Epoch 420, training loss: 0.044970933347940445 = 0.03800318390130997 + 0.001 * 6.967747688293457
Epoch 420, val loss: 0.7710661292076111
Epoch 430, training loss: 0.041707005351781845 = 0.03471442312002182 + 0.001 * 6.992581367492676
Epoch 430, val loss: 0.7826312780380249
Epoch 440, training loss: 0.038778841495513916 = 0.031814176589250565 + 0.001 * 6.96466588973999
Epoch 440, val loss: 0.7939552664756775
Epoch 450, training loss: 0.03620424121618271 = 0.02924838662147522 + 0.001 * 6.955855846405029
Epoch 450, val loss: 0.8051022887229919
Epoch 460, training loss: 0.033920880407094955 = 0.026968343183398247 + 0.001 * 6.952537536621094
Epoch 460, val loss: 0.816040575504303
Epoch 470, training loss: 0.03191589191555977 = 0.024934839457273483 + 0.001 * 6.981053352355957
Epoch 470, val loss: 0.8267031908035278
Epoch 480, training loss: 0.03006933256983757 = 0.023114483803510666 + 0.001 * 6.9548492431640625
Epoch 480, val loss: 0.837096631526947
Epoch 490, training loss: 0.028432514518499374 = 0.021479696035385132 + 0.001 * 6.952817916870117
Epoch 490, val loss: 0.8472763895988464
Epoch 500, training loss: 0.026953255757689476 = 0.020007241517305374 + 0.001 * 6.946014404296875
Epoch 500, val loss: 0.8571984171867371
Epoch 510, training loss: 0.025616653263568878 = 0.01867707073688507 + 0.001 * 6.939581871032715
Epoch 510, val loss: 0.8669019341468811
Epoch 520, training loss: 0.024444611743092537 = 0.01747201383113861 + 0.001 * 6.972597122192383
Epoch 520, val loss: 0.8763967156410217
Epoch 530, training loss: 0.023321816697716713 = 0.016376608982682228 + 0.001 * 6.945207595825195
Epoch 530, val loss: 0.8856732249259949
Epoch 540, training loss: 0.022310253232717514 = 0.01537574827671051 + 0.001 * 6.934504985809326
Epoch 540, val loss: 0.8947870135307312
Epoch 550, training loss: 0.021391691640019417 = 0.014456888660788536 + 0.001 * 6.934803009033203
Epoch 550, val loss: 0.9037148952484131
Epoch 560, training loss: 0.020541001111268997 = 0.013610550202429295 + 0.001 * 6.930451393127441
Epoch 560, val loss: 0.9125394225120544
Epoch 570, training loss: 0.019770748913288116 = 0.012829400599002838 + 0.001 * 6.941349029541016
Epoch 570, val loss: 0.9211440682411194
Epoch 580, training loss: 0.01904819719493389 = 0.01210822444409132 + 0.001 * 6.939972877502441
Epoch 580, val loss: 0.9295869469642639
Epoch 590, training loss: 0.01836283877491951 = 0.011442502029240131 + 0.001 * 6.920337200164795
Epoch 590, val loss: 0.9378823041915894
Epoch 600, training loss: 0.01775192655622959 = 0.01082720048725605 + 0.001 * 6.924725532531738
Epoch 600, val loss: 0.9459973573684692
Epoch 610, training loss: 0.017177272588014603 = 0.010258334688842297 + 0.001 * 6.918937683105469
Epoch 610, val loss: 0.9539394974708557
Epoch 620, training loss: 0.016640061512589455 = 0.009731941856443882 + 0.001 * 6.908119201660156
Epoch 620, val loss: 0.9617092609405518
Epoch 630, training loss: 0.016153696924448013 = 0.009244590997695923 + 0.001 * 6.909106254577637
Epoch 630, val loss: 0.9693386554718018
Epoch 640, training loss: 0.015692880377173424 = 0.008793005719780922 + 0.001 * 6.899874687194824
Epoch 640, val loss: 0.9768078923225403
Epoch 650, training loss: 0.015270022675395012 = 0.008373832330107689 + 0.001 * 6.8961896896362305
Epoch 650, val loss: 0.9840607643127441
Epoch 660, training loss: 0.014900327660143375 = 0.007984469644725323 + 0.001 * 6.915857791900635
Epoch 660, val loss: 0.9912644028663635
Epoch 670, training loss: 0.014517582021653652 = 0.00762220099568367 + 0.001 * 6.89538049697876
Epoch 670, val loss: 0.9982141852378845
Epoch 680, training loss: 0.014209453016519547 = 0.007284633349627256 + 0.001 * 6.924819469451904
Epoch 680, val loss: 1.0050486326217651
Epoch 690, training loss: 0.013862963765859604 = 0.006969911977648735 + 0.001 * 6.893052101135254
Epoch 690, val loss: 1.0117361545562744
Epoch 700, training loss: 0.013572479598224163 = 0.0066758557222783566 + 0.001 * 6.896623611450195
Epoch 700, val loss: 1.0182294845581055
Epoch 710, training loss: 0.013303929939866066 = 0.006400937680155039 + 0.001 * 6.90299129486084
Epoch 710, val loss: 1.024634599685669
Epoch 720, training loss: 0.01302669569849968 = 0.006143650505691767 + 0.001 * 6.883044242858887
Epoch 720, val loss: 1.0308691263198853
Epoch 730, training loss: 0.012785075232386589 = 0.00590240815654397 + 0.001 * 6.882666110992432
Epoch 730, val loss: 1.03699791431427
Epoch 740, training loss: 0.012559301219880581 = 0.0056755091063678265 + 0.001 * 6.883791923522949
Epoch 740, val loss: 1.042972445487976
Epoch 750, training loss: 0.012336349114775658 = 0.005462036468088627 + 0.001 * 6.874312877655029
Epoch 750, val loss: 1.048862338066101
Epoch 760, training loss: 0.012135840952396393 = 0.005261173006147146 + 0.001 * 6.874668121337891
Epoch 760, val loss: 1.054627537727356
Epoch 770, training loss: 0.011936504393815994 = 0.005072053521871567 + 0.001 * 6.864450931549072
Epoch 770, val loss: 1.0602474212646484
Epoch 780, training loss: 0.011812953278422356 = 0.004893737845122814 + 0.001 * 6.919215202331543
Epoch 780, val loss: 1.0657687187194824
Epoch 790, training loss: 0.011592816561460495 = 0.004725556820631027 + 0.001 * 6.8672590255737305
Epoch 790, val loss: 1.0711487531661987
Epoch 800, training loss: 0.011451151221990585 = 0.004566639196127653 + 0.001 * 6.884511470794678
Epoch 800, val loss: 1.0764662027359009
Epoch 810, training loss: 0.011278729885816574 = 0.004416146315634251 + 0.001 * 6.862583160400391
Epoch 810, val loss: 1.0816127061843872
Epoch 820, training loss: 0.011128099635243416 = 0.0042733680456876755 + 0.001 * 6.85473108291626
Epoch 820, val loss: 1.0867325067520142
Epoch 830, training loss: 0.010984493419528008 = 0.004137678537517786 + 0.001 * 6.8468146324157715
Epoch 830, val loss: 1.0917381048202515
Epoch 840, training loss: 0.010855725035071373 = 0.004008324816823006 + 0.001 * 6.847400188446045
Epoch 840, val loss: 1.0966768264770508
Epoch 850, training loss: 0.010739431716501713 = 0.0038844721857458353 + 0.001 * 6.854959487915039
Epoch 850, val loss: 1.1015832424163818
Epoch 860, training loss: 0.010613545775413513 = 0.0037655725609511137 + 0.001 * 6.847972869873047
Epoch 860, val loss: 1.1064313650131226
Epoch 870, training loss: 0.01049818191677332 = 0.003651287639513612 + 0.001 * 6.846894264221191
Epoch 870, val loss: 1.1111130714416504
Epoch 880, training loss: 0.010385679081082344 = 0.003541319165378809 + 0.001 * 6.844359397888184
Epoch 880, val loss: 1.1157587766647339
Epoch 890, training loss: 0.01030992716550827 = 0.003435489721596241 + 0.001 * 6.874436855316162
Epoch 890, val loss: 1.1205023527145386
Epoch 900, training loss: 0.0101632010191679 = 0.00333379115909338 + 0.001 * 6.829409122467041
Epoch 900, val loss: 1.1250495910644531
Epoch 910, training loss: 0.01006312109529972 = 0.003235854674130678 + 0.001 * 6.827266693115234
Epoch 910, val loss: 1.1295487880706787
Epoch 920, training loss: 0.009976784698665142 = 0.003141667228192091 + 0.001 * 6.835117340087891
Epoch 920, val loss: 1.134045958518982
Epoch 930, training loss: 0.009886220097541809 = 0.003051169216632843 + 0.001 * 6.835050106048584
Epoch 930, val loss: 1.1384650468826294
Epoch 940, training loss: 0.009786517359316349 = 0.0029645676258951426 + 0.001 * 6.821949481964111
Epoch 940, val loss: 1.1428301334381104
Epoch 950, training loss: 0.009703435003757477 = 0.0028819143772125244 + 0.001 * 6.82151985168457
Epoch 950, val loss: 1.1470947265625
Epoch 960, training loss: 0.009636243805289268 = 0.0028027468360960484 + 0.001 * 6.833496570587158
Epoch 960, val loss: 1.1512160301208496
Epoch 970, training loss: 0.00955201406031847 = 0.0027269169222563505 + 0.001 * 6.825096607208252
Epoch 970, val loss: 1.1553887128829956
Epoch 980, training loss: 0.009557632729411125 = 0.0026542197447270155 + 0.001 * 6.903412818908691
Epoch 980, val loss: 1.1594574451446533
Epoch 990, training loss: 0.009403437376022339 = 0.00258459011092782 + 0.001 * 6.818846702575684
Epoch 990, val loss: 1.163442850112915
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.8487
Flip ASR: 0.8222/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9451682567596436 = 1.936794400215149 + 0.001 * 8.373860359191895
Epoch 0, val loss: 1.9321781396865845
Epoch 10, training loss: 1.9348304271697998 = 1.9264566898345947 + 0.001 * 8.373794555664062
Epoch 10, val loss: 1.9215953350067139
Epoch 20, training loss: 1.9219952821731567 = 1.9136216640472412 + 0.001 * 8.373576164245605
Epoch 20, val loss: 1.9082560539245605
Epoch 30, training loss: 1.9038910865783691 = 1.8955180644989014 + 0.001 * 8.373055458068848
Epoch 30, val loss: 1.8894485235214233
Epoch 40, training loss: 1.8773194551467896 = 1.8689477443695068 + 0.001 * 8.371681213378906
Epoch 40, val loss: 1.8622709512710571
Epoch 50, training loss: 1.8409907817840576 = 1.8326233625411987 + 0.001 * 8.367368698120117
Epoch 50, val loss: 1.8269789218902588
Epoch 60, training loss: 1.800434947013855 = 1.7920862436294556 + 0.001 * 8.34871768951416
Epoch 60, val loss: 1.7919634580612183
Epoch 70, training loss: 1.7598541975021362 = 1.75162935256958 + 0.001 * 8.224871635437012
Epoch 70, val loss: 1.7603051662445068
Epoch 80, training loss: 1.7053509950637817 = 1.6974902153015137 + 0.001 * 7.860806941986084
Epoch 80, val loss: 1.7163631916046143
Epoch 90, training loss: 1.631428837776184 = 1.6236859560012817 + 0.001 * 7.742923736572266
Epoch 90, val loss: 1.6557165384292603
Epoch 100, training loss: 1.5400763750076294 = 1.5324755907058716 + 0.001 * 7.600793361663818
Epoch 100, val loss: 1.5832602977752686
Epoch 110, training loss: 1.4440468549728394 = 1.436631441116333 + 0.001 * 7.415408134460449
Epoch 110, val loss: 1.508301854133606
Epoch 120, training loss: 1.3493642807006836 = 1.3419886827468872 + 0.001 * 7.375585556030273
Epoch 120, val loss: 1.4374221563339233
Epoch 130, training loss: 1.2543911933898926 = 1.2470672130584717 + 0.001 * 7.32393217086792
Epoch 130, val loss: 1.3676140308380127
Epoch 140, training loss: 1.158239483833313 = 1.1509912014007568 + 0.001 * 7.24832820892334
Epoch 140, val loss: 1.2968113422393799
Epoch 150, training loss: 1.0636582374572754 = 1.0565104484558105 + 0.001 * 7.147755146026611
Epoch 150, val loss: 1.2271554470062256
Epoch 160, training loss: 0.9742196202278137 = 0.9671378135681152 + 0.001 * 7.081797122955322
Epoch 160, val loss: 1.1614364385604858
Epoch 170, training loss: 0.8907232880592346 = 0.8836665153503418 + 0.001 * 7.056753635406494
Epoch 170, val loss: 1.0998698472976685
Epoch 180, training loss: 0.8121903538703918 = 0.8051463961601257 + 0.001 * 7.0439348220825195
Epoch 180, val loss: 1.0418949127197266
Epoch 190, training loss: 0.7384956479072571 = 0.7314574122428894 + 0.001 * 7.038212299346924
Epoch 190, val loss: 0.9883029460906982
Epoch 200, training loss: 0.6702674031257629 = 0.6632335782051086 + 0.001 * 7.033817768096924
Epoch 200, val loss: 0.9410360455513
Epoch 210, training loss: 0.6074318289756775 = 0.600401759147644 + 0.001 * 7.030062675476074
Epoch 210, val loss: 0.9012811779975891
Epoch 220, training loss: 0.5488905906677246 = 0.5418646335601807 + 0.001 * 7.0259480476379395
Epoch 220, val loss: 0.8678288459777832
Epoch 230, training loss: 0.49350929260253906 = 0.4864870309829712 + 0.001 * 7.022258758544922
Epoch 230, val loss: 0.8387613296508789
Epoch 240, training loss: 0.44113895297050476 = 0.43412116169929504 + 0.001 * 7.0178046226501465
Epoch 240, val loss: 0.8127602338790894
Epoch 250, training loss: 0.39233261346817017 = 0.3853137195110321 + 0.001 * 7.018890380859375
Epoch 250, val loss: 0.7897740006446838
Epoch 260, training loss: 0.34766384959220886 = 0.34065335988998413 + 0.001 * 7.01047945022583
Epoch 260, val loss: 0.7704403400421143
Epoch 270, training loss: 0.30756744742393494 = 0.300559937953949 + 0.001 * 7.007499694824219
Epoch 270, val loss: 0.7563561797142029
Epoch 280, training loss: 0.2717944383621216 = 0.26478859782218933 + 0.001 * 7.005846977233887
Epoch 280, val loss: 0.7476317286491394
Epoch 290, training loss: 0.23984593152999878 = 0.2328418344259262 + 0.001 * 7.004095077514648
Epoch 290, val loss: 0.7435626983642578
Epoch 300, training loss: 0.2111765742301941 = 0.20417359471321106 + 0.001 * 7.002979755401611
Epoch 300, val loss: 0.7433277368545532
Epoch 310, training loss: 0.18542972207069397 = 0.1784273386001587 + 0.001 * 7.002378463745117
Epoch 310, val loss: 0.7461836934089661
Epoch 320, training loss: 0.16246147453784943 = 0.1554594337940216 + 0.001 * 7.002046585083008
Epoch 320, val loss: 0.751505970954895
Epoch 330, training loss: 0.1422015279531479 = 0.13520020246505737 + 0.001 * 7.001319885253906
Epoch 330, val loss: 0.7588599920272827
Epoch 340, training loss: 0.12452708929777145 = 0.11752616614103317 + 0.001 * 7.000924587249756
Epoch 340, val loss: 0.7679699659347534
Epoch 350, training loss: 0.10927960276603699 = 0.10227954387664795 + 0.001 * 7.000058650970459
Epoch 350, val loss: 0.7784974575042725
Epoch 360, training loss: 0.0962769165635109 = 0.08926965296268463 + 0.001 * 7.007266521453857
Epoch 360, val loss: 0.7901758551597595
Epoch 370, training loss: 0.08523594588041306 = 0.07823769003152847 + 0.001 * 6.998257160186768
Epoch 370, val loss: 0.8027952313423157
Epoch 380, training loss: 0.07589840888977051 = 0.06890062242746353 + 0.001 * 6.9977850914001465
Epoch 380, val loss: 0.8163233399391174
Epoch 390, training loss: 0.06797695904970169 = 0.06098152697086334 + 0.001 * 6.995433807373047
Epoch 390, val loss: 0.8306162357330322
Epoch 400, training loss: 0.06122438609600067 = 0.05422748997807503 + 0.001 * 6.996893882751465
Epoch 400, val loss: 0.8456060290336609
Epoch 410, training loss: 0.0554233156144619 = 0.04843150079250336 + 0.001 * 6.99181604385376
Epoch 410, val loss: 0.8611646294593811
Epoch 420, training loss: 0.05041365325450897 = 0.04342571645975113 + 0.001 * 6.987936019897461
Epoch 420, val loss: 0.8772382140159607
Epoch 430, training loss: 0.04607219249010086 = 0.039078377187252045 + 0.001 * 6.993814945220947
Epoch 430, val loss: 0.8936086893081665
Epoch 440, training loss: 0.04226750135421753 = 0.0352870337665081 + 0.001 * 6.980467796325684
Epoch 440, val loss: 0.910119354724884
Epoch 450, training loss: 0.03894727677106857 = 0.03197074681520462 + 0.001 * 6.976528644561768
Epoch 450, val loss: 0.9265745282173157
Epoch 460, training loss: 0.03604619950056076 = 0.02906196005642414 + 0.001 * 6.9842376708984375
Epoch 460, val loss: 0.9427942633628845
Epoch 470, training loss: 0.03347330167889595 = 0.02650430239737034 + 0.001 * 6.968997955322266
Epoch 470, val loss: 0.9586647152900696
Epoch 480, training loss: 0.031216369941830635 = 0.02424972876906395 + 0.001 * 6.966640949249268
Epoch 480, val loss: 0.9741221070289612
Epoch 490, training loss: 0.029235752299427986 = 0.02225680835545063 + 0.001 * 6.978943347930908
Epoch 490, val loss: 0.9891074895858765
Epoch 500, training loss: 0.027454277500510216 = 0.02049032784998417 + 0.001 * 6.963949680328369
Epoch 500, val loss: 1.0035439729690552
Epoch 510, training loss: 0.025875069200992584 = 0.018919795751571655 + 0.001 * 6.955272674560547
Epoch 510, val loss: 1.0174821615219116
Epoch 520, training loss: 0.0244815144687891 = 0.017519155517220497 + 0.001 * 6.962358474731445
Epoch 520, val loss: 1.0309135913848877
Epoch 530, training loss: 0.023218553513288498 = 0.01626613549888134 + 0.001 * 6.952417373657227
Epoch 530, val loss: 1.0438627004623413
Epoch 540, training loss: 0.02209460735321045 = 0.015141688287258148 + 0.001 * 6.952917575836182
Epoch 540, val loss: 1.056425929069519
Epoch 550, training loss: 0.02108166553080082 = 0.014129703864455223 + 0.001 * 6.951961040496826
Epoch 550, val loss: 1.0685358047485352
Epoch 560, training loss: 0.020162308588624 = 0.013216176070272923 + 0.001 * 6.946131706237793
Epoch 560, val loss: 1.0802278518676758
Epoch 570, training loss: 0.019338484853506088 = 0.012389179319143295 + 0.001 * 6.949304103851318
Epoch 570, val loss: 1.0915310382843018
Epoch 580, training loss: 0.01857811212539673 = 0.011638522148132324 + 0.001 * 6.93958854675293
Epoch 580, val loss: 1.1024534702301025
Epoch 590, training loss: 0.0178946815431118 = 0.0109553849324584 + 0.001 * 6.939295291900635
Epoch 590, val loss: 1.1130565404891968
Epoch 600, training loss: 0.01727287471294403 = 0.010332116857171059 + 0.001 * 6.9407572746276855
Epoch 600, val loss: 1.1233338117599487
Epoch 610, training loss: 0.016690585762262344 = 0.009762043133378029 + 0.001 * 6.928541660308838
Epoch 610, val loss: 1.133294701576233
Epoch 620, training loss: 0.016183892264962196 = 0.009239399805665016 + 0.001 * 6.944492816925049
Epoch 620, val loss: 1.1429623365402222
Epoch 630, training loss: 0.015688210725784302 = 0.008759142830967903 + 0.001 * 6.929067134857178
Epoch 630, val loss: 1.1523633003234863
Epoch 640, training loss: 0.015240008011460304 = 0.00831687543541193 + 0.001 * 6.923132419586182
Epoch 640, val loss: 1.1614880561828613
Epoch 650, training loss: 0.014838453382253647 = 0.007908723317086697 + 0.001 * 6.929730415344238
Epoch 650, val loss: 1.1703540086746216
Epoch 660, training loss: 0.014464551582932472 = 0.007531375624239445 + 0.001 * 6.933176040649414
Epoch 660, val loss: 1.1789672374725342
Epoch 670, training loss: 0.014104487374424934 = 0.007181855384260416 + 0.001 * 6.92263126373291
Epoch 670, val loss: 1.1873693466186523
Epoch 680, training loss: 0.013781040906906128 = 0.0068574766628444195 + 0.001 * 6.923564434051514
Epoch 680, val loss: 1.19553804397583
Epoch 690, training loss: 0.013468114659190178 = 0.006555958651006222 + 0.001 * 6.912156105041504
Epoch 690, val loss: 1.2035040855407715
Epoch 700, training loss: 0.013199999928474426 = 0.006275240331888199 + 0.001 * 6.924759864807129
Epoch 700, val loss: 1.2112747430801392
Epoch 710, training loss: 0.012917973101139069 = 0.006013421341776848 + 0.001 * 6.904551982879639
Epoch 710, val loss: 1.218862533569336
Epoch 720, training loss: 0.012690725736320019 = 0.005768851842731237 + 0.001 * 6.921873569488525
Epoch 720, val loss: 1.2262452840805054
Epoch 730, training loss: 0.012456185184419155 = 0.005540070123970509 + 0.001 * 6.916114807128906
Epoch 730, val loss: 1.2334344387054443
Epoch 740, training loss: 0.012224464677274227 = 0.00532573601230979 + 0.001 * 6.898728370666504
Epoch 740, val loss: 1.2404602766036987
Epoch 750, training loss: 0.012026827782392502 = 0.005124674644321203 + 0.001 * 6.902153491973877
Epoch 750, val loss: 1.2473077774047852
Epoch 760, training loss: 0.011832882650196552 = 0.004935828968882561 + 0.001 * 6.897053241729736
Epoch 760, val loss: 1.253987193107605
Epoch 770, training loss: 0.011659832671284676 = 0.00475820479914546 + 0.001 * 6.901627063751221
Epoch 770, val loss: 1.2605171203613281
Epoch 780, training loss: 0.0114826038479805 = 0.004590959288179874 + 0.001 * 6.89164400100708
Epoch 780, val loss: 1.266894817352295
Epoch 790, training loss: 0.011330747045576572 = 0.004433285910636187 + 0.001 * 6.8974609375
Epoch 790, val loss: 1.2731389999389648
Epoch 800, training loss: 0.01118491217494011 = 0.004284472670406103 + 0.001 * 6.9004387855529785
Epoch 800, val loss: 1.2792255878448486
Epoch 810, training loss: 0.011035241186618805 = 0.004143878817558289 + 0.001 * 6.891362190246582
Epoch 810, val loss: 1.2851794958114624
Epoch 820, training loss: 0.010894330218434334 = 0.004010892938822508 + 0.001 * 6.883437156677246
Epoch 820, val loss: 1.2909897565841675
Epoch 830, training loss: 0.01078885793685913 = 0.003884982317686081 + 0.001 * 6.903875827789307
Epoch 830, val loss: 1.2966821193695068
Epoch 840, training loss: 0.010647919960319996 = 0.0037656507920473814 + 0.001 * 6.882268905639648
Epoch 840, val loss: 1.302238941192627
Epoch 850, training loss: 0.01052719447761774 = 0.00365246064029634 + 0.001 * 6.874733924865723
Epoch 850, val loss: 1.3076727390289307
Epoch 860, training loss: 0.010413995012640953 = 0.003544998588040471 + 0.001 * 6.8689961433410645
Epoch 860, val loss: 1.3129888772964478
Epoch 870, training loss: 0.010322023183107376 = 0.0034428753424435854 + 0.001 * 6.879147529602051
Epoch 870, val loss: 1.3181889057159424
Epoch 880, training loss: 0.010217325761914253 = 0.003345735603943467 + 0.001 * 6.871589660644531
Epoch 880, val loss: 1.3232918977737427
Epoch 890, training loss: 0.010139559395611286 = 0.003253277624025941 + 0.001 * 6.8862810134887695
Epoch 890, val loss: 1.3282774686813354
Epoch 900, training loss: 0.010044106282293797 = 0.0031651905737817287 + 0.001 * 6.878915309906006
Epoch 900, val loss: 1.33317232131958
Epoch 910, training loss: 0.009953168220818043 = 0.003081207862123847 + 0.001 * 6.871959686279297
Epoch 910, val loss: 1.3379591703414917
Epoch 920, training loss: 0.009876467287540436 = 0.0030010505579411983 + 0.001 * 6.875415802001953
Epoch 920, val loss: 1.3426412343978882
Epoch 930, training loss: 0.009788168594241142 = 0.0029245628975331783 + 0.001 * 6.863605499267578
Epoch 930, val loss: 1.3472261428833008
Epoch 940, training loss: 0.009721379727125168 = 0.002851462457329035 + 0.001 * 6.869917392730713
Epoch 940, val loss: 1.3517426252365112
Epoch 950, training loss: 0.009659474715590477 = 0.002781572053208947 + 0.001 * 6.877902030944824
Epoch 950, val loss: 1.3561447858810425
Epoch 960, training loss: 0.009577751159667969 = 0.0027146884240210056 + 0.001 * 6.863062858581543
Epoch 960, val loss: 1.3604711294174194
Epoch 970, training loss: 0.009544877335429192 = 0.00265066628344357 + 0.001 * 6.8942108154296875
Epoch 970, val loss: 1.3646996021270752
Epoch 980, training loss: 0.009439058601856232 = 0.0025893598794937134 + 0.001 * 6.849698543548584
Epoch 980, val loss: 1.3688538074493408
Epoch 990, training loss: 0.009374707005918026 = 0.002530594589188695 + 0.001 * 6.844112396240234
Epoch 990, val loss: 1.3729268312454224
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6900
Flip ASR: 0.6400/225 nodes
The final ASR:0.68881, 0.13106, Accuracy:0.81975, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10546])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9601491689682007 = 1.951775312423706 + 0.001 * 8.373845100402832
Epoch 0, val loss: 1.952616572380066
Epoch 10, training loss: 1.949819803237915 = 1.94144606590271 + 0.001 * 8.373769760131836
Epoch 10, val loss: 1.9428871870040894
Epoch 20, training loss: 1.9370719194412231 = 1.9286984205245972 + 0.001 * 8.373552322387695
Epoch 20, val loss: 1.9306340217590332
Epoch 30, training loss: 1.9188493490219116 = 1.9104762077331543 + 0.001 * 8.373082160949707
Epoch 30, val loss: 1.912750244140625
Epoch 40, training loss: 1.8913798332214355 = 1.8830078840255737 + 0.001 * 8.37197208404541
Epoch 40, val loss: 1.886008381843567
Epoch 50, training loss: 1.852591633796692 = 1.8442230224609375 + 0.001 * 8.368619918823242
Epoch 50, val loss: 1.8501065969467163
Epoch 60, training loss: 1.8088372945785522 = 1.8004826307296753 + 0.001 * 8.354618072509766
Epoch 60, val loss: 1.8145487308502197
Epoch 70, training loss: 1.7700692415237427 = 1.7617942094802856 + 0.001 * 8.27498722076416
Epoch 70, val loss: 1.7860697507858276
Epoch 80, training loss: 1.7214215993881226 = 1.7135217189788818 + 0.001 * 7.899824619293213
Epoch 80, val loss: 1.7449463605880737
Epoch 90, training loss: 1.6548259258270264 = 1.6470727920532227 + 0.001 * 7.75319242477417
Epoch 90, val loss: 1.6877002716064453
Epoch 100, training loss: 1.5679806470870972 = 1.5604180097579956 + 0.001 * 7.562661170959473
Epoch 100, val loss: 1.616366982460022
Epoch 110, training loss: 1.4691778421401978 = 1.4617547988891602 + 0.001 * 7.422993183135986
Epoch 110, val loss: 1.5387916564941406
Epoch 120, training loss: 1.368897795677185 = 1.3615070581436157 + 0.001 * 7.3907036781311035
Epoch 120, val loss: 1.4597949981689453
Epoch 130, training loss: 1.2732234001159668 = 1.2658847570419312 + 0.001 * 7.338644027709961
Epoch 130, val loss: 1.3860499858856201
Epoch 140, training loss: 1.1856619119644165 = 1.1784058809280396 + 0.001 * 7.256073474884033
Epoch 140, val loss: 1.3183642625808716
Epoch 150, training loss: 1.1072120666503906 = 1.10005521774292 + 0.001 * 7.156855583190918
Epoch 150, val loss: 1.2570220232009888
Epoch 160, training loss: 1.036117672920227 = 1.0290168523788452 + 0.001 * 7.100762367248535
Epoch 160, val loss: 1.2014389038085938
Epoch 170, training loss: 0.969306230545044 = 0.9622050523757935 + 0.001 * 7.101205348968506
Epoch 170, val loss: 1.1497548818588257
Epoch 180, training loss: 0.9038743376731873 = 0.896784782409668 + 0.001 * 7.0895538330078125
Epoch 180, val loss: 1.1003018617630005
Epoch 190, training loss: 0.8378385305404663 = 0.830755352973938 + 0.001 * 7.083156108856201
Epoch 190, val loss: 1.0518503189086914
Epoch 200, training loss: 0.7707619071006775 = 0.7636858224868774 + 0.001 * 7.076098918914795
Epoch 200, val loss: 1.0044437646865845
Epoch 210, training loss: 0.7034432888031006 = 0.6963747143745422 + 0.001 * 7.0685601234436035
Epoch 210, val loss: 0.9594029188156128
Epoch 220, training loss: 0.6370418667793274 = 0.6299849152565002 + 0.001 * 7.056957721710205
Epoch 220, val loss: 0.9180732369422913
Epoch 230, training loss: 0.5728747844696045 = 0.5658343434333801 + 0.001 * 7.040462493896484
Epoch 230, val loss: 0.8819509148597717
Epoch 240, training loss: 0.5122563242912292 = 0.5052375793457031 + 0.001 * 7.018726348876953
Epoch 240, val loss: 0.852051854133606
Epoch 250, training loss: 0.45690271258354187 = 0.4499070942401886 + 0.001 * 6.995623588562012
Epoch 250, val loss: 0.8296972513198853
Epoch 260, training loss: 0.40785273909568787 = 0.4008828103542328 + 0.001 * 6.969934940338135
Epoch 260, val loss: 0.8154371380805969
Epoch 270, training loss: 0.3650013208389282 = 0.3580462336540222 + 0.001 * 6.955075740814209
Epoch 270, val loss: 0.8085828423500061
Epoch 280, training loss: 0.32747408747673035 = 0.3205205798149109 + 0.001 * 6.953500747680664
Epoch 280, val loss: 0.8076927661895752
Epoch 290, training loss: 0.2942085862159729 = 0.28726649284362793 + 0.001 * 6.942107677459717
Epoch 290, val loss: 0.8114808797836304
Epoch 300, training loss: 0.26431339979171753 = 0.2573738098144531 + 0.001 * 6.939593315124512
Epoch 300, val loss: 0.8187165856361389
Epoch 310, training loss: 0.237074613571167 = 0.2301403433084488 + 0.001 * 6.934269905090332
Epoch 310, val loss: 0.8289575576782227
Epoch 320, training loss: 0.21208864450454712 = 0.20515753328800201 + 0.001 * 6.931117534637451
Epoch 320, val loss: 0.8418116569519043
Epoch 330, training loss: 0.18921418488025665 = 0.18228304386138916 + 0.001 * 6.931135654449463
Epoch 330, val loss: 0.8573439717292786
Epoch 340, training loss: 0.16842317581176758 = 0.16149868071079254 + 0.001 * 6.924493789672852
Epoch 340, val loss: 0.8751592636108398
Epoch 350, training loss: 0.14970387518405914 = 0.14278408885002136 + 0.001 * 6.919785499572754
Epoch 350, val loss: 0.8949272036552429
Epoch 360, training loss: 0.1330093890428543 = 0.1260950267314911 + 0.001 * 6.91436243057251
Epoch 360, val loss: 0.9163517355918884
Epoch 370, training loss: 0.11825037747621536 = 0.11133591830730438 + 0.001 * 6.9144606590271
Epoch 370, val loss: 0.9390025734901428
Epoch 380, training loss: 0.10527236759662628 = 0.09836388379335403 + 0.001 * 6.908482074737549
Epoch 380, val loss: 0.9624488949775696
Epoch 390, training loss: 0.09391369670629501 = 0.08701322227716446 + 0.001 * 6.900475025177002
Epoch 390, val loss: 0.9863386154174805
Epoch 400, training loss: 0.08400466293096542 = 0.07711290568113327 + 0.001 * 6.8917555809021
Epoch 400, val loss: 1.0102837085723877
Epoch 410, training loss: 0.07538609951734543 = 0.06848608702421188 + 0.001 * 6.900008678436279
Epoch 410, val loss: 1.0340172052383423
Epoch 420, training loss: 0.06785058230161667 = 0.06097491830587387 + 0.001 * 6.875664234161377
Epoch 420, val loss: 1.0573573112487793
Epoch 430, training loss: 0.06130990758538246 = 0.05443453788757324 + 0.001 * 6.875370025634766
Epoch 430, val loss: 1.080315351486206
Epoch 440, training loss: 0.05560963973402977 = 0.0487377792596817 + 0.001 * 6.871859550476074
Epoch 440, val loss: 1.1027085781097412
Epoch 450, training loss: 0.05065455660223961 = 0.043771661818027496 + 0.001 * 6.882894992828369
Epoch 450, val loss: 1.1245286464691162
Epoch 460, training loss: 0.04629625380039215 = 0.039438508450984955 + 0.001 * 6.857745170593262
Epoch 460, val loss: 1.1457571983337402
Epoch 470, training loss: 0.042496852576732635 = 0.03565052151679993 + 0.001 * 6.846330165863037
Epoch 470, val loss: 1.1663812398910522
Epoch 480, training loss: 0.03919640928506851 = 0.03233319893479347 + 0.001 * 6.863210678100586
Epoch 480, val loss: 1.186437726020813
Epoch 490, training loss: 0.03626507520675659 = 0.029423438012599945 + 0.001 * 6.841635227203369
Epoch 490, val loss: 1.2058500051498413
Epoch 500, training loss: 0.033706486225128174 = 0.026864463463425636 + 0.001 * 6.842023849487305
Epoch 500, val loss: 1.2246235609054565
Epoch 510, training loss: 0.03145579621195793 = 0.02460807003080845 + 0.001 * 6.847725868225098
Epoch 510, val loss: 1.2427724599838257
Epoch 520, training loss: 0.029453592374920845 = 0.022612741217017174 + 0.001 * 6.840850353240967
Epoch 520, val loss: 1.2603346109390259
Epoch 530, training loss: 0.027669452130794525 = 0.02084280364215374 + 0.001 * 6.826648712158203
Epoch 530, val loss: 1.2773184776306152
Epoch 540, training loss: 0.026135530322790146 = 0.019267931580543518 + 0.001 * 6.867597579956055
Epoch 540, val loss: 1.293731689453125
Epoch 550, training loss: 0.024682827293872833 = 0.01786232553422451 + 0.001 * 6.82050085067749
Epoch 550, val loss: 1.309564232826233
Epoch 560, training loss: 0.023426810279488564 = 0.016603821888566017 + 0.001 * 6.822988510131836
Epoch 560, val loss: 1.3248510360717773
Epoch 570, training loss: 0.02231624536216259 = 0.01547329407185316 + 0.001 * 6.842950344085693
Epoch 570, val loss: 1.3396638631820679
Epoch 580, training loss: 0.021282454952597618 = 0.014454974792897701 + 0.001 * 6.827479362487793
Epoch 580, val loss: 1.3539806604385376
Epoch 590, training loss: 0.02035784348845482 = 0.013535005040466785 + 0.001 * 6.822839260101318
Epoch 590, val loss: 1.3678652048110962
Epoch 600, training loss: 0.019527625292539597 = 0.012701630592346191 + 0.001 * 6.825994491577148
Epoch 600, val loss: 1.3812772035598755
Epoch 610, training loss: 0.01874728314578533 = 0.011944623664021492 + 0.001 * 6.802658557891846
Epoch 610, val loss: 1.394258975982666
Epoch 620, training loss: 0.0180704053491354 = 0.011255240999162197 + 0.001 * 6.815164089202881
Epoch 620, val loss: 1.4068294763565063
Epoch 630, training loss: 0.017435427755117416 = 0.01062578335404396 + 0.001 * 6.80964469909668
Epoch 630, val loss: 1.4190067052841187
Epoch 640, training loss: 0.01685984805226326 = 0.010049615986645222 + 0.001 * 6.8102312088012695
Epoch 640, val loss: 1.4308215379714966
Epoch 650, training loss: 0.016318542882800102 = 0.009520990774035454 + 0.001 * 6.797552108764648
Epoch 650, val loss: 1.4422675371170044
Epoch 660, training loss: 0.015857454389333725 = 0.009034937247633934 + 0.001 * 6.822515964508057
Epoch 660, val loss: 1.4533843994140625
Epoch 670, training loss: 0.015382848680019379 = 0.008587051182985306 + 0.001 * 6.795796871185303
Epoch 670, val loss: 1.4641425609588623
Epoch 680, training loss: 0.014966608956456184 = 0.008173448033630848 + 0.001 * 6.793160438537598
Epoch 680, val loss: 1.4745832681655884
Epoch 690, training loss: 0.014583757147192955 = 0.007790758274495602 + 0.001 * 6.792998313903809
Epoch 690, val loss: 1.4847418069839478
Epoch 700, training loss: 0.014214719645678997 = 0.007436036132276058 + 0.001 * 6.778683185577393
Epoch 700, val loss: 1.4946237802505493
Epoch 710, training loss: 0.013920772820711136 = 0.007106554228812456 + 0.001 * 6.814218044281006
Epoch 710, val loss: 1.504228115081787
Epoch 720, training loss: 0.01358308270573616 = 0.006800064817070961 + 0.001 * 6.783017158508301
Epoch 720, val loss: 1.513545274734497
Epoch 730, training loss: 0.013294578529894352 = 0.006514485459774733 + 0.001 * 6.780092716217041
Epoch 730, val loss: 1.5226324796676636
Epoch 740, training loss: 0.013036916963756084 = 0.006247932557016611 + 0.001 * 6.788984298706055
Epoch 740, val loss: 1.5314825773239136
Epoch 750, training loss: 0.012803556397557259 = 0.005998731125146151 + 0.001 * 6.8048248291015625
Epoch 750, val loss: 1.540108561515808
Epoch 760, training loss: 0.012559480033814907 = 0.005765446461737156 + 0.001 * 6.794033050537109
Epoch 760, val loss: 1.5485526323318481
Epoch 770, training loss: 0.01233680546283722 = 0.005546653177589178 + 0.001 * 6.790151596069336
Epoch 770, val loss: 1.556763768196106
Epoch 780, training loss: 0.012108370661735535 = 0.005341146141290665 + 0.001 * 6.767223834991455
Epoch 780, val loss: 1.5647870302200317
Epoch 790, training loss: 0.01191578060388565 = 0.005147760268300772 + 0.001 * 6.768019676208496
Epoch 790, val loss: 1.572599172592163
Epoch 800, training loss: 0.011748380959033966 = 0.004965481348335743 + 0.001 * 6.782899856567383
Epoch 800, val loss: 1.580248475074768
Epoch 810, training loss: 0.011560063809156418 = 0.004793275613337755 + 0.001 * 6.766787528991699
Epoch 810, val loss: 1.5877244472503662
Epoch 820, training loss: 0.011395912617444992 = 0.004630218725651503 + 0.001 * 6.765693187713623
Epoch 820, val loss: 1.5950466394424438
Epoch 830, training loss: 0.011258608661592007 = 0.004475323483347893 + 0.001 * 6.783284664154053
Epoch 830, val loss: 1.602280855178833
Epoch 840, training loss: 0.011116944253444672 = 0.004327684640884399 + 0.001 * 6.789259910583496
Epoch 840, val loss: 1.609423041343689
Epoch 850, training loss: 0.010943900793790817 = 0.004186585079878569 + 0.001 * 6.757315158843994
Epoch 850, val loss: 1.616503357887268
Epoch 860, training loss: 0.010820945724844933 = 0.004051615949720144 + 0.001 * 6.769329071044922
Epoch 860, val loss: 1.6235300302505493
Epoch 870, training loss: 0.010703698731958866 = 0.003922546748071909 + 0.001 * 6.78115177154541
Epoch 870, val loss: 1.6305749416351318
Epoch 880, training loss: 0.010570388287305832 = 0.003798925783485174 + 0.001 * 6.771462440490723
Epoch 880, val loss: 1.6375194787979126
Epoch 890, training loss: 0.01043088361620903 = 0.0036806801799684763 + 0.001 * 6.750202655792236
Epoch 890, val loss: 1.6444671154022217
Epoch 900, training loss: 0.01031891256570816 = 0.0035674627870321274 + 0.001 * 6.751449108123779
Epoch 900, val loss: 1.6513499021530151
Epoch 910, training loss: 0.010221350938081741 = 0.0034591106232255697 + 0.001 * 6.762240409851074
Epoch 910, val loss: 1.6581635475158691
Epoch 920, training loss: 0.010130155831575394 = 0.0033554385881870985 + 0.001 * 6.774717330932617
Epoch 920, val loss: 1.6649178266525269
Epoch 930, training loss: 0.010000582784414291 = 0.003256286960095167 + 0.001 * 6.744295597076416
Epoch 930, val loss: 1.6715824604034424
Epoch 940, training loss: 0.009907681494951248 = 0.0031614613253623247 + 0.001 * 6.746220111846924
Epoch 940, val loss: 1.6782054901123047
Epoch 950, training loss: 0.0098235122859478 = 0.003070769365876913 + 0.001 * 6.752742290496826
Epoch 950, val loss: 1.6847697496414185
Epoch 960, training loss: 0.00972710270434618 = 0.002984047867357731 + 0.001 * 6.743054389953613
Epoch 960, val loss: 1.6912188529968262
Epoch 970, training loss: 0.009630560874938965 = 0.0029010861180722713 + 0.001 * 6.729475021362305
Epoch 970, val loss: 1.6976228952407837
Epoch 980, training loss: 0.009566842578351498 = 0.00282176467590034 + 0.001 * 6.745077133178711
Epoch 980, val loss: 1.703953504562378
Epoch 990, training loss: 0.009500749409198761 = 0.0027457857504487038 + 0.001 * 6.754963397979736
Epoch 990, val loss: 1.7101829051971436
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.3469
Flip ASR: 0.2356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9528565406799316 = 1.9444828033447266 + 0.001 * 8.373795509338379
Epoch 0, val loss: 1.935349941253662
Epoch 10, training loss: 1.9431332349777222 = 1.934759497642517 + 0.001 * 8.37372875213623
Epoch 10, val loss: 1.9255328178405762
Epoch 20, training loss: 1.9313534498214722 = 1.9229800701141357 + 0.001 * 8.373433113098145
Epoch 20, val loss: 1.912954330444336
Epoch 30, training loss: 1.9147560596466064 = 1.9063832759857178 + 0.001 * 8.372798919677734
Epoch 30, val loss: 1.8945670127868652
Epoch 40, training loss: 1.8902242183685303 = 1.8818527460098267 + 0.001 * 8.371416091918945
Epoch 40, val loss: 1.8671936988830566
Epoch 50, training loss: 1.8549729585647583 = 1.8466053009033203 + 0.001 * 8.367671966552734
Epoch 50, val loss: 1.828959584236145
Epoch 60, training loss: 1.8106080293655396 = 1.8022547960281372 + 0.001 * 8.353280067443848
Epoch 60, val loss: 1.7846276760101318
Epoch 70, training loss: 1.765725016593933 = 1.7574610710144043 + 0.001 * 8.26396656036377
Epoch 70, val loss: 1.745593786239624
Epoch 80, training loss: 1.7179433107376099 = 1.7101435661315918 + 0.001 * 7.799747467041016
Epoch 80, val loss: 1.7062164545059204
Epoch 90, training loss: 1.6530776023864746 = 1.6454724073410034 + 0.001 * 7.605241298675537
Epoch 90, val loss: 1.650733232498169
Epoch 100, training loss: 1.5649501085281372 = 1.55753493309021 + 0.001 * 7.4151787757873535
Epoch 100, val loss: 1.5764199495315552
Epoch 110, training loss: 1.4571341276168823 = 1.4498804807662964 + 0.001 * 7.2536139488220215
Epoch 110, val loss: 1.4897698163986206
Epoch 120, training loss: 1.3435016870498657 = 1.3362786769866943 + 0.001 * 7.2229743003845215
Epoch 120, val loss: 1.4019335508346558
Epoch 130, training loss: 1.2354326248168945 = 1.2282646894454956 + 0.001 * 7.167964935302734
Epoch 130, val loss: 1.3223761320114136
Epoch 140, training loss: 1.1380902528762817 = 1.1309776306152344 + 0.001 * 7.112649917602539
Epoch 140, val loss: 1.2534096240997314
Epoch 150, training loss: 1.0520572662353516 = 1.0449938774108887 + 0.001 * 7.063388824462891
Epoch 150, val loss: 1.193459391593933
Epoch 160, training loss: 0.9758406281471252 = 0.9688083529472351 + 0.001 * 7.0322585105896
Epoch 160, val loss: 1.1405150890350342
Epoch 170, training loss: 0.9064812064170837 = 0.8994708061218262 + 0.001 * 7.010426998138428
Epoch 170, val loss: 1.0918785333633423
Epoch 180, training loss: 0.8397298455238342 = 0.8327425718307495 + 0.001 * 6.987295150756836
Epoch 180, val loss: 1.044642448425293
Epoch 190, training loss: 0.7726893424987793 = 0.7657201886177063 + 0.001 * 6.969162464141846
Epoch 190, val loss: 0.9970757365226746
Epoch 200, training loss: 0.7054284811019897 = 0.6984679102897644 + 0.001 * 6.960577487945557
Epoch 200, val loss: 0.9495776295661926
Epoch 210, training loss: 0.6403098106384277 = 0.6333516240119934 + 0.001 * 6.958208084106445
Epoch 210, val loss: 0.9040945172309875
Epoch 220, training loss: 0.5794921517372131 = 0.5725337266921997 + 0.001 * 6.958438873291016
Epoch 220, val loss: 0.8628902435302734
Epoch 230, training loss: 0.5235384702682495 = 0.5165810585021973 + 0.001 * 6.957423210144043
Epoch 230, val loss: 0.8268918991088867
Epoch 240, training loss: 0.47196146845817566 = 0.4650050103664398 + 0.001 * 6.956461429595947
Epoch 240, val loss: 0.7958425879478455
Epoch 250, training loss: 0.4239867627620697 = 0.417031466960907 + 0.001 * 6.955295085906982
Epoch 250, val loss: 0.7688478827476501
Epoch 260, training loss: 0.379021018743515 = 0.3720672130584717 + 0.001 * 6.953794479370117
Epoch 260, val loss: 0.7449290752410889
Epoch 270, training loss: 0.3368208408355713 = 0.3298676311969757 + 0.001 * 6.9532084465026855
Epoch 270, val loss: 0.7240004539489746
Epoch 280, training loss: 0.29743140935897827 = 0.2904813587665558 + 0.001 * 6.9500532150268555
Epoch 280, val loss: 0.7060713768005371
Epoch 290, training loss: 0.26108625531196594 = 0.25413796305656433 + 0.001 * 6.948303699493408
Epoch 290, val loss: 0.6915004253387451
Epoch 300, training loss: 0.2283364236354828 = 0.22139360010623932 + 0.001 * 6.942829132080078
Epoch 300, val loss: 0.6809554696083069
Epoch 310, training loss: 0.19976384937763214 = 0.19282741844654083 + 0.001 * 6.936433792114258
Epoch 310, val loss: 0.6746793389320374
Epoch 320, training loss: 0.17556880414485931 = 0.1686277538537979 + 0.001 * 6.941047668457031
Epoch 320, val loss: 0.6728110909461975
Epoch 330, training loss: 0.15527860820293427 = 0.14835453033447266 + 0.001 * 6.924075603485107
Epoch 330, val loss: 0.6750165820121765
Epoch 340, training loss: 0.13830533623695374 = 0.13139043748378754 + 0.001 * 6.914892196655273
Epoch 340, val loss: 0.680526077747345
Epoch 350, training loss: 0.12402784079313278 = 0.11707227677106857 + 0.001 * 6.955563068389893
Epoch 350, val loss: 0.688373327255249
Epoch 360, training loss: 0.11174388229846954 = 0.10483574867248535 + 0.001 * 6.908134460449219
Epoch 360, val loss: 0.6978882551193237
Epoch 370, training loss: 0.1011257991194725 = 0.09423765540122986 + 0.001 * 6.888143062591553
Epoch 370, val loss: 0.7085713148117065
Epoch 380, training loss: 0.09184617549180984 = 0.08495641499757767 + 0.001 * 6.889761447906494
Epoch 380, val loss: 0.7197901010513306
Epoch 390, training loss: 0.08371631056070328 = 0.07683657109737396 + 0.001 * 6.879742622375488
Epoch 390, val loss: 0.7314305901527405
Epoch 400, training loss: 0.0764939934015274 = 0.0696171373128891 + 0.001 * 6.876853942871094
Epoch 400, val loss: 0.7433666586875916
Epoch 410, training loss: 0.07010895758867264 = 0.06322500109672546 + 0.001 * 6.883953094482422
Epoch 410, val loss: 0.7552233934402466
Epoch 420, training loss: 0.06439477205276489 = 0.057531386613845825 + 0.001 * 6.863381862640381
Epoch 420, val loss: 0.7669176459312439
Epoch 430, training loss: 0.05929985269904137 = 0.05243735387921333 + 0.001 * 6.862498760223389
Epoch 430, val loss: 0.7785277366638184
Epoch 440, training loss: 0.054743580520153046 = 0.047873932868242264 + 0.001 * 6.8696489334106445
Epoch 440, val loss: 0.7899764180183411
Epoch 450, training loss: 0.0505932942032814 = 0.04373285174369812 + 0.001 * 6.860442638397217
Epoch 450, val loss: 0.8011289238929749
Epoch 460, training loss: 0.046823106706142426 = 0.03996202349662781 + 0.001 * 6.861084938049316
Epoch 460, val loss: 0.8119797110557556
Epoch 470, training loss: 0.04336100071668625 = 0.036509592086076736 + 0.001 * 6.851408004760742
Epoch 470, val loss: 0.822638988494873
Epoch 480, training loss: 0.04016754776239395 = 0.033322542905807495 + 0.001 * 6.845003128051758
Epoch 480, val loss: 0.8329254984855652
Epoch 490, training loss: 0.03729650378227234 = 0.030448710545897484 + 0.001 * 6.847792625427246
Epoch 490, val loss: 0.8430913686752319
Epoch 500, training loss: 0.03470003977417946 = 0.02785571478307247 + 0.001 * 6.844324588775635
Epoch 500, val loss: 0.8534567952156067
Epoch 510, training loss: 0.03235942870378494 = 0.025521742179989815 + 0.001 * 6.8376874923706055
Epoch 510, val loss: 0.8636897802352905
Epoch 520, training loss: 0.030280331149697304 = 0.023446306586265564 + 0.001 * 6.834023952484131
Epoch 520, val loss: 0.8740556836128235
Epoch 530, training loss: 0.028428517282009125 = 0.02160577103495598 + 0.001 * 6.822745323181152
Epoch 530, val loss: 0.8845281600952148
Epoch 540, training loss: 0.026796570047736168 = 0.01996893249452114 + 0.001 * 6.827637195587158
Epoch 540, val loss: 0.8950645923614502
Epoch 550, training loss: 0.025331396609544754 = 0.018510257825255394 + 0.001 * 6.821138858795166
Epoch 550, val loss: 0.9054287075996399
Epoch 560, training loss: 0.024035755544900894 = 0.01720299758017063 + 0.001 * 6.832757472991943
Epoch 560, val loss: 0.9157455563545227
Epoch 570, training loss: 0.022859251126646996 = 0.01602707803249359 + 0.001 * 6.832172393798828
Epoch 570, val loss: 0.925870418548584
Epoch 580, training loss: 0.021789291873574257 = 0.014966445975005627 + 0.001 * 6.822845935821533
Epoch 580, val loss: 0.935856819152832
Epoch 590, training loss: 0.02084740623831749 = 0.014007143676280975 + 0.001 * 6.840261936187744
Epoch 590, val loss: 0.9456424713134766
Epoch 600, training loss: 0.019938908517360687 = 0.013136327266693115 + 0.001 * 6.802579879760742
Epoch 600, val loss: 0.9552397727966309
Epoch 610, training loss: 0.019147634506225586 = 0.012344679795205593 + 0.001 * 6.802953243255615
Epoch 610, val loss: 0.964576780796051
Epoch 620, training loss: 0.01847534067928791 = 0.011625779792666435 + 0.001 * 6.849560260772705
Epoch 620, val loss: 0.9738370776176453
Epoch 630, training loss: 0.01777849905192852 = 0.010968527756631374 + 0.001 * 6.809970855712891
Epoch 630, val loss: 0.9827672839164734
Epoch 640, training loss: 0.017168335616588593 = 0.01036488451063633 + 0.001 * 6.803449630737305
Epoch 640, val loss: 0.9916345477104187
Epoch 650, training loss: 0.01659565418958664 = 0.009806385263800621 + 0.001 * 6.7892680168151855
Epoch 650, val loss: 1.0001682043075562
Epoch 660, training loss: 0.016101373359560966 = 0.0092905443161726 + 0.001 * 6.81082820892334
Epoch 660, val loss: 1.0085116624832153
Epoch 670, training loss: 0.01563376747071743 = 0.008819312788546085 + 0.001 * 6.814454078674316
Epoch 670, val loss: 1.0167309045791626
Epoch 680, training loss: 0.015169824473559856 = 0.008386919274926186 + 0.001 * 6.782905101776123
Epoch 680, val loss: 1.024827003479004
Epoch 690, training loss: 0.014775265008211136 = 0.007987519726157188 + 0.001 * 6.787744522094727
Epoch 690, val loss: 1.0327116250991821
Epoch 700, training loss: 0.014394896104931831 = 0.0076162852346897125 + 0.001 * 6.778610706329346
Epoch 700, val loss: 1.040347695350647
Epoch 710, training loss: 0.014059136621654034 = 0.007272583432495594 + 0.001 * 6.786552906036377
Epoch 710, val loss: 1.0479322671890259
Epoch 720, training loss: 0.013773133046925068 = 0.006953668314963579 + 0.001 * 6.819464206695557
Epoch 720, val loss: 1.055258870124817
Epoch 730, training loss: 0.013430826365947723 = 0.006656773388385773 + 0.001 * 6.774052619934082
Epoch 730, val loss: 1.0624924898147583
Epoch 740, training loss: 0.013185814023017883 = 0.006379993166774511 + 0.001 * 6.805820941925049
Epoch 740, val loss: 1.0694741010665894
Epoch 750, training loss: 0.012883685529232025 = 0.006120688747614622 + 0.001 * 6.762996196746826
Epoch 750, val loss: 1.0764778852462769
Epoch 760, training loss: 0.012663082219660282 = 0.005873784888535738 + 0.001 * 6.789297103881836
Epoch 760, val loss: 1.0830888748168945
Epoch 770, training loss: 0.01240626536309719 = 0.005641887430101633 + 0.001 * 6.764378070831299
Epoch 770, val loss: 1.089731216430664
Epoch 780, training loss: 0.012177294120192528 = 0.005424897186458111 + 0.001 * 6.752396106719971
Epoch 780, val loss: 1.0962071418762207
Epoch 790, training loss: 0.011986937373876572 = 0.005222222302109003 + 0.001 * 6.764715194702148
Epoch 790, val loss: 1.1025285720825195
Epoch 800, training loss: 0.01179804652929306 = 0.005034890491515398 + 0.001 * 6.763155937194824
Epoch 800, val loss: 1.1087157726287842
Epoch 810, training loss: 0.011623275466263294 = 0.004859879147261381 + 0.001 * 6.7633957862854
Epoch 810, val loss: 1.1149299144744873
Epoch 820, training loss: 0.011481504887342453 = 0.004695193842053413 + 0.001 * 6.786310195922852
Epoch 820, val loss: 1.1209110021591187
Epoch 830, training loss: 0.011293086223304272 = 0.0045398385263979435 + 0.001 * 6.753247261047363
Epoch 830, val loss: 1.1267856359481812
Epoch 840, training loss: 0.011139554902911186 = 0.004393188748508692 + 0.001 * 6.746365547180176
Epoch 840, val loss: 1.1325533390045166
Epoch 850, training loss: 0.011019549332559109 = 0.004254486411809921 + 0.001 * 6.7650628089904785
Epoch 850, val loss: 1.1381852626800537
Epoch 860, training loss: 0.01087752915918827 = 0.004123140126466751 + 0.001 * 6.75438928604126
Epoch 860, val loss: 1.1437580585479736
Epoch 870, training loss: 0.010757321491837502 = 0.003998623229563236 + 0.001 * 6.758697509765625
Epoch 870, val loss: 1.1491847038269043
Epoch 880, training loss: 0.010617724619805813 = 0.0038802658673375845 + 0.001 * 6.7374587059021
Epoch 880, val loss: 1.1545555591583252
Epoch 890, training loss: 0.01049670297652483 = 0.0037677274085581303 + 0.001 * 6.728975296020508
Epoch 890, val loss: 1.1597697734832764
Epoch 900, training loss: 0.0103859081864357 = 0.0036604467313736677 + 0.001 * 6.725461483001709
Epoch 900, val loss: 1.1649900674819946
Epoch 910, training loss: 0.010313855484127998 = 0.0035579234827309847 + 0.001 * 6.755931377410889
Epoch 910, val loss: 1.1701116561889648
Epoch 920, training loss: 0.010188148356974125 = 0.003459702478721738 + 0.001 * 6.728446006774902
Epoch 920, val loss: 1.1751811504364014
Epoch 930, training loss: 0.010090716183185577 = 0.003365555079653859 + 0.001 * 6.725161075592041
Epoch 930, val loss: 1.1801775693893433
Epoch 940, training loss: 0.01000197697430849 = 0.003275008639320731 + 0.001 * 6.726967811584473
Epoch 940, val loss: 1.1851247549057007
Epoch 950, training loss: 0.009939554147422314 = 0.003187899710610509 + 0.001 * 6.751653671264648
Epoch 950, val loss: 1.1900289058685303
Epoch 960, training loss: 0.00983561109751463 = 0.0031041123438626528 + 0.001 * 6.731498718261719
Epoch 960, val loss: 1.1949131488800049
Epoch 970, training loss: 0.009742064401507378 = 0.0030234402511268854 + 0.001 * 6.718624114990234
Epoch 970, val loss: 1.1997504234313965
Epoch 980, training loss: 0.009662793017923832 = 0.0029456361662596464 + 0.001 * 6.717156887054443
Epoch 980, val loss: 1.2045331001281738
Epoch 990, training loss: 0.009602979756891727 = 0.0028706879820674658 + 0.001 * 6.7322916984558105
Epoch 990, val loss: 1.2092680931091309
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6384
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9498990774154663 = 1.9415252208709717 + 0.001 * 8.37385368347168
Epoch 0, val loss: 1.9433273077011108
Epoch 10, training loss: 1.9399287700653076 = 1.9315550327301025 + 0.001 * 8.373777389526367
Epoch 10, val loss: 1.933732509613037
Epoch 20, training loss: 1.9276329278945923 = 1.9192594289779663 + 0.001 * 8.373544692993164
Epoch 20, val loss: 1.9212419986724854
Epoch 30, training loss: 1.9104751348495483 = 1.9021021127700806 + 0.001 * 8.373050689697266
Epoch 30, val loss: 1.9034686088562012
Epoch 40, training loss: 1.8853625059127808 = 1.8769906759262085 + 0.001 * 8.37187385559082
Epoch 40, val loss: 1.8777302503585815
Epoch 50, training loss: 1.8501453399658203 = 1.841776967048645 + 0.001 * 8.368351936340332
Epoch 50, val loss: 1.8429967164993286
Epoch 60, training loss: 1.808346152305603 = 1.7999920845031738 + 0.001 * 8.35411262512207
Epoch 60, val loss: 1.8051787614822388
Epoch 70, training loss: 1.7659358978271484 = 1.7576606273651123 + 0.001 * 8.275300025939941
Epoch 70, val loss: 1.7691868543624878
Epoch 80, training loss: 1.7126137018203735 = 1.7047805786132812 + 0.001 * 7.833098411560059
Epoch 80, val loss: 1.722489595413208
Epoch 90, training loss: 1.6402214765548706 = 1.6325129270553589 + 0.001 * 7.708559513092041
Epoch 90, val loss: 1.6592440605163574
Epoch 100, training loss: 1.5485113859176636 = 1.5408704280853271 + 0.001 * 7.640981674194336
Epoch 100, val loss: 1.5813124179840088
Epoch 110, training loss: 1.4470746517181396 = 1.4395668506622314 + 0.001 * 7.507856845855713
Epoch 110, val loss: 1.4974843263626099
Epoch 120, training loss: 1.3438578844070435 = 1.336526870727539 + 0.001 * 7.331038951873779
Epoch 120, val loss: 1.4157594442367554
Epoch 130, training loss: 1.2424378395080566 = 1.2351510524749756 + 0.001 * 7.286823272705078
Epoch 130, val loss: 1.3396180868148804
Epoch 140, training loss: 1.1454015970230103 = 1.138183355331421 + 0.001 * 7.218219757080078
Epoch 140, val loss: 1.2702984809875488
Epoch 150, training loss: 1.0553219318389893 = 1.0481761693954468 + 0.001 * 7.145802021026611
Epoch 150, val loss: 1.2069567441940308
Epoch 160, training loss: 0.9729549288749695 = 0.9658516049385071 + 0.001 * 7.103307247161865
Epoch 160, val loss: 1.1492785215377808
Epoch 170, training loss: 0.8968066573143005 = 0.8897199034690857 + 0.001 * 7.086777210235596
Epoch 170, val loss: 1.0953619480133057
Epoch 180, training loss: 0.8242505788803101 = 0.8171761631965637 + 0.001 * 7.074391841888428
Epoch 180, val loss: 1.0431196689605713
Epoch 190, training loss: 0.7534999251365662 = 0.7464404702186584 + 0.001 * 7.059478759765625
Epoch 190, val loss: 0.9913623929023743
Epoch 200, training loss: 0.6846747398376465 = 0.6776323318481445 + 0.001 * 7.042433261871338
Epoch 200, val loss: 0.9406564831733704
Epoch 210, training loss: 0.6194743514060974 = 0.6124511957168579 + 0.001 * 7.02315092086792
Epoch 210, val loss: 0.8935390710830688
Epoch 220, training loss: 0.5597304105758667 = 0.5527282357215881 + 0.001 * 7.002144813537598
Epoch 220, val loss: 0.8526986241340637
Epoch 230, training loss: 0.5064916014671326 = 0.49950864911079407 + 0.001 * 6.982928276062012
Epoch 230, val loss: 0.8199276328086853
Epoch 240, training loss: 0.4597019851207733 = 0.45272567868232727 + 0.001 * 6.976315021514893
Epoch 240, val loss: 0.79497230052948
Epoch 250, training loss: 0.41829317808151245 = 0.4113282263278961 + 0.001 * 6.964942455291748
Epoch 250, val loss: 0.7764787673950195
Epoch 260, training loss: 0.38097694516181946 = 0.37401434779167175 + 0.001 * 6.962582588195801
Epoch 260, val loss: 0.763083815574646
Epoch 270, training loss: 0.34671032428741455 = 0.33975085616111755 + 0.001 * 6.959473609924316
Epoch 270, val loss: 0.7537698149681091
Epoch 280, training loss: 0.3148151636123657 = 0.30785778164863586 + 0.001 * 6.957382678985596
Epoch 280, val loss: 0.7477942109107971
Epoch 290, training loss: 0.2848896086215973 = 0.2779342830181122 + 0.001 * 6.955317497253418
Epoch 290, val loss: 0.7444267272949219
Epoch 300, training loss: 0.2567722797393799 = 0.24981902539730072 + 0.001 * 6.953256130218506
Epoch 300, val loss: 0.7434633374214172
Epoch 310, training loss: 0.2304770052433014 = 0.22352580726146698 + 0.001 * 6.951202392578125
Epoch 310, val loss: 0.7448388934135437
Epoch 320, training loss: 0.20609183609485626 = 0.1991426944732666 + 0.001 * 6.949145317077637
Epoch 320, val loss: 0.7484002113342285
Epoch 330, training loss: 0.18373900651931763 = 0.1767919808626175 + 0.001 * 6.947021007537842
Epoch 330, val loss: 0.7539650797843933
Epoch 340, training loss: 0.1635051965713501 = 0.156560480594635 + 0.001 * 6.944722652435303
Epoch 340, val loss: 0.7615156769752502
Epoch 350, training loss: 0.14539474248886108 = 0.13845182955265045 + 0.001 * 6.942905902862549
Epoch 350, val loss: 0.7707533836364746
Epoch 360, training loss: 0.12931470572948456 = 0.12237395346164703 + 0.001 * 6.940746784210205
Epoch 360, val loss: 0.7814105749130249
Epoch 370, training loss: 0.11512569338083267 = 0.10818903148174286 + 0.001 * 6.936663627624512
Epoch 370, val loss: 0.7931331992149353
Epoch 380, training loss: 0.10267036408185959 = 0.0957375317811966 + 0.001 * 6.932834148406982
Epoch 380, val loss: 0.8055351972579956
Epoch 390, training loss: 0.09179148823022842 = 0.08485466986894608 + 0.001 * 6.936819553375244
Epoch 390, val loss: 0.8183547854423523
Epoch 400, training loss: 0.08229623734951019 = 0.07537113130092621 + 0.001 * 6.925102710723877
Epoch 400, val loss: 0.8314037919044495
Epoch 410, training loss: 0.07404329627752304 = 0.06712394207715988 + 0.001 * 6.919355869293213
Epoch 410, val loss: 0.8445444703102112
Epoch 420, training loss: 0.06686969846487045 = 0.05995862931013107 + 0.001 * 6.911071300506592
Epoch 420, val loss: 0.8576599955558777
Epoch 430, training loss: 0.06062975898385048 = 0.05371575802564621 + 0.001 * 6.914000034332275
Epoch 430, val loss: 0.8707326650619507
Epoch 440, training loss: 0.055159009993076324 = 0.04825950413942337 + 0.001 * 6.8995041847229
Epoch 440, val loss: 0.883708655834198
Epoch 450, training loss: 0.05037246271967888 = 0.04348766431212425 + 0.001 * 6.8847975730896
Epoch 450, val loss: 0.8965551257133484
Epoch 460, training loss: 0.046200964599847794 = 0.03931046277284622 + 0.001 * 6.8905029296875
Epoch 460, val loss: 0.9092379808425903
Epoch 470, training loss: 0.04252716526389122 = 0.0356474444270134 + 0.001 * 6.8797221183776855
Epoch 470, val loss: 0.9217589497566223
Epoch 480, training loss: 0.0392998568713665 = 0.032429687678813934 + 0.001 * 6.8701677322387695
Epoch 480, val loss: 0.9340782165527344
Epoch 490, training loss: 0.036462146788835526 = 0.02959660440683365 + 0.001 * 6.865543365478516
Epoch 490, val loss: 0.9461508989334106
Epoch 500, training loss: 0.03395407646894455 = 0.027095574885606766 + 0.001 * 6.858499526977539
Epoch 500, val loss: 0.9579769372940063
Epoch 510, training loss: 0.031738925725221634 = 0.02488154172897339 + 0.001 * 6.857384204864502
Epoch 510, val loss: 0.9695811867713928
Epoch 520, training loss: 0.029777154326438904 = 0.022916005924344063 + 0.001 * 6.861147880554199
Epoch 520, val loss: 0.9809051752090454
Epoch 530, training loss: 0.028024809435009956 = 0.021166076883673668 + 0.001 * 6.858732223510742
Epoch 530, val loss: 0.9919448494911194
Epoch 540, training loss: 0.0264551043510437 = 0.01960303820669651 + 0.001 * 6.852065086364746
Epoch 540, val loss: 1.0027203559875488
Epoch 550, training loss: 0.025054071098566055 = 0.018202733248472214 + 0.001 * 6.851336479187012
Epoch 550, val loss: 1.0132302045822144
Epoch 560, training loss: 0.02379274182021618 = 0.016944540664553642 + 0.001 * 6.848200798034668
Epoch 560, val loss: 1.0234606266021729
Epoch 570, training loss: 0.022660909220576286 = 0.015810733661055565 + 0.001 * 6.850175380706787
Epoch 570, val loss: 1.033436894416809
Epoch 580, training loss: 0.021631088107824326 = 0.014786179177463055 + 0.001 * 6.844909191131592
Epoch 580, val loss: 1.0431573390960693
Epoch 590, training loss: 0.020701326429843903 = 0.013857697136700153 + 0.001 * 6.8436279296875
Epoch 590, val loss: 1.0526173114776611
Epoch 600, training loss: 0.019856123253703117 = 0.013013542629778385 + 0.001 * 6.8425798416137695
Epoch 600, val loss: 1.0618547201156616
Epoch 610, training loss: 0.0190815981477499 = 0.01224256306886673 + 0.001 * 6.839034080505371
Epoch 610, val loss: 1.0709023475646973
Epoch 620, training loss: 0.018370576202869415 = 0.011534581892192364 + 0.001 * 6.83599328994751
Epoch 620, val loss: 1.0798261165618896
Epoch 630, training loss: 0.017731275409460068 = 0.010881181806325912 + 0.001 * 6.850093364715576
Epoch 630, val loss: 1.088679552078247
Epoch 640, training loss: 0.017115946859121323 = 0.010276595130562782 + 0.001 * 6.839350700378418
Epoch 640, val loss: 1.0974774360656738
Epoch 650, training loss: 0.016549544408917427 = 0.009716477245092392 + 0.001 * 6.833067417144775
Epoch 650, val loss: 1.1062043905258179
Epoch 660, training loss: 0.01604204624891281 = 0.009197309613227844 + 0.001 * 6.844735622406006
Epoch 660, val loss: 1.1148266792297363
Epoch 670, training loss: 0.015545987524092197 = 0.008715944364666939 + 0.001 * 6.830042839050293
Epoch 670, val loss: 1.123346209526062
Epoch 680, training loss: 0.015102094039320946 = 0.008269212208688259 + 0.001 * 6.832880973815918
Epoch 680, val loss: 1.131760597229004
Epoch 690, training loss: 0.014689914882183075 = 0.00785449706017971 + 0.001 * 6.8354172706604
Epoch 690, val loss: 1.1400326490402222
Epoch 700, training loss: 0.014290133491158485 = 0.007469081319868565 + 0.001 * 6.821051597595215
Epoch 700, val loss: 1.1481633186340332
Epoch 710, training loss: 0.013930528424680233 = 0.007110361009836197 + 0.001 * 6.820167064666748
Epoch 710, val loss: 1.156173825263977
Epoch 720, training loss: 0.013607874512672424 = 0.006775775458663702 + 0.001 * 6.832098960876465
Epoch 720, val loss: 1.1640105247497559
Epoch 730, training loss: 0.013281136751174927 = 0.006463510449975729 + 0.001 * 6.817626476287842
Epoch 730, val loss: 1.1716783046722412
Epoch 740, training loss: 0.012986166402697563 = 0.006171553395688534 + 0.001 * 6.81461238861084
Epoch 740, val loss: 1.179293155670166
Epoch 750, training loss: 0.012711022980511189 = 0.005898341536521912 + 0.001 * 6.812681198120117
Epoch 750, val loss: 1.186716079711914
Epoch 760, training loss: 0.012460460886359215 = 0.005642506759613752 + 0.001 * 6.817953586578369
Epoch 760, val loss: 1.1940184831619263
Epoch 770, training loss: 0.012210866436362267 = 0.005402641370892525 + 0.001 * 6.808224201202393
Epoch 770, val loss: 1.2011555433273315
Epoch 780, training loss: 0.012005074881017208 = 0.005177466664463282 + 0.001 * 6.827608108520508
Epoch 780, val loss: 1.2081831693649292
Epoch 790, training loss: 0.011776608414947987 = 0.004965958185493946 + 0.001 * 6.810649871826172
Epoch 790, val loss: 1.215061068534851
Epoch 800, training loss: 0.011572119779884815 = 0.004767064470797777 + 0.001 * 6.805055141448975
Epoch 800, val loss: 1.22181236743927
Epoch 810, training loss: 0.01139896735548973 = 0.004579906351864338 + 0.001 * 6.819060802459717
Epoch 810, val loss: 1.2284175157546997
Epoch 820, training loss: 0.011213697493076324 = 0.004403649363666773 + 0.001 * 6.810047149658203
Epoch 820, val loss: 1.234908938407898
Epoch 830, training loss: 0.011060196906328201 = 0.004237536806613207 + 0.001 * 6.822660446166992
Epoch 830, val loss: 1.2412519454956055
Epoch 840, training loss: 0.010879028588533401 = 0.004081014543771744 + 0.001 * 6.798013210296631
Epoch 840, val loss: 1.2474501132965088
Epoch 850, training loss: 0.0107300765812397 = 0.003933335188776255 + 0.001 * 6.796740531921387
Epoch 850, val loss: 1.2535195350646973
Epoch 860, training loss: 0.010588748380541801 = 0.0037938382010906935 + 0.001 * 6.794909954071045
Epoch 860, val loss: 1.2594614028930664
Epoch 870, training loss: 0.010468006134033203 = 0.003661961294710636 + 0.001 * 6.806045055389404
Epoch 870, val loss: 1.2652475833892822
Epoch 880, training loss: 0.010335543192923069 = 0.0035372995771467686 + 0.001 * 6.798243522644043
Epoch 880, val loss: 1.2709418535232544
Epoch 890, training loss: 0.010217936709523201 = 0.0034193277824670076 + 0.001 * 6.798608779907227
Epoch 890, val loss: 1.2765010595321655
Epoch 900, training loss: 0.010095045901834965 = 0.003307557897642255 + 0.001 * 6.787487506866455
Epoch 900, val loss: 1.281948208808899
Epoch 910, training loss: 0.010006862692534924 = 0.003201591782271862 + 0.001 * 6.805270671844482
Epoch 910, val loss: 1.2872787714004517
Epoch 920, training loss: 0.009896626695990562 = 0.003101077163591981 + 0.001 * 6.795548915863037
Epoch 920, val loss: 1.2924799919128418
Epoch 930, training loss: 0.009798185899853706 = 0.0030056596733629704 + 0.001 * 6.792525291442871
Epoch 930, val loss: 1.2975893020629883
Epoch 940, training loss: 0.009723540395498276 = 0.0029150517657399178 + 0.001 * 6.808488845825195
Epoch 940, val loss: 1.3025634288787842
Epoch 950, training loss: 0.009613766334950924 = 0.002828938188031316 + 0.001 * 6.784828186035156
Epoch 950, val loss: 1.3074430227279663
Epoch 960, training loss: 0.009538000449538231 = 0.0027470828499644995 + 0.001 * 6.79091739654541
Epoch 960, val loss: 1.3122111558914185
Epoch 970, training loss: 0.009455698542296886 = 0.00266922521404922 + 0.001 * 6.786473274230957
Epoch 970, val loss: 1.316891074180603
Epoch 980, training loss: 0.009380050003528595 = 0.002595064230263233 + 0.001 * 6.784985542297363
Epoch 980, val loss: 1.3214645385742188
Epoch 990, training loss: 0.009306374937295914 = 0.002524398500099778 + 0.001 * 6.781975746154785
Epoch 990, val loss: 1.3259525299072266
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7934
Flip ASR: 0.7556/225 nodes
The final ASR:0.59287, 0.18510, Accuracy:0.80247, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9574])
updated graph: torch.Size([2, 10656])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9560774564743042 = 1.9477035999298096 + 0.001 * 8.3739013671875
Epoch 0, val loss: 1.9460749626159668
Epoch 10, training loss: 1.945483922958374 = 1.9371100664138794 + 0.001 * 8.373842239379883
Epoch 10, val loss: 1.93520987033844
Epoch 20, training loss: 1.932402491569519 = 1.9240288734436035 + 0.001 * 8.373644828796387
Epoch 20, val loss: 1.9216023683547974
Epoch 30, training loss: 1.9141483306884766 = 1.9057751893997192 + 0.001 * 8.373198509216309
Epoch 30, val loss: 1.902669906616211
Epoch 40, training loss: 1.88744056224823 = 1.879068374633789 + 0.001 * 8.372133255004883
Epoch 40, val loss: 1.8754817247390747
Epoch 50, training loss: 1.8503971099853516 = 1.8420277833938599 + 0.001 * 8.369324684143066
Epoch 50, val loss: 1.839690923690796
Epoch 60, training loss: 1.8079668283462524 = 1.7996069192886353 + 0.001 * 8.359956741333008
Epoch 60, val loss: 1.803377389907837
Epoch 70, training loss: 1.7686381340026855 = 1.760320782661438 + 0.001 * 8.31731128692627
Epoch 70, val loss: 1.7742469310760498
Epoch 80, training loss: 1.720070242881775 = 1.7120506763458252 + 0.001 * 8.019590377807617
Epoch 80, val loss: 1.7358503341674805
Epoch 90, training loss: 1.6517053842544556 = 1.6439005136489868 + 0.001 * 7.804900169372559
Epoch 90, val loss: 1.6788513660430908
Epoch 100, training loss: 1.5624191761016846 = 1.5548003911972046 + 0.001 * 7.6187896728515625
Epoch 100, val loss: 1.6060031652450562
Epoch 110, training loss: 1.460578441619873 = 1.453050971031189 + 0.001 * 7.527471542358398
Epoch 110, val loss: 1.527235507965088
Epoch 120, training loss: 1.3574692010879517 = 1.3499500751495361 + 0.001 * 7.519140243530273
Epoch 120, val loss: 1.4465055465698242
Epoch 130, training loss: 1.2568919658660889 = 1.2494009733200073 + 0.001 * 7.490969657897949
Epoch 130, val loss: 1.369710922241211
Epoch 140, training loss: 1.1603987216949463 = 1.1529507637023926 + 0.001 * 7.4479875564575195
Epoch 140, val loss: 1.2956727743148804
Epoch 150, training loss: 1.069820523262024 = 1.0624769926071167 + 0.001 * 7.343535423278809
Epoch 150, val loss: 1.2270588874816895
Epoch 160, training loss: 0.9867345690727234 = 0.9795249700546265 + 0.001 * 7.20960807800293
Epoch 160, val loss: 1.1650303602218628
Epoch 170, training loss: 0.9102386236190796 = 0.9030526876449585 + 0.001 * 7.185944080352783
Epoch 170, val loss: 1.109678030014038
Epoch 180, training loss: 0.8377113342285156 = 0.8305556178092957 + 0.001 * 7.155697822570801
Epoch 180, val loss: 1.0579179525375366
Epoch 190, training loss: 0.7675071358680725 = 0.7603683471679688 + 0.001 * 7.138803958892822
Epoch 190, val loss: 1.00823175907135
Epoch 200, training loss: 0.6995505690574646 = 0.6924273371696472 + 0.001 * 7.123228549957275
Epoch 200, val loss: 0.961606502532959
Epoch 210, training loss: 0.6347163319587708 = 0.6276039481163025 + 0.001 * 7.112362861633301
Epoch 210, val loss: 0.9189477562904358
Epoch 220, training loss: 0.5737453103065491 = 0.5666412711143494 + 0.001 * 7.104067802429199
Epoch 220, val loss: 0.8810282945632935
Epoch 230, training loss: 0.5169278383255005 = 0.50982666015625 + 0.001 * 7.101152420043945
Epoch 230, val loss: 0.8478906154632568
Epoch 240, training loss: 0.46401455998420715 = 0.4569181799888611 + 0.001 * 7.096380233764648
Epoch 240, val loss: 0.8191274404525757
Epoch 250, training loss: 0.4144863486289978 = 0.4073919951915741 + 0.001 * 7.094367504119873
Epoch 250, val loss: 0.7945153713226318
Epoch 260, training loss: 0.3678824305534363 = 0.3607898950576782 + 0.001 * 7.092540264129639
Epoch 260, val loss: 0.7742114067077637
Epoch 270, training loss: 0.3242163360118866 = 0.3171263337135315 + 0.001 * 7.090011119842529
Epoch 270, val loss: 0.7585592865943909
Epoch 280, training loss: 0.2839898467063904 = 0.27690693736076355 + 0.001 * 7.082923889160156
Epoch 280, val loss: 0.7479287385940552
Epoch 290, training loss: 0.24779126048088074 = 0.24071355164051056 + 0.001 * 7.077703475952148
Epoch 290, val loss: 0.7425678968429565
Epoch 300, training loss: 0.21594730019569397 = 0.20887409150600433 + 0.001 * 7.073211669921875
Epoch 300, val loss: 0.7419946193695068
Epoch 310, training loss: 0.1884428709745407 = 0.181374654173851 + 0.001 * 7.068221569061279
Epoch 310, val loss: 0.7458038926124573
Epoch 320, training loss: 0.1649978756904602 = 0.15793557465076447 + 0.001 * 7.062305927276611
Epoch 320, val loss: 0.7532951235771179
Epoch 330, training loss: 0.14517390727996826 = 0.138104647397995 + 0.001 * 7.0692596435546875
Epoch 330, val loss: 0.7639246582984924
Epoch 340, training loss: 0.1283605396747589 = 0.12130358815193176 + 0.001 * 7.056949615478516
Epoch 340, val loss: 0.7769410610198975
Epoch 350, training loss: 0.11398064345121384 = 0.10692893713712692 + 0.001 * 7.0517048835754395
Epoch 350, val loss: 0.7917819619178772
Epoch 360, training loss: 0.10154208540916443 = 0.0944945365190506 + 0.001 * 7.047548770904541
Epoch 360, val loss: 0.8080120086669922
Epoch 370, training loss: 0.09071150422096252 = 0.08366426825523376 + 0.001 * 7.047236919403076
Epoch 370, val loss: 0.825244665145874
Epoch 380, training loss: 0.08124728500843048 = 0.07420646399259567 + 0.001 * 7.0408220291137695
Epoch 380, val loss: 0.8432876467704773
Epoch 390, training loss: 0.07298633456230164 = 0.06594783812761307 + 0.001 * 7.038496971130371
Epoch 390, val loss: 0.8618423938751221
Epoch 400, training loss: 0.06578684598207474 = 0.058746833354234695 + 0.001 * 7.040013790130615
Epoch 400, val loss: 0.880768895149231
Epoch 410, training loss: 0.05951211228966713 = 0.052475620061159134 + 0.001 * 7.0364909172058105
Epoch 410, val loss: 0.8998335003852844
Epoch 420, training loss: 0.054050516337156296 = 0.04701633378863335 + 0.001 * 7.034182548522949
Epoch 420, val loss: 0.9188747406005859
Epoch 430, training loss: 0.04928535595536232 = 0.042260050773620605 + 0.001 * 7.02530574798584
Epoch 430, val loss: 0.9377883076667786
Epoch 440, training loss: 0.045136239379644394 = 0.038110941648483276 + 0.001 * 7.025296688079834
Epoch 440, val loss: 0.9565292000770569
Epoch 450, training loss: 0.04151088744401932 = 0.034485530108213425 + 0.001 * 7.025355339050293
Epoch 450, val loss: 0.9749000668525696
Epoch 460, training loss: 0.038330335170030594 = 0.03130992874503136 + 0.001 * 7.020406246185303
Epoch 460, val loss: 0.9929278492927551
Epoch 470, training loss: 0.035539865493774414 = 0.0285207349807024 + 0.001 * 7.019131660461426
Epoch 470, val loss: 1.0105832815170288
Epoch 480, training loss: 0.033078428357839584 = 0.026064176112413406 + 0.001 * 7.014252185821533
Epoch 480, val loss: 1.0277760028839111
Epoch 490, training loss: 0.030902577564120293 = 0.0238941702991724 + 0.001 * 7.008407115936279
Epoch 490, val loss: 1.0444588661193848
Epoch 500, training loss: 0.028979238122701645 = 0.021971261128783226 + 0.001 * 7.007977485656738
Epoch 500, val loss: 1.0606722831726074
Epoch 510, training loss: 0.027271412312984467 = 0.0202621016651392 + 0.001 * 7.009311199188232
Epoch 510, val loss: 1.0764436721801758
Epoch 520, training loss: 0.02574753388762474 = 0.01873825117945671 + 0.001 * 7.009281635284424
Epoch 520, val loss: 1.0917108058929443
Epoch 530, training loss: 0.02437710016965866 = 0.017375480383634567 + 0.001 * 7.001618385314941
Epoch 530, val loss: 1.1065378189086914
Epoch 540, training loss: 0.02316056378185749 = 0.016153205186128616 + 0.001 * 7.007358074188232
Epoch 540, val loss: 1.1209443807601929
Epoch 550, training loss: 0.022054795175790787 = 0.015053641051054 + 0.001 * 7.001152992248535
Epoch 550, val loss: 1.1348844766616821
Epoch 560, training loss: 0.021058447659015656 = 0.014061794616281986 + 0.001 * 6.996651649475098
Epoch 560, val loss: 1.1484034061431885
Epoch 570, training loss: 0.020158296450972557 = 0.013164632022380829 + 0.001 * 6.993663787841797
Epoch 570, val loss: 1.1615558862686157
Epoch 580, training loss: 0.019341688603162766 = 0.012350893579423428 + 0.001 * 6.990793704986572
Epoch 580, val loss: 1.1743371486663818
Epoch 590, training loss: 0.018614450469613075 = 0.011610887013375759 + 0.001 * 7.0035624504089355
Epoch 590, val loss: 1.1866823434829712
Epoch 600, training loss: 0.017928479239344597 = 0.010936258360743523 + 0.001 * 6.992219924926758
Epoch 600, val loss: 1.198698878288269
Epoch 610, training loss: 0.017305942252278328 = 0.01031982060521841 + 0.001 * 6.986120700836182
Epoch 610, val loss: 1.2103646993637085
Epoch 620, training loss: 0.01674560457468033 = 0.009755213744938374 + 0.001 * 6.990389823913574
Epoch 620, val loss: 1.2216846942901611
Epoch 630, training loss: 0.016221925616264343 = 0.009236888028681278 + 0.001 * 6.985037326812744
Epoch 630, val loss: 1.2326929569244385
Epoch 640, training loss: 0.01574193313717842 = 0.008760066702961922 + 0.001 * 6.981866359710693
Epoch 640, val loss: 1.2433902025222778
Epoch 650, training loss: 0.015301773324608803 = 0.008320512250065804 + 0.001 * 6.981260299682617
Epoch 650, val loss: 1.253782033920288
Epoch 660, training loss: 0.014889130368828773 = 0.007914545945823193 + 0.001 * 6.974584102630615
Epoch 660, val loss: 1.2639074325561523
Epoch 670, training loss: 0.014525355771183968 = 0.00753891933709383 + 0.001 * 6.986435413360596
Epoch 670, val loss: 1.2737574577331543
Epoch 680, training loss: 0.014173116534948349 = 0.007190673612058163 + 0.001 * 6.9824419021606445
Epoch 680, val loss: 1.2833536863327026
Epoch 690, training loss: 0.013838192448019981 = 0.006867255084216595 + 0.001 * 6.970937252044678
Epoch 690, val loss: 1.2926987409591675
Epoch 700, training loss: 0.013536002486944199 = 0.0065663582645356655 + 0.001 * 6.969644546508789
Epoch 700, val loss: 1.3017827272415161
Epoch 710, training loss: 0.013259395956993103 = 0.006285982206463814 + 0.001 * 6.973413944244385
Epoch 710, val loss: 1.3106639385223389
Epoch 720, training loss: 0.012997789308428764 = 0.006024366244673729 + 0.001 * 6.973423004150391
Epoch 720, val loss: 1.3193023204803467
Epoch 730, training loss: 0.01274395827203989 = 0.005779887083917856 + 0.001 * 6.964070796966553
Epoch 730, val loss: 1.327733039855957
Epoch 740, training loss: 0.012505857273936272 = 0.0055510494858026505 + 0.001 * 6.954806804656982
Epoch 740, val loss: 1.3359525203704834
Epoch 750, training loss: 0.012310284189879894 = 0.005336592439562082 + 0.001 * 6.973691463470459
Epoch 750, val loss: 1.343975305557251
Epoch 760, training loss: 0.012097475118935108 = 0.005135339684784412 + 0.001 * 6.962135314941406
Epoch 760, val loss: 1.3517991304397583
Epoch 770, training loss: 0.011896132491528988 = 0.004946228116750717 + 0.001 * 6.949903964996338
Epoch 770, val loss: 1.3594248294830322
Epoch 780, training loss: 0.011715098284184933 = 0.004768305458128452 + 0.001 * 6.9467926025390625
Epoch 780, val loss: 1.3669064044952393
Epoch 790, training loss: 0.01156659610569477 = 0.004600733518600464 + 0.001 * 6.965862274169922
Epoch 790, val loss: 1.3742350339889526
Epoch 800, training loss: 0.011392047628760338 = 0.004442708566784859 + 0.001 * 6.949339389801025
Epoch 800, val loss: 1.3813297748565674
Epoch 810, training loss: 0.011232411488890648 = 0.004293528385460377 + 0.001 * 6.938882827758789
Epoch 810, val loss: 1.3882930278778076
Epoch 820, training loss: 0.011092355474829674 = 0.004152550362050533 + 0.001 * 6.939804553985596
Epoch 820, val loss: 1.3951003551483154
Epoch 830, training loss: 0.010959520004689693 = 0.004019172862172127 + 0.001 * 6.940346717834473
Epoch 830, val loss: 1.40176522731781
Epoch 840, training loss: 0.01084312703460455 = 0.0038928696885704994 + 0.001 * 6.950256824493408
Epoch 840, val loss: 1.4082661867141724
Epoch 850, training loss: 0.010706755332648754 = 0.003773156087845564 + 0.001 * 6.93359899520874
Epoch 850, val loss: 1.4146416187286377
Epoch 860, training loss: 0.010607599280774593 = 0.0036595850251615047 + 0.001 * 6.948013782501221
Epoch 860, val loss: 1.4208784103393555
Epoch 870, training loss: 0.010485127568244934 = 0.003551750909537077 + 0.001 * 6.933376312255859
Epoch 870, val loss: 1.4269649982452393
Epoch 880, training loss: 0.010365260764956474 = 0.003449255134910345 + 0.001 * 6.916005611419678
Epoch 880, val loss: 1.4329630136489868
Epoch 890, training loss: 0.010272491723299026 = 0.003351775463670492 + 0.001 * 6.920715808868408
Epoch 890, val loss: 1.4388149976730347
Epoch 900, training loss: 0.010175302624702454 = 0.0032589733600616455 + 0.001 * 6.916328430175781
Epoch 900, val loss: 1.444588541984558
Epoch 910, training loss: 0.01011364534497261 = 0.003170577809214592 + 0.001 * 6.9430670738220215
Epoch 910, val loss: 1.4501787424087524
Epoch 920, training loss: 0.009999504312872887 = 0.0030862658750265837 + 0.001 * 6.913238048553467
Epoch 920, val loss: 1.4557013511657715
Epoch 930, training loss: 0.009925244376063347 = 0.0030057858675718307 + 0.001 * 6.919457912445068
Epoch 930, val loss: 1.461096167564392
Epoch 940, training loss: 0.009854097850620747 = 0.0029288223013281822 + 0.001 * 6.9252753257751465
Epoch 940, val loss: 1.4664050340652466
Epoch 950, training loss: 0.009763579815626144 = 0.002854946069419384 + 0.001 * 6.908632755279541
Epoch 950, val loss: 1.471634864807129
Epoch 960, training loss: 0.009702030569314957 = 0.002783575328066945 + 0.001 * 6.918455123901367
Epoch 960, val loss: 1.476825475692749
Epoch 970, training loss: 0.009623277932405472 = 0.0027141552418470383 + 0.001 * 6.909122943878174
Epoch 970, val loss: 1.4820351600646973
Epoch 980, training loss: 0.009534870274364948 = 0.0026463281828910112 + 0.001 * 6.8885416984558105
Epoch 980, val loss: 1.4872995615005493
Epoch 990, training loss: 0.009474743157625198 = 0.0025799621362239122 + 0.001 * 6.894781112670898
Epoch 990, val loss: 1.4926519393920898
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.5756
Flip ASR: 0.4978/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.963401198387146 = 1.9550273418426514 + 0.001 * 8.373892784118652
Epoch 0, val loss: 1.9487320184707642
Epoch 10, training loss: 1.9522703886032104 = 1.9438966512680054 + 0.001 * 8.373769760131836
Epoch 10, val loss: 1.936549425125122
Epoch 20, training loss: 1.9383416175842285 = 1.9299683570861816 + 0.001 * 8.373291015625
Epoch 20, val loss: 1.9200186729431152
Epoch 30, training loss: 1.9189032316207886 = 1.910530924797058 + 0.001 * 8.372321128845215
Epoch 30, val loss: 1.8959733247756958
Epoch 40, training loss: 1.8917481899261475 = 1.8833776712417603 + 0.001 * 8.370479583740234
Epoch 40, val loss: 1.8633229732513428
Epoch 50, training loss: 1.8555842638015747 = 1.847217321395874 + 0.001 * 8.366886138916016
Epoch 50, val loss: 1.8234869241714478
Epoch 60, training loss: 1.8110064268112183 = 1.802648901939392 + 0.001 * 8.357471466064453
Epoch 60, val loss: 1.7798962593078613
Epoch 70, training loss: 1.7646042108535767 = 1.7562892436981201 + 0.001 * 8.315009117126465
Epoch 70, val loss: 1.740431785583496
Epoch 80, training loss: 1.7151949405670166 = 1.707179307937622 + 0.001 * 8.015641212463379
Epoch 80, val loss: 1.7002325057983398
Epoch 90, training loss: 1.647505521774292 = 1.639785885810852 + 0.001 * 7.719654560089111
Epoch 90, val loss: 1.6428838968276978
Epoch 100, training loss: 1.5573798418045044 = 1.549759864807129 + 0.001 * 7.619999408721924
Epoch 100, val loss: 1.5674515962600708
Epoch 110, training loss: 1.4523789882659912 = 1.444817066192627 + 0.001 * 7.561939239501953
Epoch 110, val loss: 1.4828169345855713
Epoch 120, training loss: 1.3493911027908325 = 1.3419533967971802 + 0.001 * 7.4376654624938965
Epoch 120, val loss: 1.404581069946289
Epoch 130, training loss: 1.2579442262649536 = 1.2505899667739868 + 0.001 * 7.354241847991943
Epoch 130, val loss: 1.3406161069869995
Epoch 140, training loss: 1.1774089336395264 = 1.1701327562332153 + 0.001 * 7.276182174682617
Epoch 140, val loss: 1.2879724502563477
Epoch 150, training loss: 1.102059245109558 = 1.0948448181152344 + 0.001 * 7.2144646644592285
Epoch 150, val loss: 1.238979697227478
Epoch 160, training loss: 1.026311993598938 = 1.0191277265548706 + 0.001 * 7.18425178527832
Epoch 160, val loss: 1.1875107288360596
Epoch 170, training loss: 0.9478436708450317 = 0.9406771659851074 + 0.001 * 7.166505813598633
Epoch 170, val loss: 1.1321475505828857
Epoch 180, training loss: 0.8680132627487183 = 0.8608722686767578 + 0.001 * 7.141022682189941
Epoch 180, val loss: 1.075537919998169
Epoch 190, training loss: 0.7902812361717224 = 0.7831676602363586 + 0.001 * 7.113602638244629
Epoch 190, val loss: 1.0207874774932861
Epoch 200, training loss: 0.7175757884979248 = 0.710477888584137 + 0.001 * 7.09789514541626
Epoch 200, val loss: 0.9699418544769287
Epoch 210, training loss: 0.6509060859680176 = 0.64380943775177 + 0.001 * 7.096622943878174
Epoch 210, val loss: 0.9245381355285645
Epoch 220, training loss: 0.5897796750068665 = 0.5826858878135681 + 0.001 * 7.093809127807617
Epoch 220, val loss: 0.8848766684532166
Epoch 230, training loss: 0.5335381031036377 = 0.5264453291893005 + 0.001 * 7.092794418334961
Epoch 230, val loss: 0.8509053587913513
Epoch 240, training loss: 0.48190584778785706 = 0.4748145043849945 + 0.001 * 7.09134578704834
Epoch 240, val loss: 0.8225812315940857
Epoch 250, training loss: 0.43491217494010925 = 0.4278223216533661 + 0.001 * 7.089848041534424
Epoch 250, val loss: 0.7994544506072998
Epoch 260, training loss: 0.39235731959342957 = 0.3852688670158386 + 0.001 * 7.088450908660889
Epoch 260, val loss: 0.7815906405448914
Epoch 270, training loss: 0.35378268361091614 = 0.34669554233551025 + 0.001 * 7.0871381759643555
Epoch 270, val loss: 0.7684492468833923
Epoch 280, training loss: 0.31864482164382935 = 0.31155893206596375 + 0.001 * 7.0858893394470215
Epoch 280, val loss: 0.7589752078056335
Epoch 290, training loss: 0.28661391139030457 = 0.27952781319618225 + 0.001 * 7.086111545562744
Epoch 290, val loss: 0.7529000043869019
Epoch 300, training loss: 0.2573220133781433 = 0.2502376437187195 + 0.001 * 7.084360122680664
Epoch 300, val loss: 0.7492860555648804
Epoch 310, training loss: 0.2305002063512802 = 0.22341638803482056 + 0.001 * 7.083813667297363
Epoch 310, val loss: 0.7483851313591003
Epoch 320, training loss: 0.20594792068004608 = 0.19886469841003418 + 0.001 * 7.083215713500977
Epoch 320, val loss: 0.7495384216308594
Epoch 330, training loss: 0.18365946412086487 = 0.17657388746738434 + 0.001 * 7.085573196411133
Epoch 330, val loss: 0.7525203824043274
Epoch 340, training loss: 0.1635102778673172 = 0.156426802277565 + 0.001 * 7.083472728729248
Epoch 340, val loss: 0.7571225762367249
Epoch 350, training loss: 0.14546246826648712 = 0.13838213682174683 + 0.001 * 7.080326557159424
Epoch 350, val loss: 0.7632078528404236
Epoch 360, training loss: 0.1294548660516739 = 0.12237676233053207 + 0.001 * 7.078104019165039
Epoch 360, val loss: 0.7706024050712585
Epoch 370, training loss: 0.11535912752151489 = 0.10828335583209991 + 0.001 * 7.075774192810059
Epoch 370, val loss: 0.7789276838302612
Epoch 380, training loss: 0.10299075394868851 = 0.09591485559940338 + 0.001 * 7.075896739959717
Epoch 380, val loss: 0.7880300879478455
Epoch 390, training loss: 0.09213268756866455 = 0.08505696058273315 + 0.001 * 7.075725555419922
Epoch 390, val loss: 0.7979029417037964
Epoch 400, training loss: 0.08260001987218857 = 0.07552409917116165 + 0.001 * 7.075922012329102
Epoch 400, val loss: 0.8083876371383667
Epoch 410, training loss: 0.07431482523679733 = 0.0672440230846405 + 0.001 * 7.070805072784424
Epoch 410, val loss: 0.8195158839225769
Epoch 420, training loss: 0.06720596551895142 = 0.06014049053192139 + 0.001 * 7.0654706954956055
Epoch 420, val loss: 0.8312852382659912
Epoch 430, training loss: 0.06110396981239319 = 0.054042719304561615 + 0.001 * 7.061251640319824
Epoch 430, val loss: 0.8432983160018921
Epoch 440, training loss: 0.05586400628089905 = 0.04879375174641609 + 0.001 * 7.070252895355225
Epoch 440, val loss: 0.8550024628639221
Epoch 450, training loss: 0.051307838410139084 = 0.044254180043935776 + 0.001 * 7.0536580085754395
Epoch 450, val loss: 0.8667807579040527
Epoch 460, training loss: 0.04735243692994118 = 0.0403020903468132 + 0.001 * 7.050344944000244
Epoch 460, val loss: 0.8786813020706177
Epoch 470, training loss: 0.043893180787563324 = 0.03684147447347641 + 0.001 * 7.051706790924072
Epoch 470, val loss: 0.8906315565109253
Epoch 480, training loss: 0.04084699973464012 = 0.033797118812799454 + 0.001 * 7.04987907409668
Epoch 480, val loss: 0.9025012254714966
Epoch 490, training loss: 0.038151711225509644 = 0.031106840819120407 + 0.001 * 7.044870376586914
Epoch 490, val loss: 0.9142161011695862
Epoch 500, training loss: 0.03578769415616989 = 0.028719065710902214 + 0.001 * 7.068628311157227
Epoch 500, val loss: 0.9256281852722168
Epoch 510, training loss: 0.03364107012748718 = 0.026590296998620033 + 0.001 * 7.0507707595825195
Epoch 510, val loss: 0.936835765838623
Epoch 520, training loss: 0.031724005937576294 = 0.024685829877853394 + 0.001 * 7.0381760597229
Epoch 520, val loss: 0.9477664828300476
Epoch 530, training loss: 0.030007794499397278 = 0.022974472492933273 + 0.001 * 7.033321857452393
Epoch 530, val loss: 0.9584672451019287
Epoch 540, training loss: 0.028463751077651978 = 0.02143048122525215 + 0.001 * 7.033270359039307
Epoch 540, val loss: 0.9688706994056702
Epoch 550, training loss: 0.027065394446253777 = 0.020036226138472557 + 0.001 * 7.029167652130127
Epoch 550, val loss: 0.9790651798248291
Epoch 560, training loss: 0.025800511240959167 = 0.018773531541228294 + 0.001 * 7.026980400085449
Epoch 560, val loss: 0.9889875650405884
Epoch 570, training loss: 0.0246749185025692 = 0.01762414164841175 + 0.001 * 7.050775527954102
Epoch 570, val loss: 0.998609185218811
Epoch 580, training loss: 0.023594912141561508 = 0.016574861481785774 + 0.001 * 7.020049571990967
Epoch 580, val loss: 1.0079652070999146
Epoch 590, training loss: 0.0226304791867733 = 0.015613689087331295 + 0.001 * 7.01678991317749
Epoch 590, val loss: 1.0171140432357788
Epoch 600, training loss: 0.021752621978521347 = 0.01473095640540123 + 0.001 * 7.021666049957275
Epoch 600, val loss: 1.0260075330734253
Epoch 610, training loss: 0.020929504185914993 = 0.013917934149503708 + 0.001 * 7.011569023132324
Epoch 610, val loss: 1.0346041917800903
Epoch 620, training loss: 0.020181195810437202 = 0.013167526572942734 + 0.001 * 7.013669013977051
Epoch 620, val loss: 1.0429946184158325
Epoch 630, training loss: 0.019496774300932884 = 0.012473688460886478 + 0.001 * 7.023085117340088
Epoch 630, val loss: 1.0511406660079956
Epoch 640, training loss: 0.018841003999114037 = 0.011830820702016354 + 0.001 * 7.0101823806762695
Epoch 640, val loss: 1.0590907335281372
Epoch 650, training loss: 0.018236789852380753 = 0.011234156787395477 + 0.001 * 7.0026326179504395
Epoch 650, val loss: 1.0668220520019531
Epoch 660, training loss: 0.017679980024695396 = 0.01067932229489088 + 0.001 * 7.000657558441162
Epoch 660, val loss: 1.0743850469589233
Epoch 670, training loss: 0.017155488952994347 = 0.01016301941126585 + 0.001 * 6.992469310760498
Epoch 670, val loss: 1.0816986560821533
Epoch 680, training loss: 0.01667790487408638 = 0.00968165136873722 + 0.001 * 6.996253967285156
Epoch 680, val loss: 1.0888711214065552
Epoch 690, training loss: 0.016225120052695274 = 0.009232371114194393 + 0.001 * 6.992748737335205
Epoch 690, val loss: 1.0958678722381592
Epoch 700, training loss: 0.01580232009291649 = 0.008812514133751392 + 0.001 * 6.989804744720459
Epoch 700, val loss: 1.1026551723480225
Epoch 710, training loss: 0.01540787611156702 = 0.008419989608228207 + 0.001 * 6.98788595199585
Epoch 710, val loss: 1.1092802286148071
Epoch 720, training loss: 0.015029694885015488 = 0.008052187971770763 + 0.001 * 6.977506637573242
Epoch 720, val loss: 1.1157563924789429
Epoch 730, training loss: 0.014699211344122887 = 0.007707442156970501 + 0.001 * 6.991769313812256
Epoch 730, val loss: 1.1220765113830566
Epoch 740, training loss: 0.014365902170538902 = 0.00738376472145319 + 0.001 * 6.9821367263793945
Epoch 740, val loss: 1.1282027959823608
Epoch 750, training loss: 0.014075715094804764 = 0.007075482048094273 + 0.001 * 7.000233173370361
Epoch 750, val loss: 1.1342355012893677
Epoch 760, training loss: 0.013765223324298859 = 0.006788282189518213 + 0.001 * 6.976940631866455
Epoch 760, val loss: 1.1401046514511108
Epoch 770, training loss: 0.013493002392351627 = 0.006518722511827946 + 0.001 * 6.974279403686523
Epoch 770, val loss: 1.1458379030227661
Epoch 780, training loss: 0.01323961652815342 = 0.0062606739811599255 + 0.001 * 6.97894287109375
Epoch 780, val loss: 1.151403784751892
Epoch 790, training loss: 0.012973366305232048 = 0.006020683329552412 + 0.001 * 6.952682018280029
Epoch 790, val loss: 1.1568361520767212
Epoch 800, training loss: 0.01276627741754055 = 0.005795932840555906 + 0.001 * 6.970344066619873
Epoch 800, val loss: 1.1621817350387573
Epoch 810, training loss: 0.012546874582767487 = 0.0055845193564891815 + 0.001 * 6.96235466003418
Epoch 810, val loss: 1.1674091815948486
Epoch 820, training loss: 0.012379450723528862 = 0.005384308286011219 + 0.001 * 6.995141983032227
Epoch 820, val loss: 1.172447919845581
Epoch 830, training loss: 0.012144806794822216 = 0.005194314755499363 + 0.001 * 6.950491905212402
Epoch 830, val loss: 1.1774063110351562
Epoch 840, training loss: 0.01199735514819622 = 0.005014257039874792 + 0.001 * 6.983097553253174
Epoch 840, val loss: 1.1822466850280762
Epoch 850, training loss: 0.011785009875893593 = 0.004844232928007841 + 0.001 * 6.94077730178833
Epoch 850, val loss: 1.186937928199768
Epoch 860, training loss: 0.011613754555583 = 0.004683820065110922 + 0.001 * 6.929934501647949
Epoch 860, val loss: 1.1915243864059448
Epoch 870, training loss: 0.011474896222352982 = 0.004532515071332455 + 0.001 * 6.942380428314209
Epoch 870, val loss: 1.1959989070892334
Epoch 880, training loss: 0.011356722563505173 = 0.0043891919776797295 + 0.001 * 6.967529773712158
Epoch 880, val loss: 1.2003968954086304
Epoch 890, training loss: 0.011180299334228039 = 0.004253143444657326 + 0.001 * 6.927155494689941
Epoch 890, val loss: 1.2047173976898193
Epoch 900, training loss: 0.011047102510929108 = 0.004123895429074764 + 0.001 * 6.923206329345703
Epoch 900, val loss: 1.208917260169983
Epoch 910, training loss: 0.010927589610219002 = 0.0040014395490288734 + 0.001 * 6.926149845123291
Epoch 910, val loss: 1.213032841682434
Epoch 920, training loss: 0.010812351480126381 = 0.0038852801080793142 + 0.001 * 6.927070617675781
Epoch 920, val loss: 1.2170612812042236
Epoch 930, training loss: 0.010715590789914131 = 0.0037749377079308033 + 0.001 * 6.940652370452881
Epoch 930, val loss: 1.2209941148757935
Epoch 940, training loss: 0.010592897422611713 = 0.003670068923383951 + 0.001 * 6.922828197479248
Epoch 940, val loss: 1.2248626947402954
Epoch 950, training loss: 0.010485077276825905 = 0.003570252563804388 + 0.001 * 6.914824962615967
Epoch 950, val loss: 1.2286412715911865
Epoch 960, training loss: 0.010387802496552467 = 0.003475232282653451 + 0.001 * 6.912569522857666
Epoch 960, val loss: 1.2323607206344604
Epoch 970, training loss: 0.010328906588256359 = 0.003384578973054886 + 0.001 * 6.944327354431152
Epoch 970, val loss: 1.235993504524231
Epoch 980, training loss: 0.01019277237355709 = 0.003298097522929311 + 0.001 * 6.894674301147461
Epoch 980, val loss: 1.2395610809326172
Epoch 990, training loss: 0.010140672326087952 = 0.003215185133740306 + 0.001 * 6.925487041473389
Epoch 990, val loss: 1.243071436882019
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7934
Flip ASR: 0.7600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.945398211479187 = 1.9370242357254028 + 0.001 * 8.373939514160156
Epoch 0, val loss: 1.923746943473816
Epoch 10, training loss: 1.9349087476730347 = 1.92653489112854 + 0.001 * 8.373908996582031
Epoch 10, val loss: 1.913909912109375
Epoch 20, training loss: 1.922000765800476 = 1.913627028465271 + 0.001 * 8.373788833618164
Epoch 20, val loss: 1.9014620780944824
Epoch 30, training loss: 1.9039589166641235 = 1.8955854177474976 + 0.001 * 8.373517036437988
Epoch 30, val loss: 1.883765459060669
Epoch 40, training loss: 1.8777201175689697 = 1.8693472146987915 + 0.001 * 8.37285327911377
Epoch 40, val loss: 1.858275294303894
Epoch 50, training loss: 1.8419928550720215 = 1.833621859550476 + 0.001 * 8.370960235595703
Epoch 50, val loss: 1.8253840208053589
Epoch 60, training loss: 1.8019115924835205 = 1.7935473918914795 + 0.001 * 8.36417293548584
Epoch 60, val loss: 1.7927930355072021
Epoch 70, training loss: 1.7615559101104736 = 1.75322425365448 + 0.001 * 8.33165454864502
Epoch 70, val loss: 1.7628302574157715
Epoch 80, training loss: 1.707553744316101 = 1.6994342803955078 + 0.001 * 8.119489669799805
Epoch 80, val loss: 1.7193548679351807
Epoch 90, training loss: 1.633750081062317 = 1.6258399486541748 + 0.001 * 7.910120010375977
Epoch 90, val loss: 1.657490849494934
Epoch 100, training loss: 1.5419397354125977 = 1.5340917110443115 + 0.001 * 7.84805965423584
Epoch 100, val loss: 1.582547903060913
Epoch 110, training loss: 1.4423226118087769 = 1.4345784187316895 + 0.001 * 7.744172096252441
Epoch 110, val loss: 1.504780650138855
Epoch 120, training loss: 1.3428912162780762 = 1.335329532623291 + 0.001 * 7.561640739440918
Epoch 120, val loss: 1.431535005569458
Epoch 130, training loss: 1.2446085214614868 = 1.237094521522522 + 0.001 * 7.514026165008545
Epoch 130, val loss: 1.3614425659179688
Epoch 140, training loss: 1.146321177482605 = 1.1388415098190308 + 0.001 * 7.479649066925049
Epoch 140, val loss: 1.2907265424728394
Epoch 150, training loss: 1.0482943058013916 = 1.0408580303192139 + 0.001 * 7.436246871948242
Epoch 150, val loss: 1.2186180353164673
Epoch 160, training loss: 0.9523611068725586 = 0.9449757933616638 + 0.001 * 7.385331630706787
Epoch 160, val loss: 1.1482417583465576
Epoch 170, training loss: 0.8619139790534973 = 0.854604959487915 + 0.001 * 7.309032440185547
Epoch 170, val loss: 1.0831730365753174
Epoch 180, training loss: 0.780735194683075 = 0.773525595664978 + 0.001 * 7.209589004516602
Epoch 180, val loss: 1.0272611379623413
Epoch 190, training loss: 0.7104522585868835 = 0.7033292055130005 + 0.001 * 7.123041152954102
Epoch 190, val loss: 0.9815480709075928
Epoch 200, training loss: 0.6494364738464355 = 0.6423544883728027 + 0.001 * 7.0819807052612305
Epoch 200, val loss: 0.9446078538894653
Epoch 210, training loss: 0.5947948098182678 = 0.587725043296814 + 0.001 * 7.069748878479004
Epoch 210, val loss: 0.9143635630607605
Epoch 220, training loss: 0.5442439317703247 = 0.5371760129928589 + 0.001 * 7.0678887367248535
Epoch 220, val loss: 0.8891979455947876
Epoch 230, training loss: 0.49661657214164734 = 0.489549458026886 + 0.001 * 7.067117691040039
Epoch 230, val loss: 0.8681336641311646
Epoch 240, training loss: 0.45151540637016296 = 0.4444490373134613 + 0.001 * 7.066362380981445
Epoch 240, val loss: 0.8513304591178894
Epoch 250, training loss: 0.4088605046272278 = 0.4017948806285858 + 0.001 * 7.065629959106445
Epoch 250, val loss: 0.839142382144928
Epoch 260, training loss: 0.3686380386352539 = 0.3615727126598358 + 0.001 * 7.065324306488037
Epoch 260, val loss: 0.8311169743537903
Epoch 270, training loss: 0.3309544622898102 = 0.32388895750045776 + 0.001 * 7.065493106842041
Epoch 270, val loss: 0.8268416523933411
Epoch 280, training loss: 0.2959443926811218 = 0.28887832164764404 + 0.001 * 7.066075325012207
Epoch 280, val loss: 0.8255224823951721
Epoch 290, training loss: 0.26373451948165894 = 0.2566675543785095 + 0.001 * 7.066954135894775
Epoch 290, val loss: 0.8267980813980103
Epoch 300, training loss: 0.23439425230026245 = 0.22732621431350708 + 0.001 * 7.068037509918213
Epoch 300, val loss: 0.8305234313011169
Epoch 310, training loss: 0.20800593495368958 = 0.20093661546707153 + 0.001 * 7.06931209564209
Epoch 310, val loss: 0.8366621136665344
Epoch 320, training loss: 0.18456076085567474 = 0.17748846113681793 + 0.001 * 7.072294235229492
Epoch 320, val loss: 0.8451918363571167
Epoch 330, training loss: 0.1639673411846161 = 0.15689441561698914 + 0.001 * 7.072926044464111
Epoch 330, val loss: 0.8559804558753967
Epoch 340, training loss: 0.14599677920341492 = 0.13892285525798798 + 0.001 * 7.073916435241699
Epoch 340, val loss: 0.8687744140625
Epoch 350, training loss: 0.13035531342029572 = 0.12328000366687775 + 0.001 * 7.07530403137207
Epoch 350, val loss: 0.8833451271057129
Epoch 360, training loss: 0.11673332005739212 = 0.10965662449598312 + 0.001 * 7.076693534851074
Epoch 360, val loss: 0.8992538452148438
Epoch 370, training loss: 0.10484194755554199 = 0.09776383638381958 + 0.001 * 7.078108310699463
Epoch 370, val loss: 0.9160906076431274
Epoch 380, training loss: 0.09443005919456482 = 0.08735060691833496 + 0.001 * 7.079455852508545
Epoch 380, val loss: 0.9335916638374329
Epoch 390, training loss: 0.08529240638017654 = 0.07821168750524521 + 0.001 * 7.080717086791992
Epoch 390, val loss: 0.9514555931091309
Epoch 400, training loss: 0.0772557407617569 = 0.07017385214567184 + 0.001 * 7.081884860992432
Epoch 400, val loss: 0.9694736003875732
Epoch 410, training loss: 0.07016797363758087 = 0.06308507174253464 + 0.001 * 7.082898139953613
Epoch 410, val loss: 0.987605094909668
Epoch 420, training loss: 0.06390170007944107 = 0.056816600263118744 + 0.001 * 7.085102558135986
Epoch 420, val loss: 1.0056922435760498
Epoch 430, training loss: 0.058353278785943985 = 0.051267046481370926 + 0.001 * 7.086230754852295
Epoch 430, val loss: 1.0235475301742554
Epoch 440, training loss: 0.05343440920114517 = 0.04634866118431091 + 0.001 * 7.085747718811035
Epoch 440, val loss: 1.0410616397857666
Epoch 450, training loss: 0.04907337948679924 = 0.04198727756738663 + 0.001 * 7.0861005783081055
Epoch 450, val loss: 1.0581713914871216
Epoch 460, training loss: 0.04520691558718681 = 0.03811975568532944 + 0.001 * 7.087158203125
Epoch 460, val loss: 1.074832797050476
Epoch 470, training loss: 0.04177578538656235 = 0.03468829020857811 + 0.001 * 7.087494373321533
Epoch 470, val loss: 1.090922474861145
Epoch 480, training loss: 0.038730572909116745 = 0.03164275363087654 + 0.001 * 7.087817668914795
Epoch 480, val loss: 1.1063827276229858
Epoch 490, training loss: 0.03602472320199013 = 0.028936805203557014 + 0.001 * 7.087917327880859
Epoch 490, val loss: 1.121213674545288
Epoch 500, training loss: 0.03361720219254494 = 0.02652960643172264 + 0.001 * 7.087594032287598
Epoch 500, val loss: 1.1354544162750244
Epoch 510, training loss: 0.03147236257791519 = 0.024384675547480583 + 0.001 * 7.0876874923706055
Epoch 510, val loss: 1.149078130722046
Epoch 520, training loss: 0.029565095901489258 = 0.022469136863946915 + 0.001 * 7.095958709716797
Epoch 520, val loss: 1.1621453762054443
Epoch 530, training loss: 0.02784283086657524 = 0.020754581317305565 + 0.001 * 7.088249206542969
Epoch 530, val loss: 1.174626350402832
Epoch 540, training loss: 0.026303954422473907 = 0.019217247143387794 + 0.001 * 7.086708068847656
Epoch 540, val loss: 1.1865662336349487
Epoch 550, training loss: 0.024921271950006485 = 0.017835672944784164 + 0.001 * 7.085599422454834
Epoch 550, val loss: 1.1980026960372925
Epoch 560, training loss: 0.02368139661848545 = 0.016591060906648636 + 0.001 * 7.090335845947266
Epoch 560, val loss: 1.2089298963546753
Epoch 570, training loss: 0.022553754970431328 = 0.015467528253793716 + 0.001 * 7.086226463317871
Epoch 570, val loss: 1.2194002866744995
Epoch 580, training loss: 0.02153397910296917 = 0.014450326561927795 + 0.001 * 7.083652019500732
Epoch 580, val loss: 1.2294341325759888
Epoch 590, training loss: 0.020616494119167328 = 0.013525660149753094 + 0.001 * 7.0908331871032715
Epoch 590, val loss: 1.2391250133514404
Epoch 600, training loss: 0.019764795899391174 = 0.012680004350841045 + 0.001 * 7.0847907066345215
Epoch 600, val loss: 1.2485930919647217
Epoch 610, training loss: 0.01898477040231228 = 0.01190274115651846 + 0.001 * 7.082029342651367
Epoch 610, val loss: 1.2579219341278076
Epoch 620, training loss: 0.01826649345457554 = 0.011186771094799042 + 0.001 * 7.0797224044799805
Epoch 620, val loss: 1.2671722173690796
Epoch 630, training loss: 0.017616840079426765 = 0.010526751168072224 + 0.001 * 7.090088367462158
Epoch 630, val loss: 1.2762905359268188
Epoch 640, training loss: 0.01699542999267578 = 0.009918290190398693 + 0.001 * 7.077139854431152
Epoch 640, val loss: 1.2852641344070435
Epoch 650, training loss: 0.016437839716672897 = 0.00935740303248167 + 0.001 * 7.080436706542969
Epoch 650, val loss: 1.2940444946289062
Epoch 660, training loss: 0.015911893919110298 = 0.008840215392410755 + 0.001 * 7.071678161621094
Epoch 660, val loss: 1.302647590637207
Epoch 670, training loss: 0.015442758798599243 = 0.008363096974790096 + 0.001 * 7.0796613693237305
Epoch 670, val loss: 1.31102454662323
Epoch 680, training loss: 0.014997182413935661 = 0.007922873832285404 + 0.001 * 7.0743088722229
Epoch 680, val loss: 1.3191704750061035
Epoch 690, training loss: 0.014590006321668625 = 0.007516218349337578 + 0.001 * 7.073787689208984
Epoch 690, val loss: 1.3270635604858398
Epoch 700, training loss: 0.01421277318149805 = 0.00714015681296587 + 0.001 * 7.072616100311279
Epoch 700, val loss: 1.3347113132476807
Epoch 710, training loss: 0.013850169256329536 = 0.006791955791413784 + 0.001 * 7.058213233947754
Epoch 710, val loss: 1.3420963287353516
Epoch 720, training loss: 0.0135333351790905 = 0.006469046231359243 + 0.001 * 7.064288139343262
Epoch 720, val loss: 1.349267601966858
Epoch 730, training loss: 0.013228613883256912 = 0.006169119384139776 + 0.001 * 7.059494495391846
Epoch 730, val loss: 1.35617995262146
Epoch 740, training loss: 0.012949815951287746 = 0.0058901323936879635 + 0.001 * 7.059683322906494
Epoch 740, val loss: 1.3628785610198975
Epoch 750, training loss: 0.01269178930670023 = 0.005630215629935265 + 0.001 * 7.061573505401611
Epoch 750, val loss: 1.3693331480026245
Epoch 760, training loss: 0.012458715587854385 = 0.005387726705521345 + 0.001 * 7.070988178253174
Epoch 760, val loss: 1.375576376914978
Epoch 770, training loss: 0.012206878513097763 = 0.00516117550432682 + 0.001 * 7.045702934265137
Epoch 770, val loss: 1.3816189765930176
Epoch 780, training loss: 0.011998248286545277 = 0.004949197638779879 + 0.001 * 7.049050331115723
Epoch 780, val loss: 1.3874599933624268
Epoch 790, training loss: 0.01177932322025299 = 0.004750602412968874 + 0.001 * 7.028720855712891
Epoch 790, val loss: 1.3931102752685547
Epoch 800, training loss: 0.011600075289607048 = 0.004564404021948576 + 0.001 * 7.035670757293701
Epoch 800, val loss: 1.39857816696167
Epoch 810, training loss: 0.011422649025917053 = 0.004389556124806404 + 0.001 * 7.033092021942139
Epoch 810, val loss: 1.4038562774658203
Epoch 820, training loss: 0.011257696896791458 = 0.004225193522870541 + 0.001 * 7.032503604888916
Epoch 820, val loss: 1.408964991569519
Epoch 830, training loss: 0.011084940284490585 = 0.004070477094501257 + 0.001 * 7.014462947845459
Epoch 830, val loss: 1.4139032363891602
Epoch 840, training loss: 0.011022842489182949 = 0.003924687393009663 + 0.001 * 7.098154544830322
Epoch 840, val loss: 1.41869056224823
Epoch 850, training loss: 0.010801313444972038 = 0.0037871659733355045 + 0.001 * 7.014147758483887
Epoch 850, val loss: 1.423313856124878
Epoch 860, training loss: 0.010651234537363052 = 0.003657306544482708 + 0.001 * 6.993927001953125
Epoch 860, val loss: 1.4278051853179932
Epoch 870, training loss: 0.010557667352259159 = 0.0035345423966646194 + 0.001 * 7.023124694824219
Epoch 870, val loss: 1.4321460723876953
Epoch 880, training loss: 0.010432284325361252 = 0.003418218344449997 + 0.001 * 7.014065265655518
Epoch 880, val loss: 1.4363906383514404
Epoch 890, training loss: 0.010329623706638813 = 0.003308027284219861 + 0.001 * 7.021596431732178
Epoch 890, val loss: 1.4404563903808594
Epoch 900, training loss: 0.010202419944107533 = 0.00320331915281713 + 0.001 * 6.999100685119629
Epoch 900, val loss: 1.444449543952942
Epoch 910, training loss: 0.010081295855343342 = 0.0031038778834044933 + 0.001 * 6.977417469024658
Epoch 910, val loss: 1.4483568668365479
Epoch 920, training loss: 0.010014774277806282 = 0.0030092366505414248 + 0.001 * 7.005537509918213
Epoch 920, val loss: 1.4521234035491943
Epoch 930, training loss: 0.009930072352290154 = 0.0029192501679062843 + 0.001 * 7.010822296142578
Epoch 930, val loss: 1.4558193683624268
Epoch 940, training loss: 0.009795836172997952 = 0.002833643462508917 + 0.001 * 6.962192535400391
Epoch 940, val loss: 1.4593727588653564
Epoch 950, training loss: 0.009800253435969353 = 0.002751814667135477 + 0.001 * 7.048438549041748
Epoch 950, val loss: 1.4628726243972778
Epoch 960, training loss: 0.009663667529821396 = 0.002673899522051215 + 0.001 * 6.989768028259277
Epoch 960, val loss: 1.466286063194275
Epoch 970, training loss: 0.009579714387655258 = 0.0025995317846536636 + 0.001 * 6.98018217086792
Epoch 970, val loss: 1.4695736169815063
Epoch 980, training loss: 0.00949688907712698 = 0.002528465585783124 + 0.001 * 6.968422889709473
Epoch 980, val loss: 1.4727845191955566
Epoch 990, training loss: 0.009451648220419884 = 0.0024604909121990204 + 0.001 * 6.991157531738281
Epoch 990, val loss: 1.4759557247161865
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7860
Flip ASR: 0.7422/225 nodes
The final ASR:0.71833, 0.10094, Accuracy:0.79136, 0.01259
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11566])
remove edge: torch.Size([2, 9448])
updated graph: torch.Size([2, 10458])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9538898468017578 = 1.9455159902572632 + 0.001 * 8.373834609985352
Epoch 0, val loss: 1.9469841718673706
Epoch 10, training loss: 1.9440577030181885 = 1.9356839656829834 + 0.001 * 8.373760223388672
Epoch 10, val loss: 1.9376713037490845
Epoch 20, training loss: 1.93238365650177 = 1.924010157585144 + 0.001 * 8.373516082763672
Epoch 20, val loss: 1.9262620210647583
Epoch 30, training loss: 1.916176676750183 = 1.9078036546707153 + 0.001 * 8.373026847839355
Epoch 30, val loss: 1.9101831912994385
Epoch 40, training loss: 1.8921167850494385 = 1.8837448358535767 + 0.001 * 8.371949195861816
Epoch 40, val loss: 1.886383295059204
Epoch 50, training loss: 1.8571056127548218 = 1.8487366437911987 + 0.001 * 8.368971824645996
Epoch 50, val loss: 1.852826714515686
Epoch 60, training loss: 1.8139824867248535 = 1.8056254386901855 + 0.001 * 8.357023239135742
Epoch 60, val loss: 1.8145296573638916
Epoch 70, training loss: 1.7726372480392456 = 1.7643526792526245 + 0.001 * 8.284584999084473
Epoch 70, val loss: 1.780500888824463
Epoch 80, training loss: 1.7239327430725098 = 1.7160712480545044 + 0.001 * 7.861517906188965
Epoch 80, val loss: 1.737358570098877
Epoch 90, training loss: 1.6553865671157837 = 1.6477621793746948 + 0.001 * 7.624408721923828
Epoch 90, val loss: 1.677419900894165
Epoch 100, training loss: 1.5631083250045776 = 1.5556293725967407 + 0.001 * 7.478977680206299
Epoch 100, val loss: 1.6001672744750977
Epoch 110, training loss: 1.4528993368148804 = 1.4455965757369995 + 0.001 * 7.3028130531311035
Epoch 110, val loss: 1.5084784030914307
Epoch 120, training loss: 1.3374658823013306 = 1.330284833908081 + 0.001 * 7.180992603302002
Epoch 120, val loss: 1.414177417755127
Epoch 130, training loss: 1.2268177270889282 = 1.219759464263916 + 0.001 * 7.058241844177246
Epoch 130, val loss: 1.3261938095092773
Epoch 140, training loss: 1.1263436079025269 = 1.1193912029266357 + 0.001 * 6.952409267425537
Epoch 140, val loss: 1.2491742372512817
Epoch 150, training loss: 1.0378172397613525 = 1.0309096574783325 + 0.001 * 6.907585620880127
Epoch 150, val loss: 1.18386971950531
Epoch 160, training loss: 0.959428071975708 = 0.9525313973426819 + 0.001 * 6.896676063537598
Epoch 160, val loss: 1.1282583475112915
Epoch 170, training loss: 0.8878428936004639 = 0.8809546232223511 + 0.001 * 6.888288974761963
Epoch 170, val loss: 1.079017996788025
Epoch 180, training loss: 0.8197460770606995 = 0.8128633499145508 + 0.001 * 6.882723808288574
Epoch 180, val loss: 1.0337103605270386
Epoch 190, training loss: 0.7531210780143738 = 0.7462431788444519 + 0.001 * 6.877909183502197
Epoch 190, val loss: 0.9906299710273743
Epoch 200, training loss: 0.6879093050956726 = 0.6810359954833984 + 0.001 * 6.873337745666504
Epoch 200, val loss: 0.9495199918746948
Epoch 210, training loss: 0.6257482171058655 = 0.6188794374465942 + 0.001 * 6.868780136108398
Epoch 210, val loss: 0.9115370512008667
Epoch 220, training loss: 0.5689594149589539 = 0.5620957612991333 + 0.001 * 6.863663673400879
Epoch 220, val loss: 0.8790328502655029
Epoch 230, training loss: 0.5191776752471924 = 0.5123189687728882 + 0.001 * 6.858719348907471
Epoch 230, val loss: 0.8535082340240479
Epoch 240, training loss: 0.4763716161251068 = 0.46952003240585327 + 0.001 * 6.851573467254639
Epoch 240, val loss: 0.8349824547767639
Epoch 250, training loss: 0.43899303674697876 = 0.43214914202690125 + 0.001 * 6.843883514404297
Epoch 250, val loss: 0.8220950365066528
Epoch 260, training loss: 0.40485382080078125 = 0.3980182707309723 + 0.001 * 6.835562705993652
Epoch 260, val loss: 0.8126499652862549
Epoch 270, training loss: 0.3719373345375061 = 0.36511093378067017 + 0.001 * 6.82638692855835
Epoch 270, val loss: 0.8048918843269348
Epoch 280, training loss: 0.338871568441391 = 0.3320547938346863 + 0.001 * 6.816780090332031
Epoch 280, val loss: 0.7975524067878723
Epoch 290, training loss: 0.30520012974739075 = 0.29839465022087097 + 0.001 * 6.80548620223999
Epoch 290, val loss: 0.7901886701583862
Epoch 300, training loss: 0.2713887393474579 = 0.26459333300590515 + 0.001 * 6.795393943786621
Epoch 300, val loss: 0.7831811308860779
Epoch 310, training loss: 0.23845671117305756 = 0.23167431354522705 + 0.001 * 6.7823991775512695
Epoch 310, val loss: 0.7774621248245239
Epoch 320, training loss: 0.20764070749282837 = 0.20086784660816193 + 0.001 * 6.772862434387207
Epoch 320, val loss: 0.7739384174346924
Epoch 330, training loss: 0.17999263107776642 = 0.17322854697704315 + 0.001 * 6.7640814781188965
Epoch 330, val loss: 0.773679256439209
Epoch 340, training loss: 0.15614256262779236 = 0.14938722550868988 + 0.001 * 6.755341529846191
Epoch 340, val loss: 0.777166485786438
Epoch 350, training loss: 0.1360531598329544 = 0.12930439412593842 + 0.001 * 6.748770236968994
Epoch 350, val loss: 0.7844312787055969
Epoch 360, training loss: 0.11926238238811493 = 0.11251353472471237 + 0.001 * 6.748849391937256
Epoch 360, val loss: 0.7948114275932312
Epoch 370, training loss: 0.10518433153629303 = 0.09843926131725311 + 0.001 * 6.745067596435547
Epoch 370, val loss: 0.8075284957885742
Epoch 380, training loss: 0.09329761564731598 = 0.08655473589897156 + 0.001 * 6.74287748336792
Epoch 380, val loss: 0.8216092586517334
Epoch 390, training loss: 0.0831647589802742 = 0.07642308622598648 + 0.001 * 6.741671562194824
Epoch 390, val loss: 0.8364380598068237
Epoch 400, training loss: 0.07446102052927017 = 0.06771960109472275 + 0.001 * 6.741419792175293
Epoch 400, val loss: 0.851780891418457
Epoch 410, training loss: 0.06694934517145157 = 0.06020237132906914 + 0.001 * 6.746972560882568
Epoch 410, val loss: 0.8673329949378967
Epoch 420, training loss: 0.06042536720633507 = 0.05368591845035553 + 0.001 * 6.739447593688965
Epoch 420, val loss: 0.8829286694526672
Epoch 430, training loss: 0.05476014316082001 = 0.0480230487883091 + 0.001 * 6.737094879150391
Epoch 430, val loss: 0.8983102440834045
Epoch 440, training loss: 0.04982839524745941 = 0.04309108108282089 + 0.001 * 6.737312316894531
Epoch 440, val loss: 0.9135015606880188
Epoch 450, training loss: 0.04552723467350006 = 0.03878621384501457 + 0.001 * 6.741022109985352
Epoch 450, val loss: 0.9284036159515381
Epoch 460, training loss: 0.04175800085067749 = 0.035021912306547165 + 0.001 * 6.73608922958374
Epoch 460, val loss: 0.942993700504303
Epoch 470, training loss: 0.038455575704574585 = 0.03172340616583824 + 0.001 * 6.732171058654785
Epoch 470, val loss: 0.9571844935417175
Epoch 480, training loss: 0.035558976233005524 = 0.028826506808400154 + 0.001 * 6.732468128204346
Epoch 480, val loss: 0.9710644483566284
Epoch 490, training loss: 0.033010389655828476 = 0.026275932788848877 + 0.001 * 6.734457969665527
Epoch 490, val loss: 0.984484851360321
Epoch 500, training loss: 0.030756982043385506 = 0.024024059996008873 + 0.001 * 6.732921600341797
Epoch 500, val loss: 0.9975719451904297
Epoch 510, training loss: 0.02875957079231739 = 0.022030595690011978 + 0.001 * 6.728974342346191
Epoch 510, val loss: 1.0102776288986206
Epoch 520, training loss: 0.02699655294418335 = 0.020261047407984734 + 0.001 * 6.735505104064941
Epoch 520, val loss: 1.0225127935409546
Epoch 530, training loss: 0.025415578857064247 = 0.018685882911086082 + 0.001 * 6.729695796966553
Epoch 530, val loss: 1.0343949794769287
Epoch 540, training loss: 0.024004554376006126 = 0.017280058935284615 + 0.001 * 6.7244954109191895
Epoch 540, val loss: 1.0459039211273193
Epoch 550, training loss: 0.0227515809237957 = 0.016021931543946266 + 0.001 * 6.729649066925049
Epoch 550, val loss: 1.0570149421691895
Epoch 560, training loss: 0.02162005566060543 = 0.01489269733428955 + 0.001 * 6.727357387542725
Epoch 560, val loss: 1.0677462816238403
Epoch 570, training loss: 0.020595161244273186 = 0.013876456767320633 + 0.001 * 6.7187042236328125
Epoch 570, val loss: 1.0781506299972534
Epoch 580, training loss: 0.019682276993989944 = 0.012959406711161137 + 0.001 * 6.722868919372559
Epoch 580, val loss: 1.088201642036438
Epoch 590, training loss: 0.018847206607460976 = 0.0121297687292099 + 0.001 * 6.717437744140625
Epoch 590, val loss: 1.0979725122451782
Epoch 600, training loss: 0.018103931099176407 = 0.01137729361653328 + 0.001 * 6.726637840270996
Epoch 600, val loss: 1.107414722442627
Epoch 610, training loss: 0.017410291358828545 = 0.010693068616092205 + 0.001 * 6.717222213745117
Epoch 610, val loss: 1.1165657043457031
Epoch 620, training loss: 0.016784468665719032 = 0.01006949320435524 + 0.001 * 6.714975833892822
Epoch 620, val loss: 1.1254205703735352
Epoch 630, training loss: 0.016212966293096542 = 0.009499800391495228 + 0.001 * 6.713165283203125
Epoch 630, val loss: 1.1340333223342896
Epoch 640, training loss: 0.01569119095802307 = 0.008978191763162613 + 0.001 * 6.71299934387207
Epoch 640, val loss: 1.1423990726470947
Epoch 650, training loss: 0.015210776589810848 = 0.00849952083081007 + 0.001 * 6.7112555503845215
Epoch 650, val loss: 1.1504634618759155
Epoch 660, training loss: 0.014763823710381985 = 0.008059360086917877 + 0.001 * 6.704463481903076
Epoch 660, val loss: 1.158367395401001
Epoch 670, training loss: 0.01436031237244606 = 0.007653769105672836 + 0.001 * 6.706542491912842
Epoch 670, val loss: 1.1660035848617554
Epoch 680, training loss: 0.013980984687805176 = 0.007279318757355213 + 0.001 * 6.701664924621582
Epoch 680, val loss: 1.173458456993103
Epoch 690, training loss: 0.013636462390422821 = 0.006932978052645922 + 0.001 * 6.703483581542969
Epoch 690, val loss: 1.1806983947753906
Epoch 700, training loss: 0.013311042450368404 = 0.006612044759094715 + 0.001 * 6.698997497558594
Epoch 700, val loss: 1.187760353088379
Epoch 710, training loss: 0.01301257498562336 = 0.0063141342252492905 + 0.001 * 6.698440074920654
Epoch 710, val loss: 1.1946065425872803
Epoch 720, training loss: 0.012747508473694324 = 0.0060371155850589275 + 0.001 * 6.710392475128174
Epoch 720, val loss: 1.2012842893600464
Epoch 730, training loss: 0.012484097853302956 = 0.005779059138149023 + 0.001 * 6.705038070678711
Epoch 730, val loss: 1.207802653312683
Epoch 740, training loss: 0.012239913456141949 = 0.005538306664675474 + 0.001 * 6.701606273651123
Epoch 740, val loss: 1.2141491174697876
Epoch 750, training loss: 0.012008061632514 = 0.005313364323228598 + 0.001 * 6.694697380065918
Epoch 750, val loss: 1.2203521728515625
Epoch 760, training loss: 0.01179824024438858 = 0.005102834664285183 + 0.001 * 6.695405960083008
Epoch 760, val loss: 1.226403832435608
Epoch 770, training loss: 0.011594281531870365 = 0.004905505105853081 + 0.001 * 6.688776016235352
Epoch 770, val loss: 1.232292652130127
Epoch 780, training loss: 0.011413823813199997 = 0.0047201961278915405 + 0.001 * 6.693627834320068
Epoch 780, val loss: 1.2380475997924805
Epoch 790, training loss: 0.011236203834414482 = 0.004545569885522127 + 0.001 * 6.690633296966553
Epoch 790, val loss: 1.2437008619308472
Epoch 800, training loss: 0.011081378906965256 = 0.004380690865218639 + 0.001 * 6.700688362121582
Epoch 800, val loss: 1.2492046356201172
Epoch 810, training loss: 0.010916009545326233 = 0.004224531818181276 + 0.001 * 6.6914777755737305
Epoch 810, val loss: 1.2545857429504395
Epoch 820, training loss: 0.010778410360217094 = 0.00407616188749671 + 0.001 * 6.7022480964660645
Epoch 820, val loss: 1.2598875761032104
Epoch 830, training loss: 0.010626992210745811 = 0.003934779670089483 + 0.001 * 6.692211627960205
Epoch 830, val loss: 1.2650948762893677
Epoch 840, training loss: 0.010480941273272038 = 0.003799814498052001 + 0.001 * 6.681126117706299
Epoch 840, val loss: 1.2702019214630127
Epoch 850, training loss: 0.010368434712290764 = 0.003670772770419717 + 0.001 * 6.697661876678467
Epoch 850, val loss: 1.2752043008804321
Epoch 860, training loss: 0.010224292054772377 = 0.0035474703181535006 + 0.001 * 6.676821231842041
Epoch 860, val loss: 1.280109167098999
Epoch 870, training loss: 0.0101171238347888 = 0.003429608652368188 + 0.001 * 6.6875152587890625
Epoch 870, val loss: 1.2849184274673462
Epoch 880, training loss: 0.009997250512242317 = 0.0033169290982186794 + 0.001 * 6.680320739746094
Epoch 880, val loss: 1.2896617650985718
Epoch 890, training loss: 0.00989086925983429 = 0.0032092805486172438 + 0.001 * 6.681588649749756
Epoch 890, val loss: 1.29427969455719
Epoch 900, training loss: 0.009801107458770275 = 0.0031065361108630896 + 0.001 * 6.694571495056152
Epoch 900, val loss: 1.2988176345825195
Epoch 910, training loss: 0.009680097922682762 = 0.003008506027981639 + 0.001 * 6.671591281890869
Epoch 910, val loss: 1.3032745122909546
Epoch 920, training loss: 0.009586085565388203 = 0.0029149840120226145 + 0.001 * 6.671101093292236
Epoch 920, val loss: 1.307641863822937
Epoch 930, training loss: 0.009520663879811764 = 0.002825773786753416 + 0.001 * 6.694889545440674
Epoch 930, val loss: 1.3119244575500488
Epoch 940, training loss: 0.009411141276359558 = 0.00274073239415884 + 0.001 * 6.670408725738525
Epoch 940, val loss: 1.3161252737045288
Epoch 950, training loss: 0.009344900026917458 = 0.0026596595998853445 + 0.001 * 6.685240268707275
Epoch 950, val loss: 1.3202234506607056
Epoch 960, training loss: 0.009248706512153149 = 0.0025823700707405806 + 0.001 * 6.666336536407471
Epoch 960, val loss: 1.3242632150650024
Epoch 970, training loss: 0.009182089939713478 = 0.0025086705572903156 + 0.001 * 6.6734185218811035
Epoch 970, val loss: 1.3282127380371094
Epoch 980, training loss: 0.009110098704695702 = 0.0024383519776165485 + 0.001 * 6.671745777130127
Epoch 980, val loss: 1.3320733308792114
Epoch 990, training loss: 0.009058812633156776 = 0.0023712438996881247 + 0.001 * 6.687568187713623
Epoch 990, val loss: 1.3358615636825562
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6827
Flip ASR: 0.6311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9408206939697266 = 1.932446837425232 + 0.001 * 8.373801231384277
Epoch 0, val loss: 1.9405910968780518
Epoch 10, training loss: 1.9315948486328125 = 1.9232211112976074 + 0.001 * 8.373708724975586
Epoch 10, val loss: 1.930475115776062
Epoch 20, training loss: 1.9202312231063843 = 1.9118578433990479 + 0.001 * 8.373408317565918
Epoch 20, val loss: 1.9176839590072632
Epoch 30, training loss: 1.904323935508728 = 1.8959511518478394 + 0.001 * 8.372746467590332
Epoch 30, val loss: 1.89961838722229
Epoch 40, training loss: 1.8811168670654297 = 1.8727456331253052 + 0.001 * 8.371191024780273
Epoch 40, val loss: 1.8736146688461304
Epoch 50, training loss: 1.8488948345184326 = 1.8405280113220215 + 0.001 * 8.366854667663574
Epoch 50, val loss: 1.8387500047683716
Epoch 60, training loss: 1.8110164403915405 = 1.8026652336120605 + 0.001 * 8.351162910461426
Epoch 60, val loss: 1.8006969690322876
Epoch 70, training loss: 1.7743659019470215 = 1.7660895586013794 + 0.001 * 8.276379585266113
Epoch 70, val loss: 1.7668266296386719
Epoch 80, training loss: 1.7296704053878784 = 1.7217360734939575 + 0.001 * 7.93432092666626
Epoch 80, val loss: 1.7270160913467407
Epoch 90, training loss: 1.6662782430648804 = 1.658553957939148 + 0.001 * 7.724332332611084
Epoch 90, val loss: 1.6727933883666992
Epoch 100, training loss: 1.5813825130462646 = 1.5737918615341187 + 0.001 * 7.590646743774414
Epoch 100, val loss: 1.6017969846725464
Epoch 110, training loss: 1.4795575141906738 = 1.4721351861953735 + 0.001 * 7.422300815582275
Epoch 110, val loss: 1.5193332433700562
Epoch 120, training loss: 1.3721625804901123 = 1.3648675680160522 + 0.001 * 7.295061111450195
Epoch 120, val loss: 1.434989094734192
Epoch 130, training loss: 1.2672921419143677 = 1.260007381439209 + 0.001 * 7.284787178039551
Epoch 130, val loss: 1.3562772274017334
Epoch 140, training loss: 1.1684561967849731 = 1.161205530166626 + 0.001 * 7.250720024108887
Epoch 140, val loss: 1.2839242219924927
Epoch 150, training loss: 1.0780755281448364 = 1.0708554983139038 + 0.001 * 7.22004508972168
Epoch 150, val loss: 1.2182097434997559
Epoch 160, training loss: 0.9968695044517517 = 0.9897012710571289 + 0.001 * 7.168231010437012
Epoch 160, val loss: 1.1592929363250732
Epoch 170, training loss: 0.9233585000038147 = 0.9162745475769043 + 0.001 * 7.083922863006592
Epoch 170, val loss: 1.1066491603851318
Epoch 180, training loss: 0.8542562127113342 = 0.8472563028335571 + 0.001 * 6.999889850616455
Epoch 180, val loss: 1.0584181547164917
Epoch 190, training loss: 0.7868773341178894 = 0.7799105644226074 + 0.001 * 6.966779708862305
Epoch 190, val loss: 1.0128471851348877
Epoch 200, training loss: 0.7209466695785522 = 0.714009702205658 + 0.001 * 6.936960697174072
Epoch 200, val loss: 0.9706350564956665
Epoch 210, training loss: 0.658085286617279 = 0.6511687636375427 + 0.001 * 6.916521072387695
Epoch 210, val loss: 0.9334192872047424
Epoch 220, training loss: 0.5998210906982422 = 0.5929235219955444 + 0.001 * 6.897545337677002
Epoch 220, val loss: 0.9025644659996033
Epoch 230, training loss: 0.5463465452194214 = 0.539463222026825 + 0.001 * 6.883302688598633
Epoch 230, val loss: 0.878311276435852
Epoch 240, training loss: 0.4970467984676361 = 0.4901711642742157 + 0.001 * 6.8756327629089355
Epoch 240, val loss: 0.860331118106842
Epoch 250, training loss: 0.4510801136493683 = 0.44421133399009705 + 0.001 * 6.868780136108398
Epoch 250, val loss: 0.8470191955566406
Epoch 260, training loss: 0.40777769684791565 = 0.4009123146533966 + 0.001 * 6.865375518798828
Epoch 260, val loss: 0.8374145030975342
Epoch 270, training loss: 0.36679595708847046 = 0.35993391275405884 + 0.001 * 6.862049102783203
Epoch 270, val loss: 0.8305429816246033
Epoch 280, training loss: 0.3281208872795105 = 0.32126185297966003 + 0.001 * 6.859021186828613
Epoch 280, val loss: 0.8266036510467529
Epoch 290, training loss: 0.29202693700790405 = 0.28517091274261475 + 0.001 * 6.856029987335205
Epoch 290, val loss: 0.8258366584777832
Epoch 300, training loss: 0.25894200801849365 = 0.2520889937877655 + 0.001 * 6.852999210357666
Epoch 300, val loss: 0.8290002346038818
Epoch 310, training loss: 0.22926922142505646 = 0.22241924703121185 + 0.001 * 6.849979877471924
Epoch 310, val loss: 0.8356730341911316
Epoch 320, training loss: 0.20277543365955353 = 0.1959291249513626 + 0.001 * 6.846307754516602
Epoch 320, val loss: 0.8454110622406006
Epoch 330, training loss: 0.17938272655010223 = 0.17253650724887848 + 0.001 * 6.846215724945068
Epoch 330, val loss: 0.8582025170326233
Epoch 340, training loss: 0.15882593393325806 = 0.1519853174686432 + 0.001 * 6.840621471405029
Epoch 340, val loss: 0.8731803894042969
Epoch 350, training loss: 0.14086638391017914 = 0.1340298354625702 + 0.001 * 6.836548805236816
Epoch 350, val loss: 0.8900378942489624
Epoch 360, training loss: 0.12546788156032562 = 0.11863026767969131 + 0.001 * 6.837611198425293
Epoch 360, val loss: 0.9081495404243469
Epoch 370, training loss: 0.11242818087339401 = 0.10559653490781784 + 0.001 * 6.831647872924805
Epoch 370, val loss: 0.9269418120384216
Epoch 380, training loss: 0.10139097273349762 = 0.09456567466259003 + 0.001 * 6.825300216674805
Epoch 380, val loss: 0.9462779760360718
Epoch 390, training loss: 0.09195101261138916 = 0.08512996137142181 + 0.001 * 6.821053981781006
Epoch 390, val loss: 0.9658195376396179
Epoch 400, training loss: 0.0838017463684082 = 0.07698024809360504 + 0.001 * 6.8215012550354
Epoch 400, val loss: 0.9854151010513306
Epoch 410, training loss: 0.07668454945087433 = 0.06986963748931885 + 0.001 * 6.814910888671875
Epoch 410, val loss: 1.0049948692321777
Epoch 420, training loss: 0.0704072043299675 = 0.06359695643186569 + 0.001 * 6.810250759124756
Epoch 420, val loss: 1.0244956016540527
Epoch 430, training loss: 0.06482796370983124 = 0.05801144242286682 + 0.001 * 6.8165178298950195
Epoch 430, val loss: 1.0439952611923218
Epoch 440, training loss: 0.05979178845882416 = 0.0529845766723156 + 0.001 * 6.8072099685668945
Epoch 440, val loss: 1.0634771585464478
Epoch 450, training loss: 0.05522707477211952 = 0.04842513054609299 + 0.001 * 6.801942348480225
Epoch 450, val loss: 1.0828986167907715
Epoch 460, training loss: 0.0510660819709301 = 0.04426807910203934 + 0.001 * 6.79800271987915
Epoch 460, val loss: 1.1024224758148193
Epoch 470, training loss: 0.047256626188755035 = 0.040458712726831436 + 0.001 * 6.797913074493408
Epoch 470, val loss: 1.121751070022583
Epoch 480, training loss: 0.043738894164562225 = 0.03694795072078705 + 0.001 * 6.790943145751953
Epoch 480, val loss: 1.1411137580871582
Epoch 490, training loss: 0.04050092771649361 = 0.03371589630842209 + 0.001 * 6.785031795501709
Epoch 490, val loss: 1.1604974269866943
Epoch 500, training loss: 0.03753798082470894 = 0.030757058411836624 + 0.001 * 6.780923366546631
Epoch 500, val loss: 1.1801196336746216
Epoch 510, training loss: 0.03485956788063049 = 0.028080692514777184 + 0.001 * 6.77887487411499
Epoch 510, val loss: 1.1995282173156738
Epoch 520, training loss: 0.0324568934738636 = 0.02568363957107067 + 0.001 * 6.77325439453125
Epoch 520, val loss: 1.2189337015151978
Epoch 530, training loss: 0.030325038358569145 = 0.023546498268842697 + 0.001 * 6.778540134429932
Epoch 530, val loss: 1.237926721572876
Epoch 540, training loss: 0.028409814462065697 = 0.021645406261086464 + 0.001 * 6.764408111572266
Epoch 540, val loss: 1.2568954229354858
Epoch 550, training loss: 0.02670905739068985 = 0.01994990184903145 + 0.001 * 6.759154319763184
Epoch 550, val loss: 1.2752721309661865
Epoch 560, training loss: 0.0252180527895689 = 0.018432971090078354 + 0.001 * 6.785081386566162
Epoch 560, val loss: 1.2934114933013916
Epoch 570, training loss: 0.023835577070713043 = 0.01707109808921814 + 0.001 * 6.7644782066345215
Epoch 570, val loss: 1.3110871315002441
Epoch 580, training loss: 0.02259967103600502 = 0.01584731601178646 + 0.001 * 6.752355575561523
Epoch 580, val loss: 1.3284509181976318
Epoch 590, training loss: 0.021493742242455482 = 0.014744884334504604 + 0.001 * 6.748857021331787
Epoch 590, val loss: 1.3453630208969116
Epoch 600, training loss: 0.020493565127253532 = 0.013749842531979084 + 0.001 * 6.743722438812256
Epoch 600, val loss: 1.361932396888733
Epoch 610, training loss: 0.019604159519076347 = 0.012849514372646809 + 0.001 * 6.754644393920898
Epoch 610, val loss: 1.3781098127365112
Epoch 620, training loss: 0.01877707988023758 = 0.012033185921609402 + 0.001 * 6.743892669677734
Epoch 620, val loss: 1.393891453742981
Epoch 630, training loss: 0.01802636869251728 = 0.01129007339477539 + 0.001 * 6.736294746398926
Epoch 630, val loss: 1.4093528985977173
Epoch 640, training loss: 0.017347268760204315 = 0.010608505457639694 + 0.001 * 6.738761901855469
Epoch 640, val loss: 1.4244245290756226
Epoch 650, training loss: 0.016718793660402298 = 0.009985270909965038 + 0.001 * 6.733522415161133
Epoch 650, val loss: 1.4393686056137085
Epoch 660, training loss: 0.016145583242177963 = 0.009416227228939533 + 0.001 * 6.729356288909912
Epoch 660, val loss: 1.453894019126892
Epoch 670, training loss: 0.015629025176167488 = 0.008894587866961956 + 0.001 * 6.734436988830566
Epoch 670, val loss: 1.4679948091506958
Epoch 680, training loss: 0.015140500850975513 = 0.008415400050580502 + 0.001 * 6.725100517272949
Epoch 680, val loss: 1.4818120002746582
Epoch 690, training loss: 0.014710063114762306 = 0.00797494687139988 + 0.001 * 6.735116481781006
Epoch 690, val loss: 1.4953070878982544
Epoch 700, training loss: 0.014291249215602875 = 0.0075691775418818 + 0.001 * 6.722070693969727
Epoch 700, val loss: 1.5084458589553833
Epoch 710, training loss: 0.013947837054729462 = 0.00719487713649869 + 0.001 * 6.752959728240967
Epoch 710, val loss: 1.5213475227355957
Epoch 720, training loss: 0.013581417500972748 = 0.006849075201898813 + 0.001 * 6.732341289520264
Epoch 720, val loss: 1.5338937044143677
Epoch 730, training loss: 0.013248968869447708 = 0.006528859026730061 + 0.001 * 6.720109939575195
Epoch 730, val loss: 1.546210765838623
Epoch 740, training loss: 0.012948310002684593 = 0.00623177969828248 + 0.001 * 6.716529846191406
Epoch 740, val loss: 1.5582870244979858
Epoch 750, training loss: 0.012683735229074955 = 0.005956002976745367 + 0.001 * 6.727731704711914
Epoch 750, val loss: 1.5700727701187134
Epoch 760, training loss: 0.01241392083466053 = 0.005699423607438803 + 0.001 * 6.714496612548828
Epoch 760, val loss: 1.581644892692566
Epoch 770, training loss: 0.012172604911029339 = 0.005460208747535944 + 0.001 * 6.712395668029785
Epoch 770, val loss: 1.5928853750228882
Epoch 780, training loss: 0.011948752216994762 = 0.005236966535449028 + 0.001 * 6.711785316467285
Epoch 780, val loss: 1.6039613485336304
Epoch 790, training loss: 0.011737204156816006 = 0.005028314422816038 + 0.001 * 6.708889484405518
Epoch 790, val loss: 1.6147998571395874
Epoch 800, training loss: 0.011556202545762062 = 0.004832916427403688 + 0.001 * 6.72328519821167
Epoch 800, val loss: 1.6254078149795532
Epoch 810, training loss: 0.011367404833436012 = 0.004649714566767216 + 0.001 * 6.7176899909973145
Epoch 810, val loss: 1.6358141899108887
Epoch 820, training loss: 0.011182134971022606 = 0.004477718845009804 + 0.001 * 6.704416275024414
Epoch 820, val loss: 1.6460069417953491
Epoch 830, training loss: 0.011028322391211987 = 0.004316070582717657 + 0.001 * 6.712251663208008
Epoch 830, val loss: 1.656005859375
Epoch 840, training loss: 0.010867173783481121 = 0.004163946956396103 + 0.001 * 6.703226566314697
Epoch 840, val loss: 1.6658276319503784
Epoch 850, training loss: 0.010731235146522522 = 0.004020496271550655 + 0.001 * 6.7107391357421875
Epoch 850, val loss: 1.6754658222198486
Epoch 860, training loss: 0.010588089935481548 = 0.0038850102573633194 + 0.001 * 6.7030792236328125
Epoch 860, val loss: 1.6848852634429932
Epoch 870, training loss: 0.010462774895131588 = 0.0037570942658931017 + 0.001 * 6.705679893493652
Epoch 870, val loss: 1.694218635559082
Epoch 880, training loss: 0.0103513915091753 = 0.0036361534148454666 + 0.001 * 6.715238094329834
Epoch 880, val loss: 1.7033575773239136
Epoch 890, training loss: 0.01022619754076004 = 0.00352175859734416 + 0.001 * 6.704438209533691
Epoch 890, val loss: 1.712310552597046
Epoch 900, training loss: 0.01011455524712801 = 0.0034134776797145605 + 0.001 * 6.701076984405518
Epoch 900, val loss: 1.7211170196533203
Epoch 910, training loss: 0.010008003562688828 = 0.003310946747660637 + 0.001 * 6.697056770324707
Epoch 910, val loss: 1.729775071144104
Epoch 920, training loss: 0.009918369352817535 = 0.003213654737919569 + 0.001 * 6.704714775085449
Epoch 920, val loss: 1.7382358312606812
Epoch 930, training loss: 0.009821528568863869 = 0.0031212668400257826 + 0.001 * 6.70026159286499
Epoch 930, val loss: 1.7466018199920654
Epoch 940, training loss: 0.009733851999044418 = 0.0030333944596350193 + 0.001 * 6.700457572937012
Epoch 940, val loss: 1.754783272743225
Epoch 950, training loss: 0.009654964320361614 = 0.002949764020740986 + 0.001 * 6.7052001953125
Epoch 950, val loss: 1.7628573179244995
Epoch 960, training loss: 0.009572898969054222 = 0.002870057011023164 + 0.001 * 6.702841758728027
Epoch 960, val loss: 1.770767092704773
Epoch 970, training loss: 0.00948900543153286 = 0.0027940645813941956 + 0.001 * 6.694940567016602
Epoch 970, val loss: 1.778586506843567
Epoch 980, training loss: 0.009410426020622253 = 0.0027215536683797836 + 0.001 * 6.688872337341309
Epoch 980, val loss: 1.7862799167633057
Epoch 990, training loss: 0.009342577308416367 = 0.0026523403357714415 + 0.001 * 6.690237045288086
Epoch 990, val loss: 1.7938339710235596
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.5018
Flip ASR: 0.4578/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9451907873153687 = 1.936816930770874 + 0.001 * 8.373842239379883
Epoch 0, val loss: 1.9281563758850098
Epoch 10, training loss: 1.9350378513336182 = 1.926664113998413 + 0.001 * 8.37371826171875
Epoch 10, val loss: 1.9183733463287354
Epoch 20, training loss: 1.922515869140625 = 1.9141424894332886 + 0.001 * 8.373394966125488
Epoch 20, val loss: 1.9059057235717773
Epoch 30, training loss: 1.9051207304000854 = 1.8967479467391968 + 0.001 * 8.372749328613281
Epoch 30, val loss: 1.8883767127990723
Epoch 40, training loss: 1.8799042701721191 = 1.871532917022705 + 0.001 * 8.371342658996582
Epoch 40, val loss: 1.8633317947387695
Epoch 50, training loss: 1.8449170589447021 = 1.8365494012832642 + 0.001 * 8.367635726928711
Epoch 50, val loss: 1.8301081657409668
Epoch 60, training loss: 1.8034415245056152 = 1.795087456703186 + 0.001 * 8.354082107543945
Epoch 60, val loss: 1.7935672998428345
Epoch 70, training loss: 1.760892629623413 = 1.7526147365570068 + 0.001 * 8.277851104736328
Epoch 70, val loss: 1.7571710348129272
Epoch 80, training loss: 1.7077463865280151 = 1.699912190437317 + 0.001 * 7.834225654602051
Epoch 80, val loss: 1.7090284824371338
Epoch 90, training loss: 1.6356987953186035 = 1.6281406879425049 + 0.001 * 7.558109283447266
Epoch 90, val loss: 1.6457816362380981
Epoch 100, training loss: 1.5448393821716309 = 1.5374058485031128 + 0.001 * 7.43355131149292
Epoch 100, val loss: 1.571386694908142
Epoch 110, training loss: 1.4452184438705444 = 1.4379448890686035 + 0.001 * 7.273594856262207
Epoch 110, val loss: 1.4921239614486694
Epoch 120, training loss: 1.3479712009429932 = 1.3408883810043335 + 0.001 * 7.0828447341918945
Epoch 120, val loss: 1.4180984497070312
Epoch 130, training loss: 1.2576864957809448 = 1.2506576776504517 + 0.001 * 7.028780460357666
Epoch 130, val loss: 1.352181315422058
Epoch 140, training loss: 1.1734371185302734 = 1.1664412021636963 + 0.001 * 6.995892524719238
Epoch 140, val loss: 1.2922054529190063
Epoch 150, training loss: 1.090843915939331 = 1.0838565826416016 + 0.001 * 6.987318515777588
Epoch 150, val loss: 1.2334771156311035
Epoch 160, training loss: 1.0054575204849243 = 0.9984760284423828 + 0.001 * 6.981523036956787
Epoch 160, val loss: 1.1711807250976562
Epoch 170, training loss: 0.9158094525337219 = 0.9088307023048401 + 0.001 * 6.978774070739746
Epoch 170, val loss: 1.1041786670684814
Epoch 180, training loss: 0.8241788744926453 = 0.8172027468681335 + 0.001 * 6.976131916046143
Epoch 180, val loss: 1.0339686870574951
Epoch 190, training loss: 0.7344061136245728 = 0.7274328470230103 + 0.001 * 6.973294258117676
Epoch 190, val loss: 0.9646110534667969
Epoch 200, training loss: 0.6500545740127563 = 0.6430845260620117 + 0.001 * 6.970035552978516
Epoch 200, val loss: 0.9008020758628845
Epoch 210, training loss: 0.5734667181968689 = 0.566500723361969 + 0.001 * 6.965970039367676
Epoch 210, val loss: 0.8457837104797363
Epoch 220, training loss: 0.5056590437889099 = 0.4986981451511383 + 0.001 * 6.960913181304932
Epoch 220, val loss: 0.8016921281814575
Epoch 230, training loss: 0.4464690387248993 = 0.4395143985748291 + 0.001 * 6.95465087890625
Epoch 230, val loss: 0.7680736780166626
Epoch 240, training loss: 0.3950034976005554 = 0.3880568742752075 + 0.001 * 6.9466142654418945
Epoch 240, val loss: 0.743241012096405
Epoch 250, training loss: 0.3500041365623474 = 0.3430684804916382 + 0.001 * 6.935652732849121
Epoch 250, val loss: 0.7253481149673462
Epoch 260, training loss: 0.31014955043792725 = 0.30322980880737305 + 0.001 * 6.919741630554199
Epoch 260, val loss: 0.7133171558380127
Epoch 270, training loss: 0.27447155117988586 = 0.26756739616394043 + 0.001 * 6.904165267944336
Epoch 270, val loss: 0.706242024898529
Epoch 280, training loss: 0.24237856268882751 = 0.23549894988536835 + 0.001 * 6.87961483001709
Epoch 280, val loss: 0.7033092379570007
Epoch 290, training loss: 0.21363626420497894 = 0.20677775144577026 + 0.001 * 6.858518123626709
Epoch 290, val loss: 0.7040262222290039
Epoch 300, training loss: 0.1881612241268158 = 0.18131738901138306 + 0.001 * 6.8438310623168945
Epoch 300, val loss: 0.7078455090522766
Epoch 310, training loss: 0.16584062576293945 = 0.15900865197181702 + 0.001 * 6.831969738006592
Epoch 310, val loss: 0.7142689824104309
Epoch 320, training loss: 0.1465127319097519 = 0.13969118893146515 + 0.001 * 6.821537017822266
Epoch 320, val loss: 0.7228024005889893
Epoch 330, training loss: 0.12990771234035492 = 0.12309429794549942 + 0.001 * 6.813414096832275
Epoch 330, val loss: 0.7330068349838257
Epoch 340, training loss: 0.11569152772426605 = 0.10888765752315521 + 0.001 * 6.803867340087891
Epoch 340, val loss: 0.7445198893547058
Epoch 350, training loss: 0.10351253300905228 = 0.09671684354543686 + 0.001 * 6.79569149017334
Epoch 350, val loss: 0.7570410966873169
Epoch 360, training loss: 0.09304121136665344 = 0.08625306934118271 + 0.001 * 6.788140773773193
Epoch 360, val loss: 0.770301103591919
Epoch 370, training loss: 0.08400064706802368 = 0.07721336930990219 + 0.001 * 6.78727912902832
Epoch 370, val loss: 0.784247100353241
Epoch 380, training loss: 0.07613804936408997 = 0.06936456263065338 + 0.001 * 6.773482799530029
Epoch 380, val loss: 0.7988332509994507
Epoch 390, training loss: 0.06927501410245895 = 0.06251371651887894 + 0.001 * 6.761299133300781
Epoch 390, val loss: 0.8138710856437683
Epoch 400, training loss: 0.06326079368591309 = 0.05650447681546211 + 0.001 * 6.756314754486084
Epoch 400, val loss: 0.8294630646705627
Epoch 410, training loss: 0.057963959872722626 = 0.05120869353413582 + 0.001 * 6.7552642822265625
Epoch 410, val loss: 0.8453112840652466
Epoch 420, training loss: 0.05327446758747101 = 0.04652164503931999 + 0.001 * 6.75282096862793
Epoch 420, val loss: 0.8614954948425293
Epoch 430, training loss: 0.04910845682024956 = 0.04235893487930298 + 0.001 * 6.749520301818848
Epoch 430, val loss: 0.8778250217437744
Epoch 440, training loss: 0.045398738235235214 = 0.03865155950188637 + 0.001 * 6.74717903137207
Epoch 440, val loss: 0.894301176071167
Epoch 450, training loss: 0.04208875447511673 = 0.035342149436473846 + 0.001 * 6.746605396270752
Epoch 450, val loss: 0.9107767939567566
Epoch 460, training loss: 0.0391324944794178 = 0.03238273039460182 + 0.001 * 6.749763011932373
Epoch 460, val loss: 0.9271650910377502
Epoch 470, training loss: 0.036481935530900955 = 0.02973501943051815 + 0.001 * 6.7469162940979
Epoch 470, val loss: 0.9433082938194275
Epoch 480, training loss: 0.03410894051194191 = 0.02736351080238819 + 0.001 * 6.745430946350098
Epoch 480, val loss: 0.9590955972671509
Epoch 490, training loss: 0.03198276832699776 = 0.0252356119453907 + 0.001 * 6.747157096862793
Epoch 490, val loss: 0.974433183670044
Epoch 500, training loss: 0.030064623802900314 = 0.023320261389017105 + 0.001 * 6.7443623542785645
Epoch 500, val loss: 0.9894124269485474
Epoch 510, training loss: 0.028334464877843857 = 0.021589193493127823 + 0.001 * 6.745270252227783
Epoch 510, val loss: 1.003960132598877
Epoch 520, training loss: 0.026765897870063782 = 0.02001979760825634 + 0.001 * 6.746099948883057
Epoch 520, val loss: 1.0179948806762695
Epoch 530, training loss: 0.02533879317343235 = 0.01859547570347786 + 0.001 * 6.743316650390625
Epoch 530, val loss: 1.0317407846450806
Epoch 540, training loss: 0.024049019441008568 = 0.017302244901657104 + 0.001 * 6.746774196624756
Epoch 540, val loss: 1.0450907945632935
Epoch 550, training loss: 0.02287166193127632 = 0.016127677634358406 + 0.001 * 6.743984699249268
Epoch 550, val loss: 1.0580211877822876
Epoch 560, training loss: 0.02180357091128826 = 0.01506027765572071 + 0.001 * 6.743292808532715
Epoch 560, val loss: 1.0705689191818237
Epoch 570, training loss: 0.020831767469644547 = 0.014089375734329224 + 0.001 * 6.742392063140869
Epoch 570, val loss: 1.0827009677886963
Epoch 580, training loss: 0.019955532625317574 = 0.013205164112150669 + 0.001 * 6.750368595123291
Epoch 580, val loss: 1.0944546461105347
Epoch 590, training loss: 0.01914006471633911 = 0.012398764491081238 + 0.001 * 6.7413010597229
Epoch 590, val loss: 1.1058096885681152
Epoch 600, training loss: 0.018402358517050743 = 0.011662143282592297 + 0.001 * 6.7402143478393555
Epoch 600, val loss: 1.1167972087860107
Epoch 610, training loss: 0.017728136852383614 = 0.010988209396600723 + 0.001 * 6.739927768707275
Epoch 610, val loss: 1.127449631690979
Epoch 620, training loss: 0.017110265791416168 = 0.010370587930083275 + 0.001 * 6.739677906036377
Epoch 620, val loss: 1.1377718448638916
Epoch 630, training loss: 0.01654517650604248 = 0.009803571738302708 + 0.001 * 6.741605281829834
Epoch 630, val loss: 1.1478087902069092
Epoch 640, training loss: 0.016022756695747375 = 0.009282039478421211 + 0.001 * 6.740716457366943
Epoch 640, val loss: 1.1575337648391724
Epoch 650, training loss: 0.015540177933871746 = 0.008801531046628952 + 0.001 * 6.738646507263184
Epoch 650, val loss: 1.1669464111328125
Epoch 660, training loss: 0.015095461159944534 = 0.008358023129403591 + 0.001 * 6.7374372482299805
Epoch 660, val loss: 1.1760878562927246
Epoch 670, training loss: 0.014685247093439102 = 0.007947944104671478 + 0.001 * 6.737303256988525
Epoch 670, val loss: 1.1849485635757446
Epoch 680, training loss: 0.014304984360933304 = 0.007568153087049723 + 0.001 * 6.736830711364746
Epoch 680, val loss: 1.1935479640960693
Epoch 690, training loss: 0.013955440372228622 = 0.0072158328257501125 + 0.001 * 6.739607810974121
Epoch 690, val loss: 1.201889157295227
Epoch 700, training loss: 0.01362308207899332 = 0.006888457573950291 + 0.001 * 6.73462438583374
Epoch 700, val loss: 1.210008978843689
Epoch 710, training loss: 0.013318253681063652 = 0.006583810318261385 + 0.001 * 6.734443664550781
Epoch 710, val loss: 1.217895269393921
Epoch 720, training loss: 0.013033958151936531 = 0.006299891509115696 + 0.001 * 6.734066486358643
Epoch 720, val loss: 1.225551724433899
Epoch 730, training loss: 0.012769743800163269 = 0.006034854333847761 + 0.001 * 6.734889030456543
Epoch 730, val loss: 1.2330011129379272
Epoch 740, training loss: 0.01253032498061657 = 0.00578702986240387 + 0.001 * 6.743295192718506
Epoch 740, val loss: 1.2402431964874268
Epoch 750, training loss: 0.012291401624679565 = 0.0055548325181007385 + 0.001 * 6.736568450927734
Epoch 750, val loss: 1.247300148010254
Epoch 760, training loss: 0.012069886550307274 = 0.005336685571819544 + 0.001 * 6.733200550079346
Epoch 760, val loss: 1.2541697025299072
Epoch 770, training loss: 0.011861229315400124 = 0.0051310365088284016 + 0.001 * 6.730192184448242
Epoch 770, val loss: 1.2609031200408936
Epoch 780, training loss: 0.011677113361656666 = 0.004936456214636564 + 0.001 * 6.740656852722168
Epoch 780, val loss: 1.267490267753601
Epoch 790, training loss: 0.011483517475426197 = 0.004751723725348711 + 0.001 * 6.731793403625488
Epoch 790, val loss: 1.273987889289856
Epoch 800, training loss: 0.011305983178317547 = 0.0045759729109704494 + 0.001 * 6.730010032653809
Epoch 800, val loss: 1.280388355255127
Epoch 810, training loss: 0.011136597022414207 = 0.004408549051731825 + 0.001 * 6.7280473709106445
Epoch 810, val loss: 1.2867014408111572
Epoch 820, training loss: 0.010981915518641472 = 0.004249009769409895 + 0.001 * 6.732905864715576
Epoch 820, val loss: 1.2929503917694092
Epoch 830, training loss: 0.010828694328665733 = 0.004097029101103544 + 0.001 * 6.731664657592773
Epoch 830, val loss: 1.2991427183151245
Epoch 840, training loss: 0.010677835904061794 = 0.003952308092266321 + 0.001 * 6.725527286529541
Epoch 840, val loss: 1.305260181427002
Epoch 850, training loss: 0.010541791096329689 = 0.0038145571015775204 + 0.001 * 6.72723388671875
Epoch 850, val loss: 1.3113043308258057
Epoch 860, training loss: 0.01041040476411581 = 0.003683542599901557 + 0.001 * 6.726861476898193
Epoch 860, val loss: 1.3172742128372192
Epoch 870, training loss: 0.010284245945513248 = 0.0035589353647083044 + 0.001 * 6.725310325622559
Epoch 870, val loss: 1.3231520652770996
Epoch 880, training loss: 0.010163506492972374 = 0.003440476953983307 + 0.001 * 6.723029136657715
Epoch 880, val loss: 1.328967809677124
Epoch 890, training loss: 0.010052384808659554 = 0.003327858168631792 + 0.001 * 6.7245259284973145
Epoch 890, val loss: 1.3347053527832031
Epoch 900, training loss: 0.009947467595338821 = 0.003220793092623353 + 0.001 * 6.7266740798950195
Epoch 900, val loss: 1.3403393030166626
Epoch 910, training loss: 0.009841294959187508 = 0.0031189979054033756 + 0.001 * 6.722296237945557
Epoch 910, val loss: 1.3458815813064575
Epoch 920, training loss: 0.009745641611516476 = 0.0030221869237720966 + 0.001 * 6.723454475402832
Epoch 920, val loss: 1.3513453006744385
Epoch 930, training loss: 0.009653221815824509 = 0.002930103801190853 + 0.001 * 6.723117351531982
Epoch 930, val loss: 1.3567094802856445
Epoch 940, training loss: 0.009566457942128181 = 0.002842493588104844 + 0.001 * 6.723964214324951
Epoch 940, val loss: 1.3619939088821411
Epoch 950, training loss: 0.00947826448827982 = 0.002759078750386834 + 0.001 * 6.719185829162598
Epoch 950, val loss: 1.3672014474868774
Epoch 960, training loss: 0.009397980757057667 = 0.0026796371676027775 + 0.001 * 6.718343257904053
Epoch 960, val loss: 1.3722971677780151
Epoch 970, training loss: 0.009350426495075226 = 0.002603927394375205 + 0.001 * 6.746499061584473
Epoch 970, val loss: 1.3772962093353271
Epoch 980, training loss: 0.009257286787033081 = 0.002531806007027626 + 0.001 * 6.725481033325195
Epoch 980, val loss: 1.3822165727615356
Epoch 990, training loss: 0.009177763015031815 = 0.0024630269035696983 + 0.001 * 6.714735984802246
Epoch 990, val loss: 1.3870394229888916
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9041
Flip ASR: 0.8844/225 nodes
The final ASR:0.69619, 0.16448, Accuracy:0.79383, 0.02124
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11608])
remove edge: torch.Size([2, 9496])
updated graph: torch.Size([2, 10548])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
The final ASR:0.98401, 0.00870, Accuracy:0.83086, 0.00462
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.939265489578247 = 1.9308916330337524 + 0.001 * 8.373807907104492
Epoch 0, val loss: 1.9182454347610474
Epoch 10, training loss: 1.9295076131820679 = 1.9211338758468628 + 0.001 * 8.373713493347168
Epoch 10, val loss: 1.9094011783599854
Epoch 20, training loss: 1.9173424243927002 = 1.9089690446853638 + 0.001 * 8.373437881469727
Epoch 20, val loss: 1.8980823755264282
Epoch 30, training loss: 1.9001141786575317 = 1.8917412757873535 + 0.001 * 8.372879981994629
Epoch 30, val loss: 1.8819310665130615
Epoch 40, training loss: 1.8747081756591797 = 1.866336703300476 + 0.001 * 8.371509552001953
Epoch 40, val loss: 1.8584436178207397
Epoch 50, training loss: 1.8400123119354248 = 1.831645131111145 + 0.001 * 8.36715316772461
Epoch 50, val loss: 1.8284910917282104
Epoch 60, training loss: 1.8031173944473267 = 1.7947684526443481 + 0.001 * 8.34888744354248
Epoch 60, val loss: 1.8012826442718506
Epoch 70, training loss: 1.7668579816818237 = 1.7586268186569214 + 0.001 * 8.231196403503418
Epoch 70, val loss: 1.7745274305343628
Epoch 80, training loss: 1.7160780429840088 = 1.7082639932632446 + 0.001 * 7.814101219177246
Epoch 80, val loss: 1.7314013242721558
Epoch 90, training loss: 1.646466612815857 = 1.6388156414031982 + 0.001 * 7.650964736938477
Epoch 90, val loss: 1.6724820137023926
Epoch 100, training loss: 1.5576947927474976 = 1.5502246618270874 + 0.001 * 7.4701104164123535
Epoch 100, val loss: 1.6002391576766968
Epoch 110, training loss: 1.4625887870788574 = 1.4553616046905518 + 0.001 * 7.227217197418213
Epoch 110, val loss: 1.524550199508667
Epoch 120, training loss: 1.3713009357452393 = 1.364154577255249 + 0.001 * 7.146356105804443
Epoch 120, val loss: 1.4558607339859009
Epoch 130, training loss: 1.282947301864624 = 1.275872826576233 + 0.001 * 7.074525833129883
Epoch 130, val loss: 1.392119288444519
Epoch 140, training loss: 1.1940420866012573 = 1.187031626701355 + 0.001 * 7.010435104370117
Epoch 140, val loss: 1.3280811309814453
Epoch 150, training loss: 1.1044809818267822 = 1.0975065231323242 + 0.001 * 6.9744696617126465
Epoch 150, val loss: 1.2642102241516113
Epoch 160, training loss: 1.0165835618972778 = 1.0096232891082764 + 0.001 * 6.960273265838623
Epoch 160, val loss: 1.202506422996521
Epoch 170, training loss: 0.9322133660316467 = 0.9252607226371765 + 0.001 * 6.952635765075684
Epoch 170, val loss: 1.143521785736084
Epoch 180, training loss: 0.8519884347915649 = 0.8450465798377991 + 0.001 * 6.94185733795166
Epoch 180, val loss: 1.0880929231643677
Epoch 190, training loss: 0.7758896946907043 = 0.768964409828186 + 0.001 * 6.925291061401367
Epoch 190, val loss: 1.0360288619995117
Epoch 200, training loss: 0.7040094137191772 = 0.697108268737793 + 0.001 * 6.901160717010498
Epoch 200, val loss: 0.9882647395133972
Epoch 210, training loss: 0.636283278465271 = 0.6294144988059998 + 0.001 * 6.86879301071167
Epoch 210, val loss: 0.9452247619628906
Epoch 220, training loss: 0.572123110294342 = 0.5652767419815063 + 0.001 * 6.846343040466309
Epoch 220, val loss: 0.9066221714019775
Epoch 230, training loss: 0.5104893445968628 = 0.5036627650260925 + 0.001 * 6.826592922210693
Epoch 230, val loss: 0.8711275458335876
Epoch 240, training loss: 0.45098915696144104 = 0.44417086243629456 + 0.001 * 6.81828498840332
Epoch 240, val loss: 0.8385308980941772
Epoch 250, training loss: 0.3941706120967865 = 0.3873564302921295 + 0.001 * 6.814188480377197
Epoch 250, val loss: 0.8098613023757935
Epoch 260, training loss: 0.34147554636001587 = 0.3346632421016693 + 0.001 * 6.8122968673706055
Epoch 260, val loss: 0.7870067954063416
Epoch 270, training loss: 0.2945122718811035 = 0.2877010405063629 + 0.001 * 6.811230659484863
Epoch 270, val loss: 0.771758496761322
Epoch 280, training loss: 0.2539580464363098 = 0.24714398384094238 + 0.001 * 6.814047813415527
Epoch 280, val loss: 0.7645314931869507
Epoch 290, training loss: 0.21949037909507751 = 0.21267934143543243 + 0.001 * 6.811033248901367
Epoch 290, val loss: 0.7641456723213196
Epoch 300, training loss: 0.190383642911911 = 0.18357349932193756 + 0.001 * 6.81013822555542
Epoch 300, val loss: 0.7694244980812073
Epoch 310, training loss: 0.16579577326774597 = 0.15898694097995758 + 0.001 * 6.808836460113525
Epoch 310, val loss: 0.7790964841842651
Epoch 320, training loss: 0.14497900009155273 = 0.13816899061203003 + 0.001 * 6.8100152015686035
Epoch 320, val loss: 0.7920439839363098
Epoch 330, training loss: 0.12731119990348816 = 0.12050402164459229 + 0.001 * 6.807185173034668
Epoch 330, val loss: 0.8073201775550842
Epoch 340, training loss: 0.11230430752038956 = 0.10549905151128769 + 0.001 * 6.805256366729736
Epoch 340, val loss: 0.8241592645645142
Epoch 350, training loss: 0.09954479336738586 = 0.0927392914891243 + 0.001 * 6.805502891540527
Epoch 350, val loss: 0.8417708873748779
Epoch 360, training loss: 0.08867085725069046 = 0.08186828345060349 + 0.001 * 6.802571773529053
Epoch 360, val loss: 0.8595654964447021
Epoch 370, training loss: 0.07937076687812805 = 0.07257256656885147 + 0.001 * 6.798199653625488
Epoch 370, val loss: 0.8771352171897888
Epoch 380, training loss: 0.0713859498500824 = 0.06458837538957596 + 0.001 * 6.797574996948242
Epoch 380, val loss: 0.8942784667015076
Epoch 390, training loss: 0.06448817253112793 = 0.05769786611199379 + 0.001 * 6.790306568145752
Epoch 390, val loss: 0.9110309481620789
Epoch 400, training loss: 0.05850853770971298 = 0.051719509065151215 + 0.001 * 6.789030075073242
Epoch 400, val loss: 0.9273941516876221
Epoch 410, training loss: 0.05328885093331337 = 0.046506404876708984 + 0.001 * 6.782444953918457
Epoch 410, val loss: 0.9433838725090027
Epoch 420, training loss: 0.04872126877307892 = 0.041942842304706573 + 0.001 * 6.778426647186279
Epoch 420, val loss: 0.9590484499931335
Epoch 430, training loss: 0.04471004009246826 = 0.03793668374419212 + 0.001 * 6.77335786819458
Epoch 430, val loss: 0.974383533000946
Epoch 440, training loss: 0.04119356349110603 = 0.03441333770751953 + 0.001 * 6.7802252769470215
Epoch 440, val loss: 0.989376962184906
Epoch 450, training loss: 0.03807418420910835 = 0.03130883723497391 + 0.001 * 6.765347957611084
Epoch 450, val loss: 1.0039794445037842
Epoch 460, training loss: 0.03532658889889717 = 0.028567487373948097 + 0.001 * 6.759102821350098
Epoch 460, val loss: 1.0182242393493652
Epoch 470, training loss: 0.032921962440013885 = 0.026142345741391182 + 0.001 * 6.779618263244629
Epoch 470, val loss: 1.0320881605148315
Epoch 480, training loss: 0.030744779855012894 = 0.023992251604795456 + 0.001 * 6.752527236938477
Epoch 480, val loss: 1.0455936193466187
Epoch 490, training loss: 0.028836926445364952 = 0.02208080142736435 + 0.001 * 6.756124019622803
Epoch 490, val loss: 1.0586820840835571
Epoch 500, training loss: 0.02713518962264061 = 0.020377300679683685 + 0.001 * 6.757887840270996
Epoch 500, val loss: 1.0714359283447266
Epoch 510, training loss: 0.02559790387749672 = 0.018854742869734764 + 0.001 * 6.743159770965576
Epoch 510, val loss: 1.0838148593902588
Epoch 520, training loss: 0.02421863004565239 = 0.017490549013018608 + 0.001 * 6.7280802726745605
Epoch 520, val loss: 1.0958611965179443
Epoch 530, training loss: 0.022994816303253174 = 0.01626535691320896 + 0.001 * 6.729459285736084
Epoch 530, val loss: 1.107532024383545
Epoch 540, training loss: 0.02188546396791935 = 0.015162013471126556 + 0.001 * 6.72344970703125
Epoch 540, val loss: 1.1188907623291016
Epoch 550, training loss: 0.020898625254631042 = 0.014165408909320831 + 0.001 * 6.733216285705566
Epoch 550, val loss: 1.129906177520752
Epoch 560, training loss: 0.019988179206848145 = 0.013263009488582611 + 0.001 * 6.725170135498047
Epoch 560, val loss: 1.1406188011169434
Epoch 570, training loss: 0.019160307943820953 = 0.01244413759559393 + 0.001 * 6.716169834136963
Epoch 570, val loss: 1.1510510444641113
Epoch 580, training loss: 0.01841023378074169 = 0.011698960326611996 + 0.001 * 6.711272716522217
Epoch 580, val loss: 1.1612013578414917
Epoch 590, training loss: 0.01773121953010559 = 0.011018899269402027 + 0.001 * 6.7123188972473145
Epoch 590, val loss: 1.1710702180862427
Epoch 600, training loss: 0.017116114497184753 = 0.010395637713372707 + 0.001 * 6.72047758102417
Epoch 600, val loss: 1.1807101964950562
Epoch 610, training loss: 0.01654142513871193 = 0.009821330197155476 + 0.001 * 6.720094680786133
Epoch 610, val loss: 1.190093994140625
Epoch 620, training loss: 0.015991874039173126 = 0.009289218112826347 + 0.001 * 6.702656269073486
Epoch 620, val loss: 1.1992831230163574
Epoch 630, training loss: 0.015499331057071686 = 0.008794931694865227 + 0.001 * 6.704399585723877
Epoch 630, val loss: 1.2083158493041992
Epoch 640, training loss: 0.015056811273097992 = 0.008335410617291927 + 0.001 * 6.721400737762451
Epoch 640, val loss: 1.2171903848648071
Epoch 650, training loss: 0.014620695263147354 = 0.007908194325864315 + 0.001 * 6.71250057220459
Epoch 650, val loss: 1.2259403467178345
Epoch 660, training loss: 0.01420570071786642 = 0.007510969415307045 + 0.001 * 6.694730758666992
Epoch 660, val loss: 1.234508752822876
Epoch 670, training loss: 0.013831966556608677 = 0.007141599431633949 + 0.001 * 6.690366744995117
Epoch 670, val loss: 1.2429295778274536
Epoch 680, training loss: 0.013500692322850227 = 0.006798025220632553 + 0.001 * 6.702666282653809
Epoch 680, val loss: 1.2511622905731201
Epoch 690, training loss: 0.013186801224946976 = 0.006478307768702507 + 0.001 * 6.708492755889893
Epoch 690, val loss: 1.2592519521713257
Epoch 700, training loss: 0.012877317145466805 = 0.006180567666888237 + 0.001 * 6.696748733520508
Epoch 700, val loss: 1.2671499252319336
Epoch 710, training loss: 0.012593839317560196 = 0.00590306194499135 + 0.001 * 6.69077730178833
Epoch 710, val loss: 1.274895191192627
Epoch 720, training loss: 0.012329298071563244 = 0.00564412260428071 + 0.001 * 6.685174942016602
Epoch 720, val loss: 1.2824815511703491
Epoch 730, training loss: 0.01208139955997467 = 0.005402322858572006 + 0.001 * 6.679076194763184
Epoch 730, val loss: 1.2899081707000732
Epoch 740, training loss: 0.011868519708514214 = 0.0051761954091489315 + 0.001 * 6.692323684692383
Epoch 740, val loss: 1.297188401222229
Epoch 750, training loss: 0.011634317226707935 = 0.004964602645486593 + 0.001 * 6.669714450836182
Epoch 750, val loss: 1.3042954206466675
Epoch 760, training loss: 0.011439998634159565 = 0.004766348283737898 + 0.001 * 6.67365026473999
Epoch 760, val loss: 1.3112529516220093
Epoch 770, training loss: 0.011251702904701233 = 0.004580463748425245 + 0.001 * 6.671239376068115
Epoch 770, val loss: 1.3180509805679321
Epoch 780, training loss: 0.011109263636171818 = 0.004405921790748835 + 0.001 * 6.703341484069824
Epoch 780, val loss: 1.3247020244598389
Epoch 790, training loss: 0.01089957170188427 = 0.0042419712990522385 + 0.001 * 6.657599925994873
Epoch 790, val loss: 1.3311896324157715
Epoch 800, training loss: 0.010744506493210793 = 0.004087660927325487 + 0.001 * 6.656845569610596
Epoch 800, val loss: 1.3375440835952759
Epoch 810, training loss: 0.010595575906336308 = 0.003942341543734074 + 0.001 * 6.653234004974365
Epoch 810, val loss: 1.3437577486038208
Epoch 820, training loss: 0.010461102239787579 = 0.0038053474854677916 + 0.001 * 6.655754089355469
Epoch 820, val loss: 1.349838137626648
Epoch 830, training loss: 0.010334949940443039 = 0.0036760319489985704 + 0.001 * 6.6589179039001465
Epoch 830, val loss: 1.3557852506637573
Epoch 840, training loss: 0.010226523503661156 = 0.0035538203082978725 + 0.001 * 6.672703266143799
Epoch 840, val loss: 1.3615927696228027
Epoch 850, training loss: 0.010093364864587784 = 0.003438210114836693 + 0.001 * 6.655154228210449
Epoch 850, val loss: 1.3673056364059448
Epoch 860, training loss: 0.010011252015829086 = 0.0033287510741502047 + 0.001 * 6.68250036239624
Epoch 860, val loss: 1.3728750944137573
Epoch 870, training loss: 0.009898938238620758 = 0.003225065302103758 + 0.001 * 6.673872947692871
Epoch 870, val loss: 1.3783433437347412
Epoch 880, training loss: 0.009786832146346569 = 0.003126793308183551 + 0.001 * 6.660038948059082
Epoch 880, val loss: 1.3836642503738403
Epoch 890, training loss: 0.009691888466477394 = 0.0030334738548845053 + 0.001 * 6.658414363861084
Epoch 890, val loss: 1.3888866901397705
Epoch 900, training loss: 0.0095919044688344 = 0.0029448000714182854 + 0.001 * 6.647104263305664
Epoch 900, val loss: 1.3940074443817139
Epoch 910, training loss: 0.009500732645392418 = 0.0028604601975530386 + 0.001 * 6.64027214050293
Epoch 910, val loss: 1.3990081548690796
Epoch 920, training loss: 0.009417221881449223 = 0.002780225360766053 + 0.001 * 6.636995792388916
Epoch 920, val loss: 1.4039207696914673
Epoch 930, training loss: 0.009357037022709846 = 0.002703903242945671 + 0.001 * 6.653133869171143
Epoch 930, val loss: 1.4087117910385132
Epoch 940, training loss: 0.009268198162317276 = 0.0026311872061342 + 0.001 * 6.6370110511779785
Epoch 940, val loss: 1.4134225845336914
Epoch 950, training loss: 0.009204124100506306 = 0.002561806468293071 + 0.001 * 6.642317295074463
Epoch 950, val loss: 1.4180257320404053
Epoch 960, training loss: 0.0091323833912611 = 0.0024956276174634695 + 0.001 * 6.636755466461182
Epoch 960, val loss: 1.4225382804870605
Epoch 970, training loss: 0.00907939113676548 = 0.0024324071127921343 + 0.001 * 6.646983623504639
Epoch 970, val loss: 1.4269671440124512
Epoch 980, training loss: 0.009014664217829704 = 0.0023720371536910534 + 0.001 * 6.6426262855529785
Epoch 980, val loss: 1.4312794208526611
Epoch 990, training loss: 0.008958266116678715 = 0.0023142891004681587 + 0.001 * 6.64397668838501
Epoch 990, val loss: 1.4355244636535645
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9734711647033691 = 1.9650973081588745 + 0.001 * 8.373881340026855
Epoch 0, val loss: 1.9521201848983765
Epoch 10, training loss: 1.9627598524093628 = 1.9543859958648682 + 0.001 * 8.373828887939453
Epoch 10, val loss: 1.942297339439392
Epoch 20, training loss: 1.949855923652649 = 1.9414823055267334 + 0.001 * 8.373658180236816
Epoch 20, val loss: 1.9301432371139526
Epoch 30, training loss: 1.9319429397583008 = 1.923569679260254 + 0.001 * 8.373270034790039
Epoch 30, val loss: 1.9130176305770874
Epoch 40, training loss: 1.9052544832229614 = 1.8968820571899414 + 0.001 * 8.372371673583984
Epoch 40, val loss: 1.8876506090164185
Epoch 50, training loss: 1.8665610551834106 = 1.8581910133361816 + 0.001 * 8.370034217834473
Epoch 50, val loss: 1.852518916130066
Epoch 60, training loss: 1.8198474645614624 = 1.8114854097366333 + 0.001 * 8.36207103729248
Epoch 60, val loss: 1.8149775266647339
Epoch 70, training loss: 1.7782191038131714 = 1.7698944807052612 + 0.001 * 8.324623107910156
Epoch 70, val loss: 1.7868468761444092
Epoch 80, training loss: 1.732094407081604 = 1.7239705324172974 + 0.001 * 8.123855590820312
Epoch 80, val loss: 1.7495288848876953
Epoch 90, training loss: 1.667830228805542 = 1.6599273681640625 + 0.001 * 7.902843952178955
Epoch 90, val loss: 1.6926493644714355
Epoch 100, training loss: 1.5818759202957153 = 1.5741831064224243 + 0.001 * 7.692843437194824
Epoch 100, val loss: 1.6193927526474
Epoch 110, training loss: 1.4817358255386353 = 1.4743133783340454 + 0.001 * 7.422455310821533
Epoch 110, val loss: 1.5389729738235474
Epoch 120, training loss: 1.3828163146972656 = 1.3755223751068115 + 0.001 * 7.293941974639893
Epoch 120, val loss: 1.462493658065796
Epoch 130, training loss: 1.2901527881622314 = 1.2828845977783203 + 0.001 * 7.268244743347168
Epoch 130, val loss: 1.3934286832809448
Epoch 140, training loss: 1.202148199081421 = 1.1949223279953003 + 0.001 * 7.225886344909668
Epoch 140, val loss: 1.3298211097717285
Epoch 150, training loss: 1.1190155744552612 = 1.1118619441986084 + 0.001 * 7.153642654418945
Epoch 150, val loss: 1.2707254886627197
Epoch 160, training loss: 1.0426063537597656 = 1.0355334281921387 + 0.001 * 7.07297945022583
Epoch 160, val loss: 1.218012809753418
Epoch 170, training loss: 0.9733232855796814 = 0.9663121700286865 + 0.001 * 7.011087894439697
Epoch 170, val loss: 1.1715195178985596
Epoch 180, training loss: 0.9086804389953613 = 0.9017025828361511 + 0.001 * 6.977847576141357
Epoch 180, val loss: 1.1282680034637451
Epoch 190, training loss: 0.8446119427680969 = 0.8376534581184387 + 0.001 * 6.958481311798096
Epoch 190, val loss: 1.0849641561508179
Epoch 200, training loss: 0.7779026031494141 = 0.7709624171257019 + 0.001 * 6.940174102783203
Epoch 200, val loss: 1.0392714738845825
Epoch 210, training loss: 0.7081961631774902 = 0.7012680172920227 + 0.001 * 6.928136348724365
Epoch 210, val loss: 0.9911966919898987
Epoch 220, training loss: 0.6384847164154053 = 0.6315630078315735 + 0.001 * 6.921724796295166
Epoch 220, val loss: 0.94368976354599
Epoch 230, training loss: 0.5738502144813538 = 0.566931426525116 + 0.001 * 6.918768882751465
Epoch 230, val loss: 0.9014917016029358
Epoch 240, training loss: 0.5179381370544434 = 0.5110211372375488 + 0.001 * 6.917011260986328
Epoch 240, val loss: 0.8678609132766724
Epoch 250, training loss: 0.47127091884613037 = 0.46435534954071045 + 0.001 * 6.915560245513916
Epoch 250, val loss: 0.8433990478515625
Epoch 260, training loss: 0.4320944845676422 = 0.42517998814582825 + 0.001 * 6.914510250091553
Epoch 260, val loss: 0.8264864087104797
Epoch 270, training loss: 0.39793872833251953 = 0.39102524518966675 + 0.001 * 6.913476467132568
Epoch 270, val loss: 0.8148815631866455
Epoch 280, training loss: 0.3666911721229553 = 0.3597801923751831 + 0.001 * 6.910991668701172
Epoch 280, val loss: 0.8066688776016235
Epoch 290, training loss: 0.3371705412864685 = 0.33026111125946045 + 0.001 * 6.9094367027282715
Epoch 290, val loss: 0.8007739782333374
Epoch 300, training loss: 0.3089912533760071 = 0.30208393931388855 + 0.001 * 6.907327175140381
Epoch 300, val loss: 0.7969071865081787
Epoch 310, training loss: 0.2823525071144104 = 0.27544763684272766 + 0.001 * 6.904872894287109
Epoch 310, val loss: 0.7951475977897644
Epoch 320, training loss: 0.25742307305336 = 0.2505210340023041 + 0.001 * 6.902052402496338
Epoch 320, val loss: 0.7955490946769714
Epoch 330, training loss: 0.23394757509231567 = 0.22704893350601196 + 0.001 * 6.898645401000977
Epoch 330, val loss: 0.7978935837745667
Epoch 340, training loss: 0.21119210124015808 = 0.20429767668247223 + 0.001 * 6.8944244384765625
Epoch 340, val loss: 0.8014109134674072
Epoch 350, training loss: 0.18841713666915894 = 0.18152789771556854 + 0.001 * 6.889236927032471
Epoch 350, val loss: 0.8055782914161682
Epoch 360, training loss: 0.16563981771469116 = 0.15875469148159027 + 0.001 * 6.885128021240234
Epoch 360, val loss: 0.8099897503852844
Epoch 370, training loss: 0.14392586052417755 = 0.13704615831375122 + 0.001 * 6.879704475402832
Epoch 370, val loss: 0.8151795268058777
Epoch 380, training loss: 0.12452657520771027 = 0.1176467016339302 + 0.001 * 6.879877090454102
Epoch 380, val loss: 0.8219471573829651
Epoch 390, training loss: 0.1079484075307846 = 0.10107984393835068 + 0.001 * 6.868564605712891
Epoch 390, val loss: 0.8309048414230347
Epoch 400, training loss: 0.09408532828092575 = 0.0872204601764679 + 0.001 * 6.8648681640625
Epoch 400, val loss: 0.8421388268470764
Epoch 410, training loss: 0.08255472779273987 = 0.07568856328725815 + 0.001 * 6.866168022155762
Epoch 410, val loss: 0.8552657961845398
Epoch 420, training loss: 0.07292374223470688 = 0.06606519222259521 + 0.001 * 6.858548164367676
Epoch 420, val loss: 0.8696801662445068
Epoch 430, training loss: 0.06485018879175186 = 0.05799325183033943 + 0.001 * 6.856936454772949
Epoch 430, val loss: 0.8849783539772034
Epoch 440, training loss: 0.05803627893328667 = 0.05118387192487717 + 0.001 * 6.8524065017700195
Epoch 440, val loss: 0.9007195234298706
Epoch 450, training loss: 0.05225823074579239 = 0.045409005135297775 + 0.001 * 6.8492255210876465
Epoch 450, val loss: 0.9165769219398499
Epoch 460, training loss: 0.04733480513095856 = 0.04048782214522362 + 0.001 * 6.846982002258301
Epoch 460, val loss: 0.9323358535766602
Epoch 470, training loss: 0.043122123926877975 = 0.036274343729019165 + 0.001 * 6.847778797149658
Epoch 470, val loss: 0.94781893491745
Epoch 480, training loss: 0.03949514031410217 = 0.03264923021197319 + 0.001 * 6.8459086418151855
Epoch 480, val loss: 0.9629706740379333
Epoch 490, training loss: 0.036363668739795685 = 0.029515622183680534 + 0.001 * 6.848048210144043
Epoch 490, val loss: 0.9777376651763916
Epoch 500, training loss: 0.03363993018865585 = 0.026794811710715294 + 0.001 * 6.845117568969727
Epoch 500, val loss: 0.9920997023582458
Epoch 510, training loss: 0.031264692544937134 = 0.024422191083431244 + 0.001 * 6.842502593994141
Epoch 510, val loss: 1.0060149431228638
Epoch 520, training loss: 0.029186878353357315 = 0.022343941032886505 + 0.001 * 6.842936038970947
Epoch 520, val loss: 1.0194878578186035
Epoch 530, training loss: 0.027354897931218147 = 0.02051590383052826 + 0.001 * 6.838994026184082
Epoch 530, val loss: 1.0325795412063599
Epoch 540, training loss: 0.025749176740646362 = 0.018901538103818893 + 0.001 * 6.847638130187988
Epoch 540, val loss: 1.0452910661697388
Epoch 550, training loss: 0.02431020885705948 = 0.01747000962495804 + 0.001 * 6.840198040008545
Epoch 550, val loss: 1.0576283931732178
Epoch 560, training loss: 0.023032117635011673 = 0.016195867210626602 + 0.001 * 6.836249828338623
Epoch 560, val loss: 1.0695663690567017
Epoch 570, training loss: 0.0218926053494215 = 0.015057364478707314 + 0.001 * 6.835239887237549
Epoch 570, val loss: 1.0811392068862915
Epoch 580, training loss: 0.020878147333860397 = 0.014036480337381363 + 0.001 * 6.841667175292969
Epoch 580, val loss: 1.092395305633545
Epoch 590, training loss: 0.0199538916349411 = 0.013117988593876362 + 0.001 * 6.835902690887451
Epoch 590, val loss: 1.1032930612564087
Epoch 600, training loss: 0.019120600074529648 = 0.012289054691791534 + 0.001 * 6.831545829772949
Epoch 600, val loss: 1.1138644218444824
Epoch 610, training loss: 0.018379397690296173 = 0.011538484133780003 + 0.001 * 6.840914249420166
Epoch 610, val loss: 1.1241313219070435
Epoch 620, training loss: 0.017687298357486725 = 0.010856812819838524 + 0.001 * 6.830484390258789
Epoch 620, val loss: 1.1340973377227783
Epoch 630, training loss: 0.017064271494746208 = 0.01023561879992485 + 0.001 * 6.828652381896973
Epoch 630, val loss: 1.1437792778015137
Epoch 640, training loss: 0.016495050862431526 = 0.009666980244219303 + 0.001 * 6.828070640563965
Epoch 640, val loss: 1.1532127857208252
Epoch 650, training loss: 0.015975933521986008 = 0.009144102223217487 + 0.001 * 6.831830024719238
Epoch 650, val loss: 1.1624194383621216
Epoch 660, training loss: 0.015488134697079659 = 0.008661202155053616 + 0.001 * 6.826932430267334
Epoch 660, val loss: 1.1714714765548706
Epoch 670, training loss: 0.015042334794998169 = 0.008213699795305729 + 0.001 * 6.828635215759277
Epoch 670, val loss: 1.1802769899368286
Epoch 680, training loss: 0.014622984454035759 = 0.007798191159963608 + 0.001 * 6.824792861938477
Epoch 680, val loss: 1.1889159679412842
Epoch 690, training loss: 0.014235571958124638 = 0.007411946076899767 + 0.001 * 6.823625564575195
Epoch 690, val loss: 1.1974154710769653
Epoch 700, training loss: 0.013873068615794182 = 0.007052640430629253 + 0.001 * 6.820427894592285
Epoch 700, val loss: 1.2057133913040161
Epoch 710, training loss: 0.013551194220781326 = 0.006718271411955357 + 0.001 * 6.832922458648682
Epoch 710, val loss: 1.2138495445251465
Epoch 720, training loss: 0.0132317915558815 = 0.0064070262014865875 + 0.001 * 6.824764728546143
Epoch 720, val loss: 1.221817135810852
Epoch 730, training loss: 0.012939974665641785 = 0.006117041688412428 + 0.001 * 6.822932243347168
Epoch 730, val loss: 1.2296040058135986
Epoch 740, training loss: 0.01266357209533453 = 0.005846749525517225 + 0.001 * 6.816822052001953
Epoch 740, val loss: 1.23725426197052
Epoch 750, training loss: 0.012426910921931267 = 0.005594471003860235 + 0.001 * 6.832439422607422
Epoch 750, val loss: 1.2446956634521484
Epoch 760, training loss: 0.012174933217465878 = 0.0053588165901601315 + 0.001 * 6.8161163330078125
Epoch 760, val loss: 1.2519770860671997
Epoch 770, training loss: 0.01195584051311016 = 0.005138453561812639 + 0.001 * 6.817387104034424
Epoch 770, val loss: 1.2591190338134766
Epoch 780, training loss: 0.011752912774682045 = 0.0049321288242936134 + 0.001 * 6.820783615112305
Epoch 780, val loss: 1.2660949230194092
Epoch 790, training loss: 0.011550942435860634 = 0.004738758783787489 + 0.001 * 6.812183856964111
Epoch 790, val loss: 1.2729158401489258
Epoch 800, training loss: 0.01137407124042511 = 0.004557350184768438 + 0.001 * 6.816720485687256
Epoch 800, val loss: 1.2796064615249634
Epoch 810, training loss: 0.011195532977581024 = 0.0043870192021131516 + 0.001 * 6.80851411819458
Epoch 810, val loss: 1.2861226797103882
Epoch 820, training loss: 0.011043762788176537 = 0.004226855468004942 + 0.001 * 6.8169074058532715
Epoch 820, val loss: 1.2924813032150269
Epoch 830, training loss: 0.010890820994973183 = 0.004076098557561636 + 0.001 * 6.814721584320068
Epoch 830, val loss: 1.298718810081482
Epoch 840, training loss: 0.010741930454969406 = 0.003934032283723354 + 0.001 * 6.80789852142334
Epoch 840, val loss: 1.3047956228256226
Epoch 850, training loss: 0.010617699474096298 = 0.0038000494241714478 + 0.001 * 6.817649841308594
Epoch 850, val loss: 1.310759425163269
Epoch 860, training loss: 0.010484050959348679 = 0.0036735516041517258 + 0.001 * 6.8104987144470215
Epoch 860, val loss: 1.316601037979126
Epoch 870, training loss: 0.010367904789745808 = 0.0035539858508855104 + 0.001 * 6.813918590545654
Epoch 870, val loss: 1.3222897052764893
Epoch 880, training loss: 0.010243777185678482 = 0.003440877189859748 + 0.001 * 6.8028998374938965
Epoch 880, val loss: 1.327874779701233
Epoch 890, training loss: 0.0101334722712636 = 0.0033337820786982775 + 0.001 * 6.799689769744873
Epoch 890, val loss: 1.3333550691604614
Epoch 900, training loss: 0.0100372564047575 = 0.003232275601476431 + 0.001 * 6.804980754852295
Epoch 900, val loss: 1.338728904724121
Epoch 910, training loss: 0.009932689368724823 = 0.003135987790301442 + 0.001 * 6.796700954437256
Epoch 910, val loss: 1.3439555168151855
Epoch 920, training loss: 0.009857759810984135 = 0.0030445544980466366 + 0.001 * 6.813204765319824
Epoch 920, val loss: 1.3490829467773438
Epoch 930, training loss: 0.009760191664099693 = 0.0029576625674962997 + 0.001 * 6.8025288581848145
Epoch 930, val loss: 1.354127049446106
Epoch 940, training loss: 0.00967096071690321 = 0.0028750270139425993 + 0.001 * 6.795933723449707
Epoch 940, val loss: 1.3590610027313232
Epoch 950, training loss: 0.009590488858520985 = 0.002796353306621313 + 0.001 * 6.794135093688965
Epoch 950, val loss: 1.3639004230499268
Epoch 960, training loss: 0.009535394608974457 = 0.0027214016299694777 + 0.001 * 6.813992500305176
Epoch 960, val loss: 1.368666410446167
Epoch 970, training loss: 0.009443096816539764 = 0.0026499556843191385 + 0.001 * 6.793140888214111
Epoch 970, val loss: 1.3733110427856445
Epoch 980, training loss: 0.00937024038285017 = 0.002581792650744319 + 0.001 * 6.788447856903076
Epoch 980, val loss: 1.377876877784729
Epoch 990, training loss: 0.009313296526670456 = 0.0025167111307382584 + 0.001 * 6.796584606170654
Epoch 990, val loss: 1.382347583770752
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.956699252128601 = 1.9483253955841064 + 0.001 * 8.373892784118652
Epoch 0, val loss: 1.9560554027557373
Epoch 10, training loss: 1.9472566843032837 = 1.938882827758789 + 0.001 * 8.373821258544922
Epoch 10, val loss: 1.946374773979187
Epoch 20, training loss: 1.9353357553482056 = 1.92696213722229 + 0.001 * 8.373591423034668
Epoch 20, val loss: 1.9339929819107056
Epoch 30, training loss: 1.9182639122009277 = 1.90989089012146 + 0.001 * 8.373048782348633
Epoch 30, val loss: 1.9162455797195435
Epoch 40, training loss: 1.8924615383148193 = 1.8840899467468262 + 0.001 * 8.371630668640137
Epoch 40, val loss: 1.889649510383606
Epoch 50, training loss: 1.8550634384155273 = 1.8466962575912476 + 0.001 * 8.367142677307129
Epoch 50, val loss: 1.8525995016098022
Epoch 60, training loss: 1.811124563217163 = 1.8027762174606323 + 0.001 * 8.348353385925293
Epoch 60, val loss: 1.8132299184799194
Epoch 70, training loss: 1.7721917629241943 = 1.763953447341919 + 0.001 * 8.238301277160645
Epoch 70, val loss: 1.7810509204864502
Epoch 80, training loss: 1.7236860990524292 = 1.7158708572387695 + 0.001 * 7.815290927886963
Epoch 80, val loss: 1.737351417541504
Epoch 90, training loss: 1.6569334268569946 = 1.6492348909378052 + 0.001 * 7.698525428771973
Epoch 90, val loss: 1.6790821552276611
Epoch 100, training loss: 1.5708097219467163 = 1.5632295608520508 + 0.001 * 7.580219268798828
Epoch 100, val loss: 1.6080904006958008
Epoch 110, training loss: 1.4723620414733887 = 1.4649797677993774 + 0.001 * 7.382243633270264
Epoch 110, val loss: 1.5260231494903564
Epoch 120, training loss: 1.371792197227478 = 1.364504098892212 + 0.001 * 7.288144588470459
Epoch 120, val loss: 1.4439119100570679
Epoch 130, training loss: 1.270105004310608 = 1.2628649473190308 + 0.001 * 7.240085124969482
Epoch 130, val loss: 1.3627293109893799
Epoch 140, training loss: 1.1661250591278076 = 1.1589443683624268 + 0.001 * 7.1806793212890625
Epoch 140, val loss: 1.2827799320220947
Epoch 150, training loss: 1.0599344968795776 = 1.0528205633163452 + 0.001 * 7.113992214202881
Epoch 150, val loss: 1.2026338577270508
Epoch 160, training loss: 0.9542136788368225 = 0.9471313953399658 + 0.001 * 7.0822553634643555
Epoch 160, val loss: 1.123572587966919
Epoch 170, training loss: 0.8531063795089722 = 0.8460345268249512 + 0.001 * 7.071833610534668
Epoch 170, val loss: 1.0489882230758667
Epoch 180, training loss: 0.7601986527442932 = 0.7531383633613586 + 0.001 * 7.060303688049316
Epoch 180, val loss: 0.9823971390724182
Epoch 190, training loss: 0.6769257187843323 = 0.6698741316795349 + 0.001 * 7.051609039306641
Epoch 190, val loss: 0.9254857897758484
Epoch 200, training loss: 0.602940022945404 = 0.5958972573280334 + 0.001 * 7.042761325836182
Epoch 200, val loss: 0.8780585527420044
Epoch 210, training loss: 0.5374162793159485 = 0.5303843021392822 + 0.001 * 7.031951904296875
Epoch 210, val loss: 0.8390607833862305
Epoch 220, training loss: 0.47932368516921997 = 0.4723067581653595 + 0.001 * 7.016937732696533
Epoch 220, val loss: 0.8075888752937317
Epoch 230, training loss: 0.42743974924087524 = 0.4204394817352295 + 0.001 * 7.000273704528809
Epoch 230, val loss: 0.7825113534927368
Epoch 240, training loss: 0.38067781925201416 = 0.3736964762210846 + 0.001 * 6.9813337326049805
Epoch 240, val loss: 0.7629231810569763
Epoch 250, training loss: 0.3381020426750183 = 0.33113744854927063 + 0.001 * 6.964598178863525
Epoch 250, val loss: 0.7475916743278503
Epoch 260, training loss: 0.29904964566230774 = 0.2920946478843689 + 0.001 * 6.954999923706055
Epoch 260, val loss: 0.735348641872406
Epoch 270, training loss: 0.2629951536655426 = 0.2560434937477112 + 0.001 * 6.951646327972412
Epoch 270, val loss: 0.7256174087524414
Epoch 280, training loss: 0.22965288162231445 = 0.2227042317390442 + 0.001 * 6.948652267456055
Epoch 280, val loss: 0.7182457447052002
Epoch 290, training loss: 0.1990485042333603 = 0.1921008825302124 + 0.001 * 6.947623252868652
Epoch 290, val loss: 0.7130107283592224
Epoch 300, training loss: 0.17154675722122192 = 0.16460031270980835 + 0.001 * 6.946445465087891
Epoch 300, val loss: 0.7101955413818359
Epoch 310, training loss: 0.14751675724983215 = 0.14056538045406342 + 0.001 * 6.951380729675293
Epoch 310, val loss: 0.7100399136543274
Epoch 320, training loss: 0.12700572609901428 = 0.12006089091300964 + 0.001 * 6.944835662841797
Epoch 320, val loss: 0.7124984860420227
Epoch 330, training loss: 0.10978185385465622 = 0.10283897072076797 + 0.001 * 6.942880630493164
Epoch 330, val loss: 0.7177014946937561
Epoch 340, training loss: 0.09541283547878265 = 0.08846928924322128 + 0.001 * 6.943548202514648
Epoch 340, val loss: 0.7252464890480042
Epoch 350, training loss: 0.08343325555324554 = 0.07649408280849457 + 0.001 * 6.93917179107666
Epoch 350, val loss: 0.7349081039428711
Epoch 360, training loss: 0.07343858480453491 = 0.06650178879499435 + 0.001 * 6.936794281005859
Epoch 360, val loss: 0.7461408972740173
Epoch 370, training loss: 0.06508666276931763 = 0.05814823508262634 + 0.001 * 6.938426494598389
Epoch 370, val loss: 0.7586266398429871
Epoch 380, training loss: 0.05807715281844139 = 0.05114721134305 + 0.001 * 6.929940223693848
Epoch 380, val loss: 0.7719252109527588
Epoch 390, training loss: 0.05218711867928505 = 0.045260533690452576 + 0.001 * 6.926586151123047
Epoch 390, val loss: 0.7856898903846741
Epoch 400, training loss: 0.04722313955426216 = 0.04028983414173126 + 0.001 * 6.9333038330078125
Epoch 400, val loss: 0.799675464630127
Epoch 410, training loss: 0.042987316846847534 = 0.0360703319311142 + 0.001 * 6.916984558105469
Epoch 410, val loss: 0.813650369644165
Epoch 420, training loss: 0.039392296224832535 = 0.03246685862541199 + 0.001 * 6.925436019897461
Epoch 420, val loss: 0.8275089263916016
Epoch 430, training loss: 0.036279141902923584 = 0.029371250420808792 + 0.001 * 6.907892227172852
Epoch 430, val loss: 0.8410791158676147
Epoch 440, training loss: 0.03361249715089798 = 0.0266963392496109 + 0.001 * 6.9161577224731445
Epoch 440, val loss: 0.8545122742652893
Epoch 450, training loss: 0.0312756784260273 = 0.024371298030018806 + 0.001 * 6.904381275177002
Epoch 450, val loss: 0.8675037026405334
Epoch 460, training loss: 0.02923300862312317 = 0.02233915776014328 + 0.001 * 6.893850326538086
Epoch 460, val loss: 0.8802699446678162
Epoch 470, training loss: 0.02747182920575142 = 0.020553868263959885 + 0.001 * 6.9179606437683105
Epoch 470, val loss: 0.8926336765289307
Epoch 480, training loss: 0.025875266641378403 = 0.018977168947458267 + 0.001 * 6.898097515106201
Epoch 480, val loss: 0.9046422839164734
Epoch 490, training loss: 0.024457363411784172 = 0.017578139901161194 + 0.001 * 6.879223346710205
Epoch 490, val loss: 0.9163134098052979
Epoch 500, training loss: 0.02320317178964615 = 0.016331041231751442 + 0.001 * 6.872129917144775
Epoch 500, val loss: 0.9276477694511414
Epoch 510, training loss: 0.022092003375291824 = 0.015215007588267326 + 0.001 * 6.876996040344238
Epoch 510, val loss: 0.9386711716651917
Epoch 520, training loss: 0.021089699119329453 = 0.014212647452950478 + 0.001 * 6.877051830291748
Epoch 520, val loss: 0.9493474960327148
Epoch 530, training loss: 0.02017754316329956 = 0.013308997265994549 + 0.001 * 6.868546009063721
Epoch 530, val loss: 0.9597339630126953
Epoch 540, training loss: 0.019358284771442413 = 0.012491358444094658 + 0.001 * 6.8669257164001465
Epoch 540, val loss: 0.9697527885437012
Epoch 550, training loss: 0.018614310771226883 = 0.011749288067221642 + 0.001 * 6.865021705627441
Epoch 550, val loss: 0.9795553684234619
Epoch 560, training loss: 0.017925627529621124 = 0.011073848232626915 + 0.001 * 6.851779937744141
Epoch 560, val loss: 0.989035964012146
Epoch 570, training loss: 0.01730368658900261 = 0.010457396507263184 + 0.001 * 6.846290588378906
Epoch 570, val loss: 0.9982640147209167
Epoch 580, training loss: 0.016736481338739395 = 0.009893217124044895 + 0.001 * 6.843263149261475
Epoch 580, val loss: 1.0072150230407715
Epoch 590, training loss: 0.01623166725039482 = 0.009375565685331821 + 0.001 * 6.856100082397461
Epoch 590, val loss: 1.0159107446670532
Epoch 600, training loss: 0.0157387163490057 = 0.008899485692381859 + 0.001 * 6.839230537414551
Epoch 600, val loss: 1.0243892669677734
Epoch 610, training loss: 0.015298334881663322 = 0.008460069075226784 + 0.001 * 6.8382649421691895
Epoch 610, val loss: 1.0326306819915771
Epoch 620, training loss: 0.014915559440851212 = 0.008052671328186989 + 0.001 * 6.862887859344482
Epoch 620, val loss: 1.040722370147705
Epoch 630, training loss: 0.014508765190839767 = 0.007672497536987066 + 0.001 * 6.836266994476318
Epoch 630, val loss: 1.0487116575241089
Epoch 640, training loss: 0.014138169586658478 = 0.007315241731703281 + 0.001 * 6.822926998138428
Epoch 640, val loss: 1.0566565990447998
Epoch 650, training loss: 0.01382371410727501 = 0.006978614255785942 + 0.001 * 6.845098972320557
Epoch 650, val loss: 1.0644588470458984
Epoch 660, training loss: 0.013499625027179718 = 0.006661782972514629 + 0.001 * 6.837841033935547
Epoch 660, val loss: 1.0722233057022095
Epoch 670, training loss: 0.013184959068894386 = 0.006363662891089916 + 0.001 * 6.821296215057373
Epoch 670, val loss: 1.0798654556274414
Epoch 680, training loss: 0.012898740358650684 = 0.006083547603338957 + 0.001 * 6.815192222595215
Epoch 680, val loss: 1.0873949527740479
Epoch 690, training loss: 0.012669148854911327 = 0.005820583086460829 + 0.001 * 6.848565578460693
Epoch 690, val loss: 1.094768762588501
Epoch 700, training loss: 0.012395044788718224 = 0.005573991220444441 + 0.001 * 6.821053504943848
Epoch 700, val loss: 1.102031946182251
Epoch 710, training loss: 0.012179645709693432 = 0.005342612508684397 + 0.001 * 6.837032794952393
Epoch 710, val loss: 1.1091755628585815
Epoch 720, training loss: 0.011937156319618225 = 0.005125563591718674 + 0.001 * 6.8115925788879395
Epoch 720, val loss: 1.1161983013153076
Epoch 730, training loss: 0.011767054907977581 = 0.004921793472021818 + 0.001 * 6.845261096954346
Epoch 730, val loss: 1.1230270862579346
Epoch 740, training loss: 0.011534703895449638 = 0.004730484914034605 + 0.001 * 6.804218769073486
Epoch 740, val loss: 1.129788875579834
Epoch 750, training loss: 0.011367259547114372 = 0.00455072708427906 + 0.001 * 6.816532135009766
Epoch 750, val loss: 1.1363188028335571
Epoch 760, training loss: 0.011189814656972885 = 0.0043817018158733845 + 0.001 * 6.808112621307373
Epoch 760, val loss: 1.1427886486053467
Epoch 770, training loss: 0.011042702943086624 = 0.00422251783311367 + 0.001 * 6.820184230804443
Epoch 770, val loss: 1.1491258144378662
Epoch 780, training loss: 0.01085282489657402 = 0.004072506912052631 + 0.001 * 6.780318260192871
Epoch 780, val loss: 1.1552599668502808
Epoch 790, training loss: 0.010767688974738121 = 0.003931065090000629 + 0.001 * 6.836623668670654
Epoch 790, val loss: 1.1613086462020874
Epoch 800, training loss: 0.010570903308689594 = 0.0037975546438246965 + 0.001 * 6.773348808288574
Epoch 800, val loss: 1.16719651222229
Epoch 810, training loss: 0.01045790035277605 = 0.0036713823210448027 + 0.001 * 6.786518096923828
Epoch 810, val loss: 1.173064947128296
Epoch 820, training loss: 0.01034117303788662 = 0.0035520594101399183 + 0.001 * 6.7891130447387695
Epoch 820, val loss: 1.1787368059158325
Epoch 830, training loss: 0.010245144367218018 = 0.003439144929870963 + 0.001 * 6.805998802185059
Epoch 830, val loss: 1.1843098402023315
Epoch 840, training loss: 0.010104837827384472 = 0.003332157153636217 + 0.001 * 6.772680282592773
Epoch 840, val loss: 1.189727544784546
Epoch 850, training loss: 0.009995369240641594 = 0.0032306741923093796 + 0.001 * 6.764694690704346
Epoch 850, val loss: 1.1950205564498901
Epoch 860, training loss: 0.009910755790770054 = 0.0031343845184892416 + 0.001 * 6.776371002197266
Epoch 860, val loss: 1.2003421783447266
Epoch 870, training loss: 0.009801187552511692 = 0.003042917000129819 + 0.001 * 6.758270263671875
Epoch 870, val loss: 1.2054680585861206
Epoch 880, training loss: 0.009730104357004166 = 0.002955935662612319 + 0.001 * 6.774168491363525
Epoch 880, val loss: 1.2104177474975586
Epoch 890, training loss: 0.009641909971833229 = 0.0028731778729707003 + 0.001 * 6.768731594085693
Epoch 890, val loss: 1.2153624296188354
Epoch 900, training loss: 0.009537279605865479 = 0.0027943872846663 + 0.001 * 6.742892265319824
Epoch 900, val loss: 1.220123529434204
Epoch 910, training loss: 0.009455029852688313 = 0.002719275886192918 + 0.001 * 6.735754013061523
Epoch 910, val loss: 1.2248966693878174
Epoch 920, training loss: 0.009406054392457008 = 0.002647650195285678 + 0.001 * 6.75840425491333
Epoch 920, val loss: 1.2295564413070679
Epoch 930, training loss: 0.009341083467006683 = 0.0025793262757360935 + 0.001 * 6.7617573738098145
Epoch 930, val loss: 1.2341231107711792
Epoch 940, training loss: 0.009284403175115585 = 0.0025140801444649696 + 0.001 * 6.770322322845459
Epoch 940, val loss: 1.2386022806167603
Epoch 950, training loss: 0.009179949760437012 = 0.0024517597630620003 + 0.001 * 6.728188991546631
Epoch 950, val loss: 1.2428916692733765
Epoch 960, training loss: 0.009121181443333626 = 0.002392188645899296 + 0.001 * 6.728991985321045
Epoch 960, val loss: 1.2471346855163574
Epoch 970, training loss: 0.00906400941312313 = 0.0023351770360022783 + 0.001 * 6.728832244873047
Epoch 970, val loss: 1.2513030767440796
Epoch 980, training loss: 0.009009961038827896 = 0.0022805982735008 + 0.001 * 6.729362487792969
Epoch 980, val loss: 1.2554653882980347
Epoch 990, training loss: 0.008968863636255264 = 0.0022282879799604416 + 0.001 * 6.740575313568115
Epoch 990, val loss: 1.25946843624115
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9435614347457886 = 1.935187578201294 + 0.001 * 8.373897552490234
Epoch 0, val loss: 1.9260826110839844
Epoch 10, training loss: 1.9335225820541382 = 1.9251487255096436 + 0.001 * 8.373845100402832
Epoch 10, val loss: 1.9171583652496338
Epoch 20, training loss: 1.9215803146362305 = 1.913206696510315 + 0.001 * 8.373653411865234
Epoch 20, val loss: 1.9063864946365356
Epoch 30, training loss: 1.9051860570907593 = 1.8968127965927124 + 0.001 * 8.37321662902832
Epoch 30, val loss: 1.8914376497268677
Epoch 40, training loss: 1.8811320066452026 = 1.8727598190307617 + 0.001 * 8.372185707092285
Epoch 40, val loss: 1.8698233366012573
Epoch 50, training loss: 1.8473308086395264 = 1.8389614820480347 + 0.001 * 8.369319915771484
Epoch 50, val loss: 1.8410851955413818
Epoch 60, training loss: 1.8084906339645386 = 1.8001320362091064 + 0.001 * 8.358597755432129
Epoch 60, val loss: 1.8119617700576782
Epoch 70, training loss: 1.7719714641571045 = 1.7636690139770508 + 0.001 * 8.302489280700684
Epoch 70, val loss: 1.7855262756347656
Epoch 80, training loss: 1.7233467102050781 = 1.7154185771942139 + 0.001 * 7.928133487701416
Epoch 80, val loss: 1.7423793077468872
Epoch 90, training loss: 1.6553595066070557 = 1.647598147392273 + 0.001 * 7.761322975158691
Epoch 90, val loss: 1.6822421550750732
Epoch 100, training loss: 1.5682172775268555 = 1.5604771375656128 + 0.001 * 7.740116119384766
Epoch 100, val loss: 1.6113656759262085
Epoch 110, training loss: 1.4729183912277222 = 1.46519935131073 + 0.001 * 7.719013214111328
Epoch 110, val loss: 1.5349138975143433
Epoch 120, training loss: 1.3798774480819702 = 1.3722095489501953 + 0.001 * 7.667863845825195
Epoch 120, val loss: 1.4625215530395508
Epoch 130, training loss: 1.2895784378051758 = 1.2820634841918945 + 0.001 * 7.514997482299805
Epoch 130, val loss: 1.3929024934768677
Epoch 140, training loss: 1.2001159191131592 = 1.192728877067566 + 0.001 * 7.387002468109131
Epoch 140, val loss: 1.325147271156311
Epoch 150, training loss: 1.1126585006713867 = 1.105319619178772 + 0.001 * 7.338851451873779
Epoch 150, val loss: 1.2602038383483887
Epoch 160, training loss: 1.0297669172286987 = 1.0225311517715454 + 0.001 * 7.235783576965332
Epoch 160, val loss: 1.1999784708023071
Epoch 170, training loss: 0.9528657793998718 = 0.9457313418388367 + 0.001 * 7.134456634521484
Epoch 170, val loss: 1.1455316543579102
Epoch 180, training loss: 0.881937563419342 = 0.8748490810394287 + 0.001 * 7.088465690612793
Epoch 180, val loss: 1.0971194505691528
Epoch 190, training loss: 0.816204309463501 = 0.8091288208961487 + 0.001 * 7.075459957122803
Epoch 190, val loss: 1.0546493530273438
Epoch 200, training loss: 0.754187822341919 = 0.747127890586853 + 0.001 * 7.05990743637085
Epoch 200, val loss: 1.0166656970977783
Epoch 210, training loss: 0.6943007111549377 = 0.6872521042823792 + 0.001 * 7.048584461212158
Epoch 210, val loss: 0.9808885455131531
Epoch 220, training loss: 0.6355748772621155 = 0.6285371780395508 + 0.001 * 7.0376811027526855
Epoch 220, val loss: 0.9460846781730652
Epoch 230, training loss: 0.5783708095550537 = 0.5713453888893127 + 0.001 * 7.0254292488098145
Epoch 230, val loss: 0.9124073386192322
Epoch 240, training loss: 0.5239384174346924 = 0.5169287323951721 + 0.001 * 7.009679794311523
Epoch 240, val loss: 0.8811525106430054
Epoch 250, training loss: 0.47297027707099915 = 0.46598193049430847 + 0.001 * 6.988338470458984
Epoch 250, val loss: 0.8540186882019043
Epoch 260, training loss: 0.42543941736221313 = 0.4184759855270386 + 0.001 * 6.96342134475708
Epoch 260, val loss: 0.8318707942962646
Epoch 270, training loss: 0.38099104166030884 = 0.3740505576133728 + 0.001 * 6.94047212600708
Epoch 270, val loss: 0.8147412538528442
Epoch 280, training loss: 0.3392108678817749 = 0.332293301820755 + 0.001 * 6.917552947998047
Epoch 280, val loss: 0.8022028803825378
Epoch 290, training loss: 0.2997859716415405 = 0.2928791642189026 + 0.001 * 6.906810283660889
Epoch 290, val loss: 0.7937197089195251
Epoch 300, training loss: 0.26272526383399963 = 0.25582560896873474 + 0.001 * 6.899641036987305
Epoch 300, val loss: 0.7891965508460999
Epoch 310, training loss: 0.22849901020526886 = 0.22160686552524567 + 0.001 * 6.892144203186035
Epoch 310, val loss: 0.7886669635772705
Epoch 320, training loss: 0.1977335661649704 = 0.19084541499614716 + 0.001 * 6.888156890869141
Epoch 320, val loss: 0.7921707034111023
Epoch 330, training loss: 0.17085668444633484 = 0.1639726459980011 + 0.001 * 6.884031772613525
Epoch 330, val loss: 0.7998104095458984
Epoch 340, training loss: 0.14784401655197144 = 0.14096345007419586 + 0.001 * 6.880572319030762
Epoch 340, val loss: 0.811395525932312
Epoch 350, training loss: 0.1283436268568039 = 0.12147039920091629 + 0.001 * 6.873233795166016
Epoch 350, val loss: 0.8263213038444519
Epoch 360, training loss: 0.11188177019357681 = 0.10501312464475632 + 0.001 * 6.868642330169678
Epoch 360, val loss: 0.843788206577301
Epoch 370, training loss: 0.09799187630414963 = 0.09112654626369476 + 0.001 * 6.865331649780273
Epoch 370, val loss: 0.8629576563835144
Epoch 380, training loss: 0.08625323325395584 = 0.0793878361582756 + 0.001 * 6.865396022796631
Epoch 380, val loss: 0.8830599784851074
Epoch 390, training loss: 0.07631047815084457 = 0.06944926083087921 + 0.001 * 6.861213684082031
Epoch 390, val loss: 0.9035153985023499
Epoch 400, training loss: 0.06787622720003128 = 0.06101757660508156 + 0.001 * 6.8586530685424805
Epoch 400, val loss: 0.9238675832748413
Epoch 410, training loss: 0.0607077032327652 = 0.05384846031665802 + 0.001 * 6.859240531921387
Epoch 410, val loss: 0.9437103271484375
Epoch 420, training loss: 0.054597899317741394 = 0.0477391853928566 + 0.001 * 6.858712196350098
Epoch 420, val loss: 0.9628865718841553
Epoch 430, training loss: 0.04937287047505379 = 0.04251944273710251 + 0.001 * 6.853425979614258
Epoch 430, val loss: 0.9813234806060791
Epoch 440, training loss: 0.04489626735448837 = 0.03804445266723633 + 0.001 * 6.851815223693848
Epoch 440, val loss: 0.9990538358688354
Epoch 450, training loss: 0.04104442149400711 = 0.034192752093076706 + 0.001 * 6.851670265197754
Epoch 450, val loss: 1.016088604927063
Epoch 460, training loss: 0.03771190345287323 = 0.030863458290696144 + 0.001 * 6.848446369171143
Epoch 460, val loss: 1.032444715499878
Epoch 470, training loss: 0.0348183698952198 = 0.02797308936715126 + 0.001 * 6.845279216766357
Epoch 470, val loss: 1.0481635332107544
Epoch 480, training loss: 0.032297223806381226 = 0.025453800335526466 + 0.001 * 6.843422889709473
Epoch 480, val loss: 1.0632884502410889
Epoch 490, training loss: 0.030088316649198532 = 0.02324788086116314 + 0.001 * 6.840435028076172
Epoch 490, val loss: 1.0778237581253052
Epoch 500, training loss: 0.028147635981440544 = 0.02130899764597416 + 0.001 * 6.838637828826904
Epoch 500, val loss: 1.0918424129486084
Epoch 510, training loss: 0.02643546089529991 = 0.01959778554737568 + 0.001 * 6.837675094604492
Epoch 510, val loss: 1.1054739952087402
Epoch 520, training loss: 0.0249188132584095 = 0.01808159612119198 + 0.001 * 6.837216377258301
Epoch 520, val loss: 1.1185119152069092
Epoch 530, training loss: 0.0235667135566473 = 0.016733204945921898 + 0.001 * 6.833508491516113
Epoch 530, val loss: 1.1311757564544678
Epoch 540, training loss: 0.022357260808348656 = 0.015529850497841835 + 0.001 * 6.8274102210998535
Epoch 540, val loss: 1.1433651447296143
Epoch 550, training loss: 0.021278254687786102 = 0.014452124014496803 + 0.001 * 6.82612943649292
Epoch 550, val loss: 1.1552009582519531
Epoch 560, training loss: 0.020313233137130737 = 0.013483698479831219 + 0.001 * 6.829534530639648
Epoch 560, val loss: 1.1666462421417236
Epoch 570, training loss: 0.019434727728366852 = 0.012610829435288906 + 0.001 * 6.823897361755371
Epoch 570, val loss: 1.177735447883606
Epoch 580, training loss: 0.018644366413354874 = 0.011821646243333817 + 0.001 * 6.822719573974609
Epoch 580, val loss: 1.1884406805038452
Epoch 590, training loss: 0.017924312502145767 = 0.01110603753477335 + 0.001 * 6.81827449798584
Epoch 590, val loss: 1.198860764503479
Epoch 600, training loss: 0.017272185534238815 = 0.010455450974404812 + 0.001 * 6.81673526763916
Epoch 600, val loss: 1.2089565992355347
Epoch 610, training loss: 0.016672536730766296 = 0.009862546809017658 + 0.001 * 6.8099894523620605
Epoch 610, val loss: 1.2187554836273193
Epoch 620, training loss: 0.016153015196323395 = 0.009320465847849846 + 0.001 * 6.83254861831665
Epoch 620, val loss: 1.2282613515853882
Epoch 630, training loss: 0.01563776656985283 = 0.008823824115097523 + 0.001 * 6.813941955566406
Epoch 630, val loss: 1.2375034093856812
Epoch 640, training loss: 0.015175675973296165 = 0.00836764182895422 + 0.001 * 6.808033466339111
Epoch 640, val loss: 1.2464687824249268
Epoch 650, training loss: 0.014746954664587975 = 0.007947755977511406 + 0.001 * 6.799197673797607
Epoch 650, val loss: 1.2552213668823242
Epoch 660, training loss: 0.014364216476678848 = 0.007560376077890396 + 0.001 * 6.803840160369873
Epoch 660, val loss: 1.2636982202529907
Epoch 670, training loss: 0.01400068774819374 = 0.007202321197837591 + 0.001 * 6.798366546630859
Epoch 670, val loss: 1.2719523906707764
Epoch 680, training loss: 0.013667833060026169 = 0.006870678626000881 + 0.001 * 6.797153949737549
Epoch 680, val loss: 1.279983401298523
Epoch 690, training loss: 0.013359279371798038 = 0.006562867667526007 + 0.001 * 6.796411514282227
Epoch 690, val loss: 1.287834882736206
Epoch 700, training loss: 0.013080937787890434 = 0.006276695057749748 + 0.001 * 6.804243087768555
Epoch 700, val loss: 1.2954376935958862
Epoch 710, training loss: 0.012796987779438496 = 0.006010204553604126 + 0.001 * 6.786782741546631
Epoch 710, val loss: 1.3028780221939087
Epoch 720, training loss: 0.012555072084069252 = 0.00576166482642293 + 0.001 * 6.793406963348389
Epoch 720, val loss: 1.3101028203964233
Epoch 730, training loss: 0.012313381768763065 = 0.0055295065976679325 + 0.001 * 6.783874988555908
Epoch 730, val loss: 1.3171871900558472
Epoch 740, training loss: 0.012097850441932678 = 0.005312301684170961 + 0.001 * 6.785548210144043
Epoch 740, val loss: 1.3240876197814941
Epoch 750, training loss: 0.011893199756741524 = 0.005108771845698357 + 0.001 * 6.784428119659424
Epoch 750, val loss: 1.3307857513427734
Epoch 760, training loss: 0.011698749847710133 = 0.004917805548757315 + 0.001 * 6.780943870544434
Epoch 760, val loss: 1.3373664617538452
Epoch 770, training loss: 0.01152205653488636 = 0.004738432355225086 + 0.001 * 6.783624172210693
Epoch 770, val loss: 1.343781590461731
Epoch 780, training loss: 0.01136365719139576 = 0.004569705110043287 + 0.001 * 6.793951988220215
Epoch 780, val loss: 1.3500595092773438
Epoch 790, training loss: 0.011194129474461079 = 0.0044107940047979355 + 0.001 * 6.783335208892822
Epoch 790, val loss: 1.356166124343872
Epoch 800, training loss: 0.011034926399588585 = 0.004260951187461615 + 0.001 * 6.773974418640137
Epoch 800, val loss: 1.3621869087219238
Epoch 810, training loss: 0.010893755592405796 = 0.004119485151022673 + 0.001 * 6.774270057678223
Epoch 810, val loss: 1.3680472373962402
Epoch 820, training loss: 0.01075945608317852 = 0.003985786810517311 + 0.001 * 6.77366828918457
Epoch 820, val loss: 1.3737887144088745
Epoch 830, training loss: 0.010624921880662441 = 0.0038593008648604155 + 0.001 * 6.765620708465576
Epoch 830, val loss: 1.3793883323669434
Epoch 840, training loss: 0.010543443262577057 = 0.0037394941318780184 + 0.001 * 6.803948402404785
Epoch 840, val loss: 1.3848735094070435
Epoch 850, training loss: 0.010387694463133812 = 0.0036259121261537075 + 0.001 * 6.761782646179199
Epoch 850, val loss: 1.3902225494384766
Epoch 860, training loss: 0.010280772112309933 = 0.003518130397424102 + 0.001 * 6.762641429901123
Epoch 860, val loss: 1.3954658508300781
Epoch 870, training loss: 0.010186349973082542 = 0.0034157538320869207 + 0.001 * 6.770595550537109
Epoch 870, val loss: 1.400590181350708
Epoch 880, training loss: 0.010082381777465343 = 0.0033184317871928215 + 0.001 * 6.763949871063232
Epoch 880, val loss: 1.4056211709976196
Epoch 890, training loss: 0.009993400424718857 = 0.0032258392311632633 + 0.001 * 6.767560958862305
Epoch 890, val loss: 1.410565733909607
Epoch 900, training loss: 0.009906984865665436 = 0.003137698397040367 + 0.001 * 6.769285678863525
Epoch 900, val loss: 1.4153521060943604
Epoch 910, training loss: 0.009814025834202766 = 0.0030537103302776814 + 0.001 * 6.76031494140625
Epoch 910, val loss: 1.4200859069824219
Epoch 920, training loss: 0.009729286655783653 = 0.002973598660901189 + 0.001 * 6.755687236785889
Epoch 920, val loss: 1.424717903137207
Epoch 930, training loss: 0.009668350219726562 = 0.00289715058170259 + 0.001 * 6.7711992263793945
Epoch 930, val loss: 1.429255485534668
Epoch 940, training loss: 0.009574969299137592 = 0.002824119059368968 + 0.001 * 6.750849723815918
Epoch 940, val loss: 1.4337040185928345
Epoch 950, training loss: 0.009513895958662033 = 0.002754336455836892 + 0.001 * 6.759559631347656
Epoch 950, val loss: 1.4380767345428467
Epoch 960, training loss: 0.009434811770915985 = 0.002687572967261076 + 0.001 * 6.7472381591796875
Epoch 960, val loss: 1.4423481225967407
Epoch 970, training loss: 0.00939740240573883 = 0.002623677719384432 + 0.001 * 6.77372407913208
Epoch 970, val loss: 1.4465272426605225
Epoch 980, training loss: 0.009313203394412994 = 0.0025624921545386314 + 0.001 * 6.750710964202881
Epoch 980, val loss: 1.4506553411483765
Epoch 990, training loss: 0.009242243133485317 = 0.0025038335006684065 + 0.001 * 6.738409042358398
Epoch 990, val loss: 1.4547067880630493
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9501235485076904 = 1.9417496919631958 + 0.001 * 8.373912811279297
Epoch 0, val loss: 1.9373008012771606
Epoch 10, training loss: 1.9400196075439453 = 1.9316457509994507 + 0.001 * 8.373862266540527
Epoch 10, val loss: 1.927907943725586
Epoch 20, training loss: 1.9275856018066406 = 1.9192118644714355 + 0.001 * 8.373696327209473
Epoch 20, val loss: 1.9157823324203491
Epoch 30, training loss: 1.9100604057312012 = 1.9016871452331543 + 0.001 * 8.373300552368164
Epoch 30, val loss: 1.8984967470169067
Epoch 40, training loss: 1.8841747045516968 = 1.8758023977279663 + 0.001 * 8.372300148010254
Epoch 40, val loss: 1.8732961416244507
Epoch 50, training loss: 1.8479045629501343 = 1.8395352363586426 + 0.001 * 8.369277954101562
Epoch 50, val loss: 1.8400214910507202
Epoch 60, training loss: 1.8064824342727661 = 1.7981244325637817 + 0.001 * 8.35798454284668
Epoch 60, val loss: 1.8064448833465576
Epoch 70, training loss: 1.7670269012451172 = 1.758723497390747 + 0.001 * 8.303375244140625
Epoch 70, val loss: 1.7765368223190308
Epoch 80, training loss: 1.7148394584655762 = 1.706861972808838 + 0.001 * 7.97746467590332
Epoch 80, val loss: 1.7316466569900513
Epoch 90, training loss: 1.6428369283676147 = 1.635047435760498 + 0.001 * 7.7894816398620605
Epoch 90, val loss: 1.669548749923706
Epoch 100, training loss: 1.5540128946304321 = 1.5462663173675537 + 0.001 * 7.746524333953857
Epoch 100, val loss: 1.5969187021255493
Epoch 110, training loss: 1.460608720779419 = 1.4528985023498535 + 0.001 * 7.710165977478027
Epoch 110, val loss: 1.5211681127548218
Epoch 120, training loss: 1.3712847232818604 = 1.363655924797058 + 0.001 * 7.62877082824707
Epoch 120, val loss: 1.4526820182800293
Epoch 130, training loss: 1.2850497961044312 = 1.2776554822921753 + 0.001 * 7.394291877746582
Epoch 130, val loss: 1.388211727142334
Epoch 140, training loss: 1.1988178491592407 = 1.1915491819381714 + 0.001 * 7.268671035766602
Epoch 140, val loss: 1.3252748250961304
Epoch 150, training loss: 1.1110899448394775 = 1.1038970947265625 + 0.001 * 7.192883014678955
Epoch 150, val loss: 1.2617738246917725
Epoch 160, training loss: 1.0220813751220703 = 1.014929175376892 + 0.001 * 7.1522345542907715
Epoch 160, val loss: 1.1975239515304565
Epoch 170, training loss: 0.9323326945304871 = 0.9252060651779175 + 0.001 * 7.126644134521484
Epoch 170, val loss: 1.1319373846054077
Epoch 180, training loss: 0.8431666493415833 = 0.8360569477081299 + 0.001 * 7.10968017578125
Epoch 180, val loss: 1.0659934282302856
Epoch 190, training loss: 0.7569170594215393 = 0.7498186826705933 + 0.001 * 7.098388195037842
Epoch 190, val loss: 1.0013155937194824
Epoch 200, training loss: 0.6765463352203369 = 0.6694595813751221 + 0.001 * 7.086742877960205
Epoch 200, val loss: 0.9407356381416321
Epoch 210, training loss: 0.6044812202453613 = 0.5974061489105225 + 0.001 * 7.075053691864014
Epoch 210, val loss: 0.8880155682563782
Epoch 220, training loss: 0.5416405200958252 = 0.5345796942710876 + 0.001 * 7.06082010269165
Epoch 220, val loss: 0.8448725938796997
Epoch 230, training loss: 0.486945241689682 = 0.4799034893512726 + 0.001 * 7.041746616363525
Epoch 230, val loss: 0.8112410306930542
Epoch 240, training loss: 0.43804967403411865 = 0.43102458119392395 + 0.001 * 7.025091171264648
Epoch 240, val loss: 0.7848084568977356
Epoch 250, training loss: 0.3924896717071533 = 0.38547196984291077 + 0.001 * 7.017698287963867
Epoch 250, val loss: 0.7636943459510803
Epoch 260, training loss: 0.3487624526023865 = 0.34174734354019165 + 0.001 * 7.015115737915039
Epoch 260, val loss: 0.7469164133071899
Epoch 270, training loss: 0.30675235390663147 = 0.29973843693733215 + 0.001 * 7.013914108276367
Epoch 270, val loss: 0.7342722415924072
Epoch 280, training loss: 0.26738041639328003 = 0.26036766171455383 + 0.001 * 7.01276159286499
Epoch 280, val loss: 0.7256954908370972
Epoch 290, training loss: 0.23178082704544067 = 0.22476914525032043 + 0.001 * 7.011678695678711
Epoch 290, val loss: 0.7214246988296509
Epoch 300, training loss: 0.2006298452615738 = 0.1936192363500595 + 0.001 * 7.010607719421387
Epoch 300, val loss: 0.7211841940879822
Epoch 310, training loss: 0.17404600977897644 = 0.1670345515012741 + 0.001 * 7.011460781097412
Epoch 310, val loss: 0.7246944308280945
Epoch 320, training loss: 0.15168660879135132 = 0.14467692375183105 + 0.001 * 7.009685516357422
Epoch 320, val loss: 0.7315973043441772
Epoch 330, training loss: 0.1329975426197052 = 0.12598857283592224 + 0.001 * 7.008970260620117
Epoch 330, val loss: 0.7413119673728943
Epoch 340, training loss: 0.11738039553165436 = 0.11036534607410431 + 0.001 * 7.015049934387207
Epoch 340, val loss: 0.7533387541770935
Epoch 350, training loss: 0.10423853993415833 = 0.09723107516765594 + 0.001 * 7.007465362548828
Epoch 350, val loss: 0.7672078609466553
Epoch 360, training loss: 0.09311804920434952 = 0.08611088991165161 + 0.001 * 7.007155895233154
Epoch 360, val loss: 0.7826125025749207
Epoch 370, training loss: 0.08363811671733856 = 0.07663211971521378 + 0.001 * 7.005996227264404
Epoch 370, val loss: 0.7992018461227417
Epoch 380, training loss: 0.07550966739654541 = 0.06850158423185349 + 0.001 * 7.008085250854492
Epoch 380, val loss: 0.8167177438735962
Epoch 390, training loss: 0.06850241124629974 = 0.06148923188447952 + 0.001 * 7.013181686401367
Epoch 390, val loss: 0.8348647356033325
Epoch 400, training loss: 0.06241386756300926 = 0.055408280342817307 + 0.001 * 7.00558614730835
Epoch 400, val loss: 0.8533877730369568
Epoch 410, training loss: 0.057111673057079315 = 0.05011142045259476 + 0.001 * 7.000251293182373
Epoch 410, val loss: 0.8721756339073181
Epoch 420, training loss: 0.05247775837779045 = 0.045479435473680496 + 0.001 * 6.998323917388916
Epoch 420, val loss: 0.8909770846366882
Epoch 430, training loss: 0.048408180475234985 = 0.04141194000840187 + 0.001 * 6.996242046356201
Epoch 430, val loss: 0.9097884297370911
Epoch 440, training loss: 0.04481935128569603 = 0.0378255732357502 + 0.001 * 6.993776321411133
Epoch 440, val loss: 0.9284790754318237
Epoch 450, training loss: 0.041647862643003464 = 0.034651726484298706 + 0.001 * 6.996134281158447
Epoch 450, val loss: 0.9468945264816284
Epoch 460, training loss: 0.03882678225636482 = 0.03183378279209137 + 0.001 * 6.9929986000061035
Epoch 460, val loss: 0.9650714993476868
Epoch 470, training loss: 0.036313921213150024 = 0.029323924332857132 + 0.001 * 6.989997386932373
Epoch 470, val loss: 0.9830390214920044
Epoch 480, training loss: 0.03407250717282295 = 0.027081722393631935 + 0.001 * 6.990784645080566
Epoch 480, val loss: 1.0005204677581787
Epoch 490, training loss: 0.03206276893615723 = 0.025072438642382622 + 0.001 * 6.990328788757324
Epoch 490, val loss: 1.0177514553070068
Epoch 500, training loss: 0.030252259224653244 = 0.023266883566975594 + 0.001 * 6.985375881195068
Epoch 500, val loss: 1.0345340967178345
Epoch 510, training loss: 0.028618959710001945 = 0.02164037525653839 + 0.001 * 6.978583812713623
Epoch 510, val loss: 1.0510133504867554
Epoch 520, training loss: 0.027156535536050797 = 0.0201717521995306 + 0.001 * 6.984784126281738
Epoch 520, val loss: 1.067062258720398
Epoch 530, training loss: 0.025812750682234764 = 0.018842097371816635 + 0.001 * 6.9706525802612305
Epoch 530, val loss: 1.0827510356903076
Epoch 540, training loss: 0.024621902033686638 = 0.017635390162467957 + 0.001 * 6.986511707305908
Epoch 540, val loss: 1.098015308380127
Epoch 550, training loss: 0.02350117638707161 = 0.01653759554028511 + 0.001 * 6.9635796546936035
Epoch 550, val loss: 1.1128865480422974
Epoch 560, training loss: 0.02251621149480343 = 0.015536795370280743 + 0.001 * 6.979415416717529
Epoch 560, val loss: 1.1273597478866577
Epoch 570, training loss: 0.02159576304256916 = 0.014622473157942295 + 0.001 * 6.973289966583252
Epoch 570, val loss: 1.1414374113082886
Epoch 580, training loss: 0.02073635719716549 = 0.013785459101200104 + 0.001 * 6.950897693634033
Epoch 580, val loss: 1.1551307439804077
Epoch 590, training loss: 0.019977064803242683 = 0.013017700985074043 + 0.001 * 6.959362983703613
Epoch 590, val loss: 1.1684426069259644
Epoch 600, training loss: 0.019255807623267174 = 0.01231180690228939 + 0.001 * 6.943999767303467
Epoch 600, val loss: 1.1813808679580688
Epoch 610, training loss: 0.01861695758998394 = 0.011661515571177006 + 0.001 * 6.955441474914551
Epoch 610, val loss: 1.1940191984176636
Epoch 620, training loss: 0.017992131412029266 = 0.011061673052608967 + 0.001 * 6.93045711517334
Epoch 620, val loss: 1.2062867879867554
Epoch 630, training loss: 0.017472190782427788 = 0.01050707045942545 + 0.001 * 6.965120315551758
Epoch 630, val loss: 1.2181875705718994
Epoch 640, training loss: 0.016940725967288017 = 0.009993570856750011 + 0.001 * 6.947155475616455
Epoch 640, val loss: 1.2298146486282349
Epoch 650, training loss: 0.01644379459321499 = 0.00951746292412281 + 0.001 * 6.926331996917725
Epoch 650, val loss: 1.2411705255508423
Epoch 660, training loss: 0.015977604314684868 = 0.009075447916984558 + 0.001 * 6.90215539932251
Epoch 660, val loss: 1.252191424369812
Epoch 670, training loss: 0.015584822744131088 = 0.008664348162710667 + 0.001 * 6.920474052429199
Epoch 670, val loss: 1.2629696130752563
Epoch 680, training loss: 0.015187360346317291 = 0.008281483314931393 + 0.001 * 6.905877113342285
Epoch 680, val loss: 1.2734110355377197
Epoch 690, training loss: 0.014805108308792114 = 0.007924395613372326 + 0.001 * 6.880712509155273
Epoch 690, val loss: 1.2836281061172485
Epoch 700, training loss: 0.014464233070611954 = 0.00759081682190299 + 0.001 * 6.873416423797607
Epoch 700, val loss: 1.2935844659805298
Epoch 710, training loss: 0.014176196418702602 = 0.00727882469072938 + 0.001 * 6.897371292114258
Epoch 710, val loss: 1.3032690286636353
Epoch 720, training loss: 0.013889912515878677 = 0.006986623629927635 + 0.001 * 6.903288841247559
Epoch 720, val loss: 1.3127248287200928
Epoch 730, training loss: 0.013591799885034561 = 0.006712603848427534 + 0.001 * 6.879195213317871
Epoch 730, val loss: 1.3219614028930664
Epoch 740, training loss: 0.013327856548130512 = 0.0064553432166576385 + 0.001 * 6.8725128173828125
Epoch 740, val loss: 1.3310030698776245
Epoch 750, training loss: 0.013081156648695469 = 0.0062135010957717896 + 0.001 * 6.867655277252197
Epoch 750, val loss: 1.3398077487945557
Epoch 760, training loss: 0.012848040089011192 = 0.005985735449939966 + 0.001 * 6.862303733825684
Epoch 760, val loss: 1.3484444618225098
Epoch 770, training loss: 0.012618123553693295 = 0.005771073512732983 + 0.001 * 6.847049713134766
Epoch 770, val loss: 1.3568364381790161
Epoch 780, training loss: 0.012414073571562767 = 0.005568444263190031 + 0.001 * 6.84562873840332
Epoch 780, val loss: 1.365082859992981
Epoch 790, training loss: 0.012219879776239395 = 0.005377115216106176 + 0.001 * 6.842764377593994
Epoch 790, val loss: 1.3731260299682617
Epoch 800, training loss: 0.012061684392392635 = 0.005196322221308947 + 0.001 * 6.86536169052124
Epoch 800, val loss: 1.3810356855392456
Epoch 810, training loss: 0.011898603290319443 = 0.005025333724915981 + 0.001 * 6.873269081115723
Epoch 810, val loss: 1.3887487649917603
Epoch 820, training loss: 0.011690976098179817 = 0.004863421898335218 + 0.001 * 6.827553749084473
Epoch 820, val loss: 1.3963218927383423
Epoch 830, training loss: 0.011552130803465843 = 0.004709984175860882 + 0.001 * 6.842146873474121
Epoch 830, val loss: 1.40370774269104
Epoch 840, training loss: 0.011408139020204544 = 0.004564348608255386 + 0.001 * 6.843790054321289
Epoch 840, val loss: 1.4109612703323364
Epoch 850, training loss: 0.01123867742717266 = 0.0044258893467485905 + 0.001 * 6.812788009643555
Epoch 850, val loss: 1.4180448055267334
Epoch 860, training loss: 0.01114608719944954 = 0.004294002428650856 + 0.001 * 6.852084159851074
Epoch 860, val loss: 1.4250545501708984
Epoch 870, training loss: 0.0109897181391716 = 0.004167977254837751 + 0.001 * 6.82174015045166
Epoch 870, val loss: 1.4319343566894531
Epoch 880, training loss: 0.010873883962631226 = 0.00404693465679884 + 0.001 * 6.826949119567871
Epoch 880, val loss: 1.4387915134429932
Epoch 890, training loss: 0.01076025702059269 = 0.003930282779037952 + 0.001 * 6.829973220825195
Epoch 890, val loss: 1.445642113685608
Epoch 900, training loss: 0.010614944621920586 = 0.003817416727542877 + 0.001 * 6.797527313232422
Epoch 900, val loss: 1.4525349140167236
Epoch 910, training loss: 0.010500255040824413 = 0.003708089701831341 + 0.001 * 6.7921648025512695
Epoch 910, val loss: 1.4594634771347046
Epoch 920, training loss: 0.010438227094709873 = 0.0036021696869283915 + 0.001 * 6.836057186126709
Epoch 920, val loss: 1.4664134979248047
Epoch 930, training loss: 0.01031508855521679 = 0.0034995656460523605 + 0.001 * 6.81552267074585
Epoch 930, val loss: 1.4734512567520142
Epoch 940, training loss: 0.010188035666942596 = 0.0034001285675913095 + 0.001 * 6.787906646728516
Epoch 940, val loss: 1.480534315109253
Epoch 950, training loss: 0.01009813230484724 = 0.00330391526222229 + 0.001 * 6.794216632843018
Epoch 950, val loss: 1.4876409769058228
Epoch 960, training loss: 0.009994200430810452 = 0.0032109012827277184 + 0.001 * 6.783298969268799
Epoch 960, val loss: 1.4947564601898193
Epoch 970, training loss: 0.009897626005113125 = 0.003121195128187537 + 0.001 * 6.776430606842041
Epoch 970, val loss: 1.5019434690475464
Epoch 980, training loss: 0.009862754493951797 = 0.003034784458577633 + 0.001 * 6.827970027923584
Epoch 980, val loss: 1.5091301202774048
Epoch 990, training loss: 0.009743273258209229 = 0.0029515898786485195 + 0.001 * 6.791683673858643
Epoch 990, val loss: 1.5162323713302612
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9630345106124878 = 1.9546607732772827 + 0.001 * 8.373775482177734
Epoch 0, val loss: 1.949026346206665
Epoch 10, training loss: 1.9525777101516724 = 1.9442039728164673 + 0.001 * 8.37370777130127
Epoch 10, val loss: 1.9390442371368408
Epoch 20, training loss: 1.9398095607757568 = 1.9314360618591309 + 0.001 * 8.373448371887207
Epoch 20, val loss: 1.926866054534912
Epoch 30, training loss: 1.9219608306884766 = 1.9135878086090088 + 0.001 * 8.372968673706055
Epoch 30, val loss: 1.9098823070526123
Epoch 40, training loss: 1.8956483602523804 = 1.887276291847229 + 0.001 * 8.37200927734375
Epoch 40, val loss: 1.8849884271621704
Epoch 50, training loss: 1.858432412147522 = 1.8500628471374512 + 0.001 * 8.36956787109375
Epoch 50, val loss: 1.8513597249984741
Epoch 60, training loss: 1.8143457174301147 = 1.805985450744629 + 0.001 * 8.360250473022461
Epoch 60, val loss: 1.8155491352081299
Epoch 70, training loss: 1.7746456861495972 = 1.766345739364624 + 0.001 * 8.299915313720703
Epoch 70, val loss: 1.787177562713623
Epoch 80, training loss: 1.7305259704589844 = 1.7226128578186035 + 0.001 * 7.913086414337158
Epoch 80, val loss: 1.7501174211502075
Epoch 90, training loss: 1.6687235832214355 = 1.661093831062317 + 0.001 * 7.629693984985352
Epoch 90, val loss: 1.6962134838104248
Epoch 100, training loss: 1.5863698720932007 = 1.5790166854858398 + 0.001 * 7.353200912475586
Epoch 100, val loss: 1.627403736114502
Epoch 110, training loss: 1.4886218309402466 = 1.4814836978912354 + 0.001 * 7.1381330490112305
Epoch 110, val loss: 1.5483782291412354
Epoch 120, training loss: 1.3872023820877075 = 1.3801406621932983 + 0.001 * 7.0616912841796875
Epoch 120, val loss: 1.4660841226577759
Epoch 130, training loss: 1.2863059043884277 = 1.2793300151824951 + 0.001 * 6.975851535797119
Epoch 130, val loss: 1.3857522010803223
Epoch 140, training loss: 1.1830511093139648 = 1.176100492477417 + 0.001 * 6.950618267059326
Epoch 140, val loss: 1.3036494255065918
Epoch 150, training loss: 1.07673978805542 = 1.06980299949646 + 0.001 * 6.936789512634277
Epoch 150, val loss: 1.2208144664764404
Epoch 160, training loss: 0.9711626172065735 = 0.9642410278320312 + 0.001 * 6.921573638916016
Epoch 160, val loss: 1.1389613151550293
Epoch 170, training loss: 0.872794508934021 = 0.8658849596977234 + 0.001 * 6.90957498550415
Epoch 170, val loss: 1.063550353050232
Epoch 180, training loss: 0.7869067788124084 = 0.7800070643424988 + 0.001 * 6.8997321128845215
Epoch 180, val loss: 0.9986992478370667
Epoch 190, training loss: 0.7143310904502869 = 0.7074368596076965 + 0.001 * 6.894216537475586
Epoch 190, val loss: 0.9457637667655945
Epoch 200, training loss: 0.6521298885345459 = 0.6452361345291138 + 0.001 * 6.893747329711914
Epoch 200, val loss: 0.9027842879295349
Epoch 210, training loss: 0.5962164998054504 = 0.5893265604972839 + 0.001 * 6.889945983886719
Epoch 210, val loss: 0.8666051626205444
Epoch 220, training loss: 0.5435819029808044 = 0.5366929769515991 + 0.001 * 6.888899803161621
Epoch 220, val loss: 0.8348414897918701
Epoch 230, training loss: 0.4928562343120575 = 0.4859684407711029 + 0.001 * 6.887781143188477
Epoch 230, val loss: 0.8063156008720398
Epoch 240, training loss: 0.44388923048973083 = 0.4370028078556061 + 0.001 * 6.886432647705078
Epoch 240, val loss: 0.780921459197998
Epoch 250, training loss: 0.397187739610672 = 0.3903029263019562 + 0.001 * 6.8848185539245605
Epoch 250, val loss: 0.7592845559120178
Epoch 260, training loss: 0.3533746302127838 = 0.3464917540550232 + 0.001 * 6.8828816413879395
Epoch 260, val loss: 0.742111325263977
Epoch 270, training loss: 0.31285518407821655 = 0.30597448348999023 + 0.001 * 6.880705833435059
Epoch 270, val loss: 0.7298634648323059
Epoch 280, training loss: 0.2758655846118927 = 0.26898735761642456 + 0.001 * 6.878215789794922
Epoch 280, val loss: 0.7225912809371948
Epoch 290, training loss: 0.2425333559513092 = 0.23565565049648285 + 0.001 * 6.877711772918701
Epoch 290, val loss: 0.7200987935066223
Epoch 300, training loss: 0.21291545033454895 = 0.20604178309440613 + 0.001 * 6.873665809631348
Epoch 300, val loss: 0.7222364544868469
Epoch 310, training loss: 0.1869467794895172 = 0.18007715046405792 + 0.001 * 6.869635105133057
Epoch 310, val loss: 0.7288690209388733
Epoch 320, training loss: 0.16443349421024323 = 0.15756772458553314 + 0.001 * 6.8657708168029785
Epoch 320, val loss: 0.7394829988479614
Epoch 330, training loss: 0.14502030611038208 = 0.13815076649188995 + 0.001 * 6.869543552398682
Epoch 330, val loss: 0.753356397151947
Epoch 340, training loss: 0.12830643355846405 = 0.1214471161365509 + 0.001 * 6.859320163726807
Epoch 340, val loss: 0.7698894739151001
Epoch 350, training loss: 0.11392499506473541 = 0.10707198083400726 + 0.001 * 6.853017330169678
Epoch 350, val loss: 0.7882980704307556
Epoch 360, training loss: 0.10153965651988983 = 0.09467561542987823 + 0.001 * 6.864037036895752
Epoch 360, val loss: 0.8079555630683899
Epoch 370, training loss: 0.09080859273672104 = 0.08396704494953156 + 0.001 * 6.841547012329102
Epoch 370, val loss: 0.8282986879348755
Epoch 380, training loss: 0.08153409510850906 = 0.07470358163118362 + 0.001 * 6.830511569976807
Epoch 380, val loss: 0.8489143252372742
Epoch 390, training loss: 0.07349960505962372 = 0.06667588651180267 + 0.001 * 6.823716640472412
Epoch 390, val loss: 0.8695314526557922
Epoch 400, training loss: 0.06653855741024017 = 0.05970284715294838 + 0.001 * 6.835709095001221
Epoch 400, val loss: 0.8899368643760681
Epoch 410, training loss: 0.06043584272265434 = 0.053633689880371094 + 0.001 * 6.802151203155518
Epoch 410, val loss: 0.9100402593612671
Epoch 420, training loss: 0.055163655430078506 = 0.048340488225221634 + 0.001 * 6.823166847229004
Epoch 420, val loss: 0.9298080801963806
Epoch 430, training loss: 0.05051659792661667 = 0.043711163103580475 + 0.001 * 6.805436134338379
Epoch 430, val loss: 0.949264407157898
Epoch 440, training loss: 0.046443112194538116 = 0.03965301066637039 + 0.001 * 6.790100574493408
Epoch 440, val loss: 0.9683451056480408
Epoch 450, training loss: 0.04287386313080788 = 0.036086056381464005 + 0.001 * 6.787806510925293
Epoch 450, val loss: 0.9870079755783081
Epoch 460, training loss: 0.039722833782434464 = 0.032941967248916626 + 0.001 * 6.780867099761963
Epoch 460, val loss: 1.0052725076675415
Epoch 470, training loss: 0.036952756345272064 = 0.0301629938185215 + 0.001 * 6.789760112762451
Epoch 470, val loss: 1.0230673551559448
Epoch 480, training loss: 0.03448633849620819 = 0.027700208127498627 + 0.001 * 6.786130428314209
Epoch 480, val loss: 1.0403894186019897
Epoch 490, training loss: 0.03229324147105217 = 0.02551169879734516 + 0.001 * 6.78154182434082
Epoch 490, val loss: 1.0572510957717896
Epoch 500, training loss: 0.030331742018461227 = 0.02356097288429737 + 0.001 * 6.770768642425537
Epoch 500, val loss: 1.0736256837844849
Epoch 510, training loss: 0.028588978573679924 = 0.021817391738295555 + 0.001 * 6.7715864181518555
Epoch 510, val loss: 1.0895224809646606
Epoch 520, training loss: 0.0270211324095726 = 0.020253963768482208 + 0.001 * 6.767168045043945
Epoch 520, val loss: 1.1049394607543945
Epoch 530, training loss: 0.025638623163104057 = 0.018848484382033348 + 0.001 * 6.790138244628906
Epoch 530, val loss: 1.119916319847107
Epoch 540, training loss: 0.024340597912669182 = 0.017581844702363014 + 0.001 * 6.758753299713135
Epoch 540, val loss: 1.1344401836395264
Epoch 550, training loss: 0.023195724934339523 = 0.016437429934740067 + 0.001 * 6.758295059204102
Epoch 550, val loss: 1.1485633850097656
Epoch 560, training loss: 0.022190064191818237 = 0.015401000156998634 + 0.001 * 6.789062976837158
Epoch 560, val loss: 1.1622941493988037
Epoch 570, training loss: 0.02122326008975506 = 0.014459768310189247 + 0.001 * 6.763491630554199
Epoch 570, val loss: 1.175641655921936
Epoch 580, training loss: 0.02035335637629032 = 0.013602769002318382 + 0.001 * 6.750587463378906
Epoch 580, val loss: 1.1886074542999268
Epoch 590, training loss: 0.019573647528886795 = 0.012820577248930931 + 0.001 * 6.753070831298828
Epoch 590, val loss: 1.2012091875076294
Epoch 600, training loss: 0.018861180171370506 = 0.012104924768209457 + 0.001 * 6.75625467300415
Epoch 600, val loss: 1.2134863138198853
Epoch 610, training loss: 0.018199920654296875 = 0.011448696255683899 + 0.001 * 6.751224517822266
Epoch 610, val loss: 1.2254343032836914
Epoch 620, training loss: 0.017595265060663223 = 0.0108456676825881 + 0.001 * 6.749598026275635
Epoch 620, val loss: 1.237067699432373
Epoch 630, training loss: 0.017038077116012573 = 0.010290354490280151 + 0.001 * 6.74772310256958
Epoch 630, val loss: 1.2483986616134644
Epoch 640, training loss: 0.016527362167835236 = 0.009777970612049103 + 0.001 * 6.749391555786133
Epoch 640, val loss: 1.2594300508499146
Epoch 650, training loss: 0.01605193130671978 = 0.009304367005825043 + 0.001 * 6.747563362121582
Epoch 650, val loss: 1.2702083587646484
Epoch 660, training loss: 0.015597489662468433 = 0.008865742944180965 + 0.001 * 6.731746196746826
Epoch 660, val loss: 1.2807031869888306
Epoch 670, training loss: 0.015238254331052303 = 0.008458792231976986 + 0.001 * 6.779461860656738
Epoch 670, val loss: 1.2909362316131592
Epoch 680, training loss: 0.014817003160715103 = 0.00808063056319952 + 0.001 * 6.736372947692871
Epoch 680, val loss: 1.3009182214736938
Epoch 690, training loss: 0.01445815246552229 = 0.007728691212832928 + 0.001 * 6.729460716247559
Epoch 690, val loss: 1.3106735944747925
Epoch 700, training loss: 0.014147719368338585 = 0.0074005573987960815 + 0.001 * 6.747162342071533
Epoch 700, val loss: 1.3201981782913208
Epoch 710, training loss: 0.013822996988892555 = 0.007094142027199268 + 0.001 * 6.728854179382324
Epoch 710, val loss: 1.3294740915298462
Epoch 720, training loss: 0.013525135815143585 = 0.006807667203247547 + 0.001 * 6.717468738555908
Epoch 720, val loss: 1.3385478258132935
Epoch 730, training loss: 0.01327657699584961 = 0.0065393708646297455 + 0.001 * 6.737205505371094
Epoch 730, val loss: 1.347427487373352
Epoch 740, training loss: 0.013009343296289444 = 0.006287852302193642 + 0.001 * 6.721490859985352
Epoch 740, val loss: 1.3561207056045532
Epoch 750, training loss: 0.012765476480126381 = 0.006051686592400074 + 0.001 * 6.713789939880371
Epoch 750, val loss: 1.3646118640899658
Epoch 760, training loss: 0.012561684474349022 = 0.0058296783827245235 + 0.001 * 6.732006072998047
Epoch 760, val loss: 1.372928500175476
Epoch 770, training loss: 0.012334680184721947 = 0.005620666779577732 + 0.001 * 6.71401309967041
Epoch 770, val loss: 1.3810632228851318
Epoch 780, training loss: 0.01213439367711544 = 0.00542366411536932 + 0.001 * 6.710729122161865
Epoch 780, val loss: 1.389011263847351
Epoch 790, training loss: 0.011976160109043121 = 0.005237686447799206 + 0.001 * 6.738473892211914
Epoch 790, val loss: 1.3968218564987183
Epoch 800, training loss: 0.011769719421863556 = 0.005061350762844086 + 0.001 * 6.708367824554443
Epoch 800, val loss: 1.4044904708862305
Epoch 810, training loss: 0.011608690023422241 = 0.004893063567578793 + 0.001 * 6.7156267166137695
Epoch 810, val loss: 1.4120804071426392
Epoch 820, training loss: 0.011488957330584526 = 0.004731254186481237 + 0.001 * 6.757702350616455
Epoch 820, val loss: 1.4196258783340454
Epoch 830, training loss: 0.011278370395302773 = 0.004575229715555906 + 0.001 * 6.703140735626221
Epoch 830, val loss: 1.4272042512893677
Epoch 840, training loss: 0.011134220287203789 = 0.004424713551998138 + 0.001 * 6.709506511688232
Epoch 840, val loss: 1.434792160987854
Epoch 850, training loss: 0.011024314910173416 = 0.004279738757759333 + 0.001 * 6.7445759773254395
Epoch 850, val loss: 1.442353367805481
Epoch 860, training loss: 0.010844653472304344 = 0.004140517208725214 + 0.001 * 6.704135894775391
Epoch 860, val loss: 1.4498817920684814
Epoch 870, training loss: 0.010711923241615295 = 0.004007084295153618 + 0.001 * 6.704838275909424
Epoch 870, val loss: 1.4574010372161865
Epoch 880, training loss: 0.01059555634856224 = 0.0038796719163656235 + 0.001 * 6.715883731842041
Epoch 880, val loss: 1.4648100137710571
Epoch 890, training loss: 0.01045635249465704 = 0.0037582393269985914 + 0.001 * 6.698112487792969
Epoch 890, val loss: 1.4721323251724243
Epoch 900, training loss: 0.010357975959777832 = 0.0036423574201762676 + 0.001 * 6.71561861038208
Epoch 900, val loss: 1.4793717861175537
Epoch 910, training loss: 0.010233678855001926 = 0.003532004775479436 + 0.001 * 6.70167350769043
Epoch 910, val loss: 1.4865145683288574
Epoch 920, training loss: 0.010119801387190819 = 0.0034268859308212996 + 0.001 * 6.692914962768555
Epoch 920, val loss: 1.4935530424118042
Epoch 930, training loss: 0.01003897376358509 = 0.003326683770865202 + 0.001 * 6.712289333343506
Epoch 930, val loss: 1.5005061626434326
Epoch 940, training loss: 0.009941842406988144 = 0.003231172217056155 + 0.001 * 6.710669994354248
Epoch 940, val loss: 1.5073331594467163
Epoch 950, training loss: 0.00984504260122776 = 0.003140272106975317 + 0.001 * 6.704770565032959
Epoch 950, val loss: 1.5140901803970337
Epoch 960, training loss: 0.009746846742928028 = 0.0030535738915205 + 0.001 * 6.693272590637207
Epoch 960, val loss: 1.5207078456878662
Epoch 970, training loss: 0.009665753692388535 = 0.0029709211084991693 + 0.001 * 6.694831848144531
Epoch 970, val loss: 1.527246117591858
Epoch 980, training loss: 0.009576753713190556 = 0.0028920702170580626 + 0.001 * 6.684683322906494
Epoch 980, val loss: 1.5336583852767944
Epoch 990, training loss: 0.009497437626123428 = 0.002816768130287528 + 0.001 * 6.680668830871582
Epoch 990, val loss: 1.5399781465530396
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5941
Flip ASR: 0.5111/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9532363414764404 = 1.9448624849319458 + 0.001 * 8.373844146728516
Epoch 0, val loss: 1.9426690340042114
Epoch 10, training loss: 1.9431580305099487 = 1.9347842931747437 + 0.001 * 8.37376594543457
Epoch 10, val loss: 1.9325873851776123
Epoch 20, training loss: 1.9311329126358032 = 1.9227594137191772 + 0.001 * 8.373537063598633
Epoch 20, val loss: 1.9201099872589111
Epoch 30, training loss: 1.914794921875 = 1.9064218997955322 + 0.001 * 8.37306022644043
Epoch 30, val loss: 1.9024839401245117
Epoch 40, training loss: 1.89114511013031 = 1.8827731609344482 + 0.001 * 8.371993064880371
Epoch 40, val loss: 1.8766093254089355
Epoch 50, training loss: 1.8570432662963867 = 1.8486741781234741 + 0.001 * 8.36914348602295
Epoch 50, val loss: 1.8400487899780273
Epoch 60, training loss: 1.8133426904678345 = 1.8049836158752441 + 0.001 * 8.359063148498535
Epoch 60, val loss: 1.7958574295043945
Epoch 70, training loss: 1.766921877861023 = 1.758614420890808 + 0.001 * 8.307417869567871
Epoch 70, val loss: 1.7533926963806152
Epoch 80, training loss: 1.7136015892028809 = 1.7056576013565063 + 0.001 * 7.943929672241211
Epoch 80, val loss: 1.7073389291763306
Epoch 90, training loss: 1.641546368598938 = 1.633946180343628 + 0.001 * 7.600207805633545
Epoch 90, val loss: 1.6456987857818604
Epoch 100, training loss: 1.5483062267303467 = 1.5408668518066406 + 0.001 * 7.439403057098389
Epoch 100, val loss: 1.5660642385482788
Epoch 110, training loss: 1.440317153930664 = 1.4330028295516968 + 0.001 * 7.314297676086426
Epoch 110, val loss: 1.4771127700805664
Epoch 120, training loss: 1.3295459747314453 = 1.3223096132278442 + 0.001 * 7.236369609832764
Epoch 120, val loss: 1.3907502889633179
Epoch 130, training loss: 1.2226966619491577 = 1.2155373096466064 + 0.001 * 7.15937614440918
Epoch 130, val loss: 1.3116215467453003
Epoch 140, training loss: 1.1232237815856934 = 1.1161588430404663 + 0.001 * 7.064991474151611
Epoch 140, val loss: 1.2412621974945068
Epoch 150, training loss: 1.0336416959762573 = 1.0266358852386475 + 0.001 * 7.005863666534424
Epoch 150, val loss: 1.1783671379089355
Epoch 160, training loss: 0.9540256261825562 = 0.9470494389533997 + 0.001 * 6.976194381713867
Epoch 160, val loss: 1.1224398612976074
Epoch 170, training loss: 0.8818288445472717 = 0.8748771548271179 + 0.001 * 6.951673984527588
Epoch 170, val loss: 1.071784257888794
Epoch 180, training loss: 0.8137662410736084 = 0.806837260723114 + 0.001 * 6.928956508636475
Epoch 180, val loss: 1.023792028427124
Epoch 190, training loss: 0.7476106286048889 = 0.7406985759735107 + 0.001 * 6.912060737609863
Epoch 190, val loss: 0.9768486022949219
Epoch 200, training loss: 0.6827459335327148 = 0.6758440732955933 + 0.001 * 6.901864528656006
Epoch 200, val loss: 0.9308662414550781
Epoch 210, training loss: 0.6197803616523743 = 0.612883448600769 + 0.001 * 6.8968892097473145
Epoch 210, val loss: 0.8866844773292542
Epoch 220, training loss: 0.559941828250885 = 0.553047776222229 + 0.001 * 6.894076824188232
Epoch 220, val loss: 0.8453739881515503
Epoch 230, training loss: 0.5043163895606995 = 0.4974249601364136 + 0.001 * 6.891410827636719
Epoch 230, val loss: 0.8085939884185791
Epoch 240, training loss: 0.4533379375934601 = 0.44644996523857117 + 0.001 * 6.887975215911865
Epoch 240, val loss: 0.7776398658752441
Epoch 250, training loss: 0.40694501996040344 = 0.4000602662563324 + 0.001 * 6.884755611419678
Epoch 250, val loss: 0.7532844543457031
Epoch 260, training loss: 0.3646761476993561 = 0.3577975034713745 + 0.001 * 6.878647327423096
Epoch 260, val loss: 0.7350970506668091
Epoch 270, training loss: 0.32616880536079407 = 0.31929752230644226 + 0.001 * 6.871291160583496
Epoch 270, val loss: 0.7227485179901123
Epoch 280, training loss: 0.29142165184020996 = 0.284559041261673 + 0.001 * 6.8626017570495605
Epoch 280, val loss: 0.7157963514328003
Epoch 290, training loss: 0.26033541560173035 = 0.25348418951034546 + 0.001 * 6.85122537612915
Epoch 290, val loss: 0.7131998538970947
Epoch 300, training loss: 0.23257891833782196 = 0.2257409691810608 + 0.001 * 6.837950706481934
Epoch 300, val loss: 0.7141735553741455
Epoch 310, training loss: 0.2079072892665863 = 0.20108352601528168 + 0.001 * 6.823766231536865
Epoch 310, val loss: 0.7179701328277588
Epoch 320, training loss: 0.1862657368183136 = 0.17945191264152527 + 0.001 * 6.81381893157959
Epoch 320, val loss: 0.7241649627685547
Epoch 330, training loss: 0.16718736290931702 = 0.16038799285888672 + 0.001 * 6.799369812011719
Epoch 330, val loss: 0.7319271564483643
Epoch 340, training loss: 0.15024295449256897 = 0.1434491127729416 + 0.001 * 6.793839454650879
Epoch 340, val loss: 0.740783154964447
Epoch 350, training loss: 0.1351262629032135 = 0.12833522260189056 + 0.001 * 6.791036128997803
Epoch 350, val loss: 0.7503537535667419
Epoch 360, training loss: 0.12156610935926437 = 0.1147788017988205 + 0.001 * 6.787303924560547
Epoch 360, val loss: 0.7606252431869507
Epoch 370, training loss: 0.10936963558197021 = 0.10258418321609497 + 0.001 * 6.7854509353637695
Epoch 370, val loss: 0.7712490558624268
Epoch 380, training loss: 0.09839697927236557 = 0.09161240607500076 + 0.001 * 6.784571170806885
Epoch 380, val loss: 0.7822361588478088
Epoch 390, training loss: 0.08854427188634872 = 0.08176129311323166 + 0.001 * 6.782981872558594
Epoch 390, val loss: 0.7933308482170105
Epoch 400, training loss: 0.07972889393568039 = 0.07294518500566483 + 0.001 * 6.7837090492248535
Epoch 400, val loss: 0.8045969605445862
Epoch 410, training loss: 0.07187460362911224 = 0.0650939792394638 + 0.001 * 6.780622482299805
Epoch 410, val loss: 0.8158692717552185
Epoch 420, training loss: 0.06491268426179886 = 0.058133386075496674 + 0.001 * 6.779297828674316
Epoch 420, val loss: 0.8271310925483704
Epoch 430, training loss: 0.05876480042934418 = 0.05198761075735092 + 0.001 * 6.777189254760742
Epoch 430, val loss: 0.8382788896560669
Epoch 440, training loss: 0.053356487303972244 = 0.04657955840229988 + 0.001 * 6.7769269943237305
Epoch 440, val loss: 0.849315345287323
Epoch 450, training loss: 0.048607904464006424 = 0.041833095252513885 + 0.001 * 6.774808406829834
Epoch 450, val loss: 0.8601560592651367
Epoch 460, training loss: 0.044442735612392426 = 0.03767199069261551 + 0.001 * 6.770743370056152
Epoch 460, val loss: 0.8708875775337219
Epoch 470, training loss: 0.0407959446310997 = 0.03402598947286606 + 0.001 * 6.76995325088501
Epoch 470, val loss: 0.8814495205879211
Epoch 480, training loss: 0.03759592026472092 = 0.030829468742012978 + 0.001 * 6.766450881958008
Epoch 480, val loss: 0.8919007182121277
Epoch 490, training loss: 0.034788455814123154 = 0.02802496775984764 + 0.001 * 6.76348876953125
Epoch 490, val loss: 0.9021787047386169
Epoch 500, training loss: 0.03232618048787117 = 0.025561010465025902 + 0.001 * 6.765170097351074
Epoch 500, val loss: 0.9124155044555664
Epoch 510, training loss: 0.030150368809700012 = 0.02339216135442257 + 0.001 * 6.75820779800415
Epoch 510, val loss: 0.922563374042511
Epoch 520, training loss: 0.028231723234057426 = 0.02147873304784298 + 0.001 * 6.752989768981934
Epoch 520, val loss: 0.9325602650642395
Epoch 530, training loss: 0.026533912867307663 = 0.019785987213253975 + 0.001 * 6.747926235198975
Epoch 530, val loss: 0.9424527883529663
Epoch 540, training loss: 0.025030285120010376 = 0.018283803015947342 + 0.001 * 6.746482849121094
Epoch 540, val loss: 0.952171266078949
Epoch 550, training loss: 0.023690463975071907 = 0.01694677583873272 + 0.001 * 6.743688106536865
Epoch 550, val loss: 0.9617324471473694
Epoch 560, training loss: 0.022495046257972717 = 0.015752548351883888 + 0.001 * 6.742498397827148
Epoch 560, val loss: 0.9711581468582153
Epoch 570, training loss: 0.021425673738121986 = 0.014682113192975521 + 0.001 * 6.743560314178467
Epoch 570, val loss: 0.9803785681724548
Epoch 580, training loss: 0.020465021952986717 = 0.013719885610044003 + 0.001 * 6.745136260986328
Epoch 580, val loss: 0.9894093871116638
Epoch 590, training loss: 0.01959325186908245 = 0.012852253392338753 + 0.001 * 6.740998268127441
Epoch 590, val loss: 0.9982636570930481
Epoch 600, training loss: 0.018803179264068604 = 0.01206738967448473 + 0.001 * 6.735790252685547
Epoch 600, val loss: 1.0069339275360107
Epoch 610, training loss: 0.018099987879395485 = 0.011354960501194 + 0.001 * 6.745027542114258
Epoch 610, val loss: 1.015397071838379
Epoch 620, training loss: 0.01743808202445507 = 0.010706916451454163 + 0.001 * 6.731165409088135
Epoch 620, val loss: 1.0236701965332031
Epoch 630, training loss: 0.016847850754857063 = 0.010115984827280045 + 0.001 * 6.731865406036377
Epoch 630, val loss: 1.0317480564117432
Epoch 640, training loss: 0.016305584460496902 = 0.009575381875038147 + 0.001 * 6.730201244354248
Epoch 640, val loss: 1.039671540260315
Epoch 650, training loss: 0.015819549560546875 = 0.009079624898731709 + 0.001 * 6.73992395401001
Epoch 650, val loss: 1.0474176406860352
Epoch 660, training loss: 0.015352057293057442 = 0.008623925969004631 + 0.001 * 6.72813081741333
Epoch 660, val loss: 1.054937481880188
Epoch 670, training loss: 0.014930559322237968 = 0.008203999139368534 + 0.001 * 6.726559638977051
Epoch 670, val loss: 1.0622918605804443
Epoch 680, training loss: 0.014542043209075928 = 0.007816240191459656 + 0.001 * 6.725802421569824
Epoch 680, val loss: 1.0694860219955444
Epoch 690, training loss: 0.014193777926266193 = 0.007457394152879715 + 0.001 * 6.736383438110352
Epoch 690, val loss: 1.0764830112457275
Epoch 700, training loss: 0.013847775757312775 = 0.007124634459614754 + 0.001 * 6.723141670227051
Epoch 700, val loss: 1.0833218097686768
Epoch 710, training loss: 0.013538196682929993 = 0.006815500091761351 + 0.001 * 6.722696781158447
Epoch 710, val loss: 1.0900359153747559
Epoch 720, training loss: 0.013250371441245079 = 0.0065278238616883755 + 0.001 * 6.7225470542907715
Epoch 720, val loss: 1.0965793132781982
Epoch 730, training loss: 0.012979187071323395 = 0.006259692832827568 + 0.001 * 6.719493389129639
Epoch 730, val loss: 1.1029752492904663
Epoch 740, training loss: 0.012734477408230305 = 0.006009324919432402 + 0.001 * 6.725152015686035
Epoch 740, val loss: 1.109226107597351
Epoch 750, training loss: 0.012498565018177032 = 0.005775142926722765 + 0.001 * 6.723422050476074
Epoch 750, val loss: 1.1153849363327026
Epoch 760, training loss: 0.01227409765124321 = 0.005555806681513786 + 0.001 * 6.718291282653809
Epoch 760, val loss: 1.1213864088058472
Epoch 770, training loss: 0.012067081406712532 = 0.005350044928491116 + 0.001 * 6.71703577041626
Epoch 770, val loss: 1.127259612083435
Epoch 780, training loss: 0.011882085353136063 = 0.005156794097274542 + 0.001 * 6.725290775299072
Epoch 780, val loss: 1.1329915523529053
Epoch 790, training loss: 0.011691257357597351 = 0.004975022282451391 + 0.001 * 6.716235160827637
Epoch 790, val loss: 1.138609766960144
Epoch 800, training loss: 0.0115157850086689 = 0.004803878255188465 + 0.001 * 6.711906909942627
Epoch 800, val loss: 1.1441247463226318
Epoch 810, training loss: 0.011357150971889496 = 0.00464246841147542 + 0.001 * 6.714682102203369
Epoch 810, val loss: 1.1495320796966553
Epoch 820, training loss: 0.011201288551092148 = 0.004490104503929615 + 0.001 * 6.711183547973633
Epoch 820, val loss: 1.1548150777816772
Epoch 830, training loss: 0.011073474772274494 = 0.004346108064055443 + 0.001 * 6.7273664474487305
Epoch 830, val loss: 1.1599881649017334
Epoch 840, training loss: 0.010921496897935867 = 0.0042098406702280045 + 0.001 * 6.71165657043457
Epoch 840, val loss: 1.1650714874267578
Epoch 850, training loss: 0.01078670285642147 = 0.004080781247466803 + 0.001 * 6.705921649932861
Epoch 850, val loss: 1.1700661182403564
Epoch 860, training loss: 0.01066693663597107 = 0.0039584138430655 + 0.001 * 6.708521842956543
Epoch 860, val loss: 1.1749430894851685
Epoch 870, training loss: 0.01054514478892088 = 0.003842281876131892 + 0.001 * 6.702862739562988
Epoch 870, val loss: 1.179740309715271
Epoch 880, training loss: 0.01043795421719551 = 0.0037319541443139315 + 0.001 * 6.705999374389648
Epoch 880, val loss: 1.1844382286071777
Epoch 890, training loss: 0.010327465832233429 = 0.00362704461440444 + 0.001 * 6.700420379638672
Epoch 890, val loss: 1.189058780670166
Epoch 900, training loss: 0.010231373831629753 = 0.003527195192873478 + 0.001 * 6.704178333282471
Epoch 900, val loss: 1.1935820579528809
Epoch 910, training loss: 0.010133126750588417 = 0.0034321050625294447 + 0.001 * 6.701021194458008
Epoch 910, val loss: 1.1980164051055908
Epoch 920, training loss: 0.010049182921648026 = 0.0033414687495678663 + 0.001 * 6.707714080810547
Epoch 920, val loss: 1.2023786306381226
Epoch 930, training loss: 0.009953893721103668 = 0.003254984738305211 + 0.001 * 6.69890832901001
Epoch 930, val loss: 1.2066502571105957
Epoch 940, training loss: 0.009871203452348709 = 0.0031724318396300077 + 0.001 * 6.698770999908447
Epoch 940, val loss: 1.2108397483825684
Epoch 950, training loss: 0.009796133264899254 = 0.003093549283221364 + 0.001 * 6.7025837898254395
Epoch 950, val loss: 1.214967131614685
Epoch 960, training loss: 0.009712995029985905 = 0.0030181207694113255 + 0.001 * 6.694873809814453
Epoch 960, val loss: 1.2190085649490356
Epoch 970, training loss: 0.009640456177294254 = 0.0029459658544510603 + 0.001 * 6.694490432739258
Epoch 970, val loss: 1.2229923009872437
Epoch 980, training loss: 0.00957469828426838 = 0.0028768752235919237 + 0.001 * 6.697822570800781
Epoch 980, val loss: 1.2268962860107422
Epoch 990, training loss: 0.009519970044493675 = 0.002810683334246278 + 0.001 * 6.709286212921143
Epoch 990, val loss: 1.2307400703430176
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6347
Flip ASR: 0.6000/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9690688848495483 = 1.9606951475143433 + 0.001 * 8.373788833618164
Epoch 0, val loss: 1.9628440141677856
Epoch 10, training loss: 1.9589322805404663 = 1.9505586624145508 + 0.001 * 8.373671531677246
Epoch 10, val loss: 1.9525994062423706
Epoch 20, training loss: 1.9464857578277588 = 1.9381123781204224 + 0.001 * 8.373358726501465
Epoch 20, val loss: 1.939855694770813
Epoch 30, training loss: 1.9288395643234253 = 1.9204668998718262 + 0.001 * 8.372697830200195
Epoch 30, val loss: 1.9217441082000732
Epoch 40, training loss: 1.9022691249847412 = 1.8938978910446167 + 0.001 * 8.37119197845459
Epoch 40, val loss: 1.8946789503097534
Epoch 50, training loss: 1.8639241456985474 = 1.8555572032928467 + 0.001 * 8.366937637329102
Epoch 50, val loss: 1.8569512367248535
Epoch 60, training loss: 1.8179075717926025 = 1.809557557106018 + 0.001 * 8.349969863891602
Epoch 60, val loss: 1.8154898881912231
Epoch 70, training loss: 1.776570439338684 = 1.768324375152588 + 0.001 * 8.246088027954102
Epoch 70, val loss: 1.7820218801498413
Epoch 80, training loss: 1.7313125133514404 = 1.7234081029891968 + 0.001 * 7.904350757598877
Epoch 80, val loss: 1.7424925565719604
Epoch 90, training loss: 1.669289469718933 = 1.6617954969406128 + 0.001 * 7.49401330947876
Epoch 90, val loss: 1.6875226497650146
Epoch 100, training loss: 1.586700677871704 = 1.5795083045959473 + 0.001 * 7.192334175109863
Epoch 100, val loss: 1.6171903610229492
Epoch 110, training loss: 1.4868688583374023 = 1.4797921180725098 + 0.001 * 7.076700210571289
Epoch 110, val loss: 1.5352171659469604
Epoch 120, training loss: 1.3800716400146484 = 1.3730608224868774 + 0.001 * 7.010848045349121
Epoch 120, val loss: 1.4479743242263794
Epoch 130, training loss: 1.2730249166488647 = 1.266049861907959 + 0.001 * 6.975081443786621
Epoch 130, val loss: 1.3610758781433105
Epoch 140, training loss: 1.1684722900390625 = 1.1615196466445923 + 0.001 * 6.952606201171875
Epoch 140, val loss: 1.2776421308517456
Epoch 150, training loss: 1.0691958665847778 = 1.0622657537460327 + 0.001 * 6.930061340332031
Epoch 150, val loss: 1.1998027563095093
Epoch 160, training loss: 0.9775967597961426 = 0.9706900715827942 + 0.001 * 6.906661510467529
Epoch 160, val loss: 1.1296970844268799
Epoch 170, training loss: 0.8936303853988647 = 0.8867461085319519 + 0.001 * 6.8842973709106445
Epoch 170, val loss: 1.0671573877334595
Epoch 180, training loss: 0.8150811791419983 = 0.808212399482727 + 0.001 * 6.868765830993652
Epoch 180, val loss: 1.0098440647125244
Epoch 190, training loss: 0.7400870323181152 = 0.7332243919372559 + 0.001 * 6.862619400024414
Epoch 190, val loss: 0.9555506706237793
Epoch 200, training loss: 0.6681131720542908 = 0.6612589359283447 + 0.001 * 6.854213237762451
Epoch 200, val loss: 0.9041557312011719
Epoch 210, training loss: 0.600216805934906 = 0.5933730602264404 + 0.001 * 6.843758583068848
Epoch 210, val loss: 0.8570452928543091
Epoch 220, training loss: 0.5383037328720093 = 0.5314739346504211 + 0.001 * 6.829817295074463
Epoch 220, val loss: 0.8161478638648987
Epoch 230, training loss: 0.4836875796318054 = 0.4768674075603485 + 0.001 * 6.820159912109375
Epoch 230, val loss: 0.7836284637451172
Epoch 240, training loss: 0.4360769987106323 = 0.4292761981487274 + 0.001 * 6.800792694091797
Epoch 240, val loss: 0.7586904168128967
Epoch 250, training loss: 0.3941747546195984 = 0.38738590478897095 + 0.001 * 6.788861274719238
Epoch 250, val loss: 0.7395778894424438
Epoch 260, training loss: 0.35652032494544983 = 0.34973809123039246 + 0.001 * 6.782242298126221
Epoch 260, val loss: 0.7246898412704468
Epoch 270, training loss: 0.3220538794994354 = 0.31528061628341675 + 0.001 * 6.773254871368408
Epoch 270, val loss: 0.7131385207176208
Epoch 280, training loss: 0.2901841104030609 = 0.2834140658378601 + 0.001 * 6.7700300216674805
Epoch 280, val loss: 0.7044366002082825
Epoch 290, training loss: 0.26055073738098145 = 0.25378233194351196 + 0.001 * 6.768401622772217
Epoch 290, val loss: 0.6980730295181274
Epoch 300, training loss: 0.23297818005084991 = 0.22620950639247894 + 0.001 * 6.768671035766602
Epoch 300, val loss: 0.6940681338310242
Epoch 310, training loss: 0.20741383731365204 = 0.20064616203308105 + 0.001 * 6.767678737640381
Epoch 310, val loss: 0.6924968361854553
Epoch 320, training loss: 0.18398138880729675 = 0.17721322178840637 + 0.001 * 6.768169403076172
Epoch 320, val loss: 0.6931818723678589
Epoch 330, training loss: 0.16281811892986298 = 0.15605193376541138 + 0.001 * 6.766179084777832
Epoch 330, val loss: 0.6958840489387512
Epoch 340, training loss: 0.14401572942733765 = 0.13725149631500244 + 0.001 * 6.764238357543945
Epoch 340, val loss: 0.7004274725914001
Epoch 350, training loss: 0.12752771377563477 = 0.1207658126950264 + 0.001 * 6.761903285980225
Epoch 350, val loss: 0.7065472602844238
Epoch 360, training loss: 0.11320794373750687 = 0.10643313080072403 + 0.001 * 6.774811267852783
Epoch 360, val loss: 0.7137496471405029
Epoch 370, training loss: 0.10076507925987244 = 0.09400336444377899 + 0.001 * 6.761712074279785
Epoch 370, val loss: 0.7217477560043335
Epoch 380, training loss: 0.08996173739433289 = 0.08320590108633041 + 0.001 * 6.755834579467773
Epoch 380, val loss: 0.7302113771438599
Epoch 390, training loss: 0.08055419474840164 = 0.07379984110593796 + 0.001 * 6.754354953765869
Epoch 390, val loss: 0.7389194965362549
Epoch 400, training loss: 0.07233645021915436 = 0.06558483093976974 + 0.001 * 6.751616954803467
Epoch 400, val loss: 0.7478567361831665
Epoch 410, training loss: 0.06515660136938095 = 0.058400336652994156 + 0.001 * 6.756263732910156
Epoch 410, val loss: 0.7569531202316284
Epoch 420, training loss: 0.05886555090546608 = 0.052113667130470276 + 0.001 * 6.751882076263428
Epoch 420, val loss: 0.7662773132324219
Epoch 430, training loss: 0.053357139229774475 = 0.046611469238996506 + 0.001 * 6.745670795440674
Epoch 430, val loss: 0.7756682634353638
Epoch 440, training loss: 0.048542462289333344 = 0.04179614782333374 + 0.001 * 6.7463154792785645
Epoch 440, val loss: 0.7851404547691345
Epoch 450, training loss: 0.04432324692606926 = 0.037582289427518845 + 0.001 * 6.7409586906433105
Epoch 450, val loss: 0.7946475744247437
Epoch 460, training loss: 0.040636926889419556 = 0.033893950283527374 + 0.001 * 6.742978096008301
Epoch 460, val loss: 0.8040620684623718
Epoch 470, training loss: 0.03739873692393303 = 0.030662022531032562 + 0.001 * 6.736714839935303
Epoch 470, val loss: 0.8133729100227356
Epoch 480, training loss: 0.03456222265958786 = 0.027827154844999313 + 0.001 * 6.7350687980651855
Epoch 480, val loss: 0.8225482702255249
Epoch 490, training loss: 0.032070305198431015 = 0.025335540995001793 + 0.001 * 6.7347636222839355
Epoch 490, val loss: 0.8315428495407104
Epoch 500, training loss: 0.029883209615945816 = 0.02314048632979393 + 0.001 * 6.742722034454346
Epoch 500, val loss: 0.8403498530387878
Epoch 510, training loss: 0.02793547697365284 = 0.02120199427008629 + 0.001 * 6.733482360839844
Epoch 510, val loss: 0.8489030599594116
Epoch 520, training loss: 0.026219192892313004 = 0.019485589116811752 + 0.001 * 6.733604431152344
Epoch 520, val loss: 0.8572571277618408
Epoch 530, training loss: 0.024706238880753517 = 0.017962336540222168 + 0.001 * 6.743902206420898
Epoch 530, val loss: 0.8654089570045471
Epoch 540, training loss: 0.023353634402155876 = 0.016606660559773445 + 0.001 * 6.746973991394043
Epoch 540, val loss: 0.8733131289482117
Epoch 550, training loss: 0.022129613906145096 = 0.015396694652736187 + 0.001 * 6.7329182624816895
Epoch 550, val loss: 0.8809677958488464
Epoch 560, training loss: 0.021042637526988983 = 0.014313665218651295 + 0.001 * 6.728971004486084
Epoch 560, val loss: 0.8884310722351074
Epoch 570, training loss: 0.020065786316990852 = 0.01334140170365572 + 0.001 * 6.724384307861328
Epoch 570, val loss: 0.8957195281982422
Epoch 580, training loss: 0.019194435328245163 = 0.012465975247323513 + 0.001 * 6.728458881378174
Epoch 580, val loss: 0.9027831554412842
Epoch 590, training loss: 0.018409837037324905 = 0.011675682850182056 + 0.001 * 6.734152793884277
Epoch 590, val loss: 0.9096274971961975
Epoch 600, training loss: 0.017682068049907684 = 0.010960313491523266 + 0.001 * 6.721754550933838
Epoch 600, val loss: 0.916282594203949
Epoch 610, training loss: 0.017032593488693237 = 0.010310937650501728 + 0.001 * 6.721654415130615
Epoch 610, val loss: 0.9227626919746399
Epoch 620, training loss: 0.016444558277726173 = 0.009719792753458023 + 0.001 * 6.724765300750732
Epoch 620, val loss: 0.9290701746940613
Epoch 630, training loss: 0.015910200774669647 = 0.009180271066725254 + 0.001 * 6.729929447174072
Epoch 630, val loss: 0.9352195858955383
Epoch 640, training loss: 0.015409272164106369 = 0.008686724118888378 + 0.001 * 6.7225470542907715
Epoch 640, val loss: 0.941189169883728
Epoch 650, training loss: 0.014950379729270935 = 0.008234115317463875 + 0.001 * 6.716264247894287
Epoch 650, val loss: 0.9470134377479553
Epoch 660, training loss: 0.014538032002747059 = 0.007818127050995827 + 0.001 * 6.71990442276001
Epoch 660, val loss: 0.9526852965354919
Epoch 670, training loss: 0.014148208312690258 = 0.007434943690896034 + 0.001 * 6.713264465332031
Epoch 670, val loss: 0.9581961035728455
Epoch 680, training loss: 0.013804093934595585 = 0.007081242743879557 + 0.001 * 6.722850799560547
Epoch 680, val loss: 0.9636030197143555
Epoch 690, training loss: 0.013467838056385517 = 0.006754130125045776 + 0.001 * 6.713707447052002
Epoch 690, val loss: 0.9688404202461243
Epoch 700, training loss: 0.013169077225029469 = 0.006450982298702002 + 0.001 * 6.718094825744629
Epoch 700, val loss: 0.9739696383476257
Epoch 710, training loss: 0.01288021169602871 = 0.006169536616653204 + 0.001 * 6.71067476272583
Epoch 710, val loss: 0.9789697527885437
Epoch 720, training loss: 0.01262681744992733 = 0.00590780284255743 + 0.001 * 6.719014644622803
Epoch 720, val loss: 0.9838657975196838
Epoch 730, training loss: 0.012380368076264858 = 0.0056639499962329865 + 0.001 * 6.7164177894592285
Epoch 730, val loss: 0.9886314272880554
Epoch 740, training loss: 0.012145822867751122 = 0.005436380859464407 + 0.001 * 6.709441184997559
Epoch 740, val loss: 0.993293821811676
Epoch 750, training loss: 0.0119296470656991 = 0.005223681218922138 + 0.001 * 6.705965518951416
Epoch 750, val loss: 0.9978235363960266
Epoch 760, training loss: 0.011735811829566956 = 0.005024636629968882 + 0.001 * 6.711175441741943
Epoch 760, val loss: 1.0022560358047485
Epoch 770, training loss: 0.01154525950551033 = 0.004838038235902786 + 0.001 * 6.707221031188965
Epoch 770, val loss: 1.0065970420837402
Epoch 780, training loss: 0.011369027197360992 = 0.004662876017391682 + 0.001 * 6.706151485443115
Epoch 780, val loss: 1.010834813117981
Epoch 790, training loss: 0.011203426867723465 = 0.004498213995248079 + 0.001 * 6.705212116241455
Epoch 790, val loss: 1.014994502067566
Epoch 800, training loss: 0.011045318096876144 = 0.004343223292380571 + 0.001 * 6.702095031738281
Epoch 800, val loss: 1.0190515518188477
Epoch 810, training loss: 0.01090187020599842 = 0.004197156056761742 + 0.001 * 6.704714298248291
Epoch 810, val loss: 1.023030161857605
Epoch 820, training loss: 0.010765079408884048 = 0.004059336613863707 + 0.001 * 6.705742359161377
Epoch 820, val loss: 1.0269266366958618
Epoch 830, training loss: 0.010629238560795784 = 0.003929146099835634 + 0.001 * 6.70009183883667
Epoch 830, val loss: 1.0307341814041138
Epoch 840, training loss: 0.010507242754101753 = 0.0038060294464230537 + 0.001 * 6.7012128829956055
Epoch 840, val loss: 1.0344632863998413
Epoch 850, training loss: 0.010393690317869186 = 0.0036894921213388443 + 0.001 * 6.704197406768799
Epoch 850, val loss: 1.0381091833114624
Epoch 860, training loss: 0.010274499654769897 = 0.003579029580578208 + 0.001 * 6.695469379425049
Epoch 860, val loss: 1.0416994094848633
Epoch 870, training loss: 0.01017056219279766 = 0.0034742564894258976 + 0.001 * 6.696304798126221
Epoch 870, val loss: 1.0452204942703247
Epoch 880, training loss: 0.010067041963338852 = 0.003374777501448989 + 0.001 * 6.692264080047607
Epoch 880, val loss: 1.048658847808838
Epoch 890, training loss: 0.009983152151107788 = 0.003280237317085266 + 0.001 * 6.702914714813232
Epoch 890, val loss: 1.0520248413085938
Epoch 900, training loss: 0.009882073849439621 = 0.0031902892515063286 + 0.001 * 6.691784858703613
Epoch 900, val loss: 1.0553287267684937
Epoch 910, training loss: 0.009810352697968483 = 0.003104663919657469 + 0.001 * 6.705687999725342
Epoch 910, val loss: 1.0585603713989258
Epoch 920, training loss: 0.00972320418804884 = 0.0030230532865971327 + 0.001 * 6.700150966644287
Epoch 920, val loss: 1.061755657196045
Epoch 930, training loss: 0.009632975794374943 = 0.0029452508315443993 + 0.001 * 6.687724590301514
Epoch 930, val loss: 1.0648473501205444
Epoch 940, training loss: 0.009559545665979385 = 0.0028709862381219864 + 0.001 * 6.688559532165527
Epoch 940, val loss: 1.0679104328155518
Epoch 950, training loss: 0.00948541983962059 = 0.002800056478008628 + 0.001 * 6.685363292694092
Epoch 950, val loss: 1.070900321006775
Epoch 960, training loss: 0.00943342037498951 = 0.0027322694659233093 + 0.001 * 6.701149940490723
Epoch 960, val loss: 1.0738470554351807
Epoch 970, training loss: 0.009363574907183647 = 0.0026674261316657066 + 0.001 * 6.696147918701172
Epoch 970, val loss: 1.0767126083374023
Epoch 980, training loss: 0.009286371991038322 = 0.0026053739711642265 + 0.001 * 6.680997848510742
Epoch 980, val loss: 1.0795540809631348
Epoch 990, training loss: 0.009245187975466251 = 0.0025458841118961573 + 0.001 * 6.69930362701416
Epoch 990, val loss: 1.0823312997817993
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9483
Flip ASR: 0.9378/225 nodes
The final ASR:0.72571, 0.15829, Accuracy:0.81605, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11582])
remove edge: torch.Size([2, 9366])
updated graph: torch.Size([2, 10392])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83210, 0.00462
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9439116716384888 = 1.9355379343032837 + 0.001 * 8.373779296875
Epoch 0, val loss: 1.9305580854415894
Epoch 10, training loss: 1.9340234994888306 = 1.9256497621536255 + 0.001 * 8.373703956604004
Epoch 10, val loss: 1.9209239482879639
Epoch 20, training loss: 1.9218928813934326 = 1.9135195016860962 + 0.001 * 8.373438835144043
Epoch 20, val loss: 1.9090664386749268
Epoch 30, training loss: 1.904709815979004 = 1.8963370323181152 + 0.001 * 8.37282943725586
Epoch 30, val loss: 1.8925879001617432
Epoch 40, training loss: 1.8794124126434326 = 1.871041178703308 + 0.001 * 8.3712739944458
Epoch 40, val loss: 1.869125247001648
Epoch 50, training loss: 1.8450027704238892 = 1.8366364240646362 + 0.001 * 8.366316795349121
Epoch 50, val loss: 1.8392971754074097
Epoch 60, training loss: 1.80746328830719 = 1.7991188764572144 + 0.001 * 8.34437370300293
Epoch 60, val loss: 1.810465931892395
Epoch 70, training loss: 1.7720259428024292 = 1.7637990713119507 + 0.001 * 8.226839065551758
Epoch 70, val loss: 1.7832462787628174
Epoch 80, training loss: 1.7251163721084595 = 1.717316746711731 + 0.001 * 7.79965353012085
Epoch 80, val loss: 1.7421596050262451
Epoch 90, training loss: 1.6594045162200928 = 1.6517034769058228 + 0.001 * 7.7009968757629395
Epoch 90, val loss: 1.6856868267059326
Epoch 100, training loss: 1.5735217332839966 = 1.5658706426620483 + 0.001 * 7.651078224182129
Epoch 100, val loss: 1.615256905555725
Epoch 110, training loss: 1.4767346382141113 = 1.46918785572052 + 0.001 * 7.546792984008789
Epoch 110, val loss: 1.5376402139663696
Epoch 120, training loss: 1.3783237934112549 = 1.3710535764694214 + 0.001 * 7.270211696624756
Epoch 120, val loss: 1.4593887329101562
Epoch 130, training loss: 1.2816210985183716 = 1.2745593786239624 + 0.001 * 7.061672210693359
Epoch 130, val loss: 1.3854197263717651
Epoch 140, training loss: 1.1877511739730835 = 1.1807067394256592 + 0.001 * 7.044405460357666
Epoch 140, val loss: 1.3158504962921143
Epoch 150, training loss: 1.097391963005066 = 1.0903605222702026 + 0.001 * 7.031495094299316
Epoch 150, val loss: 1.2500816583633423
Epoch 160, training loss: 1.010441780090332 = 1.0034226179122925 + 0.001 * 7.019206523895264
Epoch 160, val loss: 1.1877020597457886
Epoch 170, training loss: 0.926395833492279 = 0.9193820953369141 + 0.001 * 7.013734817504883
Epoch 170, val loss: 1.1263720989227295
Epoch 180, training loss: 0.845234751701355 = 0.8382259607315063 + 0.001 * 7.00881814956665
Epoch 180, val loss: 1.0668519735336304
Epoch 190, training loss: 0.7675322890281677 = 0.7605295777320862 + 0.001 * 7.002720832824707
Epoch 190, val loss: 1.0098109245300293
Epoch 200, training loss: 0.6943740844726562 = 0.6873809099197388 + 0.001 * 6.993171691894531
Epoch 200, val loss: 0.9574739933013916
Epoch 210, training loss: 0.6265540719032288 = 0.6195751428604126 + 0.001 * 6.978938102722168
Epoch 210, val loss: 0.9110620021820068
Epoch 220, training loss: 0.5641591548919678 = 0.5572002530097961 + 0.001 * 6.95892333984375
Epoch 220, val loss: 0.8711296916007996
Epoch 230, training loss: 0.5069116353988647 = 0.4999806582927704 + 0.001 * 6.9309492111206055
Epoch 230, val loss: 0.8375183939933777
Epoch 240, training loss: 0.45435577630996704 = 0.4474591016769409 + 0.001 * 6.896664142608643
Epoch 240, val loss: 0.810226559638977
Epoch 250, training loss: 0.40584856271743774 = 0.39898112416267395 + 0.001 * 6.867440700531006
Epoch 250, val loss: 0.7887187004089355
Epoch 260, training loss: 0.3607071042060852 = 0.3538525700569153 + 0.001 * 6.854523181915283
Epoch 260, val loss: 0.7723673582077026
Epoch 270, training loss: 0.3183978796005249 = 0.3115510642528534 + 0.001 * 6.846801280975342
Epoch 270, val loss: 0.7603794932365417
Epoch 280, training loss: 0.27877122163772583 = 0.27193012833595276 + 0.001 * 6.8410820960998535
Epoch 280, val loss: 0.751903772354126
Epoch 290, training loss: 0.24223662912845612 = 0.23539750277996063 + 0.001 * 6.839118957519531
Epoch 290, val loss: 0.7468052506446838
Epoch 300, training loss: 0.20945346355438232 = 0.20261560380458832 + 0.001 * 6.837856769561768
Epoch 300, val loss: 0.7454438805580139
Epoch 310, training loss: 0.18085460364818573 = 0.174017533659935 + 0.001 * 6.837067127227783
Epoch 310, val loss: 0.7480202913284302
Epoch 320, training loss: 0.15651635825634003 = 0.14967988431453705 + 0.001 * 6.8364715576171875
Epoch 320, val loss: 0.7541748285293579
Epoch 330, training loss: 0.13614854216575623 = 0.12931260466575623 + 0.001 * 6.835935592651367
Epoch 330, val loss: 0.7633883953094482
Epoch 340, training loss: 0.11923570930957794 = 0.11240024119615555 + 0.001 * 6.835465908050537
Epoch 340, val loss: 0.7749335169792175
Epoch 350, training loss: 0.1051744818687439 = 0.09833947569131851 + 0.001 * 6.835002422332764
Epoch 350, val loss: 0.7881451845169067
Epoch 360, training loss: 0.09342656284570694 = 0.08659200370311737 + 0.001 * 6.834556579589844
Epoch 360, val loss: 0.8025479912757874
Epoch 370, training loss: 0.08354102820158005 = 0.07670694589614868 + 0.001 * 6.834083557128906
Epoch 370, val loss: 0.8176160454750061
Epoch 380, training loss: 0.0751563012599945 = 0.06832274049520493 + 0.001 * 6.833558559417725
Epoch 380, val loss: 0.8330759406089783
Epoch 390, training loss: 0.06798329204320908 = 0.061150338500738144 + 0.001 * 6.832955837249756
Epoch 390, val loss: 0.8486765027046204
Epoch 400, training loss: 0.061791401356458664 = 0.05495915561914444 + 0.001 * 6.832245349884033
Epoch 400, val loss: 0.8642585277557373
Epoch 410, training loss: 0.05640046298503876 = 0.04956907406449318 + 0.001 * 6.831387519836426
Epoch 410, val loss: 0.8796923756599426
Epoch 420, training loss: 0.05167172849178314 = 0.04484139755368233 + 0.001 * 6.8303303718566895
Epoch 420, val loss: 0.8949521780014038
Epoch 430, training loss: 0.0474960096180439 = 0.04066700488328934 + 0.001 * 6.829002857208252
Epoch 430, val loss: 0.90992271900177
Epoch 440, training loss: 0.043789539486169815 = 0.036962226033210754 + 0.001 * 6.8273138999938965
Epoch 440, val loss: 0.9246274828910828
Epoch 450, training loss: 0.04048655927181244 = 0.03366092965006828 + 0.001 * 6.825627326965332
Epoch 450, val loss: 0.9390509724617004
Epoch 460, training loss: 0.0375366248190403 = 0.030712798237800598 + 0.001 * 6.823824882507324
Epoch 460, val loss: 0.9532086253166199
Epoch 470, training loss: 0.03489762917160988 = 0.02807694301009178 + 0.001 * 6.820685386657715
Epoch 470, val loss: 0.9670785665512085
Epoch 480, training loss: 0.032539188861846924 = 0.025719644501805305 + 0.001 * 6.819543361663818
Epoch 480, val loss: 0.9806446433067322
Epoch 490, training loss: 0.030427787452936172 = 0.023611506447196007 + 0.001 * 6.816280364990234
Epoch 490, val loss: 0.9938977360725403
Epoch 500, training loss: 0.02853773534297943 = 0.021726205945014954 + 0.001 * 6.811529159545898
Epoch 500, val loss: 1.0068892240524292
Epoch 510, training loss: 0.026852896437048912 = 0.020039979368448257 + 0.001 * 6.812916278839111
Epoch 510, val loss: 1.019553780555725
Epoch 520, training loss: 0.025329571217298508 = 0.018527764827013016 + 0.001 * 6.801805019378662
Epoch 520, val loss: 1.0319691896438599
Epoch 530, training loss: 0.023970890790224075 = 0.017170943319797516 + 0.001 * 6.799946308135986
Epoch 530, val loss: 1.043904185295105
Epoch 540, training loss: 0.02274921163916588 = 0.015953345224261284 + 0.001 * 6.795865535736084
Epoch 540, val loss: 1.055553674697876
Epoch 550, training loss: 0.02168065682053566 = 0.014855843968689442 + 0.001 * 6.824811935424805
Epoch 550, val loss: 1.0669238567352295
Epoch 560, training loss: 0.020650876685976982 = 0.013865726999938488 + 0.001 * 6.785149097442627
Epoch 560, val loss: 1.0779664516448975
Epoch 570, training loss: 0.01974654011428356 = 0.012970606796443462 + 0.001 * 6.775933265686035
Epoch 570, val loss: 1.0885967016220093
Epoch 580, training loss: 0.018927745521068573 = 0.012158055789768696 + 0.001 * 6.76969051361084
Epoch 580, val loss: 1.0992110967636108
Epoch 590, training loss: 0.018227901309728622 = 0.011417138390243053 + 0.001 * 6.810762882232666
Epoch 590, val loss: 1.1092599630355835
Epoch 600, training loss: 0.017504015937447548 = 0.010737413540482521 + 0.001 * 6.766602039337158
Epoch 600, val loss: 1.1192450523376465
Epoch 610, training loss: 0.016870301216840744 = 0.010109676979482174 + 0.001 * 6.760624885559082
Epoch 610, val loss: 1.1288444995880127
Epoch 620, training loss: 0.016273263841867447 = 0.009525533765554428 + 0.001 * 6.747730255126953
Epoch 620, val loss: 1.1383253335952759
Epoch 630, training loss: 0.015773698687553406 = 0.008980483748018742 + 0.001 * 6.793215274810791
Epoch 630, val loss: 1.1475824117660522
Epoch 640, training loss: 0.015222720801830292 = 0.008471394889056683 + 0.001 * 6.751325607299805
Epoch 640, val loss: 1.1566460132598877
Epoch 650, training loss: 0.014726396650075912 = 0.007996108382940292 + 0.001 * 6.730288505554199
Epoch 650, val loss: 1.1654542684555054
Epoch 660, training loss: 0.014300921931862831 = 0.007553464733064175 + 0.001 * 6.7474565505981445
Epoch 660, val loss: 1.174027442932129
Epoch 670, training loss: 0.013875732198357582 = 0.007142950780689716 + 0.001 * 6.732781410217285
Epoch 670, val loss: 1.1823639869689941
Epoch 680, training loss: 0.01350071094930172 = 0.0067615751177072525 + 0.001 * 6.7391357421875
Epoch 680, val loss: 1.1904879808425903
Epoch 690, training loss: 0.013138765469193459 = 0.006407712120562792 + 0.001 * 6.731052398681641
Epoch 690, val loss: 1.1983898878097534
Epoch 700, training loss: 0.01278657279908657 = 0.006079412531107664 + 0.001 * 6.707159996032715
Epoch 700, val loss: 1.2060985565185547
Epoch 710, training loss: 0.012482283636927605 = 0.005774929188191891 + 0.001 * 6.7073540687561035
Epoch 710, val loss: 1.2136940956115723
Epoch 720, training loss: 0.012221605516970158 = 0.005492333322763443 + 0.001 * 6.72927188873291
Epoch 720, val loss: 1.2208982706069946
Epoch 730, training loss: 0.011942843906581402 = 0.0052306619472801685 + 0.001 * 6.712181568145752
Epoch 730, val loss: 1.2279534339904785
Epoch 740, training loss: 0.011680593714118004 = 0.004987443331629038 + 0.001 * 6.693150043487549
Epoch 740, val loss: 1.2348240613937378
Epoch 750, training loss: 0.011449704878032207 = 0.004761178977787495 + 0.001 * 6.688525676727295
Epoch 750, val loss: 1.241621494293213
Epoch 760, training loss: 0.011247540824115276 = 0.004550658166408539 + 0.001 * 6.696882247924805
Epoch 760, val loss: 1.2481842041015625
Epoch 770, training loss: 0.01103453990072012 = 0.004354753997176886 + 0.001 * 6.67978572845459
Epoch 770, val loss: 1.2546179294586182
Epoch 780, training loss: 0.010927438735961914 = 0.00417197635397315 + 0.001 * 6.755462646484375
Epoch 780, val loss: 1.2608702182769775
Epoch 790, training loss: 0.010686417110264301 = 0.00400127749890089 + 0.001 * 6.685139179229736
Epoch 790, val loss: 1.2670243978500366
Epoch 800, training loss: 0.010529255494475365 = 0.0038416185416281223 + 0.001 * 6.6876373291015625
Epoch 800, val loss: 1.272987723350525
Epoch 810, training loss: 0.01037346851080656 = 0.003692035796120763 + 0.001 * 6.681432723999023
Epoch 810, val loss: 1.278780221939087
Epoch 820, training loss: 0.010222651064395905 = 0.0035518179647624493 + 0.001 * 6.670833110809326
Epoch 820, val loss: 1.2844732999801636
Epoch 830, training loss: 0.010105251334607601 = 0.0034201161470264196 + 0.001 * 6.685134410858154
Epoch 830, val loss: 1.2900519371032715
Epoch 840, training loss: 0.009967844001948833 = 0.0032963408157229424 + 0.001 * 6.671503067016602
Epoch 840, val loss: 1.2954657077789307
Epoch 850, training loss: 0.009857861325144768 = 0.0031799557618796825 + 0.001 * 6.677905559539795
Epoch 850, val loss: 1.3007681369781494
Epoch 860, training loss: 0.009740021079778671 = 0.003070314647629857 + 0.001 * 6.669705867767334
Epoch 860, val loss: 1.3059399127960205
Epoch 870, training loss: 0.009640351869165897 = 0.0029668868519365788 + 0.001 * 6.673464775085449
Epoch 870, val loss: 1.311031460762024
Epoch 880, training loss: 0.009534680284559727 = 0.002869365504011512 + 0.001 * 6.665314197540283
Epoch 880, val loss: 1.3159894943237305
Epoch 890, training loss: 0.009436476044356823 = 0.002777279121801257 + 0.001 * 6.659196376800537
Epoch 890, val loss: 1.3208731412887573
Epoch 900, training loss: 0.00935791153460741 = 0.0026901429519057274 + 0.001 * 6.667768478393555
Epoch 900, val loss: 1.3256299495697021
Epoch 910, training loss: 0.009265340864658356 = 0.0026076440699398518 + 0.001 * 6.65769624710083
Epoch 910, val loss: 1.330304503440857
Epoch 920, training loss: 0.009190494194626808 = 0.002529475837945938 + 0.001 * 6.661017894744873
Epoch 920, val loss: 1.3348833322525024
Epoch 930, training loss: 0.009115062654018402 = 0.002455314854159951 + 0.001 * 6.659747123718262
Epoch 930, val loss: 1.3393938541412354
Epoch 940, training loss: 0.009051032364368439 = 0.002384896157309413 + 0.001 * 6.666136264801025
Epoch 940, val loss: 1.3437782526016235
Epoch 950, training loss: 0.008973057381808758 = 0.002318042330443859 + 0.001 * 6.655014514923096
Epoch 950, val loss: 1.3481009006500244
Epoch 960, training loss: 0.008907568641006947 = 0.002254436956718564 + 0.001 * 6.653131484985352
Epoch 960, val loss: 1.352324366569519
Epoch 970, training loss: 0.008849235251545906 = 0.0021938802674412727 + 0.001 * 6.6553544998168945
Epoch 970, val loss: 1.356453537940979
Epoch 980, training loss: 0.008787267841398716 = 0.0021361801773309708 + 0.001 * 6.651087284088135
Epoch 980, val loss: 1.3605338335037231
Epoch 990, training loss: 0.00876875314861536 = 0.002081169979646802 + 0.001 * 6.687582969665527
Epoch 990, val loss: 1.3645422458648682
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.6421
Flip ASR: 0.5778/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.960631012916565 = 1.9522572755813599 + 0.001 * 8.373766899108887
Epoch 0, val loss: 1.9443522691726685
Epoch 10, training loss: 1.950317144393921 = 1.9419435262680054 + 0.001 * 8.373639106750488
Epoch 10, val loss: 1.9339642524719238
Epoch 20, training loss: 1.9379655122756958 = 1.929592251777649 + 0.001 * 8.373249053955078
Epoch 20, val loss: 1.9209246635437012
Epoch 30, training loss: 1.9208182096481323 = 1.9124456644058228 + 0.001 * 8.372516632080078
Epoch 30, val loss: 1.9024531841278076
Epoch 40, training loss: 1.8952909708023071 = 1.8869199752807617 + 0.001 * 8.371045112609863
Epoch 40, val loss: 1.8751782178878784
Epoch 50, training loss: 1.8585073947906494 = 1.8501399755477905 + 0.001 * 8.367416381835938
Epoch 50, val loss: 1.8373641967773438
Epoch 60, training loss: 1.813090443611145 = 1.8047361373901367 + 0.001 * 8.35425853729248
Epoch 60, val loss: 1.7947111129760742
Epoch 70, training loss: 1.7692426443099976 = 1.7609614133834839 + 0.001 * 8.28125
Epoch 70, val loss: 1.7576417922973633
Epoch 80, training loss: 1.7189148664474487 = 1.711083173751831 + 0.001 * 7.831660747528076
Epoch 80, val loss: 1.7140324115753174
Epoch 90, training loss: 1.6492129564285278 = 1.641559362411499 + 0.001 * 7.653578281402588
Epoch 90, val loss: 1.653767704963684
Epoch 100, training loss: 1.5588048696517944 = 1.5511869192123413 + 0.001 * 7.617910861968994
Epoch 100, val loss: 1.579376220703125
Epoch 110, training loss: 1.459707260131836 = 1.4521156549453735 + 0.001 * 7.591579437255859
Epoch 110, val loss: 1.502374291419983
Epoch 120, training loss: 1.3689804077148438 = 1.3614606857299805 + 0.001 * 7.519777297973633
Epoch 120, val loss: 1.4372597932815552
Epoch 130, training loss: 1.2924221754074097 = 1.2851117849349976 + 0.001 * 7.310418128967285
Epoch 130, val loss: 1.3877226114273071
Epoch 140, training loss: 1.2283676862716675 = 1.2212482690811157 + 0.001 * 7.11941385269165
Epoch 140, val loss: 1.3501341342926025
Epoch 150, training loss: 1.1721166372299194 = 1.1651098728179932 + 0.001 * 7.006795883178711
Epoch 150, val loss: 1.3185064792633057
Epoch 160, training loss: 1.1177587509155273 = 1.1108094453811646 + 0.001 * 6.949358940124512
Epoch 160, val loss: 1.2883986234664917
Epoch 170, training loss: 1.0600675344467163 = 1.053146481513977 + 0.001 * 6.921051979064941
Epoch 170, val loss: 1.2557110786437988
Epoch 180, training loss: 0.9965239763259888 = 0.989626944065094 + 0.001 * 6.897059440612793
Epoch 180, val loss: 1.2180888652801514
Epoch 190, training loss: 0.9276713132858276 = 0.9208035469055176 + 0.001 * 6.867746353149414
Epoch 190, val loss: 1.1752554178237915
Epoch 200, training loss: 0.8555095791816711 = 0.8486686944961548 + 0.001 * 6.84086275100708
Epoch 200, val loss: 1.1283684968948364
Epoch 210, training loss: 0.7819753289222717 = 0.7751548886299133 + 0.001 * 6.8204216957092285
Epoch 210, val loss: 1.077890157699585
Epoch 220, training loss: 0.7090076208114624 = 0.7022004723548889 + 0.001 * 6.807151794433594
Epoch 220, val loss: 1.0257201194763184
Epoch 230, training loss: 0.6388081312179565 = 0.632010281085968 + 0.001 * 6.79782772064209
Epoch 230, val loss: 0.9752216339111328
Epoch 240, training loss: 0.5730555057525635 = 0.5662677884101868 + 0.001 * 6.7877020835876465
Epoch 240, val loss: 0.9298455715179443
Epoch 250, training loss: 0.5120914578437805 = 0.505313515663147 + 0.001 * 6.777951717376709
Epoch 250, val loss: 0.8903205990791321
Epoch 260, training loss: 0.45526695251464844 = 0.4484984874725342 + 0.001 * 6.768466472625732
Epoch 260, val loss: 0.8565434813499451
Epoch 270, training loss: 0.4018785059452057 = 0.3951156735420227 + 0.001 * 6.762838363647461
Epoch 270, val loss: 0.8280267119407654
Epoch 280, training loss: 0.3518446683883667 = 0.34508752822875977 + 0.001 * 6.757129192352295
Epoch 280, val loss: 0.8040390014648438
Epoch 290, training loss: 0.30583980679512024 = 0.29908546805381775 + 0.001 * 6.754324913024902
Epoch 290, val loss: 0.7845205664634705
Epoch 300, training loss: 0.264474481344223 = 0.257721483707428 + 0.001 * 6.752996444702148
Epoch 300, val loss: 0.7695960998535156
Epoch 310, training loss: 0.22791114449501038 = 0.22115813195705414 + 0.001 * 6.753018379211426
Epoch 310, val loss: 0.7595347166061401
Epoch 320, training loss: 0.196116104722023 = 0.1893625557422638 + 0.001 * 6.753543376922607
Epoch 320, val loss: 0.7540997266769409
Epoch 330, training loss: 0.16895700991153717 = 0.16220271587371826 + 0.001 * 6.75429630279541
Epoch 330, val loss: 0.752599835395813
Epoch 340, training loss: 0.14625315368175507 = 0.13949783146381378 + 0.001 * 6.755326747894287
Epoch 340, val loss: 0.7544873356819153
Epoch 350, training loss: 0.12750034034252167 = 0.12074429541826248 + 0.001 * 6.7560505867004395
Epoch 350, val loss: 0.7593752145767212
Epoch 360, training loss: 0.11204908043146133 = 0.10529230535030365 + 0.001 * 6.756773948669434
Epoch 360, val loss: 0.7662940621376038
Epoch 370, training loss: 0.09921862930059433 = 0.09246155619621277 + 0.001 * 6.757075309753418
Epoch 370, val loss: 0.7747495770454407
Epoch 380, training loss: 0.08838106691837311 = 0.08162370324134827 + 0.001 * 6.757366180419922
Epoch 380, val loss: 0.7845140695571899
Epoch 390, training loss: 0.07910998165607452 = 0.07235243916511536 + 0.001 * 6.757539749145508
Epoch 390, val loss: 0.7952550649642944
Epoch 400, training loss: 0.07112075388431549 = 0.06436286121606827 + 0.001 * 6.757895469665527
Epoch 400, val loss: 0.8066344857215881
Epoch 410, training loss: 0.06420931965112686 = 0.0574520006775856 + 0.001 * 6.7573161125183105
Epoch 410, val loss: 0.8185431957244873
Epoch 420, training loss: 0.05818423256278038 = 0.05142701044678688 + 0.001 * 6.757222652435303
Epoch 420, val loss: 0.8304970264434814
Epoch 430, training loss: 0.052880335599184036 = 0.04612351208925247 + 0.001 * 6.75682258605957
Epoch 430, val loss: 0.8426195979118347
Epoch 440, training loss: 0.04815021529793739 = 0.041393786668777466 + 0.001 * 6.756427764892578
Epoch 440, val loss: 0.8547112345695496
Epoch 450, training loss: 0.04394540935754776 = 0.03718912973999977 + 0.001 * 6.756277561187744
Epoch 450, val loss: 0.866831362247467
Epoch 460, training loss: 0.04023536294698715 = 0.03348051756620407 + 0.001 * 6.754843235015869
Epoch 460, val loss: 0.8787814378738403
Epoch 470, training loss: 0.03696334362030029 = 0.030209356918931007 + 0.001 * 6.753986358642578
Epoch 470, val loss: 0.8908525109291077
Epoch 480, training loss: 0.03406384214758873 = 0.027310285717248917 + 0.001 * 6.7535552978515625
Epoch 480, val loss: 0.9023786187171936
Epoch 490, training loss: 0.03157147020101547 = 0.024818774312734604 + 0.001 * 6.752695560455322
Epoch 490, val loss: 0.9137313365936279
Epoch 500, training loss: 0.029444515705108643 = 0.022692017257213593 + 0.001 * 6.752499103546143
Epoch 500, val loss: 0.9251567125320435
Epoch 510, training loss: 0.027605991810560226 = 0.02085234969854355 + 0.001 * 6.7536420822143555
Epoch 510, val loss: 0.9364916086196899
Epoch 520, training loss: 0.025989819318056107 = 0.0192403607070446 + 0.001 * 6.749458312988281
Epoch 520, val loss: 0.9477763772010803
Epoch 530, training loss: 0.024554289877414703 = 0.0178074948489666 + 0.001 * 6.746793746948242
Epoch 530, val loss: 0.9589098691940308
Epoch 540, training loss: 0.023271897807717323 = 0.01652524247765541 + 0.001 * 6.746654987335205
Epoch 540, val loss: 0.9697801470756531
Epoch 550, training loss: 0.022119808942079544 = 0.015374894253909588 + 0.001 * 6.74491548538208
Epoch 550, val loss: 0.9804348349571228
Epoch 560, training loss: 0.021082274615764618 = 0.014339285902678967 + 0.001 * 6.742987632751465
Epoch 560, val loss: 0.9908037185668945
Epoch 570, training loss: 0.02015179954469204 = 0.013410788029432297 + 0.001 * 6.741011142730713
Epoch 570, val loss: 1.0010055303573608
Epoch 580, training loss: 0.01930922642350197 = 0.012568588368594646 + 0.001 * 6.740638732910156
Epoch 580, val loss: 1.0110852718353271
Epoch 590, training loss: 0.01853562518954277 = 0.011798819527029991 + 0.001 * 6.7368059158325195
Epoch 590, val loss: 1.0209606885910034
Epoch 600, training loss: 0.017826713621616364 = 0.011092551052570343 + 0.001 * 6.7341628074646
Epoch 600, val loss: 1.0307185649871826
Epoch 610, training loss: 0.017190102487802505 = 0.010442813858389854 + 0.001 * 6.747288703918457
Epoch 610, val loss: 1.0402382612228394
Epoch 620, training loss: 0.016578424721956253 = 0.00984627939760685 + 0.001 * 6.732145309448242
Epoch 620, val loss: 1.0496309995651245
Epoch 630, training loss: 0.016026554629206657 = 0.009297913871705532 + 0.001 * 6.728640556335449
Epoch 630, val loss: 1.0588210821151733
Epoch 640, training loss: 0.015520753338932991 = 0.008792911656200886 + 0.001 * 6.727841377258301
Epoch 640, val loss: 1.0678815841674805
Epoch 650, training loss: 0.01505201030522585 = 0.008327223360538483 + 0.001 * 6.724786758422852
Epoch 650, val loss: 1.0767159461975098
Epoch 660, training loss: 0.014621458947658539 = 0.00789732951670885 + 0.001 * 6.724129676818848
Epoch 660, val loss: 1.0853394269943237
Epoch 670, training loss: 0.014222053810954094 = 0.007500333245843649 + 0.001 * 6.721719741821289
Epoch 670, val loss: 1.0937976837158203
Epoch 680, training loss: 0.013856508769094944 = 0.007132761180400848 + 0.001 * 6.723747253417969
Epoch 680, val loss: 1.1020302772521973
Epoch 690, training loss: 0.013515232130885124 = 0.006791937630623579 + 0.001 * 6.723294258117676
Epoch 690, val loss: 1.1101055145263672
Epoch 700, training loss: 0.013199592009186745 = 0.00647560553625226 + 0.001 * 6.723986625671387
Epoch 700, val loss: 1.1180206537246704
Epoch 710, training loss: 0.01290374156087637 = 0.006181617733091116 + 0.001 * 6.722123622894287
Epoch 710, val loss: 1.1257508993148804
Epoch 720, training loss: 0.012627333402633667 = 0.005907765123993158 + 0.001 * 6.71956729888916
Epoch 720, val loss: 1.133261799812317
Epoch 730, training loss: 0.01236594095826149 = 0.005652448628097773 + 0.001 * 6.713491439819336
Epoch 730, val loss: 1.1406502723693848
Epoch 740, training loss: 0.01214146800339222 = 0.005414223298430443 + 0.001 * 6.7272443771362305
Epoch 740, val loss: 1.147888422012329
Epoch 750, training loss: 0.011903086677193642 = 0.005191437434405088 + 0.001 * 6.711648464202881
Epoch 750, val loss: 1.154935359954834
Epoch 760, training loss: 0.011693518608808517 = 0.004982994869351387 + 0.001 * 6.710524082183838
Epoch 760, val loss: 1.16191828250885
Epoch 770, training loss: 0.0114977415651083 = 0.004787717014551163 + 0.001 * 6.710024356842041
Epoch 770, val loss: 1.1686538457870483
Epoch 780, training loss: 0.011318061500787735 = 0.004604168236255646 + 0.001 * 6.713892936706543
Epoch 780, val loss: 1.1753007173538208
Epoch 790, training loss: 0.011152548715472221 = 0.004431447479873896 + 0.001 * 6.721100330352783
Epoch 790, val loss: 1.1818344593048096
Epoch 800, training loss: 0.010983964428305626 = 0.004268912132829428 + 0.001 * 6.715051651000977
Epoch 800, val loss: 1.1881706714630127
Epoch 810, training loss: 0.01083560474216938 = 0.00411599688231945 + 0.001 * 6.719607830047607
Epoch 810, val loss: 1.1943777799606323
Epoch 820, training loss: 0.010683035477995872 = 0.003971993923187256 + 0.001 * 6.711041450500488
Epoch 820, val loss: 1.200473427772522
Epoch 830, training loss: 0.010542897507548332 = 0.0038360641337931156 + 0.001 * 6.7068328857421875
Epoch 830, val loss: 1.2064625024795532
Epoch 840, training loss: 0.010417469777166843 = 0.0037077695596963167 + 0.001 * 6.709699630737305
Epoch 840, val loss: 1.2123183012008667
Epoch 850, training loss: 0.010288967750966549 = 0.003586475271731615 + 0.001 * 6.7024922370910645
Epoch 850, val loss: 1.2180227041244507
Epoch 860, training loss: 0.010188967920839787 = 0.003471718868240714 + 0.001 * 6.717248916625977
Epoch 860, val loss: 1.2236398458480835
Epoch 870, training loss: 0.010062403976917267 = 0.003363090567290783 + 0.001 * 6.699313163757324
Epoch 870, val loss: 1.2291271686553955
Epoch 880, training loss: 0.009961935691535473 = 0.003259960561990738 + 0.001 * 6.701974868774414
Epoch 880, val loss: 1.2345510721206665
Epoch 890, training loss: 0.009856056421995163 = 0.0031619600486010313 + 0.001 * 6.694095611572266
Epoch 890, val loss: 1.2398347854614258
Epoch 900, training loss: 0.009768249467015266 = 0.0030689258128404617 + 0.001 * 6.699323654174805
Epoch 900, val loss: 1.2450307607650757
Epoch 910, training loss: 0.009677047841250896 = 0.0029803921934217215 + 0.001 * 6.6966552734375
Epoch 910, val loss: 1.2500944137573242
Epoch 920, training loss: 0.009596182033419609 = 0.0028962790966033936 + 0.001 * 6.699902057647705
Epoch 920, val loss: 1.2551223039627075
Epoch 930, training loss: 0.009516114369034767 = 0.0028161287773400545 + 0.001 * 6.699985027313232
Epoch 930, val loss: 1.260001540184021
Epoch 940, training loss: 0.009464713744819164 = 0.002739832503721118 + 0.001 * 6.724881172180176
Epoch 940, val loss: 1.2648205757141113
Epoch 950, training loss: 0.009361729957163334 = 0.0026671786326915026 + 0.001 * 6.694551467895508
Epoch 950, val loss: 1.2695279121398926
Epoch 960, training loss: 0.009293178096413612 = 0.002597822342067957 + 0.001 * 6.6953558921813965
Epoch 960, val loss: 1.2742079496383667
Epoch 970, training loss: 0.00923154130578041 = 0.002531645819544792 + 0.001 * 6.699894905090332
Epoch 970, val loss: 1.278679370880127
Epoch 980, training loss: 0.009162195026874542 = 0.0024684930685907602 + 0.001 * 6.693701267242432
Epoch 980, val loss: 1.283198356628418
Epoch 990, training loss: 0.009090575389564037 = 0.0024080818984657526 + 0.001 * 6.682493209838867
Epoch 990, val loss: 1.2875410318374634
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6753
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9555251598358154 = 1.9471514225006104 + 0.001 * 8.373763084411621
Epoch 0, val loss: 1.9392926692962646
Epoch 10, training loss: 1.9453245401382446 = 1.936950922012329 + 0.001 * 8.373639106750488
Epoch 10, val loss: 1.9292514324188232
Epoch 20, training loss: 1.9322785139083862 = 1.9239052534103394 + 0.001 * 8.373276710510254
Epoch 20, val loss: 1.915857195854187
Epoch 30, training loss: 1.9134418964385986 = 1.905069351196289 + 0.001 * 8.37254810333252
Epoch 30, val loss: 1.89613938331604
Epoch 40, training loss: 1.8853914737701416 = 1.8770205974578857 + 0.001 * 8.370888710021973
Epoch 40, val loss: 1.8671320676803589
Epoch 50, training loss: 1.8467921018600464 = 1.8384265899658203 + 0.001 * 8.365540504455566
Epoch 50, val loss: 1.8295209407806396
Epoch 60, training loss: 1.8051198720932007 = 1.7967795133590698 + 0.001 * 8.340350151062012
Epoch 60, val loss: 1.7943975925445557
Epoch 70, training loss: 1.7672014236450195 = 1.7590166330337524 + 0.001 * 8.184741973876953
Epoch 70, val loss: 1.7666234970092773
Epoch 80, training loss: 1.7192678451538086 = 1.7115322351455688 + 0.001 * 7.735568046569824
Epoch 80, val loss: 1.7282443046569824
Epoch 90, training loss: 1.6538165807724 = 1.6463391780853271 + 0.001 * 7.477409362792969
Epoch 90, val loss: 1.6732516288757324
Epoch 100, training loss: 1.5684483051300049 = 1.5611671209335327 + 0.001 * 7.281130313873291
Epoch 100, val loss: 1.6021240949630737
Epoch 110, training loss: 1.471630334854126 = 1.4644190073013306 + 0.001 * 7.211317539215088
Epoch 110, val loss: 1.5242801904678345
Epoch 120, training loss: 1.372117519378662 = 1.364928126335144 + 0.001 * 7.18942928314209
Epoch 120, val loss: 1.447609305381775
Epoch 130, training loss: 1.2714028358459473 = 1.2642549276351929 + 0.001 * 7.147895812988281
Epoch 130, val loss: 1.3714860677719116
Epoch 140, training loss: 1.1691975593566895 = 1.1621037721633911 + 0.001 * 7.09381103515625
Epoch 140, val loss: 1.2938041687011719
Epoch 150, training loss: 1.0669901371002197 = 1.0599392652511597 + 0.001 * 7.050885200500488
Epoch 150, val loss: 1.215867519378662
Epoch 160, training loss: 0.9680818319320679 = 0.9610497355461121 + 0.001 * 7.032090663909912
Epoch 160, val loss: 1.1406632661819458
Epoch 170, training loss: 0.8764150738716125 = 0.8694047927856445 + 0.001 * 7.010303974151611
Epoch 170, val loss: 1.0723434686660767
Epoch 180, training loss: 0.7949590086936951 = 0.7879741787910461 + 0.001 * 6.984829902648926
Epoch 180, val loss: 1.0141806602478027
Epoch 190, training loss: 0.7240744829177856 = 0.7171189785003662 + 0.001 * 6.955489158630371
Epoch 190, val loss: 0.9669103622436523
Epoch 200, training loss: 0.6619864106178284 = 0.6550634503364563 + 0.001 * 6.922978401184082
Epoch 200, val loss: 0.9292498230934143
Epoch 210, training loss: 0.6064899563789368 = 0.5995904207229614 + 0.001 * 6.8995513916015625
Epoch 210, val loss: 0.8988971710205078
Epoch 220, training loss: 0.5558149814605713 = 0.5489256978034973 + 0.001 * 6.889260292053223
Epoch 220, val loss: 0.8737168908119202
Epoch 230, training loss: 0.5089433789253235 = 0.5020644664764404 + 0.001 * 6.878907680511475
Epoch 230, val loss: 0.8523949980735779
Epoch 240, training loss: 0.4651036262512207 = 0.45822975039482117 + 0.001 * 6.873862266540527
Epoch 240, val loss: 0.8346332907676697
Epoch 250, training loss: 0.4234837591648102 = 0.4166136384010315 + 0.001 * 6.870120048522949
Epoch 250, val loss: 0.8200539946556091
Epoch 260, training loss: 0.38328197598457336 = 0.3764144480228424 + 0.001 * 6.867517948150635
Epoch 260, val loss: 0.8082295060157776
Epoch 270, training loss: 0.3440256416797638 = 0.33716344833374023 + 0.001 * 6.862184524536133
Epoch 270, val loss: 0.7990689277648926
Epoch 280, training loss: 0.3057228624820709 = 0.29886725544929504 + 0.001 * 6.855611801147461
Epoch 280, val loss: 0.7920716404914856
Epoch 290, training loss: 0.26880672574043274 = 0.261957049369812 + 0.001 * 6.8496904373168945
Epoch 290, val loss: 0.7869088649749756
Epoch 300, training loss: 0.23400554060935974 = 0.22715573012828827 + 0.001 * 6.849803924560547
Epoch 300, val loss: 0.7832072973251343
Epoch 310, training loss: 0.20205053687095642 = 0.19521312415599823 + 0.001 * 6.837418079376221
Epoch 310, val loss: 0.7807537913322449
Epoch 320, training loss: 0.17359237372875214 = 0.16675950586795807 + 0.001 * 6.832865238189697
Epoch 320, val loss: 0.7801568508148193
Epoch 330, training loss: 0.1489402800798416 = 0.14211763441562653 + 0.001 * 6.822638511657715
Epoch 330, val loss: 0.7815842628479004
Epoch 340, training loss: 0.12807820737361908 = 0.12125352025032043 + 0.001 * 6.824690818786621
Epoch 340, val loss: 0.7852214574813843
Epoch 350, training loss: 0.11063778400421143 = 0.10382082313299179 + 0.001 * 6.816963195800781
Epoch 350, val loss: 0.7910972833633423
Epoch 360, training loss: 0.09615927189588547 = 0.08934945613145828 + 0.001 * 6.809812068939209
Epoch 360, val loss: 0.7989903688430786
Epoch 370, training loss: 0.08415187895298004 = 0.07734829932451248 + 0.001 * 6.803582668304443
Epoch 370, val loss: 0.8084113597869873
Epoch 380, training loss: 0.0741729810833931 = 0.06737017631530762 + 0.001 * 6.802806377410889
Epoch 380, val loss: 0.8190070986747742
Epoch 390, training loss: 0.065833680331707 = 0.05903259292244911 + 0.001 * 6.801085948944092
Epoch 390, val loss: 0.8304001092910767
Epoch 400, training loss: 0.05882594361901283 = 0.05202785134315491 + 0.001 * 6.798093318939209
Epoch 400, val loss: 0.8422671556472778
Epoch 410, training loss: 0.052902597934007645 = 0.04610808938741684 + 0.001 * 6.794507026672363
Epoch 410, val loss: 0.8543283939361572
Epoch 420, training loss: 0.047870442271232605 = 0.0410790853202343 + 0.001 * 6.791354656219482
Epoch 420, val loss: 0.8664016723632812
Epoch 430, training loss: 0.04357569292187691 = 0.03678485006093979 + 0.001 * 6.790842533111572
Epoch 430, val loss: 0.8784449696540833
Epoch 440, training loss: 0.03990817815065384 = 0.0331001915037632 + 0.001 * 6.807985305786133
Epoch 440, val loss: 0.8903398513793945
Epoch 450, training loss: 0.036713555455207825 = 0.029923031106591225 + 0.001 * 6.790524959564209
Epoch 450, val loss: 0.902043879032135
Epoch 460, training loss: 0.03396105766296387 = 0.027171088382601738 + 0.001 * 6.789968013763428
Epoch 460, val loss: 0.9134941101074219
Epoch 470, training loss: 0.031561605632305145 = 0.024775730445981026 + 0.001 * 6.785876274108887
Epoch 470, val loss: 0.9247350096702576
Epoch 480, training loss: 0.029467806220054626 = 0.022681768983602524 + 0.001 * 6.786036968231201
Epoch 480, val loss: 0.9357491731643677
Epoch 490, training loss: 0.027628716081380844 = 0.02084239199757576 + 0.001 * 6.786323070526123
Epoch 490, val loss: 0.9464502930641174
Epoch 500, training loss: 0.025999631732702255 = 0.01921900175511837 + 0.001 * 6.780629634857178
Epoch 500, val loss: 0.9569184184074402
Epoch 510, training loss: 0.02455909550189972 = 0.017780296504497528 + 0.001 * 6.778799057006836
Epoch 510, val loss: 0.9671356081962585
Epoch 520, training loss: 0.023274848237633705 = 0.016499865800142288 + 0.001 * 6.77498197555542
Epoch 520, val loss: 0.9770386815071106
Epoch 530, training loss: 0.022166796028614044 = 0.015356083400547504 + 0.001 * 6.810711860656738
Epoch 530, val loss: 0.9866726398468018
Epoch 540, training loss: 0.021108245477080345 = 0.014330299571156502 + 0.001 * 6.7779459953308105
Epoch 540, val loss: 0.9960389137268066
Epoch 550, training loss: 0.02018079347908497 = 0.013407113030552864 + 0.001 * 6.773679733276367
Epoch 550, val loss: 1.0051733255386353
Epoch 560, training loss: 0.019347554072737694 = 0.01257290318608284 + 0.001 * 6.7746500968933105
Epoch 560, val loss: 1.0140377283096313
Epoch 570, training loss: 0.018586259335279465 = 0.01181635633111 + 0.001 * 6.76990270614624
Epoch 570, val loss: 1.0226562023162842
Epoch 580, training loss: 0.017918402329087257 = 0.011125344783067703 + 0.001 * 6.793057918548584
Epoch 580, val loss: 1.031058430671692
Epoch 590, training loss: 0.017257392406463623 = 0.010493937879800797 + 0.001 * 6.763453483581543
Epoch 590, val loss: 1.0392791032791138
Epoch 600, training loss: 0.016678638756275177 = 0.009914074093103409 + 0.001 * 6.764564037322998
Epoch 600, val loss: 1.0473383665084839
Epoch 610, training loss: 0.016157910227775574 = 0.00938000064343214 + 0.001 * 6.777908802032471
Epoch 610, val loss: 1.0551952123641968
Epoch 620, training loss: 0.01565343700349331 = 0.008887812495231628 + 0.001 * 6.765624046325684
Epoch 620, val loss: 1.0629208087921143
Epoch 630, training loss: 0.015194293111562729 = 0.008432877250015736 + 0.001 * 6.761415958404541
Epoch 630, val loss: 1.0704246759414673
Epoch 640, training loss: 0.014774427749216557 = 0.008011944591999054 + 0.001 * 6.762482643127441
Epoch 640, val loss: 1.0778064727783203
Epoch 650, training loss: 0.014371785335242748 = 0.0076217083260416985 + 0.001 * 6.750076770782471
Epoch 650, val loss: 1.0850131511688232
Epoch 660, training loss: 0.0140341492369771 = 0.0072596510872244835 + 0.001 * 6.774497985839844
Epoch 660, val loss: 1.0920878648757935
Epoch 670, training loss: 0.013667352497577667 = 0.006923258304595947 + 0.001 * 6.744093418121338
Epoch 670, val loss: 1.0990025997161865
Epoch 680, training loss: 0.013371207751333714 = 0.006610069423913956 + 0.001 * 6.761137962341309
Epoch 680, val loss: 1.1057523488998413
Epoch 690, training loss: 0.013060593977570534 = 0.006318303756415844 + 0.001 * 6.742290496826172
Epoch 690, val loss: 1.1123716831207275
Epoch 700, training loss: 0.012790530920028687 = 0.0060460385866463184 + 0.001 * 6.7444915771484375
Epoch 700, val loss: 1.118866205215454
Epoch 710, training loss: 0.012539545074105263 = 0.005791813135147095 + 0.001 * 6.7477312088012695
Epoch 710, val loss: 1.1251801252365112
Epoch 720, training loss: 0.012303091585636139 = 0.005554160103201866 + 0.001 * 6.748930931091309
Epoch 720, val loss: 1.1314082145690918
Epoch 730, training loss: 0.012069166637957096 = 0.005331437569111586 + 0.001 * 6.737728595733643
Epoch 730, val loss: 1.137511134147644
Epoch 740, training loss: 0.01185881718993187 = 0.005122349597513676 + 0.001 * 6.7364678382873535
Epoch 740, val loss: 1.1434872150421143
Epoch 750, training loss: 0.011654086410999298 = 0.0049258568324148655 + 0.001 * 6.728229522705078
Epoch 750, val loss: 1.149338722229004
Epoch 760, training loss: 0.011478782631456852 = 0.004740995820611715 + 0.001 * 6.737786293029785
Epoch 760, val loss: 1.1550544500350952
Epoch 770, training loss: 0.011296858079731464 = 0.0045668925158679485 + 0.001 * 6.7299652099609375
Epoch 770, val loss: 1.1607197523117065
Epoch 780, training loss: 0.01117174606770277 = 0.004402497783303261 + 0.001 * 6.769248008728027
Epoch 780, val loss: 1.166222095489502
Epoch 790, training loss: 0.010981524363160133 = 0.004247263539582491 + 0.001 * 6.7342610359191895
Epoch 790, val loss: 1.1716500520706177
Epoch 800, training loss: 0.010831087827682495 = 0.004100444260984659 + 0.001 * 6.730643272399902
Epoch 800, val loss: 1.1769816875457764
Epoch 810, training loss: 0.010684148408472538 = 0.003961281850934029 + 0.001 * 6.722866058349609
Epoch 810, val loss: 1.1822463274002075
Epoch 820, training loss: 0.010560061782598495 = 0.0038286796770989895 + 0.001 * 6.731381416320801
Epoch 820, val loss: 1.1875309944152832
Epoch 830, training loss: 0.01043633557856083 = 0.00370230246335268 + 0.001 * 6.734032154083252
Epoch 830, val loss: 1.1927545070648193
Epoch 840, training loss: 0.010306968353688717 = 0.0035819762852042913 + 0.001 * 6.724991798400879
Epoch 840, val loss: 1.197862982749939
Epoch 850, training loss: 0.010199366137385368 = 0.0034673898480832577 + 0.001 * 6.73197603225708
Epoch 850, val loss: 1.2029117345809937
Epoch 860, training loss: 0.01008266769349575 = 0.0033585838973522186 + 0.001 * 6.72408390045166
Epoch 860, val loss: 1.2078957557678223
Epoch 870, training loss: 0.010005165822803974 = 0.0032549716997891665 + 0.001 * 6.7501935958862305
Epoch 870, val loss: 1.2127455472946167
Epoch 880, training loss: 0.009887896478176117 = 0.0031562969088554382 + 0.001 * 6.731598854064941
Epoch 880, val loss: 1.2175103425979614
Epoch 890, training loss: 0.009803435765206814 = 0.003062277566641569 + 0.001 * 6.7411580085754395
Epoch 890, val loss: 1.2222647666931152
Epoch 900, training loss: 0.00968981347978115 = 0.0029725469648838043 + 0.001 * 6.717266082763672
Epoch 900, val loss: 1.2268741130828857
Epoch 910, training loss: 0.009617692790925503 = 0.002886820351704955 + 0.001 * 6.730871677398682
Epoch 910, val loss: 1.2314530611038208
Epoch 920, training loss: 0.009521927684545517 = 0.0028048802632838488 + 0.001 * 6.717047214508057
Epoch 920, val loss: 1.2359535694122314
Epoch 930, training loss: 0.009466364979743958 = 0.0027265388052910566 + 0.001 * 6.73982572555542
Epoch 930, val loss: 1.2403565645217896
Epoch 940, training loss: 0.009361359290778637 = 0.0026518735103309155 + 0.001 * 6.7094855308532715
Epoch 940, val loss: 1.2447758913040161
Epoch 950, training loss: 0.009340215474367142 = 0.0025805537588894367 + 0.001 * 6.7596611976623535
Epoch 950, val loss: 1.2490332126617432
Epoch 960, training loss: 0.009205581620335579 = 0.002512603299692273 + 0.001 * 6.6929779052734375
Epoch 960, val loss: 1.2530571222305298
Epoch 970, training loss: 0.009150493890047073 = 0.0024476367980241776 + 0.001 * 6.70285701751709
Epoch 970, val loss: 1.2570780515670776
Epoch 980, training loss: 0.009072339162230492 = 0.002385516883805394 + 0.001 * 6.686821937561035
Epoch 980, val loss: 1.2610081434249878
Epoch 990, training loss: 0.009040307253599167 = 0.0023260433226823807 + 0.001 * 6.714262962341309
Epoch 990, val loss: 1.2648720741271973
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8635
Flip ASR: 0.8356/225 nodes
The final ASR:0.72694, 0.09749, Accuracy:0.81605, 0.01720
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10508])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98278, 0.00460, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9504185914993286 = 1.942044734954834 + 0.001 * 8.373835563659668
Epoch 0, val loss: 1.9410090446472168
Epoch 10, training loss: 1.9404780864715576 = 1.9321043491363525 + 0.001 * 8.373724937438965
Epoch 10, val loss: 1.9312734603881836
Epoch 20, training loss: 1.9279307126998901 = 1.9195572137832642 + 0.001 * 8.373444557189941
Epoch 20, val loss: 1.918630599975586
Epoch 30, training loss: 1.9098330736160278 = 1.9014601707458496 + 0.001 * 8.37287425994873
Epoch 30, val loss: 1.9002960920333862
Epoch 40, training loss: 1.8826911449432373 = 1.8743195533752441 + 0.001 * 8.371618270874023
Epoch 40, val loss: 1.873444676399231
Epoch 50, training loss: 1.8448991775512695 = 1.8365308046340942 + 0.001 * 8.368378639221191
Epoch 50, val loss: 1.83842933177948
Epoch 60, training loss: 1.8033427000045776 = 1.7949857711791992 + 0.001 * 8.35697078704834
Epoch 60, val loss: 1.8046334981918335
Epoch 70, training loss: 1.7643901109695435 = 1.7560940980911255 + 0.001 * 8.296035766601562
Epoch 70, val loss: 1.7738624811172485
Epoch 80, training loss: 1.7127277851104736 = 1.7047151327133179 + 0.001 * 8.012664794921875
Epoch 80, val loss: 1.7282447814941406
Epoch 90, training loss: 1.6416728496551514 = 1.6339231729507446 + 0.001 * 7.749716758728027
Epoch 90, val loss: 1.6664166450500488
Epoch 100, training loss: 1.5509008169174194 = 1.5433526039123535 + 0.001 * 7.548187255859375
Epoch 100, val loss: 1.5897585153579712
Epoch 110, training loss: 1.4518749713897705 = 1.4445736408233643 + 0.001 * 7.30133581161499
Epoch 110, val loss: 1.5086055994033813
Epoch 120, training loss: 1.3558491468429565 = 1.3486311435699463 + 0.001 * 7.218052864074707
Epoch 120, val loss: 1.433917760848999
Epoch 130, training loss: 1.2634036540985107 = 1.256304383277893 + 0.001 * 7.099297523498535
Epoch 130, val loss: 1.3646864891052246
Epoch 140, training loss: 1.1742634773254395 = 1.1672444343566895 + 0.001 * 7.019015312194824
Epoch 140, val loss: 1.3004835844039917
Epoch 150, training loss: 1.0889923572540283 = 1.0820000171661377 + 0.001 * 6.9923224449157715
Epoch 150, val loss: 1.2405098676681519
Epoch 160, training loss: 1.0080482959747314 = 1.0010755062103271 + 0.001 * 6.972811222076416
Epoch 160, val loss: 1.1843860149383545
Epoch 170, training loss: 0.9316231608390808 = 0.9246676564216614 + 0.001 * 6.9554853439331055
Epoch 170, val loss: 1.1311928033828735
Epoch 180, training loss: 0.8592563271522522 = 0.8523181676864624 + 0.001 * 6.9381585121154785
Epoch 180, val loss: 1.0806339979171753
Epoch 190, training loss: 0.7901362776756287 = 0.7832111120223999 + 0.001 * 6.925166606903076
Epoch 190, val loss: 1.0321171283721924
Epoch 200, training loss: 0.7240496873855591 = 0.7171312570571899 + 0.001 * 6.918405055999756
Epoch 200, val loss: 0.986297607421875
Epoch 210, training loss: 0.6612480878829956 = 0.6543365120887756 + 0.001 * 6.911594390869141
Epoch 210, val loss: 0.9442213177680969
Epoch 220, training loss: 0.6022043228149414 = 0.5952982306480408 + 0.001 * 6.906077861785889
Epoch 220, val loss: 0.9066202044487
Epoch 230, training loss: 0.5471706986427307 = 0.540271520614624 + 0.001 * 6.899181842803955
Epoch 230, val loss: 0.8737926483154297
Epoch 240, training loss: 0.49634042382240295 = 0.48944902420043945 + 0.001 * 6.891411781311035
Epoch 240, val loss: 0.8463364839553833
Epoch 250, training loss: 0.4495370388031006 = 0.44265568256378174 + 0.001 * 6.881369590759277
Epoch 250, val loss: 0.8242014050483704
Epoch 260, training loss: 0.40618374943733215 = 0.3993130624294281 + 0.001 * 6.870698928833008
Epoch 260, val loss: 0.8068883419036865
Epoch 270, training loss: 0.36578646302223206 = 0.35892465710639954 + 0.001 * 6.861793041229248
Epoch 270, val loss: 0.7940026521682739
Epoch 280, training loss: 0.3280823826789856 = 0.3212329149246216 + 0.001 * 6.849462985992432
Epoch 280, val loss: 0.7849342823028564
Epoch 290, training loss: 0.2930908799171448 = 0.28624799847602844 + 0.001 * 6.842889785766602
Epoch 290, val loss: 0.7789701819419861
Epoch 300, training loss: 0.26091212034225464 = 0.2540479898452759 + 0.001 * 6.864138126373291
Epoch 300, val loss: 0.776057243347168
Epoch 310, training loss: 0.23151449859142303 = 0.22467288374900818 + 0.001 * 6.841609477996826
Epoch 310, val loss: 0.7760148048400879
Epoch 320, training loss: 0.20501738786697388 = 0.19818615913391113 + 0.001 * 6.831226348876953
Epoch 320, val loss: 0.7787612676620483
Epoch 330, training loss: 0.18144989013671875 = 0.17462532222270966 + 0.001 * 6.824569225311279
Epoch 330, val loss: 0.7839440107345581
Epoch 340, training loss: 0.16075219213962555 = 0.15392473340034485 + 0.001 * 6.827455997467041
Epoch 340, val loss: 0.79145348072052
Epoch 350, training loss: 0.14271704852581024 = 0.13589194416999817 + 0.001 * 6.82511043548584
Epoch 350, val loss: 0.8010402917861938
Epoch 360, training loss: 0.12706701457500458 = 0.12024421989917755 + 0.001 * 6.822787284851074
Epoch 360, val loss: 0.8123463988304138
Epoch 370, training loss: 0.11349394917488098 = 0.1066739410161972 + 0.001 * 6.820010662078857
Epoch 370, val loss: 0.8250502943992615
Epoch 380, training loss: 0.10169714689254761 = 0.09487786889076233 + 0.001 * 6.81928014755249
Epoch 380, val loss: 0.8388544321060181
Epoch 390, training loss: 0.09141263365745544 = 0.08459466695785522 + 0.001 * 6.817966461181641
Epoch 390, val loss: 0.8534485101699829
Epoch 400, training loss: 0.08242351561784744 = 0.0756073072552681 + 0.001 * 6.816205978393555
Epoch 400, val loss: 0.8685634732246399
Epoch 410, training loss: 0.07457242906093597 = 0.06774002313613892 + 0.001 * 6.832406997680664
Epoch 410, val loss: 0.8840265274047852
Epoch 420, training loss: 0.06765866279602051 = 0.06084452569484711 + 0.001 * 6.814137935638428
Epoch 420, val loss: 0.8995780944824219
Epoch 430, training loss: 0.06160541623830795 = 0.05479465425014496 + 0.001 * 6.810760974884033
Epoch 430, val loss: 0.9150634407997131
Epoch 440, training loss: 0.05628804862499237 = 0.04948024824261665 + 0.001 * 6.807798385620117
Epoch 440, val loss: 0.9303354024887085
Epoch 450, training loss: 0.051610782742500305 = 0.04480515792965889 + 0.001 * 6.8056230545043945
Epoch 450, val loss: 0.9453890323638916
Epoch 460, training loss: 0.04749556630849838 = 0.0406852550804615 + 0.001 * 6.8103108406066895
Epoch 460, val loss: 0.9601482152938843
Epoch 470, training loss: 0.04386334866285324 = 0.037047069519758224 + 0.001 * 6.816277980804443
Epoch 470, val loss: 0.9744686484336853
Epoch 480, training loss: 0.04063351824879646 = 0.03382759913802147 + 0.001 * 6.8059186935424805
Epoch 480, val loss: 0.9883976578712463
Epoch 490, training loss: 0.03777153789997101 = 0.030973035842180252 + 0.001 * 6.798503398895264
Epoch 490, val loss: 1.0019508600234985
Epoch 500, training loss: 0.03523062914609909 = 0.028436094522476196 + 0.001 * 6.794532299041748
Epoch 500, val loss: 1.0151433944702148
Epoch 510, training loss: 0.03297053277492523 = 0.02617652527987957 + 0.001 * 6.794005393981934
Epoch 510, val loss: 1.0279285907745361
Epoch 520, training loss: 0.030952295288443565 = 0.024158917367458344 + 0.001 * 6.793376922607422
Epoch 520, val loss: 1.0403159856796265
Epoch 530, training loss: 0.029141606763005257 = 0.022352149710059166 + 0.001 * 6.789457321166992
Epoch 530, val loss: 1.0523488521575928
Epoch 540, training loss: 0.027523202821612358 = 0.020730366930365562 + 0.001 * 6.792835235595703
Epoch 540, val loss: 1.0640301704406738
Epoch 550, training loss: 0.026064667850732803 = 0.019270939752459526 + 0.001 * 6.793726921081543
Epoch 550, val loss: 1.0753446817398071
Epoch 560, training loss: 0.02474522404372692 = 0.017949122935533524 + 0.001 * 6.796100616455078
Epoch 560, val loss: 1.086389183998108
Epoch 570, training loss: 0.023540884256362915 = 0.016747822985053062 + 0.001 * 6.793059825897217
Epoch 570, val loss: 1.0970689058303833
Epoch 580, training loss: 0.022444454953074455 = 0.015652215108275414 + 0.001 * 6.792239189147949
Epoch 580, val loss: 1.1075042486190796
Epoch 590, training loss: 0.021455537527799606 = 0.0146512221544981 + 0.001 * 6.804314136505127
Epoch 590, val loss: 1.1176214218139648
Epoch 600, training loss: 0.02051842398941517 = 0.013735671527683735 + 0.001 * 6.782752513885498
Epoch 600, val loss: 1.1275277137756348
Epoch 610, training loss: 0.01967991329729557 = 0.01289747841656208 + 0.001 * 6.782433986663818
Epoch 610, val loss: 1.1371339559555054
Epoch 620, training loss: 0.018916867673397064 = 0.012129519134759903 + 0.001 * 6.78734827041626
Epoch 620, val loss: 1.146537184715271
Epoch 630, training loss: 0.018207920715212822 = 0.011425123549997807 + 0.001 * 6.782796382904053
Epoch 630, val loss: 1.1556921005249023
Epoch 640, training loss: 0.01755657233297825 = 0.010778402909636497 + 0.001 * 6.77816915512085
Epoch 640, val loss: 1.1645976305007935
Epoch 650, training loss: 0.016948455944657326 = 0.010183976963162422 + 0.001 * 6.764479160308838
Epoch 650, val loss: 1.1732642650604248
Epoch 660, training loss: 0.016398033127188683 = 0.009636894799768925 + 0.001 * 6.761137962341309
Epoch 660, val loss: 1.1816807985305786
Epoch 670, training loss: 0.015896452590823174 = 0.00913270190358162 + 0.001 * 6.763751029968262
Epoch 670, val loss: 1.1898776292800903
Epoch 680, training loss: 0.015450687147676945 = 0.008667189627885818 + 0.001 * 6.783497333526611
Epoch 680, val loss: 1.1978777647018433
Epoch 690, training loss: 0.015005694702267647 = 0.008236851543188095 + 0.001 * 6.7688422203063965
Epoch 690, val loss: 1.2056783437728882
Epoch 700, training loss: 0.014598153531551361 = 0.00783834420144558 + 0.001 * 6.759809494018555
Epoch 700, val loss: 1.2132728099822998
Epoch 710, training loss: 0.014233909547328949 = 0.007468817289918661 + 0.001 * 6.765092372894287
Epoch 710, val loss: 1.2206701040267944
Epoch 720, training loss: 0.013900365680456161 = 0.007125615142285824 + 0.001 * 6.774750709533691
Epoch 720, val loss: 1.2278774976730347
Epoch 730, training loss: 0.013573212549090385 = 0.006806418765336275 + 0.001 * 6.766793727874756
Epoch 730, val loss: 1.2349305152893066
Epoch 740, training loss: 0.013265417888760567 = 0.006509129889309406 + 0.001 * 6.756287574768066
Epoch 740, val loss: 1.2417858839035034
Epoch 750, training loss: 0.012979920953512192 = 0.006231934763491154 + 0.001 * 6.747985363006592
Epoch 750, val loss: 1.2484854459762573
Epoch 760, training loss: 0.012723746709525585 = 0.005973063409328461 + 0.001 * 6.750682830810547
Epoch 760, val loss: 1.2549915313720703
Epoch 770, training loss: 0.0124981589615345 = 0.005730970297008753 + 0.001 * 6.76718807220459
Epoch 770, val loss: 1.2613557577133179
Epoch 780, training loss: 0.012254508212208748 = 0.005504346452653408 + 0.001 * 6.750162124633789
Epoch 780, val loss: 1.2675763368606567
Epoch 790, training loss: 0.012033995240926743 = 0.0052918680012226105 + 0.001 * 6.742126941680908
Epoch 790, val loss: 1.2736129760742188
Epoch 800, training loss: 0.011836386285722256 = 0.005092398729175329 + 0.001 * 6.743987083435059
Epoch 800, val loss: 1.2795416116714478
Epoch 810, training loss: 0.011673960834741592 = 0.004904945380985737 + 0.001 * 6.769014835357666
Epoch 810, val loss: 1.2853312492370605
Epoch 820, training loss: 0.011479869484901428 = 0.004728628788143396 + 0.001 * 6.7512407302856445
Epoch 820, val loss: 1.2909716367721558
Epoch 830, training loss: 0.011305542662739754 = 0.004562535788863897 + 0.001 * 6.743006229400635
Epoch 830, val loss: 1.2964887619018555
Epoch 840, training loss: 0.011164175346493721 = 0.004405890591442585 + 0.001 * 6.758284568786621
Epoch 840, val loss: 1.3019040822982788
Epoch 850, training loss: 0.010998833924531937 = 0.004257981665432453 + 0.001 * 6.740851879119873
Epoch 850, val loss: 1.3071736097335815
Epoch 860, training loss: 0.01085939072072506 = 0.0041182199493050575 + 0.001 * 6.7411699295043945
Epoch 860, val loss: 1.3123376369476318
Epoch 870, training loss: 0.010718508623540401 = 0.003985969815403223 + 0.001 * 6.73253870010376
Epoch 870, val loss: 1.3173829317092896
Epoch 880, training loss: 0.010601259768009186 = 0.0038607348687946796 + 0.001 * 6.740524768829346
Epoch 880, val loss: 1.3223248720169067
Epoch 890, training loss: 0.010477904230356216 = 0.003742055967450142 + 0.001 * 6.735848426818848
Epoch 890, val loss: 1.3271658420562744
Epoch 900, training loss: 0.010358313098549843 = 0.0036295009776949883 + 0.001 * 6.728811740875244
Epoch 900, val loss: 1.3318878412246704
Epoch 910, training loss: 0.01026139035820961 = 0.003522634506225586 + 0.001 * 6.73875617980957
Epoch 910, val loss: 1.3365333080291748
Epoch 920, training loss: 0.01017118338495493 = 0.003421100089326501 + 0.001 * 6.750082969665527
Epoch 920, val loss: 1.3411027193069458
Epoch 930, training loss: 0.010066020302474499 = 0.0033245126251131296 + 0.001 * 6.741507053375244
Epoch 930, val loss: 1.345548391342163
Epoch 940, training loss: 0.009959018789231777 = 0.0032325878273695707 + 0.001 * 6.726430416107178
Epoch 940, val loss: 1.3499212265014648
Epoch 950, training loss: 0.00986677035689354 = 0.0031450139358639717 + 0.001 * 6.721756458282471
Epoch 950, val loss: 1.3541994094848633
Epoch 960, training loss: 0.00978594459593296 = 0.003061512717977166 + 0.001 * 6.724431991577148
Epoch 960, val loss: 1.358385443687439
Epoch 970, training loss: 0.009712107479572296 = 0.002981841564178467 + 0.001 * 6.7302656173706055
Epoch 970, val loss: 1.3625006675720215
Epoch 980, training loss: 0.009635845199227333 = 0.0029057711362838745 + 0.001 * 6.73007345199585
Epoch 980, val loss: 1.366537094116211
Epoch 990, training loss: 0.00955064594745636 = 0.0028330942150205374 + 0.001 * 6.717551231384277
Epoch 990, val loss: 1.3704948425292969
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5424
Flip ASR: 0.4578/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9483400583267212 = 1.9399662017822266 + 0.001 * 8.373814582824707
Epoch 0, val loss: 1.9368661642074585
Epoch 10, training loss: 1.937599539756775 = 1.9292258024215698 + 0.001 * 8.37372875213623
Epoch 10, val loss: 1.9253110885620117
Epoch 20, training loss: 1.9243940114974976 = 1.9160205125808716 + 0.001 * 8.373448371887207
Epoch 20, val loss: 1.9106601476669312
Epoch 30, training loss: 1.9059172868728638 = 1.8975443840026855 + 0.001 * 8.372854232788086
Epoch 30, val loss: 1.8898367881774902
Epoch 40, training loss: 1.879260540008545 = 1.8708889484405518 + 0.001 * 8.371542930603027
Epoch 40, val loss: 1.8599960803985596
Epoch 50, training loss: 1.8429051637649536 = 1.834537148475647 + 0.001 * 8.368033409118652
Epoch 50, val loss: 1.8208820819854736
Epoch 60, training loss: 1.8006011247634888 = 1.7922461032867432 + 0.001 * 8.35507869720459
Epoch 60, val loss: 1.7789521217346191
Epoch 70, training loss: 1.7567917108535767 = 1.7485051155090332 + 0.001 * 8.286614418029785
Epoch 70, val loss: 1.7398661375045776
Epoch 80, training loss: 1.7015341520309448 = 1.6936235427856445 + 0.001 * 7.910585880279541
Epoch 80, val loss: 1.6928315162658691
Epoch 90, training loss: 1.6270140409469604 = 1.619330883026123 + 0.001 * 7.683198928833008
Epoch 90, val loss: 1.629739761352539
Epoch 100, training loss: 1.5351545810699463 = 1.5275708436965942 + 0.001 * 7.583775997161865
Epoch 100, val loss: 1.553595781326294
Epoch 110, training loss: 1.4361584186553955 = 1.4286836385726929 + 0.001 * 7.4748148918151855
Epoch 110, val loss: 1.475860834121704
Epoch 120, training loss: 1.3391742706298828 = 1.3317880630493164 + 0.001 * 7.386245250701904
Epoch 120, val loss: 1.4047558307647705
Epoch 130, training loss: 1.247281789779663 = 1.2400015592575073 + 0.001 * 7.280277252197266
Epoch 130, val loss: 1.3431109189987183
Epoch 140, training loss: 1.1603786945343018 = 1.1532227993011475 + 0.001 * 7.155925273895264
Epoch 140, val loss: 1.2882074117660522
Epoch 150, training loss: 1.0777487754821777 = 1.0706875324249268 + 0.001 * 7.06123685836792
Epoch 150, val loss: 1.2369396686553955
Epoch 160, training loss: 0.9991374611854553 = 0.9921278953552246 + 0.001 * 7.009565353393555
Epoch 160, val loss: 1.189378023147583
Epoch 170, training loss: 0.924525797367096 = 0.9175446033477783 + 0.001 * 6.981164455413818
Epoch 170, val loss: 1.1437697410583496
Epoch 180, training loss: 0.8539300560951233 = 0.8469671010971069 + 0.001 * 6.962940216064453
Epoch 180, val loss: 1.1005949974060059
Epoch 190, training loss: 0.7870140671730042 = 0.7800607681274414 + 0.001 * 6.953300952911377
Epoch 190, val loss: 1.0594959259033203
Epoch 200, training loss: 0.7228255271911621 = 0.7158788442611694 + 0.001 * 6.9466938972473145
Epoch 200, val loss: 1.0207988023757935
Epoch 210, training loss: 0.6600963473320007 = 0.6531556844711304 + 0.001 * 6.940638542175293
Epoch 210, val loss: 0.9837688207626343
Epoch 220, training loss: 0.5977074503898621 = 0.5907732248306274 + 0.001 * 6.934237957000732
Epoch 220, val loss: 0.9478431344032288
Epoch 230, training loss: 0.5357947945594788 = 0.5288676023483276 + 0.001 * 6.927202224731445
Epoch 230, val loss: 0.9127539396286011
Epoch 240, training loss: 0.47615882754325867 = 0.4692404568195343 + 0.001 * 6.918357849121094
Epoch 240, val loss: 0.8804482221603394
Epoch 250, training loss: 0.42135199904441833 = 0.4144419729709625 + 0.001 * 6.910027980804443
Epoch 250, val loss: 0.853904128074646
Epoch 260, training loss: 0.37331968545913696 = 0.3664197623729706 + 0.001 * 6.899929523468018
Epoch 260, val loss: 0.8347790241241455
Epoch 270, training loss: 0.3323167562484741 = 0.325424462556839 + 0.001 * 6.892307281494141
Epoch 270, val loss: 0.8226169943809509
Epoch 280, training loss: 0.29709702730178833 = 0.2902185916900635 + 0.001 * 6.87843656539917
Epoch 280, val loss: 0.8157416582107544
Epoch 290, training loss: 0.26602813601493835 = 0.2591589689254761 + 0.001 * 6.869176387786865
Epoch 290, val loss: 0.8124437928199768
Epoch 300, training loss: 0.23794320225715637 = 0.23108303546905518 + 0.001 * 6.860160827636719
Epoch 300, val loss: 0.8115584254264832
Epoch 310, training loss: 0.21228256821632385 = 0.20543715357780457 + 0.001 * 6.845408916473389
Epoch 310, val loss: 0.8126041293144226
Epoch 320, training loss: 0.18894539773464203 = 0.18210148811340332 + 0.001 * 6.843904495239258
Epoch 320, val loss: 0.8154492378234863
Epoch 330, training loss: 0.16802459955215454 = 0.16118425130844116 + 0.001 * 6.8403472900390625
Epoch 330, val loss: 0.8202765583992004
Epoch 340, training loss: 0.14952722191810608 = 0.14269647002220154 + 0.001 * 6.830749988555908
Epoch 340, val loss: 0.8275578618049622
Epoch 350, training loss: 0.133416086435318 = 0.12658782303333282 + 0.001 * 6.828265190124512
Epoch 350, val loss: 0.8366609215736389
Epoch 360, training loss: 0.11954543739557266 = 0.11271801590919495 + 0.001 * 6.827418804168701
Epoch 360, val loss: 0.848089337348938
Epoch 370, training loss: 0.10761132836341858 = 0.100785031914711 + 0.001 * 6.826296806335449
Epoch 370, val loss: 0.8610159158706665
Epoch 380, training loss: 0.0973857045173645 = 0.09055941551923752 + 0.001 * 6.826287269592285
Epoch 380, val loss: 0.8752063512802124
Epoch 390, training loss: 0.08857812732458115 = 0.08174873143434525 + 0.001 * 6.8293938636779785
Epoch 390, val loss: 0.8905438184738159
Epoch 400, training loss: 0.08095110952854156 = 0.07412435114383698 + 0.001 * 6.826760292053223
Epoch 400, val loss: 0.9062601327896118
Epoch 410, training loss: 0.07430393248796463 = 0.06747941672801971 + 0.001 * 6.824517726898193
Epoch 410, val loss: 0.9221691489219666
Epoch 420, training loss: 0.06847956776618958 = 0.06165405362844467 + 0.001 * 6.825512886047363
Epoch 420, val loss: 0.938116192817688
Epoch 430, training loss: 0.06333937495946884 = 0.05651620775461197 + 0.001 * 6.82316780090332
Epoch 430, val loss: 0.9541053771972656
Epoch 440, training loss: 0.05878891423344612 = 0.05196624621748924 + 0.001 * 6.822668552398682
Epoch 440, val loss: 0.9695900082588196
Epoch 450, training loss: 0.0547483004629612 = 0.047926418483257294 + 0.001 * 6.82188081741333
Epoch 450, val loss: 0.9850210547447205
Epoch 460, training loss: 0.05115848407149315 = 0.04433734714984894 + 0.001 * 6.82113790512085
Epoch 460, val loss: 1.000072956085205
Epoch 470, training loss: 0.047959115356206894 = 0.04113459587097168 + 0.001 * 6.824520587921143
Epoch 470, val loss: 1.014771819114685
Epoch 480, training loss: 0.045077718794345856 = 0.03825509920716286 + 0.001 * 6.822620391845703
Epoch 480, val loss: 1.029075026512146
Epoch 490, training loss: 0.04247678071260452 = 0.03565702959895134 + 0.001 * 6.819750785827637
Epoch 490, val loss: 1.0428354740142822
Epoch 500, training loss: 0.04011445865035057 = 0.033298175781965256 + 0.001 * 6.816283702850342
Epoch 500, val loss: 1.0561195611953735
Epoch 510, training loss: 0.037950579077005386 = 0.031132055446505547 + 0.001 * 6.8185248374938965
Epoch 510, val loss: 1.0689752101898193
Epoch 520, training loss: 0.035878900438547134 = 0.029066000133752823 + 0.001 * 6.812900543212891
Epoch 520, val loss: 1.0813192129135132
Epoch 530, training loss: 0.033939462155103683 = 0.02712905779480934 + 0.001 * 6.810404300689697
Epoch 530, val loss: 1.0930702686309814
Epoch 540, training loss: 0.03213917464017868 = 0.02532299794256687 + 0.001 * 6.8161773681640625
Epoch 540, val loss: 1.1046249866485596
Epoch 550, training loss: 0.030423937365412712 = 0.02361387573182583 + 0.001 * 6.810061454772949
Epoch 550, val loss: 1.1156476736068726
Epoch 560, training loss: 0.028798583894968033 = 0.021992135792970657 + 0.001 * 6.8064470291137695
Epoch 560, val loss: 1.1261664628982544
Epoch 570, training loss: 0.027260756120085716 = 0.020449195057153702 + 0.001 * 6.81156063079834
Epoch 570, val loss: 1.1363167762756348
Epoch 580, training loss: 0.02573334611952305 = 0.018933555111289024 + 0.001 * 6.79979133605957
Epoch 580, val loss: 1.1462868452072144
Epoch 590, training loss: 0.02419506572186947 = 0.0173963513225317 + 0.001 * 6.798713684082031
Epoch 590, val loss: 1.1554031372070312
Epoch 600, training loss: 0.022731374949216843 = 0.015913499519228935 + 0.001 * 6.817875385284424
Epoch 600, val loss: 1.1640336513519287
Epoch 610, training loss: 0.02132353000342846 = 0.014522194862365723 + 0.001 * 6.801334381103516
Epoch 610, val loss: 1.1730629205703735
Epoch 620, training loss: 0.020025677978992462 = 0.013235451653599739 + 0.001 * 6.790225505828857
Epoch 620, val loss: 1.1815061569213867
Epoch 630, training loss: 0.018938086926937103 = 0.012149699963629246 + 0.001 * 6.788386821746826
Epoch 630, val loss: 1.1899082660675049
Epoch 640, training loss: 0.01802704483270645 = 0.011241280473768711 + 0.001 * 6.785763740539551
Epoch 640, val loss: 1.197965383529663
Epoch 650, training loss: 0.01728883385658264 = 0.01049869880080223 + 0.001 * 6.790134906768799
Epoch 650, val loss: 1.2060928344726562
Epoch 660, training loss: 0.016664518043398857 = 0.009873046539723873 + 0.001 * 6.791471481323242
Epoch 660, val loss: 1.2143731117248535
Epoch 670, training loss: 0.01611630618572235 = 0.009328095242381096 + 0.001 * 6.788210391998291
Epoch 670, val loss: 1.2225147485733032
Epoch 680, training loss: 0.01564362831413746 = 0.008842270821332932 + 0.001 * 6.801356792449951
Epoch 680, val loss: 1.2304632663726807
Epoch 690, training loss: 0.015184113755822182 = 0.00840360950678587 + 0.001 * 6.78050422668457
Epoch 690, val loss: 1.238417387008667
Epoch 700, training loss: 0.014779270626604557 = 0.008004268631339073 + 0.001 * 6.775001525878906
Epoch 700, val loss: 1.2462399005889893
Epoch 710, training loss: 0.014409175142645836 = 0.007637621369212866 + 0.001 * 6.7715535163879395
Epoch 710, val loss: 1.2539479732513428
Epoch 720, training loss: 0.014079045504331589 = 0.007298063021153212 + 0.001 * 6.78098201751709
Epoch 720, val loss: 1.2615008354187012
Epoch 730, training loss: 0.013749644160270691 = 0.006981490645557642 + 0.001 * 6.768153190612793
Epoch 730, val loss: 1.2688697576522827
Epoch 740, training loss: 0.013455851003527641 = 0.006684652995318174 + 0.001 * 6.771198272705078
Epoch 740, val loss: 1.2761672735214233
Epoch 750, training loss: 0.013177499175071716 = 0.0064055114053189754 + 0.001 * 6.771987438201904
Epoch 750, val loss: 1.2832800149917603
Epoch 760, training loss: 0.012903289869427681 = 0.006142626982182264 + 0.001 * 6.760662078857422
Epoch 760, val loss: 1.290236473083496
Epoch 770, training loss: 0.012662786990404129 = 0.0058947340585291386 + 0.001 * 6.768052577972412
Epoch 770, val loss: 1.297075867652893
Epoch 780, training loss: 0.012424925342202187 = 0.005661108531057835 + 0.001 * 6.763816833496094
Epoch 780, val loss: 1.3037166595458984
Epoch 790, training loss: 0.012205609120428562 = 0.0054408712312579155 + 0.001 * 6.764737606048584
Epoch 790, val loss: 1.3102909326553345
Epoch 800, training loss: 0.01200170163065195 = 0.005233344156295061 + 0.001 * 6.768357276916504
Epoch 800, val loss: 1.316690444946289
Epoch 810, training loss: 0.011801056563854218 = 0.005037790164351463 + 0.001 * 6.763266086578369
Epoch 810, val loss: 1.3229491710662842
Epoch 820, training loss: 0.011606477200984955 = 0.00485352985560894 + 0.001 * 6.752946853637695
Epoch 820, val loss: 1.3290843963623047
Epoch 830, training loss: 0.011432047933340073 = 0.004679704084992409 + 0.001 * 6.75234317779541
Epoch 830, val loss: 1.3350919485092163
Epoch 840, training loss: 0.011271822266280651 = 0.004515609238296747 + 0.001 * 6.7562127113342285
Epoch 840, val loss: 1.3409407138824463
Epoch 850, training loss: 0.011112276464700699 = 0.004360652063041925 + 0.001 * 6.75162410736084
Epoch 850, val loss: 1.3466910123825073
Epoch 860, training loss: 0.010980598628520966 = 0.004214110784232616 + 0.001 * 6.7664875984191895
Epoch 860, val loss: 1.3523223400115967
Epoch 870, training loss: 0.010824296623468399 = 0.004075455013662577 + 0.001 * 6.748841285705566
Epoch 870, val loss: 1.3578296899795532
Epoch 880, training loss: 0.01071269903331995 = 0.003944173455238342 + 0.001 * 6.768525123596191
Epoch 880, val loss: 1.363183856010437
Epoch 890, training loss: 0.010578230023384094 = 0.003819723380729556 + 0.001 * 6.7585062980651855
Epoch 890, val loss: 1.3684407472610474
Epoch 900, training loss: 0.01045753713697195 = 0.003701736219227314 + 0.001 * 6.755800724029541
Epoch 900, val loss: 1.373619794845581
Epoch 910, training loss: 0.010334352031350136 = 0.0035897190682590008 + 0.001 * 6.744632720947266
Epoch 910, val loss: 1.3786537647247314
Epoch 920, training loss: 0.010238396935164928 = 0.003483349457383156 + 0.001 * 6.75504732131958
Epoch 920, val loss: 1.383589267730713
Epoch 930, training loss: 0.010120988823473454 = 0.003382269060239196 + 0.001 * 6.7387189865112305
Epoch 930, val loss: 1.3884303569793701
Epoch 940, training loss: 0.010056108236312866 = 0.0032861160580068827 + 0.001 * 6.769991874694824
Epoch 940, val loss: 1.3931646347045898
Epoch 950, training loss: 0.009942293167114258 = 0.0031946017406880856 + 0.001 * 6.747691631317139
Epoch 950, val loss: 1.3978080749511719
Epoch 960, training loss: 0.009859409183263779 = 0.0031074287835508585 + 0.001 * 6.751979827880859
Epoch 960, val loss: 1.402306318283081
Epoch 970, training loss: 0.009768537245690823 = 0.0030243031214922667 + 0.001 * 6.744234085083008
Epoch 970, val loss: 1.4067659378051758
Epoch 980, training loss: 0.009685276076197624 = 0.002944991923868656 + 0.001 * 6.740284442901611
Epoch 980, val loss: 1.4111039638519287
Epoch 990, training loss: 0.00961549486964941 = 0.002869137329980731 + 0.001 * 6.746356964111328
Epoch 990, val loss: 1.4153598546981812
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.8376
Flip ASR: 0.8044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9649204015731812 = 1.9565465450286865 + 0.001 * 8.37384033203125
Epoch 0, val loss: 1.9519060850143433
Epoch 10, training loss: 1.954392671585083 = 1.946018934249878 + 0.001 * 8.373772621154785
Epoch 10, val loss: 1.9412541389465332
Epoch 20, training loss: 1.94209623336792 = 1.933722734451294 + 0.001 * 8.373512268066406
Epoch 20, val loss: 1.9286121129989624
Epoch 30, training loss: 1.925586462020874 = 1.9172135591506958 + 0.001 * 8.372946739196777
Epoch 30, val loss: 1.9116240739822388
Epoch 40, training loss: 1.9017524719238281 = 1.8933807611465454 + 0.001 * 8.371711730957031
Epoch 40, val loss: 1.8873274326324463
Epoch 50, training loss: 1.8677499294281006 = 1.8593813180923462 + 0.001 * 8.368552207946777
Epoch 50, val loss: 1.8538020849227905
Epoch 60, training loss: 1.8245843648910522 = 1.8162267208099365 + 0.001 * 8.357619285583496
Epoch 60, val loss: 1.8140462636947632
Epoch 70, training loss: 1.780253529548645 = 1.7719571590423584 + 0.001 * 8.296353340148926
Epoch 70, val loss: 1.7770617008209229
Epoch 80, training loss: 1.733163595199585 = 1.7251918315887451 + 0.001 * 7.971714973449707
Epoch 80, val loss: 1.7368332147598267
Epoch 90, training loss: 1.668643593788147 = 1.6609629392623901 + 0.001 * 7.680657863616943
Epoch 90, val loss: 1.6795207262039185
Epoch 100, training loss: 1.582762360572815 = 1.5753735303878784 + 0.001 * 7.388786315917969
Epoch 100, val loss: 1.6062577962875366
Epoch 110, training loss: 1.4777700901031494 = 1.4704991579055786 + 0.001 * 7.2709760665893555
Epoch 110, val loss: 1.520595669746399
Epoch 120, training loss: 1.3658933639526367 = 1.3586585521697998 + 0.001 * 7.234797477722168
Epoch 120, val loss: 1.430577278137207
Epoch 130, training loss: 1.2562878131866455 = 1.249119520187378 + 0.001 * 7.168324947357178
Epoch 130, val loss: 1.3459575176239014
Epoch 140, training loss: 1.1543030738830566 = 1.147214412689209 + 0.001 * 7.0886149406433105
Epoch 140, val loss: 1.2695107460021973
Epoch 150, training loss: 1.0636171102523804 = 1.0566037893295288 + 0.001 * 7.013298988342285
Epoch 150, val loss: 1.2032890319824219
Epoch 160, training loss: 0.9854837656021118 = 0.9785124063491821 + 0.001 * 6.9713850021362305
Epoch 160, val loss: 1.1477067470550537
Epoch 170, training loss: 0.9173764586448669 = 0.9104189276695251 + 0.001 * 6.957545757293701
Epoch 170, val loss: 1.1001489162445068
Epoch 180, training loss: 0.8545281887054443 = 0.8475767970085144 + 0.001 * 6.95141077041626
Epoch 180, val loss: 1.0566049814224243
Epoch 190, training loss: 0.7924022078514099 = 0.7854551076889038 + 0.001 * 6.947121620178223
Epoch 190, val loss: 1.0128583908081055
Epoch 200, training loss: 0.7280619740486145 = 0.7211177945137024 + 0.001 * 6.944202423095703
Epoch 200, val loss: 0.9668514728546143
Epoch 210, training loss: 0.6603838205337524 = 0.6534416079521179 + 0.001 * 6.942221641540527
Epoch 210, val loss: 0.9174547791481018
Epoch 220, training loss: 0.5901860594749451 = 0.5832455158233643 + 0.001 * 6.940533638000488
Epoch 220, val loss: 0.8665406703948975
Epoch 230, training loss: 0.5199054479598999 = 0.5129666328430176 + 0.001 * 6.93883752822876
Epoch 230, val loss: 0.8171510696411133
Epoch 240, training loss: 0.45264551043510437 = 0.4457085132598877 + 0.001 * 6.937006950378418
Epoch 240, val loss: 0.7731772661209106
Epoch 250, training loss: 0.3909544050693512 = 0.3840193450450897 + 0.001 * 6.935068130493164
Epoch 250, val loss: 0.7370210886001587
Epoch 260, training loss: 0.33620065450668335 = 0.32926756143569946 + 0.001 * 6.933082103729248
Epoch 260, val loss: 0.7094011306762695
Epoch 270, training loss: 0.28877943754196167 = 0.281848281621933 + 0.001 * 6.931162357330322
Epoch 270, val loss: 0.6896846294403076
Epoch 280, training loss: 0.24847987294197083 = 0.2415505200624466 + 0.001 * 6.929351329803467
Epoch 280, val loss: 0.6769235730171204
Epoch 290, training loss: 0.21473073959350586 = 0.20780308544635773 + 0.001 * 6.927653789520264
Epoch 290, val loss: 0.66994708776474
Epoch 300, training loss: 0.18665926158428192 = 0.17973220348358154 + 0.001 * 6.927064895629883
Epoch 300, val loss: 0.6676068305969238
Epoch 310, training loss: 0.16333885490894318 = 0.1564135104417801 + 0.001 * 6.925337791442871
Epoch 310, val loss: 0.6688559055328369
Epoch 320, training loss: 0.14389698207378387 = 0.13697338104248047 + 0.001 * 6.9236016273498535
Epoch 320, val loss: 0.6727948188781738
Epoch 330, training loss: 0.12758450210094452 = 0.12066137790679932 + 0.001 * 6.923130512237549
Epoch 330, val loss: 0.678641140460968
Epoch 340, training loss: 0.11377952247858047 = 0.10685848444700241 + 0.001 * 6.9210405349731445
Epoch 340, val loss: 0.6859055757522583
Epoch 350, training loss: 0.10199382156133652 = 0.09507443010807037 + 0.001 * 6.919390678405762
Epoch 350, val loss: 0.6941929459571838
Epoch 360, training loss: 0.09185002744197845 = 0.0849326103925705 + 0.001 * 6.917413234710693
Epoch 360, val loss: 0.703267514705658
Epoch 370, training loss: 0.08305595815181732 = 0.07614029198884964 + 0.001 * 6.915666103363037
Epoch 370, val loss: 0.7128508687019348
Epoch 380, training loss: 0.07538532465696335 = 0.06847158074378967 + 0.001 * 6.913742542266846
Epoch 380, val loss: 0.7227942943572998
Epoch 390, training loss: 0.06866569072008133 = 0.061752744019031525 + 0.001 * 6.9129462242126465
Epoch 390, val loss: 0.7328895926475525
Epoch 400, training loss: 0.06275489926338196 = 0.055845003575086594 + 0.001 * 6.909895896911621
Epoch 400, val loss: 0.7431228160858154
Epoch 410, training loss: 0.05754346027970314 = 0.05063602328300476 + 0.001 * 6.907436847686768
Epoch 410, val loss: 0.7534743547439575
Epoch 420, training loss: 0.052947867661714554 = 0.04603193327784538 + 0.001 * 6.915935516357422
Epoch 420, val loss: 0.7637619376182556
Epoch 430, training loss: 0.04885060340166092 = 0.04194300249218941 + 0.001 * 6.907599925994873
Epoch 430, val loss: 0.7740076780319214
Epoch 440, training loss: 0.045198164880275726 = 0.03829551860690117 + 0.001 * 6.902645587921143
Epoch 440, val loss: 0.7842625975608826
Epoch 450, training loss: 0.041933342814445496 = 0.03503050655126572 + 0.001 * 6.902834415435791
Epoch 450, val loss: 0.794367790222168
Epoch 460, training loss: 0.03900807350873947 = 0.032101821154356 + 0.001 * 6.906250953674316
Epoch 460, val loss: 0.8043249249458313
Epoch 470, training loss: 0.036371055990457535 = 0.029472243040800095 + 0.001 * 6.898812770843506
Epoch 470, val loss: 0.814100444316864
Epoch 480, training loss: 0.03400113061070442 = 0.027111049741506577 + 0.001 * 6.89008092880249
Epoch 480, val loss: 0.8236862421035767
Epoch 490, training loss: 0.031882114708423615 = 0.024989724159240723 + 0.001 * 6.892390251159668
Epoch 490, val loss: 0.8330673575401306
Epoch 500, training loss: 0.030004728585481644 = 0.023083005100488663 + 0.001 * 6.921722412109375
Epoch 500, val loss: 0.8422577381134033
Epoch 510, training loss: 0.02825469896197319 = 0.021367723122239113 + 0.001 * 6.886975288391113
Epoch 510, val loss: 0.851215124130249
Epoch 520, training loss: 0.026705926284193993 = 0.01982218772172928 + 0.001 * 6.883738040924072
Epoch 520, val loss: 0.8599798679351807
Epoch 530, training loss: 0.025341778993606567 = 0.01842729188501835 + 0.001 * 6.914487838745117
Epoch 530, val loss: 0.8685252666473389
Epoch 540, training loss: 0.024050582200288773 = 0.017166631296277046 + 0.001 * 6.883951187133789
Epoch 540, val loss: 0.8768406510353088
Epoch 550, training loss: 0.022890694439411163 = 0.016024533659219742 + 0.001 * 6.8661603927612305
Epoch 550, val loss: 0.8849866986274719
Epoch 560, training loss: 0.02185051143169403 = 0.014987812377512455 + 0.001 * 6.862699031829834
Epoch 560, val loss: 0.8929237723350525
Epoch 570, training loss: 0.02090654894709587 = 0.014045001938939095 + 0.001 * 6.861547470092773
Epoch 570, val loss: 0.9006506204605103
Epoch 580, training loss: 0.020043615251779556 = 0.013185620307922363 + 0.001 * 6.857994079589844
Epoch 580, val loss: 0.9082178473472595
Epoch 590, training loss: 0.01926087588071823 = 0.012400683015584946 + 0.001 * 6.860193252563477
Epoch 590, val loss: 0.9156033992767334
Epoch 600, training loss: 0.01853373646736145 = 0.011682240292429924 + 0.001 * 6.851496696472168
Epoch 600, val loss: 0.922791063785553
Epoch 610, training loss: 0.017876628786325455 = 0.01102337334305048 + 0.001 * 6.853254795074463
Epoch 610, val loss: 0.9297848343849182
Epoch 620, training loss: 0.017267221584916115 = 0.010418124496936798 + 0.001 * 6.84909725189209
Epoch 620, val loss: 0.9366663098335266
Epoch 630, training loss: 0.016709398478269577 = 0.009861055761575699 + 0.001 * 6.848341464996338
Epoch 630, val loss: 0.9433558583259583
Epoch 640, training loss: 0.016190577298402786 = 0.009347417391836643 + 0.001 * 6.843158721923828
Epoch 640, val loss: 0.9498878717422485
Epoch 650, training loss: 0.015715863555669785 = 0.008872997015714645 + 0.001 * 6.842865467071533
Epoch 650, val loss: 0.9562583565711975
Epoch 660, training loss: 0.015272298827767372 = 0.008434046059846878 + 0.001 * 6.838252544403076
Epoch 660, val loss: 0.9624907374382019
Epoch 670, training loss: 0.014865754172205925 = 0.008027233183383942 + 0.001 * 6.838520526885986
Epoch 670, val loss: 0.9685781002044678
Epoch 680, training loss: 0.01449245773255825 = 0.007649620994925499 + 0.001 * 6.842835903167725
Epoch 680, val loss: 0.9745304584503174
Epoch 690, training loss: 0.014155186712741852 = 0.007298568729311228 + 0.001 * 6.856616973876953
Epoch 690, val loss: 0.9803412556648254
Epoch 700, training loss: 0.013819235377013683 = 0.0069717769511044025 + 0.001 * 6.8474578857421875
Epoch 700, val loss: 0.9860193133354187
Epoch 710, training loss: 0.01350481528788805 = 0.006667074281722307 + 0.001 * 6.837740898132324
Epoch 710, val loss: 0.9915995001792908
Epoch 720, training loss: 0.013229500502347946 = 0.0063825948163867 + 0.001 * 6.84690523147583
Epoch 720, val loss: 0.9970200061798096
Epoch 730, training loss: 0.012947356328368187 = 0.006116604898124933 + 0.001 * 6.830751419067383
Epoch 730, val loss: 1.0023550987243652
Epoch 740, training loss: 0.012688733637332916 = 0.005867624189704657 + 0.001 * 6.821108818054199
Epoch 740, val loss: 1.0075478553771973
Epoch 750, training loss: 0.012469077482819557 = 0.005634264554828405 + 0.001 * 6.834812641143799
Epoch 750, val loss: 1.0126322507858276
Epoch 760, training loss: 0.01223386824131012 = 0.005415243562310934 + 0.001 * 6.818624019622803
Epoch 760, val loss: 1.0176279544830322
Epoch 770, training loss: 0.01205434836447239 = 0.005209457129240036 + 0.001 * 6.844891548156738
Epoch 770, val loss: 1.022515058517456
Epoch 780, training loss: 0.011834206990897655 = 0.005015884526073933 + 0.001 * 6.81832218170166
Epoch 780, val loss: 1.027303695678711
Epoch 790, training loss: 0.011667447164654732 = 0.004833589773625135 + 0.001 * 6.833857536315918
Epoch 790, val loss: 1.031999111175537
Epoch 800, training loss: 0.011477379128336906 = 0.004661722108721733 + 0.001 * 6.8156561851501465
Epoch 800, val loss: 1.0366071462631226
Epoch 810, training loss: 0.011328757740557194 = 0.004499486181885004 + 0.001 * 6.82927131652832
Epoch 810, val loss: 1.0411015748977661
Epoch 820, training loss: 0.011173149570822716 = 0.0043462165631353855 + 0.001 * 6.826932907104492
Epoch 820, val loss: 1.0455281734466553
Epoch 830, training loss: 0.011016502976417542 = 0.004201275296509266 + 0.001 * 6.815227508544922
Epoch 830, val loss: 1.049868106842041
Epoch 840, training loss: 0.01087216753512621 = 0.004064109176397324 + 0.001 * 6.808058261871338
Epoch 840, val loss: 1.0541141033172607
Epoch 850, training loss: 0.010737799108028412 = 0.0039341333322227 + 0.001 * 6.8036651611328125
Epoch 850, val loss: 1.0582754611968994
Epoch 860, training loss: 0.01060718484222889 = 0.0038107335567474365 + 0.001 * 6.796451091766357
Epoch 860, val loss: 1.0623639822006226
Epoch 870, training loss: 0.010492419824004173 = 0.003692908212542534 + 0.001 * 6.799510955810547
Epoch 870, val loss: 1.0663819313049316
Epoch 880, training loss: 0.010382633656263351 = 0.0035794200375676155 + 0.001 * 6.803212642669678
Epoch 880, val loss: 1.0703527927398682
Epoch 890, training loss: 0.010264894925057888 = 0.0034693367779254913 + 0.001 * 6.795557975769043
Epoch 890, val loss: 1.0742555856704712
Epoch 900, training loss: 0.01016689371317625 = 0.003362377407029271 + 0.001 * 6.804515838623047
Epoch 900, val loss: 1.0781803131103516
Epoch 910, training loss: 0.010066606104373932 = 0.0032585125882178545 + 0.001 * 6.808093070983887
Epoch 910, val loss: 1.0820610523223877
Epoch 920, training loss: 0.009966518729925156 = 0.003157981438562274 + 0.001 * 6.808537006378174
Epoch 920, val loss: 1.0859766006469727
Epoch 930, training loss: 0.009865687228739262 = 0.0030610202811658382 + 0.001 * 6.804666519165039
Epoch 930, val loss: 1.0898396968841553
Epoch 940, training loss: 0.009765957482159138 = 0.002967699198052287 + 0.001 * 6.798258304595947
Epoch 940, val loss: 1.0937288999557495
Epoch 950, training loss: 0.009687613695859909 = 0.0028781285509467125 + 0.001 * 6.809484958648682
Epoch 950, val loss: 1.0975505113601685
Epoch 960, training loss: 0.009574178606271744 = 0.002792715560644865 + 0.001 * 6.781462669372559
Epoch 960, val loss: 1.1013939380645752
Epoch 970, training loss: 0.009485697373747826 = 0.002710973843932152 + 0.001 * 6.774723529815674
Epoch 970, val loss: 1.1051009893417358
Epoch 980, training loss: 0.009422066621482372 = 0.0026328417006880045 + 0.001 * 6.789224147796631
Epoch 980, val loss: 1.1087985038757324
Epoch 990, training loss: 0.009337345138192177 = 0.0025584432296454906 + 0.001 * 6.778901100158691
Epoch 990, val loss: 1.1124413013458252
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8524
Flip ASR: 0.8311/225 nodes
The final ASR:0.74416, 0.14277, Accuracy:0.80247, 0.01552
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9508])
updated graph: torch.Size([2, 10566])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
The final ASR:0.98032, 0.01254, Accuracy:0.83827, 0.00873
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9577001333236694 = 1.9493262767791748 + 0.001 * 8.37387752532959
Epoch 0, val loss: 1.9494003057479858
Epoch 10, training loss: 1.9472061395645142 = 1.9388322830200195 + 0.001 * 8.373830795288086
Epoch 10, val loss: 1.9387156963348389
Epoch 20, training loss: 1.9342865943908691 = 1.9259129762649536 + 0.001 * 8.373655319213867
Epoch 20, val loss: 1.9253790378570557
Epoch 30, training loss: 1.91586434841156 = 1.9074910879135132 + 0.001 * 8.373260498046875
Epoch 30, val loss: 1.9063619375228882
Epoch 40, training loss: 1.8884154558181763 = 1.8800431489944458 + 0.001 * 8.372323036193848
Epoch 40, val loss: 1.8784358501434326
Epoch 50, training loss: 1.8502616882324219 = 1.841891884803772 + 0.001 * 8.369782447814941
Epoch 50, val loss: 1.841625452041626
Epoch 60, training loss: 1.8082270622253418 = 1.7998658418655396 + 0.001 * 8.361163139343262
Epoch 60, val loss: 1.8058549165725708
Epoch 70, training loss: 1.7710613012313843 = 1.7627371549606323 + 0.001 * 8.324176788330078
Epoch 70, val loss: 1.7775747776031494
Epoch 80, training loss: 1.723275899887085 = 1.7152036428451538 + 0.001 * 8.072253227233887
Epoch 80, val loss: 1.7375147342681885
Epoch 90, training loss: 1.657634973526001 = 1.6498005390167236 + 0.001 * 7.834396839141846
Epoch 90, val loss: 1.681807518005371
Epoch 100, training loss: 1.5722405910491943 = 1.5644713640213013 + 0.001 * 7.76919412612915
Epoch 100, val loss: 1.6106890439987183
Epoch 110, training loss: 1.4760528802871704 = 1.468375325202942 + 0.001 * 7.677563667297363
Epoch 110, val loss: 1.5317944288253784
Epoch 120, training loss: 1.378550410270691 = 1.371075987815857 + 0.001 * 7.474424362182617
Epoch 120, val loss: 1.4555712938308716
Epoch 130, training loss: 1.28329598903656 = 1.2759521007537842 + 0.001 * 7.343880653381348
Epoch 130, val loss: 1.385321021080017
Epoch 140, training loss: 1.1889135837554932 = 1.1815986633300781 + 0.001 * 7.3149189949035645
Epoch 140, val loss: 1.318308711051941
Epoch 150, training loss: 1.0938138961791992 = 1.086553931236267 + 0.001 * 7.259970188140869
Epoch 150, val loss: 1.2511630058288574
Epoch 160, training loss: 0.9973631501197815 = 0.9901612401008606 + 0.001 * 7.201926231384277
Epoch 160, val loss: 1.1844291687011719
Epoch 170, training loss: 0.902547299861908 = 0.8954124450683594 + 0.001 * 7.13486909866333
Epoch 170, val loss: 1.1206763982772827
Epoch 180, training loss: 0.8153830766677856 = 0.8082982897758484 + 0.001 * 7.08476448059082
Epoch 180, val loss: 1.0643631219863892
Epoch 190, training loss: 0.740257740020752 = 0.7332015633583069 + 0.001 * 7.056198596954346
Epoch 190, val loss: 1.0184175968170166
Epoch 200, training loss: 0.677095353603363 = 0.6700710654258728 + 0.001 * 7.024269104003906
Epoch 200, val loss: 0.9831032156944275
Epoch 210, training loss: 0.6226851940155029 = 0.615686297416687 + 0.001 * 6.998902797698975
Epoch 210, val loss: 0.9550008177757263
Epoch 220, training loss: 0.5734564065933228 = 0.5664680004119873 + 0.001 * 6.988402843475342
Epoch 220, val loss: 0.9313766956329346
Epoch 230, training loss: 0.5269025564193726 = 0.519917368888855 + 0.001 * 6.9851861000061035
Epoch 230, val loss: 0.9100056290626526
Epoch 240, training loss: 0.4817769229412079 = 0.47479450702667236 + 0.001 * 6.98242712020874
Epoch 240, val loss: 0.8904913067817688
Epoch 250, training loss: 0.43777045607566833 = 0.4307911992073059 + 0.001 * 6.979255199432373
Epoch 250, val loss: 0.873106062412262
Epoch 260, training loss: 0.39495497941970825 = 0.38797998428344727 + 0.001 * 6.974990367889404
Epoch 260, val loss: 0.8578489422798157
Epoch 270, training loss: 0.35355788469314575 = 0.34658798575401306 + 0.001 * 6.969906806945801
Epoch 270, val loss: 0.8448196053504944
Epoch 280, training loss: 0.313981294631958 = 0.30701595544815063 + 0.001 * 6.9653449058532715
Epoch 280, val loss: 0.8342803716659546
Epoch 290, training loss: 0.2768166661262512 = 0.26985445618629456 + 0.001 * 6.9622015953063965
Epoch 290, val loss: 0.8265336751937866
Epoch 300, training loss: 0.24276438355445862 = 0.23580718040466309 + 0.001 * 6.95719575881958
Epoch 300, val loss: 0.8217905163764954
Epoch 310, training loss: 0.21239647269248962 = 0.20544320344924927 + 0.001 * 6.953274250030518
Epoch 310, val loss: 0.8205311894416809
Epoch 320, training loss: 0.18596158921718597 = 0.1790110021829605 + 0.001 * 6.950584888458252
Epoch 320, val loss: 0.8228148818016052
Epoch 330, training loss: 0.1633204221725464 = 0.1563713252544403 + 0.001 * 6.949096202850342
Epoch 330, val loss: 0.8285596966743469
Epoch 340, training loss: 0.14407534897327423 = 0.13712789118289948 + 0.001 * 6.947463512420654
Epoch 340, val loss: 0.8374556303024292
Epoch 350, training loss: 0.127736896276474 = 0.12078838050365448 + 0.001 * 6.94851541519165
Epoch 350, val loss: 0.8490256071090698
Epoch 360, training loss: 0.11381634324789047 = 0.10686780512332916 + 0.001 * 6.948537349700928
Epoch 360, val loss: 0.8627158403396606
Epoch 370, training loss: 0.10189300775527954 = 0.09494602680206299 + 0.001 * 6.94698429107666
Epoch 370, val loss: 0.8781481981277466
Epoch 380, training loss: 0.09163018316030502 = 0.08468391746282578 + 0.001 * 6.946268558502197
Epoch 380, val loss: 0.8948920965194702
Epoch 390, training loss: 0.08275475353002548 = 0.07580890506505966 + 0.001 * 6.945850372314453
Epoch 390, val loss: 0.9125321507453918
Epoch 400, training loss: 0.07505159080028534 = 0.06810395419597626 + 0.001 * 6.947635650634766
Epoch 400, val loss: 0.9308202266693115
Epoch 410, training loss: 0.06833657622337341 = 0.06139053776860237 + 0.001 * 6.94603967666626
Epoch 410, val loss: 0.9493939280509949
Epoch 420, training loss: 0.06246510520577431 = 0.055519916117191315 + 0.001 * 6.945189952850342
Epoch 420, val loss: 0.9680459499359131
Epoch 430, training loss: 0.05731171742081642 = 0.05036911368370056 + 0.001 * 6.942605018615723
Epoch 430, val loss: 0.9867276549339294
Epoch 440, training loss: 0.05277911573648453 = 0.045834410935640335 + 0.001 * 6.944703578948975
Epoch 440, val loss: 1.0052766799926758
Epoch 450, training loss: 0.04877195134758949 = 0.04182972386479378 + 0.001 * 6.942227363586426
Epoch 450, val loss: 1.023621916770935
Epoch 460, training loss: 0.04521965608000755 = 0.03828258067369461 + 0.001 * 6.937073707580566
Epoch 460, val loss: 1.0417344570159912
Epoch 470, training loss: 0.04206647351384163 = 0.0351313054561615 + 0.001 * 6.935169219970703
Epoch 470, val loss: 1.0595093965530396
Epoch 480, training loss: 0.03928195685148239 = 0.032324034720659256 + 0.001 * 6.957923889160156
Epoch 480, val loss: 1.0769723653793335
Epoch 490, training loss: 0.03675977513194084 = 0.029816927388310432 + 0.001 * 6.942848205566406
Epoch 490, val loss: 1.0940974950790405
Epoch 500, training loss: 0.0345039963722229 = 0.027572205290198326 + 0.001 * 6.931788921356201
Epoch 500, val loss: 1.1109310388565063
Epoch 510, training loss: 0.03248617798089981 = 0.02555720880627632 + 0.001 * 6.928969383239746
Epoch 510, val loss: 1.127350926399231
Epoch 520, training loss: 0.03067025914788246 = 0.023743905127048492 + 0.001 * 6.9263529777526855
Epoch 520, val loss: 1.1433961391448975
Epoch 530, training loss: 0.029038017615675926 = 0.022108115255832672 + 0.001 * 6.929901599884033
Epoch 530, val loss: 1.159040093421936
Epoch 540, training loss: 0.027550069615244865 = 0.020629223436117172 + 0.001 * 6.9208455085754395
Epoch 540, val loss: 1.17431640625
Epoch 550, training loss: 0.02620680443942547 = 0.019289083778858185 + 0.001 * 6.917720794677734
Epoch 550, val loss: 1.1892588138580322
Epoch 560, training loss: 0.024991149082779884 = 0.01807173155248165 + 0.001 * 6.919417381286621
Epoch 560, val loss: 1.2037969827651978
Epoch 570, training loss: 0.02387668937444687 = 0.01696345955133438 + 0.001 * 6.913229465484619
Epoch 570, val loss: 1.2179874181747437
Epoch 580, training loss: 0.02285812422633171 = 0.01595236547291279 + 0.001 * 6.905758380889893
Epoch 580, val loss: 1.2318038940429688
Epoch 590, training loss: 0.021935157477855682 = 0.015027959831058979 + 0.001 * 6.907198429107666
Epoch 590, val loss: 1.2453135251998901
Epoch 600, training loss: 0.021130044013261795 = 0.014181100763380527 + 0.001 * 6.9489426612854
Epoch 600, val loss: 1.258465051651001
Epoch 610, training loss: 0.020323308184742928 = 0.013403773307800293 + 0.001 * 6.919534206390381
Epoch 610, val loss: 1.2712640762329102
Epoch 620, training loss: 0.01958671770989895 = 0.012688867747783661 + 0.001 * 6.897849082946777
Epoch 620, val loss: 1.2837508916854858
Epoch 630, training loss: 0.018922120332717896 = 0.012030089274048805 + 0.001 * 6.892031669616699
Epoch 630, val loss: 1.2959402799606323
Epoch 640, training loss: 0.018310105428099632 = 0.011421876028180122 + 0.001 * 6.8882293701171875
Epoch 640, val loss: 1.307834506034851
Epoch 650, training loss: 0.017740357667207718 = 0.01085929200053215 + 0.001 * 6.881064414978027
Epoch 650, val loss: 1.3194293975830078
Epoch 660, training loss: 0.01722170040011406 = 0.01033797487616539 + 0.001 * 6.883725166320801
Epoch 660, val loss: 1.3307421207427979
Epoch 670, training loss: 0.016732625663280487 = 0.009853159077465534 + 0.001 * 6.879466533660889
Epoch 670, val loss: 1.341779351234436
Epoch 680, training loss: 0.016293419525027275 = 0.009402113035321236 + 0.001 * 6.891306400299072
Epoch 680, val loss: 1.3525861501693726
Epoch 690, training loss: 0.015853166580200195 = 0.008981617167592049 + 0.001 * 6.871548652648926
Epoch 690, val loss: 1.3631515502929688
Epoch 700, training loss: 0.015459071844816208 = 0.008588829077780247 + 0.001 * 6.870242595672607
Epoch 700, val loss: 1.373496651649475
Epoch 710, training loss: 0.015081438235938549 = 0.008221268653869629 + 0.001 * 6.860169410705566
Epoch 710, val loss: 1.3835959434509277
Epoch 720, training loss: 0.01473718136548996 = 0.007876626215875149 + 0.001 * 6.8605546951293945
Epoch 720, val loss: 1.3935388326644897
Epoch 730, training loss: 0.01440995093435049 = 0.007552923168987036 + 0.001 * 6.857027530670166
Epoch 730, val loss: 1.4032902717590332
Epoch 740, training loss: 0.01411752961575985 = 0.007248565088957548 + 0.001 * 6.868964672088623
Epoch 740, val loss: 1.41286301612854
Epoch 750, training loss: 0.013812663033604622 = 0.006962103769183159 + 0.001 * 6.850558280944824
Epoch 750, val loss: 1.4222692251205444
Epoch 760, training loss: 0.013563764281570911 = 0.006692103110253811 + 0.001 * 6.8716607093811035
Epoch 760, val loss: 1.4315019845962524
Epoch 770, training loss: 0.01328249555081129 = 0.0064375512301921844 + 0.001 * 6.844944000244141
Epoch 770, val loss: 1.4405770301818848
Epoch 780, training loss: 0.013038810342550278 = 0.006197288166731596 + 0.001 * 6.841521739959717
Epoch 780, val loss: 1.4495060443878174
Epoch 790, training loss: 0.012827812694013119 = 0.005970324389636517 + 0.001 * 6.85748815536499
Epoch 790, val loss: 1.4582711458206177
Epoch 800, training loss: 0.012616491876542568 = 0.005755889229476452 + 0.001 * 6.860602378845215
Epoch 800, val loss: 1.4668759107589722
Epoch 810, training loss: 0.012400150299072266 = 0.005552920047193766 + 0.001 * 6.847230434417725
Epoch 810, val loss: 1.4753375053405762
Epoch 820, training loss: 0.012198761105537415 = 0.005360737442970276 + 0.001 * 6.8380231857299805
Epoch 820, val loss: 1.4836641550064087
Epoch 830, training loss: 0.012031394988298416 = 0.005178619641810656 + 0.001 * 6.852774620056152
Epoch 830, val loss: 1.4918252229690552
Epoch 840, training loss: 0.011846693232655525 = 0.005005970131605864 + 0.001 * 6.840723037719727
Epoch 840, val loss: 1.4998618364334106
Epoch 850, training loss: 0.011686854995787144 = 0.00484211090952158 + 0.001 * 6.844743728637695
Epoch 850, val loss: 1.5077667236328125
Epoch 860, training loss: 0.01152682863175869 = 0.004686479922384024 + 0.001 * 6.840348720550537
Epoch 860, val loss: 1.5155493021011353
Epoch 870, training loss: 0.011382769793272018 = 0.004538674373179674 + 0.001 * 6.844094753265381
Epoch 870, val loss: 1.5231763124465942
Epoch 880, training loss: 0.011229331605136395 = 0.004398134537041187 + 0.001 * 6.8311967849731445
Epoch 880, val loss: 1.5307049751281738
Epoch 890, training loss: 0.011085286736488342 = 0.004264470189809799 + 0.001 * 6.820815563201904
Epoch 890, val loss: 1.5380712747573853
Epoch 900, training loss: 0.010952243581414223 = 0.004137354902923107 + 0.001 * 6.8148884773254395
Epoch 900, val loss: 1.5453320741653442
Epoch 910, training loss: 0.010836213827133179 = 0.004016311839222908 + 0.001 * 6.819901466369629
Epoch 910, val loss: 1.5524513721466064
Epoch 920, training loss: 0.0107122752815485 = 0.0039009423926472664 + 0.001 * 6.811332702636719
Epoch 920, val loss: 1.5594456195831299
Epoch 930, training loss: 0.010609800927340984 = 0.003790865885093808 + 0.001 * 6.818934440612793
Epoch 930, val loss: 1.5663293600082397
Epoch 940, training loss: 0.010504554025828838 = 0.0036856404040008783 + 0.001 * 6.818913459777832
Epoch 940, val loss: 1.573107361793518
Epoch 950, training loss: 0.010397931560873985 = 0.003584684571251273 + 0.001 * 6.813246250152588
Epoch 950, val loss: 1.5797674655914307
Epoch 960, training loss: 0.010295175947248936 = 0.0034872994292527437 + 0.001 * 6.807876110076904
Epoch 960, val loss: 1.586379885673523
Epoch 970, training loss: 0.010211597196757793 = 0.003392969723790884 + 0.001 * 6.81862735748291
Epoch 970, val loss: 1.5929423570632935
Epoch 980, training loss: 0.01010294072329998 = 0.003301304532214999 + 0.001 * 6.8016357421875
Epoch 980, val loss: 1.5994782447814941
Epoch 990, training loss: 0.010023039765655994 = 0.003211966482922435 + 0.001 * 6.811073303222656
Epoch 990, val loss: 1.6060467958450317
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5756
Flip ASR: 0.4978/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9429317712783813 = 1.9345577955245972 + 0.001 * 8.373943328857422
Epoch 0, val loss: 1.9322453737258911
Epoch 10, training loss: 1.9339030981063843 = 1.9255292415618896 + 0.001 * 8.373902320861816
Epoch 10, val loss: 1.9226727485656738
Epoch 20, training loss: 1.9227582216262817 = 1.9143844842910767 + 0.001 * 8.373773574829102
Epoch 20, val loss: 1.9107428789138794
Epoch 30, training loss: 1.9071601629257202 = 1.8987866640090942 + 0.001 * 8.373514175415039
Epoch 30, val loss: 1.894065499305725
Epoch 40, training loss: 1.8840782642364502 = 1.875705361366272 + 0.001 * 8.372932434082031
Epoch 40, val loss: 1.8697782754898071
Epoch 50, training loss: 1.8510855436325073 = 1.8427140712738037 + 0.001 * 8.371417045593262
Epoch 50, val loss: 1.8364596366882324
Epoch 60, training loss: 1.810879111289978 = 1.8025126457214355 + 0.001 * 8.366503715515137
Epoch 60, val loss: 1.7997863292694092
Epoch 70, training loss: 1.7711870670318604 = 1.7628419399261475 + 0.001 * 8.345087051391602
Epoch 70, val loss: 1.7682216167449951
Epoch 80, training loss: 1.7216622829437256 = 1.7134732007980347 + 0.001 * 8.18903923034668
Epoch 80, val loss: 1.7280826568603516
Epoch 90, training loss: 1.6512975692749023 = 1.6434279680252075 + 0.001 * 7.869649887084961
Epoch 90, val loss: 1.6698410511016846
Epoch 100, training loss: 1.5602259635925293 = 1.5524224042892456 + 0.001 * 7.803549766540527
Epoch 100, val loss: 1.5950448513031006
Epoch 110, training loss: 1.4580209255218506 = 1.4503462314605713 + 0.001 * 7.674651622772217
Epoch 110, val loss: 1.5140056610107422
Epoch 120, training loss: 1.3558404445648193 = 1.348369836807251 + 0.001 * 7.470592021942139
Epoch 120, val loss: 1.4360719919204712
Epoch 130, training loss: 1.2571104764938354 = 1.249691367149353 + 0.001 * 7.419126033782959
Epoch 130, val loss: 1.3614559173583984
Epoch 140, training loss: 1.1620962619781494 = 1.1547523736953735 + 0.001 * 7.343928337097168
Epoch 140, val loss: 1.2903082370758057
Epoch 150, training loss: 1.0715584754943848 = 1.0642836093902588 + 0.001 * 7.274843215942383
Epoch 150, val loss: 1.2219597101211548
Epoch 160, training loss: 0.9866564869880676 = 0.9794666171073914 + 0.001 * 7.189883708953857
Epoch 160, val loss: 1.1583534479141235
Epoch 170, training loss: 0.9075660705566406 = 0.9004462361335754 + 0.001 * 7.119843482971191
Epoch 170, val loss: 1.1018190383911133
Epoch 180, training loss: 0.8333838582038879 = 0.8262994885444641 + 0.001 * 7.084368705749512
Epoch 180, val loss: 1.0517728328704834
Epoch 190, training loss: 0.7636909484863281 = 0.7566191554069519 + 0.001 * 7.071807384490967
Epoch 190, val loss: 1.0073784589767456
Epoch 200, training loss: 0.6988537311553955 = 0.6917881369590759 + 0.001 * 7.06558084487915
Epoch 200, val loss: 0.9674938321113586
Epoch 210, training loss: 0.6388906240463257 = 0.6318297386169434 + 0.001 * 7.060878276824951
Epoch 210, val loss: 0.9310156106948853
Epoch 220, training loss: 0.5828428268432617 = 0.5757863521575928 + 0.001 * 7.056474685668945
Epoch 220, val loss: 0.8969980478286743
Epoch 230, training loss: 0.5294679999351501 = 0.522416353225708 + 0.001 * 7.051644802093506
Epoch 230, val loss: 0.8644552826881409
Epoch 240, training loss: 0.4778311550617218 = 0.47078466415405273 + 0.001 * 7.046494007110596
Epoch 240, val loss: 0.8330485820770264
Epoch 250, training loss: 0.4277307093143463 = 0.42068952322006226 + 0.001 * 7.041195869445801
Epoch 250, val loss: 0.8032723069190979
Epoch 260, training loss: 0.37955528497695923 = 0.37251776456832886 + 0.001 * 7.037525653839111
Epoch 260, val loss: 0.7757664322853088
Epoch 270, training loss: 0.3340199589729309 = 0.326987624168396 + 0.001 * 7.032321453094482
Epoch 270, val loss: 0.7508783936500549
Epoch 280, training loss: 0.29202136397361755 = 0.2849937975406647 + 0.001 * 7.0275678634643555
Epoch 280, val loss: 0.7292178273200989
Epoch 290, training loss: 0.2541607618331909 = 0.24713782966136932 + 0.001 * 7.022939205169678
Epoch 290, val loss: 0.7108953595161438
Epoch 300, training loss: 0.2208605706691742 = 0.21384134888648987 + 0.001 * 7.019214630126953
Epoch 300, val loss: 0.6967151165008545
Epoch 310, training loss: 0.19219209253787994 = 0.1851806491613388 + 0.001 * 7.011441707611084
Epoch 310, val loss: 0.686738908290863
Epoch 320, training loss: 0.1678260713815689 = 0.1608206331729889 + 0.001 * 7.005441665649414
Epoch 320, val loss: 0.6804594993591309
Epoch 330, training loss: 0.14721326529979706 = 0.14021630585193634 + 0.001 * 6.996964454650879
Epoch 330, val loss: 0.6776174902915955
Epoch 340, training loss: 0.12976938486099243 = 0.1227743998169899 + 0.001 * 6.994983673095703
Epoch 340, val loss: 0.6778697967529297
Epoch 350, training loss: 0.11492373794317245 = 0.1079399511218071 + 0.001 * 6.983785152435303
Epoch 350, val loss: 0.6805227398872375
Epoch 360, training loss: 0.10222574323415756 = 0.09523425251245499 + 0.001 * 6.99149227142334
Epoch 360, val loss: 0.6849401593208313
Epoch 370, training loss: 0.09125785529613495 = 0.08428480476140976 + 0.001 * 6.973048686981201
Epoch 370, val loss: 0.6908281445503235
Epoch 380, training loss: 0.08177802711725235 = 0.07480651140213013 + 0.001 * 6.971518039703369
Epoch 380, val loss: 0.6977893710136414
Epoch 390, training loss: 0.07353933155536652 = 0.06657259911298752 + 0.001 * 6.966729640960693
Epoch 390, val loss: 0.705561101436615
Epoch 400, training loss: 0.0663742646574974 = 0.05940811708569527 + 0.001 * 6.96614408493042
Epoch 400, val loss: 0.7139073610305786
Epoch 410, training loss: 0.06013145670294762 = 0.053161874413490295 + 0.001 * 6.969581127166748
Epoch 410, val loss: 0.7225896120071411
Epoch 420, training loss: 0.05466572567820549 = 0.04770072177052498 + 0.001 * 6.965002536773682
Epoch 420, val loss: 0.7315627336502075
Epoch 430, training loss: 0.04989175498485565 = 0.04292457550764084 + 0.001 * 6.967179775238037
Epoch 430, val loss: 0.7406712174415588
Epoch 440, training loss: 0.04570635035634041 = 0.03874162584543228 + 0.001 * 6.964725494384766
Epoch 440, val loss: 0.7499654293060303
Epoch 450, training loss: 0.0420403853058815 = 0.03507765755057335 + 0.001 * 6.962728977203369
Epoch 450, val loss: 0.7591419219970703
Epoch 460, training loss: 0.0388234406709671 = 0.031862322241067886 + 0.001 * 6.961119651794434
Epoch 460, val loss: 0.7682713866233826
Epoch 470, training loss: 0.03599388152360916 = 0.02903495542705059 + 0.001 * 6.958927631378174
Epoch 470, val loss: 0.7773749828338623
Epoch 480, training loss: 0.03350474312901497 = 0.02654244378209114 + 0.001 * 6.96229887008667
Epoch 480, val loss: 0.7863826751708984
Epoch 490, training loss: 0.0313081294298172 = 0.024341654032468796 + 0.001 * 6.96647310256958
Epoch 490, val loss: 0.7952167391777039
Epoch 500, training loss: 0.0293554849922657 = 0.02239312045276165 + 0.001 * 6.962363243103027
Epoch 500, val loss: 0.8039543032646179
Epoch 510, training loss: 0.027617447078227997 = 0.020663203671574593 + 0.001 * 6.954243183135986
Epoch 510, val loss: 0.8124780654907227
Epoch 520, training loss: 0.026093803346157074 = 0.01912292279303074 + 0.001 * 6.970879554748535
Epoch 520, val loss: 0.8208698630332947
Epoch 530, training loss: 0.024699464440345764 = 0.017747094854712486 + 0.001 * 6.95236873626709
Epoch 530, val loss: 0.8290897607803345
Epoch 540, training loss: 0.023467546328902245 = 0.01651456020772457 + 0.001 * 6.952986240386963
Epoch 540, val loss: 0.8371140956878662
Epoch 550, training loss: 0.022369608283042908 = 0.015406381338834763 + 0.001 * 6.963226795196533
Epoch 550, val loss: 0.8449316024780273
Epoch 560, training loss: 0.021365487948060036 = 0.014407996088266373 + 0.001 * 6.957491397857666
Epoch 560, val loss: 0.8526487350463867
Epoch 570, training loss: 0.020457297563552856 = 0.013505342416465282 + 0.001 * 6.951955795288086
Epoch 570, val loss: 0.8601253628730774
Epoch 580, training loss: 0.01963464729487896 = 0.01268673874437809 + 0.001 * 6.947908878326416
Epoch 580, val loss: 0.8673970699310303
Epoch 590, training loss: 0.018890712410211563 = 0.01194265577942133 + 0.001 * 6.948055744171143
Epoch 590, val loss: 0.8745703101158142
Epoch 600, training loss: 0.01820698194205761 = 0.01126406341791153 + 0.001 * 6.942918300628662
Epoch 600, val loss: 0.8815729022026062
Epoch 610, training loss: 0.01762024685740471 = 0.01064365729689598 + 0.001 * 6.976590156555176
Epoch 610, val loss: 0.8883813619613647
Epoch 620, training loss: 0.01702049933373928 = 0.010075063444674015 + 0.001 * 6.945435047149658
Epoch 620, val loss: 0.8950342535972595
Epoch 630, training loss: 0.016489529982209206 = 0.009552794508635998 + 0.001 * 6.9367356300354
Epoch 630, val loss: 0.9015290141105652
Epoch 640, training loss: 0.016009705141186714 = 0.0090719573199749 + 0.001 * 6.937747001647949
Epoch 640, val loss: 0.9078843593597412
Epoch 650, training loss: 0.015572839416563511 = 0.008628211915493011 + 0.001 * 6.944627285003662
Epoch 650, val loss: 0.9141005277633667
Epoch 660, training loss: 0.0151523407548666 = 0.00821808073669672 + 0.001 * 6.934259414672852
Epoch 660, val loss: 0.9201552867889404
Epoch 670, training loss: 0.014797816053032875 = 0.007838119752705097 + 0.001 * 6.959695816040039
Epoch 670, val loss: 0.9260766506195068
Epoch 680, training loss: 0.014427630230784416 = 0.007485497277230024 + 0.001 * 6.942132472991943
Epoch 680, val loss: 0.9318678975105286
Epoch 690, training loss: 0.01410180889070034 = 0.007157708052545786 + 0.001 * 6.944100856781006
Epoch 690, val loss: 0.9375211000442505
Epoch 700, training loss: 0.01378617249429226 = 0.006852394435554743 + 0.001 * 6.933777332305908
Epoch 700, val loss: 0.9430508017539978
Epoch 710, training loss: 0.013489004224538803 = 0.006567599717527628 + 0.001 * 6.9214043617248535
Epoch 710, val loss: 0.9484648704528809
Epoch 720, training loss: 0.013251555152237415 = 0.006301576271653175 + 0.001 * 6.949978351593018
Epoch 720, val loss: 0.9537426829338074
Epoch 730, training loss: 0.012970097362995148 = 0.006052644923329353 + 0.001 * 6.917452335357666
Epoch 730, val loss: 0.9589114785194397
Epoch 740, training loss: 0.012731576338410378 = 0.005819422192871571 + 0.001 * 6.912154197692871
Epoch 740, val loss: 0.9639760255813599
Epoch 750, training loss: 0.012540791183710098 = 0.005600585602223873 + 0.001 * 6.940204620361328
Epoch 750, val loss: 0.9689231514930725
Epoch 760, training loss: 0.012307455763220787 = 0.005395002197474241 + 0.001 * 6.9124531745910645
Epoch 760, val loss: 0.9737692475318909
Epoch 770, training loss: 0.012113292701542377 = 0.005201613064855337 + 0.001 * 6.911679267883301
Epoch 770, val loss: 0.9785251021385193
Epoch 780, training loss: 0.011922210454940796 = 0.005019465461373329 + 0.001 * 6.902745246887207
Epoch 780, val loss: 0.9831699132919312
Epoch 790, training loss: 0.011749380268156528 = 0.004847658798098564 + 0.001 * 6.901721000671387
Epoch 790, val loss: 0.9877212047576904
Epoch 800, training loss: 0.011591700837016106 = 0.004685489926487207 + 0.001 * 6.906209945678711
Epoch 800, val loss: 0.992184579372406
Epoch 810, training loss: 0.01143028773367405 = 0.004532197490334511 + 0.001 * 6.898089408874512
Epoch 810, val loss: 0.9965478777885437
Epoch 820, training loss: 0.011301937513053417 = 0.004387133754789829 + 0.001 * 6.914803504943848
Epoch 820, val loss: 1.0008326768875122
Epoch 830, training loss: 0.011158502660691738 = 0.004249760881066322 + 0.001 * 6.908741474151611
Epoch 830, val loss: 1.0050314664840698
Epoch 840, training loss: 0.011037440970540047 = 0.004119479563087225 + 0.001 * 6.917961120605469
Epoch 840, val loss: 1.0091429948806763
Epoch 850, training loss: 0.010895431973040104 = 0.003995850682258606 + 0.001 * 6.899580955505371
Epoch 850, val loss: 1.0131621360778809
Epoch 860, training loss: 0.010768106207251549 = 0.003878440707921982 + 0.001 * 6.889664649963379
Epoch 860, val loss: 1.0171082019805908
Epoch 870, training loss: 0.010662706568837166 = 0.003766815410926938 + 0.001 * 6.895890712738037
Epoch 870, val loss: 1.020990252494812
Epoch 880, training loss: 0.010546817444264889 = 0.003660663031041622 + 0.001 * 6.8861541748046875
Epoch 880, val loss: 1.0247855186462402
Epoch 890, training loss: 0.010506225749850273 = 0.003559564705938101 + 0.001 * 6.946660995483398
Epoch 890, val loss: 1.0285313129425049
Epoch 900, training loss: 0.010338827967643738 = 0.003463208209723234 + 0.001 * 6.875618934631348
Epoch 900, val loss: 1.0321935415267944
Epoch 910, training loss: 0.010266872122883797 = 0.0033713444136083126 + 0.001 * 6.8955278396606445
Epoch 910, val loss: 1.0357916355133057
Epoch 920, training loss: 0.010180806741118431 = 0.003283668542280793 + 0.001 * 6.897137641906738
Epoch 920, val loss: 1.039322018623352
Epoch 930, training loss: 0.010093405842781067 = 0.003199927508831024 + 0.001 * 6.893477439880371
Epoch 930, val loss: 1.0427823066711426
Epoch 940, training loss: 0.010000353679060936 = 0.0031198987271636724 + 0.001 * 6.8804545402526855
Epoch 940, val loss: 1.046186089515686
Epoch 950, training loss: 0.009913556277751923 = 0.00304337777197361 + 0.001 * 6.870178699493408
Epoch 950, val loss: 1.0495222806930542
Epoch 960, training loss: 0.00983944907784462 = 0.0029701374005526304 + 0.001 * 6.869311332702637
Epoch 960, val loss: 1.052786946296692
Epoch 970, training loss: 0.00976928137242794 = 0.0029000174254179 + 0.001 * 6.869263172149658
Epoch 970, val loss: 1.0560139417648315
Epoch 980, training loss: 0.009712238796055317 = 0.002832829486578703 + 0.001 * 6.879408836364746
Epoch 980, val loss: 1.059179663658142
Epoch 990, training loss: 0.009636150673031807 = 0.002768409438431263 + 0.001 * 6.867740631103516
Epoch 990, val loss: 1.062298059463501
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.4945
Flip ASR: 0.4089/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9336656332015991 = 1.925291657447815 + 0.001 * 8.37393856048584
Epoch 0, val loss: 1.9173728227615356
Epoch 10, training loss: 1.9242628812789917 = 1.915889024734497 + 0.001 * 8.373889923095703
Epoch 10, val loss: 1.90847647190094
Epoch 20, training loss: 1.9126368761062622 = 1.9042631387710571 + 0.001 * 8.373750686645508
Epoch 20, val loss: 1.8973113298416138
Epoch 30, training loss: 1.8963066339492798 = 1.8879331350326538 + 0.001 * 8.373449325561523
Epoch 30, val loss: 1.8814886808395386
Epoch 40, training loss: 1.8721424341201782 = 1.863769769668579 + 0.001 * 8.372722625732422
Epoch 40, val loss: 1.8583664894104004
Epoch 50, training loss: 1.8385257720947266 = 1.8301552534103394 + 0.001 * 8.370521545410156
Epoch 50, val loss: 1.8278149366378784
Epoch 60, training loss: 1.7995238304138184 = 1.7911620140075684 + 0.001 * 8.36184024810791
Epoch 60, val loss: 1.7957333326339722
Epoch 70, training loss: 1.7574323415756226 = 1.7491146326065063 + 0.001 * 8.317707061767578
Epoch 70, val loss: 1.7611867189407349
Epoch 80, training loss: 1.6980228424072266 = 1.6899828910827637 + 0.001 * 8.039990425109863
Epoch 80, val loss: 1.709442377090454
Epoch 90, training loss: 1.6193270683288574 = 1.6114768981933594 + 0.001 * 7.850163459777832
Epoch 90, val loss: 1.6434675455093384
Epoch 100, training loss: 1.5289851427078247 = 1.5211586952209473 + 0.001 * 7.826414108276367
Epoch 100, val loss: 1.5707546472549438
Epoch 110, training loss: 1.4411640167236328 = 1.433363676071167 + 0.001 * 7.800357341766357
Epoch 110, val loss: 1.5036040544509888
Epoch 120, training loss: 1.3580925464630127 = 1.3503518104553223 + 0.001 * 7.7407026290893555
Epoch 120, val loss: 1.4438999891281128
Epoch 130, training loss: 1.2769643068313599 = 1.2693921327590942 + 0.001 * 7.572214126586914
Epoch 130, val loss: 1.3891328573226929
Epoch 140, training loss: 1.196094036102295 = 1.1886487007141113 + 0.001 * 7.445382118225098
Epoch 140, val loss: 1.3356050252914429
Epoch 150, training loss: 1.1161038875579834 = 1.1086801290512085 + 0.001 * 7.423790454864502
Epoch 150, val loss: 1.2835109233856201
Epoch 160, training loss: 1.0379321575164795 = 1.0305453538894653 + 0.001 * 7.386833190917969
Epoch 160, val loss: 1.2323466539382935
Epoch 170, training loss: 0.9615583419799805 = 0.9542107582092285 + 0.001 * 7.34759521484375
Epoch 170, val loss: 1.1813386678695679
Epoch 180, training loss: 0.8861904144287109 = 0.8788948059082031 + 0.001 * 7.295612335205078
Epoch 180, val loss: 1.1303797960281372
Epoch 190, training loss: 0.811222493648529 = 0.8039983510971069 + 0.001 * 7.22415018081665
Epoch 190, val loss: 1.0787385702133179
Epoch 200, training loss: 0.7376302480697632 = 0.7304521799087524 + 0.001 * 7.178071975708008
Epoch 200, val loss: 1.0278198719024658
Epoch 210, training loss: 0.6674825549125671 = 0.6603267788887024 + 0.001 * 7.155749320983887
Epoch 210, val loss: 0.9796879291534424
Epoch 220, training loss: 0.602043092250824 = 0.5949147939682007 + 0.001 * 7.128285884857178
Epoch 220, val loss: 0.936248242855072
Epoch 230, training loss: 0.5413036942481995 = 0.534209668636322 + 0.001 * 7.094027042388916
Epoch 230, val loss: 0.8991518616676331
Epoch 240, training loss: 0.4852534532546997 = 0.4781840741634369 + 0.001 * 7.06937837600708
Epoch 240, val loss: 0.8693088889122009
Epoch 250, training loss: 0.4346599280834198 = 0.42760100960731506 + 0.001 * 7.058914661407471
Epoch 250, val loss: 0.8476272821426392
Epoch 260, training loss: 0.3898255527019501 = 0.3827667534351349 + 0.001 * 7.058800220489502
Epoch 260, val loss: 0.8333747982978821
Epoch 270, training loss: 0.35018080472946167 = 0.34312185645103455 + 0.001 * 7.058935642242432
Epoch 270, val loss: 0.8254222869873047
Epoch 280, training loss: 0.31475400924682617 = 0.3076951503753662 + 0.001 * 7.058844566345215
Epoch 280, val loss: 0.8222324848175049
Epoch 290, training loss: 0.28258585929870605 = 0.27552688121795654 + 0.001 * 7.058975696563721
Epoch 290, val loss: 0.8225635290145874
Epoch 300, training loss: 0.2530295252799988 = 0.245970219373703 + 0.001 * 7.059306621551514
Epoch 300, val loss: 0.8254280090332031
Epoch 310, training loss: 0.22568491101264954 = 0.21862515807151794 + 0.001 * 7.059752464294434
Epoch 310, val loss: 0.8301717042922974
Epoch 320, training loss: 0.2003871649503708 = 0.19332686066627502 + 0.001 * 7.060309410095215
Epoch 320, val loss: 0.8361014127731323
Epoch 330, training loss: 0.17716483771800995 = 0.17010387778282166 + 0.001 * 7.060962200164795
Epoch 330, val loss: 0.8430699706077576
Epoch 340, training loss: 0.1561831533908844 = 0.14912103116512299 + 0.001 * 7.0621209144592285
Epoch 340, val loss: 0.8507508635520935
Epoch 350, training loss: 0.13758023083209991 = 0.1305166482925415 + 0.001 * 7.063586711883545
Epoch 350, val loss: 0.8589726090431213
Epoch 360, training loss: 0.12136907130479813 = 0.11430521309375763 + 0.001 * 7.063859939575195
Epoch 360, val loss: 0.8678397536277771
Epoch 370, training loss: 0.10741452127695084 = 0.10034992545843124 + 0.001 * 7.0645952224731445
Epoch 370, val loss: 0.8772039413452148
Epoch 380, training loss: 0.09548211097717285 = 0.08841699361801147 + 0.001 * 7.065115928649902
Epoch 380, val loss: 0.8871390223503113
Epoch 390, training loss: 0.08530303090810776 = 0.07823335379362106 + 0.001 * 7.069675922393799
Epoch 390, val loss: 0.8976908922195435
Epoch 400, training loss: 0.07658974081277847 = 0.06952261179685593 + 0.001 * 7.067131042480469
Epoch 400, val loss: 0.9084639549255371
Epoch 410, training loss: 0.0691082626581192 = 0.062042299658060074 + 0.001 * 7.065964221954346
Epoch 410, val loss: 0.9195387959480286
Epoch 420, training loss: 0.0626319870352745 = 0.0555659681558609 + 0.001 * 7.066019535064697
Epoch 420, val loss: 0.9305093884468079
Epoch 430, training loss: 0.05698495730757713 = 0.04991913586854935 + 0.001 * 7.065822601318359
Epoch 430, val loss: 0.9415399432182312
Epoch 440, training loss: 0.052037205547094345 = 0.04496309533715248 + 0.001 * 7.074108600616455
Epoch 440, val loss: 0.9525758028030396
Epoch 450, training loss: 0.04765624180436134 = 0.040592119097709656 + 0.001 * 7.064121246337891
Epoch 450, val loss: 0.9635223150253296
Epoch 460, training loss: 0.04378977045416832 = 0.03672562167048454 + 0.001 * 7.064149379730225
Epoch 460, val loss: 0.9743194580078125
Epoch 470, training loss: 0.04036783054471016 = 0.03330569341778755 + 0.001 * 7.062137126922607
Epoch 470, val loss: 0.9850378036499023
Epoch 480, training loss: 0.03733884543180466 = 0.030278310179710388 + 0.001 * 7.060535907745361
Epoch 480, val loss: 0.9954193234443665
Epoch 490, training loss: 0.03466345742344856 = 0.027594370767474174 + 0.001 * 7.069087505340576
Epoch 490, val loss: 1.0057059526443481
Epoch 500, training loss: 0.032282840460538864 = 0.02521432377398014 + 0.001 * 7.068516254425049
Epoch 500, val loss: 1.0158010721206665
Epoch 510, training loss: 0.030161790549755096 = 0.0231013260781765 + 0.001 * 7.060464382171631
Epoch 510, val loss: 1.0257140398025513
Epoch 520, training loss: 0.028279945254325867 = 0.021222155541181564 + 0.001 * 7.0577898025512695
Epoch 520, val loss: 1.0354375839233398
Epoch 530, training loss: 0.026601925492286682 = 0.019547801464796066 + 0.001 * 7.054123878479004
Epoch 530, val loss: 1.0449460744857788
Epoch 540, training loss: 0.025103773921728134 = 0.018052633851766586 + 0.001 * 7.051139831542969
Epoch 540, val loss: 1.054215669631958
Epoch 550, training loss: 0.023788193240761757 = 0.01671389304101467 + 0.001 * 7.074299335479736
Epoch 550, val loss: 1.0633124113082886
Epoch 560, training loss: 0.022568684071302414 = 0.015512768179178238 + 0.001 * 7.055915832519531
Epoch 560, val loss: 1.0721126794815063
Epoch 570, training loss: 0.02147609181702137 = 0.014432174153625965 + 0.001 * 7.043917655944824
Epoch 570, val loss: 1.0806809663772583
Epoch 580, training loss: 0.020504401996731758 = 0.013456829823553562 + 0.001 * 7.047571659088135
Epoch 580, val loss: 1.089053988456726
Epoch 590, training loss: 0.019610555842518806 = 0.012575274333357811 + 0.001 * 7.035281658172607
Epoch 590, val loss: 1.0970970392227173
Epoch 600, training loss: 0.01880544237792492 = 0.011776545085012913 + 0.001 * 7.028897285461426
Epoch 600, val loss: 1.1049566268920898
Epoch 610, training loss: 0.01807462051510811 = 0.011051161214709282 + 0.001 * 7.023458957672119
Epoch 610, val loss: 1.112575888633728
Epoch 620, training loss: 0.0174443069845438 = 0.010390947572886944 + 0.001 * 7.053359508514404
Epoch 620, val loss: 1.119972586631775
Epoch 630, training loss: 0.016816671937704086 = 0.009788701310753822 + 0.001 * 7.027971267700195
Epoch 630, val loss: 1.1271870136260986
Epoch 640, training loss: 0.016267074272036552 = 0.009238241240382195 + 0.001 * 7.028832912445068
Epoch 640, val loss: 1.1341755390167236
Epoch 650, training loss: 0.01579061895608902 = 0.00873402040451765 + 0.001 * 7.056599140167236
Epoch 650, val loss: 1.1409574747085571
Epoch 660, training loss: 0.015311533585190773 = 0.008271032012999058 + 0.001 * 7.040500640869141
Epoch 660, val loss: 1.147553563117981
Epoch 670, training loss: 0.014846542850136757 = 0.007845090702176094 + 0.001 * 7.0014519691467285
Epoch 670, val loss: 1.1539828777313232
Epoch 680, training loss: 0.014456423930823803 = 0.007452438119798899 + 0.001 * 7.003985404968262
Epoch 680, val loss: 1.1601723432540894
Epoch 690, training loss: 0.014076882973313332 = 0.007089701946824789 + 0.001 * 6.987181186676025
Epoch 690, val loss: 1.1661871671676636
Epoch 700, training loss: 0.013736824505031109 = 0.006754058878868818 + 0.001 * 6.982765197753906
Epoch 700, val loss: 1.1720049381256104
Epoch 710, training loss: 0.01340443640947342 = 0.006442981772124767 + 0.001 * 6.961453914642334
Epoch 710, val loss: 1.1776783466339111
Epoch 720, training loss: 0.013110369443893433 = 0.006154236383736134 + 0.001 * 6.956132888793945
Epoch 720, val loss: 1.1831979751586914
Epoch 730, training loss: 0.012837820686399937 = 0.005885736551135778 + 0.001 * 6.952083587646484
Epoch 730, val loss: 1.188519835472107
Epoch 740, training loss: 0.012591281905770302 = 0.005635648034512997 + 0.001 * 6.955634117126465
Epoch 740, val loss: 1.1937291622161865
Epoch 750, training loss: 0.012356450781226158 = 0.0054023172706365585 + 0.001 * 6.954132556915283
Epoch 750, val loss: 1.1987539529800415
Epoch 760, training loss: 0.012167401611804962 = 0.005184256937354803 + 0.001 * 6.983144760131836
Epoch 760, val loss: 1.203637957572937
Epoch 770, training loss: 0.011913053691387177 = 0.004980217665433884 + 0.001 * 6.932836055755615
Epoch 770, val loss: 1.2083969116210938
Epoch 780, training loss: 0.01172544527798891 = 0.0047890194691717625 + 0.001 * 6.936425685882568
Epoch 780, val loss: 1.213033676147461
Epoch 790, training loss: 0.011545582674443722 = 0.004609616473317146 + 0.001 * 6.9359660148620605
Epoch 790, val loss: 1.2175348997116089
Epoch 800, training loss: 0.011366890743374825 = 0.0044410559348762035 + 0.001 * 6.9258341789245605
Epoch 800, val loss: 1.2219326496124268
Epoch 810, training loss: 0.011219318956136703 = 0.004282482899725437 + 0.001 * 6.936835289001465
Epoch 810, val loss: 1.2261791229248047
Epoch 820, training loss: 0.011080171912908554 = 0.004133136942982674 + 0.001 * 6.94703483581543
Epoch 820, val loss: 1.2303560972213745
Epoch 830, training loss: 0.010921517387032509 = 0.0039923349395394325 + 0.001 * 6.929182529449463
Epoch 830, val loss: 1.2343950271606445
Epoch 840, training loss: 0.01078583113849163 = 0.0038594405632466078 + 0.001 * 6.926390171051025
Epoch 840, val loss: 1.238336443901062
Epoch 850, training loss: 0.010663118213415146 = 0.0037338649854063988 + 0.001 * 6.929253101348877
Epoch 850, val loss: 1.242128610610962
Epoch 860, training loss: 0.010533476248383522 = 0.003615072462707758 + 0.001 * 6.918403625488281
Epoch 860, val loss: 1.245876431465149
Epoch 870, training loss: 0.010414098389446735 = 0.00350261596031487 + 0.001 * 6.911482334136963
Epoch 870, val loss: 1.2495275735855103
Epoch 880, training loss: 0.010308022610843182 = 0.003396034939214587 + 0.001 * 6.9119873046875
Epoch 880, val loss: 1.2530683279037476
Epoch 890, training loss: 0.010211819782853127 = 0.0032949382439255714 + 0.001 * 6.9168806076049805
Epoch 890, val loss: 1.256523609161377
Epoch 900, training loss: 0.010107353329658508 = 0.003198926104232669 + 0.001 * 6.908426761627197
Epoch 900, val loss: 1.259893536567688
Epoch 910, training loss: 0.010042106732726097 = 0.00310768885537982 + 0.001 * 6.934418201446533
Epoch 910, val loss: 1.2631770372390747
Epoch 920, training loss: 0.009909022599458694 = 0.003020893782377243 + 0.001 * 6.888128280639648
Epoch 920, val loss: 1.2663735151290894
Epoch 930, training loss: 0.009862473234534264 = 0.00293824658729136 + 0.001 * 6.924226760864258
Epoch 930, val loss: 1.269457459449768
Epoch 940, training loss: 0.009763880632817745 = 0.0028595326002687216 + 0.001 * 6.904347896575928
Epoch 940, val loss: 1.272511601448059
Epoch 950, training loss: 0.009660979732871056 = 0.00278447731398046 + 0.001 * 6.876502513885498
Epoch 950, val loss: 1.275475263595581
Epoch 960, training loss: 0.009619693271815777 = 0.002712860470637679 + 0.001 * 6.906832218170166
Epoch 960, val loss: 1.2783197164535522
Epoch 970, training loss: 0.009524280205368996 = 0.0026444685645401478 + 0.001 * 6.879811763763428
Epoch 970, val loss: 1.2811763286590576
Epoch 980, training loss: 0.009480394423007965 = 0.0025791102088987827 + 0.001 * 6.9012837409973145
Epoch 980, val loss: 1.2838928699493408
Epoch 990, training loss: 0.00939308013767004 = 0.0025166061241179705 + 0.001 * 6.876473426818848
Epoch 990, val loss: 1.2865753173828125
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7454
Flip ASR: 0.7022/225 nodes
The final ASR:0.60517, 0.10454, Accuracy:0.81111, 0.01386
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11666])
remove edge: torch.Size([2, 9440])
updated graph: torch.Size([2, 10550])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98155, 0.00301, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9652899503707886 = 1.956916093826294 + 0.001 * 8.373846054077148
Epoch 0, val loss: 1.9574123620986938
Epoch 10, training loss: 1.9538789987564087 = 1.9455052614212036 + 0.001 * 8.373738288879395
Epoch 10, val loss: 1.9462991952896118
Epoch 20, training loss: 1.9399985074996948 = 1.9316250085830688 + 0.001 * 8.373456954956055
Epoch 20, val loss: 1.9323432445526123
Epoch 30, training loss: 1.921122670173645 = 1.9127498865127563 + 0.001 * 8.372820854187012
Epoch 30, val loss: 1.9130438566207886
Epoch 40, training loss: 1.8942158222198486 = 1.8858445882797241 + 0.001 * 8.371214866638184
Epoch 40, val loss: 1.8857054710388184
Epoch 50, training loss: 1.857375979423523 = 1.8490101099014282 + 0.001 * 8.365900039672852
Epoch 50, val loss: 1.8499830961227417
Epoch 60, training loss: 1.8147956132888794 = 1.8064559698104858 + 0.001 * 8.339683532714844
Epoch 60, val loss: 1.8128104209899902
Epoch 70, training loss: 1.7760878801345825 = 1.7679524421691895 + 0.001 * 8.13547420501709
Epoch 70, val loss: 1.7831460237503052
Epoch 80, training loss: 1.7320433855056763 = 1.7242952585220337 + 0.001 * 7.748168468475342
Epoch 80, val loss: 1.7464746236801147
Epoch 90, training loss: 1.6700851917266846 = 1.6625694036483765 + 0.001 * 7.515749931335449
Epoch 90, val loss: 1.6937426328659058
Epoch 100, training loss: 1.586338996887207 = 1.579006552696228 + 0.001 * 7.33245849609375
Epoch 100, val loss: 1.6245449781417847
Epoch 110, training loss: 1.484257698059082 = 1.4770270586013794 + 0.001 * 7.230588436126709
Epoch 110, val loss: 1.5410819053649902
Epoch 120, training loss: 1.3768731355667114 = 1.3697082996368408 + 0.001 * 7.1648383140563965
Epoch 120, val loss: 1.4544167518615723
Epoch 130, training loss: 1.2711443901062012 = 1.2640612125396729 + 0.001 * 7.083157539367676
Epoch 130, val loss: 1.3704231977462769
Epoch 140, training loss: 1.168883204460144 = 1.1618633270263672 + 0.001 * 7.019898414611816
Epoch 140, val loss: 1.2907904386520386
Epoch 150, training loss: 1.0719025135040283 = 1.0649245977401733 + 0.001 * 6.9779510498046875
Epoch 150, val loss: 1.2166606187820435
Epoch 160, training loss: 0.9819396734237671 = 0.9749926924705505 + 0.001 * 6.9469895362854
Epoch 160, val loss: 1.1487650871276855
Epoch 170, training loss: 0.8990121483802795 = 0.8920795917510986 + 0.001 * 6.93255090713501
Epoch 170, val loss: 1.086403727531433
Epoch 180, training loss: 0.8224759697914124 = 0.815552830696106 + 0.001 * 6.923146724700928
Epoch 180, val loss: 1.0289238691329956
Epoch 190, training loss: 0.7522143721580505 = 0.7452949285507202 + 0.001 * 6.919419765472412
Epoch 190, val loss: 0.9769555926322937
Epoch 200, training loss: 0.688295841217041 = 0.6813811659812927 + 0.001 * 6.914680480957031
Epoch 200, val loss: 0.931270182132721
Epoch 210, training loss: 0.6300371885299683 = 0.6231272220611572 + 0.001 * 6.9099955558776855
Epoch 210, val loss: 0.8920313715934753
Epoch 220, training loss: 0.5759017467498779 = 0.5689978003501892 + 0.001 * 6.903916358947754
Epoch 220, val loss: 0.8581460118293762
Epoch 230, training loss: 0.5243984460830688 = 0.5175009965896606 + 0.001 * 6.8974385261535645
Epoch 230, val loss: 0.8281166553497314
Epoch 240, training loss: 0.47465062141418457 = 0.46775951981544495 + 0.001 * 6.891097068786621
Epoch 240, val loss: 0.8010773658752441
Epoch 250, training loss: 0.42651253938674927 = 0.41962870955467224 + 0.001 * 6.883842945098877
Epoch 250, val loss: 0.7772082686424255
Epoch 260, training loss: 0.38035523891448975 = 0.3734796643257141 + 0.001 * 6.875579833984375
Epoch 260, val loss: 0.7572547793388367
Epoch 270, training loss: 0.3369368612766266 = 0.3300587832927704 + 0.001 * 6.878077507019043
Epoch 270, val loss: 0.7422130107879639
Epoch 280, training loss: 0.2969666123390198 = 0.29010209441185 + 0.001 * 6.864513397216797
Epoch 280, val loss: 0.7324631810188293
Epoch 290, training loss: 0.2609107494354248 = 0.25405243039131165 + 0.001 * 6.858315944671631
Epoch 290, val loss: 0.7279037237167358
Epoch 300, training loss: 0.22889912128448486 = 0.22203217446804047 + 0.001 * 6.86694860458374
Epoch 300, val loss: 0.7283163070678711
Epoch 310, training loss: 0.20077452063560486 = 0.19391606748104095 + 0.001 * 6.858456611633301
Epoch 310, val loss: 0.7332858443260193
Epoch 320, training loss: 0.1763235330581665 = 0.16947132349014282 + 0.001 * 6.852202892303467
Epoch 320, val loss: 0.7421926259994507
Epoch 330, training loss: 0.15518242120742798 = 0.14833229780197144 + 0.001 * 6.850120544433594
Epoch 330, val loss: 0.754435122013092
Epoch 340, training loss: 0.13691700994968414 = 0.13007108867168427 + 0.001 * 6.845925331115723
Epoch 340, val loss: 0.7694894075393677
Epoch 350, training loss: 0.12112974375486374 = 0.11428692936897278 + 0.001 * 6.84281587600708
Epoch 350, val loss: 0.7868692874908447
Epoch 360, training loss: 0.10745753347873688 = 0.10061527043581009 + 0.001 * 6.842262268066406
Epoch 360, val loss: 0.8059291839599609
Epoch 370, training loss: 0.09559114277362823 = 0.08874921500682831 + 0.001 * 6.841927528381348
Epoch 370, val loss: 0.8262659907341003
Epoch 380, training loss: 0.08527150750160217 = 0.07843345403671265 + 0.001 * 6.838055610656738
Epoch 380, val loss: 0.847420871257782
Epoch 390, training loss: 0.07629729062318802 = 0.06945797801017761 + 0.001 * 6.839311599731445
Epoch 390, val loss: 0.8690181970596313
Epoch 400, training loss: 0.06849240511655807 = 0.0616513192653656 + 0.001 * 6.84108304977417
Epoch 400, val loss: 0.8907586336135864
Epoch 410, training loss: 0.061701249331235886 = 0.05486610159277916 + 0.001 * 6.835146903991699
Epoch 410, val loss: 0.9123856425285339
Epoch 420, training loss: 0.05581633746623993 = 0.048975344747304916 + 0.001 * 6.840994358062744
Epoch 420, val loss: 0.9336165189743042
Epoch 430, training loss: 0.05070064589381218 = 0.04386410862207413 + 0.001 * 6.836535453796387
Epoch 430, val loss: 0.9543383717536926
Epoch 440, training loss: 0.0462605282664299 = 0.039426662027835846 + 0.001 * 6.833865165710449
Epoch 440, val loss: 0.9743980169296265
Epoch 450, training loss: 0.042409807443618774 = 0.03556869179010391 + 0.001 * 6.841115474700928
Epoch 450, val loss: 0.9937147498130798
Epoch 460, training loss: 0.03903576731681824 = 0.032207190990448 + 0.001 * 6.8285746574401855
Epoch 460, val loss: 1.0123181343078613
Epoch 470, training loss: 0.03609819710254669 = 0.029270067811012268 + 0.001 * 6.828129291534424
Epoch 470, val loss: 1.0302021503448486
Epoch 480, training loss: 0.03352176398038864 = 0.026695463806390762 + 0.001 * 6.826297760009766
Epoch 480, val loss: 1.0473994016647339
Epoch 490, training loss: 0.031268250197172165 = 0.02443140745162964 + 0.001 * 6.836841106414795
Epoch 490, val loss: 1.064002275466919
Epoch 500, training loss: 0.029255010187625885 = 0.022433733567595482 + 0.001 * 6.821275234222412
Epoch 500, val loss: 1.0799875259399414
Epoch 510, training loss: 0.027494318783283234 = 0.020664874464273453 + 0.001 * 6.829443454742432
Epoch 510, val loss: 1.095396637916565
Epoch 520, training loss: 0.025915339589118958 = 0.019093206152319908 + 0.001 * 6.822132110595703
Epoch 520, val loss: 1.110298752784729
Epoch 530, training loss: 0.024509379640221596 = 0.01769215427339077 + 0.001 * 6.817224979400635
Epoch 530, val loss: 1.124671459197998
Epoch 540, training loss: 0.023261351510882378 = 0.016439087688922882 + 0.001 * 6.822263240814209
Epoch 540, val loss: 1.1385836601257324
Epoch 550, training loss: 0.022127665579319 = 0.015314748510718346 + 0.001 * 6.812916278839111
Epoch 550, val loss: 1.1520229578018188
Epoch 560, training loss: 0.021122192963957787 = 0.014302671886980534 + 0.001 * 6.819520473480225
Epoch 560, val loss: 1.165012001991272
Epoch 570, training loss: 0.020204609259963036 = 0.0133888628333807 + 0.001 * 6.815746784210205
Epoch 570, val loss: 1.1775990724563599
Epoch 580, training loss: 0.01937054470181465 = 0.012561440467834473 + 0.001 * 6.809104919433594
Epoch 580, val loss: 1.1897926330566406
Epoch 590, training loss: 0.018630053848028183 = 0.01181007456034422 + 0.001 * 6.819978713989258
Epoch 590, val loss: 1.201628565788269
Epoch 600, training loss: 0.017931705340743065 = 0.011126015335321426 + 0.001 * 6.805689334869385
Epoch 600, val loss: 1.2131023406982422
Epoch 610, training loss: 0.017313402146100998 = 0.010501549579203129 + 0.001 * 6.81185245513916
Epoch 610, val loss: 1.2242292165756226
Epoch 620, training loss: 0.01673426479101181 = 0.009930085390806198 + 0.001 * 6.804179668426514
Epoch 620, val loss: 1.235028862953186
Epoch 630, training loss: 0.016207385808229446 = 0.009405933320522308 + 0.001 * 6.80145263671875
Epoch 630, val loss: 1.2455025911331177
Epoch 640, training loss: 0.015723010525107384 = 0.008924066089093685 + 0.001 * 6.798943996429443
Epoch 640, val loss: 1.2556798458099365
Epoch 650, training loss: 0.015304473228752613 = 0.008480117656290531 + 0.001 * 6.824355125427246
Epoch 650, val loss: 1.2655675411224365
Epoch 660, training loss: 0.014863448217511177 = 0.008070271462202072 + 0.001 * 6.793177127838135
Epoch 660, val loss: 1.2751796245574951
Epoch 670, training loss: 0.014505505561828613 = 0.007691176608204842 + 0.001 * 6.814328670501709
Epoch 670, val loss: 1.2845357656478882
Epoch 680, training loss: 0.014133211225271225 = 0.007339812815189362 + 0.001 * 6.793398380279541
Epoch 680, val loss: 1.2936041355133057
Epoch 690, training loss: 0.013800185173749924 = 0.007013546768575907 + 0.001 * 6.786637783050537
Epoch 690, val loss: 1.3024734258651733
Epoch 700, training loss: 0.013503476977348328 = 0.006710049696266651 + 0.001 * 6.793427467346191
Epoch 700, val loss: 1.3110765218734741
Epoch 710, training loss: 0.013209998607635498 = 0.006427336949855089 + 0.001 * 6.7826619148254395
Epoch 710, val loss: 1.3194935321807861
Epoch 720, training loss: 0.012956889346241951 = 0.006163485813885927 + 0.001 * 6.793402671813965
Epoch 720, val loss: 1.327681541442871
Epoch 730, training loss: 0.01271128561347723 = 0.005916886497288942 + 0.001 * 6.794398784637451
Epoch 730, val loss: 1.335655689239502
Epoch 740, training loss: 0.012470286339521408 = 0.005686041433364153 + 0.001 * 6.784244060516357
Epoch 740, val loss: 1.3434337377548218
Epoch 750, training loss: 0.012264786288142204 = 0.005469583440572023 + 0.001 * 6.79520320892334
Epoch 750, val loss: 1.3510345220565796
Epoch 760, training loss: 0.012035715393722057 = 0.0052661774680018425 + 0.001 * 6.769537448883057
Epoch 760, val loss: 1.35847008228302
Epoch 770, training loss: 0.011856290511786938 = 0.005074735265225172 + 0.001 * 6.781554698944092
Epoch 770, val loss: 1.365739107131958
Epoch 780, training loss: 0.011674439534544945 = 0.004894070792943239 + 0.001 * 6.780367851257324
Epoch 780, val loss: 1.3728678226470947
Epoch 790, training loss: 0.011501006782054901 = 0.0047231013886630535 + 0.001 * 6.777904987335205
Epoch 790, val loss: 1.379878044128418
Epoch 800, training loss: 0.011328291147947311 = 0.00456088874489069 + 0.001 * 6.767401695251465
Epoch 800, val loss: 1.3867794275283813
Epoch 810, training loss: 0.01116953231394291 = 0.004406512249261141 + 0.001 * 6.76301908493042
Epoch 810, val loss: 1.3935824632644653
Epoch 820, training loss: 0.011029954999685287 = 0.004259414505213499 + 0.001 * 6.7705397605896
Epoch 820, val loss: 1.4003474712371826
Epoch 830, training loss: 0.010872239246964455 = 0.004118785727769136 + 0.001 * 6.753453254699707
Epoch 830, val loss: 1.4070323705673218
Epoch 840, training loss: 0.010746067389845848 = 0.003984265495091677 + 0.001 * 6.761801719665527
Epoch 840, val loss: 1.4136710166931152
Epoch 850, training loss: 0.01062631607055664 = 0.0038555096834897995 + 0.001 * 6.770806312561035
Epoch 850, val loss: 1.4202475547790527
Epoch 860, training loss: 0.010490288957953453 = 0.00373258488252759 + 0.001 * 6.7577033042907715
Epoch 860, val loss: 1.426711082458496
Epoch 870, training loss: 0.010371718555688858 = 0.0036150747910141945 + 0.001 * 6.756642818450928
Epoch 870, val loss: 1.4330917596817017
Epoch 880, training loss: 0.010260118171572685 = 0.0035028334241360426 + 0.001 * 6.757284164428711
Epoch 880, val loss: 1.439398169517517
Epoch 890, training loss: 0.010151267051696777 = 0.003395598381757736 + 0.001 * 6.7556681632995605
Epoch 890, val loss: 1.4456621408462524
Epoch 900, training loss: 0.010044383816421032 = 0.003293250920251012 + 0.001 * 6.751132965087891
Epoch 900, val loss: 1.4517486095428467
Epoch 910, training loss: 0.009973166510462761 = 0.003195407334715128 + 0.001 * 6.777758598327637
Epoch 910, val loss: 1.4577922821044922
Epoch 920, training loss: 0.009835334494709969 = 0.0031017884612083435 + 0.001 * 6.733545780181885
Epoch 920, val loss: 1.4637558460235596
Epoch 930, training loss: 0.009749051183462143 = 0.0030120944138616323 + 0.001 * 6.7369561195373535
Epoch 930, val loss: 1.4696122407913208
Epoch 940, training loss: 0.009713568724691868 = 0.0029259517323225737 + 0.001 * 6.78761625289917
Epoch 940, val loss: 1.4754692316055298
Epoch 950, training loss: 0.009592048823833466 = 0.0028431236278265715 + 0.001 * 6.748924732208252
Epoch 950, val loss: 1.4812448024749756
Epoch 960, training loss: 0.009493938647210598 = 0.002763332799077034 + 0.001 * 6.730605602264404
Epoch 960, val loss: 1.4870094060897827
Epoch 970, training loss: 0.009433863684535027 = 0.0026864251121878624 + 0.001 * 6.747438430786133
Epoch 970, val loss: 1.4927160739898682
Epoch 980, training loss: 0.009346792474389076 = 0.0026122673880308867 + 0.001 * 6.734525203704834
Epoch 980, val loss: 1.4984230995178223
Epoch 990, training loss: 0.009272327646613121 = 0.002540888264775276 + 0.001 * 6.731438636779785
Epoch 990, val loss: 1.5040689706802368
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6384
Flip ASR: 0.5644/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.965016484260559 = 1.9566426277160645 + 0.001 * 8.373811721801758
Epoch 0, val loss: 1.9493905305862427
Epoch 10, training loss: 1.9553817510604858 = 1.9470080137252808 + 0.001 * 8.373764038085938
Epoch 10, val loss: 1.9393482208251953
Epoch 20, training loss: 1.943724513053894 = 1.935351014137268 + 0.001 * 8.373547554016113
Epoch 20, val loss: 1.9271507263183594
Epoch 30, training loss: 1.9272735118865967 = 1.9189003705978394 + 0.001 * 8.3731107711792
Epoch 30, val loss: 1.909922480583191
Epoch 40, training loss: 1.9023981094360352 = 1.8940259218215942 + 0.001 * 8.372146606445312
Epoch 40, val loss: 1.8840185403823853
Epoch 50, training loss: 1.8653898239135742 = 1.8570204973220825 + 0.001 * 8.36937427520752
Epoch 50, val loss: 1.8464701175689697
Epoch 60, training loss: 1.8174057006835938 = 1.8090476989746094 + 0.001 * 8.357953071594238
Epoch 60, val loss: 1.8009732961654663
Epoch 70, training loss: 1.7681688070297241 = 1.7598822116851807 + 0.001 * 8.286648750305176
Epoch 70, val loss: 1.7588645219802856
Epoch 80, training loss: 1.7146475315093994 = 1.706821322441101 + 0.001 * 7.826198577880859
Epoch 80, val loss: 1.713073968887329
Epoch 90, training loss: 1.64516282081604 = 1.6376055479049683 + 0.001 * 7.5572967529296875
Epoch 90, val loss: 1.651617169380188
Epoch 100, training loss: 1.5549286603927612 = 1.547611117362976 + 0.001 * 7.317592620849609
Epoch 100, val loss: 1.5737264156341553
Epoch 110, training loss: 1.4489482641220093 = 1.4417833089828491 + 0.001 * 7.164990425109863
Epoch 110, val loss: 1.4869896173477173
Epoch 120, training loss: 1.3390194177627563 = 1.3319512605667114 + 0.001 * 7.068098068237305
Epoch 120, val loss: 1.403113603591919
Epoch 130, training loss: 1.2329314947128296 = 1.2259070873260498 + 0.001 * 7.024438381195068
Epoch 130, val loss: 1.3263764381408691
Epoch 140, training loss: 1.1340346336364746 = 1.1270252466201782 + 0.001 * 7.009378910064697
Epoch 140, val loss: 1.2561839818954468
Epoch 150, training loss: 1.0438816547393799 = 1.0368878841400146 + 0.001 * 6.993723392486572
Epoch 150, val loss: 1.192507266998291
Epoch 160, training loss: 0.9625459909439087 = 0.9555678367614746 + 0.001 * 6.9781622886657715
Epoch 160, val loss: 1.1357024908065796
Epoch 170, training loss: 0.8879229426383972 = 0.8809641599655151 + 0.001 * 6.958775997161865
Epoch 170, val loss: 1.0838946104049683
Epoch 180, training loss: 0.816790759563446 = 0.8098495006561279 + 0.001 * 6.941239833831787
Epoch 180, val loss: 1.0349384546279907
Epoch 190, training loss: 0.7469704151153564 = 0.7400408983230591 + 0.001 * 6.929535388946533
Epoch 190, val loss: 0.9868332743644714
Epoch 200, training loss: 0.6781421303749084 = 0.6712183952331543 + 0.001 * 6.9237141609191895
Epoch 200, val loss: 0.9400272369384766
Epoch 210, training loss: 0.6116251349449158 = 0.6047044396400452 + 0.001 * 6.9206671714782715
Epoch 210, val loss: 0.8960204124450684
Epoch 220, training loss: 0.5493084788322449 = 0.542392373085022 + 0.001 * 6.916098594665527
Epoch 220, val loss: 0.8569888472557068
Epoch 230, training loss: 0.492825984954834 = 0.485914021730423 + 0.001 * 6.911974906921387
Epoch 230, val loss: 0.8245090246200562
Epoch 240, training loss: 0.44295111298561096 = 0.4360419809818268 + 0.001 * 6.909143924713135
Epoch 240, val loss: 0.7990948557853699
Epoch 250, training loss: 0.3996928930282593 = 0.39278915524482727 + 0.001 * 6.903731346130371
Epoch 250, val loss: 0.780338704586029
Epoch 260, training loss: 0.36229559779167175 = 0.355397492647171 + 0.001 * 6.898117542266846
Epoch 260, val loss: 0.7672985196113586
Epoch 270, training loss: 0.32967570424079895 = 0.3227742910385132 + 0.001 * 6.901426792144775
Epoch 270, val loss: 0.758862316608429
Epoch 280, training loss: 0.3006910979747772 = 0.2938006520271301 + 0.001 * 6.890454292297363
Epoch 280, val loss: 0.7539929151535034
Epoch 290, training loss: 0.27444061636924744 = 0.2675565183162689 + 0.001 * 6.884093761444092
Epoch 290, val loss: 0.7520232796669006
Epoch 300, training loss: 0.25018465518951416 = 0.24330806732177734 + 0.001 * 6.876601219177246
Epoch 300, val loss: 0.7521359324455261
Epoch 310, training loss: 0.22745275497436523 = 0.2205689400434494 + 0.001 * 6.883821964263916
Epoch 310, val loss: 0.7540214657783508
Epoch 320, training loss: 0.20594453811645508 = 0.19907605648040771 + 0.001 * 6.868484020233154
Epoch 320, val loss: 0.7572950124740601
Epoch 330, training loss: 0.18568803369998932 = 0.1788232922554016 + 0.001 * 6.864735126495361
Epoch 330, val loss: 0.762119472026825
Epoch 340, training loss: 0.16683584451675415 = 0.15996195375919342 + 0.001 * 6.873894691467285
Epoch 340, val loss: 0.7682543992996216
Epoch 350, training loss: 0.14955268800258636 = 0.1426984816789627 + 0.001 * 6.85420560836792
Epoch 350, val loss: 0.7756816744804382
Epoch 360, training loss: 0.13398534059524536 = 0.12713178992271423 + 0.001 * 6.853547096252441
Epoch 360, val loss: 0.7842254638671875
Epoch 370, training loss: 0.120130255818367 = 0.11326806992292404 + 0.001 * 6.862187385559082
Epoch 370, val loss: 0.793899416923523
Epoch 380, training loss: 0.10787470638751984 = 0.10103092342615128 + 0.001 * 6.843780040740967
Epoch 380, val loss: 0.80448317527771
Epoch 390, training loss: 0.09710831940174103 = 0.09026733040809631 + 0.001 * 6.8409881591796875
Epoch 390, val loss: 0.8159482479095459
Epoch 400, training loss: 0.0876237228512764 = 0.0807887390255928 + 0.001 * 6.834984302520752
Epoch 400, val loss: 0.8281195163726807
Epoch 410, training loss: 0.07920674234628677 = 0.07236683368682861 + 0.001 * 6.839906215667725
Epoch 410, val loss: 0.84084552526474
Epoch 420, training loss: 0.07167361676692963 = 0.06483560800552368 + 0.001 * 6.838012218475342
Epoch 420, val loss: 0.8536458611488342
Epoch 430, training loss: 0.0649077370762825 = 0.05807312950491905 + 0.001 * 6.8346052169799805
Epoch 430, val loss: 0.8666509985923767
Epoch 440, training loss: 0.058825284242630005 = 0.051994532346725464 + 0.001 * 6.830750942230225
Epoch 440, val loss: 0.8798227906227112
Epoch 450, training loss: 0.05343569070100784 = 0.04660329222679138 + 0.001 * 6.8323974609375
Epoch 450, val loss: 0.8929859399795532
Epoch 460, training loss: 0.048737868666648865 = 0.04191116988658905 + 0.001 * 6.826697826385498
Epoch 460, val loss: 0.9064000248908997
Epoch 470, training loss: 0.04468381032347679 = 0.037860769778490067 + 0.001 * 6.82304048538208
Epoch 470, val loss: 0.9202656745910645
Epoch 480, training loss: 0.04119928553700447 = 0.03434290364384651 + 0.001 * 6.856382369995117
Epoch 480, val loss: 0.9342434406280518
Epoch 490, training loss: 0.03811398148536682 = 0.031283728778362274 + 0.001 * 6.830253601074219
Epoch 490, val loss: 0.9483574628829956
Epoch 500, training loss: 0.03542304411530495 = 0.028605347499251366 + 0.001 * 6.817695617675781
Epoch 500, val loss: 0.9621655344963074
Epoch 510, training loss: 0.03307003155350685 = 0.026246855035424232 + 0.001 * 6.823174953460693
Epoch 510, val loss: 0.9753957390785217
Epoch 520, training loss: 0.030981875956058502 = 0.024164043366909027 + 0.001 * 6.817831993103027
Epoch 520, val loss: 0.9884206056594849
Epoch 530, training loss: 0.029130112379789352 = 0.02231733314692974 + 0.001 * 6.812779903411865
Epoch 530, val loss: 1.0009357929229736
Epoch 540, training loss: 0.027492418885231018 = 0.02067212201654911 + 0.001 * 6.820296287536621
Epoch 540, val loss: 1.01322603225708
Epoch 550, training loss: 0.026016298681497574 = 0.019202785566449165 + 0.001 * 6.813511848449707
Epoch 550, val loss: 1.0251994132995605
Epoch 560, training loss: 0.0246928408741951 = 0.01788497157394886 + 0.001 * 6.807868003845215
Epoch 560, val loss: 1.0367717742919922
Epoch 570, training loss: 0.02350713685154915 = 0.016697164624929428 + 0.001 * 6.809972763061523
Epoch 570, val loss: 1.0479999780654907
Epoch 580, training loss: 0.02242564968764782 = 0.015621420927345753 + 0.001 * 6.804228782653809
Epoch 580, val loss: 1.0589735507965088
Epoch 590, training loss: 0.02146337926387787 = 0.014643198810517788 + 0.001 * 6.820180892944336
Epoch 590, val loss: 1.069640040397644
Epoch 600, training loss: 0.02055668644607067 = 0.013750230893492699 + 0.001 * 6.806455612182617
Epoch 600, val loss: 1.0800920724868774
Epoch 610, training loss: 0.01972983032464981 = 0.012932204641401768 + 0.001 * 6.797626495361328
Epoch 610, val loss: 1.0902185440063477
Epoch 620, training loss: 0.01898464746773243 = 0.012181480415165424 + 0.001 * 6.80316686630249
Epoch 620, val loss: 1.1002235412597656
Epoch 630, training loss: 0.01829112507402897 = 0.01149190217256546 + 0.001 * 6.799222469329834
Epoch 630, val loss: 1.1099294424057007
Epoch 640, training loss: 0.01765315793454647 = 0.010857445187866688 + 0.001 * 6.795711994171143
Epoch 640, val loss: 1.1194509267807007
Epoch 650, training loss: 0.017067402601242065 = 0.01027227845042944 + 0.001 * 6.795123100280762
Epoch 650, val loss: 1.1286967992782593
Epoch 660, training loss: 0.01652536168694496 = 0.009730096906423569 + 0.001 * 6.795264720916748
Epoch 660, val loss: 1.1377358436584473
Epoch 670, training loss: 0.016023363918066025 = 0.009225314483046532 + 0.001 * 6.7980499267578125
Epoch 670, val loss: 1.1466379165649414
Epoch 680, training loss: 0.015548214316368103 = 0.008754207752645016 + 0.001 * 6.794005870819092
Epoch 680, val loss: 1.1553829908370972
Epoch 690, training loss: 0.015112876892089844 = 0.008314392529428005 + 0.001 * 6.798483371734619
Epoch 690, val loss: 1.1639248132705688
Epoch 700, training loss: 0.01469450630247593 = 0.007904323749244213 + 0.001 * 6.790182590484619
Epoch 700, val loss: 1.1723278760910034
Epoch 710, training loss: 0.014317880384624004 = 0.0075222221203148365 + 0.001 * 6.795658111572266
Epoch 710, val loss: 1.1805342435836792
Epoch 720, training loss: 0.013946527615189552 = 0.007166207768023014 + 0.001 * 6.780320167541504
Epoch 720, val loss: 1.1885699033737183
Epoch 730, training loss: 0.013621206395328045 = 0.006834532134234905 + 0.001 * 6.7866740226745605
Epoch 730, val loss: 1.1964596509933472
Epoch 740, training loss: 0.013312255032360554 = 0.006525686476379633 + 0.001 * 6.7865681648254395
Epoch 740, val loss: 1.2041691541671753
Epoch 750, training loss: 0.013009684160351753 = 0.006237510591745377 + 0.001 * 6.772172927856445
Epoch 750, val loss: 1.2117055654525757
Epoch 760, training loss: 0.012751017697155476 = 0.005968532990664244 + 0.001 * 6.782484531402588
Epoch 760, val loss: 1.219092845916748
Epoch 770, training loss: 0.012486186809837818 = 0.0057173799723386765 + 0.001 * 6.768806457519531
Epoch 770, val loss: 1.2262762784957886
Epoch 780, training loss: 0.012261785566806793 = 0.005482590291649103 + 0.001 * 6.7791948318481445
Epoch 780, val loss: 1.2333130836486816
Epoch 790, training loss: 0.01203419454395771 = 0.005262823309749365 + 0.001 * 6.7713704109191895
Epoch 790, val loss: 1.240210771560669
Epoch 800, training loss: 0.011834998615086079 = 0.005056700669229031 + 0.001 * 6.778297424316406
Epoch 800, val loss: 1.2469197511672974
Epoch 810, training loss: 0.011632267385721207 = 0.004863355774432421 + 0.001 * 6.768911361694336
Epoch 810, val loss: 1.2535271644592285
Epoch 820, training loss: 0.011449353769421577 = 0.004681864287704229 + 0.001 * 6.767488956451416
Epoch 820, val loss: 1.2599561214447021
Epoch 830, training loss: 0.011288627982139587 = 0.004511191044002771 + 0.001 * 6.777436256408691
Epoch 830, val loss: 1.2662217617034912
Epoch 840, training loss: 0.011121677234768867 = 0.004350568167865276 + 0.001 * 6.771108150482178
Epoch 840, val loss: 1.2722960710525513
Epoch 850, training loss: 0.010969918221235275 = 0.004199151881039143 + 0.001 * 6.770766258239746
Epoch 850, val loss: 1.2783315181732178
Epoch 860, training loss: 0.010819077491760254 = 0.004056346137076616 + 0.001 * 6.762730598449707
Epoch 860, val loss: 1.2842228412628174
Epoch 870, training loss: 0.010680710896849632 = 0.003921449184417725 + 0.001 * 6.759261608123779
Epoch 870, val loss: 1.289943814277649
Epoch 880, training loss: 0.010547108948230743 = 0.0037939059548079967 + 0.001 * 6.753202438354492
Epoch 880, val loss: 1.2955858707427979
Epoch 890, training loss: 0.010426511988043785 = 0.003672997932881117 + 0.001 * 6.753513813018799
Epoch 890, val loss: 1.3010793924331665
Epoch 900, training loss: 0.01032582949846983 = 0.003558357013389468 + 0.001 * 6.767471790313721
Epoch 900, val loss: 1.3065472841262817
Epoch 910, training loss: 0.010195559822022915 = 0.003449587384238839 + 0.001 * 6.7459716796875
Epoch 910, val loss: 1.3117823600769043
Epoch 920, training loss: 0.010110029019415379 = 0.003346245037391782 + 0.001 * 6.7637834548950195
Epoch 920, val loss: 1.3169599771499634
Epoch 930, training loss: 0.00998540036380291 = 0.0032479576766490936 + 0.001 * 6.737442970275879
Epoch 930, val loss: 1.322037696838379
Epoch 940, training loss: 0.009924033656716347 = 0.0031544931698590517 + 0.001 * 6.769540309906006
Epoch 940, val loss: 1.3270390033721924
Epoch 950, training loss: 0.009814426302909851 = 0.0030655846931040287 + 0.001 * 6.748840808868408
Epoch 950, val loss: 1.3318595886230469
Epoch 960, training loss: 0.00973205454647541 = 0.0029809768311679363 + 0.001 * 6.751077651977539
Epoch 960, val loss: 1.3366680145263672
Epoch 970, training loss: 0.009657827205955982 = 0.002900335704907775 + 0.001 * 6.757491111755371
Epoch 970, val loss: 1.3413341045379639
Epoch 980, training loss: 0.009552301838994026 = 0.0028234387282282114 + 0.001 * 6.72886323928833
Epoch 980, val loss: 1.3459300994873047
Epoch 990, training loss: 0.009496664628386497 = 0.0027500835712999105 + 0.001 * 6.746581077575684
Epoch 990, val loss: 1.3503895998001099
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7491
Flip ASR: 0.7067/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9596185684204102 = 1.9512447118759155 + 0.001 * 8.3738431930542
Epoch 0, val loss: 1.9547141790390015
Epoch 10, training loss: 1.9490915536880493 = 1.9407178163528442 + 0.001 * 8.373738288879395
Epoch 10, val loss: 1.9434624910354614
Epoch 20, training loss: 1.9361376762390137 = 1.9277642965316772 + 0.001 * 8.37336540222168
Epoch 20, val loss: 1.928564190864563
Epoch 30, training loss: 1.917781114578247 = 1.909408450126648 + 0.001 * 8.3726167678833
Epoch 30, val loss: 1.9067763090133667
Epoch 40, training loss: 1.8908663988113403 = 1.8824952840805054 + 0.001 * 8.371169090270996
Epoch 40, val loss: 1.875320553779602
Epoch 50, training loss: 1.8534221649169922 = 1.8450541496276855 + 0.001 * 8.368002891540527
Epoch 50, val loss: 1.834232211112976
Epoch 60, training loss: 1.8081371784210205 = 1.7997783422470093 + 0.001 * 8.358850479125977
Epoch 60, val loss: 1.7897359132766724
Epoch 70, training loss: 1.7639137506484985 = 1.7555935382843018 + 0.001 * 8.320229530334473
Epoch 70, val loss: 1.752280592918396
Epoch 80, training loss: 1.7179757356643677 = 1.7099196910858154 + 0.001 * 8.056052207946777
Epoch 80, val loss: 1.715669870376587
Epoch 90, training loss: 1.6554646492004395 = 1.6477916240692139 + 0.001 * 7.67303466796875
Epoch 90, val loss: 1.6658027172088623
Epoch 100, training loss: 1.5721889734268188 = 1.5646214485168457 + 0.001 * 7.567555904388428
Epoch 100, val loss: 1.6003693342208862
Epoch 110, training loss: 1.4720799922943115 = 1.464607834815979 + 0.001 * 7.472146987915039
Epoch 110, val loss: 1.5249963998794556
Epoch 120, training loss: 1.3689347505569458 = 1.3616352081298828 + 0.001 * 7.299582481384277
Epoch 120, val loss: 1.451647400856018
Epoch 130, training loss: 1.272632360458374 = 1.265398383140564 + 0.001 * 7.233997344970703
Epoch 130, val loss: 1.386636734008789
Epoch 140, training loss: 1.1845378875732422 = 1.1773368120193481 + 0.001 * 7.201076030731201
Epoch 140, val loss: 1.329124093055725
Epoch 150, training loss: 1.101259708404541 = 1.0940841436386108 + 0.001 * 7.17559814453125
Epoch 150, val loss: 1.2728281021118164
Epoch 160, training loss: 1.0200116634368896 = 1.0128755569458008 + 0.001 * 7.13614559173584
Epoch 160, val loss: 1.2145359516143799
Epoch 170, training loss: 0.9410064816474915 = 0.9339117407798767 + 0.001 * 7.094742774963379
Epoch 170, val loss: 1.1558806896209717
Epoch 180, training loss: 0.8651649355888367 = 0.8580989837646484 + 0.001 * 7.0659284591674805
Epoch 180, val loss: 1.1007916927337646
Epoch 190, training loss: 0.7924538254737854 = 0.7854070663452148 + 0.001 * 7.046774864196777
Epoch 190, val loss: 1.0491631031036377
Epoch 200, training loss: 0.7227798104286194 = 0.7157517075538635 + 0.001 * 7.028102397918701
Epoch 200, val loss: 1.0009524822235107
Epoch 210, training loss: 0.6566625237464905 = 0.6496556997299194 + 0.001 * 7.006811141967773
Epoch 210, val loss: 0.9562546610832214
Epoch 220, training loss: 0.594292938709259 = 0.5873060822486877 + 0.001 * 6.98687219619751
Epoch 220, val loss: 0.915346622467041
Epoch 230, training loss: 0.5350354909896851 = 0.528063178062439 + 0.001 * 6.972294807434082
Epoch 230, val loss: 0.879763126373291
Epoch 240, training loss: 0.4781554043292999 = 0.47118833661079407 + 0.001 * 6.967076301574707
Epoch 240, val loss: 0.8494128584861755
Epoch 250, training loss: 0.42373085021972656 = 0.4167642891407013 + 0.001 * 6.966573715209961
Epoch 250, val loss: 0.8231614232063293
Epoch 260, training loss: 0.372645765542984 = 0.36567968130111694 + 0.001 * 6.966092109680176
Epoch 260, val loss: 0.8001071214675903
Epoch 270, training loss: 0.3259231448173523 = 0.3189559876918793 + 0.001 * 6.967168807983398
Epoch 270, val loss: 0.7799587249755859
Epoch 280, training loss: 0.28429552912712097 = 0.27732735872268677 + 0.001 * 6.968160629272461
Epoch 280, val loss: 0.7627301812171936
Epoch 290, training loss: 0.2482341080904007 = 0.24126490950584412 + 0.001 * 6.969203948974609
Epoch 290, val loss: 0.7492018342018127
Epoch 300, training loss: 0.21761493384838104 = 0.21064460277557373 + 0.001 * 6.970333576202393
Epoch 300, val loss: 0.7396829128265381
Epoch 310, training loss: 0.19184017181396484 = 0.18486864864826202 + 0.001 * 6.971519947052002
Epoch 310, val loss: 0.7340603470802307
Epoch 320, training loss: 0.1701231598854065 = 0.16315044462680817 + 0.001 * 6.972710132598877
Epoch 320, val loss: 0.7317955493927002
Epoch 330, training loss: 0.15165822207927704 = 0.14468440413475037 + 0.001 * 6.973817348480225
Epoch 330, val loss: 0.7321696281433105
Epoch 340, training loss: 0.1357932984828949 = 0.12881851196289062 + 0.001 * 6.974784851074219
Epoch 340, val loss: 0.7348250150680542
Epoch 350, training loss: 0.12202005088329315 = 0.11504453420639038 + 0.001 * 6.975516319274902
Epoch 350, val loss: 0.7391772866249084
Epoch 360, training loss: 0.10998851805925369 = 0.10301259160041809 + 0.001 * 6.975927352905273
Epoch 360, val loss: 0.7448042631149292
Epoch 370, training loss: 0.09936954081058502 = 0.09239359200000763 + 0.001 * 6.97594690322876
Epoch 370, val loss: 0.7513116598129272
Epoch 380, training loss: 0.08996884524822235 = 0.08299288898706436 + 0.001 * 6.97595739364624
Epoch 380, val loss: 0.7584192752838135
Epoch 390, training loss: 0.08157773315906525 = 0.07460136711597443 + 0.001 * 6.9763641357421875
Epoch 390, val loss: 0.7659794688224792
Epoch 400, training loss: 0.07396692782640457 = 0.0669919028878212 + 0.001 * 6.975025653839111
Epoch 400, val loss: 0.773758590221405
Epoch 410, training loss: 0.06692511588335037 = 0.0599513053894043 + 0.001 * 6.973811149597168
Epoch 410, val loss: 0.781557559967041
Epoch 420, training loss: 0.06027649715542793 = 0.05330445617437363 + 0.001 * 6.972039699554443
Epoch 420, val loss: 0.789069652557373
Epoch 430, training loss: 0.0541076585650444 = 0.04713139683008194 + 0.001 * 6.976262092590332
Epoch 430, val loss: 0.7966799139976501
Epoch 440, training loss: 0.04857051745057106 = 0.04159979522228241 + 0.001 * 6.970722675323486
Epoch 440, val loss: 0.8047998547554016
Epoch 450, training loss: 0.043830446898937225 = 0.03686251491308212 + 0.001 * 6.967931270599365
Epoch 450, val loss: 0.8147377371788025
Epoch 460, training loss: 0.03982266038656235 = 0.03285859897732735 + 0.001 * 6.9640607833862305
Epoch 460, val loss: 0.8261059522628784
Epoch 470, training loss: 0.036454156041145325 = 0.02949395403265953 + 0.001 * 6.960201263427734
Epoch 470, val loss: 0.8381737470626831
Epoch 480, training loss: 0.03361939638853073 = 0.026662888005375862 + 0.001 * 6.956506729125977
Epoch 480, val loss: 0.8501667380332947
Epoch 490, training loss: 0.03120269812643528 = 0.02425127848982811 + 0.001 * 6.951419830322266
Epoch 490, val loss: 0.8617951273918152
Epoch 500, training loss: 0.02911566011607647 = 0.022168995812535286 + 0.001 * 6.946664333343506
Epoch 500, val loss: 0.873116672039032
Epoch 510, training loss: 0.02730516716837883 = 0.020353537052869797 + 0.001 * 6.951629638671875
Epoch 510, val loss: 0.8839050531387329
Epoch 520, training loss: 0.025693677365779877 = 0.018758300691843033 + 0.001 * 6.935375213623047
Epoch 520, val loss: 0.8944065570831299
Epoch 530, training loss: 0.024278488010168076 = 0.017346123233437538 + 0.001 * 6.932363986968994
Epoch 530, val loss: 0.9046359062194824
Epoch 540, training loss: 0.02302016131579876 = 0.016089068725705147 + 0.001 * 6.931092739105225
Epoch 540, val loss: 0.9146801829338074
Epoch 550, training loss: 0.02188698947429657 = 0.014965877868235111 + 0.001 * 6.921112060546875
Epoch 550, val loss: 0.9244782328605652
Epoch 560, training loss: 0.02094094268977642 = 0.013958051800727844 + 0.001 * 6.982890605926514
Epoch 560, val loss: 0.9339246153831482
Epoch 570, training loss: 0.019945833832025528 = 0.013051222078502178 + 0.001 * 6.894611358642578
Epoch 570, val loss: 0.9432060122489929
Epoch 580, training loss: 0.019125008955597878 = 0.012231503613293171 + 0.001 * 6.893505573272705
Epoch 580, val loss: 0.9522179365158081
Epoch 590, training loss: 0.018378213047981262 = 0.011488082818686962 + 0.001 * 6.890129089355469
Epoch 590, val loss: 0.9609799981117249
Epoch 600, training loss: 0.01770038530230522 = 0.010812241584062576 + 0.001 * 6.888143062591553
Epoch 600, val loss: 0.9694905877113342
Epoch 610, training loss: 0.0170888751745224 = 0.010196216404438019 + 0.001 * 6.8926591873168945
Epoch 610, val loss: 0.9778280854225159
Epoch 620, training loss: 0.0165226012468338 = 0.009633667767047882 + 0.001 * 6.888934135437012
Epoch 620, val loss: 0.9859294891357422
Epoch 630, training loss: 0.015986599028110504 = 0.009118662215769291 + 0.001 * 6.867936134338379
Epoch 630, val loss: 0.9938215613365173
Epoch 640, training loss: 0.01551453024148941 = 0.008645538240671158 + 0.001 * 6.868991851806641
Epoch 640, val loss: 1.001549482345581
Epoch 650, training loss: 0.015079764649271965 = 0.008207404054701328 + 0.001 * 6.872360706329346
Epoch 650, val loss: 1.0091328620910645
Epoch 660, training loss: 0.014655321836471558 = 0.007801195606589317 + 0.001 * 6.854125499725342
Epoch 660, val loss: 1.0164265632629395
Epoch 670, training loss: 0.014275489374995232 = 0.007427750155329704 + 0.001 * 6.847739219665527
Epoch 670, val loss: 1.0235830545425415
Epoch 680, training loss: 0.013931442983448505 = 0.007082478608936071 + 0.001 * 6.848964214324951
Epoch 680, val loss: 1.0305949449539185
Epoch 690, training loss: 0.013617300428450108 = 0.00676233135163784 + 0.001 * 6.854968547821045
Epoch 690, val loss: 1.0374897718429565
Epoch 700, training loss: 0.013307795859873295 = 0.00646500289440155 + 0.001 * 6.842792510986328
Epoch 700, val loss: 1.0441919565200806
Epoch 710, training loss: 0.013032481074333191 = 0.006188347004354 + 0.001 * 6.8441338539123535
Epoch 710, val loss: 1.0507091283798218
Epoch 720, training loss: 0.012764209881424904 = 0.005930416751652956 + 0.001 * 6.833792686462402
Epoch 720, val loss: 1.0570687055587769
Epoch 730, training loss: 0.012535205110907555 = 0.005689610727131367 + 0.001 * 6.84559440612793
Epoch 730, val loss: 1.063232660293579
Epoch 740, training loss: 0.01229564193636179 = 0.005464452318847179 + 0.001 * 6.831189155578613
Epoch 740, val loss: 1.0692678689956665
Epoch 750, training loss: 0.012086591683328152 = 0.005253547336906195 + 0.001 * 6.833044052124023
Epoch 750, val loss: 1.0751537084579468
Epoch 760, training loss: 0.011883613653481007 = 0.005055753979831934 + 0.001 * 6.827859401702881
Epoch 760, val loss: 1.080905556678772
Epoch 770, training loss: 0.011704480275511742 = 0.004869907628744841 + 0.001 * 6.834571838378906
Epoch 770, val loss: 1.0865298509597778
Epoch 780, training loss: 0.011519763618707657 = 0.004695145413279533 + 0.001 * 6.824618339538574
Epoch 780, val loss: 1.0920342206954956
Epoch 790, training loss: 0.01136147789657116 = 0.004530643578618765 + 0.001 * 6.830833435058594
Epoch 790, val loss: 1.0974165201187134
Epoch 800, training loss: 0.01120113767683506 = 0.004375583957880735 + 0.001 * 6.8255534172058105
Epoch 800, val loss: 1.10267972946167
Epoch 810, training loss: 0.011048593558371067 = 0.004229223355650902 + 0.001 * 6.819369792938232
Epoch 810, val loss: 1.1078389883041382
Epoch 820, training loss: 0.010909821838140488 = 0.004090933594852686 + 0.001 * 6.818887710571289
Epoch 820, val loss: 1.1128751039505005
Epoch 830, training loss: 0.010797107592225075 = 0.00396012281998992 + 0.001 * 6.836984157562256
Epoch 830, val loss: 1.1178221702575684
Epoch 840, training loss: 0.010653306730091572 = 0.003836308605968952 + 0.001 * 6.81699800491333
Epoch 840, val loss: 1.1226493120193481
Epoch 850, training loss: 0.010541655123233795 = 0.0037189642898738384 + 0.001 * 6.822690486907959
Epoch 850, val loss: 1.1273835897445679
Epoch 860, training loss: 0.010418351739645004 = 0.003607626538723707 + 0.001 * 6.810725212097168
Epoch 860, val loss: 1.1320233345031738
Epoch 870, training loss: 0.010306461714208126 = 0.0035018878988921642 + 0.001 * 6.8045735359191895
Epoch 870, val loss: 1.1365715265274048
Epoch 880, training loss: 0.010214868001639843 = 0.003401347668841481 + 0.001 * 6.813520431518555
Epoch 880, val loss: 1.1410226821899414
Epoch 890, training loss: 0.01011874619871378 = 0.0033057068940252066 + 0.001 * 6.8130388259887695
Epoch 890, val loss: 1.1454052925109863
Epoch 900, training loss: 0.010013420134782791 = 0.0032145963050425053 + 0.001 * 6.798823356628418
Epoch 900, val loss: 1.1496995687484741
Epoch 910, training loss: 0.00997941568493843 = 0.003127719508484006 + 0.001 * 6.851696014404297
Epoch 910, val loss: 1.1539092063903809
Epoch 920, training loss: 0.009856563061475754 = 0.003044883254915476 + 0.001 * 6.811679840087891
Epoch 920, val loss: 1.1580429077148438
Epoch 930, training loss: 0.009787646122276783 = 0.0029657885897904634 + 0.001 * 6.821857452392578
Epoch 930, val loss: 1.1621038913726807
Epoch 940, training loss: 0.009681657887995243 = 0.002890288596972823 + 0.001 * 6.7913689613342285
Epoch 940, val loss: 1.1660979986190796
Epoch 950, training loss: 0.0096278116106987 = 0.002818071749061346 + 0.001 * 6.80974006652832
Epoch 950, val loss: 1.1700090169906616
Epoch 960, training loss: 0.009542688727378845 = 0.0027490127831697464 + 0.001 * 6.793675422668457
Epoch 960, val loss: 1.1738675832748413
Epoch 970, training loss: 0.009476459585130215 = 0.002682885155081749 + 0.001 * 6.793574333190918
Epoch 970, val loss: 1.1776349544525146
Epoch 980, training loss: 0.009415892884135246 = 0.0026195587124675512 + 0.001 * 6.796334266662598
Epoch 980, val loss: 1.181361436843872
Epoch 990, training loss: 0.009344897232949734 = 0.0025588558055460453 + 0.001 * 6.786041259765625
Epoch 990, val loss: 1.1850085258483887
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8266
Flip ASR: 0.8000/225 nodes
The final ASR:0.73801, 0.07723, Accuracy:0.80988, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11630])
remove edge: torch.Size([2, 9452])
updated graph: torch.Size([2, 10526])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9572508335113525 = 1.9488768577575684 + 0.001 * 8.373916625976562
Epoch 0, val loss: 1.9525302648544312
Epoch 10, training loss: 1.9478752613067627 = 1.939501404762268 + 0.001 * 8.373902320861816
Epoch 10, val loss: 1.9439982175827026
Epoch 20, training loss: 1.9367130994796753 = 1.9283393621444702 + 0.001 * 8.373783111572266
Epoch 20, val loss: 1.9334388971328735
Epoch 30, training loss: 1.9210896492004395 = 1.9127161502838135 + 0.001 * 8.373499870300293
Epoch 30, val loss: 1.918267846107483
Epoch 40, training loss: 1.8975756168365479 = 1.8892028331756592 + 0.001 * 8.372831344604492
Epoch 40, val loss: 1.8955016136169434
Epoch 50, training loss: 1.8629142045974731 = 1.8545432090759277 + 0.001 * 8.371018409729004
Epoch 50, val loss: 1.8628697395324707
Epoch 60, training loss: 1.818725347518921 = 1.810360312461853 + 0.001 * 8.365073204040527
Epoch 60, val loss: 1.8238006830215454
Epoch 70, training loss: 1.7741150856018066 = 1.7657732963562012 + 0.001 * 8.341761589050293
Epoch 70, val loss: 1.7864502668380737
Epoch 80, training loss: 1.7234541177749634 = 1.7152303457260132 + 0.001 * 8.2238130569458
Epoch 80, val loss: 1.7393112182617188
Epoch 90, training loss: 1.6527607440948486 = 1.6448501348495483 + 0.001 * 7.910566806793213
Epoch 90, val loss: 1.6744989156723022
Epoch 100, training loss: 1.5589441061019897 = 1.5511177778244019 + 0.001 * 7.826361656188965
Epoch 100, val loss: 1.5937050580978394
Epoch 110, training loss: 1.4496490955352783 = 1.4418954849243164 + 0.001 * 7.7536301612854
Epoch 110, val loss: 1.5024141073226929
Epoch 120, training loss: 1.3385083675384521 = 1.33090341091156 + 0.001 * 7.6049628257751465
Epoch 120, val loss: 1.4136314392089844
Epoch 130, training loss: 1.2344279289245605 = 1.2270220518112183 + 0.001 * 7.40587043762207
Epoch 130, val loss: 1.3352291584014893
Epoch 140, training loss: 1.1397194862365723 = 1.132359266281128 + 0.001 * 7.3601861000061035
Epoch 140, val loss: 1.2680002450942993
Epoch 150, training loss: 1.0531213283538818 = 1.045856237411499 + 0.001 * 7.2651166915893555
Epoch 150, val loss: 1.2082265615463257
Epoch 160, training loss: 0.9735946655273438 = 0.9663960933685303 + 0.001 * 7.198544502258301
Epoch 160, val loss: 1.154565453529358
Epoch 170, training loss: 0.9006770849227905 = 0.8934904932975769 + 0.001 * 7.186574459075928
Epoch 170, val loss: 1.1059818267822266
Epoch 180, training loss: 0.8334662914276123 = 0.8262885212898254 + 0.001 * 7.177783012390137
Epoch 180, val loss: 1.0616291761398315
Epoch 190, training loss: 0.7710978388786316 = 0.7639396786689758 + 0.001 * 7.158130645751953
Epoch 190, val loss: 1.0214388370513916
Epoch 200, training loss: 0.7131304740905762 = 0.7060021758079529 + 0.001 * 7.128327369689941
Epoch 200, val loss: 0.9847824573516846
Epoch 210, training loss: 0.6589195132255554 = 0.6518287062644958 + 0.001 * 7.090826034545898
Epoch 210, val loss: 0.9511881470680237
Epoch 220, training loss: 0.6067676544189453 = 0.5997056365013123 + 0.001 * 7.062003135681152
Epoch 220, val loss: 0.9192588925361633
Epoch 230, training loss: 0.5543656945228577 = 0.547308087348938 + 0.001 * 7.057619094848633
Epoch 230, val loss: 0.8875746130943298
Epoch 240, training loss: 0.4999708831310272 = 0.49291595816612244 + 0.001 * 7.054934978485107
Epoch 240, val loss: 0.8552684783935547
Epoch 250, training loss: 0.44358253479003906 = 0.4365273118019104 + 0.001 * 7.055208683013916
Epoch 250, val loss: 0.8226284980773926
Epoch 260, training loss: 0.38702166080474854 = 0.3799669146537781 + 0.001 * 7.054753303527832
Epoch 260, val loss: 0.7909009456634521
Epoch 270, training loss: 0.3333503305912018 = 0.32629621028900146 + 0.001 * 7.0541229248046875
Epoch 270, val loss: 0.7625604271888733
Epoch 280, training loss: 0.28522583842277527 = 0.2781726121902466 + 0.001 * 7.053231716156006
Epoch 280, val loss: 0.7396313548088074
Epoch 290, training loss: 0.24382111430168152 = 0.2367689311504364 + 0.001 * 7.052189350128174
Epoch 290, val loss: 0.7226586937904358
Epoch 300, training loss: 0.2090620994567871 = 0.20201098918914795 + 0.001 * 7.0511088371276855
Epoch 300, val loss: 0.7112764716148376
Epoch 310, training loss: 0.1801740825176239 = 0.1731240451335907 + 0.001 * 7.050034999847412
Epoch 310, val loss: 0.7047189474105835
Epoch 320, training loss: 0.15621855854988098 = 0.1491696536540985 + 0.001 * 7.04890251159668
Epoch 320, val loss: 0.7021632194519043
Epoch 330, training loss: 0.13630901277065277 = 0.12926143407821655 + 0.001 * 7.047582149505615
Epoch 330, val loss: 0.7028160095214844
Epoch 340, training loss: 0.11967836320400238 = 0.11263249069452286 + 0.001 * 7.045868873596191
Epoch 340, val loss: 0.7061195373535156
Epoch 350, training loss: 0.10568612068891525 = 0.09863889962434769 + 0.001 * 7.0472235679626465
Epoch 350, val loss: 0.7115692496299744
Epoch 360, training loss: 0.09381365776062012 = 0.08677142858505249 + 0.001 * 7.042229652404785
Epoch 360, val loss: 0.718776524066925
Epoch 370, training loss: 0.08367080986499786 = 0.07663183659315109 + 0.001 * 7.038972854614258
Epoch 370, val loss: 0.7273930907249451
Epoch 380, training loss: 0.07495223730802536 = 0.06791749596595764 + 0.001 * 7.034741401672363
Epoch 380, val loss: 0.737040102481842
Epoch 390, training loss: 0.06743613630533218 = 0.06039882078766823 + 0.001 * 7.037317276000977
Epoch 390, val loss: 0.7474772334098816
Epoch 400, training loss: 0.060919925570487976 = 0.053895846009254456 + 0.001 * 7.0240797996521
Epoch 400, val loss: 0.7585346698760986
Epoch 410, training loss: 0.05528118833899498 = 0.048263370990753174 + 0.001 * 7.017816543579102
Epoch 410, val loss: 0.7699399590492249
Epoch 420, training loss: 0.05038813129067421 = 0.04337722808122635 + 0.001 * 7.0109028816223145
Epoch 420, val loss: 0.7815344929695129
Epoch 430, training loss: 0.046137526631355286 = 0.03912964463233948 + 0.001 * 7.007883548736572
Epoch 430, val loss: 0.7931667566299438
Epoch 440, training loss: 0.042433008551597595 = 0.0354267880320549 + 0.001 * 7.006219863891602
Epoch 440, val loss: 0.8047344088554382
Epoch 450, training loss: 0.039186783134937286 = 0.03218535706400871 + 0.001 * 7.001426696777344
Epoch 450, val loss: 0.8162056803703308
Epoch 460, training loss: 0.036328528076410294 = 0.02933145873248577 + 0.001 * 6.997068881988525
Epoch 460, val loss: 0.827496349811554
Epoch 470, training loss: 0.033808015286922455 = 0.02680649422109127 + 0.001 * 7.001519203186035
Epoch 470, val loss: 0.8385783433914185
Epoch 480, training loss: 0.031556762754917145 = 0.02456650510430336 + 0.001 * 6.990258693695068
Epoch 480, val loss: 0.8494322896003723
Epoch 490, training loss: 0.02956261672079563 = 0.022575028240680695 + 0.001 * 6.987588405609131
Epoch 490, val loss: 0.8601024150848389
Epoch 500, training loss: 0.027792321518063545 = 0.020800799131393433 + 0.001 * 6.991522312164307
Epoch 500, val loss: 0.8705365061759949
Epoch 510, training loss: 0.026204479858279228 = 0.019216980785131454 + 0.001 * 6.987498760223389
Epoch 510, val loss: 0.8806898593902588
Epoch 520, training loss: 0.024785257875919342 = 0.01779978722333908 + 0.001 * 6.985469818115234
Epoch 520, val loss: 0.8906176090240479
Epoch 530, training loss: 0.023510757833719254 = 0.01652870699763298 + 0.001 * 6.982051372528076
Epoch 530, val loss: 0.900242805480957
Epoch 540, training loss: 0.02236674353480339 = 0.015385921113193035 + 0.001 * 6.980822563171387
Epoch 540, val loss: 0.909635603427887
Epoch 550, training loss: 0.021333662793040276 = 0.014355940744280815 + 0.001 * 6.977721691131592
Epoch 550, val loss: 0.9187860488891602
Epoch 560, training loss: 0.02040417119860649 = 0.013425301760435104 + 0.001 * 6.978869915008545
Epoch 560, val loss: 0.9276831150054932
Epoch 570, training loss: 0.01955634355545044 = 0.012582356110215187 + 0.001 * 6.973987102508545
Epoch 570, val loss: 0.9363577365875244
Epoch 580, training loss: 0.018790297210216522 = 0.011816846206784248 + 0.001 * 6.973451137542725
Epoch 580, val loss: 0.9447792172431946
Epoch 590, training loss: 0.018106916919350624 = 0.01111994031816721 + 0.001 * 6.986976146697998
Epoch 590, val loss: 0.9529765844345093
Epoch 600, training loss: 0.017458947375416756 = 0.010484043508768082 + 0.001 * 6.974903583526611
Epoch 600, val loss: 0.9609619975090027
Epoch 610, training loss: 0.016868827864527702 = 0.009902427904307842 + 0.001 * 6.966399669647217
Epoch 610, val loss: 0.9687241315841675
Epoch 620, training loss: 0.016337845474481583 = 0.009369216859340668 + 0.001 * 6.968628883361816
Epoch 620, val loss: 0.9762652516365051
Epoch 630, training loss: 0.015845932066440582 = 0.008879431523382664 + 0.001 * 6.966501235961914
Epoch 630, val loss: 0.9836326241493225
Epoch 640, training loss: 0.015387680381536484 = 0.008428382687270641 + 0.001 * 6.9592976570129395
Epoch 640, val loss: 0.9907781481742859
Epoch 650, training loss: 0.014973534271121025 = 0.008012201637029648 + 0.001 * 6.961332321166992
Epoch 650, val loss: 0.997756838798523
Epoch 660, training loss: 0.014591568149626255 = 0.007627500221133232 + 0.001 * 6.964067459106445
Epoch 660, val loss: 1.0045263767242432
Epoch 670, training loss: 0.01422780193388462 = 0.007271221838891506 + 0.001 * 6.956579685211182
Epoch 670, val loss: 1.0111366510391235
Epoch 680, training loss: 0.013890951871871948 = 0.006940674968063831 + 0.001 * 6.9502763748168945
Epoch 680, val loss: 1.0175572633743286
Epoch 690, training loss: 0.013609720394015312 = 0.006633402314037085 + 0.001 * 6.976317882537842
Epoch 690, val loss: 1.0238275527954102
Epoch 700, training loss: 0.013300223276019096 = 0.006347361020743847 + 0.001 * 6.952861309051514
Epoch 700, val loss: 1.0299358367919922
Epoch 710, training loss: 0.013021537102758884 = 0.006080664694309235 + 0.001 * 6.9408721923828125
Epoch 710, val loss: 1.0359150171279907
Epoch 720, training loss: 0.01277683675289154 = 0.005831575952470303 + 0.001 * 6.945261001586914
Epoch 720, val loss: 1.0417425632476807
Epoch 730, training loss: 0.012550020590424538 = 0.005598562769591808 + 0.001 * 6.951457500457764
Epoch 730, val loss: 1.047410488128662
Epoch 740, training loss: 0.01232086680829525 = 0.0053803445771336555 + 0.001 * 6.940521717071533
Epoch 740, val loss: 1.0529687404632568
Epoch 750, training loss: 0.01212453842163086 = 0.005175660364329815 + 0.001 * 6.948877811431885
Epoch 750, val loss: 1.0583629608154297
Epoch 760, training loss: 0.0119134820997715 = 0.004983422812074423 + 0.001 * 6.930058479309082
Epoch 760, val loss: 1.0636506080627441
Epoch 770, training loss: 0.011731481179594994 = 0.004802670795470476 + 0.001 * 6.928810119628906
Epoch 770, val loss: 1.0688186883926392
Epoch 780, training loss: 0.011548718437552452 = 0.004632481373846531 + 0.001 * 6.9162373542785645
Epoch 780, val loss: 1.0738658905029297
Epoch 790, training loss: 0.011389927938580513 = 0.004472068976610899 + 0.001 * 6.917858600616455
Epoch 790, val loss: 1.0787935256958008
Epoch 800, training loss: 0.011236336082220078 = 0.0043207150883972645 + 0.001 * 6.915620803833008
Epoch 800, val loss: 1.0836220979690552
Epoch 810, training loss: 0.011085142381489277 = 0.0041777268052101135 + 0.001 * 6.907415390014648
Epoch 810, val loss: 1.0883398056030273
Epoch 820, training loss: 0.010969927534461021 = 0.004042521584779024 + 0.001 * 6.92740535736084
Epoch 820, val loss: 1.0929386615753174
Epoch 830, training loss: 0.010823734104633331 = 0.003914533648639917 + 0.001 * 6.9091997146606445
Epoch 830, val loss: 1.0974502563476562
Epoch 840, training loss: 0.010750031098723412 = 0.0037932689301669598 + 0.001 * 6.956762313842773
Epoch 840, val loss: 1.1018691062927246
Epoch 850, training loss: 0.010574059560894966 = 0.0036782645620405674 + 0.001 * 6.89579439163208
Epoch 850, val loss: 1.1062082052230835
Epoch 860, training loss: 0.010468723252415657 = 0.0035690809600055218 + 0.001 * 6.899642467498779
Epoch 860, val loss: 1.1104166507720947
Epoch 870, training loss: 0.010391952469944954 = 0.00346534326672554 + 0.001 * 6.926609039306641
Epoch 870, val loss: 1.1145511865615845
Epoch 880, training loss: 0.010259419679641724 = 0.0033667003735899925 + 0.001 * 6.892719268798828
Epoch 880, val loss: 1.118617296218872
Epoch 890, training loss: 0.010166840627789497 = 0.0032728328369557858 + 0.001 * 6.894007205963135
Epoch 890, val loss: 1.1225895881652832
Epoch 900, training loss: 0.010097993537783623 = 0.0031833911780267954 + 0.001 * 6.914602279663086
Epoch 900, val loss: 1.12647545337677
Epoch 910, training loss: 0.009990595281124115 = 0.0030981646850705147 + 0.001 * 6.892430305480957
Epoch 910, val loss: 1.1302896738052368
Epoch 920, training loss: 0.009884201921522617 = 0.003016867907717824 + 0.001 * 6.867333889007568
Epoch 920, val loss: 1.134033203125
Epoch 930, training loss: 0.009815621189773083 = 0.0029392673168331385 + 0.001 * 6.8763532638549805
Epoch 930, val loss: 1.137662649154663
Epoch 940, training loss: 0.009743315167725086 = 0.002865147078409791 + 0.001 * 6.878168106079102
Epoch 940, val loss: 1.1412712335586548
Epoch 950, training loss: 0.009658675640821457 = 0.0027942778542637825 + 0.001 * 6.864398002624512
Epoch 950, val loss: 1.1448076963424683
Epoch 960, training loss: 0.009588745422661304 = 0.0027264580130577087 + 0.001 * 6.8622870445251465
Epoch 960, val loss: 1.1482487916946411
Epoch 970, training loss: 0.009535596705973148 = 0.002661588368937373 + 0.001 * 6.8740081787109375
Epoch 970, val loss: 1.1516332626342773
Epoch 980, training loss: 0.009502273052930832 = 0.002599455416202545 + 0.001 * 6.9028167724609375
Epoch 980, val loss: 1.154952883720398
Epoch 990, training loss: 0.009395326487720013 = 0.0025398663710802794 + 0.001 * 6.855460166931152
Epoch 990, val loss: 1.1582225561141968
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.5683
Flip ASR: 0.4844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9704766273498535 = 1.9621027708053589 + 0.001 * 8.373913764953613
Epoch 0, val loss: 1.9578968286514282
Epoch 10, training loss: 1.9600753784179688 = 1.9517015218734741 + 0.001 * 8.373876571655273
Epoch 10, val loss: 1.9479424953460693
Epoch 20, training loss: 1.9477275609970093 = 1.9393538236618042 + 0.001 * 8.373720169067383
Epoch 20, val loss: 1.935755968093872
Epoch 30, training loss: 1.9308515787124634 = 1.922478199005127 + 0.001 * 8.373353958129883
Epoch 30, val loss: 1.918792724609375
Epoch 40, training loss: 1.9061312675476074 = 1.8977587223052979 + 0.001 * 8.372485160827637
Epoch 40, val loss: 1.89404296875
Epoch 50, training loss: 1.8704103231430054 = 1.8620401620864868 + 0.001 * 8.370152473449707
Epoch 50, val loss: 1.8593813180923462
Epoch 60, training loss: 1.8255422115325928 = 1.8171799182891846 + 0.001 * 8.362298965454102
Epoch 60, val loss: 1.8188587427139282
Epoch 70, training loss: 1.7814289331436157 = 1.7731010913848877 + 0.001 * 8.327798843383789
Epoch 70, val loss: 1.7820327281951904
Epoch 80, training loss: 1.7342709302902222 = 1.7261855602264404 + 0.001 * 8.085358619689941
Epoch 80, val loss: 1.7408289909362793
Epoch 90, training loss: 1.6706466674804688 = 1.662744164466858 + 0.001 * 7.902554512023926
Epoch 90, val loss: 1.6852977275848389
Epoch 100, training loss: 1.5863349437713623 = 1.5785566568374634 + 0.001 * 7.778273582458496
Epoch 100, val loss: 1.6149604320526123
Epoch 110, training loss: 1.4862242937088013 = 1.4785709381103516 + 0.001 * 7.653379917144775
Epoch 110, val loss: 1.5332320928573608
Epoch 120, training loss: 1.382639765739441 = 1.3751786947250366 + 0.001 * 7.461031913757324
Epoch 120, val loss: 1.4489935636520386
Epoch 130, training loss: 1.2816518545150757 = 1.274258017539978 + 0.001 * 7.39381742477417
Epoch 130, val loss: 1.3685193061828613
Epoch 140, training loss: 1.184365153312683 = 1.17707097530365 + 0.001 * 7.294180393218994
Epoch 140, val loss: 1.2923333644866943
Epoch 150, training loss: 1.0931472778320312 = 1.0859194993972778 + 0.001 * 7.227753639221191
Epoch 150, val loss: 1.2227613925933838
Epoch 160, training loss: 1.010050654411316 = 1.0028676986694336 + 0.001 * 7.182941436767578
Epoch 160, val loss: 1.1613483428955078
Epoch 170, training loss: 0.9341179132461548 = 0.9269626140594482 + 0.001 * 7.155313491821289
Epoch 170, val loss: 1.1065924167633057
Epoch 180, training loss: 0.8617910742759705 = 0.8546535968780518 + 0.001 * 7.137477397918701
Epoch 180, val loss: 1.0549830198287964
Epoch 190, training loss: 0.7893760204315186 = 0.7822629809379578 + 0.001 * 7.113048553466797
Epoch 190, val loss: 1.0028204917907715
Epoch 200, training loss: 0.7151533961296082 = 0.7080720067024231 + 0.001 * 7.081401348114014
Epoch 200, val loss: 0.9488478899002075
Epoch 210, training loss: 0.6402387619018555 = 0.6331790089607239 + 0.001 * 7.059776306152344
Epoch 210, val loss: 0.8940260410308838
Epoch 220, training loss: 0.5676173567771912 = 0.5605754256248474 + 0.001 * 7.041932582855225
Epoch 220, val loss: 0.8417470455169678
Epoch 230, training loss: 0.5002732276916504 = 0.4932402968406677 + 0.001 * 7.032912254333496
Epoch 230, val loss: 0.7953384518623352
Epoch 240, training loss: 0.43973734974861145 = 0.43270981311798096 + 0.001 * 7.027527332305908
Epoch 240, val loss: 0.7567102909088135
Epoch 250, training loss: 0.3859978914260864 = 0.37897539138793945 + 0.001 * 7.022497177124023
Epoch 250, val loss: 0.7260845303535461
Epoch 260, training loss: 0.33825069665908813 = 0.3312316834926605 + 0.001 * 7.019010066986084
Epoch 260, val loss: 0.7023953199386597
Epoch 270, training loss: 0.29547926783561707 = 0.2884678244590759 + 0.001 * 7.011455535888672
Epoch 270, val loss: 0.6845746040344238
Epoch 280, training loss: 0.25686565041542053 = 0.24985861778259277 + 0.001 * 7.00701904296875
Epoch 280, val loss: 0.6716741919517517
Epoch 290, training loss: 0.22206416726112366 = 0.21506395936012268 + 0.001 * 7.000209331512451
Epoch 290, val loss: 0.663233757019043
Epoch 300, training loss: 0.19118168950080872 = 0.18417198956012726 + 0.001 * 7.009706020355225
Epoch 300, val loss: 0.6591419577598572
Epoch 310, training loss: 0.16440708935260773 = 0.1574084609746933 + 0.001 * 6.998620986938477
Epoch 310, val loss: 0.6590669751167297
Epoch 320, training loss: 0.14175297319889069 = 0.13476695120334625 + 0.001 * 6.986019134521484
Epoch 320, val loss: 0.6625527739524841
Epoch 330, training loss: 0.12288231402635574 = 0.11590098589658737 + 0.001 * 6.981325626373291
Epoch 330, val loss: 0.6691202521324158
Epoch 340, training loss: 0.10722776502370834 = 0.100252166390419 + 0.001 * 6.975598335266113
Epoch 340, val loss: 0.6780179738998413
Epoch 350, training loss: 0.09422391653060913 = 0.08724652230739594 + 0.001 * 6.977395057678223
Epoch 350, val loss: 0.688529372215271
Epoch 360, training loss: 0.08335454016923904 = 0.07638032734394073 + 0.001 * 6.974215507507324
Epoch 360, val loss: 0.7001542448997498
Epoch 370, training loss: 0.0742160975933075 = 0.0672459602355957 + 0.001 * 6.970137119293213
Epoch 370, val loss: 0.7124919891357422
Epoch 380, training loss: 0.06648560613393784 = 0.05951867252588272 + 0.001 * 6.9669365882873535
Epoch 380, val loss: 0.7251185178756714
Epoch 390, training loss: 0.05990161746740341 = 0.05293573439121246 + 0.001 * 6.965882778167725
Epoch 390, val loss: 0.7379375696182251
Epoch 400, training loss: 0.05425697937607765 = 0.04728977754712105 + 0.001 * 6.9672017097473145
Epoch 400, val loss: 0.7508841156959534
Epoch 410, training loss: 0.0493880957365036 = 0.042420752346515656 + 0.001 * 6.967342853546143
Epoch 410, val loss: 0.763780951499939
Epoch 420, training loss: 0.04517333209514618 = 0.03820271044969559 + 0.001 * 6.970620155334473
Epoch 420, val loss: 0.7765295505523682
Epoch 430, training loss: 0.04150150716304779 = 0.034535542130470276 + 0.001 * 6.965966701507568
Epoch 430, val loss: 0.7891795635223389
Epoch 440, training loss: 0.038300126791000366 = 0.03133709356188774 + 0.001 * 6.963034152984619
Epoch 440, val loss: 0.8015643954277039
Epoch 450, training loss: 0.03549448028206825 = 0.028536971658468246 + 0.001 * 6.957509517669678
Epoch 450, val loss: 0.8138152956962585
Epoch 460, training loss: 0.033030834048986435 = 0.02607751451432705 + 0.001 * 6.953320503234863
Epoch 460, val loss: 0.825810968875885
Epoch 470, training loss: 0.030865732580423355 = 0.023910200223326683 + 0.001 * 6.955533027648926
Epoch 470, val loss: 0.8375402092933655
Epoch 480, training loss: 0.02894996479153633 = 0.021993130445480347 + 0.001 * 6.9568328857421875
Epoch 480, val loss: 0.849043607711792
Epoch 490, training loss: 0.02724275551736355 = 0.0202914010733366 + 0.001 * 6.951354026794434
Epoch 490, val loss: 0.8602339029312134
Epoch 500, training loss: 0.02572900801897049 = 0.01877577230334282 + 0.001 * 6.953235626220703
Epoch 500, val loss: 0.8711733818054199
Epoch 510, training loss: 0.024368468672037125 = 0.01742132194340229 + 0.001 * 6.947145462036133
Epoch 510, val loss: 0.8818250894546509
Epoch 520, training loss: 0.023153796792030334 = 0.016206739470362663 + 0.001 * 6.947056293487549
Epoch 520, val loss: 0.8922459483146667
Epoch 530, training loss: 0.022059660404920578 = 0.015114137902855873 + 0.001 * 6.945522308349609
Epoch 530, val loss: 0.902375340461731
Epoch 540, training loss: 0.021075166761875153 = 0.014128334820270538 + 0.001 * 6.946831226348877
Epoch 540, val loss: 0.9122559428215027
Epoch 550, training loss: 0.020178331062197685 = 0.01323630940169096 + 0.001 * 6.942020893096924
Epoch 550, val loss: 0.921858549118042
Epoch 560, training loss: 0.01936575397849083 = 0.012426899746060371 + 0.001 * 6.938853740692139
Epoch 560, val loss: 0.9312343597412109
Epoch 570, training loss: 0.018629195168614388 = 0.011690358631312847 + 0.001 * 6.938836574554443
Epoch 570, val loss: 0.9403930902481079
Epoch 580, training loss: 0.01795756258070469 = 0.011017982847988605 + 0.001 * 6.939579010009766
Epoch 580, val loss: 0.9493113160133362
Epoch 590, training loss: 0.017332106828689575 = 0.01040126383304596 + 0.001 * 6.93084192276001
Epoch 590, val loss: 0.9581040143966675
Epoch 600, training loss: 0.016783658415079117 = 0.009831706993281841 + 0.001 * 6.951951026916504
Epoch 600, val loss: 0.9668146371841431
Epoch 610, training loss: 0.01623515412211418 = 0.009302828460931778 + 0.001 * 6.93232536315918
Epoch 610, val loss: 0.9754840731620789
Epoch 620, training loss: 0.015735028311610222 = 0.008810619823634624 + 0.001 * 6.924407482147217
Epoch 620, val loss: 0.9840822815895081
Epoch 630, training loss: 0.015276513993740082 = 0.008352428674697876 + 0.001 * 6.924084663391113
Epoch 630, val loss: 0.9925984740257263
Epoch 640, training loss: 0.014845620840787888 = 0.007926062680780888 + 0.001 * 6.919558048248291
Epoch 640, val loss: 1.0010132789611816
Epoch 650, training loss: 0.014453962445259094 = 0.007529540918767452 + 0.001 * 6.9244208335876465
Epoch 650, val loss: 1.009299397468567
Epoch 660, training loss: 0.014076609164476395 = 0.00716087082400918 + 0.001 * 6.915737628936768
Epoch 660, val loss: 1.01746666431427
Epoch 670, training loss: 0.013741536997258663 = 0.006817979272454977 + 0.001 * 6.923557281494141
Epoch 670, val loss: 1.025505542755127
Epoch 680, training loss: 0.013407686725258827 = 0.006498955190181732 + 0.001 * 6.908730983734131
Epoch 680, val loss: 1.0333716869354248
Epoch 690, training loss: 0.013113947585225105 = 0.006201865617185831 + 0.001 * 6.912082195281982
Epoch 690, val loss: 1.0410966873168945
Epoch 700, training loss: 0.012831506319344044 = 0.0059249913319945335 + 0.001 * 6.906514644622803
Epoch 700, val loss: 1.0486606359481812
Epoch 710, training loss: 0.012585838325321674 = 0.005666746757924557 + 0.001 * 6.91909122467041
Epoch 710, val loss: 1.056077003479004
Epoch 720, training loss: 0.012335805222392082 = 0.005425641778856516 + 0.001 * 6.910162925720215
Epoch 720, val loss: 1.063356637954712
Epoch 730, training loss: 0.012095645070075989 = 0.0052002426236867905 + 0.001 * 6.895402431488037
Epoch 730, val loss: 1.0704858303070068
Epoch 740, training loss: 0.011882605031132698 = 0.00498931435868144 + 0.001 * 6.893290042877197
Epoch 740, val loss: 1.0774798393249512
Epoch 750, training loss: 0.011698044836521149 = 0.004791662096977234 + 0.001 * 6.9063825607299805
Epoch 750, val loss: 1.0843197107315063
Epoch 760, training loss: 0.011514617130160332 = 0.004606246016919613 + 0.001 * 6.908371448516846
Epoch 760, val loss: 1.091051459312439
Epoch 770, training loss: 0.011324126273393631 = 0.004432121757417917 + 0.001 * 6.892004013061523
Epoch 770, val loss: 1.0976325273513794
Epoch 780, training loss: 0.011166669428348541 = 0.004268379881978035 + 0.001 * 6.898288726806641
Epoch 780, val loss: 1.104101538658142
Epoch 790, training loss: 0.01100762840360403 = 0.004114311654120684 + 0.001 * 6.893316268920898
Epoch 790, val loss: 1.1104533672332764
Epoch 800, training loss: 0.010854743421077728 = 0.003969138488173485 + 0.001 * 6.885604381561279
Epoch 800, val loss: 1.1166760921478271
Epoch 810, training loss: 0.010713275521993637 = 0.003832191228866577 + 0.001 * 6.881084442138672
Epoch 810, val loss: 1.1227847337722778
Epoch 820, training loss: 0.010585597716271877 = 0.0037028745282441378 + 0.001 * 6.882722854614258
Epoch 820, val loss: 1.1287933588027954
Epoch 830, training loss: 0.010457722470164299 = 0.0035806545056402683 + 0.001 * 6.877067565917969
Epoch 830, val loss: 1.1346958875656128
Epoch 840, training loss: 0.01033363863825798 = 0.00346501637250185 + 0.001 * 6.868621349334717
Epoch 840, val loss: 1.1404809951782227
Epoch 850, training loss: 0.010229542851448059 = 0.0033555040135979652 + 0.001 * 6.8740386962890625
Epoch 850, val loss: 1.1461650133132935
Epoch 860, training loss: 0.010112512856721878 = 0.0032517160288989544 + 0.001 * 6.860795974731445
Epoch 860, val loss: 1.151761531829834
Epoch 870, training loss: 0.010013699531555176 = 0.003153239842504263 + 0.001 * 6.860458850860596
Epoch 870, val loss: 1.1572483777999878
Epoch 880, training loss: 0.00992090255022049 = 0.0030597285367548466 + 0.001 * 6.861173152923584
Epoch 880, val loss: 1.1626496315002441
Epoch 890, training loss: 0.009826122783124447 = 0.0029708684887737036 + 0.001 * 6.855254173278809
Epoch 890, val loss: 1.1679742336273193
Epoch 900, training loss: 0.009729092009365559 = 0.0028863914776593447 + 0.001 * 6.842700481414795
Epoch 900, val loss: 1.1731919050216675
Epoch 910, training loss: 0.009658952243626118 = 0.002805971773341298 + 0.001 * 6.852980136871338
Epoch 910, val loss: 1.1783275604248047
Epoch 920, training loss: 0.009601037949323654 = 0.0027293874882161617 + 0.001 * 6.871650695800781
Epoch 920, val loss: 1.1833531856536865
Epoch 930, training loss: 0.009539905935525894 = 0.00265641906298697 + 0.001 * 6.883486270904541
Epoch 930, val loss: 1.1883094310760498
Epoch 940, training loss: 0.009429199621081352 = 0.0025868082884699106 + 0.001 * 6.842391014099121
Epoch 940, val loss: 1.1932097673416138
Epoch 950, training loss: 0.009378654882311821 = 0.002520356560125947 + 0.001 * 6.858297824859619
Epoch 950, val loss: 1.1980321407318115
Epoch 960, training loss: 0.009284869767725468 = 0.0024569130036979914 + 0.001 * 6.827956199645996
Epoch 960, val loss: 1.2027472257614136
Epoch 970, training loss: 0.009238764643669128 = 0.0023962417617440224 + 0.001 * 6.842522621154785
Epoch 970, val loss: 1.2074164152145386
Epoch 980, training loss: 0.009173952043056488 = 0.0023382180370390415 + 0.001 * 6.8357343673706055
Epoch 980, val loss: 1.2119896411895752
Epoch 990, training loss: 0.009117798879742622 = 0.002282707951962948 + 0.001 * 6.8350911140441895
Epoch 990, val loss: 1.216516375541687
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8303
Flip ASR: 0.7956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9408694505691528 = 1.9324955940246582 + 0.001 * 8.37388801574707
Epoch 0, val loss: 1.9280732870101929
Epoch 10, training loss: 1.9304091930389404 = 1.9220353364944458 + 0.001 * 8.373819351196289
Epoch 10, val loss: 1.918223261833191
Epoch 20, training loss: 1.9172474145889282 = 1.9088737964630127 + 0.001 * 8.37362289428711
Epoch 20, val loss: 1.9053869247436523
Epoch 30, training loss: 1.898550271987915 = 1.8901770114898682 + 0.001 * 8.373224258422852
Epoch 30, val loss: 1.887054443359375
Epoch 40, training loss: 1.871438980102539 = 1.8630666732788086 + 0.001 * 8.37236213684082
Epoch 40, val loss: 1.860971450805664
Epoch 50, training loss: 1.8350329399108887 = 1.8266627788543701 + 0.001 * 8.370125770568848
Epoch 50, val loss: 1.8278224468231201
Epoch 60, training loss: 1.7940562963485718 = 1.7856937646865845 + 0.001 * 8.362513542175293
Epoch 60, val loss: 1.7928588390350342
Epoch 70, training loss: 1.7504209280014038 = 1.74209463596344 + 0.001 * 8.326239585876465
Epoch 70, val loss: 1.753979206085205
Epoch 80, training loss: 1.6905224323272705 = 1.6824604272842407 + 0.001 * 8.061957359313965
Epoch 80, val loss: 1.698682188987732
Epoch 90, training loss: 1.6097207069396973 = 1.6018948554992676 + 0.001 * 7.8258256912231445
Epoch 90, val loss: 1.6290979385375977
Epoch 100, training loss: 1.5130525827407837 = 1.5052772760391235 + 0.001 * 7.775330066680908
Epoch 100, val loss: 1.5504424571990967
Epoch 110, training loss: 1.414744257926941 = 1.4070181846618652 + 0.001 * 7.726017951965332
Epoch 110, val loss: 1.4727174043655396
Epoch 120, training loss: 1.3251956701278687 = 1.317565679550171 + 0.001 * 7.630035400390625
Epoch 120, val loss: 1.4063373804092407
Epoch 130, training loss: 1.2469919919967651 = 1.239537000656128 + 0.001 * 7.455048084259033
Epoch 130, val loss: 1.3522567749023438
Epoch 140, training loss: 1.1768208742141724 = 1.1694748401641846 + 0.001 * 7.346020221710205
Epoch 140, val loss: 1.3062474727630615
Epoch 150, training loss: 1.1092056035995483 = 1.1019152402877808 + 0.001 * 7.290310859680176
Epoch 150, val loss: 1.2634409666061401
Epoch 160, training loss: 1.039764642715454 = 1.0325015783309937 + 0.001 * 7.26312255859375
Epoch 160, val loss: 1.2193483114242554
Epoch 170, training loss: 0.9669047594070435 = 0.9596894383430481 + 0.001 * 7.215304374694824
Epoch 170, val loss: 1.1719632148742676
Epoch 180, training loss: 0.8914347887039185 = 0.8842666745185852 + 0.001 * 7.168128967285156
Epoch 180, val loss: 1.1211587190628052
Epoch 190, training loss: 0.8150891065597534 = 0.8079644441604614 + 0.001 * 7.124648094177246
Epoch 190, val loss: 1.068021297454834
Epoch 200, training loss: 0.7395008206367493 = 0.7324160933494568 + 0.001 * 7.084733486175537
Epoch 200, val loss: 1.0137110948562622
Epoch 210, training loss: 0.666297435760498 = 0.6592469811439514 + 0.001 * 7.05047082901001
Epoch 210, val loss: 0.9602506160736084
Epoch 220, training loss: 0.597111701965332 = 0.5900713205337524 + 0.001 * 7.0403618812561035
Epoch 220, val loss: 0.9099060893058777
Epoch 230, training loss: 0.5331882238388062 = 0.5261561274528503 + 0.001 * 7.032125949859619
Epoch 230, val loss: 0.8648667335510254
Epoch 240, training loss: 0.4749853014945984 = 0.46795549988746643 + 0.001 * 7.029796123504639
Epoch 240, val loss: 0.8267619013786316
Epoch 250, training loss: 0.42204296588897705 = 0.4150155782699585 + 0.001 * 7.027395725250244
Epoch 250, val loss: 0.7960063219070435
Epoch 260, training loss: 0.37337812781333923 = 0.36635297536849976 + 0.001 * 7.0251665115356445
Epoch 260, val loss: 0.7720949649810791
Epoch 270, training loss: 0.32797518372535706 = 0.3209522068500519 + 0.001 * 7.02298641204834
Epoch 270, val loss: 0.7539501190185547
Epoch 280, training loss: 0.2857504189014435 = 0.2787293493747711 + 0.001 * 7.0210652351379395
Epoch 280, val loss: 0.7413543462753296
Epoch 290, training loss: 0.24731071293354034 = 0.24029122292995453 + 0.001 * 7.019494533538818
Epoch 290, val loss: 0.7350101470947266
Epoch 300, training loss: 0.21340326964855194 = 0.20638501644134521 + 0.001 * 7.01825475692749
Epoch 300, val loss: 0.7352071404457092
Epoch 310, training loss: 0.18440364301204681 = 0.17737945914268494 + 0.001 * 7.024176597595215
Epoch 310, val loss: 0.7418273091316223
Epoch 320, training loss: 0.1601213663816452 = 0.15310341119766235 + 0.001 * 7.017949104309082
Epoch 320, val loss: 0.7539785504341125
Epoch 330, training loss: 0.1399485319852829 = 0.13293132185935974 + 0.001 * 7.017203330993652
Epoch 330, val loss: 0.7702462077140808
Epoch 340, training loss: 0.12311002612113953 = 0.11609270423650742 + 0.001 * 7.0173234939575195
Epoch 340, val loss: 0.7894473075866699
Epoch 350, training loss: 0.10889601707458496 = 0.10187835246324539 + 0.001 * 7.017666816711426
Epoch 350, val loss: 0.8103964924812317
Epoch 360, training loss: 0.09676678478717804 = 0.08974926173686981 + 0.001 * 7.017526626586914
Epoch 360, val loss: 0.8323104381561279
Epoch 370, training loss: 0.0863514095544815 = 0.0793323889374733 + 0.001 * 7.019020080566406
Epoch 370, val loss: 0.8547301888465881
Epoch 380, training loss: 0.07736939191818237 = 0.07035070657730103 + 0.001 * 7.01868200302124
Epoch 380, val loss: 0.8773224353790283
Epoch 390, training loss: 0.06960216909646988 = 0.06258399784564972 + 0.001 * 7.018170356750488
Epoch 390, val loss: 0.8998275399208069
Epoch 400, training loss: 0.06287551671266556 = 0.05585723742842674 + 0.001 * 7.018281936645508
Epoch 400, val loss: 0.9221910834312439
Epoch 410, training loss: 0.05703866109251976 = 0.05001894384622574 + 0.001 * 7.019715785980225
Epoch 410, val loss: 0.9443790316581726
Epoch 420, training loss: 0.051962193101644516 = 0.04494490474462509 + 0.001 * 7.017287254333496
Epoch 420, val loss: 0.9662846326828003
Epoch 430, training loss: 0.04753890261054039 = 0.04052422195672989 + 0.001 * 7.014679431915283
Epoch 430, val loss: 0.9877734184265137
Epoch 440, training loss: 0.04368049278855324 = 0.03666496276855469 + 0.001 * 7.015528678894043
Epoch 440, val loss: 1.0087335109710693
Epoch 450, training loss: 0.04029899463057518 = 0.03328852728009224 + 0.001 * 7.0104660987854
Epoch 450, val loss: 1.0291707515716553
Epoch 460, training loss: 0.03734537214040756 = 0.030325960367918015 + 0.001 * 7.019412994384766
Epoch 460, val loss: 1.0489920377731323
Epoch 470, training loss: 0.03472931683063507 = 0.027719609439373016 + 0.001 * 7.009706497192383
Epoch 470, val loss: 1.068192958831787
Epoch 480, training loss: 0.03243284672498703 = 0.025420159101486206 + 0.001 * 7.012688159942627
Epoch 480, val loss: 1.0866667032241821
Epoch 490, training loss: 0.03038555197417736 = 0.023384984582662582 + 0.001 * 7.0005669593811035
Epoch 490, val loss: 1.1045215129852295
Epoch 500, training loss: 0.028573235496878624 = 0.021577779203653336 + 0.001 * 6.995455265045166
Epoch 500, val loss: 1.1217288970947266
Epoch 510, training loss: 0.02695734240114689 = 0.01996772363781929 + 0.001 * 6.989618301391602
Epoch 510, val loss: 1.138346791267395
Epoch 520, training loss: 0.025538794696331024 = 0.018529033288359642 + 0.001 * 7.009761810302734
Epoch 520, val loss: 1.15437650680542
Epoch 530, training loss: 0.024225961416959763 = 0.017239339649677277 + 0.001 * 6.986622333526611
Epoch 530, val loss: 1.169857144355774
Epoch 540, training loss: 0.023057978600263596 = 0.016079792752861977 + 0.001 * 6.978184700012207
Epoch 540, val loss: 1.184841275215149
Epoch 550, training loss: 0.02202194184064865 = 0.015034177340567112 + 0.001 * 6.987764835357666
Epoch 550, val loss: 1.199323296546936
Epoch 560, training loss: 0.02105804905295372 = 0.014088553376495838 + 0.001 * 6.9694952964782715
Epoch 560, val loss: 1.2132699489593506
Epoch 570, training loss: 0.02019176445901394 = 0.01323086116462946 + 0.001 * 6.960903167724609
Epoch 570, val loss: 1.226768136024475
Epoch 580, training loss: 0.01940351538360119 = 0.012450599111616611 + 0.001 * 6.952916145324707
Epoch 580, val loss: 1.2397958040237427
Epoch 590, training loss: 0.01869078539311886 = 0.011738890781998634 + 0.001 * 6.9518938064575195
Epoch 590, val loss: 1.252482533454895
Epoch 600, training loss: 0.01806478761136532 = 0.011088134720921516 + 0.001 * 6.9766526222229
Epoch 600, val loss: 1.2647078037261963
Epoch 610, training loss: 0.017450807616114616 = 0.010491727851331234 + 0.001 * 6.959079742431641
Epoch 610, val loss: 1.2765841484069824
Epoch 620, training loss: 0.01689184457063675 = 0.009943819604814053 + 0.001 * 6.948023796081543
Epoch 620, val loss: 1.2881336212158203
Epoch 630, training loss: 0.016368113458156586 = 0.009439286775887012 + 0.001 * 6.928826332092285
Epoch 630, val loss: 1.299306869506836
Epoch 640, training loss: 0.01594424992799759 = 0.008973700925707817 + 0.001 * 6.970549583435059
Epoch 640, val loss: 1.3101763725280762
Epoch 650, training loss: 0.015485385432839394 = 0.008543325588107109 + 0.001 * 6.942059516906738
Epoch 650, val loss: 1.3207085132598877
Epoch 660, training loss: 0.015083800069987774 = 0.008144681341946125 + 0.001 * 6.939118385314941
Epoch 660, val loss: 1.3309259414672852
Epoch 670, training loss: 0.014690517447888851 = 0.007774780038744211 + 0.001 * 6.915737152099609
Epoch 670, val loss: 1.3408929109573364
Epoch 680, training loss: 0.014344239607453346 = 0.007430995348840952 + 0.001 * 6.913243770599365
Epoch 680, val loss: 1.3506214618682861
Epoch 690, training loss: 0.014070926234126091 = 0.007110880687832832 + 0.001 * 6.96004581451416
Epoch 690, val loss: 1.360076904296875
Epoch 700, training loss: 0.013737322762608528 = 0.006812361069023609 + 0.001 * 6.924961566925049
Epoch 700, val loss: 1.3693184852600098
Epoch 710, training loss: 0.013439258560538292 = 0.006533510982990265 + 0.001 * 6.905747413635254
Epoch 710, val loss: 1.3782734870910645
Epoch 720, training loss: 0.013179425150156021 = 0.006272675935178995 + 0.001 * 6.906749248504639
Epoch 720, val loss: 1.3870428800582886
Epoch 730, training loss: 0.012932145968079567 = 0.006028358358889818 + 0.001 * 6.903787136077881
Epoch 730, val loss: 1.3956013917922974
Epoch 740, training loss: 0.012697625905275345 = 0.005799205973744392 + 0.001 * 6.898419380187988
Epoch 740, val loss: 1.4039180278778076
Epoch 750, training loss: 0.012478549033403397 = 0.005583963356912136 + 0.001 * 6.894584655761719
Epoch 750, val loss: 1.412061333656311
Epoch 760, training loss: 0.01228126510977745 = 0.005381531082093716 + 0.001 * 6.899734020233154
Epoch 760, val loss: 1.4200429916381836
Epoch 770, training loss: 0.012082022614777088 = 0.005190919153392315 + 0.001 * 6.891103267669678
Epoch 770, val loss: 1.4277838468551636
Epoch 780, training loss: 0.011968614533543587 = 0.005011290777474642 + 0.001 * 6.957324028015137
Epoch 780, val loss: 1.4353300333023071
Epoch 790, training loss: 0.011743608862161636 = 0.0048418594524264336 + 0.001 * 6.9017486572265625
Epoch 790, val loss: 1.4427604675292969
Epoch 800, training loss: 0.011571258306503296 = 0.0046817827969789505 + 0.001 * 6.8894758224487305
Epoch 800, val loss: 1.4499568939208984
Epoch 810, training loss: 0.011425964534282684 = 0.004530349280685186 + 0.001 * 6.8956146240234375
Epoch 810, val loss: 1.457053780555725
Epoch 820, training loss: 0.011267445981502533 = 0.004386957734823227 + 0.001 * 6.880488395690918
Epoch 820, val loss: 1.4639804363250732
Epoch 830, training loss: 0.011162146925926208 = 0.004251050762832165 + 0.001 * 6.911095142364502
Epoch 830, val loss: 1.4707740545272827
Epoch 840, training loss: 0.011018889024853706 = 0.004122144542634487 + 0.001 * 6.8967437744140625
Epoch 840, val loss: 1.4773764610290527
Epoch 850, training loss: 0.010874906554818153 = 0.003999724984169006 + 0.001 * 6.875181198120117
Epoch 850, val loss: 1.4839019775390625
Epoch 860, training loss: 0.010749710723757744 = 0.003883381374180317 + 0.001 * 6.866329669952393
Epoch 860, val loss: 1.4902632236480713
Epoch 870, training loss: 0.010641580447554588 = 0.0037727162707597017 + 0.001 * 6.868864059448242
Epoch 870, val loss: 1.4964796304702759
Epoch 880, training loss: 0.010544476099312305 = 0.0036673282738775015 + 0.001 * 6.877147674560547
Epoch 880, val loss: 1.5025802850723267
Epoch 890, training loss: 0.010454611852765083 = 0.0035668928176164627 + 0.001 * 6.887718200683594
Epoch 890, val loss: 1.5085393190383911
Epoch 900, training loss: 0.01033216156065464 = 0.0034711218904703856 + 0.001 * 6.861039161682129
Epoch 900, val loss: 1.5144882202148438
Epoch 910, training loss: 0.010242140851914883 = 0.003379754489287734 + 0.001 * 6.8623857498168945
Epoch 910, val loss: 1.5202014446258545
Epoch 920, training loss: 0.010143782943487167 = 0.0032925070263445377 + 0.001 * 6.85127592086792
Epoch 920, val loss: 1.525839924812317
Epoch 930, training loss: 0.010062461718916893 = 0.0032091368921101093 + 0.001 * 6.853324890136719
Epoch 930, val loss: 1.5313825607299805
Epoch 940, training loss: 0.009977384470403194 = 0.0031294215004891157 + 0.001 * 6.847962379455566
Epoch 940, val loss: 1.5367939472198486
Epoch 950, training loss: 0.009895861148834229 = 0.00305305328220129 + 0.001 * 6.842807292938232
Epoch 950, val loss: 1.542155385017395
Epoch 960, training loss: 0.009873541072010994 = 0.002979784971103072 + 0.001 * 6.893755912780762
Epoch 960, val loss: 1.547363519668579
Epoch 970, training loss: 0.009751562029123306 = 0.002909588860347867 + 0.001 * 6.841973304748535
Epoch 970, val loss: 1.5525444746017456
Epoch 980, training loss: 0.009697324596345425 = 0.002842301968485117 + 0.001 * 6.855022430419922
Epoch 980, val loss: 1.5575296878814697
Epoch 990, training loss: 0.009611140936613083 = 0.0027777445502579212 + 0.001 * 6.8333964347839355
Epoch 990, val loss: 1.5625452995300293
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8266
Flip ASR: 0.7911/225 nodes
The final ASR:0.74170, 0.12264, Accuracy:0.81852, 0.01684
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9482])
updated graph: torch.Size([2, 10554])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9557
Flip ASR: 0.9467/225 nodes
The final ASR:0.96802, 0.00920, Accuracy:0.83210, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9722176790237427 = 1.9638439416885376 + 0.001 * 8.373779296875
Epoch 0, val loss: 1.9660723209381104
Epoch 10, training loss: 1.9611603021621704 = 1.9527865648269653 + 0.001 * 8.373689651489258
Epoch 10, val loss: 1.9549187421798706
Epoch 20, training loss: 1.9475401639938354 = 1.939166784286499 + 0.001 * 8.37341594696045
Epoch 20, val loss: 1.9411673545837402
Epoch 30, training loss: 1.928477168083191 = 1.9201042652130127 + 0.001 * 8.37288761138916
Epoch 30, val loss: 1.9220818281173706
Epoch 40, training loss: 1.9003429412841797 = 1.8919711112976074 + 0.001 * 8.37183666229248
Epoch 40, val loss: 1.8943815231323242
Epoch 50, training loss: 1.8604248762130737 = 1.8520556688308716 + 0.001 * 8.369247436523438
Epoch 50, val loss: 1.8568788766860962
Epoch 60, training loss: 1.8139089345932007 = 1.8055483102798462 + 0.001 * 8.360675811767578
Epoch 60, val loss: 1.8179606199264526
Epoch 70, training loss: 1.7734918594360352 = 1.7651726007461548 + 0.001 * 8.319305419921875
Epoch 70, val loss: 1.7880758047103882
Epoch 80, training loss: 1.7275456190109253 = 1.7195169925689697 + 0.001 * 8.028593063354492
Epoch 80, val loss: 1.748722791671753
Epoch 90, training loss: 1.664482593536377 = 1.6567714214324951 + 0.001 * 7.711187839508057
Epoch 90, val loss: 1.6936590671539307
Epoch 100, training loss: 1.5797077417373657 = 1.5722754001617432 + 0.001 * 7.432343482971191
Epoch 100, val loss: 1.623014211654663
Epoch 110, training loss: 1.4761009216308594 = 1.4688060283660889 + 0.001 * 7.294917583465576
Epoch 110, val loss: 1.5390057563781738
Epoch 120, training loss: 1.3649271726608276 = 1.3576401472091675 + 0.001 * 7.287064075469971
Epoch 120, val loss: 1.4493471384048462
Epoch 130, training loss: 1.2518327236175537 = 1.2445671558380127 + 0.001 * 7.265601634979248
Epoch 130, val loss: 1.3580931425094604
Epoch 140, training loss: 1.1394267082214355 = 1.1321775913238525 + 0.001 * 7.249114036560059
Epoch 140, val loss: 1.2672430276870728
Epoch 150, training loss: 1.030867338180542 = 1.0236492156982422 + 0.001 * 7.218164920806885
Epoch 150, val loss: 1.179592490196228
Epoch 160, training loss: 0.9305649399757385 = 0.9234045147895813 + 0.001 * 7.160412788391113
Epoch 160, val loss: 1.0997669696807861
Epoch 170, training loss: 0.8414032459259033 = 0.8343254923820496 + 0.001 * 7.077739715576172
Epoch 170, val loss: 1.0313713550567627
Epoch 180, training loss: 0.7634945511817932 = 0.7564637064933777 + 0.001 * 7.030841827392578
Epoch 180, val loss: 0.9751243591308594
Epoch 190, training loss: 0.6948896646499634 = 0.687893807888031 + 0.001 * 6.995844841003418
Epoch 190, val loss: 0.9291550517082214
Epoch 200, training loss: 0.6337199807167053 = 0.6267602443695068 + 0.001 * 6.959763050079346
Epoch 200, val loss: 0.8917301297187805
Epoch 210, training loss: 0.5792636275291443 = 0.5723421573638916 + 0.001 * 6.921490669250488
Epoch 210, val loss: 0.8616271018981934
Epoch 220, training loss: 0.5311769247055054 = 0.524287223815918 + 0.001 * 6.889688491821289
Epoch 220, val loss: 0.8382118940353394
Epoch 230, training loss: 0.48871171474456787 = 0.4818439483642578 + 0.001 * 6.867773056030273
Epoch 230, val loss: 0.8204517960548401
Epoch 240, training loss: 0.45054227113723755 = 0.4436899423599243 + 0.001 * 6.852330684661865
Epoch 240, val loss: 0.8069169521331787
Epoch 250, training loss: 0.4151838719844818 = 0.4083419144153595 + 0.001 * 6.841968536376953
Epoch 250, val loss: 0.7961451411247253
Epoch 260, training loss: 0.38121017813682556 = 0.3743792176246643 + 0.001 * 6.830965042114258
Epoch 260, val loss: 0.7870346903800964
Epoch 270, training loss: 0.3477095663547516 = 0.34088534116744995 + 0.001 * 6.8242387771606445
Epoch 270, val loss: 0.7788119912147522
Epoch 280, training loss: 0.3141978085041046 = 0.3073820173740387 + 0.001 * 6.815795421600342
Epoch 280, val loss: 0.771367073059082
Epoch 290, training loss: 0.28057730197906494 = 0.27376627922058105 + 0.001 * 6.811031341552734
Epoch 290, val loss: 0.7648569345474243
Epoch 300, training loss: 0.2471139132976532 = 0.24030767381191254 + 0.001 * 6.806240081787109
Epoch 300, val loss: 0.7593697905540466
Epoch 310, training loss: 0.21459665894508362 = 0.20779359340667725 + 0.001 * 6.803061485290527
Epoch 310, val loss: 0.7553882598876953
Epoch 320, training loss: 0.18425703048706055 = 0.17745918035507202 + 0.001 * 6.797844409942627
Epoch 320, val loss: 0.7537619471549988
Epoch 330, training loss: 0.15720847249031067 = 0.1504114866256714 + 0.001 * 6.796989440917969
Epoch 330, val loss: 0.754960298538208
Epoch 340, training loss: 0.1340113878250122 = 0.12721899151802063 + 0.001 * 6.792389869689941
Epoch 340, val loss: 0.7586784362792969
Epoch 350, training loss: 0.1146477535367012 = 0.10785679519176483 + 0.001 * 6.790957927703857
Epoch 350, val loss: 0.7647378444671631
Epoch 360, training loss: 0.09879554063081741 = 0.09200898557901382 + 0.001 * 6.786558151245117
Epoch 360, val loss: 0.772795557975769
Epoch 370, training loss: 0.08594213426113129 = 0.07915206253528595 + 0.001 * 6.790067672729492
Epoch 370, val loss: 0.7823286056518555
Epoch 380, training loss: 0.07544495910406113 = 0.06866136938333511 + 0.001 * 6.783591270446777
Epoch 380, val loss: 0.7930024862289429
Epoch 390, training loss: 0.06679884344339371 = 0.06001439690589905 + 0.001 * 6.784446716308594
Epoch 390, val loss: 0.8044882416725159
Epoch 400, training loss: 0.059599269181489944 = 0.05281836539506912 + 0.001 * 6.780901908874512
Epoch 400, val loss: 0.8164693713188171
Epoch 410, training loss: 0.05355997011065483 = 0.04677979648113251 + 0.001 * 6.780173301696777
Epoch 410, val loss: 0.8287519812583923
Epoch 420, training loss: 0.0484599769115448 = 0.04167511314153671 + 0.001 * 6.78486442565918
Epoch 420, val loss: 0.8412376642227173
Epoch 430, training loss: 0.0441134013235569 = 0.037333257496356964 + 0.001 * 6.780142784118652
Epoch 430, val loss: 0.8537717461585999
Epoch 440, training loss: 0.04039059579372406 = 0.03361812233924866 + 0.001 * 6.772474765777588
Epoch 440, val loss: 0.8662950992584229
Epoch 450, training loss: 0.03719311207532883 = 0.030420470982789993 + 0.001 * 6.772639751434326
Epoch 450, val loss: 0.8787004947662354
Epoch 460, training loss: 0.034432604908943176 = 0.027653692290186882 + 0.001 * 6.778911113739014
Epoch 460, val loss: 0.8909955024719238
Epoch 470, training loss: 0.0320153534412384 = 0.025247465819120407 + 0.001 * 6.767888069152832
Epoch 470, val loss: 0.9030914902687073
Epoch 480, training loss: 0.029911845922470093 = 0.023143872618675232 + 0.001 * 6.767973899841309
Epoch 480, val loss: 0.9149505496025085
Epoch 490, training loss: 0.02806825563311577 = 0.021295392885804176 + 0.001 * 6.772863388061523
Epoch 490, val loss: 0.9265238642692566
Epoch 500, training loss: 0.0264243446290493 = 0.019663168117403984 + 0.001 * 6.761175155639648
Epoch 500, val loss: 0.9378347992897034
Epoch 510, training loss: 0.024975843727588654 = 0.018215415999293327 + 0.001 * 6.760428428649902
Epoch 510, val loss: 0.9488698244094849
Epoch 520, training loss: 0.02370113879442215 = 0.016925550997257233 + 0.001 * 6.775588035583496
Epoch 520, val loss: 0.9596452713012695
Epoch 530, training loss: 0.02252587117254734 = 0.015771664679050446 + 0.001 * 6.75420618057251
Epoch 530, val loss: 0.970143735408783
Epoch 540, training loss: 0.021489975973963737 = 0.014735614880919456 + 0.001 * 6.754360198974609
Epoch 540, val loss: 0.9803768396377563
Epoch 550, training loss: 0.0205514058470726 = 0.013802130706608295 + 0.001 * 6.749275207519531
Epoch 550, val loss: 0.9903443455696106
Epoch 560, training loss: 0.019723696634173393 = 0.012958231382071972 + 0.001 * 6.7654643058776855
Epoch 560, val loss: 1.000048041343689
Epoch 570, training loss: 0.018940826877951622 = 0.01219277922064066 + 0.001 * 6.748047828674316
Epoch 570, val loss: 1.0095254182815552
Epoch 580, training loss: 0.018242038786411285 = 0.011496162042021751 + 0.001 * 6.745875358581543
Epoch 580, val loss: 1.0187406539916992
Epoch 590, training loss: 0.017603900283575058 = 0.010860307142138481 + 0.001 * 6.743591785430908
Epoch 590, val loss: 1.027721881866455
Epoch 600, training loss: 0.017022347077727318 = 0.010278444737195969 + 0.001 * 6.743902683258057
Epoch 600, val loss: 1.036492109298706
Epoch 610, training loss: 0.01648852787911892 = 0.009744602255523205 + 0.001 * 6.743925094604492
Epoch 610, val loss: 1.0450396537780762
Epoch 620, training loss: 0.01599239557981491 = 0.009253586642444134 + 0.001 * 6.738809585571289
Epoch 620, val loss: 1.0533833503723145
Epoch 630, training loss: 0.015538966283202171 = 0.008800892159342766 + 0.001 * 6.73807430267334
Epoch 630, val loss: 1.06151282787323
Epoch 640, training loss: 0.015142274089157581 = 0.0083826445043087 + 0.001 * 6.759629249572754
Epoch 640, val loss: 1.0694512128829956
Epoch 650, training loss: 0.014737344346940517 = 0.00799544807523489 + 0.001 * 6.741896152496338
Epoch 650, val loss: 1.0771762132644653
Epoch 660, training loss: 0.014368907548487186 = 0.007636273745447397 + 0.001 * 6.732633590698242
Epoch 660, val loss: 1.0847265720367432
Epoch 670, training loss: 0.014034559950232506 = 0.007302431855350733 + 0.001 * 6.732127666473389
Epoch 670, val loss: 1.0920902490615845
Epoch 680, training loss: 0.01373165100812912 = 0.006991570815443993 + 0.001 * 6.740079402923584
Epoch 680, val loss: 1.099278211593628
Epoch 690, training loss: 0.01344013586640358 = 0.006701607257127762 + 0.001 * 6.738528251647949
Epoch 690, val loss: 1.106292963027954
Epoch 700, training loss: 0.013165894895792007 = 0.006430745590478182 + 0.001 * 6.735149383544922
Epoch 700, val loss: 1.1131466627120972
Epoch 710, training loss: 0.01291082613170147 = 0.0061773317866027355 + 0.001 * 6.733494281768799
Epoch 710, val loss: 1.1198464632034302
Epoch 720, training loss: 0.012666290625929832 = 0.0059398771263659 + 0.001 * 6.726413726806641
Epoch 720, val loss: 1.1263978481292725
Epoch 730, training loss: 0.012457851320505142 = 0.005717061460018158 + 0.001 * 6.740789413452148
Epoch 730, val loss: 1.1327896118164062
Epoch 740, training loss: 0.01223239116370678 = 0.0055076973512768745 + 0.001 * 6.724693298339844
Epoch 740, val loss: 1.1390621662139893
Epoch 750, training loss: 0.012043433263897896 = 0.005310741253197193 + 0.001 * 6.732691287994385
Epoch 750, val loss: 1.1452062129974365
Epoch 760, training loss: 0.011861743405461311 = 0.005125225521624088 + 0.001 * 6.736517429351807
Epoch 760, val loss: 1.1511995792388916
Epoch 770, training loss: 0.011673897504806519 = 0.004950246773660183 + 0.001 * 6.723649978637695
Epoch 770, val loss: 1.1570794582366943
Epoch 780, training loss: 0.01150854304432869 = 0.004785022232681513 + 0.001 * 6.723520278930664
Epoch 780, val loss: 1.162827491760254
Epoch 790, training loss: 0.01134532131254673 = 0.004628818016499281 + 0.001 * 6.716503620147705
Epoch 790, val loss: 1.1684659719467163
Epoch 800, training loss: 0.011205265298485756 = 0.004480994306504726 + 0.001 * 6.724270820617676
Epoch 800, val loss: 1.1739826202392578
Epoch 810, training loss: 0.011058416217565536 = 0.0043409764766693115 + 0.001 * 6.717439651489258
Epoch 810, val loss: 1.1793893575668335
Epoch 820, training loss: 0.010924412868916988 = 0.00420822249725461 + 0.001 * 6.716189861297607
Epoch 820, val loss: 1.1846836805343628
Epoch 830, training loss: 0.01079858373850584 = 0.004082184284925461 + 0.001 * 6.716399192810059
Epoch 830, val loss: 1.1898821592330933
Epoch 840, training loss: 0.010697532445192337 = 0.003962488379329443 + 0.001 * 6.735043525695801
Epoch 840, val loss: 1.19497811794281
Epoch 850, training loss: 0.010560929775238037 = 0.0038486425764858723 + 0.001 * 6.712286472320557
Epoch 850, val loss: 1.199973225593567
Epoch 860, training loss: 0.01046043448150158 = 0.0037403451278805733 + 0.001 * 6.720088958740234
Epoch 860, val loss: 1.2048749923706055
Epoch 870, training loss: 0.010359744541347027 = 0.0036371483001857996 + 0.001 * 6.722595691680908
Epoch 870, val loss: 1.209688663482666
Epoch 880, training loss: 0.010254167020320892 = 0.0035388204269111156 + 0.001 * 6.715346336364746
Epoch 880, val loss: 1.214402437210083
Epoch 890, training loss: 0.010154098272323608 = 0.0034449994564056396 + 0.001 * 6.709097862243652
Epoch 890, val loss: 1.2190234661102295
Epoch 900, training loss: 0.01006393413990736 = 0.0033554425463080406 + 0.001 * 6.708491325378418
Epoch 900, val loss: 1.223559856414795
Epoch 910, training loss: 0.009996372275054455 = 0.0032699108123779297 + 0.001 * 6.726460933685303
Epoch 910, val loss: 1.228001356124878
Epoch 920, training loss: 0.009897902607917786 = 0.003188104834407568 + 0.001 * 6.7097978591918945
Epoch 920, val loss: 1.2323801517486572
Epoch 930, training loss: 0.00982093345373869 = 0.0031098942272365093 + 0.001 * 6.711039066314697
Epoch 930, val loss: 1.2366647720336914
Epoch 940, training loss: 0.009740342386066914 = 0.003034984227269888 + 0.001 * 6.705358028411865
Epoch 940, val loss: 1.240876317024231
Epoch 950, training loss: 0.00967852771282196 = 0.0029632693622261286 + 0.001 * 6.71525764465332
Epoch 950, val loss: 1.2450196743011475
Epoch 960, training loss: 0.009599633514881134 = 0.0028945188969373703 + 0.001 * 6.705114841461182
Epoch 960, val loss: 1.2490864992141724
Epoch 970, training loss: 0.009525824338197708 = 0.00282859243452549 + 0.001 * 6.697232246398926
Epoch 970, val loss: 1.253084421157837
Epoch 980, training loss: 0.009467805735766888 = 0.0027653079014271498 + 0.001 * 6.702497482299805
Epoch 980, val loss: 1.2570182085037231
Epoch 990, training loss: 0.009405178017914295 = 0.0027045488823205233 + 0.001 * 6.700629234313965
Epoch 990, val loss: 1.2608904838562012
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.5314
Flip ASR: 0.4489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.951809287071228 = 1.9434354305267334 + 0.001 * 8.373822212219238
Epoch 0, val loss: 1.9427064657211304
Epoch 10, training loss: 1.9413788318634033 = 1.9330050945281982 + 0.001 * 8.373741149902344
Epoch 10, val loss: 1.9319223165512085
Epoch 20, training loss: 1.9286340475082397 = 1.9202605485916138 + 0.001 * 8.373528480529785
Epoch 20, val loss: 1.9189037084579468
Epoch 30, training loss: 1.9108389616012573 = 1.9024658203125 + 0.001 * 8.373124122619629
Epoch 30, val loss: 1.9007374048233032
Epoch 40, training loss: 1.8846499919891357 = 1.8762776851654053 + 0.001 * 8.372332572937012
Epoch 40, val loss: 1.8743548393249512
Epoch 50, training loss: 1.8479936122894287 = 1.839623212814331 + 0.001 * 8.37035083770752
Epoch 50, val loss: 1.8388251066207886
Epoch 60, training loss: 1.8041702508926392 = 1.7958072423934937 + 0.001 * 8.36302661895752
Epoch 60, val loss: 1.7985947132110596
Epoch 70, training loss: 1.7595628499984741 = 1.751243233680725 + 0.001 * 8.31957721710205
Epoch 70, val loss: 1.7574365139007568
Epoch 80, training loss: 1.7046489715576172 = 1.6966454982757568 + 0.001 * 8.003503799438477
Epoch 80, val loss: 1.7044904232025146
Epoch 90, training loss: 1.6290473937988281 = 1.6212620735168457 + 0.001 * 7.785356044769287
Epoch 90, val loss: 1.6361255645751953
Epoch 100, training loss: 1.533388376235962 = 1.525658130645752 + 0.001 * 7.7303009033203125
Epoch 100, val loss: 1.556652545928955
Epoch 110, training loss: 1.427268385887146 = 1.4196014404296875 + 0.001 * 7.666957378387451
Epoch 110, val loss: 1.469378113746643
Epoch 120, training loss: 1.3235443830490112 = 1.3160760402679443 + 0.001 * 7.4683027267456055
Epoch 120, val loss: 1.3873398303985596
Epoch 130, training loss: 1.2281221151351929 = 1.220811367034912 + 0.001 * 7.310785293579102
Epoch 130, val loss: 1.3141804933547974
Epoch 140, training loss: 1.1394017934799194 = 1.1321618556976318 + 0.001 * 7.239964485168457
Epoch 140, val loss: 1.2480984926223755
Epoch 150, training loss: 1.0533429384231567 = 1.046168327331543 + 0.001 * 7.174643516540527
Epoch 150, val loss: 1.185103416442871
Epoch 160, training loss: 0.967529296875 = 0.9604429602622986 + 0.001 * 7.086309909820557
Epoch 160, val loss: 1.1222714185714722
Epoch 170, training loss: 0.8832665681838989 = 0.8762595653533936 + 0.001 * 7.007009983062744
Epoch 170, val loss: 1.060555100440979
Epoch 180, training loss: 0.8041242957115173 = 0.7971463799476624 + 0.001 * 6.977904319763184
Epoch 180, val loss: 1.0031144618988037
Epoch 190, training loss: 0.7334097027778625 = 0.726439893245697 + 0.001 * 6.969834804534912
Epoch 190, val loss: 0.9527089595794678
Epoch 200, training loss: 0.6720775365829468 = 0.6651113033294678 + 0.001 * 6.966211318969727
Epoch 200, val loss: 0.9106584191322327
Epoch 210, training loss: 0.6183829307556152 = 0.6114177703857422 + 0.001 * 6.96513557434082
Epoch 210, val loss: 0.8757902979850769
Epoch 220, training loss: 0.5691156387329102 = 0.5621512532234192 + 0.001 * 6.964362144470215
Epoch 220, val loss: 0.8459392786026001
Epoch 230, training loss: 0.5215044021606445 = 0.51454097032547 + 0.001 * 6.9634199142456055
Epoch 230, val loss: 0.818977415561676
Epoch 240, training loss: 0.4741345942020416 = 0.46717211604118347 + 0.001 * 6.962463855743408
Epoch 240, val loss: 0.7933589816093445
Epoch 250, training loss: 0.42690804600715637 = 0.4199471175670624 + 0.001 * 6.960935115814209
Epoch 250, val loss: 0.7691435217857361
Epoch 260, training loss: 0.3808229863643646 = 0.37386342883110046 + 0.001 * 6.9595489501953125
Epoch 260, val loss: 0.747284471988678
Epoch 270, training loss: 0.3374321460723877 = 0.33047422766685486 + 0.001 * 6.957929611206055
Epoch 270, val loss: 0.7287853360176086
Epoch 280, training loss: 0.29789140820503235 = 0.2909350097179413 + 0.001 * 6.95640230178833
Epoch 280, val loss: 0.7142488956451416
Epoch 290, training loss: 0.2627185583114624 = 0.2557642459869385 + 0.001 * 6.954326152801514
Epoch 290, val loss: 0.7043104767799377
Epoch 300, training loss: 0.23190531134605408 = 0.22495360672473907 + 0.001 * 6.951699733734131
Epoch 300, val loss: 0.6990904808044434
Epoch 310, training loss: 0.2051188200712204 = 0.1981707215309143 + 0.001 * 6.948101997375488
Epoch 310, val loss: 0.6982586979866028
Epoch 320, training loss: 0.18189449608325958 = 0.17495040595531464 + 0.001 * 6.944095611572266
Epoch 320, val loss: 0.7010425925254822
Epoch 330, training loss: 0.1617562621831894 = 0.1548149585723877 + 0.001 * 6.941299915313721
Epoch 330, val loss: 0.7069880962371826
Epoch 340, training loss: 0.14427882432937622 = 0.13734929263591766 + 0.001 * 6.929529666900635
Epoch 340, val loss: 0.7154577970504761
Epoch 350, training loss: 0.12907235324382782 = 0.1221521645784378 + 0.001 * 6.920182704925537
Epoch 350, val loss: 0.7258538603782654
Epoch 360, training loss: 0.11582903563976288 = 0.10890405625104904 + 0.001 * 6.924980640411377
Epoch 360, val loss: 0.7377954721450806
Epoch 370, training loss: 0.10422969609498978 = 0.09732412546873093 + 0.001 * 6.905569553375244
Epoch 370, val loss: 0.7508506178855896
Epoch 380, training loss: 0.0940566211938858 = 0.08716314285993576 + 0.001 * 6.893474102020264
Epoch 380, val loss: 0.7648831009864807
Epoch 390, training loss: 0.08509408682584763 = 0.07821565121412277 + 0.001 * 6.878434658050537
Epoch 390, val loss: 0.7796658873558044
Epoch 400, training loss: 0.07721388339996338 = 0.07031746953725815 + 0.001 * 6.896413803100586
Epoch 400, val loss: 0.7951691746711731
Epoch 410, training loss: 0.0702119693160057 = 0.063328318297863 + 0.001 * 6.883648872375488
Epoch 410, val loss: 0.8110867142677307
Epoch 420, training loss: 0.06400219351053238 = 0.057131681591272354 + 0.001 * 6.870510578155518
Epoch 420, val loss: 0.8273215293884277
Epoch 430, training loss: 0.05849693715572357 = 0.051633283495903015 + 0.001 * 6.863651275634766
Epoch 430, val loss: 0.843728244304657
Epoch 440, training loss: 0.05361024662852287 = 0.04674951732158661 + 0.001 * 6.8607282638549805
Epoch 440, val loss: 0.8603419661521912
Epoch 450, training loss: 0.04926537722349167 = 0.04240686818957329 + 0.001 * 6.858509540557861
Epoch 450, val loss: 0.8769514560699463
Epoch 460, training loss: 0.04539889097213745 = 0.03854179009795189 + 0.001 * 6.857100486755371
Epoch 460, val loss: 0.8933918476104736
Epoch 470, training loss: 0.04195738583803177 = 0.03510133922100067 + 0.001 * 6.856046199798584
Epoch 470, val loss: 0.909646213054657
Epoch 480, training loss: 0.038895752280950546 = 0.03204003721475601 + 0.001 * 6.855714321136475
Epoch 480, val loss: 0.9255141615867615
Epoch 490, training loss: 0.03617031127214432 = 0.029315924271941185 + 0.001 * 6.854386806488037
Epoch 490, val loss: 0.940980076789856
Epoch 500, training loss: 0.03374425321817398 = 0.02688983827829361 + 0.001 * 6.854413032531738
Epoch 500, val loss: 0.9558885097503662
Epoch 510, training loss: 0.03157962113618851 = 0.02472686767578125 + 0.001 * 6.852753639221191
Epoch 510, val loss: 0.9703081846237183
Epoch 520, training loss: 0.029650835320353508 = 0.022795097902417183 + 0.001 * 6.85573673248291
Epoch 520, val loss: 0.9841814041137695
Epoch 530, training loss: 0.02792011946439743 = 0.021066607907414436 + 0.001 * 6.853511810302734
Epoch 530, val loss: 0.9974822998046875
Epoch 540, training loss: 0.026366960257291794 = 0.019516102969646454 + 0.001 * 6.850857734680176
Epoch 540, val loss: 1.0102505683898926
Epoch 550, training loss: 0.024974621832370758 = 0.018122605979442596 + 0.001 * 6.852015972137451
Epoch 550, val loss: 1.022579312324524
Epoch 560, training loss: 0.023713253438472748 = 0.016867278143763542 + 0.001 * 6.845974922180176
Epoch 560, val loss: 1.0343724489212036
Epoch 570, training loss: 0.022579263895750046 = 0.01573331281542778 + 0.001 * 6.845950126647949
Epoch 570, val loss: 1.0457218885421753
Epoch 580, training loss: 0.021557070314884186 = 0.014706552028656006 + 0.001 * 6.850518226623535
Epoch 580, val loss: 1.0566747188568115
Epoch 590, training loss: 0.020620372146368027 = 0.013774747028946877 + 0.001 * 6.8456244468688965
Epoch 590, val loss: 1.0671998262405396
Epoch 600, training loss: 0.01977160945534706 = 0.012926989234983921 + 0.001 * 6.844620704650879
Epoch 600, val loss: 1.0773450136184692
Epoch 610, training loss: 0.019002605229616165 = 0.01215340569615364 + 0.001 * 6.8491997718811035
Epoch 610, val loss: 1.0871500968933105
Epoch 620, training loss: 0.018282810226082802 = 0.011445204727351665 + 0.001 * 6.8376054763793945
Epoch 620, val loss: 1.0966237783432007
Epoch 630, training loss: 0.01763235777616501 = 0.010794923640787601 + 0.001 * 6.837433338165283
Epoch 630, val loss: 1.1058330535888672
Epoch 640, training loss: 0.017046943306922913 = 0.010195574723184109 + 0.001 * 6.8513689041137695
Epoch 640, val loss: 1.1147115230560303
Epoch 650, training loss: 0.01647471822798252 = 0.009641345590353012 + 0.001 * 6.833371639251709
Epoch 650, val loss: 1.1234556436538696
Epoch 660, training loss: 0.015957359224557877 = 0.009126679971814156 + 0.001 * 6.830679893493652
Epoch 660, val loss: 1.1319983005523682
Epoch 670, training loss: 0.015480384230613708 = 0.008648702874779701 + 0.001 * 6.831681251525879
Epoch 670, val loss: 1.1401450634002686
Epoch 680, training loss: 0.015033233910799026 = 0.00820477120578289 + 0.001 * 6.82846212387085
Epoch 680, val loss: 1.1482702493667603
Epoch 690, training loss: 0.014636612497270107 = 0.007792501710355282 + 0.001 * 6.844110488891602
Epoch 690, val loss: 1.1560171842575073
Epoch 700, training loss: 0.014237383380532265 = 0.007410085294395685 + 0.001 * 6.827297210693359
Epoch 700, val loss: 1.1636582612991333
Epoch 710, training loss: 0.013874990865588188 = 0.0070544020272791386 + 0.001 * 6.8205885887146
Epoch 710, val loss: 1.1710577011108398
Epoch 720, training loss: 0.013541534543037415 = 0.006723213940858841 + 0.001 * 6.818320274353027
Epoch 720, val loss: 1.178233027458191
Epoch 730, training loss: 0.013233395293354988 = 0.0064145480282604694 + 0.001 * 6.818846702575684
Epoch 730, val loss: 1.1852458715438843
Epoch 740, training loss: 0.012944824993610382 = 0.00612669950351119 + 0.001 * 6.818125247955322
Epoch 740, val loss: 1.1920398473739624
Epoch 750, training loss: 0.012681889347732067 = 0.005857940763235092 + 0.001 * 6.823948383331299
Epoch 750, val loss: 1.198689341545105
Epoch 760, training loss: 0.012427790090441704 = 0.0056062862277030945 + 0.001 * 6.821503162384033
Epoch 760, val loss: 1.205143928527832
Epoch 770, training loss: 0.012179874815046787 = 0.005369271617382765 + 0.001 * 6.81060266494751
Epoch 770, val loss: 1.2114057540893555
Epoch 780, training loss: 0.01196999754756689 = 0.005144517403095961 + 0.001 * 6.825479984283447
Epoch 780, val loss: 1.2174806594848633
Epoch 790, training loss: 0.011754217557609081 = 0.0049306899309158325 + 0.001 * 6.8235273361206055
Epoch 790, val loss: 1.2233997583389282
Epoch 800, training loss: 0.011532200500369072 = 0.004727199673652649 + 0.001 * 6.8050007820129395
Epoch 800, val loss: 1.2291756868362427
Epoch 810, training loss: 0.011397852562367916 = 0.004533688072115183 + 0.001 * 6.864164352416992
Epoch 810, val loss: 1.2346981763839722
Epoch 820, training loss: 0.01116714347153902 = 0.0043501583859324455 + 0.001 * 6.8169846534729
Epoch 820, val loss: 1.240061640739441
Epoch 830, training loss: 0.010974390432238579 = 0.004176520276814699 + 0.001 * 6.797869682312012
Epoch 830, val loss: 1.245374083518982
Epoch 840, training loss: 0.01080817636102438 = 0.004012471064925194 + 0.001 * 6.7957048416137695
Epoch 840, val loss: 1.2504738569259644
Epoch 850, training loss: 0.010657061822712421 = 0.0038576973602175713 + 0.001 * 6.79936408996582
Epoch 850, val loss: 1.2553762197494507
Epoch 860, training loss: 0.010503488592803478 = 0.003712221747264266 + 0.001 * 6.791266441345215
Epoch 860, val loss: 1.2602837085723877
Epoch 870, training loss: 0.010357893072068691 = 0.003575274720788002 + 0.001 * 6.782618045806885
Epoch 870, val loss: 1.2649767398834229
Epoch 880, training loss: 0.010230546817183495 = 0.003446157556027174 + 0.001 * 6.784388542175293
Epoch 880, val loss: 1.2694952487945557
Epoch 890, training loss: 0.010108056478202343 = 0.003324367804452777 + 0.001 * 6.783688068389893
Epoch 890, val loss: 1.273886799812317
Epoch 900, training loss: 0.01001994963735342 = 0.0032094756606966257 + 0.001 * 6.810473442077637
Epoch 900, val loss: 1.2781422138214111
Epoch 910, training loss: 0.009888643398880959 = 0.0031011118553578854 + 0.001 * 6.787531852722168
Epoch 910, val loss: 1.2823561429977417
Epoch 920, training loss: 0.009768811985850334 = 0.0029987741727381945 + 0.001 * 6.770037651062012
Epoch 920, val loss: 1.2863966226577759
Epoch 930, training loss: 0.00969315692782402 = 0.0029022786766290665 + 0.001 * 6.7908782958984375
Epoch 930, val loss: 1.2904458045959473
Epoch 940, training loss: 0.00963788852095604 = 0.002811119658872485 + 0.001 * 6.826768398284912
Epoch 940, val loss: 1.2943006753921509
Epoch 950, training loss: 0.009492085315287113 = 0.00272491411305964 + 0.001 * 6.767170429229736
Epoch 950, val loss: 1.2980512380599976
Epoch 960, training loss: 0.00941872876137495 = 0.0026432068552821875 + 0.001 * 6.775521755218506
Epoch 960, val loss: 1.3017232418060303
Epoch 970, training loss: 0.00934512261301279 = 0.0025657047517597675 + 0.001 * 6.779417514801025
Epoch 970, val loss: 1.305251121520996
Epoch 980, training loss: 0.009254028089344501 = 0.0024921211879700422 + 0.001 * 6.761906147003174
Epoch 980, val loss: 1.3087377548217773
Epoch 990, training loss: 0.009213426150381565 = 0.002422251971438527 + 0.001 * 6.791173934936523
Epoch 990, val loss: 1.312258243560791
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7232
Flip ASR: 0.6844/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9675613641738892 = 1.959187626838684 + 0.001 * 8.37376594543457
Epoch 0, val loss: 1.9603326320648193
Epoch 10, training loss: 1.9563884735107422 = 1.9480148553848267 + 0.001 * 8.373655319213867
Epoch 10, val loss: 1.949117660522461
Epoch 20, training loss: 1.9427814483642578 = 1.934408187866211 + 0.001 * 8.373299598693848
Epoch 20, val loss: 1.9348628520965576
Epoch 30, training loss: 1.9238970279693604 = 1.9155244827270508 + 0.001 * 8.37255573272705
Epoch 30, val loss: 1.9145867824554443
Epoch 40, training loss: 1.8964980840682983 = 1.888127088546753 + 0.001 * 8.370966911315918
Epoch 40, val loss: 1.885214924812317
Epoch 50, training loss: 1.8583589792251587 = 1.8499919176101685 + 0.001 * 8.367009162902832
Epoch 50, val loss: 1.8457214832305908
Epoch 60, training loss: 1.8130522966384888 = 1.8046987056732178 + 0.001 * 8.353585243225098
Epoch 60, val loss: 1.8025059700012207
Epoch 70, training loss: 1.7698220014572144 = 1.7615360021591187 + 0.001 * 8.285999298095703
Epoch 70, val loss: 1.7653738260269165
Epoch 80, training loss: 1.7209019660949707 = 1.712984323501587 + 0.001 * 7.917604446411133
Epoch 80, val loss: 1.7229282855987549
Epoch 90, training loss: 1.6550254821777344 = 1.6474781036376953 + 0.001 * 7.547352313995361
Epoch 90, val loss: 1.666290521621704
Epoch 100, training loss: 1.5703907012939453 = 1.5630764961242676 + 0.001 * 7.314198970794678
Epoch 100, val loss: 1.5952296257019043
Epoch 110, training loss: 1.4715659618377686 = 1.464350938796997 + 0.001 * 7.215081214904785
Epoch 110, val loss: 1.5153535604476929
Epoch 120, training loss: 1.366937518119812 = 1.3597489595413208 + 0.001 * 7.188528060913086
Epoch 120, val loss: 1.4339734315872192
Epoch 130, training loss: 1.2599551677703857 = 1.2527848482131958 + 0.001 * 7.1703667640686035
Epoch 130, val loss: 1.3546568155288696
Epoch 140, training loss: 1.1518279314041138 = 1.144679307937622 + 0.001 * 7.148650169372559
Epoch 140, val loss: 1.2759373188018799
Epoch 150, training loss: 1.0440564155578613 = 1.036933183670044 + 0.001 * 7.123195171356201
Epoch 150, val loss: 1.1980947256088257
Epoch 160, training loss: 0.9409210681915283 = 0.9338294267654419 + 0.001 * 7.091647148132324
Epoch 160, val loss: 1.123870611190796
Epoch 170, training loss: 0.8478100895881653 = 0.8407538533210754 + 0.001 * 7.056247234344482
Epoch 170, val loss: 1.0582571029663086
Epoch 180, training loss: 0.7675513029098511 = 0.7605253458023071 + 0.001 * 7.025976657867432
Epoch 180, val loss: 1.0037612915039062
Epoch 190, training loss: 0.6986643671989441 = 0.6916574835777283 + 0.001 * 7.006889343261719
Epoch 190, val loss: 0.9594004154205322
Epoch 200, training loss: 0.6374208927154541 = 0.6304274201393127 + 0.001 * 6.993498802185059
Epoch 200, val loss: 0.9224867224693298
Epoch 210, training loss: 0.579944372177124 = 0.5729637742042542 + 0.001 * 6.98058557510376
Epoch 210, val loss: 0.8905452489852905
Epoch 220, training loss: 0.5234326124191284 = 0.5164669752120972 + 0.001 * 6.965664863586426
Epoch 220, val loss: 0.8616726994514465
Epoch 230, training loss: 0.46682411432266235 = 0.45987287163734436 + 0.001 * 6.951247692108154
Epoch 230, val loss: 0.8355637192726135
Epoch 240, training loss: 0.41047772765159607 = 0.4035413861274719 + 0.001 * 6.936334609985352
Epoch 240, val loss: 0.8129652142524719
Epoch 250, training loss: 0.35612404346466064 = 0.34919673204421997 + 0.001 * 6.92732048034668
Epoch 250, val loss: 0.794152021408081
Epoch 260, training loss: 0.30600520968437195 = 0.2990902364253998 + 0.001 * 6.914979934692383
Epoch 260, val loss: 0.7799873352050781
Epoch 270, training loss: 0.26188480854034424 = 0.25497278571128845 + 0.001 * 6.912031173706055
Epoch 270, val loss: 0.770935595035553
Epoch 280, training loss: 0.2243877798318863 = 0.21748456358909607 + 0.001 * 6.903209686279297
Epoch 280, val loss: 0.766788125038147
Epoch 290, training loss: 0.19316606223583221 = 0.18626460433006287 + 0.001 * 6.901451587677002
Epoch 290, val loss: 0.7671018838882446
Epoch 300, training loss: 0.1674206703901291 = 0.16051961481571198 + 0.001 * 6.901053428649902
Epoch 300, val loss: 0.7714249491691589
Epoch 310, training loss: 0.14616841077804565 = 0.1392691433429718 + 0.001 * 6.899264812469482
Epoch 310, val loss: 0.7785720229148865
Epoch 320, training loss: 0.12849770486354828 = 0.12160142511129379 + 0.001 * 6.896273136138916
Epoch 320, val loss: 0.7879856824874878
Epoch 330, training loss: 0.1136832982301712 = 0.10678985714912415 + 0.001 * 6.893442153930664
Epoch 330, val loss: 0.7991473078727722
Epoch 340, training loss: 0.10114352405071259 = 0.09424454718828201 + 0.001 * 6.898978233337402
Epoch 340, val loss: 0.8115062117576599
Epoch 350, training loss: 0.09040521085262299 = 0.0835156962275505 + 0.001 * 6.889514446258545
Epoch 350, val loss: 0.8248563408851624
Epoch 360, training loss: 0.08116637915372849 = 0.07427894324064255 + 0.001 * 6.887438774108887
Epoch 360, val loss: 0.8389359712600708
Epoch 370, training loss: 0.07315587997436523 = 0.06625589728355408 + 0.001 * 6.89998197555542
Epoch 370, val loss: 0.8537054657936096
Epoch 380, training loss: 0.06613709032535553 = 0.05924896150827408 + 0.001 * 6.888128280639648
Epoch 380, val loss: 0.8691179156303406
Epoch 390, training loss: 0.05998614430427551 = 0.053104694932699203 + 0.001 * 6.8814496994018555
Epoch 390, val loss: 0.8848870396614075
Epoch 400, training loss: 0.05458194017410278 = 0.04770178720355034 + 0.001 * 6.88015079498291
Epoch 400, val loss: 0.9009877443313599
Epoch 410, training loss: 0.049832455813884735 = 0.042946040630340576 + 0.001 * 6.886415958404541
Epoch 410, val loss: 0.917296826839447
Epoch 420, training loss: 0.04563312232494354 = 0.03875799477100372 + 0.001 * 6.875125408172607
Epoch 420, val loss: 0.933683454990387
Epoch 430, training loss: 0.041947439312934875 = 0.035069797188043594 + 0.001 * 6.87764310836792
Epoch 430, val loss: 0.9500181078910828
Epoch 440, training loss: 0.03869352117180824 = 0.03182109817862511 + 0.001 * 6.87242317199707
Epoch 440, val loss: 0.9661519527435303
Epoch 450, training loss: 0.035825688391923904 = 0.028958037495613098 + 0.001 * 6.867650508880615
Epoch 450, val loss: 0.9820970296859741
Epoch 460, training loss: 0.0332990363240242 = 0.026431240141391754 + 0.001 * 6.867795467376709
Epoch 460, val loss: 0.997596025466919
Epoch 470, training loss: 0.031057804822921753 = 0.024196848273277283 + 0.001 * 6.86095666885376
Epoch 470, val loss: 1.0127781629562378
Epoch 480, training loss: 0.029075277969241142 = 0.02221611700952053 + 0.001 * 6.859160423278809
Epoch 480, val loss: 1.0274758338928223
Epoch 490, training loss: 0.027307437732815742 = 0.0204565841704607 + 0.001 * 6.850852966308594
Epoch 490, val loss: 1.041795253753662
Epoch 500, training loss: 0.025789642706513405 = 0.018889032304286957 + 0.001 * 6.900609493255615
Epoch 500, val loss: 1.0556763410568237
Epoch 510, training loss: 0.02434075064957142 = 0.017488013952970505 + 0.001 * 6.852736949920654
Epoch 510, val loss: 1.0692038536071777
Epoch 520, training loss: 0.023084796965122223 = 0.016232552006840706 + 0.001 * 6.852244853973389
Epoch 520, val loss: 1.0822721719741821
Epoch 530, training loss: 0.021943677216768265 = 0.015104468911886215 + 0.001 * 6.839209079742432
Epoch 530, val loss: 1.0950087308883667
Epoch 540, training loss: 0.020935669541358948 = 0.014087771065533161 + 0.001 * 6.847898483276367
Epoch 540, val loss: 1.1073899269104004
Epoch 550, training loss: 0.02000020071864128 = 0.013167438097298145 + 0.001 * 6.832762718200684
Epoch 550, val loss: 1.1194308996200562
Epoch 560, training loss: 0.01917712762951851 = 0.012330288998782635 + 0.001 * 6.846837520599365
Epoch 560, val loss: 1.1312135457992554
Epoch 570, training loss: 0.018398979678750038 = 0.011565981432795525 + 0.001 * 6.832998275756836
Epoch 570, val loss: 1.1427503824234009
Epoch 580, training loss: 0.017686735838651657 = 0.010862409137189388 + 0.001 * 6.824326992034912
Epoch 580, val loss: 1.1541333198547363
Epoch 590, training loss: 0.017063241451978683 = 0.010211017914116383 + 0.001 * 6.852223873138428
Epoch 590, val loss: 1.1654454469680786
Epoch 600, training loss: 0.016431324183940887 = 0.0096076475456357 + 0.001 * 6.823676109313965
Epoch 600, val loss: 1.1765918731689453
Epoch 610, training loss: 0.01587367057800293 = 0.009048407897353172 + 0.001 * 6.825263023376465
Epoch 610, val loss: 1.1875948905944824
Epoch 620, training loss: 0.01535479910671711 = 0.008530539460480213 + 0.001 * 6.824259281158447
Epoch 620, val loss: 1.1985183954238892
Epoch 630, training loss: 0.014897255226969719 = 0.008051189593970776 + 0.001 * 6.846065521240234
Epoch 630, val loss: 1.2091530561447144
Epoch 640, training loss: 0.014431266114115715 = 0.007607860956341028 + 0.001 * 6.8234052658081055
Epoch 640, val loss: 1.2196626663208008
Epoch 650, training loss: 0.01404031366109848 = 0.00719729159027338 + 0.001 * 6.843022346496582
Epoch 650, val loss: 1.2299423217773438
Epoch 660, training loss: 0.013627954758703709 = 0.006816042587161064 + 0.001 * 6.8119120597839355
Epoch 660, val loss: 1.2401012182235718
Epoch 670, training loss: 0.013275389559566975 = 0.006461158860474825 + 0.001 * 6.814230442047119
Epoch 670, val loss: 1.2501100301742554
Epoch 680, training loss: 0.012949405238032341 = 0.0061301481910049915 + 0.001 * 6.8192572593688965
Epoch 680, val loss: 1.259848952293396
Epoch 690, training loss: 0.012642893940210342 = 0.005821079947054386 + 0.001 * 6.821814060211182
Epoch 690, val loss: 1.2695410251617432
Epoch 700, training loss: 0.01234273798763752 = 0.005532430950552225 + 0.001 * 6.810306072235107
Epoch 700, val loss: 1.2790695428848267
Epoch 710, training loss: 0.012061964720487595 = 0.0052629560232162476 + 0.001 * 6.799008846282959
Epoch 710, val loss: 1.2884314060211182
Epoch 720, training loss: 0.01181972585618496 = 0.005011363886296749 + 0.001 * 6.808361053466797
Epoch 720, val loss: 1.2976447343826294
Epoch 730, training loss: 0.011594749987125397 = 0.004776437766849995 + 0.001 * 6.8183112144470215
Epoch 730, val loss: 1.3066173791885376
Epoch 740, training loss: 0.011357411742210388 = 0.004556941334158182 + 0.001 * 6.800470352172852
Epoch 740, val loss: 1.3154807090759277
Epoch 750, training loss: 0.011160669848322868 = 0.004351798910647631 + 0.001 * 6.808870792388916
Epoch 750, val loss: 1.324148178100586
Epoch 760, training loss: 0.010960428044199944 = 0.0041602677665650845 + 0.001 * 6.8001604080200195
Epoch 760, val loss: 1.3326278924942017
Epoch 770, training loss: 0.010775686241686344 = 0.003981270827353001 + 0.001 * 6.79441499710083
Epoch 770, val loss: 1.3409295082092285
Epoch 780, training loss: 0.010600253939628601 = 0.0038137780502438545 + 0.001 * 6.786475658416748
Epoch 780, val loss: 1.3490393161773682
Epoch 790, training loss: 0.010450598783791065 = 0.0036568769719451666 + 0.001 * 6.793721675872803
Epoch 790, val loss: 1.356959581375122
Epoch 800, training loss: 0.010291087441146374 = 0.0035097936633974314 + 0.001 * 6.781293869018555
Epoch 800, val loss: 1.364748239517212
Epoch 810, training loss: 0.010158050805330276 = 0.0033717306796461344 + 0.001 * 6.786319732666016
Epoch 810, val loss: 1.3723722696304321
Epoch 820, training loss: 0.010044288821518421 = 0.0032421196810901165 + 0.001 * 6.802168846130371
Epoch 820, val loss: 1.3798397779464722
Epoch 830, training loss: 0.009905727580189705 = 0.003120295237749815 + 0.001 * 6.7854323387146
Epoch 830, val loss: 1.3871219158172607
Epoch 840, training loss: 0.009785745292901993 = 0.0030056899413466454 + 0.001 * 6.780055522918701
Epoch 840, val loss: 1.3942646980285645
Epoch 850, training loss: 0.009679065085947514 = 0.0028977810870856047 + 0.001 * 6.781283378601074
Epoch 850, val loss: 1.4012523889541626
Epoch 860, training loss: 0.009572029113769531 = 0.0027960743755102158 + 0.001 * 6.775953769683838
Epoch 860, val loss: 1.4080911874771118
Epoch 870, training loss: 0.009473435580730438 = 0.0027001623529940844 + 0.001 * 6.773272514343262
Epoch 870, val loss: 1.4147822856903076
Epoch 880, training loss: 0.009382974356412888 = 0.0026095842476934195 + 0.001 * 6.7733893394470215
Epoch 880, val loss: 1.4213486909866333
Epoch 890, training loss: 0.009291130118072033 = 0.0025240059476345778 + 0.001 * 6.767123699188232
Epoch 890, val loss: 1.4277738332748413
Epoch 900, training loss: 0.009211037307977676 = 0.002443047473207116 + 0.001 * 6.767989635467529
Epoch 900, val loss: 1.4340300559997559
Epoch 910, training loss: 0.009138291701674461 = 0.0023664594627916813 + 0.001 * 6.77183198928833
Epoch 910, val loss: 1.4401524066925049
Epoch 920, training loss: 0.009064917452633381 = 0.0022938635665923357 + 0.001 * 6.771053791046143
Epoch 920, val loss: 1.4461588859558105
Epoch 930, training loss: 0.009012171998620033 = 0.002225032774731517 + 0.001 * 6.78713846206665
Epoch 930, val loss: 1.4520477056503296
Epoch 940, training loss: 0.008943301625549793 = 0.0021597445011138916 + 0.001 * 6.783556938171387
Epoch 940, val loss: 1.457762598991394
Epoch 950, training loss: 0.008868513628840446 = 0.0020977496169507504 + 0.001 * 6.770764350891113
Epoch 950, val loss: 1.4634205102920532
Epoch 960, training loss: 0.00879925861954689 = 0.00203884718939662 + 0.001 * 6.760411262512207
Epoch 960, val loss: 1.4689222574234009
Epoch 970, training loss: 0.00874786451458931 = 0.001982823945581913 + 0.001 * 6.765039920806885
Epoch 970, val loss: 1.4742965698242188
Epoch 980, training loss: 0.008682286366820335 = 0.0019294798839837313 + 0.001 * 6.752805709838867
Epoch 980, val loss: 1.4796000719070435
Epoch 990, training loss: 0.008630619384348392 = 0.0018786591244861484 + 0.001 * 6.751960277557373
Epoch 990, val loss: 1.4847209453582764
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9077
Flip ASR: 0.8889/225 nodes
The final ASR:0.72079, 0.15367, Accuracy:0.81235, 0.01145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9448])
updated graph: torch.Size([2, 10488])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9735405445098877 = 1.9651668071746826 + 0.001 * 8.373738288879395
Epoch 0, val loss: 1.9664419889450073
Epoch 10, training loss: 1.9634647369384766 = 1.955091118812561 + 0.001 * 8.373669624328613
Epoch 10, val loss: 1.957147479057312
Epoch 20, training loss: 1.9511879682540894 = 1.942814588546753 + 0.001 * 8.3733491897583
Epoch 20, val loss: 1.9454420804977417
Epoch 30, training loss: 1.9339311122894287 = 1.92555832862854 + 0.001 * 8.372730255126953
Epoch 30, val loss: 1.9287694692611694
Epoch 40, training loss: 1.9083877801895142 = 1.9000163078308105 + 0.001 * 8.371472358703613
Epoch 40, val loss: 1.9042216539382935
Epoch 50, training loss: 1.8715466260910034 = 1.8631786108016968 + 0.001 * 8.3679780960083
Epoch 50, val loss: 1.8701859712600708
Epoch 60, training loss: 1.8257869482040405 = 1.8174340724945068 + 0.001 * 8.352889060974121
Epoch 60, val loss: 1.8316961526870728
Epoch 70, training loss: 1.7816908359527588 = 1.7734463214874268 + 0.001 * 8.244479179382324
Epoch 70, val loss: 1.7979120016098022
Epoch 80, training loss: 1.7339779138565063 = 1.7262805700302124 + 0.001 * 7.697395324707031
Epoch 80, val loss: 1.7575682401657104
Epoch 90, training loss: 1.6703155040740967 = 1.662920355796814 + 0.001 * 7.39512300491333
Epoch 90, val loss: 1.7028800249099731
Epoch 100, training loss: 1.586121916770935 = 1.5789133310317993 + 0.001 * 7.208591461181641
Epoch 100, val loss: 1.6329084634780884
Epoch 110, training loss: 1.4834659099578857 = 1.476367712020874 + 0.001 * 7.098196983337402
Epoch 110, val loss: 1.5506114959716797
Epoch 120, training loss: 1.3729314804077148 = 1.365875482559204 + 0.001 * 7.056037425994873
Epoch 120, val loss: 1.4632909297943115
Epoch 130, training loss: 1.2643933296203613 = 1.257370114326477 + 0.001 * 7.023184299468994
Epoch 130, val loss: 1.379556655883789
Epoch 140, training loss: 1.1626044511795044 = 1.1556148529052734 + 0.001 * 6.989572048187256
Epoch 140, val loss: 1.3009949922561646
Epoch 150, training loss: 1.0691967010498047 = 1.0622432231903076 + 0.001 * 6.953502655029297
Epoch 150, val loss: 1.2283886671066284
Epoch 160, training loss: 0.9833231568336487 = 0.9764063358306885 + 0.001 * 6.9168500900268555
Epoch 160, val loss: 1.1609550714492798
Epoch 170, training loss: 0.9023539423942566 = 0.8954652547836304 + 0.001 * 6.8886637687683105
Epoch 170, val loss: 1.0975958108901978
Epoch 180, training loss: 0.8244979381561279 = 0.8176265954971313 + 0.001 * 6.8713531494140625
Epoch 180, val loss: 1.038479208946228
Epoch 190, training loss: 0.7495028376579285 = 0.7426407337188721 + 0.001 * 6.862115859985352
Epoch 190, val loss: 0.9830051064491272
Epoch 200, training loss: 0.678287148475647 = 0.6714318990707397 + 0.001 * 6.855237007141113
Epoch 200, val loss: 0.9316922426223755
Epoch 210, training loss: 0.611822247505188 = 0.6049708127975464 + 0.001 * 6.85145378112793
Epoch 210, val loss: 0.8853122591972351
Epoch 220, training loss: 0.5508394241333008 = 0.5439916253089905 + 0.001 * 6.847796440124512
Epoch 220, val loss: 0.8444773554801941
Epoch 230, training loss: 0.4956298768520355 = 0.48878592252731323 + 0.001 * 6.843944549560547
Epoch 230, val loss: 0.8098465800285339
Epoch 240, training loss: 0.445802241563797 = 0.43896254897117615 + 0.001 * 6.83970308303833
Epoch 240, val loss: 0.7815158367156982
Epoch 250, training loss: 0.400642454624176 = 0.39380770921707153 + 0.001 * 6.834753513336182
Epoch 250, val loss: 0.7588645219802856
Epoch 260, training loss: 0.35934925079345703 = 0.35251951217651367 + 0.001 * 6.829738616943359
Epoch 260, val loss: 0.7414937615394592
Epoch 270, training loss: 0.3212132453918457 = 0.314389705657959 + 0.001 * 6.823533058166504
Epoch 270, val loss: 0.728917121887207
Epoch 280, training loss: 0.2857179641723633 = 0.27890288829803467 + 0.001 * 6.8150634765625
Epoch 280, val loss: 0.7205085754394531
Epoch 290, training loss: 0.25270816683769226 = 0.2459026575088501 + 0.001 * 6.805502414703369
Epoch 290, val loss: 0.715954601764679
Epoch 300, training loss: 0.22237564623355865 = 0.2155742049217224 + 0.001 * 6.801437854766846
Epoch 300, val loss: 0.7150335311889648
Epoch 310, training loss: 0.19504642486572266 = 0.18826015293598175 + 0.001 * 6.786274433135986
Epoch 310, val loss: 0.7174201607704163
Epoch 320, training loss: 0.17101609706878662 = 0.16422942280769348 + 0.001 * 6.786670684814453
Epoch 320, val loss: 0.7229565382003784
Epoch 330, training loss: 0.15023960173130035 = 0.14347214996814728 + 0.001 * 6.767457008361816
Epoch 330, val loss: 0.7313758730888367
Epoch 340, training loss: 0.13248369097709656 = 0.12571629881858826 + 0.001 * 6.767387866973877
Epoch 340, val loss: 0.7422502040863037
Epoch 350, training loss: 0.11732886731624603 = 0.11056721955537796 + 0.001 * 6.761650562286377
Epoch 350, val loss: 0.7551277279853821
Epoch 360, training loss: 0.10438061505556107 = 0.09763344377279282 + 0.001 * 6.747172832489014
Epoch 360, val loss: 0.7696026563644409
Epoch 370, training loss: 0.09329747408628464 = 0.08655492216348648 + 0.001 * 6.742553234100342
Epoch 370, val loss: 0.7852582931518555
Epoch 380, training loss: 0.08376844972372055 = 0.07702521234750748 + 0.001 * 6.743236541748047
Epoch 380, val loss: 0.8018198609352112
Epoch 390, training loss: 0.07553566992282867 = 0.0687907263636589 + 0.001 * 6.7449445724487305
Epoch 390, val loss: 0.8190476298332214
Epoch 400, training loss: 0.06838098913431168 = 0.061647798866033554 + 0.001 * 6.733189105987549
Epoch 400, val loss: 0.8366697430610657
Epoch 410, training loss: 0.06216234713792801 = 0.05542878806591034 + 0.001 * 6.733560085296631
Epoch 410, val loss: 0.8545669913291931
Epoch 420, training loss: 0.056729286909103394 = 0.049999628216028214 + 0.001 * 6.729658603668213
Epoch 420, val loss: 0.8724719882011414
Epoch 430, training loss: 0.05197460949420929 = 0.04524470865726471 + 0.001 * 6.72990083694458
Epoch 430, val loss: 0.8903379440307617
Epoch 440, training loss: 0.04779112711548805 = 0.04106830060482025 + 0.001 * 6.7228264808654785
Epoch 440, val loss: 0.9080013036727905
Epoch 450, training loss: 0.0441197007894516 = 0.03739163279533386 + 0.001 * 6.7280683517456055
Epoch 450, val loss: 0.9253664612770081
Epoch 460, training loss: 0.0408635251224041 = 0.03414484113454819 + 0.001 * 6.718682289123535
Epoch 460, val loss: 0.9424804449081421
Epoch 470, training loss: 0.03798694908618927 = 0.03127094730734825 + 0.001 * 6.716001987457275
Epoch 470, val loss: 0.9592219591140747
Epoch 480, training loss: 0.03543858975172043 = 0.028720233589410782 + 0.001 * 6.718356132507324
Epoch 480, val loss: 0.9756298065185547
Epoch 490, training loss: 0.033158816397190094 = 0.02645004726946354 + 0.001 * 6.708767890930176
Epoch 490, val loss: 0.9916058778762817
Epoch 500, training loss: 0.03114255890250206 = 0.024422483518719673 + 0.001 * 6.7200751304626465
Epoch 500, val loss: 1.00723135471344
Epoch 510, training loss: 0.02931596152484417 = 0.02260412648320198 + 0.001 * 6.711834907531738
Epoch 510, val loss: 1.0224413871765137
Epoch 520, training loss: 0.027669737115502357 = 0.02096671424806118 + 0.001 * 6.703022003173828
Epoch 520, val loss: 1.0371853113174438
Epoch 530, training loss: 0.02621478959918022 = 0.019487792626023293 + 0.001 * 6.726996898651123
Epoch 530, val loss: 1.0515416860580444
Epoch 540, training loss: 0.0248540248721838 = 0.018149886280298233 + 0.001 * 6.704137802124023
Epoch 540, val loss: 1.065513253211975
Epoch 550, training loss: 0.02363884635269642 = 0.0169374942779541 + 0.001 * 6.701352119445801
Epoch 550, val loss: 1.0790369510650635
Epoch 560, training loss: 0.022535644471645355 = 0.015837060287594795 + 0.001 * 6.698583126068115
Epoch 560, val loss: 1.0921838283538818
Epoch 570, training loss: 0.021525708958506584 = 0.014836748130619526 + 0.001 * 6.688960075378418
Epoch 570, val loss: 1.1049413681030273
Epoch 580, training loss: 0.020642658695578575 = 0.013925803825259209 + 0.001 * 6.716855049133301
Epoch 580, val loss: 1.1173280477523804
Epoch 590, training loss: 0.019780052825808525 = 0.01309478934854269 + 0.001 * 6.685263633728027
Epoch 590, val loss: 1.1293658018112183
Epoch 600, training loss: 0.019017253071069717 = 0.012333940714597702 + 0.001 * 6.683312892913818
Epoch 600, val loss: 1.1411316394805908
Epoch 610, training loss: 0.01831858791410923 = 0.011635813862085342 + 0.001 * 6.682774066925049
Epoch 610, val loss: 1.1525874137878418
Epoch 620, training loss: 0.017670460045337677 = 0.010992886498570442 + 0.001 * 6.677574157714844
Epoch 620, val loss: 1.1637669801712036
Epoch 630, training loss: 0.017080295830965042 = 0.01039926614612341 + 0.001 * 6.681028366088867
Epoch 630, val loss: 1.1746877431869507
Epoch 640, training loss: 0.016525501385331154 = 0.009849846363067627 + 0.001 * 6.675654888153076
Epoch 640, val loss: 1.1853655576705933
Epoch 650, training loss: 0.016032667830586433 = 0.009340455755591393 + 0.001 * 6.692212104797363
Epoch 650, val loss: 1.195855975151062
Epoch 660, training loss: 0.015545958653092384 = 0.008867948316037655 + 0.001 * 6.678009986877441
Epoch 660, val loss: 1.206028699874878
Epoch 670, training loss: 0.015100879594683647 = 0.00842904020100832 + 0.001 * 6.671839237213135
Epoch 670, val loss: 1.216020107269287
Epoch 680, training loss: 0.014708967879414558 = 0.008020403794944286 + 0.001 * 6.688563346862793
Epoch 680, val loss: 1.2258005142211914
Epoch 690, training loss: 0.01431124098598957 = 0.007639109622687101 + 0.001 * 6.672131061553955
Epoch 690, val loss: 1.2354305982589722
Epoch 700, training loss: 0.01394700538367033 = 0.007281550671905279 + 0.001 * 6.665454387664795
Epoch 700, val loss: 1.2449404001235962
Epoch 710, training loss: 0.013641799800097942 = 0.006944986525923014 + 0.001 * 6.696813106536865
Epoch 710, val loss: 1.2543591260910034
Epoch 720, training loss: 0.013299146667122841 = 0.006628208328038454 + 0.001 * 6.670937538146973
Epoch 720, val loss: 1.2637076377868652
Epoch 730, training loss: 0.012999817728996277 = 0.006329791620373726 + 0.001 * 6.6700263023376465
Epoch 730, val loss: 1.272993564605713
Epoch 740, training loss: 0.012710537761449814 = 0.006049015559256077 + 0.001 * 6.6615214347839355
Epoch 740, val loss: 1.2820827960968018
Epoch 750, training loss: 0.012454099953174591 = 0.005785252898931503 + 0.001 * 6.668846607208252
Epoch 750, val loss: 1.2911032438278198
Epoch 760, training loss: 0.012198362499475479 = 0.005537640769034624 + 0.001 * 6.660721302032471
Epoch 760, val loss: 1.2999504804611206
Epoch 770, training loss: 0.011978641152381897 = 0.0053052110597491264 + 0.001 * 6.673429489135742
Epoch 770, val loss: 1.3087159395217896
Epoch 780, training loss: 0.011738559231162071 = 0.005086921621114016 + 0.001 * 6.651637554168701
Epoch 780, val loss: 1.3172364234924316
Epoch 790, training loss: 0.011541726067662239 = 0.004881891887634993 + 0.001 * 6.659833908081055
Epoch 790, val loss: 1.325632929801941
Epoch 800, training loss: 0.011353692039847374 = 0.004689371678978205 + 0.001 * 6.6643195152282715
Epoch 800, val loss: 1.33383309841156
Epoch 810, training loss: 0.011187456548213959 = 0.004508575890213251 + 0.001 * 6.678879737854004
Epoch 810, val loss: 1.3419562578201294
Epoch 820, training loss: 0.01100202463567257 = 0.004338568076491356 + 0.001 * 6.663456916809082
Epoch 820, val loss: 1.349825143814087
Epoch 830, training loss: 0.010827537626028061 = 0.0041786725632846355 + 0.001 * 6.64886474609375
Epoch 830, val loss: 1.3576704263687134
Epoch 840, training loss: 0.010689138434827328 = 0.00402809539809823 + 0.001 * 6.6610426902771
Epoch 840, val loss: 1.3652961254119873
Epoch 850, training loss: 0.010544885881245136 = 0.003886134596541524 + 0.001 * 6.658751010894775
Epoch 850, val loss: 1.3727694749832153
Epoch 860, training loss: 0.010400747880339622 = 0.0037522499915212393 + 0.001 * 6.648497581481934
Epoch 860, val loss: 1.3800896406173706
Epoch 870, training loss: 0.010273071005940437 = 0.0036259109620004892 + 0.001 * 6.647160053253174
Epoch 870, val loss: 1.3872621059417725
Epoch 880, training loss: 0.01015702448785305 = 0.0035065142437815666 + 0.001 * 6.650509357452393
Epoch 880, val loss: 1.3943085670471191
Epoch 890, training loss: 0.010030072182416916 = 0.003393582534044981 + 0.001 * 6.6364898681640625
Epoch 890, val loss: 1.4012274742126465
Epoch 900, training loss: 0.009921716526150703 = 0.003286751452833414 + 0.001 * 6.634964466094971
Epoch 900, val loss: 1.40800142288208
Epoch 910, training loss: 0.009831245988607407 = 0.003185566049069166 + 0.001 * 6.645679473876953
Epoch 910, val loss: 1.4146279096603394
Epoch 920, training loss: 0.009725409559905529 = 0.0030895944219082594 + 0.001 * 6.635815143585205
Epoch 920, val loss: 1.4211413860321045
Epoch 930, training loss: 0.009660610929131508 = 0.00299847568385303 + 0.001 * 6.662135124206543
Epoch 930, val loss: 1.4275295734405518
Epoch 940, training loss: 0.00953830499202013 = 0.002911899471655488 + 0.001 * 6.626405239105225
Epoch 940, val loss: 1.4338022470474243
Epoch 950, training loss: 0.009482961148023605 = 0.0028296082746237516 + 0.001 * 6.6533522605896
Epoch 950, val loss: 1.4399648904800415
Epoch 960, training loss: 0.009379907511174679 = 0.0027513832319527864 + 0.001 * 6.628524303436279
Epoch 960, val loss: 1.4459717273712158
Epoch 970, training loss: 0.009309646673500538 = 0.002676894422620535 + 0.001 * 6.632751941680908
Epoch 970, val loss: 1.4518872499465942
Epoch 980, training loss: 0.009238019585609436 = 0.00260597700253129 + 0.001 * 6.632042407989502
Epoch 980, val loss: 1.4576740264892578
Epoch 990, training loss: 0.009159314446151257 = 0.0025383683387190104 + 0.001 * 6.620945453643799
Epoch 990, val loss: 1.463385820388794
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6310
Flip ASR: 0.5556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9686076641082764 = 1.9602339267730713 + 0.001 * 8.373699188232422
Epoch 0, val loss: 1.9564493894577026
Epoch 10, training loss: 1.9569588899612427 = 1.9485853910446167 + 0.001 * 8.37354850769043
Epoch 10, val loss: 1.945093035697937
Epoch 20, training loss: 1.9419872760772705 = 1.9336141347885132 + 0.001 * 8.373119354248047
Epoch 20, val loss: 1.9299551248550415
Epoch 30, training loss: 1.9202170372009277 = 1.9118448495864868 + 0.001 * 8.372235298156738
Epoch 30, val loss: 1.9076848030090332
Epoch 40, training loss: 1.8878822326660156 = 1.879512071609497 + 0.001 * 8.370129585266113
Epoch 40, val loss: 1.8750312328338623
Epoch 50, training loss: 1.844620943069458 = 1.8362574577331543 + 0.001 * 8.363486289978027
Epoch 50, val loss: 1.8334622383117676
Epoch 60, training loss: 1.7995694875717163 = 1.791238784790039 + 0.001 * 8.330737113952637
Epoch 60, val loss: 1.7936105728149414
Epoch 70, training loss: 1.7587324380874634 = 1.750636339187622 + 0.001 * 8.096112251281738
Epoch 70, val loss: 1.7569514513015747
Epoch 80, training loss: 1.706018328666687 = 1.6982421875 + 0.001 * 7.776148319244385
Epoch 80, val loss: 1.7070462703704834
Epoch 90, training loss: 1.6350986957550049 = 1.6275540590286255 + 0.001 * 7.544610500335693
Epoch 90, val loss: 1.6431406736373901
Epoch 100, training loss: 1.5443943738937378 = 1.5370535850524902 + 0.001 * 7.340769290924072
Epoch 100, val loss: 1.566017508506775
Epoch 110, training loss: 1.4453821182250977 = 1.438262701034546 + 0.001 * 7.119382858276367
Epoch 110, val loss: 1.4862289428710938
Epoch 120, training loss: 1.3515467643737793 = 1.3444981575012207 + 0.001 * 7.048551559448242
Epoch 120, val loss: 1.415825605392456
Epoch 130, training loss: 1.2646287679672241 = 1.257623314857483 + 0.001 * 7.00544548034668
Epoch 130, val loss: 1.3547968864440918
Epoch 140, training loss: 1.1817693710327148 = 1.174773931503296 + 0.001 * 6.995473861694336
Epoch 140, val loss: 1.2986860275268555
Epoch 150, training loss: 1.1000726222991943 = 1.0930842161178589 + 0.001 * 6.988426685333252
Epoch 150, val loss: 1.2433171272277832
Epoch 160, training loss: 1.0176585912704468 = 1.0106794834136963 + 0.001 * 6.979066371917725
Epoch 160, val loss: 1.1880130767822266
Epoch 170, training loss: 0.9347810745239258 = 0.9278122782707214 + 0.001 * 6.968801975250244
Epoch 170, val loss: 1.1318786144256592
Epoch 180, training loss: 0.853609561920166 = 0.846653163433075 + 0.001 * 6.956414222717285
Epoch 180, val loss: 1.0767016410827637
Epoch 190, training loss: 0.7767779231071472 = 0.7698357105255127 + 0.001 * 6.942183971405029
Epoch 190, val loss: 1.0242414474487305
Epoch 200, training loss: 0.7062095403671265 = 0.6992820501327515 + 0.001 * 6.927519798278809
Epoch 200, val loss: 0.9767777323722839
Epoch 210, training loss: 0.6428542137145996 = 0.635940432548523 + 0.001 * 6.9137678146362305
Epoch 210, val loss: 0.9352697730064392
Epoch 220, training loss: 0.586896538734436 = 0.5799928307533264 + 0.001 * 6.903717994689941
Epoch 220, val loss: 0.9000586271286011
Epoch 230, training loss: 0.5378143191337585 = 0.5309174656867981 + 0.001 * 6.896865367889404
Epoch 230, val loss: 0.8715671300888062
Epoch 240, training loss: 0.49441665410995483 = 0.48752713203430176 + 0.001 * 6.889521598815918
Epoch 240, val loss: 0.8488017320632935
Epoch 250, training loss: 0.45519083738327026 = 0.44830554723739624 + 0.001 * 6.88530158996582
Epoch 250, val loss: 0.8308547139167786
Epoch 260, training loss: 0.4187297224998474 = 0.4118483364582062 + 0.001 * 6.881374835968018
Epoch 260, val loss: 0.8166273236274719
Epoch 270, training loss: 0.3839088976383209 = 0.3770309388637543 + 0.001 * 6.877955436706543
Epoch 270, val loss: 0.8048663139343262
Epoch 280, training loss: 0.3500150740146637 = 0.343140184879303 + 0.001 * 6.874895095825195
Epoch 280, val loss: 0.7949144840240479
Epoch 290, training loss: 0.31687209010124207 = 0.3099992871284485 + 0.001 * 6.872815132141113
Epoch 290, val loss: 0.7867385745048523
Epoch 300, training loss: 0.2846514880657196 = 0.27778246998786926 + 0.001 * 6.869011402130127
Epoch 300, val loss: 0.7802881002426147
Epoch 310, training loss: 0.25363439321517944 = 0.24676905572414398 + 0.001 * 6.865324974060059
Epoch 310, val loss: 0.7755686044692993
Epoch 320, training loss: 0.2243754267692566 = 0.21750913560390472 + 0.001 * 6.866283893585205
Epoch 320, val loss: 0.7735991477966309
Epoch 330, training loss: 0.1972886323928833 = 0.1904287040233612 + 0.001 * 6.859928607940674
Epoch 330, val loss: 0.7743200659751892
Epoch 340, training loss: 0.1726887971162796 = 0.16583402454853058 + 0.001 * 6.854772090911865
Epoch 340, val loss: 0.7783175706863403
Epoch 350, training loss: 0.15001066029071808 = 0.14315956830978394 + 0.001 * 6.85109281539917
Epoch 350, val loss: 0.7856283783912659
Epoch 360, training loss: 0.1292218118906021 = 0.1223745122551918 + 0.001 * 6.84729528427124
Epoch 360, val loss: 0.7952525615692139
Epoch 370, training loss: 0.11000432074069977 = 0.10316415131092072 + 0.001 * 6.840166091918945
Epoch 370, val loss: 0.8082478046417236
Epoch 380, training loss: 0.09309383481740952 = 0.08625036478042603 + 0.001 * 6.843469619750977
Epoch 380, val loss: 0.8244471549987793
Epoch 390, training loss: 0.08001904934644699 = 0.07318839430809021 + 0.001 * 6.8306565284729
Epoch 390, val loss: 0.8438230752944946
Epoch 400, training loss: 0.07018841058015823 = 0.06336791813373566 + 0.001 * 6.820489406585693
Epoch 400, val loss: 0.8644258379936218
Epoch 410, training loss: 0.06233365461230278 = 0.05552242323756218 + 0.001 * 6.811229705810547
Epoch 410, val loss: 0.8848965764045715
Epoch 420, training loss: 0.055795829743146896 = 0.04897485300898552 + 0.001 * 6.820976734161377
Epoch 420, val loss: 0.9042587876319885
Epoch 430, training loss: 0.05032920092344284 = 0.04352112486958504 + 0.001 * 6.808074474334717
Epoch 430, val loss: 0.9235011339187622
Epoch 440, training loss: 0.04570881277322769 = 0.03889835998415947 + 0.001 * 6.810451507568359
Epoch 440, val loss: 0.9425923824310303
Epoch 450, training loss: 0.04176236689090729 = 0.03496047481894493 + 0.001 * 6.801890850067139
Epoch 450, val loss: 0.9615071415901184
Epoch 460, training loss: 0.03836049884557724 = 0.03157973289489746 + 0.001 * 6.780766487121582
Epoch 460, val loss: 0.9797330498695374
Epoch 470, training loss: 0.0354452058672905 = 0.028655584901571274 + 0.001 * 6.7896199226379395
Epoch 470, val loss: 0.9972307085990906
Epoch 480, training loss: 0.032895587384700775 = 0.026109836995601654 + 0.001 * 6.785749912261963
Epoch 480, val loss: 1.014087438583374
Epoch 490, training loss: 0.03066643327474594 = 0.02387966588139534 + 0.001 * 6.78676700592041
Epoch 490, val loss: 1.030394434928894
Epoch 500, training loss: 0.028685428202152252 = 0.021913638338446617 + 0.001 * 6.771789073944092
Epoch 500, val loss: 1.046230435371399
Epoch 510, training loss: 0.026942115277051926 = 0.02017022855579853 + 0.001 * 6.771886348724365
Epoch 510, val loss: 1.061642050743103
Epoch 520, training loss: 0.025388453155755997 = 0.018617339432239532 + 0.001 * 6.77111291885376
Epoch 520, val loss: 1.0766236782073975
Epoch 530, training loss: 0.02399495244026184 = 0.01722903922200203 + 0.001 * 6.7659125328063965
Epoch 530, val loss: 1.0912418365478516
Epoch 540, training loss: 0.022748257964849472 = 0.015984272584319115 + 0.001 * 6.763984680175781
Epoch 540, val loss: 1.1054736375808716
Epoch 550, training loss: 0.021628890186548233 = 0.014865148812532425 + 0.001 * 6.763740539550781
Epoch 550, val loss: 1.119370460510254
Epoch 560, training loss: 0.020620569586753845 = 0.013856441713869572 + 0.001 * 6.764126777648926
Epoch 560, val loss: 1.1328617334365845
Epoch 570, training loss: 0.019702939316630363 = 0.012945011258125305 + 0.001 * 6.757928371429443
Epoch 570, val loss: 1.1459814310073853
Epoch 580, training loss: 0.01889999955892563 = 0.012119502760469913 + 0.001 * 6.780497074127197
Epoch 580, val loss: 1.1587311029434204
Epoch 590, training loss: 0.018125751987099648 = 0.011370188556611538 + 0.001 * 6.755563735961914
Epoch 590, val loss: 1.171133041381836
Epoch 600, training loss: 0.017444435507059097 = 0.010687298141419888 + 0.001 * 6.757137298583984
Epoch 600, val loss: 1.1831188201904297
Epoch 610, training loss: 0.01682780683040619 = 0.010063349269330502 + 0.001 * 6.764457702636719
Epoch 610, val loss: 1.1948833465576172
Epoch 620, training loss: 0.01624550111591816 = 0.009491442702710629 + 0.001 * 6.754058361053467
Epoch 620, val loss: 1.206274390220642
Epoch 630, training loss: 0.015727102756500244 = 0.008965736255049706 + 0.001 * 6.761367321014404
Epoch 630, val loss: 1.2173545360565186
Epoch 640, training loss: 0.01522896345704794 = 0.008481289260089397 + 0.001 * 6.747673988342285
Epoch 640, val loss: 1.22816801071167
Epoch 650, training loss: 0.014780840836465359 = 0.008033904246985912 + 0.001 * 6.746936321258545
Epoch 650, val loss: 1.2387471199035645
Epoch 660, training loss: 0.01437535509467125 = 0.0076198698952794075 + 0.001 * 6.755485534667969
Epoch 660, val loss: 1.2491066455841064
Epoch 670, training loss: 0.013990087434649467 = 0.007236222270876169 + 0.001 * 6.753864765167236
Epoch 670, val loss: 1.2592167854309082
Epoch 680, training loss: 0.013620647601783276 = 0.006880004424601793 + 0.001 * 6.74064302444458
Epoch 680, val loss: 1.2691539525985718
Epoch 690, training loss: 0.013324534520506859 = 0.006548906676471233 + 0.001 * 6.775628089904785
Epoch 690, val loss: 1.2788233757019043
Epoch 700, training loss: 0.012993021868169308 = 0.006240826100111008 + 0.001 * 6.752195358276367
Epoch 700, val loss: 1.2883256673812866
Epoch 710, training loss: 0.012695299461483955 = 0.005953858606517315 + 0.001 * 6.741440296173096
Epoch 710, val loss: 1.2976574897766113
Epoch 720, training loss: 0.012419180944561958 = 0.005686504766345024 + 0.001 * 6.732676029205322
Epoch 720, val loss: 1.306786060333252
Epoch 730, training loss: 0.012170093134045601 = 0.005437242332845926 + 0.001 * 6.732850551605225
Epoch 730, val loss: 1.3157745599746704
Epoch 740, training loss: 0.011930071748793125 = 0.005204436369240284 + 0.001 * 6.725635051727295
Epoch 740, val loss: 1.3245116472244263
Epoch 750, training loss: 0.011715462431311607 = 0.0049867830239236355 + 0.001 * 6.7286787033081055
Epoch 750, val loss: 1.333024501800537
Epoch 760, training loss: 0.011509024538099766 = 0.00478313909843564 + 0.001 * 6.725884914398193
Epoch 760, val loss: 1.3413276672363281
Epoch 770, training loss: 0.011317543685436249 = 0.004592333920300007 + 0.001 * 6.725208759307861
Epoch 770, val loss: 1.3494651317596436
Epoch 780, training loss: 0.011139717884361744 = 0.004413297865539789 + 0.001 * 6.726419925689697
Epoch 780, val loss: 1.3574076890945435
Epoch 790, training loss: 0.010963307693600655 = 0.004245102405548096 + 0.001 * 6.718205451965332
Epoch 790, val loss: 1.3652000427246094
Epoch 800, training loss: 0.01080029085278511 = 0.0040869442746043205 + 0.001 * 6.713346481323242
Epoch 800, val loss: 1.3728704452514648
Epoch 810, training loss: 0.010669282637536526 = 0.003938106819987297 + 0.001 * 6.731175422668457
Epoch 810, val loss: 1.3804067373275757
Epoch 820, training loss: 0.01052994653582573 = 0.003797908779233694 + 0.001 * 6.73203706741333
Epoch 820, val loss: 1.3877629041671753
Epoch 830, training loss: 0.010384319350123405 = 0.0036656451411545277 + 0.001 * 6.718673229217529
Epoch 830, val loss: 1.3949297666549683
Epoch 840, training loss: 0.010252617299556732 = 0.0035407654941082 + 0.001 * 6.711852073669434
Epoch 840, val loss: 1.401970624923706
Epoch 850, training loss: 0.010128887370228767 = 0.0034228109288960695 + 0.001 * 6.706075668334961
Epoch 850, val loss: 1.4088581800460815
Epoch 860, training loss: 0.010038823820650578 = 0.0033112873788923025 + 0.001 * 6.727535724639893
Epoch 860, val loss: 1.4155691862106323
Epoch 870, training loss: 0.009934298694133759 = 0.003205728018656373 + 0.001 * 6.728570461273193
Epoch 870, val loss: 1.4222017526626587
Epoch 880, training loss: 0.009806773625314236 = 0.0031056629959493876 + 0.001 * 6.701109886169434
Epoch 880, val loss: 1.4286750555038452
Epoch 890, training loss: 0.009732409380376339 = 0.0030107584316283464 + 0.001 * 6.721651077270508
Epoch 890, val loss: 1.4350032806396484
Epoch 900, training loss: 0.00963602028787136 = 0.002920634113252163 + 0.001 * 6.715385437011719
Epoch 900, val loss: 1.4412256479263306
Epoch 910, training loss: 0.009546104818582535 = 0.002834973856806755 + 0.001 * 6.711130142211914
Epoch 910, val loss: 1.4473133087158203
Epoch 920, training loss: 0.009452852420508862 = 0.002753483597189188 + 0.001 * 6.699368476867676
Epoch 920, val loss: 1.4532947540283203
Epoch 930, training loss: 0.009370565414428711 = 0.002675909548997879 + 0.001 * 6.694654941558838
Epoch 930, val loss: 1.4591304063796997
Epoch 940, training loss: 0.009289950132369995 = 0.0026019872166216373 + 0.001 * 6.687963008880615
Epoch 940, val loss: 1.4648652076721191
Epoch 950, training loss: 0.009219758212566376 = 0.002531516831368208 + 0.001 * 6.688241481781006
Epoch 950, val loss: 1.4704937934875488
Epoch 960, training loss: 0.00916006974875927 = 0.0024643123615533113 + 0.001 * 6.6957573890686035
Epoch 960, val loss: 1.4759819507598877
Epoch 970, training loss: 0.009099796414375305 = 0.00240011396817863 + 0.001 * 6.699681758880615
Epoch 970, val loss: 1.4813802242279053
Epoch 980, training loss: 0.00905090756714344 = 0.0023387419059872627 + 0.001 * 6.712164878845215
Epoch 980, val loss: 1.4866796731948853
Epoch 990, training loss: 0.008990979753434658 = 0.0022800567094236612 + 0.001 * 6.710922718048096
Epoch 990, val loss: 1.4919337034225464
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6863
Flip ASR: 0.6356/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9609415531158447 = 1.9525678157806396 + 0.001 * 8.37375259399414
Epoch 0, val loss: 1.94599449634552
Epoch 10, training loss: 1.9504907131195068 = 1.9421169757843018 + 0.001 * 8.373696327209473
Epoch 10, val loss: 1.9360578060150146
Epoch 20, training loss: 1.9379394054412842 = 1.9295659065246582 + 0.001 * 8.373441696166992
Epoch 20, val loss: 1.9237598180770874
Epoch 30, training loss: 1.9207595586776733 = 1.9123866558074951 + 0.001 * 8.372942924499512
Epoch 30, val loss: 1.9066171646118164
Epoch 40, training loss: 1.8956652879714966 = 1.8872933387756348 + 0.001 * 8.37192440032959
Epoch 40, val loss: 1.8816919326782227
Epoch 50, training loss: 1.8598113059997559 = 1.8514419794082642 + 0.001 * 8.369346618652344
Epoch 50, val loss: 1.8475440740585327
Epoch 60, training loss: 1.8163971900939941 = 1.808037281036377 + 0.001 * 8.359859466552734
Epoch 60, val loss: 1.8100157976150513
Epoch 70, training loss: 1.7754713296890259 = 1.7671650648117065 + 0.001 * 8.306280136108398
Epoch 70, val loss: 1.7781057357788086
Epoch 80, training loss: 1.7285469770431519 = 1.7206660509109497 + 0.001 * 7.8809051513671875
Epoch 80, val loss: 1.7381690740585327
Epoch 90, training loss: 1.6637684106826782 = 1.6562020778656006 + 0.001 * 7.56638240814209
Epoch 90, val loss: 1.6817808151245117
Epoch 100, training loss: 1.5778844356536865 = 1.5705223083496094 + 0.001 * 7.362077713012695
Epoch 100, val loss: 1.6091830730438232
Epoch 110, training loss: 1.4746904373168945 = 1.467504620552063 + 0.001 * 7.1857757568359375
Epoch 110, val loss: 1.5239425897598267
Epoch 120, training loss: 1.3651373386383057 = 1.3580307960510254 + 0.001 * 7.106593608856201
Epoch 120, val loss: 1.4356739521026611
Epoch 130, training loss: 1.2549749612808228 = 1.247928261756897 + 0.001 * 7.046736717224121
Epoch 130, val loss: 1.3497884273529053
Epoch 140, training loss: 1.145658016204834 = 1.1386603116989136 + 0.001 * 6.997727394104004
Epoch 140, val loss: 1.2659443616867065
Epoch 150, training loss: 1.0399831533432007 = 1.03300142288208 + 0.001 * 6.981764793395996
Epoch 150, val loss: 1.1861985921859741
Epoch 160, training loss: 0.941437840461731 = 0.9344685077667236 + 0.001 * 6.969352722167969
Epoch 160, val loss: 1.113324522972107
Epoch 170, training loss: 0.8518517017364502 = 0.8448935747146606 + 0.001 * 6.958106994628906
Epoch 170, val loss: 1.0489665269851685
Epoch 180, training loss: 0.771409273147583 = 0.7644644379615784 + 0.001 * 6.944807529449463
Epoch 180, val loss: 0.9930349588394165
Epoch 190, training loss: 0.6996974349021912 = 0.6927695274353027 + 0.001 * 6.927899360656738
Epoch 190, val loss: 0.9455270767211914
Epoch 200, training loss: 0.6359474062919617 = 0.629041314125061 + 0.001 * 6.906111240386963
Epoch 200, val loss: 0.9058750867843628
Epoch 210, training loss: 0.579069197177887 = 0.5721896290779114 + 0.001 * 6.8795390129089355
Epoch 210, val loss: 0.8731400966644287
Epoch 220, training loss: 0.5277824401855469 = 0.5209272503852844 + 0.001 * 6.855216026306152
Epoch 220, val loss: 0.8462998867034912
Epoch 230, training loss: 0.4807486832141876 = 0.47390517592430115 + 0.001 * 6.8434929847717285
Epoch 230, val loss: 0.8241589665412903
Epoch 240, training loss: 0.43664664030075073 = 0.42981165647506714 + 0.001 * 6.834986209869385
Epoch 240, val loss: 0.8058967590332031
Epoch 250, training loss: 0.3942605257034302 = 0.38743025064468384 + 0.001 * 6.83028507232666
Epoch 250, val loss: 0.790734589099884
Epoch 260, training loss: 0.3528805673122406 = 0.3460550606250763 + 0.001 * 6.825516223907471
Epoch 260, val loss: 0.7780141234397888
Epoch 270, training loss: 0.3125474154949188 = 0.30572694540023804 + 0.001 * 6.820474624633789
Epoch 270, val loss: 0.7673661112785339
Epoch 280, training loss: 0.2738408148288727 = 0.2670268416404724 + 0.001 * 6.813969135284424
Epoch 280, val loss: 0.7590478658676147
Epoch 290, training loss: 0.23764905333518982 = 0.23084159195423126 + 0.001 * 6.8074631690979
Epoch 290, val loss: 0.753341555595398
Epoch 300, training loss: 0.20492202043533325 = 0.19812320172786713 + 0.001 * 6.798813343048096
Epoch 300, val loss: 0.7507643699645996
Epoch 310, training loss: 0.17639580368995667 = 0.16960056126117706 + 0.001 * 6.795244216918945
Epoch 310, val loss: 0.7515456676483154
Epoch 320, training loss: 0.1522773802280426 = 0.14549563825130463 + 0.001 * 6.781748294830322
Epoch 320, val loss: 0.7554845809936523
Epoch 330, training loss: 0.1322840005159378 = 0.12550845742225647 + 0.001 * 6.775547027587891
Epoch 330, val loss: 0.7618727684020996
Epoch 340, training loss: 0.11577657610177994 = 0.10900934040546417 + 0.001 * 6.7672319412231445
Epoch 340, val loss: 0.7701910138130188
Epoch 350, training loss: 0.10207469761371613 = 0.09530455619096756 + 0.001 * 6.770140171051025
Epoch 350, val loss: 0.7799922823905945
Epoch 360, training loss: 0.09055649489164352 = 0.08379772305488586 + 0.001 * 6.7587738037109375
Epoch 360, val loss: 0.7910721302032471
Epoch 370, training loss: 0.08078797161579132 = 0.07402964681386948 + 0.001 * 6.758324146270752
Epoch 370, val loss: 0.8030922412872314
Epoch 380, training loss: 0.07242558896541595 = 0.0656701847910881 + 0.001 * 6.755401134490967
Epoch 380, val loss: 0.8158885836601257
Epoch 390, training loss: 0.06522399932146072 = 0.058469969779253006 + 0.001 * 6.754028797149658
Epoch 390, val loss: 0.8293673396110535
Epoch 400, training loss: 0.05900256335735321 = 0.05224727839231491 + 0.001 * 6.755286693572998
Epoch 400, val loss: 0.8433915972709656
Epoch 410, training loss: 0.05360853672027588 = 0.04685654118657112 + 0.001 * 6.751994609832764
Epoch 410, val loss: 0.8577131032943726
Epoch 420, training loss: 0.04892847687005997 = 0.042175546288490295 + 0.001 * 6.752931118011475
Epoch 420, val loss: 0.8722038269042969
Epoch 430, training loss: 0.04485250264406204 = 0.03810030594468117 + 0.001 * 6.752195358276367
Epoch 430, val loss: 0.8867026567459106
Epoch 440, training loss: 0.04129564017057419 = 0.034543655812740326 + 0.001 * 6.751985549926758
Epoch 440, val loss: 0.9011261463165283
Epoch 450, training loss: 0.038181621581315994 = 0.03143048286437988 + 0.001 * 6.7511396408081055
Epoch 450, val loss: 0.9153681993484497
Epoch 460, training loss: 0.03544612228870392 = 0.02869594283401966 + 0.001 * 6.750181198120117
Epoch 460, val loss: 0.929328441619873
Epoch 470, training loss: 0.03303663805127144 = 0.026285521686077118 + 0.001 * 6.751116752624512
Epoch 470, val loss: 0.9430183172225952
Epoch 480, training loss: 0.030902501195669174 = 0.02415328100323677 + 0.001 * 6.749220371246338
Epoch 480, val loss: 0.9563873410224915
Epoch 490, training loss: 0.029008284211158752 = 0.022260131314396858 + 0.001 * 6.748153209686279
Epoch 490, val loss: 0.9694383144378662
Epoch 500, training loss: 0.02732248231768608 = 0.020573066547513008 + 0.001 * 6.749415874481201
Epoch 500, val loss: 0.9821820259094238
Epoch 510, training loss: 0.025814879685640335 = 0.019064534455537796 + 0.001 * 6.750345706939697
Epoch 510, val loss: 0.9945782423019409
Epoch 520, training loss: 0.024457357823848724 = 0.0177114550024271 + 0.001 * 6.745902061462402
Epoch 520, val loss: 1.0066618919372559
Epoch 530, training loss: 0.023242736235260963 = 0.01649405248463154 + 0.001 * 6.748682975769043
Epoch 530, val loss: 1.018439531326294
Epoch 540, training loss: 0.0221438966691494 = 0.015395646914839745 + 0.001 * 6.748249053955078
Epoch 540, val loss: 1.029909372329712
Epoch 550, training loss: 0.021145258098840714 = 0.014401920139789581 + 0.001 * 6.743337631225586
Epoch 550, val loss: 1.0410836935043335
Epoch 560, training loss: 0.020243238657712936 = 0.013500383123755455 + 0.001 * 6.742854595184326
Epoch 560, val loss: 1.0519907474517822
Epoch 570, training loss: 0.019425639882683754 = 0.012680650688707829 + 0.001 * 6.744988441467285
Epoch 570, val loss: 1.062608242034912
Epoch 580, training loss: 0.01867472007870674 = 0.011933570727705956 + 0.001 * 6.741149425506592
Epoch 580, val loss: 1.0729542970657349
Epoch 590, training loss: 0.0179955642670393 = 0.01125116366893053 + 0.001 * 6.744400501251221
Epoch 590, val loss: 1.0830202102661133
Epoch 600, training loss: 0.017368346452713013 = 0.010626368224620819 + 0.001 * 6.741977691650391
Epoch 600, val loss: 1.0928387641906738
Epoch 610, training loss: 0.016791364178061485 = 0.0100531792268157 + 0.001 * 6.738183975219727
Epoch 610, val loss: 1.1024130582809448
Epoch 620, training loss: 0.0162692591547966 = 0.009526274167001247 + 0.001 * 6.742985725402832
Epoch 620, val loss: 1.1117357015609741
Epoch 630, training loss: 0.01577760837972164 = 0.009040940552949905 + 0.001 * 6.736667633056641
Epoch 630, val loss: 1.120823860168457
Epoch 640, training loss: 0.015332438051700592 = 0.008593066595494747 + 0.001 * 6.7393717765808105
Epoch 640, val loss: 1.1296716928482056
Epoch 650, training loss: 0.014916270039975643 = 0.00817896332591772 + 0.001 * 6.737306594848633
Epoch 650, val loss: 1.1383106708526611
Epoch 660, training loss: 0.014530864544212818 = 0.007795407902449369 + 0.001 * 6.735456466674805
Epoch 660, val loss: 1.1467320919036865
Epoch 670, training loss: 0.014171617105603218 = 0.007439541630446911 + 0.001 * 6.7320756912231445
Epoch 670, val loss: 1.1549495458602905
Epoch 680, training loss: 0.013839472085237503 = 0.007108818739652634 + 0.001 * 6.730653285980225
Epoch 680, val loss: 1.1629606485366821
Epoch 690, training loss: 0.013540133833885193 = 0.006800961215049028 + 0.001 * 6.739172458648682
Epoch 690, val loss: 1.1707847118377686
Epoch 700, training loss: 0.01324434019625187 = 0.006513950414955616 + 0.001 * 6.7303900718688965
Epoch 700, val loss: 1.1784343719482422
Epoch 710, training loss: 0.01297525130212307 = 0.0062459795735776424 + 0.001 * 6.729271411895752
Epoch 710, val loss: 1.1858842372894287
Epoch 720, training loss: 0.01272374577820301 = 0.005995424464344978 + 0.001 * 6.728321552276611
Epoch 720, val loss: 1.1931706666946411
Epoch 730, training loss: 0.012490569613873959 = 0.005760836880654097 + 0.001 * 6.729732513427734
Epoch 730, val loss: 1.200277328491211
Epoch 740, training loss: 0.012266217730939388 = 0.005540869198739529 + 0.001 * 6.725347995758057
Epoch 740, val loss: 1.207226276397705
Epoch 750, training loss: 0.012067211791872978 = 0.00533435121178627 + 0.001 * 6.7328596115112305
Epoch 750, val loss: 1.2140061855316162
Epoch 760, training loss: 0.011866094544529915 = 0.005140223540365696 + 0.001 * 6.725870132446289
Epoch 760, val loss: 1.2206354141235352
Epoch 770, training loss: 0.01168113574385643 = 0.004957528319209814 + 0.001 * 6.723607540130615
Epoch 770, val loss: 1.2271300554275513
Epoch 780, training loss: 0.011508280411362648 = 0.004785351455211639 + 0.001 * 6.722929000854492
Epoch 780, val loss: 1.2334685325622559
Epoch 790, training loss: 0.011341946199536324 = 0.004622926935553551 + 0.001 * 6.719018459320068
Epoch 790, val loss: 1.239664912223816
Epoch 800, training loss: 0.011212186887860298 = 0.004469534382224083 + 0.001 * 6.742652416229248
Epoch 800, val loss: 1.2457334995269775
Epoch 810, training loss: 0.011046267114579678 = 0.0043245102278888226 + 0.001 * 6.721756458282471
Epoch 810, val loss: 1.2516652345657349
Epoch 820, training loss: 0.010904250666499138 = 0.004187282640486956 + 0.001 * 6.716967582702637
Epoch 820, val loss: 1.2574788331985474
Epoch 830, training loss: 0.01077280193567276 = 0.004057282581925392 + 0.001 * 6.715519428253174
Epoch 830, val loss: 1.2631641626358032
Epoch 840, training loss: 0.010667618364095688 = 0.003934011794626713 + 0.001 * 6.733606815338135
Epoch 840, val loss: 1.2687249183654785
Epoch 850, training loss: 0.010534025728702545 = 0.0038170202169567347 + 0.001 * 6.717005252838135
Epoch 850, val loss: 1.2741756439208984
Epoch 860, training loss: 0.010419514961540699 = 0.0037058766465634108 + 0.001 * 6.713637828826904
Epoch 860, val loss: 1.27950918674469
Epoch 870, training loss: 0.01031545177102089 = 0.0036001959815621376 + 0.001 * 6.7152557373046875
Epoch 870, val loss: 1.2847480773925781
Epoch 880, training loss: 0.01022653840482235 = 0.0034996429458260536 + 0.001 * 6.726895809173584
Epoch 880, val loss: 1.2898625135421753
Epoch 890, training loss: 0.010118413716554642 = 0.00340388179756701 + 0.001 * 6.714531898498535
Epoch 890, val loss: 1.2948808670043945
Epoch 900, training loss: 0.010021972469985485 = 0.003312604269012809 + 0.001 * 6.709367752075195
Epoch 900, val loss: 1.299797534942627
Epoch 910, training loss: 0.009931989014148712 = 0.00322553189471364 + 0.001 * 6.706457138061523
Epoch 910, val loss: 1.3046272993087769
Epoch 920, training loss: 0.009859207086265087 = 0.003142418572679162 + 0.001 * 6.716787815093994
Epoch 920, val loss: 1.3093570470809937
Epoch 930, training loss: 0.009772112593054771 = 0.003063025651499629 + 0.001 * 6.709086894989014
Epoch 930, val loss: 1.3139933347702026
Epoch 940, training loss: 0.009694404900074005 = 0.002987134037539363 + 0.001 * 6.707270622253418
Epoch 940, val loss: 1.318545937538147
Epoch 950, training loss: 0.00961602944880724 = 0.002914539072662592 + 0.001 * 6.7014899253845215
Epoch 950, val loss: 1.3230172395706177
Epoch 960, training loss: 0.009546228684484959 = 0.002845049137249589 + 0.001 * 6.701179504394531
Epoch 960, val loss: 1.3274093866348267
Epoch 970, training loss: 0.009492890909314156 = 0.002778491238132119 + 0.001 * 6.714399814605713
Epoch 970, val loss: 1.331721305847168
Epoch 980, training loss: 0.009420463815331459 = 0.002714703558012843 + 0.001 * 6.705759525299072
Epoch 980, val loss: 1.3359465599060059
Epoch 990, training loss: 0.009374524466693401 = 0.0026535417418926954 + 0.001 * 6.720982551574707
Epoch 990, val loss: 1.3400896787643433
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
The final ASR:0.71710, 0.08566, Accuracy:0.81728, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11654])
remove edge: torch.Size([2, 9506])
updated graph: torch.Size([2, 10604])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83333, 0.00302
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9662593603134155 = 1.957885503768921 + 0.001 * 8.373882293701172
Epoch 0, val loss: 1.9579172134399414
Epoch 10, training loss: 1.9557170867919922 = 1.9473432302474976 + 0.001 * 8.373806953430176
Epoch 10, val loss: 1.9481397867202759
Epoch 20, training loss: 1.9425034523010254 = 1.9341298341751099 + 0.001 * 8.373617172241211
Epoch 20, val loss: 1.9355188608169556
Epoch 30, training loss: 1.923876404762268 = 1.9155031442642212 + 0.001 * 8.373236656188965
Epoch 30, val loss: 1.9174811840057373
Epoch 40, training loss: 1.8963253498077393 = 1.8879529237747192 + 0.001 * 8.372467041015625
Epoch 40, val loss: 1.891005039215088
Epoch 50, training loss: 1.8574246168136597 = 1.849053978919983 + 0.001 * 8.370680809020996
Epoch 50, val loss: 1.855223298072815
Epoch 60, training loss: 1.8123738765716553 = 1.8040087223052979 + 0.001 * 8.365160942077637
Epoch 60, val loss: 1.8177564144134521
Epoch 70, training loss: 1.7728990316390991 = 1.7645593881607056 + 0.001 * 8.339681625366211
Epoch 70, val loss: 1.7871472835540771
Epoch 80, training loss: 1.7266405820846558 = 1.718475341796875 + 0.001 * 8.165214538574219
Epoch 80, val loss: 1.7462141513824463
Epoch 90, training loss: 1.6624923944473267 = 1.6546247005462646 + 0.001 * 7.867647647857666
Epoch 90, val loss: 1.6911704540252686
Epoch 100, training loss: 1.5790998935699463 = 1.5713914632797241 + 0.001 * 7.708411693572998
Epoch 100, val loss: 1.6234363317489624
Epoch 110, training loss: 1.4838380813598633 = 1.476312518119812 + 0.001 * 7.52553653717041
Epoch 110, val loss: 1.5476374626159668
Epoch 120, training loss: 1.3884183168411255 = 1.3810003995895386 + 0.001 * 7.417940616607666
Epoch 120, val loss: 1.4723271131515503
Epoch 130, training loss: 1.2950114011764526 = 1.2876147031784058 + 0.001 * 7.396749496459961
Epoch 130, val loss: 1.4006037712097168
Epoch 140, training loss: 1.2028971910476685 = 1.1955540180206299 + 0.001 * 7.3431620597839355
Epoch 140, val loss: 1.3310801982879639
Epoch 150, training loss: 1.1129282712936401 = 1.1056610345840454 + 0.001 * 7.2672553062438965
Epoch 150, val loss: 1.2631101608276367
Epoch 160, training loss: 1.0273619890213013 = 1.020194172859192 + 0.001 * 7.167862415313721
Epoch 160, val loss: 1.1992002725601196
Epoch 170, training loss: 0.9474570751190186 = 0.9403602480888367 + 0.001 * 7.096809387207031
Epoch 170, val loss: 1.1407182216644287
Epoch 180, training loss: 0.871940553188324 = 0.8648686408996582 + 0.001 * 7.0719075202941895
Epoch 180, val loss: 1.0864015817642212
Epoch 190, training loss: 0.7987438440322876 = 0.7916856408119202 + 0.001 * 7.058230876922607
Epoch 190, val loss: 1.0357332229614258
Epoch 200, training loss: 0.7274161577224731 = 0.7203654050827026 + 0.001 * 7.050765514373779
Epoch 200, val loss: 0.9884518980979919
Epoch 210, training loss: 0.658886730670929 = 0.6518396735191345 + 0.001 * 7.0470428466796875
Epoch 210, val loss: 0.9455839395523071
Epoch 220, training loss: 0.5940220355987549 = 0.5869772434234619 + 0.001 * 7.044788360595703
Epoch 220, val loss: 0.9079551696777344
Epoch 230, training loss: 0.5328361988067627 = 0.5257936120033264 + 0.001 * 7.042590141296387
Epoch 230, val loss: 0.8761132955551147
Epoch 240, training loss: 0.4750138223171234 = 0.4679734408855438 + 0.001 * 7.040386199951172
Epoch 240, val loss: 0.8492608666419983
Epoch 250, training loss: 0.42085641622543335 = 0.4138182997703552 + 0.001 * 7.038129806518555
Epoch 250, val loss: 0.8275808095932007
Epoch 260, training loss: 0.3712676167488098 = 0.36423179507255554 + 0.001 * 7.0358076095581055
Epoch 260, val loss: 0.8121780753135681
Epoch 270, training loss: 0.32686811685562134 = 0.3198344111442566 + 0.001 * 7.033695220947266
Epoch 270, val loss: 0.8034393787384033
Epoch 280, training loss: 0.28739091753959656 = 0.28035929799079895 + 0.001 * 7.031615257263184
Epoch 280, val loss: 0.8010448813438416
Epoch 290, training loss: 0.2518705129623413 = 0.24484048783779144 + 0.001 * 7.030020236968994
Epoch 290, val loss: 0.8037479519844055
Epoch 300, training loss: 0.21937675774097443 = 0.2123485803604126 + 0.001 * 7.028183460235596
Epoch 300, val loss: 0.8104010224342346
Epoch 310, training loss: 0.18968838453292847 = 0.18266186118125916 + 0.001 * 7.026515483856201
Epoch 310, val loss: 0.820088803768158
Epoch 320, training loss: 0.16314205527305603 = 0.15611696243286133 + 0.001 * 7.025088310241699
Epoch 320, val loss: 0.8324955105781555
Epoch 330, training loss: 0.1401454508304596 = 0.13312162458896637 + 0.001 * 7.023822784423828
Epoch 330, val loss: 0.8472721576690674
Epoch 340, training loss: 0.12076477706432343 = 0.11374173313379288 + 0.001 * 7.023042678833008
Epoch 340, val loss: 0.8640915155410767
Epoch 350, training loss: 0.10470777750015259 = 0.09768592566251755 + 0.001 * 7.021853446960449
Epoch 350, val loss: 0.8824875950813293
Epoch 360, training loss: 0.09151449799537659 = 0.08449394255876541 + 0.001 * 7.020557880401611
Epoch 360, val loss: 0.9019749760627747
Epoch 370, training loss: 0.08068279176950455 = 0.07366383075714111 + 0.001 * 7.01896333694458
Epoch 370, val loss: 0.9218719601631165
Epoch 380, training loss: 0.07175026834011078 = 0.06473232805728912 + 0.001 * 7.017942905426025
Epoch 380, val loss: 0.9418914318084717
Epoch 390, training loss: 0.06433133780956268 = 0.057315584272146225 + 0.001 * 7.015754699707031
Epoch 390, val loss: 0.9616588354110718
Epoch 400, training loss: 0.058111995458602905 = 0.05109880492091179 + 0.001 * 7.013190269470215
Epoch 400, val loss: 0.9810512065887451
Epoch 410, training loss: 0.05284992605447769 = 0.04583834856748581 + 0.001 * 7.011575698852539
Epoch 410, val loss: 0.9999276399612427
Epoch 420, training loss: 0.04834971949458122 = 0.041344135999679565 + 0.001 * 7.005582332611084
Epoch 420, val loss: 1.0183566808700562
Epoch 430, training loss: 0.04447381943464279 = 0.037468310445547104 + 0.001 * 7.005508899688721
Epoch 430, val loss: 1.0363421440124512
Epoch 440, training loss: 0.0410974845290184 = 0.03409864753484726 + 0.001 * 6.998837471008301
Epoch 440, val loss: 1.0537755489349365
Epoch 450, training loss: 0.03814011067152023 = 0.03114980459213257 + 0.001 * 6.990304470062256
Epoch 450, val loss: 1.0706690549850464
Epoch 460, training loss: 0.03553645685315132 = 0.028553245589137077 + 0.001 * 6.983210563659668
Epoch 460, val loss: 1.0872269868850708
Epoch 470, training loss: 0.033238351345062256 = 0.026255227625370026 + 0.001 * 6.983124732971191
Epoch 470, val loss: 1.103494644165039
Epoch 480, training loss: 0.031185800209641457 = 0.0242126677185297 + 0.001 * 6.973132610321045
Epoch 480, val loss: 1.1193020343780518
Epoch 490, training loss: 0.029353352263569832 = 0.02239082381129265 + 0.001 * 6.962528228759766
Epoch 490, val loss: 1.1347013711929321
Epoch 500, training loss: 0.027722716331481934 = 0.020760249346494675 + 0.001 * 6.962467193603516
Epoch 500, val loss: 1.149790644645691
Epoch 510, training loss: 0.026260826736688614 = 0.019294817000627518 + 0.001 * 6.966009616851807
Epoch 510, val loss: 1.1645723581314087
Epoch 520, training loss: 0.024938717484474182 = 0.017974412068724632 + 0.001 * 6.964304447174072
Epoch 520, val loss: 1.178918480873108
Epoch 530, training loss: 0.02373378723859787 = 0.01678195409476757 + 0.001 * 6.951831817626953
Epoch 530, val loss: 1.1930216550827026
Epoch 540, training loss: 0.022648466750979424 = 0.01570240594446659 + 0.001 * 6.946061134338379
Epoch 540, val loss: 1.206878662109375
Epoch 550, training loss: 0.021683841943740845 = 0.014722251333296299 + 0.001 * 6.961589813232422
Epoch 550, val loss: 1.2203119993209839
Epoch 560, training loss: 0.020772259682416916 = 0.013830076903104782 + 0.001 * 6.942181587219238
Epoch 560, val loss: 1.23348867893219
Epoch 570, training loss: 0.01995779201388359 = 0.013015915639698505 + 0.001 * 6.941874980926514
Epoch 570, val loss: 1.246269702911377
Epoch 580, training loss: 0.019220734015107155 = 0.012270164676010609 + 0.001 * 6.950569152832031
Epoch 580, val loss: 1.2588684558868408
Epoch 590, training loss: 0.01852792501449585 = 0.011586412787437439 + 0.001 * 6.941512584686279
Epoch 590, val loss: 1.2710192203521729
Epoch 600, training loss: 0.01789739914238453 = 0.010958247818052769 + 0.001 * 6.939150810241699
Epoch 600, val loss: 1.2828630208969116
Epoch 610, training loss: 0.017318813130259514 = 0.010380223393440247 + 0.001 * 6.938589572906494
Epoch 610, val loss: 1.294460654258728
Epoch 620, training loss: 0.01677878201007843 = 0.00984703004360199 + 0.001 * 6.931752681732178
Epoch 620, val loss: 1.3058009147644043
Epoch 630, training loss: 0.016300378367304802 = 0.009354260750114918 + 0.001 * 6.946117401123047
Epoch 630, val loss: 1.3168671131134033
Epoch 640, training loss: 0.01582844741642475 = 0.00889820046722889 + 0.001 * 6.9302473068237305
Epoch 640, val loss: 1.3276792764663696
Epoch 650, training loss: 0.015407191589474678 = 0.008475336246192455 + 0.001 * 6.931854724884033
Epoch 650, val loss: 1.3381562232971191
Epoch 660, training loss: 0.01501484401524067 = 0.008082317188382149 + 0.001 * 6.9325270652771
Epoch 660, val loss: 1.348435878753662
Epoch 670, training loss: 0.014642925933003426 = 0.007716147229075432 + 0.001 * 6.926778793334961
Epoch 670, val loss: 1.3584017753601074
Epoch 680, training loss: 0.014313314110040665 = 0.007374290376901627 + 0.001 * 6.939023971557617
Epoch 680, val loss: 1.368230938911438
Epoch 690, training loss: 0.013983186334371567 = 0.007055549882352352 + 0.001 * 6.927636623382568
Epoch 690, val loss: 1.3777705430984497
Epoch 700, training loss: 0.013689065352082253 = 0.006757708266377449 + 0.001 * 6.931356906890869
Epoch 700, val loss: 1.3871604204177856
Epoch 710, training loss: 0.013402868062257767 = 0.006479018367826939 + 0.001 * 6.923849105834961
Epoch 710, val loss: 1.396308183670044
Epoch 720, training loss: 0.01314140111207962 = 0.006218042224645615 + 0.001 * 6.923358917236328
Epoch 720, val loss: 1.4052374362945557
Epoch 730, training loss: 0.012891617603600025 = 0.005973306018859148 + 0.001 * 6.91831111907959
Epoch 730, val loss: 1.4139336347579956
Epoch 740, training loss: 0.012668127194046974 = 0.005743563175201416 + 0.001 * 6.924563407897949
Epoch 740, val loss: 1.4225153923034668
Epoch 750, training loss: 0.01244572177529335 = 0.005527593195438385 + 0.001 * 6.918128490447998
Epoch 750, val loss: 1.4308507442474365
Epoch 760, training loss: 0.012243332341313362 = 0.005324224475771189 + 0.001 * 6.919107913970947
Epoch 760, val loss: 1.4390217065811157
Epoch 770, training loss: 0.012054424732923508 = 0.005132409278303385 + 0.001 * 6.9220147132873535
Epoch 770, val loss: 1.4469950199127197
Epoch 780, training loss: 0.011860436759889126 = 0.00495123490691185 + 0.001 * 6.909201622009277
Epoch 780, val loss: 1.454796552658081
Epoch 790, training loss: 0.011707990430295467 = 0.004780085291713476 + 0.001 * 6.9279046058654785
Epoch 790, val loss: 1.4624290466308594
Epoch 800, training loss: 0.011528273113071918 = 0.004618167877197266 + 0.001 * 6.910104751586914
Epoch 800, val loss: 1.4699151515960693
Epoch 810, training loss: 0.01137564331293106 = 0.004464976489543915 + 0.001 * 6.910665988922119
Epoch 810, val loss: 1.4771958589553833
Epoch 820, training loss: 0.011228224262595177 = 0.004319796338677406 + 0.001 * 6.9084272384643555
Epoch 820, val loss: 1.4843980073928833
Epoch 830, training loss: 0.0111114252358675 = 0.00418171426281333 + 0.001 * 6.929710865020752
Epoch 830, val loss: 1.4913289546966553
Epoch 840, training loss: 0.010966237634420395 = 0.004051299765706062 + 0.001 * 6.9149370193481445
Epoch 840, val loss: 1.4982410669326782
Epoch 850, training loss: 0.010835996828973293 = 0.003927466459572315 + 0.001 * 6.908530235290527
Epoch 850, val loss: 1.5050336122512817
Epoch 860, training loss: 0.010713179595768452 = 0.0038098241202533245 + 0.001 * 6.903355121612549
Epoch 860, val loss: 1.511583924293518
Epoch 870, training loss: 0.010604048147797585 = 0.003698006272315979 + 0.001 * 6.906042098999023
Epoch 870, val loss: 1.5180282592773438
Epoch 880, training loss: 0.010512081906199455 = 0.0035916557535529137 + 0.001 * 6.920426368713379
Epoch 880, val loss: 1.5243570804595947
Epoch 890, training loss: 0.010396525263786316 = 0.003490434493869543 + 0.001 * 6.90609073638916
Epoch 890, val loss: 1.530606746673584
Epoch 900, training loss: 0.010294638574123383 = 0.003394021186977625 + 0.001 * 6.900616645812988
Epoch 900, val loss: 1.5366765260696411
Epoch 910, training loss: 0.010200857184827328 = 0.003302121302112937 + 0.001 * 6.898735523223877
Epoch 910, val loss: 1.5426064729690552
Epoch 920, training loss: 0.010101480409502983 = 0.003214429598301649 + 0.001 * 6.887050151824951
Epoch 920, val loss: 1.5484733581542969
Epoch 930, training loss: 0.0100173931568861 = 0.003130705561488867 + 0.001 * 6.8866868019104
Epoch 930, val loss: 1.554202675819397
Epoch 940, training loss: 0.009948395192623138 = 0.003050720551982522 + 0.001 * 6.897674560546875
Epoch 940, val loss: 1.5598430633544922
Epoch 950, training loss: 0.009862484410405159 = 0.002974169561639428 + 0.001 * 6.888314723968506
Epoch 950, val loss: 1.565339207649231
Epoch 960, training loss: 0.009793831035494804 = 0.0029009422287344933 + 0.001 * 6.892888069152832
Epoch 960, val loss: 1.5707757472991943
Epoch 970, training loss: 0.009707042947411537 = 0.0028308401815593243 + 0.001 * 6.87620210647583
Epoch 970, val loss: 1.5761035680770874
Epoch 980, training loss: 0.009652791544795036 = 0.0027636925224214792 + 0.001 * 6.88909912109375
Epoch 980, val loss: 1.5813077688217163
Epoch 990, training loss: 0.009577041491866112 = 0.0026993523351848125 + 0.001 * 6.877688407897949
Epoch 990, val loss: 1.5863819122314453
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6458
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9578369855880737 = 1.9494633674621582 + 0.001 * 8.37366008758545
Epoch 0, val loss: 1.940054178237915
Epoch 10, training loss: 1.9459826946258545 = 1.9376095533370972 + 0.001 * 8.373106956481934
Epoch 10, val loss: 1.926590085029602
Epoch 20, training loss: 1.9314929246902466 = 1.9231207370758057 + 0.001 * 8.372179985046387
Epoch 20, val loss: 1.9087580442428589
Epoch 30, training loss: 1.9124513864517212 = 1.9040802717208862 + 0.001 * 8.371079444885254
Epoch 30, val loss: 1.885060429573059
Epoch 40, training loss: 1.887853741645813 = 1.879483938217163 + 0.001 * 8.369843482971191
Epoch 40, val loss: 1.85665762424469
Epoch 50, training loss: 1.8556898832321167 = 1.8473221063613892 + 0.001 * 8.367817878723145
Epoch 50, val loss: 1.8237495422363281
Epoch 60, training loss: 1.8152885437011719 = 1.8069264888763428 + 0.001 * 8.362008094787598
Epoch 60, val loss: 1.7872101068496704
Epoch 70, training loss: 1.7720686197280884 = 1.7637360095977783 + 0.001 * 8.332625389099121
Epoch 70, val loss: 1.7522070407867432
Epoch 80, training loss: 1.724586009979248 = 1.7164853811264038 + 0.001 * 8.100682258605957
Epoch 80, val loss: 1.7121294736862183
Epoch 90, training loss: 1.6589070558547974 = 1.6511024236679077 + 0.001 * 7.804618835449219
Epoch 90, val loss: 1.6535050868988037
Epoch 100, training loss: 1.5704214572906494 = 1.5627738237380981 + 0.001 * 7.647674560546875
Epoch 100, val loss: 1.5777562856674194
Epoch 110, training loss: 1.4652774333953857 = 1.4578489065170288 + 0.001 * 7.428580284118652
Epoch 110, val loss: 1.4915521144866943
Epoch 120, training loss: 1.3603551387786865 = 1.3530006408691406 + 0.001 * 7.354515075683594
Epoch 120, val loss: 1.4086369276046753
Epoch 130, training loss: 1.2655402421951294 = 1.2582207918167114 + 0.001 * 7.319452285766602
Epoch 130, val loss: 1.3381208181381226
Epoch 140, training loss: 1.1808160543441772 = 1.1735152006149292 + 0.001 * 7.300826072692871
Epoch 140, val loss: 1.2798686027526855
Epoch 150, training loss: 1.1006484031677246 = 1.0933791399002075 + 0.001 * 7.269275188446045
Epoch 150, val loss: 1.2267295122146606
Epoch 160, training loss: 1.0194674730300903 = 1.0122454166412354 + 0.001 * 7.222066879272461
Epoch 160, val loss: 1.1722372770309448
Epoch 170, training loss: 0.9358450770378113 = 0.9286777377128601 + 0.001 * 7.167344570159912
Epoch 170, val loss: 1.1150752305984497
Epoch 180, training loss: 0.8528364896774292 = 0.8456992506980896 + 0.001 * 7.137232303619385
Epoch 180, val loss: 1.0585956573486328
Epoch 190, training loss: 0.7748035788536072 = 0.7676880359649658 + 0.001 * 7.115523815155029
Epoch 190, val loss: 1.0069268941879272
Epoch 200, training loss: 0.704154372215271 = 0.6970610022544861 + 0.001 * 7.093394756317139
Epoch 200, val loss: 0.9626348614692688
Epoch 210, training loss: 0.6405900120735168 = 0.6335229873657227 + 0.001 * 7.067048072814941
Epoch 210, val loss: 0.9258397221565247
Epoch 220, training loss: 0.582270085811615 = 0.5752275586128235 + 0.001 * 7.042535781860352
Epoch 220, val loss: 0.8954655528068542
Epoch 230, training loss: 0.5276511311531067 = 0.5206310153007507 + 0.001 * 7.020127773284912
Epoch 230, val loss: 0.8710895776748657
Epoch 240, training loss: 0.47625890374183655 = 0.469251811504364 + 0.001 * 7.007099151611328
Epoch 240, val loss: 0.8528231382369995
Epoch 250, training loss: 0.42856600880622864 = 0.421573281288147 + 0.001 * 6.992727756500244
Epoch 250, val loss: 0.8409876227378845
Epoch 260, training loss: 0.3853277266025543 = 0.3783419132232666 + 0.001 * 6.985805511474609
Epoch 260, val loss: 0.8352030515670776
Epoch 270, training loss: 0.34678950905799866 = 0.33981144428253174 + 0.001 * 6.978056907653809
Epoch 270, val loss: 0.8346766233444214
Epoch 280, training loss: 0.3125396966934204 = 0.3055686056613922 + 0.001 * 6.971080780029297
Epoch 280, val loss: 0.8384847044944763
Epoch 290, training loss: 0.28193503618240356 = 0.2749696969985962 + 0.001 * 6.965333461761475
Epoch 290, val loss: 0.8455580472946167
Epoch 300, training loss: 0.25436803698539734 = 0.24741095304489136 + 0.001 * 6.957070827484131
Epoch 300, val loss: 0.8550013303756714
Epoch 310, training loss: 0.2293693572282791 = 0.22241830825805664 + 0.001 * 6.95104455947876
Epoch 310, val loss: 0.8665663003921509
Epoch 320, training loss: 0.20667892694473267 = 0.19973136484622955 + 0.001 * 6.947554111480713
Epoch 320, val loss: 0.8799106478691101
Epoch 330, training loss: 0.18610775470733643 = 0.17916320264339447 + 0.001 * 6.9445481300354
Epoch 330, val loss: 0.8948045969009399
Epoch 340, training loss: 0.16754144430160522 = 0.16060221195220947 + 0.001 * 6.939236640930176
Epoch 340, val loss: 0.9112256765365601
Epoch 350, training loss: 0.151027649641037 = 0.1440931260585785 + 0.001 * 6.934529781341553
Epoch 350, val loss: 0.9280151128768921
Epoch 360, training loss: 0.13633354008197784 = 0.1294037103652954 + 0.001 * 6.929830551147461
Epoch 360, val loss: 0.9452171921730042
Epoch 370, training loss: 0.12324914336204529 = 0.1162976548075676 + 0.001 * 6.951488018035889
Epoch 370, val loss: 0.9649176597595215
Epoch 380, training loss: 0.1115245372056961 = 0.1045989841222763 + 0.001 * 6.925552845001221
Epoch 380, val loss: 0.9856426119804382
Epoch 390, training loss: 0.10113374143838882 = 0.09421242028474808 + 0.001 * 6.92132043838501
Epoch 390, val loss: 1.0065921545028687
Epoch 400, training loss: 0.0917501300573349 = 0.08482951670885086 + 0.001 * 6.92061185836792
Epoch 400, val loss: 1.02897047996521
Epoch 410, training loss: 0.08337181061506271 = 0.07645450532436371 + 0.001 * 6.917303085327148
Epoch 410, val loss: 1.0515307188034058
Epoch 420, training loss: 0.0757572278380394 = 0.06882677972316742 + 0.001 * 6.930450916290283
Epoch 420, val loss: 1.0744529962539673
Epoch 430, training loss: 0.06879529356956482 = 0.06188087910413742 + 0.001 * 6.9144158363342285
Epoch 430, val loss: 1.0983725786209106
Epoch 440, training loss: 0.06247858703136444 = 0.055565737187862396 + 0.001 * 6.9128499031066895
Epoch 440, val loss: 1.1223914623260498
Epoch 450, training loss: 0.056976672261953354 = 0.050065778195858 + 0.001 * 6.910892486572266
Epoch 450, val loss: 1.1463704109191895
Epoch 460, training loss: 0.052146825939416885 = 0.04523653909564018 + 0.001 * 6.910286903381348
Epoch 460, val loss: 1.17059326171875
Epoch 470, training loss: 0.0478719063103199 = 0.04096107557415962 + 0.001 * 6.910831928253174
Epoch 470, val loss: 1.1950336694717407
Epoch 480, training loss: 0.044139645993709564 = 0.03722694516181946 + 0.001 * 6.912700176239014
Epoch 480, val loss: 1.2190767526626587
Epoch 490, training loss: 0.040857572108507156 = 0.03395329415798187 + 0.001 * 6.904276371002197
Epoch 490, val loss: 1.2426632642745972
Epoch 500, training loss: 0.037923503667116165 = 0.031014813110232353 + 0.001 * 6.908689498901367
Epoch 500, val loss: 1.2661187648773193
Epoch 510, training loss: 0.035317450761795044 = 0.028415508568286896 + 0.001 * 6.90194034576416
Epoch 510, val loss: 1.289355993270874
Epoch 520, training loss: 0.03299439325928688 = 0.02609223686158657 + 0.001 * 6.902156829833984
Epoch 520, val loss: 1.3122838735580444
Epoch 530, training loss: 0.030894899740815163 = 0.02400040440261364 + 0.001 * 6.894495010375977
Epoch 530, val loss: 1.3347834348678589
Epoch 540, training loss: 0.02901705726981163 = 0.022116543725132942 + 0.001 * 6.900513172149658
Epoch 540, val loss: 1.3569109439849854
Epoch 550, training loss: 0.027331259101629257 = 0.02043859288096428 + 0.001 * 6.892664909362793
Epoch 550, val loss: 1.378533959388733
Epoch 560, training loss: 0.025811243802309036 = 0.018922239542007446 + 0.001 * 6.889004230499268
Epoch 560, val loss: 1.3998351097106934
Epoch 570, training loss: 0.024448640644550323 = 0.017558705061674118 + 0.001 * 6.889935493469238
Epoch 570, val loss: 1.4205572605133057
Epoch 580, training loss: 0.02321956306695938 = 0.016325190663337708 + 0.001 * 6.894371509552002
Epoch 580, val loss: 1.4408607482910156
Epoch 590, training loss: 0.022085659205913544 = 0.015202761627733707 + 0.001 * 6.882898330688477
Epoch 590, val loss: 1.4608298540115356
Epoch 600, training loss: 0.021058987826108932 = 0.01417857687920332 + 0.001 * 6.880411148071289
Epoch 600, val loss: 1.4801831245422363
Epoch 610, training loss: 0.02013523504137993 = 0.013245000503957272 + 0.001 * 6.890233516693115
Epoch 610, val loss: 1.4992140531539917
Epoch 620, training loss: 0.01927782967686653 = 0.01239260658621788 + 0.001 * 6.885223388671875
Epoch 620, val loss: 1.5177005529403687
Epoch 630, training loss: 0.018495794385671616 = 0.011610768735408783 + 0.001 * 6.885025978088379
Epoch 630, val loss: 1.5357853174209595
Epoch 640, training loss: 0.017777379602193832 = 0.010897217318415642 + 0.001 * 6.880162715911865
Epoch 640, val loss: 1.5535094738006592
Epoch 650, training loss: 0.017126573249697685 = 0.01024394016712904 + 0.001 * 6.882632732391357
Epoch 650, val loss: 1.5707035064697266
Epoch 660, training loss: 0.016528461128473282 = 0.009644991718232632 + 0.001 * 6.883470058441162
Epoch 660, val loss: 1.5875147581100464
Epoch 670, training loss: 0.01596936210989952 = 0.009094891138374805 + 0.001 * 6.874469757080078
Epoch 670, val loss: 1.6038434505462646
Epoch 680, training loss: 0.015451806597411633 = 0.008588741533458233 + 0.001 * 6.863064765930176
Epoch 680, val loss: 1.6197625398635864
Epoch 690, training loss: 0.014983663335442543 = 0.008122210390865803 + 0.001 * 6.861453056335449
Epoch 690, val loss: 1.6352964639663696
Epoch 700, training loss: 0.014566700905561447 = 0.007691882085055113 + 0.001 * 6.874818325042725
Epoch 700, val loss: 1.6504461765289307
Epoch 710, training loss: 0.014160784892737865 = 0.007294757291674614 + 0.001 * 6.866027355194092
Epoch 710, val loss: 1.665188193321228
Epoch 720, training loss: 0.013785144314169884 = 0.006927733309566975 + 0.001 * 6.857410907745361
Epoch 720, val loss: 1.6795116662979126
Epoch 730, training loss: 0.013444261625409126 = 0.00658770278096199 + 0.001 * 6.856558322906494
Epoch 730, val loss: 1.693535327911377
Epoch 740, training loss: 0.013127253390848637 = 0.006272198166698217 + 0.001 * 6.85505485534668
Epoch 740, val loss: 1.7071181535720825
Epoch 750, training loss: 0.012827941216528416 = 0.005979330278933048 + 0.001 * 6.8486104011535645
Epoch 750, val loss: 1.7203962802886963
Epoch 760, training loss: 0.012559114955365658 = 0.0057068997994065285 + 0.001 * 6.852214813232422
Epoch 760, val loss: 1.7333437204360962
Epoch 770, training loss: 0.012318168766796589 = 0.0054527772590518 + 0.001 * 6.865391254425049
Epoch 770, val loss: 1.7459957599639893
Epoch 780, training loss: 0.012059422209858894 = 0.005215019453316927 + 0.001 * 6.844402313232422
Epoch 780, val loss: 1.7583531141281128
Epoch 790, training loss: 0.011847969144582748 = 0.004993103910237551 + 0.001 * 6.854865550994873
Epoch 790, val loss: 1.7704057693481445
Epoch 800, training loss: 0.011635817587375641 = 0.0047858646139502525 + 0.001 * 6.8499531745910645
Epoch 800, val loss: 1.782122254371643
Epoch 810, training loss: 0.011431983672082424 = 0.004592239856719971 + 0.001 * 6.839743614196777
Epoch 810, val loss: 1.7936244010925293
Epoch 820, training loss: 0.011248137801885605 = 0.004410385154187679 + 0.001 * 6.837752819061279
Epoch 820, val loss: 1.8047806024551392
Epoch 830, training loss: 0.01107744686305523 = 0.004239450208842754 + 0.001 * 6.837996959686279
Epoch 830, val loss: 1.8156993389129639
Epoch 840, training loss: 0.010914772748947144 = 0.004078480880707502 + 0.001 * 6.836291790008545
Epoch 840, val loss: 1.8264294862747192
Epoch 850, training loss: 0.010762747377157211 = 0.003926651086658239 + 0.001 * 6.836095809936523
Epoch 850, val loss: 1.8368546962738037
Epoch 860, training loss: 0.010616805404424667 = 0.0037831447552889585 + 0.001 * 6.833660125732422
Epoch 860, val loss: 1.8470379114151
Epoch 870, training loss: 0.010482823476195335 = 0.0036473015788942575 + 0.001 * 6.835521697998047
Epoch 870, val loss: 1.8569906949996948
Epoch 880, training loss: 0.010352796874940395 = 0.003518381854519248 + 0.001 * 6.834414958953857
Epoch 880, val loss: 1.8666832447052002
Epoch 890, training loss: 0.010218758136034012 = 0.003396146697923541 + 0.001 * 6.822611331939697
Epoch 890, val loss: 1.8761999607086182
Epoch 900, training loss: 0.010107161477208138 = 0.0032802110072225332 + 0.001 * 6.8269500732421875
Epoch 900, val loss: 1.8854273557662964
Epoch 910, training loss: 0.010011584497988224 = 0.003170208539813757 + 0.001 * 6.841375827789307
Epoch 910, val loss: 1.894392490386963
Epoch 920, training loss: 0.009906742721796036 = 0.0030658056493848562 + 0.001 * 6.84093713760376
Epoch 920, val loss: 1.9031462669372559
Epoch 930, training loss: 0.00979006476700306 = 0.0029665869660675526 + 0.001 * 6.823477268218994
Epoch 930, val loss: 1.911682367324829
Epoch 940, training loss: 0.009685730561614037 = 0.002872220939025283 + 0.001 * 6.813508987426758
Epoch 940, val loss: 1.919984221458435
Epoch 950, training loss: 0.009596172720193863 = 0.002782433293759823 + 0.001 * 6.813739776611328
Epoch 950, val loss: 1.928080439567566
Epoch 960, training loss: 0.00951000489294529 = 0.002696885261684656 + 0.001 * 6.813119888305664
Epoch 960, val loss: 1.9359625577926636
Epoch 970, training loss: 0.009426316246390343 = 0.0026155435480177402 + 0.001 * 6.810771942138672
Epoch 970, val loss: 1.9436408281326294
Epoch 980, training loss: 0.009353004395961761 = 0.0025385343469679356 + 0.001 * 6.814470291137695
Epoch 980, val loss: 1.9510984420776367
Epoch 990, training loss: 0.009278908371925354 = 0.0024651680141687393 + 0.001 * 6.8137407302856445
Epoch 990, val loss: 1.9583231210708618
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.7823
Flip ASR: 0.7467/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.953049898147583 = 1.9446760416030884 + 0.001 * 8.373817443847656
Epoch 0, val loss: 1.9389917850494385
Epoch 10, training loss: 1.9428319931030273 = 1.9344582557678223 + 0.001 * 8.373734474182129
Epoch 10, val loss: 1.929372787475586
Epoch 20, training loss: 1.9300227165222168 = 1.9216492176055908 + 0.001 * 8.373481750488281
Epoch 20, val loss: 1.9168481826782227
Epoch 30, training loss: 1.9116272926330566 = 1.9032543897628784 + 0.001 * 8.372961044311523
Epoch 30, val loss: 1.8984463214874268
Epoch 40, training loss: 1.8840261697769165 = 1.8756543397903442 + 0.001 * 8.371784210205078
Epoch 40, val loss: 1.8709771633148193
Epoch 50, training loss: 1.8454928398132324 = 1.8371243476867676 + 0.001 * 8.368514060974121
Epoch 50, val loss: 1.8343753814697266
Epoch 60, training loss: 1.8030670881271362 = 1.794710636138916 + 0.001 * 8.356404304504395
Epoch 60, val loss: 1.798294186592102
Epoch 70, training loss: 1.7654080390930176 = 1.7571160793304443 + 0.001 * 8.291972160339355
Epoch 70, val loss: 1.7681949138641357
Epoch 80, training loss: 1.7159152030944824 = 1.7079627513885498 + 0.001 * 7.952484130859375
Epoch 80, val loss: 1.725884199142456
Epoch 90, training loss: 1.6484683752059937 = 1.6406939029693604 + 0.001 * 7.774415016174316
Epoch 90, val loss: 1.6698322296142578
Epoch 100, training loss: 1.563676118850708 = 1.5559895038604736 + 0.001 * 7.686646461486816
Epoch 100, val loss: 1.6010764837265015
Epoch 110, training loss: 1.4721753597259521 = 1.4646185636520386 + 0.001 * 7.5567851066589355
Epoch 110, val loss: 1.5266218185424805
Epoch 120, training loss: 1.3826221227645874 = 1.375312089920044 + 0.001 * 7.309973239898682
Epoch 120, val loss: 1.455992341041565
Epoch 130, training loss: 1.2940462827682495 = 1.2868043184280396 + 0.001 * 7.2419753074646
Epoch 130, val loss: 1.3857803344726562
Epoch 140, training loss: 1.2012079954147339 = 1.1940209865570068 + 0.001 * 7.1869988441467285
Epoch 140, val loss: 1.313010811805725
Epoch 150, training loss: 1.1016496419906616 = 1.094515085220337 + 0.001 * 7.134541988372803
Epoch 150, val loss: 1.2356857061386108
Epoch 160, training loss: 0.9969903826713562 = 0.9898879528045654 + 0.001 * 7.102452278137207
Epoch 160, val loss: 1.155667781829834
Epoch 170, training loss: 0.8927006125450134 = 0.885613203048706 + 0.001 * 7.087408065795898
Epoch 170, val loss: 1.077231526374817
Epoch 180, training loss: 0.7958809733390808 = 0.7888031005859375 + 0.001 * 7.07784366607666
Epoch 180, val loss: 1.0059386491775513
Epoch 190, training loss: 0.7110931873321533 = 0.7040222883224487 + 0.001 * 7.070908546447754
Epoch 190, val loss: 0.9452441334724426
Epoch 200, training loss: 0.6381974220275879 = 0.6311326622962952 + 0.001 * 7.064768314361572
Epoch 200, val loss: 0.8952100276947021
Epoch 210, training loss: 0.5744171738624573 = 0.5673599243164062 + 0.001 * 7.057267665863037
Epoch 210, val loss: 0.853812575340271
Epoch 220, training loss: 0.5172579288482666 = 0.5102121829986572 + 0.001 * 7.045755863189697
Epoch 220, val loss: 0.8191705346107483
Epoch 230, training loss: 0.46523427963256836 = 0.45820721983909607 + 0.001 * 7.027053356170654
Epoch 230, val loss: 0.790724515914917
Epoch 240, training loss: 0.4173799455165863 = 0.4103822112083435 + 0.001 * 6.997725009918213
Epoch 240, val loss: 0.7680553793907166
Epoch 250, training loss: 0.3730291724205017 = 0.36605724692344666 + 0.001 * 6.971937656402588
Epoch 250, val loss: 0.7503310441970825
Epoch 260, training loss: 0.33177751302719116 = 0.3248238265514374 + 0.001 * 6.953696250915527
Epoch 260, val loss: 0.7362139225006104
Epoch 270, training loss: 0.2935371398925781 = 0.28659355640411377 + 0.001 * 6.94358491897583
Epoch 270, val loss: 0.7247841954231262
Epoch 280, training loss: 0.2583302855491638 = 0.25139114260673523 + 0.001 * 6.9391398429870605
Epoch 280, val loss: 0.715844988822937
Epoch 290, training loss: 0.22633932530879974 = 0.21940279006958008 + 0.001 * 6.936532974243164
Epoch 290, val loss: 0.7092954516410828
Epoch 300, training loss: 0.19776087999343872 = 0.1908281445503235 + 0.001 * 6.932733535766602
Epoch 300, val loss: 0.70501708984375
Epoch 310, training loss: 0.17266225814819336 = 0.16573145985603333 + 0.001 * 6.930791854858398
Epoch 310, val loss: 0.7030025124549866
Epoch 320, training loss: 0.1509017050266266 = 0.1439739614725113 + 0.001 * 6.927745819091797
Epoch 320, val loss: 0.7032157182693481
Epoch 330, training loss: 0.13218218088150024 = 0.12525609135627747 + 0.001 * 6.926089763641357
Epoch 330, val loss: 0.7054692506790161
Epoch 340, training loss: 0.11614757031202316 = 0.10922269523143768 + 0.001 * 6.924875259399414
Epoch 340, val loss: 0.7095054388046265
Epoch 350, training loss: 0.10241758078336716 = 0.09549517929553986 + 0.001 * 6.92240047454834
Epoch 350, val loss: 0.7151147127151489
Epoch 360, training loss: 0.09064093977212906 = 0.08372073620557785 + 0.001 * 6.9202046394348145
Epoch 360, val loss: 0.7221121191978455
Epoch 370, training loss: 0.08050982654094696 = 0.0735916867852211 + 0.001 * 6.918135643005371
Epoch 370, val loss: 0.7301746010780334
Epoch 380, training loss: 0.07177029550075531 = 0.0648532435297966 + 0.001 * 6.917054653167725
Epoch 380, val loss: 0.7390770316123962
Epoch 390, training loss: 0.06422053277492523 = 0.05730529502034187 + 0.001 * 6.915240287780762
Epoch 390, val loss: 0.7486956119537354
Epoch 400, training loss: 0.05769503116607666 = 0.050782300531864166 + 0.001 * 6.9127302169799805
Epoch 400, val loss: 0.7586941123008728
Epoch 410, training loss: 0.052055567502975464 = 0.045146241784095764 + 0.001 * 6.90932559967041
Epoch 410, val loss: 0.7688848376274109
Epoch 420, training loss: 0.04718927666544914 = 0.040276408195495605 + 0.001 * 6.912867069244385
Epoch 420, val loss: 0.7791903614997864
Epoch 430, training loss: 0.04297082871198654 = 0.03606600686907768 + 0.001 * 6.904821872711182
Epoch 430, val loss: 0.7894974946975708
Epoch 440, training loss: 0.0393187589943409 = 0.032420456409454346 + 0.001 * 6.898301124572754
Epoch 440, val loss: 0.7997299432754517
Epoch 450, training loss: 0.036150768399238586 = 0.029258035123348236 + 0.001 * 6.892733097076416
Epoch 450, val loss: 0.8098352551460266
Epoch 460, training loss: 0.03341173753142357 = 0.02650766260921955 + 0.001 * 6.904074192047119
Epoch 460, val loss: 0.8197692036628723
Epoch 470, training loss: 0.03100232034921646 = 0.024108590558171272 + 0.001 * 6.893730163574219
Epoch 470, val loss: 0.8295199871063232
Epoch 480, training loss: 0.028888605535030365 = 0.022008996456861496 + 0.001 * 6.879609107971191
Epoch 480, val loss: 0.8390641212463379
Epoch 490, training loss: 0.027041291818022728 = 0.020164474844932556 + 0.001 * 6.876817226409912
Epoch 490, val loss: 0.8483965396881104
Epoch 500, training loss: 0.025419382378458977 = 0.018537919968366623 + 0.001 * 6.881462097167969
Epoch 500, val loss: 0.8575068712234497
Epoch 510, training loss: 0.023971106857061386 = 0.01709837093949318 + 0.001 * 6.872735500335693
Epoch 510, val loss: 0.8663936853408813
Epoch 520, training loss: 0.02269277535378933 = 0.01581941358745098 + 0.001 * 6.873361110687256
Epoch 520, val loss: 0.8750624656677246
Epoch 530, training loss: 0.021535072475671768 = 0.014678962528705597 + 0.001 * 6.856110095977783
Epoch 530, val loss: 0.8835070133209229
Epoch 540, training loss: 0.020510125905275345 = 0.013658448122441769 + 0.001 * 6.851677894592285
Epoch 540, val loss: 0.8917321562767029
Epoch 550, training loss: 0.019589725881814957 = 0.012742034159600735 + 0.001 * 6.847692012786865
Epoch 550, val loss: 0.8997474312782288
Epoch 560, training loss: 0.018763206899166107 = 0.0119163254275918 + 0.001 * 6.846881866455078
Epoch 560, val loss: 0.9075452089309692
Epoch 570, training loss: 0.018011847510933876 = 0.011170213110744953 + 0.001 * 6.8416337966918945
Epoch 570, val loss: 0.9151344299316406
Epoch 580, training loss: 0.01734161004424095 = 0.010494078509509563 + 0.001 * 6.847532272338867
Epoch 580, val loss: 0.922523558139801
Epoch 590, training loss: 0.016717635095119476 = 0.009879456833004951 + 0.001 * 6.838178634643555
Epoch 590, val loss: 0.9297316670417786
Epoch 600, training loss: 0.016196738928556442 = 0.009319179691374302 + 0.001 * 6.877559185028076
Epoch 600, val loss: 0.9367596507072449
Epoch 610, training loss: 0.01565108262002468 = 0.008807267993688583 + 0.001 * 6.843814849853516
Epoch 610, val loss: 0.9435918927192688
Epoch 620, training loss: 0.015179824084043503 = 0.008338266983628273 + 0.001 * 6.841556549072266
Epoch 620, val loss: 0.9502512812614441
Epoch 630, training loss: 0.014739170670509338 = 0.007907507941126823 + 0.001 * 6.831662654876709
Epoch 630, val loss: 0.9567526578903198
Epoch 640, training loss: 0.01434616930782795 = 0.007511029951274395 + 0.001 * 6.835139274597168
Epoch 640, val loss: 0.9630988240242004
Epoch 650, training loss: 0.013981780037283897 = 0.007145335432142019 + 0.001 * 6.836444854736328
Epoch 650, val loss: 0.9692878723144531
Epoch 660, training loss: 0.013640357181429863 = 0.006807340309023857 + 0.001 * 6.833016395568848
Epoch 660, val loss: 0.9753389954566956
Epoch 670, training loss: 0.013322124257683754 = 0.006494299508631229 + 0.001 * 6.827824115753174
Epoch 670, val loss: 0.981249988079071
Epoch 680, training loss: 0.013037975877523422 = 0.006203838158398867 + 0.001 * 6.834137439727783
Epoch 680, val loss: 0.9870185256004333
Epoch 690, training loss: 0.012758666649460793 = 0.00593393063172698 + 0.001 * 6.82473611831665
Epoch 690, val loss: 0.9926360845565796
Epoch 700, training loss: 0.012502903118729591 = 0.005682623479515314 + 0.001 * 6.820279598236084
Epoch 700, val loss: 0.9981451034545898
Epoch 710, training loss: 0.012268060818314552 = 0.005448244046419859 + 0.001 * 6.8198161125183105
Epoch 710, val loss: 1.003516435623169
Epoch 720, training loss: 0.012073817662894726 = 0.005229283589869738 + 0.001 * 6.844533920288086
Epoch 720, val loss: 1.0087676048278809
Epoch 730, training loss: 0.011848567053675652 = 0.005024453159421682 + 0.001 * 6.824113845825195
Epoch 730, val loss: 1.0138965845108032
Epoch 740, training loss: 0.0116538405418396 = 0.004832541570067406 + 0.001 * 6.821298599243164
Epoch 740, val loss: 1.0189080238342285
Epoch 750, training loss: 0.011477296240627766 = 0.0046525076031684875 + 0.001 * 6.8247880935668945
Epoch 750, val loss: 1.0238133668899536
Epoch 760, training loss: 0.011308261193335056 = 0.004483377560973167 + 0.001 * 6.824883460998535
Epoch 760, val loss: 1.028620719909668
Epoch 770, training loss: 0.01113479770720005 = 0.004324285313487053 + 0.001 * 6.810512542724609
Epoch 770, val loss: 1.0333209037780762
Epoch 780, training loss: 0.010985730215907097 = 0.004174424335360527 + 0.001 * 6.811305522918701
Epoch 780, val loss: 1.0379363298416138
Epoch 790, training loss: 0.010841412469744682 = 0.004033141303807497 + 0.001 * 6.8082709312438965
Epoch 790, val loss: 1.042450189590454
Epoch 800, training loss: 0.010706772096455097 = 0.0038997619412839413 + 0.001 * 6.807009696960449
Epoch 800, val loss: 1.0468748807907104
Epoch 810, training loss: 0.010596095584332943 = 0.0037737046368420124 + 0.001 * 6.822390556335449
Epoch 810, val loss: 1.0512040853500366
Epoch 820, training loss: 0.010474956594407558 = 0.0036544562317430973 + 0.001 * 6.820499897003174
Epoch 820, val loss: 1.0554535388946533
Epoch 830, training loss: 0.01035146601498127 = 0.0035415245220065117 + 0.001 * 6.809940814971924
Epoch 830, val loss: 1.059600591659546
Epoch 840, training loss: 0.010234499350190163 = 0.003434463171288371 + 0.001 * 6.8000359535217285
Epoch 840, val loss: 1.063687801361084
Epoch 850, training loss: 0.010137560777366161 = 0.003332882421091199 + 0.001 * 6.804677963256836
Epoch 850, val loss: 1.0676814317703247
Epoch 860, training loss: 0.010036651976406574 = 0.003236411139369011 + 0.001 * 6.800240516662598
Epoch 860, val loss: 1.0716044902801514
Epoch 870, training loss: 0.009951981715857983 = 0.003144707763567567 + 0.001 * 6.8072733879089355
Epoch 870, val loss: 1.0754520893096924
Epoch 880, training loss: 0.009860928170382977 = 0.0030574670527130365 + 0.001 * 6.803460597991943
Epoch 880, val loss: 1.079210638999939
Epoch 890, training loss: 0.009769153781235218 = 0.002974391682073474 + 0.001 * 6.794761657714844
Epoch 890, val loss: 1.082921028137207
Epoch 900, training loss: 0.009702677838504314 = 0.0028952236752957106 + 0.001 * 6.8074541091918945
Epoch 900, val loss: 1.0865720510482788
Epoch 910, training loss: 0.009623263962566853 = 0.002819750225171447 + 0.001 * 6.803513050079346
Epoch 910, val loss: 1.0901448726654053
Epoch 920, training loss: 0.009543984197080135 = 0.0027477056719362736 + 0.001 * 6.79627799987793
Epoch 920, val loss: 1.0936741828918457
Epoch 930, training loss: 0.009476513601839542 = 0.0026789105031639338 + 0.001 * 6.797603130340576
Epoch 930, val loss: 1.0971325635910034
Epoch 940, training loss: 0.00941531639546156 = 0.002613138873130083 + 0.001 * 6.802177429199219
Epoch 940, val loss: 1.1005398035049438
Epoch 950, training loss: 0.009344872087240219 = 0.002550245262682438 + 0.001 * 6.794626712799072
Epoch 950, val loss: 1.1038480997085571
Epoch 960, training loss: 0.00928183551877737 = 0.0024900573771446943 + 0.001 * 6.791778087615967
Epoch 960, val loss: 1.1071300506591797
Epoch 970, training loss: 0.009228810667991638 = 0.002432415960356593 + 0.001 * 6.796394348144531
Epoch 970, val loss: 1.1103435754776
Epoch 980, training loss: 0.009164132177829742 = 0.0023771619889885187 + 0.001 * 6.7869696617126465
Epoch 980, val loss: 1.113492727279663
Epoch 990, training loss: 0.009107997640967369 = 0.0023241788148880005 + 0.001 * 6.78381872177124
Epoch 990, val loss: 1.1166057586669922
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9188
Flip ASR: 0.9022/225 nodes
The final ASR:0.78229, 0.11148, Accuracy:0.80247, 0.02310
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11560])
remove edge: torch.Size([2, 9528])
updated graph: torch.Size([2, 10532])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83580, 0.00698
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9525649547576904 = 1.9441910982131958 + 0.001 * 8.373879432678223
Epoch 0, val loss: 1.9473752975463867
Epoch 10, training loss: 1.9427462816238403 = 1.9343724250793457 + 0.001 * 8.373815536499023
Epoch 10, val loss: 1.9376304149627686
Epoch 20, training loss: 1.9309452772140503 = 1.9225716590881348 + 0.001 * 8.373615264892578
Epoch 20, val loss: 1.925785779953003
Epoch 30, training loss: 1.9146524667739868 = 1.9062793254852295 + 0.001 * 8.373187065124512
Epoch 30, val loss: 1.9095540046691895
Epoch 40, training loss: 1.8908246755599976 = 1.882452368736267 + 0.001 * 8.372262954711914
Epoch 40, val loss: 1.8861913681030273
Epoch 50, training loss: 1.8569605350494385 = 1.8485907316207886 + 0.001 * 8.36978816986084
Epoch 50, val loss: 1.854295015335083
Epoch 60, training loss: 1.815484642982483 = 1.8071233034133911 + 0.001 * 8.361348152160645
Epoch 60, val loss: 1.818355679512024
Epoch 70, training loss: 1.7750773429870605 = 1.7667527198791504 + 0.001 * 8.32459831237793
Epoch 70, val loss: 1.7860478162765503
Epoch 80, training loss: 1.7288641929626465 = 1.7207648754119873 + 0.001 * 8.099282264709473
Epoch 80, val loss: 1.7464593648910522
Epoch 90, training loss: 1.6647346019744873 = 1.6568197011947632 + 0.001 * 7.914895057678223
Epoch 90, val loss: 1.6916190385818481
Epoch 100, training loss: 1.5801585912704468 = 1.5723915100097656 + 0.001 * 7.767138957977295
Epoch 100, val loss: 1.6216888427734375
Epoch 110, training loss: 1.4809648990631104 = 1.4734188318252563 + 0.001 * 7.546074390411377
Epoch 110, val loss: 1.54362952709198
Epoch 120, training loss: 1.3791877031326294 = 1.3718321323394775 + 0.001 * 7.3556108474731445
Epoch 120, val loss: 1.4634528160095215
Epoch 130, training loss: 1.2795865535736084 = 1.2723146677017212 + 0.001 * 7.271928310394287
Epoch 130, val loss: 1.385640263557434
Epoch 140, training loss: 1.1816428899765015 = 1.1744556427001953 + 0.001 * 7.1872334480285645
Epoch 140, val loss: 1.3093550205230713
Epoch 150, training loss: 1.0875799655914307 = 1.0804320573806763 + 0.001 * 7.147906303405762
Epoch 150, val loss: 1.2360293865203857
Epoch 160, training loss: 1.0009143352508545 = 0.9937888383865356 + 0.001 * 7.125524520874023
Epoch 160, val loss: 1.1689951419830322
Epoch 170, training loss: 0.9221259951591492 = 0.915024995803833 + 0.001 * 7.1010050773620605
Epoch 170, val loss: 1.1096045970916748
Epoch 180, training loss: 0.8480679392814636 = 0.8409896492958069 + 0.001 * 7.078271865844727
Epoch 180, val loss: 1.0554653406143188
Epoch 190, training loss: 0.7752096652984619 = 0.7681458592414856 + 0.001 * 7.06378698348999
Epoch 190, val loss: 1.0032848119735718
Epoch 200, training loss: 0.7019569277763367 = 0.6949003338813782 + 0.001 * 7.056564807891846
Epoch 200, val loss: 0.9516633152961731
Epoch 210, training loss: 0.6291842460632324 = 0.6221309304237366 + 0.001 * 7.053287029266357
Epoch 210, val loss: 0.9009041786193848
Epoch 220, training loss: 0.5592063665390015 = 0.5521557331085205 + 0.001 * 7.050629138946533
Epoch 220, val loss: 0.8533186316490173
Epoch 230, training loss: 0.49440446496009827 = 0.48735684156417847 + 0.001 * 7.04763650894165
Epoch 230, val loss: 0.8120518326759338
Epoch 240, training loss: 0.4359697699546814 = 0.42892614006996155 + 0.001 * 7.043637752532959
Epoch 240, val loss: 0.7792955040931702
Epoch 250, training loss: 0.3839244842529297 = 0.37688595056533813 + 0.001 * 7.038536071777344
Epoch 250, val loss: 0.7551557421684265
Epoch 260, training loss: 0.33782070875167847 = 0.3307885229587555 + 0.001 * 7.032192707061768
Epoch 260, val loss: 0.7384797930717468
Epoch 270, training loss: 0.2971762716770172 = 0.2901488244533539 + 0.001 * 7.027449607849121
Epoch 270, val loss: 0.7278754711151123
Epoch 280, training loss: 0.26138800382614136 = 0.25437232851982117 + 0.001 * 7.015688896179199
Epoch 280, val loss: 0.7221189737319946
Epoch 290, training loss: 0.22970272600650787 = 0.22268831729888916 + 0.001 * 7.014413356781006
Epoch 290, val loss: 0.7197704911231995
Epoch 300, training loss: 0.20144620537757874 = 0.19445419311523438 + 0.001 * 6.992011070251465
Epoch 300, val loss: 0.7199940085411072
Epoch 310, training loss: 0.17631635069847107 = 0.1693355143070221 + 0.001 * 6.980833530426025
Epoch 310, val loss: 0.7224062085151672
Epoch 320, training loss: 0.15419521927833557 = 0.14721821248531342 + 0.001 * 6.976999282836914
Epoch 320, val loss: 0.7268683910369873
Epoch 330, training loss: 0.1349947601556778 = 0.12803427875041962 + 0.001 * 6.9604811668396
Epoch 330, val loss: 0.7333674430847168
Epoch 340, training loss: 0.11858350783586502 = 0.11163078248500824 + 0.001 * 6.952723503112793
Epoch 340, val loss: 0.7419577240943909
Epoch 350, training loss: 0.1046927198767662 = 0.0977388545870781 + 0.001 * 6.953862190246582
Epoch 350, val loss: 0.7524140477180481
Epoch 360, training loss: 0.09296469390392303 = 0.08602054417133331 + 0.001 * 6.944149494171143
Epoch 360, val loss: 0.7643023729324341
Epoch 370, training loss: 0.08305558562278748 = 0.07610703259706497 + 0.001 * 6.948551654815674
Epoch 370, val loss: 0.7773305177688599
Epoch 380, training loss: 0.07461287081241608 = 0.06767241656780243 + 0.001 * 6.940452575683594
Epoch 380, val loss: 0.7912798523902893
Epoch 390, training loss: 0.06737831979990005 = 0.060443151742219925 + 0.001 * 6.935170650482178
Epoch 390, val loss: 0.8058347702026367
Epoch 400, training loss: 0.061130985617637634 = 0.054201286286115646 + 0.001 * 6.929699420928955
Epoch 400, val loss: 0.8208432793617249
Epoch 410, training loss: 0.0557110570371151 = 0.048782095313072205 + 0.001 * 6.928959846496582
Epoch 410, val loss: 0.8359832763671875
Epoch 420, training loss: 0.050979480147361755 = 0.04405585303902626 + 0.001 * 6.923628330230713
Epoch 420, val loss: 0.8512887358665466
Epoch 430, training loss: 0.046839285641908646 = 0.03991962969303131 + 0.001 * 6.919656276702881
Epoch 430, val loss: 0.8665891885757446
Epoch 440, training loss: 0.04321771860122681 = 0.0362873412668705 + 0.001 * 6.930378437042236
Epoch 440, val loss: 0.8818026185035706
Epoch 450, training loss: 0.040007583796978 = 0.03308524563908577 + 0.001 * 6.922336101531982
Epoch 450, val loss: 0.8967629075050354
Epoch 460, training loss: 0.03716351464390755 = 0.03024682216346264 + 0.001 * 6.916691780090332
Epoch 460, val loss: 0.9115891456604004
Epoch 470, training loss: 0.03463459014892578 = 0.027718545868992805 + 0.001 * 6.916044235229492
Epoch 470, val loss: 0.9262242913246155
Epoch 480, training loss: 0.03237630054354668 = 0.025458896532654762 + 0.001 * 6.917402744293213
Epoch 480, val loss: 0.9405375123023987
Epoch 490, training loss: 0.030340157449245453 = 0.023431692272424698 + 0.001 * 6.908465385437012
Epoch 490, val loss: 0.9549791216850281
Epoch 500, training loss: 0.028522396460175514 = 0.021608496084809303 + 0.001 * 6.913900375366211
Epoch 500, val loss: 0.9690455794334412
Epoch 510, training loss: 0.026877112686634064 = 0.019965779036283493 + 0.001 * 6.911332607269287
Epoch 510, val loss: 0.9829275608062744
Epoch 520, training loss: 0.025390854105353355 = 0.018484828993678093 + 0.001 * 6.90602445602417
Epoch 520, val loss: 0.9966681599617004
Epoch 530, training loss: 0.024062221869826317 = 0.017147913575172424 + 0.001 * 6.914308547973633
Epoch 530, val loss: 1.0101253986358643
Epoch 540, training loss: 0.022848788648843765 = 0.015939587727189064 + 0.001 * 6.909200668334961
Epoch 540, val loss: 1.0233091115951538
Epoch 550, training loss: 0.021743902936577797 = 0.01484591979533434 + 0.001 * 6.897983074188232
Epoch 550, val loss: 1.0362974405288696
Epoch 560, training loss: 0.020776741206645966 = 0.013854735530912876 + 0.001 * 6.922004699707031
Epoch 560, val loss: 1.0489298105239868
Epoch 570, training loss: 0.019850756973028183 = 0.012954743579030037 + 0.001 * 6.8960137367248535
Epoch 570, val loss: 1.0613000392913818
Epoch 580, training loss: 0.01903078332543373 = 0.012136043980717659 + 0.001 * 6.894739151000977
Epoch 580, val loss: 1.0733782052993774
Epoch 590, training loss: 0.018279440701007843 = 0.01139010488986969 + 0.001 * 6.889336109161377
Epoch 590, val loss: 1.08513343334198
Epoch 600, training loss: 0.017603401094675064 = 0.010708969086408615 + 0.001 * 6.894432067871094
Epoch 600, val loss: 1.0966298580169678
Epoch 610, training loss: 0.01697639934718609 = 0.010085797868669033 + 0.001 * 6.890601634979248
Epoch 610, val loss: 1.1078273057937622
Epoch 620, training loss: 0.016402894631028175 = 0.00951455533504486 + 0.001 * 6.888339042663574
Epoch 620, val loss: 1.1187639236450195
Epoch 630, training loss: 0.015876974910497665 = 0.008989902213215828 + 0.001 * 6.887072563171387
Epoch 630, val loss: 1.1294090747833252
Epoch 640, training loss: 0.015405027195811272 = 0.0085063511505723 + 0.001 * 6.89867639541626
Epoch 640, val loss: 1.1397526264190674
Epoch 650, training loss: 0.014944862574338913 = 0.008060281164944172 + 0.001 * 6.884580612182617
Epoch 650, val loss: 1.1498923301696777
Epoch 660, training loss: 0.014527503401041031 = 0.007648338098078966 + 0.001 * 6.8791656494140625
Epoch 660, val loss: 1.1597901582717896
Epoch 670, training loss: 0.014158723875880241 = 0.007267466746270657 + 0.001 * 6.891256809234619
Epoch 670, val loss: 1.1694413423538208
Epoch 680, training loss: 0.013800648972392082 = 0.006914738565683365 + 0.001 * 6.8859100341796875
Epoch 680, val loss: 1.1788380146026611
Epoch 690, training loss: 0.013462001457810402 = 0.006587543990463018 + 0.001 * 6.874457836151123
Epoch 690, val loss: 1.18800950050354
Epoch 700, training loss: 0.013159925118088722 = 0.006283515132963657 + 0.001 * 6.876409530639648
Epoch 700, val loss: 1.1969497203826904
Epoch 710, training loss: 0.012877214699983597 = 0.006000605411827564 + 0.001 * 6.876608848571777
Epoch 710, val loss: 1.2056975364685059
Epoch 720, training loss: 0.012606410309672356 = 0.005736932624131441 + 0.001 * 6.86947774887085
Epoch 720, val loss: 1.2141809463500977
Epoch 730, training loss: 0.012358203530311584 = 0.0054906378500163555 + 0.001 * 6.867565155029297
Epoch 730, val loss: 1.2225338220596313
Epoch 740, training loss: 0.012133726850152016 = 0.0052604288794100285 + 0.001 * 6.873298168182373
Epoch 740, val loss: 1.2306618690490723
Epoch 750, training loss: 0.011917751282453537 = 0.005045060534030199 + 0.001 * 6.872689723968506
Epoch 750, val loss: 1.238615870475769
Epoch 760, training loss: 0.011706309393048286 = 0.004843323025852442 + 0.001 * 6.862985610961914
Epoch 760, val loss: 1.2464183568954468
Epoch 770, training loss: 0.01151442714035511 = 0.0046541099436581135 + 0.001 * 6.860316276550293
Epoch 770, val loss: 1.2540100812911987
Epoch 780, training loss: 0.011335552670061588 = 0.004476446192711592 + 0.001 * 6.859106063842773
Epoch 780, val loss: 1.2614526748657227
Epoch 790, training loss: 0.01116950437426567 = 0.0043094223365187645 + 0.001 * 6.860081195831299
Epoch 790, val loss: 1.2687218189239502
Epoch 800, training loss: 0.011007624678313732 = 0.00415226723998785 + 0.001 * 6.8553571701049805
Epoch 800, val loss: 1.2758386135101318
Epoch 810, training loss: 0.010857556015253067 = 0.004004211165010929 + 0.001 * 6.853343963623047
Epoch 810, val loss: 1.2827825546264648
Epoch 820, training loss: 0.010715900920331478 = 0.003864542581140995 + 0.001 * 6.851357936859131
Epoch 820, val loss: 1.2896184921264648
Epoch 830, training loss: 0.010597673244774342 = 0.0037326763849705458 + 0.001 * 6.864996433258057
Epoch 830, val loss: 1.2962498664855957
Epoch 840, training loss: 0.010454236529767513 = 0.0036080542486160994 + 0.001 * 6.846182346343994
Epoch 840, val loss: 1.3027888536453247
Epoch 850, training loss: 0.010346648283302784 = 0.0034901518374681473 + 0.001 * 6.856496334075928
Epoch 850, val loss: 1.3091676235198975
Epoch 860, training loss: 0.01022095512598753 = 0.0033785172272473574 + 0.001 * 6.842437744140625
Epoch 860, val loss: 1.3154453039169312
Epoch 870, training loss: 0.010126106441020966 = 0.003272728528827429 + 0.001 * 6.853377342224121
Epoch 870, val loss: 1.3215408325195312
Epoch 880, training loss: 0.01001850888133049 = 0.003172395983710885 + 0.001 * 6.846112251281738
Epoch 880, val loss: 1.3275434970855713
Epoch 890, training loss: 0.009929780848324299 = 0.0030771244782954454 + 0.001 * 6.852656364440918
Epoch 890, val loss: 1.333433985710144
Epoch 900, training loss: 0.00982828252017498 = 0.002986609237268567 + 0.001 * 6.841673374176025
Epoch 900, val loss: 1.3391815423965454
Epoch 910, training loss: 0.009750597178936005 = 0.0029005480464547873 + 0.001 * 6.850049018859863
Epoch 910, val loss: 1.3448402881622314
Epoch 920, training loss: 0.009652050212025642 = 0.002818654989823699 + 0.001 * 6.833394527435303
Epoch 920, val loss: 1.3503785133361816
Epoch 930, training loss: 0.009580595418810844 = 0.002740684896707535 + 0.001 * 6.839909553527832
Epoch 930, val loss: 1.3558039665222168
Epoch 940, training loss: 0.009505050256848335 = 0.002666385378688574 + 0.001 * 6.838665008544922
Epoch 940, val loss: 1.361135721206665
Epoch 950, training loss: 0.00944126583635807 = 0.0025955296587198973 + 0.001 * 6.845735549926758
Epoch 950, val loss: 1.3663206100463867
Epoch 960, training loss: 0.009359782561659813 = 0.0025278967805206776 + 0.001 * 6.831885814666748
Epoch 960, val loss: 1.3714438676834106
Epoch 970, training loss: 0.00930740125477314 = 0.0024633007124066353 + 0.001 * 6.844099521636963
Epoch 970, val loss: 1.3764415979385376
Epoch 980, training loss: 0.009228317998349667 = 0.0024015509989112616 + 0.001 * 6.826766490936279
Epoch 980, val loss: 1.3813873529434204
Epoch 990, training loss: 0.009178169071674347 = 0.0023424813989549875 + 0.001 * 6.835687637329102
Epoch 990, val loss: 1.386172890663147
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7269
Flip ASR: 0.6711/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9448646306991577 = 1.936490774154663 + 0.001 * 8.373858451843262
Epoch 0, val loss: 1.9327266216278076
Epoch 10, training loss: 1.9348578453063965 = 1.9264839887619019 + 0.001 * 8.373806953430176
Epoch 10, val loss: 1.9222122430801392
Epoch 20, training loss: 1.922546148300171 = 1.9141725301742554 + 0.001 * 8.373623847961426
Epoch 20, val loss: 1.9091252088546753
Epoch 30, training loss: 1.9053552150726318 = 1.8969820737838745 + 0.001 * 8.373198509216309
Epoch 30, val loss: 1.891013264656067
Epoch 40, training loss: 1.8804107904434204 = 1.8720386028289795 + 0.001 * 8.372172355651855
Epoch 40, val loss: 1.8651456832885742
Epoch 50, training loss: 1.845868468284607 = 1.8374992609024048 + 0.001 * 8.369176864624023
Epoch 50, val loss: 1.830964207649231
Epoch 60, training loss: 1.8060410022735596 = 1.7976831197738647 + 0.001 * 8.357833862304688
Epoch 60, val loss: 1.7953962087631226
Epoch 70, training loss: 1.7660273313522339 = 1.7577264308929443 + 0.001 * 8.30085277557373
Epoch 70, val loss: 1.762563705444336
Epoch 80, training loss: 1.714148759841919 = 1.7061874866485596 + 0.001 * 7.961241245269775
Epoch 80, val loss: 1.718490719795227
Epoch 90, training loss: 1.6431612968444824 = 1.6353306770324707 + 0.001 * 7.830644607543945
Epoch 90, val loss: 1.6568958759307861
Epoch 100, training loss: 1.5526200532913208 = 1.5448544025421143 + 0.001 * 7.765628337860107
Epoch 100, val loss: 1.5813133716583252
Epoch 110, training loss: 1.452843427658081 = 1.4451732635498047 + 0.001 * 7.670161247253418
Epoch 110, val loss: 1.5005285739898682
Epoch 120, training loss: 1.3522436618804932 = 1.3447201251983643 + 0.001 * 7.523550510406494
Epoch 120, val loss: 1.4219589233398438
Epoch 130, training loss: 1.2538278102874756 = 1.2463419437408447 + 0.001 * 7.485856533050537
Epoch 130, val loss: 1.3472243547439575
Epoch 140, training loss: 1.159983515739441 = 1.1525403261184692 + 0.001 * 7.443182945251465
Epoch 140, val loss: 1.278023362159729
Epoch 150, training loss: 1.0755623579025269 = 1.0681986808776855 + 0.001 * 7.363715648651123
Epoch 150, val loss: 1.2179514169692993
Epoch 160, training loss: 1.0029245615005493 = 0.9956809282302856 + 0.001 * 7.243619441986084
Epoch 160, val loss: 1.1691133975982666
Epoch 170, training loss: 0.9401935338973999 = 0.9330036044120789 + 0.001 * 7.189919948577881
Epoch 170, val loss: 1.129075288772583
Epoch 180, training loss: 0.8829428553581238 = 0.8757708668708801 + 0.001 * 7.171974182128906
Epoch 180, val loss: 1.0942529439926147
Epoch 190, training loss: 0.826688289642334 = 0.819548487663269 + 0.001 * 7.139804363250732
Epoch 190, val loss: 1.0602070093154907
Epoch 200, training loss: 0.7682793736457825 = 0.7611775994300842 + 0.001 * 7.101785182952881
Epoch 200, val loss: 1.0240317583084106
Epoch 210, training loss: 0.7066152095794678 = 0.6995575428009033 + 0.001 * 7.057644367218018
Epoch 210, val loss: 0.9847245812416077
Epoch 220, training loss: 0.6423819065093994 = 0.6353470683097839 + 0.001 * 7.034862995147705
Epoch 220, val loss: 0.9432367086410522
Epoch 230, training loss: 0.5771890878677368 = 0.5701707601547241 + 0.001 * 7.0183563232421875
Epoch 230, val loss: 0.9012780785560608
Epoch 240, training loss: 0.513085663318634 = 0.5060743093490601 + 0.001 * 7.011364459991455
Epoch 240, val loss: 0.8609347939491272
Epoch 250, training loss: 0.45242613554000854 = 0.4454193711280823 + 0.001 * 7.006764888763428
Epoch 250, val loss: 0.8247843980789185
Epoch 260, training loss: 0.39737385511398315 = 0.3903708755970001 + 0.001 * 7.002986907958984
Epoch 260, val loss: 0.7957370281219482
Epoch 270, training loss: 0.34884876012802124 = 0.3418496251106262 + 0.001 * 6.999142646789551
Epoch 270, val loss: 0.7748157978057861
Epoch 280, training loss: 0.306495726108551 = 0.2994994819164276 + 0.001 * 6.996238708496094
Epoch 280, val loss: 0.7617950439453125
Epoch 290, training loss: 0.2692531645298004 = 0.2622614800930023 + 0.001 * 6.9916863441467285
Epoch 290, val loss: 0.7556110620498657
Epoch 300, training loss: 0.23604339361190796 = 0.22905239462852478 + 0.001 * 6.990991592407227
Epoch 300, val loss: 0.7544257640838623
Epoch 310, training loss: 0.2062636762857437 = 0.19927670061588287 + 0.001 * 6.986973285675049
Epoch 310, val loss: 0.7568065524101257
Epoch 320, training loss: 0.17988069355487823 = 0.17289680242538452 + 0.001 * 6.983896255493164
Epoch 320, val loss: 0.7619450092315674
Epoch 330, training loss: 0.15695981681346893 = 0.1499783992767334 + 0.001 * 6.981423377990723
Epoch 330, val loss: 0.7692677974700928
Epoch 340, training loss: 0.13736100494861603 = 0.13037998974323273 + 0.001 * 6.981009483337402
Epoch 340, val loss: 0.7784340381622314
Epoch 350, training loss: 0.12074598670005798 = 0.11376592516899109 + 0.001 * 6.980064392089844
Epoch 350, val loss: 0.7891477942466736
Epoch 360, training loss: 0.10665793716907501 = 0.09967916458845139 + 0.001 * 6.978775501251221
Epoch 360, val loss: 0.8008003234863281
Epoch 370, training loss: 0.09467311948537827 = 0.08769647777080536 + 0.001 * 6.9766387939453125
Epoch 370, val loss: 0.8132138252258301
Epoch 380, training loss: 0.08444815874099731 = 0.07747217267751694 + 0.001 * 6.975982666015625
Epoch 380, val loss: 0.8260454535484314
Epoch 390, training loss: 0.07568813115358353 = 0.0687084048986435 + 0.001 * 6.979726314544678
Epoch 390, val loss: 0.8391778469085693
Epoch 400, training loss: 0.06813852488994598 = 0.061162836849689484 + 0.001 * 6.975688934326172
Epoch 400, val loss: 0.8524349927902222
Epoch 410, training loss: 0.061631061136722565 = 0.05465919151902199 + 0.001 * 6.971867561340332
Epoch 410, val loss: 0.8658093214035034
Epoch 420, training loss: 0.05601542070508003 = 0.049040716141462326 + 0.001 * 6.974703788757324
Epoch 420, val loss: 0.8791694641113281
Epoch 430, training loss: 0.05113769322633743 = 0.044166065752506256 + 0.001 * 6.9716267585754395
Epoch 430, val loss: 0.8924667835235596
Epoch 440, training loss: 0.04688218981027603 = 0.0399131216108799 + 0.001 * 6.969066143035889
Epoch 440, val loss: 0.9057328104972839
Epoch 450, training loss: 0.04317159205675125 = 0.036180347204208374 + 0.001 * 6.991244316101074
Epoch 450, val loss: 0.9187911152839661
Epoch 460, training loss: 0.039853714406490326 = 0.0328863225877285 + 0.001 * 6.967392921447754
Epoch 460, val loss: 0.9316538572311401
Epoch 470, training loss: 0.03693601116538048 = 0.029973166063427925 + 0.001 * 6.962844371795654
Epoch 470, val loss: 0.9442599415779114
Epoch 480, training loss: 0.03435545414686203 = 0.02739245817065239 + 0.001 * 6.962993621826172
Epoch 480, val loss: 0.9566118121147156
Epoch 490, training loss: 0.032063957303762436 = 0.025102894753217697 + 0.001 * 6.961061000823975
Epoch 490, val loss: 0.9686102867126465
Epoch 500, training loss: 0.03003424033522606 = 0.02306750975549221 + 0.001 * 6.966730117797852
Epoch 500, val loss: 0.9802969694137573
Epoch 510, training loss: 0.028212593868374825 = 0.021254325285553932 + 0.001 * 6.958268642425537
Epoch 510, val loss: 0.9917359352111816
Epoch 520, training loss: 0.02658906951546669 = 0.019635820761322975 + 0.001 * 6.953248977661133
Epoch 520, val loss: 1.0028259754180908
Epoch 530, training loss: 0.02514629438519478 = 0.018187163397669792 + 0.001 * 6.95913028717041
Epoch 530, val loss: 1.0136550664901733
Epoch 540, training loss: 0.023838317021727562 = 0.01688738353550434 + 0.001 * 6.950932502746582
Epoch 540, val loss: 1.0241345167160034
Epoch 550, training loss: 0.022666223347187042 = 0.01571846380829811 + 0.001 * 6.947759628295898
Epoch 550, val loss: 1.0343104600906372
Epoch 560, training loss: 0.02161690965294838 = 0.014664306305348873 + 0.001 * 6.952601909637451
Epoch 560, val loss: 1.0441813468933105
Epoch 570, training loss: 0.02065781131386757 = 0.01371118426322937 + 0.001 * 6.946625709533691
Epoch 570, val loss: 1.0537711381912231
Epoch 580, training loss: 0.01979033648967743 = 0.01284793857485056 + 0.001 * 6.942396640777588
Epoch 580, val loss: 1.0630719661712646
Epoch 590, training loss: 0.019000956788659096 = 0.012063819915056229 + 0.001 * 6.937136650085449
Epoch 590, val loss: 1.071977972984314
Epoch 600, training loss: 0.01829751767218113 = 0.01134970411658287 + 0.001 * 6.947812557220459
Epoch 600, val loss: 1.0807491540908813
Epoch 610, training loss: 0.017642855644226074 = 0.010697499848902225 + 0.001 * 6.9453558921813965
Epoch 610, val loss: 1.0891101360321045
Epoch 620, training loss: 0.01702551357448101 = 0.010100717656314373 + 0.001 * 6.924796104431152
Epoch 620, val loss: 1.0973303318023682
Epoch 630, training loss: 0.016483375802636147 = 0.009553417563438416 + 0.001 * 6.929958343505859
Epoch 630, val loss: 1.105228066444397
Epoch 640, training loss: 0.015977103263139725 = 0.009050521068274975 + 0.001 * 6.926581382751465
Epoch 640, val loss: 1.112919807434082
Epoch 650, training loss: 0.015513075515627861 = 0.008587537333369255 + 0.001 * 6.925538539886475
Epoch 650, val loss: 1.1204088926315308
Epoch 660, training loss: 0.015071157366037369 = 0.008160454221069813 + 0.001 * 6.910703182220459
Epoch 660, val loss: 1.1276291608810425
Epoch 670, training loss: 0.014678390696644783 = 0.007765788119286299 + 0.001 * 6.912602424621582
Epoch 670, val loss: 1.1347393989562988
Epoch 680, training loss: 0.014308048412203789 = 0.007400364149361849 + 0.001 * 6.907683372497559
Epoch 680, val loss: 1.141568660736084
Epoch 690, training loss: 0.014000678434967995 = 0.007061396725475788 + 0.001 * 6.939281463623047
Epoch 690, val loss: 1.1482188701629639
Epoch 700, training loss: 0.013642899692058563 = 0.006746296305209398 + 0.001 * 6.896602630615234
Epoch 700, val loss: 1.1546447277069092
Epoch 710, training loss: 0.013362782076001167 = 0.0064527071081101894 + 0.001 * 6.9100751876831055
Epoch 710, val loss: 1.1609221696853638
Epoch 720, training loss: 0.013089654967188835 = 0.006178432609885931 + 0.001 * 6.911222457885742
Epoch 720, val loss: 1.1670418977737427
Epoch 730, training loss: 0.012832315638661385 = 0.0059219906106591225 + 0.001 * 6.910325050354004
Epoch 730, val loss: 1.1731610298156738
Epoch 740, training loss: 0.012561935000121593 = 0.005681326612830162 + 0.001 * 6.880608081817627
Epoch 740, val loss: 1.178998589515686
Epoch 750, training loss: 0.01235237903892994 = 0.0054550678469240665 + 0.001 * 6.897310733795166
Epoch 750, val loss: 1.1846716403961182
Epoch 760, training loss: 0.012121181935071945 = 0.005241757724434137 + 0.001 * 6.879424571990967
Epoch 760, val loss: 1.1902329921722412
Epoch 770, training loss: 0.011954285204410553 = 0.005040413234382868 + 0.001 * 6.913872241973877
Epoch 770, val loss: 1.1956599950790405
Epoch 780, training loss: 0.011755069717764854 = 0.004850141704082489 + 0.001 * 6.904927730560303
Epoch 780, val loss: 1.200881838798523
Epoch 790, training loss: 0.011568378657102585 = 0.004670342896133661 + 0.001 * 6.898035526275635
Epoch 790, val loss: 1.2060879468917847
Epoch 800, training loss: 0.011360736563801765 = 0.0045001208782196045 + 0.001 * 6.860614776611328
Epoch 800, val loss: 1.2111079692840576
Epoch 810, training loss: 0.011213152669370174 = 0.004338758531957865 + 0.001 * 6.874393939971924
Epoch 810, val loss: 1.2160667181015015
Epoch 820, training loss: 0.01105508953332901 = 0.004185654688626528 + 0.001 * 6.869434833526611
Epoch 820, val loss: 1.2208189964294434
Epoch 830, training loss: 0.010893622413277626 = 0.0040406109765172005 + 0.001 * 6.853010654449463
Epoch 830, val loss: 1.2254804372787476
Epoch 840, training loss: 0.010794490575790405 = 0.0039032194763422012 + 0.001 * 6.891271114349365
Epoch 840, val loss: 1.2300611734390259
Epoch 850, training loss: 0.010642227716743946 = 0.0037729365285485983 + 0.001 * 6.869290828704834
Epoch 850, val loss: 1.2344611883163452
Epoch 860, training loss: 0.010506128892302513 = 0.003649459220468998 + 0.001 * 6.8566694259643555
Epoch 860, val loss: 1.2387974262237549
Epoch 870, training loss: 0.010411357507109642 = 0.003532259026542306 + 0.001 * 6.879098415374756
Epoch 870, val loss: 1.2430187463760376
Epoch 880, training loss: 0.010262079536914825 = 0.003421021392568946 + 0.001 * 6.841058254241943
Epoch 880, val loss: 1.2470955848693848
Epoch 890, training loss: 0.010157085955142975 = 0.0033153844997286797 + 0.001 * 6.841701030731201
Epoch 890, val loss: 1.2511072158813477
Epoch 900, training loss: 0.010053221136331558 = 0.0032150023616850376 + 0.001 * 6.8382182121276855
Epoch 900, val loss: 1.2550565004348755
Epoch 910, training loss: 0.009962335228919983 = 0.0031196356285363436 + 0.001 * 6.84269905090332
Epoch 910, val loss: 1.2588645219802856
Epoch 920, training loss: 0.009900134056806564 = 0.0030289380811154842 + 0.001 * 6.8711957931518555
Epoch 920, val loss: 1.2625975608825684
Epoch 930, training loss: 0.009808061644434929 = 0.0029426587279886007 + 0.001 * 6.865402698516846
Epoch 930, val loss: 1.2662771940231323
Epoch 940, training loss: 0.009708316996693611 = 0.0028604744002223015 + 0.001 * 6.847841739654541
Epoch 940, val loss: 1.269838571548462
Epoch 950, training loss: 0.009619084186851978 = 0.0027822095435112715 + 0.001 * 6.836874008178711
Epoch 950, val loss: 1.2732982635498047
Epoch 960, training loss: 0.00956136267632246 = 0.0027076478581875563 + 0.001 * 6.853714466094971
Epoch 960, val loss: 1.2766941785812378
Epoch 970, training loss: 0.00945668388158083 = 0.0026365399826318026 + 0.001 * 6.820143699645996
Epoch 970, val loss: 1.279992699623108
Epoch 980, training loss: 0.009390258230268955 = 0.0025686637964099646 + 0.001 * 6.821593761444092
Epoch 980, val loss: 1.2832616567611694
Epoch 990, training loss: 0.009334949776530266 = 0.00250380695797503 + 0.001 * 6.831142425537109
Epoch 990, val loss: 1.2864301204681396
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.4613
Flip ASR: 0.4133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9560225009918213 = 1.9476486444473267 + 0.001 * 8.37388801574707
Epoch 0, val loss: 1.94944167137146
Epoch 10, training loss: 1.9463261365890503 = 1.9379522800445557 + 0.001 * 8.373835563659668
Epoch 10, val loss: 1.939901351928711
Epoch 20, training loss: 1.934590220451355 = 1.9262166023254395 + 0.001 * 8.373635292053223
Epoch 20, val loss: 1.927954077720642
Epoch 30, training loss: 1.9181933403015137 = 1.9098201990127563 + 0.001 * 8.37317180633545
Epoch 30, val loss: 1.9109691381454468
Epoch 40, training loss: 1.8937478065490723 = 1.885375738143921 + 0.001 * 8.372068405151367
Epoch 40, val loss: 1.8856689929962158
Epoch 50, training loss: 1.858485221862793 = 1.8501161336898804 + 0.001 * 8.369034767150879
Epoch 50, val loss: 1.8504756689071655
Epoch 60, training loss: 1.8161686658859253 = 1.8078103065490723 + 0.001 * 8.358390808105469
Epoch 60, val loss: 1.8115400075912476
Epoch 70, training loss: 1.7774142026901245 = 1.769109845161438 + 0.001 * 8.304364204406738
Epoch 70, val loss: 1.7792928218841553
Epoch 80, training loss: 1.7331172227859497 = 1.725144624710083 + 0.001 * 7.97263240814209
Epoch 80, val loss: 1.7415156364440918
Epoch 90, training loss: 1.671830415725708 = 1.6641250848770142 + 0.001 * 7.705376148223877
Epoch 90, val loss: 1.6897004842758179
Epoch 100, training loss: 1.5895671844482422 = 1.5821644067764282 + 0.001 * 7.402815818786621
Epoch 100, val loss: 1.6212855577468872
Epoch 110, training loss: 1.4890543222427368 = 1.4818278551101685 + 0.001 * 7.226504802703857
Epoch 110, val loss: 1.5389150381088257
Epoch 120, training loss: 1.3793504238128662 = 1.3721554279327393 + 0.001 * 7.194995880126953
Epoch 120, val loss: 1.4484108686447144
Epoch 130, training loss: 1.2663865089416504 = 1.2592116594314575 + 0.001 * 7.174888610839844
Epoch 130, val loss: 1.3555024862289429
Epoch 140, training loss: 1.152937412261963 = 1.1457828283309937 + 0.001 * 7.154601097106934
Epoch 140, val loss: 1.262303113937378
Epoch 150, training loss: 1.0419397354125977 = 1.0348047018051147 + 0.001 * 7.135069370269775
Epoch 150, val loss: 1.1720267534255981
Epoch 160, training loss: 0.9368290305137634 = 0.9297091960906982 + 0.001 * 7.119828224182129
Epoch 160, val loss: 1.0879243612289429
Epoch 170, training loss: 0.8396894931793213 = 0.8325812816619873 + 0.001 * 7.10823392868042
Epoch 170, val loss: 1.011484980583191
Epoch 180, training loss: 0.7511695027351379 = 0.7440738677978516 + 0.001 * 7.095627307891846
Epoch 180, val loss: 0.9428697228431702
Epoch 190, training loss: 0.6715928316116333 = 0.6645147800445557 + 0.001 * 7.078062534332275
Epoch 190, val loss: 0.8829614520072937
Epoch 200, training loss: 0.601223349571228 = 0.5941711664199829 + 0.001 * 7.0521745681762695
Epoch 200, val loss: 0.8323879241943359
Epoch 210, training loss: 0.5398471355438232 = 0.5328205227851868 + 0.001 * 7.026601791381836
Epoch 210, val loss: 0.7916946411132812
Epoch 220, training loss: 0.48649194836616516 = 0.4794827997684479 + 0.001 * 7.0091423988342285
Epoch 220, val loss: 0.7604680061340332
Epoch 230, training loss: 0.4393991231918335 = 0.4324002265930176 + 0.001 * 6.998881816864014
Epoch 230, val loss: 0.737335741519928
Epoch 240, training loss: 0.3965529203414917 = 0.389560341835022 + 0.001 * 6.992578506469727
Epoch 240, val loss: 0.7201056480407715
Epoch 250, training loss: 0.35636118054389954 = 0.3493804335594177 + 0.001 * 6.98075532913208
Epoch 250, val loss: 0.7067456245422363
Epoch 260, training loss: 0.3179934620857239 = 0.31101322174072266 + 0.001 * 6.980235576629639
Epoch 260, val loss: 0.6961367726325989
Epoch 270, training loss: 0.28150129318237305 = 0.27453890442848206 + 0.001 * 6.962399959564209
Epoch 270, val loss: 0.6877351999282837
Epoch 280, training loss: 0.24741916358470917 = 0.24046367406845093 + 0.001 * 6.95548677444458
Epoch 280, val loss: 0.6816812753677368
Epoch 290, training loss: 0.21625791490077972 = 0.2093060314655304 + 0.001 * 6.951879024505615
Epoch 290, val loss: 0.6782274842262268
Epoch 300, training loss: 0.18834277987480164 = 0.18138691782951355 + 0.001 * 6.955862998962402
Epoch 300, val loss: 0.6771134734153748
Epoch 310, training loss: 0.16374912858009338 = 0.1568053960800171 + 0.001 * 6.943735122680664
Epoch 310, val loss: 0.6784458160400391
Epoch 320, training loss: 0.14242114126682281 = 0.13548527657985687 + 0.001 * 6.935868263244629
Epoch 320, val loss: 0.6820772886276245
Epoch 330, training loss: 0.12418326735496521 = 0.11724741756916046 + 0.001 * 6.935853004455566
Epoch 330, val loss: 0.6877008080482483
Epoch 340, training loss: 0.1087387204170227 = 0.10180386900901794 + 0.001 * 6.934847354888916
Epoch 340, val loss: 0.6953369975090027
Epoch 350, training loss: 0.09573708474636078 = 0.08880501985549927 + 0.001 * 6.932065963745117
Epoch 350, val loss: 0.704690158367157
Epoch 360, training loss: 0.08480323106050491 = 0.07787395268678665 + 0.001 * 6.9292755126953125
Epoch 360, val loss: 0.7153703570365906
Epoch 370, training loss: 0.07558981329202652 = 0.06865742802619934 + 0.001 * 6.932384967803955
Epoch 370, val loss: 0.7271171808242798
Epoch 380, training loss: 0.06777964532375336 = 0.06085260957479477 + 0.001 * 6.927036285400391
Epoch 380, val loss: 0.7396234273910522
Epoch 390, training loss: 0.06112877279520035 = 0.05420045554637909 + 0.001 * 6.928318023681641
Epoch 390, val loss: 0.7527056336402893
Epoch 400, training loss: 0.05542341247200966 = 0.04849433898925781 + 0.001 * 6.929074287414551
Epoch 400, val loss: 0.7660557627677917
Epoch 410, training loss: 0.05050010606646538 = 0.04357336461544037 + 0.001 * 6.9267425537109375
Epoch 410, val loss: 0.7795817255973816
Epoch 420, training loss: 0.04623311012983322 = 0.03930765390396118 + 0.001 * 6.925457954406738
Epoch 420, val loss: 0.7931103706359863
Epoch 430, training loss: 0.04251736029982567 = 0.035592686384916306 + 0.001 * 6.9246745109558105
Epoch 430, val loss: 0.8064836263656616
Epoch 440, training loss: 0.03926730901002884 = 0.032343633472919464 + 0.001 * 6.923675537109375
Epoch 440, val loss: 0.8197659254074097
Epoch 450, training loss: 0.036412037909030914 = 0.02949088253080845 + 0.001 * 6.921154499053955
Epoch 450, val loss: 0.8328466415405273
Epoch 460, training loss: 0.03389471024274826 = 0.026976624503731728 + 0.001 * 6.918084144592285
Epoch 460, val loss: 0.8456707000732422
Epoch 470, training loss: 0.03167731314897537 = 0.02475353330373764 + 0.001 * 6.923778533935547
Epoch 470, val loss: 0.8582409620285034
Epoch 480, training loss: 0.029695944860577583 = 0.022780759260058403 + 0.001 * 6.915185451507568
Epoch 480, val loss: 0.8704622983932495
Epoch 490, training loss: 0.027940338477492332 = 0.021024487912654877 + 0.001 * 6.915850639343262
Epoch 490, val loss: 0.8824187517166138
Epoch 500, training loss: 0.026372205466032028 = 0.019456176087260246 + 0.001 * 6.916028022766113
Epoch 500, val loss: 0.8940903544425964
Epoch 510, training loss: 0.024965640157461166 = 0.01805146597325802 + 0.001 * 6.914174556732178
Epoch 510, val loss: 0.9054277539253235
Epoch 520, training loss: 0.023701148107647896 = 0.016789807006716728 + 0.001 * 6.911341190338135
Epoch 520, val loss: 0.9165263772010803
Epoch 530, training loss: 0.022557325661182404 = 0.015653332695364952 + 0.001 * 6.903992176055908
Epoch 530, val loss: 0.927276074886322
Epoch 540, training loss: 0.021533407270908356 = 0.014626654796302319 + 0.001 * 6.906752109527588
Epoch 540, val loss: 0.9377824068069458
Epoch 550, training loss: 0.020601576194167137 = 0.013696745038032532 + 0.001 * 6.904831409454346
Epoch 550, val loss: 0.9479396939277649
Epoch 560, training loss: 0.019765641540288925 = 0.012852425687015057 + 0.001 * 6.913215160369873
Epoch 560, val loss: 0.9578992128372192
Epoch 570, training loss: 0.018984876573085785 = 0.012084003537893295 + 0.001 * 6.90087366104126
Epoch 570, val loss: 0.9675650000572205
Epoch 580, training loss: 0.018282722681760788 = 0.011383007280528545 + 0.001 * 6.899714469909668
Epoch 580, val loss: 0.976972222328186
Epoch 590, training loss: 0.017637912184000015 = 0.010741987265646458 + 0.001 * 6.8959245681762695
Epoch 590, val loss: 0.9861030578613281
Epoch 600, training loss: 0.01704483851790428 = 0.010154543444514275 + 0.001 * 6.890295028686523
Epoch 600, val loss: 0.995011031627655
Epoch 610, training loss: 0.016523286700248718 = 0.009614977985620499 + 0.001 * 6.908308506011963
Epoch 610, val loss: 1.0036499500274658
Epoch 620, training loss: 0.01600443758070469 = 0.009118402376770973 + 0.001 * 6.8860344886779785
Epoch 620, val loss: 1.0120728015899658
Epoch 630, training loss: 0.015563571825623512 = 0.00866053905338049 + 0.001 * 6.903032302856445
Epoch 630, val loss: 1.020277500152588
Epoch 640, training loss: 0.015125904232263565 = 0.008237512782216072 + 0.001 * 6.888391017913818
Epoch 640, val loss: 1.028273582458496
Epoch 650, training loss: 0.014753337949514389 = 0.007845938205718994 + 0.001 * 6.9073991775512695
Epoch 650, val loss: 1.0360575914382935
Epoch 660, training loss: 0.014367478899657726 = 0.00748285511508584 + 0.001 * 6.8846235275268555
Epoch 660, val loss: 1.0436675548553467
Epoch 670, training loss: 0.014020011760294437 = 0.007145590148866177 + 0.001 * 6.874421119689941
Epoch 670, val loss: 1.0510541200637817
Epoch 680, training loss: 0.01370193064212799 = 0.006831772159785032 + 0.001 * 6.8701581954956055
Epoch 680, val loss: 1.0582902431488037
Epoch 690, training loss: 0.013404076918959618 = 0.006539313122630119 + 0.001 * 6.8647637367248535
Epoch 690, val loss: 1.065305233001709
Epoch 700, training loss: 0.013142402283847332 = 0.006266297306865454 + 0.001 * 6.876104831695557
Epoch 700, val loss: 1.072192668914795
Epoch 710, training loss: 0.012888696044683456 = 0.006011131219565868 + 0.001 * 6.877564907073975
Epoch 710, val loss: 1.0788822174072266
Epoch 720, training loss: 0.012645931914448738 = 0.005772293545305729 + 0.001 * 6.87363862991333
Epoch 720, val loss: 1.0854041576385498
Epoch 730, training loss: 0.01241602934896946 = 0.005548451561480761 + 0.001 * 6.867577075958252
Epoch 730, val loss: 1.091820478439331
Epoch 740, training loss: 0.01220116950571537 = 0.0053383708000183105 + 0.001 * 6.862798690795898
Epoch 740, val loss: 1.0980525016784668
Epoch 750, training loss: 0.011995602399110794 = 0.005140978842973709 + 0.001 * 6.854623317718506
Epoch 750, val loss: 1.1041337251663208
Epoch 760, training loss: 0.011839333921670914 = 0.004955267067998648 + 0.001 * 6.884066581726074
Epoch 760, val loss: 1.110094428062439
Epoch 770, training loss: 0.011634210124611855 = 0.004780343733727932 + 0.001 * 6.853865623474121
Epoch 770, val loss: 1.1159001588821411
Epoch 780, training loss: 0.011467386037111282 = 0.004615400917828083 + 0.001 * 6.851984977722168
Epoch 780, val loss: 1.1215678453445435
Epoch 790, training loss: 0.01132951583713293 = 0.004459662828594446 + 0.001 * 6.869852542877197
Epoch 790, val loss: 1.1271461248397827
Epoch 800, training loss: 0.011151188984513283 = 0.004312504548579454 + 0.001 * 6.838684558868408
Epoch 800, val loss: 1.1325689554214478
Epoch 810, training loss: 0.011011075228452682 = 0.004173284862190485 + 0.001 * 6.837790489196777
Epoch 810, val loss: 1.1378536224365234
Epoch 820, training loss: 0.01087835244834423 = 0.004041455686092377 + 0.001 * 6.8368964195251465
Epoch 820, val loss: 1.1430827379226685
Epoch 830, training loss: 0.010782908648252487 = 0.003916486166417599 + 0.001 * 6.866422176361084
Epoch 830, val loss: 1.1481539011001587
Epoch 840, training loss: 0.010636366903781891 = 0.0037979744374752045 + 0.001 * 6.83839225769043
Epoch 840, val loss: 1.1530917882919312
Epoch 850, training loss: 0.01055164448916912 = 0.0036853901110589504 + 0.001 * 6.8662543296813965
Epoch 850, val loss: 1.157976746559143
Epoch 860, training loss: 0.010407932102680206 = 0.003578424220904708 + 0.001 * 6.829507350921631
Epoch 860, val loss: 1.1626931428909302
Epoch 870, training loss: 0.010315107181668282 = 0.003476648824289441 + 0.001 * 6.8384575843811035
Epoch 870, val loss: 1.1673791408538818
Epoch 880, training loss: 0.010193001478910446 = 0.0033796867355704308 + 0.001 * 6.813314437866211
Epoch 880, val loss: 1.1719496250152588
Epoch 890, training loss: 0.01011732779443264 = 0.003287011291831732 + 0.001 * 6.830316066741943
Epoch 890, val loss: 1.1764390468597412
Epoch 900, training loss: 0.010031022131443024 = 0.0031978515908122063 + 0.001 * 6.833169937133789
Epoch 900, val loss: 1.1809487342834473
Epoch 910, training loss: 0.009952354244887829 = 0.0031112569849938154 + 0.001 * 6.841097354888916
Epoch 910, val loss: 1.185473918914795
Epoch 920, training loss: 0.009851183742284775 = 0.0030264900997281075 + 0.001 * 6.82469367980957
Epoch 920, val loss: 1.1900439262390137
Epoch 930, training loss: 0.009739946573972702 = 0.0029434598982334137 + 0.001 * 6.796485900878906
Epoch 930, val loss: 1.194712519645691
Epoch 940, training loss: 0.00966806709766388 = 0.0028623039834201336 + 0.001 * 6.805762767791748
Epoch 940, val loss: 1.1994606256484985
Epoch 950, training loss: 0.00960724800825119 = 0.0027833126951009035 + 0.001 * 6.823934555053711
Epoch 950, val loss: 1.204269528388977
Epoch 960, training loss: 0.009499326348304749 = 0.0027067605406045914 + 0.001 * 6.792565822601318
Epoch 960, val loss: 1.2090704441070557
Epoch 970, training loss: 0.009421603754162788 = 0.0026328295934945345 + 0.001 * 6.788774013519287
Epoch 970, val loss: 1.2139118909835815
Epoch 980, training loss: 0.009347708895802498 = 0.0025615282356739044 + 0.001 * 6.7861809730529785
Epoch 980, val loss: 1.2187292575836182
Epoch 990, training loss: 0.009272514842450619 = 0.0024929256178438663 + 0.001 * 6.77958869934082
Epoch 990, val loss: 1.2235504388809204
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8303
Flip ASR: 0.7956/225 nodes
The final ASR:0.67282, 0.15543, Accuracy:0.81235, 0.00462
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9428])
updated graph: torch.Size([2, 10500])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83333, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9638360738754272 = 1.9554622173309326 + 0.001 * 8.373846054077148
Epoch 0, val loss: 1.959874153137207
Epoch 10, training loss: 1.9538637399673462 = 1.9454900026321411 + 0.001 * 8.373770713806152
Epoch 10, val loss: 1.9505280256271362
Epoch 20, training loss: 1.941412329673767 = 1.9330388307571411 + 0.001 * 8.373543739318848
Epoch 20, val loss: 1.9384117126464844
Epoch 30, training loss: 1.9237802028656006 = 1.9154070615768433 + 0.001 * 8.373085021972656
Epoch 30, val loss: 1.9209929704666138
Epoch 40, training loss: 1.8976068496704102 = 1.8892347812652588 + 0.001 * 8.37209415435791
Epoch 40, val loss: 1.8953033685684204
Epoch 50, training loss: 1.8604815006256104 = 1.8521121740341187 + 0.001 * 8.369300842285156
Epoch 50, val loss: 1.860233187675476
Epoch 60, training loss: 1.8166847229003906 = 1.8083256483078003 + 0.001 * 8.35910415649414
Epoch 60, val loss: 1.8223618268966675
Epoch 70, training loss: 1.7765611410140991 = 1.768250823020935 + 0.001 * 8.310261726379395
Epoch 70, val loss: 1.7897202968597412
Epoch 80, training loss: 1.7309859991073608 = 1.7229914665222168 + 0.001 * 7.994478225708008
Epoch 80, val loss: 1.7487208843231201
Epoch 90, training loss: 1.6696339845657349 = 1.6619147062301636 + 0.001 * 7.719326496124268
Epoch 90, val loss: 1.6942764520645142
Epoch 100, training loss: 1.587342381477356 = 1.5798189640045166 + 0.001 * 7.523468971252441
Epoch 100, val loss: 1.6243513822555542
Epoch 110, training loss: 1.4859354496002197 = 1.478595495223999 + 0.001 * 7.339963912963867
Epoch 110, val loss: 1.5407711267471313
Epoch 120, training loss: 1.3745763301849365 = 1.3673967123031616 + 0.001 * 7.1796650886535645
Epoch 120, val loss: 1.4501214027404785
Epoch 130, training loss: 1.26140558719635 = 1.2543020248413086 + 0.001 * 7.103556156158447
Epoch 130, val loss: 1.3594340085983276
Epoch 140, training loss: 1.1515024900436401 = 1.1444737911224365 + 0.001 * 7.028661251068115
Epoch 140, val loss: 1.2724480628967285
Epoch 150, training loss: 1.0495878458023071 = 1.042596459388733 + 0.001 * 6.991419315338135
Epoch 150, val loss: 1.1932440996170044
Epoch 160, training loss: 0.9577136039733887 = 0.9507443904876709 + 0.001 * 6.969193458557129
Epoch 160, val loss: 1.1225529909133911
Epoch 170, training loss: 0.8739392161369324 = 0.8669832944869995 + 0.001 * 6.955896854400635
Epoch 170, val loss: 1.0587736368179321
Epoch 180, training loss: 0.7949188947677612 = 0.7879704833030701 + 0.001 * 6.948390483856201
Epoch 180, val loss: 0.9986242055892944
Epoch 190, training loss: 0.7187539935112 = 0.7118085026741028 + 0.001 * 6.945512771606445
Epoch 190, val loss: 0.9404171705245972
Epoch 200, training loss: 0.6455371379852295 = 0.6385922431945801 + 0.001 * 6.944869041442871
Epoch 200, val loss: 0.8845726251602173
Epoch 210, training loss: 0.5760639905929565 = 0.5691195130348206 + 0.001 * 6.944500923156738
Epoch 210, val loss: 0.8326786160469055
Epoch 220, training loss: 0.5108126401901245 = 0.5038689970970154 + 0.001 * 6.943637371063232
Epoch 220, val loss: 0.7863491773605347
Epoch 230, training loss: 0.45010510087013245 = 0.44316279888153076 + 0.001 * 6.942298889160156
Epoch 230, val loss: 0.7468571662902832
Epoch 240, training loss: 0.3943503797054291 = 0.3874095380306244 + 0.001 * 6.940842628479004
Epoch 240, val loss: 0.7141809463500977
Epoch 250, training loss: 0.34391865134239197 = 0.3369790017604828 + 0.001 * 6.939634799957275
Epoch 250, val loss: 0.6873113512992859
Epoch 260, training loss: 0.29892081022262573 = 0.2919820249080658 + 0.001 * 6.938797950744629
Epoch 260, val loss: 0.6650537848472595
Epoch 270, training loss: 0.25923651456832886 = 0.25229814648628235 + 0.001 * 6.938373565673828
Epoch 270, val loss: 0.6467128396034241
Epoch 280, training loss: 0.2246953248977661 = 0.217756986618042 + 0.001 * 6.9383440017700195
Epoch 280, val loss: 0.6325002312660217
Epoch 290, training loss: 0.19505026936531067 = 0.18811166286468506 + 0.001 * 6.938611030578613
Epoch 290, val loss: 0.6221890449523926
Epoch 300, training loss: 0.16991722583770752 = 0.1629781574010849 + 0.001 * 6.939065456390381
Epoch 300, val loss: 0.615679919719696
Epoch 310, training loss: 0.1487605720758438 = 0.14182095229625702 + 0.001 * 6.9396209716796875
Epoch 310, val loss: 0.6125588417053223
Epoch 320, training loss: 0.13097578287124634 = 0.12403534352779388 + 0.001 * 6.940431594848633
Epoch 320, val loss: 0.6123414039611816
Epoch 330, training loss: 0.11597426980733871 = 0.10903368890285492 + 0.001 * 6.940579414367676
Epoch 330, val loss: 0.6145195364952087
Epoch 340, training loss: 0.10324931889772415 = 0.09630871564149857 + 0.001 * 6.940601348876953
Epoch 340, val loss: 0.6187092661857605
Epoch 350, training loss: 0.09238588064908981 = 0.0854455828666687 + 0.001 * 6.940299987792969
Epoch 350, val loss: 0.6245144605636597
Epoch 360, training loss: 0.08305223286151886 = 0.07611291855573654 + 0.001 * 6.939316749572754
Epoch 360, val loss: 0.6315903067588806
Epoch 370, training loss: 0.07499435544013977 = 0.06805266439914703 + 0.001 * 6.9416937828063965
Epoch 370, val loss: 0.6396324038505554
Epoch 380, training loss: 0.06799834221601486 = 0.061061955988407135 + 0.001 * 6.936384201049805
Epoch 380, val loss: 0.6483919620513916
Epoch 390, training loss: 0.06191187724471092 = 0.05497786030173302 + 0.001 * 6.934016227722168
Epoch 390, val loss: 0.6576432585716248
Epoch 400, training loss: 0.056596919894218445 = 0.04966755583882332 + 0.001 * 6.92936372756958
Epoch 400, val loss: 0.6672353148460388
Epoch 410, training loss: 0.05194320157170296 = 0.04501881077885628 + 0.001 * 6.924391269683838
Epoch 410, val loss: 0.6770171523094177
Epoch 420, training loss: 0.04785715416073799 = 0.040938083082437515 + 0.001 * 6.919069290161133
Epoch 420, val loss: 0.686926007270813
Epoch 430, training loss: 0.044255468994379044 = 0.03734469786286354 + 0.001 * 6.910772323608398
Epoch 430, val loss: 0.6968960165977478
Epoch 440, training loss: 0.04111170768737793 = 0.034170981496572495 + 0.001 * 6.940723896026611
Epoch 440, val loss: 0.706821620464325
Epoch 450, training loss: 0.03825880587100983 = 0.03135883808135986 + 0.001 * 6.899965763092041
Epoch 450, val loss: 0.7166858315467834
Epoch 460, training loss: 0.03575527295470238 = 0.028859950602054596 + 0.001 * 6.895323276519775
Epoch 460, val loss: 0.7264226078987122
Epoch 470, training loss: 0.03351736068725586 = 0.02663232386112213 + 0.001 * 6.885034561157227
Epoch 470, val loss: 0.7359903454780579
Epoch 480, training loss: 0.031521379947662354 = 0.024641012772917747 + 0.001 * 6.880368709564209
Epoch 480, val loss: 0.7454143762588501
Epoch 490, training loss: 0.029732268303632736 = 0.022855352610349655 + 0.001 * 6.876914978027344
Epoch 490, val loss: 0.7546879053115845
Epoch 500, training loss: 0.028129348531365395 = 0.021250039339065552 + 0.001 * 6.879308223724365
Epoch 500, val loss: 0.7637689709663391
Epoch 510, training loss: 0.026676226407289505 = 0.01980271376669407 + 0.001 * 6.8735127449035645
Epoch 510, val loss: 0.7727051973342896
Epoch 520, training loss: 0.02536826953291893 = 0.01849423721432686 + 0.001 * 6.874032020568848
Epoch 520, val loss: 0.7814329266548157
Epoch 530, training loss: 0.024178434163331985 = 0.01730814017355442 + 0.001 * 6.870294094085693
Epoch 530, val loss: 0.7899919152259827
Epoch 540, training loss: 0.02309918776154518 = 0.016230275854468346 + 0.001 * 6.8689117431640625
Epoch 540, val loss: 0.7983592748641968
Epoch 550, training loss: 0.022116156294941902 = 0.015248389914631844 + 0.001 * 6.8677659034729
Epoch 550, val loss: 0.8065548539161682
Epoch 560, training loss: 0.021220114082098007 = 0.014351933263242245 + 0.001 * 6.868180751800537
Epoch 560, val loss: 0.8145715594291687
Epoch 570, training loss: 0.020402342081069946 = 0.013531678356230259 + 0.001 * 6.870662689208984
Epoch 570, val loss: 0.8224067091941833
Epoch 580, training loss: 0.019642746075987816 = 0.012779531069099903 + 0.001 * 6.863214015960693
Epoch 580, val loss: 0.8300744891166687
Epoch 590, training loss: 0.018949836492538452 = 0.012088378891348839 + 0.001 * 6.861457347869873
Epoch 590, val loss: 0.8375696539878845
Epoch 600, training loss: 0.01831364631652832 = 0.011452005244791508 + 0.001 * 6.861639976501465
Epoch 600, val loss: 0.8448817133903503
Epoch 610, training loss: 0.017724277451634407 = 0.010863400995731354 + 0.001 * 6.860876083374023
Epoch 610, val loss: 0.8520208597183228
Epoch 620, training loss: 0.017172755673527718 = 0.010315785184502602 + 0.001 * 6.856970310211182
Epoch 620, val loss: 0.8588991165161133
Epoch 630, training loss: 0.016676289960741997 = 0.009806368499994278 + 0.001 * 6.86992073059082
Epoch 630, val loss: 0.8659780025482178
Epoch 640, training loss: 0.016182247549295425 = 0.009330510161817074 + 0.001 * 6.851737976074219
Epoch 640, val loss: 0.8727172613143921
Epoch 650, training loss: 0.015737667679786682 = 0.008885631337761879 + 0.001 * 6.852036476135254
Epoch 650, val loss: 0.8792229890823364
Epoch 660, training loss: 0.015323452651500702 = 0.008469246327877045 + 0.001 * 6.85420560836792
Epoch 660, val loss: 0.8858258724212646
Epoch 670, training loss: 0.014925328083336353 = 0.008079325780272484 + 0.001 * 6.846002101898193
Epoch 670, val loss: 0.8923317193984985
Epoch 680, training loss: 0.014575589448213577 = 0.007714658509939909 + 0.001 * 6.860930442810059
Epoch 680, val loss: 0.8984024524688721
Epoch 690, training loss: 0.01422487385571003 = 0.007373190484941006 + 0.001 * 6.851683139801025
Epoch 690, val loss: 0.9047707915306091
Epoch 700, training loss: 0.013895916752517223 = 0.007053439971059561 + 0.001 * 6.8424763679504395
Epoch 700, val loss: 0.9108066558837891
Epoch 710, training loss: 0.013586511835455894 = 0.006753826513886452 + 0.001 * 6.832685470581055
Epoch 710, val loss: 0.916710376739502
Epoch 720, training loss: 0.01332026906311512 = 0.006472953595221043 + 0.001 * 6.847314834594727
Epoch 720, val loss: 0.9225792288780212
Epoch 730, training loss: 0.013040435500442982 = 0.006209512706845999 + 0.001 * 6.830922603607178
Epoch 730, val loss: 0.9283241033554077
Epoch 740, training loss: 0.012839792296290398 = 0.00596218416467309 + 0.001 * 6.877607345581055
Epoch 740, val loss: 0.933901846408844
Epoch 750, training loss: 0.012564904987812042 = 0.0057300119660794735 + 0.001 * 6.834892749786377
Epoch 750, val loss: 0.9394791722297668
Epoch 760, training loss: 0.012330576777458191 = 0.00551173510029912 + 0.001 * 6.818841934204102
Epoch 760, val loss: 0.9448569416999817
Epoch 770, training loss: 0.012142179533839226 = 0.0053063188679516315 + 0.001 * 6.835860729217529
Epoch 770, val loss: 0.9501596093177795
Epoch 780, training loss: 0.011942183598876 = 0.005112950224429369 + 0.001 * 6.829233646392822
Epoch 780, val loss: 0.9553297162055969
Epoch 790, training loss: 0.011764507740736008 = 0.004930590745061636 + 0.001 * 6.833916664123535
Epoch 790, val loss: 0.9604715704917908
Epoch 800, training loss: 0.011572830379009247 = 0.0047585382126271725 + 0.001 * 6.814291954040527
Epoch 800, val loss: 0.965462327003479
Epoch 810, training loss: 0.011401493102312088 = 0.004595981910824776 + 0.001 * 6.805510520935059
Epoch 810, val loss: 0.9703390598297119
Epoch 820, training loss: 0.011257770471274853 = 0.004442388657480478 + 0.001 * 6.8153815269470215
Epoch 820, val loss: 0.9751574397087097
Epoch 830, training loss: 0.011103738099336624 = 0.0042970869690179825 + 0.001 * 6.806650638580322
Epoch 830, val loss: 0.9798339605331421
Epoch 840, training loss: 0.010958684608340263 = 0.004159399773925543 + 0.001 * 6.799283981323242
Epoch 840, val loss: 0.9844806790351868
Epoch 850, training loss: 0.010852191597223282 = 0.004028963390737772 + 0.001 * 6.823227405548096
Epoch 850, val loss: 0.9890403747558594
Epoch 860, training loss: 0.010701527819037437 = 0.0039053400978446007 + 0.001 * 6.796187877655029
Epoch 860, val loss: 0.9934349656105042
Epoch 870, training loss: 0.01058219000697136 = 0.003787923138588667 + 0.001 * 6.794266223907471
Epoch 870, val loss: 0.9977644085884094
Epoch 880, training loss: 0.010500692762434483 = 0.0036763411480933428 + 0.001 * 6.8243513107299805
Epoch 880, val loss: 1.0020500421524048
Epoch 890, training loss: 0.010366246104240417 = 0.003570297732949257 + 0.001 * 6.795948505401611
Epoch 890, val loss: 1.0062568187713623
Epoch 900, training loss: 0.010268068872392178 = 0.003469350514933467 + 0.001 * 6.798717975616455
Epoch 900, val loss: 1.0102895498275757
Epoch 910, training loss: 0.010164170525968075 = 0.0033732035662978888 + 0.001 * 6.790966510772705
Epoch 910, val loss: 1.0143861770629883
Epoch 920, training loss: 0.010066606104373932 = 0.003281583543866873 + 0.001 * 6.785022258758545
Epoch 920, val loss: 1.0183144807815552
Epoch 930, training loss: 0.009979771450161934 = 0.0031941866036504507 + 0.001 * 6.785584926605225
Epoch 930, val loss: 1.0221959352493286
Epoch 940, training loss: 0.009899352677166462 = 0.003110727993771434 + 0.001 * 6.788624286651611
Epoch 940, val loss: 1.0259850025177002
Epoch 950, training loss: 0.009799872525036335 = 0.0030309793073683977 + 0.001 * 6.768893241882324
Epoch 950, val loss: 1.029694676399231
Epoch 960, training loss: 0.00972818210721016 = 0.0029547582380473614 + 0.001 * 6.773423194885254
Epoch 960, val loss: 1.033393144607544
Epoch 970, training loss: 0.00965170282870531 = 0.0028818626888096333 + 0.001 * 6.769839763641357
Epoch 970, val loss: 1.036986231803894
Epoch 980, training loss: 0.009600795805454254 = 0.0028120740316808224 + 0.001 * 6.788722038269043
Epoch 980, val loss: 1.0405070781707764
Epoch 990, training loss: 0.009529588744044304 = 0.002745274920016527 + 0.001 * 6.784313201904297
Epoch 990, val loss: 1.0440150499343872
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.6089
Flip ASR: 0.5289/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.957872986793518 = 1.9494991302490234 + 0.001 * 8.373852729797363
Epoch 0, val loss: 1.9412871599197388
Epoch 10, training loss: 1.9477715492248535 = 1.9393976926803589 + 0.001 * 8.37380313873291
Epoch 10, val loss: 1.9312235116958618
Epoch 20, training loss: 1.9354212284088135 = 1.927047610282898 + 0.001 * 8.373566627502441
Epoch 20, val loss: 1.918371558189392
Epoch 30, training loss: 1.9178121089935303 = 1.9094390869140625 + 0.001 * 8.373030662536621
Epoch 30, val loss: 1.8995262384414673
Epoch 40, training loss: 1.8912737369537354 = 1.882901906967163 + 0.001 * 8.371814727783203
Epoch 40, val loss: 1.8710811138153076
Epoch 50, training loss: 1.8530930280685425 = 1.844724416732788 + 0.001 * 8.368663787841797
Epoch 50, val loss: 1.8318979740142822
Epoch 60, training loss: 1.807018518447876 = 1.7986609935760498 + 0.001 * 8.357548713684082
Epoch 60, val loss: 1.7888572216033936
Epoch 70, training loss: 1.7616842985153198 = 1.7533884048461914 + 0.001 * 8.295878410339355
Epoch 70, val loss: 1.7518688440322876
Epoch 80, training loss: 1.7071588039398193 = 1.6992840766906738 + 0.001 * 7.874743938446045
Epoch 80, val loss: 1.708027958869934
Epoch 90, training loss: 1.6337025165557861 = 1.6260720491409302 + 0.001 * 7.630414962768555
Epoch 90, val loss: 1.6456278562545776
Epoch 100, training loss: 1.539036750793457 = 1.5315791368484497 + 0.001 * 7.457660675048828
Epoch 100, val loss: 1.565368413925171
Epoch 110, training loss: 1.4307007789611816 = 1.4233371019363403 + 0.001 * 7.3637213706970215
Epoch 110, val loss: 1.4785915613174438
Epoch 120, training loss: 1.3207980394363403 = 1.3135080337524414 + 0.001 * 7.290027141571045
Epoch 120, val loss: 1.3939054012298584
Epoch 130, training loss: 1.2166978120803833 = 1.2094920873641968 + 0.001 * 7.205774307250977
Epoch 130, val loss: 1.3170973062515259
Epoch 140, training loss: 1.1208808422088623 = 1.1137311458587646 + 0.001 * 7.149712085723877
Epoch 140, val loss: 1.2469755411148071
Epoch 150, training loss: 1.0341591835021973 = 1.0270347595214844 + 0.001 * 7.124444007873535
Epoch 150, val loss: 1.1839640140533447
Epoch 160, training loss: 0.9556194543838501 = 0.9485098123550415 + 0.001 * 7.109643459320068
Epoch 160, val loss: 1.1271865367889404
Epoch 170, training loss: 0.8833248019218445 = 0.876227855682373 + 0.001 * 7.09696102142334
Epoch 170, val loss: 1.0768170356750488
Epoch 180, training loss: 0.8145232200622559 = 0.8074390292167664 + 0.001 * 7.084209442138672
Epoch 180, val loss: 1.030659556388855
Epoch 190, training loss: 0.747533917427063 = 0.7404636740684509 + 0.001 * 7.070261001586914
Epoch 190, val loss: 0.9867962002754211
Epoch 200, training loss: 0.6826770305633545 = 0.6756205558776855 + 0.001 * 7.056478977203369
Epoch 200, val loss: 0.9449482560157776
Epoch 210, training loss: 0.6217390894889832 = 0.6146972179412842 + 0.001 * 7.041855335235596
Epoch 210, val loss: 0.9067879319190979
Epoch 220, training loss: 0.5663139224052429 = 0.5592812895774841 + 0.001 * 7.032658576965332
Epoch 220, val loss: 0.8733271360397339
Epoch 230, training loss: 0.5166647434234619 = 0.5096414685249329 + 0.001 * 7.023290157318115
Epoch 230, val loss: 0.8449476957321167
Epoch 240, training loss: 0.4719066023826599 = 0.46489036083221436 + 0.001 * 7.016232967376709
Epoch 240, val loss: 0.8219479322433472
Epoch 250, training loss: 0.4307670295238495 = 0.4237552881240845 + 0.001 * 7.0117363929748535
Epoch 250, val loss: 0.8039512634277344
Epoch 260, training loss: 0.39206403493881226 = 0.3850589692592621 + 0.001 * 7.005066394805908
Epoch 260, val loss: 0.7898771166801453
Epoch 270, training loss: 0.35510075092315674 = 0.34810230135917664 + 0.001 * 6.99845027923584
Epoch 270, val loss: 0.7785481214523315
Epoch 280, training loss: 0.3196762204170227 = 0.31267333030700684 + 0.001 * 7.0028910636901855
Epoch 280, val loss: 0.7696162462234497
Epoch 290, training loss: 0.2859279215335846 = 0.2789338529109955 + 0.001 * 6.994072914123535
Epoch 290, val loss: 0.7632230520248413
Epoch 300, training loss: 0.2542218863964081 = 0.24724030494689941 + 0.001 * 6.981585502624512
Epoch 300, val loss: 0.7590087652206421
Epoch 310, training loss: 0.22492754459381104 = 0.217952162027359 + 0.001 * 6.975389003753662
Epoch 310, val loss: 0.7571068406105042
Epoch 320, training loss: 0.19877862930297852 = 0.19179894030094147 + 0.001 * 6.979695796966553
Epoch 320, val loss: 0.7578068375587463
Epoch 330, training loss: 0.17577548325061798 = 0.1688082069158554 + 0.001 * 6.967282295227051
Epoch 330, val loss: 0.761315643787384
Epoch 340, training loss: 0.1556987166404724 = 0.14874067902565002 + 0.001 * 6.95803165435791
Epoch 340, val loss: 0.7677163481712341
Epoch 350, training loss: 0.13827773928642273 = 0.13131698966026306 + 0.001 * 6.960745334625244
Epoch 350, val loss: 0.776724100112915
Epoch 360, training loss: 0.12317438423633575 = 0.11622422933578491 + 0.001 * 6.9501543045043945
Epoch 360, val loss: 0.7879515290260315
Epoch 370, training loss: 0.11012719571590424 = 0.1031813994050026 + 0.001 * 6.945796012878418
Epoch 370, val loss: 0.8010493516921997
Epoch 380, training loss: 0.09884099662303925 = 0.09189947694540024 + 0.001 * 6.941520690917969
Epoch 380, val loss: 0.8156979084014893
Epoch 390, training loss: 0.0890459269285202 = 0.08211173862218857 + 0.001 * 6.934190273284912
Epoch 390, val loss: 0.8315448760986328
Epoch 400, training loss: 0.08051903545856476 = 0.07358045130968094 + 0.001 * 6.938584804534912
Epoch 400, val loss: 0.8483477234840393
Epoch 410, training loss: 0.07304215431213379 = 0.06611055880784988 + 0.001 * 6.931594371795654
Epoch 410, val loss: 0.8659026622772217
Epoch 420, training loss: 0.06647823750972748 = 0.05955220386385918 + 0.001 * 6.9260334968566895
Epoch 420, val loss: 0.884001612663269
Epoch 430, training loss: 0.06071208044886589 = 0.05377054214477539 + 0.001 * 6.941538333892822
Epoch 430, val loss: 0.9025943279266357
Epoch 440, training loss: 0.055592797696590424 = 0.04866872727870941 + 0.001 * 6.924071788787842
Epoch 440, val loss: 0.9216152429580688
Epoch 450, training loss: 0.05107052996754646 = 0.044151049107313156 + 0.001 * 6.9194793701171875
Epoch 450, val loss: 0.940902054309845
Epoch 460, training loss: 0.04708075895905495 = 0.04014232009649277 + 0.001 * 6.938438415527344
Epoch 460, val loss: 0.9602571725845337
Epoch 470, training loss: 0.04350000247359276 = 0.03658263012766838 + 0.001 * 6.917372703552246
Epoch 470, val loss: 0.9796180129051208
Epoch 480, training loss: 0.04033711180090904 = 0.03341975063085556 + 0.001 * 6.917359352111816
Epoch 480, val loss: 0.9988836050033569
Epoch 490, training loss: 0.0375201553106308 = 0.03060351312160492 + 0.001 * 6.916640281677246
Epoch 490, val loss: 1.017824649810791
Epoch 500, training loss: 0.035012152045965195 = 0.028096891939640045 + 0.001 * 6.915261268615723
Epoch 500, val loss: 1.0363389253616333
Epoch 510, training loss: 0.03276774287223816 = 0.025864461436867714 + 0.001 * 6.9032793045043945
Epoch 510, val loss: 1.054369330406189
Epoch 520, training loss: 0.030778296291828156 = 0.02387368679046631 + 0.001 * 6.904609680175781
Epoch 520, val loss: 1.07191002368927
Epoch 530, training loss: 0.029004715383052826 = 0.022095484659075737 + 0.001 * 6.909229755401611
Epoch 530, val loss: 1.0888762474060059
Epoch 540, training loss: 0.027411632239818573 = 0.02050250954926014 + 0.001 * 6.909122467041016
Epoch 540, val loss: 1.1052255630493164
Epoch 550, training loss: 0.025966379791498184 = 0.019072357565164566 + 0.001 * 6.894021034240723
Epoch 550, val loss: 1.1210829019546509
Epoch 560, training loss: 0.024717634543776512 = 0.01778383180499077 + 0.001 * 6.933802127838135
Epoch 560, val loss: 1.1363378763198853
Epoch 570, training loss: 0.023511068895459175 = 0.016620147973299026 + 0.001 * 6.890921115875244
Epoch 570, val loss: 1.1511050462722778
Epoch 580, training loss: 0.022460751235485077 = 0.015566090121865273 + 0.001 * 6.8946614265441895
Epoch 580, val loss: 1.165377140045166
Epoch 590, training loss: 0.02150445058941841 = 0.01460874080657959 + 0.001 * 6.895709037780762
Epoch 590, val loss: 1.1791846752166748
Epoch 600, training loss: 0.020635858178138733 = 0.013737431727349758 + 0.001 * 6.898426532745361
Epoch 600, val loss: 1.1925636529922485
Epoch 610, training loss: 0.019832300022244453 = 0.012942529283463955 + 0.001 * 6.8897705078125
Epoch 610, val loss: 1.2055243253707886
Epoch 620, training loss: 0.019092828035354614 = 0.01221555657684803 + 0.001 * 6.877270221710205
Epoch 620, val loss: 1.2180769443511963
Epoch 630, training loss: 0.01842050999403 = 0.01154913567006588 + 0.001 * 6.871374607086182
Epoch 630, val loss: 1.2302314043045044
Epoch 640, training loss: 0.01783197931945324 = 0.010937037877738476 + 0.001 * 6.8949408531188965
Epoch 640, val loss: 1.242019534111023
Epoch 650, training loss: 0.017249591648578644 = 0.010373626835644245 + 0.001 * 6.875964641571045
Epoch 650, val loss: 1.2534565925598145
Epoch 660, training loss: 0.016718965023756027 = 0.009854001924395561 + 0.001 * 6.864963054656982
Epoch 660, val loss: 1.264554738998413
Epoch 670, training loss: 0.016247153282165527 = 0.00937387440353632 + 0.001 * 6.873279094696045
Epoch 670, val loss: 1.2753281593322754
Epoch 680, training loss: 0.01582670584321022 = 0.008929373696446419 + 0.001 * 6.897332191467285
Epoch 680, val loss: 1.2858130931854248
Epoch 690, training loss: 0.015376133844256401 = 0.008517120964825153 + 0.001 * 6.859012126922607
Epoch 690, val loss: 1.2960151433944702
Epoch 700, training loss: 0.014997129328548908 = 0.008134010247886181 + 0.001 * 6.863118648529053
Epoch 700, val loss: 1.305930256843567
Epoch 710, training loss: 0.014639434404671192 = 0.007777401711791754 + 0.001 * 6.862032413482666
Epoch 710, val loss: 1.3155995607376099
Epoch 720, training loss: 0.014298928901553154 = 0.007444638293236494 + 0.001 * 6.85429048538208
Epoch 720, val loss: 1.324997901916504
Epoch 730, training loss: 0.0139794135466218 = 0.007133179344236851 + 0.001 * 6.84623384475708
Epoch 730, val loss: 1.3341847658157349
Epoch 740, training loss: 0.013687018305063248 = 0.006840263959020376 + 0.001 * 6.8467535972595215
Epoch 740, val loss: 1.3431886434555054
Epoch 750, training loss: 0.013403113931417465 = 0.006563625764101744 + 0.001 * 6.839487552642822
Epoch 750, val loss: 1.352036952972412
Epoch 760, training loss: 0.013140547089278698 = 0.006301657762378454 + 0.001 * 6.838889122009277
Epoch 760, val loss: 1.3607468605041504
Epoch 770, training loss: 0.012893706560134888 = 0.0060536302626132965 + 0.001 * 6.840075492858887
Epoch 770, val loss: 1.3692947626113892
Epoch 780, training loss: 0.012662055902183056 = 0.005818817764520645 + 0.001 * 6.84323787689209
Epoch 780, val loss: 1.3777165412902832
Epoch 790, training loss: 0.012422692030668259 = 0.005596605129539967 + 0.001 * 6.82608699798584
Epoch 790, val loss: 1.3859964609146118
Epoch 800, training loss: 0.012231385335326195 = 0.005386454984545708 + 0.001 * 6.844930648803711
Epoch 800, val loss: 1.3941055536270142
Epoch 810, training loss: 0.012023558840155602 = 0.005187479313462973 + 0.001 * 6.836079120635986
Epoch 810, val loss: 1.4020988941192627
Epoch 820, training loss: 0.011845052242279053 = 0.004999174270778894 + 0.001 * 6.845877647399902
Epoch 820, val loss: 1.4099252223968506
Epoch 830, training loss: 0.011634621769189835 = 0.004820939153432846 + 0.001 * 6.813681602478027
Epoch 830, val loss: 1.4176007509231567
Epoch 840, training loss: 0.011485615745186806 = 0.004652209579944611 + 0.001 * 6.8334059715271
Epoch 840, val loss: 1.4251383543014526
Epoch 850, training loss: 0.011319911107420921 = 0.004492481704801321 + 0.001 * 6.827428817749023
Epoch 850, val loss: 1.4325053691864014
Epoch 860, training loss: 0.011159236542880535 = 0.00434114271774888 + 0.001 * 6.818093299865723
Epoch 860, val loss: 1.4397530555725098
Epoch 870, training loss: 0.010997578501701355 = 0.004197792615741491 + 0.001 * 6.799785137176514
Epoch 870, val loss: 1.446841835975647
Epoch 880, training loss: 0.010875213891267776 = 0.004061997402459383 + 0.001 * 6.813216209411621
Epoch 880, val loss: 1.4537901878356934
Epoch 890, training loss: 0.010713459923863411 = 0.003933131229132414 + 0.001 * 6.780328750610352
Epoch 890, val loss: 1.4606164693832397
Epoch 900, training loss: 0.01066529005765915 = 0.003810727968811989 + 0.001 * 6.854561805725098
Epoch 900, val loss: 1.467293381690979
Epoch 910, training loss: 0.010494548827409744 = 0.0036944448947906494 + 0.001 * 6.800104141235352
Epoch 910, val loss: 1.4738540649414062
Epoch 920, training loss: 0.010375382378697395 = 0.0035839052870869637 + 0.001 * 6.791476249694824
Epoch 920, val loss: 1.4802695512771606
Epoch 930, training loss: 0.010262606665492058 = 0.0034788278862833977 + 0.001 * 6.783779144287109
Epoch 930, val loss: 1.486581802368164
Epoch 940, training loss: 0.010161329992115498 = 0.003378880675882101 + 0.001 * 6.782448768615723
Epoch 940, val loss: 1.4927347898483276
Epoch 950, training loss: 0.01011194009333849 = 0.0032837397884577513 + 0.001 * 6.828199863433838
Epoch 950, val loss: 1.498769760131836
Epoch 960, training loss: 0.009975964203476906 = 0.003193124895915389 + 0.001 * 6.782839298248291
Epoch 960, val loss: 1.504702091217041
Epoch 970, training loss: 0.009862995706498623 = 0.0031067063100636005 + 0.001 * 6.756289005279541
Epoch 970, val loss: 1.5104999542236328
Epoch 980, training loss: 0.009773876518011093 = 0.003024279372766614 + 0.001 * 6.749597072601318
Epoch 980, val loss: 1.5161876678466797
Epoch 990, training loss: 0.009713884443044662 = 0.0029456245247274637 + 0.001 * 6.7682600021362305
Epoch 990, val loss: 1.5217828750610352
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7638
Flip ASR: 0.7244/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.962701439857483 = 1.9543275833129883 + 0.001 * 8.373913764953613
Epoch 0, val loss: 1.9559515714645386
Epoch 10, training loss: 1.9523921012878418 = 1.9440182447433472 + 0.001 * 8.373865127563477
Epoch 10, val loss: 1.9449384212493896
Epoch 20, training loss: 1.939886212348938 = 1.931512475013733 + 0.001 * 8.373710632324219
Epoch 20, val loss: 1.9312307834625244
Epoch 30, training loss: 1.9224910736083984 = 1.914117693901062 + 0.001 * 8.373395919799805
Epoch 30, val loss: 1.9119207859039307
Epoch 40, training loss: 1.8969568014144897 = 1.888584017753601 + 0.001 * 8.372742652893066
Epoch 40, val loss: 1.8837882280349731
Epoch 50, training loss: 1.8604763746261597 = 1.8521052598953247 + 0.001 * 8.37112045288086
Epoch 50, val loss: 1.8452205657958984
Epoch 60, training loss: 1.8153797388076782 = 1.8070141077041626 + 0.001 * 8.365615844726562
Epoch 60, val loss: 1.8018335103988647
Epoch 70, training loss: 1.7708779573440552 = 1.7625375986099243 + 0.001 * 8.340407371520996
Epoch 70, val loss: 1.7653015851974487
Epoch 80, training loss: 1.7206262350082397 = 1.712449073791504 + 0.001 * 8.17714786529541
Epoch 80, val loss: 1.7259416580200195
Epoch 90, training loss: 1.653443455696106 = 1.6455708742141724 + 0.001 * 7.872531890869141
Epoch 90, val loss: 1.672003984451294
Epoch 100, training loss: 1.5660895109176636 = 1.5585540533065796 + 0.001 * 7.535426616668701
Epoch 100, val loss: 1.6002360582351685
Epoch 110, training loss: 1.461734414100647 = 1.4544371366500854 + 0.001 * 7.297248840332031
Epoch 110, val loss: 1.514708399772644
Epoch 120, training loss: 1.3477181196212769 = 1.3404563665390015 + 0.001 * 7.261798858642578
Epoch 120, val loss: 1.422565221786499
Epoch 130, training loss: 1.2287148237228394 = 1.221474528312683 + 0.001 * 7.240272521972656
Epoch 130, val loss: 1.3282179832458496
Epoch 140, training loss: 1.1091561317443848 = 1.1019331216812134 + 0.001 * 7.223063945770264
Epoch 140, val loss: 1.2334067821502686
Epoch 150, training loss: 0.9943001866340637 = 0.9870923161506653 + 0.001 * 7.207848072052002
Epoch 150, val loss: 1.1419477462768555
Epoch 160, training loss: 0.8885641694068909 = 0.8813696503639221 + 0.001 * 7.19452428817749
Epoch 160, val loss: 1.0573129653930664
Epoch 170, training loss: 0.7951954007148743 = 0.7880167365074158 + 0.001 * 7.17864990234375
Epoch 170, val loss: 0.9828623533248901
Epoch 180, training loss: 0.7150522470474243 = 0.7078933715820312 + 0.001 * 7.15889310836792
Epoch 180, val loss: 0.9198316335678101
Epoch 190, training loss: 0.6463279724121094 = 0.6391943693161011 + 0.001 * 7.133592128753662
Epoch 190, val loss: 0.8676793575286865
Epoch 200, training loss: 0.5862139463424683 = 0.5791137218475342 + 0.001 * 7.100250720977783
Epoch 200, val loss: 0.8247326612472534
Epoch 210, training loss: 0.5322152972221375 = 0.525140643119812 + 0.001 * 7.074639320373535
Epoch 210, val loss: 0.7891258001327515
Epoch 220, training loss: 0.4827639162540436 = 0.47571444511413574 + 0.001 * 7.049470901489258
Epoch 220, val loss: 0.7599232792854309
Epoch 230, training loss: 0.43690094351768494 = 0.4298643171787262 + 0.001 * 7.0366315841674805
Epoch 230, val loss: 0.7366938591003418
Epoch 240, training loss: 0.3939308226108551 = 0.3869054615497589 + 0.001 * 7.025352954864502
Epoch 240, val loss: 0.719234049320221
Epoch 250, training loss: 0.353436678647995 = 0.3464232385158539 + 0.001 * 7.013439655303955
Epoch 250, val loss: 0.707104504108429
Epoch 260, training loss: 0.315381795167923 = 0.30837899446487427 + 0.001 * 7.002786636352539
Epoch 260, val loss: 0.6992154121398926
Epoch 270, training loss: 0.28000572323799133 = 0.2730133831501007 + 0.001 * 6.992335319519043
Epoch 270, val loss: 0.6948986649513245
Epoch 280, training loss: 0.24775734543800354 = 0.24077413976192474 + 0.001 * 6.9832048416137695
Epoch 280, val loss: 0.6936026215553284
Epoch 290, training loss: 0.21906697750091553 = 0.21208937466144562 + 0.001 * 6.977596282958984
Epoch 290, val loss: 0.6954752206802368
Epoch 300, training loss: 0.19401617348194122 = 0.18704207241535187 + 0.001 * 6.974096298217773
Epoch 300, val loss: 0.7003337740898132
Epoch 310, training loss: 0.17237094044685364 = 0.16539707779884338 + 0.001 * 6.973857402801514
Epoch 310, val loss: 0.7076509594917297
Epoch 320, training loss: 0.1537216603755951 = 0.14675219357013702 + 0.001 * 6.969460964202881
Epoch 320, val loss: 0.7169199585914612
Epoch 330, training loss: 0.13763871788978577 = 0.1306716799736023 + 0.001 * 6.967031955718994
Epoch 330, val loss: 0.7276937961578369
Epoch 340, training loss: 0.12371183931827545 = 0.1167430579662323 + 0.001 * 6.96877908706665
Epoch 340, val loss: 0.7395408749580383
Epoch 350, training loss: 0.11158598214387894 = 0.10461931675672531 + 0.001 * 6.966663360595703
Epoch 350, val loss: 0.7520411610603333
Epoch 360, training loss: 0.10098770260810852 = 0.09402137249708176 + 0.001 * 6.966330051422119
Epoch 360, val loss: 0.7649614810943604
Epoch 370, training loss: 0.09168342500925064 = 0.0847170501947403 + 0.001 * 6.966373443603516
Epoch 370, val loss: 0.7781311869621277
Epoch 380, training loss: 0.0834854394197464 = 0.07651989907026291 + 0.001 * 6.965543746948242
Epoch 380, val loss: 0.791408360004425
Epoch 390, training loss: 0.07624164968729019 = 0.06927649676799774 + 0.001 * 6.965153217315674
Epoch 390, val loss: 0.8047612309455872
Epoch 400, training loss: 0.06982719153165817 = 0.06286070495843887 + 0.001 * 6.966485977172852
Epoch 400, val loss: 0.8181148767471313
Epoch 410, training loss: 0.06412903964519501 = 0.05716294050216675 + 0.001 * 6.966101169586182
Epoch 410, val loss: 0.8313604593276978
Epoch 420, training loss: 0.05905747786164284 = 0.05209241434931755 + 0.001 * 6.965063571929932
Epoch 420, val loss: 0.8444076180458069
Epoch 430, training loss: 0.054535873234272 = 0.047571416944265366 + 0.001 * 6.964456081390381
Epoch 430, val loss: 0.85723876953125
Epoch 440, training loss: 0.05049818009138107 = 0.0435321070253849 + 0.001 * 6.966071128845215
Epoch 440, val loss: 0.8698102235794067
Epoch 450, training loss: 0.046881649643182755 = 0.039916034787893295 + 0.001 * 6.965614318847656
Epoch 450, val loss: 0.8820576071739197
Epoch 460, training loss: 0.0436352863907814 = 0.03667224571108818 + 0.001 * 6.963042259216309
Epoch 460, val loss: 0.8939631581306458
Epoch 470, training loss: 0.040743086487054825 = 0.03375597298145294 + 0.001 * 6.987112045288086
Epoch 470, val loss: 0.9055079817771912
Epoch 480, training loss: 0.03809785097837448 = 0.03112873248755932 + 0.001 * 6.969117164611816
Epoch 480, val loss: 0.9167287945747375
Epoch 490, training loss: 0.03572141379117966 = 0.028756296262145042 + 0.001 * 6.965117454528809
Epoch 490, val loss: 0.9275581240653992
Epoch 500, training loss: 0.03357226029038429 = 0.026609888300299644 + 0.001 * 6.962372303009033
Epoch 500, val loss: 0.9380298256874084
Epoch 510, training loss: 0.03162550553679466 = 0.024664469063282013 + 0.001 * 6.96103572845459
Epoch 510, val loss: 0.9481807947158813
Epoch 520, training loss: 0.029857750982046127 = 0.022897588089108467 + 0.001 * 6.960162162780762
Epoch 520, val loss: 0.9579839706420898
Epoch 530, training loss: 0.028254996985197067 = 0.021290145814418793 + 0.001 * 6.964850902557373
Epoch 530, val loss: 0.9674350023269653
Epoch 540, training loss: 0.026784352958202362 = 0.019825493916869164 + 0.001 * 6.958858013153076
Epoch 540, val loss: 0.9765082597732544
Epoch 550, training loss: 0.025449207052588463 = 0.01848866604268551 + 0.001 * 6.960541248321533
Epoch 550, val loss: 0.9852794408798218
Epoch 560, training loss: 0.02422531135380268 = 0.017266925424337387 + 0.001 * 6.958385944366455
Epoch 560, val loss: 0.9937619566917419
Epoch 570, training loss: 0.023127442225813866 = 0.016148999333381653 + 0.001 * 6.978443145751953
Epoch 570, val loss: 1.0019179582595825
Epoch 580, training loss: 0.02208784781396389 = 0.015125063247978687 + 0.001 * 6.962784290313721
Epoch 580, val loss: 1.0097894668579102
Epoch 590, training loss: 0.02114216983318329 = 0.014186417683959007 + 0.001 * 6.955751419067383
Epoch 590, val loss: 1.0174165964126587
Epoch 600, training loss: 0.02027939260005951 = 0.013324993662536144 + 0.001 * 6.9543986320495605
Epoch 600, val loss: 1.0247611999511719
Epoch 610, training loss: 0.01948634535074234 = 0.012533742003142834 + 0.001 * 6.952603816986084
Epoch 610, val loss: 1.031839370727539
Epoch 620, training loss: 0.018760280683636665 = 0.01180627103894949 + 0.001 * 6.954010009765625
Epoch 620, val loss: 1.0386886596679688
Epoch 630, training loss: 0.018098363652825356 = 0.011136709712445736 + 0.001 * 6.961653232574463
Epoch 630, val loss: 1.045249104499817
Epoch 640, training loss: 0.017473876476287842 = 0.01051980908960104 + 0.001 * 6.954067707061768
Epoch 640, val loss: 1.0515893697738647
Epoch 650, training loss: 0.01691422425210476 = 0.009950829669833183 + 0.001 * 6.963394641876221
Epoch 650, val loss: 1.0576953887939453
Epoch 660, training loss: 0.016373224556446075 = 0.009425428695976734 + 0.001 * 6.947795391082764
Epoch 660, val loss: 1.0635708570480347
Epoch 670, training loss: 0.015886034816503525 = 0.00893972348421812 + 0.001 * 6.946311950683594
Epoch 670, val loss: 1.069258451461792
Epoch 680, training loss: 0.015435049310326576 = 0.008490180596709251 + 0.001 * 6.944869041442871
Epoch 680, val loss: 1.0747605562210083
Epoch 690, training loss: 0.015073557384312153 = 0.008073695003986359 + 0.001 * 6.999862194061279
Epoch 690, val loss: 1.0800904035568237
Epoch 700, training loss: 0.014641095884144306 = 0.007687321398407221 + 0.001 * 6.9537739753723145
Epoch 700, val loss: 1.0852633714675903
Epoch 710, training loss: 0.014269858598709106 = 0.007328443229198456 + 0.001 * 6.941415309906006
Epoch 710, val loss: 1.0902167558670044
Epoch 720, training loss: 0.01393565721809864 = 0.006994706578552723 + 0.001 * 6.9409499168396
Epoch 720, val loss: 1.0950080156326294
Epoch 730, training loss: 0.013629026710987091 = 0.006683941464871168 + 0.001 * 6.945085048675537
Epoch 730, val loss: 1.09968101978302
Epoch 740, training loss: 0.013336319476366043 = 0.006394125986844301 + 0.001 * 6.942192554473877
Epoch 740, val loss: 1.1042029857635498
Epoch 750, training loss: 0.013058090582489967 = 0.0061235674656927586 + 0.001 * 6.93452262878418
Epoch 750, val loss: 1.1085747480392456
Epoch 760, training loss: 0.01280885934829712 = 0.005870681256055832 + 0.001 * 6.938177585601807
Epoch 760, val loss: 1.1128114461898804
Epoch 770, training loss: 0.012572561390697956 = 0.005634056404232979 + 0.001 * 6.938504695892334
Epoch 770, val loss: 1.1169074773788452
Epoch 780, training loss: 0.012350603938102722 = 0.005412357859313488 + 0.001 * 6.9382452964782715
Epoch 780, val loss: 1.120890498161316
Epoch 790, training loss: 0.012134429067373276 = 0.005204434972256422 + 0.001 * 6.929993629455566
Epoch 790, val loss: 1.1247748136520386
Epoch 800, training loss: 0.011982999742031097 = 0.005009178072214127 + 0.001 * 6.973821640014648
Epoch 800, val loss: 1.1285455226898193
Epoch 810, training loss: 0.011759502813220024 = 0.004825599957257509 + 0.001 * 6.933902740478516
Epoch 810, val loss: 1.1322021484375
Epoch 820, training loss: 0.01157599687576294 = 0.0046528661623597145 + 0.001 * 6.923130512237549
Epoch 820, val loss: 1.1357654333114624
Epoch 830, training loss: 0.01141199842095375 = 0.004490140825510025 + 0.001 * 6.921856880187988
Epoch 830, val loss: 1.1392375230789185
Epoch 840, training loss: 0.011267544701695442 = 0.0043366593308746815 + 0.001 * 6.93088436126709
Epoch 840, val loss: 1.1426149606704712
Epoch 850, training loss: 0.011115910485386848 = 0.004191756248474121 + 0.001 * 6.9241533279418945
Epoch 850, val loss: 1.1459214687347412
Epoch 860, training loss: 0.01096844021230936 = 0.004054847173392773 + 0.001 * 6.91359281539917
Epoch 860, val loss: 1.149127721786499
Epoch 870, training loss: 0.010863864794373512 = 0.003925342112779617 + 0.001 * 6.938522815704346
Epoch 870, val loss: 1.1522501707077026
Epoch 880, training loss: 0.010713967494666576 = 0.003802728373557329 + 0.001 * 6.911238670349121
Epoch 880, val loss: 1.155301809310913
Epoch 890, training loss: 0.010611526668071747 = 0.003686554729938507 + 0.001 * 6.924971580505371
Epoch 890, val loss: 1.1582658290863037
Epoch 900, training loss: 0.010498046875 = 0.003576344810426235 + 0.001 * 6.921701908111572
Epoch 900, val loss: 1.1611675024032593
Epoch 910, training loss: 0.010383948683738708 = 0.0034717342350631952 + 0.001 * 6.912214279174805
Epoch 910, val loss: 1.1639814376831055
Epoch 920, training loss: 0.010278834961354733 = 0.0033723360393196344 + 0.001 * 6.906498432159424
Epoch 920, val loss: 1.1667429208755493
Epoch 930, training loss: 0.010168712586164474 = 0.0032778214663267136 + 0.001 * 6.890890598297119
Epoch 930, val loss: 1.1694358587265015
Epoch 940, training loss: 0.010099636390805244 = 0.0031878859736025333 + 0.001 * 6.911749839782715
Epoch 940, val loss: 1.1720480918884277
Epoch 950, training loss: 0.010018477216362953 = 0.0031022157054394484 + 0.001 * 6.916260719299316
Epoch 950, val loss: 1.1746174097061157
Epoch 960, training loss: 0.009935107082128525 = 0.003020551521331072 + 0.001 * 6.914555549621582
Epoch 960, val loss: 1.1771267652511597
Epoch 970, training loss: 0.009853261522948742 = 0.002942655235528946 + 0.001 * 6.9106059074401855
Epoch 970, val loss: 1.1795697212219238
Epoch 980, training loss: 0.009741738438606262 = 0.00286829168908298 + 0.001 * 6.873445987701416
Epoch 980, val loss: 1.18194580078125
Epoch 990, training loss: 0.009690424427390099 = 0.0027972753159701824 + 0.001 * 6.893148422241211
Epoch 990, val loss: 1.1842938661575317
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8450
Flip ASR: 0.8178/225 nodes
The final ASR:0.73924, 0.09797, Accuracy:0.81852, 0.02095
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11604])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10512])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00627, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.94013249874115 = 1.9317586421966553 + 0.001 * 8.373900413513184
Epoch 0, val loss: 1.9321016073226929
Epoch 10, training loss: 1.931376338005066 = 1.9230024814605713 + 0.001 * 8.373861312866211
Epoch 10, val loss: 1.9242521524429321
Epoch 20, training loss: 1.9207422733306885 = 1.9123685359954834 + 0.001 * 8.373685836791992
Epoch 20, val loss: 1.9144113063812256
Epoch 30, training loss: 1.9055368900299072 = 1.8971636295318604 + 0.001 * 8.373285293579102
Epoch 30, val loss: 1.9001867771148682
Epoch 40, training loss: 1.8824284076690674 = 1.8740559816360474 + 0.001 * 8.372382164001465
Epoch 40, val loss: 1.8788634538650513
Epoch 50, training loss: 1.8487825393676758 = 1.8404124975204468 + 0.001 * 8.369988441467285
Epoch 50, val loss: 1.8490042686462402
Epoch 60, training loss: 1.8065046072006226 = 1.7981427907943726 + 0.001 * 8.361777305603027
Epoch 60, val loss: 1.813901662826538
Epoch 70, training loss: 1.7619829177856445 = 1.7536611557006836 + 0.001 * 8.32176399230957
Epoch 70, val loss: 1.7775347232818604
Epoch 80, training loss: 1.7064416408538818 = 1.6984018087387085 + 0.001 * 8.039837837219238
Epoch 80, val loss: 1.7284746170043945
Epoch 90, training loss: 1.629800796508789 = 1.6219905614852905 + 0.001 * 7.810206890106201
Epoch 90, val loss: 1.6632575988769531
Epoch 100, training loss: 1.5319395065307617 = 1.524264931678772 + 0.001 * 7.674519062042236
Epoch 100, val loss: 1.5841578245162964
Epoch 110, training loss: 1.4205657243728638 = 1.413022518157959 + 0.001 * 7.543217658996582
Epoch 110, val loss: 1.4930460453033447
Epoch 120, training loss: 1.3080044984817505 = 1.3005033731460571 + 0.001 * 7.5010786056518555
Epoch 120, val loss: 1.4022815227508545
Epoch 130, training loss: 1.2013174295425415 = 1.1938636302947998 + 0.001 * 7.453798770904541
Epoch 130, val loss: 1.315956950187683
Epoch 140, training loss: 1.1046870946884155 = 1.0973198413848877 + 0.001 * 7.367215633392334
Epoch 140, val loss: 1.2398611307144165
Epoch 150, training loss: 1.0192025899887085 = 1.0119426250457764 + 0.001 * 7.26001501083374
Epoch 150, val loss: 1.1743525266647339
Epoch 160, training loss: 0.9425911903381348 = 0.9353587627410889 + 0.001 * 7.232429504394531
Epoch 160, val loss: 1.117639183998108
Epoch 170, training loss: 0.8711684942245483 = 0.8639378547668457 + 0.001 * 7.2306342124938965
Epoch 170, val loss: 1.0660945177078247
Epoch 180, training loss: 0.801913321018219 = 0.7946962714195251 + 0.001 * 7.2170538902282715
Epoch 180, val loss: 1.0161230564117432
Epoch 190, training loss: 0.7331995368003845 = 0.7259894013404846 + 0.001 * 7.210143089294434
Epoch 190, val loss: 0.9664433002471924
Epoch 200, training loss: 0.6645030379295349 = 0.6572996973991394 + 0.001 * 7.203343391418457
Epoch 200, val loss: 0.917396605014801
Epoch 210, training loss: 0.5967977046966553 = 0.5896009802818298 + 0.001 * 7.196722030639648
Epoch 210, val loss: 0.8700903058052063
Epoch 220, training loss: 0.5322467088699341 = 0.5250566005706787 + 0.001 * 7.190112590789795
Epoch 220, val loss: 0.8268537521362305
Epoch 230, training loss: 0.473125159740448 = 0.4659414291381836 + 0.001 * 7.1837158203125
Epoch 230, val loss: 0.7902472019195557
Epoch 240, training loss: 0.4204397201538086 = 0.4132627546787262 + 0.001 * 7.176950454711914
Epoch 240, val loss: 0.7613160014152527
Epoch 250, training loss: 0.3737369775772095 = 0.3665676414966583 + 0.001 * 7.169343948364258
Epoch 250, val loss: 0.7389872074127197
Epoch 260, training loss: 0.33176282048225403 = 0.3246018886566162 + 0.001 * 7.160934925079346
Epoch 260, val loss: 0.7214335203170776
Epoch 270, training loss: 0.2933661937713623 = 0.28621670603752136 + 0.001 * 7.149480819702148
Epoch 270, val loss: 0.7071398496627808
Epoch 280, training loss: 0.2580290138721466 = 0.25089338421821594 + 0.001 * 7.1356425285339355
Epoch 280, val loss: 0.6958324313163757
Epoch 290, training loss: 0.2258201241493225 = 0.21869006752967834 + 0.001 * 7.130058288574219
Epoch 290, val loss: 0.6876725554466248
Epoch 300, training loss: 0.19699661433696747 = 0.18989557027816772 + 0.001 * 7.101046085357666
Epoch 300, val loss: 0.6829960942268372
Epoch 310, training loss: 0.17173656821250916 = 0.164646714925766 + 0.001 * 7.089848041534424
Epoch 310, val loss: 0.6820574402809143
Epoch 320, training loss: 0.14986994862556458 = 0.142796128988266 + 0.001 * 7.073825359344482
Epoch 320, val loss: 0.6848518252372742
Epoch 330, training loss: 0.13110245764255524 = 0.12403596937656403 + 0.001 * 7.0664825439453125
Epoch 330, val loss: 0.6909750699996948
Epoch 340, training loss: 0.11503417044878006 = 0.10799843817949295 + 0.001 * 7.035728454589844
Epoch 340, val loss: 0.6997852921485901
Epoch 350, training loss: 0.10135763138532639 = 0.09432622045278549 + 0.001 * 7.031407356262207
Epoch 350, val loss: 0.710537314414978
Epoch 360, training loss: 0.08971447497606277 = 0.08268048614263535 + 0.001 * 7.033989906311035
Epoch 360, val loss: 0.7226110100746155
Epoch 370, training loss: 0.07976990938186646 = 0.07275760918855667 + 0.001 * 7.012296199798584
Epoch 370, val loss: 0.7354466319084167
Epoch 380, training loss: 0.07130546122789383 = 0.06429491937160492 + 0.001 * 7.0105438232421875
Epoch 380, val loss: 0.7487355470657349
Epoch 390, training loss: 0.06406081467866898 = 0.05705616623163223 + 0.001 * 7.0046515464782715
Epoch 390, val loss: 0.7622618675231934
Epoch 400, training loss: 0.05784773826599121 = 0.05084837228059769 + 0.001 * 6.9993672370910645
Epoch 400, val loss: 0.7758667469024658
Epoch 410, training loss: 0.05249663069844246 = 0.045501451939344406 + 0.001 * 6.995177745819092
Epoch 410, val loss: 0.7894371747970581
Epoch 420, training loss: 0.047877516597509384 = 0.04087655618786812 + 0.001 * 7.000960826873779
Epoch 420, val loss: 0.802918016910553
Epoch 430, training loss: 0.043850306421518326 = 0.03686218708753586 + 0.001 * 6.988118648529053
Epoch 430, val loss: 0.8162606954574585
Epoch 440, training loss: 0.04036019369959831 = 0.03336555138230324 + 0.001 * 6.9946417808532715
Epoch 440, val loss: 0.8293954133987427
Epoch 450, training loss: 0.03730121627449989 = 0.030309872701764107 + 0.001 * 6.9913434982299805
Epoch 450, val loss: 0.8423336744308472
Epoch 460, training loss: 0.03461625799536705 = 0.027632741257548332 + 0.001 * 6.983517646789551
Epoch 460, val loss: 0.855074405670166
Epoch 470, training loss: 0.03226374834775925 = 0.02527957409620285 + 0.001 * 6.984172821044922
Epoch 470, val loss: 0.8675829172134399
Epoch 480, training loss: 0.03019704297184944 = 0.023205069825053215 + 0.001 * 6.991973876953125
Epoch 480, val loss: 0.879813551902771
Epoch 490, training loss: 0.028348300606012344 = 0.02137005142867565 + 0.001 * 6.978248596191406
Epoch 490, val loss: 0.8917830586433411
Epoch 500, training loss: 0.026723479852080345 = 0.019740832969546318 + 0.001 * 6.9826459884643555
Epoch 500, val loss: 0.903464674949646
Epoch 510, training loss: 0.02527148649096489 = 0.01828991249203682 + 0.001 * 6.98157262802124
Epoch 510, val loss: 0.9148627519607544
Epoch 520, training loss: 0.02397746406495571 = 0.01699441857635975 + 0.001 * 6.983044624328613
Epoch 520, val loss: 0.925958514213562
Epoch 530, training loss: 0.022808145731687546 = 0.01583348587155342 + 0.001 * 6.974660396575928
Epoch 530, val loss: 0.9367430806159973
Epoch 540, training loss: 0.02176317200064659 = 0.01478916872292757 + 0.001 * 6.974003314971924
Epoch 540, val loss: 0.9472616910934448
Epoch 550, training loss: 0.02082250639796257 = 0.013846287503838539 + 0.001 * 6.976218223571777
Epoch 550, val loss: 0.9574692249298096
Epoch 560, training loss: 0.019984833896160126 = 0.012992621399462223 + 0.001 * 6.992211818695068
Epoch 560, val loss: 0.9674296975135803
Epoch 570, training loss: 0.019187601283192635 = 0.012218200601637363 + 0.001 * 6.969399929046631
Epoch 570, val loss: 0.977118730545044
Epoch 580, training loss: 0.01848541386425495 = 0.011513450182974339 + 0.001 * 6.971963882446289
Epoch 580, val loss: 0.9865090847015381
Epoch 590, training loss: 0.017838232219219208 = 0.010869632475078106 + 0.001 * 6.968599796295166
Epoch 590, val loss: 0.9956833720207214
Epoch 600, training loss: 0.01724511943757534 = 0.010277715511620045 + 0.001 * 6.967403888702393
Epoch 600, val loss: 1.0047099590301514
Epoch 610, training loss: 0.016697557643055916 = 0.009728933684527874 + 0.001 * 6.968623638153076
Epoch 610, val loss: 1.013679027557373
Epoch 620, training loss: 0.01618603989481926 = 0.009218194521963596 + 0.001 * 6.9678449630737305
Epoch 620, val loss: 1.0226047039031982
Epoch 630, training loss: 0.015697922557592392 = 0.008742179721593857 + 0.001 * 6.9557414054870605
Epoch 630, val loss: 1.0314791202545166
Epoch 640, training loss: 0.015259811654686928 = 0.008298404514789581 + 0.001 * 6.96140718460083
Epoch 640, val loss: 1.0402675867080688
Epoch 650, training loss: 0.01483981404453516 = 0.00788474828004837 + 0.001 * 6.9550652503967285
Epoch 650, val loss: 1.048933506011963
Epoch 660, training loss: 0.014452872797846794 = 0.007499703671783209 + 0.001 * 6.953168869018555
Epoch 660, val loss: 1.0574761629104614
Epoch 670, training loss: 0.014089258387684822 = 0.007141377776861191 + 0.001 * 6.947880744934082
Epoch 670, val loss: 1.0658490657806396
Epoch 680, training loss: 0.013751953840255737 = 0.0068078357726335526 + 0.001 * 6.944117546081543
Epoch 680, val loss: 1.0740693807601929
Epoch 690, training loss: 0.013447904959321022 = 0.006497313734143972 + 0.001 * 6.950591087341309
Epoch 690, val loss: 1.082106351852417
Epoch 700, training loss: 0.013158148154616356 = 0.006207853090018034 + 0.001 * 6.950294494628906
Epoch 700, val loss: 1.089999794960022
Epoch 710, training loss: 0.012889143079519272 = 0.005937779322266579 + 0.001 * 6.9513630867004395
Epoch 710, val loss: 1.0977184772491455
Epoch 720, training loss: 0.012623529881238937 = 0.005685565993189812 + 0.001 * 6.937963962554932
Epoch 720, val loss: 1.1052842140197754
Epoch 730, training loss: 0.012396220117807388 = 0.005449752323329449 + 0.001 * 6.946467876434326
Epoch 730, val loss: 1.112685203552246
Epoch 740, training loss: 0.012185592204332352 = 0.005229094065725803 + 0.001 * 6.956497669219971
Epoch 740, val loss: 1.1199064254760742
Epoch 750, training loss: 0.011957468464970589 = 0.005022319965064526 + 0.001 * 6.9351487159729
Epoch 750, val loss: 1.1269944906234741
Epoch 760, training loss: 0.011771636083722115 = 0.004828351084142923 + 0.001 * 6.943284511566162
Epoch 760, val loss: 1.1339194774627686
Epoch 770, training loss: 0.011581871658563614 = 0.004646214656531811 + 0.001 * 6.935656547546387
Epoch 770, val loss: 1.1406980752944946
Epoch 780, training loss: 0.011401878669857979 = 0.004475030116736889 + 0.001 * 6.926848888397217
Epoch 780, val loss: 1.1473230123519897
Epoch 790, training loss: 0.011239677667617798 = 0.0043138861656188965 + 0.001 * 6.9257917404174805
Epoch 790, val loss: 1.1538108587265015
Epoch 800, training loss: 0.011088615283370018 = 0.0041620465926826 + 0.001 * 6.926568984985352
Epoch 800, val loss: 1.1601585149765015
Epoch 810, training loss: 0.010958664119243622 = 0.004018811043351889 + 0.001 * 6.939853191375732
Epoch 810, val loss: 1.166368842124939
Epoch 820, training loss: 0.010793407447636127 = 0.0038835841696709394 + 0.001 * 6.909822940826416
Epoch 820, val loss: 1.1724494695663452
Epoch 830, training loss: 0.010678885504603386 = 0.0037557603791356087 + 0.001 * 6.923125267028809
Epoch 830, val loss: 1.1783956289291382
Epoch 840, training loss: 0.010550351813435555 = 0.003634845372289419 + 0.001 * 6.915506362915039
Epoch 840, val loss: 1.184206485748291
Epoch 850, training loss: 0.010442489758133888 = 0.0035203713923692703 + 0.001 * 6.922118186950684
Epoch 850, val loss: 1.1899096965789795
Epoch 860, training loss: 0.01032605953514576 = 0.00341192539781332 + 0.001 * 6.914133548736572
Epoch 860, val loss: 1.1954702138900757
Epoch 870, training loss: 0.0102097038179636 = 0.0033090482465922832 + 0.001 * 6.900655269622803
Epoch 870, val loss: 1.2009249925613403
Epoch 880, training loss: 0.01012822613120079 = 0.003211378585547209 + 0.001 * 6.9168477058410645
Epoch 880, val loss: 1.2062631845474243
Epoch 890, training loss: 0.01002111192792654 = 0.0031185559928417206 + 0.001 * 6.902555465698242
Epoch 890, val loss: 1.211493968963623
Epoch 900, training loss: 0.00993708148598671 = 0.0030302570667117834 + 0.001 * 6.906824111938477
Epoch 900, val loss: 1.216618299484253
Epoch 910, training loss: 0.00984186865389347 = 0.0029462487436830997 + 0.001 * 6.8956193923950195
Epoch 910, val loss: 1.2216289043426514
Epoch 920, training loss: 0.009765127673745155 = 0.002866253489628434 + 0.001 * 6.898874282836914
Epoch 920, val loss: 1.2265483140945435
Epoch 930, training loss: 0.0096964156255126 = 0.0027900284621864557 + 0.001 * 6.906386852264404
Epoch 930, val loss: 1.2313439846038818
Epoch 940, training loss: 0.009621385484933853 = 0.0027172942645847797 + 0.001 * 6.9040913581848145
Epoch 940, val loss: 1.2360800504684448
Epoch 950, training loss: 0.009528649970889091 = 0.002647905144840479 + 0.001 * 6.880744934082031
Epoch 950, val loss: 1.2406964302062988
Epoch 960, training loss: 0.009476874954998493 = 0.002581627806648612 + 0.001 * 6.895246982574463
Epoch 960, val loss: 1.2452330589294434
Epoch 970, training loss: 0.00943148322403431 = 0.002518305554986 + 0.001 * 6.913177490234375
Epoch 970, val loss: 1.2496683597564697
Epoch 980, training loss: 0.009332500398159027 = 0.0024577565491199493 + 0.001 * 6.8747429847717285
Epoch 980, val loss: 1.2540369033813477
Epoch 990, training loss: 0.009272119030356407 = 0.0023998054675757885 + 0.001 * 6.872312545776367
Epoch 990, val loss: 1.2582966089248657
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.94870924949646 = 1.9403355121612549 + 0.001 * 8.373744010925293
Epoch 0, val loss: 1.9393898248672485
Epoch 10, training loss: 1.9388331174850464 = 1.9304594993591309 + 0.001 * 8.373648643493652
Epoch 10, val loss: 1.930119514465332
Epoch 20, training loss: 1.9266536235809326 = 1.9182802438735962 + 0.001 * 8.37333869934082
Epoch 20, val loss: 1.9181987047195435
Epoch 30, training loss: 1.909485936164856 = 1.9011131525039673 + 0.001 * 8.372743606567383
Epoch 30, val loss: 1.9011555910110474
Epoch 40, training loss: 1.8840276002883911 = 1.8756561279296875 + 0.001 * 8.3714599609375
Epoch 40, val loss: 1.876146912574768
Epoch 50, training loss: 1.8482683897018433 = 1.8399003744125366 + 0.001 * 8.367997169494629
Epoch 50, val loss: 1.8426520824432373
Epoch 60, training loss: 1.8069953918457031 = 1.7986408472061157 + 0.001 * 8.35457992553711
Epoch 60, val loss: 1.8079050779342651
Epoch 70, training loss: 1.7677538394927979 = 1.7594850063323975 + 0.001 * 8.268800735473633
Epoch 70, val loss: 1.777419924736023
Epoch 80, training loss: 1.7179049253463745 = 1.7100768089294434 + 0.001 * 7.828119277954102
Epoch 80, val loss: 1.7351723909378052
Epoch 90, training loss: 1.6496816873550415 = 1.642086148262024 + 0.001 * 7.595568656921387
Epoch 90, val loss: 1.6773236989974976
Epoch 100, training loss: 1.5619089603424072 = 1.5545434951782227 + 0.001 * 7.365492343902588
Epoch 100, val loss: 1.6058564186096191
Epoch 110, training loss: 1.4631658792495728 = 1.456046223640442 + 0.001 * 7.1195969581604
Epoch 110, val loss: 1.5258324146270752
Epoch 120, training loss: 1.361879587173462 = 1.3548296689987183 + 0.001 * 7.049920082092285
Epoch 120, val loss: 1.4450398683547974
Epoch 130, training loss: 1.2588554620742798 = 1.2518553733825684 + 0.001 * 7.0000901222229
Epoch 130, val loss: 1.3640284538269043
Epoch 140, training loss: 1.1523797512054443 = 1.145398497581482 + 0.001 * 6.981306076049805
Epoch 140, val loss: 1.2811763286590576
Epoch 150, training loss: 1.0439207553863525 = 1.0369555950164795 + 0.001 * 6.965129375457764
Epoch 150, val loss: 1.196974277496338
Epoch 160, training loss: 0.9380548596382141 = 0.9310986995697021 + 0.001 * 6.956157684326172
Epoch 160, val loss: 1.1146502494812012
Epoch 170, training loss: 0.8394983410835266 = 0.8325471878051758 + 0.001 * 6.951133728027344
Epoch 170, val loss: 1.0382462739944458
Epoch 180, training loss: 0.7510369420051575 = 0.7440885901451111 + 0.001 * 6.948376655578613
Epoch 180, val loss: 0.9704107046127319
Epoch 190, training loss: 0.673445463180542 = 0.6664987802505493 + 0.001 * 6.94666051864624
Epoch 190, val loss: 0.9127455353736877
Epoch 200, training loss: 0.6057450771331787 = 0.5987998843193054 + 0.001 * 6.945219039916992
Epoch 200, val loss: 0.8655802011489868
Epoch 210, training loss: 0.5462283492088318 = 0.5392847657203674 + 0.001 * 6.943606853485107
Epoch 210, val loss: 0.8278226852416992
Epoch 220, training loss: 0.4932307004928589 = 0.486288845539093 + 0.001 * 6.941863059997559
Epoch 220, val loss: 0.7981377243995667
Epoch 230, training loss: 0.44511812925338745 = 0.43817809224128723 + 0.001 * 6.9400482177734375
Epoch 230, val loss: 0.7748405337333679
Epoch 240, training loss: 0.4003722071647644 = 0.3934340476989746 + 0.001 * 6.938149452209473
Epoch 240, val loss: 0.7562447786331177
Epoch 250, training loss: 0.3579471707344055 = 0.35101088881492615 + 0.001 * 6.936288356781006
Epoch 250, val loss: 0.7415300607681274
Epoch 260, training loss: 0.3175618648529053 = 0.3106272518634796 + 0.001 * 6.93460750579834
Epoch 260, val loss: 0.730263352394104
Epoch 270, training loss: 0.279507577419281 = 0.27257442474365234 + 0.001 * 6.933152675628662
Epoch 270, val loss: 0.722160816192627
Epoch 280, training loss: 0.24444392323493958 = 0.23751193284988403 + 0.001 * 6.931985855102539
Epoch 280, val loss: 0.7172496318817139
Epoch 290, training loss: 0.2130267322063446 = 0.20609577000141144 + 0.001 * 6.930955410003662
Epoch 290, val loss: 0.7156063318252563
Epoch 300, training loss: 0.18555332720279694 = 0.17862360179424286 + 0.001 * 6.929731369018555
Epoch 300, val loss: 0.7172486186027527
Epoch 310, training loss: 0.16195672750473022 = 0.1550282984972 + 0.001 * 6.928425312042236
Epoch 310, val loss: 0.722004771232605
Epoch 320, training loss: 0.1419062614440918 = 0.13497783243656158 + 0.001 * 6.928423881530762
Epoch 320, val loss: 0.7294861674308777
Epoch 330, training loss: 0.12495660036802292 = 0.11803065985441208 + 0.001 * 6.925939083099365
Epoch 330, val loss: 0.7392107248306274
Epoch 340, training loss: 0.1106242686510086 = 0.10369989275932312 + 0.001 * 6.924379348754883
Epoch 340, val loss: 0.7506917119026184
Epoch 350, training loss: 0.09845367074012756 = 0.09153079241514206 + 0.001 * 6.922877311706543
Epoch 350, val loss: 0.763471782207489
Epoch 360, training loss: 0.08805909007787704 = 0.08113855123519897 + 0.001 * 6.920539855957031
Epoch 360, val loss: 0.7771093249320984
Epoch 370, training loss: 0.0791299119591713 = 0.07220734655857086 + 0.001 * 6.92256498336792
Epoch 370, val loss: 0.7913495302200317
Epoch 380, training loss: 0.0714016929268837 = 0.06448552757501602 + 0.001 * 6.916163444519043
Epoch 380, val loss: 0.8058749437332153
Epoch 390, training loss: 0.06468433886766434 = 0.05777472257614136 + 0.001 * 6.909617900848389
Epoch 390, val loss: 0.8205419182777405
Epoch 400, training loss: 0.058821190148591995 = 0.05191786214709282 + 0.001 * 6.903326988220215
Epoch 400, val loss: 0.8352056741714478
Epoch 410, training loss: 0.05369093269109726 = 0.046789612621068954 + 0.001 * 6.901318073272705
Epoch 410, val loss: 0.8497899174690247
Epoch 420, training loss: 0.0491817407310009 = 0.042288023978471756 + 0.001 * 6.893717288970947
Epoch 420, val loss: 0.8642083406448364
Epoch 430, training loss: 0.045206692069768906 = 0.0383276492357254 + 0.001 * 6.879044055938721
Epoch 430, val loss: 0.8783421516418457
Epoch 440, training loss: 0.04172034561634064 = 0.03483615815639496 + 0.001 * 6.884187698364258
Epoch 440, val loss: 0.8922715187072754
Epoch 450, training loss: 0.03861599415540695 = 0.03175215795636177 + 0.001 * 6.863834381103516
Epoch 450, val loss: 0.9059528112411499
Epoch 460, training loss: 0.035881854593753815 = 0.02902228757739067 + 0.001 * 6.859567642211914
Epoch 460, val loss: 0.9193897843360901
Epoch 470, training loss: 0.03345375135540962 = 0.026600994169712067 + 0.001 * 6.852756977081299
Epoch 470, val loss: 0.9324942827224731
Epoch 480, training loss: 0.03130575641989708 = 0.024449441581964493 + 0.001 * 6.856313228607178
Epoch 480, val loss: 0.9453719258308411
Epoch 490, training loss: 0.029385851696133614 = 0.022532673552632332 + 0.001 * 6.853177547454834
Epoch 490, val loss: 0.9579045176506042
Epoch 500, training loss: 0.027672696858644485 = 0.020821435377001762 + 0.001 * 6.851261138916016
Epoch 500, val loss: 0.9701704382896423
Epoch 510, training loss: 0.026136400178074837 = 0.019290214404463768 + 0.001 * 6.846185684204102
Epoch 510, val loss: 0.9820584654808044
Epoch 520, training loss: 0.02475595474243164 = 0.01791660487651825 + 0.001 * 6.839349746704102
Epoch 520, val loss: 0.993657648563385
Epoch 530, training loss: 0.023519277572631836 = 0.01668129488825798 + 0.001 * 6.837981700897217
Epoch 530, val loss: 1.0049408674240112
Epoch 540, training loss: 0.022404618561267853 = 0.015567704103887081 + 0.001 * 6.836915016174316
Epoch 540, val loss: 1.015889048576355
Epoch 550, training loss: 0.021400131285190582 = 0.014561179094016552 + 0.001 * 6.838951110839844
Epoch 550, val loss: 1.0265545845031738
Epoch 560, training loss: 0.020485136657953262 = 0.013649233616888523 + 0.001 * 6.835903167724609
Epoch 560, val loss: 1.0369077920913696
Epoch 570, training loss: 0.019656039774417877 = 0.01282088365405798 + 0.001 * 6.835155963897705
Epoch 570, val loss: 1.0469468832015991
Epoch 580, training loss: 0.018903300166130066 = 0.0120666129514575 + 0.001 * 6.836688041687012
Epoch 580, val loss: 1.0567070245742798
Epoch 590, training loss: 0.018211547285318375 = 0.011378213763237 + 0.001 * 6.8333330154418945
Epoch 590, val loss: 1.066192865371704
Epoch 600, training loss: 0.017594749107956886 = 0.010748355649411678 + 0.001 * 6.84639310836792
Epoch 600, val loss: 1.0754324197769165
Epoch 610, training loss: 0.017006617039442062 = 0.010170865803956985 + 0.001 * 6.835750579833984
Epoch 610, val loss: 1.0843915939331055
Epoch 620, training loss: 0.01647256873548031 = 0.009640252217650414 + 0.001 * 6.832315921783447
Epoch 620, val loss: 1.0931179523468018
Epoch 630, training loss: 0.01599300280213356 = 0.00915162917226553 + 0.001 * 6.841373443603516
Epoch 630, val loss: 1.1016119718551636
Epoch 640, training loss: 0.015536773949861526 = 0.00870074424892664 + 0.001 * 6.836030006408691
Epoch 640, val loss: 1.1098861694335938
Epoch 650, training loss: 0.015112882480025291 = 0.008283887058496475 + 0.001 * 6.828995704650879
Epoch 650, val loss: 1.1179251670837402
Epoch 660, training loss: 0.014729289337992668 = 0.007897761650383472 + 0.001 * 6.831526756286621
Epoch 660, val loss: 1.125757098197937
Epoch 670, training loss: 0.014364825561642647 = 0.007539496757090092 + 0.001 * 6.825328350067139
Epoch 670, val loss: 1.1333738565444946
Epoch 680, training loss: 0.014040566980838776 = 0.007206491194665432 + 0.001 * 6.834075450897217
Epoch 680, val loss: 1.140798568725586
Epoch 690, training loss: 0.013721626251935959 = 0.006896426435559988 + 0.001 * 6.825199127197266
Epoch 690, val loss: 1.1480380296707153
Epoch 700, training loss: 0.013434447348117828 = 0.006607274524867535 + 0.001 * 6.827172756195068
Epoch 700, val loss: 1.1550933122634888
Epoch 710, training loss: 0.01316075399518013 = 0.0063372221775352955 + 0.001 * 6.823531150817871
Epoch 710, val loss: 1.1619755029678345
Epoch 720, training loss: 0.01292937807738781 = 0.0060846093110740185 + 0.001 * 6.844768524169922
Epoch 720, val loss: 1.1687003374099731
Epoch 730, training loss: 0.012669932097196579 = 0.005848026368767023 + 0.001 * 6.821905612945557
Epoch 730, val loss: 1.1752545833587646
Epoch 740, training loss: 0.012444738298654556 = 0.005626100115478039 + 0.001 * 6.818637847900391
Epoch 740, val loss: 1.181647777557373
Epoch 750, training loss: 0.012255014851689339 = 0.005417647771537304 + 0.001 * 6.837366580963135
Epoch 750, val loss: 1.1878994703292847
Epoch 760, training loss: 0.012041201815009117 = 0.005221607629209757 + 0.001 * 6.819593906402588
Epoch 760, val loss: 1.1939756870269775
Epoch 770, training loss: 0.011857517063617706 = 0.005037015303969383 + 0.001 * 6.82050085067749
Epoch 770, val loss: 1.199937105178833
Epoch 780, training loss: 0.011678354814648628 = 0.0048629832454025745 + 0.001 * 6.815370559692383
Epoch 780, val loss: 1.20576810836792
Epoch 790, training loss: 0.011528050526976585 = 0.004698699805885553 + 0.001 * 6.829349994659424
Epoch 790, val loss: 1.2114508152008057
Epoch 800, training loss: 0.011356484144926071 = 0.004543419927358627 + 0.001 * 6.813064098358154
Epoch 800, val loss: 1.2170212268829346
Epoch 810, training loss: 0.011212872341275215 = 0.004396403208374977 + 0.001 * 6.816469192504883
Epoch 810, val loss: 1.222456693649292
Epoch 820, training loss: 0.011069845408201218 = 0.004256945103406906 + 0.001 * 6.812900543212891
Epoch 820, val loss: 1.227777361869812
Epoch 830, training loss: 0.010943690314888954 = 0.004124389495700598 + 0.001 * 6.819301128387451
Epoch 830, val loss: 1.2330009937286377
Epoch 840, training loss: 0.01080908440053463 = 0.00399820227175951 + 0.001 * 6.810881614685059
Epoch 840, val loss: 1.2381120920181274
Epoch 850, training loss: 0.010679008439183235 = 0.003877889597788453 + 0.001 * 6.801118850708008
Epoch 850, val loss: 1.243127703666687
Epoch 860, training loss: 0.010562601499259472 = 0.003762992797419429 + 0.001 * 6.7996087074279785
Epoch 860, val loss: 1.2480390071868896
Epoch 870, training loss: 0.010453211143612862 = 0.0036527349147945642 + 0.001 * 6.800475597381592
Epoch 870, val loss: 1.2529284954071045
Epoch 880, training loss: 0.010365210473537445 = 0.0035453317686915398 + 0.001 * 6.819878101348877
Epoch 880, val loss: 1.2577974796295166
Epoch 890, training loss: 0.010245990939438343 = 0.0034433137625455856 + 0.001 * 6.802676677703857
Epoch 890, val loss: 1.262406826019287
Epoch 900, training loss: 0.01016385480761528 = 0.0033454697113484144 + 0.001 * 6.818384647369385
Epoch 900, val loss: 1.2670118808746338
Epoch 910, training loss: 0.010045814327895641 = 0.003251818474382162 + 0.001 * 6.793995380401611
Epoch 910, val loss: 1.271541953086853
Epoch 920, training loss: 0.009958875365555286 = 0.003161960979923606 + 0.001 * 6.7969136238098145
Epoch 920, val loss: 1.27596914768219
Epoch 930, training loss: 0.009860711172223091 = 0.0030762474052608013 + 0.001 * 6.784463882446289
Epoch 930, val loss: 1.2802817821502686
Epoch 940, training loss: 0.00977939274162054 = 0.0029942661058157682 + 0.001 * 6.785126209259033
Epoch 940, val loss: 1.2845426797866821
Epoch 950, training loss: 0.009707202203571796 = 0.00291584269143641 + 0.001 * 6.791358947753906
Epoch 950, val loss: 1.2887399196624756
Epoch 960, training loss: 0.009620464406907558 = 0.002840769710019231 + 0.001 * 6.779694080352783
Epoch 960, val loss: 1.2928434610366821
Epoch 970, training loss: 0.009547626599669456 = 0.002768827835097909 + 0.001 * 6.7787981033325195
Epoch 970, val loss: 1.2968659400939941
Epoch 980, training loss: 0.009481051936745644 = 0.0026999670080840588 + 0.001 * 6.7810845375061035
Epoch 980, val loss: 1.3008111715316772
Epoch 990, training loss: 0.009419247508049011 = 0.0026340775657445192 + 0.001 * 6.78516960144043
Epoch 990, val loss: 1.3047109842300415
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9587684869766235 = 1.9503947496414185 + 0.001 * 8.37376880645752
Epoch 0, val loss: 1.9476946592330933
Epoch 10, training loss: 1.947790265083313 = 1.9394166469573975 + 0.001 * 8.373641967773438
Epoch 10, val loss: 1.9373571872711182
Epoch 20, training loss: 1.9337741136550903 = 1.925400733947754 + 0.001 * 8.373332977294922
Epoch 20, val loss: 1.9238357543945312
Epoch 30, training loss: 1.9138240814208984 = 1.9054514169692993 + 0.001 * 8.372703552246094
Epoch 30, val loss: 1.904577612876892
Epoch 40, training loss: 1.8845651149749756 = 1.876193881034851 + 0.001 * 8.371255874633789
Epoch 40, val loss: 1.8769607543945312
Epoch 50, training loss: 1.8449331521987915 = 1.8365662097930908 + 0.001 * 8.366959571838379
Epoch 50, val loss: 1.841902256011963
Epoch 60, training loss: 1.802653431892395 = 1.7943044900894165 + 0.001 * 8.348909378051758
Epoch 60, val loss: 1.808980107307434
Epoch 70, training loss: 1.765956163406372 = 1.7577033042907715 + 0.001 * 8.25285530090332
Epoch 70, val loss: 1.7810200452804565
Epoch 80, training loss: 1.7185628414154053 = 1.7107394933700562 + 0.001 * 7.823328971862793
Epoch 80, val loss: 1.7397032976150513
Epoch 90, training loss: 1.653883457183838 = 1.6462563276290894 + 0.001 * 7.627161979675293
Epoch 90, val loss: 1.685254454612732
Epoch 100, training loss: 1.5679571628570557 = 1.5605335235595703 + 0.001 * 7.423668384552002
Epoch 100, val loss: 1.6154695749282837
Epoch 110, training loss: 1.4662986993789673 = 1.4589935541152954 + 0.001 * 7.305090427398682
Epoch 110, val loss: 1.53317391872406
Epoch 120, training loss: 1.3590342998504639 = 1.3517818450927734 + 0.001 * 7.252476692199707
Epoch 120, val loss: 1.4463486671447754
Epoch 130, training loss: 1.255195140838623 = 1.248011589050293 + 0.001 * 7.183507919311523
Epoch 130, val loss: 1.3626453876495361
Epoch 140, training loss: 1.158692717552185 = 1.1516014337539673 + 0.001 * 7.091280937194824
Epoch 140, val loss: 1.2847304344177246
Epoch 150, training loss: 1.0711040496826172 = 1.0640724897384644 + 0.001 * 7.031590938568115
Epoch 150, val loss: 1.2150392532348633
Epoch 160, training loss: 0.9916279315948486 = 0.9846274256706238 + 0.001 * 7.000476837158203
Epoch 160, val loss: 1.1535959243774414
Epoch 170, training loss: 0.9169429540634155 = 0.9099755883216858 + 0.001 * 6.967388153076172
Epoch 170, val loss: 1.0985313653945923
Epoch 180, training loss: 0.8426936268806458 = 0.8357676267623901 + 0.001 * 6.92598819732666
Epoch 180, val loss: 1.0448815822601318
Epoch 190, training loss: 0.76622474193573 = 0.759332537651062 + 0.001 * 6.892182350158691
Epoch 190, val loss: 0.9895266890525818
Epoch 200, training loss: 0.6881142854690552 = 0.6812361478805542 + 0.001 * 6.87810754776001
Epoch 200, val loss: 0.9327397346496582
Epoch 210, training loss: 0.6114848852157593 = 0.6046150326728821 + 0.001 * 6.869870662689209
Epoch 210, val loss: 0.8776198029518127
Epoch 220, training loss: 0.5397891998291016 = 0.532925009727478 + 0.001 * 6.864212989807129
Epoch 220, val loss: 0.8283424973487854
Epoch 230, training loss: 0.47485753893852234 = 0.46799901127815247 + 0.001 * 6.858531951904297
Epoch 230, val loss: 0.7877289652824402
Epoch 240, training loss: 0.4167920649051666 = 0.4099401533603668 + 0.001 * 6.8519062995910645
Epoch 240, val loss: 0.7555320858955383
Epoch 250, training loss: 0.3650308847427368 = 0.3581869602203369 + 0.001 * 6.843912124633789
Epoch 250, val loss: 0.730405867099762
Epoch 260, training loss: 0.31893956661224365 = 0.3121050000190735 + 0.001 * 6.834564685821533
Epoch 260, val loss: 0.7105497121810913
Epoch 270, training loss: 0.2780936360359192 = 0.2712678909301758 + 0.001 * 6.82575798034668
Epoch 270, val loss: 0.6949156522750854
Epoch 280, training loss: 0.2422332912683487 = 0.23541390895843506 + 0.001 * 6.8193840980529785
Epoch 280, val loss: 0.6832633018493652
Epoch 290, training loss: 0.21109962463378906 = 0.20428849756717682 + 0.001 * 6.811120510101318
Epoch 290, val loss: 0.6752756834030151
Epoch 300, training loss: 0.18436627089977264 = 0.1775495857000351 + 0.001 * 6.816682815551758
Epoch 300, val loss: 0.6706920862197876
Epoch 310, training loss: 0.1615222543478012 = 0.15471741557121277 + 0.001 * 6.804842472076416
Epoch 310, val loss: 0.6693710088729858
Epoch 320, training loss: 0.14205896854400635 = 0.1352607011795044 + 0.001 * 6.79826545715332
Epoch 320, val loss: 0.6710521578788757
Epoch 330, training loss: 0.1254764348268509 = 0.1186802089214325 + 0.001 * 6.796232223510742
Epoch 330, val loss: 0.6753995418548584
Epoch 340, training loss: 0.11131954938173294 = 0.1045224517583847 + 0.001 * 6.797093868255615
Epoch 340, val loss: 0.6820937395095825
Epoch 350, training loss: 0.09919243305921555 = 0.09239936619997025 + 0.001 * 6.793063163757324
Epoch 350, val loss: 0.690775454044342
Epoch 360, training loss: 0.08876946568489075 = 0.08197548240423203 + 0.001 * 6.793980121612549
Epoch 360, val loss: 0.7010444402694702
Epoch 370, training loss: 0.07976502925157547 = 0.07297225296497345 + 0.001 * 6.792779445648193
Epoch 370, val loss: 0.7126126885414124
Epoch 380, training loss: 0.0719517320394516 = 0.06515934318304062 + 0.001 * 6.792389392852783
Epoch 380, val loss: 0.7252069115638733
Epoch 390, training loss: 0.06514609605073929 = 0.058353301137685776 + 0.001 * 6.792792320251465
Epoch 390, val loss: 0.7385554313659668
Epoch 400, training loss: 0.0591963566839695 = 0.052403856068849564 + 0.001 * 6.792501449584961
Epoch 400, val loss: 0.7524740695953369
Epoch 410, training loss: 0.05398155003786087 = 0.04718802124261856 + 0.001 * 6.793529033660889
Epoch 410, val loss: 0.7667464017868042
Epoch 420, training loss: 0.04940042644739151 = 0.0426083579659462 + 0.001 * 6.792069911956787
Epoch 420, val loss: 0.7812085151672363
Epoch 430, training loss: 0.04537327215075493 = 0.038581617176532745 + 0.001 * 6.791656017303467
Epoch 430, val loss: 0.7957189083099365
Epoch 440, training loss: 0.041827987879514694 = 0.035036541521549225 + 0.001 * 6.791447162628174
Epoch 440, val loss: 0.81009840965271
Epoch 450, training loss: 0.038702912628650665 = 0.03191210329532623 + 0.001 * 6.790807723999023
Epoch 450, val loss: 0.8242506980895996
Epoch 460, training loss: 0.03595433011651039 = 0.029155150055885315 + 0.001 * 6.799180507659912
Epoch 460, val loss: 0.8381000757217407
Epoch 470, training loss: 0.03351015970110893 = 0.026717405766248703 + 0.001 * 6.792754650115967
Epoch 470, val loss: 0.8516271710395813
Epoch 480, training loss: 0.03134644776582718 = 0.024555779993534088 + 0.001 * 6.790666103363037
Epoch 480, val loss: 0.8647351861000061
Epoch 490, training loss: 0.029423901811242104 = 0.022633833810687065 + 0.001 * 6.790067195892334
Epoch 490, val loss: 0.8774609565734863
Epoch 500, training loss: 0.0277089923620224 = 0.02091999724507332 + 0.001 * 6.788995742797852
Epoch 500, val loss: 0.8897940516471863
Epoch 510, training loss: 0.026175880804657936 = 0.019386950880289078 + 0.001 * 6.7889299392700195
Epoch 510, val loss: 0.9017627239227295
Epoch 520, training loss: 0.024800507351756096 = 0.0180119127035141 + 0.001 * 6.788593769073486
Epoch 520, val loss: 0.9133461713790894
Epoch 530, training loss: 0.023567236959934235 = 0.01677481085062027 + 0.001 * 6.792425632476807
Epoch 530, val loss: 0.9245930314064026
Epoch 540, training loss: 0.02244924008846283 = 0.015658652409911156 + 0.001 * 6.790586471557617
Epoch 540, val loss: 0.9354948997497559
Epoch 550, training loss: 0.021435445174574852 = 0.01464872807264328 + 0.001 * 6.786716938018799
Epoch 550, val loss: 0.9460776448249817
Epoch 560, training loss: 0.020518682897090912 = 0.013732756488025188 + 0.001 * 6.785926818847656
Epoch 560, val loss: 0.9563275575637817
Epoch 570, training loss: 0.019687151536345482 = 0.012899819761514664 + 0.001 * 6.7873311042785645
Epoch 570, val loss: 0.9662908315658569
Epoch 580, training loss: 0.018925458192825317 = 0.01214103028178215 + 0.001 * 6.784426689147949
Epoch 580, val loss: 0.9759541749954224
Epoch 590, training loss: 0.018238713964819908 = 0.011447926983237267 + 0.001 * 6.790786266326904
Epoch 590, val loss: 0.985356867313385
Epoch 600, training loss: 0.017597828060388565 = 0.010813125409185886 + 0.001 * 6.784701824188232
Epoch 600, val loss: 0.9945155382156372
Epoch 610, training loss: 0.01701517589390278 = 0.01023055985569954 + 0.001 * 6.784615993499756
Epoch 610, val loss: 1.0034302473068237
Epoch 620, training loss: 0.01647808775305748 = 0.009694842621684074 + 0.001 * 6.783245086669922
Epoch 620, val loss: 1.012107253074646
Epoch 630, training loss: 0.015986548736691475 = 0.009201128967106342 + 0.001 * 6.785419940948486
Epoch 630, val loss: 1.0205551385879517
Epoch 640, training loss: 0.01552610844373703 = 0.008745195344090462 + 0.001 * 6.780912399291992
Epoch 640, val loss: 1.028787612915039
Epoch 650, training loss: 0.015104362741112709 = 0.00832334253937006 + 0.001 * 6.781020164489746
Epoch 650, val loss: 1.0368080139160156
Epoch 660, training loss: 0.014711955562233925 = 0.007932264357805252 + 0.001 * 6.779690742492676
Epoch 660, val loss: 1.0446298122406006
Epoch 670, training loss: 0.014348139986395836 = 0.0075692362152040005 + 0.001 * 6.778903484344482
Epoch 670, val loss: 1.0522429943084717
Epoch 680, training loss: 0.014015939086675644 = 0.007231672760099173 + 0.001 * 6.784266471862793
Epoch 680, val loss: 1.0596712827682495
Epoch 690, training loss: 0.013695420697331429 = 0.006917268969118595 + 0.001 * 6.778151035308838
Epoch 690, val loss: 1.0669361352920532
Epoch 700, training loss: 0.013401797041296959 = 0.006624139845371246 + 0.001 * 6.777657508850098
Epoch 700, val loss: 1.0740199089050293
Epoch 710, training loss: 0.013128167949616909 = 0.006350359879434109 + 0.001 * 6.777807712554932
Epoch 710, val loss: 1.0809388160705566
Epoch 720, training loss: 0.01287111546844244 = 0.0060942526906728745 + 0.001 * 6.776862621307373
Epoch 720, val loss: 1.0877153873443604
Epoch 730, training loss: 0.012636346742510796 = 0.005854251328855753 + 0.001 * 6.782094955444336
Epoch 730, val loss: 1.0943347215652466
Epoch 740, training loss: 0.012404447421431541 = 0.005629102233797312 + 0.001 * 6.775344371795654
Epoch 740, val loss: 1.1007999181747437
Epoch 750, training loss: 0.012189429253339767 = 0.005417635664343834 + 0.001 * 6.771792888641357
Epoch 750, val loss: 1.1071316003799438
Epoch 760, training loss: 0.011991607025265694 = 0.0052187456749379635 + 0.001 * 6.772861480712891
Epoch 760, val loss: 1.11332368850708
Epoch 770, training loss: 0.011801136657595634 = 0.0050314911641180515 + 0.001 * 6.769644737243652
Epoch 770, val loss: 1.1193952560424805
Epoch 780, training loss: 0.011634185910224915 = 0.004855010658502579 + 0.001 * 6.779175281524658
Epoch 780, val loss: 1.1253312826156616
Epoch 790, training loss: 0.011456326581537724 = 0.004688453860580921 + 0.001 * 6.767872333526611
Epoch 790, val loss: 1.1311427354812622
Epoch 800, training loss: 0.011302413418889046 = 0.004531139507889748 + 0.001 * 6.771274089813232
Epoch 800, val loss: 1.1368277072906494
Epoch 810, training loss: 0.01114923506975174 = 0.004382369574159384 + 0.001 * 6.766865253448486
Epoch 810, val loss: 1.1424115896224976
Epoch 820, training loss: 0.011009100824594498 = 0.004241533111780882 + 0.001 * 6.767567157745361
Epoch 820, val loss: 1.1478784084320068
Epoch 830, training loss: 0.010871094651520252 = 0.004108081571757793 + 0.001 * 6.763012886047363
Epoch 830, val loss: 1.1532407999038696
Epoch 840, training loss: 0.010746102780103683 = 0.003981513436883688 + 0.001 * 6.764589309692383
Epoch 840, val loss: 1.1584914922714233
Epoch 850, training loss: 0.010631892830133438 = 0.0038613565266132355 + 0.001 * 6.770536422729492
Epoch 850, val loss: 1.1636593341827393
Epoch 860, training loss: 0.010509430430829525 = 0.003747200360521674 + 0.001 * 6.762229919433594
Epoch 860, val loss: 1.1687226295471191
Epoch 870, training loss: 0.010400318540632725 = 0.003638637252151966 + 0.001 * 6.761681079864502
Epoch 870, val loss: 1.1736940145492554
Epoch 880, training loss: 0.010298769921064377 = 0.003535300260409713 + 0.001 * 6.763469696044922
Epoch 880, val loss: 1.1785833835601807
Epoch 890, training loss: 0.010207433253526688 = 0.003436880186200142 + 0.001 * 6.770553112030029
Epoch 890, val loss: 1.1833746433258057
Epoch 900, training loss: 0.010104291141033173 = 0.003343067830428481 + 0.001 * 6.761222839355469
Epoch 900, val loss: 1.1880849599838257
Epoch 910, training loss: 0.010017485357820988 = 0.0032535686623305082 + 0.001 * 6.763916492462158
Epoch 910, val loss: 1.1927034854888916
Epoch 920, training loss: 0.009921985678374767 = 0.003168136579915881 + 0.001 * 6.753848552703857
Epoch 920, val loss: 1.1972532272338867
Epoch 930, training loss: 0.009851336479187012 = 0.003086534794420004 + 0.001 * 6.764801979064941
Epoch 930, val loss: 1.20172119140625
Epoch 940, training loss: 0.009762393310666084 = 0.003008556319400668 + 0.001 * 6.753836631774902
Epoch 940, val loss: 1.2061140537261963
Epoch 950, training loss: 0.00968855433166027 = 0.0029339720495045185 + 0.001 * 6.754581928253174
Epoch 950, val loss: 1.2104313373565674
Epoch 960, training loss: 0.00961470790207386 = 0.0028625736013054848 + 0.001 * 6.752133846282959
Epoch 960, val loss: 1.2146832942962646
Epoch 970, training loss: 0.009554999880492687 = 0.0027942019514739513 + 0.001 * 6.760797500610352
Epoch 970, val loss: 1.2188665866851807
Epoch 980, training loss: 0.009481269866228104 = 0.0027286820113658905 + 0.001 * 6.75258731842041
Epoch 980, val loss: 1.2229700088500977
Epoch 990, training loss: 0.00941136572510004 = 0.0026658547576516867 + 0.001 * 6.745510578155518
Epoch 990, val loss: 1.2270132303237915
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5129
Flip ASR: 0.4178/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9486216306686401 = 1.940247893333435 + 0.001 * 8.373714447021484
Epoch 0, val loss: 1.9401746988296509
Epoch 10, training loss: 1.9386441707611084 = 1.9302705526351929 + 0.001 * 8.37362289428711
Epoch 10, val loss: 1.9300239086151123
Epoch 20, training loss: 1.9265704154968262 = 1.9181970357894897 + 0.001 * 8.373322486877441
Epoch 20, val loss: 1.9169187545776367
Epoch 30, training loss: 1.9095938205718994 = 1.9012211561203003 + 0.001 * 8.372679710388184
Epoch 30, val loss: 1.8978703022003174
Epoch 40, training loss: 1.8843872547149658 = 1.8760161399841309 + 0.001 * 8.371167182922363
Epoch 40, val loss: 1.8696128129959106
Epoch 50, training loss: 1.84864342212677 = 1.840276837348938 + 0.001 * 8.366565704345703
Epoch 50, val loss: 1.8310370445251465
Epoch 60, training loss: 1.8059303760528564 = 1.7975859642028809 + 0.001 * 8.344451904296875
Epoch 60, val loss: 1.7887307405471802
Epoch 70, training loss: 1.7622772455215454 = 1.7540863752365112 + 0.001 * 8.190825462341309
Epoch 70, val loss: 1.749822974205017
Epoch 80, training loss: 1.7073884010314941 = 1.699664831161499 + 0.001 * 7.723624229431152
Epoch 80, val loss: 1.7026524543762207
Epoch 90, training loss: 1.6328144073486328 = 1.6252827644348145 + 0.001 * 7.5316619873046875
Epoch 90, val loss: 1.638282060623169
Epoch 100, training loss: 1.5381699800491333 = 1.5308085680007935 + 0.001 * 7.361388206481934
Epoch 100, val loss: 1.5582917928695679
Epoch 110, training loss: 1.4338077306747437 = 1.4265908002853394 + 0.001 * 7.216953277587891
Epoch 110, val loss: 1.474705696105957
Epoch 120, training loss: 1.3305445909500122 = 1.3233652114868164 + 0.001 * 7.179369926452637
Epoch 120, val loss: 1.39854097366333
Epoch 130, training loss: 1.2322341203689575 = 1.2250981330871582 + 0.001 * 7.135929107666016
Epoch 130, val loss: 1.3312777280807495
Epoch 140, training loss: 1.1392794847488403 = 1.1321861743927002 + 0.001 * 7.093301773071289
Epoch 140, val loss: 1.268689751625061
Epoch 150, training loss: 1.052567958831787 = 1.0455280542373657 + 0.001 * 7.03995418548584
Epoch 150, val loss: 1.2094647884368896
Epoch 160, training loss: 0.9729072451591492 = 0.9659223556518555 + 0.001 * 6.984893321990967
Epoch 160, val loss: 1.1545227766036987
Epoch 170, training loss: 0.8994668126106262 = 0.8925243020057678 + 0.001 * 6.942497730255127
Epoch 170, val loss: 1.103621482849121
Epoch 180, training loss: 0.830192506313324 = 0.8232740759849548 + 0.001 * 6.918430805206299
Epoch 180, val loss: 1.055237889289856
Epoch 190, training loss: 0.7638427019119263 = 0.7569395303726196 + 0.001 * 6.903153419494629
Epoch 190, val loss: 1.0090007781982422
Epoch 200, training loss: 0.700920820236206 = 0.6940257549285889 + 0.001 * 6.895049095153809
Epoch 200, val loss: 0.9651146531105042
Epoch 210, training loss: 0.6425203680992126 = 0.6356335282325745 + 0.001 * 6.886844635009766
Epoch 210, val loss: 0.9249844551086426
Epoch 220, training loss: 0.5886483192443848 = 0.5817651152610779 + 0.001 * 6.883193492889404
Epoch 220, val loss: 0.8885365724563599
Epoch 230, training loss: 0.5383065342903137 = 0.5314271450042725 + 0.001 * 6.879372596740723
Epoch 230, val loss: 0.8558387160301208
Epoch 240, training loss: 0.4906117916107178 = 0.48373714089393616 + 0.001 * 6.874652862548828
Epoch 240, val loss: 0.8269408345222473
Epoch 250, training loss: 0.4452005624771118 = 0.43833237886428833 + 0.001 * 6.868180751800537
Epoch 250, val loss: 0.8017249703407288
Epoch 260, training loss: 0.40222877264022827 = 0.39536476135253906 + 0.001 * 6.864015579223633
Epoch 260, val loss: 0.780530571937561
Epoch 270, training loss: 0.3620339334011078 = 0.35518380999565125 + 0.001 * 6.850129127502441
Epoch 270, val loss: 0.7639185786247253
Epoch 280, training loss: 0.32481375336647034 = 0.3179726302623749 + 0.001 * 6.841111660003662
Epoch 280, val loss: 0.7516698837280273
Epoch 290, training loss: 0.29065778851509094 = 0.2838275730609894 + 0.001 * 6.830202102661133
Epoch 290, val loss: 0.7439733743667603
Epoch 300, training loss: 0.25970855355262756 = 0.2528875172138214 + 0.001 * 6.8210272789001465
Epoch 300, val loss: 0.7405944466590881
Epoch 310, training loss: 0.23204240202903748 = 0.22522810101509094 + 0.001 * 6.814296722412109
Epoch 310, val loss: 0.7416189312934875
Epoch 320, training loss: 0.20747564733028412 = 0.20066659152507782 + 0.001 * 6.809052467346191
Epoch 320, val loss: 0.746801495552063
Epoch 330, training loss: 0.18577860295772552 = 0.17897465825080872 + 0.001 * 6.80394983291626
Epoch 330, val loss: 0.7557176351547241
Epoch 340, training loss: 0.1666848063468933 = 0.15988269448280334 + 0.001 * 6.802106857299805
Epoch 340, val loss: 0.7680227160453796
Epoch 350, training loss: 0.1497514545917511 = 0.14295104146003723 + 0.001 * 6.800413131713867
Epoch 350, val loss: 0.782764196395874
Epoch 360, training loss: 0.13463547825813293 = 0.12783609330654144 + 0.001 * 6.79939079284668
Epoch 360, val loss: 0.7996465563774109
Epoch 370, training loss: 0.12109316885471344 = 0.1142948716878891 + 0.001 * 6.798293590545654
Epoch 370, val loss: 0.8180438280105591
Epoch 380, training loss: 0.1088806614279747 = 0.10207904875278473 + 0.001 * 6.801612854003906
Epoch 380, val loss: 0.8374736905097961
Epoch 390, training loss: 0.09768593311309814 = 0.09088907390832901 + 0.001 * 6.796862602233887
Epoch 390, val loss: 0.8574082255363464
Epoch 400, training loss: 0.08735082298517227 = 0.08055777102708817 + 0.001 * 6.7930521965026855
Epoch 400, val loss: 0.8780263066291809
Epoch 410, training loss: 0.07798250019550323 = 0.07118013501167297 + 0.001 * 6.802361011505127
Epoch 410, val loss: 0.8989564180374146
Epoch 420, training loss: 0.0695880725979805 = 0.06279617547988892 + 0.001 * 6.791894912719727
Epoch 420, val loss: 0.9201725721359253
Epoch 430, training loss: 0.06242602318525314 = 0.055640049278736115 + 0.001 * 6.785975456237793
Epoch 430, val loss: 0.9416067004203796
Epoch 440, training loss: 0.05630650371313095 = 0.04952096939086914 + 0.001 * 6.785534381866455
Epoch 440, val loss: 0.9625185132026672
Epoch 450, training loss: 0.05098797380924225 = 0.04420984163880348 + 0.001 * 6.778132438659668
Epoch 450, val loss: 0.9825015664100647
Epoch 460, training loss: 0.04627680778503418 = 0.03949994593858719 + 0.001 * 6.776861190795898
Epoch 460, val loss: 1.0016264915466309
Epoch 470, training loss: 0.042171910405159 = 0.03539779782295227 + 0.001 * 6.774113178253174
Epoch 470, val loss: 1.0202778577804565
Epoch 480, training loss: 0.03862897679209709 = 0.031854208558797836 + 0.001 * 6.774767875671387
Epoch 480, val loss: 1.0385444164276123
Epoch 490, training loss: 0.03556600213050842 = 0.028803637251257896 + 0.001 * 6.762362480163574
Epoch 490, val loss: 1.0562374591827393
Epoch 500, training loss: 0.032957542687654495 = 0.02616136334836483 + 0.001 * 6.796178817749023
Epoch 500, val loss: 1.0734056234359741
Epoch 510, training loss: 0.030628997832536697 = 0.023858947679400444 + 0.001 * 6.770049095153809
Epoch 510, val loss: 1.0899722576141357
Epoch 520, training loss: 0.0286162868142128 = 0.021858761087059975 + 0.001 * 6.75752592086792
Epoch 520, val loss: 1.1059030294418335
Epoch 530, training loss: 0.026869837194681168 = 0.020108124241232872 + 0.001 * 6.761712074279785
Epoch 530, val loss: 1.1212269067764282
Epoch 540, training loss: 0.025314176455140114 = 0.018565714359283447 + 0.001 * 6.7484612464904785
Epoch 540, val loss: 1.1359182596206665
Epoch 550, training loss: 0.023954907432198524 = 0.017200229689478874 + 0.001 * 6.7546772956848145
Epoch 550, val loss: 1.1500592231750488
Epoch 560, training loss: 0.022723739966750145 = 0.01598558947443962 + 0.001 * 6.738150596618652
Epoch 560, val loss: 1.1636487245559692
Epoch 570, training loss: 0.021637476980686188 = 0.014900373294949532 + 0.001 * 6.73710298538208
Epoch 570, val loss: 1.1767762899398804
Epoch 580, training loss: 0.02068011462688446 = 0.013926239684224129 + 0.001 * 6.753873825073242
Epoch 580, val loss: 1.1894540786743164
Epoch 590, training loss: 0.019789664074778557 = 0.013048535212874413 + 0.001 * 6.741128444671631
Epoch 590, val loss: 1.201681137084961
Epoch 600, training loss: 0.018995732069015503 = 0.012254794128239155 + 0.001 * 6.740936756134033
Epoch 600, val loss: 1.21352219581604
Epoch 610, training loss: 0.018267812207341194 = 0.011534632183611393 + 0.001 * 6.733179569244385
Epoch 610, val loss: 1.2249890565872192
Epoch 620, training loss: 0.017606839537620544 = 0.010879172943532467 + 0.001 * 6.72766637802124
Epoch 620, val loss: 1.2360705137252808
Epoch 630, training loss: 0.017011838033795357 = 0.010280994698405266 + 0.001 * 6.730843544006348
Epoch 630, val loss: 1.246815800666809
Epoch 640, training loss: 0.016457725316286087 = 0.009733491577208042 + 0.001 * 6.7242326736450195
Epoch 640, val loss: 1.257209300994873
Epoch 650, training loss: 0.01595597341656685 = 0.009231180883944035 + 0.001 * 6.724791526794434
Epoch 650, val loss: 1.2673070430755615
Epoch 660, training loss: 0.015500042587518692 = 0.008769212290644646 + 0.001 * 6.730830192565918
Epoch 660, val loss: 1.2771077156066895
Epoch 670, training loss: 0.01506892777979374 = 0.008343304507434368 + 0.001 * 6.725623607635498
Epoch 670, val loss: 1.286630392074585
Epoch 680, training loss: 0.014666490256786346 = 0.007949773222208023 + 0.001 * 6.716716289520264
Epoch 680, val loss: 1.2959041595458984
Epoch 690, training loss: 0.014303343370556831 = 0.007585441693663597 + 0.001 * 6.717901706695557
Epoch 690, val loss: 1.3049428462982178
Epoch 700, training loss: 0.013963887467980385 = 0.007247590459883213 + 0.001 * 6.716296195983887
Epoch 700, val loss: 1.3137463331222534
Epoch 710, training loss: 0.01365133561193943 = 0.006933553609997034 + 0.001 * 6.717781066894531
Epoch 710, val loss: 1.322295069694519
Epoch 720, training loss: 0.0133552560582757 = 0.006641179323196411 + 0.001 * 6.714076519012451
Epoch 720, val loss: 1.3306366205215454
Epoch 730, training loss: 0.013082226738333702 = 0.006368536967784166 + 0.001 * 6.713689804077148
Epoch 730, val loss: 1.3387712240219116
Epoch 740, training loss: 0.0128325205296278 = 0.006113662850111723 + 0.001 * 6.718857765197754
Epoch 740, val loss: 1.3467057943344116
Epoch 750, training loss: 0.01259018387645483 = 0.005874627735465765 + 0.001 * 6.715555667877197
Epoch 750, val loss: 1.3544161319732666
Epoch 760, training loss: 0.012359187006950378 = 0.005650450475513935 + 0.001 * 6.708736419677734
Epoch 760, val loss: 1.3619099855422974
Epoch 770, training loss: 0.01215413585305214 = 0.005440128967165947 + 0.001 * 6.714005947113037
Epoch 770, val loss: 1.3692355155944824
Epoch 780, training loss: 0.011958735063672066 = 0.005242578219622374 + 0.001 * 6.716156959533691
Epoch 780, val loss: 1.3764114379882812
Epoch 790, training loss: 0.011782203800976276 = 0.005056766327470541 + 0.001 * 6.725437164306641
Epoch 790, val loss: 1.3834187984466553
Epoch 800, training loss: 0.011596634984016418 = 0.00488177640363574 + 0.001 * 6.714858055114746
Epoch 800, val loss: 1.3902630805969238
Epoch 810, training loss: 0.011422684416174889 = 0.004716781433671713 + 0.001 * 6.705902576446533
Epoch 810, val loss: 1.3969477415084839
Epoch 820, training loss: 0.011269155889749527 = 0.004561062436550856 + 0.001 * 6.70809268951416
Epoch 820, val loss: 1.4034876823425293
Epoch 830, training loss: 0.011117270216345787 = 0.004413898102939129 + 0.001 * 6.703372478485107
Epoch 830, val loss: 1.4098749160766602
Epoch 840, training loss: 0.010981232859194279 = 0.004274670034646988 + 0.001 * 6.706562519073486
Epoch 840, val loss: 1.416136622428894
Epoch 850, training loss: 0.010841866955161095 = 0.00414280965924263 + 0.001 * 6.699057579040527
Epoch 850, val loss: 1.4222614765167236
Epoch 860, training loss: 0.010724435560405254 = 0.004017784725874662 + 0.001 * 6.706650733947754
Epoch 860, val loss: 1.4282692670822144
Epoch 870, training loss: 0.010598890483379364 = 0.003899147268384695 + 0.001 * 6.699742317199707
Epoch 870, val loss: 1.4341349601745605
Epoch 880, training loss: 0.010488657280802727 = 0.003786466084420681 + 0.001 * 6.702190399169922
Epoch 880, val loss: 1.4399011135101318
Epoch 890, training loss: 0.01037283893674612 = 0.003679334418848157 + 0.001 * 6.693504333496094
Epoch 890, val loss: 1.445533037185669
Epoch 900, training loss: 0.01026932243257761 = 0.00357740162871778 + 0.001 * 6.691920280456543
Epoch 900, val loss: 1.451064944267273
Epoch 910, training loss: 0.01017711777240038 = 0.003480335231870413 + 0.001 * 6.696782112121582
Epoch 910, val loss: 1.4564868211746216
Epoch 920, training loss: 0.01007908210158348 = 0.003387812525033951 + 0.001 * 6.691269397735596
Epoch 920, val loss: 1.4618045091629028
Epoch 930, training loss: 0.009994477964937687 = 0.003299551783129573 + 0.001 * 6.6949262619018555
Epoch 930, val loss: 1.4670192003250122
Epoch 940, training loss: 0.009900546632707119 = 0.0032152896746993065 + 0.001 * 6.685256481170654
Epoch 940, val loss: 1.4721359014511108
Epoch 950, training loss: 0.009834066033363342 = 0.0031347954645752907 + 0.001 * 6.699269771575928
Epoch 950, val loss: 1.4771636724472046
Epoch 960, training loss: 0.009760066866874695 = 0.003057842142879963 + 0.001 * 6.7022247314453125
Epoch 960, val loss: 1.4820997714996338
Epoch 970, training loss: 0.009664853103458881 = 0.0029842229560017586 + 0.001 * 6.680629730224609
Epoch 970, val loss: 1.4869352579116821
Epoch 980, training loss: 0.009612825699150562 = 0.0029137853998690844 + 0.001 * 6.699039936065674
Epoch 980, val loss: 1.491674542427063
Epoch 990, training loss: 0.009543739259243011 = 0.0028463120106607676 + 0.001 * 6.697426795959473
Epoch 990, val loss: 1.4963370561599731
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6531
Flip ASR: 0.6133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.946345329284668 = 1.9379717111587524 + 0.001 * 8.373629570007324
Epoch 0, val loss: 1.9318829774856567
Epoch 10, training loss: 1.936263084411621 = 1.9278895854949951 + 0.001 * 8.373483657836914
Epoch 10, val loss: 1.9217933416366577
Epoch 20, training loss: 1.9235039949417114 = 1.9151309728622437 + 0.001 * 8.373056411743164
Epoch 20, val loss: 1.90847647190094
Epoch 30, training loss: 1.9050543308258057 = 1.8966820240020752 + 0.001 * 8.372246742248535
Epoch 30, val loss: 1.88883638381958
Epoch 40, training loss: 1.8775008916854858 = 1.8691303730010986 + 0.001 * 8.370538711547852
Epoch 40, val loss: 1.8599177598953247
Epoch 50, training loss: 1.8397494554519653 = 1.8313840627670288 + 0.001 * 8.365355491638184
Epoch 50, val loss: 1.822453260421753
Epoch 60, training loss: 1.7987779378890991 = 1.7904373407363892 + 0.001 * 8.340616226196289
Epoch 60, val loss: 1.7864861488342285
Epoch 70, training loss: 1.7598646879196167 = 1.7517304420471191 + 0.001 * 8.134297370910645
Epoch 70, val loss: 1.7558177709579468
Epoch 80, training loss: 1.706756830215454 = 1.699076533317566 + 0.001 * 7.6803507804870605
Epoch 80, val loss: 1.7123874425888062
Epoch 90, training loss: 1.635111927986145 = 1.6276181936264038 + 0.001 * 7.493729591369629
Epoch 90, val loss: 1.652712345123291
Epoch 100, training loss: 1.5462043285369873 = 1.5388838052749634 + 0.001 * 7.320551872253418
Epoch 100, val loss: 1.579683780670166
Epoch 110, training loss: 1.451982855796814 = 1.444861650466919 + 0.001 * 7.1212615966796875
Epoch 110, val loss: 1.5030897855758667
Epoch 120, training loss: 1.3594295978546143 = 1.3523989915847778 + 0.001 * 7.0306549072265625
Epoch 120, val loss: 1.4311089515686035
Epoch 130, training loss: 1.2667603492736816 = 1.2597874402999878 + 0.001 * 6.972867965698242
Epoch 130, val loss: 1.3606723546981812
Epoch 140, training loss: 1.1704744100570679 = 1.163521409034729 + 0.001 * 6.95297908782959
Epoch 140, val loss: 1.2885030508041382
Epoch 150, training loss: 1.0696717500686646 = 1.0627306699752808 + 0.001 * 6.941093444824219
Epoch 150, val loss: 1.2139592170715332
Epoch 160, training loss: 0.9662606716156006 = 0.9593337178230286 + 0.001 * 6.92694616317749
Epoch 160, val loss: 1.1380600929260254
Epoch 170, training loss: 0.8647282719612122 = 0.8578130006790161 + 0.001 * 6.915299892425537
Epoch 170, val loss: 1.0637290477752686
Epoch 180, training loss: 0.7703216075897217 = 0.7634153962135315 + 0.001 * 6.906201362609863
Epoch 180, val loss: 0.9949030876159668
Epoch 190, training loss: 0.686646580696106 = 0.6797471642494202 + 0.001 * 6.8994140625
Epoch 190, val loss: 0.934827983379364
Epoch 200, training loss: 0.6143603920936584 = 0.6074656844139099 + 0.001 * 6.894696235656738
Epoch 200, val loss: 0.8850718140602112
Epoch 210, training loss: 0.5521864295005798 = 0.5452952980995178 + 0.001 * 6.891136646270752
Epoch 210, val loss: 0.844726026058197
Epoch 220, training loss: 0.4983128011226654 = 0.4914243221282959 + 0.001 * 6.888467788696289
Epoch 220, val loss: 0.8125122785568237
Epoch 230, training loss: 0.45103275775909424 = 0.4441470801830292 + 0.001 * 6.885669231414795
Epoch 230, val loss: 0.7873131036758423
Epoch 240, training loss: 0.4087313115596771 = 0.40184909105300903 + 0.001 * 6.8822102546691895
Epoch 240, val loss: 0.7678707242012024
Epoch 250, training loss: 0.37020790576934814 = 0.36332905292510986 + 0.001 * 6.878849506378174
Epoch 250, val loss: 0.7527218461036682
Epoch 260, training loss: 0.33462968468666077 = 0.3277527987957001 + 0.001 * 6.876888275146484
Epoch 260, val loss: 0.7408168911933899
Epoch 270, training loss: 0.30139675736427307 = 0.29452401399612427 + 0.001 * 6.872742176055908
Epoch 270, val loss: 0.7312765121459961
Epoch 280, training loss: 0.27003732323646545 = 0.26316913962364197 + 0.001 * 6.868180751800537
Epoch 280, val loss: 0.7239816784858704
Epoch 290, training loss: 0.24039460718631744 = 0.23353013396263123 + 0.001 * 6.864468097686768
Epoch 290, val loss: 0.719111979007721
Epoch 300, training loss: 0.21261855959892273 = 0.20575612783432007 + 0.001 * 6.862433910369873
Epoch 300, val loss: 0.7166169881820679
Epoch 310, training loss: 0.1870003342628479 = 0.18014340102672577 + 0.001 * 6.856927394866943
Epoch 310, val loss: 0.7161456346511841
Epoch 320, training loss: 0.1639689952135086 = 0.1571168601512909 + 0.001 * 6.852134704589844
Epoch 320, val loss: 0.717924952507019
Epoch 330, training loss: 0.14366485178470612 = 0.13681510090827942 + 0.001 * 6.849746227264404
Epoch 330, val loss: 0.7216211557388306
Epoch 340, training loss: 0.12601763010025024 = 0.11917608976364136 + 0.001 * 6.841537952423096
Epoch 340, val loss: 0.7267510890960693
Epoch 350, training loss: 0.1108655035495758 = 0.10402599722146988 + 0.001 * 6.839508056640625
Epoch 350, val loss: 0.7329310774803162
Epoch 360, training loss: 0.0979108139872551 = 0.09108017385005951 + 0.001 * 6.830641269683838
Epoch 360, val loss: 0.7399161458015442
Epoch 370, training loss: 0.0868164449930191 = 0.07999394834041595 + 0.001 * 6.8224968910217285
Epoch 370, val loss: 0.7474012970924377
Epoch 380, training loss: 0.0773092657327652 = 0.07045228779315948 + 0.001 * 6.85698127746582
Epoch 380, val loss: 0.7553454637527466
Epoch 390, training loss: 0.06901471316814423 = 0.06220652163028717 + 0.001 * 6.808193683624268
Epoch 390, val loss: 0.7635528445243835
Epoch 400, training loss: 0.06186537817120552 = 0.0550619438290596 + 0.001 * 6.803434371948242
Epoch 400, val loss: 0.7720382213592529
Epoch 410, training loss: 0.05566297844052315 = 0.04887189343571663 + 0.001 * 6.7910847663879395
Epoch 410, val loss: 0.7807872295379639
Epoch 420, training loss: 0.05030693858861923 = 0.04350883886218071 + 0.001 * 6.798100471496582
Epoch 420, val loss: 0.789721667766571
Epoch 430, training loss: 0.04566522315144539 = 0.038864366710186005 + 0.001 * 6.8008575439453125
Epoch 430, val loss: 0.7989060878753662
Epoch 440, training loss: 0.04162207245826721 = 0.03483878821134567 + 0.001 * 6.783283233642578
Epoch 440, val loss: 0.80829918384552
Epoch 450, training loss: 0.03812374174594879 = 0.031345169991254807 + 0.001 * 6.778570652008057
Epoch 450, val loss: 0.8178646564483643
Epoch 460, training loss: 0.03508182242512703 = 0.028308290988206863 + 0.001 * 6.773531436920166
Epoch 460, val loss: 0.8275870084762573
Epoch 470, training loss: 0.032434627413749695 = 0.02566341869533062 + 0.001 * 6.771207332611084
Epoch 470, val loss: 0.837372362613678
Epoch 480, training loss: 0.030123401433229446 = 0.02335427701473236 + 0.001 * 6.769123554229736
Epoch 480, val loss: 0.8471815586090088
Epoch 490, training loss: 0.02810291014611721 = 0.02133246511220932 + 0.001 * 6.770445346832275
Epoch 490, val loss: 0.8569685220718384
Epoch 500, training loss: 0.026326578110456467 = 0.019556313753128052 + 0.001 * 6.770264625549316
Epoch 500, val loss: 0.8666647672653198
Epoch 510, training loss: 0.024755852296948433 = 0.017990076914429665 + 0.001 * 6.765775203704834
Epoch 510, val loss: 0.8762657046318054
Epoch 520, training loss: 0.02337004616856575 = 0.016603397205471992 + 0.001 * 6.76664924621582
Epoch 520, val loss: 0.8857433795928955
Epoch 530, training loss: 0.022137124091386795 = 0.015368577092885971 + 0.001 * 6.768546104431152
Epoch 530, val loss: 0.8951038122177124
Epoch 540, training loss: 0.021025894209742546 = 0.01426183246076107 + 0.001 * 6.76406192779541
Epoch 540, val loss: 0.9043821692466736
Epoch 550, training loss: 0.02003546617925167 = 0.013264387845993042 + 0.001 * 6.771078586578369
Epoch 550, val loss: 0.9134974479675293
Epoch 560, training loss: 0.019127093255519867 = 0.012362060137093067 + 0.001 * 6.765031814575195
Epoch 560, val loss: 0.9224122762680054
Epoch 570, training loss: 0.01830475777387619 = 0.011544162407517433 + 0.001 * 6.760595321655273
Epoch 570, val loss: 0.931198239326477
Epoch 580, training loss: 0.017563648521900177 = 0.010801926255226135 + 0.001 * 6.761721611022949
Epoch 580, val loss: 0.9397656917572021
Epoch 590, training loss: 0.016887426376342773 = 0.010127102956175804 + 0.001 * 6.760323524475098
Epoch 590, val loss: 0.9481359124183655
Epoch 600, training loss: 0.016273915767669678 = 0.009512767195701599 + 0.001 * 6.761148929595947
Epoch 600, val loss: 0.9563188552856445
Epoch 610, training loss: 0.015710927546024323 = 0.008952630683779716 + 0.001 * 6.758297443389893
Epoch 610, val loss: 0.9643175005912781
Epoch 620, training loss: 0.015194911509752274 = 0.008440887555480003 + 0.001 * 6.75402307510376
Epoch 620, val loss: 0.9721292853355408
Epoch 630, training loss: 0.014735120348632336 = 0.007972477935254574 + 0.001 * 6.762641906738281
Epoch 630, val loss: 0.9797590374946594
Epoch 640, training loss: 0.01429761853069067 = 0.0075430492870509624 + 0.001 * 6.754569053649902
Epoch 640, val loss: 0.9872075915336609
Epoch 650, training loss: 0.013918692246079445 = 0.007148429285734892 + 0.001 * 6.770262241363525
Epoch 650, val loss: 0.9944692850112915
Epoch 660, training loss: 0.013543235138058662 = 0.006785089150071144 + 0.001 * 6.758146286010742
Epoch 660, val loss: 1.0015524625778198
Epoch 670, training loss: 0.01320242416113615 = 0.00644991360604763 + 0.001 * 6.752510070800781
Epoch 670, val loss: 1.008483648300171
Epoch 680, training loss: 0.01289784349501133 = 0.006140189245343208 + 0.001 * 6.757654190063477
Epoch 680, val loss: 1.0152300596237183
Epoch 690, training loss: 0.01260810811072588 = 0.005853472277522087 + 0.001 * 6.754635334014893
Epoch 690, val loss: 1.02182137966156
Epoch 700, training loss: 0.012336707673966885 = 0.005587568040937185 + 0.001 * 6.749139308929443
Epoch 700, val loss: 1.0282306671142578
Epoch 710, training loss: 0.012091784738004208 = 0.00534047232940793 + 0.001 * 6.751312255859375
Epoch 710, val loss: 1.0344945192337036
Epoch 720, training loss: 0.011853847652673721 = 0.0051104836165905 + 0.001 * 6.743364334106445
Epoch 720, val loss: 1.0405935049057007
Epoch 730, training loss: 0.011642450466752052 = 0.00489609083160758 + 0.001 * 6.746359825134277
Epoch 730, val loss: 1.0465412139892578
Epoch 740, training loss: 0.011443030089139938 = 0.004695962183177471 + 0.001 * 6.747066974639893
Epoch 740, val loss: 1.052362322807312
Epoch 750, training loss: 0.011259175837039948 = 0.004508870653808117 + 0.001 * 6.750304222106934
Epoch 750, val loss: 1.0580475330352783
Epoch 760, training loss: 0.011067016050219536 = 0.004333633463829756 + 0.001 * 6.733382225036621
Epoch 760, val loss: 1.063602328300476
Epoch 770, training loss: 0.010903213173151016 = 0.00416912604123354 + 0.001 * 6.734086513519287
Epoch 770, val loss: 1.0690345764160156
Epoch 780, training loss: 0.01074649766087532 = 0.00401404220610857 + 0.001 * 6.732455730438232
Epoch 780, val loss: 1.074334979057312
Epoch 790, training loss: 0.01061078067868948 = 0.0038671584334224463 + 0.001 * 6.743622303009033
Epoch 790, val loss: 1.0795578956604004
Epoch 800, training loss: 0.01046608667820692 = 0.0037273033522069454 + 0.001 * 6.73878288269043
Epoch 800, val loss: 1.0846871137619019
Epoch 810, training loss: 0.010335026308894157 = 0.00359367742203176 + 0.001 * 6.7413482666015625
Epoch 810, val loss: 1.0897289514541626
Epoch 820, training loss: 0.010206755250692368 = 0.0034659523516893387 + 0.001 * 6.740802764892578
Epoch 820, val loss: 1.0947200059890747
Epoch 830, training loss: 0.01007977407425642 = 0.0033439036924391985 + 0.001 * 6.735870361328125
Epoch 830, val loss: 1.099617600440979
Epoch 840, training loss: 0.009967023506760597 = 0.003227406647056341 + 0.001 * 6.7396159172058105
Epoch 840, val loss: 1.104421615600586
Epoch 850, training loss: 0.009839850477874279 = 0.003116531064733863 + 0.001 * 6.723319053649902
Epoch 850, val loss: 1.1091023683547974
Epoch 860, training loss: 0.009737556800246239 = 0.0030109852086752653 + 0.001 * 6.726571559906006
Epoch 860, val loss: 1.1136913299560547
Epoch 870, training loss: 0.009622354991734028 = 0.0029106526635587215 + 0.001 * 6.7117018699646
Epoch 870, val loss: 1.118196725845337
Epoch 880, training loss: 0.009538596495985985 = 0.0028154300525784492 + 0.001 * 6.723165512084961
Epoch 880, val loss: 1.122631311416626
Epoch 890, training loss: 0.009441366419196129 = 0.0027250703424215317 + 0.001 * 6.7162957191467285
Epoch 890, val loss: 1.126957654953003
Epoch 900, training loss: 0.00937864650040865 = 0.002639234298840165 + 0.001 * 6.739412307739258
Epoch 900, val loss: 1.1312192678451538
Epoch 910, training loss: 0.009265821427106857 = 0.0025576697662472725 + 0.001 * 6.708151340484619
Epoch 910, val loss: 1.1353908777236938
Epoch 920, training loss: 0.009219297207891941 = 0.0024801448453217745 + 0.001 * 6.739152431488037
Epoch 920, val loss: 1.1395078897476196
Epoch 930, training loss: 0.00911327451467514 = 0.002406495623290539 + 0.001 * 6.706778526306152
Epoch 930, val loss: 1.143530011177063
Epoch 940, training loss: 0.009038126096129417 = 0.0023365283850580454 + 0.001 * 6.701597690582275
Epoch 940, val loss: 1.1474846601486206
Epoch 950, training loss: 0.008970129303634167 = 0.00226998352445662 + 0.001 * 6.700145244598389
Epoch 950, val loss: 1.151360273361206
Epoch 960, training loss: 0.008921184577047825 = 0.0022066987585276365 + 0.001 * 6.714485168457031
Epoch 960, val loss: 1.1551567316055298
Epoch 970, training loss: 0.008844387717545033 = 0.002146501326933503 + 0.001 * 6.6978864669799805
Epoch 970, val loss: 1.1588656902313232
Epoch 980, training loss: 0.008806348778307438 = 0.0020892161410301924 + 0.001 * 6.717132568359375
Epoch 980, val loss: 1.1625425815582275
Epoch 990, training loss: 0.008736781775951385 = 0.0020346699748188257 + 0.001 * 6.702111721038818
Epoch 990, val loss: 1.1661274433135986
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.7454
Flip ASR: 0.7111/225 nodes
The final ASR:0.63715, 0.09558, Accuracy:0.80370, 0.00605
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11660])
remove edge: torch.Size([2, 9452])
updated graph: torch.Size([2, 10556])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97786, 0.00603, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9458768367767334 = 1.9375030994415283 + 0.001 * 8.373795509338379
Epoch 0, val loss: 1.928848147392273
Epoch 10, training loss: 1.935986042022705 = 1.9276123046875 + 0.001 * 8.373709678649902
Epoch 10, val loss: 1.9197614192962646
Epoch 20, training loss: 1.9238569736480713 = 1.9154834747314453 + 0.001 * 8.373469352722168
Epoch 20, val loss: 1.9083396196365356
Epoch 30, training loss: 1.9069095849990845 = 1.8985365629196167 + 0.001 * 8.37299919128418
Epoch 30, val loss: 1.8921998739242554
Epoch 40, training loss: 1.882170557975769 = 1.8737986087799072 + 0.001 * 8.371989250183105
Epoch 40, val loss: 1.8688926696777344
Epoch 50, training loss: 1.8476686477661133 = 1.8392994403839111 + 0.001 * 8.369200706481934
Epoch 50, val loss: 1.8381232023239136
Epoch 60, training loss: 1.8070915937423706 = 1.7987334728240967 + 0.001 * 8.358139038085938
Epoch 60, val loss: 1.8060858249664307
Epoch 70, training loss: 1.7663500308990479 = 1.7580548524856567 + 0.001 * 8.295193672180176
Epoch 70, val loss: 1.7766892910003662
Epoch 80, training loss: 1.7138164043426514 = 1.7058627605438232 + 0.001 * 7.953674793243408
Epoch 80, val loss: 1.733554482460022
Epoch 90, training loss: 1.640937328338623 = 1.6331473588943481 + 0.001 * 7.789942741394043
Epoch 90, val loss: 1.6720046997070312
Epoch 100, training loss: 1.5475573539733887 = 1.5398869514465332 + 0.001 * 7.670401096343994
Epoch 100, val loss: 1.5965672731399536
Epoch 110, training loss: 1.4428292512893677 = 1.435381293296814 + 0.001 * 7.448004722595215
Epoch 110, val loss: 1.5145056247711182
Epoch 120, training loss: 1.3376482725143433 = 1.3303700685501099 + 0.001 * 7.278241157531738
Epoch 120, val loss: 1.4354052543640137
Epoch 130, training loss: 1.2374483346939087 = 1.2302337884902954 + 0.001 * 7.214550495147705
Epoch 130, val loss: 1.3625168800354004
Epoch 140, training loss: 1.1439518928527832 = 1.136805772781372 + 0.001 * 7.146130084991455
Epoch 140, val loss: 1.2962225675582886
Epoch 150, training loss: 1.0582126379013062 = 1.0510905981063843 + 0.001 * 7.12200927734375
Epoch 150, val loss: 1.2358819246292114
Epoch 160, training loss: 0.979667067527771 = 0.9725533127784729 + 0.001 * 7.11377477645874
Epoch 160, val loss: 1.1800498962402344
Epoch 170, training loss: 0.9060908555984497 = 0.8989875912666321 + 0.001 * 7.103278160095215
Epoch 170, val loss: 1.1272879838943481
Epoch 180, training loss: 0.8341262936592102 = 0.8270338773727417 + 0.001 * 7.092422008514404
Epoch 180, val loss: 1.075883388519287
Epoch 190, training loss: 0.7609226107597351 = 0.7538428902626038 + 0.001 * 7.079749584197998
Epoch 190, val loss: 1.0239545106887817
Epoch 200, training loss: 0.685665488243103 = 0.6786013245582581 + 0.001 * 7.06416654586792
Epoch 200, val loss: 0.9715481400489807
Epoch 210, training loss: 0.6099988222122192 = 0.6029546856880188 + 0.001 * 7.044118404388428
Epoch 210, val loss: 0.9198374152183533
Epoch 220, training loss: 0.5370935797691345 = 0.5300696492195129 + 0.001 * 7.023914337158203
Epoch 220, val loss: 0.8718731999397278
Epoch 230, training loss: 0.4701007604598999 = 0.46309104561805725 + 0.001 * 7.009706974029541
Epoch 230, val loss: 0.8308403491973877
Epoch 240, training loss: 0.41068705916404724 = 0.4036897122859955 + 0.001 * 6.997348785400391
Epoch 240, val loss: 0.798726499080658
Epoch 250, training loss: 0.35887303948402405 = 0.3518822491168976 + 0.001 * 6.9907965660095215
Epoch 250, val loss: 0.7755374312400818
Epoch 260, training loss: 0.31376999616622925 = 0.30677610635757446 + 0.001 * 6.993900775909424
Epoch 260, val loss: 0.7597798705101013
Epoch 270, training loss: 0.2742159068584442 = 0.26722973585128784 + 0.001 * 6.986158847808838
Epoch 270, val loss: 0.7497918605804443
Epoch 280, training loss: 0.23935893177986145 = 0.23237839341163635 + 0.001 * 6.980532646179199
Epoch 280, val loss: 0.7443544864654541
Epoch 290, training loss: 0.20868800580501556 = 0.20171242952346802 + 0.001 * 6.975569725036621
Epoch 290, val loss: 0.7428430914878845
Epoch 300, training loss: 0.18192891776561737 = 0.17495658993721008 + 0.001 * 6.972334384918213
Epoch 300, val loss: 0.744610071182251
Epoch 310, training loss: 0.15884467959403992 = 0.15187598764896393 + 0.001 * 6.968685626983643
Epoch 310, val loss: 0.7493759393692017
Epoch 320, training loss: 0.1391662359237671 = 0.1321951448917389 + 0.001 * 6.971086025238037
Epoch 320, val loss: 0.7568724751472473
Epoch 330, training loss: 0.1225026324391365 = 0.11554011702537537 + 0.001 * 6.962514877319336
Epoch 330, val loss: 0.766701340675354
Epoch 340, training loss: 0.10843425989151001 = 0.10147769004106522 + 0.001 * 6.956567764282227
Epoch 340, val loss: 0.778468668460846
Epoch 350, training loss: 0.09653627872467041 = 0.08957045525312424 + 0.001 * 6.965823650360107
Epoch 350, val loss: 0.7917798757553101
Epoch 360, training loss: 0.08639334887266159 = 0.07944022864103317 + 0.001 * 6.953117847442627
Epoch 360, val loss: 0.8061589002609253
Epoch 370, training loss: 0.07773125171661377 = 0.0707796961069107 + 0.001 * 6.951553821563721
Epoch 370, val loss: 0.8212525248527527
Epoch 380, training loss: 0.07028120011091232 = 0.06333529949188232 + 0.001 * 6.9459004402160645
Epoch 380, val loss: 0.8368855118751526
Epoch 390, training loss: 0.06384941190481186 = 0.056905630975961685 + 0.001 * 6.943781852722168
Epoch 390, val loss: 0.8528703451156616
Epoch 400, training loss: 0.05826019495725632 = 0.05132598429918289 + 0.001 * 6.934209823608398
Epoch 400, val loss: 0.8690615296363831
Epoch 410, training loss: 0.05339772254228592 = 0.04646659269928932 + 0.001 * 6.931130409240723
Epoch 410, val loss: 0.885222852230072
Epoch 420, training loss: 0.04915238171815872 = 0.04222029075026512 + 0.001 * 6.932092189788818
Epoch 420, val loss: 0.9013155102729797
Epoch 430, training loss: 0.04542473703622818 = 0.03849417716264725 + 0.001 * 6.930558204650879
Epoch 430, val loss: 0.9172002077102661
Epoch 440, training loss: 0.04214119911193848 = 0.03521338477730751 + 0.001 * 6.927813529968262
Epoch 440, val loss: 0.9329877495765686
Epoch 450, training loss: 0.039238136261701584 = 0.03231428563594818 + 0.001 * 6.923851490020752
Epoch 450, val loss: 0.9485272169113159
Epoch 460, training loss: 0.036665137857198715 = 0.02974386140704155 + 0.001 * 6.921276569366455
Epoch 460, val loss: 0.963663637638092
Epoch 470, training loss: 0.034373585134744644 = 0.027456583455204964 + 0.001 * 6.917001724243164
Epoch 470, val loss: 0.9785496592521667
Epoch 480, training loss: 0.03234723582863808 = 0.025414688512682915 + 0.001 * 6.932546138763428
Epoch 480, val loss: 0.9930950999259949
Epoch 490, training loss: 0.03050433099269867 = 0.02358625829219818 + 0.001 * 6.91807222366333
Epoch 490, val loss: 1.0072875022888184
Epoch 500, training loss: 0.02885577641427517 = 0.02194313518702984 + 0.001 * 6.912641525268555
Epoch 500, val loss: 1.021128535270691
Epoch 510, training loss: 0.027376843616366386 = 0.020462194457650185 + 0.001 * 6.914648532867432
Epoch 510, val loss: 1.0345745086669922
Epoch 520, training loss: 0.026033475995063782 = 0.019123218953609467 + 0.001 * 6.9102559089660645
Epoch 520, val loss: 1.0476573705673218
Epoch 530, training loss: 0.024819891899824142 = 0.01790953427553177 + 0.001 * 6.910357475280762
Epoch 530, val loss: 1.0604026317596436
Epoch 540, training loss: 0.023723158985376358 = 0.016806017607450485 + 0.001 * 6.917140483856201
Epoch 540, val loss: 1.07283616065979
Epoch 550, training loss: 0.022712400183081627 = 0.01580016501247883 + 0.001 * 6.912234306335449
Epoch 550, val loss: 1.0848841667175293
Epoch 560, training loss: 0.021791478618979454 = 0.014881174080073833 + 0.001 * 6.910303592681885
Epoch 560, val loss: 1.0966588258743286
Epoch 570, training loss: 0.020951468497514725 = 0.014039249159395695 + 0.001 * 6.912219047546387
Epoch 570, val loss: 1.108060598373413
Epoch 580, training loss: 0.020172590389847755 = 0.013266502879559994 + 0.001 * 6.9060869216918945
Epoch 580, val loss: 1.119188904762268
Epoch 590, training loss: 0.019458621740341187 = 0.01255545299500227 + 0.001 * 6.903168678283691
Epoch 590, val loss: 1.1300140619277954
Epoch 600, training loss: 0.01880299486219883 = 0.011899867095053196 + 0.001 * 6.903127670288086
Epoch 600, val loss: 1.1405601501464844
Epoch 610, training loss: 0.01819423958659172 = 0.011293982155621052 + 0.001 * 6.900256156921387
Epoch 610, val loss: 1.1507879495620728
Epoch 620, training loss: 0.01763223670423031 = 0.010732867754995823 + 0.001 * 6.899369239807129
Epoch 620, val loss: 1.1607989072799683
Epoch 630, training loss: 0.017107708379626274 = 0.01021096482872963 + 0.001 * 6.896742820739746
Epoch 630, val loss: 1.1705529689788818
Epoch 640, training loss: 0.016639068722724915 = 0.009723510593175888 + 0.001 * 6.915558338165283
Epoch 640, val loss: 1.1801552772521973
Epoch 650, training loss: 0.016161585226655006 = 0.009266423061490059 + 0.001 * 6.895162105560303
Epoch 650, val loss: 1.189563274383545
Epoch 660, training loss: 0.015729425475001335 = 0.008836486376821995 + 0.001 * 6.89293909072876
Epoch 660, val loss: 1.198837161064148
Epoch 670, training loss: 0.015338387340307236 = 0.008431385271251202 + 0.001 * 6.907001495361328
Epoch 670, val loss: 1.2079654932022095
Epoch 680, training loss: 0.01493855845183134 = 0.008049674332141876 + 0.001 * 6.888883590698242
Epoch 680, val loss: 1.2170002460479736
Epoch 690, training loss: 0.014574220404028893 = 0.007690411526709795 + 0.001 * 6.8838090896606445
Epoch 690, val loss: 1.2258909940719604
Epoch 700, training loss: 0.014240710064768791 = 0.0073524401523172855 + 0.001 * 6.888268947601318
Epoch 700, val loss: 1.2346879243850708
Epoch 710, training loss: 0.013919329270720482 = 0.007034617010504007 + 0.001 * 6.884711265563965
Epoch 710, val loss: 1.24327552318573
Epoch 720, training loss: 0.013658476993441582 = 0.006735857576131821 + 0.001 * 6.922618865966797
Epoch 720, val loss: 1.2517430782318115
Epoch 730, training loss: 0.013343339785933495 = 0.006454814225435257 + 0.001 * 6.888525009155273
Epoch 730, val loss: 1.2600494623184204
Epoch 740, training loss: 0.013066437095403671 = 0.006189960055053234 + 0.001 * 6.876477241516113
Epoch 740, val loss: 1.268211007118225
Epoch 750, training loss: 0.012824112549424171 = 0.005940128583461046 + 0.001 * 6.883984088897705
Epoch 750, val loss: 1.276235818862915
Epoch 760, training loss: 0.012590516358613968 = 0.005704890470951796 + 0.001 * 6.88562536239624
Epoch 760, val loss: 1.284084439277649
Epoch 770, training loss: 0.012354301288723946 = 0.0054833414033055305 + 0.001 * 6.870960235595703
Epoch 770, val loss: 1.291810393333435
Epoch 780, training loss: 0.012150270864367485 = 0.005274605471640825 + 0.001 * 6.875665664672852
Epoch 780, val loss: 1.299330472946167
Epoch 790, training loss: 0.011945636942982674 = 0.005077765788882971 + 0.001 * 6.867871284484863
Epoch 790, val loss: 1.306732177734375
Epoch 800, training loss: 0.011769900098443031 = 0.004891960881650448 + 0.001 * 6.877938270568848
Epoch 800, val loss: 1.3139615058898926
Epoch 810, training loss: 0.011589445173740387 = 0.004716545343399048 + 0.001 * 6.872899055480957
Epoch 810, val loss: 1.3210692405700684
Epoch 820, training loss: 0.01140937115997076 = 0.004550797399133444 + 0.001 * 6.8585734367370605
Epoch 820, val loss: 1.3280185461044312
Epoch 830, training loss: 0.011254568584263325 = 0.004394072107970715 + 0.001 * 6.8604960441589355
Epoch 830, val loss: 1.3348109722137451
Epoch 840, training loss: 0.011108886450529099 = 0.004245723132044077 + 0.001 * 6.863162994384766
Epoch 840, val loss: 1.3414844274520874
Epoch 850, training loss: 0.010966421104967594 = 0.004105288069695234 + 0.001 * 6.861132621765137
Epoch 850, val loss: 1.3480224609375
Epoch 860, training loss: 0.010825064033269882 = 0.003972161095589399 + 0.001 * 6.852901935577393
Epoch 860, val loss: 1.3544206619262695
Epoch 870, training loss: 0.010700298473238945 = 0.0038459175266325474 + 0.001 * 6.854380130767822
Epoch 870, val loss: 1.3606964349746704
Epoch 880, training loss: 0.010575739666819572 = 0.0037260737735778093 + 0.001 * 6.849665641784668
Epoch 880, val loss: 1.3668338060379028
Epoch 890, training loss: 0.010457290336489677 = 0.003612156491726637 + 0.001 * 6.845133304595947
Epoch 890, val loss: 1.3728256225585938
Epoch 900, training loss: 0.010372110642492771 = 0.0035037228371948004 + 0.001 * 6.868387699127197
Epoch 900, val loss: 1.3787304162979126
Epoch 910, training loss: 0.010256914421916008 = 0.003400525078177452 + 0.001 * 6.856388568878174
Epoch 910, val loss: 1.3845161199569702
Epoch 920, training loss: 0.010160708799958229 = 0.003302167635411024 + 0.001 * 6.858541488647461
Epoch 920, val loss: 1.3901687860488892
Epoch 930, training loss: 0.010048608295619488 = 0.003208432113751769 + 0.001 * 6.840176105499268
Epoch 930, val loss: 1.3957266807556152
Epoch 940, training loss: 0.009963750839233398 = 0.003119091968983412 + 0.001 * 6.844658374786377
Epoch 940, val loss: 1.4011785984039307
Epoch 950, training loss: 0.009865807369351387 = 0.0030337851494550705 + 0.001 * 6.832021236419678
Epoch 950, val loss: 1.406527042388916
Epoch 960, training loss: 0.009819661267101765 = 0.002952391980215907 + 0.001 * 6.8672685623168945
Epoch 960, val loss: 1.4117900133132935
Epoch 970, training loss: 0.009703584015369415 = 0.002874694997444749 + 0.001 * 6.828888416290283
Epoch 970, val loss: 1.416918396949768
Epoch 980, training loss: 0.009621025994420052 = 0.002800491638481617 + 0.001 * 6.820533752441406
Epoch 980, val loss: 1.4219918251037598
Epoch 990, training loss: 0.00954902172088623 = 0.0027295395266264677 + 0.001 * 6.81948184967041
Epoch 990, val loss: 1.42693030834198
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.8081
Flip ASR: 0.7689/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9509193897247314 = 1.9425455331802368 + 0.001 * 8.373835563659668
Epoch 0, val loss: 1.9324300289154053
Epoch 10, training loss: 1.9406170845031738 = 1.9322433471679688 + 0.001 * 8.373741149902344
Epoch 10, val loss: 1.9218326807022095
Epoch 20, training loss: 1.9278788566589355 = 1.9195053577423096 + 0.001 * 8.373492240905762
Epoch 20, val loss: 1.9086248874664307
Epoch 30, training loss: 1.9099948406219482 = 1.9016218185424805 + 0.001 * 8.37300968170166
Epoch 30, val loss: 1.8903084993362427
Epoch 40, training loss: 1.8837686777114868 = 1.8753966093063354 + 0.001 * 8.372015953063965
Epoch 40, val loss: 1.8642550706863403
Epoch 50, training loss: 1.8470230102539062 = 1.838653564453125 + 0.001 * 8.369386672973633
Epoch 50, val loss: 1.8293776512145996
Epoch 60, training loss: 1.8022480010986328 = 1.793889045715332 + 0.001 * 8.358895301818848
Epoch 60, val loss: 1.7900563478469849
Epoch 70, training loss: 1.7548671960830688 = 1.7465769052505493 + 0.001 * 8.290321350097656
Epoch 70, val loss: 1.7502490282058716
Epoch 80, training loss: 1.6955900192260742 = 1.6876758337020874 + 0.001 * 7.914157867431641
Epoch 80, val loss: 1.696568489074707
Epoch 90, training loss: 1.6163330078125 = 1.6085692644119263 + 0.001 * 7.76378059387207
Epoch 90, val loss: 1.6254222393035889
Epoch 100, training loss: 1.5184730291366577 = 1.5108054876327515 + 0.001 * 7.667525291442871
Epoch 100, val loss: 1.544008493423462
Epoch 110, training loss: 1.4147111177444458 = 1.4072669744491577 + 0.001 * 7.444156169891357
Epoch 110, val loss: 1.4620939493179321
Epoch 120, training loss: 1.3167484998703003 = 1.3095242977142334 + 0.001 * 7.224151134490967
Epoch 120, val loss: 1.3895621299743652
Epoch 130, training loss: 1.2290626764297485 = 1.2219167947769165 + 0.001 * 7.1458420753479
Epoch 130, val loss: 1.3280014991760254
Epoch 140, training loss: 1.149617314338684 = 1.1425336599349976 + 0.001 * 7.083637237548828
Epoch 140, val loss: 1.274372935295105
Epoch 150, training loss: 1.0744880437850952 = 1.0674446821212769 + 0.001 * 7.043370723724365
Epoch 150, val loss: 1.2242722511291504
Epoch 160, training loss: 1.0014351606369019 = 0.9944166541099548 + 0.001 * 7.018543243408203
Epoch 160, val loss: 1.1753722429275513
Epoch 170, training loss: 0.9301596283912659 = 0.9231600761413574 + 0.001 * 6.999551773071289
Epoch 170, val loss: 1.1265789270401
Epoch 180, training loss: 0.860575258731842 = 0.8535900115966797 + 0.001 * 6.985230922698975
Epoch 180, val loss: 1.0780038833618164
Epoch 190, training loss: 0.7916468977928162 = 0.7846698760986328 + 0.001 * 6.977012634277344
Epoch 190, val loss: 1.028761386871338
Epoch 200, training loss: 0.7225000858306885 = 0.7155280113220215 + 0.001 * 6.972097396850586
Epoch 200, val loss: 0.9786485433578491
Epoch 210, training loss: 0.6533260345458984 = 0.6463582515716553 + 0.001 * 6.967799663543701
Epoch 210, val loss: 0.9288285374641418
Epoch 220, training loss: 0.5853161811828613 = 0.578352689743042 + 0.001 * 6.963486671447754
Epoch 220, val loss: 0.8814886808395386
Epoch 230, training loss: 0.5199941396713257 = 0.5130350589752197 + 0.001 * 6.9590888023376465
Epoch 230, val loss: 0.8392589688301086
Epoch 240, training loss: 0.45886772871017456 = 0.45191317796707153 + 0.001 * 6.954535484313965
Epoch 240, val loss: 0.8035463690757751
Epoch 250, training loss: 0.4032362699508667 = 0.3962864875793457 + 0.001 * 6.949791431427002
Epoch 250, val loss: 0.7743914127349854
Epoch 260, training loss: 0.3537881076335907 = 0.3468433618545532 + 0.001 * 6.944758892059326
Epoch 260, val loss: 0.751581609249115
Epoch 270, training loss: 0.3103484809398651 = 0.30340975522994995 + 0.001 * 6.938712120056152
Epoch 270, val loss: 0.7346989512443542
Epoch 280, training loss: 0.2721615731716156 = 0.26522958278656006 + 0.001 * 6.931997299194336
Epoch 280, val loss: 0.7228227853775024
Epoch 290, training loss: 0.23850823938846588 = 0.2315818965435028 + 0.001 * 6.92633581161499
Epoch 290, val loss: 0.7151544690132141
Epoch 300, training loss: 0.20899006724357605 = 0.20207306742668152 + 0.001 * 6.917004585266113
Epoch 300, val loss: 0.7111877202987671
Epoch 310, training loss: 0.1833951622247696 = 0.17647312581539154 + 0.001 * 6.9220356941223145
Epoch 310, val loss: 0.7109493017196655
Epoch 320, training loss: 0.16141879558563232 = 0.15451118350028992 + 0.001 * 6.907613277435303
Epoch 320, val loss: 0.7142414450645447
Epoch 330, training loss: 0.1426825374364853 = 0.1357872039079666 + 0.001 * 6.895329475402832
Epoch 330, val loss: 0.7204501032829285
Epoch 340, training loss: 0.12681233882904053 = 0.11992458254098892 + 0.001 * 6.88775634765625
Epoch 340, val loss: 0.7291465401649475
Epoch 350, training loss: 0.11335661262273788 = 0.10647168010473251 + 0.001 * 6.884929656982422
Epoch 350, val loss: 0.7399086952209473
Epoch 360, training loss: 0.10183896124362946 = 0.09496196359395981 + 0.001 * 6.876999855041504
Epoch 360, val loss: 0.7518961429595947
Epoch 370, training loss: 0.09193402528762817 = 0.0850527361035347 + 0.001 * 6.881288051605225
Epoch 370, val loss: 0.7650063633918762
Epoch 380, training loss: 0.08330059051513672 = 0.07642617076635361 + 0.001 * 6.874416351318359
Epoch 380, val loss: 0.7787744998931885
Epoch 390, training loss: 0.07571809738874435 = 0.06885125488042831 + 0.001 * 6.866844654083252
Epoch 390, val loss: 0.7928732633590698
Epoch 400, training loss: 0.06899253278970718 = 0.06212766841053963 + 0.001 * 6.864863395690918
Epoch 400, val loss: 0.8069997429847717
Epoch 410, training loss: 0.06296258419752121 = 0.056095726788043976 + 0.001 * 6.866857051849365
Epoch 410, val loss: 0.8211196064949036
Epoch 420, training loss: 0.05750570073723793 = 0.0506422333419323 + 0.001 * 6.863467693328857
Epoch 420, val loss: 0.8351088762283325
Epoch 430, training loss: 0.05256602540612221 = 0.045700803399086 + 0.001 * 6.865222930908203
Epoch 430, val loss: 0.8490508198738098
Epoch 440, training loss: 0.04809776693582535 = 0.041233256459236145 + 0.001 * 6.864510536193848
Epoch 440, val loss: 0.8629341125488281
Epoch 450, training loss: 0.04409285634756088 = 0.03723384812474251 + 0.001 * 6.859006404876709
Epoch 450, val loss: 0.8767181038856506
Epoch 460, training loss: 0.04053743928670883 = 0.033680450171232224 + 0.001 * 6.856990814208984
Epoch 460, val loss: 0.8904321789741516
Epoch 470, training loss: 0.03742709755897522 = 0.030560985207557678 + 0.001 * 6.866113662719727
Epoch 470, val loss: 0.9039013981819153
Epoch 480, training loss: 0.03469153866171837 = 0.027835091575980186 + 0.001 * 6.856448173522949
Epoch 480, val loss: 0.9175953269004822
Epoch 490, training loss: 0.03230522945523262 = 0.025450805202126503 + 0.001 * 6.8544230461120605
Epoch 490, val loss: 0.9310994744300842
Epoch 500, training loss: 0.030208608135581017 = 0.02335904724895954 + 0.001 * 6.849560260772705
Epoch 500, val loss: 0.9443047046661377
Epoch 510, training loss: 0.028365565463900566 = 0.021517349407076836 + 0.001 * 6.848215579986572
Epoch 510, val loss: 0.9571843147277832
Epoch 520, training loss: 0.026734037324786186 = 0.01988794654607773 + 0.001 * 6.846089839935303
Epoch 520, val loss: 0.9697040915489197
Epoch 530, training loss: 0.025283023715019226 = 0.018439659848809242 + 0.001 * 6.8433637619018555
Epoch 530, val loss: 0.9818179607391357
Epoch 540, training loss: 0.023991912603378296 = 0.017147935926914215 + 0.001 * 6.84397554397583
Epoch 540, val loss: 0.9935316443443298
Epoch 550, training loss: 0.022836720570921898 = 0.015991589054465294 + 0.001 * 6.8451313972473145
Epoch 550, val loss: 1.004897117614746
Epoch 560, training loss: 0.0217947568744421 = 0.01495253574103117 + 0.001 * 6.842220306396484
Epoch 560, val loss: 1.0158966779708862
Epoch 570, training loss: 0.02084987610578537 = 0.01401532907038927 + 0.001 * 6.834545612335205
Epoch 570, val loss: 1.026632308959961
Epoch 580, training loss: 0.020000476390123367 = 0.013166108168661594 + 0.001 * 6.8343682289123535
Epoch 580, val loss: 1.0370354652404785
Epoch 590, training loss: 0.01922757178544998 = 0.012394503690302372 + 0.001 * 6.833068370819092
Epoch 590, val loss: 1.0471508502960205
Epoch 600, training loss: 0.01851923204958439 = 0.011691633611917496 + 0.001 * 6.827598571777344
Epoch 600, val loss: 1.056965947151184
Epoch 610, training loss: 0.01787498965859413 = 0.011049824766814709 + 0.001 * 6.825165271759033
Epoch 610, val loss: 1.0665173530578613
Epoch 620, training loss: 0.01729501597583294 = 0.010463573969900608 + 0.001 * 6.831442356109619
Epoch 620, val loss: 1.0757697820663452
Epoch 630, training loss: 0.01675180159509182 = 0.009925554506480694 + 0.001 * 6.826246738433838
Epoch 630, val loss: 1.084741473197937
Epoch 640, training loss: 0.01625324971973896 = 0.00943037774413824 + 0.001 * 6.822871208190918
Epoch 640, val loss: 1.0934456586837769
Epoch 650, training loss: 0.015789203345775604 = 0.008973641321063042 + 0.001 * 6.815561771392822
Epoch 650, val loss: 1.1019309759140015
Epoch 660, training loss: 0.015401876531541348 = 0.00855143740773201 + 0.001 * 6.850438594818115
Epoch 660, val loss: 1.1102030277252197
Epoch 670, training loss: 0.014988835901021957 = 0.008160248398780823 + 0.001 * 6.828586578369141
Epoch 670, val loss: 1.1182255744934082
Epoch 680, training loss: 0.014613719657063484 = 0.0077972132712602615 + 0.001 * 6.816505432128906
Epoch 680, val loss: 1.126112937927246
Epoch 690, training loss: 0.014267797581851482 = 0.007459447719156742 + 0.001 * 6.808349609375
Epoch 690, val loss: 1.1337553262710571
Epoch 700, training loss: 0.013964168727397919 = 0.0071447743102908134 + 0.001 * 6.819393634796143
Epoch 700, val loss: 1.1412322521209717
Epoch 710, training loss: 0.01366896741092205 = 0.006850832607597113 + 0.001 * 6.818134784698486
Epoch 710, val loss: 1.1485459804534912
Epoch 720, training loss: 0.01338079571723938 = 0.006576120853424072 + 0.001 * 6.80467414855957
Epoch 720, val loss: 1.1556742191314697
Epoch 730, training loss: 0.013119079172611237 = 0.006318733561784029 + 0.001 * 6.800344944000244
Epoch 730, val loss: 1.1626954078674316
Epoch 740, training loss: 0.012877022847533226 = 0.0060770767740905285 + 0.001 * 6.799946308135986
Epoch 740, val loss: 1.169564127922058
Epoch 750, training loss: 0.012668638490140438 = 0.005849597975611687 + 0.001 * 6.819040298461914
Epoch 750, val loss: 1.1763582229614258
Epoch 760, training loss: 0.012430761009454727 = 0.005635370034724474 + 0.001 * 6.795390605926514
Epoch 760, val loss: 1.182931900024414
Epoch 770, training loss: 0.012243425473570824 = 0.005433799233287573 + 0.001 * 6.809625625610352
Epoch 770, val loss: 1.1892863512039185
Epoch 780, training loss: 0.012044084258377552 = 0.005243662744760513 + 0.001 * 6.800421237945557
Epoch 780, val loss: 1.1955795288085938
Epoch 790, training loss: 0.011860206723213196 = 0.005064013414084911 + 0.001 * 6.7961931228637695
Epoch 790, val loss: 1.2018287181854248
Epoch 800, training loss: 0.011692885309457779 = 0.004894246347248554 + 0.001 * 6.798638820648193
Epoch 800, val loss: 1.2079447507858276
Epoch 810, training loss: 0.011525211855769157 = 0.004733571317046881 + 0.001 * 6.791640281677246
Epoch 810, val loss: 1.2138898372650146
Epoch 820, training loss: 0.011365756392478943 = 0.004581392277032137 + 0.001 * 6.784364223480225
Epoch 820, val loss: 1.2197362184524536
Epoch 830, training loss: 0.01122937723994255 = 0.004437088966369629 + 0.001 * 6.792287349700928
Epoch 830, val loss: 1.2254782915115356
Epoch 840, training loss: 0.011085143312811852 = 0.0043001361191272736 + 0.001 * 6.785006523132324
Epoch 840, val loss: 1.231155276298523
Epoch 850, training loss: 0.010960867628455162 = 0.00417002197355032 + 0.001 * 6.79084587097168
Epoch 850, val loss: 1.2366914749145508
Epoch 860, training loss: 0.0108279287815094 = 0.004046319518238306 + 0.001 * 6.781609058380127
Epoch 860, val loss: 1.24210786819458
Epoch 870, training loss: 0.010717084631323814 = 0.003928736317902803 + 0.001 * 6.7883477210998535
Epoch 870, val loss: 1.2474955320358276
Epoch 880, training loss: 0.010596424341201782 = 0.0038167841266840696 + 0.001 * 6.779639720916748
Epoch 880, val loss: 1.2526861429214478
Epoch 890, training loss: 0.010509088635444641 = 0.0037100864574313164 + 0.001 * 6.799002170562744
Epoch 890, val loss: 1.2578328847885132
Epoch 900, training loss: 0.010382621549069881 = 0.0036083348095417023 + 0.001 * 6.774286270141602
Epoch 900, val loss: 1.2629172801971436
Epoch 910, training loss: 0.010280751623213291 = 0.003511186456307769 + 0.001 * 6.769565105438232
Epoch 910, val loss: 1.2678855657577515
Epoch 920, training loss: 0.010208097286522388 = 0.0034183799289166927 + 0.001 * 6.789717197418213
Epoch 920, val loss: 1.27278470993042
Epoch 930, training loss: 0.010109594091773033 = 0.0033296996261924505 + 0.001 * 6.77989387512207
Epoch 930, val loss: 1.2775863409042358
Epoch 940, training loss: 0.010018400847911835 = 0.003244953230023384 + 0.001 * 6.7734479904174805
Epoch 940, val loss: 1.2823258638381958
Epoch 950, training loss: 0.009933523833751678 = 0.0031638960354030132 + 0.001 * 6.769628047943115
Epoch 950, val loss: 1.2869199514389038
Epoch 960, training loss: 0.009856847114861012 = 0.003086329670622945 + 0.001 * 6.770516872406006
Epoch 960, val loss: 1.2915266752243042
Epoch 970, training loss: 0.009771285578608513 = 0.0030119323637336493 + 0.001 * 6.759353160858154
Epoch 970, val loss: 1.2959762811660767
Epoch 980, training loss: 0.009711449034512043 = 0.002940552541986108 + 0.001 * 6.7708964347839355
Epoch 980, val loss: 1.3003994226455688
Epoch 990, training loss: 0.009647144936025143 = 0.0028720940463244915 + 0.001 * 6.775050640106201
Epoch 990, val loss: 1.3047239780426025
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.6531
Flip ASR: 0.6044/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9511897563934326 = 1.942815899848938 + 0.001 * 8.37380313873291
Epoch 0, val loss: 1.935978651046753
Epoch 10, training loss: 1.9415489435195923 = 1.9331752061843872 + 0.001 * 8.373746871948242
Epoch 10, val loss: 1.9263628721237183
Epoch 20, training loss: 1.9304615259170532 = 1.9220880270004272 + 0.001 * 8.373516082763672
Epoch 20, val loss: 1.915196180343628
Epoch 30, training loss: 1.915737509727478 = 1.9073644876480103 + 0.001 * 8.373048782348633
Epoch 30, val loss: 1.900567650794983
Epoch 40, training loss: 1.8947036266326904 = 1.886331558227539 + 0.001 * 8.372092247009277
Epoch 40, val loss: 1.8800143003463745
Epoch 50, training loss: 1.864532232284546 = 1.856162428855896 + 0.001 * 8.369817733764648
Epoch 50, val loss: 1.8516383171081543
Epoch 60, training loss: 1.8251705169677734 = 1.8168079853057861 + 0.001 * 8.362534523010254
Epoch 60, val loss: 1.8177319765090942
Epoch 70, training loss: 1.784152626991272 = 1.7758259773254395 + 0.001 * 8.326623916625977
Epoch 70, val loss: 1.7863967418670654
Epoch 80, training loss: 1.740544080734253 = 1.7324849367141724 + 0.001 * 8.059163093566895
Epoch 80, val loss: 1.7508254051208496
Epoch 90, training loss: 1.6800353527069092 = 1.6724414825439453 + 0.001 * 7.593887805938721
Epoch 90, val loss: 1.6980234384536743
Epoch 100, training loss: 1.5977070331573486 = 1.5904605388641357 + 0.001 * 7.246487140655518
Epoch 100, val loss: 1.6282864809036255
Epoch 110, training loss: 1.492203712463379 = 1.4850714206695557 + 0.001 * 7.1323347091674805
Epoch 110, val loss: 1.5408936738967896
Epoch 120, training loss: 1.370379090309143 = 1.363323450088501 + 0.001 * 7.0556206703186035
Epoch 120, val loss: 1.440289855003357
Epoch 130, training loss: 1.240801215171814 = 1.2338039875030518 + 0.001 * 6.997276306152344
Epoch 130, val loss: 1.333650827407837
Epoch 140, training loss: 1.1097971200942993 = 1.1028215885162354 + 0.001 * 6.9755167961120605
Epoch 140, val loss: 1.2265276908874512
Epoch 150, training loss: 0.9839506149291992 = 0.9769878387451172 + 0.001 * 6.962803840637207
Epoch 150, val loss: 1.1255240440368652
Epoch 160, training loss: 0.8690282702445984 = 0.8620733022689819 + 0.001 * 6.954995632171631
Epoch 160, val loss: 1.0351572036743164
Epoch 170, training loss: 0.7685602903366089 = 0.7616097331047058 + 0.001 * 6.950577735900879
Epoch 170, val loss: 0.9582770466804504
Epoch 180, training loss: 0.6831034421920776 = 0.6761564612388611 + 0.001 * 6.946956157684326
Epoch 180, val loss: 0.8957275152206421
Epoch 190, training loss: 0.6108041405677795 = 0.6038618683815002 + 0.001 * 6.942248821258545
Epoch 190, val loss: 0.8462846875190735
Epoch 200, training loss: 0.548797070980072 = 0.5418619513511658 + 0.001 * 6.9351043701171875
Epoch 200, val loss: 0.8076307773590088
Epoch 210, training loss: 0.49433252215385437 = 0.4874073565006256 + 0.001 * 6.925153732299805
Epoch 210, val loss: 0.7772077918052673
Epoch 220, training loss: 0.44511887431144714 = 0.4382038116455078 + 0.001 * 6.915072917938232
Epoch 220, val loss: 0.7526463866233826
Epoch 230, training loss: 0.39965108036994934 = 0.39274948835372925 + 0.001 * 6.9015793800354
Epoch 230, val loss: 0.7324732542037964
Epoch 240, training loss: 0.3572119474411011 = 0.3503214716911316 + 0.001 * 6.8904805183410645
Epoch 240, val loss: 0.7161599397659302
Epoch 250, training loss: 0.31771141290664673 = 0.3108307421207428 + 0.001 * 6.8806586265563965
Epoch 250, val loss: 0.7035253643989563
Epoch 260, training loss: 0.28131547570228577 = 0.27443739771842957 + 0.001 * 6.878077507019043
Epoch 260, val loss: 0.6946307420730591
Epoch 270, training loss: 0.24816912412643433 = 0.24129891395568848 + 0.001 * 6.870217323303223
Epoch 270, val loss: 0.6892991065979004
Epoch 280, training loss: 0.2183658927679062 = 0.2115020751953125 + 0.001 * 6.863814353942871
Epoch 280, val loss: 0.6872320771217346
Epoch 290, training loss: 0.19188207387924194 = 0.18501907587051392 + 0.001 * 6.8629961013793945
Epoch 290, val loss: 0.6883057355880737
Epoch 300, training loss: 0.1686009019613266 = 0.16173996031284332 + 0.001 * 6.860942363739014
Epoch 300, val loss: 0.692343533039093
Epoch 310, training loss: 0.14834678173065186 = 0.14148466289043427 + 0.001 * 6.862112998962402
Epoch 310, val loss: 0.6991539001464844
Epoch 320, training loss: 0.13084377348423004 = 0.12398313730955124 + 0.001 * 6.860631465911865
Epoch 320, val loss: 0.7084721326828003
Epoch 330, training loss: 0.11577709019184113 = 0.10891716927289963 + 0.001 * 6.859919548034668
Epoch 330, val loss: 0.7199590802192688
Epoch 340, training loss: 0.10282027721405029 = 0.09596078097820282 + 0.001 * 6.859494209289551
Epoch 340, val loss: 0.7332100868225098
Epoch 350, training loss: 0.09167102724313736 = 0.08480747789144516 + 0.001 * 6.863551616668701
Epoch 350, val loss: 0.7478241920471191
Epoch 360, training loss: 0.08204100281000137 = 0.07518114149570465 + 0.001 * 6.85985803604126
Epoch 360, val loss: 0.7634018063545227
Epoch 370, training loss: 0.07371573895215988 = 0.06685654073953629 + 0.001 * 6.859199047088623
Epoch 370, val loss: 0.7796013355255127
Epoch 380, training loss: 0.06650357693433762 = 0.05964302271604538 + 0.001 * 6.860555171966553
Epoch 380, val loss: 0.7960577011108398
Epoch 390, training loss: 0.060238972306251526 = 0.05338092893362045 + 0.001 * 6.8580427169799805
Epoch 390, val loss: 0.8124889731407166
Epoch 400, training loss: 0.05479854345321655 = 0.04793539643287659 + 0.001 * 6.863144874572754
Epoch 400, val loss: 0.8287332653999329
Epoch 410, training loss: 0.0500493049621582 = 0.043190598487854004 + 0.001 * 6.858705997467041
Epoch 410, val loss: 0.8446701765060425
Epoch 420, training loss: 0.045904893428087234 = 0.0390491858124733 + 0.001 * 6.855706691741943
Epoch 420, val loss: 0.8602212071418762
Epoch 430, training loss: 0.04229196533560753 = 0.03542530536651611 + 0.001 * 6.8666605949401855
Epoch 430, val loss: 0.8753629922866821
Epoch 440, training loss: 0.039102766662836075 = 0.03224616125226021 + 0.001 * 6.8566060066223145
Epoch 440, val loss: 0.8900848031044006
Epoch 450, training loss: 0.036302387714385986 = 0.02944893203675747 + 0.001 * 6.853455066680908
Epoch 450, val loss: 0.9044017791748047
Epoch 460, training loss: 0.0338330939412117 = 0.026980193331837654 + 0.001 * 6.852901935577393
Epoch 460, val loss: 0.9183363318443298
Epoch 470, training loss: 0.03165988624095917 = 0.02479485236108303 + 0.001 * 6.865034580230713
Epoch 470, val loss: 0.9318708777427673
Epoch 480, training loss: 0.029710909351706505 = 0.022854166105389595 + 0.001 * 6.856742858886719
Epoch 480, val loss: 0.9450310468673706
Epoch 490, training loss: 0.027977436780929565 = 0.02112537808716297 + 0.001 * 6.852058410644531
Epoch 490, val loss: 0.957807183265686
Epoch 500, training loss: 0.026429472491145134 = 0.01958051323890686 + 0.001 * 6.848958492279053
Epoch 500, val loss: 0.9702208638191223
Epoch 510, training loss: 0.0250505693256855 = 0.018195169046521187 + 0.001 * 6.855400085449219
Epoch 510, val loss: 0.9822841882705688
Epoch 520, training loss: 0.023796401917934418 = 0.01694762147963047 + 0.001 * 6.848781108856201
Epoch 520, val loss: 0.9940381050109863
Epoch 530, training loss: 0.022664710879325867 = 0.015819508582353592 + 0.001 * 6.845201015472412
Epoch 530, val loss: 1.0054702758789062
Epoch 540, training loss: 0.02164795994758606 = 0.01479571033269167 + 0.001 * 6.8522491455078125
Epoch 540, val loss: 1.0166558027267456
Epoch 550, training loss: 0.020710723474621773 = 0.013864103704690933 + 0.001 * 6.846619606018066
Epoch 550, val loss: 1.027585744857788
Epoch 560, training loss: 0.019856328144669533 = 0.013013994321227074 + 0.001 * 6.8423333168029785
Epoch 560, val loss: 1.0382682085037231
Epoch 570, training loss: 0.019094986841082573 = 0.012236885726451874 + 0.001 * 6.8581013679504395
Epoch 570, val loss: 1.0487265586853027
Epoch 580, training loss: 0.018368303775787354 = 0.011525866575539112 + 0.001 * 6.842436790466309
Epoch 580, val loss: 1.0589183568954468
Epoch 590, training loss: 0.01771341636776924 = 0.010873985476791859 + 0.001 * 6.839430332183838
Epoch 590, val loss: 1.0688989162445068
Epoch 600, training loss: 0.017130881547927856 = 0.010275404900312424 + 0.001 * 6.855475425720215
Epoch 600, val loss: 1.078653335571289
Epoch 610, training loss: 0.016559844836592674 = 0.009725055657327175 + 0.001 * 6.8347883224487305
Epoch 610, val loss: 1.088135838508606
Epoch 620, training loss: 0.01605379581451416 = 0.009217990562319756 + 0.001 * 6.835805892944336
Epoch 620, val loss: 1.0974185466766357
Epoch 630, training loss: 0.015587720088660717 = 0.008749953471124172 + 0.001 * 6.837766170501709
Epoch 630, val loss: 1.106494665145874
Epoch 640, training loss: 0.015161491930484772 = 0.008317455649375916 + 0.001 * 6.844036102294922
Epoch 640, val loss: 1.115351676940918
Epoch 650, training loss: 0.014749303460121155 = 0.007917139679193497 + 0.001 * 6.832163333892822
Epoch 650, val loss: 1.1239967346191406
Epoch 660, training loss: 0.0143787432461977 = 0.0075459349900484085 + 0.001 * 6.832808017730713
Epoch 660, val loss: 1.1324471235275269
Epoch 670, training loss: 0.014030147343873978 = 0.00720123341307044 + 0.001 * 6.828914165496826
Epoch 670, val loss: 1.140704870223999
Epoch 680, training loss: 0.01370471902191639 = 0.0068806433118879795 + 0.001 * 6.824074745178223
Epoch 680, val loss: 1.1487627029418945
Epoch 690, training loss: 0.013410985469818115 = 0.00658196397125721 + 0.001 * 6.8290205001831055
Epoch 690, val loss: 1.1566481590270996
Epoch 700, training loss: 0.013132676482200623 = 0.006303485482931137 + 0.001 * 6.829190254211426
Epoch 700, val loss: 1.1643673181533813
Epoch 710, training loss: 0.012866602279245853 = 0.006043415516614914 + 0.001 * 6.82318639755249
Epoch 710, val loss: 1.171920895576477
Epoch 720, training loss: 0.012617402710020542 = 0.005800110287964344 + 0.001 * 6.817292213439941
Epoch 720, val loss: 1.1792997121810913
Epoch 730, training loss: 0.012402698397636414 = 0.005572175141423941 + 0.001 * 6.8305230140686035
Epoch 730, val loss: 1.186524510383606
Epoch 740, training loss: 0.012180090881884098 = 0.0053584580309689045 + 0.001 * 6.821632385253906
Epoch 740, val loss: 1.1935575008392334
Epoch 750, training loss: 0.01198074035346508 = 0.005157737992703915 + 0.001 * 6.823001861572266
Epoch 750, val loss: 1.2004632949829102
Epoch 760, training loss: 0.011797968298196793 = 0.004968997556716204 + 0.001 * 6.828970909118652
Epoch 760, val loss: 1.2072081565856934
Epoch 770, training loss: 0.01160626020282507 = 0.0047911484725773335 + 0.001 * 6.8151116371154785
Epoch 770, val loss: 1.2138341665267944
Epoch 780, training loss: 0.011450989171862602 = 0.004623148590326309 + 0.001 * 6.827840805053711
Epoch 780, val loss: 1.2203402519226074
Epoch 790, training loss: 0.011278380639851093 = 0.00446367496624589 + 0.001 * 6.8147053718566895
Epoch 790, val loss: 1.226760745048523
Epoch 800, training loss: 0.011116734705865383 = 0.004311209078878164 + 0.001 * 6.805525302886963
Epoch 800, val loss: 1.2331432104110718
Epoch 810, training loss: 0.010980016551911831 = 0.004164814483374357 + 0.001 * 6.815201759338379
Epoch 810, val loss: 1.239526391029358
Epoch 820, training loss: 0.01081913523375988 = 0.0040240599773824215 + 0.001 * 6.795075416564941
Epoch 820, val loss: 1.2458703517913818
Epoch 830, training loss: 0.01068994589149952 = 0.0038887131959199905 + 0.001 * 6.801232814788818
Epoch 830, val loss: 1.252192497253418
Epoch 840, training loss: 0.01055813580751419 = 0.003758827457204461 + 0.001 * 6.7993083000183105
Epoch 840, val loss: 1.2584682703018188
Epoch 850, training loss: 0.010427901521325111 = 0.003634381340816617 + 0.001 * 6.793519973754883
Epoch 850, val loss: 1.2647038698196411
Epoch 860, training loss: 0.010308216325938702 = 0.003515467746183276 + 0.001 * 6.79274845123291
Epoch 860, val loss: 1.270837664604187
Epoch 870, training loss: 0.010208376683294773 = 0.003402039408683777 + 0.001 * 6.806336879730225
Epoch 870, val loss: 1.276893973350525
Epoch 880, training loss: 0.010081004351377487 = 0.003293803194537759 + 0.001 * 6.787200927734375
Epoch 880, val loss: 1.2828755378723145
Epoch 890, training loss: 0.00999488402158022 = 0.0031906741205602884 + 0.001 * 6.8042097091674805
Epoch 890, val loss: 1.288784146308899
Epoch 900, training loss: 0.009889170527458191 = 0.0030924510210752487 + 0.001 * 6.796719551086426
Epoch 900, val loss: 1.2945610284805298
Epoch 910, training loss: 0.009779715910553932 = 0.0029989266768097878 + 0.001 * 6.780788898468018
Epoch 910, val loss: 1.3002599477767944
Epoch 920, training loss: 0.0096925999969244 = 0.002909846603870392 + 0.001 * 6.782752513885498
Epoch 920, val loss: 1.3058651685714722
Epoch 930, training loss: 0.009607205167412758 = 0.002825012896209955 + 0.001 * 6.782191753387451
Epoch 930, val loss: 1.3113627433776855
Epoch 940, training loss: 0.009528805501759052 = 0.002744255820289254 + 0.001 * 6.784549713134766
Epoch 940, val loss: 1.3167846202850342
Epoch 950, training loss: 0.009469564072787762 = 0.0026672633830457926 + 0.001 * 6.802299976348877
Epoch 950, val loss: 1.3221043348312378
Epoch 960, training loss: 0.009383137337863445 = 0.0025939627084881067 + 0.001 * 6.789174556732178
Epoch 960, val loss: 1.3273347616195679
Epoch 970, training loss: 0.009298295713961124 = 0.002524047391489148 + 0.001 * 6.774248123168945
Epoch 970, val loss: 1.332446813583374
Epoch 980, training loss: 0.009233402088284492 = 0.0024573758710175753 + 0.001 * 6.776025772094727
Epoch 980, val loss: 1.3374890089035034
Epoch 990, training loss: 0.009166046045720577 = 0.0023937516380101442 + 0.001 * 6.772294521331787
Epoch 990, val loss: 1.342414140701294
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.81058, 0.12957, Accuracy:0.81481, 0.01889
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9532])
updated graph: torch.Size([2, 10588])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.82840, 0.00462
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9574611186981201 = 1.9490872621536255 + 0.001 * 8.373866081237793
Epoch 0, val loss: 1.9458305835723877
Epoch 10, training loss: 1.9475514888763428 = 1.9391776323318481 + 0.001 * 8.3738431930542
Epoch 10, val loss: 1.9360381364822388
Epoch 20, training loss: 1.9357553720474243 = 1.9273816347122192 + 0.001 * 8.37369155883789
Epoch 20, val loss: 1.9242826700210571
Epoch 30, training loss: 1.9193166494369507 = 1.9109432697296143 + 0.001 * 8.37336254119873
Epoch 30, val loss: 1.9080702066421509
Epoch 40, training loss: 1.8948408365249634 = 1.8864681720733643 + 0.001 * 8.372653007507324
Epoch 40, val loss: 1.8842835426330566
Epoch 50, training loss: 1.8591910600662231 = 1.8508201837539673 + 0.001 * 8.370901107788086
Epoch 50, val loss: 1.8510992527008057
Epoch 60, training loss: 1.814543604850769 = 1.806178092956543 + 0.001 * 8.36548137664795
Epoch 60, val loss: 1.8133058547973633
Epoch 70, training loss: 1.7714697122573853 = 1.7631256580352783 + 0.001 * 8.343998908996582
Epoch 70, val loss: 1.7807496786117554
Epoch 80, training loss: 1.7231594324111938 = 1.7149502038955688 + 0.001 * 8.209192276000977
Epoch 80, val loss: 1.740281343460083
Epoch 90, training loss: 1.6555017232894897 = 1.6477059125900269 + 0.001 * 7.795866012573242
Epoch 90, val loss: 1.6820402145385742
Epoch 100, training loss: 1.566175937652588 = 1.5585931539535522 + 0.001 * 7.582798957824707
Epoch 100, val loss: 1.6079988479614258
Epoch 110, training loss: 1.456592321395874 = 1.4491339921951294 + 0.001 * 7.4583611488342285
Epoch 110, val loss: 1.519370198249817
Epoch 120, training loss: 1.3380873203277588 = 1.3306430578231812 + 0.001 * 7.444291114807129
Epoch 120, val loss: 1.423270583152771
Epoch 130, training loss: 1.2195181846618652 = 1.2120983600616455 + 0.001 * 7.4198317527771
Epoch 130, val loss: 1.3280783891677856
Epoch 140, training loss: 1.1055090427398682 = 1.098118543624878 + 0.001 * 7.3904852867126465
Epoch 140, val loss: 1.2369234561920166
Epoch 150, training loss: 0.9987619519233704 = 0.9914234280586243 + 0.001 * 7.338537693023682
Epoch 150, val loss: 1.1521739959716797
Epoch 160, training loss: 0.9016473889350891 = 0.8944006562232971 + 0.001 * 7.246757984161377
Epoch 160, val loss: 1.076630711555481
Epoch 170, training loss: 0.8158690929412842 = 0.8086876273155212 + 0.001 * 7.18146276473999
Epoch 170, val loss: 1.0115034580230713
Epoch 180, training loss: 0.741651177406311 = 0.7344885468482971 + 0.001 * 7.162608623504639
Epoch 180, val loss: 0.9568342566490173
Epoch 190, training loss: 0.6775712370872498 = 0.6704354286193848 + 0.001 * 7.135811805725098
Epoch 190, val loss: 0.9123464226722717
Epoch 200, training loss: 0.621225118637085 = 0.6141103506088257 + 0.001 * 7.114743709564209
Epoch 200, val loss: 0.8766835331916809
Epoch 210, training loss: 0.5703015923500061 = 0.5632059574127197 + 0.001 * 7.095623970031738
Epoch 210, val loss: 0.8481776714324951
Epoch 220, training loss: 0.5233334302902222 = 0.5162470936775208 + 0.001 * 7.0863237380981445
Epoch 220, val loss: 0.8253178596496582
Epoch 230, training loss: 0.47921064496040344 = 0.4721340835094452 + 0.001 * 7.076557636260986
Epoch 230, val loss: 0.8066704273223877
Epoch 240, training loss: 0.4368370473384857 = 0.4297652542591095 + 0.001 * 7.071803569793701
Epoch 240, val loss: 0.7912647724151611
Epoch 250, training loss: 0.3954451382160187 = 0.38837680220603943 + 0.001 * 7.068324089050293
Epoch 250, val loss: 0.7785439491271973
Epoch 260, training loss: 0.35500362515449524 = 0.3479387164115906 + 0.001 * 7.064917087554932
Epoch 260, val loss: 0.7684383988380432
Epoch 270, training loss: 0.31614553928375244 = 0.3090837895870209 + 0.001 * 7.061743259429932
Epoch 270, val loss: 0.7610639333724976
Epoch 280, training loss: 0.27957379817962646 = 0.272512823343277 + 0.001 * 7.060976505279541
Epoch 280, val loss: 0.7567983269691467
Epoch 290, training loss: 0.24568374454975128 = 0.23862655460834503 + 0.001 * 7.057183742523193
Epoch 290, val loss: 0.7557137608528137
Epoch 300, training loss: 0.21473516523838043 = 0.2076817750930786 + 0.001 * 7.0533833503723145
Epoch 300, val loss: 0.757627546787262
Epoch 310, training loss: 0.18701237440109253 = 0.1799618899822235 + 0.001 * 7.0504865646362305
Epoch 310, val loss: 0.7625063061714172
Epoch 320, training loss: 0.16271676123142242 = 0.15566857159137726 + 0.001 * 7.048184871673584
Epoch 320, val loss: 0.7700656652450562
Epoch 330, training loss: 0.14184021949768066 = 0.13479526340961456 + 0.001 * 7.044960021972656
Epoch 330, val loss: 0.7800112962722778
Epoch 340, training loss: 0.12410912662744522 = 0.11706635355949402 + 0.001 * 7.042769432067871
Epoch 340, val loss: 0.7920147776603699
Epoch 350, training loss: 0.10910913348197937 = 0.10206679254770279 + 0.001 * 7.0423431396484375
Epoch 350, val loss: 0.8057078123092651
Epoch 360, training loss: 0.09640742838382721 = 0.08937089890241623 + 0.001 * 7.036527156829834
Epoch 360, val loss: 0.8207935690879822
Epoch 370, training loss: 0.08562052249908447 = 0.07858717441558838 + 0.001 * 7.033349514007568
Epoch 370, val loss: 0.8369296789169312
Epoch 380, training loss: 0.07641983032226562 = 0.06938963383436203 + 0.001 * 7.030196189880371
Epoch 380, val loss: 0.8537968993186951
Epoch 390, training loss: 0.06853712350130081 = 0.06151321530342102 + 0.001 * 7.023905277252197
Epoch 390, val loss: 0.8711119294166565
Epoch 400, training loss: 0.061760708689689636 = 0.05474073812365532 + 0.001 * 7.01997184753418
Epoch 400, val loss: 0.8886098265647888
Epoch 410, training loss: 0.055909376591444016 = 0.048893675208091736 + 0.001 * 7.015701770782471
Epoch 410, val loss: 0.9061042070388794
Epoch 420, training loss: 0.05083726346492767 = 0.043827589601278305 + 0.001 * 7.009674549102783
Epoch 420, val loss: 0.9232928156852722
Epoch 430, training loss: 0.04642719775438309 = 0.03942163288593292 + 0.001 * 7.005563735961914
Epoch 430, val loss: 0.9402094483375549
Epoch 440, training loss: 0.042581744492053986 = 0.03557691350579262 + 0.001 * 7.004830837249756
Epoch 440, val loss: 0.956673264503479
Epoch 450, training loss: 0.03921359404921532 = 0.032212767750024796 + 0.001 * 7.00082540512085
Epoch 450, val loss: 0.9727882742881775
Epoch 460, training loss: 0.03626013174653053 = 0.029259510338306427 + 0.001 * 7.000622272491455
Epoch 460, val loss: 0.9884026646614075
Epoch 470, training loss: 0.03365922346711159 = 0.026661187410354614 + 0.001 * 6.998037338256836
Epoch 470, val loss: 1.0034435987472534
Epoch 480, training loss: 0.03136294335126877 = 0.024370145052671432 + 0.001 * 6.992797374725342
Epoch 480, val loss: 1.018025279045105
Epoch 490, training loss: 0.029338382184505463 = 0.022344714030623436 + 0.001 * 6.9936676025390625
Epoch 490, val loss: 1.0321077108383179
Epoch 500, training loss: 0.027536218985915184 = 0.02054976113140583 + 0.001 * 6.986457824707031
Epoch 500, val loss: 1.0456852912902832
Epoch 510, training loss: 0.0259503573179245 = 0.018954705446958542 + 0.001 * 6.995652198791504
Epoch 510, val loss: 1.0587962865829468
Epoch 520, training loss: 0.024518093094229698 = 0.01753341220319271 + 0.001 * 6.98468017578125
Epoch 520, val loss: 1.0714844465255737
Epoch 530, training loss: 0.023243548348546028 = 0.016263294965028763 + 0.001 * 6.980253219604492
Epoch 530, val loss: 1.0837349891662598
Epoch 540, training loss: 0.022104470059275627 = 0.01512497290968895 + 0.001 * 6.979496955871582
Epoch 540, val loss: 1.0955487489700317
Epoch 550, training loss: 0.021078722551465034 = 0.01410183310508728 + 0.001 * 6.976889133453369
Epoch 550, val loss: 1.106972575187683
Epoch 560, training loss: 0.02016127109527588 = 0.013179689645767212 + 0.001 * 6.981581687927246
Epoch 560, val loss: 1.1180109977722168
Epoch 570, training loss: 0.019322533160448074 = 0.01234601903706789 + 0.001 * 6.976512908935547
Epoch 570, val loss: 1.1286901235580444
Epoch 580, training loss: 0.01856139674782753 = 0.011590392328798771 + 0.001 * 6.971004962921143
Epoch 580, val loss: 1.138978123664856
Epoch 590, training loss: 0.01787026785314083 = 0.010903662070631981 + 0.001 * 6.9666056632995605
Epoch 590, val loss: 1.1489582061767578
Epoch 600, training loss: 0.017240792512893677 = 0.010277909226715565 + 0.001 * 6.962882995605469
Epoch 600, val loss: 1.1585959196090698
Epoch 610, training loss: 0.016688115894794464 = 0.009706308133900166 + 0.001 * 6.98180627822876
Epoch 610, val loss: 1.1679348945617676
Epoch 620, training loss: 0.016140608116984367 = 0.009182911366224289 + 0.001 * 6.957696914672852
Epoch 620, val loss: 1.1769850254058838
Epoch 630, training loss: 0.01565798930823803 = 0.008702686056494713 + 0.001 * 6.955303192138672
Epoch 630, val loss: 1.1857658624649048
Epoch 640, training loss: 0.01521635614335537 = 0.008260990492999554 + 0.001 * 6.955365180969238
Epoch 640, val loss: 1.1942986249923706
Epoch 650, training loss: 0.014824042096734047 = 0.007853860966861248 + 0.001 * 6.970180988311768
Epoch 650, val loss: 1.2025641202926636
Epoch 660, training loss: 0.014433665201067924 = 0.007477905135601759 + 0.001 * 6.9557600021362305
Epoch 660, val loss: 1.2106096744537354
Epoch 670, training loss: 0.014081287197768688 = 0.007130017038434744 + 0.001 * 6.951269626617432
Epoch 670, val loss: 1.2184219360351562
Epoch 680, training loss: 0.013749630190432072 = 0.006807450205087662 + 0.001 * 6.9421796798706055
Epoch 680, val loss: 1.2260394096374512
Epoch 690, training loss: 0.01347423903644085 = 0.006507839076220989 + 0.001 * 6.966400146484375
Epoch 690, val loss: 1.2334375381469727
Epoch 700, training loss: 0.013166412711143494 = 0.00622907979413867 + 0.001 * 6.9373321533203125
Epoch 700, val loss: 1.24064040184021
Epoch 710, training loss: 0.01290431059896946 = 0.005969276186078787 + 0.001 * 6.935034275054932
Epoch 710, val loss: 1.24765145778656
Epoch 720, training loss: 0.012675819918513298 = 0.0057267253287136555 + 0.001 * 6.949094295501709
Epoch 720, val loss: 1.254454255104065
Epoch 730, training loss: 0.012444552034139633 = 0.005499958526343107 + 0.001 * 6.9445929527282715
Epoch 730, val loss: 1.261078953742981
Epoch 740, training loss: 0.012227499857544899 = 0.0052876858972013 + 0.001 * 6.939813137054443
Epoch 740, val loss: 1.2675572633743286
Epoch 750, training loss: 0.012018656358122826 = 0.0050886813551187515 + 0.001 * 6.92997407913208
Epoch 750, val loss: 1.2738527059555054
Epoch 760, training loss: 0.011842193081974983 = 0.004901837091892958 + 0.001 * 6.940356254577637
Epoch 760, val loss: 1.2799832820892334
Epoch 770, training loss: 0.011681536212563515 = 0.004726189188659191 + 0.001 * 6.955346584320068
Epoch 770, val loss: 1.2859712839126587
Epoch 780, training loss: 0.011481445282697678 = 0.004560880362987518 + 0.001 * 6.920564651489258
Epoch 780, val loss: 1.2917969226837158
Epoch 790, training loss: 0.011332633905112743 = 0.004405095241963863 + 0.001 * 6.9275383949279785
Epoch 790, val loss: 1.2974754571914673
Epoch 800, training loss: 0.011178357526659966 = 0.004258084576576948 + 0.001 * 6.920273303985596
Epoch 800, val loss: 1.303054928779602
Epoch 810, training loss: 0.011038623750209808 = 0.004119225777685642 + 0.001 * 6.919397354125977
Epoch 810, val loss: 1.3084760904312134
Epoch 820, training loss: 0.01089163776487112 = 0.003987920004874468 + 0.001 * 6.903717517852783
Epoch 820, val loss: 1.313765525817871
Epoch 830, training loss: 0.01082143560051918 = 0.0038636433891952038 + 0.001 * 6.957791805267334
Epoch 830, val loss: 1.3189457654953003
Epoch 840, training loss: 0.010645357891917229 = 0.003745893482118845 + 0.001 * 6.8994646072387695
Epoch 840, val loss: 1.3240020275115967
Epoch 850, training loss: 0.010531987063586712 = 0.0036342311650514603 + 0.001 * 6.8977556228637695
Epoch 850, val loss: 1.3289436101913452
Epoch 860, training loss: 0.010464362800121307 = 0.0035282161552459 + 0.001 * 6.9361467361450195
Epoch 860, val loss: 1.3337856531143188
Epoch 870, training loss: 0.010317189618945122 = 0.0034274947829544544 + 0.001 * 6.8896942138671875
Epoch 870, val loss: 1.3385194540023804
Epoch 880, training loss: 0.010250000283122063 = 0.0033317029010504484 + 0.001 * 6.918296813964844
Epoch 880, val loss: 1.3431342840194702
Epoch 890, training loss: 0.010135580785572529 = 0.003240537364035845 + 0.001 * 6.895042896270752
Epoch 890, val loss: 1.3476792573928833
Epoch 900, training loss: 0.01004837080836296 = 0.0031536943279206753 + 0.001 * 6.894676208496094
Epoch 900, val loss: 1.3520984649658203
Epoch 910, training loss: 0.009975322522222996 = 0.0030709076672792435 + 0.001 * 6.904414653778076
Epoch 910, val loss: 1.356421947479248
Epoch 920, training loss: 0.009871568530797958 = 0.0029919093940407038 + 0.001 * 6.8796586990356445
Epoch 920, val loss: 1.360673189163208
Epoch 930, training loss: 0.009797473438084126 = 0.002916475525125861 + 0.001 * 6.880997657775879
Epoch 930, val loss: 1.3648035526275635
Epoch 940, training loss: 0.009747988544404507 = 0.002844427013769746 + 0.001 * 6.903561592102051
Epoch 940, val loss: 1.3688997030258179
Epoch 950, training loss: 0.00965351052582264 = 0.0027755270712077618 + 0.001 * 6.877983570098877
Epoch 950, val loss: 1.372864842414856
Epoch 960, training loss: 0.009578313678503036 = 0.002709598047658801 + 0.001 * 6.868715286254883
Epoch 960, val loss: 1.3767763376235962
Epoch 970, training loss: 0.00952642410993576 = 0.0026464734692126513 + 0.001 * 6.879950523376465
Epoch 970, val loss: 1.3805848360061646
Epoch 980, training loss: 0.009469165466725826 = 0.002585994079709053 + 0.001 * 6.883171081542969
Epoch 980, val loss: 1.3843541145324707
Epoch 990, training loss: 0.009426903910934925 = 0.0025280152913182974 + 0.001 * 6.898888111114502
Epoch 990, val loss: 1.3880218267440796
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
Traceback (most recent call last):
  File "./run_pre_bkd.py", line 730, in <module>
    torch.cuda.empty_cache()
  File "/home/mfl5681/anaconda3/envs/py38_torch120/lib/python3.8/site-packages/torch/cuda/memory.py", line 121, in empty_cache
    torch._C._cuda_emptyCache()
RuntimeError: CUDA error: out of memory
CUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.
For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9627474546432495 = 1.9543735980987549 + 0.001 * 8.373885154724121
Epoch 0, val loss: 1.9548602104187012
Epoch 10, training loss: 1.9525072574615479 = 1.9441334009170532 + 0.001 * 8.37385368347168
Epoch 10, val loss: 1.9441139698028564
Epoch 20, training loss: 1.9405593872070312 = 1.9321856498718262 + 0.001 * 8.37369441986084
Epoch 20, val loss: 1.9313011169433594
Epoch 30, training loss: 1.9245399236679077 = 1.9161665439605713 + 0.001 * 8.373370170593262
Epoch 30, val loss: 1.9139853715896606
Epoch 40, training loss: 1.9015759229660034 = 1.8932032585144043 + 0.001 * 8.372689247131348
Epoch 40, val loss: 1.8892837762832642
Epoch 50, training loss: 1.8687849044799805 = 1.8604137897491455 + 0.001 * 8.37112808227539
Epoch 50, val loss: 1.8551397323608398
Epoch 60, training loss: 1.8268661499023438 = 1.8184995651245117 + 0.001 * 8.3666353225708
Epoch 60, val loss: 1.8151522874832153
Epoch 70, training loss: 1.7847965955734253 = 1.7764477729797363 + 0.001 * 8.348875045776367
Epoch 70, val loss: 1.7811933755874634
Epoch 80, training loss: 1.7411537170410156 = 1.732927918434143 + 0.001 * 8.225844383239746
Epoch 80, val loss: 1.7480639219284058
Epoch 90, training loss: 1.68027925491333 = 1.6724904775619507 + 0.001 * 7.788814544677734
Epoch 90, val loss: 1.698199987411499
Epoch 100, training loss: 1.5982624292373657 = 1.5907527208328247 + 0.001 * 7.5096755027771
Epoch 100, val loss: 1.6291708946228027
Epoch 110, training loss: 1.4942103624343872 = 1.486849308013916 + 0.001 * 7.361043453216553
Epoch 110, val loss: 1.5425591468811035
Epoch 120, training loss: 1.3761762380599976 = 1.3688608407974243 + 0.001 * 7.315418243408203
Epoch 120, val loss: 1.4462286233901978
Epoch 130, training loss: 1.2513012886047363 = 1.2440251111984253 + 0.001 * 7.276135444641113
Epoch 130, val loss: 1.345720648765564
Epoch 140, training loss: 1.1252950429916382 = 1.118075966835022 + 0.001 * 7.219061851501465
Epoch 140, val loss: 1.244987964630127
Epoch 150, training loss: 1.0055663585662842 = 0.9983999133110046 + 0.001 * 7.166503429412842
Epoch 150, val loss: 1.1510194540023804
Epoch 160, training loss: 0.8998028039932251 = 0.8926648497581482 + 0.001 * 7.137937068939209
Epoch 160, val loss: 1.0704197883605957
Epoch 170, training loss: 0.8116694688796997 = 0.8045568466186523 + 0.001 * 7.112597942352295
Epoch 170, val loss: 1.0060782432556152
Epoch 180, training loss: 0.7398861646652222 = 0.732807457447052 + 0.001 * 7.078693389892578
Epoch 180, val loss: 0.9563748836517334
Epoch 190, training loss: 0.6802923083305359 = 0.6732525825500488 + 0.001 * 7.039753437042236
Epoch 190, val loss: 0.9179608821868896
Epoch 200, training loss: 0.628237783908844 = 0.6212230324745178 + 0.001 * 7.014752388000488
Epoch 200, val loss: 0.8870733380317688
Epoch 210, training loss: 0.5800401568412781 = 0.5730392932891846 + 0.001 * 7.0008416175842285
Epoch 210, val loss: 0.8607444763183594
Epoch 220, training loss: 0.5334113836288452 = 0.5264132618904114 + 0.001 * 6.998093128204346
Epoch 220, val loss: 0.8374042510986328
Epoch 230, training loss: 0.48707064986228943 = 0.48007503151893616 + 0.001 * 6.9956159591674805
Epoch 230, val loss: 0.8157156109809875
Epoch 240, training loss: 0.4406660795211792 = 0.4336722493171692 + 0.001 * 6.993821144104004
Epoch 240, val loss: 0.7951133847236633
Epoch 250, training loss: 0.3948403298854828 = 0.3878496289253235 + 0.001 * 6.990693092346191
Epoch 250, val loss: 0.7763766050338745
Epoch 260, training loss: 0.3505280613899231 = 0.3435410261154175 + 0.001 * 6.987040996551514
Epoch 260, val loss: 0.7603976726531982
Epoch 270, training loss: 0.3084736764431 = 0.30149075388908386 + 0.001 * 6.982926368713379
Epoch 270, val loss: 0.747529149055481
Epoch 280, training loss: 0.2693309187889099 = 0.26235252618789673 + 0.001 * 6.978400230407715
Epoch 280, val loss: 0.7378795742988586
Epoch 290, training loss: 0.23398961126804352 = 0.2270130068063736 + 0.001 * 6.9766106605529785
Epoch 290, val loss: 0.7316083908081055
Epoch 300, training loss: 0.20324526727199554 = 0.1962755024433136 + 0.001 * 6.969760417938232
Epoch 300, val loss: 0.7290452122688293
Epoch 310, training loss: 0.17727036774158478 = 0.17030632495880127 + 0.001 * 6.964048385620117
Epoch 310, val loss: 0.730262041091919
Epoch 320, training loss: 0.1555871218442917 = 0.1486286073923111 + 0.001 * 6.958518981933594
Epoch 320, val loss: 0.735038697719574
Epoch 330, training loss: 0.13746388256549835 = 0.13051123917102814 + 0.001 * 6.952643394470215
Epoch 330, val loss: 0.7428291440010071
Epoch 340, training loss: 0.12218569964170456 = 0.11523739248514175 + 0.001 * 6.948306083679199
Epoch 340, val loss: 0.7530301213264465
Epoch 350, training loss: 0.10917408764362335 = 0.10222838073968887 + 0.001 * 6.945706367492676
Epoch 350, val loss: 0.7651577591896057
Epoch 360, training loss: 0.09798033535480499 = 0.09104049950838089 + 0.001 * 6.939831733703613
Epoch 360, val loss: 0.7787420153617859
Epoch 370, training loss: 0.08828049153089523 = 0.08133937418460846 + 0.001 * 6.941117286682129
Epoch 370, val loss: 0.7934147715568542
Epoch 380, training loss: 0.07980486750602722 = 0.07287265360355377 + 0.001 * 6.932211875915527
Epoch 380, val loss: 0.8087484836578369
Epoch 390, training loss: 0.07237851619720459 = 0.06544981896877289 + 0.001 * 6.9286980628967285
Epoch 390, val loss: 0.8245420455932617
Epoch 400, training loss: 0.06584765762090683 = 0.05892101675271988 + 0.001 * 6.926642894744873
Epoch 400, val loss: 0.8406127095222473
Epoch 410, training loss: 0.060093171894550323 = 0.053167641162872314 + 0.001 * 6.925528526306152
Epoch 410, val loss: 0.8567226529121399
Epoch 420, training loss: 0.055013854056596756 = 0.048089783638715744 + 0.001 * 6.924070835113525
Epoch 420, val loss: 0.872738242149353
Epoch 430, training loss: 0.05053161084651947 = 0.04360290244221687 + 0.001 * 6.928707122802734
Epoch 430, val loss: 0.8885416388511658
Epoch 440, training loss: 0.046559326350688934 = 0.039635468274354935 + 0.001 * 6.92385721206665
Epoch 440, val loss: 0.9040451645851135
Epoch 450, training loss: 0.043044909834861755 = 0.036122895777225494 + 0.001 * 6.922015190124512
Epoch 450, val loss: 0.9191411733627319
Epoch 460, training loss: 0.03992786258459091 = 0.03300902992486954 + 0.001 * 6.918830394744873
Epoch 460, val loss: 0.9338074326515198
Epoch 470, training loss: 0.03715987876057625 = 0.030243737623095512 + 0.001 * 6.916139602661133
Epoch 470, val loss: 0.947993278503418
Epoch 480, training loss: 0.03469986096024513 = 0.027783432975411415 + 0.001 * 6.9164276123046875
Epoch 480, val loss: 0.961705207824707
Epoch 490, training loss: 0.03250551223754883 = 0.025589652359485626 + 0.001 * 6.915859699249268
Epoch 490, val loss: 0.9749575853347778
Epoch 500, training loss: 0.03053825907409191 = 0.023629503324627876 + 0.001 * 6.908755302429199
Epoch 500, val loss: 0.9877673983573914
Epoch 510, training loss: 0.028779104351997375 = 0.021874375641345978 + 0.001 * 6.904727458953857
Epoch 510, val loss: 1.0001482963562012
Epoch 520, training loss: 0.02721422351896763 = 0.02029871568083763 + 0.001 * 6.9155073165893555
Epoch 520, val loss: 1.0121058225631714
Epoch 530, training loss: 0.025778038427233696 = 0.01888088323175907 + 0.001 * 6.897155284881592
Epoch 530, val loss: 1.0236402750015259
Epoch 540, training loss: 0.024497387930750847 = 0.01760202646255493 + 0.001 * 6.895360946655273
Epoch 540, val loss: 1.0347943305969238
Epoch 550, training loss: 0.02335556596517563 = 0.016445834189653397 + 0.001 * 6.909731864929199
Epoch 550, val loss: 1.0455894470214844
Epoch 560, training loss: 0.022289050742983818 = 0.01539806928485632 + 0.001 * 6.890981674194336
Epoch 560, val loss: 1.0560407638549805
Epoch 570, training loss: 0.02133331447839737 = 0.014446190558373928 + 0.001 * 6.887123107910156
Epoch 570, val loss: 1.0661675930023193
Epoch 580, training loss: 0.020516689866781235 = 0.013579473830759525 + 0.001 * 6.937216758728027
Epoch 580, val loss: 1.0759739875793457
Epoch 590, training loss: 0.019668055698275566 = 0.012788621708750725 + 0.001 * 6.879433631896973
Epoch 590, val loss: 1.0854688882827759
Epoch 600, training loss: 0.018948592245578766 = 0.012065456248819828 + 0.001 * 6.8831353187561035
Epoch 600, val loss: 1.0946813821792603
Epoch 610, training loss: 0.01827394589781761 = 0.011402631178498268 + 0.001 * 6.8713154792785645
Epoch 610, val loss: 1.1036241054534912
Epoch 620, training loss: 0.01767890714108944 = 0.010793783701956272 + 0.001 * 6.8851237297058105
Epoch 620, val loss: 1.1122938394546509
Epoch 630, training loss: 0.01710028201341629 = 0.010233452543616295 + 0.001 * 6.8668293952941895
Epoch 630, val loss: 1.1207047700881958
Epoch 640, training loss: 0.016591966152191162 = 0.009716781787574291 + 0.001 * 6.875183582305908
Epoch 640, val loss: 1.1288763284683228
Epoch 650, training loss: 0.016098687425255775 = 0.00923942681401968 + 0.001 * 6.859260559082031
Epoch 650, val loss: 1.1368205547332764
Epoch 660, training loss: 0.015731394290924072 = 0.008797582238912582 + 0.001 * 6.933810710906982
Epoch 660, val loss: 1.1445462703704834
Epoch 670, training loss: 0.015256127342581749 = 0.00838793721050024 + 0.001 * 6.868190288543701
Epoch 670, val loss: 1.152051329612732
Epoch 680, training loss: 0.014874337241053581 = 0.008007491007447243 + 0.001 * 6.866846084594727
Epoch 680, val loss: 1.159348964691162
Epoch 690, training loss: 0.014513345435261726 = 0.007653531618416309 + 0.001 * 6.859814167022705
Epoch 690, val loss: 1.1664470434188843
Epoch 700, training loss: 0.014175618067383766 = 0.007323711644858122 + 0.001 * 6.851905822753906
Epoch 700, val loss: 1.1733546257019043
Epoch 710, training loss: 0.013887012377381325 = 0.007015946786850691 + 0.001 * 6.871065616607666
Epoch 710, val loss: 1.1800822019577026
Epoch 720, training loss: 0.013572994619607925 = 0.006728346459567547 + 0.001 * 6.844647407531738
Epoch 720, val loss: 1.1866270303726196
Epoch 730, training loss: 0.013311033137142658 = 0.00645917933434248 + 0.001 * 6.851853370666504
Epoch 730, val loss: 1.193004846572876
Epoch 740, training loss: 0.013051142916083336 = 0.006206929218024015 + 0.001 * 6.844213962554932
Epoch 740, val loss: 1.1992119550704956
Epoch 750, training loss: 0.012815579771995544 = 0.0059702410362660885 + 0.001 * 6.845338821411133
Epoch 750, val loss: 1.2052661180496216
Epoch 760, training loss: 0.012603651732206345 = 0.005747882183641195 + 0.001 * 6.85576868057251
Epoch 760, val loss: 1.2111601829528809
Epoch 770, training loss: 0.01238565519452095 = 0.0055387020111083984 + 0.001 * 6.846952438354492
Epoch 770, val loss: 1.2169214487075806
Epoch 780, training loss: 0.012174269184470177 = 0.0053416951559484005 + 0.001 * 6.832574367523193
Epoch 780, val loss: 1.2225347757339478
Epoch 790, training loss: 0.011999411508440971 = 0.005155926104635 + 0.001 * 6.843485355377197
Epoch 790, val loss: 1.228026270866394
Epoch 800, training loss: 0.011807705275714397 = 0.004980588797479868 + 0.001 * 6.827116012573242
Epoch 800, val loss: 1.2333741188049316
Epoch 810, training loss: 0.011646049097180367 = 0.004814895335584879 + 0.001 * 6.831153392791748
Epoch 810, val loss: 1.2385990619659424
Epoch 820, training loss: 0.011489740572869778 = 0.004658170510083437 + 0.001 * 6.831569671630859
Epoch 820, val loss: 1.2437041997909546
Epoch 830, training loss: 0.011345533654093742 = 0.004509798251092434 + 0.001 * 6.835734844207764
Epoch 830, val loss: 1.248680591583252
Epoch 840, training loss: 0.011193670332431793 = 0.0043691652826964855 + 0.001 * 6.82450532913208
Epoch 840, val loss: 1.2535523176193237
Epoch 850, training loss: 0.011089349165558815 = 0.004235772415995598 + 0.001 * 6.85357666015625
Epoch 850, val loss: 1.2583160400390625
Epoch 860, training loss: 0.010938162915408611 = 0.004109123721718788 + 0.001 * 6.829039096832275
Epoch 860, val loss: 1.2629642486572266
Epoch 870, training loss: 0.010817191563546658 = 0.003988771699368954 + 0.001 * 6.8284196853637695
Epoch 870, val loss: 1.2675127983093262
Epoch 880, training loss: 0.010699836537241936 = 0.0038743019104003906 + 0.001 * 6.825533866882324
Epoch 880, val loss: 1.2719674110412598
Epoch 890, training loss: 0.010575397871434689 = 0.003765353700146079 + 0.001 * 6.810043811798096
Epoch 890, val loss: 1.2763198614120483
Epoch 900, training loss: 0.01047765277326107 = 0.0036615622229874134 + 0.001 * 6.816090106964111
Epoch 900, val loss: 1.2805675268173218
Epoch 910, training loss: 0.010374622419476509 = 0.0035626303870230913 + 0.001 * 6.8119916915893555
Epoch 910, val loss: 1.2847346067428589
Epoch 920, training loss: 0.010311054065823555 = 0.003468236653134227 + 0.001 * 6.8428168296813965
Epoch 920, val loss: 1.288807988166809
Epoch 930, training loss: 0.01020270399749279 = 0.0033781344536691904 + 0.001 * 6.824569225311279
Epoch 930, val loss: 1.292800784111023
Epoch 940, training loss: 0.010114196687936783 = 0.0032920576632022858 + 0.001 * 6.822138786315918
Epoch 940, val loss: 1.2966974973678589
Epoch 950, training loss: 0.01003231480717659 = 0.0032097534276545048 + 0.001 * 6.822561740875244
Epoch 950, val loss: 1.3005330562591553
Epoch 960, training loss: 0.00993107259273529 = 0.0031310231424868107 + 0.001 * 6.800048828125
Epoch 960, val loss: 1.3042775392532349
Epoch 970, training loss: 0.009862061589956284 = 0.0030556467827409506 + 0.001 * 6.806414604187012
Epoch 970, val loss: 1.3079588413238525
Epoch 980, training loss: 0.00979234091937542 = 0.0029834515880793333 + 0.001 * 6.808889389038086
Epoch 980, val loss: 1.3115562200546265
Epoch 990, training loss: 0.009713872335851192 = 0.0029142480343580246 + 0.001 * 6.799623966217041
Epoch 990, val loss: 1.3150869607925415
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.4686
Flip ASR: 0.3644/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9296135902404785 = 1.9212398529052734 + 0.001 * 8.373777389526367
Epoch 0, val loss: 1.9236773252487183
Epoch 10, training loss: 1.9204825162887573 = 1.9121088981628418 + 0.001 * 8.373651504516602
Epoch 10, val loss: 1.914015531539917
Epoch 20, training loss: 1.9086045026779175 = 1.9002312421798706 + 0.001 * 8.373303413391113
Epoch 20, val loss: 1.9011790752410889
Epoch 30, training loss: 1.8913735151290894 = 1.8830009698867798 + 0.001 * 8.372588157653809
Epoch 30, val loss: 1.8826757669448853
Epoch 40, training loss: 1.8661108016967773 = 1.857739806175232 + 0.001 * 8.3709716796875
Epoch 40, val loss: 1.8561351299285889
Epoch 50, training loss: 1.8322505950927734 = 1.8238840103149414 + 0.001 * 8.366556167602539
Epoch 50, val loss: 1.8222118616104126
Epoch 60, training loss: 1.7944605350494385 = 1.786110758781433 + 0.001 * 8.349801063537598
Epoch 60, val loss: 1.7868207693099976
Epoch 70, training loss: 1.7534575462341309 = 1.7452199459075928 + 0.001 * 8.2376127243042
Epoch 70, val loss: 1.7487616539001465
Epoch 80, training loss: 1.6965768337249756 = 1.6886993646621704 + 0.001 * 7.877429008483887
Epoch 80, val loss: 1.6970447301864624
Epoch 90, training loss: 1.6193331480026245 = 1.6116478443145752 + 0.001 * 7.685276985168457
Epoch 90, val loss: 1.6295005083084106
Epoch 100, training loss: 1.5242570638656616 = 1.5168286561965942 + 0.001 * 7.428422927856445
Epoch 100, val loss: 1.54937744140625
Epoch 110, training loss: 1.4203596115112305 = 1.413079857826233 + 0.001 * 7.279740810394287
Epoch 110, val loss: 1.4601448774337769
Epoch 120, training loss: 1.312677264213562 = 1.3054447174072266 + 0.001 * 7.232515335083008
Epoch 120, val loss: 1.3711122274398804
Epoch 130, training loss: 1.2021310329437256 = 1.194953441619873 + 0.001 * 7.1775431632995605
Epoch 130, val loss: 1.281981348991394
Epoch 140, training loss: 1.0888488292694092 = 1.0817234516143799 + 0.001 * 7.125356674194336
Epoch 140, val loss: 1.1923902034759521
Epoch 150, training loss: 0.974466860294342 = 0.9673827290534973 + 0.001 * 7.084129333496094
Epoch 150, val loss: 1.10450279712677
Epoch 160, training loss: 0.8632283806800842 = 0.8561716675758362 + 0.001 * 7.056699275970459
Epoch 160, val loss: 1.022151231765747
Epoch 170, training loss: 0.7615509033203125 = 0.7545152306556702 + 0.001 * 7.035694122314453
Epoch 170, val loss: 0.9504963159561157
Epoch 180, training loss: 0.6743201017379761 = 0.6672985553741455 + 0.001 * 7.021551609039307
Epoch 180, val loss: 0.8938246965408325
Epoch 190, training loss: 0.6022522449493408 = 0.5952369570732117 + 0.001 * 7.01527214050293
Epoch 190, val loss: 0.8526484370231628
Epoch 200, training loss: 0.5431578755378723 = 0.5361448526382446 + 0.001 * 7.0130462646484375
Epoch 200, val loss: 0.8243736624717712
Epoch 210, training loss: 0.4937838315963745 = 0.4867722988128662 + 0.001 * 7.011542797088623
Epoch 210, val loss: 0.8056381940841675
Epoch 220, training loss: 0.45089003443717957 = 0.4438805878162384 + 0.001 * 7.009431838989258
Epoch 220, val loss: 0.7931649088859558
Epoch 230, training loss: 0.4121563136577606 = 0.40514928102493286 + 0.001 * 7.007018089294434
Epoch 230, val loss: 0.7847802042961121
Epoch 240, training loss: 0.37615877389907837 = 0.3691544234752655 + 0.001 * 7.004356861114502
Epoch 240, val loss: 0.7793148159980774
Epoch 250, training loss: 0.34220993518829346 = 0.3352087438106537 + 0.001 * 7.001178741455078
Epoch 250, val loss: 0.7759053707122803
Epoch 260, training loss: 0.3101119101047516 = 0.3031107187271118 + 0.001 * 7.001201152801514
Epoch 260, val loss: 0.7741669416427612
Epoch 270, training loss: 0.27989450097084045 = 0.2729003429412842 + 0.001 * 6.994144916534424
Epoch 270, val loss: 0.773872435092926
Epoch 280, training loss: 0.2515304982662201 = 0.24454006552696228 + 0.001 * 6.990440368652344
Epoch 280, val loss: 0.7752814292907715
Epoch 290, training loss: 0.22495709359645844 = 0.21797291934490204 + 0.001 * 6.984171390533447
Epoch 290, val loss: 0.7786626219749451
Epoch 300, training loss: 0.20031626522541046 = 0.1933215707540512 + 0.001 * 6.994687557220459
Epoch 300, val loss: 0.7845178842544556
Epoch 310, training loss: 0.17782412469387054 = 0.17084668576717377 + 0.001 * 6.977439880371094
Epoch 310, val loss: 0.7931492328643799
Epoch 320, training loss: 0.15773752331733704 = 0.15077397227287292 + 0.001 * 6.963545799255371
Epoch 320, val loss: 0.8045037984848022
Epoch 330, training loss: 0.1400928497314453 = 0.1331426501274109 + 0.001 * 6.950192928314209
Epoch 330, val loss: 0.8184186816215515
Epoch 340, training loss: 0.12472327053546906 = 0.1177852600812912 + 0.001 * 6.938013553619385
Epoch 340, val loss: 0.834365725517273
Epoch 350, training loss: 0.11136125028133392 = 0.10443534702062607 + 0.001 * 6.925899505615234
Epoch 350, val loss: 0.8519544005393982
Epoch 360, training loss: 0.0997442826628685 = 0.09282246977090836 + 0.001 * 6.921813011169434
Epoch 360, val loss: 0.870742917060852
Epoch 370, training loss: 0.08960820734500885 = 0.08270468562841415 + 0.001 * 6.903520107269287
Epoch 370, val loss: 0.8904229402542114
Epoch 380, training loss: 0.08076488226652145 = 0.07387267798185349 + 0.001 * 6.892201900482178
Epoch 380, val loss: 0.9106975197792053
Epoch 390, training loss: 0.07302968204021454 = 0.06614398211240768 + 0.001 * 6.8857011795043945
Epoch 390, val loss: 0.9313639402389526
Epoch 400, training loss: 0.06625045835971832 = 0.05936187505722046 + 0.001 * 6.888583183288574
Epoch 400, val loss: 0.9522621631622314
Epoch 410, training loss: 0.06026536971330643 = 0.05339367687702179 + 0.001 * 6.871694087982178
Epoch 410, val loss: 0.9733110070228577
Epoch 420, training loss: 0.05503900349140167 = 0.04812932386994362 + 0.001 * 6.909677505493164
Epoch 420, val loss: 0.994399905204773
Epoch 430, training loss: 0.05035626143217087 = 0.04347643256187439 + 0.001 * 6.879828929901123
Epoch 430, val loss: 1.0154820680618286
Epoch 440, training loss: 0.04622230306267738 = 0.03935803472995758 + 0.001 * 6.864266395568848
Epoch 440, val loss: 1.0363786220550537
Epoch 450, training loss: 0.042570143938064575 = 0.03570983558893204 + 0.001 * 6.8603081703186035
Epoch 450, val loss: 1.0569933652877808
Epoch 460, training loss: 0.03935834765434265 = 0.032476749271154404 + 0.001 * 6.881598472595215
Epoch 460, val loss: 1.0771939754486084
Epoch 470, training loss: 0.036478202790021896 = 0.029611000791192055 + 0.001 * 6.867201328277588
Epoch 470, val loss: 1.096871256828308
Epoch 480, training loss: 0.03392340987920761 = 0.027070287615060806 + 0.001 * 6.853123664855957
Epoch 480, val loss: 1.1159272193908691
Epoch 490, training loss: 0.031670600175857544 = 0.02481648698449135 + 0.001 * 6.854111194610596
Epoch 490, val loss: 1.1343022584915161
Epoch 500, training loss: 0.02966591715812683 = 0.022814763709902763 + 0.001 * 6.851153373718262
Epoch 500, val loss: 1.15195894241333
Epoch 510, training loss: 0.02788299322128296 = 0.021034087985754013 + 0.001 * 6.84890604019165
Epoch 510, val loss: 1.1688741445541382
Epoch 520, training loss: 0.026303181424736977 = 0.019446615129709244 + 0.001 * 6.856566429138184
Epoch 520, val loss: 1.1850777864456177
Epoch 530, training loss: 0.024874534457921982 = 0.018028102815151215 + 0.001 * 6.846430778503418
Epoch 530, val loss: 1.2005749940872192
Epoch 540, training loss: 0.023598868399858475 = 0.016757074743509293 + 0.001 * 6.841794013977051
Epoch 540, val loss: 1.2153761386871338
Epoch 550, training loss: 0.022458400577306747 = 0.015614830888807774 + 0.001 * 6.843570232391357
Epoch 550, val loss: 1.2295286655426025
Epoch 560, training loss: 0.021428214386105537 = 0.014585447497665882 + 0.001 * 6.842766284942627
Epoch 560, val loss: 1.2430659532546997
Epoch 570, training loss: 0.02049381285905838 = 0.013655116781592369 + 0.001 * 6.838696479797363
Epoch 570, val loss: 1.2560139894485474
Epoch 580, training loss: 0.01964198425412178 = 0.012811808846890926 + 0.001 * 6.830173969268799
Epoch 580, val loss: 1.268436312675476
Epoch 590, training loss: 0.018879365175962448 = 0.012045305222272873 + 0.001 * 6.834060192108154
Epoch 590, val loss: 1.2803759574890137
Epoch 600, training loss: 0.018200509250164032 = 0.011346683837473392 + 0.001 * 6.85382604598999
Epoch 600, val loss: 1.2918611764907837
Epoch 610, training loss: 0.0175346527248621 = 0.01070832647383213 + 0.001 * 6.826326370239258
Epoch 610, val loss: 1.3028733730316162
Epoch 620, training loss: 0.016953948885202408 = 0.010123697109520435 + 0.001 * 6.830251693725586
Epoch 620, val loss: 1.3135040998458862
Epoch 630, training loss: 0.01642201840877533 = 0.009586887434124947 + 0.001 * 6.835131645202637
Epoch 630, val loss: 1.323754906654358
Epoch 640, training loss: 0.015909433364868164 = 0.009092720225453377 + 0.001 * 6.816712856292725
Epoch 640, val loss: 1.333648681640625
Epoch 650, training loss: 0.015471698716282845 = 0.008636720478534698 + 0.001 * 6.8349785804748535
Epoch 650, val loss: 1.3432083129882812
Epoch 660, training loss: 0.015047489665448666 = 0.00821501575410366 + 0.001 * 6.8324737548828125
Epoch 660, val loss: 1.3524583578109741
Epoch 670, training loss: 0.014646954834461212 = 0.007824176922440529 + 0.001 * 6.822778224945068
Epoch 670, val loss: 1.361398458480835
Epoch 680, training loss: 0.014281339943408966 = 0.007460848428308964 + 0.001 * 6.820490837097168
Epoch 680, val loss: 1.3700840473175049
Epoch 690, training loss: 0.013952191919088364 = 0.007122347131371498 + 0.001 * 6.8298444747924805
Epoch 690, val loss: 1.37850022315979
Epoch 700, training loss: 0.013617243617773056 = 0.006806498859077692 + 0.001 * 6.810743808746338
Epoch 700, val loss: 1.3866511583328247
Epoch 710, training loss: 0.01332852989435196 = 0.006511241663247347 + 0.001 * 6.817287445068359
Epoch 710, val loss: 1.3945502042770386
Epoch 720, training loss: 0.01304270513355732 = 0.006234719883650541 + 0.001 * 6.807985305786133
Epoch 720, val loss: 1.40224027633667
Epoch 730, training loss: 0.01278119720518589 = 0.005975435953587294 + 0.001 * 6.805760860443115
Epoch 730, val loss: 1.409686803817749
Epoch 740, training loss: 0.012531701475381851 = 0.0057319640181958675 + 0.001 * 6.799737453460693
Epoch 740, val loss: 1.4169526100158691
Epoch 750, training loss: 0.012304962612688541 = 0.005503151565790176 + 0.001 * 6.8018107414245605
Epoch 750, val loss: 1.423980712890625
Epoch 760, training loss: 0.012093551456928253 = 0.0052878111600875854 + 0.001 * 6.805739879608154
Epoch 760, val loss: 1.4308394193649292
Epoch 770, training loss: 0.011888924054801464 = 0.005085041280835867 + 0.001 * 6.803882598876953
Epoch 770, val loss: 1.4374557733535767
Epoch 780, training loss: 0.011689245700836182 = 0.004893897566944361 + 0.001 * 6.795347213745117
Epoch 780, val loss: 1.4439314603805542
Epoch 790, training loss: 0.011520158499479294 = 0.004713640082627535 + 0.001 * 6.8065185546875
Epoch 790, val loss: 1.4502164125442505
Epoch 800, training loss: 0.011337869800627232 = 0.004543422721326351 + 0.001 * 6.79444694519043
Epoch 800, val loss: 1.4563161134719849
Epoch 810, training loss: 0.011182683520019054 = 0.004382542800158262 + 0.001 * 6.800140380859375
Epoch 810, val loss: 1.4622551202774048
Epoch 820, training loss: 0.011008955538272858 = 0.0042304047383368015 + 0.001 * 6.778550624847412
Epoch 820, val loss: 1.4680213928222656
Epoch 830, training loss: 0.010870836675167084 = 0.004086412955075502 + 0.001 * 6.784423351287842
Epoch 830, val loss: 1.4736473560333252
Epoch 840, training loss: 0.010723361745476723 = 0.003950049169361591 + 0.001 * 6.773312568664551
Epoch 840, val loss: 1.4791051149368286
Epoch 850, training loss: 0.010600456967949867 = 0.0038207555189728737 + 0.001 * 6.779700756072998
Epoch 850, val loss: 1.4844250679016113
Epoch 860, training loss: 0.010467834770679474 = 0.003698143409565091 + 0.001 * 6.769690990447998
Epoch 860, val loss: 1.4896035194396973
Epoch 870, training loss: 0.010361029766499996 = 0.0035818431060761213 + 0.001 * 6.779186248779297
Epoch 870, val loss: 1.4946255683898926
Epoch 880, training loss: 0.01024402491748333 = 0.0034714224748313427 + 0.001 * 6.772602558135986
Epoch 880, val loss: 1.4995278120040894
Epoch 890, training loss: 0.010129710659384727 = 0.0033665166702121496 + 0.001 * 6.763193607330322
Epoch 890, val loss: 1.504291296005249
Epoch 900, training loss: 0.010029790922999382 = 0.003266788786277175 + 0.001 * 6.763001918792725
Epoch 900, val loss: 1.5089187622070312
Epoch 910, training loss: 0.009960310533642769 = 0.0031719226390123367 + 0.001 * 6.788388252258301
Epoch 910, val loss: 1.5134669542312622
Epoch 920, training loss: 0.009859317913651466 = 0.0030815601348876953 + 0.001 * 6.777757167816162
Epoch 920, val loss: 1.5178495645523071
Epoch 930, training loss: 0.009770757518708706 = 0.002995394403114915 + 0.001 * 6.775362968444824
Epoch 930, val loss: 1.5221216678619385
Epoch 940, training loss: 0.009677448309957981 = 0.002913249423727393 + 0.001 * 6.7641987800598145
Epoch 940, val loss: 1.5262925624847412
Epoch 950, training loss: 0.00961061380803585 = 0.002834995510056615 + 0.001 * 6.775617599487305
Epoch 950, val loss: 1.5303860902786255
Epoch 960, training loss: 0.00953613966703415 = 0.002760309027507901 + 0.001 * 6.775830268859863
Epoch 960, val loss: 1.5343265533447266
Epoch 970, training loss: 0.009430019184947014 = 0.0026889813598245382 + 0.001 * 6.741037368774414
Epoch 970, val loss: 1.5381814241409302
Epoch 980, training loss: 0.009365266188979149 = 0.0026207496412098408 + 0.001 * 6.744516372680664
Epoch 980, val loss: 1.5419461727142334
Epoch 990, training loss: 0.009304594248533249 = 0.0025554464664310217 + 0.001 * 6.749147415161133
Epoch 990, val loss: 1.5455812215805054
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8487
Flip ASR: 0.8178/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.955580711364746 = 1.9472068548202515 + 0.001 * 8.373889923095703
Epoch 0, val loss: 1.9430598020553589
Epoch 10, training loss: 1.9455801248550415 = 1.9372062683105469 + 0.001 * 8.373851776123047
Epoch 10, val loss: 1.9330172538757324
Epoch 20, training loss: 1.9334626197814941 = 1.925088882446289 + 0.001 * 8.373702049255371
Epoch 20, val loss: 1.9204117059707642
Epoch 30, training loss: 1.916141390800476 = 1.9077680110931396 + 0.001 * 8.373418807983398
Epoch 30, val loss: 1.902052640914917
Epoch 40, training loss: 1.8901118040084839 = 1.8817389011383057 + 0.001 * 8.372843742370605
Epoch 40, val loss: 1.8743802309036255
Epoch 50, training loss: 1.852800726890564 = 1.8444292545318604 + 0.001 * 8.371414184570312
Epoch 50, val loss: 1.8361804485321045
Epoch 60, training loss: 1.8088735342025757 = 1.8005064725875854 + 0.001 * 8.367018699645996
Epoch 60, val loss: 1.7949854135513306
Epoch 70, training loss: 1.7685651779174805 = 1.7602158784866333 + 0.001 * 8.349279403686523
Epoch 70, val loss: 1.7615171670913696
Epoch 80, training loss: 1.7195935249328613 = 1.7113537788391113 + 0.001 * 8.23975658416748
Epoch 80, val loss: 1.7206504344940186
Epoch 90, training loss: 1.650917649269104 = 1.6430472135543823 + 0.001 * 7.870408535003662
Epoch 90, val loss: 1.6630935668945312
Epoch 100, training loss: 1.5606262683868408 = 1.5528265237808228 + 0.001 * 7.799741744995117
Epoch 100, val loss: 1.588004469871521
Epoch 110, training loss: 1.4566171169281006 = 1.4489201307296753 + 0.001 * 7.696956157684326
Epoch 110, val loss: 1.502995491027832
Epoch 120, training loss: 1.349824070930481 = 1.3423274755477905 + 0.001 * 7.496555805206299
Epoch 120, val loss: 1.418580174446106
Epoch 130, training loss: 1.2446503639221191 = 1.2372379302978516 + 0.001 * 7.412447929382324
Epoch 130, val loss: 1.3385733366012573
Epoch 140, training loss: 1.141659140586853 = 1.1343008279800415 + 0.001 * 7.358341217041016
Epoch 140, val loss: 1.2624863386154175
Epoch 150, training loss: 1.041865348815918 = 1.0345723628997803 + 0.001 * 7.292930603027344
Epoch 150, val loss: 1.1899114847183228
Epoch 160, training loss: 0.9467543363571167 = 0.9394975900650024 + 0.001 * 7.2567524909973145
Epoch 160, val loss: 1.1213923692703247
Epoch 170, training loss: 0.8596287965774536 = 0.8523963093757629 + 0.001 * 7.232503890991211
Epoch 170, val loss: 1.0589215755462646
Epoch 180, training loss: 0.7841291427612305 = 0.7769325971603394 + 0.001 * 7.19656229019165
Epoch 180, val loss: 1.0051707029342651
Epoch 190, training loss: 0.7218939661979675 = 0.7147486209869385 + 0.001 * 7.145318984985352
Epoch 190, val loss: 0.9620360136032104
Epoch 200, training loss: 0.6715945601463318 = 0.6645097732543945 + 0.001 * 7.084805488586426
Epoch 200, val loss: 0.9291877746582031
Epoch 210, training loss: 0.6298067569732666 = 0.6227636933326721 + 0.001 * 7.043045997619629
Epoch 210, val loss: 0.9042192101478577
Epoch 220, training loss: 0.5926834344863892 = 0.5856506824493408 + 0.001 * 7.032726287841797
Epoch 220, val loss: 0.8841826915740967
Epoch 230, training loss: 0.5568134784698486 = 0.549787163734436 + 0.001 * 7.026320457458496
Epoch 230, val loss: 0.8664716482162476
Epoch 240, training loss: 0.5196540951728821 = 0.5126335024833679 + 0.001 * 7.020564079284668
Epoch 240, val loss: 0.8490415215492249
Epoch 250, training loss: 0.47965648770332336 = 0.47264036536216736 + 0.001 * 7.016123294830322
Epoch 250, val loss: 0.830843985080719
Epoch 260, training loss: 0.436674028635025 = 0.42966321110725403 + 0.001 * 7.010804653167725
Epoch 260, val loss: 0.8120114803314209
Epoch 270, training loss: 0.39187803864479065 = 0.3848729431629181 + 0.001 * 7.005091190338135
Epoch 270, val loss: 0.7935534715652466
Epoch 280, training loss: 0.34709402918815613 = 0.3400941789150238 + 0.001 * 6.9998555183410645
Epoch 280, val loss: 0.7770358324050903
Epoch 290, training loss: 0.30414482951164246 = 0.29714927077293396 + 0.001 * 6.995553970336914
Epoch 290, val loss: 0.7636265158653259
Epoch 300, training loss: 0.2645106911659241 = 0.257518470287323 + 0.001 * 6.992229461669922
Epoch 300, val loss: 0.7541192770004272
Epoch 310, training loss: 0.2289947122335434 = 0.22200487554073334 + 0.001 * 6.989832401275635
Epoch 310, val loss: 0.748428463935852
Epoch 320, training loss: 0.19788743555545807 = 0.19089923799037933 + 0.001 * 6.988193511962891
Epoch 320, val loss: 0.7459508776664734
Epoch 330, training loss: 0.1711662858724594 = 0.16417960822582245 + 0.001 * 6.986677169799805
Epoch 330, val loss: 0.7465496063232422
Epoch 340, training loss: 0.14849691092967987 = 0.14151178300380707 + 0.001 * 6.985134601593018
Epoch 340, val loss: 0.7501348257064819
Epoch 350, training loss: 0.12941783666610718 = 0.1224338710308075 + 0.001 * 6.983965873718262
Epoch 350, val loss: 0.7564086318016052
Epoch 360, training loss: 0.11342096328735352 = 0.10643790662288666 + 0.001 * 6.983059883117676
Epoch 360, val loss: 0.7647599577903748
Epoch 370, training loss: 0.099992536008358 = 0.09300978481769562 + 0.001 * 6.982753753662109
Epoch 370, val loss: 0.7745376229286194
Epoch 380, training loss: 0.08867844939231873 = 0.08169608563184738 + 0.001 * 6.982365131378174
Epoch 380, val loss: 0.7853022217750549
Epoch 390, training loss: 0.07909867912530899 = 0.07211688160896301 + 0.001 * 6.981797218322754
Epoch 390, val loss: 0.7967309355735779
Epoch 400, training loss: 0.07093820720911026 = 0.06395620107650757 + 0.001 * 6.982003688812256
Epoch 400, val loss: 0.8085184097290039
Epoch 410, training loss: 0.06394410878419876 = 0.05696280673146248 + 0.001 * 6.981298446655273
Epoch 410, val loss: 0.8205404877662659
Epoch 420, training loss: 0.057918671518564224 = 0.05093866214156151 + 0.001 * 6.980009078979492
Epoch 420, val loss: 0.8327211141586304
Epoch 430, training loss: 0.052706848829984665 = 0.04572664946317673 + 0.001 * 6.980198860168457
Epoch 430, val loss: 0.8450814485549927
Epoch 440, training loss: 0.048179131001234055 = 0.04119512811303139 + 0.001 * 6.98400354385376
Epoch 440, val loss: 0.8575819730758667
Epoch 450, training loss: 0.04421991482377052 = 0.03724011406302452 + 0.001 * 6.979800701141357
Epoch 450, val loss: 0.8702864050865173
Epoch 460, training loss: 0.04075387120246887 = 0.033778414130210876 + 0.001 * 6.975454807281494
Epoch 460, val loss: 0.8830264210700989
Epoch 470, training loss: 0.03771146759390831 = 0.030737577006220818 + 0.001 * 6.973891258239746
Epoch 470, val loss: 0.8958207368850708
Epoch 480, training loss: 0.03502543643116951 = 0.02805355004966259 + 0.001 * 6.971887111663818
Epoch 480, val loss: 0.9086536765098572
Epoch 490, training loss: 0.03265876695513725 = 0.025677263736724854 + 0.001 * 6.981502532958984
Epoch 490, val loss: 0.9214807152748108
Epoch 500, training loss: 0.03054277040064335 = 0.023567330092191696 + 0.001 * 6.97544002532959
Epoch 500, val loss: 0.9341431856155396
Epoch 510, training loss: 0.02865947037935257 = 0.021689873188734055 + 0.001 * 6.969597816467285
Epoch 510, val loss: 0.9467099905014038
Epoch 520, training loss: 0.026979822665452957 = 0.020014852285385132 + 0.001 * 6.964969635009766
Epoch 520, val loss: 0.9589605331420898
Epoch 530, training loss: 0.025482719764113426 = 0.018516890704631805 + 0.001 * 6.965829372406006
Epoch 530, val loss: 0.9709475040435791
Epoch 540, training loss: 0.024135654792189598 = 0.01717395894229412 + 0.001 * 6.961696147918701
Epoch 540, val loss: 0.9826098680496216
Epoch 550, training loss: 0.022926025092601776 = 0.01596752181649208 + 0.001 * 6.958502769470215
Epoch 550, val loss: 0.994040846824646
Epoch 560, training loss: 0.0218349639326334 = 0.014881092123687267 + 0.001 * 6.953871250152588
Epoch 560, val loss: 1.005103349685669
Epoch 570, training loss: 0.02085556834936142 = 0.01390050444751978 + 0.001 * 6.955064296722412
Epoch 570, val loss: 1.0158792734146118
Epoch 580, training loss: 0.019970905035734177 = 0.013013109564781189 + 0.001 * 6.957796096801758
Epoch 580, val loss: 1.0263056755065918
Epoch 590, training loss: 0.01917163096368313 = 0.012208215892314911 + 0.001 * 6.963415145874023
Epoch 590, val loss: 1.03642737865448
Epoch 600, training loss: 0.018421432003378868 = 0.011476300656795502 + 0.001 * 6.945130348205566
Epoch 600, val loss: 1.0462095737457275
Epoch 610, training loss: 0.01775066927075386 = 0.01080931443721056 + 0.001 * 6.941353797912598
Epoch 610, val loss: 1.0557339191436768
Epoch 620, training loss: 0.01714090071618557 = 0.010200059041380882 + 0.001 * 6.940840721130371
Epoch 620, val loss: 1.0649757385253906
Epoch 630, training loss: 0.016580237075686455 = 0.009642272256314754 + 0.001 * 6.937964916229248
Epoch 630, val loss: 1.07390558719635
Epoch 640, training loss: 0.01606753095984459 = 0.009130432270467281 + 0.001 * 6.937098503112793
Epoch 640, val loss: 1.0825538635253906
Epoch 650, training loss: 0.015596676617860794 = 0.008659775368869305 + 0.001 * 6.936900615692139
Epoch 650, val loss: 1.0910143852233887
Epoch 660, training loss: 0.015162259340286255 = 0.008226172998547554 + 0.001 * 6.936086654663086
Epoch 660, val loss: 1.0992072820663452
Epoch 670, training loss: 0.01475905068218708 = 0.00782586820423603 + 0.001 * 6.933182239532471
Epoch 670, val loss: 1.1071621179580688
Epoch 680, training loss: 0.014416187070310116 = 0.007455658167600632 + 0.001 * 6.960528373718262
Epoch 680, val loss: 1.1148860454559326
Epoch 690, training loss: 0.014044934883713722 = 0.007112655322998762 + 0.001 * 6.932278633117676
Epoch 690, val loss: 1.1224175691604614
Epoch 700, training loss: 0.01372462511062622 = 0.006794275250285864 + 0.001 * 6.930349826812744
Epoch 700, val loss: 1.129750370979309
Epoch 710, training loss: 0.013422444462776184 = 0.006498223170638084 + 0.001 * 6.924221038818359
Epoch 710, val loss: 1.1368709802627563
Epoch 720, training loss: 0.013169328682124615 = 0.0062224725261330605 + 0.001 * 6.9468560218811035
Epoch 720, val loss: 1.1437959671020508
Epoch 730, training loss: 0.012885565869510174 = 0.0059652142226696014 + 0.001 * 6.920351505279541
Epoch 730, val loss: 1.150551438331604
Epoch 740, training loss: 0.01264149323105812 = 0.0057249306701123714 + 0.001 * 6.916562080383301
Epoch 740, val loss: 1.1571271419525146
Epoch 750, training loss: 0.012418922036886215 = 0.005500102881342173 + 0.001 * 6.918819427490234
Epoch 750, val loss: 1.1635432243347168
Epoch 760, training loss: 0.012204499915242195 = 0.005289396271109581 + 0.001 * 6.915103912353516
Epoch 760, val loss: 1.1698026657104492
Epoch 770, training loss: 0.01200011931359768 = 0.005091721657663584 + 0.001 * 6.908397197723389
Epoch 770, val loss: 1.1759066581726074
Epoch 780, training loss: 0.011820897459983826 = 0.004906029906123877 + 0.001 * 6.914867877960205
Epoch 780, val loss: 1.1818501949310303
Epoch 790, training loss: 0.0116431238129735 = 0.00473135057836771 + 0.001 * 6.911772727966309
Epoch 790, val loss: 1.1876623630523682
Epoch 800, training loss: 0.011491340585052967 = 0.004566848743706942 + 0.001 * 6.9244914054870605
Epoch 800, val loss: 1.1933311223983765
Epoch 810, training loss: 0.011315455660223961 = 0.004411743022501469 + 0.001 * 6.903712749481201
Epoch 810, val loss: 1.1988554000854492
Epoch 820, training loss: 0.011170992627739906 = 0.0042653330601751804 + 0.001 * 6.9056596755981445
Epoch 820, val loss: 1.2042689323425293
Epoch 830, training loss: 0.011020505800843239 = 0.004127005580812693 + 0.001 * 6.893499851226807
Epoch 830, val loss: 1.2095608711242676
Epoch 840, training loss: 0.010904421098530293 = 0.003996154759079218 + 0.001 * 6.908266067504883
Epoch 840, val loss: 1.2147159576416016
Epoch 850, training loss: 0.010768846608698368 = 0.0038722443860024214 + 0.001 * 6.896602153778076
Epoch 850, val loss: 1.2197718620300293
Epoch 860, training loss: 0.010670634917914867 = 0.0037548032123595476 + 0.001 * 6.915831565856934
Epoch 860, val loss: 1.2247434854507446
Epoch 870, training loss: 0.010536838322877884 = 0.0036433846689760685 + 0.001 * 6.8934526443481445
Epoch 870, val loss: 1.2295910120010376
Epoch 880, training loss: 0.010427160188555717 = 0.0035375969018787146 + 0.001 * 6.889563083648682
Epoch 880, val loss: 1.2343261241912842
Epoch 890, training loss: 0.010360310785472393 = 0.0034370298963040113 + 0.001 * 6.923280239105225
Epoch 890, val loss: 1.2389601469039917
Epoch 900, training loss: 0.010249373503029346 = 0.0033413649071007967 + 0.001 * 6.908008575439453
Epoch 900, val loss: 1.24350905418396
Epoch 910, training loss: 0.010141560807824135 = 0.0032502850517630577 + 0.001 * 6.891275405883789
Epoch 910, val loss: 1.247967004776001
Epoch 920, training loss: 0.010053316131234169 = 0.0031635062769055367 + 0.001 * 6.8898091316223145
Epoch 920, val loss: 1.252323031425476
Epoch 930, training loss: 0.009974624961614609 = 0.003080766648054123 + 0.001 * 6.893857479095459
Epoch 930, val loss: 1.2565761804580688
Epoch 940, training loss: 0.009866940788924694 = 0.003001802135258913 + 0.001 * 6.865138530731201
Epoch 940, val loss: 1.260753870010376
Epoch 950, training loss: 0.009817177429795265 = 0.002926401561126113 + 0.001 * 6.890775680541992
Epoch 950, val loss: 1.2648379802703857
Epoch 960, training loss: 0.009716721251606941 = 0.002854331396520138 + 0.001 * 6.86238956451416
Epoch 960, val loss: 1.2688480615615845
Epoch 970, training loss: 0.00969165749847889 = 0.0027854067739099264 + 0.001 * 6.906250476837158
Epoch 970, val loss: 1.2727824449539185
Epoch 980, training loss: 0.009585501626133919 = 0.0027194570284336805 + 0.001 * 6.866044044494629
Epoch 980, val loss: 1.2766276597976685
Epoch 990, training loss: 0.009515908546745777 = 0.0026563003193587065 + 0.001 * 6.859607696533203
Epoch 990, val loss: 1.2803946733474731
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6863
Flip ASR: 0.6489/225 nodes
The final ASR:0.66790, 0.15571, Accuracy:0.81605, 0.01552
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10630])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.4 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9410861730575562 = 1.932712197303772 + 0.001 * 8.373939514160156
Epoch 0, val loss: 1.9235202074050903
Epoch 10, training loss: 1.9319902658462524 = 1.9236164093017578 + 0.001 * 8.373905181884766
Epoch 10, val loss: 1.9145532846450806
Epoch 20, training loss: 1.9208526611328125 = 1.9124789237976074 + 0.001 * 8.373773574829102
Epoch 20, val loss: 1.9036792516708374
Epoch 30, training loss: 1.9052976369857788 = 1.8969241380691528 + 0.001 * 8.373477935791016
Epoch 30, val loss: 1.8889145851135254
Epoch 40, training loss: 1.8821415901184082 = 1.87376868724823 + 0.001 * 8.372844696044922
Epoch 40, val loss: 1.867682695388794
Epoch 50, training loss: 1.8487814664840698 = 1.8404101133346558 + 0.001 * 8.37130069732666
Epoch 50, val loss: 1.8387815952301025
Epoch 60, training loss: 1.8072729110717773 = 1.7989062070846558 + 0.001 * 8.36670207977295
Epoch 60, val loss: 1.8059941530227661
Epoch 70, training loss: 1.764077067375183 = 1.7557287216186523 + 0.001 * 8.348340034484863
Epoch 70, val loss: 1.7735885381698608
Epoch 80, training loss: 1.7105200290679932 = 1.7022860050201416 + 0.001 * 8.234016418457031
Epoch 80, val loss: 1.7275946140289307
Epoch 90, training loss: 1.635408639907837 = 1.6274909973144531 + 0.001 * 7.917672157287598
Epoch 90, val loss: 1.662766456604004
Epoch 100, training loss: 1.5397896766662598 = 1.5319348573684692 + 0.001 * 7.854870796203613
Epoch 100, val loss: 1.5862065553665161
Epoch 110, training loss: 1.4311821460723877 = 1.4233927726745605 + 0.001 * 7.789338111877441
Epoch 110, val loss: 1.4972138404846191
Epoch 120, training loss: 1.320220708847046 = 1.3125920295715332 + 0.001 * 7.628716945648193
Epoch 120, val loss: 1.4083595275878906
Epoch 130, training loss: 1.2134212255477905 = 1.2059555053710938 + 0.001 * 7.465684413909912
Epoch 130, val loss: 1.3239195346832275
Epoch 140, training loss: 1.1140868663787842 = 1.1066783666610718 + 0.001 * 7.408556938171387
Epoch 140, val loss: 1.2484568357467651
Epoch 150, training loss: 1.023826003074646 = 1.016540765762329 + 0.001 * 7.285295009613037
Epoch 150, val loss: 1.1819628477096558
Epoch 160, training loss: 0.9419387578964233 = 0.9347532391548157 + 0.001 * 7.185503959655762
Epoch 160, val loss: 1.123002290725708
Epoch 170, training loss: 0.8658557534217834 = 0.8587121963500977 + 0.001 * 7.143534183502197
Epoch 170, val loss: 1.068402886390686
Epoch 180, training loss: 0.7934395670890808 = 0.7863233685493469 + 0.001 * 7.116203308105469
Epoch 180, val loss: 1.0161964893341064
Epoch 190, training loss: 0.7240552306175232 = 0.7169566750526428 + 0.001 * 7.098554611206055
Epoch 190, val loss: 0.9664073586463928
Epoch 200, training loss: 0.6582022309303284 = 0.6511101722717285 + 0.001 * 7.092078685760498
Epoch 200, val loss: 0.9197940230369568
Epoch 210, training loss: 0.5967631936073303 = 0.5896727442741394 + 0.001 * 7.090467929840088
Epoch 210, val loss: 0.8772640824317932
Epoch 220, training loss: 0.5405995845794678 = 0.5335099101066589 + 0.001 * 7.089653491973877
Epoch 220, val loss: 0.8398194313049316
Epoch 230, training loss: 0.49016788601875305 = 0.48307889699935913 + 0.001 * 7.088982105255127
Epoch 230, val loss: 0.8082804083824158
Epoch 240, training loss: 0.44494616985321045 = 0.4378577470779419 + 0.001 * 7.088418006896973
Epoch 240, val loss: 0.7827184796333313
Epoch 250, training loss: 0.4036163091659546 = 0.3965282142162323 + 0.001 * 7.088084697723389
Epoch 250, val loss: 0.7623759508132935
Epoch 260, training loss: 0.3645799458026886 = 0.357492059469223 + 0.001 * 7.087897777557373
Epoch 260, val loss: 0.7461024522781372
Epoch 270, training loss: 0.32663875818252563 = 0.31955093145370483 + 0.001 * 7.08782434463501
Epoch 270, val loss: 0.7325881719589233
Epoch 280, training loss: 0.28956058621406555 = 0.28247272968292236 + 0.001 * 7.087863445281982
Epoch 280, val loss: 0.7212421298027039
Epoch 290, training loss: 0.2540234327316284 = 0.2469356656074524 + 0.001 * 7.087752342224121
Epoch 290, val loss: 0.7118252515792847
Epoch 300, training loss: 0.22115665674209595 = 0.2140692174434662 + 0.001 * 7.087439060211182
Epoch 300, val loss: 0.7046055197715759
Epoch 310, training loss: 0.19188739359378815 = 0.18480075895786285 + 0.001 * 7.086640357971191
Epoch 310, val loss: 0.7000104784965515
Epoch 320, training loss: 0.1665399670600891 = 0.15945422649383545 + 0.001 * 7.085737228393555
Epoch 320, val loss: 0.6981220245361328
Epoch 330, training loss: 0.14494501054286957 = 0.1378609538078308 + 0.001 * 7.08405876159668
Epoch 330, val loss: 0.6989090442657471
Epoch 340, training loss: 0.12666967511177063 = 0.11958779394626617 + 0.001 * 7.081887722015381
Epoch 340, val loss: 0.7021811008453369
Epoch 350, training loss: 0.11123848706483841 = 0.104157455265522 + 0.001 * 7.0810322761535645
Epoch 350, val loss: 0.7075111269950867
Epoch 360, training loss: 0.09819939732551575 = 0.09112302213907242 + 0.001 * 7.07637357711792
Epoch 360, val loss: 0.7145234942436218
Epoch 370, training loss: 0.0871642604470253 = 0.08009280264377594 + 0.001 * 7.071457862854004
Epoch 370, val loss: 0.722825288772583
Epoch 380, training loss: 0.0777965560555458 = 0.07072952389717102 + 0.001 * 7.067030429840088
Epoch 380, val loss: 0.7320993542671204
Epoch 390, training loss: 0.06981359422206879 = 0.06275493651628494 + 0.001 * 7.058655261993408
Epoch 390, val loss: 0.7420607805252075
Epoch 400, training loss: 0.06298496574163437 = 0.05593324825167656 + 0.001 * 7.051715850830078
Epoch 400, val loss: 0.7525562047958374
Epoch 410, training loss: 0.057120662182569504 = 0.0500708743929863 + 0.001 * 7.049787998199463
Epoch 410, val loss: 0.7633939981460571
Epoch 420, training loss: 0.05205008387565613 = 0.04500886797904968 + 0.001 * 7.0412163734436035
Epoch 420, val loss: 0.7744342684745789
Epoch 430, training loss: 0.0476594939827919 = 0.04061800613999367 + 0.001 * 7.041486740112305
Epoch 430, val loss: 0.7855520248413086
Epoch 440, training loss: 0.04383083060383797 = 0.03679085150361061 + 0.001 * 7.039979934692383
Epoch 440, val loss: 0.7966828942298889
Epoch 450, training loss: 0.04046431928873062 = 0.03344058617949486 + 0.001 * 7.023731708526611
Epoch 450, val loss: 0.8077709078788757
Epoch 460, training loss: 0.03751733899116516 = 0.030496789142489433 + 0.001 * 7.0205488204956055
Epoch 460, val loss: 0.8187834024429321
Epoch 470, training loss: 0.03491908311843872 = 0.027901126071810722 + 0.001 * 7.017958164215088
Epoch 470, val loss: 0.829683244228363
Epoch 480, training loss: 0.032618846744298935 = 0.025604771450161934 + 0.001 * 7.014076232910156
Epoch 480, val loss: 0.8404632210731506
Epoch 490, training loss: 0.03057960607111454 = 0.023566942662000656 + 0.001 * 7.0126633644104
Epoch 490, val loss: 0.8511098027229309
Epoch 500, training loss: 0.028766708448529243 = 0.02175326645374298 + 0.001 * 7.013441562652588
Epoch 500, val loss: 0.8615955710411072
Epoch 510, training loss: 0.027149714529514313 = 0.02013428322970867 + 0.001 * 7.0154314041137695
Epoch 510, val loss: 0.8719232678413391
Epoch 520, training loss: 0.025690192356705666 = 0.018684953451156616 + 0.001 * 7.005239009857178
Epoch 520, val loss: 0.8820874691009521
Epoch 530, training loss: 0.024391314014792442 = 0.017383692786097527 + 0.001 * 7.007620334625244
Epoch 530, val loss: 0.8920732140541077
Epoch 540, training loss: 0.023213759064674377 = 0.01621205545961857 + 0.001 * 7.001704216003418
Epoch 540, val loss: 0.9018571972846985
Epoch 550, training loss: 0.02215387113392353 = 0.015154076740145683 + 0.001 * 6.999793529510498
Epoch 550, val loss: 0.9114707112312317
Epoch 560, training loss: 0.02119772508740425 = 0.014196222648024559 + 0.001 * 7.001502513885498
Epoch 560, val loss: 0.9208747744560242
Epoch 570, training loss: 0.02032371610403061 = 0.013326702639460564 + 0.001 * 6.997013568878174
Epoch 570, val loss: 0.9300981163978577
Epoch 580, training loss: 0.01952458918094635 = 0.0125353392213583 + 0.001 * 6.989250659942627
Epoch 580, val loss: 0.93911212682724
Epoch 590, training loss: 0.018821513280272484 = 0.011813262477517128 + 0.001 * 7.008250713348389
Epoch 590, val loss: 0.9479326009750366
Epoch 600, training loss: 0.01814502477645874 = 0.011152931489050388 + 0.001 * 6.992094039916992
Epoch 600, val loss: 0.9565702080726624
Epoch 610, training loss: 0.017532963305711746 = 0.010547701269388199 + 0.001 * 6.985261917114258
Epoch 610, val loss: 0.9650014042854309
Epoch 620, training loss: 0.01697719469666481 = 0.009991733357310295 + 0.001 * 6.9854607582092285
Epoch 620, val loss: 0.9732438921928406
Epoch 630, training loss: 0.01646837778389454 = 0.009479958564043045 + 0.001 * 6.988419055938721
Epoch 630, val loss: 0.9812990427017212
Epoch 640, training loss: 0.015986479818820953 = 0.009007913991808891 + 0.001 * 6.978564739227295
Epoch 640, val loss: 0.9891764521598816
Epoch 650, training loss: 0.015545720234513283 = 0.008571656420826912 + 0.001 * 6.974063873291016
Epoch 650, val loss: 0.9968776106834412
Epoch 660, training loss: 0.015165615826845169 = 0.008167710155248642 + 0.001 * 6.997905254364014
Epoch 660, val loss: 1.0044121742248535
Epoch 670, training loss: 0.01475845742970705 = 0.007793016266077757 + 0.001 * 6.96544075012207
Epoch 670, val loss: 1.011791467666626
Epoch 680, training loss: 0.014408471994102001 = 0.007444888353347778 + 0.001 * 6.963583469390869
Epoch 680, val loss: 1.0189933776855469
Epoch 690, training loss: 0.014089716598391533 = 0.007120889611542225 + 0.001 * 6.9688262939453125
Epoch 690, val loss: 1.0260525941848755
Epoch 700, training loss: 0.013786446303129196 = 0.00681887986138463 + 0.001 * 6.967565536499023
Epoch 700, val loss: 1.0329694747924805
Epoch 710, training loss: 0.013498811051249504 = 0.006536922883242369 + 0.001 * 6.961888313293457
Epoch 710, val loss: 1.0397348403930664
Epoch 720, training loss: 0.013227976858615875 = 0.006273303180932999 + 0.001 * 6.9546732902526855
Epoch 720, val loss: 1.046366572380066
Epoch 730, training loss: 0.013029620982706547 = 0.006026469636708498 + 0.001 * 7.003150939941406
Epoch 730, val loss: 1.0528454780578613
Epoch 740, training loss: 0.012747704982757568 = 0.005795029457658529 + 0.001 * 6.952675819396973
Epoch 740, val loss: 1.0592148303985596
Epoch 750, training loss: 0.01253008283674717 = 0.00557781383395195 + 0.001 * 6.952268600463867
Epoch 750, val loss: 1.0654383897781372
Epoch 760, training loss: 0.0123529601842165 = 0.005373641382902861 + 0.001 * 6.979319095611572
Epoch 760, val loss: 1.0715463161468506
Epoch 770, training loss: 0.012130329385399818 = 0.005181498359888792 + 0.001 * 6.948831081390381
Epoch 770, val loss: 1.0775409936904907
Epoch 780, training loss: 0.011971919797360897 = 0.005000476259738207 + 0.001 * 6.971443176269531
Epoch 780, val loss: 1.0834187269210815
Epoch 790, training loss: 0.011768252588808537 = 0.004829722456634045 + 0.001 * 6.938529968261719
Epoch 790, val loss: 1.0891814231872559
Epoch 800, training loss: 0.01160413958132267 = 0.004668485373258591 + 0.001 * 6.935654163360596
Epoch 800, val loss: 1.094843864440918
Epoch 810, training loss: 0.011451544240117073 = 0.004516069311648607 + 0.001 * 6.935474395751953
Epoch 810, val loss: 1.100388765335083
Epoch 820, training loss: 0.011303767561912537 = 0.004371880553662777 + 0.001 * 6.931886672973633
Epoch 820, val loss: 1.1058298349380493
Epoch 830, training loss: 0.011184975504875183 = 0.004235285334289074 + 0.001 * 6.949690341949463
Epoch 830, val loss: 1.1111875772476196
Epoch 840, training loss: 0.011024429462850094 = 0.004105802625417709 + 0.001 * 6.918626308441162
Epoch 840, val loss: 1.1164395809173584
Epoch 850, training loss: 0.010895665735006332 = 0.003982905298471451 + 0.001 * 6.912759780883789
Epoch 850, val loss: 1.1215887069702148
Epoch 860, training loss: 0.010783705860376358 = 0.0038661686703562737 + 0.001 * 6.917537212371826
Epoch 860, val loss: 1.1266584396362305
Epoch 870, training loss: 0.010707512497901917 = 0.0037551114801317453 + 0.001 * 6.9524006843566895
Epoch 870, val loss: 1.1316239833831787
Epoch 880, training loss: 0.010567953810095787 = 0.003649459220468998 + 0.001 * 6.918494701385498
Epoch 880, val loss: 1.1365422010421753
Epoch 890, training loss: 0.01048848032951355 = 0.0035488156136125326 + 0.001 * 6.939664840698242
Epoch 890, val loss: 1.141331672668457
Epoch 900, training loss: 0.010358649305999279 = 0.0034528912510722876 + 0.001 * 6.905757427215576
Epoch 900, val loss: 1.1460670232772827
Epoch 910, training loss: 0.010283839888870716 = 0.003361369948834181 + 0.001 * 6.922469615936279
Epoch 910, val loss: 1.1507028341293335
Epoch 920, training loss: 0.010188326239585876 = 0.003274022601544857 + 0.001 * 6.914302825927734
Epoch 920, val loss: 1.155279517173767
Epoch 930, training loss: 0.010087670758366585 = 0.0031905905343592167 + 0.001 * 6.897080421447754
Epoch 930, val loss: 1.1597747802734375
Epoch 940, training loss: 0.010006283409893513 = 0.003110838821157813 + 0.001 * 6.895444393157959
Epoch 940, val loss: 1.1642041206359863
Epoch 950, training loss: 0.009939124807715416 = 0.003034532070159912 + 0.001 * 6.904592990875244
Epoch 950, val loss: 1.1685634851455688
Epoch 960, training loss: 0.009892692789435387 = 0.0029614765662699938 + 0.001 * 6.931215763092041
Epoch 960, val loss: 1.1728521585464478
Epoch 970, training loss: 0.009782791137695312 = 0.0028914851136505604 + 0.001 * 6.891305446624756
Epoch 970, val loss: 1.1770726442337036
Epoch 980, training loss: 0.00970996730029583 = 0.0028243993874639273 + 0.001 * 6.885567665100098
Epoch 980, val loss: 1.181221842765808
Epoch 990, training loss: 0.009638994932174683 = 0.0027600442990660667 + 0.001 * 6.8789496421813965
Epoch 990, val loss: 1.185317873954773
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.5904
Flip ASR: 0.5156/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9465185403823853 = 1.938144564628601 + 0.001 * 8.373937606811523
Epoch 0, val loss: 1.9428941011428833
Epoch 10, training loss: 1.936518907546997 = 1.9281450510025024 + 0.001 * 8.373907089233398
Epoch 10, val loss: 1.9321954250335693
Epoch 20, training loss: 1.9244447946548462 = 1.9160710573196411 + 0.001 * 8.373780250549316
Epoch 20, val loss: 1.9191763401031494
Epoch 30, training loss: 1.908085584640503 = 1.899712085723877 + 0.001 * 8.373509407043457
Epoch 30, val loss: 1.9016057252883911
Epoch 40, training loss: 1.8846321105957031 = 1.876259207725525 + 0.001 * 8.372891426086426
Epoch 40, val loss: 1.87678062915802
Epoch 50, training loss: 1.8518919944763184 = 1.8435206413269043 + 0.001 * 8.371365547180176
Epoch 50, val loss: 1.843159556388855
Epoch 60, training loss: 1.8111430406570435 = 1.8027762174606323 + 0.001 * 8.366819381713867
Epoch 60, val loss: 1.8038146495819092
Epoch 70, training loss: 1.7673156261444092 = 1.758967399597168 + 0.001 * 8.348225593566895
Epoch 70, val loss: 1.7640107870101929
Epoch 80, training loss: 1.71417236328125 = 1.7059504985809326 + 0.001 * 8.221923828125
Epoch 80, val loss: 1.716396450996399
Epoch 90, training loss: 1.640094518661499 = 1.632228136062622 + 0.001 * 7.866356372833252
Epoch 90, val loss: 1.6525744199752808
Epoch 100, training loss: 1.5445111989974976 = 1.5367279052734375 + 0.001 * 7.783246994018555
Epoch 100, val loss: 1.573249101638794
Epoch 110, training loss: 1.4357868432998657 = 1.4280844926834106 + 0.001 * 7.70231819152832
Epoch 110, val loss: 1.4859603643417358
Epoch 120, training loss: 1.326241135597229 = 1.3186917304992676 + 0.001 * 7.549378395080566
Epoch 120, val loss: 1.4007761478424072
Epoch 130, training loss: 1.2226674556732178 = 1.2152397632598877 + 0.001 * 7.4276885986328125
Epoch 130, val loss: 1.3228257894515991
Epoch 140, training loss: 1.1261886358261108 = 1.1188600063323975 + 0.001 * 7.32863187789917
Epoch 140, val loss: 1.2512280941009521
Epoch 150, training loss: 1.0355502367019653 = 1.0282565355300903 + 0.001 * 7.293644428253174
Epoch 150, val loss: 1.1835861206054688
Epoch 160, training loss: 0.950331449508667 = 0.9430657029151917 + 0.001 * 7.265719413757324
Epoch 160, val loss: 1.1193722486495972
Epoch 170, training loss: 0.8710150718688965 = 0.8637828826904297 + 0.001 * 7.232202529907227
Epoch 170, val loss: 1.0592052936553955
Epoch 180, training loss: 0.7978802919387817 = 0.790687620639801 + 0.001 * 7.192654609680176
Epoch 180, val loss: 1.0035239458084106
Epoch 190, training loss: 0.7310327291488647 = 0.723875880241394 + 0.001 * 7.15685510635376
Epoch 190, val loss: 0.9529297947883606
Epoch 200, training loss: 0.6703425049781799 = 0.6632038354873657 + 0.001 * 7.138646602630615
Epoch 200, val loss: 0.9082308411598206
Epoch 210, training loss: 0.6148542165756226 = 0.6077176928520203 + 0.001 * 7.136533737182617
Epoch 210, val loss: 0.8690513968467712
Epoch 220, training loss: 0.562886655330658 = 0.5557506680488586 + 0.001 * 7.135984420776367
Epoch 220, val loss: 0.8343679904937744
Epoch 230, training loss: 0.5126301646232605 = 0.5054938793182373 + 0.001 * 7.136293411254883
Epoch 230, val loss: 0.8029444813728333
Epoch 240, training loss: 0.46285897493362427 = 0.45572152733802795 + 0.001 * 7.137442588806152
Epoch 240, val loss: 0.7738019227981567
Epoch 250, training loss: 0.4135245978832245 = 0.4063858985900879 + 0.001 * 7.138709545135498
Epoch 250, val loss: 0.7466983199119568
Epoch 260, training loss: 0.36545971035957336 = 0.3583197295665741 + 0.001 * 7.139967918395996
Epoch 260, val loss: 0.7219241857528687
Epoch 270, training loss: 0.319970965385437 = 0.31282973289489746 + 0.001 * 7.141242980957031
Epoch 270, val loss: 0.7002204656600952
Epoch 280, training loss: 0.2781347334384918 = 0.2709920108318329 + 0.001 * 7.142721652984619
Epoch 280, val loss: 0.6822056770324707
Epoch 290, training loss: 0.2408164143562317 = 0.23367196321487427 + 0.001 * 7.144449710845947
Epoch 290, val loss: 0.6685631275177002
Epoch 300, training loss: 0.20860740542411804 = 0.20145906507968903 + 0.001 * 7.148340225219727
Epoch 300, val loss: 0.6595562696456909
Epoch 310, training loss: 0.18141140043735504 = 0.17426200211048126 + 0.001 * 7.149401664733887
Epoch 310, val loss: 0.6550370454788208
Epoch 320, training loss: 0.15872497856616974 = 0.15157389640808105 + 0.001 * 7.151085376739502
Epoch 320, val loss: 0.6544577479362488
Epoch 330, training loss: 0.13987770676612854 = 0.13272462785243988 + 0.001 * 7.153074741363525
Epoch 330, val loss: 0.6571131348609924
Epoch 340, training loss: 0.12415547668933868 = 0.11700089275836945 + 0.001 * 7.154586315155029
Epoch 340, val loss: 0.6625953316688538
Epoch 350, training loss: 0.11094933748245239 = 0.10379375517368317 + 0.001 * 7.155585765838623
Epoch 350, val loss: 0.670274555683136
Epoch 360, training loss: 0.099757120013237 = 0.09260173887014389 + 0.001 * 7.155383586883545
Epoch 360, val loss: 0.6795635223388672
Epoch 370, training loss: 0.09015960246324539 = 0.08300483226776123 + 0.001 * 7.15476655960083
Epoch 370, val loss: 0.6900302171707153
Epoch 380, training loss: 0.08181444555521011 = 0.07466044276952744 + 0.001 * 7.153999328613281
Epoch 380, val loss: 0.701261579990387
Epoch 390, training loss: 0.07446027547121048 = 0.06730711460113525 + 0.001 * 7.153158187866211
Epoch 390, val loss: 0.7130609750747681
Epoch 400, training loss: 0.06786371767520905 = 0.060715608298778534 + 0.001 * 7.148112773895264
Epoch 400, val loss: 0.7252662777900696
Epoch 410, training loss: 0.06181671842932701 = 0.05467669293284416 + 0.001 * 7.140025615692139
Epoch 410, val loss: 0.7378478050231934
Epoch 420, training loss: 0.05623907223343849 = 0.0491010881960392 + 0.001 * 7.137984752655029
Epoch 420, val loss: 0.7507604360580444
Epoch 430, training loss: 0.05111044645309448 = 0.04397730529308319 + 0.001 * 7.133139610290527
Epoch 430, val loss: 0.7641639709472656
Epoch 440, training loss: 0.04655107483267784 = 0.039423901587724686 + 0.001 * 7.127171993255615
Epoch 440, val loss: 0.777947187423706
Epoch 450, training loss: 0.042695920914411545 = 0.035572513937950134 + 0.001 * 7.123407363891602
Epoch 450, val loss: 0.7921199798583984
Epoch 460, training loss: 0.039450984448194504 = 0.032332129776477814 + 0.001 * 7.11885404586792
Epoch 460, val loss: 0.8067163228988647
Epoch 470, training loss: 0.03663567826151848 = 0.02951989509165287 + 0.001 * 7.115784168243408
Epoch 470, val loss: 0.8212748169898987
Epoch 480, training loss: 0.03417851775884628 = 0.02706354670226574 + 0.001 * 7.114968776702881
Epoch 480, val loss: 0.8351510763168335
Epoch 490, training loss: 0.03202299401164055 = 0.024899497628211975 + 0.001 * 7.123495578765869
Epoch 490, val loss: 0.848429799079895
Epoch 500, training loss: 0.0300863329321146 = 0.022975992411375046 + 0.001 * 7.110340118408203
Epoch 500, val loss: 0.8614793419837952
Epoch 510, training loss: 0.028360486030578613 = 0.02125558629631996 + 0.001 * 7.104899883270264
Epoch 510, val loss: 0.874343991279602
Epoch 520, training loss: 0.0268122386187315 = 0.0197108406573534 + 0.001 * 7.1013970375061035
Epoch 520, val loss: 0.8870636820793152
Epoch 530, training loss: 0.02543342299759388 = 0.018319956958293915 + 0.001 * 7.113465785980225
Epoch 530, val loss: 0.8994340896606445
Epoch 540, training loss: 0.024167627096176147 = 0.017065182328224182 + 0.001 * 7.102444648742676
Epoch 540, val loss: 0.9115267992019653
Epoch 550, training loss: 0.023022634908556938 = 0.015930386260151863 + 0.001 * 7.092248916625977
Epoch 550, val loss: 0.9233222007751465
Epoch 560, training loss: 0.021987032145261765 = 0.014901665970683098 + 0.001 * 7.085365295410156
Epoch 560, val loss: 0.9348204731941223
Epoch 570, training loss: 0.021053649485111237 = 0.013967268168926239 + 0.001 * 7.086380958557129
Epoch 570, val loss: 0.9460596442222595
Epoch 580, training loss: 0.020195774734020233 = 0.013116659596562386 + 0.001 * 7.0791144371032715
Epoch 580, val loss: 0.9569993615150452
Epoch 590, training loss: 0.019429698586463928 = 0.012340554036200047 + 0.001 * 7.089144229888916
Epoch 590, val loss: 0.9676303267478943
Epoch 600, training loss: 0.01870713382959366 = 0.011630729772150517 + 0.001 * 7.0764031410217285
Epoch 600, val loss: 0.9780161380767822
Epoch 610, training loss: 0.018043862655758858 = 0.010979363694787025 + 0.001 * 7.0644989013671875
Epoch 610, val loss: 0.988192081451416
Epoch 620, training loss: 0.017501551657915115 = 0.010377621278166771 + 0.001 * 7.123929977416992
Epoch 620, val loss: 0.9981030225753784
Epoch 630, training loss: 0.016868500038981438 = 0.009821681305766106 + 0.001 * 7.046818256378174
Epoch 630, val loss: 1.0077818632125854
Epoch 640, training loss: 0.01636655628681183 = 0.009306928142905235 + 0.001 * 7.059628486633301
Epoch 640, val loss: 1.017286777496338
Epoch 650, training loss: 0.01590043492615223 = 0.008829683065414429 + 0.001 * 7.070751190185547
Epoch 650, val loss: 1.0264981985092163
Epoch 660, training loss: 0.01542789675295353 = 0.008386608213186264 + 0.001 * 7.041288375854492
Epoch 660, val loss: 1.0355333089828491
Epoch 670, training loss: 0.015001360327005386 = 0.00797299761325121 + 0.001 * 7.028362274169922
Epoch 670, val loss: 1.0443055629730225
Epoch 680, training loss: 0.014626102522015572 = 0.007586685474961996 + 0.001 * 7.039416313171387
Epoch 680, val loss: 1.0530145168304443
Epoch 690, training loss: 0.014258012175559998 = 0.007224273402243853 + 0.001 * 7.03373908996582
Epoch 690, val loss: 1.0615565776824951
Epoch 700, training loss: 0.013911917805671692 = 0.00688388105481863 + 0.001 * 7.028037071228027
Epoch 700, val loss: 1.0699678659439087
Epoch 710, training loss: 0.01361004076898098 = 0.006562959868460894 + 0.001 * 7.047080993652344
Epoch 710, val loss: 1.0782973766326904
Epoch 720, training loss: 0.013262445107102394 = 0.006260444410145283 + 0.001 * 7.002000331878662
Epoch 720, val loss: 1.0865263938903809
Epoch 730, training loss: 0.012979380786418915 = 0.005975206382572651 + 0.001 * 7.004173755645752
Epoch 730, val loss: 1.0947117805480957
Epoch 740, training loss: 0.01270759291946888 = 0.005706537049263716 + 0.001 * 7.00105619430542
Epoch 740, val loss: 1.1028035879135132
Epoch 750, training loss: 0.012482326477766037 = 0.0054536741226911545 + 0.001 * 7.028651714324951
Epoch 750, val loss: 1.1108331680297852
Epoch 760, training loss: 0.012202266603708267 = 0.0052161915227770805 + 0.001 * 6.986074447631836
Epoch 760, val loss: 1.1187771558761597
Epoch 770, training loss: 0.0119815394282341 = 0.004992963280528784 + 0.001 * 6.9885759353637695
Epoch 770, val loss: 1.1266337633132935
Epoch 780, training loss: 0.011762011796236038 = 0.004783444572240114 + 0.001 * 6.978566646575928
Epoch 780, val loss: 1.134371042251587
Epoch 790, training loss: 0.01155571173876524 = 0.004586611408740282 + 0.001 * 6.969099998474121
Epoch 790, val loss: 1.1420245170593262
Epoch 800, training loss: 0.011378437280654907 = 0.004401843063533306 + 0.001 * 6.976593971252441
Epoch 800, val loss: 1.1495602130889893
Epoch 810, training loss: 0.011201349087059498 = 0.004228274337947369 + 0.001 * 6.973074436187744
Epoch 810, val loss: 1.1569759845733643
Epoch 820, training loss: 0.011044804006814957 = 0.004065209999680519 + 0.001 * 6.9795942306518555
Epoch 820, val loss: 1.1642478704452515
Epoch 830, training loss: 0.010899453423917294 = 0.003911886364221573 + 0.001 * 6.987566947937012
Epoch 830, val loss: 1.171444058418274
Epoch 840, training loss: 0.010720946826040745 = 0.003767718793824315 + 0.001 * 6.953227519989014
Epoch 840, val loss: 1.1785064935684204
Epoch 850, training loss: 0.010610942728817463 = 0.0036319089122116566 + 0.001 * 6.979033470153809
Epoch 850, val loss: 1.1854335069656372
Epoch 860, training loss: 0.010449433699250221 = 0.003503819229081273 + 0.001 * 6.945613861083984
Epoch 860, val loss: 1.19229257106781
Epoch 870, training loss: 0.010372193530201912 = 0.0033829021267592907 + 0.001 * 6.989291191101074
Epoch 870, val loss: 1.1990196704864502
Epoch 880, training loss: 0.010210668668150902 = 0.003268906380981207 + 0.001 * 6.941761493682861
Epoch 880, val loss: 1.2056089639663696
Epoch 890, training loss: 0.010121366940438747 = 0.003161216387525201 + 0.001 * 6.960150241851807
Epoch 890, val loss: 1.212114691734314
Epoch 900, training loss: 0.00999454502016306 = 0.0030594083946198225 + 0.001 * 6.935135841369629
Epoch 900, val loss: 1.2184524536132812
Epoch 910, training loss: 0.00993424840271473 = 0.0029629848431795835 + 0.001 * 6.971263408660889
Epoch 910, val loss: 1.2247143983840942
Epoch 920, training loss: 0.009798426181077957 = 0.0028716803062707186 + 0.001 * 6.926745414733887
Epoch 920, val loss: 1.2308661937713623
Epoch 930, training loss: 0.009724030271172523 = 0.0027851094491779804 + 0.001 * 6.938920497894287
Epoch 930, val loss: 1.2368806600570679
Epoch 940, training loss: 0.009649408981204033 = 0.002703031525015831 + 0.001 * 6.946376800537109
Epoch 940, val loss: 1.2428110837936401
Epoch 950, training loss: 0.009593820199370384 = 0.0026250318624079227 + 0.001 * 6.968787670135498
Epoch 950, val loss: 1.2486212253570557
Epoch 960, training loss: 0.0094656553119421 = 0.0025510056875646114 + 0.001 * 6.914649963378906
Epoch 960, val loss: 1.2543528079986572
Epoch 970, training loss: 0.009389178827404976 = 0.0024806391447782516 + 0.001 * 6.908539772033691
Epoch 970, val loss: 1.259988784790039
Epoch 980, training loss: 0.009320330806076527 = 0.002413747366517782 + 0.001 * 6.906583309173584
Epoch 980, val loss: 1.265486717224121
Epoch 990, training loss: 0.009279962629079819 = 0.0023500225506722927 + 0.001 * 6.9299397468566895
Epoch 990, val loss: 1.2709211111068726
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.6162
Flip ASR: 0.5556/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9554446935653687 = 1.947070837020874 + 0.001 * 8.373907089233398
Epoch 0, val loss: 1.9461907148361206
Epoch 10, training loss: 1.9443919658660889 = 1.9360181093215942 + 0.001 * 8.373856544494629
Epoch 10, val loss: 1.9352561235427856
Epoch 20, training loss: 1.9308607578277588 = 1.9224871397018433 + 0.001 * 8.373674392700195
Epoch 20, val loss: 1.9220011234283447
Epoch 30, training loss: 1.9119280576705933 = 1.9035547971725464 + 0.001 * 8.373266220092773
Epoch 30, val loss: 1.9036593437194824
Epoch 40, training loss: 1.8843897581100464 = 1.876017451286316 + 0.001 * 8.37235164642334
Epoch 40, val loss: 1.8775967359542847
Epoch 50, training loss: 1.8461387157440186 = 1.8377686738967896 + 0.001 * 8.36999225616455
Epoch 50, val loss: 1.8431565761566162
Epoch 60, training loss: 1.8009006977081299 = 1.7925382852554321 + 0.001 * 8.362386703491211
Epoch 60, val loss: 1.8053133487701416
Epoch 70, training loss: 1.7549254894256592 = 1.7465972900390625 + 0.001 * 8.32819652557373
Epoch 70, val loss: 1.7667405605316162
Epoch 80, training loss: 1.6977821588516235 = 1.6896766424179077 + 0.001 * 8.105466842651367
Epoch 80, val loss: 1.7149052619934082
Epoch 90, training loss: 1.6206660270690918 = 1.6127798557281494 + 0.001 * 7.886229038238525
Epoch 90, val loss: 1.6475074291229248
Epoch 100, training loss: 1.5241808891296387 = 1.5163642168045044 + 0.001 * 7.816723346710205
Epoch 100, val loss: 1.56803297996521
Epoch 110, training loss: 1.4170762300491333 = 1.4093397855758667 + 0.001 * 7.7364678382873535
Epoch 110, val loss: 1.4803757667541504
Epoch 120, training loss: 1.3075112104415894 = 1.3000080585479736 + 0.001 * 7.503095626831055
Epoch 120, val loss: 1.3926336765289307
Epoch 130, training loss: 1.2004112005233765 = 1.1929931640625 + 0.001 * 7.418081760406494
Epoch 130, val loss: 1.3082029819488525
Epoch 140, training loss: 1.0990034341812134 = 1.0916893482208252 + 0.001 * 7.314077854156494
Epoch 140, val loss: 1.2303084135055542
Epoch 150, training loss: 1.0064287185668945 = 0.999152421951294 + 0.001 * 7.2763142585754395
Epoch 150, val loss: 1.160471796989441
Epoch 160, training loss: 0.923951268196106 = 0.9166820645332336 + 0.001 * 7.269232273101807
Epoch 160, val loss: 1.099348783493042
Epoch 170, training loss: 0.8508423566818237 = 0.843585193157196 + 0.001 * 7.257141590118408
Epoch 170, val loss: 1.0462034940719604
Epoch 180, training loss: 0.7852171063423157 = 0.7779725193977356 + 0.001 * 7.244563102722168
Epoch 180, val loss: 0.9998191595077515
Epoch 190, training loss: 0.7251711487770081 = 0.7179418802261353 + 0.001 * 7.2292680740356445
Epoch 190, val loss: 0.9587475657463074
Epoch 200, training loss: 0.669136643409729 = 0.661927342414856 + 0.001 * 7.209281921386719
Epoch 200, val loss: 0.92198646068573
Epoch 210, training loss: 0.6159706115722656 = 0.60878986120224 + 0.001 * 7.180737495422363
Epoch 210, val loss: 0.8885191082954407
Epoch 220, training loss: 0.5650534629821777 = 0.5579131245613098 + 0.001 * 7.140342712402344
Epoch 220, val loss: 0.8578671216964722
Epoch 230, training loss: 0.5162506103515625 = 0.5091436505317688 + 0.001 * 7.106955051422119
Epoch 230, val loss: 0.8298441767692566
Epoch 240, training loss: 0.46942460536956787 = 0.46234723925590515 + 0.001 * 7.0773797035217285
Epoch 240, val loss: 0.804751992225647
Epoch 250, training loss: 0.42420002818107605 = 0.41713619232177734 + 0.001 * 7.063843727111816
Epoch 250, val loss: 0.7826876044273376
Epoch 260, training loss: 0.3802087903022766 = 0.37315481901168823 + 0.001 * 7.053982734680176
Epoch 260, val loss: 0.7642305493354797
Epoch 270, training loss: 0.3375715911388397 = 0.3305245637893677 + 0.001 * 7.047028064727783
Epoch 270, val loss: 0.7497197389602661
Epoch 280, training loss: 0.29703524708747864 = 0.28999096155166626 + 0.001 * 7.0442938804626465
Epoch 280, val loss: 0.7390398979187012
Epoch 290, training loss: 0.2596126198768616 = 0.25257739424705505 + 0.001 * 7.035212993621826
Epoch 290, val loss: 0.7321091890335083
Epoch 300, training loss: 0.22623485326766968 = 0.21920453011989594 + 0.001 * 7.030325889587402
Epoch 300, val loss: 0.7288278937339783
Epoch 310, training loss: 0.19718948006629944 = 0.1901632398366928 + 0.001 * 7.026236057281494
Epoch 310, val loss: 0.7288248538970947
Epoch 320, training loss: 0.17228975892066956 = 0.16526782512664795 + 0.001 * 7.021939754486084
Epoch 320, val loss: 0.7315943837165833
Epoch 330, training loss: 0.1510971337556839 = 0.14407771825790405 + 0.001 * 7.019421577453613
Epoch 330, val loss: 0.7367953658103943
Epoch 340, training loss: 0.13309608399868011 = 0.12607966363430023 + 0.001 * 7.016415119171143
Epoch 340, val loss: 0.7439973950386047
Epoch 350, training loss: 0.11777424067258835 = 0.11076254397630692 + 0.001 * 7.01169490814209
Epoch 350, val loss: 0.7527878880500793
Epoch 360, training loss: 0.10468333959579468 = 0.09766988456249237 + 0.001 * 7.013453483581543
Epoch 360, val loss: 0.7628346681594849
Epoch 370, training loss: 0.09345521032810211 = 0.08644603192806244 + 0.001 * 7.009176254272461
Epoch 370, val loss: 0.7737693190574646
Epoch 380, training loss: 0.08381971716880798 = 0.07681016623973846 + 0.001 * 7.009549140930176
Epoch 380, val loss: 0.7852845191955566
Epoch 390, training loss: 0.07553108036518097 = 0.06852167844772339 + 0.001 * 7.0093994140625
Epoch 390, val loss: 0.7970512509346008
Epoch 400, training loss: 0.06837528198957443 = 0.061369746923446655 + 0.001 * 7.005533218383789
Epoch 400, val loss: 0.8089035153388977
Epoch 410, training loss: 0.062187083065509796 = 0.05517672374844551 + 0.001 * 7.010356903076172
Epoch 410, val loss: 0.8206539750099182
Epoch 420, training loss: 0.05679764226078987 = 0.04979409649968147 + 0.001 * 7.003545761108398
Epoch 420, val loss: 0.8323242664337158
Epoch 430, training loss: 0.05209288001060486 = 0.045089397579431534 + 0.001 * 7.003481864929199
Epoch 430, val loss: 0.8437374830245972
Epoch 440, training loss: 0.04796229675412178 = 0.04095625504851341 + 0.001 * 7.006040096282959
Epoch 440, val loss: 0.8548877239227295
Epoch 450, training loss: 0.044309958815574646 = 0.03731068596243858 + 0.001 * 6.999270439147949
Epoch 450, val loss: 0.8657616972923279
Epoch 460, training loss: 0.041083503514528275 = 0.03408345952630043 + 0.001 * 7.00004243850708
Epoch 460, val loss: 0.8763414025306702
Epoch 470, training loss: 0.038219526410102844 = 0.031216945499181747 + 0.001 * 7.002580642700195
Epoch 470, val loss: 0.8866239786148071
Epoch 480, training loss: 0.03566247224807739 = 0.028663678094744682 + 0.001 * 6.998793125152588
Epoch 480, val loss: 0.8967169523239136
Epoch 490, training loss: 0.03337885066866875 = 0.026383668184280396 + 0.001 * 6.99518346786499
Epoch 490, val loss: 0.9065691232681274
Epoch 500, training loss: 0.03135640174150467 = 0.02434203028678894 + 0.001 * 7.014369964599609
Epoch 500, val loss: 0.9162123799324036
Epoch 510, training loss: 0.029503095895051956 = 0.022508684545755386 + 0.001 * 6.994410991668701
Epoch 510, val loss: 0.925679624080658
Epoch 520, training loss: 0.027845589444041252 = 0.02085823565721512 + 0.001 * 6.9873528480529785
Epoch 520, val loss: 0.9349265694618225
Epoch 530, training loss: 0.02635888196527958 = 0.0193690974265337 + 0.001 * 6.989784240722656
Epoch 530, val loss: 0.9439921379089355
Epoch 540, training loss: 0.025007694959640503 = 0.01802215166389942 + 0.001 * 6.985543727874756
Epoch 540, val loss: 0.9529184103012085
Epoch 550, training loss: 0.023784156888723373 = 0.0167989581823349 + 0.001 * 6.985198020935059
Epoch 550, val loss: 0.9617173075675964
Epoch 560, training loss: 0.022672832012176514 = 0.015681704506278038 + 0.001 * 6.9911274909973145
Epoch 560, val loss: 0.9704602360725403
Epoch 570, training loss: 0.021637091413140297 = 0.01465681940317154 + 0.001 * 6.98027229309082
Epoch 570, val loss: 0.9792017340660095
Epoch 580, training loss: 0.020699258893728256 = 0.013715149834752083 + 0.001 * 6.984108924865723
Epoch 580, val loss: 0.9878848791122437
Epoch 590, training loss: 0.019830219447612762 = 0.012849940918385983 + 0.001 * 6.980277061462402
Epoch 590, val loss: 0.9964807629585266
Epoch 600, training loss: 0.019039269536733627 = 0.012055283412337303 + 0.001 * 6.983985424041748
Epoch 600, val loss: 1.0049933195114136
Epoch 610, training loss: 0.018299872055649757 = 0.011325745843350887 + 0.001 * 6.974125385284424
Epoch 610, val loss: 1.0133856534957886
Epoch 620, training loss: 0.017623431980609894 = 0.010655886493623257 + 0.001 * 6.967545032501221
Epoch 620, val loss: 1.0216648578643799
Epoch 630, training loss: 0.017010953277349472 = 0.010040544904768467 + 0.001 * 6.9704084396362305
Epoch 630, val loss: 1.0298209190368652
Epoch 640, training loss: 0.01643930934369564 = 0.009474867023527622 + 0.001 * 6.964442253112793
Epoch 640, val loss: 1.037842869758606
Epoch 650, training loss: 0.015922103077173233 = 0.008954386226832867 + 0.001 * 6.96771764755249
Epoch 650, val loss: 1.0457184314727783
Epoch 660, training loss: 0.01544523611664772 = 0.008474880829453468 + 0.001 * 6.970355033874512
Epoch 660, val loss: 1.0534569025039673
Epoch 670, training loss: 0.015002195723354816 = 0.008032476529479027 + 0.001 * 6.969718933105469
Epoch 670, val loss: 1.0610432624816895
Epoch 680, training loss: 0.014586327597498894 = 0.007623633369803429 + 0.001 * 6.96269416809082
Epoch 680, val loss: 1.0684804916381836
Epoch 690, training loss: 0.01420038752257824 = 0.007245251443237066 + 0.001 * 6.955135822296143
Epoch 690, val loss: 1.0757856369018555
Epoch 700, training loss: 0.013847912661731243 = 0.006894540507346392 + 0.001 * 6.953372001647949
Epoch 700, val loss: 1.0829520225524902
Epoch 710, training loss: 0.013533766381442547 = 0.006568383425474167 + 0.001 * 6.9653825759887695
Epoch 710, val loss: 1.089982271194458
Epoch 720, training loss: 0.013214804232120514 = 0.006265393458306789 + 0.001 * 6.9494099617004395
Epoch 720, val loss: 1.096859097480774
Epoch 730, training loss: 0.012941936030983925 = 0.005982963368296623 + 0.001 * 6.958971977233887
Epoch 730, val loss: 1.1036403179168701
Epoch 740, training loss: 0.012666557915508747 = 0.005719425622373819 + 0.001 * 6.947132110595703
Epoch 740, val loss: 1.1102659702301025
Epoch 750, training loss: 0.012417912483215332 = 0.005473070777952671 + 0.001 * 6.9448418617248535
Epoch 750, val loss: 1.116769790649414
Epoch 760, training loss: 0.012182716280221939 = 0.005242502316832542 + 0.001 * 6.940214157104492
Epoch 760, val loss: 1.1231547594070435
Epoch 770, training loss: 0.011983947828412056 = 0.005026348866522312 + 0.001 * 6.9575982093811035
Epoch 770, val loss: 1.1294233798980713
Epoch 780, training loss: 0.011762483045458794 = 0.004823633469641209 + 0.001 * 6.938848972320557
Epoch 780, val loss: 1.1355433464050293
Epoch 790, training loss: 0.011595455929636955 = 0.004633238073438406 + 0.001 * 6.962217330932617
Epoch 790, val loss: 1.1415678262710571
Epoch 800, training loss: 0.01139133982360363 = 0.004454282112419605 + 0.001 * 6.937057971954346
Epoch 800, val loss: 1.1474727392196655
Epoch 810, training loss: 0.011217952705919743 = 0.004285913892090321 + 0.001 * 6.932038307189941
Epoch 810, val loss: 1.1532663106918335
Epoch 820, training loss: 0.011058392003178596 = 0.004127219319343567 + 0.001 * 6.931171894073486
Epoch 820, val loss: 1.1589349508285522
Epoch 830, training loss: 0.010925959795713425 = 0.003977550193667412 + 0.001 * 6.948409080505371
Epoch 830, val loss: 1.1645079851150513
Epoch 840, training loss: 0.01076323352754116 = 0.00383626576513052 + 0.001 * 6.926968097686768
Epoch 840, val loss: 1.1699622869491577
Epoch 850, training loss: 0.010629133321344852 = 0.0037028745282441378 + 0.001 * 6.926258087158203
Epoch 850, val loss: 1.1753153800964355
Epoch 860, training loss: 0.010493464767932892 = 0.003576788818463683 + 0.001 * 6.916676044464111
Epoch 860, val loss: 1.1805633306503296
Epoch 870, training loss: 0.010370357893407345 = 0.003457443555817008 + 0.001 * 6.912913799285889
Epoch 870, val loss: 1.185708999633789
Epoch 880, training loss: 0.010260804556310177 = 0.0033444708678871393 + 0.001 * 6.9163336753845215
Epoch 880, val loss: 1.1907503604888916
Epoch 890, training loss: 0.010156681761145592 = 0.0032374646980315447 + 0.001 * 6.919217109680176
Epoch 890, val loss: 1.195699691772461
Epoch 900, training loss: 0.010044151917099953 = 0.003135921433568001 + 0.001 * 6.908230781555176
Epoch 900, val loss: 1.2005640268325806
Epoch 910, training loss: 0.009942352771759033 = 0.0030396035872399807 + 0.001 * 6.9027485847473145
Epoch 910, val loss: 1.2053332328796387
Epoch 920, training loss: 0.009904537349939346 = 0.0029481262899935246 + 0.001 * 6.956411361694336
Epoch 920, val loss: 1.210016131401062
Epoch 930, training loss: 0.009754114784300327 = 0.002861249726265669 + 0.001 * 6.89286470413208
Epoch 930, val loss: 1.2145963907241821
Epoch 940, training loss: 0.009677558206021786 = 0.002778657479211688 + 0.001 * 6.898900032043457
Epoch 940, val loss: 1.2190752029418945
Epoch 950, training loss: 0.009595798328518867 = 0.002700026147067547 + 0.001 * 6.8957719802856445
Epoch 950, val loss: 1.2234925031661987
Epoch 960, training loss: 0.009517918340861797 = 0.002625085646286607 + 0.001 * 6.8928327560424805
Epoch 960, val loss: 1.2278143167495728
Epoch 970, training loss: 0.009445744566619396 = 0.002553621307015419 + 0.001 * 6.892122745513916
Epoch 970, val loss: 1.232055425643921
Epoch 980, training loss: 0.00939328595995903 = 0.0024853914510458708 + 0.001 * 6.907894134521484
Epoch 980, val loss: 1.2362558841705322
Epoch 990, training loss: 0.00931181013584137 = 0.002420139266178012 + 0.001 * 6.891670227050781
Epoch 990, val loss: 1.2403182983398438
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.8339
Flip ASR: 0.8000/225 nodes
The final ASR:0.68020, 0.10923, Accuracy:0.81481, 0.01600
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11596])
remove edge: torch.Size([2, 9452])
updated graph: torch.Size([2, 10492])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98155, 0.00797, Accuracy:0.82963, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9633983373641968 = 1.9550244808197021 + 0.001 * 8.373880386352539
Epoch 0, val loss: 1.9525858163833618
Epoch 10, training loss: 1.9529516696929932 = 1.9445778131484985 + 0.001 * 8.373804092407227
Epoch 10, val loss: 1.9422731399536133
Epoch 20, training loss: 1.9402612447738647 = 1.9318876266479492 + 0.001 * 8.373598098754883
Epoch 20, val loss: 1.9295588731765747
Epoch 30, training loss: 1.9225819110870361 = 1.9142087697982788 + 0.001 * 8.373167037963867
Epoch 30, val loss: 1.9119877815246582
Epoch 40, training loss: 1.8966768980026245 = 1.8883047103881836 + 0.001 * 8.37220287322998
Epoch 40, val loss: 1.8868117332458496
Epoch 50, training loss: 1.8601353168487549 = 1.8517656326293945 + 0.001 * 8.369664192199707
Epoch 50, val loss: 1.8530893325805664
Epoch 60, training loss: 1.816194772720337 = 1.8078337907791138 + 0.001 * 8.360968589782715
Epoch 60, val loss: 1.8162871599197388
Epoch 70, training loss: 1.774585247039795 = 1.7662688493728638 + 0.001 * 8.316429138183594
Epoch 70, val loss: 1.7849777936935425
Epoch 80, training loss: 1.7280299663543701 = 1.7199270725250244 + 0.001 * 8.102910995483398
Epoch 80, val loss: 1.7452911138534546
Epoch 90, training loss: 1.664093017578125 = 1.656351089477539 + 0.001 * 7.7419843673706055
Epoch 90, val loss: 1.6877174377441406
Epoch 100, training loss: 1.5795612335205078 = 1.5721808671951294 + 0.001 * 7.380329608917236
Epoch 100, val loss: 1.614686369895935
Epoch 110, training loss: 1.475709080696106 = 1.4684789180755615 + 0.001 * 7.230149269104004
Epoch 110, val loss: 1.5284521579742432
Epoch 120, training loss: 1.3634421825408936 = 1.3562918901443481 + 0.001 * 7.150252342224121
Epoch 120, val loss: 1.4359383583068848
Epoch 130, training loss: 1.2512619495391846 = 1.244197964668274 + 0.001 * 7.064001560211182
Epoch 130, val loss: 1.344377875328064
Epoch 140, training loss: 1.1436232328414917 = 1.1366132497787476 + 0.001 * 7.009974479675293
Epoch 140, val loss: 1.2572388648986816
Epoch 150, training loss: 1.0432963371276855 = 1.0363051891326904 + 0.001 * 6.991134166717529
Epoch 150, val loss: 1.1769766807556152
Epoch 160, training loss: 0.9508364200592041 = 0.943854808807373 + 0.001 * 6.981594085693359
Epoch 160, val loss: 1.1040258407592773
Epoch 170, training loss: 0.8648885488510132 = 0.8579153418540955 + 0.001 * 6.9732208251953125
Epoch 170, val loss: 1.0373423099517822
Epoch 180, training loss: 0.7844106554985046 = 0.7774434089660645 + 0.001 * 6.967222690582275
Epoch 180, val loss: 0.9763913750648499
Epoch 190, training loss: 0.7088358998298645 = 0.7018726468086243 + 0.001 * 6.963232517242432
Epoch 190, val loss: 0.9206830859184265
Epoch 200, training loss: 0.6379832029342651 = 0.6310228705406189 + 0.001 * 6.960341453552246
Epoch 200, val loss: 0.8701369166374207
Epoch 210, training loss: 0.5720685124397278 = 0.5651106238365173 + 0.001 * 6.957860469818115
Epoch 210, val loss: 0.8250312805175781
Epoch 220, training loss: 0.5115417242050171 = 0.5045859813690186 + 0.001 * 6.955759525299072
Epoch 220, val loss: 0.7862401008605957
Epoch 230, training loss: 0.4564906358718872 = 0.4495365619659424 + 0.001 * 6.954076290130615
Epoch 230, val loss: 0.7543776631355286
Epoch 240, training loss: 0.4066515266895294 = 0.39969873428344727 + 0.001 * 6.952804088592529
Epoch 240, val loss: 0.7293378114700317
Epoch 250, training loss: 0.36153680086135864 = 0.35458481311798096 + 0.001 * 6.951999664306641
Epoch 250, val loss: 0.7103214263916016
Epoch 260, training loss: 0.32054588198661804 = 0.3135946989059448 + 0.001 * 6.951186180114746
Epoch 260, val loss: 0.6961153745651245
Epoch 270, training loss: 0.2832397222518921 = 0.2762889862060547 + 0.001 * 6.950730800628662
Epoch 270, val loss: 0.6857656240463257
Epoch 280, training loss: 0.249357670545578 = 0.24240712821483612 + 0.001 * 6.950540065765381
Epoch 280, val loss: 0.6790432333946228
Epoch 290, training loss: 0.2188461720943451 = 0.21189555525779724 + 0.001 * 6.950618267059326
Epoch 290, val loss: 0.6756729483604431
Epoch 300, training loss: 0.1917533427476883 = 0.18480224907398224 + 0.001 * 6.951096057891846
Epoch 300, val loss: 0.6755877137184143
Epoch 310, training loss: 0.16803321242332458 = 0.16108150780200958 + 0.001 * 6.9517083168029785
Epoch 310, val loss: 0.6786319017410278
Epoch 320, training loss: 0.1474917232990265 = 0.14053912460803986 + 0.001 * 6.952591419219971
Epoch 320, val loss: 0.6840829849243164
Epoch 330, training loss: 0.12980203330516815 = 0.12284841388463974 + 0.001 * 6.953621864318848
Epoch 330, val loss: 0.6916095614433289
Epoch 340, training loss: 0.11461207270622253 = 0.10765731334686279 + 0.001 * 6.954758167266846
Epoch 340, val loss: 0.7006619572639465
Epoch 350, training loss: 0.101586252450943 = 0.09463053941726685 + 0.001 * 6.955708980560303
Epoch 350, val loss: 0.710748016834259
Epoch 360, training loss: 0.09042640030384064 = 0.08346986770629883 + 0.001 * 6.9565348625183105
Epoch 360, val loss: 0.7214565277099609
Epoch 370, training loss: 0.08087015897035599 = 0.07391241937875748 + 0.001 * 6.957742691040039
Epoch 370, val loss: 0.7323902249336243
Epoch 380, training loss: 0.07267703115940094 = 0.06571845710277557 + 0.001 * 6.958570957183838
Epoch 380, val loss: 0.743242621421814
Epoch 390, training loss: 0.06563710421323776 = 0.058678217232227325 + 0.001 * 6.958887100219727
Epoch 390, val loss: 0.7539550065994263
Epoch 400, training loss: 0.05956178531050682 = 0.05260218307375908 + 0.001 * 6.959603309631348
Epoch 400, val loss: 0.7643271684646606
Epoch 410, training loss: 0.05429387092590332 = 0.047334007918834686 + 0.001 * 6.9598612785339355
Epoch 410, val loss: 0.7743437886238098
Epoch 420, training loss: 0.04970626160502434 = 0.04274648055434227 + 0.001 * 6.959781646728516
Epoch 420, val loss: 0.7840317487716675
Epoch 430, training loss: 0.04569070786237717 = 0.03872823342680931 + 0.001 * 6.962472438812256
Epoch 430, val loss: 0.7934595346450806
Epoch 440, training loss: 0.042145024985075 = 0.035185106098651886 + 0.001 * 6.959917068481445
Epoch 440, val loss: 0.802635669708252
Epoch 450, training loss: 0.03900544345378876 = 0.03204711526632309 + 0.001 * 6.958327293395996
Epoch 450, val loss: 0.811575710773468
Epoch 460, training loss: 0.03622500225901604 = 0.029260678216814995 + 0.001 * 6.964323043823242
Epoch 460, val loss: 0.8202443718910217
Epoch 470, training loss: 0.03374643623828888 = 0.0267892275005579 + 0.001 * 6.957208633422852
Epoch 470, val loss: 0.828709602355957
Epoch 480, training loss: 0.031544387340545654 = 0.024586888030171394 + 0.001 * 6.957499980926514
Epoch 480, val loss: 0.837083637714386
Epoch 490, training loss: 0.02958729863166809 = 0.02262652851641178 + 0.001 * 6.960770130157471
Epoch 490, val loss: 0.8452933430671692
Epoch 500, training loss: 0.027828997001051903 = 0.020874690264463425 + 0.001 * 6.954306602478027
Epoch 500, val loss: 0.8533418774604797
Epoch 510, training loss: 0.026264633983373642 = 0.019306998699903488 + 0.001 * 6.957635879516602
Epoch 510, val loss: 0.8611793518066406
Epoch 520, training loss: 0.024851663038134575 = 0.01790088601410389 + 0.001 * 6.950777053833008
Epoch 520, val loss: 0.8687490224838257
Epoch 530, training loss: 0.023586532101035118 = 0.016636930406093597 + 0.001 * 6.949600696563721
Epoch 530, val loss: 0.8761928677558899
Epoch 540, training loss: 0.022479034960269928 = 0.015497682616114616 + 0.001 * 6.981351852416992
Epoch 540, val loss: 0.8834316730499268
Epoch 550, training loss: 0.02141844853758812 = 0.014468340203166008 + 0.001 * 6.950108051300049
Epoch 550, val loss: 0.8904834985733032
Epoch 560, training loss: 0.020474866032600403 = 0.013536541722714901 + 0.001 * 6.938323497772217
Epoch 560, val loss: 0.8974194526672363
Epoch 570, training loss: 0.019625436514616013 = 0.012691140174865723 + 0.001 * 6.934296607971191
Epoch 570, val loss: 0.9041898250579834
Epoch 580, training loss: 0.018852191045880318 = 0.011922362260520458 + 0.001 * 6.929828643798828
Epoch 580, val loss: 0.9107607007026672
Epoch 590, training loss: 0.01817990280687809 = 0.01122189313173294 + 0.001 * 6.958008766174316
Epoch 590, val loss: 0.9172099232673645
Epoch 600, training loss: 0.017524976283311844 = 0.010582762770354748 + 0.001 * 6.9422125816345215
Epoch 600, val loss: 0.9234700202941895
Epoch 610, training loss: 0.016938326880335808 = 0.009997767396271229 + 0.001 * 6.940558910369873
Epoch 610, val loss: 0.9296656847000122
Epoch 620, training loss: 0.016404500231146812 = 0.00946116354316473 + 0.001 * 6.943336486816406
Epoch 620, val loss: 0.9356927871704102
Epoch 630, training loss: 0.015883658081293106 = 0.008968021720647812 + 0.001 * 6.915635585784912
Epoch 630, val loss: 0.9416018724441528
Epoch 640, training loss: 0.015423900447785854 = 0.008513935841619968 + 0.001 * 6.909964084625244
Epoch 640, val loss: 0.9473690390586853
Epoch 650, training loss: 0.015003163367509842 = 0.008094971999526024 + 0.001 * 6.908190727233887
Epoch 650, val loss: 0.9529973864555359
Epoch 660, training loss: 0.014607205986976624 = 0.007707703392952681 + 0.001 * 6.899501800537109
Epoch 660, val loss: 0.9585202932357788
Epoch 670, training loss: 0.014245860278606415 = 0.007349077146500349 + 0.001 * 6.896782398223877
Epoch 670, val loss: 0.9639007449150085
Epoch 680, training loss: 0.013903331011533737 = 0.007016381248831749 + 0.001 * 6.88694953918457
Epoch 680, val loss: 0.9691670536994934
Epoch 690, training loss: 0.01355831790715456 = 0.006707262713462114 + 0.001 * 6.851054668426514
Epoch 690, val loss: 0.9743390083312988
Epoch 700, training loss: 0.013274592347443104 = 0.0064195627346634865 + 0.001 * 6.855029106140137
Epoch 700, val loss: 0.9794223308563232
Epoch 710, training loss: 0.013020668178796768 = 0.006151361856609583 + 0.001 * 6.8693060874938965
Epoch 710, val loss: 0.9843733906745911
Epoch 720, training loss: 0.012769294902682304 = 0.005900937132537365 + 0.001 * 6.868357181549072
Epoch 720, val loss: 0.9892555475234985
Epoch 730, training loss: 0.012515026144683361 = 0.005666822195053101 + 0.001 * 6.848203659057617
Epoch 730, val loss: 0.9939558506011963
Epoch 740, training loss: 0.01231758389621973 = 0.005447639152407646 + 0.001 * 6.8699445724487305
Epoch 740, val loss: 0.9986278414726257
Epoch 750, training loss: 0.012069977819919586 = 0.005242111161351204 + 0.001 * 6.827866077423096
Epoch 750, val loss: 1.0032105445861816
Epoch 760, training loss: 0.011876789852976799 = 0.0050491211004555225 + 0.001 * 6.827668190002441
Epoch 760, val loss: 1.0077017545700073
Epoch 770, training loss: 0.011720189824700356 = 0.004867677576839924 + 0.001 * 6.852512359619141
Epoch 770, val loss: 1.0120348930358887
Epoch 780, training loss: 0.011527773924171925 = 0.004696929361671209 + 0.001 * 6.830844402313232
Epoch 780, val loss: 1.0163366794586182
Epoch 790, training loss: 0.011368071660399437 = 0.004535996820777655 + 0.001 * 6.8320746421813965
Epoch 790, val loss: 1.0205657482147217
Epoch 800, training loss: 0.01121748611330986 = 0.004384116735309362 + 0.001 * 6.833369731903076
Epoch 800, val loss: 1.0246670246124268
Epoch 810, training loss: 0.011055322363972664 = 0.004240579903125763 + 0.001 * 6.814742565155029
Epoch 810, val loss: 1.0287127494812012
Epoch 820, training loss: 0.010934903286397457 = 0.004104792606085539 + 0.001 * 6.830110549926758
Epoch 820, val loss: 1.0326935052871704
Epoch 830, training loss: 0.010782846249639988 = 0.003976145293563604 + 0.001 * 6.806700706481934
Epoch 830, val loss: 1.0365967750549316
Epoch 840, training loss: 0.01067616418004036 = 0.0038541844114661217 + 0.001 * 6.821979522705078
Epoch 840, val loss: 1.0404112339019775
Epoch 850, training loss: 0.010600348934531212 = 0.003738313913345337 + 0.001 * 6.862034797668457
Epoch 850, val loss: 1.0442311763763428
Epoch 860, training loss: 0.010430753231048584 = 0.0036282276269048452 + 0.001 * 6.802525520324707
Epoch 860, val loss: 1.047981858253479
Epoch 870, training loss: 0.010345706716179848 = 0.0035235430113971233 + 0.001 * 6.822164058685303
Epoch 870, val loss: 1.0516458749771118
Epoch 880, training loss: 0.010234927758574486 = 0.0034240400418639183 + 0.001 * 6.810886859893799
Epoch 880, val loss: 1.0552641153335571
Epoch 890, training loss: 0.01012408547103405 = 0.0033289040438830853 + 0.001 * 6.7951812744140625
Epoch 890, val loss: 1.0588165521621704
Epoch 900, training loss: 0.010066780261695385 = 0.003238468198105693 + 0.001 * 6.828311443328857
Epoch 900, val loss: 1.062307596206665
Epoch 910, training loss: 0.009959608316421509 = 0.0031521478667855263 + 0.001 * 6.807459831237793
Epoch 910, val loss: 1.0657552480697632
Epoch 920, training loss: 0.009872098453342915 = 0.003069473197683692 + 0.001 * 6.8026251792907715
Epoch 920, val loss: 1.0691139698028564
Epoch 930, training loss: 0.009781653061509132 = 0.0029907163698226213 + 0.001 * 6.79093599319458
Epoch 930, val loss: 1.0724077224731445
Epoch 940, training loss: 0.00970421265810728 = 0.0029151509515941143 + 0.001 * 6.789061546325684
Epoch 940, val loss: 1.0756522417068481
Epoch 950, training loss: 0.009646185673773289 = 0.0028430093079805374 + 0.001 * 6.803175926208496
Epoch 950, val loss: 1.0788955688476562
Epoch 960, training loss: 0.00957291666418314 = 0.0027736262418329716 + 0.001 * 6.799290180206299
Epoch 960, val loss: 1.0820212364196777
Epoch 970, training loss: 0.00949227437376976 = 0.0027073356322944164 + 0.001 * 6.784937858581543
Epoch 970, val loss: 1.085129737854004
Epoch 980, training loss: 0.009452737867832184 = 0.002643299987539649 + 0.001 * 6.809437274932861
Epoch 980, val loss: 1.0882208347320557
Epoch 990, training loss: 0.009371093474328518 = 0.0025814808905124664 + 0.001 * 6.789612293243408
Epoch 990, val loss: 1.0912667512893677
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7232
Flip ASR: 0.6711/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9714782238006592 = 1.9631043672561646 + 0.001 * 8.373833656311035
Epoch 0, val loss: 1.969605565071106
Epoch 10, training loss: 1.9606618881225586 = 1.9522881507873535 + 0.001 * 8.37373161315918
Epoch 10, val loss: 1.958927869796753
Epoch 20, training loss: 1.9472180604934692 = 1.9388445615768433 + 0.001 * 8.373475074768066
Epoch 20, val loss: 1.945327877998352
Epoch 30, training loss: 1.9281365871429443 = 1.9197635650634766 + 0.001 * 8.372989654541016
Epoch 30, val loss: 1.9256445169448853
Epoch 40, training loss: 1.899776816368103 = 1.8914048671722412 + 0.001 * 8.371966361999512
Epoch 40, val loss: 1.896336555480957
Epoch 50, training loss: 1.8597419261932373 = 1.851372480392456 + 0.001 * 8.369429588317871
Epoch 50, val loss: 1.8562592267990112
Epoch 60, training loss: 1.813241958618164 = 1.8048810958862305 + 0.001 * 8.360862731933594
Epoch 60, val loss: 1.8129602670669556
Epoch 70, training loss: 1.771450400352478 = 1.7631324529647827 + 0.001 * 8.31794261932373
Epoch 70, val loss: 1.7758240699768066
Epoch 80, training loss: 1.7226734161376953 = 1.7146127223968506 + 0.001 * 8.060648918151855
Epoch 80, val loss: 1.7304046154022217
Epoch 90, training loss: 1.6558871269226074 = 1.6481611728668213 + 0.001 * 7.725976467132568
Epoch 90, val loss: 1.6712428331375122
Epoch 100, training loss: 1.5686686038970947 = 1.5610684156417847 + 0.001 * 7.600161552429199
Epoch 100, val loss: 1.597517490386963
Epoch 110, training loss: 1.4692981243133545 = 1.461836576461792 + 0.001 * 7.4615583419799805
Epoch 110, val loss: 1.5152876377105713
Epoch 120, training loss: 1.3716615438461304 = 1.3644753694534302 + 0.001 * 7.186143398284912
Epoch 120, val loss: 1.4378005266189575
Epoch 130, training loss: 1.2811336517333984 = 1.2740060091018677 + 0.001 * 7.127604961395264
Epoch 130, val loss: 1.3691213130950928
Epoch 140, training loss: 1.1970869302749634 = 1.1900001764297485 + 0.001 * 7.086787223815918
Epoch 140, val loss: 1.3083223104476929
Epoch 150, training loss: 1.1181336641311646 = 1.111061930656433 + 0.001 * 7.071735382080078
Epoch 150, val loss: 1.2528246641159058
Epoch 160, training loss: 1.0431311130523682 = 1.0360746383666992 + 0.001 * 7.056467056274414
Epoch 160, val loss: 1.200437068939209
Epoch 170, training loss: 0.971602201461792 = 0.9645660519599915 + 0.001 * 7.0361328125
Epoch 170, val loss: 1.1510511636734009
Epoch 180, training loss: 0.9035372734069824 = 0.8965222835540771 + 0.001 * 7.014981269836426
Epoch 180, val loss: 1.1045761108398438
Epoch 190, training loss: 0.8383551239967346 = 0.8313633799552917 + 0.001 * 6.99174165725708
Epoch 190, val loss: 1.0599942207336426
Epoch 200, training loss: 0.7750149369239807 = 0.768050491809845 + 0.001 * 6.964462757110596
Epoch 200, val loss: 1.016244888305664
Epoch 210, training loss: 0.713145911693573 = 0.7062127590179443 + 0.001 * 6.933163642883301
Epoch 210, val loss: 0.9735224843025208
Epoch 220, training loss: 0.653170108795166 = 0.6462708115577698 + 0.001 * 6.899324417114258
Epoch 220, val loss: 0.932702362537384
Epoch 230, training loss: 0.5959054827690125 = 0.5890317559242249 + 0.001 * 6.873751640319824
Epoch 230, val loss: 0.8942656517028809
Epoch 240, training loss: 0.541965901851654 = 0.5351053476333618 + 0.001 * 6.860531330108643
Epoch 240, val loss: 0.8587761521339417
Epoch 250, training loss: 0.49145159125328064 = 0.4846033751964569 + 0.001 * 6.848207473754883
Epoch 250, val loss: 0.8260764479637146
Epoch 260, training loss: 0.4440017342567444 = 0.4371604919433594 + 0.001 * 6.84123420715332
Epoch 260, val loss: 0.7968473434448242
Epoch 270, training loss: 0.39920586347579956 = 0.3923723101615906 + 0.001 * 6.8335676193237305
Epoch 270, val loss: 0.7708384394645691
Epoch 280, training loss: 0.3571740984916687 = 0.35034453868865967 + 0.001 * 6.829562187194824
Epoch 280, val loss: 0.7483770251274109
Epoch 290, training loss: 0.3183344602584839 = 0.3115096986293793 + 0.001 * 6.824754238128662
Epoch 290, val loss: 0.7290828824043274
Epoch 300, training loss: 0.283171683549881 = 0.2763494849205017 + 0.001 * 6.822189807891846
Epoch 300, val loss: 0.7131875157356262
Epoch 310, training loss: 0.2517349421977997 = 0.24491438269615173 + 0.001 * 6.820559978485107
Epoch 310, val loss: 0.700710117816925
Epoch 320, training loss: 0.22369682788848877 = 0.21687644720077515 + 0.001 * 6.820380687713623
Epoch 320, val loss: 0.6918492913246155
Epoch 330, training loss: 0.19864939153194427 = 0.19183146953582764 + 0.001 * 6.817916393280029
Epoch 330, val loss: 0.6864166259765625
Epoch 340, training loss: 0.17623241245746613 = 0.1694147288799286 + 0.001 * 6.817682266235352
Epoch 340, val loss: 0.6839262843132019
Epoch 350, training loss: 0.1560952067375183 = 0.14927750825881958 + 0.001 * 6.8177008628845215
Epoch 350, val loss: 0.6842337250709534
Epoch 360, training loss: 0.1378728747367859 = 0.1310545802116394 + 0.001 * 6.818291664123535
Epoch 360, val loss: 0.6869494318962097
Epoch 370, training loss: 0.12123816460371017 = 0.11442014575004578 + 0.001 * 6.818017482757568
Epoch 370, val loss: 0.6921946406364441
Epoch 380, training loss: 0.1059979796409607 = 0.09918004274368286 + 0.001 * 6.817935466766357
Epoch 380, val loss: 0.6993452310562134
Epoch 390, training loss: 0.09269759058952332 = 0.08587782829999924 + 0.001 * 6.819761753082275
Epoch 390, val loss: 0.7081295847892761
Epoch 400, training loss: 0.08188201487064362 = 0.07506322860717773 + 0.001 * 6.81878662109375
Epoch 400, val loss: 0.7191064953804016
Epoch 410, training loss: 0.0730224996805191 = 0.06620541960000992 + 0.001 * 6.817078590393066
Epoch 410, val loss: 0.7321986556053162
Epoch 420, training loss: 0.06554865837097168 = 0.05873104929924011 + 0.001 * 6.8176116943359375
Epoch 420, val loss: 0.7466234564781189
Epoch 430, training loss: 0.059156954288482666 = 0.05234150588512421 + 0.001 * 6.815446376800537
Epoch 430, val loss: 0.7619439363479614
Epoch 440, training loss: 0.053653284907341 = 0.04683828353881836 + 0.001 * 6.814999580383301
Epoch 440, val loss: 0.7777076363563538
Epoch 450, training loss: 0.048888593912124634 = 0.0420764796435833 + 0.001 * 6.812112331390381
Epoch 450, val loss: 0.7935802936553955
Epoch 460, training loss: 0.04475199431180954 = 0.0379401333630085 + 0.001 * 6.811862468719482
Epoch 460, val loss: 0.8094103932380676
Epoch 470, training loss: 0.04114336520433426 = 0.03433376923203468 + 0.001 * 6.809595584869385
Epoch 470, val loss: 0.8251450061798096
Epoch 480, training loss: 0.03798958286643028 = 0.031181838363409042 + 0.001 * 6.807743072509766
Epoch 480, val loss: 0.8406199812889099
Epoch 490, training loss: 0.03522314131259918 = 0.028421465307474136 + 0.001 * 6.8016743659973145
Epoch 490, val loss: 0.8557767271995544
Epoch 500, training loss: 0.032796461135149 = 0.02599846012890339 + 0.001 * 6.798001289367676
Epoch 500, val loss: 0.8705373406410217
Epoch 510, training loss: 0.030664298683404922 = 0.02386467531323433 + 0.001 * 6.799623012542725
Epoch 510, val loss: 0.8848652243614197
Epoch 520, training loss: 0.02877214550971985 = 0.02197919599711895 + 0.001 * 6.7929487228393555
Epoch 520, val loss: 0.8987798690795898
Epoch 530, training loss: 0.027132969349622726 = 0.020306967198848724 + 0.001 * 6.82600212097168
Epoch 530, val loss: 0.9122821092605591
Epoch 540, training loss: 0.025608059018850327 = 0.018818125128746033 + 0.001 * 6.789932727813721
Epoch 540, val loss: 0.9253119230270386
Epoch 550, training loss: 0.024272684007883072 = 0.017487460747361183 + 0.001 * 6.78522253036499
Epoch 550, val loss: 0.9378980994224548
Epoch 560, training loss: 0.023076249286532402 = 0.016293616965413094 + 0.001 * 6.7826313972473145
Epoch 560, val loss: 0.9501409530639648
Epoch 570, training loss: 0.021997539326548576 = 0.015219653025269508 + 0.001 * 6.777886390686035
Epoch 570, val loss: 0.9619984030723572
Epoch 580, training loss: 0.021026425063610077 = 0.014250047504901886 + 0.001 * 6.776376247406006
Epoch 580, val loss: 0.973505437374115
Epoch 590, training loss: 0.02016105316579342 = 0.013372189365327358 + 0.001 * 6.788863182067871
Epoch 590, val loss: 0.984707772731781
Epoch 600, training loss: 0.019352763891220093 = 0.01257476955652237 + 0.001 * 6.777993202209473
Epoch 600, val loss: 0.9955864548683167
Epoch 610, training loss: 0.018638087436556816 = 0.011848289519548416 + 0.001 * 6.789797782897949
Epoch 610, val loss: 1.0061708688735962
Epoch 620, training loss: 0.017945677042007446 = 0.011184556409716606 + 0.001 * 6.7611212730407715
Epoch 620, val loss: 1.0164517164230347
Epoch 630, training loss: 0.017336074262857437 = 0.010576500557363033 + 0.001 * 6.759573459625244
Epoch 630, val loss: 1.0264836549758911
Epoch 640, training loss: 0.016784731298685074 = 0.010018311440944672 + 0.001 * 6.766419887542725
Epoch 640, val loss: 1.0362653732299805
Epoch 650, training loss: 0.016304440796375275 = 0.009504510089755058 + 0.001 * 6.799931049346924
Epoch 650, val loss: 1.0457960367202759
Epoch 660, training loss: 0.015787160024046898 = 0.009030082263052464 + 0.001 * 6.757077693939209
Epoch 660, val loss: 1.0551105737686157
Epoch 670, training loss: 0.015357389114797115 = 0.008591040037572384 + 0.001 * 6.766348838806152
Epoch 670, val loss: 1.0641769170761108
Epoch 680, training loss: 0.014935672283172607 = 0.008183577097952366 + 0.001 * 6.7520952224731445
Epoch 680, val loss: 1.0730491876602173
Epoch 690, training loss: 0.014564314857125282 = 0.007804548367857933 + 0.001 * 6.759766101837158
Epoch 690, val loss: 1.0817217826843262
Epoch 700, training loss: 0.01420817244797945 = 0.007451203186064959 + 0.001 * 6.756968975067139
Epoch 700, val loss: 1.0902198553085327
Epoch 710, training loss: 0.013882627710700035 = 0.007121062371879816 + 0.001 * 6.7615647315979
Epoch 710, val loss: 1.0985463857650757
Epoch 720, training loss: 0.013564259745180607 = 0.006811897270381451 + 0.001 * 6.752362251281738
Epoch 720, val loss: 1.1066631078720093
Epoch 730, training loss: 0.013278204947710037 = 0.006522046402096748 + 0.001 * 6.756158351898193
Epoch 730, val loss: 1.1146575212478638
Epoch 740, training loss: 0.013001373037695885 = 0.006249980069696903 + 0.001 * 6.7513933181762695
Epoch 740, val loss: 1.1224809885025024
Epoch 750, training loss: 0.012737440876662731 = 0.0059943669475615025 + 0.001 * 6.743073463439941
Epoch 750, val loss: 1.1301391124725342
Epoch 760, training loss: 0.012515278533101082 = 0.005753908772021532 + 0.001 * 6.761368751525879
Epoch 760, val loss: 1.1376752853393555
Epoch 770, training loss: 0.012268366292119026 = 0.005527276545763016 + 0.001 * 6.7410888671875
Epoch 770, val loss: 1.1450656652450562
Epoch 780, training loss: 0.012059534899890423 = 0.005312726832926273 + 0.001 * 6.74680757522583
Epoch 780, val loss: 1.1523900032043457
Epoch 790, training loss: 0.011852893978357315 = 0.005108624696731567 + 0.001 * 6.744269371032715
Epoch 790, val loss: 1.1596460342407227
Epoch 800, training loss: 0.011666023172438145 = 0.0049134548753499985 + 0.001 * 6.752567768096924
Epoch 800, val loss: 1.1668850183486938
Epoch 810, training loss: 0.01146866474300623 = 0.004726136568933725 + 0.001 * 6.742527961730957
Epoch 810, val loss: 1.1742388010025024
Epoch 820, training loss: 0.01129818707704544 = 0.004546348471194506 + 0.001 * 6.751837730407715
Epoch 820, val loss: 1.1815893650054932
Epoch 830, training loss: 0.011118536815047264 = 0.004373990930616856 + 0.001 * 6.744545936584473
Epoch 830, val loss: 1.188997745513916
Epoch 840, training loss: 0.0109429731965065 = 0.004209192469716072 + 0.001 * 6.733780860900879
Epoch 840, val loss: 1.1964267492294312
Epoch 850, training loss: 0.01079199742525816 = 0.004051848314702511 + 0.001 * 6.740149021148682
Epoch 850, val loss: 1.2038472890853882
Epoch 860, training loss: 0.010633694007992744 = 0.0039019640535116196 + 0.001 * 6.731729984283447
Epoch 860, val loss: 1.2112112045288086
Epoch 870, training loss: 0.010513939894735813 = 0.003759385086596012 + 0.001 * 6.754554271697998
Epoch 870, val loss: 1.21853768825531
Epoch 880, training loss: 0.010357387363910675 = 0.0036240159533917904 + 0.001 * 6.733370780944824
Epoch 880, val loss: 1.2258086204528809
Epoch 890, training loss: 0.010222569108009338 = 0.0034953951835632324 + 0.001 * 6.727173805236816
Epoch 890, val loss: 1.2329946756362915
Epoch 900, training loss: 0.010110365226864815 = 0.0033732925076037645 + 0.001 * 6.737072467803955
Epoch 900, val loss: 1.2400741577148438
Epoch 910, training loss: 0.010001464746892452 = 0.0032573225907981396 + 0.001 * 6.744142055511475
Epoch 910, val loss: 1.247063159942627
Epoch 920, training loss: 0.009881488047540188 = 0.0031471550464630127 + 0.001 * 6.73433256149292
Epoch 920, val loss: 1.254008412361145
Epoch 930, training loss: 0.00977237056940794 = 0.003042156109586358 + 0.001 * 6.730214595794678
Epoch 930, val loss: 1.260818362236023
Epoch 940, training loss: 0.009680637158453465 = 0.002942474791780114 + 0.001 * 6.738162040710449
Epoch 940, val loss: 1.2675269842147827
Epoch 950, training loss: 0.009579440578818321 = 0.0028477320447564125 + 0.001 * 6.73170804977417
Epoch 950, val loss: 1.2740838527679443
Epoch 960, training loss: 0.009486807510256767 = 0.002757496200501919 + 0.001 * 6.729310512542725
Epoch 960, val loss: 1.2805802822113037
Epoch 970, training loss: 0.009393002837896347 = 0.0026715793646872044 + 0.001 * 6.721423625946045
Epoch 970, val loss: 1.2869404554367065
Epoch 980, training loss: 0.009307242929935455 = 0.0025896725710481405 + 0.001 * 6.7175703048706055
Epoch 980, val loss: 1.2932170629501343
Epoch 990, training loss: 0.009232498705387115 = 0.0025116028264164925 + 0.001 * 6.720896244049072
Epoch 990, val loss: 1.2993230819702148
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.6494
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9495357275009155 = 1.941161870956421 + 0.001 * 8.373849868774414
Epoch 0, val loss: 1.9479055404663086
Epoch 10, training loss: 1.9393577575683594 = 1.9309840202331543 + 0.001 * 8.373766899108887
Epoch 10, val loss: 1.9372502565383911
Epoch 20, training loss: 1.9264992475509644 = 1.9181257486343384 + 0.001 * 8.373534202575684
Epoch 20, val loss: 1.9236352443695068
Epoch 30, training loss: 1.9084079265594482 = 1.9000349044799805 + 0.001 * 8.373052597045898
Epoch 30, val loss: 1.9045274257659912
Epoch 40, training loss: 1.8820090293884277 = 1.873637080192566 + 0.001 * 8.37198543548584
Epoch 40, val loss: 1.8772454261779785
Epoch 50, training loss: 1.8458747863769531 = 1.837505578994751 + 0.001 * 8.369190216064453
Epoch 50, val loss: 1.8418561220169067
Epoch 60, training loss: 1.8052213191986084 = 1.796862244606018 + 0.001 * 8.3590669631958
Epoch 60, val loss: 1.8057565689086914
Epoch 70, training loss: 1.7666376829147339 = 1.7583311796188354 + 0.001 * 8.306478500366211
Epoch 70, val loss: 1.7733575105667114
Epoch 80, training loss: 1.716238021850586 = 1.7082148790359497 + 0.001 * 8.023113250732422
Epoch 80, val loss: 1.7287606000900269
Epoch 90, training loss: 1.6462737321853638 = 1.638481616973877 + 0.001 * 7.792103290557861
Epoch 90, val loss: 1.6683201789855957
Epoch 100, training loss: 1.5567421913146973 = 1.5490796566009521 + 0.001 * 7.662477970123291
Epoch 100, val loss: 1.594489574432373
Epoch 110, training loss: 1.4560550451278687 = 1.4486289024353027 + 0.001 * 7.426156044006348
Epoch 110, val loss: 1.5116668939590454
Epoch 120, training loss: 1.3527755737304688 = 1.3456718921661377 + 0.001 * 7.1037163734436035
Epoch 120, val loss: 1.425933837890625
Epoch 130, training loss: 1.2492618560791016 = 1.2421793937683105 + 0.001 * 7.082511901855469
Epoch 130, val loss: 1.3402148485183716
Epoch 140, training loss: 1.1457862854003906 = 1.1387428045272827 + 0.001 * 7.043494701385498
Epoch 140, val loss: 1.2559641599655151
Epoch 150, training loss: 1.0440926551818848 = 1.0370569229125977 + 0.001 * 7.035674095153809
Epoch 150, val loss: 1.1749447584152222
Epoch 160, training loss: 0.9467076063156128 = 0.9396753311157227 + 0.001 * 7.032290935516357
Epoch 160, val loss: 1.098372220993042
Epoch 170, training loss: 0.8554977178573608 = 0.8484705686569214 + 0.001 * 7.027167320251465
Epoch 170, val loss: 1.0277469158172607
Epoch 180, training loss: 0.7714242935180664 = 0.7644022703170776 + 0.001 * 7.022006988525391
Epoch 180, val loss: 0.9635302424430847
Epoch 190, training loss: 0.6951180100440979 = 0.688103973865509 + 0.001 * 7.014054298400879
Epoch 190, val loss: 0.9063590168952942
Epoch 200, training loss: 0.6270751953125 = 0.6200750470161438 + 0.001 * 7.000176429748535
Epoch 200, val loss: 0.856964111328125
Epoch 210, training loss: 0.5674280524253845 = 0.5604503750801086 + 0.001 * 6.977697849273682
Epoch 210, val loss: 0.8159685134887695
Epoch 220, training loss: 0.5157108306884766 = 0.5087624192237854 + 0.001 * 6.948397159576416
Epoch 220, val loss: 0.783205509185791
Epoch 230, training loss: 0.47075071930885315 = 0.4638241231441498 + 0.001 * 6.926596641540527
Epoch 230, val loss: 0.7579625248908997
Epoch 240, training loss: 0.4308006763458252 = 0.4238813817501068 + 0.001 * 6.919301986694336
Epoch 240, val loss: 0.7389302849769592
Epoch 250, training loss: 0.3939202129840851 = 0.38700541853904724 + 0.001 * 6.914796829223633
Epoch 250, val loss: 0.7248638272285461
Epoch 260, training loss: 0.35855063796043396 = 0.3516451120376587 + 0.001 * 6.905532360076904
Epoch 260, val loss: 0.7146444916725159
Epoch 270, training loss: 0.3238418400287628 = 0.316947340965271 + 0.001 * 6.894510269165039
Epoch 270, val loss: 0.7074982523918152
Epoch 280, training loss: 0.28963473439216614 = 0.28275322914123535 + 0.001 * 6.881505489349365
Epoch 280, val loss: 0.7029318809509277
Epoch 290, training loss: 0.25644686818122864 = 0.24957917630672455 + 0.001 * 6.86770486831665
Epoch 290, val loss: 0.7006456851959229
Epoch 300, training loss: 0.22520576417446136 = 0.21834975481033325 + 0.001 * 6.856003761291504
Epoch 300, val loss: 0.7006420493125916
Epoch 310, training loss: 0.19685763120651245 = 0.1900104284286499 + 0.001 * 6.84720516204834
Epoch 310, val loss: 0.7029783129692078
Epoch 320, training loss: 0.17193995416164398 = 0.16510075330734253 + 0.001 * 6.839198589324951
Epoch 320, val loss: 0.7076660990715027
Epoch 330, training loss: 0.15044789016246796 = 0.14361266791820526 + 0.001 * 6.8352155685424805
Epoch 330, val loss: 0.7142260670661926
Epoch 340, training loss: 0.13206127285957336 = 0.1252276599407196 + 0.001 * 6.833606719970703
Epoch 340, val loss: 0.7224333882331848
Epoch 350, training loss: 0.11633075028657913 = 0.10949945449829102 + 0.001 * 6.831292152404785
Epoch 350, val loss: 0.7319831252098083
Epoch 360, training loss: 0.1028270497918129 = 0.09599736332893372 + 0.001 * 6.829686164855957
Epoch 360, val loss: 0.7426424026489258
Epoch 370, training loss: 0.09119364619255066 = 0.08436408638954163 + 0.001 * 6.82956075668335
Epoch 370, val loss: 0.7542408108711243
Epoch 380, training loss: 0.08114438503980637 = 0.07431431859731674 + 0.001 * 6.8300652503967285
Epoch 380, val loss: 0.7664557099342346
Epoch 390, training loss: 0.07244952023029327 = 0.06561917066574097 + 0.001 * 6.830345630645752
Epoch 390, val loss: 0.7791587710380554
Epoch 400, training loss: 0.064922034740448 = 0.05809196084737778 + 0.001 * 6.830070972442627
Epoch 400, val loss: 0.7921485900878906
Epoch 410, training loss: 0.058413319289684296 = 0.05158267170190811 + 0.001 * 6.83064603805542
Epoch 410, val loss: 0.8052474856376648
Epoch 420, training loss: 0.052781373262405396 = 0.04595097154378891 + 0.001 * 6.83040189743042
Epoch 420, val loss: 0.8182806968688965
Epoch 430, training loss: 0.047909028828144073 = 0.0410783626139164 + 0.001 * 6.830665111541748
Epoch 430, val loss: 0.8312180042266846
Epoch 440, training loss: 0.043692369014024734 = 0.036861732602119446 + 0.001 * 6.8306355476379395
Epoch 440, val loss: 0.8440214991569519
Epoch 450, training loss: 0.04003667086362839 = 0.03320607542991638 + 0.001 * 6.830595970153809
Epoch 450, val loss: 0.8565474152565002
Epoch 460, training loss: 0.03686080873012543 = 0.030030621215701103 + 0.001 * 6.830185890197754
Epoch 460, val loss: 0.8688459992408752
Epoch 470, training loss: 0.03409682214260101 = 0.027265923097729683 + 0.001 * 6.830899238586426
Epoch 470, val loss: 0.880928099155426
Epoch 480, training loss: 0.0316813699901104 = 0.02485131286084652 + 0.001 * 6.830056667327881
Epoch 480, val loss: 0.8926685452461243
Epoch 490, training loss: 0.029563749209046364 = 0.02273431420326233 + 0.001 * 6.829434394836426
Epoch 490, val loss: 0.9041245579719543
Epoch 500, training loss: 0.027704093605279922 = 0.02087096869945526 + 0.001 * 6.833123683929443
Epoch 500, val loss: 0.9152888655662537
Epoch 510, training loss: 0.02605373226106167 = 0.019224679097533226 + 0.001 * 6.829052448272705
Epoch 510, val loss: 0.9261400103569031
Epoch 520, training loss: 0.024593938142061234 = 0.01776549406349659 + 0.001 * 6.828444004058838
Epoch 520, val loss: 0.9367455244064331
Epoch 530, training loss: 0.02329646423459053 = 0.016466805711388588 + 0.001 * 6.8296589851379395
Epoch 530, val loss: 0.9470556378364563
Epoch 540, training loss: 0.022134944796562195 = 0.015306293033063412 + 0.001 * 6.828652381896973
Epoch 540, val loss: 0.9571011066436768
Epoch 550, training loss: 0.021092426031827927 = 0.014265280216932297 + 0.001 * 6.827146053314209
Epoch 550, val loss: 0.9667816162109375
Epoch 560, training loss: 0.020154323428869247 = 0.01332840695977211 + 0.001 * 6.825916767120361
Epoch 560, val loss: 0.9762001037597656
Epoch 570, training loss: 0.019309714436531067 = 0.012482442893087864 + 0.001 * 6.827270984649658
Epoch 570, val loss: 0.9853664636611938
Epoch 580, training loss: 0.01854133978486061 = 0.011716420762240887 + 0.001 * 6.824918746948242
Epoch 580, val loss: 0.994316041469574
Epoch 590, training loss: 0.017845433205366135 = 0.011020668782293797 + 0.001 * 6.824764728546143
Epoch 590, val loss: 1.0030204057693481
Epoch 600, training loss: 0.017211735248565674 = 0.010386900044977665 + 0.001 * 6.824835300445557
Epoch 600, val loss: 1.0114078521728516
Epoch 610, training loss: 0.016634467989206314 = 0.009808026254177094 + 0.001 * 6.826441287994385
Epoch 610, val loss: 1.0196254253387451
Epoch 620, training loss: 0.016099952161312103 = 0.009277939796447754 + 0.001 * 6.822011470794678
Epoch 620, val loss: 1.0275875329971313
Epoch 630, training loss: 0.01561303623020649 = 0.008791341446340084 + 0.001 * 6.821694850921631
Epoch 630, val loss: 1.0353450775146484
Epoch 640, training loss: 0.015166671946644783 = 0.008343564346432686 + 0.001 * 6.823107719421387
Epoch 640, val loss: 1.042892336845398
Epoch 650, training loss: 0.0147507693618536 = 0.007930746302008629 + 0.001 * 6.820023059844971
Epoch 650, val loss: 1.0502488613128662
Epoch 660, training loss: 0.01436946727335453 = 0.0075493804179131985 + 0.001 * 6.82008695602417
Epoch 660, val loss: 1.057408332824707
Epoch 670, training loss: 0.014017445966601372 = 0.007196396589279175 + 0.001 * 6.821049690246582
Epoch 670, val loss: 1.0643821954727173
Epoch 680, training loss: 0.013687040656805038 = 0.006869027856737375 + 0.001 * 6.818012714385986
Epoch 680, val loss: 1.0711848735809326
Epoch 690, training loss: 0.013382412493228912 = 0.006564891431480646 + 0.001 * 6.817521095275879
Epoch 690, val loss: 1.0778288841247559
Epoch 700, training loss: 0.013100700452923775 = 0.006281830836087465 + 0.001 * 6.818868637084961
Epoch 700, val loss: 1.0843085050582886
Epoch 710, training loss: 0.012833881191909313 = 0.006017941515892744 + 0.001 * 6.815939426422119
Epoch 710, val loss: 1.0906267166137695
Epoch 720, training loss: 0.012590013444423676 = 0.005771538708359003 + 0.001 * 6.818473815917969
Epoch 720, val loss: 1.0967919826507568
Epoch 730, training loss: 0.012356862425804138 = 0.00554111460223794 + 0.001 * 6.815747261047363
Epoch 730, val loss: 1.1028085947036743
Epoch 740, training loss: 0.012141799554228783 = 0.005325311794877052 + 0.001 * 6.8164873123168945
Epoch 740, val loss: 1.1086987257003784
Epoch 750, training loss: 0.011933890171349049 = 0.005122943315654993 + 0.001 * 6.810946464538574
Epoch 750, val loss: 1.114443063735962
Epoch 760, training loss: 0.011746484786272049 = 0.004932923708111048 + 0.001 * 6.813560485839844
Epoch 760, val loss: 1.120065689086914
Epoch 770, training loss: 0.011563289910554886 = 0.004754244349896908 + 0.001 * 6.809045791625977
Epoch 770, val loss: 1.1255455017089844
Epoch 780, training loss: 0.011398162692785263 = 0.0045860521495342255 + 0.001 * 6.812110900878906
Epoch 780, val loss: 1.130926489830017
Epoch 790, training loss: 0.011236349120736122 = 0.004427524283528328 + 0.001 * 6.80882453918457
Epoch 790, val loss: 1.1361640691757202
Epoch 800, training loss: 0.011098140850663185 = 0.004277938976883888 + 0.001 * 6.820201396942139
Epoch 800, val loss: 1.1412723064422607
Epoch 810, training loss: 0.01094244234263897 = 0.004136629868298769 + 0.001 * 6.805812835693359
Epoch 810, val loss: 1.1462806463241577
Epoch 820, training loss: 0.010813350789248943 = 0.004002999514341354 + 0.001 * 6.8103508949279785
Epoch 820, val loss: 1.1511878967285156
Epoch 830, training loss: 0.01068192720413208 = 0.00387648306787014 + 0.001 * 6.805443286895752
Epoch 830, val loss: 1.1559853553771973
Epoch 840, training loss: 0.01055648922920227 = 0.003756531048566103 + 0.001 * 6.799957752227783
Epoch 840, val loss: 1.1606882810592651
Epoch 850, training loss: 0.010442220605909824 = 0.0036424887366592884 + 0.001 * 6.799731731414795
Epoch 850, val loss: 1.1653101444244385
Epoch 860, training loss: 0.010333452373743057 = 0.0035333256237208843 + 0.001 * 6.800126075744629
Epoch 860, val loss: 1.1699060201644897
Epoch 870, training loss: 0.010233842767775059 = 0.003427815856412053 + 0.001 * 6.806026458740234
Epoch 870, val loss: 1.1744507551193237
Epoch 880, training loss: 0.010124698281288147 = 0.003325096797198057 + 0.001 * 6.799600601196289
Epoch 880, val loss: 1.178897738456726
Epoch 890, training loss: 0.010023519396781921 = 0.0032248925417661667 + 0.001 * 6.798625946044922
Epoch 890, val loss: 1.1832432746887207
Epoch 900, training loss: 0.009946542792022228 = 0.003127357456833124 + 0.001 * 6.81918478012085
Epoch 900, val loss: 1.1874840259552002
Epoch 910, training loss: 0.009824538603425026 = 0.00303279934450984 + 0.001 * 6.791738986968994
Epoch 910, val loss: 1.191684603691101
Epoch 920, training loss: 0.009743261151015759 = 0.002941468730568886 + 0.001 * 6.801792144775391
Epoch 920, val loss: 1.195651888847351
Epoch 930, training loss: 0.00966528058052063 = 0.0028535560704767704 + 0.001 * 6.811724662780762
Epoch 930, val loss: 1.199580192565918
Epoch 940, training loss: 0.009555173106491566 = 0.0027691659051924944 + 0.001 * 6.786006927490234
Epoch 940, val loss: 1.2033090591430664
Epoch 950, training loss: 0.009477276355028152 = 0.002688315464183688 + 0.001 * 6.788960933685303
Epoch 950, val loss: 1.2069662809371948
Epoch 960, training loss: 0.009409558027982712 = 0.0026109812315553427 + 0.001 * 6.798576354980469
Epoch 960, val loss: 1.2105120420455933
Epoch 970, training loss: 0.009315467439591885 = 0.002537078456953168 + 0.001 * 6.778388500213623
Epoch 970, val loss: 1.2139943838119507
Epoch 980, training loss: 0.009297959506511688 = 0.0024665286764502525 + 0.001 * 6.831429958343506
Epoch 980, val loss: 1.217280626296997
Epoch 990, training loss: 0.00917849037796259 = 0.002399192424491048 + 0.001 * 6.779297828674316
Epoch 990, val loss: 1.220585823059082
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.9151
Flip ASR: 0.8978/225 nodes
The final ASR:0.76261, 0.11198, Accuracy:0.82099, 0.01397
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11552])
remove edge: torch.Size([2, 9418])
updated graph: torch.Size([2, 10414])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.83086, 0.00761
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9714800119400024 = 1.9631061553955078 + 0.001 * 8.373797416687012
Epoch 0, val loss: 1.9593127965927124
Epoch 10, training loss: 1.9611139297485352 = 1.95274019241333 + 0.001 * 8.373720169067383
Epoch 10, val loss: 1.9495468139648438
Epoch 20, training loss: 1.9486703872680664 = 1.9402968883514404 + 0.001 * 8.37346076965332
Epoch 20, val loss: 1.9374204874038696
Epoch 30, training loss: 1.9316198825836182 = 1.9232468605041504 + 0.001 * 8.372992515563965
Epoch 30, val loss: 1.9204260110855103
Epoch 40, training loss: 1.906521201133728 = 1.8981491327285767 + 0.001 * 8.372071266174316
Epoch 40, val loss: 1.8953495025634766
Epoch 50, training loss: 1.8696893453598022 = 1.861319661140442 + 0.001 * 8.36972713470459
Epoch 50, val loss: 1.859904408454895
Epoch 60, training loss: 1.823252558708191 = 1.814892053604126 + 0.001 * 8.360503196716309
Epoch 60, val loss: 1.8189970254898071
Epoch 70, training loss: 1.7798773050308228 = 1.7715786695480347 + 0.001 * 8.298677444458008
Epoch 70, val loss: 1.7857664823532104
Epoch 80, training loss: 1.7360188961029053 = 1.7281343936920166 + 0.001 * 7.884503364562988
Epoch 80, val loss: 1.7495179176330566
Epoch 90, training loss: 1.6755340099334717 = 1.6678781509399414 + 0.001 * 7.655918121337891
Epoch 90, val loss: 1.6968300342559814
Epoch 100, training loss: 1.5948213338851929 = 1.5873401165008545 + 0.001 * 7.481198787689209
Epoch 100, val loss: 1.6299736499786377
Epoch 110, training loss: 1.4976210594177246 = 1.490416169166565 + 0.001 * 7.20484733581543
Epoch 110, val loss: 1.5515565872192383
Epoch 120, training loss: 1.396122694015503 = 1.3891545534133911 + 0.001 * 6.968186378479004
Epoch 120, val loss: 1.4692777395248413
Epoch 130, training loss: 1.2981116771697998 = 1.2911770343780518 + 0.001 * 6.934669494628906
Epoch 130, val loss: 1.3922326564788818
Epoch 140, training loss: 1.2040600776672363 = 1.1971582174301147 + 0.001 * 6.901889801025391
Epoch 140, val loss: 1.3209450244903564
Epoch 150, training loss: 1.1135351657867432 = 1.106657862663269 + 0.001 * 6.877278804779053
Epoch 150, val loss: 1.2542816400527954
Epoch 160, training loss: 1.0264148712158203 = 1.0195595026016235 + 0.001 * 6.855367660522461
Epoch 160, val loss: 1.19145667552948
Epoch 170, training loss: 0.9433903098106384 = 0.9365518689155579 + 0.001 * 6.8384222984313965
Epoch 170, val loss: 1.131867527961731
Epoch 180, training loss: 0.865705132484436 = 0.8588773012161255 + 0.001 * 6.827844619750977
Epoch 180, val loss: 1.0759466886520386
Epoch 190, training loss: 0.7938711047172546 = 0.7870501279830933 + 0.001 * 6.820958137512207
Epoch 190, val loss: 1.0241872072219849
Epoch 200, training loss: 0.727385938167572 = 0.7205696702003479 + 0.001 * 6.816256999969482
Epoch 200, val loss: 0.976940929889679
Epoch 210, training loss: 0.6647629141807556 = 0.657949686050415 + 0.001 * 6.813232898712158
Epoch 210, val loss: 0.9331772327423096
Epoch 220, training loss: 0.604468822479248 = 0.5976576209068298 + 0.001 * 6.811221599578857
Epoch 220, val loss: 0.8920236825942993
Epoch 230, training loss: 0.5457165241241455 = 0.5389073491096497 + 0.001 * 6.8091511726379395
Epoch 230, val loss: 0.8533258438110352
Epoch 240, training loss: 0.4890446662902832 = 0.4822382628917694 + 0.001 * 6.806402206420898
Epoch 240, val loss: 0.8178941011428833
Epoch 250, training loss: 0.43552565574645996 = 0.4287228584289551 + 0.001 * 6.802805423736572
Epoch 250, val loss: 0.7868881821632385
Epoch 260, training loss: 0.386066734790802 = 0.3792686462402344 + 0.001 * 6.798079013824463
Epoch 260, val loss: 0.7612271904945374
Epoch 270, training loss: 0.3409741222858429 = 0.3341825306415558 + 0.001 * 6.791604042053223
Epoch 270, val loss: 0.740780770778656
Epoch 280, training loss: 0.300171822309494 = 0.2933894693851471 + 0.001 * 6.78236722946167
Epoch 280, val loss: 0.7251749038696289
Epoch 290, training loss: 0.2632690668106079 = 0.25649479031562805 + 0.001 * 6.774282455444336
Epoch 290, val loss: 0.7135414481163025
Epoch 300, training loss: 0.22983452677726746 = 0.22307343780994415 + 0.001 * 6.761096000671387
Epoch 300, val loss: 0.7054626941680908
Epoch 310, training loss: 0.19963619112968445 = 0.19288402795791626 + 0.001 * 6.752160549163818
Epoch 310, val loss: 0.7005318999290466
Epoch 320, training loss: 0.17269811034202576 = 0.1659563034772873 + 0.001 * 6.741804122924805
Epoch 320, val loss: 0.6982131600379944
Epoch 330, training loss: 0.14913342893123627 = 0.14239493012428284 + 0.001 * 6.738492488861084
Epoch 330, val loss: 0.6982365250587463
Epoch 340, training loss: 0.1288997381925583 = 0.12216690927743912 + 0.001 * 6.732832908630371
Epoch 340, val loss: 0.7003485560417175
Epoch 350, training loss: 0.111759252846241 = 0.10502699762582779 + 0.001 * 6.7322516441345215
Epoch 350, val loss: 0.704376757144928
Epoch 360, training loss: 0.09734638780355453 = 0.09061422944068909 + 0.001 * 6.732161521911621
Epoch 360, val loss: 0.7100031971931458
Epoch 370, training loss: 0.08526550978422165 = 0.0785331279039383 + 0.001 * 6.73237943649292
Epoch 370, val loss: 0.7168713808059692
Epoch 380, training loss: 0.07515198737382889 = 0.06841833144426346 + 0.001 * 6.733656406402588
Epoch 380, val loss: 0.7247108221054077
Epoch 390, training loss: 0.06667770445346832 = 0.059944845736026764 + 0.001 * 6.7328596115112305
Epoch 390, val loss: 0.7332050204277039
Epoch 400, training loss: 0.05956543982028961 = 0.052831850945949554 + 0.001 * 6.733587265014648
Epoch 400, val loss: 0.7422354221343994
Epoch 410, training loss: 0.05357431620359421 = 0.04684096947312355 + 0.001 * 6.7333455085754395
Epoch 410, val loss: 0.7515688538551331
Epoch 420, training loss: 0.04850616306066513 = 0.04177276790142059 + 0.001 * 6.733394145965576
Epoch 420, val loss: 0.7610785365104675
Epoch 430, training loss: 0.04419570416212082 = 0.037461571395397186 + 0.001 * 6.734130382537842
Epoch 430, val loss: 0.7706432938575745
Epoch 440, training loss: 0.04050758108496666 = 0.03377415984869003 + 0.001 * 6.73342227935791
Epoch 440, val loss: 0.780245304107666
Epoch 450, training loss: 0.037336066365242004 = 0.03060106560587883 + 0.001 * 6.734999656677246
Epoch 450, val loss: 0.7897986173629761
Epoch 460, training loss: 0.0345890149474144 = 0.02785443514585495 + 0.001 * 6.734580993652344
Epoch 460, val loss: 0.7992377877235413
Epoch 470, training loss: 0.03219687566161156 = 0.025463294237852097 + 0.001 * 6.733580112457275
Epoch 470, val loss: 0.8086001873016357
Epoch 480, training loss: 0.03010311722755432 = 0.02337012253701687 + 0.001 * 6.732994079589844
Epoch 480, val loss: 0.8177815079689026
Epoch 490, training loss: 0.028259508311748505 = 0.021527783945202827 + 0.001 * 6.731723308563232
Epoch 490, val loss: 0.8268328309059143
Epoch 500, training loss: 0.02663840726017952 = 0.019897865131497383 + 0.001 * 6.740541934967041
Epoch 500, val loss: 0.8357207775115967
Epoch 510, training loss: 0.02518261969089508 = 0.018449077382683754 + 0.001 * 6.7335429191589355
Epoch 510, val loss: 0.8444086313247681
Epoch 520, training loss: 0.02388749271631241 = 0.01715625450015068 + 0.001 * 6.731237411499023
Epoch 520, val loss: 0.8529525995254517
Epoch 530, training loss: 0.02272767573595047 = 0.01599773019552231 + 0.001 * 6.729944705963135
Epoch 530, val loss: 0.8612866401672363
Epoch 540, training loss: 0.02168447896838188 = 0.014955518767237663 + 0.001 * 6.728959083557129
Epoch 540, val loss: 0.8693986535072327
Epoch 550, training loss: 0.020745839923620224 = 0.014014665968716145 + 0.001 * 6.731172561645508
Epoch 550, val loss: 0.8773627877235413
Epoch 560, training loss: 0.019892115145921707 = 0.013162452727556229 + 0.001 * 6.729660987854004
Epoch 560, val loss: 0.8851268887519836
Epoch 570, training loss: 0.019115973263978958 = 0.01238816138356924 + 0.001 * 6.727810859680176
Epoch 570, val loss: 0.8927011489868164
Epoch 580, training loss: 0.01840917207300663 = 0.01168273389339447 + 0.001 * 6.726438045501709
Epoch 580, val loss: 0.9000893831253052
Epoch 590, training loss: 0.017766360193490982 = 0.011038105934858322 + 0.001 * 6.728253364562988
Epoch 590, val loss: 0.9073172211647034
Epoch 600, training loss: 0.01717403531074524 = 0.010447649285197258 + 0.001 * 6.72638463973999
Epoch 600, val loss: 0.9143868088722229
Epoch 610, training loss: 0.016629137098789215 = 0.009905376471579075 + 0.001 * 6.72376012802124
Epoch 610, val loss: 0.9212324023246765
Epoch 620, training loss: 0.0161337461322546 = 0.00940621830523014 + 0.001 * 6.727528095245361
Epoch 620, val loss: 0.9279882907867432
Epoch 630, training loss: 0.015666507184505463 = 0.008945626206696033 + 0.001 * 6.720880508422852
Epoch 630, val loss: 0.9345405697822571
Epoch 640, training loss: 0.015238573774695396 = 0.00851974543184042 + 0.001 * 6.718828201293945
Epoch 640, val loss: 0.9409307241439819
Epoch 650, training loss: 0.014850402250885963 = 0.008125167340040207 + 0.001 * 6.725234508514404
Epoch 650, val loss: 0.9471873641014099
Epoch 660, training loss: 0.014477640390396118 = 0.00775883998721838 + 0.001 * 6.718800067901611
Epoch 660, val loss: 0.953315019607544
Epoch 670, training loss: 0.014135399833321571 = 0.007417717948555946 + 0.001 * 6.717680931091309
Epoch 670, val loss: 0.9592977166175842
Epoch 680, training loss: 0.013813707046210766 = 0.007098404690623283 + 0.001 * 6.715301990509033
Epoch 680, val loss: 0.9651244878768921
Epoch 690, training loss: 0.013513406738638878 = 0.0067983162589371204 + 0.001 * 6.715089797973633
Epoch 690, val loss: 0.970878541469574
Epoch 700, training loss: 0.013239889405667782 = 0.006515189539641142 + 0.001 * 6.7246994972229
Epoch 700, val loss: 0.9764705896377563
Epoch 710, training loss: 0.012975985184311867 = 0.006247623823583126 + 0.001 * 6.728360652923584
Epoch 710, val loss: 0.9819962978363037
Epoch 720, training loss: 0.012705599889159203 = 0.005994671955704689 + 0.001 * 6.710927963256836
Epoch 720, val loss: 0.9874509572982788
Epoch 730, training loss: 0.012466290965676308 = 0.005755594000220299 + 0.001 * 6.710696697235107
Epoch 730, val loss: 0.9927775263786316
Epoch 740, training loss: 0.01223493181169033 = 0.005529752001166344 + 0.001 * 6.705179691314697
Epoch 740, val loss: 0.9980061650276184
Epoch 750, training loss: 0.012026777490973473 = 0.005316529422998428 + 0.001 * 6.710247993469238
Epoch 750, val loss: 1.00316321849823
Epoch 760, training loss: 0.011817670427262783 = 0.005115266889333725 + 0.001 * 6.7024030685424805
Epoch 760, val loss: 1.0082018375396729
Epoch 770, training loss: 0.011628217995166779 = 0.004925214219838381 + 0.001 * 6.7030029296875
Epoch 770, val loss: 1.0131618976593018
Epoch 780, training loss: 0.011456597596406937 = 0.004745776765048504 + 0.001 * 6.710820198059082
Epoch 780, val loss: 1.0180280208587646
Epoch 790, training loss: 0.0112781822681427 = 0.004576326813548803 + 0.001 * 6.701854705810547
Epoch 790, val loss: 1.0228019952774048
Epoch 800, training loss: 0.011138315312564373 = 0.004416214302182198 + 0.001 * 6.722100734710693
Epoch 800, val loss: 1.0274795293807983
Epoch 810, training loss: 0.010956859216094017 = 0.004264914896339178 + 0.001 * 6.691944122314453
Epoch 810, val loss: 1.032101035118103
Epoch 820, training loss: 0.010826405137777328 = 0.00412182928994298 + 0.001 * 6.704575538635254
Epoch 820, val loss: 1.036623477935791
Epoch 830, training loss: 0.010681355372071266 = 0.003986458294093609 + 0.001 * 6.694896221160889
Epoch 830, val loss: 1.0410866737365723
Epoch 840, training loss: 0.010544100776314735 = 0.0038582810666412115 + 0.001 * 6.685819625854492
Epoch 840, val loss: 1.0454638004302979
Epoch 850, training loss: 0.010459371842443943 = 0.003736806334927678 + 0.001 * 6.722565650939941
Epoch 850, val loss: 1.0497251749038696
Epoch 860, training loss: 0.01031123474240303 = 0.003621673211455345 + 0.001 * 6.689561367034912
Epoch 860, val loss: 1.0539296865463257
Epoch 870, training loss: 0.01020811777561903 = 0.0035123969428241253 + 0.001 * 6.695720672607422
Epoch 870, val loss: 1.0580930709838867
Epoch 880, training loss: 0.010086135938763618 = 0.003408578224480152 + 0.001 * 6.677557468414307
Epoch 880, val loss: 1.062149167060852
Epoch 890, training loss: 0.00999663956463337 = 0.003309877822175622 + 0.001 * 6.686761856079102
Epoch 890, val loss: 1.0661497116088867
Epoch 900, training loss: 0.009898290038108826 = 0.0032160321716219187 + 0.001 * 6.682257175445557
Epoch 900, val loss: 1.0700912475585938
Epoch 910, training loss: 0.009819522500038147 = 0.003126677591353655 + 0.001 * 6.692844390869141
Epoch 910, val loss: 1.0739428997039795
Epoch 920, training loss: 0.009734902530908585 = 0.0030415791552513838 + 0.001 * 6.693323135375977
Epoch 920, val loss: 1.0777397155761719
Epoch 930, training loss: 0.009642948396503925 = 0.0029604595620185137 + 0.001 * 6.682488441467285
Epoch 930, val loss: 1.0814697742462158
Epoch 940, training loss: 0.009556051343679428 = 0.0028830640949308872 + 0.001 * 6.67298698425293
Epoch 940, val loss: 1.0851171016693115
Epoch 950, training loss: 0.0094846710562706 = 0.0028091694694012403 + 0.001 * 6.675500869750977
Epoch 950, val loss: 1.0887316465377808
Epoch 960, training loss: 0.009415976703166962 = 0.0027385817375034094 + 0.001 * 6.677394866943359
Epoch 960, val loss: 1.0922596454620361
Epoch 970, training loss: 0.009375009685754776 = 0.0026711085811257362 + 0.001 * 6.7039008140563965
Epoch 970, val loss: 1.0957425832748413
Epoch 980, training loss: 0.009271505288779736 = 0.0026065881829708815 + 0.001 * 6.6649169921875
Epoch 980, val loss: 1.0991734266281128
Epoch 990, training loss: 0.009214568883180618 = 0.0025448400992900133 + 0.001 * 6.6697282791137695
Epoch 990, val loss: 1.1025240421295166
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.7232
Flip ASR: 0.6756/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9600615501403809 = 1.9516876935958862 + 0.001 * 8.373819351196289
Epoch 0, val loss: 1.9449632167816162
Epoch 10, training loss: 1.9498957395553589 = 1.9415220022201538 + 0.001 * 8.37374496459961
Epoch 10, val loss: 1.9342228174209595
Epoch 20, training loss: 1.937680721282959 = 1.929307222366333 + 0.001 * 8.373510360717773
Epoch 20, val loss: 1.921168327331543
Epoch 30, training loss: 1.920824408531189 = 1.9124513864517212 + 0.001 * 8.373067855834961
Epoch 30, val loss: 1.9031087160110474
Epoch 40, training loss: 1.8962572813034058 = 1.8878850936889648 + 0.001 * 8.372169494628906
Epoch 40, val loss: 1.876893162727356
Epoch 50, training loss: 1.861021637916565 = 1.852651596069336 + 0.001 * 8.36999797821045
Epoch 50, val loss: 1.8405914306640625
Epoch 60, training loss: 1.816214919090271 = 1.8078519105911255 + 0.001 * 8.362961769104004
Epoch 60, val loss: 1.7984905242919922
Epoch 70, training loss: 1.768251895904541 = 1.7599228620529175 + 0.001 * 8.328981399536133
Epoch 70, val loss: 1.7591862678527832
Epoch 80, training loss: 1.711713194847107 = 1.7036397457122803 + 0.001 * 8.073457717895508
Epoch 80, val loss: 1.7137222290039062
Epoch 90, training loss: 1.6359913349151611 = 1.6283588409423828 + 0.001 * 7.632492542266846
Epoch 90, val loss: 1.6494027376174927
Epoch 100, training loss: 1.538903832435608 = 1.5314775705337524 + 0.001 * 7.426208019256592
Epoch 100, val loss: 1.5656431913375854
Epoch 110, training loss: 1.428650975227356 = 1.4213656187057495 + 0.001 * 7.285322666168213
Epoch 110, val loss: 1.474831223487854
Epoch 120, training loss: 1.3172402381896973 = 1.3100056648254395 + 0.001 * 7.234602928161621
Epoch 120, val loss: 1.3889073133468628
Epoch 130, training loss: 1.2121015787124634 = 1.204882025718689 + 0.001 * 7.219571113586426
Epoch 130, val loss: 1.3125473260879517
Epoch 140, training loss: 1.1171690225601196 = 1.109985589981079 + 0.001 * 7.183407783508301
Epoch 140, val loss: 1.2466868162155151
Epoch 150, training loss: 1.0336192846298218 = 1.026482105255127 + 0.001 * 7.137129783630371
Epoch 150, val loss: 1.1900861263275146
Epoch 160, training loss: 0.9597123265266418 = 0.952633798122406 + 0.001 * 7.0785441398620605
Epoch 160, val loss: 1.140595555305481
Epoch 170, training loss: 0.892249345779419 = 0.8852275013923645 + 0.001 * 7.021848678588867
Epoch 170, val loss: 1.0956165790557861
Epoch 180, training loss: 0.8279532194137573 = 0.8209771513938904 + 0.001 * 6.976093292236328
Epoch 180, val loss: 1.0527188777923584
Epoch 190, training loss: 0.7646939158439636 = 0.757754385471344 + 0.001 * 6.939534664154053
Epoch 190, val loss: 1.0107593536376953
Epoch 200, training loss: 0.70207279920578 = 0.6951642632484436 + 0.001 * 6.908562660217285
Epoch 200, val loss: 0.9696930050849915
Epoch 210, training loss: 0.6407856941223145 = 0.6339028477668762 + 0.001 * 6.882875442504883
Epoch 210, val loss: 0.9307376146316528
Epoch 220, training loss: 0.5820357799530029 = 0.5751653909683228 + 0.001 * 6.870388031005859
Epoch 220, val loss: 0.8955634236335754
Epoch 230, training loss: 0.5270752310752869 = 0.5202116370201111 + 0.001 * 6.863584041595459
Epoch 230, val loss: 0.8655247688293457
Epoch 240, training loss: 0.47677162289619446 = 0.4699103534221649 + 0.001 * 6.861265659332275
Epoch 240, val loss: 0.8417287468910217
Epoch 250, training loss: 0.43128159642219543 = 0.42442119121551514 + 0.001 * 6.860414028167725
Epoch 250, val loss: 0.824313223361969
Epoch 260, training loss: 0.39012429118156433 = 0.3832643926143646 + 0.001 * 6.859885215759277
Epoch 260, val loss: 0.8126052021980286
Epoch 270, training loss: 0.35248205065727234 = 0.3456225097179413 + 0.001 * 6.859543323516846
Epoch 270, val loss: 0.8055043816566467
Epoch 280, training loss: 0.3175812363624573 = 0.3107219934463501 + 0.001 * 6.859240531921387
Epoch 280, val loss: 0.8018757104873657
Epoch 290, training loss: 0.28501906991004944 = 0.27816006541252136 + 0.001 * 6.859015941619873
Epoch 290, val loss: 0.8008880019187927
Epoch 300, training loss: 0.25479787588119507 = 0.24793900549411774 + 0.001 * 6.8588738441467285
Epoch 300, val loss: 0.801628053188324
Epoch 310, training loss: 0.2270505130290985 = 0.22019170224666595 + 0.001 * 6.8588175773620605
Epoch 310, val loss: 0.804006040096283
Epoch 320, training loss: 0.20190468430519104 = 0.1950458288192749 + 0.001 * 6.858854293823242
Epoch 320, val loss: 0.8077394366264343
Epoch 330, training loss: 0.17937077581882477 = 0.17251183092594147 + 0.001 * 6.8589396476745605
Epoch 330, val loss: 0.8130362629890442
Epoch 340, training loss: 0.1593800187110901 = 0.15252098441123962 + 0.001 * 6.859035015106201
Epoch 340, val loss: 0.8198991417884827
Epoch 350, training loss: 0.1418154388666153 = 0.13495633006095886 + 0.001 * 6.859114646911621
Epoch 350, val loss: 0.8281294703483582
Epoch 360, training loss: 0.12651276588439941 = 0.1196536123752594 + 0.001 * 6.859147548675537
Epoch 360, val loss: 0.8375331163406372
Epoch 370, training loss: 0.11327142268419266 = 0.10641230642795563 + 0.001 * 6.8591132164001465
Epoch 370, val loss: 0.8478367328643799
Epoch 380, training loss: 0.1018463596701622 = 0.0949874147772789 + 0.001 * 6.858946800231934
Epoch 380, val loss: 0.8587641716003418
Epoch 390, training loss: 0.0920058935880661 = 0.08514731377363205 + 0.001 * 6.858579158782959
Epoch 390, val loss: 0.8701929450035095
Epoch 400, training loss: 0.08352202922105789 = 0.07666411995887756 + 0.001 * 6.857906341552734
Epoch 400, val loss: 0.8820140361785889
Epoch 410, training loss: 0.07617802917957306 = 0.06931886821985245 + 0.001 * 6.859158515930176
Epoch 410, val loss: 0.8940756320953369
Epoch 420, training loss: 0.06977421790361404 = 0.06291840970516205 + 0.001 * 6.85581111907959
Epoch 420, val loss: 0.9062826037406921
Epoch 430, training loss: 0.06416349858045578 = 0.0573093481361866 + 0.001 * 6.8541483879089355
Epoch 430, val loss: 0.91861891746521
Epoch 440, training loss: 0.05921114236116409 = 0.05235956236720085 + 0.001 * 6.851578235626221
Epoch 440, val loss: 0.9309481978416443
Epoch 450, training loss: 0.05480910837650299 = 0.04796050116419792 + 0.001 * 6.84860897064209
Epoch 450, val loss: 0.9432199001312256
Epoch 460, training loss: 0.0508713573217392 = 0.04402484372258186 + 0.001 * 6.8465142250061035
Epoch 460, val loss: 0.9553660154342651
Epoch 470, training loss: 0.047323077917099 = 0.04048231989145279 + 0.001 * 6.840755939483643
Epoch 470, val loss: 0.9673954844474792
Epoch 480, training loss: 0.04412221163511276 = 0.037272773683071136 + 0.001 * 6.84943962097168
Epoch 480, val loss: 0.9793075919151306
Epoch 490, training loss: 0.04120253399014473 = 0.03435536101460457 + 0.001 * 6.847171306610107
Epoch 490, val loss: 0.9910081624984741
Epoch 500, training loss: 0.03851683810353279 = 0.031691767275333405 + 0.001 * 6.825069427490234
Epoch 500, val loss: 1.0025914907455444
Epoch 510, training loss: 0.03606810048222542 = 0.029250632971525192 + 0.001 * 6.817467212677002
Epoch 510, val loss: 1.0140875577926636
Epoch 520, training loss: 0.033822137862443924 = 0.02701292745769024 + 0.001 * 6.809208869934082
Epoch 520, val loss: 1.025352954864502
Epoch 530, training loss: 0.031767476350069046 = 0.024959910660982132 + 0.001 * 6.807566165924072
Epoch 530, val loss: 1.0363950729370117
Epoch 540, training loss: 0.02987802028656006 = 0.023075101897120476 + 0.001 * 6.802918910980225
Epoch 540, val loss: 1.047316312789917
Epoch 550, training loss: 0.02820092998445034 = 0.02134045772254467 + 0.001 * 6.860472202301025
Epoch 550, val loss: 1.05801260471344
Epoch 560, training loss: 0.02655068412423134 = 0.019743146374821663 + 0.001 * 6.807537078857422
Epoch 560, val loss: 1.0685513019561768
Epoch 570, training loss: 0.025072738528251648 = 0.018286550417542458 + 0.001 * 6.786186695098877
Epoch 570, val loss: 1.0788477659225464
Epoch 580, training loss: 0.023746168240904808 = 0.01696588099002838 + 0.001 * 6.780287265777588
Epoch 580, val loss: 1.0889662504196167
Epoch 590, training loss: 0.02254997193813324 = 0.015771521255373955 + 0.001 * 6.778451442718506
Epoch 590, val loss: 1.0988645553588867
Epoch 600, training loss: 0.021462999284267426 = 0.014690420590341091 + 0.001 * 6.772578716278076
Epoch 600, val loss: 1.1085790395736694
Epoch 610, training loss: 0.020480789244174957 = 0.013711520470678806 + 0.001 * 6.769268035888672
Epoch 610, val loss: 1.118090033531189
Epoch 620, training loss: 0.019602160900831223 = 0.012823701836168766 + 0.001 * 6.778459072113037
Epoch 620, val loss: 1.1273934841156006
Epoch 630, training loss: 0.01877870038151741 = 0.012014655396342278 + 0.001 * 6.764043807983398
Epoch 630, val loss: 1.1365150213241577
Epoch 640, training loss: 0.01803414151072502 = 0.011273587122559547 + 0.001 * 6.760554790496826
Epoch 640, val loss: 1.1454064846038818
Epoch 650, training loss: 0.017350779846310616 = 0.010591373778879642 + 0.001 * 6.759405612945557
Epoch 650, val loss: 1.1541380882263184
Epoch 660, training loss: 0.016744038090109825 = 0.009958610869944096 + 0.001 * 6.785426616668701
Epoch 660, val loss: 1.162784218788147
Epoch 670, training loss: 0.016126949340105057 = 0.009371055290102959 + 0.001 * 6.755892753601074
Epoch 670, val loss: 1.1713262796401978
Epoch 680, training loss: 0.015585525892674923 = 0.008825569413602352 + 0.001 * 6.759956359863281
Epoch 680, val loss: 1.179689645767212
Epoch 690, training loss: 0.015076516196131706 = 0.008320070803165436 + 0.001 * 6.756444931030273
Epoch 690, val loss: 1.187881588935852
Epoch 700, training loss: 0.014611434191465378 = 0.007852282375097275 + 0.001 * 6.759150981903076
Epoch 700, val loss: 1.1958839893341064
Epoch 710, training loss: 0.014175096526741982 = 0.007419835310429335 + 0.001 * 6.755260467529297
Epoch 710, val loss: 1.2037217617034912
Epoch 720, training loss: 0.013770220801234245 = 0.007020294666290283 + 0.001 * 6.7499260902404785
Epoch 720, val loss: 1.2113816738128662
Epoch 730, training loss: 0.013415697030723095 = 0.006651200354099274 + 0.001 * 6.764496326446533
Epoch 730, val loss: 1.2188137769699097
Epoch 740, training loss: 0.013055987656116486 = 0.006310231983661652 + 0.001 * 6.745755195617676
Epoch 740, val loss: 1.2260746955871582
Epoch 750, training loss: 0.012742329388856888 = 0.005994801875203848 + 0.001 * 6.7475266456604
Epoch 750, val loss: 1.2331430912017822
Epoch 760, training loss: 0.012448476627469063 = 0.00570263946428895 + 0.001 * 6.7458367347717285
Epoch 760, val loss: 1.2400141954421997
Epoch 770, training loss: 0.012174243107438087 = 0.00543174659833312 + 0.001 * 6.742496490478516
Epoch 770, val loss: 1.2467260360717773
Epoch 780, training loss: 0.011929193511605263 = 0.005180301610380411 + 0.001 * 6.748891353607178
Epoch 780, val loss: 1.2532421350479126
Epoch 790, training loss: 0.011685231700539589 = 0.004946781788021326 + 0.001 * 6.738449573516846
Epoch 790, val loss: 1.259611964225769
Epoch 800, training loss: 0.011469634249806404 = 0.004729467909783125 + 0.001 * 6.740166664123535
Epoch 800, val loss: 1.2657923698425293
Epoch 810, training loss: 0.011280598118901253 = 0.00452707102522254 + 0.001 * 6.7535271644592285
Epoch 810, val loss: 1.2717992067337036
Epoch 820, training loss: 0.011074654757976532 = 0.00433813501149416 + 0.001 * 6.736519813537598
Epoch 820, val loss: 1.277708649635315
Epoch 830, training loss: 0.010901082307100296 = 0.00416154507547617 + 0.001 * 6.739536285400391
Epoch 830, val loss: 1.2834330797195435
Epoch 840, training loss: 0.010727815330028534 = 0.003996388521045446 + 0.001 * 6.73142671585083
Epoch 840, val loss: 1.2890608310699463
Epoch 850, training loss: 0.010578276589512825 = 0.0038415533490478992 + 0.001 * 6.736722946166992
Epoch 850, val loss: 1.2945473194122314
Epoch 860, training loss: 0.010440243408083916 = 0.003696146421134472 + 0.001 * 6.744096279144287
Epoch 860, val loss: 1.29985511302948
Epoch 870, training loss: 0.010281860828399658 = 0.0035592832136899233 + 0.001 * 6.7225775718688965
Epoch 870, val loss: 1.305106520652771
Epoch 880, training loss: 0.010162169113755226 = 0.0034302901476621628 + 0.001 * 6.731879234313965
Epoch 880, val loss: 1.3102412223815918
Epoch 890, training loss: 0.010040519759058952 = 0.0033087420742958784 + 0.001 * 6.731777191162109
Epoch 890, val loss: 1.315231442451477
Epoch 900, training loss: 0.0099103394895792 = 0.003193985903635621 + 0.001 * 6.716353416442871
Epoch 900, val loss: 1.3201446533203125
Epoch 910, training loss: 0.009809333831071854 = 0.0030854702927172184 + 0.001 * 6.72386360168457
Epoch 910, val loss: 1.3249597549438477
Epoch 920, training loss: 0.009699639864265919 = 0.0029828303959220648 + 0.001 * 6.716809272766113
Epoch 920, val loss: 1.3296579122543335
Epoch 930, training loss: 0.009627912193536758 = 0.0028855830896645784 + 0.001 * 6.742328643798828
Epoch 930, val loss: 1.3342775106430054
Epoch 940, training loss: 0.009520577266812325 = 0.002793392166495323 + 0.001 * 6.727185249328613
Epoch 940, val loss: 1.338781476020813
Epoch 950, training loss: 0.009430446662008762 = 0.0027059072162956 + 0.001 * 6.724538803100586
Epoch 950, val loss: 1.3432780504226685
Epoch 960, training loss: 0.009338073432445526 = 0.002622960601001978 + 0.001 * 6.715113162994385
Epoch 960, val loss: 1.347568154335022
Epoch 970, training loss: 0.00926179252564907 = 0.0025441553443670273 + 0.001 * 6.717637538909912
Epoch 970, val loss: 1.3518346548080444
Epoch 980, training loss: 0.009170987643301487 = 0.002469306578859687 + 0.001 * 6.701681137084961
Epoch 980, val loss: 1.3560082912445068
Epoch 990, training loss: 0.009100878611207008 = 0.0023981838021427393 + 0.001 * 6.702694892883301
Epoch 990, val loss: 1.3601104021072388
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.5166
Flip ASR: 0.4667/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9513252973556519 = 1.9429515600204468 + 0.001 * 8.373772621154785
Epoch 0, val loss: 1.944610357284546
Epoch 10, training loss: 1.9401519298553467 = 1.9317783117294312 + 0.001 * 8.373632431030273
Epoch 10, val loss: 1.9329503774642944
Epoch 20, training loss: 1.925726056098938 = 1.9173527956008911 + 0.001 * 8.373309135437012
Epoch 20, val loss: 1.9176315069198608
Epoch 30, training loss: 1.905180811882019 = 1.89680814743042 + 0.001 * 8.3726806640625
Epoch 30, val loss: 1.8954933881759644
Epoch 40, training loss: 1.8757258653640747 = 1.8673545122146606 + 0.001 * 8.371357917785645
Epoch 40, val loss: 1.8639636039733887
Epoch 50, training loss: 1.837526559829712 = 1.8291587829589844 + 0.001 * 8.367801666259766
Epoch 50, val loss: 1.8251512050628662
Epoch 60, training loss: 1.7979178428649902 = 1.7895636558532715 + 0.001 * 8.354145050048828
Epoch 60, val loss: 1.7893551588058472
Epoch 70, training loss: 1.7580199241638184 = 1.7497467994689941 + 0.001 * 8.273112297058105
Epoch 70, val loss: 1.7563576698303223
Epoch 80, training loss: 1.7031477689743042 = 1.6953026056289673 + 0.001 * 7.845170974731445
Epoch 80, val loss: 1.7111599445343018
Epoch 90, training loss: 1.6287312507629395 = 1.6210201978683472 + 0.001 * 7.7111077308654785
Epoch 90, val loss: 1.6487454175949097
Epoch 100, training loss: 1.5374786853790283 = 1.5298436880111694 + 0.001 * 7.63497257232666
Epoch 100, val loss: 1.5731980800628662
Epoch 110, training loss: 1.4405319690704346 = 1.4330466985702515 + 0.001 * 7.485272407531738
Epoch 110, val loss: 1.4930284023284912
Epoch 120, training loss: 1.3432291746139526 = 1.3359906673431396 + 0.001 * 7.238532543182373
Epoch 120, val loss: 1.4174320697784424
Epoch 130, training loss: 1.2475441694259644 = 1.240354299545288 + 0.001 * 7.18991231918335
Epoch 130, val loss: 1.3469737768173218
Epoch 140, training loss: 1.1542378664016724 = 1.1471099853515625 + 0.001 * 7.127936363220215
Epoch 140, val loss: 1.2807494401931763
Epoch 150, training loss: 1.0643136501312256 = 1.05722177028656 + 0.001 * 7.091893196105957
Epoch 150, val loss: 1.218187928199768
Epoch 160, training loss: 0.978184700012207 = 0.9711302518844604 + 0.001 * 7.0544586181640625
Epoch 160, val loss: 1.1581708192825317
Epoch 170, training loss: 0.8955127000808716 = 0.8884903788566589 + 0.001 * 7.0223069190979
Epoch 170, val loss: 1.1000723838806152
Epoch 180, training loss: 0.8154267072677612 = 0.8084354996681213 + 0.001 * 6.991219997406006
Epoch 180, val loss: 1.043219804763794
Epoch 190, training loss: 0.7372051477432251 = 0.7302454113960266 + 0.001 * 6.959722518920898
Epoch 190, val loss: 0.9874480366706848
Epoch 200, training loss: 0.6614393591880798 = 0.6545090675354004 + 0.001 * 6.930276870727539
Epoch 200, val loss: 0.9342334866523743
Epoch 210, training loss: 0.5903468728065491 = 0.5834347605705261 + 0.001 * 6.912102699279785
Epoch 210, val loss: 0.8860579133033752
Epoch 220, training loss: 0.5260851383209229 = 0.5191768407821655 + 0.001 * 6.908303737640381
Epoch 220, val loss: 0.8455697894096375
Epoch 230, training loss: 0.46935606002807617 = 0.4624520242214203 + 0.001 * 6.904033660888672
Epoch 230, val loss: 0.813718318939209
Epoch 240, training loss: 0.4194992780685425 = 0.4125983417034149 + 0.001 * 6.900929927825928
Epoch 240, val loss: 0.7900040149688721
Epoch 250, training loss: 0.3755851686000824 = 0.3686865270137787 + 0.001 * 6.898636341094971
Epoch 250, val loss: 0.7734244465827942
Epoch 260, training loss: 0.33686086535453796 = 0.3299640119075775 + 0.001 * 6.896850109100342
Epoch 260, val loss: 0.762924313545227
Epoch 270, training loss: 0.30249419808387756 = 0.2955993115901947 + 0.001 * 6.89488410949707
Epoch 270, val loss: 0.7574506402015686
Epoch 280, training loss: 0.27146440744400024 = 0.26457154750823975 + 0.001 * 6.892858982086182
Epoch 280, val loss: 0.756022036075592
Epoch 290, training loss: 0.2427324652671814 = 0.2358417510986328 + 0.001 * 6.890712261199951
Epoch 290, val loss: 0.7572963237762451
Epoch 300, training loss: 0.215601846575737 = 0.2087099254131317 + 0.001 * 6.8919172286987305
Epoch 300, val loss: 0.7602385878562927
Epoch 310, training loss: 0.1899794340133667 = 0.18309113383293152 + 0.001 * 6.888301372528076
Epoch 310, val loss: 0.764349639415741
Epoch 320, training loss: 0.16634610295295715 = 0.1594613641500473 + 0.001 * 6.884744644165039
Epoch 320, val loss: 0.7694998383522034
Epoch 330, training loss: 0.1453343778848648 = 0.13845185935497284 + 0.001 * 6.882519721984863
Epoch 330, val loss: 0.7759162783622742
Epoch 340, training loss: 0.12725569307804108 = 0.12037581950426102 + 0.001 * 6.879876136779785
Epoch 340, val loss: 0.7840248346328735
Epoch 350, training loss: 0.11202284693717957 = 0.10514573752880096 + 0.001 * 6.877110004425049
Epoch 350, val loss: 0.793851912021637
Epoch 360, training loss: 0.09928019344806671 = 0.09240449219942093 + 0.001 * 6.875697612762451
Epoch 360, val loss: 0.8052376508712769
Epoch 370, training loss: 0.08859166502952576 = 0.08171819150447845 + 0.001 * 6.873472213745117
Epoch 370, val loss: 0.817791223526001
Epoch 380, training loss: 0.07955018430948257 = 0.07268182188272476 + 0.001 * 6.868361949920654
Epoch 380, val loss: 0.8311225175857544
Epoch 390, training loss: 0.0718172937631607 = 0.06495388597249985 + 0.001 * 6.863409042358398
Epoch 390, val loss: 0.8449345231056213
Epoch 400, training loss: 0.06512881070375443 = 0.058270104229450226 + 0.001 * 6.858707427978516
Epoch 400, val loss: 0.8590126037597656
Epoch 410, training loss: 0.05928628891706467 = 0.05242975428700447 + 0.001 * 6.856535911560059
Epoch 410, val loss: 0.8732503056526184
Epoch 420, training loss: 0.054135601967573166 = 0.04728899896144867 + 0.001 * 6.846602439880371
Epoch 420, val loss: 0.8874583840370178
Epoch 430, training loss: 0.049584489315748215 = 0.042745303362607956 + 0.001 * 6.839186191558838
Epoch 430, val loss: 0.9016600847244263
Epoch 440, training loss: 0.04555901512503624 = 0.03872153162956238 + 0.001 * 6.837482929229736
Epoch 440, val loss: 0.9157729744911194
Epoch 450, training loss: 0.0419892892241478 = 0.035156235098838806 + 0.001 * 6.833052635192871
Epoch 450, val loss: 0.9297811388969421
Epoch 460, training loss: 0.03884201496839523 = 0.03199721872806549 + 0.001 * 6.844797611236572
Epoch 460, val loss: 0.9436086416244507
Epoch 470, training loss: 0.03601536527276039 = 0.029197461903095245 + 0.001 * 6.817903995513916
Epoch 470, val loss: 0.9572963118553162
Epoch 480, training loss: 0.03351418673992157 = 0.02671331912279129 + 0.001 * 6.800866603851318
Epoch 480, val loss: 0.9707862138748169
Epoch 490, training loss: 0.031295496970415115 = 0.02450559288263321 + 0.001 * 6.789902687072754
Epoch 490, val loss: 0.984075129032135
Epoch 500, training loss: 0.02932433784008026 = 0.022539829835295677 + 0.001 * 6.784506797790527
Epoch 500, val loss: 0.9971411824226379
Epoch 510, training loss: 0.027578799054026604 = 0.020785998553037643 + 0.001 * 6.792800426483154
Epoch 510, val loss: 1.0099822282791138
Epoch 520, training loss: 0.02600027061998844 = 0.019217560067772865 + 0.001 * 6.782710552215576
Epoch 520, val loss: 1.0224940776824951
Epoch 530, training loss: 0.02459448203444481 = 0.01781131699681282 + 0.001 * 6.7831645011901855
Epoch 530, val loss: 1.034745216369629
Epoch 540, training loss: 0.023319458588957787 = 0.01654748246073723 + 0.001 * 6.771975994110107
Epoch 540, val loss: 1.046716570854187
Epoch 550, training loss: 0.022201307117938995 = 0.015408664010465145 + 0.001 * 6.792642116546631
Epoch 550, val loss: 1.0583860874176025
Epoch 560, training loss: 0.021146077662706375 = 0.014380073174834251 + 0.001 * 6.76600456237793
Epoch 560, val loss: 1.0697890520095825
Epoch 570, training loss: 0.020215889438986778 = 0.013448785059154034 + 0.001 * 6.767104625701904
Epoch 570, val loss: 1.0808748006820679
Epoch 580, training loss: 0.01937851496040821 = 0.012603678740561008 + 0.001 * 6.77483606338501
Epoch 580, val loss: 1.0916441679000854
Epoch 590, training loss: 0.018603548407554626 = 0.011834923177957535 + 0.001 * 6.768624305725098
Epoch 590, val loss: 1.1021690368652344
Epoch 600, training loss: 0.01789786107838154 = 0.011133966036140919 + 0.001 * 6.763894557952881
Epoch 600, val loss: 1.1124026775360107
Epoch 610, training loss: 0.01725725457072258 = 0.010493416339159012 + 0.001 * 6.763836860656738
Epoch 610, val loss: 1.122404932975769
Epoch 620, training loss: 0.016667691990733147 = 0.00990680605173111 + 0.001 * 6.760885715484619
Epoch 620, val loss: 1.1321287155151367
Epoch 630, training loss: 0.016131103038787842 = 0.009368440136313438 + 0.001 * 6.762662410736084
Epoch 630, val loss: 1.1416343450546265
Epoch 640, training loss: 0.015635386109352112 = 0.008873379789292812 + 0.001 * 6.762004852294922
Epoch 640, val loss: 1.150884747505188
Epoch 650, training loss: 0.01518668606877327 = 0.00841723196208477 + 0.001 * 6.769454479217529
Epoch 650, val loss: 1.1599282026290894
Epoch 660, training loss: 0.014748817309737206 = 0.007996100932359695 + 0.001 * 6.752716064453125
Epoch 660, val loss: 1.1687320470809937
Epoch 670, training loss: 0.014365244656801224 = 0.007606588304042816 + 0.001 * 6.758656024932861
Epoch 670, val loss: 1.1773240566253662
Epoch 680, training loss: 0.014000087976455688 = 0.007245704066008329 + 0.001 * 6.754383087158203
Epoch 680, val loss: 1.1857200860977173
Epoch 690, training loss: 0.013665066100656986 = 0.006910828873515129 + 0.001 * 6.754236698150635
Epoch 690, val loss: 1.193927526473999
Epoch 700, training loss: 0.013363448902964592 = 0.006599531974643469 + 0.001 * 6.763916492462158
Epoch 700, val loss: 1.2019020318984985
Epoch 710, training loss: 0.013061536476016045 = 0.006309676915407181 + 0.001 * 6.751858711242676
Epoch 710, val loss: 1.2097179889678955
Epoch 720, training loss: 0.012799903750419617 = 0.006039366591721773 + 0.001 * 6.760537147521973
Epoch 720, val loss: 1.2173457145690918
Epoch 730, training loss: 0.012537246569991112 = 0.005786941386759281 + 0.001 * 6.750304698944092
Epoch 730, val loss: 1.2247930765151978
Epoch 740, training loss: 0.012304212898015976 = 0.0055509330704808235 + 0.001 * 6.753279685974121
Epoch 740, val loss: 1.2320588827133179
Epoch 750, training loss: 0.012078228406608105 = 0.005329952109605074 + 0.001 * 6.7482757568359375
Epoch 750, val loss: 1.2391841411590576
Epoch 760, training loss: 0.011867397464811802 = 0.005122746806591749 + 0.001 * 6.744650363922119
Epoch 760, val loss: 1.246130347251892
Epoch 770, training loss: 0.011692892760038376 = 0.00492823077365756 + 0.001 * 6.76466178894043
Epoch 770, val loss: 1.2529393434524536
Epoch 780, training loss: 0.01148381270468235 = 0.004745349753648043 + 0.001 * 6.738462448120117
Epoch 780, val loss: 1.2595882415771484
Epoch 790, training loss: 0.011311700567603111 = 0.004573237616568804 + 0.001 * 6.738462924957275
Epoch 790, val loss: 1.2661062479019165
Epoch 800, training loss: 0.011148225516080856 = 0.004411036614328623 + 0.001 * 6.737188339233398
Epoch 800, val loss: 1.272464394569397
Epoch 810, training loss: 0.011022789403796196 = 0.004258020780980587 + 0.001 * 6.764768600463867
Epoch 810, val loss: 1.2786909341812134
Epoch 820, training loss: 0.01084515918046236 = 0.004113530740141869 + 0.001 * 6.731627941131592
Epoch 820, val loss: 1.2847861051559448
Epoch 830, training loss: 0.010732561349868774 = 0.003976956009864807 + 0.001 * 6.7556047439575195
Epoch 830, val loss: 1.290745735168457
Epoch 840, training loss: 0.010585527867078781 = 0.0038477270863950253 + 0.001 * 6.737800598144531
Epoch 840, val loss: 1.2965909242630005
Epoch 850, training loss: 0.010457945987582207 = 0.003725305199623108 + 0.001 * 6.732639789581299
Epoch 850, val loss: 1.3023401498794556
Epoch 860, training loss: 0.010334441438317299 = 0.0036092484369874 + 0.001 * 6.725192546844482
Epoch 860, val loss: 1.3079233169555664
Epoch 870, training loss: 0.010232613421976566 = 0.0034991358406841755 + 0.001 * 6.7334771156311035
Epoch 870, val loss: 1.3134149312973022
Epoch 880, training loss: 0.010136937722563744 = 0.003394577419385314 + 0.001 * 6.742359638214111
Epoch 880, val loss: 1.3188203573226929
Epoch 890, training loss: 0.010016788728535175 = 0.0032952053006738424 + 0.001 * 6.721582889556885
Epoch 890, val loss: 1.3240803480148315
Epoch 900, training loss: 0.009950791485607624 = 0.0032006860710680485 + 0.001 * 6.750104904174805
Epoch 900, val loss: 1.3292508125305176
Epoch 910, training loss: 0.009832157753407955 = 0.0031107154209166765 + 0.001 * 6.721442222595215
Epoch 910, val loss: 1.3343204259872437
Epoch 920, training loss: 0.00974787026643753 = 0.0030249932315200567 + 0.001 * 6.722877025604248
Epoch 920, val loss: 1.339295744895935
Epoch 930, training loss: 0.009685654193162918 = 0.0029432568699121475 + 0.001 * 6.742396354675293
Epoch 930, val loss: 1.3441497087478638
Epoch 940, training loss: 0.009583246894180775 = 0.002865260699763894 + 0.001 * 6.7179856300354
Epoch 940, val loss: 1.3489630222320557
Epoch 950, training loss: 0.009526237845420837 = 0.0027907886542379856 + 0.001 * 6.735448837280273
Epoch 950, val loss: 1.3536511659622192
Epoch 960, training loss: 0.009438453242182732 = 0.002719626994803548 + 0.001 * 6.718825817108154
Epoch 960, val loss: 1.3582756519317627
Epoch 970, training loss: 0.009371364489197731 = 0.002651589922606945 + 0.001 * 6.7197747230529785
Epoch 970, val loss: 1.3627997636795044
Epoch 980, training loss: 0.009301205165684223 = 0.0025865002535283566 + 0.001 * 6.714704513549805
Epoch 980, val loss: 1.3672618865966797
Epoch 990, training loss: 0.009249797090888023 = 0.0025241952389478683 + 0.001 * 6.725601673126221
Epoch 990, val loss: 1.3716204166412354
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.9114
Flip ASR: 0.8933/225 nodes
The final ASR:0.71710, 0.16125, Accuracy:0.79012, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10570])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98524, 0.00301, Accuracy:0.83457, 0.00175
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9447320699691772 = 1.9363582134246826 + 0.001 * 8.373823165893555
Epoch 0, val loss: 1.9341700077056885
Epoch 10, training loss: 1.93535578250885 = 1.926982045173645 + 0.001 * 8.373723983764648
Epoch 10, val loss: 1.9256519079208374
Epoch 20, training loss: 1.923671841621399 = 1.9152984619140625 + 0.001 * 8.373429298400879
Epoch 20, val loss: 1.9148046970367432
Epoch 30, training loss: 1.907029390335083 = 1.8986566066741943 + 0.001 * 8.372825622558594
Epoch 30, val loss: 1.8992173671722412
Epoch 40, training loss: 1.8823243379592896 = 1.873952865600586 + 0.001 * 8.371451377868652
Epoch 40, val loss: 1.8764615058898926
Epoch 50, training loss: 1.8475377559661865 = 1.8391700983047485 + 0.001 * 8.367708206176758
Epoch 50, val loss: 1.846145749092102
Epoch 60, training loss: 1.806546926498413 = 1.7981928586959839 + 0.001 * 8.354110717773438
Epoch 60, val loss: 1.8137967586517334
Epoch 70, training loss: 1.7655539512634277 = 1.757270097732544 + 0.001 * 8.283872604370117
Epoch 70, val loss: 1.7819957733154297
Epoch 80, training loss: 1.7136441469192505 = 1.7056474685668945 + 0.001 * 7.996652126312256
Epoch 80, val loss: 1.7360197305679321
Epoch 90, training loss: 1.6416590213775635 = 1.6339290142059326 + 0.001 * 7.730064868927002
Epoch 90, val loss: 1.6727849245071411
Epoch 100, training loss: 1.5498515367507935 = 1.5423895120620728 + 0.001 * 7.462070465087891
Epoch 100, val loss: 1.5968585014343262
Epoch 110, training loss: 1.4469149112701416 = 1.4395805597305298 + 0.001 * 7.334301471710205
Epoch 110, val loss: 1.5150096416473389
Epoch 120, training loss: 1.341491937637329 = 1.3341879844665527 + 0.001 * 7.303976058959961
Epoch 120, val loss: 1.4316480159759521
Epoch 130, training loss: 1.235158920288086 = 1.227899432182312 + 0.001 * 7.259505748748779
Epoch 130, val loss: 1.3489793539047241
Epoch 140, training loss: 1.1274996995925903 = 1.1202889680862427 + 0.001 * 7.210742473602295
Epoch 140, val loss: 1.2657768726348877
Epoch 150, training loss: 1.020512580871582 = 1.0133534669876099 + 0.001 * 7.159095764160156
Epoch 150, val loss: 1.1835050582885742
Epoch 160, training loss: 0.9177866578102112 = 0.9106748700141907 + 0.001 * 7.111804962158203
Epoch 160, val loss: 1.105346441268921
Epoch 170, training loss: 0.8220816254615784 = 0.8149985074996948 + 0.001 * 7.083095550537109
Epoch 170, val loss: 1.034229040145874
Epoch 180, training loss: 0.7351539731025696 = 0.7280886173248291 + 0.001 * 7.065370559692383
Epoch 180, val loss: 0.9726085662841797
Epoch 190, training loss: 0.6576825380325317 = 0.6506326794624329 + 0.001 * 7.049868106842041
Epoch 190, val loss: 0.9212252497673035
Epoch 200, training loss: 0.5893334746360779 = 0.5823052525520325 + 0.001 * 7.028250694274902
Epoch 200, val loss: 0.8796266317367554
Epoch 210, training loss: 0.5291564464569092 = 0.5221558213233948 + 0.001 * 7.000596523284912
Epoch 210, val loss: 0.8473384380340576
Epoch 220, training loss: 0.4758860468864441 = 0.46890774369239807 + 0.001 * 6.978311538696289
Epoch 220, val loss: 0.8232620358467102
Epoch 230, training loss: 0.4282342493534088 = 0.4212668538093567 + 0.001 * 6.967409610748291
Epoch 230, val loss: 0.8061757683753967
Epoch 240, training loss: 0.385260671377182 = 0.37829965353012085 + 0.001 * 6.961025714874268
Epoch 240, val loss: 0.7947913408279419
Epoch 250, training loss: 0.34661251306533813 = 0.33965426683425903 + 0.001 * 6.958249092102051
Epoch 250, val loss: 0.7880979776382446
Epoch 260, training loss: 0.31219282746315 = 0.30523717403411865 + 0.001 * 6.955644130706787
Epoch 260, val loss: 0.7856239676475525
Epoch 270, training loss: 0.28172460198402405 = 0.27477163076400757 + 0.001 * 6.952962875366211
Epoch 270, val loss: 0.7870825529098511
Epoch 280, training loss: 0.2547168731689453 = 0.24776576459407806 + 0.001 * 6.951120376586914
Epoch 280, val loss: 0.7918701767921448
Epoch 290, training loss: 0.2305493801832199 = 0.22360144555568695 + 0.001 * 6.947940349578857
Epoch 290, val loss: 0.7997984886169434
Epoch 300, training loss: 0.20846305787563324 = 0.20151786506175995 + 0.001 * 6.945186138153076
Epoch 300, val loss: 0.8102595210075378
Epoch 310, training loss: 0.18783335387706757 = 0.18088939785957336 + 0.001 * 6.943953037261963
Epoch 310, val loss: 0.822736918926239
Epoch 320, training loss: 0.1683807373046875 = 0.16144122183322906 + 0.001 * 6.939509868621826
Epoch 320, val loss: 0.8367277979850769
Epoch 330, training loss: 0.15018978714942932 = 0.14325298368930817 + 0.001 * 6.936803340911865
Epoch 330, val loss: 0.851954996585846
Epoch 340, training loss: 0.1334543526172638 = 0.1265222728252411 + 0.001 * 6.932077407836914
Epoch 340, val loss: 0.8682959079742432
Epoch 350, training loss: 0.11834970116615295 = 0.11142214387655258 + 0.001 * 6.927557945251465
Epoch 350, val loss: 0.8855143785476685
Epoch 360, training loss: 0.10492511093616486 = 0.09800003468990326 + 0.001 * 6.925079822540283
Epoch 360, val loss: 0.9033021330833435
Epoch 370, training loss: 0.0931064635515213 = 0.08618621528148651 + 0.001 * 6.9202470779418945
Epoch 370, val loss: 0.9215332269668579
Epoch 380, training loss: 0.08277279138565063 = 0.07586066424846649 + 0.001 * 6.912129878997803
Epoch 380, val loss: 0.9398403167724609
Epoch 390, training loss: 0.0737944170832634 = 0.06688997149467468 + 0.001 * 6.90444803237915
Epoch 390, val loss: 0.9581161141395569
Epoch 400, training loss: 0.06603282690048218 = 0.059126585721969604 + 0.001 * 6.9062418937683105
Epoch 400, val loss: 0.9762964248657227
Epoch 410, training loss: 0.0593099370598793 = 0.05242375656962395 + 0.001 * 6.886180400848389
Epoch 410, val loss: 0.994128406047821
Epoch 420, training loss: 0.05351540446281433 = 0.04664168506860733 + 0.001 * 6.873720169067383
Epoch 420, val loss: 1.0117193460464478
Epoch 430, training loss: 0.048539258539676666 = 0.04165423661470413 + 0.001 * 6.885021209716797
Epoch 430, val loss: 1.0289530754089355
Epoch 440, training loss: 0.0442282073199749 = 0.037349458783864975 + 0.001 * 6.878747463226318
Epoch 440, val loss: 1.0458765029907227
Epoch 450, training loss: 0.040488351136446 = 0.033627983182668686 + 0.001 * 6.860367774963379
Epoch 450, val loss: 1.0623664855957031
Epoch 460, training loss: 0.03726266324520111 = 0.030403992161154747 + 0.001 * 6.85867166519165
Epoch 460, val loss: 1.0784515142440796
Epoch 470, training loss: 0.034451037645339966 = 0.02760297805070877 + 0.001 * 6.848059177398682
Epoch 470, val loss: 1.094130516052246
Epoch 480, training loss: 0.03200845420360565 = 0.02516079507768154 + 0.001 * 6.847659111022949
Epoch 480, val loss: 1.1093041896820068
Epoch 490, training loss: 0.029875868931412697 = 0.023023264482617378 + 0.001 * 6.852603435516357
Epoch 490, val loss: 1.1240804195404053
Epoch 500, training loss: 0.02799062617123127 = 0.02114466205239296 + 0.001 * 6.845963478088379
Epoch 500, val loss: 1.1383970975875854
Epoch 510, training loss: 0.026331285014748573 = 0.019486483186483383 + 0.001 * 6.844801425933838
Epoch 510, val loss: 1.1523382663726807
Epoch 520, training loss: 0.024856535717844963 = 0.0180168729275465 + 0.001 * 6.839662551879883
Epoch 520, val loss: 1.1658815145492554
Epoch 530, training loss: 0.023558977991342545 = 0.01670900732278824 + 0.001 * 6.849969863891602
Epoch 530, val loss: 1.1789977550506592
Epoch 540, training loss: 0.02238016203045845 = 0.01554054580628872 + 0.001 * 6.839615345001221
Epoch 540, val loss: 1.1916759014129639
Epoch 550, training loss: 0.0213327556848526 = 0.014492925256490707 + 0.001 * 6.83983039855957
Epoch 550, val loss: 1.2040321826934814
Epoch 560, training loss: 0.0203976109623909 = 0.013550223782658577 + 0.001 * 6.847386360168457
Epoch 560, val loss: 1.2159947156906128
Epoch 570, training loss: 0.01953401416540146 = 0.01269914023578167 + 0.001 * 6.834873676300049
Epoch 570, val loss: 1.2276161909103394
Epoch 580, training loss: 0.018775776028633118 = 0.01192840002477169 + 0.001 * 6.84737491607666
Epoch 580, val loss: 1.2388895750045776
Epoch 590, training loss: 0.018060922622680664 = 0.011228349059820175 + 0.001 * 6.832574367523193
Epoch 590, val loss: 1.2498419284820557
Epoch 600, training loss: 0.017422594130039215 = 0.010590710677206516 + 0.001 * 6.831881999969482
Epoch 600, val loss: 1.2605130672454834
Epoch 610, training loss: 0.016839638352394104 = 0.010008261539041996 + 0.001 * 6.8313775062561035
Epoch 610, val loss: 1.2708510160446167
Epoch 620, training loss: 0.01629994809627533 = 0.009474859572947025 + 0.001 * 6.825087547302246
Epoch 620, val loss: 1.2809327840805054
Epoch 630, training loss: 0.015833741053938866 = 0.008985192514955997 + 0.001 * 6.848548412322998
Epoch 630, val loss: 1.2907483577728271
Epoch 640, training loss: 0.015358086675405502 = 0.008534645661711693 + 0.001 * 6.823440074920654
Epoch 640, val loss: 1.3002833127975464
Epoch 650, training loss: 0.014944256283342838 = 0.008119163103401661 + 0.001 * 6.825092792510986
Epoch 650, val loss: 1.3095468282699585
Epoch 660, training loss: 0.01455686241388321 = 0.007735185790807009 + 0.001 * 6.821676731109619
Epoch 660, val loss: 1.318560004234314
Epoch 670, training loss: 0.014197833836078644 = 0.007379620801657438 + 0.001 * 6.818212509155273
Epoch 670, val loss: 1.3273407220840454
Epoch 680, training loss: 0.013864708133041859 = 0.0070497277192771435 + 0.001 * 6.8149800300598145
Epoch 680, val loss: 1.3358787298202515
Epoch 690, training loss: 0.01356092281639576 = 0.006743133533746004 + 0.001 * 6.817789554595947
Epoch 690, val loss: 1.3442282676696777
Epoch 700, training loss: 0.013261719606816769 = 0.006457705982029438 + 0.001 * 6.804013252258301
Epoch 700, val loss: 1.3523560762405396
Epoch 710, training loss: 0.013006556779146194 = 0.006191508378833532 + 0.001 * 6.8150482177734375
Epoch 710, val loss: 1.3602880239486694
Epoch 720, training loss: 0.0127458106726408 = 0.005942839197814465 + 0.001 * 6.802971839904785
Epoch 720, val loss: 1.3680336475372314
Epoch 730, training loss: 0.012510914355516434 = 0.005710198078304529 + 0.001 * 6.800716400146484
Epoch 730, val loss: 1.375585913658142
Epoch 740, training loss: 0.012300213798880577 = 0.005492242518812418 + 0.001 * 6.8079705238342285
Epoch 740, val loss: 1.3829389810562134
Epoch 750, training loss: 0.012111818417906761 = 0.005287729203701019 + 0.001 * 6.824089527130127
Epoch 750, val loss: 1.3901422023773193
Epoch 760, training loss: 0.011886181309819221 = 0.005095599219202995 + 0.001 * 6.790581703186035
Epoch 760, val loss: 1.3971771001815796
Epoch 770, training loss: 0.01170797273516655 = 0.004914843011647463 + 0.001 * 6.7931294441223145
Epoch 770, val loss: 1.4040507078170776
Epoch 780, training loss: 0.011537786573171616 = 0.004744596313685179 + 0.001 * 6.7931904792785645
Epoch 780, val loss: 1.4107677936553955
Epoch 790, training loss: 0.011380637995898724 = 0.004584051668643951 + 0.001 * 6.796586036682129
Epoch 790, val loss: 1.4173355102539062
Epoch 800, training loss: 0.011216532438993454 = 0.0044324686750769615 + 0.001 * 6.784063816070557
Epoch 800, val loss: 1.4237611293792725
Epoch 810, training loss: 0.01107820775359869 = 0.0042891972698271275 + 0.001 * 6.789010047912598
Epoch 810, val loss: 1.4300683736801147
Epoch 820, training loss: 0.01093391515314579 = 0.004153616260737181 + 0.001 * 6.780298709869385
Epoch 820, val loss: 1.4362205266952515
Epoch 830, training loss: 0.01081935316324234 = 0.004025205038487911 + 0.001 * 6.794147968292236
Epoch 830, val loss: 1.4422202110290527
Epoch 840, training loss: 0.01069536805152893 = 0.003903494216501713 + 0.001 * 6.791873455047607
Epoch 840, val loss: 1.4481269121170044
Epoch 850, training loss: 0.010568883270025253 = 0.003787968074902892 + 0.001 * 6.780914783477783
Epoch 850, val loss: 1.4539244174957275
Epoch 860, training loss: 0.010454979725182056 = 0.0036782289389520884 + 0.001 * 6.776750087738037
Epoch 860, val loss: 1.459567904472351
Epoch 870, training loss: 0.010366534814238548 = 0.003573901718482375 + 0.001 * 6.792632579803467
Epoch 870, val loss: 1.4651063680648804
Epoch 880, training loss: 0.010260120965540409 = 0.0034746157471090555 + 0.001 * 6.785505294799805
Epoch 880, val loss: 1.470546007156372
Epoch 890, training loss: 0.010152327828109264 = 0.0033800697419792414 + 0.001 * 6.7722578048706055
Epoch 890, val loss: 1.4758728742599487
Epoch 900, training loss: 0.010067062452435493 = 0.003289956832304597 + 0.001 * 6.77710485458374
Epoch 900, val loss: 1.4810737371444702
Epoch 910, training loss: 0.010015556588768959 = 0.003204008098691702 + 0.001 * 6.811548233032227
Epoch 910, val loss: 1.4861903190612793
Epoch 920, training loss: 0.009898217394948006 = 0.003121974179521203 + 0.001 * 6.776242733001709
Epoch 920, val loss: 1.4912141561508179
Epoch 930, training loss: 0.009812168776988983 = 0.0030436147935688496 + 0.001 * 6.768553733825684
Epoch 930, val loss: 1.496145248413086
Epoch 940, training loss: 0.009739031083881855 = 0.0029687199275940657 + 0.001 * 6.770310401916504
Epoch 940, val loss: 1.5009820461273193
Epoch 950, training loss: 0.009665838442742825 = 0.002897104015573859 + 0.001 * 6.768734455108643
Epoch 950, val loss: 1.5057250261306763
Epoch 960, training loss: 0.009601796045899391 = 0.00282856123521924 + 0.001 * 6.773234844207764
Epoch 960, val loss: 1.510390281677246
Epoch 970, training loss: 0.0095461281016469 = 0.002762892283499241 + 0.001 * 6.783235549926758
Epoch 970, val loss: 1.5149587392807007
Epoch 980, training loss: 0.009452665224671364 = 0.002699957462027669 + 0.001 * 6.752707004547119
Epoch 980, val loss: 1.5194538831710815
Epoch 990, training loss: 0.009396621026098728 = 0.00263961567543447 + 0.001 * 6.757004737854004
Epoch 990, val loss: 1.5238568782806396
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7519
Overall ASR: 0.5867
Flip ASR: 0.5067/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.947404384613037 = 1.9390305280685425 + 0.001 * 8.37385082244873
Epoch 0, val loss: 1.9297754764556885
Epoch 10, training loss: 1.937639594078064 = 1.9292658567428589 + 0.001 * 8.373757362365723
Epoch 10, val loss: 1.9196878671646118
Epoch 20, training loss: 1.9257466793060303 = 1.9173731803894043 + 0.001 * 8.373479843139648
Epoch 20, val loss: 1.9074747562408447
Epoch 30, training loss: 1.909295678138733 = 1.9009227752685547 + 0.001 * 8.37288761138916
Epoch 30, val loss: 1.8908578157424927
Epoch 40, training loss: 1.8852375745773315 = 1.876866102218628 + 0.001 * 8.371528625488281
Epoch 40, val loss: 1.8669747114181519
Epoch 50, training loss: 1.851122498512268 = 1.842754602432251 + 0.001 * 8.367839813232422
Epoch 50, val loss: 1.8344184160232544
Epoch 60, training loss: 1.809126615524292 = 1.8007726669311523 + 0.001 * 8.35390567779541
Epoch 60, val loss: 1.7969340085983276
Epoch 70, training loss: 1.76645028591156 = 1.758172869682312 + 0.001 * 8.277400016784668
Epoch 70, val loss: 1.7608062028884888
Epoch 80, training loss: 1.715720295906067 = 1.7077234983444214 + 0.001 * 7.996779441833496
Epoch 80, val loss: 1.7149120569229126
Epoch 90, training loss: 1.6458137035369873 = 1.638193964958191 + 0.001 * 7.619790077209473
Epoch 90, val loss: 1.6512261629104614
Epoch 100, training loss: 1.5545448064804077 = 1.5472229719161987 + 0.001 * 7.321878910064697
Epoch 100, val loss: 1.572320818901062
Epoch 110, training loss: 1.4467829465866089 = 1.4396425485610962 + 0.001 * 7.140410423278809
Epoch 110, val loss: 1.4805680513381958
Epoch 120, training loss: 1.3334696292877197 = 1.3263497352600098 + 0.001 * 7.119880199432373
Epoch 120, val loss: 1.3870543241500854
Epoch 130, training loss: 1.221447467803955 = 1.2143361568450928 + 0.001 * 7.111340522766113
Epoch 130, val loss: 1.2957707643508911
Epoch 140, training loss: 1.1131572723388672 = 1.1060465574264526 + 0.001 * 7.110706806182861
Epoch 140, val loss: 1.21030855178833
Epoch 150, training loss: 1.0099748373031616 = 1.0028644800186157 + 0.001 * 7.110304832458496
Epoch 150, val loss: 1.1306394338607788
Epoch 160, training loss: 0.9136057496070862 = 0.9064977765083313 + 0.001 * 7.107950687408447
Epoch 160, val loss: 1.0574619770050049
Epoch 170, training loss: 0.8258829712867737 = 0.8187780976295471 + 0.001 * 7.1048665046691895
Epoch 170, val loss: 0.992439329624176
Epoch 180, training loss: 0.7477714419364929 = 0.7406717538833618 + 0.001 * 7.0996880531311035
Epoch 180, val loss: 0.9366323947906494
Epoch 190, training loss: 0.6786983609199524 = 0.6716070175170898 + 0.001 * 7.091314315795898
Epoch 190, val loss: 0.8896654844284058
Epoch 200, training loss: 0.6168975830078125 = 0.6098193526268005 + 0.001 * 7.0782270431518555
Epoch 200, val loss: 0.8503056168556213
Epoch 210, training loss: 0.5604512691497803 = 0.553394615650177 + 0.001 * 7.056644916534424
Epoch 210, val loss: 0.8169614672660828
Epoch 220, training loss: 0.5079803466796875 = 0.5009593367576599 + 0.001 * 7.020988464355469
Epoch 220, val loss: 0.7889584302902222
Epoch 230, training loss: 0.45894956588745117 = 0.4519463777542114 + 0.001 * 7.003186225891113
Epoch 230, val loss: 0.7661879658699036
Epoch 240, training loss: 0.41320523619651794 = 0.40624210238456726 + 0.001 * 6.963133811950684
Epoch 240, val loss: 0.7482308745384216
Epoch 250, training loss: 0.3706057667732239 = 0.3636535108089447 + 0.001 * 6.952251434326172
Epoch 250, val loss: 0.7345185875892639
Epoch 260, training loss: 0.33117061853408813 = 0.3242260217666626 + 0.001 * 6.944588661193848
Epoch 260, val loss: 0.7244454026222229
Epoch 270, training loss: 0.2949177324771881 = 0.2879781723022461 + 0.001 * 6.939550876617432
Epoch 270, val loss: 0.7176275253295898
Epoch 280, training loss: 0.2619171142578125 = 0.2549801170825958 + 0.001 * 6.93699312210083
Epoch 280, val loss: 0.7140645980834961
Epoch 290, training loss: 0.23225876688957214 = 0.2253263294696808 + 0.001 * 6.932438373565674
Epoch 290, val loss: 0.7133663296699524
Epoch 300, training loss: 0.2060007005929947 = 0.19907143712043762 + 0.001 * 6.929261684417725
Epoch 300, val loss: 0.7154690027236938
Epoch 310, training loss: 0.18304197490215302 = 0.1761169284582138 + 0.001 * 6.925053119659424
Epoch 310, val loss: 0.720197856426239
Epoch 320, training loss: 0.16311825811862946 = 0.15619364380836487 + 0.001 * 6.92460823059082
Epoch 320, val loss: 0.7273877263069153
Epoch 330, training loss: 0.1458604633808136 = 0.13893839716911316 + 0.001 * 6.922070503234863
Epoch 330, val loss: 0.7367826104164124
Epoch 340, training loss: 0.13088294863700867 = 0.12396898120641708 + 0.001 * 6.913972854614258
Epoch 340, val loss: 0.7480580806732178
Epoch 350, training loss: 0.11784923076629639 = 0.11094427108764648 + 0.001 * 6.904956340789795
Epoch 350, val loss: 0.7610305547714233
Epoch 360, training loss: 0.10649589449167252 = 0.09955795854330063 + 0.001 * 6.937934875488281
Epoch 360, val loss: 0.7752891778945923
Epoch 370, training loss: 0.09646520763635635 = 0.08955956995487213 + 0.001 * 6.905639171600342
Epoch 370, val loss: 0.7905021905899048
Epoch 380, training loss: 0.08764510601758957 = 0.08075013756752014 + 0.001 * 6.894970893859863
Epoch 380, val loss: 0.8064970970153809
Epoch 390, training loss: 0.07984980195760727 = 0.07296242564916611 + 0.001 * 6.887379169464111
Epoch 390, val loss: 0.8229690194129944
Epoch 400, training loss: 0.07295539975166321 = 0.06606698781251907 + 0.001 * 6.8884124755859375
Epoch 400, val loss: 0.8397744297981262
Epoch 410, training loss: 0.0668334886431694 = 0.05995408073067665 + 0.001 * 6.87940788269043
Epoch 410, val loss: 0.8568257689476013
Epoch 420, training loss: 0.06140259653329849 = 0.05452355369925499 + 0.001 * 6.879040718078613
Epoch 420, val loss: 0.874012291431427
Epoch 430, training loss: 0.0565585121512413 = 0.049687501043081284 + 0.001 * 6.871010780334473
Epoch 430, val loss: 0.891194760799408
Epoch 440, training loss: 0.052247967571020126 = 0.04537602514028549 + 0.001 * 6.871942043304443
Epoch 440, val loss: 0.9083361625671387
Epoch 450, training loss: 0.048387542366981506 = 0.041523076593875885 + 0.001 * 6.864465236663818
Epoch 450, val loss: 0.9254549145698547
Epoch 460, training loss: 0.044931620359420776 = 0.03807167708873749 + 0.001 * 6.859941005706787
Epoch 460, val loss: 0.9424092173576355
Epoch 470, training loss: 0.04184173420071602 = 0.03497805446386337 + 0.001 * 6.863679885864258
Epoch 470, val loss: 0.9592018127441406
Epoch 480, training loss: 0.039053428918123245 = 0.03219848871231079 + 0.001 * 6.854940414428711
Epoch 480, val loss: 0.9758852124214172
Epoch 490, training loss: 0.03656300902366638 = 0.029691919684410095 + 0.001 * 6.871090412139893
Epoch 490, val loss: 0.992436408996582
Epoch 500, training loss: 0.034283384680747986 = 0.02742644026875496 + 0.001 * 6.856945991516113
Epoch 500, val loss: 1.0088155269622803
Epoch 510, training loss: 0.03222033381462097 = 0.025375891476869583 + 0.001 * 6.844443321228027
Epoch 510, val loss: 1.025010108947754
Epoch 520, training loss: 0.0303809717297554 = 0.023518655449151993 + 0.001 * 6.8623151779174805
Epoch 520, val loss: 1.0409501791000366
Epoch 530, training loss: 0.028675507754087448 = 0.021835170686244965 + 0.001 * 6.840336799621582
Epoch 530, val loss: 1.0566847324371338
Epoch 540, training loss: 0.027155060321092606 = 0.020308371633291245 + 0.001 * 6.846688270568848
Epoch 540, val loss: 1.0721062421798706
Epoch 550, training loss: 0.025774654000997543 = 0.01892256550490856 + 0.001 * 6.852087497711182
Epoch 550, val loss: 1.0872457027435303
Epoch 560, training loss: 0.024513857439160347 = 0.0176630150526762 + 0.001 * 6.850841999053955
Epoch 560, val loss: 1.1020468473434448
Epoch 570, training loss: 0.02334553748369217 = 0.016516903415322304 + 0.001 * 6.828633785247803
Epoch 570, val loss: 1.1165217161178589
Epoch 580, training loss: 0.02230004593729973 = 0.015472629107534885 + 0.001 * 6.82741641998291
Epoch 580, val loss: 1.1306538581848145
Epoch 590, training loss: 0.021348275244235992 = 0.014519737102091312 + 0.001 * 6.8285369873046875
Epoch 590, val loss: 1.1445083618164062
Epoch 600, training loss: 0.020466836169362068 = 0.01364884339272976 + 0.001 * 6.817992210388184
Epoch 600, val loss: 1.1579798460006714
Epoch 610, training loss: 0.01967722363770008 = 0.012851021252572536 + 0.001 * 6.826202392578125
Epoch 610, val loss: 1.1711130142211914
Epoch 620, training loss: 0.018945738673210144 = 0.012119467370212078 + 0.001 * 6.826271057128906
Epoch 620, val loss: 1.1839044094085693
Epoch 630, training loss: 0.018277373164892197 = 0.01144745945930481 + 0.001 * 6.829914093017578
Epoch 630, val loss: 1.196352481842041
Epoch 640, training loss: 0.01764073222875595 = 0.010829288512468338 + 0.001 * 6.8114423751831055
Epoch 640, val loss: 1.208457112312317
Epoch 650, training loss: 0.017125651240348816 = 0.01025980245321989 + 0.001 * 6.865847587585449
Epoch 650, val loss: 1.2203062772750854
Epoch 660, training loss: 0.016551656648516655 = 0.009734267368912697 + 0.001 * 6.817388534545898
Epoch 660, val loss: 1.2318083047866821
Epoch 670, training loss: 0.01608292944729328 = 0.009248419664800167 + 0.001 * 6.834508895874023
Epoch 670, val loss: 1.2430144548416138
Epoch 680, training loss: 0.015612658113241196 = 0.00879825558513403 + 0.001 * 6.8144025802612305
Epoch 680, val loss: 1.2539591789245605
Epoch 690, training loss: 0.015178028494119644 = 0.008380439132452011 + 0.001 * 6.79758882522583
Epoch 690, val loss: 1.2646119594573975
Epoch 700, training loss: 0.014810627326369286 = 0.007992066442966461 + 0.001 * 6.818560600280762
Epoch 700, val loss: 1.2750369310379028
Epoch 710, training loss: 0.014438681304454803 = 0.007629900239408016 + 0.001 * 6.808781147003174
Epoch 710, val loss: 1.2852137088775635
Epoch 720, training loss: 0.014106997288763523 = 0.007290547713637352 + 0.001 * 6.816449165344238
Epoch 720, val loss: 1.2951382398605347
Epoch 730, training loss: 0.013776829466223717 = 0.00697278929874301 + 0.001 * 6.80403995513916
Epoch 730, val loss: 1.3047480583190918
Epoch 740, training loss: 0.013483811169862747 = 0.0066760932095348835 + 0.001 * 6.807717800140381
Epoch 740, val loss: 1.3142964839935303
Epoch 750, training loss: 0.013189231976866722 = 0.006397951859980822 + 0.001 * 6.791279315948486
Epoch 750, val loss: 1.3235045671463013
Epoch 760, training loss: 0.012933161109685898 = 0.006137031130492687 + 0.001 * 6.79612922668457
Epoch 760, val loss: 1.3325257301330566
Epoch 770, training loss: 0.012670437805354595 = 0.005892122630029917 + 0.001 * 6.77831506729126
Epoch 770, val loss: 1.3413138389587402
Epoch 780, training loss: 0.012455640360713005 = 0.005662075709551573 + 0.001 * 6.793564319610596
Epoch 780, val loss: 1.3499672412872314
Epoch 790, training loss: 0.012242384254932404 = 0.005445785820484161 + 0.001 * 6.796597957611084
Epoch 790, val loss: 1.358372688293457
Epoch 800, training loss: 0.012034011073410511 = 0.005242020823061466 + 0.001 * 6.791989803314209
Epoch 800, val loss: 1.3666237592697144
Epoch 810, training loss: 0.011843336746096611 = 0.005049996078014374 + 0.001 * 6.793339729309082
Epoch 810, val loss: 1.3746618032455444
Epoch 820, training loss: 0.011642467230558395 = 0.004868797492235899 + 0.001 * 6.7736687660217285
Epoch 820, val loss: 1.3825390338897705
Epoch 830, training loss: 0.011478174477815628 = 0.0046977330930531025 + 0.001 * 6.7804412841796875
Epoch 830, val loss: 1.3902009725570679
Epoch 840, training loss: 0.011310487985610962 = 0.004536098334938288 + 0.001 * 6.774389266967773
Epoch 840, val loss: 1.3977378606796265
Epoch 850, training loss: 0.01115476619452238 = 0.004383224528282881 + 0.001 * 6.771541118621826
Epoch 850, val loss: 1.4050486087799072
Epoch 860, training loss: 0.011001449078321457 = 0.004238506779074669 + 0.001 * 6.762941837310791
Epoch 860, val loss: 1.4122319221496582
Epoch 870, training loss: 0.010881396010518074 = 0.004101302474737167 + 0.001 * 6.780093669891357
Epoch 870, val loss: 1.4192498922348022
Epoch 880, training loss: 0.010739791207015514 = 0.003971156198531389 + 0.001 * 6.768634796142578
Epoch 880, val loss: 1.4260891675949097
Epoch 890, training loss: 0.010611893609166145 = 0.0038476884365081787 + 0.001 * 6.764205455780029
Epoch 890, val loss: 1.4328383207321167
Epoch 900, training loss: 0.010485153645277023 = 0.003730509663000703 + 0.001 * 6.754643440246582
Epoch 900, val loss: 1.439359188079834
Epoch 910, training loss: 0.010375039651989937 = 0.0036191153340041637 + 0.001 * 6.755924224853516
Epoch 910, val loss: 1.4457751512527466
Epoch 920, training loss: 0.010281486436724663 = 0.0035132141783833504 + 0.001 * 6.768272399902344
Epoch 920, val loss: 1.4520442485809326
Epoch 930, training loss: 0.01016269437968731 = 0.0034124511294066906 + 0.001 * 6.750243186950684
Epoch 930, val loss: 1.4581868648529053
Epoch 940, training loss: 0.010073186829686165 = 0.0033164420165121555 + 0.001 * 6.756743907928467
Epoch 940, val loss: 1.4641999006271362
Epoch 950, training loss: 0.009974955581128597 = 0.0032249349169433117 + 0.001 * 6.750020503997803
Epoch 950, val loss: 1.4700655937194824
Epoch 960, training loss: 0.009898144751787186 = 0.003137677675113082 + 0.001 * 6.760466575622559
Epoch 960, val loss: 1.4758093357086182
Epoch 970, training loss: 0.009801039472222328 = 0.003054441884160042 + 0.001 * 6.746596813201904
Epoch 970, val loss: 1.4814373254776
Epoch 980, training loss: 0.009718834422528744 = 0.00297495536506176 + 0.001 * 6.7438788414001465
Epoch 980, val loss: 1.4869006872177124
Epoch 990, training loss: 0.009644126519560814 = 0.002898981561884284 + 0.001 * 6.745144367218018
Epoch 990, val loss: 1.4923005104064941
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8303
Flip ASR: 0.7956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9613685607910156 = 1.952994704246521 + 0.001 * 8.373846054077148
Epoch 0, val loss: 1.9495930671691895
Epoch 10, training loss: 1.9510494470596313 = 1.9426757097244263 + 0.001 * 8.373740196228027
Epoch 10, val loss: 1.9397242069244385
Epoch 20, training loss: 1.9378554821014404 = 1.9294819831848145 + 0.001 * 8.373444557189941
Epoch 20, val loss: 1.9266854524612427
Epoch 30, training loss: 1.9188435077667236 = 1.9104706048965454 + 0.001 * 8.372875213623047
Epoch 30, val loss: 1.9076719284057617
Epoch 40, training loss: 1.8904558420181274 = 1.8820841312408447 + 0.001 * 8.371759414672852
Epoch 40, val loss: 1.8796074390411377
Epoch 50, training loss: 1.8507022857666016 = 1.8423330783843994 + 0.001 * 8.369162559509277
Epoch 50, val loss: 1.8417905569076538
Epoch 60, training loss: 1.804901123046875 = 1.7965401411056519 + 0.001 * 8.360942840576172
Epoch 60, val loss: 1.8010133504867554
Epoch 70, training loss: 1.7622944116592407 = 1.7539734840393066 + 0.001 * 8.320940971374512
Epoch 70, val loss: 1.76375412940979
Epoch 80, training loss: 1.710913062095642 = 1.7028553485870361 + 0.001 * 8.057727813720703
Epoch 80, val loss: 1.7160981893539429
Epoch 90, training loss: 1.6402970552444458 = 1.6326407194137573 + 0.001 * 7.656344413757324
Epoch 90, val loss: 1.6535066366195679
Epoch 100, training loss: 1.5480782985687256 = 1.5406898260116577 + 0.001 * 7.388485431671143
Epoch 100, val loss: 1.576177954673767
Epoch 110, training loss: 1.4404819011688232 = 1.4331835508346558 + 0.001 * 7.298300743103027
Epoch 110, val loss: 1.4874902963638306
Epoch 120, training loss: 1.328136682510376 = 1.3208788633346558 + 0.001 * 7.2578582763671875
Epoch 120, val loss: 1.3962585926055908
Epoch 130, training loss: 1.2157591581344604 = 1.2085500955581665 + 0.001 * 7.2090301513671875
Epoch 130, val loss: 1.3075616359710693
Epoch 140, training loss: 1.103776216506958 = 1.0966171026229858 + 0.001 * 7.159144878387451
Epoch 140, val loss: 1.221161961555481
Epoch 150, training loss: 0.9953727126121521 = 0.9882441163063049 + 0.001 * 7.12856912612915
Epoch 150, val loss: 1.1395403146743774
Epoch 160, training loss: 0.8960162401199341 = 0.8888967633247375 + 0.001 * 7.119463920593262
Epoch 160, val loss: 1.0674753189086914
Epoch 170, training loss: 0.808880090713501 = 0.8017693161964417 + 0.001 * 7.110791206359863
Epoch 170, val loss: 1.007707118988037
Epoch 180, training loss: 0.7332522869110107 = 0.726148247718811 + 0.001 * 7.104064464569092
Epoch 180, val loss: 0.959116518497467
Epoch 190, training loss: 0.6665741205215454 = 0.6594773530960083 + 0.001 * 7.096761703491211
Epoch 190, val loss: 0.919754147529602
Epoch 200, training loss: 0.606575071811676 = 0.5994864702224731 + 0.001 * 7.088594436645508
Epoch 200, val loss: 0.8872512578964233
Epoch 210, training loss: 0.5520225167274475 = 0.5449447631835938 + 0.001 * 7.077747344970703
Epoch 210, val loss: 0.8597131371498108
Epoch 220, training loss: 0.5024210810661316 = 0.4953596591949463 + 0.001 * 7.061445713043213
Epoch 220, val loss: 0.8365660905838013
Epoch 230, training loss: 0.45746544003486633 = 0.450420618057251 + 0.001 * 7.044827938079834
Epoch 230, val loss: 0.8174224495887756
Epoch 240, training loss: 0.4166165888309479 = 0.40960055589675903 + 0.001 * 7.016019344329834
Epoch 240, val loss: 0.8027850389480591
Epoch 250, training loss: 0.37917935848236084 = 0.37218713760375977 + 0.001 * 6.992232799530029
Epoch 250, val loss: 0.7924771308898926
Epoch 260, training loss: 0.34440404176712036 = 0.3374200761318207 + 0.001 * 6.983975410461426
Epoch 260, val loss: 0.785762369632721
Epoch 270, training loss: 0.31172478199005127 = 0.30475324392318726 + 0.001 * 6.9715256690979
Epoch 270, val loss: 0.7819157838821411
Epoch 280, training loss: 0.28087958693504333 = 0.2739151120185852 + 0.001 * 6.9644694328308105
Epoch 280, val loss: 0.7806472778320312
Epoch 290, training loss: 0.2519397735595703 = 0.24497781693935394 + 0.001 * 6.9619460105896
Epoch 290, val loss: 0.7818928956985474
Epoch 300, training loss: 0.2251388132572174 = 0.21817898750305176 + 0.001 * 6.959822177886963
Epoch 300, val loss: 0.7856627106666565
Epoch 310, training loss: 0.20082806050777435 = 0.1938701868057251 + 0.001 * 6.957875728607178
Epoch 310, val loss: 0.7919264435768127
Epoch 320, training loss: 0.1791304647922516 = 0.17217466235160828 + 0.001 * 6.955801010131836
Epoch 320, val loss: 0.8004509210586548
Epoch 330, training loss: 0.15992522239685059 = 0.15297093987464905 + 0.001 * 6.954275131225586
Epoch 330, val loss: 0.8105887174606323
Epoch 340, training loss: 0.14289307594299316 = 0.13593915104866028 + 0.001 * 6.9539313316345215
Epoch 340, val loss: 0.8215867877006531
Epoch 350, training loss: 0.12778735160827637 = 0.12083042412996292 + 0.001 * 6.9569196701049805
Epoch 350, val loss: 0.8336395621299744
Epoch 360, training loss: 0.11446640640497208 = 0.10751191526651382 + 0.001 * 6.954493522644043
Epoch 360, val loss: 0.8464981913566589
Epoch 370, training loss: 0.10274025797843933 = 0.09578604251146317 + 0.001 * 6.954214096069336
Epoch 370, val loss: 0.8597552180290222
Epoch 380, training loss: 0.09243205934762955 = 0.08548004180192947 + 0.001 * 6.952014446258545
Epoch 380, val loss: 0.8736531138420105
Epoch 390, training loss: 0.08339173346757889 = 0.07643891125917435 + 0.001 * 6.952820301055908
Epoch 390, val loss: 0.8878679871559143
Epoch 400, training loss: 0.0754687711596489 = 0.06851623207330704 + 0.001 * 6.952535629272461
Epoch 400, val loss: 0.9022132754325867
Epoch 410, training loss: 0.06851426512002945 = 0.06156511977314949 + 0.001 * 6.949145317077637
Epoch 410, val loss: 0.9164395332336426
Epoch 420, training loss: 0.06239629164338112 = 0.055449847131967545 + 0.001 * 6.946442604064941
Epoch 420, val loss: 0.9304627180099487
Epoch 430, training loss: 0.05700363963842392 = 0.05005835369229317 + 0.001 * 6.945283889770508
Epoch 430, val loss: 0.9442099332809448
Epoch 440, training loss: 0.052242252975702286 = 0.045294519513845444 + 0.001 * 6.9477338790893555
Epoch 440, val loss: 0.9575485587120056
Epoch 450, training loss: 0.04802175611257553 = 0.04107779264450073 + 0.001 * 6.943962097167969
Epoch 450, val loss: 0.9705328941345215
Epoch 460, training loss: 0.04428652301430702 = 0.037340570241212845 + 0.001 * 6.945950984954834
Epoch 460, val loss: 0.9831705689430237
Epoch 470, training loss: 0.04096340015530586 = 0.03402450308203697 + 0.001 * 6.938897132873535
Epoch 470, val loss: 0.9953816533088684
Epoch 480, training loss: 0.03801751881837845 = 0.03107948787510395 + 0.001 * 6.938031196594238
Epoch 480, val loss: 1.007249355316162
Epoch 490, training loss: 0.03539729863405228 = 0.028462126851081848 + 0.001 * 6.935173034667969
Epoch 490, val loss: 1.0187604427337646
Epoch 500, training loss: 0.033058423548936844 = 0.026129573583602905 + 0.001 * 6.9288506507873535
Epoch 500, val loss: 1.029915452003479
Epoch 510, training loss: 0.030974555760622025 = 0.024044029414653778 + 0.001 * 6.930525779724121
Epoch 510, val loss: 1.0408025979995728
Epoch 520, training loss: 0.02909974940121174 = 0.022171085700392723 + 0.001 * 6.9286627769470215
Epoch 520, val loss: 1.0513839721679688
Epoch 530, training loss: 0.027403926476836205 = 0.02048177644610405 + 0.001 * 6.922150135040283
Epoch 530, val loss: 1.0617589950561523
Epoch 540, training loss: 0.025916166603565216 = 0.018951570615172386 + 0.001 * 6.964595794677734
Epoch 540, val loss: 1.0717941522598267
Epoch 550, training loss: 0.024480432271957397 = 0.017563387751579285 + 0.001 * 6.9170451164245605
Epoch 550, val loss: 1.0816417932510376
Epoch 560, training loss: 0.02321806363761425 = 0.01630319096148014 + 0.001 * 6.914872646331787
Epoch 560, val loss: 1.091283917427063
Epoch 570, training loss: 0.022070951759815216 = 0.015159744769334793 + 0.001 * 6.911207675933838
Epoch 570, val loss: 1.1007318496704102
Epoch 580, training loss: 0.021025579422712326 = 0.014122278429567814 + 0.001 * 6.903300762176514
Epoch 580, val loss: 1.1099298000335693
Epoch 590, training loss: 0.0200958251953125 = 0.01318075880408287 + 0.001 * 6.915067195892334
Epoch 590, val loss: 1.1189285516738892
Epoch 600, training loss: 0.019229326397180557 = 0.012327015399932861 + 0.001 * 6.902310371398926
Epoch 600, val loss: 1.1277077198028564
Epoch 610, training loss: 0.01845480501651764 = 0.01155159343034029 + 0.001 * 6.903212070465088
Epoch 610, val loss: 1.1362959146499634
Epoch 620, training loss: 0.01775270141661167 = 0.010845760814845562 + 0.001 * 6.90693998336792
Epoch 620, val loss: 1.1446607112884521
Epoch 630, training loss: 0.017117656767368317 = 0.01020239107310772 + 0.001 * 6.9152655601501465
Epoch 630, val loss: 1.1528033018112183
Epoch 640, training loss: 0.01649675890803337 = 0.009615117684006691 + 0.001 * 6.881640434265137
Epoch 640, val loss: 1.160790205001831
Epoch 650, training loss: 0.01596304215490818 = 0.00907786563038826 + 0.001 * 6.885175704956055
Epoch 650, val loss: 1.1685489416122437
Epoch 660, training loss: 0.01545732282102108 = 0.008585323579609394 + 0.001 * 6.871998310089111
Epoch 660, val loss: 1.1761542558670044
Epoch 670, training loss: 0.015031566843390465 = 0.008132710121572018 + 0.001 * 6.8988566398620605
Epoch 670, val loss: 1.183535099029541
Epoch 680, training loss: 0.0145968496799469 = 0.0077163903042674065 + 0.001 * 6.880459308624268
Epoch 680, val loss: 1.190755844116211
Epoch 690, training loss: 0.014219405129551888 = 0.0073324586264789104 + 0.001 * 6.886945724487305
Epoch 690, val loss: 1.1978108882904053
Epoch 700, training loss: 0.013861734420061111 = 0.006977769546210766 + 0.001 * 6.883965015411377
Epoch 700, val loss: 1.2046852111816406
Epoch 710, training loss: 0.013517547398805618 = 0.006649504415690899 + 0.001 * 6.8680419921875
Epoch 710, val loss: 1.2113858461380005
Epoch 720, training loss: 0.013222134672105312 = 0.006345055066049099 + 0.001 * 6.877079486846924
Epoch 720, val loss: 1.2179388999938965
Epoch 730, training loss: 0.012929781340062618 = 0.006062244530767202 + 0.001 * 6.867536544799805
Epoch 730, val loss: 1.224334716796875
Epoch 740, training loss: 0.01265936903655529 = 0.005799129139631987 + 0.001 * 6.8602399826049805
Epoch 740, val loss: 1.2305784225463867
Epoch 750, training loss: 0.012402281165122986 = 0.005553662311285734 + 0.001 * 6.848618507385254
Epoch 750, val loss: 1.2366611957550049
Epoch 760, training loss: 0.012180736288428307 = 0.005324174650013447 + 0.001 * 6.856561183929443
Epoch 760, val loss: 1.2426625490188599
Epoch 770, training loss: 0.01195139903575182 = 0.005108881276100874 + 0.001 * 6.842517375946045
Epoch 770, val loss: 1.2484779357910156
Epoch 780, training loss: 0.011740580201148987 = 0.004906266462057829 + 0.001 * 6.834313869476318
Epoch 780, val loss: 1.2542275190353394
Epoch 790, training loss: 0.011552654206752777 = 0.004714741837233305 + 0.001 * 6.837912082672119
Epoch 790, val loss: 1.2598907947540283
Epoch 800, training loss: 0.011385171674191952 = 0.004533173516392708 + 0.001 * 6.8519978523254395
Epoch 800, val loss: 1.265503168106079
Epoch 810, training loss: 0.011185579001903534 = 0.00436073262244463 + 0.001 * 6.8248467445373535
Epoch 810, val loss: 1.271025538444519
Epoch 820, training loss: 0.01103951595723629 = 0.004196743480861187 + 0.001 * 6.842772006988525
Epoch 820, val loss: 1.2764731645584106
Epoch 830, training loss: 0.010869305580854416 = 0.004040917847305536 + 0.001 * 6.82838773727417
Epoch 830, val loss: 1.2818832397460938
Epoch 840, training loss: 0.010747786611318588 = 0.003892825683578849 + 0.001 * 6.854960918426514
Epoch 840, val loss: 1.2872412204742432
Epoch 850, training loss: 0.010574813932180405 = 0.0037522795610129833 + 0.001 * 6.822534561157227
Epoch 850, val loss: 1.292494535446167
Epoch 860, training loss: 0.010425884276628494 = 0.0036188701633363962 + 0.001 * 6.807013511657715
Epoch 860, val loss: 1.2977094650268555
Epoch 870, training loss: 0.010308805853128433 = 0.003492233343422413 + 0.001 * 6.8165717124938965
Epoch 870, val loss: 1.302841067314148
Epoch 880, training loss: 0.01020128559321165 = 0.003372231964021921 + 0.001 * 6.8290534019470215
Epoch 880, val loss: 1.3078819513320923
Epoch 890, training loss: 0.010068747214972973 = 0.0032584897708147764 + 0.001 * 6.810257434844971
Epoch 890, val loss: 1.312839388847351
Epoch 900, training loss: 0.009952904656529427 = 0.0031506100203841925 + 0.001 * 6.8022942543029785
Epoch 900, val loss: 1.3177348375320435
Epoch 910, training loss: 0.009850525297224522 = 0.003048322396352887 + 0.001 * 6.802202224731445
Epoch 910, val loss: 1.3224990367889404
Epoch 920, training loss: 0.009764760732650757 = 0.002951284870505333 + 0.001 * 6.813476085662842
Epoch 920, val loss: 1.32720947265625
Epoch 930, training loss: 0.009647923521697521 = 0.002859153551980853 + 0.001 * 6.788769721984863
Epoch 930, val loss: 1.3318281173706055
Epoch 940, training loss: 0.009576131589710712 = 0.0027716592885553837 + 0.001 * 6.804471969604492
Epoch 940, val loss: 1.3363627195358276
Epoch 950, training loss: 0.009485035203397274 = 0.0026885862462222576 + 0.001 * 6.796448707580566
Epoch 950, val loss: 1.3408055305480957
Epoch 960, training loss: 0.009410172700881958 = 0.0026096294168382883 + 0.001 * 6.800543308258057
Epoch 960, val loss: 1.3451745510101318
Epoch 970, training loss: 0.00934695452451706 = 0.002534599509090185 + 0.001 * 6.812354564666748
Epoch 970, val loss: 1.3494151830673218
Epoch 980, training loss: 0.009269747883081436 = 0.0024632883723825216 + 0.001 * 6.806459426879883
Epoch 980, val loss: 1.3536593914031982
Epoch 990, training loss: 0.009196682833135128 = 0.0023954007774591446 + 0.001 * 6.801281929016113
Epoch 990, val loss: 1.3577501773834229
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.8007
Flip ASR: 0.7600/225 nodes
The final ASR:0.73924, 0.10852, Accuracy:0.78889, 0.02688
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11624])
remove edge: torch.Size([2, 9372])
updated graph: torch.Size([2, 10440])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9594
Flip ASR: 0.9511/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97294, 0.01058, Accuracy:0.83333, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.945765733718872 = 1.937391757965088 + 0.001 * 8.373918533325195
Epoch 0, val loss: 1.9305883646011353
Epoch 10, training loss: 1.9356868267059326 = 1.927312970161438 + 0.001 * 8.373884201049805
Epoch 10, val loss: 1.920307993888855
Epoch 20, training loss: 1.923560380935669 = 1.9151866436004639 + 0.001 * 8.373730659484863
Epoch 20, val loss: 1.9075655937194824
Epoch 30, training loss: 1.906605839729309 = 1.8982324600219727 + 0.001 * 8.373353958129883
Epoch 30, val loss: 1.8896597623825073
Epoch 40, training loss: 1.881852626800537 = 1.873480200767517 + 0.001 * 8.372400283813477
Epoch 40, val loss: 1.8640035390853882
Epoch 50, training loss: 1.8479489088058472 = 1.8395793437957764 + 0.001 * 8.369538307189941
Epoch 50, val loss: 1.8309743404388428
Epoch 60, training loss: 1.8105374574661255 = 1.802178978919983 + 0.001 * 8.3585205078125
Epoch 60, val loss: 1.7992008924484253
Epoch 70, training loss: 1.7746917009353638 = 1.76638925075531 + 0.001 * 8.302392959594727
Epoch 70, val loss: 1.7719184160232544
Epoch 80, training loss: 1.7267645597457886 = 1.7187987565994263 + 0.001 * 7.965798854827881
Epoch 80, val loss: 1.7324644327163696
Epoch 90, training loss: 1.6606158018112183 = 1.6527976989746094 + 0.001 * 7.818079948425293
Epoch 90, val loss: 1.6759018898010254
Epoch 100, training loss: 1.5744835138320923 = 1.5667423009872437 + 0.001 * 7.741190433502197
Epoch 100, val loss: 1.60286283493042
Epoch 110, training loss: 1.4780607223510742 = 1.4704234600067139 + 0.001 * 7.637292861938477
Epoch 110, val loss: 1.5224069356918335
Epoch 120, training loss: 1.379286527633667 = 1.3718539476394653 + 0.001 * 7.432595729827881
Epoch 120, val loss: 1.4437967538833618
Epoch 130, training loss: 1.2779655456542969 = 1.2706934213638306 + 0.001 * 7.272178649902344
Epoch 130, val loss: 1.366721510887146
Epoch 140, training loss: 1.1718103885650635 = 1.164600133895874 + 0.001 * 7.210248947143555
Epoch 140, val loss: 1.288462519645691
Epoch 150, training loss: 1.0620369911193848 = 1.0548875331878662 + 0.001 * 7.1495041847229
Epoch 150, val loss: 1.2083417177200317
Epoch 160, training loss: 0.95384281873703 = 0.9467165470123291 + 0.001 * 7.126277923583984
Epoch 160, val loss: 1.1298305988311768
Epoch 170, training loss: 0.8539745807647705 = 0.8468664884567261 + 0.001 * 7.1081037521362305
Epoch 170, val loss: 1.0582823753356934
Epoch 180, training loss: 0.7669798731803894 = 0.7598928213119507 + 0.001 * 7.087033271789551
Epoch 180, val loss: 0.997756838798523
Epoch 190, training loss: 0.6926591396331787 = 0.6855955719947815 + 0.001 * 7.063559055328369
Epoch 190, val loss: 0.9482104182243347
Epoch 200, training loss: 0.6274902820587158 = 0.6204534769058228 + 0.001 * 7.0367937088012695
Epoch 200, val loss: 0.9065398573875427
Epoch 210, training loss: 0.5683130621910095 = 0.5612974166870117 + 0.001 * 7.01563835144043
Epoch 210, val loss: 0.8705004453659058
Epoch 220, training loss: 0.5141995549201965 = 0.5071890950202942 + 0.001 * 7.010437488555908
Epoch 220, val loss: 0.8397926092147827
Epoch 230, training loss: 0.4655291438102722 = 0.45852020382881165 + 0.001 * 7.008938789367676
Epoch 230, val loss: 0.8158758878707886
Epoch 240, training loss: 0.4218853712081909 = 0.4148777425289154 + 0.001 * 7.007640838623047
Epoch 240, val loss: 0.7986346483230591
Epoch 250, training loss: 0.38173869252204895 = 0.3747321367263794 + 0.001 * 7.0065531730651855
Epoch 250, val loss: 0.7864433526992798
Epoch 260, training loss: 0.3436400890350342 = 0.336635023355484 + 0.001 * 7.0050506591796875
Epoch 260, val loss: 0.7775124907493591
Epoch 270, training loss: 0.306918203830719 = 0.29991456866264343 + 0.001 * 7.003625392913818
Epoch 270, val loss: 0.770982563495636
Epoch 280, training loss: 0.27177977561950684 = 0.2647772431373596 + 0.001 * 7.002523899078369
Epoch 280, val loss: 0.7663951516151428
Epoch 290, training loss: 0.2389410436153412 = 0.23193927109241486 + 0.001 * 7.001770973205566
Epoch 290, val loss: 0.7641744017601013
Epoch 300, training loss: 0.20920953154563904 = 0.20220820605754852 + 0.001 * 7.001330375671387
Epoch 300, val loss: 0.7644251585006714
Epoch 310, training loss: 0.1831287443637848 = 0.17612546682357788 + 0.001 * 7.003271102905273
Epoch 310, val loss: 0.7675006985664368
Epoch 320, training loss: 0.16077759861946106 = 0.1537754088640213 + 0.001 * 7.002191066741943
Epoch 320, val loss: 0.7731471061706543
Epoch 330, training loss: 0.14183004200458527 = 0.1348278820514679 + 0.001 * 7.002166271209717
Epoch 330, val loss: 0.7810238599777222
Epoch 340, training loss: 0.12578636407852173 = 0.11878370493650436 + 0.001 * 7.002661228179932
Epoch 340, val loss: 0.7906932830810547
Epoch 350, training loss: 0.11215359717607498 = 0.10514923185110092 + 0.001 * 7.00436544418335
Epoch 350, val loss: 0.8016873002052307
Epoch 360, training loss: 0.10049781948328018 = 0.09349343180656433 + 0.001 * 7.004387378692627
Epoch 360, val loss: 0.8136892914772034
Epoch 370, training loss: 0.09046144783496857 = 0.08345657587051392 + 0.001 * 7.004868507385254
Epoch 370, val loss: 0.8265057802200317
Epoch 380, training loss: 0.08175860345363617 = 0.07475341856479645 + 0.001 * 7.005187034606934
Epoch 380, val loss: 0.8400271534919739
Epoch 390, training loss: 0.07417018711566925 = 0.0671641007065773 + 0.001 * 7.006086826324463
Epoch 390, val loss: 0.8540964126586914
Epoch 400, training loss: 0.06752010434865952 = 0.06051439419388771 + 0.001 * 7.005707740783691
Epoch 400, val loss: 0.8685566782951355
Epoch 410, training loss: 0.06167251616716385 = 0.054666996002197266 + 0.001 * 7.005519866943359
Epoch 410, val loss: 0.8833267092704773
Epoch 420, training loss: 0.05651521682739258 = 0.04950851574540138 + 0.001 * 7.006699562072754
Epoch 420, val loss: 0.8981979489326477
Epoch 430, training loss: 0.05194908380508423 = 0.044943634420633316 + 0.001 * 7.00545072555542
Epoch 430, val loss: 0.9131430983543396
Epoch 440, training loss: 0.047892726957798004 = 0.040889836847782135 + 0.001 * 7.002887725830078
Epoch 440, val loss: 0.9280542135238647
Epoch 450, training loss: 0.04428725689649582 = 0.037281569093465805 + 0.001 * 7.005688667297363
Epoch 450, val loss: 0.9429746270179749
Epoch 460, training loss: 0.04106387123465538 = 0.03406457230448723 + 0.001 * 6.999297618865967
Epoch 460, val loss: 0.9578287601470947
Epoch 470, training loss: 0.03819260746240616 = 0.031194595620036125 + 0.001 * 6.99800968170166
Epoch 470, val loss: 0.9725634455680847
Epoch 480, training loss: 0.035626817494630814 = 0.028630582615733147 + 0.001 * 6.9962358474731445
Epoch 480, val loss: 0.9870597720146179
Epoch 490, training loss: 0.03335464373230934 = 0.026336923241615295 + 0.001 * 7.017719268798828
Epoch 490, val loss: 1.001325249671936
Epoch 500, training loss: 0.03127140924334526 = 0.024282313883304596 + 0.001 * 6.989095687866211
Epoch 500, val loss: 1.015334963798523
Epoch 510, training loss: 0.029422584921121597 = 0.022438785061240196 + 0.001 * 6.983799457550049
Epoch 510, val loss: 1.029037594795227
Epoch 520, training loss: 0.027772482484579086 = 0.020781559869647026 + 0.001 * 6.990922927856445
Epoch 520, val loss: 1.0424723625183105
Epoch 530, training loss: 0.026268471032381058 = 0.019289009273052216 + 0.001 * 6.979461669921875
Epoch 530, val loss: 1.0555983781814575
Epoch 540, training loss: 0.024915700778365135 = 0.017942050471901894 + 0.001 * 6.973649501800537
Epoch 540, val loss: 1.0683817863464355
Epoch 550, training loss: 0.023689227178692818 = 0.016723984852433205 + 0.001 * 6.9652419090271
Epoch 550, val loss: 1.080866813659668
Epoch 560, training loss: 0.022590145468711853 = 0.015620063990354538 + 0.001 * 6.970080375671387
Epoch 560, val loss: 1.0930414199829102
Epoch 570, training loss: 0.02157631143927574 = 0.014617502689361572 + 0.001 * 6.958808422088623
Epoch 570, val loss: 1.104936122894287
Epoch 580, training loss: 0.020683571696281433 = 0.01370509434491396 + 0.001 * 6.978477478027344
Epoch 580, val loss: 1.116508960723877
Epoch 590, training loss: 0.01981385238468647 = 0.012873049825429916 + 0.001 * 6.940802097320557
Epoch 590, val loss: 1.12778639793396
Epoch 600, training loss: 0.019056199118494987 = 0.012112806551158428 + 0.001 * 6.943392753601074
Epoch 600, val loss: 1.1387652158737183
Epoch 610, training loss: 0.01835680939257145 = 0.011416728608310223 + 0.001 * 6.9400811195373535
Epoch 610, val loss: 1.1494449377059937
Epoch 620, training loss: 0.01772094890475273 = 0.010778242722153664 + 0.001 * 6.942705154418945
Epoch 620, val loss: 1.1598669290542603
Epoch 630, training loss: 0.017114175483584404 = 0.010191488079726696 + 0.001 * 6.92268705368042
Epoch 630, val loss: 1.1700327396392822
Epoch 640, training loss: 0.016579454764723778 = 0.009651261381804943 + 0.001 * 6.928193092346191
Epoch 640, val loss: 1.1799259185791016
Epoch 650, training loss: 0.016063451766967773 = 0.009153048507869244 + 0.001 * 6.910402774810791
Epoch 650, val loss: 1.1895817518234253
Epoch 660, training loss: 0.015648245811462402 = 0.008692753501236439 + 0.001 * 6.955491065979004
Epoch 660, val loss: 1.1989935636520386
Epoch 670, training loss: 0.015163034200668335 = 0.008266736753284931 + 0.001 * 6.896297454833984
Epoch 670, val loss: 1.2081621885299683
Epoch 680, training loss: 0.01476526539772749 = 0.00787185039371252 + 0.001 * 6.893414497375488
Epoch 680, val loss: 1.217106819152832
Epoch 690, training loss: 0.014382198452949524 = 0.007505245506763458 + 0.001 * 6.876953125
Epoch 690, val loss: 1.2258315086364746
Epoch 700, training loss: 0.014067833311855793 = 0.007164305076003075 + 0.001 * 6.903527736663818
Epoch 700, val loss: 1.2343240976333618
Epoch 710, training loss: 0.013730193488299847 = 0.006846820004284382 + 0.001 * 6.883373260498047
Epoch 710, val loss: 1.2426648139953613
Epoch 720, training loss: 0.013417314738035202 = 0.006550739053636789 + 0.001 * 6.866574764251709
Epoch 720, val loss: 1.2507513761520386
Epoch 730, training loss: 0.01314048282802105 = 0.006274201441556215 + 0.001 * 6.866280555725098
Epoch 730, val loss: 1.2586908340454102
Epoch 740, training loss: 0.012867203913629055 = 0.006015603430569172 + 0.001 * 6.851600170135498
Epoch 740, val loss: 1.2664275169372559
Epoch 750, training loss: 0.01262237224727869 = 0.005773426033556461 + 0.001 * 6.8489460945129395
Epoch 750, val loss: 1.2739876508712769
Epoch 760, training loss: 0.012395030818879604 = 0.005546411965042353 + 0.001 * 6.848618507385254
Epoch 760, val loss: 1.2813689708709717
Epoch 770, training loss: 0.012177791446447372 = 0.0053333500400185585 + 0.001 * 6.844440937042236
Epoch 770, val loss: 1.2886079549789429
Epoch 780, training loss: 0.011986165307462215 = 0.0051331063732504845 + 0.001 * 6.853058815002441
Epoch 780, val loss: 1.2956523895263672
Epoch 790, training loss: 0.011787684634327888 = 0.004944678395986557 + 0.001 * 6.843006134033203
Epoch 790, val loss: 1.3025904893875122
Epoch 800, training loss: 0.011600254103541374 = 0.0047671678476035595 + 0.001 * 6.8330864906311035
Epoch 800, val loss: 1.3093444108963013
Epoch 810, training loss: 0.011455826461315155 = 0.004599742591381073 + 0.001 * 6.856082916259766
Epoch 810, val loss: 1.3159263134002686
Epoch 820, training loss: 0.011269545182585716 = 0.004441690165549517 + 0.001 * 6.827855110168457
Epoch 820, val loss: 1.3224055767059326
Epoch 830, training loss: 0.011149024590849876 = 0.004292350262403488 + 0.001 * 6.856673717498779
Epoch 830, val loss: 1.3286972045898438
Epoch 840, training loss: 0.010977412573993206 = 0.004151100292801857 + 0.001 * 6.826312065124512
Epoch 840, val loss: 1.3348760604858398
Epoch 850, training loss: 0.01086755283176899 = 0.004017336759716272 + 0.001 * 6.850215911865234
Epoch 850, val loss: 1.3408851623535156
Epoch 860, training loss: 0.010746633633971214 = 0.0038906217087060213 + 0.001 * 6.856011867523193
Epoch 860, val loss: 1.3467682600021362
Epoch 870, training loss: 0.010600167326629162 = 0.003770427778363228 + 0.001 * 6.829739093780518
Epoch 870, val loss: 1.3525137901306152
Epoch 880, training loss: 0.010459057986736298 = 0.003656342625617981 + 0.001 * 6.8027143478393555
Epoch 880, val loss: 1.3581321239471436
Epoch 890, training loss: 0.010354805737733841 = 0.0035479445941746235 + 0.001 * 6.806861400604248
Epoch 890, val loss: 1.3636386394500732
Epoch 900, training loss: 0.010255812667310238 = 0.003444859990850091 + 0.001 * 6.810952663421631
Epoch 900, val loss: 1.3690245151519775
Epoch 910, training loss: 0.010152801871299744 = 0.0033467751927673817 + 0.001 * 6.806025981903076
Epoch 910, val loss: 1.3743114471435547
Epoch 920, training loss: 0.010044755414128304 = 0.003253350732848048 + 0.001 * 6.791404724121094
Epoch 920, val loss: 1.379474401473999
Epoch 930, training loss: 0.009954368695616722 = 0.00316433422267437 + 0.001 * 6.790034294128418
Epoch 930, val loss: 1.38453209400177
Epoch 940, training loss: 0.009881251491606236 = 0.003079415764659643 + 0.001 * 6.801835536956787
Epoch 940, val loss: 1.3894991874694824
Epoch 950, training loss: 0.00978014525026083 = 0.002998368116095662 + 0.001 * 6.7817769050598145
Epoch 950, val loss: 1.3943589925765991
Epoch 960, training loss: 0.009733150713145733 = 0.0029209682252258062 + 0.001 * 6.8121819496154785
Epoch 960, val loss: 1.3991215229034424
Epoch 970, training loss: 0.009702702052891254 = 0.002846973482519388 + 0.001 * 6.8557281494140625
Epoch 970, val loss: 1.4037890434265137
Epoch 980, training loss: 0.00954265333712101 = 0.0027762341778725386 + 0.001 * 6.76641845703125
Epoch 980, val loss: 1.408370018005371
Epoch 990, training loss: 0.009492307901382446 = 0.002708554733544588 + 0.001 * 6.783752918243408
Epoch 990, val loss: 1.4128713607788086
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.4096
Flip ASR: 0.3067/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9349632263183594 = 1.9265892505645752 + 0.001 * 8.373927116394043
Epoch 0, val loss: 1.9215435981750488
Epoch 10, training loss: 1.9253897666931152 = 1.9170159101486206 + 0.001 * 8.373884201049805
Epoch 10, val loss: 1.9119929075241089
Epoch 20, training loss: 1.913716435432434 = 1.905342698097229 + 0.001 * 8.373716354370117
Epoch 20, val loss: 1.899770975112915
Epoch 30, training loss: 1.8973032236099243 = 1.888929843902588 + 0.001 * 8.373355865478516
Epoch 30, val loss: 1.882285237312317
Epoch 40, training loss: 1.8732317686080933 = 1.8648592233657837 + 0.001 * 8.372598648071289
Epoch 40, val loss: 1.8568952083587646
Epoch 50, training loss: 1.8393458127975464 = 1.83097505569458 + 0.001 * 8.370756149291992
Epoch 50, val loss: 1.8225709199905396
Epoch 60, training loss: 1.7979096174240112 = 1.7895445823669434 + 0.001 * 8.364995002746582
Epoch 60, val loss: 1.7828670740127563
Epoch 70, training loss: 1.7522025108337402 = 1.7438645362854004 + 0.001 * 8.337989807128906
Epoch 70, val loss: 1.740038275718689
Epoch 80, training loss: 1.6917027235031128 = 1.6835860013961792 + 0.001 * 8.116755485534668
Epoch 80, val loss: 1.6829900741577148
Epoch 90, training loss: 1.609647274017334 = 1.6018708944320679 + 0.001 * 7.776377201080322
Epoch 90, val loss: 1.610061764717102
Epoch 100, training loss: 1.5098721981048584 = 1.5021374225616455 + 0.001 * 7.734732627868652
Epoch 100, val loss: 1.5264087915420532
Epoch 110, training loss: 1.405072569847107 = 1.397439956665039 + 0.001 * 7.632583141326904
Epoch 110, val loss: 1.440075397491455
Epoch 120, training loss: 1.3049534559249878 = 1.2975256443023682 + 0.001 * 7.427796840667725
Epoch 120, val loss: 1.3630717992782593
Epoch 130, training loss: 1.2143560647964478 = 1.2070010900497437 + 0.001 * 7.354984760284424
Epoch 130, val loss: 1.2987455129623413
Epoch 140, training loss: 1.1319799423217773 = 1.1246867179870605 + 0.001 * 7.293214797973633
Epoch 140, val loss: 1.243808627128601
Epoch 150, training loss: 1.052653193473816 = 1.0454002618789673 + 0.001 * 7.252918720245361
Epoch 150, val loss: 1.1915844678878784
Epoch 160, training loss: 0.971723735332489 = 0.9645070433616638 + 0.001 * 7.216679573059082
Epoch 160, val loss: 1.1386241912841797
Epoch 170, training loss: 0.8879392743110657 = 0.8807459473609924 + 0.001 * 7.193352699279785
Epoch 170, val loss: 1.0838285684585571
Epoch 180, training loss: 0.8033087849617004 = 0.7961263060569763 + 0.001 * 7.182469844818115
Epoch 180, val loss: 1.028853416442871
Epoch 190, training loss: 0.7215842604637146 = 0.7144098877906799 + 0.001 * 7.1743903160095215
Epoch 190, val loss: 0.9765423536300659
Epoch 200, training loss: 0.6462337970733643 = 0.6390668153762817 + 0.001 * 7.166962623596191
Epoch 200, val loss: 0.9302119612693787
Epoch 210, training loss: 0.5791113376617432 = 0.5719513893127441 + 0.001 * 7.1599297523498535
Epoch 210, val loss: 0.891567587852478
Epoch 220, training loss: 0.520137369632721 = 0.5129839777946472 + 0.001 * 7.153363227844238
Epoch 220, val loss: 0.8604957461357117
Epoch 230, training loss: 0.4678315818309784 = 0.46068519353866577 + 0.001 * 7.146388530731201
Epoch 230, val loss: 0.8364200592041016
Epoch 240, training loss: 0.42072582244873047 = 0.4135889708995819 + 0.001 * 7.136838436126709
Epoch 240, val loss: 0.8183377385139465
Epoch 250, training loss: 0.3776246905326843 = 0.37050312757492065 + 0.001 * 7.121553897857666
Epoch 250, val loss: 0.8049736618995667
Epoch 260, training loss: 0.3382004499435425 = 0.3310938775539398 + 0.001 * 7.106566429138184
Epoch 260, val loss: 0.7964140176773071
Epoch 270, training loss: 0.30230122804641724 = 0.2952226996421814 + 0.001 * 7.078532695770264
Epoch 270, val loss: 0.7918553948402405
Epoch 280, training loss: 0.269952654838562 = 0.2628788948059082 + 0.001 * 7.073764801025391
Epoch 280, val loss: 0.7909198999404907
Epoch 290, training loss: 0.24100375175476074 = 0.23396076261997223 + 0.001 * 7.04299259185791
Epoch 290, val loss: 0.7933594584465027
Epoch 300, training loss: 0.21526530385017395 = 0.20823326706886292 + 0.001 * 7.0320305824279785
Epoch 300, val loss: 0.7992847561836243
Epoch 310, training loss: 0.19243775308132172 = 0.18541251122951508 + 0.001 * 7.0252461433410645
Epoch 310, val loss: 0.8082485198974609
Epoch 320, training loss: 0.17217427492141724 = 0.16515406966209412 + 0.001 * 7.02020788192749
Epoch 320, val loss: 0.8195016980171204
Epoch 330, training loss: 0.1541745662689209 = 0.14716100692749023 + 0.001 * 7.013562202453613
Epoch 330, val loss: 0.8330211639404297
Epoch 340, training loss: 0.1381956785917282 = 0.1311892569065094 + 0.001 * 7.0064167976379395
Epoch 340, val loss: 0.8481587767601013
Epoch 350, training loss: 0.12404880672693253 = 0.11704331636428833 + 0.001 * 7.005488872528076
Epoch 350, val loss: 0.8646985292434692
Epoch 360, training loss: 0.11155196279287338 = 0.1045582965016365 + 0.001 * 6.993668079376221
Epoch 360, val loss: 0.8823426365852356
Epoch 370, training loss: 0.10056272894144058 = 0.09357684850692749 + 0.001 * 6.985878944396973
Epoch 370, val loss: 0.9008360505104065
Epoch 380, training loss: 0.09092304855585098 = 0.08394171297550201 + 0.001 * 6.981334686279297
Epoch 380, val loss: 0.9198725819587708
Epoch 390, training loss: 0.0824592188000679 = 0.07548300176858902 + 0.001 * 6.976217269897461
Epoch 390, val loss: 0.9392785429954529
Epoch 400, training loss: 0.07502321898937225 = 0.06804607063531876 + 0.001 * 6.977144718170166
Epoch 400, val loss: 0.9588073492050171
Epoch 410, training loss: 0.06847109645605087 = 0.061497364193201065 + 0.001 * 6.973731517791748
Epoch 410, val loss: 0.9782993793487549
Epoch 420, training loss: 0.06269044429063797 = 0.05572102218866348 + 0.001 * 6.969419956207275
Epoch 420, val loss: 0.9975962042808533
Epoch 430, training loss: 0.057581692934036255 = 0.050616126507520676 + 0.001 * 6.965567111968994
Epoch 430, val loss: 1.016641616821289
Epoch 440, training loss: 0.053057026118040085 = 0.04609496518969536 + 0.001 * 6.962060928344727
Epoch 440, val loss: 1.0353471040725708
Epoch 450, training loss: 0.04904548078775406 = 0.042081743478775024 + 0.001 * 6.963737487792969
Epoch 450, val loss: 1.0536868572235107
Epoch 460, training loss: 0.045470546931028366 = 0.038512200117111206 + 0.001 * 6.95834493637085
Epoch 460, val loss: 1.071585774421692
Epoch 470, training loss: 0.04228665307164192 = 0.035330381244421005 + 0.001 * 6.956272125244141
Epoch 470, val loss: 1.089070200920105
Epoch 480, training loss: 0.039446357637643814 = 0.032487817108631134 + 0.001 * 6.958540916442871
Epoch 480, val loss: 1.106087327003479
Epoch 490, training loss: 0.03689417243003845 = 0.02993910014629364 + 0.001 * 6.955073356628418
Epoch 490, val loss: 1.1226580142974854
Epoch 500, training loss: 0.034606579691171646 = 0.027645519003272057 + 0.001 * 6.961061477661133
Epoch 500, val loss: 1.1389083862304688
Epoch 510, training loss: 0.032526805996894836 = 0.025573065504431725 + 0.001 * 6.953742027282715
Epoch 510, val loss: 1.1548467874526978
Epoch 520, training loss: 0.030646231025457382 = 0.0236970167607069 + 0.001 * 6.949214458465576
Epoch 520, val loss: 1.1704646348953247
Epoch 530, training loss: 0.028952622786164284 = 0.021996358409523964 + 0.001 * 6.956264019012451
Epoch 530, val loss: 1.1858128309249878
Epoch 540, training loss: 0.027404386550188065 = 0.02045351080596447 + 0.001 * 6.950874328613281
Epoch 540, val loss: 1.2008285522460938
Epoch 550, training loss: 0.026002828031778336 = 0.019052842631936073 + 0.001 * 6.949985504150391
Epoch 550, val loss: 1.2154775857925415
Epoch 560, training loss: 0.02472710609436035 = 0.017779726535081863 + 0.001 * 6.947378158569336
Epoch 560, val loss: 1.2298616170883179
Epoch 570, training loss: 0.0235667172819376 = 0.016620688140392303 + 0.001 * 6.946028709411621
Epoch 570, val loss: 1.243856430053711
Epoch 580, training loss: 0.02250603213906288 = 0.015564242377877235 + 0.001 * 6.941789150238037
Epoch 580, val loss: 1.2575522661209106
Epoch 590, training loss: 0.021539859473705292 = 0.014600205235183239 + 0.001 * 6.939654350280762
Epoch 590, val loss: 1.2709146738052368
Epoch 600, training loss: 0.020655307918787003 = 0.013719187118113041 + 0.001 * 6.93612003326416
Epoch 600, val loss: 1.283941626548767
Epoch 610, training loss: 0.01985284686088562 = 0.012913173995912075 + 0.001 * 6.939671516418457
Epoch 610, val loss: 1.2966140508651733
Epoch 620, training loss: 0.0191133301705122 = 0.012174335308372974 + 0.001 * 6.938994407653809
Epoch 620, val loss: 1.3090091943740845
Epoch 630, training loss: 0.01843087747693062 = 0.011496051214635372 + 0.001 * 6.934826374053955
Epoch 630, val loss: 1.3210395574569702
Epoch 640, training loss: 0.017809933051466942 = 0.010872201062738895 + 0.001 * 6.937732219696045
Epoch 640, val loss: 1.3327438831329346
Epoch 650, training loss: 0.01722332090139389 = 0.010297590866684914 + 0.001 * 6.925730228424072
Epoch 650, val loss: 1.3441367149353027
Epoch 660, training loss: 0.0166940838098526 = 0.009767373092472553 + 0.001 * 6.926711082458496
Epoch 660, val loss: 1.3552334308624268
Epoch 670, training loss: 0.016201358288526535 = 0.009277398698031902 + 0.001 * 6.923960208892822
Epoch 670, val loss: 1.3660306930541992
Epoch 680, training loss: 0.01574384793639183 = 0.008823777548968792 + 0.001 * 6.920071125030518
Epoch 680, val loss: 1.3765288591384888
Epoch 690, training loss: 0.01533736102283001 = 0.008403151296079159 + 0.001 * 6.934209823608398
Epoch 690, val loss: 1.3867673873901367
Epoch 700, training loss: 0.014938294887542725 = 0.008012580685317516 + 0.001 * 6.925714492797852
Epoch 700, val loss: 1.3967301845550537
Epoch 710, training loss: 0.01456101518124342 = 0.00764937000349164 + 0.001 * 6.91164493560791
Epoch 710, val loss: 1.4064099788665771
Epoch 720, training loss: 0.014231299050152302 = 0.007311077788472176 + 0.001 * 6.920220851898193
Epoch 720, val loss: 1.4158576726913452
Epoch 730, training loss: 0.013915443792939186 = 0.006995598319917917 + 0.001 * 6.9198455810546875
Epoch 730, val loss: 1.4250316619873047
Epoch 740, training loss: 0.013621347956359386 = 0.006700966041535139 + 0.001 * 6.920381546020508
Epoch 740, val loss: 1.4339936971664429
Epoch 750, training loss: 0.013331028632819653 = 0.006425436586141586 + 0.001 * 6.9055914878845215
Epoch 750, val loss: 1.4427024126052856
Epoch 760, training loss: 0.01307896338403225 = 0.006167421117424965 + 0.001 * 6.911541938781738
Epoch 760, val loss: 1.4511970281600952
Epoch 770, training loss: 0.012838544324040413 = 0.005925509613007307 + 0.001 * 6.913034915924072
Epoch 770, val loss: 1.459489345550537
Epoch 780, training loss: 0.012591773644089699 = 0.005698432680219412 + 0.001 * 6.893340587615967
Epoch 780, val loss: 1.4675425291061401
Epoch 790, training loss: 0.01238282211124897 = 0.0054850452579557896 + 0.001 * 6.897776126861572
Epoch 790, val loss: 1.4754183292388916
Epoch 800, training loss: 0.012184510007500648 = 0.0052842977456748486 + 0.001 * 6.900211811065674
Epoch 800, val loss: 1.4830923080444336
Epoch 810, training loss: 0.011998960748314857 = 0.0050952318124473095 + 0.001 * 6.903728008270264
Epoch 810, val loss: 1.4905937910079956
Epoch 820, training loss: 0.011807667091488838 = 0.004916989244520664 + 0.001 * 6.890676975250244
Epoch 820, val loss: 1.4978729486465454
Epoch 830, training loss: 0.011649271473288536 = 0.004748707637190819 + 0.001 * 6.900563716888428
Epoch 830, val loss: 1.505003809928894
Epoch 840, training loss: 0.01147097535431385 = 0.0045897033996880054 + 0.001 * 6.881271839141846
Epoch 840, val loss: 1.511953592300415
Epoch 850, training loss: 0.011338824406266212 = 0.004439284559339285 + 0.001 * 6.899538993835449
Epoch 850, val loss: 1.5187442302703857
Epoch 860, training loss: 0.011180814355611801 = 0.004296917002648115 + 0.001 * 6.883896350860596
Epoch 860, val loss: 1.525374174118042
Epoch 870, training loss: 0.011031098663806915 = 0.0041619958356022835 + 0.001 * 6.869102478027344
Epoch 870, val loss: 1.5318567752838135
Epoch 880, training loss: 0.010908315889537334 = 0.004034027922898531 + 0.001 * 6.8742876052856445
Epoch 880, val loss: 1.5381826162338257
Epoch 890, training loss: 0.010789694264531136 = 0.003912576474249363 + 0.001 * 6.877118110656738
Epoch 890, val loss: 1.5443552732467651
Epoch 900, training loss: 0.010667609982192516 = 0.003797203302383423 + 0.001 * 6.870406150817871
Epoch 900, val loss: 1.5503811836242676
Epoch 910, training loss: 0.010551109910011292 = 0.003687469055876136 + 0.001 * 6.863640785217285
Epoch 910, val loss: 1.5562835931777954
Epoch 920, training loss: 0.01048063300549984 = 0.0035830368287861347 + 0.001 * 6.89759635925293
Epoch 920, val loss: 1.5620683431625366
Epoch 930, training loss: 0.010347934439778328 = 0.0034835562109947205 + 0.001 * 6.864377975463867
Epoch 930, val loss: 1.5677063465118408
Epoch 940, training loss: 0.010248592123389244 = 0.0033887289464473724 + 0.001 * 6.85986328125
Epoch 940, val loss: 1.5732473134994507
Epoch 950, training loss: 0.010179506614804268 = 0.0032982765696942806 + 0.001 * 6.881229400634766
Epoch 950, val loss: 1.5786405801773071
Epoch 960, training loss: 0.010073419660329819 = 0.0032119175884872675 + 0.001 * 6.861501693725586
Epoch 960, val loss: 1.5839263200759888
Epoch 970, training loss: 0.01000077836215496 = 0.0031294298823922873 + 0.001 * 6.871347904205322
Epoch 970, val loss: 1.5891169309616089
Epoch 980, training loss: 0.009899651631712914 = 0.003050589933991432 + 0.001 * 6.849061012268066
Epoch 980, val loss: 1.5941762924194336
Epoch 990, training loss: 0.009835835546255112 = 0.002975187264382839 + 0.001 * 6.8606486320495605
Epoch 990, val loss: 1.599128007888794
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7232
Flip ASR: 0.6889/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9585249423980713 = 1.9501510858535767 + 0.001 * 8.37389087677002
Epoch 0, val loss: 1.9481072425842285
Epoch 10, training loss: 1.9479191303253174 = 1.9395452737808228 + 0.001 * 8.373842239379883
Epoch 10, val loss: 1.937459945678711
Epoch 20, training loss: 1.9352500438690186 = 1.9268763065338135 + 0.001 * 8.373678207397461
Epoch 20, val loss: 1.92416250705719
Epoch 30, training loss: 1.9180349111557007 = 1.9096615314483643 + 0.001 * 8.373345375061035
Epoch 30, val loss: 1.905563235282898
Epoch 40, training loss: 1.893332839012146 = 1.8849601745605469 + 0.001 * 8.372607231140137
Epoch 40, val loss: 1.8788384199142456
Epoch 50, training loss: 1.8587099313735962 = 1.8503391742706299 + 0.001 * 8.370719909667969
Epoch 50, val loss: 1.8427287340164185
Epoch 60, training loss: 1.8157657384872437 = 1.807401180267334 + 0.001 * 8.3645658493042
Epoch 60, val loss: 1.80123770236969
Epoch 70, training loss: 1.7716363668441772 = 1.7632983922958374 + 0.001 * 8.337944030761719
Epoch 70, val loss: 1.7621512413024902
Epoch 80, training loss: 1.7242465019226074 = 1.7160934209823608 + 0.001 * 8.153106689453125
Epoch 80, val loss: 1.7203855514526367
Epoch 90, training loss: 1.662604570388794 = 1.654757022857666 + 0.001 * 7.8475823402404785
Epoch 90, val loss: 1.6664643287658691
Epoch 100, training loss: 1.5813628435134888 = 1.5737920999526978 + 0.001 * 7.570779323577881
Epoch 100, val loss: 1.5983598232269287
Epoch 110, training loss: 1.4788432121276855 = 1.4715299606323242 + 0.001 * 7.313292026519775
Epoch 110, val loss: 1.5152671337127686
Epoch 120, training loss: 1.3633809089660645 = 1.356154203414917 + 0.001 * 7.226705551147461
Epoch 120, val loss: 1.4226384162902832
Epoch 130, training loss: 1.2449002265930176 = 1.2377126216888428 + 0.001 * 7.187575817108154
Epoch 130, val loss: 1.3294756412506104
Epoch 140, training loss: 1.1304391622543335 = 1.1232815980911255 + 0.001 * 7.157563209533691
Epoch 140, val loss: 1.24006187915802
Epoch 150, training loss: 1.024139642715454 = 1.0170063972473145 + 0.001 * 7.133190631866455
Epoch 150, val loss: 1.1583982706069946
Epoch 160, training loss: 0.9274981021881104 = 0.9203829169273376 + 0.001 * 7.115203857421875
Epoch 160, val loss: 1.0849144458770752
Epoch 170, training loss: 0.8397219777107239 = 0.8326199054718018 + 0.001 * 7.102043151855469
Epoch 170, val loss: 1.0195083618164062
Epoch 180, training loss: 0.7594528794288635 = 0.7523640990257263 + 0.001 * 7.088803291320801
Epoch 180, val loss: 0.9606704115867615
Epoch 190, training loss: 0.6861486434936523 = 0.6790764927864075 + 0.001 * 7.07216739654541
Epoch 190, val loss: 0.9072492122650146
Epoch 200, training loss: 0.6196825504302979 = 0.6126309633255005 + 0.001 * 7.051600456237793
Epoch 200, val loss: 0.8595126271247864
Epoch 210, training loss: 0.5596941709518433 = 0.5526638031005859 + 0.001 * 7.03034782409668
Epoch 210, val loss: 0.8185387253761292
Epoch 220, training loss: 0.5056722164154053 = 0.49864980578422546 + 0.001 * 7.022418975830078
Epoch 220, val loss: 0.7843061685562134
Epoch 230, training loss: 0.45714694261550903 = 0.45012885332107544 + 0.001 * 7.018086910247803
Epoch 230, val loss: 0.7564745545387268
Epoch 240, training loss: 0.4134296774864197 = 0.40641266107559204 + 0.001 * 7.0170111656188965
Epoch 240, val loss: 0.7343143224716187
Epoch 250, training loss: 0.37374037504196167 = 0.36672481894493103 + 0.001 * 7.015562534332275
Epoch 250, val loss: 0.7172042727470398
Epoch 260, training loss: 0.33723804354667664 = 0.3302251100540161 + 0.001 * 7.012944221496582
Epoch 260, val loss: 0.7037550806999207
Epoch 270, training loss: 0.3034621477127075 = 0.29645246267318726 + 0.001 * 7.009677886962891
Epoch 270, val loss: 0.6931332349777222
Epoch 280, training loss: 0.2720426023006439 = 0.2650330364704132 + 0.001 * 7.0095624923706055
Epoch 280, val loss: 0.6851789355278015
Epoch 290, training loss: 0.24290364980697632 = 0.23590020835399628 + 0.001 * 7.003435134887695
Epoch 290, val loss: 0.6800233125686646
Epoch 300, training loss: 0.21609501540660858 = 0.20909549295902252 + 0.001 * 6.99951696395874
Epoch 300, val loss: 0.6772974133491516
Epoch 310, training loss: 0.19167885184288025 = 0.18468424677848816 + 0.001 * 6.994608402252197
Epoch 310, val loss: 0.6770174503326416
Epoch 320, training loss: 0.16972529888153076 = 0.16273003816604614 + 0.001 * 6.995259761810303
Epoch 320, val loss: 0.6789116859436035
Epoch 330, training loss: 0.15024134516716003 = 0.14325793087482452 + 0.001 * 6.983410358428955
Epoch 330, val loss: 0.6825429797172546
Epoch 340, training loss: 0.13312380015850067 = 0.12614648044109344 + 0.001 * 6.977324962615967
Epoch 340, val loss: 0.6877978444099426
Epoch 350, training loss: 0.11820648610591888 = 0.11123595386743546 + 0.001 * 6.9705281257629395
Epoch 350, val loss: 0.6945440769195557
Epoch 360, training loss: 0.1052936241030693 = 0.09831894934177399 + 0.001 * 6.974673748016357
Epoch 360, val loss: 0.7026462554931641
Epoch 370, training loss: 0.0940929725766182 = 0.08714237809181213 + 0.001 * 6.950592041015625
Epoch 370, val loss: 0.7118017673492432
Epoch 380, training loss: 0.08441754430532455 = 0.07746464014053345 + 0.001 * 6.952905178070068
Epoch 380, val loss: 0.7218202948570251
Epoch 390, training loss: 0.0759977325797081 = 0.0690658912062645 + 0.001 * 6.931843280792236
Epoch 390, val loss: 0.7325940728187561
Epoch 400, training loss: 0.06867784261703491 = 0.06175912544131279 + 0.001 * 6.918717384338379
Epoch 400, val loss: 0.7438279390335083
Epoch 410, training loss: 0.06229887157678604 = 0.055386193096637726 + 0.001 * 6.912677764892578
Epoch 410, val loss: 0.7554189562797546
Epoch 420, training loss: 0.05672873929142952 = 0.049817364662885666 + 0.001 * 6.911374568939209
Epoch 420, val loss: 0.7671411633491516
Epoch 430, training loss: 0.051837146282196045 = 0.04494159668684006 + 0.001 * 6.89555025100708
Epoch 430, val loss: 0.7788699269294739
Epoch 440, training loss: 0.04756935313344002 = 0.04066293686628342 + 0.001 * 6.906416416168213
Epoch 440, val loss: 0.7905710339546204
Epoch 450, training loss: 0.04379794001579285 = 0.03689905256032944 + 0.001 * 6.898889064788818
Epoch 450, val loss: 0.8021533489227295
Epoch 460, training loss: 0.04046813026070595 = 0.03357689455151558 + 0.001 * 6.891236782073975
Epoch 460, val loss: 0.8135743737220764
Epoch 470, training loss: 0.03753030300140381 = 0.030630523338913918 + 0.001 * 6.899781227111816
Epoch 470, val loss: 0.824817955493927
Epoch 480, training loss: 0.03489844501018524 = 0.028008153662085533 + 0.001 * 6.890290260314941
Epoch 480, val loss: 0.8358398079872131
Epoch 490, training loss: 0.0325651690363884 = 0.025667715817689896 + 0.001 * 6.897450923919678
Epoch 490, val loss: 0.8467106819152832
Epoch 500, training loss: 0.030457139015197754 = 0.023576049134135246 + 0.001 * 6.88109016418457
Epoch 500, val loss: 0.8574448227882385
Epoch 510, training loss: 0.028584260493516922 = 0.021705083549022675 + 0.001 * 6.879176139831543
Epoch 510, val loss: 0.867967963218689
Epoch 520, training loss: 0.026904340833425522 = 0.020029468461871147 + 0.001 * 6.87487268447876
Epoch 520, val loss: 0.8782645463943481
Epoch 530, training loss: 0.025419916957616806 = 0.018526555970311165 + 0.001 * 6.8933610916137695
Epoch 530, val loss: 0.8883854150772095
Epoch 540, training loss: 0.024055536836385727 = 0.017176294699311256 + 0.001 * 6.879241466522217
Epoch 540, val loss: 0.8982406258583069
Epoch 550, training loss: 0.02284056879580021 = 0.01596085913479328 + 0.001 * 6.879708766937256
Epoch 550, val loss: 0.9078547954559326
Epoch 560, training loss: 0.0217311829328537 = 0.014864792115986347 + 0.001 * 6.866389751434326
Epoch 560, val loss: 0.9172489047050476
Epoch 570, training loss: 0.020755721256136894 = 0.013874090276658535 + 0.001 * 6.881630897521973
Epoch 570, val loss: 0.926409125328064
Epoch 580, training loss: 0.019853338599205017 = 0.012976692989468575 + 0.001 * 6.876644611358643
Epoch 580, val loss: 0.9353296756744385
Epoch 590, training loss: 0.019029077142477036 = 0.012161887250840664 + 0.001 * 6.867188453674316
Epoch 590, val loss: 0.9440361857414246
Epoch 600, training loss: 0.018311701714992523 = 0.01142024714499712 + 0.001 * 6.891454219818115
Epoch 600, val loss: 0.952555239200592
Epoch 610, training loss: 0.01760556548833847 = 0.010742977261543274 + 0.001 * 6.862586975097656
Epoch 610, val loss: 0.9609081149101257
Epoch 620, training loss: 0.016985643655061722 = 0.010123034939169884 + 0.001 * 6.862607955932617
Epoch 620, val loss: 0.9690473675727844
Epoch 630, training loss: 0.016432449221611023 = 0.009554647840559483 + 0.001 * 6.877800941467285
Epoch 630, val loss: 0.9769939184188843
Epoch 640, training loss: 0.015898868441581726 = 0.009032550267875195 + 0.001 * 6.8663177490234375
Epoch 640, val loss: 0.9847856163978577
Epoch 650, training loss: 0.015413266606628895 = 0.008551527746021748 + 0.001 * 6.861738681793213
Epoch 650, val loss: 0.9924237132072449
Epoch 660, training loss: 0.014962054789066315 = 0.00810700561851263 + 0.001 * 6.855048656463623
Epoch 660, val loss: 0.9999002814292908
Epoch 670, training loss: 0.014555886387825012 = 0.007696277461946011 + 0.001 * 6.859609127044678
Epoch 670, val loss: 1.0072271823883057
Epoch 680, training loss: 0.014174532145261765 = 0.007316791918128729 + 0.001 * 6.85774040222168
Epoch 680, val loss: 1.0143851041793823
Epoch 690, training loss: 0.013809160329401493 = 0.006964470259845257 + 0.001 * 6.844689846038818
Epoch 690, val loss: 1.0214053392410278
Epoch 700, training loss: 0.013494051992893219 = 0.006636494304984808 + 0.001 * 6.857557773590088
Epoch 700, val loss: 1.028260350227356
Epoch 710, training loss: 0.013169602490961552 = 0.006331313867121935 + 0.001 * 6.838288307189941
Epoch 710, val loss: 1.0349916219711304
Epoch 720, training loss: 0.012909529730677605 = 0.006047597620636225 + 0.001 * 6.861931800842285
Epoch 720, val loss: 1.041571021080017
Epoch 730, training loss: 0.012635763734579086 = 0.0057831378653645515 + 0.001 * 6.852624893188477
Epoch 730, val loss: 1.0479986667633057
Epoch 740, training loss: 0.012374786660075188 = 0.00553620420396328 + 0.001 * 6.838582515716553
Epoch 740, val loss: 1.05427086353302
Epoch 750, training loss: 0.012143578380346298 = 0.005305309779942036 + 0.001 * 6.838268280029297
Epoch 750, val loss: 1.0604124069213867
Epoch 760, training loss: 0.01192125678062439 = 0.005089263431727886 + 0.001 * 6.8319926261901855
Epoch 760, val loss: 1.0664390325546265
Epoch 770, training loss: 0.011712666600942612 = 0.004886813927441835 + 0.001 * 6.825851917266846
Epoch 770, val loss: 1.0723168849945068
Epoch 780, training loss: 0.01155182346701622 = 0.004696786403656006 + 0.001 * 6.855036735534668
Epoch 780, val loss: 1.078110933303833
Epoch 790, training loss: 0.011352816596627235 = 0.004518309608101845 + 0.001 * 6.834506034851074
Epoch 790, val loss: 1.0837249755859375
Epoch 800, training loss: 0.011230532079935074 = 0.004350519273430109 + 0.001 * 6.8800129890441895
Epoch 800, val loss: 1.0892541408538818
Epoch 810, training loss: 0.01102715265005827 = 0.0041926102712750435 + 0.001 * 6.834542274475098
Epoch 810, val loss: 1.094671607017517
Epoch 820, training loss: 0.010864531621336937 = 0.004043743945658207 + 0.001 * 6.82078742980957
Epoch 820, val loss: 1.0999398231506348
Epoch 830, training loss: 0.010718346573412418 = 0.0039034418296068907 + 0.001 * 6.814904689788818
Epoch 830, val loss: 1.1051105260849
Epoch 840, training loss: 0.01058960147202015 = 0.0037710447795689106 + 0.001 * 6.818556785583496
Epoch 840, val loss: 1.110177993774414
Epoch 850, training loss: 0.010473936796188354 = 0.003645951859652996 + 0.001 * 6.82798433303833
Epoch 850, val loss: 1.1150798797607422
Epoch 860, training loss: 0.010346517898142338 = 0.003527560504153371 + 0.001 * 6.8189568519592285
Epoch 860, val loss: 1.119936466217041
Epoch 870, training loss: 0.010255880653858185 = 0.0034154150635004044 + 0.001 * 6.8404645919799805
Epoch 870, val loss: 1.12468683719635
Epoch 880, training loss: 0.010137366130948067 = 0.0033092619851231575 + 0.001 * 6.828104019165039
Epoch 880, val loss: 1.1292976140975952
Epoch 890, training loss: 0.010006936267018318 = 0.0032084244303405285 + 0.001 * 6.798511505126953
Epoch 890, val loss: 1.1338449716567993
Epoch 900, training loss: 0.009920009411871433 = 0.0031124937813729048 + 0.001 * 6.807515621185303
Epoch 900, val loss: 1.1382839679718018
Epoch 910, training loss: 0.009851218201220036 = 0.003021277952939272 + 0.001 * 6.829939842224121
Epoch 910, val loss: 1.142642855644226
Epoch 920, training loss: 0.009733946062624454 = 0.0029345520306378603 + 0.001 * 6.799394130706787
Epoch 920, val loss: 1.1469250917434692
Epoch 930, training loss: 0.009657260030508041 = 0.0028519309125840664 + 0.001 * 6.805328845977783
Epoch 930, val loss: 1.151102900505066
Epoch 940, training loss: 0.009571264497935772 = 0.002773054176941514 + 0.001 * 6.798210144042969
Epoch 940, val loss: 1.1552200317382812
Epoch 950, training loss: 0.009544922038912773 = 0.0026974836364388466 + 0.001 * 6.847437858581543
Epoch 950, val loss: 1.1592416763305664
Epoch 960, training loss: 0.009422563947737217 = 0.0026250535156577826 + 0.001 * 6.797509670257568
Epoch 960, val loss: 1.1632388830184937
Epoch 970, training loss: 0.009327860549092293 = 0.0025555749889463186 + 0.001 * 6.772285461425781
Epoch 970, val loss: 1.1671842336654663
Epoch 980, training loss: 0.00927749089896679 = 0.002488876925781369 + 0.001 * 6.788613319396973
Epoch 980, val loss: 1.1710631847381592
Epoch 990, training loss: 0.009219159372150898 = 0.0024248980917036533 + 0.001 * 6.7942609786987305
Epoch 990, val loss: 1.1748442649841309
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.5646
Flip ASR: 0.5244/225 nodes
The final ASR:0.56581, 0.12805, Accuracy:0.81111, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10568])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83827, 0.00349
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9327356815338135 = 1.9243618249893188 + 0.001 * 8.373866081237793
Epoch 0, val loss: 1.9246249198913574
Epoch 10, training loss: 1.9233362674713135 = 1.9149624109268188 + 0.001 * 8.373818397521973
Epoch 10, val loss: 1.9157048463821411
Epoch 20, training loss: 1.9120254516601562 = 1.9036518335342407 + 0.001 * 8.373604774475098
Epoch 20, val loss: 1.9045462608337402
Epoch 30, training loss: 1.8963016271591187 = 1.8879284858703613 + 0.001 * 8.373162269592285
Epoch 30, val loss: 1.8888219594955444
Epoch 40, training loss: 1.8734549283981323 = 1.8650827407836914 + 0.001 * 8.372198104858398
Epoch 40, val loss: 1.8661917448043823
Epoch 50, training loss: 1.841694712638855 = 1.8333251476287842 + 0.001 * 8.369617462158203
Epoch 50, val loss: 1.8361117839813232
Epoch 60, training loss: 1.803815484046936 = 1.7954550981521606 + 0.001 * 8.360374450683594
Epoch 60, val loss: 1.8033124208450317
Epoch 70, training loss: 1.763861060142517 = 1.7555484771728516 + 0.001 * 8.312590599060059
Epoch 70, val loss: 1.7703325748443604
Epoch 80, training loss: 1.7103354930877686 = 1.70237398147583 + 0.001 * 7.961524963378906
Epoch 80, val loss: 1.7238187789916992
Epoch 90, training loss: 1.635654330253601 = 1.6278506517410278 + 0.001 * 7.803668022155762
Epoch 90, val loss: 1.6602381467819214
Epoch 100, training loss: 1.5408687591552734 = 1.533111810684204 + 0.001 * 7.756987571716309
Epoch 100, val loss: 1.5832856893539429
Epoch 110, training loss: 1.4362397193908691 = 1.4285832643508911 + 0.001 * 7.656484127044678
Epoch 110, val loss: 1.498292088508606
Epoch 120, training loss: 1.3307209014892578 = 1.323269248008728 + 0.001 * 7.451619625091553
Epoch 120, val loss: 1.4159036874771118
Epoch 130, training loss: 1.2287228107452393 = 1.2213964462280273 + 0.001 * 7.326390743255615
Epoch 130, val loss: 1.338496208190918
Epoch 140, training loss: 1.1327617168426514 = 1.1255313158035278 + 0.001 * 7.230452537536621
Epoch 140, val loss: 1.2681039571762085
Epoch 150, training loss: 1.0438895225524902 = 1.0367668867111206 + 0.001 * 7.1226677894592285
Epoch 150, val loss: 1.2044930458068848
Epoch 160, training loss: 0.9616197943687439 = 0.954552412033081 + 0.001 * 7.067383766174316
Epoch 160, val loss: 1.1459417343139648
Epoch 170, training loss: 0.884570300579071 = 0.877538800239563 + 0.001 * 7.031505107879639
Epoch 170, val loss: 1.0907870531082153
Epoch 180, training loss: 0.8107584714889526 = 0.8037481307983398 + 0.001 * 7.010337829589844
Epoch 180, val loss: 1.037840723991394
Epoch 190, training loss: 0.738677978515625 = 0.7316717505455017 + 0.001 * 7.006223201751709
Epoch 190, val loss: 0.9862905740737915
Epoch 200, training loss: 0.6684873104095459 = 0.6614812016487122 + 0.001 * 7.006102561950684
Epoch 200, val loss: 0.9361939430236816
Epoch 210, training loss: 0.6017624735832214 = 0.5947580337524414 + 0.001 * 7.004428386688232
Epoch 210, val loss: 0.889039158821106
Epoch 220, training loss: 0.5403114557266235 = 0.5333090424537659 + 0.001 * 7.002438545227051
Epoch 220, val loss: 0.8466435074806213
Epoch 230, training loss: 0.48505955934524536 = 0.47805970907211304 + 0.001 * 6.999853134155273
Epoch 230, val loss: 0.8104283213615417
Epoch 240, training loss: 0.43557223677635193 = 0.4285753667354584 + 0.001 * 6.99686861038208
Epoch 240, val loss: 0.7804049849510193
Epoch 250, training loss: 0.3905791938304901 = 0.3835846185684204 + 0.001 * 6.994570732116699
Epoch 250, val loss: 0.755773663520813
Epoch 260, training loss: 0.34878483414649963 = 0.34179338812828064 + 0.001 * 6.991448402404785
Epoch 260, val loss: 0.7353742122650146
Epoch 270, training loss: 0.3094164729118347 = 0.3024272322654724 + 0.001 * 6.9892354011535645
Epoch 270, val loss: 0.7183842658996582
Epoch 280, training loss: 0.27218303084373474 = 0.26519542932510376 + 0.001 * 6.987590789794922
Epoch 280, val loss: 0.7042092084884644
Epoch 290, training loss: 0.23732072114944458 = 0.2303341031074524 + 0.001 * 6.986621379852295
Epoch 290, val loss: 0.6926723718643188
Epoch 300, training loss: 0.20544767379760742 = 0.19846144318580627 + 0.001 * 6.986230373382568
Epoch 300, val loss: 0.6842711567878723
Epoch 310, training loss: 0.17708711326122284 = 0.17009972035884857 + 0.001 * 6.987388610839844
Epoch 310, val loss: 0.6792519092559814
Epoch 320, training loss: 0.15248392522335052 = 0.1454969346523285 + 0.001 * 6.986985206604004
Epoch 320, val loss: 0.6778762936592102
Epoch 330, training loss: 0.13155855238437653 = 0.12457136809825897 + 0.001 * 6.987176895141602
Epoch 330, val loss: 0.6798485517501831
Epoch 340, training loss: 0.11396889388561249 = 0.10698037594556808 + 0.001 * 6.988519191741943
Epoch 340, val loss: 0.6846931576728821
Epoch 350, training loss: 0.09924423694610596 = 0.09225738793611526 + 0.001 * 6.986851215362549
Epoch 350, val loss: 0.6918218731880188
Epoch 360, training loss: 0.08693118393421173 = 0.07994575798511505 + 0.001 * 6.985422134399414
Epoch 360, val loss: 0.7006903886795044
Epoch 370, training loss: 0.07661532610654831 = 0.06963219493627548 + 0.001 * 6.983129024505615
Epoch 370, val loss: 0.7107161283493042
Epoch 380, training loss: 0.06795051693916321 = 0.06096971407532692 + 0.001 * 6.980804920196533
Epoch 380, val loss: 0.7214905619621277
Epoch 390, training loss: 0.06065063923597336 = 0.053672049194574356 + 0.001 * 6.9785895347595215
Epoch 390, val loss: 0.7326963543891907
Epoch 400, training loss: 0.05447103828191757 = 0.04749925807118416 + 0.001 * 6.971780776977539
Epoch 400, val loss: 0.7441043257713318
Epoch 410, training loss: 0.04922213777899742 = 0.04225755110383034 + 0.001 * 6.964584827423096
Epoch 410, val loss: 0.7554305195808411
Epoch 420, training loss: 0.0447496697306633 = 0.03778499364852905 + 0.001 * 6.964676856994629
Epoch 420, val loss: 0.7666319608688354
Epoch 430, training loss: 0.04089547321200371 = 0.0339500717818737 + 0.001 * 6.9454026222229
Epoch 430, val loss: 0.7776875495910645
Epoch 440, training loss: 0.037595804780721664 = 0.030645182356238365 + 0.001 * 6.950622081756592
Epoch 440, val loss: 0.7885241508483887
Epoch 450, training loss: 0.03472287952899933 = 0.027782483026385307 + 0.001 * 6.940395832061768
Epoch 450, val loss: 0.7990145087242126
Epoch 460, training loss: 0.03221467509865761 = 0.025291068479418755 + 0.001 * 6.923605918884277
Epoch 460, val loss: 0.8092713356018066
Epoch 470, training loss: 0.030043724924325943 = 0.023112699389457703 + 0.001 * 6.931024074554443
Epoch 470, val loss: 0.8192659616470337
Epoch 480, training loss: 0.028131671249866486 = 0.021199580281972885 + 0.001 * 6.93209171295166
Epoch 480, val loss: 0.8289546370506287
Epoch 490, training loss: 0.026441706344485283 = 0.019512303173542023 + 0.001 * 6.929403305053711
Epoch 490, val loss: 0.8383946418762207
Epoch 500, training loss: 0.024940118193626404 = 0.018017824739217758 + 0.001 * 6.922293663024902
Epoch 500, val loss: 0.8475228548049927
Epoch 510, training loss: 0.02360215038061142 = 0.016688859090209007 + 0.001 * 6.913290023803711
Epoch 510, val loss: 0.8564122319221497
Epoch 520, training loss: 0.022410057485103607 = 0.015502386726439 + 0.001 * 6.907669544219971
Epoch 520, val loss: 0.865002453327179
Epoch 530, training loss: 0.021340154111385345 = 0.014439735561609268 + 0.001 * 6.900418281555176
Epoch 530, val loss: 0.8733847737312317
Epoch 540, training loss: 0.020389031618833542 = 0.013484925031661987 + 0.001 * 6.904106616973877
Epoch 540, val loss: 0.8814733028411865
Epoch 550, training loss: 0.019519265741109848 = 0.012623977847397327 + 0.001 * 6.895287990570068
Epoch 550, val loss: 0.8893152475357056
Epoch 560, training loss: 0.018736734986305237 = 0.011845188215374947 + 0.001 * 6.89154577255249
Epoch 560, val loss: 0.8969599008560181
Epoch 570, training loss: 0.018035510554909706 = 0.011138592846691608 + 0.001 * 6.896917819976807
Epoch 570, val loss: 0.9043849110603333
Epoch 580, training loss: 0.01738283783197403 = 0.010495712980628014 + 0.001 * 6.887125492095947
Epoch 580, val loss: 0.9115932583808899
Epoch 590, training loss: 0.016795871779322624 = 0.009909320622682571 + 0.001 * 6.8865509033203125
Epoch 590, val loss: 0.9185875058174133
Epoch 600, training loss: 0.016280855983495712 = 0.009373023174703121 + 0.001 * 6.907832145690918
Epoch 600, val loss: 0.9253951907157898
Epoch 610, training loss: 0.01575983688235283 = 0.008881334215402603 + 0.001 * 6.878503322601318
Epoch 610, val loss: 0.9320266246795654
Epoch 620, training loss: 0.01529979519546032 = 0.008429453708231449 + 0.001 * 6.8703413009643555
Epoch 620, val loss: 0.9384636878967285
Epoch 630, training loss: 0.014887960627675056 = 0.008013182319700718 + 0.001 * 6.874777317047119
Epoch 630, val loss: 0.9447417259216309
Epoch 640, training loss: 0.01450527086853981 = 0.007628903724253178 + 0.001 * 6.876366138458252
Epoch 640, val loss: 0.9508593678474426
Epoch 650, training loss: 0.01416543684899807 = 0.007273470517247915 + 0.001 * 6.891966342926025
Epoch 650, val loss: 0.9568117260932922
Epoch 660, training loss: 0.013805268332362175 = 0.0069440267980098724 + 0.001 * 6.861241340637207
Epoch 660, val loss: 0.9626270532608032
Epoch 670, training loss: 0.013498896732926369 = 0.006638120394200087 + 0.001 * 6.860775470733643
Epoch 670, val loss: 0.968282163143158
Epoch 680, training loss: 0.013275783509016037 = 0.006353512406349182 + 0.001 * 6.922271251678467
Epoch 680, val loss: 0.9738122820854187
Epoch 690, training loss: 0.012950142845511436 = 0.006088302005082369 + 0.001 * 6.86184024810791
Epoch 690, val loss: 0.9792218208312988
Epoch 700, training loss: 0.012688282877206802 = 0.0058407410979270935 + 0.001 * 6.847540855407715
Epoch 700, val loss: 0.9844858646392822
Epoch 710, training loss: 0.012508360669016838 = 0.005609319545328617 + 0.001 * 6.899041175842285
Epoch 710, val loss: 0.9896208643913269
Epoch 720, training loss: 0.012247635051608086 = 0.005392659921199083 + 0.001 * 6.85497522354126
Epoch 720, val loss: 0.994657039642334
Epoch 730, training loss: 0.012039361521601677 = 0.005189540330320597 + 0.001 * 6.849820613861084
Epoch 730, val loss: 0.9995635151863098
Epoch 740, training loss: 0.011853684671223164 = 0.00499884644523263 + 0.001 * 6.854837894439697
Epoch 740, val loss: 1.0043766498565674
Epoch 750, training loss: 0.011682405136525631 = 0.0048195980489254 + 0.001 * 6.862806797027588
Epoch 750, val loss: 1.0090943574905396
Epoch 760, training loss: 0.011500251479446888 = 0.004650881513953209 + 0.001 * 6.849369525909424
Epoch 760, val loss: 1.0136741399765015
Epoch 770, training loss: 0.011346487328410149 = 0.0044918661005795 + 0.001 * 6.854620933532715
Epoch 770, val loss: 1.018167495727539
Epoch 780, training loss: 0.011188188567757607 = 0.00434182258322835 + 0.001 * 6.846365928649902
Epoch 780, val loss: 1.0225566625595093
Epoch 790, training loss: 0.011032854206860065 = 0.0042000869289040565 + 0.001 * 6.832767009735107
Epoch 790, val loss: 1.02688729763031
Epoch 800, training loss: 0.010911902412772179 = 0.004066091496497393 + 0.001 * 6.845810413360596
Epoch 800, val loss: 1.0310912132263184
Epoch 810, training loss: 0.010766149498522282 = 0.0039392332546412945 + 0.001 * 6.826915740966797
Epoch 810, val loss: 1.0352210998535156
Epoch 820, training loss: 0.010644684545695782 = 0.0038189657498151064 + 0.001 * 6.825718402862549
Epoch 820, val loss: 1.0392581224441528
Epoch 830, training loss: 0.01053017470985651 = 0.003704905044287443 + 0.001 * 6.8252692222595215
Epoch 830, val loss: 1.0432451963424683
Epoch 840, training loss: 0.010426848195493221 = 0.0035966194700449705 + 0.001 * 6.830228328704834
Epoch 840, val loss: 1.0471293926239014
Epoch 850, training loss: 0.010333390906453133 = 0.0034937162417918444 + 0.001 * 6.83967399597168
Epoch 850, val loss: 1.0509517192840576
Epoch 860, training loss: 0.010217372328042984 = 0.0033958458807319403 + 0.001 * 6.821526527404785
Epoch 860, val loss: 1.0546807050704956
Epoch 870, training loss: 0.010125456377863884 = 0.0033026784658432007 + 0.001 * 6.82277774810791
Epoch 870, val loss: 1.0583356618881226
Epoch 880, training loss: 0.010029307566583157 = 0.0032139376271516085 + 0.001 * 6.815369606018066
Epoch 880, val loss: 1.06193208694458
Epoch 890, training loss: 0.009955373592674732 = 0.0031293330248445272 + 0.001 * 6.826039791107178
Epoch 890, val loss: 1.0654561519622803
Epoch 900, training loss: 0.009855765849351883 = 0.003048602258786559 + 0.001 * 6.807163715362549
Epoch 900, val loss: 1.068924069404602
Epoch 910, training loss: 0.009788869880139828 = 0.00297151948325336 + 0.001 * 6.817349910736084
Epoch 910, val loss: 1.0723079442977905
Epoch 920, training loss: 0.009711598977446556 = 0.0028978739865124226 + 0.001 * 6.813724994659424
Epoch 920, val loss: 1.0756524801254272
Epoch 930, training loss: 0.009657813236117363 = 0.0028274457436054945 + 0.001 * 6.830367565155029
Epoch 930, val loss: 1.07890784740448
Epoch 940, training loss: 0.009563627652823925 = 0.0027600713074207306 + 0.001 * 6.803555965423584
Epoch 940, val loss: 1.0821260213851929
Epoch 950, training loss: 0.009497455321252346 = 0.002695549977943301 + 0.001 * 6.801905155181885
Epoch 950, val loss: 1.0852802991867065
Epoch 960, training loss: 0.009440572932362556 = 0.002633732045069337 + 0.001 * 6.806840896606445
Epoch 960, val loss: 1.088355541229248
Epoch 970, training loss: 0.009381230920553207 = 0.0025744647718966007 + 0.001 * 6.806765556335449
Epoch 970, val loss: 1.091428518295288
Epoch 980, training loss: 0.009328287094831467 = 0.0025176138151437044 + 0.001 * 6.810673236846924
Epoch 980, val loss: 1.0944103002548218
Epoch 990, training loss: 0.009256578050553799 = 0.0024630564730614424 + 0.001 * 6.793521404266357
Epoch 990, val loss: 1.0973575115203857
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.7934
Flip ASR: 0.7511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9357644319534302 = 1.9273905754089355 + 0.001 * 8.373882293701172
Epoch 0, val loss: 1.9198421239852905
Epoch 10, training loss: 1.9267971515655518 = 1.9184232950210571 + 0.001 * 8.37380313873291
Epoch 10, val loss: 1.9111124277114868
Epoch 20, training loss: 1.9156757593154907 = 1.9073021411895752 + 0.001 * 8.373574256896973
Epoch 20, val loss: 1.8999520540237427
Epoch 30, training loss: 1.9001400470733643 = 1.8917670249938965 + 0.001 * 8.373062133789062
Epoch 30, val loss: 1.8840221166610718
Epoch 40, training loss: 1.8772188425064087 = 1.8688470125198364 + 0.001 * 8.371869087219238
Epoch 40, val loss: 1.8606681823730469
Epoch 50, training loss: 1.8446601629257202 = 1.8362915515899658 + 0.001 * 8.36858081817627
Epoch 50, val loss: 1.8287889957427979
Epoch 60, training loss: 1.8043746948242188 = 1.7960182428359985 + 0.001 * 8.356400489807129
Epoch 60, val loss: 1.7919518947601318
Epoch 70, training loss: 1.7608489990234375 = 1.752557396888733 + 0.001 * 8.291543960571289
Epoch 70, val loss: 1.753708004951477
Epoch 80, training loss: 1.706284761428833 = 1.6984044313430786 + 0.001 * 7.880359172821045
Epoch 80, val loss: 1.7045475244522095
Epoch 90, training loss: 1.6327012777328491 = 1.6249771118164062 + 0.001 * 7.7241106033325195
Epoch 90, val loss: 1.639710545539856
Epoch 100, training loss: 1.5390499830245972 = 1.5314123630523682 + 0.001 * 7.637663841247559
Epoch 100, val loss: 1.5613270998001099
Epoch 110, training loss: 1.4345974922180176 = 1.4271396398544312 + 0.001 * 7.457822322845459
Epoch 110, val loss: 1.477518916130066
Epoch 120, training loss: 1.3288915157318115 = 1.3216521739959717 + 0.001 * 7.239354610443115
Epoch 120, val loss: 1.397274374961853
Epoch 130, training loss: 1.2257460355758667 = 1.218522548675537 + 0.001 * 7.223483085632324
Epoch 130, val loss: 1.3213350772857666
Epoch 140, training loss: 1.1262576580047607 = 1.1190770864486694 + 0.001 * 7.180517196655273
Epoch 140, val loss: 1.2486025094985962
Epoch 150, training loss: 1.0334855318069458 = 1.026336908340454 + 0.001 * 7.148627758026123
Epoch 150, val loss: 1.1811765432357788
Epoch 160, training loss: 0.9497372508049011 = 0.9426115155220032 + 0.001 * 7.125730991363525
Epoch 160, val loss: 1.12117600440979
Epoch 170, training loss: 0.8737613558769226 = 0.86665278673172 + 0.001 * 7.108570098876953
Epoch 170, val loss: 1.067549228668213
Epoch 180, training loss: 0.8023898601531982 = 0.7952971458435059 + 0.001 * 7.092708110809326
Epoch 180, val loss: 1.0174413919448853
Epoch 190, training loss: 0.7330869436264038 = 0.7260119915008545 + 0.001 * 7.0749359130859375
Epoch 190, val loss: 0.9687478542327881
Epoch 200, training loss: 0.6649846434593201 = 0.6579309105873108 + 0.001 * 7.053719520568848
Epoch 200, val loss: 0.9211739897727966
Epoch 210, training loss: 0.598801851272583 = 0.5917758941650391 + 0.001 * 7.025979042053223
Epoch 210, val loss: 0.8755931258201599
Epoch 220, training loss: 0.535995602607727 = 0.5289977192878723 + 0.001 * 6.997859954833984
Epoch 220, val loss: 0.8335200548171997
Epoch 230, training loss: 0.47835221886634827 = 0.4713711142539978 + 0.001 * 6.981113910675049
Epoch 230, val loss: 0.7969776391983032
Epoch 240, training loss: 0.426776647567749 = 0.4198032319545746 + 0.001 * 6.973423957824707
Epoch 240, val loss: 0.7675566077232361
Epoch 250, training loss: 0.3812033236026764 = 0.3742333948612213 + 0.001 * 6.9699177742004395
Epoch 250, val loss: 0.7447580695152283
Epoch 260, training loss: 0.34061697125434875 = 0.33364951610565186 + 0.001 * 6.96746301651001
Epoch 260, val loss: 0.7278090715408325
Epoch 270, training loss: 0.30411413311958313 = 0.29714852571487427 + 0.001 * 6.965609073638916
Epoch 270, val loss: 0.7152093648910522
Epoch 280, training loss: 0.2711477279663086 = 0.2641834020614624 + 0.001 * 6.964316368103027
Epoch 280, val loss: 0.7062767744064331
Epoch 290, training loss: 0.24138350784778595 = 0.2344200164079666 + 0.001 * 6.963493347167969
Epoch 290, val loss: 0.7007727026939392
Epoch 300, training loss: 0.2145261913537979 = 0.20756210386753082 + 0.001 * 6.9640889167785645
Epoch 300, val loss: 0.6981298923492432
Epoch 310, training loss: 0.19032861292362213 = 0.1833653301000595 + 0.001 * 6.963276386260986
Epoch 310, val loss: 0.6981140971183777
Epoch 320, training loss: 0.1686697006225586 = 0.16170620918273926 + 0.001 * 6.963494300842285
Epoch 320, val loss: 0.7005301713943481
Epoch 330, training loss: 0.14942944049835205 = 0.142465278506279 + 0.001 * 6.964160442352295
Epoch 330, val loss: 0.7051922082901001
Epoch 340, training loss: 0.13247069716453552 = 0.12550820410251617 + 0.001 * 6.962493896484375
Epoch 340, val loss: 0.71160489320755
Epoch 350, training loss: 0.11761299520730972 = 0.11065170913934708 + 0.001 * 6.961283206939697
Epoch 350, val loss: 0.7195974588394165
Epoch 360, training loss: 0.10466562211513519 = 0.09770280867815018 + 0.001 * 6.96281623840332
Epoch 360, val loss: 0.728956401348114
Epoch 370, training loss: 0.09335743635892868 = 0.08639813214540482 + 0.001 * 6.959305286407471
Epoch 370, val loss: 0.7391912341117859
Epoch 380, training loss: 0.08346053957939148 = 0.07650037109851837 + 0.001 * 6.960171222686768
Epoch 380, val loss: 0.7500737309455872
Epoch 390, training loss: 0.07480211555957794 = 0.06784406304359436 + 0.001 * 6.958053112030029
Epoch 390, val loss: 0.7613534331321716
Epoch 400, training loss: 0.06721261143684387 = 0.06024620682001114 + 0.001 * 6.966407775878906
Epoch 400, val loss: 0.7727907299995422
Epoch 410, training loss: 0.06047895923256874 = 0.053523607552051544 + 0.001 * 6.955350875854492
Epoch 410, val loss: 0.7841832637786865
Epoch 420, training loss: 0.054415032267570496 = 0.047464873641729355 + 0.001 * 6.950159549713135
Epoch 420, val loss: 0.7954429388046265
Epoch 430, training loss: 0.0490071102976799 = 0.04204520955681801 + 0.001 * 6.96190071105957
Epoch 430, val loss: 0.8065621256828308
Epoch 440, training loss: 0.04413314908742905 = 0.037179943174123764 + 0.001 * 6.9532036781311035
Epoch 440, val loss: 0.8178368210792542
Epoch 450, training loss: 0.03993575647473335 = 0.032973214983940125 + 0.001 * 6.9625420570373535
Epoch 450, val loss: 0.8289097547531128
Epoch 460, training loss: 0.03641362115740776 = 0.029475251212716103 + 0.001 * 6.938368797302246
Epoch 460, val loss: 0.8399478197097778
Epoch 470, training loss: 0.033516719937324524 = 0.026582365855574608 + 0.001 * 6.934353351593018
Epoch 470, val loss: 0.8507579565048218
Epoch 480, training loss: 0.031070668250322342 = 0.024126043543219566 + 0.001 * 6.944625377655029
Epoch 480, val loss: 0.861394464969635
Epoch 490, training loss: 0.028946245089173317 = 0.022015661001205444 + 0.001 * 6.930583953857422
Epoch 490, val loss: 0.8718534708023071
Epoch 500, training loss: 0.027117647230625153 = 0.020182378590106964 + 0.001 * 6.935268402099609
Epoch 500, val loss: 0.8823072910308838
Epoch 510, training loss: 0.025508815422654152 = 0.01857759803533554 + 0.001 * 6.931217670440674
Epoch 510, val loss: 0.8923686146736145
Epoch 520, training loss: 0.024124696850776672 = 0.01716327667236328 + 0.001 * 6.961420059204102
Epoch 520, val loss: 0.9020606875419617
Epoch 530, training loss: 0.022823331877589226 = 0.015909826382994652 + 0.001 * 6.913505554199219
Epoch 530, val loss: 0.9115864634513855
Epoch 540, training loss: 0.021703843027353287 = 0.014792040921747684 + 0.001 * 6.911801815032959
Epoch 540, val loss: 0.9209665060043335
Epoch 550, training loss: 0.020779171958565712 = 0.013791053555905819 + 0.001 * 6.988117694854736
Epoch 550, val loss: 0.9301289319992065
Epoch 560, training loss: 0.019816860556602478 = 0.01289133820682764 + 0.001 * 6.925522327423096
Epoch 560, val loss: 0.9390454888343811
Epoch 570, training loss: 0.01900869607925415 = 0.0120795127004385 + 0.001 * 6.929183006286621
Epoch 570, val loss: 0.9477182030677795
Epoch 580, training loss: 0.0182407908141613 = 0.011344773694872856 + 0.001 * 6.896016597747803
Epoch 580, val loss: 0.9561907649040222
Epoch 590, training loss: 0.017567822709679604 = 0.01067767757922411 + 0.001 * 6.890144348144531
Epoch 590, val loss: 0.9644660949707031
Epoch 600, training loss: 0.016989335417747498 = 0.010070827789604664 + 0.001 * 6.9185075759887695
Epoch 600, val loss: 0.9725143909454346
Epoch 610, training loss: 0.01641564816236496 = 0.009516680613160133 + 0.001 * 6.898966312408447
Epoch 610, val loss: 0.9803587794303894
Epoch 620, training loss: 0.015902217477560043 = 0.009009216912090778 + 0.001 * 6.893000602722168
Epoch 620, val loss: 0.9880024194717407
Epoch 630, training loss: 0.015420304611325264 = 0.008543168194591999 + 0.001 * 6.87713623046875
Epoch 630, val loss: 0.9954721927642822
Epoch 640, training loss: 0.015013076364994049 = 0.008114326745271683 + 0.001 * 6.898748874664307
Epoch 640, val loss: 1.0027614831924438
Epoch 650, training loss: 0.014583731070160866 = 0.007719251327216625 + 0.001 * 6.864479064941406
Epoch 650, val loss: 1.0098516941070557
Epoch 660, training loss: 0.014207355678081512 = 0.007354159839451313 + 0.001 * 6.853195667266846
Epoch 660, val loss: 1.016765832901001
Epoch 670, training loss: 0.013888638466596603 = 0.007016059011220932 + 0.001 * 6.872579097747803
Epoch 670, val loss: 1.0235060453414917
Epoch 680, training loss: 0.013570522889494896 = 0.0067026494070887566 + 0.001 * 6.867873191833496
Epoch 680, val loss: 1.03009831905365
Epoch 690, training loss: 0.013280500657856464 = 0.006411627400666475 + 0.001 * 6.868873119354248
Epoch 690, val loss: 1.0365161895751953
Epoch 700, training loss: 0.012974994257092476 = 0.006140696816146374 + 0.001 * 6.834297180175781
Epoch 700, val loss: 1.0427825450897217
Epoch 710, training loss: 0.012731311842799187 = 0.005887966137379408 + 0.001 * 6.8433451652526855
Epoch 710, val loss: 1.048909068107605
Epoch 720, training loss: 0.012500986456871033 = 0.005651781801134348 + 0.001 * 6.8492045402526855
Epoch 720, val loss: 1.0548981428146362
Epoch 730, training loss: 0.012286023236811161 = 0.005431045312434435 + 0.001 * 6.854977607727051
Epoch 730, val loss: 1.0607359409332275
Epoch 740, training loss: 0.012042803689837456 = 0.005224253516644239 + 0.001 * 6.818549156188965
Epoch 740, val loss: 1.0664269924163818
Epoch 750, training loss: 0.011863240972161293 = 0.005030221305787563 + 0.001 * 6.833018779754639
Epoch 750, val loss: 1.072001338005066
Epoch 760, training loss: 0.011656278744339943 = 0.004848024342209101 + 0.001 * 6.808254718780518
Epoch 760, val loss: 1.0774482488632202
Epoch 770, training loss: 0.011514797806739807 = 0.0046766349114477634 + 0.001 * 6.838162422180176
Epoch 770, val loss: 1.0827611684799194
Epoch 780, training loss: 0.011331896297633648 = 0.0045152329839766026 + 0.001 * 6.816662788391113
Epoch 780, val loss: 1.0879548788070679
Epoch 790, training loss: 0.01116246823221445 = 0.004363049753010273 + 0.001 * 6.799417972564697
Epoch 790, val loss: 1.0930423736572266
Epoch 800, training loss: 0.011037472635507584 = 0.004219429567456245 + 0.001 * 6.818042278289795
Epoch 800, val loss: 1.0980160236358643
Epoch 810, training loss: 0.010888228192925453 = 0.004083717707544565 + 0.001 * 6.804510593414307
Epoch 810, val loss: 1.102882981300354
Epoch 820, training loss: 0.01075558178126812 = 0.003955313470214605 + 0.001 * 6.800268173217773
Epoch 820, val loss: 1.1076453924179077
Epoch 830, training loss: 0.010628229938447475 = 0.0038337267469614744 + 0.001 * 6.7945027351379395
Epoch 830, val loss: 1.112304449081421
Epoch 840, training loss: 0.010514790192246437 = 0.003718398977071047 + 0.001 * 6.796391010284424
Epoch 840, val loss: 1.116865634918213
Epoch 850, training loss: 0.010403909720480442 = 0.0036089299246668816 + 0.001 * 6.794979572296143
Epoch 850, val loss: 1.121340036392212
Epoch 860, training loss: 0.010332016274333 = 0.0035049528814852238 + 0.001 * 6.82706356048584
Epoch 860, val loss: 1.1257179975509644
Epoch 870, training loss: 0.010193036869168282 = 0.0034061435144394636 + 0.001 * 6.786892890930176
Epoch 870, val loss: 1.1300115585327148
Epoch 880, training loss: 0.010100971907377243 = 0.0033121302258223295 + 0.001 * 6.788841247558594
Epoch 880, val loss: 1.1342110633850098
Epoch 890, training loss: 0.010022399015724659 = 0.0032226426992565393 + 0.001 * 6.799756050109863
Epoch 890, val loss: 1.138323187828064
Epoch 900, training loss: 0.00994386151432991 = 0.0031373787205666304 + 0.001 * 6.806482791900635
Epoch 900, val loss: 1.142347812652588
Epoch 910, training loss: 0.009858673438429832 = 0.0030560181476175785 + 0.001 * 6.802655220031738
Epoch 910, val loss: 1.146295189857483
Epoch 920, training loss: 0.009805271402001381 = 0.00297835236415267 + 0.001 * 6.826918601989746
Epoch 920, val loss: 1.1501634120941162
Epoch 930, training loss: 0.009661074727773666 = 0.0029041916131973267 + 0.001 * 6.756883144378662
Epoch 930, val loss: 1.15394926071167
Epoch 940, training loss: 0.009623805060982704 = 0.0028332779183983803 + 0.001 * 6.79052734375
Epoch 940, val loss: 1.1576567888259888
Epoch 950, training loss: 0.009545482695102692 = 0.0027654364239424467 + 0.001 * 6.780045509338379
Epoch 950, val loss: 1.1612868309020996
Epoch 960, training loss: 0.009459898807108402 = 0.0027005153242498636 + 0.001 * 6.759382724761963
Epoch 960, val loss: 1.1648527383804321
Epoch 970, training loss: 0.009421131573617458 = 0.0026383097283542156 + 0.001 * 6.7828216552734375
Epoch 970, val loss: 1.1683430671691895
Epoch 980, training loss: 0.009342191740870476 = 0.002578701823949814 + 0.001 * 6.763490200042725
Epoch 980, val loss: 1.1717668771743774
Epoch 990, training loss: 0.009286081418395042 = 0.002521559363231063 + 0.001 * 6.764521598815918
Epoch 990, val loss: 1.1751272678375244
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.7122
Flip ASR: 0.6533/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.947253942489624 = 1.9388800859451294 + 0.001 * 8.373847961425781
Epoch 0, val loss: 1.9471380710601807
Epoch 10, training loss: 1.936948299407959 = 1.928574562072754 + 0.001 * 8.373767852783203
Epoch 10, val loss: 1.9359544515609741
Epoch 20, training loss: 1.9243158102035522 = 1.9159423112869263 + 0.001 * 8.373481750488281
Epoch 20, val loss: 1.921960711479187
Epoch 30, training loss: 1.9066109657287598 = 1.8982380628585815 + 0.001 * 8.372848510742188
Epoch 30, val loss: 1.9022486209869385
Epoch 40, training loss: 1.8804954290390015 = 1.8721240758895874 + 0.001 * 8.37135124206543
Epoch 40, val loss: 1.8735363483428955
Epoch 50, training loss: 1.8439527750015259 = 1.8355858325958252 + 0.001 * 8.366945266723633
Epoch 50, val loss: 1.8352220058441162
Epoch 60, training loss: 1.8018403053283691 = 1.793491005897522 + 0.001 * 8.349352836608887
Epoch 60, val loss: 1.7951267957687378
Epoch 70, training loss: 1.7586979866027832 = 1.7504425048828125 + 0.001 * 8.255526542663574
Epoch 70, val loss: 1.7570528984069824
Epoch 80, training loss: 1.7012873888015747 = 1.6934525966644287 + 0.001 * 7.834779262542725
Epoch 80, val loss: 1.7076359987258911
Epoch 90, training loss: 1.62456214427948 = 1.616862177848816 + 0.001 * 7.699982166290283
Epoch 90, val loss: 1.6431159973144531
Epoch 100, training loss: 1.5302890539169312 = 1.5227702856063843 + 0.001 * 7.518743991851807
Epoch 100, val loss: 1.5668293237686157
Epoch 110, training loss: 1.4316086769104004 = 1.4242947101593018 + 0.001 * 7.314023017883301
Epoch 110, val loss: 1.4909942150115967
Epoch 120, training loss: 1.3358594179153442 = 1.3286205530166626 + 0.001 * 7.238898754119873
Epoch 120, val loss: 1.4219688177108765
Epoch 130, training loss: 1.2414642572402954 = 1.234291911125183 + 0.001 * 7.172323703765869
Epoch 130, val loss: 1.3557575941085815
Epoch 140, training loss: 1.145713448524475 = 1.1385912895202637 + 0.001 * 7.122124195098877
Epoch 140, val loss: 1.2880146503448486
Epoch 150, training loss: 1.0484557151794434 = 1.0413591861724854 + 0.001 * 7.09648323059082
Epoch 150, val loss: 1.2177520990371704
Epoch 160, training loss: 0.9524720311164856 = 0.9453879594802856 + 0.001 * 7.0840606689453125
Epoch 160, val loss: 1.148725152015686
Epoch 170, training loss: 0.8616924285888672 = 0.854616105556488 + 0.001 * 7.0763373374938965
Epoch 170, val loss: 1.0848610401153564
Epoch 180, training loss: 0.7789332270622253 = 0.7718625068664551 + 0.001 * 7.070712089538574
Epoch 180, val loss: 1.028315544128418
Epoch 190, training loss: 0.7049907445907593 = 0.6979256272315979 + 0.001 * 7.065117835998535
Epoch 190, val loss: 0.9797536134719849
Epoch 200, training loss: 0.6389594674110413 = 0.6318994760513306 + 0.001 * 7.059972763061523
Epoch 200, val loss: 0.9388678669929504
Epoch 210, training loss: 0.5792548656463623 = 0.5721993446350098 + 0.001 * 7.055504322052002
Epoch 210, val loss: 0.9047710299491882
Epoch 220, training loss: 0.5242007970809937 = 0.5171489119529724 + 0.001 * 7.051889896392822
Epoch 220, val loss: 0.8757829666137695
Epoch 230, training loss: 0.4724452495574951 = 0.4653962552547455 + 0.001 * 7.048983097076416
Epoch 230, val loss: 0.8513846397399902
Epoch 240, training loss: 0.42307063937187195 = 0.41602423787117004 + 0.001 * 7.046408176422119
Epoch 240, val loss: 0.8314709663391113
Epoch 250, training loss: 0.3757678270339966 = 0.368724524974823 + 0.001 * 7.043287754058838
Epoch 250, val loss: 0.8160668611526489
Epoch 260, training loss: 0.3307994604110718 = 0.3237612247467041 + 0.001 * 7.038249969482422
Epoch 260, val loss: 0.8041280508041382
Epoch 270, training loss: 0.28905609250068665 = 0.282024085521698 + 0.001 * 7.031993865966797
Epoch 270, val loss: 0.7948862910270691
Epoch 280, training loss: 0.25132423639297485 = 0.24430061876773834 + 0.001 * 7.023623943328857
Epoch 280, val loss: 0.788395345211029
Epoch 290, training loss: 0.2179371863603592 = 0.21092663705348969 + 0.001 * 7.0105485916137695
Epoch 290, val loss: 0.7845244407653809
Epoch 300, training loss: 0.18875865638256073 = 0.1817600280046463 + 0.001 * 6.998624801635742
Epoch 300, val loss: 0.782694399356842
Epoch 310, training loss: 0.16353589296340942 = 0.15654684603214264 + 0.001 * 6.989039897918701
Epoch 310, val loss: 0.7832450270652771
Epoch 320, training loss: 0.14181514084339142 = 0.13484208285808563 + 0.001 * 6.973056793212891
Epoch 320, val loss: 0.7858193516731262
Epoch 330, training loss: 0.12319052964448929 = 0.11621546000242233 + 0.001 * 6.975070953369141
Epoch 330, val loss: 0.7904482483863831
Epoch 340, training loss: 0.10720480233430862 = 0.10024414211511612 + 0.001 * 6.960659980773926
Epoch 340, val loss: 0.7969455718994141
Epoch 350, training loss: 0.09352448582649231 = 0.08656079322099686 + 0.001 * 6.963690280914307
Epoch 350, val loss: 0.8051567077636719
Epoch 360, training loss: 0.08190496265888214 = 0.0749426931142807 + 0.001 * 6.962265491485596
Epoch 360, val loss: 0.8153299689292908
Epoch 370, training loss: 0.07215436547994614 = 0.06519456952810287 + 0.001 * 6.959793567657471
Epoch 370, val loss: 0.8272489309310913
Epoch 380, training loss: 0.06403502076864243 = 0.05707598105072975 + 0.001 * 6.95904016494751
Epoch 380, val loss: 0.8407564163208008
Epoch 390, training loss: 0.05725058913230896 = 0.05029437690973282 + 0.001 * 6.956210613250732
Epoch 390, val loss: 0.8550333976745605
Epoch 400, training loss: 0.05154049023985863 = 0.044587407261133194 + 0.001 * 6.95308256149292
Epoch 400, val loss: 0.8696273565292358
Epoch 410, training loss: 0.04669797793030739 = 0.03974785655736923 + 0.001 * 6.9501214027404785
Epoch 410, val loss: 0.8841824531555176
Epoch 420, training loss: 0.04256686940789223 = 0.035621948540210724 + 0.001 * 6.944919586181641
Epoch 420, val loss: 0.898719072341919
Epoch 430, training loss: 0.03903409093618393 = 0.032088905572891235 + 0.001 * 6.94518518447876
Epoch 430, val loss: 0.9128983020782471
Epoch 440, training loss: 0.035992324352264404 = 0.029046377167105675 + 0.001 * 6.9459452629089355
Epoch 440, val loss: 0.9267686009407043
Epoch 450, training loss: 0.03335626423358917 = 0.026411844417452812 + 0.001 * 6.944418430328369
Epoch 450, val loss: 0.940370500087738
Epoch 460, training loss: 0.031050194054841995 = 0.024114498868584633 + 0.001 * 6.935694694519043
Epoch 460, val loss: 0.9536295533180237
Epoch 470, training loss: 0.029047423973679543 = 0.02210252918303013 + 0.001 * 6.944894790649414
Epoch 470, val loss: 0.9665881991386414
Epoch 480, training loss: 0.027266215533018112 = 0.02033371850848198 + 0.001 * 6.932497501373291
Epoch 480, val loss: 0.9792197942733765
Epoch 490, training loss: 0.02570538967847824 = 0.018770543858408928 + 0.001 * 6.934845447540283
Epoch 490, val loss: 0.9914432168006897
Epoch 500, training loss: 0.024310994893312454 = 0.017382752150297165 + 0.001 * 6.928243160247803
Epoch 500, val loss: 1.0033303499221802
Epoch 510, training loss: 0.023069487884640694 = 0.016145359724760056 + 0.001 * 6.924127578735352
Epoch 510, val loss: 1.0148743391036987
Epoch 520, training loss: 0.021955430507659912 = 0.015038364566862583 + 0.001 * 6.917065620422363
Epoch 520, val loss: 1.026132583618164
Epoch 530, training loss: 0.020964741706848145 = 0.014043939299881458 + 0.001 * 6.920802593231201
Epoch 530, val loss: 1.0370984077453613
Epoch 540, training loss: 0.02005537785589695 = 0.013147330842912197 + 0.001 * 6.908046245574951
Epoch 540, val loss: 1.0477219820022583
Epoch 550, training loss: 0.019240740686655045 = 0.012336306273937225 + 0.001 * 6.904435157775879
Epoch 550, val loss: 1.0580586194992065
Epoch 560, training loss: 0.018500979989767075 = 0.011599958874285221 + 0.001 * 6.901020050048828
Epoch 560, val loss: 1.0681616067886353
Epoch 570, training loss: 0.017848802730441093 = 0.010929767973721027 + 0.001 * 6.919034004211426
Epoch 570, val loss: 1.0779670476913452
Epoch 580, training loss: 0.01721162721514702 = 0.01031817402690649 + 0.001 * 6.893453598022461
Epoch 580, val loss: 1.087518334388733
Epoch 590, training loss: 0.01666056364774704 = 0.009758600033819675 + 0.001 * 6.90196418762207
Epoch 590, val loss: 1.0968246459960938
Epoch 600, training loss: 0.01615707203745842 = 0.009245365858078003 + 0.001 * 6.91170597076416
Epoch 600, val loss: 1.1058752536773682
Epoch 610, training loss: 0.015667365863919258 = 0.00877333339303732 + 0.001 * 6.894031524658203
Epoch 610, val loss: 1.1147170066833496
Epoch 620, training loss: 0.015208128839731216 = 0.008338257670402527 + 0.001 * 6.869871139526367
Epoch 620, val loss: 1.123322606086731
Epoch 630, training loss: 0.014802737161517143 = 0.007936762645840645 + 0.001 * 6.865974426269531
Epoch 630, val loss: 1.1317270994186401
Epoch 640, training loss: 0.01442929171025753 = 0.0075654154643416405 + 0.001 * 6.863875865936279
Epoch 640, val loss: 1.139907717704773
Epoch 650, training loss: 0.01408479269593954 = 0.00722124008461833 + 0.001 * 6.863552093505859
Epoch 650, val loss: 1.1478677988052368
Epoch 660, training loss: 0.013740766793489456 = 0.0069016325287520885 + 0.001 * 6.839133262634277
Epoch 660, val loss: 1.1556330919265747
Epoch 670, training loss: 0.013497905805706978 = 0.00660444563254714 + 0.001 * 6.893459796905518
Epoch 670, val loss: 1.1631821393966675
Epoch 680, training loss: 0.013191446661949158 = 0.006327570416033268 + 0.001 * 6.863875389099121
Epoch 680, val loss: 1.1705877780914307
Epoch 690, training loss: 0.012956218793988228 = 0.0060691493563354015 + 0.001 * 6.887069225311279
Epoch 690, val loss: 1.1778039932250977
Epoch 700, training loss: 0.012679394334554672 = 0.005827554501593113 + 0.001 * 6.851839542388916
Epoch 700, val loss: 1.1848649978637695
Epoch 710, training loss: 0.01244848221540451 = 0.005601254291832447 + 0.001 * 6.847227096557617
Epoch 710, val loss: 1.191733479499817
Epoch 720, training loss: 0.012225156649947166 = 0.005388792138546705 + 0.001 * 6.836363792419434
Epoch 720, val loss: 1.198459267616272
Epoch 730, training loss: 0.012051863595843315 = 0.005189262796193361 + 0.001 * 6.862600803375244
Epoch 730, val loss: 1.2050410509109497
Epoch 740, training loss: 0.011816514655947685 = 0.005001725163310766 + 0.001 * 6.814789295196533
Epoch 740, val loss: 1.2114650011062622
Epoch 750, training loss: 0.011645148508250713 = 0.004825329873710871 + 0.001 * 6.819818496704102
Epoch 750, val loss: 1.2177395820617676
Epoch 760, training loss: 0.011465414427220821 = 0.004659133963286877 + 0.001 * 6.806280136108398
Epoch 760, val loss: 1.223889708518982
Epoch 770, training loss: 0.011307889595627785 = 0.004502386320382357 + 0.001 * 6.805502891540527
Epoch 770, val loss: 1.22987961769104
Epoch 780, training loss: 0.011186645366251469 = 0.004354356788098812 + 0.001 * 6.8322882652282715
Epoch 780, val loss: 1.2357478141784668
Epoch 790, training loss: 0.011018983088433743 = 0.004214351065456867 + 0.001 * 6.80463171005249
Epoch 790, val loss: 1.241500973701477
Epoch 800, training loss: 0.010891305282711983 = 0.00408182991668582 + 0.001 * 6.809474945068359
Epoch 800, val loss: 1.2471387386322021
Epoch 810, training loss: 0.010749481618404388 = 0.003956269007176161 + 0.001 * 6.793212413787842
Epoch 810, val loss: 1.252669095993042
Epoch 820, training loss: 0.010626276023685932 = 0.003837171010673046 + 0.001 * 6.789104461669922
Epoch 820, val loss: 1.2580840587615967
Epoch 830, training loss: 0.010527093894779682 = 0.003724134061485529 + 0.001 * 6.802959442138672
Epoch 830, val loss: 1.2633978128433228
Epoch 840, training loss: 0.010435814969241619 = 0.0036167153157293797 + 0.001 * 6.819099426269531
Epoch 840, val loss: 1.268614411354065
Epoch 850, training loss: 0.010324866510927677 = 0.0035145652946084738 + 0.001 * 6.810301303863525
Epoch 850, val loss: 1.2737020254135132
Epoch 860, training loss: 0.010201992467045784 = 0.0034173340536653996 + 0.001 * 6.7846574783325195
Epoch 860, val loss: 1.278720498085022
Epoch 870, training loss: 0.010108806192874908 = 0.0033247137907892466 + 0.001 * 6.784092426300049
Epoch 870, val loss: 1.2836196422576904
Epoch 880, training loss: 0.010030511766672134 = 0.003236406482756138 + 0.001 * 6.79410457611084
Epoch 880, val loss: 1.2884447574615479
Epoch 890, training loss: 0.009924096055328846 = 0.0031521569471806288 + 0.001 * 6.771938800811768
Epoch 890, val loss: 1.2931673526763916
Epoch 900, training loss: 0.009862298145890236 = 0.0030717074405401945 + 0.001 * 6.790590763092041
Epoch 900, val loss: 1.2978073358535767
Epoch 910, training loss: 0.009800442494452 = 0.002994864247739315 + 0.001 * 6.805577754974365
Epoch 910, val loss: 1.3023672103881836
Epoch 920, training loss: 0.009719623252749443 = 0.0029213635716587305 + 0.001 * 6.798259258270264
Epoch 920, val loss: 1.3068441152572632
Epoch 930, training loss: 0.00960767176002264 = 0.0028510328847914934 + 0.001 * 6.756639003753662
Epoch 930, val loss: 1.3112423419952393
Epoch 940, training loss: 0.009547663852572441 = 0.002783700590953231 + 0.001 * 6.763962745666504
Epoch 940, val loss: 1.3155368566513062
Epoch 950, training loss: 0.009494978003203869 = 0.002719209296628833 + 0.001 * 6.775768280029297
Epoch 950, val loss: 1.3197909593582153
Epoch 960, training loss: 0.009414007887244225 = 0.0026574304793030024 + 0.001 * 6.756577014923096
Epoch 960, val loss: 1.323954463005066
Epoch 970, training loss: 0.00935627892613411 = 0.0025981548242270947 + 0.001 * 6.758123397827148
Epoch 970, val loss: 1.3280489444732666
Epoch 980, training loss: 0.009302916005253792 = 0.0025412742979824543 + 0.001 * 6.761641979217529
Epoch 980, val loss: 1.3320786952972412
Epoch 990, training loss: 0.009257948026061058 = 0.002486647106707096 + 0.001 * 6.771300792694092
Epoch 990, val loss: 1.3360190391540527
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.8044
Flip ASR: 0.7733/225 nodes
The final ASR:0.76999, 0.04113, Accuracy:0.81852, 0.01600
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9414])
updated graph: torch.Size([2, 10470])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00603, Accuracy:0.82963, 0.00605
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.957351803779602 = 1.9489779472351074 + 0.001 * 8.373912811279297
Epoch 0, val loss: 1.9474629163742065
Epoch 10, training loss: 1.946766972541809 = 1.9383931159973145 + 0.001 * 8.373883247375488
Epoch 10, val loss: 1.9379496574401855
Epoch 20, training loss: 1.9343119859695435 = 1.9259382486343384 + 0.001 * 8.373736381530762
Epoch 20, val loss: 1.9262202978134155
Epoch 30, training loss: 1.9172282218933105 = 1.9088548421859741 + 0.001 * 8.373390197753906
Epoch 30, val loss: 1.909901738166809
Epoch 40, training loss: 1.892376184463501 = 1.8840036392211914 + 0.001 * 8.372584342956543
Epoch 40, val loss: 1.8863911628723145
Epoch 50, training loss: 1.8573744297027588 = 1.8490040302276611 + 0.001 * 8.370403289794922
Epoch 50, val loss: 1.8547135591506958
Epoch 60, training loss: 1.8155224323272705 = 1.807159423828125 + 0.001 * 8.362956047058105
Epoch 60, val loss: 1.8204692602157593
Epoch 70, training loss: 1.776289939880371 = 1.7679606676101685 + 0.001 * 8.32921314239502
Epoch 70, val loss: 1.790619969367981
Epoch 80, training loss: 1.7295361757278442 = 1.721461296081543 + 0.001 * 8.074931144714355
Epoch 80, val loss: 1.749480128288269
Epoch 90, training loss: 1.6635493040084839 = 1.6556366682052612 + 0.001 * 7.91264009475708
Epoch 90, val loss: 1.6915216445922852
Epoch 100, training loss: 1.576149821281433 = 1.5682998895645142 + 0.001 * 7.849958896636963
Epoch 100, val loss: 1.619594931602478
Epoch 110, training loss: 1.4763767719268799 = 1.4685616493225098 + 0.001 * 7.815165996551514
Epoch 110, val loss: 1.5393532514572144
Epoch 120, training loss: 1.3775924444198608 = 1.369831919670105 + 0.001 * 7.760490894317627
Epoch 120, val loss: 1.4617493152618408
Epoch 130, training loss: 1.2853553295135498 = 1.2777838706970215 + 0.001 * 7.571506500244141
Epoch 130, val loss: 1.392693281173706
Epoch 140, training loss: 1.2003427743911743 = 1.1928925514221191 + 0.001 * 7.450170516967773
Epoch 140, val loss: 1.3321369886398315
Epoch 150, training loss: 1.1208529472351074 = 1.1134940385818481 + 0.001 * 7.358860015869141
Epoch 150, val loss: 1.276884913444519
Epoch 160, training loss: 1.0434905290603638 = 1.0362093448638916 + 0.001 * 7.281208038330078
Epoch 160, val loss: 1.2245968580245972
Epoch 170, training loss: 0.9660674333572388 = 0.9588269591331482 + 0.001 * 7.24049711227417
Epoch 170, val loss: 1.1724587678909302
Epoch 180, training loss: 0.8886529803276062 = 0.8814273476600647 + 0.001 * 7.225639343261719
Epoch 180, val loss: 1.1189814805984497
Epoch 190, training loss: 0.8117972016334534 = 0.8045966029167175 + 0.001 * 7.20062255859375
Epoch 190, val loss: 1.0642812252044678
Epoch 200, training loss: 0.7361886501312256 = 0.7290212512016296 + 0.001 * 7.16741943359375
Epoch 200, val loss: 1.0092002153396606
Epoch 210, training loss: 0.663348376750946 = 0.6562163829803467 + 0.001 * 7.132003307342529
Epoch 210, val loss: 0.9561629891395569
Epoch 220, training loss: 0.5954471826553345 = 0.588327944278717 + 0.001 * 7.119245529174805
Epoch 220, val loss: 0.908126711845398
Epoch 230, training loss: 0.5343292355537415 = 0.5272197723388672 + 0.001 * 7.109487533569336
Epoch 230, val loss: 0.8673929572105408
Epoch 240, training loss: 0.48071813583374023 = 0.4736137092113495 + 0.001 * 7.1044230461120605
Epoch 240, val loss: 0.8350820541381836
Epoch 250, training loss: 0.4338882863521576 = 0.4267857074737549 + 0.001 * 7.102569103240967
Epoch 250, val loss: 0.8106778860092163
Epoch 260, training loss: 0.3923394978046417 = 0.3852384090423584 + 0.001 * 7.101095676422119
Epoch 260, val loss: 0.7928915619850159
Epoch 270, training loss: 0.35460275411605835 = 0.3475039005279541 + 0.001 * 7.098840236663818
Epoch 270, val loss: 0.7803077697753906
Epoch 280, training loss: 0.3195151388645172 = 0.3124200999736786 + 0.001 * 7.095027923583984
Epoch 280, val loss: 0.7714704275131226
Epoch 290, training loss: 0.2863404452800751 = 0.27925169467926025 + 0.001 * 7.088736534118652
Epoch 290, val loss: 0.7652466893196106
Epoch 300, training loss: 0.25477340817451477 = 0.24768994748592377 + 0.001 * 7.083461284637451
Epoch 300, val loss: 0.7610850930213928
Epoch 310, training loss: 0.22502155601978302 = 0.21795439720153809 + 0.001 * 7.067157745361328
Epoch 310, val loss: 0.7585375905036926
Epoch 320, training loss: 0.19760762155056 = 0.1905486136674881 + 0.001 * 7.0590128898620605
Epoch 320, val loss: 0.758001446723938
Epoch 330, training loss: 0.17292232811450958 = 0.1658821552991867 + 0.001 * 7.040177345275879
Epoch 330, val loss: 0.7597429156303406
Epoch 340, training loss: 0.15113747119903564 = 0.14410121738910675 + 0.001 * 7.036247730255127
Epoch 340, val loss: 0.7635859847068787
Epoch 350, training loss: 0.13214607536792755 = 0.12512342631816864 + 0.001 * 7.0226545333862305
Epoch 350, val loss: 0.7696385979652405
Epoch 360, training loss: 0.11575916409492493 = 0.10873595625162125 + 0.001 * 7.023208141326904
Epoch 360, val loss: 0.7776944637298584
Epoch 370, training loss: 0.10169585049152374 = 0.0946740135550499 + 0.001 * 7.021833419799805
Epoch 370, val loss: 0.7874404788017273
Epoch 380, training loss: 0.08968421071767807 = 0.08266517519950867 + 0.001 * 7.019035339355469
Epoch 380, val loss: 0.7985547184944153
Epoch 390, training loss: 0.07946394383907318 = 0.07244749367237091 + 0.001 * 7.016446590423584
Epoch 390, val loss: 0.8105921149253845
Epoch 400, training loss: 0.0707789808511734 = 0.06376241892576218 + 0.001 * 7.016564846038818
Epoch 400, val loss: 0.823177695274353
Epoch 410, training loss: 0.06339534372091293 = 0.05637872964143753 + 0.001 * 7.0166144371032715
Epoch 410, val loss: 0.8360385298728943
Epoch 420, training loss: 0.057099245488643646 = 0.05008534714579582 + 0.001 * 7.0138959884643555
Epoch 420, val loss: 0.8488748669624329
Epoch 430, training loss: 0.051720160990953445 = 0.04469754174351692 + 0.001 * 7.022617816925049
Epoch 430, val loss: 0.8616154789924622
Epoch 440, training loss: 0.04707761108875275 = 0.04006197303533554 + 0.001 * 7.0156378746032715
Epoch 440, val loss: 0.874042809009552
Epoch 450, training loss: 0.04306422173976898 = 0.036053359508514404 + 0.001 * 7.010862350463867
Epoch 450, val loss: 0.8863316178321838
Epoch 460, training loss: 0.03957925736904144 = 0.032570675015449524 + 0.001 * 7.008584022521973
Epoch 460, val loss: 0.8982386589050293
Epoch 470, training loss: 0.03655175119638443 = 0.02953263930976391 + 0.001 * 7.019113540649414
Epoch 470, val loss: 0.9098120927810669
Epoch 480, training loss: 0.033880800008773804 = 0.026873096823692322 + 0.001 * 7.0077033042907715
Epoch 480, val loss: 0.9211359024047852
Epoch 490, training loss: 0.03154279291629791 = 0.02453736774623394 + 0.001 * 7.0054240226745605
Epoch 490, val loss: 0.9321675896644592
Epoch 500, training loss: 0.0294855497777462 = 0.02247942052781582 + 0.001 * 7.006128787994385
Epoch 500, val loss: 0.9429261088371277
Epoch 510, training loss: 0.02766498550772667 = 0.020660344511270523 + 0.001 * 7.004639625549316
Epoch 510, val loss: 0.9533644914627075
Epoch 520, training loss: 0.026052065193653107 = 0.01904764212667942 + 0.001 * 7.004422187805176
Epoch 520, val loss: 0.9635375142097473
Epoch 530, training loss: 0.024613741785287857 = 0.017614418640732765 + 0.001 * 6.99932336807251
Epoch 530, val loss: 0.9734269976615906
Epoch 540, training loss: 0.023333894088864326 = 0.016335949301719666 + 0.001 * 6.9979448318481445
Epoch 540, val loss: 0.9831192493438721
Epoch 550, training loss: 0.022185765206813812 = 0.01519222091883421 + 0.001 * 6.9935431480407715
Epoch 550, val loss: 0.9924886226654053
Epoch 560, training loss: 0.021162940189242363 = 0.014165478758513927 + 0.001 * 6.997460842132568
Epoch 560, val loss: 1.0016307830810547
Epoch 570, training loss: 0.020229125395417213 = 0.013241082429885864 + 0.001 * 6.98804235458374
Epoch 570, val loss: 1.010454535484314
Epoch 580, training loss: 0.01939142495393753 = 0.012406307272613049 + 0.001 * 6.985118389129639
Epoch 580, val loss: 1.0190637111663818
Epoch 590, training loss: 0.018635082989931107 = 0.011650142259895802 + 0.001 * 6.984940052032471
Epoch 590, val loss: 1.0274102687835693
Epoch 600, training loss: 0.01794193871319294 = 0.01096328441053629 + 0.001 * 6.978653430938721
Epoch 600, val loss: 1.0355263948440552
Epoch 610, training loss: 0.017337633296847343 = 0.01033771876245737 + 0.001 * 6.999914169311523
Epoch 610, val loss: 1.0433993339538574
Epoch 620, training loss: 0.016744859516620636 = 0.009766461327672005 + 0.001 * 6.97839879989624
Epoch 620, val loss: 1.051068902015686
Epoch 630, training loss: 0.016214262694120407 = 0.009243492968380451 + 0.001 * 6.970770359039307
Epoch 630, val loss: 1.0584757328033447
Epoch 640, training loss: 0.0157444030046463 = 0.008763578720390797 + 0.001 * 6.980824947357178
Epoch 640, val loss: 1.0657254457473755
Epoch 650, training loss: 0.01528589241206646 = 0.008322233334183693 + 0.001 * 6.963659286499023
Epoch 650, val loss: 1.0727490186691284
Epoch 660, training loss: 0.01488753966987133 = 0.007915408350527287 + 0.001 * 6.972131252288818
Epoch 660, val loss: 1.079582691192627
Epoch 670, training loss: 0.014507438987493515 = 0.007539627607911825 + 0.001 * 6.96781063079834
Epoch 670, val loss: 1.086213231086731
Epoch 680, training loss: 0.014161542057991028 = 0.0071918838657438755 + 0.001 * 6.969657897949219
Epoch 680, val loss: 1.0926616191864014
Epoch 690, training loss: 0.013827895745635033 = 0.006869389675557613 + 0.001 * 6.958505153656006
Epoch 690, val loss: 1.0989396572113037
Epoch 700, training loss: 0.013526717200875282 = 0.006569800898432732 + 0.001 * 6.956915378570557
Epoch 700, val loss: 1.105080246925354
Epoch 710, training loss: 0.013235383667051792 = 0.006290992721915245 + 0.001 * 6.944390773773193
Epoch 710, val loss: 1.1110047101974487
Epoch 720, training loss: 0.012992069125175476 = 0.006030964199453592 + 0.001 * 6.961103916168213
Epoch 720, val loss: 1.1168276071548462
Epoch 730, training loss: 0.012727456167340279 = 0.005788097623735666 + 0.001 * 6.939358711242676
Epoch 730, val loss: 1.1224702596664429
Epoch 740, training loss: 0.012552938424050808 = 0.005560712888836861 + 0.001 * 6.992225170135498
Epoch 740, val loss: 1.1279562711715698
Epoch 750, training loss: 0.012282844632863998 = 0.0053475056774914265 + 0.001 * 6.935338497161865
Epoch 750, val loss: 1.1333307027816772
Epoch 760, training loss: 0.012074143625795841 = 0.005147086922079325 + 0.001 * 6.927056312561035
Epoch 760, val loss: 1.1385931968688965
Epoch 770, training loss: 0.011916793882846832 = 0.004958105273544788 + 0.001 * 6.958687782287598
Epoch 770, val loss: 1.1436952352523804
Epoch 780, training loss: 0.011710649356245995 = 0.004779312293976545 + 0.001 * 6.931336402893066
Epoch 780, val loss: 1.148712158203125
Epoch 790, training loss: 0.011524667963385582 = 0.0046096071600914 + 0.001 * 6.915060043334961
Epoch 790, val loss: 1.1536178588867188
Epoch 800, training loss: 0.011375543661415577 = 0.0044480133801698685 + 0.001 * 6.927529811859131
Epoch 800, val loss: 1.1584875583648682
Epoch 810, training loss: 0.011204158887267113 = 0.004294037353247404 + 0.001 * 6.910120964050293
Epoch 810, val loss: 1.1632554531097412
Epoch 820, training loss: 0.011063738726079464 = 0.004147263243794441 + 0.001 * 6.916475296020508
Epoch 820, val loss: 1.168007493019104
Epoch 830, training loss: 0.010955839417874813 = 0.004007325042039156 + 0.001 * 6.948513984680176
Epoch 830, val loss: 1.172684907913208
Epoch 840, training loss: 0.010778645053505898 = 0.0038739354349672794 + 0.001 * 6.904709339141846
Epoch 840, val loss: 1.1773042678833008
Epoch 850, training loss: 0.010649040341377258 = 0.003746866714209318 + 0.001 * 6.902173042297363
Epoch 850, val loss: 1.1818841695785522
Epoch 860, training loss: 0.010530859231948853 = 0.003625806188210845 + 0.001 * 6.905052661895752
Epoch 860, val loss: 1.1863844394683838
Epoch 870, training loss: 0.010456145741045475 = 0.0035105161368846893 + 0.001 * 6.945629119873047
Epoch 870, val loss: 1.1908361911773682
Epoch 880, training loss: 0.010293375700712204 = 0.003400788176804781 + 0.001 * 6.892587184906006
Epoch 880, val loss: 1.1951853036880493
Epoch 890, training loss: 0.010220612399280071 = 0.0032963191624730825 + 0.001 * 6.924293041229248
Epoch 890, val loss: 1.1995433568954468
Epoch 900, training loss: 0.010082593187689781 = 0.0031968397088348866 + 0.001 * 6.8857526779174805
Epoch 900, val loss: 1.2037103176116943
Epoch 910, training loss: 0.00999198667705059 = 0.0031020573806017637 + 0.001 * 6.889928817749023
Epoch 910, val loss: 1.2078776359558105
Epoch 920, training loss: 0.009888795204460621 = 0.0030117330607026815 + 0.001 * 6.877061367034912
Epoch 920, val loss: 1.2119368314743042
Epoch 930, training loss: 0.009812035597860813 = 0.0029256532434374094 + 0.001 * 6.88638162612915
Epoch 930, val loss: 1.215921401977539
Epoch 940, training loss: 0.009738808497786522 = 0.0028435627464205027 + 0.001 * 6.89524507522583
Epoch 940, val loss: 1.219847559928894
Epoch 950, training loss: 0.009655154310166836 = 0.0027652503922581673 + 0.001 * 6.889903545379639
Epoch 950, val loss: 1.2236518859863281
Epoch 960, training loss: 0.009585870429873466 = 0.0026904912665486336 + 0.001 * 6.895378589630127
Epoch 960, val loss: 1.2274357080459595
Epoch 970, training loss: 0.009492171928286552 = 0.0026191063225269318 + 0.001 * 6.873064994812012
Epoch 970, val loss: 1.2310930490493774
Epoch 980, training loss: 0.00942481029778719 = 0.002550884149968624 + 0.001 * 6.873925685882568
Epoch 980, val loss: 1.2346744537353516
Epoch 990, training loss: 0.009352780878543854 = 0.0024856775999069214 + 0.001 * 6.867103099822998
Epoch 990, val loss: 1.2382344007492065
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.4834
Flip ASR: 0.3778/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.96042001247406 = 1.9520460367202759 + 0.001 * 8.37392520904541
Epoch 0, val loss: 1.9501266479492188
Epoch 10, training loss: 1.949863314628601 = 1.9414894580841064 + 0.001 * 8.373876571655273
Epoch 10, val loss: 1.9397741556167603
Epoch 20, training loss: 1.937238097190857 = 1.9288643598556519 + 0.001 * 8.373726844787598
Epoch 20, val loss: 1.9269051551818848
Epoch 30, training loss: 1.9199153184890747 = 1.9115419387817383 + 0.001 * 8.373387336730957
Epoch 30, val loss: 1.9089542627334595
Epoch 40, training loss: 1.8949111700057983 = 1.8865385055541992 + 0.001 * 8.372623443603516
Epoch 40, val loss: 1.8832827806472778
Epoch 50, training loss: 1.8595401048660278 = 1.851169466972351 + 0.001 * 8.370686531066895
Epoch 50, val loss: 1.8482334613800049
Epoch 60, training loss: 1.8154317140579224 = 1.8070671558380127 + 0.001 * 8.364614486694336
Epoch 60, val loss: 1.8071136474609375
Epoch 70, training loss: 1.7707678079605103 = 1.762429118156433 + 0.001 * 8.338740348815918
Epoch 70, val loss: 1.767683982849121
Epoch 80, training loss: 1.7208095788955688 = 1.7126545906066895 + 0.001 * 8.155046463012695
Epoch 80, val loss: 1.7224420309066772
Epoch 90, training loss: 1.6513175964355469 = 1.6434568166732788 + 0.001 * 7.860780715942383
Epoch 90, val loss: 1.6620467901229858
Epoch 100, training loss: 1.5604028701782227 = 1.552667260169983 + 0.001 * 7.735607624053955
Epoch 100, val loss: 1.586495280265808
Epoch 110, training loss: 1.454991102218628 = 1.4474029541015625 + 0.001 * 7.588150978088379
Epoch 110, val loss: 1.4996806383132935
Epoch 120, training loss: 1.3475534915924072 = 1.3401503562927246 + 0.001 * 7.403126239776611
Epoch 120, val loss: 1.412003755569458
Epoch 130, training loss: 1.244555115699768 = 1.2371578216552734 + 0.001 * 7.3972673416137695
Epoch 130, val loss: 1.3306512832641602
Epoch 140, training loss: 1.1470662355422974 = 1.139709234237671 + 0.001 * 7.357004165649414
Epoch 140, val loss: 1.2558488845825195
Epoch 150, training loss: 1.0546804666519165 = 1.0473538637161255 + 0.001 * 7.326550483703613
Epoch 150, val loss: 1.1861498355865479
Epoch 160, training loss: 0.9664689302444458 = 0.9591854810714722 + 0.001 * 7.283463478088379
Epoch 160, val loss: 1.1202926635742188
Epoch 170, training loss: 0.8818790316581726 = 0.8746597766876221 + 0.001 * 7.219278335571289
Epoch 170, val loss: 1.0569474697113037
Epoch 180, training loss: 0.8010000586509705 = 0.7938456535339355 + 0.001 * 7.154409408569336
Epoch 180, val loss: 0.9970279335975647
Epoch 190, training loss: 0.724720299243927 = 0.7175926566123962 + 0.001 * 7.127615928649902
Epoch 190, val loss: 0.9418452978134155
Epoch 200, training loss: 0.6543030738830566 = 0.6471834182739258 + 0.001 * 7.119658470153809
Epoch 200, val loss: 0.8932065367698669
Epoch 210, training loss: 0.590334415435791 = 0.583217978477478 + 0.001 * 7.116433620452881
Epoch 210, val loss: 0.8520110249519348
Epoch 220, training loss: 0.5323233604431152 = 0.5252110362052917 + 0.001 * 7.112339019775391
Epoch 220, val loss: 0.8175908327102661
Epoch 230, training loss: 0.4793180823326111 = 0.4722103774547577 + 0.001 * 7.107702255249023
Epoch 230, val loss: 0.7889010906219482
Epoch 240, training loss: 0.43069157004356384 = 0.4235892593860626 + 0.001 * 7.102321624755859
Epoch 240, val loss: 0.7658030390739441
Epoch 250, training loss: 0.3861670196056366 = 0.3790704011917114 + 0.001 * 7.096619606018066
Epoch 250, val loss: 0.7479614019393921
Epoch 260, training loss: 0.3456535339355469 = 0.33855754137039185 + 0.001 * 7.0959906578063965
Epoch 260, val loss: 0.7346879243850708
Epoch 270, training loss: 0.30895596742630005 = 0.30186885595321655 + 0.001 * 7.087100028991699
Epoch 270, val loss: 0.7256113886833191
Epoch 280, training loss: 0.27575623989105225 = 0.26867157220840454 + 0.001 * 7.084657669067383
Epoch 280, val loss: 0.720413327217102
Epoch 290, training loss: 0.24583882093429565 = 0.2387578785419464 + 0.001 * 7.080939769744873
Epoch 290, val loss: 0.7187240123748779
Epoch 300, training loss: 0.21901999413967133 = 0.21194186806678772 + 0.001 * 7.078125953674316
Epoch 300, val loss: 0.7204740643501282
Epoch 310, training loss: 0.19522453844547272 = 0.18814633786678314 + 0.001 * 7.0781941413879395
Epoch 310, val loss: 0.7253501415252686
Epoch 320, training loss: 0.17429465055465698 = 0.16722118854522705 + 0.001 * 7.073466777801514
Epoch 320, val loss: 0.7329703569412231
Epoch 330, training loss: 0.15592502057552338 = 0.14885327219963074 + 0.001 * 7.071744441986084
Epoch 330, val loss: 0.7430234551429749
Epoch 340, training loss: 0.1398109346628189 = 0.13273942470550537 + 0.001 * 7.071512699127197
Epoch 340, val loss: 0.7551251649856567
Epoch 350, training loss: 0.12564817070960999 = 0.11857927590608597 + 0.001 * 7.068897724151611
Epoch 350, val loss: 0.7689517140388489
Epoch 360, training loss: 0.11319643259048462 = 0.10613071173429489 + 0.001 * 7.065722942352295
Epoch 360, val loss: 0.7839807271957397
Epoch 370, training loss: 0.1022024154663086 = 0.09514216333627701 + 0.001 * 7.0602545738220215
Epoch 370, val loss: 0.8000325560569763
Epoch 380, training loss: 0.0924542248249054 = 0.08539322763681412 + 0.001 * 7.060993671417236
Epoch 380, val loss: 0.8168002963066101
Epoch 390, training loss: 0.08375933021306992 = 0.07671086490154266 + 0.001 * 7.048467636108398
Epoch 390, val loss: 0.8341189622879028
Epoch 400, training loss: 0.07601095736026764 = 0.06896854937076569 + 0.001 * 7.04240608215332
Epoch 400, val loss: 0.8517152070999146
Epoch 410, training loss: 0.06907723844051361 = 0.06203777343034744 + 0.001 * 7.039463520050049
Epoch 410, val loss: 0.869526207447052
Epoch 420, training loss: 0.06286439299583435 = 0.055830854922533035 + 0.001 * 7.033541679382324
Epoch 420, val loss: 0.887475311756134
Epoch 430, training loss: 0.05729394406080246 = 0.050268083810806274 + 0.001 * 7.025860786437988
Epoch 430, val loss: 0.9054454565048218
Epoch 440, training loss: 0.05230645090341568 = 0.04528552293777466 + 0.001 * 7.020928382873535
Epoch 440, val loss: 0.9233892560005188
Epoch 450, training loss: 0.04786236584186554 = 0.04085296764969826 + 0.001 * 7.009396553039551
Epoch 450, val loss: 0.9411669969558716
Epoch 460, training loss: 0.04393719136714935 = 0.03693270683288574 + 0.001 * 7.004482746124268
Epoch 460, val loss: 0.9587093591690063
Epoch 470, training loss: 0.04046638309955597 = 0.03347300738096237 + 0.001 * 6.993374347686768
Epoch 470, val loss: 0.975916862487793
Epoch 480, training loss: 0.03739985451102257 = 0.030416229739785194 + 0.001 * 6.983625888824463
Epoch 480, val loss: 0.9928757548332214
Epoch 490, training loss: 0.034699540585279465 = 0.027723923325538635 + 0.001 * 6.975615501403809
Epoch 490, val loss: 1.0093793869018555
Epoch 500, training loss: 0.032315026968717575 = 0.025350525975227356 + 0.001 * 6.964501857757568
Epoch 500, val loss: 1.0254576206207275
Epoch 510, training loss: 0.030219141393899918 = 0.023251211270689964 + 0.001 * 6.967930793762207
Epoch 510, val loss: 1.0410549640655518
Epoch 520, training loss: 0.028346765786409378 = 0.02139008603990078 + 0.001 * 6.956678867340088
Epoch 520, val loss: 1.0560753345489502
Epoch 530, training loss: 0.026689356192946434 = 0.01973704621195793 + 0.001 * 6.9523091316223145
Epoch 530, val loss: 1.0707165002822876
Epoch 540, training loss: 0.02522842213511467 = 0.018264057114720345 + 0.001 * 6.964365005493164
Epoch 540, val loss: 1.0848416090011597
Epoch 550, training loss: 0.023896023631095886 = 0.01694626174867153 + 0.001 * 6.949760437011719
Epoch 550, val loss: 1.0985088348388672
Epoch 560, training loss: 0.022705649957060814 = 0.01576356403529644 + 0.001 * 6.942085266113281
Epoch 560, val loss: 1.111750602722168
Epoch 570, training loss: 0.021649552509188652 = 0.014699445106089115 + 0.001 * 6.950107097625732
Epoch 570, val loss: 1.1245379447937012
Epoch 580, training loss: 0.020687147974967957 = 0.013738706707954407 + 0.001 * 6.948441028594971
Epoch 580, val loss: 1.1369259357452393
Epoch 590, training loss: 0.019814468920230865 = 0.01286860927939415 + 0.001 * 6.945858478546143
Epoch 590, val loss: 1.1489782333374023
Epoch 600, training loss: 0.019013207405805588 = 0.012078936211764812 + 0.001 * 6.93427038192749
Epoch 600, val loss: 1.160607933998108
Epoch 610, training loss: 0.01828787848353386 = 0.011360232718288898 + 0.001 * 6.927645683288574
Epoch 610, val loss: 1.1718637943267822
Epoch 620, training loss: 0.01763620227575302 = 0.010704346001148224 + 0.001 * 6.931856632232666
Epoch 620, val loss: 1.1827905178070068
Epoch 630, training loss: 0.01703241840004921 = 0.010105009190738201 + 0.001 * 6.9274091720581055
Epoch 630, val loss: 1.1933659315109253
Epoch 640, training loss: 0.01648348569869995 = 0.009556270204484463 + 0.001 * 6.927215576171875
Epoch 640, val loss: 1.2036539316177368
Epoch 650, training loss: 0.01598161831498146 = 0.009052068926393986 + 0.001 * 6.929548740386963
Epoch 650, val loss: 1.213614821434021
Epoch 660, training loss: 0.015510284341871738 = 0.008588055148720741 + 0.001 * 6.922228813171387
Epoch 660, val loss: 1.2232850790023804
Epoch 670, training loss: 0.015084372833371162 = 0.008160279132425785 + 0.001 * 6.924092769622803
Epoch 670, val loss: 1.2326699495315552
Epoch 680, training loss: 0.014681483618915081 = 0.0077651445753872395 + 0.001 * 6.916338920593262
Epoch 680, val loss: 1.2418341636657715
Epoch 690, training loss: 0.014331085607409477 = 0.007399299647659063 + 0.001 * 6.931785583496094
Epoch 690, val loss: 1.2506974935531616
Epoch 700, training loss: 0.01396721787750721 = 0.007060079835355282 + 0.001 * 6.907137393951416
Epoch 700, val loss: 1.2593419551849365
Epoch 710, training loss: 0.013656176626682281 = 0.006745030637830496 + 0.001 * 6.911145210266113
Epoch 710, val loss: 1.267760157585144
Epoch 720, training loss: 0.013380957767367363 = 0.006451812107115984 + 0.001 * 6.929145336151123
Epoch 720, val loss: 1.2759422063827515
Epoch 730, training loss: 0.013087945058941841 = 0.006178378593176603 + 0.001 * 6.909566402435303
Epoch 730, val loss: 1.2838945388793945
Epoch 740, training loss: 0.012841355055570602 = 0.005921979900449514 + 0.001 * 6.919375419616699
Epoch 740, val loss: 1.291528344154358
Epoch 750, training loss: 0.012584353797137737 = 0.005683160852640867 + 0.001 * 6.901192665100098
Epoch 750, val loss: 1.2991291284561157
Epoch 760, training loss: 0.01234903559088707 = 0.005459604784846306 + 0.001 * 6.889430046081543
Epoch 760, val loss: 1.3065581321716309
Epoch 770, training loss: 0.012151140719652176 = 0.005249800160527229 + 0.001 * 6.901340484619141
Epoch 770, val loss: 1.3137545585632324
Epoch 780, training loss: 0.011949028819799423 = 0.005052962340414524 + 0.001 * 6.896066665649414
Epoch 780, val loss: 1.3208016157150269
Epoch 790, training loss: 0.011754348874092102 = 0.004868001211434603 + 0.001 * 6.886347770690918
Epoch 790, val loss: 1.3276476860046387
Epoch 800, training loss: 0.011577798053622246 = 0.004693937953561544 + 0.001 * 6.883859634399414
Epoch 800, val loss: 1.3343429565429688
Epoch 810, training loss: 0.011415781453251839 = 0.004529948811978102 + 0.001 * 6.885831832885742
Epoch 810, val loss: 1.3408839702606201
Epoch 820, training loss: 0.011255640536546707 = 0.0043752193450927734 + 0.001 * 6.880420684814453
Epoch 820, val loss: 1.3472423553466797
Epoch 830, training loss: 0.01111341267824173 = 0.004228921607136726 + 0.001 * 6.884490489959717
Epoch 830, val loss: 1.3534685373306274
Epoch 840, training loss: 0.010978877544403076 = 0.004090503323823214 + 0.001 * 6.888374328613281
Epoch 840, val loss: 1.3595199584960938
Epoch 850, training loss: 0.010844925418496132 = 0.0039595309644937515 + 0.001 * 6.8853936195373535
Epoch 850, val loss: 1.3654595613479614
Epoch 860, training loss: 0.010709099471569061 = 0.00383554445579648 + 0.001 * 6.873554229736328
Epoch 860, val loss: 1.3712561130523682
Epoch 870, training loss: 0.010600301437079906 = 0.00371803087182343 + 0.001 * 6.882270336151123
Epoch 870, val loss: 1.3769278526306152
Epoch 880, training loss: 0.010494732297956944 = 0.0036065864842385054 + 0.001 * 6.888145446777344
Epoch 880, val loss: 1.3824489116668701
Epoch 890, training loss: 0.01037728600203991 = 0.0035007798578590155 + 0.001 * 6.876505374908447
Epoch 890, val loss: 1.3878707885742188
Epoch 900, training loss: 0.010264103300869465 = 0.0034001905005425215 + 0.001 * 6.863912105560303
Epoch 900, val loss: 1.3931623697280884
Epoch 910, training loss: 0.010162465274333954 = 0.0033044866286218166 + 0.001 * 6.857978820800781
Epoch 910, val loss: 1.398353934288025
Epoch 920, training loss: 0.010071033611893654 = 0.003213467076420784 + 0.001 * 6.857566833496094
Epoch 920, val loss: 1.403441309928894
Epoch 930, training loss: 0.009994089603424072 = 0.0031268191523849964 + 0.001 * 6.867270469665527
Epoch 930, val loss: 1.4084136486053467
Epoch 940, training loss: 0.009903915226459503 = 0.0030441719572991133 + 0.001 * 6.859742641448975
Epoch 940, val loss: 1.4132699966430664
Epoch 950, training loss: 0.0098351389169693 = 0.0029652626253664494 + 0.001 * 6.869876384735107
Epoch 950, val loss: 1.4180164337158203
Epoch 960, training loss: 0.009747522883117199 = 0.002889780793339014 + 0.001 * 6.857741832733154
Epoch 960, val loss: 1.4226973056793213
Epoch 970, training loss: 0.009664961136877537 = 0.002817522268742323 + 0.001 * 6.847438335418701
Epoch 970, val loss: 1.4272665977478027
Epoch 980, training loss: 0.009590293280780315 = 0.0027482998557388783 + 0.001 * 6.84199333190918
Epoch 980, val loss: 1.4317597150802612
Epoch 990, training loss: 0.00954326894134283 = 0.00268182042054832 + 0.001 * 6.861447811126709
Epoch 990, val loss: 1.4361947774887085
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.2583
Flip ASR: 0.2489/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9344264268875122 = 1.926052451133728 + 0.001 * 8.373918533325195
Epoch 0, val loss: 1.9275015592575073
Epoch 10, training loss: 1.9252933263778687 = 1.916919469833374 + 0.001 * 8.373881340026855
Epoch 10, val loss: 1.9179719686508179
Epoch 20, training loss: 1.9144922494888306 = 1.9061185121536255 + 0.001 * 8.373716354370117
Epoch 20, val loss: 1.9064692258834839
Epoch 30, training loss: 1.8997035026550293 = 1.8913301229476929 + 0.001 * 8.373372077941895
Epoch 30, val loss: 1.8908264636993408
Epoch 40, training loss: 1.8783848285675049 = 1.8700121641159058 + 0.001 * 8.372624397277832
Epoch 40, val loss: 1.8686267137527466
Epoch 50, training loss: 1.8484539985656738 = 1.840083360671997 + 0.001 * 8.370676040649414
Epoch 50, val loss: 1.8387904167175293
Epoch 60, training loss: 1.811875820159912 = 1.803511619567871 + 0.001 * 8.364148139953613
Epoch 60, val loss: 1.8053362369537354
Epoch 70, training loss: 1.7730324268341064 = 1.764699101448059 + 0.001 * 8.333333969116211
Epoch 70, val loss: 1.7733205556869507
Epoch 80, training loss: 1.7246066331863403 = 1.7165194749832153 + 0.001 * 8.087106704711914
Epoch 80, val loss: 1.733465313911438
Epoch 90, training loss: 1.6571346521377563 = 1.6493607759475708 + 0.001 * 7.773897171020508
Epoch 90, val loss: 1.6756298542022705
Epoch 100, training loss: 1.5665380954742432 = 1.5590273141860962 + 0.001 * 7.510791301727295
Epoch 100, val loss: 1.598388433456421
Epoch 110, training loss: 1.4557454586029053 = 1.4483846426010132 + 0.001 * 7.360805988311768
Epoch 110, val loss: 1.507796287536621
Epoch 120, training loss: 1.3344547748565674 = 1.3271692991256714 + 0.001 * 7.28549337387085
Epoch 120, val loss: 1.408137559890747
Epoch 130, training loss: 1.2124032974243164 = 1.2052065134048462 + 0.001 * 7.196725368499756
Epoch 130, val loss: 1.3094221353530884
Epoch 140, training loss: 1.0968371629714966 = 1.0896819829940796 + 0.001 * 7.155139446258545
Epoch 140, val loss: 1.2160847187042236
Epoch 150, training loss: 0.9914966821670532 = 0.9843440055847168 + 0.001 * 7.152659893035889
Epoch 150, val loss: 1.13266921043396
Epoch 160, training loss: 0.8970257043838501 = 0.889879584312439 + 0.001 * 7.1461076736450195
Epoch 160, val loss: 1.0592942237854004
Epoch 170, training loss: 0.8124749660491943 = 0.805339515209198 + 0.001 * 7.135463237762451
Epoch 170, val loss: 0.9950248599052429
Epoch 180, training loss: 0.7368321418762207 = 0.7297110557556152 + 0.001 * 7.121074199676514
Epoch 180, val loss: 0.9390610456466675
Epoch 190, training loss: 0.6693398952484131 = 0.6622401475906372 + 0.001 * 7.099761486053467
Epoch 190, val loss: 0.8909872770309448
Epoch 200, training loss: 0.6087601780891418 = 0.6016831994056702 + 0.001 * 7.076953887939453
Epoch 200, val loss: 0.8500985503196716
Epoch 210, training loss: 0.5533157587051392 = 0.5462497472763062 + 0.001 * 7.066012859344482
Epoch 210, val loss: 0.8152064085006714
Epoch 220, training loss: 0.5012202262878418 = 0.494157999753952 + 0.001 * 7.062249183654785
Epoch 220, val loss: 0.7848430871963501
Epoch 230, training loss: 0.4515101909637451 = 0.4444482624530792 + 0.001 * 7.0619401931762695
Epoch 230, val loss: 0.7585238218307495
Epoch 240, training loss: 0.4035712480545044 = 0.3965092897415161 + 0.001 * 7.061953544616699
Epoch 240, val loss: 0.7359099388122559
Epoch 250, training loss: 0.35726597905158997 = 0.3502040505409241 + 0.001 * 7.061936855316162
Epoch 250, val loss: 0.7163733839988708
Epoch 260, training loss: 0.31311434507369995 = 0.30605220794677734 + 0.001 * 7.0621442794799805
Epoch 260, val loss: 0.6996607184410095
Epoch 270, training loss: 0.27199506759643555 = 0.26493242383003235 + 0.001 * 7.062653064727783
Epoch 270, val loss: 0.6859230995178223
Epoch 280, training loss: 0.2348012626171112 = 0.22773776948451996 + 0.001 * 7.063498497009277
Epoch 280, val loss: 0.6755637526512146
Epoch 290, training loss: 0.20214006304740906 = 0.19507504999637604 + 0.001 * 7.065006256103516
Epoch 290, val loss: 0.6691722273826599
Epoch 300, training loss: 0.1740914285182953 = 0.16702529788017273 + 0.001 * 7.066132068634033
Epoch 300, val loss: 0.6666775941848755
Epoch 310, training loss: 0.15040916204452515 = 0.14334185421466827 + 0.001 * 7.067302227020264
Epoch 310, val loss: 0.6681626439094543
Epoch 320, training loss: 0.13059161603450775 = 0.1235232800245285 + 0.001 * 7.068331241607666
Epoch 320, val loss: 0.6731592416763306
Epoch 330, training loss: 0.11407814174890518 = 0.10700872540473938 + 0.001 * 7.069417476654053
Epoch 330, val loss: 0.6810941696166992
Epoch 340, training loss: 0.10031302273273468 = 0.09324224293231964 + 0.001 * 7.070779800415039
Epoch 340, val loss: 0.691320538520813
Epoch 350, training loss: 0.08877965807914734 = 0.08170664310455322 + 0.001 * 7.0730180740356445
Epoch 350, val loss: 0.7031357884407043
Epoch 360, training loss: 0.07904644310474396 = 0.0719730481505394 + 0.001 * 7.073394775390625
Epoch 360, val loss: 0.7161033749580383
Epoch 370, training loss: 0.07079817354679108 = 0.06372502446174622 + 0.001 * 7.073148727416992
Epoch 370, val loss: 0.7298421859741211
Epoch 380, training loss: 0.06378799676895142 = 0.056714966893196106 + 0.001 * 7.073031425476074
Epoch 380, val loss: 0.7441165447235107
Epoch 390, training loss: 0.05780006945133209 = 0.05072711035609245 + 0.001 * 7.072960376739502
Epoch 390, val loss: 0.7586032152175903
Epoch 400, training loss: 0.052661310881376266 = 0.045587148517370224 + 0.001 * 7.074163436889648
Epoch 400, val loss: 0.7730829119682312
Epoch 410, training loss: 0.048223234713077545 = 0.04115094989538193 + 0.001 * 7.072282791137695
Epoch 410, val loss: 0.7874129414558411
Epoch 420, training loss: 0.04437030851840973 = 0.03729905188083649 + 0.001 * 7.071254253387451
Epoch 420, val loss: 0.8015028238296509
Epoch 430, training loss: 0.04100655019283295 = 0.03393512964248657 + 0.001 * 7.071421146392822
Epoch 430, val loss: 0.8154177069664001
Epoch 440, training loss: 0.038057416677474976 = 0.030988581478595734 + 0.001 * 7.06883430480957
Epoch 440, val loss: 0.8290280699729919
Epoch 450, training loss: 0.03547105938196182 = 0.02840256690979004 + 0.001 * 7.068493843078613
Epoch 450, val loss: 0.8423948884010315
Epoch 460, training loss: 0.03318188339471817 = 0.026117471978068352 + 0.001 * 7.0644121170043945
Epoch 460, val loss: 0.8553273677825928
Epoch 470, training loss: 0.03115304559469223 = 0.024090219289064407 + 0.001 * 7.062827110290527
Epoch 470, val loss: 0.8679558038711548
Epoch 480, training loss: 0.02934372052550316 = 0.022284971550107002 + 0.001 * 7.058749675750732
Epoch 480, val loss: 0.8802440166473389
Epoch 490, training loss: 0.027720749378204346 = 0.020671438425779343 + 0.001 * 7.04931116104126
Epoch 490, val loss: 0.8922195434570312
Epoch 500, training loss: 0.02628716640174389 = 0.019224563613533974 + 0.001 * 7.0626020431518555
Epoch 500, val loss: 0.9038612246513367
Epoch 510, training loss: 0.024963662028312683 = 0.01792186126112938 + 0.001 * 7.041800498962402
Epoch 510, val loss: 0.9151743650436401
Epoch 520, training loss: 0.023786496371030807 = 0.01674496755003929 + 0.001 * 7.041528224945068
Epoch 520, val loss: 0.9262325167655945
Epoch 530, training loss: 0.022739466279745102 = 0.01567937433719635 + 0.001 * 7.060091495513916
Epoch 530, val loss: 0.9370271563529968
Epoch 540, training loss: 0.02174081839621067 = 0.014710410498082638 + 0.001 * 7.030407905578613
Epoch 540, val loss: 0.9475184679031372
Epoch 550, training loss: 0.02085171267390251 = 0.013828044757246971 + 0.001 * 7.023667812347412
Epoch 550, val loss: 0.957791268825531
Epoch 560, training loss: 0.02007443830370903 = 0.01302351988852024 + 0.001 * 7.050917625427246
Epoch 560, val loss: 0.9678122997283936
Epoch 570, training loss: 0.01928529143333435 = 0.012288562022149563 + 0.001 * 6.996728897094727
Epoch 570, val loss: 0.9775649309158325
Epoch 580, training loss: 0.018614336848258972 = 0.011615375056862831 + 0.001 * 6.998960971832275
Epoch 580, val loss: 0.9870948195457458
Epoch 590, training loss: 0.01798902451992035 = 0.010997354984283447 + 0.001 * 6.99167013168335
Epoch 590, val loss: 0.9963264465332031
Epoch 600, training loss: 0.01743532344698906 = 0.010429340414702892 + 0.001 * 7.005983829498291
Epoch 600, val loss: 1.0054094791412354
Epoch 610, training loss: 0.016870420426130295 = 0.009906748309731483 + 0.001 * 6.9636712074279785
Epoch 610, val loss: 1.0142439603805542
Epoch 620, training loss: 0.016381146386265755 = 0.00942388828843832 + 0.001 * 6.957257270812988
Epoch 620, val loss: 1.0228322744369507
Epoch 630, training loss: 0.015939079225063324 = 0.008976859971880913 + 0.001 * 6.962217807769775
Epoch 630, val loss: 1.0312086343765259
Epoch 640, training loss: 0.015530155971646309 = 0.00856219045817852 + 0.001 * 6.967965602874756
Epoch 640, val loss: 1.0393884181976318
Epoch 650, training loss: 0.015136461704969406 = 0.008176908828318119 + 0.001 * 6.959552764892578
Epoch 650, val loss: 1.0473437309265137
Epoch 660, training loss: 0.014759772457182407 = 0.007818339392542839 + 0.001 * 6.941432952880859
Epoch 660, val loss: 1.0551599264144897
Epoch 670, training loss: 0.0144219771027565 = 0.007484107743948698 + 0.001 * 6.937868595123291
Epoch 670, val loss: 1.0628077983856201
Epoch 680, training loss: 0.014109072275459766 = 0.007172084879130125 + 0.001 * 6.936986923217773
Epoch 680, val loss: 1.0702428817749023
Epoch 690, training loss: 0.013823888264596462 = 0.006880469154566526 + 0.001 * 6.943418979644775
Epoch 690, val loss: 1.0775396823883057
Epoch 700, training loss: 0.013567354530096054 = 0.006607379764318466 + 0.001 * 6.9599738121032715
Epoch 700, val loss: 1.0846573114395142
Epoch 710, training loss: 0.013299744576215744 = 0.006351321469992399 + 0.001 * 6.948422908782959
Epoch 710, val loss: 1.0916345119476318
Epoch 720, training loss: 0.013057014904916286 = 0.006110903806984425 + 0.001 * 6.946110725402832
Epoch 720, val loss: 1.0984423160552979
Epoch 730, training loss: 0.012807346880435944 = 0.005884920712560415 + 0.001 * 6.922425746917725
Epoch 730, val loss: 1.105122685432434
Epoch 740, training loss: 0.012614969164133072 = 0.005672220606356859 + 0.001 * 6.942748069763184
Epoch 740, val loss: 1.1116591691970825
Epoch 750, training loss: 0.012420819140970707 = 0.005471804644912481 + 0.001 * 6.949014186859131
Epoch 750, val loss: 1.1180198192596436
Epoch 760, training loss: 0.012221398763358593 = 0.005282814614474773 + 0.001 * 6.938583850860596
Epoch 760, val loss: 1.124318242073059
Epoch 770, training loss: 0.012040549889206886 = 0.00510434340685606 + 0.001 * 6.936206340789795
Epoch 770, val loss: 1.1304479837417603
Epoch 780, training loss: 0.01186852902173996 = 0.0049355956725776196 + 0.001 * 6.932932376861572
Epoch 780, val loss: 1.1364420652389526
Epoch 790, training loss: 0.011689025908708572 = 0.004775910638272762 + 0.001 * 6.91311502456665
Epoch 790, val loss: 1.1423474550247192
Epoch 800, training loss: 0.01157140452414751 = 0.004624670837074518 + 0.001 * 6.946733474731445
Epoch 800, val loss: 1.1481385231018066
Epoch 810, training loss: 0.011403469368815422 = 0.004481257405132055 + 0.001 * 6.922211170196533
Epoch 810, val loss: 1.1537975072860718
Epoch 820, training loss: 0.011289343237876892 = 0.004345124587416649 + 0.001 * 6.944217681884766
Epoch 820, val loss: 1.1593562364578247
Epoch 830, training loss: 0.011122740805149078 = 0.004215761553496122 + 0.001 * 6.906979084014893
Epoch 830, val loss: 1.1647882461547852
Epoch 840, training loss: 0.010981475934386253 = 0.004092805087566376 + 0.001 * 6.888670921325684
Epoch 840, val loss: 1.170135736465454
Epoch 850, training loss: 0.010853825137019157 = 0.003975747153162956 + 0.001 * 6.878077983856201
Epoch 850, val loss: 1.1753946542739868
Epoch 860, training loss: 0.01074307318776846 = 0.003864320693537593 + 0.001 * 6.878751754760742
Epoch 860, val loss: 1.180543303489685
Epoch 870, training loss: 0.010676286183297634 = 0.0037582903169095516 + 0.001 * 6.917995452880859
Epoch 870, val loss: 1.1855894327163696
Epoch 880, training loss: 0.010544372722506523 = 0.003656964050605893 + 0.001 * 6.887408256530762
Epoch 880, val loss: 1.190539002418518
Epoch 890, training loss: 0.01043281052261591 = 0.003560298588126898 + 0.001 * 6.872511386871338
Epoch 890, val loss: 1.195405125617981
Epoch 900, training loss: 0.01034048292785883 = 0.0034679945092648268 + 0.001 * 6.872488021850586
Epoch 900, val loss: 1.2001641988754272
Epoch 910, training loss: 0.010262473486363888 = 0.0033797507639974356 + 0.001 * 6.8827223777771
Epoch 910, val loss: 1.2048920392990112
Epoch 920, training loss: 0.010148423723876476 = 0.003295414848253131 + 0.001 * 6.85300874710083
Epoch 920, val loss: 1.2094401121139526
Epoch 930, training loss: 0.010119546204805374 = 0.00321459979750216 + 0.001 * 6.9049458503723145
Epoch 930, val loss: 1.214019536972046
Epoch 940, training loss: 0.010016401298344135 = 0.0031372879166156054 + 0.001 * 6.87911319732666
Epoch 940, val loss: 1.2184524536132812
Epoch 950, training loss: 0.009942140430212021 = 0.003063127864152193 + 0.001 * 6.879012584686279
Epoch 950, val loss: 1.2228158712387085
Epoch 960, training loss: 0.009853949770331383 = 0.0029920171946287155 + 0.001 * 6.861931800842285
Epoch 960, val loss: 1.227168083190918
Epoch 970, training loss: 0.009810047224164009 = 0.0029238515999168158 + 0.001 * 6.886195659637451
Epoch 970, val loss: 1.231393814086914
Epoch 980, training loss: 0.009703951887786388 = 0.0028582829982042313 + 0.001 * 6.845668792724609
Epoch 980, val loss: 1.2355724573135376
Epoch 990, training loss: 0.00966775231063366 = 0.0027952552773058414 + 0.001 * 6.872496604919434
Epoch 990, val loss: 1.2397035360336304
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.6679
Flip ASR: 0.6178/225 nodes
The final ASR:0.46986, 0.16749, Accuracy:0.82222, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9480])
updated graph: torch.Size([2, 10526])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00174, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9535242319107056 = 1.945150375366211 + 0.001 * 8.373833656311035
Epoch 0, val loss: 1.9556330442428589
Epoch 10, training loss: 1.9434788227081299 = 1.9351050853729248 + 0.001 * 8.37375259399414
Epoch 10, val loss: 1.9455735683441162
Epoch 20, training loss: 1.9308626651763916 = 1.9224891662597656 + 0.001 * 8.373501777648926
Epoch 20, val loss: 1.9329336881637573
Epoch 30, training loss: 1.912842035293579 = 1.9044690132141113 + 0.001 * 8.372995376586914
Epoch 30, val loss: 1.914952039718628
Epoch 40, training loss: 1.8864707946777344 = 1.878098964691162 + 0.001 * 8.371820449829102
Epoch 40, val loss: 1.8889200687408447
Epoch 50, training loss: 1.8498330116271973 = 1.8414647579193115 + 0.001 * 8.368212699890137
Epoch 50, val loss: 1.8538799285888672
Epoch 60, training loss: 1.8068381547927856 = 1.7984851598739624 + 0.001 * 8.353033065795898
Epoch 60, val loss: 1.8153119087219238
Epoch 70, training loss: 1.7652631998062134 = 1.756988763809204 + 0.001 * 8.274447441101074
Epoch 70, val loss: 1.7797119617462158
Epoch 80, training loss: 1.7136592864990234 = 1.7057173252105713 + 0.001 * 7.94193696975708
Epoch 80, val loss: 1.73382568359375
Epoch 90, training loss: 1.6422322988510132 = 1.6345643997192383 + 0.001 * 7.6678466796875
Epoch 90, val loss: 1.6723026037216187
Epoch 100, training loss: 1.5487205982208252 = 1.541242241859436 + 0.001 * 7.478316783905029
Epoch 100, val loss: 1.5944949388504028
Epoch 110, training loss: 1.4372694492340088 = 1.4299328327178955 + 0.001 * 7.336668968200684
Epoch 110, val loss: 1.502917766571045
Epoch 120, training loss: 1.319943904876709 = 1.3126364946365356 + 0.001 * 7.307431697845459
Epoch 120, val loss: 1.4068820476531982
Epoch 130, training loss: 1.2055302858352661 = 1.1982715129852295 + 0.001 * 7.258791446685791
Epoch 130, val loss: 1.3141529560089111
Epoch 140, training loss: 1.100300669670105 = 1.0930917263031006 + 0.001 * 7.208914756774902
Epoch 140, val loss: 1.230377435684204
Epoch 150, training loss: 1.0058579444885254 = 0.9987066388130188 + 0.001 * 7.151320457458496
Epoch 150, val loss: 1.1561003923416138
Epoch 160, training loss: 0.9205938577651978 = 0.9134739637374878 + 0.001 * 7.119912624359131
Epoch 160, val loss: 1.0892671346664429
Epoch 170, training loss: 0.842685341835022 = 0.8355759978294373 + 0.001 * 7.109340667724609
Epoch 170, val loss: 1.028975486755371
Epoch 180, training loss: 0.772087812423706 = 0.7649924755096436 + 0.001 * 7.09531831741333
Epoch 180, val loss: 0.9758317470550537
Epoch 190, training loss: 0.7091840505599976 = 0.7021042704582214 + 0.001 * 7.079767227172852
Epoch 190, val loss: 0.9316187500953674
Epoch 200, training loss: 0.6531447172164917 = 0.6460872888565063 + 0.001 * 7.057411193847656
Epoch 200, val loss: 0.8963426947593689
Epoch 210, training loss: 0.6020838022232056 = 0.5950635671615601 + 0.001 * 7.020252704620361
Epoch 210, val loss: 0.8685635328292847
Epoch 220, training loss: 0.5541231632232666 = 0.547147274017334 + 0.001 * 6.975891590118408
Epoch 220, val loss: 0.8464154601097107
Epoch 230, training loss: 0.5078662037849426 = 0.5009278655052185 + 0.001 * 6.938323497772217
Epoch 230, val loss: 0.8282045125961304
Epoch 240, training loss: 0.46242547035217285 = 0.455506294965744 + 0.001 * 6.919169902801514
Epoch 240, val loss: 0.8135637044906616
Epoch 250, training loss: 0.4174991548061371 = 0.41058963537216187 + 0.001 * 6.909525394439697
Epoch 250, val loss: 0.802711546421051
Epoch 260, training loss: 0.3735620081424713 = 0.3666574954986572 + 0.001 * 6.904506206512451
Epoch 260, val loss: 0.7964211702346802
Epoch 270, training loss: 0.331656813621521 = 0.32475727796554565 + 0.001 * 6.899549961090088
Epoch 270, val loss: 0.79537433385849
Epoch 280, training loss: 0.293099045753479 = 0.2862054109573364 + 0.001 * 6.893635272979736
Epoch 280, val loss: 0.7997366786003113
Epoch 290, training loss: 0.2587582767009735 = 0.2518700659275055 + 0.001 * 6.888212203979492
Epoch 290, val loss: 0.808879554271698
Epoch 300, training loss: 0.2288208305835724 = 0.22193501889705658 + 0.001 * 6.8858184814453125
Epoch 300, val loss: 0.8220410346984863
Epoch 310, training loss: 0.20283858478069305 = 0.19596415758132935 + 0.001 * 6.8744282722473145
Epoch 310, val loss: 0.8384042978286743
Epoch 320, training loss: 0.18022170662879944 = 0.1733543574810028 + 0.001 * 6.867355823516846
Epoch 320, val loss: 0.8573564887046814
Epoch 330, training loss: 0.16042128205299377 = 0.15355339646339417 + 0.001 * 6.867878437042236
Epoch 330, val loss: 0.8783334493637085
Epoch 340, training loss: 0.1429981142282486 = 0.13614383339881897 + 0.001 * 6.854287147521973
Epoch 340, val loss: 0.9007025361061096
Epoch 350, training loss: 0.12765085697174072 = 0.1208038181066513 + 0.001 * 6.8470377922058105
Epoch 350, val loss: 0.9240278601646423
Epoch 360, training loss: 0.11411768943071365 = 0.10727295279502869 + 0.001 * 6.844736576080322
Epoch 360, val loss: 0.9480299949645996
Epoch 370, training loss: 0.1021711528301239 = 0.09533684700727463 + 0.001 * 6.834306716918945
Epoch 370, val loss: 0.9724764227867126
Epoch 380, training loss: 0.09163904190063477 = 0.08480850607156754 + 0.001 * 6.830534934997559
Epoch 380, val loss: 0.9971691966056824
Epoch 390, training loss: 0.08235659450292587 = 0.07552619278430939 + 0.001 * 6.8304009437561035
Epoch 390, val loss: 1.0219508409500122
Epoch 400, training loss: 0.07418535649776459 = 0.06735321134328842 + 0.001 * 6.832146644592285
Epoch 400, val loss: 1.046533465385437
Epoch 410, training loss: 0.06699667870998383 = 0.060167096555233 + 0.001 * 6.829578399658203
Epoch 410, val loss: 1.0706545114517212
Epoch 420, training loss: 0.06068206951022148 = 0.05385525897145271 + 0.001 * 6.826809883117676
Epoch 420, val loss: 1.0942491292953491
Epoch 430, training loss: 0.055140361189842224 = 0.04831686243414879 + 0.001 * 6.823496341705322
Epoch 430, val loss: 1.117230772972107
Epoch 440, training loss: 0.05028822273015976 = 0.0434587299823761 + 0.001 * 6.829493045806885
Epoch 440, val loss: 1.139486312866211
Epoch 450, training loss: 0.046020884066820145 = 0.03919573500752449 + 0.001 * 6.825148582458496
Epoch 450, val loss: 1.1610136032104492
Epoch 460, training loss: 0.042273350059986115 = 0.03545273095369339 + 0.001 * 6.820617198944092
Epoch 460, val loss: 1.1817967891693115
Epoch 470, training loss: 0.038982126861810684 = 0.032162465155124664 + 0.001 * 6.819660663604736
Epoch 470, val loss: 1.2019997835159302
Epoch 480, training loss: 0.03608665615320206 = 0.029265526682138443 + 0.001 * 6.821128845214844
Epoch 480, val loss: 1.2215584516525269
Epoch 490, training loss: 0.03352823480963707 = 0.02671094238758087 + 0.001 * 6.8172926902771
Epoch 490, val loss: 1.2403730154037476
Epoch 500, training loss: 0.03127610683441162 = 0.024452123790979385 + 0.001 * 6.823984622955322
Epoch 500, val loss: 1.2586488723754883
Epoch 510, training loss: 0.029269857332110405 = 0.02245018258690834 + 0.001 * 6.819674015045166
Epoch 510, val loss: 1.2762634754180908
Epoch 520, training loss: 0.02748631127178669 = 0.02067173458635807 + 0.001 * 6.814575672149658
Epoch 520, val loss: 1.2932792901992798
Epoch 530, training loss: 0.025902297347784042 = 0.019087446853518486 + 0.001 * 6.814850807189941
Epoch 530, val loss: 1.309787392616272
Epoch 540, training loss: 0.024485360831022263 = 0.01767241209745407 + 0.001 * 6.812947750091553
Epoch 540, val loss: 1.3257502317428589
Epoch 550, training loss: 0.02321760542690754 = 0.016405239701271057 + 0.001 * 6.8123650550842285
Epoch 550, val loss: 1.3411900997161865
Epoch 560, training loss: 0.02207881025969982 = 0.015267545357346535 + 0.001 * 6.811264514923096
Epoch 560, val loss: 1.356192708015442
Epoch 570, training loss: 0.02105102874338627 = 0.014243247918784618 + 0.001 * 6.807780742645264
Epoch 570, val loss: 1.3707332611083984
Epoch 580, training loss: 0.02012583799660206 = 0.013318635523319244 + 0.001 * 6.807202339172363
Epoch 580, val loss: 1.384769082069397
Epoch 590, training loss: 0.0192872267216444 = 0.01248173788189888 + 0.001 * 6.805488109588623
Epoch 590, val loss: 1.3983957767486572
Epoch 600, training loss: 0.01853705383837223 = 0.011722280643880367 + 0.001 * 6.814773082733154
Epoch 600, val loss: 1.4116315841674805
Epoch 610, training loss: 0.017836621031165123 = 0.0110316202044487 + 0.001 * 6.8050007820129395
Epoch 610, val loss: 1.4244887828826904
Epoch 620, training loss: 0.01720300130546093 = 0.010401722975075245 + 0.001 * 6.801278591156006
Epoch 620, val loss: 1.4370050430297852
Epoch 630, training loss: 0.016643352806568146 = 0.009825795888900757 + 0.001 * 6.817556858062744
Epoch 630, val loss: 1.4491245746612549
Epoch 640, training loss: 0.01609802059829235 = 0.009297972545027733 + 0.001 * 6.800047397613525
Epoch 640, val loss: 1.4609150886535645
Epoch 650, training loss: 0.015611398965120316 = 0.008813071064651012 + 0.001 * 6.7983269691467285
Epoch 650, val loss: 1.4724162817001343
Epoch 660, training loss: 0.015162505209445953 = 0.008366012014448643 + 0.001 * 6.796492576599121
Epoch 660, val loss: 1.483627200126648
Epoch 670, training loss: 0.01475266832858324 = 0.007951374165713787 + 0.001 * 6.801293849945068
Epoch 670, val loss: 1.4945473670959473
Epoch 680, training loss: 0.014366399496793747 = 0.007564527913928032 + 0.001 * 6.8018717765808105
Epoch 680, val loss: 1.5052423477172852
Epoch 690, training loss: 0.013996537774801254 = 0.007201889995485544 + 0.001 * 6.794647693634033
Epoch 690, val loss: 1.51565682888031
Epoch 700, training loss: 0.013650145381689072 = 0.006861203815788031 + 0.001 * 6.788940906524658
Epoch 700, val loss: 1.5259077548980713
Epoch 710, training loss: 0.013331180438399315 = 0.0065412092953920364 + 0.001 * 6.789970874786377
Epoch 710, val loss: 1.5358917713165283
Epoch 720, training loss: 0.013047251850366592 = 0.006240970455110073 + 0.001 * 6.806280612945557
Epoch 720, val loss: 1.5456469058990479
Epoch 730, training loss: 0.012749852612614632 = 0.005959589499980211 + 0.001 * 6.790262699127197
Epoch 730, val loss: 1.555167555809021
Epoch 740, training loss: 0.012483274564146996 = 0.005695785861462355 + 0.001 * 6.7874884605407715
Epoch 740, val loss: 1.5644280910491943
Epoch 750, training loss: 0.012240098789334297 = 0.00544874370098114 + 0.001 * 6.791354179382324
Epoch 750, val loss: 1.5736018419265747
Epoch 760, training loss: 0.012008280493319035 = 0.005217473953962326 + 0.001 * 6.790806293487549
Epoch 760, val loss: 1.5824462175369263
Epoch 770, training loss: 0.011786205694079399 = 0.0050008841790258884 + 0.001 * 6.7853217124938965
Epoch 770, val loss: 1.5911067724227905
Epoch 780, training loss: 0.011581990867853165 = 0.004797944333404303 + 0.001 * 6.784046649932861
Epoch 780, val loss: 1.5995217561721802
Epoch 790, training loss: 0.011392245069146156 = 0.004607704933732748 + 0.001 * 6.784540176391602
Epoch 790, val loss: 1.6077561378479004
Epoch 800, training loss: 0.011220354586839676 = 0.004429250489920378 + 0.001 * 6.791103363037109
Epoch 800, val loss: 1.615752100944519
Epoch 810, training loss: 0.011043024249374866 = 0.0042617302387952805 + 0.001 * 6.781293869018555
Epoch 810, val loss: 1.6236039400100708
Epoch 820, training loss: 0.01089022122323513 = 0.00410430459305644 + 0.001 * 6.785916328430176
Epoch 820, val loss: 1.631266474723816
Epoch 830, training loss: 0.010737531818449497 = 0.003956279251724482 + 0.001 * 6.781252384185791
Epoch 830, val loss: 1.6387501955032349
Epoch 840, training loss: 0.010595749132335186 = 0.0038169713225215673 + 0.001 * 6.778777599334717
Epoch 840, val loss: 1.646022915840149
Epoch 850, training loss: 0.01046951487660408 = 0.003685740288347006 + 0.001 * 6.783774375915527
Epoch 850, val loss: 1.6532002687454224
Epoch 860, training loss: 0.010336369276046753 = 0.0035619919653981924 + 0.001 * 6.774377346038818
Epoch 860, val loss: 1.6602234840393066
Epoch 870, training loss: 0.010221311822533607 = 0.0034451892133802176 + 0.001 * 6.776122093200684
Epoch 870, val loss: 1.6670780181884766
Epoch 880, training loss: 0.010096784681081772 = 0.0033347855787724257 + 0.001 * 6.761998653411865
Epoch 880, val loss: 1.673784613609314
Epoch 890, training loss: 0.009993799962103367 = 0.0032303647603839636 + 0.001 * 6.763434886932373
Epoch 890, val loss: 1.6803134679794312
Epoch 900, training loss: 0.009896719828248024 = 0.0031314906664192677 + 0.001 * 6.765228748321533
Epoch 900, val loss: 1.6867517232894897
Epoch 910, training loss: 0.009808093309402466 = 0.003037773771211505 + 0.001 * 6.770318984985352
Epoch 910, val loss: 1.693050742149353
Epoch 920, training loss: 0.009701341390609741 = 0.0029488764703273773 + 0.001 * 6.75246524810791
Epoch 920, val loss: 1.6992104053497314
Epoch 930, training loss: 0.009627608582377434 = 0.002864476991817355 + 0.001 * 6.763131618499756
Epoch 930, val loss: 1.7052642107009888
Epoch 940, training loss: 0.009539750404655933 = 0.002784299897029996 + 0.001 * 6.755450248718262
Epoch 940, val loss: 1.7111726999282837
Epoch 950, training loss: 0.009457350708544254 = 0.0027080464642494917 + 0.001 * 6.749304294586182
Epoch 950, val loss: 1.7169609069824219
Epoch 960, training loss: 0.009403174743056297 = 0.00263544381596148 + 0.001 * 6.767730236053467
Epoch 960, val loss: 1.7226518392562866
Epoch 970, training loss: 0.00931619107723236 = 0.002566279610618949 + 0.001 * 6.749910831451416
Epoch 970, val loss: 1.728196382522583
Epoch 980, training loss: 0.009249145165085793 = 0.0025003396440297365 + 0.001 * 6.748805522918701
Epoch 980, val loss: 1.7336783409118652
Epoch 990, training loss: 0.009188197553157806 = 0.002437405753880739 + 0.001 * 6.750791072845459
Epoch 990, val loss: 1.7390285730361938
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7667
Overall ASR: 0.5203
Flip ASR: 0.4400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9844697713851929 = 1.9760959148406982 + 0.001 * 8.373825073242188
Epoch 0, val loss: 1.9764162302017212
Epoch 10, training loss: 1.9734904766082764 = 1.9651167392730713 + 0.001 * 8.373747825622559
Epoch 10, val loss: 1.9649335145950317
Epoch 20, training loss: 1.9600216150283813 = 1.9516481161117554 + 0.001 * 8.373509407043457
Epoch 20, val loss: 1.9507204294204712
Epoch 30, training loss: 1.9411753416061401 = 1.9328023195266724 + 0.001 * 8.373051643371582
Epoch 30, val loss: 1.930967926979065
Epoch 40, training loss: 1.9131717681884766 = 1.9047996997833252 + 0.001 * 8.372076988220215
Epoch 40, val loss: 1.9021830558776855
Epoch 50, training loss: 1.8725968599319458 = 1.8642271757125854 + 0.001 * 8.369707107543945
Epoch 50, val loss: 1.8621362447738647
Epoch 60, training loss: 1.8219186067581177 = 1.8135563135147095 + 0.001 * 8.362248420715332
Epoch 60, val loss: 1.8154417276382446
Epoch 70, training loss: 1.7743287086486816 = 1.7660043239593506 + 0.001 * 8.324357032775879
Epoch 70, val loss: 1.7734054327011108
Epoch 80, training loss: 1.728523850440979 = 1.720436692237854 + 0.001 * 8.087138175964355
Epoch 80, val loss: 1.729560375213623
Epoch 90, training loss: 1.6679151058197021 = 1.6601340770721436 + 0.001 * 7.7809882164001465
Epoch 90, val loss: 1.6756646633148193
Epoch 100, training loss: 1.5882103443145752 = 1.580681324005127 + 0.001 * 7.529036998748779
Epoch 100, val loss: 1.609963297843933
Epoch 110, training loss: 1.4912384748458862 = 1.4839680194854736 + 0.001 * 7.270434379577637
Epoch 110, val loss: 1.5306668281555176
Epoch 120, training loss: 1.3872779607772827 = 1.3801665306091309 + 0.001 * 7.111377239227295
Epoch 120, val loss: 1.4449870586395264
Epoch 130, training loss: 1.2858093976974487 = 1.2787584066390991 + 0.001 * 7.050933361053467
Epoch 130, val loss: 1.3619190454483032
Epoch 140, training loss: 1.1897106170654297 = 1.1827068328857422 + 0.001 * 7.003824710845947
Epoch 140, val loss: 1.2849812507629395
Epoch 150, training loss: 1.0975762605667114 = 1.0906111001968384 + 0.001 * 6.965202331542969
Epoch 150, val loss: 1.2133928537368774
Epoch 160, training loss: 1.007927417755127 = 1.0009897947311401 + 0.001 * 6.937567710876465
Epoch 160, val loss: 1.1441293954849243
Epoch 170, training loss: 0.9206038117408752 = 0.9136839509010315 + 0.001 * 6.919869422912598
Epoch 170, val loss: 1.0767624378204346
Epoch 180, training loss: 0.8368769288063049 = 0.8299646377563477 + 0.001 * 6.912288188934326
Epoch 180, val loss: 1.0121275186538696
Epoch 190, training loss: 0.7585359215736389 = 0.751628577709198 + 0.001 * 6.907343864440918
Epoch 190, val loss: 0.9522007703781128
Epoch 200, training loss: 0.6872212886810303 = 0.6803188920021057 + 0.001 * 6.902407646179199
Epoch 200, val loss: 0.8987230062484741
Epoch 210, training loss: 0.6238394379615784 = 0.6169424057006836 + 0.001 * 6.897029399871826
Epoch 210, val loss: 0.8529967665672302
Epoch 220, training loss: 0.5682488083839417 = 0.5613594651222229 + 0.001 * 6.889364719390869
Epoch 220, val loss: 0.8150426745414734
Epoch 230, training loss: 0.5194166302680969 = 0.5125359892845154 + 0.001 * 6.880632400512695
Epoch 230, val loss: 0.7845448851585388
Epoch 240, training loss: 0.47566646337509155 = 0.46879667043685913 + 0.001 * 6.8698039054870605
Epoch 240, val loss: 0.7603775858879089
Epoch 250, training loss: 0.43520358204841614 = 0.42833438515663147 + 0.001 * 6.869208335876465
Epoch 250, val loss: 0.7413656711578369
Epoch 260, training loss: 0.39632800221443176 = 0.38947054743766785 + 0.001 * 6.8574419021606445
Epoch 260, val loss: 0.7262879014015198
Epoch 270, training loss: 0.35798612236976624 = 0.3511398434638977 + 0.001 * 6.846273422241211
Epoch 270, val loss: 0.7143991589546204
Epoch 280, training loss: 0.32021474838256836 = 0.3133743703365326 + 0.001 * 6.840363025665283
Epoch 280, val loss: 0.7056666612625122
Epoch 290, training loss: 0.28368937969207764 = 0.27685025334358215 + 0.001 * 6.8391313552856445
Epoch 290, val loss: 0.7005958557128906
Epoch 300, training loss: 0.24957959353923798 = 0.24274690449237823 + 0.001 * 6.832687854766846
Epoch 300, val loss: 0.6989713311195374
Epoch 310, training loss: 0.21868650615215302 = 0.2118544727563858 + 0.001 * 6.832032680511475
Epoch 310, val loss: 0.7010154724121094
Epoch 320, training loss: 0.1915879249572754 = 0.18475879728794098 + 0.001 * 6.829120635986328
Epoch 320, val loss: 0.7065203189849854
Epoch 330, training loss: 0.16839499771595 = 0.16156812012195587 + 0.001 * 6.82688045501709
Epoch 330, val loss: 0.7154097557067871
Epoch 340, training loss: 0.14877061545848846 = 0.1419358253479004 + 0.001 * 6.834789276123047
Epoch 340, val loss: 0.7269324660301208
Epoch 350, training loss: 0.13200527429580688 = 0.12517690658569336 + 0.001 * 6.828373432159424
Epoch 350, val loss: 0.7403286099433899
Epoch 360, training loss: 0.11756682395935059 = 0.11074119806289673 + 0.001 * 6.825626373291016
Epoch 360, val loss: 0.7547191977500916
Epoch 370, training loss: 0.10506858676671982 = 0.09824659675359726 + 0.001 * 6.821986198425293
Epoch 370, val loss: 0.7697033286094666
Epoch 380, training loss: 0.09420911967754364 = 0.08738583326339722 + 0.001 * 6.823287487030029
Epoch 380, val loss: 0.7850761413574219
Epoch 390, training loss: 0.08473000675439835 = 0.07791060954332352 + 0.001 * 6.819398403167725
Epoch 390, val loss: 0.8007720112800598
Epoch 400, training loss: 0.07643335312604904 = 0.06961633265018463 + 0.001 * 6.81702184677124
Epoch 400, val loss: 0.8166049122810364
Epoch 410, training loss: 0.0691429153084755 = 0.06232451647520065 + 0.001 * 6.818400859832764
Epoch 410, val loss: 0.8323754072189331
Epoch 420, training loss: 0.06271927058696747 = 0.05590379983186722 + 0.001 * 6.815469741821289
Epoch 420, val loss: 0.8482356667518616
Epoch 430, training loss: 0.05704426020383835 = 0.05023207142949104 + 0.001 * 6.812190532684326
Epoch 430, val loss: 0.8638849854469299
Epoch 440, training loss: 0.05204656347632408 = 0.04522376507520676 + 0.001 * 6.822798728942871
Epoch 440, val loss: 0.8795130252838135
Epoch 450, training loss: 0.047613125294446945 = 0.0408058799803257 + 0.001 * 6.807244300842285
Epoch 450, val loss: 0.8948902487754822
Epoch 460, training loss: 0.04371785372495651 = 0.03691190481185913 + 0.001 * 6.805947303771973
Epoch 460, val loss: 0.9102898240089417
Epoch 470, training loss: 0.04029190167784691 = 0.03348361700773239 + 0.001 * 6.808284282684326
Epoch 470, val loss: 0.9253554344177246
Epoch 480, training loss: 0.037269093096256256 = 0.030463026836514473 + 0.001 * 6.806066989898682
Epoch 480, val loss: 0.9402230978012085
Epoch 490, training loss: 0.03459732607007027 = 0.027798419818282127 + 0.001 * 6.798907279968262
Epoch 490, val loss: 0.9547869563102722
Epoch 500, training loss: 0.03224077448248863 = 0.025443192571401596 + 0.001 * 6.797582149505615
Epoch 500, val loss: 0.9690399169921875
Epoch 510, training loss: 0.0301782488822937 = 0.02335633523762226 + 0.001 * 6.8219122886657715
Epoch 510, val loss: 0.9828912615776062
Epoch 520, training loss: 0.028310604393482208 = 0.0215029027312994 + 0.001 * 6.807701110839844
Epoch 520, val loss: 0.9964264035224915
Epoch 530, training loss: 0.02665029466152191 = 0.019851386547088623 + 0.001 * 6.798908233642578
Epoch 530, val loss: 1.0094664096832275
Epoch 540, training loss: 0.025164272636175156 = 0.01837323047220707 + 0.001 * 6.791041374206543
Epoch 540, val loss: 1.0221621990203857
Epoch 550, training loss: 0.02385118417441845 = 0.01704571023583412 + 0.001 * 6.805473327636719
Epoch 550, val loss: 1.0345107316970825
Epoch 560, training loss: 0.022639280185103416 = 0.01584874652326107 + 0.001 * 6.790533065795898
Epoch 560, val loss: 1.0465312004089355
Epoch 570, training loss: 0.02155401185154915 = 0.014766523614525795 + 0.001 * 6.7874884605407715
Epoch 570, val loss: 1.0583245754241943
Epoch 580, training loss: 0.02057580091059208 = 0.01378536969423294 + 0.001 * 6.790431499481201
Epoch 580, val loss: 1.069761037826538
Epoch 590, training loss: 0.01968407817184925 = 0.01289424579590559 + 0.001 * 6.78983211517334
Epoch 590, val loss: 1.0809224843978882
Epoch 600, training loss: 0.018866974860429764 = 0.012083246372640133 + 0.001 * 6.783727645874023
Epoch 600, val loss: 1.0917768478393555
Epoch 610, training loss: 0.018126050010323524 = 0.011343416757881641 + 0.001 * 6.782633304595947
Epoch 610, val loss: 1.1023799180984497
Epoch 620, training loss: 0.017452478408813477 = 0.010667463764548302 + 0.001 * 6.785013198852539
Epoch 620, val loss: 1.1126947402954102
Epoch 630, training loss: 0.01683640293776989 = 0.01004890538752079 + 0.001 * 6.787496566772461
Epoch 630, val loss: 1.1227672100067139
Epoch 640, training loss: 0.016265906393527985 = 0.009481805376708508 + 0.001 * 6.784101486206055
Epoch 640, val loss: 1.1325823068618774
Epoch 650, training loss: 0.01573950983583927 = 0.008961115032434464 + 0.001 * 6.778395175933838
Epoch 650, val loss: 1.1421070098876953
Epoch 660, training loss: 0.015258854255080223 = 0.008482156321406364 + 0.001 * 6.776698112487793
Epoch 660, val loss: 1.1514025926589966
Epoch 670, training loss: 0.014844868332147598 = 0.008040884509682655 + 0.001 * 6.80398416519165
Epoch 670, val loss: 1.1604225635528564
Epoch 680, training loss: 0.01441175490617752 = 0.0076337228529155254 + 0.001 * 6.778032302856445
Epoch 680, val loss: 1.1692231893539429
Epoch 690, training loss: 0.014032507315278053 = 0.007257360499352217 + 0.001 * 6.775146961212158
Epoch 690, val loss: 1.1778067350387573
Epoch 700, training loss: 0.013687718659639359 = 0.00690893130376935 + 0.001 * 6.778787612915039
Epoch 700, val loss: 1.1861412525177002
Epoch 710, training loss: 0.013359974138438702 = 0.006585824303328991 + 0.001 * 6.774149417877197
Epoch 710, val loss: 1.1942763328552246
Epoch 720, training loss: 0.01307004876434803 = 0.006285747978836298 + 0.001 * 6.784300327301025
Epoch 720, val loss: 1.2021931409835815
Epoch 730, training loss: 0.012787716463208199 = 0.006006688345223665 + 0.001 * 6.7810282707214355
Epoch 730, val loss: 1.2099621295928955
Epoch 740, training loss: 0.012514820322394371 = 0.005746789276599884 + 0.001 * 6.768031120300293
Epoch 740, val loss: 1.2174911499023438
Epoch 750, training loss: 0.012276014313101768 = 0.005504366010427475 + 0.001 * 6.771648406982422
Epoch 750, val loss: 1.2248413562774658
Epoch 760, training loss: 0.012055786326527596 = 0.005277861375361681 + 0.001 * 6.77792501449585
Epoch 760, val loss: 1.232048749923706
Epoch 770, training loss: 0.011834118515253067 = 0.005065926816314459 + 0.001 * 6.768191814422607
Epoch 770, val loss: 1.2390470504760742
Epoch 780, training loss: 0.01163442898541689 = 0.004867403768002987 + 0.001 * 6.767024993896484
Epoch 780, val loss: 1.2459083795547485
Epoch 790, training loss: 0.011446940712630749 = 0.004681213293224573 + 0.001 * 6.7657270431518555
Epoch 790, val loss: 1.2525701522827148
Epoch 800, training loss: 0.01130247674882412 = 0.004506377968937159 + 0.001 * 6.796098709106445
Epoch 800, val loss: 1.259090781211853
Epoch 810, training loss: 0.011112552136182785 = 0.004342075902968645 + 0.001 * 6.7704758644104
Epoch 810, val loss: 1.2654974460601807
Epoch 820, training loss: 0.010949540883302689 = 0.004187429323792458 + 0.001 * 6.762110710144043
Epoch 820, val loss: 1.271741271018982
Epoch 830, training loss: 0.010818162001669407 = 0.004041685722768307 + 0.001 * 6.77647590637207
Epoch 830, val loss: 1.2778172492980957
Epoch 840, training loss: 0.010660945437848568 = 0.00390420900657773 + 0.001 * 6.7567362785339355
Epoch 840, val loss: 1.2837880849838257
Epoch 850, training loss: 0.010528992861509323 = 0.003774387063458562 + 0.001 * 6.754605293273926
Epoch 850, val loss: 1.2896260023117065
Epoch 860, training loss: 0.010416673496365547 = 0.003651633393019438 + 0.001 * 6.765039920806885
Epoch 860, val loss: 1.2953227758407593
Epoch 870, training loss: 0.010294249281287193 = 0.003535475581884384 + 0.001 * 6.7587738037109375
Epoch 870, val loss: 1.300930380821228
Epoch 880, training loss: 0.010181451216340065 = 0.003425474278628826 + 0.001 * 6.755976676940918
Epoch 880, val loss: 1.3064011335372925
Epoch 890, training loss: 0.010077378712594509 = 0.0033211831469088793 + 0.001 * 6.756195545196533
Epoch 890, val loss: 1.311767816543579
Epoch 900, training loss: 0.009977847337722778 = 0.0032221984583884478 + 0.001 * 6.755648136138916
Epoch 900, val loss: 1.3170056343078613
Epoch 910, training loss: 0.009883894585072994 = 0.0031281958799809217 + 0.001 * 6.7556986808776855
Epoch 910, val loss: 1.322151780128479
Epoch 920, training loss: 0.009790807962417603 = 0.0030388711020350456 + 0.001 * 6.751935958862305
Epoch 920, val loss: 1.3271945714950562
Epoch 930, training loss: 0.009721536189317703 = 0.002953920979052782 + 0.001 * 6.767614364624023
Epoch 930, val loss: 1.332119107246399
Epoch 940, training loss: 0.009618762880563736 = 0.0028730551712214947 + 0.001 * 6.7457075119018555
Epoch 940, val loss: 1.3369719982147217
Epoch 950, training loss: 0.009561873972415924 = 0.0027960166335105896 + 0.001 * 6.765857219696045
Epoch 950, val loss: 1.3417218923568726
Epoch 960, training loss: 0.009479629807174206 = 0.002722563222050667 + 0.001 * 6.757066249847412
Epoch 960, val loss: 1.3463973999023438
Epoch 970, training loss: 0.009401149116456509 = 0.0026524753775447607 + 0.001 * 6.748673439025879
Epoch 970, val loss: 1.3509645462036133
Epoch 980, training loss: 0.009329413995146751 = 0.0025855381973087788 + 0.001 * 6.743875503540039
Epoch 980, val loss: 1.3554636240005493
Epoch 990, training loss: 0.009265781380236149 = 0.00252155982889235 + 0.001 * 6.744221210479736
Epoch 990, val loss: 1.3598806858062744
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.8450
Flip ASR: 0.8133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9464908838272095 = 1.9381170272827148 + 0.001 * 8.373798370361328
Epoch 0, val loss: 1.9258710145950317
Epoch 10, training loss: 1.9359331130981445 = 1.9275593757629395 + 0.001 * 8.373682022094727
Epoch 10, val loss: 1.9150503873825073
Epoch 20, training loss: 1.9229315519332886 = 1.9145581722259521 + 0.001 * 8.373398780822754
Epoch 20, val loss: 1.9015220403671265
Epoch 30, training loss: 1.904803991317749 = 1.8964312076568604 + 0.001 * 8.372841835021973
Epoch 30, val loss: 1.8826228380203247
Epoch 40, training loss: 1.8784795999526978 = 1.870107889175415 + 0.001 * 8.371665000915527
Epoch 40, val loss: 1.8557485342025757
Epoch 50, training loss: 1.842661738395691 = 1.834293007850647 + 0.001 * 8.368682861328125
Epoch 50, val loss: 1.8214796781539917
Epoch 60, training loss: 1.802854299545288 = 1.7944962978363037 + 0.001 * 8.358037948608398
Epoch 60, val loss: 1.788623571395874
Epoch 70, training loss: 1.7646836042404175 = 1.756385326385498 + 0.001 * 8.298280715942383
Epoch 70, val loss: 1.761427402496338
Epoch 80, training loss: 1.7148253917694092 = 1.7068705558776855 + 0.001 * 7.954830169677734
Epoch 80, val loss: 1.7227073907852173
Epoch 90, training loss: 1.6463426351547241 = 1.6387163400650024 + 0.001 * 7.62634801864624
Epoch 90, val loss: 1.6667394638061523
Epoch 100, training loss: 1.5585829019546509 = 1.5512566566467285 + 0.001 * 7.3262619972229
Epoch 100, val loss: 1.5957871675491333
Epoch 110, training loss: 1.4595959186553955 = 1.4524449110031128 + 0.001 * 7.1509904861450195
Epoch 110, val loss: 1.5181242227554321
Epoch 120, training loss: 1.357541561126709 = 1.3504538536071777 + 0.001 * 7.087730884552002
Epoch 120, val loss: 1.4396270513534546
Epoch 130, training loss: 1.255613088607788 = 1.2485854625701904 + 0.001 * 7.027623176574707
Epoch 130, val loss: 1.3630001544952393
Epoch 140, training loss: 1.1544771194458008 = 1.1474764347076416 + 0.001 * 7.000640869140625
Epoch 140, val loss: 1.2868731021881104
Epoch 150, training loss: 1.055953860282898 = 1.0489821434020996 + 0.001 * 6.971715927124023
Epoch 150, val loss: 1.2112672328948975
Epoch 160, training loss: 0.9622890949249268 = 0.9553412795066833 + 0.001 * 6.947799205780029
Epoch 160, val loss: 1.1377639770507812
Epoch 170, training loss: 0.8745839595794678 = 0.8676530122756958 + 0.001 * 6.930971622467041
Epoch 170, val loss: 1.0685838460922241
Epoch 180, training loss: 0.7932521104812622 = 0.7863309383392334 + 0.001 * 6.921157360076904
Epoch 180, val loss: 1.0043350458145142
Epoch 190, training loss: 0.7182815670967102 = 0.7113652229309082 + 0.001 * 6.916337013244629
Epoch 190, val loss: 0.946711003780365
Epoch 200, training loss: 0.6493309736251831 = 0.6424160003662109 + 0.001 * 6.91497802734375
Epoch 200, val loss: 0.8953413367271423
Epoch 210, training loss: 0.5855686068534851 = 0.5786538124084473 + 0.001 * 6.914813995361328
Epoch 210, val loss: 0.8501969575881958
Epoch 220, training loss: 0.5258917808532715 = 0.5189770460128784 + 0.001 * 6.9147515296936035
Epoch 220, val loss: 0.8104084134101868
Epoch 230, training loss: 0.4693349599838257 = 0.46242013573646545 + 0.001 * 6.914825916290283
Epoch 230, val loss: 0.7756973505020142
Epoch 240, training loss: 0.4152761399745941 = 0.4083609879016876 + 0.001 * 6.915157794952393
Epoch 240, val loss: 0.7456954717636108
Epoch 250, training loss: 0.3636234402656555 = 0.3567076325416565 + 0.001 * 6.91580867767334
Epoch 250, val loss: 0.7201148271560669
Epoch 260, training loss: 0.31474611163139343 = 0.30782991647720337 + 0.001 * 6.916197299957275
Epoch 260, val loss: 0.6993023753166199
Epoch 270, training loss: 0.269648015499115 = 0.26273113489151 + 0.001 * 6.91687536239624
Epoch 270, val loss: 0.6832748651504517
Epoch 280, training loss: 0.22942352294921875 = 0.2225058525800705 + 0.001 * 6.917669296264648
Epoch 280, val loss: 0.6722887754440308
Epoch 290, training loss: 0.19474557042121887 = 0.18782658874988556 + 0.001 * 6.918975830078125
Epoch 290, val loss: 0.6661281585693359
Epoch 300, training loss: 0.16565777361392975 = 0.15873822569847107 + 0.001 * 6.919553279876709
Epoch 300, val loss: 0.6644973158836365
Epoch 310, training loss: 0.14167828857898712 = 0.13475768268108368 + 0.001 * 6.92060661315918
Epoch 310, val loss: 0.6671152710914612
Epoch 320, training loss: 0.12207429111003876 = 0.11515171825885773 + 0.001 * 6.922571659088135
Epoch 320, val loss: 0.6733601093292236
Epoch 330, training loss: 0.10607247799634933 = 0.09914989024400711 + 0.001 * 6.922590255737305
Epoch 330, val loss: 0.6823287010192871
Epoch 340, training loss: 0.09296263754367828 = 0.08603966236114502 + 0.001 * 6.922972679138184
Epoch 340, val loss: 0.6931704878807068
Epoch 350, training loss: 0.08215571194887161 = 0.07523223012685776 + 0.001 * 6.9234819412231445
Epoch 350, val loss: 0.7052357792854309
Epoch 360, training loss: 0.07317594438791275 = 0.0662519782781601 + 0.001 * 6.923965930938721
Epoch 360, val loss: 0.7179707884788513
Epoch 370, training loss: 0.06564757972955704 = 0.058725085109472275 + 0.001 * 6.922492027282715
Epoch 370, val loss: 0.7309818863868713
Epoch 380, training loss: 0.05928579717874527 = 0.052364446222782135 + 0.001 * 6.921350479125977
Epoch 380, val loss: 0.7440939545631409
Epoch 390, training loss: 0.05386871099472046 = 0.046944841742515564 + 0.001 * 6.923867702484131
Epoch 390, val loss: 0.75717693567276
Epoch 400, training loss: 0.04921235144138336 = 0.04229329526424408 + 0.001 * 6.91905403137207
Epoch 400, val loss: 0.7701256275177002
Epoch 410, training loss: 0.04519082233309746 = 0.038273949176073074 + 0.001 * 6.916873931884766
Epoch 410, val loss: 0.7828964591026306
Epoch 420, training loss: 0.041691891849040985 = 0.034778449684381485 + 0.001 * 6.913440227508545
Epoch 420, val loss: 0.7953969240188599
Epoch 430, training loss: 0.03863057494163513 = 0.03172185644507408 + 0.001 * 6.908718109130859
Epoch 430, val loss: 0.8076032996177673
Epoch 440, training loss: 0.03594223037362099 = 0.029035326093435287 + 0.001 * 6.906905174255371
Epoch 440, val loss: 0.8195407390594482
Epoch 450, training loss: 0.03356925770640373 = 0.026663318276405334 + 0.001 * 6.905939102172852
Epoch 450, val loss: 0.831220805644989
Epoch 460, training loss: 0.031454846262931824 = 0.024559959769248962 + 0.001 * 6.894887924194336
Epoch 460, val loss: 0.8426226377487183
Epoch 470, training loss: 0.029587648808956146 = 0.022687382996082306 + 0.001 * 6.900265216827393
Epoch 470, val loss: 0.8537418246269226
Epoch 480, training loss: 0.027909427881240845 = 0.021014761179685593 + 0.001 * 6.894665241241455
Epoch 480, val loss: 0.8646032810211182
Epoch 490, training loss: 0.026391319930553436 = 0.019515782594680786 + 0.001 * 6.87553596496582
Epoch 490, val loss: 0.8751884698867798
Epoch 500, training loss: 0.025051213800907135 = 0.018168319016695023 + 0.001 * 6.882894039154053
Epoch 500, val loss: 0.8855070471763611
Epoch 510, training loss: 0.023843295872211456 = 0.01695343852043152 + 0.001 * 6.889857769012451
Epoch 510, val loss: 0.8955618739128113
Epoch 520, training loss: 0.022716689854860306 = 0.01585514284670353 + 0.001 * 6.861546993255615
Epoch 520, val loss: 0.9053941965103149
Epoch 530, training loss: 0.02171281725168228 = 0.014859700575470924 + 0.001 * 6.853116989135742
Epoch 530, val loss: 0.9149880409240723
Epoch 540, training loss: 0.02082636207342148 = 0.013954902067780495 + 0.001 * 6.871460437774658
Epoch 540, val loss: 0.924360454082489
Epoch 550, training loss: 0.019970986992120743 = 0.013130487874150276 + 0.001 * 6.840498924255371
Epoch 550, val loss: 0.9334795475006104
Epoch 560, training loss: 0.01922737993299961 = 0.012377564795315266 + 0.001 * 6.849815368652344
Epoch 560, val loss: 0.9424073696136475
Epoch 570, training loss: 0.018535638228058815 = 0.011688324622809887 + 0.001 * 6.847313404083252
Epoch 570, val loss: 0.9510859847068787
Epoch 580, training loss: 0.01790839433670044 = 0.011055934242904186 + 0.001 * 6.852460861206055
Epoch 580, val loss: 0.9595898389816284
Epoch 590, training loss: 0.017319321632385254 = 0.010474569164216518 + 0.001 * 6.844752788543701
Epoch 590, val loss: 0.9678687453269958
Epoch 600, training loss: 0.016782496124505997 = 0.009938998147845268 + 0.001 * 6.843497276306152
Epoch 600, val loss: 0.9760033488273621
Epoch 610, training loss: 0.016272637993097305 = 0.009444656781852245 + 0.001 * 6.827980995178223
Epoch 610, val loss: 0.9839341044425964
Epoch 620, training loss: 0.015839919447898865 = 0.008987491950392723 + 0.001 * 6.8524274826049805
Epoch 620, val loss: 0.9917110800743103
Epoch 630, training loss: 0.015384865924715996 = 0.008563872426748276 + 0.001 * 6.820992946624756
Epoch 630, val loss: 0.9992966055870056
Epoch 640, training loss: 0.01499221846461296 = 0.008170741610229015 + 0.001 * 6.821476936340332
Epoch 640, val loss: 1.0066951513290405
Epoch 650, training loss: 0.014657199382781982 = 0.007805284578353167 + 0.001 * 6.851913928985596
Epoch 650, val loss: 1.0139453411102295
Epoch 660, training loss: 0.01428309641778469 = 0.007464988622814417 + 0.001 * 6.8181071281433105
Epoch 660, val loss: 1.0210306644439697
Epoch 670, training loss: 0.01397329568862915 = 0.007147626020014286 + 0.001 * 6.825668811798096
Epoch 670, val loss: 1.0279793739318848
Epoch 680, training loss: 0.013661370612680912 = 0.006851158104836941 + 0.001 * 6.810212135314941
Epoch 680, val loss: 1.0347756147384644
Epoch 690, training loss: 0.013427067548036575 = 0.0065739210695028305 + 0.001 * 6.853146076202393
Epoch 690, val loss: 1.0414671897888184
Epoch 700, training loss: 0.013131147250533104 = 0.006314254831522703 + 0.001 * 6.816892147064209
Epoch 700, val loss: 1.0480188131332397
Epoch 710, training loss: 0.01289578527212143 = 0.006070704665035009 + 0.001 * 6.825080394744873
Epoch 710, val loss: 1.0544012784957886
Epoch 720, training loss: 0.012652300298213959 = 0.005841994192451239 + 0.001 * 6.810306072235107
Epoch 720, val loss: 1.060709834098816
Epoch 730, training loss: 0.012461105361580849 = 0.005626888945698738 + 0.001 * 6.834216594696045
Epoch 730, val loss: 1.0668617486953735
Epoch 740, training loss: 0.012223316356539726 = 0.005424422677606344 + 0.001 * 6.798893451690674
Epoch 740, val loss: 1.0728942155838013
Epoch 750, training loss: 0.012047426775097847 = 0.005233563482761383 + 0.001 * 6.8138628005981445
Epoch 750, val loss: 1.078843116760254
Epoch 760, training loss: 0.011862793937325478 = 0.005053495988249779 + 0.001 * 6.809297561645508
Epoch 760, val loss: 1.0846649408340454
Epoch 770, training loss: 0.011683419346809387 = 0.004883415065705776 + 0.001 * 6.800004482269287
Epoch 770, val loss: 1.0903993844985962
Epoch 780, training loss: 0.011519020423293114 = 0.004722583573311567 + 0.001 * 6.796435832977295
Epoch 780, val loss: 1.096002221107483
Epoch 790, training loss: 0.011363573372364044 = 0.004570395220071077 + 0.001 * 6.793177604675293
Epoch 790, val loss: 1.1015146970748901
Epoch 800, training loss: 0.011226564645767212 = 0.004426213912665844 + 0.001 * 6.800350189208984
Epoch 800, val loss: 1.1069151163101196
Epoch 810, training loss: 0.01107693463563919 = 0.00428951159119606 + 0.001 * 6.7874226570129395
Epoch 810, val loss: 1.11224365234375
Epoch 820, training loss: 0.010951755568385124 = 0.0041597639210522175 + 0.001 * 6.791991710662842
Epoch 820, val loss: 1.1174877882003784
Epoch 830, training loss: 0.010830738581717014 = 0.004036514088511467 + 0.001 * 6.794224262237549
Epoch 830, val loss: 1.1226295232772827
Epoch 840, training loss: 0.010719066485762596 = 0.003919344861060381 + 0.001 * 6.7997212409973145
Epoch 840, val loss: 1.1276733875274658
Epoch 850, training loss: 0.010603505186736584 = 0.003807856934145093 + 0.001 * 6.795648097991943
Epoch 850, val loss: 1.1326544284820557
Epoch 860, training loss: 0.010501698590815067 = 0.0037016698624938726 + 0.001 * 6.800028324127197
Epoch 860, val loss: 1.137539267539978
Epoch 870, training loss: 0.01039038971066475 = 0.003600481664761901 + 0.001 * 6.789907932281494
Epoch 870, val loss: 1.142338514328003
Epoch 880, training loss: 0.010298449546098709 = 0.0035039775539189577 + 0.001 * 6.794471263885498
Epoch 880, val loss: 1.1470897197723389
Epoch 890, training loss: 0.010192615911364555 = 0.0034118550829589367 + 0.001 * 6.780760288238525
Epoch 890, val loss: 1.151747226715088
Epoch 900, training loss: 0.010100296698510647 = 0.00332383019849658 + 0.001 * 6.776466369628906
Epoch 900, val loss: 1.1563359498977661
Epoch 910, training loss: 0.01003540214151144 = 0.003239687066525221 + 0.001 * 6.795714855194092
Epoch 910, val loss: 1.160853385925293
Epoch 920, training loss: 0.009934883564710617 = 0.0031590645667165518 + 0.001 * 6.775818347930908
Epoch 920, val loss: 1.1652815341949463
Epoch 930, training loss: 0.009853895753622055 = 0.0030817552469670773 + 0.001 * 6.7721405029296875
Epoch 930, val loss: 1.169692873954773
Epoch 940, training loss: 0.009781710803508759 = 0.0030074622482061386 + 0.001 * 6.774248123168945
Epoch 940, val loss: 1.1739901304244995
Epoch 950, training loss: 0.009727143682539463 = 0.002935874043032527 + 0.001 * 6.791269779205322
Epoch 950, val loss: 1.1782759428024292
Epoch 960, training loss: 0.009634589776396751 = 0.0028667033184319735 + 0.001 * 6.767885684967041
Epoch 960, val loss: 1.1824675798416138
Epoch 970, training loss: 0.009560469537973404 = 0.002799667650833726 + 0.001 * 6.760801792144775
Epoch 970, val loss: 1.1866451501846313
Epoch 980, training loss: 0.00951105821877718 = 0.0027345262933522463 + 0.001 * 6.77653169631958
Epoch 980, val loss: 1.1908066272735596
Epoch 990, training loss: 0.009446410462260246 = 0.002671145834028721 + 0.001 * 6.775264263153076
Epoch 990, val loss: 1.1949394941329956
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.8266
Flip ASR: 0.7911/225 nodes
The final ASR:0.73063, 0.14892, Accuracy:0.79136, 0.01968
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11600])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10560])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9557
Flip ASR: 0.9467/225 nodes
The final ASR:0.97294, 0.01254, Accuracy:0.83457, 0.00761
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.938175916671753 = 1.9298021793365479 + 0.001 * 8.373766899108887
Epoch 0, val loss: 1.9266680479049683
Epoch 10, training loss: 1.9283896684646606 = 1.9200160503387451 + 0.001 * 8.373671531677246
Epoch 10, val loss: 1.9173970222473145
Epoch 20, training loss: 1.9162230491638184 = 1.907849669456482 + 0.001 * 8.373364448547363
Epoch 20, val loss: 1.9055256843566895
Epoch 30, training loss: 1.8989360332489014 = 1.8905632495880127 + 0.001 * 8.372726440429688
Epoch 30, val loss: 1.8883402347564697
Epoch 40, training loss: 1.8736411333084106 = 1.8652697801589966 + 0.001 * 8.371342658996582
Epoch 40, val loss: 1.8635069131851196
Epoch 50, training loss: 1.8395452499389648 = 1.8311775922775269 + 0.001 * 8.367602348327637
Epoch 50, val loss: 1.8319967985153198
Epoch 60, training loss: 1.803194522857666 = 1.7948402166366577 + 0.001 * 8.354291915893555
Epoch 60, val loss: 1.8025507926940918
Epoch 70, training loss: 1.768580436706543 = 1.760293960571289 + 0.001 * 8.286450386047363
Epoch 70, val loss: 1.7763116359710693
Epoch 80, training loss: 1.7216665744781494 = 1.7137030363082886 + 0.001 * 7.9635186195373535
Epoch 80, val loss: 1.7372866868972778
Epoch 90, training loss: 1.6576979160308838 = 1.650011658668518 + 0.001 * 7.686209201812744
Epoch 90, val loss: 1.6831122636795044
Epoch 100, training loss: 1.5765111446380615 = 1.569067120552063 + 0.001 * 7.444082736968994
Epoch 100, val loss: 1.6160141229629517
Epoch 110, training loss: 1.4869074821472168 = 1.479610800743103 + 0.001 * 7.29668664932251
Epoch 110, val loss: 1.54449462890625
Epoch 120, training loss: 1.3955481052398682 = 1.388280987739563 + 0.001 * 7.267091274261475
Epoch 120, val loss: 1.4728577136993408
Epoch 130, training loss: 1.302524447441101 = 1.2953122854232788 + 0.001 * 7.212210655212402
Epoch 130, val loss: 1.4016084671020508
Epoch 140, training loss: 1.206071376800537 = 1.1989442110061646 + 0.001 * 7.127220153808594
Epoch 140, val loss: 1.3265618085861206
Epoch 150, training loss: 1.1061341762542725 = 1.0991179943084717 + 0.001 * 7.016208171844482
Epoch 150, val loss: 1.2486655712127686
Epoch 160, training loss: 1.0055128335952759 = 0.9985476732254028 + 0.001 * 6.965147972106934
Epoch 160, val loss: 1.170414924621582
Epoch 170, training loss: 0.908067524433136 = 0.9011222124099731 + 0.001 * 6.945303916931152
Epoch 170, val loss: 1.0958620309829712
Epoch 180, training loss: 0.8173401951789856 = 0.8104110956192017 + 0.001 * 6.92911958694458
Epoch 180, val loss: 1.0285544395446777
Epoch 190, training loss: 0.7358046770095825 = 0.7288854718208313 + 0.001 * 6.919197082519531
Epoch 190, val loss: 0.9702653884887695
Epoch 200, training loss: 0.6640998721122742 = 0.6571875810623169 + 0.001 * 6.91231107711792
Epoch 200, val loss: 0.9226170778274536
Epoch 210, training loss: 0.6011317372322083 = 0.5942240357398987 + 0.001 * 6.907716274261475
Epoch 210, val loss: 0.8847489356994629
Epoch 220, training loss: 0.5451470017433167 = 0.5382432341575623 + 0.001 * 6.9037675857543945
Epoch 220, val loss: 0.8555757999420166
Epoch 230, training loss: 0.4947400987148285 = 0.4878402650356293 + 0.001 * 6.8998284339904785
Epoch 230, val loss: 0.8335691690444946
Epoch 240, training loss: 0.44887322187423706 = 0.4419771432876587 + 0.001 * 6.896089553833008
Epoch 240, val loss: 0.8175007700920105
Epoch 250, training loss: 0.4068624675273895 = 0.3999701142311096 + 0.001 * 6.892353534698486
Epoch 250, val loss: 0.8062954545021057
Epoch 260, training loss: 0.36846572160720825 = 0.3615765869617462 + 0.001 * 6.889127731323242
Epoch 260, val loss: 0.7996034026145935
Epoch 270, training loss: 0.33360108733177185 = 0.3267149031162262 + 0.001 * 6.886194229125977
Epoch 270, val loss: 0.7974174618721008
Epoch 280, training loss: 0.3020813763141632 = 0.295197457075119 + 0.001 * 6.883923053741455
Epoch 280, val loss: 0.799466609954834
Epoch 290, training loss: 0.27339884638786316 = 0.26651766896247864 + 0.001 * 6.881168365478516
Epoch 290, val loss: 0.8049986362457275
Epoch 300, training loss: 0.24690693616867065 = 0.24002787470817566 + 0.001 * 6.879058361053467
Epoch 300, val loss: 0.813197135925293
Epoch 310, training loss: 0.22198498249053955 = 0.21510788798332214 + 0.001 * 6.877100467681885
Epoch 310, val loss: 0.823767364025116
Epoch 320, training loss: 0.19839198887348175 = 0.1915162354707718 + 0.001 * 6.875753402709961
Epoch 320, val loss: 0.836443305015564
Epoch 330, training loss: 0.1762816309928894 = 0.16940778493881226 + 0.001 * 6.873844146728516
Epoch 330, val loss: 0.8515287637710571
Epoch 340, training loss: 0.15600377321243286 = 0.14913205802440643 + 0.001 * 6.871715068817139
Epoch 340, val loss: 0.8690147399902344
Epoch 350, training loss: 0.1378004252910614 = 0.13093014061450958 + 0.001 * 6.8702898025512695
Epoch 350, val loss: 0.8887489438056946
Epoch 360, training loss: 0.12169913947582245 = 0.11483077704906464 + 0.001 * 6.868363380432129
Epoch 360, val loss: 0.9106523394584656
Epoch 370, training loss: 0.10757988691329956 = 0.10071366280317307 + 0.001 * 6.866226673126221
Epoch 370, val loss: 0.9345051050186157
Epoch 380, training loss: 0.09525766223669052 = 0.08839371055364609 + 0.001 * 6.863954544067383
Epoch 380, val loss: 0.959752082824707
Epoch 390, training loss: 0.084548719227314 = 0.07768672704696655 + 0.001 * 6.8619914054870605
Epoch 390, val loss: 0.985791802406311
Epoch 400, training loss: 0.07527215778827667 = 0.06841328740119934 + 0.001 * 6.858870029449463
Epoch 400, val loss: 1.0120221376419067
Epoch 410, training loss: 0.06725630909204483 = 0.06040068715810776 + 0.001 * 6.855620861053467
Epoch 410, val loss: 1.037962555885315
Epoch 420, training loss: 0.06034507229924202 = 0.053491994738578796 + 0.001 * 6.853076934814453
Epoch 420, val loss: 1.0632658004760742
Epoch 430, training loss: 0.05438905954360962 = 0.04754050821065903 + 0.001 * 6.8485493659973145
Epoch 430, val loss: 1.087760090827942
Epoch 440, training loss: 0.04925692081451416 = 0.042411793023347855 + 0.001 * 6.845127105712891
Epoch 440, val loss: 1.1113048791885376
Epoch 450, training loss: 0.04482102394104004 = 0.03798553720116615 + 0.001 * 6.835487365722656
Epoch 450, val loss: 1.1338361501693726
Epoch 460, training loss: 0.04102667421102524 = 0.03415656462311745 + 0.001 * 6.870107650756836
Epoch 460, val loss: 1.1553632020950317
Epoch 470, training loss: 0.03766975551843643 = 0.030836625024676323 + 0.001 * 6.833130359649658
Epoch 470, val loss: 1.1759164333343506
Epoch 480, training loss: 0.03476332500576973 = 0.02794891968369484 + 0.001 * 6.814403533935547
Epoch 480, val loss: 1.1956019401550293
Epoch 490, training loss: 0.03223414346575737 = 0.025428270921111107 + 0.001 * 6.805872440338135
Epoch 490, val loss: 1.214457392692566
Epoch 500, training loss: 0.030061330646276474 = 0.023220133036375046 + 0.001 * 6.8411970138549805
Epoch 500, val loss: 1.2324938774108887
Epoch 510, training loss: 0.028079986572265625 = 0.021278362721204758 + 0.001 * 6.8016228675842285
Epoch 510, val loss: 1.249746561050415
Epoch 520, training loss: 0.02636174112558365 = 0.019564565271139145 + 0.001 * 6.797176361083984
Epoch 520, val loss: 1.2662363052368164
Epoch 530, training loss: 0.024840109050273895 = 0.018046284094452858 + 0.001 * 6.793824672698975
Epoch 530, val loss: 1.2820371389389038
Epoch 540, training loss: 0.023483527824282646 = 0.016696371138095856 + 0.001 * 6.787156105041504
Epoch 540, val loss: 1.2972161769866943
Epoch 550, training loss: 0.022292666137218475 = 0.015491940081119537 + 0.001 * 6.800726413726807
Epoch 550, val loss: 1.3117834329605103
Epoch 560, training loss: 0.02119123376905918 = 0.014413613826036453 + 0.001 * 6.777619361877441
Epoch 560, val loss: 1.325786828994751
Epoch 570, training loss: 0.020221926271915436 = 0.01344405859708786 + 0.001 * 6.777866363525391
Epoch 570, val loss: 1.3392584323883057
Epoch 580, training loss: 0.019340649247169495 = 0.012567806988954544 + 0.001 * 6.772840976715088
Epoch 580, val loss: 1.352314829826355
Epoch 590, training loss: 0.018538933247327805 = 0.011769027449190617 + 0.001 * 6.769906044006348
Epoch 590, val loss: 1.3650565147399902
Epoch 600, training loss: 0.01780349388718605 = 0.011036199517548084 + 0.001 * 6.767293453216553
Epoch 600, val loss: 1.3775551319122314
Epoch 610, training loss: 0.017128854990005493 = 0.010361156426370144 + 0.001 * 6.767698287963867
Epoch 610, val loss: 1.389891266822815
Epoch 620, training loss: 0.016506075859069824 = 0.009738516062498093 + 0.001 * 6.767560005187988
Epoch 620, val loss: 1.4020140171051025
Epoch 630, training loss: 0.015930088236927986 = 0.009164202027022839 + 0.001 * 6.765885353088379
Epoch 630, val loss: 1.4139072895050049
Epoch 640, training loss: 0.01539644692093134 = 0.008634481579065323 + 0.001 * 6.761964797973633
Epoch 640, val loss: 1.4255651235580444
Epoch 650, training loss: 0.01490810327231884 = 0.008145890198647976 + 0.001 * 6.762212753295898
Epoch 650, val loss: 1.4368963241577148
Epoch 660, training loss: 0.01445651613175869 = 0.007695211097598076 + 0.001 * 6.761305332183838
Epoch 660, val loss: 1.447972297668457
Epoch 670, training loss: 0.014039501547813416 = 0.0072792693972587585 + 0.001 * 6.760231971740723
Epoch 670, val loss: 1.458767056465149
Epoch 680, training loss: 0.01365661434829235 = 0.006895139347761869 + 0.001 * 6.761474609375
Epoch 680, val loss: 1.46925687789917
Epoch 690, training loss: 0.013297377154231071 = 0.0065401336178183556 + 0.001 * 6.757243633270264
Epoch 690, val loss: 1.479468822479248
Epoch 700, training loss: 0.012966885231435299 = 0.006211743224412203 + 0.001 * 6.755141735076904
Epoch 700, val loss: 1.4893935918807983
Epoch 710, training loss: 0.012660764157772064 = 0.005907684564590454 + 0.001 * 6.753079414367676
Epoch 710, val loss: 1.4990447759628296
Epoch 720, training loss: 0.012378543615341187 = 0.00562578858807683 + 0.001 * 6.752754211425781
Epoch 720, val loss: 1.508418321609497
Epoch 730, training loss: 0.01211806945502758 = 0.0053641474805772305 + 0.001 * 6.7539215087890625
Epoch 730, val loss: 1.517516851425171
Epoch 740, training loss: 0.011880336329340935 = 0.005121015477925539 + 0.001 * 6.759320259094238
Epoch 740, val loss: 1.5263489484786987
Epoch 750, training loss: 0.011643616482615471 = 0.004894712008535862 + 0.001 * 6.748904705047607
Epoch 750, val loss: 1.5349376201629639
Epoch 760, training loss: 0.011433931067585945 = 0.004683852195739746 + 0.001 * 6.750079154968262
Epoch 760, val loss: 1.5432765483856201
Epoch 770, training loss: 0.011241355910897255 = 0.004487111698836088 + 0.001 * 6.75424337387085
Epoch 770, val loss: 1.551361322402954
Epoch 780, training loss: 0.011046073399484158 = 0.0043032546527683735 + 0.001 * 6.742818355560303
Epoch 780, val loss: 1.559255599975586
Epoch 790, training loss: 0.010897485539317131 = 0.0041312421672046185 + 0.001 * 6.7662434577941895
Epoch 790, val loss: 1.566926121711731
Epoch 800, training loss: 0.010721294209361076 = 0.0039701275527477264 + 0.001 * 6.751166343688965
Epoch 800, val loss: 1.5743813514709473
Epoch 810, training loss: 0.01055947132408619 = 0.003819018602371216 + 0.001 * 6.740452289581299
Epoch 810, val loss: 1.5816373825073242
Epoch 820, training loss: 0.010424517095088959 = 0.003677003551274538 + 0.001 * 6.747513294219971
Epoch 820, val loss: 1.5886943340301514
Epoch 830, training loss: 0.01028457935899496 = 0.003543501952663064 + 0.001 * 6.741077423095703
Epoch 830, val loss: 1.5955942869186401
Epoch 840, training loss: 0.010160855948925018 = 0.0034179671201854944 + 0.001 * 6.742888450622559
Epoch 840, val loss: 1.602295994758606
Epoch 850, training loss: 0.010036328807473183 = 0.003299664705991745 + 0.001 * 6.736663818359375
Epoch 850, val loss: 1.6088141202926636
Epoch 860, training loss: 0.009925918653607368 = 0.0031880708411335945 + 0.001 * 6.737846851348877
Epoch 860, val loss: 1.6151820421218872
Epoch 870, training loss: 0.009821845218539238 = 0.003082714742049575 + 0.001 * 6.739130020141602
Epoch 870, val loss: 1.6213840246200562
Epoch 880, training loss: 0.00972579326480627 = 0.0029831365682184696 + 0.001 * 6.742656230926514
Epoch 880, val loss: 1.6274316310882568
Epoch 890, training loss: 0.009627844206988811 = 0.002888909075409174 + 0.001 * 6.7389349937438965
Epoch 890, val loss: 1.6333556175231934
Epoch 900, training loss: 0.009532347321510315 = 0.0027996813878417015 + 0.001 * 6.732665061950684
Epoch 900, val loss: 1.6391243934631348
Epoch 910, training loss: 0.009468684904277325 = 0.0027150458190590143 + 0.001 * 6.753639221191406
Epoch 910, val loss: 1.6447376012802124
Epoch 920, training loss: 0.009374255314469337 = 0.0026347627863287926 + 0.001 * 6.739492416381836
Epoch 920, val loss: 1.6502463817596436
Epoch 930, training loss: 0.009293768554925919 = 0.002558509586378932 + 0.001 * 6.735259056091309
Epoch 930, val loss: 1.6555981636047363
Epoch 940, training loss: 0.009219255298376083 = 0.0024860200937837362 + 0.001 * 6.733234882354736
Epoch 940, val loss: 1.6608209609985352
Epoch 950, training loss: 0.009143700823187828 = 0.0024170789401978254 + 0.001 * 6.726621150970459
Epoch 950, val loss: 1.6659306287765503
Epoch 960, training loss: 0.00908290222287178 = 0.0023514563217759132 + 0.001 * 6.731446266174316
Epoch 960, val loss: 1.6708916425704956
Epoch 970, training loss: 0.009019496850669384 = 0.002288921969011426 + 0.001 * 6.730574607849121
Epoch 970, val loss: 1.6757668256759644
Epoch 980, training loss: 0.00895717553794384 = 0.0022292863577604294 + 0.001 * 6.727889060974121
Epoch 980, val loss: 1.6804983615875244
Epoch 990, training loss: 0.008897649124264717 = 0.0021722898818552494 + 0.001 * 6.725358486175537
Epoch 990, val loss: 1.6851298809051514
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.6273
Flip ASR: 0.5644/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9684536457061768 = 1.9600799083709717 + 0.001 * 8.373726844787598
Epoch 0, val loss: 1.9583163261413574
Epoch 10, training loss: 1.9581769704818726 = 1.949803352355957 + 0.001 * 8.373598098754883
Epoch 10, val loss: 1.9476474523544312
Epoch 20, training loss: 1.9456188678741455 = 1.9372456073760986 + 0.001 * 8.373250961303711
Epoch 20, val loss: 1.9343465566635132
Epoch 30, training loss: 1.928133249282837 = 1.9197607040405273 + 0.001 * 8.372568130493164
Epoch 30, val loss: 1.915776252746582
Epoch 40, training loss: 1.9021252393722534 = 1.8937541246414185 + 0.001 * 8.371082305908203
Epoch 40, val loss: 1.8883999586105347
Epoch 50, training loss: 1.8643536567687988 = 1.8559867143630981 + 0.001 * 8.36697769165039
Epoch 50, val loss: 1.8501423597335815
Epoch 60, training loss: 1.8186936378479004 = 1.810342788696289 + 0.001 * 8.350852012634277
Epoch 60, val loss: 1.8082033395767212
Epoch 70, training loss: 1.7788479328155518 = 1.7706056833267212 + 0.001 * 8.242264747619629
Epoch 70, val loss: 1.7764499187469482
Epoch 80, training loss: 1.7363306283950806 = 1.728481650352478 + 0.001 * 7.84900426864624
Epoch 80, val loss: 1.7407652139663696
Epoch 90, training loss: 1.677573323249817 = 1.6701099872589111 + 0.001 * 7.463389873504639
Epoch 90, val loss: 1.6913942098617554
Epoch 100, training loss: 1.5988318920135498 = 1.5916979312896729 + 0.001 * 7.134005069732666
Epoch 100, val loss: 1.6259716749191284
Epoch 110, training loss: 1.5032836198806763 = 1.4962821006774902 + 0.001 * 7.001477241516113
Epoch 110, val loss: 1.5476477146148682
Epoch 120, training loss: 1.3999727964401245 = 1.3930189609527588 + 0.001 * 6.953869342803955
Epoch 120, val loss: 1.4625647068023682
Epoch 130, training loss: 1.2954906225204468 = 1.2885633707046509 + 0.001 * 6.927271366119385
Epoch 130, val loss: 1.3800277709960938
Epoch 140, training loss: 1.1913659572601318 = 1.1844525337219238 + 0.001 * 6.913391590118408
Epoch 140, val loss: 1.2988337278366089
Epoch 150, training loss: 1.0889811515808105 = 1.0820856094360352 + 0.001 * 6.895554065704346
Epoch 150, val loss: 1.2195920944213867
Epoch 160, training loss: 0.9906808137893677 = 0.9837998747825623 + 0.001 * 6.880941867828369
Epoch 160, val loss: 1.1441553831100464
Epoch 170, training loss: 0.897793173789978 = 0.8909218907356262 + 0.001 * 6.871306419372559
Epoch 170, val loss: 1.0736098289489746
Epoch 180, training loss: 0.8109314441680908 = 0.8040637373924255 + 0.001 * 6.867692947387695
Epoch 180, val loss: 1.0092371702194214
Epoch 190, training loss: 0.7310484051704407 = 0.7241824865341187 + 0.001 * 6.865933418273926
Epoch 190, val loss: 0.9520552158355713
Epoch 200, training loss: 0.659006655216217 = 0.6521430015563965 + 0.001 * 6.8636794090271
Epoch 200, val loss: 0.9028064012527466
Epoch 210, training loss: 0.5944953560829163 = 0.5876348614692688 + 0.001 * 6.860482692718506
Epoch 210, val loss: 0.8607302904129028
Epoch 220, training loss: 0.5363912582397461 = 0.529535174369812 + 0.001 * 6.856067180633545
Epoch 220, val loss: 0.8251380920410156
Epoch 230, training loss: 0.4837256968021393 = 0.476875901222229 + 0.001 * 6.849783420562744
Epoch 230, val loss: 0.7949678897857666
Epoch 240, training loss: 0.43583640456199646 = 0.4289957880973816 + 0.001 * 6.840630531311035
Epoch 240, val loss: 0.770153284072876
Epoch 250, training loss: 0.3923265337944031 = 0.38549771904945374 + 0.001 * 6.8288187980651855
Epoch 250, val loss: 0.7505502700805664
Epoch 260, training loss: 0.35299769043922424 = 0.346182644367218 + 0.001 * 6.815033435821533
Epoch 260, val loss: 0.7360506653785706
Epoch 270, training loss: 0.3175693154335022 = 0.3107704222202301 + 0.001 * 6.798890590667725
Epoch 270, val loss: 0.7261767983436584
Epoch 280, training loss: 0.2854965031147003 = 0.27871203422546387 + 0.001 * 6.784470081329346
Epoch 280, val loss: 0.7203251719474792
Epoch 290, training loss: 0.2559910714626312 = 0.24921417236328125 + 0.001 * 6.776907444000244
Epoch 290, val loss: 0.7173603773117065
Epoch 300, training loss: 0.22836878895759583 = 0.2215980738401413 + 0.001 * 6.770708084106445
Epoch 300, val loss: 0.7164856791496277
Epoch 310, training loss: 0.20245400071144104 = 0.19568780064582825 + 0.001 * 6.76619291305542
Epoch 310, val loss: 0.7173178195953369
Epoch 320, training loss: 0.1785769909620285 = 0.1718103289604187 + 0.001 * 6.76666259765625
Epoch 320, val loss: 0.7196610569953918
Epoch 330, training loss: 0.15722626447677612 = 0.15045887231826782 + 0.001 * 6.767399311065674
Epoch 330, val loss: 0.7238215208053589
Epoch 340, training loss: 0.13867127895355225 = 0.1319037228822708 + 0.001 * 6.767560958862305
Epoch 340, val loss: 0.7299041748046875
Epoch 350, training loss: 0.1227869987487793 = 0.11601952463388443 + 0.001 * 6.767477035522461
Epoch 350, val loss: 0.7377682328224182
Epoch 360, training loss: 0.10925384610891342 = 0.1024865210056305 + 0.001 * 6.767328262329102
Epoch 360, val loss: 0.7474315762519836
Epoch 370, training loss: 0.09770488739013672 = 0.09093783795833588 + 0.001 * 6.767048358917236
Epoch 370, val loss: 0.7585166096687317
Epoch 380, training loss: 0.08779777586460114 = 0.0810309648513794 + 0.001 * 6.766811847686768
Epoch 380, val loss: 0.7707051634788513
Epoch 390, training loss: 0.07924487441778183 = 0.07247865945100784 + 0.001 * 6.76621150970459
Epoch 390, val loss: 0.7838497161865234
Epoch 400, training loss: 0.07181873172521591 = 0.06505268812179565 + 0.001 * 6.766044616699219
Epoch 400, val loss: 0.7977680563926697
Epoch 410, training loss: 0.06533388793468475 = 0.05856935307383537 + 0.001 * 6.764537811279297
Epoch 410, val loss: 0.812252402305603
Epoch 420, training loss: 0.05964481830596924 = 0.05288153886795044 + 0.001 * 6.763278961181641
Epoch 420, val loss: 0.8271373510360718
Epoch 430, training loss: 0.05463818460702896 = 0.04787176847457886 + 0.001 * 6.766414642333984
Epoch 430, val loss: 0.842340350151062
Epoch 440, training loss: 0.05021016672253609 = 0.04344668984413147 + 0.001 * 6.7634758949279785
Epoch 440, val loss: 0.8577145338058472
Epoch 450, training loss: 0.04628954455256462 = 0.03953034058213234 + 0.001 * 6.759202480316162
Epoch 450, val loss: 0.8731211423873901
Epoch 460, training loss: 0.042816177010536194 = 0.03605831041932106 + 0.001 * 6.757866382598877
Epoch 460, val loss: 0.8883569836616516
Epoch 470, training loss: 0.03973222151398659 = 0.0329764150083065 + 0.001 * 6.755805969238281
Epoch 470, val loss: 0.9034323692321777
Epoch 480, training loss: 0.03699344024062157 = 0.03023763746023178 + 0.001 * 6.755801677703857
Epoch 480, val loss: 0.9182058572769165
Epoch 490, training loss: 0.03455261513590813 = 0.027799218893051147 + 0.001 * 6.7533955574035645
Epoch 490, val loss: 0.9326220750808716
Epoch 500, training loss: 0.03237227723002434 = 0.02562151849269867 + 0.001 * 6.750758647918701
Epoch 500, val loss: 0.946666419506073
Epoch 510, training loss: 0.030427318066358566 = 0.023667961359024048 + 0.001 * 6.759355545043945
Epoch 510, val loss: 0.9603437781333923
Epoch 520, training loss: 0.028653990477323532 = 0.021907944232225418 + 0.001 * 6.746046543121338
Epoch 520, val loss: 0.973625898361206
Epoch 530, training loss: 0.027062993496656418 = 0.020318400114774704 + 0.001 * 6.744593620300293
Epoch 530, val loss: 0.9865161180496216
Epoch 540, training loss: 0.025634853169322014 = 0.018881021067500114 + 0.001 * 6.7538323402404785
Epoch 540, val loss: 0.999003529548645
Epoch 550, training loss: 0.0243172999471426 = 0.01757996156811714 + 0.001 * 6.737338542938232
Epoch 550, val loss: 1.011096477508545
Epoch 560, training loss: 0.023139532655477524 = 0.016400819644331932 + 0.001 * 6.738712787628174
Epoch 560, val loss: 1.0228216648101807
Epoch 570, training loss: 0.02207612432539463 = 0.01533083152025938 + 0.001 * 6.7452921867370605
Epoch 570, val loss: 1.0341713428497314
Epoch 580, training loss: 0.021090667694807053 = 0.014358285814523697 + 0.001 * 6.732382297515869
Epoch 580, val loss: 1.0451775789260864
Epoch 590, training loss: 0.020212724804878235 = 0.013472776859998703 + 0.001 * 6.739948272705078
Epoch 590, val loss: 1.055867075920105
Epoch 600, training loss: 0.0193919874727726 = 0.012665082700550556 + 0.001 * 6.72690486907959
Epoch 600, val loss: 1.0662121772766113
Epoch 610, training loss: 0.018658911809325218 = 0.011926974169909954 + 0.001 * 6.731937885284424
Epoch 610, val loss: 1.0762596130371094
Epoch 620, training loss: 0.017984818667173386 = 0.011251186951994896 + 0.001 * 6.7336320877075195
Epoch 620, val loss: 1.0859949588775635
Epoch 630, training loss: 0.017347725108265877 = 0.01063116081058979 + 0.001 * 6.716563701629639
Epoch 630, val loss: 1.0954532623291016
Epoch 640, training loss: 0.016839265823364258 = 0.010061010718345642 + 0.001 * 6.778255462646484
Epoch 640, val loss: 1.1046408414840698
Epoch 650, training loss: 0.016253776848316193 = 0.009536012075841427 + 0.001 * 6.717763900756836
Epoch 650, val loss: 1.113556146621704
Epoch 660, training loss: 0.015769843012094498 = 0.00905199907720089 + 0.001 * 6.717843055725098
Epoch 660, val loss: 1.1221981048583984
Epoch 670, training loss: 0.015310770832002163 = 0.00860485527664423 + 0.001 * 6.705915451049805
Epoch 670, val loss: 1.1305806636810303
Epoch 680, training loss: 0.014913325197994709 = 0.008190942928195 + 0.001 * 6.722382068634033
Epoch 680, val loss: 1.1388206481933594
Epoch 690, training loss: 0.014524683356285095 = 0.007807204034179449 + 0.001 * 6.717479228973389
Epoch 690, val loss: 1.1468055248260498
Epoch 700, training loss: 0.01416013203561306 = 0.007450783159583807 + 0.001 * 6.709348201751709
Epoch 700, val loss: 1.1545854806900024
Epoch 710, training loss: 0.013815922662615776 = 0.0071192248724401 + 0.001 * 6.69669771194458
Epoch 710, val loss: 1.1621736288070679
Epoch 720, training loss: 0.013508819043636322 = 0.006810334045439959 + 0.001 * 6.698485374450684
Epoch 720, val loss: 1.1695657968521118
Epoch 730, training loss: 0.013215392827987671 = 0.006522092502564192 + 0.001 * 6.693299770355225
Epoch 730, val loss: 1.1767640113830566
Epoch 740, training loss: 0.012951791286468506 = 0.006252769380807877 + 0.001 * 6.699021339416504
Epoch 740, val loss: 1.1837860345840454
Epoch 750, training loss: 0.012690391391515732 = 0.00600079819560051 + 0.001 * 6.689593315124512
Epoch 750, val loss: 1.1906341314315796
Epoch 760, training loss: 0.01245207991451025 = 0.00576471583917737 + 0.001 * 6.687363624572754
Epoch 760, val loss: 1.197320580482483
Epoch 770, training loss: 0.012246886268258095 = 0.005543223582208157 + 0.001 * 6.703661918640137
Epoch 770, val loss: 1.2038490772247314
Epoch 780, training loss: 0.012030944228172302 = 0.0053351642563939095 + 0.001 * 6.695779800415039
Epoch 780, val loss: 1.2102082967758179
Epoch 790, training loss: 0.011831806972622871 = 0.0051394738256931305 + 0.001 * 6.692332744598389
Epoch 790, val loss: 1.2164522409439087
Epoch 800, training loss: 0.011643447913229465 = 0.004955189302563667 + 0.001 * 6.688258171081543
Epoch 800, val loss: 1.2225501537322998
Epoch 810, training loss: 0.011471851728856564 = 0.0047814869321882725 + 0.001 * 6.690364360809326
Epoch 810, val loss: 1.2285056114196777
Epoch 820, training loss: 0.011310040950775146 = 0.004617560654878616 + 0.001 * 6.692480564117432
Epoch 820, val loss: 1.2343246936798096
Epoch 830, training loss: 0.011159343644976616 = 0.0044626803137362 + 0.001 * 6.696662425994873
Epoch 830, val loss: 1.240020513534546
Epoch 840, training loss: 0.011008924804627895 = 0.004316236823797226 + 0.001 * 6.692687511444092
Epoch 840, val loss: 1.2456023693084717
Epoch 850, training loss: 0.010883092880249023 = 0.004177616909146309 + 0.001 * 6.705475330352783
Epoch 850, val loss: 1.2510567903518677
Epoch 860, training loss: 0.010741010308265686 = 0.004046253394335508 + 0.001 * 6.694756984710693
Epoch 860, val loss: 1.256400227546692
Epoch 870, training loss: 0.010602481663227081 = 0.0039216927252709866 + 0.001 * 6.680788516998291
Epoch 870, val loss: 1.2616299390792847
Epoch 880, training loss: 0.010484261438250542 = 0.003803436178714037 + 0.001 * 6.680824279785156
Epoch 880, val loss: 1.2667667865753174
Epoch 890, training loss: 0.010366366244852543 = 0.0036911065690219402 + 0.001 * 6.675259590148926
Epoch 890, val loss: 1.27177894115448
Epoch 900, training loss: 0.010255750268697739 = 0.0035842929501086473 + 0.001 * 6.671457290649414
Epoch 900, val loss: 1.2767058610916138
Epoch 910, training loss: 0.01015110407024622 = 0.0034826488699764013 + 0.001 * 6.668455123901367
Epoch 910, val loss: 1.281525731086731
Epoch 920, training loss: 0.010053676553070545 = 0.0033858399838209152 + 0.001 * 6.6678361892700195
Epoch 920, val loss: 1.2862505912780762
Epoch 930, training loss: 0.009957338683307171 = 0.0032935738563537598 + 0.001 * 6.663764476776123
Epoch 930, val loss: 1.2908772230148315
Epoch 940, training loss: 0.009891372174024582 = 0.0032055722549557686 + 0.001 * 6.6857991218566895
Epoch 940, val loss: 1.295430302619934
Epoch 950, training loss: 0.00978446938097477 = 0.0031215681228786707 + 0.001 * 6.662901401519775
Epoch 950, val loss: 1.2998762130737305
Epoch 960, training loss: 0.009715232066810131 = 0.0030413249041885138 + 0.001 * 6.6739068031311035
Epoch 960, val loss: 1.3042457103729248
Epoch 970, training loss: 0.009628488682210445 = 0.002964641200378537 + 0.001 * 6.663846969604492
Epoch 970, val loss: 1.3085217475891113
Epoch 980, training loss: 0.009565094485878944 = 0.002891290234401822 + 0.001 * 6.673803806304932
Epoch 980, val loss: 1.3127262592315674
Epoch 990, training loss: 0.009495561011135578 = 0.0028210729360580444 + 0.001 * 6.674487590789795
Epoch 990, val loss: 1.3168377876281738
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.8561
Flip ASR: 0.8311/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9607627391815186 = 1.9523890018463135 + 0.001 * 8.373786926269531
Epoch 0, val loss: 1.95754873752594
Epoch 10, training loss: 1.9502782821655273 = 1.9419045448303223 + 0.001 * 8.373680114746094
Epoch 10, val loss: 1.9470601081848145
Epoch 20, training loss: 1.9375230073928833 = 1.9291496276855469 + 0.001 * 8.373380661010742
Epoch 20, val loss: 1.9336386919021606
Epoch 30, training loss: 1.919953465461731 = 1.9115806818008423 + 0.001 * 8.372788429260254
Epoch 30, val loss: 1.9145838022232056
Epoch 40, training loss: 1.8945237398147583 = 1.8861522674560547 + 0.001 * 8.371466636657715
Epoch 40, val loss: 1.8870294094085693
Epoch 50, training loss: 1.8586491346359253 = 1.8502813577651978 + 0.001 * 8.367774963378906
Epoch 50, val loss: 1.849324107170105
Epoch 60, training loss: 1.8144543170928955 = 1.8061012029647827 + 0.001 * 8.35313606262207
Epoch 60, val loss: 1.8058598041534424
Epoch 70, training loss: 1.7703171968460083 = 1.7620458602905273 + 0.001 * 8.271380424499512
Epoch 70, val loss: 1.7658476829528809
Epoch 80, training loss: 1.720977544784546 = 1.7131227254867554 + 0.001 * 7.854796409606934
Epoch 80, val loss: 1.7216886281967163
Epoch 90, training loss: 1.6545144319534302 = 1.6469545364379883 + 0.001 * 7.559865951538086
Epoch 90, val loss: 1.6630102396011353
Epoch 100, training loss: 1.5681744813919067 = 1.5608365535736084 + 0.001 * 7.337945461273193
Epoch 100, val loss: 1.5893126726150513
Epoch 110, training loss: 1.4661222696304321 = 1.4589242935180664 + 0.001 * 7.19799280166626
Epoch 110, val loss: 1.5059144496917725
Epoch 120, training loss: 1.3592109680175781 = 1.3520666360855103 + 0.001 * 7.144309043884277
Epoch 120, val loss: 1.420867919921875
Epoch 130, training loss: 1.2522008419036865 = 1.2451297044754028 + 0.001 * 7.071132183074951
Epoch 130, val loss: 1.3386366367340088
Epoch 140, training loss: 1.1461292505264282 = 1.1391233205795288 + 0.001 * 7.005975246429443
Epoch 140, val loss: 1.2590631246566772
Epoch 150, training loss: 1.0424796342849731 = 1.0355168581008911 + 0.001 * 6.962803363800049
Epoch 150, val loss: 1.1813859939575195
Epoch 160, training loss: 0.9437212347984314 = 0.9367960691452026 + 0.001 * 6.92515230178833
Epoch 160, val loss: 1.1073893308639526
Epoch 170, training loss: 0.8525488376617432 = 0.8456510305404663 + 0.001 * 6.897809982299805
Epoch 170, val loss: 1.039689064025879
Epoch 180, training loss: 0.7705298066139221 = 0.7636455297470093 + 0.001 * 6.884276866912842
Epoch 180, val loss: 0.9799465537071228
Epoch 190, training loss: 0.6977695226669312 = 0.6908911466598511 + 0.001 * 6.878380298614502
Epoch 190, val loss: 0.9284452795982361
Epoch 200, training loss: 0.6333931684494019 = 0.6265174746513367 + 0.001 * 6.875699996948242
Epoch 200, val loss: 0.8854727745056152
Epoch 210, training loss: 0.5760784149169922 = 0.5692047476768494 + 0.001 * 6.873696327209473
Epoch 210, val loss: 0.8508008122444153
Epoch 220, training loss: 0.5245339274406433 = 0.5176626443862915 + 0.001 * 6.871307849884033
Epoch 220, val loss: 0.8235318660736084
Epoch 230, training loss: 0.4774203300476074 = 0.47055259346961975 + 0.001 * 6.8677287101745605
Epoch 230, val loss: 0.8021387457847595
Epoch 240, training loss: 0.433332234621048 = 0.42646974325180054 + 0.001 * 6.862494468688965
Epoch 240, val loss: 0.7854523658752441
Epoch 250, training loss: 0.3910004794597626 = 0.38414517045021057 + 0.001 * 6.855316162109375
Epoch 250, val loss: 0.772441029548645
Epoch 260, training loss: 0.34970808029174805 = 0.34286218881607056 + 0.001 * 6.84589147567749
Epoch 260, val loss: 0.7623908519744873
Epoch 270, training loss: 0.30956780910491943 = 0.3027307987213135 + 0.001 * 6.837013244628906
Epoch 270, val loss: 0.7551894187927246
Epoch 280, training loss: 0.2714162766933441 = 0.2645913362503052 + 0.001 * 6.824949264526367
Epoch 280, val loss: 0.750933825969696
Epoch 290, training loss: 0.23628805577754974 = 0.2294752448797226 + 0.001 * 6.81281042098999
Epoch 290, val loss: 0.7497580051422119
Epoch 300, training loss: 0.2047637701034546 = 0.19796119630336761 + 0.001 * 6.802579402923584
Epoch 300, val loss: 0.751430869102478
Epoch 310, training loss: 0.17712140083312988 = 0.17032822966575623 + 0.001 * 6.793177604675293
Epoch 310, val loss: 0.75584477186203
Epoch 320, training loss: 0.1534319967031479 = 0.14663037657737732 + 0.001 * 6.801613807678223
Epoch 320, val loss: 0.763039231300354
Epoch 330, training loss: 0.13339722156524658 = 0.1266106814146042 + 0.001 * 6.786543846130371
Epoch 330, val loss: 0.7726855278015137
Epoch 340, training loss: 0.11649836599826813 = 0.10971366614103317 + 0.001 * 6.784697532653809
Epoch 340, val loss: 0.7842705845832825
Epoch 350, training loss: 0.10227406769990921 = 0.09549221396446228 + 0.001 * 6.781856536865234
Epoch 350, val loss: 0.7976023554801941
Epoch 360, training loss: 0.09028083086013794 = 0.08350032567977905 + 0.001 * 6.780506610870361
Epoch 360, val loss: 0.8121640682220459
Epoch 370, training loss: 0.08011779189109802 = 0.07333778589963913 + 0.001 * 6.780006408691406
Epoch 370, val loss: 0.8274428844451904
Epoch 380, training loss: 0.07148463279008865 = 0.0647047758102417 + 0.001 * 6.779853820800781
Epoch 380, val loss: 0.8431990146636963
Epoch 390, training loss: 0.06414523720741272 = 0.057365451008081436 + 0.001 * 6.779783248901367
Epoch 390, val loss: 0.8591906428337097
Epoch 400, training loss: 0.05788174644112587 = 0.05110207945108414 + 0.001 * 6.779666900634766
Epoch 400, val loss: 0.8754443526268005
Epoch 410, training loss: 0.052513666450977325 = 0.045734163373708725 + 0.001 * 6.779503345489502
Epoch 410, val loss: 0.8916966319084167
Epoch 420, training loss: 0.04787788167595863 = 0.041098590940237045 + 0.001 * 6.779291152954102
Epoch 420, val loss: 0.9078720211982727
Epoch 430, training loss: 0.043866049498319626 = 0.037087004631757736 + 0.001 * 6.779043197631836
Epoch 430, val loss: 0.9239395260810852
Epoch 440, training loss: 0.040380656719207764 = 0.03360190615057945 + 0.001 * 6.778749465942383
Epoch 440, val loss: 0.9397361874580383
Epoch 450, training loss: 0.03733713552355766 = 0.030558757483959198 + 0.001 * 6.778378963470459
Epoch 450, val loss: 0.9551993012428284
Epoch 460, training loss: 0.03466961532831192 = 0.027891691774129868 + 0.001 * 6.777921199798584
Epoch 460, val loss: 0.9703720808029175
Epoch 470, training loss: 0.03232498839497566 = 0.025547603145241737 + 0.001 * 6.777385234832764
Epoch 470, val loss: 0.9852205514907837
Epoch 480, training loss: 0.0302591435611248 = 0.023482371121644974 + 0.001 * 6.776771545410156
Epoch 480, val loss: 0.999646008014679
Epoch 490, training loss: 0.028433717787265778 = 0.02165231481194496 + 0.001 * 6.781402587890625
Epoch 490, val loss: 1.0136514902114868
Epoch 500, training loss: 0.026804719120264053 = 0.02002539113163948 + 0.001 * 6.779327392578125
Epoch 500, val loss: 1.0272932052612305
Epoch 510, training loss: 0.025350375100970268 = 0.018574029207229614 + 0.001 * 6.776345729827881
Epoch 510, val loss: 1.0404802560806274
Epoch 520, training loss: 0.02404816262423992 = 0.017273616045713425 + 0.001 * 6.774546146392822
Epoch 520, val loss: 1.0533729791641235
Epoch 530, training loss: 0.022874802350997925 = 0.016100963577628136 + 0.001 * 6.773838520050049
Epoch 530, val loss: 1.0658549070358276
Epoch 540, training loss: 0.021808959543704987 = 0.01503597293049097 + 0.001 * 6.77298641204834
Epoch 540, val loss: 1.0780397653579712
Epoch 550, training loss: 0.020836343988776207 = 0.014064342714846134 + 0.001 * 6.772001266479492
Epoch 550, val loss: 1.0899691581726074
Epoch 560, training loss: 0.019947487860918045 = 0.013176425360143185 + 0.001 * 6.77106237411499
Epoch 560, val loss: 1.1016772985458374
Epoch 570, training loss: 0.01913580298423767 = 0.012364490889012814 + 0.001 * 6.771312236785889
Epoch 570, val loss: 1.113083004951477
Epoch 580, training loss: 0.018393969163298607 = 0.011621777899563313 + 0.001 * 6.772191524505615
Epoch 580, val loss: 1.124230146408081
Epoch 590, training loss: 0.017710819840431213 = 0.010941877029836178 + 0.001 * 6.768941402435303
Epoch 590, val loss: 1.1351426839828491
Epoch 600, training loss: 0.01708635501563549 = 0.010318814776837826 + 0.001 * 6.7675395011901855
Epoch 600, val loss: 1.1457916498184204
Epoch 610, training loss: 0.016518842428922653 = 0.009747142903506756 + 0.001 * 6.771698951721191
Epoch 610, val loss: 1.15618097782135
Epoch 620, training loss: 0.015990450978279114 = 0.009221838787198067 + 0.001 * 6.768612384796143
Epoch 620, val loss: 1.1663392782211304
Epoch 630, training loss: 0.015502440743148327 = 0.008738511241972446 + 0.001 * 6.76392936706543
Epoch 630, val loss: 1.1762703657150269
Epoch 640, training loss: 0.01505596749484539 = 0.008293072693049908 + 0.001 * 6.762894630432129
Epoch 640, val loss: 1.1859685182571411
Epoch 650, training loss: 0.014653142541646957 = 0.007881822995841503 + 0.001 * 6.771319389343262
Epoch 650, val loss: 1.1954190731048584
Epoch 660, training loss: 0.014265071600675583 = 0.007501513231545687 + 0.001 * 6.7635579109191895
Epoch 660, val loss: 1.2046571969985962
Epoch 670, training loss: 0.01391245424747467 = 0.00714920274913311 + 0.001 * 6.763251781463623
Epoch 670, val loss: 1.2136955261230469
Epoch 680, training loss: 0.01358381099998951 = 0.006822289898991585 + 0.001 * 6.761520862579346
Epoch 680, val loss: 1.2224963903427124
Epoch 690, training loss: 0.013276120647788048 = 0.006518473383039236 + 0.001 * 6.757646560668945
Epoch 690, val loss: 1.231124758720398
Epoch 700, training loss: 0.012996483594179153 = 0.006235641427338123 + 0.001 * 6.7608418464660645
Epoch 700, val loss: 1.2395657300949097
Epoch 710, training loss: 0.01273147203028202 = 0.005971964914351702 + 0.001 * 6.759506702423096
Epoch 710, val loss: 1.2478255033493042
Epoch 720, training loss: 0.012481361627578735 = 0.005725765135139227 + 0.001 * 6.755595684051514
Epoch 720, val loss: 1.2558929920196533
Epoch 730, training loss: 0.01225338689982891 = 0.005495547782629728 + 0.001 * 6.757838249206543
Epoch 730, val loss: 1.263775110244751
Epoch 740, training loss: 0.012031356804072857 = 0.005279994569718838 + 0.001 * 6.751361846923828
Epoch 740, val loss: 1.2714956998825073
Epoch 750, training loss: 0.011828329414129257 = 0.00507787661626935 + 0.001 * 6.750452995300293
Epoch 750, val loss: 1.2790406942367554
Epoch 760, training loss: 0.011650077998638153 = 0.004888175055384636 + 0.001 * 6.761903285980225
Epoch 760, val loss: 1.2864478826522827
Epoch 770, training loss: 0.01146330963820219 = 0.004709874279797077 + 0.001 * 6.753435134887695
Epoch 770, val loss: 1.2936879396438599
Epoch 780, training loss: 0.011291487142443657 = 0.004542064853012562 + 0.001 * 6.7494215965271
Epoch 780, val loss: 1.300790786743164
Epoch 790, training loss: 0.011132480576634407 = 0.004383963067084551 + 0.001 * 6.74851655960083
Epoch 790, val loss: 1.3077528476715088
Epoch 800, training loss: 0.0109797902405262 = 0.004234836902469397 + 0.001 * 6.744953632354736
Epoch 800, val loss: 1.3145625591278076
Epoch 810, training loss: 0.010842847637832165 = 0.004093996714800596 + 0.001 * 6.7488508224487305
Epoch 810, val loss: 1.3212504386901855
Epoch 820, training loss: 0.010717758908867836 = 0.003960858099162579 + 0.001 * 6.756900310516357
Epoch 820, val loss: 1.3278007507324219
Epoch 830, training loss: 0.01058103609830141 = 0.0038348764646798372 + 0.001 * 6.746159553527832
Epoch 830, val loss: 1.3342281579971313
Epoch 840, training loss: 0.0104628661647439 = 0.003715531202033162 + 0.001 * 6.747334957122803
Epoch 840, val loss: 1.3405309915542603
Epoch 850, training loss: 0.0103456424549222 = 0.0036023417487740517 + 0.001 * 6.743300437927246
Epoch 850, val loss: 1.3467072248458862
Epoch 860, training loss: 0.01023964025080204 = 0.003494822885841131 + 0.001 * 6.74481725692749
Epoch 860, val loss: 1.3527848720550537
Epoch 870, training loss: 0.010130373761057854 = 0.0033923564478754997 + 0.001 * 6.7380170822143555
Epoch 870, val loss: 1.3587757349014282
Epoch 880, training loss: 0.010039526969194412 = 0.003294109832495451 + 0.001 * 6.74541711807251
Epoch 880, val loss: 1.3647420406341553
Epoch 890, training loss: 0.00994172878563404 = 0.003199090017005801 + 0.001 * 6.742638111114502
Epoch 890, val loss: 1.3707863092422485
Epoch 900, training loss: 0.00985049456357956 = 0.0031066741794347763 + 0.001 * 6.743819713592529
Epoch 900, val loss: 1.3769563436508179
Epoch 910, training loss: 0.009750258177518845 = 0.003016581991687417 + 0.001 * 6.733675956726074
Epoch 910, val loss: 1.3831894397735596
Epoch 920, training loss: 0.009662576019763947 = 0.00292877946048975 + 0.001 * 6.7337965965271
Epoch 920, val loss: 1.389551043510437
Epoch 930, training loss: 0.00958030391484499 = 0.002843457041308284 + 0.001 * 6.736846923828125
Epoch 930, val loss: 1.3960013389587402
Epoch 940, training loss: 0.009501686319708824 = 0.002760796807706356 + 0.001 * 6.740888595581055
Epoch 940, val loss: 1.4025287628173828
Epoch 950, training loss: 0.009410589933395386 = 0.0026810485869646072 + 0.001 * 6.729541301727295
Epoch 950, val loss: 1.4091004133224487
Epoch 960, training loss: 0.009352842345833778 = 0.0026043266989290714 + 0.001 * 6.748515605926514
Epoch 960, val loss: 1.4156723022460938
Epoch 970, training loss: 0.009266884997487068 = 0.002530799712985754 + 0.001 * 6.736085414886475
Epoch 970, val loss: 1.4222105741500854
Epoch 980, training loss: 0.009194176644086838 = 0.0024604054633527994 + 0.001 * 6.733770847320557
Epoch 980, val loss: 1.4287039041519165
Epoch 990, training loss: 0.009120280854403973 = 0.0023931630421429873 + 0.001 * 6.727117538452148
Epoch 990, val loss: 1.4351533651351929
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6494
Flip ASR: 0.6044/225 nodes
The final ASR:0.71095, 0.10303, Accuracy:0.79136, 0.01222
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11670])
remove edge: torch.Size([2, 9560])
updated graph: torch.Size([2, 10674])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.83086, 0.00175
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9478801488876343 = 1.9395062923431396 + 0.001 * 8.373859405517578
Epoch 0, val loss: 1.9314498901367188
Epoch 10, training loss: 1.9374828338623047 = 1.9291090965270996 + 0.001 * 8.37378978729248
Epoch 10, val loss: 1.921342372894287
Epoch 20, training loss: 1.9242768287658691 = 1.9159032106399536 + 0.001 * 8.373597145080566
Epoch 20, val loss: 1.908442497253418
Epoch 30, training loss: 1.9054033756256104 = 1.8970301151275635 + 0.001 * 8.373234748840332
Epoch 30, val loss: 1.8901244401931763
Epoch 40, training loss: 1.877705693244934 = 1.8693331480026245 + 0.001 * 8.372544288635254
Epoch 40, val loss: 1.8640097379684448
Epoch 50, training loss: 1.8402074575424194 = 1.831836462020874 + 0.001 * 8.370994567871094
Epoch 50, val loss: 1.8309355974197388
Epoch 60, training loss: 1.798972249031067 = 1.7906062602996826 + 0.001 * 8.365983009338379
Epoch 60, val loss: 1.798750877380371
Epoch 70, training loss: 1.7579760551452637 = 1.7496371269226074 + 0.001 * 8.338906288146973
Epoch 70, val loss: 1.7671334743499756
Epoch 80, training loss: 1.7030516862869263 = 1.6949316263198853 + 0.001 * 8.120068550109863
Epoch 80, val loss: 1.7190046310424805
Epoch 90, training loss: 1.6284935474395752 = 1.620687484741211 + 0.001 * 7.8060102462768555
Epoch 90, val loss: 1.6544244289398193
Epoch 100, training loss: 1.5365275144577026 = 1.5288089513778687 + 0.001 * 7.718531608581543
Epoch 100, val loss: 1.5785936117172241
Epoch 110, training loss: 1.4379853010177612 = 1.430418848991394 + 0.001 * 7.566439628601074
Epoch 110, val loss: 1.4995461702346802
Epoch 120, training loss: 1.3397976160049438 = 1.3323951959609985 + 0.001 * 7.402458667755127
Epoch 120, val loss: 1.4239884614944458
Epoch 130, training loss: 1.2448230981826782 = 1.2374356985092163 + 0.001 * 7.387416362762451
Epoch 130, val loss: 1.352926254272461
Epoch 140, training loss: 1.1545162200927734 = 1.147183895111084 + 0.001 * 7.332269668579102
Epoch 140, val loss: 1.2874062061309814
Epoch 150, training loss: 1.0711696147918701 = 1.0638964176177979 + 0.001 * 7.273166179656982
Epoch 150, val loss: 1.2291724681854248
Epoch 160, training loss: 0.9957568049430847 = 0.9885537624359131 + 0.001 * 7.203047275543213
Epoch 160, val loss: 1.1781469583511353
Epoch 170, training loss: 0.9263796806335449 = 0.9192201495170593 + 0.001 * 7.159548759460449
Epoch 170, val loss: 1.1325476169586182
Epoch 180, training loss: 0.8590936660766602 = 0.8519430756568909 + 0.001 * 7.1505656242370605
Epoch 180, val loss: 1.0884736776351929
Epoch 190, training loss: 0.7901409864425659 = 0.7829961180686951 + 0.001 * 7.144883632659912
Epoch 190, val loss: 1.0430623292922974
Epoch 200, training loss: 0.7184052467346191 = 0.7112669348716736 + 0.001 * 7.138308525085449
Epoch 200, val loss: 0.9951514601707458
Epoch 210, training loss: 0.6460618376731873 = 0.6389292478561401 + 0.001 * 7.132613182067871
Epoch 210, val loss: 0.9474749565124512
Epoch 220, training loss: 0.577244758605957 = 0.5701188445091248 + 0.001 * 7.12589693069458
Epoch 220, val loss: 0.9038146734237671
Epoch 230, training loss: 0.5152195692062378 = 0.5081033706665039 + 0.001 * 7.116213798522949
Epoch 230, val loss: 0.8683531284332275
Epoch 240, training loss: 0.460284948348999 = 0.45318448543548584 + 0.001 * 7.100467681884766
Epoch 240, val loss: 0.8424802422523499
Epoch 250, training loss: 0.41034066677093506 = 0.4032595157623291 + 0.001 * 7.081142425537109
Epoch 250, val loss: 0.82453852891922
Epoch 260, training loss: 0.36294832825660706 = 0.35589468479156494 + 0.001 * 7.053647518157959
Epoch 260, val loss: 0.8119563460350037
Epoch 270, training loss: 0.3169524669647217 = 0.30992433428764343 + 0.001 * 7.028139591217041
Epoch 270, val loss: 0.8028408288955688
Epoch 280, training loss: 0.27291139960289 = 0.2658955752849579 + 0.001 * 7.015828609466553
Epoch 280, val loss: 0.7969105839729309
Epoch 290, training loss: 0.2325328290462494 = 0.22553297877311707 + 0.001 * 6.999849319458008
Epoch 290, val loss: 0.7947373390197754
Epoch 300, training loss: 0.19747279584407806 = 0.19047866761684418 + 0.001 * 6.994132041931152
Epoch 300, val loss: 0.7966522574424744
Epoch 310, training loss: 0.1683913618326187 = 0.161400705575943 + 0.001 * 6.990655899047852
Epoch 310, val loss: 0.8025268316268921
Epoch 320, training loss: 0.14484186470508575 = 0.1378529816865921 + 0.001 * 6.988889217376709
Epoch 320, val loss: 0.8119043707847595
Epoch 330, training loss: 0.12581585347652435 = 0.11882767826318741 + 0.001 * 6.988176345825195
Epoch 330, val loss: 0.8241172432899475
Epoch 340, training loss: 0.11027807742357254 = 0.10329056531190872 + 0.001 * 6.987514972686768
Epoch 340, val loss: 0.8384296894073486
Epoch 350, training loss: 0.09741109609603882 = 0.09042666852474213 + 0.001 * 6.98443078994751
Epoch 350, val loss: 0.8543482422828674
Epoch 360, training loss: 0.0866318941116333 = 0.07963676750659943 + 0.001 * 6.995125770568848
Epoch 360, val loss: 0.8714569211006165
Epoch 370, training loss: 0.07748088985681534 = 0.07049442827701569 + 0.001 * 6.986460208892822
Epoch 370, val loss: 0.8894277215003967
Epoch 380, training loss: 0.06966987252235413 = 0.06268776953220367 + 0.001 * 6.982105255126953
Epoch 380, val loss: 0.9079784750938416
Epoch 390, training loss: 0.06296044588088989 = 0.05598117783665657 + 0.001 * 6.979269027709961
Epoch 390, val loss: 0.9268730878829956
Epoch 400, training loss: 0.057170093059539795 = 0.05019097030162811 + 0.001 * 6.979122161865234
Epoch 400, val loss: 0.945818305015564
Epoch 410, training loss: 0.05214553698897362 = 0.04517006129026413 + 0.001 * 6.975474834442139
Epoch 410, val loss: 0.9646878838539124
Epoch 420, training loss: 0.04776901379227638 = 0.040798794478178024 + 0.001 * 6.970219135284424
Epoch 420, val loss: 0.9833707213401794
Epoch 430, training loss: 0.04394904524087906 = 0.0369798019528389 + 0.001 * 6.969243049621582
Epoch 430, val loss: 1.001734972000122
Epoch 440, training loss: 0.04059514030814171 = 0.03363155201077461 + 0.001 * 6.963588714599609
Epoch 440, val loss: 1.019721508026123
Epoch 450, training loss: 0.03764554113149643 = 0.030685357749462128 + 0.001 * 6.9601826667785645
Epoch 450, val loss: 1.0372904539108276
Epoch 460, training loss: 0.03503468632698059 = 0.02808377891778946 + 0.001 * 6.950908660888672
Epoch 460, val loss: 1.0543947219848633
Epoch 470, training loss: 0.03273528441786766 = 0.025778790935873985 + 0.001 * 6.9564924240112305
Epoch 470, val loss: 1.0711110830307007
Epoch 480, training loss: 0.03067188709974289 = 0.02373019978404045 + 0.001 * 6.941687107086182
Epoch 480, val loss: 1.087365746498108
Epoch 490, training loss: 0.028870344161987305 = 0.02190372534096241 + 0.001 * 6.966618537902832
Epoch 490, val loss: 1.1031852960586548
Epoch 500, training loss: 0.027206260710954666 = 0.020269865170121193 + 0.001 * 6.936395645141602
Epoch 500, val loss: 1.118584156036377
Epoch 510, training loss: 0.02572042867541313 = 0.01880423165857792 + 0.001 * 6.916196823120117
Epoch 510, val loss: 1.1335417032241821
Epoch 520, training loss: 0.02442178875207901 = 0.01748582161962986 + 0.001 * 6.9359660148620605
Epoch 520, val loss: 1.1480854749679565
Epoch 530, training loss: 0.023202620446681976 = 0.016296889632940292 + 0.001 * 6.905730724334717
Epoch 530, val loss: 1.1622892618179321
Epoch 540, training loss: 0.022118639200925827 = 0.015222134068608284 + 0.001 * 6.896504878997803
Epoch 540, val loss: 1.1760870218276978
Epoch 550, training loss: 0.02114042267203331 = 0.014248115010559559 + 0.001 * 6.892307281494141
Epoch 550, val loss: 1.1894584894180298
Epoch 560, training loss: 0.020263083279132843 = 0.01336309127509594 + 0.001 * 6.899991035461426
Epoch 560, val loss: 1.2025129795074463
Epoch 570, training loss: 0.019450068473815918 = 0.012556279078125954 + 0.001 * 6.893789291381836
Epoch 570, val loss: 1.2152074575424194
Epoch 580, training loss: 0.01871507614850998 = 0.011816776357591152 + 0.001 * 6.898298740386963
Epoch 580, val loss: 1.2276452779769897
Epoch 590, training loss: 0.01802239380776882 = 0.011135000735521317 + 0.001 * 6.887392520904541
Epoch 590, val loss: 1.2398372888565063
Epoch 600, training loss: 0.01741120219230652 = 0.010504424571990967 + 0.001 * 6.906777858734131
Epoch 600, val loss: 1.2517938613891602
Epoch 610, training loss: 0.016807295382022858 = 0.009920733980834484 + 0.001 * 6.886561870574951
Epoch 610, val loss: 1.263540267944336
Epoch 620, training loss: 0.01626245677471161 = 0.009380398318171501 + 0.001 * 6.882058143615723
Epoch 620, val loss: 1.2750840187072754
Epoch 630, training loss: 0.015760580077767372 = 0.008880220353603363 + 0.001 * 6.880359649658203
Epoch 630, val loss: 1.2864024639129639
Epoch 640, training loss: 0.015317962504923344 = 0.008417169563472271 + 0.001 * 6.900792598724365
Epoch 640, val loss: 1.297456979751587
Epoch 650, training loss: 0.014855212531983852 = 0.00798853300511837 + 0.001 * 6.8666791915893555
Epoch 650, val loss: 1.3083182573318481
Epoch 660, training loss: 0.0144602470099926 = 0.007591375149786472 + 0.001 * 6.868872165679932
Epoch 660, val loss: 1.3188923597335815
Epoch 670, training loss: 0.01408749632537365 = 0.007223054300993681 + 0.001 * 6.864441394805908
Epoch 670, val loss: 1.329249382019043
Epoch 680, training loss: 0.013741975650191307 = 0.006881205830723047 + 0.001 * 6.860769271850586
Epoch 680, val loss: 1.3393677473068237
Epoch 690, training loss: 0.01342040952295065 = 0.006563533563166857 + 0.001 * 6.856875419616699
Epoch 690, val loss: 1.349228024482727
Epoch 700, training loss: 0.013130067847669125 = 0.006268016528338194 + 0.001 * 6.862051010131836
Epoch 700, val loss: 1.3588894605636597
Epoch 710, training loss: 0.012850366532802582 = 0.005992847494781017 + 0.001 * 6.857518196105957
Epoch 710, val loss: 1.3683346509933472
Epoch 720, training loss: 0.012588391080498695 = 0.005736295133829117 + 0.001 * 6.852095127105713
Epoch 720, val loss: 1.377533197402954
Epoch 730, training loss: 0.012347044423222542 = 0.0054967524483799934 + 0.001 * 6.8502912521362305
Epoch 730, val loss: 1.3865193128585815
Epoch 740, training loss: 0.012116290628910065 = 0.005272848065942526 + 0.001 * 6.843442916870117
Epoch 740, val loss: 1.395301103591919
Epoch 750, training loss: 0.011937430128455162 = 0.0050632837228477 + 0.001 * 6.874146461486816
Epoch 750, val loss: 1.403852939605713
Epoch 760, training loss: 0.01170635037124157 = 0.004866935312747955 + 0.001 * 6.839415073394775
Epoch 760, val loss: 1.4122096300125122
Epoch 770, training loss: 0.011520490050315857 = 0.004682696890085936 + 0.001 * 6.837792873382568
Epoch 770, val loss: 1.4203706979751587
Epoch 780, training loss: 0.011358290910720825 = 0.00450967438519001 + 0.001 * 6.848616123199463
Epoch 780, val loss: 1.4283709526062012
Epoch 790, training loss: 0.011178931221365929 = 0.0043469807133078575 + 0.001 * 6.831950664520264
Epoch 790, val loss: 1.4361580610275269
Epoch 800, training loss: 0.011025192216038704 = 0.0041938466019928455 + 0.001 * 6.831345081329346
Epoch 800, val loss: 1.4437434673309326
Epoch 810, training loss: 0.010885579511523247 = 0.004049533512443304 + 0.001 * 6.836045265197754
Epoch 810, val loss: 1.4512194395065308
Epoch 820, training loss: 0.010737210512161255 = 0.003913360182195902 + 0.001 * 6.823850154876709
Epoch 820, val loss: 1.4584643840789795
Epoch 830, training loss: 0.010607232339680195 = 0.0037847626954317093 + 0.001 * 6.822469234466553
Epoch 830, val loss: 1.4655702114105225
Epoch 840, training loss: 0.01048413384705782 = 0.00366318435408175 + 0.001 * 6.820949554443359
Epoch 840, val loss: 1.472514033317566
Epoch 850, training loss: 0.010377040132880211 = 0.003548139939084649 + 0.001 * 6.82889986038208
Epoch 850, val loss: 1.479334831237793
Epoch 860, training loss: 0.010272171348333359 = 0.003439153078943491 + 0.001 * 6.833017826080322
Epoch 860, val loss: 1.485952377319336
Epoch 870, training loss: 0.01016078982502222 = 0.003335775574669242 + 0.001 * 6.825013637542725
Epoch 870, val loss: 1.4925079345703125
Epoch 880, training loss: 0.010064354166388512 = 0.0032376693561673164 + 0.001 * 6.82668399810791
Epoch 880, val loss: 1.4988478422164917
Epoch 890, training loss: 0.009968641214072704 = 0.0031444907654076815 + 0.001 * 6.824150085449219
Epoch 890, val loss: 1.5051064491271973
Epoch 900, training loss: 0.009873014874756336 = 0.00305589335039258 + 0.001 * 6.8171210289001465
Epoch 900, val loss: 1.5111724138259888
Epoch 910, training loss: 0.009774476289749146 = 0.0029715460259467363 + 0.001 * 6.802929878234863
Epoch 910, val loss: 1.5171834230422974
Epoch 920, training loss: 0.009701576083898544 = 0.002891191514208913 + 0.001 * 6.8103837966918945
Epoch 920, val loss: 1.5230332612991333
Epoch 930, training loss: 0.009616445749998093 = 0.002814582781866193 + 0.001 * 6.801862716674805
Epoch 930, val loss: 1.5288150310516357
Epoch 940, training loss: 0.009548739530146122 = 0.0027414862997829914 + 0.001 * 6.807252883911133
Epoch 940, val loss: 1.5344040393829346
Epoch 950, training loss: 0.009469534270465374 = 0.002671692054718733 + 0.001 * 6.797842025756836
Epoch 950, val loss: 1.5400047302246094
Epoch 960, training loss: 0.009403383359313011 = 0.0026049523148685694 + 0.001 * 6.798430919647217
Epoch 960, val loss: 1.5454187393188477
Epoch 970, training loss: 0.009342605248093605 = 0.002541070571169257 + 0.001 * 6.801534652709961
Epoch 970, val loss: 1.5507597923278809
Epoch 980, training loss: 0.009272892959415913 = 0.0024798952508717775 + 0.001 * 6.792997360229492
Epoch 980, val loss: 1.5559842586517334
Epoch 990, training loss: 0.009222429245710373 = 0.0024212540592998266 + 0.001 * 6.801175117492676
Epoch 990, val loss: 1.5611437559127808
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.6347
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9538140296936035 = 1.9454401731491089 + 0.001 * 8.373847961425781
Epoch 0, val loss: 1.940164566040039
Epoch 10, training loss: 1.9440480470657349 = 1.9356743097305298 + 0.001 * 8.37374496459961
Epoch 10, val loss: 1.93040931224823
Epoch 20, training loss: 1.9314779043197632 = 1.9231044054031372 + 0.001 * 8.373482704162598
Epoch 20, val loss: 1.9176628589630127
Epoch 30, training loss: 1.9133062362670898 = 1.904933214187622 + 0.001 * 8.372983932495117
Epoch 30, val loss: 1.8991118669509888
Epoch 40, training loss: 1.8860657215118408 = 1.877693772315979 + 0.001 * 8.371977806091309
Epoch 40, val loss: 1.8717178106307983
Epoch 50, training loss: 1.8474948406219482 = 1.8391252756118774 + 0.001 * 8.369559288024902
Epoch 50, val loss: 1.834499716758728
Epoch 60, training loss: 1.8022377490997314 = 1.7938765287399292 + 0.001 * 8.361244201660156
Epoch 60, val loss: 1.7943570613861084
Epoch 70, training loss: 1.7581231594085693 = 1.7498074769973755 + 0.001 * 8.315717697143555
Epoch 70, val loss: 1.757974624633789
Epoch 80, training loss: 1.7037651538848877 = 1.6957706212997437 + 0.001 * 7.994566917419434
Epoch 80, val loss: 1.7112590074539185
Epoch 90, training loss: 1.6304330825805664 = 1.6227843761444092 + 0.001 * 7.64869499206543
Epoch 90, val loss: 1.6472854614257812
Epoch 100, training loss: 1.5366613864898682 = 1.5292601585388184 + 0.001 * 7.401186466217041
Epoch 100, val loss: 1.5680928230285645
Epoch 110, training loss: 1.4288240671157837 = 1.4215375185012817 + 0.001 * 7.286598205566406
Epoch 110, val loss: 1.4794672727584839
Epoch 120, training loss: 1.315804123878479 = 1.3085936307907104 + 0.001 * 7.210541725158691
Epoch 120, val loss: 1.387838363647461
Epoch 130, training loss: 1.2038488388061523 = 1.196687936782837 + 0.001 * 7.160926818847656
Epoch 130, val loss: 1.2990694046020508
Epoch 140, training loss: 1.096725344657898 = 1.0895781517028809 + 0.001 * 7.147227764129639
Epoch 140, val loss: 1.2150861024856567
Epoch 150, training loss: 0.9973292350769043 = 0.9901884198188782 + 0.001 * 7.14082670211792
Epoch 150, val loss: 1.137982964515686
Epoch 160, training loss: 0.9074462056159973 = 0.9003117084503174 + 0.001 * 7.134483814239502
Epoch 160, val loss: 1.069812536239624
Epoch 170, training loss: 0.8272486925125122 = 0.8201243877410889 + 0.001 * 7.124276161193848
Epoch 170, val loss: 1.0109597444534302
Epoch 180, training loss: 0.7560281157493591 = 0.7489175796508789 + 0.001 * 7.110527038574219
Epoch 180, val loss: 0.9614637494087219
Epoch 190, training loss: 0.6927793622016907 = 0.6856900453567505 + 0.001 * 7.0893144607543945
Epoch 190, val loss: 0.9206954836845398
Epoch 200, training loss: 0.6364246010780334 = 0.6293689608573914 + 0.001 * 7.055647373199463
Epoch 200, val loss: 0.8877373337745667
Epoch 210, training loss: 0.585972785949707 = 0.5789579153060913 + 0.001 * 7.014871120452881
Epoch 210, val loss: 0.861391544342041
Epoch 220, training loss: 0.5407004952430725 = 0.5337105393409729 + 0.001 * 6.989981651306152
Epoch 220, val loss: 0.8404099345207214
Epoch 230, training loss: 0.49993833899497986 = 0.49296194314956665 + 0.001 * 6.976395606994629
Epoch 230, val loss: 0.8238198161125183
Epoch 240, training loss: 0.4627356231212616 = 0.45576679706573486 + 0.001 * 6.968812942504883
Epoch 240, val loss: 0.8103744983673096
Epoch 250, training loss: 0.4276129901409149 = 0.42064833641052246 + 0.001 * 6.96465539932251
Epoch 250, val loss: 0.7986750602722168
Epoch 260, training loss: 0.3930593729019165 = 0.38609829545021057 + 0.001 * 6.961063385009766
Epoch 260, val loss: 0.7879181504249573
Epoch 270, training loss: 0.35790300369262695 = 0.3509449362754822 + 0.001 * 6.958073139190674
Epoch 270, val loss: 0.7775168418884277
Epoch 280, training loss: 0.3217829763889313 = 0.3148244321346283 + 0.001 * 6.958540439605713
Epoch 280, val loss: 0.7674670219421387
Epoch 290, training loss: 0.28526929020881653 = 0.2783171534538269 + 0.001 * 6.95212984085083
Epoch 290, val loss: 0.7580212354660034
Epoch 300, training loss: 0.24985384941101074 = 0.2429048866033554 + 0.001 * 6.948956489562988
Epoch 300, val loss: 0.7500844597816467
Epoch 310, training loss: 0.21715541183948517 = 0.21020813286304474 + 0.001 * 6.9472784996032715
Epoch 310, val loss: 0.7447556257247925
Epoch 320, training loss: 0.18832582235336304 = 0.18138380348682404 + 0.001 * 6.942019939422607
Epoch 320, val loss: 0.7430515885353088
Epoch 330, training loss: 0.16378074884414673 = 0.1568375825881958 + 0.001 * 6.943164825439453
Epoch 330, val loss: 0.7451857924461365
Epoch 340, training loss: 0.1432286649942398 = 0.13629348576068878 + 0.001 * 6.935177326202393
Epoch 340, val loss: 0.7512248754501343
Epoch 350, training loss: 0.1260705590248108 = 0.11914068460464478 + 0.001 * 6.929879665374756
Epoch 350, val loss: 0.7606495022773743
Epoch 360, training loss: 0.11165723949670792 = 0.10473016649484634 + 0.001 * 6.927074432373047
Epoch 360, val loss: 0.7728421092033386
Epoch 370, training loss: 0.09940160810947418 = 0.09248815476894379 + 0.001 * 6.913449287414551
Epoch 370, val loss: 0.7872230410575867
Epoch 380, training loss: 0.08888977020978928 = 0.08198220282793045 + 0.001 * 6.90756368637085
Epoch 380, val loss: 0.8031142354011536
Epoch 390, training loss: 0.07977435737848282 = 0.07286646962165833 + 0.001 * 6.907884120941162
Epoch 390, val loss: 0.8199371695518494
Epoch 400, training loss: 0.07182851433753967 = 0.0649338960647583 + 0.001 * 6.894618034362793
Epoch 400, val loss: 0.8372470140457153
Epoch 410, training loss: 0.0649363324046135 = 0.058037687093019485 + 0.001 * 6.898642063140869
Epoch 410, val loss: 0.8549497723579407
Epoch 420, training loss: 0.0589187815785408 = 0.05202079191803932 + 0.001 * 6.897989273071289
Epoch 420, val loss: 0.8728250861167908
Epoch 430, training loss: 0.05368225648999214 = 0.04678898677229881 + 0.001 * 6.893270492553711
Epoch 430, val loss: 0.890762448310852
Epoch 440, training loss: 0.049117252230644226 = 0.04222926124930382 + 0.001 * 6.887989521026611
Epoch 440, val loss: 0.908562183380127
Epoch 450, training loss: 0.045131340622901917 = 0.03824210166931152 + 0.001 * 6.889237880706787
Epoch 450, val loss: 0.9262044429779053
Epoch 460, training loss: 0.04163168743252754 = 0.034752923995256424 + 0.001 * 6.878764629364014
Epoch 460, val loss: 0.9435045123100281
Epoch 470, training loss: 0.038568466901779175 = 0.03168916702270508 + 0.001 * 6.879300594329834
Epoch 470, val loss: 0.9605050683021545
Epoch 480, training loss: 0.03586661443114281 = 0.028990117833018303 + 0.001 * 6.8764967918396
Epoch 480, val loss: 0.9771404266357422
Epoch 490, training loss: 0.033478569239377975 = 0.02660338580608368 + 0.001 * 6.875184535980225
Epoch 490, val loss: 0.9933559894561768
Epoch 500, training loss: 0.03135748207569122 = 0.0244834516197443 + 0.001 * 6.8740315437316895
Epoch 500, val loss: 1.0091663599014282
Epoch 510, training loss: 0.02946527674794197 = 0.0225935410708189 + 0.001 * 6.871734619140625
Epoch 510, val loss: 1.0245836973190308
Epoch 520, training loss: 0.027772720903158188 = 0.020905528217554092 + 0.001 * 6.867192268371582
Epoch 520, val loss: 1.039579153060913
Epoch 530, training loss: 0.02625521644949913 = 0.01939423196017742 + 0.001 * 6.860985279083252
Epoch 530, val loss: 1.0541592836380005
Epoch 540, training loss: 0.024904966354370117 = 0.018037304282188416 + 0.001 * 6.86766242980957
Epoch 540, val loss: 1.0683437585830688
Epoch 550, training loss: 0.023682381957769394 = 0.016815127804875374 + 0.001 * 6.86725378036499
Epoch 550, val loss: 1.0821458101272583
Epoch 560, training loss: 0.02256709337234497 = 0.015711089596152306 + 0.001 * 6.8560028076171875
Epoch 560, val loss: 1.0955308675765991
Epoch 570, training loss: 0.021566662937402725 = 0.014710994437336922 + 0.001 * 6.855668544769287
Epoch 570, val loss: 1.108574390411377
Epoch 580, training loss: 0.02069019339978695 = 0.013802768662571907 + 0.001 * 6.887424945831299
Epoch 580, val loss: 1.1212408542633057
Epoch 590, training loss: 0.019831335172057152 = 0.012976196594536304 + 0.001 * 6.855137825012207
Epoch 590, val loss: 1.1335443258285522
Epoch 600, training loss: 0.019064221531152725 = 0.012221905402839184 + 0.001 * 6.842316627502441
Epoch 600, val loss: 1.145520567893982
Epoch 610, training loss: 0.018386369571089745 = 0.011532071977853775 + 0.001 * 6.854296684265137
Epoch 610, val loss: 1.1571612358093262
Epoch 620, training loss: 0.017746470868587494 = 0.010899659246206284 + 0.001 * 6.846810817718506
Epoch 620, val loss: 1.168495774269104
Epoch 630, training loss: 0.017154894769191742 = 0.010318632237613201 + 0.001 * 6.8362627029418945
Epoch 630, val loss: 1.179510474205017
Epoch 640, training loss: 0.01662249118089676 = 0.009783799760043621 + 0.001 * 6.838690280914307
Epoch 640, val loss: 1.1902565956115723
Epoch 650, training loss: 0.0161274466663599 = 0.009290541522204876 + 0.001 * 6.836905479431152
Epoch 650, val loss: 1.2007149457931519
Epoch 660, training loss: 0.01566769927740097 = 0.008834850043058395 + 0.001 * 6.832849502563477
Epoch 660, val loss: 1.2109131813049316
Epoch 670, training loss: 0.015267675742506981 = 0.008413023315370083 + 0.001 * 6.854651927947998
Epoch 670, val loss: 1.2208311557769775
Epoch 680, training loss: 0.0148643022403121 = 0.008021969348192215 + 0.001 * 6.842332363128662
Epoch 680, val loss: 1.2305192947387695
Epoch 690, training loss: 0.0145008135586977 = 0.007658785674721003 + 0.001 * 6.8420281410217285
Epoch 690, val loss: 1.239944577217102
Epoch 700, training loss: 0.014145372435450554 = 0.007320953533053398 + 0.001 * 6.824418067932129
Epoch 700, val loss: 1.2491241693496704
Epoch 710, training loss: 0.013850151561200619 = 0.007006231229752302 + 0.001 * 6.8439202308654785
Epoch 710, val loss: 1.258096694946289
Epoch 720, training loss: 0.013534801080822945 = 0.006712653208523989 + 0.001 * 6.822147369384766
Epoch 720, val loss: 1.2668079137802124
Epoch 730, training loss: 0.013267656788229942 = 0.006438323762267828 + 0.001 * 6.82933235168457
Epoch 730, val loss: 1.275353193283081
Epoch 740, training loss: 0.013033783994615078 = 0.006181593984365463 + 0.001 * 6.852189540863037
Epoch 740, val loss: 1.2836774587631226
Epoch 750, training loss: 0.012755255214869976 = 0.00594102218747139 + 0.001 * 6.81423282623291
Epoch 750, val loss: 1.2918046712875366
Epoch 760, training loss: 0.012530156411230564 = 0.005715274251997471 + 0.001 * 6.814881801605225
Epoch 760, val loss: 1.2997422218322754
Epoch 770, training loss: 0.01231652032583952 = 0.005503190215677023 + 0.001 * 6.813329696655273
Epoch 770, val loss: 1.3074965476989746
Epoch 780, training loss: 0.012119552120566368 = 0.0053036995232105255 + 0.001 * 6.81585168838501
Epoch 780, val loss: 1.3150651454925537
Epoch 790, training loss: 0.01192496158182621 = 0.005115844309329987 + 0.001 * 6.809116840362549
Epoch 790, val loss: 1.3224804401397705
Epoch 800, training loss: 0.011772176250815392 = 0.004938727710396051 + 0.001 * 6.8334479331970215
Epoch 800, val loss: 1.3297384977340698
Epoch 810, training loss: 0.011585621163249016 = 0.004771549254655838 + 0.001 * 6.8140716552734375
Epoch 810, val loss: 1.3368033170700073
Epoch 820, training loss: 0.011418447829782963 = 0.0046135857701301575 + 0.001 * 6.804861545562744
Epoch 820, val loss: 1.3437373638153076
Epoch 830, training loss: 0.011269107460975647 = 0.004464197438210249 + 0.001 * 6.804909706115723
Epoch 830, val loss: 1.3504936695098877
Epoch 840, training loss: 0.011117679066956043 = 0.004322749562561512 + 0.001 * 6.794929027557373
Epoch 840, val loss: 1.3571245670318604
Epoch 850, training loss: 0.011042706668376923 = 0.004188683815300465 + 0.001 * 6.854022026062012
Epoch 850, val loss: 1.3636151552200317
Epoch 860, training loss: 0.010872354730963707 = 0.00406149635091424 + 0.001 * 6.810858726501465
Epoch 860, val loss: 1.3699367046356201
Epoch 870, training loss: 0.010738912969827652 = 0.003940758295357227 + 0.001 * 6.798153877258301
Epoch 870, val loss: 1.3761541843414307
Epoch 880, training loss: 0.010629854165017605 = 0.003826051950454712 + 0.001 * 6.803802013397217
Epoch 880, val loss: 1.3822286128997803
Epoch 890, training loss: 0.01053706370294094 = 0.0037169361021369696 + 0.001 * 6.820127487182617
Epoch 890, val loss: 1.3881738185882568
Epoch 900, training loss: 0.010402752086520195 = 0.0036130950320512056 + 0.001 * 6.789656639099121
Epoch 900, val loss: 1.3940242528915405
Epoch 910, training loss: 0.010341194458305836 = 0.0035141820553690195 + 0.001 * 6.827012538909912
Epoch 910, val loss: 1.3997238874435425
Epoch 920, training loss: 0.010199790820479393 = 0.0034198835492134094 + 0.001 * 6.779906749725342
Epoch 920, val loss: 1.4053515195846558
Epoch 930, training loss: 0.010133919306099415 = 0.003329911967739463 + 0.001 * 6.804006576538086
Epoch 930, val loss: 1.410837173461914
Epoch 940, training loss: 0.010024798102676868 = 0.003244014224037528 + 0.001 * 6.780783176422119
Epoch 940, val loss: 1.4162036180496216
Epoch 950, training loss: 0.009964888915419579 = 0.003161964239552617 + 0.001 * 6.802924633026123
Epoch 950, val loss: 1.4214736223220825
Epoch 960, training loss: 0.009868508204817772 = 0.003083523828536272 + 0.001 * 6.784984111785889
Epoch 960, val loss: 1.4266632795333862
Epoch 970, training loss: 0.00981985591351986 = 0.003008476924151182 + 0.001 * 6.811378479003906
Epoch 970, val loss: 1.431766152381897
Epoch 980, training loss: 0.009723692201077938 = 0.0029366209637373686 + 0.001 * 6.7870707511901855
Epoch 980, val loss: 1.436704158782959
Epoch 990, training loss: 0.009634082205593586 = 0.002867800882086158 + 0.001 * 6.7662811279296875
Epoch 990, val loss: 1.441594123840332
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.6790
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9568054676055908 = 1.9484316110610962 + 0.001 * 8.37388801574707
Epoch 0, val loss: 1.949197769165039
Epoch 10, training loss: 1.9462941884994507 = 1.937920331954956 + 0.001 * 8.373812675476074
Epoch 10, val loss: 1.9388657808303833
Epoch 20, training loss: 1.9330569505691528 = 1.9246833324432373 + 0.001 * 8.373618125915527
Epoch 20, val loss: 1.925338625907898
Epoch 30, training loss: 1.914283037185669 = 1.905909776687622 + 0.001 * 8.373247146606445
Epoch 30, val loss: 1.9056898355484009
Epoch 40, training loss: 1.8864057064056396 = 1.87803316116333 + 0.001 * 8.372520446777344
Epoch 40, val loss: 1.8768723011016846
Epoch 50, training loss: 1.8480626344680786 = 1.8396917581558228 + 0.001 * 8.370853424072266
Epoch 50, val loss: 1.839472770690918
Epoch 60, training loss: 1.8068552017211914 = 1.7984896898269653 + 0.001 * 8.365554809570312
Epoch 60, val loss: 1.8040469884872437
Epoch 70, training loss: 1.7684319019317627 = 1.7600936889648438 + 0.001 * 8.338179588317871
Epoch 70, val loss: 1.7733772993087769
Epoch 80, training loss: 1.7184442281723022 = 1.7103039026260376 + 0.001 * 8.140316009521484
Epoch 80, val loss: 1.7321078777313232
Epoch 90, training loss: 1.6501545906066895 = 1.642269492149353 + 0.001 * 7.885088920593262
Epoch 90, val loss: 1.6768535375595093
Epoch 100, training loss: 1.563909888267517 = 1.5562022924423218 + 0.001 * 7.707610607147217
Epoch 100, val loss: 1.6086037158966064
Epoch 110, training loss: 1.4707467555999756 = 1.463338017463684 + 0.001 * 7.4087815284729
Epoch 110, val loss: 1.5356980562210083
Epoch 120, training loss: 1.3794642686843872 = 1.3721026182174683 + 0.001 * 7.3616461753845215
Epoch 120, val loss: 1.4666078090667725
Epoch 130, training loss: 1.2894545793533325 = 1.2821418046951294 + 0.001 * 7.312763214111328
Epoch 130, val loss: 1.400704264640808
Epoch 140, training loss: 1.1991214752197266 = 1.191855549812317 + 0.001 * 7.265983581542969
Epoch 140, val loss: 1.3357046842575073
Epoch 150, training loss: 1.1093536615371704 = 1.1021510362625122 + 0.001 * 7.202666282653809
Epoch 150, val loss: 1.271790623664856
Epoch 160, training loss: 1.0225919485092163 = 1.0154414176940918 + 0.001 * 7.150489807128906
Epoch 160, val loss: 1.2107149362564087
Epoch 170, training loss: 0.9401071071624756 = 0.9329883456230164 + 0.001 * 7.118741035461426
Epoch 170, val loss: 1.1536258459091187
Epoch 180, training loss: 0.8619560599327087 = 0.8548685908317566 + 0.001 * 7.087465763092041
Epoch 180, val loss: 1.1000776290893555
Epoch 190, training loss: 0.7883470058441162 = 0.781303882598877 + 0.001 * 7.043149948120117
Epoch 190, val loss: 1.050175428390503
Epoch 200, training loss: 0.7198660373687744 = 0.7128656506538391 + 0.001 * 7.00037145614624
Epoch 200, val loss: 1.0048195123672485
Epoch 210, training loss: 0.6564176678657532 = 0.6494502425193787 + 0.001 * 6.967412948608398
Epoch 210, val loss: 0.9634659290313721
Epoch 220, training loss: 0.5968481302261353 = 0.5899085998535156 + 0.001 * 6.93955659866333
Epoch 220, val loss: 0.9260510206222534
Epoch 230, training loss: 0.5399110913276672 = 0.5329809188842773 + 0.001 * 6.930146217346191
Epoch 230, val loss: 0.8929215669631958
Epoch 240, training loss: 0.48543334007263184 = 0.47851747274398804 + 0.001 * 6.915859222412109
Epoch 240, val loss: 0.8654223680496216
Epoch 250, training loss: 0.43424052000045776 = 0.42732709646224976 + 0.001 * 6.913422107696533
Epoch 250, val loss: 0.8448081016540527
Epoch 260, training loss: 0.3872126340866089 = 0.3803110420703888 + 0.001 * 6.9016032218933105
Epoch 260, val loss: 0.8308471441268921
Epoch 270, training loss: 0.3445459306240082 = 0.3376483917236328 + 0.001 * 6.897538185119629
Epoch 270, val loss: 0.8226361274719238
Epoch 280, training loss: 0.30564460158348083 = 0.29874950647354126 + 0.001 * 6.895086288452148
Epoch 280, val loss: 0.8189337253570557
Epoch 290, training loss: 0.26967671513557434 = 0.2627835273742676 + 0.001 * 6.893184661865234
Epoch 290, val loss: 0.8183609843254089
Epoch 300, training loss: 0.23628748953342438 = 0.22938945889472961 + 0.001 * 6.898024082183838
Epoch 300, val loss: 0.8198399543762207
Epoch 310, training loss: 0.20574268698692322 = 0.19884724915027618 + 0.001 * 6.895437717437744
Epoch 310, val loss: 0.8227660655975342
Epoch 320, training loss: 0.17855553328990936 = 0.17166242003440857 + 0.001 * 6.8931121826171875
Epoch 320, val loss: 0.8272509574890137
Epoch 330, training loss: 0.15501807630062103 = 0.14812536537647247 + 0.001 * 6.892711162567139
Epoch 330, val loss: 0.8335391879081726
Epoch 340, training loss: 0.13503721356391907 = 0.12813861668109894 + 0.001 * 6.898603439331055
Epoch 340, val loss: 0.8418189287185669
Epoch 350, training loss: 0.11821234971284866 = 0.11131729930639267 + 0.001 * 6.895052909851074
Epoch 350, val loss: 0.8518673777580261
Epoch 360, training loss: 0.10406867414712906 = 0.09717477858066559 + 0.001 * 6.893894672393799
Epoch 360, val loss: 0.8636106252670288
Epoch 370, training loss: 0.09214327484369278 = 0.08524370938539505 + 0.001 * 6.899563312530518
Epoch 370, val loss: 0.8766021132469177
Epoch 380, training loss: 0.0820271372795105 = 0.07513008266687393 + 0.001 * 6.897055625915527
Epoch 380, val loss: 0.8905490040779114
Epoch 390, training loss: 0.07340896129608154 = 0.06651411950588226 + 0.001 * 6.894845008850098
Epoch 390, val loss: 0.9051433801651001
Epoch 400, training loss: 0.06603635847568512 = 0.05914147198200226 + 0.001 * 6.894883632659912
Epoch 400, val loss: 0.9201692938804626
Epoch 410, training loss: 0.05970469117164612 = 0.0528080053627491 + 0.001 * 6.896684169769287
Epoch 410, val loss: 0.9353970885276794
Epoch 420, training loss: 0.054243169724941254 = 0.04734760895371437 + 0.001 * 6.895562171936035
Epoch 420, val loss: 0.9506746530532837
Epoch 430, training loss: 0.049523573368787766 = 0.0426236167550087 + 0.001 * 6.899954795837402
Epoch 430, val loss: 0.9659080505371094
Epoch 440, training loss: 0.04541952162981033 = 0.03852201998233795 + 0.001 * 6.897500991821289
Epoch 440, val loss: 0.9810341000556946
Epoch 450, training loss: 0.04184390604496002 = 0.03494789078831673 + 0.001 * 6.89601469039917
Epoch 450, val loss: 0.9959520101547241
Epoch 460, training loss: 0.03871551901102066 = 0.03182162716984749 + 0.001 * 6.893890380859375
Epoch 460, val loss: 1.0106604099273682
Epoch 470, training loss: 0.035976361483335495 = 0.029076699167490005 + 0.001 * 6.899662017822266
Epoch 470, val loss: 1.0250803232192993
Epoch 480, training loss: 0.03355342149734497 = 0.026657532900571823 + 0.001 * 6.895886421203613
Epoch 480, val loss: 1.0391948223114014
Epoch 490, training loss: 0.0314108245074749 = 0.024517720565199852 + 0.001 * 6.893103122711182
Epoch 490, val loss: 1.0529792308807373
Epoch 500, training loss: 0.02951160818338394 = 0.022617854177951813 + 0.001 * 6.8937530517578125
Epoch 500, val loss: 1.0664185285568237
Epoch 510, training loss: 0.02781793661415577 = 0.020925087854266167 + 0.001 * 6.892848491668701
Epoch 510, val loss: 1.0795429944992065
Epoch 520, training loss: 0.02630450949072838 = 0.01941169984638691 + 0.001 * 6.8928093910217285
Epoch 520, val loss: 1.092340111732483
Epoch 530, training loss: 0.024943463504314423 = 0.018054218962788582 + 0.001 * 6.88924503326416
Epoch 530, val loss: 1.1047848463058472
Epoch 540, training loss: 0.02372290939092636 = 0.01683267578482628 + 0.001 * 6.890233516693115
Epoch 540, val loss: 1.1169525384902954
Epoch 550, training loss: 0.022622346878051758 = 0.015730297192931175 + 0.001 * 6.892050266265869
Epoch 550, val loss: 1.1287809610366821
Epoch 560, training loss: 0.021622173488140106 = 0.014732582494616508 + 0.001 * 6.889591693878174
Epoch 560, val loss: 1.140294075012207
Epoch 570, training loss: 0.020713593810796738 = 0.013827062211930752 + 0.001 * 6.886530876159668
Epoch 570, val loss: 1.1515796184539795
Epoch 580, training loss: 0.019894057884812355 = 0.01300320215523243 + 0.001 * 6.890854835510254
Epoch 580, val loss: 1.1625293493270874
Epoch 590, training loss: 0.0191360916942358 = 0.012251662090420723 + 0.001 * 6.884429931640625
Epoch 590, val loss: 1.173211693763733
Epoch 600, training loss: 0.018445508554577827 = 0.011564355343580246 + 0.001 * 6.881153106689453
Epoch 600, val loss: 1.1836128234863281
Epoch 610, training loss: 0.017814669758081436 = 0.010934349149465561 + 0.001 * 6.8803205490112305
Epoch 610, val loss: 1.193771243095398
Epoch 620, training loss: 0.017237666994333267 = 0.010355666279792786 + 0.001 * 6.881999492645264
Epoch 620, val loss: 1.203671932220459
Epoch 630, training loss: 0.01670096069574356 = 0.009823056869208813 + 0.001 * 6.877902984619141
Epoch 630, val loss: 1.2133218050003052
Epoch 640, training loss: 0.016219768673181534 = 0.009331793524324894 + 0.001 * 6.887974262237549
Epoch 640, val loss: 1.222731590270996
Epoch 650, training loss: 0.015756426379084587 = 0.008877814747393131 + 0.001 * 6.8786115646362305
Epoch 650, val loss: 1.2319207191467285
Epoch 660, training loss: 0.015334020368754864 = 0.008457477204501629 + 0.001 * 6.876543045043945
Epoch 660, val loss: 1.2408833503723145
Epoch 670, training loss: 0.014942941255867481 = 0.008067579939961433 + 0.001 * 6.87536096572876
Epoch 670, val loss: 1.2496169805526733
Epoch 680, training loss: 0.014601504430174828 = 0.00770535646006465 + 0.001 * 6.896147727966309
Epoch 680, val loss: 1.2581616640090942
Epoch 690, training loss: 0.014243356883525848 = 0.007368306163698435 + 0.001 * 6.875050067901611
Epoch 690, val loss: 1.2665296792984009
Epoch 700, training loss: 0.013924522325396538 = 0.007054159417748451 + 0.001 * 6.870363235473633
Epoch 700, val loss: 1.2746442556381226
Epoch 710, training loss: 0.013633505441248417 = 0.006760874763131142 + 0.001 * 6.8726301193237305
Epoch 710, val loss: 1.282601237297058
Epoch 720, training loss: 0.013357587158679962 = 0.00648663891479373 + 0.001 * 6.870948314666748
Epoch 720, val loss: 1.2903695106506348
Epoch 730, training loss: 0.013107516802847385 = 0.006229889579117298 + 0.001 * 6.877626895904541
Epoch 730, val loss: 1.297946810722351
Epoch 740, training loss: 0.012861227616667747 = 0.005989174824208021 + 0.001 * 6.8720526695251465
Epoch 740, val loss: 1.3053776025772095
Epoch 750, training loss: 0.0126276146620512 = 0.0057630958035588264 + 0.001 * 6.864518642425537
Epoch 750, val loss: 1.3126295804977417
Epoch 760, training loss: 0.01241164281964302 = 0.00555010000243783 + 0.001 * 6.861542224884033
Epoch 760, val loss: 1.3197349309921265
Epoch 770, training loss: 0.012217000126838684 = 0.005347985774278641 + 0.001 * 6.869014263153076
Epoch 770, val loss: 1.3267309665679932
Epoch 780, training loss: 0.01202633511275053 = 0.005154495593160391 + 0.001 * 6.8718390464782715
Epoch 780, val loss: 1.3337218761444092
Epoch 790, training loss: 0.011829462833702564 = 0.004968354478478432 + 0.001 * 6.86110782623291
Epoch 790, val loss: 1.3408104181289673
Epoch 800, training loss: 0.011646611616015434 = 0.0047892918810248375 + 0.001 * 6.8573198318481445
Epoch 800, val loss: 1.347974181175232
Epoch 810, training loss: 0.01147657074034214 = 0.004617391619831324 + 0.001 * 6.8591790199279785
Epoch 810, val loss: 1.355275273323059
Epoch 820, training loss: 0.011308835819363594 = 0.004452720750123262 + 0.001 * 6.856114387512207
Epoch 820, val loss: 1.3626240491867065
Epoch 830, training loss: 0.01114743947982788 = 0.004295427352190018 + 0.001 * 6.852011203765869
Epoch 830, val loss: 1.3700201511383057
Epoch 840, training loss: 0.010996954515576363 = 0.004145518876612186 + 0.001 * 6.851434707641602
Epoch 840, val loss: 1.3774065971374512
Epoch 850, training loss: 0.010863474570214748 = 0.004002832807600498 + 0.001 * 6.8606414794921875
Epoch 850, val loss: 1.3848196268081665
Epoch 860, training loss: 0.010728766210377216 = 0.0038671952206641436 + 0.001 * 6.861570358276367
Epoch 860, val loss: 1.3922145366668701
Epoch 870, training loss: 0.01059236004948616 = 0.0037383176386356354 + 0.001 * 6.8540425300598145
Epoch 870, val loss: 1.3995487689971924
Epoch 880, training loss: 0.0104690445587039 = 0.003615908557549119 + 0.001 * 6.853135585784912
Epoch 880, val loss: 1.4068655967712402
Epoch 890, training loss: 0.010340698063373566 = 0.0034996550530195236 + 0.001 * 6.841042518615723
Epoch 890, val loss: 1.4140946865081787
Epoch 900, training loss: 0.010229927487671375 = 0.003389213001355529 + 0.001 * 6.840713977813721
Epoch 900, val loss: 1.4212839603424072
Epoch 910, training loss: 0.010135285556316376 = 0.003284282051026821 + 0.001 * 6.851003646850586
Epoch 910, val loss: 1.4283959865570068
Epoch 920, training loss: 0.010031931102275848 = 0.003184569999575615 + 0.001 * 6.847360610961914
Epoch 920, val loss: 1.4354283809661865
Epoch 930, training loss: 0.00993255153298378 = 0.00308975693769753 + 0.001 * 6.842793941497803
Epoch 930, val loss: 1.4424060583114624
Epoch 940, training loss: 0.009838203899562359 = 0.0029995180666446686 + 0.001 * 6.838685512542725
Epoch 940, val loss: 1.4492677450180054
Epoch 950, training loss: 0.009764373302459717 = 0.0029135961085557938 + 0.001 * 6.850776195526123
Epoch 950, val loss: 1.4560357332229614
Epoch 960, training loss: 0.009664681740105152 = 0.0028317708056420088 + 0.001 * 6.832910537719727
Epoch 960, val loss: 1.4627524614334106
Epoch 970, training loss: 0.009580706246197224 = 0.002753770211711526 + 0.001 * 6.826935768127441
Epoch 970, val loss: 1.4693585634231567
Epoch 980, training loss: 0.00951714813709259 = 0.0026793743018060923 + 0.001 * 6.83777379989624
Epoch 980, val loss: 1.475897192955017
Epoch 990, training loss: 0.009449713863432407 = 0.002608377719298005 + 0.001 * 6.841336250305176
Epoch 990, val loss: 1.482327938079834
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.7749
Flip ASR: 0.7289/225 nodes
The final ASR:0.69619, 0.05853, Accuracy:0.79630, 0.00800
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9554])
updated graph: torch.Size([2, 10588])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8481
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98032, 0.00348, Accuracy:0.83457, 0.00972
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9549765586853027 = 1.946602702140808 + 0.001 * 8.373894691467285
Epoch 0, val loss: 1.9484328031539917
Epoch 10, training loss: 1.9452303647994995 = 1.9368565082550049 + 0.001 * 8.373862266540527
Epoch 10, val loss: 1.9389126300811768
Epoch 20, training loss: 1.9330800771713257 = 1.9247063398361206 + 0.001 * 8.373713493347168
Epoch 20, val loss: 1.9270238876342773
Epoch 30, training loss: 1.9151198863983154 = 1.906746506690979 + 0.001 * 8.373363494873047
Epoch 30, val loss: 1.90964674949646
Epoch 40, training loss: 1.887330174446106 = 1.8789576292037964 + 0.001 * 8.372492790222168
Epoch 40, val loss: 1.8833733797073364
Epoch 50, training loss: 1.8479669094085693 = 1.8395968675613403 + 0.001 * 8.369989395141602
Epoch 50, val loss: 1.8481804132461548
Epoch 60, training loss: 1.8049753904342651 = 1.796614408493042 + 0.001 * 8.360973358154297
Epoch 60, val loss: 1.8135794401168823
Epoch 70, training loss: 1.7676455974578857 = 1.7593278884887695 + 0.001 * 8.317707061767578
Epoch 70, val loss: 1.7830158472061157
Epoch 80, training loss: 1.7183187007904053 = 1.7102819681167603 + 0.001 * 8.036687850952148
Epoch 80, val loss: 1.7375643253326416
Epoch 90, training loss: 1.6508194208145142 = 1.6429274082183838 + 0.001 * 7.89205265045166
Epoch 90, val loss: 1.6786844730377197
Epoch 100, training loss: 1.565167784690857 = 1.5573291778564453 + 0.001 * 7.838634014129639
Epoch 100, val loss: 1.6077454090118408
Epoch 110, training loss: 1.4734493494033813 = 1.4656462669372559 + 0.001 * 7.803135395050049
Epoch 110, val loss: 1.5329164266586304
Epoch 120, training loss: 1.3848823308944702 = 1.3771629333496094 + 0.001 * 7.71943473815918
Epoch 120, val loss: 1.4635101556777954
Epoch 130, training loss: 1.2992626428604126 = 1.2917358875274658 + 0.001 * 7.526808261871338
Epoch 130, val loss: 1.3992410898208618
Epoch 140, training loss: 1.2132675647735596 = 1.205798625946045 + 0.001 * 7.468893051147461
Epoch 140, val loss: 1.3360010385513306
Epoch 150, training loss: 1.1252650022506714 = 1.117889642715454 + 0.001 * 7.375343322753906
Epoch 150, val loss: 1.2725417613983154
Epoch 160, training loss: 1.0360897779464722 = 1.0287811756134033 + 0.001 * 7.308658123016357
Epoch 160, val loss: 1.2096003293991089
Epoch 170, training loss: 0.9480759501457214 = 0.9407992362976074 + 0.001 * 7.276719570159912
Epoch 170, val loss: 1.1485521793365479
Epoch 180, training loss: 0.8638817667961121 = 0.856653094291687 + 0.001 * 7.228663921356201
Epoch 180, val loss: 1.0920993089675903
Epoch 190, training loss: 0.7854527831077576 = 0.7782740592956543 + 0.001 * 7.1787238121032715
Epoch 190, val loss: 1.0415842533111572
Epoch 200, training loss: 0.7129077911376953 = 0.7057499885559082 + 0.001 * 7.157818794250488
Epoch 200, val loss: 0.9965579509735107
Epoch 210, training loss: 0.6453553438186646 = 0.6382030844688416 + 0.001 * 7.152270793914795
Epoch 210, val loss: 0.956184983253479
Epoch 220, training loss: 0.5822891592979431 = 0.5751399993896484 + 0.001 * 7.1491851806640625
Epoch 220, val loss: 0.9201844334602356
Epoch 230, training loss: 0.5237737894058228 = 0.5166284441947937 + 0.001 * 7.145369052886963
Epoch 230, val loss: 0.8891519904136658
Epoch 240, training loss: 0.46970081329345703 = 0.46255892515182495 + 0.001 * 7.1418938636779785
Epoch 240, val loss: 0.8637775778770447
Epoch 250, training loss: 0.4196576476097107 = 0.41251975297927856 + 0.001 * 7.137890338897705
Epoch 250, val loss: 0.8439759612083435
Epoch 260, training loss: 0.37339454889297485 = 0.36626097559928894 + 0.001 * 7.133582592010498
Epoch 260, val loss: 0.8287846446037292
Epoch 270, training loss: 0.3310219347476959 = 0.32389315962791443 + 0.001 * 7.128766059875488
Epoch 270, val loss: 0.8177350759506226
Epoch 280, training loss: 0.2926897704601288 = 0.2855619192123413 + 0.001 * 7.1278486251831055
Epoch 280, val loss: 0.8104625344276428
Epoch 290, training loss: 0.2581651210784912 = 0.25104451179504395 + 0.001 * 7.120610237121582
Epoch 290, val loss: 0.8069018125534058
Epoch 300, training loss: 0.22691099345684052 = 0.2197963446378708 + 0.001 * 7.114647388458252
Epoch 300, val loss: 0.8067549467086792
Epoch 310, training loss: 0.19842970371246338 = 0.19132381677627563 + 0.001 * 7.105892181396484
Epoch 310, val loss: 0.8098153471946716
Epoch 320, training loss: 0.17253956198692322 = 0.16544431447982788 + 0.001 * 7.095245838165283
Epoch 320, val loss: 0.8156546354293823
Epoch 330, training loss: 0.1494031548500061 = 0.1423141211271286 + 0.001 * 7.089028835296631
Epoch 330, val loss: 0.823939859867096
Epoch 340, training loss: 0.12925836443901062 = 0.12217645347118378 + 0.001 * 7.081907749176025
Epoch 340, val loss: 0.834496259689331
Epoch 350, training loss: 0.11214356124401093 = 0.10508329421281815 + 0.001 * 7.060268878936768
Epoch 350, val loss: 0.8469476699829102
Epoch 360, training loss: 0.09785681962966919 = 0.09082108736038208 + 0.001 * 7.0357346534729
Epoch 360, val loss: 0.8609175682067871
Epoch 370, training loss: 0.08604583144187927 = 0.07899919897317886 + 0.001 * 7.046629428863525
Epoch 370, val loss: 0.8758589625358582
Epoch 380, training loss: 0.07620514184236526 = 0.06917889416217804 + 0.001 * 7.026244640350342
Epoch 380, val loss: 0.8913816213607788
Epoch 390, training loss: 0.06795134395360947 = 0.06095204874873161 + 0.001 * 6.99929666519165
Epoch 390, val loss: 0.9071842432022095
Epoch 400, training loss: 0.060995738953351974 = 0.053992461413145065 + 0.001 * 7.003278732299805
Epoch 400, val loss: 0.9230123162269592
Epoch 410, training loss: 0.055039018392562866 = 0.04805586487054825 + 0.001 * 6.983152866363525
Epoch 410, val loss: 0.9386983513832092
Epoch 420, training loss: 0.04993728920817375 = 0.042958661913871765 + 0.001 * 6.978628158569336
Epoch 420, val loss: 0.9541577100753784
Epoch 430, training loss: 0.045539602637290955 = 0.03856012970209122 + 0.001 * 6.97947359085083
Epoch 430, val loss: 0.9692729711532593
Epoch 440, training loss: 0.04172613471746445 = 0.034749776124954224 + 0.001 * 6.976360321044922
Epoch 440, val loss: 0.9840308427810669
Epoch 450, training loss: 0.038406405597925186 = 0.03143526613712311 + 0.001 * 6.971138000488281
Epoch 450, val loss: 0.9982931613922119
Epoch 460, training loss: 0.03550390154123306 = 0.02854071743786335 + 0.001 * 6.963184356689453
Epoch 460, val loss: 1.012152910232544
Epoch 470, training loss: 0.03296844661235809 = 0.026003822684288025 + 0.001 * 6.96462345123291
Epoch 470, val loss: 1.0255162715911865
Epoch 480, training loss: 0.030735008418560028 = 0.023774750530719757 + 0.001 * 6.960257053375244
Epoch 480, val loss: 1.0384231805801392
Epoch 490, training loss: 0.028784744441509247 = 0.021815255284309387 + 0.001 * 6.969489574432373
Epoch 490, val loss: 1.050871729850769
Epoch 500, training loss: 0.02703779563307762 = 0.020082656294107437 + 0.001 * 6.955138683319092
Epoch 500, val loss: 1.0628458261489868
Epoch 510, training loss: 0.02550746314227581 = 0.018545689061284065 + 0.001 * 6.961773872375488
Epoch 510, val loss: 1.0743980407714844
Epoch 520, training loss: 0.02414514869451523 = 0.017176548019051552 + 0.001 * 6.968599796295166
Epoch 520, val loss: 1.0854917764663696
Epoch 530, training loss: 0.02289830707013607 = 0.01595333032310009 + 0.001 * 6.944976329803467
Epoch 530, val loss: 1.09623384475708
Epoch 540, training loss: 0.021814167499542236 = 0.014856935478746891 + 0.001 * 6.9572319984436035
Epoch 540, val loss: 1.10660719871521
Epoch 550, training loss: 0.02081996016204357 = 0.013872797600924969 + 0.001 * 6.947161674499512
Epoch 550, val loss: 1.1166828870773315
Epoch 560, training loss: 0.019924530759453773 = 0.012984871864318848 + 0.001 * 6.9396586418151855
Epoch 560, val loss: 1.12641179561615
Epoch 570, training loss: 0.01911819726228714 = 0.012181409634649754 + 0.001 * 6.936787128448486
Epoch 570, val loss: 1.135825514793396
Epoch 580, training loss: 0.01839490607380867 = 0.011452291160821915 + 0.001 * 6.9426140785217285
Epoch 580, val loss: 1.144907832145691
Epoch 590, training loss: 0.017730053514242172 = 0.010788780637085438 + 0.001 * 6.941272735595703
Epoch 590, val loss: 1.1536966562271118
Epoch 600, training loss: 0.017123635858297348 = 0.010183271020650864 + 0.001 * 6.940364837646484
Epoch 600, val loss: 1.16222083568573
Epoch 610, training loss: 0.016565373167395592 = 0.009629415348172188 + 0.001 * 6.935956954956055
Epoch 610, val loss: 1.1704820394515991
Epoch 620, training loss: 0.01605628989636898 = 0.00912155956029892 + 0.001 * 6.934730052947998
Epoch 620, val loss: 1.1784766912460327
Epoch 630, training loss: 0.015580518171191216 = 0.008654879406094551 + 0.001 * 6.925638675689697
Epoch 630, val loss: 1.1862291097640991
Epoch 640, training loss: 0.015143334865570068 = 0.008224950172007084 + 0.001 * 6.918384075164795
Epoch 640, val loss: 1.193739414215088
Epoch 650, training loss: 0.014767002314329147 = 0.007828067988157272 + 0.001 * 6.938933849334717
Epoch 650, val loss: 1.2010400295257568
Epoch 660, training loss: 0.014390199445188046 = 0.007460978347808123 + 0.001 * 6.929220676422119
Epoch 660, val loss: 1.2081329822540283
Epoch 670, training loss: 0.014051375910639763 = 0.007120757829397917 + 0.001 * 6.930617332458496
Epoch 670, val loss: 1.2150217294692993
Epoch 680, training loss: 0.01372340228408575 = 0.006804831326007843 + 0.001 * 6.918570518493652
Epoch 680, val loss: 1.221739649772644
Epoch 690, training loss: 0.013420863077044487 = 0.006510997656732798 + 0.001 * 6.90986442565918
Epoch 690, val loss: 1.2282698154449463
Epoch 700, training loss: 0.013148985803127289 = 0.00623721769079566 + 0.001 * 6.911767482757568
Epoch 700, val loss: 1.2346465587615967
Epoch 710, training loss: 0.012884516268968582 = 0.005981702823191881 + 0.001 * 6.902812480926514
Epoch 710, val loss: 1.2408342361450195
Epoch 720, training loss: 0.012655285187065601 = 0.005742895882576704 + 0.001 * 6.912388801574707
Epoch 720, val loss: 1.2468938827514648
Epoch 730, training loss: 0.012433174066245556 = 0.00551936449483037 + 0.001 * 6.913809299468994
Epoch 730, val loss: 1.252777338027954
Epoch 740, training loss: 0.012210896238684654 = 0.0053098625503480434 + 0.001 * 6.901033878326416
Epoch 740, val loss: 1.2585104703903198
Epoch 750, training loss: 0.012014033272862434 = 0.005113143473863602 + 0.001 * 6.900888919830322
Epoch 750, val loss: 1.264107346534729
Epoch 760, training loss: 0.01181688904762268 = 0.00492825685068965 + 0.001 * 6.888631343841553
Epoch 760, val loss: 1.2695529460906982
Epoch 770, training loss: 0.011653603054583073 = 0.0047542196698486805 + 0.001 * 6.899383068084717
Epoch 770, val loss: 1.2748790979385376
Epoch 780, training loss: 0.011504977941513062 = 0.004590201657265425 + 0.001 * 6.914775371551514
Epoch 780, val loss: 1.2800849676132202
Epoch 790, training loss: 0.011322272941470146 = 0.004435439594089985 + 0.001 * 6.886833190917969
Epoch 790, val loss: 1.2851630449295044
Epoch 800, training loss: 0.011178236454725266 = 0.004289285279810429 + 0.001 * 6.888950347900391
Epoch 800, val loss: 1.2901175022125244
Epoch 810, training loss: 0.011043019592761993 = 0.00415113614872098 + 0.001 * 6.89188289642334
Epoch 810, val loss: 1.2949705123901367
Epoch 820, training loss: 0.010916815139353275 = 0.004020352847874165 + 0.001 * 6.8964619636535645
Epoch 820, val loss: 1.299720287322998
Epoch 830, training loss: 0.010789548978209496 = 0.0038964664563536644 + 0.001 * 6.893082618713379
Epoch 830, val loss: 1.3043638467788696
Epoch 840, training loss: 0.01066331472247839 = 0.003778936341404915 + 0.001 * 6.884377956390381
Epoch 840, val loss: 1.3089098930358887
Epoch 850, training loss: 0.01053360104560852 = 0.0036673794966191053 + 0.001 * 6.866220951080322
Epoch 850, val loss: 1.3133549690246582
Epoch 860, training loss: 0.010427627712488174 = 0.0035613796208053827 + 0.001 * 6.86624813079834
Epoch 860, val loss: 1.3177253007888794
Epoch 870, training loss: 0.010317828506231308 = 0.003460552077740431 + 0.001 * 6.857276439666748
Epoch 870, val loss: 1.3219865560531616
Epoch 880, training loss: 0.010265442542731762 = 0.0033646265510469675 + 0.001 * 6.900815963745117
Epoch 880, val loss: 1.326138973236084
Epoch 890, training loss: 0.010127710178494453 = 0.0032732461113482714 + 0.001 * 6.854463577270508
Epoch 890, val loss: 1.3302305936813354
Epoch 900, training loss: 0.010058733634650707 = 0.003186152083799243 + 0.001 * 6.8725810050964355
Epoch 900, val loss: 1.3342293500900269
Epoch 910, training loss: 0.009977058507502079 = 0.003103033872321248 + 0.001 * 6.874023914337158
Epoch 910, val loss: 1.338152289390564
Epoch 920, training loss: 0.009918440133333206 = 0.003023688215762377 + 0.001 * 6.89475154876709
Epoch 920, val loss: 1.3419897556304932
Epoch 930, training loss: 0.009794015437364578 = 0.002947896486148238 + 0.001 * 6.846118450164795
Epoch 930, val loss: 1.3457672595977783
Epoch 940, training loss: 0.00971203949302435 = 0.0028754491358995438 + 0.001 * 6.836589813232422
Epoch 940, val loss: 1.3494541645050049
Epoch 950, training loss: 0.009637262672185898 = 0.002806099597364664 + 0.001 * 6.831162929534912
Epoch 950, val loss: 1.3530725240707397
Epoch 960, training loss: 0.009573365561664104 = 0.00273976125754416 + 0.001 * 6.833604335784912
Epoch 960, val loss: 1.3566105365753174
Epoch 970, training loss: 0.009500951506197453 = 0.0026761179324239492 + 0.001 * 6.824832916259766
Epoch 970, val loss: 1.360112190246582
Epoch 980, training loss: 0.009505482390522957 = 0.002615161705762148 + 0.001 * 6.890320301055908
Epoch 980, val loss: 1.363526701927185
Epoch 990, training loss: 0.00944120530039072 = 0.002556663239374757 + 0.001 * 6.8845415115356445
Epoch 990, val loss: 1.3668932914733887
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.5572
Flip ASR: 0.4800/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9695286750793457 = 1.961154818534851 + 0.001 * 8.373886108398438
Epoch 0, val loss: 1.9671601057052612
Epoch 10, training loss: 1.9583044052124023 = 1.9499305486679077 + 0.001 * 8.373802185058594
Epoch 10, val loss: 1.954957127571106
Epoch 20, training loss: 1.9442534446716309 = 1.9358799457550049 + 0.001 * 8.373555183410645
Epoch 20, val loss: 1.9393643140792847
Epoch 30, training loss: 1.9242101907730103 = 1.9158371686935425 + 0.001 * 8.37302017211914
Epoch 30, val loss: 1.9168307781219482
Epoch 40, training loss: 1.8942575454711914 = 1.8858857154846191 + 0.001 * 8.371803283691406
Epoch 40, val loss: 1.8832286596298218
Epoch 50, training loss: 1.8523280620574951 = 1.8439596891403198 + 0.001 * 8.368429183959961
Epoch 50, val loss: 1.8378239870071411
Epoch 60, training loss: 1.8057994842529297 = 1.797443151473999 + 0.001 * 8.35635757446289
Epoch 60, val loss: 1.7919070720672607
Epoch 70, training loss: 1.7666617631912231 = 1.7583630084991455 + 0.001 * 8.298799514770508
Epoch 70, val loss: 1.7577927112579346
Epoch 80, training loss: 1.7181490659713745 = 1.7101789712905884 + 0.001 * 7.970071315765381
Epoch 80, val loss: 1.7163360118865967
Epoch 90, training loss: 1.6513959169387817 = 1.6435635089874268 + 0.001 * 7.832380771636963
Epoch 90, val loss: 1.6587486267089844
Epoch 100, training loss: 1.56459641456604 = 1.5568286180496216 + 0.001 * 7.767787933349609
Epoch 100, val loss: 1.5840647220611572
Epoch 110, training loss: 1.4694892168045044 = 1.461769461631775 + 0.001 * 7.719712257385254
Epoch 110, val loss: 1.505869746208191
Epoch 120, training loss: 1.380014419555664 = 1.3724092245101929 + 0.001 * 7.60518741607666
Epoch 120, val loss: 1.4380362033843994
Epoch 130, training loss: 1.297290563583374 = 1.2898752689361572 + 0.001 * 7.415266513824463
Epoch 130, val loss: 1.3814436197280884
Epoch 140, training loss: 1.2188717126846313 = 1.2114837169647217 + 0.001 * 7.387940406799316
Epoch 140, val loss: 1.331587314605713
Epoch 150, training loss: 1.1438921689987183 = 1.136560082435608 + 0.001 * 7.33210563659668
Epoch 150, val loss: 1.285261869430542
Epoch 160, training loss: 1.073216438293457 = 1.0659523010253906 + 0.001 * 7.264196395874023
Epoch 160, val loss: 1.2417207956314087
Epoch 170, training loss: 1.0079891681671143 = 1.000815987586975 + 0.001 * 7.173145771026611
Epoch 170, val loss: 1.2009968757629395
Epoch 180, training loss: 0.948308527469635 = 0.9412077069282532 + 0.001 * 7.100808143615723
Epoch 180, val loss: 1.163530707359314
Epoch 190, training loss: 0.8925057649612427 = 0.8854338526725769 + 0.001 * 7.071887969970703
Epoch 190, val loss: 1.1282423734664917
Epoch 200, training loss: 0.8376597762107849 = 0.8306049704551697 + 0.001 * 7.0548272132873535
Epoch 200, val loss: 1.0933587551116943
Epoch 210, training loss: 0.7809982299804688 = 0.7739512324333191 + 0.001 * 7.046997547149658
Epoch 210, val loss: 1.0570460557937622
Epoch 220, training loss: 0.7211604714393616 = 0.714116632938385 + 0.001 * 7.043811321258545
Epoch 220, val loss: 1.017624020576477
Epoch 230, training loss: 0.6589682102203369 = 0.651924729347229 + 0.001 * 7.043476104736328
Epoch 230, val loss: 0.9754815101623535
Epoch 240, training loss: 0.5971931219100952 = 0.59014892578125 + 0.001 * 7.044185638427734
Epoch 240, val loss: 0.9326048493385315
Epoch 250, training loss: 0.5392672419548035 = 0.532222330570221 + 0.001 * 7.044930934906006
Epoch 250, val loss: 0.8925617337226868
Epoch 260, training loss: 0.48748114705085754 = 0.4804357588291168 + 0.001 * 7.045380592346191
Epoch 260, val loss: 0.8580341935157776
Epoch 270, training loss: 0.4417285621166229 = 0.4346831738948822 + 0.001 * 7.045402526855469
Epoch 270, val loss: 0.8292578458786011
Epoch 280, training loss: 0.40022099018096924 = 0.39317598938941956 + 0.001 * 7.045015335083008
Epoch 280, val loss: 0.8054854869842529
Epoch 290, training loss: 0.36113470792770386 = 0.35409030318260193 + 0.001 * 7.04440450668335
Epoch 290, val loss: 0.78510981798172
Epoch 300, training loss: 0.32354235649108887 = 0.31649863719940186 + 0.001 * 7.043713092803955
Epoch 300, val loss: 0.7672985196113586
Epoch 310, training loss: 0.28751340508461 = 0.2804703712463379 + 0.001 * 7.043032646179199
Epoch 310, val loss: 0.7522428035736084
Epoch 320, training loss: 0.253654807806015 = 0.24661242961883545 + 0.001 * 7.042368412017822
Epoch 320, val loss: 0.7403706312179565
Epoch 330, training loss: 0.2225469946861267 = 0.2155054658651352 + 0.001 * 7.041534423828125
Epoch 330, val loss: 0.7322089076042175
Epoch 340, training loss: 0.19473165273666382 = 0.18769143521785736 + 0.001 * 7.040213584899902
Epoch 340, val loss: 0.7280206084251404
Epoch 350, training loss: 0.17042770981788635 = 0.16338980197906494 + 0.001 * 7.037914276123047
Epoch 350, val loss: 0.7275506258010864
Epoch 360, training loss: 0.14964750409126282 = 0.14261354506015778 + 0.001 * 7.033965587615967
Epoch 360, val loss: 0.7306485176086426
Epoch 370, training loss: 0.131998673081398 = 0.12496671825647354 + 0.001 * 7.031953811645508
Epoch 370, val loss: 0.7367058396339417
Epoch 380, training loss: 0.11691755056381226 = 0.10989607125520706 + 0.001 * 7.021476745605469
Epoch 380, val loss: 0.7448329329490662
Epoch 390, training loss: 0.10399309545755386 = 0.09698019921779633 + 0.001 * 7.012896537780762
Epoch 390, val loss: 0.7547339200973511
Epoch 400, training loss: 0.09285716712474823 = 0.0858548954129219 + 0.001 * 7.002268314361572
Epoch 400, val loss: 0.7658620476722717
Epoch 410, training loss: 0.08327336609363556 = 0.07627830654382706 + 0.001 * 6.99506139755249
Epoch 410, val loss: 0.7782172560691833
Epoch 420, training loss: 0.07498063147068024 = 0.06800077855587006 + 0.001 * 6.979852676391602
Epoch 420, val loss: 0.7912670373916626
Epoch 430, training loss: 0.06780968606472015 = 0.060837604105472565 + 0.001 * 6.972085475921631
Epoch 430, val loss: 0.8049875497817993
Epoch 440, training loss: 0.061593834310770035 = 0.05461376532912254 + 0.001 * 6.980067729949951
Epoch 440, val loss: 0.8188921809196472
Epoch 450, training loss: 0.05615934357047081 = 0.049197301268577576 + 0.001 * 6.962040424346924
Epoch 450, val loss: 0.8329658508300781
Epoch 460, training loss: 0.05143330991268158 = 0.04447253420948982 + 0.001 * 6.960777282714844
Epoch 460, val loss: 0.8470209240913391
Epoch 470, training loss: 0.04729717969894409 = 0.040338773280382156 + 0.001 * 6.958404541015625
Epoch 470, val loss: 0.8609364628791809
Epoch 480, training loss: 0.043663688004016876 = 0.036706678569316864 + 0.001 * 6.95700740814209
Epoch 480, val loss: 0.8746030330657959
Epoch 490, training loss: 0.040465570986270905 = 0.03350989148020744 + 0.001 * 6.9556779861450195
Epoch 490, val loss: 0.8880336880683899
Epoch 500, training loss: 0.037640027701854706 = 0.030685197561979294 + 0.001 * 6.954830169677734
Epoch 500, val loss: 0.9011938571929932
Epoch 510, training loss: 0.03513495624065399 = 0.028177419677376747 + 0.001 * 6.957535743713379
Epoch 510, val loss: 0.9141278266906738
Epoch 520, training loss: 0.03289157897233963 = 0.02593647688627243 + 0.001 * 6.955103397369385
Epoch 520, val loss: 0.9267569780349731
Epoch 530, training loss: 0.03088577464222908 = 0.023933198302984238 + 0.001 * 6.952576160430908
Epoch 530, val loss: 0.9390825629234314
Epoch 540, training loss: 0.02909628301858902 = 0.022143827751278877 + 0.001 * 6.952455043792725
Epoch 540, val loss: 0.9512291550636292
Epoch 550, training loss: 0.0274974312633276 = 0.020543305203318596 + 0.001 * 6.954126358032227
Epoch 550, val loss: 0.9631223082542419
Epoch 560, training loss: 0.026060279458761215 = 0.019107980653643608 + 0.001 * 6.952299118041992
Epoch 560, val loss: 0.9747352004051208
Epoch 570, training loss: 0.02476533129811287 = 0.017815841361880302 + 0.001 * 6.949489593505859
Epoch 570, val loss: 0.9860724806785583
Epoch 580, training loss: 0.023597057908773422 = 0.01664865016937256 + 0.001 * 6.9484076499938965
Epoch 580, val loss: 0.9971596002578735
Epoch 590, training loss: 0.022547747939825058 = 0.01559573132544756 + 0.001 * 6.952017307281494
Epoch 590, val loss: 1.0079749822616577
Epoch 600, training loss: 0.02159462310373783 = 0.014642507769167423 + 0.001 * 6.952114582061768
Epoch 600, val loss: 1.0185905694961548
Epoch 610, training loss: 0.02072075754404068 = 0.01377515122294426 + 0.001 * 6.945606231689453
Epoch 610, val loss: 1.0290096998214722
Epoch 620, training loss: 0.019933337345719337 = 0.012983388267457485 + 0.001 * 6.949948787689209
Epoch 620, val loss: 1.0391418933868408
Epoch 630, training loss: 0.01920609548687935 = 0.012258527800440788 + 0.001 * 6.947566986083984
Epoch 630, val loss: 1.0490187406539917
Epoch 640, training loss: 0.01853865385055542 = 0.011593361385166645 + 0.001 * 6.9452924728393555
Epoch 640, val loss: 1.0587666034698486
Epoch 650, training loss: 0.017935045063495636 = 0.010981655679643154 + 0.001 * 6.953389644622803
Epoch 650, val loss: 1.0682891607284546
Epoch 660, training loss: 0.017358820885419846 = 0.01041819155216217 + 0.001 * 6.940629005432129
Epoch 660, val loss: 1.0776145458221436
Epoch 670, training loss: 0.016843199729919434 = 0.009898207150399685 + 0.001 * 6.944991588592529
Epoch 670, val loss: 1.086729645729065
Epoch 680, training loss: 0.01635991968214512 = 0.009417428635060787 + 0.001 * 6.942491054534912
Epoch 680, val loss: 1.0956509113311768
Epoch 690, training loss: 0.0159137099981308 = 0.00897209718823433 + 0.001 * 6.941612720489502
Epoch 690, val loss: 1.104371428489685
Epoch 700, training loss: 0.015496497973799706 = 0.008558875881135464 + 0.001 * 6.937621593475342
Epoch 700, val loss: 1.1129226684570312
Epoch 710, training loss: 0.0151178278028965 = 0.00817472580820322 + 0.001 * 6.943101406097412
Epoch 710, val loss: 1.121256947517395
Epoch 720, training loss: 0.01476344931870699 = 0.007817183621227741 + 0.001 * 6.94626522064209
Epoch 720, val loss: 1.1294227838516235
Epoch 730, training loss: 0.014417875558137894 = 0.007483748719096184 + 0.001 * 6.934126377105713
Epoch 730, val loss: 1.1374177932739258
Epoch 740, training loss: 0.014103876426815987 = 0.007172559387981892 + 0.001 * 6.93131685256958
Epoch 740, val loss: 1.1452523469924927
Epoch 750, training loss: 0.013830102048814297 = 0.006881703156977892 + 0.001 * 6.948398590087891
Epoch 750, val loss: 1.1528879404067993
Epoch 760, training loss: 0.013542945496737957 = 0.006609391886740923 + 0.001 * 6.933553218841553
Epoch 760, val loss: 1.1603931188583374
Epoch 770, training loss: 0.013281406834721565 = 0.006354046985507011 + 0.001 * 6.927359104156494
Epoch 770, val loss: 1.1677148342132568
Epoch 780, training loss: 0.013062686659395695 = 0.006114293355494738 + 0.001 * 6.948392868041992
Epoch 780, val loss: 1.1748684644699097
Epoch 790, training loss: 0.012816067785024643 = 0.005888910964131355 + 0.001 * 6.9271559715271
Epoch 790, val loss: 1.181902289390564
Epoch 800, training loss: 0.012600146234035492 = 0.0056768376380205154 + 0.001 * 6.923308372497559
Epoch 800, val loss: 1.1887727975845337
Epoch 810, training loss: 0.012400154024362564 = 0.005477002821862698 + 0.001 * 6.92315149307251
Epoch 810, val loss: 1.1955206394195557
Epoch 820, training loss: 0.012210662476718426 = 0.005288516636937857 + 0.001 * 6.922145366668701
Epoch 820, val loss: 1.2021135091781616
Epoch 830, training loss: 0.012054823338985443 = 0.005110076162964106 + 0.001 * 6.944747447967529
Epoch 830, val loss: 1.2086023092269897
Epoch 840, training loss: 0.01185579877346754 = 0.00494100758805871 + 0.001 * 6.914790630340576
Epoch 840, val loss: 1.21493399143219
Epoch 850, training loss: 0.01170017197728157 = 0.004780913703143597 + 0.001 * 6.919258117675781
Epoch 850, val loss: 1.221163034439087
Epoch 860, training loss: 0.011556441895663738 = 0.00462921941652894 + 0.001 * 6.92722225189209
Epoch 860, val loss: 1.2272934913635254
Epoch 870, training loss: 0.011394519358873367 = 0.00448503578081727 + 0.001 * 6.909482955932617
Epoch 870, val loss: 1.23331618309021
Epoch 880, training loss: 0.011267624795436859 = 0.004347206559032202 + 0.001 * 6.920417308807373
Epoch 880, val loss: 1.2392709255218506
Epoch 890, training loss: 0.011120162904262543 = 0.004214512649923563 + 0.001 * 6.905649662017822
Epoch 890, val loss: 1.2451859712600708
Epoch 900, training loss: 0.01098678819835186 = 0.004085457883775234 + 0.001 * 6.90132999420166
Epoch 900, val loss: 1.2510093450546265
Epoch 910, training loss: 0.010877168737351894 = 0.003959647845476866 + 0.001 * 6.917520523071289
Epoch 910, val loss: 1.2568862438201904
Epoch 920, training loss: 0.010739075019955635 = 0.003836468793451786 + 0.001 * 6.902606010437012
Epoch 920, val loss: 1.2627664804458618
Epoch 930, training loss: 0.010635366663336754 = 0.003714972408488393 + 0.001 * 6.920393943786621
Epoch 930, val loss: 1.2688260078430176
Epoch 940, training loss: 0.010492446832358837 = 0.0035944515839219093 + 0.001 * 6.8979949951171875
Epoch 940, val loss: 1.2751926183700562
Epoch 950, training loss: 0.010373212397098541 = 0.0034745896700769663 + 0.001 * 6.898622035980225
Epoch 950, val loss: 1.2818337678909302
Epoch 960, training loss: 0.01030145213007927 = 0.0033556802663952112 + 0.001 * 6.945771217346191
Epoch 960, val loss: 1.2887130975723267
Epoch 970, training loss: 0.010138634592294693 = 0.0032382900826632977 + 0.001 * 6.900344371795654
Epoch 970, val loss: 1.2958406209945679
Epoch 980, training loss: 0.010026377625763416 = 0.0031228982843458652 + 0.001 * 6.903479099273682
Epoch 980, val loss: 1.3030905723571777
Epoch 990, training loss: 0.0098948422819376 = 0.0030102606397122145 + 0.001 * 6.884581565856934
Epoch 990, val loss: 1.310485601425171
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6790
Flip ASR: 0.6444/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9549875259399414 = 1.9466136693954468 + 0.001 * 8.373886108398438
Epoch 0, val loss: 1.9454050064086914
Epoch 10, training loss: 1.9442335367202759 = 1.9358596801757812 + 0.001 * 8.37385082244873
Epoch 10, val loss: 1.934255599975586
Epoch 20, training loss: 1.9314870834350586 = 1.9231133460998535 + 0.001 * 8.373695373535156
Epoch 20, val loss: 1.920760989189148
Epoch 30, training loss: 1.913867712020874 = 1.9054943323135376 + 0.001 * 8.373351097106934
Epoch 30, val loss: 1.9022160768508911
Epoch 40, training loss: 1.8881462812423706 = 1.879773736000061 + 0.001 * 8.37252140045166
Epoch 40, val loss: 1.8757885694503784
Epoch 50, training loss: 1.852433443069458 = 1.84406316280365 + 0.001 * 8.370246887207031
Epoch 50, val loss: 1.8411558866500854
Epoch 60, training loss: 1.8117954730987549 = 1.803432822227478 + 0.001 * 8.362701416015625
Epoch 60, val loss: 1.8062374591827393
Epoch 70, training loss: 1.7724955081939697 = 1.7641661167144775 + 0.001 * 8.329385757446289
Epoch 70, val loss: 1.7753020524978638
Epoch 80, training loss: 1.7225499153137207 = 1.7144299745559692 + 0.001 * 8.11998176574707
Epoch 80, val loss: 1.7333146333694458
Epoch 90, training loss: 1.6543772220611572 = 1.646488904953003 + 0.001 * 7.888286590576172
Epoch 90, val loss: 1.6754730939865112
Epoch 100, training loss: 1.5648670196533203 = 1.5571174621582031 + 0.001 * 7.749574184417725
Epoch 100, val loss: 1.6005586385726929
Epoch 110, training loss: 1.4605084657669067 = 1.4529602527618408 + 0.001 * 7.548190593719482
Epoch 110, val loss: 1.5141839981079102
Epoch 120, training loss: 1.3490813970565796 = 1.341709852218628 + 0.001 * 7.371523857116699
Epoch 120, val loss: 1.4238072633743286
Epoch 130, training loss: 1.233683466911316 = 1.2263152599334717 + 0.001 * 7.368156909942627
Epoch 130, val loss: 1.3317103385925293
Epoch 140, training loss: 1.1156699657440186 = 1.1083307266235352 + 0.001 * 7.339200973510742
Epoch 140, val loss: 1.2398985624313354
Epoch 150, training loss: 0.9979506134986877 = 0.9906333684921265 + 0.001 * 7.317263126373291
Epoch 150, val loss: 1.1499027013778687
Epoch 160, training loss: 0.8850877285003662 = 0.8778073191642761 + 0.001 * 7.280436992645264
Epoch 160, val loss: 1.06650972366333
Epoch 170, training loss: 0.7825074195861816 = 0.7752959132194519 + 0.001 * 7.211484432220459
Epoch 170, val loss: 0.9937649369239807
Epoch 180, training loss: 0.6938037872314453 = 0.6866909861564636 + 0.001 * 7.112794399261475
Epoch 180, val loss: 0.9350067377090454
Epoch 190, training loss: 0.6192442178726196 = 0.612181544303894 + 0.001 * 7.062647819519043
Epoch 190, val loss: 0.890163779258728
Epoch 200, training loss: 0.5564305782318115 = 0.5493875741958618 + 0.001 * 7.042996883392334
Epoch 200, val loss: 0.8569573760032654
Epoch 210, training loss: 0.5023323893547058 = 0.4952956438064575 + 0.001 * 7.0367512702941895
Epoch 210, val loss: 0.8327720761299133
Epoch 220, training loss: 0.4545900821685791 = 0.44755420088768005 + 0.001 * 7.035891532897949
Epoch 220, val loss: 0.8158101439476013
Epoch 230, training loss: 0.4116327464580536 = 0.40459737181663513 + 0.001 * 7.035377025604248
Epoch 230, val loss: 0.8044315576553345
Epoch 240, training loss: 0.3723645508289337 = 0.3653297424316406 + 0.001 * 7.0348052978515625
Epoch 240, val loss: 0.7971611022949219
Epoch 250, training loss: 0.3359259366989136 = 0.3288920521736145 + 0.001 * 7.03387451171875
Epoch 250, val loss: 0.7929970026016235
Epoch 260, training loss: 0.3016502857208252 = 0.2946174740791321 + 0.001 * 7.032804489135742
Epoch 260, val loss: 0.7910946607589722
Epoch 270, training loss: 0.269163578748703 = 0.26213183999061584 + 0.001 * 7.031752586364746
Epoch 270, val loss: 0.7909002304077148
Epoch 280, training loss: 0.2384590208530426 = 0.23142816126346588 + 0.001 * 7.030857563018799
Epoch 280, val loss: 0.7920868396759033
Epoch 290, training loss: 0.20993608236312866 = 0.20290367305278778 + 0.001 * 7.032407283782959
Epoch 290, val loss: 0.7944848537445068
Epoch 300, training loss: 0.18409475684165955 = 0.17706429958343506 + 0.001 * 7.030449867248535
Epoch 300, val loss: 0.7979874014854431
Epoch 310, training loss: 0.16125404834747314 = 0.1542242020368576 + 0.001 * 7.029846668243408
Epoch 310, val loss: 0.8026296496391296
Epoch 320, training loss: 0.14143238961696625 = 0.1344027817249298 + 0.001 * 7.029607772827148
Epoch 320, val loss: 0.8084171414375305
Epoch 330, training loss: 0.12440095841884613 = 0.11737076193094254 + 0.001 * 7.030196189880371
Epoch 330, val loss: 0.8152419328689575
Epoch 340, training loss: 0.10981575399637222 = 0.10278628766536713 + 0.001 * 7.029465198516846
Epoch 340, val loss: 0.822981059551239
Epoch 350, training loss: 0.09731557220220566 = 0.0902877151966095 + 0.001 * 7.027856349945068
Epoch 350, val loss: 0.8315020799636841
Epoch 360, training loss: 0.08657588809728622 = 0.07954797148704529 + 0.001 * 7.027919292449951
Epoch 360, val loss: 0.8405898213386536
Epoch 370, training loss: 0.07731460779905319 = 0.07028946280479431 + 0.001 * 7.025144577026367
Epoch 370, val loss: 0.8500992059707642
Epoch 380, training loss: 0.06931500136852264 = 0.06228923425078392 + 0.001 * 7.025763034820557
Epoch 380, val loss: 0.8598732352256775
Epoch 390, training loss: 0.06238763779401779 = 0.05536598339676857 + 0.001 * 7.021653652191162
Epoch 390, val loss: 0.8698034286499023
Epoch 400, training loss: 0.05638682097196579 = 0.04936961457133293 + 0.001 * 7.017206192016602
Epoch 400, val loss: 0.8798204660415649
Epoch 410, training loss: 0.051194868981838226 = 0.044171128422021866 + 0.001 * 7.023739337921143
Epoch 410, val loss: 0.889889657497406
Epoch 420, training loss: 0.04667302221059799 = 0.03966183960437775 + 0.001 * 7.011180400848389
Epoch 420, val loss: 0.8999747633934021
Epoch 430, training loss: 0.04274643585085869 = 0.03574371710419655 + 0.001 * 7.002717018127441
Epoch 430, val loss: 0.910010039806366
Epoch 440, training loss: 0.039344221353530884 = 0.03233280032873154 + 0.001 * 7.011420249938965
Epoch 440, val loss: 0.9199832081794739
Epoch 450, training loss: 0.03634880483150482 = 0.029356181621551514 + 0.001 * 6.992623805999756
Epoch 450, val loss: 0.9298511743545532
Epoch 460, training loss: 0.03375937417149544 = 0.02675137296319008 + 0.001 * 7.008001804351807
Epoch 460, val loss: 0.9396003484725952
Epoch 470, training loss: 0.03143909573554993 = 0.024464718997478485 + 0.001 * 6.974376678466797
Epoch 470, val loss: 0.9492105841636658
Epoch 480, training loss: 0.029448186978697777 = 0.02245071530342102 + 0.001 * 6.997471809387207
Epoch 480, val loss: 0.9586758017539978
Epoch 490, training loss: 0.027628954499959946 = 0.020670561119914055 + 0.001 * 6.958393573760986
Epoch 490, val loss: 0.967964231967926
Epoch 500, training loss: 0.02603725902736187 = 0.01909184455871582 + 0.001 * 6.9454145431518555
Epoch 500, val loss: 0.9770862460136414
Epoch 510, training loss: 0.02464621514081955 = 0.017686735838651657 + 0.001 * 6.959479808807373
Epoch 510, val loss: 0.9859882593154907
Epoch 520, training loss: 0.023377250880002975 = 0.016432058066129684 + 0.001 * 6.945192337036133
Epoch 520, val loss: 0.9946858882904053
Epoch 530, training loss: 0.02224155329167843 = 0.015307478606700897 + 0.001 * 6.934074878692627
Epoch 530, val loss: 1.0032060146331787
Epoch 540, training loss: 0.021258456632494926 = 0.014295981265604496 + 0.001 * 6.962475299835205
Epoch 540, val loss: 1.0115286111831665
Epoch 550, training loss: 0.020309606567025185 = 0.013383366167545319 + 0.001 * 6.92624044418335
Epoch 550, val loss: 1.0196410417556763
Epoch 560, training loss: 0.019488757476210594 = 0.012557456269860268 + 0.001 * 6.931301116943359
Epoch 560, val loss: 1.0275720357894897
Epoch 570, training loss: 0.018727857619524002 = 0.011807631701231003 + 0.001 * 6.920224666595459
Epoch 570, val loss: 1.0353200435638428
Epoch 580, training loss: 0.018035253509879112 = 0.011125222779810429 + 0.001 * 6.910030364990234
Epoch 580, val loss: 1.0428743362426758
Epoch 590, training loss: 0.017415929585695267 = 0.01050239335745573 + 0.001 * 6.91353702545166
Epoch 590, val loss: 1.050255298614502
Epoch 600, training loss: 0.01683088205754757 = 0.009932512417435646 + 0.001 * 6.898369789123535
Epoch 600, val loss: 1.0574495792388916
Epoch 610, training loss: 0.016309913247823715 = 0.009409897029399872 + 0.001 * 6.9000163078308105
Epoch 610, val loss: 1.064473032951355
Epoch 620, training loss: 0.015825625509023666 = 0.008929427713155746 + 0.001 * 6.89619779586792
Epoch 620, val loss: 1.0713250637054443
Epoch 630, training loss: 0.015381131321191788 = 0.008486761711537838 + 0.001 * 6.894368648529053
Epoch 630, val loss: 1.0780261754989624
Epoch 640, training loss: 0.014959979802370071 = 0.008078028447926044 + 0.001 * 6.881950855255127
Epoch 640, val loss: 1.08455491065979
Epoch 650, training loss: 0.014580972492694855 = 0.0076998258009552956 + 0.001 * 6.88114595413208
Epoch 650, val loss: 1.090941309928894
Epoch 660, training loss: 0.014226896688342094 = 0.007349252700805664 + 0.001 * 6.87764310836792
Epoch 660, val loss: 1.097177505493164
Epoch 670, training loss: 0.01389552466571331 = 0.0070236981846392155 + 0.001 * 6.871826171875
Epoch 670, val loss: 1.1032744646072388
Epoch 680, training loss: 0.013587359338998795 = 0.006720842327922583 + 0.001 * 6.86651611328125
Epoch 680, val loss: 1.1092345714569092
Epoch 690, training loss: 0.013295520097017288 = 0.0064386059530079365 + 0.001 * 6.856914043426514
Epoch 690, val loss: 1.1150548458099365
Epoch 700, training loss: 0.013057214207947254 = 0.006175100803375244 + 0.001 * 6.882112979888916
Epoch 700, val loss: 1.120732069015503
Epoch 710, training loss: 0.012808081693947315 = 0.005928801838308573 + 0.001 * 6.879279613494873
Epoch 710, val loss: 1.1262961626052856
Epoch 720, training loss: 0.0125545309856534 = 0.0056981975212693214 + 0.001 * 6.856333255767822
Epoch 720, val loss: 1.1317262649536133
Epoch 730, training loss: 0.012362964451313019 = 0.005481942091137171 + 0.001 * 6.881021499633789
Epoch 730, val loss: 1.137050747871399
Epoch 740, training loss: 0.01212693564593792 = 0.005278958007693291 + 0.001 * 6.847977161407471
Epoch 740, val loss: 1.1422674655914307
Epoch 750, training loss: 0.011935451999306679 = 0.00508815748617053 + 0.001 * 6.847293853759766
Epoch 750, val loss: 1.1473621129989624
Epoch 760, training loss: 0.011746124364435673 = 0.004908496513962746 + 0.001 * 6.837627410888672
Epoch 760, val loss: 1.1523538827896118
Epoch 770, training loss: 0.011583814397454262 = 0.004739183466881514 + 0.001 * 6.844630241394043
Epoch 770, val loss: 1.1572363376617432
Epoch 780, training loss: 0.011423936113715172 = 0.004579407628625631 + 0.001 * 6.8445281982421875
Epoch 780, val loss: 1.1620290279388428
Epoch 790, training loss: 0.011276056990027428 = 0.0044284602627158165 + 0.001 * 6.847596645355225
Epoch 790, val loss: 1.166711449623108
Epoch 800, training loss: 0.011119427159428596 = 0.00428572203963995 + 0.001 * 6.833704948425293
Epoch 800, val loss: 1.1713062524795532
Epoch 810, training loss: 0.010978681966662407 = 0.004150608088821173 + 0.001 * 6.828073978424072
Epoch 810, val loss: 1.17581045627594
Epoch 820, training loss: 0.010855589061975479 = 0.004022559151053429 + 0.001 * 6.833029747009277
Epoch 820, val loss: 1.1802310943603516
Epoch 830, training loss: 0.010733732022345066 = 0.0039010716136544943 + 0.001 * 6.83266019821167
Epoch 830, val loss: 1.1845569610595703
Epoch 840, training loss: 0.010657791048288345 = 0.003785723354667425 + 0.001 * 6.872066974639893
Epoch 840, val loss: 1.18880295753479
Epoch 850, training loss: 0.010506616905331612 = 0.003676116233691573 + 0.001 * 6.83050012588501
Epoch 850, val loss: 1.1929658651351929
Epoch 860, training loss: 0.010400579310953617 = 0.003571879118680954 + 0.001 * 6.828700065612793
Epoch 860, val loss: 1.1970552206039429
Epoch 870, training loss: 0.01029148418456316 = 0.003472612937912345 + 0.001 * 6.818871021270752
Epoch 870, val loss: 1.2010667324066162
Epoch 880, training loss: 0.010201053693890572 = 0.003378049936145544 + 0.001 * 6.823003768920898
Epoch 880, val loss: 1.205004096031189
Epoch 890, training loss: 0.010108320973813534 = 0.003287907689809799 + 0.001 * 6.820413112640381
Epoch 890, val loss: 1.2088632583618164
Epoch 900, training loss: 0.010013413615524769 = 0.0032019014470279217 + 0.001 * 6.811511993408203
Epoch 900, val loss: 1.2126600742340088
Epoch 910, training loss: 0.00995252188295126 = 0.0031197669450193644 + 0.001 * 6.832754611968994
Epoch 910, val loss: 1.2163829803466797
Epoch 920, training loss: 0.009843192063272 = 0.003041301853954792 + 0.001 * 6.801889896392822
Epoch 920, val loss: 1.22003972530365
Epoch 930, training loss: 0.009771015495061874 = 0.002966261235997081 + 0.001 * 6.80475378036499
Epoch 930, val loss: 1.223619818687439
Epoch 940, training loss: 0.009699450805783272 = 0.00289445323869586 + 0.001 * 6.804997444152832
Epoch 940, val loss: 1.2271465063095093
Epoch 950, training loss: 0.009636704809963703 = 0.0028257069643586874 + 0.001 * 6.81099796295166
Epoch 950, val loss: 1.2305940389633179
Epoch 960, training loss: 0.009553328156471252 = 0.002759850351139903 + 0.001 * 6.7934770584106445
Epoch 960, val loss: 1.2339998483657837
Epoch 970, training loss: 0.009503115899860859 = 0.0026967120356857777 + 0.001 * 6.806403636932373
Epoch 970, val loss: 1.237338662147522
Epoch 980, training loss: 0.009438995271921158 = 0.00263614347204566 + 0.001 * 6.80285120010376
Epoch 980, val loss: 1.2406190633773804
Epoch 990, training loss: 0.00938126165419817 = 0.002578006824478507 + 0.001 * 6.803254127502441
Epoch 990, val loss: 1.2438393831253052
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.9262
Flip ASR: 0.9111/225 nodes
The final ASR:0.72079, 0.15352, Accuracy:0.80123, 0.00761
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11580])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10486])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98770, 0.00969, Accuracy:0.83333, 0.00907
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9514747858047485 = 1.943100929260254 + 0.001 * 8.373893737792969
Epoch 0, val loss: 1.9340139627456665
Epoch 10, training loss: 1.9407341480255127 = 1.932360291481018 + 0.001 * 8.373866081237793
Epoch 10, val loss: 1.9239625930786133
Epoch 20, training loss: 1.928025484085083 = 1.919651746749878 + 0.001 * 8.373708724975586
Epoch 20, val loss: 1.9116332530975342
Epoch 30, training loss: 1.9106152057647705 = 1.902241826057434 + 0.001 * 8.373333930969238
Epoch 30, val loss: 1.894215703010559
Epoch 40, training loss: 1.8856176137924194 = 1.8772450685501099 + 0.001 * 8.37248706817627
Epoch 40, val loss: 1.869035005569458
Epoch 50, training loss: 1.851426601409912 = 1.843056321144104 + 0.001 * 8.370269775390625
Epoch 50, val loss: 1.8360841274261475
Epoch 60, training loss: 1.8129807710647583 = 1.8046177625656128 + 0.001 * 8.363064765930176
Epoch 60, val loss: 1.8034961223602295
Epoch 70, training loss: 1.7781527042388916 = 1.7698196172714233 + 0.001 * 8.333045959472656
Epoch 70, val loss: 1.7791941165924072
Epoch 80, training loss: 1.7349070310592651 = 1.7267663478851318 + 0.001 * 8.140677452087402
Epoch 80, val loss: 1.7466100454330444
Epoch 90, training loss: 1.6736680269241333 = 1.6658213138580322 + 0.001 * 7.846696376800537
Epoch 90, val loss: 1.696797490119934
Epoch 100, training loss: 1.5911586284637451 = 1.583461880683899 + 0.001 * 7.6967902183532715
Epoch 100, val loss: 1.6288572549819946
Epoch 110, training loss: 1.4932446479797363 = 1.4857267141342163 + 0.001 * 7.517888069152832
Epoch 110, val loss: 1.550865888595581
Epoch 120, training loss: 1.3896769285202026 = 1.3822226524353027 + 0.001 * 7.4542317390441895
Epoch 120, val loss: 1.4721568822860718
Epoch 130, training loss: 1.284247636795044 = 1.2768077850341797 + 0.001 * 7.439861297607422
Epoch 130, val loss: 1.3942233324050903
Epoch 140, training loss: 1.1769675016403198 = 1.1695598363876343 + 0.001 * 7.407682418823242
Epoch 140, val loss: 1.3146617412567139
Epoch 150, training loss: 1.0689778327941895 = 1.0616008043289185 + 0.001 * 7.377050399780273
Epoch 150, val loss: 1.2333571910858154
Epoch 160, training loss: 0.9626166224479675 = 0.9552867412567139 + 0.001 * 7.3298516273498535
Epoch 160, val loss: 1.1525031328201294
Epoch 170, training loss: 0.8605398535728455 = 0.8532662391662598 + 0.001 * 7.273603916168213
Epoch 170, val loss: 1.0761215686798096
Epoch 180, training loss: 0.7661048173904419 = 0.7588655352592468 + 0.001 * 7.239299297332764
Epoch 180, val loss: 1.007467269897461
Epoch 190, training loss: 0.6820719242095947 = 0.6748577356338501 + 0.001 * 7.214166641235352
Epoch 190, val loss: 0.9493114352226257
Epoch 200, training loss: 0.6091271042823792 = 0.6019410490989685 + 0.001 * 7.18604850769043
Epoch 200, val loss: 0.902561366558075
Epoch 210, training loss: 0.5458700656890869 = 0.5387082099914551 + 0.001 * 7.161828994750977
Epoch 210, val loss: 0.8664591312408447
Epoch 220, training loss: 0.48996537923812866 = 0.4828251302242279 + 0.001 * 7.140242099761963
Epoch 220, val loss: 0.838498055934906
Epoch 230, training loss: 0.43905895948410034 = 0.4319353997707367 + 0.001 * 7.1235504150390625
Epoch 230, val loss: 0.8161883354187012
Epoch 240, training loss: 0.3914199769496918 = 0.3843068778514862 + 0.001 * 7.113084316253662
Epoch 240, val loss: 0.7977842688560486
Epoch 250, training loss: 0.3463013172149658 = 0.3391943871974945 + 0.001 * 7.106939315795898
Epoch 250, val loss: 0.7823165655136108
Epoch 260, training loss: 0.3036624789237976 = 0.29655712842941284 + 0.001 * 7.1053595542907715
Epoch 260, val loss: 0.7694084644317627
Epoch 270, training loss: 0.2639894187450409 = 0.25688663125038147 + 0.001 * 7.102783203125
Epoch 270, val loss: 0.7592552900314331
Epoch 280, training loss: 0.22789056599140167 = 0.22078870236873627 + 0.001 * 7.101868629455566
Epoch 280, val loss: 0.752443253993988
Epoch 290, training loss: 0.19593366980552673 = 0.1888330578804016 + 0.001 * 7.100614070892334
Epoch 290, val loss: 0.7495169639587402
Epoch 300, training loss: 0.168378084897995 = 0.16127866506576538 + 0.001 * 7.099416255950928
Epoch 300, val loss: 0.7507842183113098
Epoch 310, training loss: 0.14515630900859833 = 0.13805659115314484 + 0.001 * 7.09971284866333
Epoch 310, val loss: 0.7562985420227051
Epoch 320, training loss: 0.12591475248336792 = 0.11881578713655472 + 0.001 * 7.098962783813477
Epoch 320, val loss: 0.7657907605171204
Epoch 330, training loss: 0.1100851371884346 = 0.1029878482222557 + 0.001 * 7.097285270690918
Epoch 330, val loss: 0.7785437703132629
Epoch 340, training loss: 0.0970425084233284 = 0.08994659781455994 + 0.001 * 7.09591007232666
Epoch 340, val loss: 0.7937303185462952
Epoch 350, training loss: 0.08621981739997864 = 0.07912519574165344 + 0.001 * 7.0946221351623535
Epoch 350, val loss: 0.8106155395507812
Epoch 360, training loss: 0.07715705782175064 = 0.07006163150072098 + 0.001 * 7.0954270362854
Epoch 360, val loss: 0.828585147857666
Epoch 370, training loss: 0.06948709487915039 = 0.06239495053887367 + 0.001 * 7.092147350311279
Epoch 370, val loss: 0.8471559286117554
Epoch 380, training loss: 0.06294289976358414 = 0.0558505579829216 + 0.001 * 7.092338562011719
Epoch 380, val loss: 0.865996778011322
Epoch 390, training loss: 0.05730919539928436 = 0.05021947622299194 + 0.001 * 7.089716911315918
Epoch 390, val loss: 0.884894073009491
Epoch 400, training loss: 0.052430905401706696 = 0.04534253850579262 + 0.001 * 7.088367462158203
Epoch 400, val loss: 0.9037099480628967
Epoch 410, training loss: 0.04818406701087952 = 0.041097015142440796 + 0.001 * 7.087049961090088
Epoch 410, val loss: 0.9222809076309204
Epoch 420, training loss: 0.04446854069828987 = 0.037384431809186935 + 0.001 * 7.084107398986816
Epoch 420, val loss: 0.9405009150505066
Epoch 430, training loss: 0.04120515659451485 = 0.03412475436925888 + 0.001 * 7.080402374267578
Epoch 430, val loss: 0.9583103656768799
Epoch 440, training loss: 0.03837473317980766 = 0.03125203773379326 + 0.001 * 7.122694969177246
Epoch 440, val loss: 0.9756907224655151
Epoch 450, training loss: 0.03579748421907425 = 0.028711048886179924 + 0.001 * 7.086435794830322
Epoch 450, val loss: 0.9926177859306335
Epoch 460, training loss: 0.03353329747915268 = 0.02645539864897728 + 0.001 * 7.0778985023498535
Epoch 460, val loss: 1.0090606212615967
Epoch 470, training loss: 0.03151983022689819 = 0.024446016177535057 + 0.001 * 7.073814868927002
Epoch 470, val loss: 1.0250364542007446
Epoch 480, training loss: 0.0297185517847538 = 0.02264990843832493 + 0.001 * 7.068643093109131
Epoch 480, val loss: 1.0405378341674805
Epoch 490, training loss: 0.028102559968829155 = 0.021039092913269997 + 0.001 * 7.063467025756836
Epoch 490, val loss: 1.0555944442749023
Epoch 500, training loss: 0.026695357635617256 = 0.019590100273489952 + 0.001 * 7.1052565574646
Epoch 500, val loss: 1.0702075958251953
Epoch 510, training loss: 0.025353915989398956 = 0.018282899633049965 + 0.001 * 7.07101583480835
Epoch 510, val loss: 1.0843991041183472
Epoch 520, training loss: 0.024154270067811012 = 0.01710015907883644 + 0.001 * 7.054111003875732
Epoch 520, val loss: 1.0981913805007935
Epoch 530, training loss: 0.023086706176400185 = 0.01602732390165329 + 0.001 * 7.059381484985352
Epoch 530, val loss: 1.1116095781326294
Epoch 540, training loss: 0.022092651575803757 = 0.015051853843033314 + 0.001 * 7.040797233581543
Epoch 540, val loss: 1.124630093574524
Epoch 550, training loss: 0.021225523203611374 = 0.014162613078951836 + 0.001 * 7.062910079956055
Epoch 550, val loss: 1.137313723564148
Epoch 560, training loss: 0.020387761294841766 = 0.01335011888295412 + 0.001 * 7.037642002105713
Epoch 560, val loss: 1.1496702432632446
Epoch 570, training loss: 0.019630372524261475 = 0.012606102041900158 + 0.001 * 7.024270057678223
Epoch 570, val loss: 1.1617008447647095
Epoch 580, training loss: 0.0189522635191679 = 0.011923247016966343 + 0.001 * 7.02901554107666
Epoch 580, val loss: 1.1734191179275513
Epoch 590, training loss: 0.018321583047509193 = 0.011294896714389324 + 0.001 * 7.02668571472168
Epoch 590, val loss: 1.1848056316375732
Epoch 600, training loss: 0.017735131084918976 = 0.010714828036725521 + 0.001 * 7.020301818847656
Epoch 600, val loss: 1.1959081888198853
Epoch 610, training loss: 0.017173511907458305 = 0.010177943855524063 + 0.001 * 6.99556827545166
Epoch 610, val loss: 1.206721544265747
Epoch 620, training loss: 0.01669241487979889 = 0.009679561480879784 + 0.001 * 7.012852191925049
Epoch 620, val loss: 1.217285394668579
Epoch 630, training loss: 0.016201552003622055 = 0.009215986356139183 + 0.001 * 6.985566139221191
Epoch 630, val loss: 1.2275843620300293
Epoch 640, training loss: 0.015764232724905014 = 0.008783768862485886 + 0.001 * 6.980462551116943
Epoch 640, val loss: 1.2376760244369507
Epoch 650, training loss: 0.015377409756183624 = 0.008379955776035786 + 0.001 * 6.997453689575195
Epoch 650, val loss: 1.2475645542144775
Epoch 660, training loss: 0.014990651980042458 = 0.008002893067896366 + 0.001 * 6.987758636474609
Epoch 660, val loss: 1.2572238445281982
Epoch 670, training loss: 0.014693306758999825 = 0.007650110870599747 + 0.001 * 7.0431952476501465
Epoch 670, val loss: 1.2667160034179688
Epoch 680, training loss: 0.014287339523434639 = 0.00732017494738102 + 0.001 * 6.967164516448975
Epoch 680, val loss: 1.2759824991226196
Epoch 690, training loss: 0.014009758830070496 = 0.007010908331722021 + 0.001 * 6.998849868774414
Epoch 690, val loss: 1.2850712537765503
Epoch 700, training loss: 0.013702522963285446 = 0.006721073761582375 + 0.001 * 6.981448173522949
Epoch 700, val loss: 1.2939574718475342
Epoch 710, training loss: 0.013394258916378021 = 0.006449142005294561 + 0.001 * 6.9451165199279785
Epoch 710, val loss: 1.3027122020721436
Epoch 720, training loss: 0.013130705803632736 = 0.0061936951242387295 + 0.001 * 6.937010765075684
Epoch 720, val loss: 1.3112754821777344
Epoch 730, training loss: 0.012898491695523262 = 0.005953541025519371 + 0.001 * 6.944950580596924
Epoch 730, val loss: 1.3196609020233154
Epoch 740, training loss: 0.01265012752264738 = 0.0057275197468698025 + 0.001 * 6.922607421875
Epoch 740, val loss: 1.3278990983963013
Epoch 750, training loss: 0.01244615763425827 = 0.005514765623956919 + 0.001 * 6.931391716003418
Epoch 750, val loss: 1.3359923362731934
Epoch 760, training loss: 0.012257302179932594 = 0.005314288195222616 + 0.001 * 6.9430131912231445
Epoch 760, val loss: 1.3438736200332642
Epoch 770, training loss: 0.012042723596096039 = 0.005125121213495731 + 0.001 * 6.9176025390625
Epoch 770, val loss: 1.3516408205032349
Epoch 780, training loss: 0.011876603588461876 = 0.004946361295878887 + 0.001 * 6.930241584777832
Epoch 780, val loss: 1.3592443466186523
Epoch 790, training loss: 0.011699151247739792 = 0.0047773635014891624 + 0.001 * 6.921787738800049
Epoch 790, val loss: 1.3666993379592896
Epoch 800, training loss: 0.011601867154240608 = 0.004617433995008469 + 0.001 * 6.984432697296143
Epoch 800, val loss: 1.3740650415420532
Epoch 810, training loss: 0.011376027017831802 = 0.004466040059924126 + 0.001 * 6.90998649597168
Epoch 810, val loss: 1.3812390565872192
Epoch 820, training loss: 0.011258897371590137 = 0.004322588909417391 + 0.001 * 6.936307907104492
Epoch 820, val loss: 1.3882696628570557
Epoch 830, training loss: 0.011078765615820885 = 0.004186610225588083 + 0.001 * 6.892155647277832
Epoch 830, val loss: 1.3951711654663086
Epoch 840, training loss: 0.010984089225530624 = 0.004057513549923897 + 0.001 * 6.926575183868408
Epoch 840, val loss: 1.4019272327423096
Epoch 850, training loss: 0.01082792691886425 = 0.003934888169169426 + 0.001 * 6.893038272857666
Epoch 850, val loss: 1.408588171005249
Epoch 860, training loss: 0.0108055230230093 = 0.0038183098658919334 + 0.001 * 6.987213134765625
Epoch 860, val loss: 1.4151297807693481
Epoch 870, training loss: 0.010592065751552582 = 0.00370736513286829 + 0.001 * 6.884699821472168
Epoch 870, val loss: 1.42153000831604
Epoch 880, training loss: 0.010490513406693935 = 0.003601678879931569 + 0.001 * 6.888833999633789
Epoch 880, val loss: 1.427812933921814
Epoch 890, training loss: 0.010431937873363495 = 0.003500712336972356 + 0.001 * 6.931224822998047
Epoch 890, val loss: 1.434032678604126
Epoch 900, training loss: 0.010266994126141071 = 0.0034037360455840826 + 0.001 * 6.86325740814209
Epoch 900, val loss: 1.440140962600708
Epoch 910, training loss: 0.010172814130783081 = 0.003309811232611537 + 0.001 * 6.863002777099609
Epoch 910, val loss: 1.4462010860443115
Epoch 920, training loss: 0.010106157511472702 = 0.003218225669115782 + 0.001 * 6.887930870056152
Epoch 920, val loss: 1.4522899389266968
Epoch 930, training loss: 0.010012594982981682 = 0.0031289737671613693 + 0.001 * 6.883620738983154
Epoch 930, val loss: 1.458318829536438
Epoch 940, training loss: 0.009896881878376007 = 0.003042412456125021 + 0.001 * 6.8544697761535645
Epoch 940, val loss: 1.4643241167068481
Epoch 950, training loss: 0.009815430268645287 = 0.0029585440643131733 + 0.001 * 6.856886386871338
Epoch 950, val loss: 1.470299482345581
Epoch 960, training loss: 0.009764659218490124 = 0.002877238905057311 + 0.001 * 6.887419700622559
Epoch 960, val loss: 1.4762674570083618
Epoch 970, training loss: 0.00967241171747446 = 0.00279886182397604 + 0.001 * 6.873549461364746
Epoch 970, val loss: 1.4821772575378418
Epoch 980, training loss: 0.00958782248198986 = 0.002723152982071042 + 0.001 * 6.864669322967529
Epoch 980, val loss: 1.488121509552002
Epoch 990, training loss: 0.009530709125101566 = 0.0026502604596316814 + 0.001 * 6.880448341369629
Epoch 990, val loss: 1.4939603805541992
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.5092
Flip ASR: 0.4311/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9638047218322754 = 1.9554308652877808 + 0.001 * 8.37382698059082
Epoch 0, val loss: 1.9591541290283203
Epoch 10, training loss: 1.9523253440856934 = 1.9439516067504883 + 0.001 * 8.373697280883789
Epoch 10, val loss: 1.9474303722381592
Epoch 20, training loss: 1.9379918575286865 = 1.9296185970306396 + 0.001 * 8.373292922973633
Epoch 20, val loss: 1.9320188760757446
Epoch 30, training loss: 1.917999505996704 = 1.909627079963684 + 0.001 * 8.372416496276855
Epoch 30, val loss: 1.909773588180542
Epoch 40, training loss: 1.8891258239746094 = 1.8807551860809326 + 0.001 * 8.37065601348877
Epoch 40, val loss: 1.8774827718734741
Epoch 50, training loss: 1.8496109247207642 = 1.8412439823150635 + 0.001 * 8.366893768310547
Epoch 50, val loss: 1.8347338438034058
Epoch 60, training loss: 1.803885817527771 = 1.7955290079116821 + 0.001 * 8.356829643249512
Epoch 60, val loss: 1.788827657699585
Epoch 70, training loss: 1.7598963975906372 = 1.7515878677368164 + 0.001 * 8.308548927307129
Epoch 70, val loss: 1.7482413053512573
Epoch 80, training loss: 1.7089200019836426 = 1.7010380029678345 + 0.001 * 7.882025241851807
Epoch 80, val loss: 1.7030441761016846
Epoch 90, training loss: 1.6400129795074463 = 1.6323853731155396 + 0.001 * 7.627593517303467
Epoch 90, val loss: 1.6445930004119873
Epoch 100, training loss: 1.5508763790130615 = 1.5433610677719116 + 0.001 * 7.515308380126953
Epoch 100, val loss: 1.5712685585021973
Epoch 110, training loss: 1.4492051601409912 = 1.4417874813079834 + 0.001 * 7.4176530838012695
Epoch 110, val loss: 1.4909952878952026
Epoch 120, training loss: 1.349275827407837 = 1.3419185876846313 + 0.001 * 7.357247829437256
Epoch 120, val loss: 1.4164396524429321
Epoch 130, training loss: 1.256014347076416 = 1.2487084865570068 + 0.001 * 7.305800437927246
Epoch 130, val loss: 1.3516287803649902
Epoch 140, training loss: 1.169431447982788 = 1.1621662378311157 + 0.001 * 7.265166759490967
Epoch 140, val loss: 1.2948359251022339
Epoch 150, training loss: 1.0891289710998535 = 1.0819097757339478 + 0.001 * 7.219200611114502
Epoch 150, val loss: 1.2437586784362793
Epoch 160, training loss: 1.0141714811325073 = 1.007010579109192 + 0.001 * 7.160959243774414
Epoch 160, val loss: 1.1964198350906372
Epoch 170, training loss: 0.9440061450004578 = 0.9368886947631836 + 0.001 * 7.117470741271973
Epoch 170, val loss: 1.1517257690429688
Epoch 180, training loss: 0.8782025575637817 = 0.8711012005805969 + 0.001 * 7.101382732391357
Epoch 180, val loss: 1.109826683998108
Epoch 190, training loss: 0.8157985806465149 = 0.8087037801742554 + 0.001 * 7.094786643981934
Epoch 190, val loss: 1.0695849657058716
Epoch 200, training loss: 0.7558246850967407 = 0.7487372159957886 + 0.001 * 7.087446212768555
Epoch 200, val loss: 1.0303988456726074
Epoch 210, training loss: 0.6975753903388977 = 0.6904991269111633 + 0.001 * 7.076269149780273
Epoch 210, val loss: 0.9927079677581787
Epoch 220, training loss: 0.6406677961349487 = 0.633608341217041 + 0.001 * 7.059473037719727
Epoch 220, val loss: 0.9563019871711731
Epoch 230, training loss: 0.5852343440055847 = 0.5781973600387573 + 0.001 * 7.036979675292969
Epoch 230, val loss: 0.9218530058860779
Epoch 240, training loss: 0.5318523645401001 = 0.5248399376869202 + 0.001 * 7.012443542480469
Epoch 240, val loss: 0.8909869194030762
Epoch 250, training loss: 0.48126035928726196 = 0.47426894307136536 + 0.001 * 6.991420269012451
Epoch 250, val loss: 0.8656127452850342
Epoch 260, training loss: 0.43401315808296204 = 0.42703452706336975 + 0.001 * 6.978616237640381
Epoch 260, val loss: 0.8467604517936707
Epoch 270, training loss: 0.39023271203041077 = 0.38325992226600647 + 0.001 * 6.972797393798828
Epoch 270, val loss: 0.8344770073890686
Epoch 280, training loss: 0.3498099148273468 = 0.34284189343452454 + 0.001 * 6.968028545379639
Epoch 280, val loss: 0.8282872438430786
Epoch 290, training loss: 0.3127509653568268 = 0.30578991770744324 + 0.001 * 6.961061954498291
Epoch 290, val loss: 0.8272284269332886
Epoch 300, training loss: 0.279084175825119 = 0.272125780582428 + 0.001 * 6.958395481109619
Epoch 300, val loss: 0.8303778767585754
Epoch 310, training loss: 0.24890761077404022 = 0.24195680022239685 + 0.001 * 6.950811386108398
Epoch 310, val loss: 0.8371477127075195
Epoch 320, training loss: 0.22217179834842682 = 0.21522797644138336 + 0.001 * 6.943828582763672
Epoch 320, val loss: 0.8471260070800781
Epoch 330, training loss: 0.19867448508739471 = 0.1917342096567154 + 0.001 * 6.940272331237793
Epoch 330, val loss: 0.859698474407196
Epoch 340, training loss: 0.17813363671302795 = 0.17120544612407684 + 0.001 * 6.928186416625977
Epoch 340, val loss: 0.8743851780891418
Epoch 350, training loss: 0.1602044254541397 = 0.15328305959701538 + 0.001 * 6.921359539031982
Epoch 350, val loss: 0.8908539414405823
Epoch 360, training loss: 0.1445377916097641 = 0.137620747089386 + 0.001 * 6.917038440704346
Epoch 360, val loss: 0.9083463549613953
Epoch 370, training loss: 0.1308036893606186 = 0.12389542907476425 + 0.001 * 6.908255100250244
Epoch 370, val loss: 0.926605224609375
Epoch 380, training loss: 0.11874735355377197 = 0.1118244007229805 + 0.001 * 6.9229536056518555
Epoch 380, val loss: 0.9451680183410645
Epoch 390, training loss: 0.10804059356451035 = 0.10114091634750366 + 0.001 * 6.899680137634277
Epoch 390, val loss: 0.9639347791671753
Epoch 400, training loss: 0.09857238084077835 = 0.09166984260082245 + 0.001 * 6.9025373458862305
Epoch 400, val loss: 0.9825796484947205
Epoch 410, training loss: 0.0901380106806755 = 0.08325185626745224 + 0.001 * 6.886153221130371
Epoch 410, val loss: 1.0011764764785767
Epoch 420, training loss: 0.08260795474052429 = 0.07572692632675171 + 0.001 * 6.8810319900512695
Epoch 420, val loss: 1.0195534229278564
Epoch 430, training loss: 0.07584045082330704 = 0.06896285712718964 + 0.001 * 6.8775954246521
Epoch 430, val loss: 1.0375782251358032
Epoch 440, training loss: 0.06975137442350388 = 0.06286441534757614 + 0.001 * 6.886960983276367
Epoch 440, val loss: 1.0551233291625977
Epoch 450, training loss: 0.06422789394855499 = 0.057360317558050156 + 0.001 * 6.867574214935303
Epoch 450, val loss: 1.072323203086853
Epoch 460, training loss: 0.0592438280582428 = 0.052374862134456635 + 0.001 * 6.8689656257629395
Epoch 460, val loss: 1.0890564918518066
Epoch 470, training loss: 0.054727934300899506 = 0.047848623245954514 + 0.001 * 6.879312515258789
Epoch 470, val loss: 1.105506181716919
Epoch 480, training loss: 0.05060115456581116 = 0.04374251514673233 + 0.001 * 6.858641147613525
Epoch 480, val loss: 1.121593952178955
Epoch 490, training loss: 0.04689165577292442 = 0.040033888071775436 + 0.001 * 6.857767105102539
Epoch 490, val loss: 1.1376146078109741
Epoch 500, training loss: 0.04355809465050697 = 0.03670520707964897 + 0.001 * 6.85288667678833
Epoch 500, val loss: 1.1536245346069336
Epoch 510, training loss: 0.040596358478069305 = 0.033719904720783234 + 0.001 * 6.8764543533325195
Epoch 510, val loss: 1.1692289113998413
Epoch 520, training loss: 0.03790856525301933 = 0.031050793826580048 + 0.001 * 6.8577704429626465
Epoch 520, val loss: 1.184340000152588
Epoch 530, training loss: 0.03551650419831276 = 0.028664547950029373 + 0.001 * 6.851957321166992
Epoch 530, val loss: 1.1988130807876587
Epoch 540, training loss: 0.03336399048566818 = 0.026511823758482933 + 0.001 * 6.852168083190918
Epoch 540, val loss: 1.2130661010742188
Epoch 550, training loss: 0.03142336755990982 = 0.024573111906647682 + 0.001 * 6.850257396697998
Epoch 550, val loss: 1.2268208265304565
Epoch 560, training loss: 0.02965683862566948 = 0.022812288254499435 + 0.001 * 6.844550132751465
Epoch 560, val loss: 1.2399722337722778
Epoch 570, training loss: 0.028060242533683777 = 0.02121991291642189 + 0.001 * 6.840328693389893
Epoch 570, val loss: 1.2526928186416626
Epoch 580, training loss: 0.02661415934562683 = 0.019772974774241447 + 0.001 * 6.841185092926025
Epoch 580, val loss: 1.265119194984436
Epoch 590, training loss: 0.025294633582234383 = 0.018450399860739708 + 0.001 * 6.844233989715576
Epoch 590, val loss: 1.277048945426941
Epoch 600, training loss: 0.02406342327594757 = 0.017225196585059166 + 0.001 * 6.838225841522217
Epoch 600, val loss: 1.2888152599334717
Epoch 610, training loss: 0.022955767810344696 = 0.016122518107295036 + 0.001 * 6.833250522613525
Epoch 610, val loss: 1.2997899055480957
Epoch 620, training loss: 0.021948181092739105 = 0.015115645714104176 + 0.001 * 6.832534313201904
Epoch 620, val loss: 1.310773253440857
Epoch 630, training loss: 0.02102731354534626 = 0.01419756654649973 + 0.001 * 6.829746723175049
Epoch 630, val loss: 1.3213640451431274
Epoch 640, training loss: 0.020196273922920227 = 0.013356182724237442 + 0.001 * 6.840090751647949
Epoch 640, val loss: 1.3318895101547241
Epoch 650, training loss: 0.01942063868045807 = 0.012583410367369652 + 0.001 * 6.837228298187256
Epoch 650, val loss: 1.3421071767807007
Epoch 660, training loss: 0.018710870295763016 = 0.011872556991875172 + 0.001 * 6.838313579559326
Epoch 660, val loss: 1.3520368337631226
Epoch 670, training loss: 0.01804613135755062 = 0.01121736504137516 + 0.001 * 6.828765869140625
Epoch 670, val loss: 1.3616485595703125
Epoch 680, training loss: 0.017446985468268394 = 0.010613551363348961 + 0.001 * 6.833433151245117
Epoch 680, val loss: 1.3709819316864014
Epoch 690, training loss: 0.0168734323233366 = 0.010048845782876015 + 0.001 * 6.824586868286133
Epoch 690, val loss: 1.380096435546875
Epoch 700, training loss: 0.016354400664567947 = 0.00952267087996006 + 0.001 * 6.831728458404541
Epoch 700, val loss: 1.38899827003479
Epoch 710, training loss: 0.01586749032139778 = 0.00903583038598299 + 0.001 * 6.831660270690918
Epoch 710, val loss: 1.3975553512573242
Epoch 720, training loss: 0.015411438420414925 = 0.008588611148297787 + 0.001 * 6.822826385498047
Epoch 720, val loss: 1.4060016870498657
Epoch 730, training loss: 0.014992715790867805 = 0.008175436407327652 + 0.001 * 6.8172783851623535
Epoch 730, val loss: 1.414266586303711
Epoch 740, training loss: 0.014614680781960487 = 0.0077928598038852215 + 0.001 * 6.821821212768555
Epoch 740, val loss: 1.4223753213882446
Epoch 750, training loss: 0.014258647337555885 = 0.007436871994286776 + 0.001 * 6.821774482727051
Epoch 750, val loss: 1.4303765296936035
Epoch 760, training loss: 0.013928348198533058 = 0.007105313707143068 + 0.001 * 6.823034286499023
Epoch 760, val loss: 1.4380488395690918
Epoch 770, training loss: 0.013607773929834366 = 0.006796049885451794 + 0.001 * 6.811723709106445
Epoch 770, val loss: 1.4457061290740967
Epoch 780, training loss: 0.01332906261086464 = 0.0065067484974861145 + 0.001 * 6.822314262390137
Epoch 780, val loss: 1.4531149864196777
Epoch 790, training loss: 0.013044113293290138 = 0.006234605796635151 + 0.001 * 6.809507369995117
Epoch 790, val loss: 1.4603115320205688
Epoch 800, training loss: 0.012799892574548721 = 0.0059812297113239765 + 0.001 * 6.818662166595459
Epoch 800, val loss: 1.4672753810882568
Epoch 810, training loss: 0.012562825344502926 = 0.005743538029491901 + 0.001 * 6.819286823272705
Epoch 810, val loss: 1.474162220954895
Epoch 820, training loss: 0.012337777763605118 = 0.005520591977983713 + 0.001 * 6.817185401916504
Epoch 820, val loss: 1.4808831214904785
Epoch 830, training loss: 0.012110470794141293 = 0.005310095846652985 + 0.001 * 6.800374507904053
Epoch 830, val loss: 1.487398624420166
Epoch 840, training loss: 0.01191877294331789 = 0.005111878737807274 + 0.001 * 6.806893825531006
Epoch 840, val loss: 1.4938573837280273
Epoch 850, training loss: 0.011730114929378033 = 0.004925233777612448 + 0.001 * 6.804880619049072
Epoch 850, val loss: 1.5000150203704834
Epoch 860, training loss: 0.011547999456524849 = 0.004749417770653963 + 0.001 * 6.798581123352051
Epoch 860, val loss: 1.5061109066009521
Epoch 870, training loss: 0.011387508362531662 = 0.004584112670272589 + 0.001 * 6.8033952713012695
Epoch 870, val loss: 1.5121204853057861
Epoch 880, training loss: 0.011225216090679169 = 0.004428582265973091 + 0.001 * 6.796633720397949
Epoch 880, val loss: 1.5179630517959595
Epoch 890, training loss: 0.011080768890678883 = 0.004281503148376942 + 0.001 * 6.799265384674072
Epoch 890, val loss: 1.5236793756484985
Epoch 900, training loss: 0.010941209271550179 = 0.004142474383115768 + 0.001 * 6.79873514175415
Epoch 900, val loss: 1.5292365550994873
Epoch 910, training loss: 0.010801409371197224 = 0.004010975360870361 + 0.001 * 6.790433883666992
Epoch 910, val loss: 1.5347050428390503
Epoch 920, training loss: 0.01069399993866682 = 0.003886414924636483 + 0.001 * 6.807584285736084
Epoch 920, val loss: 1.5400044918060303
Epoch 930, training loss: 0.010577653534710407 = 0.003768407041206956 + 0.001 * 6.809246063232422
Epoch 930, val loss: 1.5452378988265991
Epoch 940, training loss: 0.010457664728164673 = 0.003656448097899556 + 0.001 * 6.8012166023254395
Epoch 940, val loss: 1.5504043102264404
Epoch 950, training loss: 0.01034424640238285 = 0.003550149966031313 + 0.001 * 6.79409646987915
Epoch 950, val loss: 1.555417776107788
Epoch 960, training loss: 0.0102354995906353 = 0.0034491640981286764 + 0.001 * 6.786335468292236
Epoch 960, val loss: 1.5603795051574707
Epoch 970, training loss: 0.010133035480976105 = 0.0033525358885526657 + 0.001 * 6.780499458312988
Epoch 970, val loss: 1.565176248550415
Epoch 980, training loss: 0.010042019188404083 = 0.003260379657149315 + 0.001 * 6.781639099121094
Epoch 980, val loss: 1.569952130317688
Epoch 990, training loss: 0.00995483435690403 = 0.0031725193839520216 + 0.001 * 6.782314300537109
Epoch 990, val loss: 1.5744811296463013
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7593
Overall ASR: 0.8081
Flip ASR: 0.7778/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.958176851272583 = 1.9498029947280884 + 0.001 * 8.373869895935059
Epoch 0, val loss: 1.9499872922897339
Epoch 10, training loss: 1.9477671384811401 = 1.939393401145935 + 0.001 * 8.373791694641113
Epoch 10, val loss: 1.9400821924209595
Epoch 20, training loss: 1.9347225427627563 = 1.9263489246368408 + 0.001 * 8.373592376708984
Epoch 20, val loss: 1.9273560047149658
Epoch 30, training loss: 1.9161248207092285 = 1.9077515602111816 + 0.001 * 8.373208045959473
Epoch 30, val loss: 1.909022331237793
Epoch 40, training loss: 1.8886464834213257 = 1.8802740573883057 + 0.001 * 8.372384071350098
Epoch 40, val loss: 1.8824374675750732
Epoch 50, training loss: 1.8503706455230713 = 1.8420004844665527 + 0.001 * 8.3701753616333
Epoch 50, val loss: 1.8471978902816772
Epoch 60, training loss: 1.8067734241485596 = 1.7984113693237305 + 0.001 * 8.362060546875
Epoch 60, val loss: 1.810287356376648
Epoch 70, training loss: 1.766504168510437 = 1.7581835985183716 + 0.001 * 8.320611000061035
Epoch 70, val loss: 1.7765392065048218
Epoch 80, training loss: 1.71682608127594 = 1.708798885345459 + 0.001 * 8.027249336242676
Epoch 80, val loss: 1.7309584617614746
Epoch 90, training loss: 1.6487236022949219 = 1.6409165859222412 + 0.001 * 7.807013511657715
Epoch 90, val loss: 1.6722978353500366
Epoch 100, training loss: 1.558948278427124 = 1.5512233972549438 + 0.001 * 7.724929332733154
Epoch 100, val loss: 1.5996447801589966
Epoch 110, training loss: 1.4550801515579224 = 1.4474703073501587 + 0.001 * 7.609845161437988
Epoch 110, val loss: 1.517099142074585
Epoch 120, training loss: 1.3497741222381592 = 1.3423793315887451 + 0.001 * 7.394815921783447
Epoch 120, val loss: 1.436509609222412
Epoch 130, training loss: 1.2503654956817627 = 1.2430531978607178 + 0.001 * 7.312295436859131
Epoch 130, val loss: 1.3631601333618164
Epoch 140, training loss: 1.1578199863433838 = 1.1505460739135742 + 0.001 * 7.273909568786621
Epoch 140, val loss: 1.297480583190918
Epoch 150, training loss: 1.0726945400238037 = 1.0654629468917847 + 0.001 * 7.231624126434326
Epoch 150, val loss: 1.2374918460845947
Epoch 160, training loss: 0.9953439831733704 = 0.9881623983383179 + 0.001 * 7.181581020355225
Epoch 160, val loss: 1.1824332475662231
Epoch 170, training loss: 0.9245622754096985 = 0.9174352884292603 + 0.001 * 7.126979351043701
Epoch 170, val loss: 1.131196141242981
Epoch 180, training loss: 0.8573278188705444 = 0.8502465486526489 + 0.001 * 7.0812811851501465
Epoch 180, val loss: 1.0814032554626465
Epoch 190, training loss: 0.7903730869293213 = 0.783320963382721 + 0.001 * 7.052140235900879
Epoch 190, val loss: 1.0307329893112183
Epoch 200, training loss: 0.7224628925323486 = 0.7154315710067749 + 0.001 * 7.031301975250244
Epoch 200, val loss: 0.978121817111969
Epoch 210, training loss: 0.6550270318984985 = 0.6480099558830261 + 0.001 * 7.017080307006836
Epoch 210, val loss: 0.9257994890213013
Epoch 220, training loss: 0.590774416923523 = 0.5837640762329102 + 0.001 * 7.010318756103516
Epoch 220, val loss: 0.8775985240936279
Epoch 230, training loss: 0.5316671133041382 = 0.5246593952178955 + 0.001 * 7.0077104568481445
Epoch 230, val loss: 0.8361601233482361
Epoch 240, training loss: 0.47828811407089233 = 0.47128233313560486 + 0.001 * 7.005772590637207
Epoch 240, val loss: 0.8023613095283508
Epoch 250, training loss: 0.4301649332046509 = 0.4231608510017395 + 0.001 * 7.004089832305908
Epoch 250, val loss: 0.775176465511322
Epoch 260, training loss: 0.38603463768959045 = 0.37903228402137756 + 0.001 * 7.002357482910156
Epoch 260, val loss: 0.7529668211936951
Epoch 270, training loss: 0.34443989396095276 = 0.33743900060653687 + 0.001 * 7.0009074211120605
Epoch 270, val loss: 0.7348963618278503
Epoch 280, training loss: 0.30452272295951843 = 0.29752296209335327 + 0.001 * 6.999759197235107
Epoch 280, val loss: 0.7204961776733398
Epoch 290, training loss: 0.2664235532283783 = 0.25942471623420715 + 0.001 * 6.99884033203125
Epoch 290, val loss: 0.7097977995872498
Epoch 300, training loss: 0.23106828331947327 = 0.22406966984272003 + 0.001 * 6.9986162185668945
Epoch 300, val loss: 0.7031092047691345
Epoch 310, training loss: 0.19946783781051636 = 0.19247007369995117 + 0.001 * 6.9977641105651855
Epoch 310, val loss: 0.7010546922683716
Epoch 320, training loss: 0.1721591204404831 = 0.16516165435314178 + 0.001 * 6.997465133666992
Epoch 320, val loss: 0.7038724422454834
Epoch 330, training loss: 0.1491079330444336 = 0.14211133122444153 + 0.001 * 6.996605396270752
Epoch 330, val loss: 0.7109249830245972
Epoch 340, training loss: 0.1298574060201645 = 0.12285842001438141 + 0.001 * 6.998988628387451
Epoch 340, val loss: 0.7213186621665955
Epoch 350, training loss: 0.11379766464233398 = 0.10680269449949265 + 0.001 * 6.994969844818115
Epoch 350, val loss: 0.7340723872184753
Epoch 360, training loss: 0.1003502681851387 = 0.09335780888795853 + 0.001 * 6.992459774017334
Epoch 360, val loss: 0.7484781742095947
Epoch 370, training loss: 0.08899460732936859 = 0.08200489729642868 + 0.001 * 6.98970890045166
Epoch 370, val loss: 0.7638416290283203
Epoch 380, training loss: 0.07933342456817627 = 0.07234518975019455 + 0.001 * 6.9882330894470215
Epoch 380, val loss: 0.7796322703361511
Epoch 390, training loss: 0.07107327878475189 = 0.06409116089344025 + 0.001 * 6.982118129730225
Epoch 390, val loss: 0.7958385944366455
Epoch 400, training loss: 0.0639626681804657 = 0.05698816478252411 + 0.001 * 6.974499225616455
Epoch 400, val loss: 0.8121376037597656
Epoch 410, training loss: 0.057823292911052704 = 0.05085368826985359 + 0.001 * 6.969604015350342
Epoch 410, val loss: 0.8284886479377747
Epoch 420, training loss: 0.05250808596611023 = 0.04554041102528572 + 0.001 * 6.967675685882568
Epoch 420, val loss: 0.8447978496551514
Epoch 430, training loss: 0.047885287553071976 = 0.04092966765165329 + 0.001 * 6.955620765686035
Epoch 430, val loss: 0.8608466386795044
Epoch 440, training loss: 0.04387536272406578 = 0.036920495331287384 + 0.001 * 6.954867839813232
Epoch 440, val loss: 0.8767229318618774
Epoch 450, training loss: 0.04037601500749588 = 0.03342730179429054 + 0.001 * 6.948710918426514
Epoch 450, val loss: 0.8922467231750488
Epoch 460, training loss: 0.03732354938983917 = 0.03037555329501629 + 0.001 * 6.947994709014893
Epoch 460, val loss: 0.9074517488479614
Epoch 470, training loss: 0.03464667499065399 = 0.02770211547613144 + 0.001 * 6.944560527801514
Epoch 470, val loss: 0.9222567081451416
Epoch 480, training loss: 0.032289136201143265 = 0.02535223588347435 + 0.001 * 6.936901092529297
Epoch 480, val loss: 0.9366205930709839
Epoch 490, training loss: 0.030214762315154076 = 0.023279735818505287 + 0.001 * 6.9350266456604
Epoch 490, val loss: 0.950559139251709
Epoch 500, training loss: 0.028379445895552635 = 0.021445341408252716 + 0.001 * 6.934103488922119
Epoch 500, val loss: 0.9640721082687378
Epoch 510, training loss: 0.026748770847916603 = 0.019815834239125252 + 0.001 * 6.932936191558838
Epoch 510, val loss: 0.9771572351455688
Epoch 520, training loss: 0.025293711572885513 = 0.01836306042969227 + 0.001 * 6.930651664733887
Epoch 520, val loss: 0.9898155927658081
Epoch 530, training loss: 0.023992598056793213 = 0.01706325262784958 + 0.001 * 6.929344654083252
Epoch 530, val loss: 1.0020805597305298
Epoch 540, training loss: 0.02282627485692501 = 0.015896569937467575 + 0.001 * 6.9297051429748535
Epoch 540, val loss: 1.013976812362671
Epoch 550, training loss: 0.02177630551159382 = 0.0148458918556571 + 0.001 * 6.930413246154785
Epoch 550, val loss: 1.025498628616333
Epoch 560, training loss: 0.020826488733291626 = 0.013896739110350609 + 0.001 * 6.929749488830566
Epoch 560, val loss: 1.0366742610931396
Epoch 570, training loss: 0.01995982974767685 = 0.0130366962403059 + 0.001 * 6.923133850097656
Epoch 570, val loss: 1.0475144386291504
Epoch 580, training loss: 0.01918002776801586 = 0.012255208566784859 + 0.001 * 6.924819469451904
Epoch 580, val loss: 1.0580382347106934
Epoch 590, training loss: 0.018462123349308968 = 0.011543204076588154 + 0.001 * 6.918919086456299
Epoch 590, val loss: 1.0682300329208374
Epoch 600, training loss: 0.01781272329390049 = 0.01089282613247633 + 0.001 * 6.919897079467773
Epoch 600, val loss: 1.0781370401382446
Epoch 610, training loss: 0.017215466126799583 = 0.010297329165041447 + 0.001 * 6.9181365966796875
Epoch 610, val loss: 1.0877814292907715
Epoch 620, training loss: 0.016669316217303276 = 0.009750768542289734 + 0.001 * 6.9185471534729
Epoch 620, val loss: 1.097129464149475
Epoch 630, training loss: 0.01616240292787552 = 0.009248019196093082 + 0.001 * 6.914382457733154
Epoch 630, val loss: 1.1062458753585815
Epoch 640, training loss: 0.015697091817855835 = 0.008784595876932144 + 0.001 * 6.9124956130981445
Epoch 640, val loss: 1.1151268482208252
Epoch 650, training loss: 0.015262540429830551 = 0.008356543257832527 + 0.001 * 6.905997276306152
Epoch 650, val loss: 1.1237595081329346
Epoch 660, training loss: 0.014869841746985912 = 0.007960419170558453 + 0.001 * 6.909422397613525
Epoch 660, val loss: 1.1321735382080078
Epoch 670, training loss: 0.01449662633240223 = 0.007593133021146059 + 0.001 * 6.903493404388428
Epoch 670, val loss: 1.140370488166809
Epoch 680, training loss: 0.014152592048048973 = 0.007251990959048271 + 0.001 * 6.900601387023926
Epoch 680, val loss: 1.1483497619628906
Epoch 690, training loss: 0.013836841098964214 = 0.006934645585715771 + 0.001 * 6.902194976806641
Epoch 690, val loss: 1.1561503410339355
Epoch 700, training loss: 0.01355195976793766 = 0.006638898514211178 + 0.001 * 6.913061141967773
Epoch 700, val loss: 1.1637353897094727
Epoch 710, training loss: 0.013251155614852905 = 0.006362884305417538 + 0.001 * 6.888270378112793
Epoch 710, val loss: 1.1711359024047852
Epoch 720, training loss: 0.01302567683160305 = 0.006104891654103994 + 0.001 * 6.920784950256348
Epoch 720, val loss: 1.1783826351165771
Epoch 730, training loss: 0.012761745601892471 = 0.0058634113520383835 + 0.001 * 6.898334503173828
Epoch 730, val loss: 1.1854336261749268
Epoch 740, training loss: 0.012518679723143578 = 0.005637022666633129 + 0.001 * 6.881656169891357
Epoch 740, val loss: 1.1923434734344482
Epoch 750, training loss: 0.012300622649490833 = 0.005424480885267258 + 0.001 * 6.876141548156738
Epoch 750, val loss: 1.1990911960601807
Epoch 760, training loss: 0.012123879045248032 = 0.005224701017141342 + 0.001 * 6.8991780281066895
Epoch 760, val loss: 1.205664873123169
Epoch 770, training loss: 0.011915644630789757 = 0.0050367312505841255 + 0.001 * 6.878912448883057
Epoch 770, val loss: 1.2121262550354004
Epoch 780, training loss: 0.011734819039702415 = 0.004859643522650003 + 0.001 * 6.875175476074219
Epoch 780, val loss: 1.2184395790100098
Epoch 790, training loss: 0.011569393798708916 = 0.004692601040005684 + 0.001 * 6.8767924308776855
Epoch 790, val loss: 1.2246177196502686
Epoch 800, training loss: 0.011403344571590424 = 0.004534884821623564 + 0.001 * 6.868459701538086
Epoch 800, val loss: 1.2306472063064575
Epoch 810, training loss: 0.011262151412665844 = 0.004385790787637234 + 0.001 * 6.8763604164123535
Epoch 810, val loss: 1.2365520000457764
Epoch 820, training loss: 0.011112455278635025 = 0.0042447238229215145 + 0.001 * 6.867731094360352
Epoch 820, val loss: 1.2423299551010132
Epoch 830, training loss: 0.010992737486958504 = 0.004111096728593111 + 0.001 * 6.8816399574279785
Epoch 830, val loss: 1.2479904890060425
Epoch 840, training loss: 0.010846603661775589 = 0.003984394017606974 + 0.001 * 6.862209320068359
Epoch 840, val loss: 1.2535364627838135
Epoch 850, training loss: 0.010719459503889084 = 0.0038641607388854027 + 0.001 * 6.8552985191345215
Epoch 850, val loss: 1.2589633464813232
Epoch 860, training loss: 0.010632634162902832 = 0.0037499500904232264 + 0.001 * 6.882683753967285
Epoch 860, val loss: 1.2643144130706787
Epoch 870, training loss: 0.010487538762390614 = 0.0036413792986422777 + 0.001 * 6.846158981323242
Epoch 870, val loss: 1.2695186138153076
Epoch 880, training loss: 0.010378600098192692 = 0.0035380844492465258 + 0.001 * 6.84051513671875
Epoch 880, val loss: 1.274640679359436
Epoch 890, training loss: 0.010292782448232174 = 0.003439720720052719 + 0.001 * 6.853061199188232
Epoch 890, val loss: 1.2796602249145508
Epoch 900, training loss: 0.010203337296843529 = 0.0033459835685789585 + 0.001 * 6.857353210449219
Epoch 900, val loss: 1.2845909595489502
Epoch 910, training loss: 0.01012441050261259 = 0.0032565772999078035 + 0.001 * 6.867832660675049
Epoch 910, val loss: 1.289446234703064
Epoch 920, training loss: 0.010033346712589264 = 0.003171249059960246 + 0.001 * 6.86209774017334
Epoch 920, val loss: 1.2941592931747437
Epoch 930, training loss: 0.009932741522789001 = 0.003089779056608677 + 0.001 * 6.842962741851807
Epoch 930, val loss: 1.2988437414169312
Epoch 940, training loss: 0.009858062490820885 = 0.003011897671967745 + 0.001 * 6.846164226531982
Epoch 940, val loss: 1.3034064769744873
Epoch 950, training loss: 0.009772270917892456 = 0.0029374267905950546 + 0.001 * 6.834843158721924
Epoch 950, val loss: 1.3078718185424805
Epoch 960, training loss: 0.009696045890450478 = 0.0028661529067903757 + 0.001 * 6.829893112182617
Epoch 960, val loss: 1.3122838735580444
Epoch 970, training loss: 0.009638624265789986 = 0.00279791047796607 + 0.001 * 6.8407135009765625
Epoch 970, val loss: 1.3166100978851318
Epoch 980, training loss: 0.009561354294419289 = 0.0027325174305588007 + 0.001 * 6.828836917877197
Epoch 980, val loss: 1.3208415508270264
Epoch 990, training loss: 0.009507529437541962 = 0.002669821260496974 + 0.001 * 6.837707996368408
Epoch 990, val loss: 1.3249785900115967
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.9373
Flip ASR: 0.9244/225 nodes
The final ASR:0.75154, 0.17927, Accuracy:0.79630, 0.02636
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11676])
remove edge: torch.Size([2, 9464])
updated graph: torch.Size([2, 10584])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97786, 0.00000, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9495549201965332 = 1.9411810636520386 + 0.001 * 8.373897552490234
Epoch 0, val loss: 1.9287432432174683
Epoch 10, training loss: 1.938653826713562 = 1.9302799701690674 + 0.001 * 8.373809814453125
Epoch 10, val loss: 1.918043851852417
Epoch 20, training loss: 1.9249446392059326 = 1.916571021080017 + 0.001 * 8.373580932617188
Epoch 20, val loss: 1.9048084020614624
Epoch 30, training loss: 1.9056739807128906 = 1.8973008394241333 + 0.001 * 8.3731050491333
Epoch 30, val loss: 1.8865728378295898
Epoch 40, training loss: 1.877826452255249 = 1.8694545030593872 + 0.001 * 8.371981620788574
Epoch 40, val loss: 1.8611884117126465
Epoch 50, training loss: 1.8406052589416504 = 1.832236647605896 + 0.001 * 8.368573188781738
Epoch 50, val loss: 1.8298665285110474
Epoch 60, training loss: 1.8006901741027832 = 1.7923356294631958 + 0.001 * 8.354530334472656
Epoch 60, val loss: 1.800960898399353
Epoch 70, training loss: 1.7621504068374634 = 1.753883957862854 + 0.001 * 8.266390800476074
Epoch 70, val loss: 1.7729887962341309
Epoch 80, training loss: 1.7094110250473022 = 1.7014631032943726 + 0.001 * 7.94793701171875
Epoch 80, val loss: 1.7274287939071655
Epoch 90, training loss: 1.63750422000885 = 1.6296541690826416 + 0.001 * 7.850028038024902
Epoch 90, val loss: 1.6660268306732178
Epoch 100, training loss: 1.5491998195648193 = 1.541398525238037 + 0.001 * 7.801260471343994
Epoch 100, val loss: 1.5960944890975952
Epoch 110, training loss: 1.4576700925827026 = 1.4499292373657227 + 0.001 * 7.740857124328613
Epoch 110, val loss: 1.5243897438049316
Epoch 120, training loss: 1.3709784746170044 = 1.3634086847305298 + 0.001 * 7.569823741912842
Epoch 120, val loss: 1.458733081817627
Epoch 130, training loss: 1.2886414527893066 = 1.281241536140442 + 0.001 * 7.39993143081665
Epoch 130, val loss: 1.3975147008895874
Epoch 140, training loss: 1.2068841457366943 = 1.1995712518692017 + 0.001 * 7.312870502471924
Epoch 140, val loss: 1.3379051685333252
Epoch 150, training loss: 1.123347520828247 = 1.116105318069458 + 0.001 * 7.242157936096191
Epoch 150, val loss: 1.2785946130752563
Epoch 160, training loss: 1.0370385646820068 = 1.0298091173171997 + 0.001 * 7.229441165924072
Epoch 160, val loss: 1.2184721231460571
Epoch 170, training loss: 0.949059247970581 = 0.9418356418609619 + 0.001 * 7.223591327667236
Epoch 170, val loss: 1.1581844091415405
Epoch 180, training loss: 0.8635188341140747 = 0.8563012480735779 + 0.001 * 7.217581748962402
Epoch 180, val loss: 1.1006579399108887
Epoch 190, training loss: 0.7852040529251099 = 0.7779936790466309 + 0.001 * 7.210386276245117
Epoch 190, val loss: 1.05044686794281
Epoch 200, training loss: 0.7170232534408569 = 0.7098252177238464 + 0.001 * 7.19803524017334
Epoch 200, val loss: 1.0100913047790527
Epoch 210, training loss: 0.6588970422744751 = 0.6517215371131897 + 0.001 * 7.175522327423096
Epoch 210, val loss: 0.9797754287719727
Epoch 220, training loss: 0.6081810593605042 = 0.6010476350784302 + 0.001 * 7.133449554443359
Epoch 220, val loss: 0.95759117603302
Epoch 230, training loss: 0.5611881613731384 = 0.5540973544120789 + 0.001 * 7.090789794921875
Epoch 230, val loss: 0.9403572082519531
Epoch 240, training loss: 0.514525294303894 = 0.5074564814567566 + 0.001 * 7.068783760070801
Epoch 240, val loss: 0.9254493713378906
Epoch 250, training loss: 0.4661359190940857 = 0.45907118916511536 + 0.001 * 7.0647406578063965
Epoch 250, val loss: 0.9113715887069702
Epoch 260, training loss: 0.4160458743572235 = 0.4089813232421875 + 0.001 * 7.064544200897217
Epoch 260, val loss: 0.8972944021224976
Epoch 270, training loss: 0.36636435985565186 = 0.3592999577522278 + 0.001 * 7.064414978027344
Epoch 270, val loss: 0.8846835494041443
Epoch 280, training loss: 0.3198132812976837 = 0.31274867057800293 + 0.001 * 7.0646162033081055
Epoch 280, val loss: 0.8763954639434814
Epoch 290, training loss: 0.27799490094184875 = 0.27092990279197693 + 0.001 * 7.06498908996582
Epoch 290, val loss: 0.8741166591644287
Epoch 300, training loss: 0.24112802743911743 = 0.2340625375509262 + 0.001 * 7.065483093261719
Epoch 300, val loss: 0.8777575492858887
Epoch 310, training loss: 0.20889295637607574 = 0.2018270492553711 + 0.001 * 7.065906524658203
Epoch 310, val loss: 0.8863759636878967
Epoch 320, training loss: 0.1809895634651184 = 0.1739233136177063 + 0.001 * 7.066250324249268
Epoch 320, val loss: 0.8987544775009155
Epoch 330, training loss: 0.1572583019733429 = 0.15019136667251587 + 0.001 * 7.066937446594238
Epoch 330, val loss: 0.9139783978462219
Epoch 340, training loss: 0.13738323748111725 = 0.13031361997127533 + 0.001 * 7.069611072540283
Epoch 340, val loss: 0.9313955903053284
Epoch 350, training loss: 0.12084263563156128 = 0.11377513408660889 + 0.001 * 7.067502498626709
Epoch 350, val loss: 0.9503528475761414
Epoch 360, training loss: 0.10704207420349121 = 0.09997496753931046 + 0.001 * 7.067102909088135
Epoch 360, val loss: 0.9703255891799927
Epoch 370, training loss: 0.09542917460203171 = 0.08836264908313751 + 0.001 * 7.066525936126709
Epoch 370, val loss: 0.9909536838531494
Epoch 380, training loss: 0.08558621257543564 = 0.07850231230258942 + 0.001 * 7.083902359008789
Epoch 380, val loss: 1.0117477178573608
Epoch 390, training loss: 0.07711423933506012 = 0.07004961371421814 + 0.001 * 7.064622402191162
Epoch 390, val loss: 1.0323675870895386
Epoch 400, training loss: 0.06982411444187164 = 0.06276117265224457 + 0.001 * 7.062941551208496
Epoch 400, val loss: 1.0525438785552979
Epoch 410, training loss: 0.0634949803352356 = 0.056434229016304016 + 0.001 * 7.0607476234436035
Epoch 410, val loss: 1.0721379518508911
Epoch 420, training loss: 0.05797219276428223 = 0.05090708285570145 + 0.001 * 7.065110206604004
Epoch 420, val loss: 1.091230034828186
Epoch 430, training loss: 0.05310773849487305 = 0.04604397341609001 + 0.001 * 7.063764572143555
Epoch 430, val loss: 1.109745740890503
Epoch 440, training loss: 0.04880254343152046 = 0.04175182804465294 + 0.001 * 7.050714015960693
Epoch 440, val loss: 1.1275850534439087
Epoch 450, training loss: 0.044989582151174545 = 0.037946633994579315 + 0.001 * 7.0429463386535645
Epoch 450, val loss: 1.1447685956954956
Epoch 460, training loss: 0.041615042835474014 = 0.03456571698188782 + 0.001 * 7.049326419830322
Epoch 460, val loss: 1.1614347696304321
Epoch 470, training loss: 0.038596391677856445 = 0.03155843913555145 + 0.001 * 7.037953853607178
Epoch 470, val loss: 1.177563190460205
Epoch 480, training loss: 0.03591031953692436 = 0.028882337734103203 + 0.001 * 7.027980804443359
Epoch 480, val loss: 1.193156361579895
Epoch 490, training loss: 0.03352810814976692 = 0.026497941464185715 + 0.001 * 7.030167579650879
Epoch 490, val loss: 1.2083368301391602
Epoch 500, training loss: 0.03138557821512222 = 0.024371134117245674 + 0.001 * 7.014444828033447
Epoch 500, val loss: 1.2230533361434937
Epoch 510, training loss: 0.029481958597898483 = 0.022470319643616676 + 0.001 * 7.01163911819458
Epoch 510, val loss: 1.2372982501983643
Epoch 520, training loss: 0.02777625247836113 = 0.020769339054822922 + 0.001 * 7.006912708282471
Epoch 520, val loss: 1.2510825395584106
Epoch 530, training loss: 0.026263779029250145 = 0.01924467459321022 + 0.001 * 7.01910400390625
Epoch 530, val loss: 1.2644684314727783
Epoch 540, training loss: 0.024867165833711624 = 0.017875080928206444 + 0.001 * 6.992084980010986
Epoch 540, val loss: 1.2774714231491089
Epoch 550, training loss: 0.023644734174013138 = 0.01664140447974205 + 0.001 * 7.003328800201416
Epoch 550, val loss: 1.2901246547698975
Epoch 560, training loss: 0.022520828992128372 = 0.015527774579823017 + 0.001 * 6.993053436279297
Epoch 560, val loss: 1.3023648262023926
Epoch 570, training loss: 0.02150147035717964 = 0.014520155265927315 + 0.001 * 6.981315612792969
Epoch 570, val loss: 1.31427800655365
Epoch 580, training loss: 0.020576074719429016 = 0.013606160879135132 + 0.001 * 6.969912528991699
Epoch 580, val loss: 1.3258575201034546
Epoch 590, training loss: 0.019767817109823227 = 0.0127756018191576 + 0.001 * 6.992215156555176
Epoch 590, val loss: 1.3370939493179321
Epoch 600, training loss: 0.018987193703651428 = 0.012018959037959576 + 0.001 * 6.968235015869141
Epoch 600, val loss: 1.3480507135391235
Epoch 610, training loss: 0.01828681491315365 = 0.011327628046274185 + 0.001 * 6.95918607711792
Epoch 610, val loss: 1.3586429357528687
Epoch 620, training loss: 0.017661910504102707 = 0.010693369433283806 + 0.001 * 6.968540668487549
Epoch 620, val loss: 1.369065284729004
Epoch 630, training loss: 0.017076274380087852 = 0.010108775459229946 + 0.001 * 6.967498779296875
Epoch 630, val loss: 1.3791788816452026
Epoch 640, training loss: 0.016529034823179245 = 0.009567099623382092 + 0.001 * 6.9619340896606445
Epoch 640, val loss: 1.3890490531921387
Epoch 650, training loss: 0.016033079475164413 = 0.00906351674348116 + 0.001 * 6.969561576843262
Epoch 650, val loss: 1.3987692594528198
Epoch 660, training loss: 0.015543421730399132 = 0.008594985119998455 + 0.001 * 6.948436737060547
Epoch 660, val loss: 1.408294916152954
Epoch 670, training loss: 0.015101825818419456 = 0.0081587890163064 + 0.001 * 6.943037033081055
Epoch 670, val loss: 1.4176076650619507
Epoch 680, training loss: 0.014695331454277039 = 0.007752605248242617 + 0.001 * 6.942725658416748
Epoch 680, val loss: 1.4267182350158691
Epoch 690, training loss: 0.014306272380053997 = 0.007374431472271681 + 0.001 * 6.931840419769287
Epoch 690, val loss: 1.4356609582901
Epoch 700, training loss: 0.013955523259937763 = 0.0070221214555203915 + 0.001 * 6.933401584625244
Epoch 700, val loss: 1.444379448890686
Epoch 710, training loss: 0.013640470802783966 = 0.006693459581583738 + 0.001 * 6.947010517120361
Epoch 710, val loss: 1.4529306888580322
Epoch 720, training loss: 0.013316210359334946 = 0.006386398337781429 + 0.001 * 6.929811954498291
Epoch 720, val loss: 1.4613418579101562
Epoch 730, training loss: 0.013067761436104774 = 0.006098615936934948 + 0.001 * 6.969144821166992
Epoch 730, val loss: 1.4695563316345215
Epoch 740, training loss: 0.012774625793099403 = 0.005828214809298515 + 0.001 * 6.9464111328125
Epoch 740, val loss: 1.477712869644165
Epoch 750, training loss: 0.012510532513260841 = 0.00557377003133297 + 0.001 * 6.93676233291626
Epoch 750, val loss: 1.4856938123703003
Epoch 760, training loss: 0.012246830388903618 = 0.005334199871867895 + 0.001 * 6.9126296043396
Epoch 760, val loss: 1.4934881925582886
Epoch 770, training loss: 0.012045435607433319 = 0.0051082223653793335 + 0.001 * 6.93721342086792
Epoch 770, val loss: 1.5013313293457031
Epoch 780, training loss: 0.011826742440462112 = 0.004895275924354792 + 0.001 * 6.931466579437256
Epoch 780, val loss: 1.5090638399124146
Epoch 790, training loss: 0.011610506102442741 = 0.004694218281656504 + 0.001 * 6.916287899017334
Epoch 790, val loss: 1.5165867805480957
Epoch 800, training loss: 0.011411667801439762 = 0.004504363052546978 + 0.001 * 6.907304286956787
Epoch 800, val loss: 1.5241153240203857
Epoch 810, training loss: 0.011231495067477226 = 0.0043251789174973965 + 0.001 * 6.906315803527832
Epoch 810, val loss: 1.531492829322815
Epoch 820, training loss: 0.01106023695319891 = 0.004156067036092281 + 0.001 * 6.90416955947876
Epoch 820, val loss: 1.5387134552001953
Epoch 830, training loss: 0.010915523394942284 = 0.003996350336819887 + 0.001 * 6.919172763824463
Epoch 830, val loss: 1.5459028482437134
Epoch 840, training loss: 0.01074440311640501 = 0.0038454479072242975 + 0.001 * 6.89895486831665
Epoch 840, val loss: 1.5529383420944214
Epoch 850, training loss: 0.01059810072183609 = 0.0037028968799859285 + 0.001 * 6.895203590393066
Epoch 850, val loss: 1.5598701238632202
Epoch 860, training loss: 0.01046903058886528 = 0.0035681200679391623 + 0.001 * 6.900910377502441
Epoch 860, val loss: 1.5666792392730713
Epoch 870, training loss: 0.010330915451049805 = 0.0034407549537718296 + 0.001 * 6.890159606933594
Epoch 870, val loss: 1.5733394622802734
Epoch 880, training loss: 0.010221338830888271 = 0.003320395015180111 + 0.001 * 6.900943279266357
Epoch 880, val loss: 1.579893708229065
Epoch 890, training loss: 0.010093954391777515 = 0.0032065901905298233 + 0.001 * 6.887363910675049
Epoch 890, val loss: 1.586363434791565
Epoch 900, training loss: 0.009997894056141376 = 0.003098973073065281 + 0.001 * 6.89892053604126
Epoch 900, val loss: 1.5926439762115479
Epoch 910, training loss: 0.009905182756483555 = 0.002997183706611395 + 0.001 * 6.907998561859131
Epoch 910, val loss: 1.5987915992736816
Epoch 920, training loss: 0.0098064374178648 = 0.0029007880948483944 + 0.001 * 6.905648708343506
Epoch 920, val loss: 1.6048818826675415
Epoch 930, training loss: 0.009683019481599331 = 0.0028093636501580477 + 0.001 * 6.873655796051025
Epoch 930, val loss: 1.6107622385025024
Epoch 940, training loss: 0.009607526473701 = 0.0027226528618484735 + 0.001 * 6.884872913360596
Epoch 940, val loss: 1.6165788173675537
Epoch 950, training loss: 0.009526930749416351 = 0.002640423597767949 + 0.001 * 6.886507034301758
Epoch 950, val loss: 1.6222474575042725
Epoch 960, training loss: 0.009472249075770378 = 0.0025623333640396595 + 0.001 * 6.909914970397949
Epoch 960, val loss: 1.627780556678772
Epoch 970, training loss: 0.009356657043099403 = 0.0024880750570446253 + 0.001 * 6.868581771850586
Epoch 970, val loss: 1.6331965923309326
Epoch 980, training loss: 0.009276669472455978 = 0.002417430281639099 + 0.001 * 6.859239101409912
Epoch 980, val loss: 1.6385862827301025
Epoch 990, training loss: 0.009217741899192333 = 0.0023500528186559677 + 0.001 * 6.8676886558532715
Epoch 990, val loss: 1.643871784210205
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.5055
Flip ASR: 0.4400/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9512932300567627 = 1.942919373512268 + 0.001 * 8.3739013671875
Epoch 0, val loss: 1.9411230087280273
Epoch 10, training loss: 1.9412416219711304 = 1.9328677654266357 + 0.001 * 8.373858451843262
Epoch 10, val loss: 1.9305616617202759
Epoch 20, training loss: 1.9291483163833618 = 1.9207745790481567 + 0.001 * 8.373690605163574
Epoch 20, val loss: 1.9177695512771606
Epoch 30, training loss: 1.9122117757797241 = 1.9038385152816772 + 0.001 * 8.37329387664795
Epoch 30, val loss: 1.9000060558319092
Epoch 40, training loss: 1.8872679471969604 = 1.8788955211639404 + 0.001 * 8.372366905212402
Epoch 40, val loss: 1.8744511604309082
Epoch 50, training loss: 1.852272629737854 = 1.8439027070999146 + 0.001 * 8.36991024017334
Epoch 50, val loss: 1.8404732942581177
Epoch 60, training loss: 1.8109841346740723 = 1.8026219606399536 + 0.001 * 8.362128257751465
Epoch 60, val loss: 1.8042174577713013
Epoch 70, training loss: 1.7716131210327148 = 1.7632818222045898 + 0.001 * 8.331331253051758
Epoch 70, val loss: 1.772974967956543
Epoch 80, training loss: 1.7238681316375732 = 1.715718150138855 + 0.001 * 8.14997673034668
Epoch 80, val loss: 1.732462763786316
Epoch 90, training loss: 1.6569668054580688 = 1.6490707397460938 + 0.001 * 7.896122932434082
Epoch 90, val loss: 1.6750560998916626
Epoch 100, training loss: 1.567299485206604 = 1.5596457719802856 + 0.001 * 7.653674602508545
Epoch 100, val loss: 1.5996733903884888
Epoch 110, training loss: 1.4586381912231445 = 1.4512232542037964 + 0.001 * 7.414950847625732
Epoch 110, val loss: 1.5110224485397339
Epoch 120, training loss: 1.3408424854278564 = 1.333497405052185 + 0.001 * 7.3451385498046875
Epoch 120, val loss: 1.4142357110977173
Epoch 130, training loss: 1.2217810153961182 = 1.2144992351531982 + 0.001 * 7.281800270080566
Epoch 130, val loss: 1.3183740377426147
Epoch 140, training loss: 1.1072320938110352 = 1.0999889373779297 + 0.001 * 7.243148326873779
Epoch 140, val loss: 1.226789116859436
Epoch 150, training loss: 1.0008602142333984 = 0.9936268329620361 + 0.001 * 7.233426570892334
Epoch 150, val loss: 1.1424674987792969
Epoch 160, training loss: 0.9042669534683228 = 0.8970392346382141 + 0.001 * 7.227748394012451
Epoch 160, val loss: 1.0665220022201538
Epoch 170, training loss: 0.8175218105316162 = 0.8102993369102478 + 0.001 * 7.222470760345459
Epoch 170, val loss: 0.9988786578178406
Epoch 180, training loss: 0.7405982613563538 = 0.7333810925483704 + 0.001 * 7.217190742492676
Epoch 180, val loss: 0.9404541254043579
Epoch 190, training loss: 0.6731784343719482 = 0.6659653782844543 + 0.001 * 7.213033676147461
Epoch 190, val loss: 0.8918631672859192
Epoch 200, training loss: 0.6139426827430725 = 0.6067334413528442 + 0.001 * 7.209247589111328
Epoch 200, val loss: 0.8529621362686157
Epoch 210, training loss: 0.5611178874969482 = 0.5539121031761169 + 0.001 * 7.205812454223633
Epoch 210, val loss: 0.8225314617156982
Epoch 220, training loss: 0.5131340622901917 = 0.505931556224823 + 0.001 * 7.202490329742432
Epoch 220, val loss: 0.7989761233329773
Epoch 230, training loss: 0.4686990976333618 = 0.46150004863739014 + 0.001 * 7.199043273925781
Epoch 230, val loss: 0.7807508111000061
Epoch 240, training loss: 0.4266660213470459 = 0.41947081685066223 + 0.001 * 7.195207595825195
Epoch 240, val loss: 0.766430139541626
Epoch 250, training loss: 0.3862382471561432 = 0.37904754281044006 + 0.001 * 7.190716743469238
Epoch 250, val loss: 0.7553603053092957
Epoch 260, training loss: 0.3471681475639343 = 0.3399834632873535 + 0.001 * 7.184689521789551
Epoch 260, val loss: 0.7472784519195557
Epoch 270, training loss: 0.3097212612628937 = 0.3025436997413635 + 0.001 * 7.177572727203369
Epoch 270, val loss: 0.7420315146446228
Epoch 280, training loss: 0.27445757389068604 = 0.26729369163513184 + 0.001 * 7.163869857788086
Epoch 280, val loss: 0.7394998073577881
Epoch 290, training loss: 0.24194292724132538 = 0.2347956746816635 + 0.001 * 7.14725923538208
Epoch 290, val loss: 0.7398632168769836
Epoch 300, training loss: 0.21257056295871735 = 0.20544125139713287 + 0.001 * 7.129306316375732
Epoch 300, val loss: 0.742889940738678
Epoch 310, training loss: 0.18649174273014069 = 0.17937763035297394 + 0.001 * 7.114114761352539
Epoch 310, val loss: 0.748365581035614
Epoch 320, training loss: 0.16361798346042633 = 0.15650926530361176 + 0.001 * 7.108724117279053
Epoch 320, val loss: 0.7561909556388855
Epoch 330, training loss: 0.1436900496482849 = 0.13659045100212097 + 0.001 * 7.099593639373779
Epoch 330, val loss: 0.7662038207054138
Epoch 340, training loss: 0.126412495970726 = 0.11931959539651871 + 0.001 * 7.092897415161133
Epoch 340, val loss: 0.7780494689941406
Epoch 350, training loss: 0.11148525774478912 = 0.10439468920230865 + 0.001 * 7.090566635131836
Epoch 350, val loss: 0.7914000153541565
Epoch 360, training loss: 0.09862515330314636 = 0.09153412282466888 + 0.001 * 7.091033458709717
Epoch 360, val loss: 0.8057883977890015
Epoch 370, training loss: 0.08756110817193985 = 0.0804799273610115 + 0.001 * 7.081178188323975
Epoch 370, val loss: 0.8209041953086853
Epoch 380, training loss: 0.07809320837259293 = 0.07099583745002747 + 0.001 * 7.097367286682129
Epoch 380, val loss: 0.8363264799118042
Epoch 390, training loss: 0.06994128227233887 = 0.06286617368459702 + 0.001 * 7.075105667114258
Epoch 390, val loss: 0.8516663908958435
Epoch 400, training loss: 0.06295466423034668 = 0.05588264390826225 + 0.001 * 7.072017192840576
Epoch 400, val loss: 0.8667444586753845
Epoch 410, training loss: 0.05692894756793976 = 0.049862638115882874 + 0.001 * 7.066308498382568
Epoch 410, val loss: 0.8813244700431824
Epoch 420, training loss: 0.05172399803996086 = 0.044653795659542084 + 0.001 * 7.070202827453613
Epoch 420, val loss: 0.8954618573188782
Epoch 430, training loss: 0.0471949577331543 = 0.04012906551361084 + 0.001 * 7.065893650054932
Epoch 430, val loss: 0.9090657830238342
Epoch 440, training loss: 0.043242260813713074 = 0.036184780299663544 + 0.001 * 7.057481288909912
Epoch 440, val loss: 0.9222334027290344
Epoch 450, training loss: 0.03980410099029541 = 0.03273649886250496 + 0.001 * 7.067601203918457
Epoch 450, val loss: 0.9349784851074219
Epoch 460, training loss: 0.036759957671165466 = 0.02971406653523445 + 0.001 * 7.045889377593994
Epoch 460, val loss: 0.9473850727081299
Epoch 470, training loss: 0.03409234806895256 = 0.02705756016075611 + 0.001 * 7.034787654876709
Epoch 470, val loss: 0.9594869017601013
Epoch 480, training loss: 0.03175679221749306 = 0.02471739426255226 + 0.001 * 7.039399147033691
Epoch 480, val loss: 0.9712688326835632
Epoch 490, training loss: 0.029683467000722885 = 0.022650878876447678 + 0.001 * 7.032588481903076
Epoch 490, val loss: 0.9827304482460022
Epoch 500, training loss: 0.027847277000546455 = 0.02082045190036297 + 0.001 * 7.026824951171875
Epoch 500, val loss: 0.9938884377479553
Epoch 510, training loss: 0.026224985718727112 = 0.01919400505721569 + 0.001 * 7.030979633331299
Epoch 510, val loss: 1.0046802759170532
Epoch 520, training loss: 0.024757426232099533 = 0.017744965851306915 + 0.001 * 7.012460708618164
Epoch 520, val loss: 1.015181303024292
Epoch 530, training loss: 0.02347220852971077 = 0.0164500679820776 + 0.001 * 7.022140979766846
Epoch 530, val loss: 1.0253255367279053
Epoch 540, training loss: 0.022290252149105072 = 0.015289787203073502 + 0.001 * 7.000464916229248
Epoch 540, val loss: 1.035206913948059
Epoch 550, training loss: 0.021255068480968475 = 0.014247074723243713 + 0.001 * 7.007993698120117
Epoch 550, val loss: 1.0447276830673218
Epoch 560, training loss: 0.020316073670983315 = 0.013307285495102406 + 0.001 * 7.008787631988525
Epoch 560, val loss: 1.0539445877075195
Epoch 570, training loss: 0.019451454281806946 = 0.012457962147891521 + 0.001 * 6.9934916496276855
Epoch 570, val loss: 1.062851071357727
Epoch 580, training loss: 0.018692851066589355 = 0.011688154190778732 + 0.001 * 7.004696846008301
Epoch 580, val loss: 1.0714974403381348
Epoch 590, training loss: 0.017954953014850616 = 0.010988657362759113 + 0.001 * 6.966294288635254
Epoch 590, val loss: 1.0798308849334717
Epoch 600, training loss: 0.01731731742620468 = 0.010351406410336494 + 0.001 * 6.965910911560059
Epoch 600, val loss: 1.0879119634628296
Epoch 610, training loss: 0.01676950789988041 = 0.009769340977072716 + 0.001 * 7.000166416168213
Epoch 610, val loss: 1.0957504510879517
Epoch 620, training loss: 0.016217675060033798 = 0.0092355040833354 + 0.001 * 6.982171535491943
Epoch 620, val loss: 1.1033899784088135
Epoch 630, training loss: 0.015723858028650284 = 0.008745008148252964 + 0.001 * 6.978848934173584
Epoch 630, val loss: 1.110737681388855
Epoch 640, training loss: 0.01525791734457016 = 0.008292323909699917 + 0.001 * 6.965593338012695
Epoch 640, val loss: 1.1179100275039673
Epoch 650, training loss: 0.014808863401412964 = 0.007873748429119587 + 0.001 * 6.935114860534668
Epoch 650, val loss: 1.1248698234558105
Epoch 660, training loss: 0.014469508081674576 = 0.007485243026167154 + 0.001 * 6.984265327453613
Epoch 660, val loss: 1.1317074298858643
Epoch 670, training loss: 0.01405157707631588 = 0.007124235853552818 + 0.001 * 6.927341461181641
Epoch 670, val loss: 1.1383304595947266
Epoch 680, training loss: 0.013720566406846046 = 0.006788021419197321 + 0.001 * 6.932544231414795
Epoch 680, val loss: 1.1447795629501343
Epoch 690, training loss: 0.013397583737969398 = 0.006474463269114494 + 0.001 * 6.92311954498291
Epoch 690, val loss: 1.1511049270629883
Epoch 700, training loss: 0.013142609968781471 = 0.006181953940540552 + 0.001 * 6.96065616607666
Epoch 700, val loss: 1.1572293043136597
Epoch 710, training loss: 0.012827442958950996 = 0.005908776540309191 + 0.001 * 6.918666362762451
Epoch 710, val loss: 1.1632429361343384
Epoch 720, training loss: 0.01256361324340105 = 0.005653461907058954 + 0.001 * 6.91015100479126
Epoch 720, val loss: 1.1690723896026611
Epoch 730, training loss: 0.012336920015513897 = 0.005414483603090048 + 0.001 * 6.922436237335205
Epoch 730, val loss: 1.1747736930847168
Epoch 740, training loss: 0.01210754830390215 = 0.0051908646710217 + 0.001 * 6.916683197021484
Epoch 740, val loss: 1.1803207397460938
Epoch 750, training loss: 0.011950861662626266 = 0.0049813902005553246 + 0.001 * 6.969470977783203
Epoch 750, val loss: 1.185716986656189
Epoch 760, training loss: 0.011678548529744148 = 0.004785242024809122 + 0.001 * 6.893305778503418
Epoch 760, val loss: 1.190981388092041
Epoch 770, training loss: 0.011544648557901382 = 0.004601011984050274 + 0.001 * 6.943636417388916
Epoch 770, val loss: 1.1960728168487549
Epoch 780, training loss: 0.011306915432214737 = 0.0044280183501541615 + 0.001 * 6.878897190093994
Epoch 780, val loss: 1.2010639905929565
Epoch 790, training loss: 0.011163865216076374 = 0.0042653498239815235 + 0.001 * 6.898515224456787
Epoch 790, val loss: 1.2059223651885986
Epoch 800, training loss: 0.010974220931529999 = 0.0041121370159089565 + 0.001 * 6.862083911895752
Epoch 800, val loss: 1.2106459140777588
Epoch 810, training loss: 0.010852750390768051 = 0.003967728931456804 + 0.001 * 6.885021209716797
Epoch 810, val loss: 1.2152727842330933
Epoch 820, training loss: 0.010725376196205616 = 0.003831544192507863 + 0.001 * 6.893831729888916
Epoch 820, val loss: 1.2197738885879517
Epoch 830, training loss: 0.010609859600663185 = 0.0037029229570180178 + 0.001 * 6.906936168670654
Epoch 830, val loss: 1.2241504192352295
Epoch 840, training loss: 0.01042106281965971 = 0.0035814810544252396 + 0.001 * 6.839581489562988
Epoch 840, val loss: 1.22842276096344
Epoch 850, training loss: 0.010321898385882378 = 0.0034665062557905912 + 0.001 * 6.855391502380371
Epoch 850, val loss: 1.2326048612594604
Epoch 860, training loss: 0.010258226655423641 = 0.0033576583955436945 + 0.001 * 6.900568008422852
Epoch 860, val loss: 1.2366617918014526
Epoch 870, training loss: 0.010117130354046822 = 0.0032545779831707478 + 0.001 * 6.862552642822266
Epoch 870, val loss: 1.2406270503997803
Epoch 880, training loss: 0.010020293295383453 = 0.0031566971447318792 + 0.001 * 6.863595485687256
Epoch 880, val loss: 1.2444895505905151
Epoch 890, training loss: 0.009931910783052444 = 0.0030637665186077356 + 0.001 * 6.8681440353393555
Epoch 890, val loss: 1.2482776641845703
Epoch 900, training loss: 0.009804576635360718 = 0.002975585637614131 + 0.001 * 6.828990936279297
Epoch 900, val loss: 1.2519447803497314
Epoch 910, training loss: 0.009731030091643333 = 0.002891648095101118 + 0.001 * 6.839382171630859
Epoch 910, val loss: 1.2555899620056152
Epoch 920, training loss: 0.009641481563448906 = 0.0028118030168116093 + 0.001 * 6.829678535461426
Epoch 920, val loss: 1.259076476097107
Epoch 930, training loss: 0.00957360677421093 = 0.002735825488343835 + 0.001 * 6.837780952453613
Epoch 930, val loss: 1.2625230550765991
Epoch 940, training loss: 0.009500234387814999 = 0.002663417486473918 + 0.001 * 6.836816310882568
Epoch 940, val loss: 1.2658618688583374
Epoch 950, training loss: 0.009424926713109016 = 0.002594409743323922 + 0.001 * 6.830516815185547
Epoch 950, val loss: 1.269102931022644
Epoch 960, training loss: 0.009421005845069885 = 0.0025284222792834044 + 0.001 * 6.892582893371582
Epoch 960, val loss: 1.2723031044006348
Epoch 970, training loss: 0.009283533319830894 = 0.0024656455498188734 + 0.001 * 6.817887306213379
Epoch 970, val loss: 1.275384545326233
Epoch 980, training loss: 0.009215540252625942 = 0.0024054364766925573 + 0.001 * 6.810103416442871
Epoch 980, val loss: 1.2784333229064941
Epoch 990, training loss: 0.009210518561303616 = 0.0023479617666453123 + 0.001 * 6.862556457519531
Epoch 990, val loss: 1.2813949584960938
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6642
Flip ASR: 0.5956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9574105739593506 = 1.9490365982055664 + 0.001 * 8.373917579650879
Epoch 0, val loss: 1.9449580907821655
Epoch 10, training loss: 1.9473494291305542 = 1.9389755725860596 + 0.001 * 8.373871803283691
Epoch 10, val loss: 1.9354747533798218
Epoch 20, training loss: 1.935164451599121 = 1.926790714263916 + 0.001 * 8.37372875213623
Epoch 20, val loss: 1.9234777688980103
Epoch 30, training loss: 1.9182348251342773 = 1.9098613262176514 + 0.001 * 8.373444557189941
Epoch 30, val loss: 1.9064679145812988
Epoch 40, training loss: 1.8932526111602783 = 1.8848798274993896 + 0.001 * 8.372823715209961
Epoch 40, val loss: 1.881500482559204
Epoch 50, training loss: 1.8576349020004272 = 1.8492636680603027 + 0.001 * 8.37121868133545
Epoch 50, val loss: 1.8474162817001343
Epoch 60, training loss: 1.8147964477539062 = 1.806430459022522 + 0.001 * 8.365998268127441
Epoch 60, val loss: 1.8098862171173096
Epoch 70, training loss: 1.7731797695159912 = 1.7648361921310425 + 0.001 * 8.343552589416504
Epoch 70, val loss: 1.775903582572937
Epoch 80, training loss: 1.7232894897460938 = 1.715090036392212 + 0.001 * 8.1993989944458
Epoch 80, val loss: 1.7318881750106812
Epoch 90, training loss: 1.6540777683258057 = 1.6461684703826904 + 0.001 * 7.9093017578125
Epoch 90, val loss: 1.671785831451416
Epoch 100, training loss: 1.5641440153121948 = 1.556309461593628 + 0.001 * 7.834595203399658
Epoch 100, val loss: 1.5970871448516846
Epoch 110, training loss: 1.4632712602615356 = 1.4554877281188965 + 0.001 * 7.783504962921143
Epoch 110, val loss: 1.5149911642074585
Epoch 120, training loss: 1.3631871938705444 = 1.3555139303207397 + 0.001 * 7.673300743103027
Epoch 120, val loss: 1.4379667043685913
Epoch 130, training loss: 1.2682843208312988 = 1.2608580589294434 + 0.001 * 7.426309108734131
Epoch 130, val loss: 1.3696626424789429
Epoch 140, training loss: 1.178707242012024 = 1.1713193655014038 + 0.001 * 7.387898921966553
Epoch 140, val loss: 1.3074740171432495
Epoch 150, training loss: 1.0927571058273315 = 1.0854361057281494 + 0.001 * 7.320981979370117
Epoch 150, val loss: 1.2487891912460327
Epoch 160, training loss: 1.0093532800674438 = 1.0020605325698853 + 0.001 * 7.292715072631836
Epoch 160, val loss: 1.190996766090393
Epoch 170, training loss: 0.928532063961029 = 0.9212549328804016 + 0.001 * 7.277144908905029
Epoch 170, val loss: 1.1334640979766846
Epoch 180, training loss: 0.8512823581695557 = 0.844018816947937 + 0.001 * 7.2635369300842285
Epoch 180, val loss: 1.0773271322250366
Epoch 190, training loss: 0.7790796160697937 = 0.7718255519866943 + 0.001 * 7.254075050354004
Epoch 190, val loss: 1.0244392156600952
Epoch 200, training loss: 0.7130373120307922 = 0.705792248249054 + 0.001 * 7.245038032531738
Epoch 200, val loss: 0.9770258069038391
Epoch 210, training loss: 0.6527149081230164 = 0.645480215549469 + 0.001 * 7.234692096710205
Epoch 210, val loss: 0.9361283183097839
Epoch 220, training loss: 0.5959583520889282 = 0.5887354612350464 + 0.001 * 7.222878932952881
Epoch 220, val loss: 0.9005594253540039
Epoch 230, training loss: 0.5404396057128906 = 0.5332302451133728 + 0.001 * 7.209346294403076
Epoch 230, val loss: 0.8689023852348328
Epoch 240, training loss: 0.48513704538345337 = 0.4779442250728607 + 0.001 * 7.192823886871338
Epoch 240, val loss: 0.8409120440483093
Epoch 250, training loss: 0.43062055110931396 = 0.42345017194747925 + 0.001 * 7.170370101928711
Epoch 250, val loss: 0.8169986009597778
Epoch 260, training loss: 0.3783705532550812 = 0.3712262809276581 + 0.001 * 7.144264221191406
Epoch 260, val loss: 0.7977661490440369
Epoch 270, training loss: 0.32953935861587524 = 0.322414755821228 + 0.001 * 7.124599456787109
Epoch 270, val loss: 0.7831183075904846
Epoch 280, training loss: 0.2845861613750458 = 0.2774670422077179 + 0.001 * 7.119128704071045
Epoch 280, val loss: 0.7727360725402832
Epoch 290, training loss: 0.2437380701303482 = 0.23663093149662018 + 0.001 * 7.107140064239502
Epoch 290, val loss: 0.7662556171417236
Epoch 300, training loss: 0.2074326127767563 = 0.20032401382923126 + 0.001 * 7.1086015701293945
Epoch 300, val loss: 0.763559103012085
Epoch 310, training loss: 0.17613358795642853 = 0.16902659833431244 + 0.001 * 7.106987953186035
Epoch 310, val loss: 0.7647511959075928
Epoch 320, training loss: 0.14995184540748596 = 0.1428447812795639 + 0.001 * 7.107061862945557
Epoch 320, val loss: 0.7696932554244995
Epoch 330, training loss: 0.1285325288772583 = 0.12142719328403473 + 0.001 * 7.105329990386963
Epoch 330, val loss: 0.7780278921127319
Epoch 340, training loss: 0.11116527765989304 = 0.10405705869197845 + 0.001 * 7.108218669891357
Epoch 340, val loss: 0.7892261743545532
Epoch 350, training loss: 0.09704029560089111 = 0.08993632346391678 + 0.001 * 7.103973388671875
Epoch 350, val loss: 0.80266934633255
Epoch 360, training loss: 0.08545613288879395 = 0.07835322618484497 + 0.001 * 7.102907657623291
Epoch 360, val loss: 0.8177186250686646
Epoch 370, training loss: 0.075835682451725 = 0.06873767077922821 + 0.001 * 7.098008155822754
Epoch 370, val loss: 0.8338633179664612
Epoch 380, training loss: 0.06777235865592957 = 0.06066698208451271 + 0.001 * 7.10537576675415
Epoch 380, val loss: 0.8506438732147217
Epoch 390, training loss: 0.06092846766114235 = 0.053829703480005264 + 0.001 * 7.098764419555664
Epoch 390, val loss: 0.867789626121521
Epoch 400, training loss: 0.05508154630661011 = 0.047989439219236374 + 0.001 * 7.092108726501465
Epoch 400, val loss: 0.8851152062416077
Epoch 410, training loss: 0.050053901970386505 = 0.04296739026904106 + 0.001 * 7.086512565612793
Epoch 410, val loss: 0.9023683667182922
Epoch 420, training loss: 0.04570409283041954 = 0.03862275928258896 + 0.001 * 7.081333160400391
Epoch 420, val loss: 0.9194561243057251
Epoch 430, training loss: 0.041949063539505005 = 0.03484705835580826 + 0.001 * 7.102004051208496
Epoch 430, val loss: 0.9362227320671082
Epoch 440, training loss: 0.038623180240392685 = 0.03155222535133362 + 0.001 * 7.070954322814941
Epoch 440, val loss: 0.9526634216308594
Epoch 450, training loss: 0.03573286533355713 = 0.028666434809565544 + 0.001 * 7.066430568695068
Epoch 450, val loss: 0.9687415361404419
Epoch 460, training loss: 0.033192433416843414 = 0.026130810379981995 + 0.001 * 7.061621189117432
Epoch 460, val loss: 0.9844188690185547
Epoch 470, training loss: 0.03094853088259697 = 0.02389112487435341 + 0.001 * 7.0574049949646
Epoch 470, val loss: 0.9999293684959412
Epoch 480, training loss: 0.028956690803170204 = 0.021907871589064598 + 0.001 * 7.048819065093994
Epoch 480, val loss: 1.014906644821167
Epoch 490, training loss: 0.027206484228372574 = 0.020145362243056297 + 0.001 * 7.061121940612793
Epoch 490, val loss: 1.0296401977539062
Epoch 500, training loss: 0.025621715933084488 = 0.018573714420199394 + 0.001 * 7.048000335693359
Epoch 500, val loss: 1.0438640117645264
Epoch 510, training loss: 0.024216433987021446 = 0.017168469727039337 + 0.001 * 7.047964572906494
Epoch 510, val loss: 1.0578327178955078
Epoch 520, training loss: 0.022927802056074142 = 0.015908634290099144 + 0.001 * 7.019166946411133
Epoch 520, val loss: 1.0714372396469116
Epoch 530, training loss: 0.02180749922990799 = 0.014776387251913548 + 0.001 * 7.031111240386963
Epoch 530, val loss: 1.0846538543701172
Epoch 540, training loss: 0.02075991779565811 = 0.013756205327808857 + 0.001 * 7.003713130950928
Epoch 540, val loss: 1.0975013971328735
Epoch 550, training loss: 0.019861388951539993 = 0.0128348832949996 + 0.001 * 7.026506423950195
Epoch 550, val loss: 1.1100409030914307
Epoch 560, training loss: 0.019012048840522766 = 0.012001067399978638 + 0.001 * 7.010981559753418
Epoch 560, val loss: 1.1222147941589355
Epoch 570, training loss: 0.018243424594402313 = 0.011244389228522778 + 0.001 * 6.999034404754639
Epoch 570, val loss: 1.1340479850769043
Epoch 580, training loss: 0.017573967576026917 = 0.01055618654936552 + 0.001 * 7.01777982711792
Epoch 580, val loss: 1.1455588340759277
Epoch 590, training loss: 0.016914449632167816 = 0.009928976185619831 + 0.001 * 6.985472679138184
Epoch 590, val loss: 1.1567565202713013
Epoch 600, training loss: 0.016354497522115707 = 0.009356055408716202 + 0.001 * 6.998441696166992
Epoch 600, val loss: 1.1676394939422607
Epoch 610, training loss: 0.015827838331460953 = 0.008831620216369629 + 0.001 * 6.996216773986816
Epoch 610, val loss: 1.1782244443893433
Epoch 620, training loss: 0.015352275222539902 = 0.00835039746016264 + 0.001 * 7.001877307891846
Epoch 620, val loss: 1.1885013580322266
Epoch 630, training loss: 0.014883381314575672 = 0.007908018305897713 + 0.001 * 6.975362777709961
Epoch 630, val loss: 1.198508620262146
Epoch 640, training loss: 0.014468880370259285 = 0.007500601466745138 + 0.001 * 6.968278408050537
Epoch 640, val loss: 1.2082937955856323
Epoch 650, training loss: 0.014094391837716103 = 0.007124750874936581 + 0.001 * 6.969640254974365
Epoch 650, val loss: 1.2177892923355103
Epoch 660, training loss: 0.013754785992205143 = 0.006777250673621893 + 0.001 * 6.977534770965576
Epoch 660, val loss: 1.2270349264144897
Epoch 670, training loss: 0.01341186836361885 = 0.006455495487898588 + 0.001 * 6.95637321472168
Epoch 670, val loss: 1.2360997200012207
Epoch 680, training loss: 0.013124853372573853 = 0.006157040596008301 + 0.001 * 6.967812538146973
Epoch 680, val loss: 1.244876503944397
Epoch 690, training loss: 0.012827849015593529 = 0.0058798715472221375 + 0.001 * 6.947977542877197
Epoch 690, val loss: 1.2534955739974976
Epoch 700, training loss: 0.012595059350132942 = 0.00562199717387557 + 0.001 * 6.973062515258789
Epoch 700, val loss: 1.2618498802185059
Epoch 710, training loss: 0.012331503443419933 = 0.005381696857511997 + 0.001 * 6.949806213378906
Epoch 710, val loss: 1.2699915170669556
Epoch 720, training loss: 0.012105727568268776 = 0.005157446023076773 + 0.001 * 6.948281288146973
Epoch 720, val loss: 1.2779754400253296
Epoch 730, training loss: 0.011889539659023285 = 0.004947883542627096 + 0.001 * 6.94165563583374
Epoch 730, val loss: 1.2857601642608643
Epoch 740, training loss: 0.01169918105006218 = 0.004751693457365036 + 0.001 * 6.947486877441406
Epoch 740, val loss: 1.293344259262085
Epoch 750, training loss: 0.011503973975777626 = 0.004567827098071575 + 0.001 * 6.936146259307861
Epoch 750, val loss: 1.300721526145935
Epoch 760, training loss: 0.011325260624289513 = 0.004395306576043367 + 0.001 * 6.929953098297119
Epoch 760, val loss: 1.3080178499221802
Epoch 770, training loss: 0.011170102283358574 = 0.004233164247125387 + 0.001 * 6.936938285827637
Epoch 770, val loss: 1.3150689601898193
Epoch 780, training loss: 0.011020270176231861 = 0.004080681595951319 + 0.001 * 6.9395880699157715
Epoch 780, val loss: 1.3220031261444092
Epoch 790, training loss: 0.010856370441615582 = 0.003937076777219772 + 0.001 * 6.919293403625488
Epoch 790, val loss: 1.32876718044281
Epoch 800, training loss: 0.010733822360634804 = 0.0038016631733626127 + 0.001 * 6.932158470153809
Epoch 800, val loss: 1.3353878259658813
Epoch 810, training loss: 0.010586581192910671 = 0.003673854283988476 + 0.001 * 6.912726402282715
Epoch 810, val loss: 1.3418699502944946
Epoch 820, training loss: 0.010527919046580791 = 0.00355309690348804 + 0.001 * 6.974822044372559
Epoch 820, val loss: 1.3482662439346313
Epoch 830, training loss: 0.010369173251092434 = 0.00343898287974298 + 0.001 * 6.930189609527588
Epoch 830, val loss: 1.3543460369110107
Epoch 840, training loss: 0.010244840756058693 = 0.0033309971913695335 + 0.001 * 6.913842678070068
Epoch 840, val loss: 1.3603941202163696
Epoch 850, training loss: 0.010152848437428474 = 0.0032286399509757757 + 0.001 * 6.924208164215088
Epoch 850, val loss: 1.3663451671600342
Epoch 860, training loss: 0.010029526427388191 = 0.003131597302854061 + 0.001 * 6.8979291915893555
Epoch 860, val loss: 1.372136116027832
Epoch 870, training loss: 0.009953980334103107 = 0.0030394194182008505 + 0.001 * 6.914560794830322
Epoch 870, val loss: 1.3778096437454224
Epoch 880, training loss: 0.009839528240263462 = 0.0029518825467675924 + 0.001 * 6.887645244598389
Epoch 880, val loss: 1.383341908454895
Epoch 890, training loss: 0.009762280620634556 = 0.002868605311959982 + 0.001 * 6.893674850463867
Epoch 890, val loss: 1.38880455493927
Epoch 900, training loss: 0.00966932624578476 = 0.0027893739752471447 + 0.001 * 6.879952430725098
Epoch 900, val loss: 1.3941725492477417
Epoch 910, training loss: 0.00958581268787384 = 0.0027138988953083754 + 0.001 * 6.871913433074951
Epoch 910, val loss: 1.3993589878082275
Epoch 920, training loss: 0.009514285251498222 = 0.0026419598143547773 + 0.001 * 6.872325420379639
Epoch 920, val loss: 1.4045015573501587
Epoch 930, training loss: 0.009440986439585686 = 0.002573289442807436 + 0.001 * 6.867697238922119
Epoch 930, val loss: 1.4094963073730469
Epoch 940, training loss: 0.009367484599351883 = 0.0025078351609408855 + 0.001 * 6.859649658203125
Epoch 940, val loss: 1.4144718647003174
Epoch 950, training loss: 0.009307323023676872 = 0.0024451904464513063 + 0.001 * 6.862132549285889
Epoch 950, val loss: 1.4192646741867065
Epoch 960, training loss: 0.009239083155989647 = 0.0023853748571127653 + 0.001 * 6.853707790374756
Epoch 960, val loss: 1.4240152835845947
Epoch 970, training loss: 0.009191131219267845 = 0.0023281604517251253 + 0.001 * 6.86297082901001
Epoch 970, val loss: 1.428648829460144
Epoch 980, training loss: 0.009108621627092361 = 0.0022734212689101696 + 0.001 * 6.83519983291626
Epoch 980, val loss: 1.4332107305526733
Epoch 990, training loss: 0.009072466753423214 = 0.002220995957031846 + 0.001 * 6.851470470428467
Epoch 990, val loss: 1.4376312494277954
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9520
Flip ASR: 0.9422/225 nodes
The final ASR:0.70726, 0.18480, Accuracy:0.80864, 0.02229
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11598])
remove edge: torch.Size([2, 9410])
updated graph: torch.Size([2, 10452])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9378072023391724 = 1.9294334650039673 + 0.001 * 8.373708724975586
Epoch 0, val loss: 1.9371023178100586
Epoch 10, training loss: 1.9282416105270386 = 1.919867992401123 + 0.001 * 8.37360954284668
Epoch 10, val loss: 1.9278087615966797
Epoch 20, training loss: 1.9164460897445679 = 1.908072829246521 + 0.001 * 8.373265266418457
Epoch 20, val loss: 1.915724515914917
Epoch 30, training loss: 1.9000338315963745 = 1.891661286354065 + 0.001 * 8.372602462768555
Epoch 30, val loss: 1.8984687328338623
Epoch 40, training loss: 1.8763283491134644 = 1.8679571151733398 + 0.001 * 8.371236801147461
Epoch 40, val loss: 1.8736809492111206
Epoch 50, training loss: 1.843591570854187 = 1.8352237939834595 + 0.001 * 8.367721557617188
Epoch 50, val loss: 1.8405946493148804
Epoch 60, training loss: 1.8047266006469727 = 1.7963722944259644 + 0.001 * 8.354248046875
Epoch 60, val loss: 1.8044099807739258
Epoch 70, training loss: 1.764040470123291 = 1.7557920217514038 + 0.001 * 8.248431205749512
Epoch 70, val loss: 1.770562767982483
Epoch 80, training loss: 1.711005449295044 = 1.7034245729446411 + 0.001 * 7.58083963394165
Epoch 80, val loss: 1.7284305095672607
Epoch 90, training loss: 1.6378287076950073 = 1.6305010318756104 + 0.001 * 7.327705383300781
Epoch 90, val loss: 1.6697595119476318
Epoch 100, training loss: 1.542271614074707 = 1.5350323915481567 + 0.001 * 7.239223480224609
Epoch 100, val loss: 1.5920333862304688
Epoch 110, training loss: 1.4282500743865967 = 1.421065092086792 + 0.001 * 7.184973239898682
Epoch 110, val loss: 1.5008904933929443
Epoch 120, training loss: 1.3052021265029907 = 1.2980661392211914 + 0.001 * 7.1359758377075195
Epoch 120, val loss: 1.402868628501892
Epoch 130, training loss: 1.1816496849060059 = 1.1745496988296509 + 0.001 * 7.100030899047852
Epoch 130, val loss: 1.303551197052002
Epoch 140, training loss: 1.0630522966384888 = 1.0559700727462769 + 0.001 * 7.082264423370361
Epoch 140, val loss: 1.2076624631881714
Epoch 150, training loss: 0.9519006013870239 = 0.944833517074585 + 0.001 * 7.067078590393066
Epoch 150, val loss: 1.118376612663269
Epoch 160, training loss: 0.8502885103225708 = 0.8432422876358032 + 0.001 * 7.046207427978516
Epoch 160, val loss: 1.0386000871658325
Epoch 170, training loss: 0.7600544691085815 = 0.7530336380004883 + 0.001 * 7.02084493637085
Epoch 170, val loss: 0.9691559076309204
Epoch 180, training loss: 0.6817026138305664 = 0.674708902835846 + 0.001 * 6.993683338165283
Epoch 180, val loss: 0.9102153778076172
Epoch 190, training loss: 0.6141905784606934 = 0.6072213649749756 + 0.001 * 6.969184398651123
Epoch 190, val loss: 0.8620229363441467
Epoch 200, training loss: 0.5557236671447754 = 0.548768162727356 + 0.001 * 6.955516338348389
Epoch 200, val loss: 0.8248705863952637
Epoch 210, training loss: 0.5042877793312073 = 0.4973376989364624 + 0.001 * 6.950101852416992
Epoch 210, val loss: 0.7979264855384827
Epoch 220, training loss: 0.4582410752773285 = 0.4512951672077179 + 0.001 * 6.94590425491333
Epoch 220, val loss: 0.7795450687408447
Epoch 230, training loss: 0.4166027307510376 = 0.4096603989601135 + 0.001 * 6.94234561920166
Epoch 230, val loss: 0.7676540017127991
Epoch 240, training loss: 0.379006028175354 = 0.3720669448375702 + 0.001 * 6.939070701599121
Epoch 240, val loss: 0.7606884837150574
Epoch 250, training loss: 0.3452543020248413 = 0.3383185565471649 + 0.001 * 6.935741901397705
Epoch 250, val loss: 0.7577319741249084
Epoch 260, training loss: 0.31491413712501526 = 0.3079816401004791 + 0.001 * 6.932497501373291
Epoch 260, val loss: 0.758040726184845
Epoch 270, training loss: 0.28714579343795776 = 0.2802163362503052 + 0.001 * 6.929453372955322
Epoch 270, val loss: 0.7606761455535889
Epoch 280, training loss: 0.260925829410553 = 0.25399863719940186 + 0.001 * 6.927193641662598
Epoch 280, val loss: 0.7647226452827454
Epoch 290, training loss: 0.2354457676410675 = 0.22852103412151337 + 0.001 * 6.9247355461120605
Epoch 290, val loss: 0.7693928480148315
Epoch 300, training loss: 0.2105039358139038 = 0.20358148217201233 + 0.001 * 6.922458648681641
Epoch 300, val loss: 0.7743891477584839
Epoch 310, training loss: 0.18662841618061066 = 0.1797078400850296 + 0.001 * 6.920577049255371
Epoch 310, val loss: 0.7801651358604431
Epoch 320, training loss: 0.16465851664543152 = 0.15773962438106537 + 0.001 * 6.918887615203857
Epoch 320, val loss: 0.787329912185669
Epoch 330, training loss: 0.1451718807220459 = 0.13825371861457825 + 0.001 * 6.9181623458862305
Epoch 330, val loss: 0.7961516380310059
Epoch 340, training loss: 0.12828783690929413 = 0.12137141078710556 + 0.001 * 6.916424751281738
Epoch 340, val loss: 0.8068417906761169
Epoch 350, training loss: 0.11379964649677277 = 0.10688517242670059 + 0.001 * 6.914475917816162
Epoch 350, val loss: 0.819260835647583
Epoch 360, training loss: 0.10137849301099777 = 0.09446588158607483 + 0.001 * 6.912613868713379
Epoch 360, val loss: 0.8332699537277222
Epoch 370, training loss: 0.09069397300481796 = 0.08378373831510544 + 0.001 * 6.910236358642578
Epoch 370, val loss: 0.8485378623008728
Epoch 380, training loss: 0.08146974444389343 = 0.07455512136220932 + 0.001 * 6.914621353149414
Epoch 380, val loss: 0.8646795749664307
Epoch 390, training loss: 0.07346097379922867 = 0.06655281782150269 + 0.001 * 6.908158779144287
Epoch 390, val loss: 0.8812924027442932
Epoch 400, training loss: 0.06649665534496307 = 0.05959246680140495 + 0.001 * 6.904186725616455
Epoch 400, val loss: 0.8980570435523987
Epoch 410, training loss: 0.06042208522558212 = 0.05352130904793739 + 0.001 * 6.9007744789123535
Epoch 410, val loss: 0.9147162437438965
Epoch 420, training loss: 0.055130280554294586 = 0.04821336269378662 + 0.001 * 6.91691780090332
Epoch 420, val loss: 0.9311063885688782
Epoch 430, training loss: 0.05046114698052406 = 0.04356275498867035 + 0.001 * 6.898392677307129
Epoch 430, val loss: 0.9471120834350586
Epoch 440, training loss: 0.04637043923139572 = 0.039479706436395645 + 0.001 * 6.890731334686279
Epoch 440, val loss: 0.9626895189285278
Epoch 450, training loss: 0.04277578741312027 = 0.03588760271668434 + 0.001 * 6.888185977935791
Epoch 450, val loss: 0.9777480363845825
Epoch 460, training loss: 0.03960558399558067 = 0.03272048011422157 + 0.001 * 6.885103702545166
Epoch 460, val loss: 0.9922470450401306
Epoch 470, training loss: 0.036802396178245544 = 0.029921619221568108 + 0.001 * 6.880777359008789
Epoch 470, val loss: 1.0062098503112793
Epoch 480, training loss: 0.034315817058086395 = 0.027442075312137604 + 0.001 * 6.873742580413818
Epoch 480, val loss: 1.0196373462677002
Epoch 490, training loss: 0.032128121703863144 = 0.025239842012524605 + 0.001 * 6.888278961181641
Epoch 490, val loss: 1.0325342416763306
Epoch 500, training loss: 0.03015318512916565 = 0.02327844128012657 + 0.001 * 6.8747429847717285
Epoch 500, val loss: 1.0449020862579346
Epoch 510, training loss: 0.028387688100337982 = 0.021526576951146126 + 0.001 * 6.861111640930176
Epoch 510, val loss: 1.0568205118179321
Epoch 520, training loss: 0.026809338480234146 = 0.0199575275182724 + 0.001 * 6.851811408996582
Epoch 520, val loss: 1.0682376623153687
Epoch 530, training loss: 0.025402693077921867 = 0.018548501655459404 + 0.001 * 6.854191303253174
Epoch 530, val loss: 1.0792181491851807
Epoch 540, training loss: 0.024152390658855438 = 0.017279714345932007 + 0.001 * 6.872676849365234
Epoch 540, val loss: 1.0897798538208008
Epoch 550, training loss: 0.022991517558693886 = 0.016133975237607956 + 0.001 * 6.857542514801025
Epoch 550, val loss: 1.099938154220581
Epoch 560, training loss: 0.021943146362900734 = 0.01509680226445198 + 0.001 * 6.846344470977783
Epoch 560, val loss: 1.1097203493118286
Epoch 570, training loss: 0.02103709988296032 = 0.014155453070998192 + 0.001 * 6.881646633148193
Epoch 570, val loss: 1.1191293001174927
Epoch 580, training loss: 0.020135268568992615 = 0.013299104757606983 + 0.001 * 6.836162567138672
Epoch 580, val loss: 1.1281964778900146
Epoch 590, training loss: 0.019358446821570396 = 0.012518327683210373 + 0.001 * 6.840118408203125
Epoch 590, val loss: 1.136916160583496
Epoch 600, training loss: 0.018651993945240974 = 0.011804727837443352 + 0.001 * 6.847265243530273
Epoch 600, val loss: 1.1453365087509155
Epoch 610, training loss: 0.017980331555008888 = 0.011151080951094627 + 0.001 * 6.829249858856201
Epoch 610, val loss: 1.1534571647644043
Epoch 620, training loss: 0.017400916665792465 = 0.01055106706917286 + 0.001 * 6.84984827041626
Epoch 620, val loss: 1.1612876653671265
Epoch 630, training loss: 0.016818564385175705 = 0.009999159723520279 + 0.001 * 6.819403648376465
Epoch 630, val loss: 1.1688604354858398
Epoch 640, training loss: 0.016323547810316086 = 0.009490502066910267 + 0.001 * 6.83304500579834
Epoch 640, val loss: 1.1761631965637207
Epoch 650, training loss: 0.015866946429014206 = 0.009020745754241943 + 0.001 * 6.846200466156006
Epoch 650, val loss: 1.183220624923706
Epoch 660, training loss: 0.015399427153170109 = 0.008586193434894085 + 0.001 * 6.813233375549316
Epoch 660, val loss: 1.1900626420974731
Epoch 670, training loss: 0.01498114038258791 = 0.008183527737855911 + 0.001 * 6.797612190246582
Epoch 670, val loss: 1.1966980695724487
Epoch 680, training loss: 0.014604540541768074 = 0.007809690665453672 + 0.001 * 6.794848918914795
Epoch 680, val loss: 1.2030658721923828
Epoch 690, training loss: 0.014255417510867119 = 0.0074620237573981285 + 0.001 * 6.793393135070801
Epoch 690, val loss: 1.2092725038528442
Epoch 700, training loss: 0.013986477628350258 = 0.007138133980333805 + 0.001 * 6.848343849182129
Epoch 700, val loss: 1.2152602672576904
Epoch 710, training loss: 0.013634716160595417 = 0.006835952401161194 + 0.001 * 6.798763275146484
Epoch 710, val loss: 1.2210886478424072
Epoch 720, training loss: 0.013340485282242298 = 0.006553548853844404 + 0.001 * 6.786936283111572
Epoch 720, val loss: 1.2267124652862549
Epoch 730, training loss: 0.013078231364488602 = 0.00628894055262208 + 0.001 * 6.789290904998779
Epoch 730, val loss: 1.2322050333023071
Epoch 740, training loss: 0.012833508662879467 = 0.0060406154952943325 + 0.001 * 6.792892932891846
Epoch 740, val loss: 1.237545132637024
Epoch 750, training loss: 0.012606912292540073 = 0.005807131063193083 + 0.001 * 6.79978084564209
Epoch 750, val loss: 1.2427339553833008
Epoch 760, training loss: 0.012398414313793182 = 0.0055869328789412975 + 0.001 * 6.811480522155762
Epoch 760, val loss: 1.2477883100509644
Epoch 770, training loss: 0.01220371201634407 = 0.0053788465447723866 + 0.001 * 6.824865341186523
Epoch 770, val loss: 1.2527189254760742
Epoch 780, training loss: 0.011965137906372547 = 0.005181807558983564 + 0.001 * 6.783329963684082
Epoch 780, val loss: 1.257535457611084
Epoch 790, training loss: 0.011769186705350876 = 0.004994792863726616 + 0.001 * 6.7743940353393555
Epoch 790, val loss: 1.262220025062561
Epoch 800, training loss: 0.011595379561185837 = 0.004817002918571234 + 0.001 * 6.778375625610352
Epoch 800, val loss: 1.266789436340332
Epoch 810, training loss: 0.011456985026597977 = 0.004647857043892145 + 0.001 * 6.809127330780029
Epoch 810, val loss: 1.271239161491394
Epoch 820, training loss: 0.011253761127591133 = 0.00448714941740036 + 0.001 * 6.7666120529174805
Epoch 820, val loss: 1.2755471467971802
Epoch 830, training loss: 0.011105231940746307 = 0.004334405530244112 + 0.001 * 6.77082633972168
Epoch 830, val loss: 1.2797242403030396
Epoch 840, training loss: 0.01095518283545971 = 0.004189162980765104 + 0.001 * 6.766019821166992
Epoch 840, val loss: 1.2837963104248047
Epoch 850, training loss: 0.010828070342540741 = 0.004051072522997856 + 0.001 * 6.7769975662231445
Epoch 850, val loss: 1.2877262830734253
Epoch 860, training loss: 0.01069318875670433 = 0.003919652197510004 + 0.001 * 6.773536682128906
Epoch 860, val loss: 1.291548490524292
Epoch 870, training loss: 0.01055394858121872 = 0.0037945169024169445 + 0.001 * 6.759430885314941
Epoch 870, val loss: 1.295251488685608
Epoch 880, training loss: 0.010428511537611485 = 0.0036753586027771235 + 0.001 * 6.753152370452881
Epoch 880, val loss: 1.298831582069397
Epoch 890, training loss: 0.010351816192269325 = 0.0035618124529719353 + 0.001 * 6.790003299713135
Epoch 890, val loss: 1.3022788763046265
Epoch 900, training loss: 0.010208751074969769 = 0.0034536472521722317 + 0.001 * 6.755103588104248
Epoch 900, val loss: 1.3056243658065796
Epoch 910, training loss: 0.010105056688189507 = 0.003350551938638091 + 0.001 * 6.754504680633545
Epoch 910, val loss: 1.3088717460632324
Epoch 920, training loss: 0.010003783740103245 = 0.0032522953115403652 + 0.001 * 6.751488208770752
Epoch 920, val loss: 1.3119901418685913
Epoch 930, training loss: 0.009920189157128334 = 0.003158594947308302 + 0.001 * 6.761593341827393
Epoch 930, val loss: 1.3150064945220947
Epoch 940, training loss: 0.009812701493501663 = 0.003069178434088826 + 0.001 * 6.743522644042969
Epoch 940, val loss: 1.3179253339767456
Epoch 950, training loss: 0.009753890335559845 = 0.0029837852343916893 + 0.001 * 6.770104885101318
Epoch 950, val loss: 1.3207488059997559
Epoch 960, training loss: 0.00964323990046978 = 0.002902273554354906 + 0.001 * 6.740965366363525
Epoch 960, val loss: 1.3234872817993164
Epoch 970, training loss: 0.009570551104843616 = 0.0028244482818990946 + 0.001 * 6.746102809906006
Epoch 970, val loss: 1.326130747795105
Epoch 980, training loss: 0.009507499635219574 = 0.0027501177974045277 + 0.001 * 6.757381439208984
Epoch 980, val loss: 1.3286713361740112
Epoch 990, training loss: 0.00945612508803606 = 0.0026790061965584755 + 0.001 * 6.777118682861328
Epoch 990, val loss: 1.3311398029327393
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.4539
Flip ASR: 0.3600/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9523423910140991 = 1.943968653678894 + 0.001 * 8.373705863952637
Epoch 0, val loss: 1.9340330362319946
Epoch 10, training loss: 1.9408100843429565 = 1.9324365854263306 + 0.001 * 8.373494148254395
Epoch 10, val loss: 1.9218777418136597
Epoch 20, training loss: 1.926192283630371 = 1.9178193807601929 + 0.001 * 8.372856140136719
Epoch 20, val loss: 1.9053195714950562
Epoch 30, training loss: 1.9052075147628784 = 1.8968358039855957 + 0.001 * 8.371763229370117
Epoch 30, val loss: 1.8808015584945679
Epoch 40, training loss: 1.875571608543396 = 1.867201566696167 + 0.001 * 8.370037078857422
Epoch 40, val loss: 1.8473899364471436
Epoch 50, training loss: 1.8373222351074219 = 1.8289557695388794 + 0.001 * 8.366484642028809
Epoch 50, val loss: 1.8085036277770996
Epoch 60, training loss: 1.794666051864624 = 1.7863126993179321 + 0.001 * 8.353314399719238
Epoch 60, val loss: 1.770984411239624
Epoch 70, training loss: 1.7537133693695068 = 1.7454532384872437 + 0.001 * 8.260141372680664
Epoch 70, val loss: 1.7384260892868042
Epoch 80, training loss: 1.7032626867294312 = 1.69540274143219 + 0.001 * 7.859976768493652
Epoch 80, val loss: 1.6950327157974243
Epoch 90, training loss: 1.6326606273651123 = 1.6250574588775635 + 0.001 * 7.603150367736816
Epoch 90, val loss: 1.633879542350769
Epoch 100, training loss: 1.5408419370651245 = 1.5334712266921997 + 0.001 * 7.370752334594727
Epoch 100, val loss: 1.5577911138534546
Epoch 110, training loss: 1.4349091053009033 = 1.4276783466339111 + 0.001 * 7.230706691741943
Epoch 110, val loss: 1.4729499816894531
Epoch 120, training loss: 1.3266510963439941 = 1.3194284439086914 + 0.001 * 7.222665309906006
Epoch 120, val loss: 1.3881657123565674
Epoch 130, training loss: 1.221494197845459 = 1.2143137454986572 + 0.001 * 7.180477619171143
Epoch 130, val loss: 1.307955026626587
Epoch 140, training loss: 1.1216509342193604 = 1.1145105361938477 + 0.001 * 7.140420436859131
Epoch 140, val loss: 1.233771800994873
Epoch 150, training loss: 1.0278204679489136 = 1.0207111835479736 + 0.001 * 7.109227657318115
Epoch 150, val loss: 1.164475679397583
Epoch 160, training loss: 0.9402090311050415 = 0.9331238865852356 + 0.001 * 7.085148334503174
Epoch 160, val loss: 1.099424123764038
Epoch 170, training loss: 0.8596542477607727 = 0.8525869250297546 + 0.001 * 7.067324638366699
Epoch 170, val loss: 1.0399147272109985
Epoch 180, training loss: 0.7871790528297424 = 0.7801303267478943 + 0.001 * 7.048705101013184
Epoch 180, val loss: 0.9874845147132874
Epoch 190, training loss: 0.7234398722648621 = 0.7164152264595032 + 0.001 * 7.024623394012451
Epoch 190, val loss: 0.9442938566207886
Epoch 200, training loss: 0.6679050922393799 = 0.6609088778495789 + 0.001 * 6.996209144592285
Epoch 200, val loss: 0.9113610982894897
Epoch 210, training loss: 0.6192982792854309 = 0.6123179793357849 + 0.001 * 6.980318069458008
Epoch 210, val loss: 0.8877029418945312
Epoch 220, training loss: 0.5762891173362732 = 0.5693265199661255 + 0.001 * 6.962620735168457
Epoch 220, val loss: 0.8717327117919922
Epoch 230, training loss: 0.5380591154098511 = 0.531104326248169 + 0.001 * 6.9547953605651855
Epoch 230, val loss: 0.8619363903999329
Epoch 240, training loss: 0.5038422346115112 = 0.4968932569026947 + 0.001 * 6.949006080627441
Epoch 240, val loss: 0.8570417761802673
Epoch 250, training loss: 0.4727665185928345 = 0.4658222794532776 + 0.001 * 6.94422721862793
Epoch 250, val loss: 0.8557687997817993
Epoch 260, training loss: 0.4440244138240814 = 0.43708622455596924 + 0.001 * 6.938198566436768
Epoch 260, val loss: 0.8568044304847717
Epoch 270, training loss: 0.4168989360332489 = 0.40996864438056946 + 0.001 * 6.930298328399658
Epoch 270, val loss: 0.8590875864028931
Epoch 280, training loss: 0.3901285231113434 = 0.38320794701576233 + 0.001 * 6.92056941986084
Epoch 280, val loss: 0.8623747825622559
Epoch 290, training loss: 0.36325371265411377 = 0.3563443422317505 + 0.001 * 6.909371852874756
Epoch 290, val loss: 0.8660182952880859
Epoch 300, training loss: 0.33546653389930725 = 0.32856279611587524 + 0.001 * 6.903733253479004
Epoch 300, val loss: 0.8702769875526428
Epoch 310, training loss: 0.3066118657588959 = 0.29972171783447266 + 0.001 * 6.890135765075684
Epoch 310, val loss: 0.8751629590988159
Epoch 320, training loss: 0.276590496301651 = 0.2697117030620575 + 0.001 * 6.878796100616455
Epoch 320, val loss: 0.8808531165122986
Epoch 330, training loss: 0.24612531065940857 = 0.23925232887268066 + 0.001 * 6.872982501983643
Epoch 330, val loss: 0.887019693851471
Epoch 340, training loss: 0.21600541472434998 = 0.2091398388147354 + 0.001 * 6.865572452545166
Epoch 340, val loss: 0.8942943811416626
Epoch 350, training loss: 0.18755587935447693 = 0.18068210780620575 + 0.001 * 6.87377405166626
Epoch 350, val loss: 0.9031972289085388
Epoch 360, training loss: 0.1619253009557724 = 0.1550636738538742 + 0.001 * 6.861623764038086
Epoch 360, val loss: 0.9139559268951416
Epoch 370, training loss: 0.13976208865642548 = 0.13290388882160187 + 0.001 * 6.858192443847656
Epoch 370, val loss: 0.9266927242279053
Epoch 380, training loss: 0.12104496359825134 = 0.11418871581554413 + 0.001 * 6.856248378753662
Epoch 380, val loss: 0.9412111043930054
Epoch 390, training loss: 0.10538143664598465 = 0.0985267162322998 + 0.001 * 6.854716777801514
Epoch 390, val loss: 0.9574501514434814
Epoch 400, training loss: 0.09230603277683258 = 0.08545327931642532 + 0.001 * 6.852756977081299
Epoch 400, val loss: 0.9751908779144287
Epoch 410, training loss: 0.08134787529706955 = 0.07449674606323242 + 0.001 * 6.851127624511719
Epoch 410, val loss: 0.9940127730369568
Epoch 420, training loss: 0.07212768495082855 = 0.06527712941169739 + 0.001 * 6.850554466247559
Epoch 420, val loss: 1.0136916637420654
Epoch 430, training loss: 0.06434625387191772 = 0.057492949068546295 + 0.001 * 6.853301048278809
Epoch 430, val loss: 1.0338436365127563
Epoch 440, training loss: 0.05774684622883797 = 0.05089261010289192 + 0.001 * 6.854237079620361
Epoch 440, val loss: 1.054345726966858
Epoch 450, training loss: 0.052115678787231445 = 0.045268286019563675 + 0.001 * 6.8473944664001465
Epoch 450, val loss: 1.0750138759613037
Epoch 460, training loss: 0.047303635627031326 = 0.04045674204826355 + 0.001 * 6.846893787384033
Epoch 460, val loss: 1.0955438613891602
Epoch 470, training loss: 0.04316848888993263 = 0.03632305562496185 + 0.001 * 6.845434188842773
Epoch 470, val loss: 1.1156994104385376
Epoch 480, training loss: 0.039602771401405334 = 0.03275853022933006 + 0.001 * 6.844242095947266
Epoch 480, val loss: 1.1355122327804565
Epoch 490, training loss: 0.03651406615972519 = 0.029670987278223038 + 0.001 * 6.843080043792725
Epoch 490, val loss: 1.1547638177871704
Epoch 500, training loss: 0.033827509731054306 = 0.026985256001353264 + 0.001 * 6.842253684997559
Epoch 500, val loss: 1.1734906435012817
Epoch 510, training loss: 0.03148183599114418 = 0.02463941089808941 + 0.001 * 6.842424392700195
Epoch 510, val loss: 1.1916037797927856
Epoch 520, training loss: 0.02942223660647869 = 0.02258121781051159 + 0.001 * 6.8410186767578125
Epoch 520, val loss: 1.2091039419174194
Epoch 530, training loss: 0.02761055715382099 = 0.020767442882061005 + 0.001 * 6.843114376068115
Epoch 530, val loss: 1.2260075807571411
Epoch 540, training loss: 0.026001350954174995 = 0.019162343814969063 + 0.001 * 6.8390069007873535
Epoch 540, val loss: 1.2422857284545898
Epoch 550, training loss: 0.024572350084781647 = 0.017735274508595467 + 0.001 * 6.837075233459473
Epoch 550, val loss: 1.258018136024475
Epoch 560, training loss: 0.02329951897263527 = 0.01646123267710209 + 0.001 * 6.838286876678467
Epoch 560, val loss: 1.2731956243515015
Epoch 570, training loss: 0.022160112857818604 = 0.01531964261084795 + 0.001 * 6.840468883514404
Epoch 570, val loss: 1.287837028503418
Epoch 580, training loss: 0.021129954606294632 = 0.01429325807839632 + 0.001 * 6.836695671081543
Epoch 580, val loss: 1.3019846677780151
Epoch 590, training loss: 0.020201195031404495 = 0.013368431478738785 + 0.001 * 6.832763671875
Epoch 590, val loss: 1.3156160116195679
Epoch 600, training loss: 0.0193630438297987 = 0.012531553395092487 + 0.001 * 6.831490516662598
Epoch 600, val loss: 1.3287726640701294
Epoch 610, training loss: 0.01860390044748783 = 0.011772110126912594 + 0.001 * 6.831790447235107
Epoch 610, val loss: 1.3414742946624756
Epoch 620, training loss: 0.01791192591190338 = 0.011081256903707981 + 0.001 * 6.830668926239014
Epoch 620, val loss: 1.3537684679031372
Epoch 630, training loss: 0.017277928069233894 = 0.010450911708176136 + 0.001 * 6.827016353607178
Epoch 630, val loss: 1.3656436204910278
Epoch 640, training loss: 0.016702676191926003 = 0.009874355979263783 + 0.001 * 6.828319549560547
Epoch 640, val loss: 1.377121925354004
Epoch 650, training loss: 0.016171788796782494 = 0.009345738217234612 + 0.001 * 6.8260498046875
Epoch 650, val loss: 1.3882620334625244
Epoch 660, training loss: 0.015682632103562355 = 0.008859892375767231 + 0.001 * 6.822739601135254
Epoch 660, val loss: 1.3990167379379272
Epoch 670, training loss: 0.015238108113408089 = 0.008412498049438 + 0.001 * 6.82560920715332
Epoch 670, val loss: 1.4094558954238892
Epoch 680, training loss: 0.014828743413090706 = 0.007999589666724205 + 0.001 * 6.829154014587402
Epoch 680, val loss: 1.4195725917816162
Epoch 690, training loss: 0.014435259625315666 = 0.007617745082825422 + 0.001 * 6.817514896392822
Epoch 690, val loss: 1.4294103384017944
Epoch 700, training loss: 0.01407797820866108 = 0.007264088373631239 + 0.001 * 6.813889980316162
Epoch 700, val loss: 1.4389392137527466
Epoch 710, training loss: 0.013781317509710789 = 0.0069358330219984055 + 0.001 * 6.845484256744385
Epoch 710, val loss: 1.448184609413147
Epoch 720, training loss: 0.013440687209367752 = 0.006630643270909786 + 0.001 * 6.810043811798096
Epoch 720, val loss: 1.4571858644485474
Epoch 730, training loss: 0.013161523267626762 = 0.006346349138766527 + 0.001 * 6.815174102783203
Epoch 730, val loss: 1.4659223556518555
Epoch 740, training loss: 0.01290120743215084 = 0.006081124767661095 + 0.001 * 6.820082664489746
Epoch 740, val loss: 1.4744117259979248
Epoch 750, training loss: 0.012640997767448425 = 0.005833300296217203 + 0.001 * 6.807697296142578
Epoch 750, val loss: 1.4826667308807373
Epoch 760, training loss: 0.01240486465394497 = 0.005601278506219387 + 0.001 * 6.803586483001709
Epoch 760, val loss: 1.4906930923461914
Epoch 770, training loss: 0.012187030166387558 = 0.005383952520787716 + 0.001 * 6.803077697753906
Epoch 770, val loss: 1.4985074996948242
Epoch 780, training loss: 0.012002184987068176 = 0.005180032923817635 + 0.001 * 6.822152137756348
Epoch 780, val loss: 1.5060968399047852
Epoch 790, training loss: 0.011787507683038712 = 0.0049885050393640995 + 0.001 * 6.799001693725586
Epoch 790, val loss: 1.51349675655365
Epoch 800, training loss: 0.011612553149461746 = 0.004808331839740276 + 0.001 * 6.804221153259277
Epoch 800, val loss: 1.520695686340332
Epoch 810, training loss: 0.011428013443946838 = 0.004638703539967537 + 0.001 * 6.789309501647949
Epoch 810, val loss: 1.527740716934204
Epoch 820, training loss: 0.011300468817353249 = 0.004478754475712776 + 0.001 * 6.821713924407959
Epoch 820, val loss: 1.5345945358276367
Epoch 830, training loss: 0.011127838864922523 = 0.00432781595736742 + 0.001 * 6.800022602081299
Epoch 830, val loss: 1.5412440299987793
Epoch 840, training loss: 0.01097353920340538 = 0.004185173660516739 + 0.001 * 6.788365364074707
Epoch 840, val loss: 1.5477702617645264
Epoch 850, training loss: 0.010855981148779392 = 0.00405022082850337 + 0.001 * 6.805759906768799
Epoch 850, val loss: 1.5541255474090576
Epoch 860, training loss: 0.010702420026063919 = 0.003922451753169298 + 0.001 * 6.77996826171875
Epoch 860, val loss: 1.5603257417678833
Epoch 870, training loss: 0.010584206320345402 = 0.003801363753154874 + 0.001 * 6.782842636108398
Epoch 870, val loss: 1.5663681030273438
Epoch 880, training loss: 0.010468710213899612 = 0.0036864965222775936 + 0.001 * 6.78221321105957
Epoch 880, val loss: 1.572273850440979
Epoch 890, training loss: 0.010380277410149574 = 0.003577436553314328 + 0.001 * 6.802840232849121
Epoch 890, val loss: 1.5780341625213623
Epoch 900, training loss: 0.01028091087937355 = 0.003473804332315922 + 0.001 * 6.807105541229248
Epoch 900, val loss: 1.5837037563323975
Epoch 910, training loss: 0.010172875598073006 = 0.0033752762246876955 + 0.001 * 6.7975993156433105
Epoch 910, val loss: 1.589200735092163
Epoch 920, training loss: 0.010077690705657005 = 0.003281455021351576 + 0.001 * 6.79623556137085
Epoch 920, val loss: 1.5945944786071777
Epoch 930, training loss: 0.009980332106351852 = 0.0031920582987368107 + 0.001 * 6.788273811340332
Epoch 930, val loss: 1.5998622179031372
Epoch 940, training loss: 0.009888209402561188 = 0.0031064478680491447 + 0.001 * 6.7817606925964355
Epoch 940, val loss: 1.6049771308898926
Epoch 950, training loss: 0.009808877483010292 = 0.003024909645318985 + 0.001 * 6.783967971801758
Epoch 950, val loss: 1.6100088357925415
Epoch 960, training loss: 0.009728698059916496 = 0.002947279019281268 + 0.001 * 6.781418323516846
Epoch 960, val loss: 1.614898920059204
Epoch 970, training loss: 0.009660108014941216 = 0.002873104065656662 + 0.001 * 6.787003040313721
Epoch 970, val loss: 1.6197104454040527
Epoch 980, training loss: 0.009570000693202019 = 0.0028021670877933502 + 0.001 * 6.767833232879639
Epoch 980, val loss: 1.6244052648544312
Epoch 990, training loss: 0.00949788372963667 = 0.0027343053370714188 + 0.001 * 6.763577938079834
Epoch 990, val loss: 1.629003643989563
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7222
Overall ASR: 0.1550
Flip ASR: 0.1733/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9662151336669922 = 1.9578412771224976 + 0.001 * 8.373807907104492
Epoch 0, val loss: 1.9554448127746582
Epoch 10, training loss: 1.9548369646072388 = 1.9464632272720337 + 0.001 * 8.373737335205078
Epoch 10, val loss: 1.944575309753418
Epoch 20, training loss: 1.9410195350646973 = 1.9326460361480713 + 0.001 * 8.373490333557129
Epoch 20, val loss: 1.9309574365615845
Epoch 30, training loss: 1.9216318130493164 = 1.9132587909698486 + 0.001 * 8.3729887008667
Epoch 30, val loss: 1.9115313291549683
Epoch 40, training loss: 1.893242359161377 = 1.8848702907562256 + 0.001 * 8.37201976776123
Epoch 40, val loss: 1.8832910060882568
Epoch 50, training loss: 1.8539060354232788 = 1.8455363512039185 + 0.001 * 8.369657516479492
Epoch 50, val loss: 1.8461641073226929
Epoch 60, training loss: 1.8086799383163452 = 1.8003183603286743 + 0.001 * 8.361626625061035
Epoch 60, val loss: 1.8079768419265747
Epoch 70, training loss: 1.7669401168823242 = 1.7586246728897095 + 0.001 * 8.315414428710938
Epoch 70, val loss: 1.7753205299377441
Epoch 80, training loss: 1.7156884670257568 = 1.7077230215072632 + 0.001 * 7.9654998779296875
Epoch 80, val loss: 1.7307614088058472
Epoch 90, training loss: 1.6453328132629395 = 1.637734293937683 + 0.001 * 7.5984978675842285
Epoch 90, val loss: 1.6703853607177734
Epoch 100, training loss: 1.5547722578048706 = 1.5474025011062622 + 0.001 * 7.369736194610596
Epoch 100, val loss: 1.5951169729232788
Epoch 110, training loss: 1.4536856412887573 = 1.4463955163955688 + 0.001 * 7.290157794952393
Epoch 110, val loss: 1.5136940479278564
Epoch 120, training loss: 1.3511488437652588 = 1.343910813331604 + 0.001 * 7.2380828857421875
Epoch 120, val loss: 1.4328274726867676
Epoch 130, training loss: 1.2475930452346802 = 1.2404119968414307 + 0.001 * 7.181094169616699
Epoch 130, val loss: 1.3537225723266602
Epoch 140, training loss: 1.141579508781433 = 1.1344434022903442 + 0.001 * 7.1360578536987305
Epoch 140, val loss: 1.274052619934082
Epoch 150, training loss: 1.0334417819976807 = 1.0263348817825317 + 0.001 * 7.10690975189209
Epoch 150, val loss: 1.192416787147522
Epoch 160, training loss: 0.9268893003463745 = 0.9197987914085388 + 0.001 * 7.090479850769043
Epoch 160, val loss: 1.1131099462509155
Epoch 170, training loss: 0.8280982375144958 = 0.8210211396217346 + 0.001 * 7.077110767364502
Epoch 170, val loss: 1.042252779006958
Epoch 180, training loss: 0.7416146397590637 = 0.7345526814460754 + 0.001 * 7.061951160430908
Epoch 180, val loss: 0.9833914041519165
Epoch 190, training loss: 0.667766809463501 = 0.6607275605201721 + 0.001 * 7.039224624633789
Epoch 190, val loss: 0.936947762966156
Epoch 200, training loss: 0.6042957901954651 = 0.5972846746444702 + 0.001 * 7.011089324951172
Epoch 200, val loss: 0.9012058973312378
Epoch 210, training loss: 0.5488395690917969 = 0.5418437123298645 + 0.001 * 6.995848655700684
Epoch 210, val loss: 0.8740325570106506
Epoch 220, training loss: 0.49975135922431946 = 0.4927650988101959 + 0.001 * 6.986263275146484
Epoch 220, val loss: 0.8541105389595032
Epoch 230, training loss: 0.45581310987472534 = 0.4488317370414734 + 0.001 * 6.981368064880371
Epoch 230, val loss: 0.8407295346260071
Epoch 240, training loss: 0.41611963510513306 = 0.409140408039093 + 0.001 * 6.979231357574463
Epoch 240, val loss: 0.8332096338272095
Epoch 250, training loss: 0.38004904985427856 = 0.3730713427066803 + 0.001 * 6.977719783782959
Epoch 250, val loss: 0.8307173252105713
Epoch 260, training loss: 0.34723061323165894 = 0.34025365114212036 + 0.001 * 6.976961612701416
Epoch 260, val loss: 0.8325749039649963
Epoch 270, training loss: 0.3173169195652008 = 0.31033962965011597 + 0.001 * 6.97728157043457
Epoch 270, val loss: 0.8383587002754211
Epoch 280, training loss: 0.2898779511451721 = 0.28290194272994995 + 0.001 * 6.976010799407959
Epoch 280, val loss: 0.8474761247634888
Epoch 290, training loss: 0.2644125521183014 = 0.2574373483657837 + 0.001 * 6.975212097167969
Epoch 290, val loss: 0.8596116304397583
Epoch 300, training loss: 0.24047806859016418 = 0.2335035651922226 + 0.001 * 6.974501132965088
Epoch 300, val loss: 0.8742972612380981
Epoch 310, training loss: 0.21789933741092682 = 0.2109251618385315 + 0.001 * 6.9741740226745605
Epoch 310, val loss: 0.8912849426269531
Epoch 320, training loss: 0.19675377011299133 = 0.18977490067481995 + 0.001 * 6.978873252868652
Epoch 320, val loss: 0.9102573990821838
Epoch 330, training loss: 0.17720268666744232 = 0.17022868990898132 + 0.001 * 6.973995208740234
Epoch 330, val loss: 0.9307218790054321
Epoch 340, training loss: 0.15938474237918854 = 0.15241298079490662 + 0.001 * 6.971754550933838
Epoch 340, val loss: 0.9526841640472412
Epoch 350, training loss: 0.14331986010074615 = 0.13634757697582245 + 0.001 * 6.972282886505127
Epoch 350, val loss: 0.9760650396347046
Epoch 360, training loss: 0.12894079089164734 = 0.12197066843509674 + 0.001 * 6.970123767852783
Epoch 360, val loss: 1.0008882284164429
Epoch 370, training loss: 0.11614242941141129 = 0.10916779935359955 + 0.001 * 6.9746270179748535
Epoch 370, val loss: 1.02694571018219
Epoch 380, training loss: 0.10477107018232346 = 0.09780281037092209 + 0.001 * 6.968256950378418
Epoch 380, val loss: 1.054020643234253
Epoch 390, training loss: 0.09470579028129578 = 0.08773955702781677 + 0.001 * 6.966231346130371
Epoch 390, val loss: 1.0818709135055542
Epoch 400, training loss: 0.08582264930009842 = 0.07885799556970596 + 0.001 * 6.964655876159668
Epoch 400, val loss: 1.110421061515808
Epoch 410, training loss: 0.07799550890922546 = 0.07103361189365387 + 0.001 * 6.961897373199463
Epoch 410, val loss: 1.1394602060317993
Epoch 420, training loss: 0.07111053913831711 = 0.06415054202079773 + 0.001 * 6.960000038146973
Epoch 420, val loss: 1.1686687469482422
Epoch 430, training loss: 0.06504969298839569 = 0.05809129402041435 + 0.001 * 6.958399295806885
Epoch 430, val loss: 1.1978225708007812
Epoch 440, training loss: 0.05970272421836853 = 0.052748773247003555 + 0.001 * 6.953949451446533
Epoch 440, val loss: 1.2266381978988647
Epoch 450, training loss: 0.054982222616672516 = 0.04803166165947914 + 0.001 * 6.950559139251709
Epoch 450, val loss: 1.2549870014190674
Epoch 460, training loss: 0.05080965906381607 = 0.043858036398887634 + 0.001 * 6.95162296295166
Epoch 460, val loss: 1.2827876806259155
Epoch 470, training loss: 0.04710474610328674 = 0.04015500843524933 + 0.001 * 6.94973611831665
Epoch 470, val loss: 1.3099159002304077
Epoch 480, training loss: 0.043797314167022705 = 0.03686017170548439 + 0.001 * 6.937143325805664
Epoch 480, val loss: 1.3363664150238037
Epoch 490, training loss: 0.04086454585194588 = 0.03392118588089943 + 0.001 * 6.943359851837158
Epoch 490, val loss: 1.3620842695236206
Epoch 500, training loss: 0.03822087123990059 = 0.03129303455352783 + 0.001 * 6.927834987640381
Epoch 500, val loss: 1.3870569467544556
Epoch 510, training loss: 0.03586003929376602 = 0.028937121853232384 + 0.001 * 6.922916412353516
Epoch 510, val loss: 1.411321997642517
Epoch 520, training loss: 0.03374480828642845 = 0.02681984007358551 + 0.001 * 6.924968242645264
Epoch 520, val loss: 1.4348500967025757
Epoch 530, training loss: 0.03183132782578468 = 0.024912318214774132 + 0.001 * 6.919009685516357
Epoch 530, val loss: 1.4576605558395386
Epoch 540, training loss: 0.030104463919997215 = 0.023189609870314598 + 0.001 * 6.914853572845459
Epoch 540, val loss: 1.479764461517334
Epoch 550, training loss: 0.028549812734127045 = 0.02162996679544449 + 0.001 * 6.919846534729004
Epoch 550, val loss: 1.5011272430419922
Epoch 560, training loss: 0.027115710079669952 = 0.020214157178997993 + 0.001 * 6.901552677154541
Epoch 560, val loss: 1.521845817565918
Epoch 570, training loss: 0.02584495022892952 = 0.018926480785012245 + 0.001 * 6.918467998504639
Epoch 570, val loss: 1.541890025138855
Epoch 580, training loss: 0.02463674359023571 = 0.01775296963751316 + 0.001 * 6.8837738037109375
Epoch 580, val loss: 1.5613205432891846
Epoch 590, training loss: 0.02356966957449913 = 0.016681388020515442 + 0.001 * 6.888281345367432
Epoch 590, val loss: 1.5801531076431274
Epoch 600, training loss: 0.022586628794670105 = 0.01570054702460766 + 0.001 * 6.886081695556641
Epoch 600, val loss: 1.5984338521957397
Epoch 610, training loss: 0.021705131977796555 = 0.014801316894590855 + 0.001 * 6.903815269470215
Epoch 610, val loss: 1.6161797046661377
Epoch 620, training loss: 0.020857729017734528 = 0.013975279405713081 + 0.001 * 6.882450103759766
Epoch 620, val loss: 1.6334047317504883
Epoch 630, training loss: 0.020109133794903755 = 0.013214939273893833 + 0.001 * 6.89419412612915
Epoch 630, val loss: 1.6501444578170776
Epoch 640, training loss: 0.019399164244532585 = 0.012513871304690838 + 0.001 * 6.8852925300598145
Epoch 640, val loss: 1.666463851928711
Epoch 650, training loss: 0.01872788369655609 = 0.011866406537592411 + 0.001 * 6.861475944519043
Epoch 650, val loss: 1.6823090314865112
Epoch 660, training loss: 0.018130769953131676 = 0.011267256923019886 + 0.001 * 6.863512992858887
Epoch 660, val loss: 1.6977357864379883
Epoch 670, training loss: 0.017573563382029533 = 0.010712048970162868 + 0.001 * 6.861514091491699
Epoch 670, val loss: 1.7127693891525269
Epoch 680, training loss: 0.01704944111406803 = 0.010196741670370102 + 0.001 * 6.852698802947998
Epoch 680, val loss: 1.727389931678772
Epoch 690, training loss: 0.016580844298005104 = 0.009717785753309727 + 0.001 * 6.863057613372803
Epoch 690, val loss: 1.7416261434555054
Epoch 700, training loss: 0.016119353473186493 = 0.009272098541259766 + 0.001 * 6.847254753112793
Epoch 700, val loss: 1.7555123567581177
Epoch 710, training loss: 0.015718020498752594 = 0.0088566979393363 + 0.001 * 6.861323356628418
Epoch 710, val loss: 1.7690467834472656
Epoch 720, training loss: 0.015316098928451538 = 0.00846890639513731 + 0.001 * 6.84719181060791
Epoch 720, val loss: 1.782277226448059
Epoch 730, training loss: 0.014939989894628525 = 0.008106378838419914 + 0.001 * 6.833611011505127
Epoch 730, val loss: 1.7951771020889282
Epoch 740, training loss: 0.014614751562476158 = 0.007766890339553356 + 0.001 * 6.847861289978027
Epoch 740, val loss: 1.8077549934387207
Epoch 750, training loss: 0.014278952032327652 = 0.007448463700711727 + 0.001 * 6.830488681793213
Epoch 750, val loss: 1.8200500011444092
Epoch 760, training loss: 0.013994662091135979 = 0.007149276323616505 + 0.001 * 6.8453850746154785
Epoch 760, val loss: 1.832062840461731
Epoch 770, training loss: 0.01370135135948658 = 0.006867561489343643 + 0.001 * 6.833789825439453
Epoch 770, val loss: 1.8438061475753784
Epoch 780, training loss: 0.013452553190290928 = 0.006601692643016577 + 0.001 * 6.850860118865967
Epoch 780, val loss: 1.8552693128585815
Epoch 790, training loss: 0.013199248351156712 = 0.006350790616124868 + 0.001 * 6.848457336425781
Epoch 790, val loss: 1.866513967514038
Epoch 800, training loss: 0.01298556663095951 = 0.006113531067967415 + 0.001 * 6.872034549713135
Epoch 800, val loss: 1.877480387687683
Epoch 810, training loss: 0.012734300456941128 = 0.005889204330742359 + 0.001 * 6.845095634460449
Epoch 810, val loss: 1.888198971748352
Epoch 820, training loss: 0.01250524166971445 = 0.005676644388586283 + 0.001 * 6.828597068786621
Epoch 820, val loss: 1.89874267578125
Epoch 830, training loss: 0.012314708903431892 = 0.005474998615682125 + 0.001 * 6.839709281921387
Epoch 830, val loss: 1.909026861190796
Epoch 840, training loss: 0.01210970338433981 = 0.0052837468683719635 + 0.001 * 6.825956344604492
Epoch 840, val loss: 1.9191404581069946
Epoch 850, training loss: 0.01191810704767704 = 0.005102162715047598 + 0.001 * 6.815944671630859
Epoch 850, val loss: 1.9290199279785156
Epoch 860, training loss: 0.011749893426895142 = 0.004929995629936457 + 0.001 * 6.819897651672363
Epoch 860, val loss: 1.938716173171997
Epoch 870, training loss: 0.011572552844882011 = 0.004766501486301422 + 0.001 * 6.806050777435303
Epoch 870, val loss: 1.9482046365737915
Epoch 880, training loss: 0.011442137882113457 = 0.004611060954630375 + 0.001 * 6.831076622009277
Epoch 880, val loss: 1.957524299621582
Epoch 890, training loss: 0.011263302527368069 = 0.004463316407054663 + 0.001 * 6.799985885620117
Epoch 890, val loss: 1.9666334390640259
Epoch 900, training loss: 0.011113202199339867 = 0.004322731401771307 + 0.001 * 6.790471076965332
Epoch 900, val loss: 1.9755760431289673
Epoch 910, training loss: 0.01098712533712387 = 0.004188856575638056 + 0.001 * 6.7982683181762695
Epoch 910, val loss: 1.9843336343765259
Epoch 920, training loss: 0.010848559439182281 = 0.004061337560415268 + 0.001 * 6.787221431732178
Epoch 920, val loss: 1.992936372756958
Epoch 930, training loss: 0.010736318305134773 = 0.003939906135201454 + 0.001 * 6.796412467956543
Epoch 930, val loss: 2.0013341903686523
Epoch 940, training loss: 0.010634806007146835 = 0.0038243611343204975 + 0.001 * 6.810443878173828
Epoch 940, val loss: 2.009572744369507
Epoch 950, training loss: 0.01050163246691227 = 0.003714208258315921 + 0.001 * 6.787424087524414
Epoch 950, val loss: 2.017627000808716
Epoch 960, training loss: 0.010402066633105278 = 0.0036091150250285864 + 0.001 * 6.792951583862305
Epoch 960, val loss: 2.025559902191162
Epoch 970, training loss: 0.010295126587152481 = 0.0035088094882667065 + 0.001 * 6.786316394805908
Epoch 970, val loss: 2.033329963684082
Epoch 980, training loss: 0.010200916789472103 = 0.0034130539279431105 + 0.001 * 6.787862777709961
Epoch 980, val loss: 2.0409555435180664
Epoch 990, training loss: 0.010110164061188698 = 0.0033215717412531376 + 0.001 * 6.788592338562012
Epoch 990, val loss: 2.0484397411346436
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7741
Overall ASR: 0.8561
Flip ASR: 0.8267/225 nodes
The final ASR:0.48831, 0.28726, Accuracy:0.76296, 0.02978
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11578])
remove edge: torch.Size([2, 9514])
updated graph: torch.Size([2, 10536])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98032, 0.00758, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9593417644500732 = 1.9509680271148682 + 0.001 * 8.373702049255371
Epoch 0, val loss: 1.9522709846496582
Epoch 10, training loss: 1.949045181274414 = 1.9406715631484985 + 0.001 * 8.373608589172363
Epoch 10, val loss: 1.9429199695587158
Epoch 20, training loss: 1.9363906383514404 = 1.9280173778533936 + 0.001 * 8.373276710510254
Epoch 20, val loss: 1.9310053586959839
Epoch 30, training loss: 1.9186983108520508 = 1.9103257656097412 + 0.001 * 8.372564315795898
Epoch 30, val loss: 1.9141845703125
Epoch 40, training loss: 1.8925347328186035 = 1.884163737297058 + 0.001 * 8.371047019958496
Epoch 40, val loss: 1.8895097970962524
Epoch 50, training loss: 1.855285406112671 = 1.8469185829162598 + 0.001 * 8.366869926452637
Epoch 50, val loss: 1.8557521104812622
Epoch 60, training loss: 1.8106440305709839 = 1.8022946119308472 + 0.001 * 8.34941577911377
Epoch 60, val loss: 1.8188865184783936
Epoch 70, training loss: 1.7691386938095093 = 1.760908842086792 + 0.001 * 8.229814529418945
Epoch 70, val loss: 1.7864782810211182
Epoch 80, training loss: 1.7201188802719116 = 1.71237313747406 + 0.001 * 7.745730400085449
Epoch 80, val loss: 1.7433452606201172
Epoch 90, training loss: 1.6518152952194214 = 1.6444082260131836 + 0.001 * 7.407029628753662
Epoch 90, val loss: 1.6855522394180298
Epoch 100, training loss: 1.5623492002487183 = 1.5551217794418335 + 0.001 * 7.227436065673828
Epoch 100, val loss: 1.6139256954193115
Epoch 110, training loss: 1.454797387123108 = 1.4476022720336914 + 0.001 * 7.195162773132324
Epoch 110, val loss: 1.5269865989685059
Epoch 120, training loss: 1.3400847911834717 = 1.3329360485076904 + 0.001 * 7.148782730102539
Epoch 120, val loss: 1.433702826499939
Epoch 130, training loss: 1.2269502878189087 = 1.219868779182434 + 0.001 * 7.081524848937988
Epoch 130, val loss: 1.3416759967803955
Epoch 140, training loss: 1.119953989982605 = 1.112951636314392 + 0.001 * 7.0023274421691895
Epoch 140, val loss: 1.2551671266555786
Epoch 150, training loss: 1.020340085029602 = 1.01338529586792 + 0.001 * 6.954783916473389
Epoch 150, val loss: 1.1757431030273438
Epoch 160, training loss: 0.9274566173553467 = 0.9205334186553955 + 0.001 * 6.923182964324951
Epoch 160, val loss: 1.1034663915634155
Epoch 170, training loss: 0.8407093286514282 = 0.8338149189949036 + 0.001 * 6.894415855407715
Epoch 170, val loss: 1.0376449823379517
Epoch 180, training loss: 0.7609370946884155 = 0.7540627717971802 + 0.001 * 6.874296188354492
Epoch 180, val loss: 0.9790638089179993
Epoch 190, training loss: 0.6890674233436584 = 0.6822023987770081 + 0.001 * 6.865047931671143
Epoch 190, val loss: 0.9288597106933594
Epoch 200, training loss: 0.6249955892562866 = 0.6181358098983765 + 0.001 * 6.859790325164795
Epoch 200, val loss: 0.8875040411949158
Epoch 210, training loss: 0.5677367448806763 = 0.5608819723129272 + 0.001 * 6.854759216308594
Epoch 210, val loss: 0.8544963002204895
Epoch 220, training loss: 0.5161501169204712 = 0.5093016624450684 + 0.001 * 6.848434925079346
Epoch 220, val loss: 0.8290414214134216
Epoch 230, training loss: 0.46919581294059753 = 0.46235141158103943 + 0.001 * 6.844412803649902
Epoch 230, val loss: 0.8100218176841736
Epoch 240, training loss: 0.426117479801178 = 0.41928353905677795 + 0.001 * 6.833945274353027
Epoch 240, val loss: 0.796576976776123
Epoch 250, training loss: 0.3865784704685211 = 0.37975364923477173 + 0.001 * 6.824832439422607
Epoch 250, val loss: 0.7880573868751526
Epoch 260, training loss: 0.3505478501319885 = 0.3437315821647644 + 0.001 * 6.81626033782959
Epoch 260, val loss: 0.7840593457221985
Epoch 270, training loss: 0.3178308308124542 = 0.31102684140205383 + 0.001 * 6.803985118865967
Epoch 270, val loss: 0.7838144898414612
Epoch 280, training loss: 0.2877880036830902 = 0.2809900939464569 + 0.001 * 6.7979207038879395
Epoch 280, val loss: 0.7863436937332153
Epoch 290, training loss: 0.25963127613067627 = 0.25281471014022827 + 0.001 * 6.816568374633789
Epoch 290, val loss: 0.7906164526939392
Epoch 300, training loss: 0.23273539543151855 = 0.22595326602458954 + 0.001 * 6.782132148742676
Epoch 300, val loss: 0.7960421442985535
Epoch 310, training loss: 0.20710758864879608 = 0.20034077763557434 + 0.001 * 6.766808986663818
Epoch 310, val loss: 0.8022907972335815
Epoch 320, training loss: 0.1831086128950119 = 0.17634133994579315 + 0.001 * 6.767267227172852
Epoch 320, val loss: 0.8094108700752258
Epoch 330, training loss: 0.1612396389245987 = 0.15447549521923065 + 0.001 * 6.764147758483887
Epoch 330, val loss: 0.8176732659339905
Epoch 340, training loss: 0.14184214174747467 = 0.13508902490139008 + 0.001 * 6.753116130828857
Epoch 340, val loss: 0.8272530436515808
Epoch 350, training loss: 0.12498790770769119 = 0.11824484169483185 + 0.001 * 6.743066310882568
Epoch 350, val loss: 0.8383254408836365
Epoch 360, training loss: 0.11051762849092484 = 0.1037730947136879 + 0.001 * 6.74453067779541
Epoch 360, val loss: 0.8507957458496094
Epoch 370, training loss: 0.09812523424625397 = 0.09138540923595428 + 0.001 * 6.7398271560668945
Epoch 370, val loss: 0.8645210862159729
Epoch 380, training loss: 0.08751998096704483 = 0.08075913786888123 + 0.001 * 6.7608442306518555
Epoch 380, val loss: 0.8792036175727844
Epoch 390, training loss: 0.07833526283502579 = 0.07160063832998276 + 0.001 * 6.734621047973633
Epoch 390, val loss: 0.8944636583328247
Epoch 400, training loss: 0.07040062546730042 = 0.06367318332195282 + 0.001 * 6.727441310882568
Epoch 400, val loss: 0.9100592732429504
Epoch 410, training loss: 0.0635099709033966 = 0.05678596720099449 + 0.001 * 6.723999977111816
Epoch 410, val loss: 0.9256616830825806
Epoch 420, training loss: 0.05750926956534386 = 0.050787702202796936 + 0.001 * 6.721568584442139
Epoch 420, val loss: 0.941088855266571
Epoch 430, training loss: 0.05228734388947487 = 0.045554302632808685 + 0.001 * 6.733042240142822
Epoch 430, val loss: 0.9562214016914368
Epoch 440, training loss: 0.047696832567453384 = 0.040981464087963104 + 0.001 * 6.715368747711182
Epoch 440, val loss: 0.9709537029266357
Epoch 450, training loss: 0.043716803193092346 = 0.036979854106903076 + 0.001 * 6.736948013305664
Epoch 450, val loss: 0.9852555990219116
Epoch 460, training loss: 0.04018842428922653 = 0.03347388282418251 + 0.001 * 6.714539051055908
Epoch 460, val loss: 0.9990819692611694
Epoch 470, training loss: 0.03711331635713577 = 0.0303964763879776 + 0.001 * 6.7168378829956055
Epoch 470, val loss: 1.0124166011810303
Epoch 480, training loss: 0.03440459445118904 = 0.027689989656209946 + 0.001 * 6.714602947235107
Epoch 480, val loss: 1.0252934694290161
Epoch 490, training loss: 0.03201993182301521 = 0.025304744020104408 + 0.001 * 6.715188503265381
Epoch 490, val loss: 1.0376888513565063
Epoch 500, training loss: 0.029917627573013306 = 0.023197468370199203 + 0.001 * 6.72015905380249
Epoch 500, val loss: 1.0496325492858887
Epoch 510, training loss: 0.028036225587129593 = 0.02133045345544815 + 0.001 * 6.705772399902344
Epoch 510, val loss: 1.0611317157745361
Epoch 520, training loss: 0.02638355642557144 = 0.019671369343996048 + 0.001 * 6.71218729019165
Epoch 520, val loss: 1.0722295045852661
Epoch 530, training loss: 0.02489370107650757 = 0.018192969262599945 + 0.001 * 6.700730800628662
Epoch 530, val loss: 1.082909107208252
Epoch 540, training loss: 0.023580878973007202 = 0.016871877014636993 + 0.001 * 6.7090020179748535
Epoch 540, val loss: 1.0932035446166992
Epoch 550, training loss: 0.022388994693756104 = 0.01568768359720707 + 0.001 * 6.7013115882873535
Epoch 550, val loss: 1.1031253337860107
Epoch 560, training loss: 0.021343449130654335 = 0.014623110182583332 + 0.001 * 6.720338821411133
Epoch 560, val loss: 1.1127040386199951
Epoch 570, training loss: 0.020361294969916344 = 0.01366348285228014 + 0.001 * 6.697812080383301
Epoch 570, val loss: 1.1219364404678345
Epoch 580, training loss: 0.01950441114604473 = 0.012795860879123211 + 0.001 * 6.708549499511719
Epoch 580, val loss: 1.1308623552322388
Epoch 590, training loss: 0.018713759258389473 = 0.012009257450699806 + 0.001 * 6.704502105712891
Epoch 590, val loss: 1.1394917964935303
Epoch 600, training loss: 0.017986560240387917 = 0.011294359341263771 + 0.001 * 6.692200183868408
Epoch 600, val loss: 1.1478400230407715
Epoch 610, training loss: 0.017332572489976883 = 0.010642838664352894 + 0.001 * 6.689733505249023
Epoch 610, val loss: 1.1559205055236816
Epoch 620, training loss: 0.016742557287216187 = 0.010047581046819687 + 0.001 * 6.69497537612915
Epoch 620, val loss: 1.1637572050094604
Epoch 630, training loss: 0.016203800216317177 = 0.009502473287284374 + 0.001 * 6.701326847076416
Epoch 630, val loss: 1.1713454723358154
Epoch 640, training loss: 0.01569652371108532 = 0.00900223571807146 + 0.001 * 6.694287300109863
Epoch 640, val loss: 1.1787042617797852
Epoch 650, training loss: 0.015245545655488968 = 0.008542170748114586 + 0.001 * 6.703374862670898
Epoch 650, val loss: 1.1858242750167847
Epoch 660, training loss: 0.014813655987381935 = 0.008118114434182644 + 0.001 * 6.6955413818359375
Epoch 660, val loss: 1.1927412748336792
Epoch 670, training loss: 0.014415617100894451 = 0.007726456969976425 + 0.001 * 6.689159870147705
Epoch 670, val loss: 1.1994483470916748
Epoch 680, training loss: 0.014044279232621193 = 0.007364065386354923 + 0.001 * 6.680213928222656
Epoch 680, val loss: 1.2059533596038818
Epoch 690, training loss: 0.013705635443329811 = 0.007028058171272278 + 0.001 * 6.677576541900635
Epoch 690, val loss: 1.2122783660888672
Epoch 700, training loss: 0.01341872289776802 = 0.006715959869325161 + 0.001 * 6.702762126922607
Epoch 700, val loss: 1.2184321880340576
Epoch 710, training loss: 0.01311677135527134 = 0.006425670348107815 + 0.001 * 6.691100597381592
Epoch 710, val loss: 1.2244057655334473
Epoch 720, training loss: 0.012848680838942528 = 0.0061551653780043125 + 0.001 * 6.693514823913574
Epoch 720, val loss: 1.2302250862121582
Epoch 730, training loss: 0.012578589841723442 = 0.0059026870876550674 + 0.001 * 6.675902843475342
Epoch 730, val loss: 1.2358732223510742
Epoch 740, training loss: 0.012339829467236996 = 0.005666667129844427 + 0.001 * 6.67316198348999
Epoch 740, val loss: 1.2413849830627441
Epoch 750, training loss: 0.012121299281716347 = 0.005445708055049181 + 0.001 * 6.675590991973877
Epoch 750, val loss: 1.24674391746521
Epoch 760, training loss: 0.01190805435180664 = 0.005238573532551527 + 0.001 * 6.669479846954346
Epoch 760, val loss: 1.2519450187683105
Epoch 770, training loss: 0.011719072237610817 = 0.005044141784310341 + 0.001 * 6.674930572509766
Epoch 770, val loss: 1.2570027112960815
Epoch 780, training loss: 0.011545272544026375 = 0.004861410707235336 + 0.001 * 6.683861255645752
Epoch 780, val loss: 1.2619160413742065
Epoch 790, training loss: 0.011368105188012123 = 0.004689441993832588 + 0.001 * 6.6786627769470215
Epoch 790, val loss: 1.266705870628357
Epoch 800, training loss: 0.011199425905942917 = 0.004527418874204159 + 0.001 * 6.672006607055664
Epoch 800, val loss: 1.2713866233825684
Epoch 810, training loss: 0.011039421893656254 = 0.0043745627626776695 + 0.001 * 6.664858818054199
Epoch 810, val loss: 1.2759519815444946
Epoch 820, training loss: 0.010895159095525742 = 0.004230216145515442 + 0.001 * 6.664942264556885
Epoch 820, val loss: 1.280413031578064
Epoch 830, training loss: 0.010769539512693882 = 0.004093702882528305 + 0.001 * 6.675836086273193
Epoch 830, val loss: 1.2847692966461182
Epoch 840, training loss: 0.010641773231327534 = 0.003964511677622795 + 0.001 * 6.6772613525390625
Epoch 840, val loss: 1.2890249490737915
Epoch 850, training loss: 0.010512343607842922 = 0.003842149395495653 + 0.001 * 6.670193672180176
Epoch 850, val loss: 1.2931632995605469
Epoch 860, training loss: 0.010406577959656715 = 0.0037261182442307472 + 0.001 * 6.680459022521973
Epoch 860, val loss: 1.297212839126587
Epoch 870, training loss: 0.010301616974174976 = 0.003615994704887271 + 0.001 * 6.685622215270996
Epoch 870, val loss: 1.3011647462844849
Epoch 880, training loss: 0.010182445868849754 = 0.0035113340709358454 + 0.001 * 6.671111106872559
Epoch 880, val loss: 1.3050246238708496
Epoch 890, training loss: 0.010083497501909733 = 0.0034118674229830503 + 0.001 * 6.671629905700684
Epoch 890, val loss: 1.3087947368621826
Epoch 900, training loss: 0.009966934099793434 = 0.003317193128168583 + 0.001 * 6.649740695953369
Epoch 900, val loss: 1.312477469444275
Epoch 910, training loss: 0.009890993125736713 = 0.0032270236406475306 + 0.001 * 6.66396951675415
Epoch 910, val loss: 1.3160871267318726
Epoch 920, training loss: 0.009815869852900505 = 0.003141092834994197 + 0.001 * 6.674776554107666
Epoch 920, val loss: 1.3196063041687012
Epoch 930, training loss: 0.009718259796500206 = 0.003059128299355507 + 0.001 * 6.659130573272705
Epoch 930, val loss: 1.323062777519226
Epoch 940, training loss: 0.009639119729399681 = 0.002980884863063693 + 0.001 * 6.658234596252441
Epoch 940, val loss: 1.3264285326004028
Epoch 950, training loss: 0.009563183411955833 = 0.0029061511158943176 + 0.001 * 6.657032012939453
Epoch 950, val loss: 1.329719066619873
Epoch 960, training loss: 0.009487209841609001 = 0.0028347037732601166 + 0.001 * 6.652505874633789
Epoch 960, val loss: 1.3329451084136963
Epoch 970, training loss: 0.009415235370397568 = 0.002766344929113984 + 0.001 * 6.648890018463135
Epoch 970, val loss: 1.3360965251922607
Epoch 980, training loss: 0.009368770755827427 = 0.002700915327295661 + 0.001 * 6.6678547859191895
Epoch 980, val loss: 1.339186191558838
Epoch 990, training loss: 0.009308625012636185 = 0.002638248261064291 + 0.001 * 6.670375823974609
Epoch 990, val loss: 1.342198371887207
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.4207
Flip ASR: 0.3200/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9732705354690552 = 1.9648966789245605 + 0.001 * 8.373822212219238
Epoch 0, val loss: 1.958246111869812
Epoch 10, training loss: 1.9622557163238525 = 1.9538819789886475 + 0.001 * 8.373685836791992
Epoch 10, val loss: 1.9471392631530762
Epoch 20, training loss: 1.9485777616500854 = 1.940204381942749 + 0.001 * 8.373366355895996
Epoch 20, val loss: 1.9327526092529297
Epoch 30, training loss: 1.92934250831604 = 1.9209697246551514 + 0.001 * 8.37275218963623
Epoch 30, val loss: 1.9123200178146362
Epoch 40, training loss: 1.9009315967559814 = 1.8925600051879883 + 0.001 * 8.371535301208496
Epoch 40, val loss: 1.8825570344924927
Epoch 50, training loss: 1.8605033159255981 = 1.8521345853805542 + 0.001 * 8.368704795837402
Epoch 50, val loss: 1.8420065641403198
Epoch 60, training loss: 1.8122367858886719 = 1.8038779497146606 + 0.001 * 8.358789443969727
Epoch 60, val loss: 1.7971818447113037
Epoch 70, training loss: 1.7677757740020752 = 1.759474277496338 + 0.001 * 8.301528930664062
Epoch 70, val loss: 1.7581888437271118
Epoch 80, training loss: 1.718887448310852 = 1.7109649181365967 + 0.001 * 7.922482490539551
Epoch 80, val loss: 1.713160514831543
Epoch 90, training loss: 1.652207374572754 = 1.6444956064224243 + 0.001 * 7.711735725402832
Epoch 90, val loss: 1.6540277004241943
Epoch 100, training loss: 1.5650604963302612 = 1.5574331283569336 + 0.001 * 7.627358436584473
Epoch 100, val loss: 1.581063151359558
Epoch 110, training loss: 1.467010736465454 = 1.4594699144363403 + 0.001 * 7.54082727432251
Epoch 110, val loss: 1.5036247968673706
Epoch 120, training loss: 1.3737789392471313 = 1.3664679527282715 + 0.001 * 7.311032295227051
Epoch 120, val loss: 1.4332438707351685
Epoch 130, training loss: 1.2909276485443115 = 1.2837265729904175 + 0.001 * 7.201048374176025
Epoch 130, val loss: 1.3744255304336548
Epoch 140, training loss: 1.214670181274414 = 1.207521677017212 + 0.001 * 7.148532390594482
Epoch 140, val loss: 1.323795199394226
Epoch 150, training loss: 1.1404553651809692 = 1.1333301067352295 + 0.001 * 7.125198841094971
Epoch 150, val loss: 1.2751067876815796
Epoch 160, training loss: 1.0654079914093018 = 1.0583162307739258 + 0.001 * 7.091740131378174
Epoch 160, val loss: 1.2257386445999146
Epoch 170, training loss: 0.9890016913414001 = 0.9819532632827759 + 0.001 * 7.048410892486572
Epoch 170, val loss: 1.1747585535049438
Epoch 180, training loss: 0.9135531783103943 = 0.9065505862236023 + 0.001 * 7.002583026885986
Epoch 180, val loss: 1.1235891580581665
Epoch 190, training loss: 0.8423590660095215 = 0.8353976011276245 + 0.001 * 6.961442947387695
Epoch 190, val loss: 1.0754508972167969
Epoch 200, training loss: 0.7772055864334106 = 0.7702691555023193 + 0.001 * 6.936446189880371
Epoch 200, val loss: 1.0320364236831665
Epoch 210, training loss: 0.7179150581359863 = 0.7109912633895874 + 0.001 * 6.923786163330078
Epoch 210, val loss: 0.9937078356742859
Epoch 220, training loss: 0.6630805730819702 = 0.6561625599861145 + 0.001 * 6.918035984039307
Epoch 220, val loss: 0.9591741561889648
Epoch 230, training loss: 0.6113263368606567 = 0.6044116020202637 + 0.001 * 6.914743900299072
Epoch 230, val loss: 0.9276701211929321
Epoch 240, training loss: 0.5618337392807007 = 0.5549218654632568 + 0.001 * 6.911858081817627
Epoch 240, val loss: 0.8987683057785034
Epoch 250, training loss: 0.5145668983459473 = 0.5076575875282288 + 0.001 * 6.909318923950195
Epoch 250, val loss: 0.8723289966583252
Epoch 260, training loss: 0.46960145235061646 = 0.4626946747303009 + 0.001 * 6.906780242919922
Epoch 260, val loss: 0.8491618037223816
Epoch 270, training loss: 0.4270482659339905 = 0.4201439619064331 + 0.001 * 6.904298782348633
Epoch 270, val loss: 0.8297449946403503
Epoch 280, training loss: 0.38697123527526855 = 0.38006946444511414 + 0.001 * 6.901766300201416
Epoch 280, val loss: 0.8145352005958557
Epoch 290, training loss: 0.34943199157714844 = 0.34253278374671936 + 0.001 * 6.899201393127441
Epoch 290, val loss: 0.8034328818321228
Epoch 300, training loss: 0.3145648241043091 = 0.30766820907592773 + 0.001 * 6.896621227264404
Epoch 300, val loss: 0.7966178059577942
Epoch 310, training loss: 0.28253546357154846 = 0.2756419777870178 + 0.001 * 6.89348030090332
Epoch 310, val loss: 0.7938309907913208
Epoch 320, training loss: 0.25345009565353394 = 0.24656105041503906 + 0.001 * 6.889045715332031
Epoch 320, val loss: 0.7947401404380798
Epoch 330, training loss: 0.2272648960351944 = 0.2203814834356308 + 0.001 * 6.8834052085876465
Epoch 330, val loss: 0.7991176247596741
Epoch 340, training loss: 0.20373766124248505 = 0.19686131179332733 + 0.001 * 6.876342296600342
Epoch 340, val loss: 0.806373119354248
Epoch 350, training loss: 0.1826614886522293 = 0.17579412460327148 + 0.001 * 6.867365837097168
Epoch 350, val loss: 0.8160898089408875
Epoch 360, training loss: 0.1635778844356537 = 0.1567220687866211 + 0.001 * 6.855819225311279
Epoch 360, val loss: 0.8278870582580566
Epoch 370, training loss: 0.14637361466884613 = 0.13952098786830902 + 0.001 * 6.852630138397217
Epoch 370, val loss: 0.8411834836006165
Epoch 380, training loss: 0.13115297257900238 = 0.12431944906711578 + 0.001 * 6.833527088165283
Epoch 380, val loss: 0.8556935787200928
Epoch 390, training loss: 0.1179427057504654 = 0.11112049221992493 + 0.001 * 6.822212219238281
Epoch 390, val loss: 0.8711537718772888
Epoch 400, training loss: 0.10645407438278198 = 0.09964480251073837 + 0.001 * 6.809268474578857
Epoch 400, val loss: 0.8873288035392761
Epoch 410, training loss: 0.09641866385936737 = 0.089615598320961 + 0.001 * 6.803067207336426
Epoch 410, val loss: 0.9038963317871094
Epoch 420, training loss: 0.08755230903625488 = 0.08074700832366943 + 0.001 * 6.805304050445557
Epoch 420, val loss: 0.9204556941986084
Epoch 430, training loss: 0.07960917055606842 = 0.07281162589788437 + 0.001 * 6.797545433044434
Epoch 430, val loss: 0.9371910691261292
Epoch 440, training loss: 0.07250566780567169 = 0.06570570915937424 + 0.001 * 6.799962043762207
Epoch 440, val loss: 0.9537387490272522
Epoch 450, training loss: 0.06613464653491974 = 0.059338439255952835 + 0.001 * 6.796203136444092
Epoch 450, val loss: 0.9702059030532837
Epoch 460, training loss: 0.06040022149682045 = 0.05360746383666992 + 0.001 * 6.792756080627441
Epoch 460, val loss: 0.9862563610076904
Epoch 470, training loss: 0.05519964173436165 = 0.0484074205160141 + 0.001 * 6.792222023010254
Epoch 470, val loss: 1.002117395401001
Epoch 480, training loss: 0.05051266774535179 = 0.043719492852687836 + 0.001 * 6.7931742668151855
Epoch 480, val loss: 1.0175247192382812
Epoch 490, training loss: 0.04598039388656616 = 0.039189424365758896 + 0.001 * 6.7909674644470215
Epoch 490, val loss: 1.0328205823898315
Epoch 500, training loss: 0.04146710783243179 = 0.03467387706041336 + 0.001 * 6.793232440948486
Epoch 500, val loss: 1.0479135513305664
Epoch 510, training loss: 0.03755040466785431 = 0.030756479129195213 + 0.001 * 6.793923377990723
Epoch 510, val loss: 1.0627273321151733
Epoch 520, training loss: 0.03431074321269989 = 0.027522146701812744 + 0.001 * 6.788595199584961
Epoch 520, val loss: 1.077881097793579
Epoch 530, training loss: 0.03160718083381653 = 0.024816449731588364 + 0.001 * 6.790731906890869
Epoch 530, val loss: 1.0929194688796997
Epoch 540, training loss: 0.029317226260900497 = 0.022530129179358482 + 0.001 * 6.7870965003967285
Epoch 540, val loss: 1.1074247360229492
Epoch 550, training loss: 0.02737029641866684 = 0.020580178126692772 + 0.001 * 6.7901177406311035
Epoch 550, val loss: 1.1215412616729736
Epoch 560, training loss: 0.025691015645861626 = 0.01890142448246479 + 0.001 * 6.789590835571289
Epoch 560, val loss: 1.1351618766784668
Epoch 570, training loss: 0.024240177124738693 = 0.01745670847594738 + 0.001 * 6.7834672927856445
Epoch 570, val loss: 1.1483509540557861
Epoch 580, training loss: 0.022981984540820122 = 0.016193455085158348 + 0.001 * 6.788528919219971
Epoch 580, val loss: 1.1610357761383057
Epoch 590, training loss: 0.02185911126434803 = 0.015076940879225731 + 0.001 * 6.782170295715332
Epoch 590, val loss: 1.173171043395996
Epoch 600, training loss: 0.020867876708507538 = 0.014083596877753735 + 0.001 * 6.784279823303223
Epoch 600, val loss: 1.1850858926773071
Epoch 610, training loss: 0.019975263625383377 = 0.013193365186452866 + 0.001 * 6.7818989753723145
Epoch 610, val loss: 1.1963826417922974
Epoch 620, training loss: 0.01917209103703499 = 0.012389453127980232 + 0.001 * 6.782637119293213
Epoch 620, val loss: 1.2073637247085571
Epoch 630, training loss: 0.018436703830957413 = 0.01166075374931097 + 0.001 * 6.775949954986572
Epoch 630, val loss: 1.217982530593872
Epoch 640, training loss: 0.01778385601937771 = 0.010997677221894264 + 0.001 * 6.786178112030029
Epoch 640, val loss: 1.2282047271728516
Epoch 650, training loss: 0.01716502197086811 = 0.010392309166491032 + 0.001 * 6.772712230682373
Epoch 650, val loss: 1.2381237745285034
Epoch 660, training loss: 0.016613300889730453 = 0.009838023222982883 + 0.001 * 6.775277137756348
Epoch 660, val loss: 1.2477470636367798
Epoch 670, training loss: 0.016102014109492302 = 0.009329138323664665 + 0.001 * 6.772875785827637
Epoch 670, val loss: 1.2570505142211914
Epoch 680, training loss: 0.01563384011387825 = 0.008860752917826176 + 0.001 * 6.773087024688721
Epoch 680, val loss: 1.2661000490188599
Epoch 690, training loss: 0.015201658010482788 = 0.008428625762462616 + 0.001 * 6.773031711578369
Epoch 690, val loss: 1.2748727798461914
Epoch 700, training loss: 0.014793936163187027 = 0.00802870374172926 + 0.001 * 6.765232086181641
Epoch 700, val loss: 1.2834194898605347
Epoch 710, training loss: 0.014427975751459599 = 0.0076581137254834175 + 0.001 * 6.769861698150635
Epoch 710, val loss: 1.2916984558105469
Epoch 720, training loss: 0.014081673696637154 = 0.007313929498195648 + 0.001 * 6.7677435874938965
Epoch 720, val loss: 1.2997483015060425
Epoch 730, training loss: 0.01375949289649725 = 0.00699386652559042 + 0.001 * 6.765625953674316
Epoch 730, val loss: 1.3075804710388184
Epoch 740, training loss: 0.013459832407534122 = 0.006695720367133617 + 0.001 * 6.764111518859863
Epoch 740, val loss: 1.3152014017105103
Epoch 750, training loss: 0.013180771842598915 = 0.006417537573724985 + 0.001 * 6.763233661651611
Epoch 750, val loss: 1.3226238489151
Epoch 760, training loss: 0.012919100932776928 = 0.006157590541988611 + 0.001 * 6.761509895324707
Epoch 760, val loss: 1.3298343420028687
Epoch 770, training loss: 0.01267203502357006 = 0.0059143127873539925 + 0.001 * 6.7577223777771
Epoch 770, val loss: 1.3368871212005615
Epoch 780, training loss: 0.012449708767235279 = 0.005686364136636257 + 0.001 * 6.7633442878723145
Epoch 780, val loss: 1.3437409400939941
Epoch 790, training loss: 0.012252219952642918 = 0.005472403950989246 + 0.001 * 6.779815673828125
Epoch 790, val loss: 1.3504161834716797
Epoch 800, training loss: 0.012026317417621613 = 0.005271310452371836 + 0.001 * 6.755006790161133
Epoch 800, val loss: 1.3569250106811523
Epoch 810, training loss: 0.011832785792648792 = 0.00508208479732275 + 0.001 * 6.7507004737854
Epoch 810, val loss: 1.3632795810699463
Epoch 820, training loss: 0.011675223708152771 = 0.004903856664896011 + 0.001 * 6.771366119384766
Epoch 820, val loss: 1.3694918155670166
Epoch 830, training loss: 0.011482955887913704 = 0.004735804162919521 + 0.001 * 6.747150897979736
Epoch 830, val loss: 1.375489592552185
Epoch 840, training loss: 0.011353332549333572 = 0.00457714544609189 + 0.001 * 6.776186943054199
Epoch 840, val loss: 1.3813778162002563
Epoch 850, training loss: 0.011179003864526749 = 0.004427172709256411 + 0.001 * 6.7518310546875
Epoch 850, val loss: 1.3871142864227295
Epoch 860, training loss: 0.011040665209293365 = 0.004285309463739395 + 0.001 * 6.755354881286621
Epoch 860, val loss: 1.392716646194458
Epoch 870, training loss: 0.010900592431426048 = 0.004150908440351486 + 0.001 * 6.7496843338012695
Epoch 870, val loss: 1.3981784582138062
Epoch 880, training loss: 0.01077864971011877 = 0.004023469053208828 + 0.001 * 6.755180358886719
Epoch 880, val loss: 1.4035433530807495
Epoch 890, training loss: 0.010664108209311962 = 0.0039024881552904844 + 0.001 * 6.761619567871094
Epoch 890, val loss: 1.4087589979171753
Epoch 900, training loss: 0.010540754534304142 = 0.0037875049747526646 + 0.001 * 6.753249168395996
Epoch 900, val loss: 1.4138675928115845
Epoch 910, training loss: 0.010438892990350723 = 0.003678136272355914 + 0.001 * 6.760756015777588
Epoch 910, val loss: 1.418859601020813
Epoch 920, training loss: 0.010321960784494877 = 0.0035740153398364782 + 0.001 * 6.747945308685303
Epoch 920, val loss: 1.4237408638000488
Epoch 930, training loss: 0.01022139098495245 = 0.003474789671599865 + 0.001 * 6.746601104736328
Epoch 930, val loss: 1.428512454032898
Epoch 940, training loss: 0.010120714083313942 = 0.003380101639777422 + 0.001 * 6.740611553192139
Epoch 940, val loss: 1.4331905841827393
Epoch 950, training loss: 0.010035560466349125 = 0.0032895852345973253 + 0.001 * 6.745974540710449
Epoch 950, val loss: 1.4377604722976685
Epoch 960, training loss: 0.009952335618436337 = 0.0032029272988438606 + 0.001 * 6.749407768249512
Epoch 960, val loss: 1.4422270059585571
Epoch 970, training loss: 0.0098616573959589 = 0.0031198877841234207 + 0.001 * 6.741769790649414
Epoch 970, val loss: 1.4466274976730347
Epoch 980, training loss: 0.009784089401364326 = 0.0030402406118810177 + 0.001 * 6.7438483238220215
Epoch 980, val loss: 1.4509223699569702
Epoch 990, training loss: 0.009688980877399445 = 0.0029637119732797146 + 0.001 * 6.725268840789795
Epoch 990, val loss: 1.4551656246185303
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.7085
Flip ASR: 0.6622/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.963165044784546 = 1.9547913074493408 + 0.001 * 8.373734474182129
Epoch 0, val loss: 1.9589190483093262
Epoch 10, training loss: 1.952447533607483 = 1.9440739154815674 + 0.001 * 8.373583793640137
Epoch 10, val loss: 1.9480427503585815
Epoch 20, training loss: 1.9392517805099487 = 1.9308786392211914 + 0.001 * 8.37314510345459
Epoch 20, val loss: 1.9340465068817139
Epoch 30, training loss: 1.9207152128219604 = 1.9123427867889404 + 0.001 * 8.372370719909668
Epoch 30, val loss: 1.9137752056121826
Epoch 40, training loss: 1.8935304880142212 = 1.8851596117019653 + 0.001 * 8.37092399597168
Epoch 40, val loss: 1.8839906454086304
Epoch 50, training loss: 1.8552829027175903 = 1.846915364265442 + 0.001 * 8.367497444152832
Epoch 50, val loss: 1.843455195426941
Epoch 60, training loss: 1.8091959953308105 = 1.8008400201797485 + 0.001 * 8.355916976928711
Epoch 60, val loss: 1.7977532148361206
Epoch 70, training loss: 1.7644151449203491 = 1.756120204925537 + 0.001 * 8.294947624206543
Epoch 70, val loss: 1.756042242050171
Epoch 80, training loss: 1.7146481275558472 = 1.7067291736602783 + 0.001 * 7.918895244598389
Epoch 80, val loss: 1.7102357149124146
Epoch 90, training loss: 1.6479073762893677 = 1.6402448415756226 + 0.001 * 7.6625285148620605
Epoch 90, val loss: 1.652817964553833
Epoch 100, training loss: 1.5600404739379883 = 1.5525473356246948 + 0.001 * 7.493137359619141
Epoch 100, val loss: 1.581741213798523
Epoch 110, training loss: 1.4554439783096313 = 1.448168396949768 + 0.001 * 7.275581359863281
Epoch 110, val loss: 1.4985748529434204
Epoch 120, training loss: 1.3473303318023682 = 1.3402060270309448 + 0.001 * 7.124320983886719
Epoch 120, val loss: 1.4141556024551392
Epoch 130, training loss: 1.2433832883834839 = 1.2362967729568481 + 0.001 * 7.086569309234619
Epoch 130, val loss: 1.3350063562393188
Epoch 140, training loss: 1.1444103717803955 = 1.1373463869094849 + 0.001 * 7.064043045043945
Epoch 140, val loss: 1.2600764036178589
Epoch 150, training loss: 1.0482280254364014 = 1.041179895401001 + 0.001 * 7.048072814941406
Epoch 150, val loss: 1.1870766878128052
Epoch 160, training loss: 0.9545823931694031 = 0.9475491046905518 + 0.001 * 7.033288478851318
Epoch 160, val loss: 1.1157586574554443
Epoch 170, training loss: 0.8662201762199402 = 0.859205961227417 + 0.001 * 7.014228343963623
Epoch 170, val loss: 1.0484281778335571
Epoch 180, training loss: 0.7865047454833984 = 0.7795125246047974 + 0.001 * 6.992249488830566
Epoch 180, val loss: 0.9877018928527832
Epoch 190, training loss: 0.7171238660812378 = 0.7101567983627319 + 0.001 * 6.967077732086182
Epoch 190, val loss: 0.936030387878418
Epoch 200, training loss: 0.6569409966468811 = 0.6499987244606018 + 0.001 * 6.942293643951416
Epoch 200, val loss: 0.893854022026062
Epoch 210, training loss: 0.6029524207115173 = 0.5960278511047363 + 0.001 * 6.92455530166626
Epoch 210, val loss: 0.8590529561042786
Epoch 220, training loss: 0.5518231987953186 = 0.5449014902114868 + 0.001 * 6.921689987182617
Epoch 220, val loss: 0.8287116885185242
Epoch 230, training loss: 0.5014059543609619 = 0.494489461183548 + 0.001 * 6.916471481323242
Epoch 230, val loss: 0.801424503326416
Epoch 240, training loss: 0.45133769512176514 = 0.4444218873977661 + 0.001 * 6.915822505950928
Epoch 240, val loss: 0.7764449715614319
Epoch 250, training loss: 0.40275028347969055 = 0.39583584666252136 + 0.001 * 6.914438247680664
Epoch 250, val loss: 0.7545926570892334
Epoch 260, training loss: 0.357461541891098 = 0.3505484163761139 + 0.001 * 6.913134574890137
Epoch 260, val loss: 0.7370938062667847
Epoch 270, training loss: 0.31689223647117615 = 0.3099801540374756 + 0.001 * 6.912082195281982
Epoch 270, val loss: 0.7245303392410278
Epoch 280, training loss: 0.2813602685928345 = 0.2744492292404175 + 0.001 * 6.911025047302246
Epoch 280, val loss: 0.7169226408004761
Epoch 290, training loss: 0.25015148520469666 = 0.24324172735214233 + 0.001 * 6.909760475158691
Epoch 290, val loss: 0.7136383056640625
Epoch 300, training loss: 0.2222401648759842 = 0.21533186733722687 + 0.001 * 6.908302307128906
Epoch 300, val loss: 0.7134441137313843
Epoch 310, training loss: 0.1967897117137909 = 0.18988294899463654 + 0.001 * 6.906769752502441
Epoch 310, val loss: 0.7156851291656494
Epoch 320, training loss: 0.17353494465351105 = 0.1666298806667328 + 0.001 * 6.9050679206848145
Epoch 320, val loss: 0.719839870929718
Epoch 330, training loss: 0.15264225006103516 = 0.14573918282985687 + 0.001 * 6.903059959411621
Epoch 330, val loss: 0.7258775234222412
Epoch 340, training loss: 0.1342277228832245 = 0.12732703983783722 + 0.001 * 6.900675296783447
Epoch 340, val loss: 0.7330408096313477
Epoch 350, training loss: 0.11828793585300446 = 0.11138749122619629 + 0.001 * 6.900441646575928
Epoch 350, val loss: 0.7415003776550293
Epoch 360, training loss: 0.10463017970323563 = 0.09773416072130203 + 0.001 * 6.896017551422119
Epoch 360, val loss: 0.7511149048805237
Epoch 370, training loss: 0.09286932647228241 = 0.08597830682992935 + 0.001 * 6.89102029800415
Epoch 370, val loss: 0.7613450288772583
Epoch 380, training loss: 0.08274397253990173 = 0.07585401087999344 + 0.001 * 6.889960765838623
Epoch 380, val loss: 0.7721717953681946
Epoch 390, training loss: 0.07394923269748688 = 0.0670706033706665 + 0.001 * 6.878627300262451
Epoch 390, val loss: 0.7833541631698608
Epoch 400, training loss: 0.06632018089294434 = 0.05944916605949402 + 0.001 * 6.871010780334473
Epoch 400, val loss: 0.795161247253418
Epoch 410, training loss: 0.059718623757362366 = 0.05286061018705368 + 0.001 * 6.858012676239014
Epoch 410, val loss: 0.8071425557136536
Epoch 420, training loss: 0.054084885865449905 = 0.04724115505814552 + 0.001 * 6.843729496002197
Epoch 420, val loss: 0.8198332190513611
Epoch 430, training loss: 0.049283742904663086 = 0.04243849217891693 + 0.001 * 6.845250606536865
Epoch 430, val loss: 0.8330703973770142
Epoch 440, training loss: 0.04512372985482216 = 0.03830595687031746 + 0.001 * 6.817773342132568
Epoch 440, val loss: 0.846483588218689
Epoch 450, training loss: 0.04152302443981171 = 0.03471678867936134 + 0.001 * 6.806236743927002
Epoch 450, val loss: 0.859764814376831
Epoch 460, training loss: 0.0384175106883049 = 0.031580742448568344 + 0.001 * 6.836767196655273
Epoch 460, val loss: 0.8727646470069885
Epoch 470, training loss: 0.0356379933655262 = 0.02883567474782467 + 0.001 * 6.802316665649414
Epoch 470, val loss: 0.8857171535491943
Epoch 480, training loss: 0.033201709389686584 = 0.02642195299267769 + 0.001 * 6.779757976531982
Epoch 480, val loss: 0.8983412981033325
Epoch 490, training loss: 0.0310678593814373 = 0.024289187043905258 + 0.001 * 6.77867317199707
Epoch 490, val loss: 0.9107053875923157
Epoch 500, training loss: 0.02916916087269783 = 0.022397903725504875 + 0.001 * 6.771256923675537
Epoch 500, val loss: 0.9227579832077026
Epoch 510, training loss: 0.027513861656188965 = 0.02071288228034973 + 0.001 * 6.800979137420654
Epoch 510, val loss: 0.9344990849494934
Epoch 520, training loss: 0.025978000834584236 = 0.01920069195330143 + 0.001 * 6.777308464050293
Epoch 520, val loss: 0.9460197687149048
Epoch 530, training loss: 0.024608023464679718 = 0.01784338988363743 + 0.001 * 6.7646331787109375
Epoch 530, val loss: 0.9573540687561035
Epoch 540, training loss: 0.023386575281620026 = 0.016623923555016518 + 0.001 * 6.762652397155762
Epoch 540, val loss: 0.968356192111969
Epoch 550, training loss: 0.02228229120373726 = 0.015521695837378502 + 0.001 * 6.760593891143799
Epoch 550, val loss: 0.9791030287742615
Epoch 560, training loss: 0.021282095462083817 = 0.014523855410516262 + 0.001 * 6.758238792419434
Epoch 560, val loss: 0.989598274230957
Epoch 570, training loss: 0.020372338593006134 = 0.013617578893899918 + 0.001 * 6.754758834838867
Epoch 570, val loss: 0.9998085498809814
Epoch 580, training loss: 0.019547518342733383 = 0.01279271300882101 + 0.001 * 6.754805564880371
Epoch 580, val loss: 1.0097782611846924
Epoch 590, training loss: 0.018801379948854446 = 0.01204350683838129 + 0.001 * 6.757871627807617
Epoch 590, val loss: 1.0195367336273193
Epoch 600, training loss: 0.01811741478741169 = 0.0113602876663208 + 0.001 * 6.757127285003662
Epoch 600, val loss: 1.0290255546569824
Epoch 610, training loss: 0.01748393476009369 = 0.010735037736594677 + 0.001 * 6.748897552490234
Epoch 610, val loss: 1.0382981300354004
Epoch 620, training loss: 0.016908952966332436 = 0.010161719284951687 + 0.001 * 6.7472333908081055
Epoch 620, val loss: 1.0473355054855347
Epoch 630, training loss: 0.016388021409511566 = 0.009634903632104397 + 0.001 * 6.753117084503174
Epoch 630, val loss: 1.056128740310669
Epoch 640, training loss: 0.01589512825012207 = 0.009149464778602123 + 0.001 * 6.745663166046143
Epoch 640, val loss: 1.064717411994934
Epoch 650, training loss: 0.015445331111550331 = 0.008700772188603878 + 0.001 * 6.744558811187744
Epoch 650, val loss: 1.0731319189071655
Epoch 660, training loss: 0.015028249472379684 = 0.008285150863230228 + 0.001 * 6.74309778213501
Epoch 660, val loss: 1.081333875656128
Epoch 670, training loss: 0.014646314084529877 = 0.007900174707174301 + 0.001 * 6.746139049530029
Epoch 670, val loss: 1.0893754959106445
Epoch 680, training loss: 0.014288172125816345 = 0.007542903535068035 + 0.001 * 6.745267868041992
Epoch 680, val loss: 1.0972193479537964
Epoch 690, training loss: 0.01395426131784916 = 0.0072107212617993355 + 0.001 * 6.743539333343506
Epoch 690, val loss: 1.104841709136963
Epoch 700, training loss: 0.013646863400936127 = 0.0069010816514492035 + 0.001 * 6.745781898498535
Epoch 700, val loss: 1.1123249530792236
Epoch 710, training loss: 0.01335500180721283 = 0.006612267345190048 + 0.001 * 6.742734432220459
Epoch 710, val loss: 1.1196621656417847
Epoch 720, training loss: 0.013083365745842457 = 0.0063424669206142426 + 0.001 * 6.740898609161377
Epoch 720, val loss: 1.1268175840377808
Epoch 730, training loss: 0.012835701927542686 = 0.00609010411426425 + 0.001 * 6.745596885681152
Epoch 730, val loss: 1.1338173151016235
Epoch 740, training loss: 0.012589629739522934 = 0.005853915587067604 + 0.001 * 6.735713481903076
Epoch 740, val loss: 1.140692949295044
Epoch 750, training loss: 0.012370610609650612 = 0.005632488057017326 + 0.001 * 6.73812198638916
Epoch 750, val loss: 1.1474190950393677
Epoch 760, training loss: 0.012160023674368858 = 0.005424560979008675 + 0.001 * 6.735461711883545
Epoch 760, val loss: 1.1540207862854004
Epoch 770, training loss: 0.011968998238444328 = 0.00522908940911293 + 0.001 * 6.739908695220947
Epoch 770, val loss: 1.1605113744735718
Epoch 780, training loss: 0.011774111539125443 = 0.005045192316174507 + 0.001 * 6.728918552398682
Epoch 780, val loss: 1.1668554544448853
Epoch 790, training loss: 0.011605962179601192 = 0.004871840588748455 + 0.001 * 6.734121322631836
Epoch 790, val loss: 1.1730653047561646
Epoch 800, training loss: 0.011437296867370605 = 0.004708294756710529 + 0.001 * 6.729001998901367
Epoch 800, val loss: 1.1791772842407227
Epoch 810, training loss: 0.011280548758804798 = 0.004553806968033314 + 0.001 * 6.726741313934326
Epoch 810, val loss: 1.185141921043396
Epoch 820, training loss: 0.011133937165141106 = 0.004407697822898626 + 0.001 * 6.726239204406738
Epoch 820, val loss: 1.191015601158142
Epoch 830, training loss: 0.010989650152623653 = 0.004269368480890989 + 0.001 * 6.72028112411499
Epoch 830, val loss: 1.1967859268188477
Epoch 840, training loss: 0.010874776169657707 = 0.0041382890194654465 + 0.001 * 6.736486434936523
Epoch 840, val loss: 1.2024677991867065
Epoch 850, training loss: 0.010743893682956696 = 0.004013887140899897 + 0.001 * 6.730005741119385
Epoch 850, val loss: 1.2080352306365967
Epoch 860, training loss: 0.010631026700139046 = 0.0038958084769546986 + 0.001 * 6.735217571258545
Epoch 860, val loss: 1.2135010957717896
Epoch 870, training loss: 0.010505911894142628 = 0.0037835752591490746 + 0.001 * 6.722336292266846
Epoch 870, val loss: 1.218867540359497
Epoch 880, training loss: 0.010394132696092129 = 0.0036768035497516394 + 0.001 * 6.717329025268555
Epoch 880, val loss: 1.2241568565368652
Epoch 890, training loss: 0.01030701957643032 = 0.003575162496417761 + 0.001 * 6.731856346130371
Epoch 890, val loss: 1.2293516397476196
Epoch 900, training loss: 0.010198468342423439 = 0.0034783161245286465 + 0.001 * 6.720151424407959
Epoch 900, val loss: 1.234459638595581
Epoch 910, training loss: 0.010104082524776459 = 0.0033859761897474527 + 0.001 * 6.718105792999268
Epoch 910, val loss: 1.2394909858703613
Epoch 920, training loss: 0.010012215003371239 = 0.00329784257337451 + 0.001 * 6.714372158050537
Epoch 920, val loss: 1.2444580793380737
Epoch 930, training loss: 0.009921404533088207 = 0.0032136777881532907 + 0.001 * 6.707726001739502
Epoch 930, val loss: 1.2493159770965576
Epoch 940, training loss: 0.009856256656348705 = 0.003133240155875683 + 0.001 * 6.723016262054443
Epoch 940, val loss: 1.2541042566299438
Epoch 950, training loss: 0.009772831574082375 = 0.003056304296478629 + 0.001 * 6.716526985168457
Epoch 950, val loss: 1.258855938911438
Epoch 960, training loss: 0.00969751551747322 = 0.0029826792888343334 + 0.001 * 6.7148356437683105
Epoch 960, val loss: 1.2635029554367065
Epoch 970, training loss: 0.009629834443330765 = 0.002912190044298768 + 0.001 * 6.717643737792969
Epoch 970, val loss: 1.2680902481079102
Epoch 980, training loss: 0.00956203043460846 = 0.0028446444775909185 + 0.001 * 6.717385292053223
Epoch 980, val loss: 1.2725763320922852
Epoch 990, training loss: 0.009481911547482014 = 0.002779864240437746 + 0.001 * 6.702046871185303
Epoch 990, val loss: 1.2770311832427979
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6162
Flip ASR: 0.5644/225 nodes
The final ASR:0.58180, 0.12000, Accuracy:0.79877, 0.00873
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11614])
remove edge: torch.Size([2, 9478])
updated graph: torch.Size([2, 10536])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.97909, 0.00696, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9330801963806152 = 1.9247063398361206 + 0.001 * 8.3738374710083
Epoch 0, val loss: 1.9177422523498535
Epoch 10, training loss: 1.9241892099380493 = 1.9158154726028442 + 0.001 * 8.373791694641113
Epoch 10, val loss: 1.9093189239501953
Epoch 20, training loss: 1.9133970737457275 = 1.905023455619812 + 0.001 * 8.373597145080566
Epoch 20, val loss: 1.899246096611023
Epoch 30, training loss: 1.8981388807296753 = 1.8897656202316284 + 0.001 * 8.373201370239258
Epoch 30, val loss: 1.8852076530456543
Epoch 40, training loss: 1.8753671646118164 = 1.8669947385787964 + 0.001 * 8.372400283813477
Epoch 40, val loss: 1.864753007888794
Epoch 50, training loss: 1.8431047201156616 = 1.834734320640564 + 0.001 * 8.37043571472168
Epoch 50, val loss: 1.837370753288269
Epoch 60, training loss: 1.8042277097702026 = 1.7958637475967407 + 0.001 * 8.363936424255371
Epoch 60, val loss: 1.8074941635131836
Epoch 70, training loss: 1.7630723714828491 = 1.754743218421936 + 0.001 * 8.329192161560059
Epoch 70, val loss: 1.7760238647460938
Epoch 80, training loss: 1.708892822265625 = 1.7008274793624878 + 0.001 * 8.065327644348145
Epoch 80, val loss: 1.7281177043914795
Epoch 90, training loss: 1.6339339017868042 = 1.6262142658233643 + 0.001 * 7.719694137573242
Epoch 90, val loss: 1.6619211435317993
Epoch 100, training loss: 1.5370460748672485 = 1.5295343399047852 + 0.001 * 7.511739730834961
Epoch 100, val loss: 1.5809195041656494
Epoch 110, training loss: 1.4259124994277954 = 1.418562412261963 + 0.001 * 7.350061416625977
Epoch 110, val loss: 1.4896034002304077
Epoch 120, training loss: 1.3100762367248535 = 1.3027772903442383 + 0.001 * 7.298894882202148
Epoch 120, val loss: 1.3959940671920776
Epoch 130, training loss: 1.1963988542556763 = 1.1891807317733765 + 0.001 * 7.2180867195129395
Epoch 130, val loss: 1.3049321174621582
Epoch 140, training loss: 1.0910859107971191 = 1.0839205980300903 + 0.001 * 7.165314197540283
Epoch 140, val loss: 1.2227598428726196
Epoch 150, training loss: 0.9977176785469055 = 0.990569531917572 + 0.001 * 7.148136138916016
Epoch 150, val loss: 1.1516762971878052
Epoch 160, training loss: 0.9152660369873047 = 0.9081300497055054 + 0.001 * 7.1359639167785645
Epoch 160, val loss: 1.0900778770446777
Epoch 170, training loss: 0.8405317068099976 = 0.8334166407585144 + 0.001 * 7.115062713623047
Epoch 170, val loss: 1.0348173379898071
Epoch 180, training loss: 0.7712207436561584 = 0.7641329765319824 + 0.001 * 7.08779239654541
Epoch 180, val loss: 0.9843606352806091
Epoch 190, training loss: 0.7066256403923035 = 0.6995670795440674 + 0.001 * 7.058570861816406
Epoch 190, val loss: 0.9381871819496155
Epoch 200, training loss: 0.6463811993598938 = 0.6393457055091858 + 0.001 * 7.0354695320129395
Epoch 200, val loss: 0.8970996141433716
Epoch 210, training loss: 0.5899255871772766 = 0.5828970074653625 + 0.001 * 7.028558731079102
Epoch 210, val loss: 0.86116623878479
Epoch 220, training loss: 0.5363652110099792 = 0.5293432474136353 + 0.001 * 7.021946430206299
Epoch 220, val loss: 0.8291897177696228
Epoch 230, training loss: 0.4854303002357483 = 0.4784102737903595 + 0.001 * 7.02001953125
Epoch 230, val loss: 0.8012399077415466
Epoch 240, training loss: 0.4371528923511505 = 0.4301356077194214 + 0.001 * 7.017279624938965
Epoch 240, val loss: 0.776901364326477
Epoch 250, training loss: 0.39140820503234863 = 0.3843933939933777 + 0.001 * 7.014796733856201
Epoch 250, val loss: 0.7563239336013794
Epoch 260, training loss: 0.348063200712204 = 0.34105098247528076 + 0.001 * 7.012231826782227
Epoch 260, val loss: 0.7397041320800781
Epoch 270, training loss: 0.30713963508605957 = 0.3001299798488617 + 0.001 * 7.009659290313721
Epoch 270, val loss: 0.7269603610038757
Epoch 280, training loss: 0.2690035402774811 = 0.2619965970516205 + 0.001 * 7.006943225860596
Epoch 280, val loss: 0.7181118726730347
Epoch 290, training loss: 0.23430362343788147 = 0.2272997498512268 + 0.001 * 7.003865718841553
Epoch 290, val loss: 0.7133094668388367
Epoch 300, training loss: 0.20363733172416687 = 0.19663727283477783 + 0.001 * 7.0000529289245605
Epoch 300, val loss: 0.7125005125999451
Epoch 310, training loss: 0.17725178599357605 = 0.17025688290596008 + 0.001 * 6.994898319244385
Epoch 310, val loss: 0.7154655456542969
Epoch 320, training loss: 0.15494655072689056 = 0.1479591429233551 + 0.001 * 6.987402439117432
Epoch 320, val loss: 0.7217756509780884
Epoch 330, training loss: 0.13623708486557007 = 0.12925130128860474 + 0.001 * 6.985781192779541
Epoch 330, val loss: 0.7308667898178101
Epoch 340, training loss: 0.12050915509462357 = 0.1135367751121521 + 0.001 * 6.9723801612854
Epoch 340, val loss: 0.7421149611473083
Epoch 350, training loss: 0.1072130799293518 = 0.10025530308485031 + 0.001 * 6.957776069641113
Epoch 350, val loss: 0.7550174593925476
Epoch 360, training loss: 0.09588757902383804 = 0.0889417752623558 + 0.001 * 6.94580602645874
Epoch 360, val loss: 0.7691453695297241
Epoch 370, training loss: 0.08616060763597488 = 0.07922680675983429 + 0.001 * 6.933799743652344
Epoch 370, val loss: 0.7841537594795227
Epoch 380, training loss: 0.07774747908115387 = 0.07082327455282211 + 0.001 * 6.924204349517822
Epoch 380, val loss: 0.7998358011245728
Epoch 390, training loss: 0.07043401896953583 = 0.06351510435342789 + 0.001 * 6.9189133644104
Epoch 390, val loss: 0.8160008788108826
Epoch 400, training loss: 0.0640474259853363 = 0.05712686479091644 + 0.001 * 6.920558929443359
Epoch 400, val loss: 0.8325662016868591
Epoch 410, training loss: 0.05843485891819 = 0.05152305215597153 + 0.001 * 6.911806106567383
Epoch 410, val loss: 0.8493638038635254
Epoch 420, training loss: 0.053503960371017456 = 0.04659393057227135 + 0.001 * 6.91002893447876
Epoch 420, val loss: 0.8662316799163818
Epoch 430, training loss: 0.04915783181786537 = 0.04224936664104462 + 0.001 * 6.908466339111328
Epoch 430, val loss: 0.8830792903900146
Epoch 440, training loss: 0.0453205369412899 = 0.0384158231317997 + 0.001 * 6.904715061187744
Epoch 440, val loss: 0.8997635245323181
Epoch 450, training loss: 0.04192692041397095 = 0.03502394258975983 + 0.001 * 6.9029765129089355
Epoch 450, val loss: 0.9161958694458008
Epoch 460, training loss: 0.038935329765081406 = 0.032018132507801056 + 0.001 * 6.917198181152344
Epoch 460, val loss: 0.9322952032089233
Epoch 470, training loss: 0.036252252757549286 = 0.0293487049639225 + 0.001 * 6.90354585647583
Epoch 470, val loss: 0.9480639696121216
Epoch 480, training loss: 0.03387220576405525 = 0.02697235904633999 + 0.001 * 6.899846076965332
Epoch 480, val loss: 0.9634381532669067
Epoch 490, training loss: 0.03174664080142975 = 0.024851473048329353 + 0.001 * 6.895169258117676
Epoch 490, val loss: 0.9784248471260071
Epoch 500, training loss: 0.029849261045455933 = 0.02295554243028164 + 0.001 * 6.893718719482422
Epoch 500, val loss: 0.9929606318473816
Epoch 510, training loss: 0.02814798802137375 = 0.02125599794089794 + 0.001 * 6.8919901847839355
Epoch 510, val loss: 1.0070948600769043
Epoch 520, training loss: 0.02662096545100212 = 0.01972789503633976 + 0.001 * 6.893069744110107
Epoch 520, val loss: 1.0208699703216553
Epoch 530, training loss: 0.025235814973711967 = 0.018346425145864487 + 0.001 * 6.889389514923096
Epoch 530, val loss: 1.0343656539916992
Epoch 540, training loss: 0.023980462923645973 = 0.01708950661122799 + 0.001 * 6.890955924987793
Epoch 540, val loss: 1.0475791692733765
Epoch 550, training loss: 0.02283114194869995 = 0.015941990539431572 + 0.001 * 6.889150142669678
Epoch 550, val loss: 1.0604838132858276
Epoch 560, training loss: 0.02179129421710968 = 0.01489383913576603 + 0.001 * 6.897454738616943
Epoch 560, val loss: 1.0730727910995483
Epoch 570, training loss: 0.02081359550356865 = 0.013936414383351803 + 0.001 * 6.877181053161621
Epoch 570, val loss: 1.0853482484817505
Epoch 580, training loss: 0.019941197708249092 = 0.013061649166047573 + 0.001 * 6.8795485496521
Epoch 580, val loss: 1.097326397895813
Epoch 590, training loss: 0.019133243709802628 = 0.012261928990483284 + 0.001 * 6.871314525604248
Epoch 590, val loss: 1.1090067625045776
Epoch 600, training loss: 0.018402153626084328 = 0.011530467309057713 + 0.001 * 6.8716864585876465
Epoch 600, val loss: 1.1203699111938477
Epoch 610, training loss: 0.01774323172867298 = 0.010860772803425789 + 0.001 * 6.8824591636657715
Epoch 610, val loss: 1.1314196586608887
Epoch 620, training loss: 0.0171198733150959 = 0.010246748104691505 + 0.001 * 6.873125076293945
Epoch 620, val loss: 1.1421900987625122
Epoch 630, training loss: 0.01655089668929577 = 0.009682946838438511 + 0.001 * 6.867949962615967
Epoch 630, val loss: 1.1526658535003662
Epoch 640, training loss: 0.016030550003051758 = 0.009164519608020782 + 0.001 * 6.866030693054199
Epoch 640, val loss: 1.1628543138504028
Epoch 650, training loss: 0.015561877749860287 = 0.008687060326337814 + 0.001 * 6.87481689453125
Epoch 650, val loss: 1.17275869846344
Epoch 660, training loss: 0.015105914324522018 = 0.00824662484228611 + 0.001 * 6.859288692474365
Epoch 660, val loss: 1.1824179887771606
Epoch 670, training loss: 0.014697637408971786 = 0.007839607074856758 + 0.001 * 6.858029842376709
Epoch 670, val loss: 1.191825270652771
Epoch 680, training loss: 0.014320461079478264 = 0.007462882436811924 + 0.001 * 6.857577800750732
Epoch 680, val loss: 1.2009696960449219
Epoch 690, training loss: 0.01398082822561264 = 0.007113627623766661 + 0.001 * 6.86720085144043
Epoch 690, val loss: 1.209901213645935
Epoch 700, training loss: 0.013649647124111652 = 0.006789323873817921 + 0.001 * 6.860322952270508
Epoch 700, val loss: 1.2185888290405273
Epoch 710, training loss: 0.013346755877137184 = 0.006487634964287281 + 0.001 * 6.859119892120361
Epoch 710, val loss: 1.2270629405975342
Epoch 720, training loss: 0.013067786581814289 = 0.006206406280398369 + 0.001 * 6.861380100250244
Epoch 720, val loss: 1.2353368997573853
Epoch 730, training loss: 0.01280137337744236 = 0.005943272262811661 + 0.001 * 6.8581013679504395
Epoch 730, val loss: 1.2433981895446777
Epoch 740, training loss: 0.012545816600322723 = 0.005695600062608719 + 0.001 * 6.850216865539551
Epoch 740, val loss: 1.251359224319458
Epoch 750, training loss: 0.012316263280808926 = 0.005461099091917276 + 0.001 * 6.855164051055908
Epoch 750, val loss: 1.2592062950134277
Epoch 760, training loss: 0.012092557735741138 = 0.005238378420472145 + 0.001 * 6.8541789054870605
Epoch 760, val loss: 1.2670003175735474
Epoch 770, training loss: 0.01187426783144474 = 0.005026639439165592 + 0.001 * 6.847628116607666
Epoch 770, val loss: 1.2747160196304321
Epoch 780, training loss: 0.011681491509079933 = 0.00482549425214529 + 0.001 * 6.855997562408447
Epoch 780, val loss: 1.2823950052261353
Epoch 790, training loss: 0.011484572663903236 = 0.004634711891412735 + 0.001 * 6.849860668182373
Epoch 790, val loss: 1.290007472038269
Epoch 800, training loss: 0.01131034642457962 = 0.0044540041126310825 + 0.001 * 6.85634183883667
Epoch 800, val loss: 1.2975751161575317
Epoch 810, training loss: 0.011126718483865261 = 0.004283026792109013 + 0.001 * 6.843691349029541
Epoch 810, val loss: 1.30504310131073
Epoch 820, training loss: 0.010961898602545261 = 0.004121394362300634 + 0.001 * 6.840503692626953
Epoch 820, val loss: 1.3124295473098755
Epoch 830, training loss: 0.01080853771418333 = 0.0039686281234025955 + 0.001 * 6.839909076690674
Epoch 830, val loss: 1.319714903831482
Epoch 840, training loss: 0.01068010926246643 = 0.0038242528680711985 + 0.001 * 6.855855941772461
Epoch 840, val loss: 1.326869249343872
Epoch 850, training loss: 0.010535724461078644 = 0.0036878520622849464 + 0.001 * 6.847871780395508
Epoch 850, val loss: 1.3339403867721558
Epoch 860, training loss: 0.010399339720606804 = 0.003558928379788995 + 0.001 * 6.8404107093811035
Epoch 860, val loss: 1.3408936262130737
Epoch 870, training loss: 0.010291066952049732 = 0.003437059698626399 + 0.001 * 6.854006767272949
Epoch 870, val loss: 1.3477650880813599
Epoch 880, training loss: 0.010163857601583004 = 0.003321768017485738 + 0.001 * 6.842089653015137
Epoch 880, val loss: 1.3544572591781616
Epoch 890, training loss: 0.010052148252725601 = 0.0032126798760145903 + 0.001 * 6.839468479156494
Epoch 890, val loss: 1.3610941171646118
Epoch 900, training loss: 0.009962327778339386 = 0.0031093459110707045 + 0.001 * 6.852981090545654
Epoch 900, val loss: 1.3675751686096191
Epoch 910, training loss: 0.009844847954809666 = 0.0030114438850432634 + 0.001 * 6.833403587341309
Epoch 910, val loss: 1.3739722967147827
Epoch 920, training loss: 0.009751696139574051 = 0.002918586600571871 + 0.001 * 6.8331098556518555
Epoch 920, val loss: 1.3802435398101807
Epoch 930, training loss: 0.00966881588101387 = 0.0028304820880293846 + 0.001 * 6.8383331298828125
Epoch 930, val loss: 1.3864141702651978
Epoch 940, training loss: 0.00957690179347992 = 0.0027468609623610973 + 0.001 * 6.83004093170166
Epoch 940, val loss: 1.3924771547317505
Epoch 950, training loss: 0.009492187760770321 = 0.0026674114633351564 + 0.001 * 6.824775695800781
Epoch 950, val loss: 1.3984249830245972
Epoch 960, training loss: 0.009419916197657585 = 0.002591893309727311 + 0.001 * 6.8280229568481445
Epoch 960, val loss: 1.4042681455612183
Epoch 970, training loss: 0.009348353371024132 = 0.002520073438063264 + 0.001 * 6.828279972076416
Epoch 970, val loss: 1.4100117683410645
Epoch 980, training loss: 0.00927850790321827 = 0.00245166989043355 + 0.001 * 6.826837539672852
Epoch 980, val loss: 1.415634036064148
Epoch 990, training loss: 0.009213118813931942 = 0.002386520616710186 + 0.001 * 6.826597690582275
Epoch 990, val loss: 1.4211658239364624
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.5203
Flip ASR: 0.4356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9576001167297363 = 1.9492263793945312 + 0.001 * 8.373777389526367
Epoch 0, val loss: 1.9485692977905273
Epoch 10, training loss: 1.9470287561416626 = 1.938655138015747 + 0.001 * 8.373672485351562
Epoch 10, val loss: 1.9372329711914062
Epoch 20, training loss: 1.934546947479248 = 1.9261735677719116 + 0.001 * 8.373357772827148
Epoch 20, val loss: 1.9236897230148315
Epoch 30, training loss: 1.9174424409866333 = 1.9090696573257446 + 0.001 * 8.372730255126953
Epoch 30, val loss: 1.9051283597946167
Epoch 40, training loss: 1.8923898935317993 = 1.8840184211730957 + 0.001 * 8.371458053588867
Epoch 40, val loss: 1.8784767389297485
Epoch 50, training loss: 1.8570395708084106 = 1.848671317100525 + 0.001 * 8.368224143981934
Epoch 50, val loss: 1.8425266742706299
Epoch 60, training loss: 1.8145281076431274 = 1.8061721324920654 + 0.001 * 8.355972290039062
Epoch 60, val loss: 1.8029385805130005
Epoch 70, training loss: 1.773114562034607 = 1.7648335695266724 + 0.001 * 8.281023025512695
Epoch 70, val loss: 1.7676396369934082
Epoch 80, training loss: 1.7249319553375244 = 1.7169901132583618 + 0.001 * 7.941880226135254
Epoch 80, val loss: 1.7254760265350342
Epoch 90, training loss: 1.6590393781661987 = 1.6514208316802979 + 0.001 * 7.618588924407959
Epoch 90, val loss: 1.6677525043487549
Epoch 100, training loss: 1.572643518447876 = 1.5653260946273804 + 0.001 * 7.317481994628906
Epoch 100, val loss: 1.5932084321975708
Epoch 110, training loss: 1.4702935218811035 = 1.4631136655807495 + 0.001 * 7.179854869842529
Epoch 110, val loss: 1.5073095560073853
Epoch 120, training loss: 1.360795497894287 = 1.3536933660507202 + 0.001 * 7.102176666259766
Epoch 120, val loss: 1.4164984226226807
Epoch 130, training loss: 1.2491778135299683 = 1.2421494722366333 + 0.001 * 7.02833890914917
Epoch 130, val loss: 1.326758861541748
Epoch 140, training loss: 1.1365348100662231 = 1.1295334100723267 + 0.001 * 7.001381874084473
Epoch 140, val loss: 1.2380679845809937
Epoch 150, training loss: 1.0252881050109863 = 1.0182956457138062 + 0.001 * 6.9924187660217285
Epoch 150, val loss: 1.1521754264831543
Epoch 160, training loss: 0.9199455976486206 = 0.9129592180252075 + 0.001 * 6.986359596252441
Epoch 160, val loss: 1.0731207132339478
Epoch 170, training loss: 0.8243589997291565 = 0.8173775672912598 + 0.001 * 6.9814066886901855
Epoch 170, val loss: 1.0037823915481567
Epoch 180, training loss: 0.74056476354599 = 0.7335876226425171 + 0.001 * 6.977153778076172
Epoch 180, val loss: 0.9457915425300598
Epoch 190, training loss: 0.6693288087844849 = 0.6623549461364746 + 0.001 * 6.973841667175293
Epoch 190, val loss: 0.9001217484474182
Epoch 200, training loss: 0.6102375388145447 = 0.603266716003418 + 0.001 * 6.970818042755127
Epoch 200, val loss: 0.8661060333251953
Epoch 210, training loss: 0.5617145299911499 = 0.5547469854354858 + 0.001 * 6.967535018920898
Epoch 210, val loss: 0.8418459892272949
Epoch 220, training loss: 0.521468997001648 = 0.5145053267478943 + 0.001 * 6.963665962219238
Epoch 220, val loss: 0.8249382972717285
Epoch 230, training loss: 0.487230509519577 = 0.4802716076374054 + 0.001 * 6.958911895751953
Epoch 230, val loss: 0.8130934834480286
Epoch 240, training loss: 0.4569658637046814 = 0.45001301169395447 + 0.001 * 6.9528398513793945
Epoch 240, val loss: 0.8046837449073792
Epoch 250, training loss: 0.428992360830307 = 0.42204755544662476 + 0.001 * 6.9448041915893555
Epoch 250, val loss: 0.7985360622406006
Epoch 260, training loss: 0.40206852555274963 = 0.39513328671455383 + 0.001 * 6.935230255126953
Epoch 260, val loss: 0.7939548492431641
Epoch 270, training loss: 0.37540021538734436 = 0.3684748411178589 + 0.001 * 6.92537784576416
Epoch 270, val loss: 0.7903785705566406
Epoch 280, training loss: 0.3487842082977295 = 0.34187451004981995 + 0.001 * 6.909700870513916
Epoch 280, val loss: 0.788066565990448
Epoch 290, training loss: 0.3225858509540558 = 0.3156927824020386 + 0.001 * 6.893062114715576
Epoch 290, val loss: 0.787397027015686
Epoch 300, training loss: 0.29725268483161926 = 0.29037705063819885 + 0.001 * 6.8756422996521
Epoch 300, val loss: 0.788709819316864
Epoch 310, training loss: 0.2729381322860718 = 0.26608601212501526 + 0.001 * 6.852119445800781
Epoch 310, val loss: 0.7917224168777466
Epoch 320, training loss: 0.24968133866786957 = 0.2428312748670578 + 0.001 * 6.850067138671875
Epoch 320, val loss: 0.7961528301239014
Epoch 330, training loss: 0.22748447954654694 = 0.2206570953130722 + 0.001 * 6.827388286590576
Epoch 330, val loss: 0.8018249869346619
Epoch 340, training loss: 0.2064819484949112 = 0.19966626167297363 + 0.001 * 6.815683841705322
Epoch 340, val loss: 0.8087051510810852
Epoch 350, training loss: 0.18679776787757874 = 0.179987370967865 + 0.001 * 6.810389995574951
Epoch 350, val loss: 0.8166199922561646
Epoch 360, training loss: 0.16861706972122192 = 0.1618087887763977 + 0.001 * 6.808278560638428
Epoch 360, val loss: 0.8257021307945251
Epoch 370, training loss: 0.15191306173801422 = 0.1451101154088974 + 0.001 * 6.802952289581299
Epoch 370, val loss: 0.8360143303871155
Epoch 380, training loss: 0.13680560886859894 = 0.12999974191188812 + 0.001 * 6.805868625640869
Epoch 380, val loss: 0.8473421335220337
Epoch 390, training loss: 0.12320397049188614 = 0.11639919131994247 + 0.001 * 6.804776668548584
Epoch 390, val loss: 0.8598416447639465
Epoch 400, training loss: 0.11093492060899734 = 0.10413335263729095 + 0.001 * 6.801565647125244
Epoch 400, val loss: 0.8731880187988281
Epoch 410, training loss: 0.09978364408016205 = 0.09298155456781387 + 0.001 * 6.802091598510742
Epoch 410, val loss: 0.8872067928314209
Epoch 420, training loss: 0.0895066112279892 = 0.08270207792520523 + 0.001 * 6.804533958435059
Epoch 420, val loss: 0.9014564752578735
Epoch 430, training loss: 0.08004642277956009 = 0.07324235886335373 + 0.001 * 6.804062843322754
Epoch 430, val loss: 0.915734052658081
Epoch 440, training loss: 0.07127449661493301 = 0.0644710436463356 + 0.001 * 6.803452491760254
Epoch 440, val loss: 0.9304733872413635
Epoch 450, training loss: 0.06324359774589539 = 0.05643916130065918 + 0.001 * 6.8044328689575195
Epoch 450, val loss: 0.945125162601471
Epoch 460, training loss: 0.056243497878313065 = 0.049440283328294754 + 0.001 * 6.803212642669678
Epoch 460, val loss: 0.9599196910858154
Epoch 470, training loss: 0.05047963187098503 = 0.0436607226729393 + 0.001 * 6.818907260894775
Epoch 470, val loss: 0.9750690460205078
Epoch 480, training loss: 0.04565342888236046 = 0.03884933143854141 + 0.001 * 6.80409574508667
Epoch 480, val loss: 0.9910298585891724
Epoch 490, training loss: 0.04151277616620064 = 0.034706972539424896 + 0.001 * 6.805803298950195
Epoch 490, val loss: 1.0069295167922974
Epoch 500, training loss: 0.03793558478355408 = 0.031131817027926445 + 0.001 * 6.803769111633301
Epoch 500, val loss: 1.0224303007125854
Epoch 510, training loss: 0.03485386446118355 = 0.028047189116477966 + 0.001 * 6.806675434112549
Epoch 510, val loss: 1.0376152992248535
Epoch 520, training loss: 0.03219018131494522 = 0.02538207918405533 + 0.001 * 6.808102607727051
Epoch 520, val loss: 1.0524531602859497
Epoch 530, training loss: 0.029872387647628784 = 0.023068133741617203 + 0.001 * 6.804254531860352
Epoch 530, val loss: 1.0667072534561157
Epoch 540, training loss: 0.027857769280672073 = 0.021049294620752335 + 0.001 * 6.808474540710449
Epoch 540, val loss: 1.0805039405822754
Epoch 550, training loss: 0.02608543448150158 = 0.019280560314655304 + 0.001 * 6.804873943328857
Epoch 550, val loss: 1.0938254594802856
Epoch 560, training loss: 0.024521926417946815 = 0.017720624804496765 + 0.001 * 6.801301956176758
Epoch 560, val loss: 1.1066597700119019
Epoch 570, training loss: 0.023140667006373405 = 0.016340861096978188 + 0.001 * 6.799805641174316
Epoch 570, val loss: 1.1189334392547607
Epoch 580, training loss: 0.021918971091508865 = 0.015116571448743343 + 0.001 * 6.8024001121521
Epoch 580, val loss: 1.1308845281600952
Epoch 590, training loss: 0.020830538123846054 = 0.014026871882379055 + 0.001 * 6.803667068481445
Epoch 590, val loss: 1.1424225568771362
Epoch 600, training loss: 0.019854500889778137 = 0.013053233735263348 + 0.001 * 6.801267623901367
Epoch 600, val loss: 1.1535608768463135
Epoch 610, training loss: 0.01897992379963398 = 0.012180195190012455 + 0.001 * 6.799728870391846
Epoch 610, val loss: 1.1643201112747192
Epoch 620, training loss: 0.01819392293691635 = 0.011394867673516273 + 0.001 * 6.799056053161621
Epoch 620, val loss: 1.1746852397918701
Epoch 630, training loss: 0.017484396696090698 = 0.010685065761208534 + 0.001 * 6.799330234527588
Epoch 630, val loss: 1.1847105026245117
Epoch 640, training loss: 0.016841363161802292 = 0.010042029432952404 + 0.001 * 6.799334526062012
Epoch 640, val loss: 1.19440758228302
Epoch 650, training loss: 0.01625194028019905 = 0.009457976557314396 + 0.001 * 6.793963432312012
Epoch 650, val loss: 1.203791856765747
Epoch 660, training loss: 0.015720628201961517 = 0.008926081471145153 + 0.001 * 6.7945475578308105
Epoch 660, val loss: 1.2128962278366089
Epoch 670, training loss: 0.015232231467962265 = 0.008439925499260426 + 0.001 * 6.792305946350098
Epoch 670, val loss: 1.2216628789901733
Epoch 680, training loss: 0.01479111984372139 = 0.007994317449629307 + 0.001 * 6.796802520751953
Epoch 680, val loss: 1.2302111387252808
Epoch 690, training loss: 0.014383315108716488 = 0.007585316896438599 + 0.001 * 6.797997951507568
Epoch 690, val loss: 1.238508701324463
Epoch 700, training loss: 0.01400216855108738 = 0.007209228817373514 + 0.001 * 6.79293966293335
Epoch 700, val loss: 1.2464898824691772
Epoch 710, training loss: 0.013650653883814812 = 0.006862876936793327 + 0.001 * 6.787776947021484
Epoch 710, val loss: 1.2542821168899536
Epoch 720, training loss: 0.013336168602108955 = 0.006542959716171026 + 0.001 * 6.793208122253418
Epoch 720, val loss: 1.2618327140808105
Epoch 730, training loss: 0.01303446851670742 = 0.006246753502637148 + 0.001 * 6.787714004516602
Epoch 730, val loss: 1.2691453695297241
Epoch 740, training loss: 0.012755687348544598 = 0.00597187178209424 + 0.001 * 6.783815383911133
Epoch 740, val loss: 1.276285171508789
Epoch 750, training loss: 0.012506325729191303 = 0.00571637274697423 + 0.001 * 6.789952754974365
Epoch 750, val loss: 1.2831958532333374
Epoch 760, training loss: 0.012269863858819008 = 0.005478448234498501 + 0.001 * 6.791414737701416
Epoch 760, val loss: 1.2899020910263062
Epoch 770, training loss: 0.012041125446557999 = 0.005256579257547855 + 0.001 * 6.7845458984375
Epoch 770, val loss: 1.2964508533477783
Epoch 780, training loss: 0.011847356334328651 = 0.0050493162125349045 + 0.001 * 6.79803991317749
Epoch 780, val loss: 1.3027926683425903
Epoch 790, training loss: 0.011632677167654037 = 0.004855383653193712 + 0.001 * 6.777293682098389
Epoch 790, val loss: 1.3089802265167236
Epoch 800, training loss: 0.011452395468950272 = 0.004673532210290432 + 0.001 * 6.778862953186035
Epoch 800, val loss: 1.3150471448898315
Epoch 810, training loss: 0.011293888092041016 = 0.004502825438976288 + 0.001 * 6.791062355041504
Epoch 810, val loss: 1.3209306001663208
Epoch 820, training loss: 0.01112301368266344 = 0.0043424502946436405 + 0.001 * 6.780562877655029
Epoch 820, val loss: 1.3266133069992065
Epoch 830, training loss: 0.010984117165207863 = 0.004191627725958824 + 0.001 * 6.792489051818848
Epoch 830, val loss: 1.332171082496643
Epoch 840, training loss: 0.010827719233930111 = 0.004049601033329964 + 0.001 * 6.778117656707764
Epoch 840, val loss: 1.3375985622406006
Epoch 850, training loss: 0.010688086971640587 = 0.003915701061487198 + 0.001 * 6.772385120391846
Epoch 850, val loss: 1.3428540229797363
Epoch 860, training loss: 0.010564607568085194 = 0.003789302194491029 + 0.001 * 6.775305271148682
Epoch 860, val loss: 1.347995400428772
Epoch 870, training loss: 0.010443142615258694 = 0.0036698414478451014 + 0.001 * 6.773301124572754
Epoch 870, val loss: 1.352996826171875
Epoch 880, training loss: 0.010326727293431759 = 0.0035568764433264732 + 0.001 * 6.769850730895996
Epoch 880, val loss: 1.3579180240631104
Epoch 890, training loss: 0.010217055678367615 = 0.0034498563036322594 + 0.001 * 6.76719856262207
Epoch 890, val loss: 1.3627099990844727
Epoch 900, training loss: 0.010124249383807182 = 0.0033483747392892838 + 0.001 * 6.77587366104126
Epoch 900, val loss: 1.367402195930481
Epoch 910, training loss: 0.010018942877650261 = 0.0032521160319447517 + 0.001 * 6.766826629638672
Epoch 910, val loss: 1.3718639612197876
Epoch 920, training loss: 0.009922552853822708 = 0.0031606482807546854 + 0.001 * 6.761904716491699
Epoch 920, val loss: 1.376331090927124
Epoch 930, training loss: 0.009848233312368393 = 0.0030736832413822412 + 0.001 * 6.77454948425293
Epoch 930, val loss: 1.3806828260421753
Epoch 940, training loss: 0.00976203940808773 = 0.0029909058939665556 + 0.001 * 6.771132946014404
Epoch 940, val loss: 1.3849403858184814
Epoch 950, training loss: 0.009677495807409286 = 0.002912081778049469 + 0.001 * 6.765413284301758
Epoch 950, val loss: 1.38908052444458
Epoch 960, training loss: 0.009600539691746235 = 0.002836952218785882 + 0.001 * 6.763587474822998
Epoch 960, val loss: 1.3931429386138916
Epoch 970, training loss: 0.009534578770399094 = 0.002765282988548279 + 0.001 * 6.7692952156066895
Epoch 970, val loss: 1.3971285820007324
Epoch 980, training loss: 0.009451029822230339 = 0.0026968191377818584 + 0.001 * 6.754209995269775
Epoch 980, val loss: 1.400990605354309
Epoch 990, training loss: 0.009401305578649044 = 0.002631392562761903 + 0.001 * 6.769912242889404
Epoch 990, val loss: 1.404787302017212
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7815
Overall ASR: 0.7343
Flip ASR: 0.6933/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.944531798362732 = 1.9361579418182373 + 0.001 * 8.37382984161377
Epoch 0, val loss: 1.9342689514160156
Epoch 10, training loss: 1.9343329668045044 = 1.9259592294692993 + 0.001 * 8.373696327209473
Epoch 10, val loss: 1.923522710800171
Epoch 20, training loss: 1.9220454692840576 = 1.9136720895767212 + 0.001 * 8.373344421386719
Epoch 20, val loss: 1.9096603393554688
Epoch 30, training loss: 1.905166506767273 = 1.8967938423156738 + 0.001 * 8.372701644897461
Epoch 30, val loss: 1.8898576498031616
Epoch 40, training loss: 1.881222128868103 = 1.8728505373001099 + 0.001 * 8.371578216552734
Epoch 40, val loss: 1.8621257543563843
Epoch 50, training loss: 1.8487545251846313 = 1.84038507938385 + 0.001 * 8.369461059570312
Epoch 50, val loss: 1.8271023035049438
Epoch 60, training loss: 1.808152198791504 = 1.7997881174087524 + 0.001 * 8.364123344421387
Epoch 60, val loss: 1.787692904472351
Epoch 70, training loss: 1.7631360292434692 = 1.7547948360443115 + 0.001 * 8.341164588928223
Epoch 70, val loss: 1.7487980127334595
Epoch 80, training loss: 1.7132625579833984 = 1.705108642578125 + 0.001 * 8.153910636901855
Epoch 80, val loss: 1.7079014778137207
Epoch 90, training loss: 1.6461069583892822 = 1.6384902000427246 + 0.001 * 7.6167120933532715
Epoch 90, val loss: 1.6521798372268677
Epoch 100, training loss: 1.557337999343872 = 1.5498740673065186 + 0.001 * 7.463979244232178
Epoch 100, val loss: 1.579087257385254
Epoch 110, training loss: 1.4504811763763428 = 1.443164587020874 + 0.001 * 7.316614151000977
Epoch 110, val loss: 1.495879888534546
Epoch 120, training loss: 1.336491584777832 = 1.3292574882507324 + 0.001 * 7.234097957611084
Epoch 120, val loss: 1.4096359014511108
Epoch 130, training loss: 1.2224370241165161 = 1.2152462005615234 + 0.001 * 7.190812110900879
Epoch 130, val loss: 1.324832797050476
Epoch 140, training loss: 1.111945629119873 = 1.1047855615615845 + 0.001 * 7.160123825073242
Epoch 140, val loss: 1.2436224222183228
Epoch 150, training loss: 1.007476568222046 = 1.000340223312378 + 0.001 * 7.136329174041748
Epoch 150, val loss: 1.1676346063613892
Epoch 160, training loss: 0.9108666181564331 = 0.90374755859375 + 0.001 * 7.11907434463501
Epoch 160, val loss: 1.0989320278167725
Epoch 170, training loss: 0.8231235146522522 = 0.816015362739563 + 0.001 * 7.108158111572266
Epoch 170, val loss: 1.0378931760787964
Epoch 180, training loss: 0.7441437244415283 = 0.7370448112487793 + 0.001 * 7.098926067352295
Epoch 180, val loss: 0.9843452572822571
Epoch 190, training loss: 0.6733368039131165 = 0.6662466526031494 + 0.001 * 7.090123653411865
Epoch 190, val loss: 0.937747061252594
Epoch 200, training loss: 0.6096000075340271 = 0.6025208830833435 + 0.001 * 7.07914400100708
Epoch 200, val loss: 0.8976263403892517
Epoch 210, training loss: 0.5514025688171387 = 0.5443397760391235 + 0.001 * 7.062804222106934
Epoch 210, val loss: 0.8638931512832642
Epoch 220, training loss: 0.49743714928627014 = 0.490398645401001 + 0.001 * 7.038509368896484
Epoch 220, val loss: 0.8359714150428772
Epoch 230, training loss: 0.4470559060573578 = 0.44004663825035095 + 0.001 * 7.009263038635254
Epoch 230, val loss: 0.8124220371246338
Epoch 240, training loss: 0.4001957178115845 = 0.3931999206542969 + 0.001 * 6.995805740356445
Epoch 240, val loss: 0.7926797270774841
Epoch 250, training loss: 0.35691019892692566 = 0.34991976618766785 + 0.001 * 6.990434169769287
Epoch 250, val loss: 0.7763112187385559
Epoch 260, training loss: 0.31727516651153564 = 0.31028980016708374 + 0.001 * 6.98537540435791
Epoch 260, val loss: 0.7631374597549438
Epoch 270, training loss: 0.28135615587234497 = 0.2743726968765259 + 0.001 * 6.983470916748047
Epoch 270, val loss: 0.7529416680335999
Epoch 280, training loss: 0.24902933835983276 = 0.24204707145690918 + 0.001 * 6.982271194458008
Epoch 280, val loss: 0.7454478144645691
Epoch 290, training loss: 0.22005409002304077 = 0.2130722850561142 + 0.001 * 6.981805324554443
Epoch 290, val loss: 0.7407611608505249
Epoch 300, training loss: 0.19407010078430176 = 0.18708853423595428 + 0.001 * 6.981570720672607
Epoch 300, val loss: 0.7383319139480591
Epoch 310, training loss: 0.17077964544296265 = 0.16379804909229279 + 0.001 * 6.981592655181885
Epoch 310, val loss: 0.7384795546531677
Epoch 320, training loss: 0.15024042129516602 = 0.14325860142707825 + 0.001 * 6.981826305389404
Epoch 320, val loss: 0.7413809299468994
Epoch 330, training loss: 0.13235251605510712 = 0.12537029385566711 + 0.001 * 6.982228755950928
Epoch 330, val loss: 0.7472663521766663
Epoch 340, training loss: 0.11692474037408829 = 0.10994207113981247 + 0.001 * 6.982671737670898
Epoch 340, val loss: 0.755477786064148
Epoch 350, training loss: 0.1036074161529541 = 0.0966244488954544 + 0.001 * 6.982964038848877
Epoch 350, val loss: 0.7654702663421631
Epoch 360, training loss: 0.09209207445383072 = 0.08510912209749222 + 0.001 * 6.9829535484313965
Epoch 360, val loss: 0.7764191031455994
Epoch 370, training loss: 0.08216331154108047 = 0.07517961412668228 + 0.001 * 6.983694553375244
Epoch 370, val loss: 0.7879417538642883
Epoch 380, training loss: 0.07362429052591324 = 0.06664197891950607 + 0.001 * 6.982311725616455
Epoch 380, val loss: 0.7996794581413269
Epoch 390, training loss: 0.06626322120428085 = 0.05928180739283562 + 0.001 * 6.98141622543335
Epoch 390, val loss: 0.8113837242126465
Epoch 400, training loss: 0.059890978038311005 = 0.052910689264535904 + 0.001 * 6.980286598205566
Epoch 400, val loss: 0.8230310678482056
Epoch 410, training loss: 0.054360903799533844 = 0.047382812947034836 + 0.001 * 6.978091239929199
Epoch 410, val loss: 0.8344850540161133
Epoch 420, training loss: 0.04954541474580765 = 0.042573388665914536 + 0.001 * 6.972026824951172
Epoch 420, val loss: 0.8457958102226257
Epoch 430, training loss: 0.04535030573606491 = 0.03838105499744415 + 0.001 * 6.969249248504639
Epoch 430, val loss: 0.8569158911705017
Epoch 440, training loss: 0.04168839380145073 = 0.03471747785806656 + 0.001 * 6.970914840698242
Epoch 440, val loss: 0.8678212761878967
Epoch 450, training loss: 0.0384703204035759 = 0.03151082992553711 + 0.001 * 6.959488868713379
Epoch 450, val loss: 0.8785383105278015
Epoch 460, training loss: 0.035637058317661285 = 0.028692135587334633 + 0.001 * 6.944923400878906
Epoch 460, val loss: 0.88905930519104
Epoch 470, training loss: 0.033148348331451416 = 0.026204295456409454 + 0.001 * 6.9440531730651855
Epoch 470, val loss: 0.8994579315185547
Epoch 480, training loss: 0.030937187373638153 = 0.024000661447644234 + 0.001 * 6.936524868011475
Epoch 480, val loss: 0.9096999764442444
Epoch 490, training loss: 0.02895341068506241 = 0.022042447701096535 + 0.001 * 6.9109625816345215
Epoch 490, val loss: 0.9197876453399658
Epoch 500, training loss: 0.027216769754886627 = 0.02029844932258129 + 0.001 * 6.918321132659912
Epoch 500, val loss: 0.9296305775642395
Epoch 510, training loss: 0.025640834122896194 = 0.018740849569439888 + 0.001 * 6.899984359741211
Epoch 510, val loss: 0.9392099380493164
Epoch 520, training loss: 0.02423933893442154 = 0.017346784472465515 + 0.001 * 6.892553806304932
Epoch 520, val loss: 0.9486260414123535
Epoch 530, training loss: 0.022989219054579735 = 0.0160963274538517 + 0.001 * 6.892890930175781
Epoch 530, val loss: 0.9578253626823425
Epoch 540, training loss: 0.02186083421111107 = 0.014971830882132053 + 0.001 * 6.889003276824951
Epoch 540, val loss: 0.9667739272117615
Epoch 550, training loss: 0.020844103768467903 = 0.013958033174276352 + 0.001 * 6.886070728302002
Epoch 550, val loss: 0.9755151867866516
Epoch 560, training loss: 0.019925527274608612 = 0.013041895814239979 + 0.001 * 6.883631706237793
Epoch 560, val loss: 0.9840419292449951
Epoch 570, training loss: 0.019095251336693764 = 0.012211875058710575 + 0.001 * 6.883376598358154
Epoch 570, val loss: 0.9923444986343384
Epoch 580, training loss: 0.018339263275265694 = 0.011458019725978374 + 0.001 * 6.881243705749512
Epoch 580, val loss: 1.000465989112854
Epoch 590, training loss: 0.017646998167037964 = 0.010771935805678368 + 0.001 * 6.87506103515625
Epoch 590, val loss: 1.008337140083313
Epoch 600, training loss: 0.01701398938894272 = 0.010145895183086395 + 0.001 * 6.868094444274902
Epoch 600, val loss: 1.0160013437271118
Epoch 610, training loss: 0.01645505800843239 = 0.009573235176503658 + 0.001 * 6.881823539733887
Epoch 610, val loss: 1.0234594345092773
Epoch 620, training loss: 0.015924345701932907 = 0.00904835481196642 + 0.001 * 6.875991344451904
Epoch 620, val loss: 1.0307879447937012
Epoch 630, training loss: 0.015429934486746788 = 0.00856641586869955 + 0.001 * 6.863517761230469
Epoch 630, val loss: 1.0378899574279785
Epoch 640, training loss: 0.014987155795097351 = 0.00812294241040945 + 0.001 * 6.864212989807129
Epoch 640, val loss: 1.0447916984558105
Epoch 650, training loss: 0.014583462849259377 = 0.0077140601351857185 + 0.001 * 6.869402885437012
Epoch 650, val loss: 1.0515167713165283
Epoch 660, training loss: 0.014204482547938824 = 0.0073364800773561 + 0.001 * 6.868001937866211
Epoch 660, val loss: 1.058076024055481
Epoch 670, training loss: 0.013839216902852058 = 0.00698708975687623 + 0.001 * 6.852126598358154
Epoch 670, val loss: 1.0644772052764893
Epoch 680, training loss: 0.01353672705590725 = 0.0066631995141506195 + 0.001 * 6.8735270500183105
Epoch 680, val loss: 1.0706887245178223
Epoch 690, training loss: 0.013219932094216347 = 0.006362267304211855 + 0.001 * 6.857664585113525
Epoch 690, val loss: 1.0767714977264404
Epoch 700, training loss: 0.012951331213116646 = 0.006082269828766584 + 0.001 * 6.869060516357422
Epoch 700, val loss: 1.0827162265777588
Epoch 710, training loss: 0.012670040130615234 = 0.005821410100907087 + 0.001 * 6.848629474639893
Epoch 710, val loss: 1.0884991884231567
Epoch 720, training loss: 0.012432893738150597 = 0.0055780233815312386 + 0.001 * 6.854869365692139
Epoch 720, val loss: 1.0941498279571533
Epoch 730, training loss: 0.012196042574942112 = 0.005350670777261257 + 0.001 * 6.845371246337891
Epoch 730, val loss: 1.0996580123901367
Epoch 740, training loss: 0.011977838352322578 = 0.005137958563864231 + 0.001 * 6.839879989624023
Epoch 740, val loss: 1.1050031185150146
Epoch 750, training loss: 0.011787976138293743 = 0.004938666243106127 + 0.001 * 6.84930944442749
Epoch 750, val loss: 1.1102477312088013
Epoch 760, training loss: 0.011592412367463112 = 0.004751754458993673 + 0.001 * 6.840657711029053
Epoch 760, val loss: 1.1153541803359985
Epoch 770, training loss: 0.011409429833292961 = 0.004576270934194326 + 0.001 * 6.833158493041992
Epoch 770, val loss: 1.120340347290039
Epoch 780, training loss: 0.01124519295990467 = 0.004411187022924423 + 0.001 * 6.834005355834961
Epoch 780, val loss: 1.1251908540725708
Epoch 790, training loss: 0.011090675368905067 = 0.004255737643688917 + 0.001 * 6.834937572479248
Epoch 790, val loss: 1.129942774772644
Epoch 800, training loss: 0.010956503450870514 = 0.004109192173928022 + 0.001 * 6.847311019897461
Epoch 800, val loss: 1.1345903873443604
Epoch 810, training loss: 0.010808196850121021 = 0.00397088099271059 + 0.001 * 6.837315559387207
Epoch 810, val loss: 1.1391417980194092
Epoch 820, training loss: 0.010666554793715477 = 0.0038401992060244083 + 0.001 * 6.82635498046875
Epoch 820, val loss: 1.1435742378234863
Epoch 830, training loss: 0.010567879304289818 = 0.003716603387147188 + 0.001 * 6.85127592086792
Epoch 830, val loss: 1.147939682006836
Epoch 840, training loss: 0.010422172024846077 = 0.003599579678848386 + 0.001 * 6.822591781616211
Epoch 840, val loss: 1.152184247970581
Epoch 850, training loss: 0.010317670181393623 = 0.0034886698704212904 + 0.001 * 6.828999996185303
Epoch 850, val loss: 1.1563210487365723
Epoch 860, training loss: 0.010214531794190407 = 0.0033834604546427727 + 0.001 * 6.831070899963379
Epoch 860, val loss: 1.160403847694397
Epoch 870, training loss: 0.010129150003194809 = 0.0032835854217410088 + 0.001 * 6.845564842224121
Epoch 870, val loss: 1.1643702983856201
Epoch 880, training loss: 0.010013176128268242 = 0.003188682720065117 + 0.001 * 6.824493408203125
Epoch 880, val loss: 1.1682696342468262
Epoch 890, training loss: 0.009936646558344364 = 0.0030984270852059126 + 0.001 * 6.838218688964844
Epoch 890, val loss: 1.1720693111419678
Epoch 900, training loss: 0.009833228774368763 = 0.0030125488992780447 + 0.001 * 6.820679187774658
Epoch 900, val loss: 1.1758003234863281
Epoch 910, training loss: 0.00974327139556408 = 0.0029307387303560972 + 0.001 * 6.812532424926758
Epoch 910, val loss: 1.179424524307251
Epoch 920, training loss: 0.009657757356762886 = 0.0028527488466352224 + 0.001 * 6.805008411407471
Epoch 920, val loss: 1.1830018758773804
Epoch 930, training loss: 0.009599197655916214 = 0.002778381807729602 + 0.001 * 6.820815086364746
Epoch 930, val loss: 1.1864802837371826
Epoch 940, training loss: 0.00953633151948452 = 0.0027074338868260384 + 0.001 * 6.828896999359131
Epoch 940, val loss: 1.1898530721664429
Epoch 950, training loss: 0.009441236965358257 = 0.0026396492030471563 + 0.001 * 6.8015875816345215
Epoch 950, val loss: 1.1931933164596558
Epoch 960, training loss: 0.009397664107382298 = 0.0025748719926923513 + 0.001 * 6.822791576385498
Epoch 960, val loss: 1.1964401006698608
Epoch 970, training loss: 0.009316044859588146 = 0.002512916224077344 + 0.001 * 6.803128242492676
Epoch 970, val loss: 1.1996560096740723
Epoch 980, training loss: 0.009247824549674988 = 0.0024536065757274628 + 0.001 * 6.794217109680176
Epoch 980, val loss: 1.202772617340088
Epoch 990, training loss: 0.009190200828015804 = 0.002396791009232402 + 0.001 * 6.79340934753418
Epoch 990, val loss: 1.2058508396148682
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9188
Flip ASR: 0.9022/225 nodes
The final ASR:0.72448, 0.16285, Accuracy:0.81111, 0.02117
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9502])
updated graph: torch.Size([2, 10534])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.98524, 0.00904, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.956360936164856 = 1.9479870796203613 + 0.001 * 8.373858451843262
Epoch 0, val loss: 1.9505254030227661
Epoch 10, training loss: 1.9470038414001465 = 1.9386299848556519 + 0.001 * 8.373817443847656
Epoch 10, val loss: 1.941900610923767
Epoch 20, training loss: 1.9354006052017212 = 1.9270269870758057 + 0.001 * 8.37363338470459
Epoch 20, val loss: 1.9312613010406494
Epoch 30, training loss: 1.918647289276123 = 1.9102740287780762 + 0.001 * 8.37324047088623
Epoch 30, val loss: 1.915959358215332
Epoch 40, training loss: 1.8932232856750488 = 1.8848508596420288 + 0.001 * 8.37238597869873
Epoch 40, val loss: 1.8931605815887451
Epoch 50, training loss: 1.8563032150268555 = 1.8479331731796265 + 0.001 * 8.370091438293457
Epoch 50, val loss: 1.861266016960144
Epoch 60, training loss: 1.8109313249588013 = 1.8025696277618408 + 0.001 * 8.361645698547363
Epoch 60, val loss: 1.8240392208099365
Epoch 70, training loss: 1.7655726671218872 = 1.7572587728500366 + 0.001 * 8.313901901245117
Epoch 70, val loss: 1.7852377891540527
Epoch 80, training loss: 1.7118618488311768 = 1.7039388418197632 + 0.001 * 7.923046588897705
Epoch 80, val loss: 1.7336457967758179
Epoch 90, training loss: 1.638147234916687 = 1.6303560733795166 + 0.001 * 7.791153430938721
Epoch 90, val loss: 1.6669259071350098
Epoch 100, training loss: 1.5437800884246826 = 1.5360867977142334 + 0.001 * 7.693309783935547
Epoch 100, val loss: 1.5873863697052002
Epoch 110, training loss: 1.437638282775879 = 1.4301369190216064 + 0.001 * 7.501338005065918
Epoch 110, val loss: 1.4982362985610962
Epoch 120, training loss: 1.3316272497177124 = 1.3242782354354858 + 0.001 * 7.3490142822265625
Epoch 120, val loss: 1.4091633558273315
Epoch 130, training loss: 1.2299630641937256 = 1.2226372957229614 + 0.001 * 7.3257904052734375
Epoch 130, val loss: 1.3248205184936523
Epoch 140, training loss: 1.1313042640686035 = 1.1240241527557373 + 0.001 * 7.280130386352539
Epoch 140, val loss: 1.2456741333007812
Epoch 150, training loss: 1.0332708358764648 = 1.0260388851165771 + 0.001 * 7.231975078582764
Epoch 150, val loss: 1.1685899496078491
Epoch 160, training loss: 0.9348791241645813 = 0.9276924133300781 + 0.001 * 7.1867194175720215
Epoch 160, val loss: 1.092600703239441
Epoch 170, training loss: 0.8388487696647644 = 0.8316899538040161 + 0.001 * 7.158844947814941
Epoch 170, val loss: 1.0194470882415771
Epoch 180, training loss: 0.7501543760299683 = 0.743019163608551 + 0.001 * 7.135202884674072
Epoch 180, val loss: 0.9539090991020203
Epoch 190, training loss: 0.6720857620239258 = 0.6649780869483948 + 0.001 * 7.107656002044678
Epoch 190, val loss: 0.8994733691215515
Epoch 200, training loss: 0.6048263907432556 = 0.5977453589439392 + 0.001 * 7.08104944229126
Epoch 200, val loss: 0.8568955659866333
Epoch 210, training loss: 0.5470653772354126 = 0.539997398853302 + 0.001 * 7.067965507507324
Epoch 210, val loss: 0.8249835968017578
Epoch 220, training loss: 0.49708497524261475 = 0.49002113938331604 + 0.001 * 7.063844680786133
Epoch 220, val loss: 0.8021097779273987
Epoch 230, training loss: 0.4530629515647888 = 0.4460037350654602 + 0.001 * 7.0592041015625
Epoch 230, val loss: 0.7863873839378357
Epoch 240, training loss: 0.41349995136260986 = 0.4064459204673767 + 0.001 * 7.054017066955566
Epoch 240, val loss: 0.7763989567756653
Epoch 250, training loss: 0.3774840533733368 = 0.3704368770122528 + 0.001 * 7.047173023223877
Epoch 250, val loss: 0.7713199853897095
Epoch 260, training loss: 0.34473854303359985 = 0.3376990854740143 + 0.001 * 7.039450168609619
Epoch 260, val loss: 0.7706558108329773
Epoch 270, training loss: 0.31524232029914856 = 0.3082123100757599 + 0.001 * 7.029996871948242
Epoch 270, val loss: 0.7739031910896301
Epoch 280, training loss: 0.2887883186340332 = 0.2817707061767578 + 0.001 * 7.017615795135498
Epoch 280, val loss: 0.7804560661315918
Epoch 290, training loss: 0.2648990750312805 = 0.2578851580619812 + 0.001 * 7.01392936706543
Epoch 290, val loss: 0.7897774577140808
Epoch 300, training loss: 0.24291014671325684 = 0.2359083890914917 + 0.001 * 7.0017571449279785
Epoch 300, val loss: 0.8013225197792053
Epoch 310, training loss: 0.22222575545310974 = 0.21523183584213257 + 0.001 * 6.9939117431640625
Epoch 310, val loss: 0.814911961555481
Epoch 320, training loss: 0.20247212052345276 = 0.19548021256923676 + 0.001 * 6.991912364959717
Epoch 320, val loss: 0.8305277228355408
Epoch 330, training loss: 0.1835743486881256 = 0.17658646404743195 + 0.001 * 6.987876892089844
Epoch 330, val loss: 0.8481536507606506
Epoch 340, training loss: 0.16571153700351715 = 0.158726304769516 + 0.001 * 6.985229015350342
Epoch 340, val loss: 0.8678977489471436
Epoch 350, training loss: 0.14911815524101257 = 0.1421339064836502 + 0.001 * 6.984241008758545
Epoch 350, val loss: 0.8896435499191284
Epoch 360, training loss: 0.133942112326622 = 0.12695837020874023 + 0.001 * 6.983739376068115
Epoch 360, val loss: 0.9132152795791626
Epoch 370, training loss: 0.12022649496793747 = 0.11324537545442581 + 0.001 * 6.981119632720947
Epoch 370, val loss: 0.9384057521820068
Epoch 380, training loss: 0.10795612633228302 = 0.1009734570980072 + 0.001 * 6.982668876647949
Epoch 380, val loss: 0.9651123881340027
Epoch 390, training loss: 0.09705822914838791 = 0.09007660299539566 + 0.001 * 6.981625080108643
Epoch 390, val loss: 0.9930853843688965
Epoch 400, training loss: 0.08745791763067245 = 0.08047749102115631 + 0.001 * 6.980429649353027
Epoch 400, val loss: 1.0218039751052856
Epoch 410, training loss: 0.0790570005774498 = 0.07207693159580231 + 0.001 * 6.980070114135742
Epoch 410, val loss: 1.0507569313049316
Epoch 420, training loss: 0.07172652333974838 = 0.06474852561950684 + 0.001 * 6.977995872497559
Epoch 420, val loss: 1.0794740915298462
Epoch 430, training loss: 0.06533050537109375 = 0.0583534799516201 + 0.001 * 6.977025985717773
Epoch 430, val loss: 1.107505202293396
Epoch 440, training loss: 0.05973489582538605 = 0.05275816470384598 + 0.001 * 6.976729869842529
Epoch 440, val loss: 1.1347527503967285
Epoch 450, training loss: 0.054826293140649796 = 0.04784625396132469 + 0.001 * 6.980037689208984
Epoch 450, val loss: 1.1611993312835693
Epoch 460, training loss: 0.05049668624997139 = 0.043519310653209686 + 0.001 * 6.977375507354736
Epoch 460, val loss: 1.1868181228637695
Epoch 470, training loss: 0.046670977026224136 = 0.03969478979706764 + 0.001 * 6.976186752319336
Epoch 470, val loss: 1.2116467952728271
Epoch 480, training loss: 0.04327672719955444 = 0.03630479797720909 + 0.001 * 6.971929550170898
Epoch 480, val loss: 1.2357555627822876
Epoch 490, training loss: 0.04026290774345398 = 0.03329150751233101 + 0.001 * 6.971400737762451
Epoch 490, val loss: 1.2591118812561035
Epoch 500, training loss: 0.037573665380477905 = 0.030605116859078407 + 0.001 * 6.96854829788208
Epoch 500, val loss: 1.281705379486084
Epoch 510, training loss: 0.03518686071038246 = 0.028204163536429405 + 0.001 * 6.982696533203125
Epoch 510, val loss: 1.303585171699524
Epoch 520, training loss: 0.03302178531885147 = 0.02605270966887474 + 0.001 * 6.96907377243042
Epoch 520, val loss: 1.3247383832931519
Epoch 530, training loss: 0.031086811795830727 = 0.024120183661580086 + 0.001 * 6.966627597808838
Epoch 530, val loss: 1.3451601266860962
Epoch 540, training loss: 0.02934860810637474 = 0.022380460053682327 + 0.001 * 6.968146800994873
Epoch 540, val loss: 1.3648760318756104
Epoch 550, training loss: 0.027777059003710747 = 0.02081099897623062 + 0.001 * 6.966059684753418
Epoch 550, val loss: 1.3839198350906372
Epoch 560, training loss: 0.026351049542427063 = 0.019392065703868866 + 0.001 * 6.958983898162842
Epoch 560, val loss: 1.4022890329360962
Epoch 570, training loss: 0.02506568841636181 = 0.01810639165341854 + 0.001 * 6.959296226501465
Epoch 570, val loss: 1.4200297594070435
Epoch 580, training loss: 0.02390301413834095 = 0.01693907380104065 + 0.001 * 6.963940143585205
Epoch 580, val loss: 1.4371224641799927
Epoch 590, training loss: 0.022829962894320488 = 0.015876995399594307 + 0.001 * 6.952967643737793
Epoch 590, val loss: 1.453643560409546
Epoch 600, training loss: 0.02189558744430542 = 0.014908860437572002 + 0.001 * 6.9867262840271
Epoch 600, val loss: 1.4695889949798584
Epoch 610, training loss: 0.020977338775992393 = 0.014024795033037663 + 0.001 * 6.952543258666992
Epoch 610, val loss: 1.484973669052124
Epoch 620, training loss: 0.020163148641586304 = 0.013215840794146061 + 0.001 * 6.947307586669922
Epoch 620, val loss: 1.4998366832733154
Epoch 630, training loss: 0.019428566098213196 = 0.01247419323772192 + 0.001 * 6.954372406005859
Epoch 630, val loss: 1.5141901969909668
Epoch 640, training loss: 0.018745537847280502 = 0.011792964302003384 + 0.001 * 6.952574253082275
Epoch 640, val loss: 1.5280616283416748
Epoch 650, training loss: 0.018113110214471817 = 0.011166095733642578 + 0.001 * 6.947013854980469
Epoch 650, val loss: 1.5414743423461914
Epoch 660, training loss: 0.017541810870170593 = 0.01058821752667427 + 0.001 * 6.953592300415039
Epoch 660, val loss: 1.5544350147247314
Epoch 670, training loss: 0.016989553347229958 = 0.010054510086774826 + 0.001 * 6.935042858123779
Epoch 670, val loss: 1.5669745206832886
Epoch 680, training loss: 0.016490135341882706 = 0.009560749866068363 + 0.001 * 6.929385185241699
Epoch 680, val loss: 1.5790990591049194
Epoch 690, training loss: 0.01604626327753067 = 0.009103208780288696 + 0.001 * 6.94305419921875
Epoch 690, val loss: 1.5908234119415283
Epoch 700, training loss: 0.015614325180649757 = 0.008678555488586426 + 0.001 * 6.935769081115723
Epoch 700, val loss: 1.6021853685379028
Epoch 710, training loss: 0.01522967778146267 = 0.008283872157335281 + 0.001 * 6.945804595947266
Epoch 710, val loss: 1.613194227218628
Epoch 720, training loss: 0.01483642216771841 = 0.007916407659649849 + 0.001 * 6.920014381408691
Epoch 720, val loss: 1.6238386631011963
Epoch 730, training loss: 0.014486300759017467 = 0.007573809940367937 + 0.001 * 6.912490367889404
Epoch 730, val loss: 1.634179949760437
Epoch 740, training loss: 0.014188967645168304 = 0.007253933232277632 + 0.001 * 6.935033798217773
Epoch 740, val loss: 1.6442232131958008
Epoch 750, training loss: 0.013875861652195454 = 0.0069548701867461205 + 0.001 * 6.920990943908691
Epoch 750, val loss: 1.6539480686187744
Epoch 760, training loss: 0.01362713985145092 = 0.006674949545413256 + 0.001 * 6.9521894454956055
Epoch 760, val loss: 1.6634106636047363
Epoch 770, training loss: 0.0133399348706007 = 0.006412518210709095 + 0.001 * 6.927416801452637
Epoch 770, val loss: 1.6725683212280273
Epoch 780, training loss: 0.01306411437690258 = 0.0061662341468036175 + 0.001 * 6.897879600524902
Epoch 780, val loss: 1.6814912557601929
Epoch 790, training loss: 0.012853090651333332 = 0.005934788379818201 + 0.001 * 6.918302059173584
Epoch 790, val loss: 1.690150499343872
Epoch 800, training loss: 0.012624138966202736 = 0.005717078689485788 + 0.001 * 6.907060146331787
Epoch 800, val loss: 1.6985570192337036
Epoch 810, training loss: 0.012417573481798172 = 0.005512051749974489 + 0.001 * 6.905521869659424
Epoch 810, val loss: 1.7067492008209229
Epoch 820, training loss: 0.012203892692923546 = 0.005318700335919857 + 0.001 * 6.885191917419434
Epoch 820, val loss: 1.714668869972229
Epoch 830, training loss: 0.012026764452457428 = 0.005136185325682163 + 0.001 * 6.8905792236328125
Epoch 830, val loss: 1.7224236726760864
Epoch 840, training loss: 0.01188887469470501 = 0.00496371416375041 + 0.001 * 6.9251604080200195
Epoch 840, val loss: 1.729928731918335
Epoch 850, training loss: 0.011699661612510681 = 0.004800627939403057 + 0.001 * 6.899034023284912
Epoch 850, val loss: 1.7372417449951172
Epoch 860, training loss: 0.01152035128325224 = 0.004646185785531998 + 0.001 * 6.874165058135986
Epoch 860, val loss: 1.7443715333938599
Epoch 870, training loss: 0.011395850218832493 = 0.004499807022511959 + 0.001 * 6.896042823791504
Epoch 870, val loss: 1.7512998580932617
Epoch 880, training loss: 0.011243866756558418 = 0.004360971041023731 + 0.001 * 6.882894992828369
Epoch 880, val loss: 1.7580705881118774
Epoch 890, training loss: 0.011095385998487473 = 0.004229165147989988 + 0.001 * 6.866220951080322
Epoch 890, val loss: 1.7646658420562744
Epoch 900, training loss: 0.010972891934216022 = 0.0041038901545107365 + 0.001 * 6.869001388549805
Epoch 900, val loss: 1.7710994482040405
Epoch 910, training loss: 0.01084819808602333 = 0.003984788898378611 + 0.001 * 6.863409519195557
Epoch 910, val loss: 1.7773653268814087
Epoch 920, training loss: 0.010727398097515106 = 0.0038714653346687555 + 0.001 * 6.855932235717773
Epoch 920, val loss: 1.783463954925537
Epoch 930, training loss: 0.01064453087747097 = 0.003763499204069376 + 0.001 * 6.881031036376953
Epoch 930, val loss: 1.789410948753357
Epoch 940, training loss: 0.010525145567953587 = 0.0036605747882276773 + 0.001 * 6.864570140838623
Epoch 940, val loss: 1.795209527015686
Epoch 950, training loss: 0.010459976270794868 = 0.0035623859148472548 + 0.001 * 6.897590160369873
Epoch 950, val loss: 1.8008880615234375
Epoch 960, training loss: 0.010339225642383099 = 0.0034686413127928972 + 0.001 * 6.870584011077881
Epoch 960, val loss: 1.80642569065094
Epoch 970, training loss: 0.01022273674607277 = 0.0033790920861065388 + 0.001 * 6.843644142150879
Epoch 970, val loss: 1.8118205070495605
Epoch 980, training loss: 0.010151350870728493 = 0.003293489571660757 + 0.001 * 6.857860565185547
Epoch 980, val loss: 1.817085862159729
Epoch 990, training loss: 0.010058939456939697 = 0.0032116007059812546 + 0.001 * 6.847338676452637
Epoch 990, val loss: 1.8222323656082153
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7630
Overall ASR: 0.4207
Flip ASR: 0.3244/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9675525426864624 = 1.9591786861419678 + 0.001 * 8.373875617980957
Epoch 0, val loss: 1.9598908424377441
Epoch 10, training loss: 1.9577555656433105 = 1.949381709098816 + 0.001 * 8.373798370361328
Epoch 10, val loss: 1.950059413909912
Epoch 20, training loss: 1.9458516836166382 = 1.9374780654907227 + 0.001 * 8.373575210571289
Epoch 20, val loss: 1.9374889135360718
Epoch 30, training loss: 1.9291640520095825 = 1.9207909107208252 + 0.001 * 8.373102188110352
Epoch 30, val loss: 1.919433355331421
Epoch 40, training loss: 1.9040446281433105 = 1.8956725597381592 + 0.001 * 8.372082710266113
Epoch 40, val loss: 1.8922343254089355
Epoch 50, training loss: 1.8667171001434326 = 1.8583474159240723 + 0.001 * 8.369722366333008
Epoch 50, val loss: 1.8527300357818604
Epoch 60, training loss: 1.8175073862075806 = 1.8091448545455933 + 0.001 * 8.362568855285645
Epoch 60, val loss: 1.8033599853515625
Epoch 70, training loss: 1.7668864727020264 = 1.7585569620132446 + 0.001 * 8.329540252685547
Epoch 70, val loss: 1.7565644979476929
Epoch 80, training loss: 1.7176717519760132 = 1.7096256017684937 + 0.001 * 8.0462064743042
Epoch 80, val loss: 1.7114568948745728
Epoch 90, training loss: 1.6537837982177734 = 1.6459978818893433 + 0.001 * 7.785918712615967
Epoch 90, val loss: 1.6543824672698975
Epoch 100, training loss: 1.5690324306488037 = 1.561434268951416 + 0.001 * 7.5981831550598145
Epoch 100, val loss: 1.583147644996643
Epoch 110, training loss: 1.464479684829712 = 1.4570890665054321 + 0.001 * 7.390622138977051
Epoch 110, val loss: 1.4982070922851562
Epoch 120, training loss: 1.3537719249725342 = 1.3465090990066528 + 0.001 * 7.2628173828125
Epoch 120, val loss: 1.4095864295959473
Epoch 130, training loss: 1.247054100036621 = 1.239865779876709 + 0.001 * 7.188350200653076
Epoch 130, val loss: 1.3268287181854248
Epoch 140, training loss: 1.1465262174606323 = 1.1393646001815796 + 0.001 * 7.161562442779541
Epoch 140, val loss: 1.2517120838165283
Epoch 150, training loss: 1.0516830682754517 = 1.0445315837860107 + 0.001 * 7.151471138000488
Epoch 150, val loss: 1.1820462942123413
Epoch 160, training loss: 0.9624645113945007 = 0.9553307890892029 + 0.001 * 7.133694648742676
Epoch 160, val loss: 1.117419958114624
Epoch 170, training loss: 0.879749059677124 = 0.8726344704627991 + 0.001 * 7.1145734786987305
Epoch 170, val loss: 1.0580122470855713
Epoch 180, training loss: 0.8043383955955505 = 0.7972479462623596 + 0.001 * 7.090422630310059
Epoch 180, val loss: 1.0042917728424072
Epoch 190, training loss: 0.7358155250549316 = 0.7287488579750061 + 0.001 * 7.066660404205322
Epoch 190, val loss: 0.9563751220703125
Epoch 200, training loss: 0.6727622151374817 = 0.6657118201255798 + 0.001 * 7.050386428833008
Epoch 200, val loss: 0.9136322736740112
Epoch 210, training loss: 0.613579511642456 = 0.6065362691879272 + 0.001 * 7.043240070343018
Epoch 210, val loss: 0.8746525645256042
Epoch 220, training loss: 0.5572436451911926 = 0.5502024292945862 + 0.001 * 7.041229724884033
Epoch 220, val loss: 0.8383148908615112
Epoch 230, training loss: 0.5035175085067749 = 0.49647656083106995 + 0.001 * 7.040942668914795
Epoch 230, val loss: 0.8042575716972351
Epoch 240, training loss: 0.4529310464859009 = 0.44589048624038696 + 0.001 * 7.040569305419922
Epoch 240, val loss: 0.7730156779289246
Epoch 250, training loss: 0.40633976459503174 = 0.39929938316345215 + 0.001 * 7.040387153625488
Epoch 250, val loss: 0.7457305192947388
Epoch 260, training loss: 0.3643952012062073 = 0.3573548495769501 + 0.001 * 7.040353775024414
Epoch 260, val loss: 0.7235278487205505
Epoch 270, training loss: 0.32718637585639954 = 0.3201460540294647 + 0.001 * 7.040311336517334
Epoch 270, val loss: 0.7070015668869019
Epoch 280, training loss: 0.294231653213501 = 0.2871873676776886 + 0.001 * 7.044288158416748
Epoch 280, val loss: 0.6957293748855591
Epoch 290, training loss: 0.26475974917411804 = 0.25771814584732056 + 0.001 * 7.041593074798584
Epoch 290, val loss: 0.6887084245681763
Epoch 300, training loss: 0.2379050999879837 = 0.23086468875408173 + 0.001 * 7.040409088134766
Epoch 300, val loss: 0.6852188110351562
Epoch 310, training loss: 0.2129978984594345 = 0.2059587836265564 + 0.001 * 7.039112091064453
Epoch 310, val loss: 0.684478759765625
Epoch 320, training loss: 0.18969783186912537 = 0.18265922367572784 + 0.001 * 7.038609504699707
Epoch 320, val loss: 0.6861161589622498
Epoch 330, training loss: 0.1679810881614685 = 0.1609475016593933 + 0.001 * 7.033592224121094
Epoch 330, val loss: 0.6898486614227295
Epoch 340, training loss: 0.14803627133369446 = 0.14100545644760132 + 0.001 * 7.03081750869751
Epoch 340, val loss: 0.6954625844955444
Epoch 350, training loss: 0.13005103170871735 = 0.1230243518948555 + 0.001 * 7.026678562164307
Epoch 350, val loss: 0.702786922454834
Epoch 360, training loss: 0.11412651836872101 = 0.10710485279560089 + 0.001 * 7.021661281585693
Epoch 360, val loss: 0.7115683555603027
Epoch 370, training loss: 0.10019727051258087 = 0.09318788349628448 + 0.001 * 7.009390354156494
Epoch 370, val loss: 0.7215849161148071
Epoch 380, training loss: 0.08819621801376343 = 0.08117841184139252 + 0.001 * 7.017804145812988
Epoch 380, val loss: 0.7326658964157104
Epoch 390, training loss: 0.07792265713214874 = 0.07092094421386719 + 0.001 * 7.001710414886475
Epoch 390, val loss: 0.7446260452270508
Epoch 400, training loss: 0.06920257955789566 = 0.06221530959010124 + 0.001 * 6.987268924713135
Epoch 400, val loss: 0.7571334838867188
Epoch 410, training loss: 0.06184149533510208 = 0.054843831807374954 + 0.001 * 6.997661113739014
Epoch 410, val loss: 0.7699399590492249
Epoch 420, training loss: 0.05557498335838318 = 0.048591405153274536 + 0.001 * 6.983577251434326
Epoch 420, val loss: 0.782813310623169
Epoch 430, training loss: 0.05024483799934387 = 0.04327785596251488 + 0.001 * 6.966982364654541
Epoch 430, val loss: 0.7956036329269409
Epoch 440, training loss: 0.04571232199668884 = 0.03874143213033676 + 0.001 * 6.970889091491699
Epoch 440, val loss: 0.8082724809646606
Epoch 450, training loss: 0.0418141707777977 = 0.03484518080949783 + 0.001 * 6.96898889541626
Epoch 450, val loss: 0.8207687139511108
Epoch 460, training loss: 0.038434337824583054 = 0.031476330012083054 + 0.001 * 6.958007335662842
Epoch 460, val loss: 0.8330762982368469
Epoch 470, training loss: 0.035511866211891174 = 0.028547482565045357 + 0.001 * 6.964381694793701
Epoch 470, val loss: 0.8451786041259766
Epoch 480, training loss: 0.0329439714550972 = 0.025992171838879585 + 0.001 * 6.951799392700195
Epoch 480, val loss: 0.8570759296417236
Epoch 490, training loss: 0.03069756180047989 = 0.023753061890602112 + 0.001 * 6.944499492645264
Epoch 490, val loss: 0.8686815500259399
Epoch 500, training loss: 0.028731325641274452 = 0.0217836182564497 + 0.001 * 6.947707176208496
Epoch 500, val loss: 0.8800458312034607
Epoch 510, training loss: 0.026979032903909683 = 0.02004546858370304 + 0.001 * 6.93356466293335
Epoch 510, val loss: 0.8911083936691284
Epoch 520, training loss: 0.025444617494940758 = 0.01850535348057747 + 0.001 * 6.939263820648193
Epoch 520, val loss: 0.90188068151474
Epoch 530, training loss: 0.024072328582406044 = 0.017135748639702797 + 0.001 * 6.93657922744751
Epoch 530, val loss: 0.9124051928520203
Epoch 540, training loss: 0.022857237607240677 = 0.015913009643554688 + 0.001 * 6.94422721862793
Epoch 540, val loss: 0.9226042628288269
Epoch 550, training loss: 0.02174113690853119 = 0.014817459508776665 + 0.001 * 6.923676490783691
Epoch 550, val loss: 0.9325591325759888
Epoch 560, training loss: 0.02075762301683426 = 0.013832185417413712 + 0.001 * 6.925436973571777
Epoch 560, val loss: 0.9422593712806702
Epoch 570, training loss: 0.019857697188854218 = 0.01294270996004343 + 0.001 * 6.9149861335754395
Epoch 570, val loss: 0.9517154097557068
Epoch 580, training loss: 0.019050173461437225 = 0.012135935947299004 + 0.001 * 6.914237022399902
Epoch 580, val loss: 0.9609067440032959
Epoch 590, training loss: 0.018311481922864914 = 0.011399728246033192 + 0.001 * 6.911752700805664
Epoch 590, val loss: 0.969878613948822
Epoch 600, training loss: 0.017658066004514694 = 0.010724836029112339 + 0.001 * 6.933228969573975
Epoch 600, val loss: 0.978680431842804
Epoch 610, training loss: 0.01701883040368557 = 0.010105066932737827 + 0.001 * 6.91376256942749
Epoch 610, val loss: 0.9873105883598328
Epoch 620, training loss: 0.01644742116332054 = 0.009534109383821487 + 0.001 * 6.913310527801514
Epoch 620, val loss: 0.995771050453186
Epoch 630, training loss: 0.015916654840111732 = 0.009008149616420269 + 0.001 * 6.908505439758301
Epoch 630, val loss: 1.004114031791687
Epoch 640, training loss: 0.015447705052793026 = 0.00852296408265829 + 0.001 * 6.924740791320801
Epoch 640, val loss: 1.0122538805007935
Epoch 650, training loss: 0.014964563772082329 = 0.008074969053268433 + 0.001 * 6.889594554901123
Epoch 650, val loss: 1.0202465057373047
Epoch 660, training loss: 0.01455649733543396 = 0.007660866715013981 + 0.001 * 6.8956298828125
Epoch 660, val loss: 1.0280718803405762
Epoch 670, training loss: 0.014179015532135963 = 0.007277768105268478 + 0.001 * 6.901247501373291
Epoch 670, val loss: 1.035731554031372
Epoch 680, training loss: 0.013798883184790611 = 0.00692319218069315 + 0.001 * 6.875690937042236
Epoch 680, val loss: 1.043223261833191
Epoch 690, training loss: 0.013466685079038143 = 0.006594428792595863 + 0.001 * 6.872255802154541
Epoch 690, val loss: 1.0505536794662476
Epoch 700, training loss: 0.013180768117308617 = 0.00628927256911993 + 0.001 * 6.891495704650879
Epoch 700, val loss: 1.0577082633972168
Epoch 710, training loss: 0.01288621872663498 = 0.006005500443279743 + 0.001 * 6.880717754364014
Epoch 710, val loss: 1.0646981000900269
Epoch 720, training loss: 0.012609202414751053 = 0.005741333123296499 + 0.001 * 6.867868900299072
Epoch 720, val loss: 1.0715253353118896
Epoch 730, training loss: 0.012377745471894741 = 0.0054950653575360775 + 0.001 * 6.8826799392700195
Epoch 730, val loss: 1.0782098770141602
Epoch 740, training loss: 0.012126065790653229 = 0.005265258252620697 + 0.001 * 6.8608078956604
Epoch 740, val loss: 1.0847232341766357
Epoch 750, training loss: 0.01195111870765686 = 0.005050478503108025 + 0.001 * 6.900639533996582
Epoch 750, val loss: 1.091101050376892
Epoch 760, training loss: 0.011714019812643528 = 0.004849571734666824 + 0.001 * 6.864447593688965
Epoch 760, val loss: 1.0973248481750488
Epoch 770, training loss: 0.011506771668791771 = 0.004661428742110729 + 0.001 * 6.845343112945557
Epoch 770, val loss: 1.1034120321273804
Epoch 780, training loss: 0.01135946437716484 = 0.004484919831156731 + 0.001 * 6.874544620513916
Epoch 780, val loss: 1.109362006187439
Epoch 790, training loss: 0.011190386489033699 = 0.004319183062762022 + 0.001 * 6.8712029457092285
Epoch 790, val loss: 1.1151789426803589
Epoch 800, training loss: 0.011013085022568703 = 0.004163401201367378 + 0.001 * 6.849682807922363
Epoch 800, val loss: 1.1208609342575073
Epoch 810, training loss: 0.010861814022064209 = 0.004016779363155365 + 0.001 * 6.845034122467041
Epoch 810, val loss: 1.1264264583587646
Epoch 820, training loss: 0.010772976092994213 = 0.0038785869255661964 + 0.001 * 6.894388675689697
Epoch 820, val loss: 1.1318633556365967
Epoch 830, training loss: 0.010577256791293621 = 0.0037481526378542185 + 0.001 * 6.829103469848633
Epoch 830, val loss: 1.1371815204620361
Epoch 840, training loss: 0.01044475194066763 = 0.0036248175892978907 + 0.001 * 6.819934368133545
Epoch 840, val loss: 1.1423883438110352
Epoch 850, training loss: 0.010393168777227402 = 0.003508066525682807 + 0.001 * 6.885102272033691
Epoch 850, val loss: 1.1475002765655518
Epoch 860, training loss: 0.010260019451379776 = 0.00339739047922194 + 0.001 * 6.86262845993042
Epoch 860, val loss: 1.1525236368179321
Epoch 870, training loss: 0.010131003335118294 = 0.0032921847887337208 + 0.001 * 6.838818550109863
Epoch 870, val loss: 1.157439947128296
Epoch 880, training loss: 0.010044867172837257 = 0.003192027099430561 + 0.001 * 6.852839469909668
Epoch 880, val loss: 1.162274956703186
Epoch 890, training loss: 0.009950160048902035 = 0.0030963304452598095 + 0.001 * 6.853829383850098
Epoch 890, val loss: 1.1670453548431396
Epoch 900, training loss: 0.009840426035225391 = 0.0030047756154090166 + 0.001 * 6.835650444030762
Epoch 900, val loss: 1.1717569828033447
Epoch 910, training loss: 0.009719918482005596 = 0.0029168217442929745 + 0.001 * 6.803096294403076
Epoch 910, val loss: 1.176437258720398
Epoch 920, training loss: 0.009648790583014488 = 0.0028321919962763786 + 0.001 * 6.816598892211914
Epoch 920, val loss: 1.181082010269165
Epoch 930, training loss: 0.009564640000462532 = 0.0027507140766829252 + 0.001 * 6.813925743103027
Epoch 930, val loss: 1.185686707496643
Epoch 940, training loss: 0.009477697312831879 = 0.002672223374247551 + 0.001 * 6.805474281311035
Epoch 940, val loss: 1.1902365684509277
Epoch 950, training loss: 0.009390722028911114 = 0.0025967112742364407 + 0.001 * 6.794010639190674
Epoch 950, val loss: 1.1947633028030396
Epoch 960, training loss: 0.009331896901130676 = 0.0025241177063435316 + 0.001 * 6.807778835296631
Epoch 960, val loss: 1.199231743812561
Epoch 970, training loss: 0.009228819981217384 = 0.0024542894680052996 + 0.001 * 6.774529933929443
Epoch 970, val loss: 1.2036707401275635
Epoch 980, training loss: 0.009192800149321556 = 0.0023872482124716043 + 0.001 * 6.805551528930664
Epoch 980, val loss: 1.2080398797988892
Epoch 990, training loss: 0.009121525101363659 = 0.002322969725355506 + 0.001 * 6.798555374145508
Epoch 990, val loss: 1.21235990524292
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.9963
Flip ASR: 0.9956/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.970328450202942 = 1.9619544744491577 + 0.001 * 8.373916625976562
Epoch 0, val loss: 1.9660866260528564
Epoch 10, training loss: 1.9590386152267456 = 1.950664758682251 + 0.001 * 8.373866081237793
Epoch 10, val loss: 1.9547916650772095
Epoch 20, training loss: 1.9451384544372559 = 1.9367647171020508 + 0.001 * 8.373723030090332
Epoch 20, val loss: 1.9404102563858032
Epoch 30, training loss: 1.9256877899169922 = 1.9173144102096558 + 0.001 * 8.373408317565918
Epoch 30, val loss: 1.9198734760284424
Epoch 40, training loss: 1.8969788551330566 = 1.8886061906814575 + 0.001 * 8.372660636901855
Epoch 40, val loss: 1.8898712396621704
Epoch 50, training loss: 1.8563508987426758 = 1.847980260848999 + 0.001 * 8.370662689208984
Epoch 50, val loss: 1.8491312265396118
Epoch 60, training loss: 1.8095372915267944 = 1.801173448562622 + 0.001 * 8.363834381103516
Epoch 60, val loss: 1.8065872192382812
Epoch 70, training loss: 1.7692739963531494 = 1.7609409093856812 + 0.001 * 8.333108901977539
Epoch 70, val loss: 1.7737751007080078
Epoch 80, training loss: 1.7222799062728882 = 1.7141518592834473 + 0.001 * 8.128060340881348
Epoch 80, val loss: 1.7324002981185913
Epoch 90, training loss: 1.6577211618423462 = 1.649791955947876 + 0.001 * 7.929238319396973
Epoch 90, val loss: 1.6762478351593018
Epoch 100, training loss: 1.5737299919128418 = 1.565856695175171 + 0.001 * 7.873328685760498
Epoch 100, val loss: 1.604999303817749
Epoch 110, training loss: 1.4783737659454346 = 1.4705806970596313 + 0.001 * 7.79304313659668
Epoch 110, val loss: 1.5252149105072021
Epoch 120, training loss: 1.3824892044067383 = 1.3749243021011353 + 0.001 * 7.564916133880615
Epoch 120, val loss: 1.4467189311981201
Epoch 130, training loss: 1.2881484031677246 = 1.2807927131652832 + 0.001 * 7.355740070343018
Epoch 130, val loss: 1.3727266788482666
Epoch 140, training loss: 1.193078875541687 = 1.1857666969299316 + 0.001 * 7.3121337890625
Epoch 140, val loss: 1.299726963043213
Epoch 150, training loss: 1.0957790613174438 = 1.0884895324707031 + 0.001 * 7.289587020874023
Epoch 150, val loss: 1.2256157398223877
Epoch 160, training loss: 0.9973574280738831 = 0.9900878071784973 + 0.001 * 7.269649982452393
Epoch 160, val loss: 1.1513770818710327
Epoch 170, training loss: 0.9010242819786072 = 0.8937779068946838 + 0.001 * 7.246363162994385
Epoch 170, val loss: 1.0790215730667114
Epoch 180, training loss: 0.8106571435928345 = 0.8034367561340332 + 0.001 * 7.220364093780518
Epoch 180, val loss: 1.0112149715423584
Epoch 190, training loss: 0.7291715741157532 = 0.7219809293746948 + 0.001 * 7.190640926361084
Epoch 190, val loss: 0.9509338140487671
Epoch 200, training loss: 0.6576218605041504 = 0.6504572033882141 + 0.001 * 7.16463565826416
Epoch 200, val loss: 0.8995769023895264
Epoch 210, training loss: 0.5954483151435852 = 0.5882967114448547 + 0.001 * 7.151589393615723
Epoch 210, val loss: 0.8569946885108948
Epoch 220, training loss: 0.5412808060646057 = 0.5341330170631409 + 0.001 * 7.147762298583984
Epoch 220, val loss: 0.8226443529129028
Epoch 230, training loss: 0.4932945668697357 = 0.48614853620529175 + 0.001 * 7.146030902862549
Epoch 230, val loss: 0.795121967792511
Epoch 240, training loss: 0.4494487941265106 = 0.442306250333786 + 0.001 * 7.142552852630615
Epoch 240, val loss: 0.7730639576911926
Epoch 250, training loss: 0.40779751539230347 = 0.4006602466106415 + 0.001 * 7.137263774871826
Epoch 250, val loss: 0.7552545666694641
Epoch 260, training loss: 0.36691784858703613 = 0.35978779196739197 + 0.001 * 7.130056858062744
Epoch 260, val loss: 0.7405896186828613
Epoch 270, training loss: 0.3265296220779419 = 0.3194071054458618 + 0.001 * 7.122525691986084
Epoch 270, val loss: 0.7289038896560669
Epoch 280, training loss: 0.28752413392066956 = 0.2804131507873535 + 0.001 * 7.110996246337891
Epoch 280, val loss: 0.7202398180961609
Epoch 290, training loss: 0.25124868750572205 = 0.24415253102779388 + 0.001 * 7.0961456298828125
Epoch 290, val loss: 0.7148159742355347
Epoch 300, training loss: 0.21886125206947327 = 0.21177425980567932 + 0.001 * 7.086989879608154
Epoch 300, val loss: 0.7125983238220215
Epoch 310, training loss: 0.19081588089466095 = 0.1837499439716339 + 0.001 * 7.065937519073486
Epoch 310, val loss: 0.7133496999740601
Epoch 320, training loss: 0.16695299744606018 = 0.15990038216114044 + 0.001 * 7.052621364593506
Epoch 320, val loss: 0.7166797518730164
Epoch 330, training loss: 0.14674822986125946 = 0.13970082998275757 + 0.001 * 7.047405242919922
Epoch 330, val loss: 0.7221240997314453
Epoch 340, training loss: 0.12960955500602722 = 0.12255384027957916 + 0.001 * 7.0557074546813965
Epoch 340, val loss: 0.7291040420532227
Epoch 350, training loss: 0.11499065160751343 = 0.10795681178569794 + 0.001 * 7.033840179443359
Epoch 350, val loss: 0.7371929883956909
Epoch 360, training loss: 0.10251085460186005 = 0.09548285603523254 + 0.001 * 7.0279951095581055
Epoch 360, val loss: 0.7461140751838684
Epoch 370, training loss: 0.09180355817079544 = 0.08478300273418427 + 0.001 * 7.020554065704346
Epoch 370, val loss: 0.7556195855140686
Epoch 380, training loss: 0.0825771614909172 = 0.07555687427520752 + 0.001 * 7.020289897918701
Epoch 380, val loss: 0.7655892968177795
Epoch 390, training loss: 0.07458668202161789 = 0.06756407022476196 + 0.001 * 7.022613525390625
Epoch 390, val loss: 0.7758495807647705
Epoch 400, training loss: 0.06763055175542831 = 0.06061071902513504 + 0.001 * 7.019829273223877
Epoch 400, val loss: 0.7862370610237122
Epoch 410, training loss: 0.061554983258247375 = 0.05453880876302719 + 0.001 * 7.016176223754883
Epoch 410, val loss: 0.7966542840003967
Epoch 420, training loss: 0.05623416230082512 = 0.049218155443668365 + 0.001 * 7.016007900238037
Epoch 420, val loss: 0.8070421814918518
Epoch 430, training loss: 0.051558949053287506 = 0.04454164579510689 + 0.001 * 7.017303943634033
Epoch 430, val loss: 0.817379891872406
Epoch 440, training loss: 0.0474359355866909 = 0.04041856899857521 + 0.001 * 7.017367839813232
Epoch 440, val loss: 0.8276514410972595
Epoch 450, training loss: 0.04379110410809517 = 0.03677690029144287 + 0.001 * 7.014204502105713
Epoch 450, val loss: 0.837868332862854
Epoch 460, training loss: 0.04056448116898537 = 0.03355469927191734 + 0.001 * 7.0097808837890625
Epoch 460, val loss: 0.8480001091957092
Epoch 470, training loss: 0.03770671784877777 = 0.03069942630827427 + 0.001 * 7.007290363311768
Epoch 470, val loss: 0.8580600619316101
Epoch 480, training loss: 0.03517096862196922 = 0.028165139257907867 + 0.001 * 7.005829811096191
Epoch 480, val loss: 0.8680537343025208
Epoch 490, training loss: 0.032919399440288544 = 0.025911830365657806 + 0.001 * 7.007569789886475
Epoch 490, val loss: 0.8779130578041077
Epoch 500, training loss: 0.030914485454559326 = 0.023904182016849518 + 0.001 * 7.0103020668029785
Epoch 500, val loss: 0.8876744508743286
Epoch 510, training loss: 0.02911718562245369 = 0.022110888734459877 + 0.001 * 7.006295680999756
Epoch 510, val loss: 0.8973390460014343
Epoch 520, training loss: 0.027507945895195007 = 0.02050502598285675 + 0.001 * 7.002918720245361
Epoch 520, val loss: 0.9068540334701538
Epoch 530, training loss: 0.026061154901981354 = 0.01906302385032177 + 0.001 * 6.9981303215026855
Epoch 530, val loss: 0.9162351489067078
Epoch 540, training loss: 0.02476963959634304 = 0.017764678224921227 + 0.001 * 7.004961013793945
Epoch 540, val loss: 0.9254187345504761
Epoch 550, training loss: 0.02359110303223133 = 0.01659243367612362 + 0.001 * 6.998669147491455
Epoch 550, val loss: 0.9344624280929565
Epoch 560, training loss: 0.022534016519784927 = 0.015531052835285664 + 0.001 * 7.002964019775391
Epoch 560, val loss: 0.943297266960144
Epoch 570, training loss: 0.021564310416579247 = 0.014567594975233078 + 0.001 * 6.9967145919799805
Epoch 570, val loss: 0.9519286155700684
Epoch 580, training loss: 0.02068064548075199 = 0.013690833933651447 + 0.001 * 6.989811420440674
Epoch 580, val loss: 0.9603712558746338
Epoch 590, training loss: 0.019900046288967133 = 0.012890943326056004 + 0.001 * 7.009103298187256
Epoch 590, val loss: 0.9686166048049927
Epoch 600, training loss: 0.01914817839860916 = 0.012159480713307858 + 0.001 * 6.988697528839111
Epoch 600, val loss: 0.9766722321510315
Epoch 610, training loss: 0.018480779603123665 = 0.011489126831293106 + 0.001 * 6.991652965545654
Epoch 610, val loss: 0.9845383167266846
Epoch 620, training loss: 0.017861351370811462 = 0.01087344903498888 + 0.001 * 6.98790168762207
Epoch 620, val loss: 0.9922068119049072
Epoch 630, training loss: 0.017288099974393845 = 0.01030689012259245 + 0.001 * 6.981209754943848
Epoch 630, val loss: 0.9996975660324097
Epoch 640, training loss: 0.016766352578997612 = 0.009784418158233166 + 0.001 * 6.98193359375
Epoch 640, val loss: 1.006975769996643
Epoch 650, training loss: 0.016290917992591858 = 0.009301740676164627 + 0.001 * 6.9891767501831055
Epoch 650, val loss: 1.014073133468628
Epoch 660, training loss: 0.015835382044315338 = 0.008854967541992664 + 0.001 * 6.980414390563965
Epoch 660, val loss: 1.0210175514221191
Epoch 670, training loss: 0.015416475012898445 = 0.00844070315361023 + 0.001 * 6.975770950317383
Epoch 670, val loss: 1.0277785062789917
Epoch 680, training loss: 0.01502816192805767 = 0.00805595237761736 + 0.001 * 6.972209453582764
Epoch 680, val loss: 1.0343648195266724
Epoch 690, training loss: 0.014686666429042816 = 0.007698012050241232 + 0.001 * 6.988654613494873
Epoch 690, val loss: 1.0407978296279907
Epoch 700, training loss: 0.014346021227538586 = 0.007364487741142511 + 0.001 * 6.981533050537109
Epoch 700, val loss: 1.0470805168151855
Epoch 710, training loss: 0.014027913101017475 = 0.007053224369883537 + 0.001 * 6.974688529968262
Epoch 710, val loss: 1.0532137155532837
Epoch 720, training loss: 0.013731492683291435 = 0.00676232622936368 + 0.001 * 6.969165802001953
Epoch 720, val loss: 1.059200406074524
Epoch 730, training loss: 0.013463880866765976 = 0.006490070838481188 + 0.001 * 6.973809719085693
Epoch 730, val loss: 1.0650498867034912
Epoch 740, training loss: 0.013195931911468506 = 0.006234918721020222 + 0.001 * 6.961012840270996
Epoch 740, val loss: 1.0707614421844482
Epoch 750, training loss: 0.012986615300178528 = 0.005995445419102907 + 0.001 * 6.9911699295043945
Epoch 750, val loss: 1.0763436555862427
Epoch 760, training loss: 0.012729443609714508 = 0.005770411808043718 + 0.001 * 6.959031581878662
Epoch 760, val loss: 1.0817923545837402
Epoch 770, training loss: 0.012512186542153358 = 0.005558735225349665 + 0.001 * 6.953451633453369
Epoch 770, val loss: 1.08712637424469
Epoch 780, training loss: 0.012311261147260666 = 0.005359360482543707 + 0.001 * 6.951900005340576
Epoch 780, val loss: 1.0923361778259277
Epoch 790, training loss: 0.012136616744101048 = 0.0051713683642446995 + 0.001 * 6.965248107910156
Epoch 790, val loss: 1.097434639930725
Epoch 800, training loss: 0.011944517493247986 = 0.004993895534425974 + 0.001 * 6.950621128082275
Epoch 800, val loss: 1.1024116277694702
Epoch 810, training loss: 0.011779772117733955 = 0.004826213698834181 + 0.001 * 6.95355749130249
Epoch 810, val loss: 1.1072747707366943
Epoch 820, training loss: 0.011612072587013245 = 0.00466759642586112 + 0.001 * 6.944476127624512
Epoch 820, val loss: 1.1120420694351196
Epoch 830, training loss: 0.011458951979875565 = 0.0045174467377364635 + 0.001 * 6.94150447845459
Epoch 830, val loss: 1.1167047023773193
Epoch 840, training loss: 0.011313286609947681 = 0.004375097807496786 + 0.001 * 6.938188552856445
Epoch 840, val loss: 1.121259093284607
Epoch 850, training loss: 0.011179951019585133 = 0.004240091890096664 + 0.001 * 6.939858913421631
Epoch 850, val loss: 1.1257269382476807
Epoch 860, training loss: 0.011074542067945004 = 0.00411186134442687 + 0.001 * 6.962680339813232
Epoch 860, val loss: 1.1300978660583496
Epoch 870, training loss: 0.010933678597211838 = 0.003990056458860636 + 0.001 * 6.94362211227417
Epoch 870, val loss: 1.134372353553772
Epoch 880, training loss: 0.010830861516296864 = 0.003874157788231969 + 0.001 * 6.956703186035156
Epoch 880, val loss: 1.1385746002197266
Epoch 890, training loss: 0.010693847201764584 = 0.003763846354559064 + 0.001 * 6.930000305175781
Epoch 890, val loss: 1.1426697969436646
Epoch 900, training loss: 0.010591129772365093 = 0.0036587726790457964 + 0.001 * 6.932356834411621
Epoch 900, val loss: 1.1466944217681885
Epoch 910, training loss: 0.010491134598851204 = 0.0035585351288318634 + 0.001 * 6.9325995445251465
Epoch 910, val loss: 1.1506259441375732
Epoch 920, training loss: 0.010385153815150261 = 0.0034629441797733307 + 0.001 * 6.9222092628479
Epoch 920, val loss: 1.1544734239578247
Epoch 930, training loss: 0.010288877412676811 = 0.0033716459292918444 + 0.001 * 6.91723108291626
Epoch 930, val loss: 1.1582472324371338
Epoch 940, training loss: 0.010203332640230656 = 0.0032844131346791983 + 0.001 * 6.918919563293457
Epoch 940, val loss: 1.1619549989700317
Epoch 950, training loss: 0.01011784840375185 = 0.00320101878605783 + 0.001 * 6.9168291091918945
Epoch 950, val loss: 1.1655888557434082
Epoch 960, training loss: 0.010030371136963367 = 0.003121215384453535 + 0.001 * 6.909155368804932
Epoch 960, val loss: 1.1691457033157349
Epoch 970, training loss: 0.009977838024497032 = 0.003044831333681941 + 0.001 * 6.933006763458252
Epoch 970, val loss: 1.172632098197937
Epoch 980, training loss: 0.009884990751743317 = 0.002971622860059142 + 0.001 * 6.91336727142334
Epoch 980, val loss: 1.1760574579238892
Epoch 990, training loss: 0.00981562677770853 = 0.002901423489674926 + 0.001 * 6.91420316696167
Epoch 990, val loss: 1.1794147491455078
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9041
Flip ASR: 0.8844/225 nodes
The final ASR:0.77368, 0.25244, Accuracy:0.79753, 0.02607
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11634])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10594])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83086, 0.00462
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9438507556915283 = 1.9354768991470337 + 0.001 * 8.373846054077148
Epoch 0, val loss: 1.923460602760315
Epoch 10, training loss: 1.9338589906692505 = 1.9254852533340454 + 0.001 * 8.373774528503418
Epoch 10, val loss: 1.914133906364441
Epoch 20, training loss: 1.9212764501571655 = 1.91290283203125 + 0.001 * 8.373559951782227
Epoch 20, val loss: 1.9019651412963867
Epoch 30, training loss: 1.9032933712005615 = 1.8949202299118042 + 0.001 * 8.373137474060059
Epoch 30, val loss: 1.8843653202056885
Epoch 40, training loss: 1.876637578010559 = 1.8682652711868286 + 0.001 * 8.372268676757812
Epoch 40, val loss: 1.8586751222610474
Epoch 50, training loss: 1.8399388790130615 = 1.831568956375122 + 0.001 * 8.369969367980957
Epoch 50, val loss: 1.8258416652679443
Epoch 60, training loss: 1.7996233701705933 = 1.7912617921829224 + 0.001 * 8.361559867858887
Epoch 60, val loss: 1.7952481508255005
Epoch 70, training loss: 1.76072359085083 = 1.752409815788269 + 0.001 * 8.313729286193848
Epoch 70, val loss: 1.7681337594985962
Epoch 80, training loss: 1.7071399688720703 = 1.6992082595825195 + 0.001 * 7.931756019592285
Epoch 80, val loss: 1.724181056022644
Epoch 90, training loss: 1.6331905126571655 = 1.625462532043457 + 0.001 * 7.727983474731445
Epoch 90, val loss: 1.661609411239624
Epoch 100, training loss: 1.5405337810516357 = 1.5329532623291016 + 0.001 * 7.580547332763672
Epoch 100, val loss: 1.5867923498153687
Epoch 110, training loss: 1.4408327341079712 = 1.4333761930465698 + 0.001 * 7.456503868103027
Epoch 110, val loss: 1.5076895952224731
Epoch 120, training loss: 1.3417870998382568 = 1.3343684673309326 + 0.001 * 7.418610572814941
Epoch 120, val loss: 1.431970238685608
Epoch 130, training loss: 1.2446235418319702 = 1.2372350692749023 + 0.001 * 7.388483047485352
Epoch 130, val loss: 1.3579211235046387
Epoch 140, training loss: 1.1497650146484375 = 1.1424516439437866 + 0.001 * 7.3134074211120605
Epoch 140, val loss: 1.2863378524780273
Epoch 150, training loss: 1.0604865550994873 = 1.0532877445220947 + 0.001 * 7.198797702789307
Epoch 150, val loss: 1.2195607423782349
Epoch 160, training loss: 0.9799818396568298 = 0.9728655219078064 + 0.001 * 7.116344451904297
Epoch 160, val loss: 1.1600360870361328
Epoch 170, training loss: 0.9082026481628418 = 0.9011165499687195 + 0.001 * 7.086106777191162
Epoch 170, val loss: 1.1082054376602173
Epoch 180, training loss: 0.8428673148155212 = 0.8358001112937927 + 0.001 * 7.0672197341918945
Epoch 180, val loss: 1.061956763267517
Epoch 190, training loss: 0.7819810509681702 = 0.7749215364456177 + 0.001 * 7.059541702270508
Epoch 190, val loss: 1.0201725959777832
Epoch 200, training loss: 0.7241877317428589 = 0.7171308994293213 + 0.001 * 7.056854248046875
Epoch 200, val loss: 0.9817174673080444
Epoch 210, training loss: 0.6681213974952698 = 0.6610651016235352 + 0.001 * 7.056270122528076
Epoch 210, val loss: 0.9452127814292908
Epoch 220, training loss: 0.612632691860199 = 0.6055769324302673 + 0.001 * 7.055737495422363
Epoch 220, val loss: 0.9094431400299072
Epoch 230, training loss: 0.5576801300048828 = 0.5506252646446228 + 0.001 * 7.05483865737915
Epoch 230, val loss: 0.8743408918380737
Epoch 240, training loss: 0.5044490098953247 = 0.4973955452442169 + 0.001 * 7.0534749031066895
Epoch 240, val loss: 0.841003954410553
Epoch 250, training loss: 0.4541243612766266 = 0.447072833776474 + 0.001 * 7.0515289306640625
Epoch 250, val loss: 0.811211347579956
Epoch 260, training loss: 0.40715229511260986 = 0.4001031816005707 + 0.001 * 7.049116134643555
Epoch 260, val loss: 0.7864426374435425
Epoch 270, training loss: 0.3635426461696625 = 0.35649624466896057 + 0.001 * 7.046408653259277
Epoch 270, val loss: 0.7668505311012268
Epoch 280, training loss: 0.3230057954788208 = 0.3159620761871338 + 0.001 * 7.043709754943848
Epoch 280, val loss: 0.7516533136367798
Epoch 290, training loss: 0.28532299399375916 = 0.2782818078994751 + 0.001 * 7.041199207305908
Epoch 290, val loss: 0.7397921681404114
Epoch 300, training loss: 0.25042471289634705 = 0.24338001012802124 + 0.001 * 7.044714450836182
Epoch 300, val loss: 0.7308834195137024
Epoch 310, training loss: 0.2184685319662094 = 0.21143057942390442 + 0.001 * 7.037948131561279
Epoch 310, val loss: 0.724800705909729
Epoch 320, training loss: 0.18981002271175385 = 0.182772696018219 + 0.001 * 7.037332534790039
Epoch 320, val loss: 0.721660315990448
Epoch 330, training loss: 0.16473907232284546 = 0.1577029675245285 + 0.001 * 7.036103248596191
Epoch 330, val loss: 0.7217891216278076
Epoch 340, training loss: 0.14327968657016754 = 0.13624432682991028 + 0.001 * 7.0353569984436035
Epoch 340, val loss: 0.7252187132835388
Epoch 350, training loss: 0.12514500319957733 = 0.11811059713363647 + 0.001 * 7.034411907196045
Epoch 350, val loss: 0.7317953705787659
Epoch 360, training loss: 0.10989712178707123 = 0.10286395251750946 + 0.001 * 7.033165454864502
Epoch 360, val loss: 0.7410120368003845
Epoch 370, training loss: 0.0970650389790535 = 0.09003140777349472 + 0.001 * 7.033628463745117
Epoch 370, val loss: 0.7523952722549438
Epoch 380, training loss: 0.08621108531951904 = 0.07918239384889603 + 0.001 * 7.028689861297607
Epoch 380, val loss: 0.7653506398200989
Epoch 390, training loss: 0.07699009031057358 = 0.06995892524719238 + 0.001 * 7.031165599822998
Epoch 390, val loss: 0.7794506549835205
Epoch 400, training loss: 0.06911023706197739 = 0.06208864971995354 + 0.001 * 7.0215840339660645
Epoch 400, val loss: 0.7942955493927002
Epoch 410, training loss: 0.062369078397750854 = 0.05534582585096359 + 0.001 * 7.023252010345459
Epoch 410, val loss: 0.8095564842224121
Epoch 420, training loss: 0.05655703321099281 = 0.049549300223588943 + 0.001 * 7.007731914520264
Epoch 420, val loss: 0.8249819874763489
Epoch 430, training loss: 0.05154644325375557 = 0.04454673454165459 + 0.001 * 6.999709606170654
Epoch 430, val loss: 0.8404904007911682
Epoch 440, training loss: 0.04719177633523941 = 0.040212586522102356 + 0.001 * 6.979188442230225
Epoch 440, val loss: 0.8558483719825745
Epoch 450, training loss: 0.04341905936598778 = 0.03644203767180443 + 0.001 * 6.977022171020508
Epoch 450, val loss: 0.8710134625434875
Epoch 460, training loss: 0.040116891264915466 = 0.03314846381545067 + 0.001 * 6.9684271812438965
Epoch 460, val loss: 0.8859291672706604
Epoch 470, training loss: 0.03723013028502464 = 0.030260752886533737 + 0.001 * 6.9693779945373535
Epoch 470, val loss: 0.9004948735237122
Epoch 480, training loss: 0.03466545045375824 = 0.02771780639886856 + 0.001 * 6.947644233703613
Epoch 480, val loss: 0.9147951006889343
Epoch 490, training loss: 0.032424330711364746 = 0.025469930842518806 + 0.001 * 6.954400062561035
Epoch 490, val loss: 0.9287090301513672
Epoch 500, training loss: 0.030429484322667122 = 0.023475585505366325 + 0.001 * 6.953898906707764
Epoch 500, val loss: 0.942291796207428
Epoch 510, training loss: 0.02865549363195896 = 0.02169979363679886 + 0.001 * 6.9556989669799805
Epoch 510, val loss: 0.9555109143257141
Epoch 520, training loss: 0.027058742940425873 = 0.02011265978217125 + 0.001 * 6.946083068847656
Epoch 520, val loss: 0.9683372378349304
Epoch 530, training loss: 0.025634173303842545 = 0.018689366057515144 + 0.001 * 6.944807529449463
Epoch 530, val loss: 0.9808720350265503
Epoch 540, training loss: 0.024349279701709747 = 0.01740914396941662 + 0.001 * 6.940134525299072
Epoch 540, val loss: 0.9930327534675598
Epoch 550, training loss: 0.023187795653939247 = 0.016254214569926262 + 0.001 * 6.9335808753967285
Epoch 550, val loss: 1.0048232078552246
Epoch 560, training loss: 0.022148914635181427 = 0.015209252014756203 + 0.001 * 6.939663410186768
Epoch 560, val loss: 1.0163195133209229
Epoch 570, training loss: 0.021195031702518463 = 0.014261323027312756 + 0.001 * 6.933708190917969
Epoch 570, val loss: 1.0274907350540161
Epoch 580, training loss: 0.020330868661403656 = 0.013399126008152962 + 0.001 * 6.931741714477539
Epoch 580, val loss: 1.0383379459381104
Epoch 590, training loss: 0.01954258792102337 = 0.012612937949597836 + 0.001 * 6.929649829864502
Epoch 590, val loss: 1.0488630533218384
Epoch 600, training loss: 0.018820298835635185 = 0.01189438533037901 + 0.001 * 6.925913333892822
Epoch 600, val loss: 1.059155821800232
Epoch 610, training loss: 0.018162991851568222 = 0.011236031539738178 + 0.001 * 6.926959991455078
Epoch 610, val loss: 1.0691678524017334
Epoch 620, training loss: 0.017553040757775307 = 0.010631276294589043 + 0.001 * 6.921763896942139
Epoch 620, val loss: 1.078892707824707
Epoch 630, training loss: 0.01700160652399063 = 0.010073664598166943 + 0.001 * 6.927942276000977
Epoch 630, val loss: 1.0883489847183228
Epoch 640, training loss: 0.016478879377245903 = 0.009557553566992283 + 0.001 * 6.921325206756592
Epoch 640, val loss: 1.0976097583770752
Epoch 650, training loss: 0.016004640609025955 = 0.009077681228518486 + 0.001 * 6.926958084106445
Epoch 650, val loss: 1.1066224575042725
Epoch 660, training loss: 0.015553697943687439 = 0.008629993535578251 + 0.001 * 6.923703670501709
Epoch 660, val loss: 1.1154723167419434
Epoch 670, training loss: 0.01513099204748869 = 0.008211467415094376 + 0.001 * 6.919524192810059
Epoch 670, val loss: 1.1240942478179932
Epoch 680, training loss: 0.014746302738785744 = 0.007819894701242447 + 0.001 * 6.926407337188721
Epoch 680, val loss: 1.1325511932373047
Epoch 690, training loss: 0.014367818832397461 = 0.007453510537743568 + 0.001 * 6.914307594299316
Epoch 690, val loss: 1.140849232673645
Epoch 700, training loss: 0.014029625803232193 = 0.0071105812676250935 + 0.001 * 6.919044017791748
Epoch 700, val loss: 1.1489821672439575
Epoch 710, training loss: 0.013702575117349625 = 0.0067896246910095215 + 0.001 * 6.912950038909912
Epoch 710, val loss: 1.1569246053695679
Epoch 720, training loss: 0.013394635170698166 = 0.006489162798970938 + 0.001 * 6.905472278594971
Epoch 720, val loss: 1.1647472381591797
Epoch 730, training loss: 0.01311347633600235 = 0.0062077841721475124 + 0.001 * 6.905691146850586
Epoch 730, val loss: 1.17238187789917
Epoch 740, training loss: 0.012851733714342117 = 0.005944139324128628 + 0.001 * 6.907594680786133
Epoch 740, val loss: 1.1798629760742188
Epoch 750, training loss: 0.012610822916030884 = 0.005696941167116165 + 0.001 * 6.913880825042725
Epoch 750, val loss: 1.1871914863586426
Epoch 760, training loss: 0.012367989867925644 = 0.005465047433972359 + 0.001 * 6.902942180633545
Epoch 760, val loss: 1.1943657398223877
Epoch 770, training loss: 0.012146811932325363 = 0.005247375927865505 + 0.001 * 6.899435520172119
Epoch 770, val loss: 1.2013912200927734
Epoch 780, training loss: 0.011959748342633247 = 0.0050428458489477634 + 0.001 * 6.916902542114258
Epoch 780, val loss: 1.2082328796386719
Epoch 790, training loss: 0.011751258745789528 = 0.004850541241466999 + 0.001 * 6.900717735290527
Epoch 790, val loss: 1.2149748802185059
Epoch 800, training loss: 0.011574964970350266 = 0.004669543355703354 + 0.001 * 6.905420780181885
Epoch 800, val loss: 1.2215443849563599
Epoch 810, training loss: 0.011395834386348724 = 0.004499050322920084 + 0.001 * 6.89678430557251
Epoch 810, val loss: 1.227986454963684
Epoch 820, training loss: 0.011226028203964233 = 0.004338293336331844 + 0.001 * 6.887734889984131
Epoch 820, val loss: 1.234320044517517
Epoch 830, training loss: 0.011095335707068443 = 0.004186590202152729 + 0.001 * 6.908744812011719
Epoch 830, val loss: 1.2404868602752686
Epoch 840, training loss: 0.010926082730293274 = 0.004043326713144779 + 0.001 * 6.882755756378174
Epoch 840, val loss: 1.2465721368789673
Epoch 850, training loss: 0.01081017404794693 = 0.003907917533069849 + 0.001 * 6.902256011962891
Epoch 850, val loss: 1.2525348663330078
Epoch 860, training loss: 0.010667041875422001 = 0.003779863240197301 + 0.001 * 6.887178421020508
Epoch 860, val loss: 1.2583357095718384
Epoch 870, training loss: 0.010536939837038517 = 0.0036585754714906216 + 0.001 * 6.878364086151123
Epoch 870, val loss: 1.264023780822754
Epoch 880, training loss: 0.010436790995299816 = 0.003543609520420432 + 0.001 * 6.893181324005127
Epoch 880, val loss: 1.2696353197097778
Epoch 890, training loss: 0.010312213562428951 = 0.003434565616771579 + 0.001 * 6.877647876739502
Epoch 890, val loss: 1.2750959396362305
Epoch 900, training loss: 0.01020967960357666 = 0.0033310316503047943 + 0.001 * 6.878647327423096
Epoch 900, val loss: 1.280447006225586
Epoch 910, training loss: 0.010115348733961582 = 0.003232678398489952 + 0.001 * 6.882669925689697
Epoch 910, val loss: 1.2857081890106201
Epoch 920, training loss: 0.010018201544880867 = 0.0031391610391438007 + 0.001 * 6.879039764404297
Epoch 920, val loss: 1.2908575534820557
Epoch 930, training loss: 0.009925652295351028 = 0.0030502057634294033 + 0.001 * 6.87544584274292
Epoch 930, val loss: 1.2958998680114746
Epoch 940, training loss: 0.009821655228734016 = 0.002965482184663415 + 0.001 * 6.856172561645508
Epoch 940, val loss: 1.300845980644226
Epoch 950, training loss: 0.009746885858476162 = 0.0028847402427345514 + 0.001 * 6.862144947052002
Epoch 950, val loss: 1.3056952953338623
Epoch 960, training loss: 0.009663810953497887 = 0.002807735465466976 + 0.001 * 6.856075286865234
Epoch 960, val loss: 1.3104318380355835
Epoch 970, training loss: 0.009590938687324524 = 0.0027342766989022493 + 0.001 * 6.856661796569824
Epoch 970, val loss: 1.3151044845581055
Epoch 980, training loss: 0.009519373998045921 = 0.0026641404256224632 + 0.001 * 6.8552327156066895
Epoch 980, val loss: 1.3196614980697632
Epoch 990, training loss: 0.009462278336286545 = 0.002597145037725568 + 0.001 * 6.865133285522461
Epoch 990, val loss: 1.324167013168335
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.6089
Flip ASR: 0.5333/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9444807767868042 = 1.9361069202423096 + 0.001 * 8.373862266540527
Epoch 0, val loss: 1.9290716648101807
Epoch 10, training loss: 1.9345427751541138 = 1.9261689186096191 + 0.001 * 8.37381362915039
Epoch 10, val loss: 1.9191057682037354
Epoch 20, training loss: 1.9225722551345825 = 1.914198637008667 + 0.001 * 8.373639106750488
Epoch 20, val loss: 1.9065970182418823
Epoch 30, training loss: 1.9058597087860107 = 1.8974864482879639 + 0.001 * 8.373270034790039
Epoch 30, val loss: 1.8886103630065918
Epoch 40, training loss: 1.8811085224151611 = 1.8727360963821411 + 0.001 * 8.372426986694336
Epoch 40, val loss: 1.862006664276123
Epoch 50, training loss: 1.846254587173462 = 1.8378843069076538 + 0.001 * 8.370301246643066
Epoch 50, val loss: 1.8259172439575195
Epoch 60, training loss: 1.805476427078247 = 1.7971131801605225 + 0.001 * 8.363202095031738
Epoch 60, val loss: 1.7878139019012451
Epoch 70, training loss: 1.7661420106887817 = 1.7578120231628418 + 0.001 * 8.330044746398926
Epoch 70, val loss: 1.7558916807174683
Epoch 80, training loss: 1.7152842283248901 = 1.7072314023971558 + 0.001 * 8.052852630615234
Epoch 80, val loss: 1.7130573987960815
Epoch 90, training loss: 1.6439704895019531 = 1.6361563205718994 + 0.001 * 7.81417179107666
Epoch 90, val loss: 1.6513161659240723
Epoch 100, training loss: 1.5524213314056396 = 1.5446568727493286 + 0.001 * 7.764449596405029
Epoch 100, val loss: 1.5749003887176514
Epoch 110, training loss: 1.4532766342163086 = 1.4455811977386475 + 0.001 * 7.695450305938721
Epoch 110, val loss: 1.4948514699935913
Epoch 120, training loss: 1.3572897911071777 = 1.3497744798660278 + 0.001 * 7.515350818634033
Epoch 120, val loss: 1.421064853668213
Epoch 130, training loss: 1.266899824142456 = 1.2595500946044922 + 0.001 * 7.3497138023376465
Epoch 130, val loss: 1.3544808626174927
Epoch 140, training loss: 1.1822271347045898 = 1.174940586090088 + 0.001 * 7.286539554595947
Epoch 140, val loss: 1.2942758798599243
Epoch 150, training loss: 1.1033852100372314 = 1.0961886644363403 + 0.001 * 7.196537494659424
Epoch 150, val loss: 1.2394132614135742
Epoch 160, training loss: 1.0312931537628174 = 1.0241636037826538 + 0.001 * 7.129489898681641
Epoch 160, val loss: 1.1902977228164673
Epoch 170, training loss: 0.966315746307373 = 0.959221601486206 + 0.001 * 7.0941290855407715
Epoch 170, val loss: 1.1474230289459229
Epoch 180, training loss: 0.9071334004402161 = 0.9000661373138428 + 0.001 * 7.067253112792969
Epoch 180, val loss: 1.1089966297149658
Epoch 190, training loss: 0.8503819108009338 = 0.8433434367179871 + 0.001 * 7.038463115692139
Epoch 190, val loss: 1.0722626447677612
Epoch 200, training loss: 0.7922471165657043 = 0.7852327227592468 + 0.001 * 7.014392852783203
Epoch 200, val loss: 1.034239649772644
Epoch 210, training loss: 0.730190634727478 = 0.7231862545013428 + 0.001 * 7.004391670227051
Epoch 210, val loss: 0.9927515983581543
Epoch 220, training loss: 0.66403728723526 = 0.6570358872413635 + 0.001 * 7.001406669616699
Epoch 220, val loss: 0.9476510286331177
Epoch 230, training loss: 0.5962621569633484 = 0.5892622470855713 + 0.001 * 6.999906063079834
Epoch 230, val loss: 0.9003722071647644
Epoch 240, training loss: 0.5307856798171997 = 0.5237860679626465 + 0.001 * 6.999589443206787
Epoch 240, val loss: 0.8550605177879333
Epoch 250, training loss: 0.470791757106781 = 0.46379292011260986 + 0.001 * 6.998850345611572
Epoch 250, val loss: 0.8154839277267456
Epoch 260, training loss: 0.41730526089668274 = 0.410307377576828 + 0.001 * 6.9978861808776855
Epoch 260, val loss: 0.7834916114807129
Epoch 270, training loss: 0.36955174803733826 = 0.3625549376010895 + 0.001 * 6.996809482574463
Epoch 270, val loss: 0.7591863870620728
Epoch 280, training loss: 0.3264320194721222 = 0.3194364607334137 + 0.001 * 6.995556354522705
Epoch 280, val loss: 0.7415422201156616
Epoch 290, training loss: 0.28721460700035095 = 0.28022056818008423 + 0.001 * 6.994034290313721
Epoch 290, val loss: 0.729412853717804
Epoch 300, training loss: 0.25161653757095337 = 0.24462459981441498 + 0.001 * 6.991935729980469
Epoch 300, val loss: 0.7220956087112427
Epoch 310, training loss: 0.21962551772594452 = 0.21263593435287476 + 0.001 * 6.9895853996276855
Epoch 310, val loss: 0.7193354964256287
Epoch 320, training loss: 0.19132910668849945 = 0.18434366583824158 + 0.001 * 6.985434055328369
Epoch 320, val loss: 0.7206944227218628
Epoch 330, training loss: 0.16678424179553986 = 0.15980248153209686 + 0.001 * 6.981762886047363
Epoch 330, val loss: 0.7257772088050842
Epoch 340, training loss: 0.1457090526819229 = 0.1387331336736679 + 0.001 * 6.975916862487793
Epoch 340, val loss: 0.7339024543762207
Epoch 350, training loss: 0.127732053399086 = 0.12075652182102203 + 0.001 * 6.97552490234375
Epoch 350, val loss: 0.7442317605018616
Epoch 360, training loss: 0.11235658079385757 = 0.10539289563894272 + 0.001 * 6.963684558868408
Epoch 360, val loss: 0.7560100555419922
Epoch 370, training loss: 0.09921135008335114 = 0.09225773811340332 + 0.001 * 6.95361328125
Epoch 370, val loss: 0.7686706185340881
Epoch 380, training loss: 0.08801648020744324 = 0.08103658258914948 + 0.001 * 6.9798970222473145
Epoch 380, val loss: 0.7821265459060669
Epoch 390, training loss: 0.07836893945932388 = 0.07142791897058487 + 0.001 * 6.94102144241333
Epoch 390, val loss: 0.7959246635437012
Epoch 400, training loss: 0.07010343670845032 = 0.0631740465760231 + 0.001 * 6.929388999938965
Epoch 400, val loss: 0.8099145889282227
Epoch 410, training loss: 0.06300540268421173 = 0.056081660091876984 + 0.001 * 6.923741340637207
Epoch 410, val loss: 0.824245274066925
Epoch 420, training loss: 0.0568992905318737 = 0.04998493567109108 + 0.001 * 6.914355278015137
Epoch 420, val loss: 0.8383466005325317
Epoch 430, training loss: 0.05164996534585953 = 0.04473212733864784 + 0.001 * 6.9178361892700195
Epoch 430, val loss: 0.8523148894309998
Epoch 440, training loss: 0.047112274914979935 = 0.040203601121902466 + 0.001 * 6.908671855926514
Epoch 440, val loss: 0.8662900328636169
Epoch 450, training loss: 0.04318646341562271 = 0.03628239780664444 + 0.001 * 6.90406608581543
Epoch 450, val loss: 0.8800579905509949
Epoch 460, training loss: 0.03978818655014038 = 0.03287375345826149 + 0.001 * 6.914432048797607
Epoch 460, val loss: 0.8934370279312134
Epoch 470, training loss: 0.0368056520819664 = 0.029898734763264656 + 0.001 * 6.906917572021484
Epoch 470, val loss: 0.9066861271858215
Epoch 480, training loss: 0.03418821096420288 = 0.027283789590001106 + 0.001 * 6.904419898986816
Epoch 480, val loss: 0.9195683002471924
Epoch 490, training loss: 0.03187059238553047 = 0.024970490485429764 + 0.001 * 6.9001030921936035
Epoch 490, val loss: 0.9320494532585144
Epoch 500, training loss: 0.029834337532520294 = 0.02292829193174839 + 0.001 * 6.9060444831848145
Epoch 500, val loss: 0.9441124796867371
Epoch 510, training loss: 0.028016604483127594 = 0.02111683040857315 + 0.001 * 6.899773120880127
Epoch 510, val loss: 0.9561829566955566
Epoch 520, training loss: 0.02641100250184536 = 0.01951347105205059 + 0.001 * 6.897530555725098
Epoch 520, val loss: 0.9677473306655884
Epoch 530, training loss: 0.024986352771520615 = 0.018094439059495926 + 0.001 * 6.891912937164307
Epoch 530, val loss: 0.9790698289871216
Epoch 540, training loss: 0.02371932938694954 = 0.016825789585709572 + 0.001 * 6.893538951873779
Epoch 540, val loss: 0.9900561571121216
Epoch 550, training loss: 0.022579621523618698 = 0.01568671129643917 + 0.001 * 6.892910957336426
Epoch 550, val loss: 1.0005983114242554
Epoch 560, training loss: 0.021549668163061142 = 0.014660426415503025 + 0.001 * 6.8892412185668945
Epoch 560, val loss: 1.0109643936157227
Epoch 570, training loss: 0.02062615007162094 = 0.01373300887644291 + 0.001 * 6.8931403160095215
Epoch 570, val loss: 1.0210107564926147
Epoch 580, training loss: 0.019777771085500717 = 0.012892202474176884 + 0.001 * 6.885567665100098
Epoch 580, val loss: 1.0307539701461792
Epoch 590, training loss: 0.019012395292520523 = 0.012127497233450413 + 0.001 * 6.884896755218506
Epoch 590, val loss: 1.040253758430481
Epoch 600, training loss: 0.018320051953196526 = 0.011430537328124046 + 0.001 * 6.889514446258545
Epoch 600, val loss: 1.0494472980499268
Epoch 610, training loss: 0.017673904076218605 = 0.010793349705636501 + 0.001 * 6.880554676055908
Epoch 610, val loss: 1.0582882165908813
Epoch 620, training loss: 0.017087016254663467 = 0.010209336876869202 + 0.001 * 6.877678394317627
Epoch 620, val loss: 1.067023515701294
Epoch 630, training loss: 0.016544144600629807 = 0.009672691114246845 + 0.001 * 6.871453285217285
Epoch 630, val loss: 1.075461983680725
Epoch 640, training loss: 0.016043908894062042 = 0.009178855456411839 + 0.001 * 6.865052700042725
Epoch 640, val loss: 1.0836392641067505
Epoch 650, training loss: 0.015613645315170288 = 0.008723322302103043 + 0.001 * 6.890322208404541
Epoch 650, val loss: 1.0916640758514404
Epoch 660, training loss: 0.015163719654083252 = 0.008302169851958752 + 0.001 * 6.861548900604248
Epoch 660, val loss: 1.0994340181350708
Epoch 670, training loss: 0.014774134382605553 = 0.007912008091807365 + 0.001 * 6.862125396728516
Epoch 670, val loss: 1.1069835424423218
Epoch 680, training loss: 0.01442672498524189 = 0.007550030946731567 + 0.001 * 6.876693248748779
Epoch 680, val loss: 1.1143971681594849
Epoch 690, training loss: 0.014091596007347107 = 0.0072133042849600315 + 0.001 * 6.878292083740234
Epoch 690, val loss: 1.121564269065857
Epoch 700, training loss: 0.013758931308984756 = 0.006899612955749035 + 0.001 * 6.859318256378174
Epoch 700, val loss: 1.1285760402679443
Epoch 710, training loss: 0.013454333879053593 = 0.006607165094465017 + 0.001 * 6.847168445587158
Epoch 710, val loss: 1.1353663206100464
Epoch 720, training loss: 0.013228373602032661 = 0.006334313191473484 + 0.001 * 6.894060134887695
Epoch 720, val loss: 1.1420432329177856
Epoch 730, training loss: 0.012943156063556671 = 0.006079269107431173 + 0.001 * 6.863887310028076
Epoch 730, val loss: 1.1485121250152588
Epoch 740, training loss: 0.012683370150625706 = 0.0058404989540576935 + 0.001 * 6.842870712280273
Epoch 740, val loss: 1.1548535823822021
Epoch 750, training loss: 0.012474190443754196 = 0.005616652313619852 + 0.001 * 6.857537269592285
Epoch 750, val loss: 1.1610746383666992
Epoch 760, training loss: 0.012247583828866482 = 0.005406579002737999 + 0.001 * 6.841004371643066
Epoch 760, val loss: 1.1670764684677124
Epoch 770, training loss: 0.012060333043336868 = 0.005209045484662056 + 0.001 * 6.851287364959717
Epoch 770, val loss: 1.1730470657348633
Epoch 780, training loss: 0.011868512257933617 = 0.0050230626948177814 + 0.001 * 6.845449447631836
Epoch 780, val loss: 1.1787389516830444
Epoch 790, training loss: 0.011676294729113579 = 0.004847689066082239 + 0.001 * 6.828604698181152
Epoch 790, val loss: 1.1843791007995605
Epoch 800, training loss: 0.011513122357428074 = 0.004682233091443777 + 0.001 * 6.830888748168945
Epoch 800, val loss: 1.1898837089538574
Epoch 810, training loss: 0.011367679573595524 = 0.004525934346020222 + 0.001 * 6.841744899749756
Epoch 810, val loss: 1.1952561140060425
Epoch 820, training loss: 0.011208862066268921 = 0.004378101322799921 + 0.001 * 6.830760478973389
Epoch 820, val loss: 1.2005106210708618
Epoch 830, training loss: 0.01108465250581503 = 0.004238094203174114 + 0.001 * 6.846558094024658
Epoch 830, val loss: 1.205702543258667
Epoch 840, training loss: 0.010929342359304428 = 0.0041052596643567085 + 0.001 * 6.824082374572754
Epoch 840, val loss: 1.2107579708099365
Epoch 850, training loss: 0.010807126760482788 = 0.003978715278208256 + 0.001 * 6.828410625457764
Epoch 850, val loss: 1.2156858444213867
Epoch 860, training loss: 0.010672986507415771 = 0.0038574764039367437 + 0.001 * 6.81550931930542
Epoch 860, val loss: 1.2205768823623657
Epoch 870, training loss: 0.010555940680205822 = 0.0037405923940241337 + 0.001 * 6.815348148345947
Epoch 870, val loss: 1.2254571914672852
Epoch 880, training loss: 0.010446283966302872 = 0.003627227619290352 + 0.001 * 6.819056034088135
Epoch 880, val loss: 1.2302459478378296
Epoch 890, training loss: 0.010390980169177055 = 0.0035171685740351677 + 0.001 * 6.873810768127441
Epoch 890, val loss: 1.2349879741668701
Epoch 900, training loss: 0.0102284150198102 = 0.003410489298403263 + 0.001 * 6.817925453186035
Epoch 900, val loss: 1.2398020029067993
Epoch 910, training loss: 0.010130775161087513 = 0.0033072957303375006 + 0.001 * 6.823478698730469
Epoch 910, val loss: 1.244430661201477
Epoch 920, training loss: 0.010031810961663723 = 0.0032077638898044825 + 0.001 * 6.824047088623047
Epoch 920, val loss: 1.2491140365600586
Epoch 930, training loss: 0.009930828586220741 = 0.0031119303312152624 + 0.001 * 6.818897724151611
Epoch 930, val loss: 1.2537084817886353
Epoch 940, training loss: 0.00982414186000824 = 0.0030197976157069206 + 0.001 * 6.8043437004089355
Epoch 940, val loss: 1.2582201957702637
Epoch 950, training loss: 0.009753303602337837 = 0.00293143093585968 + 0.001 * 6.821872711181641
Epoch 950, val loss: 1.2626575231552124
Epoch 960, training loss: 0.009676591493189335 = 0.0028467741794884205 + 0.001 * 6.829816818237305
Epoch 960, val loss: 1.2670509815216064
Epoch 970, training loss: 0.009581529535353184 = 0.0027658066246658564 + 0.001 * 6.815722942352295
Epoch 970, val loss: 1.2713263034820557
Epoch 980, training loss: 0.009481039829552174 = 0.0026883219834417105 + 0.001 * 6.792717456817627
Epoch 980, val loss: 1.2755600214004517
Epoch 990, training loss: 0.009428845718502998 = 0.0026142746210098267 + 0.001 * 6.814571380615234
Epoch 990, val loss: 1.2796058654785156
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.4391
Flip ASR: 0.3911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9548182487487793 = 1.9464443922042847 + 0.001 * 8.373878479003906
Epoch 0, val loss: 1.9535633325576782
Epoch 10, training loss: 1.9445750713348389 = 1.9362012147903442 + 0.001 * 8.373797416687012
Epoch 10, val loss: 1.942936897277832
Epoch 20, training loss: 1.931933879852295 = 1.9235602617263794 + 0.001 * 8.373600006103516
Epoch 20, val loss: 1.9295258522033691
Epoch 30, training loss: 1.9143714904785156 = 1.9059982299804688 + 0.001 * 8.373249053955078
Epoch 30, val loss: 1.9107873439788818
Epoch 40, training loss: 1.8886009454727173 = 1.8802282810211182 + 0.001 * 8.37261962890625
Epoch 40, val loss: 1.8836922645568848
Epoch 50, training loss: 1.8523321151733398 = 1.8439608812332153 + 0.001 * 8.371217727661133
Epoch 50, val loss: 1.8469501733779907
Epoch 60, training loss: 1.8096442222595215 = 1.8012771606445312 + 0.001 * 8.367072105407715
Epoch 60, val loss: 1.8070710897445679
Epoch 70, training loss: 1.7701188325881958 = 1.761769413948059 + 0.001 * 8.349430084228516
Epoch 70, val loss: 1.7734345197677612
Epoch 80, training loss: 1.7229604721069336 = 1.7147380113601685 + 0.001 * 8.222500801086426
Epoch 80, val loss: 1.7327924966812134
Epoch 90, training loss: 1.657395839691162 = 1.6496018171310425 + 0.001 * 7.794068813323975
Epoch 90, val loss: 1.6774365901947021
Epoch 100, training loss: 1.569970965385437 = 1.5623689889907837 + 0.001 * 7.602026462554932
Epoch 100, val loss: 1.6038137674331665
Epoch 110, training loss: 1.4638923406600952 = 1.4564883708953857 + 0.001 * 7.403954029083252
Epoch 110, val loss: 1.5171105861663818
Epoch 120, training loss: 1.3507267236709595 = 1.3434152603149414 + 0.001 * 7.311476707458496
Epoch 120, val loss: 1.425411581993103
Epoch 130, training loss: 1.2391362190246582 = 1.2318931818008423 + 0.001 * 7.243086814880371
Epoch 130, val loss: 1.3351142406463623
Epoch 140, training loss: 1.1342134475708008 = 1.127017617225647 + 0.001 * 7.195871353149414
Epoch 140, val loss: 1.2510737180709839
Epoch 150, training loss: 1.0385268926620483 = 1.031346082687378 + 0.001 * 7.180792331695557
Epoch 150, val loss: 1.1750969886779785
Epoch 160, training loss: 0.9523754715919495 = 0.9452033042907715 + 0.001 * 7.172170639038086
Epoch 160, val loss: 1.1081182956695557
Epoch 170, training loss: 0.8741092085838318 = 0.866951048374176 + 0.001 * 7.15816068649292
Epoch 170, val loss: 1.0485268831253052
Epoch 180, training loss: 0.8021814227104187 = 0.7950419783592224 + 0.001 * 7.1394429206848145
Epoch 180, val loss: 0.9943836331367493
Epoch 190, training loss: 0.7360178828239441 = 0.7289032340049744 + 0.001 * 7.114638328552246
Epoch 190, val loss: 0.9453513622283936
Epoch 200, training loss: 0.6749505996704102 = 0.6678698658943176 + 0.001 * 7.080738067626953
Epoch 200, val loss: 0.9010978937149048
Epoch 210, training loss: 0.617667555809021 = 0.6106245517730713 + 0.001 * 7.043001174926758
Epoch 210, val loss: 0.8609722852706909
Epoch 220, training loss: 0.5626150965690613 = 0.5555957555770874 + 0.001 * 7.019321918487549
Epoch 220, val loss: 0.8242624998092651
Epoch 230, training loss: 0.5087743997573853 = 0.5017614960670471 + 0.001 * 7.012929916381836
Epoch 230, val loss: 0.7905770540237427
Epoch 240, training loss: 0.4560757279396057 = 0.4490639269351959 + 0.001 * 7.011809825897217
Epoch 240, val loss: 0.7600188851356506
Epoch 250, training loss: 0.405082106590271 = 0.3980713486671448 + 0.001 * 7.010752201080322
Epoch 250, val loss: 0.7326897382736206
Epoch 260, training loss: 0.3565748333930969 = 0.3495643734931946 + 0.001 * 7.010467529296875
Epoch 260, val loss: 0.7086317539215088
Epoch 270, training loss: 0.3113890588283539 = 0.3043783903121948 + 0.001 * 7.010654926300049
Epoch 270, val loss: 0.6877911686897278
Epoch 280, training loss: 0.2702755630016327 = 0.26326456665992737 + 0.001 * 7.0110063552856445
Epoch 280, val loss: 0.6699451208114624
Epoch 290, training loss: 0.23369215428829193 = 0.22668077051639557 + 0.001 * 7.011378765106201
Epoch 290, val loss: 0.6552413702011108
Epoch 300, training loss: 0.20177152752876282 = 0.19475972652435303 + 0.001 * 7.011797904968262
Epoch 300, val loss: 0.6438717842102051
Epoch 310, training loss: 0.17441481351852417 = 0.16739746928215027 + 0.001 * 7.017343997955322
Epoch 310, val loss: 0.6358442306518555
Epoch 320, training loss: 0.15125523507595062 = 0.14424218237400055 + 0.001 * 7.013049125671387
Epoch 320, val loss: 0.630976676940918
Epoch 330, training loss: 0.1317831575870514 = 0.1247696503996849 + 0.001 * 7.013504981994629
Epoch 330, val loss: 0.6290050148963928
Epoch 340, training loss: 0.1154194101691246 = 0.10840549319982529 + 0.001 * 7.013914585113525
Epoch 340, val loss: 0.6296048760414124
Epoch 350, training loss: 0.10163109004497528 = 0.0946173369884491 + 0.001 * 7.013750076293945
Epoch 350, val loss: 0.6323864459991455
Epoch 360, training loss: 0.08996506780385971 = 0.08295165747404099 + 0.001 * 7.013407230377197
Epoch 360, val loss: 0.637017548084259
Epoch 370, training loss: 0.08005326986312866 = 0.07303818315267563 + 0.001 * 7.015089511871338
Epoch 370, val loss: 0.6431077122688293
Epoch 380, training loss: 0.07159685343503952 = 0.06458219140768051 + 0.001 * 7.014658451080322
Epoch 380, val loss: 0.6502876281738281
Epoch 390, training loss: 0.0643581748008728 = 0.0573449581861496 + 0.001 * 7.013219356536865
Epoch 390, val loss: 0.6582744717597961
Epoch 400, training loss: 0.058147452771663666 = 0.051129624247550964 + 0.001 * 7.017826557159424
Epoch 400, val loss: 0.6668091416358948
Epoch 410, training loss: 0.05278823524713516 = 0.04577566683292389 + 0.001 * 7.012569904327393
Epoch 410, val loss: 0.675686776638031
Epoch 420, training loss: 0.04815949499607086 = 0.04114992916584015 + 0.001 * 7.009565353393555
Epoch 420, val loss: 0.6847832202911377
Epoch 430, training loss: 0.04414717108011246 = 0.037139054387807846 + 0.001 * 7.008114814758301
Epoch 430, val loss: 0.6940059661865234
Epoch 440, training loss: 0.04065500572323799 = 0.03364868834614754 + 0.001 * 7.006317138671875
Epoch 440, val loss: 0.703263521194458
Epoch 450, training loss: 0.03760738670825958 = 0.030600400641560555 + 0.001 * 7.006985664367676
Epoch 450, val loss: 0.7124928832054138
Epoch 460, training loss: 0.03493421897292137 = 0.02792791835963726 + 0.001 * 7.006299018859863
Epoch 460, val loss: 0.7216529250144958
Epoch 470, training loss: 0.032575059682130814 = 0.025576569139957428 + 0.001 * 6.998488903045654
Epoch 470, val loss: 0.7306962609291077
Epoch 480, training loss: 0.03050166182219982 = 0.023500092327594757 + 0.001 * 7.001569747924805
Epoch 480, val loss: 0.7396226525306702
Epoch 490, training loss: 0.028649114072322845 = 0.02165931463241577 + 0.001 * 6.989799976348877
Epoch 490, val loss: 0.7483983635902405
Epoch 500, training loss: 0.027010705322027206 = 0.02002093754708767 + 0.001 * 6.989767551422119
Epoch 500, val loss: 0.7570220232009888
Epoch 510, training loss: 0.025538746267557144 = 0.01855328306555748 + 0.001 * 6.985461711883545
Epoch 510, val loss: 0.7655454874038696
Epoch 520, training loss: 0.02420520782470703 = 0.017229506745934486 + 0.001 * 6.9756999015808105
Epoch 520, val loss: 0.7739899754524231
Epoch 530, training loss: 0.023014439269900322 = 0.01603061519563198 + 0.001 * 6.983824253082275
Epoch 530, val loss: 0.7823790311813354
Epoch 540, training loss: 0.021906182169914246 = 0.014942851848900318 + 0.001 * 6.963330268859863
Epoch 540, val loss: 0.7906858921051025
Epoch 550, training loss: 0.020911991596221924 = 0.01395475398749113 + 0.001 * 6.957237243652344
Epoch 550, val loss: 0.7989070415496826
Epoch 560, training loss: 0.020004434511065483 = 0.013056241907179356 + 0.001 * 6.948192596435547
Epoch 560, val loss: 0.8070192933082581
Epoch 570, training loss: 0.019213004037737846 = 0.012238197028636932 + 0.001 * 6.974807262420654
Epoch 570, val loss: 0.8150213956832886
Epoch 580, training loss: 0.018439054489135742 = 0.011492658406496048 + 0.001 * 6.946394920349121
Epoch 580, val loss: 0.8228791356086731
Epoch 590, training loss: 0.017741752788424492 = 0.0108119435608387 + 0.001 * 6.929809093475342
Epoch 590, val loss: 0.8306005001068115
Epoch 600, training loss: 0.01713753305375576 = 0.010189306922256947 + 0.001 * 6.948226451873779
Epoch 600, val loss: 0.8381878137588501
Epoch 610, training loss: 0.01653612032532692 = 0.009618905372917652 + 0.001 * 6.917214393615723
Epoch 610, val loss: 0.8456093072891235
Epoch 620, training loss: 0.016018657013773918 = 0.009095494635403156 + 0.001 * 6.92316198348999
Epoch 620, val loss: 0.8528745174407959
Epoch 630, training loss: 0.015522071160376072 = 0.00861438736319542 + 0.001 * 6.907683372497559
Epoch 630, val loss: 0.8599998354911804
Epoch 640, training loss: 0.015075131319463253 = 0.008171269670128822 + 0.001 * 6.9038615226745605
Epoch 640, val loss: 0.8669801950454712
Epoch 650, training loss: 0.014704566448926926 = 0.007762361317873001 + 0.001 * 6.942205429077148
Epoch 650, val loss: 0.8738157153129578
Epoch 660, training loss: 0.014297682791948318 = 0.007384428754448891 + 0.001 * 6.913253307342529
Epoch 660, val loss: 0.8805031776428223
Epoch 670, training loss: 0.013931810855865479 = 0.007034485228359699 + 0.001 * 6.897325038909912
Epoch 670, val loss: 0.8870497345924377
Epoch 680, training loss: 0.01361745223402977 = 0.006709878332912922 + 0.001 * 6.907573699951172
Epoch 680, val loss: 0.8934498429298401
Epoch 690, training loss: 0.01329372264444828 = 0.0064083244651556015 + 0.001 * 6.8853983879089355
Epoch 690, val loss: 0.8997175693511963
Epoch 700, training loss: 0.013008549809455872 = 0.006127645727247 + 0.001 * 6.880904197692871
Epoch 700, val loss: 0.9058418273925781
Epoch 710, training loss: 0.012757105752825737 = 0.005866033956408501 + 0.001 * 6.89107084274292
Epoch 710, val loss: 0.9118494391441345
Epoch 720, training loss: 0.0125176552683115 = 0.005621886346489191 + 0.001 * 6.895768165588379
Epoch 720, val loss: 0.9177072644233704
Epoch 730, training loss: 0.012301672250032425 = 0.005393687169998884 + 0.001 * 6.907984733581543
Epoch 730, val loss: 0.9234566688537598
Epoch 740, training loss: 0.01204773411154747 = 0.005180077627301216 + 0.001 * 6.867656230926514
Epoch 740, val loss: 0.9290683269500732
Epoch 750, training loss: 0.011885285377502441 = 0.004979845602065325 + 0.001 * 6.9054388999938965
Epoch 750, val loss: 0.9345762133598328
Epoch 760, training loss: 0.011660467833280563 = 0.0047919852659106255 + 0.001 * 6.86848258972168
Epoch 760, val loss: 0.9399623274803162
Epoch 770, training loss: 0.01149386540055275 = 0.004615490324795246 + 0.001 * 6.8783745765686035
Epoch 770, val loss: 0.9452256560325623
Epoch 780, training loss: 0.011310338973999023 = 0.004449455998837948 + 0.001 * 6.8608832359313965
Epoch 780, val loss: 0.9503989815711975
Epoch 790, training loss: 0.011173315346240997 = 0.004293045029044151 + 0.001 * 6.880269527435303
Epoch 790, val loss: 0.9554516673088074
Epoch 800, training loss: 0.011016031727194786 = 0.004145576152950525 + 0.001 * 6.870454788208008
Epoch 800, val loss: 0.9604071974754333
Epoch 810, training loss: 0.010851942002773285 = 0.004006304778158665 + 0.001 * 6.845636367797852
Epoch 810, val loss: 0.9652552008628845
Epoch 820, training loss: 0.010725497268140316 = 0.003874583635479212 + 0.001 * 6.8509135246276855
Epoch 820, val loss: 0.970026433467865
Epoch 830, training loss: 0.010602657683193684 = 0.0037498692981898785 + 0.001 * 6.852787971496582
Epoch 830, val loss: 0.9746925830841064
Epoch 840, training loss: 0.010477736592292786 = 0.003631583647802472 + 0.001 * 6.8461527824401855
Epoch 840, val loss: 0.9792645573616028
Epoch 850, training loss: 0.010372729040682316 = 0.0035191967617720366 + 0.001 * 6.853532314300537
Epoch 850, val loss: 0.9837712049484253
Epoch 860, training loss: 0.010263852775096893 = 0.0034122243523597717 + 0.001 * 6.851628303527832
Epoch 860, val loss: 0.9881857633590698
Epoch 870, training loss: 0.010167011991143227 = 0.003310220083221793 + 0.001 * 6.8567914962768555
Epoch 870, val loss: 0.9924991130828857
Epoch 880, training loss: 0.010040258057415485 = 0.003212754847481847 + 0.001 * 6.827502727508545
Epoch 880, val loss: 0.9967715740203857
Epoch 890, training loss: 0.009977095760405064 = 0.003119431668892503 + 0.001 * 6.857664108276367
Epoch 890, val loss: 1.0009653568267822
Epoch 900, training loss: 0.00985330156981945 = 0.0030301022343337536 + 0.001 * 6.823198318481445
Epoch 900, val loss: 1.0050926208496094
Epoch 910, training loss: 0.00976461824029684 = 0.002944480162113905 + 0.001 * 6.820137977600098
Epoch 910, val loss: 1.0091544389724731
Epoch 920, training loss: 0.009700605645775795 = 0.0028623160906136036 + 0.001 * 6.8382887840271
Epoch 920, val loss: 1.0131502151489258
Epoch 930, training loss: 0.009622840210795403 = 0.0027834312058985233 + 0.001 * 6.8394083976745605
Epoch 930, val loss: 1.0171124935150146
Epoch 940, training loss: 0.009519364684820175 = 0.002707601524889469 + 0.001 * 6.811762809753418
Epoch 940, val loss: 1.0209897756576538
Epoch 950, training loss: 0.009458939544856548 = 0.002634731587022543 + 0.001 * 6.824207782745361
Epoch 950, val loss: 1.0248398780822754
Epoch 960, training loss: 0.009387517347931862 = 0.0025647568982094526 + 0.001 * 6.822760105133057
Epoch 960, val loss: 1.0286059379577637
Epoch 970, training loss: 0.009328164160251617 = 0.0024975547567009926 + 0.001 * 6.830609321594238
Epoch 970, val loss: 1.0323349237442017
Epoch 980, training loss: 0.009235257282853127 = 0.0024330527521669865 + 0.001 * 6.802204608917236
Epoch 980, val loss: 1.0360069274902344
Epoch 990, training loss: 0.00918293371796608 = 0.0023711812682449818 + 0.001 * 6.811751842498779
Epoch 990, val loss: 1.0396292209625244
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9151
Flip ASR: 0.9022/225 nodes
The final ASR:0.65437, 0.19698, Accuracy:0.82099, 0.01145
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10562])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97663, 0.00627, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9435862302780151 = 1.9352123737335205 + 0.001 * 8.373886108398438
Epoch 0, val loss: 1.936905026435852
Epoch 10, training loss: 1.9333730936050415 = 1.9249992370605469 + 0.001 * 8.37381649017334
Epoch 10, val loss: 1.9263269901275635
Epoch 20, training loss: 1.9203282594680786 = 1.911954641342163 + 0.001 * 8.373608589172363
Epoch 20, val loss: 1.9127681255340576
Epoch 30, training loss: 1.9015593528747559 = 1.8931862115859985 + 0.001 * 8.373108863830566
Epoch 30, val loss: 1.893475890159607
Epoch 40, training loss: 1.873937964439392 = 1.8655661344528198 + 0.001 * 8.371798515319824
Epoch 40, val loss: 1.8659855127334595
Epoch 50, training loss: 1.8376227617263794 = 1.8292549848556519 + 0.001 * 8.367746353149414
Epoch 50, val loss: 1.8328616619110107
Epoch 60, training loss: 1.8019555807113647 = 1.7936028242111206 + 0.001 * 8.352730751037598
Epoch 60, val loss: 1.805732011795044
Epoch 70, training loss: 1.766137957572937 = 1.7578589916229248 + 0.001 * 8.278980255126953
Epoch 70, val loss: 1.7782654762268066
Epoch 80, training loss: 1.714374303817749 = 1.706394910812378 + 0.001 * 7.979339122772217
Epoch 80, val loss: 1.7347066402435303
Epoch 90, training loss: 1.6434003114700317 = 1.6354933977127075 + 0.001 * 7.906932830810547
Epoch 90, val loss: 1.6750619411468506
Epoch 100, training loss: 1.5575028657913208 = 1.5496280193328857 + 0.001 * 7.874866485595703
Epoch 100, val loss: 1.6055355072021484
Epoch 110, training loss: 1.469849944114685 = 1.461991548538208 + 0.001 * 7.858388900756836
Epoch 110, val loss: 1.5364702939987183
Epoch 120, training loss: 1.3856112957000732 = 1.377768635749817 + 0.001 * 7.84266471862793
Epoch 120, val loss: 1.4745750427246094
Epoch 130, training loss: 1.3042771816253662 = 1.2964726686477661 + 0.001 * 7.804563045501709
Epoch 130, val loss: 1.4182535409927368
Epoch 140, training loss: 1.2240666151046753 = 1.2163975238800049 + 0.001 * 7.669042110443115
Epoch 140, val loss: 1.3644899129867554
Epoch 150, training loss: 1.1453933715820312 = 1.1378633975982666 + 0.001 * 7.529952049255371
Epoch 150, val loss: 1.3130793571472168
Epoch 160, training loss: 1.0699684619903564 = 1.062451720237732 + 0.001 * 7.516738414764404
Epoch 160, val loss: 1.2648497819900513
Epoch 170, training loss: 0.9987697601318359 = 0.9912837147712708 + 0.001 * 7.486048221588135
Epoch 170, val loss: 1.2203328609466553
Epoch 180, training loss: 0.9308586716651917 = 0.9234020113945007 + 0.001 * 7.456687927246094
Epoch 180, val loss: 1.1782639026641846
Epoch 190, training loss: 0.8633660078048706 = 0.8559461832046509 + 0.001 * 7.419829845428467
Epoch 190, val loss: 1.1362581253051758
Epoch 200, training loss: 0.7935174703598022 = 0.786159336566925 + 0.001 * 7.358131408691406
Epoch 200, val loss: 1.0917810201644897
Epoch 210, training loss: 0.7218553423881531 = 0.714580237865448 + 0.001 * 7.275090217590332
Epoch 210, val loss: 1.04628586769104
Epoch 220, training loss: 0.6527379751205444 = 0.6454880237579346 + 0.001 * 7.249934196472168
Epoch 220, val loss: 1.0051374435424805
Epoch 230, training loss: 0.5911563038825989 = 0.5839266180992126 + 0.001 * 7.229709148406982
Epoch 230, val loss: 0.9732532501220703
Epoch 240, training loss: 0.5393869876861572 = 0.5321698188781738 + 0.001 * 7.217179775238037
Epoch 240, val loss: 0.9521272778511047
Epoch 250, training loss: 0.4965475797653198 = 0.48934510350227356 + 0.001 * 7.2024641036987305
Epoch 250, val loss: 0.9401886463165283
Epoch 260, training loss: 0.4602721035480499 = 0.45308807492256165 + 0.001 * 7.1840362548828125
Epoch 260, val loss: 0.934674084186554
Epoch 270, training loss: 0.4281865954399109 = 0.4210202693939209 + 0.001 * 7.166338920593262
Epoch 270, val loss: 0.9331961870193481
Epoch 280, training loss: 0.3983582854270935 = 0.391202837228775 + 0.001 * 7.155447959899902
Epoch 280, val loss: 0.9337772130966187
Epoch 290, training loss: 0.36945638060569763 = 0.36230945587158203 + 0.001 * 7.14691686630249
Epoch 290, val loss: 0.9353554844856262
Epoch 300, training loss: 0.3407663404941559 = 0.33362293243408203 + 0.001 * 7.143407821655273
Epoch 300, val loss: 0.9373365640640259
Epoch 310, training loss: 0.31221529841423035 = 0.30507248640060425 + 0.001 * 7.142817497253418
Epoch 310, val loss: 0.9395514130592346
Epoch 320, training loss: 0.2840307652950287 = 0.2768881916999817 + 0.001 * 7.142576694488525
Epoch 320, val loss: 0.9424334168434143
Epoch 330, training loss: 0.25645264983177185 = 0.24930983781814575 + 0.001 * 7.142814636230469
Epoch 330, val loss: 0.9464148283004761
Epoch 340, training loss: 0.22968874871730804 = 0.22254502773284912 + 0.001 * 7.143714427947998
Epoch 340, val loss: 0.9514337778091431
Epoch 350, training loss: 0.2039773166179657 = 0.19683338701725006 + 0.001 * 7.143930912017822
Epoch 350, val loss: 0.9575751423835754
Epoch 360, training loss: 0.1796870231628418 = 0.1725386381149292 + 0.001 * 7.148378849029541
Epoch 360, val loss: 0.9649540781974792
Epoch 370, training loss: 0.15720434486865997 = 0.15005716681480408 + 0.001 * 7.147183418273926
Epoch 370, val loss: 0.9737755656242371
Epoch 380, training loss: 0.13687336444854736 = 0.12972714006900787 + 0.001 * 7.146222114562988
Epoch 380, val loss: 0.984034538269043
Epoch 390, training loss: 0.11888258159160614 = 0.11173565685749054 + 0.001 * 7.14692497253418
Epoch 390, val loss: 0.995734453201294
Epoch 400, training loss: 0.10326866805553436 = 0.09612128883600235 + 0.001 * 7.1473774909973145
Epoch 400, val loss: 1.0087791681289673
Epoch 410, training loss: 0.08992566168308258 = 0.08277785778045654 + 0.001 * 7.147806167602539
Epoch 410, val loss: 1.0229339599609375
Epoch 420, training loss: 0.07864392548799515 = 0.07149581611156464 + 0.001 * 7.148105621337891
Epoch 420, val loss: 1.0379434823989868
Epoch 430, training loss: 0.06916500627994537 = 0.06201649084687233 + 0.001 * 7.148513317108154
Epoch 430, val loss: 1.0535122156143188
Epoch 440, training loss: 0.06122249364852905 = 0.054067980498075485 + 0.001 * 7.15451192855835
Epoch 440, val loss: 1.069347620010376
Epoch 450, training loss: 0.05455230548977852 = 0.0473996177315712 + 0.001 * 7.152687072753906
Epoch 450, val loss: 1.0852017402648926
Epoch 460, training loss: 0.04893670976161957 = 0.04178822413086891 + 0.001 * 7.148484230041504
Epoch 460, val loss: 1.1009000539779663
Epoch 470, training loss: 0.044190652668476105 = 0.03704110160470009 + 0.001 * 7.14954948425293
Epoch 470, val loss: 1.1162797212600708
Epoch 480, training loss: 0.04014642536640167 = 0.03299827501177788 + 0.001 * 7.148149013519287
Epoch 480, val loss: 1.1311473846435547
Epoch 490, training loss: 0.03668295964598656 = 0.02953576110303402 + 0.001 * 7.1471991539001465
Epoch 490, val loss: 1.1455318927764893
Epoch 500, training loss: 0.03370330482721329 = 0.02655714750289917 + 0.001 * 7.146155834197998
Epoch 500, val loss: 1.1594045162200928
Epoch 510, training loss: 0.03112911246716976 = 0.023984123021364212 + 0.001 * 7.144989013671875
Epoch 510, val loss: 1.1727848052978516
Epoch 520, training loss: 0.028898468241095543 = 0.021752288565039635 + 0.001 * 7.146179676055908
Epoch 520, val loss: 1.1857093572616577
Epoch 530, training loss: 0.02695373445749283 = 0.019808676093816757 + 0.001 * 7.145058631896973
Epoch 530, val loss: 1.1981488466262817
Epoch 540, training loss: 0.025251276791095734 = 0.018109576776623726 + 0.001 * 7.14169979095459
Epoch 540, val loss: 1.210237979888916
Epoch 550, training loss: 0.023757852613925934 = 0.016617968678474426 + 0.001 * 7.139883041381836
Epoch 550, val loss: 1.2218360900878906
Epoch 560, training loss: 0.022441210225224495 = 0.015302903018891811 + 0.001 * 7.138306617736816
Epoch 560, val loss: 1.233031153678894
Epoch 570, training loss: 0.02128228172659874 = 0.01413847878575325 + 0.001 * 7.143801689147949
Epoch 570, val loss: 1.2438721656799316
Epoch 580, training loss: 0.020237287506461143 = 0.013103618286550045 + 0.001 * 7.133669376373291
Epoch 580, val loss: 1.2543556690216064
Epoch 590, training loss: 0.0193116907030344 = 0.012180475518107414 + 0.001 * 7.131214618682861
Epoch 590, val loss: 1.2644833326339722
Epoch 600, training loss: 0.018482301384210587 = 0.01135331578552723 + 0.001 * 7.128984451293945
Epoch 600, val loss: 1.274267554283142
Epoch 610, training loss: 0.01773439161479473 = 0.010608920827507973 + 0.001 * 7.125471115112305
Epoch 610, val loss: 1.2837854623794556
Epoch 620, training loss: 0.017072705551981926 = 0.009936543181538582 + 0.001 * 7.136161804199219
Epoch 620, val loss: 1.2930330038070679
Epoch 630, training loss: 0.016447803005576134 = 0.009326557628810406 + 0.001 * 7.121244430541992
Epoch 630, val loss: 1.3020360469818115
Epoch 640, training loss: 0.015894969925284386 = 0.008770766668021679 + 0.001 * 7.124203205108643
Epoch 640, val loss: 1.310784101486206
Epoch 650, training loss: 0.015372509136795998 = 0.008262722752988338 + 0.001 * 7.109786033630371
Epoch 650, val loss: 1.3193416595458984
Epoch 660, training loss: 0.014922007918357849 = 0.0077970088459551334 + 0.001 * 7.124998092651367
Epoch 660, val loss: 1.3276809453964233
Epoch 670, training loss: 0.014476427808403969 = 0.0073692407459020615 + 0.001 * 7.107187271118164
Epoch 670, val loss: 1.3358310461044312
Epoch 680, training loss: 0.014076124876737595 = 0.006975748110562563 + 0.001 * 7.100376605987549
Epoch 680, val loss: 1.3437962532043457
Epoch 690, training loss: 0.013715438544750214 = 0.0066132512874901295 + 0.001 * 7.102186679840088
Epoch 690, val loss: 1.351544737815857
Epoch 700, training loss: 0.0133824422955513 = 0.006278766319155693 + 0.001 * 7.103675365447998
Epoch 700, val loss: 1.3590996265411377
Epoch 710, training loss: 0.013055676594376564 = 0.005969712510704994 + 0.001 * 7.085963249206543
Epoch 710, val loss: 1.3664876222610474
Epoch 720, training loss: 0.012761237099766731 = 0.005683779716491699 + 0.001 * 7.077456951141357
Epoch 720, val loss: 1.3736958503723145
Epoch 730, training loss: 0.01249939389526844 = 0.005418941378593445 + 0.001 * 7.080451965332031
Epoch 730, val loss: 1.3807711601257324
Epoch 740, training loss: 0.012282045558094978 = 0.005173376761376858 + 0.001 * 7.108668804168701
Epoch 740, val loss: 1.3876316547393799
Epoch 750, training loss: 0.012010800652205944 = 0.004945297259837389 + 0.001 * 7.065503120422363
Epoch 750, val loss: 1.3942971229553223
Epoch 760, training loss: 0.011814817786216736 = 0.004733096342533827 + 0.001 * 7.081721782684326
Epoch 760, val loss: 1.400831937789917
Epoch 770, training loss: 0.011600282043218613 = 0.004535339307039976 + 0.001 * 7.064942359924316
Epoch 770, val loss: 1.407204508781433
Epoch 780, training loss: 0.0114202369004488 = 0.004350847564637661 + 0.001 * 7.069389343261719
Epoch 780, val loss: 1.4134248495101929
Epoch 790, training loss: 0.011225132271647453 = 0.004178561270236969 + 0.001 * 7.046571254730225
Epoch 790, val loss: 1.419549584388733
Epoch 800, training loss: 0.011097799055278301 = 0.004017465282231569 + 0.001 * 7.080333232879639
Epoch 800, val loss: 1.4255142211914062
Epoch 810, training loss: 0.010946140624582767 = 0.00386664061807096 + 0.001 * 7.079499244689941
Epoch 810, val loss: 1.431288242340088
Epoch 820, training loss: 0.010763734579086304 = 0.0037251885514706373 + 0.001 * 7.038546085357666
Epoch 820, val loss: 1.4369561672210693
Epoch 830, training loss: 0.010619391687214375 = 0.0035923023242503405 + 0.001 * 7.027088642120361
Epoch 830, val loss: 1.4424997568130493
Epoch 840, training loss: 0.010510140098631382 = 0.0034673248883336782 + 0.001 * 7.042815208435059
Epoch 840, val loss: 1.4479097127914429
Epoch 850, training loss: 0.01039380207657814 = 0.003349738661199808 + 0.001 * 7.044063568115234
Epoch 850, val loss: 1.4531774520874023
Epoch 860, training loss: 0.010278880596160889 = 0.003238827921450138 + 0.001 * 7.040052890777588
Epoch 860, val loss: 1.4583368301391602
Epoch 870, training loss: 0.010180199518799782 = 0.0031341214198619127 + 0.001 * 7.046078205108643
Epoch 870, val loss: 1.463393211364746
Epoch 880, training loss: 0.01005985401570797 = 0.003035245230421424 + 0.001 * 7.024608135223389
Epoch 880, val loss: 1.4683310985565186
Epoch 890, training loss: 0.009961022064089775 = 0.0029417460318654776 + 0.001 * 7.019276142120361
Epoch 890, val loss: 1.4731457233428955
Epoch 900, training loss: 0.009860099293291569 = 0.0028532494325190783 + 0.001 * 7.006849765777588
Epoch 900, val loss: 1.4778751134872437
Epoch 910, training loss: 0.009766343981027603 = 0.0027693649753928185 + 0.001 * 6.996978282928467
Epoch 910, val loss: 1.4824843406677246
Epoch 920, training loss: 0.00969637744128704 = 0.0026898093055933714 + 0.001 * 7.006567478179932
Epoch 920, val loss: 1.4870270490646362
Epoch 930, training loss: 0.009612750262022018 = 0.0026142755523324013 + 0.001 * 6.998475074768066
Epoch 930, val loss: 1.491448163986206
Epoch 940, training loss: 0.009528161957859993 = 0.002542494796216488 + 0.001 * 6.985666751861572
Epoch 940, val loss: 1.495761752128601
Epoch 950, training loss: 0.009464317001402378 = 0.0024742200039327145 + 0.001 * 6.990096569061279
Epoch 950, val loss: 1.500015377998352
Epoch 960, training loss: 0.009419641457498074 = 0.0024092234671115875 + 0.001 * 7.010417461395264
Epoch 960, val loss: 1.504185438156128
Epoch 970, training loss: 0.009341662749648094 = 0.0023473319597542286 + 0.001 * 6.994330406188965
Epoch 970, val loss: 1.508224368095398
Epoch 980, training loss: 0.009267089888453484 = 0.0022883224301040173 + 0.001 * 6.978766918182373
Epoch 980, val loss: 1.5121781826019287
Epoch 990, training loss: 0.009262719191610813 = 0.0022320111747831106 + 0.001 * 7.030707836151123
Epoch 990, val loss: 1.5160402059555054
=== picking the best model according to the performance on validation ===
target class rate on Vs: 0.8000
accuracy on clean test nodes: 0.7296
Overall ASR: 0.3801
Flip ASR: 0.2800/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9508172273635864 = 1.9424433708190918 + 0.001 * 8.37390422821045
Epoch 0, val loss: 1.9472683668136597
Epoch 10, training loss: 1.941136121749878 = 1.9327622652053833 + 0.001 * 8.373872756958008
Epoch 10, val loss: 1.9379968643188477
Epoch 20, training loss: 1.9293432235717773 = 1.9209694862365723 + 0.001 * 8.37374210357666
Epoch 20, val loss: 1.9261761903762817
Epoch 30, training loss: 1.912753939628601 = 1.904380440711975 + 0.001 * 8.373453140258789
Epoch 30, val loss: 1.9092639684677124
Epoch 40, training loss: 1.8881181478500366 = 1.879745364189148 + 0.001 * 8.372804641723633
Epoch 40, val loss: 1.884263277053833
Epoch 50, training loss: 1.8524556159973145 = 1.8440845012664795 + 0.001 * 8.37113094329834
Epoch 50, val loss: 1.8489717245101929
Epoch 60, training loss: 1.8070933818817139 = 1.7987275123596191 + 0.001 * 8.365859985351562
Epoch 60, val loss: 1.8056505918502808
Epoch 70, training loss: 1.7571481466293335 = 1.7488043308258057 + 0.001 * 8.343772888183594
Epoch 70, val loss: 1.7583034038543701
Epoch 80, training loss: 1.6948890686035156 = 1.6866852045059204 + 0.001 * 8.203843116760254
Epoch 80, val loss: 1.698997139930725
Epoch 90, training loss: 1.6106390953063965 = 1.6027520895004272 + 0.001 * 7.887063026428223
Epoch 90, val loss: 1.6249008178710938
Epoch 100, training loss: 1.5083489418029785 = 1.500493049621582 + 0.001 * 7.8559489250183105
Epoch 100, val loss: 1.5417195558547974
Epoch 110, training loss: 1.4012295007705688 = 1.393390417098999 + 0.001 * 7.839133262634277
Epoch 110, val loss: 1.4554818868637085
Epoch 120, training loss: 1.3018901348114014 = 1.2940744161605835 + 0.001 * 7.815695762634277
Epoch 120, val loss: 1.3794668912887573
Epoch 130, training loss: 1.2141196727752686 = 1.2063933610916138 + 0.001 * 7.726302623748779
Epoch 130, val loss: 1.3156973123550415
Epoch 140, training loss: 1.1342802047729492 = 1.126788854598999 + 0.001 * 7.491399765014648
Epoch 140, val loss: 1.2587978839874268
Epoch 150, training loss: 1.057599663734436 = 1.0501083135604858 + 0.001 * 7.491302013397217
Epoch 150, val loss: 1.2030935287475586
Epoch 160, training loss: 0.9811626076698303 = 0.9737141728401184 + 0.001 * 7.448431015014648
Epoch 160, val loss: 1.1462318897247314
Epoch 170, training loss: 0.9056487679481506 = 0.8982149362564087 + 0.001 * 7.433826923370361
Epoch 170, val loss: 1.088820457458496
Epoch 180, training loss: 0.8331526517868042 = 0.8257365822792053 + 0.001 * 7.416088104248047
Epoch 180, val loss: 1.0334391593933105
Epoch 190, training loss: 0.766102135181427 = 0.7587093114852905 + 0.001 * 7.39285135269165
Epoch 190, val loss: 0.9829423427581787
Epoch 200, training loss: 0.706044614315033 = 0.6986742615699768 + 0.001 * 7.370370864868164
Epoch 200, val loss: 0.9397414326667786
Epoch 210, training loss: 0.6528356671333313 = 0.6454981565475464 + 0.001 * 7.3375091552734375
Epoch 210, val loss: 0.9045677185058594
Epoch 220, training loss: 0.6046259999275208 = 0.5973553657531738 + 0.001 * 7.270618915557861
Epoch 220, val loss: 0.8766048550605774
Epoch 230, training loss: 0.5592954754829407 = 0.5520946979522705 + 0.001 * 7.200770854949951
Epoch 230, val loss: 0.854489803314209
Epoch 240, training loss: 0.5150133967399597 = 0.5078758001327515 + 0.001 * 7.137594699859619
Epoch 240, val loss: 0.8370105624198914
Epoch 250, training loss: 0.47075769305229187 = 0.4636428654193878 + 0.001 * 7.11483907699585
Epoch 250, val loss: 0.8230774402618408
Epoch 260, training loss: 0.42632198333740234 = 0.4192253351211548 + 0.001 * 7.096656322479248
Epoch 260, val loss: 0.8124793171882629
Epoch 270, training loss: 0.38222309947013855 = 0.3751363754272461 + 0.001 * 7.086729526519775
Epoch 270, val loss: 0.8053038716316223
Epoch 280, training loss: 0.33930617570877075 = 0.3322318494319916 + 0.001 * 7.074320316314697
Epoch 280, val loss: 0.8016490340232849
Epoch 290, training loss: 0.29855063557624817 = 0.29148948192596436 + 0.001 * 7.061164379119873
Epoch 290, val loss: 0.8016844987869263
Epoch 300, training loss: 0.2606573700904846 = 0.25360748171806335 + 0.001 * 7.0498762130737305
Epoch 300, val loss: 0.8055352568626404
Epoch 310, training loss: 0.2268563210964203 = 0.21981795132160187 + 0.001 * 7.038370132446289
Epoch 310, val loss: 0.8133623003959656
Epoch 320, training loss: 0.1974831521511078 = 0.19044889509677887 + 0.001 * 7.034252166748047
Epoch 320, val loss: 0.8248724341392517
Epoch 330, training loss: 0.17251479625701904 = 0.16548320651054382 + 0.001 * 7.03159236907959
Epoch 330, val loss: 0.8396937847137451
Epoch 340, training loss: 0.15141010284423828 = 0.14438781142234802 + 0.001 * 7.022292137145996
Epoch 340, val loss: 0.8573483824729919
Epoch 350, training loss: 0.13349729776382446 = 0.1264769285917282 + 0.001 * 7.020369052886963
Epoch 350, val loss: 0.8769612908363342
Epoch 360, training loss: 0.11816276609897614 = 0.11114625632762909 + 0.001 * 7.016509532928467
Epoch 360, val loss: 0.8980022668838501
Epoch 370, training loss: 0.10494871437549591 = 0.09793446958065033 + 0.001 * 7.014248371124268
Epoch 370, val loss: 0.9200599193572998
Epoch 380, training loss: 0.09349473565816879 = 0.08648266643285751 + 0.001 * 7.01206636428833
Epoch 380, val loss: 0.942781925201416
Epoch 390, training loss: 0.0835297629237175 = 0.07650908827781677 + 0.001 * 7.020674705505371
Epoch 390, val loss: 0.9658513069152832
Epoch 400, training loss: 0.07484981417655945 = 0.06783659011125565 + 0.001 * 7.013223171234131
Epoch 400, val loss: 0.9893836975097656
Epoch 410, training loss: 0.0673505961894989 = 0.060340043157339096 + 0.001 * 7.010554790496826
Epoch 410, val loss: 1.0131323337554932
Epoch 420, training loss: 0.0608498640358448 = 0.05384192243218422 + 0.001 * 7.007941246032715
Epoch 420, val loss: 1.0371168851852417
Epoch 430, training loss: 0.05521576106548309 = 0.04820767045021057 + 0.001 * 7.008090019226074
Epoch 430, val loss: 1.0610671043395996
Epoch 440, training loss: 0.050316184759140015 = 0.04330874979496002 + 0.001 * 7.007433891296387
Epoch 440, val loss: 1.0847958326339722
Epoch 450, training loss: 0.046043988317251205 = 0.039039645344018936 + 0.001 * 7.0043439865112305
Epoch 450, val loss: 1.1080998182296753
Epoch 460, training loss: 0.0423184297978878 = 0.03531308099627495 + 0.001 * 7.005349636077881
Epoch 460, val loss: 1.1308354139328003
Epoch 470, training loss: 0.03905618563294411 = 0.03205442801117897 + 0.001 * 7.001755714416504
Epoch 470, val loss: 1.152867317199707
Epoch 480, training loss: 0.036196690052747726 = 0.02919645793735981 + 0.001 * 7.000232219696045
Epoch 480, val loss: 1.1741904020309448
Epoch 490, training loss: 0.03369094431400299 = 0.02668057009577751 + 0.001 * 7.010373115539551
Epoch 490, val loss: 1.1947641372680664
Epoch 500, training loss: 0.03145400807261467 = 0.024460570886731148 + 0.001 * 6.993437767028809
Epoch 500, val loss: 1.2145953178405762
Epoch 510, training loss: 0.029489848762750626 = 0.0224960558116436 + 0.001 * 6.99379301071167
Epoch 510, val loss: 1.2336970567703247
Epoch 520, training loss: 0.027747001498937607 = 0.02075173333287239 + 0.001 * 6.995267391204834
Epoch 520, val loss: 1.2520419359207153
Epoch 530, training loss: 0.02618340030312538 = 0.019197814166545868 + 0.001 * 6.9855852127075195
Epoch 530, val loss: 1.2697842121124268
Epoch 540, training loss: 0.024790750816464424 = 0.01780957728624344 + 0.001 * 6.981173515319824
Epoch 540, val loss: 1.2869036197662354
Epoch 550, training loss: 0.023547928780317307 = 0.01656550168991089 + 0.001 * 6.98242712020874
Epoch 550, val loss: 1.3033922910690308
Epoch 560, training loss: 0.022427979856729507 = 0.015447132289409637 + 0.001 * 6.98084831237793
Epoch 560, val loss: 1.3193546533584595
Epoch 570, training loss: 0.02141522243618965 = 0.014438549056649208 + 0.001 * 6.976672649383545
Epoch 570, val loss: 1.3347817659378052
Epoch 580, training loss: 0.020497005432844162 = 0.013526389375329018 + 0.001 * 6.970616817474365
Epoch 580, val loss: 1.3496618270874023
Epoch 590, training loss: 0.01968119479715824 = 0.012698871083557606 + 0.001 * 6.98232364654541
Epoch 590, val loss: 1.3640995025634766
Epoch 600, training loss: 0.018916361033916473 = 0.011946174316108227 + 0.001 * 6.970187187194824
Epoch 600, val loss: 1.3780853748321533
Epoch 610, training loss: 0.01823391206562519 = 0.011259784922003746 + 0.001 * 6.974127292633057
Epoch 610, val loss: 1.3915984630584717
Epoch 620, training loss: 0.017591048032045364 = 0.010632258839905262 + 0.001 * 6.958788871765137
Epoch 620, val loss: 1.4047200679779053
Epoch 630, training loss: 0.01702357456088066 = 0.010057204402983189 + 0.001 * 6.966370582580566
Epoch 630, val loss: 1.4174420833587646
Epoch 640, training loss: 0.016487039625644684 = 0.009529032744467258 + 0.001 * 6.958007335662842
Epoch 640, val loss: 1.4297475814819336
Epoch 650, training loss: 0.015990884974598885 = 0.009042849764227867 + 0.001 * 6.94803524017334
Epoch 650, val loss: 1.441688060760498
Epoch 660, training loss: 0.015554678626358509 = 0.008594454266130924 + 0.001 * 6.960224151611328
Epoch 660, val loss: 1.4533005952835083
Epoch 670, training loss: 0.015136883594095707 = 0.008180156350135803 + 0.001 * 6.956727027893066
Epoch 670, val loss: 1.4645756483078003
Epoch 680, training loss: 0.014756690710783005 = 0.007796529680490494 + 0.001 * 6.960161209106445
Epoch 680, val loss: 1.4755456447601318
Epoch 690, training loss: 0.014391450211405754 = 0.007440642919391394 + 0.001 * 6.950806617736816
Epoch 690, val loss: 1.486192226409912
Epoch 700, training loss: 0.014045584946870804 = 0.007109855301678181 + 0.001 * 6.93572998046875
Epoch 700, val loss: 1.4965434074401855
Epoch 710, training loss: 0.01374894380569458 = 0.006801898125559092 + 0.001 * 6.947044849395752
Epoch 710, val loss: 1.5066101551055908
Epoch 720, training loss: 0.013447064906358719 = 0.006514774169772863 + 0.001 * 6.932291030883789
Epoch 720, val loss: 1.5164021253585815
Epoch 730, training loss: 0.013184946030378342 = 0.006246630568057299 + 0.001 * 6.938314914703369
Epoch 730, val loss: 1.525936245918274
Epoch 740, training loss: 0.012923579663038254 = 0.005995870102196932 + 0.001 * 6.927709102630615
Epoch 740, val loss: 1.5352132320404053
Epoch 750, training loss: 0.012680307030677795 = 0.005761045031249523 + 0.001 * 6.9192609786987305
Epoch 750, val loss: 1.5442825555801392
Epoch 760, training loss: 0.012476079165935516 = 0.005540846381336451 + 0.001 * 6.935233116149902
Epoch 760, val loss: 1.5530767440795898
Epoch 770, training loss: 0.012274390086531639 = 0.005334064364433289 + 0.001 * 6.9403252601623535
Epoch 770, val loss: 1.561657428741455
Epoch 780, training loss: 0.012072570621967316 = 0.005139648448675871 + 0.001 * 6.93292236328125
Epoch 780, val loss: 1.570023775100708
Epoch 790, training loss: 0.011881557293236256 = 0.004956649616360664 + 0.001 * 6.924907207489014
Epoch 790, val loss: 1.5782122611999512
Epoch 800, training loss: 0.011702103540301323 = 0.004784177057445049 + 0.001 * 6.91792631149292
Epoch 800, val loss: 1.586181640625
Epoch 810, training loss: 0.01152816228568554 = 0.004621440079063177 + 0.001 * 6.906722545623779
Epoch 810, val loss: 1.593930959701538
Epoch 820, training loss: 0.011372994631528854 = 0.00446772575378418 + 0.001 * 6.90526819229126
Epoch 820, val loss: 1.601527452468872
Epoch 830, training loss: 0.011251143179833889 = 0.004322363995015621 + 0.001 * 6.928778648376465
Epoch 830, val loss: 1.6089131832122803
Epoch 840, training loss: 0.01109166257083416 = 0.004184769466519356 + 0.001 * 6.906893253326416
Epoch 840, val loss: 1.6161572933197021
Epoch 850, training loss: 0.010957455262541771 = 0.0040544006042182446 + 0.001 * 6.903054714202881
Epoch 850, val loss: 1.6232173442840576
Epoch 860, training loss: 0.01085739117115736 = 0.003930750768631697 + 0.001 * 6.926640033721924
Epoch 860, val loss: 1.630123496055603
Epoch 870, training loss: 0.010717565193772316 = 0.0038133820053189993 + 0.001 * 6.9041829109191895
Epoch 870, val loss: 1.6368663311004639
Epoch 880, training loss: 0.01060408167541027 = 0.0037018698640167713 + 0.001 * 6.9022111892700195
Epoch 880, val loss: 1.643479824066162
Epoch 890, training loss: 0.010493449866771698 = 0.003595818066969514 + 0.001 * 6.897631645202637
Epoch 890, val loss: 1.649900197982788
Epoch 900, training loss: 0.010397844016551971 = 0.0034948955290019512 + 0.001 * 6.902948379516602
Epoch 900, val loss: 1.6562144756317139
Epoch 910, training loss: 0.010289357043802738 = 0.0033987881615757942 + 0.001 * 6.890568733215332
Epoch 910, val loss: 1.6623872518539429
Epoch 920, training loss: 0.010186601430177689 = 0.003307164879515767 + 0.001 * 6.879436016082764
Epoch 920, val loss: 1.668397068977356
Epoch 930, training loss: 0.010095360688865185 = 0.0032197684049606323 + 0.001 * 6.87559175491333
Epoch 930, val loss: 1.6742960214614868
Epoch 940, training loss: 0.010005595162510872 = 0.003136323532089591 + 0.001 * 6.869271755218506
Epoch 940, val loss: 1.6800715923309326
Epoch 950, training loss: 0.00993594340980053 = 0.0030565992929041386 + 0.001 * 6.8793439865112305
Epoch 950, val loss: 1.6857343912124634
Epoch 960, training loss: 0.009853375144302845 = 0.0029803630895912647 + 0.001 * 6.873011589050293
Epoch 960, val loss: 1.691259503364563
Epoch 970, training loss: 0.009787152521312237 = 0.0029074426274746656 + 0.001 * 6.879709243774414
Epoch 970, val loss: 1.6966556310653687
Epoch 980, training loss: 0.009715273976325989 = 0.002837661886587739 + 0.001 * 6.8776116371154785
Epoch 980, val loss: 1.7019881010055542
Epoch 990, training loss: 0.009648533537983894 = 0.0027708380948752165 + 0.001 * 6.877695560455322
Epoch 990, val loss: 1.7071659564971924
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.8672
Flip ASR: 0.8400/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9652012586593628 = 1.9568274021148682 + 0.001 * 8.373884201049805
Epoch 0, val loss: 1.9572269916534424
Epoch 10, training loss: 1.954930305480957 = 1.9465564489364624 + 0.001 * 8.373847007751465
Epoch 10, val loss: 1.946517825126648
Epoch 20, training loss: 1.9421383142471313 = 1.9337645769119263 + 0.001 * 8.373698234558105
Epoch 20, val loss: 1.932977557182312
Epoch 30, training loss: 1.9237868785858154 = 1.915413498878479 + 0.001 * 8.373405456542969
Epoch 30, val loss: 1.913641095161438
Epoch 40, training loss: 1.8960527181625366 = 1.887679934501648 + 0.001 * 8.372781753540039
Epoch 40, val loss: 1.8847901821136475
Epoch 50, training loss: 1.8563390970230103 = 1.8479678630828857 + 0.001 * 8.371270179748535
Epoch 50, val loss: 1.8450802564620972
Epoch 60, training loss: 1.8103830814361572 = 1.802016258239746 + 0.001 * 8.366809844970703
Epoch 60, val loss: 1.8033267259597778
Epoch 70, training loss: 1.7711875438690186 = 1.762837529182434 + 0.001 * 8.349961280822754
Epoch 70, val loss: 1.7716859579086304
Epoch 80, training loss: 1.7264373302459717 = 1.7181901931762695 + 0.001 * 8.247105598449707
Epoch 80, val loss: 1.733526349067688
Epoch 90, training loss: 1.6653226613998413 = 1.65742826461792 + 0.001 * 7.89439058303833
Epoch 90, val loss: 1.682418942451477
Epoch 100, training loss: 1.5844203233718872 = 1.5766644477844238 + 0.001 * 7.755824089050293
Epoch 100, val loss: 1.615600347518921
Epoch 110, training loss: 1.4869600534439087 = 1.4794065952301025 + 0.001 * 7.553481578826904
Epoch 110, val loss: 1.5361775159835815
Epoch 120, training loss: 1.3830740451812744 = 1.3757997751235962 + 0.001 * 7.274226665496826
Epoch 120, val loss: 1.4538432359695435
Epoch 130, training loss: 1.278778076171875 = 1.271560549736023 + 0.001 * 7.217530727386475
Epoch 130, val loss: 1.3739089965820312
Epoch 140, training loss: 1.1747088432312012 = 1.1675736904144287 + 0.001 * 7.13515567779541
Epoch 140, val loss: 1.2963006496429443
Epoch 150, training loss: 1.0727907419204712 = 1.0657060146331787 + 0.001 * 7.084700584411621
Epoch 150, val loss: 1.2208956480026245
Epoch 160, training loss: 0.9755955934524536 = 0.9685357213020325 + 0.001 * 7.0598931312561035
Epoch 160, val loss: 1.1484297513961792
Epoch 170, training loss: 0.884165346622467 = 0.8771167993545532 + 0.001 * 7.048572540283203
Epoch 170, val loss: 1.0797150135040283
Epoch 180, training loss: 0.7978613376617432 = 0.7908177971839905 + 0.001 * 7.043568134307861
Epoch 180, val loss: 1.013915777206421
Epoch 190, training loss: 0.7161417007446289 = 0.7091001272201538 + 0.001 * 7.0415754318237305
Epoch 190, val loss: 0.9506215453147888
Epoch 200, training loss: 0.6395497918128967 = 0.6325083374977112 + 0.001 * 7.041444301605225
Epoch 200, val loss: 0.8909652233123779
Epoch 210, training loss: 0.5692668557167053 = 0.5622251033782959 + 0.001 * 7.04172420501709
Epoch 210, val loss: 0.8368866443634033
Epoch 220, training loss: 0.506004810333252 = 0.49896278977394104 + 0.001 * 7.042009353637695
Epoch 220, val loss: 0.7898587584495544
Epoch 230, training loss: 0.4495278000831604 = 0.4424854516983032 + 0.001 * 7.042352199554443
Epoch 230, val loss: 0.7506903409957886
Epoch 240, training loss: 0.39913466572761536 = 0.3920917809009552 + 0.001 * 7.04288911819458
Epoch 240, val loss: 0.7188805341720581
Epoch 250, training loss: 0.3540509343147278 = 0.34700724482536316 + 0.001 * 7.043686866760254
Epoch 250, val loss: 0.6936194896697998
Epoch 260, training loss: 0.3136436343193054 = 0.3065989315509796 + 0.001 * 7.044708251953125
Epoch 260, val loss: 0.6737815737724304
Epoch 270, training loss: 0.2772742807865143 = 0.27022844552993774 + 0.001 * 7.045845031738281
Epoch 270, val loss: 0.6585543751716614
Epoch 280, training loss: 0.24446234107017517 = 0.2374153584241867 + 0.001 * 7.046984672546387
Epoch 280, val loss: 0.6473060250282288
Epoch 290, training loss: 0.21487939357757568 = 0.20783129334449768 + 0.001 * 7.048101902008057
Epoch 290, val loss: 0.6396594643592834
Epoch 300, training loss: 0.18837054073810577 = 0.18132127821445465 + 0.001 * 7.049258708953857
Epoch 300, val loss: 0.6352549195289612
Epoch 310, training loss: 0.16486383974552155 = 0.15781237185001373 + 0.001 * 7.05146598815918
Epoch 310, val loss: 0.6337156891822815
Epoch 320, training loss: 0.14427754282951355 = 0.13722528517246246 + 0.001 * 7.052255153656006
Epoch 320, val loss: 0.6347880959510803
Epoch 330, training loss: 0.12646342813968658 = 0.11940918862819672 + 0.001 * 7.054239273071289
Epoch 330, val loss: 0.6381132006645203
Epoch 340, training loss: 0.1111910417675972 = 0.10413525253534317 + 0.001 * 7.0557861328125
Epoch 340, val loss: 0.6433705687522888
Epoch 350, training loss: 0.09816896915435791 = 0.09111181646585464 + 0.001 * 7.057156085968018
Epoch 350, val loss: 0.6502164602279663
Epoch 360, training loss: 0.08708884567022324 = 0.08002954721450806 + 0.001 * 7.059295654296875
Epoch 360, val loss: 0.6583449244499207
Epoch 370, training loss: 0.07765869796276093 = 0.07059970498085022 + 0.001 * 7.05898904800415
Epoch 370, val loss: 0.667468786239624
Epoch 380, training loss: 0.06962113082408905 = 0.06256216764450073 + 0.001 * 7.058964729309082
Epoch 380, val loss: 0.6772474646568298
Epoch 390, training loss: 0.06275314837694168 = 0.05569504573941231 + 0.001 * 7.058104991912842
Epoch 390, val loss: 0.6874858140945435
Epoch 400, training loss: 0.056864745914936066 = 0.049807626754045486 + 0.001 * 7.057117938995361
Epoch 400, val loss: 0.6979916095733643
Epoch 410, training loss: 0.05179630592465401 = 0.04474275931715965 + 0.001 * 7.053546905517578
Epoch 410, val loss: 0.708559513092041
Epoch 420, training loss: 0.04742525890469551 = 0.04036968573927879 + 0.001 * 7.055572509765625
Epoch 420, val loss: 0.7190843820571899
Epoch 430, training loss: 0.0436251163482666 = 0.03657776862382889 + 0.001 * 7.047346115112305
Epoch 430, val loss: 0.7294628620147705
Epoch 440, training loss: 0.040311768651008606 = 0.03327547758817673 + 0.001 * 7.036291599273682
Epoch 440, val loss: 0.7397199273109436
Epoch 450, training loss: 0.03743252530694008 = 0.03038795106112957 + 0.001 * 7.044572830200195
Epoch 450, val loss: 0.7496768832206726
Epoch 460, training loss: 0.034880928695201874 = 0.027851946651935577 + 0.001 * 7.028980731964111
Epoch 460, val loss: 0.7594621777534485
Epoch 470, training loss: 0.03263077512383461 = 0.025615662336349487 + 0.001 * 7.015110969543457
Epoch 470, val loss: 0.7689715623855591
Epoch 480, training loss: 0.030642082914710045 = 0.023634454235434532 + 0.001 * 7.007628440856934
Epoch 480, val loss: 0.7782526016235352
Epoch 490, training loss: 0.0288771390914917 = 0.021872425451874733 + 0.001 * 7.0047125816345215
Epoch 490, val loss: 0.7872825264930725
Epoch 500, training loss: 0.027311598882079124 = 0.020299701020121574 + 0.001 * 7.011898040771484
Epoch 500, val loss: 0.7960292100906372
Epoch 510, training loss: 0.025899112224578857 = 0.018890708684921265 + 0.001 * 7.008403778076172
Epoch 510, val loss: 0.8045629858970642
Epoch 520, training loss: 0.024631468579173088 = 0.01762397773563862 + 0.001 * 7.007490634918213
Epoch 520, val loss: 0.8129053115844727
Epoch 530, training loss: 0.023483233526349068 = 0.016481447964906693 + 0.001 * 7.001785755157471
Epoch 530, val loss: 0.8209781050682068
Epoch 540, training loss: 0.0224453117698431 = 0.015447738580405712 + 0.001 * 6.997572422027588
Epoch 540, val loss: 0.8288524150848389
Epoch 550, training loss: 0.02151547372341156 = 0.014509730972349644 + 0.001 * 7.005743503570557
Epoch 550, val loss: 0.8365163803100586
Epoch 560, training loss: 0.02065170742571354 = 0.01365605928003788 + 0.001 * 6.99564790725708
Epoch 560, val loss: 0.8439511060714722
Epoch 570, training loss: 0.019873326644301414 = 0.012877106666564941 + 0.001 * 6.996219158172607
Epoch 570, val loss: 0.8512193560600281
Epoch 580, training loss: 0.019183365628123283 = 0.012164647690951824 + 0.001 * 7.018717288970947
Epoch 580, val loss: 0.8582972884178162
Epoch 590, training loss: 0.018507031723856926 = 0.011511389166116714 + 0.001 * 6.995642185211182
Epoch 590, val loss: 0.8651983737945557
Epoch 600, training loss: 0.017902730032801628 = 0.01091096643358469 + 0.001 * 6.991763114929199
Epoch 600, val loss: 0.8718999624252319
Epoch 610, training loss: 0.01734955795109272 = 0.010357173159718513 + 0.001 * 6.992384433746338
Epoch 610, val loss: 0.8784701824188232
Epoch 620, training loss: 0.01683664880692959 = 0.009842938743531704 + 0.001 * 6.993709564208984
Epoch 620, val loss: 0.8849866986274719
Epoch 630, training loss: 0.01635168492794037 = 0.009361169300973415 + 0.001 * 6.99051570892334
Epoch 630, val loss: 0.891539454460144
Epoch 640, training loss: 0.015895288437604904 = 0.008907957933843136 + 0.001 * 6.987330436706543
Epoch 640, val loss: 0.8980857729911804
Epoch 650, training loss: 0.015472806990146637 = 0.008481724187731743 + 0.001 * 6.991082191467285
Epoch 650, val loss: 0.9046427607536316
Epoch 660, training loss: 0.015071427449584007 = 0.008081361651420593 + 0.001 * 6.990065574645996
Epoch 660, val loss: 0.9111823439598083
Epoch 670, training loss: 0.014706340618431568 = 0.007705923169851303 + 0.001 * 7.000417232513428
Epoch 670, val loss: 0.917615532875061
Epoch 680, training loss: 0.014336513355374336 = 0.007354393135756254 + 0.001 * 6.982120037078857
Epoch 680, val loss: 0.9239690899848938
Epoch 690, training loss: 0.014007048681378365 = 0.00702523672953248 + 0.001 * 6.981812000274658
Epoch 690, val loss: 0.930249810218811
Epoch 700, training loss: 0.013703380711376667 = 0.0067168050445616245 + 0.001 * 6.986575126647949
Epoch 700, val loss: 0.936468243598938
Epoch 710, training loss: 0.013410934247076511 = 0.006427134852856398 + 0.001 * 6.983798980712891
Epoch 710, val loss: 0.942584753036499
Epoch 720, training loss: 0.013132426887750626 = 0.0061540971510112286 + 0.001 * 6.978329658508301
Epoch 720, val loss: 0.9486706852912903
Epoch 730, training loss: 0.01288309134542942 = 0.005895793437957764 + 0.001 * 6.987297058105469
Epoch 730, val loss: 0.9547532796859741
Epoch 740, training loss: 0.012625647708773613 = 0.0056510204449296 + 0.001 * 6.974626541137695
Epoch 740, val loss: 0.9608001708984375
Epoch 750, training loss: 0.012392660602927208 = 0.005419203080236912 + 0.001 * 6.9734578132629395
Epoch 750, val loss: 0.9668188095092773
Epoch 760, training loss: 0.012177886441349983 = 0.005199680104851723 + 0.001 * 6.978206157684326
Epoch 760, val loss: 0.9727922081947327
Epoch 770, training loss: 0.011954415589571 = 0.004992084112018347 + 0.001 * 6.962331771850586
Epoch 770, val loss: 0.9787296056747437
Epoch 780, training loss: 0.011774837039411068 = 0.004795820917934179 + 0.001 * 6.979015827178955
Epoch 780, val loss: 0.9846212863922119
Epoch 790, training loss: 0.011587732471525669 = 0.004610605072230101 + 0.001 * 6.9771270751953125
Epoch 790, val loss: 0.9904311299324036
Epoch 800, training loss: 0.011392971500754356 = 0.0044357022270560265 + 0.001 * 6.957269191741943
Epoch 800, val loss: 0.9961556792259216
Epoch 810, training loss: 0.011235794983804226 = 0.004270597361028194 + 0.001 * 6.9651970863342285
Epoch 810, val loss: 1.0018202066421509
Epoch 820, training loss: 0.011091379448771477 = 0.004114690702408552 + 0.001 * 6.976688861846924
Epoch 820, val loss: 1.0073879957199097
Epoch 830, training loss: 0.010920868255198002 = 0.003967465367168188 + 0.001 * 6.953402519226074
Epoch 830, val loss: 1.0128523111343384
Epoch 840, training loss: 0.010793915018439293 = 0.003828400745987892 + 0.001 * 6.965514183044434
Epoch 840, val loss: 1.0182452201843262
Epoch 850, training loss: 0.010655255988240242 = 0.003696848638355732 + 0.001 * 6.958406448364258
Epoch 850, val loss: 1.0235416889190674
Epoch 860, training loss: 0.010519731789827347 = 0.0035724437329918146 + 0.001 * 6.9472880363464355
Epoch 860, val loss: 1.028742790222168
Epoch 870, training loss: 0.010399715043604374 = 0.0034547150135040283 + 0.001 * 6.944999694824219
Epoch 870, val loss: 1.0338919162750244
Epoch 880, training loss: 0.010315828025341034 = 0.0033432140480726957 + 0.001 * 6.972613334655762
Epoch 880, val loss: 1.038926362991333
Epoch 890, training loss: 0.010173044167459011 = 0.003237595083191991 + 0.001 * 6.935449123382568
Epoch 890, val loss: 1.0438896417617798
Epoch 900, training loss: 0.01007777638733387 = 0.0031374311074614525 + 0.001 * 6.940345287322998
Epoch 900, val loss: 1.048718810081482
Epoch 910, training loss: 0.01001670677214861 = 0.0030423880089074373 + 0.001 * 6.974318504333496
Epoch 910, val loss: 1.053528904914856
Epoch 920, training loss: 0.009908090345561504 = 0.0029521742835640907 + 0.001 * 6.955915927886963
Epoch 920, val loss: 1.0582247972488403
Epoch 930, training loss: 0.009797215461730957 = 0.002866413677111268 + 0.001 * 6.9308013916015625
Epoch 930, val loss: 1.0628235340118408
Epoch 940, training loss: 0.009721165522933006 = 0.0027848449535667896 + 0.001 * 6.936320781707764
Epoch 940, val loss: 1.0673342943191528
Epoch 950, training loss: 0.009644575417041779 = 0.0027071519289165735 + 0.001 * 6.937423229217529
Epoch 950, val loss: 1.0717966556549072
Epoch 960, training loss: 0.00955745205283165 = 0.0026331553235650063 + 0.001 * 6.9242963790893555
Epoch 960, val loss: 1.0761703252792358
Epoch 970, training loss: 0.009489831514656544 = 0.0025626644492149353 + 0.001 * 6.927166938781738
Epoch 970, val loss: 1.0804883241653442
Epoch 980, training loss: 0.009424101561307907 = 0.0024954713881015778 + 0.001 * 6.9286298751831055
Epoch 980, val loss: 1.0846928358078003
Epoch 990, training loss: 0.009400780312716961 = 0.0024313468020409346 + 0.001 * 6.969432830810547
Epoch 990, val loss: 1.088873267173767
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9151
Flip ASR: 0.8978/225 nodes
The final ASR:0.72079, 0.24172, Accuracy:0.78642, 0.04291
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11656])
remove edge: torch.Size([2, 9554])
updated graph: torch.Size([2, 10654])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
The final ASR:0.97417, 0.00301, Accuracy:0.83457, 0.00349
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9541451930999756 = 1.9457714557647705 + 0.001 * 8.373716354370117
Epoch 0, val loss: 1.9445490837097168
Epoch 10, training loss: 1.9440703392028809 = 1.9356967210769653 + 0.001 * 8.373648643493652
Epoch 10, val loss: 1.9342631101608276
Epoch 20, training loss: 1.932023048400879 = 1.9236496686935425 + 0.001 * 8.3733549118042
Epoch 20, val loss: 1.921777367591858
Epoch 30, training loss: 1.915417194366455 = 1.9070444107055664 + 0.001 * 8.372743606567383
Epoch 30, val loss: 1.9045674800872803
Epoch 40, training loss: 1.8910236358642578 = 1.8826521635055542 + 0.001 * 8.3714599609375
Epoch 40, val loss: 1.8798153400421143
Epoch 50, training loss: 1.8563448190689087 = 1.847976803779602 + 0.001 * 8.368064880371094
Epoch 50, val loss: 1.846361517906189
Epoch 60, training loss: 1.8144917488098145 = 1.806136965751648 + 0.001 * 8.354766845703125
Epoch 60, val loss: 1.8101189136505127
Epoch 70, training loss: 1.774513840675354 = 1.766252040863037 + 0.001 * 8.261805534362793
Epoch 70, val loss: 1.7799479961395264
Epoch 80, training loss: 1.727500081062317 = 1.7196474075317383 + 0.001 * 7.852723121643066
Epoch 80, val loss: 1.7419456243515015
Epoch 90, training loss: 1.6626462936401367 = 1.6549999713897705 + 0.001 * 7.646352767944336
Epoch 90, val loss: 1.6875040531158447
Epoch 100, training loss: 1.5776381492614746 = 1.570251703262329 + 0.001 * 7.386446475982666
Epoch 100, val loss: 1.6185460090637207
Epoch 110, training loss: 1.4790116548538208 = 1.4718940258026123 + 0.001 * 7.117635726928711
Epoch 110, val loss: 1.5408076047897339
Epoch 120, training loss: 1.377354383468628 = 1.3702354431152344 + 0.001 * 7.118908882141113
Epoch 120, val loss: 1.4614498615264893
Epoch 130, training loss: 1.2763794660568237 = 1.269283413887024 + 0.001 * 7.096043586730957
Epoch 130, val loss: 1.3843905925750732
Epoch 140, training loss: 1.1764400005340576 = 1.1693503856658936 + 0.001 * 7.089658737182617
Epoch 140, val loss: 1.3099488019943237
Epoch 150, training loss: 1.0783822536468506 = 1.071299433708191 + 0.001 * 7.082767009735107
Epoch 150, val loss: 1.2370491027832031
Epoch 160, training loss: 0.9834140539169312 = 0.9763423800468445 + 0.001 * 7.071676731109619
Epoch 160, val loss: 1.1657041311264038
Epoch 170, training loss: 0.8924715518951416 = 0.8854145407676697 + 0.001 * 7.0569939613342285
Epoch 170, val loss: 1.0963783264160156
Epoch 180, training loss: 0.8063277006149292 = 0.7992913126945496 + 0.001 * 7.036376953125
Epoch 180, val loss: 1.029970645904541
Epoch 190, training loss: 0.7260035872459412 = 0.7189956307411194 + 0.001 * 7.007957458496094
Epoch 190, val loss: 0.9685491323471069
Epoch 200, training loss: 0.6523866057395935 = 0.645412027835846 + 0.001 * 6.9745965003967285
Epoch 200, val loss: 0.9141570329666138
Epoch 210, training loss: 0.5857658386230469 = 0.578818142414093 + 0.001 * 6.947701454162598
Epoch 210, val loss: 0.8679481148719788
Epoch 220, training loss: 0.5255563259124756 = 0.5186241269111633 + 0.001 * 6.932193756103516
Epoch 220, val loss: 0.8300161361694336
Epoch 230, training loss: 0.47059252858161926 = 0.46367132663726807 + 0.001 * 6.921198844909668
Epoch 230, val loss: 0.7995322942733765
Epoch 240, training loss: 0.4196091890335083 = 0.41269975900650024 + 0.001 * 6.909438610076904
Epoch 240, val loss: 0.7761073112487793
Epoch 250, training loss: 0.371847540140152 = 0.36495184898376465 + 0.001 * 6.895695686340332
Epoch 250, val loss: 0.7592830657958984
Epoch 260, training loss: 0.32728102803230286 = 0.3204004168510437 + 0.001 * 6.880614757537842
Epoch 260, val loss: 0.748652458190918
Epoch 270, training loss: 0.2865157127380371 = 0.27964770793914795 + 0.001 * 6.86799955368042
Epoch 270, val loss: 0.7437636852264404
Epoch 280, training loss: 0.2502458393573761 = 0.2433890849351883 + 0.001 * 6.856743812561035
Epoch 280, val loss: 0.7442399263381958
Epoch 290, training loss: 0.2187860757112503 = 0.21193814277648926 + 0.001 * 6.847927570343018
Epoch 290, val loss: 0.749538779258728
Epoch 300, training loss: 0.1919194906949997 = 0.18508358299732208 + 0.001 * 6.8359055519104
Epoch 300, val loss: 0.7588140368461609
Epoch 310, training loss: 0.16911093890666962 = 0.16228026151657104 + 0.001 * 6.830682277679443
Epoch 310, val loss: 0.7713558673858643
Epoch 320, training loss: 0.14970549941062927 = 0.1428774744272232 + 0.001 * 6.828022480010986
Epoch 320, val loss: 0.7865844368934631
Epoch 330, training loss: 0.13306918740272522 = 0.12624958157539368 + 0.001 * 6.819605827331543
Epoch 330, val loss: 0.8038954734802246
Epoch 340, training loss: 0.11871376633644104 = 0.11189798265695572 + 0.001 * 6.815783500671387
Epoch 340, val loss: 0.8227388262748718
Epoch 350, training loss: 0.1062542125582695 = 0.09944373369216919 + 0.001 * 6.810478210449219
Epoch 350, val loss: 0.8427157998085022
Epoch 360, training loss: 0.09540487825870514 = 0.08859992027282715 + 0.001 * 6.804957866668701
Epoch 360, val loss: 0.863484263420105
Epoch 370, training loss: 0.08594923466444016 = 0.07913775742053986 + 0.001 * 6.811478137969971
Epoch 370, val loss: 0.8847686052322388
Epoch 380, training loss: 0.07766611129045486 = 0.07086621969938278 + 0.001 * 6.799892902374268
Epoch 380, val loss: 0.9063741564750671
Epoch 390, training loss: 0.070424385368824 = 0.06362679600715637 + 0.001 * 6.7975897789001465
Epoch 390, val loss: 0.9281294941902161
Epoch 400, training loss: 0.06407692283391953 = 0.05728134885430336 + 0.001 * 6.7955708503723145
Epoch 400, val loss: 0.9499462246894836
Epoch 410, training loss: 0.05851165950298309 = 0.05171237140893936 + 0.001 * 6.799289703369141
Epoch 410, val loss: 0.9716764092445374
Epoch 420, training loss: 0.05361570045351982 = 0.046817876398563385 + 0.001 * 6.797824859619141
Epoch 420, val loss: 0.9931821227073669
Epoch 430, training loss: 0.04930505156517029 = 0.042510587722063065 + 0.001 * 6.794463157653809
Epoch 430, val loss: 1.0144751071929932
Epoch 440, training loss: 0.045503854751586914 = 0.03871079906821251 + 0.001 * 6.793057441711426
Epoch 440, val loss: 1.0353971719741821
Epoch 450, training loss: 0.04215191677212715 = 0.035352084785699844 + 0.001 * 6.799831390380859
Epoch 450, val loss: 1.0558809041976929
Epoch 460, training loss: 0.03917230665683746 = 0.03237634524703026 + 0.001 * 6.795961856842041
Epoch 460, val loss: 1.0759776830673218
Epoch 470, training loss: 0.036527641117572784 = 0.029733305796980858 + 0.001 * 6.794336318969727
Epoch 470, val loss: 1.0956004858016968
Epoch 480, training loss: 0.03417230024933815 = 0.027380945160984993 + 0.001 * 6.791354179382324
Epoch 480, val loss: 1.114746332168579
Epoch 490, training loss: 0.03208112716674805 = 0.025281153619289398 + 0.001 * 6.799972057342529
Epoch 490, val loss: 1.1333760023117065
Epoch 500, training loss: 0.03019612841308117 = 0.02340206503868103 + 0.001 * 6.794062614440918
Epoch 500, val loss: 1.1515312194824219
Epoch 510, training loss: 0.028506474569439888 = 0.021716192364692688 + 0.001 * 6.790282249450684
Epoch 510, val loss: 1.1692237854003906
Epoch 520, training loss: 0.026989279314875603 = 0.020199360325932503 + 0.001 * 6.789918422698975
Epoch 520, val loss: 1.1864103078842163
Epoch 530, training loss: 0.025629200041294098 = 0.018831787630915642 + 0.001 * 6.797412872314453
Epoch 530, val loss: 1.2031018733978271
Epoch 540, training loss: 0.024384982883930206 = 0.017594926059246063 + 0.001 * 6.790055751800537
Epoch 540, val loss: 1.2193573713302612
Epoch 550, training loss: 0.02326573058962822 = 0.01647382415831089 + 0.001 * 6.791906356811523
Epoch 550, val loss: 1.2351102828979492
Epoch 560, training loss: 0.022243373095989227 = 0.015455453656613827 + 0.001 * 6.787919998168945
Epoch 560, val loss: 1.250457525253296
Epoch 570, training loss: 0.02131730504333973 = 0.014528523199260235 + 0.001 * 6.788782119750977
Epoch 570, val loss: 1.2653836011886597
Epoch 580, training loss: 0.02047225832939148 = 0.013682249933481216 + 0.001 * 6.7900071144104
Epoch 580, val loss: 1.2798855304718018
Epoch 590, training loss: 0.019693594425916672 = 0.01290805172175169 + 0.001 * 6.785543441772461
Epoch 590, val loss: 1.2939728498458862
Epoch 600, training loss: 0.018986424431204796 = 0.01219810452312231 + 0.001 * 6.788319110870361
Epoch 600, val loss: 1.3076858520507812
Epoch 610, training loss: 0.018331041559576988 = 0.01154578197747469 + 0.001 * 6.785258769989014
Epoch 610, val loss: 1.3210755586624146
Epoch 620, training loss: 0.017729828134179115 = 0.010945228859782219 + 0.001 * 6.784599304199219
Epoch 620, val loss: 1.3341209888458252
Epoch 630, training loss: 0.017176257446408272 = 0.010391284711658955 + 0.001 * 6.784972190856934
Epoch 630, val loss: 1.346832275390625
Epoch 640, training loss: 0.01666385307908058 = 0.009879342280328274 + 0.001 * 6.784509658813477
Epoch 640, val loss: 1.3592101335525513
Epoch 650, training loss: 0.01618734374642372 = 0.009405367076396942 + 0.001 * 6.781976222991943
Epoch 650, val loss: 1.3713078498840332
Epoch 660, training loss: 0.01575217954814434 = 0.008965800516307354 + 0.001 * 6.786379337310791
Epoch 660, val loss: 1.383093237876892
Epoch 670, training loss: 0.01533904206007719 = 0.008557474240660667 + 0.001 * 6.781567573547363
Epoch 670, val loss: 1.3946151733398438
Epoch 680, training loss: 0.014957495965063572 = 0.008177530020475388 + 0.001 * 6.779965400695801
Epoch 680, val loss: 1.4058212041854858
Epoch 690, training loss: 0.014606230892241001 = 0.00782347097992897 + 0.001 * 6.782759666442871
Epoch 690, val loss: 1.4167790412902832
Epoch 700, training loss: 0.014269259758293629 = 0.007493050768971443 + 0.001 * 6.776208877563477
Epoch 700, val loss: 1.4274672269821167
Epoch 710, training loss: 0.013967890292406082 = 0.007184223271906376 + 0.001 * 6.783667087554932
Epoch 710, val loss: 1.437928318977356
Epoch 720, training loss: 0.01367134042084217 = 0.006895154248923063 + 0.001 * 6.776185989379883
Epoch 720, val loss: 1.4481043815612793
Epoch 730, training loss: 0.013398276641964912 = 0.006624225527048111 + 0.001 * 6.774051189422607
Epoch 730, val loss: 1.458070993423462
Epoch 740, training loss: 0.013148213736712933 = 0.006369980052113533 + 0.001 * 6.778233528137207
Epoch 740, val loss: 1.4678127765655518
Epoch 750, training loss: 0.01290902215987444 = 0.006131128408014774 + 0.001 * 6.777893543243408
Epoch 750, val loss: 1.477354884147644
Epoch 760, training loss: 0.01268338318914175 = 0.005906431004405022 + 0.001 * 6.776951789855957
Epoch 760, val loss: 1.4866909980773926
Epoch 770, training loss: 0.012469690293073654 = 0.005694858264178038 + 0.001 * 6.774832248687744
Epoch 770, val loss: 1.4958088397979736
Epoch 780, training loss: 0.012274002656340599 = 0.005495390389114618 + 0.001 * 6.77861213684082
Epoch 780, val loss: 1.504733920097351
Epoch 790, training loss: 0.012077773921191692 = 0.005307110492140055 + 0.001 * 6.770663261413574
Epoch 790, val loss: 1.5134732723236084
Epoch 800, training loss: 0.011896409094333649 = 0.005129205994307995 + 0.001 * 6.767202854156494
Epoch 800, val loss: 1.522023320198059
Epoch 810, training loss: 0.011735919862985611 = 0.004960937891155481 + 0.001 * 6.7749810218811035
Epoch 810, val loss: 1.530396580696106
Epoch 820, training loss: 0.011573037132620811 = 0.004801623523235321 + 0.001 * 6.771413326263428
Epoch 820, val loss: 1.5386055707931519
Epoch 830, training loss: 0.011415338143706322 = 0.004650646820664406 + 0.001 * 6.764690399169922
Epoch 830, val loss: 1.5466598272323608
Epoch 840, training loss: 0.01127873919904232 = 0.0045074583031237125 + 0.001 * 6.771280288696289
Epoch 840, val loss: 1.554575800895691
Epoch 850, training loss: 0.011138828471302986 = 0.004371512681245804 + 0.001 * 6.767315864562988
Epoch 850, val loss: 1.5623197555541992
Epoch 860, training loss: 0.011002715677022934 = 0.004242353606969118 + 0.001 * 6.760361194610596
Epoch 860, val loss: 1.5698996782302856
Epoch 870, training loss: 0.010880541056394577 = 0.004119520075619221 + 0.001 * 6.761021137237549
Epoch 870, val loss: 1.5773260593414307
Epoch 880, training loss: 0.010767132043838501 = 0.0040026162751019 + 0.001 * 6.764515399932861
Epoch 880, val loss: 1.5846425294876099
Epoch 890, training loss: 0.010648161172866821 = 0.00389125756919384 + 0.001 * 6.756902694702148
Epoch 890, val loss: 1.591813087463379
Epoch 900, training loss: 0.01055003609508276 = 0.003785115433856845 + 0.001 * 6.764920234680176
Epoch 900, val loss: 1.5988649129867554
Epoch 910, training loss: 0.010451601818203926 = 0.003683851333335042 + 0.001 * 6.767750263214111
Epoch 910, val loss: 1.6057592630386353
Epoch 920, training loss: 0.010346897877752781 = 0.00358716887421906 + 0.001 * 6.75972843170166
Epoch 920, val loss: 1.6125502586364746
Epoch 930, training loss: 0.01024946104735136 = 0.003494809614494443 + 0.001 * 6.7546515464782715
Epoch 930, val loss: 1.6191856861114502
Epoch 940, training loss: 0.010158101096749306 = 0.003406517906114459 + 0.001 * 6.751582622528076
Epoch 940, val loss: 1.6257281303405762
Epoch 950, training loss: 0.010072722099721432 = 0.0033220595214515924 + 0.001 * 6.750661849975586
Epoch 950, val loss: 1.632158637046814
Epoch 960, training loss: 0.009994346648454666 = 0.0032412188593298197 + 0.001 * 6.753127574920654
Epoch 960, val loss: 1.6384745836257935
Epoch 970, training loss: 0.009919676929712296 = 0.0031637928914278746 + 0.001 * 6.755883693695068
Epoch 970, val loss: 1.6446870565414429
Epoch 980, training loss: 0.00985277071595192 = 0.0030895976815372705 + 0.001 * 6.763172626495361
Epoch 980, val loss: 1.6507725715637207
Epoch 990, training loss: 0.009780636988580227 = 0.0030184416100382805 + 0.001 * 6.762195110321045
Epoch 990, val loss: 1.6567312479019165
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7963
Overall ASR: 0.6236
Flip ASR: 0.5556/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9385851621627808 = 1.9302113056182861 + 0.001 * 8.373831748962402
Epoch 0, val loss: 1.9287523031234741
Epoch 10, training loss: 1.9283844232559204 = 1.9200106859207153 + 0.001 * 8.373746871948242
Epoch 10, val loss: 1.9180307388305664
Epoch 20, training loss: 1.9157180786132812 = 1.9073445796966553 + 0.001 * 8.373539924621582
Epoch 20, val loss: 1.904739260673523
Epoch 30, training loss: 1.8982266187667847 = 1.8898534774780273 + 0.001 * 8.373169898986816
Epoch 30, val loss: 1.8865708112716675
Epoch 40, training loss: 1.873008131980896 = 1.864635705947876 + 0.001 * 8.372452735900879
Epoch 40, val loss: 1.8610392808914185
Epoch 50, training loss: 1.8385735750198364 = 1.8302028179168701 + 0.001 * 8.370737075805664
Epoch 50, val loss: 1.8280879259109497
Epoch 60, training loss: 1.799323558807373 = 1.7909584045410156 + 0.001 * 8.365097999572754
Epoch 60, val loss: 1.7940322160720825
Epoch 70, training loss: 1.758256196975708 = 1.749920129776001 + 0.001 * 8.3361177444458
Epoch 70, val loss: 1.7595478296279907
Epoch 80, training loss: 1.7019599676132202 = 1.6938436031341553 + 0.001 * 8.11638355255127
Epoch 80, val loss: 1.7111953496932983
Epoch 90, training loss: 1.6262714862823486 = 1.618490219116211 + 0.001 * 7.78125
Epoch 90, val loss: 1.649158000946045
Epoch 100, training loss: 1.5344372987747192 = 1.526699185371399 + 0.001 * 7.738142013549805
Epoch 100, val loss: 1.5762927532196045
Epoch 110, training loss: 1.438028335571289 = 1.4303745031356812 + 0.001 * 7.653857707977295
Epoch 110, val loss: 1.4998186826705933
Epoch 120, training loss: 1.3433234691619873 = 1.3358739614486694 + 0.001 * 7.449523448944092
Epoch 120, val loss: 1.4260703325271606
Epoch 130, training loss: 1.249153971672058 = 1.2417941093444824 + 0.001 * 7.359860420227051
Epoch 130, val loss: 1.3537042140960693
Epoch 140, training loss: 1.154176115989685 = 1.1468124389648438 + 0.001 * 7.363633155822754
Epoch 140, val loss: 1.2808369398117065
Epoch 150, training loss: 1.0600370168685913 = 1.0526989698410034 + 0.001 * 7.338085174560547
Epoch 150, val loss: 1.2097151279449463
Epoch 160, training loss: 0.9699015617370605 = 0.9625818729400635 + 0.001 * 7.319695949554443
Epoch 160, val loss: 1.1421900987625122
Epoch 170, training loss: 0.8862210512161255 = 0.8789316415786743 + 0.001 * 7.28938102722168
Epoch 170, val loss: 1.079864740371704
Epoch 180, training loss: 0.8106833100318909 = 0.8034477829933167 + 0.001 * 7.23550271987915
Epoch 180, val loss: 1.024457335472107
Epoch 190, training loss: 0.7442857623100281 = 0.7371358275413513 + 0.001 * 7.149927139282227
Epoch 190, val loss: 0.9776646494865417
Epoch 200, training loss: 0.6864327788352966 = 0.679327130317688 + 0.001 * 7.105624675750732
Epoch 200, val loss: 0.9396893978118896
Epoch 210, training loss: 0.6344894170761108 = 0.6273947954177856 + 0.001 * 7.094615936279297
Epoch 210, val loss: 0.9090484380722046
Epoch 220, training loss: 0.5851097106933594 = 0.5780282616615295 + 0.001 * 7.081474781036377
Epoch 220, val loss: 0.8830934762954712
Epoch 230, training loss: 0.5357277989387512 = 0.5286528468132019 + 0.001 * 7.074925899505615
Epoch 230, val loss: 0.8593894839286804
Epoch 240, training loss: 0.48534321784973145 = 0.4782748818397522 + 0.001 * 7.068347454071045
Epoch 240, val loss: 0.8367271423339844
Epoch 250, training loss: 0.4344092607498169 = 0.4273473918437958 + 0.001 * 7.06186056137085
Epoch 250, val loss: 0.8153275847434998
Epoch 260, training loss: 0.38467440009117126 = 0.3776208758354187 + 0.001 * 7.053532600402832
Epoch 260, val loss: 0.7969136238098145
Epoch 270, training loss: 0.33811429142951965 = 0.3310728371143341 + 0.001 * 7.041468620300293
Epoch 270, val loss: 0.7831207513809204
Epoch 280, training loss: 0.2960735261440277 = 0.2890501916408539 + 0.001 * 7.023322582244873
Epoch 280, val loss: 0.775340735912323
Epoch 290, training loss: 0.25875622034072876 = 0.2517516016960144 + 0.001 * 7.004624366760254
Epoch 290, val loss: 0.7730218172073364
Epoch 300, training loss: 0.22589564323425293 = 0.21891091763973236 + 0.001 * 6.9847259521484375
Epoch 300, val loss: 0.7749294638633728
Epoch 310, training loss: 0.1971709281206131 = 0.19020022451877594 + 0.001 * 6.970709800720215
Epoch 310, val loss: 0.7798765301704407
Epoch 320, training loss: 0.17223471403121948 = 0.16527143120765686 + 0.001 * 6.963289260864258
Epoch 320, val loss: 0.7874013185501099
Epoch 330, training loss: 0.15069060027599335 = 0.14373472332954407 + 0.001 * 6.955880165100098
Epoch 330, val loss: 0.7968006134033203
Epoch 340, training loss: 0.13212525844573975 = 0.12517252564430237 + 0.001 * 6.952730178833008
Epoch 340, val loss: 0.8076629638671875
Epoch 350, training loss: 0.11611627042293549 = 0.1091676875948906 + 0.001 * 6.948585033416748
Epoch 350, val loss: 0.8198082447052002
Epoch 360, training loss: 0.10231775045394897 = 0.09537670761346817 + 0.001 * 6.9410400390625
Epoch 360, val loss: 0.8329206705093384
Epoch 370, training loss: 0.09046322107315063 = 0.08352724462747574 + 0.001 * 6.935977458953857
Epoch 370, val loss: 0.8465856313705444
Epoch 380, training loss: 0.08029137551784515 = 0.07336578518152237 + 0.001 * 6.925590991973877
Epoch 380, val loss: 0.8605220317840576
Epoch 390, training loss: 0.07157891988754272 = 0.06466446071863174 + 0.001 * 6.914454936981201
Epoch 390, val loss: 0.8745926022529602
Epoch 400, training loss: 0.06412073969841003 = 0.05721128731966019 + 0.001 * 6.909449577331543
Epoch 400, val loss: 0.8887348175048828
Epoch 410, training loss: 0.057722169905900955 = 0.0508175790309906 + 0.001 * 6.9045891761779785
Epoch 410, val loss: 0.9028850197792053
Epoch 420, training loss: 0.05221893638372421 = 0.04532304033637047 + 0.001 * 6.8958940505981445
Epoch 420, val loss: 0.9169522523880005
Epoch 430, training loss: 0.0474754236638546 = 0.040582720190286636 + 0.001 * 6.892701625823975
Epoch 430, val loss: 0.9308958053588867
Epoch 440, training loss: 0.04336603358387947 = 0.03647569939494133 + 0.001 * 6.890333652496338
Epoch 440, val loss: 0.9446248412132263
Epoch 450, training loss: 0.0397944450378418 = 0.03290298953652382 + 0.001 * 6.891454696655273
Epoch 450, val loss: 0.9580976963043213
Epoch 460, training loss: 0.03667555749416351 = 0.029785431921482086 + 0.001 * 6.890124320983887
Epoch 460, val loss: 0.9712737202644348
Epoch 470, training loss: 0.03395175188779831 = 0.02705848217010498 + 0.001 * 6.893270492553711
Epoch 470, val loss: 0.9842323660850525
Epoch 480, training loss: 0.03155645728111267 = 0.02466571517288685 + 0.001 * 6.890741348266602
Epoch 480, val loss: 0.9968838095664978
Epoch 490, training loss: 0.02944779023528099 = 0.022559311240911484 + 0.001 * 6.888478755950928
Epoch 490, val loss: 1.009155511856079
Epoch 500, training loss: 0.027590734884142876 = 0.02070043422281742 + 0.001 * 6.8902997970581055
Epoch 500, val loss: 1.0210933685302734
Epoch 510, training loss: 0.025943197309970856 = 0.01905500888824463 + 0.001 * 6.888187408447266
Epoch 510, val loss: 1.0327460765838623
Epoch 520, training loss: 0.024483829736709595 = 0.017593739554286003 + 0.001 * 6.890089988708496
Epoch 520, val loss: 1.044094204902649
Epoch 530, training loss: 0.02317890338599682 = 0.016291577368974686 + 0.001 * 6.887326240539551
Epoch 530, val loss: 1.0551426410675049
Epoch 540, training loss: 0.022019270807504654 = 0.015127941966056824 + 0.001 * 6.89132833480835
Epoch 540, val loss: 1.0658694505691528
Epoch 550, training loss: 0.020972270518541336 = 0.01408449187874794 + 0.001 * 6.887778282165527
Epoch 550, val loss: 1.0762921571731567
Epoch 560, training loss: 0.020032087340950966 = 0.013145560398697853 + 0.001 * 6.886526107788086
Epoch 560, val loss: 1.086435079574585
Epoch 570, training loss: 0.019190387800335884 = 0.012298164889216423 + 0.001 * 6.892222881317139
Epoch 570, val loss: 1.0962728261947632
Epoch 580, training loss: 0.018417203798890114 = 0.011531385593116283 + 0.001 * 6.885818004608154
Epoch 580, val loss: 1.105870246887207
Epoch 590, training loss: 0.01771930418908596 = 0.010835317894816399 + 0.001 * 6.883985996246338
Epoch 590, val loss: 1.1151310205459595
Epoch 600, training loss: 0.017084971070289612 = 0.010201808996498585 + 0.001 * 6.8831610679626465
Epoch 600, val loss: 1.1241661310195923
Epoch 610, training loss: 0.01650659739971161 = 0.009623740799725056 + 0.001 * 6.882856369018555
Epoch 610, val loss: 1.132908821105957
Epoch 620, training loss: 0.015979818999767303 = 0.00909504946321249 + 0.001 * 6.884768486022949
Epoch 620, val loss: 1.1414426565170288
Epoch 630, training loss: 0.015491782687604427 = 0.008610274642705917 + 0.001 * 6.881507873535156
Epoch 630, val loss: 1.1497029066085815
Epoch 640, training loss: 0.01504833810031414 = 0.008164756931364536 + 0.001 * 6.883580684661865
Epoch 640, val loss: 1.157770037651062
Epoch 650, training loss: 0.014633763581514359 = 0.007754460442811251 + 0.001 * 6.879302501678467
Epoch 650, val loss: 1.1656410694122314
Epoch 660, training loss: 0.014258475974202156 = 0.007375777233392 + 0.001 * 6.882698059082031
Epoch 660, val loss: 1.1732535362243652
Epoch 670, training loss: 0.01390291191637516 = 0.007025591097772121 + 0.001 * 6.877320766448975
Epoch 670, val loss: 1.1807057857513428
Epoch 680, training loss: 0.013584470376372337 = 0.006701144389808178 + 0.001 * 6.883326053619385
Epoch 680, val loss: 1.1879255771636963
Epoch 690, training loss: 0.013277053833007812 = 0.006400024052709341 + 0.001 * 6.8770294189453125
Epoch 690, val loss: 1.194990873336792
Epoch 700, training loss: 0.012994073331356049 = 0.006120023783296347 + 0.001 * 6.874049663543701
Epoch 700, val loss: 1.2018436193466187
Epoch 710, training loss: 0.012749368324875832 = 0.005859227851033211 + 0.001 * 6.890140056610107
Epoch 710, val loss: 1.2085623741149902
Epoch 720, training loss: 0.01248855423182249 = 0.005615953356027603 + 0.001 * 6.872600555419922
Epoch 720, val loss: 1.2150743007659912
Epoch 730, training loss: 0.012260989286005497 = 0.005388516932725906 + 0.001 * 6.872471809387207
Epoch 730, val loss: 1.2214584350585938
Epoch 740, training loss: 0.012045742943882942 = 0.005175584461539984 + 0.001 * 6.8701581954956055
Epoch 740, val loss: 1.2276496887207031
Epoch 750, training loss: 0.01184639148414135 = 0.004976069554686546 + 0.001 * 6.870321273803711
Epoch 750, val loss: 1.2337355613708496
Epoch 760, training loss: 0.011656984686851501 = 0.004788870923221111 + 0.001 * 6.868113040924072
Epoch 760, val loss: 1.23963463306427
Epoch 770, training loss: 0.01149040088057518 = 0.004612998571246862 + 0.001 * 6.877401828765869
Epoch 770, val loss: 1.2454009056091309
Epoch 780, training loss: 0.011317823082208633 = 0.004447542130947113 + 0.001 * 6.870281219482422
Epoch 780, val loss: 1.2510490417480469
Epoch 790, training loss: 0.011157050728797913 = 0.004291716497391462 + 0.001 * 6.865333557128906
Epoch 790, val loss: 1.2565665245056152
Epoch 800, training loss: 0.011012502014636993 = 0.004144780803471804 + 0.001 * 6.8677215576171875
Epoch 800, val loss: 1.2619637250900269
Epoch 810, training loss: 0.010873028077185154 = 0.004006095230579376 + 0.001 * 6.866932392120361
Epoch 810, val loss: 1.2672441005706787
Epoch 820, training loss: 0.01073633786290884 = 0.003875045571476221 + 0.001 * 6.861291885375977
Epoch 820, val loss: 1.2724239826202393
Epoch 830, training loss: 0.010616599582135677 = 0.0037510914262384176 + 0.001 * 6.865508079528809
Epoch 830, val loss: 1.2774672508239746
Epoch 840, training loss: 0.01048972923308611 = 0.0036336968187242746 + 0.001 * 6.856031894683838
Epoch 840, val loss: 1.2824383974075317
Epoch 850, training loss: 0.010387147776782513 = 0.003522390266880393 + 0.001 * 6.864757061004639
Epoch 850, val loss: 1.2872850894927979
Epoch 860, training loss: 0.010279543697834015 = 0.0034167554695159197 + 0.001 * 6.862788200378418
Epoch 860, val loss: 1.292007327079773
Epoch 870, training loss: 0.010171572677791119 = 0.0033164399210363626 + 0.001 * 6.855132579803467
Epoch 870, val loss: 1.2966450452804565
Epoch 880, training loss: 0.010075842030346394 = 0.0032211043871939182 + 0.001 * 6.854737281799316
Epoch 880, val loss: 1.301181674003601
Epoch 890, training loss: 0.00998888723552227 = 0.003130423603579402 + 0.001 * 6.858463764190674
Epoch 890, val loss: 1.3056353330612183
Epoch 900, training loss: 0.009895948693156242 = 0.003044114215299487 + 0.001 * 6.851834297180176
Epoch 900, val loss: 1.3099919557571411
Epoch 910, training loss: 0.009813925251364708 = 0.002961901482194662 + 0.001 * 6.852024078369141
Epoch 910, val loss: 1.3142591714859009
Epoch 920, training loss: 0.009736019186675549 = 0.0028835395351052284 + 0.001 * 6.852479457855225
Epoch 920, val loss: 1.318445086479187
Epoch 930, training loss: 0.009665213525295258 = 0.002808747813105583 + 0.001 * 6.8564653396606445
Epoch 930, val loss: 1.3225675821304321
Epoch 940, training loss: 0.009589344263076782 = 0.0027373405173420906 + 0.001 * 6.852004051208496
Epoch 940, val loss: 1.3265941143035889
Epoch 950, training loss: 0.009514870122075081 = 0.002669125096872449 + 0.001 * 6.845744609832764
Epoch 950, val loss: 1.3305604457855225
Epoch 960, training loss: 0.009451235644519329 = 0.0026039229705929756 + 0.001 * 6.8473124504089355
Epoch 960, val loss: 1.334444284439087
Epoch 970, training loss: 0.009392543695867062 = 0.0025415222626179457 + 0.001 * 6.851020812988281
Epoch 970, val loss: 1.3382641077041626
Epoch 980, training loss: 0.009324970655143261 = 0.002481782576069236 + 0.001 * 6.84318733215332
Epoch 980, val loss: 1.3420060873031616
Epoch 990, training loss: 0.009275619871914387 = 0.002424531849101186 + 0.001 * 6.851088047027588
Epoch 990, val loss: 1.3456851243972778
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.4170
Flip ASR: 0.3200/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9532854557037354 = 1.9449115991592407 + 0.001 * 8.373817443847656
Epoch 0, val loss: 1.9451314210891724
Epoch 10, training loss: 1.9432029724121094 = 1.9348292350769043 + 0.001 * 8.373762130737305
Epoch 10, val loss: 1.9353628158569336
Epoch 20, training loss: 1.9312812089920044 = 1.9229077100753784 + 0.001 * 8.373547554016113
Epoch 20, val loss: 1.9232919216156006
Epoch 30, training loss: 1.9150716066360474 = 1.90669846534729 + 0.001 * 8.373117446899414
Epoch 30, val loss: 1.9065269231796265
Epoch 40, training loss: 1.8916043043136597 = 1.8832321166992188 + 0.001 * 8.372203826904297
Epoch 40, val loss: 1.8821372985839844
Epoch 50, training loss: 1.857841968536377 = 1.8494720458984375 + 0.001 * 8.369932174682617
Epoch 50, val loss: 1.8480550050735474
Epoch 60, training loss: 1.8148629665374756 = 1.8065004348754883 + 0.001 * 8.362509727478027
Epoch 60, val loss: 1.8077423572540283
Epoch 70, training loss: 1.7702898979187012 = 1.7619645595550537 + 0.001 * 8.325309753417969
Epoch 70, val loss: 1.7703638076782227
Epoch 80, training loss: 1.720794677734375 = 1.712756872177124 + 0.001 * 8.037766456604004
Epoch 80, val loss: 1.7290856838226318
Epoch 90, training loss: 1.6516693830490112 = 1.6440519094467163 + 0.001 * 7.617468357086182
Epoch 90, val loss: 1.6706054210662842
Epoch 100, training loss: 1.5593252182006836 = 1.551956295967102 + 0.001 * 7.368895053863525
Epoch 100, val loss: 1.594078779220581
Epoch 110, training loss: 1.444839596748352 = 1.43751859664917 + 0.001 * 7.3209638595581055
Epoch 110, val loss: 1.500449776649475
Epoch 120, training loss: 1.3194794654846191 = 1.3121931552886963 + 0.001 * 7.286349296569824
Epoch 120, val loss: 1.3989322185516357
Epoch 130, training loss: 1.1957049369812012 = 1.1884660720825195 + 0.001 * 7.238916397094727
Epoch 130, val loss: 1.300648808479309
Epoch 140, training loss: 1.080993890762329 = 1.0738074779510498 + 0.001 * 7.18640661239624
Epoch 140, val loss: 1.2112258672714233
Epoch 150, training loss: 0.9782371520996094 = 0.9711100459098816 + 0.001 * 7.127092361450195
Epoch 150, val loss: 1.132531762123108
Epoch 160, training loss: 0.8868235349655151 = 0.8797586560249329 + 0.001 * 7.064849853515625
Epoch 160, val loss: 1.0643589496612549
Epoch 170, training loss: 0.8049606680870056 = 0.7979521155357361 + 0.001 * 7.0085368156433105
Epoch 170, val loss: 1.0052416324615479
Epoch 180, training loss: 0.7312521934509277 = 0.724277913570404 + 0.001 * 6.974272727966309
Epoch 180, val loss: 0.9538489580154419
Epoch 190, training loss: 0.6648075580596924 = 0.6578406095504761 + 0.001 * 6.966954708099365
Epoch 190, val loss: 0.9097405076026917
Epoch 200, training loss: 0.6048486828804016 = 0.5978865027427673 + 0.001 * 6.962188720703125
Epoch 200, val loss: 0.87257319688797
Epoch 210, training loss: 0.5506153702735901 = 0.5436564087867737 + 0.001 * 6.958951950073242
Epoch 210, val loss: 0.8418021202087402
Epoch 220, training loss: 0.5014526844024658 = 0.4944968819618225 + 0.001 * 6.955785274505615
Epoch 220, val loss: 0.8171215653419495
Epoch 230, training loss: 0.45683348178863525 = 0.44988155364990234 + 0.001 * 6.951932430267334
Epoch 230, val loss: 0.7979215979576111
Epoch 240, training loss: 0.41620731353759766 = 0.40926024317741394 + 0.001 * 6.947068691253662
Epoch 240, val loss: 0.7834890484809875
Epoch 250, training loss: 0.3789147734642029 = 0.37197408080101013 + 0.001 * 6.940699577331543
Epoch 250, val loss: 0.7730087637901306
Epoch 260, training loss: 0.34416860342025757 = 0.3372310698032379 + 0.001 * 6.937519550323486
Epoch 260, val loss: 0.7654888033866882
Epoch 270, training loss: 0.311180055141449 = 0.3042530417442322 + 0.001 * 6.927026748657227
Epoch 270, val loss: 0.7600289583206177
Epoch 280, training loss: 0.27941766381263733 = 0.2725014090538025 + 0.001 * 6.916252136230469
Epoch 280, val loss: 0.7561356425285339
Epoch 290, training loss: 0.24876905977725983 = 0.2418590486049652 + 0.001 * 6.910009384155273
Epoch 290, val loss: 0.7536768913269043
Epoch 300, training loss: 0.21961890161037445 = 0.2127203643321991 + 0.001 * 6.8985419273376465
Epoch 300, val loss: 0.7527151107788086
Epoch 310, training loss: 0.1926274597644806 = 0.1857362985610962 + 0.001 * 6.8911590576171875
Epoch 310, val loss: 0.7535725831985474
Epoch 320, training loss: 0.1683778166770935 = 0.16149991750717163 + 0.001 * 6.877902984619141
Epoch 320, val loss: 0.7567044496536255
Epoch 330, training loss: 0.14717631042003632 = 0.14028947055339813 + 0.001 * 6.886834144592285
Epoch 330, val loss: 0.7624324560165405
Epoch 340, training loss: 0.128898486495018 = 0.12202981859445572 + 0.001 * 6.868667125701904
Epoch 340, val loss: 0.7708008885383606
Epoch 350, training loss: 0.11330482363700867 = 0.10644211620092392 + 0.001 * 6.86270809173584
Epoch 350, val loss: 0.7815432548522949
Epoch 360, training loss: 0.10001415759325027 = 0.09315623342990875 + 0.001 * 6.857922077178955
Epoch 360, val loss: 0.7943421602249146
Epoch 370, training loss: 0.08867698162794113 = 0.0818222239613533 + 0.001 * 6.854759216308594
Epoch 370, val loss: 0.8086935877799988
Epoch 380, training loss: 0.0790092945098877 = 0.07213661819696426 + 0.001 * 6.872677326202393
Epoch 380, val loss: 0.8240581750869751
Epoch 390, training loss: 0.07070759683847427 = 0.06385055184364319 + 0.001 * 6.857045650482178
Epoch 390, val loss: 0.8400422930717468
Epoch 400, training loss: 0.0636032298207283 = 0.05674928054213524 + 0.001 * 6.85394811630249
Epoch 400, val loss: 0.856202244758606
Epoch 410, training loss: 0.05749830976128578 = 0.05064759775996208 + 0.001 * 6.85071325302124
Epoch 410, val loss: 0.8722085356712341
Epoch 420, training loss: 0.05224061757326126 = 0.04538898915052414 + 0.001 * 6.851629257202148
Epoch 420, val loss: 0.8879371881484985
Epoch 430, training loss: 0.04769251123070717 = 0.04084261879324913 + 0.001 * 6.849893569946289
Epoch 430, val loss: 0.9031983613967896
Epoch 440, training loss: 0.04374835640192032 = 0.036897819489240646 + 0.001 * 6.850537300109863
Epoch 440, val loss: 0.9179847240447998
Epoch 450, training loss: 0.04031632840633392 = 0.033462703227996826 + 0.001 * 6.85362434387207
Epoch 450, val loss: 0.9322824478149414
Epoch 460, training loss: 0.03731447085738182 = 0.030460361391305923 + 0.001 * 6.854107856750488
Epoch 460, val loss: 0.9461420178413391
Epoch 470, training loss: 0.03467662259936333 = 0.027826685458421707 + 0.001 * 6.849936485290527
Epoch 470, val loss: 0.9595282673835754
Epoch 480, training loss: 0.03235621005296707 = 0.025507312268018723 + 0.001 * 6.848898410797119
Epoch 480, val loss: 0.972483217716217
Epoch 490, training loss: 0.030304936692118645 = 0.0234568789601326 + 0.001 * 6.848056793212891
Epoch 490, val loss: 0.9850269556045532
Epoch 500, training loss: 0.028500625863671303 = 0.021638134494423866 + 0.001 * 6.862491607666016
Epoch 500, val loss: 0.9972133040428162
Epoch 510, training loss: 0.02686631679534912 = 0.020019274204969406 + 0.001 * 6.847041606903076
Epoch 510, val loss: 1.0089730024337769
Epoch 520, training loss: 0.025419460609555244 = 0.01857340708374977 + 0.001 * 6.846053123474121
Epoch 520, val loss: 1.0203865766525269
Epoch 530, training loss: 0.024122588336467743 = 0.017277969047427177 + 0.001 * 6.8446197509765625
Epoch 530, val loss: 1.0314186811447144
Epoch 540, training loss: 0.022959960624575615 = 0.0161137618124485 + 0.001 * 6.846199035644531
Epoch 540, val loss: 1.0421092510223389
Epoch 550, training loss: 0.021913502365350723 = 0.015064447186887264 + 0.001 * 6.849055767059326
Epoch 550, val loss: 1.0524687767028809
Epoch 560, training loss: 0.020962027832865715 = 0.014115972444415092 + 0.001 * 6.846055507659912
Epoch 560, val loss: 1.0624943971633911
Epoch 570, training loss: 0.020098714157938957 = 0.013256247155368328 + 0.001 * 6.842467308044434
Epoch 570, val loss: 1.07222580909729
Epoch 580, training loss: 0.019319474697113037 = 0.012474832125008106 + 0.001 * 6.8446431159973145
Epoch 580, val loss: 1.08167564868927
Epoch 590, training loss: 0.018603403121232986 = 0.011762736365199089 + 0.001 * 6.840665817260742
Epoch 590, val loss: 1.090861201286316
Epoch 600, training loss: 0.01795116625726223 = 0.011112150736153126 + 0.001 * 6.839015007019043
Epoch 600, val loss: 1.0997741222381592
Epoch 610, training loss: 0.017355477437376976 = 0.010516416281461716 + 0.001 * 6.839061260223389
Epoch 610, val loss: 1.1084494590759277
Epoch 620, training loss: 0.01681053452193737 = 0.009969535283744335 + 0.001 * 6.840999603271484
Epoch 620, val loss: 1.1168776750564575
Epoch 630, training loss: 0.01630430668592453 = 0.009466399438679218 + 0.001 * 6.837907791137695
Epoch 630, val loss: 1.125074863433838
Epoch 640, training loss: 0.015840880572795868 = 0.009002575650811195 + 0.001 * 6.8383049964904785
Epoch 640, val loss: 1.1330411434173584
Epoch 650, training loss: 0.015409570187330246 = 0.008574067614972591 + 0.001 * 6.835501670837402
Epoch 650, val loss: 1.1408112049102783
Epoch 660, training loss: 0.015012226998806 = 0.008177408017218113 + 0.001 * 6.834818363189697
Epoch 660, val loss: 1.1483699083328247
Epoch 670, training loss: 0.014646927826106548 = 0.007809517439454794 + 0.001 * 6.837409973144531
Epoch 670, val loss: 1.1557406187057495
Epoch 680, training loss: 0.014301445335149765 = 0.007467683870345354 + 0.001 * 6.833761692047119
Epoch 680, val loss: 1.1629321575164795
Epoch 690, training loss: 0.013982798904180527 = 0.007149356883019209 + 0.001 * 6.833441734313965
Epoch 690, val loss: 1.1699340343475342
Epoch 700, training loss: 0.013685164973139763 = 0.006851815618574619 + 0.001 * 6.833348751068115
Epoch 700, val loss: 1.1767897605895996
Epoch 710, training loss: 0.013401041738688946 = 0.006571620237082243 + 0.001 * 6.829421043395996
Epoch 710, val loss: 1.183508038520813
Epoch 720, training loss: 0.013139193877577782 = 0.0063048494048416615 + 0.001 * 6.834343910217285
Epoch 720, val loss: 1.1901379823684692
Epoch 730, training loss: 0.012883653864264488 = 0.006048756651580334 + 0.001 * 6.834897518157959
Epoch 730, val loss: 1.196694254875183
Epoch 740, training loss: 0.012628043070435524 = 0.005801897495985031 + 0.001 * 6.826145648956299
Epoch 740, val loss: 1.203268051147461
Epoch 750, training loss: 0.012397069483995438 = 0.005564126651734114 + 0.001 * 6.832942485809326
Epoch 750, val loss: 1.2098746299743652
Epoch 760, training loss: 0.012171037495136261 = 0.005335951689630747 + 0.001 * 6.835085391998291
Epoch 760, val loss: 1.2165484428405762
Epoch 770, training loss: 0.011948604136705399 = 0.005117686465382576 + 0.001 * 6.830916881561279
Epoch 770, val loss: 1.2232999801635742
Epoch 780, training loss: 0.01173547375947237 = 0.0049095144495368 + 0.001 * 6.825959205627441
Epoch 780, val loss: 1.2300441265106201
Epoch 790, training loss: 0.011529446579515934 = 0.004711533430963755 + 0.001 * 6.817913055419922
Epoch 790, val loss: 1.2368247509002686
Epoch 800, training loss: 0.01134187076240778 = 0.0045234691351652145 + 0.001 * 6.818401336669922
Epoch 800, val loss: 1.2436130046844482
Epoch 810, training loss: 0.011177416890859604 = 0.004345193970948458 + 0.001 * 6.832221984863281
Epoch 810, val loss: 1.2503513097763062
Epoch 820, training loss: 0.010998735204339027 = 0.004176358692348003 + 0.001 * 6.822376251220703
Epoch 820, val loss: 1.2570583820343018
Epoch 830, training loss: 0.010827595368027687 = 0.004016300663352013 + 0.001 * 6.8112945556640625
Epoch 830, val loss: 1.263687252998352
Epoch 840, training loss: 0.010678157210350037 = 0.0038641656283289194 + 0.001 * 6.813991546630859
Epoch 840, val loss: 1.27032470703125
Epoch 850, training loss: 0.010535415261983871 = 0.003719079541042447 + 0.001 * 6.816335678100586
Epoch 850, val loss: 1.276903748512268
Epoch 860, training loss: 0.010393362492322922 = 0.0035803988575935364 + 0.001 * 6.812963008880615
Epoch 860, val loss: 1.2834482192993164
Epoch 870, training loss: 0.010284893214702606 = 0.00344750564545393 + 0.001 * 6.837387561798096
Epoch 870, val loss: 1.289982557296753
Epoch 880, training loss: 0.01013557892292738 = 0.003320322372019291 + 0.001 * 6.815256118774414
Epoch 880, val loss: 1.2964352369308472
Epoch 890, training loss: 0.010012407787144184 = 0.003198749152943492 + 0.001 * 6.813658714294434
Epoch 890, val loss: 1.302897572517395
Epoch 900, training loss: 0.009890341199934483 = 0.003082704497501254 + 0.001 * 6.807636260986328
Epoch 900, val loss: 1.3092765808105469
Epoch 910, training loss: 0.009795408695936203 = 0.002972015179693699 + 0.001 * 6.823393821716309
Epoch 910, val loss: 1.3156318664550781
Epoch 920, training loss: 0.009681662544608116 = 0.00286659668199718 + 0.001 * 6.815065383911133
Epoch 920, val loss: 1.3219099044799805
Epoch 930, training loss: 0.009563086554408073 = 0.0027663097716867924 + 0.001 * 6.79677677154541
Epoch 930, val loss: 1.3281364440917969
Epoch 940, training loss: 0.009480791166424751 = 0.002670952118933201 + 0.001 * 6.809838771820068
Epoch 940, val loss: 1.3342559337615967
Epoch 950, training loss: 0.009378314018249512 = 0.002580434549599886 + 0.001 * 6.797879695892334
Epoch 950, val loss: 1.3403725624084473
Epoch 960, training loss: 0.009302325546741486 = 0.002494579181075096 + 0.001 * 6.807745933532715
Epoch 960, val loss: 1.346371054649353
Epoch 970, training loss: 0.009214434772729874 = 0.0024131019599735737 + 0.001 * 6.801332473754883
Epoch 970, val loss: 1.352332592010498
Epoch 980, training loss: 0.009132378734648228 = 0.0023356724996119738 + 0.001 * 6.796706199645996
Epoch 980, val loss: 1.358165979385376
Epoch 990, training loss: 0.009056609123945236 = 0.002262193476781249 + 0.001 * 6.79441499710083
Epoch 990, val loss: 1.3639124631881714
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.8413
Flip ASR: 0.8089/225 nodes
The final ASR:0.62731, 0.17326, Accuracy:0.80617, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11534])
remove edge: torch.Size([2, 9518])
updated graph: torch.Size([2, 10496])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.98893, 0.00904, Accuracy:0.83704, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9421910047531128 = 1.9338171482086182 + 0.001 * 8.37380313873291
Epoch 0, val loss: 1.9356926679611206
Epoch 10, training loss: 1.9331296682357788 = 1.9247559309005737 + 0.001 * 8.373696327209473
Epoch 10, val loss: 1.9264180660247803
Epoch 20, training loss: 1.9220513105392456 = 1.9136779308319092 + 0.001 * 8.37342643737793
Epoch 20, val loss: 1.9149361848831177
Epoch 30, training loss: 1.9066545963287354 = 1.8982816934585571 + 0.001 * 8.372937202453613
Epoch 30, val loss: 1.8989613056182861
Epoch 40, training loss: 1.883858561515808 = 1.8754864931106567 + 0.001 * 8.372037887573242
Epoch 40, val loss: 1.8756234645843506
Epoch 50, training loss: 1.8508583307266235 = 1.842488408088684 + 0.001 * 8.369935989379883
Epoch 50, val loss: 1.8433568477630615
Epoch 60, training loss: 1.8091274499893188 = 1.8007644414901733 + 0.001 * 8.36305046081543
Epoch 60, val loss: 1.806130290031433
Epoch 70, training loss: 1.7643141746520996 = 1.7559891939163208 + 0.001 * 8.325000762939453
Epoch 70, val loss: 1.7709282636642456
Epoch 80, training loss: 1.708412528038025 = 1.7004367113113403 + 0.001 * 7.97586727142334
Epoch 80, val loss: 1.7268102169036865
Epoch 90, training loss: 1.6308825016021729 = 1.6234053373336792 + 0.001 * 7.477149963378906
Epoch 90, val loss: 1.6629362106323242
Epoch 100, training loss: 1.5302598476409912 = 1.522944450378418 + 0.001 * 7.315408229827881
Epoch 100, val loss: 1.5812172889709473
Epoch 110, training loss: 1.4128003120422363 = 1.4055267572402954 + 0.001 * 7.273599624633789
Epoch 110, val loss: 1.4896584749221802
Epoch 120, training loss: 1.2913328409194946 = 1.284106969833374 + 0.001 * 7.22589635848999
Epoch 120, val loss: 1.3943341970443726
Epoch 130, training loss: 1.1766047477722168 = 1.1694350242614746 + 0.001 * 7.169759273529053
Epoch 130, val loss: 1.3063867092132568
Epoch 140, training loss: 1.0729742050170898 = 1.0658695697784424 + 0.001 * 7.104674339294434
Epoch 140, val loss: 1.2280266284942627
Epoch 150, training loss: 0.9804784655570984 = 0.9734116792678833 + 0.001 * 7.066799163818359
Epoch 150, val loss: 1.1599680185317993
Epoch 160, training loss: 0.8977175354957581 = 0.8906605839729309 + 0.001 * 7.056955814361572
Epoch 160, val loss: 1.100776195526123
Epoch 170, training loss: 0.8228489756584167 = 0.8157972693443298 + 0.001 * 7.051730155944824
Epoch 170, val loss: 1.0478993654251099
Epoch 180, training loss: 0.7552286386489868 = 0.7481873631477356 + 0.001 * 7.041293144226074
Epoch 180, val loss: 1.0008090734481812
Epoch 190, training loss: 0.6947922706604004 = 0.6877627968788147 + 0.001 * 7.029474258422852
Epoch 190, val loss: 0.9603465795516968
Epoch 200, training loss: 0.6409003138542175 = 0.633887767791748 + 0.001 * 7.012542247772217
Epoch 200, val loss: 0.9269526600837708
Epoch 210, training loss: 0.592078685760498 = 0.5850905179977417 + 0.001 * 6.988166809082031
Epoch 210, val loss: 0.9003691077232361
Epoch 220, training loss: 0.5465900301933289 = 0.5396331548690796 + 0.001 * 6.956876277923584
Epoch 220, val loss: 0.8792917132377625
Epoch 230, training loss: 0.5030936002731323 = 0.49616217613220215 + 0.001 * 6.931404113769531
Epoch 230, val loss: 0.8622419834136963
Epoch 240, training loss: 0.46067920327186584 = 0.453771710395813 + 0.001 * 6.907503604888916
Epoch 240, val loss: 0.8478959798812866
Epoch 250, training loss: 0.4192284345626831 = 0.4123362898826599 + 0.001 * 6.892145156860352
Epoch 250, val loss: 0.8363646864891052
Epoch 260, training loss: 0.3790181279182434 = 0.37213560938835144 + 0.001 * 6.882510185241699
Epoch 260, val loss: 0.8281286954879761
Epoch 270, training loss: 0.34048712253570557 = 0.33361661434173584 + 0.001 * 6.870498180389404
Epoch 270, val loss: 0.8232395648956299
Epoch 280, training loss: 0.3041050136089325 = 0.297246515750885 + 0.001 * 6.858485698699951
Epoch 280, val loss: 0.8212193250656128
Epoch 290, training loss: 0.27019068598747253 = 0.263338565826416 + 0.001 * 6.852116584777832
Epoch 290, val loss: 0.8218995332717896
Epoch 300, training loss: 0.2388909012079239 = 0.23205344378948212 + 0.001 * 6.83746337890625
Epoch 300, val loss: 0.8250053524971008
Epoch 310, training loss: 0.21028272807598114 = 0.20345623791217804 + 0.001 * 6.826492786407471
Epoch 310, val loss: 0.8300721049308777
Epoch 320, training loss: 0.1843184530735016 = 0.17749826610088348 + 0.001 * 6.820185661315918
Epoch 320, val loss: 0.8366307020187378
Epoch 330, training loss: 0.16098003089427948 = 0.1541655957698822 + 0.001 * 6.814437389373779
Epoch 330, val loss: 0.8443683981895447
Epoch 340, training loss: 0.14024117588996887 = 0.1334356963634491 + 0.001 * 6.805476188659668
Epoch 340, val loss: 0.8533122539520264
Epoch 350, training loss: 0.12207819521427155 = 0.11527223140001297 + 0.001 * 6.805962562561035
Epoch 350, val loss: 0.8633349537849426
Epoch 360, training loss: 0.10637828707695007 = 0.09958349913358688 + 0.001 * 6.794791221618652
Epoch 360, val loss: 0.8744670748710632
Epoch 370, training loss: 0.09301254898309708 = 0.08621075749397278 + 0.001 * 6.801792144775391
Epoch 370, val loss: 0.8865781426429749
Epoch 380, training loss: 0.0817098617553711 = 0.07492436468601227 + 0.001 * 6.785499572753906
Epoch 380, val loss: 0.8996798992156982
Epoch 390, training loss: 0.07224006205797195 = 0.0654517412185669 + 0.001 * 6.7883195877075195
Epoch 390, val loss: 0.9135012626647949
Epoch 400, training loss: 0.06429214030504227 = 0.05751550942659378 + 0.001 * 6.776632785797119
Epoch 400, val loss: 0.9278849363327026
Epoch 410, training loss: 0.05762844532728195 = 0.05085393786430359 + 0.001 * 6.774505138397217
Epoch 410, val loss: 0.9426694512367249
Epoch 420, training loss: 0.05202452465891838 = 0.04523878172039986 + 0.001 * 6.78574275970459
Epoch 420, val loss: 0.9576618075370789
Epoch 430, training loss: 0.04724910110235214 = 0.04047559201717377 + 0.001 * 6.773507118225098
Epoch 430, val loss: 0.9726654887199402
Epoch 440, training loss: 0.04317520186305046 = 0.03640737012028694 + 0.001 * 6.767831325531006
Epoch 440, val loss: 0.9876497387886047
Epoch 450, training loss: 0.039677608758211136 = 0.03290892392396927 + 0.001 * 6.768683910369873
Epoch 450, val loss: 1.0024957656860352
Epoch 460, training loss: 0.03663969039916992 = 0.029880618676543236 + 0.001 * 6.759069442749023
Epoch 460, val loss: 1.0171148777008057
Epoch 470, training loss: 0.0340004526078701 = 0.027242379263043404 + 0.001 * 6.758073806762695
Epoch 470, val loss: 1.031425952911377
Epoch 480, training loss: 0.031690433621406555 = 0.024931056424975395 + 0.001 * 6.759375095367432
Epoch 480, val loss: 1.0454246997833252
Epoch 490, training loss: 0.029658351093530655 = 0.0228965412825346 + 0.001 * 6.761809349060059
Epoch 490, val loss: 1.0590611696243286
Epoch 500, training loss: 0.02785429172217846 = 0.021097350865602493 + 0.001 * 6.756940841674805
Epoch 500, val loss: 1.0723991394042969
Epoch 510, training loss: 0.026248008012771606 = 0.019498933106660843 + 0.001 * 6.749075412750244
Epoch 510, val loss: 1.0853512287139893
Epoch 520, training loss: 0.024823101237416267 = 0.018073396757245064 + 0.001 * 6.749704360961914
Epoch 520, val loss: 1.0979779958724976
Epoch 530, training loss: 0.02356552891433239 = 0.01679757982492447 + 0.001 * 6.767948150634766
Epoch 530, val loss: 1.1102458238601685
Epoch 540, training loss: 0.022397901862859726 = 0.015651118010282516 + 0.001 * 6.74678373336792
Epoch 540, val loss: 1.12217378616333
Epoch 550, training loss: 0.021371109411120415 = 0.01461788546293974 + 0.001 * 6.753222942352295
Epoch 550, val loss: 1.1337560415267944
Epoch 560, training loss: 0.020426247268915176 = 0.013684061355888844 + 0.001 * 6.742184638977051
Epoch 560, val loss: 1.1450324058532715
Epoch 570, training loss: 0.019596368074417114 = 0.01283781137317419 + 0.001 * 6.758555889129639
Epoch 570, val loss: 1.1560440063476562
Epoch 580, training loss: 0.01881539076566696 = 0.012068902142345905 + 0.001 * 6.746488571166992
Epoch 580, val loss: 1.1667280197143555
Epoch 590, training loss: 0.018110036849975586 = 0.011368421837687492 + 0.001 * 6.741613864898682
Epoch 590, val loss: 1.1771527528762817
Epoch 600, training loss: 0.01747576706111431 = 0.010728660970926285 + 0.001 * 6.747106075286865
Epoch 600, val loss: 1.1872895956039429
Epoch 610, training loss: 0.01689133793115616 = 0.010142912156879902 + 0.001 * 6.748425483703613
Epoch 610, val loss: 1.1971511840820312
Epoch 620, training loss: 0.016345078125596046 = 0.009605498984456062 + 0.001 * 6.7395782470703125
Epoch 620, val loss: 1.2067725658416748
Epoch 630, training loss: 0.015852969139814377 = 0.00911132711917162 + 0.001 * 6.741641521453857
Epoch 630, val loss: 1.2161047458648682
Epoch 640, training loss: 0.015389848500490189 = 0.008655902929604053 + 0.001 * 6.733944892883301
Epoch 640, val loss: 1.2252031564712524
Epoch 650, training loss: 0.014973847195506096 = 0.008235340937972069 + 0.001 * 6.7385053634643555
Epoch 650, val loss: 1.234094500541687
Epoch 660, training loss: 0.014583428390324116 = 0.00784640945494175 + 0.001 * 6.737018585205078
Epoch 660, val loss: 1.2426962852478027
Epoch 670, training loss: 0.014243376441299915 = 0.007485661190003157 + 0.001 * 6.757714748382568
Epoch 670, val loss: 1.251151442527771
Epoch 680, training loss: 0.01388071570545435 = 0.007150307297706604 + 0.001 * 6.730408191680908
Epoch 680, val loss: 1.2593965530395508
Epoch 690, training loss: 0.013571755960583687 = 0.006838180590420961 + 0.001 * 6.733574390411377
Epoch 690, val loss: 1.2674113512039185
Epoch 700, training loss: 0.013276343233883381 = 0.006547136697918177 + 0.001 * 6.729206085205078
Epoch 700, val loss: 1.275239109992981
Epoch 710, training loss: 0.013011686503887177 = 0.006275542080402374 + 0.001 * 6.736144065856934
Epoch 710, val loss: 1.2828868627548218
Epoch 720, training loss: 0.012755481526255608 = 0.0060217007994651794 + 0.001 * 6.7337799072265625
Epoch 720, val loss: 1.290311574935913
Epoch 730, training loss: 0.012509440071880817 = 0.0057840547524392605 + 0.001 * 6.7253851890563965
Epoch 730, val loss: 1.297635555267334
Epoch 740, training loss: 0.012290388345718384 = 0.005561144556850195 + 0.001 * 6.729243755340576
Epoch 740, val loss: 1.304777979850769
Epoch 750, training loss: 0.01208653673529625 = 0.005351733416318893 + 0.001 * 6.734802722930908
Epoch 750, val loss: 1.3117393255233765
Epoch 760, training loss: 0.011877795681357384 = 0.005154517013579607 + 0.001 * 6.723278045654297
Epoch 760, val loss: 1.3186321258544922
Epoch 770, training loss: 0.011690758168697357 = 0.004968099761754274 + 0.001 * 6.722657680511475
Epoch 770, val loss: 1.325422763824463
Epoch 780, training loss: 0.011516289785504341 = 0.004791137762367725 + 0.001 * 6.725151062011719
Epoch 780, val loss: 1.3320993185043335
Epoch 790, training loss: 0.011351336725056171 = 0.004622499458491802 + 0.001 * 6.728837013244629
Epoch 790, val loss: 1.338729739189148
Epoch 800, training loss: 0.011191440746188164 = 0.004461401142179966 + 0.001 * 6.730039119720459
Epoch 800, val loss: 1.3453023433685303
Epoch 810, training loss: 0.011035306379199028 = 0.004307368770241737 + 0.001 * 6.727937698364258
Epoch 810, val loss: 1.3518203496932983
Epoch 820, training loss: 0.010883230715990067 = 0.004159992095082998 + 0.001 * 6.723238945007324
Epoch 820, val loss: 1.3582861423492432
Epoch 830, training loss: 0.010737496428191662 = 0.0040190196596086025 + 0.001 * 6.718476295471191
Epoch 830, val loss: 1.3646634817123413
Epoch 840, training loss: 0.010594358667731285 = 0.00388441514223814 + 0.001 * 6.7099432945251465
Epoch 840, val loss: 1.3709596395492554
Epoch 850, training loss: 0.010469228029251099 = 0.0037559284828603268 + 0.001 * 6.71329927444458
Epoch 850, val loss: 1.3772114515304565
Epoch 860, training loss: 0.010345948860049248 = 0.0036335226614028215 + 0.001 * 6.712425708770752
Epoch 860, val loss: 1.3833204507827759
Epoch 870, training loss: 0.010226555168628693 = 0.0035169317852705717 + 0.001 * 6.709623336791992
Epoch 870, val loss: 1.3893711566925049
Epoch 880, training loss: 0.010117394849658012 = 0.0034059130121022463 + 0.001 * 6.711481094360352
Epoch 880, val loss: 1.3953737020492554
Epoch 890, training loss: 0.010009290650486946 = 0.0033001950941979885 + 0.001 * 6.709095001220703
Epoch 890, val loss: 1.4012271165847778
Epoch 900, training loss: 0.009905781596899033 = 0.003199552418664098 + 0.001 * 6.706229209899902
Epoch 900, val loss: 1.406982183456421
Epoch 910, training loss: 0.009810327552258968 = 0.0031037626322358847 + 0.001 * 6.706564903259277
Epoch 910, val loss: 1.412647008895874
Epoch 920, training loss: 0.009736405685544014 = 0.0030125644989311695 + 0.001 * 6.723841190338135
Epoch 920, val loss: 1.4182099103927612
Epoch 930, training loss: 0.009627562947571278 = 0.0029257044661790133 + 0.001 * 6.7018585205078125
Epoch 930, val loss: 1.4236645698547363
Epoch 940, training loss: 0.009560748934745789 = 0.002842907328158617 + 0.001 * 6.717841148376465
Epoch 940, val loss: 1.4290250539779663
Epoch 950, training loss: 0.009475912898778915 = 0.0027640084736049175 + 0.001 * 6.711904525756836
Epoch 950, val loss: 1.434260606765747
Epoch 960, training loss: 0.009384333156049252 = 0.0026887566782534122 + 0.001 * 6.695576190948486
Epoch 960, val loss: 1.4394536018371582
Epoch 970, training loss: 0.009343363344669342 = 0.0026169545017182827 + 0.001 * 6.726408004760742
Epoch 970, val loss: 1.4445221424102783
Epoch 980, training loss: 0.009241804480552673 = 0.002548469929024577 + 0.001 * 6.693334579467773
Epoch 980, val loss: 1.4495075941085815
Epoch 990, training loss: 0.009185582399368286 = 0.0024830522015690804 + 0.001 * 6.7025299072265625
Epoch 990, val loss: 1.4544012546539307
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7778
Overall ASR: 0.5646
Flip ASR: 0.4800/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.945182204246521 = 1.936808466911316 + 0.001 * 8.373706817626953
Epoch 0, val loss: 1.9419399499893188
Epoch 10, training loss: 1.9353764057159424 = 1.9270029067993164 + 0.001 * 8.373536109924316
Epoch 10, val loss: 1.9309773445129395
Epoch 20, training loss: 1.9231846332550049 = 1.9148114919662476 + 0.001 * 8.373160362243652
Epoch 20, val loss: 1.9171264171600342
Epoch 30, training loss: 1.906266212463379 = 1.8978937864303589 + 0.001 * 8.372476577758789
Epoch 30, val loss: 1.897795557975769
Epoch 40, training loss: 1.881836175918579 = 1.8734650611877441 + 0.001 * 8.371086120605469
Epoch 40, val loss: 1.8700147867202759
Epoch 50, training loss: 1.8474633693695068 = 1.839095950126648 + 0.001 * 8.3673677444458
Epoch 50, val loss: 1.8319822549819946
Epoch 60, training loss: 1.8047266006469727 = 1.7963745594024658 + 0.001 * 8.352009773254395
Epoch 60, val loss: 1.7877696752548218
Epoch 70, training loss: 1.758661150932312 = 1.7504202127456665 + 0.001 * 8.240888595581055
Epoch 70, val loss: 1.744978427886963
Epoch 80, training loss: 1.7021852731704712 = 1.6945300102233887 + 0.001 * 7.6552324295043945
Epoch 80, val loss: 1.6970285177230835
Epoch 90, training loss: 1.6266473531723022 = 1.6193017959594727 + 0.001 * 7.34553337097168
Epoch 90, val loss: 1.6338768005371094
Epoch 100, training loss: 1.5302917957305908 = 1.5231292247772217 + 0.001 * 7.162519454956055
Epoch 100, val loss: 1.5526243448257446
Epoch 110, training loss: 1.4203660488128662 = 1.413283348083496 + 0.001 * 7.082647323608398
Epoch 110, val loss: 1.463371992111206
Epoch 120, training loss: 1.305564045906067 = 1.2984942197799683 + 0.001 * 7.06977653503418
Epoch 120, val loss: 1.373875379562378
Epoch 130, training loss: 1.1913319826126099 = 1.1842718124389648 + 0.001 * 7.060156345367432
Epoch 130, val loss: 1.2860510349273682
Epoch 140, training loss: 1.0814541578292847 = 1.0744000673294067 + 0.001 * 7.054086685180664
Epoch 140, val loss: 1.203013300895691
Epoch 150, training loss: 0.9798453450202942 = 0.9727957844734192 + 0.001 * 7.049569606781006
Epoch 150, val loss: 1.1271990537643433
Epoch 160, training loss: 0.8900114893913269 = 0.8829649686813354 + 0.001 * 7.0464935302734375
Epoch 160, val loss: 1.0624966621398926
Epoch 170, training loss: 0.8136097192764282 = 0.8065658807754517 + 0.001 * 7.04384183883667
Epoch 170, val loss: 1.0094999074935913
Epoch 180, training loss: 0.7496291995048523 = 0.7425879836082458 + 0.001 * 7.041223526000977
Epoch 180, val loss: 0.9666298031806946
Epoch 190, training loss: 0.6957311630249023 = 0.6886933445930481 + 0.001 * 7.037825584411621
Epoch 190, val loss: 0.9319245219230652
Epoch 200, training loss: 0.6489275097846985 = 0.6418941617012024 + 0.001 * 7.033360004425049
Epoch 200, val loss: 0.9035970568656921
Epoch 210, training loss: 0.6061744689941406 = 0.5991467237472534 + 0.001 * 7.02773904800415
Epoch 210, val loss: 0.8787053823471069
Epoch 220, training loss: 0.5650166869163513 = 0.557996392250061 + 0.001 * 7.020271301269531
Epoch 220, val loss: 0.8557228446006775
Epoch 230, training loss: 0.5238272547721863 = 0.5168174505233765 + 0.001 * 7.009823799133301
Epoch 230, val loss: 0.8336071968078613
Epoch 240, training loss: 0.48189160227775574 = 0.4748968183994293 + 0.001 * 6.994771957397461
Epoch 240, val loss: 0.8117491602897644
Epoch 250, training loss: 0.43932029604911804 = 0.43234771490097046 + 0.001 * 6.972592353820801
Epoch 250, val loss: 0.7904002666473389
Epoch 260, training loss: 0.3968522250652313 = 0.3899056911468506 + 0.001 * 6.946538925170898
Epoch 260, val loss: 0.7705661058425903
Epoch 270, training loss: 0.35539835691452026 = 0.3484741151332855 + 0.001 * 6.924239158630371
Epoch 270, val loss: 0.7531561851501465
Epoch 280, training loss: 0.31618404388427734 = 0.30928292870521545 + 0.001 * 6.901121139526367
Epoch 280, val loss: 0.739717960357666
Epoch 290, training loss: 0.28024527430534363 = 0.2733696401119232 + 0.001 * 6.875637054443359
Epoch 290, val loss: 0.7307242155075073
Epoch 300, training loss: 0.24802318215370178 = 0.24115781486034393 + 0.001 * 6.865368843078613
Epoch 300, val loss: 0.7260160446166992
Epoch 310, training loss: 0.21956689655780792 = 0.212712362408638 + 0.001 * 6.854531288146973
Epoch 310, val loss: 0.7251136302947998
Epoch 320, training loss: 0.19474303722381592 = 0.18789853155612946 + 0.001 * 6.844506740570068
Epoch 320, val loss: 0.7273966073989868
Epoch 330, training loss: 0.17323389649391174 = 0.16640214622020721 + 0.001 * 6.831753253936768
Epoch 330, val loss: 0.7322984933853149
Epoch 340, training loss: 0.1546407788991928 = 0.14780974388122559 + 0.001 * 6.831031322479248
Epoch 340, val loss: 0.7391502261161804
Epoch 350, training loss: 0.13857652246952057 = 0.13175269961357117 + 0.001 * 6.823823928833008
Epoch 350, val loss: 0.7474202513694763
Epoch 360, training loss: 0.1246505081653595 = 0.1178349107503891 + 0.001 * 6.81559944152832
Epoch 360, val loss: 0.7568473815917969
Epoch 370, training loss: 0.11249935626983643 = 0.10568787157535553 + 0.001 * 6.811488151550293
Epoch 370, val loss: 0.767284631729126
Epoch 380, training loss: 0.10176574438810349 = 0.09496226161718369 + 0.001 * 6.803483963012695
Epoch 380, val loss: 0.7781870365142822
Epoch 390, training loss: 0.09222935140132904 = 0.08541206270456314 + 0.001 * 6.817285060882568
Epoch 390, val loss: 0.7896132469177246
Epoch 400, training loss: 0.08363040536642075 = 0.07683306932449341 + 0.001 * 6.797338962554932
Epoch 400, val loss: 0.8014724254608154
Epoch 410, training loss: 0.07586117833852768 = 0.06906933337450027 + 0.001 * 6.791841983795166
Epoch 410, val loss: 0.8135117292404175
Epoch 420, training loss: 0.06880198419094086 = 0.062002141028642654 + 0.001 * 6.799840927124023
Epoch 420, val loss: 0.8258705735206604
Epoch 430, training loss: 0.06233438104391098 = 0.055540695786476135 + 0.001 * 6.793684005737305
Epoch 430, val loss: 0.8383790254592896
Epoch 440, training loss: 0.0564597025513649 = 0.04967278242111206 + 0.001 * 6.786918640136719
Epoch 440, val loss: 0.8512014150619507
Epoch 450, training loss: 0.05121130496263504 = 0.04441677778959274 + 0.001 * 6.794525146484375
Epoch 450, val loss: 0.8644230365753174
Epoch 460, training loss: 0.04659982770681381 = 0.0398174487054348 + 0.001 * 6.782378196716309
Epoch 460, val loss: 0.8782941102981567
Epoch 470, training loss: 0.04260654374957085 = 0.03583119064569473 + 0.001 * 6.775352478027344
Epoch 470, val loss: 0.8930665254592896
Epoch 480, training loss: 0.03915223106741905 = 0.03237433731555939 + 0.001 * 6.777894973754883
Epoch 480, val loss: 0.9075103998184204
Epoch 490, training loss: 0.036139339208602905 = 0.029369955882430077 + 0.001 * 6.769384860992432
Epoch 490, val loss: 0.9218387603759766
Epoch 500, training loss: 0.033519014716148376 = 0.026749061420559883 + 0.001 * 6.769951343536377
Epoch 500, val loss: 0.9357675313949585
Epoch 510, training loss: 0.03123643808066845 = 0.024456065148115158 + 0.001 * 6.780372142791748
Epoch 510, val loss: 0.9496127367019653
Epoch 520, training loss: 0.029206395149230957 = 0.022441288456320763 + 0.001 * 6.765105724334717
Epoch 520, val loss: 0.9629532694816589
Epoch 530, training loss: 0.027426302433013916 = 0.020662618800997734 + 0.001 * 6.7636823654174805
Epoch 530, val loss: 0.9759670495986938
Epoch 540, training loss: 0.02585904859006405 = 0.019086237996816635 + 0.001 * 6.772809982299805
Epoch 540, val loss: 0.9885324239730835
Epoch 550, training loss: 0.02444092556834221 = 0.017683198675513268 + 0.001 * 6.757727146148682
Epoch 550, val loss: 1.0008184909820557
Epoch 560, training loss: 0.023200340569019318 = 0.016429748386144638 + 0.001 * 6.770592212677002
Epoch 560, val loss: 1.0126559734344482
Epoch 570, training loss: 0.02206750027835369 = 0.01530579011887312 + 0.001 * 6.761710166931152
Epoch 570, val loss: 1.024208903312683
Epoch 580, training loss: 0.021055297926068306 = 0.014294282533228397 + 0.001 * 6.761014938354492
Epoch 580, val loss: 1.0354299545288086
Epoch 590, training loss: 0.020143624395132065 = 0.013380954973399639 + 0.001 * 6.762668609619141
Epoch 590, val loss: 1.046370267868042
Epoch 600, training loss: 0.019307591021060944 = 0.012553069740533829 + 0.001 * 6.75452184677124
Epoch 600, val loss: 1.056963324546814
Epoch 610, training loss: 0.018563592806458473 = 0.011800780892372131 + 0.001 * 6.76281213760376
Epoch 610, val loss: 1.0673294067382812
Epoch 620, training loss: 0.017853206023573875 = 0.0111120929941535 + 0.001 * 6.741112232208252
Epoch 620, val loss: 1.0773553848266602
Epoch 630, training loss: 0.01723051443696022 = 0.01048061903566122 + 0.001 * 6.749894142150879
Epoch 630, val loss: 1.0870575904846191
Epoch 640, training loss: 0.016639523208141327 = 0.009902752935886383 + 0.001 * 6.736769676208496
Epoch 640, val loss: 1.0965502262115479
Epoch 650, training loss: 0.016131624579429626 = 0.009374326094985008 + 0.001 * 6.757298946380615
Epoch 650, val loss: 1.1058294773101807
Epoch 660, training loss: 0.015621757134795189 = 0.008890972472727299 + 0.001 * 6.730783939361572
Epoch 660, val loss: 1.1149252653121948
Epoch 670, training loss: 0.015182152390480042 = 0.0084465853869915 + 0.001 * 6.735567092895508
Epoch 670, val loss: 1.1237176656723022
Epoch 680, training loss: 0.014777839183807373 = 0.008036542683839798 + 0.001 * 6.74129581451416
Epoch 680, val loss: 1.1322641372680664
Epoch 690, training loss: 0.01440752949565649 = 0.007657285779714584 + 0.001 * 6.750243186950684
Epoch 690, val loss: 1.1405478715896606
Epoch 700, training loss: 0.014043927192687988 = 0.007305411156266928 + 0.001 * 6.738516330718994
Epoch 700, val loss: 1.1486566066741943
Epoch 710, training loss: 0.013701362535357475 = 0.006977254059165716 + 0.001 * 6.7241082191467285
Epoch 710, val loss: 1.1566481590270996
Epoch 720, training loss: 0.013416163623332977 = 0.006667718756943941 + 0.001 * 6.748444080352783
Epoch 720, val loss: 1.1646064519882202
Epoch 730, training loss: 0.013104293495416641 = 0.006373630836606026 + 0.001 * 6.730661869049072
Epoch 730, val loss: 1.1724885702133179
Epoch 740, training loss: 0.012819685973227024 = 0.006094923242926598 + 0.001 * 6.724762439727783
Epoch 740, val loss: 1.1802937984466553
Epoch 750, training loss: 0.012552903033792973 = 0.00583160575479269 + 0.001 * 6.721296787261963
Epoch 750, val loss: 1.1880258321762085
Epoch 760, training loss: 0.012306900694966316 = 0.005584264174103737 + 0.001 * 6.722636699676514
Epoch 760, val loss: 1.1956645250320435
Epoch 770, training loss: 0.01207730546593666 = 0.005352465435862541 + 0.001 * 6.724839210510254
Epoch 770, val loss: 1.2032123804092407
Epoch 780, training loss: 0.011884165927767754 = 0.005134952254593372 + 0.001 * 6.749213218688965
Epoch 780, val loss: 1.2106379270553589
Epoch 790, training loss: 0.011645864695310593 = 0.004930793773382902 + 0.001 * 6.7150702476501465
Epoch 790, val loss: 1.2179430723190308
Epoch 800, training loss: 0.011479439213871956 = 0.0047392346896231174 + 0.001 * 6.740204334259033
Epoch 800, val loss: 1.2250510454177856
Epoch 810, training loss: 0.01127099059522152 = 0.004559410735964775 + 0.001 * 6.7115797996521
Epoch 810, val loss: 1.2319965362548828
Epoch 820, training loss: 0.011102193035185337 = 0.004390602931380272 + 0.001 * 6.711589813232422
Epoch 820, val loss: 1.2388595342636108
Epoch 830, training loss: 0.010955866426229477 = 0.004231883212924004 + 0.001 * 6.723983287811279
Epoch 830, val loss: 1.2455143928527832
Epoch 840, training loss: 0.010806821286678314 = 0.004082634113729 + 0.001 * 6.724186897277832
Epoch 840, val loss: 1.2520228624343872
Epoch 850, training loss: 0.010654116980731487 = 0.003942147362977266 + 0.001 * 6.711969375610352
Epoch 850, val loss: 1.25846266746521
Epoch 860, training loss: 0.010512314736843109 = 0.0038096923381090164 + 0.001 * 6.702621936798096
Epoch 860, val loss: 1.2646416425704956
Epoch 870, training loss: 0.010397521778941154 = 0.003684800583869219 + 0.001 * 6.7127203941345215
Epoch 870, val loss: 1.2707233428955078
Epoch 880, training loss: 0.010280631482601166 = 0.0035668164491653442 + 0.001 * 6.713815212249756
Epoch 880, val loss: 1.276687741279602
Epoch 890, training loss: 0.010157281532883644 = 0.0034553396981209517 + 0.001 * 6.70194149017334
Epoch 890, val loss: 1.282536506652832
Epoch 900, training loss: 0.010061590000987053 = 0.003349872073158622 + 0.001 * 6.71171760559082
Epoch 900, val loss: 1.2882813215255737
Epoch 910, training loss: 0.009949633851647377 = 0.003249998204410076 + 0.001 * 6.6996355056762695
Epoch 910, val loss: 1.2938770055770874
Epoch 920, training loss: 0.009871207177639008 = 0.0031553315930068493 + 0.001 * 6.715874671936035
Epoch 920, val loss: 1.2993532419204712
Epoch 930, training loss: 0.00976809673011303 = 0.00306551530957222 + 0.001 * 6.70258092880249
Epoch 930, val loss: 1.3047116994857788
Epoch 940, training loss: 0.00967810396105051 = 0.002980190794914961 + 0.001 * 6.697912693023682
Epoch 940, val loss: 1.3099555969238281
Epoch 950, training loss: 0.009590040892362595 = 0.0028990984428673983 + 0.001 * 6.690942287445068
Epoch 950, val loss: 1.3151088953018188
Epoch 960, training loss: 0.009512236341834068 = 0.00282193161547184 + 0.001 * 6.690304279327393
Epoch 960, val loss: 1.3201886415481567
Epoch 970, training loss: 0.009453605860471725 = 0.0027484302408993244 + 0.001 * 6.705175399780273
Epoch 970, val loss: 1.3250925540924072
Epoch 980, training loss: 0.009362946264445782 = 0.0026783235371112823 + 0.001 * 6.684622287750244
Epoch 980, val loss: 1.3299150466918945
Epoch 990, training loss: 0.009300803765654564 = 0.0026114708743989468 + 0.001 * 6.689332962036133
Epoch 990, val loss: 1.3346481323242188
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.5978
Flip ASR: 0.5422/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9617987871170044 = 1.9534250497817993 + 0.001 * 8.37375545501709
Epoch 0, val loss: 1.9501534700393677
Epoch 10, training loss: 1.951560616493225 = 1.9431869983673096 + 0.001 * 8.373666763305664
Epoch 10, val loss: 1.9404511451721191
Epoch 20, training loss: 1.9389578104019165 = 1.93058443069458 + 0.001 * 8.373400688171387
Epoch 20, val loss: 1.928080677986145
Epoch 30, training loss: 1.9212757349014282 = 1.91290283203125 + 0.001 * 8.37289810180664
Epoch 30, val loss: 1.9103235006332397
Epoch 40, training loss: 1.895084261894226 = 1.8867124319076538 + 0.001 * 8.371857643127441
Epoch 40, val loss: 1.8840126991271973
Epoch 50, training loss: 1.8579246997833252 = 1.8495556116104126 + 0.001 * 8.369041442871094
Epoch 50, val loss: 1.8481372594833374
Epoch 60, training loss: 1.8146735429763794 = 1.8063158988952637 + 0.001 * 8.357610702514648
Epoch 60, val loss: 1.8107528686523438
Epoch 70, training loss: 1.7771878242492676 = 1.768898367881775 + 0.001 * 8.289510726928711
Epoch 70, val loss: 1.7818878889083862
Epoch 80, training loss: 1.7341117858886719 = 1.7262071371078491 + 0.001 * 7.904603004455566
Epoch 80, val loss: 1.7454476356506348
Epoch 90, training loss: 1.675524353981018 = 1.6678645610809326 + 0.001 * 7.659826278686523
Epoch 90, val loss: 1.696437954902649
Epoch 100, training loss: 1.5988163948059082 = 1.5914306640625 + 0.001 * 7.385695457458496
Epoch 100, val loss: 1.6350674629211426
Epoch 110, training loss: 1.5096882581710815 = 1.5025782585144043 + 0.001 * 7.109996795654297
Epoch 110, val loss: 1.565051555633545
Epoch 120, training loss: 1.4186255931854248 = 1.4115349054336548 + 0.001 * 7.090649127960205
Epoch 120, val loss: 1.492140531539917
Epoch 130, training loss: 1.327864170074463 = 1.3207873106002808 + 0.001 * 7.076903820037842
Epoch 130, val loss: 1.4200365543365479
Epoch 140, training loss: 1.2340176105499268 = 1.226947546005249 + 0.001 * 7.070123195648193
Epoch 140, val loss: 1.3451056480407715
Epoch 150, training loss: 1.1346650123596191 = 1.1276038885116577 + 0.001 * 7.061101913452148
Epoch 150, val loss: 1.2644121646881104
Epoch 160, training loss: 1.031147837638855 = 1.0240978002548218 + 0.001 * 7.050043106079102
Epoch 160, val loss: 1.1802908182144165
Epoch 170, training loss: 0.9276109337806702 = 0.9205790758132935 + 0.001 * 7.031848430633545
Epoch 170, val loss: 1.096638560295105
Epoch 180, training loss: 0.8287806510925293 = 0.8217740654945374 + 0.001 * 7.00657320022583
Epoch 180, val loss: 1.0180126428604126
Epoch 190, training loss: 0.7392741441726685 = 0.7323017716407776 + 0.001 * 6.972383975982666
Epoch 190, val loss: 0.9486979246139526
Epoch 200, training loss: 0.6615039110183716 = 0.6545645594596863 + 0.001 * 6.9393720626831055
Epoch 200, val loss: 0.8916486501693726
Epoch 210, training loss: 0.595016598701477 = 0.5880894660949707 + 0.001 * 6.927124977111816
Epoch 210, val loss: 0.8465300798416138
Epoch 220, training loss: 0.5379519462585449 = 0.5310296416282654 + 0.001 * 6.922301769256592
Epoch 220, val loss: 0.8114242553710938
Epoch 230, training loss: 0.4879804253578186 = 0.48106181621551514 + 0.001 * 6.918607234954834
Epoch 230, val loss: 0.783942461013794
Epoch 240, training loss: 0.44278037548065186 = 0.4358660578727722 + 0.001 * 6.914329528808594
Epoch 240, val loss: 0.7618361711502075
Epoch 250, training loss: 0.40039145946502686 = 0.39348119497299194 + 0.001 * 6.910250186920166
Epoch 250, val loss: 0.7435110807418823
Epoch 260, training loss: 0.3594616651535034 = 0.3525555431842804 + 0.001 * 6.906110763549805
Epoch 260, val loss: 0.7277085185050964
Epoch 270, training loss: 0.3195231854915619 = 0.31262168288230896 + 0.001 * 6.901492595672607
Epoch 270, val loss: 0.7138871550559998
Epoch 280, training loss: 0.28093260526657104 = 0.2740364670753479 + 0.001 * 6.896140098571777
Epoch 280, val loss: 0.7018665671348572
Epoch 290, training loss: 0.24448272585868835 = 0.2375888228416443 + 0.001 * 6.893908500671387
Epoch 290, val loss: 0.6919730305671692
Epoch 300, training loss: 0.2110716998577118 = 0.2041882425546646 + 0.001 * 6.883453845977783
Epoch 300, val loss: 0.6845607161521912
Epoch 310, training loss: 0.18142588436603546 = 0.17454937100410461 + 0.001 * 6.876518249511719
Epoch 310, val loss: 0.6801698803901672
Epoch 320, training loss: 0.15580891072750092 = 0.1489410400390625 + 0.001 * 6.8678693771362305
Epoch 320, val loss: 0.6789932250976562
Epoch 330, training loss: 0.13409872353076935 = 0.1272386908531189 + 0.001 * 6.860032558441162
Epoch 330, val loss: 0.6810896992683411
Epoch 340, training loss: 0.11591199785470963 = 0.10906088352203369 + 0.001 * 6.851110935211182
Epoch 340, val loss: 0.6863008141517639
Epoch 350, training loss: 0.10076088458299637 = 0.093916155397892 + 0.001 * 6.844732284545898
Epoch 350, val loss: 0.694248378276825
Epoch 360, training loss: 0.08813998848199844 = 0.08131056278944016 + 0.001 * 6.829422473907471
Epoch 360, val loss: 0.7043863534927368
Epoch 370, training loss: 0.07764387130737305 = 0.0708015114068985 + 0.001 * 6.842360496520996
Epoch 370, val loss: 0.7161396145820618
Epoch 380, training loss: 0.06883829087018967 = 0.06201056018471718 + 0.001 * 6.827732086181641
Epoch 380, val loss: 0.7290328741073608
Epoch 390, training loss: 0.06144670397043228 = 0.05462765693664551 + 0.001 * 6.819047927856445
Epoch 390, val loss: 0.74265056848526
Epoch 400, training loss: 0.05520804226398468 = 0.048400621861219406 + 0.001 * 6.807420253753662
Epoch 400, val loss: 0.7566468119621277
Epoch 410, training loss: 0.04992116242647171 = 0.04312162473797798 + 0.001 * 6.7995381355285645
Epoch 410, val loss: 0.770638108253479
Epoch 420, training loss: 0.045429009944200516 = 0.038623277097940445 + 0.001 * 6.805732727050781
Epoch 420, val loss: 0.7845126390457153
Epoch 430, training loss: 0.04158221185207367 = 0.03476816043257713 + 0.001 * 6.814050197601318
Epoch 430, val loss: 0.7981851696968079
Epoch 440, training loss: 0.038234174251556396 = 0.03144540265202522 + 0.001 * 6.78877067565918
Epoch 440, val loss: 0.811619758605957
Epoch 450, training loss: 0.03535022586584091 = 0.02856551669538021 + 0.001 * 6.784707069396973
Epoch 450, val loss: 0.8247290253639221
Epoch 460, training loss: 0.032859861850738525 = 0.026055967435240746 + 0.001 * 6.803894996643066
Epoch 460, val loss: 0.8374781608581543
Epoch 470, training loss: 0.03064192831516266 = 0.023858165368437767 + 0.001 * 6.783762454986572
Epoch 470, val loss: 0.8498977422714233
Epoch 480, training loss: 0.028704484924674034 = 0.021923817694187164 + 0.001 * 6.780667304992676
Epoch 480, val loss: 0.8619677424430847
Epoch 490, training loss: 0.026991331949830055 = 0.02021367847919464 + 0.001 * 6.777653694152832
Epoch 490, val loss: 0.8737151026725769
Epoch 500, training loss: 0.025480911135673523 = 0.018695594742894173 + 0.001 * 6.785315990447998
Epoch 500, val loss: 0.8851104378700256
Epoch 510, training loss: 0.024113081395626068 = 0.017342770472168922 + 0.001 * 6.770310401916504
Epoch 510, val loss: 0.8961845636367798
Epoch 520, training loss: 0.022937405854463577 = 0.016132593154907227 + 0.001 * 6.804811954498291
Epoch 520, val loss: 0.9069284200668335
Epoch 530, training loss: 0.021819787099957466 = 0.015045848675072193 + 0.001 * 6.773938179016113
Epoch 530, val loss: 0.9173566699028015
Epoch 540, training loss: 0.020840151235461235 = 0.014066630974411964 + 0.001 * 6.773520469665527
Epoch 540, val loss: 0.9275020360946655
Epoch 550, training loss: 0.019945399835705757 = 0.013181201182305813 + 0.001 * 6.764197826385498
Epoch 550, val loss: 0.937356173992157
Epoch 560, training loss: 0.019143078476190567 = 0.01237786840647459 + 0.001 * 6.765209674835205
Epoch 560, val loss: 0.9469525218009949
Epoch 570, training loss: 0.018403757363557816 = 0.011646561324596405 + 0.001 * 6.757196426391602
Epoch 570, val loss: 0.9562802910804749
Epoch 580, training loss: 0.017743371427059174 = 0.010976379737257957 + 0.001 * 6.766991138458252
Epoch 580, val loss: 0.9653667211532593
Epoch 590, training loss: 0.017120009288191795 = 0.01036114152520895 + 0.001 * 6.7588677406311035
Epoch 590, val loss: 0.9742343425750732
Epoch 600, training loss: 0.016558783128857613 = 0.009793947450816631 + 0.001 * 6.764834880828857
Epoch 600, val loss: 0.9829793572425842
Epoch 610, training loss: 0.016026657074689865 = 0.009270265698432922 + 0.001 * 6.75639009475708
Epoch 610, val loss: 0.9915677905082703
Epoch 620, training loss: 0.015542840585112572 = 0.008786196820437908 + 0.001 * 6.756643772125244
Epoch 620, val loss: 0.9999361634254456
Epoch 630, training loss: 0.015094693750143051 = 0.008338067680597305 + 0.001 * 6.756625652313232
Epoch 630, val loss: 1.0081406831741333
Epoch 640, training loss: 0.014674056321382523 = 0.007922790944576263 + 0.001 * 6.751264572143555
Epoch 640, val loss: 1.0161666870117188
Epoch 650, training loss: 0.014292960986495018 = 0.0075377123430371284 + 0.001 * 6.755248069763184
Epoch 650, val loss: 1.024067997932434
Epoch 660, training loss: 0.013935191556811333 = 0.007180176209658384 + 0.001 * 6.755014896392822
Epoch 660, val loss: 1.0318247079849243
Epoch 670, training loss: 0.013601680286228657 = 0.006847678683698177 + 0.001 * 6.754001140594482
Epoch 670, val loss: 1.039409875869751
Epoch 680, training loss: 0.013283873908221722 = 0.006538097746670246 + 0.001 * 6.7457756996154785
Epoch 680, val loss: 1.0468336343765259
Epoch 690, training loss: 0.012992670759558678 = 0.006249677389860153 + 0.001 * 6.742992877960205
Epoch 690, val loss: 1.054073691368103
Epoch 700, training loss: 0.012727513909339905 = 0.005980556830763817 + 0.001 * 6.7469563484191895
Epoch 700, val loss: 1.0611883401870728
Epoch 710, training loss: 0.0124848373234272 = 0.005729209631681442 + 0.001 * 6.755627632141113
Epoch 710, val loss: 1.068131685256958
Epoch 720, training loss: 0.012237440794706345 = 0.005494224838912487 + 0.001 * 6.743215084075928
Epoch 720, val loss: 1.074952483177185
Epoch 730, training loss: 0.01201539859175682 = 0.005274159833788872 + 0.001 * 6.741238594055176
Epoch 730, val loss: 1.0816301107406616
Epoch 740, training loss: 0.011818284168839455 = 0.005067818332463503 + 0.001 * 6.750464916229248
Epoch 740, val loss: 1.0881683826446533
Epoch 750, training loss: 0.0116193238645792 = 0.004874121863394976 + 0.001 * 6.745201110839844
Epoch 750, val loss: 1.0945947170257568
Epoch 760, training loss: 0.011448883451521397 = 0.00469211395829916 + 0.001 * 6.756769180297852
Epoch 760, val loss: 1.1008886098861694
Epoch 770, training loss: 0.011250194162130356 = 0.004520983900874853 + 0.001 * 6.729209899902344
Epoch 770, val loss: 1.1070120334625244
Epoch 780, training loss: 0.011102795600891113 = 0.00435986090451479 + 0.001 * 6.742934703826904
Epoch 780, val loss: 1.1130404472351074
Epoch 790, training loss: 0.01094566285610199 = 0.004207981284707785 + 0.001 * 6.737680912017822
Epoch 790, val loss: 1.118986964225769
Epoch 800, training loss: 0.01079691294580698 = 0.004064586479216814 + 0.001 * 6.732326030731201
Epoch 800, val loss: 1.1247888803482056
Epoch 810, training loss: 0.010657019913196564 = 0.0039291176944971085 + 0.001 * 6.727902412414551
Epoch 810, val loss: 1.1305017471313477
Epoch 820, training loss: 0.010522141121327877 = 0.003800978185608983 + 0.001 * 6.721162796020508
Epoch 820, val loss: 1.1360665559768677
Epoch 830, training loss: 0.010413874872028828 = 0.0036796790082007647 + 0.001 * 6.734195232391357
Epoch 830, val loss: 1.141575813293457
Epoch 840, training loss: 0.010290353558957577 = 0.0035647896584123373 + 0.001 * 6.7255635261535645
Epoch 840, val loss: 1.146950602531433
Epoch 850, training loss: 0.010209417901933193 = 0.0034558705519884825 + 0.001 * 6.753547191619873
Epoch 850, val loss: 1.1522345542907715
Epoch 860, training loss: 0.010069120675325394 = 0.003352463711053133 + 0.001 * 6.7166571617126465
Epoch 860, val loss: 1.15740168094635
Epoch 870, training loss: 0.009978166781365871 = 0.0032542096450924873 + 0.001 * 6.72395658493042
Epoch 870, val loss: 1.1624782085418701
Epoch 880, training loss: 0.009886987507343292 = 0.003160824067890644 + 0.001 * 6.726162910461426
Epoch 880, val loss: 1.1674848794937134
Epoch 890, training loss: 0.009788407012820244 = 0.0030719591304659843 + 0.001 * 6.716447353363037
Epoch 890, val loss: 1.1723840236663818
Epoch 900, training loss: 0.009702981449663639 = 0.0029873314779251814 + 0.001 * 6.715649604797363
Epoch 900, val loss: 1.1772154569625854
Epoch 910, training loss: 0.009616678580641747 = 0.002906682901084423 + 0.001 * 6.709994792938232
Epoch 910, val loss: 1.1819316148757935
Epoch 920, training loss: 0.009552892297506332 = 0.0028297933749854565 + 0.001 * 6.723099231719971
Epoch 920, val loss: 1.1866012811660767
Epoch 930, training loss: 0.009479393251240253 = 0.0027563837356865406 + 0.001 * 6.72300910949707
Epoch 930, val loss: 1.1911979913711548
Epoch 940, training loss: 0.009389897808432579 = 0.0026862851809710264 + 0.001 * 6.703611850738525
Epoch 940, val loss: 1.1957041025161743
Epoch 950, training loss: 0.009327556937932968 = 0.00261930120177567 + 0.001 * 6.708255767822266
Epoch 950, val loss: 1.2001298666000366
Epoch 960, training loss: 0.009301931597292423 = 0.0025552904698997736 + 0.001 * 6.746641159057617
Epoch 960, val loss: 1.2045221328735352
Epoch 970, training loss: 0.009201861917972565 = 0.0024940287694334984 + 0.001 * 6.707833290100098
Epoch 970, val loss: 1.2087956666946411
Epoch 980, training loss: 0.00913908425718546 = 0.0024353694170713425 + 0.001 * 6.703714370727539
Epoch 980, val loss: 1.213015079498291
Epoch 990, training loss: 0.009090335108339787 = 0.0023791680578142405 + 0.001 * 6.711166858673096
Epoch 990, val loss: 1.2171895503997803
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.8081
Flip ASR: 0.7733/225 nodes
The final ASR:0.65683, 0.10783, Accuracy:0.81235, 0.02463
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10518])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9668
Flip ASR: 0.9600/225 nodes
The final ASR:0.97171, 0.00348, Accuracy:0.83210, 0.00761
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9662868976593018 = 1.9579130411148071 + 0.001 * 8.373822212219238
Epoch 0, val loss: 1.9532066583633423
Epoch 10, training loss: 1.9561036825180054 = 1.9477299451828003 + 0.001 * 8.373736381530762
Epoch 10, val loss: 1.9439729452133179
Epoch 20, training loss: 1.9432060718536377 = 1.9348325729370117 + 0.001 * 8.373501777648926
Epoch 20, val loss: 1.9321722984313965
Epoch 30, training loss: 1.9247591495513916 = 1.9163861274719238 + 0.001 * 8.373071670532227
Epoch 30, val loss: 1.9150975942611694
Epoch 40, training loss: 1.8972840309143066 = 1.8889117240905762 + 0.001 * 8.372246742248535
Epoch 40, val loss: 1.8899177312850952
Epoch 50, training loss: 1.858502745628357 = 1.8501324653625488 + 0.001 * 8.37024211883545
Epoch 50, val loss: 1.8560962677001953
Epoch 60, training loss: 1.8144539594650269 = 1.8060904741287231 + 0.001 * 8.363526344299316
Epoch 60, val loss: 1.8213196992874146
Epoch 70, training loss: 1.7764972448349 = 1.768172264099121 + 0.001 * 8.325011253356934
Epoch 70, val loss: 1.7910610437393188
Epoch 80, training loss: 1.7321420907974243 = 1.7241228818893433 + 0.001 * 8.019261360168457
Epoch 80, val loss: 1.749189853668213
Epoch 90, training loss: 1.6709315776824951 = 1.6631509065628052 + 0.001 * 7.780679225921631
Epoch 90, val loss: 1.6952080726623535
Epoch 100, training loss: 1.589303731918335 = 1.581697702407837 + 0.001 * 7.606036186218262
Epoch 100, val loss: 1.6275770664215088
Epoch 110, training loss: 1.493321418762207 = 1.4859638214111328 + 0.001 * 7.357585906982422
Epoch 110, val loss: 1.5483485460281372
Epoch 120, training loss: 1.395531177520752 = 1.3882278203964233 + 0.001 * 7.303385257720947
Epoch 120, val loss: 1.4670674800872803
Epoch 130, training loss: 1.3006688356399536 = 1.2934579849243164 + 0.001 * 7.210855484008789
Epoch 130, val loss: 1.3885140419006348
Epoch 140, training loss: 1.207434892654419 = 1.200290560722351 + 0.001 * 7.14434289932251
Epoch 140, val loss: 1.3132460117340088
Epoch 150, training loss: 1.1136541366577148 = 1.106532335281372 + 0.001 * 7.121859073638916
Epoch 150, val loss: 1.2397592067718506
Epoch 160, training loss: 1.0182056427001953 = 1.0110961198806763 + 0.001 * 7.109533309936523
Epoch 160, val loss: 1.166603684425354
Epoch 170, training loss: 0.9225206971168518 = 0.9154258966445923 + 0.001 * 7.094825267791748
Epoch 170, val loss: 1.0945161581039429
Epoch 180, training loss: 0.8305721879005432 = 0.8234990835189819 + 0.001 * 7.0731282234191895
Epoch 180, val loss: 1.027219295501709
Epoch 190, training loss: 0.7462546229362488 = 0.73921138048172 + 0.001 * 7.043264389038086
Epoch 190, val loss: 0.9684998393058777
Epoch 200, training loss: 0.670891284942627 = 0.6638715267181396 + 0.001 * 7.019769668579102
Epoch 200, val loss: 0.9196600317955017
Epoch 210, training loss: 0.6036733388900757 = 0.5966733694076538 + 0.001 * 6.999980926513672
Epoch 210, val loss: 0.8796057105064392
Epoch 220, training loss: 0.5436060428619385 = 0.5366169810295105 + 0.001 * 6.989059925079346
Epoch 220, val loss: 0.8467555642127991
Epoch 230, training loss: 0.4900089204311371 = 0.4830297529697418 + 0.001 * 6.979165077209473
Epoch 230, val loss: 0.819854736328125
Epoch 240, training loss: 0.4422016143798828 = 0.4352308511734009 + 0.001 * 6.970771789550781
Epoch 240, val loss: 0.7979809045791626
Epoch 250, training loss: 0.3992740511894226 = 0.39231133460998535 + 0.001 * 6.962711334228516
Epoch 250, val loss: 0.7807044982910156
Epoch 260, training loss: 0.360385924577713 = 0.3534314036369324 + 0.001 * 6.954507827758789
Epoch 260, val loss: 0.7678206562995911
Epoch 270, training loss: 0.3249318599700928 = 0.31798702478408813 + 0.001 * 6.944838523864746
Epoch 270, val loss: 0.7591429352760315
Epoch 280, training loss: 0.29248353838920593 = 0.28554198145866394 + 0.001 * 6.941547393798828
Epoch 280, val loss: 0.7541870474815369
Epoch 290, training loss: 0.2627575993537903 = 0.2558231055736542 + 0.001 * 6.934487819671631
Epoch 290, val loss: 0.7526944279670715
Epoch 300, training loss: 0.2356024533510208 = 0.22866833209991455 + 0.001 * 6.934122085571289
Epoch 300, val loss: 0.7546565532684326
Epoch 310, training loss: 0.2109353095293045 = 0.20401202142238617 + 0.001 * 6.923288822174072
Epoch 310, val loss: 0.7599253058433533
Epoch 320, training loss: 0.18875721096992493 = 0.18183986842632294 + 0.001 * 6.917335033416748
Epoch 320, val loss: 0.7682330012321472
Epoch 330, training loss: 0.16900107264518738 = 0.16208654642105103 + 0.001 * 6.9145188331604
Epoch 330, val loss: 0.7792640328407288
Epoch 340, training loss: 0.15150487422943115 = 0.14459554851055145 + 0.001 * 6.909319877624512
Epoch 340, val loss: 0.7926109433174133
Epoch 350, training loss: 0.13604453206062317 = 0.12914058566093445 + 0.001 * 6.9039483070373535
Epoch 350, val loss: 0.8080045580863953
Epoch 360, training loss: 0.12238054722547531 = 0.11548484861850739 + 0.001 * 6.895700931549072
Epoch 360, val loss: 0.8251257538795471
Epoch 370, training loss: 0.1103089228272438 = 0.10342001169919968 + 0.001 * 6.8889079093933105
Epoch 370, val loss: 0.8436530232429504
Epoch 380, training loss: 0.09964613616466522 = 0.09275778383016586 + 0.001 * 6.888355255126953
Epoch 380, val loss: 0.8633304238319397
Epoch 390, training loss: 0.09021485596895218 = 0.08333209156990051 + 0.001 * 6.88276481628418
Epoch 390, val loss: 0.8839617967605591
Epoch 400, training loss: 0.08186567574739456 = 0.07498749345541 + 0.001 * 6.87817907333374
Epoch 400, val loss: 0.9052910208702087
Epoch 410, training loss: 0.07446864992380142 = 0.06759221851825714 + 0.001 * 6.876428604125977
Epoch 410, val loss: 0.927228569984436
Epoch 420, training loss: 0.06790640205144882 = 0.06103280186653137 + 0.001 * 6.87360143661499
Epoch 420, val loss: 0.9495746493339539
Epoch 430, training loss: 0.062074992805719376 = 0.05520392954349518 + 0.001 * 6.871062755584717
Epoch 430, val loss: 0.9722017645835876
Epoch 440, training loss: 0.05688942223787308 = 0.0500197596848011 + 0.001 * 6.869662284851074
Epoch 440, val loss: 0.9949985146522522
Epoch 450, training loss: 0.05227581039071083 = 0.04540861397981644 + 0.001 * 6.867195129394531
Epoch 450, val loss: 1.017836093902588
Epoch 460, training loss: 0.04819779843091965 = 0.041310615837574005 + 0.001 * 6.887181282043457
Epoch 460, val loss: 1.040480375289917
Epoch 470, training loss: 0.044534794986248016 = 0.03766921907663345 + 0.001 * 6.865576267242432
Epoch 470, val loss: 1.0627567768096924
Epoch 480, training loss: 0.0413028821349144 = 0.03443453460931778 + 0.001 * 6.86834716796875
Epoch 480, val loss: 1.084625244140625
Epoch 490, training loss: 0.03842908889055252 = 0.03156180679798126 + 0.001 * 6.867282390594482
Epoch 490, val loss: 1.1059256792068481
Epoch 500, training loss: 0.03587856888771057 = 0.029009737074375153 + 0.001 * 6.868833065032959
Epoch 500, val loss: 1.1265836954116821
Epoch 510, training loss: 0.03360435739159584 = 0.026738431304693222 + 0.001 * 6.86592435836792
Epoch 510, val loss: 1.1464818716049194
Epoch 520, training loss: 0.03157968074083328 = 0.02471364289522171 + 0.001 * 6.866035461425781
Epoch 520, val loss: 1.1657720804214478
Epoch 530, training loss: 0.029777999967336655 = 0.022904008626937866 + 0.001 * 6.8739914894104
Epoch 530, val loss: 1.1842881441116333
Epoch 540, training loss: 0.028143260627985 = 0.021281670778989792 + 0.001 * 6.861588478088379
Epoch 540, val loss: 1.202242374420166
Epoch 550, training loss: 0.02668345719575882 = 0.019823316484689713 + 0.001 * 6.860139846801758
Epoch 550, val loss: 1.2195523977279663
Epoch 560, training loss: 0.02536867931485176 = 0.018508542329072952 + 0.001 * 6.86013650894165
Epoch 560, val loss: 1.236314296722412
Epoch 570, training loss: 0.024177949875593185 = 0.017320018261671066 + 0.001 * 6.857931137084961
Epoch 570, val loss: 1.2524638175964355
Epoch 580, training loss: 0.023101257160305977 = 0.016242602840065956 + 0.001 * 6.858653545379639
Epoch 580, val loss: 1.2681282758712769
Epoch 590, training loss: 0.02213248983025551 = 0.015263384208083153 + 0.001 * 6.869104385375977
Epoch 590, val loss: 1.2832956314086914
Epoch 600, training loss: 0.021226566284894943 = 0.014370881952345371 + 0.001 * 6.855682849884033
Epoch 600, val loss: 1.298003077507019
Epoch 610, training loss: 0.020411543548107147 = 0.013555525802075863 + 0.001 * 6.85601806640625
Epoch 610, val loss: 1.3122690916061401
Epoch 620, training loss: 0.019665375351905823 = 0.012808696366846561 + 0.001 * 6.856678009033203
Epoch 620, val loss: 1.3261171579360962
Epoch 630, training loss: 0.018977347761392593 = 0.012123041786253452 + 0.001 * 6.854304790496826
Epoch 630, val loss: 1.3395493030548096
Epoch 640, training loss: 0.01834275759756565 = 0.011492151767015457 + 0.001 * 6.850605010986328
Epoch 640, val loss: 1.3526687622070312
Epoch 650, training loss: 0.017771480605006218 = 0.01091042160987854 + 0.001 * 6.861058235168457
Epoch 650, val loss: 1.3654067516326904
Epoch 660, training loss: 0.017222467809915543 = 0.010372970253229141 + 0.001 * 6.849496364593506
Epoch 660, val loss: 1.377834439277649
Epoch 670, training loss: 0.016733981668949127 = 0.009875506162643433 + 0.001 * 6.858475208282471
Epoch 670, val loss: 1.3899108171463013
Epoch 680, training loss: 0.016261940822005272 = 0.009414179250597954 + 0.001 * 6.8477606773376465
Epoch 680, val loss: 1.4017035961151123
Epoch 690, training loss: 0.015835128724575043 = 0.008985599502921104 + 0.001 * 6.8495283126831055
Epoch 690, val loss: 1.4131768941879272
Epoch 700, training loss: 0.015438774600625038 = 0.008586780168116093 + 0.001 * 6.851994037628174
Epoch 700, val loss: 1.424391269683838
Epoch 710, training loss: 0.015061110258102417 = 0.008215011097490788 + 0.001 * 6.846098899841309
Epoch 710, val loss: 1.4353351593017578
Epoch 720, training loss: 0.014723065309226513 = 0.007867946289479733 + 0.001 * 6.855118751525879
Epoch 720, val loss: 1.4460043907165527
Epoch 730, training loss: 0.014386098831892014 = 0.0075434911996126175 + 0.001 * 6.8426079750061035
Epoch 730, val loss: 1.4564241170883179
Epoch 740, training loss: 0.014084797352552414 = 0.0072397273033857346 + 0.001 * 6.8450703620910645
Epoch 740, val loss: 1.4666193723678589
Epoch 750, training loss: 0.01379467360675335 = 0.0069549414329230785 + 0.001 * 6.839731216430664
Epoch 750, val loss: 1.476548671722412
Epoch 760, training loss: 0.01352749578654766 = 0.006687624379992485 + 0.001 * 6.839870452880859
Epoch 760, val loss: 1.486280083656311
Epoch 770, training loss: 0.013282937929034233 = 0.006436385679990053 + 0.001 * 6.846551895141602
Epoch 770, val loss: 1.4957878589630127
Epoch 780, training loss: 0.013042421080172062 = 0.0062000141479074955 + 0.001 * 6.842406749725342
Epoch 780, val loss: 1.505074143409729
Epoch 790, training loss: 0.012816760689020157 = 0.005977308843284845 + 0.001 * 6.839450836181641
Epoch 790, val loss: 1.514161467552185
Epoch 800, training loss: 0.012609085068106651 = 0.005767322611063719 + 0.001 * 6.841762542724609
Epoch 800, val loss: 1.523085355758667
Epoch 810, training loss: 0.012402808293700218 = 0.0055690729059278965 + 0.001 * 6.83373498916626
Epoch 810, val loss: 1.53177809715271
Epoch 820, training loss: 0.012222048826515675 = 0.005381735507398844 + 0.001 * 6.840312957763672
Epoch 820, val loss: 1.5402696132659912
Epoch 830, training loss: 0.012042825110256672 = 0.005204491782933474 + 0.001 * 6.8383331298828125
Epoch 830, val loss: 1.5486191511154175
Epoch 840, training loss: 0.011870605871081352 = 0.005036641843616962 + 0.001 * 6.8339643478393555
Epoch 840, val loss: 1.556762933731079
Epoch 850, training loss: 0.011713230051100254 = 0.004877548664808273 + 0.001 * 6.835680961608887
Epoch 850, val loss: 1.5647621154785156
Epoch 860, training loss: 0.011562108993530273 = 0.004726606421172619 + 0.001 * 6.8355021476745605
Epoch 860, val loss: 1.5725866556167603
Epoch 870, training loss: 0.011411555111408234 = 0.0045832977630198 + 0.001 * 6.828257083892822
Epoch 870, val loss: 1.5802483558654785
Epoch 880, training loss: 0.01128102745860815 = 0.004447090905159712 + 0.001 * 6.8339362144470215
Epoch 880, val loss: 1.5877772569656372
Epoch 890, training loss: 0.011145910248160362 = 0.0043175178579986095 + 0.001 * 6.8283915519714355
Epoch 890, val loss: 1.5951142311096191
Epoch 900, training loss: 0.011023178696632385 = 0.004194186069071293 + 0.001 * 6.828991889953613
Epoch 900, val loss: 1.6023404598236084
Epoch 910, training loss: 0.010904675349593163 = 0.00407669460400939 + 0.001 * 6.827980041503906
Epoch 910, val loss: 1.6093946695327759
Epoch 920, training loss: 0.010787367820739746 = 0.003964676056057215 + 0.001 * 6.822690963745117
Epoch 920, val loss: 1.6163549423217773
Epoch 930, training loss: 0.010679999366402626 = 0.003857811214402318 + 0.001 * 6.822187900543213
Epoch 930, val loss: 1.6231603622436523
Epoch 940, training loss: 0.010575280524790287 = 0.003755782498046756 + 0.001 * 6.819497585296631
Epoch 940, val loss: 1.6298562288284302
Epoch 950, training loss: 0.010482638143002987 = 0.0036582984030246735 + 0.001 * 6.824339389801025
Epoch 950, val loss: 1.6363991498947144
Epoch 960, training loss: 0.010381180793046951 = 0.0035651035141199827 + 0.001 * 6.81607723236084
Epoch 960, val loss: 1.6428298950195312
Epoch 970, training loss: 0.010298707522451878 = 0.003475941251963377 + 0.001 * 6.822765827178955
Epoch 970, val loss: 1.6491131782531738
Epoch 980, training loss: 0.01020776852965355 = 0.0033905908931046724 + 0.001 * 6.817177772521973
Epoch 980, val loss: 1.6553066968917847
Epoch 990, training loss: 0.010121488012373447 = 0.0033088296186178923 + 0.001 * 6.812658309936523
Epoch 990, val loss: 1.6613844633102417
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.4613
Flip ASR: 0.3644/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9589152336120605 = 1.950541377067566 + 0.001 * 8.373819351196289
Epoch 0, val loss: 1.9514904022216797
Epoch 10, training loss: 1.948817253112793 = 1.940443515777588 + 0.001 * 8.373709678649902
Epoch 10, val loss: 1.9410203695297241
Epoch 20, training loss: 1.9360231161117554 = 1.9276496171951294 + 0.001 * 8.373440742492676
Epoch 20, val loss: 1.9276100397109985
Epoch 30, training loss: 1.9176243543624878 = 1.9092514514923096 + 0.001 * 8.372934341430664
Epoch 30, val loss: 1.908427119255066
Epoch 40, training loss: 1.8901060819625854 = 1.8817342519760132 + 0.001 * 8.371879577636719
Epoch 40, val loss: 1.8802769184112549
Epoch 50, training loss: 1.8510922193527222 = 1.8427233695983887 + 0.001 * 8.368849754333496
Epoch 50, val loss: 1.8421763181686401
Epoch 60, training loss: 1.8059660196304321 = 1.797609567642212 + 0.001 * 8.3564453125
Epoch 60, val loss: 1.80204176902771
Epoch 70, training loss: 1.764904499053955 = 1.7566313743591309 + 0.001 * 8.273151397705078
Epoch 70, val loss: 1.767556071281433
Epoch 80, training loss: 1.7141013145446777 = 1.7061418294906616 + 0.001 * 7.9594573974609375
Epoch 80, val loss: 1.722052812576294
Epoch 90, training loss: 1.6434555053710938 = 1.6356664896011353 + 0.001 * 7.789046287536621
Epoch 90, val loss: 1.6614749431610107
Epoch 100, training loss: 1.5533528327941895 = 1.5457595586776733 + 0.001 * 7.5933146476745605
Epoch 100, val loss: 1.5873866081237793
Epoch 110, training loss: 1.455570101737976 = 1.448218584060669 + 0.001 * 7.351563930511475
Epoch 110, val loss: 1.5078567266464233
Epoch 120, training loss: 1.3614912033081055 = 1.3541672229766846 + 0.001 * 7.323935031890869
Epoch 120, val loss: 1.433150053024292
Epoch 130, training loss: 1.2723069190979004 = 1.265061378479004 + 0.001 * 7.245532035827637
Epoch 130, val loss: 1.364534616470337
Epoch 140, training loss: 1.1862307786941528 = 1.1790800094604492 + 0.001 * 7.1507368087768555
Epoch 140, val loss: 1.3002592325210571
Epoch 150, training loss: 1.1020046472549438 = 1.0949351787567139 + 0.001 * 7.069458961486816
Epoch 150, val loss: 1.238504409790039
Epoch 160, training loss: 1.0195093154907227 = 1.0124731063842773 + 0.001 * 7.036228656768799
Epoch 160, val loss: 1.1780040264129639
Epoch 170, training loss: 0.9396642446517944 = 0.932639479637146 + 0.001 * 7.024750232696533
Epoch 170, val loss: 1.1191636323928833
Epoch 180, training loss: 0.8638553023338318 = 0.8568374514579773 + 0.001 * 7.017879009246826
Epoch 180, val loss: 1.0633695125579834
Epoch 190, training loss: 0.7934484481811523 = 0.7864360809326172 + 0.001 * 7.012392520904541
Epoch 190, val loss: 1.0120271444320679
Epoch 200, training loss: 0.7299419045448303 = 0.7229333519935608 + 0.001 * 7.008542537689209
Epoch 200, val loss: 0.9670844078063965
Epoch 210, training loss: 0.6741823554039001 = 0.6671761870384216 + 0.001 * 7.006180763244629
Epoch 210, val loss: 0.9299952387809753
Epoch 220, training loss: 0.6254751682281494 = 0.6184705495834351 + 0.001 * 7.004639148712158
Epoch 220, val loss: 0.9005546569824219
Epoch 230, training loss: 0.5817822813987732 = 0.5747795701026917 + 0.001 * 7.002725124359131
Epoch 230, val loss: 0.8770150542259216
Epoch 240, training loss: 0.5411170125007629 = 0.5341167449951172 + 0.001 * 7.000280857086182
Epoch 240, val loss: 0.8571500182151794
Epoch 250, training loss: 0.5021297335624695 = 0.49513351917266846 + 0.001 * 6.9961934089660645
Epoch 250, val loss: 0.8397156596183777
Epoch 260, training loss: 0.4639089107513428 = 0.4569128453731537 + 0.001 * 6.996071815490723
Epoch 260, val loss: 0.8241466283798218
Epoch 270, training loss: 0.4258177876472473 = 0.41883185505867004 + 0.001 * 6.985937118530273
Epoch 270, val loss: 0.8105078935623169
Epoch 280, training loss: 0.3876417279243469 = 0.38066279888153076 + 0.001 * 6.978943347930908
Epoch 280, val loss: 0.7985620498657227
Epoch 290, training loss: 0.3496996760368347 = 0.3427251875400543 + 0.001 * 6.974483013153076
Epoch 290, val loss: 0.788750410079956
Epoch 300, training loss: 0.31279027462005615 = 0.3058263659477234 + 0.001 * 6.963905334472656
Epoch 300, val loss: 0.7811850905418396
Epoch 310, training loss: 0.27800291776657104 = 0.27103832364082336 + 0.001 * 6.964601039886475
Epoch 310, val loss: 0.7764090299606323
Epoch 320, training loss: 0.24617666006088257 = 0.2392311543226242 + 0.001 * 6.945502281188965
Epoch 320, val loss: 0.7747250199317932
Epoch 330, training loss: 0.21778683364391327 = 0.2108418196439743 + 0.001 * 6.945012092590332
Epoch 330, val loss: 0.775833010673523
Epoch 340, training loss: 0.19285699725151062 = 0.18592648208141327 + 0.001 * 6.930509090423584
Epoch 340, val loss: 0.7795001268386841
Epoch 350, training loss: 0.17117677628993988 = 0.16424940526485443 + 0.001 * 6.927369117736816
Epoch 350, val loss: 0.7851954698562622
Epoch 360, training loss: 0.1523955911397934 = 0.14548031985759735 + 0.001 * 6.9152655601501465
Epoch 360, val loss: 0.7925739288330078
Epoch 370, training loss: 0.13614921271800995 = 0.12924470007419586 + 0.001 * 6.904512882232666
Epoch 370, val loss: 0.8012024760246277
Epoch 380, training loss: 0.12206033617258072 = 0.11515793949365616 + 0.001 * 6.902394771575928
Epoch 380, val loss: 0.8107168078422546
Epoch 390, training loss: 0.10980257391929626 = 0.10289952903985977 + 0.001 * 6.9030442237854
Epoch 390, val loss: 0.8208448886871338
Epoch 400, training loss: 0.09910506010055542 = 0.09221016615629196 + 0.001 * 6.894890308380127
Epoch 400, val loss: 0.8314843773841858
Epoch 410, training loss: 0.08974813669919968 = 0.08285214751958847 + 0.001 * 6.895991325378418
Epoch 410, val loss: 0.8425971865653992
Epoch 420, training loss: 0.08152548968791962 = 0.07461991161108017 + 0.001 * 6.905573844909668
Epoch 420, val loss: 0.8538379669189453
Epoch 430, training loss: 0.0742591992020607 = 0.06736388802528381 + 0.001 * 6.895312309265137
Epoch 430, val loss: 0.8652507662773132
Epoch 440, training loss: 0.06784674525260925 = 0.060956813395023346 + 0.001 * 6.889928340911865
Epoch 440, val loss: 0.8769076466560364
Epoch 450, training loss: 0.06217557191848755 = 0.055284008383750916 + 0.001 * 6.891564846038818
Epoch 450, val loss: 0.888701856136322
Epoch 460, training loss: 0.057128097862005234 = 0.05023935064673424 + 0.001 * 6.888747215270996
Epoch 460, val loss: 0.9005045294761658
Epoch 470, training loss: 0.05258838087320328 = 0.04569987580180168 + 0.001 * 6.888503074645996
Epoch 470, val loss: 0.9123605489730835
Epoch 480, training loss: 0.04849682003259659 = 0.04159188270568848 + 0.001 * 6.90493631362915
Epoch 480, val loss: 0.9239099621772766
Epoch 490, training loss: 0.044721540063619614 = 0.03783346712589264 + 0.001 * 6.888071060180664
Epoch 490, val loss: 0.9351657629013062
Epoch 500, training loss: 0.04128050431609154 = 0.034392308443784714 + 0.001 * 6.888195991516113
Epoch 500, val loss: 0.9464661478996277
Epoch 510, training loss: 0.03809134662151337 = 0.031193261966109276 + 0.001 * 6.898082733154297
Epoch 510, val loss: 0.9572429656982422
Epoch 520, training loss: 0.035176534205675125 = 0.028290074318647385 + 0.001 * 6.886460781097412
Epoch 520, val loss: 0.9679991602897644
Epoch 530, training loss: 0.032542649656534195 = 0.02565961331129074 + 0.001 * 6.883037567138672
Epoch 530, val loss: 0.9785662889480591
Epoch 540, training loss: 0.030227314680814743 = 0.023331543430685997 + 0.001 * 6.895771026611328
Epoch 540, val loss: 0.9889976382255554
Epoch 550, training loss: 0.02818470075726509 = 0.021298205479979515 + 0.001 * 6.886495113372803
Epoch 550, val loss: 0.9988569617271423
Epoch 560, training loss: 0.026434380561113358 = 0.019550424069166183 + 0.001 * 6.883955955505371
Epoch 560, val loss: 1.0090588331222534
Epoch 570, training loss: 0.024916548281908035 = 0.018037807196378708 + 0.001 * 6.87874174118042
Epoch 570, val loss: 1.0189040899276733
Epoch 580, training loss: 0.02358286641538143 = 0.01671084575355053 + 0.001 * 6.872020244598389
Epoch 580, val loss: 1.0286813974380493
Epoch 590, training loss: 0.02240157313644886 = 0.015534668229520321 + 0.001 * 6.866905212402344
Epoch 590, val loss: 1.0383191108703613
Epoch 600, training loss: 0.021347135305404663 = 0.014481940306723118 + 0.001 * 6.865193843841553
Epoch 600, val loss: 1.047913670539856
Epoch 610, training loss: 0.02040412463247776 = 0.01353413425385952 + 0.001 * 6.869989395141602
Epoch 610, val loss: 1.0571260452270508
Epoch 620, training loss: 0.01954343356192112 = 0.012680151499807835 + 0.001 * 6.86328125
Epoch 620, val loss: 1.0661919116973877
Epoch 630, training loss: 0.01876862347126007 = 0.011908232234418392 + 0.001 * 6.8603901863098145
Epoch 630, val loss: 1.0750921964645386
Epoch 640, training loss: 0.018118837848305702 = 0.011208022013306618 + 0.001 * 6.910815715789795
Epoch 640, val loss: 1.083720326423645
Epoch 650, training loss: 0.017430007457733154 = 0.010571101680397987 + 0.001 * 6.8589043617248535
Epoch 650, val loss: 1.0922199487686157
Epoch 660, training loss: 0.016857527196407318 = 0.009989442303776741 + 0.001 * 6.868084907531738
Epoch 660, val loss: 1.1004996299743652
Epoch 670, training loss: 0.0163164883852005 = 0.00945728924125433 + 0.001 * 6.859199047088623
Epoch 670, val loss: 1.1085511445999146
Epoch 680, training loss: 0.015842188149690628 = 0.008968519046902657 + 0.001 * 6.8736677169799805
Epoch 680, val loss: 1.1164352893829346
Epoch 690, training loss: 0.015375951305031776 = 0.008518712595105171 + 0.001 * 6.85723876953125
Epoch 690, val loss: 1.1241567134857178
Epoch 700, training loss: 0.014957555569708347 = 0.008103770203888416 + 0.001 * 6.853785037994385
Epoch 700, val loss: 1.1316438913345337
Epoch 710, training loss: 0.014594603329896927 = 0.007720472291111946 + 0.001 * 6.874130725860596
Epoch 710, val loss: 1.138985514640808
Epoch 720, training loss: 0.014222145080566406 = 0.007365667726844549 + 0.001 * 6.8564772605896
Epoch 720, val loss: 1.1461715698242188
Epoch 730, training loss: 0.013884873129427433 = 0.007037052419036627 + 0.001 * 6.847820281982422
Epoch 730, val loss: 1.1531453132629395
Epoch 740, training loss: 0.013594549149274826 = 0.006731658708304167 + 0.001 * 6.862889766693115
Epoch 740, val loss: 1.1600018739700317
Epoch 750, training loss: 0.013296223245561123 = 0.00644737109541893 + 0.001 * 6.848851680755615
Epoch 750, val loss: 1.1666523218154907
Epoch 760, training loss: 0.013042881153523922 = 0.00618224311619997 + 0.001 * 6.860637664794922
Epoch 760, val loss: 1.1731492280960083
Epoch 770, training loss: 0.012782122939825058 = 0.0059346347115933895 + 0.001 * 6.847487449645996
Epoch 770, val loss: 1.1795670986175537
Epoch 780, training loss: 0.012543069198727608 = 0.00570289883762598 + 0.001 * 6.840170383453369
Epoch 780, val loss: 1.1857833862304688
Epoch 790, training loss: 0.012350574135780334 = 0.005485747009515762 + 0.001 * 6.864826679229736
Epoch 790, val loss: 1.191876769065857
Epoch 800, training loss: 0.012133656069636345 = 0.005282036028802395 + 0.001 * 6.851619720458984
Epoch 800, val loss: 1.1978657245635986
Epoch 810, training loss: 0.011925766244530678 = 0.005090690217912197 + 0.001 * 6.835075855255127
Epoch 810, val loss: 1.203734278678894
Epoch 820, training loss: 0.01178213581442833 = 0.0049106664955616 + 0.001 * 6.871468544006348
Epoch 820, val loss: 1.209450125694275
Epoch 830, training loss: 0.011575393378734589 = 0.004741062875837088 + 0.001 * 6.834329605102539
Epoch 830, val loss: 1.215084433555603
Epoch 840, training loss: 0.011420298367738724 = 0.004581142216920853 + 0.001 * 6.839156150817871
Epoch 840, val loss: 1.2205519676208496
Epoch 850, training loss: 0.011269684880971909 = 0.004430207423865795 + 0.001 * 6.839477062225342
Epoch 850, val loss: 1.2259715795516968
Epoch 860, training loss: 0.011157505214214325 = 0.004287547431886196 + 0.001 * 6.86995792388916
Epoch 860, val loss: 1.2312027215957642
Epoch 870, training loss: 0.010985219851136208 = 0.004152498673647642 + 0.001 * 6.83272123336792
Epoch 870, val loss: 1.2363975048065186
Epoch 880, training loss: 0.010881399735808372 = 0.0040245153941214085 + 0.001 * 6.856884479522705
Epoch 880, val loss: 1.241438627243042
Epoch 890, training loss: 0.010736159980297089 = 0.0039029186591506004 + 0.001 * 6.8332414627075195
Epoch 890, val loss: 1.2464600801467896
Epoch 900, training loss: 0.01062740758061409 = 0.003787195309996605 + 0.001 * 6.840211868286133
Epoch 900, val loss: 1.2513083219528198
Epoch 910, training loss: 0.010505550540983677 = 0.003676734631881118 + 0.001 * 6.828815937042236
Epoch 910, val loss: 1.2560791969299316
Epoch 920, training loss: 0.01040444802492857 = 0.0035710264928638935 + 0.001 * 6.833421230316162
Epoch 920, val loss: 1.2607872486114502
Epoch 930, training loss: 0.010300273075699806 = 0.0034696413204073906 + 0.001 * 6.830631256103516
Epoch 930, val loss: 1.265346646308899
Epoch 940, training loss: 0.010202090255916119 = 0.0033721073996275663 + 0.001 * 6.829982280731201
Epoch 940, val loss: 1.2698893547058105
Epoch 950, training loss: 0.010099542327225208 = 0.0032780913170427084 + 0.001 * 6.821450710296631
Epoch 950, val loss: 1.2742844820022583
Epoch 960, training loss: 0.010006757453083992 = 0.0031873551197350025 + 0.001 * 6.81940221786499
Epoch 960, val loss: 1.2786530256271362
Epoch 970, training loss: 0.009938796982169151 = 0.00309981987811625 + 0.001 * 6.8389763832092285
Epoch 970, val loss: 1.2829160690307617
Epoch 980, training loss: 0.009829973801970482 = 0.0030152720864862204 + 0.001 * 6.814701080322266
Epoch 980, val loss: 1.28713059425354
Epoch 990, training loss: 0.009758739732205868 = 0.002933741081506014 + 0.001 * 6.824998378753662
Epoch 990, val loss: 1.2911851406097412
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7926
Overall ASR: 0.6753
Flip ASR: 0.6267/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.967294692993164 = 1.9589208364486694 + 0.001 * 8.373873710632324
Epoch 0, val loss: 1.966170072555542
Epoch 10, training loss: 1.9567861557006836 = 1.948412299156189 + 0.001 * 8.373809814453125
Epoch 10, val loss: 1.955861210823059
Epoch 20, training loss: 1.9437742233276367 = 1.9354006052017212 + 0.001 * 8.373641967773438
Epoch 20, val loss: 1.9425389766693115
Epoch 30, training loss: 1.9254955053329468 = 1.9171221256256104 + 0.001 * 8.373329162597656
Epoch 30, val loss: 1.9233698844909668
Epoch 40, training loss: 1.8986936807632446 = 1.890320897102356 + 0.001 * 8.37272834777832
Epoch 40, val loss: 1.89515221118927
Epoch 50, training loss: 1.860946536064148 = 1.8525750637054443 + 0.001 * 8.371478080749512
Epoch 50, val loss: 1.8565399646759033
Epoch 60, training loss: 1.8161866664886475 = 1.8078186511993408 + 0.001 * 8.368043899536133
Epoch 60, val loss: 1.8140642642974854
Epoch 70, training loss: 1.775498390197754 = 1.7671453952789307 + 0.001 * 8.353026390075684
Epoch 70, val loss: 1.778916358947754
Epoch 80, training loss: 1.7301443815231323 = 1.7219047546386719 + 0.001 * 8.239680290222168
Epoch 80, val loss: 1.739522099494934
Epoch 90, training loss: 1.667236566543579 = 1.6593562364578247 + 0.001 * 7.880297660827637
Epoch 90, val loss: 1.6870927810668945
Epoch 100, training loss: 1.5836801528930664 = 1.575929045677185 + 0.001 * 7.751060485839844
Epoch 100, val loss: 1.6189638376235962
Epoch 110, training loss: 1.4845224618911743 = 1.4769577980041504 + 0.001 * 7.564634323120117
Epoch 110, val loss: 1.5388022661209106
Epoch 120, training loss: 1.381765365600586 = 1.3745027780532837 + 0.001 * 7.262604236602783
Epoch 120, val loss: 1.4576863050460815
Epoch 130, training loss: 1.2828854322433472 = 1.275672197341919 + 0.001 * 7.213214874267578
Epoch 130, val loss: 1.3829556703567505
Epoch 140, training loss: 1.188279628753662 = 1.1811059713363647 + 0.001 * 7.173684597015381
Epoch 140, val loss: 1.3143606185913086
Epoch 150, training loss: 1.0959216356277466 = 1.0887640714645386 + 0.001 * 7.157588958740234
Epoch 150, val loss: 1.2470670938491821
Epoch 160, training loss: 1.0039869546890259 = 0.9968394637107849 + 0.001 * 7.147446632385254
Epoch 160, val loss: 1.1783807277679443
Epoch 170, training loss: 0.9124237298965454 = 0.9052896499633789 + 0.001 * 7.134073734283447
Epoch 170, val loss: 1.1080268621444702
Epoch 180, training loss: 0.8231093883514404 = 0.8159930109977722 + 0.001 * 7.116379261016846
Epoch 180, val loss: 1.0377968549728394
Epoch 190, training loss: 0.7382841110229492 = 0.7311939597129822 + 0.001 * 7.090131759643555
Epoch 190, val loss: 0.9708850979804993
Epoch 200, training loss: 0.6595590710639954 = 0.6525017619132996 + 0.001 * 7.057331562042236
Epoch 200, val loss: 0.9101345539093018
Epoch 210, training loss: 0.5876014232635498 = 0.5805675983428955 + 0.001 * 7.033823490142822
Epoch 210, val loss: 0.8577866554260254
Epoch 220, training loss: 0.5223432183265686 = 0.5153117179870605 + 0.001 * 7.031477928161621
Epoch 220, val loss: 0.8145697116851807
Epoch 230, training loss: 0.46363717317581177 = 0.4566090703010559 + 0.001 * 7.028107643127441
Epoch 230, val loss: 0.7799628973007202
Epoch 240, training loss: 0.41138336062431335 = 0.40435588359832764 + 0.001 * 7.027464866638184
Epoch 240, val loss: 0.7528622150421143
Epoch 250, training loss: 0.3654882311820984 = 0.35846075415611267 + 0.001 * 7.027491092681885
Epoch 250, val loss: 0.7323088645935059
Epoch 260, training loss: 0.325471431016922 = 0.3184448182582855 + 0.001 * 7.026601791381836
Epoch 260, val loss: 0.7177238464355469
Epoch 270, training loss: 0.29051491618156433 = 0.28348931670188904 + 0.001 * 7.025596618652344
Epoch 270, val loss: 0.7085169553756714
Epoch 280, training loss: 0.2597348690032959 = 0.25271040201187134 + 0.001 * 7.0244646072387695
Epoch 280, val loss: 0.7038449048995972
Epoch 290, training loss: 0.23230670392513275 = 0.22528362274169922 + 0.001 * 7.023085594177246
Epoch 290, val loss: 0.7029073238372803
Epoch 300, training loss: 0.20762518048286438 = 0.20060396194458008 + 0.001 * 7.02121114730835
Epoch 300, val loss: 0.7051169872283936
Epoch 310, training loss: 0.18528686463832855 = 0.17826618254184723 + 0.001 * 7.020687580108643
Epoch 310, val loss: 0.7099933624267578
Epoch 320, training loss: 0.16506776213645935 = 0.15805266797542572 + 0.001 * 7.015092372894287
Epoch 320, val loss: 0.7172741889953613
Epoch 330, training loss: 0.14684519171714783 = 0.1398337334394455 + 0.001 * 7.011450290679932
Epoch 330, val loss: 0.7266685366630554
Epoch 340, training loss: 0.13052402436733246 = 0.1235196590423584 + 0.001 * 7.004368782043457
Epoch 340, val loss: 0.7378698587417603
Epoch 350, training loss: 0.11600980907678604 = 0.10901155322790146 + 0.001 * 6.998258113861084
Epoch 350, val loss: 0.7506413459777832
Epoch 360, training loss: 0.10318699479103088 = 0.09619683027267456 + 0.001 * 6.990161418914795
Epoch 360, val loss: 0.7646944522857666
Epoch 370, training loss: 0.0919361561536789 = 0.08495646715164185 + 0.001 * 6.979691505432129
Epoch 370, val loss: 0.7797829508781433
Epoch 380, training loss: 0.08215279132127762 = 0.07516196370124817 + 0.001 * 6.990826606750488
Epoch 380, val loss: 0.7955928444862366
Epoch 390, training loss: 0.07363513857126236 = 0.0666721984744072 + 0.001 * 6.962937355041504
Epoch 390, val loss: 0.8119088411331177
Epoch 400, training loss: 0.06630527973175049 = 0.05934388190507889 + 0.001 * 6.961398124694824
Epoch 400, val loss: 0.828464150428772
Epoch 410, training loss: 0.05998283252120018 = 0.05302480608224869 + 0.001 * 6.958027362823486
Epoch 410, val loss: 0.8450272083282471
Epoch 420, training loss: 0.05452912300825119 = 0.04757257550954819 + 0.001 * 6.956546306610107
Epoch 420, val loss: 0.8614025115966797
Epoch 430, training loss: 0.04981353506445885 = 0.04285962134599686 + 0.001 * 6.953912734985352
Epoch 430, val loss: 0.8774672746658325
Epoch 440, training loss: 0.04572547227144241 = 0.038772135972976685 + 0.001 * 6.953335285186768
Epoch 440, val loss: 0.8931753635406494
Epoch 450, training loss: 0.04216787964105606 = 0.0352134145796299 + 0.001 * 6.954466819763184
Epoch 450, val loss: 0.9084644317626953
Epoch 460, training loss: 0.039053864777088165 = 0.032102737575769424 + 0.001 * 6.951127529144287
Epoch 460, val loss: 0.9233453273773193
Epoch 470, training loss: 0.036322273313999176 = 0.029372254386544228 + 0.001 * 6.950018405914307
Epoch 470, val loss: 0.9378025531768799
Epoch 480, training loss: 0.033915866166353226 = 0.026965998113155365 + 0.001 * 6.949866771697998
Epoch 480, val loss: 0.9518347978591919
Epoch 490, training loss: 0.03178919479250908 = 0.024836543947458267 + 0.001 * 6.9526495933532715
Epoch 490, val loss: 0.9654425978660583
Epoch 500, training loss: 0.02989468351006508 = 0.022944914177060127 + 0.001 * 6.94976806640625
Epoch 500, val loss: 0.9786280989646912
Epoch 510, training loss: 0.0282086580991745 = 0.021258050575852394 + 0.001 * 6.950606346130371
Epoch 510, val loss: 0.9914239048957825
Epoch 520, training loss: 0.026697305962443352 = 0.019748281687498093 + 0.001 * 6.949024200439453
Epoch 520, val loss: 1.0038576126098633
Epoch 530, training loss: 0.02534172497689724 = 0.01839255355298519 + 0.001 * 6.949171543121338
Epoch 530, val loss: 1.0159153938293457
Epoch 540, training loss: 0.02411932870745659 = 0.01717122457921505 + 0.001 * 6.948104381561279
Epoch 540, val loss: 1.027651309967041
Epoch 550, training loss: 0.02301241271197796 = 0.016067128628492355 + 0.001 * 6.94528341293335
Epoch 550, val loss: 1.039011836051941
Epoch 560, training loss: 0.022012367844581604 = 0.015065488405525684 + 0.001 * 6.946879863739014
Epoch 560, val loss: 1.050031304359436
Epoch 570, training loss: 0.021096128970384598 = 0.01415356993675232 + 0.001 * 6.942559719085693
Epoch 570, val loss: 1.0607620477676392
Epoch 580, training loss: 0.020261861383914948 = 0.013320009224116802 + 0.001 * 6.9418511390686035
Epoch 580, val loss: 1.071221113204956
Epoch 590, training loss: 0.019499460235238075 = 0.012555016204714775 + 0.001 * 6.944444179534912
Epoch 590, val loss: 1.0814521312713623
Epoch 600, training loss: 0.018790515139698982 = 0.011850506067276001 + 0.001 * 6.940008163452148
Epoch 600, val loss: 1.0914931297302246
Epoch 610, training loss: 0.018140695989131927 = 0.011200403794646263 + 0.001 * 6.940290927886963
Epoch 610, val loss: 1.1013323068618774
Epoch 620, training loss: 0.01753774657845497 = 0.010599585250020027 + 0.001 * 6.938161849975586
Epoch 620, val loss: 1.1109479665756226
Epoch 630, training loss: 0.016985809430480003 = 0.01004352979362011 + 0.001 * 6.94227933883667
Epoch 630, val loss: 1.1203839778900146
Epoch 640, training loss: 0.01646609418094158 = 0.009528427384793758 + 0.001 * 6.937665939331055
Epoch 640, val loss: 1.1296346187591553
Epoch 650, training loss: 0.015990350395441055 = 0.00905084703117609 + 0.001 * 6.939504146575928
Epoch 650, val loss: 1.1386620998382568
Epoch 660, training loss: 0.015546813607215881 = 0.00860763993114233 + 0.001 * 6.939173221588135
Epoch 660, val loss: 1.1474838256835938
Epoch 670, training loss: 0.015132838860154152 = 0.00819582398980856 + 0.001 * 6.937014102935791
Epoch 670, val loss: 1.156105637550354
Epoch 680, training loss: 0.01474444568157196 = 0.007812815718352795 + 0.001 * 6.931629657745361
Epoch 680, val loss: 1.164549708366394
Epoch 690, training loss: 0.014390109106898308 = 0.007456147111952305 + 0.001 * 6.933962345123291
Epoch 690, val loss: 1.172792911529541
Epoch 700, training loss: 0.014055375009775162 = 0.0071236565709114075 + 0.001 * 6.931717872619629
Epoch 700, val loss: 1.1808453798294067
Epoch 710, training loss: 0.013747777789831161 = 0.006813326384872198 + 0.001 * 6.934451580047607
Epoch 710, val loss: 1.188719391822815
Epoch 720, training loss: 0.013456815853714943 = 0.006523337680846453 + 0.001 * 6.933477878570557
Epoch 720, val loss: 1.196423053741455
Epoch 730, training loss: 0.013180678710341454 = 0.006252049934118986 + 0.001 * 6.928627967834473
Epoch 730, val loss: 1.2039361000061035
Epoch 740, training loss: 0.012925604358315468 = 0.005998017732053995 + 0.001 * 6.927586555480957
Epoch 740, val loss: 1.211286187171936
Epoch 750, training loss: 0.012687699869275093 = 0.00575982453301549 + 0.001 * 6.927874565124512
Epoch 750, val loss: 1.2184637784957886
Epoch 760, training loss: 0.012461145408451557 = 0.00553624052554369 + 0.001 * 6.9249043464660645
Epoch 760, val loss: 1.2254667282104492
Epoch 770, training loss: 0.012252245098352432 = 0.005326106213033199 + 0.001 * 6.926138401031494
Epoch 770, val loss: 1.2323473691940308
Epoch 780, training loss: 0.012058330699801445 = 0.005128366407006979 + 0.001 * 6.929964065551758
Epoch 780, val loss: 1.2390540838241577
Epoch 790, training loss: 0.011863086372613907 = 0.004942189436405897 + 0.001 * 6.920897006988525
Epoch 790, val loss: 1.2456144094467163
Epoch 800, training loss: 0.011691709980368614 = 0.004766670521348715 + 0.001 * 6.925039291381836
Epoch 800, val loss: 1.2520278692245483
Epoch 810, training loss: 0.011520203202962875 = 0.004601012449711561 + 0.001 * 6.919189929962158
Epoch 810, val loss: 1.258300542831421
Epoch 820, training loss: 0.011362742632627487 = 0.0044445013627409935 + 0.001 * 6.918240547180176
Epoch 820, val loss: 1.2644323110580444
Epoch 830, training loss: 0.011216778308153152 = 0.004296373575925827 + 0.001 * 6.92040491104126
Epoch 830, val loss: 1.2704508304595947
Epoch 840, training loss: 0.01107145007699728 = 0.004155914764851332 + 0.001 * 6.915534973144531
Epoch 840, val loss: 1.2763559818267822
Epoch 850, training loss: 0.010936751030385494 = 0.004022194072604179 + 0.001 * 6.914556503295898
Epoch 850, val loss: 1.2821953296661377
Epoch 860, training loss: 0.010808647610247135 = 0.0038941518869251013 + 0.001 * 6.91449499130249
Epoch 860, val loss: 1.288008689880371
Epoch 870, training loss: 0.010681744664907455 = 0.0037709474563598633 + 0.001 * 6.910797119140625
Epoch 870, val loss: 1.2938330173492432
Epoch 880, training loss: 0.010563076473772526 = 0.003651808714494109 + 0.001 * 6.911267280578613
Epoch 880, val loss: 1.299679160118103
Epoch 890, training loss: 0.0104522705078125 = 0.003536392468959093 + 0.001 * 6.9158782958984375
Epoch 890, val loss: 1.3055744171142578
Epoch 900, training loss: 0.010341247543692589 = 0.003424655878916383 + 0.001 * 6.916591167449951
Epoch 900, val loss: 1.3114964962005615
Epoch 910, training loss: 0.010224570520222187 = 0.0033167917281389236 + 0.001 * 6.907778263092041
Epoch 910, val loss: 1.31742262840271
Epoch 920, training loss: 0.010123476386070251 = 0.0032128342427313328 + 0.001 * 6.910641193389893
Epoch 920, val loss: 1.3233717679977417
Epoch 930, training loss: 0.01001960039138794 = 0.003112869570031762 + 0.001 * 6.9067301750183105
Epoch 930, val loss: 1.3292713165283203
Epoch 940, training loss: 0.009925308637320995 = 0.003016937989741564 + 0.001 * 6.908370494842529
Epoch 940, val loss: 1.3351765871047974
Epoch 950, training loss: 0.009821197018027306 = 0.0029251824598759413 + 0.001 * 6.89601469039917
Epoch 950, val loss: 1.3410313129425049
Epoch 960, training loss: 0.009746291674673557 = 0.0028373755048960447 + 0.001 * 6.908915996551514
Epoch 960, val loss: 1.3468233346939087
Epoch 970, training loss: 0.00965641438961029 = 0.0027535324916243553 + 0.001 * 6.902881145477295
Epoch 970, val loss: 1.3525668382644653
Epoch 980, training loss: 0.0095727713778615 = 0.0026734520215541124 + 0.001 * 6.899319171905518
Epoch 980, val loss: 1.3582830429077148
Epoch 990, training loss: 0.009496827609837055 = 0.0025969536509364843 + 0.001 * 6.899873733520508
Epoch 990, val loss: 1.3638710975646973
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8007
Flip ASR: 0.7689/225 nodes
The final ASR:0.64576, 0.14016, Accuracy:0.81235, 0.01666
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11656])
remove edge: torch.Size([2, 9510])
updated graph: torch.Size([2, 10610])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98278, 0.00696, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9790691137313843 = 1.9706952571868896 + 0.001 * 8.3739013671875
Epoch 0, val loss: 1.9677273035049438
Epoch 10, training loss: 1.9678035974502563 = 1.9594297409057617 + 0.001 * 8.373833656311035
Epoch 10, val loss: 1.957234263420105
Epoch 20, training loss: 1.9535037279129028 = 1.9451301097869873 + 0.001 * 8.37364673614502
Epoch 20, val loss: 1.94355309009552
Epoch 30, training loss: 1.9330261945724487 = 1.9246529340744019 + 0.001 * 8.373274803161621
Epoch 30, val loss: 1.9237661361694336
Epoch 40, training loss: 1.9024189710617065 = 1.8940465450286865 + 0.001 * 8.372450828552246
Epoch 40, val loss: 1.8944981098175049
Epoch 50, training loss: 1.859591007232666 = 1.851220726966858 + 0.001 * 8.370296478271484
Epoch 50, val loss: 1.855631947517395
Epoch 60, training loss: 1.8124362230300903 = 1.8040733337402344 + 0.001 * 8.362857818603516
Epoch 60, val loss: 1.8178808689117432
Epoch 70, training loss: 1.7742010354995728 = 1.7658737897872925 + 0.001 * 8.327279090881348
Epoch 70, val loss: 1.7892999649047852
Epoch 80, training loss: 1.7295302152633667 = 1.7214651107788086 + 0.001 * 8.065136909484863
Epoch 80, val loss: 1.7487787008285522
Epoch 90, training loss: 1.6678909063339233 = 1.6600042581558228 + 0.001 * 7.886642932891846
Epoch 90, val loss: 1.6951063871383667
Epoch 100, training loss: 1.585854172706604 = 1.5780366659164429 + 0.001 * 7.817503929138184
Epoch 100, val loss: 1.6269820928573608
Epoch 110, training loss: 1.4904654026031494 = 1.482702374458313 + 0.001 * 7.763036251068115
Epoch 110, val loss: 1.5491876602172852
Epoch 120, training loss: 1.394133448600769 = 1.3865009546279907 + 0.001 * 7.632464408874512
Epoch 120, val loss: 1.471319556236267
Epoch 130, training loss: 1.3018925189971924 = 1.2944341897964478 + 0.001 * 7.458337306976318
Epoch 130, val loss: 1.398278832435608
Epoch 140, training loss: 1.2129924297332764 = 1.2055662870407104 + 0.001 * 7.426141738891602
Epoch 140, val loss: 1.3293914794921875
Epoch 150, training loss: 1.126636266708374 = 1.119297742843628 + 0.001 * 7.338561058044434
Epoch 150, val loss: 1.2642720937728882
Epoch 160, training loss: 1.0433964729309082 = 1.0361695289611816 + 0.001 * 7.226966857910156
Epoch 160, val loss: 1.202629566192627
Epoch 170, training loss: 0.963796079158783 = 0.9566397666931152 + 0.001 * 7.156334400177002
Epoch 170, val loss: 1.1443002223968506
Epoch 180, training loss: 0.8874797224998474 = 0.8803489208221436 + 0.001 * 7.130781173706055
Epoch 180, val loss: 1.0885695219039917
Epoch 190, training loss: 0.8130869269371033 = 0.8059840202331543 + 0.001 * 7.102882385253906
Epoch 190, val loss: 1.0341476202011108
Epoch 200, training loss: 0.7400354743003845 = 0.7329521775245667 + 0.001 * 7.083316326141357
Epoch 200, val loss: 0.9814619421958923
Epoch 210, training loss: 0.669477641582489 = 0.6623997688293457 + 0.001 * 7.077885627746582
Epoch 210, val loss: 0.9315508604049683
Epoch 220, training loss: 0.6036887168884277 = 0.5966120362281799 + 0.001 * 7.076662063598633
Epoch 220, val loss: 0.8867830634117126
Epoch 230, training loss: 0.5445268154144287 = 0.5374512076377869 + 0.001 * 7.075623989105225
Epoch 230, val loss: 0.8491625189781189
Epoch 240, training loss: 0.49244076013565063 = 0.48536619544029236 + 0.001 * 7.07456111907959
Epoch 240, val loss: 0.8192518353462219
Epoch 250, training loss: 0.4464437663555145 = 0.439370721578598 + 0.001 * 7.073039531707764
Epoch 250, val loss: 0.7959409952163696
Epoch 260, training loss: 0.40511003136634827 = 0.3980388641357422 + 0.001 * 7.0711669921875
Epoch 260, val loss: 0.7776810526847839
Epoch 270, training loss: 0.3673107624053955 = 0.3602416217327118 + 0.001 * 7.069152355194092
Epoch 270, val loss: 0.7633184790611267
Epoch 280, training loss: 0.33257463574409485 = 0.3255074620246887 + 0.001 * 7.067172050476074
Epoch 280, val loss: 0.7526485323905945
Epoch 290, training loss: 0.3007538318634033 = 0.29368820786476135 + 0.001 * 7.065619945526123
Epoch 290, val loss: 0.7456567287445068
Epoch 300, training loss: 0.27163165807724 = 0.2645682096481323 + 0.001 * 7.063436031341553
Epoch 300, val loss: 0.7422841191291809
Epoch 310, training loss: 0.2447751760482788 = 0.23771348595619202 + 0.001 * 7.061685085296631
Epoch 310, val loss: 0.7424072623252869
Epoch 320, training loss: 0.21969246864318848 = 0.2126326858997345 + 0.001 * 7.059789657592773
Epoch 320, val loss: 0.7457306981086731
Epoch 330, training loss: 0.19610874354839325 = 0.1890484094619751 + 0.001 * 7.060338973999023
Epoch 330, val loss: 0.7519433498382568
Epoch 340, training loss: 0.1740780472755432 = 0.16702155768871307 + 0.001 * 7.056486129760742
Epoch 340, val loss: 0.7609434127807617
Epoch 350, training loss: 0.15389391779899597 = 0.14683882892131805 + 0.001 * 7.055089950561523
Epoch 350, val loss: 0.7726476788520813
Epoch 360, training loss: 0.13583029806613922 = 0.12877483665943146 + 0.001 * 7.055459499359131
Epoch 360, val loss: 0.786937415599823
Epoch 370, training loss: 0.11994747072458267 = 0.11289497464895248 + 0.001 * 7.05249547958374
Epoch 370, val loss: 0.8033431768417358
Epoch 380, training loss: 0.10613880306482315 = 0.0990886241197586 + 0.001 * 7.050175189971924
Epoch 380, val loss: 0.8213681578636169
Epoch 390, training loss: 0.09420280903577805 = 0.08715483546257019 + 0.001 * 7.047975540161133
Epoch 390, val loss: 0.8403533697128296
Epoch 400, training loss: 0.08392709493637085 = 0.07687130570411682 + 0.001 * 7.055786609649658
Epoch 400, val loss: 0.8597739934921265
Epoch 410, training loss: 0.0750577300786972 = 0.0680130124092102 + 0.001 * 7.0447187423706055
Epoch 410, val loss: 0.8791581988334656
Epoch 420, training loss: 0.06742781400680542 = 0.060380447655916214 + 0.001 * 7.047368049621582
Epoch 420, val loss: 0.8982563018798828
Epoch 430, training loss: 0.06082979217171669 = 0.05379321798682213 + 0.001 * 7.036572456359863
Epoch 430, val loss: 0.9169574975967407
Epoch 440, training loss: 0.05514487996697426 = 0.048103466629981995 + 0.001 * 7.041411876678467
Epoch 440, val loss: 0.9351457357406616
Epoch 450, training loss: 0.05020231008529663 = 0.043179187923669815 + 0.001 * 7.023123741149902
Epoch 450, val loss: 0.9528871774673462
Epoch 460, training loss: 0.0459267795085907 = 0.03890564665198326 + 0.001 * 7.021131992340088
Epoch 460, val loss: 0.9701056480407715
Epoch 470, training loss: 0.04220946878194809 = 0.0351875014603138 + 0.001 * 7.021968364715576
Epoch 470, val loss: 0.9868050813674927
Epoch 480, training loss: 0.038945067673921585 = 0.031943369656801224 + 0.001 * 7.001697540283203
Epoch 480, val loss: 1.002974271774292
Epoch 490, training loss: 0.03612241894006729 = 0.029104318469762802 + 0.001 * 7.018101692199707
Epoch 490, val loss: 1.018629550933838
Epoch 500, training loss: 0.03361761197447777 = 0.026611527428030968 + 0.001 * 7.006084442138672
Epoch 500, val loss: 1.0336670875549316
Epoch 510, training loss: 0.03142555058002472 = 0.024415185675024986 + 0.001 * 7.010366439819336
Epoch 510, val loss: 1.0482425689697266
Epoch 520, training loss: 0.02945026569068432 = 0.022472990676760674 + 0.001 * 6.9772748947143555
Epoch 520, val loss: 1.0623236894607544
Epoch 530, training loss: 0.02772841975092888 = 0.020749839022755623 + 0.001 * 6.978579521179199
Epoch 530, val loss: 1.075934886932373
Epoch 540, training loss: 0.026185836642980576 = 0.019215818494558334 + 0.001 * 6.970017433166504
Epoch 540, val loss: 1.0891170501708984
Epoch 550, training loss: 0.024813860654830933 = 0.017845505848526955 + 0.001 * 6.968355178833008
Epoch 550, val loss: 1.1018414497375488
Epoch 560, training loss: 0.02358773536980152 = 0.0166174229234457 + 0.001 * 6.970312118530273
Epoch 560, val loss: 1.1141735315322876
Epoch 570, training loss: 0.022474147379398346 = 0.015513081103563309 + 0.001 * 6.961066246032715
Epoch 570, val loss: 1.1260769367218018
Epoch 580, training loss: 0.021528176963329315 = 0.014517011120915413 + 0.001 * 7.011166095733643
Epoch 580, val loss: 1.137601375579834
Epoch 590, training loss: 0.02058357745409012 = 0.01361599937081337 + 0.001 * 6.9675774574279785
Epoch 590, val loss: 1.1487680673599243
Epoch 600, training loss: 0.019749518483877182 = 0.012798696756362915 + 0.001 * 6.950822353363037
Epoch 600, val loss: 1.159593939781189
Epoch 610, training loss: 0.019029395654797554 = 0.012055178172886372 + 0.001 * 6.974216461181641
Epoch 610, val loss: 1.1700820922851562
Epoch 620, training loss: 0.018324337899684906 = 0.011376958340406418 + 0.001 * 6.947378158569336
Epoch 620, val loss: 1.1802397966384888
Epoch 630, training loss: 0.017691832035779953 = 0.010756779462099075 + 0.001 * 6.935052871704102
Epoch 630, val loss: 1.1900956630706787
Epoch 640, training loss: 0.0171404629945755 = 0.01018823217600584 + 0.001 * 6.952229976654053
Epoch 640, val loss: 1.199669599533081
Epoch 650, training loss: 0.016607757657766342 = 0.009665902704000473 + 0.001 * 6.941855430603027
Epoch 650, val loss: 1.2089585065841675
Epoch 660, training loss: 0.01611671783030033 = 0.009184896014630795 + 0.001 * 6.931821823120117
Epoch 660, val loss: 1.217981219291687
Epoch 670, training loss: 0.015673886984586716 = 0.008740951307117939 + 0.001 * 6.9329352378845215
Epoch 670, val loss: 1.2267781496047974
Epoch 680, training loss: 0.015264688059687614 = 0.008330409415066242 + 0.001 * 6.9342780113220215
Epoch 680, val loss: 1.2353183031082153
Epoch 690, training loss: 0.014949791133403778 = 0.007950055412948132 + 0.001 * 6.999735355377197
Epoch 690, val loss: 1.2436494827270508
Epoch 700, training loss: 0.014519823715090752 = 0.007596942596137524 + 0.001 * 6.92288064956665
Epoch 700, val loss: 1.2517340183258057
Epoch 710, training loss: 0.014179387129843235 = 0.007268581073731184 + 0.001 * 6.910805702209473
Epoch 710, val loss: 1.2596259117126465
Epoch 720, training loss: 0.013896229676902294 = 0.006962659768760204 + 0.001 * 6.933569431304932
Epoch 720, val loss: 1.2673193216323853
Epoch 730, training loss: 0.013579189777374268 = 0.006677248980849981 + 0.001 * 6.90194034576416
Epoch 730, val loss: 1.2747949361801147
Epoch 740, training loss: 0.01334927324205637 = 0.006410549394786358 + 0.001 * 6.938723564147949
Epoch 740, val loss: 1.282097578048706
Epoch 750, training loss: 0.013102946802973747 = 0.006160919088870287 + 0.001 * 6.942028045654297
Epoch 750, val loss: 1.2892146110534668
Epoch 760, training loss: 0.012851152569055557 = 0.005926921032369137 + 0.001 * 6.924231052398682
Epoch 760, val loss: 1.296169400215149
Epoch 770, training loss: 0.012598227709531784 = 0.005707299802452326 + 0.001 * 6.890927791595459
Epoch 770, val loss: 1.3029701709747314
Epoch 780, training loss: 0.01241527870297432 = 0.005500860512256622 + 0.001 * 6.9144182205200195
Epoch 780, val loss: 1.3096123933792114
Epoch 790, training loss: 0.01220286637544632 = 0.005306558683514595 + 0.001 * 6.896307468414307
Epoch 790, val loss: 1.3160887956619263
Epoch 800, training loss: 0.012012005783617496 = 0.005123480223119259 + 0.001 * 6.888525009155273
Epoch 800, val loss: 1.3224247694015503
Epoch 810, training loss: 0.011865038424730301 = 0.004950758535414934 + 0.001 * 6.914279460906982
Epoch 810, val loss: 1.328612208366394
Epoch 820, training loss: 0.011661173775792122 = 0.0047876061871647835 + 0.001 * 6.873566627502441
Epoch 820, val loss: 1.3346797227859497
Epoch 830, training loss: 0.011513952165842056 = 0.00463336193934083 + 0.001 * 6.880589485168457
Epoch 830, val loss: 1.3406082391738892
Epoch 840, training loss: 0.011374421417713165 = 0.004487370606511831 + 0.001 * 6.887051105499268
Epoch 840, val loss: 1.3464107513427734
Epoch 850, training loss: 0.011218646541237831 = 0.004349066875874996 + 0.001 * 6.869579315185547
Epoch 850, val loss: 1.3520779609680176
Epoch 860, training loss: 0.01108536683022976 = 0.004217894747853279 + 0.001 * 6.867471218109131
Epoch 860, val loss: 1.3576147556304932
Epoch 870, training loss: 0.010990269482135773 = 0.0040933857671916485 + 0.001 * 6.896883487701416
Epoch 870, val loss: 1.3630316257476807
Epoch 880, training loss: 0.010844149626791477 = 0.003975057974457741 + 0.001 * 6.869091510772705
Epoch 880, val loss: 1.3683583736419678
Epoch 890, training loss: 0.010730800218880177 = 0.003862501587718725 + 0.001 * 6.868298530578613
Epoch 890, val loss: 1.37356698513031
Epoch 900, training loss: 0.010608570650219917 = 0.003755273763090372 + 0.001 * 6.853297233581543
Epoch 900, val loss: 1.3786592483520508
Epoch 910, training loss: 0.010515149682760239 = 0.003652839921414852 + 0.001 * 6.862309455871582
Epoch 910, val loss: 1.3836700916290283
Epoch 920, training loss: 0.010430817492306232 = 0.0035545595455914736 + 0.001 * 6.87625789642334
Epoch 920, val loss: 1.3885530233383179
Epoch 930, training loss: 0.01036532036960125 = 0.0034598838537931442 + 0.001 * 6.9054365158081055
Epoch 930, val loss: 1.393393635749817
Epoch 940, training loss: 0.010219788178801537 = 0.003368438920006156 + 0.001 * 6.851348876953125
Epoch 940, val loss: 1.3981026411056519
Epoch 950, training loss: 0.01016438752412796 = 0.0032797660678625107 + 0.001 * 6.884620666503906
Epoch 950, val loss: 1.4027622938156128
Epoch 960, training loss: 0.010035686194896698 = 0.003193487413227558 + 0.001 * 6.842197895050049
Epoch 960, val loss: 1.4073678255081177
Epoch 970, training loss: 0.0099484883248806 = 0.0031094010919332504 + 0.001 * 6.839086532592773
Epoch 970, val loss: 1.4119536876678467
Epoch 980, training loss: 0.009864535182714462 = 0.0030272286385297775 + 0.001 * 6.837305545806885
Epoch 980, val loss: 1.4165393114089966
Epoch 990, training loss: 0.00977462437003851 = 0.002946790074929595 + 0.001 * 6.827834129333496
Epoch 990, val loss: 1.4211175441741943
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8000
Overall ASR: 0.5720
Flip ASR: 0.4844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9384733438491821 = 1.9300994873046875 + 0.001 * 8.373871803283691
Epoch 0, val loss: 1.9296996593475342
Epoch 10, training loss: 1.929038405418396 = 1.9206645488739014 + 0.001 * 8.373822212219238
Epoch 10, val loss: 1.9193388223648071
Epoch 20, training loss: 1.9174596071243286 = 1.909085988998413 + 0.001 * 8.373641967773438
Epoch 20, val loss: 1.9065266847610474
Epoch 30, training loss: 1.901087760925293 = 1.892714500427246 + 0.001 * 8.373278617858887
Epoch 30, val loss: 1.8883334398269653
Epoch 40, training loss: 1.8769688606262207 = 1.8685963153839111 + 0.001 * 8.3724946975708
Epoch 40, val loss: 1.861660361289978
Epoch 50, training loss: 1.843170166015625 = 1.8347996473312378 + 0.001 * 8.370539665222168
Epoch 50, val loss: 1.8254978656768799
Epoch 60, training loss: 1.8026121854782104 = 1.7942478656768799 + 0.001 * 8.36434555053711
Epoch 60, val loss: 1.7850052118301392
Epoch 70, training loss: 1.7616119384765625 = 1.7532751560211182 + 0.001 * 8.33682918548584
Epoch 70, val loss: 1.7471095323562622
Epoch 80, training loss: 1.711700201034546 = 1.703568696975708 + 0.001 * 8.131476402282715
Epoch 80, val loss: 1.7024495601654053
Epoch 90, training loss: 1.6430824995040894 = 1.6352579593658447 + 0.001 * 7.824489116668701
Epoch 90, val loss: 1.643726110458374
Epoch 100, training loss: 1.554520606994629 = 1.5468206405639648 + 0.001 * 7.699996471405029
Epoch 100, val loss: 1.5691170692443848
Epoch 110, training loss: 1.4513664245605469 = 1.4438591003417969 + 0.001 * 7.507349014282227
Epoch 110, val loss: 1.4841594696044922
Epoch 120, training loss: 1.3428056240081787 = 1.3353952169418335 + 0.001 * 7.4103474617004395
Epoch 120, val loss: 1.3978452682495117
Epoch 130, training loss: 1.2322396039962769 = 1.2248467206954956 + 0.001 * 7.392870903015137
Epoch 130, val loss: 1.3127540349960327
Epoch 140, training loss: 1.1214585304260254 = 1.1141016483306885 + 0.001 * 7.356849193572998
Epoch 140, val loss: 1.2297359704971313
Epoch 150, training loss: 1.012820839881897 = 1.0055053234100342 + 0.001 * 7.3155293464660645
Epoch 150, val loss: 1.1494221687316895
Epoch 160, training loss: 0.910599946975708 = 0.9033283591270447 + 0.001 * 7.271614074707031
Epoch 160, val loss: 1.0743461847305298
Epoch 170, training loss: 0.8198040723800659 = 0.8125626444816589 + 0.001 * 7.2414021492004395
Epoch 170, val loss: 1.008234977722168
Epoch 180, training loss: 0.7425137758255005 = 0.7352890968322754 + 0.001 * 7.224661350250244
Epoch 180, val loss: 0.953900158405304
Epoch 190, training loss: 0.6767462491989136 = 0.6695381999015808 + 0.001 * 7.208043575286865
Epoch 190, val loss: 0.9104685187339783
Epoch 200, training loss: 0.6187149286270142 = 0.6115214228630066 + 0.001 * 7.193490982055664
Epoch 200, val loss: 0.8748612999916077
Epoch 210, training loss: 0.5653862953186035 = 0.5582085847854614 + 0.001 * 7.17771053314209
Epoch 210, val loss: 0.8446425199508667
Epoch 220, training loss: 0.5150248408317566 = 0.5078678131103516 + 0.001 * 7.157014846801758
Epoch 220, val loss: 0.8182238936424255
Epoch 230, training loss: 0.4673040509223938 = 0.4601772427558899 + 0.001 * 7.126819610595703
Epoch 230, val loss: 0.7956039309501648
Epoch 240, training loss: 0.4227193593978882 = 0.41561102867126465 + 0.001 * 7.1083292961120605
Epoch 240, val loss: 0.7771098017692566
Epoch 250, training loss: 0.38216450810432434 = 0.3750893175601959 + 0.001 * 7.075189113616943
Epoch 250, val loss: 0.7630939483642578
Epoch 260, training loss: 0.34578025341033936 = 0.33871185779571533 + 0.001 * 7.06839656829834
Epoch 260, val loss: 0.7535110712051392
Epoch 270, training loss: 0.3130757510662079 = 0.3060101866722107 + 0.001 * 7.065564155578613
Epoch 270, val loss: 0.7484778165817261
Epoch 280, training loss: 0.2836373746395111 = 0.2765715718269348 + 0.001 * 7.065814971923828
Epoch 280, val loss: 0.7472971081733704
Epoch 290, training loss: 0.25688624382019043 = 0.24981969594955444 + 0.001 * 7.066562175750732
Epoch 290, val loss: 0.7495446801185608
Epoch 300, training loss: 0.23254843056201935 = 0.22548089921474457 + 0.001 * 7.0675272941589355
Epoch 300, val loss: 0.754841148853302
Epoch 310, training loss: 0.21050837635993958 = 0.20343996584415436 + 0.001 * 7.068409442901611
Epoch 310, val loss: 0.762778639793396
Epoch 320, training loss: 0.19062016904354095 = 0.18355099856853485 + 0.001 * 7.0691657066345215
Epoch 320, val loss: 0.7730515599250793
Epoch 330, training loss: 0.17275159060955048 = 0.16568176448345184 + 0.001 * 7.069823741912842
Epoch 330, val loss: 0.7851672768592834
Epoch 340, training loss: 0.15666212141513824 = 0.1495918333530426 + 0.001 * 7.0702924728393555
Epoch 340, val loss: 0.7987670302391052
Epoch 350, training loss: 0.1421462893486023 = 0.13507001101970673 + 0.001 * 7.076284885406494
Epoch 350, val loss: 0.8133637309074402
Epoch 360, training loss: 0.1289864182472229 = 0.12191493064165115 + 0.001 * 7.071482181549072
Epoch 360, val loss: 0.8287023901939392
Epoch 370, training loss: 0.11700301617383957 = 0.10993179678916931 + 0.001 * 7.0712199211120605
Epoch 370, val loss: 0.8445286154747009
Epoch 380, training loss: 0.10606298595666885 = 0.09899253398180008 + 0.001 * 7.070453643798828
Epoch 380, val loss: 0.8606415390968323
Epoch 390, training loss: 0.09589097648859024 = 0.08882137387990952 + 0.001 * 7.069602966308594
Epoch 390, val loss: 0.8770875930786133
Epoch 400, training loss: 0.08655060827732086 = 0.07948140799999237 + 0.001 * 7.0692009925842285
Epoch 400, val loss: 0.8937312364578247
Epoch 410, training loss: 0.07798803597688675 = 0.07092217355966568 + 0.001 * 7.0658650398254395
Epoch 410, val loss: 0.9109240770339966
Epoch 420, training loss: 0.07024674117565155 = 0.06318195909261703 + 0.001 * 7.06478214263916
Epoch 420, val loss: 0.9285579323768616
Epoch 430, training loss: 0.06330493092536926 = 0.056243471801280975 + 0.001 * 7.06145715713501
Epoch 430, val loss: 0.9465612173080444
Epoch 440, training loss: 0.05714014917612076 = 0.0500861331820488 + 0.001 * 7.054015159606934
Epoch 440, val loss: 0.9647423624992371
Epoch 450, training loss: 0.05174478143453598 = 0.044698361307382584 + 0.001 * 7.046419143676758
Epoch 450, val loss: 0.9833734631538391
Epoch 460, training loss: 0.047102417796850204 = 0.04005665332078934 + 0.001 * 7.045762538909912
Epoch 460, val loss: 1.0020686388015747
Epoch 470, training loss: 0.04311294108629227 = 0.03607802838087082 + 0.001 * 7.034913539886475
Epoch 470, val loss: 1.0207566022872925
Epoch 480, training loss: 0.03967490792274475 = 0.03263557702302933 + 0.001 * 7.03933048248291
Epoch 480, val loss: 1.0391703844070435
Epoch 490, training loss: 0.03666849806904793 = 0.029641367495059967 + 0.001 * 7.0271315574646
Epoch 490, val loss: 1.0571755170822144
Epoch 500, training loss: 0.03404261916875839 = 0.027032112702727318 + 0.001 * 7.010507583618164
Epoch 500, val loss: 1.0747390985488892
Epoch 510, training loss: 0.031756747514009476 = 0.024751968681812286 + 0.001 * 7.004776954650879
Epoch 510, val loss: 1.0918513536453247
Epoch 520, training loss: 0.029770003631711006 = 0.022752035409212112 + 0.001 * 7.01796817779541
Epoch 520, val loss: 1.1086159944534302
Epoch 530, training loss: 0.027971474453806877 = 0.02098817378282547 + 0.001 * 6.983300685882568
Epoch 530, val loss: 1.1248661279678345
Epoch 540, training loss: 0.026416419073939323 = 0.019424766302108765 + 0.001 * 6.991652965545654
Epoch 540, val loss: 1.1406902074813843
Epoch 550, training loss: 0.025027472525835037 = 0.018032744526863098 + 0.001 * 6.994728088378906
Epoch 550, val loss: 1.1560277938842773
Epoch 560, training loss: 0.023763637989759445 = 0.016789047047495842 + 0.001 * 6.974591255187988
Epoch 560, val loss: 1.1709474325180054
Epoch 570, training loss: 0.022644763812422752 = 0.015672560781240463 + 0.001 * 6.972203254699707
Epoch 570, val loss: 1.1854411363601685
Epoch 580, training loss: 0.021632200106978416 = 0.014666641131043434 + 0.001 * 6.9655585289001465
Epoch 580, val loss: 1.199495792388916
Epoch 590, training loss: 0.020747292786836624 = 0.013757285661995411 + 0.001 * 6.990006923675537
Epoch 590, val loss: 1.2131398916244507
Epoch 600, training loss: 0.019902603700757027 = 0.01293211430311203 + 0.001 * 6.970489025115967
Epoch 600, val loss: 1.2264131307601929
Epoch 610, training loss: 0.019142428413033485 = 0.012181336060166359 + 0.001 * 6.9610915184021
Epoch 610, val loss: 1.2393637895584106
Epoch 620, training loss: 0.018462566658854485 = 0.011496289633214474 + 0.001 * 6.966276168823242
Epoch 620, val loss: 1.2518980503082275
Epoch 630, training loss: 0.017832167446613312 = 0.010869516991078854 + 0.001 * 6.962649345397949
Epoch 630, val loss: 1.2640777826309204
Epoch 640, training loss: 0.017241492867469788 = 0.010293260216712952 + 0.001 * 6.948232650756836
Epoch 640, val loss: 1.2759379148483276
Epoch 650, training loss: 0.01671678200364113 = 0.009763302281498909 + 0.001 * 6.9534783363342285
Epoch 650, val loss: 1.2875771522521973
Epoch 660, training loss: 0.01624988205730915 = 0.009275219403207302 + 0.001 * 6.974661827087402
Epoch 660, val loss: 1.2989516258239746
Epoch 670, training loss: 0.0157806146889925 = 0.008824574761092663 + 0.001 * 6.956038951873779
Epoch 670, val loss: 1.309999942779541
Epoch 680, training loss: 0.015348318964242935 = 0.00840783677995205 + 0.001 * 6.940481185913086
Epoch 680, val loss: 1.32071852684021
Epoch 690, training loss: 0.014955983497202396 = 0.008021547459065914 + 0.001 * 6.934435844421387
Epoch 690, val loss: 1.33111572265625
Epoch 700, training loss: 0.014592764899134636 = 0.0076631661504507065 + 0.001 * 6.929598808288574
Epoch 700, val loss: 1.3412877321243286
Epoch 710, training loss: 0.014267388731241226 = 0.007329822052270174 + 0.001 * 6.93756628036499
Epoch 710, val loss: 1.3511877059936523
Epoch 720, training loss: 0.013949476182460785 = 0.007019433658570051 + 0.001 * 6.930042743682861
Epoch 720, val loss: 1.3608591556549072
Epoch 730, training loss: 0.013702809810638428 = 0.006729906424880028 + 0.001 * 6.972903251647949
Epoch 730, val loss: 1.370292067527771
Epoch 740, training loss: 0.01338954921811819 = 0.00645937817171216 + 0.001 * 6.93017053604126
Epoch 740, val loss: 1.379494071006775
Epoch 750, training loss: 0.013150792568922043 = 0.006206297315657139 + 0.001 * 6.944494724273682
Epoch 750, val loss: 1.3884609937667847
Epoch 760, training loss: 0.012903394177556038 = 0.005969167221337557 + 0.001 * 6.9342265129089355
Epoch 760, val loss: 1.3971976041793823
Epoch 770, training loss: 0.012684337794780731 = 0.0057466523721814156 + 0.001 * 6.937685489654541
Epoch 770, val loss: 1.4057384729385376
Epoch 780, training loss: 0.01247207447886467 = 0.005537527147680521 + 0.001 * 6.934547424316406
Epoch 780, val loss: 1.4140770435333252
Epoch 790, training loss: 0.01226605474948883 = 0.005340860225260258 + 0.001 * 6.92519474029541
Epoch 790, val loss: 1.4222365617752075
Epoch 800, training loss: 0.012054601684212685 = 0.005155516322702169 + 0.001 * 6.899085521697998
Epoch 800, val loss: 1.4302184581756592
Epoch 810, training loss: 0.01192743331193924 = 0.0049806381575763226 + 0.001 * 6.946795463562012
Epoch 810, val loss: 1.4380351305007935
Epoch 820, training loss: 0.011731979437172413 = 0.004815550986677408 + 0.001 * 6.916428089141846
Epoch 820, val loss: 1.4456470012664795
Epoch 830, training loss: 0.011561237275600433 = 0.004659605678170919 + 0.001 * 6.9016313552856445
Epoch 830, val loss: 1.4531383514404297
Epoch 840, training loss: 0.0114081259816885 = 0.004511483013629913 + 0.001 * 6.896642684936523
Epoch 840, val loss: 1.4605236053466797
Epoch 850, training loss: 0.01127074845135212 = 0.004370265640318394 + 0.001 * 6.900482177734375
Epoch 850, val loss: 1.4678083658218384
Epoch 860, training loss: 0.01112308818846941 = 0.004234722815454006 + 0.001 * 6.888365268707275
Epoch 860, val loss: 1.475068211555481
Epoch 870, training loss: 0.010991120710968971 = 0.004103899467736483 + 0.001 * 6.887221336364746
Epoch 870, val loss: 1.4823654890060425
Epoch 880, training loss: 0.01088043674826622 = 0.003977615851908922 + 0.001 * 6.902820110321045
Epoch 880, val loss: 1.489652156829834
Epoch 890, training loss: 0.010746624320745468 = 0.0038557895459234715 + 0.001 * 6.890833854675293
Epoch 890, val loss: 1.4969539642333984
Epoch 900, training loss: 0.010628699325025082 = 0.003738200757652521 + 0.001 * 6.890498161315918
Epoch 900, val loss: 1.5042282342910767
Epoch 910, training loss: 0.010519448667764664 = 0.0036251877900213003 + 0.001 * 6.894260406494141
Epoch 910, val loss: 1.5114779472351074
Epoch 920, training loss: 0.010402871295809746 = 0.0035166325978934765 + 0.001 * 6.886238098144531
Epoch 920, val loss: 1.5186363458633423
Epoch 930, training loss: 0.010296211577951908 = 0.003412749618291855 + 0.001 * 6.8834614753723145
Epoch 930, val loss: 1.5257315635681152
Epoch 940, training loss: 0.010178516618907452 = 0.0033131269738078117 + 0.001 * 6.865389347076416
Epoch 940, val loss: 1.5327287912368774
Epoch 950, training loss: 0.010097108781337738 = 0.0032177709508687258 + 0.001 * 6.879337787628174
Epoch 950, val loss: 1.5396432876586914
Epoch 960, training loss: 0.009995900094509125 = 0.003126665949821472 + 0.001 * 6.869233131408691
Epoch 960, val loss: 1.5464773178100586
Epoch 970, training loss: 0.009919113479554653 = 0.0030394827481359243 + 0.001 * 6.8796305656433105
Epoch 970, val loss: 1.5531576871871948
Epoch 980, training loss: 0.00981212593615055 = 0.0029561822302639484 + 0.001 * 6.855942726135254
Epoch 980, val loss: 1.5597296953201294
Epoch 990, training loss: 0.00974125973880291 = 0.002876640995964408 + 0.001 * 6.864618301391602
Epoch 990, val loss: 1.5661972761154175
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.0849
Flip ASR: 0.0667/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.947495460510254 = 1.9391216039657593 + 0.001 * 8.373852729797363
Epoch 0, val loss: 1.939660906791687
Epoch 10, training loss: 1.937865138053894 = 1.929491400718689 + 0.001 * 8.373791694641113
Epoch 10, val loss: 1.9302465915679932
Epoch 20, training loss: 1.9261367321014404 = 1.917763113975525 + 0.001 * 8.37360954284668
Epoch 20, val loss: 1.9186620712280273
Epoch 30, training loss: 1.9094980955123901 = 1.9011248350143433 + 0.001 * 8.37325668334961
Epoch 30, val loss: 1.9020763635635376
Epoch 40, training loss: 1.8846614360809326 = 1.876288890838623 + 0.001 * 8.372535705566406
Epoch 40, val loss: 1.8776829242706299
Epoch 50, training loss: 1.8495087623596191 = 1.8411380052566528 + 0.001 * 8.370770454406738
Epoch 50, val loss: 1.8447346687316895
Epoch 60, training loss: 1.8081272840499878 = 1.799762487411499 + 0.001 * 8.364741325378418
Epoch 60, val loss: 1.8096132278442383
Epoch 70, training loss: 1.7674750089645386 = 1.7591415643692017 + 0.001 * 8.333464622497559
Epoch 70, val loss: 1.7766320705413818
Epoch 80, training loss: 1.7150421142578125 = 1.7069629430770874 + 0.001 * 8.079215049743652
Epoch 80, val loss: 1.7302453517913818
Epoch 90, training loss: 1.6429893970489502 = 1.6351737976074219 + 0.001 * 7.8156561851501465
Epoch 90, val loss: 1.667789101600647
Epoch 100, training loss: 1.5515284538269043 = 1.5437846183776855 + 0.001 * 7.7437896728515625
Epoch 100, val loss: 1.5920004844665527
Epoch 110, training loss: 1.4509767293930054 = 1.443390130996704 + 0.001 * 7.586568355560303
Epoch 110, val loss: 1.5112004280090332
Epoch 120, training loss: 1.350940465927124 = 1.343519687652588 + 0.001 * 7.420799732208252
Epoch 120, val loss: 1.433264970779419
Epoch 130, training loss: 1.2527762651443481 = 1.2453712224960327 + 0.001 * 7.405091285705566
Epoch 130, val loss: 1.3574738502502441
Epoch 140, training loss: 1.1554794311523438 = 1.148141622543335 + 0.001 * 7.337839126586914
Epoch 140, val loss: 1.2821917533874512
Epoch 150, training loss: 1.0609049797058105 = 1.053656816482544 + 0.001 * 7.248213291168213
Epoch 150, val loss: 1.2093125581741333
Epoch 160, training loss: 0.9714414477348328 = 0.9642508625984192 + 0.001 * 7.190610885620117
Epoch 160, val loss: 1.1408467292785645
Epoch 170, training loss: 0.8877705335617065 = 0.8805936574935913 + 0.001 * 7.176846027374268
Epoch 170, val loss: 1.0777164697647095
Epoch 180, training loss: 0.8095182776451111 = 0.8023692965507507 + 0.001 * 7.148966312408447
Epoch 180, val loss: 1.0196095705032349
Epoch 190, training loss: 0.7370546460151672 = 0.729938805103302 + 0.001 * 7.115848064422607
Epoch 190, val loss: 0.9670807719230652
Epoch 200, training loss: 0.670731782913208 = 0.6636481285095215 + 0.001 * 7.0836310386657715
Epoch 200, val loss: 0.9209240674972534
Epoch 210, training loss: 0.6097231507301331 = 0.6026521325111389 + 0.001 * 7.071008205413818
Epoch 210, val loss: 0.8815109133720398
Epoch 220, training loss: 0.5522550344467163 = 0.5451858043670654 + 0.001 * 7.0692033767700195
Epoch 220, val loss: 0.8476884961128235
Epoch 230, training loss: 0.4970843195915222 = 0.4900157153606415 + 0.001 * 7.068594932556152
Epoch 230, val loss: 0.8188994526863098
Epoch 240, training loss: 0.44343748688697815 = 0.43636903166770935 + 0.001 * 7.068454742431641
Epoch 240, val loss: 0.7945060729980469
Epoch 250, training loss: 0.39151331782341003 = 0.3844441771507263 + 0.001 * 7.069126129150391
Epoch 250, val loss: 0.7743436694145203
Epoch 260, training loss: 0.3421150743961334 = 0.33504509925842285 + 0.001 * 7.069978713989258
Epoch 260, val loss: 0.758388102054596
Epoch 270, training loss: 0.29661065340042114 = 0.2895394563674927 + 0.001 * 7.071210861206055
Epoch 270, val loss: 0.7469889521598816
Epoch 280, training loss: 0.256195604801178 = 0.24912290275096893 + 0.001 * 7.07269287109375
Epoch 280, val loss: 0.7405540347099304
Epoch 290, training loss: 0.2214633971452713 = 0.21438907086849213 + 0.001 * 7.074329853057861
Epoch 290, val loss: 0.7389492392539978
Epoch 300, training loss: 0.19229203462600708 = 0.18521597981452942 + 0.001 * 7.076049327850342
Epoch 300, val loss: 0.7415899038314819
Epoch 310, training loss: 0.16803446412086487 = 0.1609567552804947 + 0.001 * 7.077714920043945
Epoch 310, val loss: 0.7477096319198608
Epoch 320, training loss: 0.1478666216135025 = 0.1407863050699234 + 0.001 * 7.080322265625
Epoch 320, val loss: 0.7565749287605286
Epoch 330, training loss: 0.1310075968503952 = 0.12392626702785492 + 0.001 * 7.081326484680176
Epoch 330, val loss: 0.7675473093986511
Epoch 340, training loss: 0.11679928749799728 = 0.10971670597791672 + 0.001 * 7.082581996917725
Epoch 340, val loss: 0.7801576256752014
Epoch 350, training loss: 0.1047164797782898 = 0.09763263165950775 + 0.001 * 7.0838470458984375
Epoch 350, val loss: 0.7940866947174072
Epoch 360, training loss: 0.09434441477060318 = 0.08725936710834503 + 0.001 * 7.085043907165527
Epoch 360, val loss: 0.8091996312141418
Epoch 370, training loss: 0.08536224067211151 = 0.07827619463205338 + 0.001 * 7.086042404174805
Epoch 370, val loss: 0.8252455592155457
Epoch 380, training loss: 0.077524334192276 = 0.07043727487325668 + 0.001 * 7.0870585441589355
Epoch 380, val loss: 0.8421493172645569
Epoch 390, training loss: 0.07064152508974075 = 0.06355053931474686 + 0.001 * 7.0909881591796875
Epoch 390, val loss: 0.8597345948219299
Epoch 400, training loss: 0.06455613672733307 = 0.057467687875032425 + 0.001 * 7.088446617126465
Epoch 400, val loss: 0.8779343962669373
Epoch 410, training loss: 0.05916208773851395 = 0.05207321047782898 + 0.001 * 7.088875770568848
Epoch 410, val loss: 0.8966107368469238
Epoch 420, training loss: 0.05436546728014946 = 0.04727623611688614 + 0.001 * 7.089232444763184
Epoch 420, val loss: 0.9156600832939148
Epoch 430, training loss: 0.050090376287698746 = 0.04300127178430557 + 0.001 * 7.0891032218933105
Epoch 430, val loss: 0.9348891973495483
Epoch 440, training loss: 0.046277374029159546 = 0.03918842971324921 + 0.001 * 7.088943004608154
Epoch 440, val loss: 0.9542267918586731
Epoch 450, training loss: 0.042876921594142914 = 0.03578632324934006 + 0.001 * 7.090597629547119
Epoch 450, val loss: 0.973473310470581
Epoch 460, training loss: 0.03983982652425766 = 0.032750699669122696 + 0.001 * 7.089127063751221
Epoch 460, val loss: 0.9924659132957458
Epoch 470, training loss: 0.037128958851099014 = 0.03004174679517746 + 0.001 * 7.087212085723877
Epoch 470, val loss: 1.011056900024414
Epoch 480, training loss: 0.034709278494119644 = 0.027623139321804047 + 0.001 * 7.08613920211792
Epoch 480, val loss: 1.029250144958496
Epoch 490, training loss: 0.032545968890190125 = 0.025461630895733833 + 0.001 * 7.084339141845703
Epoch 490, val loss: 1.047015905380249
Epoch 500, training loss: 0.03061610646545887 = 0.023527009412646294 + 0.001 * 7.089096546173096
Epoch 500, val loss: 1.0643103122711182
Epoch 510, training loss: 0.028875257819890976 = 0.021792560815811157 + 0.001 * 7.082695960998535
Epoch 510, val loss: 1.0810996294021606
Epoch 520, training loss: 0.02731349878013134 = 0.020234134048223495 + 0.001 * 7.07936429977417
Epoch 520, val loss: 1.0973412990570068
Epoch 530, training loss: 0.02590615302324295 = 0.01883067563176155 + 0.001 * 7.075477600097656
Epoch 530, val loss: 1.1130882501602173
Epoch 540, training loss: 0.024636108428239822 = 0.017563894391059875 + 0.001 * 7.072214126586914
Epoch 540, val loss: 1.1283438205718994
Epoch 550, training loss: 0.023501355201005936 = 0.01641758345067501 + 0.001 * 7.083771705627441
Epoch 550, val loss: 1.1431101560592651
Epoch 560, training loss: 0.022441936656832695 = 0.015377783216536045 + 0.001 * 7.06415319442749
Epoch 560, val loss: 1.157427430152893
Epoch 570, training loss: 0.021548978984355927 = 0.014432321302592754 + 0.001 * 7.11665678024292
Epoch 570, val loss: 1.1713026762008667
Epoch 580, training loss: 0.020632771775126457 = 0.013570655137300491 + 0.001 * 7.062115669250488
Epoch 580, val loss: 1.1847747564315796
Epoch 590, training loss: 0.0198352187871933 = 0.012783694081008434 + 0.001 * 7.051525115966797
Epoch 590, val loss: 1.197808027267456
Epoch 600, training loss: 0.019124113023281097 = 0.01206333190202713 + 0.001 * 7.060780048370361
Epoch 600, val loss: 1.2104707956314087
Epoch 610, training loss: 0.01847105473279953 = 0.01140260323882103 + 0.001 * 7.068450450897217
Epoch 610, val loss: 1.2227623462677002
Epoch 620, training loss: 0.017814408987760544 = 0.010795428417623043 + 0.001 * 7.018980503082275
Epoch 620, val loss: 1.2347050905227661
Epoch 630, training loss: 0.01729400083422661 = 0.010236273519694805 + 0.001 * 7.057727813720703
Epoch 630, val loss: 1.246320366859436
Epoch 640, training loss: 0.01674131117761135 = 0.009720338508486748 + 0.001 * 7.020972728729248
Epoch 640, val loss: 1.257633924484253
Epoch 650, training loss: 0.01623542793095112 = 0.009243427775800228 + 0.001 * 6.991999626159668
Epoch 650, val loss: 1.2686349153518677
Epoch 660, training loss: 0.015806814655661583 = 0.008801835589110851 + 0.001 * 7.004978656768799
Epoch 660, val loss: 1.2793368101119995
Epoch 670, training loss: 0.015416966751217842 = 0.008392203599214554 + 0.001 * 7.024763107299805
Epoch 670, val loss: 1.2898019552230835
Epoch 680, training loss: 0.015028547495603561 = 0.00801160093396902 + 0.001 * 7.016946315765381
Epoch 680, val loss: 1.299943447113037
Epoch 690, training loss: 0.014629574492573738 = 0.00765743013471365 + 0.001 * 6.97214412689209
Epoch 690, val loss: 1.3098574876785278
Epoch 700, training loss: 0.014351963996887207 = 0.007327322382479906 + 0.001 * 7.024641513824463
Epoch 700, val loss: 1.3195064067840576
Epoch 710, training loss: 0.013998666778206825 = 0.007019244600087404 + 0.001 * 6.979422092437744
Epoch 710, val loss: 1.3289117813110352
Epoch 720, training loss: 0.013807173818349838 = 0.006731271278113127 + 0.001 * 7.075902462005615
Epoch 720, val loss: 1.3381062746047974
Epoch 730, training loss: 0.013426987454295158 = 0.00646174093708396 + 0.001 * 6.965246677398682
Epoch 730, val loss: 1.3470624685287476
Epoch 740, training loss: 0.01319139450788498 = 0.006209154613316059 + 0.001 * 6.982240200042725
Epoch 740, val loss: 1.3558297157287598
Epoch 750, training loss: 0.0129618588835001 = 0.005972083657979965 + 0.001 * 6.989774703979492
Epoch 750, val loss: 1.364371657371521
Epoch 760, training loss: 0.012689191848039627 = 0.0057493350468575954 + 0.001 * 6.93985652923584
Epoch 760, val loss: 1.3727253675460815
Epoch 770, training loss: 0.012489160522818565 = 0.005539777688682079 + 0.001 * 6.949382305145264
Epoch 770, val loss: 1.3808757066726685
Epoch 780, training loss: 0.012270564213395119 = 0.005342382006347179 + 0.001 * 6.928182125091553
Epoch 780, val loss: 1.3888136148452759
Epoch 790, training loss: 0.012080268934369087 = 0.005156234372407198 + 0.001 * 6.924034118652344
Epoch 790, val loss: 1.3966203927993774
Epoch 800, training loss: 0.011918301694095135 = 0.00498048635199666 + 0.001 * 6.937815189361572
Epoch 800, val loss: 1.4042078256607056
Epoch 810, training loss: 0.011748265475034714 = 0.0048143574967980385 + 0.001 * 6.933907508850098
Epoch 810, val loss: 1.411646842956543
Epoch 820, training loss: 0.011602120473980904 = 0.004657090175896883 + 0.001 * 6.9450297355651855
Epoch 820, val loss: 1.4189478158950806
Epoch 830, training loss: 0.011426566168665886 = 0.004507972858846188 + 0.001 * 6.918593406677246
Epoch 830, val loss: 1.4260623455047607
Epoch 840, training loss: 0.01128180418163538 = 0.004366325214505196 + 0.001 * 6.915478706359863
Epoch 840, val loss: 1.433021068572998
Epoch 850, training loss: 0.011214136146008968 = 0.004231294617056847 + 0.001 * 6.9828410148620605
Epoch 850, val loss: 1.4398356676101685
Epoch 860, training loss: 0.010998314246535301 = 0.004101973492652178 + 0.001 * 6.896340847015381
Epoch 860, val loss: 1.4465194940567017
Epoch 870, training loss: 0.010886411182582378 = 0.0039779325015842915 + 0.001 * 6.908478260040283
Epoch 870, val loss: 1.4530632495880127
Epoch 880, training loss: 0.010752652771770954 = 0.003858703887090087 + 0.001 * 6.893948554992676
Epoch 880, val loss: 1.4595218896865845
Epoch 890, training loss: 0.010682829655706882 = 0.003744036192074418 + 0.001 * 6.938792705535889
Epoch 890, val loss: 1.4658693075180054
Epoch 900, training loss: 0.010524570941925049 = 0.003633792046457529 + 0.001 * 6.8907790184021
Epoch 900, val loss: 1.4720746278762817
Epoch 910, training loss: 0.010426031425595284 = 0.0035276839043945074 + 0.001 * 6.8983473777771
Epoch 910, val loss: 1.4782322645187378
Epoch 920, training loss: 0.010365014895796776 = 0.0034256435465067625 + 0.001 * 6.939371109008789
Epoch 920, val loss: 1.4842127561569214
Epoch 930, training loss: 0.010217600502073765 = 0.003327722428366542 + 0.001 * 6.889877796173096
Epoch 930, val loss: 1.4901574850082397
Epoch 940, training loss: 0.010126150213181973 = 0.0032335410360246897 + 0.001 * 6.892608642578125
Epoch 940, val loss: 1.4960110187530518
Epoch 950, training loss: 0.010037291795015335 = 0.0031431259121745825 + 0.001 * 6.894165992736816
Epoch 950, val loss: 1.5017300844192505
Epoch 960, training loss: 0.009959324263036251 = 0.003056296380236745 + 0.001 * 6.903027534484863
Epoch 960, val loss: 1.5073953866958618
Epoch 970, training loss: 0.009893592447042465 = 0.0029730533715337515 + 0.001 * 6.920538902282715
Epoch 970, val loss: 1.5129497051239014
Epoch 980, training loss: 0.009764882735908031 = 0.002893162425607443 + 0.001 * 6.871719837188721
Epoch 980, val loss: 1.5184082984924316
Epoch 990, training loss: 0.009703217074275017 = 0.0028165532276034355 + 0.001 * 6.886663913726807
Epoch 990, val loss: 1.5238093137741089
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7269
Flip ASR: 0.6756/225 nodes
The final ASR:0.46125, 0.27356, Accuracy:0.80988, 0.00924
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9562])
updated graph: torch.Size([2, 10646])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.98032, 0.00348, Accuracy:0.82963, 0.00302
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9513871669769287 = 1.943013310432434 + 0.001 * 8.373847961425781
Epoch 0, val loss: 1.948149561882019
Epoch 10, training loss: 1.9413721561431885 = 1.9329984188079834 + 0.001 * 8.373785018920898
Epoch 10, val loss: 1.938482403755188
Epoch 20, training loss: 1.929303765296936 = 1.9209301471710205 + 0.001 * 8.373580932617188
Epoch 20, val loss: 1.926151990890503
Epoch 30, training loss: 1.9125392436981201 = 1.9041661024093628 + 0.001 * 8.373175621032715
Epoch 30, val loss: 1.9086142778396606
Epoch 40, training loss: 1.8881601095199585 = 1.879787802696228 + 0.001 * 8.37236213684082
Epoch 40, val loss: 1.8832566738128662
Epoch 50, training loss: 1.8540682792663574 = 1.8456978797912598 + 0.001 * 8.370392799377441
Epoch 50, val loss: 1.849269986152649
Epoch 60, training loss: 1.8141450881958008 = 1.8057810068130493 + 0.001 * 8.364116668701172
Epoch 60, val loss: 1.813085913658142
Epoch 70, training loss: 1.7766984701156616 = 1.7683634757995605 + 0.001 * 8.335009574890137
Epoch 70, val loss: 1.78287672996521
Epoch 80, training loss: 1.731907844543457 = 1.723800539970398 + 0.001 * 8.107322692871094
Epoch 80, val loss: 1.7451759576797485
Epoch 90, training loss: 1.6689443588256836 = 1.6612253189086914 + 0.001 * 7.719011306762695
Epoch 90, val loss: 1.691528558731079
Epoch 100, training loss: 1.584517478942871 = 1.5770200490951538 + 0.001 * 7.497464656829834
Epoch 100, val loss: 1.620090126991272
Epoch 110, training loss: 1.4818507432937622 = 1.4745409488677979 + 0.001 * 7.309787750244141
Epoch 110, val loss: 1.5343937873840332
Epoch 120, training loss: 1.3697935342788696 = 1.362538456916809 + 0.001 * 7.255086421966553
Epoch 120, val loss: 1.441915512084961
Epoch 130, training loss: 1.2529067993164062 = 1.2457084655761719 + 0.001 * 7.198285102844238
Epoch 130, val loss: 1.3472929000854492
Epoch 140, training loss: 1.133433222770691 = 1.126280665397644 + 0.001 * 7.152541160583496
Epoch 140, val loss: 1.251869559288025
Epoch 150, training loss: 1.0157641172409058 = 1.008628487586975 + 0.001 * 7.135614395141602
Epoch 150, val loss: 1.1589046716690063
Epoch 160, training loss: 0.9052006006240845 = 0.8980807065963745 + 0.001 * 7.1198859214782715
Epoch 160, val loss: 1.0722768306732178
Epoch 170, training loss: 0.8050990700721741 = 0.7979956269264221 + 0.001 * 7.103471279144287
Epoch 170, val loss: 0.9947528839111328
Epoch 180, training loss: 0.7165771126747131 = 0.7094908356666565 + 0.001 * 7.086305141448975
Epoch 180, val loss: 0.9281778335571289
Epoch 190, training loss: 0.6394191384315491 = 0.6323484778404236 + 0.001 * 7.070634841918945
Epoch 190, val loss: 0.8725066184997559
Epoch 200, training loss: 0.5727030038833618 = 0.5656430125236511 + 0.001 * 7.060013294219971
Epoch 200, val loss: 0.8274945616722107
Epoch 210, training loss: 0.5149747133255005 = 0.5079219341278076 + 0.001 * 7.0527544021606445
Epoch 210, val loss: 0.7921359539031982
Epoch 220, training loss: 0.4644257426261902 = 0.4573783278465271 + 0.001 * 7.047418594360352
Epoch 220, val loss: 0.764685332775116
Epoch 230, training loss: 0.4191969931125641 = 0.4121539890766144 + 0.001 * 7.042993068695068
Epoch 230, val loss: 0.7432728409767151
Epoch 240, training loss: 0.3776824474334717 = 0.3706441819667816 + 0.001 * 7.038279056549072
Epoch 240, val loss: 0.7261966466903687
Epoch 250, training loss: 0.3387482762336731 = 0.33171403408050537 + 0.001 * 7.0342488288879395
Epoch 250, val loss: 0.7124308943748474
Epoch 260, training loss: 0.3016834557056427 = 0.2946541905403137 + 0.001 * 7.029269695281982
Epoch 260, val loss: 0.7010179162025452
Epoch 270, training loss: 0.2662200331687927 = 0.2591957747936249 + 0.001 * 7.024260520935059
Epoch 270, val loss: 0.6914248466491699
Epoch 280, training loss: 0.23263096809387207 = 0.22561174631118774 + 0.001 * 7.019214153289795
Epoch 280, val loss: 0.6834636926651001
Epoch 290, training loss: 0.20162056386470795 = 0.19460682570934296 + 0.001 * 7.01373291015625
Epoch 290, val loss: 0.6774711608886719
Epoch 300, training loss: 0.17395028471946716 = 0.16694031655788422 + 0.001 * 7.009963512420654
Epoch 300, val loss: 0.6741053462028503
Epoch 310, training loss: 0.15006297826766968 = 0.14306055009365082 + 0.001 * 7.002423286437988
Epoch 310, val loss: 0.673677384853363
Epoch 320, training loss: 0.1299227774143219 = 0.12292458117008209 + 0.001 * 6.998196601867676
Epoch 320, val loss: 0.6763423085212708
Epoch 330, training loss: 0.11313071846961975 = 0.10613676905632019 + 0.001 * 6.993950843811035
Epoch 330, val loss: 0.6819331645965576
Epoch 340, training loss: 0.0991537943482399 = 0.09216559678316116 + 0.001 * 6.988197326660156
Epoch 340, val loss: 0.6899960041046143
Epoch 350, training loss: 0.08747709542512894 = 0.08049134910106659 + 0.001 * 6.985746383666992
Epoch 350, val loss: 0.7001702785491943
Epoch 360, training loss: 0.07765589654445648 = 0.07067516446113586 + 0.001 * 6.9807353019714355
Epoch 360, val loss: 0.7118896245956421
Epoch 370, training loss: 0.06934842467308044 = 0.06236853823065758 + 0.001 * 6.979884624481201
Epoch 370, val loss: 0.7246910929679871
Epoch 380, training loss: 0.06227625533938408 = 0.05530116334557533 + 0.001 * 6.975090980529785
Epoch 380, val loss: 0.7382141947746277
Epoch 390, training loss: 0.05623333901166916 = 0.04925880208611488 + 0.001 * 6.974536895751953
Epoch 390, val loss: 0.7521388530731201
Epoch 400, training loss: 0.05104523152112961 = 0.044073112308979034 + 0.001 * 6.972119331359863
Epoch 400, val loss: 0.7662339210510254
Epoch 410, training loss: 0.046574030071496964 = 0.03960621356964111 + 0.001 * 6.9678168296813965
Epoch 410, val loss: 0.780293881893158
Epoch 420, training loss: 0.04270666837692261 = 0.03574423864483833 + 0.001 * 6.962430000305176
Epoch 420, val loss: 0.7941800355911255
Epoch 430, training loss: 0.03935519605875015 = 0.03239157050848007 + 0.001 * 6.963626384735107
Epoch 430, val loss: 0.807808518409729
Epoch 440, training loss: 0.036424484103918076 = 0.029468854889273643 + 0.001 * 6.955630302429199
Epoch 440, val loss: 0.821113109588623
Epoch 450, training loss: 0.033864617347717285 = 0.026910308748483658 + 0.001 * 6.954308032989502
Epoch 450, val loss: 0.8340622782707214
Epoch 460, training loss: 0.031611476093530655 = 0.024661164730787277 + 0.001 * 6.950311183929443
Epoch 460, val loss: 0.8466666340827942
Epoch 470, training loss: 0.02962789684534073 = 0.022676341235637665 + 0.001 * 6.951554775238037
Epoch 470, val loss: 0.8589186668395996
Epoch 480, training loss: 0.027865709736943245 = 0.02091786079108715 + 0.001 * 6.947849273681641
Epoch 480, val loss: 0.8708168864250183
Epoch 490, training loss: 0.02629745751619339 = 0.019354084506630898 + 0.001 * 6.94337272644043
Epoch 490, val loss: 0.8824178576469421
Epoch 500, training loss: 0.024900075048208237 = 0.01795823499560356 + 0.001 * 6.94183874130249
Epoch 500, val loss: 0.8936580419540405
Epoch 510, training loss: 0.023654235526919365 = 0.016708051785826683 + 0.001 * 6.946183681488037
Epoch 510, val loss: 0.9045734405517578
Epoch 520, training loss: 0.0225219763815403 = 0.015584547072649002 + 0.001 * 6.937428951263428
Epoch 520, val loss: 0.9151663780212402
Epoch 530, training loss: 0.021499285474419594 = 0.014571594074368477 + 0.001 * 6.927690505981445
Epoch 530, val loss: 0.9254441857337952
Epoch 540, training loss: 0.020647697150707245 = 0.013655516318976879 + 0.001 * 6.992180824279785
Epoch 540, val loss: 0.935437798500061
Epoch 550, training loss: 0.019765201956033707 = 0.012824821285903454 + 0.001 * 6.940381050109863
Epoch 550, val loss: 0.9451364874839783
Epoch 560, training loss: 0.01898733712732792 = 0.012069356627762318 + 0.001 * 6.917980194091797
Epoch 560, val loss: 0.9545619487762451
Epoch 570, training loss: 0.018293362110853195 = 0.011380433104932308 + 0.001 * 6.912928104400635
Epoch 570, val loss: 0.9637142419815063
Epoch 580, training loss: 0.017690509557724 = 0.01075065229088068 + 0.001 * 6.939856052398682
Epoch 580, val loss: 0.9726423621177673
Epoch 590, training loss: 0.017093556001782417 = 0.010173562914133072 + 0.001 * 6.9199934005737305
Epoch 590, val loss: 0.9813148975372314
Epoch 600, training loss: 0.016548944637179375 = 0.009643545374274254 + 0.001 * 6.905398368835449
Epoch 600, val loss: 0.9897359013557434
Epoch 610, training loss: 0.016065701842308044 = 0.009155678562819958 + 0.001 * 6.910023212432861
Epoch 610, val loss: 0.9979475140571594
Epoch 620, training loss: 0.015609784983098507 = 0.00870591402053833 + 0.001 * 6.903870582580566
Epoch 620, val loss: 1.0058469772338867
Epoch 630, training loss: 0.015190284699201584 = 0.008290271274745464 + 0.001 * 6.900013446807861
Epoch 630, val loss: 1.0136909484863281
Epoch 640, training loss: 0.014795532450079918 = 0.00790538638830185 + 0.001 * 6.890146255493164
Epoch 640, val loss: 1.0212794542312622
Epoch 650, training loss: 0.014452491886913776 = 0.0075483182445168495 + 0.001 * 6.904173374176025
Epoch 650, val loss: 1.0286999940872192
Epoch 660, training loss: 0.01411459594964981 = 0.007216467522084713 + 0.001 * 6.898128032684326
Epoch 660, val loss: 1.035898208618164
Epoch 670, training loss: 0.013784794136881828 = 0.006907529663294554 + 0.001 * 6.877264022827148
Epoch 670, val loss: 1.0429399013519287
Epoch 680, training loss: 0.013507416471838951 = 0.006619428750127554 + 0.001 * 6.887988090515137
Epoch 680, val loss: 1.0498223304748535
Epoch 690, training loss: 0.01323745958507061 = 0.006350336596369743 + 0.001 * 6.88712215423584
Epoch 690, val loss: 1.0565121173858643
Epoch 700, training loss: 0.012987361289560795 = 0.006098650395870209 + 0.001 * 6.8887104988098145
Epoch 700, val loss: 1.063052773475647
Epoch 710, training loss: 0.012737611308693886 = 0.005862912628799677 + 0.001 * 6.874697685241699
Epoch 710, val loss: 1.0694255828857422
Epoch 720, training loss: 0.012512752786278725 = 0.00564181013032794 + 0.001 * 6.870942115783691
Epoch 720, val loss: 1.0756253004074097
Epoch 730, training loss: 0.012303698807954788 = 0.00543412659317255 + 0.001 * 6.869572162628174
Epoch 730, val loss: 1.0816997289657593
Epoch 740, training loss: 0.012098011560738087 = 0.005238838028162718 + 0.001 * 6.85917329788208
Epoch 740, val loss: 1.0876214504241943
Epoch 750, training loss: 0.011938270181417465 = 0.005054906941950321 + 0.001 * 6.883362293243408
Epoch 750, val loss: 1.093397855758667
Epoch 760, training loss: 0.011751759797334671 = 0.004881520289927721 + 0.001 * 6.8702392578125
Epoch 760, val loss: 1.0990697145462036
Epoch 770, training loss: 0.011602861806750298 = 0.004717861767858267 + 0.001 * 6.884999752044678
Epoch 770, val loss: 1.1046159267425537
Epoch 780, training loss: 0.011432280763983727 = 0.004563252441585064 + 0.001 * 6.869028568267822
Epoch 780, val loss: 1.1100460290908813
Epoch 790, training loss: 0.011271834373474121 = 0.004416996613144875 + 0.001 * 6.854836940765381
Epoch 790, val loss: 1.1153326034545898
Epoch 800, training loss: 0.01113732997328043 = 0.004278518725186586 + 0.001 * 6.858810901641846
Epoch 800, val loss: 1.1205142736434937
Epoch 810, training loss: 0.011007994413375854 = 0.004147260449826717 + 0.001 * 6.8607330322265625
Epoch 810, val loss: 1.1256005764007568
Epoch 820, training loss: 0.01084885373711586 = 0.0040227496065199375 + 0.001 * 6.826104164123535
Epoch 820, val loss: 1.1305463314056396
Epoch 830, training loss: 0.010741429403424263 = 0.0039045303128659725 + 0.001 * 6.8368988037109375
Epoch 830, val loss: 1.1353914737701416
Epoch 840, training loss: 0.010606126859784126 = 0.0037921706680208445 + 0.001 * 6.813956260681152
Epoch 840, val loss: 1.1401506662368774
Epoch 850, training loss: 0.010532066226005554 = 0.0036852937191724777 + 0.001 * 6.846771717071533
Epoch 850, val loss: 1.1448255777359009
Epoch 860, training loss: 0.010471547022461891 = 0.003583547892048955 + 0.001 * 6.887998580932617
Epoch 860, val loss: 1.1493722200393677
Epoch 870, training loss: 0.010296998545527458 = 0.0034866060595959425 + 0.001 * 6.810391902923584
Epoch 870, val loss: 1.153872013092041
Epoch 880, training loss: 0.010220230557024479 = 0.0033941904548555613 + 0.001 * 6.8260393142700195
Epoch 880, val loss: 1.1582480669021606
Epoch 890, training loss: 0.010134294629096985 = 0.003305997932329774 + 0.001 * 6.828296184539795
Epoch 890, val loss: 1.1625440120697021
Epoch 900, training loss: 0.010046711191534996 = 0.003221814287826419 + 0.001 * 6.824896335601807
Epoch 900, val loss: 1.1667360067367554
Epoch 910, training loss: 0.009934266097843647 = 0.0031413596589118242 + 0.001 * 6.792906284332275
Epoch 910, val loss: 1.170880913734436
Epoch 920, training loss: 0.00988283846527338 = 0.003064414719119668 + 0.001 * 6.818423271179199
Epoch 920, val loss: 1.1749590635299683
Epoch 930, training loss: 0.009799616411328316 = 0.002990780398249626 + 0.001 * 6.808835983276367
Epoch 930, val loss: 1.178915023803711
Epoch 940, training loss: 0.009729158133268356 = 0.002920299069955945 + 0.001 * 6.808858394622803
Epoch 940, val loss: 1.1828054189682007
Epoch 950, training loss: 0.009660588577389717 = 0.0028527635149657726 + 0.001 * 6.807824611663818
Epoch 950, val loss: 1.1866294145584106
Epoch 960, training loss: 0.009584838524460793 = 0.0027880126144737005 + 0.001 * 6.796825885772705
Epoch 960, val loss: 1.1903660297393799
Epoch 970, training loss: 0.009509263560175896 = 0.0027259185444563627 + 0.001 * 6.783344268798828
Epoch 970, val loss: 1.1940221786499023
Epoch 980, training loss: 0.009440285153687 = 0.002666296437382698 + 0.001 * 6.773988246917725
Epoch 980, val loss: 1.1976429224014282
Epoch 990, training loss: 0.009432072751224041 = 0.0026090608444064856 + 0.001 * 6.823011875152588
Epoch 990, val loss: 1.2011690139770508
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.5609
Flip ASR: 0.4711/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9673986434936523 = 1.9590247869491577 + 0.001 * 8.373866081237793
Epoch 0, val loss: 1.964875340461731
Epoch 10, training loss: 1.9563416242599487 = 1.9479678869247437 + 0.001 * 8.373795509338379
Epoch 10, val loss: 1.9535343647003174
Epoch 20, training loss: 1.9429112672805786 = 1.934537649154663 + 0.001 * 8.37355899810791
Epoch 20, val loss: 1.9392369985580444
Epoch 30, training loss: 1.9241024255752563 = 1.9157294034957886 + 0.001 * 8.37307071685791
Epoch 30, val loss: 1.9186928272247314
Epoch 40, training loss: 1.896452784538269 = 1.8880807161331177 + 0.001 * 8.372025489807129
Epoch 40, val loss: 1.8883476257324219
Epoch 50, training loss: 1.85763418674469 = 1.8492647409439087 + 0.001 * 8.369458198547363
Epoch 50, val loss: 1.846902847290039
Epoch 60, training loss: 1.8121421337127686 = 1.8037807941436768 + 0.001 * 8.361305236816406
Epoch 60, val loss: 1.801609754562378
Epoch 70, training loss: 1.7708075046539307 = 1.7624844312667847 + 0.001 * 8.323019981384277
Epoch 70, val loss: 1.763870358467102
Epoch 80, training loss: 1.7241342067718506 = 1.716106653213501 + 0.001 * 8.027512550354004
Epoch 80, val loss: 1.7221472263336182
Epoch 90, training loss: 1.6594454050064087 = 1.651619553565979 + 0.001 * 7.825893402099609
Epoch 90, val loss: 1.6675447225570679
Epoch 100, training loss: 1.5738427639007568 = 1.566090703010559 + 0.001 * 7.752026081085205
Epoch 100, val loss: 1.5975992679595947
Epoch 110, training loss: 1.4739969968795776 = 1.4663655757904053 + 0.001 * 7.631431579589844
Epoch 110, val loss: 1.5186951160430908
Epoch 120, training loss: 1.374392032623291 = 1.3669590950012207 + 0.001 * 7.432969093322754
Epoch 120, val loss: 1.4437286853790283
Epoch 130, training loss: 1.2826600074768066 = 1.2752859592437744 + 0.001 * 7.37405252456665
Epoch 130, val loss: 1.379447102546692
Epoch 140, training loss: 1.1987169981002808 = 1.1914114952087402 + 0.001 * 7.305542469024658
Epoch 140, val loss: 1.3240689039230347
Epoch 150, training loss: 1.1199243068695068 = 1.112658977508545 + 0.001 * 7.265308380126953
Epoch 150, val loss: 1.2727982997894287
Epoch 160, training loss: 1.0444072484970093 = 1.0371612310409546 + 0.001 * 7.246062755584717
Epoch 160, val loss: 1.2232130765914917
Epoch 170, training loss: 0.971770703792572 = 0.9645413756370544 + 0.001 * 7.229311466217041
Epoch 170, val loss: 1.1747651100158691
Epoch 180, training loss: 0.9015294909477234 = 0.8943207263946533 + 0.001 * 7.208759784698486
Epoch 180, val loss: 1.1266852617263794
Epoch 190, training loss: 0.8322503566741943 = 0.8250666856765747 + 0.001 * 7.183643817901611
Epoch 190, val loss: 1.0781357288360596
Epoch 200, training loss: 0.7623490691184998 = 0.7551933526992798 + 0.001 * 7.155696868896484
Epoch 200, val loss: 1.028457760810852
Epoch 210, training loss: 0.6916636824607849 = 0.684535026550293 + 0.001 * 7.1286301612854
Epoch 210, val loss: 0.9779151082038879
Epoch 220, training loss: 0.6216471791267395 = 0.6145309209823608 + 0.001 * 7.116261005401611
Epoch 220, val loss: 0.9284842610359192
Epoch 230, training loss: 0.5541549921035767 = 0.5470480918884277 + 0.001 * 7.106897354125977
Epoch 230, val loss: 0.8831901550292969
Epoch 240, training loss: 0.49036845564842224 = 0.4832685887813568 + 0.001 * 7.099854469299316
Epoch 240, val loss: 0.8445935249328613
Epoch 250, training loss: 0.43093767762184143 = 0.42384427785873413 + 0.001 * 7.093388080596924
Epoch 250, val loss: 0.8136583566665649
Epoch 260, training loss: 0.3760872185230255 = 0.36900243163108826 + 0.001 * 7.084797382354736
Epoch 260, val loss: 0.7895665764808655
Epoch 270, training loss: 0.3260936439037323 = 0.31902045011520386 + 0.001 * 7.073182106018066
Epoch 270, val loss: 0.7711178064346313
Epoch 280, training loss: 0.28173449635505676 = 0.27467313408851624 + 0.001 * 7.061365127563477
Epoch 280, val loss: 0.7580345869064331
Epoch 290, training loss: 0.24349257349967957 = 0.236445814371109 + 0.001 * 7.0467610359191895
Epoch 290, val loss: 0.7504135966300964
Epoch 300, training loss: 0.2112816870212555 = 0.20424708724021912 + 0.001 * 7.03460693359375
Epoch 300, val loss: 0.7476438283920288
Epoch 310, training loss: 0.1844349503517151 = 0.17741218209266663 + 0.001 * 7.022763729095459
Epoch 310, val loss: 0.7491506934165955
Epoch 320, training loss: 0.16202570497989655 = 0.15500347316265106 + 0.001 * 7.0222296714782715
Epoch 320, val loss: 0.7542208433151245
Epoch 330, training loss: 0.1432715207338333 = 0.13626913726329803 + 0.001 * 7.002382278442383
Epoch 330, val loss: 0.7625364661216736
Epoch 340, training loss: 0.12750960886478424 = 0.12051297724246979 + 0.001 * 6.99663782119751
Epoch 340, val loss: 0.7730249762535095
Epoch 350, training loss: 0.11413773894309998 = 0.107135109603405 + 0.001 * 7.002631187438965
Epoch 350, val loss: 0.7854121327400208
Epoch 360, training loss: 0.1026846170425415 = 0.09568657726049423 + 0.001 * 6.998043060302734
Epoch 360, val loss: 0.7992780804634094
Epoch 370, training loss: 0.09279558062553406 = 0.08580887317657471 + 0.001 * 6.986705780029297
Epoch 370, val loss: 0.8140290975570679
Epoch 380, training loss: 0.08416396379470825 = 0.07717981934547424 + 0.001 * 6.984146595001221
Epoch 380, val loss: 0.8295380473136902
Epoch 390, training loss: 0.07657477259635925 = 0.06959035247564316 + 0.001 * 6.9844183921813965
Epoch 390, val loss: 0.8454675674438477
Epoch 400, training loss: 0.06984487920999527 = 0.06286436319351196 + 0.001 * 6.98051643371582
Epoch 400, val loss: 0.8617028594017029
Epoch 410, training loss: 0.06385133415460587 = 0.05687960982322693 + 0.001 * 6.971721172332764
Epoch 410, val loss: 0.8780614733695984
Epoch 420, training loss: 0.05850943177938461 = 0.05153980106115341 + 0.001 * 6.969628810882568
Epoch 420, val loss: 0.8944368958473206
Epoch 430, training loss: 0.05373778194189072 = 0.04676288366317749 + 0.001 * 6.9748969078063965
Epoch 430, val loss: 0.9108033180236816
Epoch 440, training loss: 0.04944157972931862 = 0.0424811951816082 + 0.001 * 6.960384845733643
Epoch 440, val loss: 0.9270939826965332
Epoch 450, training loss: 0.04560410603880882 = 0.038644321262836456 + 0.001 * 6.959784507751465
Epoch 450, val loss: 0.9431893825531006
Epoch 460, training loss: 0.042184535413980484 = 0.035212546586990356 + 0.001 * 6.971987724304199
Epoch 460, val loss: 0.959233820438385
Epoch 470, training loss: 0.03909796476364136 = 0.03214259818196297 + 0.001 * 6.955366134643555
Epoch 470, val loss: 0.9748709201812744
Epoch 480, training loss: 0.03634679690003395 = 0.029394928365945816 + 0.001 * 6.951866626739502
Epoch 480, val loss: 0.9901387691497803
Epoch 490, training loss: 0.033896490931510925 = 0.026939013972878456 + 0.001 * 6.957474708557129
Epoch 490, val loss: 1.0052802562713623
Epoch 500, training loss: 0.03169211000204086 = 0.024742592126131058 + 0.001 * 6.94951868057251
Epoch 500, val loss: 1.0201321840286255
Epoch 510, training loss: 0.02973640337586403 = 0.022775109857320786 + 0.001 * 6.961294174194336
Epoch 510, val loss: 1.0345556735992432
Epoch 520, training loss: 0.027951911091804504 = 0.021009275689721107 + 0.001 * 6.942634582519531
Epoch 520, val loss: 1.048691987991333
Epoch 530, training loss: 0.02637520246207714 = 0.01942199096083641 + 0.001 * 6.953211307525635
Epoch 530, val loss: 1.062491536140442
Epoch 540, training loss: 0.02494092285633087 = 0.017993250861763954 + 0.001 * 6.947671890258789
Epoch 540, val loss: 1.075954556465149
Epoch 550, training loss: 0.0236587543040514 = 0.016710005700588226 + 0.001 * 6.9487481117248535
Epoch 550, val loss: 1.0890569686889648
Epoch 560, training loss: 0.022499624639749527 = 0.015554471872746944 + 0.001 * 6.9451518058776855
Epoch 560, val loss: 1.1017231941223145
Epoch 570, training loss: 0.021451512351632118 = 0.014512048102915287 + 0.001 * 6.9394636154174805
Epoch 570, val loss: 1.1140705347061157
Epoch 580, training loss: 0.020499644801020622 = 0.013568379916250706 + 0.001 * 6.9312639236450195
Epoch 580, val loss: 1.1261399984359741
Epoch 590, training loss: 0.01963914930820465 = 0.012712020426988602 + 0.001 * 6.927127838134766
Epoch 590, val loss: 1.1378066539764404
Epoch 600, training loss: 0.018863540142774582 = 0.011934064328670502 + 0.001 * 6.929476261138916
Epoch 600, val loss: 1.1491451263427734
Epoch 610, training loss: 0.018153153359889984 = 0.011225502006709576 + 0.001 * 6.927651405334473
Epoch 610, val loss: 1.1601741313934326
Epoch 620, training loss: 0.0175161249935627 = 0.010578063316643238 + 0.001 * 6.938060283660889
Epoch 620, val loss: 1.1708967685699463
Epoch 630, training loss: 0.016918174922466278 = 0.009985608048737049 + 0.001 * 6.932566165924072
Epoch 630, val loss: 1.18132746219635
Epoch 640, training loss: 0.01636376976966858 = 0.009442160837352276 + 0.001 * 6.921608924865723
Epoch 640, val loss: 1.1914597749710083
Epoch 650, training loss: 0.015872254967689514 = 0.008942591026425362 + 0.001 * 6.929664134979248
Epoch 650, val loss: 1.2012819051742554
Epoch 660, training loss: 0.015417186543345451 = 0.008481035940349102 + 0.001 * 6.936149597167969
Epoch 660, val loss: 1.2109155654907227
Epoch 670, training loss: 0.014977577142417431 = 0.008054226636886597 + 0.001 * 6.9233503341674805
Epoch 670, val loss: 1.220184564590454
Epoch 680, training loss: 0.01458626240491867 = 0.007659692782908678 + 0.001 * 6.926569938659668
Epoch 680, val loss: 1.2292667627334595
Epoch 690, training loss: 0.014209296554327011 = 0.007294252049177885 + 0.001 * 6.915043830871582
Epoch 690, val loss: 1.2380670309066772
Epoch 700, training loss: 0.013859730213880539 = 0.006955210119485855 + 0.001 * 6.904519081115723
Epoch 700, val loss: 1.2466347217559814
Epoch 710, training loss: 0.013554884120821953 = 0.006640195846557617 + 0.001 * 6.914688587188721
Epoch 710, val loss: 1.2549998760223389
Epoch 720, training loss: 0.01326506957411766 = 0.006346888840198517 + 0.001 * 6.918180465698242
Epoch 720, val loss: 1.2631627321243286
Epoch 730, training loss: 0.012977457605302334 = 0.006073650438338518 + 0.001 * 6.903806686401367
Epoch 730, val loss: 1.271096110343933
Epoch 740, training loss: 0.012711680494248867 = 0.005818669684231281 + 0.001 * 6.89301061630249
Epoch 740, val loss: 1.2788245677947998
Epoch 750, training loss: 0.01247938722372055 = 0.005580318626016378 + 0.001 * 6.899068355560303
Epoch 750, val loss: 1.2863606214523315
Epoch 760, training loss: 0.012256418354809284 = 0.0053572580218315125 + 0.001 * 6.899159908294678
Epoch 760, val loss: 1.293750286102295
Epoch 770, training loss: 0.012046685442328453 = 0.005148343276232481 + 0.001 * 6.898341178894043
Epoch 770, val loss: 1.3009320497512817
Epoch 780, training loss: 0.01185053400695324 = 0.004952314309775829 + 0.001 * 6.898219108581543
Epoch 780, val loss: 1.3079171180725098
Epoch 790, training loss: 0.011659283190965652 = 0.00476810010150075 + 0.001 * 6.891183376312256
Epoch 790, val loss: 1.3147938251495361
Epoch 800, training loss: 0.01150819007307291 = 0.004594757687300444 + 0.001 * 6.9134321212768555
Epoch 800, val loss: 1.3214865922927856
Epoch 810, training loss: 0.011318577453494072 = 0.0044315471313893795 + 0.001 * 6.887030601501465
Epoch 810, val loss: 1.3280247449874878
Epoch 820, training loss: 0.011155711486935616 = 0.0042776502668857574 + 0.001 * 6.878061294555664
Epoch 820, val loss: 1.3344179391860962
Epoch 830, training loss: 0.01102855522185564 = 0.004132013767957687 + 0.001 * 6.896541118621826
Epoch 830, val loss: 1.340661883354187
Epoch 840, training loss: 0.010880126617848873 = 0.003994031343609095 + 0.001 * 6.88609504699707
Epoch 840, val loss: 1.346754789352417
Epoch 850, training loss: 0.010739857330918312 = 0.003863450838252902 + 0.001 * 6.876405715942383
Epoch 850, val loss: 1.3527114391326904
Epoch 860, training loss: 0.010606836527585983 = 0.003739850828424096 + 0.001 * 6.86698579788208
Epoch 860, val loss: 1.358534336090088
Epoch 870, training loss: 0.010490082204341888 = 0.003622823627665639 + 0.001 * 6.867258548736572
Epoch 870, val loss: 1.3642264604568481
Epoch 880, training loss: 0.01037808321416378 = 0.003511817427352071 + 0.001 * 6.866265296936035
Epoch 880, val loss: 1.3697946071624756
Epoch 890, training loss: 0.010265103541314602 = 0.0034063959028571844 + 0.001 * 6.858706951141357
Epoch 890, val loss: 1.3753063678741455
Epoch 900, training loss: 0.010161890648305416 = 0.0033061413560062647 + 0.001 * 6.855748653411865
Epoch 900, val loss: 1.3806267976760864
Epoch 910, training loss: 0.01005850825458765 = 0.003210781840607524 + 0.001 * 6.847725868225098
Epoch 910, val loss: 1.3858405351638794
Epoch 920, training loss: 0.00997827760875225 = 0.003120017470791936 + 0.001 * 6.858259677886963
Epoch 920, val loss: 1.3909974098205566
Epoch 930, training loss: 0.00988239049911499 = 0.0030334119219332933 + 0.001 * 6.848978519439697
Epoch 930, val loss: 1.396041989326477
Epoch 940, training loss: 0.009803754277527332 = 0.002950853668153286 + 0.001 * 6.852900505065918
Epoch 940, val loss: 1.400932788848877
Epoch 950, training loss: 0.009711319580674171 = 0.0028721080161631107 + 0.001 * 6.8392109870910645
Epoch 950, val loss: 1.4057588577270508
Epoch 960, training loss: 0.009646611288189888 = 0.0027970015071332455 + 0.001 * 6.849609375
Epoch 960, val loss: 1.4104458093643188
Epoch 970, training loss: 0.009593494236469269 = 0.0027253590524196625 + 0.001 * 6.868134498596191
Epoch 970, val loss: 1.4151053428649902
Epoch 980, training loss: 0.009505330584943295 = 0.002656862372532487 + 0.001 * 6.84846830368042
Epoch 980, val loss: 1.419575810432434
Epoch 990, training loss: 0.00942070223391056 = 0.0025913692079484463 + 0.001 * 6.8293328285217285
Epoch 990, val loss: 1.4239977598190308
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7934
Flip ASR: 0.7600/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9635779857635498 = 1.9552042484283447 + 0.001 * 8.373734474182129
Epoch 0, val loss: 1.9499545097351074
Epoch 10, training loss: 1.9521944522857666 = 1.943820834159851 + 0.001 * 8.373615264892578
Epoch 10, val loss: 1.938913345336914
Epoch 20, training loss: 1.9379950761795044 = 1.9296218156814575 + 0.001 * 8.373296737670898
Epoch 20, val loss: 1.9246222972869873
Epoch 30, training loss: 1.9179338216781616 = 1.9095611572265625 + 0.001 * 8.372660636901855
Epoch 30, val loss: 1.9039225578308105
Epoch 40, training loss: 1.888216495513916 = 1.8798452615737915 + 0.001 * 8.371252059936523
Epoch 40, val loss: 1.873477816581726
Epoch 50, training loss: 1.8467637300491333 = 1.838396430015564 + 0.001 * 8.3673095703125
Epoch 50, val loss: 1.8330782651901245
Epoch 60, training loss: 1.7993704080581665 = 1.7910196781158447 + 0.001 * 8.35073471069336
Epoch 60, val loss: 1.791261911392212
Epoch 70, training loss: 1.7535481452941895 = 1.745321273803711 + 0.001 * 8.22681999206543
Epoch 70, val loss: 1.7539715766906738
Epoch 80, training loss: 1.6969364881515503 = 1.6891233921051025 + 0.001 * 7.813091278076172
Epoch 80, val loss: 1.7061411142349243
Epoch 90, training loss: 1.6219052076339722 = 1.6142648458480835 + 0.001 * 7.6403093338012695
Epoch 90, val loss: 1.6429164409637451
Epoch 100, training loss: 1.5290199518203735 = 1.5215307474136353 + 0.001 * 7.489173889160156
Epoch 100, val loss: 1.5681092739105225
Epoch 110, training loss: 1.4296292066574097 = 1.422329306602478 + 0.001 * 7.2999372482299805
Epoch 110, val loss: 1.4909701347351074
Epoch 120, training loss: 1.3343909978866577 = 1.3271929025650024 + 0.001 * 7.198116302490234
Epoch 120, val loss: 1.421298861503601
Epoch 130, training loss: 1.247129201889038 = 1.2399977445602417 + 0.001 * 7.1315155029296875
Epoch 130, val loss: 1.3598737716674805
Epoch 140, training loss: 1.16606605052948 = 1.1589853763580322 + 0.001 * 7.080700397491455
Epoch 140, val loss: 1.3039555549621582
Epoch 150, training loss: 1.088027834892273 = 1.0809910297393799 + 0.001 * 7.0368332862854
Epoch 150, val loss: 1.248901128768921
Epoch 160, training loss: 1.0118201971054077 = 1.004810094833374 + 0.001 * 7.010139465332031
Epoch 160, val loss: 1.1939836740493774
Epoch 170, training loss: 0.9376446008682251 = 0.930646538734436 + 0.001 * 6.998056888580322
Epoch 170, val loss: 1.139908790588379
Epoch 180, training loss: 0.8657174706459045 = 0.8587234020233154 + 0.001 * 6.994073390960693
Epoch 180, val loss: 1.0874264240264893
Epoch 190, training loss: 0.7951230406761169 = 0.7881299257278442 + 0.001 * 6.993137359619141
Epoch 190, val loss: 1.0362071990966797
Epoch 200, training loss: 0.7242903709411621 = 0.7172973155975342 + 0.001 * 6.993077278137207
Epoch 200, val loss: 0.9853990077972412
Epoch 210, training loss: 0.6521886587142944 = 0.6451952457427979 + 0.001 * 6.993386745452881
Epoch 210, val loss: 0.9346857070922852
Epoch 220, training loss: 0.5787675976753235 = 0.5717737674713135 + 0.001 * 6.993849754333496
Epoch 220, val loss: 0.884650468826294
Epoch 230, training loss: 0.5051124095916748 = 0.49811801314353943 + 0.001 * 6.994396209716797
Epoch 230, val loss: 0.8364703059196472
Epoch 240, training loss: 0.43338099122047424 = 0.4263855218887329 + 0.001 * 6.995457649230957
Epoch 240, val loss: 0.7918804883956909
Epoch 250, training loss: 0.36651819944381714 = 0.3595222532749176 + 0.001 * 6.995938301086426
Epoch 250, val loss: 0.7530688643455505
Epoch 260, training loss: 0.307123601436615 = 0.3001268208026886 + 0.001 * 6.996769905090332
Epoch 260, val loss: 0.7222175002098083
Epoch 270, training loss: 0.25665926933288574 = 0.24966153502464294 + 0.001 * 6.99772834777832
Epoch 270, val loss: 0.7005967497825623
Epoch 280, training loss: 0.2151418775320053 = 0.20814257860183716 + 0.001 * 6.99929666519165
Epoch 280, val loss: 0.6877529621124268
Epoch 290, training loss: 0.18164408206939697 = 0.17464405298233032 + 0.001 * 7.000024318695068
Epoch 290, val loss: 0.6822728514671326
Epoch 300, training loss: 0.15480537712574005 = 0.14780524373054504 + 0.001 * 7.000138282775879
Epoch 300, val loss: 0.6822898983955383
Epoch 310, training loss: 0.13320955634117126 = 0.1262088119983673 + 0.001 * 7.0007429122924805
Epoch 310, val loss: 0.6862525939941406
Epoch 320, training loss: 0.1156994178891182 = 0.10869883000850677 + 0.001 * 7.000584602355957
Epoch 320, val loss: 0.6931954622268677
Epoch 330, training loss: 0.10137107968330383 = 0.09437034279108047 + 0.001 * 7.000738620758057
Epoch 330, val loss: 0.7022948861122131
Epoch 340, training loss: 0.08950325101613998 = 0.08250432461500168 + 0.001 * 6.998925685882568
Epoch 340, val loss: 0.7126467823982239
Epoch 350, training loss: 0.07958165556192398 = 0.07258432358503342 + 0.001 * 6.997333526611328
Epoch 350, val loss: 0.7240392565727234
Epoch 360, training loss: 0.07122427970170975 = 0.06422770023345947 + 0.001 * 6.996580123901367
Epoch 360, val loss: 0.7360130548477173
Epoch 370, training loss: 0.0641351267695427 = 0.05714110657572746 + 0.001 * 6.994019031524658
Epoch 370, val loss: 0.7481768131256104
Epoch 380, training loss: 0.05809152126312256 = 0.05109415948390961 + 0.001 * 6.997360706329346
Epoch 380, val loss: 0.7603726983070374
Epoch 390, training loss: 0.052894555032253265 = 0.04590710625052452 + 0.001 * 6.987450122833252
Epoch 390, val loss: 0.7724510431289673
Epoch 400, training loss: 0.048415422439575195 = 0.04143455624580383 + 0.001 * 6.980867862701416
Epoch 400, val loss: 0.784347414970398
Epoch 410, training loss: 0.044548168778419495 = 0.03755998983979225 + 0.001 * 6.988180160522461
Epoch 410, val loss: 0.7959480285644531
Epoch 420, training loss: 0.041167255491018295 = 0.03418897092342377 + 0.001 * 6.97828483581543
Epoch 420, val loss: 0.807161808013916
Epoch 430, training loss: 0.03821142390370369 = 0.031239911913871765 + 0.001 * 6.971512794494629
Epoch 430, val loss: 0.8181504011154175
Epoch 440, training loss: 0.03560764342546463 = 0.02864711359143257 + 0.001 * 6.960530757904053
Epoch 440, val loss: 0.828825056552887
Epoch 450, training loss: 0.03330583497881889 = 0.026358308270573616 + 0.001 * 6.947527885437012
Epoch 450, val loss: 0.8391173481941223
Epoch 460, training loss: 0.03127838671207428 = 0.024329058825969696 + 0.001 * 6.949326515197754
Epoch 460, val loss: 0.8491523861885071
Epoch 470, training loss: 0.02946176566183567 = 0.022522952407598495 + 0.001 * 6.938812732696533
Epoch 470, val loss: 0.8588831424713135
Epoch 480, training loss: 0.027811914682388306 = 0.020909244194626808 + 0.001 * 6.902670383453369
Epoch 480, val loss: 0.8683297634124756
Epoch 490, training loss: 0.026444239541888237 = 0.019462307915091515 + 0.001 * 6.981930732727051
Epoch 490, val loss: 0.8775073289871216
Epoch 500, training loss: 0.025092260912060738 = 0.01816052943468094 + 0.001 * 6.931731700897217
Epoch 500, val loss: 0.8864220380783081
Epoch 510, training loss: 0.02387213334441185 = 0.01698555238544941 + 0.001 * 6.886579990386963
Epoch 510, val loss: 0.895160973072052
Epoch 520, training loss: 0.022802872583270073 = 0.01592181622982025 + 0.001 * 6.88105583190918
Epoch 520, val loss: 0.9035933613777161
Epoch 530, training loss: 0.02183011919260025 = 0.014956075698137283 + 0.001 * 6.874042987823486
Epoch 530, val loss: 0.9117940664291382
Epoch 540, training loss: 0.020959436893463135 = 0.014076732099056244 + 0.001 * 6.88270378112793
Epoch 540, val loss: 0.9198353886604309
Epoch 550, training loss: 0.020141510292887688 = 0.013274102471768856 + 0.001 * 6.867406845092773
Epoch 550, val loss: 0.9275563955307007
Epoch 560, training loss: 0.01944088377058506 = 0.012539536692202091 + 0.001 * 6.901346683502197
Epoch 560, val loss: 0.9351414442062378
Epoch 570, training loss: 0.018744254484772682 = 0.011865668930113316 + 0.001 * 6.878585338592529
Epoch 570, val loss: 0.9425410628318787
Epoch 580, training loss: 0.018103530630469322 = 0.011246013455092907 + 0.001 * 6.857516288757324
Epoch 580, val loss: 0.9497472643852234
Epoch 590, training loss: 0.017552722245454788 = 0.010675078257918358 + 0.001 * 6.877642631530762
Epoch 590, val loss: 0.9567925333976746
Epoch 600, training loss: 0.01699727214872837 = 0.010147934779524803 + 0.001 * 6.849337100982666
Epoch 600, val loss: 0.9636225700378418
Epoch 610, training loss: 0.0165118258446455 = 0.009660384617745876 + 0.001 * 6.851441383361816
Epoch 610, val loss: 0.9703112840652466
Epoch 620, training loss: 0.016093270853161812 = 0.009208519011735916 + 0.001 * 6.884751319885254
Epoch 620, val loss: 0.976814866065979
Epoch 630, training loss: 0.015628978610038757 = 0.008788970299065113 + 0.001 * 6.840007305145264
Epoch 630, val loss: 0.983189582824707
Epoch 640, training loss: 0.015287808142602444 = 0.008398853242397308 + 0.001 * 6.8889546394348145
Epoch 640, val loss: 0.9893959164619446
Epoch 650, training loss: 0.014852713793516159 = 0.008035370148718357 + 0.001 * 6.817342758178711
Epoch 650, val loss: 0.9954767227172852
Epoch 660, training loss: 0.014541374519467354 = 0.007696191780269146 + 0.001 * 6.8451828956604
Epoch 660, val loss: 1.0014026165008545
Epoch 670, training loss: 0.014219557866454124 = 0.0073792217299342155 + 0.001 * 6.840335845947266
Epoch 670, val loss: 1.0072060823440552
Epoch 680, training loss: 0.013917185366153717 = 0.00708265183493495 + 0.001 * 6.83453369140625
Epoch 680, val loss: 1.0128567218780518
Epoch 690, training loss: 0.01363811269402504 = 0.006804810371249914 + 0.001 * 6.8333024978637695
Epoch 690, val loss: 1.018403172492981
Epoch 700, training loss: 0.013370245695114136 = 0.006544084753841162 + 0.001 * 6.826159954071045
Epoch 700, val loss: 1.0237889289855957
Epoch 710, training loss: 0.013117516413331032 = 0.0062991539016366005 + 0.001 * 6.818362712860107
Epoch 710, val loss: 1.0290600061416626
Epoch 720, training loss: 0.012859763577580452 = 0.006068708375096321 + 0.001 * 6.7910542488098145
Epoch 720, val loss: 1.0342378616333008
Epoch 730, training loss: 0.012657640501856804 = 0.00585170416161418 + 0.001 * 6.805936336517334
Epoch 730, val loss: 1.0392811298370361
Epoch 740, training loss: 0.012452561408281326 = 0.005647100508213043 + 0.001 * 6.805459976196289
Epoch 740, val loss: 1.0442407131195068
Epoch 750, training loss: 0.012247053906321526 = 0.005453959573060274 + 0.001 * 6.793093681335449
Epoch 750, val loss: 1.0490695238113403
Epoch 760, training loss: 0.012075904756784439 = 0.005271469242870808 + 0.001 * 6.804434776306152
Epoch 760, val loss: 1.0538212060928345
Epoch 770, training loss: 0.011899271979928017 = 0.005098855588585138 + 0.001 * 6.800415992736816
Epoch 770, val loss: 1.0584535598754883
Epoch 780, training loss: 0.011758647859096527 = 0.004935408942401409 + 0.001 * 6.823237895965576
Epoch 780, val loss: 1.0629968643188477
Epoch 790, training loss: 0.011575937271118164 = 0.004780470859259367 + 0.001 * 6.79546594619751
Epoch 790, val loss: 1.0674673318862915
Epoch 800, training loss: 0.011460697278380394 = 0.004633559379726648 + 0.001 * 6.8271379470825195
Epoch 800, val loss: 1.0717982053756714
Epoch 810, training loss: 0.011282160878181458 = 0.004494028631597757 + 0.001 * 6.788132190704346
Epoch 810, val loss: 1.0761009454727173
Epoch 820, training loss: 0.011172974482178688 = 0.004361425526440144 + 0.001 * 6.811548709869385
Epoch 820, val loss: 1.080278992652893
Epoch 830, training loss: 0.011021621525287628 = 0.004235359374433756 + 0.001 * 6.786261558532715
Epoch 830, val loss: 1.0843992233276367
Epoch 840, training loss: 0.010905440896749496 = 0.004115354735404253 + 0.001 * 6.790086269378662
Epoch 840, val loss: 1.0884013175964355
Epoch 850, training loss: 0.010772798210382462 = 0.004000996705144644 + 0.001 * 6.771801471710205
Epoch 850, val loss: 1.0923659801483154
Epoch 860, training loss: 0.010660380125045776 = 0.00389201776124537 + 0.001 * 6.768362045288086
Epoch 860, val loss: 1.0962457656860352
Epoch 870, training loss: 0.010565525852143764 = 0.0037880318704992533 + 0.001 * 6.777493476867676
Epoch 870, val loss: 1.1000268459320068
Epoch 880, training loss: 0.010459106415510178 = 0.0036887333262711763 + 0.001 * 6.7703728675842285
Epoch 880, val loss: 1.103760004043579
Epoch 890, training loss: 0.010361322201788425 = 0.00359390233643353 + 0.001 * 6.767419338226318
Epoch 890, val loss: 1.1074210405349731
Epoch 900, training loss: 0.010268542915582657 = 0.0035032092127949 + 0.001 * 6.765333652496338
Epoch 900, val loss: 1.1110018491744995
Epoch 910, training loss: 0.010187612846493721 = 0.0034164448734372854 + 0.001 * 6.771167278289795
Epoch 910, val loss: 1.1145522594451904
Epoch 920, training loss: 0.010083839297294617 = 0.003333366708829999 + 0.001 * 6.750472068786621
Epoch 920, val loss: 1.118019938468933
Epoch 930, training loss: 0.00999313872307539 = 0.0032537817023694515 + 0.001 * 6.739356517791748
Epoch 930, val loss: 1.121412754058838
Epoch 940, training loss: 0.009929828345775604 = 0.003177496138960123 + 0.001 * 6.7523322105407715
Epoch 940, val loss: 1.124751091003418
Epoch 950, training loss: 0.009843629784882069 = 0.0031043768394738436 + 0.001 * 6.73925256729126
Epoch 950, val loss: 1.1280276775360107
Epoch 960, training loss: 0.009793774224817753 = 0.0030341867823153734 + 0.001 * 6.759587287902832
Epoch 960, val loss: 1.1312556266784668
Epoch 970, training loss: 0.009729461744427681 = 0.002966818632557988 + 0.001 * 6.7626423835754395
Epoch 970, val loss: 1.1344163417816162
Epoch 980, training loss: 0.009628129191696644 = 0.0029020747169852257 + 0.001 * 6.7260541915893555
Epoch 980, val loss: 1.1375216245651245
Epoch 990, training loss: 0.009578841738402843 = 0.0028398463036864996 + 0.001 * 6.73899507522583
Epoch 990, val loss: 1.1406000852584839
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.7970
Flip ASR: 0.7556/225 nodes
The final ASR:0.71710, 0.11047, Accuracy:0.80988, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11644])
remove edge: torch.Size([2, 9494])
updated graph: torch.Size([2, 10582])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98155, 0.00522, Accuracy:0.83333, 0.00302
Begin epxeriment: cont_weight: 0.001 epoch:1000 der1:0.5 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.5, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9631036520004272 = 1.9547297954559326 + 0.001 * 8.373903274536133
Epoch 0, val loss: 1.9543300867080688
Epoch 10, training loss: 1.9525188207626343 = 1.9441449642181396 + 0.001 * 8.373882293701172
Epoch 10, val loss: 1.9434705972671509
Epoch 20, training loss: 1.9397069215774536 = 1.9313331842422485 + 0.001 * 8.373762130737305
Epoch 20, val loss: 1.9300464391708374
Epoch 30, training loss: 1.921759009361267 = 1.9133855104446411 + 0.001 * 8.373491287231445
Epoch 30, val loss: 1.9111855030059814
Epoch 40, training loss: 1.8951752185821533 = 1.886802315711975 + 0.001 * 8.372900009155273
Epoch 40, val loss: 1.8835831880569458
Epoch 50, training loss: 1.8578153848648071 = 1.8494439125061035 + 0.001 * 8.37152099609375
Epoch 50, val loss: 1.8466821908950806
Epoch 60, training loss: 1.8148120641708374 = 1.8064442873001099 + 0.001 * 8.367752075195312
Epoch 60, val loss: 1.8091322183609009
Epoch 70, training loss: 1.7771532535552979 = 1.7687982320785522 + 0.001 * 8.354962348937988
Epoch 70, val loss: 1.7820199728012085
Epoch 80, training loss: 1.734702706336975 = 1.7264158725738525 + 0.001 * 8.286870002746582
Epoch 80, val loss: 1.7494078874588013
Epoch 90, training loss: 1.675028681755066 = 1.6670492887496948 + 0.001 * 7.979399681091309
Epoch 90, val loss: 1.7005704641342163
Epoch 100, training loss: 1.5943762063980103 = 1.586533546447754 + 0.001 * 7.842695236206055
Epoch 100, val loss: 1.6343843936920166
Epoch 110, training loss: 1.4948842525482178 = 1.487189769744873 + 0.001 * 7.6944499015808105
Epoch 110, val loss: 1.5558412075042725
Epoch 120, training loss: 1.3864269256591797 = 1.3788498640060425 + 0.001 * 7.5770416259765625
Epoch 120, val loss: 1.4727938175201416
Epoch 130, training loss: 1.2787537574768066 = 1.2712270021438599 + 0.001 * 7.526770114898682
Epoch 130, val loss: 1.3912125825881958
Epoch 140, training loss: 1.1768461465835571 = 1.1694144010543823 + 0.001 * 7.431784152984619
Epoch 140, val loss: 1.315554141998291
Epoch 150, training loss: 1.081984043121338 = 1.0746605396270752 + 0.001 * 7.323538780212402
Epoch 150, val loss: 1.244073748588562
Epoch 160, training loss: 0.9931975603103638 = 0.9859246015548706 + 0.001 * 7.2729387283325195
Epoch 160, val loss: 1.1768602132797241
Epoch 170, training loss: 0.9085094928741455 = 0.9012608528137207 + 0.001 * 7.248613357543945
Epoch 170, val loss: 1.112870454788208
Epoch 180, training loss: 0.8264983892440796 = 0.8192834854125977 + 0.001 * 7.214874744415283
Epoch 180, val loss: 1.0510941743850708
Epoch 190, training loss: 0.7467414140701294 = 0.7395544052124023 + 0.001 * 7.187023639678955
Epoch 190, val loss: 0.9905933737754822
Epoch 200, training loss: 0.670621395111084 = 0.6634498834609985 + 0.001 * 7.17153787612915
Epoch 200, val loss: 0.9329196810722351
Epoch 210, training loss: 0.600878119468689 = 0.5937127470970154 + 0.001 * 7.165371894836426
Epoch 210, val loss: 0.8805610537528992
Epoch 220, training loss: 0.5397350192070007 = 0.532572865486145 + 0.001 * 7.1621317863464355
Epoch 220, val loss: 0.8363869190216064
Epoch 230, training loss: 0.4873274862766266 = 0.48016804456710815 + 0.001 * 7.1594438552856445
Epoch 230, val loss: 0.8013484477996826
Epoch 240, training loss: 0.4417940378189087 = 0.434637188911438 + 0.001 * 7.15683650970459
Epoch 240, val loss: 0.7743191719055176
Epoch 250, training loss: 0.4003864526748657 = 0.3932341933250427 + 0.001 * 7.152255535125732
Epoch 250, val loss: 0.7531160712242126
Epoch 260, training loss: 0.36059460043907166 = 0.3534483313560486 + 0.001 * 7.146279811859131
Epoch 260, val loss: 0.7355543971061707
Epoch 270, training loss: 0.320913702249527 = 0.31377607583999634 + 0.001 * 7.13762092590332
Epoch 270, val loss: 0.7203965187072754
Epoch 280, training loss: 0.2812712490558624 = 0.27414584159851074 + 0.001 * 7.125406265258789
Epoch 280, val loss: 0.7071399688720703
Epoch 290, training loss: 0.24288572371006012 = 0.23577363789081573 + 0.001 * 7.112090587615967
Epoch 290, val loss: 0.6957842707633972
Epoch 300, training loss: 0.2075764387845993 = 0.2004777193069458 + 0.001 * 7.09872579574585
Epoch 300, val loss: 0.6870256662368774
Epoch 310, training loss: 0.1766573041677475 = 0.16956792771816254 + 0.001 * 7.089375972747803
Epoch 310, val loss: 0.6813657283782959
Epoch 320, training loss: 0.15059001743793488 = 0.14350531995296478 + 0.001 * 7.0847039222717285
Epoch 320, val loss: 0.6793186068534851
Epoch 330, training loss: 0.12913356721401215 = 0.1220506802201271 + 0.001 * 7.082882881164551
Epoch 330, val loss: 0.6807591319084167
Epoch 340, training loss: 0.1116788312792778 = 0.10459670424461365 + 0.001 * 7.082128047943115
Epoch 340, val loss: 0.6855071187019348
Epoch 350, training loss: 0.09748795628547668 = 0.09040322154760361 + 0.001 * 7.084734916687012
Epoch 350, val loss: 0.6927488446235657
Epoch 360, training loss: 0.0858726054430008 = 0.0787900984287262 + 0.001 * 7.082507133483887
Epoch 360, val loss: 0.701897382736206
Epoch 370, training loss: 0.07627501338720322 = 0.06919296085834503 + 0.001 * 7.082050323486328
Epoch 370, val loss: 0.7123739719390869
Epoch 380, training loss: 0.06825973093509674 = 0.06117673218250275 + 0.001 * 7.082999229431152
Epoch 380, val loss: 0.723845362663269
Epoch 390, training loss: 0.061490751802921295 = 0.054409027099609375 + 0.001 * 7.081724643707275
Epoch 390, val loss: 0.7360939979553223
Epoch 400, training loss: 0.05572294443845749 = 0.048641011118888855 + 0.001 * 7.081934452056885
Epoch 400, val loss: 0.7487948536872864
Epoch 410, training loss: 0.05076748505234718 = 0.04368671029806137 + 0.001 * 7.080772876739502
Epoch 410, val loss: 0.7617685198783875
Epoch 420, training loss: 0.04648933187127113 = 0.0394006222486496 + 0.001 * 7.088709831237793
Epoch 420, val loss: 0.7750528454780579
Epoch 430, training loss: 0.042751725763082504 = 0.035671088844537735 + 0.001 * 7.080637454986572
Epoch 430, val loss: 0.7884190678596497
Epoch 440, training loss: 0.03948987275362015 = 0.032410357147455215 + 0.001 * 7.079517364501953
Epoch 440, val loss: 0.8018004894256592
Epoch 450, training loss: 0.03662586584687233 = 0.02954854443669319 + 0.001 * 7.0773210525512695
Epoch 450, val loss: 0.8150299191474915
Epoch 460, training loss: 0.034110505133867264 = 0.02703038789331913 + 0.001 * 7.080116271972656
Epoch 460, val loss: 0.8281832933425903
Epoch 470, training loss: 0.031890641897916794 = 0.02480947971343994 + 0.001 * 7.081160545349121
Epoch 470, val loss: 0.8410685658454895
Epoch 480, training loss: 0.02991987206041813 = 0.022844895720481873 + 0.001 * 7.074976444244385
Epoch 480, val loss: 0.8537638783454895
Epoch 490, training loss: 0.028175264596939087 = 0.021102383732795715 + 0.001 * 7.072880744934082
Epoch 490, val loss: 0.8660716414451599
Epoch 500, training loss: 0.026625312864780426 = 0.019552526995539665 + 0.001 * 7.072786331176758
Epoch 500, val loss: 0.8780884742736816
Epoch 510, training loss: 0.025244077667593956 = 0.018169064074754715 + 0.001 * 7.075012683868408
Epoch 510, val loss: 0.8898475766181946
Epoch 520, training loss: 0.02400144189596176 = 0.01692989468574524 + 0.001 * 7.071547031402588
Epoch 520, val loss: 0.9012666940689087
Epoch 530, training loss: 0.02288409322500229 = 0.01581599749624729 + 0.001 * 7.068095684051514
Epoch 530, val loss: 0.9123837351799011
Epoch 540, training loss: 0.021894406527280807 = 0.014811365865170956 + 0.001 * 7.083041191101074
Epoch 540, val loss: 0.9231173992156982
Epoch 550, training loss: 0.02097165584564209 = 0.013902177102863789 + 0.001 * 7.069477558135986
Epoch 550, val loss: 0.933586061000824
Epoch 560, training loss: 0.020140694454312325 = 0.013076726347208023 + 0.001 * 7.063967227935791
Epoch 560, val loss: 0.9437904357910156
Epoch 570, training loss: 0.019386863335967064 = 0.012325395829975605 + 0.001 * 7.06146764755249
Epoch 570, val loss: 0.9536585807800293
Epoch 580, training loss: 0.018704473972320557 = 0.011639464646577835 + 0.001 * 7.065009117126465
Epoch 580, val loss: 0.9632785320281982
Epoch 590, training loss: 0.018071049824357033 = 0.011011442169547081 + 0.001 * 7.059607982635498
Epoch 590, val loss: 0.9726378321647644
Epoch 600, training loss: 0.01752125844359398 = 0.010434882715344429 + 0.001 * 7.086376190185547
Epoch 600, val loss: 0.981726884841919
Epoch 610, training loss: 0.016961853951215744 = 0.009904323145747185 + 0.001 * 7.057530403137207
Epoch 610, val loss: 0.9906247854232788
Epoch 620, training loss: 0.016467563807964325 = 0.009414969012141228 + 0.001 * 7.05259370803833
Epoch 620, val loss: 0.9992879033088684
Epoch 630, training loss: 0.016019972041249275 = 0.008962735533714294 + 0.001 * 7.0572357177734375
Epoch 630, val loss: 1.007709264755249
Epoch 640, training loss: 0.015591265633702278 = 0.008543792180716991 + 0.001 * 7.047472953796387
Epoch 640, val loss: 1.0159308910369873
Epoch 650, training loss: 0.015208687633275986 = 0.008155045099556446 + 0.001 * 7.053642749786377
Epoch 650, val loss: 1.023942232131958
Epoch 660, training loss: 0.014845379628241062 = 0.0077937026508152485 + 0.001 * 7.0516767501831055
Epoch 660, val loss: 1.0317912101745605
Epoch 670, training loss: 0.01450149156153202 = 0.007457364816218615 + 0.001 * 7.044126510620117
Epoch 670, val loss: 1.039402723312378
Epoch 680, training loss: 0.014206020161509514 = 0.0071436557918787 + 0.001 * 7.06236457824707
Epoch 680, val loss: 1.0468504428863525
Epoch 690, training loss: 0.013888683170080185 = 0.006850585341453552 + 0.001 * 7.038096904754639
Epoch 690, val loss: 1.0541133880615234
Epoch 700, training loss: 0.013620859012007713 = 0.006576433777809143 + 0.001 * 7.0444254875183105
Epoch 700, val loss: 1.0612157583236694
Epoch 710, training loss: 0.01336298044770956 = 0.006319598760455847 + 0.001 * 7.043381214141846
Epoch 710, val loss: 1.0681357383728027
Epoch 720, training loss: 0.013110334053635597 = 0.006078613456338644 + 0.001 * 7.0317206382751465
Epoch 720, val loss: 1.0749166011810303
Epoch 730, training loss: 0.012889469042420387 = 0.005852265749126673 + 0.001 * 7.037202835083008
Epoch 730, val loss: 1.0815191268920898
Epoch 740, training loss: 0.012673904187977314 = 0.005639318376779556 + 0.001 * 7.034585475921631
Epoch 740, val loss: 1.0879865884780884
Epoch 750, training loss: 0.012467551976442337 = 0.005438771564513445 + 0.001 * 7.02877950668335
Epoch 750, val loss: 1.0943036079406738
Epoch 760, training loss: 0.01228401530534029 = 0.005249656271189451 + 0.001 * 7.034358501434326
Epoch 760, val loss: 1.1005090475082397
Epoch 770, training loss: 0.012098504230380058 = 0.005071175284683704 + 0.001 * 7.027328014373779
Epoch 770, val loss: 1.1065444946289062
Epoch 780, training loss: 0.011921826750040054 = 0.004902509972453117 + 0.001 * 7.019316673278809
Epoch 780, val loss: 1.1124694347381592
Epoch 790, training loss: 0.011774938553571701 = 0.004742954391986132 + 0.001 * 7.031984329223633
Epoch 790, val loss: 1.1182594299316406
Epoch 800, training loss: 0.011609502136707306 = 0.004591812379658222 + 0.001 * 7.017689228057861
Epoch 800, val loss: 1.1239330768585205
Epoch 810, training loss: 0.011468368582427502 = 0.004448563326150179 + 0.001 * 7.019804954528809
Epoch 810, val loss: 1.129503846168518
Epoch 820, training loss: 0.011325332336127758 = 0.004312675446271896 + 0.001 * 7.0126566886901855
Epoch 820, val loss: 1.134948968887329
Epoch 830, training loss: 0.011184405535459518 = 0.004183649085462093 + 0.001 * 7.00075626373291
Epoch 830, val loss: 1.1403071880340576
Epoch 840, training loss: 0.011069698259234428 = 0.004060999024659395 + 0.001 * 7.008699417114258
Epoch 840, val loss: 1.1455351114273071
Epoch 850, training loss: 0.010939221829175949 = 0.003944351337850094 + 0.001 * 6.994870662689209
Epoch 850, val loss: 1.150657296180725
Epoch 860, training loss: 0.010844934731721878 = 0.0038333237171173096 + 0.001 * 7.011610507965088
Epoch 860, val loss: 1.155713677406311
Epoch 870, training loss: 0.010728971101343632 = 0.0037274963688105345 + 0.001 * 7.001474857330322
Epoch 870, val loss: 1.160630464553833
Epoch 880, training loss: 0.010628804564476013 = 0.0036266392562538385 + 0.001 * 7.0021653175354
Epoch 880, val loss: 1.165465235710144
Epoch 890, training loss: 0.010515906848013401 = 0.003530419198796153 + 0.001 * 6.985487461090088
Epoch 890, val loss: 1.1702300310134888
Epoch 900, training loss: 0.010465020313858986 = 0.0034385581966489553 + 0.001 * 7.026461601257324
Epoch 900, val loss: 1.1748565435409546
Epoch 910, training loss: 0.010325456038117409 = 0.003350775921717286 + 0.001 * 6.974679946899414
Epoch 910, val loss: 1.1794402599334717
Epoch 920, training loss: 0.01025270763784647 = 0.0032668656203895807 + 0.001 * 6.985841751098633
Epoch 920, val loss: 1.1839158535003662
Epoch 930, training loss: 0.010166225954890251 = 0.0031865574419498444 + 0.001 * 6.979667663574219
Epoch 930, val loss: 1.1883156299591064
Epoch 940, training loss: 0.010110082104802132 = 0.003109712852165103 + 0.001 * 7.000368595123291
Epoch 940, val loss: 1.1926429271697998
Epoch 950, training loss: 0.010020002722740173 = 0.003036047797650099 + 0.001 * 6.983953952789307
Epoch 950, val loss: 1.1968610286712646
Epoch 960, training loss: 0.009939268231391907 = 0.0029654463287442923 + 0.001 * 6.97382116317749
Epoch 960, val loss: 1.2010458707809448
Epoch 970, training loss: 0.00987285003066063 = 0.002897757338359952 + 0.001 * 6.975091934204102
Epoch 970, val loss: 1.2051341533660889
Epoch 980, training loss: 0.009782294742763042 = 0.002832789672538638 + 0.001 * 6.949504375457764
Epoch 980, val loss: 1.2091718912124634
Epoch 990, training loss: 0.00976279191672802 = 0.002770427381619811 + 0.001 * 6.992364406585693
Epoch 990, val loss: 1.2131118774414062
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.6568
Flip ASR: 0.5867/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.944832444190979 = 1.9364585876464844 + 0.001 * 8.373910903930664
Epoch 0, val loss: 1.9276880025863647
Epoch 10, training loss: 1.935412883758545 = 1.9270390272140503 + 0.001 * 8.373863220214844
Epoch 10, val loss: 1.9185571670532227
Epoch 20, training loss: 1.92386794090271 = 1.9154942035675049 + 0.001 * 8.373702049255371
Epoch 20, val loss: 1.9070945978164673
Epoch 30, training loss: 1.907701849937439 = 1.8993284702301025 + 0.001 * 8.373366355895996
Epoch 30, val loss: 1.8908765316009521
Epoch 40, training loss: 1.8840688467025757 = 1.8756961822509766 + 0.001 * 8.372673988342285
Epoch 40, val loss: 1.8673254251480103
Epoch 50, training loss: 1.850724697113037 = 1.8423535823822021 + 0.001 * 8.371099472045898
Epoch 50, val loss: 1.8353413343429565
Epoch 60, training loss: 1.8098301887512207 = 1.8014633655548096 + 0.001 * 8.366778373718262
Epoch 60, val loss: 1.7990845441818237
Epoch 70, training loss: 1.7678310871124268 = 1.7594799995422363 + 0.001 * 8.351106643676758
Epoch 70, val loss: 1.7640342712402344
Epoch 80, training loss: 1.716976523399353 = 1.7087173461914062 + 0.001 * 8.259129524230957
Epoch 80, val loss: 1.7187895774841309
Epoch 90, training loss: 1.6460254192352295 = 1.6381449699401855 + 0.001 * 7.8804240226745605
Epoch 90, val loss: 1.6561343669891357
Epoch 100, training loss: 1.55393385887146 = 1.5461307764053345 + 0.001 * 7.803075790405273
Epoch 100, val loss: 1.5781527757644653
Epoch 110, training loss: 1.4479013681411743 = 1.4401805400848389 + 0.001 * 7.720818996429443
Epoch 110, val loss: 1.4920871257781982
Epoch 120, training loss: 1.3406938314437866 = 1.3331185579299927 + 0.001 * 7.575267791748047
Epoch 120, val loss: 1.4079985618591309
Epoch 130, training loss: 1.238092064857483 = 1.2306426763534546 + 0.001 * 7.449399948120117
Epoch 130, val loss: 1.3301926851272583
Epoch 140, training loss: 1.1399784088134766 = 1.1325730085372925 + 0.001 * 7.405354022979736
Epoch 140, val loss: 1.2569670677185059
Epoch 150, training loss: 1.0442242622375488 = 1.0368914604187012 + 0.001 * 7.332842826843262
Epoch 150, val loss: 1.185513973236084
Epoch 160, training loss: 0.950417160987854 = 0.9431312084197998 + 0.001 * 7.2859787940979
Epoch 160, val loss: 1.116585612297058
Epoch 170, training loss: 0.8602057099342346 = 0.8529308438301086 + 0.001 * 7.274866104125977
Epoch 170, val loss: 1.0516915321350098
Epoch 180, training loss: 0.7757795453071594 = 0.7685137987136841 + 0.001 * 7.265769958496094
Epoch 180, val loss: 0.9924243092536926
Epoch 190, training loss: 0.6990541815757751 = 0.6917946934700012 + 0.001 * 7.259495735168457
Epoch 190, val loss: 0.9402134418487549
Epoch 200, training loss: 0.6313292384147644 = 0.6240745186805725 + 0.001 * 7.254701614379883
Epoch 200, val loss: 0.8962253928184509
Epoch 210, training loss: 0.572953462600708 = 0.5657033324241638 + 0.001 * 7.250120162963867
Epoch 210, val loss: 0.8605061769485474
Epoch 220, training loss: 0.5230699777603149 = 0.5158237218856812 + 0.001 * 7.246284484863281
Epoch 220, val loss: 0.8325335383415222
Epoch 230, training loss: 0.47994327545166016 = 0.4727005660533905 + 0.001 * 7.242697238922119
Epoch 230, val loss: 0.8117948174476624
Epoch 240, training loss: 0.4417358934879303 = 0.43449652194976807 + 0.001 * 7.239377498626709
Epoch 240, val loss: 0.7971012592315674
Epoch 250, training loss: 0.40687477588653564 = 0.39963898062705994 + 0.001 * 7.235804080963135
Epoch 250, val loss: 0.7864787578582764
Epoch 260, training loss: 0.37412673234939575 = 0.3668965995311737 + 0.001 * 7.230147361755371
Epoch 260, val loss: 0.7786315679550171
Epoch 270, training loss: 0.34260568022727966 = 0.3353857398033142 + 0.001 * 7.219930648803711
Epoch 270, val loss: 0.7730066776275635
Epoch 280, training loss: 0.3117592930793762 = 0.30454856157302856 + 0.001 * 7.210744380950928
Epoch 280, val loss: 0.7690156102180481
Epoch 290, training loss: 0.2814546823501587 = 0.27425870299339294 + 0.001 * 7.195988178253174
Epoch 290, val loss: 0.7665512561798096
Epoch 300, training loss: 0.2518974244594574 = 0.24471288919448853 + 0.001 * 7.1845293045043945
Epoch 300, val loss: 0.7658507227897644
Epoch 310, training loss: 0.2236788123846054 = 0.2164880931377411 + 0.001 * 7.1907196044921875
Epoch 310, val loss: 0.7676100134849548
Epoch 320, training loss: 0.19742275774478912 = 0.1902417540550232 + 0.001 * 7.181003093719482
Epoch 320, val loss: 0.7720651030540466
Epoch 330, training loss: 0.17388883233070374 = 0.16671018302440643 + 0.001 * 7.178652763366699
Epoch 330, val loss: 0.7794809937477112
Epoch 340, training loss: 0.1535833477973938 = 0.14640724658966064 + 0.001 * 7.176093578338623
Epoch 340, val loss: 0.7899633049964905
Epoch 350, training loss: 0.13643836975097656 = 0.12926369905471802 + 0.001 * 7.1746745109558105
Epoch 350, val loss: 0.8021689653396606
Epoch 360, training loss: 0.12149406969547272 = 0.11431685835123062 + 0.001 * 7.177209854125977
Epoch 360, val loss: 0.815951406955719
Epoch 370, training loss: 0.10843927413225174 = 0.10126961022615433 + 0.001 * 7.169661045074463
Epoch 370, val loss: 0.8316982984542847
Epoch 380, training loss: 0.09703411906957626 = 0.08986581116914749 + 0.001 * 7.168307304382324
Epoch 380, val loss: 0.8479542136192322
Epoch 390, training loss: 0.087021604180336 = 0.07985980808734894 + 0.001 * 7.161798000335693
Epoch 390, val loss: 0.8647143840789795
Epoch 400, training loss: 0.07824680954217911 = 0.07108671963214874 + 0.001 * 7.160086631774902
Epoch 400, val loss: 0.8821547627449036
Epoch 410, training loss: 0.07058113068342209 = 0.06342765688896179 + 0.001 * 7.153472900390625
Epoch 410, val loss: 0.8999215960502625
Epoch 420, training loss: 0.06389935314655304 = 0.056755196303129196 + 0.001 * 7.144156455993652
Epoch 420, val loss: 0.9178681969642639
Epoch 430, training loss: 0.058082904666662216 = 0.05094227194786072 + 0.001 * 7.140631675720215
Epoch 430, val loss: 0.9357450604438782
Epoch 440, training loss: 0.0529983788728714 = 0.04586857557296753 + 0.001 * 7.129801273345947
Epoch 440, val loss: 0.953461229801178
Epoch 450, training loss: 0.04855556786060333 = 0.04141766205430031 + 0.001 * 7.137904167175293
Epoch 450, val loss: 0.9708699584007263
Epoch 460, training loss: 0.04461737722158432 = 0.03750762715935707 + 0.001 * 7.109751224517822
Epoch 460, val loss: 0.9877590537071228
Epoch 470, training loss: 0.04118495061993599 = 0.03407896310091019 + 0.001 * 7.10598611831665
Epoch 470, val loss: 1.00423264503479
Epoch 480, training loss: 0.03817296773195267 = 0.031067058444023132 + 0.001 * 7.105907440185547
Epoch 480, val loss: 1.0201934576034546
Epoch 490, training loss: 0.035503458231687546 = 0.028412027284502983 + 0.001 * 7.091430187225342
Epoch 490, val loss: 1.035642147064209
Epoch 500, training loss: 0.03314953297376633 = 0.02606385014951229 + 0.001 * 7.085681915283203
Epoch 500, val loss: 1.050591230392456
Epoch 510, training loss: 0.031074635684490204 = 0.023979922756552696 + 0.001 * 7.09471321105957
Epoch 510, val loss: 1.0649970769882202
Epoch 520, training loss: 0.02919721230864525 = 0.02212444134056568 + 0.001 * 7.072771072387695
Epoch 520, val loss: 1.0788758993148804
Epoch 530, training loss: 0.02753109484910965 = 0.020465455949306488 + 0.001 * 7.065638542175293
Epoch 530, val loss: 1.0922857522964478
Epoch 540, training loss: 0.026044074445962906 = 0.018977615982294083 + 0.001 * 7.066457271575928
Epoch 540, val loss: 1.1052663326263428
Epoch 550, training loss: 0.024714039638638496 = 0.01764019764959812 + 0.001 * 7.0738420486450195
Epoch 550, val loss: 1.1178381443023682
Epoch 560, training loss: 0.023490510880947113 = 0.01643635891377926 + 0.001 * 7.0541510581970215
Epoch 560, val loss: 1.129960060119629
Epoch 570, training loss: 0.022405995056033134 = 0.015350088477134705 + 0.001 * 7.055906772613525
Epoch 570, val loss: 1.141709566116333
Epoch 580, training loss: 0.021417057141661644 = 0.014367249794304371 + 0.001 * 7.049807548522949
Epoch 580, val loss: 1.1531182527542114
Epoch 590, training loss: 0.02052428387105465 = 0.01347604114562273 + 0.001 * 7.048242092132568
Epoch 590, val loss: 1.1641725301742554
Epoch 600, training loss: 0.019714437425136566 = 0.012665994465351105 + 0.001 * 7.04844331741333
Epoch 600, val loss: 1.1748955249786377
Epoch 610, training loss: 0.01896258443593979 = 0.01192774809896946 + 0.001 * 7.034836769104004
Epoch 610, val loss: 1.1852558851242065
Epoch 620, training loss: 0.018307548016309738 = 0.01125315111130476 + 0.001 * 7.054396152496338
Epoch 620, val loss: 1.1953587532043457
Epoch 630, training loss: 0.0176744032651186 = 0.010635045357048512 + 0.001 * 7.039358139038086
Epoch 630, val loss: 1.2051395177841187
Epoch 640, training loss: 0.01712680235505104 = 0.010067179799079895 + 0.001 * 7.059622764587402
Epoch 640, val loss: 1.2146410942077637
Epoch 650, training loss: 0.016583314165472984 = 0.009544547647237778 + 0.001 * 7.038765907287598
Epoch 650, val loss: 1.2238205671310425
Epoch 660, training loss: 0.016099441796541214 = 0.009062693454325199 + 0.001 * 7.036748886108398
Epoch 660, val loss: 1.2327324151992798
Epoch 670, training loss: 0.015637394040822983 = 0.008617731742560863 + 0.001 * 7.019662857055664
Epoch 670, val loss: 1.2413541078567505
Epoch 680, training loss: 0.015262429602444172 = 0.00820638332515955 + 0.001 * 7.056046009063721
Epoch 680, val loss: 1.249690055847168
Epoch 690, training loss: 0.014832727611064911 = 0.007824921980500221 + 0.001 * 7.007804870605469
Epoch 690, val loss: 1.2578108310699463
Epoch 700, training loss: 0.014522578567266464 = 0.00747067341580987 + 0.001 * 7.051905155181885
Epoch 700, val loss: 1.265681505203247
Epoch 710, training loss: 0.014129117131233215 = 0.007141043432056904 + 0.001 * 6.988072872161865
Epoch 710, val loss: 1.2733017206192017
Epoch 720, training loss: 0.013838629238307476 = 0.006833862047642469 + 0.001 * 7.004766941070557
Epoch 720, val loss: 1.2807233333587646
Epoch 730, training loss: 0.013583984225988388 = 0.0065471101552248 + 0.001 * 7.036874294281006
Epoch 730, val loss: 1.2879289388656616
Epoch 740, training loss: 0.013273902237415314 = 0.006279079709202051 + 0.001 * 6.9948225021362305
Epoch 740, val loss: 1.2949482202529907
Epoch 750, training loss: 0.01301666907966137 = 0.006028337869793177 + 0.001 * 6.988331317901611
Epoch 750, val loss: 1.301720142364502
Epoch 760, training loss: 0.012754587456583977 = 0.005793305113911629 + 0.001 * 6.961281776428223
Epoch 760, val loss: 1.3083537817001343
Epoch 770, training loss: 0.012531719170510769 = 0.005572685040533543 + 0.001 * 6.959033966064453
Epoch 770, val loss: 1.314757227897644
Epoch 780, training loss: 0.012329721823334694 = 0.005365350749343634 + 0.001 * 6.964370250701904
Epoch 780, val loss: 1.3210148811340332
Epoch 790, training loss: 0.012134918943047523 = 0.005169993732124567 + 0.001 * 6.9649248123168945
Epoch 790, val loss: 1.3271340131759644
Epoch 800, training loss: 0.011955367401242256 = 0.004985876847058535 + 0.001 * 6.9694905281066895
Epoch 800, val loss: 1.3330929279327393
Epoch 810, training loss: 0.011754270642995834 = 0.004812220111489296 + 0.001 * 6.942049980163574
Epoch 810, val loss: 1.3388572931289673
Epoch 820, training loss: 0.011594841256737709 = 0.004648328758776188 + 0.001 * 6.946511745452881
Epoch 820, val loss: 1.3444849252700806
Epoch 830, training loss: 0.011439962312579155 = 0.004493451677262783 + 0.001 * 6.946510314941406
Epoch 830, val loss: 1.349977731704712
Epoch 840, training loss: 0.011296307668089867 = 0.004347022157162428 + 0.001 * 6.94928503036499
Epoch 840, val loss: 1.3553471565246582
Epoch 850, training loss: 0.011180508881807327 = 0.004208612721413374 + 0.001 * 6.971895217895508
Epoch 850, val loss: 1.3605366945266724
Epoch 860, training loss: 0.011038476601243019 = 0.004077444784343243 + 0.001 * 6.961031913757324
Epoch 860, val loss: 1.3656448125839233
Epoch 870, training loss: 0.010876785963773727 = 0.003952965140342712 + 0.001 * 6.9238200187683105
Epoch 870, val loss: 1.3705859184265137
Epoch 880, training loss: 0.010756051167845726 = 0.0038347658701241016 + 0.001 * 6.921285152435303
Epoch 880, val loss: 1.375429630279541
Epoch 890, training loss: 0.010639904998242855 = 0.0037224034313112497 + 0.001 * 6.917500972747803
Epoch 890, val loss: 1.3801912069320679
Epoch 900, training loss: 0.01054192241281271 = 0.0036154156550765038 + 0.001 * 6.926506519317627
Epoch 900, val loss: 1.3848034143447876
Epoch 910, training loss: 0.010468089953064919 = 0.0035134328063577414 + 0.001 * 6.954656600952148
Epoch 910, val loss: 1.3892979621887207
Epoch 920, training loss: 0.010331278666853905 = 0.0034161910880357027 + 0.001 * 6.9150872230529785
Epoch 920, val loss: 1.3937175273895264
Epoch 930, training loss: 0.010252528823912144 = 0.0033230811823159456 + 0.001 * 6.929447650909424
Epoch 930, val loss: 1.3980109691619873
Epoch 940, training loss: 0.010141722857952118 = 0.0032340576872229576 + 0.001 * 6.907664775848389
Epoch 940, val loss: 1.402267336845398
Epoch 950, training loss: 0.010056726634502411 = 0.003148621879518032 + 0.001 * 6.908103942871094
Epoch 950, val loss: 1.4064019918441772
Epoch 960, training loss: 0.00994416419416666 = 0.003066601464524865 + 0.001 * 6.877562522888184
Epoch 960, val loss: 1.4105031490325928
Epoch 970, training loss: 0.009899179451167583 = 0.0029877128545194864 + 0.001 * 6.911466598510742
Epoch 970, val loss: 1.414495825767517
Epoch 980, training loss: 0.009824126958847046 = 0.0029115877114236355 + 0.001 * 6.912539005279541
Epoch 980, val loss: 1.4184746742248535
Epoch 990, training loss: 0.009725046344101429 = 0.0028383140452206135 + 0.001 * 6.88673210144043
Epoch 990, val loss: 1.4224170446395874
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7269
Flip ASR: 0.6800/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9483641386032104 = 1.9399901628494263 + 0.001 * 8.373919486999512
Epoch 0, val loss: 1.9360231161117554
Epoch 10, training loss: 1.9387085437774658 = 1.9303346872329712 + 0.001 * 8.373862266540527
Epoch 10, val loss: 1.927164077758789
Epoch 20, training loss: 1.9266611337661743 = 1.9182873964309692 + 0.001 * 8.373710632324219
Epoch 20, val loss: 1.9159016609191895
Epoch 30, training loss: 1.909738302230835 = 1.9013649225234985 + 0.001 * 8.373396873474121
Epoch 30, val loss: 1.8998686075210571
Epoch 40, training loss: 1.884548544883728 = 1.8761757612228394 + 0.001 * 8.3727388381958
Epoch 40, val loss: 1.8763177394866943
Epoch 50, training loss: 1.8485610485076904 = 1.8401899337768555 + 0.001 * 8.371121406555176
Epoch 50, val loss: 1.8443068265914917
Epoch 60, training loss: 1.8058277368545532 = 1.7974621057510376 + 0.001 * 8.365647315979004
Epoch 60, val loss: 1.8100967407226562
Epoch 70, training loss: 1.7644071578979492 = 1.7560675144195557 + 0.001 * 8.339661598205566
Epoch 70, val loss: 1.7776986360549927
Epoch 80, training loss: 1.7113622426986694 = 1.7032119035720825 + 0.001 * 8.150395393371582
Epoch 80, val loss: 1.7300620079040527
Epoch 90, training loss: 1.63847017288208 = 1.6305971145629883 + 0.001 * 7.873105049133301
Epoch 90, val loss: 1.6672462224960327
Epoch 100, training loss: 1.5459394454956055 = 1.5381317138671875 + 0.001 * 7.807765483856201
Epoch 100, val loss: 1.5927225351333618
Epoch 110, training loss: 1.445793867111206 = 1.4380594491958618 + 0.001 * 7.734462261199951
Epoch 110, val loss: 1.5135074853897095
Epoch 120, training loss: 1.3481166362762451 = 1.340533971786499 + 0.001 * 7.582636833190918
Epoch 120, val loss: 1.4365898370742798
Epoch 130, training loss: 1.2542483806610107 = 1.2467572689056396 + 0.001 * 7.491126537322998
Epoch 130, val loss: 1.3620820045471191
Epoch 140, training loss: 1.164263367652893 = 1.1568270921707153 + 0.001 * 7.436324596405029
Epoch 140, val loss: 1.2906172275543213
Epoch 150, training loss: 1.079674243927002 = 1.0723544359207153 + 0.001 * 7.319814205169678
Epoch 150, val loss: 1.2237333059310913
Epoch 160, training loss: 1.0028551816940308 = 0.9955940842628479 + 0.001 * 7.26115083694458
Epoch 160, val loss: 1.1636582612991333
Epoch 170, training loss: 0.9337557554244995 = 0.9264998435974121 + 0.001 * 7.255917549133301
Epoch 170, val loss: 1.1109684705734253
Epoch 180, training loss: 0.8700079321861267 = 0.8627673387527466 + 0.001 * 7.240573883056641
Epoch 180, val loss: 1.0640629529953003
Epoch 190, training loss: 0.8087440729141235 = 0.8015208840370178 + 0.001 * 7.223170280456543
Epoch 190, val loss: 1.0203444957733154
Epoch 200, training loss: 0.7477570176124573 = 0.7405561804771423 + 0.001 * 7.2008209228515625
Epoch 200, val loss: 0.9782389402389526
Epoch 210, training loss: 0.6856848001480103 = 0.6785182356834412 + 0.001 * 7.166587829589844
Epoch 210, val loss: 0.9366416931152344
Epoch 220, training loss: 0.6219180226325989 = 0.6147871613502502 + 0.001 * 7.130858898162842
Epoch 220, val loss: 0.8952236771583557
Epoch 230, training loss: 0.5567833185195923 = 0.5496812462806702 + 0.001 * 7.102059841156006
Epoch 230, val loss: 0.8548213243484497
Epoch 240, training loss: 0.49232685565948486 = 0.4852336347103119 + 0.001 * 7.093217372894287
Epoch 240, val loss: 0.8170420527458191
Epoch 250, training loss: 0.43170222640037537 = 0.4246092438697815 + 0.001 * 7.092976093292236
Epoch 250, val loss: 0.7837815284729004
Epoch 260, training loss: 0.3775861859321594 = 0.3704923987388611 + 0.001 * 7.0937886238098145
Epoch 260, val loss: 0.7566186189651489
Epoch 270, training loss: 0.330888032913208 = 0.32379284501075745 + 0.001 * 7.095195293426514
Epoch 270, val loss: 0.7360383868217468
Epoch 280, training loss: 0.29091909527778625 = 0.28382253646850586 + 0.001 * 7.0965728759765625
Epoch 280, val loss: 0.7213881611824036
Epoch 290, training loss: 0.2564031481742859 = 0.2493053376674652 + 0.001 * 7.097799301147461
Epoch 290, val loss: 0.7116098999977112
Epoch 300, training loss: 0.226266011595726 = 0.21916715800762177 + 0.001 * 7.098846435546875
Epoch 300, val loss: 0.705626368522644
Epoch 310, training loss: 0.19979825615882874 = 0.19269852340221405 + 0.001 * 7.0997314453125
Epoch 310, val loss: 0.7026599049568176
Epoch 320, training loss: 0.17653796076774597 = 0.1694374680519104 + 0.001 * 7.100488185882568
Epoch 320, val loss: 0.7018672227859497
Epoch 330, training loss: 0.15615783631801605 = 0.14905667304992676 + 0.001 * 7.101156711578369
Epoch 330, val loss: 0.7027098536491394
Epoch 340, training loss: 0.13835862278938293 = 0.13125687837600708 + 0.001 * 7.101744651794434
Epoch 340, val loss: 0.704869270324707
Epoch 350, training loss: 0.12284555286169052 = 0.11574330180883408 + 0.001 * 7.102251052856445
Epoch 350, val loss: 0.7081106901168823
Epoch 360, training loss: 0.10935042798519135 = 0.10224536806344986 + 0.001 * 7.105062484741211
Epoch 360, val loss: 0.7122731804847717
Epoch 370, training loss: 0.09762240201234818 = 0.09051836282014847 + 0.001 * 7.104037761688232
Epoch 370, val loss: 0.7172484397888184
Epoch 380, training loss: 0.0874493420124054 = 0.08034549653530121 + 0.001 * 7.103848934173584
Epoch 380, val loss: 0.7230075597763062
Epoch 390, training loss: 0.0786353349685669 = 0.07153140008449554 + 0.001 * 7.103935718536377
Epoch 390, val loss: 0.729506254196167
Epoch 400, training loss: 0.07099847495555878 = 0.0638948529958725 + 0.001 * 7.10361909866333
Epoch 400, val loss: 0.7366780042648315
Epoch 410, training loss: 0.06437280029058456 = 0.05726988986134529 + 0.001 * 7.102912425994873
Epoch 410, val loss: 0.7444412708282471
Epoch 420, training loss: 0.058621350675821304 = 0.05151037126779556 + 0.001 * 7.110977649688721
Epoch 420, val loss: 0.7527062296867371
Epoch 430, training loss: 0.053594157099723816 = 0.04649033397436142 + 0.001 * 7.103824615478516
Epoch 430, val loss: 0.7613747119903564
Epoch 440, training loss: 0.049204204231500626 = 0.042103853076696396 + 0.001 * 7.100351810455322
Epoch 440, val loss: 0.7703021168708801
Epoch 450, training loss: 0.045356664806604385 = 0.038258641958236694 + 0.001 * 7.098023891448975
Epoch 450, val loss: 0.7793868184089661
Epoch 460, training loss: 0.04197598993778229 = 0.034877605736255646 + 0.001 * 7.098384380340576
Epoch 460, val loss: 0.7885302305221558
Epoch 470, training loss: 0.0389893613755703 = 0.03189486637711525 + 0.001 * 7.0944952964782715
Epoch 470, val loss: 0.7977381944656372
Epoch 480, training loss: 0.0363454632461071 = 0.02925514243543148 + 0.001 * 7.090322017669678
Epoch 480, val loss: 0.8068822622299194
Epoch 490, training loss: 0.033993449062108994 = 0.026911281049251556 + 0.001 * 7.082167148590088
Epoch 490, val loss: 0.8159717917442322
Epoch 500, training loss: 0.03192092105746269 = 0.024823328480124474 + 0.001 * 7.097591400146484
Epoch 500, val loss: 0.8249567151069641
Epoch 510, training loss: 0.030034828931093216 = 0.02295658178627491 + 0.001 * 7.078247547149658
Epoch 510, val loss: 0.8338268995285034
Epoch 520, training loss: 0.02835339494049549 = 0.021281637251377106 + 0.001 * 7.0717573165893555
Epoch 520, val loss: 0.8425620794296265
Epoch 530, training loss: 0.026850471273064613 = 0.019769707694649696 + 0.001 * 7.080763816833496
Epoch 530, val loss: 0.8512173295021057
Epoch 540, training loss: 0.025479653850197792 = 0.0184004008769989 + 0.001 * 7.07925271987915
Epoch 540, val loss: 0.8596463203430176
Epoch 550, training loss: 0.024207722395658493 = 0.017156874760985374 + 0.001 * 7.05084753036499
Epoch 550, val loss: 0.8678820133209229
Epoch 560, training loss: 0.023072469979524612 = 0.016025343909859657 + 0.001 * 7.047125339508057
Epoch 560, val loss: 0.8760082125663757
Epoch 570, training loss: 0.022027745842933655 = 0.014994030818343163 + 0.001 * 7.033715724945068
Epoch 570, val loss: 0.8839791417121887
Epoch 580, training loss: 0.021140683442354202 = 0.014052504673600197 + 0.001 * 7.0881781578063965
Epoch 580, val loss: 0.8918020725250244
Epoch 590, training loss: 0.0202176570892334 = 0.013192017562687397 + 0.001 * 7.02564001083374
Epoch 590, val loss: 0.8995016813278198
Epoch 600, training loss: 0.019427523016929626 = 0.012404426001012325 + 0.001 * 7.023095607757568
Epoch 600, val loss: 0.9070258140563965
Epoch 610, training loss: 0.018704606220126152 = 0.011682692915201187 + 0.001 * 7.021912574768066
Epoch 610, val loss: 0.9143992066383362
Epoch 620, training loss: 0.018049055710434914 = 0.01102034468203783 + 0.001 * 7.02871036529541
Epoch 620, val loss: 0.9216068387031555
Epoch 630, training loss: 0.017437227070331573 = 0.010411425493657589 + 0.001 * 7.025800704956055
Epoch 630, val loss: 0.9286933541297913
Epoch 640, training loss: 0.01687067560851574 = 0.009850403293967247 + 0.001 * 7.020272254943848
Epoch 640, val loss: 0.9356149435043335
Epoch 650, training loss: 0.01633923314511776 = 0.009332215413451195 + 0.001 * 7.007018089294434
Epoch 650, val loss: 0.9423978328704834
Epoch 660, training loss: 0.015853511169552803 = 0.008851679973304272 + 0.001 * 7.001830577850342
Epoch 660, val loss: 0.9490334987640381
Epoch 670, training loss: 0.015408793464303017 = 0.008404361084103584 + 0.001 * 7.004432678222656
Epoch 670, val loss: 0.9555511474609375
Epoch 680, training loss: 0.015003795735538006 = 0.007986974902451038 + 0.001 * 7.016820430755615
Epoch 680, val loss: 0.9619266986846924
Epoch 690, training loss: 0.014602679759263992 = 0.007596513722091913 + 0.001 * 7.006165504455566
Epoch 690, val loss: 0.9682429432868958
Epoch 700, training loss: 0.014220885932445526 = 0.007229879032820463 + 0.001 * 6.991006374359131
Epoch 700, val loss: 0.9744687676429749
Epoch 710, training loss: 0.013871980831027031 = 0.006882962305098772 + 0.001 * 6.989017963409424
Epoch 710, val loss: 0.9806683659553528
Epoch 720, training loss: 0.013545727357268333 = 0.0065520447678864 + 0.001 * 6.993682384490967
Epoch 720, val loss: 0.9869474172592163
Epoch 730, training loss: 0.013215489685535431 = 0.006235586013644934 + 0.001 * 6.979902744293213
Epoch 730, val loss: 0.9933390021324158
Epoch 740, training loss: 0.012923039495944977 = 0.005932895466685295 + 0.001 * 6.990143299102783
Epoch 740, val loss: 0.9998396039009094
Epoch 750, training loss: 0.012631253339350224 = 0.005644700489938259 + 0.001 * 6.986552715301514
Epoch 750, val loss: 1.0064330101013184
Epoch 760, training loss: 0.012341414578258991 = 0.005371380597352982 + 0.001 * 6.970033645629883
Epoch 760, val loss: 1.0130120515823364
Epoch 770, training loss: 0.012084050104022026 = 0.005113086197525263 + 0.001 * 6.970963478088379
Epoch 770, val loss: 1.0195798873901367
Epoch 780, training loss: 0.011876589618623257 = 0.004870006814599037 + 0.001 * 7.006582260131836
Epoch 780, val loss: 1.0260934829711914
Epoch 790, training loss: 0.011605602689087391 = 0.004641800187528133 + 0.001 * 6.963802337646484
Epoch 790, val loss: 1.0325417518615723
Epoch 800, training loss: 0.011386245489120483 = 0.004427979700267315 + 0.001 * 6.958265781402588
Epoch 800, val loss: 1.0389131307601929
Epoch 810, training loss: 0.011202612891793251 = 0.004228018224239349 + 0.001 * 6.974593639373779
Epoch 810, val loss: 1.0451816320419312
Epoch 820, training loss: 0.011011552065610886 = 0.004040973726660013 + 0.001 * 6.970578670501709
Epoch 820, val loss: 1.051344871520996
Epoch 830, training loss: 0.010838985443115234 = 0.003866213606670499 + 0.001 * 6.972771167755127
Epoch 830, val loss: 1.0573737621307373
Epoch 840, training loss: 0.010688796639442444 = 0.003702562302350998 + 0.001 * 6.986233711242676
Epoch 840, val loss: 1.0633066892623901
Epoch 850, training loss: 0.010499037802219391 = 0.0035494461189955473 + 0.001 * 6.949591636657715
Epoch 850, val loss: 1.0691190958023071
Epoch 860, training loss: 0.010348187759518623 = 0.0034060883335769176 + 0.001 * 6.942099094390869
Epoch 860, val loss: 1.074821949005127
Epoch 870, training loss: 0.010214856825768948 = 0.0032716921996325254 + 0.001 * 6.943164348602295
Epoch 870, val loss: 1.0804095268249512
Epoch 880, training loss: 0.010088492184877396 = 0.003145721275359392 + 0.001 * 6.942770481109619
Epoch 880, val loss: 1.085864543914795
Epoch 890, training loss: 0.009966107085347176 = 0.0030275683384388685 + 0.001 * 6.938538551330566
Epoch 890, val loss: 1.0912137031555176
Epoch 900, training loss: 0.009855763986706734 = 0.0029164429288357496 + 0.001 * 6.9393205642700195
Epoch 900, val loss: 1.0964628458023071
Epoch 910, training loss: 0.009760258719325066 = 0.0028120134957134724 + 0.001 * 6.948245048522949
Epoch 910, val loss: 1.1015820503234863
Epoch 920, training loss: 0.009659690782427788 = 0.0027136506978422403 + 0.001 * 6.94603967666626
Epoch 920, val loss: 1.1065787076950073
Epoch 930, training loss: 0.009584267623722553 = 0.0026208925992250443 + 0.001 * 6.963374614715576
Epoch 930, val loss: 1.111465334892273
Epoch 940, training loss: 0.00947287306189537 = 0.002533665392547846 + 0.001 * 6.939207553863525
Epoch 940, val loss: 1.1162313222885132
Epoch 950, training loss: 0.009370514191687107 = 0.0024511872325092554 + 0.001 * 6.919326305389404
Epoch 950, val loss: 1.1208794116973877
Epoch 960, training loss: 0.009303135797381401 = 0.0023732467088848352 + 0.001 * 6.92988920211792
Epoch 960, val loss: 1.125430703163147
Epoch 970, training loss: 0.00922437570989132 = 0.0022995700128376484 + 0.001 * 6.924805164337158
Epoch 970, val loss: 1.1298682689666748
Epoch 980, training loss: 0.009156053885817528 = 0.0022297189570963383 + 0.001 * 6.926334381103516
Epoch 980, val loss: 1.1342285871505737
Epoch 990, training loss: 0.009091740474104881 = 0.002163669792935252 + 0.001 * 6.928070545196533
Epoch 990, val loss: 1.1384919881820679
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9557
Flip ASR: 0.9467/225 nodes
The final ASR:0.77983, 0.12763, Accuracy:0.82099, 0.00972
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11620])
remove edge: torch.Size([2, 9436])
updated graph: torch.Size([2, 10500])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
The final ASR:0.97540, 0.00174, Accuracy:0.82716, 0.00462
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.944161057472229 = 1.9357872009277344 + 0.001 * 8.37381649017334
Epoch 0, val loss: 1.9361120462417603
Epoch 10, training loss: 1.934522032737732 = 1.9261484146118164 + 0.001 * 8.373673439025879
Epoch 10, val loss: 1.927180528640747
Epoch 20, training loss: 1.9220887422561646 = 1.9137154817581177 + 0.001 * 8.373297691345215
Epoch 20, val loss: 1.9152640104293823
Epoch 30, training loss: 1.9041165113449097 = 1.8957440853118896 + 0.001 * 8.372475624084473
Epoch 30, val loss: 1.8977131843566895
Epoch 40, training loss: 1.8773728609085083 = 1.8690025806427002 + 0.001 * 8.370338439941406
Epoch 40, val loss: 1.8717925548553467
Epoch 50, training loss: 1.8407044410705566 = 1.8323415517807007 + 0.001 * 8.362930297851562
Epoch 50, val loss: 1.838057279586792
Epoch 60, training loss: 1.8004157543182373 = 1.7920899391174316 + 0.001 * 8.325760841369629
Epoch 60, val loss: 1.8046154975891113
Epoch 70, training loss: 1.7605305910110474 = 1.7524665594100952 + 0.001 * 8.0640287399292
Epoch 70, val loss: 1.7720032930374146
Epoch 80, training loss: 1.7065922021865845 = 1.698972225189209 + 0.001 * 7.619949817657471
Epoch 80, val loss: 1.7249643802642822
Epoch 90, training loss: 1.6327301263809204 = 1.6253725290298462 + 0.001 * 7.357570171356201
Epoch 90, val loss: 1.6624993085861206
Epoch 100, training loss: 1.5386093854904175 = 1.531461477279663 + 0.001 * 7.147853851318359
Epoch 100, val loss: 1.5859276056289673
Epoch 110, training loss: 1.4345498085021973 = 1.4274710416793823 + 0.001 * 7.078730583190918
Epoch 110, val loss: 1.5011929273605347
Epoch 120, training loss: 1.3299113512039185 = 1.322865605354309 + 0.001 * 7.045773029327393
Epoch 120, val loss: 1.4181549549102783
Epoch 130, training loss: 1.2266734838485718 = 1.2196698188781738 + 0.001 * 7.003717422485352
Epoch 130, val loss: 1.337456464767456
Epoch 140, training loss: 1.125959873199463 = 1.1189961433410645 + 0.001 * 6.96370792388916
Epoch 140, val loss: 1.260554313659668
Epoch 150, training loss: 1.0304895639419556 = 1.0235707759857178 + 0.001 * 6.918800354003906
Epoch 150, val loss: 1.1890095472335815
Epoch 160, training loss: 0.9415491819381714 = 0.9346723556518555 + 0.001 * 6.876824855804443
Epoch 160, val loss: 1.1234424114227295
Epoch 170, training loss: 0.8584533929824829 = 0.8516022562980652 + 0.001 * 6.851121425628662
Epoch 170, val loss: 1.063174843788147
Epoch 180, training loss: 0.7801878452301025 = 0.7733438611030579 + 0.001 * 6.843954563140869
Epoch 180, val loss: 1.0065116882324219
Epoch 190, training loss: 0.7067360877990723 = 0.699895441532135 + 0.001 * 6.8406291007995605
Epoch 190, val loss: 0.9539168477058411
Epoch 200, training loss: 0.6390931010246277 = 0.632256031036377 + 0.001 * 6.837098121643066
Epoch 200, val loss: 0.9065180420875549
Epoch 210, training loss: 0.5780349373817444 = 0.5712004899978638 + 0.001 * 6.834441184997559
Epoch 210, val loss: 0.8656107783317566
Epoch 220, training loss: 0.5236663222312927 = 0.516834557056427 + 0.001 * 6.831788539886475
Epoch 220, val loss: 0.8319481611251831
Epoch 230, training loss: 0.4752603769302368 = 0.4684302508831024 + 0.001 * 6.830123424530029
Epoch 230, val loss: 0.8049997687339783
Epoch 240, training loss: 0.43153202533721924 = 0.42470496892929077 + 0.001 * 6.827051162719727
Epoch 240, val loss: 0.7836049199104309
Epoch 250, training loss: 0.3911684453487396 = 0.3843446373939514 + 0.001 * 6.823793888092041
Epoch 250, val loss: 0.7665656208992004
Epoch 260, training loss: 0.35335299372673035 = 0.34653183817863464 + 0.001 * 6.821150779724121
Epoch 260, val loss: 0.7534226775169373
Epoch 270, training loss: 0.31789708137512207 = 0.31107887625694275 + 0.001 * 6.818195343017578
Epoch 270, val loss: 0.7441542148590088
Epoch 280, training loss: 0.28491729497909546 = 0.2781023681163788 + 0.001 * 6.814917087554932
Epoch 280, val loss: 0.7387811541557312
Epoch 290, training loss: 0.2544889450073242 = 0.24767571687698364 + 0.001 * 6.813230037689209
Epoch 290, val loss: 0.7368766665458679
Epoch 300, training loss: 0.22651372849941254 = 0.21970628201961517 + 0.001 * 6.807452201843262
Epoch 300, val loss: 0.7383073568344116
Epoch 310, training loss: 0.20078153908252716 = 0.19397775828838348 + 0.001 * 6.803785800933838
Epoch 310, val loss: 0.7423983216285706
Epoch 320, training loss: 0.17719592154026031 = 0.17039786279201508 + 0.001 * 6.798058986663818
Epoch 320, val loss: 0.7489954829216003
Epoch 330, training loss: 0.1558276116847992 = 0.1490347981452942 + 0.001 * 6.792818069458008
Epoch 330, val loss: 0.7582689523696899
Epoch 340, training loss: 0.1367553323507309 = 0.1299695074558258 + 0.001 * 6.78582763671875
Epoch 340, val loss: 0.770105242729187
Epoch 350, training loss: 0.12001415342092514 = 0.11323276162147522 + 0.001 * 6.781390190124512
Epoch 350, val loss: 0.7842446565628052
Epoch 360, training loss: 0.10550131648778915 = 0.09873048216104507 + 0.001 * 6.77083158493042
Epoch 360, val loss: 0.8003051280975342
Epoch 370, training loss: 0.09303702414035797 = 0.08627866208553314 + 0.001 * 6.758358478546143
Epoch 370, val loss: 0.8177048563957214
Epoch 380, training loss: 0.08240163326263428 = 0.0756417065858841 + 0.001 * 6.75992488861084
Epoch 380, val loss: 0.8357160091400146
Epoch 390, training loss: 0.07331333309412003 = 0.06657227873802185 + 0.001 * 6.741055965423584
Epoch 390, val loss: 0.8539291620254517
Epoch 400, training loss: 0.06557034701108932 = 0.058835648000240326 + 0.001 * 6.734701156616211
Epoch 400, val loss: 0.8719077706336975
Epoch 410, training loss: 0.05895495414733887 = 0.05221877619624138 + 0.001 * 6.736177921295166
Epoch 410, val loss: 0.8893927931785583
Epoch 420, training loss: 0.05327560007572174 = 0.046539582312107086 + 0.001 * 6.73601770401001
Epoch 420, val loss: 0.9063138961791992
Epoch 430, training loss: 0.04838267341256142 = 0.04165026545524597 + 0.001 * 6.732408046722412
Epoch 430, val loss: 0.9226359724998474
Epoch 440, training loss: 0.04415544867515564 = 0.037426456809043884 + 0.001 * 6.728992462158203
Epoch 440, val loss: 0.9383873343467712
Epoch 450, training loss: 0.04049421474337578 = 0.03376796096563339 + 0.001 * 6.72625207901001
Epoch 450, val loss: 0.9535808563232422
Epoch 460, training loss: 0.037305448204278946 = 0.0305859986692667 + 0.001 * 6.719447612762451
Epoch 460, val loss: 0.9682108759880066
Epoch 470, training loss: 0.03453438729047775 = 0.027808567509055138 + 0.001 * 6.725817680358887
Epoch 470, val loss: 0.9823006987571716
Epoch 480, training loss: 0.03209573030471802 = 0.02537509985268116 + 0.001 * 6.7206315994262695
Epoch 480, val loss: 0.9958352446556091
Epoch 490, training loss: 0.029952511191368103 = 0.023235175758600235 + 0.001 * 6.717334747314453
Epoch 490, val loss: 1.0088868141174316
Epoch 500, training loss: 0.028059326112270355 = 0.021346883848309517 + 0.001 * 6.712441444396973
Epoch 500, val loss: 1.0214359760284424
Epoch 510, training loss: 0.02638920582830906 = 0.01967448741197586 + 0.001 * 6.714718341827393
Epoch 510, val loss: 1.0335253477096558
Epoch 520, training loss: 0.024914322420954704 = 0.018188146874308586 + 0.001 * 6.726174831390381
Epoch 520, val loss: 1.0451971292495728
Epoch 530, training loss: 0.023572448641061783 = 0.016862699761986732 + 0.001 * 6.7097487449646
Epoch 530, val loss: 1.056447982788086
Epoch 540, training loss: 0.022384315729141235 = 0.01567675545811653 + 0.001 * 6.707559585571289
Epoch 540, val loss: 1.0672823190689087
Epoch 550, training loss: 0.021316900849342346 = 0.014612107537686825 + 0.001 * 6.704793930053711
Epoch 550, val loss: 1.0777719020843506
Epoch 560, training loss: 0.02035772055387497 = 0.013653435744345188 + 0.001 * 6.704283714294434
Epoch 560, val loss: 1.0879087448120117
Epoch 570, training loss: 0.01948973350226879 = 0.012787754647433758 + 0.001 * 6.7019782066345215
Epoch 570, val loss: 1.0977164506912231
Epoch 580, training loss: 0.018707746639847755 = 0.012003496289253235 + 0.001 * 6.704250335693359
Epoch 580, val loss: 1.1071829795837402
Epoch 590, training loss: 0.01798936352133751 = 0.011291079223155975 + 0.001 * 6.698283672332764
Epoch 590, val loss: 1.1163556575775146
Epoch 600, training loss: 0.017339618876576424 = 0.010642189532518387 + 0.001 * 6.6974287033081055
Epoch 600, val loss: 1.1252490282058716
Epoch 610, training loss: 0.016743527725338936 = 0.0100496094673872 + 0.001 * 6.693917274475098
Epoch 610, val loss: 1.1338608264923096
Epoch 620, training loss: 0.016210008412599564 = 0.009506970643997192 + 0.001 * 6.703036308288574
Epoch 620, val loss: 1.1422237157821655
Epoch 630, training loss: 0.01570158451795578 = 0.009008840657770634 + 0.001 * 6.69274377822876
Epoch 630, val loss: 1.1503477096557617
Epoch 640, training loss: 0.015237860381603241 = 0.008550087921321392 + 0.001 * 6.687772274017334
Epoch 640, val loss: 1.1582515239715576
Epoch 650, training loss: 0.014822181314229965 = 0.008126662112772465 + 0.001 * 6.695518493652344
Epoch 650, val loss: 1.1659235954284668
Epoch 660, training loss: 0.014427382498979568 = 0.00773489149287343 + 0.001 * 6.692490100860596
Epoch 660, val loss: 1.1734130382537842
Epoch 670, training loss: 0.014052016660571098 = 0.007371447514742613 + 0.001 * 6.680569171905518
Epoch 670, val loss: 1.180732011795044
Epoch 680, training loss: 0.013775242492556572 = 0.007033518515527248 + 0.001 * 6.741724014282227
Epoch 680, val loss: 1.1879106760025024
Epoch 690, training loss: 0.013408197090029716 = 0.006718828342854977 + 0.001 * 6.68936824798584
Epoch 690, val loss: 1.194899082183838
Epoch 700, training loss: 0.013100866228342056 = 0.006425078492611647 + 0.001 * 6.675786972045898
Epoch 700, val loss: 1.201741337776184
Epoch 710, training loss: 0.012845647521317005 = 0.006150415167212486 + 0.001 * 6.695231914520264
Epoch 710, val loss: 1.2084280252456665
Epoch 720, training loss: 0.01258653961122036 = 0.005893215537071228 + 0.001 * 6.693323135375977
Epoch 720, val loss: 1.2149738073349
Epoch 730, training loss: 0.012326128780841827 = 0.005652207415550947 + 0.001 * 6.673921585083008
Epoch 730, val loss: 1.2213940620422363
Epoch 740, training loss: 0.012103133834898472 = 0.0054258122108876705 + 0.001 * 6.677321434020996
Epoch 740, val loss: 1.2276772260665894
Epoch 750, training loss: 0.011912677437067032 = 0.005212937016040087 + 0.001 * 6.699740409851074
Epoch 750, val loss: 1.2338342666625977
Epoch 760, training loss: 0.011678427457809448 = 0.005012824200093746 + 0.001 * 6.665602684020996
Epoch 760, val loss: 1.2398555278778076
Epoch 770, training loss: 0.011490805074572563 = 0.0048243096098303795 + 0.001 * 6.666494846343994
Epoch 770, val loss: 1.24576735496521
Epoch 780, training loss: 0.011308297514915466 = 0.004646680783480406 + 0.001 * 6.661616325378418
Epoch 780, val loss: 1.2515448331832886
Epoch 790, training loss: 0.011172961443662643 = 0.004479041788727045 + 0.001 * 6.693918704986572
Epoch 790, val loss: 1.2572070360183716
Epoch 800, training loss: 0.010982139967381954 = 0.004320768639445305 + 0.001 * 6.661371231079102
Epoch 800, val loss: 1.2627549171447754
Epoch 810, training loss: 0.010846570134162903 = 0.004171227570623159 + 0.001 * 6.675342082977295
Epoch 810, val loss: 1.2681899070739746
Epoch 820, training loss: 0.010681163519620895 = 0.004029756411910057 + 0.001 * 6.651406764984131
Epoch 820, val loss: 1.2735203504562378
Epoch 830, training loss: 0.010548478923738003 = 0.0038959255907684565 + 0.001 * 6.652553081512451
Epoch 830, val loss: 1.2787548303604126
Epoch 840, training loss: 0.010442299768328667 = 0.0037691837642341852 + 0.001 * 6.673115253448486
Epoch 840, val loss: 1.2838844060897827
Epoch 850, training loss: 0.010303611867129803 = 0.003649105317890644 + 0.001 * 6.654506206512451
Epoch 850, val loss: 1.2889032363891602
Epoch 860, training loss: 0.010184472426772118 = 0.0035351072438061237 + 0.001 * 6.649364948272705
Epoch 860, val loss: 1.2938445806503296
Epoch 870, training loss: 0.01008585374802351 = 0.0034269497264176607 + 0.001 * 6.6589035987854
Epoch 870, val loss: 1.2986947298049927
Epoch 880, training loss: 0.009975690394639969 = 0.00332421762868762 + 0.001 * 6.651472091674805
Epoch 880, val loss: 1.3034336566925049
Epoch 890, training loss: 0.009878605604171753 = 0.0032264876645058393 + 0.001 * 6.652117729187012
Epoch 890, val loss: 1.3080967664718628
Epoch 900, training loss: 0.009789505042135715 = 0.0031335412058979273 + 0.001 * 6.65596342086792
Epoch 900, val loss: 1.3126587867736816
Epoch 910, training loss: 0.009685982950031757 = 0.003045079531148076 + 0.001 * 6.640903472900391
Epoch 910, val loss: 1.3171453475952148
Epoch 920, training loss: 0.00959727168083191 = 0.002960772719234228 + 0.001 * 6.636498928070068
Epoch 920, val loss: 1.321526050567627
Epoch 930, training loss: 0.009509935975074768 = 0.002880350686609745 + 0.001 * 6.629584789276123
Epoch 930, val loss: 1.3258379697799683
Epoch 940, training loss: 0.00946029182523489 = 0.0028036057483404875 + 0.001 * 6.656685829162598
Epoch 940, val loss: 1.3300869464874268
Epoch 950, training loss: 0.00937731470912695 = 0.0027303784154355526 + 0.001 * 6.646935939788818
Epoch 950, val loss: 1.3342249393463135
Epoch 960, training loss: 0.009306845255196095 = 0.002660389756783843 + 0.001 * 6.64645528793335
Epoch 960, val loss: 1.3382819890975952
Epoch 970, training loss: 0.009241266176104546 = 0.002593474928289652 + 0.001 * 6.647791385650635
Epoch 970, val loss: 1.3422813415527344
Epoch 980, training loss: 0.00918157771229744 = 0.002529500052332878 + 0.001 * 6.652077674865723
Epoch 980, val loss: 1.3462036848068237
Epoch 990, training loss: 0.009108295664191246 = 0.0024682674556970596 + 0.001 * 6.64002799987793
Epoch 990, val loss: 1.3500442504882812
Epoch 1000, training loss: 0.00903153046965599 = 0.0024096500128507614 + 0.001 * 6.621880054473877
Epoch 1000, val loss: 1.3538240194320679
Epoch 1010, training loss: 0.009009355679154396 = 0.00235350476577878 + 0.001 * 6.655850410461426
Epoch 1010, val loss: 1.3575494289398193
Epoch 1020, training loss: 0.008938895538449287 = 0.0022996999323368073 + 0.001 * 6.639194965362549
Epoch 1020, val loss: 1.3611769676208496
Epoch 1030, training loss: 0.008867936208844185 = 0.0022480995394289494 + 0.001 * 6.619836330413818
Epoch 1030, val loss: 1.3647685050964355
Epoch 1040, training loss: 0.00881984643638134 = 0.002198565984144807 + 0.001 * 6.621280193328857
Epoch 1040, val loss: 1.3683068752288818
Epoch 1050, training loss: 0.008766932412981987 = 0.0021510040387511253 + 0.001 * 6.6159281730651855
Epoch 1050, val loss: 1.3717681169509888
Epoch 1060, training loss: 0.008760165423154831 = 0.002105327323079109 + 0.001 * 6.6548380851745605
Epoch 1060, val loss: 1.3751834630966187
Epoch 1070, training loss: 0.008697041310369968 = 0.0020614091772586107 + 0.001 * 6.635631561279297
Epoch 1070, val loss: 1.3785114288330078
Epoch 1080, training loss: 0.008645614609122276 = 0.0020191939547657967 + 0.001 * 6.626420497894287
Epoch 1080, val loss: 1.3817999362945557
Epoch 1090, training loss: 0.008596709929406643 = 0.0019785596523433924 + 0.001 * 6.618149757385254
Epoch 1090, val loss: 1.3850340843200684
Epoch 1100, training loss: 0.008548818528652191 = 0.0019394571427255869 + 0.001 * 6.609361171722412
Epoch 1100, val loss: 1.3882133960723877
Epoch 1110, training loss: 0.008511331863701344 = 0.0019017949234694242 + 0.001 * 6.609536170959473
Epoch 1110, val loss: 1.3913516998291016
Epoch 1120, training loss: 0.00848097912967205 = 0.0018655149033293128 + 0.001 * 6.615464210510254
Epoch 1120, val loss: 1.394410252571106
Epoch 1130, training loss: 0.008444633334875107 = 0.0018305365229025483 + 0.001 * 6.614096641540527
Epoch 1130, val loss: 1.397441029548645
Epoch 1140, training loss: 0.008411423303186893 = 0.0017967966850847006 + 0.001 * 6.614625930786133
Epoch 1140, val loss: 1.4004133939743042
Epoch 1150, training loss: 0.008387020789086819 = 0.0017642162274569273 + 0.001 * 6.622804164886475
Epoch 1150, val loss: 1.4033377170562744
Epoch 1160, training loss: 0.008337383158504963 = 0.0017328078392893076 + 0.001 * 6.604574680328369
Epoch 1160, val loss: 1.4062128067016602
Epoch 1170, training loss: 0.008316395804286003 = 0.0017024708213284612 + 0.001 * 6.613924980163574
Epoch 1170, val loss: 1.4090512990951538
Epoch 1180, training loss: 0.0082840071991086 = 0.001673185615800321 + 0.001 * 6.610821723937988
Epoch 1180, val loss: 1.4118531942367554
Epoch 1190, training loss: 0.008248969912528992 = 0.0016448998358100653 + 0.001 * 6.60407018661499
Epoch 1190, val loss: 1.414596676826477
Epoch 1200, training loss: 0.008236159570515156 = 0.001617553411051631 + 0.001 * 6.618606090545654
Epoch 1200, val loss: 1.4172875881195068
Epoch 1210, training loss: 0.008198420517146587 = 0.0015911704394966364 + 0.001 * 6.607249736785889
Epoch 1210, val loss: 1.419931173324585
Epoch 1220, training loss: 0.008189286105334759 = 0.001565624144859612 + 0.001 * 6.623661994934082
Epoch 1220, val loss: 1.422561764717102
Epoch 1230, training loss: 0.008156266063451767 = 0.0015409075422212481 + 0.001 * 6.615358352661133
Epoch 1230, val loss: 1.4251240491867065
Epoch 1240, training loss: 0.008120928891003132 = 0.001516953227110207 + 0.001 * 6.603975296020508
Epoch 1240, val loss: 1.4276719093322754
Epoch 1250, training loss: 0.008093642070889473 = 0.001493759104050696 + 0.001 * 6.59988260269165
Epoch 1250, val loss: 1.4301834106445312
Epoch 1260, training loss: 0.00806579738855362 = 0.0014713227283209562 + 0.001 * 6.594474792480469
Epoch 1260, val loss: 1.4326283931732178
Epoch 1270, training loss: 0.008072743192315102 = 0.0014495843788608909 + 0.001 * 6.623158931732178
Epoch 1270, val loss: 1.4350439310073853
Epoch 1280, training loss: 0.008029604330658913 = 0.0014285529032349586 + 0.001 * 6.601051330566406
Epoch 1280, val loss: 1.437421441078186
Epoch 1290, training loss: 0.008005039766430855 = 0.0014081394765526056 + 0.001 * 6.59689998626709
Epoch 1290, val loss: 1.439774513244629
Epoch 1300, training loss: 0.007991711609065533 = 0.0013883586507290602 + 0.001 * 6.6033525466918945
Epoch 1300, val loss: 1.4420819282531738
Epoch 1310, training loss: 0.00797468051314354 = 0.001369149424135685 + 0.001 * 6.605530738830566
Epoch 1310, val loss: 1.4443602561950684
Epoch 1320, training loss: 0.007939835079014301 = 0.0013505235547199845 + 0.001 * 6.589311122894287
Epoch 1320, val loss: 1.446598768234253
Epoch 1330, training loss: 0.00792122446000576 = 0.0013324677711352706 + 0.001 * 6.588756561279297
Epoch 1330, val loss: 1.4488110542297363
Epoch 1340, training loss: 0.007905819453299046 = 0.0013149172300472856 + 0.001 * 6.590901851654053
Epoch 1340, val loss: 1.4509906768798828
Epoch 1350, training loss: 0.00789755117148161 = 0.0012978846207261086 + 0.001 * 6.599666118621826
Epoch 1350, val loss: 1.453141212463379
Epoch 1360, training loss: 0.007864155806601048 = 0.001281318487599492 + 0.001 * 6.582836627960205
Epoch 1360, val loss: 1.4552336931228638
Epoch 1370, training loss: 0.007850253023207188 = 0.001265241182409227 + 0.001 * 6.5850114822387695
Epoch 1370, val loss: 1.457353949546814
Epoch 1380, training loss: 0.007824195548892021 = 0.0012496064882725477 + 0.001 * 6.574588775634766
Epoch 1380, val loss: 1.4593958854675293
Epoch 1390, training loss: 0.007824300788342953 = 0.001234395895153284 + 0.001 * 6.58990478515625
Epoch 1390, val loss: 1.4614235162734985
Epoch 1400, training loss: 0.0078057292848825455 = 0.001219615456648171 + 0.001 * 6.586113452911377
Epoch 1400, val loss: 1.4634038209915161
Epoch 1410, training loss: 0.0078016119077801704 = 0.0012052301317453384 + 0.001 * 6.596381664276123
Epoch 1410, val loss: 1.4653862714767456
Epoch 1420, training loss: 0.007785363122820854 = 0.0011912430636584759 + 0.001 * 6.594119548797607
Epoch 1420, val loss: 1.4673246145248413
Epoch 1430, training loss: 0.007754998747259378 = 0.001177615369670093 + 0.001 * 6.577383041381836
Epoch 1430, val loss: 1.4692109823226929
Epoch 1440, training loss: 0.007762595545500517 = 0.0011643650941550732 + 0.001 * 6.598230361938477
Epoch 1440, val loss: 1.471121072769165
Epoch 1450, training loss: 0.0077364444732666016 = 0.0011514527723193169 + 0.001 * 6.584991455078125
Epoch 1450, val loss: 1.4729517698287964
Epoch 1460, training loss: 0.007710443809628487 = 0.0011388856219127774 + 0.001 * 6.571557998657227
Epoch 1460, val loss: 1.4748082160949707
Epoch 1470, training loss: 0.007700330577790737 = 0.0011266634101048112 + 0.001 * 6.573667049407959
Epoch 1470, val loss: 1.4765870571136475
Epoch 1480, training loss: 0.007698857691138983 = 0.0011147278128191829 + 0.001 * 6.584129333496094
Epoch 1480, val loss: 1.4783769845962524
Epoch 1490, training loss: 0.007692555896937847 = 0.0011031070025637746 + 0.001 * 6.589448928833008
Epoch 1490, val loss: 1.480136752128601
Epoch 1500, training loss: 0.007676062174141407 = 0.0010918129701167345 + 0.001 * 6.584249019622803
Epoch 1500, val loss: 1.4818397760391235
Epoch 1510, training loss: 0.007646636571735144 = 0.0010807793587446213 + 0.001 * 6.56585693359375
Epoch 1510, val loss: 1.4835460186004639
Epoch 1520, training loss: 0.00763454707339406 = 0.0010700304992496967 + 0.001 * 6.564516067504883
Epoch 1520, val loss: 1.4852404594421387
Epoch 1530, training loss: 0.007623225450515747 = 0.001059556845575571 + 0.001 * 6.563668251037598
Epoch 1530, val loss: 1.4868775606155396
Epoch 1540, training loss: 0.007656957022845745 = 0.0010493295267224312 + 0.001 * 6.6076273918151855
Epoch 1540, val loss: 1.4885025024414062
Epoch 1550, training loss: 0.007620126474648714 = 0.0010393847478553653 + 0.001 * 6.5807414054870605
Epoch 1550, val loss: 1.490087866783142
Epoch 1560, training loss: 0.0075887590646743774 = 0.0010296665132045746 + 0.001 * 6.559092044830322
Epoch 1560, val loss: 1.4916713237762451
Epoch 1570, training loss: 0.007587028667330742 = 0.0010201859986409545 + 0.001 * 6.566842079162598
Epoch 1570, val loss: 1.4932283163070679
Epoch 1580, training loss: 0.007592606358230114 = 0.0010109369177371264 + 0.001 * 6.581668853759766
Epoch 1580, val loss: 1.4947338104248047
Epoch 1590, training loss: 0.007563499733805656 = 0.0010018961038440466 + 0.001 * 6.56160306930542
Epoch 1590, val loss: 1.4962526559829712
Epoch 1600, training loss: 0.007566285319626331 = 0.0009930814849212766 + 0.001 * 6.5732035636901855
Epoch 1600, val loss: 1.497748613357544
Epoch 1610, training loss: 0.007575453259050846 = 0.0009844506857916713 + 0.001 * 6.591001987457275
Epoch 1610, val loss: 1.4992088079452515
Epoch 1620, training loss: 0.0075400471687316895 = 0.0009760415414348245 + 0.001 * 6.564005374908447
Epoch 1620, val loss: 1.5006505250930786
Epoch 1630, training loss: 0.007544734515249729 = 0.0009677998605184257 + 0.001 * 6.576934337615967
Epoch 1630, val loss: 1.5020661354064941
Epoch 1640, training loss: 0.0075366804376244545 = 0.0009597665630280972 + 0.001 * 6.576913356781006
Epoch 1640, val loss: 1.5035005807876587
Epoch 1650, training loss: 0.007552629802376032 = 0.0009518936858512461 + 0.001 * 6.600735664367676
Epoch 1650, val loss: 1.5048848390579224
Epoch 1660, training loss: 0.007506723515689373 = 0.0009442386799491942 + 0.001 * 6.5624847412109375
Epoch 1660, val loss: 1.5062484741210938
Epoch 1670, training loss: 0.007521383929997683 = 0.0009367371094413102 + 0.001 * 6.584646224975586
Epoch 1670, val loss: 1.5076063871383667
Epoch 1680, training loss: 0.007474727928638458 = 0.0009293839102610946 + 0.001 * 6.545343399047852
Epoch 1680, val loss: 1.5089447498321533
Epoch 1690, training loss: 0.007483163382858038 = 0.0009222081862390041 + 0.001 * 6.560955047607422
Epoch 1690, val loss: 1.510285496711731
Epoch 1700, training loss: 0.0074784886091947556 = 0.0009151675621978939 + 0.001 * 6.563321113586426
Epoch 1700, val loss: 1.5115786790847778
Epoch 1710, training loss: 0.00748469028621912 = 0.0009083043551072478 + 0.001 * 6.576385498046875
Epoch 1710, val loss: 1.5128734111785889
Epoch 1720, training loss: 0.007481065578758717 = 0.0009015731047838926 + 0.001 * 6.579492568969727
Epoch 1720, val loss: 1.5141265392303467
Epoch 1730, training loss: 0.007467722985893488 = 0.000894996162969619 + 0.001 * 6.572726249694824
Epoch 1730, val loss: 1.5153752565383911
Epoch 1740, training loss: 0.007440838031470776 = 0.0008885371498763561 + 0.001 * 6.552300453186035
Epoch 1740, val loss: 1.516614556312561
Epoch 1750, training loss: 0.007456636987626553 = 0.0008822232484817505 + 0.001 * 6.574413299560547
Epoch 1750, val loss: 1.5178357362747192
Epoch 1760, training loss: 0.007451471872627735 = 0.0008760326309129596 + 0.001 * 6.575438976287842
Epoch 1760, val loss: 1.5190562009811401
Epoch 1770, training loss: 0.007417353335767984 = 0.0008699854952283204 + 0.001 * 6.547367572784424
Epoch 1770, val loss: 1.5202175378799438
Epoch 1780, training loss: 0.007422347087413073 = 0.0008640644955448806 + 0.001 * 6.558282375335693
Epoch 1780, val loss: 1.5214238166809082
Epoch 1790, training loss: 0.00741809094324708 = 0.0008582565933465958 + 0.001 * 6.559834003448486
Epoch 1790, val loss: 1.5225478410720825
Epoch 1800, training loss: 0.007384058553725481 = 0.0008525714511051774 + 0.001 * 6.531486988067627
Epoch 1800, val loss: 1.5237090587615967
Epoch 1810, training loss: 0.007390276528894901 = 0.0008470056927762926 + 0.001 * 6.543270587921143
Epoch 1810, val loss: 1.5248453617095947
Epoch 1820, training loss: 0.007383052259683609 = 0.0008415472693741322 + 0.001 * 6.541504859924316
Epoch 1820, val loss: 1.5259571075439453
Epoch 1830, training loss: 0.007369988597929478 = 0.0008361994405277073 + 0.001 * 6.533788681030273
Epoch 1830, val loss: 1.527058482170105
Epoch 1840, training loss: 0.00736486678943038 = 0.0008309495751745999 + 0.001 * 6.53391695022583
Epoch 1840, val loss: 1.528157114982605
Epoch 1850, training loss: 0.0073799919337034225 = 0.0008257869631052017 + 0.001 * 6.55420446395874
Epoch 1850, val loss: 1.5292398929595947
Epoch 1860, training loss: 0.007386154495179653 = 0.0008207512437365949 + 0.001 * 6.565402984619141
Epoch 1860, val loss: 1.5303083658218384
Epoch 1870, training loss: 0.007353762164711952 = 0.0008157894480973482 + 0.001 * 6.537972450256348
Epoch 1870, val loss: 1.5313860177993774
Epoch 1880, training loss: 0.007340848445892334 = 0.0008109300979413092 + 0.001 * 6.529918193817139
Epoch 1880, val loss: 1.5324361324310303
Epoch 1890, training loss: 0.007349231746047735 = 0.0008061593980528414 + 0.001 * 6.54307222366333
Epoch 1890, val loss: 1.533491611480713
Epoch 1900, training loss: 0.007338284514844418 = 0.0008014880586415529 + 0.001 * 6.5367960929870605
Epoch 1900, val loss: 1.534502387046814
Epoch 1910, training loss: 0.0073180790059268475 = 0.0007968944264575839 + 0.001 * 6.521183967590332
Epoch 1910, val loss: 1.5355253219604492
Epoch 1920, training loss: 0.007311420515179634 = 0.0007923931698314846 + 0.001 * 6.519027233123779
Epoch 1920, val loss: 1.536550760269165
Epoch 1930, training loss: 0.007319979835301638 = 0.0007879751501604915 + 0.001 * 6.532004356384277
Epoch 1930, val loss: 1.5375176668167114
Epoch 1940, training loss: 0.0073194620199501514 = 0.0007836296572349966 + 0.001 * 6.535831928253174
Epoch 1940, val loss: 1.5385380983352661
Epoch 1950, training loss: 0.007295422255992889 = 0.000779361289460212 + 0.001 * 6.5160603523254395
Epoch 1950, val loss: 1.5394957065582275
Epoch 1960, training loss: 0.007309902459383011 = 0.0007751656812615693 + 0.001 * 6.534736156463623
Epoch 1960, val loss: 1.5405094623565674
Epoch 1970, training loss: 0.007304051890969276 = 0.0007710485951974988 + 0.001 * 6.533003330230713
Epoch 1970, val loss: 1.5414527654647827
Epoch 1980, training loss: 0.0073103876784443855 = 0.0007669991464354098 + 0.001 * 6.543388366699219
Epoch 1980, val loss: 1.5423943996429443
Epoch 1990, training loss: 0.0072966571897268295 = 0.0007630375912413001 + 0.001 * 6.533619403839111
Epoch 1990, val loss: 1.5433690547943115
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8037
Overall ASR: 0.6458
Flip ASR: 0.5733/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9457484483718872 = 1.9373745918273926 + 0.001 * 8.373852729797363
Epoch 0, val loss: 1.9325003623962402
Epoch 10, training loss: 1.935247778892517 = 1.926874041557312 + 0.001 * 8.373727798461914
Epoch 10, val loss: 1.92166268825531
Epoch 20, training loss: 1.922451138496399 = 1.9140777587890625 + 0.001 * 8.373377799987793
Epoch 20, val loss: 1.907873511314392
Epoch 30, training loss: 1.9048429727554321 = 1.896470308303833 + 0.001 * 8.372695922851562
Epoch 30, val loss: 1.888482689857483
Epoch 40, training loss: 1.879564881324768 = 1.8711936473846436 + 0.001 * 8.371284484863281
Epoch 40, val loss: 1.8607407808303833
Epoch 50, training loss: 1.8448357582092285 = 1.836467981338501 + 0.001 * 8.367740631103516
Epoch 50, val loss: 1.824148178100586
Epoch 60, training loss: 1.8039758205413818 = 1.795620083808899 + 0.001 * 8.355719566345215
Epoch 60, val loss: 1.7841191291809082
Epoch 70, training loss: 1.7620607614517212 = 1.7537657022476196 + 0.001 * 8.295113563537598
Epoch 70, val loss: 1.7465251684188843
Epoch 80, training loss: 1.7105661630630493 = 1.702601432800293 + 0.001 * 7.964747905731201
Epoch 80, val loss: 1.7014847993850708
Epoch 90, training loss: 1.6409311294555664 = 1.6333401203155518 + 0.001 * 7.5910258293151855
Epoch 90, val loss: 1.6403968334197998
Epoch 100, training loss: 1.5489453077316284 = 1.5416717529296875 + 0.001 * 7.273598670959473
Epoch 100, val loss: 1.5614656209945679
Epoch 110, training loss: 1.4396188259124756 = 1.4326001405715942 + 0.001 * 7.018661022186279
Epoch 110, val loss: 1.471958875656128
Epoch 120, training loss: 1.3236087560653687 = 1.3166471719741821 + 0.001 * 6.961586952209473
Epoch 120, val loss: 1.3802516460418701
Epoch 130, training loss: 1.2082834243774414 = 1.201366901397705 + 0.001 * 6.9164652824401855
Epoch 130, val loss: 1.2919636964797974
Epoch 140, training loss: 1.0999256372451782 = 1.0930325984954834 + 0.001 * 6.89303731918335
Epoch 140, val loss: 1.2103347778320312
Epoch 150, training loss: 1.0032010078430176 = 0.9963251352310181 + 0.001 * 6.8758955001831055
Epoch 150, val loss: 1.1379231214523315
Epoch 160, training loss: 0.9182191491127014 = 0.9113532900810242 + 0.001 * 6.865882396697998
Epoch 160, val loss: 1.0751159191131592
Epoch 170, training loss: 0.8413637280464172 = 0.8345038294792175 + 0.001 * 6.859882831573486
Epoch 170, val loss: 1.0182218551635742
Epoch 180, training loss: 0.7688347101211548 = 0.7619788646697998 + 0.001 * 6.855823993682861
Epoch 180, val loss: 0.9642266035079956
Epoch 190, training loss: 0.6992015838623047 = 0.6923493146896362 + 0.001 * 6.852270603179932
Epoch 190, val loss: 0.9119145274162292
Epoch 200, training loss: 0.6332773566246033 = 0.6264281272888184 + 0.001 * 6.849253177642822
Epoch 200, val loss: 0.862648606300354
Epoch 210, training loss: 0.5726205110549927 = 0.5657737851142883 + 0.001 * 6.846731662750244
Epoch 210, val loss: 0.8189385533332825
Epoch 220, training loss: 0.5175085067749023 = 0.5106644034385681 + 0.001 * 6.844121932983398
Epoch 220, val loss: 0.7818083167076111
Epoch 230, training loss: 0.46713605523109436 = 0.4602949619293213 + 0.001 * 6.8410820960998535
Epoch 230, val loss: 0.7500511407852173
Epoch 240, training loss: 0.4200703501701355 = 0.4132327735424042 + 0.001 * 6.83757209777832
Epoch 240, val loss: 0.7227454781532288
Epoch 250, training loss: 0.3752087652683258 = 0.368374764919281 + 0.001 * 6.833986759185791
Epoch 250, val loss: 0.6982862949371338
Epoch 260, training loss: 0.3321700096130371 = 0.32533928751945496 + 0.001 * 6.8307342529296875
Epoch 260, val loss: 0.6755190491676331
Epoch 270, training loss: 0.29127952456474304 = 0.2844516634941101 + 0.001 * 6.827868938446045
Epoch 270, val loss: 0.6547746658325195
Epoch 280, training loss: 0.25344160199165344 = 0.2466161549091339 + 0.001 * 6.825450420379639
Epoch 280, val loss: 0.6365503668785095
Epoch 290, training loss: 0.21974104642868042 = 0.2129174917936325 + 0.001 * 6.823554992675781
Epoch 290, val loss: 0.6220083832740784
Epoch 300, training loss: 0.1907399743795395 = 0.18391788005828857 + 0.001 * 6.822092056274414
Epoch 300, val loss: 0.6112313270568848
Epoch 310, training loss: 0.16644421219825745 = 0.15962329506874084 + 0.001 * 6.820911884307861
Epoch 310, val loss: 0.6044493317604065
Epoch 320, training loss: 0.14630275964736938 = 0.13948294520378113 + 0.001 * 6.819818019866943
Epoch 320, val loss: 0.601456880569458
Epoch 330, training loss: 0.12956340610980988 = 0.12274365872144699 + 0.001 * 6.819748401641846
Epoch 330, val loss: 0.601766049861908
Epoch 340, training loss: 0.11559855937957764 = 0.1087803915143013 + 0.001 * 6.818164348602295
Epoch 340, val loss: 0.6047306656837463
Epoch 350, training loss: 0.10380405932664871 = 0.09698744863271713 + 0.001 * 6.8166069984436035
Epoch 350, val loss: 0.6096349358558655
Epoch 360, training loss: 0.09373979270458221 = 0.0869247317314148 + 0.001 * 6.815061092376709
Epoch 360, val loss: 0.6160005927085876
Epoch 370, training loss: 0.08502840250730515 = 0.07821386307477951 + 0.001 * 6.814542293548584
Epoch 370, val loss: 0.6234459280967712
Epoch 380, training loss: 0.07742499560117722 = 0.07061318308115005 + 0.001 * 6.811809539794922
Epoch 380, val loss: 0.6315596699714661
Epoch 390, training loss: 0.07071349024772644 = 0.06390368938446045 + 0.001 * 6.809800148010254
Epoch 390, val loss: 0.6401917338371277
Epoch 400, training loss: 0.06472849100828171 = 0.05792120099067688 + 0.001 * 6.807291030883789
Epoch 400, val loss: 0.6488539576530457
Epoch 410, training loss: 0.059336692094802856 = 0.0525306761264801 + 0.001 * 6.806015968322754
Epoch 410, val loss: 0.6578577160835266
Epoch 420, training loss: 0.05448048934340477 = 0.04767642542719841 + 0.001 * 6.804062366485596
Epoch 420, val loss: 0.6669242978096008
Epoch 430, training loss: 0.05011735484004021 = 0.043311044573783875 + 0.001 * 6.806310653686523
Epoch 430, val loss: 0.6761487126350403
Epoch 440, training loss: 0.04615135118365288 = 0.0393502376973629 + 0.001 * 6.801112651824951
Epoch 440, val loss: 0.685360312461853
Epoch 450, training loss: 0.04257268086075783 = 0.035774096846580505 + 0.001 * 6.798584461212158
Epoch 450, val loss: 0.69461590051651
Epoch 460, training loss: 0.03936506807804108 = 0.03256963938474655 + 0.001 * 6.795427322387695
Epoch 460, val loss: 0.7040987014770508
Epoch 470, training loss: 0.03651667758822441 = 0.0297237541526556 + 0.001 * 6.792922496795654
Epoch 470, val loss: 0.7135870456695557
Epoch 480, training loss: 0.03402073308825493 = 0.027213923633098602 + 0.001 * 6.80681037902832
Epoch 480, val loss: 0.7232550382614136
Epoch 490, training loss: 0.0317840576171875 = 0.02499011904001236 + 0.001 * 6.79394006729126
Epoch 490, val loss: 0.7329220771789551
Epoch 500, training loss: 0.02980794943869114 = 0.023020027205348015 + 0.001 * 6.787922382354736
Epoch 500, val loss: 0.7425708770751953
Epoch 510, training loss: 0.028054770082235336 = 0.021266955882310867 + 0.001 * 6.787814140319824
Epoch 510, val loss: 0.7521030902862549
Epoch 520, training loss: 0.02649092674255371 = 0.019702255725860596 + 0.001 * 6.788669586181641
Epoch 520, val loss: 0.761520266532898
Epoch 530, training loss: 0.025082897394895554 = 0.018300045281648636 + 0.001 * 6.7828521728515625
Epoch 530, val loss: 0.7707490921020508
Epoch 540, training loss: 0.02381831407546997 = 0.017039945349097252 + 0.001 * 6.778368949890137
Epoch 540, val loss: 0.7798457741737366
Epoch 550, training loss: 0.022678768262267113 = 0.015905190259218216 + 0.001 * 6.7735772132873535
Epoch 550, val loss: 0.7887439727783203
Epoch 560, training loss: 0.02165002003312111 = 0.014879174530506134 + 0.001 * 6.770844459533691
Epoch 560, val loss: 0.7974880933761597
Epoch 570, training loss: 0.020746124908328056 = 0.013948229141533375 + 0.001 * 6.7978949546813965
Epoch 570, val loss: 0.8059926629066467
Epoch 580, training loss: 0.019870705902576447 = 0.013101477175951004 + 0.001 * 6.769228458404541
Epoch 580, val loss: 0.8143527507781982
Epoch 590, training loss: 0.019083470106124878 = 0.012329941615462303 + 0.001 * 6.753529071807861
Epoch 590, val loss: 0.8225358724594116
Epoch 600, training loss: 0.01842094212770462 = 0.011625313200056553 + 0.001 * 6.795628547668457
Epoch 600, val loss: 0.8305256962776184
Epoch 610, training loss: 0.017736632376909256 = 0.010980338789522648 + 0.001 * 6.756293773651123
Epoch 610, val loss: 0.838370144367218
Epoch 620, training loss: 0.01712987758219242 = 0.010388407856225967 + 0.001 * 6.741469860076904
Epoch 620, val loss: 0.8460037112236023
Epoch 630, training loss: 0.01658935286104679 = 0.009844329208135605 + 0.001 * 6.745022773742676
Epoch 630, val loss: 0.8534789085388184
Epoch 640, training loss: 0.01612110063433647 = 0.009343033656477928 + 0.001 * 6.778066158294678
Epoch 640, val loss: 0.8607891798019409
Epoch 650, training loss: 0.015618978068232536 = 0.008880364708602428 + 0.001 * 6.738613605499268
Epoch 650, val loss: 0.8679264187812805
Epoch 660, training loss: 0.01516927033662796 = 0.00845263246446848 + 0.001 * 6.716638088226318
Epoch 660, val loss: 0.8749329447746277
Epoch 670, training loss: 0.01478751190006733 = 0.008056331425905228 + 0.001 * 6.731180667877197
Epoch 670, val loss: 0.8817630410194397
Epoch 680, training loss: 0.014400110580027103 = 0.007688228040933609 + 0.001 * 6.7118821144104
Epoch 680, val loss: 0.8884434103965759
Epoch 690, training loss: 0.014049671590328217 = 0.007346202153712511 + 0.001 * 6.703469753265381
Epoch 690, val loss: 0.8950127363204956
Epoch 700, training loss: 0.013759873807430267 = 0.0070276446640491486 + 0.001 * 6.732229232788086
Epoch 700, val loss: 0.9014120101928711
Epoch 710, training loss: 0.013430202379822731 = 0.0067302691750228405 + 0.001 * 6.6999335289001465
Epoch 710, val loss: 0.9076881408691406
Epoch 720, training loss: 0.013169365003705025 = 0.006452192086726427 + 0.001 * 6.717172145843506
Epoch 720, val loss: 0.9138510823249817
Epoch 730, training loss: 0.012889011763036251 = 0.0061921305023133755 + 0.001 * 6.69688081741333
Epoch 730, val loss: 0.9198616743087769
Epoch 740, training loss: 0.012655306607484818 = 0.005948580801486969 + 0.001 * 6.706725597381592
Epoch 740, val loss: 0.925778865814209
Epoch 750, training loss: 0.012414115481078625 = 0.005719908978790045 + 0.001 * 6.694206237792969
Epoch 750, val loss: 0.9315453767776489
Epoch 760, training loss: 0.012213991954922676 = 0.005505167413502932 + 0.001 * 6.7088236808776855
Epoch 760, val loss: 0.9371874332427979
Epoch 770, training loss: 0.011988438665866852 = 0.005303140264004469 + 0.001 * 6.685297966003418
Epoch 770, val loss: 0.9427423477172852
Epoch 780, training loss: 0.011794451624155045 = 0.005112933926284313 + 0.001 * 6.681517124176025
Epoch 780, val loss: 0.948168158531189
Epoch 790, training loss: 0.011624576523900032 = 0.004933610092848539 + 0.001 * 6.6909661293029785
Epoch 790, val loss: 0.9535105228424072
Epoch 800, training loss: 0.011456281878054142 = 0.004764454439282417 + 0.001 * 6.691827297210693
Epoch 800, val loss: 0.9587270021438599
Epoch 810, training loss: 0.011276503093540668 = 0.004604635760188103 + 0.001 * 6.6718668937683105
Epoch 810, val loss: 0.9638531804084778
Epoch 820, training loss: 0.011121487244963646 = 0.004453044850379229 + 0.001 * 6.668442726135254
Epoch 820, val loss: 0.9688506126403809
Epoch 830, training loss: 0.011024555191397667 = 0.0043093119747936726 + 0.001 * 6.715242385864258
Epoch 830, val loss: 0.973800003528595
Epoch 840, training loss: 0.010841922834515572 = 0.004173115827143192 + 0.001 * 6.668806076049805
Epoch 840, val loss: 0.9786245822906494
Epoch 850, training loss: 0.010714476928114891 = 0.004043934866786003 + 0.001 * 6.670541763305664
Epoch 850, val loss: 0.9833528995513916
Epoch 860, training loss: 0.010594874620437622 = 0.003921465948224068 + 0.001 * 6.6734089851379395
Epoch 860, val loss: 0.9880092144012451
Epoch 870, training loss: 0.010464092716574669 = 0.003805494634434581 + 0.001 * 6.658597946166992
Epoch 870, val loss: 0.9926173686981201
Epoch 880, training loss: 0.010358388535678387 = 0.003695301478728652 + 0.001 * 6.663086891174316
Epoch 880, val loss: 0.9971117973327637
Epoch 890, training loss: 0.01025754027068615 = 0.0035904357209801674 + 0.001 * 6.667104244232178
Epoch 890, val loss: 1.0015215873718262
Epoch 900, training loss: 0.010148021392524242 = 0.003490610048174858 + 0.001 * 6.657411098480225
Epoch 900, val loss: 1.005878210067749
Epoch 910, training loss: 0.010051894001662731 = 0.0033954947721213102 + 0.001 * 6.656398773193359
Epoch 910, val loss: 1.0101354122161865
Epoch 920, training loss: 0.009962853044271469 = 0.0033048116602003574 + 0.001 * 6.658041000366211
Epoch 920, val loss: 1.0143474340438843
Epoch 930, training loss: 0.009868372231721878 = 0.0032183006405830383 + 0.001 * 6.650070667266846
Epoch 930, val loss: 1.018448829650879
Epoch 940, training loss: 0.00980808213353157 = 0.0031357051339000463 + 0.001 * 6.67237663269043
Epoch 940, val loss: 1.0224835872650146
Epoch 950, training loss: 0.009720000438392162 = 0.0030567909125238657 + 0.001 * 6.663208961486816
Epoch 950, val loss: 1.0264554023742676
Epoch 960, training loss: 0.009636389091610909 = 0.002981300465762615 + 0.001 * 6.655088424682617
Epoch 960, val loss: 1.0303542613983154
Epoch 970, training loss: 0.009559080936014652 = 0.002909074304625392 + 0.001 * 6.650006294250488
Epoch 970, val loss: 1.0341798067092896
Epoch 980, training loss: 0.009489443153142929 = 0.0028399380389600992 + 0.001 * 6.649504661560059
Epoch 980, val loss: 1.0379496812820435
Epoch 990, training loss: 0.009423637762665749 = 0.0027736471965909004 + 0.001 * 6.649989604949951
Epoch 990, val loss: 1.0416423082351685
Epoch 1000, training loss: 0.009357727132737637 = 0.0027101270388811827 + 0.001 * 6.647599697113037
Epoch 1000, val loss: 1.0453048944473267
Epoch 1010, training loss: 0.009293047711253166 = 0.0026491908356547356 + 0.001 * 6.643856525421143
Epoch 1010, val loss: 1.048857569694519
Epoch 1020, training loss: 0.009230107069015503 = 0.0025907063391059637 + 0.001 * 6.639400005340576
Epoch 1020, val loss: 1.0523945093154907
Epoch 1030, training loss: 0.009195691905915737 = 0.0025345294270664454 + 0.001 * 6.66116189956665
Epoch 1030, val loss: 1.0558589696884155
Epoch 1040, training loss: 0.009126679971814156 = 0.002480573719367385 + 0.001 * 6.646105766296387
Epoch 1040, val loss: 1.0592505931854248
Epoch 1050, training loss: 0.009061265736818314 = 0.002428672043606639 + 0.001 * 6.632593154907227
Epoch 1050, val loss: 1.0626020431518555
Epoch 1060, training loss: 0.009034935384988785 = 0.0023787394165992737 + 0.001 * 6.656196117401123
Epoch 1060, val loss: 1.065895915031433
Epoch 1070, training loss: 0.008968818932771683 = 0.0023306971415877342 + 0.001 * 6.638122081756592
Epoch 1070, val loss: 1.0691395998001099
Epoch 1080, training loss: 0.008926951326429844 = 0.002284427173435688 + 0.001 * 6.642523765563965
Epoch 1080, val loss: 1.0723315477371216
Epoch 1090, training loss: 0.00887434370815754 = 0.0022398866713047028 + 0.001 * 6.634457111358643
Epoch 1090, val loss: 1.0754756927490234
Epoch 1100, training loss: 0.008838933892548084 = 0.0021969641093164682 + 0.001 * 6.641969680786133
Epoch 1100, val loss: 1.0785906314849854
Epoch 1110, training loss: 0.008805968798696995 = 0.002155556110665202 + 0.001 * 6.650412082672119
Epoch 1110, val loss: 1.0816113948822021
Epoch 1120, training loss: 0.008753657341003418 = 0.0021156324073672295 + 0.001 * 6.63802433013916
Epoch 1120, val loss: 1.0846285820007324
Epoch 1130, training loss: 0.008704548701643944 = 0.0020770973060280085 + 0.001 * 6.627451419830322
Epoch 1130, val loss: 1.087588906288147
Epoch 1140, training loss: 0.008682413026690483 = 0.002039917279034853 + 0.001 * 6.642496109008789
Epoch 1140, val loss: 1.0904791355133057
Epoch 1150, training loss: 0.008636401034891605 = 0.0020040019880980253 + 0.001 * 6.63239860534668
Epoch 1150, val loss: 1.093345046043396
Epoch 1160, training loss: 0.008599492721259594 = 0.0019693225622177124 + 0.001 * 6.630169868469238
Epoch 1160, val loss: 1.0961785316467285
Epoch 1170, training loss: 0.008579675108194351 = 0.0019357940182089806 + 0.001 * 6.643881320953369
Epoch 1170, val loss: 1.0989248752593994
Epoch 1180, training loss: 0.008529643528163433 = 0.001903369091451168 + 0.001 * 6.626274108886719
Epoch 1180, val loss: 1.1016634702682495
Epoch 1190, training loss: 0.008513497188687325 = 0.0018720264779403806 + 0.001 * 6.641470432281494
Epoch 1190, val loss: 1.1043510437011719
Epoch 1200, training loss: 0.008463890291750431 = 0.0018416892271488905 + 0.001 * 6.6222004890441895
Epoch 1200, val loss: 1.107013463973999
Epoch 1210, training loss: 0.008434034883975983 = 0.0018123358022421598 + 0.001 * 6.621698379516602
Epoch 1210, val loss: 1.1096371412277222
Epoch 1220, training loss: 0.008437520824372768 = 0.001783901359885931 + 0.001 * 6.653619289398193
Epoch 1220, val loss: 1.1121987104415894
Epoch 1230, training loss: 0.008378217928111553 = 0.0017563762376084924 + 0.001 * 6.6218414306640625
Epoch 1230, val loss: 1.1147335767745972
Epoch 1240, training loss: 0.008353161625564098 = 0.001729703275486827 + 0.001 * 6.623457908630371
Epoch 1240, val loss: 1.1172550916671753
Epoch 1250, training loss: 0.008328696712851524 = 0.0017038470832630992 + 0.001 * 6.624849319458008
Epoch 1250, val loss: 1.1197184324264526
Epoch 1260, training loss: 0.008292644284665585 = 0.0016787793720141053 + 0.001 * 6.613864898681641
Epoch 1260, val loss: 1.1221333742141724
Epoch 1270, training loss: 0.00829572044312954 = 0.001654485589824617 + 0.001 * 6.641234397888184
Epoch 1270, val loss: 1.124531626701355
Epoch 1280, training loss: 0.008244742639362812 = 0.0016308915801346302 + 0.001 * 6.6138505935668945
Epoch 1280, val loss: 1.1269062757492065
Epoch 1290, training loss: 0.008239985443651676 = 0.001607971265912056 + 0.001 * 6.63201379776001
Epoch 1290, val loss: 1.1292099952697754
Epoch 1300, training loss: 0.008190480060875416 = 0.0015857785474509 + 0.001 * 6.604701042175293
Epoch 1300, val loss: 1.1315348148345947
Epoch 1310, training loss: 0.008196434937417507 = 0.0015642059734091163 + 0.001 * 6.632228374481201
Epoch 1310, val loss: 1.133772373199463
Epoch 1320, training loss: 0.00815243273973465 = 0.0015432615764439106 + 0.001 * 6.609170436859131
Epoch 1320, val loss: 1.1360118389129639
Epoch 1330, training loss: 0.008146052248775959 = 0.001522903679870069 + 0.001 * 6.623147964477539
Epoch 1330, val loss: 1.1382334232330322
Epoch 1340, training loss: 0.008106384426355362 = 0.0015031076036393642 + 0.001 * 6.60327672958374
Epoch 1340, val loss: 1.1403666734695435
Epoch 1350, training loss: 0.008099783211946487 = 0.001483852043747902 + 0.001 * 6.615931034088135
Epoch 1350, val loss: 1.1425279378890991
Epoch 1360, training loss: 0.008067832328379154 = 0.001465093926526606 + 0.001 * 6.602737903594971
Epoch 1360, val loss: 1.1446170806884766
Epoch 1370, training loss: 0.008053380064666271 = 0.0014468504814431071 + 0.001 * 6.606529235839844
Epoch 1370, val loss: 1.146669864654541
Epoch 1380, training loss: 0.008042095229029655 = 0.0014290978433564305 + 0.001 * 6.612996578216553
Epoch 1380, val loss: 1.1487352848052979
Epoch 1390, training loss: 0.008007771335542202 = 0.0014118303079158068 + 0.001 * 6.595941066741943
Epoch 1390, val loss: 1.1507415771484375
Epoch 1400, training loss: 0.008038201369345188 = 0.0013950311113148928 + 0.001 * 6.643170356750488
Epoch 1400, val loss: 1.152719497680664
Epoch 1410, training loss: 0.007977858185768127 = 0.0013786721974611282 + 0.001 * 6.599185466766357
Epoch 1410, val loss: 1.154708981513977
Epoch 1420, training loss: 0.007962431758642197 = 0.0013627276057377458 + 0.001 * 6.599703788757324
Epoch 1420, val loss: 1.1566115617752075
Epoch 1430, training loss: 0.007955876179039478 = 0.001347220386378467 + 0.001 * 6.6086554527282715
Epoch 1430, val loss: 1.1585503816604614
Epoch 1440, training loss: 0.007929368875920773 = 0.0013321025762706995 + 0.001 * 6.59726619720459
Epoch 1440, val loss: 1.1604381799697876
Epoch 1450, training loss: 0.007931679487228394 = 0.0013173776678740978 + 0.001 * 6.6143012046813965
Epoch 1450, val loss: 1.1623111963272095
Epoch 1460, training loss: 0.007896363735198975 = 0.0013030313421040773 + 0.001 * 6.593332290649414
Epoch 1460, val loss: 1.164162516593933
Epoch 1470, training loss: 0.007874357514083385 = 0.001289044856093824 + 0.001 * 6.585312366485596
Epoch 1470, val loss: 1.165987253189087
Epoch 1480, training loss: 0.007865140214562416 = 0.001275406451895833 + 0.001 * 6.589733600616455
Epoch 1480, val loss: 1.1677824258804321
Epoch 1490, training loss: 0.007847482338547707 = 0.0012621089117601514 + 0.001 * 6.585373401641846
Epoch 1490, val loss: 1.1695598363876343
Epoch 1500, training loss: 0.007861640304327011 = 0.0012491459492594004 + 0.001 * 6.612494468688965
Epoch 1500, val loss: 1.1713428497314453
Epoch 1510, training loss: 0.007840228267014027 = 0.0012364116264507174 + 0.001 * 6.603816032409668
Epoch 1510, val loss: 1.1730681657791138
Epoch 1520, training loss: 0.007825618609786034 = 0.0012239180505275726 + 0.001 * 6.601700782775879
Epoch 1520, val loss: 1.1748048067092896
Epoch 1530, training loss: 0.007804966066032648 = 0.0012116809375584126 + 0.001 * 6.593284606933594
Epoch 1530, val loss: 1.1764885187149048
Epoch 1540, training loss: 0.007805063389241695 = 0.0011997317196801305 + 0.001 * 6.6053314208984375
Epoch 1540, val loss: 1.178188681602478
Epoch 1550, training loss: 0.007766305468976498 = 0.0011880702804774046 + 0.001 * 6.578234672546387
Epoch 1550, val loss: 1.1798663139343262
Epoch 1560, training loss: 0.0078070759773254395 = 0.001176700578071177 + 0.001 * 6.630375385284424
Epoch 1560, val loss: 1.1814781427383423
Epoch 1570, training loss: 0.007752703037112951 = 0.0011654869886115193 + 0.001 * 6.587215900421143
Epoch 1570, val loss: 1.183162808418274
Epoch 1580, training loss: 0.00775082828477025 = 0.0011544589651748538 + 0.001 * 6.596368789672852
Epoch 1580, val loss: 1.1847529411315918
Epoch 1590, training loss: 0.007729551754891872 = 0.0011436922941356897 + 0.001 * 6.5858588218688965
Epoch 1590, val loss: 1.1863781213760376
Epoch 1600, training loss: 0.007717273198068142 = 0.0011328455293551087 + 0.001 * 6.584427356719971
Epoch 1600, val loss: 1.1880003213882446
Epoch 1610, training loss: 0.007701223250478506 = 0.0011224300833418965 + 0.001 * 6.578793048858643
Epoch 1610, val loss: 1.1895955801010132
Epoch 1620, training loss: 0.007684466429054737 = 0.0011123139411211014 + 0.001 * 6.572152137756348
Epoch 1620, val loss: 1.1911323070526123
Epoch 1630, training loss: 0.007676968351006508 = 0.0011026461143046618 + 0.001 * 6.57432222366333
Epoch 1630, val loss: 1.1926292181015015
Epoch 1640, training loss: 0.00766934035345912 = 0.0010932710720226169 + 0.001 * 6.576068878173828
Epoch 1640, val loss: 1.1941319704055786
Epoch 1650, training loss: 0.007669435348361731 = 0.0010841117473319173 + 0.001 * 6.585323333740234
Epoch 1650, val loss: 1.1956348419189453
Epoch 1660, training loss: 0.007647906430065632 = 0.0010751583613455296 + 0.001 * 6.572747707366943
Epoch 1660, val loss: 1.1970916986465454
Epoch 1670, training loss: 0.007640816271305084 = 0.001066414755769074 + 0.001 * 6.574401378631592
Epoch 1670, val loss: 1.1985375881195068
Epoch 1680, training loss: 0.00763264624401927 = 0.0010578724322840571 + 0.001 * 6.57477331161499
Epoch 1680, val loss: 1.1999506950378418
Epoch 1690, training loss: 0.007619350682944059 = 0.001049500540830195 + 0.001 * 6.569849967956543
Epoch 1690, val loss: 1.2014130353927612
Epoch 1700, training loss: 0.007617602124810219 = 0.0010413231793791056 + 0.001 * 6.576278209686279
Epoch 1700, val loss: 1.2027851343154907
Epoch 1710, training loss: 0.007597327698022127 = 0.001033314038068056 + 0.001 * 6.564013481140137
Epoch 1710, val loss: 1.2042031288146973
Epoch 1720, training loss: 0.007620364893227816 = 0.0010254746302962303 + 0.001 * 6.594890117645264
Epoch 1720, val loss: 1.2055697441101074
Epoch 1730, training loss: 0.0075871930457651615 = 0.001017788890749216 + 0.001 * 6.569403648376465
Epoch 1730, val loss: 1.2069398164749146
Epoch 1740, training loss: 0.007605851627886295 = 0.0010102755622938275 + 0.001 * 6.59557580947876
Epoch 1740, val loss: 1.2083295583724976
Epoch 1750, training loss: 0.007566235028207302 = 0.0010029138065874577 + 0.001 * 6.563321113586426
Epoch 1750, val loss: 1.2096436023712158
Epoch 1760, training loss: 0.007564850151538849 = 0.0009956926805898547 + 0.001 * 6.569157123565674
Epoch 1760, val loss: 1.2109373807907104
Epoch 1770, training loss: 0.007581834215670824 = 0.0009886178886517882 + 0.001 * 6.5932159423828125
Epoch 1770, val loss: 1.2122770547866821
Epoch 1780, training loss: 0.0075524915009737015 = 0.0009816893143579364 + 0.001 * 6.570801734924316
Epoch 1780, val loss: 1.2135882377624512
Epoch 1790, training loss: 0.007547006476670504 = 0.000974896945990622 + 0.001 * 6.572109222412109
Epoch 1790, val loss: 1.2148526906967163
Epoch 1800, training loss: 0.007528240326792002 = 0.0009682336822152138 + 0.001 * 6.560006141662598
Epoch 1800, val loss: 1.216172695159912
Epoch 1810, training loss: 0.007520070765167475 = 0.0009617168107070029 + 0.001 * 6.558353900909424
Epoch 1810, val loss: 1.217389464378357
Epoch 1820, training loss: 0.007505445275455713 = 0.000955308205448091 + 0.001 * 6.550136566162109
Epoch 1820, val loss: 1.2186477184295654
Epoch 1830, training loss: 0.007516346871852875 = 0.0009490380180068314 + 0.001 * 6.56730842590332
Epoch 1830, val loss: 1.2199203968048096
Epoch 1840, training loss: 0.007514230906963348 = 0.0009428858757019043 + 0.001 * 6.57134485244751
Epoch 1840, val loss: 1.2211369276046753
Epoch 1850, training loss: 0.007498982362449169 = 0.0009368528262712061 + 0.001 * 6.562129497528076
Epoch 1850, val loss: 1.2223460674285889
Epoch 1860, training loss: 0.007500246167182922 = 0.0009309412562288344 + 0.001 * 6.569304466247559
Epoch 1860, val loss: 1.2236099243164062
Epoch 1870, training loss: 0.007493123412132263 = 0.000925129686947912 + 0.001 * 6.5679931640625
Epoch 1870, val loss: 1.224753737449646
Epoch 1880, training loss: 0.0074721090495586395 = 0.0009194361628033221 + 0.001 * 6.552672863006592
Epoch 1880, val loss: 1.2259442806243896
Epoch 1890, training loss: 0.007466287352144718 = 0.0009138385066762567 + 0.001 * 6.552448749542236
Epoch 1890, val loss: 1.2271140813827515
Epoch 1900, training loss: 0.007456931751221418 = 0.0009083288605324924 + 0.001 * 6.54860258102417
Epoch 1900, val loss: 1.2282965183258057
Epoch 1910, training loss: 0.007468710653483868 = 0.0009029302163980901 + 0.001 * 6.565780162811279
Epoch 1910, val loss: 1.2294316291809082
Epoch 1920, training loss: 0.007467813789844513 = 0.0008976396638900042 + 0.001 * 6.570174217224121
Epoch 1920, val loss: 1.2306097745895386
Epoch 1930, training loss: 0.0074736010283231735 = 0.0008924611611291766 + 0.001 * 6.58113956451416
Epoch 1930, val loss: 1.2317246198654175
Epoch 1940, training loss: 0.0074445996433496475 = 0.0008873788174241781 + 0.001 * 6.557220935821533
Epoch 1940, val loss: 1.2328799962997437
Epoch 1950, training loss: 0.007432132493704557 = 0.000882389023900032 + 0.001 * 6.549743175506592
Epoch 1950, val loss: 1.2339690923690796
Epoch 1960, training loss: 0.007428164128214121 = 0.0008774657035246491 + 0.001 * 6.550698280334473
Epoch 1960, val loss: 1.2351009845733643
Epoch 1970, training loss: 0.007422459777444601 = 0.0008726349333301187 + 0.001 * 6.5498247146606445
Epoch 1970, val loss: 1.2361925840377808
Epoch 1980, training loss: 0.007434995844960213 = 0.0008678904268890619 + 0.001 * 6.567105293273926
Epoch 1980, val loss: 1.2372881174087524
Epoch 1990, training loss: 0.007408294826745987 = 0.0008632221142761409 + 0.001 * 6.545072555541992
Epoch 1990, val loss: 1.2383623123168945
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8519
Overall ASR: 0.7380
Flip ASR: 0.7022/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9473254680633545 = 1.9389516115188599 + 0.001 * 8.373838424682617
Epoch 0, val loss: 1.9343851804733276
Epoch 10, training loss: 1.9371318817138672 = 1.928758144378662 + 0.001 * 8.373735427856445
Epoch 10, val loss: 1.924692153930664
Epoch 20, training loss: 1.9242364168167114 = 1.9158629179000854 + 0.001 * 8.373468399047852
Epoch 20, val loss: 1.9120666980743408
Epoch 30, training loss: 1.9058184623718262 = 1.897445559501648 + 0.001 * 8.372903823852539
Epoch 30, val loss: 1.893914818763733
Epoch 40, training loss: 1.878710389137268 = 1.870338797569275 + 0.001 * 8.371545791625977
Epoch 40, val loss: 1.867567539215088
Epoch 50, training loss: 1.8421339988708496 = 1.8337669372558594 + 0.001 * 8.367066383361816
Epoch 50, val loss: 1.8340017795562744
Epoch 60, training loss: 1.8026509284973145 = 1.794304609298706 + 0.001 * 8.346297264099121
Epoch 60, val loss: 1.8016891479492188
Epoch 70, training loss: 1.7642147541046143 = 1.7559913396835327 + 0.001 * 8.223388671875
Epoch 70, val loss: 1.7706351280212402
Epoch 80, training loss: 1.7119444608688354 = 1.7041388750076294 + 0.001 * 7.805568695068359
Epoch 80, val loss: 1.7250049114227295
Epoch 90, training loss: 1.6410106420516968 = 1.633323073387146 + 0.001 * 7.687553882598877
Epoch 90, val loss: 1.6642223596572876
Epoch 100, training loss: 1.5533884763717651 = 1.5457720756530762 + 0.001 * 7.616377830505371
Epoch 100, val loss: 1.592483639717102
Epoch 110, training loss: 1.461946725845337 = 1.4544581174850464 + 0.001 * 7.488559246063232
Epoch 110, val loss: 1.517492651939392
Epoch 120, training loss: 1.3736181259155273 = 1.3663876056671143 + 0.001 * 7.2305450439453125
Epoch 120, val loss: 1.4485077857971191
Epoch 130, training loss: 1.2871037721633911 = 1.280007243156433 + 0.001 * 7.096536636352539
Epoch 130, val loss: 1.3821945190429688
Epoch 140, training loss: 1.1997601985931396 = 1.1927311420440674 + 0.001 * 7.029085636138916
Epoch 140, val loss: 1.317026972770691
Epoch 150, training loss: 1.1111841201782227 = 1.1041816473007202 + 0.001 * 7.002446174621582
Epoch 150, val loss: 1.2517715692520142
Epoch 160, training loss: 1.0217128992080688 = 1.01473069190979 + 0.001 * 6.982245922088623
Epoch 160, val loss: 1.1861969232559204
Epoch 170, training loss: 0.9324007034301758 = 0.9254412651062012 + 0.001 * 6.959451675415039
Epoch 170, val loss: 1.121230959892273
Epoch 180, training loss: 0.8445232510566711 = 0.8375827074050903 + 0.001 * 6.940541744232178
Epoch 180, val loss: 1.058122158050537
Epoch 190, training loss: 0.7595056891441345 = 0.7525839805603027 + 0.001 * 6.921725749969482
Epoch 190, val loss: 0.9983054995536804
Epoch 200, training loss: 0.6793444156646729 = 0.6724435687065125 + 0.001 * 6.900862693786621
Epoch 200, val loss: 0.9431937336921692
Epoch 210, training loss: 0.606139063835144 = 0.5992588996887207 + 0.001 * 6.880165100097656
Epoch 210, val loss: 0.8957476019859314
Epoch 220, training loss: 0.5412374138832092 = 0.5343714356422424 + 0.001 * 6.865973472595215
Epoch 220, val loss: 0.8573269844055176
Epoch 230, training loss: 0.4845848083496094 = 0.4777299463748932 + 0.001 * 6.8548665046691895
Epoch 230, val loss: 0.8286600112915039
Epoch 240, training loss: 0.43494531512260437 = 0.4280979335308075 + 0.001 * 6.847384452819824
Epoch 240, val loss: 0.8081931471824646
Epoch 250, training loss: 0.390787810087204 = 0.3839450776576996 + 0.001 * 6.842738628387451
Epoch 250, val loss: 0.7942454218864441
Epoch 260, training loss: 0.3507852852344513 = 0.34394571185112 + 0.001 * 6.839564800262451
Epoch 260, val loss: 0.7854201793670654
Epoch 270, training loss: 0.31412065029144287 = 0.3072836399078369 + 0.001 * 6.837023735046387
Epoch 270, val loss: 0.7806833982467651
Epoch 280, training loss: 0.28045156598091125 = 0.2736150026321411 + 0.001 * 6.836559295654297
Epoch 280, val loss: 0.779362678527832
Epoch 290, training loss: 0.2496756911277771 = 0.24284295737743378 + 0.001 * 6.832739353179932
Epoch 290, val loss: 0.781104564666748
Epoch 300, training loss: 0.22179487347602844 = 0.21496428549289703 + 0.001 * 6.830593585968018
Epoch 300, val loss: 0.7855629324913025
Epoch 310, training loss: 0.19678330421447754 = 0.18995504081249237 + 0.001 * 6.828263282775879
Epoch 310, val loss: 0.7925313115119934
Epoch 320, training loss: 0.1745668351650238 = 0.1677398681640625 + 0.001 * 6.826973915100098
Epoch 320, val loss: 0.8016008734703064
Epoch 330, training loss: 0.1549706906080246 = 0.14814694225788116 + 0.001 * 6.82374906539917
Epoch 330, val loss: 0.8122268915176392
Epoch 340, training loss: 0.13778649270534515 = 0.13096606731414795 + 0.001 * 6.820423603057861
Epoch 340, val loss: 0.8240377902984619
Epoch 350, training loss: 0.12277544289827347 = 0.11595848202705383 + 0.001 * 6.81696081161499
Epoch 350, val loss: 0.836718738079071
Epoch 360, training loss: 0.10969145596027374 = 0.10287795215845108 + 0.001 * 6.81350564956665
Epoch 360, val loss: 0.8499737977981567
Epoch 370, training loss: 0.09829192608594894 = 0.09148173034191132 + 0.001 * 6.810197353363037
Epoch 370, val loss: 0.8636033535003662
Epoch 380, training loss: 0.08836059272289276 = 0.08155179768800735 + 0.001 * 6.808796405792236
Epoch 380, val loss: 0.8774399757385254
Epoch 390, training loss: 0.0796932578086853 = 0.07289331406354904 + 0.001 * 6.79994010925293
Epoch 390, val loss: 0.8914836049079895
Epoch 400, training loss: 0.07212940603494644 = 0.06533318758010864 + 0.001 * 6.7962188720703125
Epoch 400, val loss: 0.9055601954460144
Epoch 410, training loss: 0.06551654636859894 = 0.058722540736198425 + 0.001 * 6.794007301330566
Epoch 410, val loss: 0.9195772409439087
Epoch 420, training loss: 0.05971885100007057 = 0.05293205380439758 + 0.001 * 6.786798477172852
Epoch 420, val loss: 0.9334694743156433
Epoch 430, training loss: 0.054630111902952194 = 0.047851573675870895 + 0.001 * 6.778539180755615
Epoch 430, val loss: 0.9471859335899353
Epoch 440, training loss: 0.05016084015369415 = 0.04338572919368744 + 0.001 * 6.77510929107666
Epoch 440, val loss: 0.960693895816803
Epoch 450, training loss: 0.046216368675231934 = 0.039450060576200485 + 0.001 * 6.766305923461914
Epoch 450, val loss: 0.9739575386047363
Epoch 460, training loss: 0.042735420167446136 = 0.03597373515367508 + 0.001 * 6.761686325073242
Epoch 460, val loss: 0.9869328141212463
Epoch 470, training loss: 0.039652708917856216 = 0.03289741650223732 + 0.001 * 6.75529146194458
Epoch 470, val loss: 0.9995999932289124
Epoch 480, training loss: 0.036922942847013474 = 0.030169475823640823 + 0.001 * 6.753467082977295
Epoch 480, val loss: 1.01186203956604
Epoch 490, training loss: 0.03449772670865059 = 0.027744220569729805 + 0.001 * 6.753504276275635
Epoch 490, val loss: 1.0238136053085327
Epoch 500, training loss: 0.03232961148023605 = 0.025582341477274895 + 0.001 * 6.747270107269287
Epoch 500, val loss: 1.0353953838348389
Epoch 510, training loss: 0.03039645217359066 = 0.023649953305721283 + 0.001 * 6.746499061584473
Epoch 510, val loss: 1.0466196537017822
Epoch 520, training loss: 0.028663240373134613 = 0.021917326375842094 + 0.001 * 6.745914459228516
Epoch 520, val loss: 1.057512640953064
Epoch 530, training loss: 0.027097919955849648 = 0.020356273278594017 + 0.001 * 6.741645812988281
Epoch 530, val loss: 1.068096399307251
Epoch 540, training loss: 0.025683049112558365 = 0.01894269697368145 + 0.001 * 6.740352153778076
Epoch 540, val loss: 1.0783969163894653
Epoch 550, training loss: 0.02440701238811016 = 0.017658712342381477 + 0.001 * 6.748300075531006
Epoch 550, val loss: 1.0884276628494263
Epoch 560, training loss: 0.02322997897863388 = 0.016490284353494644 + 0.001 * 6.739693641662598
Epoch 560, val loss: 1.0982023477554321
Epoch 570, training loss: 0.022163331508636475 = 0.015425654128193855 + 0.001 * 6.73767614364624
Epoch 570, val loss: 1.1076900959014893
Epoch 580, training loss: 0.0211959145963192 = 0.014454822055995464 + 0.001 * 6.741093158721924
Epoch 580, val loss: 1.1169184446334839
Epoch 590, training loss: 0.020306438207626343 = 0.013568246737122536 + 0.001 * 6.738192081451416
Epoch 590, val loss: 1.1258795261383057
Epoch 600, training loss: 0.019492752850055695 = 0.01275679748505354 + 0.001 * 6.735955715179443
Epoch 600, val loss: 1.1346161365509033
Epoch 610, training loss: 0.018749676644802094 = 0.012012006714940071 + 0.001 * 6.737670421600342
Epoch 610, val loss: 1.1431069374084473
Epoch 620, training loss: 0.018059883266687393 = 0.011326502077281475 + 0.001 * 6.7333807945251465
Epoch 620, val loss: 1.1513677835464478
Epoch 630, training loss: 0.01742778904736042 = 0.010693905875086784 + 0.001 * 6.733882904052734
Epoch 630, val loss: 1.159446358680725
Epoch 640, training loss: 0.016840776428580284 = 0.01010895799845457 + 0.001 * 6.731817722320557
Epoch 640, val loss: 1.1673839092254639
Epoch 650, training loss: 0.016298431903123856 = 0.009567223489284515 + 0.001 * 6.731208324432373
Epoch 650, val loss: 1.1751066446304321
Epoch 660, training loss: 0.01579626090824604 = 0.00906500592827797 + 0.001 * 6.7312541007995605
Epoch 660, val loss: 1.1826539039611816
Epoch 670, training loss: 0.015335570089519024 = 0.008599000051617622 + 0.001 * 6.736569881439209
Epoch 670, val loss: 1.190032958984375
Epoch 680, training loss: 0.014898359775543213 = 0.008166435174643993 + 0.001 * 6.731924057006836
Epoch 680, val loss: 1.1972558498382568
Epoch 690, training loss: 0.014492927119135857 = 0.007764631882309914 + 0.001 * 6.728294849395752
Epoch 690, val loss: 1.2043272256851196
Epoch 700, training loss: 0.01411668211221695 = 0.007391002029180527 + 0.001 * 6.725679397583008
Epoch 700, val loss: 1.2112243175506592
Epoch 710, training loss: 0.013768577948212624 = 0.007043292745947838 + 0.001 * 6.725284576416016
Epoch 710, val loss: 1.2179721593856812
Epoch 720, training loss: 0.01346597820520401 = 0.006719400640577078 + 0.001 * 6.746577739715576
Epoch 720, val loss: 1.2245368957519531
Epoch 730, training loss: 0.013143064454197884 = 0.0064177727326750755 + 0.001 * 6.725291728973389
Epoch 730, val loss: 1.231033205986023
Epoch 740, training loss: 0.01286032423377037 = 0.006136365234851837 + 0.001 * 6.723958492279053
Epoch 740, val loss: 1.2374674081802368
Epoch 750, training loss: 0.01259433664381504 = 0.005873461253941059 + 0.001 * 6.720875263214111
Epoch 750, val loss: 1.243619441986084
Epoch 760, training loss: 0.012349079363048077 = 0.00562762375921011 + 0.001 * 6.721455097198486
Epoch 760, val loss: 1.2496896982192993
Epoch 770, training loss: 0.012116390280425549 = 0.005397456232458353 + 0.001 * 6.718933582305908
Epoch 770, val loss: 1.2555965185165405
Epoch 780, training loss: 0.011905360966920853 = 0.005181727930903435 + 0.001 * 6.723633289337158
Epoch 780, val loss: 1.2613887786865234
Epoch 790, training loss: 0.011709621176123619 = 0.004979289136826992 + 0.001 * 6.7303314208984375
Epoch 790, val loss: 1.2670866250991821
Epoch 800, training loss: 0.011507452465593815 = 0.004789103288203478 + 0.001 * 6.718348979949951
Epoch 800, val loss: 1.2726643085479736
Epoch 810, training loss: 0.01132611557841301 = 0.004610222298651934 + 0.001 * 6.715892791748047
Epoch 810, val loss: 1.278103232383728
Epoch 820, training loss: 0.011163569055497646 = 0.004441773984581232 + 0.001 * 6.721794605255127
Epoch 820, val loss: 1.2834373712539673
Epoch 830, training loss: 0.011002871207892895 = 0.004282986279577017 + 0.001 * 6.719884395599365
Epoch 830, val loss: 1.288666844367981
Epoch 840, training loss: 0.010851766914129257 = 0.004133192356675863 + 0.001 * 6.718573570251465
Epoch 840, val loss: 1.2938652038574219
Epoch 850, training loss: 0.010702915489673615 = 0.003991684410721064 + 0.001 * 6.711230278015137
Epoch 850, val loss: 1.298888087272644
Epoch 860, training loss: 0.010567596182227135 = 0.003857928328216076 + 0.001 * 6.709668159484863
Epoch 860, val loss: 1.3038218021392822
Epoch 870, training loss: 0.01046440564095974 = 0.003731376491487026 + 0.001 * 6.733028411865234
Epoch 870, val loss: 1.3086298704147339
Epoch 880, training loss: 0.01031964085996151 = 0.003611494554206729 + 0.001 * 6.708146095275879
Epoch 880, val loss: 1.313387155532837
Epoch 890, training loss: 0.01020965725183487 = 0.0034978464245796204 + 0.001 * 6.711810111999512
Epoch 890, val loss: 1.3180705308914185
Epoch 900, training loss: 0.010095520876348019 = 0.00339004909619689 + 0.001 * 6.705471515655518
Epoch 900, val loss: 1.322589635848999
Epoch 910, training loss: 0.009994665160775185 = 0.003287671599537134 + 0.001 * 6.706993103027344
Epoch 910, val loss: 1.3270723819732666
Epoch 920, training loss: 0.009893534705042839 = 0.003190348856151104 + 0.001 * 6.70318603515625
Epoch 920, val loss: 1.3315086364746094
Epoch 930, training loss: 0.00981127843260765 = 0.0030978070572018623 + 0.001 * 6.713470935821533
Epoch 930, val loss: 1.3358412981033325
Epoch 940, training loss: 0.009720721282064915 = 0.0030097102280706167 + 0.001 * 6.711010456085205
Epoch 940, val loss: 1.3400999307632446
Epoch 950, training loss: 0.009628703817725182 = 0.0029257938731461763 + 0.001 * 6.702909469604492
Epoch 950, val loss: 1.344323754310608
Epoch 960, training loss: 0.009542977437376976 = 0.0028457967564463615 + 0.001 * 6.69718074798584
Epoch 960, val loss: 1.3484529256820679
Epoch 970, training loss: 0.009478215128183365 = 0.0027694758027791977 + 0.001 * 6.708738327026367
Epoch 970, val loss: 1.3524971008300781
Epoch 980, training loss: 0.009396150708198547 = 0.0026966421864926815 + 0.001 * 6.6995086669921875
Epoch 980, val loss: 1.3564485311508179
Epoch 990, training loss: 0.009327523410320282 = 0.002627082634717226 + 0.001 * 6.700440883636475
Epoch 990, val loss: 1.360350251197815
Epoch 1000, training loss: 0.009249329566955566 = 0.0025605903938412666 + 0.001 * 6.68873929977417
Epoch 1000, val loss: 1.3641630411148071
Epoch 1010, training loss: 0.00919436477124691 = 0.0024969950318336487 + 0.001 * 6.6973700523376465
Epoch 1010, val loss: 1.3679275512695312
Epoch 1020, training loss: 0.009144815616309643 = 0.002436151495203376 + 0.001 * 6.708663463592529
Epoch 1020, val loss: 1.3716386556625366
Epoch 1030, training loss: 0.00907176360487938 = 0.002377898897975683 + 0.001 * 6.693864822387695
Epoch 1030, val loss: 1.375301480293274
Epoch 1040, training loss: 0.009010633453726768 = 0.002322080545127392 + 0.001 * 6.688551902770996
Epoch 1040, val loss: 1.378880262374878
Epoch 1050, training loss: 0.00895176362246275 = 0.002268559532240033 + 0.001 * 6.683204174041748
Epoch 1050, val loss: 1.3824046850204468
Epoch 1060, training loss: 0.008904904127120972 = 0.0022172187454998493 + 0.001 * 6.687685489654541
Epoch 1060, val loss: 1.3858801126480103
Epoch 1070, training loss: 0.008858121931552887 = 0.002167952246963978 + 0.001 * 6.690169334411621
Epoch 1070, val loss: 1.3893107175827026
Epoch 1080, training loss: 0.00881680566817522 = 0.0021206531673669815 + 0.001 * 6.696152210235596
Epoch 1080, val loss: 1.3926811218261719
Epoch 1090, training loss: 0.008749932050704956 = 0.002075209515169263 + 0.001 * 6.674722671508789
Epoch 1090, val loss: 1.3960031270980835
Epoch 1100, training loss: 0.008719101548194885 = 0.002031526993960142 + 0.001 * 6.687574863433838
Epoch 1100, val loss: 1.3992915153503418
Epoch 1110, training loss: 0.00866708718240261 = 0.001989528303965926 + 0.001 * 6.677558898925781
Epoch 1110, val loss: 1.4024903774261475
Epoch 1120, training loss: 0.008638525381684303 = 0.001949145458638668 + 0.001 * 6.689380168914795
Epoch 1120, val loss: 1.4056717157363892
Epoch 1130, training loss: 0.008591396734118462 = 0.0019102899823337793 + 0.001 * 6.681106090545654
Epoch 1130, val loss: 1.408797025680542
Epoch 1140, training loss: 0.008539489470422268 = 0.0018728709546849132 + 0.001 * 6.666618347167969
Epoch 1140, val loss: 1.4118759632110596
Epoch 1150, training loss: 0.008504083380103111 = 0.0018368472810834646 + 0.001 * 6.667235374450684
Epoch 1150, val loss: 1.4148805141448975
Epoch 1160, training loss: 0.008488558232784271 = 0.0018021437572315335 + 0.001 * 6.686413764953613
Epoch 1160, val loss: 1.4178622961044312
Epoch 1170, training loss: 0.008443544618785381 = 0.0017686693463474512 + 0.001 * 6.674875259399414
Epoch 1170, val loss: 1.4207841157913208
Epoch 1180, training loss: 0.008465813472867012 = 0.0017363963415846229 + 0.001 * 6.729416847229004
Epoch 1180, val loss: 1.4236918687820435
Epoch 1190, training loss: 0.008382325991988182 = 0.0017052689800038934 + 0.001 * 6.677056312561035
Epoch 1190, val loss: 1.4265035390853882
Epoch 1200, training loss: 0.008367595262825489 = 0.0016751984367147088 + 0.001 * 6.692396640777588
Epoch 1200, val loss: 1.4293196201324463
Epoch 1210, training loss: 0.008306916803121567 = 0.0016461971681565046 + 0.001 * 6.66071891784668
Epoch 1210, val loss: 1.4320366382598877
Epoch 1220, training loss: 0.008277267217636108 = 0.0016181592363864183 + 0.001 * 6.659107685089111
Epoch 1220, val loss: 1.4347025156021118
Epoch 1230, training loss: 0.008254359476268291 = 0.0015910527436062694 + 0.001 * 6.663306713104248
Epoch 1230, val loss: 1.4373550415039062
Epoch 1240, training loss: 0.008226200938224792 = 0.0015648449771106243 + 0.001 * 6.661355018615723
Epoch 1240, val loss: 1.4399960041046143
Epoch 1250, training loss: 0.008240537717938423 = 0.0015395672526210546 + 0.001 * 6.70097017288208
Epoch 1250, val loss: 1.442522644996643
Epoch 1260, training loss: 0.008162181824445724 = 0.0015151114203035831 + 0.001 * 6.647069454193115
Epoch 1260, val loss: 1.4450093507766724
Epoch 1270, training loss: 0.008132374845445156 = 0.0014914475614205003 + 0.001 * 6.640926837921143
Epoch 1270, val loss: 1.4475449323654175
Epoch 1280, training loss: 0.008136129938066006 = 0.0014685451751574874 + 0.001 * 6.667584419250488
Epoch 1280, val loss: 1.4499778747558594
Epoch 1290, training loss: 0.00810687243938446 = 0.0014463677071034908 + 0.001 * 6.66050386428833
Epoch 1290, val loss: 1.452393889427185
Epoch 1300, training loss: 0.008103555999696255 = 0.0014249025844037533 + 0.001 * 6.678653240203857
Epoch 1300, val loss: 1.4547845125198364
Epoch 1310, training loss: 0.008040249347686768 = 0.0014041168615221977 + 0.001 * 6.636131763458252
Epoch 1310, val loss: 1.4570947885513306
Epoch 1320, training loss: 0.008019724860787392 = 0.0013839676976203918 + 0.001 * 6.635756969451904
Epoch 1320, val loss: 1.4594011306762695
Epoch 1330, training loss: 0.008011446334421635 = 0.0013644603313878179 + 0.001 * 6.64698600769043
Epoch 1330, val loss: 1.4616420269012451
Epoch 1340, training loss: 0.007981665432453156 = 0.0013455450534820557 + 0.001 * 6.636120319366455
Epoch 1340, val loss: 1.4638861417770386
Epoch 1350, training loss: 0.00797238852828741 = 0.00132720114197582 + 0.001 * 6.6451873779296875
Epoch 1350, val loss: 1.4660576581954956
Epoch 1360, training loss: 0.00799096655100584 = 0.001309409737586975 + 0.001 * 6.681556701660156
Epoch 1360, val loss: 1.468247890472412
Epoch 1370, training loss: 0.007935757748782635 = 0.0012921688612550497 + 0.001 * 6.643589019775391
Epoch 1370, val loss: 1.4703593254089355
Epoch 1380, training loss: 0.007931312546133995 = 0.0012754047056660056 + 0.001 * 6.65590763092041
Epoch 1380, val loss: 1.4724628925323486
Epoch 1390, training loss: 0.007897654548287392 = 0.001259139273315668 + 0.001 * 6.638515472412109
Epoch 1390, val loss: 1.4744793176651
Epoch 1400, training loss: 0.007866600528359413 = 0.0012433481169864535 + 0.001 * 6.6232523918151855
Epoch 1400, val loss: 1.4764561653137207
Epoch 1410, training loss: 0.007869856432080269 = 0.0012280127266421914 + 0.001 * 6.641842842102051
Epoch 1410, val loss: 1.4784839153289795
Epoch 1420, training loss: 0.00786564126610756 = 0.0012131199473515153 + 0.001 * 6.652521133422852
Epoch 1420, val loss: 1.4803833961486816
Epoch 1430, training loss: 0.007853229530155659 = 0.0011986319441348314 + 0.001 * 6.65459680557251
Epoch 1430, val loss: 1.4823830127716064
Epoch 1440, training loss: 0.00781598687171936 = 0.001184582244604826 + 0.001 * 6.631404399871826
Epoch 1440, val loss: 1.4841992855072021
Epoch 1450, training loss: 0.007807028479874134 = 0.0011708972742781043 + 0.001 * 6.6361308097839355
Epoch 1450, val loss: 1.486069917678833
Epoch 1460, training loss: 0.007804978173226118 = 0.0011575808748602867 + 0.001 * 6.647397041320801
Epoch 1460, val loss: 1.4878966808319092
Epoch 1470, training loss: 0.007757001556456089 = 0.0011446474818512797 + 0.001 * 6.612353801727295
Epoch 1470, val loss: 1.4897037744522095
Epoch 1480, training loss: 0.007759043946862221 = 0.0011320384219288826 + 0.001 * 6.627005100250244
Epoch 1480, val loss: 1.4914638996124268
Epoch 1490, training loss: 0.007742071058601141 = 0.0011197779094800353 + 0.001 * 6.622292518615723
Epoch 1490, val loss: 1.4932390451431274
Epoch 1500, training loss: 0.007711800280958414 = 0.001107854419387877 + 0.001 * 6.603945732116699
Epoch 1500, val loss: 1.4949548244476318
Epoch 1510, training loss: 0.00780430668964982 = 0.0010962369851768017 + 0.001 * 6.708069324493408
Epoch 1510, val loss: 1.4966285228729248
Epoch 1520, training loss: 0.007718921173363924 = 0.0010849059326574206 + 0.001 * 6.63401460647583
Epoch 1520, val loss: 1.4983328580856323
Epoch 1530, training loss: 0.007683508098125458 = 0.0010738868732005358 + 0.001 * 6.609620571136475
Epoch 1530, val loss: 1.4999746084213257
Epoch 1540, training loss: 0.007703262846916914 = 0.00106314558070153 + 0.001 * 6.6401166915893555
Epoch 1540, val loss: 1.501588225364685
Epoch 1550, training loss: 0.007670936640352011 = 0.001052697072736919 + 0.001 * 6.618238925933838
Epoch 1550, val loss: 1.5031465291976929
Epoch 1560, training loss: 0.007685292977839708 = 0.0010424904758110642 + 0.001 * 6.6428022384643555
Epoch 1560, val loss: 1.5047597885131836
Epoch 1570, training loss: 0.007652739994227886 = 0.0010325642069801688 + 0.001 * 6.620175361633301
Epoch 1570, val loss: 1.506282925605774
Epoch 1580, training loss: 0.0076509988866746426 = 0.0010228767059743404 + 0.001 * 6.628121852874756
Epoch 1580, val loss: 1.5078226327896118
Epoch 1590, training loss: 0.007610318250954151 = 0.0010134077165275812 + 0.001 * 6.59691047668457
Epoch 1590, val loss: 1.5093042850494385
Epoch 1600, training loss: 0.007617959752678871 = 0.0010041814530268312 + 0.001 * 6.613778114318848
Epoch 1600, val loss: 1.510785698890686
Epoch 1610, training loss: 0.007592055480927229 = 0.0009951937245205045 + 0.001 * 6.596861362457275
Epoch 1610, val loss: 1.5122040510177612
Epoch 1620, training loss: 0.0076855202205479145 = 0.0009864033199846745 + 0.001 * 6.6991167068481445
Epoch 1620, val loss: 1.5135934352874756
Epoch 1630, training loss: 0.007582349702715874 = 0.0009778481908142567 + 0.001 * 6.604501247406006
Epoch 1630, val loss: 1.5150291919708252
Epoch 1640, training loss: 0.007558256387710571 = 0.0009694785112515092 + 0.001 * 6.588777542114258
Epoch 1640, val loss: 1.5164363384246826
Epoch 1650, training loss: 0.007554791402071714 = 0.0009612939902581275 + 0.001 * 6.593497276306152
Epoch 1650, val loss: 1.517755150794983
Epoch 1660, training loss: 0.007552257739007473 = 0.0009532948606647551 + 0.001 * 6.598962783813477
Epoch 1660, val loss: 1.5191172361373901
Epoch 1670, training loss: 0.0075617297552526 = 0.0009454917744733393 + 0.001 * 6.616237640380859
Epoch 1670, val loss: 1.520423173904419
Epoch 1680, training loss: 0.0075446488335728645 = 0.0009378576069138944 + 0.001 * 6.606791019439697
Epoch 1680, val loss: 1.521741271018982
Epoch 1690, training loss: 0.00755325285717845 = 0.0009304158156737685 + 0.001 * 6.622836589813232
Epoch 1690, val loss: 1.5229915380477905
Epoch 1700, training loss: 0.00752472598105669 = 0.0009231390431523323 + 0.001 * 6.601586818695068
Epoch 1700, val loss: 1.5243096351623535
Epoch 1710, training loss: 0.007539886515587568 = 0.0009160069166682661 + 0.001 * 6.623879432678223
Epoch 1710, val loss: 1.5255136489868164
Epoch 1720, training loss: 0.007495645433664322 = 0.0009090200182981789 + 0.001 * 6.586625099182129
Epoch 1720, val loss: 1.5267413854599
Epoch 1730, training loss: 0.007524719461798668 = 0.0009022149606607854 + 0.001 * 6.622504234313965
Epoch 1730, val loss: 1.5279852151870728
Epoch 1740, training loss: 0.007485967129468918 = 0.0008955932571552694 + 0.001 * 6.590373516082764
Epoch 1740, val loss: 1.5290998220443726
Epoch 1750, training loss: 0.007474498823285103 = 0.0008890407625585794 + 0.001 * 6.585457801818848
Epoch 1750, val loss: 1.5302728414535522
Epoch 1760, training loss: 0.007452524267137051 = 0.0008826239500194788 + 0.001 * 6.569899559020996
Epoch 1760, val loss: 1.5314429998397827
Epoch 1770, training loss: 0.007461105473339558 = 0.0008763780351728201 + 0.001 * 6.584726810455322
Epoch 1770, val loss: 1.5325802564620972
Epoch 1780, training loss: 0.007470554672181606 = 0.0008702808408997953 + 0.001 * 6.600273609161377
Epoch 1780, val loss: 1.5336427688598633
Epoch 1790, training loss: 0.007469560019671917 = 0.0008642645552754402 + 0.001 * 6.605295181274414
Epoch 1790, val loss: 1.534779667854309
Epoch 1800, training loss: 0.0074382200837135315 = 0.0008583706221543252 + 0.001 * 6.5798492431640625
Epoch 1800, val loss: 1.5358132123947144
Epoch 1810, training loss: 0.007419615983963013 = 0.0008526353631168604 + 0.001 * 6.566980361938477
Epoch 1810, val loss: 1.5369268655776978
Epoch 1820, training loss: 0.007414740510284901 = 0.0008470192551612854 + 0.001 * 6.567720890045166
Epoch 1820, val loss: 1.537964940071106
Epoch 1830, training loss: 0.007421869318932295 = 0.0008414766052737832 + 0.001 * 6.580392360687256
Epoch 1830, val loss: 1.5390312671661377
Epoch 1840, training loss: 0.007437488995492458 = 0.0008360452484339476 + 0.001 * 6.601443290710449
Epoch 1840, val loss: 1.5401060581207275
Epoch 1850, training loss: 0.007409309037029743 = 0.0008307289099320769 + 0.001 * 6.578579902648926
Epoch 1850, val loss: 1.541092038154602
Epoch 1860, training loss: 0.007396210916340351 = 0.0008255224674940109 + 0.001 * 6.570688247680664
Epoch 1860, val loss: 1.542129397392273
Epoch 1870, training loss: 0.007383125368505716 = 0.0008204270852729678 + 0.001 * 6.562697887420654
Epoch 1870, val loss: 1.5430866479873657
Epoch 1880, training loss: 0.007403400726616383 = 0.0008153984672389925 + 0.001 * 6.588001728057861
Epoch 1880, val loss: 1.544066309928894
Epoch 1890, training loss: 0.007370101287961006 = 0.0008105150191113353 + 0.001 * 6.559586048126221
Epoch 1890, val loss: 1.5450307130813599
Epoch 1900, training loss: 0.007386345416307449 = 0.0008057065424509346 + 0.001 * 6.580638408660889
Epoch 1900, val loss: 1.545951247215271
Epoch 1910, training loss: 0.007361007388681173 = 0.0008009383454918861 + 0.001 * 6.560068607330322
Epoch 1910, val loss: 1.5468792915344238
Epoch 1920, training loss: 0.007354899775236845 = 0.0007962915115058422 + 0.001 * 6.558608055114746
Epoch 1920, val loss: 1.547800064086914
Epoch 1930, training loss: 0.007365021388977766 = 0.0007917488110251725 + 0.001 * 6.573272228240967
Epoch 1930, val loss: 1.548699975013733
Epoch 1940, training loss: 0.007363153621554375 = 0.000787276541814208 + 0.001 * 6.575876712799072
Epoch 1940, val loss: 1.5495927333831787
Epoch 1950, training loss: 0.0073411776684224606 = 0.0007828910020180047 + 0.001 * 6.558286190032959
Epoch 1950, val loss: 1.550454020500183
Epoch 1960, training loss: 0.007339094765484333 = 0.0007785878260619938 + 0.001 * 6.560506343841553
Epoch 1960, val loss: 1.551275610923767
Epoch 1970, training loss: 0.007346675731241703 = 0.0007743361056782305 + 0.001 * 6.5723395347595215
Epoch 1970, val loss: 1.5521005392074585
Epoch 1980, training loss: 0.007322754245251417 = 0.0007701694848947227 + 0.001 * 6.552584648132324
Epoch 1980, val loss: 1.5529680252075195
Epoch 1990, training loss: 0.007310924585908651 = 0.0007660906994715333 + 0.001 * 6.544833660125732
Epoch 1990, val loss: 1.5537992715835571
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7852
Overall ASR: 0.8155
Flip ASR: 0.7778/225 nodes
The final ASR:0.73309, 0.06938, Accuracy:0.81358, 0.02810
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9468])
updated graph: torch.Size([2, 10500])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97909, 0.00348, Accuracy:0.83457, 0.00924
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.968195915222168 = 1.9598220586776733 + 0.001 * 8.373822212219238
Epoch 0, val loss: 1.9573873281478882
Epoch 10, training loss: 1.9574098587036133 = 1.9490361213684082 + 0.001 * 8.37369155883789
Epoch 10, val loss: 1.9472880363464355
Epoch 20, training loss: 1.944006323814392 = 1.9356329441070557 + 0.001 * 8.373337745666504
Epoch 20, val loss: 1.9343490600585938
Epoch 30, training loss: 1.9251534938812256 = 1.916780948638916 + 0.001 * 8.372550964355469
Epoch 30, val loss: 1.9159290790557861
Epoch 40, training loss: 1.897187352180481 = 1.8888167142868042 + 0.001 * 8.370692253112793
Epoch 40, val loss: 1.8889533281326294
Epoch 50, training loss: 1.8574912548065186 = 1.8491257429122925 + 0.001 * 8.36555004119873
Epoch 50, val loss: 1.8522673845291138
Epoch 60, training loss: 1.8108378648757935 = 1.8024909496307373 + 0.001 * 8.346930503845215
Epoch 60, val loss: 1.8128607273101807
Epoch 70, training loss: 1.7688935995101929 = 1.7606335878372192 + 0.001 * 8.25997543334961
Epoch 70, val loss: 1.7789359092712402
Epoch 80, training loss: 1.7207293510437012 = 1.7128374576568604 + 0.001 * 7.891951560974121
Epoch 80, val loss: 1.7341887950897217
Epoch 90, training loss: 1.654712200164795 = 1.6470882892608643 + 0.001 * 7.62386417388916
Epoch 90, val loss: 1.6741870641708374
Epoch 100, training loss: 1.5685197114944458 = 1.561069369316101 + 0.001 * 7.450294017791748
Epoch 100, val loss: 1.6007806062698364
Epoch 110, training loss: 1.4673889875411987 = 1.4600704908370972 + 0.001 * 7.318517684936523
Epoch 110, val loss: 1.5168222188949585
Epoch 120, training loss: 1.3641343116760254 = 1.3569080829620361 + 0.001 * 7.226193904876709
Epoch 120, val loss: 1.4324743747711182
Epoch 130, training loss: 1.2665331363677979 = 1.2593680620193481 + 0.001 * 7.1651153564453125
Epoch 130, val loss: 1.3551639318466187
Epoch 140, training loss: 1.1741528511047363 = 1.1670814752578735 + 0.001 * 7.071344375610352
Epoch 140, val loss: 1.2834537029266357
Epoch 150, training loss: 1.0840845108032227 = 1.0770816802978516 + 0.001 * 7.002782821655273
Epoch 150, val loss: 1.2138947248458862
Epoch 160, training loss: 0.9933516383171082 = 0.9863689541816711 + 0.001 * 6.982673168182373
Epoch 160, val loss: 1.1432830095291138
Epoch 170, training loss: 0.9014836549758911 = 0.8945119380950928 + 0.001 * 6.971734523773193
Epoch 170, val loss: 1.0711673498153687
Epoch 180, training loss: 0.8110693693161011 = 0.8041062355041504 + 0.001 * 6.963141918182373
Epoch 180, val loss: 1.0002837181091309
Epoch 190, training loss: 0.7256928086280823 = 0.7187374234199524 + 0.001 * 6.9553680419921875
Epoch 190, val loss: 0.9345613121986389
Epoch 200, training loss: 0.6483005881309509 = 0.6413535475730896 + 0.001 * 6.9470601081848145
Epoch 200, val loss: 0.8774926662445068
Epoch 210, training loss: 0.5801408886909485 = 0.5732027888298035 + 0.001 * 6.93812370300293
Epoch 210, val loss: 0.8304570913314819
Epoch 220, training loss: 0.5211517214775085 = 0.5142236351966858 + 0.001 * 6.928073883056641
Epoch 220, val loss: 0.7934544086456299
Epoch 230, training loss: 0.4701147675514221 = 0.46319887042045593 + 0.001 * 6.915908336639404
Epoch 230, val loss: 0.7652341723442078
Epoch 240, training loss: 0.42498305439949036 = 0.41808268427848816 + 0.001 * 6.900364875793457
Epoch 240, val loss: 0.7433937788009644
Epoch 250, training loss: 0.3836067318916321 = 0.3767261505126953 + 0.001 * 6.8805718421936035
Epoch 250, val loss: 0.7257558703422546
Epoch 260, training loss: 0.34442877769470215 = 0.3375619947910309 + 0.001 * 6.866793632507324
Epoch 260, val loss: 0.7111610770225525
Epoch 270, training loss: 0.3067226707935333 = 0.2998734414577484 + 0.001 * 6.849223613739014
Epoch 270, val loss: 0.6994301080703735
Epoch 280, training loss: 0.2704499065876007 = 0.26360970735549927 + 0.001 * 6.840195655822754
Epoch 280, val loss: 0.6906691789627075
Epoch 290, training loss: 0.23595929145812988 = 0.22912463545799255 + 0.001 * 6.83465576171875
Epoch 290, val loss: 0.6852523684501648
Epoch 300, training loss: 0.20401199162006378 = 0.19717927277088165 + 0.001 * 6.8327178955078125
Epoch 300, val loss: 0.6834000945091248
Epoch 310, training loss: 0.17544275522232056 = 0.16861553490161896 + 0.001 * 6.827217102050781
Epoch 310, val loss: 0.6853281855583191
Epoch 320, training loss: 0.1507483571767807 = 0.14392292499542236 + 0.001 * 6.825438976287842
Epoch 320, val loss: 0.6909188628196716
Epoch 330, training loss: 0.12991757690906525 = 0.12309402227401733 + 0.001 * 6.823555946350098
Epoch 330, val loss: 0.6998260617256165
Epoch 340, training loss: 0.112602598965168 = 0.10578052699565887 + 0.001 * 6.82206916809082
Epoch 340, val loss: 0.7113729119300842
Epoch 350, training loss: 0.09829086810350418 = 0.09147099405527115 + 0.001 * 6.819873332977295
Epoch 350, val loss: 0.7247611880302429
Epoch 360, training loss: 0.08646002411842346 = 0.0796428769826889 + 0.001 * 6.817145347595215
Epoch 360, val loss: 0.7391381859779358
Epoch 370, training loss: 0.07664372771978378 = 0.06983023881912231 + 0.001 * 6.8134894371032715
Epoch 370, val loss: 0.7539348006248474
Epoch 380, training loss: 0.06845118850469589 = 0.06163706257939339 + 0.001 * 6.814127445220947
Epoch 380, val loss: 0.7687335014343262
Epoch 390, training loss: 0.06155241280794144 = 0.0547458678483963 + 0.001 * 6.806543827056885
Epoch 390, val loss: 0.7833437919616699
Epoch 400, training loss: 0.05571581795811653 = 0.04890641197562218 + 0.001 * 6.809405326843262
Epoch 400, val loss: 0.7976939082145691
Epoch 410, training loss: 0.050721339881420135 = 0.04392290115356445 + 0.001 * 6.798436641693115
Epoch 410, val loss: 0.8116952180862427
Epoch 420, training loss: 0.04643688350915909 = 0.03963926061987877 + 0.001 * 6.797621250152588
Epoch 420, val loss: 0.8253321647644043
Epoch 430, training loss: 0.042723920196294785 = 0.03593319281935692 + 0.001 * 6.7907280921936035
Epoch 430, val loss: 0.8386155962944031
Epoch 440, training loss: 0.03949185460805893 = 0.03270747512578964 + 0.001 * 6.7843780517578125
Epoch 440, val loss: 0.8515233993530273
Epoch 450, training loss: 0.036661297082901 = 0.029884258285164833 + 0.001 * 6.777039051055908
Epoch 450, val loss: 0.8640933036804199
Epoch 460, training loss: 0.03417417034506798 = 0.027400506660342216 + 0.001 * 6.773664474487305
Epoch 460, val loss: 0.8762786388397217
Epoch 470, training loss: 0.03197997063398361 = 0.025204824283719063 + 0.001 * 6.775144100189209
Epoch 470, val loss: 0.8881691098213196
Epoch 480, training loss: 0.030025137588381767 = 0.02325543202459812 + 0.001 * 6.769705772399902
Epoch 480, val loss: 0.8996774554252625
Epoch 490, training loss: 0.028289176523685455 = 0.021517736837267876 + 0.001 * 6.771440029144287
Epoch 490, val loss: 0.910878598690033
Epoch 500, training loss: 0.02672054059803486 = 0.019963037222623825 + 0.001 * 6.757503032684326
Epoch 500, val loss: 0.9217442274093628
Epoch 510, training loss: 0.0253218412399292 = 0.01856735162436962 + 0.001 * 6.754488468170166
Epoch 510, val loss: 0.9323074817657471
Epoch 520, training loss: 0.024052727967500687 = 0.0173103678971529 + 0.001 * 6.742358684539795
Epoch 520, val loss: 0.9425434470176697
Epoch 530, training loss: 0.022912217304110527 = 0.016174912452697754 + 0.001 * 6.737304210662842
Epoch 530, val loss: 0.952526867389679
Epoch 540, training loss: 0.021884232759475708 = 0.015146402642130852 + 0.001 * 6.737829208374023
Epoch 540, val loss: 0.9621734619140625
Epoch 550, training loss: 0.02097112499177456 = 0.014212317764759064 + 0.001 * 6.758806228637695
Epoch 550, val loss: 0.9715924263000488
Epoch 560, training loss: 0.02008834481239319 = 0.013362215831875801 + 0.001 * 6.726129531860352
Epoch 560, val loss: 0.9807315468788147
Epoch 570, training loss: 0.019315002486109734 = 0.012586594559252262 + 0.001 * 6.728407382965088
Epoch 570, val loss: 0.9896400570869446
Epoch 580, training loss: 0.018611907958984375 = 0.011877182871103287 + 0.001 * 6.734724044799805
Epoch 580, val loss: 0.9983049035072327
Epoch 590, training loss: 0.017945753410458565 = 0.011226928792893887 + 0.001 * 6.718824863433838
Epoch 590, val loss: 1.0067250728607178
Epoch 600, training loss: 0.017359746620059013 = 0.010629618540406227 + 0.001 * 6.730127334594727
Epoch 600, val loss: 1.0149415731430054
Epoch 610, training loss: 0.01680109277367592 = 0.010079902596771717 + 0.001 * 6.721190452575684
Epoch 610, val loss: 1.0229939222335815
Epoch 620, training loss: 0.016285888850688934 = 0.00957276951521635 + 0.001 * 6.7131195068359375
Epoch 620, val loss: 1.0308088064193726
Epoch 630, training loss: 0.015820149332284927 = 0.009104028344154358 + 0.001 * 6.71612024307251
Epoch 630, val loss: 1.038461685180664
Epoch 640, training loss: 0.015390775166451931 = 0.008670005016028881 + 0.001 * 6.720769882202148
Epoch 640, val loss: 1.0459147691726685
Epoch 650, training loss: 0.014977524057030678 = 0.008267449215054512 + 0.001 * 6.7100749015808105
Epoch 650, val loss: 1.053173542022705
Epoch 660, training loss: 0.014601089060306549 = 0.007893466390669346 + 0.001 * 6.707622528076172
Epoch 660, val loss: 1.0602591037750244
Epoch 670, training loss: 0.014246249571442604 = 0.007545539177954197 + 0.001 * 6.700710773468018
Epoch 670, val loss: 1.0671769380569458
Epoch 680, training loss: 0.01394239068031311 = 0.007221223786473274 + 0.001 * 6.721167087554932
Epoch 680, val loss: 1.0739400386810303
Epoch 690, training loss: 0.013622500002384186 = 0.006918481085449457 + 0.001 * 6.7040181159973145
Epoch 690, val loss: 1.0805331468582153
Epoch 700, training loss: 0.01333625614643097 = 0.006635431200265884 + 0.001 * 6.700824737548828
Epoch 700, val loss: 1.0869956016540527
Epoch 710, training loss: 0.013073770329356194 = 0.0063704331405460835 + 0.001 * 6.703336715698242
Epoch 710, val loss: 1.093310832977295
Epoch 720, training loss: 0.012823864817619324 = 0.006122025661170483 + 0.001 * 6.701838493347168
Epoch 720, val loss: 1.0994969606399536
Epoch 730, training loss: 0.012584199197590351 = 0.0058888657949864864 + 0.001 * 6.695333003997803
Epoch 730, val loss: 1.1055312156677246
Epoch 740, training loss: 0.012356456369161606 = 0.00566971767693758 + 0.001 * 6.686738014221191
Epoch 740, val loss: 1.1114537715911865
Epoch 750, training loss: 0.012151515111327171 = 0.005463490262627602 + 0.001 * 6.688024520874023
Epoch 750, val loss: 1.1172481775283813
Epoch 760, training loss: 0.011954084038734436 = 0.005269221495836973 + 0.001 * 6.68486213684082
Epoch 760, val loss: 1.1229151487350464
Epoch 770, training loss: 0.011775169521570206 = 0.005085980519652367 + 0.001 * 6.689188003540039
Epoch 770, val loss: 1.1284841299057007
Epoch 780, training loss: 0.011590849608182907 = 0.004912915639579296 + 0.001 * 6.677933216094971
Epoch 780, val loss: 1.1339318752288818
Epoch 790, training loss: 0.011440971866250038 = 0.004749376326799393 + 0.001 * 6.691595554351807
Epoch 790, val loss: 1.1392593383789062
Epoch 800, training loss: 0.011287948116660118 = 0.0045946757309138775 + 0.001 * 6.693272590637207
Epoch 800, val loss: 1.1444981098175049
Epoch 810, training loss: 0.01113519724458456 = 0.0044481647200882435 + 0.001 * 6.687032222747803
Epoch 810, val loss: 1.1496142148971558
Epoch 820, training loss: 0.01099618524312973 = 0.004309307783842087 + 0.001 * 6.686877250671387
Epoch 820, val loss: 1.1546534299850464
Epoch 830, training loss: 0.010861750692129135 = 0.004177593160420656 + 0.001 * 6.684157848358154
Epoch 830, val loss: 1.1595546007156372
Epoch 840, training loss: 0.010730101726949215 = 0.00405254028737545 + 0.001 * 6.677561283111572
Epoch 840, val loss: 1.16439688205719
Epoch 850, training loss: 0.010603172704577446 = 0.0039337328635156155 + 0.001 * 6.669439315795898
Epoch 850, val loss: 1.169133186340332
Epoch 860, training loss: 0.010489516891539097 = 0.0038207254838198423 + 0.001 * 6.668790817260742
Epoch 860, val loss: 1.1737741231918335
Epoch 870, training loss: 0.010387004353106022 = 0.0037131719291210175 + 0.001 * 6.673831939697266
Epoch 870, val loss: 1.1783618927001953
Epoch 880, training loss: 0.01028487179428339 = 0.003610716201364994 + 0.001 * 6.674155235290527
Epoch 880, val loss: 1.1828328371047974
Epoch 890, training loss: 0.010189809836447239 = 0.0035130339674651623 + 0.001 * 6.6767754554748535
Epoch 890, val loss: 1.1872375011444092
Epoch 900, training loss: 0.010087992995977402 = 0.0034198604989796877 + 0.001 * 6.6681318283081055
Epoch 900, val loss: 1.191564917564392
Epoch 910, training loss: 0.00999570731073618 = 0.003330911509692669 + 0.001 * 6.664795398712158
Epoch 910, val loss: 1.1958423852920532
Epoch 920, training loss: 0.009912599809467793 = 0.003245972329750657 + 0.001 * 6.666626930236816
Epoch 920, val loss: 1.200013518333435
Epoch 930, training loss: 0.009826001711189747 = 0.0031647842843085527 + 0.001 * 6.661217212677002
Epoch 930, val loss: 1.2041279077529907
Epoch 940, training loss: 0.009781202301383018 = 0.0030871035996824503 + 0.001 * 6.694098472595215
Epoch 940, val loss: 1.208178997039795
Epoch 950, training loss: 0.009663441218435764 = 0.003012733068317175 + 0.001 * 6.650707721710205
Epoch 950, val loss: 1.2121524810791016
Epoch 960, training loss: 0.009589474648237228 = 0.002941505052149296 + 0.001 * 6.647968769073486
Epoch 960, val loss: 1.2160663604736328
Epoch 970, training loss: 0.009517304599285126 = 0.002873226534575224 + 0.001 * 6.644078254699707
Epoch 970, val loss: 1.2199130058288574
Epoch 980, training loss: 0.009483597241342068 = 0.0028077454771846533 + 0.001 * 6.675851345062256
Epoch 980, val loss: 1.2236988544464111
Epoch 990, training loss: 0.009394262917339802 = 0.0027449207846075296 + 0.001 * 6.649341583251953
Epoch 990, val loss: 1.2274205684661865
Epoch 1000, training loss: 0.00934846792370081 = 0.0026846015825867653 + 0.001 * 6.66386604309082
Epoch 1000, val loss: 1.231087327003479
Epoch 1010, training loss: 0.009271283634006977 = 0.0026266395580023527 + 0.001 * 6.644643783569336
Epoch 1010, val loss: 1.2347105741500854
Epoch 1020, training loss: 0.009211630560457706 = 0.002570925047621131 + 0.001 * 6.640705108642578
Epoch 1020, val loss: 1.238257646560669
Epoch 1030, training loss: 0.00917421467602253 = 0.0025173781905323267 + 0.001 * 6.65683650970459
Epoch 1030, val loss: 1.2417644262313843
Epoch 1040, training loss: 0.009097445756196976 = 0.002465836936607957 + 0.001 * 6.631608963012695
Epoch 1040, val loss: 1.2451945543289185
Epoch 1050, training loss: 0.009085976518690586 = 0.002416233764961362 + 0.001 * 6.669742107391357
Epoch 1050, val loss: 1.2485827207565308
Epoch 1060, training loss: 0.009015170857310295 = 0.0023684322368353605 + 0.001 * 6.646738052368164
Epoch 1060, val loss: 1.2519315481185913
Epoch 1070, training loss: 0.009018043987452984 = 0.0023224023170769215 + 0.001 * 6.69564151763916
Epoch 1070, val loss: 1.2552242279052734
Epoch 1080, training loss: 0.008915361016988754 = 0.0022780217695981264 + 0.001 * 6.637339115142822
Epoch 1080, val loss: 1.2584689855575562
Epoch 1090, training loss: 0.008878507651388645 = 0.0022352300584316254 + 0.001 * 6.643277168273926
Epoch 1090, val loss: 1.2616643905639648
Epoch 1100, training loss: 0.0088288439437747 = 0.0021938823629170656 + 0.001 * 6.634961128234863
Epoch 1100, val loss: 1.2648285627365112
Epoch 1110, training loss: 0.008778044022619724 = 0.0021539281588047743 + 0.001 * 6.624115467071533
Epoch 1110, val loss: 1.2679170370101929
Epoch 1120, training loss: 0.008756757713854313 = 0.002115332055836916 + 0.001 * 6.641425132751465
Epoch 1120, val loss: 1.2709801197052002
Epoch 1130, training loss: 0.008739002048969269 = 0.00207801116630435 + 0.001 * 6.6609907150268555
Epoch 1130, val loss: 1.2740037441253662
Epoch 1140, training loss: 0.008673582226037979 = 0.002041943371295929 + 0.001 * 6.631638050079346
Epoch 1140, val loss: 1.2770074605941772
Epoch 1150, training loss: 0.008638431318104267 = 0.0020070034079253674 + 0.001 * 6.631427764892578
Epoch 1150, val loss: 1.2799488306045532
Epoch 1160, training loss: 0.008590993471443653 = 0.001973203383386135 + 0.001 * 6.6177897453308105
Epoch 1160, val loss: 1.282862663269043
Epoch 1170, training loss: 0.008552692830562592 = 0.0019404604099690914 + 0.001 * 6.612232685089111
Epoch 1170, val loss: 1.2857357263565063
Epoch 1180, training loss: 0.008526447229087353 = 0.0019087352557107806 + 0.001 * 6.617712020874023
Epoch 1180, val loss: 1.288580298423767
Epoch 1190, training loss: 0.008495951071381569 = 0.001877939561381936 + 0.001 * 6.618010997772217
Epoch 1190, val loss: 1.2913711071014404
Epoch 1200, training loss: 0.008478951640427113 = 0.0018480939324945211 + 0.001 * 6.630857467651367
Epoch 1200, val loss: 1.2941598892211914
Epoch 1210, training loss: 0.008434789255261421 = 0.001819118275307119 + 0.001 * 6.615671157836914
Epoch 1210, val loss: 1.2969046831130981
Epoch 1220, training loss: 0.008397884666919708 = 0.0017909640446305275 + 0.001 * 6.6069207191467285
Epoch 1220, val loss: 1.2996231317520142
Epoch 1230, training loss: 0.008390266448259354 = 0.0017635932890698314 + 0.001 * 6.626672744750977
Epoch 1230, val loss: 1.3023399114608765
Epoch 1240, training loss: 0.008396375924348831 = 0.0017369738779962063 + 0.001 * 6.659402370452881
Epoch 1240, val loss: 1.3049882650375366
Epoch 1250, training loss: 0.008351304568350315 = 0.001711163087747991 + 0.001 * 6.640141010284424
Epoch 1250, val loss: 1.3076324462890625
Epoch 1260, training loss: 0.00831459742039442 = 0.0016860708128660917 + 0.001 * 6.628526210784912
Epoch 1260, val loss: 1.3102489709854126
Epoch 1270, training loss: 0.008261973038315773 = 0.001661651418544352 + 0.001 * 6.600321292877197
Epoch 1270, val loss: 1.3128597736358643
Epoch 1280, training loss: 0.008236292749643326 = 0.0016378944274038076 + 0.001 * 6.598398208618164
Epoch 1280, val loss: 1.3154441118240356
Epoch 1290, training loss: 0.008216667920351028 = 0.001614775275811553 + 0.001 * 6.601891994476318
Epoch 1290, val loss: 1.3180108070373535
Epoch 1300, training loss: 0.00819277111440897 = 0.001592242275364697 + 0.001 * 6.600528240203857
Epoch 1300, val loss: 1.3205335140228271
Epoch 1310, training loss: 0.008170532062649727 = 0.0015702795935794711 + 0.001 * 6.600252628326416
Epoch 1310, val loss: 1.3230414390563965
Epoch 1320, training loss: 0.008138785138726234 = 0.0015488992212340236 + 0.001 * 6.589885711669922
Epoch 1320, val loss: 1.3255424499511719
Epoch 1330, training loss: 0.008119744248688221 = 0.0015280668158084154 + 0.001 * 6.591676712036133
Epoch 1330, val loss: 1.3280023336410522
Epoch 1340, training loss: 0.008094255812466145 = 0.0015077490825206041 + 0.001 * 6.5865068435668945
Epoch 1340, val loss: 1.3304567337036133
Epoch 1350, training loss: 0.00807410478591919 = 0.0014879531227052212 + 0.001 * 6.586151599884033
Epoch 1350, val loss: 1.332889199256897
Epoch 1360, training loss: 0.008074602112174034 = 0.0014686578651890159 + 0.001 * 6.60594367980957
Epoch 1360, val loss: 1.3352992534637451
Epoch 1370, training loss: 0.008046801201999187 = 0.0014498267555609345 + 0.001 * 6.596973896026611
Epoch 1370, val loss: 1.3376991748809814
Epoch 1380, training loss: 0.008026900701224804 = 0.00143144850153476 + 0.001 * 6.595451354980469
Epoch 1380, val loss: 1.340078353881836
Epoch 1390, training loss: 0.008007555268704891 = 0.0014134958619251847 + 0.001 * 6.594058990478516
Epoch 1390, val loss: 1.342440128326416
Epoch 1400, training loss: 0.008000209927558899 = 0.0013959715142846107 + 0.001 * 6.6042375564575195
Epoch 1400, val loss: 1.344788670539856
Epoch 1410, training loss: 0.007968688383698463 = 0.0013788255164399743 + 0.001 * 6.58986234664917
Epoch 1410, val loss: 1.3471078872680664
Epoch 1420, training loss: 0.007960955612361431 = 0.001362091163173318 + 0.001 * 6.598864555358887
Epoch 1420, val loss: 1.3494393825531006
Epoch 1430, training loss: 0.007943356409668922 = 0.0013457442400977015 + 0.001 * 6.597611904144287
Epoch 1430, val loss: 1.3517287969589233
Epoch 1440, training loss: 0.007959945127367973 = 0.0013297918485477567 + 0.001 * 6.630152702331543
Epoch 1440, val loss: 1.3540316820144653
Epoch 1450, training loss: 0.007898760959506035 = 0.0013142237439751625 + 0.001 * 6.584536552429199
Epoch 1450, val loss: 1.356255292892456
Epoch 1460, training loss: 0.007878177799284458 = 0.0012990316608920693 + 0.001 * 6.579145431518555
Epoch 1460, val loss: 1.3585346937179565
Epoch 1470, training loss: 0.007870348170399666 = 0.001284191501326859 + 0.001 * 6.586156368255615
Epoch 1470, val loss: 1.3607516288757324
Epoch 1480, training loss: 0.0078578544780612 = 0.0012696956982836127 + 0.001 * 6.588158130645752
Epoch 1480, val loss: 1.362979769706726
Epoch 1490, training loss: 0.007839159108698368 = 0.0012555562425404787 + 0.001 * 6.5836029052734375
Epoch 1490, val loss: 1.3651350736618042
Epoch 1500, training loss: 0.007829269394278526 = 0.001241746125742793 + 0.001 * 6.587522506713867
Epoch 1500, val loss: 1.3673070669174194
Epoch 1510, training loss: 0.007800100836902857 = 0.0012282631359994411 + 0.001 * 6.571837425231934
Epoch 1510, val loss: 1.3694993257522583
Epoch 1520, training loss: 0.007794307544827461 = 0.001215088414028287 + 0.001 * 6.579218864440918
Epoch 1520, val loss: 1.371625304222107
Epoch 1530, training loss: 0.007804992143064737 = 0.0012022178852930665 + 0.001 * 6.602774143218994
Epoch 1530, val loss: 1.3737422227859497
Epoch 1540, training loss: 0.007761514279991388 = 0.0011896052164956927 + 0.001 * 6.571908473968506
Epoch 1540, val loss: 1.3758714199066162
Epoch 1550, training loss: 0.007753575220704079 = 0.001177317462861538 + 0.001 * 6.576257228851318
Epoch 1550, val loss: 1.3779480457305908
Epoch 1560, training loss: 0.0077355047687888145 = 0.0011653200490400195 + 0.001 * 6.570184230804443
Epoch 1560, val loss: 1.3800632953643799
Epoch 1570, training loss: 0.007728574797511101 = 0.0011535927187651396 + 0.001 * 6.574982166290283
Epoch 1570, val loss: 1.382097840309143
Epoch 1580, training loss: 0.0077168140560388565 = 0.0011421365197747946 + 0.001 * 6.574676990509033
Epoch 1580, val loss: 1.384184718132019
Epoch 1590, training loss: 0.0076987529173493385 = 0.0011309176916256547 + 0.001 * 6.567834854125977
Epoch 1590, val loss: 1.3861699104309082
Epoch 1600, training loss: 0.007680450566112995 = 0.0011199526488780975 + 0.001 * 6.560497760772705
Epoch 1600, val loss: 1.3882524967193604
Epoch 1610, training loss: 0.0076782372780144215 = 0.001109254895709455 + 0.001 * 6.568982124328613
Epoch 1610, val loss: 1.3902119398117065
Epoch 1620, training loss: 0.007710188627243042 = 0.0010987867135554552 + 0.001 * 6.6114020347595215
Epoch 1620, val loss: 1.3922369480133057
Epoch 1630, training loss: 0.007668854668736458 = 0.0010885832598432899 + 0.001 * 6.580270767211914
Epoch 1630, val loss: 1.3942078351974487
Epoch 1640, training loss: 0.007655499503016472 = 0.0010785734048113227 + 0.001 * 6.576925754547119
Epoch 1640, val loss: 1.396152377128601
Epoch 1650, training loss: 0.0076525853946805 = 0.0010687747271731496 + 0.001 * 6.583810329437256
Epoch 1650, val loss: 1.398097038269043
Epoch 1660, training loss: 0.007630213163793087 = 0.0010591631289571524 + 0.001 * 6.571049690246582
Epoch 1660, val loss: 1.4000592231750488
Epoch 1670, training loss: 0.007628214079886675 = 0.0010497927432879806 + 0.001 * 6.578421115875244
Epoch 1670, val loss: 1.401931881904602
Epoch 1680, training loss: 0.0076087662018835545 = 0.001040597795508802 + 0.001 * 6.5681681632995605
Epoch 1680, val loss: 1.4039005041122437
Epoch 1690, training loss: 0.007590167224407196 = 0.0010316177504137158 + 0.001 * 6.558549404144287
Epoch 1690, val loss: 1.4057737588882446
Epoch 1700, training loss: 0.007590514607727528 = 0.0010228145401924849 + 0.001 * 6.567699432373047
Epoch 1700, val loss: 1.4076461791992188
Epoch 1710, training loss: 0.0075770290568470955 = 0.001014207722619176 + 0.001 * 6.562821388244629
Epoch 1710, val loss: 1.4095120429992676
Epoch 1720, training loss: 0.00758672459051013 = 0.001005763653665781 + 0.001 * 6.580960750579834
Epoch 1720, val loss: 1.411382794380188
Epoch 1730, training loss: 0.007562681101262569 = 0.0009974879212677479 + 0.001 * 6.565192699432373
Epoch 1730, val loss: 1.4132208824157715
Epoch 1740, training loss: 0.007577836513519287 = 0.0009893912356346846 + 0.001 * 6.588444709777832
Epoch 1740, val loss: 1.4150328636169434
Epoch 1750, training loss: 0.007546058390289545 = 0.0009814348304644227 + 0.001 * 6.5646233558654785
Epoch 1750, val loss: 1.4168977737426758
Epoch 1760, training loss: 0.007538685575127602 = 0.0009736709180288017 + 0.001 * 6.565014362335205
Epoch 1760, val loss: 1.4187074899673462
Epoch 1770, training loss: 0.0075187766924500465 = 0.0009660618379712105 + 0.001 * 6.5527143478393555
Epoch 1770, val loss: 1.420506238937378
Epoch 1780, training loss: 0.007553016301244497 = 0.0009585983352735639 + 0.001 * 6.594417572021484
Epoch 1780, val loss: 1.4223072528839111
Epoch 1790, training loss: 0.007513992954045534 = 0.0009513153345324099 + 0.001 * 6.562677383422852
Epoch 1790, val loss: 1.4240491390228271
Epoch 1800, training loss: 0.007487834431231022 = 0.0009441750007681549 + 0.001 * 6.543659210205078
Epoch 1800, val loss: 1.4257957935333252
Epoch 1810, training loss: 0.007483894471079111 = 0.0009371709311380982 + 0.001 * 6.546723365783691
Epoch 1810, val loss: 1.4275567531585693
Epoch 1820, training loss: 0.0074743591248989105 = 0.0009302843827754259 + 0.001 * 6.544074535369873
Epoch 1820, val loss: 1.4292925596237183
Epoch 1830, training loss: 0.007481777109205723 = 0.0009235389297828078 + 0.001 * 6.558237552642822
Epoch 1830, val loss: 1.4310247898101807
Epoch 1840, training loss: 0.0075224884785711765 = 0.0009169452823698521 + 0.001 * 6.6055426597595215
Epoch 1840, val loss: 1.432788372039795
Epoch 1850, training loss: 0.007487373426556587 = 0.0009104812634177506 + 0.001 * 6.576891899108887
Epoch 1850, val loss: 1.4343900680541992
Epoch 1860, training loss: 0.007452638819813728 = 0.0009041547309607267 + 0.001 * 6.548483371734619
Epoch 1860, val loss: 1.4361317157745361
Epoch 1870, training loss: 0.007441595662385225 = 0.0008979630656540394 + 0.001 * 6.543632507324219
Epoch 1870, val loss: 1.4377256631851196
Epoch 1880, training loss: 0.007429995574057102 = 0.0008918901439756155 + 0.001 * 6.538105010986328
Epoch 1880, val loss: 1.4394506216049194
Epoch 1890, training loss: 0.007425905670970678 = 0.0008859415538609028 + 0.001 * 6.539963722229004
Epoch 1890, val loss: 1.4410518407821655
Epoch 1900, training loss: 0.0074281287379562855 = 0.0008801060612313449 + 0.001 * 6.548022747039795
Epoch 1900, val loss: 1.4427496194839478
Epoch 1910, training loss: 0.007427575532346964 = 0.0008743882062844932 + 0.001 * 6.553187370300293
Epoch 1910, val loss: 1.4443142414093018
Epoch 1920, training loss: 0.007428786717355251 = 0.0008687922381795943 + 0.001 * 6.559994220733643
Epoch 1920, val loss: 1.4459350109100342
Epoch 1930, training loss: 0.007419527042657137 = 0.0008633037796244025 + 0.001 * 6.556222915649414
Epoch 1930, val loss: 1.447509765625
Epoch 1940, training loss: 0.00740395113825798 = 0.0008579112472943962 + 0.001 * 6.546039581298828
Epoch 1940, val loss: 1.4491199254989624
Epoch 1950, training loss: 0.00738074304535985 = 0.0008526243618689477 + 0.001 * 6.528118133544922
Epoch 1950, val loss: 1.450668215751648
Epoch 1960, training loss: 0.007376733236014843 = 0.0008474259520880878 + 0.001 * 6.5293073654174805
Epoch 1960, val loss: 1.4522602558135986
Epoch 1970, training loss: 0.007379869464784861 = 0.0008423385443165898 + 0.001 * 6.537530899047852
Epoch 1970, val loss: 1.4538007974624634
Epoch 1980, training loss: 0.007391382474452257 = 0.0008373482851311564 + 0.001 * 6.5540337562561035
Epoch 1980, val loss: 1.455284595489502
Epoch 1990, training loss: 0.007381314877420664 = 0.0008324548252858222 + 0.001 * 6.548859596252441
Epoch 1990, val loss: 1.4568696022033691
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.6679
Flip ASR: 0.6089/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9426194429397583 = 1.9342457056045532 + 0.001 * 8.373726844787598
Epoch 0, val loss: 1.9342583417892456
Epoch 10, training loss: 1.9318114519119263 = 1.9234379529953003 + 0.001 * 8.373531341552734
Epoch 10, val loss: 1.9219884872436523
Epoch 20, training loss: 1.9181373119354248 = 1.909764289855957 + 0.001 * 8.373008728027344
Epoch 20, val loss: 1.9061213731765747
Epoch 30, training loss: 1.898874044418335 = 1.8905019760131836 + 0.001 * 8.372039794921875
Epoch 30, val loss: 1.8834762573242188
Epoch 40, training loss: 1.871326208114624 = 1.862956166267395 + 0.001 * 8.370083808898926
Epoch 40, val loss: 1.8516278266906738
Epoch 50, training loss: 1.8347283601760864 = 1.826363205909729 + 0.001 * 8.365139961242676
Epoch 50, val loss: 1.8114571571350098
Epoch 60, training loss: 1.7939249277114868 = 1.7855781316757202 + 0.001 * 8.346761703491211
Epoch 60, val loss: 1.7707164287567139
Epoch 70, training loss: 1.752328872680664 = 1.7440825700759888 + 0.001 * 8.246321678161621
Epoch 70, val loss: 1.7333101034164429
Epoch 80, training loss: 1.6979225873947144 = 1.6901439428329468 + 0.001 * 7.77863073348999
Epoch 80, val loss: 1.686609148979187
Epoch 90, training loss: 1.6247128248214722 = 1.6172137260437012 + 0.001 * 7.499136447906494
Epoch 90, val loss: 1.6243438720703125
Epoch 100, training loss: 1.5331711769104004 = 1.5259013175964355 + 0.001 * 7.269813537597656
Epoch 100, val loss: 1.547817349433899
Epoch 110, training loss: 1.4322952032089233 = 1.425199031829834 + 0.001 * 7.096200466156006
Epoch 110, val loss: 1.4669300317764282
Epoch 120, training loss: 1.328768253326416 = 1.3217475414276123 + 0.001 * 7.02074670791626
Epoch 120, val loss: 1.3867030143737793
Epoch 130, training loss: 1.2249705791473389 = 1.2179596424102783 + 0.001 * 7.010956287384033
Epoch 130, val loss: 1.3093721866607666
Epoch 140, training loss: 1.122166395187378 = 1.1151865720748901 + 0.001 * 6.979771137237549
Epoch 140, val loss: 1.2346700429916382
Epoch 150, training loss: 1.0231221914291382 = 1.01616632938385 + 0.001 * 6.955872058868408
Epoch 150, val loss: 1.1633939743041992
Epoch 160, training loss: 0.9309430122375488 = 0.9240018129348755 + 0.001 * 6.941226482391357
Epoch 160, val loss: 1.0975362062454224
Epoch 170, training loss: 0.8476526141166687 = 0.8407213091850281 + 0.001 * 6.931275367736816
Epoch 170, val loss: 1.0384892225265503
Epoch 180, training loss: 0.7744346857070923 = 0.7675120830535889 + 0.001 * 6.922582149505615
Epoch 180, val loss: 0.9872909188270569
Epoch 190, training loss: 0.7115204334259033 = 0.7046080231666565 + 0.001 * 6.912418842315674
Epoch 190, val loss: 0.9448133707046509
Epoch 200, training loss: 0.6575188040733337 = 0.6506179571151733 + 0.001 * 6.900871753692627
Epoch 200, val loss: 0.9098713994026184
Epoch 210, training loss: 0.6096677184104919 = 0.6027798056602478 + 0.001 * 6.887931823730469
Epoch 210, val loss: 0.8801621198654175
Epoch 220, training loss: 0.5651642084121704 = 0.5582907199859619 + 0.001 * 6.873483657836914
Epoch 220, val loss: 0.8539886474609375
Epoch 230, training loss: 0.5220129489898682 = 0.515155017375946 + 0.001 * 6.857907772064209
Epoch 230, val loss: 0.8305215239524841
Epoch 240, training loss: 0.4797009825706482 = 0.47285932302474976 + 0.001 * 6.841671466827393
Epoch 240, val loss: 0.8095443248748779
Epoch 250, training loss: 0.4384756088256836 = 0.43164879083633423 + 0.001 * 6.826807022094727
Epoch 250, val loss: 0.791247546672821
Epoch 260, training loss: 0.3987649381160736 = 0.3919510841369629 + 0.001 * 6.813849925994873
Epoch 260, val loss: 0.7757659554481506
Epoch 270, training loss: 0.3607981503009796 = 0.35399577021598816 + 0.001 * 6.802365303039551
Epoch 270, val loss: 0.7631945610046387
Epoch 280, training loss: 0.32466617226600647 = 0.31787025928497314 + 0.001 * 6.795923233032227
Epoch 280, val loss: 0.7531847357749939
Epoch 290, training loss: 0.2902860939502716 = 0.2834939658641815 + 0.001 * 6.792125225067139
Epoch 290, val loss: 0.7462491393089294
Epoch 300, training loss: 0.25811612606048584 = 0.25132590532302856 + 0.001 * 6.790227890014648
Epoch 300, val loss: 0.7419729828834534
Epoch 310, training loss: 0.22899363934993744 = 0.22220449149608612 + 0.001 * 6.789144515991211
Epoch 310, val loss: 0.7408347725868225
Epoch 320, training loss: 0.2026599645614624 = 0.19587184488773346 + 0.001 * 6.788116931915283
Epoch 320, val loss: 0.7437321543693542
Epoch 330, training loss: 0.179252028465271 = 0.17246504127979279 + 0.001 * 6.786993980407715
Epoch 330, val loss: 0.7493703961372375
Epoch 340, training loss: 0.15862126648426056 = 0.1518353670835495 + 0.001 * 6.785892486572266
Epoch 340, val loss: 0.7575271129608154
Epoch 350, training loss: 0.14056061208248138 = 0.13377481698989868 + 0.001 * 6.785797595977783
Epoch 350, val loss: 0.7682314515113831
Epoch 360, training loss: 0.12476113438606262 = 0.11797723919153214 + 0.001 * 6.783898830413818
Epoch 360, val loss: 0.7807889580726624
Epoch 370, training loss: 0.11101524531841278 = 0.10423288494348526 + 0.001 * 6.7823567390441895
Epoch 370, val loss: 0.7948126196861267
Epoch 380, training loss: 0.09905850142240524 = 0.09227726608514786 + 0.001 * 6.781235218048096
Epoch 380, val loss: 0.8096576929092407
Epoch 390, training loss: 0.08863884210586548 = 0.0818590298295021 + 0.001 * 6.779813289642334
Epoch 390, val loss: 0.8251073956489563
Epoch 400, training loss: 0.07955038547515869 = 0.07277244329452515 + 0.001 * 6.777945518493652
Epoch 400, val loss: 0.8408972024917603
Epoch 410, training loss: 0.07165685296058655 = 0.06488070636987686 + 0.001 * 6.776144027709961
Epoch 410, val loss: 0.856907844543457
Epoch 420, training loss: 0.06477818638086319 = 0.058003272861242294 + 0.001 * 6.7749128341674805
Epoch 420, val loss: 0.8729940056800842
Epoch 430, training loss: 0.05879189446568489 = 0.05201880633831024 + 0.001 * 6.7730889320373535
Epoch 430, val loss: 0.8889422416687012
Epoch 440, training loss: 0.05357217416167259 = 0.046802353113889694 + 0.001 * 6.769820213317871
Epoch 440, val loss: 0.9047513604164124
Epoch 450, training loss: 0.049017056822776794 = 0.04224679619073868 + 0.001 * 6.770260334014893
Epoch 450, val loss: 0.9205079078674316
Epoch 460, training loss: 0.04502852261066437 = 0.03826358541846275 + 0.001 * 6.764936447143555
Epoch 460, val loss: 0.9360770583152771
Epoch 470, training loss: 0.041535209864377975 = 0.03477317839860916 + 0.001 * 6.762031555175781
Epoch 470, val loss: 0.9514181613922119
Epoch 480, training loss: 0.03846650570631027 = 0.03170664981007576 + 0.001 * 6.759857177734375
Epoch 480, val loss: 0.9665870070457458
Epoch 490, training loss: 0.03576229512691498 = 0.02900393307209015 + 0.001 * 6.758362293243408
Epoch 490, val loss: 0.9815136790275574
Epoch 500, training loss: 0.03336646035313606 = 0.026613587513566017 + 0.001 * 6.752873420715332
Epoch 500, val loss: 0.9962041974067688
Epoch 510, training loss: 0.031240718439221382 = 0.024488339200615883 + 0.001 * 6.752379417419434
Epoch 510, val loss: 1.0106076002120972
Epoch 520, training loss: 0.02933388575911522 = 0.022588707506656647 + 0.001 * 6.74517822265625
Epoch 520, val loss: 1.0247786045074463
Epoch 530, training loss: 0.02763756364583969 = 0.02088465541601181 + 0.001 * 6.752906799316406
Epoch 530, val loss: 1.038604736328125
Epoch 540, training loss: 0.02609396167099476 = 0.019353091716766357 + 0.001 * 6.740869522094727
Epoch 540, val loss: 1.0522387027740479
Epoch 550, training loss: 0.02470826916396618 = 0.0179738849401474 + 0.001 * 6.734383583068848
Epoch 550, val loss: 1.0655113458633423
Epoch 560, training loss: 0.02345716953277588 = 0.016729528084397316 + 0.001 * 6.7276411056518555
Epoch 560, val loss: 1.0784662961959839
Epoch 570, training loss: 0.022343626245856285 = 0.015605449676513672 + 0.001 * 6.738175868988037
Epoch 570, val loss: 1.091154932975769
Epoch 580, training loss: 0.021311335265636444 = 0.014588393270969391 + 0.001 * 6.722941875457764
Epoch 580, val loss: 1.1034377813339233
Epoch 590, training loss: 0.020375508815050125 = 0.013666396029293537 + 0.001 * 6.709112167358398
Epoch 590, val loss: 1.1154406070709229
Epoch 600, training loss: 0.019533276557922363 = 0.012828330509364605 + 0.001 * 6.7049455642700195
Epoch 600, val loss: 1.1270664930343628
Epoch 610, training loss: 0.01876908168196678 = 0.012065086513757706 + 0.001 * 6.703995704650879
Epoch 610, val loss: 1.1383912563323975
Epoch 620, training loss: 0.018067628145217896 = 0.011368563398718834 + 0.001 * 6.699063777923584
Epoch 620, val loss: 1.1494441032409668
Epoch 630, training loss: 0.017426159232854843 = 0.010731564834713936 + 0.001 * 6.6945929527282715
Epoch 630, val loss: 1.16018807888031
Epoch 640, training loss: 0.01683368720114231 = 0.010147714987397194 + 0.001 * 6.685971260070801
Epoch 640, val loss: 1.1706149578094482
Epoch 650, training loss: 0.016293976455926895 = 0.009611551649868488 + 0.001 * 6.682424545288086
Epoch 650, val loss: 1.1807756423950195
Epoch 660, training loss: 0.015797628089785576 = 0.00911812111735344 + 0.001 * 6.679506301879883
Epoch 660, val loss: 1.1906683444976807
Epoch 670, training loss: 0.015339510515332222 = 0.008663111366331577 + 0.001 * 6.676398754119873
Epoch 670, val loss: 1.2003084421157837
Epoch 680, training loss: 0.014954637736082077 = 0.008242550306022167 + 0.001 * 6.712087631225586
Epoch 680, val loss: 1.2095967531204224
Epoch 690, training loss: 0.014547440223395824 = 0.007852904498577118 + 0.001 * 6.694535255432129
Epoch 690, val loss: 1.2187687158584595
Epoch 700, training loss: 0.0141574926674366 = 0.007490960881114006 + 0.001 * 6.666532039642334
Epoch 700, val loss: 1.2276359796524048
Epoch 710, training loss: 0.013823184184730053 = 0.007154509425163269 + 0.001 * 6.668674468994141
Epoch 710, val loss: 1.2363163232803345
Epoch 720, training loss: 0.01351265236735344 = 0.00684140482917428 + 0.001 * 6.6712470054626465
Epoch 720, val loss: 1.2447710037231445
Epoch 730, training loss: 0.013208165764808655 = 0.006549607962369919 + 0.001 * 6.658557891845703
Epoch 730, val loss: 1.2529977560043335
Epoch 740, training loss: 0.012933341786265373 = 0.006277086213231087 + 0.001 * 6.65625524520874
Epoch 740, val loss: 1.2610464096069336
Epoch 750, training loss: 0.012688337825238705 = 0.006022091954946518 + 0.001 * 6.666245460510254
Epoch 750, val loss: 1.2688827514648438
Epoch 760, training loss: 0.012437105178833008 = 0.005783447064459324 + 0.001 * 6.653657913208008
Epoch 760, val loss: 1.2765394449234009
Epoch 770, training loss: 0.012230616994202137 = 0.005559466779232025 + 0.001 * 6.671149730682373
Epoch 770, val loss: 1.284034252166748
Epoch 780, training loss: 0.012004205025732517 = 0.00534899951890111 + 0.001 * 6.655205249786377
Epoch 780, val loss: 1.2913707494735718
Epoch 790, training loss: 0.01180105097591877 = 0.005151030607521534 + 0.001 * 6.650020122528076
Epoch 790, val loss: 1.2985491752624512
Epoch 800, training loss: 0.01161108911037445 = 0.0049645365215837955 + 0.001 * 6.646551609039307
Epoch 800, val loss: 1.3056246042251587
Epoch 810, training loss: 0.011444542557001114 = 0.00478858407586813 + 0.001 * 6.655958652496338
Epoch 810, val loss: 1.3125239610671997
Epoch 820, training loss: 0.011286166496574879 = 0.004622746258974075 + 0.001 * 6.663419723510742
Epoch 820, val loss: 1.3192540407180786
Epoch 830, training loss: 0.011121034622192383 = 0.004466173704713583 + 0.001 * 6.654860973358154
Epoch 830, val loss: 1.3258837461471558
Epoch 840, training loss: 0.010976637713611126 = 0.004317959304898977 + 0.001 * 6.65867805480957
Epoch 840, val loss: 1.3323240280151367
Epoch 850, training loss: 0.010816186666488647 = 0.004177138675004244 + 0.001 * 6.639047622680664
Epoch 850, val loss: 1.3386597633361816
Epoch 860, training loss: 0.010680800303816795 = 0.0040434617549180984 + 0.001 * 6.637337684631348
Epoch 860, val loss: 1.3448526859283447
Epoch 870, training loss: 0.010566312819719315 = 0.003916588146239519 + 0.001 * 6.64972448348999
Epoch 870, val loss: 1.3509221076965332
Epoch 880, training loss: 0.010432898066937923 = 0.003796071745455265 + 0.001 * 6.636826038360596
Epoch 880, val loss: 1.3568692207336426
Epoch 890, training loss: 0.01033372525125742 = 0.0036815779749304056 + 0.001 * 6.652146816253662
Epoch 890, val loss: 1.3627102375030518
Epoch 900, training loss: 0.010214045643806458 = 0.0035725324414670467 + 0.001 * 6.641512870788574
Epoch 900, val loss: 1.3684322834014893
Epoch 910, training loss: 0.010108530521392822 = 0.003468993352726102 + 0.001 * 6.6395368576049805
Epoch 910, val loss: 1.3740519285202026
Epoch 920, training loss: 0.010008672252297401 = 0.003370376769453287 + 0.001 * 6.638295650482178
Epoch 920, val loss: 1.3795198202133179
Epoch 930, training loss: 0.009907299652695656 = 0.003276372794061899 + 0.001 * 6.630926132202148
Epoch 930, val loss: 1.384909749031067
Epoch 940, training loss: 0.009811094962060452 = 0.0031866789795458317 + 0.001 * 6.624415874481201
Epoch 940, val loss: 1.3901914358139038
Epoch 950, training loss: 0.009733295068144798 = 0.0031010317616164684 + 0.001 * 6.632263660430908
Epoch 950, val loss: 1.3953908681869507
Epoch 960, training loss: 0.00968913920223713 = 0.003019191324710846 + 0.001 * 6.669947624206543
Epoch 960, val loss: 1.4004859924316406
Epoch 970, training loss: 0.009559630416333675 = 0.0029414489399641752 + 0.001 * 6.618181228637695
Epoch 970, val loss: 1.405473232269287
Epoch 980, training loss: 0.009494896978139877 = 0.0028671734035015106 + 0.001 * 6.62772274017334
Epoch 980, val loss: 1.410401463508606
Epoch 990, training loss: 0.009443530812859535 = 0.0027960040606558323 + 0.001 * 6.647526264190674
Epoch 990, val loss: 1.4152063131332397
Epoch 1000, training loss: 0.009344416670501232 = 0.0027278647758066654 + 0.001 * 6.616551399230957
Epoch 1000, val loss: 1.4199130535125732
Epoch 1010, training loss: 0.009294020012021065 = 0.0026625210884958506 + 0.001 * 6.631498336791992
Epoch 1010, val loss: 1.424554467201233
Epoch 1020, training loss: 0.009214074350893497 = 0.002599855652078986 + 0.001 * 6.614218711853027
Epoch 1020, val loss: 1.4290955066680908
Epoch 1030, training loss: 0.009164508432149887 = 0.0025397350545972586 + 0.001 * 6.6247735023498535
Epoch 1030, val loss: 1.4335718154907227
Epoch 1040, training loss: 0.00911545380949974 = 0.0024820510298013687 + 0.001 * 6.633401870727539
Epoch 1040, val loss: 1.4379668235778809
Epoch 1050, training loss: 0.009048587642610073 = 0.0024266468826681376 + 0.001 * 6.621940612792969
Epoch 1050, val loss: 1.4422532320022583
Epoch 1060, training loss: 0.009009095840156078 = 0.0023735163267701864 + 0.001 * 6.6355791091918945
Epoch 1060, val loss: 1.4465234279632568
Epoch 1070, training loss: 0.008943461813032627 = 0.002322428161278367 + 0.001 * 6.621033668518066
Epoch 1070, val loss: 1.450629711151123
Epoch 1080, training loss: 0.008883628994226456 = 0.002273289952427149 + 0.001 * 6.61033821105957
Epoch 1080, val loss: 1.4547170400619507
Epoch 1090, training loss: 0.00883426982909441 = 0.0022260204423218966 + 0.001 * 6.608249187469482
Epoch 1090, val loss: 1.4587126970291138
Epoch 1100, training loss: 0.008791409432888031 = 0.0021805032156407833 + 0.001 * 6.61090612411499
Epoch 1100, val loss: 1.4626394510269165
Epoch 1110, training loss: 0.008775404654443264 = 0.0021366318687796593 + 0.001 * 6.638772487640381
Epoch 1110, val loss: 1.466493010520935
Epoch 1120, training loss: 0.00869787484407425 = 0.002094340045005083 + 0.001 * 6.60353422164917
Epoch 1120, val loss: 1.4702885150909424
Epoch 1130, training loss: 0.00868965033441782 = 0.0020536005031317472 + 0.001 * 6.636049747467041
Epoch 1130, val loss: 1.474048376083374
Epoch 1140, training loss: 0.008615739643573761 = 0.0020143010187894106 + 0.001 * 6.601438045501709
Epoch 1140, val loss: 1.4776849746704102
Epoch 1150, training loss: 0.008564436808228493 = 0.0019763365853577852 + 0.001 * 6.588099479675293
Epoch 1150, val loss: 1.4813166856765747
Epoch 1160, training loss: 0.008539795875549316 = 0.0019397289725020528 + 0.001 * 6.600066661834717
Epoch 1160, val loss: 1.484839677810669
Epoch 1170, training loss: 0.00852084532380104 = 0.0019044110085815191 + 0.001 * 6.616433620452881
Epoch 1170, val loss: 1.488313913345337
Epoch 1180, training loss: 0.008494246751070023 = 0.0018702876986935735 + 0.001 * 6.623958587646484
Epoch 1180, val loss: 1.4917407035827637
Epoch 1190, training loss: 0.008445932529866695 = 0.0018373854691162705 + 0.001 * 6.608546733856201
Epoch 1190, val loss: 1.4950889348983765
Epoch 1200, training loss: 0.00839123036712408 = 0.0018055728869512677 + 0.001 * 6.585657119750977
Epoch 1200, val loss: 1.4983848333358765
Epoch 1210, training loss: 0.008375701494514942 = 0.0017748393584042788 + 0.001 * 6.6008620262146
Epoch 1210, val loss: 1.501662015914917
Epoch 1220, training loss: 0.008335929363965988 = 0.001745112705975771 + 0.001 * 6.590816020965576
Epoch 1220, val loss: 1.5048350095748901
Epoch 1230, training loss: 0.008317703381180763 = 0.001716381753794849 + 0.001 * 6.601321697235107
Epoch 1230, val loss: 1.5079703330993652
Epoch 1240, training loss: 0.008283869363367558 = 0.0016885765362530947 + 0.001 * 6.595292091369629
Epoch 1240, val loss: 1.5110641717910767
Epoch 1250, training loss: 0.00826421007514 = 0.0016616679495200515 + 0.001 * 6.602541923522949
Epoch 1250, val loss: 1.5141136646270752
Epoch 1260, training loss: 0.008237458765506744 = 0.0016356403939425945 + 0.001 * 6.601818561553955
Epoch 1260, val loss: 1.517067551612854
Epoch 1270, training loss: 0.00818726047873497 = 0.0016104127280414104 + 0.001 * 6.576847553253174
Epoch 1270, val loss: 1.5199904441833496
Epoch 1280, training loss: 0.00817960686981678 = 0.0015859567793086171 + 0.001 * 6.593649387359619
Epoch 1280, val loss: 1.522878646850586
Epoch 1290, training loss: 0.008137794211506844 = 0.0015622604405507445 + 0.001 * 6.575533390045166
Epoch 1290, val loss: 1.525706171989441
Epoch 1300, training loss: 0.00814023893326521 = 0.0015393075300380588 + 0.001 * 6.600931644439697
Epoch 1300, val loss: 1.5284501314163208
Epoch 1310, training loss: 0.008087067864835262 = 0.0015170355327427387 + 0.001 * 6.570032119750977
Epoch 1310, val loss: 1.5311944484710693
Epoch 1320, training loss: 0.008079650811851025 = 0.001495313597843051 + 0.001 * 6.584336757659912
Epoch 1320, val loss: 1.5338845252990723
Epoch 1330, training loss: 0.008041183464229107 = 0.0014742237981408834 + 0.001 * 6.566959381103516
Epoch 1330, val loss: 1.5365266799926758
Epoch 1340, training loss: 0.008023652248084545 = 0.0014537490205839276 + 0.001 * 6.5699028968811035
Epoch 1340, val loss: 1.5391545295715332
Epoch 1350, training loss: 0.008023742586374283 = 0.001433909055776894 + 0.001 * 6.589832782745361
Epoch 1350, val loss: 1.541701316833496
Epoch 1360, training loss: 0.007996216416358948 = 0.0014146753819659352 + 0.001 * 6.581540584564209
Epoch 1360, val loss: 1.5442335605621338
Epoch 1370, training loss: 0.00797897856682539 = 0.0013960335636511445 + 0.001 * 6.582944869995117
Epoch 1370, val loss: 1.5467009544372559
Epoch 1380, training loss: 0.007957893423736095 = 0.001377935172058642 + 0.001 * 6.579957962036133
Epoch 1380, val loss: 1.549122929573059
Epoch 1390, training loss: 0.007939052768051624 = 0.0013603640254586935 + 0.001 * 6.578688621520996
Epoch 1390, val loss: 1.5515488386154175
Epoch 1400, training loss: 0.007935569621622562 = 0.0013432888081297278 + 0.001 * 6.592280387878418
Epoch 1400, val loss: 1.5538896322250366
Epoch 1410, training loss: 0.007896291092038155 = 0.0013267416507005692 + 0.001 * 6.569549083709717
Epoch 1410, val loss: 1.5561902523040771
Epoch 1420, training loss: 0.007870534434914589 = 0.0013106587575748563 + 0.001 * 6.559875011444092
Epoch 1420, val loss: 1.5584888458251953
Epoch 1430, training loss: 0.007855265401303768 = 0.0012950159143656492 + 0.001 * 6.560249328613281
Epoch 1430, val loss: 1.560701608657837
Epoch 1440, training loss: 0.007838131859898567 = 0.0012797932140529156 + 0.001 * 6.558338642120361
Epoch 1440, val loss: 1.5629312992095947
Epoch 1450, training loss: 0.00785384327173233 = 0.0012649744749069214 + 0.001 * 6.588868141174316
Epoch 1450, val loss: 1.565048098564148
Epoch 1460, training loss: 0.007829301059246063 = 0.0012505765771493316 + 0.001 * 6.578724384307861
Epoch 1460, val loss: 1.567165493965149
Epoch 1470, training loss: 0.00782900769263506 = 0.0012365591246634722 + 0.001 * 6.5924482345581055
Epoch 1470, val loss: 1.569259762763977
Epoch 1480, training loss: 0.007785480469465256 = 0.0012229165295138955 + 0.001 * 6.562563896179199
Epoch 1480, val loss: 1.571301817893982
Epoch 1490, training loss: 0.007765626534819603 = 0.0012096574064344168 + 0.001 * 6.555968761444092
Epoch 1490, val loss: 1.5733529329299927
Epoch 1500, training loss: 0.0077529652044177055 = 0.0011967761674895883 + 0.001 * 6.556189060211182
Epoch 1500, val loss: 1.5753228664398193
Epoch 1510, training loss: 0.0077198343351483345 = 0.0011842430103570223 + 0.001 * 6.535591125488281
Epoch 1510, val loss: 1.577286720275879
Epoch 1520, training loss: 0.007707872893661261 = 0.001172030926682055 + 0.001 * 6.535841941833496
Epoch 1520, val loss: 1.5791691541671753
Epoch 1530, training loss: 0.007738957181572914 = 0.0011601276928558946 + 0.001 * 6.578829288482666
Epoch 1530, val loss: 1.5810482501983643
Epoch 1540, training loss: 0.007729758508503437 = 0.0011485418071970344 + 0.001 * 6.581216335296631
Epoch 1540, val loss: 1.582914113998413
Epoch 1550, training loss: 0.007665687240660191 = 0.0011372268199920654 + 0.001 * 6.5284600257873535
Epoch 1550, val loss: 1.584733486175537
Epoch 1560, training loss: 0.007654258981347084 = 0.0011262010084465146 + 0.001 * 6.52805757522583
Epoch 1560, val loss: 1.5864906311035156
Epoch 1570, training loss: 0.007644812576472759 = 0.001115456921979785 + 0.001 * 6.529355525970459
Epoch 1570, val loss: 1.5882378816604614
Epoch 1580, training loss: 0.007627840619534254 = 0.0011049848981201649 + 0.001 * 6.522855281829834
Epoch 1580, val loss: 1.589943528175354
Epoch 1590, training loss: 0.0076505281031131744 = 0.001094770384952426 + 0.001 * 6.55575704574585
Epoch 1590, val loss: 1.5916613340377808
Epoch 1600, training loss: 0.0076334248296916485 = 0.0010847942903637886 + 0.001 * 6.548630237579346
Epoch 1600, val loss: 1.593314528465271
Epoch 1610, training loss: 0.007619716227054596 = 0.001075032283551991 + 0.001 * 6.544683933258057
Epoch 1610, val loss: 1.5949541330337524
Epoch 1620, training loss: 0.007603855803608894 = 0.0010655440855771303 + 0.001 * 6.53831148147583
Epoch 1620, val loss: 1.5965385437011719
Epoch 1630, training loss: 0.007595718372613192 = 0.0010562640381976962 + 0.001 * 6.539453983306885
Epoch 1630, val loss: 1.598157286643982
Epoch 1640, training loss: 0.007577241398394108 = 0.0010472056455910206 + 0.001 * 6.530035495758057
Epoch 1640, val loss: 1.5996689796447754
Epoch 1650, training loss: 0.007568965200334787 = 0.0010383428307250142 + 0.001 * 6.5306220054626465
Epoch 1650, val loss: 1.6012383699417114
Epoch 1660, training loss: 0.007549586705863476 = 0.001029701204970479 + 0.001 * 6.519885063171387
Epoch 1660, val loss: 1.6027295589447021
Epoch 1670, training loss: 0.007560886442661285 = 0.0010212542256340384 + 0.001 * 6.5396318435668945
Epoch 1670, val loss: 1.6042143106460571
Epoch 1680, training loss: 0.007532963063567877 = 0.0010130060836672783 + 0.001 * 6.519956588745117
Epoch 1680, val loss: 1.6056456565856934
Epoch 1690, training loss: 0.007539650425314903 = 0.0010049561969935894 + 0.001 * 6.534693717956543
Epoch 1690, val loss: 1.607096552848816
Epoch 1700, training loss: 0.0075483378022909164 = 0.0009970631217584014 + 0.001 * 6.551274299621582
Epoch 1700, val loss: 1.6085008382797241
Epoch 1710, training loss: 0.00755454832687974 = 0.0009893840178847313 + 0.001 * 6.565164089202881
Epoch 1710, val loss: 1.6099154949188232
Epoch 1720, training loss: 0.0075179049745202065 = 0.0009818265680223703 + 0.001 * 6.536078453063965
Epoch 1720, val loss: 1.6112407445907593
Epoch 1730, training loss: 0.0075135850347578526 = 0.0009744783164933324 + 0.001 * 6.539106369018555
Epoch 1730, val loss: 1.6125777959823608
Epoch 1740, training loss: 0.007525872439146042 = 0.0009672918822616339 + 0.001 * 6.55858039855957
Epoch 1740, val loss: 1.6139248609542847
Epoch 1750, training loss: 0.007494635414332151 = 0.0009602575446479023 + 0.001 * 6.534377574920654
Epoch 1750, val loss: 1.6152194738388062
Epoch 1760, training loss: 0.007456557359546423 = 0.000953362206928432 + 0.001 * 6.503194808959961
Epoch 1760, val loss: 1.616486668586731
Epoch 1770, training loss: 0.007451226469129324 = 0.0009466375922784209 + 0.001 * 6.504588603973389
Epoch 1770, val loss: 1.6177079677581787
Epoch 1780, training loss: 0.007450002245604992 = 0.0009400683920830488 + 0.001 * 6.509933948516846
Epoch 1780, val loss: 1.6189566850662231
Epoch 1790, training loss: 0.007486947812139988 = 0.0009336383081972599 + 0.001 * 6.553308963775635
Epoch 1790, val loss: 1.620155692100525
Epoch 1800, training loss: 0.007422156631946564 = 0.0009273265604861081 + 0.001 * 6.4948296546936035
Epoch 1800, val loss: 1.6213282346725464
Epoch 1810, training loss: 0.007416119799017906 = 0.0009211534052155912 + 0.001 * 6.49496603012085
Epoch 1810, val loss: 1.6224883794784546
Epoch 1820, training loss: 0.00742327980697155 = 0.000915105571039021 + 0.001 * 6.508173942565918
Epoch 1820, val loss: 1.6236449480056763
Epoch 1830, training loss: 0.007436869665980339 = 0.0009091607644222677 + 0.001 * 6.527708530426025
Epoch 1830, val loss: 1.6247607469558716
Epoch 1840, training loss: 0.007398453541100025 = 0.0009033351088874042 + 0.001 * 6.495118141174316
Epoch 1840, val loss: 1.6258625984191895
Epoch 1850, training loss: 0.0074118198826909065 = 0.0008976154495030642 + 0.001 * 6.514204502105713
Epoch 1850, val loss: 1.6269773244857788
Epoch 1860, training loss: 0.007384464144706726 = 0.000892020994797349 + 0.001 * 6.492442607879639
Epoch 1860, val loss: 1.6280381679534912
Epoch 1870, training loss: 0.007392126601189375 = 0.0008865422569215298 + 0.001 * 6.505584239959717
Epoch 1870, val loss: 1.6290863752365112
Epoch 1880, training loss: 0.007378432434052229 = 0.0008811728330329061 + 0.001 * 6.497259616851807
Epoch 1880, val loss: 1.630141019821167
Epoch 1890, training loss: 0.007375406567007303 = 0.0008758825133554637 + 0.001 * 6.499524116516113
Epoch 1890, val loss: 1.6311566829681396
Epoch 1900, training loss: 0.007353442721068859 = 0.0008707124507054687 + 0.001 * 6.482729911804199
Epoch 1900, val loss: 1.6321321725845337
Epoch 1910, training loss: 0.007376052439212799 = 0.0008656613645143807 + 0.001 * 6.510390758514404
Epoch 1910, val loss: 1.6331169605255127
Epoch 1920, training loss: 0.007363715209066868 = 0.0008607070776633918 + 0.001 * 6.503007888793945
Epoch 1920, val loss: 1.6340781450271606
Epoch 1930, training loss: 0.007397314067929983 = 0.0008558423141948879 + 0.001 * 6.541471481323242
Epoch 1930, val loss: 1.6350069046020508
Epoch 1940, training loss: 0.0073571838438510895 = 0.0008510948391631246 + 0.001 * 6.506088733673096
Epoch 1940, val loss: 1.6359281539916992
Epoch 1950, training loss: 0.007361253257840872 = 0.0008464052225463092 + 0.001 * 6.514847755432129
Epoch 1950, val loss: 1.6368683576583862
Epoch 1960, training loss: 0.007338818162679672 = 0.0008417714270763099 + 0.001 * 6.49704647064209
Epoch 1960, val loss: 1.6377065181732178
Epoch 1970, training loss: 0.007378762122243643 = 0.0008371860021725297 + 0.001 * 6.541575908660889
Epoch 1970, val loss: 1.6385892629623413
Epoch 1980, training loss: 0.007321850396692753 = 0.0008326733950525522 + 0.001 * 6.489176273345947
Epoch 1980, val loss: 1.6394085884094238
Epoch 1990, training loss: 0.0073362537659704685 = 0.0008282472263090312 + 0.001 * 6.508006572723389
Epoch 1990, val loss: 1.6402467489242554
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.1956
Flip ASR: 0.2133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9600707292556763 = 1.9516969919204712 + 0.001 * 8.373756408691406
Epoch 0, val loss: 1.9623596668243408
Epoch 10, training loss: 1.9490219354629517 = 1.9406483173370361 + 0.001 * 8.37362289428711
Epoch 10, val loss: 1.951807975769043
Epoch 20, training loss: 1.935058355331421 = 1.926685094833374 + 0.001 * 8.37321949005127
Epoch 20, val loss: 1.9379351139068604
Epoch 30, training loss: 1.9149174690246582 = 1.9065451622009277 + 0.001 * 8.37232494354248
Epoch 30, val loss: 1.917724370956421
Epoch 40, training loss: 1.8849588632583618 = 1.8765887022018433 + 0.001 * 8.370162010192871
Epoch 40, val loss: 1.8882170915603638
Epoch 50, training loss: 1.8442059755325317 = 1.835842490196228 + 0.001 * 8.363473892211914
Epoch 50, val loss: 1.8499846458435059
Epoch 60, training loss: 1.800387978553772 = 1.7920552492141724 + 0.001 * 8.332715034484863
Epoch 60, val loss: 1.8113890886306763
Epoch 70, training loss: 1.7572767734527588 = 1.7491703033447266 + 0.001 * 8.106447219848633
Epoch 70, val loss: 1.7703360319137573
Epoch 80, training loss: 1.7002248764038086 = 1.6925716400146484 + 0.001 * 7.653204441070557
Epoch 80, val loss: 1.715782880783081
Epoch 90, training loss: 1.6237388849258423 = 1.6162471771240234 + 0.001 * 7.491668224334717
Epoch 90, val loss: 1.6487786769866943
Epoch 100, training loss: 1.5264655351638794 = 1.519169807434082 + 0.001 * 7.2956767082214355
Epoch 100, val loss: 1.5670201778411865
Epoch 110, training loss: 1.4205490350723267 = 1.4134676456451416 + 0.001 * 7.081411838531494
Epoch 110, val loss: 1.4793537855148315
Epoch 120, training loss: 1.3166918754577637 = 1.3096553087234497 + 0.001 * 7.036605358123779
Epoch 120, val loss: 1.3959579467773438
Epoch 130, training loss: 1.2177270650863647 = 1.2107386589050293 + 0.001 * 6.988353252410889
Epoch 130, val loss: 1.3203002214431763
Epoch 140, training loss: 1.1235377788543701 = 1.1166032552719116 + 0.001 * 6.934546947479248
Epoch 140, val loss: 1.2500087022781372
Epoch 150, training loss: 1.0332459211349487 = 1.0263404846191406 + 0.001 * 6.905476093292236
Epoch 150, val loss: 1.1835588216781616
Epoch 160, training loss: 0.9467592835426331 = 0.9398619532585144 + 0.001 * 6.897341728210449
Epoch 160, val loss: 1.120708703994751
Epoch 170, training loss: 0.8650721907615662 = 0.8581808805465698 + 0.001 * 6.891298770904541
Epoch 170, val loss: 1.061838984489441
Epoch 180, training loss: 0.7891160845756531 = 0.7822321653366089 + 0.001 * 6.883943557739258
Epoch 180, val loss: 1.0081182718276978
Epoch 190, training loss: 0.7188509106636047 = 0.7119767069816589 + 0.001 * 6.874218940734863
Epoch 190, val loss: 0.9599957466125488
Epoch 200, training loss: 0.6534105539321899 = 0.6465494632720947 + 0.001 * 6.861105442047119
Epoch 200, val loss: 0.9168282151222229
Epoch 210, training loss: 0.5918827056884766 = 0.5850395560264587 + 0.001 * 6.843159198760986
Epoch 210, val loss: 0.8782790303230286
Epoch 220, training loss: 0.5339198708534241 = 0.5271006226539612 + 0.001 * 6.819221019744873
Epoch 220, val loss: 0.8446498513221741
Epoch 230, training loss: 0.47968822717666626 = 0.47289782762527466 + 0.001 * 6.7904052734375
Epoch 230, val loss: 0.8163179755210876
Epoch 240, training loss: 0.42964082956314087 = 0.42287102341651917 + 0.001 * 6.769814968109131
Epoch 240, val loss: 0.79412442445755
Epoch 250, training loss: 0.3839901089668274 = 0.3772292733192444 + 0.001 * 6.760841369628906
Epoch 250, val loss: 0.77794349193573
Epoch 260, training loss: 0.3425208330154419 = 0.33576449751853943 + 0.001 * 6.756336688995361
Epoch 260, val loss: 0.7667426466941833
Epoch 270, training loss: 0.3048907220363617 = 0.29813843965530396 + 0.001 * 6.75227689743042
Epoch 270, val loss: 0.7595581412315369
Epoch 280, training loss: 0.2708067297935486 = 0.2640568017959595 + 0.001 * 6.749917030334473
Epoch 280, val loss: 0.755626380443573
Epoch 290, training loss: 0.24008965492248535 = 0.23334181308746338 + 0.001 * 6.74783992767334
Epoch 290, val loss: 0.7544824481010437
Epoch 300, training loss: 0.21260230243206024 = 0.20585617423057556 + 0.001 * 6.746130466461182
Epoch 300, val loss: 0.7558712959289551
Epoch 310, training loss: 0.18817083537578583 = 0.18142633140087128 + 0.001 * 6.744500160217285
Epoch 310, val loss: 0.7594286799430847
Epoch 320, training loss: 0.1665656566619873 = 0.15982286632061005 + 0.001 * 6.742793083190918
Epoch 320, val loss: 0.7647092342376709
Epoch 330, training loss: 0.14755336940288544 = 0.1408120095729828 + 0.001 * 6.741365432739258
Epoch 330, val loss: 0.7712485194206238
Epoch 340, training loss: 0.1309061199426651 = 0.12416134029626846 + 0.001 * 6.744773864746094
Epoch 340, val loss: 0.7788421511650085
Epoch 350, training loss: 0.11636898666620255 = 0.10963103920221329 + 0.001 * 6.737948417663574
Epoch 350, val loss: 0.7872083187103271
Epoch 360, training loss: 0.10372140258550644 = 0.09698500484228134 + 0.001 * 6.736398220062256
Epoch 360, val loss: 0.7961870431900024
Epoch 370, training loss: 0.09272213280200958 = 0.08598851412534714 + 0.001 * 6.733619689941406
Epoch 370, val loss: 0.805637538433075
Epoch 380, training loss: 0.08315809071063995 = 0.07642599195241928 + 0.001 * 6.7320990562438965
Epoch 380, val loss: 0.815392792224884
Epoch 390, training loss: 0.07483430951833725 = 0.06810509413480759 + 0.001 * 6.7292160987854
Epoch 390, val loss: 0.8253665566444397
Epoch 400, training loss: 0.06758240610361099 = 0.06085662171244621 + 0.001 * 6.725786209106445
Epoch 400, val loss: 0.8354846239089966
Epoch 410, training loss: 0.0612589456140995 = 0.05453527718782425 + 0.001 * 6.723669528961182
Epoch 410, val loss: 0.8456563353538513
Epoch 420, training loss: 0.055735401809215546 = 0.04901634156703949 + 0.001 * 6.719060897827148
Epoch 420, val loss: 0.855857789516449
Epoch 430, training loss: 0.05090831220149994 = 0.04419206827878952 + 0.001 * 6.716242790222168
Epoch 430, val loss: 0.865993857383728
Epoch 440, training loss: 0.04668080806732178 = 0.03996914625167847 + 0.001 * 6.711662769317627
Epoch 440, val loss: 0.8760501146316528
Epoch 450, training loss: 0.04297957569360733 = 0.036267150193452835 + 0.001 * 6.712423801422119
Epoch 450, val loss: 0.886001706123352
Epoch 460, training loss: 0.03973042592406273 = 0.03301573172211647 + 0.001 * 6.714694976806641
Epoch 460, val loss: 0.8958690166473389
Epoch 470, training loss: 0.03685571998357773 = 0.030153458938002586 + 0.001 * 6.7022624015808105
Epoch 470, val loss: 0.9056092500686646
Epoch 480, training loss: 0.03431524336338043 = 0.02762753516435623 + 0.001 * 6.687705993652344
Epoch 480, val loss: 0.9151736497879028
Epoch 490, training loss: 0.032094284892082214 = 0.025391966104507446 + 0.001 * 6.702317714691162
Epoch 490, val loss: 0.9245753288269043
Epoch 500, training loss: 0.030092637985944748 = 0.023407358676195145 + 0.001 * 6.685277938842773
Epoch 500, val loss: 0.933792233467102
Epoch 510, training loss: 0.028322629630565643 = 0.02163991704583168 + 0.001 * 6.682713031768799
Epoch 510, val loss: 0.9428601264953613
Epoch 520, training loss: 0.026732472702860832 = 0.020061040297150612 + 0.001 * 6.671432018280029
Epoch 520, val loss: 0.9517046809196472
Epoch 530, training loss: 0.025319095700979233 = 0.018646124750375748 + 0.001 * 6.672971725463867
Epoch 530, val loss: 0.9603492021560669
Epoch 540, training loss: 0.02403675578534603 = 0.0173740703612566 + 0.001 * 6.662684440612793
Epoch 540, val loss: 0.9688233733177185
Epoch 550, training loss: 0.02288004755973816 = 0.016227100044488907 + 0.001 * 6.652947425842285
Epoch 550, val loss: 0.9771003127098083
Epoch 560, training loss: 0.021863700821995735 = 0.01518996711820364 + 0.001 * 6.673733711242676
Epoch 560, val loss: 0.9851969480514526
Epoch 570, training loss: 0.020902110263705254 = 0.014249485917389393 + 0.001 * 6.652624607086182
Epoch 570, val loss: 0.9930942058563232
Epoch 580, training loss: 0.02002924121916294 = 0.013394241221249104 + 0.001 * 6.635000228881836
Epoch 580, val loss: 1.0008107423782349
Epoch 590, training loss: 0.019272342324256897 = 0.012614557519555092 + 0.001 * 6.657785415649414
Epoch 590, val loss: 1.008337378501892
Epoch 600, training loss: 0.01854238659143448 = 0.011902108788490295 + 0.001 * 6.640276908874512
Epoch 600, val loss: 1.0156832933425903
Epoch 610, training loss: 0.017872491851449013 = 0.011249388568103313 + 0.001 * 6.62310266494751
Epoch 610, val loss: 1.022851824760437
Epoch 620, training loss: 0.017275895923376083 = 0.010649990290403366 + 0.001 * 6.625906467437744
Epoch 620, val loss: 1.029829740524292
Epoch 630, training loss: 0.01671852171421051 = 0.010098421946167946 + 0.001 * 6.620100498199463
Epoch 630, val loss: 1.0366491079330444
Epoch 640, training loss: 0.01620672270655632 = 0.009589760564267635 + 0.001 * 6.6169610023498535
Epoch 640, val loss: 1.0433142185211182
Epoch 650, training loss: 0.015733256936073303 = 0.0091196708381176 + 0.001 * 6.613585948944092
Epoch 650, val loss: 1.049808382987976
Epoch 660, training loss: 0.015311837196350098 = 0.008684585802257061 + 0.001 * 6.627250671386719
Epoch 660, val loss: 1.0561695098876953
Epoch 670, training loss: 0.014910432510077953 = 0.008281114511191845 + 0.001 * 6.629317760467529
Epoch 670, val loss: 1.0623655319213867
Epoch 680, training loss: 0.014519091695547104 = 0.007906211540102959 + 0.001 * 6.612879753112793
Epoch 680, val loss: 1.0684345960617065
Epoch 690, training loss: 0.014155711978673935 = 0.007557323202490807 + 0.001 * 6.598388671875
Epoch 690, val loss: 1.0743507146835327
Epoch 700, training loss: 0.013827677816152573 = 0.007232096511870623 + 0.001 * 6.595581531524658
Epoch 700, val loss: 1.0801347494125366
Epoch 710, training loss: 0.013537997379899025 = 0.00692847091704607 + 0.001 * 6.60952615737915
Epoch 710, val loss: 1.0857840776443481
Epoch 720, training loss: 0.01325390674173832 = 0.006644596345722675 + 0.001 * 6.609310150146484
Epoch 720, val loss: 1.091282844543457
Epoch 730, training loss: 0.012965626083314419 = 0.006378817372024059 + 0.001 * 6.586808204650879
Epoch 730, val loss: 1.0966792106628418
Epoch 740, training loss: 0.012729784473776817 = 0.006129617802798748 + 0.001 * 6.6001667976379395
Epoch 740, val loss: 1.1019216775894165
Epoch 750, training loss: 0.01247960701584816 = 0.005895687732845545 + 0.001 * 6.583919525146484
Epoch 750, val loss: 1.107072114944458
Epoch 760, training loss: 0.012272977270185947 = 0.0056757996790111065 + 0.001 * 6.597177505493164
Epoch 760, val loss: 1.1121017932891846
Epoch 770, training loss: 0.012064456939697266 = 0.005468863062560558 + 0.001 * 6.5955939292907715
Epoch 770, val loss: 1.1170308589935303
Epoch 780, training loss: 0.01186130940914154 = 0.005273859016597271 + 0.001 * 6.58745002746582
Epoch 780, val loss: 1.1218407154083252
Epoch 790, training loss: 0.0116879902780056 = 0.005089910700917244 + 0.001 * 6.598078727722168
Epoch 790, val loss: 1.1265391111373901
Epoch 800, training loss: 0.01150437630712986 = 0.0049161966890096664 + 0.001 * 6.588178634643555
Epoch 800, val loss: 1.1311390399932861
Epoch 810, training loss: 0.011339335702359676 = 0.004751992877572775 + 0.001 * 6.587342739105225
Epoch 810, val loss: 1.1356550455093384
Epoch 820, training loss: 0.011180892586708069 = 0.004596523474901915 + 0.001 * 6.584368705749512
Epoch 820, val loss: 1.1400727033615112
Epoch 830, training loss: 0.011024925857782364 = 0.004449028987437487 + 0.001 * 6.575897216796875
Epoch 830, val loss: 1.1444404125213623
Epoch 840, training loss: 0.01088004931807518 = 0.004309049341827631 + 0.001 * 6.570999622344971
Epoch 840, val loss: 1.148710012435913
Epoch 850, training loss: 0.010748244822025299 = 0.004176037851721048 + 0.001 * 6.572206497192383
Epoch 850, val loss: 1.1529030799865723
Epoch 860, training loss: 0.01062370091676712 = 0.004049570299685001 + 0.001 * 6.574130058288574
Epoch 860, val loss: 1.1569966077804565
Epoch 870, training loss: 0.010499387048184872 = 0.003929179161787033 + 0.001 * 6.570207595825195
Epoch 870, val loss: 1.161015510559082
Epoch 880, training loss: 0.010393521748483181 = 0.003814489347860217 + 0.001 * 6.5790324211120605
Epoch 880, val loss: 1.1649808883666992
Epoch 890, training loss: 0.01028485782444477 = 0.0037051094695925713 + 0.001 * 6.579747676849365
Epoch 890, val loss: 1.1688772439956665
Epoch 900, training loss: 0.010192816145718098 = 0.0036006667651236057 + 0.001 * 6.592149257659912
Epoch 900, val loss: 1.1727102994918823
Epoch 910, training loss: 0.010073383338749409 = 0.0035008343402296305 + 0.001 * 6.572548866271973
Epoch 910, val loss: 1.1764777898788452
Epoch 920, training loss: 0.009971771389245987 = 0.0034053842537105083 + 0.001 * 6.5663862228393555
Epoch 920, val loss: 1.1801857948303223
Epoch 930, training loss: 0.00987109076231718 = 0.003314021974802017 + 0.001 * 6.557068347930908
Epoch 930, val loss: 1.1838330030441284
Epoch 940, training loss: 0.009791887365281582 = 0.003226503496989608 + 0.001 * 6.5653839111328125
Epoch 940, val loss: 1.1874200105667114
Epoch 950, training loss: 0.009731264784932137 = 0.00314260832965374 + 0.001 * 6.588656425476074
Epoch 950, val loss: 1.1909571886062622
Epoch 960, training loss: 0.009627459570765495 = 0.0030621413607150316 + 0.001 * 6.5653181076049805
Epoch 960, val loss: 1.1944477558135986
Epoch 970, training loss: 0.009538772515952587 = 0.002984903287142515 + 0.001 * 6.553868770599365
Epoch 970, val loss: 1.1978870630264282
Epoch 980, training loss: 0.009479719214141369 = 0.0029107353184372187 + 0.001 * 6.568984031677246
Epoch 980, val loss: 1.2012765407562256
Epoch 990, training loss: 0.00939597375690937 = 0.0028394742403179407 + 0.001 * 6.556499481201172
Epoch 990, val loss: 1.2046175003051758
Epoch 1000, training loss: 0.00932665541768074 = 0.0027709761634469032 + 0.001 * 6.555678367614746
Epoch 1000, val loss: 1.2079041004180908
Epoch 1010, training loss: 0.009286802262067795 = 0.0027052070945501328 + 0.001 * 6.581594944000244
Epoch 1010, val loss: 1.2111388444900513
Epoch 1020, training loss: 0.009191910736262798 = 0.002642011269927025 + 0.001 * 6.549899101257324
Epoch 1020, val loss: 1.2143234014511108
Epoch 1030, training loss: 0.009171620942652225 = 0.0025812217500060797 + 0.001 * 6.590398788452148
Epoch 1030, val loss: 1.2174497842788696
Epoch 1040, training loss: 0.009070388041436672 = 0.0025227018631994724 + 0.001 * 6.5476861000061035
Epoch 1040, val loss: 1.2205359935760498
Epoch 1050, training loss: 0.00901787169277668 = 0.002466348698362708 + 0.001 * 6.551522731781006
Epoch 1050, val loss: 1.2235751152038574
Epoch 1060, training loss: 0.008962085470557213 = 0.002412080531939864 + 0.001 * 6.550004959106445
Epoch 1060, val loss: 1.2265669107437134
Epoch 1070, training loss: 0.008895311504602432 = 0.002359809121116996 + 0.001 * 6.5355024337768555
Epoch 1070, val loss: 1.2295053005218506
Epoch 1080, training loss: 0.008849916979670525 = 0.0023094513453543186 + 0.001 * 6.540465354919434
Epoch 1080, val loss: 1.232397198677063
Epoch 1090, training loss: 0.008803592063486576 = 0.0022608775179833174 + 0.001 * 6.542714595794678
Epoch 1090, val loss: 1.2352489233016968
Epoch 1100, training loss: 0.008783713914453983 = 0.002214031992480159 + 0.001 * 6.569681644439697
Epoch 1100, val loss: 1.2380547523498535
Epoch 1110, training loss: 0.008703438565135002 = 0.002168829319998622 + 0.001 * 6.534608840942383
Epoch 1110, val loss: 1.2408145666122437
Epoch 1120, training loss: 0.008663460612297058 = 0.002125194063410163 + 0.001 * 6.538266658782959
Epoch 1120, val loss: 1.2435262203216553
Epoch 1130, training loss: 0.008640432730317116 = 0.00208306685090065 + 0.001 * 6.557365894317627
Epoch 1130, val loss: 1.2462027072906494
Epoch 1140, training loss: 0.008581127971410751 = 0.0020424709655344486 + 0.001 * 6.538657188415527
Epoch 1140, val loss: 1.2488292455673218
Epoch 1150, training loss: 0.008553432300686836 = 0.002003233414143324 + 0.001 * 6.550198554992676
Epoch 1150, val loss: 1.2514097690582275
Epoch 1160, training loss: 0.008514238521456718 = 0.001965344650670886 + 0.001 * 6.548893928527832
Epoch 1160, val loss: 1.2539558410644531
Epoch 1170, training loss: 0.008459053933620453 = 0.0019287372706457973 + 0.001 * 6.5303168296813965
Epoch 1170, val loss: 1.2564454078674316
Epoch 1180, training loss: 0.008435675874352455 = 0.0018933583050966263 + 0.001 * 6.542316913604736
Epoch 1180, val loss: 1.258894443511963
Epoch 1190, training loss: 0.008406543172895908 = 0.0018591895932331681 + 0.001 * 6.5473527908325195
Epoch 1190, val loss: 1.2613017559051514
Epoch 1200, training loss: 0.008350714109838009 = 0.0018261338118463755 + 0.001 * 6.524580001831055
Epoch 1200, val loss: 1.2636750936508179
Epoch 1210, training loss: 0.008327579125761986 = 0.0017941207624971867 + 0.001 * 6.533458232879639
Epoch 1210, val loss: 1.2660108804702759
Epoch 1220, training loss: 0.008294562809169292 = 0.001763144857250154 + 0.001 * 6.531417369842529
Epoch 1220, val loss: 1.268316388130188
Epoch 1230, training loss: 0.008259517140686512 = 0.0017331655835732818 + 0.001 * 6.526350975036621
Epoch 1230, val loss: 1.2705777883529663
Epoch 1240, training loss: 0.008218166418373585 = 0.0017041455721482635 + 0.001 * 6.5140204429626465
Epoch 1240, val loss: 1.2727969884872437
Epoch 1250, training loss: 0.00818831380456686 = 0.001676079467870295 + 0.001 * 6.512234210968018
Epoch 1250, val loss: 1.2749865055084229
Epoch 1260, training loss: 0.008159958757460117 = 0.001648899633437395 + 0.001 * 6.511058807373047
Epoch 1260, val loss: 1.2771364450454712
Epoch 1270, training loss: 0.008140627294778824 = 0.0016225781291723251 + 0.001 * 6.518048286437988
Epoch 1270, val loss: 1.2792505025863647
Epoch 1280, training loss: 0.00817667506635189 = 0.0015970937674865127 + 0.001 * 6.579581260681152
Epoch 1280, val loss: 1.281319260597229
Epoch 1290, training loss: 0.008088478818535805 = 0.0015723963733762503 + 0.001 * 6.516081809997559
Epoch 1290, val loss: 1.2833524942398071
Epoch 1300, training loss: 0.008060748688876629 = 0.0015484316973015666 + 0.001 * 6.512317180633545
Epoch 1300, val loss: 1.2853577136993408
Epoch 1310, training loss: 0.00803862139582634 = 0.0015251693548634648 + 0.001 * 6.51345157623291
Epoch 1310, val loss: 1.2873398065567017
Epoch 1320, training loss: 0.008022974245250225 = 0.0015025908360257745 + 0.001 * 6.520382881164551
Epoch 1320, val loss: 1.2892892360687256
Epoch 1330, training loss: 0.008005251176655293 = 0.001480670878663659 + 0.001 * 6.524580001831055
Epoch 1330, val loss: 1.2912064790725708
Epoch 1340, training loss: 0.007965163327753544 = 0.001459379680454731 + 0.001 * 6.505783557891846
Epoch 1340, val loss: 1.2931005954742432
Epoch 1350, training loss: 0.007948914542794228 = 0.0014387249248102307 + 0.001 * 6.510189056396484
Epoch 1350, val loss: 1.2949471473693848
Epoch 1360, training loss: 0.007935279980301857 = 0.0014186687767505646 + 0.001 * 6.516611099243164
Epoch 1360, val loss: 1.2967774868011475
Epoch 1370, training loss: 0.007903354242444038 = 0.0013991789892315865 + 0.001 * 6.504174709320068
Epoch 1370, val loss: 1.2985762357711792
Epoch 1380, training loss: 0.007889583706855774 = 0.0013802354224026203 + 0.001 * 6.509348392486572
Epoch 1380, val loss: 1.3003510236740112
Epoch 1390, training loss: 0.007902749814093113 = 0.001361833536066115 + 0.001 * 6.540915489196777
Epoch 1390, val loss: 1.3020943403244019
Epoch 1400, training loss: 0.007855779491364956 = 0.0013439649483188987 + 0.001 * 6.511814117431641
Epoch 1400, val loss: 1.3038116693496704
Epoch 1410, training loss: 0.007840640842914581 = 0.0013265854213386774 + 0.001 * 6.514055252075195
Epoch 1410, val loss: 1.3054988384246826
Epoch 1420, training loss: 0.007824950851500034 = 0.0013096692273393273 + 0.001 * 6.5152812004089355
Epoch 1420, val loss: 1.3071736097335815
Epoch 1430, training loss: 0.007811413612216711 = 0.0012932406971231103 + 0.001 * 6.518172740936279
Epoch 1430, val loss: 1.3088219165802002
Epoch 1440, training loss: 0.007775628473609686 = 0.00127725163474679 + 0.001 * 6.498376369476318
Epoch 1440, val loss: 1.310450553894043
Epoch 1450, training loss: 0.0077617475762963295 = 0.001261706231161952 + 0.001 * 6.500041484832764
Epoch 1450, val loss: 1.312040090560913
Epoch 1460, training loss: 0.007789564318954945 = 0.0012465615291148424 + 0.001 * 6.543002605438232
Epoch 1460, val loss: 1.3136236667633057
Epoch 1470, training loss: 0.00774893444031477 = 0.0012318550143390894 + 0.001 * 6.517078876495361
Epoch 1470, val loss: 1.3151657581329346
Epoch 1480, training loss: 0.0077116223983466625 = 0.0012175258016213775 + 0.001 * 6.494096279144287
Epoch 1480, val loss: 1.3166900873184204
Epoch 1490, training loss: 0.007737180218100548 = 0.0012035834370180964 + 0.001 * 6.533596515655518
Epoch 1490, val loss: 1.318185567855835
Epoch 1500, training loss: 0.007714477833360434 = 0.0011899935780093074 + 0.001 * 6.524483680725098
Epoch 1500, val loss: 1.3196700811386108
Epoch 1510, training loss: 0.007659105118364096 = 0.001176754361949861 + 0.001 * 6.4823503494262695
Epoch 1510, val loss: 1.3211321830749512
Epoch 1520, training loss: 0.007651155814528465 = 0.0011638489086180925 + 0.001 * 6.487306594848633
Epoch 1520, val loss: 1.3225653171539307
Epoch 1530, training loss: 0.0076329708099365234 = 0.0011512890923768282 + 0.001 * 6.481681823730469
Epoch 1530, val loss: 1.323978066444397
Epoch 1540, training loss: 0.007621056865900755 = 0.001139055355452001 + 0.001 * 6.482001304626465
Epoch 1540, val loss: 1.3253798484802246
Epoch 1550, training loss: 0.007605120539665222 = 0.001127110794186592 + 0.001 * 6.478009223937988
Epoch 1550, val loss: 1.3267556428909302
Epoch 1560, training loss: 0.007610291708260775 = 0.0011154651874676347 + 0.001 * 6.494826316833496
Epoch 1560, val loss: 1.3281097412109375
Epoch 1570, training loss: 0.007614421658217907 = 0.001104104914702475 + 0.001 * 6.510316371917725
Epoch 1570, val loss: 1.329458236694336
Epoch 1580, training loss: 0.0075822267681360245 = 0.0010930399876087904 + 0.001 * 6.489186763763428
Epoch 1580, val loss: 1.3307793140411377
Epoch 1590, training loss: 0.007592115551233292 = 0.0010822201147675514 + 0.001 * 6.509895324707031
Epoch 1590, val loss: 1.3320780992507935
Epoch 1600, training loss: 0.007583719212561846 = 0.0010716840624809265 + 0.001 * 6.512034893035889
Epoch 1600, val loss: 1.3333700895309448
Epoch 1610, training loss: 0.007542298175394535 = 0.0010614144848659635 + 0.001 * 6.480883598327637
Epoch 1610, val loss: 1.3346247673034668
Epoch 1620, training loss: 0.0075367288663983345 = 0.0010513829765841365 + 0.001 * 6.485345363616943
Epoch 1620, val loss: 1.3358654975891113
Epoch 1630, training loss: 0.007520636543631554 = 0.0010415858123451471 + 0.001 * 6.479050159454346
Epoch 1630, val loss: 1.3370935916900635
Epoch 1640, training loss: 0.007531290873885155 = 0.001032031374052167 + 0.001 * 6.499258995056152
Epoch 1640, val loss: 1.33830726146698
Epoch 1650, training loss: 0.007515450473874807 = 0.0010226923041045666 + 0.001 * 6.492757797241211
Epoch 1650, val loss: 1.3395017385482788
Epoch 1660, training loss: 0.007484259083867073 = 0.001013565924949944 + 0.001 * 6.470693111419678
Epoch 1660, val loss: 1.3406862020492554
Epoch 1670, training loss: 0.007482199929654598 = 0.0010046574752777815 + 0.001 * 6.477542400360107
Epoch 1670, val loss: 1.3418515920639038
Epoch 1680, training loss: 0.00745934434235096 = 0.0009959356393665075 + 0.001 * 6.463408470153809
Epoch 1680, val loss: 1.3430087566375732
Epoch 1690, training loss: 0.007456229534000158 = 0.0009874152019619942 + 0.001 * 6.468813896179199
Epoch 1690, val loss: 1.3441517353057861
Epoch 1700, training loss: 0.007473520934581757 = 0.0009791005868464708 + 0.001 * 6.494420051574707
Epoch 1700, val loss: 1.345271348953247
Epoch 1710, training loss: 0.0074758040718734264 = 0.0009709675214253366 + 0.001 * 6.504836082458496
Epoch 1710, val loss: 1.346374750137329
Epoch 1720, training loss: 0.007439528591930866 = 0.0009629932465031743 + 0.001 * 6.476535320281982
Epoch 1720, val loss: 1.347474455833435
Epoch 1730, training loss: 0.007465970236808062 = 0.0009552114643156528 + 0.001 * 6.510758399963379
Epoch 1730, val loss: 1.3485575914382935
Epoch 1740, training loss: 0.007432976271957159 = 0.0009476090781390667 + 0.001 * 6.4853668212890625
Epoch 1740, val loss: 1.349611520767212
Epoch 1750, training loss: 0.007405966054648161 = 0.0009401594288647175 + 0.001 * 6.465806484222412
Epoch 1750, val loss: 1.3506642580032349
Epoch 1760, training loss: 0.007398178800940514 = 0.000932876777369529 + 0.001 * 6.465301513671875
Epoch 1760, val loss: 1.3517040014266968
Epoch 1770, training loss: 0.007394064217805862 = 0.0009257479105144739 + 0.001 * 6.468315601348877
Epoch 1770, val loss: 1.3527270555496216
Epoch 1780, training loss: 0.0073798662051558495 = 0.0009187671821564436 + 0.001 * 6.461098670959473
Epoch 1780, val loss: 1.3537293672561646
Epoch 1790, training loss: 0.007382165640592575 = 0.0009119327878579497 + 0.001 * 6.4702324867248535
Epoch 1790, val loss: 1.3547229766845703
Epoch 1800, training loss: 0.007405206561088562 = 0.0009052312816493213 + 0.001 * 6.499975204467773
Epoch 1800, val loss: 1.3557116985321045
Epoch 1810, training loss: 0.007366598583757877 = 0.0008987006731331348 + 0.001 * 6.467897415161133
Epoch 1810, val loss: 1.356663703918457
Epoch 1820, training loss: 0.007349582854658365 = 0.0008922987035475671 + 0.001 * 6.457283973693848
Epoch 1820, val loss: 1.3576184511184692
Epoch 1830, training loss: 0.007356392219662666 = 0.00088602484902367 + 0.001 * 6.470366954803467
Epoch 1830, val loss: 1.3585658073425293
Epoch 1840, training loss: 0.007339891046285629 = 0.0008798690978437662 + 0.001 * 6.46002197265625
Epoch 1840, val loss: 1.3595025539398193
Epoch 1850, training loss: 0.007365453988313675 = 0.0008738494943827391 + 0.001 * 6.491603851318359
Epoch 1850, val loss: 1.360421895980835
Epoch 1860, training loss: 0.00732794962823391 = 0.0008679441525600851 + 0.001 * 6.460005283355713
Epoch 1860, val loss: 1.3613172769546509
Epoch 1870, training loss: 0.007308658678084612 = 0.0008621517335996032 + 0.001 * 6.446506500244141
Epoch 1870, val loss: 1.3622145652770996
Epoch 1880, training loss: 0.007307641673833132 = 0.0008564697345718741 + 0.001 * 6.451171398162842
Epoch 1880, val loss: 1.3630932569503784
Epoch 1890, training loss: 0.007299932185560465 = 0.0008509064209647477 + 0.001 * 6.4490251541137695
Epoch 1890, val loss: 1.3639605045318604
Epoch 1900, training loss: 0.0073076942935585976 = 0.0008454457274638116 + 0.001 * 6.4622483253479
Epoch 1900, val loss: 1.36481511592865
Epoch 1910, training loss: 0.007348403334617615 = 0.0008401015074923635 + 0.001 * 6.508301734924316
Epoch 1910, val loss: 1.365667700767517
Epoch 1920, training loss: 0.007299610413610935 = 0.0008348426199518144 + 0.001 * 6.4647674560546875
Epoch 1920, val loss: 1.3665035963058472
Epoch 1930, training loss: 0.007265828084200621 = 0.0008296795422211289 + 0.001 * 6.436148166656494
Epoch 1930, val loss: 1.3673280477523804
Epoch 1940, training loss: 0.007271725218743086 = 0.0008246145443990827 + 0.001 * 6.447110652923584
Epoch 1940, val loss: 1.3681468963623047
Epoch 1950, training loss: 0.007272618822753429 = 0.0008196369744837284 + 0.001 * 6.452981472015381
Epoch 1950, val loss: 1.3689476251602173
Epoch 1960, training loss: 0.007279306650161743 = 0.0008147438056766987 + 0.001 * 6.46456241607666
Epoch 1960, val loss: 1.3697400093078613
Epoch 1970, training loss: 0.007288972847163677 = 0.0008099481929093599 + 0.001 * 6.4790239334106445
Epoch 1970, val loss: 1.37053382396698
Epoch 1980, training loss: 0.007263130508363247 = 0.0008052363991737366 + 0.001 * 6.4578938484191895
Epoch 1980, val loss: 1.3712897300720215
Epoch 1990, training loss: 0.007266984321177006 = 0.0008006118005141616 + 0.001 * 6.466372013092041
Epoch 1990, val loss: 1.3720531463623047
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8074
Overall ASR: 0.7897
Flip ASR: 0.7467/225 nodes
The final ASR:0.55105, 0.25623, Accuracy:0.81481, 0.00524
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11550])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10506])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
The final ASR:0.97663, 0.00174, Accuracy:0.82469, 0.00349
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9624308347702026 = 1.9540568590164185 + 0.001 * 8.373916625976562
Epoch 0, val loss: 1.9630436897277832
Epoch 10, training loss: 1.952771544456482 = 1.9443976879119873 + 0.001 * 8.373858451843262
Epoch 10, val loss: 1.9533888101577759
Epoch 20, training loss: 1.9406603574752808 = 1.9322867393493652 + 0.001 * 8.373658180236816
Epoch 20, val loss: 1.9411742687225342
Epoch 30, training loss: 1.9234340190887451 = 1.9150607585906982 + 0.001 * 8.37321662902832
Epoch 30, val loss: 1.9239829778671265
Epoch 40, training loss: 1.8979023694992065 = 1.8895301818847656 + 0.001 * 8.372197151184082
Epoch 40, val loss: 1.8989547491073608
Epoch 50, training loss: 1.861365795135498 = 1.8529963493347168 + 0.001 * 8.369422912597656
Epoch 50, val loss: 1.8644919395446777
Epoch 60, training loss: 1.8165501356124878 = 1.8081899881362915 + 0.001 * 8.360118865966797
Epoch 60, val loss: 1.8250820636749268
Epoch 70, training loss: 1.7733478546142578 = 1.7650278806686401 + 0.001 * 8.319976806640625
Epoch 70, val loss: 1.7888765335083008
Epoch 80, training loss: 1.7254709005355835 = 1.7173765897750854 + 0.001 * 8.094283103942871
Epoch 80, val loss: 1.7446659803390503
Epoch 90, training loss: 1.660770058631897 = 1.6529642343521118 + 0.001 * 7.805774211883545
Epoch 90, val loss: 1.686738133430481
Epoch 100, training loss: 1.5756820440292358 = 1.5679845809936523 + 0.001 * 7.6975202560424805
Epoch 100, val loss: 1.6152925491333008
Epoch 110, training loss: 1.4734234809875488 = 1.4658328294754028 + 0.001 * 7.590705394744873
Epoch 110, val loss: 1.5304561853408813
Epoch 120, training loss: 1.36543607711792 = 1.3580354452133179 + 0.001 * 7.400607585906982
Epoch 120, val loss: 1.441928505897522
Epoch 130, training loss: 1.2585346698760986 = 1.2513269186019897 + 0.001 * 7.2077226638793945
Epoch 130, val loss: 1.3545620441436768
Epoch 140, training loss: 1.1547695398330688 = 1.147646427154541 + 0.001 * 7.12306547164917
Epoch 140, val loss: 1.2712215185165405
Epoch 150, training loss: 1.057360053062439 = 1.050325870513916 + 0.001 * 7.03417444229126
Epoch 150, val loss: 1.1940910816192627
Epoch 160, training loss: 0.968549370765686 = 0.9615548849105835 + 0.001 * 6.994463920593262
Epoch 160, val loss: 1.1254746913909912
Epoch 170, training loss: 0.8872035145759583 = 0.8802332282066345 + 0.001 * 6.970274925231934
Epoch 170, val loss: 1.0641114711761475
Epoch 180, training loss: 0.8101592063903809 = 0.8032134175300598 + 0.001 * 6.945790767669678
Epoch 180, val loss: 1.0070799589157104
Epoch 190, training loss: 0.7354314923286438 = 0.7285041809082031 + 0.001 * 6.927331447601318
Epoch 190, val loss: 0.9525519609451294
Epoch 200, training loss: 0.6633898019790649 = 0.6564717888832092 + 0.001 * 6.918012619018555
Epoch 200, val loss: 0.9011566638946533
Epoch 210, training loss: 0.5956463813781738 = 0.588732123374939 + 0.001 * 6.914267063140869
Epoch 210, val loss: 0.8545297980308533
Epoch 220, training loss: 0.5332982540130615 = 0.5263861417770386 + 0.001 * 6.912118911743164
Epoch 220, val loss: 0.8138732314109802
Epoch 230, training loss: 0.47614091634750366 = 0.46923118829727173 + 0.001 * 6.909735679626465
Epoch 230, val loss: 0.7792461514472961
Epoch 240, training loss: 0.4232831299304962 = 0.4163760542869568 + 0.001 * 6.907083511352539
Epoch 240, val loss: 0.7501727342605591
Epoch 250, training loss: 0.3739507496356964 = 0.3670463562011719 + 0.001 * 6.904390811920166
Epoch 250, val loss: 0.7262329459190369
Epoch 260, training loss: 0.32792043685913086 = 0.3210185170173645 + 0.001 * 6.901922225952148
Epoch 260, val loss: 0.707016110420227
Epoch 270, training loss: 0.28546202182769775 = 0.27856215834617615 + 0.001 * 6.899848937988281
Epoch 270, val loss: 0.6923600435256958
Epoch 280, training loss: 0.2469344437122345 = 0.24003617465496063 + 0.001 * 6.898270130157471
Epoch 280, val loss: 0.6820822954177856
Epoch 290, training loss: 0.2127789556980133 = 0.2058812528848648 + 0.001 * 6.897709369659424
Epoch 290, val loss: 0.6755807399749756
Epoch 300, training loss: 0.18329019844532013 = 0.17639368772506714 + 0.001 * 6.896506309509277
Epoch 300, val loss: 0.6727954745292664
Epoch 310, training loss: 0.1584206372499466 = 0.1515245884656906 + 0.001 * 6.896043300628662
Epoch 310, val loss: 0.6732618808746338
Epoch 320, training loss: 0.1377747803926468 = 0.1308792382478714 + 0.001 * 6.8955488204956055
Epoch 320, val loss: 0.6765578985214233
Epoch 330, training loss: 0.12073470652103424 = 0.11383926123380661 + 0.001 * 6.895442485809326
Epoch 330, val loss: 0.6822631359100342
Epoch 340, training loss: 0.1066330224275589 = 0.0997384563088417 + 0.001 * 6.894562244415283
Epoch 340, val loss: 0.6898415088653564
Epoch 350, training loss: 0.09487639367580414 = 0.08798228949308395 + 0.001 * 6.894101619720459
Epoch 350, val loss: 0.6989595890045166
Epoch 360, training loss: 0.08499082177877426 = 0.07809711992740631 + 0.001 * 6.893702030181885
Epoch 360, val loss: 0.7092325687408447
Epoch 370, training loss: 0.07661322504281998 = 0.06972038000822067 + 0.001 * 6.892848014831543
Epoch 370, val loss: 0.7203243970870972
Epoch 380, training loss: 0.06945984810590744 = 0.06256845593452454 + 0.001 * 6.891391754150391
Epoch 380, val loss: 0.7320002317428589
Epoch 390, training loss: 0.06331562995910645 = 0.05642217397689819 + 0.001 * 6.893455982208252
Epoch 390, val loss: 0.743970513343811
Epoch 400, training loss: 0.057994596660137177 = 0.05110467970371246 + 0.001 * 6.889915466308594
Epoch 400, val loss: 0.756100058555603
Epoch 410, training loss: 0.05336136743426323 = 0.046474114060401917 + 0.001 * 6.887252330780029
Epoch 410, val loss: 0.768269956111908
Epoch 420, training loss: 0.049304232001304626 = 0.04241809993982315 + 0.001 * 6.886130332946777
Epoch 420, val loss: 0.780360758304596
Epoch 430, training loss: 0.04573214799165726 = 0.03884460777044296 + 0.001 * 6.887537956237793
Epoch 430, val loss: 0.7923904061317444
Epoch 440, training loss: 0.04256153106689453 = 0.03568021580576897 + 0.001 * 6.881314277648926
Epoch 440, val loss: 0.8042346239089966
Epoch 450, training loss: 0.03974458575248718 = 0.0328647717833519 + 0.001 * 6.879815578460693
Epoch 450, val loss: 0.8158867359161377
Epoch 460, training loss: 0.03722713515162468 = 0.03034858964383602 + 0.001 * 6.878546714782715
Epoch 460, val loss: 0.827354371547699
Epoch 470, training loss: 0.034968823194503784 = 0.028090868145227432 + 0.001 * 6.87795352935791
Epoch 470, val loss: 0.8386358618736267
Epoch 480, training loss: 0.03293281048536301 = 0.026057705283164978 + 0.001 * 6.875103950500488
Epoch 480, val loss: 0.8497292399406433
Epoch 490, training loss: 0.031092405319213867 = 0.024221163243055344 + 0.001 * 6.871240615844727
Epoch 490, val loss: 0.8606918454170227
Epoch 500, training loss: 0.02942436933517456 = 0.022557513788342476 + 0.001 * 6.866854667663574
Epoch 500, val loss: 0.871471643447876
Epoch 510, training loss: 0.02790789306163788 = 0.021046411246061325 + 0.001 * 6.8614821434021
Epoch 510, val loss: 0.8821024894714355
Epoch 520, training loss: 0.02652350254356861 = 0.01967068761587143 + 0.001 * 6.852814197540283
Epoch 520, val loss: 0.8925741910934448
Epoch 530, training loss: 0.025278395041823387 = 0.01841570995748043 + 0.001 * 6.862684726715088
Epoch 530, val loss: 0.9028817415237427
Epoch 540, training loss: 0.024129657074809074 = 0.017268825322389603 + 0.001 * 6.860831260681152
Epoch 540, val loss: 0.9130390882492065
Epoch 550, training loss: 0.02306552790105343 = 0.016219012439250946 + 0.001 * 6.84651517868042
Epoch 550, val loss: 0.9230210781097412
Epoch 560, training loss: 0.022090647369623184 = 0.015256436541676521 + 0.001 * 6.834210395812988
Epoch 560, val loss: 0.9328212141990662
Epoch 570, training loss: 0.021228443831205368 = 0.014372510835528374 + 0.001 * 6.855932712554932
Epoch 570, val loss: 0.942437469959259
Epoch 580, training loss: 0.02038617804646492 = 0.013559791259467602 + 0.001 * 6.82638692855835
Epoch 580, val loss: 0.9518728852272034
Epoch 590, training loss: 0.019633932039141655 = 0.012811345048248768 + 0.001 * 6.822587013244629
Epoch 590, val loss: 0.96112060546875
Epoch 600, training loss: 0.018941400572657585 = 0.012121143750846386 + 0.001 * 6.820257186889648
Epoch 600, val loss: 0.9701967835426331
Epoch 610, training loss: 0.01830177940428257 = 0.011483676731586456 + 0.001 * 6.81810188293457
Epoch 610, val loss: 0.9791070818901062
Epoch 620, training loss: 0.017716163769364357 = 0.01089410949498415 + 0.001 * 6.8220534324646
Epoch 620, val loss: 0.9878248572349548
Epoch 630, training loss: 0.01716257631778717 = 0.010348104871809483 + 0.001 * 6.814472198486328
Epoch 630, val loss: 0.9963622093200684
Epoch 640, training loss: 0.01665382832288742 = 0.009841758757829666 + 0.001 * 6.812069892883301
Epoch 640, val loss: 1.004737138748169
Epoch 650, training loss: 0.016173485666513443 = 0.009371652267873287 + 0.001 * 6.801833152770996
Epoch 650, val loss: 1.0129499435424805
Epoch 660, training loss: 0.015735406428575516 = 0.00893465243279934 + 0.001 * 6.800752639770508
Epoch 660, val loss: 1.0209763050079346
Epoch 670, training loss: 0.015334093943238258 = 0.00852790754288435 + 0.001 * 6.806186199188232
Epoch 670, val loss: 1.0287933349609375
Epoch 680, training loss: 0.014957107603549957 = 0.008148767054080963 + 0.001 * 6.808339595794678
Epoch 680, val loss: 1.0364859104156494
Epoch 690, training loss: 0.01459664385765791 = 0.007794898934662342 + 0.001 * 6.80174446105957
Epoch 690, val loss: 1.0440055131912231
Epoch 700, training loss: 0.014264263212680817 = 0.007464190479367971 + 0.001 * 6.800071716308594
Epoch 700, val loss: 1.0513606071472168
Epoch 710, training loss: 0.013948490843176842 = 0.007154806051403284 + 0.001 * 6.793684959411621
Epoch 710, val loss: 1.058599591255188
Epoch 720, training loss: 0.013662558048963547 = 0.006865052040666342 + 0.001 * 6.797505855560303
Epoch 720, val loss: 1.0656579732894897
Epoch 730, training loss: 0.013390801846981049 = 0.006593324244022369 + 0.001 * 6.797476768493652
Epoch 730, val loss: 1.072587251663208
Epoch 740, training loss: 0.013125527650117874 = 0.006338221486657858 + 0.001 * 6.787306308746338
Epoch 740, val loss: 1.079349160194397
Epoch 750, training loss: 0.01288391463458538 = 0.0060983337461948395 + 0.001 * 6.785580158233643
Epoch 750, val loss: 1.085972785949707
Epoch 760, training loss: 0.012657850049436092 = 0.005872557405382395 + 0.001 * 6.785292148590088
Epoch 760, val loss: 1.0924564599990845
Epoch 770, training loss: 0.01244550384581089 = 0.0056598735973238945 + 0.001 * 6.785630226135254
Epoch 770, val loss: 1.0988256931304932
Epoch 780, training loss: 0.012246858328580856 = 0.005459336563944817 + 0.001 * 6.787521839141846
Epoch 780, val loss: 1.1050430536270142
Epoch 790, training loss: 0.012053868733346462 = 0.005270079709589481 + 0.001 * 6.783788681030273
Epoch 790, val loss: 1.1111506223678589
Epoch 800, training loss: 0.011883733794093132 = 0.00509129511192441 + 0.001 * 6.792438507080078
Epoch 800, val loss: 1.1171393394470215
Epoch 810, training loss: 0.011697234585881233 = 0.0049222013913095 + 0.001 * 6.7750325202941895
Epoch 810, val loss: 1.1229934692382812
Epoch 820, training loss: 0.01153729110956192 = 0.004762147553265095 + 0.001 * 6.775143623352051
Epoch 820, val loss: 1.1287492513656616
Epoch 830, training loss: 0.01140693947672844 = 0.004610528703778982 + 0.001 * 6.796411037445068
Epoch 830, val loss: 1.1343786716461182
Epoch 840, training loss: 0.011246774345636368 = 0.004466741345822811 + 0.001 * 6.780033111572266
Epoch 840, val loss: 1.1398999691009521
Epoch 850, training loss: 0.011104872450232506 = 0.0043302844278514385 + 0.001 * 6.774587154388428
Epoch 850, val loss: 1.1453073024749756
Epoch 860, training loss: 0.011003263294696808 = 0.004200679250061512 + 0.001 * 6.802583694458008
Epoch 860, val loss: 1.1506489515304565
Epoch 870, training loss: 0.010846195742487907 = 0.004077445715665817 + 0.001 * 6.768750190734863
Epoch 870, val loss: 1.1558886766433716
Epoch 880, training loss: 0.010727006942033768 = 0.003960215952247381 + 0.001 * 6.766790390014648
Epoch 880, val loss: 1.1609890460968018
Epoch 890, training loss: 0.01064983569085598 = 0.0038485880941152573 + 0.001 * 6.8012471199035645
Epoch 890, val loss: 1.1660239696502686
Epoch 900, training loss: 0.010505881160497665 = 0.0037422035820782185 + 0.001 * 6.76367712020874
Epoch 900, val loss: 1.1709707975387573
Epoch 910, training loss: 0.010403137654066086 = 0.0036407585721462965 + 0.001 * 6.762378692626953
Epoch 910, val loss: 1.175815224647522
Epoch 920, training loss: 0.010325323790311813 = 0.003543962026014924 + 0.001 * 6.7813615798950195
Epoch 920, val loss: 1.1805864572525024
Epoch 930, training loss: 0.010216392576694489 = 0.003451523371040821 + 0.001 * 6.76486873626709
Epoch 930, val loss: 1.1852726936340332
Epoch 940, training loss: 0.010163065046072006 = 0.003363196738064289 + 0.001 * 6.799868106842041
Epoch 940, val loss: 1.1898409128189087
Epoch 950, training loss: 0.010039182379841805 = 0.0032787376549094915 + 0.001 * 6.760444641113281
Epoch 950, val loss: 1.194357991218567
Epoch 960, training loss: 0.009958704933524132 = 0.0031979482155293226 + 0.001 * 6.760756015777588
Epoch 960, val loss: 1.1987992525100708
Epoch 970, training loss: 0.009901618584990501 = 0.0031205913983285427 + 0.001 * 6.781026840209961
Epoch 970, val loss: 1.2031348943710327
Epoch 980, training loss: 0.00979597307741642 = 0.00304647465236485 + 0.001 * 6.749497890472412
Epoch 980, val loss: 1.2074304819107056
Epoch 990, training loss: 0.009729384444653988 = 0.0029754501301795244 + 0.001 * 6.753933906555176
Epoch 990, val loss: 1.2116484642028809
Epoch 1000, training loss: 0.009677249938249588 = 0.00290733203291893 + 0.001 * 6.769917011260986
Epoch 1000, val loss: 1.215790867805481
Epoch 1010, training loss: 0.009593777358531952 = 0.0028419592417776585 + 0.001 * 6.751817226409912
Epoch 1010, val loss: 1.2198638916015625
Epoch 1020, training loss: 0.0095293577760458 = 0.002779207890853286 + 0.001 * 6.750149726867676
Epoch 1020, val loss: 1.2238636016845703
Epoch 1030, training loss: 0.00946980994194746 = 0.002718917327001691 + 0.001 * 6.750892162322998
Epoch 1030, val loss: 1.2278077602386475
Epoch 1040, training loss: 0.009404163807630539 = 0.0026609713677316904 + 0.001 * 6.743191719055176
Epoch 1040, val loss: 1.231688380241394
Epoch 1050, training loss: 0.009360034018754959 = 0.0026052608154714108 + 0.001 * 6.754772663116455
Epoch 1050, val loss: 1.2355244159698486
Epoch 1060, training loss: 0.009298174642026424 = 0.0025516555178910494 + 0.001 * 6.746519088745117
Epoch 1060, val loss: 1.2392823696136475
Epoch 1070, training loss: 0.009266147390007973 = 0.0025000562891364098 + 0.001 * 6.766091346740723
Epoch 1070, val loss: 1.2429546117782593
Epoch 1080, training loss: 0.009194234386086464 = 0.0024503611493855715 + 0.001 * 6.74387264251709
Epoch 1080, val loss: 1.2466011047363281
Epoch 1090, training loss: 0.009150872938334942 = 0.0024024825543165207 + 0.001 * 6.748390197753906
Epoch 1090, val loss: 1.250159502029419
Epoch 1100, training loss: 0.009102856740355492 = 0.0023563397116959095 + 0.001 * 6.746516704559326
Epoch 1100, val loss: 1.2536944150924683
Epoch 1110, training loss: 0.009066520258784294 = 0.0023118555545806885 + 0.001 * 6.754663944244385
Epoch 1110, val loss: 1.2571682929992676
Epoch 1120, training loss: 0.00900503620505333 = 0.0022689339239150286 + 0.001 * 6.736102104187012
Epoch 1120, val loss: 1.2605772018432617
Epoch 1130, training loss: 0.00895597506314516 = 0.002227501245215535 + 0.001 * 6.72847318649292
Epoch 1130, val loss: 1.2639415264129639
Epoch 1140, training loss: 0.008913703262805939 = 0.0021875123493373394 + 0.001 * 6.726190090179443
Epoch 1140, val loss: 1.2672737836837769
Epoch 1150, training loss: 0.00889088399708271 = 0.002148896222934127 + 0.001 * 6.741987228393555
Epoch 1150, val loss: 1.270526647567749
Epoch 1160, training loss: 0.008856390602886677 = 0.0021115734707564116 + 0.001 * 6.74481725692749
Epoch 1160, val loss: 1.2737168073654175
Epoch 1170, training loss: 0.00880681537091732 = 0.0020754996221512556 + 0.001 * 6.7313151359558105
Epoch 1170, val loss: 1.2768809795379639
Epoch 1180, training loss: 0.008784884586930275 = 0.0020406434778124094 + 0.001 * 6.744241237640381
Epoch 1180, val loss: 1.2799984216690063
Epoch 1190, training loss: 0.008733025752007961 = 0.0020069165620952845 + 0.001 * 6.726108551025391
Epoch 1190, val loss: 1.2830456495285034
Epoch 1200, training loss: 0.008694140240550041 = 0.0019742934964597225 + 0.001 * 6.719846248626709
Epoch 1200, val loss: 1.286069393157959
Epoch 1210, training loss: 0.008661841042339802 = 0.0019426974467933178 + 0.001 * 6.719143390655518
Epoch 1210, val loss: 1.2890491485595703
Epoch 1220, training loss: 0.008618508465588093 = 0.0019120860379189253 + 0.001 * 6.706421852111816
Epoch 1220, val loss: 1.2919814586639404
Epoch 1230, training loss: 0.008583301678299904 = 0.001882438431493938 + 0.001 * 6.700863361358643
Epoch 1230, val loss: 1.2948932647705078
Epoch 1240, training loss: 0.008576527237892151 = 0.0018537171417847276 + 0.001 * 6.722809791564941
Epoch 1240, val loss: 1.2977463006973267
Epoch 1250, training loss: 0.008534012362360954 = 0.0018258520867675543 + 0.001 * 6.708160400390625
Epoch 1250, val loss: 1.3005468845367432
Epoch 1260, training loss: 0.008507686667144299 = 0.0017988629406318069 + 0.001 * 6.708823204040527
Epoch 1260, val loss: 1.3032962083816528
Epoch 1270, training loss: 0.008488538675010204 = 0.0017726687947288156 + 0.001 * 6.715869426727295
Epoch 1270, val loss: 1.306053876876831
Epoch 1280, training loss: 0.008437925018370152 = 0.0017472609179094434 + 0.001 * 6.6906633377075195
Epoch 1280, val loss: 1.3087584972381592
Epoch 1290, training loss: 0.008426367305219173 = 0.0017226040363311768 + 0.001 * 6.703763008117676
Epoch 1290, val loss: 1.3113763332366943
Epoch 1300, training loss: 0.008406395092606544 = 0.0016986601985991001 + 0.001 * 6.707734107971191
Epoch 1300, val loss: 1.3140246868133545
Epoch 1310, training loss: 0.008368687704205513 = 0.0016754106618463993 + 0.001 * 6.693276405334473
Epoch 1310, val loss: 1.3165700435638428
Epoch 1320, training loss: 0.008378151804208755 = 0.0016528276028111577 + 0.001 * 6.7253241539001465
Epoch 1320, val loss: 1.3191630840301514
Epoch 1330, training loss: 0.008335844613611698 = 0.0016308884369209409 + 0.001 * 6.704955577850342
Epoch 1330, val loss: 1.3216502666473389
Epoch 1340, training loss: 0.008314907550811768 = 0.0016095628961920738 + 0.001 * 6.705344200134277
Epoch 1340, val loss: 1.3241255283355713
Epoch 1350, training loss: 0.008286349475383759 = 0.001588833867572248 + 0.001 * 6.69751501083374
Epoch 1350, val loss: 1.3265665769577026
Epoch 1360, training loss: 0.008292410522699356 = 0.0015686859842389822 + 0.001 * 6.723723888397217
Epoch 1360, val loss: 1.3289623260498047
Epoch 1370, training loss: 0.008248638361692429 = 0.0015490816440433264 + 0.001 * 6.699556350708008
Epoch 1370, val loss: 1.3313497304916382
Epoch 1380, training loss: 0.008215034380555153 = 0.0015300107188522816 + 0.001 * 6.685022830963135
Epoch 1380, val loss: 1.3337100744247437
Epoch 1390, training loss: 0.008188275620341301 = 0.001511447480879724 + 0.001 * 6.676827907562256
Epoch 1390, val loss: 1.33604097366333
Epoch 1400, training loss: 0.008188324049115181 = 0.0014933772617951035 + 0.001 * 6.6949462890625
Epoch 1400, val loss: 1.3382991552352905
Epoch 1410, training loss: 0.008181416429579258 = 0.0014757791068404913 + 0.001 * 6.705637454986572
Epoch 1410, val loss: 1.3405792713165283
Epoch 1420, training loss: 0.008160402998328209 = 0.0014586374163627625 + 0.001 * 6.701765060424805
Epoch 1420, val loss: 1.3428159952163696
Epoch 1430, training loss: 0.008112634532153606 = 0.0014419449726119637 + 0.001 * 6.670689582824707
Epoch 1430, val loss: 1.3450125455856323
Epoch 1440, training loss: 0.008125110529363155 = 0.0014256831491366029 + 0.001 * 6.699426651000977
Epoch 1440, val loss: 1.3471981287002563
Epoch 1450, training loss: 0.00810826476663351 = 0.001409849850460887 + 0.001 * 6.6984148025512695
Epoch 1450, val loss: 1.3493263721466064
Epoch 1460, training loss: 0.008120733313262463 = 0.0013944007223472 + 0.001 * 6.726332664489746
Epoch 1460, val loss: 1.3514463901519775
Epoch 1470, training loss: 0.008062724024057388 = 0.0013793536927551031 + 0.001 * 6.683370113372803
Epoch 1470, val loss: 1.353545904159546
Epoch 1480, training loss: 0.0080549456179142 = 0.0013646695297211409 + 0.001 * 6.6902756690979
Epoch 1480, val loss: 1.355619192123413
Epoch 1490, training loss: 0.00800989381968975 = 0.0013503632508218288 + 0.001 * 6.659530162811279
Epoch 1490, val loss: 1.3576453924179077
Epoch 1500, training loss: 0.008011335507035255 = 0.0013364023761823773 + 0.001 * 6.674932956695557
Epoch 1500, val loss: 1.359670877456665
Epoch 1510, training loss: 0.00798741728067398 = 0.0013227825984358788 + 0.001 * 6.664634704589844
Epoch 1510, val loss: 1.3616812229156494
Epoch 1520, training loss: 0.0080065643414855 = 0.0013094968162477016 + 0.001 * 6.6970672607421875
Epoch 1520, val loss: 1.3636302947998047
Epoch 1530, training loss: 0.00795871764421463 = 0.0012965210480615497 + 0.001 * 6.662196636199951
Epoch 1530, val loss: 1.3655915260314941
Epoch 1540, training loss: 0.007940387353301048 = 0.0012838550610467792 + 0.001 * 6.656531810760498
Epoch 1540, val loss: 1.3675109148025513
Epoch 1550, training loss: 0.007913827896118164 = 0.0012714852346107364 + 0.001 * 6.6423420906066895
Epoch 1550, val loss: 1.3694086074829102
Epoch 1560, training loss: 0.007906630635261536 = 0.0012594100553542376 + 0.001 * 6.647220611572266
Epoch 1560, val loss: 1.37128484249115
Epoch 1570, training loss: 0.007884068414568901 = 0.0012475994881242514 + 0.001 * 6.636468887329102
Epoch 1570, val loss: 1.3731591701507568
Epoch 1580, training loss: 0.007899937219917774 = 0.0012360604014247656 + 0.001 * 6.663876533508301
Epoch 1580, val loss: 1.3750026226043701
Epoch 1590, training loss: 0.007879496552050114 = 0.0012247773120179772 + 0.001 * 6.65471887588501
Epoch 1590, val loss: 1.3767919540405273
Epoch 1600, training loss: 0.00785087700933218 = 0.0012137499870732427 + 0.001 * 6.637126922607422
Epoch 1600, val loss: 1.3786098957061768
Epoch 1610, training loss: 0.00786343403160572 = 0.001202974235638976 + 0.001 * 6.660459041595459
Epoch 1610, val loss: 1.3803675174713135
Epoch 1620, training loss: 0.007854778319597244 = 0.001192426891066134 + 0.001 * 6.662350654602051
Epoch 1620, val loss: 1.382137656211853
Epoch 1630, training loss: 0.007853328250348568 = 0.0011821327498182654 + 0.001 * 6.6711955070495605
Epoch 1630, val loss: 1.3838679790496826
Epoch 1640, training loss: 0.007839684374630451 = 0.0011720455950126052 + 0.001 * 6.667638301849365
Epoch 1640, val loss: 1.3855539560317993
Epoch 1650, training loss: 0.007818746380507946 = 0.0011621996527537704 + 0.001 * 6.656546592712402
Epoch 1650, val loss: 1.3872815370559692
Epoch 1660, training loss: 0.007817069068551064 = 0.0011525541776791215 + 0.001 * 6.664514064788818
Epoch 1660, val loss: 1.3889122009277344
Epoch 1670, training loss: 0.007786762900650501 = 0.0011431288439780474 + 0.001 * 6.643633842468262
Epoch 1670, val loss: 1.3905982971191406
Epoch 1680, training loss: 0.00781448744237423 = 0.0011338888434693217 + 0.001 * 6.680598735809326
Epoch 1680, val loss: 1.392187237739563
Epoch 1690, training loss: 0.0077858176082372665 = 0.0011248409282416105 + 0.001 * 6.660975933074951
Epoch 1690, val loss: 1.3938322067260742
Epoch 1700, training loss: 0.007775552570819855 = 0.0011159921996295452 + 0.001 * 6.659560203552246
Epoch 1700, val loss: 1.39542555809021
Epoch 1710, training loss: 0.0077452706173062325 = 0.0011073246132582426 + 0.001 * 6.637946128845215
Epoch 1710, val loss: 1.3970351219177246
Epoch 1720, training loss: 0.007766502909362316 = 0.0010988208232447505 + 0.001 * 6.667681694030762
Epoch 1720, val loss: 1.398537516593933
Epoch 1730, training loss: 0.007715667597949505 = 0.0010905064409598708 + 0.001 * 6.6251606941223145
Epoch 1730, val loss: 1.400111198425293
Epoch 1740, training loss: 0.007771775592118502 = 0.001082363072782755 + 0.001 * 6.6894121170043945
Epoch 1740, val loss: 1.401620864868164
Epoch 1750, training loss: 0.007700804620981216 = 0.00107436696998775 + 0.001 * 6.626437187194824
Epoch 1750, val loss: 1.4031579494476318
Epoch 1760, training loss: 0.0077234962955117226 = 0.0010665460722520947 + 0.001 * 6.656949996948242
Epoch 1760, val loss: 1.4046540260314941
Epoch 1770, training loss: 0.007692850194871426 = 0.00105886475648731 + 0.001 * 6.6339850425720215
Epoch 1770, val loss: 1.4061230421066284
Epoch 1780, training loss: 0.00767731387168169 = 0.0010513472370803356 + 0.001 * 6.625966548919678
Epoch 1780, val loss: 1.40761399269104
Epoch 1790, training loss: 0.007682629395276308 = 0.0010439730249345303 + 0.001 * 6.638656139373779
Epoch 1790, val loss: 1.409053921699524
Epoch 1800, training loss: 0.007646376732736826 = 0.0010367329232394695 + 0.001 * 6.609643459320068
Epoch 1800, val loss: 1.4105054140090942
Epoch 1810, training loss: 0.00764847919344902 = 0.0010296383406966925 + 0.001 * 6.618840217590332
Epoch 1810, val loss: 1.4119024276733398
Epoch 1820, training loss: 0.007652895990759134 = 0.001022674492560327 + 0.001 * 6.630220890045166
Epoch 1820, val loss: 1.413344144821167
Epoch 1830, training loss: 0.007648044265806675 = 0.00101584498770535 + 0.001 * 6.632199287414551
Epoch 1830, val loss: 1.4147270917892456
Epoch 1840, training loss: 0.0076364753767848015 = 0.0010091383010149002 + 0.001 * 6.6273369789123535
Epoch 1840, val loss: 1.4161148071289062
Epoch 1850, training loss: 0.007613050751388073 = 0.0010025615338236094 + 0.001 * 6.6104888916015625
Epoch 1850, val loss: 1.4174751043319702
Epoch 1860, training loss: 0.007588598877191544 = 0.000996097456663847 + 0.001 * 6.592501163482666
Epoch 1860, val loss: 1.4188385009765625
Epoch 1870, training loss: 0.007597540505230427 = 0.0009897493291646242 + 0.001 * 6.607790946960449
Epoch 1870, val loss: 1.4201985597610474
Epoch 1880, training loss: 0.007621559780091047 = 0.0009835184318944812 + 0.001 * 6.638041019439697
Epoch 1880, val loss: 1.4215564727783203
Epoch 1890, training loss: 0.00761010404676199 = 0.0009773931233212352 + 0.001 * 6.6327104568481445
Epoch 1890, val loss: 1.422868013381958
Epoch 1900, training loss: 0.007607945706695318 = 0.0009713765466585755 + 0.001 * 6.636569023132324
Epoch 1900, val loss: 1.4242181777954102
Epoch 1910, training loss: 0.007586627267301083 = 0.0009654717869125307 + 0.001 * 6.621155261993408
Epoch 1910, val loss: 1.425495982170105
Epoch 1920, training loss: 0.007568239700049162 = 0.0009596632444299757 + 0.001 * 6.60857629776001
Epoch 1920, val loss: 1.4267897605895996
Epoch 1930, training loss: 0.007539483718574047 = 0.0009539567399770021 + 0.001 * 6.585526466369629
Epoch 1930, val loss: 1.4281120300292969
Epoch 1940, training loss: 0.007560404017567635 = 0.0009483573958277702 + 0.001 * 6.612046241760254
Epoch 1940, val loss: 1.42935311794281
Epoch 1950, training loss: 0.007549588102847338 = 0.0009428519406355917 + 0.001 * 6.606735706329346
Epoch 1950, val loss: 1.4306403398513794
Epoch 1960, training loss: 0.007528964895755053 = 0.0009374426444992423 + 0.001 * 6.591521739959717
Epoch 1960, val loss: 1.4318931102752686
Epoch 1970, training loss: 0.007516591344028711 = 0.0009321136749349535 + 0.001 * 6.584477424621582
Epoch 1970, val loss: 1.4331421852111816
Epoch 1980, training loss: 0.007546336855739355 = 0.0009268919820897281 + 0.001 * 6.619444847106934
Epoch 1980, val loss: 1.4343446493148804
Epoch 1990, training loss: 0.007542962208390236 = 0.0009217465412802994 + 0.001 * 6.621215343475342
Epoch 1990, val loss: 1.435570478439331
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.7380
Flip ASR: 0.6844/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9504685401916504 = 1.9420946836471558 + 0.001 * 8.373818397521973
Epoch 0, val loss: 1.943817377090454
Epoch 10, training loss: 1.9400593042373657 = 1.9316855669021606 + 0.001 * 8.37370491027832
Epoch 10, val loss: 1.9324290752410889
Epoch 20, training loss: 1.9272533655166626 = 1.9188799858093262 + 0.001 * 8.373323440551758
Epoch 20, val loss: 1.9182651042938232
Epoch 30, training loss: 1.909379005432129 = 1.9010065793991089 + 0.001 * 8.372422218322754
Epoch 30, val loss: 1.8984551429748535
Epoch 40, training loss: 1.8835073709487915 = 1.8751370906829834 + 0.001 * 8.370288848876953
Epoch 40, val loss: 1.8700594902038574
Epoch 50, training loss: 1.8478375673294067 = 1.839473009109497 + 0.001 * 8.36453914642334
Epoch 50, val loss: 1.8320868015289307
Epoch 60, training loss: 1.805924654006958 = 1.7975797653198242 + 0.001 * 8.344900131225586
Epoch 60, val loss: 1.7902398109436035
Epoch 70, training loss: 1.7644399404525757 = 1.7561898231506348 + 0.001 * 8.25006103515625
Epoch 70, val loss: 1.7516273260116577
Epoch 80, training loss: 1.7148799896240234 = 1.7069828510284424 + 0.001 * 7.8971405029296875
Epoch 80, val loss: 1.7066338062286377
Epoch 90, training loss: 1.6472070217132568 = 1.6397271156311035 + 0.001 * 7.479895114898682
Epoch 90, val loss: 1.6479976177215576
Epoch 100, training loss: 1.5578702688217163 = 1.550579309463501 + 0.001 * 7.290925979614258
Epoch 100, val loss: 1.572810411453247
Epoch 110, training loss: 1.451973557472229 = 1.444772481918335 + 0.001 * 7.201103210449219
Epoch 110, val loss: 1.4853579998016357
Epoch 120, training loss: 1.3402471542358398 = 1.333099603652954 + 0.001 * 7.147498607635498
Epoch 120, val loss: 1.3958452939987183
Epoch 130, training loss: 1.2298924922943115 = 1.2227826118469238 + 0.001 * 7.109858989715576
Epoch 130, val loss: 1.3107762336730957
Epoch 140, training loss: 1.122071385383606 = 1.114996075630188 + 0.001 * 7.075336933135986
Epoch 140, val loss: 1.2299606800079346
Epoch 150, training loss: 1.0160832405090332 = 1.0090584754943848 + 0.001 * 7.024718761444092
Epoch 150, val loss: 1.151617169380188
Epoch 160, training loss: 0.9144523739814758 = 0.9074894785881042 + 0.001 * 6.962893009185791
Epoch 160, val loss: 1.0786573886871338
Epoch 170, training loss: 0.8221260905265808 = 0.8152075409889221 + 0.001 * 6.9185333251953125
Epoch 170, val loss: 1.0144283771514893
Epoch 180, training loss: 0.7425685524940491 = 0.7356638312339783 + 0.001 * 6.904725551605225
Epoch 180, val loss: 0.961618959903717
Epoch 190, training loss: 0.6754661202430725 = 0.6685677170753479 + 0.001 * 6.898417949676514
Epoch 190, val loss: 0.9196886420249939
Epoch 200, training loss: 0.6179103851318359 = 0.6110156178474426 + 0.001 * 6.894766330718994
Epoch 200, val loss: 0.886124312877655
Epoch 210, training loss: 0.5668581128120422 = 0.5599665641784668 + 0.001 * 6.891561985015869
Epoch 210, val loss: 0.8582814931869507
Epoch 220, training loss: 0.520315945148468 = 0.5134278535842896 + 0.001 * 6.888108730316162
Epoch 220, val loss: 0.8347328305244446
Epoch 230, training loss: 0.477206289768219 = 0.47032278776168823 + 0.001 * 6.883488178253174
Epoch 230, val loss: 0.8154129981994629
Epoch 240, training loss: 0.4369160234928131 = 0.43003809452056885 + 0.001 * 6.8779191970825195
Epoch 240, val loss: 0.7996636033058167
Epoch 250, training loss: 0.3988868296146393 = 0.39201489090919495 + 0.001 * 6.871934413909912
Epoch 250, val loss: 0.7869292497634888
Epoch 260, training loss: 0.3627611994743347 = 0.35589486360549927 + 0.001 * 6.866339206695557
Epoch 260, val loss: 0.7767703533172607
Epoch 270, training loss: 0.32844117283821106 = 0.3215814232826233 + 0.001 * 6.8597588539123535
Epoch 270, val loss: 0.7685595750808716
Epoch 280, training loss: 0.29578331112861633 = 0.28893011808395386 + 0.001 * 6.853199005126953
Epoch 280, val loss: 0.7619580626487732
Epoch 290, training loss: 0.2648014426231384 = 0.2579532861709595 + 0.001 * 6.848141670227051
Epoch 290, val loss: 0.7570687532424927
Epoch 300, training loss: 0.23555396497249603 = 0.2287151962518692 + 0.001 * 6.838761806488037
Epoch 300, val loss: 0.753753125667572
Epoch 310, training loss: 0.20839792490005493 = 0.20156742632389069 + 0.001 * 6.830491542816162
Epoch 310, val loss: 0.751960813999176
Epoch 320, training loss: 0.1832362413406372 = 0.17641505599021912 + 0.001 * 6.821183681488037
Epoch 320, val loss: 0.751304566860199
Epoch 330, training loss: 0.16023148596286774 = 0.15341734886169434 + 0.001 * 6.814141273498535
Epoch 330, val loss: 0.7525714635848999
Epoch 340, training loss: 0.13995684683322906 = 0.13314875960350037 + 0.001 * 6.808084011077881
Epoch 340, val loss: 0.7564414739608765
Epoch 350, training loss: 0.12271023541688919 = 0.11590741574764252 + 0.001 * 6.802819728851318
Epoch 350, val loss: 0.7636547088623047
Epoch 360, training loss: 0.10818863660097122 = 0.10139342397451401 + 0.001 * 6.795214653015137
Epoch 360, val loss: 0.7733259797096252
Epoch 370, training loss: 0.09596079587936401 = 0.08917202800512314 + 0.001 * 6.788766384124756
Epoch 370, val loss: 0.7850871086120605
Epoch 380, training loss: 0.08566669374704361 = 0.07887880504131317 + 0.001 * 6.787890911102295
Epoch 380, val loss: 0.7978766560554504
Epoch 390, training loss: 0.07692477107048035 = 0.07014237344264984 + 0.001 * 6.782397270202637
Epoch 390, val loss: 0.811334490776062
Epoch 400, training loss: 0.06942335516214371 = 0.06264179199934006 + 0.001 * 6.78156042098999
Epoch 400, val loss: 0.8253059387207031
Epoch 410, training loss: 0.06291033327579498 = 0.05612735077738762 + 0.001 * 6.78298282623291
Epoch 410, val loss: 0.8395474553108215
Epoch 420, training loss: 0.057212162762880325 = 0.050433527678251266 + 0.001 * 6.778634071350098
Epoch 420, val loss: 0.8538315892219543
Epoch 430, training loss: 0.05220825970172882 = 0.04543573036789894 + 0.001 * 6.772529602050781
Epoch 430, val loss: 0.8681533932685852
Epoch 440, training loss: 0.047813769429922104 = 0.04103557765483856 + 0.001 * 6.778191566467285
Epoch 440, val loss: 0.8821810483932495
Epoch 450, training loss: 0.043932247906923294 = 0.03715803474187851 + 0.001 * 6.774211406707764
Epoch 450, val loss: 0.8959856629371643
Epoch 460, training loss: 0.04050665348768234 = 0.033739056438207626 + 0.001 * 6.767595291137695
Epoch 460, val loss: 0.90934818983078
Epoch 470, training loss: 0.037498973309993744 = 0.030725611373782158 + 0.001 * 6.7733635902404785
Epoch 470, val loss: 0.9222873449325562
Epoch 480, training loss: 0.034830641001462936 = 0.028061846271157265 + 0.001 * 6.768795967102051
Epoch 480, val loss: 0.9347409009933472
Epoch 490, training loss: 0.032465700060129166 = 0.02570267766714096 + 0.001 * 6.7630228996276855
Epoch 490, val loss: 0.946773111820221
Epoch 500, training loss: 0.03036857396364212 = 0.023609424009919167 + 0.001 * 6.759149551391602
Epoch 500, val loss: 0.9583271145820618
Epoch 510, training loss: 0.028508353978395462 = 0.02175145223736763 + 0.001 * 6.75690221786499
Epoch 510, val loss: 0.9694215059280396
Epoch 520, training loss: 0.026865288615226746 = 0.020097466185688972 + 0.001 * 6.767821788787842
Epoch 520, val loss: 0.98004150390625
Epoch 530, training loss: 0.025382887572050095 = 0.018621010705828667 + 0.001 * 6.761876106262207
Epoch 530, val loss: 0.9902102947235107
Epoch 540, training loss: 0.024053169414401054 = 0.01729665882885456 + 0.001 * 6.756509780883789
Epoch 540, val loss: 1.0000029802322388
Epoch 550, training loss: 0.022855354472994804 = 0.016104498878121376 + 0.001 * 6.750855922698975
Epoch 550, val loss: 1.0094294548034668
Epoch 560, training loss: 0.021804066374897957 = 0.01503083948045969 + 0.001 * 6.773226737976074
Epoch 560, val loss: 1.0184844732284546
Epoch 570, training loss: 0.020814165472984314 = 0.014062047004699707 + 0.001 * 6.752118110656738
Epoch 570, val loss: 1.0271934270858765
Epoch 580, training loss: 0.019935570657253265 = 0.013184843584895134 + 0.001 * 6.750725746154785
Epoch 580, val loss: 1.0355721712112427
Epoch 590, training loss: 0.01913660392165184 = 0.012388673610985279 + 0.001 * 6.747930526733398
Epoch 590, val loss: 1.0436570644378662
Epoch 600, training loss: 0.018407315015792847 = 0.01166398823261261 + 0.001 * 6.743325710296631
Epoch 600, val loss: 1.0514791011810303
Epoch 610, training loss: 0.01775147020816803 = 0.011002582497894764 + 0.001 * 6.748887538909912
Epoch 610, val loss: 1.0590122938156128
Epoch 620, training loss: 0.017141127958893776 = 0.010397495701909065 + 0.001 * 6.743631362915039
Epoch 620, val loss: 1.066288948059082
Epoch 630, training loss: 0.016590535640716553 = 0.009842704981565475 + 0.001 * 6.747830390930176
Epoch 630, val loss: 1.0733356475830078
Epoch 640, training loss: 0.016077350825071335 = 0.009332634508609772 + 0.001 * 6.744715690612793
Epoch 640, val loss: 1.0801455974578857
Epoch 650, training loss: 0.015599621459841728 = 0.008862397633492947 + 0.001 * 6.737224102020264
Epoch 650, val loss: 1.0867522954940796
Epoch 660, training loss: 0.015195703133940697 = 0.008428283967077732 + 0.001 * 6.767419338226318
Epoch 660, val loss: 1.0931787490844727
Epoch 670, training loss: 0.014766184613108635 = 0.008026235736906528 + 0.001 * 6.739948272705078
Epoch 670, val loss: 1.0994127988815308
Epoch 680, training loss: 0.014387962408363819 = 0.007652366533875465 + 0.001 * 6.735595703125
Epoch 680, val loss: 1.105501413345337
Epoch 690, training loss: 0.014043346047401428 = 0.007303208112716675 + 0.001 * 6.740137577056885
Epoch 690, val loss: 1.1114388704299927
Epoch 700, training loss: 0.01372312381863594 = 0.006976212840527296 + 0.001 * 6.74691104888916
Epoch 700, val loss: 1.117214560508728
Epoch 710, training loss: 0.013397450558841228 = 0.006669645197689533 + 0.001 * 6.727805137634277
Epoch 710, val loss: 1.1228643655776978
Epoch 720, training loss: 0.013130013830959797 = 0.006381677929311991 + 0.001 * 6.748335361480713
Epoch 720, val loss: 1.1284135580062866
Epoch 730, training loss: 0.012842077761888504 = 0.006111603230237961 + 0.001 * 6.73047399520874
Epoch 730, val loss: 1.1338244676589966
Epoch 740, training loss: 0.012590242549777031 = 0.0058578853495419025 + 0.001 * 6.732357501983643
Epoch 740, val loss: 1.1391321420669556
Epoch 750, training loss: 0.012341704219579697 = 0.005619384348392487 + 0.001 * 6.722320079803467
Epoch 750, val loss: 1.144347071647644
Epoch 760, training loss: 0.012161022052168846 = 0.005394914653152227 + 0.001 * 6.766107082366943
Epoch 760, val loss: 1.1494499444961548
Epoch 770, training loss: 0.011903325095772743 = 0.005184025503695011 + 0.001 * 6.71929931640625
Epoch 770, val loss: 1.154421091079712
Epoch 780, training loss: 0.011704867705702782 = 0.004985614214092493 + 0.001 * 6.7192535400390625
Epoch 780, val loss: 1.1592878103256226
Epoch 790, training loss: 0.011522550135850906 = 0.004798784852027893 + 0.001 * 6.723764419555664
Epoch 790, val loss: 1.164057731628418
Epoch 800, training loss: 0.011348212137818336 = 0.004622571170330048 + 0.001 * 6.725641250610352
Epoch 800, val loss: 1.1687066555023193
Epoch 810, training loss: 0.011176005005836487 = 0.004456261172890663 + 0.001 * 6.7197442054748535
Epoch 810, val loss: 1.1732553243637085
Epoch 820, training loss: 0.011034369468688965 = 0.004299183841794729 + 0.001 * 6.735185146331787
Epoch 820, val loss: 1.1777276992797852
Epoch 830, training loss: 0.010881952941417694 = 0.004150899592787027 + 0.001 * 6.731053352355957
Epoch 830, val loss: 1.182116985321045
Epoch 840, training loss: 0.010735558345913887 = 0.0040106759406626225 + 0.001 * 6.724882125854492
Epoch 840, val loss: 1.1863772869110107
Epoch 850, training loss: 0.010589182376861572 = 0.0038778576999902725 + 0.001 * 6.711324214935303
Epoch 850, val loss: 1.190582036972046
Epoch 860, training loss: 0.010471322573721409 = 0.0037518565077334642 + 0.001 * 6.719466209411621
Epoch 860, val loss: 1.1947120428085327
Epoch 870, training loss: 0.010359170846641064 = 0.0036322230007499456 + 0.001 * 6.72694730758667
Epoch 870, val loss: 1.1987510919570923
Epoch 880, training loss: 0.010230174288153648 = 0.0035183338914066553 + 0.001 * 6.7118401527404785
Epoch 880, val loss: 1.2027779817581177
Epoch 890, training loss: 0.010124122723937035 = 0.003409699071198702 + 0.001 * 6.714422702789307
Epoch 890, val loss: 1.206742286682129
Epoch 900, training loss: 0.010014212690293789 = 0.0033059061970561743 + 0.001 * 6.708305835723877
Epoch 900, val loss: 1.2107210159301758
Epoch 910, training loss: 0.009908226318657398 = 0.0032065303530544043 + 0.001 * 6.701695442199707
Epoch 910, val loss: 1.214667797088623
Epoch 920, training loss: 0.00981538649648428 = 0.0031112299766391516 + 0.001 * 6.704155921936035
Epoch 920, val loss: 1.2186284065246582
Epoch 930, training loss: 0.009767687879502773 = 0.003019793424755335 + 0.001 * 6.747894287109375
Epoch 930, val loss: 1.2225998640060425
Epoch 940, training loss: 0.009649659506976604 = 0.0029323578346520662 + 0.001 * 6.717301368713379
Epoch 940, val loss: 1.2264840602874756
Epoch 950, training loss: 0.009548990055918694 = 0.0028484954964369535 + 0.001 * 6.700494289398193
Epoch 950, val loss: 1.2303857803344727
Epoch 960, training loss: 0.00946263037621975 = 0.0027681433130055666 + 0.001 * 6.694486618041992
Epoch 960, val loss: 1.234239935874939
Epoch 970, training loss: 0.009396166540682316 = 0.002691138768568635 + 0.001 * 6.7050275802612305
Epoch 970, val loss: 1.2380832433700562
Epoch 980, training loss: 0.009317249059677124 = 0.002617344493046403 + 0.001 * 6.699903964996338
Epoch 980, val loss: 1.2418912649154663
Epoch 990, training loss: 0.00923803448677063 = 0.002546569798141718 + 0.001 * 6.691464424133301
Epoch 990, val loss: 1.2456631660461426
Epoch 1000, training loss: 0.009168645367026329 = 0.0024786991998553276 + 0.001 * 6.689945697784424
Epoch 1000, val loss: 1.2494025230407715
Epoch 1010, training loss: 0.00914379395544529 = 0.002413614187389612 + 0.001 * 6.7301788330078125
Epoch 1010, val loss: 1.2531081438064575
Epoch 1020, training loss: 0.009063998237252235 = 0.002351421397179365 + 0.001 * 6.712575912475586
Epoch 1020, val loss: 1.2567018270492554
Epoch 1030, training loss: 0.008990318514406681 = 0.0022917843889445066 + 0.001 * 6.69853401184082
Epoch 1030, val loss: 1.2603001594543457
Epoch 1040, training loss: 0.008917583152651787 = 0.0022346596233546734 + 0.001 * 6.682922840118408
Epoch 1040, val loss: 1.2638187408447266
Epoch 1050, training loss: 0.00886688381433487 = 0.002179899951443076 + 0.001 * 6.686983108520508
Epoch 1050, val loss: 1.2673039436340332
Epoch 1060, training loss: 0.008831003680825233 = 0.0021273824386298656 + 0.001 * 6.703620433807373
Epoch 1060, val loss: 1.27074134349823
Epoch 1070, training loss: 0.008765237405896187 = 0.002077030949294567 + 0.001 * 6.688206195831299
Epoch 1070, val loss: 1.2740943431854248
Epoch 1080, training loss: 0.008732017129659653 = 0.0020286934450268745 + 0.001 * 6.703322887420654
Epoch 1080, val loss: 1.2774147987365723
Epoch 1090, training loss: 0.00866640079766512 = 0.0019822779577225447 + 0.001 * 6.684122562408447
Epoch 1090, val loss: 1.2807068824768066
Epoch 1100, training loss: 0.00862145610153675 = 0.001937694614753127 + 0.001 * 6.6837615966796875
Epoch 1100, val loss: 1.2839434146881104
Epoch 1110, training loss: 0.008569343015551567 = 0.0018948552897199988 + 0.001 * 6.674487590789795
Epoch 1110, val loss: 1.2871206998825073
Epoch 1120, training loss: 0.008540879003703594 = 0.0018537059659138322 + 0.001 * 6.687172889709473
Epoch 1120, val loss: 1.2902560234069824
Epoch 1130, training loss: 0.008496548049151897 = 0.0018141587497666478 + 0.001 * 6.682388782501221
Epoch 1130, val loss: 1.2933287620544434
Epoch 1140, training loss: 0.008453736081719398 = 0.0017760891932994127 + 0.001 * 6.677646160125732
Epoch 1140, val loss: 1.296351671218872
Epoch 1150, training loss: 0.008412916213274002 = 0.001739467610605061 + 0.001 * 6.673448085784912
Epoch 1150, val loss: 1.2993459701538086
Epoch 1160, training loss: 0.008390613831579685 = 0.0017042424296960235 + 0.001 * 6.686370849609375
Epoch 1160, val loss: 1.3022879362106323
Epoch 1170, training loss: 0.008361287415027618 = 0.0016703344881534576 + 0.001 * 6.690952777862549
Epoch 1170, val loss: 1.3051518201828003
Epoch 1180, training loss: 0.008317528292536736 = 0.001637668116018176 + 0.001 * 6.679859638214111
Epoch 1180, val loss: 1.3080294132232666
Epoch 1190, training loss: 0.008275218307971954 = 0.0016061896458268166 + 0.001 * 6.669028282165527
Epoch 1190, val loss: 1.3108129501342773
Epoch 1200, training loss: 0.008246713317930698 = 0.0015758512308821082 + 0.001 * 6.670861721038818
Epoch 1200, val loss: 1.313574194908142
Epoch 1210, training loss: 0.008215031586587429 = 0.0015466309851035476 + 0.001 * 6.668400287628174
Epoch 1210, val loss: 1.316259503364563
Epoch 1220, training loss: 0.008205053396522999 = 0.0015184293733909726 + 0.001 * 6.686623573303223
Epoch 1220, val loss: 1.3189154863357544
Epoch 1230, training loss: 0.008153330534696579 = 0.0014912278857082129 + 0.001 * 6.662102222442627
Epoch 1230, val loss: 1.3214808702468872
Epoch 1240, training loss: 0.008122209459543228 = 0.0014649746008217335 + 0.001 * 6.657235145568848
Epoch 1240, val loss: 1.324052333831787
Epoch 1250, training loss: 0.008097070269286633 = 0.0014396324986591935 + 0.001 * 6.657437324523926
Epoch 1250, val loss: 1.3265539407730103
Epoch 1260, training loss: 0.008080626837909222 = 0.0014151775976642966 + 0.001 * 6.665449142456055
Epoch 1260, val loss: 1.3290048837661743
Epoch 1270, training loss: 0.008046533912420273 = 0.0013915253803133965 + 0.001 * 6.655007839202881
Epoch 1270, val loss: 1.331418514251709
Epoch 1280, training loss: 0.008045525290071964 = 0.0013686681631952524 + 0.001 * 6.676856994628906
Epoch 1280, val loss: 1.3337775468826294
Epoch 1290, training loss: 0.008020138368010521 = 0.0013465756783261895 + 0.001 * 6.673562049865723
Epoch 1290, val loss: 1.336129903793335
Epoch 1300, training loss: 0.007997524924576283 = 0.0013252099743112922 + 0.001 * 6.6723151206970215
Epoch 1300, val loss: 1.3383867740631104
Epoch 1310, training loss: 0.007965956814587116 = 0.0013045434607192874 + 0.001 * 6.661412715911865
Epoch 1310, val loss: 1.3406634330749512
Epoch 1320, training loss: 0.007946528494358063 = 0.0012845681048929691 + 0.001 * 6.661960601806641
Epoch 1320, val loss: 1.342858076095581
Epoch 1330, training loss: 0.007934549823403358 = 0.0012652293080464005 + 0.001 * 6.6693196296691895
Epoch 1330, val loss: 1.3450520038604736
Epoch 1340, training loss: 0.00790256354957819 = 0.0012465312611311674 + 0.001 * 6.656032085418701
Epoch 1340, val loss: 1.3471654653549194
Epoch 1350, training loss: 0.007868652231991291 = 0.0012284007389098406 + 0.001 * 6.640251636505127
Epoch 1350, val loss: 1.3492698669433594
Epoch 1360, training loss: 0.007852782495319843 = 0.0012108394876122475 + 0.001 * 6.641942501068115
Epoch 1360, val loss: 1.3513391017913818
Epoch 1370, training loss: 0.007854418829083443 = 0.0011938173556700349 + 0.001 * 6.6606011390686035
Epoch 1370, val loss: 1.3533661365509033
Epoch 1380, training loss: 0.007823236286640167 = 0.0011773015139624476 + 0.001 * 6.645934581756592
Epoch 1380, val loss: 1.3553715944290161
Epoch 1390, training loss: 0.007802065461874008 = 0.0011613043025135994 + 0.001 * 6.640760898590088
Epoch 1390, val loss: 1.3573400974273682
Epoch 1400, training loss: 0.007790409028530121 = 0.0011457859072834253 + 0.001 * 6.644622802734375
Epoch 1400, val loss: 1.3592705726623535
Epoch 1410, training loss: 0.007766543421894312 = 0.0011307420209050179 + 0.001 * 6.635801315307617
Epoch 1410, val loss: 1.361174464225769
Epoch 1420, training loss: 0.00777371134608984 = 0.0011161399306729436 + 0.001 * 6.657571315765381
Epoch 1420, val loss: 1.363060474395752
Epoch 1430, training loss: 0.00776123721152544 = 0.001101987436413765 + 0.001 * 6.659249305725098
Epoch 1430, val loss: 1.3648502826690674
Epoch 1440, training loss: 0.0077575817704200745 = 0.0010882476344704628 + 0.001 * 6.6693339347839355
Epoch 1440, val loss: 1.3666563034057617
Epoch 1450, training loss: 0.007719259709119797 = 0.0010749490465968847 + 0.001 * 6.644309997558594
Epoch 1450, val loss: 1.3684195280075073
Epoch 1460, training loss: 0.007699610665440559 = 0.0010620398679748178 + 0.001 * 6.637570381164551
Epoch 1460, val loss: 1.3701602220535278
Epoch 1470, training loss: 0.007676403503865004 = 0.001049513928592205 + 0.001 * 6.626889228820801
Epoch 1470, val loss: 1.3718479871749878
Epoch 1480, training loss: 0.00767793133854866 = 0.0010373773984611034 + 0.001 * 6.6405534744262695
Epoch 1480, val loss: 1.3734923601150513
Epoch 1490, training loss: 0.0076609826646745205 = 0.0010255974484607577 + 0.001 * 6.635384559631348
Epoch 1490, val loss: 1.3751202821731567
Epoch 1500, training loss: 0.007647625170648098 = 0.0010141651146113873 + 0.001 * 6.633459568023682
Epoch 1500, val loss: 1.3767356872558594
Epoch 1510, training loss: 0.007653868291527033 = 0.0010030340636149049 + 0.001 * 6.650834083557129
Epoch 1510, val loss: 1.3783589601516724
Epoch 1520, training loss: 0.007625502534210682 = 0.000992240384221077 + 0.001 * 6.633261680603027
Epoch 1520, val loss: 1.3798753023147583
Epoch 1530, training loss: 0.007607118226587772 = 0.00098173669539392 + 0.001 * 6.625380992889404
Epoch 1530, val loss: 1.38141667842865
Epoch 1540, training loss: 0.007594469003379345 = 0.0009715098422020674 + 0.001 * 6.622959136962891
Epoch 1540, val loss: 1.3829007148742676
Epoch 1550, training loss: 0.007580386474728584 = 0.0009615875314921141 + 0.001 * 6.618798732757568
Epoch 1550, val loss: 1.3843920230865479
Epoch 1560, training loss: 0.00758327217772603 = 0.0009519273298792541 + 0.001 * 6.631344795227051
Epoch 1560, val loss: 1.3858487606048584
Epoch 1570, training loss: 0.007563713937997818 = 0.0009425418684259057 + 0.001 * 6.621171951293945
Epoch 1570, val loss: 1.3872641324996948
Epoch 1580, training loss: 0.00758676091209054 = 0.0009333910420536995 + 0.001 * 6.653369426727295
Epoch 1580, val loss: 1.388704776763916
Epoch 1590, training loss: 0.007553978357464075 = 0.0009244870161637664 + 0.001 * 6.629490852355957
Epoch 1590, val loss: 1.3900498151779175
Epoch 1600, training loss: 0.007575716823339462 = 0.0009158173925243318 + 0.001 * 6.6598992347717285
Epoch 1600, val loss: 1.3914310932159424
Epoch 1610, training loss: 0.007520441431552172 = 0.0009073628461919725 + 0.001 * 6.6130781173706055
Epoch 1610, val loss: 1.3927557468414307
Epoch 1620, training loss: 0.007546959910541773 = 0.00089912029216066 + 0.001 * 6.647839546203613
Epoch 1620, val loss: 1.394100308418274
Epoch 1630, training loss: 0.00753420777618885 = 0.0008911133627407253 + 0.001 * 6.643094062805176
Epoch 1630, val loss: 1.395365834236145
Epoch 1640, training loss: 0.007509997580200434 = 0.0008832913008518517 + 0.001 * 6.626705646514893
Epoch 1640, val loss: 1.3966623544692993
Epoch 1650, training loss: 0.007495509460568428 = 0.0008756781462579966 + 0.001 * 6.61983060836792
Epoch 1650, val loss: 1.3978837728500366
Epoch 1660, training loss: 0.007477914914488792 = 0.0008682564366608858 + 0.001 * 6.609658241271973
Epoch 1660, val loss: 1.3991166353225708
Epoch 1670, training loss: 0.007468387018889189 = 0.0008610007353127003 + 0.001 * 6.607386112213135
Epoch 1670, val loss: 1.400343418121338
Epoch 1680, training loss: 0.00748277734965086 = 0.0008539193077012897 + 0.001 * 6.628857612609863
Epoch 1680, val loss: 1.401563286781311
Epoch 1690, training loss: 0.007479469291865826 = 0.0008470152970403433 + 0.001 * 6.632453441619873
Epoch 1690, val loss: 1.4027132987976074
Epoch 1700, training loss: 0.007435242645442486 = 0.0008402791572734714 + 0.001 * 6.594963073730469
Epoch 1700, val loss: 1.4038759469985962
Epoch 1710, training loss: 0.007449484430253506 = 0.0008336891187354922 + 0.001 * 6.615794658660889
Epoch 1710, val loss: 1.4050605297088623
Epoch 1720, training loss: 0.007452439982444048 = 0.0008272702107205987 + 0.001 * 6.62516975402832
Epoch 1720, val loss: 1.406124472618103
Epoch 1730, training loss: 0.007430916652083397 = 0.0008210006635636091 + 0.001 * 6.609915733337402
Epoch 1730, val loss: 1.4072798490524292
Epoch 1740, training loss: 0.0074318526312708855 = 0.0008148840861395001 + 0.001 * 6.616968154907227
Epoch 1740, val loss: 1.4083545207977295
Epoch 1750, training loss: 0.007448868360370398 = 0.0008089280454441905 + 0.001 * 6.639939785003662
Epoch 1750, val loss: 1.409387469291687
Epoch 1760, training loss: 0.007440180517733097 = 0.0008030956378206611 + 0.001 * 6.637084484100342
Epoch 1760, val loss: 1.4104779958724976
Epoch 1770, training loss: 0.007428946904838085 = 0.0007974133477546275 + 0.001 * 6.631533145904541
Epoch 1770, val loss: 1.411453366279602
Epoch 1780, training loss: 0.007420958485454321 = 0.0007918636547401547 + 0.001 * 6.62909460067749
Epoch 1780, val loss: 1.4125182628631592
Epoch 1790, training loss: 0.007375046145170927 = 0.000786436430644244 + 0.001 * 6.58860969543457
Epoch 1790, val loss: 1.4134851694107056
Epoch 1800, training loss: 0.007378333248198032 = 0.0007811231189407408 + 0.001 * 6.597209930419922
Epoch 1800, val loss: 1.4144835472106934
Epoch 1810, training loss: 0.007406226359307766 = 0.0007759226718917489 + 0.001 * 6.630303382873535
Epoch 1810, val loss: 1.4154837131500244
Epoch 1820, training loss: 0.007414151448756456 = 0.0007708355551585555 + 0.001 * 6.643315315246582
Epoch 1820, val loss: 1.4164153337478638
Epoch 1830, training loss: 0.007369700353592634 = 0.0007658444228582084 + 0.001 * 6.603855609893799
Epoch 1830, val loss: 1.4173909425735474
Epoch 1840, training loss: 0.007354104425758123 = 0.0007609861204400659 + 0.001 * 6.593118190765381
Epoch 1840, val loss: 1.4183202981948853
Epoch 1850, training loss: 0.007338637486100197 = 0.0007562285754829645 + 0.001 * 6.582408905029297
Epoch 1850, val loss: 1.4192814826965332
Epoch 1860, training loss: 0.007342491298913956 = 0.0007515798206441104 + 0.001 * 6.590911388397217
Epoch 1860, val loss: 1.4201911687850952
Epoch 1870, training loss: 0.007338273338973522 = 0.0007470315904356539 + 0.001 * 6.591241359710693
Epoch 1870, val loss: 1.4211199283599854
Epoch 1880, training loss: 0.007382656447589397 = 0.0007425693911500275 + 0.001 * 6.640086650848389
Epoch 1880, val loss: 1.4220325946807861
Epoch 1890, training loss: 0.007340726442635059 = 0.0007382218027487397 + 0.001 * 6.602504253387451
Epoch 1890, val loss: 1.4228512048721313
Epoch 1900, training loss: 0.00733048003166914 = 0.0007339566946029663 + 0.001 * 6.596522808074951
Epoch 1900, val loss: 1.4237494468688965
Epoch 1910, training loss: 0.007313119247555733 = 0.0007297703414224088 + 0.001 * 6.583348751068115
Epoch 1910, val loss: 1.4245699644088745
Epoch 1920, training loss: 0.007306368555873632 = 0.0007256711833178997 + 0.001 * 6.580697059631348
Epoch 1920, val loss: 1.425443410873413
Epoch 1930, training loss: 0.007309095002710819 = 0.0007216623635031283 + 0.001 * 6.587432384490967
Epoch 1930, val loss: 1.4262744188308716
Epoch 1940, training loss: 0.007302264217287302 = 0.0007177460356615484 + 0.001 * 6.584517955780029
Epoch 1940, val loss: 1.427088737487793
Epoch 1950, training loss: 0.0072859106585383415 = 0.0007139042718335986 + 0.001 * 6.572005748748779
Epoch 1950, val loss: 1.4278849363327026
Epoch 1960, training loss: 0.007339440751820803 = 0.0007101288065314293 + 0.001 * 6.629311561584473
Epoch 1960, val loss: 1.4287019968032837
Epoch 1970, training loss: 0.007308857049793005 = 0.0007064192905090749 + 0.001 * 6.602437496185303
Epoch 1970, val loss: 1.4294601678848267
Epoch 1980, training loss: 0.007266004104167223 = 0.0007027882966212928 + 0.001 * 6.563215732574463
Epoch 1980, val loss: 1.4302328824996948
Epoch 1990, training loss: 0.0072830598801374435 = 0.0006992269773036242 + 0.001 * 6.583832740783691
Epoch 1990, val loss: 1.4310089349746704
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8819
Flip ASR: 0.8622/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.954428791999817 = 1.9460549354553223 + 0.001 * 8.373867988586426
Epoch 0, val loss: 1.9538967609405518
Epoch 10, training loss: 1.9440922737121582 = 1.9357185363769531 + 0.001 * 8.373773574829102
Epoch 10, val loss: 1.9428274631500244
Epoch 20, training loss: 1.9311847686767578 = 1.9228112697601318 + 0.001 * 8.37352180480957
Epoch 20, val loss: 1.9286975860595703
Epoch 30, training loss: 1.9130749702453613 = 1.9047019481658936 + 0.001 * 8.373003959655762
Epoch 30, val loss: 1.9087698459625244
Epoch 40, training loss: 1.8866273164749146 = 1.8782554864883423 + 0.001 * 8.371817588806152
Epoch 40, val loss: 1.8800910711288452
Epoch 50, training loss: 1.8501278162002563 = 1.841759204864502 + 0.001 * 8.368622779846191
Epoch 50, val loss: 1.8423243761062622
Epoch 60, training loss: 1.8080718517303467 = 1.7997140884399414 + 0.001 * 8.357739448547363
Epoch 60, val loss: 1.803418517112732
Epoch 70, training loss: 1.7662334442138672 = 1.7579255104064941 + 0.001 * 8.307909965515137
Epoch 70, val loss: 1.7700122594833374
Epoch 80, training loss: 1.7135744094848633 = 1.7055503129959106 + 0.001 * 8.024136543273926
Epoch 80, val loss: 1.7286326885223389
Epoch 90, training loss: 1.6413321495056152 = 1.6337498426437378 + 0.001 * 7.582275390625
Epoch 90, val loss: 1.6697534322738647
Epoch 100, training loss: 1.5470901727676392 = 1.5397326946258545 + 0.001 * 7.357521057128906
Epoch 100, val loss: 1.5923820734024048
Epoch 110, training loss: 1.4379944801330566 = 1.4307222366333008 + 0.001 * 7.272207736968994
Epoch 110, val loss: 1.506327748298645
Epoch 120, training loss: 1.324419379234314 = 1.3171684741973877 + 0.001 * 7.250865936279297
Epoch 120, val loss: 1.4175087213516235
Epoch 130, training loss: 1.212867021560669 = 1.2056398391723633 + 0.001 * 7.2271599769592285
Epoch 130, val loss: 1.3312739133834839
Epoch 140, training loss: 1.1061197519302368 = 1.0989257097244263 + 0.001 * 7.194033145904541
Epoch 140, val loss: 1.2487963438034058
Epoch 150, training loss: 1.005612850189209 = 0.9984715580940247 + 0.001 * 7.14125919342041
Epoch 150, val loss: 1.1713930368423462
Epoch 160, training loss: 0.9131525754928589 = 0.9060933589935303 + 0.001 * 7.059238910675049
Epoch 160, val loss: 1.1019642353057861
Epoch 170, training loss: 0.8307186365127563 = 0.8237112760543823 + 0.001 * 7.007375240325928
Epoch 170, val loss: 1.042898416519165
Epoch 180, training loss: 0.7589502334594727 = 0.7519623637199402 + 0.001 * 6.987873554229736
Epoch 180, val loss: 0.9944342970848083
Epoch 190, training loss: 0.6966359615325928 = 0.6896834969520569 + 0.001 * 6.952449798583984
Epoch 190, val loss: 0.9554990530014038
Epoch 200, training loss: 0.6418932676315308 = 0.6349725127220154 + 0.001 * 6.9207539558410645
Epoch 200, val loss: 0.9250493049621582
Epoch 210, training loss: 0.592823326587677 = 0.5859238505363464 + 0.001 * 6.8994646072387695
Epoch 210, val loss: 0.9014319181442261
Epoch 220, training loss: 0.5477854609489441 = 0.5408989191055298 + 0.001 * 6.886559963226318
Epoch 220, val loss: 0.8836658000946045
Epoch 230, training loss: 0.5055092573165894 = 0.4986277222633362 + 0.001 * 6.8815083503723145
Epoch 230, val loss: 0.8704473972320557
Epoch 240, training loss: 0.465129554271698 = 0.45824915170669556 + 0.001 * 6.880404949188232
Epoch 240, val loss: 0.8610053062438965
Epoch 250, training loss: 0.42605358362197876 = 0.41917333006858826 + 0.001 * 6.8802571296691895
Epoch 250, val loss: 0.8545697927474976
Epoch 260, training loss: 0.38803011178970337 = 0.38115066289901733 + 0.001 * 6.879462242126465
Epoch 260, val loss: 0.8509317636489868
Epoch 270, training loss: 0.3510681986808777 = 0.34419000148773193 + 0.001 * 6.878183364868164
Epoch 270, val loss: 0.8499444127082825
Epoch 280, training loss: 0.3153439462184906 = 0.3084670305252075 + 0.001 * 6.87691593170166
Epoch 280, val loss: 0.8519558906555176
Epoch 290, training loss: 0.2812276780605316 = 0.27435189485549927 + 0.001 * 6.875784873962402
Epoch 290, val loss: 0.8568747639656067
Epoch 300, training loss: 0.2490987628698349 = 0.24222391843795776 + 0.001 * 6.8748369216918945
Epoch 300, val loss: 0.864790141582489
Epoch 310, training loss: 0.21935273706912994 = 0.21247880160808563 + 0.001 * 6.873935699462891
Epoch 310, val loss: 0.8758798241615295
Epoch 320, training loss: 0.19231167435646057 = 0.18543872237205505 + 0.001 * 6.872954845428467
Epoch 320, val loss: 0.8898764848709106
Epoch 330, training loss: 0.16812755167484283 = 0.16125448048114777 + 0.001 * 6.873076915740967
Epoch 330, val loss: 0.9062016606330872
Epoch 340, training loss: 0.1468113660812378 = 0.13993997871875763 + 0.001 * 6.871383190155029
Epoch 340, val loss: 0.9243388772010803
Epoch 350, training loss: 0.12829837203025818 = 0.12142808735370636 + 0.001 * 6.870280742645264
Epoch 350, val loss: 0.9436528086662292
Epoch 360, training loss: 0.1123909205198288 = 0.10552209615707397 + 0.001 * 6.868827819824219
Epoch 360, val loss: 0.9636301398277283
Epoch 370, training loss: 0.0988350585103035 = 0.09196121990680695 + 0.001 * 6.873839855194092
Epoch 370, val loss: 0.9840579628944397
Epoch 380, training loss: 0.08732116222381592 = 0.08045454323291779 + 0.001 * 6.866617679595947
Epoch 380, val loss: 1.0046199560165405
Epoch 390, training loss: 0.0775657594203949 = 0.07070133090019226 + 0.001 * 6.864429950714111
Epoch 390, val loss: 1.024930715560913
Epoch 400, training loss: 0.06928307563066483 = 0.062420763075351715 + 0.001 * 6.862314701080322
Epoch 400, val loss: 1.0448938608169556
Epoch 410, training loss: 0.06224268674850464 = 0.055367372930049896 + 0.001 * 6.8753132820129395
Epoch 410, val loss: 1.0644538402557373
Epoch 420, training loss: 0.05619343742728233 = 0.049334816634655 + 0.001 * 6.858620643615723
Epoch 420, val loss: 1.0834245681762695
Epoch 430, training loss: 0.0510098896920681 = 0.04415402561426163 + 0.001 * 6.855865001678467
Epoch 430, val loss: 1.1018480062484741
Epoch 440, training loss: 0.04654264077544212 = 0.03968772292137146 + 0.001 * 6.854918479919434
Epoch 440, val loss: 1.1197288036346436
Epoch 450, training loss: 0.04267087206244469 = 0.03582163155078888 + 0.001 * 6.849241733551025
Epoch 450, val loss: 1.1369649171829224
Epoch 460, training loss: 0.039309244602918625 = 0.03246060758829117 + 0.001 * 6.848635673522949
Epoch 460, val loss: 1.1536062955856323
Epoch 470, training loss: 0.03637205809354782 = 0.029526909813284874 + 0.001 * 6.845146179199219
Epoch 470, val loss: 1.1696312427520752
Epoch 480, training loss: 0.033795472234487534 = 0.026955489069223404 + 0.001 * 6.8399834632873535
Epoch 480, val loss: 1.1850247383117676
Epoch 490, training loss: 0.03155158460140228 = 0.024692784994840622 + 0.001 * 6.858801364898682
Epoch 490, val loss: 1.1997920274734497
Epoch 500, training loss: 0.029530303552746773 = 0.0226945411413908 + 0.001 * 6.8357625007629395
Epoch 500, val loss: 1.2140072584152222
Epoch 510, training loss: 0.02775394544005394 = 0.02092362940311432 + 0.001 * 6.830315589904785
Epoch 510, val loss: 1.2277048826217651
Epoch 520, training loss: 0.026199067011475563 = 0.01934869773685932 + 0.001 * 6.850368976593018
Epoch 520, val loss: 1.240840196609497
Epoch 530, training loss: 0.02476903796195984 = 0.01794293150305748 + 0.001 * 6.826107025146484
Epoch 530, val loss: 1.2535285949707031
Epoch 540, training loss: 0.023510031402111053 = 0.016684481874108315 + 0.001 * 6.825549125671387
Epoch 540, val loss: 1.265722632408142
Epoch 550, training loss: 0.022368408739566803 = 0.01555422879755497 + 0.001 * 6.814179420471191
Epoch 550, val loss: 1.2774685621261597
Epoch 560, training loss: 0.021352173760533333 = 0.014536085538566113 + 0.001 * 6.816087245941162
Epoch 560, val loss: 1.288805603981018
Epoch 570, training loss: 0.02043420448899269 = 0.013615908101201057 + 0.001 * 6.818295955657959
Epoch 570, val loss: 1.2997417449951172
Epoch 580, training loss: 0.0196068175137043 = 0.01278193574398756 + 0.001 * 6.824882507324219
Epoch 580, val loss: 1.3102868795394897
Epoch 590, training loss: 0.01882746070623398 = 0.012023343704640865 + 0.001 * 6.804117679595947
Epoch 590, val loss: 1.3204602003097534
Epoch 600, training loss: 0.018127303570508957 = 0.01132965087890625 + 0.001 * 6.797652721405029
Epoch 600, val loss: 1.3303676843643188
Epoch 610, training loss: 0.01748463697731495 = 0.010691186413168907 + 0.001 * 6.793450355529785
Epoch 610, val loss: 1.3400187492370605
Epoch 620, training loss: 0.0168897844851017 = 0.010100792162120342 + 0.001 * 6.7889909744262695
Epoch 620, val loss: 1.349465250968933
Epoch 630, training loss: 0.01634826324880123 = 0.009554075077176094 + 0.001 * 6.794188022613525
Epoch 630, val loss: 1.3587031364440918
Epoch 640, training loss: 0.015834610909223557 = 0.009047466330230236 + 0.001 * 6.787144184112549
Epoch 640, val loss: 1.367762565612793
Epoch 650, training loss: 0.015362763777375221 = 0.008577757515013218 + 0.001 * 6.785006523132324
Epoch 650, val loss: 1.3766061067581177
Epoch 660, training loss: 0.014930629171431065 = 0.008142254315316677 + 0.001 * 6.788374423980713
Epoch 660, val loss: 1.3853014707565308
Epoch 670, training loss: 0.014523103833198547 = 0.007738624233752489 + 0.001 * 6.784479141235352
Epoch 670, val loss: 1.3937684297561646
Epoch 680, training loss: 0.014139862731099129 = 0.007364165503531694 + 0.001 * 6.775697231292725
Epoch 680, val loss: 1.4020811319351196
Epoch 690, training loss: 0.013794876635074615 = 0.007016422227025032 + 0.001 * 6.778453826904297
Epoch 690, val loss: 1.410201072692871
Epoch 700, training loss: 0.013465194031596184 = 0.006693174131214619 + 0.001 * 6.772019386291504
Epoch 700, val loss: 1.4181365966796875
Epoch 710, training loss: 0.01315809041261673 = 0.006392403971403837 + 0.001 * 6.765685558319092
Epoch 710, val loss: 1.4259065389633179
Epoch 720, training loss: 0.012873982079327106 = 0.006112200208008289 + 0.001 * 6.761781692504883
Epoch 720, val loss: 1.4334841966629028
Epoch 730, training loss: 0.012614257633686066 = 0.005850845482200384 + 0.001 * 6.763411998748779
Epoch 730, val loss: 1.4409000873565674
Epoch 740, training loss: 0.012369655072689056 = 0.005606793332844973 + 0.001 * 6.762861728668213
Epoch 740, val loss: 1.4481288194656372
Epoch 750, training loss: 0.012151440605521202 = 0.00537854665890336 + 0.001 * 6.772892951965332
Epoch 750, val loss: 1.4551949501037598
Epoch 760, training loss: 0.011929700151085854 = 0.005164891481399536 + 0.001 * 6.764808177947998
Epoch 760, val loss: 1.4621033668518066
Epoch 770, training loss: 0.01173234824091196 = 0.004964664112776518 + 0.001 * 6.767683982849121
Epoch 770, val loss: 1.4688596725463867
Epoch 780, training loss: 0.011539276689291 = 0.00477678794413805 + 0.001 * 6.76248836517334
Epoch 780, val loss: 1.4754517078399658
Epoch 790, training loss: 0.011348593980073929 = 0.004600266460329294 + 0.001 * 6.748326778411865
Epoch 790, val loss: 1.4819048643112183
Epoch 800, training loss: 0.011190583929419518 = 0.004434266593307257 + 0.001 * 6.756317138671875
Epoch 800, val loss: 1.488206148147583
Epoch 810, training loss: 0.011031788773834705 = 0.0042779832147061825 + 0.001 * 6.753805160522461
Epoch 810, val loss: 1.4943678379058838
Epoch 820, training loss: 0.010877231135964394 = 0.004130620509386063 + 0.001 * 6.746609687805176
Epoch 820, val loss: 1.500397801399231
Epoch 830, training loss: 0.010732033289968967 = 0.00399158988147974 + 0.001 * 6.740443229675293
Epoch 830, val loss: 1.50629460811615
Epoch 840, training loss: 0.010600762441754341 = 0.003860265715047717 + 0.001 * 6.740496635437012
Epoch 840, val loss: 1.5120530128479004
Epoch 850, training loss: 0.010488576255738735 = 0.003736099461093545 + 0.001 * 6.752476692199707
Epoch 850, val loss: 1.5176764726638794
Epoch 860, training loss: 0.010361404158174992 = 0.0036185409408062696 + 0.001 * 6.742862701416016
Epoch 860, val loss: 1.5231934785842896
Epoch 870, training loss: 0.010248556733131409 = 0.003507188055664301 + 0.001 * 6.741368293762207
Epoch 870, val loss: 1.5285825729370117
Epoch 880, training loss: 0.010140836238861084 = 0.0034015863202512264 + 0.001 * 6.739250183105469
Epoch 880, val loss: 1.5338504314422607
Epoch 890, training loss: 0.010029437951743603 = 0.003301341785117984 + 0.001 * 6.728095531463623
Epoch 890, val loss: 1.5390154123306274
Epoch 900, training loss: 0.009947685524821281 = 0.003206107299774885 + 0.001 * 6.741578578948975
Epoch 900, val loss: 1.5440441370010376
Epoch 910, training loss: 0.009843666106462479 = 0.0031155748292803764 + 0.001 * 6.728090763092041
Epoch 910, val loss: 1.5489654541015625
Epoch 920, training loss: 0.009754142723977566 = 0.003029431216418743 + 0.001 * 6.7247114181518555
Epoch 920, val loss: 1.5537890195846558
Epoch 930, training loss: 0.009698772802948952 = 0.002947403583675623 + 0.001 * 6.751368999481201
Epoch 930, val loss: 1.5585075616836548
Epoch 940, training loss: 0.009594112634658813 = 0.002869244432076812 + 0.001 * 6.724867820739746
Epoch 940, val loss: 1.5631129741668701
Epoch 950, training loss: 0.00952526181936264 = 0.002794683910906315 + 0.001 * 6.73057746887207
Epoch 950, val loss: 1.5676403045654297
Epoch 960, training loss: 0.009450330398976803 = 0.002723525045439601 + 0.001 * 6.726804733276367
Epoch 960, val loss: 1.5720525979995728
Epoch 970, training loss: 0.009376022964715958 = 0.002655565505847335 + 0.001 * 6.720457553863525
Epoch 970, val loss: 1.576373815536499
Epoch 980, training loss: 0.009310388006269932 = 0.002590596443042159 + 0.001 * 6.719791412353516
Epoch 980, val loss: 1.5806078910827637
Epoch 990, training loss: 0.009243627078831196 = 0.0025284525472670794 + 0.001 * 6.715174198150635
Epoch 990, val loss: 1.5847480297088623
Epoch 1000, training loss: 0.00918571650981903 = 0.002468985738232732 + 0.001 * 6.71673059463501
Epoch 1000, val loss: 1.588844656944275
Epoch 1010, training loss: 0.009127565659582615 = 0.002412047004327178 + 0.001 * 6.715518474578857
Epoch 1010, val loss: 1.5928179025650024
Epoch 1020, training loss: 0.009074748493731022 = 0.0023574954830110073 + 0.001 * 6.717252731323242
Epoch 1020, val loss: 1.5967185497283936
Epoch 1030, training loss: 0.009021409787237644 = 0.002305173547938466 + 0.001 * 6.716236114501953
Epoch 1030, val loss: 1.6005563735961914
Epoch 1040, training loss: 0.008959083817899227 = 0.002254989231005311 + 0.001 * 6.704094409942627
Epoch 1040, val loss: 1.6042969226837158
Epoch 1050, training loss: 0.008920769207179546 = 0.002206824952736497 + 0.001 * 6.713943958282471
Epoch 1050, val loss: 1.6080114841461182
Epoch 1060, training loss: 0.008880622684955597 = 0.002160558709874749 + 0.001 * 6.72006368637085
Epoch 1060, val loss: 1.6116328239440918
Epoch 1070, training loss: 0.00885425042361021 = 0.0021160959731787443 + 0.001 * 6.738154411315918
Epoch 1070, val loss: 1.6151728630065918
Epoch 1080, training loss: 0.008780479431152344 = 0.0020733755081892014 + 0.001 * 6.707104206085205
Epoch 1080, val loss: 1.6186275482177734
Epoch 1090, training loss: 0.008730105124413967 = 0.0020322746131569147 + 0.001 * 6.6978302001953125
Epoch 1090, val loss: 1.6220513582229614
Epoch 1100, training loss: 0.00868195854127407 = 0.001992727629840374 + 0.001 * 6.689229965209961
Epoch 1100, val loss: 1.625370740890503
Epoch 1110, training loss: 0.008663367480039597 = 0.0019546598196029663 + 0.001 * 6.708707332611084
Epoch 1110, val loss: 1.6286667585372925
Epoch 1120, training loss: 0.008615822531282902 = 0.0019180028466507792 + 0.001 * 6.697819232940674
Epoch 1120, val loss: 1.631851315498352
Epoch 1130, training loss: 0.008587460033595562 = 0.0018826688174158335 + 0.001 * 6.7047905921936035
Epoch 1130, val loss: 1.6350294351577759
Epoch 1140, training loss: 0.008533443324267864 = 0.0018486225744709373 + 0.001 * 6.684820175170898
Epoch 1140, val loss: 1.63809072971344
Epoch 1150, training loss: 0.00851517915725708 = 0.0018157786689698696 + 0.001 * 6.699399948120117
Epoch 1150, val loss: 1.6411216259002686
Epoch 1160, training loss: 0.008481306955218315 = 0.0017840861110016704 + 0.001 * 6.697220325469971
Epoch 1160, val loss: 1.6441153287887573
Epoch 1170, training loss: 0.008446565829217434 = 0.0017535036895424128 + 0.001 * 6.693061828613281
Epoch 1170, val loss: 1.6470271348953247
Epoch 1180, training loss: 0.008429145440459251 = 0.0017239582957699895 + 0.001 * 6.70518684387207
Epoch 1180, val loss: 1.649914264678955
Epoch 1190, training loss: 0.008406867273151875 = 0.0016954372404143214 + 0.001 * 6.711429595947266
Epoch 1190, val loss: 1.652739405632019
Epoch 1200, training loss: 0.008358950726687908 = 0.0016678633401170373 + 0.001 * 6.691087245941162
Epoch 1200, val loss: 1.6554949283599854
Epoch 1210, training loss: 0.008321655914187431 = 0.0016411967808380723 + 0.001 * 6.680459022521973
Epoch 1210, val loss: 1.6582372188568115
Epoch 1220, training loss: 0.008298470638692379 = 0.0016154322074726224 + 0.001 * 6.683037757873535
Epoch 1220, val loss: 1.6609057188034058
Epoch 1230, training loss: 0.008262159302830696 = 0.001590494066476822 + 0.001 * 6.671664714813232
Epoch 1230, val loss: 1.6635448932647705
Epoch 1240, training loss: 0.008249438367784023 = 0.0015663643134757876 + 0.001 * 6.6830735206604
Epoch 1240, val loss: 1.6661337614059448
Epoch 1250, training loss: 0.008200770244002342 = 0.0015430073253810406 + 0.001 * 6.6577630043029785
Epoch 1250, val loss: 1.6686553955078125
Epoch 1260, training loss: 0.00820012018084526 = 0.0015203868970274925 + 0.001 * 6.679732322692871
Epoch 1260, val loss: 1.6711539030075073
Epoch 1270, training loss: 0.008195677772164345 = 0.0014984628651291132 + 0.001 * 6.697214126586914
Epoch 1270, val loss: 1.673619270324707
Epoch 1280, training loss: 0.008173082023859024 = 0.0014772305730730295 + 0.001 * 6.695850849151611
Epoch 1280, val loss: 1.6760308742523193
Epoch 1290, training loss: 0.00812052097171545 = 0.0014566357713192701 + 0.001 * 6.663885116577148
Epoch 1290, val loss: 1.6784147024154663
Epoch 1300, training loss: 0.008097684010863304 = 0.001436668331734836 + 0.001 * 6.661015033721924
Epoch 1300, val loss: 1.6807481050491333
Epoch 1310, training loss: 0.008154038339853287 = 0.0014172946102917194 + 0.001 * 6.736743927001953
Epoch 1310, val loss: 1.6830527782440186
Epoch 1320, training loss: 0.008039670996367931 = 0.001398522756062448 + 0.001 * 6.641147613525391
Epoch 1320, val loss: 1.6852943897247314
Epoch 1330, training loss: 0.008043565787374973 = 0.0013802966568619013 + 0.001 * 6.66326904296875
Epoch 1330, val loss: 1.6874983310699463
Epoch 1340, training loss: 0.008023543283343315 = 0.001362598268315196 + 0.001 * 6.66094446182251
Epoch 1340, val loss: 1.689719319343567
Epoch 1350, training loss: 0.00800664909183979 = 0.001345415716059506 + 0.001 * 6.661232948303223
Epoch 1350, val loss: 1.691838264465332
Epoch 1360, training loss: 0.008004918694496155 = 0.0013287235051393509 + 0.001 * 6.676194190979004
Epoch 1360, val loss: 1.6939764022827148
Epoch 1370, training loss: 0.008014947175979614 = 0.0013125058030709624 + 0.001 * 6.7024407386779785
Epoch 1370, val loss: 1.696058750152588
Epoch 1380, training loss: 0.007927533239126205 = 0.0012967404909431934 + 0.001 * 6.630792140960693
Epoch 1380, val loss: 1.6981157064437866
Epoch 1390, training loss: 0.007912153378129005 = 0.001281418721191585 + 0.001 * 6.630734920501709
Epoch 1390, val loss: 1.700148582458496
Epoch 1400, training loss: 0.007923933677375317 = 0.0012665338581427932 + 0.001 * 6.6573991775512695
Epoch 1400, val loss: 1.7021279335021973
Epoch 1410, training loss: 0.007914441637694836 = 0.001252046669833362 + 0.001 * 6.662395000457764
Epoch 1410, val loss: 1.704084873199463
Epoch 1420, training loss: 0.007874052040278912 = 0.0012379528488963842 + 0.001 * 6.636098861694336
Epoch 1420, val loss: 1.7060126066207886
Epoch 1430, training loss: 0.00786359328776598 = 0.00122425879817456 + 0.001 * 6.639334201812744
Epoch 1430, val loss: 1.707887053489685
Epoch 1440, training loss: 0.007860682904720306 = 0.0012109200470149517 + 0.001 * 6.649763107299805
Epoch 1440, val loss: 1.7097468376159668
Epoch 1450, training loss: 0.007817770354449749 = 0.001197930658236146 + 0.001 * 6.619839668273926
Epoch 1450, val loss: 1.711574673652649
Epoch 1460, training loss: 0.007830201648175716 = 0.0011852964526042342 + 0.001 * 6.644905090332031
Epoch 1460, val loss: 1.7133655548095703
Epoch 1470, training loss: 0.007815276272594929 = 0.0011729818070307374 + 0.001 * 6.642293930053711
Epoch 1470, val loss: 1.7151551246643066
Epoch 1480, training loss: 0.007860170677304268 = 0.0011609976645559072 + 0.001 * 6.6991729736328125
Epoch 1480, val loss: 1.7169240713119507
Epoch 1490, training loss: 0.0077572655864059925 = 0.0011493099154904485 + 0.001 * 6.607955455780029
Epoch 1490, val loss: 1.7186206579208374
Epoch 1500, training loss: 0.007765924092382193 = 0.0011379215866327286 + 0.001 * 6.628002166748047
Epoch 1500, val loss: 1.7203292846679688
Epoch 1510, training loss: 0.007791872136294842 = 0.0011268246453255415 + 0.001 * 6.6650471687316895
Epoch 1510, val loss: 1.7220441102981567
Epoch 1520, training loss: 0.007738498505204916 = 0.0011160048888996243 + 0.001 * 6.622493267059326
Epoch 1520, val loss: 1.7236584424972534
Epoch 1530, training loss: 0.007721012458205223 = 0.001105446252040565 + 0.001 * 6.615565776824951
Epoch 1530, val loss: 1.7253131866455078
Epoch 1540, training loss: 0.007710243575274944 = 0.001095158513635397 + 0.001 * 6.615084648132324
Epoch 1540, val loss: 1.7269139289855957
Epoch 1550, training loss: 0.007699763402342796 = 0.0010851173428818583 + 0.001 * 6.614645481109619
Epoch 1550, val loss: 1.7285076379776
Epoch 1560, training loss: 0.007677019573748112 = 0.001075318898074329 + 0.001 * 6.601700305938721
Epoch 1560, val loss: 1.730074167251587
Epoch 1570, training loss: 0.0076719019562006 = 0.0010657592210918665 + 0.001 * 6.606142520904541
Epoch 1570, val loss: 1.7316410541534424
Epoch 1580, training loss: 0.007654852699488401 = 0.0010564286494627595 + 0.001 * 6.598423957824707
Epoch 1580, val loss: 1.733161211013794
Epoch 1590, training loss: 0.0076659913174808025 = 0.0010473037837073207 + 0.001 * 6.618687152862549
Epoch 1590, val loss: 1.7346962690353394
Epoch 1600, training loss: 0.007682714145630598 = 0.0010383953340351582 + 0.001 * 6.644318580627441
Epoch 1600, val loss: 1.73617684841156
Epoch 1610, training loss: 0.007642888464033604 = 0.0010296953842043877 + 0.001 * 6.613192558288574
Epoch 1610, val loss: 1.73761785030365
Epoch 1620, training loss: 0.007618261501193047 = 0.0010212071938440204 + 0.001 * 6.5970540046691895
Epoch 1620, val loss: 1.739055871963501
Epoch 1630, training loss: 0.007632349617779255 = 0.0010129017755389214 + 0.001 * 6.619447708129883
Epoch 1630, val loss: 1.7405163049697876
Epoch 1640, training loss: 0.0076044281013309956 = 0.0010047955438494682 + 0.001 * 6.599632263183594
Epoch 1640, val loss: 1.7418979406356812
Epoch 1650, training loss: 0.007584324572235346 = 0.0009968659142032266 + 0.001 * 6.587458610534668
Epoch 1650, val loss: 1.7433003187179565
Epoch 1660, training loss: 0.007618904113769531 = 0.0009891130030155182 + 0.001 * 6.629790782928467
Epoch 1660, val loss: 1.744691014289856
Epoch 1670, training loss: 0.007604559417814016 = 0.0009815400699153543 + 0.001 * 6.623019218444824
Epoch 1670, val loss: 1.7460074424743652
Epoch 1680, training loss: 0.007596966344863176 = 0.0009741229587234557 + 0.001 * 6.622843265533447
Epoch 1680, val loss: 1.7473822832107544
Epoch 1690, training loss: 0.007591701112687588 = 0.0009668746497482061 + 0.001 * 6.624825954437256
Epoch 1690, val loss: 1.7486581802368164
Epoch 1700, training loss: 0.00754761416465044 = 0.0009597795433364809 + 0.001 * 6.587834358215332
Epoch 1700, val loss: 1.7499890327453613
Epoch 1710, training loss: 0.007567041087895632 = 0.0009528418304398656 + 0.001 * 6.614199161529541
Epoch 1710, val loss: 1.7512646913528442
Epoch 1720, training loss: 0.00752986827865243 = 0.0009460579021833837 + 0.001 * 6.583809852600098
Epoch 1720, val loss: 1.7525080442428589
Epoch 1730, training loss: 0.007517992053180933 = 0.0009394139633513987 + 0.001 * 6.578577995300293
Epoch 1730, val loss: 1.7537765502929688
Epoch 1740, training loss: 0.00750986160710454 = 0.000932911061681807 + 0.001 * 6.576950550079346
Epoch 1740, val loss: 1.7550233602523804
Epoch 1750, training loss: 0.007521308492869139 = 0.000926530861761421 + 0.001 * 6.5947771072387695
Epoch 1750, val loss: 1.756269931793213
Epoch 1760, training loss: 0.007518733385950327 = 0.0009202880901284516 + 0.001 * 6.598444938659668
Epoch 1760, val loss: 1.7574374675750732
Epoch 1770, training loss: 0.0075266011990606785 = 0.0009141730261035264 + 0.001 * 6.612427711486816
Epoch 1770, val loss: 1.7586724758148193
Epoch 1780, training loss: 0.0075299618765711784 = 0.0009081920143216848 + 0.001 * 6.621769428253174
Epoch 1780, val loss: 1.7598133087158203
Epoch 1790, training loss: 0.007473777048289776 = 0.0009023150196298957 + 0.001 * 6.5714616775512695
Epoch 1790, val loss: 1.760986566543579
Epoch 1800, training loss: 0.007474461570382118 = 0.0008965602028183639 + 0.001 * 6.577901363372803
Epoch 1800, val loss: 1.7621122598648071
Epoch 1810, training loss: 0.0074657415971159935 = 0.0008909209864214063 + 0.001 * 6.574820518493652
Epoch 1810, val loss: 1.7632580995559692
Epoch 1820, training loss: 0.007457187864929438 = 0.0008853926556184888 + 0.001 * 6.5717949867248535
Epoch 1820, val loss: 1.7643500566482544
Epoch 1830, training loss: 0.007471778895705938 = 0.0008799689821898937 + 0.001 * 6.5918097496032715
Epoch 1830, val loss: 1.765486717224121
Epoch 1840, training loss: 0.007480565924197435 = 0.0008746570674702525 + 0.001 * 6.605908393859863
Epoch 1840, val loss: 1.766523003578186
Epoch 1850, training loss: 0.007488681469112635 = 0.0008694329299032688 + 0.001 * 6.619248390197754
Epoch 1850, val loss: 1.767624020576477
Epoch 1860, training loss: 0.007462101988494396 = 0.0008643274195492268 + 0.001 * 6.597774028778076
Epoch 1860, val loss: 1.76865553855896
Epoch 1870, training loss: 0.007436817046254873 = 0.0008593113161623478 + 0.001 * 6.577505588531494
Epoch 1870, val loss: 1.7697230577468872
Epoch 1880, training loss: 0.007466306909918785 = 0.0008543879957869649 + 0.001 * 6.611918926239014
Epoch 1880, val loss: 1.7707312107086182
Epoch 1890, training loss: 0.00746553111821413 = 0.0008495624642819166 + 0.001 * 6.615968227386475
Epoch 1890, val loss: 1.7717632055282593
Epoch 1900, training loss: 0.007409033831208944 = 0.000844821275677532 + 0.001 * 6.564212322235107
Epoch 1900, val loss: 1.7727389335632324
Epoch 1910, training loss: 0.007437289226800203 = 0.0008401714148931205 + 0.001 * 6.597117900848389
Epoch 1910, val loss: 1.7737236022949219
Epoch 1920, training loss: 0.007430718746036291 = 0.0008355900063179433 + 0.001 * 6.595128536224365
Epoch 1920, val loss: 1.7747821807861328
Epoch 1930, training loss: 0.007396101951599121 = 0.0008310955599881709 + 0.001 * 6.565006256103516
Epoch 1930, val loss: 1.7756874561309814
Epoch 1940, training loss: 0.007394802290946245 = 0.0008266643853858113 + 0.001 * 6.5681376457214355
Epoch 1940, val loss: 1.7766900062561035
Epoch 1950, training loss: 0.007429759483784437 = 0.0008223210461437702 + 0.001 * 6.607438087463379
Epoch 1950, val loss: 1.7776316404342651
Epoch 1960, training loss: 0.007425855379551649 = 0.0008180417353287339 + 0.001 * 6.607813358306885
Epoch 1960, val loss: 1.7785552740097046
Epoch 1970, training loss: 0.007375264074653387 = 0.0008138658595271409 + 0.001 * 6.561397552490234
Epoch 1970, val loss: 1.7794653177261353
Epoch 1980, training loss: 0.007406711578369141 = 0.0008097493555396795 + 0.001 * 6.596961498260498
Epoch 1980, val loss: 1.7803735733032227
Epoch 1990, training loss: 0.007365016266703606 = 0.0008056959486566484 + 0.001 * 6.559319972991943
Epoch 1990, val loss: 1.781332015991211
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7519
Overall ASR: 0.8635
Flip ASR: 0.8400/225 nodes
The final ASR:0.82780, 0.06394, Accuracy:0.80000, 0.03435
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11602])
remove edge: torch.Size([2, 9552])
updated graph: torch.Size([2, 10598])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8444
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9889
Flip ASR: 0.9867/225 nodes
The final ASR:0.98278, 0.00627, Accuracy:0.83457, 0.00924
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9325449466705322 = 1.9241710901260376 + 0.001 * 8.373876571655273
Epoch 0, val loss: 1.9204260110855103
Epoch 10, training loss: 1.9232844114303589 = 1.9149106740951538 + 0.001 * 8.373750686645508
Epoch 10, val loss: 1.9106624126434326
Epoch 20, training loss: 1.9118627309799194 = 1.903489351272583 + 0.001 * 8.373330116271973
Epoch 20, val loss: 1.8980200290679932
Epoch 30, training loss: 1.895797848701477 = 1.887425422668457 + 0.001 * 8.372426986694336
Epoch 30, val loss: 1.8795109987258911
Epoch 40, training loss: 1.8724042177200317 = 1.864033579826355 + 0.001 * 8.370626449584961
Epoch 40, val loss: 1.8525758981704712
Epoch 50, training loss: 1.8397903442382812 = 1.8314239978790283 + 0.001 * 8.36640453338623
Epoch 50, val loss: 1.816893219947815
Epoch 60, training loss: 1.800007700920105 = 1.7916557788848877 + 0.001 * 8.3518648147583
Epoch 60, val loss: 1.778022289276123
Epoch 70, training loss: 1.7576438188552856 = 1.7493650913238525 + 0.001 * 8.278761863708496
Epoch 70, val loss: 1.7425159215927124
Epoch 80, training loss: 1.705890417098999 = 1.6980351209640503 + 0.001 * 7.8553009033203125
Epoch 80, val loss: 1.7012397050857544
Epoch 90, training loss: 1.6348475217819214 = 1.6272903680801392 + 0.001 * 7.557173252105713
Epoch 90, val loss: 1.64369535446167
Epoch 100, training loss: 1.542596459388733 = 1.5351768732070923 + 0.001 * 7.419576168060303
Epoch 100, val loss: 1.570063591003418
Epoch 110, training loss: 1.4381080865859985 = 1.430812954902649 + 0.001 * 7.295097827911377
Epoch 110, val loss: 1.4899566173553467
Epoch 120, training loss: 1.3335886001586914 = 1.3264455795288086 + 0.001 * 7.142988204956055
Epoch 120, val loss: 1.415149211883545
Epoch 130, training loss: 1.2371500730514526 = 1.2300747632980347 + 0.001 * 7.075262069702148
Epoch 130, val loss: 1.3509876728057861
Epoch 140, training loss: 1.1487178802490234 = 1.1416994333267212 + 0.001 * 7.018489837646484
Epoch 140, val loss: 1.293701171875
Epoch 150, training loss: 1.0637857913970947 = 1.0568104982376099 + 0.001 * 6.9753193855285645
Epoch 150, val loss: 1.2370444536209106
Epoch 160, training loss: 0.9783031344413757 = 0.9713530540466309 + 0.001 * 6.950093746185303
Epoch 160, val loss: 1.1765353679656982
Epoch 170, training loss: 0.8919076919555664 = 0.8849760890007019 + 0.001 * 6.931610584259033
Epoch 170, val loss: 1.1129539012908936
Epoch 180, training loss: 0.807055652141571 = 0.800142765045166 + 0.001 * 6.912915229797363
Epoch 180, val loss: 1.0492397546768188
Epoch 190, training loss: 0.727051854133606 = 0.7201521396636963 + 0.001 * 6.899726867675781
Epoch 190, val loss: 0.989006757736206
Epoch 200, training loss: 0.6540266871452332 = 0.6471332907676697 + 0.001 * 6.893422603607178
Epoch 200, val loss: 0.934865415096283
Epoch 210, training loss: 0.5875861048698425 = 0.5806954503059387 + 0.001 * 6.890649795532227
Epoch 210, val loss: 0.8873149156570435
Epoch 220, training loss: 0.5257359147071838 = 0.5188479423522949 + 0.001 * 6.88797664642334
Epoch 220, val loss: 0.8453328609466553
Epoch 230, training loss: 0.46672120690345764 = 0.45983585715293884 + 0.001 * 6.885337829589844
Epoch 230, val loss: 0.807712197303772
Epoch 240, training loss: 0.41002097725868225 = 0.4031376838684082 + 0.001 * 6.883286476135254
Epoch 240, val loss: 0.7745199203491211
Epoch 250, training loss: 0.3563048839569092 = 0.3494230806827545 + 0.001 * 6.881793975830078
Epoch 250, val loss: 0.746177613735199
Epoch 260, training loss: 0.3066580891609192 = 0.29977723956108093 + 0.001 * 6.880840301513672
Epoch 260, val loss: 0.7235748171806335
Epoch 270, training loss: 0.2621007263660431 = 0.2552211582660675 + 0.001 * 6.879558563232422
Epoch 270, val loss: 0.7067584991455078
Epoch 280, training loss: 0.22310090065002441 = 0.21622279286384583 + 0.001 * 6.878106594085693
Epoch 280, val loss: 0.6959311962127686
Epoch 290, training loss: 0.19070196151733398 = 0.18382512032985687 + 0.001 * 6.876848220825195
Epoch 290, val loss: 0.6915065050125122
Epoch 300, training loss: 0.1642056405544281 = 0.1573292315006256 + 0.001 * 6.876415729522705
Epoch 300, val loss: 0.6907793283462524
Epoch 310, training loss: 0.1423037052154541 = 0.13542860746383667 + 0.001 * 6.875096321105957
Epoch 310, val loss: 0.695252001285553
Epoch 320, training loss: 0.12430409342050552 = 0.11742986738681793 + 0.001 * 6.8742241859436035
Epoch 320, val loss: 0.7001737356185913
Epoch 330, training loss: 0.10937947034835815 = 0.10250276327133179 + 0.001 * 6.876705169677734
Epoch 330, val loss: 0.70868319272995
Epoch 340, training loss: 0.09688500314950943 = 0.09001211076974869 + 0.001 * 6.872895240783691
Epoch 340, val loss: 0.7185211777687073
Epoch 350, training loss: 0.08635131269693375 = 0.07947929203510284 + 0.001 * 6.872020721435547
Epoch 350, val loss: 0.7295543551445007
Epoch 360, training loss: 0.07740531116724014 = 0.07053423672914505 + 0.001 * 6.871070861816406
Epoch 360, val loss: 0.7417891025543213
Epoch 370, training loss: 0.06975843012332916 = 0.06288786977529526 + 0.001 * 6.870558261871338
Epoch 370, val loss: 0.7546204924583435
Epoch 380, training loss: 0.0631791427731514 = 0.056310176849365234 + 0.001 * 6.868965148925781
Epoch 380, val loss: 0.7680736184120178
Epoch 390, training loss: 0.05748318135738373 = 0.05061522498726845 + 0.001 * 6.867954730987549
Epoch 390, val loss: 0.7819089889526367
Epoch 400, training loss: 0.052518755197525024 = 0.04565314203500748 + 0.001 * 6.865614414215088
Epoch 400, val loss: 0.7960437536239624
Epoch 410, training loss: 0.048173047602176666 = 0.0413045734167099 + 0.001 * 6.868475437164307
Epoch 410, val loss: 0.810286283493042
Epoch 420, training loss: 0.04433348774909973 = 0.03747057914733887 + 0.001 * 6.862907886505127
Epoch 420, val loss: 0.8246471285820007
Epoch 430, training loss: 0.04093066602945328 = 0.034070007503032684 + 0.001 * 6.860659122467041
Epoch 430, val loss: 0.8389844298362732
Epoch 440, training loss: 0.03791518881917 = 0.031056484207510948 + 0.001 * 6.85870361328125
Epoch 440, val loss: 0.8532306551933289
Epoch 450, training loss: 0.03523947671055794 = 0.02838602103292942 + 0.001 * 6.85345458984375
Epoch 450, val loss: 0.8673650026321411
Epoch 460, training loss: 0.03288358822464943 = 0.026026515290141106 + 0.001 * 6.857071876525879
Epoch 460, val loss: 0.8812374472618103
Epoch 470, training loss: 0.030773388221859932 = 0.023929111659526825 + 0.001 * 6.844275951385498
Epoch 470, val loss: 0.8948552012443542
Epoch 480, training loss: 0.02889670804142952 = 0.022061746567487717 + 0.001 * 6.834960460662842
Epoch 480, val loss: 0.9080327749252319
Epoch 490, training loss: 0.027235165238380432 = 0.020398220047354698 + 0.001 * 6.8369460105896
Epoch 490, val loss: 0.9208812713623047
Epoch 500, training loss: 0.02574058435857296 = 0.01891244202852249 + 0.001 * 6.828142166137695
Epoch 500, val loss: 0.933290421962738
Epoch 510, training loss: 0.024419663473963737 = 0.017582032829523087 + 0.001 * 6.837630271911621
Epoch 510, val loss: 0.9453237056732178
Epoch 520, training loss: 0.02320193313062191 = 0.016387294977903366 + 0.001 * 6.814638137817383
Epoch 520, val loss: 0.9569995999336243
Epoch 530, training loss: 0.02214778959751129 = 0.015311216935515404 + 0.001 * 6.836572647094727
Epoch 530, val loss: 0.9683094024658203
Epoch 540, training loss: 0.021145327016711235 = 0.01433953084051609 + 0.001 * 6.805795192718506
Epoch 540, val loss: 0.9792892932891846
Epoch 550, training loss: 0.02025836706161499 = 0.013459475710988045 + 0.001 * 6.798890590667725
Epoch 550, val loss: 0.989885687828064
Epoch 560, training loss: 0.019453711807727814 = 0.012660129927098751 + 0.001 * 6.793580532073975
Epoch 560, val loss: 1.0001527070999146
Epoch 570, training loss: 0.01873120665550232 = 0.011932066641747952 + 0.001 * 6.79913854598999
Epoch 570, val loss: 1.0101438760757446
Epoch 580, training loss: 0.01806069351732731 = 0.011267161928117275 + 0.001 * 6.79353141784668
Epoch 580, val loss: 1.019831657409668
Epoch 590, training loss: 0.017465662211179733 = 0.0106583833694458 + 0.001 * 6.807277679443359
Epoch 590, val loss: 1.0292326211929321
Epoch 600, training loss: 0.016889577731490135 = 0.01009983941912651 + 0.001 * 6.789738178253174
Epoch 600, val loss: 1.0383375883102417
Epoch 610, training loss: 0.016383150592446327 = 0.009586084634065628 + 0.001 * 6.797065258026123
Epoch 610, val loss: 1.04719877243042
Epoch 620, training loss: 0.01589900068938732 = 0.00911248754709959 + 0.001 * 6.78651237487793
Epoch 620, val loss: 1.055840253829956
Epoch 630, training loss: 0.015449654310941696 = 0.008674941956996918 + 0.001 * 6.774711608886719
Epoch 630, val loss: 1.0642962455749512
Epoch 640, training loss: 0.015055828727781773 = 0.00826991070061922 + 0.001 * 6.78591775894165
Epoch 640, val loss: 1.0724835395812988
Epoch 650, training loss: 0.014669058844447136 = 0.007894255220890045 + 0.001 * 6.7748026847839355
Epoch 650, val loss: 1.0804897546768188
Epoch 660, training loss: 0.014315520413219929 = 0.00754520995542407 + 0.001 * 6.770309925079346
Epoch 660, val loss: 1.0882716178894043
Epoch 670, training loss: 0.013989675790071487 = 0.007220244966447353 + 0.001 * 6.769430637359619
Epoch 670, val loss: 1.0958956480026245
Epoch 680, training loss: 0.01368969026952982 = 0.006917198188602924 + 0.001 * 6.772491931915283
Epoch 680, val loss: 1.1033412218093872
Epoch 690, training loss: 0.013420913368463516 = 0.006634153891354799 + 0.001 * 6.786759376525879
Epoch 690, val loss: 1.1105904579162598
Epoch 700, training loss: 0.013128723949193954 = 0.006369384005665779 + 0.001 * 6.759339332580566
Epoch 700, val loss: 1.1176615953445435
Epoch 710, training loss: 0.012910161167383194 = 0.0061213127337396145 + 0.001 * 6.788847923278809
Epoch 710, val loss: 1.1245834827423096
Epoch 720, training loss: 0.01265876553952694 = 0.005888561252504587 + 0.001 * 6.770203590393066
Epoch 720, val loss: 1.1313532590866089
Epoch 730, training loss: 0.0124285239726305 = 0.005669882521033287 + 0.001 * 6.758641242980957
Epoch 730, val loss: 1.1379742622375488
Epoch 740, training loss: 0.012219177559018135 = 0.005464176181703806 + 0.001 * 6.755000591278076
Epoch 740, val loss: 1.1444482803344727
Epoch 750, training loss: 0.012037563137710094 = 0.005270356312394142 + 0.001 * 6.76720666885376
Epoch 750, val loss: 1.1507965326309204
Epoch 760, training loss: 0.011839400976896286 = 0.005087577737867832 + 0.001 * 6.751822471618652
Epoch 760, val loss: 1.1569806337356567
Epoch 770, training loss: 0.011665983125567436 = 0.004915002733469009 + 0.001 * 6.750980377197266
Epoch 770, val loss: 1.16304349899292
Epoch 780, training loss: 0.011511305347084999 = 0.004751886706799269 + 0.001 * 6.75941801071167
Epoch 780, val loss: 1.1689742803573608
Epoch 790, training loss: 0.011345963925123215 = 0.004597597289830446 + 0.001 * 6.748366832733154
Epoch 790, val loss: 1.1747876405715942
Epoch 800, training loss: 0.011203240603208542 = 0.004451593849807978 + 0.001 * 6.751646518707275
Epoch 800, val loss: 1.1804850101470947
Epoch 810, training loss: 0.0110652856528759 = 0.004313131794333458 + 0.001 * 6.7521538734436035
Epoch 810, val loss: 1.1860554218292236
Epoch 820, training loss: 0.010933331213891506 = 0.004181802272796631 + 0.001 * 6.751528739929199
Epoch 820, val loss: 1.1915470361709595
Epoch 830, training loss: 0.010804342105984688 = 0.004057193174958229 + 0.001 * 6.747148036956787
Epoch 830, val loss: 1.1969157457351685
Epoch 840, training loss: 0.010683827102184296 = 0.003938604611903429 + 0.001 * 6.745222568511963
Epoch 840, val loss: 1.2021782398223877
Epoch 850, training loss: 0.010563611052930355 = 0.0038254596292972565 + 0.001 * 6.7381510734558105
Epoch 850, val loss: 1.2073155641555786
Epoch 860, training loss: 0.010448972694575787 = 0.003717255312949419 + 0.001 * 6.731717109680176
Epoch 860, val loss: 1.2124145030975342
Epoch 870, training loss: 0.010367005132138729 = 0.003613072680309415 + 0.001 * 6.753931999206543
Epoch 870, val loss: 1.2174394130706787
Epoch 880, training loss: 0.01024459395557642 = 0.0035122379194945097 + 0.001 * 6.732356071472168
Epoch 880, val loss: 1.2224013805389404
Epoch 890, training loss: 0.010141059756278992 = 0.003414496313780546 + 0.001 * 6.726563453674316
Epoch 890, val loss: 1.227358102798462
Epoch 900, training loss: 0.010034357197582722 = 0.0033197070006281137 + 0.001 * 6.714649677276611
Epoch 900, val loss: 1.2322386503219604
Epoch 910, training loss: 0.009975221939384937 = 0.003227791516110301 + 0.001 * 6.747430324554443
Epoch 910, val loss: 1.2371466159820557
Epoch 920, training loss: 0.009844780899584293 = 0.003138766624033451 + 0.001 * 6.706014156341553
Epoch 920, val loss: 1.2420024871826172
Epoch 930, training loss: 0.00977206602692604 = 0.0030526360496878624 + 0.001 * 6.719429969787598
Epoch 930, val loss: 1.2467930316925049
Epoch 940, training loss: 0.009679595939815044 = 0.0029697774443775415 + 0.001 * 6.709818363189697
Epoch 940, val loss: 1.2515935897827148
Epoch 950, training loss: 0.009604066610336304 = 0.002889947034418583 + 0.001 * 6.714119911193848
Epoch 950, val loss: 1.2562834024429321
Epoch 960, training loss: 0.009518938139081001 = 0.0028131732251495123 + 0.001 * 6.7057647705078125
Epoch 960, val loss: 1.2608983516693115
Epoch 970, training loss: 0.009439831599593163 = 0.0027394250500947237 + 0.001 * 6.700406074523926
Epoch 970, val loss: 1.2654778957366943
Epoch 980, training loss: 0.00938771478831768 = 0.002668693196028471 + 0.001 * 6.719021797180176
Epoch 980, val loss: 1.270015835762024
Epoch 990, training loss: 0.009315749630331993 = 0.002600946929305792 + 0.001 * 6.714801788330078
Epoch 990, val loss: 1.2744629383087158
Epoch 1000, training loss: 0.009234183467924595 = 0.002535986015573144 + 0.001 * 6.698197364807129
Epoch 1000, val loss: 1.2788527011871338
Epoch 1010, training loss: 0.009187852032482624 = 0.002473748754709959 + 0.001 * 6.714102745056152
Epoch 1010, val loss: 1.283134937286377
Epoch 1020, training loss: 0.009109897539019585 = 0.002414136193692684 + 0.001 * 6.695761203765869
Epoch 1020, val loss: 1.2874103784561157
Epoch 1030, training loss: 0.009058624505996704 = 0.0023570475168526173 + 0.001 * 6.7015767097473145
Epoch 1030, val loss: 1.2915880680084229
Epoch 1040, training loss: 0.00898173451423645 = 0.0023022685199975967 + 0.001 * 6.6794657707214355
Epoch 1040, val loss: 1.2956464290618896
Epoch 1050, training loss: 0.008960328064858913 = 0.002249709563329816 + 0.001 * 6.710618495941162
Epoch 1050, val loss: 1.2997148036956787
Epoch 1060, training loss: 0.00893696304410696 = 0.0021993054542690516 + 0.001 * 6.73765754699707
Epoch 1060, val loss: 1.303675651550293
Epoch 1070, training loss: 0.008830919861793518 = 0.002150894608348608 + 0.001 * 6.680025100708008
Epoch 1070, val loss: 1.3075802326202393
Epoch 1080, training loss: 0.008802671916782856 = 0.002104473067447543 + 0.001 * 6.698198318481445
Epoch 1080, val loss: 1.311332106590271
Epoch 1090, training loss: 0.008751163259148598 = 0.002059919061139226 + 0.001 * 6.691244125366211
Epoch 1090, val loss: 1.3151235580444336
Epoch 1100, training loss: 0.008725265972316265 = 0.002017134567722678 + 0.001 * 6.708130836486816
Epoch 1100, val loss: 1.3188941478729248
Epoch 1110, training loss: 0.0086671756580472 = 0.001976052299141884 + 0.001 * 6.691123008728027
Epoch 1110, val loss: 1.3225011825561523
Epoch 1120, training loss: 0.008655652403831482 = 0.00193662173114717 + 0.001 * 6.719030380249023
Epoch 1120, val loss: 1.3260575532913208
Epoch 1130, training loss: 0.008570986799895763 = 0.0018987770890817046 + 0.001 * 6.6722092628479
Epoch 1130, val loss: 1.3295577764511108
Epoch 1140, training loss: 0.008567350916564465 = 0.0018624154618009925 + 0.001 * 6.704935550689697
Epoch 1140, val loss: 1.3329451084136963
Epoch 1150, training loss: 0.008512446656823158 = 0.001827481435611844 + 0.001 * 6.684964656829834
Epoch 1150, val loss: 1.3363014459609985
Epoch 1160, training loss: 0.008512632921338081 = 0.0017938115634024143 + 0.001 * 6.718821048736572
Epoch 1160, val loss: 1.3396921157836914
Epoch 1170, training loss: 0.008419272489845753 = 0.0017613655654713511 + 0.001 * 6.657906532287598
Epoch 1170, val loss: 1.3429243564605713
Epoch 1180, training loss: 0.008439299650490284 = 0.0017301220213994384 + 0.001 * 6.709177017211914
Epoch 1180, val loss: 1.3460543155670166
Epoch 1190, training loss: 0.008345209062099457 = 0.001700012944638729 + 0.001 * 6.645195484161377
Epoch 1190, val loss: 1.3492417335510254
Epoch 1200, training loss: 0.008336200378835201 = 0.001670941594056785 + 0.001 * 6.665258407592773
Epoch 1200, val loss: 1.3523112535476685
Epoch 1210, training loss: 0.008323868736624718 = 0.0016428761882707477 + 0.001 * 6.680992126464844
Epoch 1210, val loss: 1.355324149131775
Epoch 1220, training loss: 0.008273471146821976 = 0.001615795772522688 + 0.001 * 6.657675743103027
Epoch 1220, val loss: 1.358314871788025
Epoch 1230, training loss: 0.008266286924481392 = 0.001589643768966198 + 0.001 * 6.676642894744873
Epoch 1230, val loss: 1.3612558841705322
Epoch 1240, training loss: 0.008229703642427921 = 0.0015643925871700048 + 0.001 * 6.665310382843018
Epoch 1240, val loss: 1.364126205444336
Epoch 1250, training loss: 0.008200015872716904 = 0.0015400153351947665 + 0.001 * 6.660000324249268
Epoch 1250, val loss: 1.3669836521148682
Epoch 1260, training loss: 0.008151661604642868 = 0.001516438671387732 + 0.001 * 6.635222434997559
Epoch 1260, val loss: 1.3697576522827148
Epoch 1270, training loss: 0.008157922886312008 = 0.0014936703955754638 + 0.001 * 6.664252281188965
Epoch 1270, val loss: 1.372460126876831
Epoch 1280, training loss: 0.008122032508254051 = 0.001471610739827156 + 0.001 * 6.650421142578125
Epoch 1280, val loss: 1.375143051147461
Epoch 1290, training loss: 0.008124832063913345 = 0.0014503445709124207 + 0.001 * 6.6744866371154785
Epoch 1290, val loss: 1.3778038024902344
Epoch 1300, training loss: 0.008059922605752945 = 0.0014297764282673597 + 0.001 * 6.63014554977417
Epoch 1300, val loss: 1.3803695440292358
Epoch 1310, training loss: 0.008056284859776497 = 0.0014098500832915306 + 0.001 * 6.646434783935547
Epoch 1310, val loss: 1.3828918933868408
Epoch 1320, training loss: 0.008036864921450615 = 0.0013905452797189355 + 0.001 * 6.646319389343262
Epoch 1320, val loss: 1.3853765726089478
Epoch 1330, training loss: 0.008109472692012787 = 0.0013717967085540295 + 0.001 * 6.73767614364624
Epoch 1330, val loss: 1.3878329992294312
Epoch 1340, training loss: 0.008006465621292591 = 0.0013536445330828428 + 0.001 * 6.652820587158203
Epoch 1340, val loss: 1.390282392501831
Epoch 1350, training loss: 0.00798233225941658 = 0.0013360256562009454 + 0.001 * 6.646306037902832
Epoch 1350, val loss: 1.3926703929901123
Epoch 1360, training loss: 0.007964934222400188 = 0.0013189406599849463 + 0.001 * 6.645993709564209
Epoch 1360, val loss: 1.395033359527588
Epoch 1370, training loss: 0.00792072992771864 = 0.0013023469364270568 + 0.001 * 6.618382453918457
Epoch 1370, val loss: 1.3973435163497925
Epoch 1380, training loss: 0.00791122205555439 = 0.0012862341245636344 + 0.001 * 6.624988079071045
Epoch 1380, val loss: 1.3996241092681885
Epoch 1390, training loss: 0.007902403362095356 = 0.0012705641565844417 + 0.001 * 6.631838798522949
Epoch 1390, val loss: 1.4018768072128296
Epoch 1400, training loss: 0.00789934117347002 = 0.0012553551932796836 + 0.001 * 6.643986225128174
Epoch 1400, val loss: 1.4040616750717163
Epoch 1410, training loss: 0.007879129610955715 = 0.001240579062141478 + 0.001 * 6.6385498046875
Epoch 1410, val loss: 1.4062747955322266
Epoch 1420, training loss: 0.007894075475633144 = 0.0012261938536539674 + 0.001 * 6.667881488800049
Epoch 1420, val loss: 1.408440113067627
Epoch 1430, training loss: 0.007825750857591629 = 0.0012122561456635594 + 0.001 * 6.613493919372559
Epoch 1430, val loss: 1.4105318784713745
Epoch 1440, training loss: 0.007799963932484388 = 0.0011987137841060758 + 0.001 * 6.601250171661377
Epoch 1440, val loss: 1.4126026630401611
Epoch 1450, training loss: 0.00780476164072752 = 0.0011855338234454393 + 0.001 * 6.619227409362793
Epoch 1450, val loss: 1.4146687984466553
Epoch 1460, training loss: 0.007794379256665707 = 0.0011727310484275222 + 0.001 * 6.621647834777832
Epoch 1460, val loss: 1.4166603088378906
Epoch 1470, training loss: 0.007827606983482838 = 0.001160246436484158 + 0.001 * 6.667360305786133
Epoch 1470, val loss: 1.4186766147613525
Epoch 1480, training loss: 0.007759619504213333 = 0.0011480964021757245 + 0.001 * 6.611522674560547
Epoch 1480, val loss: 1.4206637144088745
Epoch 1490, training loss: 0.007778725121170282 = 0.0011363012017682195 + 0.001 * 6.642423629760742
Epoch 1490, val loss: 1.422566294670105
Epoch 1500, training loss: 0.007744940929114819 = 0.0011248327791690826 + 0.001 * 6.620107650756836
Epoch 1500, val loss: 1.4244778156280518
Epoch 1510, training loss: 0.007727666292339563 = 0.0011136461980640888 + 0.001 * 6.614019870758057
Epoch 1510, val loss: 1.426340103149414
Epoch 1520, training loss: 0.007685639895498753 = 0.0011027698637917638 + 0.001 * 6.582870006561279
Epoch 1520, val loss: 1.4281669855117798
Epoch 1530, training loss: 0.0077051930129528046 = 0.001092171180061996 + 0.001 * 6.613021373748779
Epoch 1530, val loss: 1.4299452304840088
Epoch 1540, training loss: 0.00770865473896265 = 0.0010818267473950982 + 0.001 * 6.626827716827393
Epoch 1540, val loss: 1.431749701499939
Epoch 1550, training loss: 0.007665921002626419 = 0.001071749022230506 + 0.001 * 6.59417200088501
Epoch 1550, val loss: 1.4335132837295532
Epoch 1560, training loss: 0.007667032070457935 = 0.001061915187165141 + 0.001 * 6.605116844177246
Epoch 1560, val loss: 1.4352115392684937
Epoch 1570, training loss: 0.007650410756468773 = 0.001052330480888486 + 0.001 * 6.598079681396484
Epoch 1570, val loss: 1.4368984699249268
Epoch 1580, training loss: 0.0076646083034574986 = 0.0010429766261950135 + 0.001 * 6.621631622314453
Epoch 1580, val loss: 1.438612937927246
Epoch 1590, training loss: 0.007675905246287584 = 0.0010338701540604234 + 0.001 * 6.642035007476807
Epoch 1590, val loss: 1.4402650594711304
Epoch 1600, training loss: 0.0076196687296032906 = 0.0010249583283439279 + 0.001 * 6.594710350036621
Epoch 1600, val loss: 1.4419193267822266
Epoch 1610, training loss: 0.007607117295265198 = 0.0010162878315895796 + 0.001 * 6.590829372406006
Epoch 1610, val loss: 1.443571925163269
Epoch 1620, training loss: 0.007607216015458107 = 0.0010078069753944874 + 0.001 * 6.5994086265563965
Epoch 1620, val loss: 1.4451440572738647
Epoch 1630, training loss: 0.007602164521813393 = 0.0009995519649237394 + 0.001 * 6.602612495422363
Epoch 1630, val loss: 1.446682095527649
Epoch 1640, training loss: 0.007580529898405075 = 0.0009914895053952932 + 0.001 * 6.589040279388428
Epoch 1640, val loss: 1.4482141733169556
Epoch 1650, training loss: 0.007542939390987158 = 0.0009836110984906554 + 0.001 * 6.559328079223633
Epoch 1650, val loss: 1.4497809410095215
Epoch 1660, training loss: 0.00755887757986784 = 0.0009759331587702036 + 0.001 * 6.582943916320801
Epoch 1660, val loss: 1.4512557983398438
Epoch 1670, training loss: 0.007563460618257523 = 0.0009684457909315825 + 0.001 * 6.5950140953063965
Epoch 1670, val loss: 1.4527560472488403
Epoch 1680, training loss: 0.0075409854762256145 = 0.0009611313580535352 + 0.001 * 6.579853534698486
Epoch 1680, val loss: 1.4541683197021484
Epoch 1690, training loss: 0.0075223734602332115 = 0.000953965587541461 + 0.001 * 6.5684075355529785
Epoch 1690, val loss: 1.4556323289871216
Epoch 1700, training loss: 0.007529301103204489 = 0.0009469540673308074 + 0.001 * 6.5823469161987305
Epoch 1700, val loss: 1.4570635557174683
Epoch 1710, training loss: 0.007546367589384317 = 0.0009401129209436476 + 0.001 * 6.606254577636719
Epoch 1710, val loss: 1.4584629535675049
Epoch 1720, training loss: 0.0075154476799070835 = 0.0009334102505818009 + 0.001 * 6.582037448883057
Epoch 1720, val loss: 1.4598541259765625
Epoch 1730, training loss: 0.007498325780034065 = 0.000926857115700841 + 0.001 * 6.571468353271484
Epoch 1730, val loss: 1.4612122774124146
Epoch 1740, training loss: 0.007528085261583328 = 0.0009204309899359941 + 0.001 * 6.607654094696045
Epoch 1740, val loss: 1.462589979171753
Epoch 1750, training loss: 0.007491108495742083 = 0.0009141489281319082 + 0.001 * 6.576959133148193
Epoch 1750, val loss: 1.463895320892334
Epoch 1760, training loss: 0.0074836681596934795 = 0.0009080023737624288 + 0.001 * 6.575665473937988
Epoch 1760, val loss: 1.4652093648910522
Epoch 1770, training loss: 0.007461844012141228 = 0.0009019756689667702 + 0.001 * 6.559867858886719
Epoch 1770, val loss: 1.4665091037750244
Epoch 1780, training loss: 0.007493215147405863 = 0.0008960686391219497 + 0.001 * 6.597146034240723
Epoch 1780, val loss: 1.4677953720092773
Epoch 1790, training loss: 0.007452822756022215 = 0.0008903037523850799 + 0.001 * 6.56251859664917
Epoch 1790, val loss: 1.4690678119659424
Epoch 1800, training loss: 0.007453753147274256 = 0.0008846460841596127 + 0.001 * 6.569106578826904
Epoch 1800, val loss: 1.4703099727630615
Epoch 1810, training loss: 0.00747046060860157 = 0.000879099708981812 + 0.001 * 6.591360569000244
Epoch 1810, val loss: 1.4715838432312012
Epoch 1820, training loss: 0.007448120042681694 = 0.0008736568270251155 + 0.001 * 6.574462890625
Epoch 1820, val loss: 1.4727836847305298
Epoch 1830, training loss: 0.007425339892506599 = 0.0008683331543579698 + 0.001 * 6.557006359100342
Epoch 1830, val loss: 1.4739432334899902
Epoch 1840, training loss: 0.007416111417114735 = 0.000863115128595382 + 0.001 * 6.552995681762695
Epoch 1840, val loss: 1.4751782417297363
Epoch 1850, training loss: 0.007407144643366337 = 0.0008580114808864892 + 0.001 * 6.549132823944092
Epoch 1850, val loss: 1.4763346910476685
Epoch 1860, training loss: 0.007404915057122707 = 0.0008530017221346498 + 0.001 * 6.551913261413574
Epoch 1860, val loss: 1.4774681329727173
Epoch 1870, training loss: 0.007379353977739811 = 0.0008480940014123917 + 0.001 * 6.531259536743164
Epoch 1870, val loss: 1.4786046743392944
Epoch 1880, training loss: 0.007373372558504343 = 0.0008432661416009068 + 0.001 * 6.530106067657471
Epoch 1880, val loss: 1.4797435998916626
Epoch 1890, training loss: 0.007424588315188885 = 0.0008385287947021425 + 0.001 * 6.586059093475342
Epoch 1890, val loss: 1.4808766841888428
Epoch 1900, training loss: 0.007394837215542793 = 0.0008338811458088458 + 0.001 * 6.560956001281738
Epoch 1900, val loss: 1.4820066690444946
Epoch 1910, training loss: 0.007362960837781429 = 0.0008293116698041558 + 0.001 * 6.53364896774292
Epoch 1910, val loss: 1.4830632209777832
Epoch 1920, training loss: 0.007363514043390751 = 0.0008248177473433316 + 0.001 * 6.538695812225342
Epoch 1920, val loss: 1.4841997623443604
Epoch 1930, training loss: 0.007362072821706533 = 0.0008203830802813172 + 0.001 * 6.541689395904541
Epoch 1930, val loss: 1.4852782487869263
Epoch 1940, training loss: 0.007391388528048992 = 0.0008159552817232907 + 0.001 * 6.575432777404785
Epoch 1940, val loss: 1.486319661140442
Epoch 1950, training loss: 0.007369747385382652 = 0.0008115851669572294 + 0.001 * 6.558161735534668
Epoch 1950, val loss: 1.4874114990234375
Epoch 1960, training loss: 0.007337882183492184 = 0.0008072892087511718 + 0.001 * 6.530592918395996
Epoch 1960, val loss: 1.4883887767791748
Epoch 1970, training loss: 0.007347653619945049 = 0.0008030643220990896 + 0.001 * 6.544588565826416
Epoch 1970, val loss: 1.4894448518753052
Epoch 1980, training loss: 0.00735176308080554 = 0.0007989168516360223 + 0.001 * 6.5528459548950195
Epoch 1980, val loss: 1.4904903173446655
Epoch 1990, training loss: 0.007366968784481287 = 0.0007948576821945608 + 0.001 * 6.572110652923584
Epoch 1990, val loss: 1.491476058959961
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9766494035720825 = 1.968275547027588 + 0.001 * 8.373878479003906
Epoch 0, val loss: 1.9696035385131836
Epoch 10, training loss: 1.9652574062347412 = 1.9568836688995361 + 0.001 * 8.373796463012695
Epoch 10, val loss: 1.9578832387924194
Epoch 20, training loss: 1.951074242591858 = 1.942700743675232 + 0.001 * 8.373549461364746
Epoch 20, val loss: 1.94307541847229
Epoch 30, training loss: 1.9309468269348145 = 1.9225739240646362 + 0.001 * 8.372953414916992
Epoch 30, val loss: 1.9218732118606567
Epoch 40, training loss: 1.901119351387024 = 1.8927479982376099 + 0.001 * 8.371394157409668
Epoch 40, val loss: 1.890854001045227
Epoch 50, training loss: 1.859460711479187 = 1.851094365119934 + 0.001 * 8.36629867553711
Epoch 50, val loss: 1.849568247795105
Epoch 60, training loss: 1.8136625289916992 = 1.8053184747695923 + 0.001 * 8.34410572052002
Epoch 60, val loss: 1.808786392211914
Epoch 70, training loss: 1.777158260345459 = 1.7689261436462402 + 0.001 * 8.232135772705078
Epoch 70, val loss: 1.7789894342422485
Epoch 80, training loss: 1.7350540161132812 = 1.7272061109542847 + 0.001 * 7.847881317138672
Epoch 80, val loss: 1.7410662174224854
Epoch 90, training loss: 1.6783169507980347 = 1.6705453395843506 + 0.001 * 7.7716383934021
Epoch 90, val loss: 1.690565586090088
Epoch 100, training loss: 1.6013768911361694 = 1.5937176942825317 + 0.001 * 7.659155368804932
Epoch 100, val loss: 1.6236721277236938
Epoch 110, training loss: 1.50777268409729 = 1.5002890825271606 + 0.001 * 7.483563423156738
Epoch 110, val loss: 1.543992280960083
Epoch 120, training loss: 1.410096526145935 = 1.402889609336853 + 0.001 * 7.206934452056885
Epoch 120, val loss: 1.4643064737319946
Epoch 130, training loss: 1.3157310485839844 = 1.3086344003677368 + 0.001 * 7.096651554107666
Epoch 130, val loss: 1.3907017707824707
Epoch 140, training loss: 1.2229139804840088 = 1.215867519378662 + 0.001 * 7.0464887619018555
Epoch 140, val loss: 1.3210150003433228
Epoch 150, training loss: 1.1293386220932007 = 1.1223360300064087 + 0.001 * 7.002584934234619
Epoch 150, val loss: 1.2515313625335693
Epoch 160, training loss: 1.0365022420883179 = 1.029542088508606 + 0.001 * 6.960139751434326
Epoch 160, val loss: 1.183576226234436
Epoch 170, training loss: 0.9476535320281982 = 0.9407245516777039 + 0.001 * 6.928957939147949
Epoch 170, val loss: 1.1191829442977905
Epoch 180, training loss: 0.864955723285675 = 0.858040452003479 + 0.001 * 6.915292739868164
Epoch 180, val loss: 1.0596264600753784
Epoch 190, training loss: 0.7884852290153503 = 0.781574010848999 + 0.001 * 6.911219596862793
Epoch 190, val loss: 1.004975438117981
Epoch 200, training loss: 0.7169211506843567 = 0.7100122570991516 + 0.001 * 6.9088664054870605
Epoch 200, val loss: 0.9544028043746948
Epoch 210, training loss: 0.6488237380981445 = 0.6419169306755066 + 0.001 * 6.906832695007324
Epoch 210, val loss: 0.9063418507575989
Epoch 220, training loss: 0.5834515690803528 = 0.5765466094017029 + 0.001 * 6.904984474182129
Epoch 220, val loss: 0.8599011898040771
Epoch 230, training loss: 0.5210796594619751 = 0.514176607131958 + 0.001 * 6.903079986572266
Epoch 230, val loss: 0.8157413005828857
Epoch 240, training loss: 0.46255090832710266 = 0.4556500017642975 + 0.001 * 6.900894641876221
Epoch 240, val loss: 0.7752527594566345
Epoch 250, training loss: 0.40828433632850647 = 0.4013858139514923 + 0.001 * 6.898523330688477
Epoch 250, val loss: 0.7402293682098389
Epoch 260, training loss: 0.35811087489128113 = 0.3512146770954132 + 0.001 * 6.896194934844971
Epoch 260, val loss: 0.7108988165855408
Epoch 270, training loss: 0.31181755661964417 = 0.3049233555793762 + 0.001 * 6.894199848175049
Epoch 270, val loss: 0.686965823173523
Epoch 280, training loss: 0.2695674002170563 = 0.2626744508743286 + 0.001 * 6.892941951751709
Epoch 280, val loss: 0.6681175231933594
Epoch 290, training loss: 0.23183833062648773 = 0.22494632005691528 + 0.001 * 6.892007350921631
Epoch 290, val loss: 0.6541861295700073
Epoch 300, training loss: 0.19913192093372345 = 0.19224022328853607 + 0.001 * 6.891692638397217
Epoch 300, val loss: 0.6447638869285583
Epoch 310, training loss: 0.17151083052158356 = 0.16461826860904694 + 0.001 * 6.892565727233887
Epoch 310, val loss: 0.6397122144699097
Epoch 320, training loss: 0.14852431416511536 = 0.1416315734386444 + 0.001 * 6.892735481262207
Epoch 320, val loss: 0.6386421322822571
Epoch 330, training loss: 0.12948700785636902 = 0.12259349972009659 + 0.001 * 6.893507480621338
Epoch 330, val loss: 0.6410102844238281
Epoch 340, training loss: 0.11368119716644287 = 0.10678613185882568 + 0.001 * 6.895063877105713
Epoch 340, val loss: 0.6461523175239563
Epoch 350, training loss: 0.10047326982021332 = 0.09357788413763046 + 0.001 * 6.895387649536133
Epoch 350, val loss: 0.6534480452537537
Epoch 360, training loss: 0.0893484503030777 = 0.0824522152543068 + 0.001 * 6.896234512329102
Epoch 360, val loss: 0.6624056100845337
Epoch 370, training loss: 0.07989822328090668 = 0.07300180196762085 + 0.001 * 6.896422863006592
Epoch 370, val loss: 0.6725530624389648
Epoch 380, training loss: 0.07180806249380112 = 0.06491106003522873 + 0.001 * 6.896999835968018
Epoch 380, val loss: 0.6834404468536377
Epoch 390, training loss: 0.06483088433742523 = 0.05793409049510956 + 0.001 * 6.896790027618408
Epoch 390, val loss: 0.6948842406272888
Epoch 400, training loss: 0.05878445506095886 = 0.05188595876097679 + 0.001 * 6.898498058319092
Epoch 400, val loss: 0.7066360712051392
Epoch 410, training loss: 0.053516779094934464 = 0.04662021994590759 + 0.001 * 6.896557331085205
Epoch 410, val loss: 0.7184813618659973
Epoch 420, training loss: 0.04892005771398544 = 0.04202379286289215 + 0.001 * 6.896263599395752
Epoch 420, val loss: 0.7303043603897095
Epoch 430, training loss: 0.044907208532094955 = 0.03800357133150101 + 0.001 * 6.903636932373047
Epoch 430, val loss: 0.7420728206634521
Epoch 440, training loss: 0.04137493297457695 = 0.03448110073804855 + 0.001 * 6.893833160400391
Epoch 440, val loss: 0.7537130117416382
Epoch 450, training loss: 0.038279030472040176 = 0.03138846904039383 + 0.001 * 6.890560626983643
Epoch 450, val loss: 0.765324056148529
Epoch 460, training loss: 0.03555598482489586 = 0.028667254373431206 + 0.001 * 6.888729572296143
Epoch 460, val loss: 0.7766697406768799
Epoch 470, training loss: 0.03316229209303856 = 0.026265667751431465 + 0.001 * 6.896623134613037
Epoch 470, val loss: 0.7878791689872742
Epoch 480, training loss: 0.031027693301439285 = 0.024140354245901108 + 0.001 * 6.8873395919799805
Epoch 480, val loss: 0.7988503575325012
Epoch 490, training loss: 0.0291313324123621 = 0.02225331962108612 + 0.001 * 6.878012180328369
Epoch 490, val loss: 0.8096094131469727
Epoch 500, training loss: 0.02756756730377674 = 0.020572170615196228 + 0.001 * 6.995396137237549
Epoch 500, val loss: 0.8202215433120728
Epoch 510, training loss: 0.025964850559830666 = 0.019069405272603035 + 0.001 * 6.895444869995117
Epoch 510, val loss: 0.8304890394210815
Epoch 520, training loss: 0.024584762752056122 = 0.017721878364682198 + 0.001 * 6.862883567810059
Epoch 520, val loss: 0.8405590057373047
Epoch 530, training loss: 0.023367077112197876 = 0.016509966924786568 + 0.001 * 6.857110977172852
Epoch 530, val loss: 0.8503882884979248
Epoch 540, training loss: 0.022266969084739685 = 0.015416891314089298 + 0.001 * 6.8500776290893555
Epoch 540, val loss: 0.8599258065223694
Epoch 550, training loss: 0.02127918228507042 = 0.0144278509542346 + 0.001 * 6.85133171081543
Epoch 550, val loss: 0.8692739009857178
Epoch 560, training loss: 0.020392432808876038 = 0.01353064551949501 + 0.001 * 6.861786365509033
Epoch 560, val loss: 0.8783778548240662
Epoch 570, training loss: 0.019547633826732635 = 0.012714516371488571 + 0.001 * 6.833117485046387
Epoch 570, val loss: 0.8872613906860352
Epoch 580, training loss: 0.018799062818288803 = 0.011970308609306812 + 0.001 * 6.8287529945373535
Epoch 580, val loss: 0.8958903551101685
Epoch 590, training loss: 0.018122710287570953 = 0.011290039867162704 + 0.001 * 6.832670211791992
Epoch 590, val loss: 0.9043259024620056
Epoch 600, training loss: 0.017489612102508545 = 0.010666760616004467 + 0.001 * 6.822850704193115
Epoch 600, val loss: 0.9125462770462036
Epoch 610, training loss: 0.016937796026468277 = 0.010094416327774525 + 0.001 * 6.84337854385376
Epoch 610, val loss: 0.9205888509750366
Epoch 620, training loss: 0.01638973318040371 = 0.009567909874022007 + 0.001 * 6.821823596954346
Epoch 620, val loss: 0.9284206628799438
Epoch 630, training loss: 0.015916232019662857 = 0.00908243004232645 + 0.001 * 6.833800792694092
Epoch 630, val loss: 0.9360558986663818
Epoch 640, training loss: 0.015439197421073914 = 0.008633958175778389 + 0.001 * 6.805238246917725
Epoch 640, val loss: 0.9435072541236877
Epoch 650, training loss: 0.015018110163509846 = 0.008218910545110703 + 0.001 * 6.799199104309082
Epoch 650, val loss: 0.9507735371589661
Epoch 660, training loss: 0.014635616913437843 = 0.007834062911570072 + 0.001 * 6.801554203033447
Epoch 660, val loss: 0.9578620195388794
Epoch 670, training loss: 0.014271361753344536 = 0.007476678118109703 + 0.001 * 6.79468297958374
Epoch 670, val loss: 0.9647859930992126
Epoch 680, training loss: 0.01394064910709858 = 0.0071442159824073315 + 0.001 * 6.7964324951171875
Epoch 680, val loss: 0.9715763926506042
Epoch 690, training loss: 0.013630865141749382 = 0.006834440398961306 + 0.001 * 6.79642391204834
Epoch 690, val loss: 0.9781688451766968
Epoch 700, training loss: 0.013341939076781273 = 0.006545416545122862 + 0.001 * 6.796522617340088
Epoch 700, val loss: 0.9846265912055969
Epoch 710, training loss: 0.013053109869360924 = 0.0062753260135650635 + 0.001 * 6.777782917022705
Epoch 710, val loss: 0.9909312129020691
Epoch 720, training loss: 0.01281658373773098 = 0.006022527813911438 + 0.001 * 6.794054985046387
Epoch 720, val loss: 0.9970873594284058
Epoch 730, training loss: 0.012575609609484673 = 0.005785687360912561 + 0.001 * 6.78992223739624
Epoch 730, val loss: 1.0031169652938843
Epoch 740, training loss: 0.012341294437646866 = 0.005563470534980297 + 0.001 * 6.777822971343994
Epoch 740, val loss: 1.009013295173645
Epoch 750, training loss: 0.012121308594942093 = 0.00535472109913826 + 0.001 * 6.766587734222412
Epoch 750, val loss: 1.0147876739501953
Epoch 760, training loss: 0.011945165693759918 = 0.005158357787877321 + 0.001 * 6.786808013916016
Epoch 760, val loss: 1.0204098224639893
Epoch 770, training loss: 0.011740271002054214 = 0.0049734353087842464 + 0.001 * 6.7668352127075195
Epoch 770, val loss: 1.0259373188018799
Epoch 780, training loss: 0.011585718020796776 = 0.004799098242074251 + 0.001 * 6.786619186401367
Epoch 780, val loss: 1.031337022781372
Epoch 790, training loss: 0.011408892460167408 = 0.004634612239897251 + 0.001 * 6.774280071258545
Epoch 790, val loss: 1.0366209745407104
Epoch 800, training loss: 0.011237196624279022 = 0.004479175433516502 + 0.001 * 6.758020401000977
Epoch 800, val loss: 1.0417866706848145
Epoch 810, training loss: 0.011092855595052242 = 0.004332187585532665 + 0.001 * 6.76066780090332
Epoch 810, val loss: 1.0468584299087524
Epoch 820, training loss: 0.010948421433568 = 0.004193072207272053 + 0.001 * 6.755349159240723
Epoch 820, val loss: 1.0518027544021606
Epoch 830, training loss: 0.010806092992424965 = 0.0040612551383674145 + 0.001 * 6.744837760925293
Epoch 830, val loss: 1.0566710233688354
Epoch 840, training loss: 0.010701147839426994 = 0.003936242312192917 + 0.001 * 6.7649054527282715
Epoch 840, val loss: 1.061435341835022
Epoch 850, training loss: 0.010572562925517559 = 0.0038175706285983324 + 0.001 * 6.7549920082092285
Epoch 850, val loss: 1.0661052465438843
Epoch 860, training loss: 0.010456539690494537 = 0.003704816335812211 + 0.001 * 6.751722812652588
Epoch 860, val loss: 1.070671558380127
Epoch 870, training loss: 0.01035389956086874 = 0.0035976043436676264 + 0.001 * 6.756295204162598
Epoch 870, val loss: 1.0751643180847168
Epoch 880, training loss: 0.010253824293613434 = 0.0034955793526023626 + 0.001 * 6.75824499130249
Epoch 880, val loss: 1.0795360803604126
Epoch 890, training loss: 0.010125184431672096 = 0.003398408181965351 + 0.001 * 6.726776599884033
Epoch 890, val loss: 1.0838558673858643
Epoch 900, training loss: 0.010043029673397541 = 0.0033057734835892916 + 0.001 * 6.737255573272705
Epoch 900, val loss: 1.0880876779556274
Epoch 910, training loss: 0.009953384287655354 = 0.003217420307919383 + 0.001 * 6.735963344573975
Epoch 910, val loss: 1.092246651649475
Epoch 920, training loss: 0.009852606803178787 = 0.0031330217607319355 + 0.001 * 6.719585418701172
Epoch 920, val loss: 1.0963231325149536
Epoch 930, training loss: 0.009788930416107178 = 0.0030522961169481277 + 0.001 * 6.73663330078125
Epoch 930, val loss: 1.1003351211547852
Epoch 940, training loss: 0.009728685021400452 = 0.002974902046844363 + 0.001 * 6.753782272338867
Epoch 940, val loss: 1.1042990684509277
Epoch 950, training loss: 0.00962336827069521 = 0.002900581108406186 + 0.001 * 6.722786903381348
Epoch 950, val loss: 1.1082121133804321
Epoch 960, training loss: 0.009553119540214539 = 0.002829009899869561 + 0.001 * 6.724109649658203
Epoch 960, val loss: 1.1120824813842773
Epoch 970, training loss: 0.009490590542554855 = 0.0027598526794463396 + 0.001 * 6.730737686157227
Epoch 970, val loss: 1.115923523902893
Epoch 980, training loss: 0.009413341991603374 = 0.0026928922161459923 + 0.001 * 6.720449447631836
Epoch 980, val loss: 1.119722604751587
Epoch 990, training loss: 0.009351159445941448 = 0.002627862151712179 + 0.001 * 6.723297119140625
Epoch 990, val loss: 1.1235471963882446
Epoch 1000, training loss: 0.00927114300429821 = 0.002564634894952178 + 0.001 * 6.706508159637451
Epoch 1000, val loss: 1.1273475885391235
Epoch 1010, training loss: 0.009219521656632423 = 0.0025031655095517635 + 0.001 * 6.71635627746582
Epoch 1010, val loss: 1.1311509609222412
Epoch 1020, training loss: 0.009164254181087017 = 0.0024434009101241827 + 0.001 * 6.720853328704834
Epoch 1020, val loss: 1.1349127292633057
Epoch 1030, training loss: 0.009086930193006992 = 0.0023853210732340813 + 0.001 * 6.701608657836914
Epoch 1030, val loss: 1.1386951208114624
Epoch 1040, training loss: 0.009036917239427567 = 0.0023289998061954975 + 0.001 * 6.7079176902771
Epoch 1040, val loss: 1.1424229145050049
Epoch 1050, training loss: 0.008995462208986282 = 0.002274476457387209 + 0.001 * 6.720985412597656
Epoch 1050, val loss: 1.1461594104766846
Epoch 1060, training loss: 0.008919952437281609 = 0.002221728442236781 + 0.001 * 6.698224067687988
Epoch 1060, val loss: 1.1498379707336426
Epoch 1070, training loss: 0.008874243125319481 = 0.0021707320120185614 + 0.001 * 6.7035112380981445
Epoch 1070, val loss: 1.153466820716858
Epoch 1080, training loss: 0.008817153051495552 = 0.0021214508451521397 + 0.001 * 6.69570255279541
Epoch 1080, val loss: 1.1570937633514404
Epoch 1090, training loss: 0.008789993822574615 = 0.002073867479339242 + 0.001 * 6.716125965118408
Epoch 1090, val loss: 1.1606439352035522
Epoch 1100, training loss: 0.008755470626056194 = 0.0020280524622648954 + 0.001 * 6.727417469024658
Epoch 1100, val loss: 1.1642009019851685
Epoch 1110, training loss: 0.008659067563712597 = 0.0019837457221001387 + 0.001 * 6.675321578979492
Epoch 1110, val loss: 1.167641043663025
Epoch 1120, training loss: 0.008649157360196114 = 0.0019410831155255437 + 0.001 * 6.708073616027832
Epoch 1120, val loss: 1.171067237854004
Epoch 1130, training loss: 0.008584446273744106 = 0.0019000207539647818 + 0.001 * 6.684425354003906
Epoch 1130, val loss: 1.1744709014892578
Epoch 1140, training loss: 0.008535435423254967 = 0.0018604225479066372 + 0.001 * 6.675013065338135
Epoch 1140, val loss: 1.177800178527832
Epoch 1150, training loss: 0.008523756638169289 = 0.0018222647486254573 + 0.001 * 6.701491355895996
Epoch 1150, val loss: 1.1810731887817383
Epoch 1160, training loss: 0.008439858444035053 = 0.0017854843754321337 + 0.001 * 6.654374122619629
Epoch 1160, val loss: 1.1843334436416626
Epoch 1170, training loss: 0.008443094789981842 = 0.0017500254325568676 + 0.001 * 6.693068981170654
Epoch 1170, val loss: 1.1875427961349487
Epoch 1180, training loss: 0.008400838822126389 = 0.0017158766277134418 + 0.001 * 6.684961795806885
Epoch 1180, val loss: 1.190657138824463
Epoch 1190, training loss: 0.008360592648386955 = 0.0016829356318339705 + 0.001 * 6.677657127380371
Epoch 1190, val loss: 1.1937823295593262
Epoch 1200, training loss: 0.008324651047587395 = 0.0016511938301846385 + 0.001 * 6.67345666885376
Epoch 1200, val loss: 1.1967887878417969
Epoch 1210, training loss: 0.0082927281036973 = 0.0016206845175474882 + 0.001 * 6.672043323516846
Epoch 1210, val loss: 1.1998629570007324
Epoch 1220, training loss: 0.008239678107202053 = 0.0015913816168904305 + 0.001 * 6.648296356201172
Epoch 1220, val loss: 1.2027270793914795
Epoch 1230, training loss: 0.008231673389673233 = 0.001563137979246676 + 0.001 * 6.668534755706787
Epoch 1230, val loss: 1.2056159973144531
Epoch 1240, training loss: 0.008188496343791485 = 0.0015358607051894069 + 0.001 * 6.652635097503662
Epoch 1240, val loss: 1.2084805965423584
Epoch 1250, training loss: 0.008174310438334942 = 0.0015095866983756423 + 0.001 * 6.6647233963012695
Epoch 1250, val loss: 1.2112187147140503
Epoch 1260, training loss: 0.008130474016070366 = 0.0014842901146039367 + 0.001 * 6.646183967590332
Epoch 1260, val loss: 1.2139885425567627
Epoch 1270, training loss: 0.008108716458082199 = 0.0014598789857700467 + 0.001 * 6.648836612701416
Epoch 1270, val loss: 1.2167003154754639
Epoch 1280, training loss: 0.008081554435193539 = 0.0014363046502694488 + 0.001 * 6.645249366760254
Epoch 1280, val loss: 1.21932852268219
Epoch 1290, training loss: 0.008068726398050785 = 0.0014135356759652495 + 0.001 * 6.655190467834473
Epoch 1290, val loss: 1.2219302654266357
Epoch 1300, training loss: 0.008044831454753876 = 0.0013915044255554676 + 0.001 * 6.653326988220215
Epoch 1300, val loss: 1.2244842052459717
Epoch 1310, training loss: 0.008002657443284988 = 0.001370202749967575 + 0.001 * 6.632453918457031
Epoch 1310, val loss: 1.2270251512527466
Epoch 1320, training loss: 0.008010548539459705 = 0.0013496152823790908 + 0.001 * 6.660933017730713
Epoch 1320, val loss: 1.2295197248458862
Epoch 1330, training loss: 0.007957352325320244 = 0.0013297080295160413 + 0.001 * 6.627644062042236
Epoch 1330, val loss: 1.2319146394729614
Epoch 1340, training loss: 0.007954999804496765 = 0.001310449093580246 + 0.001 * 6.644550323486328
Epoch 1340, val loss: 1.2343003749847412
Epoch 1350, training loss: 0.0079452870413661 = 0.0012918219435960054 + 0.001 * 6.653464317321777
Epoch 1350, val loss: 1.236677885055542
Epoch 1360, training loss: 0.007894708774983883 = 0.001273786649107933 + 0.001 * 6.620921611785889
Epoch 1360, val loss: 1.2389994859695435
Epoch 1370, training loss: 0.007899250835180283 = 0.0012563337804749608 + 0.001 * 6.642916679382324
Epoch 1370, val loss: 1.2412878274917603
Epoch 1380, training loss: 0.007896657101809978 = 0.0012394931400194764 + 0.001 * 6.657163143157959
Epoch 1380, val loss: 1.2434883117675781
Epoch 1390, training loss: 0.007830249145627022 = 0.0012231377186253667 + 0.001 * 6.60711145401001
Epoch 1390, val loss: 1.2456943988800049
Epoch 1400, training loss: 0.007810886483639479 = 0.0012072882382199168 + 0.001 * 6.603597640991211
Epoch 1400, val loss: 1.2478914260864258
Epoch 1410, training loss: 0.007814157754182816 = 0.0011919330572709441 + 0.001 * 6.622224807739258
Epoch 1410, val loss: 1.2499735355377197
Epoch 1420, training loss: 0.007802632171660662 = 0.001177052385173738 + 0.001 * 6.625579357147217
Epoch 1420, val loss: 1.2520874738693237
Epoch 1430, training loss: 0.007780447602272034 = 0.0011626125779002905 + 0.001 * 6.61783504486084
Epoch 1430, val loss: 1.2541100978851318
Epoch 1440, training loss: 0.007761023938655853 = 0.0011486278381198645 + 0.001 * 6.612395763397217
Epoch 1440, val loss: 1.2562034130096436
Epoch 1450, training loss: 0.0077576711773872375 = 0.0011351481080055237 + 0.001 * 6.622522830963135
Epoch 1450, val loss: 1.2580469846725464
Epoch 1460, training loss: 0.0077231633476912975 = 0.0011220303131267428 + 0.001 * 6.601132392883301
Epoch 1460, val loss: 1.2600018978118896
Epoch 1470, training loss: 0.007767984177917242 = 0.001109293312765658 + 0.001 * 6.658690452575684
Epoch 1470, val loss: 1.2619379758834839
Epoch 1480, training loss: 0.007707217242568731 = 0.0010969002032652497 + 0.001 * 6.610316753387451
Epoch 1480, val loss: 1.2638202905654907
Epoch 1490, training loss: 0.007720509078353643 = 0.0010848506353795528 + 0.001 * 6.635658264160156
Epoch 1490, val loss: 1.2656582593917847
Epoch 1500, training loss: 0.007691490463912487 = 0.0010731492657214403 + 0.001 * 6.618340969085693
Epoch 1500, val loss: 1.2675272226333618
Epoch 1510, training loss: 0.00765433581545949 = 0.0010617427760735154 + 0.001 * 6.592592716217041
Epoch 1510, val loss: 1.2693238258361816
Epoch 1520, training loss: 0.0076671927236020565 = 0.0010506625985726714 + 0.001 * 6.61652946472168
Epoch 1520, val loss: 1.2711153030395508
Epoch 1530, training loss: 0.007640644907951355 = 0.0010399187449365854 + 0.001 * 6.6007256507873535
Epoch 1530, val loss: 1.2728359699249268
Epoch 1540, training loss: 0.007622519973665476 = 0.001029490609653294 + 0.001 * 6.593029022216797
Epoch 1540, val loss: 1.2745962142944336
Epoch 1550, training loss: 0.007681424729526043 = 0.0010193499037995934 + 0.001 * 6.662074565887451
Epoch 1550, val loss: 1.2762092351913452
Epoch 1560, training loss: 0.007591331843286753 = 0.0010094419121742249 + 0.001 * 6.581889629364014
Epoch 1560, val loss: 1.2779183387756348
Epoch 1570, training loss: 0.007640372030436993 = 0.000999815878458321 + 0.001 * 6.6405558586120605
Epoch 1570, val loss: 1.279529333114624
Epoch 1580, training loss: 0.007584266364574432 = 0.0009904566686600447 + 0.001 * 6.593809127807617
Epoch 1580, val loss: 1.2811448574066162
Epoch 1590, training loss: 0.0075986022129654884 = 0.0009813562501221895 + 0.001 * 6.617245674133301
Epoch 1590, val loss: 1.2826800346374512
Epoch 1600, training loss: 0.0075844223611056805 = 0.000972515728790313 + 0.001 * 6.611906051635742
Epoch 1600, val loss: 1.2842737436294556
Epoch 1610, training loss: 0.007539278827607632 = 0.0009639038471505046 + 0.001 * 6.575374603271484
Epoch 1610, val loss: 1.2857824563980103
Epoch 1620, training loss: 0.0075494833290576935 = 0.0009555199649184942 + 0.001 * 6.593963146209717
Epoch 1620, val loss: 1.2872827053070068
Epoch 1630, training loss: 0.007545344065874815 = 0.0009473588434047997 + 0.001 * 6.597984790802002
Epoch 1630, val loss: 1.2887359857559204
Epoch 1640, training loss: 0.00751918600872159 = 0.0009393889340572059 + 0.001 * 6.57979679107666
Epoch 1640, val loss: 1.2902119159698486
Epoch 1650, training loss: 0.007504167966544628 = 0.0009316109935753047 + 0.001 * 6.572556972503662
Epoch 1650, val loss: 1.2916339635849
Epoch 1660, training loss: 0.0074979825876653194 = 0.0009240341023541987 + 0.001 * 6.573948383331299
Epoch 1660, val loss: 1.2930041551589966
Epoch 1670, training loss: 0.007492437493056059 = 0.0009166469681076705 + 0.001 * 6.5757904052734375
Epoch 1670, val loss: 1.2944151163101196
Epoch 1680, training loss: 0.007478840183466673 = 0.0009094371343962848 + 0.001 * 6.569402694702148
Epoch 1680, val loss: 1.2958083152770996
Epoch 1690, training loss: 0.0074686710722744465 = 0.0009023853344842792 + 0.001 * 6.566285133361816
Epoch 1690, val loss: 1.2971208095550537
Epoch 1700, training loss: 0.007458081003278494 = 0.0008954916847869754 + 0.001 * 6.562588691711426
Epoch 1700, val loss: 1.2984832525253296
Epoch 1710, training loss: 0.007456236518919468 = 0.0008887665462680161 + 0.001 * 6.567469596862793
Epoch 1710, val loss: 1.2998236417770386
Epoch 1720, training loss: 0.007449347525835037 = 0.0008822118397802114 + 0.001 * 6.567135334014893
Epoch 1720, val loss: 1.3010690212249756
Epoch 1730, training loss: 0.007458467967808247 = 0.0008758236654102802 + 0.001 * 6.582643985748291
Epoch 1730, val loss: 1.3023755550384521
Epoch 1740, training loss: 0.0074593909084796906 = 0.0008695980068296194 + 0.001 * 6.589792251586914
Epoch 1740, val loss: 1.3035980463027954
Epoch 1750, training loss: 0.007442866452038288 = 0.0008634930709376931 + 0.001 * 6.579373359680176
Epoch 1750, val loss: 1.3048865795135498
Epoch 1760, training loss: 0.0074382140301167965 = 0.000857520557474345 + 0.001 * 6.580693244934082
Epoch 1760, val loss: 1.3060601949691772
Epoch 1770, training loss: 0.007459317799657583 = 0.0008516758680343628 + 0.001 * 6.607641696929932
Epoch 1770, val loss: 1.3072890043258667
Epoch 1780, training loss: 0.007391324732452631 = 0.0008459583041258156 + 0.001 * 6.545365810394287
Epoch 1780, val loss: 1.3084805011749268
Epoch 1790, training loss: 0.00742726493626833 = 0.0008403612882830203 + 0.001 * 6.5869035720825195
Epoch 1790, val loss: 1.3096920251846313
Epoch 1800, training loss: 0.00740011828020215 = 0.0008348975097760558 + 0.001 * 6.565220355987549
Epoch 1800, val loss: 1.3108288049697876
Epoch 1810, training loss: 0.007399481721222401 = 0.0008295458974316716 + 0.001 * 6.5699357986450195
Epoch 1810, val loss: 1.3119813203811646
Epoch 1820, training loss: 0.007367817685008049 = 0.0008243040647357702 + 0.001 * 6.543513774871826
Epoch 1820, val loss: 1.3131282329559326
Epoch 1830, training loss: 0.007362260948866606 = 0.0008191752131097019 + 0.001 * 6.54308557510376
Epoch 1830, val loss: 1.3142223358154297
Epoch 1840, training loss: 0.007348465733230114 = 0.0008141585276462138 + 0.001 * 6.53430700302124
Epoch 1840, val loss: 1.315381407737732
Epoch 1850, training loss: 0.007379717193543911 = 0.0008092557545751333 + 0.001 * 6.570461273193359
Epoch 1850, val loss: 1.3164441585540771
Epoch 1860, training loss: 0.0073380665853619576 = 0.000804463925305754 + 0.001 * 6.533602237701416
Epoch 1860, val loss: 1.3175241947174072
Epoch 1870, training loss: 0.007336197420954704 = 0.0007997490465641022 + 0.001 * 6.536448001861572
Epoch 1870, val loss: 1.3185786008834839
Epoch 1880, training loss: 0.0073614236898720264 = 0.0007951715961098671 + 0.001 * 6.566251754760742
Epoch 1880, val loss: 1.319631814956665
Epoch 1890, training loss: 0.007329017389565706 = 0.000790658756159246 + 0.001 * 6.538358211517334
Epoch 1890, val loss: 1.3206398487091064
Epoch 1900, training loss: 0.007363605313003063 = 0.0007862505153752863 + 0.001 * 6.577354431152344
Epoch 1900, val loss: 1.3216743469238281
Epoch 1910, training loss: 0.007311392575502396 = 0.0007819082238711417 + 0.001 * 6.529484272003174
Epoch 1910, val loss: 1.3226569890975952
Epoch 1920, training loss: 0.007303874474018812 = 0.0007776481215842068 + 0.001 * 6.526226043701172
Epoch 1920, val loss: 1.323705792427063
Epoch 1930, training loss: 0.007304890546947718 = 0.0007734743994660676 + 0.001 * 6.5314154624938965
Epoch 1930, val loss: 1.3247064352035522
Epoch 1940, training loss: 0.007296815048903227 = 0.0007693865918554366 + 0.001 * 6.527428150177002
Epoch 1940, val loss: 1.3256585597991943
Epoch 1950, training loss: 0.007307617925107479 = 0.0007653726497665048 + 0.001 * 6.542244911193848
Epoch 1950, val loss: 1.3266624212265015
Epoch 1960, training loss: 0.007283689919859171 = 0.0007614207570441067 + 0.001 * 6.522268772125244
Epoch 1960, val loss: 1.3275905847549438
Epoch 1970, training loss: 0.007294160779565573 = 0.0007575663039460778 + 0.001 * 6.536594390869141
Epoch 1970, val loss: 1.3285187482833862
Epoch 1980, training loss: 0.007297397591173649 = 0.0007537897909060121 + 0.001 * 6.543607711791992
Epoch 1980, val loss: 1.3295115232467651
Epoch 1990, training loss: 0.007282786536961794 = 0.0007500486099161208 + 0.001 * 6.532737731933594
Epoch 1990, val loss: 1.3303714990615845
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.7675
Flip ASR: 0.7200/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9616347551345825 = 1.953260898590088 + 0.001 * 8.37390422821045
Epoch 0, val loss: 1.9495826959609985
Epoch 10, training loss: 1.950554609298706 = 1.9421807527542114 + 0.001 * 8.373834609985352
Epoch 10, val loss: 1.9381166696548462
Epoch 20, training loss: 1.9367400407791138 = 1.9283664226531982 + 0.001 * 8.37360668182373
Epoch 20, val loss: 1.923735499382019
Epoch 30, training loss: 1.9173598289489746 = 1.9089866876602173 + 0.001 * 8.37308406829834
Epoch 30, val loss: 1.9032753705978394
Epoch 40, training loss: 1.888980746269226 = 1.8806089162826538 + 0.001 * 8.371830940246582
Epoch 40, val loss: 1.873322606086731
Epoch 50, training loss: 1.850378394126892 = 1.8420101404190063 + 0.001 * 8.36827278137207
Epoch 50, val loss: 1.8340107202529907
Epoch 60, training loss: 1.8082842826843262 = 1.7999293804168701 + 0.001 * 8.35489559173584
Epoch 60, val loss: 1.7958600521087646
Epoch 70, training loss: 1.770099401473999 = 1.7618118524551392 + 0.001 * 8.287544250488281
Epoch 70, val loss: 1.7664873600006104
Epoch 80, training loss: 1.7217072248458862 = 1.7138243913650513 + 0.001 * 7.88287353515625
Epoch 80, val loss: 1.7280746698379517
Epoch 90, training loss: 1.6556569337844849 = 1.6478934288024902 + 0.001 * 7.76353120803833
Epoch 90, val loss: 1.6717791557312012
Epoch 100, training loss: 1.569421648979187 = 1.5617234706878662 + 0.001 * 7.698234558105469
Epoch 100, val loss: 1.5986552238464355
Epoch 110, training loss: 1.4715940952301025 = 1.4640213251113892 + 0.001 * 7.572784423828125
Epoch 110, val loss: 1.519697666168213
Epoch 120, training loss: 1.3725919723510742 = 1.3652714490890503 + 0.001 * 7.32047700881958
Epoch 120, val loss: 1.4422121047973633
Epoch 130, training loss: 1.2753947973251343 = 1.2681736946105957 + 0.001 * 7.221092700958252
Epoch 130, val loss: 1.369164228439331
Epoch 140, training loss: 1.1793814897537231 = 1.172247052192688 + 0.001 * 7.134411334991455
Epoch 140, val loss: 1.2984849214553833
Epoch 150, training loss: 1.0858551263809204 = 1.0787708759307861 + 0.001 * 7.084226131439209
Epoch 150, val loss: 1.2294052839279175
Epoch 160, training loss: 0.9974830746650696 = 0.9904197454452515 + 0.001 * 7.063316345214844
Epoch 160, val loss: 1.1645581722259521
Epoch 170, training loss: 0.9155485033988953 = 0.9085015654563904 + 0.001 * 7.046910285949707
Epoch 170, val loss: 1.1046416759490967
Epoch 180, training loss: 0.8389038443565369 = 0.8318732380867004 + 0.001 * 7.030600547790527
Epoch 180, val loss: 1.0492709875106812
Epoch 190, training loss: 0.7661203742027283 = 0.759107232093811 + 0.001 * 7.01312255859375
Epoch 190, val loss: 0.9973034858703613
Epoch 200, training loss: 0.6968779563903809 = 0.6898838877677917 + 0.001 * 6.994071960449219
Epoch 200, val loss: 0.9487881064414978
Epoch 210, training loss: 0.6320950388908386 = 0.6251187324523926 + 0.001 * 6.976314544677734
Epoch 210, val loss: 0.9046854376792908
Epoch 220, training loss: 0.5729672312736511 = 0.5660013556480408 + 0.001 * 6.965860843658447
Epoch 220, val loss: 0.866516649723053
Epoch 230, training loss: 0.520034670829773 = 0.5130800604820251 + 0.001 * 6.954623699188232
Epoch 230, val loss: 0.8348970413208008
Epoch 240, training loss: 0.4727897644042969 = 0.46584364771842957 + 0.001 * 6.946118354797363
Epoch 240, val loss: 0.8094759583473206
Epoch 250, training loss: 0.42988455295562744 = 0.4229469895362854 + 0.001 * 6.937572479248047
Epoch 250, val loss: 0.7894619703292847
Epoch 260, training loss: 0.3899683952331543 = 0.38303959369659424 + 0.001 * 6.928791522979736
Epoch 260, val loss: 0.773737907409668
Epoch 270, training loss: 0.35228514671325684 = 0.34536460041999817 + 0.001 * 6.920560359954834
Epoch 270, val loss: 0.7616562247276306
Epoch 280, training loss: 0.3167506158351898 = 0.30984053015708923 + 0.001 * 6.910091876983643
Epoch 280, val loss: 0.7525303363800049
Epoch 290, training loss: 0.28359800577163696 = 0.27669742703437805 + 0.001 * 6.900563716888428
Epoch 290, val loss: 0.7461990714073181
Epoch 300, training loss: 0.2530022859573364 = 0.24610668420791626 + 0.001 * 6.895603656768799
Epoch 300, val loss: 0.7425364255905151
Epoch 310, training loss: 0.22499990463256836 = 0.21811366081237793 + 0.001 * 6.886242866516113
Epoch 310, val loss: 0.7412300109863281
Epoch 320, training loss: 0.19957947731018066 = 0.1927005797624588 + 0.001 * 6.878897666931152
Epoch 320, val loss: 0.7420310974121094
Epoch 330, training loss: 0.1767388880252838 = 0.16986346244812012 + 0.001 * 6.875425338745117
Epoch 330, val loss: 0.7448515892028809
Epoch 340, training loss: 0.15644074976444244 = 0.1495673954486847 + 0.001 * 6.873353004455566
Epoch 340, val loss: 0.7495330572128296
Epoch 350, training loss: 0.13857904076576233 = 0.13170456886291504 + 0.001 * 6.874471664428711
Epoch 350, val loss: 0.7559343576431274
Epoch 360, training loss: 0.12297476828098297 = 0.11610552668571472 + 0.001 * 6.8692426681518555
Epoch 360, val loss: 0.7636491060256958
Epoch 370, training loss: 0.10941992700099945 = 0.10255101323127747 + 0.001 * 6.868913173675537
Epoch 370, val loss: 0.7726028561592102
Epoch 380, training loss: 0.09765376150608063 = 0.09078680723905563 + 0.001 * 6.866955757141113
Epoch 380, val loss: 0.782364547252655
Epoch 390, training loss: 0.08744146674871445 = 0.08057825267314911 + 0.001 * 6.863210678100586
Epoch 390, val loss: 0.7926568984985352
Epoch 400, training loss: 0.07857093960046768 = 0.07170571386814117 + 0.001 * 6.865227699279785
Epoch 400, val loss: 0.8033414483070374
Epoch 410, training loss: 0.07083521038293839 = 0.06397601217031479 + 0.001 * 6.859196186065674
Epoch 410, val loss: 0.8141804933547974
Epoch 420, training loss: 0.06408730149269104 = 0.05722974240779877 + 0.001 * 6.857562065124512
Epoch 420, val loss: 0.8250051140785217
Epoch 430, training loss: 0.05818802863359451 = 0.05133099853992462 + 0.001 * 6.857031345367432
Epoch 430, val loss: 0.8357363343238831
Epoch 440, training loss: 0.053028617054224014 = 0.046166885644197464 + 0.001 * 6.861732482910156
Epoch 440, val loss: 0.8462976813316345
Epoch 450, training loss: 0.04849632829427719 = 0.04163949564099312 + 0.001 * 6.856833457946777
Epoch 450, val loss: 0.8566937446594238
Epoch 460, training loss: 0.044519007205963135 = 0.037666063755750656 + 0.001 * 6.852941036224365
Epoch 460, val loss: 0.8669182658195496
Epoch 470, training loss: 0.04102407395839691 = 0.03417420759797096 + 0.001 * 6.849867820739746
Epoch 470, val loss: 0.8769445419311523
Epoch 480, training loss: 0.03794843330979347 = 0.031099746003746986 + 0.001 * 6.848687171936035
Epoch 480, val loss: 0.8867974281311035
Epoch 490, training loss: 0.03523479402065277 = 0.028387555852532387 + 0.001 * 6.847238540649414
Epoch 490, val loss: 0.8964802622795105
Epoch 500, training loss: 0.032834410667419434 = 0.02598908729851246 + 0.001 * 6.845322608947754
Epoch 500, val loss: 0.9059368968009949
Epoch 510, training loss: 0.030705276876688004 = 0.02386147528886795 + 0.001 * 6.843801975250244
Epoch 510, val loss: 0.9152687191963196
Epoch 520, training loss: 0.02881026640534401 = 0.02196735516190529 + 0.001 * 6.842911720275879
Epoch 520, val loss: 0.9244591593742371
Epoch 530, training loss: 0.0271160788834095 = 0.02027517557144165 + 0.001 * 6.8409037590026855
Epoch 530, val loss: 0.9334413409233093
Epoch 540, training loss: 0.02561178430914879 = 0.018759580329060555 + 0.001 * 6.8522047996521
Epoch 540, val loss: 0.9422423243522644
Epoch 550, training loss: 0.024236157536506653 = 0.01739904284477234 + 0.001 * 6.837115287780762
Epoch 550, val loss: 0.9508617520332336
Epoch 560, training loss: 0.02301061525940895 = 0.01617521047592163 + 0.001 * 6.835404396057129
Epoch 560, val loss: 0.9593136310577393
Epoch 570, training loss: 0.021924365311861038 = 0.01507213618606329 + 0.001 * 6.852227687835693
Epoch 570, val loss: 0.9675778746604919
Epoch 580, training loss: 0.020909197628498077 = 0.01407611183822155 + 0.001 * 6.833084583282471
Epoch 580, val loss: 0.9756681323051453
Epoch 590, training loss: 0.020008763298392296 = 0.013174907304346561 + 0.001 * 6.833855628967285
Epoch 590, val loss: 0.9835382699966431
Epoch 600, training loss: 0.019185861572623253 = 0.012357804924249649 + 0.001 * 6.828056812286377
Epoch 600, val loss: 0.9912508130073547
Epoch 610, training loss: 0.018464557826519012 = 0.011615302413702011 + 0.001 * 6.849255561828613
Epoch 610, val loss: 0.9987746477127075
Epoch 620, training loss: 0.017771761864423752 = 0.010939279571175575 + 0.001 * 6.832481861114502
Epoch 620, val loss: 1.006117343902588
Epoch 630, training loss: 0.017147332429885864 = 0.010322426445782185 + 0.001 * 6.824906349182129
Epoch 630, val loss: 1.0132793188095093
Epoch 640, training loss: 0.016587579622864723 = 0.009758180007338524 + 0.001 * 6.829399108886719
Epoch 640, val loss: 1.0202689170837402
Epoch 650, training loss: 0.016062291339039803 = 0.009240906685590744 + 0.001 * 6.821383953094482
Epoch 650, val loss: 1.0271050930023193
Epoch 660, training loss: 0.015583764761686325 = 0.00876567605882883 + 0.001 * 6.818088054656982
Epoch 660, val loss: 1.0337722301483154
Epoch 670, training loss: 0.015165640041232109 = 0.008328103460371494 + 0.001 * 6.837536334991455
Epoch 670, val loss: 1.0402987003326416
Epoch 680, training loss: 0.014744037762284279 = 0.007924389094114304 + 0.001 * 6.819648742675781
Epoch 680, val loss: 1.0466681718826294
Epoch 690, training loss: 0.014370981603860855 = 0.00755116855725646 + 0.001 * 6.819812774658203
Epoch 690, val loss: 1.0528877973556519
Epoch 700, training loss: 0.014035064727067947 = 0.007205505855381489 + 0.001 * 6.829558372497559
Epoch 700, val loss: 1.0589665174484253
Epoch 710, training loss: 0.013703631237149239 = 0.0068848151713609695 + 0.001 * 6.8188157081604
Epoch 710, val loss: 1.0648959875106812
Epoch 720, training loss: 0.013402104377746582 = 0.006586679723113775 + 0.001 * 6.81542444229126
Epoch 720, val loss: 1.0707045793533325
Epoch 730, training loss: 0.013125108554959297 = 0.006309067830443382 + 0.001 * 6.816040515899658
Epoch 730, val loss: 1.0763752460479736
Epoch 740, training loss: 0.012865359894931316 = 0.006050114519894123 + 0.001 * 6.815245151519775
Epoch 740, val loss: 1.0819181203842163
Epoch 750, training loss: 0.012619596906006336 = 0.0058081564493477345 + 0.001 * 6.8114399909973145
Epoch 750, val loss: 1.0873481035232544
Epoch 760, training loss: 0.01239006593823433 = 0.005581730045378208 + 0.001 * 6.808335304260254
Epoch 760, val loss: 1.092646837234497
Epoch 770, training loss: 0.012171315029263496 = 0.005369550548493862 + 0.001 * 6.801763534545898
Epoch 770, val loss: 1.0978357791900635
Epoch 780, training loss: 0.011976595036685467 = 0.005170431453734636 + 0.001 * 6.806163311004639
Epoch 780, val loss: 1.1029046773910522
Epoch 790, training loss: 0.01178489625453949 = 0.004983330611139536 + 0.001 * 6.801565170288086
Epoch 790, val loss: 1.1078652143478394
Epoch 800, training loss: 0.011622732505202293 = 0.004807299003005028 + 0.001 * 6.815432548522949
Epoch 800, val loss: 1.1127170324325562
Epoch 810, training loss: 0.01144501194357872 = 0.004641478415578604 + 0.001 * 6.803532600402832
Epoch 810, val loss: 1.1174707412719727
Epoch 820, training loss: 0.011301599442958832 = 0.004485063720494509 + 0.001 * 6.816535949707031
Epoch 820, val loss: 1.1221177577972412
Epoch 830, training loss: 0.011124201118946075 = 0.00433734618127346 + 0.001 * 6.786855220794678
Epoch 830, val loss: 1.1266701221466064
Epoch 840, training loss: 0.011012621223926544 = 0.004197703208774328 + 0.001 * 6.814917087554932
Epoch 840, val loss: 1.131138801574707
Epoch 850, training loss: 0.010857046581804752 = 0.004065545741468668 + 0.001 * 6.791500568389893
Epoch 850, val loss: 1.135501503944397
Epoch 860, training loss: 0.010736407712101936 = 0.003940322436392307 + 0.001 * 6.796084403991699
Epoch 860, val loss: 1.139783501625061
Epoch 870, training loss: 0.010606733150780201 = 0.003821586025878787 + 0.001 * 6.785147190093994
Epoch 870, val loss: 1.143979787826538
Epoch 880, training loss: 0.010490159504115582 = 0.0037088484968990088 + 0.001 * 6.78131103515625
Epoch 880, val loss: 1.1480880975723267
Epoch 890, training loss: 0.010387944057583809 = 0.003601767122745514 + 0.001 * 6.786177158355713
Epoch 890, val loss: 1.1521215438842773
Epoch 900, training loss: 0.010276444256305695 = 0.0034999395720660686 + 0.001 * 6.776504039764404
Epoch 900, val loss: 1.1560486555099487
Epoch 910, training loss: 0.010176410898566246 = 0.0034030205570161343 + 0.001 * 6.7733893394470215
Epoch 910, val loss: 1.1599204540252686
Epoch 920, training loss: 0.010084960609674454 = 0.0033106834162026644 + 0.001 * 6.774277210235596
Epoch 920, val loss: 1.1637167930603027
Epoch 930, training loss: 0.010018346831202507 = 0.0032226426992565393 + 0.001 * 6.795703887939453
Epoch 930, val loss: 1.1674383878707886
Epoch 940, training loss: 0.009908934123814106 = 0.003138652304187417 + 0.001 * 6.770281791687012
Epoch 940, val loss: 1.1711026430130005
Epoch 950, training loss: 0.00982678309082985 = 0.0030584775377064943 + 0.001 * 6.768304824829102
Epoch 950, val loss: 1.1746915578842163
Epoch 960, training loss: 0.009741367772221565 = 0.0029818578623235226 + 0.001 * 6.759509563446045
Epoch 960, val loss: 1.1782152652740479
Epoch 970, training loss: 0.00967502873390913 = 0.0029086058493703604 + 0.001 * 6.766422748565674
Epoch 970, val loss: 1.1816760301589966
Epoch 980, training loss: 0.009623338468372822 = 0.0028385373298078775 + 0.001 * 6.7848005294799805
Epoch 980, val loss: 1.1850661039352417
Epoch 990, training loss: 0.009538576006889343 = 0.002771432511508465 + 0.001 * 6.767143726348877
Epoch 990, val loss: 1.1884030103683472
Epoch 1000, training loss: 0.009465569630265236 = 0.0027071658987551928 + 0.001 * 6.758403778076172
Epoch 1000, val loss: 1.1916694641113281
Epoch 1010, training loss: 0.009393811225891113 = 0.0026455565821379423 + 0.001 * 6.748254299163818
Epoch 1010, val loss: 1.1948806047439575
Epoch 1020, training loss: 0.009343456476926804 = 0.0025864597409963608 + 0.001 * 6.7569966316223145
Epoch 1020, val loss: 1.1980156898498535
Epoch 1030, training loss: 0.009290415793657303 = 0.0025297491811215878 + 0.001 * 6.7606658935546875
Epoch 1030, val loss: 1.2011305093765259
Epoch 1040, training loss: 0.009220132604241371 = 0.0024752854369580746 + 0.001 * 6.744847297668457
Epoch 1040, val loss: 1.2041643857955933
Epoch 1050, training loss: 0.009182224050164223 = 0.002422978635877371 + 0.001 * 6.759244441986084
Epoch 1050, val loss: 1.207139492034912
Epoch 1060, training loss: 0.009117517620325089 = 0.0023726806975901127 + 0.001 * 6.744837284088135
Epoch 1060, val loss: 1.2100635766983032
Epoch 1070, training loss: 0.009059526957571507 = 0.0023243024479597807 + 0.001 * 6.735223770141602
Epoch 1070, val loss: 1.2129604816436768
Epoch 1080, training loss: 0.00903130043298006 = 0.0022777400445193052 + 0.001 * 6.7535600662231445
Epoch 1080, val loss: 1.2157742977142334
Epoch 1090, training loss: 0.008981676772236824 = 0.002232911065220833 + 0.001 * 6.748764991760254
Epoch 1090, val loss: 1.2185636758804321
Epoch 1100, training loss: 0.008955360390245914 = 0.002189747989177704 + 0.001 * 6.7656121253967285
Epoch 1100, val loss: 1.221312165260315
Epoch 1110, training loss: 0.008885601535439491 = 0.0021481532603502274 + 0.001 * 6.737448215484619
Epoch 1110, val loss: 1.2239848375320435
Epoch 1120, training loss: 0.008860140107572079 = 0.0021080628503113985 + 0.001 * 6.752077102661133
Epoch 1120, val loss: 1.2266076803207397
Epoch 1130, training loss: 0.008813433349132538 = 0.0020694017875939608 + 0.001 * 6.7440314292907715
Epoch 1130, val loss: 1.2291918992996216
Epoch 1140, training loss: 0.008772839792072773 = 0.0020320897456258535 + 0.001 * 6.740749835968018
Epoch 1140, val loss: 1.231752872467041
Epoch 1150, training loss: 0.008722642436623573 = 0.001996047329157591 + 0.001 * 6.726595401763916
Epoch 1150, val loss: 1.2342545986175537
Epoch 1160, training loss: 0.008686699904501438 = 0.0019612484611570835 + 0.001 * 6.7254509925842285
Epoch 1160, val loss: 1.2367429733276367
Epoch 1170, training loss: 0.00864222552627325 = 0.0019276238745078444 + 0.001 * 6.714601039886475
Epoch 1170, val loss: 1.2391709089279175
Epoch 1180, training loss: 0.008609873242676258 = 0.0018951364327222109 + 0.001 * 6.714736461639404
Epoch 1180, val loss: 1.2415542602539062
Epoch 1190, training loss: 0.00859879795461893 = 0.0018637350294739008 + 0.001 * 6.735062599182129
Epoch 1190, val loss: 1.2438832521438599
Epoch 1200, training loss: 0.008561152033507824 = 0.0018333587795495987 + 0.001 * 6.727792739868164
Epoch 1200, val loss: 1.2461907863616943
Epoch 1210, training loss: 0.008509785868227482 = 0.0018039548303931952 + 0.001 * 6.7058305740356445
Epoch 1210, val loss: 1.2484768629074097
Epoch 1220, training loss: 0.008482148870825768 = 0.001775486976839602 + 0.001 * 6.706661701202393
Epoch 1220, val loss: 1.2507176399230957
Epoch 1230, training loss: 0.008463111706078053 = 0.0017479354282841086 + 0.001 * 6.715175628662109
Epoch 1230, val loss: 1.2529025077819824
Epoch 1240, training loss: 0.008459297008812428 = 0.0017212547827512026 + 0.001 * 6.738041877746582
Epoch 1240, val loss: 1.255058765411377
Epoch 1250, training loss: 0.008413553237915039 = 0.0016953985905274749 + 0.001 * 6.7181549072265625
Epoch 1250, val loss: 1.2571817636489868
Epoch 1260, training loss: 0.008386540226638317 = 0.0016703485744073987 + 0.001 * 6.716190814971924
Epoch 1260, val loss: 1.2592872381210327
Epoch 1270, training loss: 0.00835128128528595 = 0.0016460382612422109 + 0.001 * 6.705242156982422
Epoch 1270, val loss: 1.2613210678100586
Epoch 1280, training loss: 0.008323835209012032 = 0.0016224735882133245 + 0.001 * 6.701361656188965
Epoch 1280, val loss: 1.2633721828460693
Epoch 1290, training loss: 0.008297845721244812 = 0.001599606592208147 + 0.001 * 6.698238849639893
Epoch 1290, val loss: 1.2653616666793823
Epoch 1300, training loss: 0.008268126286566257 = 0.0015774283092468977 + 0.001 * 6.69069766998291
Epoch 1300, val loss: 1.267303466796875
Epoch 1310, training loss: 0.008242459036409855 = 0.0015559092862531543 + 0.001 * 6.686549663543701
Epoch 1310, val loss: 1.2692461013793945
Epoch 1320, training loss: 0.008259500376880169 = 0.001535013085231185 + 0.001 * 6.7244873046875
Epoch 1320, val loss: 1.2711524963378906
Epoch 1330, training loss: 0.008208343759179115 = 0.0015147062949836254 + 0.001 * 6.693637371063232
Epoch 1330, val loss: 1.2730295658111572
Epoch 1340, training loss: 0.008188270963728428 = 0.001494962489232421 + 0.001 * 6.693308353424072
Epoch 1340, val loss: 1.2748603820800781
Epoch 1350, training loss: 0.008192588575184345 = 0.0014757959870621562 + 0.001 * 6.716792583465576
Epoch 1350, val loss: 1.276700735092163
Epoch 1360, training loss: 0.008145783096551895 = 0.0014571466017514467 + 0.001 * 6.688636302947998
Epoch 1360, val loss: 1.278476595878601
Epoch 1370, training loss: 0.00814855471253395 = 0.0014390150317922235 + 0.001 * 6.709539413452148
Epoch 1370, val loss: 1.2802563905715942
Epoch 1380, training loss: 0.008111711591482162 = 0.0014213842805474997 + 0.001 * 6.690326690673828
Epoch 1380, val loss: 1.2819832563400269
Epoch 1390, training loss: 0.008103989996016026 = 0.0014042597031220794 + 0.001 * 6.699730396270752
Epoch 1390, val loss: 1.2837024927139282
Epoch 1400, training loss: 0.008113855496048927 = 0.0013875729637220502 + 0.001 * 6.726282119750977
Epoch 1400, val loss: 1.2853752374649048
Epoch 1410, training loss: 0.008052755147218704 = 0.0013713511871173978 + 0.001 * 6.681403636932373
Epoch 1410, val loss: 1.2870304584503174
Epoch 1420, training loss: 0.008039293810725212 = 0.0013555451296269894 + 0.001 * 6.6837477684021
Epoch 1420, val loss: 1.28868567943573
Epoch 1430, training loss: 0.008012878708541393 = 0.0013401469914242625 + 0.001 * 6.672731399536133
Epoch 1430, val loss: 1.2902820110321045
Epoch 1440, training loss: 0.007998300716280937 = 0.0013251646887511015 + 0.001 * 6.673135280609131
Epoch 1440, val loss: 1.291892170906067
Epoch 1450, training loss: 0.007982525043189526 = 0.0013105645775794983 + 0.001 * 6.671960353851318
Epoch 1450, val loss: 1.293433666229248
Epoch 1460, training loss: 0.007970712147653103 = 0.001296340604312718 + 0.001 * 6.674370765686035
Epoch 1460, val loss: 1.2949974536895752
Epoch 1470, training loss: 0.007963319309055805 = 0.0012824671575799584 + 0.001 * 6.680851936340332
Epoch 1470, val loss: 1.296499252319336
Epoch 1480, training loss: 0.007933199405670166 = 0.00126895762514323 + 0.001 * 6.664241790771484
Epoch 1480, val loss: 1.2980095148086548
Epoch 1490, training loss: 0.007937461137771606 = 0.0012557855807244778 + 0.001 * 6.681675434112549
Epoch 1490, val loss: 1.2994701862335205
Epoch 1500, training loss: 0.007915329188108444 = 0.0012429223861545324 + 0.001 * 6.6724066734313965
Epoch 1500, val loss: 1.3009504079818726
Epoch 1510, training loss: 0.007897447794675827 = 0.0012303913244977593 + 0.001 * 6.667056083679199
Epoch 1510, val loss: 1.3024142980575562
Epoch 1520, training loss: 0.007859678938984871 = 0.001218154327943921 + 0.001 * 6.641524314880371
Epoch 1520, val loss: 1.3038132190704346
Epoch 1530, training loss: 0.0078905513510108 = 0.0012062235036864877 + 0.001 * 6.684327125549316
Epoch 1530, val loss: 1.3052386045455933
Epoch 1540, training loss: 0.007861504331231117 = 0.0011945873266085982 + 0.001 * 6.666916370391846
Epoch 1540, val loss: 1.3066319227218628
Epoch 1550, training loss: 0.00786382146179676 = 0.0011832024902105331 + 0.001 * 6.680619239807129
Epoch 1550, val loss: 1.3079657554626465
Epoch 1560, training loss: 0.007837537676095963 = 0.0011720872716978192 + 0.001 * 6.665450096130371
Epoch 1560, val loss: 1.309348225593567
Epoch 1570, training loss: 0.00784014817327261 = 0.0011612118687480688 + 0.001 * 6.678935527801514
Epoch 1570, val loss: 1.3106791973114014
Epoch 1580, training loss: 0.007797887083142996 = 0.0011505683651193976 + 0.001 * 6.647318363189697
Epoch 1580, val loss: 1.3120061159133911
Epoch 1590, training loss: 0.007805137895047665 = 0.0011401268420740962 + 0.001 * 6.665010452270508
Epoch 1590, val loss: 1.313300371170044
Epoch 1600, training loss: 0.007775746285915375 = 0.0011298961471766233 + 0.001 * 6.645849704742432
Epoch 1600, val loss: 1.3146098852157593
Epoch 1610, training loss: 0.007779138628393412 = 0.0011198952561244369 + 0.001 * 6.659243106842041
Epoch 1610, val loss: 1.3158870935440063
Epoch 1620, training loss: 0.007805731147527695 = 0.0011100531555712223 + 0.001 * 6.695677757263184
Epoch 1620, val loss: 1.3171182870864868
Epoch 1630, training loss: 0.007758191786706448 = 0.001100433524698019 + 0.001 * 6.657757759094238
Epoch 1630, val loss: 1.3183437585830688
Epoch 1640, training loss: 0.007731999736279249 = 0.0010909223929047585 + 0.001 * 6.641077041625977
Epoch 1640, val loss: 1.3196356296539307
Epoch 1650, training loss: 0.0077440133318305016 = 0.0010817513102665544 + 0.001 * 6.662261962890625
Epoch 1650, val loss: 1.3207932710647583
Epoch 1660, training loss: 0.00772076565772295 = 0.0010725948959589005 + 0.001 * 6.648170471191406
Epoch 1660, val loss: 1.322033166885376
Epoch 1670, training loss: 0.007697587832808495 = 0.0010637958766892552 + 0.001 * 6.633791923522949
Epoch 1670, val loss: 1.3231585025787354
Epoch 1680, training loss: 0.007692537270486355 = 0.0010550025617703795 + 0.001 * 6.637534141540527
Epoch 1680, val loss: 1.3244351148605347
Epoch 1690, training loss: 0.007694796193391085 = 0.0010464884107932448 + 0.001 * 6.6483073234558105
Epoch 1690, val loss: 1.3255594968795776
Epoch 1700, training loss: 0.007701822556555271 = 0.0010380599414929748 + 0.001 * 6.663762092590332
Epoch 1700, val loss: 1.3266576528549194
Epoch 1710, training loss: 0.007661948911845684 = 0.0010300043504685163 + 0.001 * 6.631944179534912
Epoch 1710, val loss: 1.3278456926345825
Epoch 1720, training loss: 0.007647453807294369 = 0.0010219047544524074 + 0.001 * 6.625548839569092
Epoch 1720, val loss: 1.3290107250213623
Epoch 1730, training loss: 0.007641954347491264 = 0.0010140773374587297 + 0.001 * 6.6278767585754395
Epoch 1730, val loss: 1.330140233039856
Epoch 1740, training loss: 0.007650114595890045 = 0.0010063498048111796 + 0.001 * 6.643764495849609
Epoch 1740, val loss: 1.3311923742294312
Epoch 1750, training loss: 0.007630778476595879 = 0.0009988662786781788 + 0.001 * 6.631911754608154
Epoch 1750, val loss: 1.3323254585266113
Epoch 1760, training loss: 0.007656352128833532 = 0.000991491018794477 + 0.001 * 6.664860725402832
Epoch 1760, val loss: 1.3333734273910522
Epoch 1770, training loss: 0.0076384455896914005 = 0.0009842892177402973 + 0.001 * 6.65415620803833
Epoch 1770, val loss: 1.3345133066177368
Epoch 1780, training loss: 0.007587055675685406 = 0.000977243180386722 + 0.001 * 6.609812259674072
Epoch 1780, val loss: 1.3355001211166382
Epoch 1790, training loss: 0.007615942042320967 = 0.0009703068644739687 + 0.001 * 6.64563512802124
Epoch 1790, val loss: 1.3366159200668335
Epoch 1800, training loss: 0.007590342313051224 = 0.0009634745656512678 + 0.001 * 6.626867294311523
Epoch 1800, val loss: 1.3376617431640625
Epoch 1810, training loss: 0.007576668169349432 = 0.0009568308596499264 + 0.001 * 6.619836807250977
Epoch 1810, val loss: 1.3386085033416748
Epoch 1820, training loss: 0.007544243708252907 = 0.0009504087502136827 + 0.001 * 6.59383487701416
Epoch 1820, val loss: 1.3396663665771484
Epoch 1830, training loss: 0.007559387944638729 = 0.0009440057910978794 + 0.001 * 6.615381717681885
Epoch 1830, val loss: 1.3407483100891113
Epoch 1840, training loss: 0.0075391014106571674 = 0.0009377487585879862 + 0.001 * 6.601352214813232
Epoch 1840, val loss: 1.341672658920288
Epoch 1850, training loss: 0.0075464025139808655 = 0.0009316272335126996 + 0.001 * 6.61477518081665
Epoch 1850, val loss: 1.3427201509475708
Epoch 1860, training loss: 0.007533198222517967 = 0.0009256564080715179 + 0.001 * 6.607541561126709
Epoch 1860, val loss: 1.3437193632125854
Epoch 1870, training loss: 0.007571149151772261 = 0.0009197486797347665 + 0.001 * 6.651400089263916
Epoch 1870, val loss: 1.3446236848831177
Epoch 1880, training loss: 0.007509671151638031 = 0.0009140119655057788 + 0.001 * 6.595658779144287
Epoch 1880, val loss: 1.3456058502197266
Epoch 1890, training loss: 0.007518184371292591 = 0.0009083414915949106 + 0.001 * 6.609842777252197
Epoch 1890, val loss: 1.3466122150421143
Epoch 1900, training loss: 0.007494042161852121 = 0.0009027320775203407 + 0.001 * 6.591310024261475
Epoch 1900, val loss: 1.347586989402771
Epoch 1910, training loss: 0.00749807758256793 = 0.0008972407667897642 + 0.001 * 6.600836277008057
Epoch 1910, val loss: 1.3486019372940063
Epoch 1920, training loss: 0.007511300500482321 = 0.0008918646490201354 + 0.001 * 6.6194353103637695
Epoch 1920, val loss: 1.3494491577148438
Epoch 1930, training loss: 0.0074997516348958015 = 0.0008866660064086318 + 0.001 * 6.6130852699279785
Epoch 1930, val loss: 1.3504464626312256
Epoch 1940, training loss: 0.007497088983654976 = 0.0008814719039946795 + 0.001 * 6.615616798400879
Epoch 1940, val loss: 1.3513870239257812
Epoch 1950, training loss: 0.007465742528438568 = 0.0008764240192249417 + 0.001 * 6.58931827545166
Epoch 1950, val loss: 1.3523263931274414
Epoch 1960, training loss: 0.0074782767333090305 = 0.000871458207257092 + 0.001 * 6.606818199157715
Epoch 1960, val loss: 1.353197455406189
Epoch 1970, training loss: 0.007486952934414148 = 0.0008666323265060782 + 0.001 * 6.6203203201293945
Epoch 1970, val loss: 1.3541873693466187
Epoch 1980, training loss: 0.007442642003297806 = 0.0008618745487183332 + 0.001 * 6.5807671546936035
Epoch 1980, val loss: 1.3549821376800537
Epoch 1990, training loss: 0.007424242794513702 = 0.0008571870857849717 + 0.001 * 6.567055702209473
Epoch 1990, val loss: 1.3560160398483276
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8819
Flip ASR: 0.8578/225 nodes
The final ASR:0.87823, 0.08892, Accuracy:0.82469, 0.00630
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11594])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10492])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8296
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.98401, 0.00627, Accuracy:0.83457, 0.00462
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.943992018699646 = 1.9356181621551514 + 0.001 * 8.373885154724121
Epoch 0, val loss: 1.936288833618164
Epoch 10, training loss: 1.9345881938934326 = 1.926214337348938 + 0.001 * 8.373807907104492
Epoch 10, val loss: 1.9270864725112915
Epoch 20, training loss: 1.9226852655410767 = 1.9143117666244507 + 0.001 * 8.373556137084961
Epoch 20, val loss: 1.9147999286651611
Epoch 30, training loss: 1.9056686162948608 = 1.8972957134246826 + 0.001 * 8.372939109802246
Epoch 30, val loss: 1.8968372344970703
Epoch 40, training loss: 1.880623698234558 = 1.872252345085144 + 0.001 * 8.371334075927734
Epoch 40, val loss: 1.8706358671188354
Epoch 50, training loss: 1.8459633588790894 = 1.8375970125198364 + 0.001 * 8.366287231445312
Epoch 50, val loss: 1.8360804319381714
Epoch 60, training loss: 1.8065392971038818 = 1.7981934547424316 + 0.001 * 8.345900535583496
Epoch 60, val loss: 1.8006258010864258
Epoch 70, training loss: 1.7689403295516968 = 1.760705590248108 + 0.001 * 8.234758377075195
Epoch 70, val loss: 1.7703379392623901
Epoch 80, training loss: 1.7198671102523804 = 1.712121844291687 + 0.001 * 7.745233535766602
Epoch 80, val loss: 1.7290891408920288
Epoch 90, training loss: 1.6515076160430908 = 1.6438884735107422 + 0.001 * 7.619172096252441
Epoch 90, val loss: 1.6703251600265503
Epoch 100, training loss: 1.5614633560180664 = 1.5540034770965576 + 0.001 * 7.45988130569458
Epoch 100, val loss: 1.5933964252471924
Epoch 110, training loss: 1.457177758216858 = 1.4498910903930664 + 0.001 * 7.286682605743408
Epoch 110, val loss: 1.5050548315048218
Epoch 120, training loss: 1.347170114517212 = 1.3400012254714966 + 0.001 * 7.168874740600586
Epoch 120, val loss: 1.414908766746521
Epoch 130, training loss: 1.2361122369766235 = 1.2290226221084595 + 0.001 * 7.0896148681640625
Epoch 130, val loss: 1.32740318775177
Epoch 140, training loss: 1.1279113292694092 = 1.1209195852279663 + 0.001 * 6.991803169250488
Epoch 140, val loss: 1.245157241821289
Epoch 150, training loss: 1.0261824131011963 = 1.0192511081695557 + 0.001 * 6.931304454803467
Epoch 150, val loss: 1.1694636344909668
Epoch 160, training loss: 0.9324480295181274 = 0.925544798374176 + 0.001 * 6.903201103210449
Epoch 160, val loss: 1.1003080606460571
Epoch 170, training loss: 0.8457219004631042 = 0.8388367891311646 + 0.001 * 6.88509464263916
Epoch 170, val loss: 1.0365983247756958
Epoch 180, training loss: 0.7644573450088501 = 0.7575796246528625 + 0.001 * 6.877711772918701
Epoch 180, val loss: 0.9770932793617249
Epoch 190, training loss: 0.6882668137550354 = 0.6813918948173523 + 0.001 * 6.8749189376831055
Epoch 190, val loss: 0.921918511390686
Epoch 200, training loss: 0.6181431412696838 = 0.6112716197967529 + 0.001 * 6.87151575088501
Epoch 200, val loss: 0.8726555705070496
Epoch 210, training loss: 0.5552530884742737 = 0.548386812210083 + 0.001 * 6.86627721786499
Epoch 210, val loss: 0.8310706615447998
Epoch 220, training loss: 0.4999050498008728 = 0.49304601550102234 + 0.001 * 6.859039783477783
Epoch 220, val loss: 0.798027753829956
Epoch 230, training loss: 0.4509751796722412 = 0.4441222846508026 + 0.001 * 6.85290002822876
Epoch 230, val loss: 0.7723959684371948
Epoch 240, training loss: 0.40679916739463806 = 0.3999564051628113 + 0.001 * 6.8427734375
Epoch 240, val loss: 0.7521724104881287
Epoch 250, training loss: 0.36613863706588745 = 0.35930541157722473 + 0.001 * 6.833219051361084
Epoch 250, val loss: 0.735622763633728
Epoch 260, training loss: 0.3284623920917511 = 0.3216393291950226 + 0.001 * 6.823053359985352
Epoch 260, val loss: 0.7216401696205139
Epoch 270, training loss: 0.2937276065349579 = 0.2869105637073517 + 0.001 * 6.8170552253723145
Epoch 270, val loss: 0.7097511291503906
Epoch 280, training loss: 0.2618829905986786 = 0.25507470965385437 + 0.001 * 6.808285236358643
Epoch 280, val loss: 0.6998672485351562
Epoch 290, training loss: 0.23270787298679352 = 0.22590331733226776 + 0.001 * 6.804549694061279
Epoch 290, val loss: 0.6916907429695129
Epoch 300, training loss: 0.2058507800102234 = 0.1990484595298767 + 0.001 * 6.80232048034668
Epoch 300, val loss: 0.685023844242096
Epoch 310, training loss: 0.18100440502166748 = 0.17420417070388794 + 0.001 * 6.800241470336914
Epoch 310, val loss: 0.6795511245727539
Epoch 320, training loss: 0.15808559954166412 = 0.15128634870052338 + 0.001 * 6.799254894256592
Epoch 320, val loss: 0.6753296256065369
Epoch 330, training loss: 0.13725632429122925 = 0.13045856356620789 + 0.001 * 6.797765254974365
Epoch 330, val loss: 0.6725067496299744
Epoch 340, training loss: 0.11875105649232864 = 0.11195441335439682 + 0.001 * 6.796645164489746
Epoch 340, val loss: 0.6712784171104431
Epoch 350, training loss: 0.10271775722503662 = 0.09592238813638687 + 0.001 * 6.795370578765869
Epoch 350, val loss: 0.6718518137931824
Epoch 360, training loss: 0.08913062512874603 = 0.08233527094125748 + 0.001 * 6.795355796813965
Epoch 360, val loss: 0.6743361353874207
Epoch 370, training loss: 0.07779398560523987 = 0.07100008428096771 + 0.001 * 6.7938971519470215
Epoch 370, val loss: 0.6785003542900085
Epoch 380, training loss: 0.06841230392456055 = 0.06162004917860031 + 0.001 * 6.792253017425537
Epoch 380, val loss: 0.684160053730011
Epoch 390, training loss: 0.06065940484404564 = 0.053868938237428665 + 0.001 * 6.790467262268066
Epoch 390, val loss: 0.6910096406936646
Epoch 400, training loss: 0.054231658577919006 = 0.04744300618767738 + 0.001 * 6.788650035858154
Epoch 400, val loss: 0.6987908482551575
Epoch 410, training loss: 0.04886976629495621 = 0.04208255186676979 + 0.001 * 6.787214756011963
Epoch 410, val loss: 0.7072644233703613
Epoch 420, training loss: 0.044362738728523254 = 0.03757667914032936 + 0.001 * 6.78605842590332
Epoch 420, val loss: 0.7162272334098816
Epoch 430, training loss: 0.04054349288344383 = 0.03375696763396263 + 0.001 * 6.786524295806885
Epoch 430, val loss: 0.7255238890647888
Epoch 440, training loss: 0.03727602958679199 = 0.030492160469293594 + 0.001 * 6.783868789672852
Epoch 440, val loss: 0.7349857687950134
Epoch 450, training loss: 0.03446614742279053 = 0.027681240811944008 + 0.001 * 6.784907341003418
Epoch 450, val loss: 0.7444702386856079
Epoch 460, training loss: 0.03202896937727928 = 0.02524385042488575 + 0.001 * 6.78511905670166
Epoch 460, val loss: 0.7539935111999512
Epoch 470, training loss: 0.029902618378400803 = 0.02311653271317482 + 0.001 * 6.7860846519470215
Epoch 470, val loss: 0.7634322643280029
Epoch 480, training loss: 0.02803286910057068 = 0.02124934270977974 + 0.001 * 6.783525466918945
Epoch 480, val loss: 0.7727633714675903
Epoch 490, training loss: 0.02638300508260727 = 0.01960153318941593 + 0.001 * 6.781470775604248
Epoch 490, val loss: 0.7820197939872742
Epoch 500, training loss: 0.024919990450143814 = 0.01814032904803753 + 0.001 * 6.779660701751709
Epoch 500, val loss: 0.7910497784614563
Epoch 510, training loss: 0.023615829646587372 = 0.016838856041431427 + 0.001 * 6.77697229385376
Epoch 510, val loss: 0.799987256526947
Epoch 520, training loss: 0.02244981750845909 = 0.015674952417612076 + 0.001 * 6.774865627288818
Epoch 520, val loss: 0.808702290058136
Epoch 530, training loss: 0.021403277292847633 = 0.014630152843892574 + 0.001 * 6.773124694824219
Epoch 530, val loss: 0.8172482848167419
Epoch 540, training loss: 0.02047029137611389 = 0.013688977807760239 + 0.001 * 6.781312942504883
Epoch 540, val loss: 0.8256031274795532
Epoch 550, training loss: 0.01960970088839531 = 0.012838375754654408 + 0.001 * 6.7713236808776855
Epoch 550, val loss: 0.8337537050247192
Epoch 560, training loss: 0.01883758418262005 = 0.012067181058228016 + 0.001 * 6.770402431488037
Epoch 560, val loss: 0.8417196869850159
Epoch 570, training loss: 0.018136747181415558 = 0.011365878395736217 + 0.001 * 6.77086877822876
Epoch 570, val loss: 0.8495153188705444
Epoch 580, training loss: 0.01750471070408821 = 0.01072636991739273 + 0.001 * 6.778339385986328
Epoch 580, val loss: 0.8571327924728394
Epoch 590, training loss: 0.01691509038209915 = 0.010141695849597454 + 0.001 * 6.77339506149292
Epoch 590, val loss: 0.8645600080490112
Epoch 600, training loss: 0.016373464837670326 = 0.009605776518583298 + 0.001 * 6.767688274383545
Epoch 600, val loss: 0.8718271851539612
Epoch 610, training loss: 0.01587946154177189 = 0.009113377891480923 + 0.001 * 6.766083717346191
Epoch 610, val loss: 0.8789198398590088
Epoch 620, training loss: 0.01542775146663189 = 0.008659927174448967 + 0.001 * 6.767824172973633
Epoch 620, val loss: 0.8858373165130615
Epoch 630, training loss: 0.015005761757493019 = 0.008241437375545502 + 0.001 * 6.764323711395264
Epoch 630, val loss: 0.8925944566726685
Epoch 640, training loss: 0.01462507713586092 = 0.00785437598824501 + 0.001 * 6.770700931549072
Epoch 640, val loss: 0.8991987109184265
Epoch 650, training loss: 0.014264538884162903 = 0.007495774421840906 + 0.001 * 6.768764019012451
Epoch 650, val loss: 0.905642032623291
Epoch 660, training loss: 0.013924968428909779 = 0.007162895984947681 + 0.001 * 6.7620720863342285
Epoch 660, val loss: 0.9119181036949158
Epoch 670, training loss: 0.013615457341074944 = 0.0068532852455973625 + 0.001 * 6.762171745300293
Epoch 670, val loss: 0.9180697202682495
Epoch 680, training loss: 0.01332659088075161 = 0.0065648192539811134 + 0.001 * 6.761770725250244
Epoch 680, val loss: 0.9240739345550537
Epoch 690, training loss: 0.01305486261844635 = 0.006295602302998304 + 0.001 * 6.759260177612305
Epoch 690, val loss: 0.9299539923667908
Epoch 700, training loss: 0.012803563848137856 = 0.0060439202934503555 + 0.001 * 6.759643077850342
Epoch 700, val loss: 0.935714066028595
Epoch 710, training loss: 0.012570589780807495 = 0.005808262154459953 + 0.001 * 6.762327194213867
Epoch 710, val loss: 0.941351056098938
Epoch 720, training loss: 0.012349480763077736 = 0.005587294232100248 + 0.001 * 6.762186527252197
Epoch 720, val loss: 0.9468663334846497
Epoch 730, training loss: 0.012142505496740341 = 0.005379735957831144 + 0.001 * 6.762768745422363
Epoch 730, val loss: 0.9522600173950195
Epoch 740, training loss: 0.0119406683370471 = 0.005184537265449762 + 0.001 * 6.756130695343018
Epoch 740, val loss: 0.9575724005699158
Epoch 750, training loss: 0.011755596846342087 = 0.005000671837478876 + 0.001 * 6.754924774169922
Epoch 750, val loss: 0.9627696871757507
Epoch 760, training loss: 0.011584014631807804 = 0.0048273345455527306 + 0.001 * 6.756679534912109
Epoch 760, val loss: 0.9678551554679871
Epoch 770, training loss: 0.011417522095143795 = 0.0046637048944830894 + 0.001 * 6.753817081451416
Epoch 770, val loss: 0.9728505611419678
Epoch 780, training loss: 0.011269163340330124 = 0.004508874379098415 + 0.001 * 6.760289192199707
Epoch 780, val loss: 0.9777215123176575
Epoch 790, training loss: 0.011114075779914856 = 0.004361825063824654 + 0.001 * 6.752249717712402
Epoch 790, val loss: 0.9824808835983276
Epoch 800, training loss: 0.010981855913996696 = 0.004222859628498554 + 0.001 * 6.758996486663818
Epoch 800, val loss: 0.9871957302093506
Epoch 810, training loss: 0.010847617872059345 = 0.004090920556336641 + 0.001 * 6.756697177886963
Epoch 810, val loss: 0.9917852282524109
Epoch 820, training loss: 0.01071277167648077 = 0.0039656194858253 + 0.001 * 6.747151851654053
Epoch 820, val loss: 0.9963131546974182
Epoch 830, training loss: 0.010608278214931488 = 0.0038464886602014303 + 0.001 * 6.761788845062256
Epoch 830, val loss: 1.0007966756820679
Epoch 840, training loss: 0.01048371847718954 = 0.003733176039531827 + 0.001 * 6.750541687011719
Epoch 840, val loss: 1.0051522254943848
Epoch 850, training loss: 0.01037505641579628 = 0.0036252555437386036 + 0.001 * 6.749800205230713
Epoch 850, val loss: 1.0094339847564697
Epoch 860, training loss: 0.010267412289977074 = 0.00352236139588058 + 0.001 * 6.74505090713501
Epoch 860, val loss: 1.0136281251907349
Epoch 870, training loss: 0.010168623179197311 = 0.0034241476096212864 + 0.001 * 6.744475364685059
Epoch 870, val loss: 1.0177841186523438
Epoch 880, training loss: 0.010087274014949799 = 0.0033303755335509777 + 0.001 * 6.756898403167725
Epoch 880, val loss: 1.021846890449524
Epoch 890, training loss: 0.009980843402445316 = 0.0032408596016466618 + 0.001 * 6.739983558654785
Epoch 890, val loss: 1.0258201360702515
Epoch 900, training loss: 0.009904356673359871 = 0.0031552878208458424 + 0.001 * 6.749068737030029
Epoch 900, val loss: 1.0297483205795288
Epoch 910, training loss: 0.009814505465328693 = 0.0030734140891581774 + 0.001 * 6.741090774536133
Epoch 910, val loss: 1.0336039066314697
Epoch 920, training loss: 0.009740980342030525 = 0.0029950335156172514 + 0.001 * 6.745946884155273
Epoch 920, val loss: 1.0374258756637573
Epoch 930, training loss: 0.009661774151027203 = 0.00291995145380497 + 0.001 * 6.741822242736816
Epoch 930, val loss: 1.0411559343338013
Epoch 940, training loss: 0.009584926068782806 = 0.0028480200562626123 + 0.001 * 6.736905574798584
Epoch 940, val loss: 1.0448338985443115
Epoch 950, training loss: 0.009529967792332172 = 0.0027790649328380823 + 0.001 * 6.75090217590332
Epoch 950, val loss: 1.048438310623169
Epoch 960, training loss: 0.00946007203310728 = 0.002712952671572566 + 0.001 * 6.747119426727295
Epoch 960, val loss: 1.0519709587097168
Epoch 970, training loss: 0.009434124454855919 = 0.002649523550644517 + 0.001 * 6.784600734710693
Epoch 970, val loss: 1.055454134941101
Epoch 980, training loss: 0.009330397471785545 = 0.002588610164821148 + 0.001 * 6.741786956787109
Epoch 980, val loss: 1.058868646621704
Epoch 990, training loss: 0.00925871916115284 = 0.0025300646666437387 + 0.001 * 6.728653907775879
Epoch 990, val loss: 1.0622512102127075
Epoch 1000, training loss: 0.009232398122549057 = 0.0024737627245485783 + 0.001 * 6.758634567260742
Epoch 1000, val loss: 1.0655854940414429
Epoch 1010, training loss: 0.009150505997240543 = 0.0024197082966566086 + 0.001 * 6.730797290802002
Epoch 1010, val loss: 1.0688230991363525
Epoch 1020, training loss: 0.009095657616853714 = 0.0023677321150898933 + 0.001 * 6.727924823760986
Epoch 1020, val loss: 1.0720117092132568
Epoch 1030, training loss: 0.009038742631673813 = 0.0023176816757768393 + 0.001 * 6.721060752868652
Epoch 1030, val loss: 1.0751829147338867
Epoch 1040, training loss: 0.009005476720631123 = 0.0022694591898471117 + 0.001 * 6.736017227172852
Epoch 1040, val loss: 1.0782703161239624
Epoch 1050, training loss: 0.00895286537706852 = 0.0022230083122849464 + 0.001 * 6.729856491088867
Epoch 1050, val loss: 1.081313967704773
Epoch 1060, training loss: 0.008913325145840645 = 0.002178235910832882 + 0.001 * 6.735089302062988
Epoch 1060, val loss: 1.0843077898025513
Epoch 1070, training loss: 0.008853122591972351 = 0.002135161543264985 + 0.001 * 6.717960357666016
Epoch 1070, val loss: 1.0872160196304321
Epoch 1080, training loss: 0.008815479464828968 = 0.0020936753135174513 + 0.001 * 6.721803665161133
Epoch 1080, val loss: 1.0900806188583374
Epoch 1090, training loss: 0.008776530623435974 = 0.002053653821349144 + 0.001 * 6.722876071929932
Epoch 1090, val loss: 1.0929088592529297
Epoch 1100, training loss: 0.008744750171899796 = 0.0020150451455265284 + 0.001 * 6.7297043800354
Epoch 1100, val loss: 1.0956898927688599
Epoch 1110, training loss: 0.008696489036083221 = 0.0019777719862759113 + 0.001 * 6.718716144561768
Epoch 1110, val loss: 1.0984270572662354
Epoch 1120, training loss: 0.008652087301015854 = 0.0019417793955653906 + 0.001 * 6.7103071212768555
Epoch 1120, val loss: 1.1011320352554321
Epoch 1130, training loss: 0.008619332686066628 = 0.0019070213893428445 + 0.001 * 6.712310791015625
Epoch 1130, val loss: 1.103779673576355
Epoch 1140, training loss: 0.008594491519033909 = 0.0018734325421974063 + 0.001 * 6.721058368682861
Epoch 1140, val loss: 1.1063792705535889
Epoch 1150, training loss: 0.008576128631830215 = 0.0018409499898552895 + 0.001 * 6.735177993774414
Epoch 1150, val loss: 1.1089651584625244
Epoch 1160, training loss: 0.008516617119312286 = 0.0018095338018611073 + 0.001 * 6.707083225250244
Epoch 1160, val loss: 1.1114952564239502
Epoch 1170, training loss: 0.008488898165524006 = 0.0017791744321584702 + 0.001 * 6.709723472595215
Epoch 1170, val loss: 1.1139740943908691
Epoch 1180, training loss: 0.008455684408545494 = 0.0017498029628768563 + 0.001 * 6.705881118774414
Epoch 1180, val loss: 1.1164045333862305
Epoch 1190, training loss: 0.008430950343608856 = 0.0017213869141414762 + 0.001 * 6.709563255310059
Epoch 1190, val loss: 1.1187959909439087
Epoch 1200, training loss: 0.008398888632655144 = 0.001693899161182344 + 0.001 * 6.704988956451416
Epoch 1200, val loss: 1.1211674213409424
Epoch 1210, training loss: 0.00836959108710289 = 0.001667294534854591 + 0.001 * 6.702296257019043
Epoch 1210, val loss: 1.1235008239746094
Epoch 1220, training loss: 0.008370806463062763 = 0.0016415193676948547 + 0.001 * 6.7292866706848145
Epoch 1220, val loss: 1.1257684230804443
Epoch 1230, training loss: 0.008316556923091412 = 0.0016165707493200898 + 0.001 * 6.699985504150391
Epoch 1230, val loss: 1.1280008554458618
Epoch 1240, training loss: 0.008285479620099068 = 0.0015924074687063694 + 0.001 * 6.693072319030762
Epoch 1240, val loss: 1.1301941871643066
Epoch 1250, training loss: 0.008275163359940052 = 0.0015689950669184327 + 0.001 * 6.706167697906494
Epoch 1250, val loss: 1.132353663444519
Epoch 1260, training loss: 0.008241647854447365 = 0.0015462774317711592 + 0.001 * 6.695370197296143
Epoch 1260, val loss: 1.1344815492630005
Epoch 1270, training loss: 0.008262865245342255 = 0.0015242232475429773 + 0.001 * 6.738641738891602
Epoch 1270, val loss: 1.1365748643875122
Epoch 1280, training loss: 0.008195742964744568 = 0.0015028535854071379 + 0.001 * 6.6928887367248535
Epoch 1280, val loss: 1.1386361122131348
Epoch 1290, training loss: 0.008165396749973297 = 0.0014821086078882217 + 0.001 * 6.683287143707275
Epoch 1290, val loss: 1.1406528949737549
Epoch 1300, training loss: 0.008147471584379673 = 0.0014619969297200441 + 0.001 * 6.685473918914795
Epoch 1300, val loss: 1.142628788948059
Epoch 1310, training loss: 0.0081550944596529 = 0.0014424575492739677 + 0.001 * 6.7126359939575195
Epoch 1310, val loss: 1.1445870399475098
Epoch 1320, training loss: 0.008141542784869671 = 0.0014234638074412942 + 0.001 * 6.718078136444092
Epoch 1320, val loss: 1.1465071439743042
Epoch 1330, training loss: 0.008122111670672894 = 0.0014049935853108764 + 0.001 * 6.717117786407471
Epoch 1330, val loss: 1.1484333276748657
Epoch 1340, training loss: 0.008055554702877998 = 0.0013870522379875183 + 0.001 * 6.668501853942871
Epoch 1340, val loss: 1.1502761840820312
Epoch 1350, training loss: 0.008039005100727081 = 0.0013696375535801053 + 0.001 * 6.66936731338501
Epoch 1350, val loss: 1.1520980596542358
Epoch 1360, training loss: 0.008057236671447754 = 0.0013527162373065948 + 0.001 * 6.704519748687744
Epoch 1360, val loss: 1.1538957357406616
Epoch 1370, training loss: 0.008022069931030273 = 0.0013362825848162174 + 0.001 * 6.685787200927734
Epoch 1370, val loss: 1.1556450128555298
Epoch 1380, training loss: 0.008012000471353531 = 0.0013202832778915763 + 0.001 * 6.69171667098999
Epoch 1380, val loss: 1.1573708057403564
Epoch 1390, training loss: 0.007972757332026958 = 0.0013047389220446348 + 0.001 * 6.668018341064453
Epoch 1390, val loss: 1.1590737104415894
Epoch 1400, training loss: 0.007965398021042347 = 0.0012896059779450297 + 0.001 * 6.675792217254639
Epoch 1400, val loss: 1.160752296447754
Epoch 1410, training loss: 0.007939273491501808 = 0.0012748881708830595 + 0.001 * 6.664384841918945
Epoch 1410, val loss: 1.1624014377593994
Epoch 1420, training loss: 0.007931001484394073 = 0.0012605506926774979 + 0.001 * 6.670450210571289
Epoch 1420, val loss: 1.164031982421875
Epoch 1430, training loss: 0.007896842435002327 = 0.001246584695763886 + 0.001 * 6.650257110595703
Epoch 1430, val loss: 1.1656404733657837
Epoch 1440, training loss: 0.007894077338278294 = 0.001232970505952835 + 0.001 * 6.661106586456299
Epoch 1440, val loss: 1.1672255992889404
Epoch 1450, training loss: 0.007871456444263458 = 0.0012196952011436224 + 0.001 * 6.651760578155518
Epoch 1450, val loss: 1.1687569618225098
Epoch 1460, training loss: 0.007857784628868103 = 0.0012067792704328895 + 0.001 * 6.651005268096924
Epoch 1460, val loss: 1.170301914215088
Epoch 1470, training loss: 0.007860695011913776 = 0.0011942028068006039 + 0.001 * 6.666491985321045
Epoch 1470, val loss: 1.1718097925186157
Epoch 1480, training loss: 0.007837546057999134 = 0.0011819537030532956 + 0.001 * 6.65559196472168
Epoch 1480, val loss: 1.1732721328735352
Epoch 1490, training loss: 0.007826665416359901 = 0.0011700080940499902 + 0.001 * 6.656656742095947
Epoch 1490, val loss: 1.1747057437896729
Epoch 1500, training loss: 0.007818260230123997 = 0.0011583525920286775 + 0.001 * 6.659907341003418
Epoch 1500, val loss: 1.1761529445648193
Epoch 1510, training loss: 0.007837135344743729 = 0.0011469749733805656 + 0.001 * 6.690160274505615
Epoch 1510, val loss: 1.1775141954421997
Epoch 1520, training loss: 0.0077827973291277885 = 0.0011358939809724689 + 0.001 * 6.646903038024902
Epoch 1520, val loss: 1.1789532899856567
Epoch 1530, training loss: 0.007761324755847454 = 0.0011250684037804604 + 0.001 * 6.636256217956543
Epoch 1530, val loss: 1.1803252696990967
Epoch 1540, training loss: 0.00776442838832736 = 0.0011145294411107898 + 0.001 * 6.649898529052734
Epoch 1540, val loss: 1.181634545326233
Epoch 1550, training loss: 0.007794498465955257 = 0.0011042430996894836 + 0.001 * 6.690255165100098
Epoch 1550, val loss: 1.1829783916473389
Epoch 1560, training loss: 0.007734660990536213 = 0.001094216015189886 + 0.001 * 6.640444755554199
Epoch 1560, val loss: 1.184272050857544
Epoch 1570, training loss: 0.007753102574497461 = 0.001084418618120253 + 0.001 * 6.6686835289001465
Epoch 1570, val loss: 1.185542106628418
Epoch 1580, training loss: 0.007699264213442802 = 0.0010748605709522963 + 0.001 * 6.624403476715088
Epoch 1580, val loss: 1.1868165731430054
Epoch 1590, training loss: 0.007703048177063465 = 0.001065523480065167 + 0.001 * 6.637524604797363
Epoch 1590, val loss: 1.1880452632904053
Epoch 1600, training loss: 0.007688856218010187 = 0.0010564100230112672 + 0.001 * 6.632445812225342
Epoch 1600, val loss: 1.189281702041626
Epoch 1610, training loss: 0.0077200583182275295 = 0.0010474961018189788 + 0.001 * 6.6725616455078125
Epoch 1610, val loss: 1.1904860734939575
Epoch 1620, training loss: 0.007670427672564983 = 0.0010387623915448785 + 0.001 * 6.631665229797363
Epoch 1620, val loss: 1.1916447877883911
Epoch 1630, training loss: 0.007648416794836521 = 0.0010302368318662047 + 0.001 * 6.6181793212890625
Epoch 1630, val loss: 1.1928056478500366
Epoch 1640, training loss: 0.0076626562513411045 = 0.0010219021933153272 + 0.001 * 6.640753746032715
Epoch 1640, val loss: 1.1939828395843506
Epoch 1650, training loss: 0.007645484991371632 = 0.0010137506760656834 + 0.001 * 6.6317338943481445
Epoch 1650, val loss: 1.195094347000122
Epoch 1660, training loss: 0.00763576477766037 = 0.0010057640029117465 + 0.001 * 6.630000114440918
Epoch 1660, val loss: 1.196221947669983
Epoch 1670, training loss: 0.007617602590471506 = 0.000997947878204286 + 0.001 * 6.619654178619385
Epoch 1670, val loss: 1.1973038911819458
Epoch 1680, training loss: 0.0075914980843663216 = 0.0009903141763061285 + 0.001 * 6.601183891296387
Epoch 1680, val loss: 1.1983973979949951
Epoch 1690, training loss: 0.007620226591825485 = 0.0009828463662415743 + 0.001 * 6.637380123138428
Epoch 1690, val loss: 1.1994690895080566
Epoch 1700, training loss: 0.007622568868100643 = 0.0009755297796800733 + 0.001 * 6.64703893661499
Epoch 1700, val loss: 1.2005430459976196
Epoch 1710, training loss: 0.007589803077280521 = 0.0009683689568191767 + 0.001 * 6.621434211730957
Epoch 1710, val loss: 1.2015291452407837
Epoch 1720, training loss: 0.007556872442364693 = 0.0009613760630600154 + 0.001 * 6.59549617767334
Epoch 1720, val loss: 1.2025552988052368
Epoch 1730, training loss: 0.007551664020866156 = 0.0009545289794914424 + 0.001 * 6.597135066986084
Epoch 1730, val loss: 1.2035750150680542
Epoch 1740, training loss: 0.007557816803455353 = 0.0009478279389441013 + 0.001 * 6.609988689422607
Epoch 1740, val loss: 1.2045323848724365
Epoch 1750, training loss: 0.007576625328511 = 0.000941281788982451 + 0.001 * 6.635343551635742
Epoch 1750, val loss: 1.2055354118347168
Epoch 1760, training loss: 0.0075610666535794735 = 0.000934857816901058 + 0.001 * 6.626208305358887
Epoch 1760, val loss: 1.2064542770385742
Epoch 1770, training loss: 0.007559194229543209 = 0.0009285729029215872 + 0.001 * 6.630620956420898
Epoch 1770, val loss: 1.2074116468429565
Epoch 1780, training loss: 0.007536349818110466 = 0.0009224193054251373 + 0.001 * 6.6139302253723145
Epoch 1780, val loss: 1.2083719968795776
Epoch 1790, training loss: 0.007517625577747822 = 0.0009163673967123032 + 0.001 * 6.601257801055908
Epoch 1790, val loss: 1.209272027015686
Epoch 1800, training loss: 0.007491188123822212 = 0.000910447386559099 + 0.001 * 6.580740451812744
Epoch 1800, val loss: 1.2101887464523315
Epoch 1810, training loss: 0.007544406224042177 = 0.0009046405903063715 + 0.001 * 6.63976526260376
Epoch 1810, val loss: 1.2110519409179688
Epoch 1820, training loss: 0.00752502353861928 = 0.0008989544003270566 + 0.001 * 6.626068592071533
Epoch 1820, val loss: 1.211945652961731
Epoch 1830, training loss: 0.007476070895791054 = 0.0008933664648793638 + 0.001 * 6.582704067230225
Epoch 1830, val loss: 1.212811827659607
Epoch 1840, training loss: 0.007480388507246971 = 0.0008878870285116136 + 0.001 * 6.592501163482666
Epoch 1840, val loss: 1.2136272192001343
Epoch 1850, training loss: 0.007499489467591047 = 0.0008825332042761147 + 0.001 * 6.616955757141113
Epoch 1850, val loss: 1.2144880294799805
Epoch 1860, training loss: 0.007456917781382799 = 0.0008772625005804002 + 0.001 * 6.579654693603516
Epoch 1860, val loss: 1.2152668237686157
Epoch 1870, training loss: 0.007467165589332581 = 0.0008720973273739219 + 0.001 * 6.595067977905273
Epoch 1870, val loss: 1.2161173820495605
Epoch 1880, training loss: 0.007512741256505251 = 0.0008670353563502431 + 0.001 * 6.645705699920654
Epoch 1880, val loss: 1.2168885469436646
Epoch 1890, training loss: 0.007430614437907934 = 0.0008620430016890168 + 0.001 * 6.568571090698242
Epoch 1890, val loss: 1.2177071571350098
Epoch 1900, training loss: 0.007424418814480305 = 0.00085714046144858 + 0.001 * 6.567277908325195
Epoch 1900, val loss: 1.2184545993804932
Epoch 1910, training loss: 0.0074221729300916195 = 0.0008523431606590748 + 0.001 * 6.56982946395874
Epoch 1910, val loss: 1.2192213535308838
Epoch 1920, training loss: 0.0074266609735786915 = 0.000847635034006089 + 0.001 * 6.579025745391846
Epoch 1920, val loss: 1.2200344800949097
Epoch 1930, training loss: 0.007440381217747927 = 0.0008430089801549911 + 0.001 * 6.597372055053711
Epoch 1930, val loss: 1.2207220792770386
Epoch 1940, training loss: 0.007434322498738766 = 0.0008384741377085447 + 0.001 * 6.595848083496094
Epoch 1940, val loss: 1.221514344215393
Epoch 1950, training loss: 0.007404714357107878 = 0.0008339944179169834 + 0.001 * 6.5707197189331055
Epoch 1950, val loss: 1.2222379446029663
Epoch 1960, training loss: 0.0074123814702034 = 0.0008296037558466196 + 0.001 * 6.58277702331543
Epoch 1960, val loss: 1.2229251861572266
Epoch 1970, training loss: 0.00744035467505455 = 0.0008252940606325865 + 0.001 * 6.615059852600098
Epoch 1970, val loss: 1.2236796617507935
Epoch 1980, training loss: 0.0073656621389091015 = 0.0008210502564907074 + 0.001 * 6.54461145401001
Epoch 1980, val loss: 1.2243409156799316
Epoch 1990, training loss: 0.007406295742839575 = 0.0008168882923200727 + 0.001 * 6.589407444000244
Epoch 1990, val loss: 1.2250643968582153
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.6974
Flip ASR: 0.6356/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.971733808517456 = 1.9633599519729614 + 0.001 * 8.373892784118652
Epoch 0, val loss: 1.9552284479141235
Epoch 10, training loss: 1.9614245891571045 = 1.9530507326126099 + 0.001 * 8.37384033203125
Epoch 10, val loss: 1.945092797279358
Epoch 20, training loss: 1.9486937522888184 = 1.9403201341629028 + 0.001 * 8.373637199401855
Epoch 20, val loss: 1.9321521520614624
Epoch 30, training loss: 1.930659294128418 = 1.9222861528396606 + 0.001 * 8.373132705688477
Epoch 30, val loss: 1.913528323173523
Epoch 40, training loss: 1.9034641981124878 = 1.8950923681259155 + 0.001 * 8.371818542480469
Epoch 40, val loss: 1.8856348991394043
Epoch 50, training loss: 1.8636274337768555 = 1.8552597761154175 + 0.001 * 8.367685317993164
Epoch 50, val loss: 1.8461989164352417
Epoch 60, training loss: 1.8146332502365112 = 1.806283712387085 + 0.001 * 8.349586486816406
Epoch 60, val loss: 1.8018474578857422
Epoch 70, training loss: 1.7700674533843994 = 1.7618122100830078 + 0.001 * 8.25519847869873
Epoch 70, val loss: 1.766041874885559
Epoch 80, training loss: 1.7219457626342773 = 1.7141155004501343 + 0.001 * 7.830210208892822
Epoch 80, val loss: 1.7259491682052612
Epoch 90, training loss: 1.655211329460144 = 1.6475210189819336 + 0.001 * 7.690295696258545
Epoch 90, val loss: 1.67048180103302
Epoch 100, training loss: 1.5678712129592896 = 1.5603888034820557 + 0.001 * 7.482353210449219
Epoch 100, val loss: 1.6002494096755981
Epoch 110, training loss: 1.464778184890747 = 1.4574940204620361 + 0.001 * 7.284151077270508
Epoch 110, val loss: 1.5168912410736084
Epoch 120, training loss: 1.357556939125061 = 1.350342035293579 + 0.001 * 7.214881420135498
Epoch 120, val loss: 1.4285123348236084
Epoch 130, training loss: 1.2498762607574463 = 1.2426854372024536 + 0.001 * 7.1908464431762695
Epoch 130, val loss: 1.341401219367981
Epoch 140, training loss: 1.1417855024337769 = 1.1346372365951538 + 0.001 * 7.148281097412109
Epoch 140, val loss: 1.2552844285964966
Epoch 150, training loss: 1.0348752737045288 = 1.0277537107467651 + 0.001 * 7.121603012084961
Epoch 150, val loss: 1.1714779138565063
Epoch 160, training loss: 0.9341646432876587 = 0.9270716309547424 + 0.001 * 7.093023300170898
Epoch 160, val loss: 1.0945762395858765
Epoch 170, training loss: 0.8453790545463562 = 0.838324785232544 + 0.001 * 7.054251670837402
Epoch 170, val loss: 1.0287961959838867
Epoch 180, training loss: 0.7713605761528015 = 0.7643447518348694 + 0.001 * 7.015798091888428
Epoch 180, val loss: 0.9761661291122437
Epoch 190, training loss: 0.7110501527786255 = 0.7040599584579468 + 0.001 * 6.990173816680908
Epoch 190, val loss: 0.9355190396308899
Epoch 200, training loss: 0.6607487201690674 = 0.6537833213806152 + 0.001 * 6.965402603149414
Epoch 200, val loss: 0.9041666388511658
Epoch 210, training loss: 0.6163086891174316 = 0.6093718409538269 + 0.001 * 6.936824321746826
Epoch 210, val loss: 0.8788926005363464
Epoch 220, training loss: 0.5744019150733948 = 0.5674885511398315 + 0.001 * 6.913337707519531
Epoch 220, val loss: 0.8574491143226624
Epoch 230, training loss: 0.5331748723983765 = 0.5262733101844788 + 0.001 * 6.9015727043151855
Epoch 230, val loss: 0.8384477496147156
Epoch 240, training loss: 0.4922780692577362 = 0.4853818118572235 + 0.001 * 6.89625358581543
Epoch 240, val loss: 0.8216432929039001
Epoch 250, training loss: 0.45236098766326904 = 0.44546645879745483 + 0.001 * 6.894528865814209
Epoch 250, val loss: 0.8077284097671509
Epoch 260, training loss: 0.4140760004520416 = 0.407182902097702 + 0.001 * 6.893095970153809
Epoch 260, val loss: 0.7971472144126892
Epoch 270, training loss: 0.3774878978729248 = 0.37059715390205383 + 0.001 * 6.890757083892822
Epoch 270, val loss: 0.7897858619689941
Epoch 280, training loss: 0.34229350090026855 = 0.335405170917511 + 0.001 * 6.888318061828613
Epoch 280, val loss: 0.7852433919906616
Epoch 290, training loss: 0.3082789182662964 = 0.30139243602752686 + 0.001 * 6.886476993560791
Epoch 290, val loss: 0.7833617925643921
Epoch 300, training loss: 0.2757267653942108 = 0.2688426971435547 + 0.001 * 6.884059429168701
Epoch 300, val loss: 0.7844223976135254
Epoch 310, training loss: 0.24512964487075806 = 0.23824572563171387 + 0.001 * 6.883922576904297
Epoch 310, val loss: 0.7886548638343811
Epoch 320, training loss: 0.21686305105686188 = 0.20998148620128632 + 0.001 * 6.881571292877197
Epoch 320, val loss: 0.7960163950920105
Epoch 330, training loss: 0.19136865437030792 = 0.18448740243911743 + 0.001 * 6.88125467300415
Epoch 330, val loss: 0.8061615228652954
Epoch 340, training loss: 0.16876210272312164 = 0.16188445687294006 + 0.001 * 6.877646446228027
Epoch 340, val loss: 0.8188583254814148
Epoch 350, training loss: 0.14892138540744781 = 0.14204685389995575 + 0.001 * 6.874525547027588
Epoch 350, val loss: 0.8335230350494385
Epoch 360, training loss: 0.13165920972824097 = 0.12478382140398026 + 0.001 * 6.875394821166992
Epoch 360, val loss: 0.8497859835624695
Epoch 370, training loss: 0.11674465984106064 = 0.10987298935651779 + 0.001 * 6.8716721534729
Epoch 370, val loss: 0.8674414753913879
Epoch 380, training loss: 0.10393685102462769 = 0.09706351906061172 + 0.001 * 6.873332500457764
Epoch 380, val loss: 0.8862802982330322
Epoch 390, training loss: 0.09295298904180527 = 0.08608734607696533 + 0.001 * 6.865642547607422
Epoch 390, val loss: 0.9058802127838135
Epoch 400, training loss: 0.08357678353786469 = 0.07671783119440079 + 0.001 * 6.858955383300781
Epoch 400, val loss: 0.9258319139480591
Epoch 410, training loss: 0.07558339834213257 = 0.06872889399528503 + 0.001 * 6.854500770568848
Epoch 410, val loss: 0.9457370638847351
Epoch 420, training loss: 0.06875316798686981 = 0.061897363513708115 + 0.001 * 6.855806350708008
Epoch 420, val loss: 0.9653554558753967
Epoch 430, training loss: 0.06285884976387024 = 0.05601013824343681 + 0.001 * 6.848709583282471
Epoch 430, val loss: 0.9844275712966919
Epoch 440, training loss: 0.05775723606348038 = 0.050913743674755096 + 0.001 * 6.8434929847717285
Epoch 440, val loss: 1.002959966659546
Epoch 450, training loss: 0.05329824611544609 = 0.04645680636167526 + 0.001 * 6.841437816619873
Epoch 450, val loss: 1.0209534168243408
Epoch 460, training loss: 0.049368973821401596 = 0.04253431037068367 + 0.001 * 6.834664344787598
Epoch 460, val loss: 1.038509488105774
Epoch 470, training loss: 0.0458623468875885 = 0.03902251645922661 + 0.001 * 6.8398284912109375
Epoch 470, val loss: 1.0554962158203125
Epoch 480, training loss: 0.04268737882375717 = 0.03585894778370857 + 0.001 * 6.828432083129883
Epoch 480, val loss: 1.0720818042755127
Epoch 490, training loss: 0.039841439574956894 = 0.03301524370908737 + 0.001 * 6.826194763183594
Epoch 490, val loss: 1.0884343385696411
Epoch 500, training loss: 0.03725143522024155 = 0.030428383499383926 + 0.001 * 6.823050498962402
Epoch 500, val loss: 1.1043951511383057
Epoch 510, training loss: 0.03488381579518318 = 0.028060436248779297 + 0.001 * 6.8233795166015625
Epoch 510, val loss: 1.120022177696228
Epoch 520, training loss: 0.03270488232374191 = 0.025880688801407814 + 0.001 * 6.824193477630615
Epoch 520, val loss: 1.1354485750198364
Epoch 530, training loss: 0.030668195337057114 = 0.02384611964225769 + 0.001 * 6.8220744132995605
Epoch 530, val loss: 1.1506983041763306
Epoch 540, training loss: 0.028698313981294632 = 0.021882660686969757 + 0.001 * 6.815652847290039
Epoch 540, val loss: 1.1658689975738525
Epoch 550, training loss: 0.0268852598965168 = 0.020068157464265823 + 0.001 * 6.817101001739502
Epoch 550, val loss: 1.180906891822815
Epoch 560, training loss: 0.02525453269481659 = 0.018431406468153 + 0.001 * 6.823126792907715
Epoch 560, val loss: 1.1959446668624878
Epoch 570, training loss: 0.02375851385295391 = 0.01693897508084774 + 0.001 * 6.8195390701293945
Epoch 570, val loss: 1.2108960151672363
Epoch 580, training loss: 0.02241474762558937 = 0.015604286454617977 + 0.001 * 6.81046199798584
Epoch 580, val loss: 1.2258965969085693
Epoch 590, training loss: 0.021223898977041245 = 0.014410757459700108 + 0.001 * 6.813141822814941
Epoch 590, val loss: 1.2407193183898926
Epoch 600, training loss: 0.020176254212856293 = 0.013366246595978737 + 0.001 * 6.810006141662598
Epoch 600, val loss: 1.2551751136779785
Epoch 610, training loss: 0.019261378794908524 = 0.01245162170380354 + 0.001 * 6.809755802154541
Epoch 610, val loss: 1.2692443132400513
Epoch 620, training loss: 0.01845609024167061 = 0.011643865145742893 + 0.001 * 6.812224864959717
Epoch 620, val loss: 1.2829281091690063
Epoch 630, training loss: 0.017724819481372833 = 0.01091611199080944 + 0.001 * 6.808707237243652
Epoch 630, val loss: 1.2961994409561157
Epoch 640, training loss: 0.017062289640307426 = 0.010257910937070847 + 0.001 * 6.804378509521484
Epoch 640, val loss: 1.3091342449188232
Epoch 650, training loss: 0.01647525653243065 = 0.009667221456766129 + 0.001 * 6.808034420013428
Epoch 650, val loss: 1.3215218782424927
Epoch 660, training loss: 0.015992067754268646 = 0.009137453511357307 + 0.001 * 6.854613304138184
Epoch 660, val loss: 1.3335272073745728
Epoch 670, training loss: 0.015475010499358177 = 0.008657612837851048 + 0.001 * 6.817397594451904
Epoch 670, val loss: 1.3452026844024658
Epoch 680, training loss: 0.01502721756696701 = 0.008221447467803955 + 0.001 * 6.805769443511963
Epoch 680, val loss: 1.3564292192459106
Epoch 690, training loss: 0.014621321111917496 = 0.00782284140586853 + 0.001 * 6.798479080200195
Epoch 690, val loss: 1.367380976676941
Epoch 700, training loss: 0.014253376051783562 = 0.007456356659531593 + 0.001 * 6.7970194816589355
Epoch 700, val loss: 1.377994418144226
Epoch 710, training loss: 0.013919406570494175 = 0.007117772940546274 + 0.001 * 6.801633358001709
Epoch 710, val loss: 1.3882871866226196
Epoch 720, training loss: 0.013610631227493286 = 0.006804916076362133 + 0.001 * 6.805714130401611
Epoch 720, val loss: 1.3982475996017456
Epoch 730, training loss: 0.013311744667589664 = 0.006515066605061293 + 0.001 * 6.796677589416504
Epoch 730, val loss: 1.407972812652588
Epoch 740, training loss: 0.013040488585829735 = 0.006245858501642942 + 0.001 * 6.794629096984863
Epoch 740, val loss: 1.4174660444259644
Epoch 750, training loss: 0.012803427875041962 = 0.005995309446007013 + 0.001 * 6.808117866516113
Epoch 750, val loss: 1.426671028137207
Epoch 760, training loss: 0.012561433017253876 = 0.005761485081166029 + 0.001 * 6.799947261810303
Epoch 760, val loss: 1.4356383085250854
Epoch 770, training loss: 0.012333091348409653 = 0.005542127415537834 + 0.001 * 6.790964126586914
Epoch 770, val loss: 1.4443992376327515
Epoch 780, training loss: 0.012125508859753609 = 0.005336446687579155 + 0.001 * 6.789061546325684
Epoch 780, val loss: 1.452917218208313
Epoch 790, training loss: 0.011947715654969215 = 0.005143602844327688 + 0.001 * 6.804111957550049
Epoch 790, val loss: 1.4612284898757935
Epoch 800, training loss: 0.011751269921660423 = 0.00496250856667757 + 0.001 * 6.788761615753174
Epoch 800, val loss: 1.4693241119384766
Epoch 810, training loss: 0.011579399928450584 = 0.004792282823473215 + 0.001 * 6.787117004394531
Epoch 810, val loss: 1.4772309064865112
Epoch 820, training loss: 0.011424245312809944 = 0.004631953313946724 + 0.001 * 6.792291641235352
Epoch 820, val loss: 1.4849456548690796
Epoch 830, training loss: 0.011275891214609146 = 0.004480719566345215 + 0.001 * 6.795170783996582
Epoch 830, val loss: 1.492477297782898
Epoch 840, training loss: 0.011125623248517513 = 0.004337843973189592 + 0.001 * 6.787778854370117
Epoch 840, val loss: 1.4998807907104492
Epoch 850, training loss: 0.010983102023601532 = 0.004202663898468018 + 0.001 * 6.780438423156738
Epoch 850, val loss: 1.5071032047271729
Epoch 860, training loss: 0.010860087350010872 = 0.004074631724506617 + 0.001 * 6.785455226898193
Epoch 860, val loss: 1.5141710042953491
Epoch 870, training loss: 0.010735180228948593 = 0.003953297156840563 + 0.001 * 6.7818827629089355
Epoch 870, val loss: 1.5210851430892944
Epoch 880, training loss: 0.010625144466757774 = 0.003838153090327978 + 0.001 * 6.786990642547607
Epoch 880, val loss: 1.5278632640838623
Epoch 890, training loss: 0.010510390624403954 = 0.003728809766471386 + 0.001 * 6.781580924987793
Epoch 890, val loss: 1.5344903469085693
Epoch 900, training loss: 0.010403690859675407 = 0.0036248620599508286 + 0.001 * 6.778829097747803
Epoch 900, val loss: 1.540995478630066
Epoch 910, training loss: 0.010304921306669712 = 0.003525925101712346 + 0.001 * 6.778995990753174
Epoch 910, val loss: 1.547367811203003
Epoch 920, training loss: 0.01020028069615364 = 0.0034317164681851864 + 0.001 * 6.768563270568848
Epoch 920, val loss: 1.553617238998413
Epoch 930, training loss: 0.010119978338479996 = 0.003341911593452096 + 0.001 * 6.778066635131836
Epoch 930, val loss: 1.5597541332244873
Epoch 940, training loss: 0.010033214464783669 = 0.0032562564592808485 + 0.001 * 6.776957988739014
Epoch 940, val loss: 1.5657570362091064
Epoch 950, training loss: 0.009945432655513287 = 0.003174463054165244 + 0.001 * 6.770969390869141
Epoch 950, val loss: 1.5716675519943237
Epoch 960, training loss: 0.009856827557086945 = 0.0030962959863245487 + 0.001 * 6.760530948638916
Epoch 960, val loss: 1.577466607093811
Epoch 970, training loss: 0.009799358434975147 = 0.003021502634510398 + 0.001 * 6.777855396270752
Epoch 970, val loss: 1.5831719636917114
Epoch 980, training loss: 0.009732259437441826 = 0.0029498967342078686 + 0.001 * 6.78236198425293
Epoch 980, val loss: 1.5887681245803833
Epoch 990, training loss: 0.00964847207069397 = 0.0028813397511839867 + 0.001 * 6.767131328582764
Epoch 990, val loss: 1.5942461490631104
Epoch 1000, training loss: 0.00958441011607647 = 0.0028156430926173925 + 0.001 * 6.7687668800354
Epoch 1000, val loss: 1.5996448993682861
Epoch 1010, training loss: 0.00951768271625042 = 0.0027526584453880787 + 0.001 * 6.765023708343506
Epoch 1010, val loss: 1.604935646057129
Epoch 1020, training loss: 0.009472066536545753 = 0.0026922193355858326 + 0.001 * 6.779847145080566
Epoch 1020, val loss: 1.610130786895752
Epoch 1030, training loss: 0.00938759557902813 = 0.0026341965422034264 + 0.001 * 6.75339937210083
Epoch 1030, val loss: 1.6152397394180298
Epoch 1040, training loss: 0.009356093592941761 = 0.0025784748140722513 + 0.001 * 6.777618408203125
Epoch 1040, val loss: 1.6202480792999268
Epoch 1050, training loss: 0.009276253171265125 = 0.00252490839920938 + 0.001 * 6.751344203948975
Epoch 1050, val loss: 1.6251651048660278
Epoch 1060, training loss: 0.009235724806785583 = 0.002473411848768592 + 0.001 * 6.762312412261963
Epoch 1060, val loss: 1.6300089359283447
Epoch 1070, training loss: 0.00917772762477398 = 0.002423869678750634 + 0.001 * 6.7538580894470215
Epoch 1070, val loss: 1.6347664594650269
Epoch 1080, training loss: 0.009124613367021084 = 0.0023761652410030365 + 0.001 * 6.748447895050049
Epoch 1080, val loss: 1.6394507884979248
Epoch 1090, training loss: 0.009075850248336792 = 0.002330249873921275 + 0.001 * 6.745599746704102
Epoch 1090, val loss: 1.6440633535385132
Epoch 1100, training loss: 0.009036466479301453 = 0.002285981085151434 + 0.001 * 6.750484943389893
Epoch 1100, val loss: 1.6486009359359741
Epoch 1110, training loss: 0.00898610521107912 = 0.0022433281410485506 + 0.001 * 6.742776870727539
Epoch 1110, val loss: 1.6530482769012451
Epoch 1120, training loss: 0.008945916779339314 = 0.002202191622927785 + 0.001 * 6.743724822998047
Epoch 1120, val loss: 1.657418966293335
Epoch 1130, training loss: 0.008902774192392826 = 0.0021624937653541565 + 0.001 * 6.7402801513671875
Epoch 1130, val loss: 1.6617376804351807
Epoch 1140, training loss: 0.008878150954842567 = 0.002124165650457144 + 0.001 * 6.753985404968262
Epoch 1140, val loss: 1.665962815284729
Epoch 1150, training loss: 0.00882501807063818 = 0.002087166765704751 + 0.001 * 6.737850666046143
Epoch 1150, val loss: 1.6701500415802002
Epoch 1160, training loss: 0.00878324918448925 = 0.002051430521532893 + 0.001 * 6.731818675994873
Epoch 1160, val loss: 1.6742533445358276
Epoch 1170, training loss: 0.008767408318817616 = 0.002016883110627532 + 0.001 * 6.750524520874023
Epoch 1170, val loss: 1.6782934665679932
Epoch 1180, training loss: 0.00871619675308466 = 0.0019834828563034534 + 0.001 * 6.73271369934082
Epoch 1180, val loss: 1.6822696924209595
Epoch 1190, training loss: 0.00867739412933588 = 0.0019511713180691004 + 0.001 * 6.726222038269043
Epoch 1190, val loss: 1.6861865520477295
Epoch 1200, training loss: 0.008653934113681316 = 0.00191992218606174 + 0.001 * 6.734011650085449
Epoch 1200, val loss: 1.6900432109832764
Epoch 1210, training loss: 0.008604820817708969 = 0.0018896569963544607 + 0.001 * 6.715163707733154
Epoch 1210, val loss: 1.6938467025756836
Epoch 1220, training loss: 0.00859776046127081 = 0.0018603611970320344 + 0.001 * 6.737398624420166
Epoch 1220, val loss: 1.6976157426834106
Epoch 1230, training loss: 0.008558939211070538 = 0.0018319928785786033 + 0.001 * 6.7269463539123535
Epoch 1230, val loss: 1.7012988328933716
Epoch 1240, training loss: 0.008524383418262005 = 0.0018045022152364254 + 0.001 * 6.719881057739258
Epoch 1240, val loss: 1.704946756362915
Epoch 1250, training loss: 0.008541316725313663 = 0.0017778475303202868 + 0.001 * 6.7634687423706055
Epoch 1250, val loss: 1.7085497379302979
Epoch 1260, training loss: 0.008489107713103294 = 0.0017520161345601082 + 0.001 * 6.737091541290283
Epoch 1260, val loss: 1.7120615243911743
Epoch 1270, training loss: 0.008466813713312149 = 0.0017269776435568929 + 0.001 * 6.739835262298584
Epoch 1270, val loss: 1.7155591249465942
Epoch 1280, training loss: 0.00843990407884121 = 0.0017026541754603386 + 0.001 * 6.73724889755249
Epoch 1280, val loss: 1.7189995050430298
Epoch 1290, training loss: 0.008396895602345467 = 0.0016790644731372595 + 0.001 * 6.717830657958984
Epoch 1290, val loss: 1.7223873138427734
Epoch 1300, training loss: 0.00836506299674511 = 0.0016561573138460517 + 0.001 * 6.708905220031738
Epoch 1300, val loss: 1.7257311344146729
Epoch 1310, training loss: 0.008394036442041397 = 0.001633902546018362 + 0.001 * 6.760134220123291
Epoch 1310, val loss: 1.7290383577346802
Epoch 1320, training loss: 0.008323261514306068 = 0.0016123165842145681 + 0.001 * 6.710944652557373
Epoch 1320, val loss: 1.7322845458984375
Epoch 1330, training loss: 0.00833628885447979 = 0.0015913121169432998 + 0.001 * 6.744976997375488
Epoch 1330, val loss: 1.7355002164840698
Epoch 1340, training loss: 0.008281546644866467 = 0.001570929423905909 + 0.001 * 6.7106170654296875
Epoch 1340, val loss: 1.7386554479599
Epoch 1350, training loss: 0.008242348209023476 = 0.0015510825905948877 + 0.001 * 6.691265106201172
Epoch 1350, val loss: 1.7417993545532227
Epoch 1360, training loss: 0.00822650920599699 = 0.0015317995566874743 + 0.001 * 6.694709300994873
Epoch 1360, val loss: 1.7448574304580688
Epoch 1370, training loss: 0.008238636888563633 = 0.001513045746833086 + 0.001 * 6.725590705871582
Epoch 1370, val loss: 1.7478941679000854
Epoch 1380, training loss: 0.008214060217142105 = 0.0014947705203667283 + 0.001 * 6.7192888259887695
Epoch 1380, val loss: 1.750921368598938
Epoch 1390, training loss: 0.008157718926668167 = 0.0014770030975341797 + 0.001 * 6.680715560913086
Epoch 1390, val loss: 1.753866195678711
Epoch 1400, training loss: 0.008143296465277672 = 0.0014597047120332718 + 0.001 * 6.683590888977051
Epoch 1400, val loss: 1.7567890882492065
Epoch 1410, training loss: 0.008182402700185776 = 0.0014428364811465144 + 0.001 * 6.739566326141357
Epoch 1410, val loss: 1.7597044706344604
Epoch 1420, training loss: 0.008116904646158218 = 0.0014264375204220414 + 0.001 * 6.690466403961182
Epoch 1420, val loss: 1.7625524997711182
Epoch 1430, training loss: 0.008082499727606773 = 0.0014104503206908703 + 0.001 * 6.672049522399902
Epoch 1430, val loss: 1.7653874158859253
Epoch 1440, training loss: 0.008091563358902931 = 0.0013948827981948853 + 0.001 * 6.696680545806885
Epoch 1440, val loss: 1.7681570053100586
Epoch 1450, training loss: 0.008079198189079762 = 0.0013797079445794225 + 0.001 * 6.699490070343018
Epoch 1450, val loss: 1.7708981037139893
Epoch 1460, training loss: 0.008045227266848087 = 0.0013649148168042302 + 0.001 * 6.680312156677246
Epoch 1460, val loss: 1.7735952138900757
Epoch 1470, training loss: 0.008024381473660469 = 0.001350509119220078 + 0.001 * 6.673871994018555
Epoch 1470, val loss: 1.776267170906067
Epoch 1480, training loss: 0.00806998647749424 = 0.0013364516198635101 + 0.001 * 6.733534336090088
Epoch 1480, val loss: 1.7789332866668701
Epoch 1490, training loss: 0.007999822497367859 = 0.0013227574527263641 + 0.001 * 6.677064418792725
Epoch 1490, val loss: 1.7815340757369995
Epoch 1500, training loss: 0.007976066321134567 = 0.0013093706220388412 + 0.001 * 6.666696071624756
Epoch 1500, val loss: 1.7841166257858276
Epoch 1510, training loss: 0.00799588579684496 = 0.0012963322224095464 + 0.001 * 6.699553489685059
Epoch 1510, val loss: 1.7866867780685425
Epoch 1520, training loss: 0.007939676754176617 = 0.001283580670133233 + 0.001 * 6.656095504760742
Epoch 1520, val loss: 1.7892372608184814
Epoch 1530, training loss: 0.007937784306704998 = 0.0012711375020444393 + 0.001 * 6.666646480560303
Epoch 1530, val loss: 1.7917330265045166
Epoch 1540, training loss: 0.007934203371405602 = 0.0012589936377480626 + 0.001 * 6.6752095222473145
Epoch 1540, val loss: 1.7942140102386475
Epoch 1550, training loss: 0.007963762618601322 = 0.0012471488444134593 + 0.001 * 6.716613292694092
Epoch 1550, val loss: 1.7966862916946411
Epoch 1560, training loss: 0.007886921986937523 = 0.0012355631915852427 + 0.001 * 6.651358604431152
Epoch 1560, val loss: 1.7991235256195068
Epoch 1570, training loss: 0.007899298332631588 = 0.0012242553057149053 + 0.001 * 6.675042152404785
Epoch 1570, val loss: 1.801521897315979
Epoch 1580, training loss: 0.007896801456809044 = 0.0012132130796089768 + 0.001 * 6.683588027954102
Epoch 1580, val loss: 1.8038935661315918
Epoch 1590, training loss: 0.007882783189415932 = 0.0012024093884974718 + 0.001 * 6.680373191833496
Epoch 1590, val loss: 1.8062511682510376
Epoch 1600, training loss: 0.007840334437787533 = 0.0011918487725779414 + 0.001 * 6.6484856605529785
Epoch 1600, val loss: 1.8085660934448242
Epoch 1610, training loss: 0.007842508144676685 = 0.001181529718451202 + 0.001 * 6.660977840423584
Epoch 1610, val loss: 1.810858964920044
Epoch 1620, training loss: 0.007853230461478233 = 0.0011714515276253223 + 0.001 * 6.681778907775879
Epoch 1620, val loss: 1.8131365776062012
Epoch 1630, training loss: 0.007821209728717804 = 0.00116156751755625 + 0.001 * 6.659641742706299
Epoch 1630, val loss: 1.8154125213623047
Epoch 1640, training loss: 0.0078003983944654465 = 0.0011519104009494185 + 0.001 * 6.648487567901611
Epoch 1640, val loss: 1.8176419734954834
Epoch 1650, training loss: 0.007780740968883038 = 0.0011424579424783587 + 0.001 * 6.638282775878906
Epoch 1650, val loss: 1.8198444843292236
Epoch 1660, training loss: 0.0077994223684072495 = 0.0011332076974213123 + 0.001 * 6.666214466094971
Epoch 1660, val loss: 1.8220453262329102
Epoch 1670, training loss: 0.007776814047247171 = 0.0011241664178669453 + 0.001 * 6.652647495269775
Epoch 1670, val loss: 1.8242155313491821
Epoch 1680, training loss: 0.007756696082651615 = 0.00111530558206141 + 0.001 * 6.641389846801758
Epoch 1680, val loss: 1.8263697624206543
Epoch 1690, training loss: 0.007736646104604006 = 0.0011066319420933723 + 0.001 * 6.630013942718506
Epoch 1690, val loss: 1.8285012245178223
Epoch 1700, training loss: 0.007782905828207731 = 0.0010981402592733502 + 0.001 * 6.684765338897705
Epoch 1700, val loss: 1.8306183815002441
Epoch 1710, training loss: 0.0077428356744349 = 0.0010898144682869315 + 0.001 * 6.653020858764648
Epoch 1710, val loss: 1.8327196836471558
Epoch 1720, training loss: 0.007722523994743824 = 0.0010816605063155293 + 0.001 * 6.640863418579102
Epoch 1720, val loss: 1.8347983360290527
Epoch 1730, training loss: 0.007723761722445488 = 0.0010736850090324879 + 0.001 * 6.650076389312744
Epoch 1730, val loss: 1.836880087852478
Epoch 1740, training loss: 0.007689003832638264 = 0.0010658518876880407 + 0.001 * 6.6231513023376465
Epoch 1740, val loss: 1.8389546871185303
Epoch 1750, training loss: 0.007687279023230076 = 0.001058182679116726 + 0.001 * 6.629096031188965
Epoch 1750, val loss: 1.8409879207611084
Epoch 1760, training loss: 0.0076681822538375854 = 0.0010506620164960623 + 0.001 * 6.617520332336426
Epoch 1760, val loss: 1.8429808616638184
Epoch 1770, training loss: 0.007699164561927319 = 0.0010432797716930509 + 0.001 * 6.655884742736816
Epoch 1770, val loss: 1.8449996709823608
Epoch 1780, training loss: 0.007657704874873161 = 0.0010360677260905504 + 0.001 * 6.621636390686035
Epoch 1780, val loss: 1.846950888633728
Epoch 1790, training loss: 0.0076496717520058155 = 0.0010289768688380718 + 0.001 * 6.620694637298584
Epoch 1790, val loss: 1.8489114046096802
Epoch 1800, training loss: 0.007666044868528843 = 0.0010220211697742343 + 0.001 * 6.644023418426514
Epoch 1800, val loss: 1.8508888483047485
Epoch 1810, training loss: 0.00767693854868412 = 0.0010152205359190702 + 0.001 * 6.661717891693115
Epoch 1810, val loss: 1.8528099060058594
Epoch 1820, training loss: 0.007652187719941139 = 0.0010085280518978834 + 0.001 * 6.643659591674805
Epoch 1820, val loss: 1.8547358512878418
Epoch 1830, training loss: 0.007609461434185505 = 0.0010019663022831082 + 0.001 * 6.607494831085205
Epoch 1830, val loss: 1.8566282987594604
Epoch 1840, training loss: 0.007594771683216095 = 0.0009955138666555285 + 0.001 * 6.599257469177246
Epoch 1840, val loss: 1.858506679534912
Epoch 1850, training loss: 0.007638299837708473 = 0.0009891879744827747 + 0.001 * 6.649111747741699
Epoch 1850, val loss: 1.8603767156600952
Epoch 1860, training loss: 0.00759697612375021 = 0.0009829798946157098 + 0.001 * 6.6139960289001465
Epoch 1860, val loss: 1.8622186183929443
Epoch 1870, training loss: 0.007609233260154724 = 0.0009768755408003926 + 0.001 * 6.632357597351074
Epoch 1870, val loss: 1.864061713218689
Epoch 1880, training loss: 0.007605723105370998 = 0.0009708712459541857 + 0.001 * 6.634851455688477
Epoch 1880, val loss: 1.8658983707427979
Epoch 1890, training loss: 0.007577600888907909 = 0.0009649857529439032 + 0.001 * 6.61261510848999
Epoch 1890, val loss: 1.8677000999450684
Epoch 1900, training loss: 0.007567618042230606 = 0.0009591936250217259 + 0.001 * 6.608423709869385
Epoch 1900, val loss: 1.8694863319396973
Epoch 1910, training loss: 0.007594722788780928 = 0.0009535200661048293 + 0.001 * 6.641202449798584
Epoch 1910, val loss: 1.871235966682434
Epoch 1920, training loss: 0.007549506612122059 = 0.0009479198488406837 + 0.001 * 6.601586818695068
Epoch 1920, val loss: 1.8729987144470215
Epoch 1930, training loss: 0.0075345877557992935 = 0.0009424318559467793 + 0.001 * 6.592155456542969
Epoch 1930, val loss: 1.8747340440750122
Epoch 1940, training loss: 0.007525753695517778 = 0.0009370273910462856 + 0.001 * 6.588726043701172
Epoch 1940, val loss: 1.876463770866394
Epoch 1950, training loss: 0.007532058283686638 = 0.0009317219955846667 + 0.001 * 6.600336074829102
Epoch 1950, val loss: 1.878165602684021
Epoch 1960, training loss: 0.007571185473352671 = 0.0009264964610338211 + 0.001 * 6.644688606262207
Epoch 1960, val loss: 1.8799006938934326
Epoch 1970, training loss: 0.007507589180022478 = 0.0009213699959218502 + 0.001 * 6.58621883392334
Epoch 1970, val loss: 1.8815362453460693
Epoch 1980, training loss: 0.007528422400355339 = 0.0009163153008557856 + 0.001 * 6.612106800079346
Epoch 1980, val loss: 1.883165955543518
Epoch 1990, training loss: 0.007510887458920479 = 0.0009113505366258323 + 0.001 * 6.599536895751953
Epoch 1990, val loss: 1.8847687244415283
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.8266
Flip ASR: 0.7911/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9659756422042847 = 1.95760178565979 + 0.001 * 8.373862266540527
Epoch 0, val loss: 1.951945185661316
Epoch 10, training loss: 1.9556933641433716 = 1.947319507598877 + 0.001 * 8.37380599975586
Epoch 10, val loss: 1.9419331550598145
Epoch 20, training loss: 1.9429783821105957 = 1.9346047639846802 + 0.001 * 8.373578071594238
Epoch 20, val loss: 1.9290846586227417
Epoch 30, training loss: 1.9247320890426636 = 1.9163590669631958 + 0.001 * 8.373062133789062
Epoch 30, val loss: 1.91021728515625
Epoch 40, training loss: 1.8971892595291138 = 1.8888174295425415 + 0.001 * 8.371780395507812
Epoch 40, val loss: 1.8819057941436768
Epoch 50, training loss: 1.8576669692993164 = 1.8492990732192993 + 0.001 * 8.367855072021484
Epoch 50, val loss: 1.842848539352417
Epoch 60, training loss: 1.8111484050750732 = 1.802796483039856 + 0.001 * 8.351920127868652
Epoch 60, val loss: 1.8009207248687744
Epoch 70, training loss: 1.7692753076553345 = 1.7610024213790894 + 0.001 * 8.272926330566406
Epoch 70, val loss: 1.7661752700805664
Epoch 80, training loss: 1.7203998565673828 = 1.712562084197998 + 0.001 * 7.837786674499512
Epoch 80, val loss: 1.722272276878357
Epoch 90, training loss: 1.6545687913894653 = 1.6468240022659302 + 0.001 * 7.744762420654297
Epoch 90, val loss: 1.663953185081482
Epoch 100, training loss: 1.5681049823760986 = 1.5604546070098877 + 0.001 * 7.650433540344238
Epoch 100, val loss: 1.5915801525115967
Epoch 110, training loss: 1.470495343208313 = 1.4630011320114136 + 0.001 * 7.494213581085205
Epoch 110, val loss: 1.5147120952606201
Epoch 120, training loss: 1.3759605884552002 = 1.3687286376953125 + 0.001 * 7.231934070587158
Epoch 120, val loss: 1.4461735486984253
Epoch 130, training loss: 1.2901644706726074 = 1.283018708229065 + 0.001 * 7.145791530609131
Epoch 130, val loss: 1.3892606496810913
Epoch 140, training loss: 1.2116657495498657 = 1.2045623064041138 + 0.001 * 7.103490352630615
Epoch 140, val loss: 1.3407952785491943
Epoch 150, training loss: 1.138002872467041 = 1.1309369802474976 + 0.001 * 7.065942287445068
Epoch 150, val loss: 1.2963918447494507
Epoch 160, training loss: 1.0674724578857422 = 1.0604591369628906 + 0.001 * 7.013296127319336
Epoch 160, val loss: 1.2539836168289185
Epoch 170, training loss: 0.999854564666748 = 0.9928972125053406 + 0.001 * 6.957345485687256
Epoch 170, val loss: 1.2127758264541626
Epoch 180, training loss: 0.9349516034126282 = 0.9280348420143127 + 0.001 * 6.916759967803955
Epoch 180, val loss: 1.172183632850647
Epoch 190, training loss: 0.8714798092842102 = 0.8645928502082825 + 0.001 * 6.886974811553955
Epoch 190, val loss: 1.1311111450195312
Epoch 200, training loss: 0.8070665001869202 = 0.8002023100852966 + 0.001 * 6.864185810089111
Epoch 200, val loss: 1.0875829458236694
Epoch 210, training loss: 0.739779531955719 = 0.7329275608062744 + 0.001 * 6.851975917816162
Epoch 210, val loss: 1.0413711071014404
Epoch 220, training loss: 0.6697254776954651 = 0.6628755331039429 + 0.001 * 6.8499436378479
Epoch 220, val loss: 0.9940196871757507
Epoch 230, training loss: 0.5997888445854187 = 0.5929405093193054 + 0.001 * 6.848306655883789
Epoch 230, val loss: 0.9492284655570984
Epoch 240, training loss: 0.5337086319923401 = 0.5268607139587402 + 0.001 * 6.847891330718994
Epoch 240, val loss: 0.9112626314163208
Epoch 250, training loss: 0.473818302154541 = 0.46697086095809937 + 0.001 * 6.847436904907227
Epoch 250, val loss: 0.8827093839645386
Epoch 260, training loss: 0.42049822211265564 = 0.4136509299278259 + 0.001 * 6.847286224365234
Epoch 260, val loss: 0.8635775446891785
Epoch 270, training loss: 0.37311625480651855 = 0.3662688732147217 + 0.001 * 6.84738302230835
Epoch 270, val loss: 0.8523768186569214
Epoch 280, training loss: 0.33080828189849854 = 0.32396063208580017 + 0.001 * 6.84764289855957
Epoch 280, val loss: 0.8476800918579102
Epoch 290, training loss: 0.29268282651901245 = 0.28583475947380066 + 0.001 * 6.848066329956055
Epoch 290, val loss: 0.8475301265716553
Epoch 300, training loss: 0.2580263316631317 = 0.25117766857147217 + 0.001 * 6.848649024963379
Epoch 300, val loss: 0.8509587645530701
Epoch 310, training loss: 0.22640109062194824 = 0.21955177187919617 + 0.001 * 6.8493242263793945
Epoch 310, val loss: 0.8575416207313538
Epoch 320, training loss: 0.19756972789764404 = 0.190718412399292 + 0.001 * 6.851314067840576
Epoch 320, val loss: 0.866402268409729
Epoch 330, training loss: 0.1715407520532608 = 0.1646895706653595 + 0.001 * 6.85117769241333
Epoch 330, val loss: 0.8774308562278748
Epoch 340, training loss: 0.14843332767486572 = 0.14158150553703308 + 0.001 * 6.851822853088379
Epoch 340, val loss: 0.8904816508293152
Epoch 350, training loss: 0.12835636734962463 = 0.1215040311217308 + 0.001 * 6.8523430824279785
Epoch 350, val loss: 0.9051593542098999
Epoch 360, training loss: 0.1112654060125351 = 0.10441254079341888 + 0.001 * 6.8528666496276855
Epoch 360, val loss: 0.9208088517189026
Epoch 370, training loss: 0.09692081809043884 = 0.0900665894150734 + 0.001 * 6.8542304039001465
Epoch 370, val loss: 0.9372357130050659
Epoch 380, training loss: 0.08500458300113678 = 0.07814998924732208 + 0.001 * 6.854591369628906
Epoch 380, val loss: 0.9540042281150818
Epoch 390, training loss: 0.07513219863176346 = 0.06827817112207413 + 0.001 * 6.854029655456543
Epoch 390, val loss: 0.9705216288566589
Epoch 400, training loss: 0.06695209443569183 = 0.06009808927774429 + 0.001 * 6.854008674621582
Epoch 400, val loss: 0.9866262674331665
Epoch 410, training loss: 0.06014471873641014 = 0.05328941345214844 + 0.001 * 6.855304718017578
Epoch 410, val loss: 1.00215482711792
Epoch 420, training loss: 0.05443933606147766 = 0.047585245221853256 + 0.001 * 6.854091167449951
Epoch 420, val loss: 1.016977071762085
Epoch 430, training loss: 0.04962005466222763 = 0.042766932398080826 + 0.001 * 6.853122711181641
Epoch 430, val loss: 1.0311806201934814
Epoch 440, training loss: 0.0455160066485405 = 0.03866196051239967 + 0.001 * 6.8540472984313965
Epoch 440, val loss: 1.04490065574646
Epoch 450, training loss: 0.041985321789979935 = 0.03513363003730774 + 0.001 * 6.851690292358398
Epoch 450, val loss: 1.058221459388733
Epoch 460, training loss: 0.038929522037506104 = 0.032077621668577194 + 0.001 * 6.85189962387085
Epoch 460, val loss: 1.0712493658065796
Epoch 470, training loss: 0.03626077249646187 = 0.029411252588033676 + 0.001 * 6.8495192527771
Epoch 470, val loss: 1.0839568376541138
Epoch 480, training loss: 0.033917926251888275 = 0.02706955373287201 + 0.001 * 6.8483710289001465
Epoch 480, val loss: 1.0963503122329712
Epoch 490, training loss: 0.031846869736909866 = 0.02500048093497753 + 0.001 * 6.846389293670654
Epoch 490, val loss: 1.1084975004196167
Epoch 500, training loss: 0.030004208907485008 = 0.023162728175520897 + 0.001 * 6.841480255126953
Epoch 500, val loss: 1.1203633546829224
Epoch 510, training loss: 0.028360353782773018 = 0.021521463990211487 + 0.001 * 6.8388895988464355
Epoch 510, val loss: 1.1319576501846313
Epoch 520, training loss: 0.02688964642584324 = 0.0200496818870306 + 0.001 * 6.839963912963867
Epoch 520, val loss: 1.143322467803955
Epoch 530, training loss: 0.025559118017554283 = 0.01872486248612404 + 0.001 * 6.834254741668701
Epoch 530, val loss: 1.1544078588485718
Epoch 540, training loss: 0.024367181584239006 = 0.017527900636196136 + 0.001 * 6.839280128479004
Epoch 540, val loss: 1.165287733078003
Epoch 550, training loss: 0.023281412199139595 = 0.016442889347672462 + 0.001 * 6.838522911071777
Epoch 550, val loss: 1.175907015800476
Epoch 560, training loss: 0.02228374592959881 = 0.015455965884029865 + 0.001 * 6.827779769897461
Epoch 560, val loss: 1.1863256692886353
Epoch 570, training loss: 0.02137690596282482 = 0.014556217938661575 + 0.001 * 6.820687294006348
Epoch 570, val loss: 1.1964670419692993
Epoch 580, training loss: 0.02055223472416401 = 0.013733156025409698 + 0.001 * 6.8190789222717285
Epoch 580, val loss: 1.2063789367675781
Epoch 590, training loss: 0.01979200914502144 = 0.012978492304682732 + 0.001 * 6.813515663146973
Epoch 590, val loss: 1.2160558700561523
Epoch 600, training loss: 0.019096974283456802 = 0.012284461408853531 + 0.001 * 6.81251335144043
Epoch 600, val loss: 1.2255361080169678
Epoch 610, training loss: 0.01845218800008297 = 0.011645029298961163 + 0.001 * 6.807158470153809
Epoch 610, val loss: 1.2347303628921509
Epoch 620, training loss: 0.01792791485786438 = 0.011054798029363155 + 0.001 * 6.873116970062256
Epoch 620, val loss: 1.243772268295288
Epoch 630, training loss: 0.01732267439365387 = 0.01050927396863699 + 0.001 * 6.813401222229004
Epoch 630, val loss: 1.2525092363357544
Epoch 640, training loss: 0.016800472512841225 = 0.010004181414842606 + 0.001 * 6.796291351318359
Epoch 640, val loss: 1.2610692977905273
Epoch 650, training loss: 0.016330568119883537 = 0.009535402990877628 + 0.001 * 6.795164108276367
Epoch 650, val loss: 1.269464373588562
Epoch 660, training loss: 0.015900149941444397 = 0.009099442511796951 + 0.001 * 6.8007073402404785
Epoch 660, val loss: 1.2776156663894653
Epoch 670, training loss: 0.015489752404391766 = 0.008693445473909378 + 0.001 * 6.796306610107422
Epoch 670, val loss: 1.285597562789917
Epoch 680, training loss: 0.0151156485080719 = 0.008314752019941807 + 0.001 * 6.800895690917969
Epoch 680, val loss: 1.2933982610702515
Epoch 690, training loss: 0.014740879647433758 = 0.007960922084748745 + 0.001 * 6.779957294464111
Epoch 690, val loss: 1.3010172843933105
Epoch 700, training loss: 0.014412220567464828 = 0.007629872299730778 + 0.001 * 6.782347679138184
Epoch 700, val loss: 1.3084076642990112
Epoch 710, training loss: 0.014103754423558712 = 0.007319689262658358 + 0.001 * 6.784064769744873
Epoch 710, val loss: 1.3156942129135132
Epoch 720, training loss: 0.013802426867187023 = 0.007028901018202305 + 0.001 * 6.773525714874268
Epoch 720, val loss: 1.322768211364746
Epoch 730, training loss: 0.013533908873796463 = 0.00675585912540555 + 0.001 * 6.778049468994141
Epoch 730, val loss: 1.3297001123428345
Epoch 740, training loss: 0.01327461190521717 = 0.006499194074422121 + 0.001 * 6.775417804718018
Epoch 740, val loss: 1.3365131616592407
Epoch 750, training loss: 0.013034310191869736 = 0.006257660686969757 + 0.001 * 6.776648998260498
Epoch 750, val loss: 1.3431535959243774
Epoch 760, training loss: 0.01281654927879572 = 0.006030072923749685 + 0.001 * 6.786476135253906
Epoch 760, val loss: 1.3496519327163696
Epoch 770, training loss: 0.0125736054033041 = 0.005815392360091209 + 0.001 * 6.758212566375732
Epoch 770, val loss: 1.3560067415237427
Epoch 780, training loss: 0.01237514242529869 = 0.005612692330032587 + 0.001 * 6.762449264526367
Epoch 780, val loss: 1.3622114658355713
Epoch 790, training loss: 0.012195641174912453 = 0.00542113883420825 + 0.001 * 6.774501800537109
Epoch 790, val loss: 1.368275761604309
Epoch 800, training loss: 0.01199601124972105 = 0.005239925347268581 + 0.001 * 6.756085395812988
Epoch 800, val loss: 1.3742483854293823
Epoch 810, training loss: 0.011827288195490837 = 0.0050683594308793545 + 0.001 * 6.758928298950195
Epoch 810, val loss: 1.3800698518753052
Epoch 820, training loss: 0.011651858687400818 = 0.00490574212744832 + 0.001 * 6.7461161613464355
Epoch 820, val loss: 1.385797381401062
Epoch 830, training loss: 0.011509479954838753 = 0.004751498345285654 + 0.001 * 6.757981777191162
Epoch 830, val loss: 1.3913897275924683
Epoch 840, training loss: 0.011407457292079926 = 0.004605045076459646 + 0.001 * 6.8024115562438965
Epoch 840, val loss: 1.396881103515625
Epoch 850, training loss: 0.011227522045373917 = 0.0044658975675702095 + 0.001 * 6.761624813079834
Epoch 850, val loss: 1.4022339582443237
Epoch 860, training loss: 0.01108018308877945 = 0.004333583638072014 + 0.001 * 6.746599197387695
Epoch 860, val loss: 1.4075002670288086
Epoch 870, training loss: 0.010947123169898987 = 0.004207691643387079 + 0.001 * 6.739431381225586
Epoch 870, val loss: 1.412636637687683
Epoch 880, training loss: 0.010815998539328575 = 0.0040877885185182095 + 0.001 * 6.728209972381592
Epoch 880, val loss: 1.4176814556121826
Epoch 890, training loss: 0.010731570422649384 = 0.00397359486669302 + 0.001 * 6.757974624633789
Epoch 890, val loss: 1.4225761890411377
Epoch 900, training loss: 0.010605599731206894 = 0.003864708822220564 + 0.001 * 6.7408905029296875
Epoch 900, val loss: 1.42745840549469
Epoch 910, training loss: 0.010492037981748581 = 0.003760778345167637 + 0.001 * 6.731259822845459
Epoch 910, val loss: 1.4322221279144287
Epoch 920, training loss: 0.01038290373980999 = 0.003661499358713627 + 0.001 * 6.721404075622559
Epoch 920, val loss: 1.4368889331817627
Epoch 930, training loss: 0.010300150141119957 = 0.003566635772585869 + 0.001 * 6.733514308929443
Epoch 930, val loss: 1.4414955377578735
Epoch 940, training loss: 0.010198403149843216 = 0.003475862555205822 + 0.001 * 6.722539901733398
Epoch 940, val loss: 1.4460184574127197
Epoch 950, training loss: 0.010131004266440868 = 0.0033888942562043667 + 0.001 * 6.742109775543213
Epoch 950, val loss: 1.4504344463348389
Epoch 960, training loss: 0.01006350852549076 = 0.003305425401777029 + 0.001 * 6.758083343505859
Epoch 960, val loss: 1.4547609090805054
Epoch 970, training loss: 0.009933377616107464 = 0.0032251484226435423 + 0.001 * 6.708228588104248
Epoch 970, val loss: 1.459036946296692
Epoch 980, training loss: 0.009860759600996971 = 0.003147733863443136 + 0.001 * 6.71302604675293
Epoch 980, val loss: 1.463239312171936
Epoch 990, training loss: 0.009789838455617428 = 0.0030728511046618223 + 0.001 * 6.716986656188965
Epoch 990, val loss: 1.4673808813095093
Epoch 1000, training loss: 0.009711169637739658 = 0.0030001692939549685 + 0.001 * 6.711000442504883
Epoch 1000, val loss: 1.471440076828003
Epoch 1010, training loss: 0.009668727405369282 = 0.0029294639825820923 + 0.001 * 6.73926305770874
Epoch 1010, val loss: 1.4754846096038818
Epoch 1020, training loss: 0.009564755484461784 = 0.002860588952898979 + 0.001 * 6.704166889190674
Epoch 1020, val loss: 1.4794137477874756
Epoch 1030, training loss: 0.00949031487107277 = 0.0027934585232287645 + 0.001 * 6.6968560218811035
Epoch 1030, val loss: 1.4833300113677979
Epoch 1040, training loss: 0.009431073442101479 = 0.00272808107547462 + 0.001 * 6.7029924392700195
Epoch 1040, val loss: 1.4871796369552612
Epoch 1050, training loss: 0.009365495294332504 = 0.0026644598692655563 + 0.001 * 6.701035499572754
Epoch 1050, val loss: 1.4909995794296265
Epoch 1060, training loss: 0.009303123690187931 = 0.00260255322791636 + 0.001 * 6.700570106506348
Epoch 1060, val loss: 1.4947881698608398
Epoch 1070, training loss: 0.00925234891474247 = 0.0025423455517739058 + 0.001 * 6.710002899169922
Epoch 1070, val loss: 1.4985332489013672
Epoch 1080, training loss: 0.009183369576931 = 0.0024838231038302183 + 0.001 * 6.6995463371276855
Epoch 1080, val loss: 1.5022013187408447
Epoch 1090, training loss: 0.009105299599468708 = 0.0024269053246825933 + 0.001 * 6.678394317626953
Epoch 1090, val loss: 1.5058950185775757
Epoch 1100, training loss: 0.009051935747265816 = 0.002371428767219186 + 0.001 * 6.680506229400635
Epoch 1100, val loss: 1.5095187425613403
Epoch 1110, training loss: 0.008996721357107162 = 0.00231720507144928 + 0.001 * 6.679516315460205
Epoch 1110, val loss: 1.513149380683899
Epoch 1120, training loss: 0.008972803130745888 = 0.0022639967501163483 + 0.001 * 6.708805561065674
Epoch 1120, val loss: 1.5167217254638672
Epoch 1130, training loss: 0.008911140263080597 = 0.0022116259206086397 + 0.001 * 6.699513912200928
Epoch 1130, val loss: 1.5202940702438354
Epoch 1140, training loss: 0.008849565871059895 = 0.002160159172490239 + 0.001 * 6.689406394958496
Epoch 1140, val loss: 1.523884654045105
Epoch 1150, training loss: 0.0088031105697155 = 0.002109732013195753 + 0.001 * 6.693378925323486
Epoch 1150, val loss: 1.527532935142517
Epoch 1160, training loss: 0.008797322399914265 = 0.0020603793673217297 + 0.001 * 6.736942768096924
Epoch 1160, val loss: 1.531143307685852
Epoch 1170, training loss: 0.008671974763274193 = 0.0020123454742133617 + 0.001 * 6.659628868103027
Epoch 1170, val loss: 1.5347615480422974
Epoch 1180, training loss: 0.00867970660328865 = 0.001965572824701667 + 0.001 * 6.714133262634277
Epoch 1180, val loss: 1.5383358001708984
Epoch 1190, training loss: 0.008578959852457047 = 0.0019202382536605 + 0.001 * 6.658720970153809
Epoch 1190, val loss: 1.5419654846191406
Epoch 1200, training loss: 0.008531827479600906 = 0.0018763327971100807 + 0.001 * 6.655494689941406
Epoch 1200, val loss: 1.545592188835144
Epoch 1210, training loss: 0.008500529453158379 = 0.0018339292146265507 + 0.001 * 6.666599273681641
Epoch 1210, val loss: 1.5491933822631836
Epoch 1220, training loss: 0.00845186784863472 = 0.001793118193745613 + 0.001 * 6.658749103546143
Epoch 1220, val loss: 1.552711844444275
Epoch 1230, training loss: 0.008415884338319302 = 0.0017538518877699971 + 0.001 * 6.662031650543213
Epoch 1230, val loss: 1.5562756061553955
Epoch 1240, training loss: 0.00840013287961483 = 0.0017160170245915651 + 0.001 * 6.684115409851074
Epoch 1240, val loss: 1.5597505569458008
Epoch 1250, training loss: 0.008333777077496052 = 0.0016796266427263618 + 0.001 * 6.654150009155273
Epoch 1250, val loss: 1.5631994009017944
Epoch 1260, training loss: 0.008320321328938007 = 0.0016445721266791224 + 0.001 * 6.6757493019104
Epoch 1260, val loss: 1.5666645765304565
Epoch 1270, training loss: 0.008266177959740162 = 0.0016109186690300703 + 0.001 * 6.655258655548096
Epoch 1270, val loss: 1.5699530839920044
Epoch 1280, training loss: 0.008249301463365555 = 0.0015785795403644443 + 0.001 * 6.670722007751465
Epoch 1280, val loss: 1.5733321905136108
Epoch 1290, training loss: 0.008181889541447163 = 0.0015474932733923197 + 0.001 * 6.634396076202393
Epoch 1290, val loss: 1.5766116380691528
Epoch 1300, training loss: 0.00818781740963459 = 0.0015176453161984682 + 0.001 * 6.670171737670898
Epoch 1300, val loss: 1.5798287391662598
Epoch 1310, training loss: 0.008151252754032612 = 0.0014889586018398404 + 0.001 * 6.662293434143066
Epoch 1310, val loss: 1.5830198526382446
Epoch 1320, training loss: 0.00810908991843462 = 0.0014613522216677666 + 0.001 * 6.647737503051758
Epoch 1320, val loss: 1.5861984491348267
Epoch 1330, training loss: 0.008088442496955395 = 0.0014347474789246917 + 0.001 * 6.653695106506348
Epoch 1330, val loss: 1.5892729759216309
Epoch 1340, training loss: 0.008060085587203503 = 0.0014091513585299253 + 0.001 * 6.650933742523193
Epoch 1340, val loss: 1.5923316478729248
Epoch 1350, training loss: 0.008037284016609192 = 0.0013844706118106842 + 0.001 * 6.652812480926514
Epoch 1350, val loss: 1.5953304767608643
Epoch 1360, training loss: 0.008001881651580334 = 0.0013607292203232646 + 0.001 * 6.6411519050598145
Epoch 1360, val loss: 1.5982615947723389
Epoch 1370, training loss: 0.007997412234544754 = 0.0013378455769270658 + 0.001 * 6.659566402435303
Epoch 1370, val loss: 1.6011416912078857
Epoch 1380, training loss: 0.007987860590219498 = 0.0013157795183360577 + 0.001 * 6.6720805168151855
Epoch 1380, val loss: 1.604023814201355
Epoch 1390, training loss: 0.007920305244624615 = 0.001294483314268291 + 0.001 * 6.625822067260742
Epoch 1390, val loss: 1.6068042516708374
Epoch 1400, training loss: 0.007906412705779076 = 0.0012739653466269374 + 0.001 * 6.632447242736816
Epoch 1400, val loss: 1.609584093093872
Epoch 1410, training loss: 0.007882232777774334 = 0.0012541498290374875 + 0.001 * 6.628082275390625
Epoch 1410, val loss: 1.6123470067977905
Epoch 1420, training loss: 0.007854485884308815 = 0.001235011382959783 + 0.001 * 6.619473934173584
Epoch 1420, val loss: 1.6149791479110718
Epoch 1430, training loss: 0.00782839022576809 = 0.0012165512889623642 + 0.001 * 6.611839294433594
Epoch 1430, val loss: 1.6176223754882812
Epoch 1440, training loss: 0.007849189452826977 = 0.0011987211182713509 + 0.001 * 6.650467872619629
Epoch 1440, val loss: 1.620190143585205
Epoch 1450, training loss: 0.007795833516865969 = 0.0011815017787739635 + 0.001 * 6.614331245422363
Epoch 1450, val loss: 1.6227147579193115
Epoch 1460, training loss: 0.007772143930196762 = 0.001164868357591331 + 0.001 * 6.607275009155273
Epoch 1460, val loss: 1.6252243518829346
Epoch 1470, training loss: 0.007768108043819666 = 0.0011487695155665278 + 0.001 * 6.619338035583496
Epoch 1470, val loss: 1.6276695728302002
Epoch 1480, training loss: 0.0077495090663433075 = 0.0011331855785101652 + 0.001 * 6.616322994232178
Epoch 1480, val loss: 1.6300371885299683
Epoch 1490, training loss: 0.00772752333432436 = 0.0011180873261764646 + 0.001 * 6.60943603515625
Epoch 1490, val loss: 1.6324186325073242
Epoch 1500, training loss: 0.007724483497440815 = 0.0011034656781703234 + 0.001 * 6.6210174560546875
Epoch 1500, val loss: 1.6347737312316895
Epoch 1510, training loss: 0.007736600935459137 = 0.001089305616915226 + 0.001 * 6.647294998168945
Epoch 1510, val loss: 1.6370519399642944
Epoch 1520, training loss: 0.007687767036259174 = 0.0010756682604551315 + 0.001 * 6.612098693847656
Epoch 1520, val loss: 1.6393067836761475
Epoch 1530, training loss: 0.007666054647415876 = 0.0010624206624925137 + 0.001 * 6.603633880615234
Epoch 1530, val loss: 1.6415354013442993
Epoch 1540, training loss: 0.007647092919796705 = 0.0010495559545233846 + 0.001 * 6.597536563873291
Epoch 1540, val loss: 1.6436948776245117
Epoch 1550, training loss: 0.007657498121261597 = 0.0010370791424065828 + 0.001 * 6.620418548583984
Epoch 1550, val loss: 1.6458994150161743
Epoch 1560, training loss: 0.007651582360267639 = 0.0010250132763758302 + 0.001 * 6.626568794250488
Epoch 1560, val loss: 1.64803946018219
Epoch 1570, training loss: 0.007643536664545536 = 0.0010133034083992243 + 0.001 * 6.630232810974121
Epoch 1570, val loss: 1.6501290798187256
Epoch 1580, training loss: 0.00759334210306406 = 0.001001953729428351 + 0.001 * 6.59138822555542
Epoch 1580, val loss: 1.652198076248169
Epoch 1590, training loss: 0.007591472938656807 = 0.0009909284999594092 + 0.001 * 6.600543975830078
Epoch 1590, val loss: 1.654266119003296
Epoch 1600, training loss: 0.007563011255115271 = 0.0009802010608837008 + 0.001 * 6.582809925079346
Epoch 1600, val loss: 1.6562631130218506
Epoch 1610, training loss: 0.007570043671876192 = 0.0009697830537334085 + 0.001 * 6.600260257720947
Epoch 1610, val loss: 1.6582187414169312
Epoch 1620, training loss: 0.0075863078236579895 = 0.0009596675517968833 + 0.001 * 6.626640319824219
Epoch 1620, val loss: 1.6601554155349731
Epoch 1630, training loss: 0.0075731584802269936 = 0.0009498193394392729 + 0.001 * 6.6233391761779785
Epoch 1630, val loss: 1.6620919704437256
Epoch 1640, training loss: 0.007525590248405933 = 0.0009402843425050378 + 0.001 * 6.585305690765381
Epoch 1640, val loss: 1.6639372110366821
Epoch 1650, training loss: 0.007521260995417833 = 0.0009309730376116931 + 0.001 * 6.590287685394287
Epoch 1650, val loss: 1.6657803058624268
Epoch 1660, training loss: 0.0075173028744757175 = 0.0009219070198014379 + 0.001 * 6.595395565032959
Epoch 1660, val loss: 1.6675165891647339
Epoch 1670, training loss: 0.00755277881398797 = 0.0009130997932516038 + 0.001 * 6.639678478240967
Epoch 1670, val loss: 1.6693391799926758
Epoch 1680, training loss: 0.007502020336687565 = 0.0009045107872225344 + 0.001 * 6.597509384155273
Epoch 1680, val loss: 1.6710363626480103
Epoch 1690, training loss: 0.00750005804002285 = 0.0008961664861999452 + 0.001 * 6.603891372680664
Epoch 1690, val loss: 1.6727908849716187
Epoch 1700, training loss: 0.0074596526101231575 = 0.0008880458772182465 + 0.001 * 6.571606636047363
Epoch 1700, val loss: 1.6744307279586792
Epoch 1710, training loss: 0.007469718344509602 = 0.0008801769581623375 + 0.001 * 6.589540958404541
Epoch 1710, val loss: 1.6760894060134888
Epoch 1720, training loss: 0.007445219904184341 = 0.0008725226507522166 + 0.001 * 6.572697162628174
Epoch 1720, val loss: 1.677690863609314
Epoch 1730, training loss: 0.007437016814947128 = 0.0008650725358165801 + 0.001 * 6.571944236755371
Epoch 1730, val loss: 1.6793352365493774
Epoch 1740, training loss: 0.007440558169037104 = 0.0008578105480410159 + 0.001 * 6.582747459411621
Epoch 1740, val loss: 1.68083918094635
Epoch 1750, training loss: 0.007428514771163464 = 0.0008507201564498246 + 0.001 * 6.577794075012207
Epoch 1750, val loss: 1.6824599504470825
Epoch 1760, training loss: 0.0074418396688997746 = 0.0008438178338110447 + 0.001 * 6.598021507263184
Epoch 1760, val loss: 1.6839215755462646
Epoch 1770, training loss: 0.0074294633232057095 = 0.0008370759314857423 + 0.001 * 6.5923871994018555
Epoch 1770, val loss: 1.6854466199874878
Epoch 1780, training loss: 0.007406491786241531 = 0.0008305464289151132 + 0.001 * 6.575944900512695
Epoch 1780, val loss: 1.686862587928772
Epoch 1790, training loss: 0.007393591571599245 = 0.000824169663246721 + 0.001 * 6.569421768188477
Epoch 1790, val loss: 1.6883010864257812
Epoch 1800, training loss: 0.007380473427474499 = 0.0008179622236639261 + 0.001 * 6.5625104904174805
Epoch 1800, val loss: 1.689706802368164
Epoch 1810, training loss: 0.0073756505735218525 = 0.0008119015255942941 + 0.001 * 6.563748836517334
Epoch 1810, val loss: 1.6911096572875977
Epoch 1820, training loss: 0.007349401246756315 = 0.0008059737156145275 + 0.001 * 6.543427467346191
Epoch 1820, val loss: 1.692488431930542
Epoch 1830, training loss: 0.0073607563972473145 = 0.0008001845562830567 + 0.001 * 6.560571670532227
Epoch 1830, val loss: 1.693852424621582
Epoch 1840, training loss: 0.007398432120680809 = 0.0007945414399728179 + 0.001 * 6.603890419006348
Epoch 1840, val loss: 1.6951643228530884
Epoch 1850, training loss: 0.00735338544473052 = 0.0007890324923209846 + 0.001 * 6.564352989196777
Epoch 1850, val loss: 1.6964932680130005
Epoch 1860, training loss: 0.007398378569632769 = 0.0007836688891984522 + 0.001 * 6.614709377288818
Epoch 1860, val loss: 1.6977474689483643
Epoch 1870, training loss: 0.007325780112296343 = 0.0007784064509905875 + 0.001 * 6.547373294830322
Epoch 1870, val loss: 1.6990033388137817
Epoch 1880, training loss: 0.007349930237978697 = 0.000773266248870641 + 0.001 * 6.576663494110107
Epoch 1880, val loss: 1.7002595663070679
Epoch 1890, training loss: 0.007314346730709076 = 0.0007682400173507631 + 0.001 * 6.546106338500977
Epoch 1890, val loss: 1.7013890743255615
Epoch 1900, training loss: 0.007360878400504589 = 0.0007633198401890695 + 0.001 * 6.59755802154541
Epoch 1900, val loss: 1.7026718854904175
Epoch 1910, training loss: 0.007310120388865471 = 0.000758503912948072 + 0.001 * 6.551616191864014
Epoch 1910, val loss: 1.7037690877914429
Epoch 1920, training loss: 0.007345603313297033 = 0.0007537912460975349 + 0.001 * 6.5918121337890625
Epoch 1920, val loss: 1.7049962282180786
Epoch 1930, training loss: 0.0072981128469109535 = 0.0007492472650483251 + 0.001 * 6.54886531829834
Epoch 1930, val loss: 1.7060034275054932
Epoch 1940, training loss: 0.007298952899873257 = 0.0007447906536981463 + 0.001 * 6.55416202545166
Epoch 1940, val loss: 1.7071235179901123
Epoch 1950, training loss: 0.007298632524907589 = 0.0007404473726637661 + 0.001 * 6.558184623718262
Epoch 1950, val loss: 1.70814049243927
Epoch 1960, training loss: 0.007293835282325745 = 0.0007362202159129083 + 0.001 * 6.557614803314209
Epoch 1960, val loss: 1.709202766418457
Epoch 1970, training loss: 0.007308716885745525 = 0.0007320607546716928 + 0.001 * 6.576655864715576
Epoch 1970, val loss: 1.710213541984558
Epoch 1980, training loss: 0.007293566130101681 = 0.000728011887986213 + 0.001 * 6.565553665161133
Epoch 1980, val loss: 1.7112101316452026
Epoch 1990, training loss: 0.007265498396009207 = 0.0007240244303829968 + 0.001 * 6.541473388671875
Epoch 1990, val loss: 1.7122594118118286
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.8708
Flip ASR: 0.8444/225 nodes
The final ASR:0.79828, 0.07357, Accuracy:0.81481, 0.00302
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11618])
remove edge: torch.Size([2, 9470])
updated graph: torch.Size([2, 10532])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97786, 0.00301, Accuracy:0.83457, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9490697383880615 = 1.9406957626342773 + 0.001 * 8.373927116394043
Epoch 0, val loss: 1.9325059652328491
Epoch 10, training loss: 1.9391239881515503 = 1.9307501316070557 + 0.001 * 8.373882293701172
Epoch 10, val loss: 1.9236364364624023
Epoch 20, training loss: 1.9269824028015137 = 1.9186086654663086 + 0.001 * 8.373712539672852
Epoch 20, val loss: 1.9126297235488892
Epoch 30, training loss: 1.9101154804229736 = 1.9017421007156372 + 0.001 * 8.373329162597656
Epoch 30, val loss: 1.897232174873352
Epoch 40, training loss: 1.8855962753295898 = 1.8772238492965698 + 0.001 * 8.372367858886719
Epoch 40, val loss: 1.8750431537628174
Epoch 50, training loss: 1.8511675596237183 = 1.842798113822937 + 0.001 * 8.36942195892334
Epoch 50, val loss: 1.8452177047729492
Epoch 60, training loss: 1.809399127960205 = 1.8010413646697998 + 0.001 * 8.35780143737793
Epoch 60, val loss: 1.8116753101348877
Epoch 70, training loss: 1.7654389142990112 = 1.7571431398391724 + 0.001 * 8.295737266540527
Epoch 70, val loss: 1.776408314704895
Epoch 80, training loss: 1.7086529731750488 = 1.7007454633712769 + 0.001 * 7.907546043395996
Epoch 80, val loss: 1.7250921726226807
Epoch 90, training loss: 1.6298645734786987 = 1.6220896244049072 + 0.001 * 7.774918079376221
Epoch 90, val loss: 1.6548941135406494
Epoch 100, training loss: 1.531901240348816 = 1.5241918563842773 + 0.001 * 7.709401607513428
Epoch 100, val loss: 1.5743860006332397
Epoch 110, training loss: 1.4270662069320679 = 1.419451117515564 + 0.001 * 7.6151123046875
Epoch 110, val loss: 1.4884560108184814
Epoch 120, training loss: 1.3261467218399048 = 1.3187329769134521 + 0.001 * 7.413764953613281
Epoch 120, val loss: 1.4090899229049683
Epoch 130, training loss: 1.2341474294662476 = 1.2268494367599487 + 0.001 * 7.297940254211426
Epoch 130, val loss: 1.3401821851730347
Epoch 140, training loss: 1.1517337560653687 = 1.1444495916366577 + 0.001 * 7.284111499786377
Epoch 140, val loss: 1.2822524309158325
Epoch 150, training loss: 1.0764098167419434 = 1.0691394805908203 + 0.001 * 7.27034854888916
Epoch 150, val loss: 1.2315157651901245
Epoch 160, training loss: 1.0055170059204102 = 0.9982609748840332 + 0.001 * 7.256033897399902
Epoch 160, val loss: 1.1846638917922974
Epoch 170, training loss: 0.9377703666687012 = 0.9305340647697449 + 0.001 * 7.236326217651367
Epoch 170, val loss: 1.1401333808898926
Epoch 180, training loss: 0.8715766668319702 = 0.8643693327903748 + 0.001 * 7.207341194152832
Epoch 180, val loss: 1.0966168642044067
Epoch 190, training loss: 0.804774284362793 = 0.7976118326187134 + 0.001 * 7.162432670593262
Epoch 190, val loss: 1.0526247024536133
Epoch 200, training loss: 0.736478865146637 = 0.7293810844421387 + 0.001 * 7.097778797149658
Epoch 200, val loss: 1.0078349113464355
Epoch 210, training loss: 0.6683058142662048 = 0.6612577438354492 + 0.001 * 7.048084735870361
Epoch 210, val loss: 0.9645721316337585
Epoch 220, training loss: 0.6031369566917419 = 0.5961205363273621 + 0.001 * 7.016407012939453
Epoch 220, val loss: 0.925584614276886
Epoch 230, training loss: 0.5434061884880066 = 0.5364055633544922 + 0.001 * 7.0005998611450195
Epoch 230, val loss: 0.8927255272865295
Epoch 240, training loss: 0.48967742919921875 = 0.4826902151107788 + 0.001 * 6.987222194671631
Epoch 240, val loss: 0.8660577535629272
Epoch 250, training loss: 0.4410988986492157 = 0.43412157893180847 + 0.001 * 6.977313041687012
Epoch 250, val loss: 0.8449747562408447
Epoch 260, training loss: 0.3963010013103485 = 0.38933229446411133 + 0.001 * 6.968703269958496
Epoch 260, val loss: 0.8288583159446716
Epoch 270, training loss: 0.35407865047454834 = 0.34712013602256775 + 0.001 * 6.958514213562012
Epoch 270, val loss: 0.8172350525856018
Epoch 280, training loss: 0.3139331340789795 = 0.3069835305213928 + 0.001 * 6.949588775634766
Epoch 280, val loss: 0.8093360662460327
Epoch 290, training loss: 0.27637189626693726 = 0.269427627325058 + 0.001 * 6.944273471832275
Epoch 290, val loss: 0.8046285510063171
Epoch 300, training loss: 0.24213530123233795 = 0.23519861698150635 + 0.001 * 6.936678886413574
Epoch 300, val loss: 0.8029878735542297
Epoch 310, training loss: 0.2117185890674591 = 0.20478180050849915 + 0.001 * 6.936781883239746
Epoch 310, val loss: 0.8046117424964905
Epoch 320, training loss: 0.18507243692874908 = 0.17814509570598602 + 0.001 * 6.927346706390381
Epoch 320, val loss: 0.8094165921211243
Epoch 330, training loss: 0.16189220547676086 = 0.15497426688671112 + 0.001 * 6.917938709259033
Epoch 330, val loss: 0.8167851567268372
Epoch 340, training loss: 0.14184615015983582 = 0.13492193818092346 + 0.001 * 6.92421293258667
Epoch 340, val loss: 0.8262059092521667
Epoch 350, training loss: 0.12456413358449936 = 0.11765209585428238 + 0.001 * 6.912038803100586
Epoch 350, val loss: 0.837218701839447
Epoch 360, training loss: 0.10974586755037308 = 0.10283508151769638 + 0.001 * 6.9107890129089355
Epoch 360, val loss: 0.849468469619751
Epoch 370, training loss: 0.09704902768135071 = 0.09014177322387695 + 0.001 * 6.907254695892334
Epoch 370, val loss: 0.8626063466072083
Epoch 380, training loss: 0.08617231994867325 = 0.07926604896783829 + 0.001 * 6.906271457672119
Epoch 380, val loss: 0.8766028881072998
Epoch 390, training loss: 0.07683894038200378 = 0.06993387639522552 + 0.001 * 6.905060768127441
Epoch 390, val loss: 0.8913747072219849
Epoch 400, training loss: 0.06880789250135422 = 0.061906345188617706 + 0.001 * 6.901548385620117
Epoch 400, val loss: 0.9067886471748352
Epoch 410, training loss: 0.06188749521970749 = 0.05498715490102768 + 0.001 * 6.900339126586914
Epoch 410, val loss: 0.922731876373291
Epoch 420, training loss: 0.05591996759176254 = 0.04901185631752014 + 0.001 * 6.908109188079834
Epoch 420, val loss: 0.9389293789863586
Epoch 430, training loss: 0.05074453353881836 = 0.043843746185302734 + 0.001 * 6.900784969329834
Epoch 430, val loss: 0.955426812171936
Epoch 440, training loss: 0.04626168683171272 = 0.039364561438560486 + 0.001 * 6.897125720977783
Epoch 440, val loss: 0.971987247467041
Epoch 450, training loss: 0.04236776754260063 = 0.03547263517975807 + 0.001 * 6.8951311111450195
Epoch 450, val loss: 0.9886465668678284
Epoch 460, training loss: 0.0389845184981823 = 0.032082270830869675 + 0.001 * 6.902245998382568
Epoch 460, val loss: 1.005102276802063
Epoch 470, training loss: 0.036015238612890244 = 0.029120391234755516 + 0.001 * 6.894847393035889
Epoch 470, val loss: 1.0213415622711182
Epoch 480, training loss: 0.03341764584183693 = 0.02652602642774582 + 0.001 * 6.891618728637695
Epoch 480, val loss: 1.0372138023376465
Epoch 490, training loss: 0.03113337606191635 = 0.024246294051408768 + 0.001 * 6.887080669403076
Epoch 490, val loss: 1.0527276992797852
Epoch 500, training loss: 0.02912314236164093 = 0.02223672904074192 + 0.001 * 6.886413097381592
Epoch 500, val loss: 1.0678256750106812
Epoch 510, training loss: 0.027343343943357468 = 0.02045970782637596 + 0.001 * 6.883636474609375
Epoch 510, val loss: 1.0824799537658691
Epoch 520, training loss: 0.025775637477636337 = 0.018882930278778076 + 0.001 * 6.892707824707031
Epoch 520, val loss: 1.096684455871582
Epoch 530, training loss: 0.02436043694615364 = 0.017479082569479942 + 0.001 * 6.881353855133057
Epoch 530, val loss: 1.1104159355163574
Epoch 540, training loss: 0.02310892567038536 = 0.016225088387727737 + 0.001 * 6.8838372230529785
Epoch 540, val loss: 1.1237282752990723
Epoch 550, training loss: 0.02197926864027977 = 0.015101684257388115 + 0.001 * 6.877584457397461
Epoch 550, val loss: 1.1366010904312134
Epoch 560, training loss: 0.020971152931451797 = 0.014091856777668 + 0.001 * 6.87929630279541
Epoch 560, val loss: 1.1490864753723145
Epoch 570, training loss: 0.020054731518030167 = 0.013181122951209545 + 0.001 * 6.873607158660889
Epoch 570, val loss: 1.1611906290054321
Epoch 580, training loss: 0.01922723650932312 = 0.012357978150248528 + 0.001 * 6.86925745010376
Epoch 580, val loss: 1.172937035560608
Epoch 590, training loss: 0.018483880907297134 = 0.011611873283982277 + 0.001 * 6.872006416320801
Epoch 590, val loss: 1.1842745542526245
Epoch 600, training loss: 0.017798524349927902 = 0.010933651588857174 + 0.001 * 6.864872455596924
Epoch 600, val loss: 1.195353388786316
Epoch 610, training loss: 0.017194587737321854 = 0.010315529070794582 + 0.001 * 6.879058361053467
Epoch 610, val loss: 1.2060497999191284
Epoch 620, training loss: 0.01661585457623005 = 0.0097505496814847 + 0.001 * 6.865303993225098
Epoch 620, val loss: 1.2164392471313477
Epoch 630, training loss: 0.016091372817754745 = 0.009232860058546066 + 0.001 * 6.858511924743652
Epoch 630, val loss: 1.2265335321426392
Epoch 640, training loss: 0.015619728714227676 = 0.008757312782108784 + 0.001 * 6.8624162673950195
Epoch 640, val loss: 1.2363344430923462
Epoch 650, training loss: 0.015178800560534 = 0.008319509215652943 + 0.001 * 6.859291076660156
Epoch 650, val loss: 1.2458769083023071
Epoch 660, training loss: 0.014776255935430527 = 0.007915589958429337 + 0.001 * 6.860665321350098
Epoch 660, val loss: 1.255155086517334
Epoch 670, training loss: 0.014400456100702286 = 0.007542046718299389 + 0.001 * 6.858408451080322
Epoch 670, val loss: 1.264161467552185
Epoch 680, training loss: 0.01405174657702446 = 0.007195680867880583 + 0.001 * 6.856064796447754
Epoch 680, val loss: 1.2729345560073853
Epoch 690, training loss: 0.013724149204790592 = 0.006873424164950848 + 0.001 * 6.850724697113037
Epoch 690, val loss: 1.2815053462982178
Epoch 700, training loss: 0.013425861485302448 = 0.006572688464075327 + 0.001 * 6.853172779083252
Epoch 700, val loss: 1.2898844480514526
Epoch 710, training loss: 0.013143573887646198 = 0.006291260477155447 + 0.001 * 6.852313041687012
Epoch 710, val loss: 1.2981128692626953
Epoch 720, training loss: 0.012871397659182549 = 0.006027002353221178 + 0.001 * 6.844395637512207
Epoch 720, val loss: 1.3061904907226562
Epoch 730, training loss: 0.01261894591152668 = 0.005778321530669928 + 0.001 * 6.840623378753662
Epoch 730, val loss: 1.314145565032959
Epoch 740, training loss: 0.012393204495310783 = 0.005543856415897608 + 0.001 * 6.8493475914001465
Epoch 740, val loss: 1.321999430656433
Epoch 750, training loss: 0.012162541970610619 = 0.0053225550800561905 + 0.001 * 6.839986324310303
Epoch 750, val loss: 1.3297659158706665
Epoch 760, training loss: 0.011947374790906906 = 0.005113550927489996 + 0.001 * 6.833823204040527
Epoch 760, val loss: 1.3374300003051758
Epoch 770, training loss: 0.011758271604776382 = 0.004916145466268063 + 0.001 * 6.84212589263916
Epoch 770, val loss: 1.344971776008606
Epoch 780, training loss: 0.011560889892280102 = 0.0047300346195697784 + 0.001 * 6.830854892730713
Epoch 780, val loss: 1.3524105548858643
Epoch 790, training loss: 0.01141805574297905 = 0.004554263781756163 + 0.001 * 6.863790988922119
Epoch 790, val loss: 1.3597251176834106
Epoch 800, training loss: 0.01121149118989706 = 0.004388557747006416 + 0.001 * 6.822933197021484
Epoch 800, val loss: 1.3669066429138184
Epoch 810, training loss: 0.011067300103604794 = 0.004231935832649469 + 0.001 * 6.835363864898682
Epoch 810, val loss: 1.37396240234375
Epoch 820, training loss: 0.010915987193584442 = 0.004083527252078056 + 0.001 * 6.832458972930908
Epoch 820, val loss: 1.3809287548065186
Epoch 830, training loss: 0.010768096894025803 = 0.003941806498914957 + 0.001 * 6.826290607452393
Epoch 830, val loss: 1.3878271579742432
Epoch 840, training loss: 0.01062985509634018 = 0.0038056063931435347 + 0.001 * 6.824248790740967
Epoch 840, val loss: 1.3947646617889404
Epoch 850, training loss: 0.010495845228433609 = 0.003674290142953396 + 0.001 * 6.821555137634277
Epoch 850, val loss: 1.4017200469970703
Epoch 860, training loss: 0.01037245336920023 = 0.0035476533230394125 + 0.001 * 6.82480001449585
Epoch 860, val loss: 1.4086322784423828
Epoch 870, training loss: 0.01024707779288292 = 0.0034260088577866554 + 0.001 * 6.821068286895752
Epoch 870, val loss: 1.4155117273330688
Epoch 880, training loss: 0.010168618522584438 = 0.0033092787489295006 + 0.001 * 6.859339237213135
Epoch 880, val loss: 1.4222770929336548
Epoch 890, training loss: 0.010010266676545143 = 0.0031976401805877686 + 0.001 * 6.812625885009766
Epoch 890, val loss: 1.4289344549179077
Epoch 900, training loss: 0.009911209344863892 = 0.00309092178940773 + 0.001 * 6.820287704467773
Epoch 900, val loss: 1.435426950454712
Epoch 910, training loss: 0.009821140207350254 = 0.0029889389406889677 + 0.001 * 6.832200527191162
Epoch 910, val loss: 1.4418590068817139
Epoch 920, training loss: 0.00969462376087904 = 0.002891611773520708 + 0.001 * 6.803011894226074
Epoch 920, val loss: 1.4481332302093506
Epoch 930, training loss: 0.00961783155798912 = 0.0027987610083073378 + 0.001 * 6.819070339202881
Epoch 930, val loss: 1.4542474746704102
Epoch 940, training loss: 0.0095085883513093 = 0.0027100241277366877 + 0.001 * 6.798563480377197
Epoch 940, val loss: 1.4603115320205688
Epoch 950, training loss: 0.009455259889364243 = 0.0026253079995512962 + 0.001 * 6.829951763153076
Epoch 950, val loss: 1.4661577939987183
Epoch 960, training loss: 0.009345753118395805 = 0.0025451488327234983 + 0.001 * 6.800603866577148
Epoch 960, val loss: 1.4718629121780396
Epoch 970, training loss: 0.009288647212088108 = 0.0024688132107257843 + 0.001 * 6.819833755493164
Epoch 970, val loss: 1.4774723052978516
Epoch 980, training loss: 0.009186195209622383 = 0.002396289724856615 + 0.001 * 6.789905548095703
Epoch 980, val loss: 1.483048915863037
Epoch 990, training loss: 0.009111635386943817 = 0.0023272731341421604 + 0.001 * 6.784361839294434
Epoch 990, val loss: 1.4884599447250366
Epoch 1000, training loss: 0.009049567393958569 = 0.002261528978124261 + 0.001 * 6.7880377769470215
Epoch 1000, val loss: 1.4938397407531738
Epoch 1010, training loss: 0.008989039808511734 = 0.0021988465450704098 + 0.001 * 6.790193557739258
Epoch 1010, val loss: 1.4990606307983398
Epoch 1020, training loss: 0.008914923295378685 = 0.0021392279304564 + 0.001 * 6.775695323944092
Epoch 1020, val loss: 1.5042146444320679
Epoch 1030, training loss: 0.008865034207701683 = 0.0020824186503887177 + 0.001 * 6.7826151847839355
Epoch 1030, val loss: 1.5091999769210815
Epoch 1040, training loss: 0.008805939927697182 = 0.002028216840699315 + 0.001 * 6.7777228355407715
Epoch 1040, val loss: 1.5140037536621094
Epoch 1050, training loss: 0.008768811821937561 = 0.0019766041077673435 + 0.001 * 6.792207717895508
Epoch 1050, val loss: 1.518776535987854
Epoch 1060, training loss: 0.008705012500286102 = 0.0019273762591183186 + 0.001 * 6.77763557434082
Epoch 1060, val loss: 1.5234419107437134
Epoch 1070, training loss: 0.008650120347738266 = 0.001880394178442657 + 0.001 * 6.769725799560547
Epoch 1070, val loss: 1.527977705001831
Epoch 1080, training loss: 0.008623307570815086 = 0.001835489645600319 + 0.001 * 6.787817001342773
Epoch 1080, val loss: 1.5324528217315674
Epoch 1090, training loss: 0.008563779294490814 = 0.001792597584426403 + 0.001 * 6.771181583404541
Epoch 1090, val loss: 1.5368212461471558
Epoch 1100, training loss: 0.008516008034348488 = 0.0017515436047688127 + 0.001 * 6.764463901519775
Epoch 1100, val loss: 1.5410951375961304
Epoch 1110, training loss: 0.008466050960123539 = 0.0017122470308095217 + 0.001 * 6.753803253173828
Epoch 1110, val loss: 1.5453039407730103
Epoch 1120, training loss: 0.008455530740320683 = 0.0016746696783229709 + 0.001 * 6.780860424041748
Epoch 1120, val loss: 1.5494283437728882
Epoch 1130, training loss: 0.008387306705117226 = 0.0016387132927775383 + 0.001 * 6.748593330383301
Epoch 1130, val loss: 1.5533872842788696
Epoch 1140, training loss: 0.008351273834705353 = 0.0016042374772951007 + 0.001 * 6.747036457061768
Epoch 1140, val loss: 1.557329773902893
Epoch 1150, training loss: 0.00833301804959774 = 0.001571204513311386 + 0.001 * 6.761813640594482
Epoch 1150, val loss: 1.5611804723739624
Epoch 1160, training loss: 0.008285855874419212 = 0.001539521967060864 + 0.001 * 6.746333599090576
Epoch 1160, val loss: 1.5649455785751343
Epoch 1170, training loss: 0.008248670026659966 = 0.001509140245616436 + 0.001 * 6.739529609680176
Epoch 1170, val loss: 1.568644642829895
Epoch 1180, training loss: 0.008265942335128784 = 0.001479964586906135 + 0.001 * 6.785977363586426
Epoch 1180, val loss: 1.572212815284729
Epoch 1190, training loss: 0.008211001753807068 = 0.0014519430696964264 + 0.001 * 6.759058475494385
Epoch 1190, val loss: 1.5757942199707031
Epoch 1200, training loss: 0.008157809264957905 = 0.0014249903615564108 + 0.001 * 6.732818603515625
Epoch 1200, val loss: 1.5792285203933716
Epoch 1210, training loss: 0.00813845545053482 = 0.0013990603620186448 + 0.001 * 6.739394187927246
Epoch 1210, val loss: 1.5826295614242554
Epoch 1220, training loss: 0.008148792199790478 = 0.001374132465571165 + 0.001 * 6.774659633636475
Epoch 1220, val loss: 1.585963249206543
Epoch 1230, training loss: 0.008104398846626282 = 0.0013501817593351007 + 0.001 * 6.754216194152832
Epoch 1230, val loss: 1.5891516208648682
Epoch 1240, training loss: 0.008062046021223068 = 0.0013271237257868052 + 0.001 * 6.734921932220459
Epoch 1240, val loss: 1.5923603773117065
Epoch 1250, training loss: 0.008024311624467373 = 0.0013049273984506726 + 0.001 * 6.719383716583252
Epoch 1250, val loss: 1.5954840183258057
Epoch 1260, training loss: 0.008029470220208168 = 0.0012835522647947073 + 0.001 * 6.745917797088623
Epoch 1260, val loss: 1.5984511375427246
Epoch 1270, training loss: 0.008001618087291718 = 0.0012629562988877296 + 0.001 * 6.738661289215088
Epoch 1270, val loss: 1.601482629776001
Epoch 1280, training loss: 0.007962078787386417 = 0.0012431388022378087 + 0.001 * 6.718939781188965
Epoch 1280, val loss: 1.6043815612792969
Epoch 1290, training loss: 0.007940211333334446 = 0.0012239889474585652 + 0.001 * 6.716222286224365
Epoch 1290, val loss: 1.6072826385498047
Epoch 1300, training loss: 0.007948658429086208 = 0.0012055339757353067 + 0.001 * 6.743124008178711
Epoch 1300, val loss: 1.6101073026657104
Epoch 1310, training loss: 0.007920295931398869 = 0.0011877086944878101 + 0.001 * 6.732586860656738
Epoch 1310, val loss: 1.6128755807876587
Epoch 1320, training loss: 0.00788611825555563 = 0.0011705050710588694 + 0.001 * 6.715612888336182
Epoch 1320, val loss: 1.6155773401260376
Epoch 1330, training loss: 0.007878255099058151 = 0.0011538774706423283 + 0.001 * 6.724376678466797
Epoch 1330, val loss: 1.6182754039764404
Epoch 1340, training loss: 0.00785499345511198 = 0.0011378393974155188 + 0.001 * 6.717153549194336
Epoch 1340, val loss: 1.6208409070968628
Epoch 1350, training loss: 0.007832630537450314 = 0.0011223579058423638 + 0.001 * 6.710271835327148
Epoch 1350, val loss: 1.6234227418899536
Epoch 1360, training loss: 0.007805435918271542 = 0.0011074105277657509 + 0.001 * 6.698025226593018
Epoch 1360, val loss: 1.6258900165557861
Epoch 1370, training loss: 0.007837305776774883 = 0.0010929505806416273 + 0.001 * 6.744354724884033
Epoch 1370, val loss: 1.6283466815948486
Epoch 1380, training loss: 0.007781715132296085 = 0.0010789423249661922 + 0.001 * 6.702772617340088
Epoch 1380, val loss: 1.6307601928710938
Epoch 1390, training loss: 0.007763097994029522 = 0.0010653778444975615 + 0.001 * 6.697719573974609
Epoch 1390, val loss: 1.6330937147140503
Epoch 1400, training loss: 0.007780640851706266 = 0.0010522542288526893 + 0.001 * 6.728386402130127
Epoch 1400, val loss: 1.6353522539138794
Epoch 1410, training loss: 0.007741300389170647 = 0.0010395345743745565 + 0.001 * 6.701765537261963
Epoch 1410, val loss: 1.6376644372940063
Epoch 1420, training loss: 0.007706666365265846 = 0.0010271993232890964 + 0.001 * 6.679466724395752
Epoch 1420, val loss: 1.6398969888687134
Epoch 1430, training loss: 0.0077241044491529465 = 0.0010152804898098111 + 0.001 * 6.7088236808776855
Epoch 1430, val loss: 1.642069697380066
Epoch 1440, training loss: 0.007681982591748238 = 0.0010037546744570136 + 0.001 * 6.678227424621582
Epoch 1440, val loss: 1.6441771984100342
Epoch 1450, training loss: 0.007664437871426344 = 0.0009925584308803082 + 0.001 * 6.671879291534424
Epoch 1450, val loss: 1.6462762355804443
Epoch 1460, training loss: 0.007683440111577511 = 0.0009816880337893963 + 0.001 * 6.701751708984375
Epoch 1460, val loss: 1.648334264755249
Epoch 1470, training loss: 0.00764872319996357 = 0.0009711610618978739 + 0.001 * 6.677562236785889
Epoch 1470, val loss: 1.6503106355667114
Epoch 1480, training loss: 0.007620498072355986 = 0.0009609478292986751 + 0.001 * 6.659549713134766
Epoch 1480, val loss: 1.6523011922836304
Epoch 1490, training loss: 0.0076073650270700455 = 0.0009510247618891299 + 0.001 * 6.6563401222229
Epoch 1490, val loss: 1.6541774272918701
Epoch 1500, training loss: 0.007617310620844364 = 0.0009413871448487043 + 0.001 * 6.6759233474731445
Epoch 1500, val loss: 1.6561108827590942
Epoch 1510, training loss: 0.007631293963640928 = 0.0009320252574980259 + 0.001 * 6.699268341064453
Epoch 1510, val loss: 1.6579855680465698
Epoch 1520, training loss: 0.007609441410750151 = 0.0009229297866113484 + 0.001 * 6.686511039733887
Epoch 1520, val loss: 1.6597249507904053
Epoch 1530, training loss: 0.0075553716160357 = 0.0009140807087533176 + 0.001 * 6.641290664672852
Epoch 1530, val loss: 1.6615585088729858
Epoch 1540, training loss: 0.00757508585229516 = 0.0009055035770870745 + 0.001 * 6.669581890106201
Epoch 1540, val loss: 1.6632635593414307
Epoch 1550, training loss: 0.007610729895532131 = 0.0008971559582278132 + 0.001 * 6.713573455810547
Epoch 1550, val loss: 1.6650052070617676
Epoch 1560, training loss: 0.007530446629971266 = 0.0008890467579476535 + 0.001 * 6.64139986038208
Epoch 1560, val loss: 1.6667510271072388
Epoch 1570, training loss: 0.00752003351226449 = 0.0008811381994746625 + 0.001 * 6.638895034790039
Epoch 1570, val loss: 1.6683789491653442
Epoch 1580, training loss: 0.007516848389059305 = 0.0008734387229196727 + 0.001 * 6.643409729003906
Epoch 1580, val loss: 1.669966220855713
Epoch 1590, training loss: 0.007506164722144604 = 0.0008659638115204871 + 0.001 * 6.640200614929199
Epoch 1590, val loss: 1.6716034412384033
Epoch 1600, training loss: 0.007531939074397087 = 0.0008586790063418448 + 0.001 * 6.673259735107422
Epoch 1600, val loss: 1.6731722354888916
Epoch 1610, training loss: 0.0075265709310770035 = 0.000851594319101423 + 0.001 * 6.674976348876953
Epoch 1610, val loss: 1.674663782119751
Epoch 1620, training loss: 0.007471841759979725 = 0.0008447009022347629 + 0.001 * 6.627140522003174
Epoch 1620, val loss: 1.6761800050735474
Epoch 1630, training loss: 0.007465587928891182 = 0.0008379762875847518 + 0.001 * 6.6276116371154785
Epoch 1630, val loss: 1.6776361465454102
Epoch 1640, training loss: 0.007439315319061279 = 0.0008314095321111381 + 0.001 * 6.607905864715576
Epoch 1640, val loss: 1.6790934801101685
Epoch 1650, training loss: 0.0074434103444218636 = 0.0008250067476183176 + 0.001 * 6.61840295791626
Epoch 1650, val loss: 1.68048894405365
Epoch 1660, training loss: 0.007490723393857479 = 0.0008187704952433705 + 0.001 * 6.671952724456787
Epoch 1660, val loss: 1.6819499731063843
Epoch 1670, training loss: 0.0074534029699862 = 0.0008126969332806766 + 0.001 * 6.6407060623168945
Epoch 1670, val loss: 1.6832538843154907
Epoch 1680, training loss: 0.007425256073474884 = 0.0008067588205449283 + 0.001 * 6.618496894836426
Epoch 1680, val loss: 1.6846442222595215
Epoch 1690, training loss: 0.007423678878694773 = 0.0008009676821529865 + 0.001 * 6.622710704803467
Epoch 1690, val loss: 1.6859380006790161
Epoch 1700, training loss: 0.007445977535098791 = 0.0007953165331855416 + 0.001 * 6.650660991668701
Epoch 1700, val loss: 1.6872422695159912
Epoch 1710, training loss: 0.007413876708596945 = 0.0007898317999206483 + 0.001 * 6.624044418334961
Epoch 1710, val loss: 1.688511848449707
Epoch 1720, training loss: 0.007386413402855396 = 0.0007844691281206906 + 0.001 * 6.6019439697265625
Epoch 1720, val loss: 1.6897737979888916
Epoch 1730, training loss: 0.0074207112193107605 = 0.0007792350952513516 + 0.001 * 6.641475677490234
Epoch 1730, val loss: 1.6909645795822144
Epoch 1740, training loss: 0.007381654344499111 = 0.0007741275476291776 + 0.001 * 6.607526779174805
Epoch 1740, val loss: 1.6921817064285278
Epoch 1750, training loss: 0.007387923542410135 = 0.0007691344362683594 + 0.001 * 6.618788719177246
Epoch 1750, val loss: 1.6933284997940063
Epoch 1760, training loss: 0.007358181290328503 = 0.0007642529672011733 + 0.001 * 6.59392786026001
Epoch 1760, val loss: 1.6945260763168335
Epoch 1770, training loss: 0.007361489348113537 = 0.000759490707423538 + 0.001 * 6.601998329162598
Epoch 1770, val loss: 1.6956226825714111
Epoch 1780, training loss: 0.007388309109956026 = 0.0007548441062681377 + 0.001 * 6.633464813232422
Epoch 1780, val loss: 1.6967236995697021
Epoch 1790, training loss: 0.007358137983828783 = 0.0007502925000153482 + 0.001 * 6.607845306396484
Epoch 1790, val loss: 1.6978626251220703
Epoch 1800, training loss: 0.0073487102054059505 = 0.0007458521286025643 + 0.001 * 6.602858066558838
Epoch 1800, val loss: 1.6988941431045532
Epoch 1810, training loss: 0.0073292250744998455 = 0.0007414956344291568 + 0.001 * 6.587728977203369
Epoch 1810, val loss: 1.6999685764312744
Epoch 1820, training loss: 0.007343374192714691 = 0.0007372384425252676 + 0.001 * 6.606135368347168
Epoch 1820, val loss: 1.7009671926498413
Epoch 1830, training loss: 0.0073571293614804745 = 0.0007330722874030471 + 0.001 * 6.624056816101074
Epoch 1830, val loss: 1.702040672302246
Epoch 1840, training loss: 0.007333014160394669 = 0.0007289883797056973 + 0.001 * 6.604025363922119
Epoch 1840, val loss: 1.7029786109924316
Epoch 1850, training loss: 0.007305522449314594 = 0.0007249782793223858 + 0.001 * 6.5805439949035645
Epoch 1850, val loss: 1.703992486000061
Epoch 1860, training loss: 0.0073260776698589325 = 0.0007210578769445419 + 0.001 * 6.605019569396973
Epoch 1860, val loss: 1.7049347162246704
Epoch 1870, training loss: 0.007355312816798687 = 0.0007172214682213962 + 0.001 * 6.638091087341309
Epoch 1870, val loss: 1.7059237957000732
Epoch 1880, training loss: 0.0073026674799621105 = 0.0007134650950320065 + 0.001 * 6.589201927185059
Epoch 1880, val loss: 1.7067885398864746
Epoch 1890, training loss: 0.007281026802957058 = 0.0007097997004166245 + 0.001 * 6.571227073669434
Epoch 1890, val loss: 1.7077445983886719
Epoch 1900, training loss: 0.00726980809122324 = 0.0007061993237584829 + 0.001 * 6.563608646392822
Epoch 1900, val loss: 1.708593487739563
Epoch 1910, training loss: 0.007314295507967472 = 0.0007026819512248039 + 0.001 * 6.6116132736206055
Epoch 1910, val loss: 1.7094639539718628
Epoch 1920, training loss: 0.007282031234353781 = 0.000699235126376152 + 0.001 * 6.5827956199646
Epoch 1920, val loss: 1.7103660106658936
Epoch 1930, training loss: 0.007265348918735981 = 0.0006958456360734999 + 0.001 * 6.569502830505371
Epoch 1930, val loss: 1.7111707925796509
Epoch 1940, training loss: 0.00725822988897562 = 0.0006925362977199256 + 0.001 * 6.565693378448486
Epoch 1940, val loss: 1.7120029926300049
Epoch 1950, training loss: 0.007256157696247101 = 0.000689277658239007 + 0.001 * 6.566879749298096
Epoch 1950, val loss: 1.7128081321716309
Epoch 1960, training loss: 0.007299984339624643 = 0.0006860900321044028 + 0.001 * 6.613893985748291
Epoch 1960, val loss: 1.7136198282241821
Epoch 1970, training loss: 0.0072455937042832375 = 0.0006829514168202877 + 0.001 * 6.5626420974731445
Epoch 1970, val loss: 1.7144280672073364
Epoch 1980, training loss: 0.007242477964609861 = 0.000679885211866349 + 0.001 * 6.562592506408691
Epoch 1980, val loss: 1.7151826620101929
Epoch 1990, training loss: 0.0072405533865094185 = 0.0006768698804080486 + 0.001 * 6.563683032989502
Epoch 1990, val loss: 1.7159596681594849
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 0.5314
Flip ASR: 0.4489/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9582616090774536 = 1.9498876333236694 + 0.001 * 8.373922348022461
Epoch 0, val loss: 1.9505234956741333
Epoch 10, training loss: 1.9481617212295532 = 1.9397878646850586 + 0.001 * 8.373870849609375
Epoch 10, val loss: 1.9404970407485962
Epoch 20, training loss: 1.9358649253845215 = 1.927491307258606 + 0.001 * 8.373673439025879
Epoch 20, val loss: 1.927789568901062
Epoch 30, training loss: 1.918906807899475 = 1.9105336666107178 + 0.001 * 8.373194694519043
Epoch 30, val loss: 1.9098379611968994
Epoch 40, training loss: 1.89405357837677 = 1.8856816291809082 + 0.001 * 8.371984481811523
Epoch 40, val loss: 1.8834452629089355
Epoch 50, training loss: 1.8583256006240845 = 1.8499572277069092 + 0.001 * 8.368356704711914
Epoch 50, val loss: 1.8465007543563843
Epoch 60, training loss: 1.8139533996582031 = 1.8055998086929321 + 0.001 * 8.353645324707031
Epoch 60, val loss: 1.8039042949676514
Epoch 70, training loss: 1.7708935737609863 = 1.7626159191131592 + 0.001 * 8.27765941619873
Epoch 70, val loss: 1.7668756246566772
Epoch 80, training loss: 1.72219717502594 = 1.7143241167068481 + 0.001 * 7.873080253601074
Epoch 80, val loss: 1.7253385782241821
Epoch 90, training loss: 1.6546939611434937 = 1.6469781398773193 + 0.001 * 7.715869426727295
Epoch 90, val loss: 1.6669186353683472
Epoch 100, training loss: 1.5651640892028809 = 1.5575907230377197 + 0.001 * 7.5733795166015625
Epoch 100, val loss: 1.591128945350647
Epoch 110, training loss: 1.4581917524337769 = 1.4507781267166138 + 0.001 * 7.413634300231934
Epoch 110, val loss: 1.5019688606262207
Epoch 120, training loss: 1.3466212749481201 = 1.3393745422363281 + 0.001 * 7.246685028076172
Epoch 120, val loss: 1.411331295967102
Epoch 130, training loss: 1.2391996383666992 = 1.2320371866226196 + 0.001 * 7.16242790222168
Epoch 130, val loss: 1.328094482421875
Epoch 140, training loss: 1.1395553350448608 = 1.1324611902236938 + 0.001 * 7.094183444976807
Epoch 140, val loss: 1.2542717456817627
Epoch 150, training loss: 1.0488618612289429 = 1.041797399520874 + 0.001 * 7.0644097328186035
Epoch 150, val loss: 1.1878024339675903
Epoch 160, training loss: 0.966468334197998 = 0.9594197869300842 + 0.001 * 7.048520565032959
Epoch 160, val loss: 1.1275523900985718
Epoch 170, training loss: 0.8907816410064697 = 0.8837466835975647 + 0.001 * 7.034975528717041
Epoch 170, val loss: 1.0720523595809937
Epoch 180, training loss: 0.8201717734336853 = 0.8131508827209473 + 0.001 * 7.020905494689941
Epoch 180, val loss: 1.019863486289978
Epoch 190, training loss: 0.7540335655212402 = 0.7470338344573975 + 0.001 * 6.999758243560791
Epoch 190, val loss: 0.9710044860839844
Epoch 200, training loss: 0.6928935647010803 = 0.6859235763549805 + 0.001 * 6.970000267028809
Epoch 200, val loss: 0.9267392754554749
Epoch 210, training loss: 0.6371838450431824 = 0.6302510499954224 + 0.001 * 6.9328083992004395
Epoch 210, val loss: 0.8884326815605164
Epoch 220, training loss: 0.5861279368400574 = 0.5792273283004761 + 0.001 * 6.900593280792236
Epoch 220, val loss: 0.8560173511505127
Epoch 230, training loss: 0.5381942987442017 = 0.5313107371330261 + 0.001 * 6.8835530281066895
Epoch 230, val loss: 0.828338623046875
Epoch 240, training loss: 0.4919826090335846 = 0.4851130247116089 + 0.001 * 6.869585037231445
Epoch 240, val loss: 0.8041536211967468
Epoch 250, training loss: 0.44700413942337036 = 0.4401412010192871 + 0.001 * 6.862951278686523
Epoch 250, val loss: 0.782519519329071
Epoch 260, training loss: 0.40341028571128845 = 0.396551251411438 + 0.001 * 6.859045028686523
Epoch 260, val loss: 0.762744128704071
Epoch 270, training loss: 0.36160457134246826 = 0.35474762320518494 + 0.001 * 6.856940269470215
Epoch 270, val loss: 0.7446749210357666
Epoch 280, training loss: 0.32179751992225647 = 0.3149416148662567 + 0.001 * 6.855910778045654
Epoch 280, val loss: 0.728244960308075
Epoch 290, training loss: 0.2842288613319397 = 0.27737343311309814 + 0.001 * 6.855429649353027
Epoch 290, val loss: 0.7139503359794617
Epoch 300, training loss: 0.24935518205165863 = 0.24249976873397827 + 0.001 * 6.855410099029541
Epoch 300, val loss: 0.702863335609436
Epoch 310, training loss: 0.21798545122146606 = 0.2111298143863678 + 0.001 * 6.855640411376953
Epoch 310, val loss: 0.6963727474212646
Epoch 320, training loss: 0.19047266244888306 = 0.18361656367778778 + 0.001 * 6.856105327606201
Epoch 320, val loss: 0.6943850517272949
Epoch 330, training loss: 0.16687145829200745 = 0.16001304984092712 + 0.001 * 6.858405590057373
Epoch 330, val loss: 0.6965203881263733
Epoch 340, training loss: 0.14702092111110687 = 0.14016376435756683 + 0.001 * 6.857151985168457
Epoch 340, val loss: 0.7025666832923889
Epoch 350, training loss: 0.1304357349872589 = 0.12357751280069351 + 0.001 * 6.858223915100098
Epoch 350, val loss: 0.7117098569869995
Epoch 360, training loss: 0.11645086109638214 = 0.10959193110466003 + 0.001 * 6.8589324951171875
Epoch 360, val loss: 0.7228966951370239
Epoch 370, training loss: 0.10452498495578766 = 0.09766599535942078 + 0.001 * 6.8589863777160645
Epoch 370, val loss: 0.7353248000144958
Epoch 380, training loss: 0.09425459057092667 = 0.08739510923624039 + 0.001 * 6.859481334686279
Epoch 380, val loss: 0.7486435770988464
Epoch 390, training loss: 0.08534607291221619 = 0.07848350703716278 + 0.001 * 6.8625664710998535
Epoch 390, val loss: 0.762525737285614
Epoch 400, training loss: 0.07755919545888901 = 0.07069893181324005 + 0.001 * 6.860266208648682
Epoch 400, val loss: 0.7766492962837219
Epoch 410, training loss: 0.07072984427213669 = 0.06386937946081161 + 0.001 * 6.860461711883545
Epoch 410, val loss: 0.7907583713531494
Epoch 420, training loss: 0.06471221148967743 = 0.05784684419631958 + 0.001 * 6.865370273590088
Epoch 420, val loss: 0.804835319519043
Epoch 430, training loss: 0.05937705188989639 = 0.05251626670360565 + 0.001 * 6.8607869148254395
Epoch 430, val loss: 0.8188424706459045
Epoch 440, training loss: 0.054644692689180374 = 0.047782812267541885 + 0.001 * 6.861880302429199
Epoch 440, val loss: 0.8326749205589294
Epoch 450, training loss: 0.05042795091867447 = 0.0435672327876091 + 0.001 * 6.860718250274658
Epoch 450, val loss: 0.8462573289871216
Epoch 460, training loss: 0.04666487127542496 = 0.03980286046862602 + 0.001 * 6.8620100021362305
Epoch 460, val loss: 0.8596076965332031
Epoch 470, training loss: 0.043297939002513885 = 0.03643712028861046 + 0.001 * 6.860820293426514
Epoch 470, val loss: 0.8726706504821777
Epoch 480, training loss: 0.040285516530275345 = 0.033425793051719666 + 0.001 * 6.859721660614014
Epoch 480, val loss: 0.8854249119758606
Epoch 490, training loss: 0.03758556395769119 = 0.03072614222764969 + 0.001 * 6.859421730041504
Epoch 490, val loss: 0.8978348970413208
Epoch 500, training loss: 0.03516674041748047 = 0.028304316103458405 + 0.001 * 6.862425804138184
Epoch 500, val loss: 0.9099491238594055
Epoch 510, training loss: 0.03298928216099739 = 0.02612837217748165 + 0.001 * 6.860908031463623
Epoch 510, val loss: 0.9217391610145569
Epoch 520, training loss: 0.03103134222328663 = 0.02417168579995632 + 0.001 * 6.859655857086182
Epoch 520, val loss: 0.9332066178321838
Epoch 530, training loss: 0.029266871511936188 = 0.02240908518433571 + 0.001 * 6.857786655426025
Epoch 530, val loss: 0.9443764090538025
Epoch 540, training loss: 0.02767530456185341 = 0.020818602293729782 + 0.001 * 6.856701374053955
Epoch 540, val loss: 0.9552346467971802
Epoch 550, training loss: 0.02623751387000084 = 0.019381482154130936 + 0.001 * 6.856030464172363
Epoch 550, val loss: 0.9657914042472839
Epoch 560, training loss: 0.024960752576589584 = 0.018080055713653564 + 0.001 * 6.880696773529053
Epoch 560, val loss: 0.97603839635849
Epoch 570, training loss: 0.023761942982673645 = 0.01689867116510868 + 0.001 * 6.863271713256836
Epoch 570, val loss: 0.9860386848449707
Epoch 580, training loss: 0.022679321467876434 = 0.015824859961867332 + 0.001 * 6.854461669921875
Epoch 580, val loss: 0.9957136511802673
Epoch 590, training loss: 0.021700792014598846 = 0.014847222715616226 + 0.001 * 6.853569507598877
Epoch 590, val loss: 1.0050833225250244
Epoch 600, training loss: 0.020808160305023193 = 0.013955336064100266 + 0.001 * 6.8528242111206055
Epoch 600, val loss: 1.014204502105713
Epoch 610, training loss: 0.019998490810394287 = 0.013139736838638783 + 0.001 * 6.858753204345703
Epoch 610, val loss: 1.023079752922058
Epoch 620, training loss: 0.01924569346010685 = 0.012392704375088215 + 0.001 * 6.852988243103027
Epoch 620, val loss: 1.0317168235778809
Epoch 630, training loss: 0.018557773903012276 = 0.011707169003784657 + 0.001 * 6.850604057312012
Epoch 630, val loss: 1.0401356220245361
Epoch 640, training loss: 0.017926957458257675 = 0.01107704360038042 + 0.001 * 6.849912643432617
Epoch 640, val loss: 1.0482914447784424
Epoch 650, training loss: 0.017354723066091537 = 0.010496629402041435 + 0.001 * 6.858093738555908
Epoch 650, val loss: 1.0562244653701782
Epoch 660, training loss: 0.016815071925520897 = 0.009961403906345367 + 0.001 * 6.853668212890625
Epoch 660, val loss: 1.063960313796997
Epoch 670, training loss: 0.016313374042510986 = 0.009466812014579773 + 0.001 * 6.846560955047607
Epoch 670, val loss: 1.0714884996414185
Epoch 680, training loss: 0.01585567556321621 = 0.009008962661027908 + 0.001 * 6.846712589263916
Epoch 680, val loss: 1.0788127183914185
Epoch 690, training loss: 0.015430748462677002 = 0.00858440063893795 + 0.001 * 6.846347332000732
Epoch 690, val loss: 1.0859358310699463
Epoch 700, training loss: 0.015044862404465675 = 0.008190074004232883 + 0.001 * 6.854788303375244
Epoch 700, val loss: 1.0928834676742554
Epoch 710, training loss: 0.014669190160930157 = 0.007823324762284756 + 0.001 * 6.845865249633789
Epoch 710, val loss: 1.0996301174163818
Epoch 720, training loss: 0.014324496500194073 = 0.007481711450964212 + 0.001 * 6.842784881591797
Epoch 720, val loss: 1.1062060594558716
Epoch 730, training loss: 0.014005925506353378 = 0.007163014262914658 + 0.001 * 6.8429107666015625
Epoch 730, val loss: 1.1126011610031128
Epoch 740, training loss: 0.013716787099838257 = 0.006865269970148802 + 0.001 * 6.851517200469971
Epoch 740, val loss: 1.1188409328460693
Epoch 750, training loss: 0.013430665247142315 = 0.006586694158613682 + 0.001 * 6.843970775604248
Epoch 750, val loss: 1.1249443292617798
Epoch 760, training loss: 0.01316363550722599 = 0.006325657479465008 + 0.001 * 6.837977886199951
Epoch 760, val loss: 1.1308904886245728
Epoch 770, training loss: 0.012925241142511368 = 0.006080770865082741 + 0.001 * 6.8444695472717285
Epoch 770, val loss: 1.136680006980896
Epoch 780, training loss: 0.012688366696238518 = 0.005850782617926598 + 0.001 * 6.837583541870117
Epoch 780, val loss: 1.1423518657684326
Epoch 790, training loss: 0.012472230941057205 = 0.005634570028632879 + 0.001 * 6.837660789489746
Epoch 790, val loss: 1.1478718519210815
Epoch 800, training loss: 0.012267272919416428 = 0.005431039724498987 + 0.001 * 6.836232662200928
Epoch 800, val loss: 1.1532658338546753
Epoch 810, training loss: 0.01208221260458231 = 0.005239240825176239 + 0.001 * 6.842971324920654
Epoch 810, val loss: 1.1585373878479004
Epoch 820, training loss: 0.011889670044183731 = 0.0050582787953317165 + 0.001 * 6.831390380859375
Epoch 820, val loss: 1.1636778116226196
Epoch 830, training loss: 0.011718245223164558 = 0.004887372720986605 + 0.001 * 6.83087158203125
Epoch 830, val loss: 1.1687260866165161
Epoch 840, training loss: 0.011560803279280663 = 0.004725803155452013 + 0.001 * 6.835000038146973
Epoch 840, val loss: 1.1736316680908203
Epoch 850, training loss: 0.011408346705138683 = 0.0045729270204901695 + 0.001 * 6.835419178009033
Epoch 850, val loss: 1.1784522533416748
Epoch 860, training loss: 0.011255492456257343 = 0.004428036976605654 + 0.001 * 6.827455043792725
Epoch 860, val loss: 1.1831799745559692
Epoch 870, training loss: 0.0111169945448637 = 0.004290679004043341 + 0.001 * 6.826314926147461
Epoch 870, val loss: 1.18778657913208
Epoch 880, training loss: 0.010998319834470749 = 0.004160333424806595 + 0.001 * 6.837985992431641
Epoch 880, val loss: 1.1922951936721802
Epoch 890, training loss: 0.010863164439797401 = 0.004036535508930683 + 0.001 * 6.826628684997559
Epoch 890, val loss: 1.196702241897583
Epoch 900, training loss: 0.010760603472590446 = 0.003918859176337719 + 0.001 * 6.841744422912598
Epoch 900, val loss: 1.2010117769241333
Epoch 910, training loss: 0.010627975687384605 = 0.0038069081492722034 + 0.001 * 6.821067810058594
Epoch 910, val loss: 1.2052420377731323
Epoch 920, training loss: 0.01052552368491888 = 0.003700328292325139 + 0.001 * 6.825194835662842
Epoch 920, val loss: 1.2093744277954102
Epoch 930, training loss: 0.01042657345533371 = 0.003598789917305112 + 0.001 * 6.827783107757568
Epoch 930, val loss: 1.213422179222107
Epoch 940, training loss: 0.010323507711291313 = 0.0035019717179238796 + 0.001 * 6.821535110473633
Epoch 940, val loss: 1.2173936367034912
Epoch 950, training loss: 0.010225391015410423 = 0.0034095942974090576 + 0.001 * 6.815796852111816
Epoch 950, val loss: 1.2212778329849243
Epoch 960, training loss: 0.010155211202800274 = 0.003321389202028513 + 0.001 * 6.833821773529053
Epoch 960, val loss: 1.2250720262527466
Epoch 970, training loss: 0.010051625780761242 = 0.003237105207517743 + 0.001 * 6.814520359039307
Epoch 970, val loss: 1.2288012504577637
Epoch 980, training loss: 0.009959903545677662 = 0.0031565201934427023 + 0.001 * 6.8033833503723145
Epoch 980, val loss: 1.2324442863464355
Epoch 990, training loss: 0.00990137830376625 = 0.003079422516748309 + 0.001 * 6.82195520401001
Epoch 990, val loss: 1.236021876335144
Epoch 1000, training loss: 0.009807627648115158 = 0.0030055951792746782 + 0.001 * 6.802032470703125
Epoch 1000, val loss: 1.2395342588424683
Epoch 1010, training loss: 0.009748192504048347 = 0.0029348875395953655 + 0.001 * 6.813304901123047
Epoch 1010, val loss: 1.2429721355438232
Epoch 1020, training loss: 0.009681143797934055 = 0.002867089817300439 + 0.001 * 6.814053535461426
Epoch 1020, val loss: 1.2463423013687134
Epoch 1030, training loss: 0.009606597945094109 = 0.002802060218527913 + 0.001 * 6.804537773132324
Epoch 1030, val loss: 1.2496395111083984
Epoch 1040, training loss: 0.009529047645628452 = 0.0027396799996495247 + 0.001 * 6.789367198944092
Epoch 1040, val loss: 1.2528818845748901
Epoch 1050, training loss: 0.009478178806602955 = 0.0026797852478921413 + 0.001 * 6.798393249511719
Epoch 1050, val loss: 1.2560515403747559
Epoch 1060, training loss: 0.00941507052630186 = 0.002622244181111455 + 0.001 * 6.792826175689697
Epoch 1060, val loss: 1.2591559886932373
Epoch 1070, training loss: 0.009362214244902134 = 0.0025669385213404894 + 0.001 * 6.7952752113342285
Epoch 1070, val loss: 1.2622212171554565
Epoch 1080, training loss: 0.009307154454290867 = 0.002513757674023509 + 0.001 * 6.79339599609375
Epoch 1080, val loss: 1.2652159929275513
Epoch 1090, training loss: 0.009257564321160316 = 0.0024625854566693306 + 0.001 * 6.794978618621826
Epoch 1090, val loss: 1.268161416053772
Epoch 1100, training loss: 0.009196017868816853 = 0.002413329901173711 + 0.001 * 6.782687187194824
Epoch 1100, val loss: 1.2710269689559937
Epoch 1110, training loss: 0.009159283712506294 = 0.0023658880963921547 + 0.001 * 6.793395042419434
Epoch 1110, val loss: 1.2738476991653442
Epoch 1120, training loss: 0.009106338024139404 = 0.0023201657459139824 + 0.001 * 6.7861714363098145
Epoch 1120, val loss: 1.2766392230987549
Epoch 1130, training loss: 0.009053822606801987 = 0.0022761039435863495 + 0.001 * 6.777719020843506
Epoch 1130, val loss: 1.279348611831665
Epoch 1140, training loss: 0.009016338735818863 = 0.002233622595667839 + 0.001 * 6.782715320587158
Epoch 1140, val loss: 1.2820161581039429
Epoch 1150, training loss: 0.008998400531709194 = 0.002192639745771885 + 0.001 * 6.805760383605957
Epoch 1150, val loss: 1.284651279449463
Epoch 1160, training loss: 0.00894024409353733 = 0.0021530890371650457 + 0.001 * 6.787154674530029
Epoch 1160, val loss: 1.2872059345245361
Epoch 1170, training loss: 0.008892146870493889 = 0.0021149180829524994 + 0.001 * 6.777228355407715
Epoch 1170, val loss: 1.2897412776947021
Epoch 1180, training loss: 0.00883474387228489 = 0.0020780465565621853 + 0.001 * 6.756696701049805
Epoch 1180, val loss: 1.2922227382659912
Epoch 1190, training loss: 0.008816955611109734 = 0.0020424379035830498 + 0.001 * 6.774517059326172
Epoch 1190, val loss: 1.2946549654006958
Epoch 1200, training loss: 0.008801449090242386 = 0.0020080143585801125 + 0.001 * 6.7934346199035645
Epoch 1200, val loss: 1.297041893005371
Epoch 1210, training loss: 0.008736119605600834 = 0.0019747240003198385 + 0.001 * 6.761395454406738
Epoch 1210, val loss: 1.2993841171264648
Epoch 1220, training loss: 0.008681444451212883 = 0.0019425267819315195 + 0.001 * 6.738917350769043
Epoch 1220, val loss: 1.3016836643218994
Epoch 1230, training loss: 0.008668208494782448 = 0.0019113807938992977 + 0.001 * 6.7568278312683105
Epoch 1230, val loss: 1.3039370775222778
Epoch 1240, training loss: 0.00868571363389492 = 0.001881236326880753 + 0.001 * 6.804476737976074
Epoch 1240, val loss: 1.3061432838439941
Epoch 1250, training loss: 0.008601494133472443 = 0.0018520451849326491 + 0.001 * 6.749448299407959
Epoch 1250, val loss: 1.3083215951919556
Epoch 1260, training loss: 0.008574243634939194 = 0.0018237786134704947 + 0.001 * 6.750464916229248
Epoch 1260, val loss: 1.3104603290557861
Epoch 1270, training loss: 0.008530565537512302 = 0.001796394819393754 + 0.001 * 6.734170436859131
Epoch 1270, val loss: 1.3125345706939697
Epoch 1280, training loss: 0.00851849652826786 = 0.001769852009601891 + 0.001 * 6.74864387512207
Epoch 1280, val loss: 1.3145873546600342
Epoch 1290, training loss: 0.008477441035211086 = 0.0017441225936636329 + 0.001 * 6.733318328857422
Epoch 1290, val loss: 1.3166073560714722
Epoch 1300, training loss: 0.008467914536595345 = 0.0017191899241879582 + 0.001 * 6.748724460601807
Epoch 1300, val loss: 1.3185763359069824
Epoch 1310, training loss: 0.008439723402261734 = 0.0016949865967035294 + 0.001 * 6.744736194610596
Epoch 1310, val loss: 1.3205127716064453
Epoch 1320, training loss: 0.008421179838478565 = 0.0016715048113837838 + 0.001 * 6.749674320220947
Epoch 1320, val loss: 1.322419285774231
Epoch 1330, training loss: 0.008416801691055298 = 0.0016487276880070567 + 0.001 * 6.768073558807373
Epoch 1330, val loss: 1.324292778968811
Epoch 1340, training loss: 0.008357865735888481 = 0.0016265932936221361 + 0.001 * 6.731272220611572
Epoch 1340, val loss: 1.326143503189087
Epoch 1350, training loss: 0.008338107727468014 = 0.0016051005804911256 + 0.001 * 6.733006954193115
Epoch 1350, val loss: 1.327936053276062
Epoch 1360, training loss: 0.00829080305993557 = 0.0015842256834730506 + 0.001 * 6.706577301025391
Epoch 1360, val loss: 1.3297070264816284
Epoch 1370, training loss: 0.008343551307916641 = 0.0015639574266970158 + 0.001 * 6.779593467712402
Epoch 1370, val loss: 1.3314568996429443
Epoch 1380, training loss: 0.00831165537238121 = 0.0015442533185705543 + 0.001 * 6.767402172088623
Epoch 1380, val loss: 1.3331494331359863
Epoch 1390, training loss: 0.008236280642449856 = 0.0015250869328156114 + 0.001 * 6.711193084716797
Epoch 1390, val loss: 1.334845781326294
Epoch 1400, training loss: 0.008208098821341991 = 0.0015064476756379008 + 0.001 * 6.701651096343994
Epoch 1400, val loss: 1.3364953994750977
Epoch 1410, training loss: 0.008196219801902771 = 0.0014883270487189293 + 0.001 * 6.707891941070557
Epoch 1410, val loss: 1.3381173610687256
Epoch 1420, training loss: 0.008222817443311214 = 0.0014707001391798258 + 0.001 * 6.752116680145264
Epoch 1420, val loss: 1.339721441268921
Epoch 1430, training loss: 0.008147045969963074 = 0.0014535363297909498 + 0.001 * 6.693509578704834
Epoch 1430, val loss: 1.3412806987762451
Epoch 1440, training loss: 0.008134490810334682 = 0.0014368387637659907 + 0.001 * 6.697651386260986
Epoch 1440, val loss: 1.3427984714508057
Epoch 1450, training loss: 0.008170251734554768 = 0.0014205824118107557 + 0.001 * 6.749668598175049
Epoch 1450, val loss: 1.3442994356155396
Epoch 1460, training loss: 0.00808252114802599 = 0.0014047372387722135 + 0.001 * 6.677783489227295
Epoch 1460, val loss: 1.3457947969436646
Epoch 1470, training loss: 0.008112311363220215 = 0.0013893266441300511 + 0.001 * 6.722984313964844
Epoch 1470, val loss: 1.3472490310668945
Epoch 1480, training loss: 0.008110555820167065 = 0.0013742989394813776 + 0.001 * 6.7362565994262695
Epoch 1480, val loss: 1.3486690521240234
Epoch 1490, training loss: 0.008081572130322456 = 0.0013596672797575593 + 0.001 * 6.721904277801514
Epoch 1490, val loss: 1.350083589553833
Epoch 1500, training loss: 0.008066890761256218 = 0.0013453918509185314 + 0.001 * 6.721498012542725
Epoch 1500, val loss: 1.3514515161514282
Epoch 1510, training loss: 0.008013674058020115 = 0.0013314764946699142 + 0.001 * 6.682197093963623
Epoch 1510, val loss: 1.3528133630752563
Epoch 1520, training loss: 0.007995215244591236 = 0.00131792516913265 + 0.001 * 6.6772894859313965
Epoch 1520, val loss: 1.35414457321167
Epoch 1530, training loss: 0.007995668798685074 = 0.0013046987587586045 + 0.001 * 6.690969467163086
Epoch 1530, val loss: 1.3554445505142212
Epoch 1540, training loss: 0.007982240058481693 = 0.0012918044812977314 + 0.001 * 6.690435409545898
Epoch 1540, val loss: 1.35673189163208
Epoch 1550, training loss: 0.007974138483405113 = 0.0012792127672582865 + 0.001 * 6.694925785064697
Epoch 1550, val loss: 1.3580071926116943
Epoch 1560, training loss: 0.007947615347802639 = 0.001266936305910349 + 0.001 * 6.680678844451904
Epoch 1560, val loss: 1.3592243194580078
Epoch 1570, training loss: 0.008008785545825958 = 0.0012549598468467593 + 0.001 * 6.7538251876831055
Epoch 1570, val loss: 1.3604369163513184
Epoch 1580, training loss: 0.007913301698863506 = 0.0012432477669790387 + 0.001 * 6.670053958892822
Epoch 1580, val loss: 1.361603856086731
Epoch 1590, training loss: 0.007895738817751408 = 0.0012318259105086327 + 0.001 * 6.663912773132324
Epoch 1590, val loss: 1.3627896308898926
Epoch 1600, training loss: 0.007905837148427963 = 0.0012206697138026357 + 0.001 * 6.68516731262207
Epoch 1600, val loss: 1.3639295101165771
Epoch 1610, training loss: 0.007889567874372005 = 0.0012097713770344853 + 0.001 * 6.679795742034912
Epoch 1610, val loss: 1.3650528192520142
Epoch 1620, training loss: 0.007880517281591892 = 0.0011991198407486081 + 0.001 * 6.681397438049316
Epoch 1620, val loss: 1.366153359413147
Epoch 1630, training loss: 0.007861115969717503 = 0.0011887145228683949 + 0.001 * 6.672400951385498
Epoch 1630, val loss: 1.367245078086853
Epoch 1640, training loss: 0.007835526019334793 = 0.0011785506503656507 + 0.001 * 6.656975269317627
Epoch 1640, val loss: 1.3683029413223267
Epoch 1650, training loss: 0.007808055728673935 = 0.001168599701486528 + 0.001 * 6.639455795288086
Epoch 1650, val loss: 1.369347095489502
Epoch 1660, training loss: 0.007820496335625648 = 0.001158880884759128 + 0.001 * 6.661614894866943
Epoch 1660, val loss: 1.370378017425537
Epoch 1670, training loss: 0.007836660370230675 = 0.001149384188465774 + 0.001 * 6.687276363372803
Epoch 1670, val loss: 1.3714009523391724
Epoch 1680, training loss: 0.007796434685587883 = 0.0011400878429412842 + 0.001 * 6.656346321105957
Epoch 1680, val loss: 1.3724050521850586
Epoch 1690, training loss: 0.007792370393872261 = 0.0011309963883832097 + 0.001 * 6.661373615264893
Epoch 1690, val loss: 1.3733700513839722
Epoch 1700, training loss: 0.007771301548928022 = 0.0011220959713682532 + 0.001 * 6.649205207824707
Epoch 1700, val loss: 1.374345302581787
Epoch 1710, training loss: 0.00774479191750288 = 0.0011133878724649549 + 0.001 * 6.631403923034668
Epoch 1710, val loss: 1.3752803802490234
Epoch 1720, training loss: 0.007749608717858791 = 0.0011048736050724983 + 0.001 * 6.644734859466553
Epoch 1720, val loss: 1.3761969804763794
Epoch 1730, training loss: 0.007745937444269657 = 0.0010965282563120127 + 0.001 * 6.64940881729126
Epoch 1730, val loss: 1.3771073818206787
Epoch 1740, training loss: 0.007750425487756729 = 0.0010883741779252887 + 0.001 * 6.662050724029541
Epoch 1740, val loss: 1.3780244588851929
Epoch 1750, training loss: 0.007743006572127342 = 0.0010803693439811468 + 0.001 * 6.662636756896973
Epoch 1750, val loss: 1.3788869380950928
Epoch 1760, training loss: 0.0077049401588737965 = 0.0010725414613261819 + 0.001 * 6.63239860534668
Epoch 1760, val loss: 1.379738211631775
Epoch 1770, training loss: 0.007724492810666561 = 0.0010648650350049138 + 0.001 * 6.659627437591553
Epoch 1770, val loss: 1.380590558052063
Epoch 1780, training loss: 0.007702343165874481 = 0.001057348563335836 + 0.001 * 6.644994258880615
Epoch 1780, val loss: 1.381409764289856
Epoch 1790, training loss: 0.0076843202114105225 = 0.0010499696945771575 + 0.001 * 6.634350299835205
Epoch 1790, val loss: 1.3822441101074219
Epoch 1800, training loss: 0.007702409289777279 = 0.0010427521774545312 + 0.001 * 6.659657001495361
Epoch 1800, val loss: 1.3830459117889404
Epoch 1810, training loss: 0.00771712139248848 = 0.0010356777347624302 + 0.001 * 6.681443214416504
Epoch 1810, val loss: 1.3838390111923218
Epoch 1820, training loss: 0.007643298711627722 = 0.0010287151671946049 + 0.001 * 6.6145830154418945
Epoch 1820, val loss: 1.3846182823181152
Epoch 1830, training loss: 0.007632210850715637 = 0.0010219101095572114 + 0.001 * 6.610300540924072
Epoch 1830, val loss: 1.385358214378357
Epoch 1840, training loss: 0.00762010645121336 = 0.0010152292670682073 + 0.001 * 6.60487699508667
Epoch 1840, val loss: 1.3861199617385864
Epoch 1850, training loss: 0.0076642814092338085 = 0.0010086818365380168 + 0.001 * 6.655599117279053
Epoch 1850, val loss: 1.3868402242660522
Epoch 1860, training loss: 0.007613463792949915 = 0.0010022566420957446 + 0.001 * 6.611206531524658
Epoch 1860, val loss: 1.3875755071640015
Epoch 1870, training loss: 0.0075882719829678535 = 0.0009959508897736669 + 0.001 * 6.592320919036865
Epoch 1870, val loss: 1.3882743120193481
Epoch 1880, training loss: 0.007622336503118277 = 0.0009897714480757713 + 0.001 * 6.632564544677734
Epoch 1880, val loss: 1.3889501094818115
Epoch 1890, training loss: 0.0076310341246426105 = 0.0009837127290666103 + 0.001 * 6.6473212242126465
Epoch 1890, val loss: 1.389657974243164
Epoch 1900, training loss: 0.007610214874148369 = 0.000977743649855256 + 0.001 * 6.632470607757568
Epoch 1900, val loss: 1.3903043270111084
Epoch 1910, training loss: 0.00754554383456707 = 0.0009718869114294648 + 0.001 * 6.5736565589904785
Epoch 1910, val loss: 1.3909809589385986
Epoch 1920, training loss: 0.007577519863843918 = 0.0009661466465331614 + 0.001 * 6.611372947692871
Epoch 1920, val loss: 1.3916412591934204
Epoch 1930, training loss: 0.0075771743431687355 = 0.0009605122613720596 + 0.001 * 6.61666202545166
Epoch 1930, val loss: 1.3922773599624634
Epoch 1940, training loss: 0.007559425663203001 = 0.0009549743845127523 + 0.001 * 6.604450702667236
Epoch 1940, val loss: 1.3928996324539185
Epoch 1950, training loss: 0.007531207520514727 = 0.000949520559515804 + 0.001 * 6.581686496734619
Epoch 1950, val loss: 1.393507719039917
Epoch 1960, training loss: 0.007569099776446819 = 0.0009441822185181081 + 0.001 * 6.624917030334473
Epoch 1960, val loss: 1.3941041231155396
Epoch 1970, training loss: 0.007566796615719795 = 0.0009389219339936972 + 0.001 * 6.627874374389648
Epoch 1970, val loss: 1.3947091102600098
Epoch 1980, training loss: 0.007505744695663452 = 0.0009337502997368574 + 0.001 * 6.571994304656982
Epoch 1980, val loss: 1.3952865600585938
Epoch 1990, training loss: 0.00751276221126318 = 0.0009286646964028478 + 0.001 * 6.584097385406494
Epoch 1990, val loss: 1.3958549499511719
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6605
Flip ASR: 0.6133/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9649518728256226 = 1.9565778970718384 + 0.001 * 8.373921394348145
Epoch 0, val loss: 1.9583107233047485
Epoch 10, training loss: 1.9536718130111694 = 1.9452979564666748 + 0.001 * 8.37386417388916
Epoch 10, val loss: 1.9474116563796997
Epoch 20, training loss: 1.9396286010742188 = 1.9312549829483032 + 0.001 * 8.373655319213867
Epoch 20, val loss: 1.9333763122558594
Epoch 30, training loss: 1.91987144947052 = 1.9114983081817627 + 0.001 * 8.3731050491333
Epoch 30, val loss: 1.913267731666565
Epoch 40, training loss: 1.8909575939178467 = 1.8825860023498535 + 0.001 * 8.371535301208496
Epoch 40, val loss: 1.8840903043746948
Epoch 50, training loss: 1.8518779277801514 = 1.843511700630188 + 0.001 * 8.36620807647705
Epoch 50, val loss: 1.84676992893219
Epoch 60, training loss: 1.8106508255004883 = 1.8023072481155396 + 0.001 * 8.343535423278809
Epoch 60, val loss: 1.8117167949676514
Epoch 70, training loss: 1.7751531600952148 = 1.766921877861023 + 0.001 * 8.231307029724121
Epoch 70, val loss: 1.7828702926635742
Epoch 80, training loss: 1.730502963066101 = 1.722650408744812 + 0.001 * 7.852601528167725
Epoch 80, val loss: 1.7439647912979126
Epoch 90, training loss: 1.670231819152832 = 1.6624103784561157 + 0.001 * 7.821465015411377
Epoch 90, val loss: 1.6930981874465942
Epoch 100, training loss: 1.591204285621643 = 1.583404302597046 + 0.001 * 7.7999653816223145
Epoch 100, val loss: 1.628182291984558
Epoch 110, training loss: 1.502944827079773 = 1.4951660633087158 + 0.001 * 7.778805255889893
Epoch 110, val loss: 1.5572335720062256
Epoch 120, training loss: 1.4183331727981567 = 1.4106007814407349 + 0.001 * 7.732376575469971
Epoch 120, val loss: 1.4919167757034302
Epoch 130, training loss: 1.338706135749817 = 1.3311182260513306 + 0.001 * 7.587893009185791
Epoch 130, val loss: 1.4326876401901245
Epoch 140, training loss: 1.2597742080688477 = 1.2524305582046509 + 0.001 * 7.343638896942139
Epoch 140, val loss: 1.3748817443847656
Epoch 150, training loss: 1.1786459684371948 = 1.1713557243347168 + 0.001 * 7.290213108062744
Epoch 150, val loss: 1.3150745630264282
Epoch 160, training loss: 1.0942351818084717 = 1.0870414972305298 + 0.001 * 7.193697929382324
Epoch 160, val loss: 1.2536433935165405
Epoch 170, training loss: 1.0070555210113525 = 0.9999449849128723 + 0.001 * 7.110507488250732
Epoch 170, val loss: 1.1909725666046143
Epoch 180, training loss: 0.9190784692764282 = 0.9120109677314758 + 0.001 * 7.067523956298828
Epoch 180, val loss: 1.1288491487503052
Epoch 190, training loss: 0.8338721990585327 = 0.8268197178840637 + 0.001 * 7.052485466003418
Epoch 190, val loss: 1.070265293121338
Epoch 200, training loss: 0.7547464370727539 = 0.7477105855941772 + 0.001 * 7.035855293273926
Epoch 200, val loss: 1.018614649772644
Epoch 210, training loss: 0.6828479170799255 = 0.6758334040641785 + 0.001 * 7.014507293701172
Epoch 210, val loss: 0.9750310778617859
Epoch 220, training loss: 0.6176358461380005 = 0.6106468439102173 + 0.001 * 6.988988876342773
Epoch 220, val loss: 0.9391911625862122
Epoch 230, training loss: 0.5584083795547485 = 0.551438570022583 + 0.001 * 6.969817638397217
Epoch 230, val loss: 0.9098864197731018
Epoch 240, training loss: 0.50498366355896 = 0.49801743030548096 + 0.001 * 6.966243267059326
Epoch 240, val loss: 0.8863345980644226
Epoch 250, training loss: 0.45716360211372375 = 0.4502037763595581 + 0.001 * 6.959839820861816
Epoch 250, val loss: 0.8681228756904602
Epoch 260, training loss: 0.4142557978630066 = 0.4072968363761902 + 0.001 * 6.958949089050293
Epoch 260, val loss: 0.854782223701477
Epoch 270, training loss: 0.37506768107414246 = 0.36810827255249023 + 0.001 * 6.959412097930908
Epoch 270, val loss: 0.8454530835151672
Epoch 280, training loss: 0.3383873999118805 = 0.3314269185066223 + 0.001 * 6.960484981536865
Epoch 280, val loss: 0.8388586044311523
Epoch 290, training loss: 0.3034287095069885 = 0.2964670658111572 + 0.001 * 6.961636543273926
Epoch 290, val loss: 0.8341729044914246
Epoch 300, training loss: 0.270109623670578 = 0.2631467878818512 + 0.001 * 6.962821960449219
Epoch 300, val loss: 0.8308866620063782
Epoch 310, training loss: 0.23884466290473938 = 0.231880784034729 + 0.001 * 6.963875770568848
Epoch 310, val loss: 0.8291816115379333
Epoch 320, training loss: 0.2102074772119522 = 0.20324277877807617 + 0.001 * 6.964693546295166
Epoch 320, val loss: 0.8294832110404968
Epoch 330, training loss: 0.18460844457149506 = 0.1776433140039444 + 0.001 * 6.965128421783447
Epoch 330, val loss: 0.8321255445480347
Epoch 340, training loss: 0.16214114427566528 = 0.15517613291740417 + 0.001 * 6.965010166168213
Epoch 340, val loss: 0.8369019627571106
Epoch 350, training loss: 0.14266157150268555 = 0.13568784296512604 + 0.001 * 6.973727703094482
Epoch 350, val loss: 0.8436627984046936
Epoch 360, training loss: 0.1258523166179657 = 0.1188865602016449 + 0.001 * 6.965756416320801
Epoch 360, val loss: 0.8520953059196472
Epoch 370, training loss: 0.11139772832393646 = 0.10443517565727234 + 0.001 * 6.962551593780518
Epoch 370, val loss: 0.8617744445800781
Epoch 380, training loss: 0.09896419942378998 = 0.0920054093003273 + 0.001 * 6.9587931632995605
Epoch 380, val loss: 0.8724700212478638
Epoch 390, training loss: 0.08825855702161789 = 0.08130396902561188 + 0.001 * 6.954586982727051
Epoch 390, val loss: 0.8838356137275696
Epoch 400, training loss: 0.07906560599803925 = 0.07208240032196045 + 0.001 * 6.9832072257995605
Epoch 400, val loss: 0.895672082901001
Epoch 410, training loss: 0.07107827812433243 = 0.06413023173809052 + 0.001 * 6.948044776916504
Epoch 410, val loss: 0.9079169631004333
Epoch 420, training loss: 0.06420847028493881 = 0.05726579204201698 + 0.001 * 6.942678928375244
Epoch 420, val loss: 0.9203696846961975
Epoch 430, training loss: 0.05826404690742493 = 0.05132992938160896 + 0.001 * 6.934117317199707
Epoch 430, val loss: 0.9329206943511963
Epoch 440, training loss: 0.053084563463926315 = 0.04615172743797302 + 0.001 * 6.932834625244141
Epoch 440, val loss: 0.9454932808876038
Epoch 450, training loss: 0.04857860505580902 = 0.04165961220860481 + 0.001 * 6.91899299621582
Epoch 450, val loss: 0.9581520557403564
Epoch 460, training loss: 0.04465315490961075 = 0.03773518279194832 + 0.001 * 6.917970180511475
Epoch 460, val loss: 0.9709674715995789
Epoch 470, training loss: 0.04121292755007744 = 0.03429959714412689 + 0.001 * 6.913328647613525
Epoch 470, val loss: 0.9833739995956421
Epoch 480, training loss: 0.03818676993250847 = 0.03128199651837349 + 0.001 * 6.904772758483887
Epoch 480, val loss: 0.9956418871879578
Epoch 490, training loss: 0.03552614152431488 = 0.0286233089864254 + 0.001 * 6.902831077575684
Epoch 490, val loss: 1.0077513456344604
Epoch 500, training loss: 0.033173345029354095 = 0.026272762566804886 + 0.001 * 6.900582313537598
Epoch 500, val loss: 1.0195902585983276
Epoch 510, training loss: 0.031086083501577377 = 0.0241873599588871 + 0.001 * 6.898722171783447
Epoch 510, val loss: 1.0311485528945923
Epoch 520, training loss: 0.029230203479528427 = 0.0223313607275486 + 0.001 * 6.8988423347473145
Epoch 520, val loss: 1.0424580574035645
Epoch 530, training loss: 0.027575816959142685 = 0.020674102008342743 + 0.001 * 6.901715278625488
Epoch 530, val loss: 1.053508996963501
Epoch 540, training loss: 0.02608700841665268 = 0.019189558923244476 + 0.001 * 6.897448539733887
Epoch 540, val loss: 1.0643113851547241
Epoch 550, training loss: 0.024754434823989868 = 0.017855552956461906 + 0.001 * 6.898880958557129
Epoch 550, val loss: 1.0748529434204102
Epoch 560, training loss: 0.023548616096377373 = 0.01665329746901989 + 0.001 * 6.895318031311035
Epoch 560, val loss: 1.0851222276687622
Epoch 570, training loss: 0.022461475804448128 = 0.015566647984087467 + 0.001 * 6.894827842712402
Epoch 570, val loss: 1.0951156616210938
Epoch 580, training loss: 0.02147393859922886 = 0.014581538736820221 + 0.001 * 6.892399311065674
Epoch 580, val loss: 1.1048833131790161
Epoch 590, training loss: 0.02057851105928421 = 0.013686232268810272 + 0.001 * 6.892277717590332
Epoch 590, val loss: 1.1143611669540405
Epoch 600, training loss: 0.019760798662900925 = 0.012870607897639275 + 0.001 * 6.890190124511719
Epoch 600, val loss: 1.1236023902893066
Epoch 610, training loss: 0.019033275544643402 = 0.012125877663493156 + 0.001 * 6.907397270202637
Epoch 610, val loss: 1.1325914859771729
Epoch 620, training loss: 0.018335727974772453 = 0.0114442715421319 + 0.001 * 6.891456604003906
Epoch 620, val loss: 1.1413604021072388
Epoch 630, training loss: 0.01770714670419693 = 0.010819044895470142 + 0.001 * 6.888100624084473
Epoch 630, val loss: 1.1498686075210571
Epoch 640, training loss: 0.01712995395064354 = 0.010244213044643402 + 0.001 * 6.885739803314209
Epoch 640, val loss: 1.158171534538269
Epoch 650, training loss: 0.016600772738456726 = 0.00971477571874857 + 0.001 * 6.885996341705322
Epoch 650, val loss: 1.1662483215332031
Epoch 660, training loss: 0.016112584620714188 = 0.009226281195878983 + 0.001 * 6.886303424835205
Epoch 660, val loss: 1.1741396188735962
Epoch 670, training loss: 0.01566123217344284 = 0.00877454038709402 + 0.001 * 6.886690616607666
Epoch 670, val loss: 1.1818182468414307
Epoch 680, training loss: 0.015239762142300606 = 0.008355957455933094 + 0.001 * 6.883803844451904
Epoch 680, val loss: 1.189307689666748
Epoch 690, training loss: 0.014849849045276642 = 0.007967553101480007 + 0.001 * 6.882295608520508
Epoch 690, val loss: 1.196624517440796
Epoch 700, training loss: 0.014485692605376244 = 0.007606554310768843 + 0.001 * 6.8791375160217285
Epoch 700, val loss: 1.2037626504898071
Epoch 710, training loss: 0.014159616082906723 = 0.007270465604960918 + 0.001 * 6.889150142669678
Epoch 710, val loss: 1.210710883140564
Epoch 720, training loss: 0.01383528858423233 = 0.006957068108022213 + 0.001 * 6.8782196044921875
Epoch 720, val loss: 1.2174954414367676
Epoch 730, training loss: 0.013541002757847309 = 0.0066644190810620785 + 0.001 * 6.876583576202393
Epoch 730, val loss: 1.2241134643554688
Epoch 740, training loss: 0.013272375799715519 = 0.006390717811882496 + 0.001 * 6.881657600402832
Epoch 740, val loss: 1.230567216873169
Epoch 750, training loss: 0.013011574745178223 = 0.006134398281574249 + 0.001 * 6.877176284790039
Epoch 750, val loss: 1.2368733882904053
Epoch 760, training loss: 0.012765435501933098 = 0.005894062574952841 + 0.001 * 6.871372699737549
Epoch 760, val loss: 1.2430332899093628
Epoch 770, training loss: 0.012546518817543983 = 0.00566840311512351 + 0.001 * 6.878115653991699
Epoch 770, val loss: 1.2490758895874023
Epoch 780, training loss: 0.012331288307905197 = 0.00545629533007741 + 0.001 * 6.874992847442627
Epoch 780, val loss: 1.254951000213623
Epoch 790, training loss: 0.012129681184887886 = 0.005256669595837593 + 0.001 * 6.873011112213135
Epoch 790, val loss: 1.2607234716415405
Epoch 800, training loss: 0.01194150559604168 = 0.00506851589307189 + 0.001 * 6.872989177703857
Epoch 800, val loss: 1.2663520574569702
Epoch 810, training loss: 0.011759892106056213 = 0.004890964832156897 + 0.001 * 6.868927001953125
Epoch 810, val loss: 1.2718465328216553
Epoch 820, training loss: 0.011601950973272324 = 0.004723143298178911 + 0.001 * 6.87880802154541
Epoch 820, val loss: 1.277250051498413
Epoch 830, training loss: 0.011428967118263245 = 0.004564316011965275 + 0.001 * 6.864650249481201
Epoch 830, val loss: 1.2825411558151245
Epoch 840, training loss: 0.011275923810899258 = 0.004413677845150232 + 0.001 * 6.862245559692383
Epoch 840, val loss: 1.2877213954925537
Epoch 850, training loss: 0.011137925088405609 = 0.004270425532013178 + 0.001 * 6.867499828338623
Epoch 850, val loss: 1.2928274869918823
Epoch 860, training loss: 0.011000670492649078 = 0.004133890382945538 + 0.001 * 6.866779804229736
Epoch 860, val loss: 1.2978184223175049
Epoch 870, training loss: 0.010860808193683624 = 0.004003405570983887 + 0.001 * 6.857402324676514
Epoch 870, val loss: 1.3027441501617432
Epoch 880, training loss: 0.010738776996731758 = 0.0038783603813499212 + 0.001 * 6.860415935516357
Epoch 880, val loss: 1.3075834512710571
Epoch 890, training loss: 0.010614737868309021 = 0.0037583615630865097 + 0.001 * 6.856375694274902
Epoch 890, val loss: 1.3123337030410767
Epoch 900, training loss: 0.010498054325580597 = 0.0036430873442441225 + 0.001 * 6.85496711730957
Epoch 900, val loss: 1.3169785737991333
Epoch 910, training loss: 0.01038285531103611 = 0.0035322685725986958 + 0.001 * 6.850586414337158
Epoch 910, val loss: 1.3215664625167847
Epoch 920, training loss: 0.010274529457092285 = 0.0034258223604410887 + 0.001 * 6.8487067222595215
Epoch 920, val loss: 1.3260728120803833
Epoch 930, training loss: 0.010174170136451721 = 0.0033236839808523655 + 0.001 * 6.8504862785339355
Epoch 930, val loss: 1.3304764032363892
Epoch 940, training loss: 0.010088454931974411 = 0.003225692082196474 + 0.001 * 6.862762451171875
Epoch 940, val loss: 1.3347941637039185
Epoch 950, training loss: 0.00997431855648756 = 0.0031316655222326517 + 0.001 * 6.842652797698975
Epoch 950, val loss: 1.33904230594635
Epoch 960, training loss: 0.009889570996165276 = 0.0030414657667279243 + 0.001 * 6.848104953765869
Epoch 960, val loss: 1.3432263135910034
Epoch 970, training loss: 0.009810317307710648 = 0.0029549754690378904 + 0.001 * 6.85534143447876
Epoch 970, val loss: 1.3473366498947144
Epoch 980, training loss: 0.009712915867567062 = 0.0028721154667437077 + 0.001 * 6.840800762176514
Epoch 980, val loss: 1.3513684272766113
Epoch 990, training loss: 0.00963763240724802 = 0.00279273116029799 + 0.001 * 6.844901084899902
Epoch 990, val loss: 1.3553115129470825
Epoch 1000, training loss: 0.009552953764796257 = 0.0027167112566530704 + 0.001 * 6.836241722106934
Epoch 1000, val loss: 1.359217882156372
Epoch 1010, training loss: 0.009495582431554794 = 0.0026438783388584852 + 0.001 * 6.851703643798828
Epoch 1010, val loss: 1.36304771900177
Epoch 1020, training loss: 0.009404243901371956 = 0.002574182115495205 + 0.001 * 6.830061435699463
Epoch 1020, val loss: 1.3668289184570312
Epoch 1030, training loss: 0.00935022346675396 = 0.0025074181612581015 + 0.001 * 6.8428053855896
Epoch 1030, val loss: 1.3705395460128784
Epoch 1040, training loss: 0.009275877848267555 = 0.002443447010591626 + 0.001 * 6.832430362701416
Epoch 1040, val loss: 1.3741791248321533
Epoch 1050, training loss: 0.009212138131260872 = 0.002382156439125538 + 0.001 * 6.829980850219727
Epoch 1050, val loss: 1.3777590990066528
Epoch 1060, training loss: 0.009150366298854351 = 0.0023234214168041945 + 0.001 * 6.826944351196289
Epoch 1060, val loss: 1.3812764883041382
Epoch 1070, training loss: 0.009099168702960014 = 0.0022671441547572613 + 0.001 * 6.832024097442627
Epoch 1070, val loss: 1.3847486972808838
Epoch 1080, training loss: 0.009047736413776875 = 0.0022131854202598333 + 0.001 * 6.834550380706787
Epoch 1080, val loss: 1.388148546218872
Epoch 1090, training loss: 0.008984589949250221 = 0.0021614250726997852 + 0.001 * 6.823163986206055
Epoch 1090, val loss: 1.3914785385131836
Epoch 1100, training loss: 0.008938080631196499 = 0.0021117241121828556 + 0.001 * 6.826356410980225
Epoch 1100, val loss: 1.3947736024856567
Epoch 1110, training loss: 0.00888036284595728 = 0.0020640024449676275 + 0.001 * 6.816359996795654
Epoch 1110, val loss: 1.3980191946029663
Epoch 1120, training loss: 0.008846442215144634 = 0.002018149709329009 + 0.001 * 6.828292369842529
Epoch 1120, val loss: 1.4011973142623901
Epoch 1130, training loss: 0.008785468526184559 = 0.0019741086289286613 + 0.001 * 6.811359405517578
Epoch 1130, val loss: 1.4043272733688354
Epoch 1140, training loss: 0.00875390786677599 = 0.0019317778060212731 + 0.001 * 6.822129726409912
Epoch 1140, val loss: 1.4073972702026367
Epoch 1150, training loss: 0.008713677525520325 = 0.0018910905346274376 + 0.001 * 6.822586536407471
Epoch 1150, val loss: 1.4104034900665283
Epoch 1160, training loss: 0.008658535778522491 = 0.0018519809236750007 + 0.001 * 6.806554794311523
Epoch 1160, val loss: 1.4133647680282593
Epoch 1170, training loss: 0.008626888506114483 = 0.0018143650377169251 + 0.001 * 6.812523365020752
Epoch 1170, val loss: 1.4162465333938599
Epoch 1180, training loss: 0.008593326434493065 = 0.0017781734932214022 + 0.001 * 6.815152168273926
Epoch 1180, val loss: 1.4191365242004395
Epoch 1190, training loss: 0.008552209474146366 = 0.0017433400498703122 + 0.001 * 6.808869361877441
Epoch 1190, val loss: 1.4219095706939697
Epoch 1200, training loss: 0.008530979976058006 = 0.0017097691306844354 + 0.001 * 6.821210861206055
Epoch 1200, val loss: 1.424685001373291
Epoch 1210, training loss: 0.008470214903354645 = 0.0016774304676800966 + 0.001 * 6.792784214019775
Epoch 1210, val loss: 1.427374005317688
Epoch 1220, training loss: 0.008463935926556587 = 0.0016462502535432577 + 0.001 * 6.817685604095459
Epoch 1220, val loss: 1.4300199747085571
Epoch 1230, training loss: 0.008417665958404541 = 0.0016162251122295856 + 0.001 * 6.801440238952637
Epoch 1230, val loss: 1.432647943496704
Epoch 1240, training loss: 0.008384115062654018 = 0.0015872445655986667 + 0.001 * 6.796870708465576
Epoch 1240, val loss: 1.435207486152649
Epoch 1250, training loss: 0.008353695273399353 = 0.00155928754247725 + 0.001 * 6.794407367706299
Epoch 1250, val loss: 1.4376964569091797
Epoch 1260, training loss: 0.008332889527082443 = 0.0015323192346841097 + 0.001 * 6.800569534301758
Epoch 1260, val loss: 1.440171718597412
Epoch 1270, training loss: 0.008347121998667717 = 0.0015062697930261493 + 0.001 * 6.8408522605896
Epoch 1270, val loss: 1.4425870180130005
Epoch 1280, training loss: 0.008275524713099003 = 0.001481101498939097 + 0.001 * 6.7944231033325195
Epoch 1280, val loss: 1.4449583292007446
Epoch 1290, training loss: 0.008282148279249668 = 0.0014567817561328411 + 0.001 * 6.825366020202637
Epoch 1290, val loss: 1.447304606437683
Epoch 1300, training loss: 0.008221271447837353 = 0.0014332679565995932 + 0.001 * 6.788003444671631
Epoch 1300, val loss: 1.449586033821106
Epoch 1310, training loss: 0.008188861422240734 = 0.0014105247100815177 + 0.001 * 6.778336524963379
Epoch 1310, val loss: 1.4518400430679321
Epoch 1320, training loss: 0.008187356404960155 = 0.0013885449152439833 + 0.001 * 6.798810958862305
Epoch 1320, val loss: 1.4540948867797852
Epoch 1330, training loss: 0.008158067241311073 = 0.0013672956265509129 + 0.001 * 6.790771961212158
Epoch 1330, val loss: 1.4562087059020996
Epoch 1340, training loss: 0.008157771080732346 = 0.001346723292954266 + 0.001 * 6.811047554016113
Epoch 1340, val loss: 1.4583419561386108
Epoch 1350, training loss: 0.008095157332718372 = 0.0013268064940348268 + 0.001 * 6.768350601196289
Epoch 1350, val loss: 1.4604551792144775
Epoch 1360, training loss: 0.008075561374425888 = 0.0013075062306597829 + 0.001 * 6.768054962158203
Epoch 1360, val loss: 1.4624834060668945
Epoch 1370, training loss: 0.00807247031480074 = 0.001288832863792777 + 0.001 * 6.783637046813965
Epoch 1370, val loss: 1.4644991159439087
Epoch 1380, training loss: 0.008050775155425072 = 0.0012707513524219394 + 0.001 * 6.780023574829102
Epoch 1380, val loss: 1.466470718383789
Epoch 1390, training loss: 0.008008960634469986 = 0.0012532202526926994 + 0.001 * 6.755740165710449
Epoch 1390, val loss: 1.4683961868286133
Epoch 1400, training loss: 0.008013278245925903 = 0.0012362069683149457 + 0.001 * 6.777070999145508
Epoch 1400, val loss: 1.470269799232483
Epoch 1410, training loss: 0.007995374500751495 = 0.0012197127798572183 + 0.001 * 6.775660991668701
Epoch 1410, val loss: 1.472159743309021
Epoch 1420, training loss: 0.007962864823639393 = 0.0012036998523399234 + 0.001 * 6.759164810180664
Epoch 1420, val loss: 1.4739383459091187
Epoch 1430, training loss: 0.007952767424285412 = 0.0011881360551342368 + 0.001 * 6.7646307945251465
Epoch 1430, val loss: 1.475749135017395
Epoch 1440, training loss: 0.007920675911009312 = 0.0011730582918971777 + 0.001 * 6.747617244720459
Epoch 1440, val loss: 1.4775267839431763
Epoch 1450, training loss: 0.007946894504129887 = 0.0011584176681935787 + 0.001 * 6.788476467132568
Epoch 1450, val loss: 1.4791995286941528
Epoch 1460, training loss: 0.007890627719461918 = 0.0011442189570516348 + 0.001 * 6.746407985687256
Epoch 1460, val loss: 1.4809064865112305
Epoch 1470, training loss: 0.007908064872026443 = 0.001130415010266006 + 0.001 * 6.777649879455566
Epoch 1470, val loss: 1.4825760126113892
Epoch 1480, training loss: 0.007855841889977455 = 0.0011170214274898171 + 0.001 * 6.7388200759887695
Epoch 1480, val loss: 1.4841763973236084
Epoch 1490, training loss: 0.00785080622881651 = 0.001103989197872579 + 0.001 * 6.746816635131836
Epoch 1490, val loss: 1.4857521057128906
Epoch 1500, training loss: 0.007853283546864986 = 0.0010913016740232706 + 0.001 * 6.76198148727417
Epoch 1500, val loss: 1.487366795539856
Epoch 1510, training loss: 0.007814179174602032 = 0.0010789695661514997 + 0.001 * 6.735208988189697
Epoch 1510, val loss: 1.488860011100769
Epoch 1520, training loss: 0.007803501561284065 = 0.0010669921757653356 + 0.001 * 6.736509323120117
Epoch 1520, val loss: 1.4903225898742676
Epoch 1530, training loss: 0.007847652770578861 = 0.0010553266620263457 + 0.001 * 6.792325496673584
Epoch 1530, val loss: 1.4917885065078735
Epoch 1540, training loss: 0.007789349649101496 = 0.0010439902544021606 + 0.001 * 6.745358943939209
Epoch 1540, val loss: 1.4932423830032349
Epoch 1550, training loss: 0.007775041274726391 = 0.0010329651413485408 + 0.001 * 6.7420759201049805
Epoch 1550, val loss: 1.494625210762024
Epoch 1560, training loss: 0.007781896740198135 = 0.0010222408454865217 + 0.001 * 6.759655952453613
Epoch 1560, val loss: 1.4960315227508545
Epoch 1570, training loss: 0.007739415392279625 = 0.0010118006030097604 + 0.001 * 6.727614402770996
Epoch 1570, val loss: 1.4973448514938354
Epoch 1580, training loss: 0.0077498359605669975 = 0.0010016374289989471 + 0.001 * 6.74819803237915
Epoch 1580, val loss: 1.4987224340438843
Epoch 1590, training loss: 0.007713912520557642 = 0.0009917556308209896 + 0.001 * 6.722156524658203
Epoch 1590, val loss: 1.499988317489624
Epoch 1600, training loss: 0.007700363174080849 = 0.0009821250569075346 + 0.001 * 6.718237400054932
Epoch 1600, val loss: 1.5012303590774536
Epoch 1610, training loss: 0.007713842671364546 = 0.0009727483266033232 + 0.001 * 6.74109411239624
Epoch 1610, val loss: 1.502503514289856
Epoch 1620, training loss: 0.0076676346361637115 = 0.0009636043687351048 + 0.001 * 6.704029560089111
Epoch 1620, val loss: 1.5037052631378174
Epoch 1630, training loss: 0.007725214120000601 = 0.0009546858491376042 + 0.001 * 6.7705278396606445
Epoch 1630, val loss: 1.5049043893814087
Epoch 1640, training loss: 0.007679772097617388 = 0.0009460020228289068 + 0.001 * 6.73376989364624
Epoch 1640, val loss: 1.5060899257659912
Epoch 1650, training loss: 0.007664167787879705 = 0.0009375314111821353 + 0.001 * 6.7266364097595215
Epoch 1650, val loss: 1.5071624517440796
Epoch 1660, training loss: 0.0077150058932602406 = 0.0009292587637901306 + 0.001 * 6.7857465744018555
Epoch 1660, val loss: 1.5083255767822266
Epoch 1670, training loss: 0.007636968046426773 = 0.0009212007280439138 + 0.001 * 6.7157673835754395
Epoch 1670, val loss: 1.509348750114441
Epoch 1680, training loss: 0.007651986554265022 = 0.0009133320418186486 + 0.001 * 6.738654136657715
Epoch 1680, val loss: 1.5103586912155151
Epoch 1690, training loss: 0.007636416703462601 = 0.000905657943803817 + 0.001 * 6.730758190155029
Epoch 1690, val loss: 1.5114641189575195
Epoch 1700, training loss: 0.007618637289851904 = 0.0008981676655821502 + 0.001 * 6.7204694747924805
Epoch 1700, val loss: 1.5123858451843262
Epoch 1710, training loss: 0.007631450891494751 = 0.0008908516028895974 + 0.001 * 6.740599155426025
Epoch 1710, val loss: 1.5134215354919434
Epoch 1720, training loss: 0.0075754523277282715 = 0.0008837393252179027 + 0.001 * 6.691712379455566
Epoch 1720, val loss: 1.5143849849700928
Epoch 1730, training loss: 0.007595413364470005 = 0.0008767833933234215 + 0.001 * 6.718629837036133
Epoch 1730, val loss: 1.5152822732925415
Epoch 1740, training loss: 0.007567327935248613 = 0.0008699899190105498 + 0.001 * 6.697337627410889
Epoch 1740, val loss: 1.516265630722046
Epoch 1750, training loss: 0.0075439754873514175 = 0.0008633666438981891 + 0.001 * 6.68060827255249
Epoch 1750, val loss: 1.5170997381210327
Epoch 1760, training loss: 0.007632119581103325 = 0.0008568896446377039 + 0.001 * 6.775229454040527
Epoch 1760, val loss: 1.5179965496063232
Epoch 1770, training loss: 0.007532425690442324 = 0.0008505666046403348 + 0.001 * 6.681858539581299
Epoch 1770, val loss: 1.5188113451004028
Epoch 1780, training loss: 0.007570731453597546 = 0.0008443698170594871 + 0.001 * 6.726361274719238
Epoch 1780, val loss: 1.5195534229278564
Epoch 1790, training loss: 0.007548085879534483 = 0.0008383156964555383 + 0.001 * 6.709770202636719
Epoch 1790, val loss: 1.5204182863235474
Epoch 1800, training loss: 0.0075197527185082436 = 0.0008323932997882366 + 0.001 * 6.68735933303833
Epoch 1800, val loss: 1.5212091207504272
Epoch 1810, training loss: 0.007566207088530064 = 0.0008265838259831071 + 0.001 * 6.739623069763184
Epoch 1810, val loss: 1.5219199657440186
Epoch 1820, training loss: 0.007500428706407547 = 0.000820900546386838 + 0.001 * 6.679527759552002
Epoch 1820, val loss: 1.5227057933807373
Epoch 1830, training loss: 0.00750073092058301 = 0.0008153322269208729 + 0.001 * 6.685398101806641
Epoch 1830, val loss: 1.5234122276306152
Epoch 1840, training loss: 0.007507222704589367 = 0.0008098728721961379 + 0.001 * 6.697349548339844
Epoch 1840, val loss: 1.524162769317627
Epoch 1850, training loss: 0.00748511403799057 = 0.0008045255672186613 + 0.001 * 6.680588245391846
Epoch 1850, val loss: 1.524835467338562
Epoch 1860, training loss: 0.0074629634618759155 = 0.000799299799837172 + 0.001 * 6.663663387298584
Epoch 1860, val loss: 1.5255006551742554
Epoch 1870, training loss: 0.007472355850040913 = 0.0007942096563056111 + 0.001 * 6.678145885467529
Epoch 1870, val loss: 1.5261917114257812
Epoch 1880, training loss: 0.007469732314348221 = 0.0007892371504567564 + 0.001 * 6.680494785308838
Epoch 1880, val loss: 1.5267795324325562
Epoch 1890, training loss: 0.007527718786150217 = 0.0007843522471375763 + 0.001 * 6.743366241455078
Epoch 1890, val loss: 1.5274584293365479
Epoch 1900, training loss: 0.00743109779432416 = 0.0007795678684487939 + 0.001 * 6.651529788970947
Epoch 1900, val loss: 1.5280678272247314
Epoch 1910, training loss: 0.007448724005371332 = 0.0007748796488158405 + 0.001 * 6.673844337463379
Epoch 1910, val loss: 1.528623104095459
Epoch 1920, training loss: 0.007481525186449289 = 0.000770280952565372 + 0.001 * 6.711243629455566
Epoch 1920, val loss: 1.5292500257492065
Epoch 1930, training loss: 0.007445797324180603 = 0.0007657788228243589 + 0.001 * 6.680017948150635
Epoch 1930, val loss: 1.529781699180603
Epoch 1940, training loss: 0.007443088106811047 = 0.0007613669149577618 + 0.001 * 6.681720733642578
Epoch 1940, val loss: 1.5303252935409546
Epoch 1950, training loss: 0.007425393909215927 = 0.0007570312009193003 + 0.001 * 6.668362617492676
Epoch 1950, val loss: 1.5308427810668945
Epoch 1960, training loss: 0.00741799920797348 = 0.0007527989218942821 + 0.001 * 6.665200233459473
Epoch 1960, val loss: 1.5313485860824585
Epoch 1970, training loss: 0.007418758235871792 = 0.0007486389367841184 + 0.001 * 6.670118808746338
Epoch 1970, val loss: 1.5318603515625
Epoch 1980, training loss: 0.007447362877428532 = 0.0007445407682098448 + 0.001 * 6.702821731567383
Epoch 1980, val loss: 1.5323460102081299
Epoch 1990, training loss: 0.007388908416032791 = 0.0007405332289636135 + 0.001 * 6.648375034332275
Epoch 1990, val loss: 1.5328493118286133
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.7860
Flip ASR: 0.7467/225 nodes
The final ASR:0.65929, 0.10395, Accuracy:0.81111, 0.01600
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11592])
remove edge: torch.Size([2, 9512])
updated graph: torch.Size([2, 10548])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9705
Flip ASR: 0.9644/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8407
Overall ASR: 0.9594
Flip ASR: 0.9511/225 nodes
The final ASR:0.97048, 0.00904, Accuracy:0.82963, 0.00800
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9846467971801758 = 1.9762729406356812 + 0.001 * 8.373828887939453
Epoch 0, val loss: 1.9751594066619873
Epoch 10, training loss: 1.9730591773986816 = 1.9646854400634766 + 0.001 * 8.373748779296875
Epoch 10, val loss: 1.9636436700820923
Epoch 20, training loss: 1.958710789680481 = 1.950337290763855 + 0.001 * 8.373520851135254
Epoch 20, val loss: 1.9491932392120361
Epoch 30, training loss: 1.9385794401168823 = 1.9302064180374146 + 0.001 * 8.373063087463379
Epoch 30, val loss: 1.9290145635604858
Epoch 40, training loss: 1.9087762832641602 = 1.9004042148590088 + 0.001 * 8.372047424316406
Epoch 40, val loss: 1.8998650312423706
Epoch 50, training loss: 1.866855502128601 = 1.8584864139556885 + 0.001 * 8.369133949279785
Epoch 50, val loss: 1.8612077236175537
Epoch 60, training loss: 1.8192516565322876 = 1.8108943700790405 + 0.001 * 8.357253074645996
Epoch 60, val loss: 1.8226426839828491
Epoch 70, training loss: 1.780368447303772 = 1.7720730304718018 + 0.001 * 8.295409202575684
Epoch 70, val loss: 1.7944258451461792
Epoch 80, training loss: 1.737553596496582 = 1.7295743227005005 + 0.001 * 7.979249954223633
Epoch 80, val loss: 1.7564339637756348
Epoch 90, training loss: 1.6790724992752075 = 1.6713221073150635 + 0.001 * 7.750435829162598
Epoch 90, val loss: 1.7051715850830078
Epoch 100, training loss: 1.5999377965927124 = 1.5922775268554688 + 0.001 * 7.660238742828369
Epoch 100, val loss: 1.6387327909469604
Epoch 110, training loss: 1.5055581331253052 = 1.4979804754257202 + 0.001 * 7.577631950378418
Epoch 110, val loss: 1.5601258277893066
Epoch 120, training loss: 1.411352276802063 = 1.4039684534072876 + 0.001 * 7.3837809562683105
Epoch 120, val loss: 1.4840620756149292
Epoch 130, training loss: 1.3240774869918823 = 1.3169496059417725 + 0.001 * 7.1278462409973145
Epoch 130, val loss: 1.415631651878357
Epoch 140, training loss: 1.2416832447052002 = 1.2346208095550537 + 0.001 * 7.062469482421875
Epoch 140, val loss: 1.3537261486053467
Epoch 150, training loss: 1.1612142324447632 = 1.1542152166366577 + 0.001 * 6.998968601226807
Epoch 150, val loss: 1.2948532104492188
Epoch 160, training loss: 1.0813572406768799 = 1.0744094848632812 + 0.001 * 6.947808742523193
Epoch 160, val loss: 1.2371160984039307
Epoch 170, training loss: 1.0019874572753906 = 0.9950816631317139 + 0.001 * 6.905755043029785
Epoch 170, val loss: 1.180538535118103
Epoch 180, training loss: 0.9232043027877808 = 0.9163190126419067 + 0.001 * 6.885270118713379
Epoch 180, val loss: 1.1242080926895142
Epoch 190, training loss: 0.8448247909545898 = 0.8379472494125366 + 0.001 * 6.877570152282715
Epoch 190, val loss: 1.0678144693374634
Epoch 200, training loss: 0.7667370438575745 = 0.7598624229431152 + 0.001 * 6.8746256828308105
Epoch 200, val loss: 1.0106992721557617
Epoch 210, training loss: 0.6900514364242554 = 0.6831798553466797 + 0.001 * 6.8715643882751465
Epoch 210, val loss: 0.9540682435035706
Epoch 220, training loss: 0.6177195906639099 = 0.6108506917953491 + 0.001 * 6.868895053863525
Epoch 220, val loss: 0.9009679555892944
Epoch 230, training loss: 0.5525497198104858 = 0.5456831455230713 + 0.001 * 6.866578102111816
Epoch 230, val loss: 0.8547796010971069
Epoch 240, training loss: 0.49541276693344116 = 0.48854848742485046 + 0.001 * 6.864294052124023
Epoch 240, val loss: 0.8171359300613403
Epoch 250, training loss: 0.4450804591178894 = 0.43821847438812256 + 0.001 * 6.861990451812744
Epoch 250, val loss: 0.7871214747428894
Epoch 260, training loss: 0.3995315730571747 = 0.3926718831062317 + 0.001 * 6.859686851501465
Epoch 260, val loss: 0.7625814080238342
Epoch 270, training loss: 0.35694462060928345 = 0.35008713603019714 + 0.001 * 6.857484817504883
Epoch 270, val loss: 0.7415809035301208
Epoch 280, training loss: 0.316245436668396 = 0.30938997864723206 + 0.001 * 6.855471134185791
Epoch 280, val loss: 0.7227156758308411
Epoch 290, training loss: 0.27720221877098083 = 0.27034854888916016 + 0.001 * 6.85366678237915
Epoch 290, val loss: 0.7056552171707153
Epoch 300, training loss: 0.24028487503528595 = 0.2334328591823578 + 0.001 * 6.852016448974609
Epoch 300, val loss: 0.6906881332397461
Epoch 310, training loss: 0.2063543051481247 = 0.1995038092136383 + 0.001 * 6.850489616394043
Epoch 310, val loss: 0.6784467101097107
Epoch 320, training loss: 0.17624643445014954 = 0.16939745843410492 + 0.001 * 6.848982810974121
Epoch 320, val loss: 0.6694915890693665
Epoch 330, training loss: 0.15048819780349731 = 0.1436396986246109 + 0.001 * 6.8485002517700195
Epoch 330, val loss: 0.6642798185348511
Epoch 340, training loss: 0.1290990710258484 = 0.12225281447172165 + 0.001 * 6.846257209777832
Epoch 340, val loss: 0.6629220247268677
Epoch 350, training loss: 0.11163614690303802 = 0.10479150712490082 + 0.001 * 6.844635963439941
Epoch 350, val loss: 0.6650238633155823
Epoch 360, training loss: 0.09744025021791458 = 0.0905977338552475 + 0.001 * 6.842519283294678
Epoch 360, val loss: 0.6700223088264465
Epoch 370, training loss: 0.08584456145763397 = 0.07900496572256088 + 0.001 * 6.839599132537842
Epoch 370, val loss: 0.6773478984832764
Epoch 380, training loss: 0.07627954334020615 = 0.06944319605827332 + 0.001 * 6.8363447189331055
Epoch 380, val loss: 0.6862993836402893
Epoch 390, training loss: 0.06829041242599487 = 0.06145786866545677 + 0.001 * 6.832543849945068
Epoch 390, val loss: 0.6964555382728577
Epoch 400, training loss: 0.061536289751529694 = 0.0547088123857975 + 0.001 * 6.827475547790527
Epoch 400, val loss: 0.7073951959609985
Epoch 410, training loss: 0.05577240139245987 = 0.04895220696926117 + 0.001 * 6.820195198059082
Epoch 410, val loss: 0.7188568711280823
Epoch 420, training loss: 0.05082178860902786 = 0.044007401913404465 + 0.001 * 6.814385414123535
Epoch 420, val loss: 0.7306002974510193
Epoch 430, training loss: 0.04654049128293991 = 0.03973514214158058 + 0.001 * 6.805348873138428
Epoch 430, val loss: 0.7424411773681641
Epoch 440, training loss: 0.04282534122467041 = 0.03602476045489311 + 0.001 * 6.800581932067871
Epoch 440, val loss: 0.7542911767959595
Epoch 450, training loss: 0.039578311145305634 = 0.032786644995212555 + 0.001 * 6.791666030883789
Epoch 450, val loss: 0.7660226821899414
Epoch 460, training loss: 0.036729760468006134 = 0.029948042705655098 + 0.001 * 6.781716823577881
Epoch 460, val loss: 0.7775951027870178
Epoch 470, training loss: 0.03422921150922775 = 0.027448682114481926 + 0.001 * 6.7805304527282715
Epoch 470, val loss: 0.7889344692230225
Epoch 480, training loss: 0.03202090412378311 = 0.02523917704820633 + 0.001 * 6.781726837158203
Epoch 480, val loss: 0.800058126449585
Epoch 490, training loss: 0.030044976621866226 = 0.023278070613741875 + 0.001 * 6.766905784606934
Epoch 490, val loss: 0.8108893632888794
Epoch 500, training loss: 0.028293026611208916 = 0.02153102681040764 + 0.001 * 6.761999607086182
Epoch 500, val loss: 0.8214725255966187
Epoch 510, training loss: 0.026732206344604492 = 0.019969094544649124 + 0.001 * 6.7631120681762695
Epoch 510, val loss: 0.8317963480949402
Epoch 520, training loss: 0.025333017110824585 = 0.018568258732557297 + 0.001 * 6.7647576332092285
Epoch 520, val loss: 0.8418302536010742
Epoch 530, training loss: 0.024069083854556084 = 0.017307518050074577 + 0.001 * 6.761565208435059
Epoch 530, val loss: 0.8516212701797485
Epoch 540, training loss: 0.02292727679014206 = 0.016169702634215355 + 0.001 * 6.75757360458374
Epoch 540, val loss: 0.8611882925033569
Epoch 550, training loss: 0.021904172375798225 = 0.015139693394303322 + 0.001 * 6.764479160308838
Epoch 550, val loss: 0.8705101013183594
Epoch 560, training loss: 0.0209646075963974 = 0.014204884879291058 + 0.001 * 6.759721279144287
Epoch 560, val loss: 0.8795780539512634
Epoch 570, training loss: 0.020108085125684738 = 0.013354356400668621 + 0.001 * 6.75372838973999
Epoch 570, val loss: 0.8884332180023193
Epoch 580, training loss: 0.019331876188516617 = 0.012578492052853107 + 0.001 * 6.753383636474609
Epoch 580, val loss: 0.8970580697059631
Epoch 590, training loss: 0.018624311313033104 = 0.011869260109961033 + 0.001 * 6.755051136016846
Epoch 590, val loss: 0.9054670333862305
Epoch 600, training loss: 0.017971914261579514 = 0.011219457723200321 + 0.001 * 6.752455711364746
Epoch 600, val loss: 0.9136739373207092
Epoch 610, training loss: 0.017375066876411438 = 0.010622646659612656 + 0.001 * 6.752420902252197
Epoch 610, val loss: 0.921686053276062
Epoch 620, training loss: 0.01682586781680584 = 0.010073394514620304 + 0.001 * 6.752472400665283
Epoch 620, val loss: 0.9294894337654114
Epoch 630, training loss: 0.01631598174571991 = 0.009566860273480415 + 0.001 * 6.749121189117432
Epoch 630, val loss: 0.9371016621589661
Epoch 640, training loss: 0.015847552567720413 = 0.009098915383219719 + 0.001 * 6.7486371994018555
Epoch 640, val loss: 0.9445309042930603
Epoch 650, training loss: 0.015412472188472748 = 0.008665729314088821 + 0.001 * 6.7467427253723145
Epoch 650, val loss: 0.9517962336540222
Epoch 660, training loss: 0.015014447271823883 = 0.008264082483947277 + 0.001 * 6.750363826751709
Epoch 660, val loss: 0.9588780403137207
Epoch 670, training loss: 0.014636898413300514 = 0.007890980690717697 + 0.001 * 6.745916843414307
Epoch 670, val loss: 0.9658059477806091
Epoch 680, training loss: 0.014294121414422989 = 0.007543809246271849 + 0.001 * 6.750312328338623
Epoch 680, val loss: 0.972546398639679
Epoch 690, training loss: 0.013968521729111671 = 0.007220323663204908 + 0.001 * 6.748197555541992
Epoch 690, val loss: 0.9791048169136047
Epoch 700, training loss: 0.013663443736732006 = 0.006918414495885372 + 0.001 * 6.745028972625732
Epoch 700, val loss: 0.9855418801307678
Epoch 710, training loss: 0.013375669717788696 = 0.006636212579905987 + 0.001 * 6.7394561767578125
Epoch 710, val loss: 0.9918501973152161
Epoch 720, training loss: 0.01311519555747509 = 0.006372056435793638 + 0.001 * 6.743139266967773
Epoch 720, val loss: 0.9979975819587708
Epoch 730, training loss: 0.012861901894211769 = 0.00612446665763855 + 0.001 * 6.7374348640441895
Epoch 730, val loss: 1.0040221214294434
Epoch 740, training loss: 0.012633338570594788 = 0.0058921161107718945 + 0.001 * 6.741221904754639
Epoch 740, val loss: 1.0099029541015625
Epoch 750, training loss: 0.012414176017045975 = 0.0056737978011369705 + 0.001 * 6.740377426147461
Epoch 750, val loss: 1.0156594514846802
Epoch 760, training loss: 0.012210410088300705 = 0.005468442570418119 + 0.001 * 6.74196720123291
Epoch 760, val loss: 1.0212924480438232
Epoch 770, training loss: 0.01200741995126009 = 0.0052750310860574245 + 0.001 * 6.732388496398926
Epoch 770, val loss: 1.0268150568008423
Epoch 780, training loss: 0.011835400015115738 = 0.005092634819447994 + 0.001 * 6.742765426635742
Epoch 780, val loss: 1.0322239398956299
Epoch 790, training loss: 0.011655997484922409 = 0.00492043187841773 + 0.001 * 6.735565662384033
Epoch 790, val loss: 1.0375239849090576
Epoch 800, training loss: 0.011489334516227245 = 0.0047577014192938805 + 0.001 * 6.731632709503174
Epoch 800, val loss: 1.04271399974823
Epoch 810, training loss: 0.011335927993059158 = 0.004603754263371229 + 0.001 * 6.732172966003418
Epoch 810, val loss: 1.0477949380874634
Epoch 820, training loss: 0.011194274760782719 = 0.004457968287169933 + 0.001 * 6.736306190490723
Epoch 820, val loss: 1.0527716875076294
Epoch 830, training loss: 0.01104488130658865 = 0.004319762345403433 + 0.001 * 6.725118637084961
Epoch 830, val loss: 1.0576504468917847
Epoch 840, training loss: 0.010913670994341373 = 0.004188616294413805 + 0.001 * 6.7250542640686035
Epoch 840, val loss: 1.062441110610962
Epoch 850, training loss: 0.010793713852763176 = 0.004064071457833052 + 0.001 * 6.729641437530518
Epoch 850, val loss: 1.0671387910842896
Epoch 860, training loss: 0.010665884241461754 = 0.003945698030292988 + 0.001 * 6.72018575668335
Epoch 860, val loss: 1.071751594543457
Epoch 870, training loss: 0.010568447411060333 = 0.003833079943433404 + 0.001 * 6.7353668212890625
Epoch 870, val loss: 1.0762423276901245
Epoch 880, training loss: 0.01044619269669056 = 0.003725850023329258 + 0.001 * 6.720342636108398
Epoch 880, val loss: 1.0806835889816284
Epoch 890, training loss: 0.010343066416680813 = 0.0036236790474504232 + 0.001 * 6.719387054443359
Epoch 890, val loss: 1.0850268602371216
Epoch 900, training loss: 0.010266241617500782 = 0.003526263637468219 + 0.001 * 6.739977836608887
Epoch 900, val loss: 1.089298129081726
Epoch 910, training loss: 0.010150891728699207 = 0.0034333032090216875 + 0.001 * 6.717587947845459
Epoch 910, val loss: 1.0934971570968628
Epoch 920, training loss: 0.010059766471385956 = 0.0033445258159190416 + 0.001 * 6.715240001678467
Epoch 920, val loss: 1.0976154804229736
Epoch 930, training loss: 0.009990569204092026 = 0.003259695367887616 + 0.001 * 6.7308735847473145
Epoch 930, val loss: 1.1016509532928467
Epoch 940, training loss: 0.009890974499285221 = 0.003178566461429 + 0.001 * 6.712408065795898
Epoch 940, val loss: 1.1055943965911865
Epoch 950, training loss: 0.009846620261669159 = 0.00310094584710896 + 0.001 * 6.745673656463623
Epoch 950, val loss: 1.1094797849655151
Epoch 960, training loss: 0.00973274651914835 = 0.003026597900316119 + 0.001 * 6.706148147583008
Epoch 960, val loss: 1.1133146286010742
Epoch 970, training loss: 0.009667518548667431 = 0.002955364529043436 + 0.001 * 6.712153911590576
Epoch 970, val loss: 1.1170706748962402
Epoch 980, training loss: 0.009588603861629963 = 0.0028870755340903997 + 0.001 * 6.7015275955200195
Epoch 980, val loss: 1.1207624673843384
Epoch 990, training loss: 0.009548516944050789 = 0.002821563743054867 + 0.001 * 6.726953029632568
Epoch 990, val loss: 1.1243928670883179
Epoch 1000, training loss: 0.009457116015255451 = 0.0027586903888732195 + 0.001 * 6.698425769805908
Epoch 1000, val loss: 1.1279451847076416
Epoch 1010, training loss: 0.009424599818885326 = 0.0026983057614415884 + 0.001 * 6.726293563842773
Epoch 1010, val loss: 1.1314514875411987
Epoch 1020, training loss: 0.009351985529065132 = 0.0026402927469462156 + 0.001 * 6.711692810058594
Epoch 1020, val loss: 1.1348844766616821
Epoch 1030, training loss: 0.009282510727643967 = 0.0025845274794846773 + 0.001 * 6.6979827880859375
Epoch 1030, val loss: 1.138245701789856
Epoch 1040, training loss: 0.009226925671100616 = 0.00253088166937232 + 0.001 * 6.696043014526367
Epoch 1040, val loss: 1.1415554285049438
Epoch 1050, training loss: 0.009178406558930874 = 0.002479257294908166 + 0.001 * 6.699149131774902
Epoch 1050, val loss: 1.1448332071304321
Epoch 1060, training loss: 0.00911942683160305 = 0.0024295661132782698 + 0.001 * 6.6898603439331055
Epoch 1060, val loss: 1.1480334997177124
Epoch 1070, training loss: 0.00911729782819748 = 0.002381687518209219 + 0.001 * 6.735609531402588
Epoch 1070, val loss: 1.151202917098999
Epoch 1080, training loss: 0.009022919461131096 = 0.0023355581797659397 + 0.001 * 6.6873602867126465
Epoch 1080, val loss: 1.154320478439331
Epoch 1090, training loss: 0.00897605437785387 = 0.0022910877596586943 + 0.001 * 6.684966087341309
Epoch 1090, val loss: 1.157353401184082
Epoch 1100, training loss: 0.008960233069956303 = 0.002248185919597745 + 0.001 * 6.712047100067139
Epoch 1100, val loss: 1.1603612899780273
Epoch 1110, training loss: 0.00889994390308857 = 0.002206796780228615 + 0.001 * 6.693146228790283
Epoch 1110, val loss: 1.1633286476135254
Epoch 1120, training loss: 0.008858191780745983 = 0.0021668397821485996 + 0.001 * 6.691351890563965
Epoch 1120, val loss: 1.1662062406539917
Epoch 1130, training loss: 0.008849387988448143 = 0.002128257416188717 + 0.001 * 6.72113037109375
Epoch 1130, val loss: 1.169035792350769
Epoch 1140, training loss: 0.00878535583615303 = 0.0020909705199301243 + 0.001 * 6.694384574890137
Epoch 1140, val loss: 1.171860933303833
Epoch 1150, training loss: 0.008735555224120617 = 0.0020549355540424585 + 0.001 * 6.680619716644287
Epoch 1150, val loss: 1.1746176481246948
Epoch 1160, training loss: 0.0086956936866045 = 0.0020200968720018864 + 0.001 * 6.675597190856934
Epoch 1160, val loss: 1.1773356199264526
Epoch 1170, training loss: 0.008668933063745499 = 0.0019864048808813095 + 0.001 * 6.682527542114258
Epoch 1170, val loss: 1.180009365081787
Epoch 1180, training loss: 0.00866498053073883 = 0.001953811151906848 + 0.001 * 6.711169242858887
Epoch 1180, val loss: 1.1826353073120117
Epoch 1190, training loss: 0.008610080927610397 = 0.0019222577102482319 + 0.001 * 6.687823295593262
Epoch 1190, val loss: 1.1852240562438965
Epoch 1200, training loss: 0.00856760423630476 = 0.001891692285425961 + 0.001 * 6.675911903381348
Epoch 1200, val loss: 1.1877621412277222
Epoch 1210, training loss: 0.008551092818379402 = 0.0018620952032506466 + 0.001 * 6.6889967918396
Epoch 1210, val loss: 1.1902673244476318
Epoch 1220, training loss: 0.008507722988724709 = 0.001833382062613964 + 0.001 * 6.674340724945068
Epoch 1220, val loss: 1.1927341222763062
Epoch 1230, training loss: 0.008506505750119686 = 0.0018055855762213469 + 0.001 * 6.7009196281433105
Epoch 1230, val loss: 1.1951441764831543
Epoch 1240, training loss: 0.008446124382317066 = 0.0017785708187147975 + 0.001 * 6.667553424835205
Epoch 1240, val loss: 1.1975607872009277
Epoch 1250, training loss: 0.00845204759389162 = 0.001752378884702921 + 0.001 * 6.6996684074401855
Epoch 1250, val loss: 1.1999086141586304
Epoch 1260, training loss: 0.008387668058276176 = 0.0017269166419282556 + 0.001 * 6.6607513427734375
Epoch 1260, val loss: 1.2022467851638794
Epoch 1270, training loss: 0.008369838818907738 = 0.0017021821113303304 + 0.001 * 6.667656421661377
Epoch 1270, val loss: 1.2045010328292847
Epoch 1280, training loss: 0.008346963673830032 = 0.0016781153390184045 + 0.001 * 6.668848037719727
Epoch 1280, val loss: 1.2067561149597168
Epoch 1290, training loss: 0.008318477310240269 = 0.0016546751139685512 + 0.001 * 6.663802146911621
Epoch 1290, val loss: 1.208979606628418
Epoch 1300, training loss: 0.008317762054502964 = 0.0016318288398906589 + 0.001 * 6.685932636260986
Epoch 1300, val loss: 1.2111645936965942
Epoch 1310, training loss: 0.008274239487946033 = 0.0016095638275146484 + 0.001 * 6.664675235748291
Epoch 1310, val loss: 1.2133280038833618
Epoch 1320, training loss: 0.008253023959696293 = 0.001587816746905446 + 0.001 * 6.6652069091796875
Epoch 1320, val loss: 1.2154638767242432
Epoch 1330, training loss: 0.008230739273130894 = 0.0015665742103010416 + 0.001 * 6.664165019989014
Epoch 1330, val loss: 1.2175625562667847
Epoch 1340, training loss: 0.008206970058381557 = 0.0015458111884072423 + 0.001 * 6.661158561706543
Epoch 1340, val loss: 1.2196273803710938
Epoch 1350, training loss: 0.008209381252527237 = 0.0015254919417202473 + 0.001 * 6.683889389038086
Epoch 1350, val loss: 1.2216989994049072
Epoch 1360, training loss: 0.008169983513653278 = 0.0015056491829454899 + 0.001 * 6.664333820343018
Epoch 1360, val loss: 1.2236980199813843
Epoch 1370, training loss: 0.008146269246935844 = 0.0014862091047689319 + 0.001 * 6.660059928894043
Epoch 1370, val loss: 1.2256900072097778
Epoch 1380, training loss: 0.008121214807033539 = 0.0014671446988359094 + 0.001 * 6.654069900512695
Epoch 1380, val loss: 1.2276421785354614
Epoch 1390, training loss: 0.008107724599540234 = 0.0014484405983239412 + 0.001 * 6.6592841148376465
Epoch 1390, val loss: 1.2296128273010254
Epoch 1400, training loss: 0.008093489333987236 = 0.0014300844632089138 + 0.001 * 6.66340446472168
Epoch 1400, val loss: 1.2315040826797485
Epoch 1410, training loss: 0.008066610433161259 = 0.001412143581546843 + 0.001 * 6.654466152191162
Epoch 1410, val loss: 1.233417272567749
Epoch 1420, training loss: 0.008048737421631813 = 0.0013945328537374735 + 0.001 * 6.65420389175415
Epoch 1420, val loss: 1.2353168725967407
Epoch 1430, training loss: 0.008041031658649445 = 0.0013772532111033797 + 0.001 * 6.663778305053711
Epoch 1430, val loss: 1.2371994256973267
Epoch 1440, training loss: 0.008015819825232029 = 0.001360293128527701 + 0.001 * 6.655526161193848
Epoch 1440, val loss: 1.2390213012695312
Epoch 1450, training loss: 0.007993131875991821 = 0.001343645271845162 + 0.001 * 6.649486064910889
Epoch 1450, val loss: 1.2408536672592163
Epoch 1460, training loss: 0.00797528587281704 = 0.0013273084769025445 + 0.001 * 6.647976875305176
Epoch 1460, val loss: 1.2426480054855347
Epoch 1470, training loss: 0.007953708991408348 = 0.0013112550368532538 + 0.001 * 6.642453670501709
Epoch 1470, val loss: 1.2444450855255127
Epoch 1480, training loss: 0.007926275953650475 = 0.001295511145144701 + 0.001 * 6.630764484405518
Epoch 1480, val loss: 1.2461862564086914
Epoch 1490, training loss: 0.007919293828308582 = 0.0012800657423213124 + 0.001 * 6.639227867126465
Epoch 1490, val loss: 1.247961401939392
Epoch 1500, training loss: 0.007921310141682625 = 0.0012649395503103733 + 0.001 * 6.656370162963867
Epoch 1500, val loss: 1.2496668100357056
Epoch 1510, training loss: 0.007885456085205078 = 0.0012501999735832214 + 0.001 * 6.635255813598633
Epoch 1510, val loss: 1.2513715028762817
Epoch 1520, training loss: 0.007890709675848484 = 0.0012357775121927261 + 0.001 * 6.654932022094727
Epoch 1520, val loss: 1.2530303001403809
Epoch 1530, training loss: 0.007845710963010788 = 0.0012216269969940186 + 0.001 * 6.624083042144775
Epoch 1530, val loss: 1.2547242641448975
Epoch 1540, training loss: 0.00785458367317915 = 0.0012077701976522803 + 0.001 * 6.64681339263916
Epoch 1540, val loss: 1.256347417831421
Epoch 1550, training loss: 0.007842713966965675 = 0.0011941984994336963 + 0.001 * 6.648515224456787
Epoch 1550, val loss: 1.257969617843628
Epoch 1560, training loss: 0.007815620861947536 = 0.0011809071293100715 + 0.001 * 6.634713172912598
Epoch 1560, val loss: 1.2595850229263306
Epoch 1570, training loss: 0.00784592516720295 = 0.0011678854934871197 + 0.001 * 6.678039073944092
Epoch 1570, val loss: 1.261197566986084
Epoch 1580, training loss: 0.007774983067065477 = 0.0011551653733476996 + 0.001 * 6.61981725692749
Epoch 1580, val loss: 1.2627698183059692
Epoch 1590, training loss: 0.007785587105900049 = 0.0011427394347265363 + 0.001 * 6.642847537994385
Epoch 1590, val loss: 1.2643429040908813
Epoch 1600, training loss: 0.007758801802992821 = 0.0011305742664262652 + 0.001 * 6.628227233886719
Epoch 1600, val loss: 1.2658679485321045
Epoch 1610, training loss: 0.007754986174404621 = 0.001118664862588048 + 0.001 * 6.636321067810059
Epoch 1610, val loss: 1.267404317855835
Epoch 1620, training loss: 0.007756200153380632 = 0.0011070024920627475 + 0.001 * 6.649197101593018
Epoch 1620, val loss: 1.2688716650009155
Epoch 1630, training loss: 0.007710783742368221 = 0.001095599145628512 + 0.001 * 6.615184307098389
Epoch 1630, val loss: 1.2703858613967896
Epoch 1640, training loss: 0.007695467676967382 = 0.0010844640200957656 + 0.001 * 6.611003398895264
Epoch 1640, val loss: 1.2718523740768433
Epoch 1650, training loss: 0.00769882183521986 = 0.0010735649848356843 + 0.001 * 6.625256538391113
Epoch 1650, val loss: 1.2732733488082886
Epoch 1660, training loss: 0.0076831975020468235 = 0.001062902738340199 + 0.001 * 6.620294570922852
Epoch 1660, val loss: 1.274733543395996
Epoch 1670, training loss: 0.007695287000387907 = 0.0010524512035772204 + 0.001 * 6.6428351402282715
Epoch 1670, val loss: 1.2761321067810059
Epoch 1680, training loss: 0.00765697006136179 = 0.0010422345949336886 + 0.001 * 6.614735126495361
Epoch 1680, val loss: 1.2775462865829468
Epoch 1690, training loss: 0.007709890138357878 = 0.0010322537273168564 + 0.001 * 6.67763614654541
Epoch 1690, val loss: 1.2789230346679688
Epoch 1700, training loss: 0.0076235136948525906 = 0.001022552140057087 + 0.001 * 6.600961208343506
Epoch 1700, val loss: 1.280286431312561
Epoch 1710, training loss: 0.0076165879145264626 = 0.001013097120448947 + 0.001 * 6.603490352630615
Epoch 1710, val loss: 1.2816076278686523
Epoch 1720, training loss: 0.0076151322573423386 = 0.0010038623586297035 + 0.001 * 6.611269474029541
Epoch 1720, val loss: 1.282995343208313
Epoch 1730, training loss: 0.007612179033458233 = 0.000994846341200173 + 0.001 * 6.617332458496094
Epoch 1730, val loss: 1.2843396663665771
Epoch 1740, training loss: 0.00759586226195097 = 0.0009860758436843753 + 0.001 * 6.609786033630371
Epoch 1740, val loss: 1.2855864763259888
Epoch 1750, training loss: 0.007596160750836134 = 0.0009774983627721667 + 0.001 * 6.618661880493164
Epoch 1750, val loss: 1.286872148513794
Epoch 1760, training loss: 0.007568785455077887 = 0.0009690983570180833 + 0.001 * 6.599686622619629
Epoch 1760, val loss: 1.2881137132644653
Epoch 1770, training loss: 0.007561505306512117 = 0.0009609069093130529 + 0.001 * 6.600597858428955
Epoch 1770, val loss: 1.2893903255462646
Epoch 1780, training loss: 0.00756062800064683 = 0.000952933740336448 + 0.001 * 6.607693672180176
Epoch 1780, val loss: 1.290585994720459
Epoch 1790, training loss: 0.007535174489021301 = 0.0009451297810301185 + 0.001 * 6.5900444984436035
Epoch 1790, val loss: 1.2918319702148438
Epoch 1800, training loss: 0.007540066726505756 = 0.0009375109802931547 + 0.001 * 6.602555751800537
Epoch 1800, val loss: 1.2930307388305664
Epoch 1810, training loss: 0.007525458000600338 = 0.000930075766518712 + 0.001 * 6.595382213592529
Epoch 1810, val loss: 1.294175624847412
Epoch 1820, training loss: 0.0075253271497786045 = 0.0009228195413015783 + 0.001 * 6.6025071144104
Epoch 1820, val loss: 1.2953276634216309
Epoch 1830, training loss: 0.007502026855945587 = 0.0009157152380794287 + 0.001 * 6.586311340332031
Epoch 1830, val loss: 1.2965120077133179
Epoch 1840, training loss: 0.007503332570195198 = 0.0009087687940336764 + 0.001 * 6.5945634841918945
Epoch 1840, val loss: 1.2976603507995605
Epoch 1850, training loss: 0.007490700576454401 = 0.0009019747376441956 + 0.001 * 6.588725566864014
Epoch 1850, val loss: 1.2987693548202515
Epoch 1860, training loss: 0.007501250132918358 = 0.0008953142678365111 + 0.001 * 6.605935573577881
Epoch 1860, val loss: 1.2998634576797485
Epoch 1870, training loss: 0.007481378968805075 = 0.0008887936128303409 + 0.001 * 6.59258508682251
Epoch 1870, val loss: 1.3009828329086304
Epoch 1880, training loss: 0.007471390999853611 = 0.0008824139949865639 + 0.001 * 6.5889763832092285
Epoch 1880, val loss: 1.3020355701446533
Epoch 1890, training loss: 0.007471255958080292 = 0.000876199861522764 + 0.001 * 6.59505558013916
Epoch 1890, val loss: 1.3031281232833862
Epoch 1900, training loss: 0.007474489975720644 = 0.0008701363694854081 + 0.001 * 6.604352951049805
Epoch 1900, val loss: 1.304113507270813
Epoch 1910, training loss: 0.007437911815941334 = 0.0008642042521387339 + 0.001 * 6.573707103729248
Epoch 1910, val loss: 1.3051679134368896
Epoch 1920, training loss: 0.007435288280248642 = 0.0008583781309425831 + 0.001 * 6.576910018920898
Epoch 1920, val loss: 1.306189775466919
Epoch 1930, training loss: 0.007430035620927811 = 0.0008526728488504887 + 0.001 * 6.577362537384033
Epoch 1930, val loss: 1.3071779012680054
Epoch 1940, training loss: 0.007425497751682997 = 0.0008470735629089177 + 0.001 * 6.578423500061035
Epoch 1940, val loss: 1.3082033395767212
Epoch 1950, training loss: 0.007424631621688604 = 0.0008415874908678234 + 0.001 * 6.583044052124023
Epoch 1950, val loss: 1.3091739416122437
Epoch 1960, training loss: 0.007434802129864693 = 0.0008362065418623388 + 0.001 * 6.598595142364502
Epoch 1960, val loss: 1.3101093769073486
Epoch 1970, training loss: 0.007415361236780882 = 0.0008309486438520253 + 0.001 * 6.584412097930908
Epoch 1970, val loss: 1.3111298084259033
Epoch 1980, training loss: 0.007409192621707916 = 0.0008257909212261438 + 0.001 * 6.583401203155518
Epoch 1980, val loss: 1.3120344877243042
Epoch 1990, training loss: 0.007398486603051424 = 0.0008207285427488387 + 0.001 * 6.577757835388184
Epoch 1990, val loss: 1.3130342960357666
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5609
Flip ASR: 0.4800/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9511849880218506 = 1.942811131477356 + 0.001 * 8.373833656311035
Epoch 0, val loss: 1.9389933347702026
Epoch 10, training loss: 1.9410874843597412 = 1.9327137470245361 + 0.001 * 8.373724937438965
Epoch 10, val loss: 1.9289121627807617
Epoch 20, training loss: 1.9284722805023193 = 1.920098900794983 + 0.001 * 8.373438835144043
Epoch 20, val loss: 1.9157003164291382
Epoch 30, training loss: 1.9106857776641846 = 1.9023128747940063 + 0.001 * 8.37287712097168
Epoch 30, val loss: 1.8965356349945068
Epoch 40, training loss: 1.8843717575073242 = 1.8760000467300415 + 0.001 * 8.371659278869629
Epoch 40, val loss: 1.8682570457458496
Epoch 50, training loss: 1.8472884893417358 = 1.83892023563385 + 0.001 * 8.368270874023438
Epoch 50, val loss: 1.8297522068023682
Epoch 60, training loss: 1.8038010597229004 = 1.7954457998275757 + 0.001 * 8.355264663696289
Epoch 60, val loss: 1.7882606983184814
Epoch 70, training loss: 1.761100172996521 = 1.7528153657913208 + 0.001 * 8.284791946411133
Epoch 70, val loss: 1.7518452405929565
Epoch 80, training loss: 1.7081105709075928 = 1.7002252340316772 + 0.001 * 7.885351181030273
Epoch 80, val loss: 1.707123041152954
Epoch 90, training loss: 1.635332465171814 = 1.627724528312683 + 0.001 * 7.607913017272949
Epoch 90, val loss: 1.6446229219436646
Epoch 100, training loss: 1.5405457019805908 = 1.5331403017044067 + 0.001 * 7.405364513397217
Epoch 100, val loss: 1.5643106698989868
Epoch 110, training loss: 1.4314371347427368 = 1.4241949319839478 + 0.001 * 7.242177486419678
Epoch 110, val loss: 1.4744442701339722
Epoch 120, training loss: 1.3184704780578613 = 1.3112404346466064 + 0.001 * 7.2300872802734375
Epoch 120, val loss: 1.3833558559417725
Epoch 130, training loss: 1.2057727575302124 = 1.198571801185608 + 0.001 * 7.200897693634033
Epoch 130, val loss: 1.2961876392364502
Epoch 140, training loss: 1.096692681312561 = 1.0895239114761353 + 0.001 * 7.168763160705566
Epoch 140, val loss: 1.2140759229660034
Epoch 150, training loss: 0.9951090216636658 = 0.987982451915741 + 0.001 * 7.126549243927002
Epoch 150, val loss: 1.1388472318649292
Epoch 160, training loss: 0.9027227759361267 = 0.8956478834152222 + 0.001 * 7.07489013671875
Epoch 160, val loss: 1.0705777406692505
Epoch 170, training loss: 0.8199469447135925 = 0.8129211664199829 + 0.001 * 7.025752067565918
Epoch 170, val loss: 1.0099319219589233
Epoch 180, training loss: 0.7472561001777649 = 0.7402656078338623 + 0.001 * 6.990487575531006
Epoch 180, val loss: 0.9575287103652954
Epoch 190, training loss: 0.6842911839485168 = 0.677327573299408 + 0.001 * 6.963630199432373
Epoch 190, val loss: 0.9138452410697937
Epoch 200, training loss: 0.6290714144706726 = 0.6221327185630798 + 0.001 * 6.93869161605835
Epoch 200, val loss: 0.8781235218048096
Epoch 210, training loss: 0.5792432427406311 = 0.5723304748535156 + 0.001 * 6.912790298461914
Epoch 210, val loss: 0.8489144444465637
Epoch 220, training loss: 0.5332169532775879 = 0.5263235569000244 + 0.001 * 6.8933820724487305
Epoch 220, val loss: 0.8251282572746277
Epoch 230, training loss: 0.49013659358024597 = 0.4832521080970764 + 0.001 * 6.8844780921936035
Epoch 230, val loss: 0.8059463500976562
Epoch 240, training loss: 0.4493831694126129 = 0.4425046145915985 + 0.001 * 6.878551483154297
Epoch 240, val loss: 0.790913462638855
Epoch 250, training loss: 0.41032376885414124 = 0.4034472107887268 + 0.001 * 6.8765435218811035
Epoch 250, val loss: 0.779360294342041
Epoch 260, training loss: 0.3725438416004181 = 0.3656684458255768 + 0.001 * 6.875382900238037
Epoch 260, val loss: 0.7706965208053589
Epoch 270, training loss: 0.33582502603530884 = 0.32895082235336304 + 0.001 * 6.874197483062744
Epoch 270, val loss: 0.7646390199661255
Epoch 280, training loss: 0.30028197169303894 = 0.29340919852256775 + 0.001 * 6.872762203216553
Epoch 280, val loss: 0.7609863877296448
Epoch 290, training loss: 0.26632165908813477 = 0.25945061445236206 + 0.001 * 6.871029853820801
Epoch 290, val loss: 0.7598682641983032
Epoch 300, training loss: 0.2344697266817093 = 0.22760063409805298 + 0.001 * 6.869091987609863
Epoch 300, val loss: 0.7611817121505737
Epoch 310, training loss: 0.20530059933662415 = 0.19843292236328125 + 0.001 * 6.867677211761475
Epoch 310, val loss: 0.765123188495636
Epoch 320, training loss: 0.17911763489246368 = 0.17225287854671478 + 0.001 * 6.86475944519043
Epoch 320, val loss: 0.7717702388763428
Epoch 330, training loss: 0.15600371360778809 = 0.14914198219776154 + 0.001 * 6.861728668212891
Epoch 330, val loss: 0.781102180480957
Epoch 340, training loss: 0.13585887849330902 = 0.1290009468793869 + 0.001 * 6.857934951782227
Epoch 340, val loss: 0.7930058240890503
Epoch 350, training loss: 0.11847130954265594 = 0.1116173267364502 + 0.001 * 6.853985786437988
Epoch 350, val loss: 0.8074719309806824
Epoch 360, training loss: 0.10356700420379639 = 0.09671430289745331 + 0.001 * 6.852704048156738
Epoch 360, val loss: 0.8239805102348328
Epoch 370, training loss: 0.09090536832809448 = 0.0840611457824707 + 0.001 * 6.844220161437988
Epoch 370, val loss: 0.8421165347099304
Epoch 380, training loss: 0.08021066337823868 = 0.07337448000907898 + 0.001 * 6.836180686950684
Epoch 380, val loss: 0.8615503311157227
Epoch 390, training loss: 0.07123150676488876 = 0.06436482071876526 + 0.001 * 6.866688251495361
Epoch 390, val loss: 0.8818249106407166
Epoch 400, training loss: 0.06358438730239868 = 0.056761324405670166 + 0.001 * 6.823061466217041
Epoch 400, val loss: 0.9024531841278076
Epoch 410, training loss: 0.057139698415994644 = 0.05032582953572273 + 0.001 * 6.813867092132568
Epoch 410, val loss: 0.9228917360305786
Epoch 420, training loss: 0.05165848881006241 = 0.044853128492832184 + 0.001 * 6.805359840393066
Epoch 420, val loss: 0.9429094195365906
Epoch 430, training loss: 0.04698222130537033 = 0.04017314314842224 + 0.001 * 6.809078693389893
Epoch 430, val loss: 0.9623145461082458
Epoch 440, training loss: 0.04295584559440613 = 0.036154888570308685 + 0.001 * 6.800957202911377
Epoch 440, val loss: 0.9809702634811401
Epoch 450, training loss: 0.03947371989488602 = 0.03268643096089363 + 0.001 * 6.787289142608643
Epoch 450, val loss: 0.9989410638809204
Epoch 460, training loss: 0.03645724803209305 = 0.029675232246518135 + 0.001 * 6.782013893127441
Epoch 460, val loss: 1.0162510871887207
Epoch 470, training loss: 0.03383252024650574 = 0.02704673819243908 + 0.001 * 6.785782814025879
Epoch 470, val loss: 1.0329114198684692
Epoch 480, training loss: 0.03151179850101471 = 0.024741480126976967 + 0.001 * 6.7703166007995605
Epoch 480, val loss: 1.0490885972976685
Epoch 490, training loss: 0.0294799841940403 = 0.02271248586475849 + 0.001 * 6.767497539520264
Epoch 490, val loss: 1.0646908283233643
Epoch 500, training loss: 0.027686376124620438 = 0.020919552072882652 + 0.001 * 6.766823768615723
Epoch 500, val loss: 1.0797609090805054
Epoch 510, training loss: 0.026095310226082802 = 0.01932918094098568 + 0.001 * 6.766129016876221
Epoch 510, val loss: 1.0943328142166138
Epoch 520, training loss: 0.024674532935023308 = 0.017913108691573143 + 0.001 * 6.7614240646362305
Epoch 520, val loss: 1.1084215641021729
Epoch 530, training loss: 0.023410893976688385 = 0.016646506264805794 + 0.001 * 6.764387130737305
Epoch 530, val loss: 1.1220753192901611
Epoch 540, training loss: 0.02226661518216133 = 0.015509825199842453 + 0.001 * 6.756789207458496
Epoch 540, val loss: 1.1353198289871216
Epoch 550, training loss: 0.021253038197755814 = 0.01448644045740366 + 0.001 * 6.766597270965576
Epoch 550, val loss: 1.148146390914917
Epoch 560, training loss: 0.020319536328315735 = 0.013562388718128204 + 0.001 * 6.75714635848999
Epoch 560, val loss: 1.1605740785598755
Epoch 570, training loss: 0.019479947164654732 = 0.012725583277642727 + 0.001 * 6.754364013671875
Epoch 570, val loss: 1.1725904941558838
Epoch 580, training loss: 0.018717527389526367 = 0.011965754441916943 + 0.001 * 6.751771926879883
Epoch 580, val loss: 1.184242606163025
Epoch 590, training loss: 0.018026502802968025 = 0.011273768730461597 + 0.001 * 6.75273323059082
Epoch 590, val loss: 1.1955740451812744
Epoch 600, training loss: 0.01738773100078106 = 0.010641531087458134 + 0.001 * 6.746199131011963
Epoch 600, val loss: 1.206536889076233
Epoch 610, training loss: 0.01681242696940899 = 0.010062828660011292 + 0.001 * 6.749598503112793
Epoch 610, val loss: 1.2172132730484009
Epoch 620, training loss: 0.016280243173241615 = 0.009532058611512184 + 0.001 * 6.748184680938721
Epoch 620, val loss: 1.2275747060775757
Epoch 630, training loss: 0.01578686572611332 = 0.009044223465025425 + 0.001 * 6.742641448974609
Epoch 630, val loss: 1.237638235092163
Epoch 640, training loss: 0.01533717941492796 = 0.008594916202127934 + 0.001 * 6.742262840270996
Epoch 640, val loss: 1.2474253177642822
Epoch 650, training loss: 0.014930585399270058 = 0.00818018987774849 + 0.001 * 6.750394821166992
Epoch 650, val loss: 1.2569262981414795
Epoch 660, training loss: 0.014537030830979347 = 0.007796502206474543 + 0.001 * 6.740527629852295
Epoch 660, val loss: 1.2661750316619873
Epoch 670, training loss: 0.014179807156324387 = 0.007440723478794098 + 0.001 * 6.739083290100098
Epoch 670, val loss: 1.2751928567886353
Epoch 680, training loss: 0.013852647505700588 = 0.007110134698450565 + 0.001 * 6.7425127029418945
Epoch 680, val loss: 1.2839581966400146
Epoch 690, training loss: 0.013536538928747177 = 0.00680259196087718 + 0.001 * 6.733947277069092
Epoch 690, val loss: 1.2925033569335938
Epoch 700, training loss: 0.01328316330909729 = 0.006516046356409788 + 0.001 * 6.767116546630859
Epoch 700, val loss: 1.300848126411438
Epoch 710, training loss: 0.012982863932847977 = 0.0062486398965120316 + 0.001 * 6.734224319458008
Epoch 710, val loss: 1.308974027633667
Epoch 720, training loss: 0.012726856395602226 = 0.005998730659484863 + 0.001 * 6.728126049041748
Epoch 720, val loss: 1.3169054985046387
Epoch 730, training loss: 0.012496601790189743 = 0.005764754023402929 + 0.001 * 6.731846809387207
Epoch 730, val loss: 1.3246310949325562
Epoch 740, training loss: 0.012279088608920574 = 0.005545397754758596 + 0.001 * 6.7336907386779785
Epoch 740, val loss: 1.33217453956604
Epoch 750, training loss: 0.012071806937456131 = 0.005339489318430424 + 0.001 * 6.732317924499512
Epoch 750, val loss: 1.3395500183105469
Epoch 760, training loss: 0.011887460947036743 = 0.005145926959812641 + 0.001 * 6.741533279418945
Epoch 760, val loss: 1.3467516899108887
Epoch 770, training loss: 0.011699028313159943 = 0.004963794723153114 + 0.001 * 6.735233783721924
Epoch 770, val loss: 1.3537923097610474
Epoch 780, training loss: 0.011515137739479542 = 0.0047921668738126755 + 0.001 * 6.722970485687256
Epoch 780, val loss: 1.36067795753479
Epoch 790, training loss: 0.011351099237799644 = 0.004630246199667454 + 0.001 * 6.720853328704834
Epoch 790, val loss: 1.3674136400222778
Epoch 800, training loss: 0.01119907945394516 = 0.004477327689528465 + 0.001 * 6.7217512130737305
Epoch 800, val loss: 1.373996376991272
Epoch 810, training loss: 0.01107214204967022 = 0.0043327356688678265 + 0.001 * 6.739406585693359
Epoch 810, val loss: 1.3804316520690918
Epoch 820, training loss: 0.01092943362891674 = 0.004195920191705227 + 0.001 * 6.733513355255127
Epoch 820, val loss: 1.386723279953003
Epoch 830, training loss: 0.01078836526721716 = 0.004066281020641327 + 0.001 * 6.722084045410156
Epoch 830, val loss: 1.392878770828247
Epoch 840, training loss: 0.010659781284630299 = 0.003943371586501598 + 0.001 * 6.716409206390381
Epoch 840, val loss: 1.3989098072052002
Epoch 850, training loss: 0.01054173894226551 = 0.003826720407232642 + 0.001 * 6.715018272399902
Epoch 850, val loss: 1.4047998189926147
Epoch 860, training loss: 0.01042899489402771 = 0.003715904662385583 + 0.001 * 6.713089466094971
Epoch 860, val loss: 1.410569667816162
Epoch 870, training loss: 0.010319264605641365 = 0.003610534593462944 + 0.001 * 6.708730220794678
Epoch 870, val loss: 1.4162201881408691
Epoch 880, training loss: 0.010221503674983978 = 0.0035102677065879107 + 0.001 * 6.711235523223877
Epoch 880, val loss: 1.4217702150344849
Epoch 890, training loss: 0.010117298923432827 = 0.003414795035496354 + 0.001 * 6.702503681182861
Epoch 890, val loss: 1.4272170066833496
Epoch 900, training loss: 0.010026532225310802 = 0.003323785262182355 + 0.001 * 6.702746391296387
Epoch 900, val loss: 1.4325419664382935
Epoch 910, training loss: 0.009956777095794678 = 0.0032369832042604685 + 0.001 * 6.719793319702148
Epoch 910, val loss: 1.4377599954605103
Epoch 920, training loss: 0.009854615665972233 = 0.0031541225034743547 + 0.001 * 6.7004923820495605
Epoch 920, val loss: 1.4428950548171997
Epoch 930, training loss: 0.009777693077921867 = 0.0030749375000596046 + 0.001 * 6.702755928039551
Epoch 930, val loss: 1.4479197263717651
Epoch 940, training loss: 0.009696146473288536 = 0.0029991159681230783 + 0.001 * 6.697030067443848
Epoch 940, val loss: 1.4528472423553467
Epoch 950, training loss: 0.00962420366704464 = 0.0029265282209962606 + 0.001 * 6.6976752281188965
Epoch 950, val loss: 1.4577150344848633
Epoch 960, training loss: 0.009587078355252743 = 0.0028568634297698736 + 0.001 * 6.730214595794678
Epoch 960, val loss: 1.4624756574630737
Epoch 970, training loss: 0.009483367204666138 = 0.0027898955158889294 + 0.001 * 6.6934709548950195
Epoch 970, val loss: 1.4671411514282227
Epoch 980, training loss: 0.009414952248334885 = 0.0027253194712102413 + 0.001 * 6.689632892608643
Epoch 980, val loss: 1.4717456102371216
Epoch 990, training loss: 0.009356608614325523 = 0.002662892686203122 + 0.001 * 6.693716049194336
Epoch 990, val loss: 1.4762699604034424
Epoch 1000, training loss: 0.009321351535618305 = 0.0026024116668850183 + 0.001 * 6.718939781188965
Epoch 1000, val loss: 1.4807368516921997
Epoch 1010, training loss: 0.009258870035409927 = 0.002543726935982704 + 0.001 * 6.715142250061035
Epoch 1010, val loss: 1.4851378202438354
Epoch 1020, training loss: 0.009200869128108025 = 0.002486670622602105 + 0.001 * 6.714198112487793
Epoch 1020, val loss: 1.4894559383392334
Epoch 1030, training loss: 0.009129535406827927 = 0.002431250875815749 + 0.001 * 6.698284149169922
Epoch 1030, val loss: 1.4937635660171509
Epoch 1040, training loss: 0.009077176451683044 = 0.002377293538302183 + 0.001 * 6.6998820304870605
Epoch 1040, val loss: 1.4980010986328125
Epoch 1050, training loss: 0.00902588665485382 = 0.002324821427464485 + 0.001 * 6.701065540313721
Epoch 1050, val loss: 1.5021849870681763
Epoch 1060, training loss: 0.008969771675765514 = 0.0022737702820450068 + 0.001 * 6.696001052856445
Epoch 1060, val loss: 1.5062929391860962
Epoch 1070, training loss: 0.00891248881816864 = 0.002224218798801303 + 0.001 * 6.68826961517334
Epoch 1070, val loss: 1.5103585720062256
Epoch 1080, training loss: 0.008879244327545166 = 0.002176100853830576 + 0.001 * 6.70314359664917
Epoch 1080, val loss: 1.5143183469772339
Epoch 1090, training loss: 0.008807693608105183 = 0.0021294422913342714 + 0.001 * 6.678250789642334
Epoch 1090, val loss: 1.5182706117630005
Epoch 1100, training loss: 0.008772346191108227 = 0.0020841937512159348 + 0.001 * 6.688152313232422
Epoch 1100, val loss: 1.5221694707870483
Epoch 1110, training loss: 0.008710213005542755 = 0.002040440682321787 + 0.001 * 6.669772148132324
Epoch 1110, val loss: 1.5259686708450317
Epoch 1120, training loss: 0.008669083006680012 = 0.0019980769138783216 + 0.001 * 6.671005725860596
Epoch 1120, val loss: 1.5297285318374634
Epoch 1130, training loss: 0.008629748597741127 = 0.001957107102498412 + 0.001 * 6.672640800476074
Epoch 1130, val loss: 1.5334292650222778
Epoch 1140, training loss: 0.008587962947785854 = 0.0019175315974280238 + 0.001 * 6.670431137084961
Epoch 1140, val loss: 1.5370817184448242
Epoch 1150, training loss: 0.008589360862970352 = 0.0018793229246512055 + 0.001 * 6.710037708282471
Epoch 1150, val loss: 1.5406785011291504
Epoch 1160, training loss: 0.00853191502392292 = 0.0018424536101520061 + 0.001 * 6.689461708068848
Epoch 1160, val loss: 1.544202208518982
Epoch 1170, training loss: 0.008521974086761475 = 0.0018068683566525578 + 0.001 * 6.715105056762695
Epoch 1170, val loss: 1.547680377960205
Epoch 1180, training loss: 0.00844794511795044 = 0.0017724687932059169 + 0.001 * 6.675476551055908
Epoch 1180, val loss: 1.5510977506637573
Epoch 1190, training loss: 0.008438811637461185 = 0.001739281928166747 + 0.001 * 6.699529647827148
Epoch 1190, val loss: 1.5544434785842896
Epoch 1200, training loss: 0.008384211920201778 = 0.0017071975162252784 + 0.001 * 6.677013874053955
Epoch 1200, val loss: 1.5577393770217896
Epoch 1210, training loss: 0.008363311178982258 = 0.0016762305749580264 + 0.001 * 6.6870808601379395
Epoch 1210, val loss: 1.5609993934631348
Epoch 1220, training loss: 0.00830760970711708 = 0.0016463208012282848 + 0.001 * 6.661288738250732
Epoch 1220, val loss: 1.564197063446045
Epoch 1230, training loss: 0.008285114541649818 = 0.0016174846095964313 + 0.001 * 6.667629718780518
Epoch 1230, val loss: 1.5673454999923706
Epoch 1240, training loss: 0.008248033933341503 = 0.0015896445838734508 + 0.001 * 6.658389091491699
Epoch 1240, val loss: 1.5704398155212402
Epoch 1250, training loss: 0.008220084011554718 = 0.0015627442626282573 + 0.001 * 6.657339572906494
Epoch 1250, val loss: 1.5734792947769165
Epoch 1260, training loss: 0.008186882361769676 = 0.0015367346350103617 + 0.001 * 6.650147438049316
Epoch 1260, val loss: 1.5764684677124023
Epoch 1270, training loss: 0.008155900985002518 = 0.0015115669229999185 + 0.001 * 6.644333362579346
Epoch 1270, val loss: 1.579407811164856
Epoch 1280, training loss: 0.008133936673402786 = 0.00148722471203655 + 0.001 * 6.646711349487305
Epoch 1280, val loss: 1.582295298576355
Epoch 1290, training loss: 0.008121803402900696 = 0.0014636985724791884 + 0.001 * 6.65810489654541
Epoch 1290, val loss: 1.5851479768753052
Epoch 1300, training loss: 0.008083121851086617 = 0.0014409737195819616 + 0.001 * 6.642148017883301
Epoch 1300, val loss: 1.5879334211349487
Epoch 1310, training loss: 0.008085308596491814 = 0.0014190244255587459 + 0.001 * 6.66628360748291
Epoch 1310, val loss: 1.5906684398651123
Epoch 1320, training loss: 0.008062245324254036 = 0.0013978163478896022 + 0.001 * 6.664429187774658
Epoch 1320, val loss: 1.5934010744094849
Epoch 1330, training loss: 0.008035666309297085 = 0.0013772909296676517 + 0.001 * 6.658374786376953
Epoch 1330, val loss: 1.5960426330566406
Epoch 1340, training loss: 0.008015187457203865 = 0.0013574324548244476 + 0.001 * 6.657754898071289
Epoch 1340, val loss: 1.5986777544021606
Epoch 1350, training loss: 0.007973069325089455 = 0.0013381713069975376 + 0.001 * 6.634897708892822
Epoch 1350, val loss: 1.6012431383132935
Epoch 1360, training loss: 0.007969429716467857 = 0.0013195183128118515 + 0.001 * 6.649910926818848
Epoch 1360, val loss: 1.6037743091583252
Epoch 1370, training loss: 0.007947598583996296 = 0.0013014475116506219 + 0.001 * 6.646151065826416
Epoch 1370, val loss: 1.6062567234039307
Epoch 1380, training loss: 0.007930007763206959 = 0.0012839349219575524 + 0.001 * 6.6460723876953125
Epoch 1380, val loss: 1.608695387840271
Epoch 1390, training loss: 0.007920373231172562 = 0.0012669600546360016 + 0.001 * 6.65341329574585
Epoch 1390, val loss: 1.6111085414886475
Epoch 1400, training loss: 0.007904027588665485 = 0.0012504758778959513 + 0.001 * 6.6535515785217285
Epoch 1400, val loss: 1.6134388446807861
Epoch 1410, training loss: 0.007882639765739441 = 0.001234482042491436 + 0.001 * 6.648157119750977
Epoch 1410, val loss: 1.6157324314117432
Epoch 1420, training loss: 0.007848955690860748 = 0.0012190095148980618 + 0.001 * 6.629946231842041
Epoch 1420, val loss: 1.6180312633514404
Epoch 1430, training loss: 0.007838532328605652 = 0.0012039998546242714 + 0.001 * 6.6345319747924805
Epoch 1430, val loss: 1.6202784776687622
Epoch 1440, training loss: 0.007822038605809212 = 0.0011894293129444122 + 0.001 * 6.632608413696289
Epoch 1440, val loss: 1.622496485710144
Epoch 1450, training loss: 0.007793891709297895 = 0.0011753090657293797 + 0.001 * 6.618582248687744
Epoch 1450, val loss: 1.6246752738952637
Epoch 1460, training loss: 0.007794761564582586 = 0.001161587075330317 + 0.001 * 6.633174419403076
Epoch 1460, val loss: 1.6268088817596436
Epoch 1470, training loss: 0.007776299491524696 = 0.0011482572881504893 + 0.001 * 6.628041744232178
Epoch 1470, val loss: 1.6289247274398804
Epoch 1480, training loss: 0.007789860479533672 = 0.001135318772867322 + 0.001 * 6.654541015625
Epoch 1480, val loss: 1.630991816520691
Epoch 1490, training loss: 0.007762696593999863 = 0.0011227275244891644 + 0.001 * 6.6399688720703125
Epoch 1490, val loss: 1.6330498456954956
Epoch 1500, training loss: 0.007756786420941353 = 0.001110489945858717 + 0.001 * 6.64629602432251
Epoch 1500, val loss: 1.6350570917129517
Epoch 1510, training loss: 0.00774336839094758 = 0.0010985647095367312 + 0.001 * 6.644803047180176
Epoch 1510, val loss: 1.637046217918396
Epoch 1520, training loss: 0.00771440751850605 = 0.00108697812538594 + 0.001 * 6.627429008483887
Epoch 1520, val loss: 1.6389864683151245
Epoch 1530, training loss: 0.007704363204538822 = 0.001075725187547505 + 0.001 * 6.628637790679932
Epoch 1530, val loss: 1.6409237384796143
Epoch 1540, training loss: 0.007687077391892672 = 0.0010647866874933243 + 0.001 * 6.62229061126709
Epoch 1540, val loss: 1.6428155899047852
Epoch 1550, training loss: 0.007669148501008749 = 0.0010541382944211364 + 0.001 * 6.615009784698486
Epoch 1550, val loss: 1.6446782350540161
Epoch 1560, training loss: 0.0076668416149914265 = 0.0010437784949317575 + 0.001 * 6.623063087463379
Epoch 1560, val loss: 1.6465163230895996
Epoch 1570, training loss: 0.007641333155333996 = 0.0010337249841541052 + 0.001 * 6.607608318328857
Epoch 1570, val loss: 1.6483224630355835
Epoch 1580, training loss: 0.007633613422513008 = 0.0010239413240924478 + 0.001 * 6.6096720695495605
Epoch 1580, val loss: 1.6501038074493408
Epoch 1590, training loss: 0.007649530190974474 = 0.0010144211119040847 + 0.001 * 6.635108470916748
Epoch 1590, val loss: 1.6518760919570923
Epoch 1600, training loss: 0.007629652041941881 = 0.0010051485151052475 + 0.001 * 6.624503135681152
Epoch 1600, val loss: 1.653611660003662
Epoch 1610, training loss: 0.007617257069796324 = 0.0009960961760953069 + 0.001 * 6.621160507202148
Epoch 1610, val loss: 1.6553242206573486
Epoch 1620, training loss: 0.007626100908964872 = 0.0009873021626845002 + 0.001 * 6.638798713684082
Epoch 1620, val loss: 1.6570199728012085
Epoch 1630, training loss: 0.007613878231495619 = 0.0009787343442440033 + 0.001 * 6.635143756866455
Epoch 1630, val loss: 1.6586333513259888
Epoch 1640, training loss: 0.007580973207950592 = 0.0009703662362881005 + 0.001 * 6.610606670379639
Epoch 1640, val loss: 1.6602939367294312
Epoch 1650, training loss: 0.0075705572962760925 = 0.0009622052311897278 + 0.001 * 6.608351707458496
Epoch 1650, val loss: 1.6618874073028564
Epoch 1660, training loss: 0.007560015190392733 = 0.0009542616317048669 + 0.001 * 6.605753421783447
Epoch 1660, val loss: 1.6634893417358398
Epoch 1670, training loss: 0.007541668601334095 = 0.000946502317674458 + 0.001 * 6.595166206359863
Epoch 1670, val loss: 1.6650527715682983
Epoch 1680, training loss: 0.007533145137131214 = 0.000938937533646822 + 0.001 * 6.594207286834717
Epoch 1680, val loss: 1.6665812730789185
Epoch 1690, training loss: 0.007538532372564077 = 0.000931550923269242 + 0.001 * 6.60698127746582
Epoch 1690, val loss: 1.6680690050125122
Epoch 1700, training loss: 0.007524872664362192 = 0.0009243481908924878 + 0.001 * 6.600523948669434
Epoch 1700, val loss: 1.669542670249939
Epoch 1710, training loss: 0.007511533331125975 = 0.0009173174621537328 + 0.001 * 6.5942158699035645
Epoch 1710, val loss: 1.6710041761398315
Epoch 1720, training loss: 0.007511190604418516 = 0.0009104552445933223 + 0.001 * 6.600734710693359
Epoch 1720, val loss: 1.6724426746368408
Epoch 1730, training loss: 0.007513714022934437 = 0.0009037561248987913 + 0.001 * 6.609957695007324
Epoch 1730, val loss: 1.673851728439331
Epoch 1740, training loss: 0.007521369494497776 = 0.0008972116047516465 + 0.001 * 6.624157905578613
Epoch 1740, val loss: 1.6752597093582153
Epoch 1750, training loss: 0.007528959773480892 = 0.0008907881565392017 + 0.001 * 6.638171195983887
Epoch 1750, val loss: 1.6766201257705688
Epoch 1760, training loss: 0.0074957492761313915 = 0.0008845154079608619 + 0.001 * 6.611233711242676
Epoch 1760, val loss: 1.677990198135376
Epoch 1770, training loss: 0.007494667544960976 = 0.0008783559314906597 + 0.001 * 6.616311073303223
Epoch 1770, val loss: 1.6793272495269775
Epoch 1780, training loss: 0.007477076258510351 = 0.0008723544306121767 + 0.001 * 6.604721546173096
Epoch 1780, val loss: 1.6806586980819702
Epoch 1790, training loss: 0.007498474791646004 = 0.0008664643974043429 + 0.001 * 6.632009983062744
Epoch 1790, val loss: 1.6819522380828857
Epoch 1800, training loss: 0.007459788583219051 = 0.0008606850169599056 + 0.001 * 6.5991034507751465
Epoch 1800, val loss: 1.6832484006881714
Epoch 1810, training loss: 0.007437712978571653 = 0.0008550466736778617 + 0.001 * 6.582665920257568
Epoch 1810, val loss: 1.6845334768295288
Epoch 1820, training loss: 0.0074942526407539845 = 0.0008495273650623858 + 0.001 * 6.6447248458862305
Epoch 1820, val loss: 1.685791254043579
Epoch 1830, training loss: 0.00743742985650897 = 0.0008441283134743571 + 0.001 * 6.593301296234131
Epoch 1830, val loss: 1.6870113611221313
Epoch 1840, training loss: 0.007446096744388342 = 0.000838868843857199 + 0.001 * 6.607227802276611
Epoch 1840, val loss: 1.6882528066635132
Epoch 1850, training loss: 0.007448649033904076 = 0.0008336973842233419 + 0.001 * 6.614951133728027
Epoch 1850, val loss: 1.6894463300704956
Epoch 1860, training loss: 0.0074370382353663445 = 0.0008286680094897747 + 0.001 * 6.608369827270508
Epoch 1860, val loss: 1.690643072128296
Epoch 1870, training loss: 0.007419308181852102 = 0.0008236923604272306 + 0.001 * 6.595615386962891
Epoch 1870, val loss: 1.691828966140747
Epoch 1880, training loss: 0.007408137898892164 = 0.0008188519277609885 + 0.001 * 6.589285850524902
Epoch 1880, val loss: 1.692991018295288
Epoch 1890, training loss: 0.007396297063678503 = 0.0008140730787999928 + 0.001 * 6.582223415374756
Epoch 1890, val loss: 1.6941473484039307
Epoch 1900, training loss: 0.0073821283876895905 = 0.000809406628832221 + 0.001 * 6.572721481323242
Epoch 1900, val loss: 1.6952768564224243
Epoch 1910, training loss: 0.007373302709311247 = 0.0008048320887610316 + 0.001 * 6.568470001220703
Epoch 1910, val loss: 1.6964420080184937
Epoch 1920, training loss: 0.007374605629593134 = 0.0008003469556570053 + 0.001 * 6.574258327484131
Epoch 1920, val loss: 1.6975198984146118
Epoch 1930, training loss: 0.0073966230265796185 = 0.0007959693321026862 + 0.001 * 6.600653648376465
Epoch 1930, val loss: 1.6986714601516724
Epoch 1940, training loss: 0.0073868404142558575 = 0.000791664351709187 + 0.001 * 6.595175743103027
Epoch 1940, val loss: 1.6997337341308594
Epoch 1950, training loss: 0.007367285899817944 = 0.0007874672301113605 + 0.001 * 6.579818248748779
Epoch 1950, val loss: 1.7008354663848877
Epoch 1960, training loss: 0.007366381585597992 = 0.0007833325071260333 + 0.001 * 6.5830488204956055
Epoch 1960, val loss: 1.7018896341323853
Epoch 1970, training loss: 0.007345044985413551 = 0.0007792760152369738 + 0.001 * 6.565768718719482
Epoch 1970, val loss: 1.7029697895050049
Epoch 1980, training loss: 0.007370706181973219 = 0.0007752922247163951 + 0.001 * 6.595413684844971
Epoch 1980, val loss: 1.7040178775787354
Epoch 1990, training loss: 0.007332880049943924 = 0.0007713677478022873 + 0.001 * 6.561511993408203
Epoch 1990, val loss: 1.7050508260726929
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.8524
Flip ASR: 0.8222/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9455687999725342 = 1.937195062637329 + 0.001 * 8.373757362365723
Epoch 0, val loss: 1.9334267377853394
Epoch 10, training loss: 1.9360988140106201 = 1.9277251958847046 + 0.001 * 8.373600959777832
Epoch 10, val loss: 1.9239708185195923
Epoch 20, training loss: 1.9245691299438477 = 1.9161959886550903 + 0.001 * 8.373128890991211
Epoch 20, val loss: 1.9119305610656738
Epoch 30, training loss: 1.9085124731063843 = 1.900140404701233 + 0.001 * 8.372111320495605
Epoch 30, val loss: 1.8947196006774902
Epoch 40, training loss: 1.884835124015808 = 1.8764653205871582 + 0.001 * 8.369844436645508
Epoch 40, val loss: 1.869328498840332
Epoch 50, training loss: 1.8507649898529053 = 1.842401146888733 + 0.001 * 8.36388874053955
Epoch 50, val loss: 1.8338205814361572
Epoch 60, training loss: 1.8068821430206299 = 1.7985416650772095 + 0.001 * 8.340472221374512
Epoch 60, val loss: 1.790980339050293
Epoch 70, training loss: 1.7581812143325806 = 1.750011682510376 + 0.001 * 8.169489860534668
Epoch 70, val loss: 1.7478828430175781
Epoch 80, training loss: 1.7002440690994263 = 1.6927142143249512 + 0.001 * 7.52982234954834
Epoch 80, val loss: 1.6998000144958496
Epoch 90, training loss: 1.6250369548797607 = 1.6176691055297852 + 0.001 * 7.3679022789001465
Epoch 90, val loss: 1.6378425359725952
Epoch 100, training loss: 1.5302259922027588 = 1.5229228734970093 + 0.001 * 7.303072452545166
Epoch 100, val loss: 1.5606993436813354
Epoch 110, training loss: 1.4245647192001343 = 1.4173179864883423 + 0.001 * 7.246734619140625
Epoch 110, val loss: 1.4787789583206177
Epoch 120, training loss: 1.3181456327438354 = 1.3109409809112549 + 0.001 * 7.204633712768555
Epoch 120, val loss: 1.399811863899231
Epoch 130, training loss: 1.2146739959716797 = 1.2075046300888062 + 0.001 * 7.169416904449463
Epoch 130, val loss: 1.326683521270752
Epoch 140, training loss: 1.1126948595046997 = 1.105567455291748 + 0.001 * 7.127404689788818
Epoch 140, val loss: 1.25527822971344
Epoch 150, training loss: 1.0123872756958008 = 1.0053068399429321 + 0.001 * 7.080432891845703
Epoch 150, val loss: 1.18477201461792
Epoch 160, training loss: 0.9165069460868835 = 0.9094887375831604 + 0.001 * 7.018215179443359
Epoch 160, val loss: 1.1185095310211182
Epoch 170, training loss: 0.828959047794342 = 0.8219982385635376 + 0.001 * 6.960782051086426
Epoch 170, val loss: 1.060591697692871
Epoch 180, training loss: 0.7521886825561523 = 0.7452608346939087 + 0.001 * 6.927831172943115
Epoch 180, val loss: 1.0134681463241577
Epoch 190, training loss: 0.6859762072563171 = 0.6790887713432312 + 0.001 * 6.8874406814575195
Epoch 190, val loss: 0.9771077632904053
Epoch 200, training loss: 0.6283016204833984 = 0.6214525699615479 + 0.001 * 6.849041938781738
Epoch 200, val loss: 0.9497360587120056
Epoch 210, training loss: 0.5767689347267151 = 0.5699540376663208 + 0.001 * 6.814905643463135
Epoch 210, val loss: 0.9286618232727051
Epoch 220, training loss: 0.5294653177261353 = 0.5226768851280212 + 0.001 * 6.788442134857178
Epoch 220, val loss: 0.9118084907531738
Epoch 230, training loss: 0.4851950705051422 = 0.4784238040447235 + 0.001 * 6.77126407623291
Epoch 230, val loss: 0.8985185623168945
Epoch 240, training loss: 0.44315779209136963 = 0.43639498949050903 + 0.001 * 6.762794017791748
Epoch 240, val loss: 0.8891065120697021
Epoch 250, training loss: 0.4030308425426483 = 0.396270215511322 + 0.001 * 6.760628700256348
Epoch 250, val loss: 0.8840015530586243
Epoch 260, training loss: 0.3651432991027832 = 0.3583872318267822 + 0.001 * 6.7560601234436035
Epoch 260, val loss: 0.883283257484436
Epoch 270, training loss: 0.33009129762649536 = 0.3233374059200287 + 0.001 * 6.753878116607666
Epoch 270, val loss: 0.8866075277328491
Epoch 280, training loss: 0.29814520478248596 = 0.29139214754104614 + 0.001 * 6.753045558929443
Epoch 280, val loss: 0.8928936719894409
Epoch 290, training loss: 0.269167959690094 = 0.2624186873435974 + 0.001 * 6.74926233291626
Epoch 290, val loss: 0.9010428786277771
Epoch 300, training loss: 0.2427016943693161 = 0.23595313727855682 + 0.001 * 6.748558521270752
Epoch 300, val loss: 0.9100052714347839
Epoch 310, training loss: 0.21833932399749756 = 0.2115923911333084 + 0.001 * 6.746932506561279
Epoch 310, val loss: 0.919489324092865
Epoch 320, training loss: 0.19608192145824432 = 0.18932890892028809 + 0.001 * 6.753006458282471
Epoch 320, val loss: 0.9290618896484375
Epoch 330, training loss: 0.17590662837028503 = 0.1691576987504959 + 0.001 * 6.748927593231201
Epoch 330, val loss: 0.9389561414718628
Epoch 340, training loss: 0.1577090322971344 = 0.15096193552017212 + 0.001 * 6.747094631195068
Epoch 340, val loss: 0.9490801692008972
Epoch 350, training loss: 0.14125943183898926 = 0.13451360166072845 + 0.001 * 6.745834827423096
Epoch 350, val loss: 0.9591899514198303
Epoch 360, training loss: 0.12653523683547974 = 0.11978930234909058 + 0.001 * 6.745928764343262
Epoch 360, val loss: 0.9699119329452515
Epoch 370, training loss: 0.11343459039926529 = 0.1066870167851448 + 0.001 * 6.7475762367248535
Epoch 370, val loss: 0.9815226197242737
Epoch 380, training loss: 0.10188905894756317 = 0.09513787925243378 + 0.001 * 6.751181602478027
Epoch 380, val loss: 0.9942424893379211
Epoch 390, training loss: 0.0918324813246727 = 0.08508595824241638 + 0.001 * 6.746521472930908
Epoch 390, val loss: 1.0090969800949097
Epoch 400, training loss: 0.08307082206010818 = 0.0763254463672638 + 0.001 * 6.745373249053955
Epoch 400, val loss: 1.0256632566452026
Epoch 410, training loss: 0.0753735899925232 = 0.0686299279332161 + 0.001 * 6.743664741516113
Epoch 410, val loss: 1.04294753074646
Epoch 420, training loss: 0.06857670098543167 = 0.06183333694934845 + 0.001 * 6.743363857269287
Epoch 420, val loss: 1.0605146884918213
Epoch 430, training loss: 0.06258092075586319 = 0.05583026632666588 + 0.001 * 6.750656604766846
Epoch 430, val loss: 1.0782039165496826
Epoch 440, training loss: 0.05727183073759079 = 0.05053011700510979 + 0.001 * 6.741715431213379
Epoch 440, val loss: 1.0959303379058838
Epoch 450, training loss: 0.05258508771657944 = 0.04584360122680664 + 0.001 * 6.74148416519165
Epoch 450, val loss: 1.1136685609817505
Epoch 460, training loss: 0.04842439666390419 = 0.04168340936303139 + 0.001 * 6.7409868240356445
Epoch 460, val loss: 1.1312434673309326
Epoch 470, training loss: 0.04471588134765625 = 0.037972886115312576 + 0.001 * 6.742994785308838
Epoch 470, val loss: 1.1486015319824219
Epoch 480, training loss: 0.041395582258701324 = 0.03465515747666359 + 0.001 * 6.740425109863281
Epoch 480, val loss: 1.1657179594039917
Epoch 490, training loss: 0.0384264774620533 = 0.031690094619989395 + 0.001 * 6.7363810539245605
Epoch 490, val loss: 1.182536244392395
Epoch 500, training loss: 0.035777732729911804 = 0.029037443920969963 + 0.001 * 6.740289211273193
Epoch 500, val loss: 1.1989331245422363
Epoch 510, training loss: 0.033409349620342255 = 0.02667335420846939 + 0.001 * 6.735995292663574
Epoch 510, val loss: 1.2148724794387817
Epoch 520, training loss: 0.03130605071783066 = 0.02456979639828205 + 0.001 * 6.736253261566162
Epoch 520, val loss: 1.2304729223251343
Epoch 530, training loss: 0.02941150777041912 = 0.02267848327755928 + 0.001 * 6.733023643493652
Epoch 530, val loss: 1.2456705570220947
Epoch 540, training loss: 0.027700770646333694 = 0.02096971683204174 + 0.001 * 6.731052875518799
Epoch 540, val loss: 1.2605327367782593
Epoch 550, training loss: 0.02616308070719242 = 0.01942364312708378 + 0.001 * 6.739437580108643
Epoch 550, val loss: 1.2750606536865234
Epoch 560, training loss: 0.024748941883444786 = 0.01801992580294609 + 0.001 * 6.729015350341797
Epoch 560, val loss: 1.2893004417419434
Epoch 570, training loss: 0.023482589051127434 = 0.016752641648054123 + 0.001 * 6.729947566986084
Epoch 570, val loss: 1.3030258417129517
Epoch 580, training loss: 0.022336376830935478 = 0.015609224326908588 + 0.001 * 6.727151870727539
Epoch 580, val loss: 1.3161797523498535
Epoch 590, training loss: 0.02129554934799671 = 0.014572531916201115 + 0.001 * 6.723016738891602
Epoch 590, val loss: 1.3289856910705566
Epoch 600, training loss: 0.020358765497803688 = 0.013630026020109653 + 0.001 * 6.7287397384643555
Epoch 600, val loss: 1.3413842916488647
Epoch 610, training loss: 0.01950519159436226 = 0.012773625552654266 + 0.001 * 6.731565475463867
Epoch 610, val loss: 1.3534057140350342
Epoch 620, training loss: 0.018715066835284233 = 0.01199418120086193 + 0.001 * 6.720885753631592
Epoch 620, val loss: 1.3649606704711914
Epoch 630, training loss: 0.018001724034547806 = 0.011283716186881065 + 0.001 * 6.718008518218994
Epoch 630, val loss: 1.3761215209960938
Epoch 640, training loss: 0.01735418476164341 = 0.010635296814143658 + 0.001 * 6.718887805938721
Epoch 640, val loss: 1.3868815898895264
Epoch 650, training loss: 0.016759255900979042 = 0.010040726512670517 + 0.001 * 6.718529224395752
Epoch 650, val loss: 1.3973031044006348
Epoch 660, training loss: 0.01621081866323948 = 0.009494749829173088 + 0.001 * 6.716068267822266
Epoch 660, val loss: 1.4074608087539673
Epoch 670, training loss: 0.015706786885857582 = 0.008992936462163925 + 0.001 * 6.713850021362305
Epoch 670, val loss: 1.4173052310943604
Epoch 680, training loss: 0.015240199863910675 = 0.008530930615961552 + 0.001 * 6.709268569946289
Epoch 680, val loss: 1.4268215894699097
Epoch 690, training loss: 0.014812670648097992 = 0.00810515508055687 + 0.001 * 6.707515239715576
Epoch 690, val loss: 1.436034917831421
Epoch 700, training loss: 0.014422737993299961 = 0.007711957208812237 + 0.001 * 6.710780620574951
Epoch 700, val loss: 1.4449539184570312
Epoch 710, training loss: 0.01406092755496502 = 0.0073486072942614555 + 0.001 * 6.712320327758789
Epoch 710, val loss: 1.4536041021347046
Epoch 720, training loss: 0.013722909614443779 = 0.007011522073298693 + 0.001 * 6.7113871574401855
Epoch 720, val loss: 1.4619694948196411
Epoch 730, training loss: 0.013410414569079876 = 0.006698320619761944 + 0.001 * 6.712093830108643
Epoch 730, val loss: 1.47011399269104
Epoch 740, training loss: 0.013127014972269535 = 0.006406885571777821 + 0.001 * 6.720129013061523
Epoch 740, val loss: 1.4780339002609253
Epoch 750, training loss: 0.012834888882935047 = 0.006135289557278156 + 0.001 * 6.699598789215088
Epoch 750, val loss: 1.4857510328292847
Epoch 760, training loss: 0.012585146352648735 = 0.005881791468709707 + 0.001 * 6.703354835510254
Epoch 760, val loss: 1.4932706356048584
Epoch 770, training loss: 0.012337157502770424 = 0.005644859280437231 + 0.001 * 6.69229793548584
Epoch 770, val loss: 1.5005912780761719
Epoch 780, training loss: 0.012133881449699402 = 0.005423085298389196 + 0.001 * 6.710795879364014
Epoch 780, val loss: 1.507694959640503
Epoch 790, training loss: 0.011903834529221058 = 0.0052151912823319435 + 0.001 * 6.688642978668213
Epoch 790, val loss: 1.514618992805481
Epoch 800, training loss: 0.011712731793522835 = 0.005020034499466419 + 0.001 * 6.692697525024414
Epoch 800, val loss: 1.5213817358016968
Epoch 810, training loss: 0.011548696085810661 = 0.004836535081267357 + 0.001 * 6.712160587310791
Epoch 810, val loss: 1.527944803237915
Epoch 820, training loss: 0.011367603205144405 = 0.004663846455514431 + 0.001 * 6.703756332397461
Epoch 820, val loss: 1.5343759059906006
Epoch 830, training loss: 0.011198660358786583 = 0.004501144867390394 + 0.001 * 6.69751501083374
Epoch 830, val loss: 1.5406172275543213
Epoch 840, training loss: 0.01103339996188879 = 0.004347703885287046 + 0.001 * 6.685695648193359
Epoch 840, val loss: 1.5467365980148315
Epoch 850, training loss: 0.010909069329500198 = 0.00420283991843462 + 0.001 * 6.7062296867370605
Epoch 850, val loss: 1.552691102027893
Epoch 860, training loss: 0.010764692910015583 = 0.004065928515046835 + 0.001 * 6.698763847351074
Epoch 860, val loss: 1.5585263967514038
Epoch 870, training loss: 0.010629719123244286 = 0.00393641646951437 + 0.001 * 6.693302631378174
Epoch 870, val loss: 1.5642027854919434
Epoch 880, training loss: 0.010501480661332607 = 0.0038137645460665226 + 0.001 * 6.687716007232666
Epoch 880, val loss: 1.5697733163833618
Epoch 890, training loss: 0.010386348702013493 = 0.00369750102981925 + 0.001 * 6.688847541809082
Epoch 890, val loss: 1.5751698017120361
Epoch 900, training loss: 0.010259082540869713 = 0.003587177023291588 + 0.001 * 6.671904563903809
Epoch 900, val loss: 1.5804749727249146
Epoch 910, training loss: 0.010163300670683384 = 0.0034824053291231394 + 0.001 * 6.68089485168457
Epoch 910, val loss: 1.585646629333496
Epoch 920, training loss: 0.010080833919346333 = 0.003382835304364562 + 0.001 * 6.697998523712158
Epoch 920, val loss: 1.590701937675476
Epoch 930, training loss: 0.009977119043469429 = 0.0032881249208003283 + 0.001 * 6.688993453979492
Epoch 930, val loss: 1.5956789255142212
Epoch 940, training loss: 0.009882756508886814 = 0.0031979538034647703 + 0.001 * 6.684802055358887
Epoch 940, val loss: 1.600540280342102
Epoch 950, training loss: 0.009791639633476734 = 0.0031120136845856905 + 0.001 * 6.679625988006592
Epoch 950, val loss: 1.6053483486175537
Epoch 960, training loss: 0.009704421274363995 = 0.003029901534318924 + 0.001 * 6.6745195388793945
Epoch 960, val loss: 1.6099679470062256
Epoch 970, training loss: 0.009610829874873161 = 0.002951446920633316 + 0.001 * 6.659382343292236
Epoch 970, val loss: 1.6145455837249756
Epoch 980, training loss: 0.009536715224385262 = 0.0028765478637069464 + 0.001 * 6.6601667404174805
Epoch 980, val loss: 1.6190071105957031
Epoch 990, training loss: 0.0094684436917305 = 0.0028049862012267113 + 0.001 * 6.663457870483398
Epoch 990, val loss: 1.6233723163604736
Epoch 1000, training loss: 0.009391309693455696 = 0.002736554481089115 + 0.001 * 6.654755592346191
Epoch 1000, val loss: 1.6276503801345825
Epoch 1010, training loss: 0.009339501149952412 = 0.002671085763722658 + 0.001 * 6.668415069580078
Epoch 1010, val loss: 1.6318318843841553
Epoch 1020, training loss: 0.009267904795706272 = 0.002608431736007333 + 0.001 * 6.659472465515137
Epoch 1020, val loss: 1.6359338760375977
Epoch 1030, training loss: 0.009200366213917732 = 0.0025484177749603987 + 0.001 * 6.65194845199585
Epoch 1030, val loss: 1.63993501663208
Epoch 1040, training loss: 0.009138758294284344 = 0.002490902552381158 + 0.001 * 6.647855281829834
Epoch 1040, val loss: 1.6438744068145752
Epoch 1050, training loss: 0.009107501246035099 = 0.0024357482325285673 + 0.001 * 6.6717529296875
Epoch 1050, val loss: 1.6477500200271606
Epoch 1060, training loss: 0.009062997065484524 = 0.002382833743467927 + 0.001 * 6.680163383483887
Epoch 1060, val loss: 1.6514962911605835
Epoch 1070, training loss: 0.008983594365417957 = 0.0023320040199905634 + 0.001 * 6.651589870452881
Epoch 1070, val loss: 1.6552233695983887
Epoch 1080, training loss: 0.008937913924455643 = 0.0022832034155726433 + 0.001 * 6.654710292816162
Epoch 1080, val loss: 1.6588350534439087
Epoch 1090, training loss: 0.008892480283975601 = 0.0022363176103681326 + 0.001 * 6.656162738800049
Epoch 1090, val loss: 1.6624085903167725
Epoch 1100, training loss: 0.008870653808116913 = 0.002191259991377592 + 0.001 * 6.6793928146362305
Epoch 1100, val loss: 1.6660220623016357
Epoch 1110, training loss: 0.008808967657387257 = 0.0021479157730937004 + 0.001 * 6.6610517501831055
Epoch 1110, val loss: 1.6694375276565552
Epoch 1120, training loss: 0.008760424330830574 = 0.002106190426275134 + 0.001 * 6.654233932495117
Epoch 1120, val loss: 1.6728031635284424
Epoch 1130, training loss: 0.00873155239969492 = 0.002066010609269142 + 0.001 * 6.665541648864746
Epoch 1130, val loss: 1.6761038303375244
Epoch 1140, training loss: 0.008673264645040035 = 0.002027281327173114 + 0.001 * 6.6459832191467285
Epoch 1140, val loss: 1.6793049573898315
Epoch 1150, training loss: 0.0086400480940938 = 0.001989950193092227 + 0.001 * 6.650097846984863
Epoch 1150, val loss: 1.6824939250946045
Epoch 1160, training loss: 0.008590568788349628 = 0.0019539580680429935 + 0.001 * 6.636610507965088
Epoch 1160, val loss: 1.6856179237365723
Epoch 1170, training loss: 0.008589882403612137 = 0.0019192309118807316 + 0.001 * 6.670650959014893
Epoch 1170, val loss: 1.688688039779663
Epoch 1180, training loss: 0.008554321713745594 = 0.0018856446258723736 + 0.001 * 6.668676853179932
Epoch 1180, val loss: 1.6917556524276733
Epoch 1190, training loss: 0.008485045284032822 = 0.0018532162066549063 + 0.001 * 6.631828308105469
Epoch 1190, val loss: 1.6947245597839355
Epoch 1200, training loss: 0.008477653376758099 = 0.0018218794139102101 + 0.001 * 6.655773162841797
Epoch 1200, val loss: 1.6976996660232544
Epoch 1210, training loss: 0.008428043685853481 = 0.0017916043289005756 + 0.001 * 6.636438846588135
Epoch 1210, val loss: 1.7005479335784912
Epoch 1220, training loss: 0.008386570028960705 = 0.0017623183084651828 + 0.001 * 6.624251842498779
Epoch 1220, val loss: 1.7033849954605103
Epoch 1230, training loss: 0.008359558880329132 = 0.0017340063350275159 + 0.001 * 6.625551700592041
Epoch 1230, val loss: 1.7061538696289062
Epoch 1240, training loss: 0.008328989148139954 = 0.00170662731397897 + 0.001 * 6.622361660003662
Epoch 1240, val loss: 1.708882451057434
Epoch 1250, training loss: 0.008335556834936142 = 0.001680114190094173 + 0.001 * 6.655442714691162
Epoch 1250, val loss: 1.7115685939788818
Epoch 1260, training loss: 0.008296935819089413 = 0.0016544627724215388 + 0.001 * 6.642473220825195
Epoch 1260, val loss: 1.7141417264938354
Epoch 1270, training loss: 0.008255477994680405 = 0.001629620441235602 + 0.001 * 6.625856876373291
Epoch 1270, val loss: 1.7166996002197266
Epoch 1280, training loss: 0.008230478502810001 = 0.001605570432730019 + 0.001 * 6.624907970428467
Epoch 1280, val loss: 1.7192165851593018
Epoch 1290, training loss: 0.008201681077480316 = 0.0015822496498003602 + 0.001 * 6.6194305419921875
Epoch 1290, val loss: 1.7216817140579224
Epoch 1300, training loss: 0.008177433162927628 = 0.0015596675220876932 + 0.001 * 6.617765426635742
Epoch 1300, val loss: 1.7241307497024536
Epoch 1310, training loss: 0.008180503733456135 = 0.0015377525705844164 + 0.001 * 6.642751216888428
Epoch 1310, val loss: 1.726564884185791
Epoch 1320, training loss: 0.008172856643795967 = 0.0015165217919275165 + 0.001 * 6.65633487701416
Epoch 1320, val loss: 1.7288939952850342
Epoch 1330, training loss: 0.008105053566396236 = 0.0014959179097786546 + 0.001 * 6.609135627746582
Epoch 1330, val loss: 1.7312275171279907
Epoch 1340, training loss: 0.008092375472187996 = 0.0014759186888113618 + 0.001 * 6.616456508636475
Epoch 1340, val loss: 1.7335213422775269
Epoch 1350, training loss: 0.008064930327236652 = 0.0014565173769369721 + 0.001 * 6.608412742614746
Epoch 1350, val loss: 1.7357439994812012
Epoch 1360, training loss: 0.00804238859564066 = 0.0014376806793734431 + 0.001 * 6.604707717895508
Epoch 1360, val loss: 1.7379533052444458
Epoch 1370, training loss: 0.008032948710024357 = 0.0014193669194355607 + 0.001 * 6.61358118057251
Epoch 1370, val loss: 1.7401355504989624
Epoch 1380, training loss: 0.008026842959225178 = 0.0014015206834301353 + 0.001 * 6.625321865081787
Epoch 1380, val loss: 1.7422584295272827
Epoch 1390, training loss: 0.008004367351531982 = 0.0013841151958331466 + 0.001 * 6.620251655578613
Epoch 1390, val loss: 1.744398593902588
Epoch 1400, training loss: 0.007979541085660458 = 0.00136721006128937 + 0.001 * 6.612330913543701
Epoch 1400, val loss: 1.7464698553085327
Epoch 1410, training loss: 0.007961811497807503 = 0.0013507641851902008 + 0.001 * 6.611046314239502
Epoch 1410, val loss: 1.748514175415039
Epoch 1420, training loss: 0.007952055893838406 = 0.0013348099309951067 + 0.001 * 6.617245674133301
Epoch 1420, val loss: 1.75053071975708
Epoch 1430, training loss: 0.007917597889900208 = 0.0013193058548495173 + 0.001 * 6.598291397094727
Epoch 1430, val loss: 1.752551555633545
Epoch 1440, training loss: 0.007898877374827862 = 0.0013042498612776399 + 0.001 * 6.5946269035339355
Epoch 1440, val loss: 1.7544867992401123
Epoch 1450, training loss: 0.00788629800081253 = 0.001289587584324181 + 0.001 * 6.596709728240967
Epoch 1450, val loss: 1.7564226388931274
Epoch 1460, training loss: 0.007872271351516247 = 0.0012753130868077278 + 0.001 * 6.596958160400391
Epoch 1460, val loss: 1.7583180665969849
Epoch 1470, training loss: 0.007877054624259472 = 0.0012614253209903836 + 0.001 * 6.615628719329834
Epoch 1470, val loss: 1.7602159976959229
Epoch 1480, training loss: 0.00784511398524046 = 0.00124788424000144 + 0.001 * 6.597229480743408
Epoch 1480, val loss: 1.7620142698287964
Epoch 1490, training loss: 0.007823474705219269 = 0.0012346955481916666 + 0.001 * 6.588778972625732
Epoch 1490, val loss: 1.7638332843780518
Epoch 1500, training loss: 0.0078097153455019 = 0.0012218400370329618 + 0.001 * 6.5878753662109375
Epoch 1500, val loss: 1.765626072883606
Epoch 1510, training loss: 0.007807507179677486 = 0.0012093114200979471 + 0.001 * 6.598195552825928
Epoch 1510, val loss: 1.7673715353012085
Epoch 1520, training loss: 0.007772773504257202 = 0.0011970967752858996 + 0.001 * 6.575676441192627
Epoch 1520, val loss: 1.7691066265106201
Epoch 1530, training loss: 0.007768701761960983 = 0.0011851959861814976 + 0.001 * 6.583505630493164
Epoch 1530, val loss: 1.7708375453948975
Epoch 1540, training loss: 0.007775438483804464 = 0.0011736047454178333 + 0.001 * 6.601833343505859
Epoch 1540, val loss: 1.772518515586853
Epoch 1550, training loss: 0.007778634782880545 = 0.0011622713645920157 + 0.001 * 6.616363048553467
Epoch 1550, val loss: 1.7741808891296387
Epoch 1560, training loss: 0.007733272388577461 = 0.0011511774500831962 + 0.001 * 6.582094669342041
Epoch 1560, val loss: 1.7759037017822266
Epoch 1570, training loss: 0.007726715877652168 = 0.0011403871467337012 + 0.001 * 6.586328506469727
Epoch 1570, val loss: 1.777598261833191
Epoch 1580, training loss: 0.007733200676739216 = 0.0011298860190436244 + 0.001 * 6.603314399719238
Epoch 1580, val loss: 1.7792173624038696
Epoch 1590, training loss: 0.007707929238677025 = 0.0011196269188076258 + 0.001 * 6.588302135467529
Epoch 1590, val loss: 1.7808058261871338
Epoch 1600, training loss: 0.007703183218836784 = 0.0011096021626144648 + 0.001 * 6.593581199645996
Epoch 1600, val loss: 1.7824114561080933
Epoch 1610, training loss: 0.007676728535443544 = 0.0010998183861374855 + 0.001 * 6.576910018920898
Epoch 1610, val loss: 1.783964991569519
Epoch 1620, training loss: 0.007696810178458691 = 0.001090248697437346 + 0.001 * 6.606561183929443
Epoch 1620, val loss: 1.785518765449524
Epoch 1630, training loss: 0.007645576260983944 = 0.00108090799767524 + 0.001 * 6.56466817855835
Epoch 1630, val loss: 1.7870028018951416
Epoch 1640, training loss: 0.007638833485543728 = 0.00107177859172225 + 0.001 * 6.567054748535156
Epoch 1640, val loss: 1.7885109186172485
Epoch 1650, training loss: 0.0076256003230810165 = 0.0010628694435581565 + 0.001 * 6.562730312347412
Epoch 1650, val loss: 1.7899705171585083
Epoch 1660, training loss: 0.007616440765559673 = 0.0010541690280660987 + 0.001 * 6.5622711181640625
Epoch 1660, val loss: 1.7914625406265259
Epoch 1670, training loss: 0.007611584849655628 = 0.0010456398595124483 + 0.001 * 6.565944671630859
Epoch 1670, val loss: 1.7929078340530396
Epoch 1680, training loss: 0.007617529481649399 = 0.0010373020777478814 + 0.001 * 6.580227375030518
Epoch 1680, val loss: 1.7943702936172485
Epoch 1690, training loss: 0.0075972359627485275 = 0.001029160339385271 + 0.001 * 6.568075180053711
Epoch 1690, val loss: 1.7957619428634644
Epoch 1700, training loss: 0.007636338472366333 = 0.0010211963672190905 + 0.001 * 6.615141868591309
Epoch 1700, val loss: 1.7971584796905518
Epoch 1710, training loss: 0.007631971500813961 = 0.0010134393814951181 + 0.001 * 6.618532180786133
Epoch 1710, val loss: 1.7985397577285767
Epoch 1720, training loss: 0.007580001372843981 = 0.0010058280313387513 + 0.001 * 6.5741729736328125
Epoch 1720, val loss: 1.7998576164245605
Epoch 1730, training loss: 0.007612568326294422 = 0.000998399336822331 + 0.001 * 6.614168643951416
Epoch 1730, val loss: 1.8011751174926758
Epoch 1740, training loss: 0.007574603892862797 = 0.0009911097586154938 + 0.001 * 6.583493709564209
Epoch 1740, val loss: 1.8025131225585938
Epoch 1750, training loss: 0.007532763760536909 = 0.000983988051302731 + 0.001 * 6.548775672912598
Epoch 1750, val loss: 1.8038045167922974
Epoch 1760, training loss: 0.007529440335929394 = 0.0009769581956788898 + 0.001 * 6.552481651306152
Epoch 1760, val loss: 1.8051012754440308
Epoch 1770, training loss: 0.0075246174819767475 = 0.0009701402741484344 + 0.001 * 6.554477214813232
Epoch 1770, val loss: 1.8063628673553467
Epoch 1780, training loss: 0.007515375502407551 = 0.0009634295129217207 + 0.001 * 6.551945686340332
Epoch 1780, val loss: 1.807618260383606
Epoch 1790, training loss: 0.00750763900578022 = 0.0009568369714543223 + 0.001 * 6.550801753997803
Epoch 1790, val loss: 1.8088804483413696
Epoch 1800, training loss: 0.007506136782467365 = 0.0009504149202257395 + 0.001 * 6.555721282958984
Epoch 1800, val loss: 1.8101348876953125
Epoch 1810, training loss: 0.00750624667853117 = 0.0009441087604500353 + 0.001 * 6.562137603759766
Epoch 1810, val loss: 1.811340093612671
Epoch 1820, training loss: 0.007509433664381504 = 0.0009379268158227205 + 0.001 * 6.571506500244141
Epoch 1820, val loss: 1.8125416040420532
Epoch 1830, training loss: 0.0074897948652505875 = 0.0009318582597188652 + 0.001 * 6.557936668395996
Epoch 1830, val loss: 1.813755989074707
Epoch 1840, training loss: 0.007497523911297321 = 0.0009259083890356123 + 0.001 * 6.571615219116211
Epoch 1840, val loss: 1.8149404525756836
Epoch 1850, training loss: 0.00748499296605587 = 0.0009200581698678434 + 0.001 * 6.564934253692627
Epoch 1850, val loss: 1.816101312637329
Epoch 1860, training loss: 0.007495635189116001 = 0.0009143252391368151 + 0.001 * 6.5813093185424805
Epoch 1860, val loss: 1.8173303604125977
Epoch 1870, training loss: 0.007461772300302982 = 0.0009087233920581639 + 0.001 * 6.553048610687256
Epoch 1870, val loss: 1.8184175491333008
Epoch 1880, training loss: 0.007474317215383053 = 0.0009032014640979469 + 0.001 * 6.571115493774414
Epoch 1880, val loss: 1.8195838928222656
Epoch 1890, training loss: 0.00742692593485117 = 0.0008978063706308603 + 0.001 * 6.52911901473999
Epoch 1890, val loss: 1.8207075595855713
Epoch 1900, training loss: 0.007420300971716642 = 0.0008924808353185654 + 0.001 * 6.527819633483887
Epoch 1900, val loss: 1.8218435049057007
Epoch 1910, training loss: 0.00741824135184288 = 0.0008872870821505785 + 0.001 * 6.530954360961914
Epoch 1910, val loss: 1.8229271173477173
Epoch 1920, training loss: 0.007408828940242529 = 0.00088216905714944 + 0.001 * 6.5266594886779785
Epoch 1920, val loss: 1.8240292072296143
Epoch 1930, training loss: 0.0074201542884111404 = 0.0008771202410571277 + 0.001 * 6.543034076690674
Epoch 1930, val loss: 1.8251146078109741
Epoch 1940, training loss: 0.007423195522278547 = 0.0008721787598915398 + 0.001 * 6.551016330718994
Epoch 1940, val loss: 1.8261734247207642
Epoch 1950, training loss: 0.0074254171922802925 = 0.0008673330303281546 + 0.001 * 6.558083534240723
Epoch 1950, val loss: 1.8272631168365479
Epoch 1960, training loss: 0.007425567600876093 = 0.0008625607588328421 + 0.001 * 6.563006401062012
Epoch 1960, val loss: 1.8283334970474243
Epoch 1970, training loss: 0.007394983898848295 = 0.0008578565320931375 + 0.001 * 6.5371270179748535
Epoch 1970, val loss: 1.8294060230255127
Epoch 1980, training loss: 0.007379787974059582 = 0.0008532567298971117 + 0.001 * 6.526530742645264
Epoch 1980, val loss: 1.8304429054260254
Epoch 1990, training loss: 0.0073831877671182156 = 0.0008487417944706976 + 0.001 * 6.534445762634277
Epoch 1990, val loss: 1.8314414024353027
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7519
Overall ASR: 0.9004
Flip ASR: 0.8800/225 nodes
The final ASR:0.77122, 0.15001, Accuracy:0.79753, 0.03234
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11638])
remove edge: torch.Size([2, 9596])
updated graph: torch.Size([2, 10678])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9779
Flip ASR: 0.9733/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8370
Overall ASR: 0.9926
Flip ASR: 0.9911/225 nodes
The final ASR:0.98401, 0.00627, Accuracy:0.83086, 0.00630
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.3
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.962955355644226 = 1.954581618309021 + 0.001 * 8.373760223388672
Epoch 0, val loss: 1.952219009399414
Epoch 10, training loss: 1.9526350498199463 = 1.9442614316940308 + 0.001 * 8.373671531677246
Epoch 10, val loss: 1.9426735639572144
Epoch 20, training loss: 1.9400696754455566 = 1.9316962957382202 + 0.001 * 8.373351097106934
Epoch 20, val loss: 1.9305768013000488
Epoch 30, training loss: 1.9224884510040283 = 1.9141159057617188 + 0.001 * 8.372601509094238
Epoch 30, val loss: 1.9132466316223145
Epoch 40, training loss: 1.8962777853012085 = 1.8879070281982422 + 0.001 * 8.3707914352417
Epoch 40, val loss: 1.8874716758728027
Epoch 50, training loss: 1.858838438987732 = 1.8504732847213745 + 0.001 * 8.365139961242676
Epoch 50, val loss: 1.85204017162323
Epoch 60, training loss: 1.8149282932281494 = 1.8065885305404663 + 0.001 * 8.33979606628418
Epoch 60, val loss: 1.8143494129180908
Epoch 70, training loss: 1.7767518758773804 = 1.7685511112213135 + 0.001 * 8.200752258300781
Epoch 70, val loss: 1.7847580909729004
Epoch 80, training loss: 1.7323709726333618 = 1.7246006727218628 + 0.001 * 7.770270824432373
Epoch 80, val loss: 1.7462393045425415
Epoch 90, training loss: 1.670694351196289 = 1.6631113290786743 + 0.001 * 7.583033084869385
Epoch 90, val loss: 1.6928061246871948
Epoch 100, training loss: 1.5897959470748901 = 1.5823723077774048 + 0.001 * 7.42360258102417
Epoch 100, val loss: 1.6250309944152832
Epoch 110, training loss: 1.4959810972213745 = 1.4887888431549072 + 0.001 * 7.192285060882568
Epoch 110, val loss: 1.548100471496582
Epoch 120, training loss: 1.401901364326477 = 1.394901156425476 + 0.001 * 7.0001983642578125
Epoch 120, val loss: 1.473254680633545
Epoch 130, training loss: 1.311052680015564 = 1.304058313369751 + 0.001 * 6.99432373046875
Epoch 130, val loss: 1.4030483961105347
Epoch 140, training loss: 1.2203480005264282 = 1.213369607925415 + 0.001 * 6.978359699249268
Epoch 140, val loss: 1.3354313373565674
Epoch 150, training loss: 1.1278631687164307 = 1.1208906173706055 + 0.001 * 6.97257137298584
Epoch 150, val loss: 1.2672269344329834
Epoch 160, training loss: 1.0338523387908936 = 1.0268868207931519 + 0.001 * 6.96552848815918
Epoch 160, val loss: 1.198479175567627
Epoch 170, training loss: 0.9401237964630127 = 0.9331642389297485 + 0.001 * 6.959551811218262
Epoch 170, val loss: 1.1295466423034668
Epoch 180, training loss: 0.84913569688797 = 0.8421819806098938 + 0.001 * 6.953693866729736
Epoch 180, val loss: 1.0611648559570312
Epoch 190, training loss: 0.7630990147590637 = 0.7561520934104919 + 0.001 * 6.946897983551025
Epoch 190, val loss: 0.9958798885345459
Epoch 200, training loss: 0.683635950088501 = 0.6766981482505798 + 0.001 * 6.93781042098999
Epoch 200, val loss: 0.9357255101203918
Epoch 210, training loss: 0.6115251183509827 = 0.6046000123023987 + 0.001 * 6.925117492675781
Epoch 210, val loss: 0.8827102780342102
Epoch 220, training loss: 0.546568751335144 = 0.5396604537963867 + 0.001 * 6.908317565917969
Epoch 220, val loss: 0.8384454846382141
Epoch 230, training loss: 0.4876552224159241 = 0.4807678461074829 + 0.001 * 6.887373924255371
Epoch 230, val loss: 0.8023160696029663
Epoch 240, training loss: 0.4332304894924164 = 0.42636722326278687 + 0.001 * 6.863259315490723
Epoch 240, val loss: 0.7726755738258362
Epoch 250, training loss: 0.3820754289627075 = 0.3752326965332031 + 0.001 * 6.842735767364502
Epoch 250, val loss: 0.7480267882347107
Epoch 260, training loss: 0.3337686061859131 = 0.3269491195678711 + 0.001 * 6.819500923156738
Epoch 260, val loss: 0.7275096774101257
Epoch 270, training loss: 0.2887183427810669 = 0.28191664814949036 + 0.001 * 6.801697731018066
Epoch 270, val loss: 0.7111243605613708
Epoch 280, training loss: 0.24776439368724823 = 0.24096797406673431 + 0.001 * 6.796419143676758
Epoch 280, val loss: 0.6991009712219238
Epoch 290, training loss: 0.2116379588842392 = 0.20484933257102966 + 0.001 * 6.788623332977295
Epoch 290, val loss: 0.6918039321899414
Epoch 300, training loss: 0.1806943714618683 = 0.1739121377468109 + 0.001 * 6.782226085662842
Epoch 300, val loss: 0.6893677711486816
Epoch 310, training loss: 0.15480083227157593 = 0.1480226069688797 + 0.001 * 6.778220176696777
Epoch 310, val loss: 0.6917493343353271
Epoch 320, training loss: 0.13346414268016815 = 0.12668877840042114 + 0.001 * 6.775362968444824
Epoch 320, val loss: 0.6984784603118896
Epoch 330, training loss: 0.11595528572797775 = 0.10918286442756653 + 0.001 * 6.772418975830078
Epoch 330, val loss: 0.7085999250411987
Epoch 340, training loss: 0.10153846442699432 = 0.094769187271595 + 0.001 * 6.769275188446045
Epoch 340, val loss: 0.7211433053016663
Epoch 350, training loss: 0.08958739042282104 = 0.0828176960349083 + 0.001 * 6.769692897796631
Epoch 350, val loss: 0.7354643940925598
Epoch 360, training loss: 0.07959231734275818 = 0.07283042371273041 + 0.001 * 6.761890888214111
Epoch 360, val loss: 0.7509164214134216
Epoch 370, training loss: 0.0711720883846283 = 0.0644141435623169 + 0.001 * 6.757940769195557
Epoch 370, val loss: 0.7671470046043396
Epoch 380, training loss: 0.06404944509267807 = 0.05726732686161995 + 0.001 * 6.782120704650879
Epoch 380, val loss: 0.7838515043258667
Epoch 390, training loss: 0.057909026741981506 = 0.05115600302815437 + 0.001 * 6.75302267074585
Epoch 390, val loss: 0.8008474707603455
Epoch 400, training loss: 0.05264372378587723 = 0.04589845985174179 + 0.001 * 6.745262622833252
Epoch 400, val loss: 0.8179567456245422
Epoch 410, training loss: 0.048093218356370926 = 0.04135315120220184 + 0.001 * 6.7400665283203125
Epoch 410, val loss: 0.8349863290786743
Epoch 420, training loss: 0.044152937829494476 = 0.03740542009472847 + 0.001 * 6.747518062591553
Epoch 420, val loss: 0.8520568013191223
Epoch 430, training loss: 0.04070170223712921 = 0.03396198898553848 + 0.001 * 6.739712238311768
Epoch 430, val loss: 0.8688246011734009
Epoch 440, training loss: 0.03767479211091995 = 0.03094707429409027 + 0.001 * 6.72771692276001
Epoch 440, val loss: 0.8853186964988708
Epoch 450, training loss: 0.03501691669225693 = 0.02829730324447155 + 0.001 * 6.719614505767822
Epoch 450, val loss: 0.9015268683433533
Epoch 460, training loss: 0.032681263983249664 = 0.02595583163201809 + 0.001 * 6.725430965423584
Epoch 460, val loss: 0.9174449443817139
Epoch 470, training loss: 0.03058905526995659 = 0.023875009268522263 + 0.001 * 6.71404504776001
Epoch 470, val loss: 0.9329586029052734
Epoch 480, training loss: 0.028722094371914864 = 0.022013019770383835 + 0.001 * 6.709074020385742
Epoch 480, val loss: 0.9482535123825073
Epoch 490, training loss: 0.02704804576933384 = 0.020337669178843498 + 0.001 * 6.710376262664795
Epoch 490, val loss: 0.9632854461669922
Epoch 500, training loss: 0.025535838678479195 = 0.01882479339838028 + 0.001 * 6.711044788360596
Epoch 500, val loss: 0.9781545996665955
Epoch 510, training loss: 0.024177124723792076 = 0.017456579953432083 + 0.001 * 6.720544338226318
Epoch 510, val loss: 0.9927176237106323
Epoch 520, training loss: 0.02291838452219963 = 0.016218336299061775 + 0.001 * 6.700047969818115
Epoch 520, val loss: 1.0070682764053345
Epoch 530, training loss: 0.021801592782139778 = 0.015096830204129219 + 0.001 * 6.7047624588012695
Epoch 530, val loss: 1.0211642980575562
Epoch 540, training loss: 0.02077278308570385 = 0.014080099761486053 + 0.001 * 6.69268274307251
Epoch 540, val loss: 1.035001516342163
Epoch 550, training loss: 0.019843582063913345 = 0.01315710786730051 + 0.001 * 6.6864728927612305
Epoch 550, val loss: 1.0485526323318481
Epoch 560, training loss: 0.01900283806025982 = 0.01231710147112608 + 0.001 * 6.685736656188965
Epoch 560, val loss: 1.0618125200271606
Epoch 570, training loss: 0.01822451874613762 = 0.011548538692295551 + 0.001 * 6.675980091094971
Epoch 570, val loss: 1.0749051570892334
Epoch 580, training loss: 0.017514608800411224 = 0.010841329582035542 + 0.001 * 6.673279285430908
Epoch 580, val loss: 1.0878134965896606
Epoch 590, training loss: 0.016863834112882614 = 0.010188945569097996 + 0.001 * 6.6748881340026855
Epoch 590, val loss: 1.100534439086914
Epoch 600, training loss: 0.016273891553282738 = 0.009586677886545658 + 0.001 * 6.687213897705078
Epoch 600, val loss: 1.1131067276000977
Epoch 610, training loss: 0.015698127448558807 = 0.009030848741531372 + 0.001 * 6.6672773361206055
Epoch 610, val loss: 1.1255154609680176
Epoch 620, training loss: 0.015212062746286392 = 0.008517909795045853 + 0.001 * 6.694152355194092
Epoch 620, val loss: 1.1377103328704834
Epoch 630, training loss: 0.014709584414958954 = 0.008044533431529999 + 0.001 * 6.665050029754639
Epoch 630, val loss: 1.1496158838272095
Epoch 640, training loss: 0.014278793707489967 = 0.007607605773955584 + 0.001 * 6.671186923980713
Epoch 640, val loss: 1.161347508430481
Epoch 650, training loss: 0.013873636722564697 = 0.007204101886600256 + 0.001 * 6.669534683227539
Epoch 650, val loss: 1.172790288925171
Epoch 660, training loss: 0.013491986319422722 = 0.006831216625869274 + 0.001 * 6.660768985748291
Epoch 660, val loss: 1.1839675903320312
Epoch 670, training loss: 0.013140058144927025 = 0.0064862691797316074 + 0.001 * 6.653788089752197
Epoch 670, val loss: 1.194919466972351
Epoch 680, training loss: 0.012834005057811737 = 0.006166897248476744 + 0.001 * 6.667108058929443
Epoch 680, val loss: 1.2055848836898804
Epoch 690, training loss: 0.012542655691504478 = 0.00587077671661973 + 0.001 * 6.671879291534424
Epoch 690, val loss: 1.216048002243042
Epoch 700, training loss: 0.012277817353606224 = 0.00559582794085145 + 0.001 * 6.6819891929626465
Epoch 700, val loss: 1.2262424230575562
Epoch 710, training loss: 0.011997098103165627 = 0.005340264178812504 + 0.001 * 6.656833171844482
Epoch 710, val loss: 1.2362219095230103
Epoch 720, training loss: 0.011759428307414055 = 0.005102358292788267 + 0.001 * 6.657070159912109
Epoch 720, val loss: 1.2459630966186523
Epoch 730, training loss: 0.011517854407429695 = 0.00488065741956234 + 0.001 * 6.637197017669678
Epoch 730, val loss: 1.2555214166641235
Epoch 740, training loss: 0.011332311667501926 = 0.00467376783490181 + 0.001 * 6.658543586730957
Epoch 740, val loss: 1.2648260593414307
Epoch 750, training loss: 0.011133024469017982 = 0.004480469506233931 + 0.001 * 6.652554512023926
Epoch 750, val loss: 1.2739310264587402
Epoch 760, training loss: 0.0109536312520504 = 0.004299608990550041 + 0.001 * 6.654021739959717
Epoch 760, val loss: 1.2828744649887085
Epoch 770, training loss: 0.01078893058001995 = 0.004130186513066292 + 0.001 * 6.6587443351745605
Epoch 770, val loss: 1.2915817499160767
Epoch 780, training loss: 0.010613810271024704 = 0.003971284721046686 + 0.001 * 6.642524719238281
Epoch 780, val loss: 1.3000836372375488
Epoch 790, training loss: 0.010464767925441265 = 0.0038220728747546673 + 0.001 * 6.64269495010376
Epoch 790, val loss: 1.3083890676498413
Epoch 800, training loss: 0.010311819612979889 = 0.0036817879881709814 + 0.001 * 6.630031585693359
Epoch 800, val loss: 1.3165384531021118
Epoch 810, training loss: 0.010196907445788383 = 0.0035497688222676516 + 0.001 * 6.647138595581055
Epoch 810, val loss: 1.3244843482971191
Epoch 820, training loss: 0.010045419447124004 = 0.0034253750927746296 + 0.001 * 6.620044231414795
Epoch 820, val loss: 1.3322815895080566
Epoch 830, training loss: 0.009959711693227291 = 0.003308025887235999 + 0.001 * 6.65168571472168
Epoch 830, val loss: 1.339915156364441
Epoch 840, training loss: 0.009831033647060394 = 0.0031972250435501337 + 0.001 * 6.633808612823486
Epoch 840, val loss: 1.3473409414291382
Epoch 850, training loss: 0.009716877713799477 = 0.0030925441533327103 + 0.001 * 6.624332904815674
Epoch 850, val loss: 1.354695200920105
Epoch 860, training loss: 0.0096318069845438 = 0.002993538975715637 + 0.001 * 6.6382670402526855
Epoch 860, val loss: 1.3618022203445435
Epoch 870, training loss: 0.00952114537358284 = 0.0028997999615967274 + 0.001 * 6.621344566345215
Epoch 870, val loss: 1.3687533140182495
Epoch 880, training loss: 0.009438641369342804 = 0.0028109303675591946 + 0.001 * 6.627711296081543
Epoch 880, val loss: 1.3756521940231323
Epoch 890, training loss: 0.009335149079561234 = 0.0027266480028629303 + 0.001 * 6.608501434326172
Epoch 890, val loss: 1.382363200187683
Epoch 900, training loss: 0.009264551103115082 = 0.002646643901243806 + 0.001 * 6.61790657043457
Epoch 900, val loss: 1.388912320137024
Epoch 910, training loss: 0.009180473163723946 = 0.002570620272308588 + 0.001 * 6.609852313995361
Epoch 910, val loss: 1.3953633308410645
Epoch 920, training loss: 0.009103328920900822 = 0.0024983235634863377 + 0.001 * 6.605005264282227
Epoch 920, val loss: 1.401667594909668
Epoch 930, training loss: 0.009043985977768898 = 0.0024295069742947817 + 0.001 * 6.614479064941406
Epoch 930, val loss: 1.4078456163406372
Epoch 940, training loss: 0.008994138799607754 = 0.0023639542050659657 + 0.001 * 6.630184173583984
Epoch 940, val loss: 1.413894534111023
Epoch 950, training loss: 0.008924285881221294 = 0.0023014878388494253 + 0.001 * 6.622797966003418
Epoch 950, val loss: 1.4198576211929321
Epoch 960, training loss: 0.008902132511138916 = 0.0022419150918722153 + 0.001 * 6.66021728515625
Epoch 960, val loss: 1.425659418106079
Epoch 970, training loss: 0.008791252039372921 = 0.0021850557532161474 + 0.001 * 6.60619592666626
Epoch 970, val loss: 1.4313750267028809
Epoch 980, training loss: 0.008746092207729816 = 0.0021307512652128935 + 0.001 * 6.615340709686279
Epoch 980, val loss: 1.436970591545105
Epoch 990, training loss: 0.008681681007146835 = 0.002078827004879713 + 0.001 * 6.602853298187256
Epoch 990, val loss: 1.442459225654602
Epoch 1000, training loss: 0.008696136064827442 = 0.002029170049354434 + 0.001 * 6.666965484619141
Epoch 1000, val loss: 1.447853684425354
Epoch 1010, training loss: 0.008581392467021942 = 0.0019816646818071604 + 0.001 * 6.599727153778076
Epoch 1010, val loss: 1.4531762599945068
Epoch 1020, training loss: 0.008525033481419086 = 0.001936157583259046 + 0.001 * 6.5888752937316895
Epoch 1020, val loss: 1.4583901166915894
Epoch 1030, training loss: 0.00851063709706068 = 0.001892571453936398 + 0.001 * 6.618065357208252
Epoch 1030, val loss: 1.4634721279144287
Epoch 1040, training loss: 0.008457865566015244 = 0.0018507950007915497 + 0.001 * 6.607069969177246
Epoch 1040, val loss: 1.4685018062591553
Epoch 1050, training loss: 0.008397804573178291 = 0.001810712506994605 + 0.001 * 6.58709192276001
Epoch 1050, val loss: 1.4733949899673462
Epoch 1060, training loss: 0.008352389559149742 = 0.0017722465563565493 + 0.001 * 6.580142974853516
Epoch 1060, val loss: 1.478280782699585
Epoch 1070, training loss: 0.008318856358528137 = 0.0017353084404021502 + 0.001 * 6.583548069000244
Epoch 1070, val loss: 1.4829972982406616
Epoch 1080, training loss: 0.008271448314189911 = 0.0016998169012367725 + 0.001 * 6.571630477905273
Epoch 1080, val loss: 1.487677812576294
Epoch 1090, training loss: 0.008266059681773186 = 0.0016657186206430197 + 0.001 * 6.600340366363525
Epoch 1090, val loss: 1.4922279119491577
Epoch 1100, training loss: 0.008205990307033062 = 0.001632926519960165 + 0.001 * 6.573063373565674
Epoch 1100, val loss: 1.4967373609542847
Epoch 1110, training loss: 0.00817625317722559 = 0.0016013856511563063 + 0.001 * 6.574866771697998
Epoch 1110, val loss: 1.5011672973632812
Epoch 1120, training loss: 0.00814904272556305 = 0.001571008819155395 + 0.001 * 6.578033924102783
Epoch 1120, val loss: 1.5055266618728638
Epoch 1130, training loss: 0.00813048705458641 = 0.0015417644754052162 + 0.001 * 6.588722229003906
Epoch 1130, val loss: 1.5097696781158447
Epoch 1140, training loss: 0.008099338039755821 = 0.0015135816065594554 + 0.001 * 6.585756301879883
Epoch 1140, val loss: 1.513977289199829
Epoch 1150, training loss: 0.008076692000031471 = 0.0014864434488117695 + 0.001 * 6.590248107910156
Epoch 1150, val loss: 1.51813805103302
Epoch 1160, training loss: 0.008048675954341888 = 0.0014602452283725142 + 0.001 * 6.588430404663086
Epoch 1160, val loss: 1.5222049951553345
Epoch 1170, training loss: 0.008024442940950394 = 0.0014350105775520205 + 0.001 * 6.5894317626953125
Epoch 1170, val loss: 1.5261974334716797
Epoch 1180, training loss: 0.0079800458624959 = 0.0014106390299275517 + 0.001 * 6.569406509399414
Epoch 1180, val loss: 1.5301134586334229
Epoch 1190, training loss: 0.007939822971820831 = 0.0013871282571926713 + 0.001 * 6.552694320678711
Epoch 1190, val loss: 1.533973217010498
Epoch 1200, training loss: 0.007936961017549038 = 0.0013644235441461205 + 0.001 * 6.572536945343018
Epoch 1200, val loss: 1.5377506017684937
Epoch 1210, training loss: 0.007907782681286335 = 0.0013424885692074895 + 0.001 * 6.565293788909912
Epoch 1210, val loss: 1.5415033102035522
Epoch 1220, training loss: 0.007882548496127129 = 0.0013213000493124127 + 0.001 * 6.561247825622559
Epoch 1220, val loss: 1.545130729675293
Epoch 1230, training loss: 0.007886807434260845 = 0.0013008123496547341 + 0.001 * 6.585994243621826
Epoch 1230, val loss: 1.5487346649169922
Epoch 1240, training loss: 0.007861237972974777 = 0.0012810188345611095 + 0.001 * 6.58021879196167
Epoch 1240, val loss: 1.5523234605789185
Epoch 1250, training loss: 0.00784884113818407 = 0.0012618592008948326 + 0.001 * 6.586981773376465
Epoch 1250, val loss: 1.5557830333709717
Epoch 1260, training loss: 0.007791135460138321 = 0.001243311446160078 + 0.001 * 6.547823905944824
Epoch 1260, val loss: 1.5591942071914673
Epoch 1270, training loss: 0.007794702425599098 = 0.0012253585737198591 + 0.001 * 6.569343566894531
Epoch 1270, val loss: 1.5626004934310913
Epoch 1280, training loss: 0.007746271789073944 = 0.0012079703155905008 + 0.001 * 6.538301467895508
Epoch 1280, val loss: 1.56589937210083
Epoch 1290, training loss: 0.007759096100926399 = 0.001191123970784247 + 0.001 * 6.567971706390381
Epoch 1290, val loss: 1.5691285133361816
Epoch 1300, training loss: 0.007731107994914055 = 0.0011748047545552254 + 0.001 * 6.556303024291992
Epoch 1300, val loss: 1.572388768196106
Epoch 1310, training loss: 0.0077046663500368595 = 0.0011589800706133246 + 0.001 * 6.545685768127441
Epoch 1310, val loss: 1.5755362510681152
Epoch 1320, training loss: 0.0076982127502560616 = 0.0011436395579949021 + 0.001 * 6.554573059082031
Epoch 1320, val loss: 1.5786525011062622
Epoch 1330, training loss: 0.00767158530652523 = 0.0011287762317806482 + 0.001 * 6.542808532714844
Epoch 1330, val loss: 1.5816655158996582
Epoch 1340, training loss: 0.00765189528465271 = 0.0011143581941723824 + 0.001 * 6.53753662109375
Epoch 1340, val loss: 1.5847060680389404
Epoch 1350, training loss: 0.007643591146916151 = 0.0011003921972587705 + 0.001 * 6.543198585510254
Epoch 1350, val loss: 1.5876736640930176
Epoch 1360, training loss: 0.007634071633219719 = 0.0010868225945159793 + 0.001 * 6.547248840332031
Epoch 1360, val loss: 1.5905426740646362
Epoch 1370, training loss: 0.00762634351849556 = 0.001073657302185893 + 0.001 * 6.552685737609863
Epoch 1370, val loss: 1.5934289693832397
Epoch 1380, training loss: 0.007608888670802116 = 0.0010608717566356063 + 0.001 * 6.548016548156738
Epoch 1380, val loss: 1.596240758895874
Epoch 1390, training loss: 0.007570001296699047 = 0.0010484501253813505 + 0.001 * 6.521551132202148
Epoch 1390, val loss: 1.5990056991577148
Epoch 1400, training loss: 0.007593159563839436 = 0.0010363974142819643 + 0.001 * 6.556762218475342
Epoch 1400, val loss: 1.6017735004425049
Epoch 1410, training loss: 0.007571541704237461 = 0.0010246713645756245 + 0.001 * 6.546870231628418
Epoch 1410, val loss: 1.6044189929962158
Epoch 1420, training loss: 0.007554918527603149 = 0.001013285364024341 + 0.001 * 6.541632652282715
Epoch 1420, val loss: 1.6070771217346191
Epoch 1430, training loss: 0.007579503580927849 = 0.0010022076312452555 + 0.001 * 6.577295303344727
Epoch 1430, val loss: 1.6097002029418945
Epoch 1440, training loss: 0.007532901130616665 = 0.00099143385887146 + 0.001 * 6.541467189788818
Epoch 1440, val loss: 1.612269401550293
Epoch 1450, training loss: 0.007553168572485447 = 0.0009809660259634256 + 0.001 * 6.572202682495117
Epoch 1450, val loss: 1.6148121356964111
Epoch 1460, training loss: 0.007510361261665821 = 0.0009707700228318572 + 0.001 * 6.539590835571289
Epoch 1460, val loss: 1.617240071296692
Epoch 1470, training loss: 0.007484613452106714 = 0.0009608576656319201 + 0.001 * 6.5237555503845215
Epoch 1470, val loss: 1.6197123527526855
Epoch 1480, training loss: 0.0074609508737921715 = 0.0009512162068858743 + 0.001 * 6.509734630584717
Epoch 1480, val loss: 1.6221121549606323
Epoch 1490, training loss: 0.007465888746082783 = 0.0009418309782631695 + 0.001 * 6.524057388305664
Epoch 1490, val loss: 1.6244932413101196
Epoch 1500, training loss: 0.007442192640155554 = 0.0009326997096650302 + 0.001 * 6.509492874145508
Epoch 1500, val loss: 1.6268441677093506
Epoch 1510, training loss: 0.0074437507428228855 = 0.0009238093043677509 + 0.001 * 6.519941329956055
Epoch 1510, val loss: 1.6291776895523071
Epoch 1520, training loss: 0.0074613215401768684 = 0.0009151393896900117 + 0.001 * 6.546181678771973
Epoch 1520, val loss: 1.6314067840576172
Epoch 1530, training loss: 0.007446322124451399 = 0.0009067013743333519 + 0.001 * 6.539620399475098
Epoch 1530, val loss: 1.6336841583251953
Epoch 1540, training loss: 0.00746456254273653 = 0.0008984743035398424 + 0.001 * 6.56608772277832
Epoch 1540, val loss: 1.6358554363250732
Epoch 1550, training loss: 0.0074296630918979645 = 0.0008904754067771137 + 0.001 * 6.539187431335449
Epoch 1550, val loss: 1.6380382776260376
Epoch 1560, training loss: 0.007396084256470203 = 0.0008826638222672045 + 0.001 * 6.513420104980469
Epoch 1560, val loss: 1.640186071395874
Epoch 1570, training loss: 0.007382393814623356 = 0.000875052937772125 + 0.001 * 6.507340431213379
Epoch 1570, val loss: 1.6422911882400513
Epoch 1580, training loss: 0.0073553528636693954 = 0.0008676223224028945 + 0.001 * 6.487730026245117
Epoch 1580, val loss: 1.644383192062378
Epoch 1590, training loss: 0.0073875547386705875 = 0.0008603754686191678 + 0.001 * 6.52717924118042
Epoch 1590, val loss: 1.6464142799377441
Epoch 1600, training loss: 0.007380798924714327 = 0.0008533056825399399 + 0.001 * 6.527493000030518
Epoch 1600, val loss: 1.648423433303833
Epoch 1610, training loss: 0.00741191953420639 = 0.0008464084239676595 + 0.001 * 6.5655107498168945
Epoch 1610, val loss: 1.6504548788070679
Epoch 1620, training loss: 0.007345426827669144 = 0.0008396895136684179 + 0.001 * 6.5057373046875
Epoch 1620, val loss: 1.6523927450180054
Epoch 1630, training loss: 0.007343294098973274 = 0.000833124911878258 + 0.001 * 6.51016902923584
Epoch 1630, val loss: 1.6543318033218384
Epoch 1640, training loss: 0.007328282576054335 = 0.0008267071098089218 + 0.001 * 6.501574993133545
Epoch 1640, val loss: 1.6562471389770508
Epoch 1650, training loss: 0.0073090530931949615 = 0.0008204550831578672 + 0.001 * 6.488597869873047
Epoch 1650, val loss: 1.6581264734268188
Epoch 1660, training loss: 0.007312593050301075 = 0.0008143357699736953 + 0.001 * 6.498256683349609
Epoch 1660, val loss: 1.659935712814331
Epoch 1670, training loss: 0.007304234430193901 = 0.0008083723951131105 + 0.001 * 6.495861530303955
Epoch 1670, val loss: 1.661820650100708
Epoch 1680, training loss: 0.00728948088362813 = 0.0008025519782677293 + 0.001 * 6.486928939819336
Epoch 1680, val loss: 1.6635723114013672
Epoch 1690, training loss: 0.007289668079465628 = 0.0007968561840243638 + 0.001 * 6.49281120300293
Epoch 1690, val loss: 1.6653776168823242
Epoch 1700, training loss: 0.007291905581951141 = 0.0007912978762760758 + 0.001 * 6.500607490539551
Epoch 1700, val loss: 1.667125940322876
Epoch 1710, training loss: 0.00726346718147397 = 0.000785847834777087 + 0.001 * 6.477619171142578
Epoch 1710, val loss: 1.6688053607940674
Epoch 1720, training loss: 0.007285939063876867 = 0.000780529691837728 + 0.001 * 6.505408763885498
Epoch 1720, val loss: 1.670490026473999
Epoch 1730, training loss: 0.007268923334777355 = 0.0007753378013148904 + 0.001 * 6.493585109710693
Epoch 1730, val loss: 1.672195553779602
Epoch 1740, training loss: 0.007300599943846464 = 0.0007702432340011001 + 0.001 * 6.530356407165527
Epoch 1740, val loss: 1.6737911701202393
Epoch 1750, training loss: 0.007268589921295643 = 0.0007652861531823874 + 0.001 * 6.503303050994873
Epoch 1750, val loss: 1.6755121946334839
Epoch 1760, training loss: 0.00726283947005868 = 0.0007604211568832397 + 0.001 * 6.502418041229248
Epoch 1760, val loss: 1.6770107746124268
Epoch 1770, training loss: 0.007245438173413277 = 0.0007556808413937688 + 0.001 * 6.489757061004639
Epoch 1770, val loss: 1.678655743598938
Epoch 1780, training loss: 0.007233654148876667 = 0.0007510312716476619 + 0.001 * 6.4826226234436035
Epoch 1780, val loss: 1.6801708936691284
Epoch 1790, training loss: 0.007223924621939659 = 0.0007464803638868034 + 0.001 * 6.477444171905518
Epoch 1790, val loss: 1.6817383766174316
Epoch 1800, training loss: 0.00721004419028759 = 0.0007420313195325434 + 0.001 * 6.468012809753418
Epoch 1800, val loss: 1.683143973350525
Epoch 1810, training loss: 0.007224314846098423 = 0.0007376873982138932 + 0.001 * 6.486627101898193
Epoch 1810, val loss: 1.6847366094589233
Epoch 1820, training loss: 0.007222042419016361 = 0.0007334258407354355 + 0.001 * 6.488616466522217
Epoch 1820, val loss: 1.68613862991333
Epoch 1830, training loss: 0.007208964321762323 = 0.0007292573573067784 + 0.001 * 6.479706764221191
Epoch 1830, val loss: 1.6875991821289062
Epoch 1840, training loss: 0.00718734273687005 = 0.0007251702481880784 + 0.001 * 6.462172508239746
Epoch 1840, val loss: 1.688995122909546
Epoch 1850, training loss: 0.0071960813365876675 = 0.0007211779011413455 + 0.001 * 6.474903106689453
Epoch 1850, val loss: 1.690419316291809
Epoch 1860, training loss: 0.00716347387060523 = 0.0007172654732130468 + 0.001 * 6.4462080001831055
Epoch 1860, val loss: 1.6917825937271118
Epoch 1870, training loss: 0.007193319499492645 = 0.0007134208572097123 + 0.001 * 6.479897975921631
Epoch 1870, val loss: 1.6931401491165161
Epoch 1880, training loss: 0.007166189141571522 = 0.0007096574991010129 + 0.001 * 6.456531047821045
Epoch 1880, val loss: 1.6944690942764282
Epoch 1890, training loss: 0.007163234055042267 = 0.000705969869159162 + 0.001 * 6.457263946533203
Epoch 1890, val loss: 1.6958179473876953
Epoch 1900, training loss: 0.007186505012214184 = 0.0007023673970252275 + 0.001 * 6.484137058258057
Epoch 1900, val loss: 1.6971856355667114
Epoch 1910, training loss: 0.007159815169870853 = 0.000698812073096633 + 0.001 * 6.461002826690674
Epoch 1910, val loss: 1.6984065771102905
Epoch 1920, training loss: 0.007137970998883247 = 0.0006953371339477599 + 0.001 * 6.442633628845215
Epoch 1920, val loss: 1.6997010707855225
Epoch 1930, training loss: 0.007156124338507652 = 0.0006919271545484662 + 0.001 * 6.464197158813477
Epoch 1930, val loss: 1.7009520530700684
Epoch 1940, training loss: 0.0071343625895679 = 0.0006885681068524718 + 0.001 * 6.445794105529785
Epoch 1940, val loss: 1.7022114992141724
Epoch 1950, training loss: 0.007136368192732334 = 0.0006852881633676589 + 0.001 * 6.451079845428467
Epoch 1950, val loss: 1.7034016847610474
Epoch 1960, training loss: 0.00714938435703516 = 0.0006820643902756274 + 0.001 * 6.467319488525391
Epoch 1960, val loss: 1.7046501636505127
Epoch 1970, training loss: 0.0071493713185191154 = 0.000678875541780144 + 0.001 * 6.470495700836182
Epoch 1970, val loss: 1.705809473991394
Epoch 1980, training loss: 0.007156371604651213 = 0.0006757566006854177 + 0.001 * 6.48061466217041
Epoch 1980, val loss: 1.7069594860076904
Epoch 1990, training loss: 0.007155058905482292 = 0.0006727104773744941 + 0.001 * 6.4823479652404785
Epoch 1990, val loss: 1.7082388401031494
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8185
Overall ASR: 0.5535
Flip ASR: 0.4622/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9622316360473633 = 1.9538578987121582 + 0.001 * 8.37371826171875
Epoch 0, val loss: 1.9605529308319092
Epoch 10, training loss: 1.951411247253418 = 1.9430376291275024 + 0.001 * 8.373574256896973
Epoch 10, val loss: 1.949192762374878
Epoch 20, training loss: 1.937925100326538 = 1.9295519590377808 + 0.001 * 8.373198509216309
Epoch 20, val loss: 1.9349335432052612
Epoch 30, training loss: 1.918922781944275 = 1.9105503559112549 + 0.001 * 8.372417449951172
Epoch 30, val loss: 1.9149556159973145
Epoch 40, training loss: 1.8911731243133545 = 1.8828026056289673 + 0.001 * 8.370573997497559
Epoch 40, val loss: 1.8862885236740112
Epoch 50, training loss: 1.853089690208435 = 1.8447248935699463 + 0.001 * 8.36476993560791
Epoch 50, val loss: 1.848699688911438
Epoch 60, training loss: 1.810097336769104 = 1.801758885383606 + 0.001 * 8.338464736938477
Epoch 60, val loss: 1.809470772743225
Epoch 70, training loss: 1.7707860469818115 = 1.7626148462295532 + 0.001 * 8.171222686767578
Epoch 70, val loss: 1.7747886180877686
Epoch 80, training loss: 1.7225583791732788 = 1.7148176431655884 + 0.001 * 7.740725517272949
Epoch 80, val loss: 1.7299950122833252
Epoch 90, training loss: 1.6552265882492065 = 1.6476558446884155 + 0.001 * 7.570754051208496
Epoch 90, val loss: 1.6703999042510986
Epoch 100, training loss: 1.567099690437317 = 1.559630036354065 + 0.001 * 7.469621181488037
Epoch 100, val loss: 1.596285343170166
Epoch 110, training loss: 1.4670708179473877 = 1.4597516059875488 + 0.001 * 7.319223403930664
Epoch 110, val loss: 1.5141046047210693
Epoch 120, training loss: 1.369984745979309 = 1.362902283668518 + 0.001 * 7.082465648651123
Epoch 120, val loss: 1.4370718002319336
Epoch 130, training loss: 1.281473159790039 = 1.274495005607605 + 0.001 * 6.9781646728515625
Epoch 130, val loss: 1.3701914548873901
Epoch 140, training loss: 1.2009702920913696 = 1.1940499544143677 + 0.001 * 6.920280933380127
Epoch 140, val loss: 1.312032699584961
Epoch 150, training loss: 1.1246955394744873 = 1.1177812814712524 + 0.001 * 6.914310455322266
Epoch 150, val loss: 1.2577260732650757
Epoch 160, training loss: 1.0481016635894775 = 1.0411903858184814 + 0.001 * 6.911221981048584
Epoch 160, val loss: 1.2037605047225952
Epoch 170, training loss: 0.9686585664749146 = 0.9617549777030945 + 0.001 * 6.903573036193848
Epoch 170, val loss: 1.1469130516052246
Epoch 180, training loss: 0.8871957659721375 = 0.8803048729896545 + 0.001 * 6.8909220695495605
Epoch 180, val loss: 1.0874673128128052
Epoch 190, training loss: 0.8067703247070312 = 0.7998966574668884 + 0.001 * 6.873696804046631
Epoch 190, val loss: 1.027999997138977
Epoch 200, training loss: 0.7306697368621826 = 0.7238165140151978 + 0.001 * 6.853219985961914
Epoch 200, val loss: 0.9721649289131165
Epoch 210, training loss: 0.6607665419578552 = 0.6539345383644104 + 0.001 * 6.831976413726807
Epoch 210, val loss: 0.9230519533157349
Epoch 220, training loss: 0.5971972346305847 = 0.5903838276863098 + 0.001 * 6.813401222229004
Epoch 220, val loss: 0.8812574148178101
Epoch 230, training loss: 0.5390574336051941 = 0.5322552919387817 + 0.001 * 6.802135944366455
Epoch 230, val loss: 0.8465353846549988
Epoch 240, training loss: 0.48521262407302856 = 0.47841736674308777 + 0.001 * 6.795253276824951
Epoch 240, val loss: 0.8176478147506714
Epoch 250, training loss: 0.4346851706504822 = 0.4278942942619324 + 0.001 * 6.790886878967285
Epoch 250, val loss: 0.7933878302574158
Epoch 260, training loss: 0.387054979801178 = 0.38026779890060425 + 0.001 * 6.787184238433838
Epoch 260, val loss: 0.7731070518493652
Epoch 270, training loss: 0.34261569380760193 = 0.33583182096481323 + 0.001 * 6.78387975692749
Epoch 270, val loss: 0.7571438550949097
Epoch 280, training loss: 0.3018874228000641 = 0.29510655999183655 + 0.001 * 6.780854225158691
Epoch 280, val loss: 0.7459689974784851
Epoch 290, training loss: 0.2652263343334198 = 0.2584485113620758 + 0.001 * 6.777826309204102
Epoch 290, val loss: 0.7396789789199829
Epoch 300, training loss: 0.23268768191337585 = 0.22591310739517212 + 0.001 * 6.7745747566223145
Epoch 300, val loss: 0.7379031181335449
Epoch 310, training loss: 0.20414374768733978 = 0.1973707675933838 + 0.001 * 6.772980690002441
Epoch 310, val loss: 0.7402864694595337
Epoch 320, training loss: 0.17934635281562805 = 0.17257773876190186 + 0.001 * 6.768611907958984
Epoch 320, val loss: 0.7460782527923584
Epoch 330, training loss: 0.15801607072353363 = 0.15125182271003723 + 0.001 * 6.764252662658691
Epoch 330, val loss: 0.7546784281730652
Epoch 340, training loss: 0.13983498513698578 = 0.13307525217533112 + 0.001 * 6.759727478027344
Epoch 340, val loss: 0.7656029462814331
Epoch 350, training loss: 0.12436982989311218 = 0.11761239916086197 + 0.001 * 6.75742769241333
Epoch 350, val loss: 0.778246283531189
Epoch 360, training loss: 0.11120278388261795 = 0.10445401817560196 + 0.001 * 6.748764991760254
Epoch 360, val loss: 0.7920078635215759
Epoch 370, training loss: 0.10003431141376495 = 0.09329254925251007 + 0.001 * 6.741759300231934
Epoch 370, val loss: 0.8065703511238098
Epoch 380, training loss: 0.09047700464725494 = 0.083742156624794 + 0.001 * 6.734846591949463
Epoch 380, val loss: 0.8214271664619446
Epoch 390, training loss: 0.08221364766359329 = 0.07547727227210999 + 0.001 * 6.736374378204346
Epoch 390, val loss: 0.836330771446228
Epoch 400, training loss: 0.07498717308044434 = 0.06826363503932953 + 0.001 * 6.723536968231201
Epoch 400, val loss: 0.8513256907463074
Epoch 410, training loss: 0.06863126158714294 = 0.061916533857584 + 0.001 * 6.714726448059082
Epoch 410, val loss: 0.8663368225097656
Epoch 420, training loss: 0.06300409883260727 = 0.056289151310920715 + 0.001 * 6.714950084686279
Epoch 420, val loss: 0.8813955187797546
Epoch 430, training loss: 0.057982321828603745 = 0.05127152428030968 + 0.001 * 6.710795879364014
Epoch 430, val loss: 0.8963975310325623
Epoch 440, training loss: 0.05350589379668236 = 0.04679622873663902 + 0.001 * 6.7096662521362305
Epoch 440, val loss: 0.9114059805870056
Epoch 450, training loss: 0.04947348311543465 = 0.042770132422447205 + 0.001 * 6.7033491134643555
Epoch 450, val loss: 0.9262971878051758
Epoch 460, training loss: 0.04582693427801132 = 0.039129164069890976 + 0.001 * 6.697771072387695
Epoch 460, val loss: 0.941100001335144
Epoch 470, training loss: 0.042517486959695816 = 0.03582682088017464 + 0.001 * 6.690667152404785
Epoch 470, val loss: 0.9557134509086609
Epoch 480, training loss: 0.03951975330710411 = 0.03282960131764412 + 0.001 * 6.690150260925293
Epoch 480, val loss: 0.9700318574905396
Epoch 490, training loss: 0.03680086135864258 = 0.030114425346255302 + 0.001 * 6.686435222625732
Epoch 490, val loss: 0.9841526746749878
Epoch 500, training loss: 0.03434409573674202 = 0.027659181505441666 + 0.001 * 6.684915065765381
Epoch 500, val loss: 0.9980173707008362
Epoch 510, training loss: 0.03213328868150711 = 0.025447538122534752 + 0.001 * 6.685749053955078
Epoch 510, val loss: 1.0115411281585693
Epoch 520, training loss: 0.030137671157717705 = 0.023458782583475113 + 0.001 * 6.678888320922852
Epoch 520, val loss: 1.0250214338302612
Epoch 530, training loss: 0.028346870094537735 = 0.021674219518899918 + 0.001 * 6.6726508140563965
Epoch 530, val loss: 1.0383347272872925
Epoch 540, training loss: 0.026805365458130836 = 0.02007431536912918 + 0.001 * 6.73105001449585
Epoch 540, val loss: 1.051375389099121
Epoch 550, training loss: 0.025321289896965027 = 0.018643220886588097 + 0.001 * 6.6780686378479
Epoch 550, val loss: 1.0641552209854126
Epoch 560, training loss: 0.02402598224580288 = 0.017358887940645218 + 0.001 * 6.6670942306518555
Epoch 560, val loss: 1.0765984058380127
Epoch 570, training loss: 0.022863592952489853 = 0.016201071441173553 + 0.001 * 6.662520408630371
Epoch 570, val loss: 1.0887197256088257
Epoch 580, training loss: 0.021817123517394066 = 0.015152988955378532 + 0.001 * 6.664133548736572
Epoch 580, val loss: 1.1005481481552124
Epoch 590, training loss: 0.020864829421043396 = 0.014199992641806602 + 0.001 * 6.664835453033447
Epoch 590, val loss: 1.1120226383209229
Epoch 600, training loss: 0.01998892053961754 = 0.013333531096577644 + 0.001 * 6.655389785766602
Epoch 600, val loss: 1.1232572793960571
Epoch 610, training loss: 0.019198432564735413 = 0.012546713463962078 + 0.001 * 6.651719570159912
Epoch 610, val loss: 1.1341304779052734
Epoch 620, training loss: 0.018478669226169586 = 0.011828948743641376 + 0.001 * 6.649721145629883
Epoch 620, val loss: 1.1448419094085693
Epoch 630, training loss: 0.017820704728364944 = 0.01117370743304491 + 0.001 * 6.646997928619385
Epoch 630, val loss: 1.155187964439392
Epoch 640, training loss: 0.017237793654203415 = 0.0105741573497653 + 0.001 * 6.663636684417725
Epoch 640, val loss: 1.1652191877365112
Epoch 650, training loss: 0.016675084829330444 = 0.010023864917457104 + 0.001 * 6.651219367980957
Epoch 650, val loss: 1.1749969720840454
Epoch 660, training loss: 0.016161251813173294 = 0.009517081081867218 + 0.001 * 6.64417028427124
Epoch 660, val loss: 1.1845579147338867
Epoch 670, training loss: 0.015697110444307327 = 0.009049287997186184 + 0.001 * 6.647822856903076
Epoch 670, val loss: 1.1938164234161377
Epoch 680, training loss: 0.015258532017469406 = 0.008616750128567219 + 0.001 * 6.641781330108643
Epoch 680, val loss: 1.2028363943099976
Epoch 690, training loss: 0.014861045405268669 = 0.008215826004743576 + 0.001 * 6.645218849182129
Epoch 690, val loss: 1.2116895914077759
Epoch 700, training loss: 0.014483710750937462 = 0.007843375205993652 + 0.001 * 6.640335559844971
Epoch 700, val loss: 1.2203104496002197
Epoch 710, training loss: 0.01414809562265873 = 0.007496184203773737 + 0.001 * 6.651911735534668
Epoch 710, val loss: 1.2288146018981934
Epoch 720, training loss: 0.013803674839437008 = 0.007171685807406902 + 0.001 * 6.631988525390625
Epoch 720, val loss: 1.2370537519454956
Epoch 730, training loss: 0.013501148670911789 = 0.0068670958280563354 + 0.001 * 6.634052753448486
Epoch 730, val loss: 1.245213270187378
Epoch 740, training loss: 0.013213828206062317 = 0.006580362096428871 + 0.001 * 6.6334662437438965
Epoch 740, val loss: 1.2531806230545044
Epoch 750, training loss: 0.012938981875777245 = 0.006309692747890949 + 0.001 * 6.629289150238037
Epoch 750, val loss: 1.261142373085022
Epoch 760, training loss: 0.012691644951701164 = 0.006053002085536718 + 0.001 * 6.63864278793335
Epoch 760, val loss: 1.2690510749816895
Epoch 770, training loss: 0.012444854713976383 = 0.005808864254504442 + 0.001 * 6.635990142822266
Epoch 770, val loss: 1.2769086360931396
Epoch 780, training loss: 0.0122092105448246 = 0.005576447118073702 + 0.001 * 6.632763385772705
Epoch 780, val loss: 1.2847226858139038
Epoch 790, training loss: 0.011982666328549385 = 0.005355248227715492 + 0.001 * 6.627418041229248
Epoch 790, val loss: 1.2924766540527344
Epoch 800, training loss: 0.011771641671657562 = 0.005145021714270115 + 0.001 * 6.626619338989258
Epoch 800, val loss: 1.3001829385757446
Epoch 810, training loss: 0.011594949290156364 = 0.004945614840835333 + 0.001 * 6.649333953857422
Epoch 810, val loss: 1.3077830076217651
Epoch 820, training loss: 0.011387145146727562 = 0.004756792448461056 + 0.001 * 6.63035249710083
Epoch 820, val loss: 1.3153789043426514
Epoch 830, training loss: 0.011203871108591557 = 0.004578114952892065 + 0.001 * 6.625755786895752
Epoch 830, val loss: 1.3228744268417358
Epoch 840, training loss: 0.011035636067390442 = 0.004409148823469877 + 0.001 * 6.626486778259277
Epoch 840, val loss: 1.3302853107452393
Epoch 850, training loss: 0.010876128450036049 = 0.004249387886375189 + 0.001 * 6.626739978790283
Epoch 850, val loss: 1.3375712633132935
Epoch 860, training loss: 0.010719066485762596 = 0.004098388832062483 + 0.001 * 6.6206769943237305
Epoch 860, val loss: 1.344787836074829
Epoch 870, training loss: 0.010619054548442364 = 0.003955548163503408 + 0.001 * 6.663506031036377
Epoch 870, val loss: 1.3519359827041626
Epoch 880, training loss: 0.010450840927660465 = 0.0038204502779990435 + 0.001 * 6.630390167236328
Epoch 880, val loss: 1.3589370250701904
Epoch 890, training loss: 0.010311868973076344 = 0.0036926039028912783 + 0.001 * 6.619265079498291
Epoch 890, val loss: 1.3658461570739746
Epoch 900, training loss: 0.010199022479355335 = 0.0035715417470782995 + 0.001 * 6.6274800300598145
Epoch 900, val loss: 1.3726251125335693
Epoch 910, training loss: 0.010073402896523476 = 0.0034568309783935547 + 0.001 * 6.616570949554443
Epoch 910, val loss: 1.3793245553970337
Epoch 920, training loss: 0.009966270066797733 = 0.003347948892042041 + 0.001 * 6.618320941925049
Epoch 920, val loss: 1.3859467506408691
Epoch 930, training loss: 0.00987514853477478 = 0.003244563238695264 + 0.001 * 6.630585193634033
Epoch 930, val loss: 1.3924462795257568
Epoch 940, training loss: 0.009770624339580536 = 0.0031463969498872757 + 0.001 * 6.6242265701293945
Epoch 940, val loss: 1.3988152742385864
Epoch 950, training loss: 0.009677371941506863 = 0.0030531471129506826 + 0.001 * 6.624224662780762
Epoch 950, val loss: 1.4050848484039307
Epoch 960, training loss: 0.009579617530107498 = 0.002964457031339407 + 0.001 * 6.615159511566162
Epoch 960, val loss: 1.411278247833252
Epoch 970, training loss: 0.009502265602350235 = 0.0028798289131373167 + 0.001 * 6.6224365234375
Epoch 970, val loss: 1.4174331426620483
Epoch 980, training loss: 0.009412280283868313 = 0.0027991493698209524 + 0.001 * 6.613130569458008
Epoch 980, val loss: 1.4234318733215332
Epoch 990, training loss: 0.009355362504720688 = 0.002722238888964057 + 0.001 * 6.633123397827148
Epoch 990, val loss: 1.429376244544983
Epoch 1000, training loss: 0.009268764406442642 = 0.0026488155126571655 + 0.001 * 6.619948387145996
Epoch 1000, val loss: 1.435269832611084
Epoch 1010, training loss: 0.009187842719256878 = 0.0025787241756916046 + 0.001 * 6.609118461608887
Epoch 1010, val loss: 1.4410134553909302
Epoch 1020, training loss: 0.009128816425800323 = 0.0025117925833910704 + 0.001 * 6.617023944854736
Epoch 1020, val loss: 1.4467229843139648
Epoch 1030, training loss: 0.009060374461114407 = 0.0024478875566273928 + 0.001 * 6.612486362457275
Epoch 1030, val loss: 1.4523195028305054
Epoch 1040, training loss: 0.008989935740828514 = 0.0023868039716035128 + 0.001 * 6.603131294250488
Epoch 1040, val loss: 1.457869529724121
Epoch 1050, training loss: 0.008944991044700146 = 0.0023283748887479305 + 0.001 * 6.6166157722473145
Epoch 1050, val loss: 1.4632869958877563
Epoch 1060, training loss: 0.008892012760043144 = 0.002272440353408456 + 0.001 * 6.619572162628174
Epoch 1060, val loss: 1.468565821647644
Epoch 1070, training loss: 0.008832687512040138 = 0.0022188578732311726 + 0.001 * 6.613829612731934
Epoch 1070, val loss: 1.473796010017395
Epoch 1080, training loss: 0.00876921322196722 = 0.00216755666770041 + 0.001 * 6.601655960083008
Epoch 1080, val loss: 1.4789948463439941
Epoch 1090, training loss: 0.008730513043701649 = 0.0021183521021157503 + 0.001 * 6.612160682678223
Epoch 1090, val loss: 1.4841439723968506
Epoch 1100, training loss: 0.008679430931806564 = 0.0020711002871394157 + 0.001 * 6.608330249786377
Epoch 1100, val loss: 1.489187479019165
Epoch 1110, training loss: 0.008645172230899334 = 0.0020257283467799425 + 0.001 * 6.619443893432617
Epoch 1110, val loss: 1.4940603971481323
Epoch 1120, training loss: 0.008589908480644226 = 0.001982172019779682 + 0.001 * 6.607735633850098
Epoch 1120, val loss: 1.4989351034164429
Epoch 1130, training loss: 0.008538797497749329 = 0.001940307323820889 + 0.001 * 6.598489761352539
Epoch 1130, val loss: 1.5036758184432983
Epoch 1140, training loss: 0.008522668853402138 = 0.0019000711617991328 + 0.001 * 6.622597694396973
Epoch 1140, val loss: 1.5083410739898682
Epoch 1150, training loss: 0.008471362292766571 = 0.0018613552674651146 + 0.001 * 6.610007286071777
Epoch 1150, val loss: 1.513059139251709
Epoch 1160, training loss: 0.0084293307736516 = 0.0018241434590891004 + 0.001 * 6.605186939239502
Epoch 1160, val loss: 1.5176489353179932
Epoch 1170, training loss: 0.008394870907068253 = 0.0017883173422887921 + 0.001 * 6.606553077697754
Epoch 1170, val loss: 1.522166132926941
Epoch 1180, training loss: 0.00835617445409298 = 0.0017538495594635606 + 0.001 * 6.60232400894165
Epoch 1180, val loss: 1.52657151222229
Epoch 1190, training loss: 0.008319871500134468 = 0.0017206623451784253 + 0.001 * 6.599208831787109
Epoch 1190, val loss: 1.5309460163116455
Epoch 1200, training loss: 0.008281199261546135 = 0.0016886771190911531 + 0.001 * 6.592522144317627
Epoch 1200, val loss: 1.5352627038955688
Epoch 1210, training loss: 0.008256356231868267 = 0.0016578474314883351 + 0.001 * 6.598508358001709
Epoch 1210, val loss: 1.5394355058670044
Epoch 1220, training loss: 0.008236431516706944 = 0.001628098194487393 + 0.001 * 6.608332633972168
Epoch 1220, val loss: 1.5435868501663208
Epoch 1230, training loss: 0.008205009624361992 = 0.001599385286681354 + 0.001 * 6.605624198913574
Epoch 1230, val loss: 1.547642469406128
Epoch 1240, training loss: 0.008166215382516384 = 0.0015716975321993232 + 0.001 * 6.594517707824707
Epoch 1240, val loss: 1.5517232418060303
Epoch 1250, training loss: 0.008137719705700874 = 0.0015449560014531016 + 0.001 * 6.592763900756836
Epoch 1250, val loss: 1.5557664632797241
Epoch 1260, training loss: 0.008108267560601234 = 0.0015191251877695322 + 0.001 * 6.589141845703125
Epoch 1260, val loss: 1.5596779584884644
Epoch 1270, training loss: 0.008122550323605537 = 0.0014941808767616749 + 0.001 * 6.628369331359863
Epoch 1270, val loss: 1.5635207891464233
Epoch 1280, training loss: 0.008054614998400211 = 0.0014700873289257288 + 0.001 * 6.584527492523193
Epoch 1280, val loss: 1.5673073530197144
Epoch 1290, training loss: 0.008044946007430553 = 0.001446795416995883 + 0.001 * 6.598150730133057
Epoch 1290, val loss: 1.5711206197738647
Epoch 1300, training loss: 0.008010080084204674 = 0.0014242562465369701 + 0.001 * 6.5858235359191895
Epoch 1300, val loss: 1.5748074054718018
Epoch 1310, training loss: 0.00800822488963604 = 0.001402455847710371 + 0.001 * 6.60576868057251
Epoch 1310, val loss: 1.5784189701080322
Epoch 1320, training loss: 0.007964001037180424 = 0.001381394569762051 + 0.001 * 6.582606315612793
Epoch 1320, val loss: 1.5820896625518799
Epoch 1330, training loss: 0.007950716651976109 = 0.001361000700853765 + 0.001 * 6.589715480804443
Epoch 1330, val loss: 1.5856562852859497
Epoch 1340, training loss: 0.007918701507151127 = 0.0013413060223683715 + 0.001 * 6.577395439147949
Epoch 1340, val loss: 1.5891292095184326
Epoch 1350, training loss: 0.007898939773440361 = 0.0013222552370280027 + 0.001 * 6.57668399810791
Epoch 1350, val loss: 1.5925543308258057
Epoch 1360, training loss: 0.007882906123995781 = 0.0013038446195423603 + 0.001 * 6.579061508178711
Epoch 1360, val loss: 1.5958613157272339
Epoch 1370, training loss: 0.007874803617596626 = 0.0012859924463555217 + 0.001 * 6.58881139755249
Epoch 1370, val loss: 1.5991603136062622
Epoch 1380, training loss: 0.007875217124819756 = 0.0012687137350440025 + 0.001 * 6.606503486633301
Epoch 1380, val loss: 1.602410078048706
Epoch 1390, training loss: 0.007836339995265007 = 0.0012519657611846924 + 0.001 * 6.584373474121094
Epoch 1390, val loss: 1.6056987047195435
Epoch 1400, training loss: 0.007810799404978752 = 0.0012357275700196624 + 0.001 * 6.575071334838867
Epoch 1400, val loss: 1.608832597732544
Epoch 1410, training loss: 0.00780116580426693 = 0.001219996833242476 + 0.001 * 6.5811686515808105
Epoch 1410, val loss: 1.6118862628936768
Epoch 1420, training loss: 0.007787617389112711 = 0.0012047399068251252 + 0.001 * 6.582877159118652
Epoch 1420, val loss: 1.6150192022323608
Epoch 1430, training loss: 0.007788697257637978 = 0.001189904985949397 + 0.001 * 6.598791599273682
Epoch 1430, val loss: 1.6179834604263306
Epoch 1440, training loss: 0.007752677891403437 = 0.0011755326995626092 + 0.001 * 6.577145099639893
Epoch 1440, val loss: 1.621031641960144
Epoch 1450, training loss: 0.007773264776915312 = 0.0011615636758506298 + 0.001 * 6.611701011657715
Epoch 1450, val loss: 1.6239070892333984
Epoch 1460, training loss: 0.007717844098806381 = 0.0011479912791401148 + 0.001 * 6.569852828979492
Epoch 1460, val loss: 1.626800298690796
Epoch 1470, training loss: 0.0077039264142513275 = 0.0011348107364028692 + 0.001 * 6.569115161895752
Epoch 1470, val loss: 1.629649043083191
Epoch 1480, training loss: 0.007708512246608734 = 0.0011220076121389866 + 0.001 * 6.5865044593811035
Epoch 1480, val loss: 1.6324348449707031
Epoch 1490, training loss: 0.007680775132030249 = 0.0011095619993284345 + 0.001 * 6.5712127685546875
Epoch 1490, val loss: 1.6352280378341675
Epoch 1500, training loss: 0.0076622674241662025 = 0.0010974600445479155 + 0.001 * 6.564806938171387
Epoch 1500, val loss: 1.6379714012145996
Epoch 1510, training loss: 0.007661730516701937 = 0.001085684634745121 + 0.001 * 6.576045513153076
Epoch 1510, val loss: 1.6405507326126099
Epoch 1520, training loss: 0.007671062834560871 = 0.001074269530363381 + 0.001 * 6.596793174743652
Epoch 1520, val loss: 1.6432723999023438
Epoch 1530, training loss: 0.00763594638556242 = 0.0010631663026288152 + 0.001 * 6.572779655456543
Epoch 1530, val loss: 1.645904779434204
Epoch 1540, training loss: 0.007618446834385395 = 0.0010523565579205751 + 0.001 * 6.566089630126953
Epoch 1540, val loss: 1.6484535932540894
Epoch 1550, training loss: 0.007641706615686417 = 0.0010418413439765573 + 0.001 * 6.599864959716797
Epoch 1550, val loss: 1.6509133577346802
Epoch 1560, training loss: 0.007605642080307007 = 0.0010316029656678438 + 0.001 * 6.574038505554199
Epoch 1560, val loss: 1.6533795595169067
Epoch 1570, training loss: 0.007594076916575432 = 0.0010216294322162867 + 0.001 * 6.572446823120117
Epoch 1570, val loss: 1.6559038162231445
Epoch 1580, training loss: 0.007569490931928158 = 0.0010119291255250573 + 0.001 * 6.55756139755249
Epoch 1580, val loss: 1.658275842666626
Epoch 1590, training loss: 0.007566499523818493 = 0.0010024571092799306 + 0.001 * 6.564042091369629
Epoch 1590, val loss: 1.6606895923614502
Epoch 1600, training loss: 0.007583435624837875 = 0.0009932152461260557 + 0.001 * 6.5902204513549805
Epoch 1600, val loss: 1.6630535125732422
Epoch 1610, training loss: 0.007553685922175646 = 0.0009842365980148315 + 0.001 * 6.569448947906494
Epoch 1610, val loss: 1.6653664112091064
Epoch 1620, training loss: 0.0075569553300738335 = 0.0009754978236742318 + 0.001 * 6.581457138061523
Epoch 1620, val loss: 1.6676920652389526
Epoch 1630, training loss: 0.007542089093476534 = 0.0009669371065683663 + 0.001 * 6.575151443481445
Epoch 1630, val loss: 1.6698919534683228
Epoch 1640, training loss: 0.0075378259643912315 = 0.0009586403029970825 + 0.001 * 6.579185485839844
Epoch 1640, val loss: 1.6721665859222412
Epoch 1650, training loss: 0.007505015004426241 = 0.0009504716144874692 + 0.001 * 6.5545430183410645
Epoch 1650, val loss: 1.674381136894226
Epoch 1660, training loss: 0.00748942606151104 = 0.000942540995310992 + 0.001 * 6.546885013580322
Epoch 1660, val loss: 1.6764925718307495
Epoch 1670, training loss: 0.007497308310121298 = 0.0009348043240606785 + 0.001 * 6.562503814697266
Epoch 1670, val loss: 1.6786940097808838
Epoch 1680, training loss: 0.007476255763322115 = 0.0009272693423554301 + 0.001 * 6.548985958099365
Epoch 1680, val loss: 1.6808005571365356
Epoch 1690, training loss: 0.007493101060390472 = 0.0009199728956446052 + 0.001 * 6.573127746582031
Epoch 1690, val loss: 1.6827948093414307
Epoch 1700, training loss: 0.007473058067262173 = 0.0009128616657108068 + 0.001 * 6.560196399688721
Epoch 1700, val loss: 1.684907078742981
Epoch 1710, training loss: 0.007454115431755781 = 0.0009059103322215378 + 0.001 * 6.54820442199707
Epoch 1710, val loss: 1.6869158744812012
Epoch 1720, training loss: 0.007476687431335449 = 0.000899115577340126 + 0.001 * 6.577571392059326
Epoch 1720, val loss: 1.6888517141342163
Epoch 1730, training loss: 0.007445137482136488 = 0.000892507960088551 + 0.001 * 6.552629470825195
Epoch 1730, val loss: 1.6908819675445557
Epoch 1740, training loss: 0.007480389904230833 = 0.000886032881680876 + 0.001 * 6.594356536865234
Epoch 1740, val loss: 1.6928317546844482
Epoch 1750, training loss: 0.007425051182508469 = 0.0008797225309535861 + 0.001 * 6.545328140258789
Epoch 1750, val loss: 1.694811224937439
Epoch 1760, training loss: 0.007420131471008062 = 0.0008735509472899139 + 0.001 * 6.5465803146362305
Epoch 1760, val loss: 1.6967049837112427
Epoch 1770, training loss: 0.007420949172228575 = 0.0008675205172039568 + 0.001 * 6.5534281730651855
Epoch 1770, val loss: 1.6985081434249878
Epoch 1780, training loss: 0.007410208694636822 = 0.0008616450941190124 + 0.001 * 6.548563003540039
Epoch 1780, val loss: 1.7003861665725708
Epoch 1790, training loss: 0.00741832097992301 = 0.0008558836998417974 + 0.001 * 6.562437057495117
Epoch 1790, val loss: 1.7021461725234985
Epoch 1800, training loss: 0.0073895566165447235 = 0.0008502688142471015 + 0.001 * 6.539287567138672
Epoch 1800, val loss: 1.7039233446121216
Epoch 1810, training loss: 0.007412192411720753 = 0.0008447878644801676 + 0.001 * 6.567404270172119
Epoch 1810, val loss: 1.7057238817214966
Epoch 1820, training loss: 0.007394303102046251 = 0.0008394063333980739 + 0.001 * 6.554896831512451
Epoch 1820, val loss: 1.7074432373046875
Epoch 1830, training loss: 0.007363449782133102 = 0.0008341243956238031 + 0.001 * 6.529325485229492
Epoch 1830, val loss: 1.7091621160507202
Epoch 1840, training loss: 0.007369745522737503 = 0.0008289854740723968 + 0.001 * 6.540759563446045
Epoch 1840, val loss: 1.7109543085098267
Epoch 1850, training loss: 0.007370654493570328 = 0.0008239280432462692 + 0.001 * 6.546726226806641
Epoch 1850, val loss: 1.7125598192214966
Epoch 1860, training loss: 0.007361436262726784 = 0.0008189983200281858 + 0.001 * 6.542437553405762
Epoch 1860, val loss: 1.7141778469085693
Epoch 1870, training loss: 0.00736823258921504 = 0.0008141744765453041 + 0.001 * 6.554058074951172
Epoch 1870, val loss: 1.7158355712890625
Epoch 1880, training loss: 0.007341655436903238 = 0.0008094087243080139 + 0.001 * 6.5322465896606445
Epoch 1880, val loss: 1.7173986434936523
Epoch 1890, training loss: 0.007342725060880184 = 0.0008047398296184838 + 0.001 * 6.537984848022461
Epoch 1890, val loss: 1.7190604209899902
Epoch 1900, training loss: 0.007357375230640173 = 0.0008001422393135726 + 0.001 * 6.557232856750488
Epoch 1900, val loss: 1.7205504179000854
Epoch 1910, training loss: 0.007317651063203812 = 0.0007956330664455891 + 0.001 * 6.522017478942871
Epoch 1910, val loss: 1.7221794128417969
Epoch 1920, training loss: 0.007329063955694437 = 0.0007912702276371419 + 0.001 * 6.537793159484863
Epoch 1920, val loss: 1.7236131429672241
Epoch 1930, training loss: 0.007329101208597422 = 0.0007870161207392812 + 0.001 * 6.542084693908691
Epoch 1930, val loss: 1.72518789768219
Epoch 1940, training loss: 0.007301868870854378 = 0.0007828304660506546 + 0.001 * 6.51903772354126
Epoch 1940, val loss: 1.7266241312026978
Epoch 1950, training loss: 0.007309837732464075 = 0.0007787245558574796 + 0.001 * 6.531113147735596
Epoch 1950, val loss: 1.7281005382537842
Epoch 1960, training loss: 0.007304300554096699 = 0.0007747233030386269 + 0.001 * 6.529576778411865
Epoch 1960, val loss: 1.729539394378662
Epoch 1970, training loss: 0.00732413912191987 = 0.0007708079065196216 + 0.001 * 6.553330898284912
Epoch 1970, val loss: 1.7310585975646973
Epoch 1980, training loss: 0.0072960155084729195 = 0.0007669388432987034 + 0.001 * 6.529076099395752
Epoch 1980, val loss: 1.7324621677398682
Epoch 1990, training loss: 0.007311517372727394 = 0.0007631704793311656 + 0.001 * 6.548346519470215
Epoch 1990, val loss: 1.7337573766708374
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7749
Flip ASR: 0.7333/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9520456790924072 = 1.9436719417572021 + 0.001 * 8.373714447021484
Epoch 0, val loss: 1.9474505186080933
Epoch 10, training loss: 1.9421672821044922 = 1.9337936639785767 + 0.001 * 8.37364387512207
Epoch 10, val loss: 1.937036156654358
Epoch 20, training loss: 1.9305169582366943 = 1.9221436977386475 + 0.001 * 8.373309135437012
Epoch 20, val loss: 1.9244818687438965
Epoch 30, training loss: 1.9144198894500732 = 1.9060472249984741 + 0.001 * 8.372608184814453
Epoch 30, val loss: 1.9071413278579712
Epoch 40, training loss: 1.8907347917556763 = 1.8823637962341309 + 0.001 * 8.37100887298584
Epoch 40, val loss: 1.8818891048431396
Epoch 50, training loss: 1.856467604637146 = 1.8481013774871826 + 0.001 * 8.366215705871582
Epoch 50, val loss: 1.846456527709961
Epoch 60, training loss: 1.8125447034835815 = 1.804198980331421 + 0.001 * 8.345765113830566
Epoch 60, val loss: 1.8037827014923096
Epoch 70, training loss: 1.7662482261657715 = 1.758021593093872 + 0.001 * 8.226655960083008
Epoch 70, val loss: 1.7627618312835693
Epoch 80, training loss: 1.7138577699661255 = 1.706171989440918 + 0.001 * 7.685797214508057
Epoch 80, val loss: 1.7177568674087524
Epoch 90, training loss: 1.6420903205871582 = 1.6347382068634033 + 0.001 * 7.352067470550537
Epoch 90, val loss: 1.6560428142547607
Epoch 100, training loss: 1.5473074913024902 = 1.5401138067245483 + 0.001 * 7.193648338317871
Epoch 100, val loss: 1.5761075019836426
Epoch 110, training loss: 1.4333974123001099 = 1.4262560606002808 + 0.001 * 7.14131498336792
Epoch 110, val loss: 1.4841830730438232
Epoch 120, training loss: 1.310941457748413 = 1.303855538368225 + 0.001 * 7.085937976837158
Epoch 120, val loss: 1.3867655992507935
Epoch 130, training loss: 1.1879608631134033 = 1.1809459924697876 + 0.001 * 7.0148234367370605
Epoch 130, val loss: 1.2918328046798706
Epoch 140, training loss: 1.068615198135376 = 1.0616813898086548 + 0.001 * 6.933857440948486
Epoch 140, val loss: 1.200560450553894
Epoch 150, training loss: 0.9571813941001892 = 0.95029616355896 + 0.001 * 6.885251998901367
Epoch 150, val loss: 1.116782307624817
Epoch 160, training loss: 0.8567584156990051 = 0.8498867750167847 + 0.001 * 6.87162446975708
Epoch 160, val loss: 1.0433348417282104
Epoch 170, training loss: 0.7679485082626343 = 0.761091947555542 + 0.001 * 6.856543064117432
Epoch 170, val loss: 0.9809551239013672
Epoch 180, training loss: 0.6897723078727722 = 0.6829298734664917 + 0.001 * 6.842458248138428
Epoch 180, val loss: 0.9287853240966797
Epoch 190, training loss: 0.6207985877990723 = 0.6139658689498901 + 0.001 * 6.832736968994141
Epoch 190, val loss: 0.8856900930404663
Epoch 200, training loss: 0.5596431493759155 = 0.5528168082237244 + 0.001 * 6.826315879821777
Epoch 200, val loss: 0.8501267433166504
Epoch 210, training loss: 0.5049147605895996 = 0.49809253215789795 + 0.001 * 6.822250843048096
Epoch 210, val loss: 0.820935845375061
Epoch 220, training loss: 0.45538395643234253 = 0.44856497645378113 + 0.001 * 6.8189897537231445
Epoch 220, val loss: 0.7971813082695007
Epoch 230, training loss: 0.40999987721443176 = 0.40318363904953003 + 0.001 * 6.816235542297363
Epoch 230, val loss: 0.7779427766799927
Epoch 240, training loss: 0.3678951859474182 = 0.36108192801475525 + 0.001 * 6.813272476196289
Epoch 240, val loss: 0.7623910307884216
Epoch 250, training loss: 0.32856589555740356 = 0.32175615429878235 + 0.001 * 6.809747219085693
Epoch 250, val loss: 0.7497835755348206
Epoch 260, training loss: 0.29184967279434204 = 0.2850440442562103 + 0.001 * 6.805638313293457
Epoch 260, val loss: 0.739812970161438
Epoch 270, training loss: 0.2578662931919098 = 0.25106438994407654 + 0.001 * 6.801891326904297
Epoch 270, val loss: 0.7325262427330017
Epoch 280, training loss: 0.22682364284992218 = 0.2200266569852829 + 0.001 * 6.796990871429443
Epoch 280, val loss: 0.7279013991355896
Epoch 290, training loss: 0.19892896711826324 = 0.19213950634002686 + 0.001 * 6.789464473724365
Epoch 290, val loss: 0.726012647151947
Epoch 300, training loss: 0.1742480993270874 = 0.16746416687965393 + 0.001 * 6.783931255340576
Epoch 300, val loss: 0.7270663976669312
Epoch 310, training loss: 0.15269407629966736 = 0.1459171622991562 + 0.001 * 6.776909351348877
Epoch 310, val loss: 0.7312409281730652
Epoch 320, training loss: 0.1340000480413437 = 0.12723104655742645 + 0.001 * 6.769006729125977
Epoch 320, val loss: 0.7382171154022217
Epoch 330, training loss: 0.11788668483495712 = 0.1111229732632637 + 0.001 * 6.763713836669922
Epoch 330, val loss: 0.7478046417236328
Epoch 340, training loss: 0.10408582538366318 = 0.09733643382787704 + 0.001 * 6.749393939971924
Epoch 340, val loss: 0.7597140669822693
Epoch 350, training loss: 0.0923173725605011 = 0.08555644750595093 + 0.001 * 6.760921001434326
Epoch 350, val loss: 0.7733893990516663
Epoch 360, training loss: 0.08221208304166794 = 0.07547535747289658 + 0.001 * 6.736728668212891
Epoch 360, val loss: 0.7884454727172852
Epoch 370, training loss: 0.07356318831443787 = 0.06684447079896927 + 0.001 * 6.718715667724609
Epoch 370, val loss: 0.804665744304657
Epoch 380, training loss: 0.06618427485227585 = 0.05943596735596657 + 0.001 * 6.74830961227417
Epoch 380, val loss: 0.8216105103492737
Epoch 390, training loss: 0.05978183448314667 = 0.05306774005293846 + 0.001 * 6.714094638824463
Epoch 390, val loss: 0.8390313982963562
Epoch 400, training loss: 0.054263923317193985 = 0.04757456108927727 + 0.001 * 6.689363479614258
Epoch 400, val loss: 0.8566596508026123
Epoch 410, training loss: 0.04953766614198685 = 0.04282392933964729 + 0.001 * 6.713735103607178
Epoch 410, val loss: 0.8743981719017029
Epoch 420, training loss: 0.04538194090127945 = 0.03870364651083946 + 0.001 * 6.678292274475098
Epoch 420, val loss: 0.8919029831886292
Epoch 430, training loss: 0.04179097339510918 = 0.03511849790811539 + 0.001 * 6.672475337982178
Epoch 430, val loss: 0.9091823101043701
Epoch 440, training loss: 0.03866996243596077 = 0.03198562189936638 + 0.001 * 6.68433952331543
Epoch 440, val loss: 0.9259942173957825
Epoch 450, training loss: 0.03590357303619385 = 0.029237894341349602 + 0.001 * 6.665676593780518
Epoch 450, val loss: 0.942383348941803
Epoch 460, training loss: 0.03348303586244583 = 0.026817988604307175 + 0.001 * 6.665046691894531
Epoch 460, val loss: 0.9582617878913879
Epoch 470, training loss: 0.03133770823478699 = 0.024676647037267685 + 0.001 * 6.661059856414795
Epoch 470, val loss: 0.9737008810043335
Epoch 480, training loss: 0.029433773830533028 = 0.022774307057261467 + 0.001 * 6.659466743469238
Epoch 480, val loss: 0.9886613488197327
Epoch 490, training loss: 0.027728287503123283 = 0.02107841707766056 + 0.001 * 6.649869441986084
Epoch 490, val loss: 1.0031567811965942
Epoch 500, training loss: 0.026211660355329514 = 0.01956099644303322 + 0.001 * 6.650664329528809
Epoch 500, val loss: 1.0172109603881836
Epoch 510, training loss: 0.0248381569981575 = 0.01819881983101368 + 0.001 * 6.639337062835693
Epoch 510, val loss: 1.0308183431625366
Epoch 520, training loss: 0.023642776533961296 = 0.01697213016450405 + 0.001 * 6.670645713806152
Epoch 520, val loss: 1.0440434217453003
Epoch 530, training loss: 0.022518176585435867 = 0.015864327549934387 + 0.001 * 6.653847694396973
Epoch 530, val loss: 1.0568549633026123
Epoch 540, training loss: 0.021504376083612442 = 0.014860984869301319 + 0.001 * 6.643391132354736
Epoch 540, val loss: 1.0693333148956299
Epoch 550, training loss: 0.02058262936770916 = 0.013950131833553314 + 0.001 * 6.632497787475586
Epoch 550, val loss: 1.0814642906188965
Epoch 560, training loss: 0.01974673382937908 = 0.01312081515789032 + 0.001 * 6.625918388366699
Epoch 560, val loss: 1.093226432800293
Epoch 570, training loss: 0.019002487882971764 = 0.012363921850919724 + 0.001 * 6.638566017150879
Epoch 570, val loss: 1.1046522855758667
Epoch 580, training loss: 0.018294746056199074 = 0.011671515181660652 + 0.001 * 6.623230457305908
Epoch 580, val loss: 1.1158151626586914
Epoch 590, training loss: 0.017671454697847366 = 0.011036687530577183 + 0.001 * 6.634768009185791
Epoch 590, val loss: 1.1266555786132812
Epoch 600, training loss: 0.017077812924981117 = 0.010453400202095509 + 0.001 * 6.624413013458252
Epoch 600, val loss: 1.1372133493423462
Epoch 610, training loss: 0.016543976962566376 = 0.009916368871927261 + 0.001 * 6.627607345581055
Epoch 610, val loss: 1.147458553314209
Epoch 620, training loss: 0.016041679307818413 = 0.009420977905392647 + 0.001 * 6.620701313018799
Epoch 620, val loss: 1.1574517488479614
Epoch 630, training loss: 0.015595324337482452 = 0.008963138796389103 + 0.001 * 6.632184982299805
Epoch 630, val loss: 1.167203426361084
Epoch 640, training loss: 0.015155978500843048 = 0.008539185859262943 + 0.001 * 6.616792678833008
Epoch 640, val loss: 1.1767051219940186
Epoch 650, training loss: 0.014781296253204346 = 0.008145976811647415 + 0.001 * 6.635319709777832
Epoch 650, val loss: 1.1859776973724365
Epoch 660, training loss: 0.014413725584745407 = 0.007780649699270725 + 0.001 * 6.633075714111328
Epoch 660, val loss: 1.195021390914917
Epoch 670, training loss: 0.014054895378649235 = 0.007440668996423483 + 0.001 * 6.6142258644104
Epoch 670, val loss: 1.2038506269454956
Epoch 680, training loss: 0.013737352564930916 = 0.00712377205491066 + 0.001 * 6.613580226898193
Epoch 680, val loss: 1.2124706506729126
Epoch 690, training loss: 0.013442231342196465 = 0.00682792067527771 + 0.001 * 6.614310264587402
Epoch 690, val loss: 1.2208703756332397
Epoch 700, training loss: 0.013184484094381332 = 0.006551295053213835 + 0.001 * 6.633188724517822
Epoch 700, val loss: 1.2290736436843872
Epoch 710, training loss: 0.012912468053400517 = 0.006292314268648624 + 0.001 * 6.620153427124023
Epoch 710, val loss: 1.2371068000793457
Epoch 720, training loss: 0.012671438045799732 = 0.006049495190382004 + 0.001 * 6.621942520141602
Epoch 720, val loss: 1.2449398040771484
Epoch 730, training loss: 0.012440821155905724 = 0.005821545608341694 + 0.001 * 6.6192755699157715
Epoch 730, val loss: 1.2526206970214844
Epoch 740, training loss: 0.012218205258250237 = 0.005607294850051403 + 0.001 * 6.610909461975098
Epoch 740, val loss: 1.2601237297058105
Epoch 750, training loss: 0.012014959007501602 = 0.005405656527727842 + 0.001 * 6.609302043914795
Epoch 750, val loss: 1.2674651145935059
Epoch 760, training loss: 0.011822119355201721 = 0.005215674638748169 + 0.001 * 6.606443881988525
Epoch 760, val loss: 1.2746301889419556
Epoch 770, training loss: 0.011651589535176754 = 0.005036456044763327 + 0.001 * 6.615133285522461
Epoch 770, val loss: 1.2816578149795532
Epoch 780, training loss: 0.011476635001599789 = 0.004867213312536478 + 0.001 * 6.609421253204346
Epoch 780, val loss: 1.2885253429412842
Epoch 790, training loss: 0.011342780664563179 = 0.0047072418965399265 + 0.001 * 6.6355390548706055
Epoch 790, val loss: 1.2952594757080078
Epoch 800, training loss: 0.011157600209116936 = 0.004555881954729557 + 0.001 * 6.601718425750732
Epoch 800, val loss: 1.3018474578857422
Epoch 810, training loss: 0.011027703061699867 = 0.004412514623254538 + 0.001 * 6.615187644958496
Epoch 810, val loss: 1.3083146810531616
Epoch 820, training loss: 0.010886132717132568 = 0.00427656015381217 + 0.001 * 6.609572887420654
Epoch 820, val loss: 1.314625859260559
Epoch 830, training loss: 0.010762598365545273 = 0.004147551022469997 + 0.001 * 6.615046977996826
Epoch 830, val loss: 1.320841670036316
Epoch 840, training loss: 0.010625205002725124 = 0.004025028087198734 + 0.001 * 6.600176811218262
Epoch 840, val loss: 1.3268921375274658
Epoch 850, training loss: 0.010531002655625343 = 0.003908552695065737 + 0.001 * 6.62244987487793
Epoch 850, val loss: 1.3328750133514404
Epoch 860, training loss: 0.010411548428237438 = 0.003797749988734722 + 0.001 * 6.613798141479492
Epoch 860, val loss: 1.3387103080749512
Epoch 870, training loss: 0.010298670269548893 = 0.0036922344006597996 + 0.001 * 6.606435775756836
Epoch 870, val loss: 1.3444385528564453
Epoch 880, training loss: 0.01018870621919632 = 0.0035916813649237156 + 0.001 * 6.597023963928223
Epoch 880, val loss: 1.3500499725341797
Epoch 890, training loss: 0.010084683075547218 = 0.003495778189972043 + 0.001 * 6.58890438079834
Epoch 890, val loss: 1.355568289756775
Epoch 900, training loss: 0.010011574253439903 = 0.0034042459446936846 + 0.001 * 6.607328414916992
Epoch 900, val loss: 1.3609771728515625
Epoch 910, training loss: 0.009940629824995995 = 0.003316836431622505 + 0.001 * 6.62379264831543
Epoch 910, val loss: 1.3662831783294678
Epoch 920, training loss: 0.00983529444783926 = 0.0032332853879779577 + 0.001 * 6.60200834274292
Epoch 920, val loss: 1.3714889287948608
Epoch 930, training loss: 0.009742110967636108 = 0.0031533660367131233 + 0.001 * 6.5887451171875
Epoch 930, val loss: 1.3766238689422607
Epoch 940, training loss: 0.009663899429142475 = 0.003076892113313079 + 0.001 * 6.587006568908691
Epoch 940, val loss: 1.3816252946853638
Epoch 950, training loss: 0.00960317812860012 = 0.0030036417301744223 + 0.001 * 6.599536418914795
Epoch 950, val loss: 1.3865833282470703
Epoch 960, training loss: 0.009518157690763474 = 0.0029334439896047115 + 0.001 * 6.584713459014893
Epoch 960, val loss: 1.3914090394973755
Epoch 970, training loss: 0.009459820576012135 = 0.002866065362468362 + 0.001 * 6.593754768371582
Epoch 970, val loss: 1.3961870670318604
Epoch 980, training loss: 0.009379852563142776 = 0.0028012809343636036 + 0.001 * 6.578571796417236
Epoch 980, val loss: 1.4008681774139404
Epoch 990, training loss: 0.009350297972559929 = 0.0027386860456317663 + 0.001 * 6.611611843109131
Epoch 990, val loss: 1.4055500030517578
Epoch 1000, training loss: 0.009266340173780918 = 0.0026779193431138992 + 0.001 * 6.588420391082764
Epoch 1000, val loss: 1.4101393222808838
Epoch 1010, training loss: 0.00919463112950325 = 0.0026184883899986744 + 0.001 * 6.576142311096191
Epoch 1010, val loss: 1.414732813835144
Epoch 1020, training loss: 0.009133584797382355 = 0.0025599633809179068 + 0.001 * 6.573620796203613
Epoch 1020, val loss: 1.419342279434204
Epoch 1030, training loss: 0.00908235926181078 = 0.0025020975153893232 + 0.001 * 6.580261707305908
Epoch 1030, val loss: 1.4240095615386963
Epoch 1040, training loss: 0.00902954488992691 = 0.0024449171032756567 + 0.001 * 6.584627628326416
Epoch 1040, val loss: 1.4287012815475464
Epoch 1050, training loss: 0.008986318483948708 = 0.002388610504567623 + 0.001 * 6.597708225250244
Epoch 1050, val loss: 1.43345308303833
Epoch 1060, training loss: 0.008907309733331203 = 0.002333401469513774 + 0.001 * 6.573907852172852
Epoch 1060, val loss: 1.4382466077804565
Epoch 1070, training loss: 0.008901003748178482 = 0.0022793833632022142 + 0.001 * 6.621619701385498
Epoch 1070, val loss: 1.4430503845214844
Epoch 1080, training loss: 0.008816486224532127 = 0.002226694952696562 + 0.001 * 6.589791297912598
Epoch 1080, val loss: 1.4478729963302612
Epoch 1090, training loss: 0.008744290098547935 = 0.0021754195913672447 + 0.001 * 6.5688700675964355
Epoch 1090, val loss: 1.4526735544204712
Epoch 1100, training loss: 0.008709733374416828 = 0.0021256578620523214 + 0.001 * 6.584075450897217
Epoch 1100, val loss: 1.4574822187423706
Epoch 1110, training loss: 0.00866440124809742 = 0.0020774216391146183 + 0.001 * 6.586978912353516
Epoch 1110, val loss: 1.4622247219085693
Epoch 1120, training loss: 0.008588990196585655 = 0.0020308871753513813 + 0.001 * 6.558102130889893
Epoch 1120, val loss: 1.4669922590255737
Epoch 1130, training loss: 0.008555011823773384 = 0.0019859636668115854 + 0.001 * 6.569047451019287
Epoch 1130, val loss: 1.471706509590149
Epoch 1140, training loss: 0.008495674468576908 = 0.0019426398212090135 + 0.001 * 6.55303430557251
Epoch 1140, val loss: 1.4763227701187134
Epoch 1150, training loss: 0.008484802208840847 = 0.00190089107491076 + 0.001 * 6.583910942077637
Epoch 1150, val loss: 1.4808872938156128
Epoch 1160, training loss: 0.008442232385277748 = 0.0018606921657919884 + 0.001 * 6.581540584564209
Epoch 1160, val loss: 1.4854341745376587
Epoch 1170, training loss: 0.00839642807841301 = 0.0018219241173937917 + 0.001 * 6.574503421783447
Epoch 1170, val loss: 1.4898539781570435
Epoch 1180, training loss: 0.008366773836314678 = 0.0017845876282081008 + 0.001 * 6.582185745239258
Epoch 1180, val loss: 1.4942700862884521
Epoch 1190, training loss: 0.008324172347784042 = 0.0017486062133684754 + 0.001 * 6.575565338134766
Epoch 1190, val loss: 1.4986001253128052
Epoch 1200, training loss: 0.008290555328130722 = 0.0017139483243227005 + 0.001 * 6.576606273651123
Epoch 1200, val loss: 1.502880573272705
Epoch 1210, training loss: 0.008261438459157944 = 0.0016805564519017935 + 0.001 * 6.5808820724487305
Epoch 1210, val loss: 1.507059097290039
Epoch 1220, training loss: 0.008209575898945332 = 0.0016483878716826439 + 0.001 * 6.561187744140625
Epoch 1220, val loss: 1.5112099647521973
Epoch 1230, training loss: 0.008179303258657455 = 0.001617378555238247 + 0.001 * 6.561923980712891
Epoch 1230, val loss: 1.5152815580368042
Epoch 1240, training loss: 0.008140296675264835 = 0.0015875055687502027 + 0.001 * 6.552790641784668
Epoch 1240, val loss: 1.5192899703979492
Epoch 1250, training loss: 0.008098862133920193 = 0.0015586803201586008 + 0.001 * 6.5401811599731445
Epoch 1250, val loss: 1.5232453346252441
Epoch 1260, training loss: 0.008080212399363518 = 0.0015308968722820282 + 0.001 * 6.549315452575684
Epoch 1260, val loss: 1.527112364768982
Epoch 1270, training loss: 0.008066355250775814 = 0.0015041081933304667 + 0.001 * 6.562247276306152
Epoch 1270, val loss: 1.5309090614318848
Epoch 1280, training loss: 0.00806440506130457 = 0.001478289719671011 + 0.001 * 6.586114883422852
Epoch 1280, val loss: 1.5346832275390625
Epoch 1290, training loss: 0.008014442399144173 = 0.0014534201472997665 + 0.001 * 6.561021327972412
Epoch 1290, val loss: 1.538319706916809
Epoch 1300, training loss: 0.00795991811901331 = 0.0014294108841568232 + 0.001 * 6.5305070877075195
Epoch 1300, val loss: 1.5419328212738037
Epoch 1310, training loss: 0.007946244440972805 = 0.0014062203699722886 + 0.001 * 6.5400238037109375
Epoch 1310, val loss: 1.5454797744750977
Epoch 1320, training loss: 0.00793086364865303 = 0.0013838072773069143 + 0.001 * 6.547055721282959
Epoch 1320, val loss: 1.548954963684082
Epoch 1330, training loss: 0.007901182398200035 = 0.0013621689286082983 + 0.001 * 6.539012908935547
Epoch 1330, val loss: 1.5523937940597534
Epoch 1340, training loss: 0.007867357693612576 = 0.001341280178166926 + 0.001 * 6.5260772705078125
Epoch 1340, val loss: 1.555752158164978
Epoch 1350, training loss: 0.007859316654503345 = 0.0013210795586928725 + 0.001 * 6.538236618041992
Epoch 1350, val loss: 1.559037446975708
Epoch 1360, training loss: 0.00784626230597496 = 0.0013015433214604855 + 0.001 * 6.5447187423706055
Epoch 1360, val loss: 1.5622749328613281
Epoch 1370, training loss: 0.007826627232134342 = 0.001282626879401505 + 0.001 * 6.543999671936035
Epoch 1370, val loss: 1.5654690265655518
Epoch 1380, training loss: 0.00782393291592598 = 0.001264326274394989 + 0.001 * 6.559606075286865
Epoch 1380, val loss: 1.5686005353927612
Epoch 1390, training loss: 0.007819443941116333 = 0.0012466027401387691 + 0.001 * 6.572841167449951
Epoch 1390, val loss: 1.5717175006866455
Epoch 1400, training loss: 0.007755092345178127 = 0.0012294492917135358 + 0.001 * 6.5256428718566895
Epoch 1400, val loss: 1.5747358798980713
Epoch 1410, training loss: 0.007745908107608557 = 0.0012128489324823022 + 0.001 * 6.533059120178223
Epoch 1410, val loss: 1.577713131904602
Epoch 1420, training loss: 0.007732410915195942 = 0.0011967937462031841 + 0.001 * 6.535616874694824
Epoch 1420, val loss: 1.5806384086608887
Epoch 1430, training loss: 0.007699128706008196 = 0.0011812212178483605 + 0.001 * 6.51790714263916
Epoch 1430, val loss: 1.583521842956543
Epoch 1440, training loss: 0.007699846290051937 = 0.001166120870038867 + 0.001 * 6.533725261688232
Epoch 1440, val loss: 1.586368441581726
Epoch 1450, training loss: 0.007716156542301178 = 0.0011514835059642792 + 0.001 * 6.564672946929932
Epoch 1450, val loss: 1.5891474485397339
Epoch 1460, training loss: 0.007678175810724497 = 0.0011373007437214255 + 0.001 * 6.540874481201172
Epoch 1460, val loss: 1.5918980836868286
Epoch 1470, training loss: 0.0076585616916418076 = 0.0011235498823225498 + 0.001 * 6.535011291503906
Epoch 1470, val loss: 1.5945684909820557
Epoch 1480, training loss: 0.007630896754562855 = 0.0011101864511147141 + 0.001 * 6.520709991455078
Epoch 1480, val loss: 1.597206473350525
Epoch 1490, training loss: 0.007616010028868914 = 0.0010972257005050778 + 0.001 * 6.518784046173096
Epoch 1490, val loss: 1.599830150604248
Epoch 1500, training loss: 0.007613809779286385 = 0.0010846195509657264 + 0.001 * 6.529189586639404
Epoch 1500, val loss: 1.6023808717727661
Epoch 1510, training loss: 0.007581926416605711 = 0.0010723896557465196 + 0.001 * 6.5095367431640625
Epoch 1510, val loss: 1.6048908233642578
Epoch 1520, training loss: 0.007562084589153528 = 0.0010605029528960586 + 0.001 * 6.501581192016602
Epoch 1520, val loss: 1.6073793172836304
Epoch 1530, training loss: 0.007551245391368866 = 0.0010489453561604023 + 0.001 * 6.502299785614014
Epoch 1530, val loss: 1.6098065376281738
Epoch 1540, training loss: 0.007556830532848835 = 0.0010377158178016543 + 0.001 * 6.5191144943237305
Epoch 1540, val loss: 1.6122101545333862
Epoch 1550, training loss: 0.007580083794891834 = 0.001026788610033691 + 0.001 * 6.553294658660889
Epoch 1550, val loss: 1.6145782470703125
Epoch 1560, training loss: 0.007540586870163679 = 0.0010161730460822582 + 0.001 * 6.524413585662842
Epoch 1560, val loss: 1.616909384727478
Epoch 1570, training loss: 0.007535071112215519 = 0.0010058496845886111 + 0.001 * 6.529221057891846
Epoch 1570, val loss: 1.6192142963409424
Epoch 1580, training loss: 0.00753236748278141 = 0.0009958005975931883 + 0.001 * 6.536566734313965
Epoch 1580, val loss: 1.6214762926101685
Epoch 1590, training loss: 0.00755954859778285 = 0.0009860241552814841 + 0.001 * 6.573523998260498
Epoch 1590, val loss: 1.6237049102783203
Epoch 1600, training loss: 0.00749459071084857 = 0.0009764998103491962 + 0.001 * 6.518090724945068
Epoch 1600, val loss: 1.625898838043213
Epoch 1610, training loss: 0.00750680873170495 = 0.0009672259911894798 + 0.001 * 6.539582252502441
Epoch 1610, val loss: 1.628058671951294
Epoch 1620, training loss: 0.007509371731430292 = 0.0009582102647982538 + 0.001 * 6.551161289215088
Epoch 1620, val loss: 1.630202054977417
Epoch 1630, training loss: 0.007484855130314827 = 0.0009494210826233029 + 0.001 * 6.535433769226074
Epoch 1630, val loss: 1.6322985887527466
Epoch 1640, training loss: 0.0074504162184894085 = 0.0009408709011040628 + 0.001 * 6.509544849395752
Epoch 1640, val loss: 1.6343472003936768
Epoch 1650, training loss: 0.0074594696052372456 = 0.000932542490772903 + 0.001 * 6.526926517486572
Epoch 1650, val loss: 1.636375904083252
Epoch 1660, training loss: 0.007418616209179163 = 0.0009244196699000895 + 0.001 * 6.49419641494751
Epoch 1660, val loss: 1.638368844985962
Epoch 1670, training loss: 0.007403989788144827 = 0.00091649463865906 + 0.001 * 6.487494945526123
Epoch 1670, val loss: 1.6403350830078125
Epoch 1680, training loss: 0.0074318484403193 = 0.0009087728103622794 + 0.001 * 6.523075103759766
Epoch 1680, val loss: 1.6422737836837769
Epoch 1690, training loss: 0.007405988872051239 = 0.0009012368391267955 + 0.001 * 6.504751682281494
Epoch 1690, val loss: 1.6441876888275146
Epoch 1700, training loss: 0.0073928190395236015 = 0.0008938952232711017 + 0.001 * 6.4989237785339355
Epoch 1700, val loss: 1.6460492610931396
Epoch 1710, training loss: 0.007365652360022068 = 0.0008867167052812874 + 0.001 * 6.478935241699219
Epoch 1710, val loss: 1.647913932800293
Epoch 1720, training loss: 0.007385342847555876 = 0.0008797196787782013 + 0.001 * 6.505622863769531
Epoch 1720, val loss: 1.6497479677200317
Epoch 1730, training loss: 0.00738391000777483 = 0.0008728956454433501 + 0.001 * 6.511013984680176
Epoch 1730, val loss: 1.6515494585037231
Epoch 1740, training loss: 0.007376234047114849 = 0.0008662424515932798 + 0.001 * 6.509991645812988
Epoch 1740, val loss: 1.6533558368682861
Epoch 1750, training loss: 0.007356892805546522 = 0.0008597242413088679 + 0.001 * 6.497168064117432
Epoch 1750, val loss: 1.655095100402832
Epoch 1760, training loss: 0.00735750887542963 = 0.0008533541695214808 + 0.001 * 6.504154682159424
Epoch 1760, val loss: 1.6568174362182617
Epoch 1770, training loss: 0.0073248520493507385 = 0.0008471307810395956 + 0.001 * 6.477720737457275
Epoch 1770, val loss: 1.6585304737091064
Epoch 1780, training loss: 0.007329989690333605 = 0.0008410466252826154 + 0.001 * 6.488942623138428
Epoch 1780, val loss: 1.6602288484573364
Epoch 1790, training loss: 0.007343817036598921 = 0.0008350842399522662 + 0.001 * 6.508732318878174
Epoch 1790, val loss: 1.661890983581543
Epoch 1800, training loss: 0.007299452554434538 = 0.0008292547427117825 + 0.001 * 6.470197677612305
Epoch 1800, val loss: 1.6635267734527588
Epoch 1810, training loss: 0.007285676430910826 = 0.0008235592977143824 + 0.001 * 6.462116718292236
Epoch 1810, val loss: 1.6651588678359985
Epoch 1820, training loss: 0.007284308783710003 = 0.0008179863216355443 + 0.001 * 6.466322422027588
Epoch 1820, val loss: 1.666742205619812
Epoch 1830, training loss: 0.007313995622098446 = 0.0008125497261062264 + 0.001 * 6.501445770263672
Epoch 1830, val loss: 1.6683236360549927
Epoch 1840, training loss: 0.0073292977176606655 = 0.000807229254860431 + 0.001 * 6.522068023681641
Epoch 1840, val loss: 1.6699296236038208
Epoch 1850, training loss: 0.0072952937334775925 = 0.0008020253153517842 + 0.001 * 6.493268013000488
Epoch 1850, val loss: 1.67146897315979
Epoch 1860, training loss: 0.007315115071833134 = 0.0007969409343786538 + 0.001 * 6.518173694610596
Epoch 1860, val loss: 1.672974944114685
Epoch 1870, training loss: 0.007296787574887276 = 0.0007919456693343818 + 0.001 * 6.5048418045043945
Epoch 1870, val loss: 1.6744580268859863
Epoch 1880, training loss: 0.0072542829439044 = 0.000787055934779346 + 0.001 * 6.467226505279541
Epoch 1880, val loss: 1.6759638786315918
Epoch 1890, training loss: 0.007261637598276138 = 0.0007822640473023057 + 0.001 * 6.479373455047607
Epoch 1890, val loss: 1.6774160861968994
Epoch 1900, training loss: 0.0072327484376728535 = 0.0007775655249133706 + 0.001 * 6.4551825523376465
Epoch 1900, val loss: 1.678858757019043
Epoch 1910, training loss: 0.007238125894218683 = 0.0007729617645964026 + 0.001 * 6.465163707733154
Epoch 1910, val loss: 1.680261254310608
Epoch 1920, training loss: 0.007253018673509359 = 0.0007684511365368962 + 0.001 * 6.484567165374756
Epoch 1920, val loss: 1.681686282157898
Epoch 1930, training loss: 0.007214393466711044 = 0.0007640241528861225 + 0.001 * 6.450369358062744
Epoch 1930, val loss: 1.6830775737762451
Epoch 1940, training loss: 0.00725436769425869 = 0.0007596856448799372 + 0.001 * 6.4946818351745605
Epoch 1940, val loss: 1.684434175491333
Epoch 1950, training loss: 0.007252004463225603 = 0.0007554235635325313 + 0.001 * 6.496580600738525
Epoch 1950, val loss: 1.68574857711792
Epoch 1960, training loss: 0.007259071338921785 = 0.0007512512966059148 + 0.001 * 6.507819652557373
Epoch 1960, val loss: 1.6869906187057495
Epoch 1970, training loss: 0.007210676092654467 = 0.0007471572025679052 + 0.001 * 6.4635186195373535
Epoch 1970, val loss: 1.68821382522583
Epoch 1980, training loss: 0.007207058370113373 = 0.0007431420963257551 + 0.001 * 6.463916301727295
Epoch 1980, val loss: 1.6894457340240479
Epoch 1990, training loss: 0.0071816835552453995 = 0.0007392005645669997 + 0.001 * 6.442482948303223
Epoch 1990, val loss: 1.6906981468200684
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8148
Overall ASR: 0.7823
Flip ASR: 0.7467/225 nodes
The final ASR:0.70357, 0.10615, Accuracy:0.81605, 0.00175
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11542])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10478])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9631
Flip ASR: 0.9556/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9852
Flip ASR: 0.9822/225 nodes
The final ASR:0.97417, 0.00904, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0.2 dfr1:0 dfr2:0.5
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.0, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.959202527999878 = 1.9508286714553833 + 0.001 * 8.373905181884766
Epoch 0, val loss: 1.9502904415130615
Epoch 10, training loss: 1.9483180046081543 = 1.9399441480636597 + 0.001 * 8.373858451843262
Epoch 10, val loss: 1.939884901046753
Epoch 20, training loss: 1.9348810911178589 = 1.9265073537826538 + 0.001 * 8.37369155883789
Epoch 20, val loss: 1.9264723062515259
Epoch 30, training loss: 1.9159895181655884 = 1.9076162576675415 + 0.001 * 8.37331771850586
Epoch 30, val loss: 1.907019019126892
Epoch 40, training loss: 1.888290524482727 = 1.879918098449707 + 0.001 * 8.372431755065918
Epoch 40, val loss: 1.878359317779541
Epoch 50, training loss: 1.8503655195236206 = 1.8419954776763916 + 0.001 * 8.370031356811523
Epoch 50, val loss: 1.840749979019165
Epoch 60, training loss: 1.808481216430664 = 1.8001195192337036 + 0.001 * 8.361705780029297
Epoch 60, val loss: 1.8036309480667114
Epoch 70, training loss: 1.7698675394058228 = 1.761543869972229 + 0.001 * 8.323623657226562
Epoch 70, val loss: 1.7734756469726562
Epoch 80, training loss: 1.7207015752792358 = 1.7125897407531738 + 0.001 * 8.111845970153809
Epoch 80, val loss: 1.7338229417800903
Epoch 90, training loss: 1.652484655380249 = 1.6446210145950317 + 0.001 * 7.8636956214904785
Epoch 90, val loss: 1.677493691444397
Epoch 100, training loss: 1.5630035400390625 = 1.5552308559417725 + 0.001 * 7.772730827331543
Epoch 100, val loss: 1.6035577058792114
Epoch 110, training loss: 1.4618386030197144 = 1.4542055130004883 + 0.001 * 7.6331024169921875
Epoch 110, val loss: 1.521355390548706
Epoch 120, training loss: 1.358420729637146 = 1.3509736061096191 + 0.001 * 7.447161674499512
Epoch 120, val loss: 1.4415379762649536
Epoch 130, training loss: 1.255976676940918 = 1.2485413551330566 + 0.001 * 7.435343265533447
Epoch 130, val loss: 1.366163730621338
Epoch 140, training loss: 1.1576696634292603 = 1.1502677202224731 + 0.001 * 7.401987552642822
Epoch 140, val loss: 1.2970046997070312
Epoch 150, training loss: 1.0683454275131226 = 1.0609679222106934 + 0.001 * 7.377483367919922
Epoch 150, val loss: 1.2353897094726562
Epoch 160, training loss: 0.9900357127189636 = 0.9826925992965698 + 0.001 * 7.3430986404418945
Epoch 160, val loss: 1.182000756263733
Epoch 170, training loss: 0.920059084892273 = 0.9127618074417114 + 0.001 * 7.297299385070801
Epoch 170, val loss: 1.1338821649551392
Epoch 180, training loss: 0.853524386882782 = 0.8462972640991211 + 0.001 * 7.227145195007324
Epoch 180, val loss: 1.087121605873108
Epoch 190, training loss: 0.7862942218780518 = 0.7791587710380554 + 0.001 * 7.135443687438965
Epoch 190, val loss: 1.0390187501907349
Epoch 200, training loss: 0.7167129516601562 = 0.7096373438835144 + 0.001 * 7.075636386871338
Epoch 200, val loss: 0.9885528683662415
Epoch 210, training loss: 0.6461958289146423 = 0.6391456127166748 + 0.001 * 7.050197124481201
Epoch 210, val loss: 0.9376236796379089
Epoch 220, training loss: 0.5780240893363953 = 0.5709933042526245 + 0.001 * 7.030794620513916
Epoch 220, val loss: 0.8893496990203857
Epoch 230, training loss: 0.5150208473205566 = 0.5080034136772156 + 0.001 * 7.017443656921387
Epoch 230, val loss: 0.8468053340911865
Epoch 240, training loss: 0.4582858383655548 = 0.4512757360935211 + 0.001 * 7.010112285614014
Epoch 240, val loss: 0.8118677139282227
Epoch 250, training loss: 0.4072478115558624 = 0.400241494178772 + 0.001 * 7.0063276290893555
Epoch 250, val loss: 0.7845688462257385
Epoch 260, training loss: 0.36067715287208557 = 0.3536733388900757 + 0.001 * 7.0037994384765625
Epoch 260, val loss: 0.7635871171951294
Epoch 270, training loss: 0.31759586930274963 = 0.3105942904949188 + 0.001 * 7.00156831741333
Epoch 270, val loss: 0.7474856376647949
Epoch 280, training loss: 0.2778513729572296 = 0.27085110545158386 + 0.001 * 7.000272750854492
Epoch 280, val loss: 0.7358057498931885
Epoch 290, training loss: 0.24175380170345306 = 0.23475414514541626 + 0.001 * 6.99966287612915
Epoch 290, val loss: 0.7285240292549133
Epoch 300, training loss: 0.20969995856285095 = 0.20270009338855743 + 0.001 * 6.999869346618652
Epoch 300, val loss: 0.7259526252746582
Epoch 310, training loss: 0.18188731372356415 = 0.17488665878772736 + 0.001 * 7.00065279006958
Epoch 310, val loss: 0.7279242873191833
Epoch 320, training loss: 0.1581539511680603 = 0.1511521339416504 + 0.001 * 7.001819133758545
Epoch 320, val loss: 0.7339472770690918
Epoch 330, training loss: 0.13808542490005493 = 0.13108223676681519 + 0.001 * 7.0031843185424805
Epoch 330, val loss: 0.7432783842086792
Epoch 340, training loss: 0.12114755809307098 = 0.11414294689893723 + 0.001 * 7.004611968994141
Epoch 340, val loss: 0.7550987601280212
Epoch 350, training loss: 0.1068275049328804 = 0.09982149302959442 + 0.001 * 7.0060133934021
Epoch 350, val loss: 0.768650233745575
Epoch 360, training loss: 0.0946759507060051 = 0.08766864240169525 + 0.001 * 7.007306098937988
Epoch 360, val loss: 0.7834363579750061
Epoch 370, training loss: 0.08432464301586151 = 0.07731620967388153 + 0.001 * 7.008433818817139
Epoch 370, val loss: 0.7989344000816345
Epoch 380, training loss: 0.07547438144683838 = 0.06846500933170319 + 0.001 * 7.009371757507324
Epoch 380, val loss: 0.8148337006568909
Epoch 390, training loss: 0.06788207590579987 = 0.06087197735905647 + 0.001 * 7.01009464263916
Epoch 390, val loss: 0.8308481574058533
Epoch 400, training loss: 0.0613500140607357 = 0.05433943122625351 + 0.001 * 7.010583400726318
Epoch 400, val loss: 0.8467356562614441
Epoch 410, training loss: 0.055714499205350876 = 0.04870368167757988 + 0.001 * 7.01081657409668
Epoch 410, val loss: 0.8624415993690491
Epoch 420, training loss: 0.05084091052412987 = 0.043827757239341736 + 0.001 * 7.0131516456604
Epoch 420, val loss: 0.8778523802757263
Epoch 430, training loss: 0.046603936702013016 = 0.03959125652909279 + 0.001 * 7.012681007385254
Epoch 430, val loss: 0.8929358124732971
Epoch 440, training loss: 0.04290202260017395 = 0.035890717059373856 + 0.001 * 7.011303424835205
Epoch 440, val loss: 0.9077622890472412
Epoch 450, training loss: 0.03965727612376213 = 0.03264622762799263 + 0.001 * 7.011047840118408
Epoch 450, val loss: 0.9221286773681641
Epoch 460, training loss: 0.03680182993412018 = 0.02979147993028164 + 0.001 * 7.010348796844482
Epoch 460, val loss: 0.9362421035766602
Epoch 470, training loss: 0.03428192064166069 = 0.027272334322333336 + 0.001 * 7.009584426879883
Epoch 470, val loss: 0.9499569535255432
Epoch 480, training loss: 0.032054852694272995 = 0.025042550638318062 + 0.001 * 7.012303352355957
Epoch 480, val loss: 0.9633042812347412
Epoch 490, training loss: 0.03007226064801216 = 0.023063156753778458 + 0.001 * 7.009103775024414
Epoch 490, val loss: 0.9763109087944031
Epoch 500, training loss: 0.028308508917689323 = 0.021300772204995155 + 0.001 * 7.007735729217529
Epoch 500, val loss: 0.9889335632324219
Epoch 510, training loss: 0.02673422545194626 = 0.019726775586605072 + 0.001 * 7.007449150085449
Epoch 510, val loss: 1.0011807680130005
Epoch 520, training loss: 0.02532203495502472 = 0.01831670291721821 + 0.001 * 7.005331993103027
Epoch 520, val loss: 1.01308274269104
Epoch 530, training loss: 0.0240529365837574 = 0.017049549147486687 + 0.001 * 7.0033860206604
Epoch 530, val loss: 1.0246191024780273
Epoch 540, training loss: 0.022925570607185364 = 0.015903638675808907 + 0.001 * 7.021931171417236
Epoch 540, val loss: 1.0357850790023804
Epoch 550, training loss: 0.021868325769901276 = 0.014863014221191406 + 0.001 * 7.005311965942383
Epoch 550, val loss: 1.0467466115951538
Epoch 560, training loss: 0.020915471017360687 = 0.013914035633206367 + 0.001 * 7.001434803009033
Epoch 560, val loss: 1.0573103427886963
Epoch 570, training loss: 0.020049620419740677 = 0.0130469910800457 + 0.001 * 7.002628803253174
Epoch 570, val loss: 1.0676209926605225
Epoch 580, training loss: 0.01924835331737995 = 0.012253682129085064 + 0.001 * 6.994670391082764
Epoch 580, val loss: 1.0776818990707397
Epoch 590, training loss: 0.01851879060268402 = 0.011527035385370255 + 0.001 * 6.991755485534668
Epoch 590, val loss: 1.0874254703521729
Epoch 600, training loss: 0.01785099133849144 = 0.01086076907813549 + 0.001 * 6.9902215003967285
Epoch 600, val loss: 1.0969412326812744
Epoch 610, training loss: 0.01723850890994072 = 0.010249121114611626 + 0.001 * 6.9893879890441895
Epoch 610, val loss: 1.1061923503875732
Epoch 620, training loss: 0.016672829166054726 = 0.009686794131994247 + 0.001 * 6.986034870147705
Epoch 620, val loss: 1.1152212619781494
Epoch 630, training loss: 0.016170967370271683 = 0.009169122204184532 + 0.001 * 7.001845836639404
Epoch 630, val loss: 1.1240317821502686
Epoch 640, training loss: 0.015665780752897263 = 0.008691851049661636 + 0.001 * 6.973930358886719
Epoch 640, val loss: 1.1326062679290771
Epoch 650, training loss: 0.015217993408441544 = 0.008251173421740532 + 0.001 * 6.966820240020752
Epoch 650, val loss: 1.1409716606140137
Epoch 660, training loss: 0.014842873439192772 = 0.00784361269325018 + 0.001 * 6.999259948730469
Epoch 660, val loss: 1.1491427421569824
Epoch 670, training loss: 0.01443798653781414 = 0.0074662016704678535 + 0.001 * 6.971785068511963
Epoch 670, val loss: 1.1570780277252197
Epoch 680, training loss: 0.014080672524869442 = 0.00711604580283165 + 0.001 * 6.964626312255859
Epoch 680, val loss: 1.1648223400115967
Epoch 690, training loss: 0.013819795101881027 = 0.0067906733602285385 + 0.001 * 7.029121398925781
Epoch 690, val loss: 1.1723753213882446
Epoch 700, training loss: 0.013426992110908031 = 0.0064879911951720715 + 0.001 * 6.939000606536865
Epoch 700, val loss: 1.1797065734863281
Epoch 710, training loss: 0.013176366686820984 = 0.006205950863659382 + 0.001 * 6.9704155921936035
Epoch 710, val loss: 1.1868245601654053
Epoch 720, training loss: 0.012912122532725334 = 0.0059427437372505665 + 0.001 * 6.969378471374512
Epoch 720, val loss: 1.1938071250915527
Epoch 730, training loss: 0.01264113001525402 = 0.005696791224181652 + 0.001 * 6.944338321685791
Epoch 730, val loss: 1.2007193565368652
Epoch 740, training loss: 0.012439406476914883 = 0.005466624163091183 + 0.001 * 6.972782135009766
Epoch 740, val loss: 1.2073386907577515
Epoch 750, training loss: 0.012166514061391354 = 0.005250945221632719 + 0.001 * 6.9155683517456055
Epoch 750, val loss: 1.2138464450836182
Epoch 760, training loss: 0.011966640129685402 = 0.005048506893217564 + 0.001 * 6.918132781982422
Epoch 760, val loss: 1.2202074527740479
Epoch 770, training loss: 0.011798003688454628 = 0.004858189262449741 + 0.001 * 6.939814567565918
Epoch 770, val loss: 1.2264891862869263
Epoch 780, training loss: 0.0115998275578022 = 0.0046790665946900845 + 0.001 * 6.9207611083984375
Epoch 780, val loss: 1.232529878616333
Epoch 790, training loss: 0.011423059739172459 = 0.004510163329541683 + 0.001 * 6.912896156311035
Epoch 790, val loss: 1.2385777235031128
Epoch 800, training loss: 0.011265084147453308 = 0.004350692965090275 + 0.001 * 6.914391040802002
Epoch 800, val loss: 1.2444342374801636
Epoch 810, training loss: 0.011113917455077171 = 0.004199900198727846 + 0.001 * 6.9140167236328125
Epoch 810, val loss: 1.2501466274261475
Epoch 820, training loss: 0.010985268279910088 = 0.004057144280523062 + 0.001 * 6.928123950958252
Epoch 820, val loss: 1.2557921409606934
Epoch 830, training loss: 0.010821744799613953 = 0.003921959083527327 + 0.001 * 6.89978551864624
Epoch 830, val loss: 1.2613047361373901
Epoch 840, training loss: 0.010707737877964973 = 0.0037937003653496504 + 0.001 * 6.914037227630615
Epoch 840, val loss: 1.266737699508667
Epoch 850, training loss: 0.010560567490756512 = 0.00367191550321877 + 0.001 * 6.8886518478393555
Epoch 850, val loss: 1.2721022367477417
Epoch 860, training loss: 0.010448168963193893 = 0.003556101815775037 + 0.001 * 6.892066955566406
Epoch 860, val loss: 1.2773733139038086
Epoch 870, training loss: 0.010330723598599434 = 0.00344586162827909 + 0.001 * 6.884861946105957
Epoch 870, val loss: 1.2826025485992432
Epoch 880, training loss: 0.010219482704997063 = 0.0033408633898943663 + 0.001 * 6.8786187171936035
Epoch 880, val loss: 1.2876490354537964
Epoch 890, training loss: 0.010147176682949066 = 0.0032408672850579023 + 0.001 * 6.906309127807617
Epoch 890, val loss: 1.292637825012207
Epoch 900, training loss: 0.01002876553684473 = 0.003145586932078004 + 0.001 * 6.883178234100342
Epoch 900, val loss: 1.2975430488586426
Epoch 910, training loss: 0.009964018128812313 = 0.0030546782072633505 + 0.001 * 6.909339427947998
Epoch 910, val loss: 1.3023344278335571
Epoch 920, training loss: 0.009861143305897713 = 0.0029679876752197742 + 0.001 * 6.893155097961426
Epoch 920, val loss: 1.3071022033691406
Epoch 930, training loss: 0.009783134795725346 = 0.0028852038085460663 + 0.001 * 6.89793062210083
Epoch 930, val loss: 1.3117516040802002
Epoch 940, training loss: 0.009687317535281181 = 0.002806109609082341 + 0.001 * 6.881207466125488
Epoch 940, val loss: 1.3163683414459229
Epoch 950, training loss: 0.009600638411939144 = 0.0027305122930556536 + 0.001 * 6.870126247406006
Epoch 950, val loss: 1.3209162950515747
Epoch 960, training loss: 0.009531112387776375 = 0.0026583103463053703 + 0.001 * 6.872801303863525
Epoch 960, val loss: 1.3253172636032104
Epoch 970, training loss: 0.009458079934120178 = 0.0025893165729939938 + 0.001 * 6.868762969970703
Epoch 970, val loss: 1.3296993970870972
Epoch 980, training loss: 0.009396341629326344 = 0.0025233288761228323 + 0.001 * 6.873012065887451
Epoch 980, val loss: 1.3339370489120483
Epoch 990, training loss: 0.009334716014564037 = 0.0024601349141448736 + 0.001 * 6.8745808601379395
Epoch 990, val loss: 1.3381561040878296
Epoch 1000, training loss: 0.009257272817194462 = 0.0023995127994567156 + 0.001 * 6.857759952545166
Epoch 1000, val loss: 1.3423283100128174
Epoch 1010, training loss: 0.009213543497025967 = 0.0023414944298565388 + 0.001 * 6.872048854827881
Epoch 1010, val loss: 1.3463772535324097
Epoch 1020, training loss: 0.009159332141280174 = 0.0022859235759824514 + 0.001 * 6.87340784072876
Epoch 1020, val loss: 1.3503371477127075
Epoch 1030, training loss: 0.009113896638154984 = 0.0022325743921101093 + 0.001 * 6.881321907043457
Epoch 1030, val loss: 1.354301929473877
Epoch 1040, training loss: 0.009051790460944176 = 0.002181378426030278 + 0.001 * 6.870411396026611
Epoch 1040, val loss: 1.3581466674804688
Epoch 1050, training loss: 0.009009160101413727 = 0.0021321820095181465 + 0.001 * 6.876977920532227
Epoch 1050, val loss: 1.3619273900985718
Epoch 1060, training loss: 0.008938603103160858 = 0.002084917388856411 + 0.001 * 6.8536858558654785
Epoch 1060, val loss: 1.3656073808670044
Epoch 1070, training loss: 0.008894329890608788 = 0.0020395515020936728 + 0.001 * 6.854777812957764
Epoch 1070, val loss: 1.369266152381897
Epoch 1080, training loss: 0.00887436605989933 = 0.0019960119388997555 + 0.001 * 6.878353118896484
Epoch 1080, val loss: 1.3728660345077515
Epoch 1090, training loss: 0.008808095939457417 = 0.0019541147630661726 + 0.001 * 6.853980541229248
Epoch 1090, val loss: 1.3763433694839478
Epoch 1100, training loss: 0.008764057420194149 = 0.001913815620355308 + 0.001 * 6.850241661071777
Epoch 1100, val loss: 1.3798372745513916
Epoch 1110, training loss: 0.008723107166588306 = 0.0018750197486951947 + 0.001 * 6.848087310791016
Epoch 1110, val loss: 1.3832228183746338
Epoch 1120, training loss: 0.008671890944242477 = 0.0018376404186710715 + 0.001 * 6.834249973297119
Epoch 1120, val loss: 1.3865638971328735
Epoch 1130, training loss: 0.008642149157822132 = 0.0018016316462308168 + 0.001 * 6.840517044067383
Epoch 1130, val loss: 1.3898757696151733
Epoch 1140, training loss: 0.008613664656877518 = 0.0017668982036411762 + 0.001 * 6.846765518188477
Epoch 1140, val loss: 1.3930401802062988
Epoch 1150, training loss: 0.008592208847403526 = 0.001733438577502966 + 0.001 * 6.858770370483398
Epoch 1150, val loss: 1.396203875541687
Epoch 1160, training loss: 0.008540909737348557 = 0.0017012848984450102 + 0.001 * 6.839624404907227
Epoch 1160, val loss: 1.3993408679962158
Epoch 1170, training loss: 0.008518879301846027 = 0.0016702166758477688 + 0.001 * 6.848662376403809
Epoch 1170, val loss: 1.4023500680923462
Epoch 1180, training loss: 0.008503922261297703 = 0.0016402782639488578 + 0.001 * 6.863644123077393
Epoch 1180, val loss: 1.405282974243164
Epoch 1190, training loss: 0.008443777449429035 = 0.001611421350389719 + 0.001 * 6.832355976104736
Epoch 1190, val loss: 1.4082210063934326
Epoch 1200, training loss: 0.008407052606344223 = 0.0015835544327273965 + 0.001 * 6.823497295379639
Epoch 1200, val loss: 1.4111202955245972
Epoch 1210, training loss: 0.008397167548537254 = 0.001556636649183929 + 0.001 * 6.8405303955078125
Epoch 1210, val loss: 1.41390061378479
Epoch 1220, training loss: 0.008357499726116657 = 0.001530671608634293 + 0.001 * 6.826827526092529
Epoch 1220, val loss: 1.4166759252548218
Epoch 1230, training loss: 0.008360104635357857 = 0.0015055398689582944 + 0.001 * 6.854564666748047
Epoch 1230, val loss: 1.4193750619888306
Epoch 1240, training loss: 0.008312963880598545 = 0.001481300569139421 + 0.001 * 6.831663131713867
Epoch 1240, val loss: 1.4220556020736694
Epoch 1250, training loss: 0.008274049498140812 = 0.0014578254194930196 + 0.001 * 6.816224098205566
Epoch 1250, val loss: 1.4246517419815063
Epoch 1260, training loss: 0.008259412832558155 = 0.0014351007994264364 + 0.001 * 6.82431173324585
Epoch 1260, val loss: 1.427212119102478
Epoch 1270, training loss: 0.00822913832962513 = 0.0014131143689155579 + 0.001 * 6.816023826599121
Epoch 1270, val loss: 1.4297409057617188
Epoch 1280, training loss: 0.008215198293328285 = 0.0013918852200731635 + 0.001 * 6.823313236236572
Epoch 1280, val loss: 1.4322298765182495
Epoch 1290, training loss: 0.008185198530554771 = 0.00137132010422647 + 0.001 * 6.813878059387207
Epoch 1290, val loss: 1.4346407651901245
Epoch 1300, training loss: 0.008167903870344162 = 0.0013513844460248947 + 0.001 * 6.816518783569336
Epoch 1300, val loss: 1.4369922876358032
Epoch 1310, training loss: 0.008136809803545475 = 0.0013321158476173878 + 0.001 * 6.804693698883057
Epoch 1310, val loss: 1.43928062915802
Epoch 1320, training loss: 0.008132937364280224 = 0.0013134211767464876 + 0.001 * 6.819516181945801
Epoch 1320, val loss: 1.44161856174469
Epoch 1330, training loss: 0.008093340322375298 = 0.0012953263940289617 + 0.001 * 6.798013210296631
Epoch 1330, val loss: 1.4438238143920898
Epoch 1340, training loss: 0.00811284501105547 = 0.0012777546653524041 + 0.001 * 6.835089683532715
Epoch 1340, val loss: 1.4460262060165405
Epoch 1350, training loss: 0.008079749532043934 = 0.0012607160024344921 + 0.001 * 6.819033145904541
Epoch 1350, val loss: 1.4481984376907349
Epoch 1360, training loss: 0.008042525500059128 = 0.0012441665166988969 + 0.001 * 6.79835844039917
Epoch 1360, val loss: 1.4502813816070557
Epoch 1370, training loss: 0.008049219846725464 = 0.0012281136587262154 + 0.001 * 6.821105480194092
Epoch 1370, val loss: 1.4523884057998657
Epoch 1380, training loss: 0.00802610069513321 = 0.0012125697685405612 + 0.001 * 6.813530921936035
Epoch 1380, val loss: 1.4544165134429932
Epoch 1390, training loss: 0.008042151108384132 = 0.0011974971275776625 + 0.001 * 6.844653606414795
Epoch 1390, val loss: 1.456369400024414
Epoch 1400, training loss: 0.007986740209162235 = 0.0011828587157651782 + 0.001 * 6.803881645202637
Epoch 1400, val loss: 1.4583311080932617
Epoch 1410, training loss: 0.008007384836673737 = 0.0011686633806675673 + 0.001 * 6.83872127532959
Epoch 1410, val loss: 1.4602203369140625
Epoch 1420, training loss: 0.007954069413244724 = 0.00115485570859164 + 0.001 * 6.799213886260986
Epoch 1420, val loss: 1.4620497226715088
Epoch 1430, training loss: 0.007924394682049751 = 0.0011414500186219811 + 0.001 * 6.782944202423096
Epoch 1430, val loss: 1.4639010429382324
Epoch 1440, training loss: 0.007937238551676273 = 0.0011284451466053724 + 0.001 * 6.808793544769287
Epoch 1440, val loss: 1.4656972885131836
Epoch 1450, training loss: 0.00793425366282463 = 0.001115786493755877 + 0.001 * 6.818466663360596
Epoch 1450, val loss: 1.4674867391586304
Epoch 1460, training loss: 0.00792749784886837 = 0.0011034790659323335 + 0.001 * 6.8240180015563965
Epoch 1460, val loss: 1.4692022800445557
Epoch 1470, training loss: 0.00787016935646534 = 0.0010915324091911316 + 0.001 * 6.7786359786987305
Epoch 1470, val loss: 1.4708901643753052
Epoch 1480, training loss: 0.007882639765739441 = 0.0010799061274155974 + 0.001 * 6.802733421325684
Epoch 1480, val loss: 1.4724689722061157
Epoch 1490, training loss: 0.007874155417084694 = 0.001068597543053329 + 0.001 * 6.8055572509765625
Epoch 1490, val loss: 1.4741108417510986
Epoch 1500, training loss: 0.007880501449108124 = 0.001057610847055912 + 0.001 * 6.822890758514404
Epoch 1500, val loss: 1.4756677150726318
Epoch 1510, training loss: 0.0078559136018157 = 0.0010468974942341447 + 0.001 * 6.809015274047852
Epoch 1510, val loss: 1.4772635698318481
Epoch 1520, training loss: 0.007826489396393299 = 0.0010364768095314503 + 0.001 * 6.790012359619141
Epoch 1520, val loss: 1.4787423610687256
Epoch 1530, training loss: 0.0078000533394515514 = 0.0010263276053592563 + 0.001 * 6.773725509643555
Epoch 1530, val loss: 1.4801920652389526
Epoch 1540, training loss: 0.007799885235726833 = 0.0010164089035242796 + 0.001 * 6.783475875854492
Epoch 1540, val loss: 1.4816217422485352
Epoch 1550, training loss: 0.007772840093821287 = 0.0010067528346553445 + 0.001 * 6.766087055206299
Epoch 1550, val loss: 1.4830551147460938
Epoch 1560, training loss: 0.007780326064676046 = 0.0009973184205591679 + 0.001 * 6.7830071449279785
Epoch 1560, val loss: 1.48447585105896
Epoch 1570, training loss: 0.0077499812468886375 = 0.000988135114312172 + 0.001 * 6.761845588684082
Epoch 1570, val loss: 1.485817313194275
Epoch 1580, training loss: 0.007750648073852062 = 0.0009791918564587831 + 0.001 * 6.771455764770508
Epoch 1580, val loss: 1.4871217012405396
Epoch 1590, training loss: 0.007761679589748383 = 0.0009704891708679497 + 0.001 * 6.791190147399902
Epoch 1590, val loss: 1.4884668588638306
Epoch 1600, training loss: 0.007732758764177561 = 0.0009620247292332351 + 0.001 * 6.770733833312988
Epoch 1600, val loss: 1.4897164106369019
Epoch 1610, training loss: 0.007764597423374653 = 0.0009537701844237745 + 0.001 * 6.810826778411865
Epoch 1610, val loss: 1.4909406900405884
Epoch 1620, training loss: 0.007706528529524803 = 0.0009457198320887983 + 0.001 * 6.76080846786499
Epoch 1620, val loss: 1.4921435117721558
Epoch 1630, training loss: 0.007733440026640892 = 0.0009378689574077725 + 0.001 * 6.7955708503723145
Epoch 1630, val loss: 1.4933152198791504
Epoch 1640, training loss: 0.007730482611805201 = 0.000930190843064338 + 0.001 * 6.800291061401367
Epoch 1640, val loss: 1.4944812059402466
Epoch 1650, training loss: 0.007682422641664743 = 0.000922713428735733 + 0.001 * 6.759708881378174
Epoch 1650, val loss: 1.4956351518630981
Epoch 1660, training loss: 0.007673863787204027 = 0.0009154294966720045 + 0.001 * 6.758433818817139
Epoch 1660, val loss: 1.4967683553695679
Epoch 1670, training loss: 0.007673851680010557 = 0.0009082927135750651 + 0.001 * 6.76555871963501
Epoch 1670, val loss: 1.4978423118591309
Epoch 1680, training loss: 0.007671583443880081 = 0.0009013235685415566 + 0.001 * 6.770259380340576
Epoch 1680, val loss: 1.4988025426864624
Epoch 1690, training loss: 0.007668576668947935 = 0.0008944891160354018 + 0.001 * 6.774086952209473
Epoch 1690, val loss: 1.4998928308486938
Epoch 1700, training loss: 0.007654178887605667 = 0.0008878384251147509 + 0.001 * 6.766340255737305
Epoch 1700, val loss: 1.5008589029312134
Epoch 1710, training loss: 0.007638165727257729 = 0.0008813362219370902 + 0.001 * 6.756829261779785
Epoch 1710, val loss: 1.5018459558486938
Epoch 1720, training loss: 0.007635758724063635 = 0.0008749840781092644 + 0.001 * 6.7607741355896
Epoch 1720, val loss: 1.5027954578399658
Epoch 1730, training loss: 0.00764148123562336 = 0.0008687924710102379 + 0.001 * 6.772688388824463
Epoch 1730, val loss: 1.5037577152252197
Epoch 1740, training loss: 0.007687331177294254 = 0.0008627102361060679 + 0.001 * 6.824620723724365
Epoch 1740, val loss: 1.5046594142913818
Epoch 1750, training loss: 0.00762478681281209 = 0.0008568064076825976 + 0.001 * 6.767980098724365
Epoch 1750, val loss: 1.5055042505264282
Epoch 1760, training loss: 0.00760613102465868 = 0.0008510283660143614 + 0.001 * 6.755102157592773
Epoch 1760, val loss: 1.5063754320144653
Epoch 1770, training loss: 0.007591635920107365 = 0.0008453630143776536 + 0.001 * 6.746272563934326
Epoch 1770, val loss: 1.5071744918823242
Epoch 1780, training loss: 0.007583922240883112 = 0.0008398131467401981 + 0.001 * 6.7441086769104
Epoch 1780, val loss: 1.508013129234314
Epoch 1790, training loss: 0.007587855216115713 = 0.0008343825465999544 + 0.001 * 6.753472328186035
Epoch 1790, val loss: 1.508762001991272
Epoch 1800, training loss: 0.007576333358883858 = 0.0008290531695820391 + 0.001 * 6.747280120849609
Epoch 1800, val loss: 1.5095175504684448
Epoch 1810, training loss: 0.007565401494503021 = 0.0008238484733738005 + 0.001 * 6.741552829742432
Epoch 1810, val loss: 1.5102710723876953
Epoch 1820, training loss: 0.007539579179137945 = 0.0008187569910660386 + 0.001 * 6.720821857452393
Epoch 1820, val loss: 1.511021614074707
Epoch 1830, training loss: 0.007577030919492245 = 0.0008137635886669159 + 0.001 * 6.7632670402526855
Epoch 1830, val loss: 1.5116957426071167
Epoch 1840, training loss: 0.007561481092125177 = 0.0008088635513558984 + 0.001 * 6.752617359161377
Epoch 1840, val loss: 1.5124473571777344
Epoch 1850, training loss: 0.007598333992063999 = 0.0008041101391427219 + 0.001 * 6.794223785400391
Epoch 1850, val loss: 1.513155460357666
Epoch 1860, training loss: 0.007518989033997059 = 0.0007994261104613543 + 0.001 * 6.719563007354736
Epoch 1860, val loss: 1.5137383937835693
Epoch 1870, training loss: 0.007506800815463066 = 0.0007948667625896633 + 0.001 * 6.7119340896606445
Epoch 1870, val loss: 1.5143996477127075
Epoch 1880, training loss: 0.007530007045716047 = 0.0007904035737738013 + 0.001 * 6.739603042602539
Epoch 1880, val loss: 1.5149497985839844
Epoch 1890, training loss: 0.0075401421636343 = 0.0007860275218263268 + 0.001 * 6.754114627838135
Epoch 1890, val loss: 1.5155235528945923
Epoch 1900, training loss: 0.00750395841896534 = 0.00078172842040658 + 0.001 * 6.722229480743408
Epoch 1900, val loss: 1.5161694288253784
Epoch 1910, training loss: 0.007507480215281248 = 0.0007775270496495068 + 0.001 * 6.729952812194824
Epoch 1910, val loss: 1.5166507959365845
Epoch 1920, training loss: 0.007514434866607189 = 0.0007733827224001288 + 0.001 * 6.74105167388916
Epoch 1920, val loss: 1.5172039270401
Epoch 1930, training loss: 0.007495041936635971 = 0.0007693280931562185 + 0.001 * 6.72571325302124
Epoch 1930, val loss: 1.517730712890625
Epoch 1940, training loss: 0.007482694927603006 = 0.0007653515785932541 + 0.001 * 6.717342853546143
Epoch 1940, val loss: 1.5182522535324097
Epoch 1950, training loss: 0.007531723007559776 = 0.0007614556816406548 + 0.001 * 6.770267009735107
Epoch 1950, val loss: 1.518707275390625
Epoch 1960, training loss: 0.00746770529076457 = 0.0007576210773549974 + 0.001 * 6.710083961486816
Epoch 1960, val loss: 1.5191829204559326
Epoch 1970, training loss: 0.00753660686314106 = 0.0007538524805568159 + 0.001 * 6.782753944396973
Epoch 1970, val loss: 1.5196945667266846
Epoch 1980, training loss: 0.007442485075443983 = 0.0007501528598368168 + 0.001 * 6.692331790924072
Epoch 1980, val loss: 1.5201443433761597
Epoch 1990, training loss: 0.007554704789072275 = 0.0007465284434147179 + 0.001 * 6.808176040649414
Epoch 1990, val loss: 1.5205429792404175
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8259
Overall ASR: 0.6236
Flip ASR: 0.5511/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9624416828155518 = 1.9540678262710571 + 0.001 * 8.373818397521973
Epoch 0, val loss: 1.9550281763076782
Epoch 10, training loss: 1.9518094062805176 = 1.9434356689453125 + 0.001 * 8.37370777130127
Epoch 10, val loss: 1.9438564777374268
Epoch 20, training loss: 1.9385777711868286 = 1.9302043914794922 + 0.001 * 8.373384475708008
Epoch 20, val loss: 1.92972731590271
Epoch 30, training loss: 1.9196926355361938 = 1.9113199710845947 + 0.001 * 8.372661590576172
Epoch 30, val loss: 1.9095953702926636
Epoch 40, training loss: 1.8918339014053345 = 1.883462905883789 + 0.001 * 8.37097454071045
Epoch 40, val loss: 1.880529761314392
Epoch 50, training loss: 1.8526520729064941 = 1.8442860841751099 + 0.001 * 8.366045951843262
Epoch 50, val loss: 1.8414318561553955
Epoch 60, training loss: 1.8049317598342896 = 1.7965868711471558 + 0.001 * 8.344930648803711
Epoch 60, val loss: 1.7968322038650513
Epoch 70, training loss: 1.7567583322525024 = 1.7485491037368774 + 0.001 * 8.209169387817383
Epoch 70, val loss: 1.7527087926864624
Epoch 80, training loss: 1.7017269134521484 = 1.6939194202423096 + 0.001 * 7.8075480461120605
Epoch 80, val loss: 1.6993916034698486
Epoch 90, training loss: 1.6288833618164062 = 1.6212663650512695 + 0.001 * 7.616992473602295
Epoch 90, val loss: 1.6328364610671997
Epoch 100, training loss: 1.5356919765472412 = 1.5282238721847534 + 0.001 * 7.468147277832031
Epoch 100, val loss: 1.555335283279419
Epoch 110, training loss: 1.430564522743225 = 1.4233431816101074 + 0.001 * 7.22128438949585
Epoch 110, val loss: 1.4698153734207153
Epoch 120, training loss: 1.3282077312469482 = 1.3210091590881348 + 0.001 * 7.198577880859375
Epoch 120, val loss: 1.3894343376159668
Epoch 130, training loss: 1.2352744340896606 = 1.2281081676483154 + 0.001 * 7.166240692138672
Epoch 130, val loss: 1.3181227445602417
Epoch 140, training loss: 1.150207281112671 = 1.143053412437439 + 0.001 * 7.153846740722656
Epoch 140, val loss: 1.2553458213806152
Epoch 150, training loss: 1.0678706169128418 = 1.0607413053512573 + 0.001 * 7.129348278045654
Epoch 150, val loss: 1.1960047483444214
Epoch 160, training loss: 0.9854418635368347 = 0.9783464670181274 + 0.001 * 7.095412254333496
Epoch 160, val loss: 1.1373735666275024
Epoch 170, training loss: 0.904050350189209 = 0.8969923257827759 + 0.001 * 7.058047294616699
Epoch 170, val loss: 1.0806721448898315
Epoch 180, training loss: 0.8269505500793457 = 0.8199270963668823 + 0.001 * 7.023476600646973
Epoch 180, val loss: 1.028489112854004
Epoch 190, training loss: 0.7568455934524536 = 0.749850869178772 + 0.001 * 6.994731426239014
Epoch 190, val loss: 0.98293536901474
Epoch 200, training loss: 0.694621741771698 = 0.6876482367515564 + 0.001 * 6.973522186279297
Epoch 200, val loss: 0.9448598027229309
Epoch 210, training loss: 0.6395715475082397 = 0.6326207518577576 + 0.001 * 6.9507856369018555
Epoch 210, val loss: 0.9140990376472473
Epoch 220, training loss: 0.5904225707054138 = 0.5834923386573792 + 0.001 * 6.930219650268555
Epoch 220, val loss: 0.8892840147018433
Epoch 230, training loss: 0.5459000468254089 = 0.5389919281005859 + 0.001 * 6.908097267150879
Epoch 230, val loss: 0.8691784739494324
Epoch 240, training loss: 0.5047680139541626 = 0.4978785812854767 + 0.001 * 6.889432430267334
Epoch 240, val loss: 0.8533675670623779
Epoch 250, training loss: 0.46583291888237 = 0.45895496010780334 + 0.001 * 6.877946376800537
Epoch 250, val loss: 0.8413057327270508
Epoch 260, training loss: 0.42802193760871887 = 0.4211556613445282 + 0.001 * 6.866287708282471
Epoch 260, val loss: 0.8328003883361816
Epoch 270, training loss: 0.39072030782699585 = 0.3838557004928589 + 0.001 * 6.864611625671387
Epoch 270, val loss: 0.8277689218521118
Epoch 280, training loss: 0.35395050048828125 = 0.34709280729293823 + 0.001 * 6.857698440551758
Epoch 280, val loss: 0.8263819813728333
Epoch 290, training loss: 0.3181286156177521 = 0.311277836561203 + 0.001 * 6.850782871246338
Epoch 290, val loss: 0.8286770582199097
Epoch 300, training loss: 0.2840057611465454 = 0.2771565616130829 + 0.001 * 6.849212646484375
Epoch 300, val loss: 0.8344533443450928
Epoch 310, training loss: 0.2522774636745453 = 0.24543552100658417 + 0.001 * 6.841937065124512
Epoch 310, val loss: 0.8425137996673584
Epoch 320, training loss: 0.22356578707695007 = 0.2167271226644516 + 0.001 * 6.838656425476074
Epoch 320, val loss: 0.852060854434967
Epoch 330, training loss: 0.19773054122924805 = 0.1908961832523346 + 0.001 * 6.834355354309082
Epoch 330, val loss: 0.8638943433761597
Epoch 340, training loss: 0.17464034259319305 = 0.16780997812747955 + 0.001 * 6.830359935760498
Epoch 340, val loss: 0.8777295351028442
Epoch 350, training loss: 0.15400131046772003 = 0.14717473089694977 + 0.001 * 6.826583385467529
Epoch 350, val loss: 0.892475962638855
Epoch 360, training loss: 0.13554799556732178 = 0.12872448563575745 + 0.001 * 6.823511123657227
Epoch 360, val loss: 0.9082702398300171
Epoch 370, training loss: 0.1192130520939827 = 0.1123921349644661 + 0.001 * 6.82091760635376
Epoch 370, val loss: 0.9245518445968628
Epoch 380, training loss: 0.10496030002832413 = 0.0981423631310463 + 0.001 * 6.817934513092041
Epoch 380, val loss: 0.9412770867347717
Epoch 390, training loss: 0.09274560958147049 = 0.08593138307332993 + 0.001 * 6.814225673675537
Epoch 390, val loss: 0.9583668112754822
Epoch 400, training loss: 0.08233338594436646 = 0.07552196830511093 + 0.001 * 6.8114166259765625
Epoch 400, val loss: 0.9755592346191406
Epoch 410, training loss: 0.07343235611915588 = 0.06662285327911377 + 0.001 * 6.809504985809326
Epoch 410, val loss: 0.9928621649742126
Epoch 420, training loss: 0.0658063068985939 = 0.05899598449468613 + 0.001 * 6.810322284698486
Epoch 420, val loss: 1.0100511312484741
Epoch 430, training loss: 0.05926240235567093 = 0.052454955875873566 + 0.001 * 6.807446002960205
Epoch 430, val loss: 1.0271044969558716
Epoch 440, training loss: 0.053642693907022476 = 0.04683002457022667 + 0.001 * 6.812668323516846
Epoch 440, val loss: 1.043846607208252
Epoch 450, training loss: 0.04879035800695419 = 0.04198124632239342 + 0.001 * 6.809112548828125
Epoch 450, val loss: 1.0603512525558472
Epoch 460, training loss: 0.04459180682897568 = 0.03778732568025589 + 0.001 * 6.804479122161865
Epoch 460, val loss: 1.0765750408172607
Epoch 470, training loss: 0.04096062853932381 = 0.03415757045149803 + 0.001 * 6.803056716918945
Epoch 470, val loss: 1.0924813747406006
Epoch 480, training loss: 0.037795137614011765 = 0.030993381515145302 + 0.001 * 6.801756858825684
Epoch 480, val loss: 1.108054518699646
Epoch 490, training loss: 0.03502634912729263 = 0.028223874047398567 + 0.001 * 6.802473545074463
Epoch 490, val loss: 1.1231006383895874
Epoch 500, training loss: 0.032596368342638016 = 0.025794507935643196 + 0.001 * 6.801859378814697
Epoch 500, val loss: 1.1377087831497192
Epoch 510, training loss: 0.030458195134997368 = 0.02365829609334469 + 0.001 * 6.799899101257324
Epoch 510, val loss: 1.1518490314483643
Epoch 520, training loss: 0.028567448258399963 = 0.02176836133003235 + 0.001 * 6.799086570739746
Epoch 520, val loss: 1.165605068206787
Epoch 530, training loss: 0.026882801204919815 = 0.020089440047740936 + 0.001 * 6.793360233306885
Epoch 530, val loss: 1.1788980960845947
Epoch 540, training loss: 0.025405723601579666 = 0.01859322562813759 + 0.001 * 6.812497138977051
Epoch 540, val loss: 1.191818118095398
Epoch 550, training loss: 0.02404564805328846 = 0.017255576327443123 + 0.001 * 6.7900710105896
Epoch 550, val loss: 1.2043131589889526
Epoch 560, training loss: 0.022847164422273636 = 0.016055878251791 + 0.001 * 6.791285991668701
Epoch 560, val loss: 1.2164015769958496
Epoch 570, training loss: 0.021765166893601418 = 0.01497653964906931 + 0.001 * 6.7886271476745605
Epoch 570, val loss: 1.2280718088150024
Epoch 580, training loss: 0.020786726847290993 = 0.014002774842083454 + 0.001 * 6.783951759338379
Epoch 580, val loss: 1.2393704652786255
Epoch 590, training loss: 0.019904937595129013 = 0.013121303170919418 + 0.001 * 6.783633708953857
Epoch 590, val loss: 1.2503412961959839
Epoch 600, training loss: 0.01910458505153656 = 0.012321045622229576 + 0.001 * 6.783538818359375
Epoch 600, val loss: 1.260939121246338
Epoch 610, training loss: 0.018406102433800697 = 0.011591439135372639 + 0.001 * 6.814663410186768
Epoch 610, val loss: 1.2712393999099731
Epoch 620, training loss: 0.017704930156469345 = 0.010925199836492538 + 0.001 * 6.779729843139648
Epoch 620, val loss: 1.281235933303833
Epoch 630, training loss: 0.017089085653424263 = 0.010315715335309505 + 0.001 * 6.773369312286377
Epoch 630, val loss: 1.2909228801727295
Epoch 640, training loss: 0.016542868688702583 = 0.009757168591022491 + 0.001 * 6.785699367523193
Epoch 640, val loss: 1.300285816192627
Epoch 650, training loss: 0.01601741462945938 = 0.009244294837117195 + 0.001 * 6.773119926452637
Epoch 650, val loss: 1.3094204664230347
Epoch 660, training loss: 0.01554254163056612 = 0.008772315457463264 + 0.001 * 6.770226001739502
Epoch 660, val loss: 1.3182991743087769
Epoch 670, training loss: 0.015103175304830074 = 0.008336962200701237 + 0.001 * 6.7662129402160645
Epoch 670, val loss: 1.3269367218017578
Epoch 680, training loss: 0.014696158468723297 = 0.007934638299047947 + 0.001 * 6.7615203857421875
Epoch 680, val loss: 1.3353065252304077
Epoch 690, training loss: 0.014323058538138866 = 0.007562105543911457 + 0.001 * 6.760952472686768
Epoch 690, val loss: 1.3434523344039917
Epoch 700, training loss: 0.013976680114865303 = 0.007216627709567547 + 0.001 * 6.76005220413208
Epoch 700, val loss: 1.3513628244400024
Epoch 710, training loss: 0.01366361603140831 = 0.00689536752179265 + 0.001 * 6.768247604370117
Epoch 710, val loss: 1.3590867519378662
Epoch 720, training loss: 0.013351041823625565 = 0.0065961661748588085 + 0.001 * 6.754875183105469
Epoch 720, val loss: 1.3666051626205444
Epoch 730, training loss: 0.013066202402114868 = 0.006317425053566694 + 0.001 * 6.748776912689209
Epoch 730, val loss: 1.3739134073257446
Epoch 740, training loss: 0.012808918952941895 = 0.006057139486074448 + 0.001 * 6.751779556274414
Epoch 740, val loss: 1.3810139894485474
Epoch 750, training loss: 0.012560700997710228 = 0.005814186297357082 + 0.001 * 6.746514320373535
Epoch 750, val loss: 1.3879808187484741
Epoch 760, training loss: 0.012329580262303352 = 0.005586727522313595 + 0.001 * 6.742852687835693
Epoch 760, val loss: 1.3947550058364868
Epoch 770, training loss: 0.012116661295294762 = 0.005373392254114151 + 0.001 * 6.743268966674805
Epoch 770, val loss: 1.4013416767120361
Epoch 780, training loss: 0.011911082081496716 = 0.005173036828637123 + 0.001 * 6.738044738769531
Epoch 780, val loss: 1.4077879190444946
Epoch 790, training loss: 0.011723008006811142 = 0.004984687548130751 + 0.001 * 6.738320827484131
Epoch 790, val loss: 1.4141106605529785
Epoch 800, training loss: 0.011544669046998024 = 0.004807376302778721 + 0.001 * 6.737292766571045
Epoch 800, val loss: 1.4202293157577515
Epoch 810, training loss: 0.01137301791459322 = 0.00464020436629653 + 0.001 * 6.732813358306885
Epoch 810, val loss: 1.4262803792953491
Epoch 820, training loss: 0.011240679770708084 = 0.004481927026063204 + 0.001 * 6.758752346038818
Epoch 820, val loss: 1.4321342706680298
Epoch 830, training loss: 0.011082174256443977 = 0.004332179203629494 + 0.001 * 6.749995231628418
Epoch 830, val loss: 1.437868356704712
Epoch 840, training loss: 0.010920180007815361 = 0.0041906218975782394 + 0.001 * 6.72955846786499
Epoch 840, val loss: 1.443487524986267
Epoch 850, training loss: 0.010802569799125195 = 0.004056903999298811 + 0.001 * 6.745665550231934
Epoch 850, val loss: 1.4489617347717285
Epoch 860, training loss: 0.010675100609660149 = 0.003930369392037392 + 0.001 * 6.7447309494018555
Epoch 860, val loss: 1.4543654918670654
Epoch 870, training loss: 0.010531174018979073 = 0.003810535417869687 + 0.001 * 6.720638275146484
Epoch 870, val loss: 1.4596569538116455
Epoch 880, training loss: 0.01044666301459074 = 0.0036968097556382418 + 0.001 * 6.749853134155273
Epoch 880, val loss: 1.4647774696350098
Epoch 890, training loss: 0.01031726598739624 = 0.0035885937977582216 + 0.001 * 6.728672027587891
Epoch 890, val loss: 1.4698235988616943
Epoch 900, training loss: 0.010223987512290478 = 0.003485957160592079 + 0.001 * 6.738029956817627
Epoch 900, val loss: 1.474770188331604
Epoch 910, training loss: 0.010111614130437374 = 0.003388327546417713 + 0.001 * 6.723286151885986
Epoch 910, val loss: 1.4795966148376465
Epoch 920, training loss: 0.010013355873525143 = 0.003295375732704997 + 0.001 * 6.717979907989502
Epoch 920, val loss: 1.4843448400497437
Epoch 930, training loss: 0.009925573132932186 = 0.003206779947504401 + 0.001 * 6.718792915344238
Epoch 930, val loss: 1.4889867305755615
Epoch 940, training loss: 0.009848122484982014 = 0.003122250083833933 + 0.001 * 6.725872039794922
Epoch 940, val loss: 1.4935556650161743
Epoch 950, training loss: 0.00977126695215702 = 0.0030415826477110386 + 0.001 * 6.7296833992004395
Epoch 950, val loss: 1.4980188608169556
Epoch 960, training loss: 0.009680363349616528 = 0.0029645017348229885 + 0.001 * 6.7158613204956055
Epoch 960, val loss: 1.5023680925369263
Epoch 970, training loss: 0.009596377611160278 = 0.0028907994274049997 + 0.001 * 6.705578327178955
Epoch 970, val loss: 1.5066684484481812
Epoch 980, training loss: 0.00952606275677681 = 0.0028202610556036234 + 0.001 * 6.705801010131836
Epoch 980, val loss: 1.5108457803726196
Epoch 990, training loss: 0.009458418935537338 = 0.0027527729980647564 + 0.001 * 6.705645561218262
Epoch 990, val loss: 1.514984130859375
Epoch 1000, training loss: 0.009386076591908932 = 0.0026881236117333174 + 0.001 * 6.697952747344971
Epoch 1000, val loss: 1.519019603729248
Epoch 1010, training loss: 0.009336581453680992 = 0.002626162488013506 + 0.001 * 6.710419178009033
Epoch 1010, val loss: 1.5229945182800293
Epoch 1020, training loss: 0.009268946945667267 = 0.002566695911809802 + 0.001 * 6.702250957489014
Epoch 1020, val loss: 1.5268747806549072
Epoch 1030, training loss: 0.009222912602126598 = 0.0025096505414694548 + 0.001 * 6.71326208114624
Epoch 1030, val loss: 1.5306910276412964
Epoch 1040, training loss: 0.00916206929832697 = 0.0024549236986786127 + 0.001 * 6.707145690917969
Epoch 1040, val loss: 1.5344904661178589
Epoch 1050, training loss: 0.009093576110899448 = 0.0024023731239140034 + 0.001 * 6.691202640533447
Epoch 1050, val loss: 1.5381888151168823
Epoch 1060, training loss: 0.00906014908105135 = 0.0023518537636846304 + 0.001 * 6.7082953453063965
Epoch 1060, val loss: 1.54181969165802
Epoch 1070, training loss: 0.009001655504107475 = 0.0023033141624182463 + 0.001 * 6.698341369628906
Epoch 1070, val loss: 1.5453851222991943
Epoch 1080, training loss: 0.008987460285425186 = 0.0022566039115190506 + 0.001 * 6.730855464935303
Epoch 1080, val loss: 1.5488947629928589
Epoch 1090, training loss: 0.008901933208107948 = 0.00221164058893919 + 0.001 * 6.690291881561279
Epoch 1090, val loss: 1.5523484945297241
Epoch 1100, training loss: 0.008852710016071796 = 0.0021683606319129467 + 0.001 * 6.684349060058594
Epoch 1100, val loss: 1.555711269378662
Epoch 1110, training loss: 0.008807129226624966 = 0.002126671141013503 + 0.001 * 6.680458068847656
Epoch 1110, val loss: 1.5590221881866455
Epoch 1120, training loss: 0.008763655088841915 = 0.002086496911942959 + 0.001 * 6.677157878875732
Epoch 1120, val loss: 1.5622913837432861
Epoch 1130, training loss: 0.008797473274171352 = 0.002047780202701688 + 0.001 * 6.749692440032959
Epoch 1130, val loss: 1.5654819011688232
Epoch 1140, training loss: 0.008700497448444366 = 0.0020104048307985067 + 0.001 * 6.690092086791992
Epoch 1140, val loss: 1.5686460733413696
Epoch 1150, training loss: 0.008665485307574272 = 0.0019743572920560837 + 0.001 * 6.691128253936768
Epoch 1150, val loss: 1.5717610120773315
Epoch 1160, training loss: 0.008642790839076042 = 0.001939560635946691 + 0.001 * 6.703229904174805
Epoch 1160, val loss: 1.5747979879379272
Epoch 1170, training loss: 0.00857884343713522 = 0.0019059375626966357 + 0.001 * 6.672905921936035
Epoch 1170, val loss: 1.5778155326843262
Epoch 1180, training loss: 0.008608326315879822 = 0.0018734929617494345 + 0.001 * 6.734833240509033
Epoch 1180, val loss: 1.5807634592056274
Epoch 1190, training loss: 0.008528727106750011 = 0.0018420927226543427 + 0.001 * 6.686634063720703
Epoch 1190, val loss: 1.5836728811264038
Epoch 1200, training loss: 0.008480535820126534 = 0.0018117581494152546 + 0.001 * 6.668777942657471
Epoch 1200, val loss: 1.5865405797958374
Epoch 1210, training loss: 0.00847464520484209 = 0.0017824150854721665 + 0.001 * 6.692229747772217
Epoch 1210, val loss: 1.5893412828445435
Epoch 1220, training loss: 0.008443805389106274 = 0.0017540002008900046 + 0.001 * 6.689804553985596
Epoch 1220, val loss: 1.5921404361724854
Epoch 1230, training loss: 0.0084120137616992 = 0.0017265296773985028 + 0.001 * 6.685483455657959
Epoch 1230, val loss: 1.5948354005813599
Epoch 1240, training loss: 0.008369434624910355 = 0.0016999096842482686 + 0.001 * 6.669524192810059
Epoch 1240, val loss: 1.597524642944336
Epoch 1250, training loss: 0.00833477359265089 = 0.0016741559375077486 + 0.001 * 6.660616874694824
Epoch 1250, val loss: 1.6001641750335693
Epoch 1260, training loss: 0.008317259140312672 = 0.0016491725109517574 + 0.001 * 6.668086528778076
Epoch 1260, val loss: 1.6027538776397705
Epoch 1270, training loss: 0.008327499963343143 = 0.0016249822219833732 + 0.001 * 6.702517509460449
Epoch 1270, val loss: 1.6053051948547363
Epoch 1280, training loss: 0.00828971341252327 = 0.0016015084693208337 + 0.001 * 6.688204288482666
Epoch 1280, val loss: 1.6078283786773682
Epoch 1290, training loss: 0.00826297141611576 = 0.0015787878073751926 + 0.001 * 6.684183120727539
Epoch 1290, val loss: 1.6102906465530396
Epoch 1300, training loss: 0.00821744091808796 = 0.0015567203518003225 + 0.001 * 6.660720348358154
Epoch 1300, val loss: 1.612738847732544
Epoch 1310, training loss: 0.008198348805308342 = 0.0015353192575275898 + 0.001 * 6.663029670715332
Epoch 1310, val loss: 1.6151587963104248
Epoch 1320, training loss: 0.008162161335349083 = 0.0015145360957831144 + 0.001 * 6.647624492645264
Epoch 1320, val loss: 1.617492914199829
Epoch 1330, training loss: 0.00814826600253582 = 0.0014943619025871158 + 0.001 * 6.653903484344482
Epoch 1330, val loss: 1.6198413372039795
Epoch 1340, training loss: 0.008122710511088371 = 0.0014747800305485725 + 0.001 * 6.647929668426514
Epoch 1340, val loss: 1.6221258640289307
Epoch 1350, training loss: 0.008102755062282085 = 0.0014557578833773732 + 0.001 * 6.64699649810791
Epoch 1350, val loss: 1.6243841648101807
Epoch 1360, training loss: 0.008090170100331306 = 0.0014372735749930143 + 0.001 * 6.652895927429199
Epoch 1360, val loss: 1.626618504524231
Epoch 1370, training loss: 0.008108499459922314 = 0.001419302774593234 + 0.001 * 6.6891961097717285
Epoch 1370, val loss: 1.628786563873291
Epoch 1380, training loss: 0.008068577386438847 = 0.0014018317451700568 + 0.001 * 6.666745662689209
Epoch 1380, val loss: 1.6309651136398315
Epoch 1390, training loss: 0.008035270497202873 = 0.0013848372036591172 + 0.001 * 6.65043306350708
Epoch 1390, val loss: 1.6330797672271729
Epoch 1400, training loss: 0.008005185052752495 = 0.001368270255625248 + 0.001 * 6.636914253234863
Epoch 1400, val loss: 1.6351909637451172
Epoch 1410, training loss: 0.007989378646016121 = 0.0013521518558263779 + 0.001 * 6.637226104736328
Epoch 1410, val loss: 1.637221097946167
Epoch 1420, training loss: 0.007971540093421936 = 0.00133646244648844 + 0.001 * 6.635077476501465
Epoch 1420, val loss: 1.6392704248428345
Epoch 1430, training loss: 0.007957293651998043 = 0.0013211971381679177 + 0.001 * 6.636096477508545
Epoch 1430, val loss: 1.6412633657455444
Epoch 1440, training loss: 0.007967847399413586 = 0.0013063360238447785 + 0.001 * 6.661510944366455
Epoch 1440, val loss: 1.643258810043335
Epoch 1450, training loss: 0.007969076745212078 = 0.0012918822467327118 + 0.001 * 6.677194118499756
Epoch 1450, val loss: 1.6451654434204102
Epoch 1460, training loss: 0.007914436981081963 = 0.0012777922675013542 + 0.001 * 6.636643886566162
Epoch 1460, val loss: 1.6471089124679565
Epoch 1470, training loss: 0.007916945964097977 = 0.0012640762142837048 + 0.001 * 6.65286922454834
Epoch 1470, val loss: 1.6489707231521606
Epoch 1480, training loss: 0.00788912083953619 = 0.0012507037026807666 + 0.001 * 6.638416767120361
Epoch 1480, val loss: 1.6508543491363525
Epoch 1490, training loss: 0.007871498353779316 = 0.0012376673985272646 + 0.001 * 6.633831024169922
Epoch 1490, val loss: 1.652673602104187
Epoch 1500, training loss: 0.007857204414904118 = 0.0012249578721821308 + 0.001 * 6.632246017456055
Epoch 1500, val loss: 1.654492735862732
Epoch 1510, training loss: 0.007860076613724232 = 0.001212562550790608 + 0.001 * 6.647513389587402
Epoch 1510, val loss: 1.656304121017456
Epoch 1520, training loss: 0.007848723791539669 = 0.0012005072785541415 + 0.001 * 6.648216724395752
Epoch 1520, val loss: 1.6580339670181274
Epoch 1530, training loss: 0.00789211317896843 = 0.0011887233704328537 + 0.001 * 6.7033891677856445
Epoch 1530, val loss: 1.6598408222198486
Epoch 1540, training loss: 0.007822485640645027 = 0.0011772383004426956 + 0.001 * 6.645247459411621
Epoch 1540, val loss: 1.661502718925476
Epoch 1550, training loss: 0.007792873308062553 = 0.0011660491582006216 + 0.001 * 6.626823425292969
Epoch 1550, val loss: 1.6631914377212524
Epoch 1560, training loss: 0.00776573084294796 = 0.0011550990166142583 + 0.001 * 6.610631465911865
Epoch 1560, val loss: 1.6648808717727661
Epoch 1570, training loss: 0.007755015045404434 = 0.001144426641985774 + 0.001 * 6.610588550567627
Epoch 1570, val loss: 1.6665077209472656
Epoch 1580, training loss: 0.007752165198326111 = 0.0011339988559484482 + 0.001 * 6.618165969848633
Epoch 1580, val loss: 1.6681594848632812
Epoch 1590, training loss: 0.0077353487722575665 = 0.0011238203151151538 + 0.001 * 6.611527919769287
Epoch 1590, val loss: 1.6697564125061035
Epoch 1600, training loss: 0.007728602737188339 = 0.0011138718109577894 + 0.001 * 6.614730358123779
Epoch 1600, val loss: 1.671386480331421
Epoch 1610, training loss: 0.007725831586867571 = 0.0011041550897061825 + 0.001 * 6.621675968170166
Epoch 1610, val loss: 1.6729295253753662
Epoch 1620, training loss: 0.007713809609413147 = 0.0010946369729936123 + 0.001 * 6.619172096252441
Epoch 1620, val loss: 1.6745201349258423
Epoch 1630, training loss: 0.007684529758989811 = 0.0010853578569367528 + 0.001 * 6.5991716384887695
Epoch 1630, val loss: 1.676051139831543
Epoch 1640, training loss: 0.007687574718147516 = 0.0010762745514512062 + 0.001 * 6.611299991607666
Epoch 1640, val loss: 1.6775672435760498
Epoch 1650, training loss: 0.007680586539208889 = 0.0010673989308997989 + 0.001 * 6.613187313079834
Epoch 1650, val loss: 1.679072380065918
Epoch 1660, training loss: 0.007707158103585243 = 0.0010587265715003014 + 0.001 * 6.648431301116943
Epoch 1660, val loss: 1.6805578470230103
Epoch 1670, training loss: 0.007676207926124334 = 0.001050262595526874 + 0.001 * 6.625945091247559
Epoch 1670, val loss: 1.6820125579833984
Epoch 1680, training loss: 0.007664993405342102 = 0.0010419616010040045 + 0.001 * 6.6230316162109375
Epoch 1680, val loss: 1.6834641695022583
Epoch 1690, training loss: 0.0076786307618021965 = 0.0010338731808587909 + 0.001 * 6.644757270812988
Epoch 1690, val loss: 1.6848963499069214
Epoch 1700, training loss: 0.007650658488273621 = 0.0010259200353175402 + 0.001 * 6.6247382164001465
Epoch 1700, val loss: 1.6863332986831665
Epoch 1710, training loss: 0.007628945633769035 = 0.00101816700771451 + 0.001 * 6.610778331756592
Epoch 1710, val loss: 1.6876893043518066
Epoch 1720, training loss: 0.007606158964335918 = 0.0010105518158525229 + 0.001 * 6.595607280731201
Epoch 1720, val loss: 1.6890939474105835
Epoch 1730, training loss: 0.007597547955811024 = 0.0010031209094449878 + 0.001 * 6.59442663192749
Epoch 1730, val loss: 1.6904445886611938
Epoch 1740, training loss: 0.007601964753121138 = 0.0009958279551938176 + 0.001 * 6.606136798858643
Epoch 1740, val loss: 1.691778302192688
Epoch 1750, training loss: 0.007580452132970095 = 0.0009886919287964702 + 0.001 * 6.59175968170166
Epoch 1750, val loss: 1.6931284666061401
Epoch 1760, training loss: 0.0075625889003276825 = 0.0009817138779908419 + 0.001 * 6.580874919891357
Epoch 1760, val loss: 1.6944482326507568
Epoch 1770, training loss: 0.00755453621968627 = 0.000974868016783148 + 0.001 * 6.579668045043945
Epoch 1770, val loss: 1.6957036256790161
Epoch 1780, training loss: 0.007543841376900673 = 0.000968164240475744 + 0.001 * 6.575676918029785
Epoch 1780, val loss: 1.6969913244247437
Epoch 1790, training loss: 0.007556847296655178 = 0.0009615830495022237 + 0.001 * 6.595263957977295
Epoch 1790, val loss: 1.6982403993606567
Epoch 1800, training loss: 0.007564965635538101 = 0.0009551430121064186 + 0.001 * 6.6098222732543945
Epoch 1800, val loss: 1.6995326280593872
Epoch 1810, training loss: 0.007538158912211657 = 0.0009488207870163023 + 0.001 * 6.5893378257751465
Epoch 1810, val loss: 1.7007842063903809
Epoch 1820, training loss: 0.00752074783667922 = 0.000942613638471812 + 0.001 * 6.578134059906006
Epoch 1820, val loss: 1.7019715309143066
Epoch 1830, training loss: 0.007543374318629503 = 0.00093654211377725 + 0.001 * 6.606832027435303
Epoch 1830, val loss: 1.703192949295044
Epoch 1840, training loss: 0.007513017393648624 = 0.0009305842686444521 + 0.001 * 6.582433223724365
Epoch 1840, val loss: 1.7044299840927124
Epoch 1850, training loss: 0.007522039581090212 = 0.0009247305570170283 + 0.001 * 6.59730863571167
Epoch 1850, val loss: 1.7055816650390625
Epoch 1860, training loss: 0.007496306672692299 = 0.0009190113632939756 + 0.001 * 6.577294826507568
Epoch 1860, val loss: 1.7067852020263672
Epoch 1870, training loss: 0.0074921888299286366 = 0.0009133876301348209 + 0.001 * 6.578800678253174
Epoch 1870, val loss: 1.7079039812088013
Epoch 1880, training loss: 0.007474039681255817 = 0.0009078708244487643 + 0.001 * 6.566168308258057
Epoch 1880, val loss: 1.709057331085205
Epoch 1890, training loss: 0.007486489601433277 = 0.0009024784667417407 + 0.001 * 6.584010601043701
Epoch 1890, val loss: 1.7102361917495728
Epoch 1900, training loss: 0.007499797269701958 = 0.0008971698116511106 + 0.001 * 6.602627277374268
Epoch 1900, val loss: 1.7113072872161865
Epoch 1910, training loss: 0.007543537300080061 = 0.0008919546380639076 + 0.001 * 6.65158224105835
Epoch 1910, val loss: 1.7124546766281128
Epoch 1920, training loss: 0.007469488307833672 = 0.0008868489530868828 + 0.001 * 6.582638740539551
Epoch 1920, val loss: 1.7135223150253296
Epoch 1930, training loss: 0.007451765239238739 = 0.0008818099740892649 + 0.001 * 6.569955348968506
Epoch 1930, val loss: 1.7146427631378174
Epoch 1940, training loss: 0.007435239385813475 = 0.0008768845582380891 + 0.001 * 6.55835485458374
Epoch 1940, val loss: 1.7157292366027832
Epoch 1950, training loss: 0.007440794724971056 = 0.0008720229379832745 + 0.001 * 6.5687713623046875
Epoch 1950, val loss: 1.71678626537323
Epoch 1960, training loss: 0.007424941752105951 = 0.000867246650159359 + 0.001 * 6.557694911956787
Epoch 1960, val loss: 1.7178447246551514
Epoch 1970, training loss: 0.0074352603405714035 = 0.0008625625632703304 + 0.001 * 6.572697639465332
Epoch 1970, val loss: 1.7188881635665894
Epoch 1980, training loss: 0.007455083541572094 = 0.0008579581044614315 + 0.001 * 6.597125053405762
Epoch 1980, val loss: 1.7199946641921997
Epoch 1990, training loss: 0.0074280244298279285 = 0.0008534064400009811 + 0.001 * 6.574617862701416
Epoch 1990, val loss: 1.7209657430648804
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7630
Overall ASR: 0.8635
Flip ASR: 0.8356/225 nodes
./selected_nodes/Cora/Overall/seed996/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9539077281951904 = 1.9455338716506958 + 0.001 * 8.373806953430176
Epoch 0, val loss: 1.952052116394043
Epoch 10, training loss: 1.942728042602539 = 1.934354305267334 + 0.001 * 8.373726844787598
Epoch 10, val loss: 1.9391623735427856
Epoch 20, training loss: 1.9288763999938965 = 1.9205029010772705 + 0.001 * 8.373446464538574
Epoch 20, val loss: 1.9226374626159668
Epoch 30, training loss: 1.9096856117248535 = 1.9013128280639648 + 0.001 * 8.372775077819824
Epoch 30, val loss: 1.8997547626495361
Epoch 40, training loss: 1.882796049118042 = 1.874424695968628 + 0.001 * 8.371320724487305
Epoch 40, val loss: 1.8692153692245483
Epoch 50, training loss: 1.8465561866760254 = 1.8381884098052979 + 0.001 * 8.367815017700195
Epoch 50, val loss: 1.8310160636901855
Epoch 60, training loss: 1.80330491065979 = 1.7949485778808594 + 0.001 * 8.356332778930664
Epoch 60, val loss: 1.7891072034835815
Epoch 70, training loss: 1.7606987953186035 = 1.7523998022079468 + 0.001 * 8.298978805541992
Epoch 70, val loss: 1.751713752746582
Epoch 80, training loss: 1.7111799716949463 = 1.7032920122146606 + 0.001 * 7.88792610168457
Epoch 80, val loss: 1.7098090648651123
Epoch 90, training loss: 1.6429681777954102 = 1.6353800296783447 + 0.001 * 7.588102340698242
Epoch 90, val loss: 1.6536740064620972
Epoch 100, training loss: 1.5529210567474365 = 1.5454554557800293 + 0.001 * 7.465594291687012
Epoch 100, val loss: 1.5813195705413818
Epoch 110, training loss: 1.4484429359436035 = 1.44105863571167 + 0.001 * 7.38433313369751
Epoch 110, val loss: 1.5012116432189941
Epoch 120, training loss: 1.3440049886703491 = 1.336731195449829 + 0.001 * 7.273746013641357
Epoch 120, val loss: 1.4251599311828613
Epoch 130, training loss: 1.2469249963760376 = 1.2397326231002808 + 0.001 * 7.192409515380859
Epoch 130, val loss: 1.3573591709136963
Epoch 140, training loss: 1.1581913232803345 = 1.151026964187622 + 0.001 * 7.164355754852295
Epoch 140, val loss: 1.2969902753829956
Epoch 150, training loss: 1.077620267868042 = 1.070467233657837 + 0.001 * 7.153007984161377
Epoch 150, val loss: 1.2413703203201294
Epoch 160, training loss: 1.0052028894424438 = 0.9980756044387817 + 0.001 * 7.12730598449707
Epoch 160, val loss: 1.1906052827835083
Epoch 170, training loss: 0.9404522776603699 = 0.9333493113517761 + 0.001 * 7.102978229522705
Epoch 170, val loss: 1.145697832107544
Epoch 180, training loss: 0.8813936710357666 = 0.8743104338645935 + 0.001 * 7.083254337310791
Epoch 180, val loss: 1.1061265468597412
Epoch 190, training loss: 0.825400710105896 = 0.8183353543281555 + 0.001 * 7.065335750579834
Epoch 190, val loss: 1.0697126388549805
Epoch 200, training loss: 0.770464301109314 = 0.763417661190033 + 0.001 * 7.046623229980469
Epoch 200, val loss: 1.0353083610534668
Epoch 210, training loss: 0.7155452966690063 = 0.7085216641426086 + 0.001 * 7.023656368255615
Epoch 210, val loss: 1.0019993782043457
Epoch 220, training loss: 0.659871518611908 = 0.6528753638267517 + 0.001 * 6.996125221252441
Epoch 220, val loss: 0.9686514139175415
Epoch 230, training loss: 0.6026344299316406 = 0.5956587791442871 + 0.001 * 6.975643157958984
Epoch 230, val loss: 0.9342457056045532
Epoch 240, training loss: 0.543719470500946 = 0.5367657542228699 + 0.001 * 6.953732967376709
Epoch 240, val loss: 0.9004769921302795
Epoch 250, training loss: 0.48488086462020874 = 0.4779428243637085 + 0.001 * 6.938032150268555
Epoch 250, val loss: 0.8701116442680359
Epoch 260, training loss: 0.42900198698043823 = 0.42207667231559753 + 0.001 * 6.925319671630859
Epoch 260, val loss: 0.8449715971946716
Epoch 270, training loss: 0.3784922659397125 = 0.371579110622406 + 0.001 * 6.91315221786499
Epoch 270, val loss: 0.8272556066513062
Epoch 280, training loss: 0.3344070613384247 = 0.3274911940097809 + 0.001 * 6.915877342224121
Epoch 280, val loss: 0.8172491192817688
Epoch 290, training loss: 0.29628556966781616 = 0.2893770933151245 + 0.001 * 6.908467769622803
Epoch 290, val loss: 0.8137115240097046
Epoch 300, training loss: 0.26267215609550476 = 0.25577035546302795 + 0.001 * 6.901796340942383
Epoch 300, val loss: 0.8145773410797119
Epoch 310, training loss: 0.23196518421173096 = 0.2250719964504242 + 0.001 * 6.893179893493652
Epoch 310, val loss: 0.8179723024368286
Epoch 320, training loss: 0.20318375527858734 = 0.19629816710948944 + 0.001 * 6.885581970214844
Epoch 320, val loss: 0.8226803541183472
Epoch 330, training loss: 0.17630454897880554 = 0.16942836344242096 + 0.001 * 6.876182556152344
Epoch 330, val loss: 0.8287047743797302
Epoch 340, training loss: 0.15200254321098328 = 0.14513060450553894 + 0.001 * 6.8719329833984375
Epoch 340, val loss: 0.8364813923835754
Epoch 350, training loss: 0.13091526925563812 = 0.12405630201101303 + 0.001 * 6.858972072601318
Epoch 350, val loss: 0.8465590476989746
Epoch 360, training loss: 0.1132231056690216 = 0.10637298226356506 + 0.001 * 6.850121974945068
Epoch 360, val loss: 0.8592684864997864
Epoch 370, training loss: 0.09861630201339722 = 0.09178241342306137 + 0.001 * 6.833889961242676
Epoch 370, val loss: 0.8746577501296997
Epoch 380, training loss: 0.08659128099679947 = 0.0797542929649353 + 0.001 * 6.83698844909668
Epoch 380, val loss: 0.8921281695365906
Epoch 390, training loss: 0.07660073786973953 = 0.06977136433124542 + 0.001 * 6.829376697540283
Epoch 390, val loss: 0.9109381437301636
Epoch 400, training loss: 0.06822437793016434 = 0.061408013105392456 + 0.001 * 6.816367149353027
Epoch 400, val loss: 0.9307377934455872
Epoch 410, training loss: 0.061147719621658325 = 0.05433390289545059 + 0.001 * 6.813817501068115
Epoch 410, val loss: 0.9510071873664856
Epoch 420, training loss: 0.05511825904250145 = 0.04830693081021309 + 0.001 * 6.811328411102295
Epoch 420, val loss: 0.9713674187660217
Epoch 430, training loss: 0.04996243491768837 = 0.043145760893821716 + 0.001 * 6.8166728019714355
Epoch 430, val loss: 0.9915127158164978
Epoch 440, training loss: 0.04552071914076805 = 0.03870818018913269 + 0.001 * 6.81253719329834
Epoch 440, val loss: 1.0112882852554321
Epoch 450, training loss: 0.041690174490213394 = 0.034878965467214584 + 0.001 * 6.8112101554870605
Epoch 450, val loss: 1.030504584312439
Epoch 460, training loss: 0.038373954594135284 = 0.03156394511461258 + 0.001 * 6.810008525848389
Epoch 460, val loss: 1.0490820407867432
Epoch 470, training loss: 0.035493697971105576 = 0.028684087097644806 + 0.001 * 6.8096113204956055
Epoch 470, val loss: 1.0670077800750732
Epoch 480, training loss: 0.03298209607601166 = 0.02617298625409603 + 0.001 * 6.809110641479492
Epoch 480, val loss: 1.08430814743042
Epoch 490, training loss: 0.030783262103796005 = 0.02397405356168747 + 0.001 * 6.809208393096924
Epoch 490, val loss: 1.1009355783462524
Epoch 500, training loss: 0.028853176161646843 = 0.022040212526917458 + 0.001 * 6.812963485717773
Epoch 500, val loss: 1.1169780492782593
Epoch 510, training loss: 0.02714337781071663 = 0.020331857725977898 + 0.001 * 6.811519145965576
Epoch 510, val loss: 1.1324406862258911
Epoch 520, training loss: 0.02562481164932251 = 0.01881624013185501 + 0.001 * 6.8085713386535645
Epoch 520, val loss: 1.1473544836044312
Epoch 530, training loss: 0.024292567744851112 = 0.01746608316898346 + 0.001 * 6.826484680175781
Epoch 530, val loss: 1.1617485284805298
Epoch 540, training loss: 0.023070786148309708 = 0.016258567571640015 + 0.001 * 6.812217712402344
Epoch 540, val loss: 1.1756577491760254
Epoch 550, training loss: 0.021983269602060318 = 0.015174532309174538 + 0.001 * 6.8087358474731445
Epoch 550, val loss: 1.189111590385437
Epoch 560, training loss: 0.02100416272878647 = 0.014197925105690956 + 0.001 * 6.806236267089844
Epoch 560, val loss: 1.2021559476852417
Epoch 570, training loss: 0.020129438489675522 = 0.013315155170857906 + 0.001 * 6.814281940460205
Epoch 570, val loss: 1.2147798538208008
Epoch 580, training loss: 0.019322767853736877 = 0.012514646165072918 + 0.001 * 6.808122158050537
Epoch 580, val loss: 1.2270328998565674
Epoch 590, training loss: 0.018591970205307007 = 0.011786499060690403 + 0.001 * 6.805471897125244
Epoch 590, val loss: 1.2389191389083862
Epoch 600, training loss: 0.01792873628437519 = 0.011122403666377068 + 0.001 * 6.806332111358643
Epoch 600, val loss: 1.2504554986953735
Epoch 610, training loss: 0.017328862100839615 = 0.010515100322663784 + 0.001 * 6.813760757446289
Epoch 610, val loss: 1.261658787727356
Epoch 620, training loss: 0.0167631134390831 = 0.009958338923752308 + 0.001 * 6.804775238037109
Epoch 620, val loss: 1.2725498676300049
Epoch 630, training loss: 0.01624992862343788 = 0.009446632117033005 + 0.001 * 6.803295612335205
Epoch 630, val loss: 1.2831450700759888
Epoch 640, training loss: 0.015775661915540695 = 0.00897528138011694 + 0.001 * 6.800380706787109
Epoch 640, val loss: 1.2934536933898926
Epoch 650, training loss: 0.015348886139690876 = 0.008540178649127483 + 0.001 * 6.808707237243652
Epoch 650, val loss: 1.3034824132919312
Epoch 660, training loss: 0.014944103546440601 = 0.008137719705700874 + 0.001 * 6.8063836097717285
Epoch 660, val loss: 1.3132529258728027
Epoch 670, training loss: 0.014566086232662201 = 0.007764699403196573 + 0.001 * 6.801385879516602
Epoch 670, val loss: 1.3227894306182861
Epoch 680, training loss: 0.014215771108865738 = 0.0074182781390845776 + 0.001 * 6.797492980957031
Epoch 680, val loss: 1.3320603370666504
Epoch 690, training loss: 0.013898273929953575 = 0.007096020970493555 + 0.001 * 6.802252769470215
Epoch 690, val loss: 1.341123104095459
Epoch 700, training loss: 0.013594221323728561 = 0.006795715540647507 + 0.001 * 6.7985053062438965
Epoch 700, val loss: 1.3499506711959839
Epoch 710, training loss: 0.01330915093421936 = 0.006515405606478453 + 0.001 * 6.7937445640563965
Epoch 710, val loss: 1.358572006225586
Epoch 720, training loss: 0.01304951123893261 = 0.006253316067159176 + 0.001 * 6.796194553375244
Epoch 720, val loss: 1.3669896125793457
Epoch 730, training loss: 0.012800771743059158 = 0.00600788276642561 + 0.001 * 6.792888164520264
Epoch 730, val loss: 1.3752236366271973
Epoch 740, training loss: 0.012570848688483238 = 0.005777668207883835 + 0.001 * 6.793180465698242
Epoch 740, val loss: 1.3832579851150513
Epoch 750, training loss: 0.012356732040643692 = 0.005560989957302809 + 0.001 * 6.795742034912109
Epoch 750, val loss: 1.3911514282226562
Epoch 760, training loss: 0.012156090699136257 = 0.0053566633723676205 + 0.001 * 6.799427032470703
Epoch 760, val loss: 1.3988527059555054
Epoch 770, training loss: 0.011961912736296654 = 0.005163389723747969 + 0.001 * 6.798521995544434
Epoch 770, val loss: 1.40642511844635
Epoch 780, training loss: 0.011771714314818382 = 0.004979995079338551 + 0.001 * 6.791718482971191
Epoch 780, val loss: 1.4138877391815186
Epoch 790, training loss: 0.01159493625164032 = 0.004805503413081169 + 0.001 * 6.789432525634766
Epoch 790, val loss: 1.421213150024414
Epoch 800, training loss: 0.011431096121668816 = 0.0046392520889639854 + 0.001 * 6.791843891143799
Epoch 800, val loss: 1.4284359216690063
Epoch 810, training loss: 0.011264605447649956 = 0.00448071351274848 + 0.001 * 6.783891677856445
Epoch 810, val loss: 1.4355599880218506
Epoch 820, training loss: 0.011115241795778275 = 0.00432927580550313 + 0.001 * 6.785965442657471
Epoch 820, val loss: 1.4425952434539795
Epoch 830, training loss: 0.010968096554279327 = 0.004184768069535494 + 0.001 * 6.783328533172607
Epoch 830, val loss: 1.4495229721069336
Epoch 840, training loss: 0.010827058926224709 = 0.0040469360537827015 + 0.001 * 6.78012228012085
Epoch 840, val loss: 1.4563429355621338
Epoch 850, training loss: 0.010704031214118004 = 0.003915378358215094 + 0.001 * 6.788652420043945
Epoch 850, val loss: 1.4630990028381348
Epoch 860, training loss: 0.010573692619800568 = 0.0037899864837527275 + 0.001 * 6.783705234527588
Epoch 860, val loss: 1.469747543334961
Epoch 870, training loss: 0.01046086847782135 = 0.0036700803320854902 + 0.001 * 6.790788173675537
Epoch 870, val loss: 1.476272463798523
Epoch 880, training loss: 0.010345524176955223 = 0.0035549921449273825 + 0.001 * 6.790532112121582
Epoch 880, val loss: 1.482748031616211
Epoch 890, training loss: 0.010221263393759727 = 0.0034438367001712322 + 0.001 * 6.777425765991211
Epoch 890, val loss: 1.4890116453170776
Epoch 900, training loss: 0.010115577839314938 = 0.0033363683614879847 + 0.001 * 6.779209613800049
Epoch 900, val loss: 1.495315432548523
Epoch 910, training loss: 0.010008199140429497 = 0.003232771996408701 + 0.001 * 6.775426864624023
Epoch 910, val loss: 1.5015872716903687
Epoch 920, training loss: 0.009914958849549294 = 0.0031329584307968616 + 0.001 * 6.782000541687012
Epoch 920, val loss: 1.507756233215332
Epoch 930, training loss: 0.00981046911329031 = 0.003037084126845002 + 0.001 * 6.7733845710754395
Epoch 930, val loss: 1.51388680934906
Epoch 940, training loss: 0.009719211608171463 = 0.0029450892470777035 + 0.001 * 6.77412223815918
Epoch 940, val loss: 1.5198715925216675
Epoch 950, training loss: 0.009632285684347153 = 0.0028568825218826532 + 0.001 * 6.775403022766113
Epoch 950, val loss: 1.525834321975708
Epoch 960, training loss: 0.009539945051074028 = 0.002772391075268388 + 0.001 * 6.767553329467773
Epoch 960, val loss: 1.5316894054412842
Epoch 970, training loss: 0.009475259110331535 = 0.0026915278285741806 + 0.001 * 6.783731460571289
Epoch 970, val loss: 1.5374951362609863
Epoch 980, training loss: 0.009387671016156673 = 0.00261426018550992 + 0.001 * 6.773410320281982
Epoch 980, val loss: 1.5431914329528809
Epoch 990, training loss: 0.009312923066318035 = 0.002540379296988249 + 0.001 * 6.772543430328369
Epoch 990, val loss: 1.5488016605377197
Epoch 1000, training loss: 0.00924216490238905 = 0.0024697447661310434 + 0.001 * 6.7724199295043945
Epoch 1000, val loss: 1.5543450117111206
Epoch 1010, training loss: 0.009170645847916603 = 0.0024022883735597134 + 0.001 * 6.768357276916504
Epoch 1010, val loss: 1.5597699880599976
Epoch 1020, training loss: 0.00910903699696064 = 0.002337799873203039 + 0.001 * 6.771237373352051
Epoch 1020, val loss: 1.565090298652649
Epoch 1030, training loss: 0.009036073461174965 = 0.002276260871440172 + 0.001 * 6.759812355041504
Epoch 1030, val loss: 1.5703293085098267
Epoch 1040, training loss: 0.00896670762449503 = 0.002217513043433428 + 0.001 * 6.749194145202637
Epoch 1040, val loss: 1.5754895210266113
Epoch 1050, training loss: 0.008938247337937355 = 0.0021613729186356068 + 0.001 * 6.776873588562012
Epoch 1050, val loss: 1.5805672407150269
Epoch 1060, training loss: 0.008876427076756954 = 0.0021077252458781004 + 0.001 * 6.768701553344727
Epoch 1060, val loss: 1.5855375528335571
Epoch 1070, training loss: 0.008809939958155155 = 0.0020563912112265825 + 0.001 * 6.7535481452941895
Epoch 1070, val loss: 1.5904321670532227
Epoch 1080, training loss: 0.008752844296395779 = 0.002007267437875271 + 0.001 * 6.74557638168335
Epoch 1080, val loss: 1.5952295064926147
Epoch 1090, training loss: 0.008712081238627434 = 0.001960242632776499 + 0.001 * 6.751837730407715
Epoch 1090, val loss: 1.599957823753357
Epoch 1100, training loss: 0.00866849534213543 = 0.0019152116728946567 + 0.001 * 6.753283500671387
Epoch 1100, val loss: 1.6045989990234375
Epoch 1110, training loss: 0.00861471239477396 = 0.0018720596563071012 + 0.001 * 6.742652416229248
Epoch 1110, val loss: 1.6091364622116089
Epoch 1120, training loss: 0.008585156872868538 = 0.001830719760619104 + 0.001 * 6.75443696975708
Epoch 1120, val loss: 1.6136279106140137
Epoch 1130, training loss: 0.008536720648407936 = 0.0017910429742187262 + 0.001 * 6.745677471160889
Epoch 1130, val loss: 1.6180307865142822
Epoch 1140, training loss: 0.00849083997309208 = 0.0017529923934489489 + 0.001 * 6.737847328186035
Epoch 1140, val loss: 1.6223477125167847
Epoch 1150, training loss: 0.008461188524961472 = 0.0017164562596008182 + 0.001 * 6.744731903076172
Epoch 1150, val loss: 1.6265977621078491
Epoch 1160, training loss: 0.008420486934483051 = 0.0016813568072393537 + 0.001 * 6.739130020141602
Epoch 1160, val loss: 1.630768895149231
Epoch 1170, training loss: 0.008401116356253624 = 0.0016476414166390896 + 0.001 * 6.753474235534668
Epoch 1170, val loss: 1.6348679065704346
Epoch 1180, training loss: 0.008357222191989422 = 0.0016152221942320466 + 0.001 * 6.74199914932251
Epoch 1180, val loss: 1.6389236450195312
Epoch 1190, training loss: 0.008328143507242203 = 0.0015840294072404504 + 0.001 * 6.744113922119141
Epoch 1190, val loss: 1.6428865194320679
Epoch 1200, training loss: 0.008294075727462769 = 0.0015540285967290401 + 0.001 * 6.740046501159668
Epoch 1200, val loss: 1.646811604499817
Epoch 1210, training loss: 0.008321263827383518 = 0.0015251239528879523 + 0.001 * 6.796139240264893
Epoch 1210, val loss: 1.6506551504135132
Epoch 1220, training loss: 0.008238493464887142 = 0.0014973512152209878 + 0.001 * 6.7411417961120605
Epoch 1220, val loss: 1.6544723510742188
Epoch 1230, training loss: 0.008247512392699718 = 0.001470656949095428 + 0.001 * 6.776854991912842
Epoch 1230, val loss: 1.6581902503967285
Epoch 1240, training loss: 0.008173790760338306 = 0.0014449371956288815 + 0.001 * 6.728853225708008
Epoch 1240, val loss: 1.6618919372558594
Epoch 1250, training loss: 0.008151823654770851 = 0.0014201519079506397 + 0.001 * 6.7316718101501465
Epoch 1250, val loss: 1.6654555797576904
Epoch 1260, training loss: 0.008109292015433311 = 0.0013962647644802928 + 0.001 * 6.713027000427246
Epoch 1260, val loss: 1.6690293550491333
Epoch 1270, training loss: 0.008096132427453995 = 0.0013732200022786856 + 0.001 * 6.722911834716797
Epoch 1270, val loss: 1.6725010871887207
Epoch 1280, training loss: 0.008072689175605774 = 0.0013509799027815461 + 0.001 * 6.72170877456665
Epoch 1280, val loss: 1.6759244203567505
Epoch 1290, training loss: 0.008037218824028969 = 0.001329522579908371 + 0.001 * 6.707696437835693
Epoch 1290, val loss: 1.6792974472045898
Epoch 1300, training loss: 0.008062471635639668 = 0.0013088457053527236 + 0.001 * 6.753625869750977
Epoch 1300, val loss: 1.6826450824737549
Epoch 1310, training loss: 0.008037964813411236 = 0.001288861152715981 + 0.001 * 6.74910306930542
Epoch 1310, val loss: 1.6858059167861938
Epoch 1320, training loss: 0.008044890128076077 = 0.0012695675250142813 + 0.001 * 6.775321960449219
Epoch 1320, val loss: 1.6890465021133423
Epoch 1330, training loss: 0.007960128597915173 = 0.0012509208172559738 + 0.001 * 6.709207534790039
Epoch 1330, val loss: 1.6921614408493042
Epoch 1340, training loss: 0.007961857132613659 = 0.0012328926241025329 + 0.001 * 6.728964328765869
Epoch 1340, val loss: 1.6952457427978516
Epoch 1350, training loss: 0.007962560281157494 = 0.0012154292780905962 + 0.001 * 6.747130870819092
Epoch 1350, val loss: 1.6983057260513306
Epoch 1360, training loss: 0.007955481298267841 = 0.0011985140154138207 + 0.001 * 6.756967067718506
Epoch 1360, val loss: 1.7013124227523804
Epoch 1370, training loss: 0.00789982546120882 = 0.0011821449734270573 + 0.001 * 6.717679977416992
Epoch 1370, val loss: 1.7041923999786377
Epoch 1380, training loss: 0.007874689064919949 = 0.0011663129553198814 + 0.001 * 6.708375930786133
Epoch 1380, val loss: 1.7071356773376465
Epoch 1390, training loss: 0.007837616838514805 = 0.0011509914183989167 + 0.001 * 6.686624526977539
Epoch 1390, val loss: 1.7099462747573853
Epoch 1400, training loss: 0.007851740345358849 = 0.001136164995841682 + 0.001 * 6.715574741363525
Epoch 1400, val loss: 1.7127971649169922
Epoch 1410, training loss: 0.007845088839530945 = 0.0011218508007004857 + 0.001 * 6.723237991333008
Epoch 1410, val loss: 1.7155611515045166
Epoch 1420, training loss: 0.007830892689526081 = 0.0011079568648710847 + 0.001 * 6.722935199737549
Epoch 1420, val loss: 1.7182697057724
Epoch 1430, training loss: 0.007806756068021059 = 0.0010944786481559277 + 0.001 * 6.712276935577393
Epoch 1430, val loss: 1.7209134101867676
Epoch 1440, training loss: 0.007798065431416035 = 0.0010814035777002573 + 0.001 * 6.7166619300842285
Epoch 1440, val loss: 1.7235080003738403
Epoch 1450, training loss: 0.0078116185031831264 = 0.0010687188478186727 + 0.001 * 6.742899417877197
Epoch 1450, val loss: 1.7260863780975342
Epoch 1460, training loss: 0.007779211271554232 = 0.001056407461874187 + 0.001 * 6.722803592681885
Epoch 1460, val loss: 1.7286827564239502
Epoch 1470, training loss: 0.007752015255391598 = 0.0010444644140079618 + 0.001 * 6.707550525665283
Epoch 1470, val loss: 1.7311538457870483
Epoch 1480, training loss: 0.0077700200490653515 = 0.0010328796925023198 + 0.001 * 6.73714017868042
Epoch 1480, val loss: 1.7336602210998535
Epoch 1490, training loss: 0.00772943627089262 = 0.0010216131340712309 + 0.001 * 6.707823276519775
Epoch 1490, val loss: 1.736024022102356
Epoch 1500, training loss: 0.00773908756673336 = 0.0010106859263032675 + 0.001 * 6.7284016609191895
Epoch 1500, val loss: 1.7384830713272095
Epoch 1510, training loss: 0.00768914632499218 = 0.0010000909678637981 + 0.001 * 6.6890549659729
Epoch 1510, val loss: 1.7407881021499634
Epoch 1520, training loss: 0.007683411706238985 = 0.0009897815762087703 + 0.001 * 6.693629741668701
Epoch 1520, val loss: 1.7430940866470337
Epoch 1530, training loss: 0.007706302683800459 = 0.0009797782404348254 + 0.001 * 6.7265238761901855
Epoch 1530, val loss: 1.7453728914260864
Epoch 1540, training loss: 0.007672949228435755 = 0.0009700826485641301 + 0.001 * 6.702866077423096
Epoch 1540, val loss: 1.7476392984390259
Epoch 1550, training loss: 0.007681666407734156 = 0.0009606191306374967 + 0.001 * 6.7210469245910645
Epoch 1550, val loss: 1.7498300075531006
Epoch 1560, training loss: 0.007656378205865622 = 0.0009514490957371891 + 0.001 * 6.704928874969482
Epoch 1560, val loss: 1.7520376443862915
Epoch 1570, training loss: 0.0076058609411120415 = 0.0009425004245713353 + 0.001 * 6.663360118865967
Epoch 1570, val loss: 1.7541418075561523
Epoch 1580, training loss: 0.007599108852446079 = 0.0009337930241599679 + 0.001 * 6.665315628051758
Epoch 1580, val loss: 1.7562462091445923
Epoch 1590, training loss: 0.007588046137243509 = 0.0009253540192730725 + 0.001 * 6.662691593170166
Epoch 1590, val loss: 1.7583072185516357
Epoch 1600, training loss: 0.007579032797366381 = 0.0009170950506813824 + 0.001 * 6.661937236785889
Epoch 1600, val loss: 1.7603507041931152
Epoch 1610, training loss: 0.007578453980386257 = 0.0009090777602978051 + 0.001 * 6.669375896453857
Epoch 1610, val loss: 1.762380838394165
Epoch 1620, training loss: 0.007575524039566517 = 0.0009012685040943325 + 0.001 * 6.67425537109375
Epoch 1620, val loss: 1.7643406391143799
Epoch 1630, training loss: 0.0075698355212807655 = 0.0008936563972383738 + 0.001 * 6.676178455352783
Epoch 1630, val loss: 1.7662663459777832
Epoch 1640, training loss: 0.007534286472946405 = 0.0008862505783326924 + 0.001 * 6.648035526275635
Epoch 1640, val loss: 1.7681946754455566
Epoch 1650, training loss: 0.007533950265496969 = 0.0008790865540504456 + 0.001 * 6.654863357543945
Epoch 1650, val loss: 1.7700868844985962
Epoch 1660, training loss: 0.0075542498379945755 = 0.0008720424375496805 + 0.001 * 6.682207107543945
Epoch 1660, val loss: 1.7719664573669434
Epoch 1670, training loss: 0.007526477333158255 = 0.0008651767857372761 + 0.001 * 6.661300182342529
Epoch 1670, val loss: 1.7737942934036255
Epoch 1680, training loss: 0.007492678705602884 = 0.0008585272007621825 + 0.001 * 6.634151458740234
Epoch 1680, val loss: 1.7755975723266602
Epoch 1690, training loss: 0.007482620421797037 = 0.0008520626579411328 + 0.001 * 6.630557060241699
Epoch 1690, val loss: 1.7773513793945312
Epoch 1700, training loss: 0.007487159688025713 = 0.0008457247167825699 + 0.001 * 6.641434669494629
Epoch 1700, val loss: 1.7791117429733276
Epoch 1710, training loss: 0.007483229506760836 = 0.0008395281038247049 + 0.001 * 6.643701076507568
Epoch 1710, val loss: 1.7808167934417725
Epoch 1720, training loss: 0.007489911280572414 = 0.0008335139136761427 + 0.001 * 6.656397342681885
Epoch 1720, val loss: 1.782500982284546
Epoch 1730, training loss: 0.007498711347579956 = 0.0008275856380350888 + 0.001 * 6.671125411987305
Epoch 1730, val loss: 1.7841402292251587
Epoch 1740, training loss: 0.007478466723114252 = 0.0008218309958465397 + 0.001 * 6.656635284423828
Epoch 1740, val loss: 1.7858190536499023
Epoch 1750, training loss: 0.007462682202458382 = 0.0008161996374838054 + 0.001 * 6.646482467651367
Epoch 1750, val loss: 1.7874594926834106
Epoch 1760, training loss: 0.007477838546037674 = 0.0008107217727228999 + 0.001 * 6.667116165161133
Epoch 1760, val loss: 1.789056420326233
Epoch 1770, training loss: 0.007455086801201105 = 0.000805371964816004 + 0.001 * 6.649714469909668
Epoch 1770, val loss: 1.7906396389007568
Epoch 1780, training loss: 0.007463379297405481 = 0.0008000984089449048 + 0.001 * 6.663280487060547
Epoch 1780, val loss: 1.7921733856201172
Epoch 1790, training loss: 0.007425406947731972 = 0.000794961815699935 + 0.001 * 6.630444526672363
Epoch 1790, val loss: 1.7937424182891846
Epoch 1800, training loss: 0.007435004226863384 = 0.0007899691117927432 + 0.001 * 6.6450347900390625
Epoch 1800, val loss: 1.7953022718429565
Epoch 1810, training loss: 0.007417217828333378 = 0.0007850725669413805 + 0.001 * 6.632145404815674
Epoch 1810, val loss: 1.7968273162841797
Epoch 1820, training loss: 0.007427610456943512 = 0.000780287547968328 + 0.001 * 6.647322654724121
Epoch 1820, val loss: 1.7982407808303833
Epoch 1830, training loss: 0.0074475412257015705 = 0.0007755885599181056 + 0.001 * 6.671952247619629
Epoch 1830, val loss: 1.7997719049453735
Epoch 1840, training loss: 0.007399361580610275 = 0.0007710116915404797 + 0.001 * 6.628349781036377
Epoch 1840, val loss: 1.8011690378189087
Epoch 1850, training loss: 0.007373712491244078 = 0.000766527431551367 + 0.001 * 6.607184886932373
Epoch 1850, val loss: 1.8026398420333862
Epoch 1860, training loss: 0.007380747236311436 = 0.0007621613331139088 + 0.001 * 6.618585586547852
Epoch 1860, val loss: 1.8041082620620728
Epoch 1870, training loss: 0.007373140659183264 = 0.0007578733493573964 + 0.001 * 6.615266799926758
Epoch 1870, val loss: 1.8055278062820435
Epoch 1880, training loss: 0.007412109058350325 = 0.0007536368211731315 + 0.001 * 6.658471584320068
Epoch 1880, val loss: 1.806884527206421
Epoch 1890, training loss: 0.007408346515148878 = 0.0007494993624277413 + 0.001 * 6.658846855163574
Epoch 1890, val loss: 1.808264970779419
Epoch 1900, training loss: 0.007377942558377981 = 0.0007455042214132845 + 0.001 * 6.6324381828308105
Epoch 1900, val loss: 1.809667944908142
Epoch 1910, training loss: 0.007386242039501667 = 0.000741545925848186 + 0.001 * 6.64469575881958
Epoch 1910, val loss: 1.8110003471374512
Epoch 1920, training loss: 0.007361975498497486 = 0.0007376472931355238 + 0.001 * 6.624327659606934
Epoch 1920, val loss: 1.8123022317886353
Epoch 1930, training loss: 0.007368279155343771 = 0.0007338785217143595 + 0.001 * 6.634400367736816
Epoch 1930, val loss: 1.8136378526687622
Epoch 1940, training loss: 0.007333492394536734 = 0.0007301461300812662 + 0.001 * 6.60334587097168
Epoch 1940, val loss: 1.8149356842041016
Epoch 1950, training loss: 0.007360608316957951 = 0.0007265142048709095 + 0.001 * 6.634093761444092
Epoch 1950, val loss: 1.8162592649459839
Epoch 1960, training loss: 0.0073467339389026165 = 0.0007229283801279962 + 0.001 * 6.623805046081543
Epoch 1960, val loss: 1.8174841403961182
Epoch 1970, training loss: 0.007315059658139944 = 0.0007194209611043334 + 0.001 * 6.595638275146484
Epoch 1970, val loss: 1.8187799453735352
Epoch 1980, training loss: 0.007306199055165052 = 0.0007159853121265769 + 0.001 * 6.590213298797607
Epoch 1980, val loss: 1.8200823068618774
Epoch 1990, training loss: 0.007318683434277773 = 0.0007126150885596871 + 0.001 * 6.606068134307861
Epoch 1990, val loss: 1.8212835788726807
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.7889
Overall ASR: 1.0000
Flip ASR: 1.0000/225 nodes
The final ASR:0.82903, 0.15558, Accuracy:0.79259, 0.02584
#Attach Nodes:5
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11588])
remove edge: torch.Size([2, 9500])
updated graph: torch.Size([2, 10532])
./selected_nodes/Cora/Overall/seed462/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed345/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8222
Overall ASR: 0.9742
Flip ASR: 0.9689/225 nodes
./selected_nodes/Cora/Overall/seed29/nodes.txt
precent of left attach nodes: 1.000
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8333
Overall ASR: 0.9815
Flip ASR: 0.9778/225 nodes
The final ASR:0.97663, 0.00348, Accuracy:0.82963, 0.00524
Begin epxeriment: cont_weight: 0.001 epoch:2000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_bkd.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=2000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=0.001, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=2, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=0.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
#Attach Nodes:5
./selected_nodes/Cora/Overall/seed265/nodes.txt
precent of left attach nodes: 1.000
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 1.9678912162780762 = 1.9595173597335815 + 0.001 * 8.373885154724121
Epoch 0, val loss: 1.9640510082244873
Epoch 10, training loss: 1.9571850299835205 = 1.9488111734390259 + 0.001 * 8.373811721801758
Epoch 10, val loss: 1.9539259672164917
Epoch 20, training loss: 1.944024920463562 = 1.9356513023376465 + 0.001 * 8.373567581176758
Epoch 20, val loss: 1.9407838582992554
Epoch 30, training loss: 1.9255534410476685 = 1.9171804189682007 + 0.001 * 8.372982025146484
Epoch 30, val loss: 1.921815276145935
Epoch 40, training loss: 1.8982206583023071 = 1.8898491859436035 + 0.001 * 8.371459007263184
Epoch 40, val loss: 1.893575668334961
Epoch 50, training loss: 1.8596447706222534 = 1.8512779474258423 + 0.001 * 8.366791725158691
Epoch 50, val loss: 1.8548849821090698
Epoch 60, training loss: 1.8148369789123535 = 1.8064881563186646 + 0.001 * 8.348794937133789
Epoch 60, val loss: 1.8134883642196655
Epoch 70, training loss: 1.7753028869628906 = 1.76703941822052 + 0.001 * 8.26348876953125
Epoch 70, val loss: 1.780420184135437
Epoch 80, training loss: 1.7288072109222412 = 1.720937728881836 + 0.001 * 7.86944055557251
Epoch 80, val loss: 1.740842342376709
Epoch 90, training loss: 1.6640989780426025 = 1.6563979387283325 + 0.001 * 7.701095104217529
Epoch 90, val loss: 1.686820387840271
Epoch 100, training loss: 1.5772757530212402 = 1.5697072744369507 + 0.001 * 7.568438529968262
Epoch 100, val loss: 1.6155816316604614
Epoch 110, training loss: 1.473591923713684 = 1.4661709070205688 + 0.001 * 7.421001434326172
Epoch 110, val loss: 1.5298795700073242
Epoch 120, training loss: 1.367197036743164 = 1.3599413633346558 + 0.001 * 7.255665302276611
Epoch 120, val loss: 1.4443392753601074
Epoch 130, training loss: 1.2671797275543213 = 1.260014295578003 + 0.001 * 7.165382385253906
Epoch 130, val loss: 1.3658803701400757
Epoch 140, training loss: 1.1761599779129028 = 1.169076919555664 + 0.001 * 7.083003997802734
Epoch 140, val loss: 1.297670602798462
Epoch 150, training loss: 1.0946502685546875 = 1.0876046419143677 + 0.001 * 7.045571327209473
Epoch 150, val loss: 1.238125205039978
Epoch 160, training loss: 1.0216774940490723 = 1.014648675918579 + 0.001 * 7.028772354125977
Epoch 160, val loss: 1.1864850521087646
Epoch 170, training loss: 0.95534747838974 = 0.9483320713043213 + 0.001 * 7.015402317047119
Epoch 170, val loss: 1.1403207778930664
Epoch 180, training loss: 0.8928749561309814 = 0.885871410369873 + 0.001 * 7.003549575805664
Epoch 180, val loss: 1.0974185466766357
Epoch 190, training loss: 0.831238865852356 = 0.8242502808570862 + 0.001 * 6.988609313964844
Epoch 190, val loss: 1.0543485879898071
Epoch 200, training loss: 0.7680171728134155 = 0.7610476016998291 + 0.001 * 6.969560146331787
Epoch 200, val loss: 1.0095884799957275
Epoch 210, training loss: 0.7025822997093201 = 0.6956325173377991 + 0.001 * 6.949802398681641
Epoch 210, val loss: 0.9621413350105286
Epoch 220, training loss: 0.6359049677848816 = 0.6289626359939575 + 0.001 * 6.942352294921875
Epoch 220, val loss: 0.9127179980278015
Epoch 230, training loss: 0.5698305368423462 = 0.5628953576087952 + 0.001 * 6.935176849365234
Epoch 230, val loss: 0.8637412786483765
Epoch 240, training loss: 0.5058102011680603 = 0.4988774359226227 + 0.001 * 6.932760715484619
Epoch 240, val loss: 0.8180146217346191
Epoch 250, training loss: 0.444658488035202 = 0.43772614002227783 + 0.001 * 6.932343482971191
Epoch 250, val loss: 0.7776127457618713
Epoch 260, training loss: 0.38682129979133606 = 0.379889577627182 + 0.001 * 6.931725978851318
Epoch 260, val loss: 0.7434977293014526
Epoch 270, training loss: 0.3329383134841919 = 0.3260073959827423 + 0.001 * 6.93093204498291
Epoch 270, val loss: 0.7158267498016357
Epoch 280, training loss: 0.28404125571250916 = 0.2771112024784088 + 0.001 * 6.930054664611816
Epoch 280, val loss: 0.6950995326042175
Epoch 290, training loss: 0.24112586677074432 = 0.23419664800167084 + 0.001 * 6.929214954376221
Epoch 290, val loss: 0.6817510724067688
Epoch 300, training loss: 0.20471607148647308 = 0.19778767228126526 + 0.001 * 6.928403377532959
Epoch 300, val loss: 0.6756910085678101
Epoch 310, training loss: 0.17464181780815125 = 0.16771435737609863 + 0.001 * 6.92746114730835
Epoch 310, val loss: 0.6761743426322937
Epoch 320, training loss: 0.15012994408607483 = 0.14320406317710876 + 0.001 * 6.925873279571533
Epoch 320, val loss: 0.6818202137947083
Epoch 330, training loss: 0.1301983743906021 = 0.1232689768075943 + 0.001 * 6.9293928146362305
Epoch 330, val loss: 0.6913196444511414
Epoch 340, training loss: 0.11387836188077927 = 0.10695485770702362 + 0.001 * 6.923503398895264
Epoch 340, val loss: 0.7033592462539673
Epoch 350, training loss: 0.10039953887462616 = 0.09347842633724213 + 0.001 * 6.921109676361084
Epoch 350, val loss: 0.7170197367668152
Epoch 360, training loss: 0.08915071934461594 = 0.08223210275173187 + 0.001 * 6.918615818023682
Epoch 360, val loss: 0.7316032648086548
Epoch 370, training loss: 0.0796777755022049 = 0.07275661081075668 + 0.001 * 6.92116641998291
Epoch 370, val loss: 0.7466059327125549
Epoch 380, training loss: 0.07161656022071838 = 0.06470243632793427 + 0.001 * 6.914124488830566
Epoch 380, val loss: 0.7617779970169067
Epoch 390, training loss: 0.06471265107393265 = 0.05780170485377312 + 0.001 * 6.910947322845459
Epoch 390, val loss: 0.7769210934638977
Epoch 400, training loss: 0.0587562657892704 = 0.05184965580701828 + 0.001 * 6.906610012054443
Epoch 400, val loss: 0.791885256767273
Epoch 410, training loss: 0.05359543859958649 = 0.046688906848430634 + 0.001 * 6.906531810760498
Epoch 410, val loss: 0.8066096305847168
Epoch 420, training loss: 0.049103666096925735 = 0.04219374805688858 + 0.001 * 6.909916400909424
Epoch 420, val loss: 0.8209882378578186
Epoch 430, training loss: 0.04516401141881943 = 0.038262177258729935 + 0.001 * 6.9018354415893555
Epoch 430, val loss: 0.8350626230239868
Epoch 440, training loss: 0.04170294478535652 = 0.03481006622314453 + 0.001 * 6.89287805557251
Epoch 440, val loss: 0.8487673997879028
Epoch 450, training loss: 0.03865882009267807 = 0.03176870942115784 + 0.001 * 6.890108585357666
Epoch 450, val loss: 0.8620640635490417
Epoch 460, training loss: 0.035979412496089935 = 0.029080674052238464 + 0.001 * 6.898738861083984
Epoch 460, val loss: 0.8749719858169556
Epoch 470, training loss: 0.03358074277639389 = 0.026697427034378052 + 0.001 * 6.8833160400390625
Epoch 470, val loss: 0.8875323534011841
Epoch 480, training loss: 0.0314553827047348 = 0.024577995762228966 + 0.001 * 6.877385139465332
Epoch 480, val loss: 0.8997344374656677
Epoch 490, training loss: 0.029561452567577362 = 0.02268766425549984 + 0.001 * 6.873788833618164
Epoch 490, val loss: 0.9116050004959106
Epoch 500, training loss: 0.02787030301988125 = 0.020996976643800735 + 0.001 * 6.873326301574707
Epoch 500, val loss: 0.923092782497406
Epoch 510, training loss: 0.02634209766983986 = 0.019480450078845024 + 0.001 * 6.861648082733154
Epoch 510, val loss: 0.9342029094696045
Epoch 520, training loss: 0.024980442598462105 = 0.01811651699244976 + 0.001 * 6.863925457000732
Epoch 520, val loss: 0.9450162053108215
Epoch 530, training loss: 0.023758888244628906 = 0.016886837780475616 + 0.001 * 6.872050762176514
Epoch 530, val loss: 0.9555218815803528
Epoch 540, training loss: 0.022630801424384117 = 0.015775494277477264 + 0.001 * 6.855307102203369
Epoch 540, val loss: 0.9657359719276428
Epoch 550, training loss: 0.021618984639644623 = 0.014768443070352077 + 0.001 * 6.850541114807129
Epoch 550, val loss: 0.9756312370300293
Epoch 560, training loss: 0.020707910880446434 = 0.013853845186531544 + 0.001 * 6.85406494140625
Epoch 560, val loss: 0.9852625131607056
Epoch 570, training loss: 0.01987515017390251 = 0.013021327555179596 + 0.001 * 6.853821754455566
Epoch 570, val loss: 0.994624137878418
Epoch 580, training loss: 0.019107025116682053 = 0.012261804193258286 + 0.001 * 6.845221519470215
Epoch 580, val loss: 1.0037113428115845
Epoch 590, training loss: 0.01839957945048809 = 0.011567628011107445 + 0.001 * 6.831951141357422
Epoch 590, val loss: 1.0125603675842285
Epoch 600, training loss: 0.01776348054409027 = 0.010931486263871193 + 0.001 * 6.831995010375977
Epoch 600, val loss: 1.0211848020553589
Epoch 610, training loss: 0.017181657254695892 = 0.010347194038331509 + 0.001 * 6.834463119506836
Epoch 610, val loss: 1.0295699834823608
Epoch 620, training loss: 0.016642669215798378 = 0.009809660725295544 + 0.001 * 6.8330078125
Epoch 620, val loss: 1.0377352237701416
Epoch 630, training loss: 0.016129743307828903 = 0.009314057417213917 + 0.001 * 6.815684795379639
Epoch 630, val loss: 1.0456817150115967
Epoch 640, training loss: 0.015668366104364395 = 0.008856339380145073 + 0.001 * 6.812027454376221
Epoch 640, val loss: 1.053428292274475
Epoch 650, training loss: 0.015256192535161972 = 0.008432830683887005 + 0.001 * 6.823360919952393
Epoch 650, val loss: 1.0609650611877441
Epoch 660, training loss: 0.014846788719296455 = 0.008040196262300014 + 0.001 * 6.806591987609863
Epoch 660, val loss: 1.0683331489562988
Epoch 670, training loss: 0.01448383741080761 = 0.007675569970160723 + 0.001 * 6.808267593383789
Epoch 670, val loss: 1.0755048990249634
Epoch 680, training loss: 0.014145873486995697 = 0.007336415816098452 + 0.001 * 6.809457778930664
Epoch 680, val loss: 1.0825260877609253
Epoch 690, training loss: 0.01385251060128212 = 0.007020389661192894 + 0.001 * 6.832120418548584
Epoch 690, val loss: 1.0893675088882446
Epoch 700, training loss: 0.013535063713788986 = 0.006725499872118235 + 0.001 * 6.809563636779785
Epoch 700, val loss: 1.0960441827774048
Epoch 710, training loss: 0.013249806128442287 = 0.006449962966144085 + 0.001 * 6.799842834472656
Epoch 710, val loss: 1.1025818586349487
Epoch 720, training loss: 0.012978645041584969 = 0.00619208300486207 + 0.001 * 6.786561965942383
Epoch 720, val loss: 1.1089786291122437
Epoch 730, training loss: 0.012760777026414871 = 0.005950420629233122 + 0.001 * 6.810356616973877
Epoch 730, val loss: 1.11522376537323
Epoch 740, training loss: 0.01251162402331829 = 0.005723685957491398 + 0.001 * 6.787938117980957
Epoch 740, val loss: 1.1213059425354004
Epoch 750, training loss: 0.01230388879776001 = 0.005510712508112192 + 0.001 * 6.793176651000977
Epoch 750, val loss: 1.12729012966156
Epoch 760, training loss: 0.01211058534681797 = 0.005310365464538336 + 0.001 * 6.800219535827637
Epoch 760, val loss: 1.1331336498260498
Epoch 770, training loss: 0.011904386803507805 = 0.005121681373566389 + 0.001 * 6.782704830169678
Epoch 770, val loss: 1.1388424634933472
Epoch 780, training loss: 0.011724833399057388 = 0.0049437726847827435 + 0.001 * 6.781060695648193
Epoch 780, val loss: 1.1444560289382935
Epoch 790, training loss: 0.011568511836230755 = 0.004775693640112877 + 0.001 * 6.792818069458008
Epoch 790, val loss: 1.149950385093689
Epoch 800, training loss: 0.011406208388507366 = 0.004616512451320887 + 0.001 * 6.789695739746094
Epoch 800, val loss: 1.1553614139556885
Epoch 810, training loss: 0.01123726274818182 = 0.0044650547206401825 + 0.001 * 6.772207736968994
Epoch 810, val loss: 1.1607073545455933
Epoch 820, training loss: 0.011104416102170944 = 0.004319971892982721 + 0.001 * 6.784443378448486
Epoch 820, val loss: 1.1660511493682861
Epoch 830, training loss: 0.01093645952641964 = 0.004180232062935829 + 0.001 * 6.756227016448975
Epoch 830, val loss: 1.171412706375122
Epoch 840, training loss: 0.010817645117640495 = 0.0040450021624565125 + 0.001 * 6.772642612457275
Epoch 840, val loss: 1.1767746210098267
Epoch 850, training loss: 0.010677671991288662 = 0.003914103843271732 + 0.001 * 6.763567924499512
Epoch 850, val loss: 1.1821999549865723
Epoch 860, training loss: 0.010551451705396175 = 0.0037875769194215536 + 0.001 * 6.763874530792236
Epoch 860, val loss: 1.1876354217529297
Epoch 870, training loss: 0.010419214144349098 = 0.003665624186396599 + 0.001 * 6.753589630126953
Epoch 870, val loss: 1.1930818557739258
Epoch 880, training loss: 0.010292508639395237 = 0.003548312932252884 + 0.001 * 6.744195461273193
Epoch 880, val loss: 1.1985201835632324
Epoch 890, training loss: 0.010203538462519646 = 0.003435766091570258 + 0.001 * 6.7677717208862305
Epoch 890, val loss: 1.2039334774017334
Epoch 900, training loss: 0.010072125121951103 = 0.003327929647639394 + 0.001 * 6.744195461273193
Epoch 900, val loss: 1.2093275785446167
Epoch 910, training loss: 0.009972469881176949 = 0.003224819665774703 + 0.001 * 6.747650146484375
Epoch 910, val loss: 1.214666485786438
Epoch 920, training loss: 0.00987061858177185 = 0.0031263662967830896 + 0.001 * 6.7442522048950195
Epoch 920, val loss: 1.2199312448501587
Epoch 930, training loss: 0.009779328480362892 = 0.0030323988758027554 + 0.001 * 6.746929168701172
Epoch 930, val loss: 1.2251337766647339
Epoch 940, training loss: 0.009723920375108719 = 0.0029427711851894855 + 0.001 * 6.781149387359619
Epoch 940, val loss: 1.230288028717041
Epoch 950, training loss: 0.009611164219677448 = 0.0028572625014930964 + 0.001 * 6.75390100479126
Epoch 950, val loss: 1.2353272438049316
Epoch 960, training loss: 0.009507056325674057 = 0.002775701228529215 + 0.001 * 6.731354713439941
Epoch 960, val loss: 1.2403186559677124
Epoch 970, training loss: 0.009431328624486923 = 0.0026979129761457443 + 0.001 * 6.733415126800537
Epoch 970, val loss: 1.2452335357666016
Epoch 980, training loss: 0.009368986822664738 = 0.0026237182319164276 + 0.001 * 6.74526834487915
Epoch 980, val loss: 1.25005042552948
Epoch 990, training loss: 0.009309791028499603 = 0.0025528911501169205 + 0.001 * 6.756899356842041
Epoch 990, val loss: 1.2547657489776611
Epoch 1000, training loss: 0.00922708585858345 = 0.0024852962233126163 + 0.001 * 6.741788864135742
Epoch 1000, val loss: 1.2594263553619385
Epoch 1010, training loss: 0.009139171801507473 = 0.0024207145906984806 + 0.001 * 6.718456745147705
Epoch 1010, val loss: 1.2640047073364258
Epoch 1020, training loss: 0.00907544419169426 = 0.00235901796258986 + 0.001 * 6.716425895690918
Epoch 1020, val loss: 1.2685201168060303
Epoch 1030, training loss: 0.009007856249809265 = 0.002300066640600562 + 0.001 * 6.707788944244385
Epoch 1030, val loss: 1.2729369401931763
Epoch 1040, training loss: 0.008981139399111271 = 0.0022436778526753187 + 0.001 * 6.737461566925049
Epoch 1040, val loss: 1.2772941589355469
Epoch 1050, training loss: 0.0089185805991292 = 0.00218973564915359 + 0.001 * 6.72884464263916
Epoch 1050, val loss: 1.2815338373184204
Epoch 1060, training loss: 0.008844808675348759 = 0.0021381054539233446 + 0.001 * 6.706703186035156
Epoch 1060, val loss: 1.2857270240783691
Epoch 1070, training loss: 0.00879171583801508 = 0.0020886771380901337 + 0.001 * 6.703038215637207
Epoch 1070, val loss: 1.2898626327514648
Epoch 1080, training loss: 0.008745463564991951 = 0.002041333355009556 + 0.001 * 6.704129695892334
Epoch 1080, val loss: 1.2939181327819824
Epoch 1090, training loss: 0.008725649677217007 = 0.0019959453493356705 + 0.001 * 6.729703903198242
Epoch 1090, val loss: 1.2978794574737549
Epoch 1100, training loss: 0.00867505744099617 = 0.0019524326780810952 + 0.001 * 6.7226243019104
Epoch 1100, val loss: 1.3017988204956055
Epoch 1110, training loss: 0.008631694130599499 = 0.001910662860609591 + 0.001 * 6.721031188964844
Epoch 1110, val loss: 1.3056401014328003
Epoch 1120, training loss: 0.008557084016501904 = 0.0018705670954659581 + 0.001 * 6.686516284942627
Epoch 1120, val loss: 1.3094265460968018
Epoch 1130, training loss: 0.008530314080417156 = 0.0018320624949410558 + 0.001 * 6.698251247406006
Epoch 1130, val loss: 1.313101053237915
Epoch 1140, training loss: 0.008487926796078682 = 0.0017950613982975483 + 0.001 * 6.692865371704102
Epoch 1140, val loss: 1.3167154788970947
Epoch 1150, training loss: 0.008488793857395649 = 0.0017594832461327314 + 0.001 * 6.729310512542725
Epoch 1150, val loss: 1.3202848434448242
Epoch 1160, training loss: 0.008416980504989624 = 0.0017252977704629302 + 0.001 * 6.691682815551758
Epoch 1160, val loss: 1.3237707614898682
Epoch 1170, training loss: 0.008368553593754768 = 0.0016923911171033978 + 0.001 * 6.676162242889404
Epoch 1170, val loss: 1.3272464275360107
Epoch 1180, training loss: 0.008341578766703606 = 0.0016607181169092655 + 0.001 * 6.680860996246338
Epoch 1180, val loss: 1.3306171894073486
Epoch 1190, training loss: 0.00831005722284317 = 0.0016302024014294147 + 0.001 * 6.679854869842529
Epoch 1190, val loss: 1.333924412727356
Epoch 1200, training loss: 0.008303881622850895 = 0.0016008081147447228 + 0.001 * 6.703073501586914
Epoch 1200, val loss: 1.337195873260498
Epoch 1210, training loss: 0.00828109122812748 = 0.0015724750701338053 + 0.001 * 6.708615779876709
Epoch 1210, val loss: 1.34040105342865
Epoch 1220, training loss: 0.008276902139186859 = 0.0015451693907380104 + 0.001 * 6.731732368469238
Epoch 1220, val loss: 1.3435256481170654
Epoch 1230, training loss: 0.00819641537964344 = 0.001518813893198967 + 0.001 * 6.6776018142700195
Epoch 1230, val loss: 1.3465908765792847
Epoch 1240, training loss: 0.008164605125784874 = 0.0014933892525732517 + 0.001 * 6.671215057373047
Epoch 1240, val loss: 1.3496451377868652
Epoch 1250, training loss: 0.008134592324495316 = 0.0014688316732645035 + 0.001 * 6.6657609939575195
Epoch 1250, val loss: 1.3526291847229004
Epoch 1260, training loss: 0.008107362315058708 = 0.0014451311435550451 + 0.001 * 6.662230491638184
Epoch 1260, val loss: 1.3555721044540405
Epoch 1270, training loss: 0.008093031123280525 = 0.00142222223803401 + 0.001 * 6.6708083152771
Epoch 1270, val loss: 1.3584074974060059
Epoch 1280, training loss: 0.008061804808676243 = 0.0014001064701005816 + 0.001 * 6.661698341369629
Epoch 1280, val loss: 1.3612964153289795
Epoch 1290, training loss: 0.008052624762058258 = 0.00137871946208179 + 0.001 * 6.673904895782471
Epoch 1290, val loss: 1.3640820980072021
Epoch 1300, training loss: 0.008065314963459969 = 0.001358061213977635 + 0.001 * 6.7072529792785645
Epoch 1300, val loss: 1.3668292760849
Epoch 1310, training loss: 0.008001898415386677 = 0.00133805931545794 + 0.001 * 6.6638383865356445
Epoch 1310, val loss: 1.3696130514144897
Epoch 1320, training loss: 0.007985609583556652 = 0.0013187264557927847 + 0.001 * 6.666882514953613
Epoch 1320, val loss: 1.3721883296966553
Epoch 1330, training loss: 0.007943673059344292 = 0.001299996511079371 + 0.001 * 6.643675804138184
Epoch 1330, val loss: 1.3749067783355713
Epoch 1340, training loss: 0.007959091104567051 = 0.0012818414252251387 + 0.001 * 6.677248954772949
Epoch 1340, val loss: 1.377526044845581
Epoch 1350, training loss: 0.007920769974589348 = 0.0012642777292057872 + 0.001 * 6.656492233276367
Epoch 1350, val loss: 1.3800405263900757
Epoch 1360, training loss: 0.007913974113762379 = 0.0012472454691305757 + 0.001 * 6.666728496551514
Epoch 1360, val loss: 1.3825522661209106
Epoch 1370, training loss: 0.00790388323366642 = 0.0012307371944189072 + 0.001 * 6.673145771026611
Epoch 1370, val loss: 1.385023832321167
Epoch 1380, training loss: 0.00785023532807827 = 0.0012147168163210154 + 0.001 * 6.6355180740356445
Epoch 1380, val loss: 1.3874202966690063
Epoch 1390, training loss: 0.007848898880183697 = 0.0011991907376796007 + 0.001 * 6.649708271026611
Epoch 1390, val loss: 1.3898003101348877
Epoch 1400, training loss: 0.007836899720132351 = 0.0011841312516480684 + 0.001 * 6.652768135070801
Epoch 1400, val loss: 1.3921282291412354
Epoch 1410, training loss: 0.007853999733924866 = 0.0011695139110088348 + 0.001 * 6.68448543548584
Epoch 1410, val loss: 1.3944573402404785
Epoch 1420, training loss: 0.0078032235614955425 = 0.0011553209042176604 + 0.001 * 6.647902011871338
Epoch 1420, val loss: 1.3967218399047852
Epoch 1430, training loss: 0.007780083920806646 = 0.0011415419867262244 + 0.001 * 6.6385416984558105
Epoch 1430, val loss: 1.3989630937576294
Epoch 1440, training loss: 0.007763823959976435 = 0.0011281498009338975 + 0.001 * 6.635673999786377
Epoch 1440, val loss: 1.4011645317077637
Epoch 1450, training loss: 0.00775484275072813 = 0.0011151438811793923 + 0.001 * 6.639698505401611
Epoch 1450, val loss: 1.403340220451355
Epoch 1460, training loss: 0.007767828647047281 = 0.0011024964042007923 + 0.001 * 6.665331840515137
Epoch 1460, val loss: 1.405463695526123
Epoch 1470, training loss: 0.0077458638697862625 = 0.0010902280919253826 + 0.001 * 6.655635356903076
Epoch 1470, val loss: 1.4075392484664917
Epoch 1480, training loss: 0.007702627684921026 = 0.001078275148756802 + 0.001 * 6.62435245513916
Epoch 1480, val loss: 1.409575343132019
Epoch 1490, training loss: 0.007684581913053989 = 0.0010666660964488983 + 0.001 * 6.617915630340576
Epoch 1490, val loss: 1.4116536378860474
Epoch 1500, training loss: 0.0077006020583212376 = 0.0010553691536188126 + 0.001 * 6.645232677459717
Epoch 1500, val loss: 1.413644552230835
Epoch 1510, training loss: 0.0077110519632697105 = 0.001044376054778695 + 0.001 * 6.666675567626953
Epoch 1510, val loss: 1.4156302213668823
Epoch 1520, training loss: 0.00764109194278717 = 0.0010336906416341662 + 0.001 * 6.607400894165039
Epoch 1520, val loss: 1.4175454378128052
Epoch 1530, training loss: 0.007666906341910362 = 0.0010232921922579408 + 0.001 * 6.643613815307617
Epoch 1530, val loss: 1.4194355010986328
Epoch 1540, training loss: 0.007652829401195049 = 0.0010131590534001589 + 0.001 * 6.639670372009277
Epoch 1540, val loss: 1.421298623085022
Epoch 1550, training loss: 0.007669765502214432 = 0.001003281562589109 + 0.001 * 6.666483402252197
Epoch 1550, val loss: 1.4231290817260742
Epoch 1560, training loss: 0.007616905495524406 = 0.0009936793940141797 + 0.001 * 6.623225688934326
Epoch 1560, val loss: 1.4249889850616455
Epoch 1570, training loss: 0.007583468221127987 = 0.0009843007428571582 + 0.001 * 6.5991668701171875
Epoch 1570, val loss: 1.4267598390579224
Epoch 1580, training loss: 0.007571008987724781 = 0.0009751594043336809 + 0.001 * 6.59584903717041
Epoch 1580, val loss: 1.4285314083099365
Epoch 1590, training loss: 0.007574773859232664 = 0.000966247171163559 + 0.001 * 6.608526229858398
Epoch 1590, val loss: 1.4302603006362915
Epoch 1600, training loss: 0.0075809769332408905 = 0.0009575590956956148 + 0.001 * 6.623417854309082
Epoch 1600, val loss: 1.4319101572036743
Epoch 1610, training loss: 0.007563749328255653 = 0.0009490899974480271 + 0.001 * 6.614659309387207
Epoch 1610, val loss: 1.4336059093475342
Epoch 1620, training loss: 0.007570113055408001 = 0.0009408260229974985 + 0.001 * 6.629286289215088
Epoch 1620, val loss: 1.4352608919143677
Epoch 1630, training loss: 0.007599185686558485 = 0.0009327579173259437 + 0.001 * 6.6664276123046875
Epoch 1630, val loss: 1.4369066953659058
Epoch 1640, training loss: 0.007540127262473106 = 0.0009248906280845404 + 0.001 * 6.615236759185791
Epoch 1640, val loss: 1.4384692907333374
Epoch 1650, training loss: 0.007513673044741154 = 0.0009172185673378408 + 0.001 * 6.59645414352417
Epoch 1650, val loss: 1.440111517906189
Epoch 1660, training loss: 0.007532600313425064 = 0.0009097259026020765 + 0.001 * 6.622873783111572
Epoch 1660, val loss: 1.4416747093200684
Epoch 1670, training loss: 0.007580288220196962 = 0.0009024114697240293 + 0.001 * 6.6778764724731445
Epoch 1670, val loss: 1.4432107210159302
Epoch 1680, training loss: 0.007494133431464434 = 0.0008952696225605905 + 0.001 * 6.59886360168457
Epoch 1680, val loss: 1.4446687698364258
Epoch 1690, training loss: 0.007482589688152075 = 0.0008882951806299388 + 0.001 * 6.594294548034668
Epoch 1690, val loss: 1.4462178945541382
Epoch 1700, training loss: 0.00746943848207593 = 0.0008814706816338003 + 0.001 * 6.587967395782471
Epoch 1700, val loss: 1.4476655721664429
Epoch 1710, training loss: 0.007504605688154697 = 0.0008747978135943413 + 0.001 * 6.629807472229004
Epoch 1710, val loss: 1.44907546043396
Epoch 1720, training loss: 0.007476105820387602 = 0.0008682836196385324 + 0.001 * 6.607821941375732
Epoch 1720, val loss: 1.4505374431610107
Epoch 1730, training loss: 0.007470587268471718 = 0.0008619208820164204 + 0.001 * 6.608665943145752
Epoch 1730, val loss: 1.4519622325897217
Epoch 1740, training loss: 0.00744258426129818 = 0.0008556905086152256 + 0.001 * 6.586893558502197
Epoch 1740, val loss: 1.4533348083496094
Epoch 1750, training loss: 0.0074231792241334915 = 0.0008495852234773338 + 0.001 * 6.573594093322754
Epoch 1750, val loss: 1.4546947479248047
Epoch 1760, training loss: 0.007422137539833784 = 0.0008436263306066394 + 0.001 * 6.578510761260986
Epoch 1760, val loss: 1.4560390710830688
Epoch 1770, training loss: 0.007478928659111261 = 0.0008377839694730937 + 0.001 * 6.641144752502441
Epoch 1770, val loss: 1.4573572874069214
Epoch 1780, training loss: 0.007442991249263287 = 0.0008320726919919252 + 0.001 * 6.6109185218811035
Epoch 1780, val loss: 1.4586632251739502
Epoch 1790, training loss: 0.007417168002575636 = 0.0008264993666671216 + 0.001 * 6.590668201446533
Epoch 1790, val loss: 1.45997154712677
Epoch 1800, training loss: 0.007409727666527033 = 0.0008210311061702669 + 0.001 * 6.588696002960205
Epoch 1800, val loss: 1.461252212524414
Epoch 1810, training loss: 0.007410072721540928 = 0.000815682637039572 + 0.001 * 6.594389915466309
Epoch 1810, val loss: 1.4625104665756226
Epoch 1820, training loss: 0.007401059381663799 = 0.0008104556472972035 + 0.001 * 6.590603351593018
Epoch 1820, val loss: 1.4637296199798584
Epoch 1830, training loss: 0.007372270803898573 = 0.0008053149213083088 + 0.001 * 6.56695556640625
Epoch 1830, val loss: 1.4649776220321655
Epoch 1840, training loss: 0.0073964763432741165 = 0.0008003001566976309 + 0.001 * 6.5961761474609375
Epoch 1840, val loss: 1.4662022590637207
Epoch 1850, training loss: 0.007401194889098406 = 0.0007953809690661728 + 0.001 * 6.605813503265381
Epoch 1850, val loss: 1.4673482179641724
Epoch 1860, training loss: 0.007407324854284525 = 0.0007905663806013763 + 0.001 * 6.616758346557617
Epoch 1860, val loss: 1.468568205833435
Epoch 1870, training loss: 0.00736362487077713 = 0.0007858567405492067 + 0.001 * 6.577767848968506
Epoch 1870, val loss: 1.469701886177063
Epoch 1880, training loss: 0.00736526632681489 = 0.0007812396506778896 + 0.001 * 6.584026336669922
Epoch 1880, val loss: 1.4708484411239624
Epoch 1890, training loss: 0.007382713723927736 = 0.0007766999187879264 + 0.001 * 6.606013774871826
Epoch 1890, val loss: 1.4719855785369873
Epoch 1900, training loss: 0.007342211902141571 = 0.0007722461014054716 + 0.001 * 6.569965362548828
Epoch 1900, val loss: 1.4730682373046875
Epoch 1910, training loss: 0.007326873950660229 = 0.0007678808178752661 + 0.001 * 6.558992862701416
Epoch 1910, val loss: 1.474207878112793
Epoch 1920, training loss: 0.007356868591159582 = 0.000763595278840512 + 0.001 * 6.593273162841797
Epoch 1920, val loss: 1.4752757549285889
Epoch 1930, training loss: 0.007399521768093109 = 0.0007594081107527018 + 0.001 * 6.64011287689209
Epoch 1930, val loss: 1.4764388799667358
Epoch 1940, training loss: 0.007348008919507265 = 0.0007552894530817866 + 0.001 * 6.592719078063965
Epoch 1940, val loss: 1.477422833442688
Epoch 1950, training loss: 0.007307162508368492 = 0.0007512511219829321 + 0.001 * 6.555911064147949
Epoch 1950, val loss: 1.4785094261169434
Epoch 1960, training loss: 0.007294646464288235 = 0.0007472693687304854 + 0.001 * 6.54737663269043
Epoch 1960, val loss: 1.4795907735824585
Epoch 1970, training loss: 0.007327074650675058 = 0.0007433647406287491 + 0.001 * 6.583709239959717
Epoch 1970, val loss: 1.480677604675293
Epoch 1980, training loss: 0.007316360250115395 = 0.0007395347347483039 + 0.001 * 6.576825141906738
Epoch 1980, val loss: 1.4815762042999268
Epoch 1990, training loss: 0.007284256163984537 = 0.0007357742288149893 + 0.001 * 6.548481464385986
Epoch 1990, val loss: 1.4826382398605347
=== picking the best model according to the performance on validation ===
target class rate on Vs: 1.0000
accuracy on clean test nodes: 0.8111
Overall ASR: 0.5793
Flip ASR: 0.4933/225 nodes
./selected_nodes/Cora/Overall/seed125/nodes.txt
precent of left attach nodes: 1.000
=== training gcn model ===
Epoch 0, training loss: 1.9683829545974731 = 1.960009217262268 + 0.001 * 8.373773574829102
Epoch 0, val loss: 1.9543994665145874
Epoch 10, training loss: 1.9568532705307007 = 1.9484800100326538 + 0.001 * 8.37331771850586
Epoch 10, val loss: 1.9413275718688965
Epoch 20, training loss: 1.942196249961853 = 1.933823823928833 + 0.001 * 8.372445106506348
Epoch 20, val loss: 1.9233009815216064
Epoch 30, training loss: 1.9221172332763672 = 1.9137461185455322 + 0.001 * 8.371068000793457
Epoch 30, val loss: 1.8979040384292603
Epoch 40, training loss: 1.8951297998428345 = 1.886760950088501 + 0.001 * 8.368867874145508
Epoch 40, val loss: 1.8653966188430786
Epoch 50, training loss: 1.8600239753723145 = 1.8516596555709839 + 0.001 * 8.364274024963379
Epoch 50, val loss: 1.8274677991867065
Epoch 60, training loss: 1.816848874092102 = 1.8085004091262817 + 0.001 * 8.348432540893555
Epoch 60, val loss: 1.7865829467773438
Epoch 70, training loss: 1.7730292081832886 = 1.7647578716278076 + 0.001 * 8.271317481994629
Epoch 70, val loss: 1.7506424188613892
Epoch 80, training loss: 1.7263747453689575 = 1.718501091003418 + 0.001 * 7.873693943023682
Epoch 80, val loss: 1.7136797904968262
Epoch 90, training loss: 1.6622065305709839 = 1.6545376777648926 + 0.001 * 7.668891429901123
Epoch 90, val loss: 1.6603004932403564
Epoch 100, training loss: 1.5754005908966064 = 1.5678536891937256 + 0.001 * 7.54690408706665
Epoch 100, val loss: 1.5878653526306152
Epoch 110, training loss: 1.47365140914917 = 1.46615731716156 + 0.001 * 7.494093418121338
Epoch 110, val loss: 1.5055909156799316
Epoch 120, training loss: 1.3712856769561768 = 1.3638871908187866 + 0.001 * 7.398534774780273
Epoch 120, val loss: 1.4276918172836304
Epoch 130, training loss: 1.277290940284729 = 1.2699891328811646 + 0.001 * 7.301802635192871
Epoch 130, val loss: 1.3632500171661377
Epoch 140, training loss: 1.193657398223877 = 1.1864668130874634 + 0.001 * 7.190596580505371
Epoch 140, val loss: 1.3123050928115845
Epoch 150, training loss: 1.1171141862869263 = 1.1099891662597656 + 0.001 * 7.125068664550781
Epoch 150, val loss: 1.268357753753662
Epoch 160, training loss: 1.0427913665771484 = 1.035701870918274 + 0.001 * 7.089507579803467
Epoch 160, val loss: 1.2268353700637817
Epoch 170, training loss: 0.9679233431816101 = 0.9608548879623413 + 0.001 * 7.068467140197754
Epoch 170, val loss: 1.1852420568466187
Epoch 180, training loss: 0.8919273614883423 = 0.8848683834075928 + 0.001 * 7.058995723724365
Epoch 180, val loss: 1.1418836116790771
Epoch 190, training loss: 0.8155379295349121 = 0.808487594127655 + 0.001 * 7.050325393676758
Epoch 190, val loss: 1.0969443321228027
Epoch 200, training loss: 0.7407788038253784 = 0.7337377071380615 + 0.001 * 7.041095733642578
Epoch 200, val loss: 1.0511267185211182
Epoch 210, training loss: 0.670229971408844 = 0.6631988883018494 + 0.001 * 7.031081199645996
Epoch 210, val loss: 1.007326364517212
Epoch 220, training loss: 0.6060786247253418 = 0.5990596413612366 + 0.001 * 7.0190110206604
Epoch 220, val loss: 0.968312680721283
Epoch 230, training loss: 0.548765242099762 = 0.5417628288269043 + 0.001 * 7.002414226531982
Epoch 230, val loss: 0.9348861575126648
Epoch 240, training loss: 0.49742594361305237 = 0.49044498801231384 + 0.001 * 6.980954170227051
Epoch 240, val loss: 0.9067459106445312
Epoch 250, training loss: 0.45062029361724854 = 0.44365155696868896 + 0.001 * 6.9687299728393555
Epoch 250, val loss: 0.8825874328613281
Epoch 260, training loss: 0.40682855248451233 = 0.3998749852180481 + 0.001 * 6.9535651206970215
Epoch 260, val loss: 0.861244797706604
Epoch 270, training loss: 0.3652508854866028 = 0.3583034574985504 + 0.001 * 6.947423458099365
Epoch 270, val loss: 0.8422372937202454
Epoch 280, training loss: 0.325422465801239 = 0.3184773027896881 + 0.001 * 6.945170879364014
Epoch 280, val loss: 0.8251780867576599
Epoch 290, training loss: 0.2875686585903168 = 0.2806253433227539 + 0.001 * 6.943308353424072
Epoch 290, val loss: 0.8103150129318237
Epoch 300, training loss: 0.2522319555282593 = 0.24529026448726654 + 0.001 * 6.941681385040283
Epoch 300, val loss: 0.7978968024253845
Epoch 310, training loss: 0.21993787586688995 = 0.21299783885478973 + 0.001 * 6.940036296844482
Epoch 310, val loss: 0.7882537841796875
Epoch 320, training loss: 0.19115588068962097 = 0.1842174381017685 + 0.001 * 6.938435077667236
Epoch 320, val loss: 0.7816592454910278
Epoch 330, training loss: 0.1661275327205658 = 0.15919038653373718 + 0.001 * 6.937148571014404
Epoch 330, val loss: 0.7782326936721802
Epoch 340, training loss: 0.14478805661201477 = 0.1378512978553772 + 0.001 * 6.936759948730469
Epoch 340, val loss: 0.7777271270751953
Epoch 350, training loss: 0.12670736014842987 = 0.11977295577526093 + 0.001 * 6.934399604797363
Epoch 350, val loss: 0.780029833316803
Epoch 360, training loss: 0.11145193874835968 = 0.10451914370059967 + 0.001 * 6.932797431945801
Epoch 360, val loss: 0.784440279006958
Epoch 370, training loss: 0.09855148941278458 = 0.09161821007728577 + 0.001 * 6.9332780838012695
Epoch 370, val loss: 0.790810763835907
Epoch 380, training loss: 0.08759905397891998 = 0.08066991716623306 + 0.001 * 6.929133415222168
Epoch 380, val loss: 0.7986740469932556
Epoch 390, training loss: 0.07830291986465454 = 0.07137468457221985 + 0.001 * 6.928235054016113
Epoch 390, val loss: 0.8077920079231262
Epoch 400, training loss: 0.07032570242881775 = 0.06339976191520691 + 0.001 * 6.925941467285156
Epoch 400, val loss: 0.8178125023841858
Epoch 410, training loss: 0.06353094428777695 = 0.05660831183195114 + 0.001 * 6.922633647918701
Epoch 410, val loss: 0.8284300565719604
Epoch 420, training loss: 0.05769168585538864 = 0.05077066272497177 + 0.001 * 6.921023845672607
Epoch 420, val loss: 0.8394604325294495
Epoch 430, training loss: 0.052629753947257996 = 0.04570890963077545 + 0.001 * 6.920843601226807
Epoch 430, val loss: 0.8507195115089417
Epoch 440, training loss: 0.04821719229221344 = 0.041304461658000946 + 0.001 * 6.912729263305664
Epoch 440, val loss: 0.8619979023933411
Epoch 450, training loss: 0.04437693580985069 = 0.037464506924152374 + 0.001 * 6.912429332733154
Epoch 450, val loss: 0.8732145428657532
Epoch 460, training loss: 0.04100985452532768 = 0.03410171717405319 + 0.001 * 6.908135890960693
Epoch 460, val loss: 0.8843836188316345
Epoch 470, training loss: 0.038028161972761154 = 0.03111204132437706 + 0.001 * 6.916121006011963
Epoch 470, val loss: 0.8952557444572449
Epoch 480, training loss: 0.03542419523000717 = 0.02851780317723751 + 0.001 * 6.906392574310303
Epoch 480, val loss: 0.9060819149017334
Epoch 490, training loss: 0.03306052088737488 = 0.02615666575729847 + 0.001 * 6.903853416442871
Epoch 490, val loss: 0.9168352484703064
Epoch 500, training loss: 0.030886951833963394 = 0.02398436889052391 + 0.001 * 6.90258264541626
Epoch 500, val loss: 0.9274578094482422
Epoch 510, training loss: 0.029009908437728882 = 0.022106958553195 + 0.001 * 6.902949333190918
Epoch 510, val loss: 0.9380207061767578
Epoch 520, training loss: 0.027378041297197342 = 0.020473690703511238 + 0.001 * 6.904350757598877
Epoch 520, val loss: 0.9479663372039795
Epoch 530, training loss: 0.025921542197465897 = 0.019019778817892075 + 0.001 * 6.901763916015625
Epoch 530, val loss: 0.957674503326416
Epoch 540, training loss: 0.024612605571746826 = 0.01771041750907898 + 0.001 * 6.902187824249268
Epoch 540, val loss: 0.967145562171936
Epoch 550, training loss: 0.023427683860063553 = 0.016529131680727005 + 0.001 * 6.898551940917969
Epoch 550, val loss: 0.9764502644538879
Epoch 560, training loss: 0.02235836535692215 = 0.015461130067706108 + 0.001 * 6.897234916687012
Epoch 560, val loss: 0.9856246113777161
Epoch 570, training loss: 0.021402059122920036 = 0.014492740854620934 + 0.001 * 6.909317970275879
Epoch 570, val loss: 0.9945327043533325
Epoch 580, training loss: 0.02050771936774254 = 0.01361184474080801 + 0.001 * 6.895874977111816
Epoch 580, val loss: 1.0032304525375366
Epoch 590, training loss: 0.019703175872564316 = 0.012808745726943016 + 0.001 * 6.894430160522461
Epoch 590, val loss: 1.0116904973983765
Epoch 600, training loss: 0.01896647736430168 = 0.012074392288923264 + 0.001 * 6.892085075378418
Epoch 600, val loss: 1.0199793577194214
Epoch 610, training loss: 0.018297970294952393 = 0.011401486583054066 + 0.001 * 6.896483898162842
Epoch 610, val loss: 1.028051733970642
Epoch 620, training loss: 0.01767715811729431 = 0.010783963836729527 + 0.001 * 6.893194198608398
Epoch 620, val loss: 1.0359519720077515
Epoch 630, training loss: 0.017107386142015457 = 0.010215887799859047 + 0.001 * 6.89149808883667
Epoch 630, val loss: 1.0436701774597168
Epoch 640, training loss: 0.01657896675169468 = 0.009692479856312275 + 0.001 * 6.886487007141113
Epoch 640, val loss: 1.051244854927063
Epoch 650, training loss: 0.01609746739268303 = 0.009208516217768192 + 0.001 * 6.888951778411865
Epoch 650, val loss: 1.0586005449295044
Epoch 660, training loss: 0.01564141735434532 = 0.008753311820328236 + 0.001 * 6.8881049156188965
Epoch 660, val loss: 1.0657933950424194
Epoch 670, training loss: 0.015204327180981636 = 0.008321975357830524 + 0.001 * 6.882351875305176
Epoch 670, val loss: 1.0728354454040527
Epoch 680, training loss: 0.014841698110103607 = 0.0079244589433074 + 0.001 * 6.917238712310791
Epoch 680, val loss: 1.0797462463378906
Epoch 690, training loss: 0.014444362372159958 = 0.007557549513876438 + 0.001 * 6.886812686920166
Epoch 690, val loss: 1.0864982604980469
Epoch 700, training loss: 0.014092333614826202 = 0.007214691024273634 + 0.001 * 6.877641677856445
Epoch 700, val loss: 1.0931193828582764
Epoch 710, training loss: 0.01377140823751688 = 0.006895377766340971 + 0.001 * 6.876029968261719
Epoch 710, val loss: 1.0996156930923462
Epoch 720, training loss: 0.01347566768527031 = 0.0065970029681921005 + 0.001 * 6.878664016723633
Epoch 720, val loss: 1.1059743165969849
Epoch 730, training loss: 0.013195562176406384 = 0.00631649000570178 + 0.001 * 6.8790717124938965
Epoch 730, val loss: 1.1122227907180786
Epoch 740, training loss: 0.012931756675243378 = 0.006052624434232712 + 0.001 * 6.879131317138672
Epoch 740, val loss: 1.1183184385299683
Epoch 750, training loss: 0.012674227356910706 = 0.0058044372126460075 + 0.001 * 6.8697896003723145
Epoch 750, val loss: 1.1243144273757935
Epoch 760, training loss: 0.012449931353330612 = 0.005570852197706699 + 0.001 * 6.879079341888428
Epoch 760, val loss: 1.1301844120025635
Epoch 770, training loss: 0.012223245576024055 = 0.005350848659873009 + 0.001 * 6.872396945953369
Epoch 770, val loss: 1.1359531879425049
Epoch 780, training loss: 0.012013683095574379 = 0.00514345895498991 + 0.001 * 6.870223522186279
Epoch 780, val loss: 1.1416088342666626
Epoch 790, training loss: 0.011816423386335373 = 0.004947695881128311 + 0.001 * 6.868727684020996
Epoch 790, val loss: 1.1471889019012451
Epoch 800, training loss: 0.011624269187450409 = 0.0047624618746340275 + 0.001 * 6.861806869506836
Epoch 800, val loss: 1.152672529220581
Epoch 810, training loss: 0.011448366567492485 = 0.004586878698319197 + 0.001 * 6.86148738861084
Epoch 810, val loss: 1.1580501794815063
Epoch 820, training loss: 0.011296526528894901 = 0.0044202012941241264 + 0.001 * 6.8763251304626465
Epoch 820, val loss: 1.1633813381195068
Epoch 830, training loss: 0.01112678274512291 = 0.004261984955519438 + 0.001 * 6.864797592163086
Epoch 830, val loss: 1.168655514717102
Epoch 840, training loss: 0.010974748060107231 = 0.004111832473427057 + 0.001 * 6.8629150390625
Epoch 840, val loss: 1.1738715171813965
Epoch 850, training loss: 0.010826900601387024 = 0.003969273064285517 + 0.001 * 6.857627868652344
Epoch 850, val loss: 1.178980827331543
Epoch 860, training loss: 0.01068732887506485 = 0.0038338506128638983 + 0.001 * 6.853477478027344
Epoch 860, val loss: 1.1840627193450928
Epoch 870, training loss: 0.010560090653598309 = 0.0037050540558993816 + 0.001 * 6.85503625869751
Epoch 870, val loss: 1.1890629529953003
Epoch 880, training loss: 0.010434442199766636 = 0.0035828002728521824 + 0.001 * 6.851641654968262
Epoch 880, val loss: 1.194021463394165
Epoch 890, training loss: 0.010328918695449829 = 0.0034666932187974453 + 0.001 * 6.862225532531738
Epoch 890, val loss: 1.1988829374313354
Epoch 900, training loss: 0.010210695676505566 = 0.0033564139157533646 + 0.001 * 6.854281425476074
Epoch 900, val loss: 1.2036561965942383
Epoch 910, training loss: 0.010102028027176857 = 0.0032514138147234917 + 0.001 * 6.850613594055176
Epoch 910, val loss: 1.20834219455719
Epoch 920, training loss: 0.010004991665482521 = 0.003151408862322569 + 0.001 * 6.85358190536499
Epoch 920, val loss: 1.2129663228988647
Epoch 930, training loss: 0.0099187595769763 = 0.0030561292078346014 + 0.001 * 6.8626298904418945
Epoch 930, val loss: 1.217494010925293
Epoch 940, training loss: 0.009804096072912216 = 0.002965424209833145 + 0.001 * 6.838672161102295
Epoch 940, val loss: 1.2219330072402954
Epoch 950, training loss: 0.009719221852719784 = 0.0028788100462406874 + 0.001 * 6.840411186218262
Epoch 950, val loss: 1.226334571838379
Epoch 960, training loss: 0.00963488407433033 = 0.0027960725128650665 + 0.001 * 6.838811874389648
Epoch 960, val loss: 1.2306636571884155
Epoch 970, training loss: 0.009560766629874706 = 0.002717056078836322 + 0.001 * 6.843710422515869
Epoch 970, val loss: 1.2349224090576172
Epoch 980, training loss: 0.009483671747148037 = 0.002641477854922414 + 0.001 * 6.842193126678467
Epoch 980, val loss: 1.2391536235809326
Epoch 990, training loss: 0.009420495480298996 = 0.002569124335423112 + 0.001 * 6.851370811462402
Epoch 990, val loss: 1.2433102130889893
Epoch 1000, training loss: 0.009333701804280281 = 0.0024999896995723248 + 0.001 * 6.83371114730835
Epoch 1000, val loss: 1.2474215030670166
Epoch 1010, training loss: 0.009260356426239014 = 0.0024338942021131516 + 0.001 * 6.826461315155029
Epoch 1010, val loss: 1.2514468431472778
Epoch 1020, training loss: 0.009208119474351406 = 0.0023705584462732077 + 0.001 * 6.837561130523682
Epoch 1020, val loss: 1.2554404735565186
Epoch 1030, training loss: 0.00913937296718359 = 0.0023099274840205908 + 0.001 * 6.8294453620910645
Epoch 1030, val loss: 1.2593621015548706
Epoch 1040, training loss: 0.009086753241717815 = 0.002251816214993596 + 0.001 * 6.83493709564209
Epoch 1040, val loss: 1.2631893157958984
Epoch 1050, training loss: 0.009047296829521656 = 0.002196162473410368 + 0.001 * 6.851133823394775
Epoch 1050, val loss: 1.2669769525527954
Epoch 1060, training loss: 0.008978661149740219 = 0.0021428202744573355 + 0.001 * 6.835840702056885
Epoch 1060, val loss: 1.2707103490829468
Epoch 1070, training loss: 0.008919110521674156 = 0.0020917137153446674 + 0.001 * 6.827396392822266
Epoch 1070, val loss: 1.2743442058563232
Epoch 1080, training loss: 0.008859250694513321 = 0.002042678650468588 + 0.001 * 6.816571235656738
Epoch 1080, val loss: 1.2779611349105835
Epoch 1090, training loss: 0.008817842230200768 = 0.00199561333283782 + 0.001 * 6.822227954864502
Epoch 1090, val loss: 1.2814592123031616
Epoch 1100, training loss: 0.008778659626841545 = 0.0019504597876220942 + 0.001 * 6.828199863433838
Epoch 1100, val loss: 1.2848789691925049
Epoch 1110, training loss: 0.008717484772205353 = 0.0019071772694587708 + 0.001 * 6.810307025909424
Epoch 1110, val loss: 1.2882916927337646
Epoch 1120, training loss: 0.008695723488926888 = 0.0018656013999134302 + 0.001 * 6.830121994018555
Epoch 1120, val loss: 1.2915973663330078
Epoch 1130, training loss: 0.00862894020974636 = 0.0018257133197039366 + 0.001 * 6.803226470947266
Epoch 1130, val loss: 1.294837236404419
Epoch 1140, training loss: 0.008610131219029427 = 0.001787390443496406 + 0.001 * 6.82274055480957
Epoch 1140, val loss: 1.2980245351791382
Epoch 1150, training loss: 0.008558827452361584 = 0.0017505224095657468 + 0.001 * 6.808304309844971
Epoch 1150, val loss: 1.3011744022369385
Epoch 1160, training loss: 0.0085223950445652 = 0.0017150696367025375 + 0.001 * 6.80732536315918
Epoch 1160, val loss: 1.304222583770752
Epoch 1170, training loss: 0.008503586985170841 = 0.0016809338703751564 + 0.001 * 6.822652816772461
Epoch 1170, val loss: 1.3072172403335571
Epoch 1180, training loss: 0.008452747948467731 = 0.0016480395570397377 + 0.001 * 6.804708003997803
Epoch 1180, val loss: 1.3101621866226196
Epoch 1190, training loss: 0.008431867696344852 = 0.0016164561966434121 + 0.001 * 6.81541109085083
Epoch 1190, val loss: 1.3130089044570923
Epoch 1200, training loss: 0.0083847064524889 = 0.001586036873050034 + 0.001 * 6.798669338226318
Epoch 1200, val loss: 1.3158725500106812
Epoch 1210, training loss: 0.008363619446754456 = 0.0015567378140985966 + 0.001 * 6.806881904602051
Epoch 1210, val loss: 1.3186274766921997
Epoch 1220, training loss: 0.008318118751049042 = 0.0015284972032532096 + 0.001 * 6.789621353149414
Epoch 1220, val loss: 1.321340560913086
Epoch 1230, training loss: 0.008318325504660606 = 0.0015012812800705433 + 0.001 * 6.817044258117676
Epoch 1230, val loss: 1.3239855766296387
Epoch 1240, training loss: 0.00826585665345192 = 0.001475044060498476 + 0.001 * 6.790812015533447
Epoch 1240, val loss: 1.3265514373779297
Epoch 1250, training loss: 0.008247848600149155 = 0.0014497593510895967 + 0.001 * 6.798089027404785
Epoch 1250, val loss: 1.3291254043579102
Epoch 1260, training loss: 0.008218333125114441 = 0.0014254291309043765 + 0.001 * 6.792903900146484
Epoch 1260, val loss: 1.331594705581665
Epoch 1270, training loss: 0.008181560784578323 = 0.0014019610825926065 + 0.001 * 6.779599666595459
Epoch 1270, val loss: 1.3340418338775635
Epoch 1280, training loss: 0.008175789378583431 = 0.0013793561374768615 + 0.001 * 6.796433448791504
Epoch 1280, val loss: 1.336464285850525
Epoch 1290, training loss: 0.008130880072712898 = 0.0013574997428804636 + 0.001 * 6.773379802703857
Epoch 1290, val loss: 1.3388575315475464
Epoch 1300, training loss: 0.008112381212413311 = 0.0013364144833758473 + 0.001 * 6.775966167449951
Epoch 1300, val loss: 1.3411259651184082
Epoch 1310, training loss: 0.008103584870696068 = 0.0013160647358745337 + 0.001 * 6.787519931793213
Epoch 1310, val loss: 1.3434324264526367
Epoch 1320, training loss: 0.008078617043793201 = 0.0012963779736310244 + 0.001 * 6.782238483428955
Epoch 1320, val loss: 1.3456718921661377
Epoch 1330, training loss: 0.008071261458098888 = 0.0012773809721693397 + 0.001 * 6.793879985809326
Epoch 1330, val loss: 1.3478630781173706
Epoch 1340, training loss: 0.008027473464608192 = 0.0012590197147801518 + 0.001 * 6.768453121185303
Epoch 1340, val loss: 1.349974274635315
Epoch 1350, training loss: 0.008019899018108845 = 0.0012412696378305554 + 0.001 * 6.778629302978516
Epoch 1350, val loss: 1.3520747423171997
Epoch 1360, training loss: 0.008036209270358086 = 0.0012241132790222764 + 0.001 * 6.812095642089844
Epoch 1360, val loss: 1.3541380167007446
Epoch 1370, training loss: 0.007982694543898106 = 0.001207485212944448 + 0.001 * 6.775208950042725
Epoch 1370, val loss: 1.356203317642212
Epoch 1380, training loss: 0.007974737323820591 = 0.0011914080241695046 + 0.001 * 6.783328533172607
Epoch 1380, val loss: 1.3581676483154297
Epoch 1390, training loss: 0.007937167771160603 = 0.0011758360778912902 + 0.001 * 6.761331081390381
Epoch 1390, val loss: 1.3601148128509521
Epoch 1400, training loss: 0.007937365211546421 = 0.0011607486521825194 + 0.001 * 6.776616096496582
Epoch 1400, val loss: 1.36203134059906
Epoch 1410, training loss: 0.007961714640259743 = 0.0011461387621238828 + 0.001 * 6.815575122833252
Epoch 1410, val loss: 1.3639053106307983
Epoch 1420, training loss: 0.007903734222054482 = 0.0011320351622998714 + 0.001 * 6.77169942855835
Epoch 1420, val loss: 1.3657450675964355
Epoch 1430, training loss: 0.007866526953876019 = 0.001118354150094092 + 0.001 * 6.748172760009766
Epoch 1430, val loss: 1.3675254583358765
Epoch 1440, training loss: 0.007870322093367577 = 0.0011050854809582233 + 0.001 * 6.7652363777160645
Epoch 1440, val loss: 1.3692848682403564
Epoch 1450, training loss: 0.007838238961994648 = 0.0010922400979325175 + 0.001 * 6.745998859405518
Epoch 1450, val loss: 1.3709849119186401
Epoch 1460, training loss: 0.00781998224556446 = 0.0010797623544931412 + 0.001 * 6.7402191162109375
Epoch 1460, val loss: 1.3726445436477661
Epoch 1470, training loss: 0.007822461426258087 = 0.0010676608653739095 + 0.001 * 6.754799842834473
Epoch 1470, val loss: 1.3742985725402832
Epoch 1480, training loss: 0.007830135524272919 = 0.0010558938374742866 + 0.001 * 6.7742414474487305
Epoch 1480, val loss: 1.3759485483169556
Epoch 1490, training loss: 0.007869772613048553 = 0.0010444552171975374 + 0.001 * 6.8253173828125
Epoch 1490, val loss: 1.3774597644805908
Epoch 1500, training loss: 0.0077813491225242615 = 0.0010333668906241655 + 0.001 * 6.747982025146484
Epoch 1500, val loss: 1.379010558128357
Epoch 1510, training loss: 0.007776182144880295 = 0.0010225963778793812 + 0.001 * 6.753585338592529
Epoch 1510, val loss: 1.38045334815979
Epoch 1520, training loss: 0.007751590572297573 = 0.0010121266823261976 + 0.001 * 6.739463806152344
Epoch 1520, val loss: 1.3818938732147217
Epoch 1530, training loss: 0.007744656875729561 = 0.0010019891196861863 + 0.001 * 6.7426676750183105
Epoch 1530, val loss: 1.3833231925964355
Epoch 1540, training loss: 0.007725986652076244 = 0.000992141431197524 + 0.001 * 6.733844757080078
Epoch 1540, val loss: 1.384708046913147
Epoch 1550, training loss: 0.00769040547311306 = 0.0009825695306062698 + 0.001 * 6.707835674285889
Epoch 1550, val loss: 1.3860878944396973
Epoch 1560, training loss: 0.007715332321822643 = 0.0009732833714224398 + 0.001 * 6.742048740386963
Epoch 1560, val loss: 1.3873869180679321
Epoch 1570, training loss: 0.0077230436727404594 = 0.0009642451768741012 + 0.001 * 6.758798122406006
Epoch 1570, val loss: 1.3887089490890503
