nohup: ignoring input
run_robust_acc.py:14: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0, add_edge_rate_2=0, attack='none', base_model='GCN', cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0005, cl_num_epochs=200, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, clf_weight=1, config='config.yaml', cont_batch_size=0, cont_weight=1, cuda=True, dataset='Citeseer', debug=True, device_id=0, drop_edge_rate_1=0.2, drop_edge_rate_2=0, drop_feat_rate_1=0.3, drop_feat_rate_2=0.2, dropout=0.5, encoder_model='Grace', hidden=128, if_smoothed=True, inv_weight=1, no_cuda=False, noisy_level=0.3, num_hidden=128, num_proj_hidden=128, prob=0.8, seed=10, select_target_ratio=0.1, tau=0.1, test_model='GCN', train_lr=0.01, weight_decay=0.0005)
beta 0.5
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803743362426758
Epoch 10, training loss: 8.802633285522461
Epoch 20, training loss: 8.800413131713867
Epoch 30, training loss: 8.689431190490723
Epoch 40, training loss: 8.195813179016113
Epoch 50, training loss: 7.776159763336182
Epoch 60, training loss: 7.398000717163086
Epoch 70, training loss: 7.036299228668213
Epoch 80, training loss: 6.855659008026123
Epoch 90, training loss: 6.529387950897217
Epoch 100, training loss: 6.421270370483398
Epoch 110, training loss: 6.343023777008057
Epoch 120, training loss: 6.138088226318359
Epoch 130, training loss: 6.089809894561768
Epoch 140, training loss: 6.006755352020264
Epoch 150, training loss: 5.966162204742432
Epoch 160, training loss: 5.925745010375977
Epoch 170, training loss: 5.790739059448242
Epoch 180, training loss: 5.800420761108398
Epoch 190, training loss: 5.732512474060059
none
Accuracy: 0.658
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803492546081543
Epoch 10, training loss: 8.80270767211914
Epoch 20, training loss: 8.800085067749023
Epoch 30, training loss: 8.677323341369629
Epoch 40, training loss: 8.175631523132324
Epoch 50, training loss: 7.868499755859375
Epoch 60, training loss: 7.4805731773376465
Epoch 70, training loss: 7.120909690856934
Epoch 80, training loss: 6.77183723449707
Epoch 90, training loss: 6.667881011962891
Epoch 100, training loss: 6.550374984741211
Epoch 110, training loss: 6.48245096206665
Epoch 120, training loss: 6.358505725860596
Epoch 130, training loss: 6.234560489654541
Epoch 140, training loss: 6.164249897003174
Epoch 150, training loss: 6.095634460449219
Epoch 160, training loss: 6.0274553298950195
Epoch 170, training loss: 5.950432777404785
Epoch 180, training loss: 5.896798610687256
Epoch 190, training loss: 5.888125896453857
none
Accuracy: 0.691
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803023338317871
Epoch 10, training loss: 8.802323341369629
Epoch 20, training loss: 8.79737377166748
Epoch 30, training loss: 8.495231628417969
Epoch 40, training loss: 7.916090488433838
Epoch 50, training loss: 7.401015758514404
Epoch 60, training loss: 7.075287818908691
Epoch 70, training loss: 6.818445682525635
Epoch 80, training loss: 6.6402764320373535
Epoch 90, training loss: 6.467509746551514
Epoch 100, training loss: 6.284963607788086
Epoch 110, training loss: 6.192216396331787
Epoch 120, training loss: 6.091508865356445
Epoch 130, training loss: 5.9704389572143555
Epoch 140, training loss: 5.916741371154785
Epoch 150, training loss: 5.836300373077393
Epoch 160, training loss: 5.771597862243652
Epoch 170, training loss: 5.711379528045654
Epoch 180, training loss: 5.65004825592041
Epoch 190, training loss: 5.587502479553223
none
Accuracy: 0.658
beta 0.6
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803601264953613
Epoch 10, training loss: 8.802688598632812
Epoch 20, training loss: 8.80122184753418
Epoch 30, training loss: 8.739594459533691
Epoch 40, training loss: 8.21911907196045
Epoch 50, training loss: 7.869137763977051
Epoch 60, training loss: 7.390052795410156
Epoch 70, training loss: 7.050063610076904
Epoch 80, training loss: 6.847832679748535
Epoch 90, training loss: 6.6834869384765625
Epoch 100, training loss: 6.402365207672119
Epoch 110, training loss: 6.28287410736084
Epoch 120, training loss: 6.152798175811768
Epoch 130, training loss: 6.092881202697754
Epoch 140, training loss: 5.959389686584473
Epoch 150, training loss: 5.924330234527588
Epoch 160, training loss: 5.8155293464660645
Epoch 170, training loss: 5.7335686683654785
Epoch 180, training loss: 5.718442440032959
Epoch 190, training loss: 5.6824445724487305
none
Accuracy: 0.669
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803376197814941
Epoch 10, training loss: 8.802595138549805
Epoch 20, training loss: 8.800689697265625
Epoch 30, training loss: 8.680320739746094
Epoch 40, training loss: 8.20787525177002
Epoch 50, training loss: 7.907566070556641
Epoch 60, training loss: 7.365912437438965
Epoch 70, training loss: 7.021971225738525
Epoch 80, training loss: 6.7574262619018555
Epoch 90, training loss: 6.666271686553955
Epoch 100, training loss: 6.526334762573242
Epoch 110, training loss: 6.472208023071289
Epoch 120, training loss: 6.396752834320068
Epoch 130, training loss: 6.2573628425598145
Epoch 140, training loss: 6.23878288269043
Epoch 150, training loss: 6.1864142417907715
Epoch 160, training loss: 6.085230350494385
Epoch 170, training loss: 6.021381855010986
Epoch 180, training loss: 5.991866588592529
Epoch 190, training loss: 5.9328694343566895
none
Accuracy: 0.629
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803487777709961
Epoch 10, training loss: 8.802518844604492
Epoch 20, training loss: 8.798003196716309
Epoch 30, training loss: 8.521392822265625
Epoch 40, training loss: 7.739575386047363
Epoch 50, training loss: 7.193994045257568
Epoch 60, training loss: 6.961630821228027
Epoch 70, training loss: 6.773580074310303
Epoch 80, training loss: 6.560317516326904
Epoch 90, training loss: 6.442800998687744
Epoch 100, training loss: 6.314529895782471
Epoch 110, training loss: 6.145751953125
Epoch 120, training loss: 6.106406211853027
Epoch 130, training loss: 6.065816879272461
Epoch 140, training loss: 5.953830718994141
Epoch 150, training loss: 5.93263578414917
Epoch 160, training loss: 5.851191520690918
Epoch 170, training loss: 5.829018592834473
Epoch 180, training loss: 5.8084940910339355
Epoch 190, training loss: 5.774321556091309
none
Accuracy: 0.693
beta 0.7
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.80323314666748
Epoch 10, training loss: 8.80262565612793
Epoch 20, training loss: 8.795214653015137
Epoch 30, training loss: 8.490665435791016
Epoch 40, training loss: 7.998668670654297
Epoch 50, training loss: 7.5689802169799805
Epoch 60, training loss: 7.370416164398193
Epoch 70, training loss: 7.192643642425537
Epoch 80, training loss: 7.021790504455566
Epoch 90, training loss: 6.882410526275635
Epoch 100, training loss: 6.670793533325195
Epoch 110, training loss: 6.588681697845459
Epoch 120, training loss: 6.484619140625
Epoch 130, training loss: 6.4146647453308105
Epoch 140, training loss: 6.388194561004639
Epoch 150, training loss: 6.278113842010498
Epoch 160, training loss: 6.221299171447754
Epoch 170, training loss: 6.179358959197998
Epoch 180, training loss: 6.139138698577881
Epoch 190, training loss: 6.088709831237793
none
Accuracy: 0.656
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803513526916504
Epoch 10, training loss: 8.802743911743164
Epoch 20, training loss: 8.799522399902344
Epoch 30, training loss: 8.640399932861328
Epoch 40, training loss: 8.151589393615723
Epoch 50, training loss: 7.860523223876953
Epoch 60, training loss: 7.435096263885498
Epoch 70, training loss: 7.091847896575928
Epoch 80, training loss: 6.878655910491943
Epoch 90, training loss: 6.7967000007629395
Epoch 100, training loss: 6.60143518447876
Epoch 110, training loss: 6.431215763092041
Epoch 120, training loss: 6.302271842956543
Epoch 130, training loss: 6.259655952453613
Epoch 140, training loss: 6.145450592041016
Epoch 150, training loss: 6.046963691711426
Epoch 160, training loss: 6.031755447387695
Epoch 170, training loss: 5.9876885414123535
Epoch 180, training loss: 5.856848239898682
Epoch 190, training loss: 5.817063331604004
none
Accuracy: 0.678
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803166389465332
Epoch 10, training loss: 8.802499771118164
Epoch 20, training loss: 8.799097061157227
Epoch 30, training loss: 8.608887672424316
Epoch 40, training loss: 8.140000343322754
Epoch 50, training loss: 7.847251892089844
Epoch 60, training loss: 7.489227771759033
Epoch 70, training loss: 7.163122177124023
Epoch 80, training loss: 6.930111408233643
Epoch 90, training loss: 6.754075050354004
Epoch 100, training loss: 6.673130989074707
Epoch 110, training loss: 6.467227935791016
Epoch 120, training loss: 6.400647163391113
Epoch 130, training loss: 6.295638561248779
Epoch 140, training loss: 6.256929874420166
Epoch 150, training loss: 6.122025966644287
Epoch 160, training loss: 6.0223798751831055
Epoch 170, training loss: 5.95015811920166
Epoch 180, training loss: 5.991180419921875
Epoch 190, training loss: 5.883345603942871
none
Accuracy: 0.66
beta 0.9
seed 265
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803303718566895
Epoch 10, training loss: 8.802536964416504
Epoch 20, training loss: 8.799317359924316
Epoch 30, training loss: 8.653969764709473
Epoch 40, training loss: 7.978963851928711
Epoch 50, training loss: 7.42083740234375
Epoch 60, training loss: 6.953172206878662
Epoch 70, training loss: 6.58320426940918
Epoch 80, training loss: 6.411067008972168
Epoch 90, training loss: 6.261363506317139
Epoch 100, training loss: 6.053987979888916
Epoch 110, training loss: 6.010139465332031
Epoch 120, training loss: 5.8887939453125
Epoch 130, training loss: 5.807628631591797
Epoch 140, training loss: 5.763754844665527
Epoch 150, training loss: 5.646809101104736
Epoch 160, training loss: 5.626912593841553
Epoch 170, training loss: 5.578660488128662
Epoch 180, training loss: 5.549616813659668
Epoch 190, training loss: 5.471126556396484
none
Accuracy: 0.647
seed 125
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803359985351562
Epoch 10, training loss: 8.802175521850586
Epoch 20, training loss: 8.794629096984863
Epoch 30, training loss: 8.435163497924805
Epoch 40, training loss: 7.772974491119385
Epoch 50, training loss: 7.131039619445801
Epoch 60, training loss: 6.762226104736328
Epoch 70, training loss: 6.66329288482666
Epoch 80, training loss: 6.435918807983398
Epoch 90, training loss: 6.318647861480713
Epoch 100, training loss: 6.160996913909912
Epoch 110, training loss: 6.020410537719727
Epoch 120, training loss: 5.880756378173828
Epoch 130, training loss: 5.834033966064453
Epoch 140, training loss: 5.791565895080566
Epoch 150, training loss: 5.722123146057129
Epoch 160, training loss: 5.655769348144531
Epoch 170, training loss: 5.616933345794678
Epoch 180, training loss: 5.561199188232422
Epoch 190, training loss: 5.5390238761901855
none
Accuracy: 0.655
seed 996
Not implement Grace
=== training gcn model ===
Epoch 0, training loss: 8.803147315979004
Epoch 10, training loss: 8.802517890930176
Epoch 20, training loss: 8.797497749328613
Epoch 30, training loss: 8.517722129821777
Epoch 40, training loss: 8.096463203430176
Epoch 50, training loss: 7.598343849182129
Epoch 60, training loss: 7.165441989898682
Epoch 70, training loss: 6.808314800262451
Epoch 80, training loss: 6.615091323852539
Epoch 90, training loss: 6.4960737228393555
Epoch 100, training loss: 6.294355392456055
Epoch 110, training loss: 6.186862945556641
Epoch 120, training loss: 6.09932804107666
Epoch 130, training loss: 6.071213722229004
Epoch 140, training loss: 5.947112560272217
Epoch 150, training loss: 5.928627967834473
Epoch 160, training loss: 5.8852739334106445
Epoch 170, training loss: 5.857870101928711
Epoch 180, training loss: 5.83858585357666
Epoch 190, training loss: 5.766848564147949
none
Accuracy: 0.647
