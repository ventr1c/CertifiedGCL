Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11628])
remove edge: torch.Size([2, 9456])
updated graph: torch.Size([2, 10528])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.79669189453125 = 1.9553427696228027 + 50.0 * 8.596826553344727
Epoch 0, val loss: 1.955989122390747
Epoch 10, training loss: 431.73956298828125 = 1.9460984468460083 + 50.0 * 8.595869064331055
Epoch 10, val loss: 1.945984125137329
Epoch 20, training loss: 431.34783935546875 = 1.9347654581069946 + 50.0 * 8.588261604309082
Epoch 20, val loss: 1.933613896369934
Epoch 30, training loss: 428.7392883300781 = 1.9201734066009521 + 50.0 * 8.536382675170898
Epoch 30, val loss: 1.9177967309951782
Epoch 40, training loss: 413.8944091796875 = 1.903698205947876 + 50.0 * 8.239814758300781
Epoch 40, val loss: 1.9008874893188477
Epoch 50, training loss: 379.23052978515625 = 1.8862066268920898 + 50.0 * 7.546886444091797
Epoch 50, val loss: 1.8838372230529785
Epoch 60, training loss: 361.8809814453125 = 1.8728117942810059 + 50.0 * 7.2001633644104
Epoch 60, val loss: 1.871277093887329
Epoch 70, training loss: 351.2905578613281 = 1.8602964878082275 + 50.0 * 6.988605499267578
Epoch 70, val loss: 1.859068751335144
Epoch 80, training loss: 344.4247741699219 = 1.848537564277649 + 50.0 * 6.851524829864502
Epoch 80, val loss: 1.8476797342300415
Epoch 90, training loss: 339.46246337890625 = 1.8384044170379639 + 50.0 * 6.752480983734131
Epoch 90, val loss: 1.8376648426055908
Epoch 100, training loss: 335.7291564941406 = 1.8301118612289429 + 50.0 * 6.677980899810791
Epoch 100, val loss: 1.8296087980270386
Epoch 110, training loss: 333.110595703125 = 1.8226526975631714 + 50.0 * 6.625758647918701
Epoch 110, val loss: 1.822139859199524
Epoch 120, training loss: 331.308837890625 = 1.8153553009033203 + 50.0 * 6.589869499206543
Epoch 120, val loss: 1.814710021018982
Epoch 130, training loss: 329.9237976074219 = 1.8081368207931519 + 50.0 * 6.562313079833984
Epoch 130, val loss: 1.8071545362472534
Epoch 140, training loss: 328.8999328613281 = 1.8009182214736938 + 50.0 * 6.541980743408203
Epoch 140, val loss: 1.7996736764907837
Epoch 150, training loss: 327.9193420410156 = 1.7936698198318481 + 50.0 * 6.522513389587402
Epoch 150, val loss: 1.7924350500106812
Epoch 160, training loss: 326.9764099121094 = 1.786437749862671 + 50.0 * 6.5037994384765625
Epoch 160, val loss: 1.7851917743682861
Epoch 170, training loss: 326.0881652832031 = 1.7788991928100586 + 50.0 * 6.486185073852539
Epoch 170, val loss: 1.7778443098068237
Epoch 180, training loss: 325.5852355957031 = 1.7710015773773193 + 50.0 * 6.476284503936768
Epoch 180, val loss: 1.7701032161712646
Epoch 190, training loss: 324.5745544433594 = 1.7621675729751587 + 50.0 * 6.456247806549072
Epoch 190, val loss: 1.761871099472046
Epoch 200, training loss: 323.851806640625 = 1.7526580095291138 + 50.0 * 6.441983222961426
Epoch 200, val loss: 1.7528773546218872
Epoch 210, training loss: 323.1552734375 = 1.7423208951950073 + 50.0 * 6.428259372711182
Epoch 210, val loss: 1.7433547973632812
Epoch 220, training loss: 322.6210632324219 = 1.7310556173324585 + 50.0 * 6.417800426483154
Epoch 220, val loss: 1.7329214811325073
Epoch 230, training loss: 322.1378173828125 = 1.718673586845398 + 50.0 * 6.408383369445801
Epoch 230, val loss: 1.7216166257858276
Epoch 240, training loss: 321.643310546875 = 1.7051774263381958 + 50.0 * 6.3987627029418945
Epoch 240, val loss: 1.7093232870101929
Epoch 250, training loss: 321.36907958984375 = 1.6904798746109009 + 50.0 * 6.393571853637695
Epoch 250, val loss: 1.6959260702133179
Epoch 260, training loss: 320.9472351074219 = 1.6744716167449951 + 50.0 * 6.385455131530762
Epoch 260, val loss: 1.681442141532898
Epoch 270, training loss: 320.5457458496094 = 1.6571322679519653 + 50.0 * 6.377772331237793
Epoch 270, val loss: 1.6660221815109253
Epoch 280, training loss: 320.405029296875 = 1.638704776763916 + 50.0 * 6.375326633453369
Epoch 280, val loss: 1.649587631225586
Epoch 290, training loss: 319.99505615234375 = 1.6189290285110474 + 50.0 * 6.367522716522217
Epoch 290, val loss: 1.6321558952331543
Epoch 300, training loss: 319.71136474609375 = 1.5982658863067627 + 50.0 * 6.362261772155762
Epoch 300, val loss: 1.6139166355133057
Epoch 310, training loss: 319.4268798828125 = 1.5766538381576538 + 50.0 * 6.357004642486572
Epoch 310, val loss: 1.59510338306427
Epoch 320, training loss: 319.296142578125 = 1.5542360544204712 + 50.0 * 6.3548383712768555
Epoch 320, val loss: 1.5757606029510498
Epoch 330, training loss: 319.21014404296875 = 1.5309678316116333 + 50.0 * 6.353583335876465
Epoch 330, val loss: 1.5555692911148071
Epoch 340, training loss: 318.8162841796875 = 1.5071378946304321 + 50.0 * 6.346182823181152
Epoch 340, val loss: 1.5352236032485962
Epoch 350, training loss: 318.5039367675781 = 1.4830421209335327 + 50.0 * 6.340418338775635
Epoch 350, val loss: 1.514738917350769
Epoch 360, training loss: 318.2811279296875 = 1.4585802555084229 + 50.0 * 6.336451053619385
Epoch 360, val loss: 1.4942048788070679
Epoch 370, training loss: 318.0887451171875 = 1.4339351654052734 + 50.0 * 6.333096504211426
Epoch 370, val loss: 1.4737170934677124
Epoch 380, training loss: 318.3095703125 = 1.409165620803833 + 50.0 * 6.338007926940918
Epoch 380, val loss: 1.453217625617981
Epoch 390, training loss: 317.7388916015625 = 1.3839246034622192 + 50.0 * 6.327099323272705
Epoch 390, val loss: 1.432919979095459
Epoch 400, training loss: 317.553466796875 = 1.3588560819625854 + 50.0 * 6.323892116546631
Epoch 400, val loss: 1.4128514528274536
Epoch 410, training loss: 317.469482421875 = 1.3338420391082764 + 50.0 * 6.3227128982543945
Epoch 410, val loss: 1.393173098564148
Epoch 420, training loss: 317.27813720703125 = 1.3088101148605347 + 50.0 * 6.3193864822387695
Epoch 420, val loss: 1.3734934329986572
Epoch 430, training loss: 317.0838623046875 = 1.2838283777236938 + 50.0 * 6.316000938415527
Epoch 430, val loss: 1.3543256521224976
Epoch 440, training loss: 317.0002136230469 = 1.259043574333191 + 50.0 * 6.314823627471924
Epoch 440, val loss: 1.3354461193084717
Epoch 450, training loss: 317.0211181640625 = 1.2340741157531738 + 50.0 * 6.315741062164307
Epoch 450, val loss: 1.3170679807662964
Epoch 460, training loss: 316.6790771484375 = 1.2094367742538452 + 50.0 * 6.309392929077148
Epoch 460, val loss: 1.2988101243972778
Epoch 470, training loss: 316.5242614746094 = 1.185041904449463 + 50.0 * 6.306784629821777
Epoch 470, val loss: 1.2811083793640137
Epoch 480, training loss: 316.6417236328125 = 1.1608355045318604 + 50.0 * 6.30961799621582
Epoch 480, val loss: 1.2638766765594482
Epoch 490, training loss: 316.3209228515625 = 1.136543869972229 + 50.0 * 6.303687572479248
Epoch 490, val loss: 1.2470886707305908
Epoch 500, training loss: 316.1369934082031 = 1.1128458976745605 + 50.0 * 6.300482749938965
Epoch 500, val loss: 1.2307766675949097
Epoch 510, training loss: 316.1213073730469 = 1.0893183946609497 + 50.0 * 6.300639629364014
Epoch 510, val loss: 1.215057611465454
Epoch 520, training loss: 315.9970397949219 = 1.0660998821258545 + 50.0 * 6.298618793487549
Epoch 520, val loss: 1.1996662616729736
Epoch 530, training loss: 315.7823181152344 = 1.043245553970337 + 50.0 * 6.29478120803833
Epoch 530, val loss: 1.184906244277954
Epoch 540, training loss: 315.7185363769531 = 1.0208030939102173 + 50.0 * 6.293954372406006
Epoch 540, val loss: 1.1707484722137451
Epoch 550, training loss: 315.5285949707031 = 0.9987019896507263 + 50.0 * 6.290598392486572
Epoch 550, val loss: 1.1571272611618042
Epoch 560, training loss: 316.14056396484375 = 0.9771906733512878 + 50.0 * 6.303267478942871
Epoch 560, val loss: 1.1438649892807007
Epoch 570, training loss: 315.41650390625 = 0.9555335640907288 + 50.0 * 6.289219379425049
Epoch 570, val loss: 1.1314631700515747
Epoch 580, training loss: 315.2475891113281 = 0.9347658157348633 + 50.0 * 6.286256790161133
Epoch 580, val loss: 1.119844913482666
Epoch 590, training loss: 315.13446044921875 = 0.9145604968070984 + 50.0 * 6.284398078918457
Epoch 590, val loss: 1.1088080406188965
Epoch 600, training loss: 315.0301818847656 = 0.894879937171936 + 50.0 * 6.282706260681152
Epoch 600, val loss: 1.098469614982605
Epoch 610, training loss: 315.0487060546875 = 0.8757191300392151 + 50.0 * 6.283459663391113
Epoch 610, val loss: 1.08876371383667
Epoch 620, training loss: 315.07470703125 = 0.856852114200592 + 50.0 * 6.28435754776001
Epoch 620, val loss: 1.0793646574020386
Epoch 630, training loss: 314.7822570800781 = 0.8385530710220337 + 50.0 * 6.278873920440674
Epoch 630, val loss: 1.0708593130111694
Epoch 640, training loss: 314.7507019042969 = 0.8208086490631104 + 50.0 * 6.278597831726074
Epoch 640, val loss: 1.0630018711090088
Epoch 650, training loss: 314.7695007324219 = 0.8035867810249329 + 50.0 * 6.279318332672119
Epoch 650, val loss: 1.0555297136306763
Epoch 660, training loss: 314.6078186035156 = 0.7867032289505005 + 50.0 * 6.276422023773193
Epoch 660, val loss: 1.049031376838684
Epoch 670, training loss: 314.4875793457031 = 0.7704532146453857 + 50.0 * 6.2743425369262695
Epoch 670, val loss: 1.0428332090377808
Epoch 680, training loss: 314.363525390625 = 0.7547388076782227 + 50.0 * 6.2721757888793945
Epoch 680, val loss: 1.0374419689178467
Epoch 690, training loss: 314.3086242675781 = 0.7395007014274597 + 50.0 * 6.2713823318481445
Epoch 690, val loss: 1.0326982736587524
Epoch 700, training loss: 314.459716796875 = 0.7245851159095764 + 50.0 * 6.274703025817871
Epoch 700, val loss: 1.0284150838851929
Epoch 710, training loss: 314.67449951171875 = 0.7098536491394043 + 50.0 * 6.279293060302734
Epoch 710, val loss: 1.0241307020187378
Epoch 720, training loss: 314.20123291015625 = 0.6955806016921997 + 50.0 * 6.270112991333008
Epoch 720, val loss: 1.0203039646148682
Epoch 730, training loss: 314.0425109863281 = 0.6818534135818481 + 50.0 * 6.267212867736816
Epoch 730, val loss: 1.017058253288269
Epoch 740, training loss: 313.9617919921875 = 0.668536901473999 + 50.0 * 6.265864849090576
Epoch 740, val loss: 1.0144389867782593
Epoch 750, training loss: 313.8686218261719 = 0.6555829644203186 + 50.0 * 6.264260768890381
Epoch 750, val loss: 1.012298583984375
Epoch 760, training loss: 313.8075256347656 = 0.6429481506347656 + 50.0 * 6.263291835784912
Epoch 760, val loss: 1.0105234384536743
Epoch 770, training loss: 313.8199462890625 = 0.6306051015853882 + 50.0 * 6.263786315917969
Epoch 770, val loss: 1.00892972946167
Epoch 780, training loss: 313.7819519042969 = 0.6183350682258606 + 50.0 * 6.263272285461426
Epoch 780, val loss: 1.0077612400054932
Epoch 790, training loss: 313.6658935546875 = 0.6064273118972778 + 50.0 * 6.2611894607543945
Epoch 790, val loss: 1.0063529014587402
Epoch 800, training loss: 313.6046447753906 = 0.5948174595832825 + 50.0 * 6.260196685791016
Epoch 800, val loss: 1.0055311918258667
Epoch 810, training loss: 313.5278625488281 = 0.5835669040679932 + 50.0 * 6.258886337280273
Epoch 810, val loss: 1.005187749862671
Epoch 820, training loss: 313.8487548828125 = 0.5725678205490112 + 50.0 * 6.265523910522461
Epoch 820, val loss: 1.0051366090774536
Epoch 830, training loss: 313.6651306152344 = 0.5615745186805725 + 50.0 * 6.262070655822754
Epoch 830, val loss: 1.0048795938491821
Epoch 840, training loss: 313.4762268066406 = 0.5508475303649902 + 50.0 * 6.25850772857666
Epoch 840, val loss: 1.004944086074829
Epoch 850, training loss: 313.63043212890625 = 0.5404179096221924 + 50.0 * 6.261800765991211
Epoch 850, val loss: 1.0053678750991821
Epoch 860, training loss: 313.3341369628906 = 0.5300456285476685 + 50.0 * 6.256081581115723
Epoch 860, val loss: 1.0062448978424072
Epoch 870, training loss: 313.2472229003906 = 0.5200316309928894 + 50.0 * 6.254544258117676
Epoch 870, val loss: 1.0070701837539673
Epoch 880, training loss: 313.1861572265625 = 0.5101808905601501 + 50.0 * 6.253519535064697
Epoch 880, val loss: 1.0083673000335693
Epoch 890, training loss: 313.41168212890625 = 0.5004924535751343 + 50.0 * 6.258224010467529
Epoch 890, val loss: 1.0096979141235352
Epoch 900, training loss: 313.1783447265625 = 0.4909091591835022 + 50.0 * 6.253748893737793
Epoch 900, val loss: 1.0111329555511475
Epoch 910, training loss: 313.08721923828125 = 0.481475830078125 + 50.0 * 6.252114772796631
Epoch 910, val loss: 1.012487530708313
Epoch 920, training loss: 313.01397705078125 = 0.4722774028778076 + 50.0 * 6.250833511352539
Epoch 920, val loss: 1.0144273042678833
Epoch 930, training loss: 313.09307861328125 = 0.4632608890533447 + 50.0 * 6.252596378326416
Epoch 930, val loss: 1.016371250152588
Epoch 940, training loss: 313.00494384765625 = 0.4542517364025116 + 50.0 * 6.25101375579834
Epoch 940, val loss: 1.0184497833251953
Epoch 950, training loss: 312.9161376953125 = 0.44548287987709045 + 50.0 * 6.249413013458252
Epoch 950, val loss: 1.020551323890686
Epoch 960, training loss: 312.8341064453125 = 0.4368375837802887 + 50.0 * 6.247945785522461
Epoch 960, val loss: 1.0228776931762695
Epoch 970, training loss: 312.9184265136719 = 0.42841556668281555 + 50.0 * 6.249800205230713
Epoch 970, val loss: 1.0253909826278687
Epoch 980, training loss: 312.8185729980469 = 0.4199816584587097 + 50.0 * 6.247971534729004
Epoch 980, val loss: 1.028080940246582
Epoch 990, training loss: 312.7225036621094 = 0.4116779863834381 + 50.0 * 6.246216297149658
Epoch 990, val loss: 1.0306438207626343
Epoch 1000, training loss: 312.7552185058594 = 0.403592050075531 + 50.0 * 6.247032642364502
Epoch 1000, val loss: 1.033490777015686
Epoch 1010, training loss: 312.65850830078125 = 0.39560362696647644 + 50.0 * 6.245258331298828
Epoch 1010, val loss: 1.0366003513336182
Epoch 1020, training loss: 312.60980224609375 = 0.3877856433391571 + 50.0 * 6.24444055557251
Epoch 1020, val loss: 1.0396267175674438
Epoch 1030, training loss: 312.5749816894531 = 0.380098432302475 + 50.0 * 6.243897438049316
Epoch 1030, val loss: 1.0428898334503174
Epoch 1040, training loss: 313.03289794921875 = 0.37250030040740967 + 50.0 * 6.253208160400391
Epoch 1040, val loss: 1.0461061000823975
Epoch 1050, training loss: 312.5751953125 = 0.36476993560791016 + 50.0 * 6.244208335876465
Epoch 1050, val loss: 1.0492390394210815
Epoch 1060, training loss: 312.4698486328125 = 0.3573593199253082 + 50.0 * 6.242249488830566
Epoch 1060, val loss: 1.0525480508804321
Epoch 1070, training loss: 312.4081115722656 = 0.3501512110233307 + 50.0 * 6.241158962249756
Epoch 1070, val loss: 1.0563024282455444
Epoch 1080, training loss: 312.3785705566406 = 0.34309110045433044 + 50.0 * 6.2407097816467285
Epoch 1080, val loss: 1.0601897239685059
Epoch 1090, training loss: 312.6361083984375 = 0.3361443877220154 + 50.0 * 6.245999336242676
Epoch 1090, val loss: 1.0641010999679565
Epoch 1100, training loss: 312.5472106933594 = 0.32911887764930725 + 50.0 * 6.244361400604248
Epoch 1100, val loss: 1.0675654411315918
Epoch 1110, training loss: 312.38934326171875 = 0.3222339451313019 + 50.0 * 6.241342067718506
Epoch 1110, val loss: 1.0716660022735596
Epoch 1120, training loss: 312.250244140625 = 0.3154999315738678 + 50.0 * 6.23869514465332
Epoch 1120, val loss: 1.0757033824920654
Epoch 1130, training loss: 312.19921875 = 0.30896928906440735 + 50.0 * 6.237804889678955
Epoch 1130, val loss: 1.079972743988037
Epoch 1140, training loss: 312.1888427734375 = 0.30255869030952454 + 50.0 * 6.237725734710693
Epoch 1140, val loss: 1.0843595266342163
Epoch 1150, training loss: 312.5834655761719 = 0.296204149723053 + 50.0 * 6.2457451820373535
Epoch 1150, val loss: 1.0886726379394531
Epoch 1160, training loss: 312.1695251464844 = 0.2898099422454834 + 50.0 * 6.2375946044921875
Epoch 1160, val loss: 1.0928512811660767
Epoch 1170, training loss: 312.27337646484375 = 0.28359630703926086 + 50.0 * 6.239795684814453
Epoch 1170, val loss: 1.0972697734832764
Epoch 1180, training loss: 312.06353759765625 = 0.2775351107120514 + 50.0 * 6.235720157623291
Epoch 1180, val loss: 1.1018646955490112
Epoch 1190, training loss: 312.046630859375 = 0.2716282904148102 + 50.0 * 6.235499858856201
Epoch 1190, val loss: 1.106573224067688
Epoch 1200, training loss: 312.0990295410156 = 0.26582252979278564 + 50.0 * 6.236664295196533
Epoch 1200, val loss: 1.1113431453704834
Epoch 1210, training loss: 312.337890625 = 0.26009616255760193 + 50.0 * 6.241555690765381
Epoch 1210, val loss: 1.1159948110580444
Epoch 1220, training loss: 311.9913330078125 = 0.2542790472507477 + 50.0 * 6.2347412109375
Epoch 1220, val loss: 1.1204768419265747
Epoch 1230, training loss: 311.9557800292969 = 0.2487255036830902 + 50.0 * 6.2341413497924805
Epoch 1230, val loss: 1.1252957582473755
Epoch 1240, training loss: 311.9054260253906 = 0.24327270686626434 + 50.0 * 6.233242511749268
Epoch 1240, val loss: 1.1302728652954102
Epoch 1250, training loss: 311.8836364746094 = 0.23796826601028442 + 50.0 * 6.232913494110107
Epoch 1250, val loss: 1.1353075504302979
Epoch 1260, training loss: 312.48626708984375 = 0.2327180802822113 + 50.0 * 6.245070934295654
Epoch 1260, val loss: 1.140049934387207
Epoch 1270, training loss: 311.9975280761719 = 0.22749869525432587 + 50.0 * 6.235400676727295
Epoch 1270, val loss: 1.1452717781066895
Epoch 1280, training loss: 311.7807922363281 = 0.22238147258758545 + 50.0 * 6.231168270111084
Epoch 1280, val loss: 1.1502737998962402
Epoch 1290, training loss: 311.7870178222656 = 0.21744413673877716 + 50.0 * 6.231391906738281
Epoch 1290, val loss: 1.155498743057251
Epoch 1300, training loss: 312.0151062011719 = 0.21260115504264832 + 50.0 * 6.236049652099609
Epoch 1300, val loss: 1.1608195304870605
Epoch 1310, training loss: 311.8746032714844 = 0.207808718085289 + 50.0 * 6.233336448669434
Epoch 1310, val loss: 1.1659382581710815
Epoch 1320, training loss: 312.028076171875 = 0.2030777782201767 + 50.0 * 6.236500263214111
Epoch 1320, val loss: 1.1710339784622192
Epoch 1330, training loss: 311.7754211425781 = 0.19850724935531616 + 50.0 * 6.23153829574585
Epoch 1330, val loss: 1.1764535903930664
Epoch 1340, training loss: 311.690673828125 = 0.1939954310655594 + 50.0 * 6.229933261871338
Epoch 1340, val loss: 1.181764841079712
Epoch 1350, training loss: 311.6293640136719 = 0.189663365483284 + 50.0 * 6.228794097900391
Epoch 1350, val loss: 1.1873775720596313
Epoch 1360, training loss: 311.77801513671875 = 0.18542224168777466 + 50.0 * 6.231852054595947
Epoch 1360, val loss: 1.1929233074188232
Epoch 1370, training loss: 311.60601806640625 = 0.181191548705101 + 50.0 * 6.228496551513672
Epoch 1370, val loss: 1.19804048538208
Epoch 1380, training loss: 311.63568115234375 = 0.1770796924829483 + 50.0 * 6.2291717529296875
Epoch 1380, val loss: 1.2035375833511353
Epoch 1390, training loss: 311.5906982421875 = 0.17307741940021515 + 50.0 * 6.2283525466918945
Epoch 1390, val loss: 1.2090332508087158
Epoch 1400, training loss: 311.9205627441406 = 0.16920813918113708 + 50.0 * 6.235026836395264
Epoch 1400, val loss: 1.214561104774475
Epoch 1410, training loss: 311.5575866699219 = 0.1652781069278717 + 50.0 * 6.227846145629883
Epoch 1410, val loss: 1.2196917533874512
Epoch 1420, training loss: 311.4927062988281 = 0.16152220964431763 + 50.0 * 6.22662353515625
Epoch 1420, val loss: 1.2252488136291504
Epoch 1430, training loss: 311.46240234375 = 0.15790484845638275 + 50.0 * 6.226089954376221
Epoch 1430, val loss: 1.2310831546783447
Epoch 1440, training loss: 311.4952697753906 = 0.15437984466552734 + 50.0 * 6.226817607879639
Epoch 1440, val loss: 1.2367072105407715
Epoch 1450, training loss: 311.7080078125 = 0.1508980095386505 + 50.0 * 6.231142044067383
Epoch 1450, val loss: 1.2422945499420166
Epoch 1460, training loss: 311.5386657714844 = 0.14748384058475494 + 50.0 * 6.227823257446289
Epoch 1460, val loss: 1.2480138540267944
Epoch 1470, training loss: 311.5713806152344 = 0.1441565901041031 + 50.0 * 6.228544235229492
Epoch 1470, val loss: 1.25356924533844
Epoch 1480, training loss: 311.45623779296875 = 0.14094705879688263 + 50.0 * 6.2263054847717285
Epoch 1480, val loss: 1.2595155239105225
Epoch 1490, training loss: 311.5651550292969 = 0.13779227435588837 + 50.0 * 6.228547096252441
Epoch 1490, val loss: 1.2652299404144287
Epoch 1500, training loss: 311.4034729003906 = 0.13469547033309937 + 50.0 * 6.225375175476074
Epoch 1500, val loss: 1.2708804607391357
Epoch 1510, training loss: 311.3358154296875 = 0.1317327916622162 + 50.0 * 6.224081516265869
Epoch 1510, val loss: 1.2767894268035889
Epoch 1520, training loss: 311.570556640625 = 0.12885773181915283 + 50.0 * 6.22883415222168
Epoch 1520, val loss: 1.2827385663986206
Epoch 1530, training loss: 311.2763671875 = 0.12595726549625397 + 50.0 * 6.223008155822754
Epoch 1530, val loss: 1.2881609201431274
Epoch 1540, training loss: 311.2671813964844 = 0.12318262457847595 + 50.0 * 6.2228803634643555
Epoch 1540, val loss: 1.2940593957901
Epoch 1550, training loss: 311.405029296875 = 0.12051848322153091 + 50.0 * 6.2256903648376465
Epoch 1550, val loss: 1.2999038696289062
Epoch 1560, training loss: 311.2568664550781 = 0.11785713583230972 + 50.0 * 6.222780227661133
Epoch 1560, val loss: 1.30552077293396
Epoch 1570, training loss: 311.2153625488281 = 0.11528140306472778 + 50.0 * 6.222001075744629
Epoch 1570, val loss: 1.311324954032898
Epoch 1580, training loss: 311.3290100097656 = 0.11279530078172684 + 50.0 * 6.2243242263793945
Epoch 1580, val loss: 1.3171430826187134
Epoch 1590, training loss: 311.4006652832031 = 0.11033349484205246 + 50.0 * 6.225807189941406
Epoch 1590, val loss: 1.3227235078811646
Epoch 1600, training loss: 311.1922302246094 = 0.10793798416852951 + 50.0 * 6.221685886383057
Epoch 1600, val loss: 1.328763723373413
Epoch 1610, training loss: 311.1508483886719 = 0.10561200231313705 + 50.0 * 6.22090482711792
Epoch 1610, val loss: 1.33450186252594
Epoch 1620, training loss: 311.11651611328125 = 0.10338125377893448 + 50.0 * 6.22026252746582
Epoch 1620, val loss: 1.3405741453170776
Epoch 1630, training loss: 311.15130615234375 = 0.10121368616819382 + 50.0 * 6.221001625061035
Epoch 1630, val loss: 1.3464998006820679
Epoch 1640, training loss: 311.3451843261719 = 0.09908180683851242 + 50.0 * 6.224921703338623
Epoch 1640, val loss: 1.3524545431137085
Epoch 1650, training loss: 311.16302490234375 = 0.09697391092777252 + 50.0 * 6.221321105957031
Epoch 1650, val loss: 1.3582223653793335
Epoch 1660, training loss: 311.1769714355469 = 0.09492503851652145 + 50.0 * 6.221640586853027
Epoch 1660, val loss: 1.364216923713684
Epoch 1670, training loss: 311.0646057128906 = 0.09292702376842499 + 50.0 * 6.219433784484863
Epoch 1670, val loss: 1.3699535131454468
Epoch 1680, training loss: 311.03204345703125 = 0.09101036190986633 + 50.0 * 6.218820571899414
Epoch 1680, val loss: 1.3760545253753662
Epoch 1690, training loss: 311.1085510253906 = 0.08914682269096375 + 50.0 * 6.220388412475586
Epoch 1690, val loss: 1.3819268941879272
Epoch 1700, training loss: 311.2660217285156 = 0.08730174601078033 + 50.0 * 6.223574161529541
Epoch 1700, val loss: 1.3877034187316895
Epoch 1710, training loss: 311.0318603515625 = 0.08549080789089203 + 50.0 * 6.218927383422852
Epoch 1710, val loss: 1.3937628269195557
Epoch 1720, training loss: 310.95526123046875 = 0.08374049514532089 + 50.0 * 6.217430114746094
Epoch 1720, val loss: 1.3997021913528442
Epoch 1730, training loss: 311.0254821777344 = 0.08206868171691895 + 50.0 * 6.218868255615234
Epoch 1730, val loss: 1.4057092666625977
Epoch 1740, training loss: 311.1067810058594 = 0.08040675520896912 + 50.0 * 6.220527172088623
Epoch 1740, val loss: 1.4115676879882812
Epoch 1750, training loss: 310.999755859375 = 0.07877657562494278 + 50.0 * 6.218419551849365
Epoch 1750, val loss: 1.41740882396698
Epoch 1760, training loss: 310.9761047363281 = 0.0772186890244484 + 50.0 * 6.217978000640869
Epoch 1760, val loss: 1.4235283136367798
Epoch 1770, training loss: 311.0560607910156 = 0.07569020986557007 + 50.0 * 6.219607353210449
Epoch 1770, val loss: 1.4294531345367432
Epoch 1780, training loss: 310.9190368652344 = 0.07418837398290634 + 50.0 * 6.216897010803223
Epoch 1780, val loss: 1.4352728128433228
Epoch 1790, training loss: 310.9552001953125 = 0.07273184508085251 + 50.0 * 6.217649459838867
Epoch 1790, val loss: 1.44122314453125
Epoch 1800, training loss: 310.9533996582031 = 0.07132486999034882 + 50.0 * 6.217641830444336
Epoch 1800, val loss: 1.447218656539917
Epoch 1810, training loss: 311.12017822265625 = 0.06993376463651657 + 50.0 * 6.221004486083984
Epoch 1810, val loss: 1.4530102014541626
Epoch 1820, training loss: 310.9015808105469 = 0.06853926926851273 + 50.0 * 6.216660976409912
Epoch 1820, val loss: 1.4585659503936768
Epoch 1830, training loss: 310.83355712890625 = 0.0672290027141571 + 50.0 * 6.21532678604126
Epoch 1830, val loss: 1.464698076248169
Epoch 1840, training loss: 310.9424133300781 = 0.06595815718173981 + 50.0 * 6.217529296875
Epoch 1840, val loss: 1.4705138206481934
Epoch 1850, training loss: 310.8539733886719 = 0.0646887868642807 + 50.0 * 6.215785503387451
Epoch 1850, val loss: 1.476405382156372
Epoch 1860, training loss: 310.8250427246094 = 0.0634642094373703 + 50.0 * 6.215231418609619
Epoch 1860, val loss: 1.4824421405792236
Epoch 1870, training loss: 310.8507385253906 = 0.06228000298142433 + 50.0 * 6.215768814086914
Epoch 1870, val loss: 1.4883323907852173
Epoch 1880, training loss: 311.0072021484375 = 0.06111970171332359 + 50.0 * 6.218921661376953
Epoch 1880, val loss: 1.4943524599075317
Epoch 1890, training loss: 310.94775390625 = 0.05998038873076439 + 50.0 * 6.2177557945251465
Epoch 1890, val loss: 1.5001459121704102
Epoch 1900, training loss: 310.8047790527344 = 0.05884098634123802 + 50.0 * 6.214918613433838
Epoch 1900, val loss: 1.505839467048645
Epoch 1910, training loss: 310.70556640625 = 0.05775932967662811 + 50.0 * 6.212955951690674
Epoch 1910, val loss: 1.5116560459136963
Epoch 1920, training loss: 310.73992919921875 = 0.056714385747909546 + 50.0 * 6.213664531707764
Epoch 1920, val loss: 1.517466425895691
Epoch 1930, training loss: 311.0242004394531 = 0.05569684877991676 + 50.0 * 6.219370365142822
Epoch 1930, val loss: 1.523064136505127
Epoch 1940, training loss: 310.7322998046875 = 0.05468267202377319 + 50.0 * 6.213552474975586
Epoch 1940, val loss: 1.529083013534546
Epoch 1950, training loss: 310.78546142578125 = 0.053694166243076324 + 50.0 * 6.214635848999023
Epoch 1950, val loss: 1.5346806049346924
Epoch 1960, training loss: 310.6851501464844 = 0.05272601917386055 + 50.0 * 6.212648391723633
Epoch 1960, val loss: 1.5404095649719238
Epoch 1970, training loss: 310.763427734375 = 0.05180957168340683 + 50.0 * 6.214231967926025
Epoch 1970, val loss: 1.546437382698059
Epoch 1980, training loss: 310.8327941894531 = 0.050888415426015854 + 50.0 * 6.215637683868408
Epoch 1980, val loss: 1.5517840385437012
Epoch 1990, training loss: 310.7459411621094 = 0.04999036714434624 + 50.0 * 6.213919162750244
Epoch 1990, val loss: 1.557476282119751
Epoch 2000, training loss: 310.62982177734375 = 0.0491168387234211 + 50.0 * 6.21161413192749
Epoch 2000, val loss: 1.563402771949768
Epoch 2010, training loss: 310.6100158691406 = 0.04827253147959709 + 50.0 * 6.2112345695495605
Epoch 2010, val loss: 1.5691455602645874
Epoch 2020, training loss: 310.780517578125 = 0.047469038516283035 + 50.0 * 6.214661121368408
Epoch 2020, val loss: 1.5750997066497803
Epoch 2030, training loss: 310.7655334472656 = 0.046642523258924484 + 50.0 * 6.2143778800964355
Epoch 2030, val loss: 1.5804443359375
Epoch 2040, training loss: 310.63677978515625 = 0.045830871909856796 + 50.0 * 6.211818695068359
Epoch 2040, val loss: 1.585672378540039
Epoch 2050, training loss: 310.570068359375 = 0.045058269053697586 + 50.0 * 6.210500240325928
Epoch 2050, val loss: 1.5916032791137695
Epoch 2060, training loss: 310.7839050292969 = 0.0443161241710186 + 50.0 * 6.214791774749756
Epoch 2060, val loss: 1.5971248149871826
Epoch 2070, training loss: 310.58270263671875 = 0.043559882789850235 + 50.0 * 6.210783004760742
Epoch 2070, val loss: 1.602319598197937
Epoch 2080, training loss: 310.564208984375 = 0.042832762002944946 + 50.0 * 6.210427284240723
Epoch 2080, val loss: 1.6079967021942139
Epoch 2090, training loss: 310.7589111328125 = 0.0421270988881588 + 50.0 * 6.214335918426514
Epoch 2090, val loss: 1.6132421493530273
Epoch 2100, training loss: 310.4982604980469 = 0.04142274335026741 + 50.0 * 6.209136962890625
Epoch 2100, val loss: 1.6188637018203735
Epoch 2110, training loss: 310.53570556640625 = 0.04075620695948601 + 50.0 * 6.209899425506592
Epoch 2110, val loss: 1.6245859861373901
Epoch 2120, training loss: 310.57904052734375 = 0.04010328650474548 + 50.0 * 6.210778713226318
Epoch 2120, val loss: 1.6300101280212402
Epoch 2130, training loss: 310.6526794433594 = 0.03945561870932579 + 50.0 * 6.212264537811279
Epoch 2130, val loss: 1.635310173034668
Epoch 2140, training loss: 310.56939697265625 = 0.03880486264824867 + 50.0 * 6.210611343383789
Epoch 2140, val loss: 1.640481948852539
Epoch 2150, training loss: 310.636474609375 = 0.038190294057130814 + 50.0 * 6.211966037750244
Epoch 2150, val loss: 1.6457425355911255
Epoch 2160, training loss: 310.4831237792969 = 0.037571489810943604 + 50.0 * 6.208911418914795
Epoch 2160, val loss: 1.6511485576629639
Epoch 2170, training loss: 310.5333557128906 = 0.036980222910642624 + 50.0 * 6.209927558898926
Epoch 2170, val loss: 1.6564421653747559
Epoch 2180, training loss: 310.788330078125 = 0.036392200738191605 + 50.0 * 6.215039253234863
Epoch 2180, val loss: 1.6612985134124756
Epoch 2190, training loss: 310.5133972167969 = 0.03581881895661354 + 50.0 * 6.2095513343811035
Epoch 2190, val loss: 1.6670539379119873
Epoch 2200, training loss: 310.449951171875 = 0.03525053337216377 + 50.0 * 6.208293914794922
Epoch 2200, val loss: 1.6721552610397339
Epoch 2210, training loss: 310.45751953125 = 0.03471718728542328 + 50.0 * 6.208455562591553
Epoch 2210, val loss: 1.6776920557022095
Epoch 2220, training loss: 310.63665771484375 = 0.03418907895684242 + 50.0 * 6.21204948425293
Epoch 2220, val loss: 1.6828453540802002
Epoch 2230, training loss: 310.4547424316406 = 0.033655665814876556 + 50.0 * 6.20842170715332
Epoch 2230, val loss: 1.687666893005371
Epoch 2240, training loss: 310.4874267578125 = 0.03313969448208809 + 50.0 * 6.209085941314697
Epoch 2240, val loss: 1.692699909210205
Epoch 2250, training loss: 310.5008544921875 = 0.03262826427817345 + 50.0 * 6.209364891052246
Epoch 2250, val loss: 1.6976960897445679
Epoch 2260, training loss: 310.416259765625 = 0.032139766961336136 + 50.0 * 6.2076826095581055
Epoch 2260, val loss: 1.7031532526016235
Epoch 2270, training loss: 310.5274353027344 = 0.03166676312685013 + 50.0 * 6.2099151611328125
Epoch 2270, val loss: 1.7083169221878052
Epoch 2280, training loss: 310.3846130371094 = 0.03118039108812809 + 50.0 * 6.20706844329834
Epoch 2280, val loss: 1.7128773927688599
Epoch 2290, training loss: 310.3691711425781 = 0.030722137540578842 + 50.0 * 6.20676851272583
Epoch 2290, val loss: 1.7181016206741333
Epoch 2300, training loss: 310.4984436035156 = 0.030278369784355164 + 50.0 * 6.2093634605407715
Epoch 2300, val loss: 1.7233072519302368
Epoch 2310, training loss: 310.39154052734375 = 0.029825948178768158 + 50.0 * 6.2072343826293945
Epoch 2310, val loss: 1.7280032634735107
Epoch 2320, training loss: 310.5409851074219 = 0.029397636651992798 + 50.0 * 6.210231781005859
Epoch 2320, val loss: 1.7331842184066772
Epoch 2330, training loss: 310.4293518066406 = 0.028955480083823204 + 50.0 * 6.2080078125
Epoch 2330, val loss: 1.7376574277877808
Epoch 2340, training loss: 310.34429931640625 = 0.028522219508886337 + 50.0 * 6.206315517425537
Epoch 2340, val loss: 1.742295503616333
Epoch 2350, training loss: 310.3016052246094 = 0.028123164549469948 + 50.0 * 6.205469608306885
Epoch 2350, val loss: 1.7475947141647339
Epoch 2360, training loss: 310.27484130859375 = 0.027726687490940094 + 50.0 * 6.204942226409912
Epoch 2360, val loss: 1.7523891925811768
Epoch 2370, training loss: 310.4893798828125 = 0.027345016598701477 + 50.0 * 6.209240913391113
Epoch 2370, val loss: 1.7571830749511719
Epoch 2380, training loss: 310.3084716796875 = 0.026946989819407463 + 50.0 * 6.205630302429199
Epoch 2380, val loss: 1.761793851852417
Epoch 2390, training loss: 310.2684020996094 = 0.026560833677649498 + 50.0 * 6.204837322235107
Epoch 2390, val loss: 1.7664365768432617
Epoch 2400, training loss: 310.2598876953125 = 0.026189448311924934 + 50.0 * 6.204674243927002
Epoch 2400, val loss: 1.771252155303955
Epoch 2410, training loss: 310.6221923828125 = 0.025834685191512108 + 50.0 * 6.2119269371032715
Epoch 2410, val loss: 1.7755206823349
Epoch 2420, training loss: 310.38482666015625 = 0.025468826293945312 + 50.0 * 6.207186698913574
Epoch 2420, val loss: 1.7800971269607544
Epoch 2430, training loss: 310.2839660644531 = 0.02510598674416542 + 50.0 * 6.205177307128906
Epoch 2430, val loss: 1.7849081754684448
Epoch 2440, training loss: 310.23785400390625 = 0.02476697601377964 + 50.0 * 6.204261779785156
Epoch 2440, val loss: 1.7898063659667969
Epoch 2450, training loss: 310.2887878417969 = 0.024437567219138145 + 50.0 * 6.205286979675293
Epoch 2450, val loss: 1.7942768335342407
Epoch 2460, training loss: 310.3778076171875 = 0.024107996374368668 + 50.0 * 6.20707368850708
Epoch 2460, val loss: 1.7986133098602295
Epoch 2470, training loss: 310.32080078125 = 0.023778265342116356 + 50.0 * 6.2059407234191895
Epoch 2470, val loss: 1.80343759059906
Epoch 2480, training loss: 310.30706787109375 = 0.023460587486624718 + 50.0 * 6.205672264099121
Epoch 2480, val loss: 1.807692289352417
Epoch 2490, training loss: 310.1944885253906 = 0.02314620651304722 + 50.0 * 6.203426837921143
Epoch 2490, val loss: 1.8125308752059937
Epoch 2500, training loss: 310.2411804199219 = 0.02285061404109001 + 50.0 * 6.204366683959961
Epoch 2500, val loss: 1.817160725593567
Epoch 2510, training loss: 310.3761291503906 = 0.02255985327064991 + 50.0 * 6.207071304321289
Epoch 2510, val loss: 1.8218203783035278
Epoch 2520, training loss: 310.3230895996094 = 0.022255653515458107 + 50.0 * 6.206017017364502
Epoch 2520, val loss: 1.8257911205291748
Epoch 2530, training loss: 310.2126770019531 = 0.021953796967864037 + 50.0 * 6.203814506530762
Epoch 2530, val loss: 1.829850673675537
Epoch 2540, training loss: 310.1768798828125 = 0.021669909358024597 + 50.0 * 6.203104496002197
Epoch 2540, val loss: 1.8343303203582764
Epoch 2550, training loss: 310.2507019042969 = 0.021397294476628304 + 50.0 * 6.204586029052734
Epoch 2550, val loss: 1.8388376235961914
Epoch 2560, training loss: 310.2497253417969 = 0.02112407237291336 + 50.0 * 6.2045722007751465
Epoch 2560, val loss: 1.8429627418518066
Epoch 2570, training loss: 310.2052917480469 = 0.02085183933377266 + 50.0 * 6.203689098358154
Epoch 2570, val loss: 1.8468040227890015
Epoch 2580, training loss: 310.4020690917969 = 0.020588913932442665 + 50.0 * 6.207629203796387
Epoch 2580, val loss: 1.8509762287139893
Epoch 2590, training loss: 310.1966247558594 = 0.020325902849435806 + 50.0 * 6.203526020050049
Epoch 2590, val loss: 1.8559792041778564
Epoch 2600, training loss: 310.10400390625 = 0.020068882033228874 + 50.0 * 6.201678276062012
Epoch 2600, val loss: 1.859849452972412
Epoch 2610, training loss: 310.10595703125 = 0.01982562616467476 + 50.0 * 6.201722621917725
Epoch 2610, val loss: 1.8640483617782593
Epoch 2620, training loss: 310.1459045410156 = 0.019588951021432877 + 50.0 * 6.202526092529297
Epoch 2620, val loss: 1.8683950901031494
Epoch 2630, training loss: 310.4339599609375 = 0.019349345937371254 + 50.0 * 6.208292007446289
Epoch 2630, val loss: 1.872008204460144
Epoch 2640, training loss: 310.20892333984375 = 0.019095567986369133 + 50.0 * 6.20379638671875
Epoch 2640, val loss: 1.8761582374572754
Epoch 2650, training loss: 310.100830078125 = 0.01885848306119442 + 50.0 * 6.201639175415039
Epoch 2650, val loss: 1.8802294731140137
Epoch 2660, training loss: 310.0813903808594 = 0.018632657825946808 + 50.0 * 6.201254844665527
Epoch 2660, val loss: 1.8846276998519897
Epoch 2670, training loss: 310.1220703125 = 0.018415046855807304 + 50.0 * 6.202073097229004
Epoch 2670, val loss: 1.888683795928955
Epoch 2680, training loss: 310.23956298828125 = 0.018199002370238304 + 50.0 * 6.204427242279053
Epoch 2680, val loss: 1.8925862312316895
Epoch 2690, training loss: 310.0570068359375 = 0.01797759346663952 + 50.0 * 6.200780868530273
Epoch 2690, val loss: 1.8962997198104858
Epoch 2700, training loss: 310.20916748046875 = 0.017770932987332344 + 50.0 * 6.203827857971191
Epoch 2700, val loss: 1.9002217054367065
Epoch 2710, training loss: 310.2285461425781 = 0.017562145367264748 + 50.0 * 6.204219818115234
Epoch 2710, val loss: 1.9042285680770874
Epoch 2720, training loss: 310.1351013183594 = 0.017353786155581474 + 50.0 * 6.202354431152344
Epoch 2720, val loss: 1.908471703529358
Epoch 2730, training loss: 310.2173767089844 = 0.017151985317468643 + 50.0 * 6.204004764556885
Epoch 2730, val loss: 1.912344217300415
Epoch 2740, training loss: 310.14141845703125 = 0.016954848542809486 + 50.0 * 6.202488899230957
Epoch 2740, val loss: 1.9163415431976318
Epoch 2750, training loss: 310.0784606933594 = 0.01675470732152462 + 50.0 * 6.201233863830566
Epoch 2750, val loss: 1.9198734760284424
Epoch 2760, training loss: 310.00714111328125 = 0.016563784331083298 + 50.0 * 6.1998114585876465
Epoch 2760, val loss: 1.9238260984420776
Epoch 2770, training loss: 310.01715087890625 = 0.01638171821832657 + 50.0 * 6.200015544891357
Epoch 2770, val loss: 1.9278292655944824
Epoch 2780, training loss: 310.2689514160156 = 0.016203468665480614 + 50.0 * 6.205054759979248
Epoch 2780, val loss: 1.9312047958374023
Epoch 2790, training loss: 310.08416748046875 = 0.016017211601138115 + 50.0 * 6.2013630867004395
Epoch 2790, val loss: 1.9349392652511597
Epoch 2800, training loss: 310.0531311035156 = 0.015836987644433975 + 50.0 * 6.200746059417725
Epoch 2800, val loss: 1.9391371011734009
Epoch 2810, training loss: 310.01873779296875 = 0.015661314129829407 + 50.0 * 6.200061798095703
Epoch 2810, val loss: 1.9428776502609253
Epoch 2820, training loss: 310.1096496582031 = 0.015495659783482552 + 50.0 * 6.201882839202881
Epoch 2820, val loss: 1.946722149848938
Epoch 2830, training loss: 310.077880859375 = 0.0153267290443182 + 50.0 * 6.201251029968262
Epoch 2830, val loss: 1.9502447843551636
Epoch 2840, training loss: 310.1821594238281 = 0.015153610147535801 + 50.0 * 6.203339576721191
Epoch 2840, val loss: 1.953352689743042
Epoch 2850, training loss: 309.98565673828125 = 0.014985805377364159 + 50.0 * 6.199413299560547
Epoch 2850, val loss: 1.9572769403457642
Epoch 2860, training loss: 309.9681091308594 = 0.01482721883803606 + 50.0 * 6.199065685272217
Epoch 2860, val loss: 1.9608131647109985
Epoch 2870, training loss: 310.1634826660156 = 0.014674731530249119 + 50.0 * 6.202976226806641
Epoch 2870, val loss: 1.964065670967102
Epoch 2880, training loss: 310.0234375 = 0.014508428983390331 + 50.0 * 6.200179100036621
Epoch 2880, val loss: 1.9677749872207642
Epoch 2890, training loss: 309.96099853515625 = 0.014352827332913876 + 50.0 * 6.198933124542236
Epoch 2890, val loss: 1.9711676836013794
Epoch 2900, training loss: 309.9635925292969 = 0.014201461337506771 + 50.0 * 6.19898796081543
Epoch 2900, val loss: 1.9748356342315674
Epoch 2910, training loss: 310.0108947753906 = 0.014057466760277748 + 50.0 * 6.199936389923096
Epoch 2910, val loss: 1.978642225265503
Epoch 2920, training loss: 310.0609130859375 = 0.013910435140132904 + 50.0 * 6.200939655303955
Epoch 2920, val loss: 1.981953501701355
Epoch 2930, training loss: 310.0455017089844 = 0.01376661378890276 + 50.0 * 6.200634479522705
Epoch 2930, val loss: 1.9854092597961426
Epoch 2940, training loss: 310.00799560546875 = 0.013620737008750439 + 50.0 * 6.199887752532959
Epoch 2940, val loss: 1.9885025024414062
Epoch 2950, training loss: 310.0307312011719 = 0.013481302186846733 + 50.0 * 6.200345039367676
Epoch 2950, val loss: 1.9920427799224854
Epoch 2960, training loss: 309.90069580078125 = 0.013342765159904957 + 50.0 * 6.197747230529785
Epoch 2960, val loss: 1.995849370956421
Epoch 2970, training loss: 309.9793395996094 = 0.013213575817644596 + 50.0 * 6.199322700500488
Epoch 2970, val loss: 1.999577522277832
Epoch 2980, training loss: 310.1308288574219 = 0.01308071706444025 + 50.0 * 6.202354431152344
Epoch 2980, val loss: 2.002763509750366
Epoch 2990, training loss: 309.95703125 = 0.012937048450112343 + 50.0 * 6.19888162612915
Epoch 2990, val loss: 2.0055060386657715
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6555555555555556
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 431.7885437011719 = 1.945243239402771 + 50.0 * 8.5968656539917
Epoch 0, val loss: 1.949265718460083
Epoch 10, training loss: 431.7459716796875 = 1.9365744590759277 + 50.0 * 8.596187591552734
Epoch 10, val loss: 1.94021737575531
Epoch 20, training loss: 431.4501647949219 = 1.9260094165802002 + 50.0 * 8.590483665466309
Epoch 20, val loss: 1.9291352033615112
Epoch 30, training loss: 429.1868591308594 = 1.9120633602142334 + 50.0 * 8.545495986938477
Epoch 30, val loss: 1.9146854877471924
Epoch 40, training loss: 414.2269287109375 = 1.8941807746887207 + 50.0 * 8.246654510498047
Epoch 40, val loss: 1.896809458732605
Epoch 50, training loss: 384.4488830566406 = 1.8748013973236084 + 50.0 * 7.651481628417969
Epoch 50, val loss: 1.8789291381835938
Epoch 60, training loss: 365.6873474121094 = 1.8630003929138184 + 50.0 * 7.276486873626709
Epoch 60, val loss: 1.868165373802185
Epoch 70, training loss: 352.1304016113281 = 1.8533295392990112 + 50.0 * 7.0055413246154785
Epoch 70, val loss: 1.8589297533035278
Epoch 80, training loss: 343.7666931152344 = 1.8450632095336914 + 50.0 * 6.838432788848877
Epoch 80, val loss: 1.8504087924957275
Epoch 90, training loss: 339.8499450683594 = 1.836345911026001 + 50.0 * 6.760272026062012
Epoch 90, val loss: 1.8413221836090088
Epoch 100, training loss: 336.7464599609375 = 1.8275679349899292 + 50.0 * 6.69837760925293
Epoch 100, val loss: 1.8324109315872192
Epoch 110, training loss: 334.35052490234375 = 1.8195265531539917 + 50.0 * 6.650619983673096
Epoch 110, val loss: 1.8242590427398682
Epoch 120, training loss: 332.32415771484375 = 1.8121157884597778 + 50.0 * 6.610240936279297
Epoch 120, val loss: 1.8165556192398071
Epoch 130, training loss: 330.63922119140625 = 1.8048852682113647 + 50.0 * 6.576686859130859
Epoch 130, val loss: 1.809048056602478
Epoch 140, training loss: 329.191650390625 = 1.7976579666137695 + 50.0 * 6.547879695892334
Epoch 140, val loss: 1.8016438484191895
Epoch 150, training loss: 328.0614929199219 = 1.790326476097107 + 50.0 * 6.525423526763916
Epoch 150, val loss: 1.794260025024414
Epoch 160, training loss: 326.95538330078125 = 1.782672643661499 + 50.0 * 6.503453731536865
Epoch 160, val loss: 1.78673255443573
Epoch 170, training loss: 326.0318908691406 = 1.7745779752731323 + 50.0 * 6.485146522521973
Epoch 170, val loss: 1.77892005443573
Epoch 180, training loss: 325.2086486816406 = 1.7657525539398193 + 50.0 * 6.468858242034912
Epoch 180, val loss: 1.7706549167633057
Epoch 190, training loss: 324.41778564453125 = 1.756208062171936 + 50.0 * 6.4532318115234375
Epoch 190, val loss: 1.7618792057037354
Epoch 200, training loss: 323.8979797363281 = 1.7458534240722656 + 50.0 * 6.443042755126953
Epoch 200, val loss: 1.752458095550537
Epoch 210, training loss: 323.14117431640625 = 1.7343348264694214 + 50.0 * 6.428136825561523
Epoch 210, val loss: 1.7421481609344482
Epoch 220, training loss: 322.71514892578125 = 1.721795678138733 + 50.0 * 6.419867038726807
Epoch 220, val loss: 1.7309662103652954
Epoch 230, training loss: 322.1878662109375 = 1.7080081701278687 + 50.0 * 6.409596920013428
Epoch 230, val loss: 1.7188831567764282
Epoch 240, training loss: 321.67401123046875 = 1.6930099725723267 + 50.0 * 6.3996195793151855
Epoch 240, val loss: 1.7057853937149048
Epoch 250, training loss: 321.28363037109375 = 1.6766794919967651 + 50.0 * 6.392139434814453
Epoch 250, val loss: 1.6915900707244873
Epoch 260, training loss: 320.87518310546875 = 1.6589994430541992 + 50.0 * 6.384323596954346
Epoch 260, val loss: 1.67623770236969
Epoch 270, training loss: 320.59857177734375 = 1.6399086713790894 + 50.0 * 6.3791728019714355
Epoch 270, val loss: 1.6597213745117188
Epoch 280, training loss: 320.2144470214844 = 1.6193780899047852 + 50.0 * 6.371901512145996
Epoch 280, val loss: 1.6420756578445435
Epoch 290, training loss: 319.8757629394531 = 1.5976362228393555 + 50.0 * 6.365562438964844
Epoch 290, val loss: 1.6235671043395996
Epoch 300, training loss: 319.713134765625 = 1.574689269065857 + 50.0 * 6.36276912689209
Epoch 300, val loss: 1.604129433631897
Epoch 310, training loss: 319.4436950683594 = 1.550290584564209 + 50.0 * 6.357868194580078
Epoch 310, val loss: 1.5836775302886963
Epoch 320, training loss: 319.06915283203125 = 1.5250955820083618 + 50.0 * 6.350881576538086
Epoch 320, val loss: 1.5625197887420654
Epoch 330, training loss: 318.779052734375 = 1.4990736246109009 + 50.0 * 6.34559965133667
Epoch 330, val loss: 1.5409666299819946
Epoch 340, training loss: 318.54364013671875 = 1.4723918437957764 + 50.0 * 6.34142541885376
Epoch 340, val loss: 1.5190798044204712
Epoch 350, training loss: 318.5796813964844 = 1.4449652433395386 + 50.0 * 6.34269380569458
Epoch 350, val loss: 1.4970747232437134
Epoch 360, training loss: 318.16033935546875 = 1.4171502590179443 + 50.0 * 6.334864139556885
Epoch 360, val loss: 1.4746097326278687
Epoch 370, training loss: 317.9579772949219 = 1.3892563581466675 + 50.0 * 6.331374645233154
Epoch 370, val loss: 1.4525130987167358
Epoch 380, training loss: 317.7004699707031 = 1.361263632774353 + 50.0 * 6.326784133911133
Epoch 380, val loss: 1.4307087659835815
Epoch 390, training loss: 317.8370056152344 = 1.3333184719085693 + 50.0 * 6.330073833465576
Epoch 390, val loss: 1.4090303182601929
Epoch 400, training loss: 317.3658752441406 = 1.3052804470062256 + 50.0 * 6.321212291717529
Epoch 400, val loss: 1.3880425691604614
Epoch 410, training loss: 317.15850830078125 = 1.2776850461959839 + 50.0 * 6.3176164627075195
Epoch 410, val loss: 1.3674589395523071
Epoch 420, training loss: 316.9759826660156 = 1.2503786087036133 + 50.0 * 6.314512252807617
Epoch 420, val loss: 1.347549557685852
Epoch 430, training loss: 316.8093566894531 = 1.2235512733459473 + 50.0 * 6.311716079711914
Epoch 430, val loss: 1.3283201456069946
Epoch 440, training loss: 316.69293212890625 = 1.1969748735427856 + 50.0 * 6.309919357299805
Epoch 440, val loss: 1.3097206354141235
Epoch 450, training loss: 316.5392761230469 = 1.170846939086914 + 50.0 * 6.307368755340576
Epoch 450, val loss: 1.291740894317627
Epoch 460, training loss: 316.39434814453125 = 1.1455285549163818 + 50.0 * 6.304975986480713
Epoch 460, val loss: 1.2746412754058838
Epoch 470, training loss: 316.2067565917969 = 1.1208810806274414 + 50.0 * 6.301717281341553
Epoch 470, val loss: 1.25859797000885
Epoch 480, training loss: 316.09075927734375 = 1.0969234704971313 + 50.0 * 6.299876689910889
Epoch 480, val loss: 1.2434073686599731
Epoch 490, training loss: 315.9383850097656 = 1.0735279321670532 + 50.0 * 6.297297477722168
Epoch 490, val loss: 1.2293453216552734
Epoch 500, training loss: 315.85406494140625 = 1.0508532524108887 + 50.0 * 6.296064376831055
Epoch 500, val loss: 1.2156600952148438
Epoch 510, training loss: 315.8751220703125 = 1.0290004014968872 + 50.0 * 6.29692268371582
Epoch 510, val loss: 1.2031809091567993
Epoch 520, training loss: 315.59442138671875 = 1.0077308416366577 + 50.0 * 6.291733741760254
Epoch 520, val loss: 1.1913985013961792
Epoch 530, training loss: 315.4504699707031 = 0.9873253107070923 + 50.0 * 6.289262771606445
Epoch 530, val loss: 1.180687427520752
Epoch 540, training loss: 315.6644592285156 = 0.9676252007484436 + 50.0 * 6.293936729431152
Epoch 540, val loss: 1.170978307723999
Epoch 550, training loss: 315.4227600097656 = 0.9482325911521912 + 50.0 * 6.289490222930908
Epoch 550, val loss: 1.1615010499954224
Epoch 560, training loss: 315.14111328125 = 0.929692268371582 + 50.0 * 6.284228801727295
Epoch 560, val loss: 1.1528393030166626
Epoch 570, training loss: 315.0263977050781 = 0.9117578268051147 + 50.0 * 6.28229284286499
Epoch 570, val loss: 1.1450265645980835
Epoch 580, training loss: 315.0851745605469 = 0.894333004951477 + 50.0 * 6.283816814422607
Epoch 580, val loss: 1.137701153755188
Epoch 590, training loss: 315.0385437011719 = 0.8773936629295349 + 50.0 * 6.2832231521606445
Epoch 590, val loss: 1.131439447402954
Epoch 600, training loss: 314.7637023925781 = 0.860922634601593 + 50.0 * 6.278055667877197
Epoch 600, val loss: 1.1248289346694946
Epoch 610, training loss: 314.7007141113281 = 0.8450103402137756 + 50.0 * 6.277114391326904
Epoch 610, val loss: 1.1193574666976929
Epoch 620, training loss: 314.7467041015625 = 0.8295183181762695 + 50.0 * 6.278343200683594
Epoch 620, val loss: 1.1140867471694946
Epoch 630, training loss: 314.572021484375 = 0.8143281936645508 + 50.0 * 6.275154113769531
Epoch 630, val loss: 1.10973060131073
Epoch 640, training loss: 314.4722900390625 = 0.7995554208755493 + 50.0 * 6.273454666137695
Epoch 640, val loss: 1.1050031185150146
Epoch 650, training loss: 314.8685607910156 = 0.7851464748382568 + 50.0 * 6.281668186187744
Epoch 650, val loss: 1.101089596748352
Epoch 660, training loss: 314.3542785644531 = 0.7709012031555176 + 50.0 * 6.27166748046875
Epoch 660, val loss: 1.097387671470642
Epoch 670, training loss: 314.1890869140625 = 0.7571066617965698 + 50.0 * 6.26863956451416
Epoch 670, val loss: 1.0941836833953857
Epoch 680, training loss: 314.13323974609375 = 0.7436832785606384 + 50.0 * 6.267791271209717
Epoch 680, val loss: 1.0914119482040405
Epoch 690, training loss: 314.2734069824219 = 0.7304943799972534 + 50.0 * 6.270858287811279
Epoch 690, val loss: 1.0889034271240234
Epoch 700, training loss: 314.0547180175781 = 0.7175859808921814 + 50.0 * 6.266742706298828
Epoch 700, val loss: 1.0862098932266235
Epoch 710, training loss: 313.9743957519531 = 0.7048185467720032 + 50.0 * 6.265391826629639
Epoch 710, val loss: 1.084234595298767
Epoch 720, training loss: 313.8634948730469 = 0.6924780607223511 + 50.0 * 6.263420104980469
Epoch 720, val loss: 1.0822561979293823
Epoch 730, training loss: 314.1941223144531 = 0.6803342700004578 + 50.0 * 6.270275592803955
Epoch 730, val loss: 1.0810046195983887
Epoch 740, training loss: 314.0154724121094 = 0.6680734157562256 + 50.0 * 6.266948223114014
Epoch 740, val loss: 1.078652262687683
Epoch 750, training loss: 313.7374267578125 = 0.6562438011169434 + 50.0 * 6.261623382568359
Epoch 750, val loss: 1.0770930051803589
Epoch 760, training loss: 313.59942626953125 = 0.6446224451065063 + 50.0 * 6.259096145629883
Epoch 760, val loss: 1.0761305093765259
Epoch 770, training loss: 313.5376892089844 = 0.6332574486732483 + 50.0 * 6.2580885887146
Epoch 770, val loss: 1.0751807689666748
Epoch 780, training loss: 314.238037109375 = 0.6220122575759888 + 50.0 * 6.272320747375488
Epoch 780, val loss: 1.0739636421203613
Epoch 790, training loss: 313.4513244628906 = 0.6107328534126282 + 50.0 * 6.25681209564209
Epoch 790, val loss: 1.0732760429382324
Epoch 800, training loss: 313.38323974609375 = 0.5997288227081299 + 50.0 * 6.255670070648193
Epoch 800, val loss: 1.0729366540908813
Epoch 810, training loss: 313.3306884765625 = 0.5889242887496948 + 50.0 * 6.25483512878418
Epoch 810, val loss: 1.0725725889205933
Epoch 820, training loss: 313.64422607421875 = 0.5782865285873413 + 50.0 * 6.261319160461426
Epoch 820, val loss: 1.071987509727478
Epoch 830, training loss: 313.5439758300781 = 0.5676072239875793 + 50.0 * 6.259527683258057
Epoch 830, val loss: 1.072221279144287
Epoch 840, training loss: 313.1797790527344 = 0.5571126341819763 + 50.0 * 6.252453327178955
Epoch 840, val loss: 1.071948766708374
Epoch 850, training loss: 313.1427001953125 = 0.5468339323997498 + 50.0 * 6.251916885375977
Epoch 850, val loss: 1.0719032287597656
Epoch 860, training loss: 313.0881652832031 = 0.5366560220718384 + 50.0 * 6.251030445098877
Epoch 860, val loss: 1.0722452402114868
Epoch 870, training loss: 313.1284484863281 = 0.5266244411468506 + 50.0 * 6.2520365715026855
Epoch 870, val loss: 1.0726085901260376
Epoch 880, training loss: 313.13720703125 = 0.5165999531745911 + 50.0 * 6.25241231918335
Epoch 880, val loss: 1.0730060338974
Epoch 890, training loss: 312.9701232910156 = 0.5066075921058655 + 50.0 * 6.249269962310791
Epoch 890, val loss: 1.0735193490982056
Epoch 900, training loss: 312.9414367675781 = 0.49686887860298157 + 50.0 * 6.248891830444336
Epoch 900, val loss: 1.0741347074508667
Epoch 910, training loss: 312.8669738769531 = 0.4872128665447235 + 50.0 * 6.247595310211182
Epoch 910, val loss: 1.074775218963623
Epoch 920, training loss: 312.7801208496094 = 0.4776667058467865 + 50.0 * 6.246048927307129
Epoch 920, val loss: 1.075713038444519
Epoch 930, training loss: 312.8616027832031 = 0.4682766795158386 + 50.0 * 6.247866630554199
Epoch 930, val loss: 1.0768095254898071
Epoch 940, training loss: 312.7630920410156 = 0.45894452929496765 + 50.0 * 6.246082782745361
Epoch 940, val loss: 1.077534556388855
Epoch 950, training loss: 312.8058166503906 = 0.4497002065181732 + 50.0 * 6.247122287750244
Epoch 950, val loss: 1.0785812139511108
Epoch 960, training loss: 312.6785888671875 = 0.44062161445617676 + 50.0 * 6.244759559631348
Epoch 960, val loss: 1.07967209815979
Epoch 970, training loss: 312.7943420410156 = 0.43172428011894226 + 50.0 * 6.247251987457275
Epoch 970, val loss: 1.0812640190124512
Epoch 980, training loss: 312.60498046875 = 0.42276933789253235 + 50.0 * 6.2436442375183105
Epoch 980, val loss: 1.0822465419769287
Epoch 990, training loss: 312.5974426269531 = 0.4140937924385071 + 50.0 * 6.243667125701904
Epoch 990, val loss: 1.0840190649032593
Epoch 1000, training loss: 312.5078125 = 0.40551480650901794 + 50.0 * 6.242045879364014
Epoch 1000, val loss: 1.0856431722640991
Epoch 1010, training loss: 312.4312744140625 = 0.39719441533088684 + 50.0 * 6.2406816482543945
Epoch 1010, val loss: 1.08780038356781
Epoch 1020, training loss: 312.4142761230469 = 0.3889595866203308 + 50.0 * 6.240506649017334
Epoch 1020, val loss: 1.0899373292922974
Epoch 1030, training loss: 312.828125 = 0.3808455467224121 + 50.0 * 6.248945236206055
Epoch 1030, val loss: 1.0921127796173096
Epoch 1040, training loss: 312.34619140625 = 0.3727579414844513 + 50.0 * 6.239468097686768
Epoch 1040, val loss: 1.093991994857788
Epoch 1050, training loss: 312.3606262207031 = 0.3648582398891449 + 50.0 * 6.239915370941162
Epoch 1050, val loss: 1.0964382886886597
Epoch 1060, training loss: 312.3004455566406 = 0.35719186067581177 + 50.0 * 6.238865375518799
Epoch 1060, val loss: 1.0991846323013306
Epoch 1070, training loss: 312.5311279296875 = 0.3496294617652893 + 50.0 * 6.2436299324035645
Epoch 1070, val loss: 1.1017534732818604
Epoch 1080, training loss: 312.3338623046875 = 0.34220367670059204 + 50.0 * 6.239833354949951
Epoch 1080, val loss: 1.1044162511825562
Epoch 1090, training loss: 312.2349853515625 = 0.33491232991218567 + 50.0 * 6.238001823425293
Epoch 1090, val loss: 1.107443928718567
Epoch 1100, training loss: 312.2567443847656 = 0.32783472537994385 + 50.0 * 6.2385783195495605
Epoch 1100, val loss: 1.1105269193649292
Epoch 1110, training loss: 312.1511535644531 = 0.3208300769329071 + 50.0 * 6.236606597900391
Epoch 1110, val loss: 1.1139161586761475
Epoch 1120, training loss: 312.393798828125 = 0.31403598189353943 + 50.0 * 6.24159574508667
Epoch 1120, val loss: 1.1173287630081177
Epoch 1130, training loss: 312.10589599609375 = 0.30721503496170044 + 50.0 * 6.235973358154297
Epoch 1130, val loss: 1.1201993227005005
Epoch 1140, training loss: 312.1363220214844 = 0.3006868362426758 + 50.0 * 6.23671293258667
Epoch 1140, val loss: 1.1238287687301636
Epoch 1150, training loss: 312.0581970214844 = 0.2942407429218292 + 50.0 * 6.235279083251953
Epoch 1150, val loss: 1.1273974180221558
Epoch 1160, training loss: 311.9907531738281 = 0.28795745968818665 + 50.0 * 6.234055995941162
Epoch 1160, val loss: 1.1314427852630615
Epoch 1170, training loss: 311.9539794921875 = 0.2818186283111572 + 50.0 * 6.233443260192871
Epoch 1170, val loss: 1.1352262496948242
Epoch 1180, training loss: 312.052978515625 = 0.27584365010261536 + 50.0 * 6.2355427742004395
Epoch 1180, val loss: 1.1393969058990479
Epoch 1190, training loss: 311.97283935546875 = 0.2699538767337799 + 50.0 * 6.234057426452637
Epoch 1190, val loss: 1.1434227228164673
Epoch 1200, training loss: 312.0123596191406 = 0.2641783356666565 + 50.0 * 6.234963417053223
Epoch 1200, val loss: 1.1476892232894897
Epoch 1210, training loss: 311.877685546875 = 0.258479505777359 + 50.0 * 6.232384204864502
Epoch 1210, val loss: 1.1519824266433716
Epoch 1220, training loss: 311.85205078125 = 0.2530260980129242 + 50.0 * 6.231980323791504
Epoch 1220, val loss: 1.1567902565002441
Epoch 1230, training loss: 312.0973205566406 = 0.24769002199172974 + 50.0 * 6.236992835998535
Epoch 1230, val loss: 1.1613267660140991
Epoch 1240, training loss: 311.82806396484375 = 0.24239641427993774 + 50.0 * 6.23171329498291
Epoch 1240, val loss: 1.1657631397247314
Epoch 1250, training loss: 311.74658203125 = 0.23727114498615265 + 50.0 * 6.2301859855651855
Epoch 1250, val loss: 1.170861840248108
Epoch 1260, training loss: 312.1885681152344 = 0.23233039677143097 + 50.0 * 6.239124774932861
Epoch 1260, val loss: 1.1761667728424072
Epoch 1270, training loss: 311.872314453125 = 0.2273026555776596 + 50.0 * 6.232900619506836
Epoch 1270, val loss: 1.1803749799728394
Epoch 1280, training loss: 311.7299499511719 = 0.2225213199853897 + 50.0 * 6.2301483154296875
Epoch 1280, val loss: 1.1859416961669922
Epoch 1290, training loss: 311.6539306640625 = 0.21783150732517242 + 50.0 * 6.228721618652344
Epoch 1290, val loss: 1.191123604774475
Epoch 1300, training loss: 312.0966796875 = 0.21329346299171448 + 50.0 * 6.237667560577393
Epoch 1300, val loss: 1.1967461109161377
Epoch 1310, training loss: 311.7644958496094 = 0.20882320404052734 + 50.0 * 6.231113433837891
Epoch 1310, val loss: 1.2014013528823853
Epoch 1320, training loss: 311.63323974609375 = 0.20440851151943207 + 50.0 * 6.22857666015625
Epoch 1320, val loss: 1.2071517705917358
Epoch 1330, training loss: 311.568115234375 = 0.2001693695783615 + 50.0 * 6.227358818054199
Epoch 1330, val loss: 1.2125827074050903
Epoch 1340, training loss: 311.69091796875 = 0.19603130221366882 + 50.0 * 6.229897499084473
Epoch 1340, val loss: 1.2181859016418457
Epoch 1350, training loss: 311.5476379394531 = 0.19191229343414307 + 50.0 * 6.227114200592041
Epoch 1350, val loss: 1.2237743139266968
Epoch 1360, training loss: 311.53582763671875 = 0.1878686547279358 + 50.0 * 6.226959228515625
Epoch 1360, val loss: 1.2293024063110352
Epoch 1370, training loss: 311.6460876464844 = 0.18395879864692688 + 50.0 * 6.229242324829102
Epoch 1370, val loss: 1.2350701093673706
Epoch 1380, training loss: 311.5005798339844 = 0.18012869358062744 + 50.0 * 6.226409435272217
Epoch 1380, val loss: 1.2410147190093994
Epoch 1390, training loss: 311.4847717285156 = 0.17640964686870575 + 50.0 * 6.22616720199585
Epoch 1390, val loss: 1.2472940683364868
Epoch 1400, training loss: 311.4407653808594 = 0.17277655005455017 + 50.0 * 6.2253594398498535
Epoch 1400, val loss: 1.2531119585037231
Epoch 1410, training loss: 311.7055969238281 = 0.16920816898345947 + 50.0 * 6.230727672576904
Epoch 1410, val loss: 1.2591215372085571
Epoch 1420, training loss: 311.55389404296875 = 0.1657344251871109 + 50.0 * 6.2277631759643555
Epoch 1420, val loss: 1.265693187713623
Epoch 1430, training loss: 311.64154052734375 = 0.16230320930480957 + 50.0 * 6.22958517074585
Epoch 1430, val loss: 1.2715463638305664
Epoch 1440, training loss: 311.41339111328125 = 0.15890508890151978 + 50.0 * 6.2250895500183105
Epoch 1440, val loss: 1.27817702293396
Epoch 1450, training loss: 311.34100341796875 = 0.15564186871051788 + 50.0 * 6.22370719909668
Epoch 1450, val loss: 1.284636378288269
Epoch 1460, training loss: 311.3660888671875 = 0.15249060094356537 + 50.0 * 6.224271774291992
Epoch 1460, val loss: 1.2913851737976074
Epoch 1470, training loss: 311.5137023925781 = 0.1493763029575348 + 50.0 * 6.227286338806152
Epoch 1470, val loss: 1.2978637218475342
Epoch 1480, training loss: 311.3151550292969 = 0.14629492163658142 + 50.0 * 6.223377227783203
Epoch 1480, val loss: 1.3044040203094482
Epoch 1490, training loss: 311.4001770019531 = 0.14332211017608643 + 50.0 * 6.225137233734131
Epoch 1490, val loss: 1.3112528324127197
Epoch 1500, training loss: 311.3821105957031 = 0.14039082825183868 + 50.0 * 6.224834442138672
Epoch 1500, val loss: 1.3181182146072388
Epoch 1510, training loss: 311.33837890625 = 0.1375100016593933 + 50.0 * 6.22401762008667
Epoch 1510, val loss: 1.3243767023086548
Epoch 1520, training loss: 311.2735595703125 = 0.13473443686962128 + 50.0 * 6.222776412963867
Epoch 1520, val loss: 1.331905722618103
Epoch 1530, training loss: 311.5061950683594 = 0.13200195133686066 + 50.0 * 6.227484226226807
Epoch 1530, val loss: 1.3385796546936035
Epoch 1540, training loss: 311.2632141113281 = 0.12933723628520966 + 50.0 * 6.222677707672119
Epoch 1540, val loss: 1.3452389240264893
Epoch 1550, training loss: 311.1602783203125 = 0.12671175599098206 + 50.0 * 6.2206711769104
Epoch 1550, val loss: 1.3522206544876099
Epoch 1560, training loss: 311.1363830566406 = 0.12417450547218323 + 50.0 * 6.220244407653809
Epoch 1560, val loss: 1.35959792137146
Epoch 1570, training loss: 311.1700134277344 = 0.12171290069818497 + 50.0 * 6.220966339111328
Epoch 1570, val loss: 1.3666857481002808
Epoch 1580, training loss: 311.47015380859375 = 0.11929218471050262 + 50.0 * 6.227017402648926
Epoch 1580, val loss: 1.3736598491668701
Epoch 1590, training loss: 311.2415771484375 = 0.11686277389526367 + 50.0 * 6.222494602203369
Epoch 1590, val loss: 1.3806934356689453
Epoch 1600, training loss: 311.37860107421875 = 0.11454471200704575 + 50.0 * 6.225281238555908
Epoch 1600, val loss: 1.3879990577697754
Epoch 1610, training loss: 311.0843200683594 = 0.1122230812907219 + 50.0 * 6.219442367553711
Epoch 1610, val loss: 1.394999384880066
Epoch 1620, training loss: 311.0618896484375 = 0.11000420898199081 + 50.0 * 6.2190375328063965
Epoch 1620, val loss: 1.4023011922836304
Epoch 1630, training loss: 311.0759582519531 = 0.10785267502069473 + 50.0 * 6.219362258911133
Epoch 1630, val loss: 1.4100168943405151
Epoch 1640, training loss: 311.4688720703125 = 0.10576006770133972 + 50.0 * 6.227262020111084
Epoch 1640, val loss: 1.4173256158828735
Epoch 1650, training loss: 311.1752014160156 = 0.10362608730792999 + 50.0 * 6.221431255340576
Epoch 1650, val loss: 1.4243627786636353
Epoch 1660, training loss: 311.10089111328125 = 0.10159768909215927 + 50.0 * 6.2199859619140625
Epoch 1660, val loss: 1.4320900440216064
Epoch 1670, training loss: 311.26458740234375 = 0.09960035234689713 + 50.0 * 6.223299503326416
Epoch 1670, val loss: 1.4396042823791504
Epoch 1680, training loss: 311.06634521484375 = 0.09762680530548096 + 50.0 * 6.219374656677246
Epoch 1680, val loss: 1.4462957382202148
Epoch 1690, training loss: 310.9654235839844 = 0.09573175758123398 + 50.0 * 6.21739387512207
Epoch 1690, val loss: 1.454108715057373
Epoch 1700, training loss: 310.94091796875 = 0.09387831389904022 + 50.0 * 6.216940879821777
Epoch 1700, val loss: 1.4619601964950562
Epoch 1710, training loss: 310.9843444824219 = 0.09207174926996231 + 50.0 * 6.217845439910889
Epoch 1710, val loss: 1.4695384502410889
Epoch 1720, training loss: 311.2007141113281 = 0.09030678868293762 + 50.0 * 6.222208023071289
Epoch 1720, val loss: 1.4770911931991577
Epoch 1730, training loss: 310.994873046875 = 0.08852333575487137 + 50.0 * 6.2181267738342285
Epoch 1730, val loss: 1.4840469360351562
Epoch 1740, training loss: 311.02081298828125 = 0.08681126683950424 + 50.0 * 6.218679904937744
Epoch 1740, val loss: 1.4918227195739746
Epoch 1750, training loss: 310.8696594238281 = 0.08515512943267822 + 50.0 * 6.2156901359558105
Epoch 1750, val loss: 1.4995019435882568
Epoch 1760, training loss: 310.8909912109375 = 0.0835530087351799 + 50.0 * 6.216148853302002
Epoch 1760, val loss: 1.5070205926895142
Epoch 1770, training loss: 311.0826416015625 = 0.08198531717061996 + 50.0 * 6.220013618469238
Epoch 1770, val loss: 1.5145539045333862
Epoch 1780, training loss: 311.0289611816406 = 0.08040157705545425 + 50.0 * 6.218970775604248
Epoch 1780, val loss: 1.5223426818847656
Epoch 1790, training loss: 310.84210205078125 = 0.07886634021997452 + 50.0 * 6.215264797210693
Epoch 1790, val loss: 1.5296088457107544
Epoch 1800, training loss: 310.8160705566406 = 0.0773930773139 + 50.0 * 6.214773654937744
Epoch 1800, val loss: 1.5375518798828125
Epoch 1810, training loss: 311.14215087890625 = 0.0759739950299263 + 50.0 * 6.221323490142822
Epoch 1810, val loss: 1.5455079078674316
Epoch 1820, training loss: 310.8735046386719 = 0.07451044768095016 + 50.0 * 6.21597957611084
Epoch 1820, val loss: 1.5520837306976318
Epoch 1830, training loss: 310.8236389160156 = 0.0731167197227478 + 50.0 * 6.215010166168213
Epoch 1830, val loss: 1.5601667165756226
Epoch 1840, training loss: 310.81707763671875 = 0.07177400588989258 + 50.0 * 6.214905738830566
Epoch 1840, val loss: 1.567584753036499
Epoch 1850, training loss: 311.00457763671875 = 0.07045633345842361 + 50.0 * 6.218682289123535
Epoch 1850, val loss: 1.5749688148498535
Epoch 1860, training loss: 310.8111267089844 = 0.06914685666561127 + 50.0 * 6.214839458465576
Epoch 1860, val loss: 1.582844853401184
Epoch 1870, training loss: 310.80584716796875 = 0.0678783729672432 + 50.0 * 6.214759349822998
Epoch 1870, val loss: 1.5902658700942993
Epoch 1880, training loss: 310.94342041015625 = 0.06664127856492996 + 50.0 * 6.217535495758057
Epoch 1880, val loss: 1.598435878753662
Epoch 1890, training loss: 310.8666687011719 = 0.06542039662599564 + 50.0 * 6.216024875640869
Epoch 1890, val loss: 1.6055140495300293
Epoch 1900, training loss: 310.8537292480469 = 0.06423237919807434 + 50.0 * 6.215789794921875
Epoch 1900, val loss: 1.6128361225128174
Epoch 1910, training loss: 310.72894287109375 = 0.06306658685207367 + 50.0 * 6.213317394256592
Epoch 1910, val loss: 1.6206707954406738
Epoch 1920, training loss: 310.725341796875 = 0.061936695128679276 + 50.0 * 6.213267803192139
Epoch 1920, val loss: 1.6280415058135986
Epoch 1930, training loss: 310.9249572753906 = 0.060829974710941315 + 50.0 * 6.217282772064209
Epoch 1930, val loss: 1.6353071928024292
Epoch 1940, training loss: 310.7166442871094 = 0.05975063145160675 + 50.0 * 6.213138103485107
Epoch 1940, val loss: 1.6434693336486816
Epoch 1950, training loss: 310.7041931152344 = 0.05868363007903099 + 50.0 * 6.212910175323486
Epoch 1950, val loss: 1.650532841682434
Epoch 1960, training loss: 310.7251281738281 = 0.0576455257833004 + 50.0 * 6.21334981918335
Epoch 1960, val loss: 1.6580541133880615
Epoch 1970, training loss: 310.7154541015625 = 0.05664162337779999 + 50.0 * 6.213176250457764
Epoch 1970, val loss: 1.6659921407699585
Epoch 1980, training loss: 310.8023681640625 = 0.05564915016293526 + 50.0 * 6.214934349060059
Epoch 1980, val loss: 1.6738276481628418
Epoch 1990, training loss: 310.63134765625 = 0.05466252192854881 + 50.0 * 6.211534023284912
Epoch 1990, val loss: 1.6811720132827759
Epoch 2000, training loss: 310.6781921386719 = 0.05371891334652901 + 50.0 * 6.212489604949951
Epoch 2000, val loss: 1.6889973878860474
Epoch 2010, training loss: 310.7800598144531 = 0.05279859900474548 + 50.0 * 6.214545249938965
Epoch 2010, val loss: 1.6964771747589111
Epoch 2020, training loss: 310.5918273925781 = 0.05186695605516434 + 50.0 * 6.210798740386963
Epoch 2020, val loss: 1.7044249773025513
Epoch 2030, training loss: 310.8130798339844 = 0.050990812480449677 + 50.0 * 6.2152419090271
Epoch 2030, val loss: 1.7125859260559082
Epoch 2040, training loss: 310.6155700683594 = 0.050108086317777634 + 50.0 * 6.211309432983398
Epoch 2040, val loss: 1.7192022800445557
Epoch 2050, training loss: 310.5682067871094 = 0.0492427758872509 + 50.0 * 6.210379123687744
Epoch 2050, val loss: 1.7274725437164307
Epoch 2060, training loss: 310.5458679199219 = 0.048412173986434937 + 50.0 * 6.209949493408203
Epoch 2060, val loss: 1.735176682472229
Epoch 2070, training loss: 310.57611083984375 = 0.04761475324630737 + 50.0 * 6.210570335388184
Epoch 2070, val loss: 1.7430135011672974
Epoch 2080, training loss: 310.8249816894531 = 0.04682521894574165 + 50.0 * 6.2155632972717285
Epoch 2080, val loss: 1.7507399320602417
Epoch 2090, training loss: 310.588134765625 = 0.046012841165065765 + 50.0 * 6.210842132568359
Epoch 2090, val loss: 1.7573217153549194
Epoch 2100, training loss: 310.48773193359375 = 0.04523887112736702 + 50.0 * 6.208849906921387
Epoch 2100, val loss: 1.7646536827087402
Epoch 2110, training loss: 310.5586853027344 = 0.04449918121099472 + 50.0 * 6.2102837562561035
Epoch 2110, val loss: 1.7725311517715454
Epoch 2120, training loss: 310.699462890625 = 0.04377108812332153 + 50.0 * 6.213114261627197
Epoch 2120, val loss: 1.779574990272522
Epoch 2130, training loss: 310.588134765625 = 0.04304558411240578 + 50.0 * 6.210901260375977
Epoch 2130, val loss: 1.7871670722961426
Epoch 2140, training loss: 310.4955749511719 = 0.0423419326543808 + 50.0 * 6.209064960479736
Epoch 2140, val loss: 1.7945480346679688
Epoch 2150, training loss: 310.5028381347656 = 0.04165898263454437 + 50.0 * 6.209223747253418
Epoch 2150, val loss: 1.8017479181289673
Epoch 2160, training loss: 310.5536193847656 = 0.04099160432815552 + 50.0 * 6.21025276184082
Epoch 2160, val loss: 1.8087526559829712
Epoch 2170, training loss: 310.684814453125 = 0.04033812880516052 + 50.0 * 6.212889194488525
Epoch 2170, val loss: 1.8162614107131958
Epoch 2180, training loss: 310.5730285644531 = 0.03968773037195206 + 50.0 * 6.210667133331299
Epoch 2180, val loss: 1.8233755826950073
Epoch 2190, training loss: 310.5507507324219 = 0.039038173854351044 + 50.0 * 6.21023416519165
Epoch 2190, val loss: 1.8309376239776611
Epoch 2200, training loss: 310.4521179199219 = 0.03840562701225281 + 50.0 * 6.2082743644714355
Epoch 2200, val loss: 1.8376009464263916
Epoch 2210, training loss: 310.390380859375 = 0.037805717438459396 + 50.0 * 6.2070512771606445
Epoch 2210, val loss: 1.8453748226165771
Epoch 2220, training loss: 310.3683166503906 = 0.03722340986132622 + 50.0 * 6.2066216468811035
Epoch 2220, val loss: 1.8524141311645508
Epoch 2230, training loss: 310.4544982910156 = 0.03665884584188461 + 50.0 * 6.208356857299805
Epoch 2230, val loss: 1.8596738576889038
Epoch 2240, training loss: 310.6050109863281 = 0.03609387204051018 + 50.0 * 6.21137809753418
Epoch 2240, val loss: 1.8659125566482544
Epoch 2250, training loss: 310.5539855957031 = 0.03551811724901199 + 50.0 * 6.210369110107422
Epoch 2250, val loss: 1.8724679946899414
Epoch 2260, training loss: 310.3623352050781 = 0.03494957461953163 + 50.0 * 6.206547737121582
Epoch 2260, val loss: 1.879353642463684
Epoch 2270, training loss: 310.34906005859375 = 0.03441822901368141 + 50.0 * 6.206292629241943
Epoch 2270, val loss: 1.886421799659729
Epoch 2280, training loss: 310.3356018066406 = 0.03389891982078552 + 50.0 * 6.206034183502197
Epoch 2280, val loss: 1.8931102752685547
Epoch 2290, training loss: 310.68243408203125 = 0.033394429832696915 + 50.0 * 6.2129807472229
Epoch 2290, val loss: 1.8998337984085083
Epoch 2300, training loss: 310.5542297363281 = 0.03289230540394783 + 50.0 * 6.2104268074035645
Epoch 2300, val loss: 1.9061907529830933
Epoch 2310, training loss: 310.37939453125 = 0.032371316105127335 + 50.0 * 6.206940174102783
Epoch 2310, val loss: 1.9122945070266724
Epoch 2320, training loss: 310.3177795410156 = 0.03189399093389511 + 50.0 * 6.20571756362915
Epoch 2320, val loss: 1.9193824529647827
Epoch 2330, training loss: 310.32952880859375 = 0.031421415507793427 + 50.0 * 6.20596170425415
Epoch 2330, val loss: 1.9260882139205933
Epoch 2340, training loss: 310.6348571777344 = 0.030973413959145546 + 50.0 * 6.212077617645264
Epoch 2340, val loss: 1.9318469762802124
Epoch 2350, training loss: 310.5812072753906 = 0.030504519119858742 + 50.0 * 6.2110137939453125
Epoch 2350, val loss: 1.9380712509155273
Epoch 2360, training loss: 310.34332275390625 = 0.030047470703721046 + 50.0 * 6.206264972686768
Epoch 2360, val loss: 1.9451180696487427
Epoch 2370, training loss: 310.2694091796875 = 0.029611125588417053 + 50.0 * 6.204795837402344
Epoch 2370, val loss: 1.9515002965927124
Epoch 2380, training loss: 310.26800537109375 = 0.029191872105002403 + 50.0 * 6.204776287078857
Epoch 2380, val loss: 1.957864761352539
Epoch 2390, training loss: 310.5650329589844 = 0.028783094137907028 + 50.0 * 6.210724830627441
Epoch 2390, val loss: 1.9640552997589111
Epoch 2400, training loss: 310.48138427734375 = 0.028362048789858818 + 50.0 * 6.2090606689453125
Epoch 2400, val loss: 1.9702801704406738
Epoch 2410, training loss: 310.4194030761719 = 0.02794564515352249 + 50.0 * 6.207828998565674
Epoch 2410, val loss: 1.9753624200820923
Epoch 2420, training loss: 310.2723693847656 = 0.027546148747205734 + 50.0 * 6.204896450042725
Epoch 2420, val loss: 1.9824345111846924
Epoch 2430, training loss: 310.2341003417969 = 0.027158334851264954 + 50.0 * 6.20413875579834
Epoch 2430, val loss: 1.9885579347610474
Epoch 2440, training loss: 310.2276611328125 = 0.026786986738443375 + 50.0 * 6.204017639160156
Epoch 2440, val loss: 1.9943146705627441
Epoch 2450, training loss: 310.60589599609375 = 0.02642817050218582 + 50.0 * 6.211589336395264
Epoch 2450, val loss: 1.99996018409729
Epoch 2460, training loss: 310.2919006347656 = 0.026041429489850998 + 50.0 * 6.205317497253418
Epoch 2460, val loss: 2.0065054893493652
Epoch 2470, training loss: 310.2844543457031 = 0.025680385529994965 + 50.0 * 6.205175876617432
Epoch 2470, val loss: 2.011518716812134
Epoch 2480, training loss: 310.3602600097656 = 0.02532694861292839 + 50.0 * 6.206698417663574
Epoch 2480, val loss: 2.0177669525146484
Epoch 2490, training loss: 310.3460388183594 = 0.02498510479927063 + 50.0 * 6.2064208984375
Epoch 2490, val loss: 2.023803472518921
Epoch 2500, training loss: 310.1832580566406 = 0.024639904499053955 + 50.0 * 6.20317268371582
Epoch 2500, val loss: 2.0294575691223145
Epoch 2510, training loss: 310.1950378417969 = 0.02430659532546997 + 50.0 * 6.2034149169921875
Epoch 2510, val loss: 2.034959077835083
Epoch 2520, training loss: 310.1562805175781 = 0.023984268307685852 + 50.0 * 6.202645778656006
Epoch 2520, val loss: 2.0412423610687256
Epoch 2530, training loss: 310.3028564453125 = 0.023674635216593742 + 50.0 * 6.205583572387695
Epoch 2530, val loss: 2.0470776557922363
Epoch 2540, training loss: 310.3450927734375 = 0.023357955738902092 + 50.0 * 6.206435203552246
Epoch 2540, val loss: 2.0515003204345703
Epoch 2550, training loss: 310.2874755859375 = 0.023037856444716454 + 50.0 * 6.205288410186768
Epoch 2550, val loss: 2.0574729442596436
Epoch 2560, training loss: 310.1673278808594 = 0.022730132564902306 + 50.0 * 6.202891826629639
Epoch 2560, val loss: 2.062744617462158
Epoch 2570, training loss: 310.20709228515625 = 0.022438153624534607 + 50.0 * 6.203693389892578
Epoch 2570, val loss: 2.068187713623047
Epoch 2580, training loss: 310.1617431640625 = 0.022143786773085594 + 50.0 * 6.202791690826416
Epoch 2580, val loss: 2.0736892223358154
Epoch 2590, training loss: 310.1231689453125 = 0.021859101951122284 + 50.0 * 6.2020263671875
Epoch 2590, val loss: 2.0790493488311768
Epoch 2600, training loss: 310.1413879394531 = 0.021584657952189445 + 50.0 * 6.202396392822266
Epoch 2600, val loss: 2.0844929218292236
Epoch 2610, training loss: 310.261962890625 = 0.021318675950169563 + 50.0 * 6.204812526702881
Epoch 2610, val loss: 2.0898232460021973
Epoch 2620, training loss: 310.47076416015625 = 0.021044950932264328 + 50.0 * 6.208994388580322
Epoch 2620, val loss: 2.094435453414917
Epoch 2630, training loss: 310.2288513183594 = 0.020759008824825287 + 50.0 * 6.204162120819092
Epoch 2630, val loss: 2.098942518234253
Epoch 2640, training loss: 310.1391296386719 = 0.020496368408203125 + 50.0 * 6.2023725509643555
Epoch 2640, val loss: 2.1049275398254395
Epoch 2650, training loss: 310.1380310058594 = 0.02024068683385849 + 50.0 * 6.202355861663818
Epoch 2650, val loss: 2.109476327896118
Epoch 2660, training loss: 310.1416931152344 = 0.01999298669397831 + 50.0 * 6.202434062957764
Epoch 2660, val loss: 2.1152408123016357
Epoch 2670, training loss: 310.0796203613281 = 0.019746800884604454 + 50.0 * 6.201197624206543
Epoch 2670, val loss: 2.119828224182129
Epoch 2680, training loss: 310.11700439453125 = 0.01950964704155922 + 50.0 * 6.2019500732421875
Epoch 2680, val loss: 2.1246917247772217
Epoch 2690, training loss: 310.29656982421875 = 0.01927732676267624 + 50.0 * 6.205545425415039
Epoch 2690, val loss: 2.1291849613189697
Epoch 2700, training loss: 310.3289794921875 = 0.019038459286093712 + 50.0 * 6.206198692321777
Epoch 2700, val loss: 2.133638620376587
Epoch 2710, training loss: 310.18603515625 = 0.018797462806105614 + 50.0 * 6.203344821929932
Epoch 2710, val loss: 2.138777494430542
Epoch 2720, training loss: 310.0705261230469 = 0.0185712780803442 + 50.0 * 6.201038837432861
Epoch 2720, val loss: 2.143550157546997
Epoch 2730, training loss: 310.00372314453125 = 0.01835116185247898 + 50.0 * 6.199707508087158
Epoch 2730, val loss: 2.148592233657837
Epoch 2740, training loss: 310.0220947265625 = 0.01814298704266548 + 50.0 * 6.20007848739624
Epoch 2740, val loss: 2.1530890464782715
Epoch 2750, training loss: 310.3467102050781 = 0.017943445593118668 + 50.0 * 6.206575393676758
Epoch 2750, val loss: 2.157785415649414
Epoch 2760, training loss: 310.1844177246094 = 0.017716174945235252 + 50.0 * 6.203333854675293
Epoch 2760, val loss: 2.1628503799438477
Epoch 2770, training loss: 310.01708984375 = 0.0175043735653162 + 50.0 * 6.199991226196289
Epoch 2770, val loss: 2.1664397716522217
Epoch 2780, training loss: 310.04931640625 = 0.017298627644777298 + 50.0 * 6.2006402015686035
Epoch 2780, val loss: 2.1713144779205322
Epoch 2790, training loss: 310.2588806152344 = 0.01710347644984722 + 50.0 * 6.204835891723633
Epoch 2790, val loss: 2.175307035446167
Epoch 2800, training loss: 310.1349792480469 = 0.016896052286028862 + 50.0 * 6.202361583709717
Epoch 2800, val loss: 2.1795527935028076
Epoch 2810, training loss: 310.0130310058594 = 0.01670427806675434 + 50.0 * 6.199926853179932
Epoch 2810, val loss: 2.1843578815460205
Epoch 2820, training loss: 309.9606018066406 = 0.016515888273715973 + 50.0 * 6.19888162612915
Epoch 2820, val loss: 2.1888880729675293
Epoch 2830, training loss: 310.0057678222656 = 0.016335319727659225 + 50.0 * 6.199788570404053
Epoch 2830, val loss: 2.1932365894317627
Epoch 2840, training loss: 310.2394714355469 = 0.016157569363713264 + 50.0 * 6.204466342926025
Epoch 2840, val loss: 2.1975018978118896
Epoch 2850, training loss: 310.16412353515625 = 0.015972260385751724 + 50.0 * 6.202963352203369
Epoch 2850, val loss: 2.200998306274414
Epoch 2860, training loss: 310.07757568359375 = 0.015788782387971878 + 50.0 * 6.201235771179199
Epoch 2860, val loss: 2.2048990726470947
Epoch 2870, training loss: 310.05670166015625 = 0.015615887939929962 + 50.0 * 6.200821399688721
Epoch 2870, val loss: 2.209451913833618
Epoch 2880, training loss: 310.019287109375 = 0.015447164885699749 + 50.0 * 6.200077056884766
Epoch 2880, val loss: 2.2136001586914062
Epoch 2890, training loss: 309.9657287597656 = 0.015277232974767685 + 50.0 * 6.199008941650391
Epoch 2890, val loss: 2.2180535793304443
Epoch 2900, training loss: 309.98077392578125 = 0.015115759335458279 + 50.0 * 6.199313640594482
Epoch 2900, val loss: 2.222212076187134
Epoch 2910, training loss: 310.17205810546875 = 0.01496351882815361 + 50.0 * 6.203142166137695
Epoch 2910, val loss: 2.2263729572296143
Epoch 2920, training loss: 310.0233154296875 = 0.014791857451200485 + 50.0 * 6.200170516967773
Epoch 2920, val loss: 2.2304341793060303
Epoch 2930, training loss: 309.9299011230469 = 0.01462814211845398 + 50.0 * 6.198305606842041
Epoch 2930, val loss: 2.2336552143096924
Epoch 2940, training loss: 310.0194396972656 = 0.014478000812232494 + 50.0 * 6.200099468231201
Epoch 2940, val loss: 2.2379791736602783
Epoch 2950, training loss: 310.03497314453125 = 0.014325957745313644 + 50.0 * 6.200413227081299
Epoch 2950, val loss: 2.2422726154327393
Epoch 2960, training loss: 310.0072326660156 = 0.014179021120071411 + 50.0 * 6.199860572814941
Epoch 2960, val loss: 2.244767189025879
Epoch 2970, training loss: 309.9702453613281 = 0.014026674441993237 + 50.0 * 6.199124336242676
Epoch 2970, val loss: 2.2490129470825195
Epoch 2980, training loss: 309.9344482421875 = 0.013877573423087597 + 50.0 * 6.198410987854004
Epoch 2980, val loss: 2.252786159515381
Epoch 2990, training loss: 309.97515869140625 = 0.01373889110982418 + 50.0 * 6.199228763580322
Epoch 2990, val loss: 2.256084680557251
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6666666666666667
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 431.79180908203125 = 1.9487615823745728 + 50.0 * 8.596860885620117
Epoch 0, val loss: 1.9479851722717285
Epoch 10, training loss: 431.7486877441406 = 1.9391640424728394 + 50.0 * 8.596190452575684
Epoch 10, val loss: 1.939050555229187
Epoch 20, training loss: 431.4714660644531 = 1.9268832206726074 + 50.0 * 8.59089183807373
Epoch 20, val loss: 1.9271111488342285
Epoch 30, training loss: 429.4519348144531 = 1.910341739654541 + 50.0 * 8.55083179473877
Epoch 30, val loss: 1.9107946157455444
Epoch 40, training loss: 416.40997314453125 = 1.8902742862701416 + 50.0 * 8.290393829345703
Epoch 40, val loss: 1.8913605213165283
Epoch 50, training loss: 380.8359680175781 = 1.8694994449615479 + 50.0 * 7.579329490661621
Epoch 50, val loss: 1.8718196153640747
Epoch 60, training loss: 367.0408935546875 = 1.8552894592285156 + 50.0 * 7.303712368011475
Epoch 60, val loss: 1.8584632873535156
Epoch 70, training loss: 356.85870361328125 = 1.844563603401184 + 50.0 * 7.100282669067383
Epoch 70, val loss: 1.8474613428115845
Epoch 80, training loss: 350.47216796875 = 1.8337652683258057 + 50.0 * 6.9727678298950195
Epoch 80, val loss: 1.8369070291519165
Epoch 90, training loss: 344.50238037109375 = 1.82447350025177 + 50.0 * 6.85355806350708
Epoch 90, val loss: 1.827341914176941
Epoch 100, training loss: 340.08233642578125 = 1.816131353378296 + 50.0 * 6.765324115753174
Epoch 100, val loss: 1.818633794784546
Epoch 110, training loss: 336.3152160644531 = 1.8083010911941528 + 50.0 * 6.690138816833496
Epoch 110, val loss: 1.8101933002471924
Epoch 120, training loss: 333.53875732421875 = 1.8006250858306885 + 50.0 * 6.634762763977051
Epoch 120, val loss: 1.801925778388977
Epoch 130, training loss: 331.4479675292969 = 1.792712688446045 + 50.0 * 6.593104839324951
Epoch 130, val loss: 1.7937777042388916
Epoch 140, training loss: 329.74261474609375 = 1.7845059633255005 + 50.0 * 6.559162139892578
Epoch 140, val loss: 1.7854373455047607
Epoch 150, training loss: 328.5393981933594 = 1.7756160497665405 + 50.0 * 6.535275936126709
Epoch 150, val loss: 1.7766369581222534
Epoch 160, training loss: 327.2945861816406 = 1.7658246755599976 + 50.0 * 6.510574817657471
Epoch 160, val loss: 1.7670674324035645
Epoch 170, training loss: 326.29254150390625 = 1.755220651626587 + 50.0 * 6.49074649810791
Epoch 170, val loss: 1.756786823272705
Epoch 180, training loss: 325.4912109375 = 1.7436448335647583 + 50.0 * 6.474951267242432
Epoch 180, val loss: 1.745767593383789
Epoch 190, training loss: 324.70025634765625 = 1.7310575246810913 + 50.0 * 6.459383964538574
Epoch 190, val loss: 1.733919382095337
Epoch 200, training loss: 324.0718688964844 = 1.717482089996338 + 50.0 * 6.447088241577148
Epoch 200, val loss: 1.7212460041046143
Epoch 210, training loss: 323.6020202636719 = 1.7027889490127563 + 50.0 * 6.437984466552734
Epoch 210, val loss: 1.7077679634094238
Epoch 220, training loss: 322.9066162109375 = 1.6869925260543823 + 50.0 * 6.4243927001953125
Epoch 220, val loss: 1.6933975219726562
Epoch 230, training loss: 322.4178771972656 = 1.670133352279663 + 50.0 * 6.414955139160156
Epoch 230, val loss: 1.6782772541046143
Epoch 240, training loss: 321.9867858886719 = 1.652299165725708 + 50.0 * 6.406689643859863
Epoch 240, val loss: 1.662541389465332
Epoch 250, training loss: 321.523681640625 = 1.6335597038269043 + 50.0 * 6.397802829742432
Epoch 250, val loss: 1.6463521718978882
Epoch 260, training loss: 321.0995788574219 = 1.614140272140503 + 50.0 * 6.389708995819092
Epoch 260, val loss: 1.6298599243164062
Epoch 270, training loss: 320.705322265625 = 1.5942988395690918 + 50.0 * 6.38222074508667
Epoch 270, val loss: 1.6134580373764038
Epoch 280, training loss: 320.3612365722656 = 1.5740129947662354 + 50.0 * 6.375744342803955
Epoch 280, val loss: 1.5971869230270386
Epoch 290, training loss: 320.04296875 = 1.5535516738891602 + 50.0 * 6.36978816986084
Epoch 290, val loss: 1.5811630487442017
Epoch 300, training loss: 319.9563293457031 = 1.5331127643585205 + 50.0 * 6.368464469909668
Epoch 300, val loss: 1.5657927989959717
Epoch 310, training loss: 319.4647521972656 = 1.5127747058868408 + 50.0 * 6.359039306640625
Epoch 310, val loss: 1.5509493350982666
Epoch 320, training loss: 319.2088928222656 = 1.4927618503570557 + 50.0 * 6.35432243347168
Epoch 320, val loss: 1.5369822978973389
Epoch 330, training loss: 318.97393798828125 = 1.4729751348495483 + 50.0 * 6.350019454956055
Epoch 330, val loss: 1.5239754915237427
Epoch 340, training loss: 318.6933898925781 = 1.4535081386566162 + 50.0 * 6.344797134399414
Epoch 340, val loss: 1.5116102695465088
Epoch 350, training loss: 318.4813232421875 = 1.4343671798706055 + 50.0 * 6.340939044952393
Epoch 350, val loss: 1.500079870223999
Epoch 360, training loss: 318.4525451660156 = 1.4153845310211182 + 50.0 * 6.340743541717529
Epoch 360, val loss: 1.4891928434371948
Epoch 370, training loss: 318.1124267578125 = 1.3965861797332764 + 50.0 * 6.334316730499268
Epoch 370, val loss: 1.4790159463882446
Epoch 380, training loss: 317.8710632324219 = 1.377886414527893 + 50.0 * 6.32986307144165
Epoch 380, val loss: 1.4690150022506714
Epoch 390, training loss: 317.6900634765625 = 1.3592368364334106 + 50.0 * 6.3266167640686035
Epoch 390, val loss: 1.4595805406570435
Epoch 400, training loss: 317.7027587890625 = 1.3404678106307983 + 50.0 * 6.327246189117432
Epoch 400, val loss: 1.4504480361938477
Epoch 410, training loss: 317.4994812011719 = 1.3214881420135498 + 50.0 * 6.323559284210205
Epoch 410, val loss: 1.4411096572875977
Epoch 420, training loss: 317.2119445800781 = 1.3022487163543701 + 50.0 * 6.3181939125061035
Epoch 420, val loss: 1.4319161176681519
Epoch 430, training loss: 317.00689697265625 = 1.282874345779419 + 50.0 * 6.314480304718018
Epoch 430, val loss: 1.422950029373169
Epoch 440, training loss: 317.03009033203125 = 1.263166069984436 + 50.0 * 6.315338611602783
Epoch 440, val loss: 1.4139196872711182
Epoch 450, training loss: 316.69512939453125 = 1.2432446479797363 + 50.0 * 6.309037208557129
Epoch 450, val loss: 1.40489661693573
Epoch 460, training loss: 316.5255432128906 = 1.2231241464614868 + 50.0 * 6.30604887008667
Epoch 460, val loss: 1.3959219455718994
Epoch 470, training loss: 316.35498046875 = 1.2028591632843018 + 50.0 * 6.303042888641357
Epoch 470, val loss: 1.38699471950531
Epoch 480, training loss: 316.53753662109375 = 1.1825017929077148 + 50.0 * 6.307100772857666
Epoch 480, val loss: 1.378090500831604
Epoch 490, training loss: 316.1878662109375 = 1.1618798971176147 + 50.0 * 6.300519943237305
Epoch 490, val loss: 1.369500756263733
Epoch 500, training loss: 315.9728088378906 = 1.1414299011230469 + 50.0 * 6.296627521514893
Epoch 500, val loss: 1.3610509634017944
Epoch 510, training loss: 315.85345458984375 = 1.1212025880813599 + 50.0 * 6.294644832611084
Epoch 510, val loss: 1.353105068206787
Epoch 520, training loss: 315.7179870605469 = 1.1012563705444336 + 50.0 * 6.29233455657959
Epoch 520, val loss: 1.3455660343170166
Epoch 530, training loss: 315.78131103515625 = 1.081613540649414 + 50.0 * 6.293994426727295
Epoch 530, val loss: 1.3382939100265503
Epoch 540, training loss: 315.84722900390625 = 1.0621187686920166 + 50.0 * 6.29570198059082
Epoch 540, val loss: 1.3318449258804321
Epoch 550, training loss: 315.4784851074219 = 1.0431928634643555 + 50.0 * 6.288706302642822
Epoch 550, val loss: 1.3255504369735718
Epoch 560, training loss: 315.30963134765625 = 1.0247766971588135 + 50.0 * 6.285696983337402
Epoch 560, val loss: 1.3201123476028442
Epoch 570, training loss: 315.1968078613281 = 1.006950855255127 + 50.0 * 6.283796787261963
Epoch 570, val loss: 1.3152406215667725
Epoch 580, training loss: 315.4408874511719 = 0.9896504282951355 + 50.0 * 6.289024829864502
Epoch 580, val loss: 1.3110963106155396
Epoch 590, training loss: 315.0354309082031 = 0.9727206826210022 + 50.0 * 6.281254291534424
Epoch 590, val loss: 1.307015061378479
Epoch 600, training loss: 315.0368957519531 = 0.9563579559326172 + 50.0 * 6.281610488891602
Epoch 600, val loss: 1.3035227060317993
Epoch 610, training loss: 314.8171081542969 = 0.9405478835105896 + 50.0 * 6.277531147003174
Epoch 610, val loss: 1.3008978366851807
Epoch 620, training loss: 314.8533020019531 = 0.9252393245697021 + 50.0 * 6.278561115264893
Epoch 620, val loss: 1.2988330125808716
Epoch 630, training loss: 314.67987060546875 = 0.910351574420929 + 50.0 * 6.275390625
Epoch 630, val loss: 1.2972410917282104
Epoch 640, training loss: 314.66436767578125 = 0.8958700299263 + 50.0 * 6.275370121002197
Epoch 640, val loss: 1.29604971408844
Epoch 650, training loss: 314.587646484375 = 0.8817258477210999 + 50.0 * 6.274118423461914
Epoch 650, val loss: 1.2951104640960693
Epoch 660, training loss: 314.4160461425781 = 0.8679835796356201 + 50.0 * 6.270961284637451
Epoch 660, val loss: 1.2944436073303223
Epoch 670, training loss: 314.4176330566406 = 0.8545695543289185 + 50.0 * 6.271261215209961
Epoch 670, val loss: 1.2943583726882935
Epoch 680, training loss: 314.3112487792969 = 0.841339647769928 + 50.0 * 6.269398212432861
Epoch 680, val loss: 1.2942054271697998
Epoch 690, training loss: 314.22137451171875 = 0.8284065127372742 + 50.0 * 6.26785945892334
Epoch 690, val loss: 1.2947887182235718
Epoch 700, training loss: 314.4943542480469 = 0.8156771659851074 + 50.0 * 6.273573875427246
Epoch 700, val loss: 1.2947986125946045
Epoch 710, training loss: 314.24151611328125 = 0.8031371235847473 + 50.0 * 6.268767833709717
Epoch 710, val loss: 1.2956327199935913
Epoch 720, training loss: 314.05633544921875 = 0.7907506823539734 + 50.0 * 6.2653117179870605
Epoch 720, val loss: 1.2964624166488647
Epoch 730, training loss: 313.9290771484375 = 0.7785926461219788 + 50.0 * 6.263009548187256
Epoch 730, val loss: 1.2974153757095337
Epoch 740, training loss: 313.96527099609375 = 0.7666059136390686 + 50.0 * 6.263973236083984
Epoch 740, val loss: 1.2983981370925903
Epoch 750, training loss: 313.8876953125 = 0.7546166777610779 + 50.0 * 6.262661457061768
Epoch 750, val loss: 1.299405574798584
Epoch 760, training loss: 313.7506408691406 = 0.7426974773406982 + 50.0 * 6.260158538818359
Epoch 760, val loss: 1.300390362739563
Epoch 770, training loss: 313.6834716796875 = 0.7309112548828125 + 50.0 * 6.259051322937012
Epoch 770, val loss: 1.3016984462738037
Epoch 780, training loss: 313.7962341308594 = 0.719190776348114 + 50.0 * 6.261541366577148
Epoch 780, val loss: 1.3028829097747803
Epoch 790, training loss: 313.9644470214844 = 0.7073836326599121 + 50.0 * 6.265141010284424
Epoch 790, val loss: 1.3034292459487915
Epoch 800, training loss: 313.5997619628906 = 0.6955699920654297 + 50.0 * 6.258084297180176
Epoch 800, val loss: 1.3049516677856445
Epoch 810, training loss: 313.51788330078125 = 0.6838566064834595 + 50.0 * 6.256680011749268
Epoch 810, val loss: 1.305902361869812
Epoch 820, training loss: 313.4162292480469 = 0.6722615957260132 + 50.0 * 6.254878997802734
Epoch 820, val loss: 1.307002067565918
Epoch 830, training loss: 313.5054016113281 = 0.6606981754302979 + 50.0 * 6.256893634796143
Epoch 830, val loss: 1.3081865310668945
Epoch 840, training loss: 313.3309631347656 = 0.6490315794944763 + 50.0 * 6.253638744354248
Epoch 840, val loss: 1.3092924356460571
Epoch 850, training loss: 313.2544250488281 = 0.6374203562736511 + 50.0 * 6.252340316772461
Epoch 850, val loss: 1.3103368282318115
Epoch 860, training loss: 313.2234802246094 = 0.6258834600448608 + 50.0 * 6.251951694488525
Epoch 860, val loss: 1.3114383220672607
Epoch 870, training loss: 313.5139465332031 = 0.614392101764679 + 50.0 * 6.257991313934326
Epoch 870, val loss: 1.313037395477295
Epoch 880, training loss: 313.15167236328125 = 0.6026880145072937 + 50.0 * 6.250979423522949
Epoch 880, val loss: 1.3133729696273804
Epoch 890, training loss: 313.1188049316406 = 0.5911976099014282 + 50.0 * 6.250552177429199
Epoch 890, val loss: 1.3150920867919922
Epoch 900, training loss: 312.9974060058594 = 0.5797756314277649 + 50.0 * 6.248352527618408
Epoch 900, val loss: 1.3162661790847778
Epoch 910, training loss: 313.26434326171875 = 0.5684416890144348 + 50.0 * 6.253917694091797
Epoch 910, val loss: 1.3175548315048218
Epoch 920, training loss: 313.07806396484375 = 0.5570217967033386 + 50.0 * 6.250421047210693
Epoch 920, val loss: 1.3191277980804443
Epoch 930, training loss: 312.93084716796875 = 0.5456425547599792 + 50.0 * 6.247703552246094
Epoch 930, val loss: 1.320403814315796
Epoch 940, training loss: 312.8288879394531 = 0.5344505906105042 + 50.0 * 6.245888710021973
Epoch 940, val loss: 1.3220322132110596
Epoch 950, training loss: 312.78582763671875 = 0.5233339071273804 + 50.0 * 6.2452497482299805
Epoch 950, val loss: 1.323866605758667
Epoch 960, training loss: 313.4111633300781 = 0.5122979283332825 + 50.0 * 6.257977485656738
Epoch 960, val loss: 1.3256008625030518
Epoch 970, training loss: 312.8795166015625 = 0.5009794235229492 + 50.0 * 6.247570514678955
Epoch 970, val loss: 1.3267096281051636
Epoch 980, training loss: 312.74957275390625 = 0.4899556040763855 + 50.0 * 6.245192050933838
Epoch 980, val loss: 1.3282524347305298
Epoch 990, training loss: 312.6004638671875 = 0.4791378080844879 + 50.0 * 6.242426872253418
Epoch 990, val loss: 1.3302834033966064
Epoch 1000, training loss: 312.57159423828125 = 0.4684668183326721 + 50.0 * 6.242062091827393
Epoch 1000, val loss: 1.332526445388794
Epoch 1010, training loss: 312.6947021484375 = 0.4579034447669983 + 50.0 * 6.2447357177734375
Epoch 1010, val loss: 1.3347339630126953
Epoch 1020, training loss: 312.576171875 = 0.4471780061721802 + 50.0 * 6.242579936981201
Epoch 1020, val loss: 1.3363847732543945
Epoch 1030, training loss: 312.5351867675781 = 0.4367077648639679 + 50.0 * 6.241969585418701
Epoch 1030, val loss: 1.3386658430099487
Epoch 1040, training loss: 312.41888427734375 = 0.42637544870376587 + 50.0 * 6.2398505210876465
Epoch 1040, val loss: 1.3410638570785522
Epoch 1050, training loss: 312.3668212890625 = 0.41622719168663025 + 50.0 * 6.239011764526367
Epoch 1050, val loss: 1.3436585664749146
Epoch 1060, training loss: 312.4444580078125 = 0.40624138712882996 + 50.0 * 6.240764141082764
Epoch 1060, val loss: 1.3463348150253296
Epoch 1070, training loss: 312.3758544921875 = 0.3962629735469818 + 50.0 * 6.239591598510742
Epoch 1070, val loss: 1.3485424518585205
Epoch 1080, training loss: 312.3334655761719 = 0.3864438533782959 + 50.0 * 6.238940238952637
Epoch 1080, val loss: 1.3509347438812256
Epoch 1090, training loss: 312.6710205078125 = 0.3767993748188019 + 50.0 * 6.245884418487549
Epoch 1090, val loss: 1.353196144104004
Epoch 1100, training loss: 312.3678283691406 = 0.3672158420085907 + 50.0 * 6.240012168884277
Epoch 1100, val loss: 1.3559823036193848
Epoch 1110, training loss: 312.1764221191406 = 0.35787731409072876 + 50.0 * 6.236371040344238
Epoch 1110, val loss: 1.3586329221725464
Epoch 1120, training loss: 312.1145324707031 = 0.348766952753067 + 50.0 * 6.235315322875977
Epoch 1120, val loss: 1.361329197883606
Epoch 1130, training loss: 312.0813293457031 = 0.33985069394111633 + 50.0 * 6.234829425811768
Epoch 1130, val loss: 1.3642592430114746
Epoch 1140, training loss: 312.0787353515625 = 0.3311088979244232 + 50.0 * 6.234952449798584
Epoch 1140, val loss: 1.3671212196350098
Epoch 1150, training loss: 312.49932861328125 = 0.3225302994251251 + 50.0 * 6.243535995483398
Epoch 1150, val loss: 1.3699871301651
Epoch 1160, training loss: 312.14288330078125 = 0.3139285445213318 + 50.0 * 6.236578941345215
Epoch 1160, val loss: 1.3721336126327515
Epoch 1170, training loss: 311.9873962402344 = 0.30564847588539124 + 50.0 * 6.233634948730469
Epoch 1170, val loss: 1.375205397605896
Epoch 1180, training loss: 311.9456787109375 = 0.2976136803627014 + 50.0 * 6.232961177825928
Epoch 1180, val loss: 1.3783022165298462
Epoch 1190, training loss: 312.0951232910156 = 0.28978168964385986 + 50.0 * 6.2361063957214355
Epoch 1190, val loss: 1.3811395168304443
Epoch 1200, training loss: 311.88983154296875 = 0.28204184770584106 + 50.0 * 6.232155799865723
Epoch 1200, val loss: 1.3845058679580688
Epoch 1210, training loss: 311.8507995605469 = 0.27451464533805847 + 50.0 * 6.231525897979736
Epoch 1210, val loss: 1.3874932527542114
Epoch 1220, training loss: 311.8536376953125 = 0.26723483204841614 + 50.0 * 6.2317280769348145
Epoch 1220, val loss: 1.3910093307495117
Epoch 1230, training loss: 311.9569091796875 = 0.2601624131202698 + 50.0 * 6.2339348793029785
Epoch 1230, val loss: 1.394216775894165
Epoch 1240, training loss: 311.95391845703125 = 0.25320422649383545 + 50.0 * 6.23401403427124
Epoch 1240, val loss: 1.3972673416137695
Epoch 1250, training loss: 311.75103759765625 = 0.2464296668767929 + 50.0 * 6.2300920486450195
Epoch 1250, val loss: 1.4007480144500732
Epoch 1260, training loss: 311.7594299316406 = 0.23989425599575043 + 50.0 * 6.230390548706055
Epoch 1260, val loss: 1.4044135808944702
Epoch 1270, training loss: 311.7889709472656 = 0.23356905579566956 + 50.0 * 6.231107711791992
Epoch 1270, val loss: 1.408246397972107
Epoch 1280, training loss: 311.9010925292969 = 0.22743119299411774 + 50.0 * 6.233473300933838
Epoch 1280, val loss: 1.4120135307312012
Epoch 1290, training loss: 311.7333679199219 = 0.2213975489139557 + 50.0 * 6.230239391326904
Epoch 1290, val loss: 1.4159274101257324
Epoch 1300, training loss: 311.82843017578125 = 0.21557748317718506 + 50.0 * 6.23225736618042
Epoch 1300, val loss: 1.4199278354644775
Epoch 1310, training loss: 311.6750183105469 = 0.20988872647285461 + 50.0 * 6.229302406311035
Epoch 1310, val loss: 1.4235036373138428
Epoch 1320, training loss: 311.57763671875 = 0.20444224774837494 + 50.0 * 6.227463722229004
Epoch 1320, val loss: 1.4278873205184937
Epoch 1330, training loss: 311.63671875 = 0.1991802155971527 + 50.0 * 6.228750705718994
Epoch 1330, val loss: 1.4323065280914307
Epoch 1340, training loss: 311.72100830078125 = 0.1940201073884964 + 50.0 * 6.230539321899414
Epoch 1340, val loss: 1.436179280281067
Epoch 1350, training loss: 311.6094970703125 = 0.1890064924955368 + 50.0 * 6.228410243988037
Epoch 1350, val loss: 1.440942645072937
Epoch 1360, training loss: 311.5506286621094 = 0.18419212102890015 + 50.0 * 6.227328300476074
Epoch 1360, val loss: 1.4452162981033325
Epoch 1370, training loss: 311.5863952636719 = 0.17953109741210938 + 50.0 * 6.228137016296387
Epoch 1370, val loss: 1.4499807357788086
Epoch 1380, training loss: 311.8100280761719 = 0.17498449981212616 + 50.0 * 6.232700824737549
Epoch 1380, val loss: 1.4543472528457642
Epoch 1390, training loss: 311.48016357421875 = 0.17048683762550354 + 50.0 * 6.226192951202393
Epoch 1390, val loss: 1.4590450525283813
Epoch 1400, training loss: 311.3836975097656 = 0.16623273491859436 + 50.0 * 6.224349498748779
Epoch 1400, val loss: 1.4638899564743042
Epoch 1410, training loss: 311.3511047363281 = 0.16211514174938202 + 50.0 * 6.223779678344727
Epoch 1410, val loss: 1.468747854232788
Epoch 1420, training loss: 311.35845947265625 = 0.15813753008842468 + 50.0 * 6.224006175994873
Epoch 1420, val loss: 1.4736523628234863
Epoch 1430, training loss: 311.92669677734375 = 0.15424257516860962 + 50.0 * 6.235449314117432
Epoch 1430, val loss: 1.478047251701355
Epoch 1440, training loss: 311.4342956542969 = 0.15042872726917267 + 50.0 * 6.225677490234375
Epoch 1440, val loss: 1.483642578125
Epoch 1450, training loss: 311.3592224121094 = 0.14673206210136414 + 50.0 * 6.224249839782715
Epoch 1450, val loss: 1.4887737035751343
Epoch 1460, training loss: 311.3653564453125 = 0.14318187534809113 + 50.0 * 6.224443435668945
Epoch 1460, val loss: 1.4938530921936035
Epoch 1470, training loss: 311.3055725097656 = 0.1397385448217392 + 50.0 * 6.223316669464111
Epoch 1470, val loss: 1.4993820190429688
Epoch 1480, training loss: 311.38177490234375 = 0.13639147579669952 + 50.0 * 6.224907875061035
Epoch 1480, val loss: 1.5045169591903687
Epoch 1490, training loss: 311.2906494140625 = 0.1331128478050232 + 50.0 * 6.223150730133057
Epoch 1490, val loss: 1.5097479820251465
Epoch 1500, training loss: 311.2438659667969 = 0.12996909022331238 + 50.0 * 6.222278118133545
Epoch 1500, val loss: 1.5154764652252197
Epoch 1510, training loss: 311.46484375 = 0.1269148737192154 + 50.0 * 6.2267584800720215
Epoch 1510, val loss: 1.5209633111953735
Epoch 1520, training loss: 311.25732421875 = 0.12387891858816147 + 50.0 * 6.2226691246032715
Epoch 1520, val loss: 1.5263868570327759
Epoch 1530, training loss: 311.1868896484375 = 0.12098540365695953 + 50.0 * 6.221318244934082
Epoch 1530, val loss: 1.532289743423462
Epoch 1540, training loss: 311.36767578125 = 0.11818568408489227 + 50.0 * 6.224989891052246
Epoch 1540, val loss: 1.5379183292388916
Epoch 1550, training loss: 311.2392272949219 = 0.11542975157499313 + 50.0 * 6.222476482391357
Epoch 1550, val loss: 1.5435700416564941
Epoch 1560, training loss: 311.13946533203125 = 0.11276490986347198 + 50.0 * 6.220534324645996
Epoch 1560, val loss: 1.5491952896118164
Epoch 1570, training loss: 311.2045593261719 = 0.11018503457307816 + 50.0 * 6.221887111663818
Epoch 1570, val loss: 1.555025339126587
Epoch 1580, training loss: 311.1907653808594 = 0.10766120254993439 + 50.0 * 6.2216620445251465
Epoch 1580, val loss: 1.5608748197555542
Epoch 1590, training loss: 311.1406555175781 = 0.10519742220640182 + 50.0 * 6.220709323883057
Epoch 1590, val loss: 1.5665596723556519
Epoch 1600, training loss: 311.1298828125 = 0.1028268113732338 + 50.0 * 6.220541477203369
Epoch 1600, val loss: 1.5728673934936523
Epoch 1610, training loss: 311.145263671875 = 0.10051163285970688 + 50.0 * 6.220894813537598
Epoch 1610, val loss: 1.5784968137741089
Epoch 1620, training loss: 311.03533935546875 = 0.09826350212097168 + 50.0 * 6.218741416931152
Epoch 1620, val loss: 1.5842642784118652
Epoch 1630, training loss: 311.18994140625 = 0.09609182924032211 + 50.0 * 6.221877098083496
Epoch 1630, val loss: 1.5901861190795898
Epoch 1640, training loss: 311.0273742675781 = 0.09393569082021713 + 50.0 * 6.2186689376831055
Epoch 1640, val loss: 1.5965412855148315
Epoch 1650, training loss: 311.0133972167969 = 0.09185752272605896 + 50.0 * 6.218430995941162
Epoch 1650, val loss: 1.6023108959197998
Epoch 1660, training loss: 311.246826171875 = 0.08986038714647293 + 50.0 * 6.22313928604126
Epoch 1660, val loss: 1.608468770980835
Epoch 1670, training loss: 311.1341857910156 = 0.08786987513303757 + 50.0 * 6.220926761627197
Epoch 1670, val loss: 1.6143534183502197
Epoch 1680, training loss: 310.9642639160156 = 0.08593561500310898 + 50.0 * 6.21756649017334
Epoch 1680, val loss: 1.6200006008148193
Epoch 1690, training loss: 310.9328918457031 = 0.08408074080944061 + 50.0 * 6.216976165771484
Epoch 1690, val loss: 1.6264528036117554
Epoch 1700, training loss: 311.02874755859375 = 0.08227658271789551 + 50.0 * 6.218929290771484
Epoch 1700, val loss: 1.63239586353302
Epoch 1710, training loss: 311.1353454589844 = 0.08051560074090958 + 50.0 * 6.221096992492676
Epoch 1710, val loss: 1.6385740041732788
Epoch 1720, training loss: 310.92333984375 = 0.07876148819923401 + 50.0 * 6.216891765594482
Epoch 1720, val loss: 1.6445591449737549
Epoch 1730, training loss: 310.8484802246094 = 0.07707937806844711 + 50.0 * 6.215427875518799
Epoch 1730, val loss: 1.6507797241210938
Epoch 1740, training loss: 310.86114501953125 = 0.0754581019282341 + 50.0 * 6.2157135009765625
Epoch 1740, val loss: 1.6569725275039673
Epoch 1750, training loss: 311.2959289550781 = 0.07388219982385635 + 50.0 * 6.224441051483154
Epoch 1750, val loss: 1.6635245084762573
Epoch 1760, training loss: 311.0474548339844 = 0.07230692356824875 + 50.0 * 6.219502925872803
Epoch 1760, val loss: 1.6690030097961426
Epoch 1770, training loss: 310.8207092285156 = 0.07077210396528244 + 50.0 * 6.214998722076416
Epoch 1770, val loss: 1.6752032041549683
Epoch 1780, training loss: 310.7989807128906 = 0.06931090354919434 + 50.0 * 6.214593410491943
Epoch 1780, val loss: 1.68122398853302
Epoch 1790, training loss: 311.07659912109375 = 0.06788583099842072 + 50.0 * 6.220174312591553
Epoch 1790, val loss: 1.687304139137268
Epoch 1800, training loss: 310.90216064453125 = 0.06647756695747375 + 50.0 * 6.216713905334473
Epoch 1800, val loss: 1.6938303709030151
Epoch 1810, training loss: 310.755126953125 = 0.06509222835302353 + 50.0 * 6.21380090713501
Epoch 1810, val loss: 1.6993122100830078
Epoch 1820, training loss: 310.7335510253906 = 0.06377621740102768 + 50.0 * 6.213395595550537
Epoch 1820, val loss: 1.7058484554290771
Epoch 1830, training loss: 310.7541809082031 = 0.06249751150608063 + 50.0 * 6.213833332061768
Epoch 1830, val loss: 1.7119643688201904
Epoch 1840, training loss: 310.8424987792969 = 0.06124751642346382 + 50.0 * 6.215625286102295
Epoch 1840, val loss: 1.7179275751113892
Epoch 1850, training loss: 310.72393798828125 = 0.06001535430550575 + 50.0 * 6.213278293609619
Epoch 1850, val loss: 1.7240504026412964
Epoch 1860, training loss: 311.1513671875 = 0.058833979070186615 + 50.0 * 6.221850395202637
Epoch 1860, val loss: 1.7299773693084717
Epoch 1870, training loss: 310.781005859375 = 0.057621169835329056 + 50.0 * 6.214468002319336
Epoch 1870, val loss: 1.7364014387130737
Epoch 1880, training loss: 310.64532470703125 = 0.05647366866469383 + 50.0 * 6.2117767333984375
Epoch 1880, val loss: 1.7423382997512817
Epoch 1890, training loss: 310.6414489746094 = 0.05537598952651024 + 50.0 * 6.211721420288086
Epoch 1890, val loss: 1.7487093210220337
Epoch 1900, training loss: 311.0986328125 = 0.054321322590112686 + 50.0 * 6.22088623046875
Epoch 1900, val loss: 1.7551555633544922
Epoch 1910, training loss: 310.8366394042969 = 0.053235940635204315 + 50.0 * 6.215668201446533
Epoch 1910, val loss: 1.7600281238555908
Epoch 1920, training loss: 310.6888427734375 = 0.052196651697158813 + 50.0 * 6.212733268737793
Epoch 1920, val loss: 1.7666443586349487
Epoch 1930, training loss: 310.6103515625 = 0.051192596554756165 + 50.0 * 6.211183547973633
Epoch 1930, val loss: 1.7727034091949463
Epoch 1940, training loss: 310.7587890625 = 0.050229430198669434 + 50.0 * 6.214171409606934
Epoch 1940, val loss: 1.7788786888122559
Epoch 1950, training loss: 310.5943908691406 = 0.04926297813653946 + 50.0 * 6.210902690887451
Epoch 1950, val loss: 1.7847650051116943
Epoch 1960, training loss: 310.6082458496094 = 0.04833469167351723 + 50.0 * 6.211198329925537
Epoch 1960, val loss: 1.7907627820968628
Epoch 1970, training loss: 310.86651611328125 = 0.047436412423849106 + 50.0 * 6.216381549835205
Epoch 1970, val loss: 1.7967941761016846
Epoch 1980, training loss: 310.6943664550781 = 0.04652068763971329 + 50.0 * 6.21295690536499
Epoch 1980, val loss: 1.8023635149002075
Epoch 1990, training loss: 310.6900329589844 = 0.0456460639834404 + 50.0 * 6.212887763977051
Epoch 1990, val loss: 1.8086791038513184
Epoch 2000, training loss: 310.5282897949219 = 0.044799041002988815 + 50.0 * 6.209669589996338
Epoch 2000, val loss: 1.8141589164733887
Epoch 2010, training loss: 310.53509521484375 = 0.043984491378068924 + 50.0 * 6.209822177886963
Epoch 2010, val loss: 1.8201252222061157
Epoch 2020, training loss: 310.67559814453125 = 0.043193429708480835 + 50.0 * 6.212648391723633
Epoch 2020, val loss: 1.8260078430175781
Epoch 2030, training loss: 310.5706481933594 = 0.04240165650844574 + 50.0 * 6.210565090179443
Epoch 2030, val loss: 1.8317430019378662
Epoch 2040, training loss: 310.7107238769531 = 0.041640207171440125 + 50.0 * 6.213382244110107
Epoch 2040, val loss: 1.8379281759262085
Epoch 2050, training loss: 310.56146240234375 = 0.040882911533117294 + 50.0 * 6.210411548614502
Epoch 2050, val loss: 1.843489170074463
Epoch 2060, training loss: 310.6015930175781 = 0.04015190154314041 + 50.0 * 6.211228847503662
Epoch 2060, val loss: 1.8491933345794678
Epoch 2070, training loss: 310.4688720703125 = 0.03943778574466705 + 50.0 * 6.20858907699585
Epoch 2070, val loss: 1.85508394241333
Epoch 2080, training loss: 310.48162841796875 = 0.03875341638922691 + 50.0 * 6.208857536315918
Epoch 2080, val loss: 1.8609533309936523
Epoch 2090, training loss: 310.5765380859375 = 0.03808451443910599 + 50.0 * 6.210769176483154
Epoch 2090, val loss: 1.8667792081832886
Epoch 2100, training loss: 310.67169189453125 = 0.03741263970732689 + 50.0 * 6.212685585021973
Epoch 2100, val loss: 1.8718706369400024
Epoch 2110, training loss: 310.4686584472656 = 0.036754775792360306 + 50.0 * 6.2086381912231445
Epoch 2110, val loss: 1.8776142597198486
Epoch 2120, training loss: 310.4325256347656 = 0.03612145036458969 + 50.0 * 6.207927703857422
Epoch 2120, val loss: 1.8834034204483032
Epoch 2130, training loss: 310.5121765136719 = 0.035511936992406845 + 50.0 * 6.209533214569092
Epoch 2130, val loss: 1.8890552520751953
Epoch 2140, training loss: 310.6285095214844 = 0.03491184860467911 + 50.0 * 6.211872100830078
Epoch 2140, val loss: 1.89463210105896
Epoch 2150, training loss: 310.53271484375 = 0.03431353345513344 + 50.0 * 6.209968566894531
Epoch 2150, val loss: 1.9003069400787354
Epoch 2160, training loss: 310.48052978515625 = 0.03373964875936508 + 50.0 * 6.208935737609863
Epoch 2160, val loss: 1.9059818983078003
Epoch 2170, training loss: 310.3999328613281 = 0.033176153898239136 + 50.0 * 6.207335472106934
Epoch 2170, val loss: 1.9115993976593018
Epoch 2180, training loss: 310.42523193359375 = 0.03263605386018753 + 50.0 * 6.207851886749268
Epoch 2180, val loss: 1.9172067642211914
Epoch 2190, training loss: 310.5741271972656 = 0.03210373967885971 + 50.0 * 6.210840702056885
Epoch 2190, val loss: 1.9226657152175903
Epoch 2200, training loss: 310.3968811035156 = 0.03157695382833481 + 50.0 * 6.207305908203125
Epoch 2200, val loss: 1.9284166097640991
Epoch 2210, training loss: 310.585205078125 = 0.03106488473713398 + 50.0 * 6.211082458496094
Epoch 2210, val loss: 1.933923602104187
Epoch 2220, training loss: 310.4266662597656 = 0.03055950440466404 + 50.0 * 6.207922458648682
Epoch 2220, val loss: 1.9386823177337646
Epoch 2230, training loss: 310.3745422363281 = 0.030069615691900253 + 50.0 * 6.206889629364014
Epoch 2230, val loss: 1.9446817636489868
Epoch 2240, training loss: 310.3385925292969 = 0.029595129191875458 + 50.0 * 6.206180095672607
Epoch 2240, val loss: 1.9500257968902588
Epoch 2250, training loss: 310.6777648925781 = 0.02914354018867016 + 50.0 * 6.212972164154053
Epoch 2250, val loss: 1.9559050798416138
Epoch 2260, training loss: 310.3917541503906 = 0.028668798506259918 + 50.0 * 6.20726203918457
Epoch 2260, val loss: 1.9604063034057617
Epoch 2270, training loss: 310.384521484375 = 0.028215043246746063 + 50.0 * 6.207126140594482
Epoch 2270, val loss: 1.9661734104156494
Epoch 2280, training loss: 310.42083740234375 = 0.027781996876001358 + 50.0 * 6.207861423492432
Epoch 2280, val loss: 1.9713537693023682
Epoch 2290, training loss: 310.3033752441406 = 0.027353931218385696 + 50.0 * 6.2055206298828125
Epoch 2290, val loss: 1.9769368171691895
Epoch 2300, training loss: 310.50164794921875 = 0.026947911828756332 + 50.0 * 6.209494113922119
Epoch 2300, val loss: 1.9822731018066406
Epoch 2310, training loss: 310.256103515625 = 0.026528093963861465 + 50.0 * 6.204591751098633
Epoch 2310, val loss: 1.987061858177185
Epoch 2320, training loss: 310.282470703125 = 0.02612648904323578 + 50.0 * 6.205127239227295
Epoch 2320, val loss: 1.9925978183746338
Epoch 2330, training loss: 310.3416748046875 = 0.02574731968343258 + 50.0 * 6.206318378448486
Epoch 2330, val loss: 1.9978561401367188
Epoch 2340, training loss: 310.4906311035156 = 0.02536999061703682 + 50.0 * 6.209305286407471
Epoch 2340, val loss: 2.0030694007873535
Epoch 2350, training loss: 310.2921447753906 = 0.024982666596770287 + 50.0 * 6.205342769622803
Epoch 2350, val loss: 2.0084517002105713
Epoch 2360, training loss: 310.271240234375 = 0.024620549753308296 + 50.0 * 6.20493221282959
Epoch 2360, val loss: 2.0136449337005615
Epoch 2370, training loss: 310.4008483886719 = 0.024269238114356995 + 50.0 * 6.207531452178955
Epoch 2370, val loss: 2.0185675621032715
Epoch 2380, training loss: 310.3957824707031 = 0.023908579722046852 + 50.0 * 6.207437992095947
Epoch 2380, val loss: 2.0235278606414795
Epoch 2390, training loss: 310.3187255859375 = 0.023562666028738022 + 50.0 * 6.205903053283691
Epoch 2390, val loss: 2.028773307800293
Epoch 2400, training loss: 310.2281494140625 = 0.023220043629407883 + 50.0 * 6.204098701477051
Epoch 2400, val loss: 2.0340116024017334
Epoch 2410, training loss: 310.21221923828125 = 0.02289523556828499 + 50.0 * 6.203786849975586
Epoch 2410, val loss: 2.039097785949707
Epoch 2420, training loss: 310.46026611328125 = 0.02258635312318802 + 50.0 * 6.20875358581543
Epoch 2420, val loss: 2.0446889400482178
Epoch 2430, training loss: 310.22802734375 = 0.02225322090089321 + 50.0 * 6.204115867614746
Epoch 2430, val loss: 2.0484824180603027
Epoch 2440, training loss: 310.18115234375 = 0.02194126509130001 + 50.0 * 6.203184127807617
Epoch 2440, val loss: 2.0539417266845703
Epoch 2450, training loss: 310.1874694824219 = 0.0216424148529768 + 50.0 * 6.203316688537598
Epoch 2450, val loss: 2.0587403774261475
Epoch 2460, training loss: 310.3103942871094 = 0.021350309252738953 + 50.0 * 6.205780506134033
Epoch 2460, val loss: 2.0636494159698486
Epoch 2470, training loss: 310.3011779785156 = 0.02105596661567688 + 50.0 * 6.205602169036865
Epoch 2470, val loss: 2.068556308746338
Epoch 2480, training loss: 310.2370910644531 = 0.020762745290994644 + 50.0 * 6.204326629638672
Epoch 2480, val loss: 2.073570966720581
Epoch 2490, training loss: 310.2687683105469 = 0.020488588139414787 + 50.0 * 6.204965591430664
Epoch 2490, val loss: 2.0782294273376465
Epoch 2500, training loss: 310.20068359375 = 0.02021021768450737 + 50.0 * 6.203609466552734
Epoch 2500, val loss: 2.0830068588256836
Epoch 2510, training loss: 310.12359619140625 = 0.01994166523218155 + 50.0 * 6.202073097229004
Epoch 2510, val loss: 2.087963581085205
Epoch 2520, training loss: 310.2900085449219 = 0.019687142223119736 + 50.0 * 6.205406188964844
Epoch 2520, val loss: 2.0928804874420166
Epoch 2530, training loss: 310.1919860839844 = 0.01942293904721737 + 50.0 * 6.203451633453369
Epoch 2530, val loss: 2.0971624851226807
Epoch 2540, training loss: 310.1344299316406 = 0.019164010882377625 + 50.0 * 6.202305316925049
Epoch 2540, val loss: 2.102226734161377
Epoch 2550, training loss: 310.2011413574219 = 0.018919719383120537 + 50.0 * 6.203644275665283
Epoch 2550, val loss: 2.107121229171753
Epoch 2560, training loss: 310.1617126464844 = 0.018676910549402237 + 50.0 * 6.2028608322143555
Epoch 2560, val loss: 2.1115264892578125
Epoch 2570, training loss: 310.1380310058594 = 0.01843869686126709 + 50.0 * 6.202392101287842
Epoch 2570, val loss: 2.1161396503448486
Epoch 2580, training loss: 310.50250244140625 = 0.01820995844900608 + 50.0 * 6.209685802459717
Epoch 2580, val loss: 2.121053457260132
Epoch 2590, training loss: 310.26812744140625 = 0.017972396686673164 + 50.0 * 6.205002784729004
Epoch 2590, val loss: 2.124821186065674
Epoch 2600, training loss: 310.0906982421875 = 0.017739813774824142 + 50.0 * 6.2014594078063965
Epoch 2600, val loss: 2.129786491394043
Epoch 2610, training loss: 310.0402526855469 = 0.01752101629972458 + 50.0 * 6.2004547119140625
Epoch 2610, val loss: 2.1344845294952393
Epoch 2620, training loss: 310.084228515625 = 0.01731197163462639 + 50.0 * 6.201338291168213
Epoch 2620, val loss: 2.139174461364746
Epoch 2630, training loss: 310.3194274902344 = 0.01710309088230133 + 50.0 * 6.2060465812683105
Epoch 2630, val loss: 2.1435940265655518
Epoch 2640, training loss: 310.1252136230469 = 0.016890542581677437 + 50.0 * 6.202166557312012
Epoch 2640, val loss: 2.147873640060425
Epoch 2650, training loss: 310.2812194824219 = 0.01669328100979328 + 50.0 * 6.205290794372559
Epoch 2650, val loss: 2.1523101329803467
Epoch 2660, training loss: 310.0477600097656 = 0.016481339931488037 + 50.0 * 6.200625896453857
Epoch 2660, val loss: 2.156843662261963
Epoch 2670, training loss: 310.05230712890625 = 0.01628691330552101 + 50.0 * 6.200720310211182
Epoch 2670, val loss: 2.161005735397339
Epoch 2680, training loss: 310.0556945800781 = 0.0160983819514513 + 50.0 * 6.200791835784912
Epoch 2680, val loss: 2.1655774116516113
Epoch 2690, training loss: 310.1613464355469 = 0.015915267169475555 + 50.0 * 6.202908515930176
Epoch 2690, val loss: 2.1700477600097656
Epoch 2700, training loss: 310.2760314941406 = 0.015723761171102524 + 50.0 * 6.205206394195557
Epoch 2700, val loss: 2.1738500595092773
Epoch 2710, training loss: 310.0518493652344 = 0.01553244050592184 + 50.0 * 6.200726509094238
Epoch 2710, val loss: 2.1782479286193848
Epoch 2720, training loss: 310.00555419921875 = 0.015353971160948277 + 50.0 * 6.199804306030273
Epoch 2720, val loss: 2.1827874183654785
Epoch 2730, training loss: 310.07476806640625 = 0.015182072296738625 + 50.0 * 6.2011919021606445
Epoch 2730, val loss: 2.1871399879455566
Epoch 2740, training loss: 310.0772705078125 = 0.015010897070169449 + 50.0 * 6.201245307922363
Epoch 2740, val loss: 2.191387414932251
Epoch 2750, training loss: 310.1040344238281 = 0.014842193573713303 + 50.0 * 6.201784133911133
Epoch 2750, val loss: 2.1956918239593506
Epoch 2760, training loss: 309.967041015625 = 0.014670041389763355 + 50.0 * 6.199047088623047
Epoch 2760, val loss: 2.1996350288391113
Epoch 2770, training loss: 309.9641418457031 = 0.014507454819977283 + 50.0 * 6.198992729187012
Epoch 2770, val loss: 2.203761100769043
Epoch 2780, training loss: 309.95599365234375 = 0.014350155368447304 + 50.0 * 6.198832988739014
Epoch 2780, val loss: 2.208005428314209
Epoch 2790, training loss: 310.4345703125 = 0.014204141683876514 + 50.0 * 6.208407878875732
Epoch 2790, val loss: 2.212005376815796
Epoch 2800, training loss: 310.24652099609375 = 0.014037162996828556 + 50.0 * 6.204649448394775
Epoch 2800, val loss: 2.215423822402954
Epoch 2810, training loss: 310.0368347167969 = 0.013871432282030582 + 50.0 * 6.200459003448486
Epoch 2810, val loss: 2.219677209854126
Epoch 2820, training loss: 309.91363525390625 = 0.013719657436013222 + 50.0 * 6.197998046875
Epoch 2820, val loss: 2.224053382873535
Epoch 2830, training loss: 309.9034729003906 = 0.01357599999755621 + 50.0 * 6.197797775268555
Epoch 2830, val loss: 2.228280544281006
Epoch 2840, training loss: 309.9781188964844 = 0.01343733910471201 + 50.0 * 6.199293613433838
Epoch 2840, val loss: 2.2322847843170166
Epoch 2850, training loss: 310.1415100097656 = 0.013297433033585548 + 50.0 * 6.202564239501953
Epoch 2850, val loss: 2.2357521057128906
Epoch 2860, training loss: 310.09893798828125 = 0.013155250810086727 + 50.0 * 6.201715469360352
Epoch 2860, val loss: 2.239881753921509
Epoch 2870, training loss: 309.9607849121094 = 0.013011316768825054 + 50.0 * 6.198955535888672
Epoch 2870, val loss: 2.2440245151519775
Epoch 2880, training loss: 309.8819580078125 = 0.01287862565368414 + 50.0 * 6.197381496429443
Epoch 2880, val loss: 2.2481179237365723
Epoch 2890, training loss: 309.95709228515625 = 0.012752301059663296 + 50.0 * 6.198886871337891
Epoch 2890, val loss: 2.252256155014038
Epoch 2900, training loss: 310.02093505859375 = 0.012621479108929634 + 50.0 * 6.200165748596191
Epoch 2900, val loss: 2.2559566497802734
Epoch 2910, training loss: 309.8831787109375 = 0.012482503429055214 + 50.0 * 6.197413921356201
Epoch 2910, val loss: 2.2594590187072754
Epoch 2920, training loss: 309.84222412109375 = 0.01235722191631794 + 50.0 * 6.196597576141357
Epoch 2920, val loss: 2.2633371353149414
Epoch 2930, training loss: 309.9002990722656 = 0.012237650342285633 + 50.0 * 6.197761535644531
Epoch 2930, val loss: 2.267552614212036
Epoch 2940, training loss: 310.12335205078125 = 0.012117653153836727 + 50.0 * 6.2022247314453125
Epoch 2940, val loss: 2.271322011947632
Epoch 2950, training loss: 310.0409851074219 = 0.011992265470325947 + 50.0 * 6.20058012008667
Epoch 2950, val loss: 2.2741737365722656
Epoch 2960, training loss: 309.927978515625 = 0.011873248964548111 + 50.0 * 6.198322296142578
Epoch 2960, val loss: 2.2785701751708984
Epoch 2970, training loss: 309.8710632324219 = 0.011757086962461472 + 50.0 * 6.19718599319458
Epoch 2970, val loss: 2.2817435264587402
Epoch 2980, training loss: 310.1928405761719 = 0.011647891253232956 + 50.0 * 6.2036237716674805
Epoch 2980, val loss: 2.286099672317505
Epoch 2990, training loss: 309.8665771484375 = 0.011528024449944496 + 50.0 * 6.19710111618042
Epoch 2990, val loss: 2.2887814044952393
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.5740740740740741
0.8044280442804429
The final CL Acc:0.63210, 0.04128, The final GNN Acc:0.80408, 0.00025
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13218])
remove edge: torch.Size([2, 7912])
updated graph: torch.Size([2, 10574])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.76611328125 = 1.9242554903030396 + 50.0 * 8.596837043762207
Epoch 0, val loss: 1.9176712036132812
Epoch 10, training loss: 431.70928955078125 = 1.9159835577011108 + 50.0 * 8.595866203308105
Epoch 10, val loss: 1.9097168445587158
Epoch 20, training loss: 431.3185119628906 = 1.9060465097427368 + 50.0 * 8.588249206542969
Epoch 20, val loss: 1.8998146057128906
Epoch 30, training loss: 428.80242919921875 = 1.8936456441879272 + 50.0 * 8.538175582885742
Epoch 30, val loss: 1.8872390985488892
Epoch 40, training loss: 416.0893249511719 = 1.880354642868042 + 50.0 * 8.2841796875
Epoch 40, val loss: 1.874266266822815
Epoch 50, training loss: 385.8820495605469 = 1.8674753904342651 + 50.0 * 7.680291652679443
Epoch 50, val loss: 1.8619849681854248
Epoch 60, training loss: 374.21435546875 = 1.8548338413238525 + 50.0 * 7.447190284729004
Epoch 60, val loss: 1.8502116203308105
Epoch 70, training loss: 362.7913513183594 = 1.8439971208572388 + 50.0 * 7.218946933746338
Epoch 70, val loss: 1.8404335975646973
Epoch 80, training loss: 351.7393798828125 = 1.8339910507202148 + 50.0 * 6.99810791015625
Epoch 80, val loss: 1.8313078880310059
Epoch 90, training loss: 345.1016540527344 = 1.8254870176315308 + 50.0 * 6.865522861480713
Epoch 90, val loss: 1.8230658769607544
Epoch 100, training loss: 340.046142578125 = 1.8178634643554688 + 50.0 * 6.764565467834473
Epoch 100, val loss: 1.815658450126648
Epoch 110, training loss: 336.7189025878906 = 1.8106727600097656 + 50.0 * 6.698164939880371
Epoch 110, val loss: 1.8086334466934204
Epoch 120, training loss: 334.1659851074219 = 1.803290843963623 + 50.0 * 6.64725399017334
Epoch 120, val loss: 1.8015987873077393
Epoch 130, training loss: 332.0340881347656 = 1.796020746231079 + 50.0 * 6.604761600494385
Epoch 130, val loss: 1.794529914855957
Epoch 140, training loss: 330.3006591796875 = 1.7890039682388306 + 50.0 * 6.57023286819458
Epoch 140, val loss: 1.788041353225708
Epoch 150, training loss: 328.4820251464844 = 1.7820305824279785 + 50.0 * 6.533999919891357
Epoch 150, val loss: 1.7814334630966187
Epoch 160, training loss: 327.0104064941406 = 1.774732232093811 + 50.0 * 6.504714012145996
Epoch 160, val loss: 1.7748031616210938
Epoch 170, training loss: 326.0592041015625 = 1.7667691707611084 + 50.0 * 6.485848903656006
Epoch 170, val loss: 1.7677357196807861
Epoch 180, training loss: 325.01324462890625 = 1.757879614830017 + 50.0 * 6.465107440948486
Epoch 180, val loss: 1.7597012519836426
Epoch 190, training loss: 324.207763671875 = 1.7478528022766113 + 50.0 * 6.449198246002197
Epoch 190, val loss: 1.7508704662322998
Epoch 200, training loss: 323.49420166015625 = 1.7368348836898804 + 50.0 * 6.435147285461426
Epoch 200, val loss: 1.7411837577819824
Epoch 210, training loss: 322.8945617675781 = 1.7248767614364624 + 50.0 * 6.423393249511719
Epoch 210, val loss: 1.7307279109954834
Epoch 220, training loss: 322.3643493652344 = 1.7115787267684937 + 50.0 * 6.413055419921875
Epoch 220, val loss: 1.7193175554275513
Epoch 230, training loss: 321.79022216796875 = 1.697235107421875 + 50.0 * 6.401859760284424
Epoch 230, val loss: 1.7070432901382446
Epoch 240, training loss: 321.31475830078125 = 1.6816563606262207 + 50.0 * 6.3926615715026855
Epoch 240, val loss: 1.6937973499298096
Epoch 250, training loss: 320.87615966796875 = 1.6648317575454712 + 50.0 * 6.3842267990112305
Epoch 250, val loss: 1.679490327835083
Epoch 260, training loss: 320.4662170410156 = 1.6466333866119385 + 50.0 * 6.376391887664795
Epoch 260, val loss: 1.6641337871551514
Epoch 270, training loss: 320.3674621582031 = 1.627072811126709 + 50.0 * 6.374807834625244
Epoch 270, val loss: 1.6475971937179565
Epoch 280, training loss: 319.7961120605469 = 1.6060081720352173 + 50.0 * 6.363801956176758
Epoch 280, val loss: 1.6299256086349487
Epoch 290, training loss: 319.4796142578125 = 1.5838052034378052 + 50.0 * 6.357916355133057
Epoch 290, val loss: 1.611296534538269
Epoch 300, training loss: 319.2158203125 = 1.560566782951355 + 50.0 * 6.353104591369629
Epoch 300, val loss: 1.591776728630066
Epoch 310, training loss: 318.9018859863281 = 1.5361225605010986 + 50.0 * 6.347314834594727
Epoch 310, val loss: 1.571498155593872
Epoch 320, training loss: 318.677734375 = 1.5108205080032349 + 50.0 * 6.3433380126953125
Epoch 320, val loss: 1.5505249500274658
Epoch 330, training loss: 318.4260559082031 = 1.4846420288085938 + 50.0 * 6.338828086853027
Epoch 330, val loss: 1.5290995836257935
Epoch 340, training loss: 318.21636962890625 = 1.457993507385254 + 50.0 * 6.335166931152344
Epoch 340, val loss: 1.5072827339172363
Epoch 350, training loss: 318.2080993652344 = 1.4305119514465332 + 50.0 * 6.335551738739014
Epoch 350, val loss: 1.4850850105285645
Epoch 360, training loss: 317.77557373046875 = 1.4030362367630005 + 50.0 * 6.327450275421143
Epoch 360, val loss: 1.4626832008361816
Epoch 370, training loss: 317.5605773925781 = 1.3753066062927246 + 50.0 * 6.323705196380615
Epoch 370, val loss: 1.4403772354125977
Epoch 380, training loss: 317.3870544433594 = 1.3475520610809326 + 50.0 * 6.320789813995361
Epoch 380, val loss: 1.4183528423309326
Epoch 390, training loss: 317.32861328125 = 1.319806694984436 + 50.0 * 6.320176601409912
Epoch 390, val loss: 1.3960340023040771
Epoch 400, training loss: 317.04541015625 = 1.2922552824020386 + 50.0 * 6.315062999725342
Epoch 400, val loss: 1.3743265867233276
Epoch 410, training loss: 316.823974609375 = 1.2650890350341797 + 50.0 * 6.311177730560303
Epoch 410, val loss: 1.352853775024414
Epoch 420, training loss: 316.6335754394531 = 1.2383424043655396 + 50.0 * 6.3079047203063965
Epoch 420, val loss: 1.331911563873291
Epoch 430, training loss: 316.5726013183594 = 1.2120662927627563 + 50.0 * 6.307210445404053
Epoch 430, val loss: 1.3112003803253174
Epoch 440, training loss: 316.4476318359375 = 1.1859441995620728 + 50.0 * 6.305233955383301
Epoch 440, val loss: 1.2912914752960205
Epoch 450, training loss: 316.2173156738281 = 1.1605303287506104 + 50.0 * 6.301136016845703
Epoch 450, val loss: 1.271459937095642
Epoch 460, training loss: 316.02728271484375 = 1.1355257034301758 + 50.0 * 6.297834873199463
Epoch 460, val loss: 1.2523454427719116
Epoch 470, training loss: 316.0236511230469 = 1.1111717224121094 + 50.0 * 6.2982497215271
Epoch 470, val loss: 1.2336763143539429
Epoch 480, training loss: 315.9924621582031 = 1.0871964693069458 + 50.0 * 6.298105239868164
Epoch 480, val loss: 1.215129017829895
Epoch 490, training loss: 315.6807556152344 = 1.0635939836502075 + 50.0 * 6.2923431396484375
Epoch 490, val loss: 1.1974554061889648
Epoch 500, training loss: 315.5428466796875 = 1.0406118631362915 + 50.0 * 6.290044784545898
Epoch 500, val loss: 1.1803901195526123
Epoch 510, training loss: 315.5275573730469 = 1.0181152820587158 + 50.0 * 6.290188789367676
Epoch 510, val loss: 1.1637521982192993
Epoch 520, training loss: 315.2973327636719 = 0.996303379535675 + 50.0 * 6.286020755767822
Epoch 520, val loss: 1.1475002765655518
Epoch 530, training loss: 315.1795654296875 = 0.9748431444168091 + 50.0 * 6.284094333648682
Epoch 530, val loss: 1.1318031549453735
Epoch 540, training loss: 315.11431884765625 = 0.9538502097129822 + 50.0 * 6.283209323883057
Epoch 540, val loss: 1.116639256477356
Epoch 550, training loss: 315.1679382324219 = 0.9332085847854614 + 50.0 * 6.284694671630859
Epoch 550, val loss: 1.1016899347305298
Epoch 560, training loss: 314.957763671875 = 0.9131442904472351 + 50.0 * 6.280892372131348
Epoch 560, val loss: 1.0872459411621094
Epoch 570, training loss: 314.77862548828125 = 0.8937079906463623 + 50.0 * 6.277698516845703
Epoch 570, val loss: 1.073349952697754
Epoch 580, training loss: 314.654541015625 = 0.8748602271080017 + 50.0 * 6.2755937576293945
Epoch 580, val loss: 1.0602188110351562
Epoch 590, training loss: 314.5486755371094 = 0.856484055519104 + 50.0 * 6.273844242095947
Epoch 590, val loss: 1.047474980354309
Epoch 600, training loss: 314.4914245605469 = 0.8386280536651611 + 50.0 * 6.2730560302734375
Epoch 600, val loss: 1.035061001777649
Epoch 610, training loss: 314.5620422363281 = 0.8208675980567932 + 50.0 * 6.2748236656188965
Epoch 610, val loss: 1.0231046676635742
Epoch 620, training loss: 314.37750244140625 = 0.8036384582519531 + 50.0 * 6.271477222442627
Epoch 620, val loss: 1.011317491531372
Epoch 630, training loss: 314.2463684082031 = 0.7869038581848145 + 50.0 * 6.269189357757568
Epoch 630, val loss: 1.0001202821731567
Epoch 640, training loss: 314.1377868652344 = 0.7707229852676392 + 50.0 * 6.267341613769531
Epoch 640, val loss: 0.989549994468689
Epoch 650, training loss: 314.04693603515625 = 0.7549842596054077 + 50.0 * 6.265839099884033
Epoch 650, val loss: 0.9794098138809204
Epoch 660, training loss: 314.6078796386719 = 0.739698052406311 + 50.0 * 6.2773637771606445
Epoch 660, val loss: 0.9693352580070496
Epoch 670, training loss: 313.9863586425781 = 0.7242451310157776 + 50.0 * 6.265242576599121
Epoch 670, val loss: 0.9600802063941956
Epoch 680, training loss: 313.86236572265625 = 0.7095564603805542 + 50.0 * 6.263055801391602
Epoch 680, val loss: 0.9508329033851624
Epoch 690, training loss: 313.775146484375 = 0.6952449679374695 + 50.0 * 6.261598110198975
Epoch 690, val loss: 0.942219078540802
Epoch 700, training loss: 313.68951416015625 = 0.6812406778335571 + 50.0 * 6.260165691375732
Epoch 700, val loss: 0.9338915348052979
Epoch 710, training loss: 313.99371337890625 = 0.6675432324409485 + 50.0 * 6.266523361206055
Epoch 710, val loss: 0.9257572293281555
Epoch 720, training loss: 313.68267822265625 = 0.6538229584693909 + 50.0 * 6.26057767868042
Epoch 720, val loss: 0.9182889461517334
Epoch 730, training loss: 313.596923828125 = 0.6404995918273926 + 50.0 * 6.259128570556641
Epoch 730, val loss: 0.9107670783996582
Epoch 740, training loss: 313.43780517578125 = 0.627534806728363 + 50.0 * 6.2562055587768555
Epoch 740, val loss: 0.9035830497741699
Epoch 750, training loss: 313.6615295410156 = 0.6147748827934265 + 50.0 * 6.260934829711914
Epoch 750, val loss: 0.8970714211463928
Epoch 760, training loss: 313.5233459472656 = 0.6021122932434082 + 50.0 * 6.258424758911133
Epoch 760, val loss: 0.8897655606269836
Epoch 770, training loss: 313.2602233886719 = 0.5896737575531006 + 50.0 * 6.253411293029785
Epoch 770, val loss: 0.8835426568984985
Epoch 780, training loss: 313.20513916015625 = 0.577514111995697 + 50.0 * 6.252552509307861
Epoch 780, val loss: 0.8773943185806274
Epoch 790, training loss: 313.5541076660156 = 0.565613329410553 + 50.0 * 6.259769916534424
Epoch 790, val loss: 0.8711414933204651
Epoch 800, training loss: 313.2486572265625 = 0.5534185171127319 + 50.0 * 6.253904819488525
Epoch 800, val loss: 0.8655951619148254
Epoch 810, training loss: 313.0980224609375 = 0.5417269468307495 + 50.0 * 6.251126289367676
Epoch 810, val loss: 0.8596439957618713
Epoch 820, training loss: 312.9928894042969 = 0.5301035642623901 + 50.0 * 6.249256134033203
Epoch 820, val loss: 0.8542642593383789
Epoch 830, training loss: 313.00238037109375 = 0.5187158584594727 + 50.0 * 6.249673366546631
Epoch 830, val loss: 0.8490319848060608
Epoch 840, training loss: 313.0299377441406 = 0.5072925686836243 + 50.0 * 6.250452995300293
Epoch 840, val loss: 0.8439789414405823
Epoch 850, training loss: 312.9082336425781 = 0.4961036145687103 + 50.0 * 6.2482428550720215
Epoch 850, val loss: 0.8387190699577332
Epoch 860, training loss: 312.7857666015625 = 0.4849904179573059 + 50.0 * 6.246015548706055
Epoch 860, val loss: 0.8341858386993408
Epoch 870, training loss: 312.956298828125 = 0.4740179181098938 + 50.0 * 6.249645233154297
Epoch 870, val loss: 0.8296383619308472
Epoch 880, training loss: 312.7745666503906 = 0.46325111389160156 + 50.0 * 6.2462263107299805
Epoch 880, val loss: 0.8250585794448853
Epoch 890, training loss: 312.71453857421875 = 0.4525195062160492 + 50.0 * 6.245240211486816
Epoch 890, val loss: 0.8208767771720886
Epoch 900, training loss: 312.8893127441406 = 0.442033052444458 + 50.0 * 6.248945236206055
Epoch 900, val loss: 0.816744327545166
Epoch 910, training loss: 312.676025390625 = 0.4314344525337219 + 50.0 * 6.244892120361328
Epoch 910, val loss: 0.8132922053337097
Epoch 920, training loss: 312.5318603515625 = 0.42121440172195435 + 50.0 * 6.242213249206543
Epoch 920, val loss: 0.8096035718917847
Epoch 930, training loss: 312.4791564941406 = 0.4111461639404297 + 50.0 * 6.241360664367676
Epoch 930, val loss: 0.8064842224121094
Epoch 940, training loss: 312.71673583984375 = 0.4012221693992615 + 50.0 * 6.246310234069824
Epoch 940, val loss: 0.8038716912269592
Epoch 950, training loss: 312.48541259765625 = 0.3914175033569336 + 50.0 * 6.241879940032959
Epoch 950, val loss: 0.8001771569252014
Epoch 960, training loss: 312.5419006347656 = 0.3817702531814575 + 50.0 * 6.2432026863098145
Epoch 960, val loss: 0.7980417609214783
Epoch 970, training loss: 312.3533630371094 = 0.37230539321899414 + 50.0 * 6.239621162414551
Epoch 970, val loss: 0.7953692078590393
Epoch 980, training loss: 312.2876892089844 = 0.3630892336368561 + 50.0 * 6.238491535186768
Epoch 980, val loss: 0.7931055426597595
Epoch 990, training loss: 312.2228698730469 = 0.3540375828742981 + 50.0 * 6.237376689910889
Epoch 990, val loss: 0.7912998795509338
Epoch 1000, training loss: 312.5188293457031 = 0.3452620506286621 + 50.0 * 6.243471145629883
Epoch 1000, val loss: 0.7895240783691406
Epoch 1010, training loss: 312.2769470214844 = 0.33648979663848877 + 50.0 * 6.238809108734131
Epoch 1010, val loss: 0.7882854342460632
Epoch 1020, training loss: 312.147216796875 = 0.3279978632926941 + 50.0 * 6.236384391784668
Epoch 1020, val loss: 0.7868560552597046
Epoch 1030, training loss: 312.27423095703125 = 0.3197660744190216 + 50.0 * 6.239089488983154
Epoch 1030, val loss: 0.785707950592041
Epoch 1040, training loss: 312.0545349121094 = 0.3115684986114502 + 50.0 * 6.234859466552734
Epoch 1040, val loss: 0.7852814793586731
Epoch 1050, training loss: 312.1131896972656 = 0.3036486506462097 + 50.0 * 6.2361907958984375
Epoch 1050, val loss: 0.7848464250564575
Epoch 1060, training loss: 312.0805358886719 = 0.29601094126701355 + 50.0 * 6.235690593719482
Epoch 1060, val loss: 0.784490704536438
Epoch 1070, training loss: 312.0821533203125 = 0.2885148227214813 + 50.0 * 6.235872745513916
Epoch 1070, val loss: 0.7843085527420044
Epoch 1080, training loss: 311.9991455078125 = 0.2811247408390045 + 50.0 * 6.234360218048096
Epoch 1080, val loss: 0.7846447229385376
Epoch 1090, training loss: 311.9121398925781 = 0.274016797542572 + 50.0 * 6.232762336730957
Epoch 1090, val loss: 0.7852921485900879
Epoch 1100, training loss: 311.9297180175781 = 0.26709747314453125 + 50.0 * 6.23325252532959
Epoch 1100, val loss: 0.785906970500946
Epoch 1110, training loss: 311.9494934082031 = 0.26035016775131226 + 50.0 * 6.23378324508667
Epoch 1110, val loss: 0.7867116332054138
Epoch 1120, training loss: 311.8486633300781 = 0.2538033723831177 + 50.0 * 6.231896877288818
Epoch 1120, val loss: 0.787190318107605
Epoch 1130, training loss: 312.1551513671875 = 0.24743276834487915 + 50.0 * 6.238154411315918
Epoch 1130, val loss: 0.7884588241577148
Epoch 1140, training loss: 311.861572265625 = 0.24105076491832733 + 50.0 * 6.232410430908203
Epoch 1140, val loss: 0.7900323867797852
Epoch 1150, training loss: 311.7394714355469 = 0.23507744073867798 + 50.0 * 6.230087757110596
Epoch 1150, val loss: 0.7914767861366272
Epoch 1160, training loss: 311.69073486328125 = 0.22923249006271362 + 50.0 * 6.2292304039001465
Epoch 1160, val loss: 0.7935022711753845
Epoch 1170, training loss: 311.8268737792969 = 0.22359539568424225 + 50.0 * 6.232065677642822
Epoch 1170, val loss: 0.7955780029296875
Epoch 1180, training loss: 311.6766662597656 = 0.21802105009555817 + 50.0 * 6.229172706604004
Epoch 1180, val loss: 0.7975424528121948
Epoch 1190, training loss: 311.6618347167969 = 0.2126232087612152 + 50.0 * 6.228984355926514
Epoch 1190, val loss: 0.7998692393302917
Epoch 1200, training loss: 311.59576416015625 = 0.20741532742977142 + 50.0 * 6.227766990661621
Epoch 1200, val loss: 0.8022276163101196
Epoch 1210, training loss: 311.8183288574219 = 0.20235911011695862 + 50.0 * 6.232319355010986
Epoch 1210, val loss: 0.8045502305030823
Epoch 1220, training loss: 311.6232604980469 = 0.1974037140607834 + 50.0 * 6.228517055511475
Epoch 1220, val loss: 0.8078048229217529
Epoch 1230, training loss: 311.5794982910156 = 0.19257600605487823 + 50.0 * 6.227738857269287
Epoch 1230, val loss: 0.8104674220085144
Epoch 1240, training loss: 311.9181213378906 = 0.18784086406230927 + 50.0 * 6.23460578918457
Epoch 1240, val loss: 0.813670814037323
Epoch 1250, training loss: 311.4794616699219 = 0.18328778445720673 + 50.0 * 6.225923538208008
Epoch 1250, val loss: 0.8163248896598816
Epoch 1260, training loss: 311.4819030761719 = 0.17888806760311127 + 50.0 * 6.226060390472412
Epoch 1260, val loss: 0.8192906379699707
Epoch 1270, training loss: 311.40606689453125 = 0.1746351569890976 + 50.0 * 6.224628925323486
Epoch 1270, val loss: 0.8227919936180115
Epoch 1280, training loss: 311.3817138671875 = 0.17054033279418945 + 50.0 * 6.224223613739014
Epoch 1280, val loss: 0.826384961605072
Epoch 1290, training loss: 311.3662109375 = 0.16654454171657562 + 50.0 * 6.223993301391602
Epoch 1290, val loss: 0.8298105597496033
Epoch 1300, training loss: 311.8273010253906 = 0.1626654863357544 + 50.0 * 6.233293056488037
Epoch 1300, val loss: 0.8330475687980652
Epoch 1310, training loss: 311.4673156738281 = 0.1587439477443695 + 50.0 * 6.226171493530273
Epoch 1310, val loss: 0.8373791575431824
Epoch 1320, training loss: 311.34661865234375 = 0.15497562289237976 + 50.0 * 6.223833084106445
Epoch 1320, val loss: 0.8408479690551758
Epoch 1330, training loss: 311.5228271484375 = 0.15137359499931335 + 50.0 * 6.227429389953613
Epoch 1330, val loss: 0.8447939157485962
Epoch 1340, training loss: 311.3337097167969 = 0.14781439304351807 + 50.0 * 6.22371768951416
Epoch 1340, val loss: 0.8484252095222473
Epoch 1350, training loss: 311.2426452636719 = 0.1443798542022705 + 50.0 * 6.221965312957764
Epoch 1350, val loss: 0.8526189923286438
Epoch 1360, training loss: 311.2404479980469 = 0.14105208218097687 + 50.0 * 6.221987724304199
Epoch 1360, val loss: 0.8566426634788513
Epoch 1370, training loss: 311.72918701171875 = 0.13779526948928833 + 50.0 * 6.231828212738037
Epoch 1370, val loss: 0.8610112071037292
Epoch 1380, training loss: 311.3621520996094 = 0.13463842868804932 + 50.0 * 6.224550247192383
Epoch 1380, val loss: 0.8645844459533691
Epoch 1390, training loss: 311.2582702636719 = 0.13149499893188477 + 50.0 * 6.222535133361816
Epoch 1390, val loss: 0.868859052658081
Epoch 1400, training loss: 311.23089599609375 = 0.12854665517807007 + 50.0 * 6.222047328948975
Epoch 1400, val loss: 0.8732957243919373
Epoch 1410, training loss: 311.1900329589844 = 0.12563826143741608 + 50.0 * 6.221287727355957
Epoch 1410, val loss: 0.8774895071983337
Epoch 1420, training loss: 311.1482849121094 = 0.12284721434116364 + 50.0 * 6.220509052276611
Epoch 1420, val loss: 0.8818921446800232
Epoch 1430, training loss: 311.192626953125 = 0.12011652439832687 + 50.0 * 6.221450328826904
Epoch 1430, val loss: 0.8862212300300598
Epoch 1440, training loss: 311.249755859375 = 0.11744345724582672 + 50.0 * 6.222646713256836
Epoch 1440, val loss: 0.8908597826957703
Epoch 1450, training loss: 311.1706237792969 = 0.11481571197509766 + 50.0 * 6.221116065979004
Epoch 1450, val loss: 0.8950551152229309
Epoch 1460, training loss: 311.1046447753906 = 0.11227484792470932 + 50.0 * 6.219847679138184
Epoch 1460, val loss: 0.8998222351074219
Epoch 1470, training loss: 311.32489013671875 = 0.10980293154716492 + 50.0 * 6.224301815032959
Epoch 1470, val loss: 0.904647707939148
Epoch 1480, training loss: 311.04278564453125 = 0.10742475837469101 + 50.0 * 6.218707084655762
Epoch 1480, val loss: 0.9084253311157227
Epoch 1490, training loss: 311.0114440917969 = 0.10509762912988663 + 50.0 * 6.2181267738342285
Epoch 1490, val loss: 0.9131354689598083
Epoch 1500, training loss: 310.9809875488281 = 0.10284353792667389 + 50.0 * 6.217562675476074
Epoch 1500, val loss: 0.9179890155792236
Epoch 1510, training loss: 311.0254211425781 = 0.10066118091344833 + 50.0 * 6.2184953689575195
Epoch 1510, val loss: 0.922548770904541
Epoch 1520, training loss: 311.18896484375 = 0.09851278364658356 + 50.0 * 6.221809387207031
Epoch 1520, val loss: 0.9269377589225769
Epoch 1530, training loss: 311.004638671875 = 0.09639401733875275 + 50.0 * 6.218164443969727
Epoch 1530, val loss: 0.9319801330566406
Epoch 1540, training loss: 311.02215576171875 = 0.09435714781284332 + 50.0 * 6.218555927276611
Epoch 1540, val loss: 0.9362890720367432
Epoch 1550, training loss: 311.0932922363281 = 0.0923752635717392 + 50.0 * 6.22001838684082
Epoch 1550, val loss: 0.940814733505249
Epoch 1560, training loss: 310.8805847167969 = 0.09040656685829163 + 50.0 * 6.215804100036621
Epoch 1560, val loss: 0.9459846615791321
Epoch 1570, training loss: 310.9103088378906 = 0.08853283524513245 + 50.0 * 6.216435432434082
Epoch 1570, val loss: 0.9509338140487671
Epoch 1580, training loss: 310.9283447265625 = 0.08671316504478455 + 50.0 * 6.216832637786865
Epoch 1580, val loss: 0.9556447267532349
Epoch 1590, training loss: 311.0083312988281 = 0.0849263072013855 + 50.0 * 6.218467712402344
Epoch 1590, val loss: 0.9602262377738953
Epoch 1600, training loss: 310.8784484863281 = 0.08319386839866638 + 50.0 * 6.21590518951416
Epoch 1600, val loss: 0.9647971987724304
Epoch 1610, training loss: 310.8147888183594 = 0.0815025046467781 + 50.0 * 6.214665412902832
Epoch 1610, val loss: 0.9696138501167297
Epoch 1620, training loss: 310.9646301269531 = 0.07986966520547867 + 50.0 * 6.217695236206055
Epoch 1620, val loss: 0.9743679761886597
Epoch 1630, training loss: 311.0710144042969 = 0.07827864587306976 + 50.0 * 6.219854831695557
Epoch 1630, val loss: 0.9785618782043457
Epoch 1640, training loss: 310.8627624511719 = 0.07666110247373581 + 50.0 * 6.21572208404541
Epoch 1640, val loss: 0.9835138320922852
Epoch 1650, training loss: 310.9129638671875 = 0.07514116913080215 + 50.0 * 6.216756343841553
Epoch 1650, val loss: 0.9877817630767822
Epoch 1660, training loss: 310.7732238769531 = 0.07363007962703705 + 50.0 * 6.213991641998291
Epoch 1660, val loss: 0.9928932189941406
Epoch 1670, training loss: 310.7335205078125 = 0.07217578589916229 + 50.0 * 6.213226795196533
Epoch 1670, val loss: 0.9980015754699707
Epoch 1680, training loss: 310.8290100097656 = 0.07077857106924057 + 50.0 * 6.215164661407471
Epoch 1680, val loss: 1.0030845403671265
Epoch 1690, training loss: 310.8260803222656 = 0.06939032673835754 + 50.0 * 6.2151336669921875
Epoch 1690, val loss: 1.007594347000122
Epoch 1700, training loss: 310.75872802734375 = 0.06805957853794098 + 50.0 * 6.213813781738281
Epoch 1700, val loss: 1.011520504951477
Epoch 1710, training loss: 310.8585510253906 = 0.066737100481987 + 50.0 * 6.215836048126221
Epoch 1710, val loss: 1.016427755355835
Epoch 1720, training loss: 310.7105407714844 = 0.06544460356235504 + 50.0 * 6.212902069091797
Epoch 1720, val loss: 1.0215986967086792
Epoch 1730, training loss: 310.6935729980469 = 0.06421326100826263 + 50.0 * 6.212587356567383
Epoch 1730, val loss: 1.0258519649505615
Epoch 1740, training loss: 310.86541748046875 = 0.06300431489944458 + 50.0 * 6.216048240661621
Epoch 1740, val loss: 1.0306360721588135
Epoch 1750, training loss: 310.6546325683594 = 0.0617985837161541 + 50.0 * 6.211856842041016
Epoch 1750, val loss: 1.0353927612304688
Epoch 1760, training loss: 310.6636047363281 = 0.06062436103820801 + 50.0 * 6.212059497833252
Epoch 1760, val loss: 1.0401114225387573
Epoch 1770, training loss: 310.7409362792969 = 0.05951419845223427 + 50.0 * 6.21362829208374
Epoch 1770, val loss: 1.0447721481323242
Epoch 1780, training loss: 310.6710510253906 = 0.05840517207980156 + 50.0 * 6.212253093719482
Epoch 1780, val loss: 1.0492558479309082
Epoch 1790, training loss: 310.72674560546875 = 0.05733524262905121 + 50.0 * 6.213387966156006
Epoch 1790, val loss: 1.0535510778427124
Epoch 1800, training loss: 310.61444091796875 = 0.05625864490866661 + 50.0 * 6.2111639976501465
Epoch 1800, val loss: 1.0586153268814087
Epoch 1810, training loss: 310.5727844238281 = 0.05523379519581795 + 50.0 * 6.21035099029541
Epoch 1810, val loss: 1.0634527206420898
Epoch 1820, training loss: 310.7034606933594 = 0.054245851933956146 + 50.0 * 6.212984085083008
Epoch 1820, val loss: 1.0683765411376953
Epoch 1830, training loss: 310.6506652832031 = 0.05324982479214668 + 50.0 * 6.211948394775391
Epoch 1830, val loss: 1.0728247165679932
Epoch 1840, training loss: 310.72515869140625 = 0.05228640139102936 + 50.0 * 6.2134575843811035
Epoch 1840, val loss: 1.0772064924240112
Epoch 1850, training loss: 310.54583740234375 = 0.05134747177362442 + 50.0 * 6.2098894119262695
Epoch 1850, val loss: 1.0815428495407104
Epoch 1860, training loss: 310.57965087890625 = 0.0504438541829586 + 50.0 * 6.2105841636657715
Epoch 1860, val loss: 1.0859413146972656
Epoch 1870, training loss: 310.62432861328125 = 0.04955495521426201 + 50.0 * 6.211495399475098
Epoch 1870, val loss: 1.0903860330581665
Epoch 1880, training loss: 310.5237121582031 = 0.04867391288280487 + 50.0 * 6.209500789642334
Epoch 1880, val loss: 1.0953859090805054
Epoch 1890, training loss: 310.66912841796875 = 0.047825153917074203 + 50.0 * 6.21242618560791
Epoch 1890, val loss: 1.1003080606460571
Epoch 1900, training loss: 310.6300354003906 = 0.04699347913265228 + 50.0 * 6.211660861968994
Epoch 1900, val loss: 1.1044514179229736
Epoch 1910, training loss: 310.4906921386719 = 0.04618457704782486 + 50.0 * 6.208890438079834
Epoch 1910, val loss: 1.1084989309310913
Epoch 1920, training loss: 310.45172119140625 = 0.045383911579847336 + 50.0 * 6.208126544952393
Epoch 1920, val loss: 1.1132844686508179
Epoch 1930, training loss: 310.48138427734375 = 0.0446162074804306 + 50.0 * 6.208735466003418
Epoch 1930, val loss: 1.1178009510040283
Epoch 1940, training loss: 310.65673828125 = 0.043871719390153885 + 50.0 * 6.212257385253906
Epoch 1940, val loss: 1.122154951095581
Epoch 1950, training loss: 310.5146179199219 = 0.043120741844177246 + 50.0 * 6.209429740905762
Epoch 1950, val loss: 1.1265485286712646
Epoch 1960, training loss: 310.5146789550781 = 0.04239379242062569 + 50.0 * 6.209445953369141
Epoch 1960, val loss: 1.1308307647705078
Epoch 1970, training loss: 310.54925537109375 = 0.041674207895994186 + 50.0 * 6.210151195526123
Epoch 1970, val loss: 1.1354591846466064
Epoch 1980, training loss: 310.62261962890625 = 0.04098457098007202 + 50.0 * 6.21163272857666
Epoch 1980, val loss: 1.1400827169418335
Epoch 1990, training loss: 310.44683837890625 = 0.04029469192028046 + 50.0 * 6.208130836486816
Epoch 1990, val loss: 1.14371657371521
Epoch 2000, training loss: 310.3952331542969 = 0.03963415324687958 + 50.0 * 6.207111835479736
Epoch 2000, val loss: 1.1479007005691528
Epoch 2010, training loss: 310.3606872558594 = 0.03899916633963585 + 50.0 * 6.2064337730407715
Epoch 2010, val loss: 1.152461290359497
Epoch 2020, training loss: 310.43804931640625 = 0.038387566804885864 + 50.0 * 6.207993030548096
Epoch 2020, val loss: 1.1565226316452026
Epoch 2030, training loss: 310.6317138671875 = 0.037777092307806015 + 50.0 * 6.211878776550293
Epoch 2030, val loss: 1.160526156425476
Epoch 2040, training loss: 310.4391174316406 = 0.037137310951948166 + 50.0 * 6.208039283752441
Epoch 2040, val loss: 1.1652165651321411
Epoch 2050, training loss: 310.34796142578125 = 0.03655179217457771 + 50.0 * 6.206227779388428
Epoch 2050, val loss: 1.16936457157135
Epoch 2060, training loss: 310.3578186035156 = 0.03598065301775932 + 50.0 * 6.206436634063721
Epoch 2060, val loss: 1.1738836765289307
Epoch 2070, training loss: 310.6326904296875 = 0.035432688891887665 + 50.0 * 6.211945056915283
Epoch 2070, val loss: 1.1780104637145996
Epoch 2080, training loss: 310.42034912109375 = 0.034854330122470856 + 50.0 * 6.207709789276123
Epoch 2080, val loss: 1.1816242933273315
Epoch 2090, training loss: 310.4507751464844 = 0.034312985837459564 + 50.0 * 6.208329200744629
Epoch 2090, val loss: 1.185801386833191
Epoch 2100, training loss: 310.35546875 = 0.03377348929643631 + 50.0 * 6.2064337730407715
Epoch 2100, val loss: 1.1901822090148926
Epoch 2110, training loss: 310.40447998046875 = 0.033250272274017334 + 50.0 * 6.207424163818359
Epoch 2110, val loss: 1.1946476697921753
Epoch 2120, training loss: 310.3275451660156 = 0.032736603170633316 + 50.0 * 6.205895900726318
Epoch 2120, val loss: 1.1986849308013916
Epoch 2130, training loss: 310.33770751953125 = 0.03223931044340134 + 50.0 * 6.206109523773193
Epoch 2130, val loss: 1.202994704246521
Epoch 2140, training loss: 310.343505859375 = 0.03175363689661026 + 50.0 * 6.206234931945801
Epoch 2140, val loss: 1.207230806350708
Epoch 2150, training loss: 310.30792236328125 = 0.031276874244213104 + 50.0 * 6.205532550811768
Epoch 2150, val loss: 1.2112481594085693
Epoch 2160, training loss: 310.46893310546875 = 0.03081946074962616 + 50.0 * 6.208762168884277
Epoch 2160, val loss: 1.2156850099563599
Epoch 2170, training loss: 310.28594970703125 = 0.03033883310854435 + 50.0 * 6.205112457275391
Epoch 2170, val loss: 1.2189772129058838
Epoch 2180, training loss: 310.20635986328125 = 0.02989514172077179 + 50.0 * 6.203528881072998
Epoch 2180, val loss: 1.222898006439209
Epoch 2190, training loss: 310.2073974609375 = 0.02946213260293007 + 50.0 * 6.203558921813965
Epoch 2190, val loss: 1.2268372774124146
Epoch 2200, training loss: 310.2582702636719 = 0.029043810442090034 + 50.0 * 6.204584121704102
Epoch 2200, val loss: 1.2307696342468262
Epoch 2210, training loss: 310.5912170410156 = 0.02863493375480175 + 50.0 * 6.211251735687256
Epoch 2210, val loss: 1.2346447706222534
Epoch 2220, training loss: 310.3476867675781 = 0.02818359062075615 + 50.0 * 6.206389904022217
Epoch 2220, val loss: 1.2390154600143433
Epoch 2230, training loss: 310.2200012207031 = 0.02776964008808136 + 50.0 * 6.2038445472717285
Epoch 2230, val loss: 1.2424739599227905
Epoch 2240, training loss: 310.1698303222656 = 0.027373697608709335 + 50.0 * 6.202849388122559
Epoch 2240, val loss: 1.2467272281646729
Epoch 2250, training loss: 310.1455383300781 = 0.02699306793510914 + 50.0 * 6.202370643615723
Epoch 2250, val loss: 1.2506604194641113
Epoch 2260, training loss: 310.1815490722656 = 0.026621144264936447 + 50.0 * 6.203098773956299
Epoch 2260, val loss: 1.2543519735336304
Epoch 2270, training loss: 310.60662841796875 = 0.026254132390022278 + 50.0 * 6.211607456207275
Epoch 2270, val loss: 1.2575416564941406
Epoch 2280, training loss: 310.1639709472656 = 0.025863857939839363 + 50.0 * 6.202762126922607
Epoch 2280, val loss: 1.261871576309204
Epoch 2290, training loss: 310.1473693847656 = 0.025497503578662872 + 50.0 * 6.202436923980713
Epoch 2290, val loss: 1.265909194946289
Epoch 2300, training loss: 310.2288513183594 = 0.025150153785943985 + 50.0 * 6.204073905944824
Epoch 2300, val loss: 1.2695269584655762
Epoch 2310, training loss: 310.2010498046875 = 0.024807220324873924 + 50.0 * 6.203525066375732
Epoch 2310, val loss: 1.273495078086853
Epoch 2320, training loss: 310.16192626953125 = 0.024467896670103073 + 50.0 * 6.202749252319336
Epoch 2320, val loss: 1.2770721912384033
Epoch 2330, training loss: 310.605224609375 = 0.024155626073479652 + 50.0 * 6.211621284484863
Epoch 2330, val loss: 1.2802232503890991
Epoch 2340, training loss: 310.2171325683594 = 0.023787545040249825 + 50.0 * 6.203866958618164
Epoch 2340, val loss: 1.2845159769058228
Epoch 2350, training loss: 310.1311340332031 = 0.023465381935238838 + 50.0 * 6.202153205871582
Epoch 2350, val loss: 1.2880067825317383
Epoch 2360, training loss: 310.08282470703125 = 0.023149214684963226 + 50.0 * 6.201193332672119
Epoch 2360, val loss: 1.292014479637146
Epoch 2370, training loss: 310.0601501464844 = 0.02285175211727619 + 50.0 * 6.200745582580566
Epoch 2370, val loss: 1.295861840248108
Epoch 2380, training loss: 310.1814880371094 = 0.02256201207637787 + 50.0 * 6.203178405761719
Epoch 2380, val loss: 1.3000801801681519
Epoch 2390, training loss: 310.1839904785156 = 0.02225683256983757 + 50.0 * 6.203235149383545
Epoch 2390, val loss: 1.3031855821609497
Epoch 2400, training loss: 310.10986328125 = 0.02195962704718113 + 50.0 * 6.201757907867432
Epoch 2400, val loss: 1.306632161140442
Epoch 2410, training loss: 310.0346984863281 = 0.021670689806342125 + 50.0 * 6.200260639190674
Epoch 2410, val loss: 1.3102883100509644
Epoch 2420, training loss: 310.032958984375 = 0.021396607160568237 + 50.0 * 6.200231075286865
Epoch 2420, val loss: 1.3138582706451416
Epoch 2430, training loss: 310.1792297363281 = 0.021138593554496765 + 50.0 * 6.203161716461182
Epoch 2430, val loss: 1.3170994520187378
Epoch 2440, training loss: 310.2000732421875 = 0.020859017968177795 + 50.0 * 6.20358419418335
Epoch 2440, val loss: 1.3206870555877686
Epoch 2450, training loss: 310.0850830078125 = 0.02057868428528309 + 50.0 * 6.201290130615234
Epoch 2450, val loss: 1.3241944313049316
Epoch 2460, training loss: 310.0495910644531 = 0.020313719287514687 + 50.0 * 6.20058536529541
Epoch 2460, val loss: 1.327924370765686
Epoch 2470, training loss: 310.0176086425781 = 0.020065097138285637 + 50.0 * 6.199951171875
Epoch 2470, val loss: 1.3314356803894043
Epoch 2480, training loss: 310.1932067871094 = 0.019825449213385582 + 50.0 * 6.20346736907959
Epoch 2480, val loss: 1.3348089456558228
Epoch 2490, training loss: 310.0177307128906 = 0.019567111507058144 + 50.0 * 6.199963092803955
Epoch 2490, val loss: 1.3383049964904785
Epoch 2500, training loss: 309.99786376953125 = 0.019316887483000755 + 50.0 * 6.199570655822754
Epoch 2500, val loss: 1.3418066501617432
Epoch 2510, training loss: 310.1235046386719 = 0.019089149311184883 + 50.0 * 6.202087879180908
Epoch 2510, val loss: 1.3450714349746704
Epoch 2520, training loss: 310.0513000488281 = 0.018851514905691147 + 50.0 * 6.200648784637451
Epoch 2520, val loss: 1.3483623266220093
Epoch 2530, training loss: 310.001220703125 = 0.018617045134305954 + 50.0 * 6.199652194976807
Epoch 2530, val loss: 1.3521883487701416
Epoch 2540, training loss: 309.9876403808594 = 0.01839482970535755 + 50.0 * 6.199384689331055
Epoch 2540, val loss: 1.3555337190628052
Epoch 2550, training loss: 310.04150390625 = 0.018179545179009438 + 50.0 * 6.200466156005859
Epoch 2550, val loss: 1.3589025735855103
Epoch 2560, training loss: 310.1469421386719 = 0.017967645078897476 + 50.0 * 6.202579498291016
Epoch 2560, val loss: 1.361864686012268
Epoch 2570, training loss: 310.0515441894531 = 0.017739959061145782 + 50.0 * 6.200676441192627
Epoch 2570, val loss: 1.3654781579971313
Epoch 2580, training loss: 310.0010681152344 = 0.017527449876070023 + 50.0 * 6.199670791625977
Epoch 2580, val loss: 1.3694120645523071
Epoch 2590, training loss: 309.9577331542969 = 0.017323803156614304 + 50.0 * 6.198807716369629
Epoch 2590, val loss: 1.372622013092041
Epoch 2600, training loss: 310.008056640625 = 0.017130253836512566 + 50.0 * 6.1998186111450195
Epoch 2600, val loss: 1.3759163618087769
Epoch 2610, training loss: 310.155517578125 = 0.016935547813773155 + 50.0 * 6.2027716636657715
Epoch 2610, val loss: 1.378688097000122
Epoch 2620, training loss: 309.9540710449219 = 0.016723668202757835 + 50.0 * 6.198747158050537
Epoch 2620, val loss: 1.3820030689239502
Epoch 2630, training loss: 309.9261169433594 = 0.016531432047486305 + 50.0 * 6.1981916427612305
Epoch 2630, val loss: 1.3855550289154053
Epoch 2640, training loss: 309.9364318847656 = 0.016349339857697487 + 50.0 * 6.19840145111084
Epoch 2640, val loss: 1.3885228633880615
Epoch 2650, training loss: 310.1953125 = 0.016174735501408577 + 50.0 * 6.203582763671875
Epoch 2650, val loss: 1.391350269317627
Epoch 2660, training loss: 310.0346374511719 = 0.015979617834091187 + 50.0 * 6.200372695922852
Epoch 2660, val loss: 1.3949613571166992
Epoch 2670, training loss: 309.8955078125 = 0.015792949125170708 + 50.0 * 6.197593688964844
Epoch 2670, val loss: 1.3979032039642334
Epoch 2680, training loss: 309.8809814453125 = 0.015618547797203064 + 50.0 * 6.197307109832764
Epoch 2680, val loss: 1.4017966985702515
Epoch 2690, training loss: 309.96832275390625 = 0.015450051054358482 + 50.0 * 6.199057579040527
Epoch 2690, val loss: 1.4048181772232056
Epoch 2700, training loss: 309.9203796386719 = 0.015280326828360558 + 50.0 * 6.198101997375488
Epoch 2700, val loss: 1.4074684381484985
Epoch 2710, training loss: 310.093017578125 = 0.015119319781661034 + 50.0 * 6.2015581130981445
Epoch 2710, val loss: 1.4102479219436646
Epoch 2720, training loss: 309.9610290527344 = 0.014941209927201271 + 50.0 * 6.1989216804504395
Epoch 2720, val loss: 1.4139219522476196
Epoch 2730, training loss: 309.8949279785156 = 0.014774209819734097 + 50.0 * 6.197603225708008
Epoch 2730, val loss: 1.4172626733779907
Epoch 2740, training loss: 309.86651611328125 = 0.014616957865655422 + 50.0 * 6.197037696838379
Epoch 2740, val loss: 1.4201945066452026
Epoch 2750, training loss: 309.8856506347656 = 0.014466332271695137 + 50.0 * 6.197423934936523
Epoch 2750, val loss: 1.4235409498214722
Epoch 2760, training loss: 309.95928955078125 = 0.01431471761316061 + 50.0 * 6.198899269104004
Epoch 2760, val loss: 1.4264602661132812
Epoch 2770, training loss: 309.8919677734375 = 0.01415965799242258 + 50.0 * 6.197556018829346
Epoch 2770, val loss: 1.4287530183792114
Epoch 2780, training loss: 309.9887390136719 = 0.014013662934303284 + 50.0 * 6.1994948387146
Epoch 2780, val loss: 1.4316041469573975
Epoch 2790, training loss: 309.8086242675781 = 0.013853596523404121 + 50.0 * 6.195895195007324
Epoch 2790, val loss: 1.4350531101226807
Epoch 2800, training loss: 309.8269958496094 = 0.013707812875509262 + 50.0 * 6.196265697479248
Epoch 2800, val loss: 1.4381589889526367
Epoch 2810, training loss: 309.8688049316406 = 0.013571822084486485 + 50.0 * 6.197104454040527
Epoch 2810, val loss: 1.4409934282302856
Epoch 2820, training loss: 309.9073486328125 = 0.013434060849249363 + 50.0 * 6.197878360748291
Epoch 2820, val loss: 1.4437751770019531
Epoch 2830, training loss: 310.0011291503906 = 0.013299500569701195 + 50.0 * 6.199756622314453
Epoch 2830, val loss: 1.4470051527023315
Epoch 2840, training loss: 309.842529296875 = 0.013147780671715736 + 50.0 * 6.196587562561035
Epoch 2840, val loss: 1.4497194290161133
Epoch 2850, training loss: 309.8654479980469 = 0.013013960793614388 + 50.0 * 6.197049140930176
Epoch 2850, val loss: 1.4530454874038696
Epoch 2860, training loss: 309.8443908691406 = 0.012881090864539146 + 50.0 * 6.196630001068115
Epoch 2860, val loss: 1.4555543661117554
Epoch 2870, training loss: 309.7841491699219 = 0.01275308895856142 + 50.0 * 6.195427894592285
Epoch 2870, val loss: 1.4578744173049927
Epoch 2880, training loss: 309.8020324707031 = 0.012630042620003223 + 50.0 * 6.1957879066467285
Epoch 2880, val loss: 1.4606678485870361
Epoch 2890, training loss: 309.8518981933594 = 0.012505998834967613 + 50.0 * 6.1967878341674805
Epoch 2890, val loss: 1.463393211364746
Epoch 2900, training loss: 309.8612365722656 = 0.01238227728754282 + 50.0 * 6.196977138519287
Epoch 2900, val loss: 1.4661694765090942
Epoch 2910, training loss: 309.8324279785156 = 0.012258931994438171 + 50.0 * 6.1964030265808105
Epoch 2910, val loss: 1.469061017036438
Epoch 2920, training loss: 309.9320983886719 = 0.012138843536376953 + 50.0 * 6.198399066925049
Epoch 2920, val loss: 1.4716399908065796
Epoch 2930, training loss: 309.7334289550781 = 0.012014075182378292 + 50.0 * 6.194427967071533
Epoch 2930, val loss: 1.4744749069213867
Epoch 2940, training loss: 309.7166442871094 = 0.011898401193320751 + 50.0 * 6.194095134735107
Epoch 2940, val loss: 1.4776338338851929
Epoch 2950, training loss: 309.74517822265625 = 0.011786231771111488 + 50.0 * 6.194667816162109
Epoch 2950, val loss: 1.480653166770935
Epoch 2960, training loss: 309.9928283691406 = 0.011674817651510239 + 50.0 * 6.199622631072998
Epoch 2960, val loss: 1.4834880828857422
Epoch 2970, training loss: 309.7607421875 = 0.011560805141925812 + 50.0 * 6.19498348236084
Epoch 2970, val loss: 1.4852403402328491
Epoch 2980, training loss: 309.8166809082031 = 0.01144894864410162 + 50.0 * 6.196105003356934
Epoch 2980, val loss: 1.4883724451065063
Epoch 2990, training loss: 309.8306884765625 = 0.011338017880916595 + 50.0 * 6.196386814117432
Epoch 2990, val loss: 1.490769624710083
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 431.78216552734375 = 1.9406465291976929 + 50.0 * 8.596830368041992
Epoch 0, val loss: 1.945444107055664
Epoch 10, training loss: 431.72845458984375 = 1.93195378780365 + 50.0 * 8.595930099487305
Epoch 10, val loss: 1.9359413385391235
Epoch 20, training loss: 431.4039001464844 = 1.92129647731781 + 50.0 * 8.589652061462402
Epoch 20, val loss: 1.9243348836898804
Epoch 30, training loss: 429.252197265625 = 1.9076870679855347 + 50.0 * 8.546890258789062
Epoch 30, val loss: 1.909546971321106
Epoch 40, training loss: 417.139892578125 = 1.891076922416687 + 50.0 * 8.304976463317871
Epoch 40, val loss: 1.8920369148254395
Epoch 50, training loss: 375.3434753417969 = 1.8723758459091187 + 50.0 * 7.469421863555908
Epoch 50, val loss: 1.873066782951355
Epoch 60, training loss: 361.80596923828125 = 1.8567732572555542 + 50.0 * 7.198983669281006
Epoch 60, val loss: 1.8585549592971802
Epoch 70, training loss: 354.3836975097656 = 1.8442134857177734 + 50.0 * 7.050789833068848
Epoch 70, val loss: 1.846785545349121
Epoch 80, training loss: 349.11370849609375 = 1.8322099447250366 + 50.0 * 6.945629596710205
Epoch 80, val loss: 1.8354662656784058
Epoch 90, training loss: 343.3715515136719 = 1.8219972848892212 + 50.0 * 6.830991268157959
Epoch 90, val loss: 1.8260945081710815
Epoch 100, training loss: 338.2330627441406 = 1.8135502338409424 + 50.0 * 6.728390216827393
Epoch 100, val loss: 1.8179638385772705
Epoch 110, training loss: 334.5736999511719 = 1.8064781427383423 + 50.0 * 6.655344009399414
Epoch 110, val loss: 1.8109054565429688
Epoch 120, training loss: 331.895263671875 = 1.7993141412734985 + 50.0 * 6.601918697357178
Epoch 120, val loss: 1.8041895627975464
Epoch 130, training loss: 329.94671630859375 = 1.7917580604553223 + 50.0 * 6.563099384307861
Epoch 130, val loss: 1.7975050210952759
Epoch 140, training loss: 328.462890625 = 1.7840837240219116 + 50.0 * 6.533576011657715
Epoch 140, val loss: 1.7906429767608643
Epoch 150, training loss: 327.2578430175781 = 1.7760472297668457 + 50.0 * 6.5096354484558105
Epoch 150, val loss: 1.7833518981933594
Epoch 160, training loss: 326.2417907714844 = 1.7675029039382935 + 50.0 * 6.489485740661621
Epoch 160, val loss: 1.7756494283676147
Epoch 170, training loss: 325.5688781738281 = 1.7584105730056763 + 50.0 * 6.4762091636657715
Epoch 170, val loss: 1.767662525177002
Epoch 180, training loss: 324.702880859375 = 1.7485140562057495 + 50.0 * 6.459087371826172
Epoch 180, val loss: 1.7592519521713257
Epoch 190, training loss: 324.0428161621094 = 1.7379791736602783 + 50.0 * 6.446096897125244
Epoch 190, val loss: 1.7502994537353516
Epoch 200, training loss: 323.4232177734375 = 1.7267274856567383 + 50.0 * 6.433929920196533
Epoch 200, val loss: 1.740837574005127
Epoch 210, training loss: 322.7682800292969 = 1.714593529701233 + 50.0 * 6.4210734367370605
Epoch 210, val loss: 1.7308404445648193
Epoch 220, training loss: 322.195556640625 = 1.7016313076019287 + 50.0 * 6.409878253936768
Epoch 220, val loss: 1.7203612327575684
Epoch 230, training loss: 321.6991271972656 = 1.6875861883163452 + 50.0 * 6.400230407714844
Epoch 230, val loss: 1.7090553045272827
Epoch 240, training loss: 321.1664733886719 = 1.6722592115402222 + 50.0 * 6.389883995056152
Epoch 240, val loss: 1.6968238353729248
Epoch 250, training loss: 320.7577209472656 = 1.6558562517166138 + 50.0 * 6.38203763961792
Epoch 250, val loss: 1.6837735176086426
Epoch 260, training loss: 320.37298583984375 = 1.6383146047592163 + 50.0 * 6.374693870544434
Epoch 260, val loss: 1.6698939800262451
Epoch 270, training loss: 320.04486083984375 = 1.6195151805877686 + 50.0 * 6.368507385253906
Epoch 270, val loss: 1.6551213264465332
Epoch 280, training loss: 319.8034973144531 = 1.5995324850082397 + 50.0 * 6.364079475402832
Epoch 280, val loss: 1.639473557472229
Epoch 290, training loss: 319.3838806152344 = 1.5786693096160889 + 50.0 * 6.356103897094727
Epoch 290, val loss: 1.6232174634933472
Epoch 300, training loss: 319.16766357421875 = 1.5569591522216797 + 50.0 * 6.352214336395264
Epoch 300, val loss: 1.6064856052398682
Epoch 310, training loss: 318.8421325683594 = 1.5343502759933472 + 50.0 * 6.346155166625977
Epoch 310, val loss: 1.589298963546753
Epoch 320, training loss: 318.59326171875 = 1.51106858253479 + 50.0 * 6.341643810272217
Epoch 320, val loss: 1.5716184377670288
Epoch 330, training loss: 318.48931884765625 = 1.4871362447738647 + 50.0 * 6.340044021606445
Epoch 330, val loss: 1.5536859035491943
Epoch 340, training loss: 318.1531982421875 = 1.4628822803497314 + 50.0 * 6.333806037902832
Epoch 340, val loss: 1.5354102849960327
Epoch 350, training loss: 317.8819274902344 = 1.4381650686264038 + 50.0 * 6.3288750648498535
Epoch 350, val loss: 1.517166018486023
Epoch 360, training loss: 317.6534729003906 = 1.4133447408676147 + 50.0 * 6.324802875518799
Epoch 360, val loss: 1.4989503622055054
Epoch 370, training loss: 317.65606689453125 = 1.3884028196334839 + 50.0 * 6.325353145599365
Epoch 370, val loss: 1.480752944946289
Epoch 380, training loss: 317.3396911621094 = 1.3631104230880737 + 50.0 * 6.3195319175720215
Epoch 380, val loss: 1.4625266790390015
Epoch 390, training loss: 317.1328125 = 1.3379571437835693 + 50.0 * 6.315896987915039
Epoch 390, val loss: 1.444557785987854
Epoch 400, training loss: 316.917724609375 = 1.3129509687423706 + 50.0 * 6.3120951652526855
Epoch 400, val loss: 1.426835298538208
Epoch 410, training loss: 317.0765380859375 = 1.2879247665405273 + 50.0 * 6.31577205657959
Epoch 410, val loss: 1.4092272520065308
Epoch 420, training loss: 316.6774597167969 = 1.263021469116211 + 50.0 * 6.30828857421875
Epoch 420, val loss: 1.3917580842971802
Epoch 430, training loss: 316.453369140625 = 1.2383917570114136 + 50.0 * 6.304299354553223
Epoch 430, val loss: 1.374639868736267
Epoch 440, training loss: 316.49420166015625 = 1.2138826847076416 + 50.0 * 6.305606365203857
Epoch 440, val loss: 1.3578540086746216
Epoch 450, training loss: 316.2619934082031 = 1.1896387338638306 + 50.0 * 6.30144739151001
Epoch 450, val loss: 1.3411649465560913
Epoch 460, training loss: 316.0016784667969 = 1.1654781103134155 + 50.0 * 6.296724319458008
Epoch 460, val loss: 1.3249539136886597
Epoch 470, training loss: 315.8438720703125 = 1.141711950302124 + 50.0 * 6.294043064117432
Epoch 470, val loss: 1.3090695142745972
Epoch 480, training loss: 316.023681640625 = 1.1181308031082153 + 50.0 * 6.2981109619140625
Epoch 480, val loss: 1.2934143543243408
Epoch 490, training loss: 315.689697265625 = 1.0945897102355957 + 50.0 * 6.2919020652771
Epoch 490, val loss: 1.2779990434646606
Epoch 500, training loss: 315.4915771484375 = 1.0715011358261108 + 50.0 * 6.2884016036987305
Epoch 500, val loss: 1.2629586458206177
Epoch 510, training loss: 315.4810485839844 = 1.0486657619476318 + 50.0 * 6.288647174835205
Epoch 510, val loss: 1.248350977897644
Epoch 520, training loss: 315.3092346191406 = 1.0260820388793945 + 50.0 * 6.28566312789917
Epoch 520, val loss: 1.2339270114898682
Epoch 530, training loss: 315.1625671386719 = 1.0037753582000732 + 50.0 * 6.283175468444824
Epoch 530, val loss: 1.2199372053146362
Epoch 540, training loss: 315.02532958984375 = 0.9818605780601501 + 50.0 * 6.280869007110596
Epoch 540, val loss: 1.2063223123550415
Epoch 550, training loss: 315.19805908203125 = 0.960244357585907 + 50.0 * 6.284756660461426
Epoch 550, val loss: 1.1930456161499023
Epoch 560, training loss: 314.8504333496094 = 0.9386777281761169 + 50.0 * 6.278234958648682
Epoch 560, val loss: 1.1799689531326294
Epoch 570, training loss: 314.7084655761719 = 0.917675793170929 + 50.0 * 6.275815963745117
Epoch 570, val loss: 1.1674679517745972
Epoch 580, training loss: 314.5975036621094 = 0.8970626592636108 + 50.0 * 6.274008750915527
Epoch 580, val loss: 1.1553723812103271
Epoch 590, training loss: 314.879150390625 = 0.8766469955444336 + 50.0 * 6.280049800872803
Epoch 590, val loss: 1.1435588598251343
Epoch 600, training loss: 314.4788513183594 = 0.8566235899925232 + 50.0 * 6.272444725036621
Epoch 600, val loss: 1.1318788528442383
Epoch 610, training loss: 314.3233642578125 = 0.836942732334137 + 50.0 * 6.269728660583496
Epoch 610, val loss: 1.120708703994751
Epoch 620, training loss: 314.25750732421875 = 0.8177273273468018 + 50.0 * 6.268795490264893
Epoch 620, val loss: 1.1099621057510376
Epoch 630, training loss: 314.3411560058594 = 0.7988843321800232 + 50.0 * 6.270845413208008
Epoch 630, val loss: 1.09954833984375
Epoch 640, training loss: 314.24591064453125 = 0.780228853225708 + 50.0 * 6.269313335418701
Epoch 640, val loss: 1.0894206762313843
Epoch 650, training loss: 313.9748840332031 = 0.7619366645812988 + 50.0 * 6.264259338378906
Epoch 650, val loss: 1.0795351266860962
Epoch 660, training loss: 313.9221496582031 = 0.7441112399101257 + 50.0 * 6.263560771942139
Epoch 660, val loss: 1.0701743364334106
Epoch 670, training loss: 314.12298583984375 = 0.7265997529029846 + 50.0 * 6.267927646636963
Epoch 670, val loss: 1.0612077713012695
Epoch 680, training loss: 313.8849792480469 = 0.7095362544059753 + 50.0 * 6.2635087966918945
Epoch 680, val loss: 1.0523908138275146
Epoch 690, training loss: 313.7572021484375 = 0.6926812529563904 + 50.0 * 6.261290073394775
Epoch 690, val loss: 1.0439872741699219
Epoch 700, training loss: 313.62371826171875 = 0.6763739585876465 + 50.0 * 6.258947372436523
Epoch 700, val loss: 1.0359596014022827
Epoch 710, training loss: 313.5462951660156 = 0.6603690385818481 + 50.0 * 6.257718563079834
Epoch 710, val loss: 1.0282927751541138
Epoch 720, training loss: 313.677001953125 = 0.6446520686149597 + 50.0 * 6.260646820068359
Epoch 720, val loss: 1.0208547115325928
Epoch 730, training loss: 313.4861755371094 = 0.6292629837989807 + 50.0 * 6.257137775421143
Epoch 730, val loss: 1.013489842414856
Epoch 740, training loss: 313.348388671875 = 0.6142938137054443 + 50.0 * 6.2546820640563965
Epoch 740, val loss: 1.0066739320755005
Epoch 750, training loss: 313.30364990234375 = 0.5997366905212402 + 50.0 * 6.254078388214111
Epoch 750, val loss: 1.0002110004425049
Epoch 760, training loss: 313.5589904785156 = 0.5855377316474915 + 50.0 * 6.259469032287598
Epoch 760, val loss: 0.9939715266227722
Epoch 770, training loss: 313.2510070800781 = 0.5713597536087036 + 50.0 * 6.2535929679870605
Epoch 770, val loss: 0.9878628253936768
Epoch 780, training loss: 313.2095642089844 = 0.5576753616333008 + 50.0 * 6.253037929534912
Epoch 780, val loss: 0.9820668697357178
Epoch 790, training loss: 313.0690612792969 = 0.5443982481956482 + 50.0 * 6.250493049621582
Epoch 790, val loss: 0.9767481088638306
Epoch 800, training loss: 312.9914245605469 = 0.5315138697624207 + 50.0 * 6.249197959899902
Epoch 800, val loss: 0.9716921448707581
Epoch 810, training loss: 313.043701171875 = 0.5189318656921387 + 50.0 * 6.250495910644531
Epoch 810, val loss: 0.9668596982955933
Epoch 820, training loss: 312.9278259277344 = 0.5064719319343567 + 50.0 * 6.248427391052246
Epoch 820, val loss: 0.9619789123535156
Epoch 830, training loss: 312.89910888671875 = 0.49432677030563354 + 50.0 * 6.248095989227295
Epoch 830, val loss: 0.9573994874954224
Epoch 840, training loss: 312.7962341308594 = 0.48251426219940186 + 50.0 * 6.246274471282959
Epoch 840, val loss: 0.9530646204948425
Epoch 850, training loss: 313.1491394042969 = 0.47105520963668823 + 50.0 * 6.253561496734619
Epoch 850, val loss: 0.9487888813018799
Epoch 860, training loss: 312.87091064453125 = 0.45947280526161194 + 50.0 * 6.248228549957275
Epoch 860, val loss: 0.944599986076355
Epoch 870, training loss: 312.70025634765625 = 0.4483853280544281 + 50.0 * 6.245037078857422
Epoch 870, val loss: 0.9405613541603088
Epoch 880, training loss: 312.5966796875 = 0.43760958313941956 + 50.0 * 6.243181228637695
Epoch 880, val loss: 0.9369046092033386
Epoch 890, training loss: 312.558349609375 = 0.4271237552165985 + 50.0 * 6.242624282836914
Epoch 890, val loss: 0.9335110783576965
Epoch 900, training loss: 312.66583251953125 = 0.4168711304664612 + 50.0 * 6.244979381561279
Epoch 900, val loss: 0.9302986860275269
Epoch 910, training loss: 312.67486572265625 = 0.4066810607910156 + 50.0 * 6.245363712310791
Epoch 910, val loss: 0.9267838001251221
Epoch 920, training loss: 312.4264831542969 = 0.39676329493522644 + 50.0 * 6.240594387054443
Epoch 920, val loss: 0.9236392378807068
Epoch 930, training loss: 312.3966979980469 = 0.3871553838253021 + 50.0 * 6.2401909828186035
Epoch 930, val loss: 0.9208440184593201
Epoch 940, training loss: 312.72088623046875 = 0.3777768313884735 + 50.0 * 6.246861934661865
Epoch 940, val loss: 0.9182351231575012
Epoch 950, training loss: 312.49005126953125 = 0.36856964230537415 + 50.0 * 6.242429733276367
Epoch 950, val loss: 0.9152871370315552
Epoch 960, training loss: 312.31103515625 = 0.35952267050743103 + 50.0 * 6.239029884338379
Epoch 960, val loss: 0.912864625453949
Epoch 970, training loss: 312.22698974609375 = 0.35084572434425354 + 50.0 * 6.237522602081299
Epoch 970, val loss: 0.9105788469314575
Epoch 980, training loss: 312.18719482421875 = 0.34237930178642273 + 50.0 * 6.236896514892578
Epoch 980, val loss: 0.9086337685585022
Epoch 990, training loss: 312.18682861328125 = 0.33415624499320984 + 50.0 * 6.237052917480469
Epoch 990, val loss: 0.9067010283470154
Epoch 1000, training loss: 312.2794189453125 = 0.32604366540908813 + 50.0 * 6.239067077636719
Epoch 1000, val loss: 0.9048113822937012
Epoch 1010, training loss: 312.1423034667969 = 0.31806111335754395 + 50.0 * 6.236485004425049
Epoch 1010, val loss: 0.9029452800750732
Epoch 1020, training loss: 312.111083984375 = 0.3103559613227844 + 50.0 * 6.236014366149902
Epoch 1020, val loss: 0.9014125466346741
Epoch 1030, training loss: 312.0774230957031 = 0.30291110277175903 + 50.0 * 6.235489845275879
Epoch 1030, val loss: 0.9001149535179138
Epoch 1040, training loss: 312.0270690917969 = 0.2956463396549225 + 50.0 * 6.234628200531006
Epoch 1040, val loss: 0.8989344239234924
Epoch 1050, training loss: 312.28131103515625 = 0.2885262370109558 + 50.0 * 6.239856243133545
Epoch 1050, val loss: 0.897925078868866
Epoch 1060, training loss: 312.017822265625 = 0.281623512506485 + 50.0 * 6.234724044799805
Epoch 1060, val loss: 0.8967345952987671
Epoch 1070, training loss: 311.9234619140625 = 0.27489957213401794 + 50.0 * 6.23297119140625
Epoch 1070, val loss: 0.8960973024368286
Epoch 1080, training loss: 311.8772888183594 = 0.26841211318969727 + 50.0 * 6.232177734375
Epoch 1080, val loss: 0.8955729007720947
Epoch 1090, training loss: 312.0094909667969 = 0.26211029291152954 + 50.0 * 6.234947681427002
Epoch 1090, val loss: 0.8951573967933655
Epoch 1100, training loss: 312.0267028808594 = 0.25584954023361206 + 50.0 * 6.235416889190674
Epoch 1100, val loss: 0.8947364687919617
Epoch 1110, training loss: 311.85296630859375 = 0.24975024163722992 + 50.0 * 6.232064247131348
Epoch 1110, val loss: 0.8944805860519409
Epoch 1120, training loss: 311.7862243652344 = 0.24388788640499115 + 50.0 * 6.230846405029297
Epoch 1120, val loss: 0.8944743871688843
Epoch 1130, training loss: 311.847412109375 = 0.2382100373506546 + 50.0 * 6.232183933258057
Epoch 1130, val loss: 0.8947854042053223
Epoch 1140, training loss: 311.7380676269531 = 0.23266327381134033 + 50.0 * 6.23010778427124
Epoch 1140, val loss: 0.8951327204704285
Epoch 1150, training loss: 311.7970275878906 = 0.2273094356060028 + 50.0 * 6.2313947677612305
Epoch 1150, val loss: 0.8955087065696716
Epoch 1160, training loss: 311.7283935546875 = 0.2220144271850586 + 50.0 * 6.230127334594727
Epoch 1160, val loss: 0.8961138129234314
Epoch 1170, training loss: 311.6593322753906 = 0.21689282357692719 + 50.0 * 6.228848934173584
Epoch 1170, val loss: 0.8970234394073486
Epoch 1180, training loss: 311.62677001953125 = 0.2119559496641159 + 50.0 * 6.228296279907227
Epoch 1180, val loss: 0.8979132771492004
Epoch 1190, training loss: 311.669677734375 = 0.20713210105895996 + 50.0 * 6.229251384735107
Epoch 1190, val loss: 0.899238109588623
Epoch 1200, training loss: 311.71630859375 = 0.20236550271511078 + 50.0 * 6.230278968811035
Epoch 1200, val loss: 0.9003206491470337
Epoch 1210, training loss: 311.55670166015625 = 0.19775843620300293 + 50.0 * 6.227179050445557
Epoch 1210, val loss: 0.9015443325042725
Epoch 1220, training loss: 311.516845703125 = 0.19327741861343384 + 50.0 * 6.226471424102783
Epoch 1220, val loss: 0.9030932784080505
Epoch 1230, training loss: 311.4822692871094 = 0.1889532506465912 + 50.0 * 6.225865840911865
Epoch 1230, val loss: 0.9048703908920288
Epoch 1240, training loss: 311.6571350097656 = 0.18474508821964264 + 50.0 * 6.229447364807129
Epoch 1240, val loss: 0.90666264295578
Epoch 1250, training loss: 311.6407165527344 = 0.18058501183986664 + 50.0 * 6.229202747344971
Epoch 1250, val loss: 0.9085074663162231
Epoch 1260, training loss: 311.6592102050781 = 0.17651845514774323 + 50.0 * 6.229653835296631
Epoch 1260, val loss: 0.9102590680122375
Epoch 1270, training loss: 311.39697265625 = 0.17253313958644867 + 50.0 * 6.224488258361816
Epoch 1270, val loss: 0.9123918414115906
Epoch 1280, training loss: 311.3663635253906 = 0.16872480511665344 + 50.0 * 6.223952770233154
Epoch 1280, val loss: 0.9147973656654358
Epoch 1290, training loss: 311.3328552246094 = 0.16505222022533417 + 50.0 * 6.223356246948242
Epoch 1290, val loss: 0.9173687696456909
Epoch 1300, training loss: 311.3723449707031 = 0.16147634387016296 + 50.0 * 6.224217414855957
Epoch 1300, val loss: 0.9199616312980652
Epoch 1310, training loss: 311.55108642578125 = 0.15793421864509583 + 50.0 * 6.227863311767578
Epoch 1310, val loss: 0.9225019812583923
Epoch 1320, training loss: 311.37060546875 = 0.15441393852233887 + 50.0 * 6.224323749542236
Epoch 1320, val loss: 0.9251400232315063
Epoch 1330, training loss: 311.2813720703125 = 0.1510789394378662 + 50.0 * 6.2226057052612305
Epoch 1330, val loss: 0.9280080795288086
Epoch 1340, training loss: 311.3411560058594 = 0.14783330261707306 + 50.0 * 6.2238664627075195
Epoch 1340, val loss: 0.9311056137084961
Epoch 1350, training loss: 311.2535400390625 = 0.1446252018213272 + 50.0 * 6.2221784591674805
Epoch 1350, val loss: 0.9340693354606628
Epoch 1360, training loss: 311.236083984375 = 0.14150071144104004 + 50.0 * 6.221891403198242
Epoch 1360, val loss: 0.9370023012161255
Epoch 1370, training loss: 311.2560729980469 = 0.13846008479595184 + 50.0 * 6.222352504730225
Epoch 1370, val loss: 0.9403266906738281
Epoch 1380, training loss: 311.1458435058594 = 0.13551385700702667 + 50.0 * 6.220206260681152
Epoch 1380, val loss: 0.9436421394348145
Epoch 1390, training loss: 311.15966796875 = 0.1326596587896347 + 50.0 * 6.2205400466918945
Epoch 1390, val loss: 0.9472092390060425
Epoch 1400, training loss: 311.2603759765625 = 0.12986841797828674 + 50.0 * 6.222609996795654
Epoch 1400, val loss: 0.9507712125778198
Epoch 1410, training loss: 311.27227783203125 = 0.12711019814014435 + 50.0 * 6.222903728485107
Epoch 1410, val loss: 0.9541614651679993
Epoch 1420, training loss: 311.2904052734375 = 0.12441211193799973 + 50.0 * 6.2233195304870605
Epoch 1420, val loss: 0.9575480222702026
Epoch 1430, training loss: 311.12872314453125 = 0.1217598095536232 + 50.0 * 6.220139026641846
Epoch 1430, val loss: 0.9614133238792419
Epoch 1440, training loss: 311.0882263183594 = 0.11921477317810059 + 50.0 * 6.2193803787231445
Epoch 1440, val loss: 0.9653075933456421
Epoch 1450, training loss: 311.0254821777344 = 0.1167592704296112 + 50.0 * 6.218174457550049
Epoch 1450, val loss: 0.9690940380096436
Epoch 1460, training loss: 311.10107421875 = 0.11437543481588364 + 50.0 * 6.219733715057373
Epoch 1460, val loss: 0.9731997847557068
Epoch 1470, training loss: 311.143310546875 = 0.1119966059923172 + 50.0 * 6.220626354217529
Epoch 1470, val loss: 0.9769701957702637
Epoch 1480, training loss: 311.0185852050781 = 0.10964705795049667 + 50.0 * 6.218178749084473
Epoch 1480, val loss: 0.981053352355957
Epoch 1490, training loss: 310.9951171875 = 0.1073988601565361 + 50.0 * 6.217754364013672
Epoch 1490, val loss: 0.9850518107414246
Epoch 1500, training loss: 311.0965881347656 = 0.10521537065505981 + 50.0 * 6.219827175140381
Epoch 1500, val loss: 0.9892321825027466
Epoch 1510, training loss: 310.9222717285156 = 0.10306452214717865 + 50.0 * 6.216384410858154
Epoch 1510, val loss: 0.9932652115821838
Epoch 1520, training loss: 310.9773864746094 = 0.1009933352470398 + 50.0 * 6.217527866363525
Epoch 1520, val loss: 0.9974763989448547
Epoch 1530, training loss: 311.02447509765625 = 0.09895318001508713 + 50.0 * 6.218510627746582
Epoch 1530, val loss: 1.0017340183258057
Epoch 1540, training loss: 311.0599060058594 = 0.09694867581129074 + 50.0 * 6.219259262084961
Epoch 1540, val loss: 1.006332278251648
Epoch 1550, training loss: 310.9250793457031 = 0.0949871689081192 + 50.0 * 6.216601848602295
Epoch 1550, val loss: 1.0104663372039795
Epoch 1560, training loss: 310.9285888671875 = 0.0930919498205185 + 50.0 * 6.216710090637207
Epoch 1560, val loss: 1.0149415731430054
Epoch 1570, training loss: 311.10760498046875 = 0.0912289097905159 + 50.0 * 6.220327854156494
Epoch 1570, val loss: 1.0192415714263916
Epoch 1580, training loss: 310.91632080078125 = 0.08937814086675644 + 50.0 * 6.216538429260254
Epoch 1580, val loss: 1.0238037109375
Epoch 1590, training loss: 310.886474609375 = 0.08761387318372726 + 50.0 * 6.215977191925049
Epoch 1590, val loss: 1.0284152030944824
Epoch 1600, training loss: 310.8357238769531 = 0.08589277416467667 + 50.0 * 6.214996337890625
Epoch 1600, val loss: 1.032971739768982
Epoch 1610, training loss: 310.8246765136719 = 0.08421680331230164 + 50.0 * 6.214808940887451
Epoch 1610, val loss: 1.0376631021499634
Epoch 1620, training loss: 310.8539123535156 = 0.08257923275232315 + 50.0 * 6.215426445007324
Epoch 1620, val loss: 1.0423319339752197
Epoch 1630, training loss: 310.83587646484375 = 0.08096300065517426 + 50.0 * 6.2150983810424805
Epoch 1630, val loss: 1.0469727516174316
Epoch 1640, training loss: 311.04595947265625 = 0.0793863907456398 + 50.0 * 6.219331741333008
Epoch 1640, val loss: 1.051607370376587
Epoch 1650, training loss: 310.8529968261719 = 0.07780046761035919 + 50.0 * 6.215504169464111
Epoch 1650, val loss: 1.056032657623291
Epoch 1660, training loss: 310.7821350097656 = 0.07628780603408813 + 50.0 * 6.21411657333374
Epoch 1660, val loss: 1.0608563423156738
Epoch 1670, training loss: 310.77239990234375 = 0.07482382655143738 + 50.0 * 6.213951587677002
Epoch 1670, val loss: 1.0655858516693115
Epoch 1680, training loss: 310.865966796875 = 0.07340404391288757 + 50.0 * 6.215851306915283
Epoch 1680, val loss: 1.0702358484268188
Epoch 1690, training loss: 310.75250244140625 = 0.07198362052440643 + 50.0 * 6.2136101722717285
Epoch 1690, val loss: 1.0750852823257446
Epoch 1700, training loss: 310.74456787109375 = 0.07061328738927841 + 50.0 * 6.213479042053223
Epoch 1700, val loss: 1.0800024271011353
Epoch 1710, training loss: 310.78546142578125 = 0.0692620649933815 + 50.0 * 6.2143235206604
Epoch 1710, val loss: 1.0844351053237915
Epoch 1720, training loss: 310.8135070800781 = 0.06793645024299622 + 50.0 * 6.214911460876465
Epoch 1720, val loss: 1.0894269943237305
Epoch 1730, training loss: 310.6542053222656 = 0.0666518360376358 + 50.0 * 6.2117509841918945
Epoch 1730, val loss: 1.0943236351013184
Epoch 1740, training loss: 310.62774658203125 = 0.06541115790605545 + 50.0 * 6.211246967315674
Epoch 1740, val loss: 1.099065899848938
Epoch 1750, training loss: 310.6170654296875 = 0.06420689821243286 + 50.0 * 6.211057186126709
Epoch 1750, val loss: 1.104014277458191
Epoch 1760, training loss: 310.6534423828125 = 0.0630359798669815 + 50.0 * 6.211807727813721
Epoch 1760, val loss: 1.1089626550674438
Epoch 1770, training loss: 311.06207275390625 = 0.061881907284259796 + 50.0 * 6.220003604888916
Epoch 1770, val loss: 1.1136157512664795
Epoch 1780, training loss: 310.7150573730469 = 0.06068234145641327 + 50.0 * 6.213088035583496
Epoch 1780, val loss: 1.1182870864868164
Epoch 1790, training loss: 310.5810546875 = 0.05955247953534126 + 50.0 * 6.210430145263672
Epoch 1790, val loss: 1.1230623722076416
Epoch 1800, training loss: 310.5475158691406 = 0.05847742781043053 + 50.0 * 6.209780693054199
Epoch 1800, val loss: 1.1281483173370361
Epoch 1810, training loss: 310.53033447265625 = 0.05743725597858429 + 50.0 * 6.209457874298096
Epoch 1810, val loss: 1.133294701576233
Epoch 1820, training loss: 310.7364196777344 = 0.056424498558044434 + 50.0 * 6.213600158691406
Epoch 1820, val loss: 1.1386287212371826
Epoch 1830, training loss: 310.53656005859375 = 0.05539167299866676 + 50.0 * 6.209623336791992
Epoch 1830, val loss: 1.1425970792770386
Epoch 1840, training loss: 310.5975341796875 = 0.05439946427941322 + 50.0 * 6.210862159729004
Epoch 1840, val loss: 1.147708535194397
Epoch 1850, training loss: 310.632080078125 = 0.05341218784451485 + 50.0 * 6.211573600769043
Epoch 1850, val loss: 1.1521453857421875
Epoch 1860, training loss: 310.5094299316406 = 0.05244731530547142 + 50.0 * 6.209139347076416
Epoch 1860, val loss: 1.1571614742279053
Epoch 1870, training loss: 310.4576416015625 = 0.05153290182352066 + 50.0 * 6.2081217765808105
Epoch 1870, val loss: 1.162084698677063
Epoch 1880, training loss: 310.47369384765625 = 0.05064840242266655 + 50.0 * 6.208460807800293
Epoch 1880, val loss: 1.1670596599578857
Epoch 1890, training loss: 310.85931396484375 = 0.049780361354351044 + 50.0 * 6.216190814971924
Epoch 1890, val loss: 1.1719982624053955
Epoch 1900, training loss: 310.5265197753906 = 0.0488865002989769 + 50.0 * 6.209552764892578
Epoch 1900, val loss: 1.1763468980789185
Epoch 1910, training loss: 310.4231872558594 = 0.04804341867566109 + 50.0 * 6.207503318786621
Epoch 1910, val loss: 1.181243896484375
Epoch 1920, training loss: 310.4219665527344 = 0.047232385724782944 + 50.0 * 6.207494735717773
Epoch 1920, val loss: 1.186112642288208
Epoch 1930, training loss: 310.6039123535156 = 0.04644535481929779 + 50.0 * 6.211149215698242
Epoch 1930, val loss: 1.1909226179122925
Epoch 1940, training loss: 310.4090881347656 = 0.04563414305448532 + 50.0 * 6.207269191741943
Epoch 1940, val loss: 1.1955339908599854
Epoch 1950, training loss: 310.5190124511719 = 0.0448562428355217 + 50.0 * 6.2094831466674805
Epoch 1950, val loss: 1.200325846672058
Epoch 1960, training loss: 310.46807861328125 = 0.04410004988312721 + 50.0 * 6.208479404449463
Epoch 1960, val loss: 1.2049418687820435
Epoch 1970, training loss: 310.4171142578125 = 0.04335731640458107 + 50.0 * 6.207475185394287
Epoch 1970, val loss: 1.209834098815918
Epoch 1980, training loss: 310.3742980957031 = 0.042634863406419754 + 50.0 * 6.206633567810059
Epoch 1980, val loss: 1.214308500289917
Epoch 1990, training loss: 310.3536376953125 = 0.04194314405322075 + 50.0 * 6.206233978271484
Epoch 1990, val loss: 1.219265103340149
Epoch 2000, training loss: 310.42584228515625 = 0.041268136352300644 + 50.0 * 6.207691669464111
Epoch 2000, val loss: 1.2240437269210815
Epoch 2010, training loss: 310.4345703125 = 0.04059496894478798 + 50.0 * 6.207879543304443
Epoch 2010, val loss: 1.2285592555999756
Epoch 2020, training loss: 310.5303955078125 = 0.039925698190927505 + 50.0 * 6.209809303283691
Epoch 2020, val loss: 1.233155369758606
Epoch 2030, training loss: 310.4161376953125 = 0.03925849497318268 + 50.0 * 6.20753812789917
Epoch 2030, val loss: 1.2374274730682373
Epoch 2040, training loss: 310.3471374511719 = 0.038623277097940445 + 50.0 * 6.206170082092285
Epoch 2040, val loss: 1.2421345710754395
Epoch 2050, training loss: 310.38494873046875 = 0.03801443427801132 + 50.0 * 6.206938743591309
Epoch 2050, val loss: 1.2467856407165527
Epoch 2060, training loss: 310.3822326660156 = 0.03740774467587471 + 50.0 * 6.2068963050842285
Epoch 2060, val loss: 1.251217246055603
Epoch 2070, training loss: 310.2797546386719 = 0.03681052103638649 + 50.0 * 6.204859256744385
Epoch 2070, val loss: 1.2556899785995483
Epoch 2080, training loss: 310.4278564453125 = 0.03624061495065689 + 50.0 * 6.207832336425781
Epoch 2080, val loss: 1.2605195045471191
Epoch 2090, training loss: 310.3432312011719 = 0.03566671907901764 + 50.0 * 6.206151008605957
Epoch 2090, val loss: 1.2648892402648926
Epoch 2100, training loss: 310.2666320800781 = 0.03510250151157379 + 50.0 * 6.2046308517456055
Epoch 2100, val loss: 1.269210696220398
Epoch 2110, training loss: 310.2634582519531 = 0.034560538828372955 + 50.0 * 6.204578399658203
Epoch 2110, val loss: 1.273630976676941
Epoch 2120, training loss: 310.34490966796875 = 0.03404214605689049 + 50.0 * 6.2062177658081055
Epoch 2120, val loss: 1.2786754369735718
Epoch 2130, training loss: 310.3206481933594 = 0.03351014107465744 + 50.0 * 6.205742835998535
Epoch 2130, val loss: 1.2824662923812866
Epoch 2140, training loss: 310.2491149902344 = 0.032992079854011536 + 50.0 * 6.204322338104248
Epoch 2140, val loss: 1.2865345478057861
Epoch 2150, training loss: 310.1943054199219 = 0.0324951596558094 + 50.0 * 6.203236103057861
Epoch 2150, val loss: 1.2912423610687256
Epoch 2160, training loss: 310.2345275878906 = 0.032014861702919006 + 50.0 * 6.204050064086914
Epoch 2160, val loss: 1.2955312728881836
Epoch 2170, training loss: 310.5058288574219 = 0.03153730928897858 + 50.0 * 6.20948600769043
Epoch 2170, val loss: 1.3000991344451904
Epoch 2180, training loss: 310.30413818359375 = 0.031054925173521042 + 50.0 * 6.205461502075195
Epoch 2180, val loss: 1.303685188293457
Epoch 2190, training loss: 310.1558837890625 = 0.030586807057261467 + 50.0 * 6.202506065368652
Epoch 2190, val loss: 1.3080830574035645
Epoch 2200, training loss: 310.1564636230469 = 0.03014654479920864 + 50.0 * 6.202526092529297
Epoch 2200, val loss: 1.3124911785125732
Epoch 2210, training loss: 310.2943420410156 = 0.029720334336161613 + 50.0 * 6.205292224884033
Epoch 2210, val loss: 1.3165403604507446
Epoch 2220, training loss: 310.3949279785156 = 0.029280465096235275 + 50.0 * 6.207313060760498
Epoch 2220, val loss: 1.3208787441253662
Epoch 2230, training loss: 310.17138671875 = 0.028832266107201576 + 50.0 * 6.202850818634033
Epoch 2230, val loss: 1.3246276378631592
Epoch 2240, training loss: 310.1501770019531 = 0.028408393263816833 + 50.0 * 6.202435493469238
Epoch 2240, val loss: 1.3289096355438232
Epoch 2250, training loss: 310.1130676269531 = 0.028010185807943344 + 50.0 * 6.2017011642456055
Epoch 2250, val loss: 1.333088755607605
Epoch 2260, training loss: 310.2204284667969 = 0.027622586116194725 + 50.0 * 6.203855991363525
Epoch 2260, val loss: 1.337544560432434
Epoch 2270, training loss: 310.1877746582031 = 0.027221767231822014 + 50.0 * 6.203211307525635
Epoch 2270, val loss: 1.3412792682647705
Epoch 2280, training loss: 310.08343505859375 = 0.026829682290554047 + 50.0 * 6.201132297515869
Epoch 2280, val loss: 1.3451051712036133
Epoch 2290, training loss: 310.0531921386719 = 0.026455899700522423 + 50.0 * 6.200534820556641
Epoch 2290, val loss: 1.3492811918258667
Epoch 2300, training loss: 310.0467529296875 = 0.02609849162399769 + 50.0 * 6.200413227081299
Epoch 2300, val loss: 1.3534343242645264
Epoch 2310, training loss: 310.19073486328125 = 0.025753572583198547 + 50.0 * 6.203299522399902
Epoch 2310, val loss: 1.357692003250122
Epoch 2320, training loss: 310.114990234375 = 0.025391628965735435 + 50.0 * 6.201792240142822
Epoch 2320, val loss: 1.361362099647522
Epoch 2330, training loss: 310.1114807128906 = 0.025031957775354385 + 50.0 * 6.2017292976379395
Epoch 2330, val loss: 1.364941954612732
Epoch 2340, training loss: 310.1180419921875 = 0.02468736469745636 + 50.0 * 6.20186710357666
Epoch 2340, val loss: 1.368666648864746
Epoch 2350, training loss: 310.1356506347656 = 0.024351652711629868 + 50.0 * 6.202226161956787
Epoch 2350, val loss: 1.3729114532470703
Epoch 2360, training loss: 309.9950256347656 = 0.024024169892072678 + 50.0 * 6.19942045211792
Epoch 2360, val loss: 1.3766546249389648
Epoch 2370, training loss: 310.0495300292969 = 0.023713672533631325 + 50.0 * 6.200516223907471
Epoch 2370, val loss: 1.3804954290390015
Epoch 2380, training loss: 310.3188171386719 = 0.023403504863381386 + 50.0 * 6.205908298492432
Epoch 2380, val loss: 1.384165644645691
Epoch 2390, training loss: 310.1089172363281 = 0.023081457242369652 + 50.0 * 6.201716899871826
Epoch 2390, val loss: 1.387797474861145
Epoch 2400, training loss: 309.98541259765625 = 0.02277064323425293 + 50.0 * 6.199253082275391
Epoch 2400, val loss: 1.3913480043411255
Epoch 2410, training loss: 309.9569091796875 = 0.022478120401501656 + 50.0 * 6.198688507080078
Epoch 2410, val loss: 1.3954105377197266
Epoch 2420, training loss: 309.94842529296875 = 0.022197986021637917 + 50.0 * 6.1985249519348145
Epoch 2420, val loss: 1.3992037773132324
Epoch 2430, training loss: 310.17724609375 = 0.02192910574376583 + 50.0 * 6.203105926513672
Epoch 2430, val loss: 1.4029338359832764
Epoch 2440, training loss: 310.0151062011719 = 0.02162686362862587 + 50.0 * 6.199869155883789
Epoch 2440, val loss: 1.405785083770752
Epoch 2450, training loss: 309.9750061035156 = 0.021335015073418617 + 50.0 * 6.199073791503906
Epoch 2450, val loss: 1.4094685316085815
Epoch 2460, training loss: 309.9608459472656 = 0.021061021834611893 + 50.0 * 6.198795795440674
Epoch 2460, val loss: 1.4128923416137695
Epoch 2470, training loss: 309.9284973144531 = 0.02080613002181053 + 50.0 * 6.198153972625732
Epoch 2470, val loss: 1.4168596267700195
Epoch 2480, training loss: 309.9694519042969 = 0.020554259419441223 + 50.0 * 6.198977947235107
Epoch 2480, val loss: 1.4204374551773071
Epoch 2490, training loss: 310.2078857421875 = 0.02030378207564354 + 50.0 * 6.203751564025879
Epoch 2490, val loss: 1.4240843057632446
Epoch 2500, training loss: 310.1308898925781 = 0.02003953605890274 + 50.0 * 6.202216625213623
Epoch 2500, val loss: 1.4273681640625
Epoch 2510, training loss: 309.9493408203125 = 0.019784579053521156 + 50.0 * 6.198591232299805
Epoch 2510, val loss: 1.4302666187286377
Epoch 2520, training loss: 309.8981018066406 = 0.01954312063753605 + 50.0 * 6.197571277618408
Epoch 2520, val loss: 1.4337238073349
Epoch 2530, training loss: 309.87115478515625 = 0.019310494884848595 + 50.0 * 6.1970367431640625
Epoch 2530, val loss: 1.4373630285263062
Epoch 2540, training loss: 309.9629211425781 = 0.019087960943579674 + 50.0 * 6.198876857757568
Epoch 2540, val loss: 1.440885305404663
Epoch 2550, training loss: 309.9521789550781 = 0.018856491893529892 + 50.0 * 6.198666095733643
Epoch 2550, val loss: 1.443954348564148
Epoch 2560, training loss: 309.9529724121094 = 0.01862674579024315 + 50.0 * 6.1986870765686035
Epoch 2560, val loss: 1.4469959735870361
Epoch 2570, training loss: 310.08447265625 = 0.01840711385011673 + 50.0 * 6.201321601867676
Epoch 2570, val loss: 1.4502456188201904
Epoch 2580, training loss: 309.90997314453125 = 0.018178101629018784 + 50.0 * 6.197835445404053
Epoch 2580, val loss: 1.4537546634674072
Epoch 2590, training loss: 309.8995056152344 = 0.01796599105000496 + 50.0 * 6.197630405426025
Epoch 2590, val loss: 1.4568926095962524
Epoch 2600, training loss: 310.28973388671875 = 0.017759140580892563 + 50.0 * 6.205439567565918
Epoch 2600, val loss: 1.4602582454681396
Epoch 2610, training loss: 309.9324951171875 = 0.01753915660083294 + 50.0 * 6.198298931121826
Epoch 2610, val loss: 1.462618112564087
Epoch 2620, training loss: 309.8338623046875 = 0.017333507537841797 + 50.0 * 6.196330547332764
Epoch 2620, val loss: 1.4661812782287598
Epoch 2630, training loss: 309.8034362792969 = 0.017138130962848663 + 50.0 * 6.195725440979004
Epoch 2630, val loss: 1.4692277908325195
Epoch 2640, training loss: 309.7928771972656 = 0.016950126737356186 + 50.0 * 6.195518493652344
Epoch 2640, val loss: 1.4725388288497925
Epoch 2650, training loss: 309.8817443847656 = 0.01676626317203045 + 50.0 * 6.197299957275391
Epoch 2650, val loss: 1.4758018255233765
Epoch 2660, training loss: 309.9180603027344 = 0.01657305285334587 + 50.0 * 6.1980299949646
Epoch 2660, val loss: 1.47849440574646
Epoch 2670, training loss: 309.86590576171875 = 0.01638178899884224 + 50.0 * 6.196990489959717
Epoch 2670, val loss: 1.4815901517868042
Epoch 2680, training loss: 309.81097412109375 = 0.016195649281144142 + 50.0 * 6.195895195007324
Epoch 2680, val loss: 1.4847856760025024
Epoch 2690, training loss: 309.9902038574219 = 0.016023926436901093 + 50.0 * 6.199483871459961
Epoch 2690, val loss: 1.4876494407653809
Epoch 2700, training loss: 309.84930419921875 = 0.015837905928492546 + 50.0 * 6.196669578552246
Epoch 2700, val loss: 1.4902997016906738
Epoch 2710, training loss: 309.7867736816406 = 0.015660027042031288 + 50.0 * 6.195422172546387
Epoch 2710, val loss: 1.4935039281845093
Epoch 2720, training loss: 309.7920227050781 = 0.015493663027882576 + 50.0 * 6.195530414581299
Epoch 2720, val loss: 1.4964278936386108
Epoch 2730, training loss: 310.0760498046875 = 0.015334171243011951 + 50.0 * 6.201214790344238
Epoch 2730, val loss: 1.4992119073867798
Epoch 2740, training loss: 309.80853271484375 = 0.015154627151787281 + 50.0 * 6.195867538452148
Epoch 2740, val loss: 1.5020421743392944
Epoch 2750, training loss: 309.7328186035156 = 0.01499386690557003 + 50.0 * 6.194356918334961
Epoch 2750, val loss: 1.5049904584884644
Epoch 2760, training loss: 309.7408142089844 = 0.014839570969343185 + 50.0 * 6.194519519805908
Epoch 2760, val loss: 1.5081508159637451
Epoch 2770, training loss: 309.84405517578125 = 0.01469079963862896 + 50.0 * 6.196587562561035
Epoch 2770, val loss: 1.5111550092697144
Epoch 2780, training loss: 309.8790283203125 = 0.014529776759445667 + 50.0 * 6.197289943695068
Epoch 2780, val loss: 1.5133682489395142
Epoch 2790, training loss: 309.7437744140625 = 0.01436617225408554 + 50.0 * 6.1945881843566895
Epoch 2790, val loss: 1.5161432027816772
Epoch 2800, training loss: 309.7120056152344 = 0.01421284954994917 + 50.0 * 6.193955898284912
Epoch 2800, val loss: 1.5187814235687256
Epoch 2810, training loss: 309.69500732421875 = 0.014070346020162106 + 50.0 * 6.1936187744140625
Epoch 2810, val loss: 1.5218510627746582
Epoch 2820, training loss: 309.88250732421875 = 0.013933003880083561 + 50.0 * 6.197371482849121
Epoch 2820, val loss: 1.5249454975128174
Epoch 2830, training loss: 309.7104187011719 = 0.013785642571747303 + 50.0 * 6.19393253326416
Epoch 2830, val loss: 1.526856780052185
Epoch 2840, training loss: 309.7561950683594 = 0.013644584454596043 + 50.0 * 6.194850921630859
Epoch 2840, val loss: 1.5296565294265747
Epoch 2850, training loss: 309.9305725097656 = 0.013507762923836708 + 50.0 * 6.198341369628906
Epoch 2850, val loss: 1.5322494506835938
Epoch 2860, training loss: 309.7362365722656 = 0.013364192098379135 + 50.0 * 6.194457530975342
Epoch 2860, val loss: 1.534881830215454
Epoch 2870, training loss: 309.69873046875 = 0.01323019526898861 + 50.0 * 6.193709850311279
Epoch 2870, val loss: 1.537500023841858
Epoch 2880, training loss: 309.7144470214844 = 0.013102960772812366 + 50.0 * 6.194026947021484
Epoch 2880, val loss: 1.5402076244354248
Epoch 2890, training loss: 309.81475830078125 = 0.012978903949260712 + 50.0 * 6.196035861968994
Epoch 2890, val loss: 1.542916178703308
Epoch 2900, training loss: 309.6794738769531 = 0.012847406789660454 + 50.0 * 6.193332672119141
Epoch 2900, val loss: 1.5450749397277832
Epoch 2910, training loss: 309.777099609375 = 0.012723756954073906 + 50.0 * 6.195287227630615
Epoch 2910, val loss: 1.5477261543273926
Epoch 2920, training loss: 309.7192077636719 = 0.012596902437508106 + 50.0 * 6.194131851196289
Epoch 2920, val loss: 1.5501132011413574
Epoch 2930, training loss: 309.6971435546875 = 0.012472494505345821 + 50.0 * 6.193693161010742
Epoch 2930, val loss: 1.5525988340377808
Epoch 2940, training loss: 309.7236633300781 = 0.012354341335594654 + 50.0 * 6.194226264953613
Epoch 2940, val loss: 1.555452585220337
Epoch 2950, training loss: 309.7989196777344 = 0.012232146225869656 + 50.0 * 6.195733547210693
Epoch 2950, val loss: 1.557494044303894
Epoch 2960, training loss: 309.6714172363281 = 0.012113947421312332 + 50.0 * 6.193186283111572
Epoch 2960, val loss: 1.5596964359283447
Epoch 2970, training loss: 309.62420654296875 = 0.011999745853245258 + 50.0 * 6.192244052886963
Epoch 2970, val loss: 1.562245488166809
Epoch 2980, training loss: 309.6316833496094 = 0.011891599744558334 + 50.0 * 6.1923956871032715
Epoch 2980, val loss: 1.5645254850387573
Epoch 2990, training loss: 309.7552490234375 = 0.011787520721554756 + 50.0 * 6.194869518280029
Epoch 2990, val loss: 1.5669108629226685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 431.7974853515625 = 1.9557822942733765 + 50.0 * 8.596834182739258
Epoch 0, val loss: 1.962162733078003
Epoch 10, training loss: 431.7417907714844 = 1.9458869695663452 + 50.0 * 8.595917701721191
Epoch 10, val loss: 1.9514566659927368
Epoch 20, training loss: 431.34832763671875 = 1.933350682258606 + 50.0 * 8.588299751281738
Epoch 20, val loss: 1.9377166032791138
Epoch 30, training loss: 428.7272033691406 = 1.9169001579284668 + 50.0 * 8.536206245422363
Epoch 30, val loss: 1.9196492433547974
Epoch 40, training loss: 416.16650390625 = 1.8987226486206055 + 50.0 * 8.285355567932129
Epoch 40, val loss: 1.9005581140518188
Epoch 50, training loss: 392.913330078125 = 1.8786903619766235 + 50.0 * 7.820693016052246
Epoch 50, val loss: 1.8794338703155518
Epoch 60, training loss: 375.8476867675781 = 1.862992763519287 + 50.0 * 7.479693412780762
Epoch 60, val loss: 1.8647022247314453
Epoch 70, training loss: 354.88946533203125 = 1.8489874601364136 + 50.0 * 7.060809135437012
Epoch 70, val loss: 1.8505312204360962
Epoch 80, training loss: 344.4947814941406 = 1.8360614776611328 + 50.0 * 6.853174209594727
Epoch 80, val loss: 1.8380290269851685
Epoch 90, training loss: 339.6290588378906 = 1.8232719898223877 + 50.0 * 6.756115436553955
Epoch 90, val loss: 1.8260092735290527
Epoch 100, training loss: 335.0576171875 = 1.8110066652297974 + 50.0 * 6.6649322509765625
Epoch 100, val loss: 1.8148224353790283
Epoch 110, training loss: 332.0980224609375 = 1.7999130487442017 + 50.0 * 6.605961799621582
Epoch 110, val loss: 1.8047256469726562
Epoch 120, training loss: 330.10455322265625 = 1.7894198894500732 + 50.0 * 6.566302299499512
Epoch 120, val loss: 1.795364260673523
Epoch 130, training loss: 328.5913391113281 = 1.7787922620773315 + 50.0 * 6.536251068115234
Epoch 130, val loss: 1.785772442817688
Epoch 140, training loss: 327.4515075683594 = 1.7674888372421265 + 50.0 * 6.513680458068848
Epoch 140, val loss: 1.7757598161697388
Epoch 150, training loss: 326.3868103027344 = 1.7553677558898926 + 50.0 * 6.492628574371338
Epoch 150, val loss: 1.7650903463363647
Epoch 160, training loss: 325.4640808105469 = 1.7422733306884766 + 50.0 * 6.474436283111572
Epoch 160, val loss: 1.7538896799087524
Epoch 170, training loss: 324.6452331542969 = 1.7281761169433594 + 50.0 * 6.458341121673584
Epoch 170, val loss: 1.7418460845947266
Epoch 180, training loss: 323.9641418457031 = 1.7128651142120361 + 50.0 * 6.445025444030762
Epoch 180, val loss: 1.7287914752960205
Epoch 190, training loss: 323.4252624511719 = 1.6960867643356323 + 50.0 * 6.43458366394043
Epoch 190, val loss: 1.7146803140640259
Epoch 200, training loss: 322.82244873046875 = 1.6779800653457642 + 50.0 * 6.422889709472656
Epoch 200, val loss: 1.69935941696167
Epoch 210, training loss: 322.2533264160156 = 1.6583811044692993 + 50.0 * 6.411898612976074
Epoch 210, val loss: 1.682924509048462
Epoch 220, training loss: 321.9197692871094 = 1.637346863746643 + 50.0 * 6.405648231506348
Epoch 220, val loss: 1.6652743816375732
Epoch 230, training loss: 321.4341735839844 = 1.6148144006729126 + 50.0 * 6.396387100219727
Epoch 230, val loss: 1.6463624238967896
Epoch 240, training loss: 320.9929504394531 = 1.5911139249801636 + 50.0 * 6.388036251068115
Epoch 240, val loss: 1.6263809204101562
Epoch 250, training loss: 320.60211181640625 = 1.5662304162979126 + 50.0 * 6.380717754364014
Epoch 250, val loss: 1.605583667755127
Epoch 260, training loss: 320.2648620605469 = 1.540401577949524 + 50.0 * 6.3744893074035645
Epoch 260, val loss: 1.584104061126709
Epoch 270, training loss: 320.23419189453125 = 1.5139130353927612 + 50.0 * 6.374405384063721
Epoch 270, val loss: 1.56191086769104
Epoch 280, training loss: 319.66778564453125 = 1.4868141412734985 + 50.0 * 6.363619327545166
Epoch 280, val loss: 1.5397452116012573
Epoch 290, training loss: 319.39251708984375 = 1.4595779180526733 + 50.0 * 6.358659267425537
Epoch 290, val loss: 1.5173344612121582
Epoch 300, training loss: 319.1833190917969 = 1.4323124885559082 + 50.0 * 6.355020046234131
Epoch 300, val loss: 1.4950259923934937
Epoch 310, training loss: 318.85986328125 = 1.4051069021224976 + 50.0 * 6.349094867706299
Epoch 310, val loss: 1.4730573892593384
Epoch 320, training loss: 318.6654357910156 = 1.3782247304916382 + 50.0 * 6.3457441329956055
Epoch 320, val loss: 1.4514867067337036
Epoch 330, training loss: 318.64202880859375 = 1.3518822193145752 + 50.0 * 6.345803260803223
Epoch 330, val loss: 1.430507779121399
Epoch 340, training loss: 318.1941833496094 = 1.3259693384170532 + 50.0 * 6.337364196777344
Epoch 340, val loss: 1.4101760387420654
Epoch 350, training loss: 318.12701416015625 = 1.3007676601409912 + 50.0 * 6.336524486541748
Epoch 350, val loss: 1.3905339241027832
Epoch 360, training loss: 318.0549011230469 = 1.2762584686279297 + 50.0 * 6.335573196411133
Epoch 360, val loss: 1.371358036994934
Epoch 370, training loss: 317.6656494140625 = 1.2523572444915771 + 50.0 * 6.328266143798828
Epoch 370, val loss: 1.3534189462661743
Epoch 380, training loss: 317.44970703125 = 1.2293298244476318 + 50.0 * 6.324407577514648
Epoch 380, val loss: 1.336085557937622
Epoch 390, training loss: 317.2732849121094 = 1.2069523334503174 + 50.0 * 6.321327209472656
Epoch 390, val loss: 1.3195664882659912
Epoch 400, training loss: 317.15557861328125 = 1.1852525472640991 + 50.0 * 6.319406509399414
Epoch 400, val loss: 1.3035180568695068
Epoch 410, training loss: 317.0338134765625 = 1.1641814708709717 + 50.0 * 6.317392826080322
Epoch 410, val loss: 1.288374900817871
Epoch 420, training loss: 316.7904357910156 = 1.1437517404556274 + 50.0 * 6.312933921813965
Epoch 420, val loss: 1.2737774848937988
Epoch 430, training loss: 316.633056640625 = 1.1238503456115723 + 50.0 * 6.310184478759766
Epoch 430, val loss: 1.2595676183700562
Epoch 440, training loss: 316.9084777832031 = 1.1044284105300903 + 50.0 * 6.3160810470581055
Epoch 440, val loss: 1.2456060647964478
Epoch 450, training loss: 316.4691467285156 = 1.085274338722229 + 50.0 * 6.307677745819092
Epoch 450, val loss: 1.2323448657989502
Epoch 460, training loss: 316.264892578125 = 1.0665156841278076 + 50.0 * 6.303967475891113
Epoch 460, val loss: 1.2192975282669067
Epoch 470, training loss: 316.09991455078125 = 1.0480458736419678 + 50.0 * 6.301037788391113
Epoch 470, val loss: 1.206586480140686
Epoch 480, training loss: 316.0920104980469 = 1.029783844947815 + 50.0 * 6.301244258880615
Epoch 480, val loss: 1.1939983367919922
Epoch 490, training loss: 316.15338134765625 = 1.0117673873901367 + 50.0 * 6.302832126617432
Epoch 490, val loss: 1.1812032461166382
Epoch 500, training loss: 315.80810546875 = 0.9937562346458435 + 50.0 * 6.2962870597839355
Epoch 500, val loss: 1.1688421964645386
Epoch 510, training loss: 315.624755859375 = 0.9758903384208679 + 50.0 * 6.292977333068848
Epoch 510, val loss: 1.1563969850540161
Epoch 520, training loss: 315.7220764160156 = 0.9581641554832458 + 50.0 * 6.295278549194336
Epoch 520, val loss: 1.1441450119018555
Epoch 530, training loss: 315.51324462890625 = 0.9404895305633545 + 50.0 * 6.291454792022705
Epoch 530, val loss: 1.1313930749893188
Epoch 540, training loss: 315.2935791015625 = 0.9229200482368469 + 50.0 * 6.287413120269775
Epoch 540, val loss: 1.1191431283950806
Epoch 550, training loss: 315.20703125 = 0.9054647088050842 + 50.0 * 6.286031723022461
Epoch 550, val loss: 1.1068133115768433
Epoch 560, training loss: 315.2996826171875 = 0.8881346583366394 + 50.0 * 6.288231372833252
Epoch 560, val loss: 1.0946732759475708
Epoch 570, training loss: 315.01593017578125 = 0.8709411025047302 + 50.0 * 6.282899856567383
Epoch 570, val loss: 1.0823975801467896
Epoch 580, training loss: 314.9295654296875 = 0.8539757132530212 + 50.0 * 6.2815117835998535
Epoch 580, val loss: 1.0702283382415771
Epoch 590, training loss: 314.84521484375 = 0.8371687531471252 + 50.0 * 6.280160903930664
Epoch 590, val loss: 1.058342456817627
Epoch 600, training loss: 314.9930419921875 = 0.8206169009208679 + 50.0 * 6.283448219299316
Epoch 600, val loss: 1.04678213596344
Epoch 610, training loss: 314.742919921875 = 0.8041541576385498 + 50.0 * 6.278774738311768
Epoch 610, val loss: 1.0355056524276733
Epoch 620, training loss: 314.5892333984375 = 0.7880393862724304 + 50.0 * 6.276023864746094
Epoch 620, val loss: 1.024168848991394
Epoch 630, training loss: 314.4897766113281 = 0.7720840573310852 + 50.0 * 6.274353504180908
Epoch 630, val loss: 1.01325523853302
Epoch 640, training loss: 314.4704284667969 = 0.756390392780304 + 50.0 * 6.274281024932861
Epoch 640, val loss: 1.0026997327804565
Epoch 650, training loss: 314.35791015625 = 0.740901529788971 + 50.0 * 6.272339820861816
Epoch 650, val loss: 0.9919620156288147
Epoch 660, training loss: 314.27227783203125 = 0.725607693195343 + 50.0 * 6.270933151245117
Epoch 660, val loss: 0.9820507764816284
Epoch 670, training loss: 314.1983947753906 = 0.7105141878128052 + 50.0 * 6.2697577476501465
Epoch 670, val loss: 0.9722180366516113
Epoch 680, training loss: 314.3316650390625 = 0.6956424117088318 + 50.0 * 6.2727203369140625
Epoch 680, val loss: 0.9626016020774841
Epoch 690, training loss: 314.0650939941406 = 0.6808237433433533 + 50.0 * 6.267685413360596
Epoch 690, val loss: 0.9532938599586487
Epoch 700, training loss: 313.98077392578125 = 0.6661747694015503 + 50.0 * 6.266292095184326
Epoch 700, val loss: 0.9441589117050171
Epoch 710, training loss: 313.8830261230469 = 0.6517526507377625 + 50.0 * 6.264625072479248
Epoch 710, val loss: 0.9352800846099854
Epoch 720, training loss: 313.92120361328125 = 0.6374207139015198 + 50.0 * 6.2656755447387695
Epoch 720, val loss: 0.9265674948692322
Epoch 730, training loss: 314.0511169433594 = 0.6232158541679382 + 50.0 * 6.268558025360107
Epoch 730, val loss: 0.9181692600250244
Epoch 740, training loss: 313.80084228515625 = 0.6090168952941895 + 50.0 * 6.26383638381958
Epoch 740, val loss: 0.9096870422363281
Epoch 750, training loss: 313.66595458984375 = 0.5949997305870056 + 50.0 * 6.26141881942749
Epoch 750, val loss: 0.9014309048652649
Epoch 760, training loss: 313.7427062988281 = 0.5811812877655029 + 50.0 * 6.263230323791504
Epoch 760, val loss: 0.8933057188987732
Epoch 770, training loss: 313.6886901855469 = 0.5674137473106384 + 50.0 * 6.262425422668457
Epoch 770, val loss: 0.885462760925293
Epoch 780, training loss: 313.4971008300781 = 0.5537187457084656 + 50.0 * 6.2588677406311035
Epoch 780, val loss: 0.8779762387275696
Epoch 790, training loss: 313.3921813964844 = 0.5402320623397827 + 50.0 * 6.2570390701293945
Epoch 790, val loss: 0.8704638481140137
Epoch 800, training loss: 313.44647216796875 = 0.5268632173538208 + 50.0 * 6.258392333984375
Epoch 800, val loss: 0.8631094694137573
Epoch 810, training loss: 313.2748718261719 = 0.5135869979858398 + 50.0 * 6.255226135253906
Epoch 810, val loss: 0.8558140993118286
Epoch 820, training loss: 313.3979797363281 = 0.5004716515541077 + 50.0 * 6.257950305938721
Epoch 820, val loss: 0.8483955264091492
Epoch 830, training loss: 313.2674560546875 = 0.48741385340690613 + 50.0 * 6.255600452423096
Epoch 830, val loss: 0.841995358467102
Epoch 840, training loss: 313.19476318359375 = 0.47456395626068115 + 50.0 * 6.254403591156006
Epoch 840, val loss: 0.8347116112709045
Epoch 850, training loss: 313.0500183105469 = 0.4618818759918213 + 50.0 * 6.251762390136719
Epoch 850, val loss: 0.8282784223556519
Epoch 860, training loss: 313.0050048828125 = 0.44942179322242737 + 50.0 * 6.2511115074157715
Epoch 860, val loss: 0.8221068382263184
Epoch 870, training loss: 313.22198486328125 = 0.4371172785758972 + 50.0 * 6.255697727203369
Epoch 870, val loss: 0.8156102895736694
Epoch 880, training loss: 313.1015625 = 0.42507994174957275 + 50.0 * 6.2535295486450195
Epoch 880, val loss: 0.8098108768463135
Epoch 890, training loss: 312.9427795410156 = 0.4131443202495575 + 50.0 * 6.2505927085876465
Epoch 890, val loss: 0.8038249015808105
Epoch 900, training loss: 312.89434814453125 = 0.4014805853366852 + 50.0 * 6.249856948852539
Epoch 900, val loss: 0.7979589700698853
Epoch 910, training loss: 312.7665710449219 = 0.39009252190589905 + 50.0 * 6.247529029846191
Epoch 910, val loss: 0.792533278465271
Epoch 920, training loss: 312.7618713378906 = 0.37900885939598083 + 50.0 * 6.247657299041748
Epoch 920, val loss: 0.787356972694397
Epoch 930, training loss: 313.0381164550781 = 0.3681809604167938 + 50.0 * 6.253398418426514
Epoch 930, val loss: 0.7822512984275818
Epoch 940, training loss: 312.6798400878906 = 0.35753610730171204 + 50.0 * 6.246445655822754
Epoch 940, val loss: 0.7773659825325012
Epoch 950, training loss: 312.64239501953125 = 0.3472459316253662 + 50.0 * 6.2459025382995605
Epoch 950, val loss: 0.7730315923690796
Epoch 960, training loss: 312.576171875 = 0.33732473850250244 + 50.0 * 6.244777202606201
Epoch 960, val loss: 0.7686884999275208
Epoch 970, training loss: 312.9006652832031 = 0.327704519033432 + 50.0 * 6.25145959854126
Epoch 970, val loss: 0.7643914222717285
Epoch 980, training loss: 312.5915222167969 = 0.3182659149169922 + 50.0 * 6.24546480178833
Epoch 980, val loss: 0.7610454559326172
Epoch 990, training loss: 312.4651794433594 = 0.30921563506126404 + 50.0 * 6.243119239807129
Epoch 990, val loss: 0.7572813034057617
Epoch 1000, training loss: 312.9161376953125 = 0.30043232440948486 + 50.0 * 6.252313613891602
Epoch 1000, val loss: 0.7542439103126526
Epoch 1010, training loss: 312.5479736328125 = 0.2919645607471466 + 50.0 * 6.245120048522949
Epoch 1010, val loss: 0.7506656050682068
Epoch 1020, training loss: 312.34722900390625 = 0.283719003200531 + 50.0 * 6.241270065307617
Epoch 1020, val loss: 0.7481783628463745
Epoch 1030, training loss: 312.2917175292969 = 0.2758060395717621 + 50.0 * 6.2403178215026855
Epoch 1030, val loss: 0.7456088066101074
Epoch 1040, training loss: 312.2431945800781 = 0.26818475127220154 + 50.0 * 6.239500045776367
Epoch 1040, val loss: 0.7434396147727966
Epoch 1050, training loss: 312.5711669921875 = 0.26078739762306213 + 50.0 * 6.2462077140808105
Epoch 1050, val loss: 0.7416731715202332
Epoch 1060, training loss: 312.4178466796875 = 0.25363683700561523 + 50.0 * 6.243284225463867
Epoch 1060, val loss: 0.739501416683197
Epoch 1070, training loss: 312.15869140625 = 0.24668100476264954 + 50.0 * 6.2382402420043945
Epoch 1070, val loss: 0.7379590272903442
Epoch 1080, training loss: 312.1380920410156 = 0.24000173807144165 + 50.0 * 6.237961769104004
Epoch 1080, val loss: 0.7367003560066223
Epoch 1090, training loss: 312.3469543457031 = 0.23356996476650238 + 50.0 * 6.242267608642578
Epoch 1090, val loss: 0.7357644438743591
Epoch 1100, training loss: 312.1663818359375 = 0.227285236120224 + 50.0 * 6.238781452178955
Epoch 1100, val loss: 0.7346954345703125
Epoch 1110, training loss: 312.067626953125 = 0.22122147679328918 + 50.0 * 6.2369279861450195
Epoch 1110, val loss: 0.7338683605194092
Epoch 1120, training loss: 312.0146179199219 = 0.2153584361076355 + 50.0 * 6.235984802246094
Epoch 1120, val loss: 0.7333666086196899
Epoch 1130, training loss: 312.2147216796875 = 0.20971320569515228 + 50.0 * 6.240099906921387
Epoch 1130, val loss: 0.7328124642372131
Epoch 1140, training loss: 312.23876953125 = 0.20420709252357483 + 50.0 * 6.240691661834717
Epoch 1140, val loss: 0.7330590486526489
Epoch 1150, training loss: 312.009765625 = 0.19877105951309204 + 50.0 * 6.236219882965088
Epoch 1150, val loss: 0.7327341437339783
Epoch 1160, training loss: 311.89617919921875 = 0.1936136782169342 + 50.0 * 6.234051704406738
Epoch 1160, val loss: 0.7332220673561096
Epoch 1170, training loss: 311.8461608886719 = 0.18859361112117767 + 50.0 * 6.233151435852051
Epoch 1170, val loss: 0.7335548400878906
Epoch 1180, training loss: 311.8830261230469 = 0.18373121321201324 + 50.0 * 6.233985424041748
Epoch 1180, val loss: 0.7342038750648499
Epoch 1190, training loss: 312.0426940917969 = 0.17898152768611908 + 50.0 * 6.237274169921875
Epoch 1190, val loss: 0.734976053237915
Epoch 1200, training loss: 311.8022766113281 = 0.17435616254806519 + 50.0 * 6.232558727264404
Epoch 1200, val loss: 0.7356529831886292
Epoch 1210, training loss: 311.74774169921875 = 0.16988922655582428 + 50.0 * 6.2315568923950195
Epoch 1210, val loss: 0.7366572022438049
Epoch 1220, training loss: 311.7373352050781 = 0.1655464619398117 + 50.0 * 6.231435775756836
Epoch 1220, val loss: 0.7379390597343445
Epoch 1230, training loss: 312.0815124511719 = 0.16132374107837677 + 50.0 * 6.238403797149658
Epoch 1230, val loss: 0.7394237518310547
Epoch 1240, training loss: 311.86199951171875 = 0.15722183883190155 + 50.0 * 6.234095573425293
Epoch 1240, val loss: 0.7404425740242004
Epoch 1250, training loss: 311.703369140625 = 0.15319767594337463 + 50.0 * 6.231003284454346
Epoch 1250, val loss: 0.7419185042381287
Epoch 1260, training loss: 311.6337585449219 = 0.14934581518173218 + 50.0 * 6.2296881675720215
Epoch 1260, val loss: 0.7435784339904785
Epoch 1270, training loss: 311.7264404296875 = 0.14559517800807953 + 50.0 * 6.231616973876953
Epoch 1270, val loss: 0.7452599406242371
Epoch 1280, training loss: 311.7684326171875 = 0.1419365108013153 + 50.0 * 6.232530117034912
Epoch 1280, val loss: 0.7472488880157471
Epoch 1290, training loss: 311.6097106933594 = 0.1383131742477417 + 50.0 * 6.229427814483643
Epoch 1290, val loss: 0.7489669919013977
Epoch 1300, training loss: 311.58367919921875 = 0.13482677936553955 + 50.0 * 6.228977203369141
Epoch 1300, val loss: 0.7512305974960327
Epoch 1310, training loss: 311.5102844238281 = 0.13146920502185822 + 50.0 * 6.22757625579834
Epoch 1310, val loss: 0.7534436583518982
Epoch 1320, training loss: 311.5390625 = 0.12821270525455475 + 50.0 * 6.228217124938965
Epoch 1320, val loss: 0.7555806636810303
Epoch 1330, training loss: 311.74481201171875 = 0.12502308189868927 + 50.0 * 6.232395648956299
Epoch 1330, val loss: 0.7579900026321411
Epoch 1340, training loss: 311.5994567871094 = 0.1218947097659111 + 50.0 * 6.229551315307617
Epoch 1340, val loss: 0.760530412197113
Epoch 1350, training loss: 311.4295959472656 = 0.11886667460203171 + 50.0 * 6.22621488571167
Epoch 1350, val loss: 0.7629690766334534
Epoch 1360, training loss: 311.4239501953125 = 0.1159389466047287 + 50.0 * 6.226160049438477
Epoch 1360, val loss: 0.7657040953636169
Epoch 1370, training loss: 311.59930419921875 = 0.11308334022760391 + 50.0 * 6.229724407196045
Epoch 1370, val loss: 0.7683715224266052
Epoch 1380, training loss: 311.4337463378906 = 0.11031100898981094 + 50.0 * 6.226468563079834
Epoch 1380, val loss: 0.7713169455528259
Epoch 1390, training loss: 311.7292785644531 = 0.10761848837137222 + 50.0 * 6.232433319091797
Epoch 1390, val loss: 0.7740523219108582
Epoch 1400, training loss: 311.4228820800781 = 0.10493914037942886 + 50.0 * 6.226358413696289
Epoch 1400, val loss: 0.7767391800880432
Epoch 1410, training loss: 311.3536682128906 = 0.10236333310604095 + 50.0 * 6.2250261306762695
Epoch 1410, val loss: 0.7799798250198364
Epoch 1420, training loss: 311.3044738769531 = 0.09987261146306992 + 50.0 * 6.224091529846191
Epoch 1420, val loss: 0.7829488515853882
Epoch 1430, training loss: 311.3212890625 = 0.09745921194553375 + 50.0 * 6.224476337432861
Epoch 1430, val loss: 0.7862522006034851
Epoch 1440, training loss: 311.6122131347656 = 0.09510741382837296 + 50.0 * 6.230342388153076
Epoch 1440, val loss: 0.7894275188446045
Epoch 1450, training loss: 311.3799743652344 = 0.09281958639621735 + 50.0 * 6.225742816925049
Epoch 1450, val loss: 0.7921654582023621
Epoch 1460, training loss: 311.30767822265625 = 0.09056159853935242 + 50.0 * 6.224342346191406
Epoch 1460, val loss: 0.795678973197937
Epoch 1470, training loss: 311.3905334472656 = 0.08839204907417297 + 50.0 * 6.226043224334717
Epoch 1470, val loss: 0.7987257242202759
Epoch 1480, training loss: 311.2328796386719 = 0.08627460896968842 + 50.0 * 6.222931861877441
Epoch 1480, val loss: 0.8022246956825256
Epoch 1490, training loss: 311.1632385253906 = 0.08421725779771805 + 50.0 * 6.2215800285339355
Epoch 1490, val loss: 0.80547034740448
Epoch 1500, training loss: 311.19268798828125 = 0.0822259932756424 + 50.0 * 6.222209453582764
Epoch 1500, val loss: 0.8089331388473511
Epoch 1510, training loss: 311.3130798339844 = 0.08029273897409439 + 50.0 * 6.224655628204346
Epoch 1510, val loss: 0.8123344779014587
Epoch 1520, training loss: 311.4833068847656 = 0.07840687036514282 + 50.0 * 6.228097915649414
Epoch 1520, val loss: 0.8155941963195801
Epoch 1530, training loss: 311.197509765625 = 0.07651256769895554 + 50.0 * 6.2224202156066895
Epoch 1530, val loss: 0.8188320398330688
Epoch 1540, training loss: 311.1610107421875 = 0.07470565289258957 + 50.0 * 6.221725940704346
Epoch 1540, val loss: 0.8220438361167908
Epoch 1550, training loss: 311.0892639160156 = 0.07296401262283325 + 50.0 * 6.220325946807861
Epoch 1550, val loss: 0.8257942199707031
Epoch 1560, training loss: 311.0445861816406 = 0.07128603011369705 + 50.0 * 6.219466209411621
Epoch 1560, val loss: 0.829319179058075
Epoch 1570, training loss: 311.02490234375 = 0.06965471059083939 + 50.0 * 6.219105243682861
Epoch 1570, val loss: 0.8329346776008606
Epoch 1580, training loss: 311.03717041015625 = 0.06807182729244232 + 50.0 * 6.219381809234619
Epoch 1580, val loss: 0.836500883102417
Epoch 1590, training loss: 311.4515380859375 = 0.0665464773774147 + 50.0 * 6.2276997566223145
Epoch 1590, val loss: 0.8399878144264221
Epoch 1600, training loss: 311.056884765625 = 0.06499101221561432 + 50.0 * 6.219837665557861
Epoch 1600, val loss: 0.843182384967804
Epoch 1610, training loss: 311.0394287109375 = 0.06351036578416824 + 50.0 * 6.219518661499023
Epoch 1610, val loss: 0.8468580842018127
Epoch 1620, training loss: 311.1559143066406 = 0.06207848712801933 + 50.0 * 6.221877098083496
Epoch 1620, val loss: 0.850441575050354
Epoch 1630, training loss: 311.0034484863281 = 0.0606810599565506 + 50.0 * 6.218855857849121
Epoch 1630, val loss: 0.8539923429489136
Epoch 1640, training loss: 311.0161437988281 = 0.0593346543610096 + 50.0 * 6.2191362380981445
Epoch 1640, val loss: 0.8573864698410034
Epoch 1650, training loss: 311.0052490234375 = 0.05802140384912491 + 50.0 * 6.218944549560547
Epoch 1650, val loss: 0.8610001802444458
Epoch 1660, training loss: 311.40338134765625 = 0.056745849549770355 + 50.0 * 6.226933002471924
Epoch 1660, val loss: 0.864458441734314
Epoch 1670, training loss: 311.04071044921875 = 0.05548335984349251 + 50.0 * 6.219704627990723
Epoch 1670, val loss: 0.8681398034095764
Epoch 1680, training loss: 310.9020690917969 = 0.05426190420985222 + 50.0 * 6.21695613861084
Epoch 1680, val loss: 0.8717020750045776
Epoch 1690, training loss: 310.86328125 = 0.05309294909238815 + 50.0 * 6.216203689575195
Epoch 1690, val loss: 0.875424861907959
Epoch 1700, training loss: 310.9029846191406 = 0.05195734649896622 + 50.0 * 6.217020511627197
Epoch 1700, val loss: 0.8790953755378723
Epoch 1710, training loss: 311.34619140625 = 0.050841834396123886 + 50.0 * 6.225906848907471
Epoch 1710, val loss: 0.8825916647911072
Epoch 1720, training loss: 310.91064453125 = 0.0497417189180851 + 50.0 * 6.217217922210693
Epoch 1720, val loss: 0.8858667016029358
Epoch 1730, training loss: 310.81219482421875 = 0.04868012294173241 + 50.0 * 6.215270519256592
Epoch 1730, val loss: 0.8894950747489929
Epoch 1740, training loss: 310.8200378417969 = 0.04766177013516426 + 50.0 * 6.215447425842285
Epoch 1740, val loss: 0.8932077884674072
Epoch 1750, training loss: 311.0321350097656 = 0.04667701572179794 + 50.0 * 6.2197089195251465
Epoch 1750, val loss: 0.8967434763908386
Epoch 1760, training loss: 310.81695556640625 = 0.04569975659251213 + 50.0 * 6.215425491333008
Epoch 1760, val loss: 0.9002924561500549
Epoch 1770, training loss: 310.8868103027344 = 0.04475327953696251 + 50.0 * 6.216841697692871
Epoch 1770, val loss: 0.9039204716682434
Epoch 1780, training loss: 310.8294372558594 = 0.04383636638522148 + 50.0 * 6.215712070465088
Epoch 1780, val loss: 0.907346785068512
Epoch 1790, training loss: 310.7720947265625 = 0.0429498665034771 + 50.0 * 6.214583396911621
Epoch 1790, val loss: 0.9109131693840027
Epoch 1800, training loss: 310.7582702636719 = 0.04208856821060181 + 50.0 * 6.2143235206604
Epoch 1800, val loss: 0.9144439101219177
Epoch 1810, training loss: 310.9829406738281 = 0.04126622900366783 + 50.0 * 6.2188334465026855
Epoch 1810, val loss: 0.9178836345672607
Epoch 1820, training loss: 310.9489440917969 = 0.040420375764369965 + 50.0 * 6.218170642852783
Epoch 1820, val loss: 0.9213594794273376
Epoch 1830, training loss: 310.784912109375 = 0.03960799053311348 + 50.0 * 6.214905738830566
Epoch 1830, val loss: 0.924802839756012
Epoch 1840, training loss: 310.71270751953125 = 0.03882439061999321 + 50.0 * 6.213477611541748
Epoch 1840, val loss: 0.9283602833747864
Epoch 1850, training loss: 310.7554016113281 = 0.03806978091597557 + 50.0 * 6.214346885681152
Epoch 1850, val loss: 0.9318701028823853
Epoch 1860, training loss: 310.7508850097656 = 0.03732913359999657 + 50.0 * 6.214271545410156
Epoch 1860, val loss: 0.9353264570236206
Epoch 1870, training loss: 310.9048767089844 = 0.03661037236452103 + 50.0 * 6.217365264892578
Epoch 1870, val loss: 0.9387629628181458
Epoch 1880, training loss: 310.68792724609375 = 0.03590849041938782 + 50.0 * 6.213040351867676
Epoch 1880, val loss: 0.9421812891960144
Epoch 1890, training loss: 310.6753234863281 = 0.03522486984729767 + 50.0 * 6.212802410125732
Epoch 1890, val loss: 0.9456143975257874
Epoch 1900, training loss: 310.6669921875 = 0.034557096660137177 + 50.0 * 6.212648868560791
Epoch 1900, val loss: 0.9489759802818298
Epoch 1910, training loss: 310.9739990234375 = 0.033914871513843536 + 50.0 * 6.218801975250244
Epoch 1910, val loss: 0.9522637128829956
Epoch 1920, training loss: 310.6961669921875 = 0.033274661749601364 + 50.0 * 6.213257789611816
Epoch 1920, val loss: 0.9557497501373291
Epoch 1930, training loss: 310.60205078125 = 0.0326581746339798 + 50.0 * 6.211388111114502
Epoch 1930, val loss: 0.9591233730316162
Epoch 1940, training loss: 310.7579345703125 = 0.03207117319107056 + 50.0 * 6.214517116546631
Epoch 1940, val loss: 0.962445080280304
Epoch 1950, training loss: 310.7132568359375 = 0.03148096799850464 + 50.0 * 6.213635444641113
Epoch 1950, val loss: 0.9656354188919067
Epoch 1960, training loss: 310.569091796875 = 0.030897222459316254 + 50.0 * 6.210764408111572
Epoch 1960, val loss: 0.969016432762146
Epoch 1970, training loss: 310.5448913574219 = 0.030341489240527153 + 50.0 * 6.210290908813477
Epoch 1970, val loss: 0.9724195599555969
Epoch 1980, training loss: 310.51885986328125 = 0.029805904254317284 + 50.0 * 6.209780693054199
Epoch 1980, val loss: 0.975782036781311
Epoch 1990, training loss: 310.5875549316406 = 0.029286803677678108 + 50.0 * 6.211165428161621
Epoch 1990, val loss: 0.9790582656860352
Epoch 2000, training loss: 310.6718444824219 = 0.028772369027137756 + 50.0 * 6.212861061096191
Epoch 2000, val loss: 0.9822499752044678
Epoch 2010, training loss: 310.542724609375 = 0.028266414999961853 + 50.0 * 6.210289478302002
Epoch 2010, val loss: 0.9854824542999268
Epoch 2020, training loss: 310.55059814453125 = 0.027778619900345802 + 50.0 * 6.210456848144531
Epoch 2020, val loss: 0.9887617230415344
Epoch 2030, training loss: 310.6326599121094 = 0.027301235124468803 + 50.0 * 6.212107181549072
Epoch 2030, val loss: 0.9919809699058533
Epoch 2040, training loss: 310.635498046875 = 0.026842178776860237 + 50.0 * 6.212172985076904
Epoch 2040, val loss: 0.9952652454376221
Epoch 2050, training loss: 310.585205078125 = 0.02638114057481289 + 50.0 * 6.211176872253418
Epoch 2050, val loss: 0.9982650279998779
Epoch 2060, training loss: 310.50732421875 = 0.02593744546175003 + 50.0 * 6.209627628326416
Epoch 2060, val loss: 1.001399278640747
Epoch 2070, training loss: 310.4523620605469 = 0.02550559863448143 + 50.0 * 6.2085371017456055
Epoch 2070, val loss: 1.0047564506530762
Epoch 2080, training loss: 310.5186462402344 = 0.025093531236052513 + 50.0 * 6.209871292114258
Epoch 2080, val loss: 1.0079425573349
Epoch 2090, training loss: 310.7064514160156 = 0.024691814556717873 + 50.0 * 6.213634967803955
Epoch 2090, val loss: 1.0108270645141602
Epoch 2100, training loss: 310.5543518066406 = 0.024274688214063644 + 50.0 * 6.210601806640625
Epoch 2100, val loss: 1.0139045715332031
Epoch 2110, training loss: 310.45684814453125 = 0.023877419531345367 + 50.0 * 6.208659648895264
Epoch 2110, val loss: 1.0170122385025024
Epoch 2120, training loss: 310.4007568359375 = 0.023489953950047493 + 50.0 * 6.207545280456543
Epoch 2120, val loss: 1.0201383829116821
Epoch 2130, training loss: 310.413330078125 = 0.02312021702528 + 50.0 * 6.207804203033447
Epoch 2130, val loss: 1.0233495235443115
Epoch 2140, training loss: 310.7822570800781 = 0.02275981940329075 + 50.0 * 6.2151899337768555
Epoch 2140, val loss: 1.0262233018875122
Epoch 2150, training loss: 310.5080871582031 = 0.02239966206252575 + 50.0 * 6.209713935852051
Epoch 2150, val loss: 1.0292572975158691
Epoch 2160, training loss: 310.5815124511719 = 0.0220412015914917 + 50.0 * 6.2111897468566895
Epoch 2160, val loss: 1.0322240591049194
Epoch 2170, training loss: 310.4111328125 = 0.02170327678322792 + 50.0 * 6.207788944244385
Epoch 2170, val loss: 1.0351636409759521
Epoch 2180, training loss: 310.3838806152344 = 0.02136552892625332 + 50.0 * 6.207250595092773
Epoch 2180, val loss: 1.038191795349121
Epoch 2190, training loss: 310.38018798828125 = 0.0210427176207304 + 50.0 * 6.20718240737915
Epoch 2190, val loss: 1.0412194728851318
Epoch 2200, training loss: 310.4453125 = 0.020725922659039497 + 50.0 * 6.208491802215576
Epoch 2200, val loss: 1.0442347526550293
Epoch 2210, training loss: 310.4571533203125 = 0.0204133503139019 + 50.0 * 6.208734512329102
Epoch 2210, val loss: 1.0472052097320557
Epoch 2220, training loss: 310.4364318847656 = 0.02010487951338291 + 50.0 * 6.20832633972168
Epoch 2220, val loss: 1.049978256225586
Epoch 2230, training loss: 310.3102722167969 = 0.019811438396573067 + 50.0 * 6.205809116363525
Epoch 2230, val loss: 1.052857756614685
Epoch 2240, training loss: 310.317626953125 = 0.019522951915860176 + 50.0 * 6.20596170425415
Epoch 2240, val loss: 1.0558069944381714
Epoch 2250, training loss: 310.467529296875 = 0.019246233627200127 + 50.0 * 6.208965301513672
Epoch 2250, val loss: 1.0586886405944824
Epoch 2260, training loss: 310.3728942871094 = 0.018961310386657715 + 50.0 * 6.20707893371582
Epoch 2260, val loss: 1.0616041421890259
Epoch 2270, training loss: 310.4324645996094 = 0.018685687333345413 + 50.0 * 6.20827579498291
Epoch 2270, val loss: 1.0643943548202515
Epoch 2280, training loss: 310.2860412597656 = 0.01841241866350174 + 50.0 * 6.205352783203125
Epoch 2280, val loss: 1.0670111179351807
Epoch 2290, training loss: 310.2948913574219 = 0.01815284602344036 + 50.0 * 6.2055344581604
Epoch 2290, val loss: 1.0699844360351562
Epoch 2300, training loss: 310.57891845703125 = 0.0178990475833416 + 50.0 * 6.2112202644348145
Epoch 2300, val loss: 1.0727216005325317
Epoch 2310, training loss: 310.309326171875 = 0.017650149762630463 + 50.0 * 6.205833911895752
Epoch 2310, val loss: 1.075534701347351
Epoch 2320, training loss: 310.2414245605469 = 0.017402829602360725 + 50.0 * 6.2044806480407715
Epoch 2320, val loss: 1.07832932472229
Epoch 2330, training loss: 310.22918701171875 = 0.01716667227447033 + 50.0 * 6.204239845275879
Epoch 2330, val loss: 1.0812169313430786
Epoch 2340, training loss: 310.585693359375 = 0.01694602146744728 + 50.0 * 6.2113752365112305
Epoch 2340, val loss: 1.0840054750442505
Epoch 2350, training loss: 310.3677673339844 = 0.016702838242053986 + 50.0 * 6.207021236419678
Epoch 2350, val loss: 1.0863925218582153
Epoch 2360, training loss: 310.2664794921875 = 0.016474325209856033 + 50.0 * 6.204999923706055
Epoch 2360, val loss: 1.0891962051391602
Epoch 2370, training loss: 310.1900634765625 = 0.016252435743808746 + 50.0 * 6.2034759521484375
Epoch 2370, val loss: 1.0919208526611328
Epoch 2380, training loss: 310.2110900878906 = 0.016041383147239685 + 50.0 * 6.2039008140563965
Epoch 2380, val loss: 1.09470796585083
Epoch 2390, training loss: 310.5067443847656 = 0.015838244929909706 + 50.0 * 6.209817886352539
Epoch 2390, val loss: 1.0972864627838135
Epoch 2400, training loss: 310.3390197753906 = 0.01562524028122425 + 50.0 * 6.206467628479004
Epoch 2400, val loss: 1.0996613502502441
Epoch 2410, training loss: 310.28546142578125 = 0.015417857095599174 + 50.0 * 6.2054009437561035
Epoch 2410, val loss: 1.1023164987564087
Epoch 2420, training loss: 310.1800231933594 = 0.015217116102576256 + 50.0 * 6.203295707702637
Epoch 2420, val loss: 1.1050336360931396
Epoch 2430, training loss: 310.2352294921875 = 0.015025727450847626 + 50.0 * 6.204404354095459
Epoch 2430, val loss: 1.1078613996505737
Epoch 2440, training loss: 310.3184814453125 = 0.014829493127763271 + 50.0 * 6.206072807312012
Epoch 2440, val loss: 1.1102503538131714
Epoch 2450, training loss: 310.1737060546875 = 0.014637067914009094 + 50.0 * 6.203181266784668
Epoch 2450, val loss: 1.1126532554626465
Epoch 2460, training loss: 310.1304626464844 = 0.014453809708356857 + 50.0 * 6.202320098876953
Epoch 2460, val loss: 1.115368366241455
Epoch 2470, training loss: 310.21136474609375 = 0.014274593442678452 + 50.0 * 6.203941345214844
Epoch 2470, val loss: 1.117952823638916
Epoch 2480, training loss: 310.25848388671875 = 0.014100375585258007 + 50.0 * 6.204887390136719
Epoch 2480, val loss: 1.1204979419708252
Epoch 2490, training loss: 310.231689453125 = 0.013930113054811954 + 50.0 * 6.204355239868164
Epoch 2490, val loss: 1.1228793859481812
Epoch 2500, training loss: 310.1121826171875 = 0.013754097744822502 + 50.0 * 6.201968193054199
Epoch 2500, val loss: 1.1254304647445679
Epoch 2510, training loss: 310.0977783203125 = 0.013588964007794857 + 50.0 * 6.20168399810791
Epoch 2510, val loss: 1.1280231475830078
Epoch 2520, training loss: 310.2290954589844 = 0.01342910248786211 + 50.0 * 6.204313278198242
Epoch 2520, val loss: 1.1304875612258911
Epoch 2530, training loss: 310.1636962890625 = 0.013267064467072487 + 50.0 * 6.203008651733398
Epoch 2530, val loss: 1.1328500509262085
Epoch 2540, training loss: 310.23712158203125 = 0.013113307766616344 + 50.0 * 6.204480171203613
Epoch 2540, val loss: 1.1352871656417847
Epoch 2550, training loss: 310.1854248046875 = 0.012956150807440281 + 50.0 * 6.203449249267578
Epoch 2550, val loss: 1.1377087831497192
Epoch 2560, training loss: 310.1351013183594 = 0.012799938209354877 + 50.0 * 6.202445983886719
Epoch 2560, val loss: 1.1400119066238403
Epoch 2570, training loss: 310.06396484375 = 0.012648667208850384 + 50.0 * 6.201026916503906
Epoch 2570, val loss: 1.1424633264541626
Epoch 2580, training loss: 310.03265380859375 = 0.01250464841723442 + 50.0 * 6.200402736663818
Epoch 2580, val loss: 1.1448910236358643
Epoch 2590, training loss: 310.3227233886719 = 0.012365873903036118 + 50.0 * 6.206207275390625
Epoch 2590, val loss: 1.147228717803955
Epoch 2600, training loss: 310.0691223144531 = 0.012219816446304321 + 50.0 * 6.201138496398926
Epoch 2600, val loss: 1.1496156454086304
Epoch 2610, training loss: 310.0188293457031 = 0.012079195119440556 + 50.0 * 6.200134754180908
Epoch 2610, val loss: 1.151859164237976
Epoch 2620, training loss: 310.2252502441406 = 0.011944405734539032 + 50.0 * 6.20426607131958
Epoch 2620, val loss: 1.15412437915802
Epoch 2630, training loss: 310.0591735839844 = 0.011808613315224648 + 50.0 * 6.200947284698486
Epoch 2630, val loss: 1.1563879251480103
Epoch 2640, training loss: 310.2096252441406 = 0.01167867612093687 + 50.0 * 6.203958511352539
Epoch 2640, val loss: 1.158616304397583
Epoch 2650, training loss: 309.9878234863281 = 0.011544052511453629 + 50.0 * 6.199525833129883
Epoch 2650, val loss: 1.1609766483306885
Epoch 2660, training loss: 309.9927673339844 = 0.011415966786444187 + 50.0 * 6.199626922607422
Epoch 2660, val loss: 1.1633822917938232
Epoch 2670, training loss: 309.9629211425781 = 0.011293075978755951 + 50.0 * 6.199032306671143
Epoch 2670, val loss: 1.1656893491744995
Epoch 2680, training loss: 310.0435791015625 = 0.011174687184393406 + 50.0 * 6.200648307800293
Epoch 2680, val loss: 1.1680248975753784
Epoch 2690, training loss: 310.1396789550781 = 0.011055350303649902 + 50.0 * 6.202572345733643
Epoch 2690, val loss: 1.170109510421753
Epoch 2700, training loss: 310.0867919921875 = 0.010940068401396275 + 50.0 * 6.201517105102539
Epoch 2700, val loss: 1.1722255945205688
Epoch 2710, training loss: 310.1083679199219 = 0.010820670984685421 + 50.0 * 6.201950550079346
Epoch 2710, val loss: 1.1743725538253784
Epoch 2720, training loss: 310.1661376953125 = 0.010712013579905033 + 50.0 * 6.203108310699463
Epoch 2720, val loss: 1.1765148639678955
Epoch 2730, training loss: 310.07000732421875 = 0.0105958366766572 + 50.0 * 6.201188564300537
Epoch 2730, val loss: 1.1787291765213013
Epoch 2740, training loss: 309.9720458984375 = 0.010479791089892387 + 50.0 * 6.1992316246032715
Epoch 2740, val loss: 1.180834174156189
Epoch 2750, training loss: 309.9406433105469 = 0.010373305529356003 + 50.0 * 6.198605060577393
Epoch 2750, val loss: 1.1830570697784424
Epoch 2760, training loss: 309.9532775878906 = 0.010270743630826473 + 50.0 * 6.198860168457031
Epoch 2760, val loss: 1.1853030920028687
Epoch 2770, training loss: 310.0724182128906 = 0.010168850421905518 + 50.0 * 6.201245307922363
Epoch 2770, val loss: 1.1874823570251465
Epoch 2780, training loss: 310.0440673828125 = 0.010065182112157345 + 50.0 * 6.200679779052734
Epoch 2780, val loss: 1.1893858909606934
Epoch 2790, training loss: 310.0457458496094 = 0.00996248796582222 + 50.0 * 6.200716018676758
Epoch 2790, val loss: 1.191473126411438
Epoch 2800, training loss: 309.96710205078125 = 0.009862946346402168 + 50.0 * 6.1991448402404785
Epoch 2800, val loss: 1.1935101747512817
Epoch 2810, training loss: 309.8868408203125 = 0.009760715067386627 + 50.0 * 6.197541236877441
Epoch 2810, val loss: 1.1956629753112793
Epoch 2820, training loss: 309.88909912109375 = 0.009664480574429035 + 50.0 * 6.1975884437561035
Epoch 2820, val loss: 1.1977286338806152
Epoch 2830, training loss: 310.0143737792969 = 0.00957506150007248 + 50.0 * 6.2000956535339355
Epoch 2830, val loss: 1.1998544931411743
Epoch 2840, training loss: 310.0399475097656 = 0.009482749737799168 + 50.0 * 6.20060920715332
Epoch 2840, val loss: 1.201698660850525
Epoch 2850, training loss: 309.90716552734375 = 0.009388708509504795 + 50.0 * 6.19795560836792
Epoch 2850, val loss: 1.2034698724746704
Epoch 2860, training loss: 309.8505859375 = 0.009294283576309681 + 50.0 * 6.1968255043029785
Epoch 2860, val loss: 1.2055442333221436
Epoch 2870, training loss: 309.86651611328125 = 0.00920722633600235 + 50.0 * 6.197145938873291
Epoch 2870, val loss: 1.207635521888733
Epoch 2880, training loss: 309.9645690917969 = 0.009123368188738823 + 50.0 * 6.199109077453613
Epoch 2880, val loss: 1.2096209526062012
Epoch 2890, training loss: 309.94091796875 = 0.009039037860929966 + 50.0 * 6.19863748550415
Epoch 2890, val loss: 1.2116056680679321
Epoch 2900, training loss: 309.98077392578125 = 0.008955284021794796 + 50.0 * 6.199436664581299
Epoch 2900, val loss: 1.2134405374526978
Epoch 2910, training loss: 310.0679931640625 = 0.008869948796927929 + 50.0 * 6.2011823654174805
Epoch 2910, val loss: 1.2154090404510498
Epoch 2920, training loss: 309.95635986328125 = 0.008787870407104492 + 50.0 * 6.198951244354248
Epoch 2920, val loss: 1.217254400253296
Epoch 2930, training loss: 309.8908996582031 = 0.008704065345227718 + 50.0 * 6.197644233703613
Epoch 2930, val loss: 1.219124436378479
Epoch 2940, training loss: 309.81439208984375 = 0.00862665195018053 + 50.0 * 6.196115016937256
Epoch 2940, val loss: 1.2211501598358154
Epoch 2950, training loss: 309.985107421875 = 0.008552462793886662 + 50.0 * 6.199531555175781
Epoch 2950, val loss: 1.2229266166687012
Epoch 2960, training loss: 309.80621337890625 = 0.00847357977181673 + 50.0 * 6.195954322814941
Epoch 2960, val loss: 1.2247226238250732
Epoch 2970, training loss: 309.813232421875 = 0.008397135883569717 + 50.0 * 6.196096897125244
Epoch 2970, val loss: 1.2267402410507202
Epoch 2980, training loss: 309.93414306640625 = 0.008324198424816132 + 50.0 * 6.198516368865967
Epoch 2980, val loss: 1.2284671068191528
Epoch 2990, training loss: 309.9559020996094 = 0.00824784953147173 + 50.0 * 6.198953151702881
Epoch 2990, val loss: 1.2302072048187256
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8339483394833949
The final CL Acc:0.74321, 0.02463, The final GNN Acc:0.83711, 0.00240
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11636])
remove edge: torch.Size([2, 9454])
updated graph: torch.Size([2, 10534])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.77581787109375 = 1.9347165822982788 + 50.0 * 8.596821784973145
Epoch 0, val loss: 1.9284188747406006
Epoch 10, training loss: 431.72222900390625 = 1.9260603189468384 + 50.0 * 8.59592342376709
Epoch 10, val loss: 1.919377088546753
Epoch 20, training loss: 431.3658752441406 = 1.9151532649993896 + 50.0 * 8.589014053344727
Epoch 20, val loss: 1.9078832864761353
Epoch 30, training loss: 429.09124755859375 = 1.9004631042480469 + 50.0 * 8.543815612792969
Epoch 30, val loss: 1.8924964666366577
Epoch 40, training loss: 417.49664306640625 = 1.8827494382858276 + 50.0 * 8.312277793884277
Epoch 40, val loss: 1.874785304069519
Epoch 50, training loss: 393.5318603515625 = 1.8618669509887695 + 50.0 * 7.833399772644043
Epoch 50, val loss: 1.8545637130737305
Epoch 60, training loss: 376.1326599121094 = 1.8473767042160034 + 50.0 * 7.485705375671387
Epoch 60, val loss: 1.8419883251190186
Epoch 70, training loss: 360.31109619140625 = 1.8384315967559814 + 50.0 * 7.169453144073486
Epoch 70, val loss: 1.8344392776489258
Epoch 80, training loss: 349.09039306640625 = 1.8324352502822876 + 50.0 * 6.945159435272217
Epoch 80, val loss: 1.828942894935608
Epoch 90, training loss: 343.0126037597656 = 1.8251675367355347 + 50.0 * 6.823748588562012
Epoch 90, val loss: 1.8222941160202026
Epoch 100, training loss: 338.8212890625 = 1.817858338356018 + 50.0 * 6.740068435668945
Epoch 100, val loss: 1.8161845207214355
Epoch 110, training loss: 335.5527038574219 = 1.8118361234664917 + 50.0 * 6.6748175621032715
Epoch 110, val loss: 1.8108019828796387
Epoch 120, training loss: 333.0710754394531 = 1.8059592247009277 + 50.0 * 6.625301837921143
Epoch 120, val loss: 1.8055521249771118
Epoch 130, training loss: 331.0506896972656 = 1.8000282049179077 + 50.0 * 6.585013389587402
Epoch 130, val loss: 1.8003696203231812
Epoch 140, training loss: 329.49786376953125 = 1.7941608428955078 + 50.0 * 6.554074287414551
Epoch 140, val loss: 1.7953540086746216
Epoch 150, training loss: 328.06036376953125 = 1.7881473302841187 + 50.0 * 6.525444507598877
Epoch 150, val loss: 1.79021155834198
Epoch 160, training loss: 326.82952880859375 = 1.7818087339401245 + 50.0 * 6.500954627990723
Epoch 160, val loss: 1.784889578819275
Epoch 170, training loss: 325.9348449707031 = 1.7751243114471436 + 50.0 * 6.483194828033447
Epoch 170, val loss: 1.779240608215332
Epoch 180, training loss: 324.90692138671875 = 1.767788052558899 + 50.0 * 6.462782859802246
Epoch 180, val loss: 1.7731893062591553
Epoch 190, training loss: 324.0866394042969 = 1.759910225868225 + 50.0 * 6.446534156799316
Epoch 190, val loss: 1.766780138015747
Epoch 200, training loss: 323.5155029296875 = 1.7512738704681396 + 50.0 * 6.43528413772583
Epoch 200, val loss: 1.759823203086853
Epoch 210, training loss: 322.859130859375 = 1.7418750524520874 + 50.0 * 6.422345161437988
Epoch 210, val loss: 1.7522526979446411
Epoch 220, training loss: 322.33013916015625 = 1.731604814529419 + 50.0 * 6.411970615386963
Epoch 220, val loss: 1.7440147399902344
Epoch 230, training loss: 321.820068359375 = 1.7205429077148438 + 50.0 * 6.4019904136657715
Epoch 230, val loss: 1.7352486848831177
Epoch 240, training loss: 321.336181640625 = 1.7085849046707153 + 50.0 * 6.392551898956299
Epoch 240, val loss: 1.7259289026260376
Epoch 250, training loss: 321.0116271972656 = 1.6956788301467896 + 50.0 * 6.386319160461426
Epoch 250, val loss: 1.7158111333847046
Epoch 260, training loss: 320.6596374511719 = 1.6817294359207153 + 50.0 * 6.379558086395264
Epoch 260, val loss: 1.7049392461776733
Epoch 270, training loss: 320.2656555175781 = 1.6668832302093506 + 50.0 * 6.371975898742676
Epoch 270, val loss: 1.6934846639633179
Epoch 280, training loss: 319.98248291015625 = 1.6511850357055664 + 50.0 * 6.366626262664795
Epoch 280, val loss: 1.6814013719558716
Epoch 290, training loss: 319.9395446777344 = 1.634401798248291 + 50.0 * 6.366102695465088
Epoch 290, val loss: 1.6685477495193481
Epoch 300, training loss: 319.4602966308594 = 1.6169240474700928 + 50.0 * 6.356867790222168
Epoch 300, val loss: 1.6553055047988892
Epoch 310, training loss: 319.2117614746094 = 1.5988142490386963 + 50.0 * 6.352259159088135
Epoch 310, val loss: 1.6416418552398682
Epoch 320, training loss: 318.939208984375 = 1.5800952911376953 + 50.0 * 6.347182273864746
Epoch 320, val loss: 1.6276005506515503
Epoch 330, training loss: 318.8816223144531 = 1.5609477758407593 + 50.0 * 6.346413612365723
Epoch 330, val loss: 1.6132363080978394
Epoch 340, training loss: 318.56304931640625 = 1.5410940647125244 + 50.0 * 6.3404388427734375
Epoch 340, val loss: 1.598738431930542
Epoch 350, training loss: 318.2711181640625 = 1.5211544036865234 + 50.0 * 6.3349995613098145
Epoch 350, val loss: 1.5841577053070068
Epoch 360, training loss: 318.1220703125 = 1.5010415315628052 + 50.0 * 6.332420825958252
Epoch 360, val loss: 1.569563388824463
Epoch 370, training loss: 318.0366516113281 = 1.4806947708129883 + 50.0 * 6.331119537353516
Epoch 370, val loss: 1.5549331903457642
Epoch 380, training loss: 317.70355224609375 = 1.4600716829299927 + 50.0 * 6.324869155883789
Epoch 380, val loss: 1.5401232242584229
Epoch 390, training loss: 317.476806640625 = 1.4395787715911865 + 50.0 * 6.320744514465332
Epoch 390, val loss: 1.5255604982376099
Epoch 400, training loss: 317.2616882324219 = 1.4190651178359985 + 50.0 * 6.316852569580078
Epoch 400, val loss: 1.511189579963684
Epoch 410, training loss: 317.3927001953125 = 1.398594856262207 + 50.0 * 6.319882392883301
Epoch 410, val loss: 1.497023105621338
Epoch 420, training loss: 316.96759033203125 = 1.3778016567230225 + 50.0 * 6.311795711517334
Epoch 420, val loss: 1.4825736284255981
Epoch 430, training loss: 316.804443359375 = 1.3572112321853638 + 50.0 * 6.3089447021484375
Epoch 430, val loss: 1.4684128761291504
Epoch 440, training loss: 316.6081848144531 = 1.336682915687561 + 50.0 * 6.3054304122924805
Epoch 440, val loss: 1.4545495510101318
Epoch 450, training loss: 316.774658203125 = 1.3162624835968018 + 50.0 * 6.309167861938477
Epoch 450, val loss: 1.440880537033081
Epoch 460, training loss: 316.34490966796875 = 1.2954245805740356 + 50.0 * 6.300990104675293
Epoch 460, val loss: 1.4265916347503662
Epoch 470, training loss: 316.25048828125 = 1.2749263048171997 + 50.0 * 6.299510955810547
Epoch 470, val loss: 1.4129687547683716
Epoch 480, training loss: 316.0673828125 = 1.254537582397461 + 50.0 * 6.2962565422058105
Epoch 480, val loss: 1.3995246887207031
Epoch 490, training loss: 315.9208984375 = 1.2343171834945679 + 50.0 * 6.293731689453125
Epoch 490, val loss: 1.3862993717193604
Epoch 500, training loss: 316.025634765625 = 1.214119791984558 + 50.0 * 6.296230316162109
Epoch 500, val loss: 1.373364806175232
Epoch 510, training loss: 315.78558349609375 = 1.1938896179199219 + 50.0 * 6.291833877563477
Epoch 510, val loss: 1.3599464893341064
Epoch 520, training loss: 315.6548767089844 = 1.1737982034683228 + 50.0 * 6.289621353149414
Epoch 520, val loss: 1.3472082614898682
Epoch 530, training loss: 315.50347900390625 = 1.1541837453842163 + 50.0 * 6.286985874176025
Epoch 530, val loss: 1.3346714973449707
Epoch 540, training loss: 315.3730163574219 = 1.13489830493927 + 50.0 * 6.284762382507324
Epoch 540, val loss: 1.3226547241210938
Epoch 550, training loss: 315.6377258300781 = 1.1159299612045288 + 50.0 * 6.290435791015625
Epoch 550, val loss: 1.3109302520751953
Epoch 560, training loss: 315.2250061035156 = 1.09686279296875 + 50.0 * 6.282562732696533
Epoch 560, val loss: 1.2992216348648071
Epoch 570, training loss: 315.1055603027344 = 1.0782842636108398 + 50.0 * 6.280545711517334
Epoch 570, val loss: 1.28806734085083
Epoch 580, training loss: 314.9947814941406 = 1.0601673126220703 + 50.0 * 6.278692245483398
Epoch 580, val loss: 1.2773845195770264
Epoch 590, training loss: 314.91241455078125 = 1.0423357486724854 + 50.0 * 6.277401447296143
Epoch 590, val loss: 1.2669771909713745
Epoch 600, training loss: 314.8476867675781 = 1.0246617794036865 + 50.0 * 6.276460647583008
Epoch 600, val loss: 1.256577730178833
Epoch 610, training loss: 314.7510681152344 = 1.0074256658554077 + 50.0 * 6.274872779846191
Epoch 610, val loss: 1.2469196319580078
Epoch 620, training loss: 314.6590576171875 = 0.9906253218650818 + 50.0 * 6.2733683586120605
Epoch 620, val loss: 1.2376197576522827
Epoch 630, training loss: 314.61480712890625 = 0.9742476940155029 + 50.0 * 6.272811412811279
Epoch 630, val loss: 1.2288082838058472
Epoch 640, training loss: 314.5874938964844 = 0.9581454396247864 + 50.0 * 6.272586822509766
Epoch 640, val loss: 1.2201294898986816
Epoch 650, training loss: 314.45843505859375 = 0.9422504901885986 + 50.0 * 6.270323753356934
Epoch 650, val loss: 1.2118926048278809
Epoch 660, training loss: 314.44232177734375 = 0.9268569946289062 + 50.0 * 6.2703094482421875
Epoch 660, val loss: 1.2041865587234497
Epoch 670, training loss: 314.3673095703125 = 0.9114288687705994 + 50.0 * 6.26911735534668
Epoch 670, val loss: 1.1962165832519531
Epoch 680, training loss: 314.3346862792969 = 0.8965664505958557 + 50.0 * 6.268762111663818
Epoch 680, val loss: 1.18905770778656
Epoch 690, training loss: 314.1552734375 = 0.8817662000656128 + 50.0 * 6.265470027923584
Epoch 690, val loss: 1.1819580793380737
Epoch 700, training loss: 314.09613037109375 = 0.8673404455184937 + 50.0 * 6.264575958251953
Epoch 700, val loss: 1.1752666234970093
Epoch 710, training loss: 314.15032958984375 = 0.8531626462936401 + 50.0 * 6.26594352722168
Epoch 710, val loss: 1.168911099433899
Epoch 720, training loss: 314.0923156738281 = 0.8390194177627563 + 50.0 * 6.265066146850586
Epoch 720, val loss: 1.1623955965042114
Epoch 730, training loss: 314.1162109375 = 0.8250551819801331 + 50.0 * 6.2658233642578125
Epoch 730, val loss: 1.1562345027923584
Epoch 740, training loss: 313.8706970214844 = 0.8113263249397278 + 50.0 * 6.2611870765686035
Epoch 740, val loss: 1.1504658460617065
Epoch 750, training loss: 313.78521728515625 = 0.7978948950767517 + 50.0 * 6.259746551513672
Epoch 750, val loss: 1.1448980569839478
Epoch 760, training loss: 313.7481384277344 = 0.7846765518188477 + 50.0 * 6.2592692375183105
Epoch 760, val loss: 1.1395015716552734
Epoch 770, training loss: 313.7877197265625 = 0.771562397480011 + 50.0 * 6.2603230476379395
Epoch 770, val loss: 1.1343237161636353
Epoch 780, training loss: 313.8030700683594 = 0.7585350275039673 + 50.0 * 6.260890483856201
Epoch 780, val loss: 1.1295068264007568
Epoch 790, training loss: 313.66162109375 = 0.74566650390625 + 50.0 * 6.258318901062012
Epoch 790, val loss: 1.1245301961898804
Epoch 800, training loss: 313.5723571777344 = 0.7329006195068359 + 50.0 * 6.256788730621338
Epoch 800, val loss: 1.1202412843704224
Epoch 810, training loss: 313.4588317871094 = 0.7205284833908081 + 50.0 * 6.254766464233398
Epoch 810, val loss: 1.1160433292388916
Epoch 820, training loss: 313.45538330078125 = 0.708292543888092 + 50.0 * 6.254941940307617
Epoch 820, val loss: 1.1122894287109375
Epoch 830, training loss: 313.4277038574219 = 0.6961258053779602 + 50.0 * 6.254631042480469
Epoch 830, val loss: 1.108358383178711
Epoch 840, training loss: 313.42584228515625 = 0.6841278076171875 + 50.0 * 6.254833698272705
Epoch 840, val loss: 1.104937195777893
Epoch 850, training loss: 313.3269958496094 = 0.6722232699394226 + 50.0 * 6.253095626831055
Epoch 850, val loss: 1.1013175249099731
Epoch 860, training loss: 313.2867126464844 = 0.6607189774513245 + 50.0 * 6.2525200843811035
Epoch 860, val loss: 1.0985214710235596
Epoch 870, training loss: 313.2460021972656 = 0.6493024230003357 + 50.0 * 6.251934051513672
Epoch 870, val loss: 1.0956294536590576
Epoch 880, training loss: 313.15423583984375 = 0.6380197405815125 + 50.0 * 6.250324249267578
Epoch 880, val loss: 1.0928713083267212
Epoch 890, training loss: 313.13592529296875 = 0.6270325183868408 + 50.0 * 6.25017786026001
Epoch 890, val loss: 1.0907026529312134
Epoch 900, training loss: 313.2042236328125 = 0.6161811351776123 + 50.0 * 6.251760959625244
Epoch 900, val loss: 1.0884230136871338
Epoch 910, training loss: 313.0682067871094 = 0.6055587530136108 + 50.0 * 6.249252796173096
Epoch 910, val loss: 1.0867187976837158
Epoch 920, training loss: 313.03802490234375 = 0.5950931310653687 + 50.0 * 6.24885892868042
Epoch 920, val loss: 1.0852006673812866
Epoch 930, training loss: 313.07012939453125 = 0.5847625732421875 + 50.0 * 6.2497076988220215
Epoch 930, val loss: 1.0835829973220825
Epoch 940, training loss: 312.9137878417969 = 0.5747203826904297 + 50.0 * 6.246781826019287
Epoch 940, val loss: 1.0823901891708374
Epoch 950, training loss: 312.8292541503906 = 0.5648531317710876 + 50.0 * 6.245287895202637
Epoch 950, val loss: 1.0814627408981323
Epoch 960, training loss: 312.7912902832031 = 0.555206835269928 + 50.0 * 6.24472188949585
Epoch 960, val loss: 1.0807782411575317
Epoch 970, training loss: 313.1661071777344 = 0.5456542372703552 + 50.0 * 6.252408981323242
Epoch 970, val loss: 1.0801641941070557
Epoch 980, training loss: 312.9564514160156 = 0.5362731218338013 + 50.0 * 6.248403072357178
Epoch 980, val loss: 1.0794708728790283
Epoch 990, training loss: 312.7174987792969 = 0.5269829034805298 + 50.0 * 6.243810176849365
Epoch 990, val loss: 1.079331874847412
Epoch 1000, training loss: 312.7683410644531 = 0.5179369449615479 + 50.0 * 6.2450079917907715
Epoch 1000, val loss: 1.0793954133987427
Epoch 1010, training loss: 312.62384033203125 = 0.5091369152069092 + 50.0 * 6.2422943115234375
Epoch 1010, val loss: 1.0795292854309082
Epoch 1020, training loss: 312.5926208496094 = 0.5004982352256775 + 50.0 * 6.241842746734619
Epoch 1020, val loss: 1.07989501953125
Epoch 1030, training loss: 312.70458984375 = 0.49205413460731506 + 50.0 * 6.244250297546387
Epoch 1030, val loss: 1.080620527267456
Epoch 1040, training loss: 312.64935302734375 = 0.4835895001888275 + 50.0 * 6.24331521987915
Epoch 1040, val loss: 1.080928087234497
Epoch 1050, training loss: 312.5542907714844 = 0.4752853810787201 + 50.0 * 6.241580009460449
Epoch 1050, val loss: 1.0818225145339966
Epoch 1060, training loss: 312.4464416503906 = 0.4672325849533081 + 50.0 * 6.239584445953369
Epoch 1060, val loss: 1.082734227180481
Epoch 1070, training loss: 312.4184875488281 = 0.45935767889022827 + 50.0 * 6.239182472229004
Epoch 1070, val loss: 1.0838955640792847
Epoch 1080, training loss: 312.7180480957031 = 0.451574444770813 + 50.0 * 6.245329856872559
Epoch 1080, val loss: 1.08500075340271
Epoch 1090, training loss: 312.3800964355469 = 0.4438404440879822 + 50.0 * 6.238725185394287
Epoch 1090, val loss: 1.0862517356872559
Epoch 1100, training loss: 312.3049621582031 = 0.4362884759902954 + 50.0 * 6.2373738288879395
Epoch 1100, val loss: 1.0878385305404663
Epoch 1110, training loss: 312.3251953125 = 0.4289620816707611 + 50.0 * 6.237925052642822
Epoch 1110, val loss: 1.0895872116088867
Epoch 1120, training loss: 312.3286437988281 = 0.42165040969848633 + 50.0 * 6.238139629364014
Epoch 1120, val loss: 1.0911905765533447
Epoch 1130, training loss: 312.24468994140625 = 0.4144240617752075 + 50.0 * 6.236605167388916
Epoch 1130, val loss: 1.0928877592086792
Epoch 1140, training loss: 312.22564697265625 = 0.4074007570743561 + 50.0 * 6.236364841461182
Epoch 1140, val loss: 1.094871997833252
Epoch 1150, training loss: 312.29156494140625 = 0.4005472660064697 + 50.0 * 6.237820148468018
Epoch 1150, val loss: 1.0970631837844849
Epoch 1160, training loss: 312.1582336425781 = 0.39373520016670227 + 50.0 * 6.235290050506592
Epoch 1160, val loss: 1.0993444919586182
Epoch 1170, training loss: 312.11572265625 = 0.38701269030570984 + 50.0 * 6.234573841094971
Epoch 1170, val loss: 1.101675033569336
Epoch 1180, training loss: 312.0867614746094 = 0.3804323375225067 + 50.0 * 6.234126567840576
Epoch 1180, val loss: 1.10430908203125
Epoch 1190, training loss: 312.3855895996094 = 0.3740045726299286 + 50.0 * 6.240231990814209
Epoch 1190, val loss: 1.1069618463516235
Epoch 1200, training loss: 312.2499694824219 = 0.3674294948577881 + 50.0 * 6.2376508712768555
Epoch 1200, val loss: 1.1092090606689453
Epoch 1210, training loss: 312.0438232421875 = 0.3611333668231964 + 50.0 * 6.233653545379639
Epoch 1210, val loss: 1.1122976541519165
Epoch 1220, training loss: 311.992431640625 = 0.3549277186393738 + 50.0 * 6.232750415802002
Epoch 1220, val loss: 1.1152740716934204
Epoch 1230, training loss: 311.998779296875 = 0.3488374352455139 + 50.0 * 6.232998847961426
Epoch 1230, val loss: 1.118368148803711
Epoch 1240, training loss: 311.980224609375 = 0.34284400939941406 + 50.0 * 6.232747554779053
Epoch 1240, val loss: 1.121570110321045
Epoch 1250, training loss: 311.8899841308594 = 0.33694908022880554 + 50.0 * 6.231060981750488
Epoch 1250, val loss: 1.1249752044677734
Epoch 1260, training loss: 312.16192626953125 = 0.3311693072319031 + 50.0 * 6.236615180969238
Epoch 1260, val loss: 1.1284109354019165
Epoch 1270, training loss: 312.0321044921875 = 0.3253196179866791 + 50.0 * 6.234135627746582
Epoch 1270, val loss: 1.1316457986831665
Epoch 1280, training loss: 311.8450927734375 = 0.3196413218975067 + 50.0 * 6.230508804321289
Epoch 1280, val loss: 1.1353647708892822
Epoch 1290, training loss: 311.7855529785156 = 0.3140648901462555 + 50.0 * 6.229430198669434
Epoch 1290, val loss: 1.1390646696090698
Epoch 1300, training loss: 311.8282165527344 = 0.30863839387893677 + 50.0 * 6.230391979217529
Epoch 1300, val loss: 1.1429355144500732
Epoch 1310, training loss: 312.01373291015625 = 0.3032320737838745 + 50.0 * 6.23421049118042
Epoch 1310, val loss: 1.1466283798217773
Epoch 1320, training loss: 311.760986328125 = 0.29770970344543457 + 50.0 * 6.2292656898498535
Epoch 1320, val loss: 1.1502420902252197
Epoch 1330, training loss: 311.7529296875 = 0.29242780804634094 + 50.0 * 6.229209899902344
Epoch 1330, val loss: 1.1542917490005493
Epoch 1340, training loss: 311.675537109375 = 0.28727373480796814 + 50.0 * 6.2277655601501465
Epoch 1340, val loss: 1.158361792564392
Epoch 1350, training loss: 311.7173767089844 = 0.28221395611763 + 50.0 * 6.228703022003174
Epoch 1350, val loss: 1.1625581979751587
Epoch 1360, training loss: 311.7107238769531 = 0.2771785259246826 + 50.0 * 6.228670597076416
Epoch 1360, val loss: 1.1667176485061646
Epoch 1370, training loss: 311.6484680175781 = 0.27224498987197876 + 50.0 * 6.227524280548096
Epoch 1370, val loss: 1.1710617542266846
Epoch 1380, training loss: 311.6116943359375 = 0.26737579703330994 + 50.0 * 6.226886749267578
Epoch 1380, val loss: 1.1754056215286255
Epoch 1390, training loss: 311.6876525878906 = 0.26256659626960754 + 50.0 * 6.228501796722412
Epoch 1390, val loss: 1.1797430515289307
Epoch 1400, training loss: 311.560546875 = 0.2578126788139343 + 50.0 * 6.226054668426514
Epoch 1400, val loss: 1.1842471361160278
Epoch 1410, training loss: 311.57177734375 = 0.2531578540802002 + 50.0 * 6.226372718811035
Epoch 1410, val loss: 1.1888450384140015
Epoch 1420, training loss: 311.4842834472656 = 0.24853470921516418 + 50.0 * 6.224714756011963
Epoch 1420, val loss: 1.1934866905212402
Epoch 1430, training loss: 311.4971618652344 = 0.2440418303012848 + 50.0 * 6.225062370300293
Epoch 1430, val loss: 1.1982485055923462
Epoch 1440, training loss: 311.65997314453125 = 0.23959976434707642 + 50.0 * 6.228407382965088
Epoch 1440, val loss: 1.202846646308899
Epoch 1450, training loss: 311.6280212402344 = 0.23518818616867065 + 50.0 * 6.227856159210205
Epoch 1450, val loss: 1.207686185836792
Epoch 1460, training loss: 311.4923400878906 = 0.23076125979423523 + 50.0 * 6.225231170654297
Epoch 1460, val loss: 1.2122267484664917
Epoch 1470, training loss: 311.4168701171875 = 0.22655019164085388 + 50.0 * 6.223806381225586
Epoch 1470, val loss: 1.2173048257827759
Epoch 1480, training loss: 311.5262451171875 = 0.22238434851169586 + 50.0 * 6.226077556610107
Epoch 1480, val loss: 1.222163438796997
Epoch 1490, training loss: 311.3581848144531 = 0.21823596954345703 + 50.0 * 6.222798824310303
Epoch 1490, val loss: 1.2270041704177856
Epoch 1500, training loss: 311.37689208984375 = 0.21417663991451263 + 50.0 * 6.223254203796387
Epoch 1500, val loss: 1.2319884300231934
Epoch 1510, training loss: 311.5235595703125 = 0.21020089089870453 + 50.0 * 6.226266860961914
Epoch 1510, val loss: 1.2370105981826782
Epoch 1520, training loss: 311.3968505859375 = 0.2062627077102661 + 50.0 * 6.223811626434326
Epoch 1520, val loss: 1.2420482635498047
Epoch 1530, training loss: 311.30712890625 = 0.20237146317958832 + 50.0 * 6.222095489501953
Epoch 1530, val loss: 1.2471731901168823
Epoch 1540, training loss: 311.2471008300781 = 0.19856573641300201 + 50.0 * 6.220970630645752
Epoch 1540, val loss: 1.2523928880691528
Epoch 1550, training loss: 311.2884826660156 = 0.1948578804731369 + 50.0 * 6.221872806549072
Epoch 1550, val loss: 1.25773024559021
Epoch 1560, training loss: 311.3860778808594 = 0.19118809700012207 + 50.0 * 6.223897933959961
Epoch 1560, val loss: 1.262953519821167
Epoch 1570, training loss: 311.2773742675781 = 0.18755196034908295 + 50.0 * 6.22179651260376
Epoch 1570, val loss: 1.2680857181549072
Epoch 1580, training loss: 311.2652282714844 = 0.18396133184432983 + 50.0 * 6.221625328063965
Epoch 1580, val loss: 1.2733838558197021
Epoch 1590, training loss: 311.3633728027344 = 0.18052265048027039 + 50.0 * 6.223657131195068
Epoch 1590, val loss: 1.2788538932800293
Epoch 1600, training loss: 311.16827392578125 = 0.1769971400499344 + 50.0 * 6.219825744628906
Epoch 1600, val loss: 1.2838053703308105
Epoch 1610, training loss: 311.118896484375 = 0.1736203134059906 + 50.0 * 6.218905448913574
Epoch 1610, val loss: 1.2892640829086304
Epoch 1620, training loss: 311.09039306640625 = 0.17032186686992645 + 50.0 * 6.2184014320373535
Epoch 1620, val loss: 1.2946860790252686
Epoch 1630, training loss: 311.153564453125 = 0.16709811985492706 + 50.0 * 6.219729900360107
Epoch 1630, val loss: 1.3000177145004272
Epoch 1640, training loss: 311.22308349609375 = 0.16390082240104675 + 50.0 * 6.2211833000183105
Epoch 1640, val loss: 1.30520498752594
Epoch 1650, training loss: 311.13336181640625 = 0.16067302227020264 + 50.0 * 6.219453811645508
Epoch 1650, val loss: 1.3103795051574707
Epoch 1660, training loss: 311.10882568359375 = 0.15757916867733002 + 50.0 * 6.219024658203125
Epoch 1660, val loss: 1.315922737121582
Epoch 1670, training loss: 311.02886962890625 = 0.1545361876487732 + 50.0 * 6.217486381530762
Epoch 1670, val loss: 1.321304440498352
Epoch 1680, training loss: 310.994873046875 = 0.1515645533800125 + 50.0 * 6.216866493225098
Epoch 1680, val loss: 1.3265838623046875
Epoch 1690, training loss: 311.4490661621094 = 0.14869272708892822 + 50.0 * 6.22600793838501
Epoch 1690, val loss: 1.3317363262176514
Epoch 1700, training loss: 311.13861083984375 = 0.14576508104801178 + 50.0 * 6.219857215881348
Epoch 1700, val loss: 1.3372515439987183
Epoch 1710, training loss: 310.963134765625 = 0.142901211977005 + 50.0 * 6.216404438018799
Epoch 1710, val loss: 1.3423774242401123
Epoch 1720, training loss: 310.9335021972656 = 0.14018040895462036 + 50.0 * 6.215866565704346
Epoch 1720, val loss: 1.347891092300415
Epoch 1730, training loss: 311.6148376464844 = 0.13755100965499878 + 50.0 * 6.229546070098877
Epoch 1730, val loss: 1.3532819747924805
Epoch 1740, training loss: 311.1017150878906 = 0.13477806746959686 + 50.0 * 6.219338893890381
Epoch 1740, val loss: 1.3581054210662842
Epoch 1750, training loss: 310.8949279785156 = 0.13216300308704376 + 50.0 * 6.215255260467529
Epoch 1750, val loss: 1.3636668920516968
Epoch 1760, training loss: 310.859619140625 = 0.1296374499797821 + 50.0 * 6.214599609375
Epoch 1760, val loss: 1.3691787719726562
Epoch 1770, training loss: 310.8548889160156 = 0.12718327343463898 + 50.0 * 6.2145538330078125
Epoch 1770, val loss: 1.3746768236160278
Epoch 1780, training loss: 311.3569030761719 = 0.124771848320961 + 50.0 * 6.224642276763916
Epoch 1780, val loss: 1.379860520362854
Epoch 1790, training loss: 310.982177734375 = 0.12233088165521622 + 50.0 * 6.217196464538574
Epoch 1790, val loss: 1.3849693536758423
Epoch 1800, training loss: 310.8271789550781 = 0.11995376646518707 + 50.0 * 6.214144229888916
Epoch 1800, val loss: 1.3904699087142944
Epoch 1810, training loss: 310.8259582519531 = 0.11768437176942825 + 50.0 * 6.214165687561035
Epoch 1810, val loss: 1.3958224058151245
Epoch 1820, training loss: 311.07427978515625 = 0.11544838547706604 + 50.0 * 6.219176769256592
Epoch 1820, val loss: 1.400969386100769
Epoch 1830, training loss: 310.9163513183594 = 0.11317810416221619 + 50.0 * 6.216063022613525
Epoch 1830, val loss: 1.406157374382019
Epoch 1840, training loss: 310.82952880859375 = 0.11101648956537247 + 50.0 * 6.214370250701904
Epoch 1840, val loss: 1.4114553928375244
Epoch 1850, training loss: 310.7531433105469 = 0.10889964550733566 + 50.0 * 6.21288537979126
Epoch 1850, val loss: 1.4168592691421509
Epoch 1860, training loss: 310.7616882324219 = 0.10685519874095917 + 50.0 * 6.213096618652344
Epoch 1860, val loss: 1.4221383333206177
Epoch 1870, training loss: 310.983154296875 = 0.10483153909444809 + 50.0 * 6.21756649017334
Epoch 1870, val loss: 1.4272410869598389
Epoch 1880, training loss: 310.9174499511719 = 0.1028486117720604 + 50.0 * 6.216292381286621
Epoch 1880, val loss: 1.4326348304748535
Epoch 1890, training loss: 310.77978515625 = 0.10084960609674454 + 50.0 * 6.213578701019287
Epoch 1890, val loss: 1.4376685619354248
Epoch 1900, training loss: 310.7193603515625 = 0.09894578903913498 + 50.0 * 6.212408542633057
Epoch 1900, val loss: 1.4430660009384155
Epoch 1910, training loss: 310.6705322265625 = 0.09709247201681137 + 50.0 * 6.211468696594238
Epoch 1910, val loss: 1.4481993913650513
Epoch 1920, training loss: 310.71136474609375 = 0.09529728442430496 + 50.0 * 6.2123212814331055
Epoch 1920, val loss: 1.4536148309707642
Epoch 1930, training loss: 310.73199462890625 = 0.09351705014705658 + 50.0 * 6.212769508361816
Epoch 1930, val loss: 1.4586832523345947
Epoch 1940, training loss: 310.7377014160156 = 0.09174549579620361 + 50.0 * 6.212919235229492
Epoch 1940, val loss: 1.4634120464324951
Epoch 1950, training loss: 310.9268798828125 = 0.09002932161092758 + 50.0 * 6.216736793518066
Epoch 1950, val loss: 1.4683618545532227
Epoch 1960, training loss: 310.7677001953125 = 0.08832421153783798 + 50.0 * 6.213587760925293
Epoch 1960, val loss: 1.4733942747116089
Epoch 1970, training loss: 310.6639099121094 = 0.08668559789657593 + 50.0 * 6.211544036865234
Epoch 1970, val loss: 1.4784903526306152
Epoch 1980, training loss: 310.6021423339844 = 0.08507338911294937 + 50.0 * 6.210341453552246
Epoch 1980, val loss: 1.4834516048431396
Epoch 1990, training loss: 310.594482421875 = 0.08352626860141754 + 50.0 * 6.210218906402588
Epoch 1990, val loss: 1.48866605758667
Epoch 2000, training loss: 310.7240905761719 = 0.08200795203447342 + 50.0 * 6.212841987609863
Epoch 2000, val loss: 1.493524432182312
Epoch 2010, training loss: 310.6062316894531 = 0.08050896972417831 + 50.0 * 6.210514545440674
Epoch 2010, val loss: 1.4986459016799927
Epoch 2020, training loss: 310.6781921386719 = 0.07903079688549042 + 50.0 * 6.2119832038879395
Epoch 2020, val loss: 1.503368854522705
Epoch 2030, training loss: 310.5503234863281 = 0.07756621390581131 + 50.0 * 6.2094550132751465
Epoch 2030, val loss: 1.508390188217163
Epoch 2040, training loss: 310.58160400390625 = 0.07616555690765381 + 50.0 * 6.210108757019043
Epoch 2040, val loss: 1.5132290124893188
Epoch 2050, training loss: 310.5875549316406 = 0.07479014992713928 + 50.0 * 6.210255146026611
Epoch 2050, val loss: 1.5181386470794678
Epoch 2060, training loss: 310.51922607421875 = 0.073444664478302 + 50.0 * 6.2089152336120605
Epoch 2060, val loss: 1.5231441259384155
Epoch 2070, training loss: 310.9455871582031 = 0.07212843000888824 + 50.0 * 6.217468738555908
Epoch 2070, val loss: 1.5276002883911133
Epoch 2080, training loss: 310.5870056152344 = 0.07080939412117004 + 50.0 * 6.210323810577393
Epoch 2080, val loss: 1.5325100421905518
Epoch 2090, training loss: 310.4723205566406 = 0.06952562928199768 + 50.0 * 6.2080559730529785
Epoch 2090, val loss: 1.5374780893325806
Epoch 2100, training loss: 310.4917907714844 = 0.06830659508705139 + 50.0 * 6.208469867706299
Epoch 2100, val loss: 1.5422985553741455
Epoch 2110, training loss: 310.6141357421875 = 0.06710588932037354 + 50.0 * 6.210940361022949
Epoch 2110, val loss: 1.5469441413879395
Epoch 2120, training loss: 310.456787109375 = 0.06592537462711334 + 50.0 * 6.207817077636719
Epoch 2120, val loss: 1.5523561239242554
Epoch 2130, training loss: 310.46795654296875 = 0.06477511674165726 + 50.0 * 6.208063125610352
Epoch 2130, val loss: 1.556959629058838
Epoch 2140, training loss: 310.6564025878906 = 0.06365074217319489 + 50.0 * 6.211854934692383
Epoch 2140, val loss: 1.5616543292999268
Epoch 2150, training loss: 310.5204162597656 = 0.06252032518386841 + 50.0 * 6.209157466888428
Epoch 2150, val loss: 1.5661869049072266
Epoch 2160, training loss: 310.4001770019531 = 0.061414144933223724 + 50.0 * 6.206775665283203
Epoch 2160, val loss: 1.5708481073379517
Epoch 2170, training loss: 310.38873291015625 = 0.060371216386556625 + 50.0 * 6.206567287445068
Epoch 2170, val loss: 1.5756717920303345
Epoch 2180, training loss: 310.4348449707031 = 0.05935106799006462 + 50.0 * 6.207509994506836
Epoch 2180, val loss: 1.5802967548370361
Epoch 2190, training loss: 310.7329406738281 = 0.05836356803774834 + 50.0 * 6.213491916656494
Epoch 2190, val loss: 1.5851008892059326
Epoch 2200, training loss: 310.4574279785156 = 0.057319607585668564 + 50.0 * 6.208002090454102
Epoch 2200, val loss: 1.5893076658248901
Epoch 2210, training loss: 310.3628234863281 = 0.056322645395994186 + 50.0 * 6.206129550933838
Epoch 2210, val loss: 1.5938918590545654
Epoch 2220, training loss: 310.334228515625 = 0.05539107695221901 + 50.0 * 6.2055768966674805
Epoch 2220, val loss: 1.598814845085144
Epoch 2230, training loss: 310.6982116699219 = 0.05448950082063675 + 50.0 * 6.212874412536621
Epoch 2230, val loss: 1.6033769845962524
Epoch 2240, training loss: 310.40106201171875 = 0.05354677513241768 + 50.0 * 6.2069501876831055
Epoch 2240, val loss: 1.6075621843338013
Epoch 2250, training loss: 310.3379821777344 = 0.05264892801642418 + 50.0 * 6.20570707321167
Epoch 2250, val loss: 1.6123541593551636
Epoch 2260, training loss: 310.2875061035156 = 0.05178253725171089 + 50.0 * 6.204714298248291
Epoch 2260, val loss: 1.616816759109497
Epoch 2270, training loss: 310.3494873046875 = 0.05095842108130455 + 50.0 * 6.205970764160156
Epoch 2270, val loss: 1.6214179992675781
Epoch 2280, training loss: 310.44384765625 = 0.050128355622291565 + 50.0 * 6.207874298095703
Epoch 2280, val loss: 1.62572181224823
Epoch 2290, training loss: 310.2967224121094 = 0.049270741641521454 + 50.0 * 6.204948902130127
Epoch 2290, val loss: 1.6302810907363892
Epoch 2300, training loss: 310.3351745605469 = 0.0484832264482975 + 50.0 * 6.205733776092529
Epoch 2300, val loss: 1.6348516941070557
Epoch 2310, training loss: 310.30853271484375 = 0.0476951040327549 + 50.0 * 6.205216407775879
Epoch 2310, val loss: 1.6388975381851196
Epoch 2320, training loss: 310.2778015136719 = 0.04691539332270622 + 50.0 * 6.204617977142334
Epoch 2320, val loss: 1.6429029703140259
Epoch 2330, training loss: 310.35150146484375 = 0.04617678374052048 + 50.0 * 6.206106662750244
Epoch 2330, val loss: 1.6473973989486694
Epoch 2340, training loss: 310.2386779785156 = 0.04544112831354141 + 50.0 * 6.203865051269531
Epoch 2340, val loss: 1.651807188987732
Epoch 2350, training loss: 310.2651672363281 = 0.0447402261197567 + 50.0 * 6.204408645629883
Epoch 2350, val loss: 1.6563037633895874
Epoch 2360, training loss: 310.4090576171875 = 0.044047728180885315 + 50.0 * 6.207300186157227
Epoch 2360, val loss: 1.6603275537490845
Epoch 2370, training loss: 310.2527160644531 = 0.04332190379500389 + 50.0 * 6.204187393188477
Epoch 2370, val loss: 1.6643149852752686
Epoch 2380, training loss: 310.3754577636719 = 0.042665187269449234 + 50.0 * 6.206655979156494
Epoch 2380, val loss: 1.668911337852478
Epoch 2390, training loss: 310.1953430175781 = 0.0419829785823822 + 50.0 * 6.203067302703857
Epoch 2390, val loss: 1.6730115413665771
Epoch 2400, training loss: 310.249755859375 = 0.04134146124124527 + 50.0 * 6.20416784286499
Epoch 2400, val loss: 1.677064061164856
Epoch 2410, training loss: 310.27838134765625 = 0.04070562496781349 + 50.0 * 6.204753398895264
Epoch 2410, val loss: 1.681078553199768
Epoch 2420, training loss: 310.16400146484375 = 0.040080469101667404 + 50.0 * 6.202478408813477
Epoch 2420, val loss: 1.6855508089065552
Epoch 2430, training loss: 310.124755859375 = 0.03947867453098297 + 50.0 * 6.201705455780029
Epoch 2430, val loss: 1.6894251108169556
Epoch 2440, training loss: 310.1878662109375 = 0.03889540955424309 + 50.0 * 6.20297908782959
Epoch 2440, val loss: 1.693373680114746
Epoch 2450, training loss: 310.38787841796875 = 0.03832342103123665 + 50.0 * 6.206990718841553
Epoch 2450, val loss: 1.697566270828247
Epoch 2460, training loss: 310.24176025390625 = 0.037731245160102844 + 50.0 * 6.204081058502197
Epoch 2460, val loss: 1.7013585567474365
Epoch 2470, training loss: 310.1910400390625 = 0.0371648371219635 + 50.0 * 6.20307731628418
Epoch 2470, val loss: 1.7056565284729004
Epoch 2480, training loss: 310.2792053222656 = 0.03662390261888504 + 50.0 * 6.2048516273498535
Epoch 2480, val loss: 1.709605097770691
Epoch 2490, training loss: 310.1761474609375 = 0.036062292754650116 + 50.0 * 6.20280122756958
Epoch 2490, val loss: 1.7129722833633423
Epoch 2500, training loss: 310.0999450683594 = 0.03552902117371559 + 50.0 * 6.20128870010376
Epoch 2500, val loss: 1.7169855833053589
Epoch 2510, training loss: 310.0836486816406 = 0.03502458333969116 + 50.0 * 6.200972557067871
Epoch 2510, val loss: 1.721077561378479
Epoch 2520, training loss: 310.0931701660156 = 0.034535087645053864 + 50.0 * 6.201172351837158
Epoch 2520, val loss: 1.7248551845550537
Epoch 2530, training loss: 310.34716796875 = 0.034059565514326096 + 50.0 * 6.206262111663818
Epoch 2530, val loss: 1.7284272909164429
Epoch 2540, training loss: 310.22906494140625 = 0.03354901820421219 + 50.0 * 6.2039103507995605
Epoch 2540, val loss: 1.7320101261138916
Epoch 2550, training loss: 310.08160400390625 = 0.03306185081601143 + 50.0 * 6.2009711265563965
Epoch 2550, val loss: 1.736303448677063
Epoch 2560, training loss: 310.14312744140625 = 0.032603662461042404 + 50.0 * 6.202210426330566
Epoch 2560, val loss: 1.7402249574661255
Epoch 2570, training loss: 310.0736999511719 = 0.032143689692020416 + 50.0 * 6.200831413269043
Epoch 2570, val loss: 1.7435486316680908
Epoch 2580, training loss: 310.06396484375 = 0.03169236332178116 + 50.0 * 6.200645446777344
Epoch 2580, val loss: 1.7474355697631836
Epoch 2590, training loss: 310.24169921875 = 0.031275127083063126 + 50.0 * 6.2042083740234375
Epoch 2590, val loss: 1.7514286041259766
Epoch 2600, training loss: 310.0705871582031 = 0.03082205168902874 + 50.0 * 6.2007951736450195
Epoch 2600, val loss: 1.7547860145568848
Epoch 2610, training loss: 310.06365966796875 = 0.03039073385298252 + 50.0 * 6.200665473937988
Epoch 2610, val loss: 1.7583407163619995
Epoch 2620, training loss: 310.1101379394531 = 0.029985787346959114 + 50.0 * 6.201602935791016
Epoch 2620, val loss: 1.7621769905090332
Epoch 2630, training loss: 310.0099792480469 = 0.029572272673249245 + 50.0 * 6.199608325958252
Epoch 2630, val loss: 1.7653969526290894
Epoch 2640, training loss: 309.9948425292969 = 0.029177570715546608 + 50.0 * 6.199313640594482
Epoch 2640, val loss: 1.769067645072937
Epoch 2650, training loss: 310.1912841796875 = 0.028797172009944916 + 50.0 * 6.203249454498291
Epoch 2650, val loss: 1.772360920906067
Epoch 2660, training loss: 310.0187683105469 = 0.028406107798218727 + 50.0 * 6.199807167053223
Epoch 2660, val loss: 1.775837779045105
Epoch 2670, training loss: 310.0987854003906 = 0.028030376881361008 + 50.0 * 6.201415538787842
Epoch 2670, val loss: 1.7793323993682861
Epoch 2680, training loss: 310.0583801269531 = 0.02765277586877346 + 50.0 * 6.2006144523620605
Epoch 2680, val loss: 1.7826353311538696
Epoch 2690, training loss: 310.00799560546875 = 0.027291081845760345 + 50.0 * 6.19961404800415
Epoch 2690, val loss: 1.7864400148391724
Epoch 2700, training loss: 310.0351867675781 = 0.02694794163107872 + 50.0 * 6.200164794921875
Epoch 2700, val loss: 1.790084958076477
Epoch 2710, training loss: 310.0582275390625 = 0.026594458147883415 + 50.0 * 6.2006330490112305
Epoch 2710, val loss: 1.793089509010315
Epoch 2720, training loss: 309.97796630859375 = 0.026245836168527603 + 50.0 * 6.199034214019775
Epoch 2720, val loss: 1.7966736555099487
Epoch 2730, training loss: 310.05810546875 = 0.025915304198861122 + 50.0 * 6.200644016265869
Epoch 2730, val loss: 1.8000293970108032
Epoch 2740, training loss: 310.1474304199219 = 0.02558780275285244 + 50.0 * 6.202436923980713
Epoch 2740, val loss: 1.8027430772781372
Epoch 2750, training loss: 309.9460144042969 = 0.025247301906347275 + 50.0 * 6.198415279388428
Epoch 2750, val loss: 1.806230902671814
Epoch 2760, training loss: 309.8913879394531 = 0.024929050356149673 + 50.0 * 6.197329044342041
Epoch 2760, val loss: 1.809624195098877
Epoch 2770, training loss: 309.9098815917969 = 0.024625839665532112 + 50.0 * 6.197704792022705
Epoch 2770, val loss: 1.812722086906433
Epoch 2780, training loss: 310.1207275390625 = 0.024332797154784203 + 50.0 * 6.20192813873291
Epoch 2780, val loss: 1.8153820037841797
Epoch 2790, training loss: 310.0150146484375 = 0.024019505828619003 + 50.0 * 6.199820041656494
Epoch 2790, val loss: 1.8185653686523438
Epoch 2800, training loss: 309.95025634765625 = 0.023714972659945488 + 50.0 * 6.198530673980713
Epoch 2800, val loss: 1.8221579790115356
Epoch 2810, training loss: 309.9394226074219 = 0.023425061255693436 + 50.0 * 6.198319911956787
Epoch 2810, val loss: 1.8253796100616455
Epoch 2820, training loss: 309.9019775390625 = 0.023143041878938675 + 50.0 * 6.197576999664307
Epoch 2820, val loss: 1.8282465934753418
Epoch 2830, training loss: 309.92034912109375 = 0.022876854985952377 + 50.0 * 6.197949409484863
Epoch 2830, val loss: 1.8318216800689697
Epoch 2840, training loss: 310.0170593261719 = 0.02260882593691349 + 50.0 * 6.199889183044434
Epoch 2840, val loss: 1.834702730178833
Epoch 2850, training loss: 309.87506103515625 = 0.022318348288536072 + 50.0 * 6.197054862976074
Epoch 2850, val loss: 1.8373745679855347
Epoch 2860, training loss: 309.9595947265625 = 0.022059589624404907 + 50.0 * 6.1987504959106445
Epoch 2860, val loss: 1.8404396772384644
Epoch 2870, training loss: 309.98114013671875 = 0.021806664764881134 + 50.0 * 6.199186325073242
Epoch 2870, val loss: 1.843566656112671
Epoch 2880, training loss: 309.9701232910156 = 0.021540507674217224 + 50.0 * 6.198971748352051
Epoch 2880, val loss: 1.8461915254592896
Epoch 2890, training loss: 309.8904113769531 = 0.02128705196082592 + 50.0 * 6.19738245010376
Epoch 2890, val loss: 1.8491367101669312
Epoch 2900, training loss: 309.9368896484375 = 0.021045010536909103 + 50.0 * 6.19831657409668
Epoch 2900, val loss: 1.8520724773406982
Epoch 2910, training loss: 309.9658508300781 = 0.02080659568309784 + 50.0 * 6.1989006996154785
Epoch 2910, val loss: 1.855000376701355
Epoch 2920, training loss: 309.846435546875 = 0.020554961636662483 + 50.0 * 6.196517467498779
Epoch 2920, val loss: 1.8577262163162231
Epoch 2930, training loss: 309.82208251953125 = 0.020322170108556747 + 50.0 * 6.196034908294678
Epoch 2930, val loss: 1.8607507944107056
Epoch 2940, training loss: 309.82525634765625 = 0.02009839192032814 + 50.0 * 6.196102619171143
Epoch 2940, val loss: 1.8635435104370117
Epoch 2950, training loss: 309.9784240722656 = 0.019881904125213623 + 50.0 * 6.19917106628418
Epoch 2950, val loss: 1.8662534952163696
Epoch 2960, training loss: 309.8687438964844 = 0.01965499110519886 + 50.0 * 6.196981906890869
Epoch 2960, val loss: 1.8695734739303589
Epoch 2970, training loss: 309.872314453125 = 0.01942864991724491 + 50.0 * 6.197057247161865
Epoch 2970, val loss: 1.8719794750213623
Epoch 2980, training loss: 309.82269287109375 = 0.019208835437893867 + 50.0 * 6.196069717407227
Epoch 2980, val loss: 1.8745691776275635
Epoch 2990, training loss: 309.8390197753906 = 0.019000690430402756 + 50.0 * 6.196400165557861
Epoch 2990, val loss: 1.8775423765182495
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6666666666666667
0.8081180811808119
=== training gcn model ===
Epoch 0, training loss: 431.7876892089844 = 1.9458296298980713 + 50.0 * 8.596837043762207
Epoch 0, val loss: 1.9475798606872559
Epoch 10, training loss: 431.7354431152344 = 1.935932993888855 + 50.0 * 8.595990180969238
Epoch 10, val loss: 1.9374558925628662
Epoch 20, training loss: 431.3988037109375 = 1.9231467247009277 + 50.0 * 8.589512825012207
Epoch 20, val loss: 1.9242712259292603
Epoch 30, training loss: 429.248291015625 = 1.906133770942688 + 50.0 * 8.546843528747559
Epoch 30, val loss: 1.9065345525741577
Epoch 40, training loss: 418.59405517578125 = 1.8866772651672363 + 50.0 * 8.334147453308105
Epoch 40, val loss: 1.886595368385315
Epoch 50, training loss: 394.71929931640625 = 1.8655855655670166 + 50.0 * 7.85707426071167
Epoch 50, val loss: 1.865628719329834
Epoch 60, training loss: 376.3547668457031 = 1.8510918617248535 + 50.0 * 7.490073204040527
Epoch 60, val loss: 1.8526122570037842
Epoch 70, training loss: 358.4820556640625 = 1.841691255569458 + 50.0 * 7.132806777954102
Epoch 70, val loss: 1.8438926935195923
Epoch 80, training loss: 349.0919189453125 = 1.835074543952942 + 50.0 * 6.945137023925781
Epoch 80, val loss: 1.8373299837112427
Epoch 90, training loss: 343.0569763183594 = 1.8289119005203247 + 50.0 * 6.82456111907959
Epoch 90, val loss: 1.8311084508895874
Epoch 100, training loss: 338.25494384765625 = 1.8222392797470093 + 50.0 * 6.728653907775879
Epoch 100, val loss: 1.824535846710205
Epoch 110, training loss: 334.5749206542969 = 1.8149893283843994 + 50.0 * 6.655198097229004
Epoch 110, val loss: 1.817469596862793
Epoch 120, training loss: 332.1770324707031 = 1.8082467317581177 + 50.0 * 6.607375621795654
Epoch 120, val loss: 1.8108218908309937
Epoch 130, training loss: 330.04473876953125 = 1.8020952939987183 + 50.0 * 6.564852714538574
Epoch 130, val loss: 1.8047000169754028
Epoch 140, training loss: 328.53753662109375 = 1.7959449291229248 + 50.0 * 6.534831523895264
Epoch 140, val loss: 1.7987507581710815
Epoch 150, training loss: 327.34429931640625 = 1.7892132997512817 + 50.0 * 6.511101722717285
Epoch 150, val loss: 1.7923740148544312
Epoch 160, training loss: 326.28277587890625 = 1.7818764448165894 + 50.0 * 6.490017890930176
Epoch 160, val loss: 1.7856767177581787
Epoch 170, training loss: 325.41180419921875 = 1.7740362882614136 + 50.0 * 6.472755432128906
Epoch 170, val loss: 1.7787755727767944
Epoch 180, training loss: 325.0069580078125 = 1.7656818628311157 + 50.0 * 6.464825630187988
Epoch 180, val loss: 1.7715696096420288
Epoch 190, training loss: 324.10638427734375 = 1.756472110748291 + 50.0 * 6.446998596191406
Epoch 190, val loss: 1.7637367248535156
Epoch 200, training loss: 323.5623779296875 = 1.7465347051620483 + 50.0 * 6.436316967010498
Epoch 200, val loss: 1.7554261684417725
Epoch 210, training loss: 323.0355529785156 = 1.7357892990112305 + 50.0 * 6.425995349884033
Epoch 210, val loss: 1.7465802431106567
Epoch 220, training loss: 322.8144836425781 = 1.7240906953811646 + 50.0 * 6.421807765960693
Epoch 220, val loss: 1.7369177341461182
Epoch 230, training loss: 322.2286682128906 = 1.711325764656067 + 50.0 * 6.410346508026123
Epoch 230, val loss: 1.726480484008789
Epoch 240, training loss: 321.76324462890625 = 1.697687029838562 + 50.0 * 6.401310920715332
Epoch 240, val loss: 1.7154312133789062
Epoch 250, training loss: 321.396728515625 = 1.6829676628112793 + 50.0 * 6.394275665283203
Epoch 250, val loss: 1.703540325164795
Epoch 260, training loss: 321.0860290527344 = 1.66710364818573 + 50.0 * 6.388378143310547
Epoch 260, val loss: 1.690750241279602
Epoch 270, training loss: 320.76104736328125 = 1.6501054763793945 + 50.0 * 6.382218837738037
Epoch 270, val loss: 1.6771799325942993
Epoch 280, training loss: 320.4382019042969 = 1.6321227550506592 + 50.0 * 6.376121997833252
Epoch 280, val loss: 1.662901520729065
Epoch 290, training loss: 320.3168029785156 = 1.6130305528640747 + 50.0 * 6.374075412750244
Epoch 290, val loss: 1.6478149890899658
Epoch 300, training loss: 319.847412109375 = 1.5930454730987549 + 50.0 * 6.365087032318115
Epoch 300, val loss: 1.6320005655288696
Epoch 310, training loss: 319.5750427246094 = 1.572219967842102 + 50.0 * 6.360055923461914
Epoch 310, val loss: 1.615659475326538
Epoch 320, training loss: 319.50146484375 = 1.5506469011306763 + 50.0 * 6.359016418457031
Epoch 320, val loss: 1.598854899406433
Epoch 330, training loss: 319.1400146484375 = 1.5281609296798706 + 50.0 * 6.352236747741699
Epoch 330, val loss: 1.5816305875778198
Epoch 340, training loss: 318.7721252441406 = 1.505312442779541 + 50.0 * 6.345336437225342
Epoch 340, val loss: 1.5641491413116455
Epoch 350, training loss: 318.689453125 = 1.4820518493652344 + 50.0 * 6.3441481590271
Epoch 350, val loss: 1.5465677976608276
Epoch 360, training loss: 318.3222961425781 = 1.4584342241287231 + 50.0 * 6.337277412414551
Epoch 360, val loss: 1.5290898084640503
Epoch 370, training loss: 318.133056640625 = 1.434566855430603 + 50.0 * 6.333969593048096
Epoch 370, val loss: 1.5115057229995728
Epoch 380, training loss: 317.9779968261719 = 1.4106045961380005 + 50.0 * 6.331347465515137
Epoch 380, val loss: 1.4940372705459595
Epoch 390, training loss: 317.7255554199219 = 1.3865915536880493 + 50.0 * 6.326778888702393
Epoch 390, val loss: 1.4765727519989014
Epoch 400, training loss: 317.4957275390625 = 1.3626182079315186 + 50.0 * 6.322662353515625
Epoch 400, val loss: 1.459652304649353
Epoch 410, training loss: 317.45733642578125 = 1.3387771844863892 + 50.0 * 6.322371006011963
Epoch 410, val loss: 1.4429471492767334
Epoch 420, training loss: 317.1361999511719 = 1.3149768114089966 + 50.0 * 6.31642484664917
Epoch 420, val loss: 1.4265090227127075
Epoch 430, training loss: 316.9700012207031 = 1.2915072441101074 + 50.0 * 6.313570022583008
Epoch 430, val loss: 1.410343885421753
Epoch 440, training loss: 316.8528137207031 = 1.2683194875717163 + 50.0 * 6.311689853668213
Epoch 440, val loss: 1.3946964740753174
Epoch 450, training loss: 316.7511291503906 = 1.2453129291534424 + 50.0 * 6.310116767883301
Epoch 450, val loss: 1.3798408508300781
Epoch 460, training loss: 316.5728759765625 = 1.2226108312606812 + 50.0 * 6.307004928588867
Epoch 460, val loss: 1.364823818206787
Epoch 470, training loss: 316.383544921875 = 1.2003389596939087 + 50.0 * 6.303663730621338
Epoch 470, val loss: 1.350750207901001
Epoch 480, training loss: 316.4174499511719 = 1.1784522533416748 + 50.0 * 6.304779529571533
Epoch 480, val loss: 1.3371607065200806
Epoch 490, training loss: 316.27105712890625 = 1.1565403938293457 + 50.0 * 6.302289962768555
Epoch 490, val loss: 1.323639154434204
Epoch 500, training loss: 316.1163024902344 = 1.135219693183899 + 50.0 * 6.29962158203125
Epoch 500, val loss: 1.3105665445327759
Epoch 510, training loss: 315.91888427734375 = 1.1142592430114746 + 50.0 * 6.296092510223389
Epoch 510, val loss: 1.2983883619308472
Epoch 520, training loss: 315.8865051269531 = 1.0936795473098755 + 50.0 * 6.295856475830078
Epoch 520, val loss: 1.28638756275177
Epoch 530, training loss: 315.79876708984375 = 1.073376178741455 + 50.0 * 6.29450798034668
Epoch 530, val loss: 1.2746692895889282
Epoch 540, training loss: 315.6397705078125 = 1.0534427165985107 + 50.0 * 6.291726589202881
Epoch 540, val loss: 1.2636334896087646
Epoch 550, training loss: 315.53045654296875 = 1.0339767932891846 + 50.0 * 6.2899298667907715
Epoch 550, val loss: 1.2532117366790771
Epoch 560, training loss: 315.5054626464844 = 1.0148237943649292 + 50.0 * 6.2898125648498535
Epoch 560, val loss: 1.2427563667297363
Epoch 570, training loss: 315.3534851074219 = 0.9960295557975769 + 50.0 * 6.287148952484131
Epoch 570, val loss: 1.2331827878952026
Epoch 580, training loss: 315.3270263671875 = 0.9775949120521545 + 50.0 * 6.286988735198975
Epoch 580, val loss: 1.2238184213638306
Epoch 590, training loss: 315.1480712890625 = 0.9596129059791565 + 50.0 * 6.283768653869629
Epoch 590, val loss: 1.2149821519851685
Epoch 600, training loss: 315.0315246582031 = 0.9419829249382019 + 50.0 * 6.281790733337402
Epoch 600, val loss: 1.2065374851226807
Epoch 610, training loss: 315.4958801269531 = 0.924626886844635 + 50.0 * 6.2914252281188965
Epoch 610, val loss: 1.1983617544174194
Epoch 620, training loss: 314.8968200683594 = 0.9076368808746338 + 50.0 * 6.279784202575684
Epoch 620, val loss: 1.1903324127197266
Epoch 630, training loss: 314.8128356933594 = 0.8910062909126282 + 50.0 * 6.27843713760376
Epoch 630, val loss: 1.1831525564193726
Epoch 640, training loss: 314.6825866699219 = 0.8748411536216736 + 50.0 * 6.276154518127441
Epoch 640, val loss: 1.1764053106307983
Epoch 650, training loss: 314.8459167480469 = 0.8590930700302124 + 50.0 * 6.279736518859863
Epoch 650, val loss: 1.1699810028076172
Epoch 660, training loss: 314.7362365722656 = 0.8433927297592163 + 50.0 * 6.277857303619385
Epoch 660, val loss: 1.1634002923965454
Epoch 670, training loss: 314.5077209472656 = 0.8281421065330505 + 50.0 * 6.2735915184021
Epoch 670, val loss: 1.1574163436889648
Epoch 680, training loss: 314.376953125 = 0.8133681416511536 + 50.0 * 6.2712721824646
Epoch 680, val loss: 1.1519745588302612
Epoch 690, training loss: 314.7231140136719 = 0.7988560199737549 + 50.0 * 6.278485298156738
Epoch 690, val loss: 1.1468781232833862
Epoch 700, training loss: 314.2425537109375 = 0.7845589518547058 + 50.0 * 6.269160270690918
Epoch 700, val loss: 1.1421353816986084
Epoch 710, training loss: 314.1913146972656 = 0.7706059813499451 + 50.0 * 6.26841402053833
Epoch 710, val loss: 1.1375102996826172
Epoch 720, training loss: 314.1181335449219 = 0.7570492029190063 + 50.0 * 6.267221927642822
Epoch 720, val loss: 1.1334010362625122
Epoch 730, training loss: 314.2705383300781 = 0.7436982989311218 + 50.0 * 6.270536422729492
Epoch 730, val loss: 1.1293376684188843
Epoch 740, training loss: 314.0967102050781 = 0.7304516434669495 + 50.0 * 6.267325401306152
Epoch 740, val loss: 1.1254384517669678
Epoch 750, training loss: 314.009521484375 = 0.7176164388656616 + 50.0 * 6.265838146209717
Epoch 750, val loss: 1.1228687763214111
Epoch 760, training loss: 313.86004638671875 = 0.7050132155418396 + 50.0 * 6.263100624084473
Epoch 760, val loss: 1.1195036172866821
Epoch 770, training loss: 313.8013610839844 = 0.6926904916763306 + 50.0 * 6.262173175811768
Epoch 770, val loss: 1.1166478395462036
Epoch 780, training loss: 313.865478515625 = 0.68067467212677 + 50.0 * 6.263696193695068
Epoch 780, val loss: 1.1144678592681885
Epoch 790, training loss: 313.7866516113281 = 0.6686203479766846 + 50.0 * 6.262360572814941
Epoch 790, val loss: 1.1116540431976318
Epoch 800, training loss: 313.6688537597656 = 0.6568644046783447 + 50.0 * 6.260240077972412
Epoch 800, val loss: 1.109578013420105
Epoch 810, training loss: 313.58648681640625 = 0.6453980803489685 + 50.0 * 6.258821964263916
Epoch 810, val loss: 1.1077427864074707
Epoch 820, training loss: 313.513427734375 = 0.6341978907585144 + 50.0 * 6.257584571838379
Epoch 820, val loss: 1.1060549020767212
Epoch 830, training loss: 313.6676940917969 = 0.6231061220169067 + 50.0 * 6.260891437530518
Epoch 830, val loss: 1.1043838262557983
Epoch 840, training loss: 313.61639404296875 = 0.6121318936347961 + 50.0 * 6.260085582733154
Epoch 840, val loss: 1.1029924154281616
Epoch 850, training loss: 313.43658447265625 = 0.6012332439422607 + 50.0 * 6.256707191467285
Epoch 850, val loss: 1.1016583442687988
Epoch 860, training loss: 313.30950927734375 = 0.5905966758728027 + 50.0 * 6.254378318786621
Epoch 860, val loss: 1.1004102230072021
Epoch 870, training loss: 313.258544921875 = 0.5802451968193054 + 50.0 * 6.253566265106201
Epoch 870, val loss: 1.0997225046157837
Epoch 880, training loss: 313.4183044433594 = 0.5701056122779846 + 50.0 * 6.256964206695557
Epoch 880, val loss: 1.0992474555969238
Epoch 890, training loss: 313.5001220703125 = 0.5598611235618591 + 50.0 * 6.258804798126221
Epoch 890, val loss: 1.0982927083969116
Epoch 900, training loss: 313.150390625 = 0.5497727394104004 + 50.0 * 6.252012252807617
Epoch 900, val loss: 1.0977654457092285
Epoch 910, training loss: 313.0865478515625 = 0.5399420857429504 + 50.0 * 6.250932216644287
Epoch 910, val loss: 1.097321629524231
Epoch 920, training loss: 313.0175476074219 = 0.5303208827972412 + 50.0 * 6.249744415283203
Epoch 920, val loss: 1.0972316265106201
Epoch 930, training loss: 313.1968688964844 = 0.520816445350647 + 50.0 * 6.253520965576172
Epoch 930, val loss: 1.0972179174423218
Epoch 940, training loss: 313.1423645019531 = 0.5112954378128052 + 50.0 * 6.252621650695801
Epoch 940, val loss: 1.0962930917739868
Epoch 950, training loss: 312.9519958496094 = 0.5019844174385071 + 50.0 * 6.249000072479248
Epoch 950, val loss: 1.0968290567398071
Epoch 960, training loss: 312.87689208984375 = 0.4928135275840759 + 50.0 * 6.247681140899658
Epoch 960, val loss: 1.0967222452163696
Epoch 970, training loss: 313.19970703125 = 0.4837198555469513 + 50.0 * 6.254319190979004
Epoch 970, val loss: 1.0967189073562622
Epoch 980, training loss: 312.9356689453125 = 0.47471022605895996 + 50.0 * 6.2492194175720215
Epoch 980, val loss: 1.0969598293304443
Epoch 990, training loss: 312.7217102050781 = 0.46578124165534973 + 50.0 * 6.245118618011475
Epoch 990, val loss: 1.0970600843429565
Epoch 1000, training loss: 312.7030944824219 = 0.457083523273468 + 50.0 * 6.244920253753662
Epoch 1000, val loss: 1.0972801446914673
Epoch 1010, training loss: 312.6634216308594 = 0.4485674500465393 + 50.0 * 6.244297027587891
Epoch 1010, val loss: 1.0978302955627441
Epoch 1020, training loss: 312.9051513671875 = 0.44009339809417725 + 50.0 * 6.249300956726074
Epoch 1020, val loss: 1.0979647636413574
Epoch 1030, training loss: 312.68927001953125 = 0.4317166805267334 + 50.0 * 6.245151519775391
Epoch 1030, val loss: 1.0989248752593994
Epoch 1040, training loss: 312.63922119140625 = 0.4234505593776703 + 50.0 * 6.244315147399902
Epoch 1040, val loss: 1.0995393991470337
Epoch 1050, training loss: 312.573486328125 = 0.4152754545211792 + 50.0 * 6.2431640625
Epoch 1050, val loss: 1.0998313426971436
Epoch 1060, training loss: 312.6002502441406 = 0.4072383940219879 + 50.0 * 6.243860721588135
Epoch 1060, val loss: 1.1008450984954834
Epoch 1070, training loss: 312.42852783203125 = 0.3992949426174164 + 50.0 * 6.240584850311279
Epoch 1070, val loss: 1.1013785600662231
Epoch 1080, training loss: 312.4396057128906 = 0.3915421664714813 + 50.0 * 6.240961074829102
Epoch 1080, val loss: 1.1020534038543701
Epoch 1090, training loss: 312.5516052246094 = 0.38386982679367065 + 50.0 * 6.243354320526123
Epoch 1090, val loss: 1.1029235124588013
Epoch 1100, training loss: 312.45318603515625 = 0.37627339363098145 + 50.0 * 6.241538047790527
Epoch 1100, val loss: 1.1040338277816772
Epoch 1110, training loss: 312.37799072265625 = 0.36877816915512085 + 50.0 * 6.240184307098389
Epoch 1110, val loss: 1.1046477556228638
Epoch 1120, training loss: 312.2837219238281 = 0.3615039587020874 + 50.0 * 6.2384443283081055
Epoch 1120, val loss: 1.105890154838562
Epoch 1130, training loss: 312.2703857421875 = 0.35435250401496887 + 50.0 * 6.238320827484131
Epoch 1130, val loss: 1.1069066524505615
Epoch 1140, training loss: 312.4142150878906 = 0.3472953140735626 + 50.0 * 6.24133825302124
Epoch 1140, val loss: 1.1079916954040527
Epoch 1150, training loss: 312.3170166015625 = 0.34022048115730286 + 50.0 * 6.239536285400391
Epoch 1150, val loss: 1.1096779108047485
Epoch 1160, training loss: 312.226318359375 = 0.33340293169021606 + 50.0 * 6.237858295440674
Epoch 1160, val loss: 1.1109358072280884
Epoch 1170, training loss: 312.3328552246094 = 0.326614111661911 + 50.0 * 6.2401251792907715
Epoch 1170, val loss: 1.1123560667037964
Epoch 1180, training loss: 312.2290954589844 = 0.31994494795799255 + 50.0 * 6.23818302154541
Epoch 1180, val loss: 1.1135985851287842
Epoch 1190, training loss: 312.08062744140625 = 0.31348204612731934 + 50.0 * 6.235342979431152
Epoch 1190, val loss: 1.1152666807174683
Epoch 1200, training loss: 312.0287780761719 = 0.307122141122818 + 50.0 * 6.234432697296143
Epoch 1200, val loss: 1.117098093032837
Epoch 1210, training loss: 312.0421142578125 = 0.30095210671424866 + 50.0 * 6.234822750091553
Epoch 1210, val loss: 1.118840217590332
Epoch 1220, training loss: 312.1199645996094 = 0.2948315739631653 + 50.0 * 6.236502647399902
Epoch 1220, val loss: 1.120888352394104
Epoch 1230, training loss: 311.9963684082031 = 0.288722962141037 + 50.0 * 6.234152793884277
Epoch 1230, val loss: 1.1225531101226807
Epoch 1240, training loss: 312.28314208984375 = 0.28287580609321594 + 50.0 * 6.2400054931640625
Epoch 1240, val loss: 1.124672770500183
Epoch 1250, training loss: 312.0028991699219 = 0.27687570452690125 + 50.0 * 6.234520435333252
Epoch 1250, val loss: 1.1264805793762207
Epoch 1260, training loss: 311.9014587402344 = 0.27118027210235596 + 50.0 * 6.232605457305908
Epoch 1260, val loss: 1.128610372543335
Epoch 1270, training loss: 311.8707580566406 = 0.26561689376831055 + 50.0 * 6.232102870941162
Epoch 1270, val loss: 1.1309622526168823
Epoch 1280, training loss: 311.9165344238281 = 0.2601841390132904 + 50.0 * 6.233127117156982
Epoch 1280, val loss: 1.1334160566329956
Epoch 1290, training loss: 311.80963134765625 = 0.25480759143829346 + 50.0 * 6.231096267700195
Epoch 1290, val loss: 1.135696291923523
Epoch 1300, training loss: 311.9735107421875 = 0.24956078827381134 + 50.0 * 6.234478950500488
Epoch 1300, val loss: 1.1384305953979492
Epoch 1310, training loss: 311.9481506347656 = 0.2443389594554901 + 50.0 * 6.234076499938965
Epoch 1310, val loss: 1.1407787799835205
Epoch 1320, training loss: 311.7531433105469 = 0.2392706722021103 + 50.0 * 6.2302775382995605
Epoch 1320, val loss: 1.1434283256530762
Epoch 1330, training loss: 311.6797790527344 = 0.2343439757823944 + 50.0 * 6.228908538818359
Epoch 1330, val loss: 1.146383285522461
Epoch 1340, training loss: 311.70574951171875 = 0.22956693172454834 + 50.0 * 6.229523658752441
Epoch 1340, val loss: 1.1495301723480225
Epoch 1350, training loss: 311.83209228515625 = 0.22485379874706268 + 50.0 * 6.232144832611084
Epoch 1350, val loss: 1.1522184610366821
Epoch 1360, training loss: 311.671875 = 0.22003942728042603 + 50.0 * 6.229036808013916
Epoch 1360, val loss: 1.1549550294876099
Epoch 1370, training loss: 311.6363830566406 = 0.2154453545808792 + 50.0 * 6.228418827056885
Epoch 1370, val loss: 1.1575829982757568
Epoch 1380, training loss: 311.6021728515625 = 0.21100923418998718 + 50.0 * 6.227823257446289
Epoch 1380, val loss: 1.1611430644989014
Epoch 1390, training loss: 311.5697937011719 = 0.2066466063261032 + 50.0 * 6.227263450622559
Epoch 1390, val loss: 1.1642401218414307
Epoch 1400, training loss: 311.8025817871094 = 0.2024165242910385 + 50.0 * 6.232003211975098
Epoch 1400, val loss: 1.1674002408981323
Epoch 1410, training loss: 311.6728515625 = 0.19810351729393005 + 50.0 * 6.229495525360107
Epoch 1410, val loss: 1.1705918312072754
Epoch 1420, training loss: 311.5492858886719 = 0.1939498782157898 + 50.0 * 6.227107048034668
Epoch 1420, val loss: 1.1737556457519531
Epoch 1430, training loss: 311.5338439941406 = 0.18994593620300293 + 50.0 * 6.2268781661987305
Epoch 1430, val loss: 1.1771399974822998
Epoch 1440, training loss: 311.5771484375 = 0.1859889179468155 + 50.0 * 6.227823257446289
Epoch 1440, val loss: 1.1806244850158691
Epoch 1450, training loss: 311.4273681640625 = 0.18210817873477936 + 50.0 * 6.224905490875244
Epoch 1450, val loss: 1.1840652227401733
Epoch 1460, training loss: 311.37872314453125 = 0.17835530638694763 + 50.0 * 6.224007606506348
Epoch 1460, val loss: 1.1878160238265991
Epoch 1470, training loss: 311.37701416015625 = 0.17472189664840698 + 50.0 * 6.224045753479004
Epoch 1470, val loss: 1.1916437149047852
Epoch 1480, training loss: 311.9289245605469 = 0.17118693888187408 + 50.0 * 6.23515510559082
Epoch 1480, val loss: 1.1954103708267212
Epoch 1490, training loss: 311.4332275390625 = 0.1674591451883316 + 50.0 * 6.225315570831299
Epoch 1490, val loss: 1.1984225511550903
Epoch 1500, training loss: 311.3556213378906 = 0.1639838069677353 + 50.0 * 6.223833084106445
Epoch 1500, val loss: 1.2022831439971924
Epoch 1510, training loss: 311.5068664550781 = 0.16063103079795837 + 50.0 * 6.226924419403076
Epoch 1510, val loss: 1.2064279317855835
Epoch 1520, training loss: 311.2562255859375 = 0.15724673867225647 + 50.0 * 6.22197961807251
Epoch 1520, val loss: 1.2099716663360596
Epoch 1530, training loss: 311.2896728515625 = 0.15401847660541534 + 50.0 * 6.222712993621826
Epoch 1530, val loss: 1.214116096496582
Epoch 1540, training loss: 311.4756774902344 = 0.15088261663913727 + 50.0 * 6.22649621963501
Epoch 1540, val loss: 1.2185465097427368
Epoch 1550, training loss: 311.25592041015625 = 0.1477370411157608 + 50.0 * 6.222163677215576
Epoch 1550, val loss: 1.2214508056640625
Epoch 1560, training loss: 311.1938781738281 = 0.14466609060764313 + 50.0 * 6.22098445892334
Epoch 1560, val loss: 1.2258988618850708
Epoch 1570, training loss: 311.1791076660156 = 0.1417258381843567 + 50.0 * 6.220747947692871
Epoch 1570, val loss: 1.2296698093414307
Epoch 1580, training loss: 311.4334411621094 = 0.1388406753540039 + 50.0 * 6.225891590118408
Epoch 1580, val loss: 1.233775019645691
Epoch 1590, training loss: 311.31048583984375 = 0.1359717696905136 + 50.0 * 6.223489761352539
Epoch 1590, val loss: 1.238047480583191
Epoch 1600, training loss: 311.1725158691406 = 0.13312062621116638 + 50.0 * 6.22078800201416
Epoch 1600, val loss: 1.2419559955596924
Epoch 1610, training loss: 311.2248229980469 = 0.1304207295179367 + 50.0 * 6.221888065338135
Epoch 1610, val loss: 1.2463715076446533
Epoch 1620, training loss: 311.2269287109375 = 0.1277569681406021 + 50.0 * 6.221983909606934
Epoch 1620, val loss: 1.2506067752838135
Epoch 1630, training loss: 311.2687072753906 = 0.12515220046043396 + 50.0 * 6.222870826721191
Epoch 1630, val loss: 1.2549245357513428
Epoch 1640, training loss: 311.15362548828125 = 0.12252458930015564 + 50.0 * 6.2206220626831055
Epoch 1640, val loss: 1.259063482284546
Epoch 1650, training loss: 311.1117858886719 = 0.12006860226392746 + 50.0 * 6.219834804534912
Epoch 1650, val loss: 1.2635104656219482
Epoch 1660, training loss: 311.11065673828125 = 0.1176271140575409 + 50.0 * 6.219860553741455
Epoch 1660, val loss: 1.268027663230896
Epoch 1670, training loss: 311.11566162109375 = 0.11526099592447281 + 50.0 * 6.22000789642334
Epoch 1670, val loss: 1.2723989486694336
Epoch 1680, training loss: 311.0487060546875 = 0.11288318783044815 + 50.0 * 6.218716621398926
Epoch 1680, val loss: 1.2763185501098633
Epoch 1690, training loss: 310.9739990234375 = 0.11062799394130707 + 50.0 * 6.2172675132751465
Epoch 1690, val loss: 1.281189203262329
Epoch 1700, training loss: 311.0667724609375 = 0.10842658579349518 + 50.0 * 6.2191667556762695
Epoch 1700, val loss: 1.2858000993728638
Epoch 1710, training loss: 311.0198974609375 = 0.10618637502193451 + 50.0 * 6.218273639678955
Epoch 1710, val loss: 1.2898224592208862
Epoch 1720, training loss: 310.9803161621094 = 0.10401178151369095 + 50.0 * 6.217526435852051
Epoch 1720, val loss: 1.294328212738037
Epoch 1730, training loss: 311.0858459472656 = 0.10189712792634964 + 50.0 * 6.21967887878418
Epoch 1730, val loss: 1.2987360954284668
Epoch 1740, training loss: 310.99176025390625 = 0.09984716773033142 + 50.0 * 6.217838287353516
Epoch 1740, val loss: 1.3033387660980225
Epoch 1750, training loss: 310.920654296875 = 0.09784622490406036 + 50.0 * 6.216456413269043
Epoch 1750, val loss: 1.308021903038025
Epoch 1760, training loss: 310.9060363769531 = 0.09590736776590347 + 50.0 * 6.216202259063721
Epoch 1760, val loss: 1.3127424716949463
Epoch 1770, training loss: 310.9826965332031 = 0.09401234984397888 + 50.0 * 6.2177734375
Epoch 1770, val loss: 1.3177666664123535
Epoch 1780, training loss: 310.87158203125 = 0.09211684763431549 + 50.0 * 6.21558952331543
Epoch 1780, val loss: 1.3220126628875732
Epoch 1790, training loss: 310.9996032714844 = 0.09027847647666931 + 50.0 * 6.218186378479004
Epoch 1790, val loss: 1.3264241218566895
Epoch 1800, training loss: 310.927001953125 = 0.08847934752702713 + 50.0 * 6.216770648956299
Epoch 1800, val loss: 1.3313850164413452
Epoch 1810, training loss: 310.8274230957031 = 0.08670695126056671 + 50.0 * 6.21481466293335
Epoch 1810, val loss: 1.3358826637268066
Epoch 1820, training loss: 310.86334228515625 = 0.08499322831630707 + 50.0 * 6.215567111968994
Epoch 1820, val loss: 1.340782880783081
Epoch 1830, training loss: 310.9422607421875 = 0.08333020657300949 + 50.0 * 6.2171783447265625
Epoch 1830, val loss: 1.3454840183258057
Epoch 1840, training loss: 310.8845520019531 = 0.08169969171285629 + 50.0 * 6.216056823730469
Epoch 1840, val loss: 1.3500852584838867
Epoch 1850, training loss: 310.8499450683594 = 0.08008047193288803 + 50.0 * 6.215397357940674
Epoch 1850, val loss: 1.3550769090652466
Epoch 1860, training loss: 310.7484436035156 = 0.07852711528539658 + 50.0 * 6.213398456573486
Epoch 1860, val loss: 1.3598757982254028
Epoch 1870, training loss: 310.7162170410156 = 0.07702243328094482 + 50.0 * 6.2127838134765625
Epoch 1870, val loss: 1.3649274110794067
Epoch 1880, training loss: 310.823486328125 = 0.07555801421403885 + 50.0 * 6.214958190917969
Epoch 1880, val loss: 1.369649887084961
Epoch 1890, training loss: 310.80010986328125 = 0.07408247143030167 + 50.0 * 6.21451997756958
Epoch 1890, val loss: 1.374282956123352
Epoch 1900, training loss: 310.8065185546875 = 0.07261931896209717 + 50.0 * 6.214677810668945
Epoch 1900, val loss: 1.379063606262207
Epoch 1910, training loss: 310.6687316894531 = 0.07120975106954575 + 50.0 * 6.211950778961182
Epoch 1910, val loss: 1.3840118646621704
Epoch 1920, training loss: 310.68487548828125 = 0.06985973566770554 + 50.0 * 6.2123003005981445
Epoch 1920, val loss: 1.3890979290008545
Epoch 1930, training loss: 310.9109802246094 = 0.0685337707400322 + 50.0 * 6.216848850250244
Epoch 1930, val loss: 1.393804907798767
Epoch 1940, training loss: 310.74188232421875 = 0.06721276789903641 + 50.0 * 6.213493347167969
Epoch 1940, val loss: 1.3986945152282715
Epoch 1950, training loss: 310.64404296875 = 0.0659109503030777 + 50.0 * 6.211562633514404
Epoch 1950, val loss: 1.4034427404403687
Epoch 1960, training loss: 310.5924072265625 = 0.06467181444168091 + 50.0 * 6.210555076599121
Epoch 1960, val loss: 1.4085606336593628
Epoch 1970, training loss: 310.5898742675781 = 0.06348507106304169 + 50.0 * 6.2105278968811035
Epoch 1970, val loss: 1.4135903120040894
Epoch 1980, training loss: 310.74346923828125 = 0.06232166290283203 + 50.0 * 6.213623046875
Epoch 1980, val loss: 1.4184118509292603
Epoch 1990, training loss: 310.6329650878906 = 0.061157163232564926 + 50.0 * 6.2114362716674805
Epoch 1990, val loss: 1.4231340885162354
Epoch 2000, training loss: 310.73712158203125 = 0.060001909732818604 + 50.0 * 6.213542461395264
Epoch 2000, val loss: 1.4275944232940674
Epoch 2010, training loss: 310.70758056640625 = 0.058871205896139145 + 50.0 * 6.2129740715026855
Epoch 2010, val loss: 1.4331228733062744
Epoch 2020, training loss: 310.5685729980469 = 0.057760484516620636 + 50.0 * 6.210216045379639
Epoch 2020, val loss: 1.4378297328948975
Epoch 2030, training loss: 310.5184326171875 = 0.0567198246717453 + 50.0 * 6.20923376083374
Epoch 2030, val loss: 1.4430222511291504
Epoch 2040, training loss: 310.52679443359375 = 0.0557054840028286 + 50.0 * 6.2094221115112305
Epoch 2040, val loss: 1.447973370552063
Epoch 2050, training loss: 310.81903076171875 = 0.05472562462091446 + 50.0 * 6.2152862548828125
Epoch 2050, val loss: 1.4530208110809326
Epoch 2060, training loss: 310.6558532714844 = 0.05369529500603676 + 50.0 * 6.212043762207031
Epoch 2060, val loss: 1.4579838514328003
Epoch 2070, training loss: 310.5738220214844 = 0.05272430554032326 + 50.0 * 6.210421562194824
Epoch 2070, val loss: 1.462250828742981
Epoch 2080, training loss: 310.5502014160156 = 0.05179067328572273 + 50.0 * 6.209968090057373
Epoch 2080, val loss: 1.4675674438476562
Epoch 2090, training loss: 310.656005859375 = 0.05088694766163826 + 50.0 * 6.21210241317749
Epoch 2090, val loss: 1.47246253490448
Epoch 2100, training loss: 310.5014953613281 = 0.04995960369706154 + 50.0 * 6.209030628204346
Epoch 2100, val loss: 1.4772673845291138
Epoch 2110, training loss: 310.46356201171875 = 0.049088962376117706 + 50.0 * 6.20828914642334
Epoch 2110, val loss: 1.4822760820388794
Epoch 2120, training loss: 310.5052795410156 = 0.048250555992126465 + 50.0 * 6.209140777587891
Epoch 2120, val loss: 1.487478256225586
Epoch 2130, training loss: 310.6842041015625 = 0.04740122705698013 + 50.0 * 6.212736129760742
Epoch 2130, val loss: 1.4920934438705444
Epoch 2140, training loss: 310.55401611328125 = 0.04655265063047409 + 50.0 * 6.210149765014648
Epoch 2140, val loss: 1.49660325050354
Epoch 2150, training loss: 310.460205078125 = 0.045752547681331635 + 50.0 * 6.20828914642334
Epoch 2150, val loss: 1.5015381574630737
Epoch 2160, training loss: 310.5179138183594 = 0.04497060552239418 + 50.0 * 6.209458827972412
Epoch 2160, val loss: 1.5062376260757446
Epoch 2170, training loss: 310.5344543457031 = 0.04420684650540352 + 50.0 * 6.209804534912109
Epoch 2170, val loss: 1.510953426361084
Epoch 2180, training loss: 310.4386901855469 = 0.04342687875032425 + 50.0 * 6.207905292510986
Epoch 2180, val loss: 1.5161038637161255
Epoch 2190, training loss: 310.38763427734375 = 0.042709384113550186 + 50.0 * 6.206898212432861
Epoch 2190, val loss: 1.5208241939544678
Epoch 2200, training loss: 310.6534423828125 = 0.04200458526611328 + 50.0 * 6.212229251861572
Epoch 2200, val loss: 1.5254353284835815
Epoch 2210, training loss: 310.44232177734375 = 0.04127994552254677 + 50.0 * 6.2080206871032715
Epoch 2210, val loss: 1.5305771827697754
Epoch 2220, training loss: 310.4398498535156 = 0.04058891162276268 + 50.0 * 6.2079854011535645
Epoch 2220, val loss: 1.5348845720291138
Epoch 2230, training loss: 310.40777587890625 = 0.03991837799549103 + 50.0 * 6.207356929779053
Epoch 2230, val loss: 1.5400680303573608
Epoch 2240, training loss: 310.4281311035156 = 0.03925859183073044 + 50.0 * 6.207777500152588
Epoch 2240, val loss: 1.5444730520248413
Epoch 2250, training loss: 310.3719177246094 = 0.03861043602228165 + 50.0 * 6.206665992736816
Epoch 2250, val loss: 1.549032211303711
Epoch 2260, training loss: 310.3340148925781 = 0.03798569738864899 + 50.0 * 6.205920219421387
Epoch 2260, val loss: 1.5539389848709106
Epoch 2270, training loss: 310.3274841308594 = 0.03737867251038551 + 50.0 * 6.205801963806152
Epoch 2270, val loss: 1.5588701963424683
Epoch 2280, training loss: 310.4953918457031 = 0.03679415583610535 + 50.0 * 6.209171772003174
Epoch 2280, val loss: 1.563580870628357
Epoch 2290, training loss: 310.5712585449219 = 0.036197878420352936 + 50.0 * 6.2107014656066895
Epoch 2290, val loss: 1.5684326887130737
Epoch 2300, training loss: 310.33038330078125 = 0.03557049483060837 + 50.0 * 6.205895900726318
Epoch 2300, val loss: 1.5725538730621338
Epoch 2310, training loss: 310.2790222167969 = 0.035007867962121964 + 50.0 * 6.204880237579346
Epoch 2310, val loss: 1.5773283243179321
Epoch 2320, training loss: 310.2596435546875 = 0.03447197005152702 + 50.0 * 6.204503059387207
Epoch 2320, val loss: 1.5820685625076294
Epoch 2330, training loss: 310.5892333984375 = 0.03395736590027809 + 50.0 * 6.2111053466796875
Epoch 2330, val loss: 1.5872633457183838
Epoch 2340, training loss: 310.2952575683594 = 0.03338959068059921 + 50.0 * 6.20523738861084
Epoch 2340, val loss: 1.590482234954834
Epoch 2350, training loss: 310.28900146484375 = 0.032877299934625626 + 50.0 * 6.205122947692871
Epoch 2350, val loss: 1.5957996845245361
Epoch 2360, training loss: 310.3736267089844 = 0.03236744925379753 + 50.0 * 6.206824779510498
Epoch 2360, val loss: 1.5999776124954224
Epoch 2370, training loss: 310.2440490722656 = 0.03186235576868057 + 50.0 * 6.2042436599731445
Epoch 2370, val loss: 1.6047828197479248
Epoch 2380, training loss: 310.2103576660156 = 0.031378552317619324 + 50.0 * 6.203579425811768
Epoch 2380, val loss: 1.6092579364776611
Epoch 2390, training loss: 310.3966064453125 = 0.030915120616555214 + 50.0 * 6.2073140144348145
Epoch 2390, val loss: 1.6137322187423706
Epoch 2400, training loss: 310.2601013183594 = 0.030447449535131454 + 50.0 * 6.204593181610107
Epoch 2400, val loss: 1.6187001466751099
Epoch 2410, training loss: 310.2044677734375 = 0.029978496953845024 + 50.0 * 6.203489780426025
Epoch 2410, val loss: 1.6226314306259155
Epoch 2420, training loss: 310.1929626464844 = 0.029542697593569756 + 50.0 * 6.203268527984619
Epoch 2420, val loss: 1.6274536848068237
Epoch 2430, training loss: 310.3846435546875 = 0.02912323549389839 + 50.0 * 6.207110404968262
Epoch 2430, val loss: 1.6319921016693115
Epoch 2440, training loss: 310.2547607421875 = 0.028682071715593338 + 50.0 * 6.204521179199219
Epoch 2440, val loss: 1.6359798908233643
Epoch 2450, training loss: 310.1399230957031 = 0.02824699506163597 + 50.0 * 6.20223331451416
Epoch 2450, val loss: 1.6403534412384033
Epoch 2460, training loss: 310.1245422363281 = 0.027842124924063683 + 50.0 * 6.201934337615967
Epoch 2460, val loss: 1.6448725461959839
Epoch 2470, training loss: 310.2133483886719 = 0.027463411912322044 + 50.0 * 6.2037177085876465
Epoch 2470, val loss: 1.6490967273712158
Epoch 2480, training loss: 310.2218933105469 = 0.027065986767411232 + 50.0 * 6.203896522521973
Epoch 2480, val loss: 1.6533604860305786
Epoch 2490, training loss: 310.1780090332031 = 0.02666955254971981 + 50.0 * 6.20302677154541
Epoch 2490, val loss: 1.6578953266143799
Epoch 2500, training loss: 310.30902099609375 = 0.02630159817636013 + 50.0 * 6.205654144287109
Epoch 2500, val loss: 1.6620804071426392
Epoch 2510, training loss: 310.14471435546875 = 0.02591679058969021 + 50.0 * 6.202376365661621
Epoch 2510, val loss: 1.6662043333053589
Epoch 2520, training loss: 310.1189270019531 = 0.025553788989782333 + 50.0 * 6.201867580413818
Epoch 2520, val loss: 1.67061448097229
Epoch 2530, training loss: 310.2477722167969 = 0.02520129270851612 + 50.0 * 6.204451084136963
Epoch 2530, val loss: 1.6746057271957397
Epoch 2540, training loss: 310.12701416015625 = 0.02485671453177929 + 50.0 * 6.202043056488037
Epoch 2540, val loss: 1.6791675090789795
Epoch 2550, training loss: 310.42657470703125 = 0.024534527212381363 + 50.0 * 6.208040714263916
Epoch 2550, val loss: 1.6834226846694946
Epoch 2560, training loss: 310.1375427246094 = 0.02415112964808941 + 50.0 * 6.202268123626709
Epoch 2560, val loss: 1.6870280504226685
Epoch 2570, training loss: 310.06329345703125 = 0.023823494091629982 + 50.0 * 6.200788974761963
Epoch 2570, val loss: 1.6914994716644287
Epoch 2580, training loss: 310.0347595214844 = 0.02350890077650547 + 50.0 * 6.200225353240967
Epoch 2580, val loss: 1.6956133842468262
Epoch 2590, training loss: 310.0666198730469 = 0.02320701815187931 + 50.0 * 6.200868606567383
Epoch 2590, val loss: 1.6998236179351807
Epoch 2600, training loss: 310.3089904785156 = 0.022905901074409485 + 50.0 * 6.205721378326416
Epoch 2600, val loss: 1.7038285732269287
Epoch 2610, training loss: 310.1929626464844 = 0.02258489839732647 + 50.0 * 6.2034077644348145
Epoch 2610, val loss: 1.7076270580291748
Epoch 2620, training loss: 310.07733154296875 = 0.02227897383272648 + 50.0 * 6.201101303100586
Epoch 2620, val loss: 1.7115780115127563
Epoch 2630, training loss: 310.045654296875 = 0.02199479565024376 + 50.0 * 6.200472831726074
Epoch 2630, val loss: 1.7161284685134888
Epoch 2640, training loss: 310.1706848144531 = 0.02171691320836544 + 50.0 * 6.20297908782959
Epoch 2640, val loss: 1.7198597192764282
Epoch 2650, training loss: 310.19317626953125 = 0.021420689299702644 + 50.0 * 6.203434944152832
Epoch 2650, val loss: 1.7239081859588623
Epoch 2660, training loss: 310.0179443359375 = 0.02113163471221924 + 50.0 * 6.199936389923096
Epoch 2660, val loss: 1.7280646562576294
Epoch 2670, training loss: 309.97784423828125 = 0.020861037075519562 + 50.0 * 6.19913911819458
Epoch 2670, val loss: 1.7318683862686157
Epoch 2680, training loss: 310.00604248046875 = 0.020603930577635765 + 50.0 * 6.199708938598633
Epoch 2680, val loss: 1.7357056140899658
Epoch 2690, training loss: 310.1217346191406 = 0.020348431542515755 + 50.0 * 6.202027320861816
Epoch 2690, val loss: 1.7396225929260254
Epoch 2700, training loss: 309.9772644042969 = 0.020089197903871536 + 50.0 * 6.199143409729004
Epoch 2700, val loss: 1.744095802307129
Epoch 2710, training loss: 310.13385009765625 = 0.01984488219022751 + 50.0 * 6.202280521392822
Epoch 2710, val loss: 1.748015284538269
Epoch 2720, training loss: 310.0530700683594 = 0.019587557762861252 + 50.0 * 6.200669765472412
Epoch 2720, val loss: 1.7512454986572266
Epoch 2730, training loss: 309.9945068359375 = 0.01933898776769638 + 50.0 * 6.199502944946289
Epoch 2730, val loss: 1.7551442384719849
Epoch 2740, training loss: 310.10467529296875 = 0.019100403413176537 + 50.0 * 6.201711177825928
Epoch 2740, val loss: 1.7587738037109375
Epoch 2750, training loss: 309.9712829589844 = 0.01886364072561264 + 50.0 * 6.1990485191345215
Epoch 2750, val loss: 1.7627307176589966
Epoch 2760, training loss: 309.93365478515625 = 0.018635733053088188 + 50.0 * 6.198299884796143
Epoch 2760, val loss: 1.7668057680130005
Epoch 2770, training loss: 309.93939208984375 = 0.01841665245592594 + 50.0 * 6.198419570922852
Epoch 2770, val loss: 1.7705392837524414
Epoch 2780, training loss: 310.1249694824219 = 0.018198993057012558 + 50.0 * 6.2021355628967285
Epoch 2780, val loss: 1.774059534072876
Epoch 2790, training loss: 310.0601806640625 = 0.017976535484194756 + 50.0 * 6.2008442878723145
Epoch 2790, val loss: 1.7774908542633057
Epoch 2800, training loss: 309.9659118652344 = 0.01776033453643322 + 50.0 * 6.198963165283203
Epoch 2800, val loss: 1.7815359830856323
Epoch 2810, training loss: 309.92974853515625 = 0.017551735043525696 + 50.0 * 6.198244094848633
Epoch 2810, val loss: 1.7851709127426147
Epoch 2820, training loss: 310.1526794433594 = 0.017354751005768776 + 50.0 * 6.202706336975098
Epoch 2820, val loss: 1.7890652418136597
Epoch 2830, training loss: 309.9443664550781 = 0.017127105966210365 + 50.0 * 6.198544979095459
Epoch 2830, val loss: 1.7920513153076172
Epoch 2840, training loss: 309.87860107421875 = 0.016922082751989365 + 50.0 * 6.197233200073242
Epoch 2840, val loss: 1.7959058284759521
Epoch 2850, training loss: 309.8713073730469 = 0.016733674332499504 + 50.0 * 6.197091579437256
Epoch 2850, val loss: 1.7995169162750244
Epoch 2860, training loss: 309.99896240234375 = 0.01654987595975399 + 50.0 * 6.199648380279541
Epoch 2860, val loss: 1.8031296730041504
Epoch 2870, training loss: 309.9010314941406 = 0.016355318948626518 + 50.0 * 6.197693347930908
Epoch 2870, val loss: 1.8064875602722168
Epoch 2880, training loss: 309.9154357910156 = 0.016168277710676193 + 50.0 * 6.1979851722717285
Epoch 2880, val loss: 1.8099477291107178
Epoch 2890, training loss: 309.8874206542969 = 0.015982061624526978 + 50.0 * 6.1974287033081055
Epoch 2890, val loss: 1.813303828239441
Epoch 2900, training loss: 309.9864807128906 = 0.01580621302127838 + 50.0 * 6.199413299560547
Epoch 2900, val loss: 1.816835880279541
Epoch 2910, training loss: 309.9291076660156 = 0.015619696117937565 + 50.0 * 6.198269367218018
Epoch 2910, val loss: 1.8200212717056274
Epoch 2920, training loss: 309.90252685546875 = 0.015440447255969048 + 50.0 * 6.197741508483887
Epoch 2920, val loss: 1.8231838941574097
Epoch 2930, training loss: 309.84063720703125 = 0.015271930955350399 + 50.0 * 6.196507453918457
Epoch 2930, val loss: 1.8269726037979126
Epoch 2940, training loss: 309.8714904785156 = 0.015111147426068783 + 50.0 * 6.197127819061279
Epoch 2940, val loss: 1.8302372694015503
Epoch 2950, training loss: 309.9538879394531 = 0.014949125237762928 + 50.0 * 6.1987786293029785
Epoch 2950, val loss: 1.8334299325942993
Epoch 2960, training loss: 309.9122314453125 = 0.014781082980334759 + 50.0 * 6.197949409484863
Epoch 2960, val loss: 1.8370863199234009
Epoch 2970, training loss: 309.82611083984375 = 0.014620691537857056 + 50.0 * 6.196229934692383
Epoch 2970, val loss: 1.8403871059417725
Epoch 2980, training loss: 309.8962097167969 = 0.014468315988779068 + 50.0 * 6.197634696960449
Epoch 2980, val loss: 1.8439455032348633
Epoch 2990, training loss: 309.8180847167969 = 0.014307728037238121 + 50.0 * 6.196075439453125
Epoch 2990, val loss: 1.8469053506851196
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6592592592592593
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 431.778564453125 = 1.9368573427200317 + 50.0 * 8.596834182739258
Epoch 0, val loss: 1.9335306882858276
Epoch 10, training loss: 431.7311096191406 = 1.928577184677124 + 50.0 * 8.596050262451172
Epoch 10, val loss: 1.9248309135437012
Epoch 20, training loss: 431.4374694824219 = 1.918150782585144 + 50.0 * 8.590386390686035
Epoch 20, val loss: 1.9139540195465088
Epoch 30, training loss: 429.5566101074219 = 1.9045624732971191 + 50.0 * 8.553040504455566
Epoch 30, val loss: 1.8998819589614868
Epoch 40, training loss: 420.44158935546875 = 1.888414740562439 + 50.0 * 8.371063232421875
Epoch 40, val loss: 1.8844773769378662
Epoch 50, training loss: 390.1278991699219 = 1.8721132278442383 + 50.0 * 7.765115737915039
Epoch 50, val loss: 1.869450330734253
Epoch 60, training loss: 371.1213684082031 = 1.8597925901412964 + 50.0 * 7.3852314949035645
Epoch 60, val loss: 1.8591711521148682
Epoch 70, training loss: 356.1857604980469 = 1.850396990776062 + 50.0 * 7.08670711517334
Epoch 70, val loss: 1.8499428033828735
Epoch 80, training loss: 347.9622802734375 = 1.8409359455108643 + 50.0 * 6.922426700592041
Epoch 80, val loss: 1.840582013130188
Epoch 90, training loss: 342.36468505859375 = 1.8320764303207397 + 50.0 * 6.810652256011963
Epoch 90, val loss: 1.8316892385482788
Epoch 100, training loss: 337.7035217285156 = 1.8240009546279907 + 50.0 * 6.71759033203125
Epoch 100, val loss: 1.8241978883743286
Epoch 110, training loss: 334.1487731933594 = 1.817846417427063 + 50.0 * 6.646618366241455
Epoch 110, val loss: 1.8186516761779785
Epoch 120, training loss: 331.5879211425781 = 1.8126364946365356 + 50.0 * 6.595506191253662
Epoch 120, val loss: 1.8138493299484253
Epoch 130, training loss: 329.66888427734375 = 1.8071637153625488 + 50.0 * 6.557234764099121
Epoch 130, val loss: 1.8087785243988037
Epoch 140, training loss: 327.9728088378906 = 1.801711916923523 + 50.0 * 6.5234222412109375
Epoch 140, val loss: 1.803862452507019
Epoch 150, training loss: 326.7846374511719 = 1.7965900897979736 + 50.0 * 6.499760627746582
Epoch 150, val loss: 1.7991827726364136
Epoch 160, training loss: 325.4488525390625 = 1.7911661863327026 + 50.0 * 6.473153591156006
Epoch 160, val loss: 1.7944172620773315
Epoch 170, training loss: 324.5460205078125 = 1.7854093313217163 + 50.0 * 6.455212116241455
Epoch 170, val loss: 1.7894145250320435
Epoch 180, training loss: 323.7647399902344 = 1.7792589664459229 + 50.0 * 6.439709663391113
Epoch 180, val loss: 1.784165620803833
Epoch 190, training loss: 323.2547302246094 = 1.772636890411377 + 50.0 * 6.4296417236328125
Epoch 190, val loss: 1.778597354888916
Epoch 200, training loss: 322.54364013671875 = 1.76534104347229 + 50.0 * 6.4155659675598145
Epoch 200, val loss: 1.7726128101348877
Epoch 210, training loss: 322.0356750488281 = 1.7574526071548462 + 50.0 * 6.405564785003662
Epoch 210, val loss: 1.7661783695220947
Epoch 220, training loss: 321.5835266113281 = 1.7487866878509521 + 50.0 * 6.396694660186768
Epoch 220, val loss: 1.7591545581817627
Epoch 230, training loss: 321.1685485839844 = 1.7393667697906494 + 50.0 * 6.388583660125732
Epoch 230, val loss: 1.7516177892684937
Epoch 240, training loss: 320.7124328613281 = 1.729175329208374 + 50.0 * 6.379664897918701
Epoch 240, val loss: 1.7434463500976562
Epoch 250, training loss: 320.3663330078125 = 1.717968463897705 + 50.0 * 6.37296724319458
Epoch 250, val loss: 1.7345424890518188
Epoch 260, training loss: 320.1859436035156 = 1.7057372331619263 + 50.0 * 6.369603633880615
Epoch 260, val loss: 1.7247631549835205
Epoch 270, training loss: 319.70794677734375 = 1.692400574684143 + 50.0 * 6.3603105545043945
Epoch 270, val loss: 1.7142127752304077
Epoch 280, training loss: 319.4755859375 = 1.6779471635818481 + 50.0 * 6.355952739715576
Epoch 280, val loss: 1.7028239965438843
Epoch 290, training loss: 319.25384521484375 = 1.66225004196167 + 50.0 * 6.351831912994385
Epoch 290, val loss: 1.690434455871582
Epoch 300, training loss: 318.90826416015625 = 1.6453793048858643 + 50.0 * 6.345257759094238
Epoch 300, val loss: 1.6771448850631714
Epoch 310, training loss: 318.72552490234375 = 1.627287745475769 + 50.0 * 6.3419647216796875
Epoch 310, val loss: 1.6629202365875244
Epoch 320, training loss: 318.51458740234375 = 1.607714056968689 + 50.0 * 6.338137149810791
Epoch 320, val loss: 1.647828459739685
Epoch 330, training loss: 318.24676513671875 = 1.5871615409851074 + 50.0 * 6.333192348480225
Epoch 330, val loss: 1.6317543983459473
Epoch 340, training loss: 318.030517578125 = 1.5652636289596558 + 50.0 * 6.3293046951293945
Epoch 340, val loss: 1.6149160861968994
Epoch 350, training loss: 317.8648681640625 = 1.542266845703125 + 50.0 * 6.326451778411865
Epoch 350, val loss: 1.5973002910614014
Epoch 360, training loss: 317.7802429199219 = 1.5183277130126953 + 50.0 * 6.32523775100708
Epoch 360, val loss: 1.578888177871704
Epoch 370, training loss: 317.5317077636719 = 1.4932787418365479 + 50.0 * 6.320768356323242
Epoch 370, val loss: 1.5598859786987305
Epoch 380, training loss: 317.34588623046875 = 1.4675469398498535 + 50.0 * 6.317566394805908
Epoch 380, val loss: 1.5405124425888062
Epoch 390, training loss: 317.14178466796875 = 1.4410961866378784 + 50.0 * 6.314013957977295
Epoch 390, val loss: 1.5208905935287476
Epoch 400, training loss: 317.1535949707031 = 1.414049506187439 + 50.0 * 6.314790725708008
Epoch 400, val loss: 1.5008875131607056
Epoch 410, training loss: 316.818603515625 = 1.3868454694747925 + 50.0 * 6.308635234832764
Epoch 410, val loss: 1.480854868888855
Epoch 420, training loss: 316.66925048828125 = 1.359281301498413 + 50.0 * 6.306199073791504
Epoch 420, val loss: 1.4607890844345093
Epoch 430, training loss: 316.5820617675781 = 1.33150053024292 + 50.0 * 6.30501127243042
Epoch 430, val loss: 1.440869688987732
Epoch 440, training loss: 316.45989990234375 = 1.3037168979644775 + 50.0 * 6.303123950958252
Epoch 440, val loss: 1.4209401607513428
Epoch 450, training loss: 316.23883056640625 = 1.2759166955947876 + 50.0 * 6.299258708953857
Epoch 450, val loss: 1.4013562202453613
Epoch 460, training loss: 316.2519836425781 = 1.2482874393463135 + 50.0 * 6.300074100494385
Epoch 460, val loss: 1.3821519613265991
Epoch 470, training loss: 316.0464782714844 = 1.2208094596862793 + 50.0 * 6.296513557434082
Epoch 470, val loss: 1.3633911609649658
Epoch 480, training loss: 315.8815002441406 = 1.1937575340270996 + 50.0 * 6.293754577636719
Epoch 480, val loss: 1.3447396755218506
Epoch 490, training loss: 315.77862548828125 = 1.1671757698059082 + 50.0 * 6.292228698730469
Epoch 490, val loss: 1.3269288539886475
Epoch 500, training loss: 315.671630859375 = 1.1409484148025513 + 50.0 * 6.290613174438477
Epoch 500, val loss: 1.309579610824585
Epoch 510, training loss: 315.54791259765625 = 1.1151928901672363 + 50.0 * 6.288654327392578
Epoch 510, val loss: 1.292799472808838
Epoch 520, training loss: 315.39990234375 = 1.0900352001190186 + 50.0 * 6.286197662353516
Epoch 520, val loss: 1.276631474494934
Epoch 530, training loss: 315.28729248046875 = 1.0655521154403687 + 50.0 * 6.284434795379639
Epoch 530, val loss: 1.2610652446746826
Epoch 540, training loss: 315.33648681640625 = 1.0415984392166138 + 50.0 * 6.285897731781006
Epoch 540, val loss: 1.2460534572601318
Epoch 550, training loss: 315.1319885253906 = 1.018089771270752 + 50.0 * 6.282277584075928
Epoch 550, val loss: 1.2316699028015137
Epoch 560, training loss: 314.9623718261719 = 0.995286762714386 + 50.0 * 6.279341697692871
Epoch 560, val loss: 1.217995285987854
Epoch 570, training loss: 315.2469482421875 = 0.9730159044265747 + 50.0 * 6.285478591918945
Epoch 570, val loss: 1.204816222190857
Epoch 580, training loss: 314.8056335449219 = 0.9515246748924255 + 50.0 * 6.2770819664001465
Epoch 580, val loss: 1.1921632289886475
Epoch 590, training loss: 314.68603515625 = 0.9304841756820679 + 50.0 * 6.275111198425293
Epoch 590, val loss: 1.180273413658142
Epoch 600, training loss: 314.58721923828125 = 0.9100795388221741 + 50.0 * 6.273542881011963
Epoch 600, val loss: 1.168861985206604
Epoch 610, training loss: 314.6756896972656 = 0.8902140855789185 + 50.0 * 6.27570915222168
Epoch 610, val loss: 1.1581127643585205
Epoch 620, training loss: 314.6264953613281 = 0.8709388375282288 + 50.0 * 6.275111198425293
Epoch 620, val loss: 1.14751398563385
Epoch 630, training loss: 314.3956298828125 = 0.851994514465332 + 50.0 * 6.270872592926025
Epoch 630, val loss: 1.1374289989471436
Epoch 640, training loss: 314.2659912109375 = 0.8337323665618896 + 50.0 * 6.2686448097229
Epoch 640, val loss: 1.1281784772872925
Epoch 650, training loss: 314.1805114746094 = 0.8159482479095459 + 50.0 * 6.267291069030762
Epoch 650, val loss: 1.119317889213562
Epoch 660, training loss: 314.4223327636719 = 0.7984989881515503 + 50.0 * 6.272476673126221
Epoch 660, val loss: 1.1108442544937134
Epoch 670, training loss: 314.13751220703125 = 0.7815704345703125 + 50.0 * 6.267119407653809
Epoch 670, val loss: 1.1027151346206665
Epoch 680, training loss: 313.9516906738281 = 0.7650878429412842 + 50.0 * 6.263732433319092
Epoch 680, val loss: 1.0952508449554443
Epoch 690, training loss: 313.8771667480469 = 0.7490899562835693 + 50.0 * 6.262561798095703
Epoch 690, val loss: 1.088209629058838
Epoch 700, training loss: 314.0897521972656 = 0.7334029674530029 + 50.0 * 6.26712703704834
Epoch 700, val loss: 1.0813345909118652
Epoch 710, training loss: 313.9182434082031 = 0.7181423902511597 + 50.0 * 6.264001846313477
Epoch 710, val loss: 1.0751338005065918
Epoch 720, training loss: 313.6847229003906 = 0.7030953764915466 + 50.0 * 6.259632587432861
Epoch 720, val loss: 1.0689994096755981
Epoch 730, training loss: 313.6338195800781 = 0.6885406374931335 + 50.0 * 6.25890588760376
Epoch 730, val loss: 1.0634328126907349
Epoch 740, training loss: 313.7752685546875 = 0.6742662787437439 + 50.0 * 6.262020111083984
Epoch 740, val loss: 1.0581668615341187
Epoch 750, training loss: 313.54571533203125 = 0.6602638959884644 + 50.0 * 6.257708549499512
Epoch 750, val loss: 1.053254246711731
Epoch 760, training loss: 313.53375244140625 = 0.6465829610824585 + 50.0 * 6.2577433586120605
Epoch 760, val loss: 1.0486209392547607
Epoch 770, training loss: 313.4390563964844 = 0.6332815289497375 + 50.0 * 6.256115436553955
Epoch 770, val loss: 1.0443286895751953
Epoch 780, training loss: 313.3191223144531 = 0.62020343542099 + 50.0 * 6.253978252410889
Epoch 780, val loss: 1.0403996706008911
Epoch 790, training loss: 313.321533203125 = 0.6074315309524536 + 50.0 * 6.254282474517822
Epoch 790, val loss: 1.0367215871810913
Epoch 800, training loss: 313.4052734375 = 0.5949141979217529 + 50.0 * 6.25620698928833
Epoch 800, val loss: 1.0333529710769653
Epoch 810, training loss: 313.233642578125 = 0.5824369788169861 + 50.0 * 6.253024578094482
Epoch 810, val loss: 1.030222773551941
Epoch 820, training loss: 313.1511535644531 = 0.5703662633895874 + 50.0 * 6.251615524291992
Epoch 820, val loss: 1.0272358655929565
Epoch 830, training loss: 313.2994384765625 = 0.558463454246521 + 50.0 * 6.254819393157959
Epoch 830, val loss: 1.0246071815490723
Epoch 840, training loss: 313.1097717285156 = 0.5469351410865784 + 50.0 * 6.251256465911865
Epoch 840, val loss: 1.0222395658493042
Epoch 850, training loss: 312.9872131347656 = 0.5354694128036499 + 50.0 * 6.249034881591797
Epoch 850, val loss: 1.0201126337051392
Epoch 860, training loss: 312.9200439453125 = 0.5244107842445374 + 50.0 * 6.247912406921387
Epoch 860, val loss: 1.0182784795761108
Epoch 870, training loss: 313.1243896484375 = 0.5135480761528015 + 50.0 * 6.252216815948486
Epoch 870, val loss: 1.0168237686157227
Epoch 880, training loss: 312.9294128417969 = 0.5026305913925171 + 50.0 * 6.248535633087158
Epoch 880, val loss: 1.014951229095459
Epoch 890, training loss: 312.7880859375 = 0.49213215708732605 + 50.0 * 6.245919227600098
Epoch 890, val loss: 1.0137068033218384
Epoch 900, training loss: 312.7220458984375 = 0.48182857036590576 + 50.0 * 6.2448039054870605
Epoch 900, val loss: 1.0128811597824097
Epoch 910, training loss: 312.76904296875 = 0.47180864214897156 + 50.0 * 6.245944976806641
Epoch 910, val loss: 1.012176752090454
Epoch 920, training loss: 312.66998291015625 = 0.4618085026741028 + 50.0 * 6.2441630363464355
Epoch 920, val loss: 1.0116238594055176
Epoch 930, training loss: 312.74102783203125 = 0.4520816504955292 + 50.0 * 6.245779037475586
Epoch 930, val loss: 1.0113805532455444
Epoch 940, training loss: 312.5777893066406 = 0.4426020681858063 + 50.0 * 6.242703914642334
Epoch 940, val loss: 1.0114582777023315
Epoch 950, training loss: 312.52099609375 = 0.433361679315567 + 50.0 * 6.241752624511719
Epoch 950, val loss: 1.0116633176803589
Epoch 960, training loss: 312.5383605957031 = 0.42432647943496704 + 50.0 * 6.242280960083008
Epoch 960, val loss: 1.0122426748275757
Epoch 970, training loss: 312.4528503417969 = 0.41537511348724365 + 50.0 * 6.240749359130859
Epoch 970, val loss: 1.0129820108413696
Epoch 980, training loss: 312.5661315917969 = 0.40658101439476013 + 50.0 * 6.243190765380859
Epoch 980, val loss: 1.0138015747070312
Epoch 990, training loss: 312.39447021484375 = 0.3980826735496521 + 50.0 * 6.239927768707275
Epoch 990, val loss: 1.0147243738174438
Epoch 1000, training loss: 312.39703369140625 = 0.3896578550338745 + 50.0 * 6.240147590637207
Epoch 1000, val loss: 1.01615309715271
Epoch 1010, training loss: 312.2828674316406 = 0.3814998269081116 + 50.0 * 6.238027095794678
Epoch 1010, val loss: 1.0180867910385132
Epoch 1020, training loss: 312.24566650390625 = 0.37353989481925964 + 50.0 * 6.237442493438721
Epoch 1020, val loss: 1.0199425220489502
Epoch 1030, training loss: 312.2296142578125 = 0.3657158315181732 + 50.0 * 6.237277984619141
Epoch 1030, val loss: 1.0221383571624756
Epoch 1040, training loss: 312.3733825683594 = 0.3580617308616638 + 50.0 * 6.240306377410889
Epoch 1040, val loss: 1.0244885683059692
Epoch 1050, training loss: 312.1843566894531 = 0.3505677282810211 + 50.0 * 6.23667573928833
Epoch 1050, val loss: 1.0272444486618042
Epoch 1060, training loss: 312.10693359375 = 0.3432071805000305 + 50.0 * 6.235274791717529
Epoch 1060, val loss: 1.0298712253570557
Epoch 1070, training loss: 312.1196594238281 = 0.3360971510410309 + 50.0 * 6.235671520233154
Epoch 1070, val loss: 1.0328359603881836
Epoch 1080, training loss: 312.0111999511719 = 0.3291059732437134 + 50.0 * 6.233642101287842
Epoch 1080, val loss: 1.0359207391738892
Epoch 1090, training loss: 312.052490234375 = 0.3223223388195038 + 50.0 * 6.234603404998779
Epoch 1090, val loss: 1.0393673181533813
Epoch 1100, training loss: 312.0472717285156 = 0.3156183958053589 + 50.0 * 6.234632968902588
Epoch 1100, val loss: 1.0425406694412231
Epoch 1110, training loss: 312.00091552734375 = 0.30910611152648926 + 50.0 * 6.2338361740112305
Epoch 1110, val loss: 1.0465539693832397
Epoch 1120, training loss: 311.947998046875 = 0.30271652340888977 + 50.0 * 6.232905864715576
Epoch 1120, val loss: 1.0498814582824707
Epoch 1130, training loss: 311.9452209472656 = 0.2965005040168762 + 50.0 * 6.232974052429199
Epoch 1130, val loss: 1.053689956665039
Epoch 1140, training loss: 311.8560791015625 = 0.29032716155052185 + 50.0 * 6.2313151359558105
Epoch 1140, val loss: 1.0572205781936646
Epoch 1150, training loss: 311.7799072265625 = 0.28438204526901245 + 50.0 * 6.229910373687744
Epoch 1150, val loss: 1.061644434928894
Epoch 1160, training loss: 311.733642578125 = 0.27859291434288025 + 50.0 * 6.229101181030273
Epoch 1160, val loss: 1.065767765045166
Epoch 1170, training loss: 311.893798828125 = 0.2729145586490631 + 50.0 * 6.232417583465576
Epoch 1170, val loss: 1.069920301437378
Epoch 1180, training loss: 311.795166015625 = 0.26729822158813477 + 50.0 * 6.230557441711426
Epoch 1180, val loss: 1.0738192796707153
Epoch 1190, training loss: 311.81103515625 = 0.2617805600166321 + 50.0 * 6.230985164642334
Epoch 1190, val loss: 1.0780256986618042
Epoch 1200, training loss: 311.6470031738281 = 0.2564464509487152 + 50.0 * 6.227811336517334
Epoch 1200, val loss: 1.0827147960662842
Epoch 1210, training loss: 311.58892822265625 = 0.2512754201889038 + 50.0 * 6.226752758026123
Epoch 1210, val loss: 1.0871846675872803
Epoch 1220, training loss: 311.7040100097656 = 0.24624498188495636 + 50.0 * 6.22915506362915
Epoch 1220, val loss: 1.0920963287353516
Epoch 1230, training loss: 311.6376037597656 = 0.24118316173553467 + 50.0 * 6.227928638458252
Epoch 1230, val loss: 1.0961648225784302
Epoch 1240, training loss: 311.5586242675781 = 0.23627227544784546 + 50.0 * 6.226447105407715
Epoch 1240, val loss: 1.1009621620178223
Epoch 1250, training loss: 311.5057067871094 = 0.2315177470445633 + 50.0 * 6.2254838943481445
Epoch 1250, val loss: 1.1055798530578613
Epoch 1260, training loss: 311.45318603515625 = 0.22692808508872986 + 50.0 * 6.224525451660156
Epoch 1260, val loss: 1.1106668710708618
Epoch 1270, training loss: 311.59320068359375 = 0.22241918742656708 + 50.0 * 6.227415561676025
Epoch 1270, val loss: 1.1157493591308594
Epoch 1280, training loss: 311.4625244140625 = 0.21796107292175293 + 50.0 * 6.224891185760498
Epoch 1280, val loss: 1.119871973991394
Epoch 1290, training loss: 311.4479064941406 = 0.21356196701526642 + 50.0 * 6.224686622619629
Epoch 1290, val loss: 1.1250677108764648
Epoch 1300, training loss: 311.5299072265625 = 0.20932143926620483 + 50.0 * 6.226411819458008
Epoch 1300, val loss: 1.1298248767852783
Epoch 1310, training loss: 311.3275146484375 = 0.2051786631345749 + 50.0 * 6.222446918487549
Epoch 1310, val loss: 1.1347538232803345
Epoch 1320, training loss: 311.34326171875 = 0.2011454850435257 + 50.0 * 6.222842216491699
Epoch 1320, val loss: 1.1401182413101196
Epoch 1330, training loss: 311.5462341308594 = 0.19719542562961578 + 50.0 * 6.226980686187744
Epoch 1330, val loss: 1.1452490091323853
Epoch 1340, training loss: 311.326904296875 = 0.19325198233127594 + 50.0 * 6.222673416137695
Epoch 1340, val loss: 1.149483323097229
Epoch 1350, training loss: 311.2831115722656 = 0.18947114050388336 + 50.0 * 6.221872806549072
Epoch 1350, val loss: 1.1549018621444702
Epoch 1360, training loss: 311.4619445800781 = 0.18578383326530457 + 50.0 * 6.225522994995117
Epoch 1360, val loss: 1.1597524881362915
Epoch 1370, training loss: 311.2718811035156 = 0.18212847411632538 + 50.0 * 6.221795082092285
Epoch 1370, val loss: 1.1651967763900757
Epoch 1380, training loss: 311.247314453125 = 0.17859134078025818 + 50.0 * 6.22137451171875
Epoch 1380, val loss: 1.1703736782073975
Epoch 1390, training loss: 311.33502197265625 = 0.17511697113513947 + 50.0 * 6.223198413848877
Epoch 1390, val loss: 1.1753346920013428
Epoch 1400, training loss: 311.2402648925781 = 0.1716596931219101 + 50.0 * 6.221372127532959
Epoch 1400, val loss: 1.1804486513137817
Epoch 1410, training loss: 311.1471862792969 = 0.16835735738277435 + 50.0 * 6.219576358795166
Epoch 1410, val loss: 1.18569016456604
Epoch 1420, training loss: 311.1538391113281 = 0.16512353718280792 + 50.0 * 6.21977424621582
Epoch 1420, val loss: 1.1910524368286133
Epoch 1430, training loss: 311.16717529296875 = 0.16196005046367645 + 50.0 * 6.220104217529297
Epoch 1430, val loss: 1.19619882106781
Epoch 1440, training loss: 311.1551208496094 = 0.15888500213623047 + 50.0 * 6.2199249267578125
Epoch 1440, val loss: 1.201532006263733
Epoch 1450, training loss: 311.07366943359375 = 0.1558479517698288 + 50.0 * 6.218356609344482
Epoch 1450, val loss: 1.2069683074951172
Epoch 1460, training loss: 311.1139221191406 = 0.15288282930850983 + 50.0 * 6.2192206382751465
Epoch 1460, val loss: 1.2123723030090332
Epoch 1470, training loss: 311.15240478515625 = 0.14997845888137817 + 50.0 * 6.220048904418945
Epoch 1470, val loss: 1.2179261445999146
Epoch 1480, training loss: 311.1312255859375 = 0.14711537957191467 + 50.0 * 6.219681739807129
Epoch 1480, val loss: 1.222720742225647
Epoch 1490, training loss: 311.0325622558594 = 0.1443173587322235 + 50.0 * 6.217764854431152
Epoch 1490, val loss: 1.2277870178222656
Epoch 1500, training loss: 311.0450134277344 = 0.1416027694940567 + 50.0 * 6.2180681228637695
Epoch 1500, val loss: 1.233309030532837
Epoch 1510, training loss: 310.9542236328125 = 0.13895316421985626 + 50.0 * 6.216305732727051
Epoch 1510, val loss: 1.2387906312942505
Epoch 1520, training loss: 310.9696350097656 = 0.13637953996658325 + 50.0 * 6.216665267944336
Epoch 1520, val loss: 1.2445462942123413
Epoch 1530, training loss: 311.14019775390625 = 0.1338675171136856 + 50.0 * 6.220126152038574
Epoch 1530, val loss: 1.2500919103622437
Epoch 1540, training loss: 310.9195556640625 = 0.13130302727222443 + 50.0 * 6.21576452255249
Epoch 1540, val loss: 1.2552375793457031
Epoch 1550, training loss: 310.867919921875 = 0.1288415640592575 + 50.0 * 6.214781761169434
Epoch 1550, val loss: 1.2603265047073364
Epoch 1560, training loss: 310.8599548339844 = 0.12648575007915497 + 50.0 * 6.214669227600098
Epoch 1560, val loss: 1.2658230066299438
Epoch 1570, training loss: 310.8597106933594 = 0.12418404966592789 + 50.0 * 6.214710712432861
Epoch 1570, val loss: 1.2712136507034302
Epoch 1580, training loss: 311.1895446777344 = 0.12194143235683441 + 50.0 * 6.2213521003723145
Epoch 1580, val loss: 1.2762632369995117
Epoch 1590, training loss: 310.9826965332031 = 0.11967553198337555 + 50.0 * 6.217260360717773
Epoch 1590, val loss: 1.282626986503601
Epoch 1600, training loss: 310.7995300292969 = 0.11744847893714905 + 50.0 * 6.213641166687012
Epoch 1600, val loss: 1.2873896360397339
Epoch 1610, training loss: 310.7588195800781 = 0.11533520370721817 + 50.0 * 6.212870121002197
Epoch 1610, val loss: 1.2929112911224365
Epoch 1620, training loss: 310.91436767578125 = 0.11328595131635666 + 50.0 * 6.216021537780762
Epoch 1620, val loss: 1.2981635332107544
Epoch 1630, training loss: 310.7419128417969 = 0.11122887581586838 + 50.0 * 6.212613582611084
Epoch 1630, val loss: 1.3040846586227417
Epoch 1640, training loss: 310.7438659667969 = 0.10923564434051514 + 50.0 * 6.212692737579346
Epoch 1640, val loss: 1.3094819784164429
Epoch 1650, training loss: 311.0551452636719 = 0.10732800513505936 + 50.0 * 6.218955993652344
Epoch 1650, val loss: 1.3150889873504639
Epoch 1660, training loss: 310.7568054199219 = 0.1053340807557106 + 50.0 * 6.213028907775879
Epoch 1660, val loss: 1.320062518119812
Epoch 1670, training loss: 310.6910400390625 = 0.10347869992256165 + 50.0 * 6.2117509841918945
Epoch 1670, val loss: 1.3257766962051392
Epoch 1680, training loss: 310.7297058105469 = 0.10167239606380463 + 50.0 * 6.212560176849365
Epoch 1680, val loss: 1.331249713897705
Epoch 1690, training loss: 310.7540588378906 = 0.09988119453191757 + 50.0 * 6.213083744049072
Epoch 1690, val loss: 1.3365235328674316
Epoch 1700, training loss: 310.7239685058594 = 0.09811963140964508 + 50.0 * 6.212516784667969
Epoch 1700, val loss: 1.3423320055007935
Epoch 1710, training loss: 310.7965393066406 = 0.09640403091907501 + 50.0 * 6.21400260925293
Epoch 1710, val loss: 1.347821593284607
Epoch 1720, training loss: 310.7044677734375 = 0.09470234811306 + 50.0 * 6.21219539642334
Epoch 1720, val loss: 1.353325366973877
Epoch 1730, training loss: 310.66357421875 = 0.09304428100585938 + 50.0 * 6.2114105224609375
Epoch 1730, val loss: 1.358743667602539
Epoch 1740, training loss: 310.6929626464844 = 0.09143280982971191 + 50.0 * 6.21203088760376
Epoch 1740, val loss: 1.3642041683197021
Epoch 1750, training loss: 310.59564208984375 = 0.08984742313623428 + 50.0 * 6.210115909576416
Epoch 1750, val loss: 1.3696115016937256
Epoch 1760, training loss: 310.63873291015625 = 0.08831438422203064 + 50.0 * 6.211008071899414
Epoch 1760, val loss: 1.3751933574676514
Epoch 1770, training loss: 310.7020568847656 = 0.08679009228944778 + 50.0 * 6.212305545806885
Epoch 1770, val loss: 1.3807176351547241
Epoch 1780, training loss: 310.64385986328125 = 0.08529552072286606 + 50.0 * 6.2111711502075195
Epoch 1780, val loss: 1.3868244886398315
Epoch 1790, training loss: 310.6022033691406 = 0.08383684605360031 + 50.0 * 6.210367679595947
Epoch 1790, val loss: 1.3922126293182373
Epoch 1800, training loss: 310.5989990234375 = 0.08241362124681473 + 50.0 * 6.210331439971924
Epoch 1800, val loss: 1.3978931903839111
Epoch 1810, training loss: 310.5360107421875 = 0.08101809769868851 + 50.0 * 6.209099769592285
Epoch 1810, val loss: 1.403290867805481
Epoch 1820, training loss: 310.52777099609375 = 0.07965687662363052 + 50.0 * 6.208962440490723
Epoch 1820, val loss: 1.4087435007095337
Epoch 1830, training loss: 310.7146301269531 = 0.07832998037338257 + 50.0 * 6.21272611618042
Epoch 1830, val loss: 1.414094090461731
Epoch 1840, training loss: 310.6405334472656 = 0.07700366526842117 + 50.0 * 6.211270809173584
Epoch 1840, val loss: 1.4194092750549316
Epoch 1850, training loss: 310.65472412109375 = 0.07569453865289688 + 50.0 * 6.211580753326416
Epoch 1850, val loss: 1.4247286319732666
Epoch 1860, training loss: 310.5033264160156 = 0.07442597299814224 + 50.0 * 6.208577632904053
Epoch 1860, val loss: 1.430298924446106
Epoch 1870, training loss: 310.4624938964844 = 0.07319825887680054 + 50.0 * 6.207785606384277
Epoch 1870, val loss: 1.4359378814697266
Epoch 1880, training loss: 310.5184326171875 = 0.07200632244348526 + 50.0 * 6.208928108215332
Epoch 1880, val loss: 1.4414032697677612
Epoch 1890, training loss: 310.5011901855469 = 0.07081920653581619 + 50.0 * 6.208607196807861
Epoch 1890, val loss: 1.446546196937561
Epoch 1900, training loss: 310.5395812988281 = 0.06966701149940491 + 50.0 * 6.20939826965332
Epoch 1900, val loss: 1.4510384798049927
Epoch 1910, training loss: 310.54058837890625 = 0.06852569431066513 + 50.0 * 6.209441661834717
Epoch 1910, val loss: 1.4569238424301147
Epoch 1920, training loss: 310.4501953125 = 0.06740780919790268 + 50.0 * 6.207655906677246
Epoch 1920, val loss: 1.4632099866867065
Epoch 1930, training loss: 310.4192199707031 = 0.06632142513990402 + 50.0 * 6.207057952880859
Epoch 1930, val loss: 1.4680312871932983
Epoch 1940, training loss: 310.5326843261719 = 0.06526032835245132 + 50.0 * 6.209348678588867
Epoch 1940, val loss: 1.4739717245101929
Epoch 1950, training loss: 310.4317626953125 = 0.06420151144266129 + 50.0 * 6.207351207733154
Epoch 1950, val loss: 1.4789918661117554
Epoch 1960, training loss: 310.3691711425781 = 0.06316983699798584 + 50.0 * 6.206120014190674
Epoch 1960, val loss: 1.4841904640197754
Epoch 1970, training loss: 310.43060302734375 = 0.06218092516064644 + 50.0 * 6.207367897033691
Epoch 1970, val loss: 1.489709734916687
Epoch 1980, training loss: 310.4061584472656 = 0.06118712201714516 + 50.0 * 6.206899642944336
Epoch 1980, val loss: 1.495348334312439
Epoch 1990, training loss: 310.40008544921875 = 0.06022106111049652 + 50.0 * 6.2067975997924805
Epoch 1990, val loss: 1.5007882118225098
Epoch 2000, training loss: 310.3546142578125 = 0.05927039682865143 + 50.0 * 6.205906867980957
Epoch 2000, val loss: 1.505988359451294
Epoch 2010, training loss: 310.2779846191406 = 0.058339640498161316 + 50.0 * 6.204392910003662
Epoch 2010, val loss: 1.5111029148101807
Epoch 2020, training loss: 310.3186950683594 = 0.057447221130132675 + 50.0 * 6.205224990844727
Epoch 2020, val loss: 1.516879677772522
Epoch 2030, training loss: 310.64013671875 = 0.05655643716454506 + 50.0 * 6.211671829223633
Epoch 2030, val loss: 1.5221198797225952
Epoch 2040, training loss: 310.42877197265625 = 0.055650461465120316 + 50.0 * 6.207462310791016
Epoch 2040, val loss: 1.526183009147644
Epoch 2050, training loss: 310.2368469238281 = 0.054772038012742996 + 50.0 * 6.203641414642334
Epoch 2050, val loss: 1.5320274829864502
Epoch 2060, training loss: 310.2395935058594 = 0.053937897086143494 + 50.0 * 6.203713417053223
Epoch 2060, val loss: 1.537479043006897
Epoch 2070, training loss: 310.23175048828125 = 0.05313194915652275 + 50.0 * 6.2035722732543945
Epoch 2070, val loss: 1.54280686378479
Epoch 2080, training loss: 310.3749084472656 = 0.052348725497722626 + 50.0 * 6.206451416015625
Epoch 2080, val loss: 1.54822838306427
Epoch 2090, training loss: 310.2298583984375 = 0.051533136516809464 + 50.0 * 6.203566074371338
Epoch 2090, val loss: 1.5530266761779785
Epoch 2100, training loss: 310.3055725097656 = 0.05074860155582428 + 50.0 * 6.20509672164917
Epoch 2100, val loss: 1.5574719905853271
Epoch 2110, training loss: 310.38531494140625 = 0.04997639358043671 + 50.0 * 6.206706523895264
Epoch 2110, val loss: 1.5621973276138306
Epoch 2120, training loss: 310.2286071777344 = 0.049201127141714096 + 50.0 * 6.203588008880615
Epoch 2120, val loss: 1.5686321258544922
Epoch 2130, training loss: 310.193603515625 = 0.04846148192882538 + 50.0 * 6.202902793884277
Epoch 2130, val loss: 1.5732572078704834
Epoch 2140, training loss: 310.1776123046875 = 0.047752946615219116 + 50.0 * 6.202597141265869
Epoch 2140, val loss: 1.57870352268219
Epoch 2150, training loss: 310.2029724121094 = 0.04706441983580589 + 50.0 * 6.203118324279785
Epoch 2150, val loss: 1.584096074104309
Epoch 2160, training loss: 310.3986511230469 = 0.04638390243053436 + 50.0 * 6.207045555114746
Epoch 2160, val loss: 1.589009165763855
Epoch 2170, training loss: 310.2204284667969 = 0.04566316679120064 + 50.0 * 6.203495502471924
Epoch 2170, val loss: 1.5937310457229614
Epoch 2180, training loss: 310.160400390625 = 0.04500255733728409 + 50.0 * 6.20230770111084
Epoch 2180, val loss: 1.5990278720855713
Epoch 2190, training loss: 310.2557067871094 = 0.04435811564326286 + 50.0 * 6.204226970672607
Epoch 2190, val loss: 1.6044301986694336
Epoch 2200, training loss: 310.1859436035156 = 0.04371436685323715 + 50.0 * 6.202845096588135
Epoch 2200, val loss: 1.6093714237213135
Epoch 2210, training loss: 310.2975769042969 = 0.04307984560728073 + 50.0 * 6.205089569091797
Epoch 2210, val loss: 1.614428162574768
Epoch 2220, training loss: 310.1341247558594 = 0.0424380898475647 + 50.0 * 6.201833724975586
Epoch 2220, val loss: 1.6188547611236572
Epoch 2230, training loss: 310.1103820800781 = 0.041839249432086945 + 50.0 * 6.201370716094971
Epoch 2230, val loss: 1.6242355108261108
Epoch 2240, training loss: 310.2077331542969 = 0.041260331869125366 + 50.0 * 6.203329563140869
Epoch 2240, val loss: 1.6300735473632812
Epoch 2250, training loss: 310.0885925292969 = 0.04066510125994682 + 50.0 * 6.200958251953125
Epoch 2250, val loss: 1.6338658332824707
Epoch 2260, training loss: 310.1647033691406 = 0.04009399935603142 + 50.0 * 6.2024922370910645
Epoch 2260, val loss: 1.6381629705429077
Epoch 2270, training loss: 310.2535095214844 = 0.03953922539949417 + 50.0 * 6.204278945922852
Epoch 2270, val loss: 1.6441553831100464
Epoch 2280, training loss: 310.1220703125 = 0.03896380215883255 + 50.0 * 6.201662063598633
Epoch 2280, val loss: 1.6478561162948608
Epoch 2290, training loss: 310.0833740234375 = 0.03842278942465782 + 50.0 * 6.200899124145508
Epoch 2290, val loss: 1.6531012058258057
Epoch 2300, training loss: 310.1974792480469 = 0.03790661320090294 + 50.0 * 6.20319128036499
Epoch 2300, val loss: 1.6581023931503296
Epoch 2310, training loss: 310.1133728027344 = 0.03736942633986473 + 50.0 * 6.201519966125488
Epoch 2310, val loss: 1.6627371311187744
Epoch 2320, training loss: 310.049560546875 = 0.03684302046895027 + 50.0 * 6.200254440307617
Epoch 2320, val loss: 1.6668401956558228
Epoch 2330, training loss: 310.0303649902344 = 0.03633805364370346 + 50.0 * 6.199880123138428
Epoch 2330, val loss: 1.672181487083435
Epoch 2340, training loss: 310.07879638671875 = 0.035853028297424316 + 50.0 * 6.2008585929870605
Epoch 2340, val loss: 1.6771403551101685
Epoch 2350, training loss: 310.1469421386719 = 0.03537069633603096 + 50.0 * 6.202231407165527
Epoch 2350, val loss: 1.6817370653152466
Epoch 2360, training loss: 310.01666259765625 = 0.03488675877451897 + 50.0 * 6.1996355056762695
Epoch 2360, val loss: 1.6860331296920776
Epoch 2370, training loss: 310.019287109375 = 0.03442772477865219 + 50.0 * 6.199697494506836
Epoch 2370, val loss: 1.6909685134887695
Epoch 2380, training loss: 310.18170166015625 = 0.03397038206458092 + 50.0 * 6.2029547691345215
Epoch 2380, val loss: 1.6956456899642944
Epoch 2390, training loss: 310.1137390136719 = 0.03351351618766785 + 50.0 * 6.20160436630249
Epoch 2390, val loss: 1.7003425359725952
Epoch 2400, training loss: 310.0277099609375 = 0.03305995091795921 + 50.0 * 6.199893474578857
Epoch 2400, val loss: 1.7042667865753174
Epoch 2410, training loss: 310.0312194824219 = 0.03262311592698097 + 50.0 * 6.199971675872803
Epoch 2410, val loss: 1.7090331315994263
Epoch 2420, training loss: 310.2577209472656 = 0.03220214694738388 + 50.0 * 6.20451021194458
Epoch 2420, val loss: 1.7141190767288208
Epoch 2430, training loss: 310.1357727050781 = 0.03176980838179588 + 50.0 * 6.202079772949219
Epoch 2430, val loss: 1.7186148166656494
Epoch 2440, training loss: 309.99517822265625 = 0.03132706880569458 + 50.0 * 6.199276447296143
Epoch 2440, val loss: 1.7229124307632446
Epoch 2450, training loss: 309.9327087402344 = 0.03092942014336586 + 50.0 * 6.198035717010498
Epoch 2450, val loss: 1.7278932332992554
Epoch 2460, training loss: 309.9102478027344 = 0.030540289357304573 + 50.0 * 6.197593688964844
Epoch 2460, val loss: 1.7324212789535522
Epoch 2470, training loss: 309.89654541015625 = 0.03016454540193081 + 50.0 * 6.197327613830566
Epoch 2470, val loss: 1.7372814416885376
Epoch 2480, training loss: 309.9208068847656 = 0.029793938621878624 + 50.0 * 6.197820663452148
Epoch 2480, val loss: 1.7417556047439575
Epoch 2490, training loss: 310.298583984375 = 0.029423212632536888 + 50.0 * 6.20538330078125
Epoch 2490, val loss: 1.7456563711166382
Epoch 2500, training loss: 310.1073303222656 = 0.02903871424496174 + 50.0 * 6.201565742492676
Epoch 2500, val loss: 1.750080943107605
Epoch 2510, training loss: 309.9153137207031 = 0.02865024283528328 + 50.0 * 6.197733402252197
Epoch 2510, val loss: 1.755001425743103
Epoch 2520, training loss: 309.8858642578125 = 0.02829483523964882 + 50.0 * 6.1971516609191895
Epoch 2520, val loss: 1.7589733600616455
Epoch 2530, training loss: 309.88909912109375 = 0.02795475162565708 + 50.0 * 6.197222709655762
Epoch 2530, val loss: 1.7642686367034912
Epoch 2540, training loss: 310.0226745605469 = 0.027623334899544716 + 50.0 * 6.199901103973389
Epoch 2540, val loss: 1.7688840627670288
Epoch 2550, training loss: 310.05169677734375 = 0.027281828224658966 + 50.0 * 6.200488090515137
Epoch 2550, val loss: 1.7726949453353882
Epoch 2560, training loss: 309.93609619140625 = 0.026932282373309135 + 50.0 * 6.198183059692383
Epoch 2560, val loss: 1.7761670351028442
Epoch 2570, training loss: 309.9629211425781 = 0.026593763381242752 + 50.0 * 6.198726654052734
Epoch 2570, val loss: 1.7801843881607056
Epoch 2580, training loss: 309.9903259277344 = 0.026269396767020226 + 50.0 * 6.199280738830566
Epoch 2580, val loss: 1.7843364477157593
Epoch 2590, training loss: 309.89013671875 = 0.025949954986572266 + 50.0 * 6.19728422164917
Epoch 2590, val loss: 1.7906756401062012
Epoch 2600, training loss: 309.8199157714844 = 0.02564069628715515 + 50.0 * 6.19588565826416
Epoch 2600, val loss: 1.7937428951263428
Epoch 2610, training loss: 309.82122802734375 = 0.02534443698823452 + 50.0 * 6.195918083190918
Epoch 2610, val loss: 1.7983403205871582
Epoch 2620, training loss: 309.8306579589844 = 0.025051286444067955 + 50.0 * 6.196112155914307
Epoch 2620, val loss: 1.8026330471038818
Epoch 2630, training loss: 310.1727600097656 = 0.024765150621533394 + 50.0 * 6.2029595375061035
Epoch 2630, val loss: 1.806500792503357
Epoch 2640, training loss: 310.0508728027344 = 0.024463217705488205 + 50.0 * 6.200527667999268
Epoch 2640, val loss: 1.8111121654510498
Epoch 2650, training loss: 309.90972900390625 = 0.02416396699845791 + 50.0 * 6.197710990905762
Epoch 2650, val loss: 1.8146679401397705
Epoch 2660, training loss: 309.8717956542969 = 0.023882562294602394 + 50.0 * 6.196958065032959
Epoch 2660, val loss: 1.8190504312515259
Epoch 2670, training loss: 309.9984130859375 = 0.023612162098288536 + 50.0 * 6.199495792388916
Epoch 2670, val loss: 1.8242470026016235
Epoch 2680, training loss: 309.8399658203125 = 0.02331678755581379 + 50.0 * 6.196332931518555
Epoch 2680, val loss: 1.8273166418075562
Epoch 2690, training loss: 309.80316162109375 = 0.023053733631968498 + 50.0 * 6.1956024169921875
Epoch 2690, val loss: 1.8314785957336426
Epoch 2700, training loss: 309.7704162597656 = 0.022793646901845932 + 50.0 * 6.194952487945557
Epoch 2700, val loss: 1.8355753421783447
Epoch 2710, training loss: 309.7821960449219 = 0.022545071318745613 + 50.0 * 6.195192813873291
Epoch 2710, val loss: 1.8394147157669067
Epoch 2720, training loss: 310.09307861328125 = 0.022301238030195236 + 50.0 * 6.201415538787842
Epoch 2720, val loss: 1.8420307636260986
Epoch 2730, training loss: 309.9651184082031 = 0.0220384169369936 + 50.0 * 6.198861598968506
Epoch 2730, val loss: 1.848389744758606
Epoch 2740, training loss: 309.8547058105469 = 0.021781306713819504 + 50.0 * 6.196658134460449
Epoch 2740, val loss: 1.8508533239364624
Epoch 2750, training loss: 309.8208312988281 = 0.02153555117547512 + 50.0 * 6.195985794067383
Epoch 2750, val loss: 1.8559457063674927
Epoch 2760, training loss: 309.7727966308594 = 0.021295078098773956 + 50.0 * 6.1950297355651855
Epoch 2760, val loss: 1.8595207929611206
Epoch 2770, training loss: 309.8186340332031 = 0.021063677966594696 + 50.0 * 6.195951461791992
Epoch 2770, val loss: 1.8638774156570435
Epoch 2780, training loss: 309.8851318359375 = 0.020834600552916527 + 50.0 * 6.1972856521606445
Epoch 2780, val loss: 1.8680604696273804
Epoch 2790, training loss: 309.7528991699219 = 0.02059839852154255 + 50.0 * 6.194645881652832
Epoch 2790, val loss: 1.8705742359161377
Epoch 2800, training loss: 309.7198181152344 = 0.020373042672872543 + 50.0 * 6.193988800048828
Epoch 2800, val loss: 1.8749713897705078
Epoch 2810, training loss: 309.79791259765625 = 0.020158015191555023 + 50.0 * 6.195554733276367
Epoch 2810, val loss: 1.879326581954956
Epoch 2820, training loss: 309.9660949707031 = 0.01993596740067005 + 50.0 * 6.198923587799072
Epoch 2820, val loss: 1.8820489645004272
Epoch 2830, training loss: 309.7655029296875 = 0.01971432939171791 + 50.0 * 6.194915771484375
Epoch 2830, val loss: 1.885858416557312
Epoch 2840, training loss: 309.7117614746094 = 0.019499795511364937 + 50.0 * 6.193845272064209
Epoch 2840, val loss: 1.8901182413101196
Epoch 2850, training loss: 309.75714111328125 = 0.019298488274216652 + 50.0 * 6.194756984710693
Epoch 2850, val loss: 1.8930524587631226
Epoch 2860, training loss: 309.8587341308594 = 0.019095061346888542 + 50.0 * 6.1967926025390625
Epoch 2860, val loss: 1.8975683450698853
Epoch 2870, training loss: 309.82806396484375 = 0.018893223255872726 + 50.0 * 6.196183681488037
Epoch 2870, val loss: 1.9012537002563477
Epoch 2880, training loss: 309.693115234375 = 0.018686603754758835 + 50.0 * 6.193489074707031
Epoch 2880, val loss: 1.9043240547180176
Epoch 2890, training loss: 309.67218017578125 = 0.018498433753848076 + 50.0 * 6.193073749542236
Epoch 2890, val loss: 1.909102201461792
Epoch 2900, training loss: 309.71929931640625 = 0.018315721303224564 + 50.0 * 6.194019794464111
Epoch 2900, val loss: 1.9125287532806396
Epoch 2910, training loss: 309.9586181640625 = 0.018134770914912224 + 50.0 * 6.198809623718262
Epoch 2910, val loss: 1.9158275127410889
Epoch 2920, training loss: 309.8598937988281 = 0.017927901819348335 + 50.0 * 6.196839332580566
Epoch 2920, val loss: 1.918635606765747
Epoch 2930, training loss: 309.72064208984375 = 0.01774100586771965 + 50.0 * 6.194057941436768
Epoch 2930, val loss: 1.9227927923202515
Epoch 2940, training loss: 309.66748046875 = 0.01756041869521141 + 50.0 * 6.19299840927124
Epoch 2940, val loss: 1.9260536432266235
Epoch 2950, training loss: 309.6777038574219 = 0.017391452565789223 + 50.0 * 6.193206310272217
Epoch 2950, val loss: 1.9298698902130127
Epoch 2960, training loss: 309.87322998046875 = 0.017225975170731544 + 50.0 * 6.197120666503906
Epoch 2960, val loss: 1.9331215620040894
Epoch 2970, training loss: 309.760009765625 = 0.017042895779013634 + 50.0 * 6.194859504699707
Epoch 2970, val loss: 1.9378117322921753
Epoch 2980, training loss: 309.6955871582031 = 0.016868509352207184 + 50.0 * 6.193573951721191
Epoch 2980, val loss: 1.9403719902038574
Epoch 2990, training loss: 309.77301025390625 = 0.01670309156179428 + 50.0 * 6.195126056671143
Epoch 2990, val loss: 1.9440149068832397
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.674074074074074
0.8112809699525567
The final CL Acc:0.66667, 0.00605, The final GNN Acc:0.81111, 0.00237
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13252])
remove edge: torch.Size([2, 7780])
updated graph: torch.Size([2, 10476])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.8094787597656 = 1.9680954217910767 + 50.0 * 8.596827507019043
Epoch 0, val loss: 1.9676393270492554
Epoch 10, training loss: 431.7490234375 = 1.95795476436615 + 50.0 * 8.595821380615234
Epoch 10, val loss: 1.9572877883911133
Epoch 20, training loss: 431.3537292480469 = 1.9457069635391235 + 50.0 * 8.588160514831543
Epoch 20, val loss: 1.9449820518493652
Epoch 30, training loss: 428.74176025390625 = 1.9302877187728882 + 50.0 * 8.536229133605957
Epoch 30, val loss: 1.929860234260559
Epoch 40, training loss: 414.5792541503906 = 1.910921335220337 + 50.0 * 8.253366470336914
Epoch 40, val loss: 1.911437749862671
Epoch 50, training loss: 378.6747741699219 = 1.8886058330535889 + 50.0 * 7.5357232093811035
Epoch 50, val loss: 1.8912001848220825
Epoch 60, training loss: 366.1692810058594 = 1.8696702718734741 + 50.0 * 7.285992622375488
Epoch 60, val loss: 1.8748748302459717
Epoch 70, training loss: 356.2206726074219 = 1.8567204475402832 + 50.0 * 7.087278842926025
Epoch 70, val loss: 1.8633345365524292
Epoch 80, training loss: 349.42279052734375 = 1.8461509943008423 + 50.0 * 6.951532363891602
Epoch 80, val loss: 1.8535526990890503
Epoch 90, training loss: 345.4674987792969 = 1.836388111114502 + 50.0 * 6.872622013092041
Epoch 90, val loss: 1.843824028968811
Epoch 100, training loss: 342.39794921875 = 1.8246827125549316 + 50.0 * 6.811465740203857
Epoch 100, val loss: 1.8318125009536743
Epoch 110, training loss: 339.0824890136719 = 1.8142503499984741 + 50.0 * 6.745365142822266
Epoch 110, val loss: 1.8214815855026245
Epoch 120, training loss: 336.6376037597656 = 1.8064541816711426 + 50.0 * 6.696622848510742
Epoch 120, val loss: 1.813191294670105
Epoch 130, training loss: 334.07879638671875 = 1.799114465713501 + 50.0 * 6.645594120025635
Epoch 130, val loss: 1.805264949798584
Epoch 140, training loss: 331.97796630859375 = 1.7922313213348389 + 50.0 * 6.603714466094971
Epoch 140, val loss: 1.7977344989776611
Epoch 150, training loss: 330.1212158203125 = 1.7853361368179321 + 50.0 * 6.566717624664307
Epoch 150, val loss: 1.7903915643692017
Epoch 160, training loss: 328.68609619140625 = 1.7780159711837769 + 50.0 * 6.538161754608154
Epoch 160, val loss: 1.7828145027160645
Epoch 170, training loss: 327.4080810546875 = 1.7698756456375122 + 50.0 * 6.5127644538879395
Epoch 170, val loss: 1.7746150493621826
Epoch 180, training loss: 326.22027587890625 = 1.7613420486450195 + 50.0 * 6.48917818069458
Epoch 180, val loss: 1.7663021087646484
Epoch 190, training loss: 325.1279296875 = 1.7526192665100098 + 50.0 * 6.467506408691406
Epoch 190, val loss: 1.757900357246399
Epoch 200, training loss: 324.30950927734375 = 1.7431586980819702 + 50.0 * 6.451326847076416
Epoch 200, val loss: 1.748816967010498
Epoch 210, training loss: 323.5588073730469 = 1.732799768447876 + 50.0 * 6.436520576477051
Epoch 210, val loss: 1.7391833066940308
Epoch 220, training loss: 322.8760681152344 = 1.7215412855148315 + 50.0 * 6.423090934753418
Epoch 220, val loss: 1.7287944555282593
Epoch 230, training loss: 322.282958984375 = 1.7094051837921143 + 50.0 * 6.411470890045166
Epoch 230, val loss: 1.7176880836486816
Epoch 240, training loss: 321.7677001953125 = 1.6963387727737427 + 50.0 * 6.401426792144775
Epoch 240, val loss: 1.7058234214782715
Epoch 250, training loss: 321.50030517578125 = 1.6820815801620483 + 50.0 * 6.396364688873291
Epoch 250, val loss: 1.6930570602416992
Epoch 260, training loss: 321.02294921875 = 1.6669679880142212 + 50.0 * 6.387119770050049
Epoch 260, val loss: 1.6794402599334717
Epoch 270, training loss: 320.5467529296875 = 1.6509655714035034 + 50.0 * 6.377915859222412
Epoch 270, val loss: 1.6652082204818726
Epoch 280, training loss: 320.15875244140625 = 1.6341583728790283 + 50.0 * 6.370491981506348
Epoch 280, val loss: 1.650383472442627
Epoch 290, training loss: 320.029541015625 = 1.616608738899231 + 50.0 * 6.368258953094482
Epoch 290, val loss: 1.6349868774414062
Epoch 300, training loss: 319.49420166015625 = 1.598185420036316 + 50.0 * 6.3579206466674805
Epoch 300, val loss: 1.6191550493240356
Epoch 310, training loss: 319.22381591796875 = 1.579452633857727 + 50.0 * 6.35288667678833
Epoch 310, val loss: 1.6031755208969116
Epoch 320, training loss: 319.1377868652344 = 1.5599920749664307 + 50.0 * 6.351555824279785
Epoch 320, val loss: 1.5867457389831543
Epoch 330, training loss: 318.72186279296875 = 1.5406135320663452 + 50.0 * 6.343624591827393
Epoch 330, val loss: 1.5704379081726074
Epoch 340, training loss: 318.3956604003906 = 1.5211355686187744 + 50.0 * 6.337490081787109
Epoch 340, val loss: 1.5542811155319214
Epoch 350, training loss: 318.1686706542969 = 1.5016671419143677 + 50.0 * 6.333339691162109
Epoch 350, val loss: 1.5383734703063965
Epoch 360, training loss: 317.958984375 = 1.482258915901184 + 50.0 * 6.32953405380249
Epoch 360, val loss: 1.5227347612380981
Epoch 370, training loss: 318.0140686035156 = 1.4627221822738647 + 50.0 * 6.331027030944824
Epoch 370, val loss: 1.5071203708648682
Epoch 380, training loss: 317.5860900878906 = 1.4435337781906128 + 50.0 * 6.322851181030273
Epoch 380, val loss: 1.4918876886367798
Epoch 390, training loss: 317.3951416015625 = 1.424805760383606 + 50.0 * 6.319406509399414
Epoch 390, val loss: 1.4772428274154663
Epoch 400, training loss: 317.2105407714844 = 1.4063061475753784 + 50.0 * 6.316084384918213
Epoch 400, val loss: 1.4630342721939087
Epoch 410, training loss: 317.0382385253906 = 1.388145923614502 + 50.0 * 6.31300163269043
Epoch 410, val loss: 1.4491522312164307
Epoch 420, training loss: 316.9244079589844 = 1.370205044746399 + 50.0 * 6.311084270477295
Epoch 420, val loss: 1.4354783296585083
Epoch 430, training loss: 316.9648132324219 = 1.352096438407898 + 50.0 * 6.312254428863525
Epoch 430, val loss: 1.421980619430542
Epoch 440, training loss: 316.62310791015625 = 1.334351658821106 + 50.0 * 6.305775165557861
Epoch 440, val loss: 1.4086425304412842
Epoch 450, training loss: 316.40826416015625 = 1.3167707920074463 + 50.0 * 6.301829814910889
Epoch 450, val loss: 1.3955661058425903
Epoch 460, training loss: 316.4835205078125 = 1.299349069595337 + 50.0 * 6.303683280944824
Epoch 460, val loss: 1.382578730583191
Epoch 470, training loss: 316.2869567871094 = 1.2814481258392334 + 50.0 * 6.300110340118408
Epoch 470, val loss: 1.3694568872451782
Epoch 480, training loss: 316.0473327636719 = 1.2638485431671143 + 50.0 * 6.2956695556640625
Epoch 480, val loss: 1.356475830078125
Epoch 490, training loss: 315.8518371582031 = 1.2462173700332642 + 50.0 * 6.292112350463867
Epoch 490, val loss: 1.34352707862854
Epoch 500, training loss: 316.1094970703125 = 1.2284072637557983 + 50.0 * 6.297622203826904
Epoch 500, val loss: 1.330465316772461
Epoch 510, training loss: 315.7564697265625 = 1.2105320692062378 + 50.0 * 6.290918827056885
Epoch 510, val loss: 1.3171701431274414
Epoch 520, training loss: 315.5033264160156 = 1.19239342212677 + 50.0 * 6.286219120025635
Epoch 520, val loss: 1.303949236869812
Epoch 530, training loss: 315.4056396484375 = 1.1742923259735107 + 50.0 * 6.2846269607543945
Epoch 530, val loss: 1.2907525300979614
Epoch 540, training loss: 315.55767822265625 = 1.1561813354492188 + 50.0 * 6.288029670715332
Epoch 540, val loss: 1.2774299383163452
Epoch 550, training loss: 315.2149963378906 = 1.1376903057098389 + 50.0 * 6.281546115875244
Epoch 550, val loss: 1.2639427185058594
Epoch 560, training loss: 315.10943603515625 = 1.1192474365234375 + 50.0 * 6.279803276062012
Epoch 560, val loss: 1.250549554824829
Epoch 570, training loss: 314.99871826171875 = 1.100843906402588 + 50.0 * 6.277957916259766
Epoch 570, val loss: 1.237195611000061
Epoch 580, training loss: 315.1037292480469 = 1.0824936628341675 + 50.0 * 6.28042459487915
Epoch 580, val loss: 1.223910927772522
Epoch 590, training loss: 315.10150146484375 = 1.0637644529342651 + 50.0 * 6.280755043029785
Epoch 590, val loss: 1.2105193138122559
Epoch 600, training loss: 314.7461242675781 = 1.0452061891555786 + 50.0 * 6.27401876449585
Epoch 600, val loss: 1.1972323656082153
Epoch 610, training loss: 314.63409423828125 = 1.0268611907958984 + 50.0 * 6.272144794464111
Epoch 610, val loss: 1.1843229532241821
Epoch 620, training loss: 314.52984619140625 = 1.0087239742279053 + 50.0 * 6.270422458648682
Epoch 620, val loss: 1.1717454195022583
Epoch 630, training loss: 314.4369201660156 = 0.9907060861587524 + 50.0 * 6.268924713134766
Epoch 630, val loss: 1.159319519996643
Epoch 640, training loss: 315.31573486328125 = 0.9728525280952454 + 50.0 * 6.286857604980469
Epoch 640, val loss: 1.1469793319702148
Epoch 650, training loss: 314.55609130859375 = 0.9545499086380005 + 50.0 * 6.272030830383301
Epoch 650, val loss: 1.134577989578247
Epoch 660, training loss: 314.2723388671875 = 0.9367747902870178 + 50.0 * 6.266711711883545
Epoch 660, val loss: 1.122843623161316
Epoch 670, training loss: 314.1292724609375 = 0.9193974733352661 + 50.0 * 6.26419734954834
Epoch 670, val loss: 1.1115368604660034
Epoch 680, training loss: 314.0403137207031 = 0.9022285342216492 + 50.0 * 6.262762069702148
Epoch 680, val loss: 1.1005185842514038
Epoch 690, training loss: 313.9612121582031 = 0.8852490186691284 + 50.0 * 6.261519432067871
Epoch 690, val loss: 1.0898555517196655
Epoch 700, training loss: 313.95599365234375 = 0.8683978319168091 + 50.0 * 6.261751651763916
Epoch 700, val loss: 1.0793942213058472
Epoch 710, training loss: 313.9071960449219 = 0.851506233215332 + 50.0 * 6.261114120483398
Epoch 710, val loss: 1.0691026449203491
Epoch 720, training loss: 313.8172912597656 = 0.8347853422164917 + 50.0 * 6.259650230407715
Epoch 720, val loss: 1.0588396787643433
Epoch 730, training loss: 313.71307373046875 = 0.8183630704879761 + 50.0 * 6.257894515991211
Epoch 730, val loss: 1.0491468906402588
Epoch 740, training loss: 313.64178466796875 = 0.8023027181625366 + 50.0 * 6.256789684295654
Epoch 740, val loss: 1.0398808717727661
Epoch 750, training loss: 313.5487060546875 = 0.7863577604293823 + 50.0 * 6.255247116088867
Epoch 750, val loss: 1.0309298038482666
Epoch 760, training loss: 313.7218933105469 = 0.7705947756767273 + 50.0 * 6.259026050567627
Epoch 760, val loss: 1.022230625152588
Epoch 770, training loss: 313.61053466796875 = 0.7546094655990601 + 50.0 * 6.2571187019348145
Epoch 770, val loss: 1.013331413269043
Epoch 780, training loss: 313.43499755859375 = 0.7388964295387268 + 50.0 * 6.253921985626221
Epoch 780, val loss: 1.0050134658813477
Epoch 790, training loss: 313.3079528808594 = 0.7234153747558594 + 50.0 * 6.25169038772583
Epoch 790, val loss: 0.9968852996826172
Epoch 800, training loss: 313.2638244628906 = 0.708081066608429 + 50.0 * 6.251114845275879
Epoch 800, val loss: 0.9891197085380554
Epoch 810, training loss: 313.4627990722656 = 0.6928188800811768 + 50.0 * 6.255399703979492
Epoch 810, val loss: 0.9815496206283569
Epoch 820, training loss: 313.30694580078125 = 0.6775374412536621 + 50.0 * 6.252587795257568
Epoch 820, val loss: 0.973966121673584
Epoch 830, training loss: 313.14361572265625 = 0.6624035239219666 + 50.0 * 6.249624252319336
Epoch 830, val loss: 0.9666828513145447
Epoch 840, training loss: 313.04132080078125 = 0.6475261449813843 + 50.0 * 6.247875690460205
Epoch 840, val loss: 0.9598158001899719
Epoch 850, training loss: 313.0000915527344 = 0.6327916383743286 + 50.0 * 6.247345924377441
Epoch 850, val loss: 0.9531106352806091
Epoch 860, training loss: 313.0562438964844 = 0.6181482076644897 + 50.0 * 6.248762130737305
Epoch 860, val loss: 0.9466100931167603
Epoch 870, training loss: 312.9005126953125 = 0.6035395264625549 + 50.0 * 6.245939254760742
Epoch 870, val loss: 0.9402907490730286
Epoch 880, training loss: 312.9324035644531 = 0.5891956686973572 + 50.0 * 6.246863842010498
Epoch 880, val loss: 0.9342414736747742
Epoch 890, training loss: 312.8213195800781 = 0.5747828483581543 + 50.0 * 6.244930744171143
Epoch 890, val loss: 0.9281695485115051
Epoch 900, training loss: 312.7236328125 = 0.5606958866119385 + 50.0 * 6.243258953094482
Epoch 900, val loss: 0.9226639866828918
Epoch 910, training loss: 312.6705627441406 = 0.5468809604644775 + 50.0 * 6.242473602294922
Epoch 910, val loss: 0.917399525642395
Epoch 920, training loss: 312.6336364746094 = 0.5333442091941833 + 50.0 * 6.242005825042725
Epoch 920, val loss: 0.9125496745109558
Epoch 930, training loss: 312.7752380371094 = 0.5199553370475769 + 50.0 * 6.245105743408203
Epoch 930, val loss: 0.9078086018562317
Epoch 940, training loss: 312.6871032714844 = 0.5066601037979126 + 50.0 * 6.2436089515686035
Epoch 940, val loss: 0.9033417105674744
Epoch 950, training loss: 312.7251892089844 = 0.4937126934528351 + 50.0 * 6.244629383087158
Epoch 950, val loss: 0.8991486430168152
Epoch 960, training loss: 312.49310302734375 = 0.48089221119880676 + 50.0 * 6.240244388580322
Epoch 960, val loss: 0.8950143456459045
Epoch 970, training loss: 312.3999328613281 = 0.46846848726272583 + 50.0 * 6.23862886428833
Epoch 970, val loss: 0.891566812992096
Epoch 980, training loss: 312.4273376464844 = 0.45642948150634766 + 50.0 * 6.2394185066223145
Epoch 980, val loss: 0.8884086608886719
Epoch 990, training loss: 312.37109375 = 0.44447967410087585 + 50.0 * 6.238532066345215
Epoch 990, val loss: 0.8853099942207336
Epoch 1000, training loss: 312.274169921875 = 0.4327499270439148 + 50.0 * 6.236828327178955
Epoch 1000, val loss: 0.8825874924659729
Epoch 1010, training loss: 312.2186584472656 = 0.4214770495891571 + 50.0 * 6.235943794250488
Epoch 1010, val loss: 0.8803278803825378
Epoch 1020, training loss: 312.25653076171875 = 0.4104421138763428 + 50.0 * 6.236922264099121
Epoch 1020, val loss: 0.8783142566680908
Epoch 1030, training loss: 312.2482604980469 = 0.3995848000049591 + 50.0 * 6.236973285675049
Epoch 1030, val loss: 0.8764479160308838
Epoch 1040, training loss: 312.13006591796875 = 0.3890109956264496 + 50.0 * 6.234821319580078
Epoch 1040, val loss: 0.8748773336410522
Epoch 1050, training loss: 312.1419982910156 = 0.3787819743156433 + 50.0 * 6.235264301300049
Epoch 1050, val loss: 0.8736519813537598
Epoch 1060, training loss: 312.2097473144531 = 0.36878085136413574 + 50.0 * 6.236819267272949
Epoch 1060, val loss: 0.8726783394813538
Epoch 1070, training loss: 312.0561218261719 = 0.35908642411231995 + 50.0 * 6.233940601348877
Epoch 1070, val loss: 0.8718709349632263
Epoch 1080, training loss: 311.99822998046875 = 0.3496556282043457 + 50.0 * 6.23297119140625
Epoch 1080, val loss: 0.8714439868927002
Epoch 1090, training loss: 312.1235656738281 = 0.3404862880706787 + 50.0 * 6.235661506652832
Epoch 1090, val loss: 0.8711579442024231
Epoch 1100, training loss: 311.9175720214844 = 0.3315867781639099 + 50.0 * 6.231719970703125
Epoch 1100, val loss: 0.8713769912719727
Epoch 1110, training loss: 312.0027770996094 = 0.3229164481163025 + 50.0 * 6.233597278594971
Epoch 1110, val loss: 0.8715444803237915
Epoch 1120, training loss: 311.8905029296875 = 0.31445690989494324 + 50.0 * 6.231521129608154
Epoch 1120, val loss: 0.8718860745429993
Epoch 1130, training loss: 311.8218994140625 = 0.3063228726387024 + 50.0 * 6.230311393737793
Epoch 1130, val loss: 0.8727869391441345
Epoch 1140, training loss: 311.8470153808594 = 0.2984240651130676 + 50.0 * 6.230971813201904
Epoch 1140, val loss: 0.8736060261726379
Epoch 1150, training loss: 311.8363342285156 = 0.2906992435455322 + 50.0 * 6.230912685394287
Epoch 1150, val loss: 0.874823808670044
Epoch 1160, training loss: 312.0228271484375 = 0.2832227051258087 + 50.0 * 6.2347917556762695
Epoch 1160, val loss: 0.875964879989624
Epoch 1170, training loss: 311.776611328125 = 0.2758145034313202 + 50.0 * 6.230015754699707
Epoch 1170, val loss: 0.8772202134132385
Epoch 1180, training loss: 311.6971740722656 = 0.2687208354473114 + 50.0 * 6.228569030761719
Epoch 1180, val loss: 0.8788697123527527
Epoch 1190, training loss: 311.6700439453125 = 0.2619464695453644 + 50.0 * 6.2281622886657715
Epoch 1190, val loss: 0.8807268142700195
Epoch 1200, training loss: 311.7265930175781 = 0.2553328275680542 + 50.0 * 6.229424953460693
Epoch 1200, val loss: 0.8826998472213745
Epoch 1210, training loss: 311.5864562988281 = 0.2488103210926056 + 50.0 * 6.226752758026123
Epoch 1210, val loss: 0.8845782279968262
Epoch 1220, training loss: 311.7111511230469 = 0.24251829087734222 + 50.0 * 6.229372501373291
Epoch 1220, val loss: 0.8867194056510925
Epoch 1230, training loss: 311.54754638671875 = 0.23645693063735962 + 50.0 * 6.226222038269043
Epoch 1230, val loss: 0.8891654014587402
Epoch 1240, training loss: 311.49163818359375 = 0.23054182529449463 + 50.0 * 6.225222110748291
Epoch 1240, val loss: 0.8916751146316528
Epoch 1250, training loss: 311.734375 = 0.22480657696723938 + 50.0 * 6.230191707611084
Epoch 1250, val loss: 0.8942214250564575
Epoch 1260, training loss: 311.5589294433594 = 0.2191648632287979 + 50.0 * 6.226795196533203
Epoch 1260, val loss: 0.8967900276184082
Epoch 1270, training loss: 311.4898681640625 = 0.21369896829128265 + 50.0 * 6.225523948669434
Epoch 1270, val loss: 0.899616539478302
Epoch 1280, training loss: 311.52679443359375 = 0.20844148099422455 + 50.0 * 6.226367473602295
Epoch 1280, val loss: 0.9025055170059204
Epoch 1290, training loss: 311.37445068359375 = 0.20332562923431396 + 50.0 * 6.223422050476074
Epoch 1290, val loss: 0.9056386947631836
Epoch 1300, training loss: 311.4079284667969 = 0.19838394224643707 + 50.0 * 6.224190711975098
Epoch 1300, val loss: 0.9088320732116699
Epoch 1310, training loss: 311.50909423828125 = 0.19353395700454712 + 50.0 * 6.226311206817627
Epoch 1310, val loss: 0.9119945168495178
Epoch 1320, training loss: 311.32281494140625 = 0.18884024024009705 + 50.0 * 6.222679138183594
Epoch 1320, val loss: 0.9155519008636475
Epoch 1330, training loss: 311.3005676269531 = 0.18432261049747467 + 50.0 * 6.222324848175049
Epoch 1330, val loss: 0.9189414381980896
Epoch 1340, training loss: 311.642578125 = 0.17993518710136414 + 50.0 * 6.229252815246582
Epoch 1340, val loss: 0.9226507544517517
Epoch 1350, training loss: 311.345703125 = 0.17556557059288025 + 50.0 * 6.223402500152588
Epoch 1350, val loss: 0.9259808659553528
Epoch 1360, training loss: 311.21527099609375 = 0.17139023542404175 + 50.0 * 6.220877647399902
Epoch 1360, val loss: 0.929735541343689
Epoch 1370, training loss: 311.15924072265625 = 0.16737142205238342 + 50.0 * 6.219837665557861
Epoch 1370, val loss: 0.9336732625961304
Epoch 1380, training loss: 311.23699951171875 = 0.16348384320735931 + 50.0 * 6.221470355987549
Epoch 1380, val loss: 0.9375333189964294
Epoch 1390, training loss: 311.3714294433594 = 0.15958712995052338 + 50.0 * 6.224236965179443
Epoch 1390, val loss: 0.9413807988166809
Epoch 1400, training loss: 311.1936950683594 = 0.15585708618164062 + 50.0 * 6.220756530761719
Epoch 1400, val loss: 0.9453150629997253
Epoch 1410, training loss: 311.1008605957031 = 0.1522391438484192 + 50.0 * 6.218972206115723
Epoch 1410, val loss: 0.9492403268814087
Epoch 1420, training loss: 311.0541687011719 = 0.14878162741661072 + 50.0 * 6.218108177185059
Epoch 1420, val loss: 0.9534975290298462
Epoch 1430, training loss: 311.05682373046875 = 0.1454232931137085 + 50.0 * 6.218227863311768
Epoch 1430, val loss: 0.9576693773269653
Epoch 1440, training loss: 311.38275146484375 = 0.14213432371616364 + 50.0 * 6.2248125076293945
Epoch 1440, val loss: 0.961737871170044
Epoch 1450, training loss: 311.2915954589844 = 0.13891275227069855 + 50.0 * 6.223053455352783
Epoch 1450, val loss: 0.9662148952484131
Epoch 1460, training loss: 311.0327453613281 = 0.13570836186408997 + 50.0 * 6.217940807342529
Epoch 1460, val loss: 0.9701946973800659
Epoch 1470, training loss: 310.9739990234375 = 0.1326730102300644 + 50.0 * 6.216826915740967
Epoch 1470, val loss: 0.9744524955749512
Epoch 1480, training loss: 311.0201416015625 = 0.12977735698223114 + 50.0 * 6.217807292938232
Epoch 1480, val loss: 0.9789408445358276
Epoch 1490, training loss: 311.0950012207031 = 0.1269102245569229 + 50.0 * 6.219361305236816
Epoch 1490, val loss: 0.9833512902259827
Epoch 1500, training loss: 310.9195251464844 = 0.12406875193119049 + 50.0 * 6.215909481048584
Epoch 1500, val loss: 0.9875087141990662
Epoch 1510, training loss: 310.9042663574219 = 0.12137110531330109 + 50.0 * 6.215657711029053
Epoch 1510, val loss: 0.9919766187667847
Epoch 1520, training loss: 310.91387939453125 = 0.11876697838306427 + 50.0 * 6.215902328491211
Epoch 1520, val loss: 0.9964935183525085
Epoch 1530, training loss: 311.08294677734375 = 0.1162211075425148 + 50.0 * 6.219334602355957
Epoch 1530, val loss: 1.0008935928344727
Epoch 1540, training loss: 310.97747802734375 = 0.11372172832489014 + 50.0 * 6.217275619506836
Epoch 1540, val loss: 1.0055866241455078
Epoch 1550, training loss: 310.8701171875 = 0.11127500981092453 + 50.0 * 6.215177059173584
Epoch 1550, val loss: 1.0100219249725342
Epoch 1560, training loss: 310.8536376953125 = 0.10891704261302948 + 50.0 * 6.2148942947387695
Epoch 1560, val loss: 1.0145522356033325
Epoch 1570, training loss: 310.83544921875 = 0.10664163529872894 + 50.0 * 6.214576721191406
Epoch 1570, val loss: 1.0192179679870605
Epoch 1580, training loss: 310.8345947265625 = 0.10442941635847092 + 50.0 * 6.214603424072266
Epoch 1580, val loss: 1.0238654613494873
Epoch 1590, training loss: 310.79583740234375 = 0.10226244479417801 + 50.0 * 6.213871479034424
Epoch 1590, val loss: 1.0284478664398193
Epoch 1600, training loss: 311.0357666015625 = 0.10016720741987228 + 50.0 * 6.218712329864502
Epoch 1600, val loss: 1.0329912900924683
Epoch 1610, training loss: 310.78851318359375 = 0.09804025292396545 + 50.0 * 6.213809490203857
Epoch 1610, val loss: 1.0373940467834473
Epoch 1620, training loss: 310.8807373046875 = 0.09603862464427948 + 50.0 * 6.215693950653076
Epoch 1620, val loss: 1.041892170906067
Epoch 1630, training loss: 310.7840270996094 = 0.09407117962837219 + 50.0 * 6.213798522949219
Epoch 1630, val loss: 1.046630620956421
Epoch 1640, training loss: 310.70556640625 = 0.09215272217988968 + 50.0 * 6.212267875671387
Epoch 1640, val loss: 1.051147699356079
Epoch 1650, training loss: 310.6535949707031 = 0.09031353145837784 + 50.0 * 6.211266040802002
Epoch 1650, val loss: 1.0558804273605347
Epoch 1660, training loss: 310.7311096191406 = 0.08853240311145782 + 50.0 * 6.212851524353027
Epoch 1660, val loss: 1.0604130029678345
Epoch 1670, training loss: 310.80731201171875 = 0.08675111830234528 + 50.0 * 6.214410781860352
Epoch 1670, val loss: 1.065132975578308
Epoch 1680, training loss: 310.6646423339844 = 0.08501625061035156 + 50.0 * 6.211592197418213
Epoch 1680, val loss: 1.069732666015625
Epoch 1690, training loss: 310.6307373046875 = 0.08333757519721985 + 50.0 * 6.2109479904174805
Epoch 1690, val loss: 1.0742161273956299
Epoch 1700, training loss: 310.6617736816406 = 0.08172405511140823 + 50.0 * 6.2116007804870605
Epoch 1700, val loss: 1.078810453414917
Epoch 1710, training loss: 310.6825256347656 = 0.08014862984418869 + 50.0 * 6.212047576904297
Epoch 1710, val loss: 1.0835644006729126
Epoch 1720, training loss: 310.6580505371094 = 0.07860109210014343 + 50.0 * 6.2115888595581055
Epoch 1720, val loss: 1.0881493091583252
Epoch 1730, training loss: 310.6775207519531 = 0.07709579914808273 + 50.0 * 6.212008476257324
Epoch 1730, val loss: 1.0927996635437012
Epoch 1740, training loss: 310.6568908691406 = 0.07561744004487991 + 50.0 * 6.211625576019287
Epoch 1740, val loss: 1.0971102714538574
Epoch 1750, training loss: 310.56573486328125 = 0.07417158037424088 + 50.0 * 6.209831714630127
Epoch 1750, val loss: 1.1019055843353271
Epoch 1760, training loss: 310.6189270019531 = 0.07278012484312057 + 50.0 * 6.210922718048096
Epoch 1760, val loss: 1.1063367128372192
Epoch 1770, training loss: 310.6207275390625 = 0.07141481339931488 + 50.0 * 6.210986614227295
Epoch 1770, val loss: 1.1110131740570068
Epoch 1780, training loss: 310.52435302734375 = 0.07009898871183395 + 50.0 * 6.209084987640381
Epoch 1780, val loss: 1.1157374382019043
Epoch 1790, training loss: 310.497802734375 = 0.06880327314138412 + 50.0 * 6.2085795402526855
Epoch 1790, val loss: 1.120316982269287
Epoch 1800, training loss: 310.5916748046875 = 0.06754458695650101 + 50.0 * 6.210483074188232
Epoch 1800, val loss: 1.1247721910476685
Epoch 1810, training loss: 310.53472900390625 = 0.06629540771245956 + 50.0 * 6.209368705749512
Epoch 1810, val loss: 1.1292893886566162
Epoch 1820, training loss: 310.6463623046875 = 0.06509514898061752 + 50.0 * 6.211625576019287
Epoch 1820, val loss: 1.1339716911315918
Epoch 1830, training loss: 310.5157775878906 = 0.063880555331707 + 50.0 * 6.209038257598877
Epoch 1830, val loss: 1.1382135152816772
Epoch 1840, training loss: 310.5116271972656 = 0.0627535954117775 + 50.0 * 6.208977699279785
Epoch 1840, val loss: 1.1429411172866821
Epoch 1850, training loss: 310.3900451660156 = 0.0616314671933651 + 50.0 * 6.206568241119385
Epoch 1850, val loss: 1.1474863290786743
Epoch 1860, training loss: 310.4342041015625 = 0.060568831861019135 + 50.0 * 6.207472324371338
Epoch 1860, val loss: 1.1520835161209106
Epoch 1870, training loss: 310.7658386230469 = 0.05952409282326698 + 50.0 * 6.2141265869140625
Epoch 1870, val loss: 1.156747579574585
Epoch 1880, training loss: 310.4658508300781 = 0.05840497836470604 + 50.0 * 6.208148956298828
Epoch 1880, val loss: 1.160718321800232
Epoch 1890, training loss: 310.36767578125 = 0.057382598519325256 + 50.0 * 6.20620584487915
Epoch 1890, val loss: 1.1653032302856445
Epoch 1900, training loss: 310.3442077636719 = 0.05639808252453804 + 50.0 * 6.205756187438965
Epoch 1900, val loss: 1.169677972793579
Epoch 1910, training loss: 310.52679443359375 = 0.05546094477176666 + 50.0 * 6.2094268798828125
Epoch 1910, val loss: 1.1742311716079712
Epoch 1920, training loss: 310.4169921875 = 0.054482050240039825 + 50.0 * 6.207250595092773
Epoch 1920, val loss: 1.1786614656448364
Epoch 1930, training loss: 310.3162536621094 = 0.05354353412985802 + 50.0 * 6.205254077911377
Epoch 1930, val loss: 1.1829103231430054
Epoch 1940, training loss: 310.2986755371094 = 0.05263408645987511 + 50.0 * 6.204920768737793
Epoch 1940, val loss: 1.1873531341552734
Epoch 1950, training loss: 310.3428039550781 = 0.05176783725619316 + 50.0 * 6.2058210372924805
Epoch 1950, val loss: 1.1917883157730103
Epoch 1960, training loss: 310.43792724609375 = 0.05090790241956711 + 50.0 * 6.207740783691406
Epoch 1960, val loss: 1.19613778591156
Epoch 1970, training loss: 310.450927734375 = 0.05004586651921272 + 50.0 * 6.208017826080322
Epoch 1970, val loss: 1.2004528045654297
Epoch 1980, training loss: 310.2748107910156 = 0.04919428750872612 + 50.0 * 6.204512596130371
Epoch 1980, val loss: 1.204585075378418
Epoch 1990, training loss: 310.32659912109375 = 0.04839501157402992 + 50.0 * 6.205564022064209
Epoch 1990, val loss: 1.2089177370071411
Epoch 2000, training loss: 310.3334655761719 = 0.04759890213608742 + 50.0 * 6.205717086791992
Epoch 2000, val loss: 1.213180422782898
Epoch 2010, training loss: 310.2674560546875 = 0.04681500047445297 + 50.0 * 6.204412937164307
Epoch 2010, val loss: 1.2174477577209473
Epoch 2020, training loss: 310.2191162109375 = 0.04606292396783829 + 50.0 * 6.203461170196533
Epoch 2020, val loss: 1.22165846824646
Epoch 2030, training loss: 310.3142395019531 = 0.04534091800451279 + 50.0 * 6.20537805557251
Epoch 2030, val loss: 1.2260677814483643
Epoch 2040, training loss: 310.2618103027344 = 0.04460515081882477 + 50.0 * 6.204343795776367
Epoch 2040, val loss: 1.230238676071167
Epoch 2050, training loss: 310.18707275390625 = 0.043879326432943344 + 50.0 * 6.202863693237305
Epoch 2050, val loss: 1.2342888116836548
Epoch 2060, training loss: 310.5450134277344 = 0.043203409761190414 + 50.0 * 6.210035800933838
Epoch 2060, val loss: 1.2383615970611572
Epoch 2070, training loss: 310.32208251953125 = 0.042505357414484024 + 50.0 * 6.205591678619385
Epoch 2070, val loss: 1.2426388263702393
Epoch 2080, training loss: 310.2101135253906 = 0.04181214049458504 + 50.0 * 6.203365802764893
Epoch 2080, val loss: 1.246505856513977
Epoch 2090, training loss: 310.19207763671875 = 0.04117254912853241 + 50.0 * 6.2030181884765625
Epoch 2090, val loss: 1.2507621049880981
Epoch 2100, training loss: 310.3056335449219 = 0.04053892567753792 + 50.0 * 6.2053022384643555
Epoch 2100, val loss: 1.2547765970230103
Epoch 2110, training loss: 310.1262512207031 = 0.03991309553384781 + 50.0 * 6.201726913452148
Epoch 2110, val loss: 1.2591652870178223
Epoch 2120, training loss: 310.09783935546875 = 0.03931049257516861 + 50.0 * 6.201170444488525
Epoch 2120, val loss: 1.2632609605789185
Epoch 2130, training loss: 310.2189025878906 = 0.03872425854206085 + 50.0 * 6.203603744506836
Epoch 2130, val loss: 1.267151951789856
Epoch 2140, training loss: 310.22589111328125 = 0.038118988275527954 + 50.0 * 6.2037553787231445
Epoch 2140, val loss: 1.2713232040405273
Epoch 2150, training loss: 310.2384033203125 = 0.0375247448682785 + 50.0 * 6.204017639160156
Epoch 2150, val loss: 1.2750190496444702
Epoch 2160, training loss: 310.09893798828125 = 0.036958858370780945 + 50.0 * 6.201239585876465
Epoch 2160, val loss: 1.279271125793457
Epoch 2170, training loss: 310.051513671875 = 0.03641683608293533 + 50.0 * 6.2003021240234375
Epoch 2170, val loss: 1.2832673788070679
Epoch 2180, training loss: 310.0265197753906 = 0.035887524485588074 + 50.0 * 6.199812889099121
Epoch 2180, val loss: 1.2872895002365112
Epoch 2190, training loss: 310.2237548828125 = 0.035389941185712814 + 50.0 * 6.2037672996521
Epoch 2190, val loss: 1.291495442390442
Epoch 2200, training loss: 310.0374450683594 = 0.03483755886554718 + 50.0 * 6.200051784515381
Epoch 2200, val loss: 1.29512357711792
Epoch 2210, training loss: 310.0095520019531 = 0.03431762382388115 + 50.0 * 6.199504375457764
Epoch 2210, val loss: 1.299222707748413
Epoch 2220, training loss: 310.0239562988281 = 0.03382444381713867 + 50.0 * 6.199802875518799
Epoch 2220, val loss: 1.3030309677124023
Epoch 2230, training loss: 310.3614807128906 = 0.03336847946047783 + 50.0 * 6.206562519073486
Epoch 2230, val loss: 1.307293176651001
Epoch 2240, training loss: 310.0451354980469 = 0.03284720703959465 + 50.0 * 6.2002458572387695
Epoch 2240, val loss: 1.310632348060608
Epoch 2250, training loss: 309.9792785644531 = 0.032379526644945145 + 50.0 * 6.198937892913818
Epoch 2250, val loss: 1.3147398233413696
Epoch 2260, training loss: 309.99737548828125 = 0.031925879418849945 + 50.0 * 6.199309349060059
Epoch 2260, val loss: 1.31862473487854
Epoch 2270, training loss: 310.15008544921875 = 0.03148382157087326 + 50.0 * 6.202372074127197
Epoch 2270, val loss: 1.322502851486206
Epoch 2280, training loss: 309.9620361328125 = 0.031035346910357475 + 50.0 * 6.198619842529297
Epoch 2280, val loss: 1.3263722658157349
Epoch 2290, training loss: 310.03912353515625 = 0.0306087639182806 + 50.0 * 6.200170516967773
Epoch 2290, val loss: 1.330120325088501
Epoch 2300, training loss: 310.0592956542969 = 0.03017965704202652 + 50.0 * 6.200582504272461
Epoch 2300, val loss: 1.3339614868164062
Epoch 2310, training loss: 309.9415283203125 = 0.029752643778920174 + 50.0 * 6.198235511779785
Epoch 2310, val loss: 1.337798833847046
Epoch 2320, training loss: 309.9334716796875 = 0.029344964772462845 + 50.0 * 6.198081970214844
Epoch 2320, val loss: 1.3415544033050537
Epoch 2330, training loss: 309.9691467285156 = 0.028956297785043716 + 50.0 * 6.198803424835205
Epoch 2330, val loss: 1.345494270324707
Epoch 2340, training loss: 310.20916748046875 = 0.02857024222612381 + 50.0 * 6.203611850738525
Epoch 2340, val loss: 1.3492481708526611
Epoch 2350, training loss: 310.01239013671875 = 0.02815345861017704 + 50.0 * 6.1996846199035645
Epoch 2350, val loss: 1.3524852991104126
Epoch 2360, training loss: 309.9424133300781 = 0.027777180075645447 + 50.0 * 6.1982927322387695
Epoch 2360, val loss: 1.356391191482544
Epoch 2370, training loss: 309.9651794433594 = 0.02741061896085739 + 50.0 * 6.198755741119385
Epoch 2370, val loss: 1.360016942024231
Epoch 2380, training loss: 309.9571838378906 = 0.02704579010605812 + 50.0 * 6.198602676391602
Epoch 2380, val loss: 1.3638025522232056
Epoch 2390, training loss: 309.9350280761719 = 0.026677705347537994 + 50.0 * 6.198166847229004
Epoch 2390, val loss: 1.3672901391983032
Epoch 2400, training loss: 309.9295959472656 = 0.026325782760977745 + 50.0 * 6.198065280914307
Epoch 2400, val loss: 1.370863914489746
Epoch 2410, training loss: 310.0185852050781 = 0.025982743129134178 + 50.0 * 6.199852466583252
Epoch 2410, val loss: 1.3746017217636108
Epoch 2420, training loss: 310.0433044433594 = 0.025644244626164436 + 50.0 * 6.200353622436523
Epoch 2420, val loss: 1.3781557083129883
Epoch 2430, training loss: 310.008544921875 = 0.02530277520418167 + 50.0 * 6.199665069580078
Epoch 2430, val loss: 1.3815251588821411
Epoch 2440, training loss: 309.8238830566406 = 0.02495916187763214 + 50.0 * 6.19597864151001
Epoch 2440, val loss: 1.385108470916748
Epoch 2450, training loss: 309.82440185546875 = 0.024645300582051277 + 50.0 * 6.195994853973389
Epoch 2450, val loss: 1.388574242591858
Epoch 2460, training loss: 309.8883056640625 = 0.0243389755487442 + 50.0 * 6.19727897644043
Epoch 2460, val loss: 1.391912579536438
Epoch 2470, training loss: 310.0197448730469 = 0.02402666211128235 + 50.0 * 6.199914455413818
Epoch 2470, val loss: 1.395438551902771
Epoch 2480, training loss: 309.9269104003906 = 0.02372027188539505 + 50.0 * 6.198063850402832
Epoch 2480, val loss: 1.399169921875
Epoch 2490, training loss: 309.81817626953125 = 0.02341420389711857 + 50.0 * 6.195895195007324
Epoch 2490, val loss: 1.4023863077163696
Epoch 2500, training loss: 309.9017639160156 = 0.02313152700662613 + 50.0 * 6.197572708129883
Epoch 2500, val loss: 1.406148910522461
Epoch 2510, training loss: 309.8805847167969 = 0.02284310944378376 + 50.0 * 6.197154521942139
Epoch 2510, val loss: 1.4092791080474854
Epoch 2520, training loss: 309.8026123046875 = 0.02254728227853775 + 50.0 * 6.195601463317871
Epoch 2520, val loss: 1.4125398397445679
Epoch 2530, training loss: 309.77484130859375 = 0.022270945832133293 + 50.0 * 6.195051193237305
Epoch 2530, val loss: 1.416022777557373
Epoch 2540, training loss: 309.9742431640625 = 0.022009240463376045 + 50.0 * 6.199044704437256
Epoch 2540, val loss: 1.4193435907363892
Epoch 2550, training loss: 309.7633972167969 = 0.021723346784710884 + 50.0 * 6.194833278656006
Epoch 2550, val loss: 1.422563076019287
Epoch 2560, training loss: 309.7953796386719 = 0.021456945687532425 + 50.0 * 6.195478439331055
Epoch 2560, val loss: 1.425833821296692
Epoch 2570, training loss: 309.9444274902344 = 0.021191244944930077 + 50.0 * 6.198464870452881
Epoch 2570, val loss: 1.4291160106658936
Epoch 2580, training loss: 309.7974853515625 = 0.020930452272295952 + 50.0 * 6.195530891418457
Epoch 2580, val loss: 1.432226538658142
Epoch 2590, training loss: 309.750732421875 = 0.02067928947508335 + 50.0 * 6.194601058959961
Epoch 2590, val loss: 1.43556547164917
Epoch 2600, training loss: 309.82373046875 = 0.020435143262147903 + 50.0 * 6.196065425872803
Epoch 2600, val loss: 1.438575029373169
Epoch 2610, training loss: 309.7628173828125 = 0.020190948620438576 + 50.0 * 6.194852352142334
Epoch 2610, val loss: 1.4419125318527222
Epoch 2620, training loss: 309.73675537109375 = 0.01995787024497986 + 50.0 * 6.1943359375
Epoch 2620, val loss: 1.4453219175338745
Epoch 2630, training loss: 309.7131042480469 = 0.019726980477571487 + 50.0 * 6.1938676834106445
Epoch 2630, val loss: 1.4485282897949219
Epoch 2640, training loss: 309.8351745605469 = 0.019503027200698853 + 50.0 * 6.196313381195068
Epoch 2640, val loss: 1.4516041278839111
Epoch 2650, training loss: 309.9313659667969 = 0.019267776980996132 + 50.0 * 6.1982421875
Epoch 2650, val loss: 1.4547297954559326
Epoch 2660, training loss: 309.7176513671875 = 0.019035689532756805 + 50.0 * 6.193972110748291
Epoch 2660, val loss: 1.4576624631881714
Epoch 2670, training loss: 309.6938171386719 = 0.018810298293828964 + 50.0 * 6.193500518798828
Epoch 2670, val loss: 1.4608851671218872
Epoch 2680, training loss: 309.66790771484375 = 0.01860177330672741 + 50.0 * 6.192985534667969
Epoch 2680, val loss: 1.4640647172927856
Epoch 2690, training loss: 309.84344482421875 = 0.018404798582196236 + 50.0 * 6.196500778198242
Epoch 2690, val loss: 1.4673949480056763
Epoch 2700, training loss: 309.6933288574219 = 0.018186278641223907 + 50.0 * 6.193502902984619
Epoch 2700, val loss: 1.4701728820800781
Epoch 2710, training loss: 309.6904602050781 = 0.017970824614167213 + 50.0 * 6.1934494972229
Epoch 2710, val loss: 1.4730932712554932
Epoch 2720, training loss: 309.6560974121094 = 0.017777219414711 + 50.0 * 6.192766189575195
Epoch 2720, val loss: 1.4764018058776855
Epoch 2730, training loss: 309.83843994140625 = 0.017585773020982742 + 50.0 * 6.196417331695557
Epoch 2730, val loss: 1.4795057773590088
Epoch 2740, training loss: 309.66473388671875 = 0.01738394983112812 + 50.0 * 6.192946910858154
Epoch 2740, val loss: 1.4820698499679565
Epoch 2750, training loss: 309.7040100097656 = 0.017190461978316307 + 50.0 * 6.193736553192139
Epoch 2750, val loss: 1.485317349433899
Epoch 2760, training loss: 309.6431884765625 = 0.01700090803205967 + 50.0 * 6.192523956298828
Epoch 2760, val loss: 1.4880372285842896
Epoch 2770, training loss: 309.6427917480469 = 0.016815243288874626 + 50.0 * 6.192519664764404
Epoch 2770, val loss: 1.4911946058273315
Epoch 2780, training loss: 309.72149658203125 = 0.01663590967655182 + 50.0 * 6.19409704208374
Epoch 2780, val loss: 1.4940301179885864
Epoch 2790, training loss: 309.5998840332031 = 0.016452433541417122 + 50.0 * 6.191668510437012
Epoch 2790, val loss: 1.496904969215393
Epoch 2800, training loss: 309.6544189453125 = 0.01628042198717594 + 50.0 * 6.192762851715088
Epoch 2800, val loss: 1.4998371601104736
Epoch 2810, training loss: 309.7750549316406 = 0.016108259558677673 + 50.0 * 6.195178985595703
Epoch 2810, val loss: 1.5026683807373047
Epoch 2820, training loss: 309.6904602050781 = 0.015926701948046684 + 50.0 * 6.193490505218506
Epoch 2820, val loss: 1.5052437782287598
Epoch 2830, training loss: 309.5945129394531 = 0.01575755700469017 + 50.0 * 6.191575050354004
Epoch 2830, val loss: 1.508238434791565
Epoch 2840, training loss: 309.57452392578125 = 0.015591728501021862 + 50.0 * 6.191178321838379
Epoch 2840, val loss: 1.5110019445419312
Epoch 2850, training loss: 310.3143005371094 = 0.015426921658217907 + 50.0 * 6.205977439880371
Epoch 2850, val loss: 1.513074517250061
Epoch 2860, training loss: 309.8390197753906 = 0.0152699314057827 + 50.0 * 6.196475505828857
Epoch 2860, val loss: 1.5166606903076172
Epoch 2870, training loss: 309.6186218261719 = 0.015087781473994255 + 50.0 * 6.192070960998535
Epoch 2870, val loss: 1.5190156698226929
Epoch 2880, training loss: 309.5375061035156 = 0.014940854161977768 + 50.0 * 6.190451145172119
Epoch 2880, val loss: 1.5220165252685547
Epoch 2890, training loss: 309.51715087890625 = 0.014791215769946575 + 50.0 * 6.190046787261963
Epoch 2890, val loss: 1.5247948169708252
Epoch 2900, training loss: 309.5252380371094 = 0.014650637283921242 + 50.0 * 6.190211772918701
Epoch 2900, val loss: 1.5276650190353394
Epoch 2910, training loss: 309.9522399902344 = 0.014508342370390892 + 50.0 * 6.198754787445068
Epoch 2910, val loss: 1.5303572416305542
Epoch 2920, training loss: 309.7288513183594 = 0.014349269680678844 + 50.0 * 6.1942901611328125
Epoch 2920, val loss: 1.5326472520828247
Epoch 2930, training loss: 309.642333984375 = 0.014195759780704975 + 50.0 * 6.192563056945801
Epoch 2930, val loss: 1.5353561639785767
Epoch 2940, training loss: 309.5354919433594 = 0.014056364074349403 + 50.0 * 6.190428256988525
Epoch 2940, val loss: 1.538130283355713
Epoch 2950, training loss: 309.5087585449219 = 0.013919353485107422 + 50.0 * 6.189897060394287
Epoch 2950, val loss: 1.5408220291137695
Epoch 2960, training loss: 309.524658203125 = 0.013787379488348961 + 50.0 * 6.1902174949646
Epoch 2960, val loss: 1.5434937477111816
Epoch 2970, training loss: 309.63104248046875 = 0.01365747768431902 + 50.0 * 6.192347526550293
Epoch 2970, val loss: 1.5460330247879028
Epoch 2980, training loss: 309.48394775390625 = 0.01351715624332428 + 50.0 * 6.189408779144287
Epoch 2980, val loss: 1.5486271381378174
Epoch 2990, training loss: 309.86083984375 = 0.013388312421739101 + 50.0 * 6.196949005126953
Epoch 2990, val loss: 1.5510683059692383
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 431.76641845703125 = 1.9262412786483765 + 50.0 * 8.596803665161133
Epoch 0, val loss: 1.9247961044311523
Epoch 10, training loss: 431.6993408203125 = 1.9178270101547241 + 50.0 * 8.595630645751953
Epoch 10, val loss: 1.916261076927185
Epoch 20, training loss: 431.2752380371094 = 1.90733802318573 + 50.0 * 8.587357521057129
Epoch 20, val loss: 1.905547022819519
Epoch 30, training loss: 428.3255615234375 = 1.8935188055038452 + 50.0 * 8.528640747070312
Epoch 30, val loss: 1.8914718627929688
Epoch 40, training loss: 406.9821472167969 = 1.875977873802185 + 50.0 * 8.102123260498047
Epoch 40, val loss: 1.8736631870269775
Epoch 50, training loss: 372.5155029296875 = 1.855803370475769 + 50.0 * 7.413194179534912
Epoch 50, val loss: 1.854110836982727
Epoch 60, training loss: 361.09454345703125 = 1.8417744636535645 + 50.0 * 7.185055732727051
Epoch 60, val loss: 1.8407386541366577
Epoch 70, training loss: 351.65533447265625 = 1.8313729763031006 + 50.0 * 6.996479511260986
Epoch 70, val loss: 1.8302966356277466
Epoch 80, training loss: 346.5729064941406 = 1.8194727897644043 + 50.0 * 6.895069122314453
Epoch 80, val loss: 1.8190677165985107
Epoch 90, training loss: 343.48529052734375 = 1.8087974786758423 + 50.0 * 6.833529949188232
Epoch 90, val loss: 1.8089960813522339
Epoch 100, training loss: 340.6333312988281 = 1.7984143495559692 + 50.0 * 6.776698589324951
Epoch 100, val loss: 1.7989145517349243
Epoch 110, training loss: 338.5761413574219 = 1.7888004779815674 + 50.0 * 6.735747337341309
Epoch 110, val loss: 1.789267659187317
Epoch 120, training loss: 336.6053771972656 = 1.7791575193405151 + 50.0 * 6.696524620056152
Epoch 120, val loss: 1.7798235416412354
Epoch 130, training loss: 334.12152099609375 = 1.770095705986023 + 50.0 * 6.647028923034668
Epoch 130, val loss: 1.7711806297302246
Epoch 140, training loss: 331.743896484375 = 1.7620666027069092 + 50.0 * 6.599636554718018
Epoch 140, val loss: 1.7634482383728027
Epoch 150, training loss: 329.8691711425781 = 1.7533155679702759 + 50.0 * 6.56231689453125
Epoch 150, val loss: 1.7552801370620728
Epoch 160, training loss: 328.3257751464844 = 1.7432141304016113 + 50.0 * 6.531651020050049
Epoch 160, val loss: 1.7457221746444702
Epoch 170, training loss: 327.131103515625 = 1.7319520711898804 + 50.0 * 6.5079827308654785
Epoch 170, val loss: 1.7354830503463745
Epoch 180, training loss: 325.75335693359375 = 1.7199199199676514 + 50.0 * 6.480669021606445
Epoch 180, val loss: 1.7244586944580078
Epoch 190, training loss: 324.75921630859375 = 1.7068358659744263 + 50.0 * 6.461047172546387
Epoch 190, val loss: 1.7124274969100952
Epoch 200, training loss: 324.2766418457031 = 1.6924660205841064 + 50.0 * 6.451683044433594
Epoch 200, val loss: 1.6992567777633667
Epoch 210, training loss: 323.30474853515625 = 1.6761293411254883 + 50.0 * 6.432572364807129
Epoch 210, val loss: 1.6845546960830688
Epoch 220, training loss: 322.57305908203125 = 1.658690094947815 + 50.0 * 6.41828727722168
Epoch 220, val loss: 1.668929934501648
Epoch 230, training loss: 321.9759521484375 = 1.6400611400604248 + 50.0 * 6.406717777252197
Epoch 230, val loss: 1.652369499206543
Epoch 240, training loss: 321.7945251464844 = 1.620255947113037 + 50.0 * 6.40348482131958
Epoch 240, val loss: 1.634926438331604
Epoch 250, training loss: 320.99359130859375 = 1.5990879535675049 + 50.0 * 6.387889862060547
Epoch 250, val loss: 1.6164051294326782
Epoch 260, training loss: 320.5714111328125 = 1.5769038200378418 + 50.0 * 6.379889965057373
Epoch 260, val loss: 1.597286581993103
Epoch 270, training loss: 320.1140441894531 = 1.5538007020950317 + 50.0 * 6.371204853057861
Epoch 270, val loss: 1.5772974491119385
Epoch 280, training loss: 319.7508544921875 = 1.529807448387146 + 50.0 * 6.3644208908081055
Epoch 280, val loss: 1.5570157766342163
Epoch 290, training loss: 319.5652770996094 = 1.505223274230957 + 50.0 * 6.361201286315918
Epoch 290, val loss: 1.5361542701721191
Epoch 300, training loss: 319.0827941894531 = 1.4797435998916626 + 50.0 * 6.3520612716674805
Epoch 300, val loss: 1.5149562358856201
Epoch 310, training loss: 318.7573547363281 = 1.4538847208023071 + 50.0 * 6.3460693359375
Epoch 310, val loss: 1.4938031435012817
Epoch 320, training loss: 318.7410583496094 = 1.427607536315918 + 50.0 * 6.346269130706787
Epoch 320, val loss: 1.4724993705749512
Epoch 330, training loss: 318.24688720703125 = 1.401175618171692 + 50.0 * 6.3369140625
Epoch 330, val loss: 1.4511685371398926
Epoch 340, training loss: 317.98284912109375 = 1.3744937181472778 + 50.0 * 6.332167625427246
Epoch 340, val loss: 1.4299765825271606
Epoch 350, training loss: 317.9833679199219 = 1.3476678133010864 + 50.0 * 6.332714080810547
Epoch 350, val loss: 1.408744215965271
Epoch 360, training loss: 317.53045654296875 = 1.320674180984497 + 50.0 * 6.324195384979248
Epoch 360, val loss: 1.3877763748168945
Epoch 370, training loss: 317.43572998046875 = 1.2936772108078003 + 50.0 * 6.322841167449951
Epoch 370, val loss: 1.3670796155929565
Epoch 380, training loss: 317.2215270996094 = 1.266686201095581 + 50.0 * 6.319096565246582
Epoch 380, val loss: 1.346448302268982
Epoch 390, training loss: 316.9690856933594 = 1.2398719787597656 + 50.0 * 6.314584255218506
Epoch 390, val loss: 1.3261271715164185
Epoch 400, training loss: 316.8359375 = 1.2131708860397339 + 50.0 * 6.312455177307129
Epoch 400, val loss: 1.306069016456604
Epoch 410, training loss: 316.6805725097656 = 1.1864985227584839 + 50.0 * 6.309881687164307
Epoch 410, val loss: 1.286195158958435
Epoch 420, training loss: 316.4969482421875 = 1.160073161125183 + 50.0 * 6.306737899780273
Epoch 420, val loss: 1.266762614250183
Epoch 430, training loss: 316.42767333984375 = 1.1340196132659912 + 50.0 * 6.305872917175293
Epoch 430, val loss: 1.2476952075958252
Epoch 440, training loss: 316.1671142578125 = 1.10835862159729 + 50.0 * 6.301175117492676
Epoch 440, val loss: 1.2291380167007446
Epoch 450, training loss: 316.10101318359375 = 1.0831502676010132 + 50.0 * 6.300356864929199
Epoch 450, val loss: 1.2110249996185303
Epoch 460, training loss: 315.9067687988281 = 1.0583683252334595 + 50.0 * 6.29696798324585
Epoch 460, val loss: 1.1933046579360962
Epoch 470, training loss: 315.8502502441406 = 1.0340969562530518 + 50.0 * 6.296323299407959
Epoch 470, val loss: 1.176393985748291
Epoch 480, training loss: 315.6462097167969 = 1.010337471961975 + 50.0 * 6.292716979980469
Epoch 480, val loss: 1.159980058670044
Epoch 490, training loss: 315.4652404785156 = 0.9873231053352356 + 50.0 * 6.289558410644531
Epoch 490, val loss: 1.1443703174591064
Epoch 500, training loss: 315.54461669921875 = 0.9648966789245605 + 50.0 * 6.291594505310059
Epoch 500, val loss: 1.1295444965362549
Epoch 510, training loss: 315.34619140625 = 0.9431546926498413 + 50.0 * 6.288060665130615
Epoch 510, val loss: 1.1151893138885498
Epoch 520, training loss: 315.0856628417969 = 0.9220494031906128 + 50.0 * 6.2832722663879395
Epoch 520, val loss: 1.1015716791152954
Epoch 530, training loss: 314.9903869628906 = 0.901654064655304 + 50.0 * 6.281774997711182
Epoch 530, val loss: 1.0888473987579346
Epoch 540, training loss: 314.8809509277344 = 0.8818220496177673 + 50.0 * 6.279982089996338
Epoch 540, val loss: 1.0766717195510864
Epoch 550, training loss: 314.9752502441406 = 0.8625441193580627 + 50.0 * 6.282253742218018
Epoch 550, val loss: 1.0653456449508667
Epoch 560, training loss: 314.6995544433594 = 0.8440238833427429 + 50.0 * 6.277110576629639
Epoch 560, val loss: 1.054388165473938
Epoch 570, training loss: 314.5251770019531 = 0.8260570168495178 + 50.0 * 6.273982524871826
Epoch 570, val loss: 1.0444234609603882
Epoch 580, training loss: 314.47918701171875 = 0.8087847232818604 + 50.0 * 6.27340841293335
Epoch 580, val loss: 1.0350760221481323
Epoch 590, training loss: 314.33258056640625 = 0.791867733001709 + 50.0 * 6.270814418792725
Epoch 590, val loss: 1.0261105298995972
Epoch 600, training loss: 314.2240905761719 = 0.7755048871040344 + 50.0 * 6.2689714431762695
Epoch 600, val loss: 1.0179044008255005
Epoch 610, training loss: 314.37841796875 = 0.7597119808197021 + 50.0 * 6.272374153137207
Epoch 610, val loss: 1.0099737644195557
Epoch 620, training loss: 314.20635986328125 = 0.7442306280136108 + 50.0 * 6.269242763519287
Epoch 620, val loss: 1.0028960704803467
Epoch 630, training loss: 313.97344970703125 = 0.7293432950973511 + 50.0 * 6.2648820877075195
Epoch 630, val loss: 0.996147096157074
Epoch 640, training loss: 313.92950439453125 = 0.7149252891540527 + 50.0 * 6.264291286468506
Epoch 640, val loss: 0.9897944927215576
Epoch 650, training loss: 313.8612060546875 = 0.7008396983146667 + 50.0 * 6.26320743560791
Epoch 650, val loss: 0.9839276075363159
Epoch 660, training loss: 313.7279968261719 = 0.6871375441551208 + 50.0 * 6.260817050933838
Epoch 660, val loss: 0.9785365462303162
Epoch 670, training loss: 313.7455749511719 = 0.6739131212234497 + 50.0 * 6.261433124542236
Epoch 670, val loss: 0.973421573638916
Epoch 680, training loss: 313.62890625 = 0.660858690738678 + 50.0 * 6.2593607902526855
Epoch 680, val loss: 0.9686532616615295
Epoch 690, training loss: 313.5821228027344 = 0.6482367515563965 + 50.0 * 6.258677959442139
Epoch 690, val loss: 0.9641731381416321
Epoch 700, training loss: 313.439697265625 = 0.6358408331871033 + 50.0 * 6.256077289581299
Epoch 700, val loss: 0.9601348638534546
Epoch 710, training loss: 313.5148010253906 = 0.6238201260566711 + 50.0 * 6.257820129394531
Epoch 710, val loss: 0.9562868475914001
Epoch 720, training loss: 313.3276062011719 = 0.6121039986610413 + 50.0 * 6.254310607910156
Epoch 720, val loss: 0.9528126120567322
Epoch 730, training loss: 313.446044921875 = 0.6006443500518799 + 50.0 * 6.256907939910889
Epoch 730, val loss: 0.949587881565094
Epoch 740, training loss: 313.2110900878906 = 0.5892839431762695 + 50.0 * 6.252435684204102
Epoch 740, val loss: 0.9465864300727844
Epoch 750, training loss: 313.119873046875 = 0.5783252716064453 + 50.0 * 6.25083065032959
Epoch 750, val loss: 0.9438742399215698
Epoch 760, training loss: 313.41314697265625 = 0.5675328969955444 + 50.0 * 6.2569122314453125
Epoch 760, val loss: 0.9414569139480591
Epoch 770, training loss: 313.026123046875 = 0.5570291876792908 + 50.0 * 6.2493815422058105
Epoch 770, val loss: 0.9389098882675171
Epoch 780, training loss: 312.9652099609375 = 0.5466997623443604 + 50.0 * 6.24837064743042
Epoch 780, val loss: 0.9368090033531189
Epoch 790, training loss: 312.9377746582031 = 0.536575973033905 + 50.0 * 6.248023509979248
Epoch 790, val loss: 0.9349544644355774
Epoch 800, training loss: 312.87261962890625 = 0.5266696810722351 + 50.0 * 6.246918678283691
Epoch 800, val loss: 0.933232843875885
Epoch 810, training loss: 312.75860595703125 = 0.5169718861579895 + 50.0 * 6.244832992553711
Epoch 810, val loss: 0.9317777752876282
Epoch 820, training loss: 312.88555908203125 = 0.5074014663696289 + 50.0 * 6.247562885284424
Epoch 820, val loss: 0.930476725101471
Epoch 830, training loss: 312.7496032714844 = 0.4979586601257324 + 50.0 * 6.245032787322998
Epoch 830, val loss: 0.9290546178817749
Epoch 840, training loss: 312.63311767578125 = 0.4887026846408844 + 50.0 * 6.242888450622559
Epoch 840, val loss: 0.9278939366340637
Epoch 850, training loss: 312.56805419921875 = 0.4796666204929352 + 50.0 * 6.241767406463623
Epoch 850, val loss: 0.9270003437995911
Epoch 860, training loss: 312.6175537109375 = 0.4707186222076416 + 50.0 * 6.242936611175537
Epoch 860, val loss: 0.9262170195579529
Epoch 870, training loss: 312.62109375 = 0.4618893265724182 + 50.0 * 6.2431840896606445
Epoch 870, val loss: 0.9256632328033447
Epoch 880, training loss: 312.4971008300781 = 0.4532264173030853 + 50.0 * 6.240877628326416
Epoch 880, val loss: 0.9248949289321899
Epoch 890, training loss: 312.40216064453125 = 0.44467979669570923 + 50.0 * 6.239150047302246
Epoch 890, val loss: 0.9247345924377441
Epoch 900, training loss: 312.5252685546875 = 0.436294823884964 + 50.0 * 6.241779327392578
Epoch 900, val loss: 0.9244239926338196
Epoch 910, training loss: 312.3691101074219 = 0.4279424846172333 + 50.0 * 6.238823413848877
Epoch 910, val loss: 0.9241676330566406
Epoch 920, training loss: 312.359619140625 = 0.4197269380092621 + 50.0 * 6.238797664642334
Epoch 920, val loss: 0.924142062664032
Epoch 930, training loss: 312.1945495605469 = 0.41167816519737244 + 50.0 * 6.235657215118408
Epoch 930, val loss: 0.9241074919700623
Epoch 940, training loss: 312.3267517089844 = 0.40371930599212646 + 50.0 * 6.238460540771484
Epoch 940, val loss: 0.9243690967559814
Epoch 950, training loss: 312.1715087890625 = 0.3957819938659668 + 50.0 * 6.2355146408081055
Epoch 950, val loss: 0.9243893623352051
Epoch 960, training loss: 312.12689208984375 = 0.3879987895488739 + 50.0 * 6.234777927398682
Epoch 960, val loss: 0.924821674823761
Epoch 970, training loss: 312.2004089355469 = 0.38033297657966614 + 50.0 * 6.236401557922363
Epoch 970, val loss: 0.9250883460044861
Epoch 980, training loss: 312.1286315917969 = 0.37273189425468445 + 50.0 * 6.2351179122924805
Epoch 980, val loss: 0.925542950630188
Epoch 990, training loss: 311.9587707519531 = 0.365176260471344 + 50.0 * 6.231872081756592
Epoch 990, val loss: 0.9261031746864319
Epoch 1000, training loss: 311.936279296875 = 0.3577519357204437 + 50.0 * 6.231570720672607
Epoch 1000, val loss: 0.9268972277641296
Epoch 1010, training loss: 312.05670166015625 = 0.35044610500335693 + 50.0 * 6.234125137329102
Epoch 1010, val loss: 0.927511990070343
Epoch 1020, training loss: 312.1221923828125 = 0.34320515394210815 + 50.0 * 6.235579967498779
Epoch 1020, val loss: 0.928204357624054
Epoch 1030, training loss: 311.90899658203125 = 0.33593106269836426 + 50.0 * 6.231461048126221
Epoch 1030, val loss: 0.9292037487030029
Epoch 1040, training loss: 311.7831726074219 = 0.32888951897621155 + 50.0 * 6.229085445404053
Epoch 1040, val loss: 0.9301440715789795
Epoch 1050, training loss: 311.7222595214844 = 0.3219294846057892 + 50.0 * 6.228006362915039
Epoch 1050, val loss: 0.9313258528709412
Epoch 1060, training loss: 311.71771240234375 = 0.3150879442691803 + 50.0 * 6.228052616119385
Epoch 1060, val loss: 0.9325098395347595
Epoch 1070, training loss: 312.01861572265625 = 0.3083060681819916 + 50.0 * 6.234206199645996
Epoch 1070, val loss: 0.9334359765052795
Epoch 1080, training loss: 311.7254333496094 = 0.30148544907569885 + 50.0 * 6.228478908538818
Epoch 1080, val loss: 0.9349370002746582
Epoch 1090, training loss: 311.88348388671875 = 0.2947995066642761 + 50.0 * 6.231773853302002
Epoch 1090, val loss: 0.9361861348152161
Epoch 1100, training loss: 311.6706237792969 = 0.28828567266464233 + 50.0 * 6.227646350860596
Epoch 1100, val loss: 0.9374018907546997
Epoch 1110, training loss: 311.5603942871094 = 0.2818353474140167 + 50.0 * 6.225571155548096
Epoch 1110, val loss: 0.9389517307281494
Epoch 1120, training loss: 311.5267639160156 = 0.2755689024925232 + 50.0 * 6.2250237464904785
Epoch 1120, val loss: 0.9404848217964172
Epoch 1130, training loss: 311.7575378417969 = 0.26938268542289734 + 50.0 * 6.229763031005859
Epoch 1130, val loss: 0.941962480545044
Epoch 1140, training loss: 311.56097412109375 = 0.2632300555706024 + 50.0 * 6.225954532623291
Epoch 1140, val loss: 0.9438034296035767
Epoch 1150, training loss: 311.5406188964844 = 0.2572277784347534 + 50.0 * 6.225667953491211
Epoch 1150, val loss: 0.9453794360160828
Epoch 1160, training loss: 311.5760803222656 = 0.2513384521007538 + 50.0 * 6.226494789123535
Epoch 1160, val loss: 0.9472432732582092
Epoch 1170, training loss: 311.435791015625 = 0.24547764658927917 + 50.0 * 6.223805904388428
Epoch 1170, val loss: 0.9492985606193542
Epoch 1180, training loss: 311.33123779296875 = 0.23983031511306763 + 50.0 * 6.221827983856201
Epoch 1180, val loss: 0.9511538743972778
Epoch 1190, training loss: 311.3453063964844 = 0.2343316376209259 + 50.0 * 6.222218990325928
Epoch 1190, val loss: 0.9531688690185547
Epoch 1200, training loss: 311.5005187988281 = 0.2289412021636963 + 50.0 * 6.225431442260742
Epoch 1200, val loss: 0.9552488923072815
Epoch 1210, training loss: 311.4134826660156 = 0.22353003919124603 + 50.0 * 6.223798751831055
Epoch 1210, val loss: 0.9578585028648376
Epoch 1220, training loss: 311.35595703125 = 0.21835437417030334 + 50.0 * 6.222752094268799
Epoch 1220, val loss: 0.9596241116523743
Epoch 1230, training loss: 311.2753601074219 = 0.2132464200258255 + 50.0 * 6.2212419509887695
Epoch 1230, val loss: 0.9622740745544434
Epoch 1240, training loss: 311.2027282714844 = 0.20830591022968292 + 50.0 * 6.219888210296631
Epoch 1240, val loss: 0.9646273851394653
Epoch 1250, training loss: 311.20587158203125 = 0.20346108078956604 + 50.0 * 6.220047950744629
Epoch 1250, val loss: 0.9670273661613464
Epoch 1260, training loss: 311.29376220703125 = 0.19873830676078796 + 50.0 * 6.221900463104248
Epoch 1260, val loss: 0.969792902469635
Epoch 1270, training loss: 311.17041015625 = 0.19407056272029877 + 50.0 * 6.219526767730713
Epoch 1270, val loss: 0.9725884199142456
Epoch 1280, training loss: 311.2920227050781 = 0.18956704437732697 + 50.0 * 6.222048759460449
Epoch 1280, val loss: 0.9752858281135559
Epoch 1290, training loss: 311.0729064941406 = 0.18516549468040466 + 50.0 * 6.217754364013672
Epoch 1290, val loss: 0.9780954122543335
Epoch 1300, training loss: 311.1799011230469 = 0.18090571463108063 + 50.0 * 6.219979763031006
Epoch 1300, val loss: 0.9805894494056702
Epoch 1310, training loss: 311.1109313964844 = 0.17666709423065186 + 50.0 * 6.218685150146484
Epoch 1310, val loss: 0.9838616251945496
Epoch 1320, training loss: 311.0023498535156 = 0.1726108342409134 + 50.0 * 6.216594696044922
Epoch 1320, val loss: 0.9865342974662781
Epoch 1330, training loss: 310.9421691894531 = 0.16865992546081543 + 50.0 * 6.215469837188721
Epoch 1330, val loss: 0.9898226261138916
Epoch 1340, training loss: 310.9255065917969 = 0.1648463010787964 + 50.0 * 6.215212821960449
Epoch 1340, val loss: 0.9930505752563477
Epoch 1350, training loss: 311.1705627441406 = 0.16112250089645386 + 50.0 * 6.220188617706299
Epoch 1350, val loss: 0.9963807463645935
Epoch 1360, training loss: 310.9627990722656 = 0.15744853019714355 + 50.0 * 6.216107368469238
Epoch 1360, val loss: 0.9993975758552551
Epoch 1370, training loss: 310.9264221191406 = 0.1538660228252411 + 50.0 * 6.215450763702393
Epoch 1370, val loss: 1.0026607513427734
Epoch 1380, training loss: 310.9526062011719 = 0.150437131524086 + 50.0 * 6.216042995452881
Epoch 1380, val loss: 1.0060218572616577
Epoch 1390, training loss: 310.9945373535156 = 0.14705289900302887 + 50.0 * 6.216949462890625
Epoch 1390, val loss: 1.0094801187515259
Epoch 1400, training loss: 310.8829040527344 = 0.14373371005058289 + 50.0 * 6.214783191680908
Epoch 1400, val loss: 1.0129560232162476
Epoch 1410, training loss: 310.84027099609375 = 0.14056943356990814 + 50.0 * 6.213994026184082
Epoch 1410, val loss: 1.0164690017700195
Epoch 1420, training loss: 310.9065246582031 = 0.13746295869350433 + 50.0 * 6.215381622314453
Epoch 1420, val loss: 1.0201003551483154
Epoch 1430, training loss: 310.8985900878906 = 0.13442085683345795 + 50.0 * 6.215283393859863
Epoch 1430, val loss: 1.023897409439087
Epoch 1440, training loss: 310.81781005859375 = 0.13148313760757446 + 50.0 * 6.213726997375488
Epoch 1440, val loss: 1.0270856618881226
Epoch 1450, training loss: 310.86651611328125 = 0.12863244116306305 + 50.0 * 6.214757919311523
Epoch 1450, val loss: 1.0307210683822632
Epoch 1460, training loss: 310.7655944824219 = 0.125825434923172 + 50.0 * 6.212795257568359
Epoch 1460, val loss: 1.0346747636795044
Epoch 1470, training loss: 310.8673400878906 = 0.12310303002595901 + 50.0 * 6.2148847579956055
Epoch 1470, val loss: 1.038273811340332
Epoch 1480, training loss: 310.6971740722656 = 0.12045987695455551 + 50.0 * 6.21153450012207
Epoch 1480, val loss: 1.0418843030929565
Epoch 1490, training loss: 310.6717224121094 = 0.11788857728242874 + 50.0 * 6.211076736450195
Epoch 1490, val loss: 1.0459719896316528
Epoch 1500, training loss: 310.9002685546875 = 0.1153714582324028 + 50.0 * 6.2156982421875
Epoch 1500, val loss: 1.049776554107666
Epoch 1510, training loss: 310.6896057128906 = 0.11293119937181473 + 50.0 * 6.211533069610596
Epoch 1510, val loss: 1.0532431602478027
Epoch 1520, training loss: 310.7716369628906 = 0.1105116754770279 + 50.0 * 6.213222503662109
Epoch 1520, val loss: 1.057334065437317
Epoch 1530, training loss: 310.6034240722656 = 0.10818491131067276 + 50.0 * 6.209904670715332
Epoch 1530, val loss: 1.060857892036438
Epoch 1540, training loss: 310.5690002441406 = 0.10593586415052414 + 50.0 * 6.209261417388916
Epoch 1540, val loss: 1.0648852586746216
Epoch 1550, training loss: 310.59844970703125 = 0.1037471741437912 + 50.0 * 6.209893703460693
Epoch 1550, val loss: 1.068832278251648
Epoch 1560, training loss: 310.72698974609375 = 0.10159071534872055 + 50.0 * 6.212508201599121
Epoch 1560, val loss: 1.0726600885391235
Epoch 1570, training loss: 310.5802307128906 = 0.09947612136602402 + 50.0 * 6.2096147537231445
Epoch 1570, val loss: 1.0765467882156372
Epoch 1580, training loss: 310.5436096191406 = 0.09742704033851624 + 50.0 * 6.208923816680908
Epoch 1580, val loss: 1.080603003501892
Epoch 1590, training loss: 310.6847229003906 = 0.09543707966804504 + 50.0 * 6.211785793304443
Epoch 1590, val loss: 1.0842562913894653
Epoch 1600, training loss: 310.55291748046875 = 0.09347370266914368 + 50.0 * 6.209188938140869
Epoch 1600, val loss: 1.0880118608474731
Epoch 1610, training loss: 310.5159606933594 = 0.09155534207820892 + 50.0 * 6.2084879875183105
Epoch 1610, val loss: 1.0921721458435059
Epoch 1620, training loss: 310.53448486328125 = 0.08968429267406464 + 50.0 * 6.208895683288574
Epoch 1620, val loss: 1.0959129333496094
Epoch 1630, training loss: 310.5686340332031 = 0.08783649653196335 + 50.0 * 6.209616184234619
Epoch 1630, val loss: 1.0999014377593994
Epoch 1640, training loss: 310.4969482421875 = 0.08604540675878525 + 50.0 * 6.208217620849609
Epoch 1640, val loss: 1.1035553216934204
Epoch 1650, training loss: 310.43536376953125 = 0.0842997282743454 + 50.0 * 6.207021236419678
Epoch 1650, val loss: 1.1074132919311523
Epoch 1660, training loss: 310.37451171875 = 0.08259659260511398 + 50.0 * 6.205838203430176
Epoch 1660, val loss: 1.1114683151245117
Epoch 1670, training loss: 310.3582458496094 = 0.08095131069421768 + 50.0 * 6.205545902252197
Epoch 1670, val loss: 1.1154639720916748
Epoch 1680, training loss: 310.51031494140625 = 0.07934699952602386 + 50.0 * 6.208619117736816
Epoch 1680, val loss: 1.1195162534713745
Epoch 1690, training loss: 310.53656005859375 = 0.07775460183620453 + 50.0 * 6.209176063537598
Epoch 1690, val loss: 1.12321937084198
Epoch 1700, training loss: 310.4354248046875 = 0.07618656009435654 + 50.0 * 6.2071852684021
Epoch 1700, val loss: 1.126610279083252
Epoch 1710, training loss: 310.3789978027344 = 0.07466292381286621 + 50.0 * 6.206086158752441
Epoch 1710, val loss: 1.1307395696640015
Epoch 1720, training loss: 310.30560302734375 = 0.07320462167263031 + 50.0 * 6.204647541046143
Epoch 1720, val loss: 1.134606957435608
Epoch 1730, training loss: 310.3431396484375 = 0.07178972661495209 + 50.0 * 6.205427169799805
Epoch 1730, val loss: 1.1385228633880615
Epoch 1740, training loss: 310.50946044921875 = 0.07038752734661102 + 50.0 * 6.208781719207764
Epoch 1740, val loss: 1.1425350904464722
Epoch 1750, training loss: 310.37091064453125 = 0.06900940835475922 + 50.0 * 6.206037998199463
Epoch 1750, val loss: 1.1460891962051392
Epoch 1760, training loss: 310.2502136230469 = 0.06766046583652496 + 50.0 * 6.203651428222656
Epoch 1760, val loss: 1.149977445602417
Epoch 1770, training loss: 310.2544250488281 = 0.06636468321084976 + 50.0 * 6.203761100769043
Epoch 1770, val loss: 1.1538993120193481
Epoch 1780, training loss: 310.4149475097656 = 0.0651150792837143 + 50.0 * 6.206996440887451
Epoch 1780, val loss: 1.1578131914138794
Epoch 1790, training loss: 310.32037353515625 = 0.06386817246675491 + 50.0 * 6.205130100250244
Epoch 1790, val loss: 1.161575436592102
Epoch 1800, training loss: 310.26995849609375 = 0.06265784054994583 + 50.0 * 6.204146385192871
Epoch 1800, val loss: 1.1651324033737183
Epoch 1810, training loss: 310.2826843261719 = 0.06147707626223564 + 50.0 * 6.2044243812561035
Epoch 1810, val loss: 1.1690467596054077
Epoch 1820, training loss: 310.2757568359375 = 0.060314297676086426 + 50.0 * 6.204308986663818
Epoch 1820, val loss: 1.1731557846069336
Epoch 1830, training loss: 310.1644592285156 = 0.059189822524785995 + 50.0 * 6.2021050453186035
Epoch 1830, val loss: 1.1766555309295654
Epoch 1840, training loss: 310.16436767578125 = 0.05810736119747162 + 50.0 * 6.202125072479248
Epoch 1840, val loss: 1.1802544593811035
Epoch 1850, training loss: 310.2794494628906 = 0.05705372616648674 + 50.0 * 6.2044477462768555
Epoch 1850, val loss: 1.1838442087173462
Epoch 1860, training loss: 310.2420654296875 = 0.05599546805024147 + 50.0 * 6.203721523284912
Epoch 1860, val loss: 1.1881096363067627
Epoch 1870, training loss: 310.19732666015625 = 0.054963041096925735 + 50.0 * 6.202847003936768
Epoch 1870, val loss: 1.1917787790298462
Epoch 1880, training loss: 310.3551025390625 = 0.0539562925696373 + 50.0 * 6.2060227394104
Epoch 1880, val loss: 1.1953834295272827
Epoch 1890, training loss: 310.14117431640625 = 0.052984245121479034 + 50.0 * 6.201764106750488
Epoch 1890, val loss: 1.1988720893859863
Epoch 1900, training loss: 310.08026123046875 = 0.052032921463251114 + 50.0 * 6.200564384460449
Epoch 1900, val loss: 1.2027676105499268
Epoch 1910, training loss: 310.04388427734375 = 0.051117174327373505 + 50.0 * 6.199855327606201
Epoch 1910, val loss: 1.2063689231872559
Epoch 1920, training loss: 310.2369689941406 = 0.050228413194417953 + 50.0 * 6.203734874725342
Epoch 1920, val loss: 1.2100971937179565
Epoch 1930, training loss: 310.0618896484375 = 0.04932951554656029 + 50.0 * 6.200251579284668
Epoch 1930, val loss: 1.2136269807815552
Epoch 1940, training loss: 310.0852355957031 = 0.04844333603978157 + 50.0 * 6.200736045837402
Epoch 1940, val loss: 1.2170385122299194
Epoch 1950, training loss: 310.056884765625 = 0.04760543256998062 + 50.0 * 6.200185298919678
Epoch 1950, val loss: 1.2207255363464355
Epoch 1960, training loss: 310.11199951171875 = 0.046791546046733856 + 50.0 * 6.2013044357299805
Epoch 1960, val loss: 1.2243348360061646
Epoch 1970, training loss: 310.1273193359375 = 0.045983150601387024 + 50.0 * 6.201626300811768
Epoch 1970, val loss: 1.227910041809082
Epoch 1980, training loss: 310.10784912109375 = 0.04517797380685806 + 50.0 * 6.201253890991211
Epoch 1980, val loss: 1.2316768169403076
Epoch 1990, training loss: 310.2327575683594 = 0.044399622827768326 + 50.0 * 6.203766822814941
Epoch 1990, val loss: 1.2350417375564575
Epoch 2000, training loss: 310.07122802734375 = 0.04364814981818199 + 50.0 * 6.200551509857178
Epoch 2000, val loss: 1.2385095357894897
Epoch 2010, training loss: 310.0580139160156 = 0.04290724918246269 + 50.0 * 6.2003021240234375
Epoch 2010, val loss: 1.2419089078903198
Epoch 2020, training loss: 310.0005798339844 = 0.04218076542019844 + 50.0 * 6.1991682052612305
Epoch 2020, val loss: 1.2457534074783325
Epoch 2030, training loss: 309.9567565917969 = 0.041480761021375656 + 50.0 * 6.198305606842041
Epoch 2030, val loss: 1.249181866645813
Epoch 2040, training loss: 309.9872741699219 = 0.040798626840114594 + 50.0 * 6.198929309844971
Epoch 2040, val loss: 1.252699851989746
Epoch 2050, training loss: 310.1966552734375 = 0.04012639448046684 + 50.0 * 6.203130722045898
Epoch 2050, val loss: 1.2559343576431274
Epoch 2060, training loss: 310.13909912109375 = 0.03945346176624298 + 50.0 * 6.201992511749268
Epoch 2060, val loss: 1.2593039274215698
Epoch 2070, training loss: 309.99432373046875 = 0.03878675773739815 + 50.0 * 6.199110507965088
Epoch 2070, val loss: 1.263037919998169
Epoch 2080, training loss: 309.94189453125 = 0.03815313056111336 + 50.0 * 6.198074817657471
Epoch 2080, val loss: 1.2662510871887207
Epoch 2090, training loss: 309.96881103515625 = 0.03753397986292839 + 50.0 * 6.198625564575195
Epoch 2090, val loss: 1.269806146621704
Epoch 2100, training loss: 309.94769287109375 = 0.03692886605858803 + 50.0 * 6.198215484619141
Epoch 2100, val loss: 1.2730345726013184
Epoch 2110, training loss: 309.9773864746094 = 0.036341797560453415 + 50.0 * 6.1988205909729
Epoch 2110, val loss: 1.27621328830719
Epoch 2120, training loss: 310.0235900878906 = 0.03575565665960312 + 50.0 * 6.199756622314453
Epoch 2120, val loss: 1.2796229124069214
Epoch 2130, training loss: 309.9237976074219 = 0.0351715162396431 + 50.0 * 6.197772026062012
Epoch 2130, val loss: 1.2831063270568848
Epoch 2140, training loss: 309.8783264160156 = 0.03461845964193344 + 50.0 * 6.196874618530273
Epoch 2140, val loss: 1.2862287759780884
Epoch 2150, training loss: 310.264404296875 = 0.03407299146056175 + 50.0 * 6.204606533050537
Epoch 2150, val loss: 1.2899894714355469
Epoch 2160, training loss: 309.9411315917969 = 0.03352201357483864 + 50.0 * 6.198152542114258
Epoch 2160, val loss: 1.2922930717468262
Epoch 2170, training loss: 309.84576416015625 = 0.03299269080162048 + 50.0 * 6.196255207061768
Epoch 2170, val loss: 1.2960296869277954
Epoch 2180, training loss: 309.8299865722656 = 0.03248369321227074 + 50.0 * 6.195950031280518
Epoch 2180, val loss: 1.2990413904190063
Epoch 2190, training loss: 310.1302185058594 = 0.03199920430779457 + 50.0 * 6.201963901519775
Epoch 2190, val loss: 1.301956057548523
Epoch 2200, training loss: 309.8723449707031 = 0.031484875828027725 + 50.0 * 6.196816921234131
Epoch 2200, val loss: 1.3058395385742188
Epoch 2210, training loss: 309.8290100097656 = 0.03099544160068035 + 50.0 * 6.19596004486084
Epoch 2210, val loss: 1.3084477186203003
Epoch 2220, training loss: 309.819580078125 = 0.030522514134645462 + 50.0 * 6.195781230926514
Epoch 2220, val loss: 1.312006950378418
Epoch 2230, training loss: 309.873779296875 = 0.030061369761824608 + 50.0 * 6.196874618530273
Epoch 2230, val loss: 1.3149300813674927
Epoch 2240, training loss: 309.9676208496094 = 0.029602086171507835 + 50.0 * 6.198760509490967
Epoch 2240, val loss: 1.3180040121078491
Epoch 2250, training loss: 309.8278503417969 = 0.029146267101168633 + 50.0 * 6.195974349975586
Epoch 2250, val loss: 1.3212010860443115
Epoch 2260, training loss: 309.775634765625 = 0.028706252574920654 + 50.0 * 6.1949381828308105
Epoch 2260, val loss: 1.3243367671966553
Epoch 2270, training loss: 309.7530517578125 = 0.028281301259994507 + 50.0 * 6.19449520111084
Epoch 2270, val loss: 1.3274116516113281
Epoch 2280, training loss: 309.75274658203125 = 0.02786567620933056 + 50.0 * 6.194497585296631
Epoch 2280, val loss: 1.3305950164794922
Epoch 2290, training loss: 310.1681213378906 = 0.02745949476957321 + 50.0 * 6.202813148498535
Epoch 2290, val loss: 1.333928108215332
Epoch 2300, training loss: 309.9088439941406 = 0.027050726115703583 + 50.0 * 6.197636127471924
Epoch 2300, val loss: 1.3364883661270142
Epoch 2310, training loss: 309.7818603515625 = 0.026638688519597054 + 50.0 * 6.195104598999023
Epoch 2310, val loss: 1.3396238088607788
Epoch 2320, training loss: 309.7923583984375 = 0.026254240423440933 + 50.0 * 6.195322513580322
Epoch 2320, val loss: 1.3427085876464844
Epoch 2330, training loss: 309.9270324707031 = 0.02586880698800087 + 50.0 * 6.198022842407227
Epoch 2330, val loss: 1.3457777500152588
Epoch 2340, training loss: 309.77197265625 = 0.02549687959253788 + 50.0 * 6.194929599761963
Epoch 2340, val loss: 1.348469853401184
Epoch 2350, training loss: 309.7003479003906 = 0.025129208341240883 + 50.0 * 6.193504810333252
Epoch 2350, val loss: 1.3515957593917847
Epoch 2360, training loss: 309.7365417480469 = 0.024779310449957848 + 50.0 * 6.194235324859619
Epoch 2360, val loss: 1.354539394378662
Epoch 2370, training loss: 309.93597412109375 = 0.024428604170680046 + 50.0 * 6.198231220245361
Epoch 2370, val loss: 1.357288122177124
Epoch 2380, training loss: 309.78912353515625 = 0.024071183055639267 + 50.0 * 6.195301055908203
Epoch 2380, val loss: 1.360475778579712
Epoch 2390, training loss: 309.9096374511719 = 0.02373131923377514 + 50.0 * 6.197717666625977
Epoch 2390, val loss: 1.3630726337432861
Epoch 2400, training loss: 309.70709228515625 = 0.0233955979347229 + 50.0 * 6.193673610687256
Epoch 2400, val loss: 1.3660370111465454
Epoch 2410, training loss: 309.730712890625 = 0.02306678518652916 + 50.0 * 6.19415283203125
Epoch 2410, val loss: 1.368691325187683
Epoch 2420, training loss: 309.6935119628906 = 0.02274397946894169 + 50.0 * 6.193415641784668
Epoch 2420, val loss: 1.3717604875564575
Epoch 2430, training loss: 309.6327819824219 = 0.022433947771787643 + 50.0 * 6.192206859588623
Epoch 2430, val loss: 1.3747458457946777
Epoch 2440, training loss: 309.6473083496094 = 0.022133994847536087 + 50.0 * 6.192503929138184
Epoch 2440, val loss: 1.3776086568832397
Epoch 2450, training loss: 309.8531799316406 = 0.021839572116732597 + 50.0 * 6.196626663208008
Epoch 2450, val loss: 1.3805955648422241
Epoch 2460, training loss: 309.7686767578125 = 0.021534033119678497 + 50.0 * 6.194942474365234
Epoch 2460, val loss: 1.3830584287643433
Epoch 2470, training loss: 309.84796142578125 = 0.02124072052538395 + 50.0 * 6.196534156799316
Epoch 2470, val loss: 1.3859128952026367
Epoch 2480, training loss: 309.6927185058594 = 0.020947767421603203 + 50.0 * 6.1934356689453125
Epoch 2480, val loss: 1.3888331651687622
Epoch 2490, training loss: 309.6283264160156 = 0.02066822536289692 + 50.0 * 6.192153453826904
Epoch 2490, val loss: 1.3913382291793823
Epoch 2500, training loss: 309.63360595703125 = 0.020396260544657707 + 50.0 * 6.192264556884766
Epoch 2500, val loss: 1.3941532373428345
Epoch 2510, training loss: 309.7403259277344 = 0.02012961357831955 + 50.0 * 6.194403648376465
Epoch 2510, val loss: 1.39708411693573
Epoch 2520, training loss: 309.6197814941406 = 0.01986398920416832 + 50.0 * 6.191998481750488
Epoch 2520, val loss: 1.3997747898101807
Epoch 2530, training loss: 309.9150085449219 = 0.01960976980626583 + 50.0 * 6.197907447814941
Epoch 2530, val loss: 1.4021761417388916
Epoch 2540, training loss: 309.7621765136719 = 0.019343052059412003 + 50.0 * 6.194856643676758
Epoch 2540, val loss: 1.404528021812439
Epoch 2550, training loss: 309.6287536621094 = 0.01908392272889614 + 50.0 * 6.192193508148193
Epoch 2550, val loss: 1.4076260328292847
Epoch 2560, training loss: 309.5742492675781 = 0.018837925046682358 + 50.0 * 6.191108226776123
Epoch 2560, val loss: 1.410141944885254
Epoch 2570, training loss: 309.5611572265625 = 0.01860228180885315 + 50.0 * 6.190850734710693
Epoch 2570, val loss: 1.4130901098251343
Epoch 2580, training loss: 309.65380859375 = 0.018370039761066437 + 50.0 * 6.192708492279053
Epoch 2580, val loss: 1.415907621383667
Epoch 2590, training loss: 309.7313537597656 = 0.01813429221510887 + 50.0 * 6.1942644119262695
Epoch 2590, val loss: 1.4187567234039307
Epoch 2600, training loss: 309.6716003417969 = 0.017903851345181465 + 50.0 * 6.193073749542236
Epoch 2600, val loss: 1.4207383394241333
Epoch 2610, training loss: 309.552490234375 = 0.017673833295702934 + 50.0 * 6.1906962394714355
Epoch 2610, val loss: 1.4233744144439697
Epoch 2620, training loss: 309.53704833984375 = 0.017460161820054054 + 50.0 * 6.190392017364502
Epoch 2620, val loss: 1.4258348941802979
Epoch 2630, training loss: 309.57232666015625 = 0.017247522249817848 + 50.0 * 6.191101551055908
Epoch 2630, val loss: 1.4284722805023193
Epoch 2640, training loss: 309.7872619628906 = 0.01703832857310772 + 50.0 * 6.195404529571533
Epoch 2640, val loss: 1.431223750114441
Epoch 2650, training loss: 309.62030029296875 = 0.01682533696293831 + 50.0 * 6.1920695304870605
Epoch 2650, val loss: 1.4339998960494995
Epoch 2660, training loss: 309.6310119628906 = 0.016619618982076645 + 50.0 * 6.192287445068359
Epoch 2660, val loss: 1.4363348484039307
Epoch 2670, training loss: 309.68988037109375 = 0.01642291061580181 + 50.0 * 6.193469047546387
Epoch 2670, val loss: 1.4387155771255493
Epoch 2680, training loss: 309.6213073730469 = 0.016223471611738205 + 50.0 * 6.19210147857666
Epoch 2680, val loss: 1.4409334659576416
Epoch 2690, training loss: 309.5722961425781 = 0.01602538861334324 + 50.0 * 6.191125392913818
Epoch 2690, val loss: 1.4438458681106567
Epoch 2700, training loss: 309.5409240722656 = 0.015837104991078377 + 50.0 * 6.190501689910889
Epoch 2700, val loss: 1.446028709411621
Epoch 2710, training loss: 309.5087890625 = 0.015650155022740364 + 50.0 * 6.1898627281188965
Epoch 2710, val loss: 1.4486762285232544
Epoch 2720, training loss: 309.5440979003906 = 0.015468401834368706 + 50.0 * 6.190572261810303
Epoch 2720, val loss: 1.451403021812439
Epoch 2730, training loss: 309.6245422363281 = 0.015287541784346104 + 50.0 * 6.192184925079346
Epoch 2730, val loss: 1.453862190246582
Epoch 2740, training loss: 309.5310363769531 = 0.01510742586106062 + 50.0 * 6.190318584442139
Epoch 2740, val loss: 1.4557501077651978
Epoch 2750, training loss: 309.53387451171875 = 0.014933692291378975 + 50.0 * 6.1903791427612305
Epoch 2750, val loss: 1.458390474319458
Epoch 2760, training loss: 309.650634765625 = 0.014762885868549347 + 50.0 * 6.192717552185059
Epoch 2760, val loss: 1.4606586694717407
Epoch 2770, training loss: 309.52935791015625 = 0.014594022184610367 + 50.0 * 6.190295219421387
Epoch 2770, val loss: 1.4629982709884644
Epoch 2780, training loss: 309.81243896484375 = 0.01443314366042614 + 50.0 * 6.19596004486084
Epoch 2780, val loss: 1.464798092842102
Epoch 2790, training loss: 309.5339660644531 = 0.014258035458624363 + 50.0 * 6.190394401550293
Epoch 2790, val loss: 1.4679901599884033
Epoch 2800, training loss: 309.4683532714844 = 0.014095108024775982 + 50.0 * 6.189085006713867
Epoch 2800, val loss: 1.4697818756103516
Epoch 2810, training loss: 309.4347839355469 = 0.013940847478806973 + 50.0 * 6.188416481018066
Epoch 2810, val loss: 1.472440481185913
Epoch 2820, training loss: 309.4200439453125 = 0.013790133409202099 + 50.0 * 6.188125133514404
Epoch 2820, val loss: 1.4747405052185059
Epoch 2830, training loss: 309.6259765625 = 0.013647863641381264 + 50.0 * 6.192246913909912
Epoch 2830, val loss: 1.4769370555877686
Epoch 2840, training loss: 309.5455627441406 = 0.013492971658706665 + 50.0 * 6.190641403198242
Epoch 2840, val loss: 1.4792743921279907
Epoch 2850, training loss: 309.5062561035156 = 0.013339395634829998 + 50.0 * 6.189858436584473
Epoch 2850, val loss: 1.4817129373550415
Epoch 2860, training loss: 309.435546875 = 0.013189203105866909 + 50.0 * 6.18844747543335
Epoch 2860, val loss: 1.4838578701019287
Epoch 2870, training loss: 309.4217834472656 = 0.013051190413534641 + 50.0 * 6.188174247741699
Epoch 2870, val loss: 1.4860285520553589
Epoch 2880, training loss: 309.5293273925781 = 0.012915524654090405 + 50.0 * 6.190328598022461
Epoch 2880, val loss: 1.4885680675506592
Epoch 2890, training loss: 309.49322509765625 = 0.012776628136634827 + 50.0 * 6.189608573913574
Epoch 2890, val loss: 1.4908543825149536
Epoch 2900, training loss: 309.45025634765625 = 0.012640107423067093 + 50.0 * 6.1887526512146
Epoch 2900, val loss: 1.4927808046340942
Epoch 2910, training loss: 309.4250183105469 = 0.0125077273696661 + 50.0 * 6.1882500648498535
Epoch 2910, val loss: 1.4949758052825928
Epoch 2920, training loss: 309.547119140625 = 0.012380402535200119 + 50.0 * 6.190694332122803
Epoch 2920, val loss: 1.4975203275680542
Epoch 2930, training loss: 309.5284423828125 = 0.012247750535607338 + 50.0 * 6.190324306488037
Epoch 2930, val loss: 1.4992088079452515
Epoch 2940, training loss: 309.3876037597656 = 0.01211513765156269 + 50.0 * 6.187510013580322
Epoch 2940, val loss: 1.501432180404663
Epoch 2950, training loss: 309.3592834472656 = 0.011989488266408443 + 50.0 * 6.186945915222168
Epoch 2950, val loss: 1.5035399198532104
Epoch 2960, training loss: 309.4136962890625 = 0.011870256625115871 + 50.0 * 6.1880364418029785
Epoch 2960, val loss: 1.5058506727218628
Epoch 2970, training loss: 309.6023254394531 = 0.011749804951250553 + 50.0 * 6.191811561584473
Epoch 2970, val loss: 1.507590889930725
Epoch 2980, training loss: 309.42279052734375 = 0.01162930577993393 + 50.0 * 6.188223361968994
Epoch 2980, val loss: 1.5102362632751465
Epoch 2990, training loss: 309.3742980957031 = 0.011512684635818005 + 50.0 * 6.187255859375
Epoch 2990, val loss: 1.5118273496627808
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 431.78350830078125 = 1.9419529438018799 + 50.0 * 8.596831321716309
Epoch 0, val loss: 1.9386632442474365
Epoch 10, training loss: 431.7293701171875 = 1.9333685636520386 + 50.0 * 8.595919609069824
Epoch 10, val loss: 1.930250883102417
Epoch 20, training loss: 431.389892578125 = 1.9225600957870483 + 50.0 * 8.589346885681152
Epoch 20, val loss: 1.9194930791854858
Epoch 30, training loss: 429.09246826171875 = 1.9084581136703491 + 50.0 * 8.543680191040039
Epoch 30, val loss: 1.9053435325622559
Epoch 40, training loss: 415.1094055175781 = 1.8910671472549438 + 50.0 * 8.26436710357666
Epoch 40, val loss: 1.888308048248291
Epoch 50, training loss: 373.92364501953125 = 1.8722597360610962 + 50.0 * 7.441028118133545
Epoch 50, val loss: 1.870319128036499
Epoch 60, training loss: 364.0000915527344 = 1.860235333442688 + 50.0 * 7.242797374725342
Epoch 60, val loss: 1.859468936920166
Epoch 70, training loss: 352.8869934082031 = 1.8494004011154175 + 50.0 * 7.020751953125
Epoch 70, val loss: 1.8489340543746948
Epoch 80, training loss: 345.0521240234375 = 1.8386645317077637 + 50.0 * 6.864269256591797
Epoch 80, val loss: 1.838877558708191
Epoch 90, training loss: 341.07403564453125 = 1.8287177085876465 + 50.0 * 6.784906387329102
Epoch 90, val loss: 1.829391598701477
Epoch 100, training loss: 336.912353515625 = 1.819072961807251 + 50.0 * 6.7018656730651855
Epoch 100, val loss: 1.820320963859558
Epoch 110, training loss: 334.00201416015625 = 1.8103872537612915 + 50.0 * 6.643832206726074
Epoch 110, val loss: 1.8122003078460693
Epoch 120, training loss: 331.4157409667969 = 1.802439570426941 + 50.0 * 6.592266082763672
Epoch 120, val loss: 1.804460883140564
Epoch 130, training loss: 329.41326904296875 = 1.7947235107421875 + 50.0 * 6.552371501922607
Epoch 130, val loss: 1.7969731092453003
Epoch 140, training loss: 327.9620056152344 = 1.7867376804351807 + 50.0 * 6.523505210876465
Epoch 140, val loss: 1.7891626358032227
Epoch 150, training loss: 326.6737060546875 = 1.7782955169677734 + 50.0 * 6.497908115386963
Epoch 150, val loss: 1.780890703201294
Epoch 160, training loss: 325.5411071777344 = 1.7691744565963745 + 50.0 * 6.475438594818115
Epoch 160, val loss: 1.7721761465072632
Epoch 170, training loss: 324.69012451171875 = 1.7593661546707153 + 50.0 * 6.458614826202393
Epoch 170, val loss: 1.7628647089004517
Epoch 180, training loss: 323.7215576171875 = 1.7489088773727417 + 50.0 * 6.439453125
Epoch 180, val loss: 1.752948522567749
Epoch 190, training loss: 322.9869689941406 = 1.7375328540802002 + 50.0 * 6.424989223480225
Epoch 190, val loss: 1.742354154586792
Epoch 200, training loss: 322.33026123046875 = 1.725062370300293 + 50.0 * 6.41210412979126
Epoch 200, val loss: 1.730800986289978
Epoch 210, training loss: 321.8702087402344 = 1.7113101482391357 + 50.0 * 6.4031782150268555
Epoch 210, val loss: 1.718052625656128
Epoch 220, training loss: 321.3731689453125 = 1.6961820125579834 + 50.0 * 6.393539905548096
Epoch 220, val loss: 1.704052209854126
Epoch 230, training loss: 320.9135437011719 = 1.6796972751617432 + 50.0 * 6.384677410125732
Epoch 230, val loss: 1.688821792602539
Epoch 240, training loss: 320.5794677734375 = 1.6618560552597046 + 50.0 * 6.378352165222168
Epoch 240, val loss: 1.67236328125
Epoch 250, training loss: 320.1830139160156 = 1.6425285339355469 + 50.0 * 6.370810031890869
Epoch 250, val loss: 1.6544417142868042
Epoch 260, training loss: 320.006103515625 = 1.6219515800476074 + 50.0 * 6.367683410644531
Epoch 260, val loss: 1.6354351043701172
Epoch 270, training loss: 319.491455078125 = 1.5998191833496094 + 50.0 * 6.357832908630371
Epoch 270, val loss: 1.6153385639190674
Epoch 280, training loss: 319.1552429199219 = 1.5766384601593018 + 50.0 * 6.351572513580322
Epoch 280, val loss: 1.5942152738571167
Epoch 290, training loss: 318.9057312011719 = 1.5522836446762085 + 50.0 * 6.347069263458252
Epoch 290, val loss: 1.5722273588180542
Epoch 300, training loss: 318.6563415527344 = 1.52683687210083 + 50.0 * 6.34259033203125
Epoch 300, val loss: 1.5492740869522095
Epoch 310, training loss: 318.2935791015625 = 1.500644326210022 + 50.0 * 6.3358588218688965
Epoch 310, val loss: 1.5260051488876343
Epoch 320, training loss: 318.0630798339844 = 1.4738085269927979 + 50.0 * 6.331785202026367
Epoch 320, val loss: 1.5022735595703125
Epoch 330, training loss: 317.9677734375 = 1.4464595317840576 + 50.0 * 6.33042573928833
Epoch 330, val loss: 1.4782624244689941
Epoch 340, training loss: 317.6783752441406 = 1.418877124786377 + 50.0 * 6.325189590454102
Epoch 340, val loss: 1.4542467594146729
Epoch 350, training loss: 317.38446044921875 = 1.391250491142273 + 50.0 * 6.319864273071289
Epoch 350, val loss: 1.430355429649353
Epoch 360, training loss: 317.3801574707031 = 1.3636305332183838 + 50.0 * 6.32033109664917
Epoch 360, val loss: 1.406591534614563
Epoch 370, training loss: 316.9727478027344 = 1.3360716104507446 + 50.0 * 6.3127336502075195
Epoch 370, val loss: 1.3833763599395752
Epoch 380, training loss: 316.7933349609375 = 1.308761477470398 + 50.0 * 6.309691905975342
Epoch 380, val loss: 1.3604456186294556
Epoch 390, training loss: 317.14569091796875 = 1.2815061807632446 + 50.0 * 6.317284107208252
Epoch 390, val loss: 1.337724208831787
Epoch 400, training loss: 316.5667724609375 = 1.254574179649353 + 50.0 * 6.306243896484375
Epoch 400, val loss: 1.3152682781219482
Epoch 410, training loss: 316.2704772949219 = 1.2280784845352173 + 50.0 * 6.300848007202148
Epoch 410, val loss: 1.2933809757232666
Epoch 420, training loss: 316.1098937988281 = 1.2018522024154663 + 50.0 * 6.298161029815674
Epoch 420, val loss: 1.2719327211380005
Epoch 430, training loss: 316.0330505371094 = 1.1759183406829834 + 50.0 * 6.29714298248291
Epoch 430, val loss: 1.250801682472229
Epoch 440, training loss: 316.0341796875 = 1.1501611471176147 + 50.0 * 6.297680377960205
Epoch 440, val loss: 1.2301270961761475
Epoch 450, training loss: 315.73785400390625 = 1.1248584985733032 + 50.0 * 6.29226016998291
Epoch 450, val loss: 1.2096872329711914
Epoch 460, training loss: 315.5845947265625 = 1.099947214126587 + 50.0 * 6.2896928787231445
Epoch 460, val loss: 1.189711093902588
Epoch 470, training loss: 315.66253662109375 = 1.075513243675232 + 50.0 * 6.291740417480469
Epoch 470, val loss: 1.1701910495758057
Epoch 480, training loss: 315.4174499511719 = 1.051329493522644 + 50.0 * 6.287322521209717
Epoch 480, val loss: 1.151371955871582
Epoch 490, training loss: 315.1849060058594 = 1.027732014656067 + 50.0 * 6.283143043518066
Epoch 490, val loss: 1.132893443107605
Epoch 500, training loss: 315.2453918457031 = 1.0046573877334595 + 50.0 * 6.284814357757568
Epoch 500, val loss: 1.1149228811264038
Epoch 510, training loss: 315.1326904296875 = 0.9818504452705383 + 50.0 * 6.283016681671143
Epoch 510, val loss: 1.0971052646636963
Epoch 520, training loss: 314.8913879394531 = 0.9596313834190369 + 50.0 * 6.278635501861572
Epoch 520, val loss: 1.0803875923156738
Epoch 530, training loss: 314.7796936035156 = 0.937966525554657 + 50.0 * 6.276834487915039
Epoch 530, val loss: 1.0639002323150635
Epoch 540, training loss: 314.7109680175781 = 0.9168121218681335 + 50.0 * 6.275883197784424
Epoch 540, val loss: 1.0482603311538696
Epoch 550, training loss: 314.6680603027344 = 0.8961109519004822 + 50.0 * 6.2754387855529785
Epoch 550, val loss: 1.03276526927948
Epoch 560, training loss: 314.5523986816406 = 0.8760088086128235 + 50.0 * 6.273528099060059
Epoch 560, val loss: 1.018417477607727
Epoch 570, training loss: 314.4306640625 = 0.8562794327735901 + 50.0 * 6.271487712860107
Epoch 570, val loss: 1.0041515827178955
Epoch 580, training loss: 314.2795715332031 = 0.8371219038963318 + 50.0 * 6.268848896026611
Epoch 580, val loss: 0.9908355474472046
Epoch 590, training loss: 314.18536376953125 = 0.8184975981712341 + 50.0 * 6.267337322235107
Epoch 590, val loss: 0.9780048727989197
Epoch 600, training loss: 314.5282287597656 = 0.8002605438232422 + 50.0 * 6.274559020996094
Epoch 600, val loss: 0.9658946990966797
Epoch 610, training loss: 314.0530700683594 = 0.7822838425636292 + 50.0 * 6.265415668487549
Epoch 610, val loss: 0.9535269141197205
Epoch 620, training loss: 313.9935607910156 = 0.7648929953575134 + 50.0 * 6.264573097229004
Epoch 620, val loss: 0.9421646595001221
Epoch 630, training loss: 313.9055480957031 = 0.7479861378669739 + 50.0 * 6.263151168823242
Epoch 630, val loss: 0.9313521981239319
Epoch 640, training loss: 313.9537658691406 = 0.7314524054527283 + 50.0 * 6.264446258544922
Epoch 640, val loss: 0.9211786389350891
Epoch 650, training loss: 313.7696228027344 = 0.7152025103569031 + 50.0 * 6.2610883712768555
Epoch 650, val loss: 0.9112758040428162
Epoch 660, training loss: 313.7175598144531 = 0.699465274810791 + 50.0 * 6.260362148284912
Epoch 660, val loss: 0.9019787907600403
Epoch 670, training loss: 313.5808410644531 = 0.6840593218803406 + 50.0 * 6.257935523986816
Epoch 670, val loss: 0.8930913209915161
Epoch 680, training loss: 313.5479736328125 = 0.6690418124198914 + 50.0 * 6.2575788497924805
Epoch 680, val loss: 0.8849208950996399
Epoch 690, training loss: 313.6670227050781 = 0.6543878316879272 + 50.0 * 6.260252475738525
Epoch 690, val loss: 0.8769353032112122
Epoch 700, training loss: 313.4287109375 = 0.6399505138397217 + 50.0 * 6.255775451660156
Epoch 700, val loss: 0.8692067265510559
Epoch 710, training loss: 313.3705749511719 = 0.6259388327598572 + 50.0 * 6.254892826080322
Epoch 710, val loss: 0.8621376156806946
Epoch 720, training loss: 313.2809753417969 = 0.6122271418571472 + 50.0 * 6.253375053405762
Epoch 720, val loss: 0.855499267578125
Epoch 730, training loss: 313.2101135253906 = 0.5989264249801636 + 50.0 * 6.252223491668701
Epoch 730, val loss: 0.8492294549942017
Epoch 740, training loss: 313.421142578125 = 0.5859480500221252 + 50.0 * 6.256704330444336
Epoch 740, val loss: 0.8435147404670715
Epoch 750, training loss: 313.195068359375 = 0.5731099247932434 + 50.0 * 6.252439022064209
Epoch 750, val loss: 0.8376938700675964
Epoch 760, training loss: 313.0783996582031 = 0.5607165098190308 + 50.0 * 6.2503533363342285
Epoch 760, val loss: 0.8327147960662842
Epoch 770, training loss: 313.0016174316406 = 0.5485917329788208 + 50.0 * 6.24906063079834
Epoch 770, val loss: 0.8281920552253723
Epoch 780, training loss: 313.1457214355469 = 0.536851704120636 + 50.0 * 6.2521772384643555
Epoch 780, val loss: 0.8239619135856628
Epoch 790, training loss: 312.9432373046875 = 0.5252802968025208 + 50.0 * 6.248359203338623
Epoch 790, val loss: 0.8195939064025879
Epoch 800, training loss: 312.8222961425781 = 0.5140791535377502 + 50.0 * 6.246164798736572
Epoch 800, val loss: 0.816148579120636
Epoch 810, training loss: 312.7762451171875 = 0.5031474828720093 + 50.0 * 6.245461940765381
Epoch 810, val loss: 0.8128522634506226
Epoch 820, training loss: 312.8887634277344 = 0.4925172030925751 + 50.0 * 6.2479248046875
Epoch 820, val loss: 0.8098360300064087
Epoch 830, training loss: 312.8592529296875 = 0.4820885956287384 + 50.0 * 6.2475433349609375
Epoch 830, val loss: 0.8070330023765564
Epoch 840, training loss: 312.6571960449219 = 0.4718897044658661 + 50.0 * 6.243706226348877
Epoch 840, val loss: 0.8050557971000671
Epoch 850, training loss: 312.6146240234375 = 0.4620397090911865 + 50.0 * 6.243051528930664
Epoch 850, val loss: 0.803013265132904
Epoch 860, training loss: 312.7193298339844 = 0.45242905616760254 + 50.0 * 6.245337963104248
Epoch 860, val loss: 0.8013559579849243
Epoch 870, training loss: 312.7119445800781 = 0.44304659962654114 + 50.0 * 6.245377540588379
Epoch 870, val loss: 0.7998261451721191
Epoch 880, training loss: 312.4721984863281 = 0.43380236625671387 + 50.0 * 6.240767955780029
Epoch 880, val loss: 0.7987497448921204
Epoch 890, training loss: 312.37530517578125 = 0.4248889088630676 + 50.0 * 6.239007949829102
Epoch 890, val loss: 0.7977844476699829
Epoch 900, training loss: 312.3388366699219 = 0.4162457287311554 + 50.0 * 6.2384514808654785
Epoch 900, val loss: 0.7971919775009155
Epoch 910, training loss: 312.33038330078125 = 0.4078006446361542 + 50.0 * 6.2384514808654785
Epoch 910, val loss: 0.7968378067016602
Epoch 920, training loss: 312.4578552246094 = 0.3994767963886261 + 50.0 * 6.2411675453186035
Epoch 920, val loss: 0.7965845465660095
Epoch 930, training loss: 312.2436828613281 = 0.3913228511810303 + 50.0 * 6.23704719543457
Epoch 930, val loss: 0.7965301275253296
Epoch 940, training loss: 312.2251281738281 = 0.38338395953178406 + 50.0 * 6.23683500289917
Epoch 940, val loss: 0.7971111536026001
Epoch 950, training loss: 312.1676025390625 = 0.37567609548568726 + 50.0 * 6.235838890075684
Epoch 950, val loss: 0.7975724935531616
Epoch 960, training loss: 312.4331359863281 = 0.36808308959007263 + 50.0 * 6.241301536560059
Epoch 960, val loss: 0.7980266809463501
Epoch 970, training loss: 312.11712646484375 = 0.360638290643692 + 50.0 * 6.2351298332214355
Epoch 970, val loss: 0.7989434003829956
Epoch 980, training loss: 312.091064453125 = 0.35338094830513 + 50.0 * 6.234753608703613
Epoch 980, val loss: 0.7999340891838074
Epoch 990, training loss: 312.0272521972656 = 0.3462944030761719 + 50.0 * 6.233619689941406
Epoch 990, val loss: 0.8011611700057983
Epoch 1000, training loss: 312.1109619140625 = 0.33936798572540283 + 50.0 * 6.235432147979736
Epoch 1000, val loss: 0.8024137020111084
Epoch 1010, training loss: 312.24798583984375 = 0.33249786496162415 + 50.0 * 6.238309860229492
Epoch 1010, val loss: 0.8036154508590698
Epoch 1020, training loss: 312.1705322265625 = 0.32573699951171875 + 50.0 * 6.236895561218262
Epoch 1020, val loss: 0.8053830862045288
Epoch 1030, training loss: 311.92694091796875 = 0.3191039562225342 + 50.0 * 6.232157230377197
Epoch 1030, val loss: 0.8072279691696167
Epoch 1040, training loss: 311.88433837890625 = 0.3126419186592102 + 50.0 * 6.231433868408203
Epoch 1040, val loss: 0.8086656332015991
Epoch 1050, training loss: 311.8203125 = 0.306356281042099 + 50.0 * 6.230278968811035
Epoch 1050, val loss: 0.81080561876297
Epoch 1060, training loss: 311.7989501953125 = 0.3002113997936249 + 50.0 * 6.229974746704102
Epoch 1060, val loss: 0.8128693699836731
Epoch 1070, training loss: 311.830078125 = 0.29415687918663025 + 50.0 * 6.23071813583374
Epoch 1070, val loss: 0.8152312636375427
Epoch 1080, training loss: 311.7679138183594 = 0.28815263509750366 + 50.0 * 6.229595184326172
Epoch 1080, val loss: 0.8173118829727173
Epoch 1090, training loss: 311.7182312011719 = 0.28224697709083557 + 50.0 * 6.228719711303711
Epoch 1090, val loss: 0.8194547891616821
Epoch 1100, training loss: 311.7356872558594 = 0.2764669358730316 + 50.0 * 6.229184627532959
Epoch 1100, val loss: 0.8218803405761719
Epoch 1110, training loss: 311.760498046875 = 0.27081605792045593 + 50.0 * 6.229793548583984
Epoch 1110, val loss: 0.8243513107299805
Epoch 1120, training loss: 311.7218933105469 = 0.26526305079460144 + 50.0 * 6.229132652282715
Epoch 1120, val loss: 0.8266357779502869
Epoch 1130, training loss: 311.6423034667969 = 0.2597910165786743 + 50.0 * 6.227650165557861
Epoch 1130, val loss: 0.8294774889945984
Epoch 1140, training loss: 311.85107421875 = 0.2544533312320709 + 50.0 * 6.231932640075684
Epoch 1140, val loss: 0.8317421078681946
Epoch 1150, training loss: 311.62567138671875 = 0.24914462864398956 + 50.0 * 6.227530479431152
Epoch 1150, val loss: 0.8349929451942444
Epoch 1160, training loss: 311.5354309082031 = 0.2440091073513031 + 50.0 * 6.225828170776367
Epoch 1160, val loss: 0.8375216722488403
Epoch 1170, training loss: 311.48388671875 = 0.23898668587207794 + 50.0 * 6.224897861480713
Epoch 1170, val loss: 0.840631365776062
Epoch 1180, training loss: 311.55108642578125 = 0.2340734750032425 + 50.0 * 6.226340293884277
Epoch 1180, val loss: 0.8434842824935913
Epoch 1190, training loss: 311.5729064941406 = 0.22916443645954132 + 50.0 * 6.226875305175781
Epoch 1190, val loss: 0.8464130759239197
Epoch 1200, training loss: 311.5086975097656 = 0.22437922656536102 + 50.0 * 6.225686550140381
Epoch 1200, val loss: 0.8495429754257202
Epoch 1210, training loss: 311.4093017578125 = 0.2196974903345108 + 50.0 * 6.22379207611084
Epoch 1210, val loss: 0.8526061773300171
Epoch 1220, training loss: 311.3680419921875 = 0.21516528725624084 + 50.0 * 6.223057270050049
Epoch 1220, val loss: 0.8557096719741821
Epoch 1230, training loss: 311.3970947265625 = 0.210732564330101 + 50.0 * 6.223727226257324
Epoch 1230, val loss: 0.858942985534668
Epoch 1240, training loss: 311.4652404785156 = 0.20635837316513062 + 50.0 * 6.225177764892578
Epoch 1240, val loss: 0.8622403740882874
Epoch 1250, training loss: 311.3520812988281 = 0.2020353525876999 + 50.0 * 6.223001003265381
Epoch 1250, val loss: 0.8655962347984314
Epoch 1260, training loss: 311.3772888183594 = 0.19782795011997223 + 50.0 * 6.2235894203186035
Epoch 1260, val loss: 0.8689992427825928
Epoch 1270, training loss: 311.4745788574219 = 0.19371533393859863 + 50.0 * 6.225616931915283
Epoch 1270, val loss: 0.8719599843025208
Epoch 1280, training loss: 311.28759765625 = 0.18959735333919525 + 50.0 * 6.221959590911865
Epoch 1280, val loss: 0.8756815791130066
Epoch 1290, training loss: 311.23309326171875 = 0.18565863370895386 + 50.0 * 6.220948219299316
Epoch 1290, val loss: 0.8794541358947754
Epoch 1300, training loss: 311.20941162109375 = 0.18176215887069702 + 50.0 * 6.220552921295166
Epoch 1300, val loss: 0.8830880522727966
Epoch 1310, training loss: 311.44097900390625 = 0.17796659469604492 + 50.0 * 6.225259780883789
Epoch 1310, val loss: 0.8872101902961731
Epoch 1320, training loss: 311.2549743652344 = 0.1741669476032257 + 50.0 * 6.221616268157959
Epoch 1320, val loss: 0.8903152346611023
Epoch 1330, training loss: 311.1846618652344 = 0.17053130269050598 + 50.0 * 6.220282554626465
Epoch 1330, val loss: 0.8939972519874573
Epoch 1340, training loss: 311.3345031738281 = 0.1669907122850418 + 50.0 * 6.2233500480651855
Epoch 1340, val loss: 0.8979449272155762
Epoch 1350, training loss: 311.1662902832031 = 0.16347777843475342 + 50.0 * 6.220056056976318
Epoch 1350, val loss: 0.9016968011856079
Epoch 1360, training loss: 311.1968078613281 = 0.1600707322359085 + 50.0 * 6.220734596252441
Epoch 1360, val loss: 0.9056088924407959
Epoch 1370, training loss: 311.11956787109375 = 0.15673446655273438 + 50.0 * 6.219256401062012
Epoch 1370, val loss: 0.9095855951309204
Epoch 1380, training loss: 311.036376953125 = 0.1535472273826599 + 50.0 * 6.21765661239624
Epoch 1380, val loss: 0.9136092662811279
Epoch 1390, training loss: 311.0244445800781 = 0.15042364597320557 + 50.0 * 6.217480659484863
Epoch 1390, val loss: 0.9176287055015564
Epoch 1400, training loss: 311.19207763671875 = 0.14740927517414093 + 50.0 * 6.220893383026123
Epoch 1400, val loss: 0.9215078353881836
Epoch 1410, training loss: 311.0878601074219 = 0.1443719118833542 + 50.0 * 6.218869686126709
Epoch 1410, val loss: 0.9256991147994995
Epoch 1420, training loss: 311.0471496582031 = 0.14144010841846466 + 50.0 * 6.218113899230957
Epoch 1420, val loss: 0.9298909902572632
Epoch 1430, training loss: 311.06982421875 = 0.13856589794158936 + 50.0 * 6.218624591827393
Epoch 1430, val loss: 0.9343463778495789
Epoch 1440, training loss: 310.9927978515625 = 0.13580110669136047 + 50.0 * 6.217140197753906
Epoch 1440, val loss: 0.9382902979850769
Epoch 1450, training loss: 310.93048095703125 = 0.13309408724308014 + 50.0 * 6.21594762802124
Epoch 1450, val loss: 0.9425947666168213
Epoch 1460, training loss: 310.9716796875 = 0.13047252595424652 + 50.0 * 6.216824054718018
Epoch 1460, val loss: 0.9468497037887573
Epoch 1470, training loss: 310.9615783691406 = 0.1278824657201767 + 50.0 * 6.216674327850342
Epoch 1470, val loss: 0.951172411441803
Epoch 1480, training loss: 311.11334228515625 = 0.1253686398267746 + 50.0 * 6.219759464263916
Epoch 1480, val loss: 0.9554154276847839
Epoch 1490, training loss: 310.8973083496094 = 0.12289648503065109 + 50.0 * 6.215488433837891
Epoch 1490, val loss: 0.9596523642539978
Epoch 1500, training loss: 310.8862609863281 = 0.12049631774425507 + 50.0 * 6.215315818786621
Epoch 1500, val loss: 0.9639331698417664
Epoch 1510, training loss: 311.0207824707031 = 0.11818520724773407 + 50.0 * 6.218051910400391
Epoch 1510, val loss: 0.9679492115974426
Epoch 1520, training loss: 310.90283203125 = 0.1158771812915802 + 50.0 * 6.2157392501831055
Epoch 1520, val loss: 0.9730109572410583
Epoch 1530, training loss: 310.8499450683594 = 0.11365974694490433 + 50.0 * 6.214725971221924
Epoch 1530, val loss: 0.9769552946090698
Epoch 1540, training loss: 310.7635498046875 = 0.11147255450487137 + 50.0 * 6.213041305541992
Epoch 1540, val loss: 0.9816170930862427
Epoch 1550, training loss: 310.78045654296875 = 0.10937175154685974 + 50.0 * 6.21342134475708
Epoch 1550, val loss: 0.98614501953125
Epoch 1560, training loss: 311.0677490234375 = 0.10730704665184021 + 50.0 * 6.219208717346191
Epoch 1560, val loss: 0.9903578162193298
Epoch 1570, training loss: 310.77294921875 = 0.10525122284889221 + 50.0 * 6.213353633880615
Epoch 1570, val loss: 0.994910478591919
Epoch 1580, training loss: 310.7085266113281 = 0.10326957702636719 + 50.0 * 6.2121052742004395
Epoch 1580, val loss: 0.9995375871658325
Epoch 1590, training loss: 310.8678894042969 = 0.10135691612958908 + 50.0 * 6.215331077575684
Epoch 1590, val loss: 1.004228949546814
Epoch 1600, training loss: 310.7044677734375 = 0.09945891797542572 + 50.0 * 6.212100028991699
Epoch 1600, val loss: 1.0084807872772217
Epoch 1610, training loss: 311.0169677734375 = 0.09759803861379623 + 50.0 * 6.218387603759766
Epoch 1610, val loss: 1.0133411884307861
Epoch 1620, training loss: 310.7534484863281 = 0.09579403698444366 + 50.0 * 6.213152885437012
Epoch 1620, val loss: 1.0170443058013916
Epoch 1630, training loss: 310.65814208984375 = 0.09401831030845642 + 50.0 * 6.211282253265381
Epoch 1630, val loss: 1.022006630897522
Epoch 1640, training loss: 310.6244812011719 = 0.09232573956251144 + 50.0 * 6.210643291473389
Epoch 1640, val loss: 1.0263704061508179
Epoch 1650, training loss: 310.82989501953125 = 0.09067588299512863 + 50.0 * 6.214784622192383
Epoch 1650, val loss: 1.0312857627868652
Epoch 1660, training loss: 310.6262512207031 = 0.0890204980969429 + 50.0 * 6.210744857788086
Epoch 1660, val loss: 1.0354766845703125
Epoch 1670, training loss: 310.65673828125 = 0.08740723133087158 + 50.0 * 6.211386680603027
Epoch 1670, val loss: 1.0403871536254883
Epoch 1680, training loss: 310.85223388671875 = 0.08583443611860275 + 50.0 * 6.215327739715576
Epoch 1680, val loss: 1.044403076171875
Epoch 1690, training loss: 310.63421630859375 = 0.08428534120321274 + 50.0 * 6.21099853515625
Epoch 1690, val loss: 1.0488662719726562
Epoch 1700, training loss: 310.55950927734375 = 0.08276918530464172 + 50.0 * 6.209535121917725
Epoch 1700, val loss: 1.0536335706710815
Epoch 1710, training loss: 310.5124206542969 = 0.0813153013586998 + 50.0 * 6.208621978759766
Epoch 1710, val loss: 1.0580259561538696
Epoch 1720, training loss: 310.5032958984375 = 0.07989101111888885 + 50.0 * 6.208467960357666
Epoch 1720, val loss: 1.062637448310852
Epoch 1730, training loss: 310.7721252441406 = 0.07851525396108627 + 50.0 * 6.213871955871582
Epoch 1730, val loss: 1.067039966583252
Epoch 1740, training loss: 310.59417724609375 = 0.0771096870303154 + 50.0 * 6.210340976715088
Epoch 1740, val loss: 1.0719209909439087
Epoch 1750, training loss: 310.60894775390625 = 0.07574114203453064 + 50.0 * 6.210663795471191
Epoch 1750, val loss: 1.076343059539795
Epoch 1760, training loss: 310.4830627441406 = 0.07441937178373337 + 50.0 * 6.20817232131958
Epoch 1760, val loss: 1.0807011127471924
Epoch 1770, training loss: 310.4760437011719 = 0.07313177734613419 + 50.0 * 6.2080583572387695
Epoch 1770, val loss: 1.085175633430481
Epoch 1780, training loss: 310.6435241699219 = 0.07188289612531662 + 50.0 * 6.211433410644531
Epoch 1780, val loss: 1.0898324251174927
Epoch 1790, training loss: 310.470947265625 = 0.07063092291355133 + 50.0 * 6.208006858825684
Epoch 1790, val loss: 1.0944188833236694
Epoch 1800, training loss: 310.5552062988281 = 0.06941739469766617 + 50.0 * 6.209715366363525
Epoch 1800, val loss: 1.0990769863128662
Epoch 1810, training loss: 310.4852294921875 = 0.06821062415838242 + 50.0 * 6.208340167999268
Epoch 1810, val loss: 1.1029773950576782
Epoch 1820, training loss: 310.571533203125 = 0.06704869866371155 + 50.0 * 6.210089683532715
Epoch 1820, val loss: 1.1073017120361328
Epoch 1830, training loss: 310.3930969238281 = 0.06588618457317352 + 50.0 * 6.206544399261475
Epoch 1830, val loss: 1.1121227741241455
Epoch 1840, training loss: 310.3861389160156 = 0.06476956605911255 + 50.0 * 6.206427574157715
Epoch 1840, val loss: 1.116762399673462
Epoch 1850, training loss: 310.3614807128906 = 0.06367981433868408 + 50.0 * 6.205955982208252
Epoch 1850, val loss: 1.1210254430770874
Epoch 1860, training loss: 310.5687561035156 = 0.06261257082223892 + 50.0 * 6.210122585296631
Epoch 1860, val loss: 1.1252440214157104
Epoch 1870, training loss: 310.34698486328125 = 0.06154040992259979 + 50.0 * 6.2057085037231445
Epoch 1870, val loss: 1.12961745262146
Epoch 1880, training loss: 310.4500732421875 = 0.06050194427371025 + 50.0 * 6.207791328430176
Epoch 1880, val loss: 1.1336206197738647
Epoch 1890, training loss: 310.3258361816406 = 0.05945940315723419 + 50.0 * 6.20532751083374
Epoch 1890, val loss: 1.1383785009384155
Epoch 1900, training loss: 310.3723449707031 = 0.05844400078058243 + 50.0 * 6.206278324127197
Epoch 1900, val loss: 1.1426645517349243
Epoch 1910, training loss: 310.35784912109375 = 0.057466745376586914 + 50.0 * 6.206007480621338
Epoch 1910, val loss: 1.1469272375106812
Epoch 1920, training loss: 310.3023986816406 = 0.05649872496724129 + 50.0 * 6.204917907714844
Epoch 1920, val loss: 1.1512290239334106
Epoch 1930, training loss: 310.3158874511719 = 0.05555948615074158 + 50.0 * 6.205206394195557
Epoch 1930, val loss: 1.1557847261428833
Epoch 1940, training loss: 310.5063781738281 = 0.0546366348862648 + 50.0 * 6.2090349197387695
Epoch 1940, val loss: 1.1601932048797607
Epoch 1950, training loss: 310.58270263671875 = 0.05372227355837822 + 50.0 * 6.210579872131348
Epoch 1950, val loss: 1.1648696660995483
Epoch 1960, training loss: 310.27655029296875 = 0.05280636250972748 + 50.0 * 6.204474925994873
Epoch 1960, val loss: 1.168392539024353
Epoch 1970, training loss: 310.2332763671875 = 0.05193747207522392 + 50.0 * 6.20362663269043
Epoch 1970, val loss: 1.1726809740066528
Epoch 1980, training loss: 310.2132873535156 = 0.051092393696308136 + 50.0 * 6.203243732452393
Epoch 1980, val loss: 1.1771163940429688
Epoch 1990, training loss: 310.1944580078125 = 0.050267547369003296 + 50.0 * 6.202884197235107
Epoch 1990, val loss: 1.181531310081482
Epoch 2000, training loss: 310.25872802734375 = 0.04946133866906166 + 50.0 * 6.2041850090026855
Epoch 2000, val loss: 1.1859867572784424
Epoch 2010, training loss: 310.3905944824219 = 0.04865951091051102 + 50.0 * 6.206839084625244
Epoch 2010, val loss: 1.1900608539581299
Epoch 2020, training loss: 310.4062194824219 = 0.04787768796086311 + 50.0 * 6.20716667175293
Epoch 2020, val loss: 1.1931753158569336
Epoch 2030, training loss: 310.2962951660156 = 0.047077666968107224 + 50.0 * 6.204984188079834
Epoch 2030, val loss: 1.1982192993164062
Epoch 2040, training loss: 310.30517578125 = 0.04631348326802254 + 50.0 * 6.205177307128906
Epoch 2040, val loss: 1.202615737915039
Epoch 2050, training loss: 310.22265625 = 0.04555801674723625 + 50.0 * 6.2035417556762695
Epoch 2050, val loss: 1.2063795328140259
Epoch 2060, training loss: 310.1538391113281 = 0.044831953942775726 + 50.0 * 6.202179908752441
Epoch 2060, val loss: 1.2105381488800049
Epoch 2070, training loss: 310.1354675292969 = 0.04411608353257179 + 50.0 * 6.201826572418213
Epoch 2070, val loss: 1.2148514986038208
Epoch 2080, training loss: 310.1504821777344 = 0.043422993272542953 + 50.0 * 6.202141284942627
Epoch 2080, val loss: 1.2189604043960571
Epoch 2090, training loss: 310.3849792480469 = 0.04274503514170647 + 50.0 * 6.206844329833984
Epoch 2090, val loss: 1.2230156660079956
Epoch 2100, training loss: 310.2109069824219 = 0.04205268993973732 + 50.0 * 6.2033772468566895
Epoch 2100, val loss: 1.2272948026657104
Epoch 2110, training loss: 310.1111145019531 = 0.04137833043932915 + 50.0 * 6.201394557952881
Epoch 2110, val loss: 1.2315421104431152
Epoch 2120, training loss: 310.1009826660156 = 0.0407266728579998 + 50.0 * 6.201204776763916
Epoch 2120, val loss: 1.235753059387207
Epoch 2130, training loss: 310.1220703125 = 0.04009327292442322 + 50.0 * 6.201639175415039
Epoch 2130, val loss: 1.2398535013198853
Epoch 2140, training loss: 310.27337646484375 = 0.03947564586997032 + 50.0 * 6.204677581787109
Epoch 2140, val loss: 1.243922233581543
Epoch 2150, training loss: 310.2690124511719 = 0.03884781524538994 + 50.0 * 6.20460319519043
Epoch 2150, val loss: 1.2475465536117554
Epoch 2160, training loss: 310.2187805175781 = 0.038227349519729614 + 50.0 * 6.203610897064209
Epoch 2160, val loss: 1.2524924278259277
Epoch 2170, training loss: 310.09759521484375 = 0.037634436041116714 + 50.0 * 6.201199054718018
Epoch 2170, val loss: 1.2562611103057861
Epoch 2180, training loss: 310.028564453125 = 0.037050578743219376 + 50.0 * 6.199830055236816
Epoch 2180, val loss: 1.2601654529571533
Epoch 2190, training loss: 310.0325927734375 = 0.03648940101265907 + 50.0 * 6.199921607971191
Epoch 2190, val loss: 1.264085054397583
Epoch 2200, training loss: 310.1660461425781 = 0.03594445064663887 + 50.0 * 6.202601909637451
Epoch 2200, val loss: 1.267921805381775
Epoch 2210, training loss: 310.13287353515625 = 0.035378556698560715 + 50.0 * 6.2019500732421875
Epoch 2210, val loss: 1.272457480430603
Epoch 2220, training loss: 310.0597839355469 = 0.034832410514354706 + 50.0 * 6.200499534606934
Epoch 2220, val loss: 1.2765477895736694
Epoch 2230, training loss: 310.0682678222656 = 0.03430836647748947 + 50.0 * 6.200679302215576
Epoch 2230, val loss: 1.2804597616195679
Epoch 2240, training loss: 310.1793518066406 = 0.03379266709089279 + 50.0 * 6.202911376953125
Epoch 2240, val loss: 1.2847464084625244
Epoch 2250, training loss: 309.9856872558594 = 0.033279359340667725 + 50.0 * 6.1990485191345215
Epoch 2250, val loss: 1.2880109548568726
Epoch 2260, training loss: 309.9753723144531 = 0.03278053551912308 + 50.0 * 6.198852062225342
Epoch 2260, val loss: 1.2922909259796143
Epoch 2270, training loss: 310.04852294921875 = 0.03230035677552223 + 50.0 * 6.200325012207031
Epoch 2270, val loss: 1.2962398529052734
Epoch 2280, training loss: 310.0613708496094 = 0.03181725740432739 + 50.0 * 6.20059061050415
Epoch 2280, val loss: 1.3001304864883423
Epoch 2290, training loss: 310.0506286621094 = 0.03133383020758629 + 50.0 * 6.200385570526123
Epoch 2290, val loss: 1.3042634725570679
Epoch 2300, training loss: 310.0896911621094 = 0.0308710765093565 + 50.0 * 6.201176166534424
Epoch 2300, val loss: 1.3079888820648193
Epoch 2310, training loss: 309.9521179199219 = 0.030414169654250145 + 50.0 * 6.198433876037598
Epoch 2310, val loss: 1.3116109371185303
Epoch 2320, training loss: 309.9300231933594 = 0.02997354045510292 + 50.0 * 6.198001384735107
Epoch 2320, val loss: 1.3155628442764282
Epoch 2330, training loss: 309.986328125 = 0.02954370714724064 + 50.0 * 6.199135780334473
Epoch 2330, val loss: 1.3193446397781372
Epoch 2340, training loss: 310.0431823730469 = 0.0291188582777977 + 50.0 * 6.200281143188477
Epoch 2340, val loss: 1.3232927322387695
Epoch 2350, training loss: 309.9871826171875 = 0.02869592420756817 + 50.0 * 6.199170112609863
Epoch 2350, val loss: 1.3275461196899414
Epoch 2360, training loss: 309.9729919433594 = 0.02827931009232998 + 50.0 * 6.198894023895264
Epoch 2360, val loss: 1.3311585187911987
Epoch 2370, training loss: 310.0676574707031 = 0.027878984808921814 + 50.0 * 6.2007951736450195
Epoch 2370, val loss: 1.334581971168518
Epoch 2380, training loss: 310.0167541503906 = 0.027479978278279305 + 50.0 * 6.1997857093811035
Epoch 2380, val loss: 1.3382301330566406
Epoch 2390, training loss: 309.90936279296875 = 0.027077890932559967 + 50.0 * 6.197646141052246
Epoch 2390, val loss: 1.3421584367752075
Epoch 2400, training loss: 309.868896484375 = 0.026698308065533638 + 50.0 * 6.196844100952148
Epoch 2400, val loss: 1.3461987972259521
Epoch 2410, training loss: 309.8809814453125 = 0.026330161839723587 + 50.0 * 6.1970930099487305
Epoch 2410, val loss: 1.3499499559402466
Epoch 2420, training loss: 309.93353271484375 = 0.025968419387936592 + 50.0 * 6.198151111602783
Epoch 2420, val loss: 1.3532475233078003
Epoch 2430, training loss: 310.0680236816406 = 0.02560998685657978 + 50.0 * 6.20084810256958
Epoch 2430, val loss: 1.3564451932907104
Epoch 2440, training loss: 309.949951171875 = 0.025248272344470024 + 50.0 * 6.1984944343566895
Epoch 2440, val loss: 1.3612092733383179
Epoch 2450, training loss: 309.8828430175781 = 0.024889947846531868 + 50.0 * 6.1971588134765625
Epoch 2450, val loss: 1.3646003007888794
Epoch 2460, training loss: 309.86907958984375 = 0.02455306239426136 + 50.0 * 6.196890354156494
Epoch 2460, val loss: 1.3681511878967285
Epoch 2470, training loss: 309.9920959472656 = 0.024220850318670273 + 50.0 * 6.199357032775879
Epoch 2470, val loss: 1.3716682195663452
Epoch 2480, training loss: 309.87982177734375 = 0.023888051509857178 + 50.0 * 6.197118282318115
Epoch 2480, val loss: 1.375329613685608
Epoch 2490, training loss: 309.8664245605469 = 0.02355843223631382 + 50.0 * 6.196857452392578
Epoch 2490, val loss: 1.3791464567184448
Epoch 2500, training loss: 309.8022155761719 = 0.023245228454470634 + 50.0 * 6.1955790519714355
Epoch 2500, val loss: 1.3826597929000854
Epoch 2510, training loss: 309.8844299316406 = 0.022938251495361328 + 50.0 * 6.197229385375977
Epoch 2510, val loss: 1.3863283395767212
Epoch 2520, training loss: 309.964111328125 = 0.0226320531219244 + 50.0 * 6.198829650878906
Epoch 2520, val loss: 1.3899849653244019
Epoch 2530, training loss: 309.8442687988281 = 0.022331438958644867 + 50.0 * 6.196438312530518
Epoch 2530, val loss: 1.393285870552063
Epoch 2540, training loss: 309.8475341796875 = 0.02203410677611828 + 50.0 * 6.196509838104248
Epoch 2540, val loss: 1.396904468536377
Epoch 2550, training loss: 309.8382568359375 = 0.021748773753643036 + 50.0 * 6.1963300704956055
Epoch 2550, val loss: 1.4002468585968018
Epoch 2560, training loss: 309.8722229003906 = 0.0214637853205204 + 50.0 * 6.197015285491943
Epoch 2560, val loss: 1.4034479856491089
Epoch 2570, training loss: 309.96343994140625 = 0.021183844655752182 + 50.0 * 6.198844909667969
Epoch 2570, val loss: 1.4069225788116455
Epoch 2580, training loss: 309.8166809082031 = 0.02090383693575859 + 50.0 * 6.195915222167969
Epoch 2580, val loss: 1.410818338394165
Epoch 2590, training loss: 309.75128173828125 = 0.020630721002817154 + 50.0 * 6.194612979888916
Epoch 2590, val loss: 1.4141263961791992
Epoch 2600, training loss: 309.7235412597656 = 0.020374612882733345 + 50.0 * 6.194063186645508
Epoch 2600, val loss: 1.4178695678710938
Epoch 2610, training loss: 309.7677001953125 = 0.020124582573771477 + 50.0 * 6.19495153427124
Epoch 2610, val loss: 1.4214991331100464
Epoch 2620, training loss: 309.9944152832031 = 0.019878476858139038 + 50.0 * 6.199491024017334
Epoch 2620, val loss: 1.4249264001846313
Epoch 2630, training loss: 309.84747314453125 = 0.019619785249233246 + 50.0 * 6.19655704498291
Epoch 2630, val loss: 1.4277271032333374
Epoch 2640, training loss: 309.77606201171875 = 0.019372032955288887 + 50.0 * 6.195133686065674
Epoch 2640, val loss: 1.431649923324585
Epoch 2650, training loss: 309.8857421875 = 0.019135966897010803 + 50.0 * 6.19733190536499
Epoch 2650, val loss: 1.434706687927246
Epoch 2660, training loss: 309.8094177246094 = 0.018896544352173805 + 50.0 * 6.195810794830322
Epoch 2660, val loss: 1.437760829925537
Epoch 2670, training loss: 309.7413330078125 = 0.018664363771677017 + 50.0 * 6.194453239440918
Epoch 2670, val loss: 1.4412479400634766
Epoch 2680, training loss: 309.74017333984375 = 0.01843423768877983 + 50.0 * 6.194435119628906
Epoch 2680, val loss: 1.444772481918335
Epoch 2690, training loss: 309.7483825683594 = 0.01821320690214634 + 50.0 * 6.194603443145752
Epoch 2690, val loss: 1.4480541944503784
Epoch 2700, training loss: 309.8189697265625 = 0.01799696311354637 + 50.0 * 6.196019172668457
Epoch 2700, val loss: 1.451670527458191
Epoch 2710, training loss: 309.7501525878906 = 0.017780575901269913 + 50.0 * 6.194647312164307
Epoch 2710, val loss: 1.4542654752731323
Epoch 2720, training loss: 309.7234191894531 = 0.017570221796631813 + 50.0 * 6.194116592407227
Epoch 2720, val loss: 1.457533359527588
Epoch 2730, training loss: 309.72515869140625 = 0.017359532415866852 + 50.0 * 6.194155693054199
Epoch 2730, val loss: 1.4607563018798828
Epoch 2740, training loss: 309.6610107421875 = 0.017157932743430138 + 50.0 * 6.192877292633057
Epoch 2740, val loss: 1.463894248008728
Epoch 2750, training loss: 309.6836242675781 = 0.016961904242634773 + 50.0 * 6.193333148956299
Epoch 2750, val loss: 1.4672681093215942
Epoch 2760, training loss: 309.8294372558594 = 0.01676989533007145 + 50.0 * 6.196252822875977
Epoch 2760, val loss: 1.470500111579895
Epoch 2770, training loss: 309.8031921386719 = 0.01656968705356121 + 50.0 * 6.195732116699219
Epoch 2770, val loss: 1.473952054977417
Epoch 2780, training loss: 309.7095642089844 = 0.016377652063965797 + 50.0 * 6.193863391876221
Epoch 2780, val loss: 1.4764556884765625
Epoch 2790, training loss: 309.6212158203125 = 0.01618720218539238 + 50.0 * 6.192100524902344
Epoch 2790, val loss: 1.4793987274169922
Epoch 2800, training loss: 309.70782470703125 = 0.01600535959005356 + 50.0 * 6.193836688995361
Epoch 2800, val loss: 1.4828077554702759
Epoch 2810, training loss: 309.77142333984375 = 0.015825878828763962 + 50.0 * 6.1951117515563965
Epoch 2810, val loss: 1.4857691526412964
Epoch 2820, training loss: 309.6610412597656 = 0.01564483903348446 + 50.0 * 6.192907810211182
Epoch 2820, val loss: 1.4887511730194092
Epoch 2830, training loss: 309.6737976074219 = 0.015472759492695332 + 50.0 * 6.193166732788086
Epoch 2830, val loss: 1.4918638467788696
Epoch 2840, training loss: 309.7272644042969 = 0.015305067412555218 + 50.0 * 6.194238662719727
Epoch 2840, val loss: 1.4945414066314697
Epoch 2850, training loss: 309.75445556640625 = 0.015140369534492493 + 50.0 * 6.194786071777344
Epoch 2850, val loss: 1.4976164102554321
Epoch 2860, training loss: 309.6462707519531 = 0.014967519789934158 + 50.0 * 6.192626476287842
Epoch 2860, val loss: 1.501028060913086
Epoch 2870, training loss: 309.6134033203125 = 0.014803990721702576 + 50.0 * 6.191971778869629
Epoch 2870, val loss: 1.5039724111557007
Epoch 2880, training loss: 309.712890625 = 0.014647099189460278 + 50.0 * 6.193964958190918
Epoch 2880, val loss: 1.5071510076522827
Epoch 2890, training loss: 309.6123352050781 = 0.014486812986433506 + 50.0 * 6.191956996917725
Epoch 2890, val loss: 1.509863018989563
Epoch 2900, training loss: 309.57318115234375 = 0.01433631032705307 + 50.0 * 6.191176891326904
Epoch 2900, val loss: 1.5125420093536377
Epoch 2910, training loss: 309.6572570800781 = 0.014190258458256721 + 50.0 * 6.192861080169678
Epoch 2910, val loss: 1.5153539180755615
Epoch 2920, training loss: 309.81427001953125 = 0.014040816575288773 + 50.0 * 6.196004867553711
Epoch 2920, val loss: 1.5183581113815308
Epoch 2930, training loss: 309.6628112792969 = 0.013884668238461018 + 50.0 * 6.192978382110596
Epoch 2930, val loss: 1.5209872722625732
Epoch 2940, training loss: 309.6544189453125 = 0.013739212416112423 + 50.0 * 6.192813873291016
Epoch 2940, val loss: 1.5234668254852295
Epoch 2950, training loss: 309.6324768066406 = 0.013596512377262115 + 50.0 * 6.19237756729126
Epoch 2950, val loss: 1.5266330242156982
Epoch 2960, training loss: 309.5296936035156 = 0.013455909676849842 + 50.0 * 6.190324783325195
Epoch 2960, val loss: 1.5296725034713745
Epoch 2970, training loss: 309.65118408203125 = 0.013323090970516205 + 50.0 * 6.1927571296691895
Epoch 2970, val loss: 1.5327672958374023
Epoch 2980, training loss: 309.6316833496094 = 0.013184227049350739 + 50.0 * 6.1923699378967285
Epoch 2980, val loss: 1.5351684093475342
Epoch 2990, training loss: 309.5544738769531 = 0.013048098422586918 + 50.0 * 6.190828323364258
Epoch 2990, val loss: 1.537800669670105
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8355297838692674
The final CL Acc:0.74321, 0.02058, The final GNN Acc:0.83658, 0.00149
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11600])
remove edge: torch.Size([2, 9504])
updated graph: torch.Size([2, 10548])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7889709472656 = 1.9460203647613525 + 50.0 * 8.596858978271484
Epoch 0, val loss: 1.9508938789367676
Epoch 10, training loss: 431.7416076660156 = 1.9368833303451538 + 50.0 * 8.596094131469727
Epoch 10, val loss: 1.9408395290374756
Epoch 20, training loss: 431.44781494140625 = 1.9254908561706543 + 50.0 * 8.590446472167969
Epoch 20, val loss: 1.9280976057052612
Epoch 30, training loss: 429.35833740234375 = 1.9109169244766235 + 50.0 * 8.548948287963867
Epoch 30, val loss: 1.9117101430892944
Epoch 40, training loss: 414.1921081542969 = 1.8934757709503174 + 50.0 * 8.245972633361816
Epoch 40, val loss: 1.892347812652588
Epoch 50, training loss: 374.66375732421875 = 1.8746542930603027 + 50.0 * 7.455781936645508
Epoch 50, val loss: 1.871686577796936
Epoch 60, training loss: 363.8106384277344 = 1.8597252368927002 + 50.0 * 7.239018440246582
Epoch 60, val loss: 1.8563235998153687
Epoch 70, training loss: 355.532470703125 = 1.8475676774978638 + 50.0 * 7.073698043823242
Epoch 70, val loss: 1.844120979309082
Epoch 80, training loss: 348.7344970703125 = 1.8358584642410278 + 50.0 * 6.9379730224609375
Epoch 80, val loss: 1.8325693607330322
Epoch 90, training loss: 342.51263427734375 = 1.8266257047653198 + 50.0 * 6.813720226287842
Epoch 90, val loss: 1.8235878944396973
Epoch 100, training loss: 339.5330810546875 = 1.8175612688064575 + 50.0 * 6.754310607910156
Epoch 100, val loss: 1.814636468887329
Epoch 110, training loss: 336.8523864746094 = 1.8084408044815063 + 50.0 * 6.700879096984863
Epoch 110, val loss: 1.8057278394699097
Epoch 120, training loss: 334.1283264160156 = 1.8000773191452026 + 50.0 * 6.646564960479736
Epoch 120, val loss: 1.7978233098983765
Epoch 130, training loss: 331.92547607421875 = 1.7922827005386353 + 50.0 * 6.602663516998291
Epoch 130, val loss: 1.7904770374298096
Epoch 140, training loss: 330.1148681640625 = 1.7841269969940186 + 50.0 * 6.566615104675293
Epoch 140, val loss: 1.7826287746429443
Epoch 150, training loss: 328.66387939453125 = 1.7751622200012207 + 50.0 * 6.537774085998535
Epoch 150, val loss: 1.7744060754776
Epoch 160, training loss: 327.67864990234375 = 1.7654153108596802 + 50.0 * 6.5182647705078125
Epoch 160, val loss: 1.7656656503677368
Epoch 170, training loss: 326.5284118652344 = 1.754934310913086 + 50.0 * 6.495469093322754
Epoch 170, val loss: 1.7562122344970703
Epoch 180, training loss: 325.5950927734375 = 1.7435959577560425 + 50.0 * 6.477030277252197
Epoch 180, val loss: 1.7461862564086914
Epoch 190, training loss: 324.8394775390625 = 1.731373906135559 + 50.0 * 6.462162017822266
Epoch 190, val loss: 1.735426425933838
Epoch 200, training loss: 324.35870361328125 = 1.7180614471435547 + 50.0 * 6.452813148498535
Epoch 200, val loss: 1.72372305393219
Epoch 210, training loss: 323.6340637207031 = 1.703487753868103 + 50.0 * 6.4386115074157715
Epoch 210, val loss: 1.711370825767517
Epoch 220, training loss: 323.1204528808594 = 1.6878200769424438 + 50.0 * 6.428653240203857
Epoch 220, val loss: 1.697957992553711
Epoch 230, training loss: 322.79913330078125 = 1.6708486080169678 + 50.0 * 6.422565937042236
Epoch 230, val loss: 1.6836718320846558
Epoch 240, training loss: 322.2933044433594 = 1.6523669958114624 + 50.0 * 6.412818431854248
Epoch 240, val loss: 1.6684753894805908
Epoch 250, training loss: 321.88629150390625 = 1.6327574253082275 + 50.0 * 6.405070781707764
Epoch 250, val loss: 1.6522231101989746
Epoch 260, training loss: 321.5889892578125 = 1.6117212772369385 + 50.0 * 6.399545192718506
Epoch 260, val loss: 1.6351196765899658
Epoch 270, training loss: 321.1714172363281 = 1.5893449783325195 + 50.0 * 6.391641139984131
Epoch 270, val loss: 1.6171576976776123
Epoch 280, training loss: 320.8781433105469 = 1.5657762289047241 + 50.0 * 6.386247634887695
Epoch 280, val loss: 1.5983039140701294
Epoch 290, training loss: 320.5025634765625 = 1.5409667491912842 + 50.0 * 6.379231929779053
Epoch 290, val loss: 1.5787742137908936
Epoch 300, training loss: 320.5667724609375 = 1.51503586769104 + 50.0 * 6.3810343742370605
Epoch 300, val loss: 1.5587860345840454
Epoch 310, training loss: 319.95391845703125 = 1.488168716430664 + 50.0 * 6.369315147399902
Epoch 310, val loss: 1.5378646850585938
Epoch 320, training loss: 319.6813049316406 = 1.4604179859161377 + 50.0 * 6.364418029785156
Epoch 320, val loss: 1.516669750213623
Epoch 330, training loss: 319.4054260253906 = 1.4320789575576782 + 50.0 * 6.359467029571533
Epoch 330, val loss: 1.49524986743927
Epoch 340, training loss: 319.1530456542969 = 1.4032238721847534 + 50.0 * 6.354996204376221
Epoch 340, val loss: 1.4736993312835693
Epoch 350, training loss: 319.0959777832031 = 1.3739275932312012 + 50.0 * 6.354440689086914
Epoch 350, val loss: 1.4520176649093628
Epoch 360, training loss: 318.7903747558594 = 1.3445119857788086 + 50.0 * 6.348917484283447
Epoch 360, val loss: 1.4303573369979858
Epoch 370, training loss: 318.56964111328125 = 1.315010905265808 + 50.0 * 6.3450927734375
Epoch 370, val loss: 1.409041404724121
Epoch 380, training loss: 318.3381652832031 = 1.285813331604004 + 50.0 * 6.3410468101501465
Epoch 380, val loss: 1.3881900310516357
Epoch 390, training loss: 318.1675109863281 = 1.2570606470108032 + 50.0 * 6.33820915222168
Epoch 390, val loss: 1.3677737712860107
Epoch 400, training loss: 317.9688720703125 = 1.2287517786026 + 50.0 * 6.334802150726318
Epoch 400, val loss: 1.3479589223861694
Epoch 410, training loss: 318.4472961425781 = 1.2012343406677246 + 50.0 * 6.344921112060547
Epoch 410, val loss: 1.3286463022232056
Epoch 420, training loss: 317.7088623046875 = 1.173866629600525 + 50.0 * 6.330699920654297
Epoch 420, val loss: 1.3102951049804688
Epoch 430, training loss: 317.47943115234375 = 1.1476889848709106 + 50.0 * 6.326634883880615
Epoch 430, val loss: 1.2928673028945923
Epoch 440, training loss: 317.29522705078125 = 1.1224591732025146 + 50.0 * 6.323455333709717
Epoch 440, val loss: 1.2764012813568115
Epoch 450, training loss: 317.1279296875 = 1.0981858968734741 + 50.0 * 6.3205952644348145
Epoch 450, val loss: 1.2610079050064087
Epoch 460, training loss: 316.9781188964844 = 1.0748002529144287 + 50.0 * 6.318066596984863
Epoch 460, val loss: 1.2465964555740356
Epoch 470, training loss: 317.39501953125 = 1.0522087812423706 + 50.0 * 6.3268561363220215
Epoch 470, val loss: 1.2331782579421997
Epoch 480, training loss: 316.87017822265625 = 1.0303581953048706 + 50.0 * 6.31679630279541
Epoch 480, val loss: 1.2203367948532104
Epoch 490, training loss: 316.6046142578125 = 1.0094276666641235 + 50.0 * 6.311903953552246
Epoch 490, val loss: 1.208512783050537
Epoch 500, training loss: 316.4507141113281 = 0.9893597960472107 + 50.0 * 6.309227466583252
Epoch 500, val loss: 1.1976749897003174
Epoch 510, training loss: 316.8430480957031 = 0.9701482653617859 + 50.0 * 6.317457675933838
Epoch 510, val loss: 1.1875659227371216
Epoch 520, training loss: 316.2452392578125 = 0.9510868191719055 + 50.0 * 6.305883407592773
Epoch 520, val loss: 1.1782792806625366
Epoch 530, training loss: 316.1213073730469 = 0.9329867362976074 + 50.0 * 6.303766250610352
Epoch 530, val loss: 1.1698813438415527
Epoch 540, training loss: 316.023193359375 = 0.9155166745185852 + 50.0 * 6.30215311050415
Epoch 540, val loss: 1.1619958877563477
Epoch 550, training loss: 315.90814208984375 = 0.8984717726707458 + 50.0 * 6.3001933097839355
Epoch 550, val loss: 1.1545754671096802
Epoch 560, training loss: 315.8284606933594 = 0.8819100856781006 + 50.0 * 6.298931121826172
Epoch 560, val loss: 1.147902488708496
Epoch 570, training loss: 315.82415771484375 = 0.8657078742980957 + 50.0 * 6.299168586730957
Epoch 570, val loss: 1.141672968864441
Epoch 580, training loss: 315.6359558105469 = 0.8499876260757446 + 50.0 * 6.295719623565674
Epoch 580, val loss: 1.1359233856201172
Epoch 590, training loss: 315.4941101074219 = 0.8346320986747742 + 50.0 * 6.293189525604248
Epoch 590, val loss: 1.1305193901062012
Epoch 600, training loss: 315.5340881347656 = 0.8196263313293457 + 50.0 * 6.2942891120910645
Epoch 600, val loss: 1.1257174015045166
Epoch 610, training loss: 315.4437561035156 = 0.8047639727592468 + 50.0 * 6.292779445648193
Epoch 610, val loss: 1.1209423542022705
Epoch 620, training loss: 315.2778625488281 = 0.7901469469070435 + 50.0 * 6.2897539138793945
Epoch 620, val loss: 1.1170824766159058
Epoch 630, training loss: 315.2749938964844 = 0.775896430015564 + 50.0 * 6.289981842041016
Epoch 630, val loss: 1.113312840461731
Epoch 640, training loss: 315.0673828125 = 0.761871874332428 + 50.0 * 6.2861104011535645
Epoch 640, val loss: 1.1095036268234253
Epoch 650, training loss: 314.9637451171875 = 0.7480632066726685 + 50.0 * 6.284313201904297
Epoch 650, val loss: 1.106502890586853
Epoch 660, training loss: 315.3566589355469 = 0.734651505947113 + 50.0 * 6.292439937591553
Epoch 660, val loss: 1.1035494804382324
Epoch 670, training loss: 314.9686279296875 = 0.7208432555198669 + 50.0 * 6.2849555015563965
Epoch 670, val loss: 1.1009716987609863
Epoch 680, training loss: 314.77178955078125 = 0.7075726389884949 + 50.0 * 6.281284332275391
Epoch 680, val loss: 1.098537564277649
Epoch 690, training loss: 314.6340026855469 = 0.6945359706878662 + 50.0 * 6.278789043426514
Epoch 690, val loss: 1.096709132194519
Epoch 700, training loss: 314.58990478515625 = 0.6816669702529907 + 50.0 * 6.278164386749268
Epoch 700, val loss: 1.0950785875320435
Epoch 710, training loss: 314.6732482910156 = 0.6688833236694336 + 50.0 * 6.280086994171143
Epoch 710, val loss: 1.0935442447662354
Epoch 720, training loss: 314.4585876464844 = 0.656093955039978 + 50.0 * 6.276049613952637
Epoch 720, val loss: 1.0922077894210815
Epoch 730, training loss: 314.4398498535156 = 0.6435880064964294 + 50.0 * 6.275925159454346
Epoch 730, val loss: 1.091004490852356
Epoch 740, training loss: 314.3555908203125 = 0.6310814619064331 + 50.0 * 6.2744903564453125
Epoch 740, val loss: 1.090211033821106
Epoch 750, training loss: 314.2354431152344 = 0.6187402009963989 + 50.0 * 6.272334098815918
Epoch 750, val loss: 1.0897836685180664
Epoch 760, training loss: 314.3682556152344 = 0.6065924763679504 + 50.0 * 6.275233268737793
Epoch 760, val loss: 1.0891084671020508
Epoch 770, training loss: 314.2689514160156 = 0.5943787693977356 + 50.0 * 6.273491382598877
Epoch 770, val loss: 1.088884949684143
Epoch 780, training loss: 314.08197021484375 = 0.5822506546974182 + 50.0 * 6.269994735717773
Epoch 780, val loss: 1.0888124704360962
Epoch 790, training loss: 313.98480224609375 = 0.5704374313354492 + 50.0 * 6.268287658691406
Epoch 790, val loss: 1.0888257026672363
Epoch 800, training loss: 313.9571533203125 = 0.5586802363395691 + 50.0 * 6.267969608306885
Epoch 800, val loss: 1.0892542600631714
Epoch 810, training loss: 314.1155700683594 = 0.5469803810119629 + 50.0 * 6.271371841430664
Epoch 810, val loss: 1.089788794517517
Epoch 820, training loss: 313.8543395996094 = 0.5352925658226013 + 50.0 * 6.26638126373291
Epoch 820, val loss: 1.0900241136550903
Epoch 830, training loss: 313.7738037109375 = 0.5238509178161621 + 50.0 * 6.264998912811279
Epoch 830, val loss: 1.0909432172775269
Epoch 840, training loss: 313.70855712890625 = 0.5125095844268799 + 50.0 * 6.263920783996582
Epoch 840, val loss: 1.091981291770935
Epoch 850, training loss: 313.6692199707031 = 0.5012968182563782 + 50.0 * 6.2633585929870605
Epoch 850, val loss: 1.0932995080947876
Epoch 860, training loss: 313.7690734863281 = 0.49021610617637634 + 50.0 * 6.26557731628418
Epoch 860, val loss: 1.0948008298873901
Epoch 870, training loss: 313.875732421875 = 0.47921690344810486 + 50.0 * 6.267930030822754
Epoch 870, val loss: 1.0960285663604736
Epoch 880, training loss: 313.6167297363281 = 0.4682583510875702 + 50.0 * 6.262969493865967
Epoch 880, val loss: 1.0973936319351196
Epoch 890, training loss: 313.4808654785156 = 0.45754551887512207 + 50.0 * 6.260466575622559
Epoch 890, val loss: 1.0993233919143677
Epoch 900, training loss: 313.3881530761719 = 0.4470156133174896 + 50.0 * 6.258822917938232
Epoch 900, val loss: 1.101264238357544
Epoch 910, training loss: 313.3349304199219 = 0.4366268515586853 + 50.0 * 6.2579665184021
Epoch 910, val loss: 1.103405475616455
Epoch 920, training loss: 313.40679931640625 = 0.42641666531562805 + 50.0 * 6.259607315063477
Epoch 920, val loss: 1.1055352687835693
Epoch 930, training loss: 313.6958312988281 = 0.41633352637290955 + 50.0 * 6.265590190887451
Epoch 930, val loss: 1.107895851135254
Epoch 940, training loss: 313.29058837890625 = 0.40614640712738037 + 50.0 * 6.257688522338867
Epoch 940, val loss: 1.1098500490188599
Epoch 950, training loss: 313.15924072265625 = 0.3963073194026947 + 50.0 * 6.255258560180664
Epoch 950, val loss: 1.112606167793274
Epoch 960, training loss: 313.1305236816406 = 0.38673949241638184 + 50.0 * 6.254875659942627
Epoch 960, val loss: 1.115580677986145
Epoch 970, training loss: 313.3913269042969 = 0.37733930349349976 + 50.0 * 6.260279655456543
Epoch 970, val loss: 1.118475079536438
Epoch 980, training loss: 313.1523132324219 = 0.3680657744407654 + 50.0 * 6.255684852600098
Epoch 980, val loss: 1.1211612224578857
Epoch 990, training loss: 313.047607421875 = 0.35889706015586853 + 50.0 * 6.253774642944336
Epoch 990, val loss: 1.1242860555648804
Epoch 1000, training loss: 312.9577331542969 = 0.3500067889690399 + 50.0 * 6.25215482711792
Epoch 1000, val loss: 1.1273291110992432
Epoch 1010, training loss: 313.0243835449219 = 0.34129688143730164 + 50.0 * 6.253661632537842
Epoch 1010, val loss: 1.1306509971618652
Epoch 1020, training loss: 312.9384765625 = 0.3326685130596161 + 50.0 * 6.2521162033081055
Epoch 1020, val loss: 1.13359797000885
Epoch 1030, training loss: 312.8974609375 = 0.3241887092590332 + 50.0 * 6.251465320587158
Epoch 1030, val loss: 1.1371333599090576
Epoch 1040, training loss: 312.94219970703125 = 0.31590938568115234 + 50.0 * 6.252525806427002
Epoch 1040, val loss: 1.1404662132263184
Epoch 1050, training loss: 312.7923278808594 = 0.307797908782959 + 50.0 * 6.249690532684326
Epoch 1050, val loss: 1.1440743207931519
Epoch 1060, training loss: 312.7364501953125 = 0.2998913526535034 + 50.0 * 6.2487311363220215
Epoch 1060, val loss: 1.1478369235992432
Epoch 1070, training loss: 312.7575988769531 = 0.2921510934829712 + 50.0 * 6.249309062957764
Epoch 1070, val loss: 1.151512861251831
Epoch 1080, training loss: 312.8489990234375 = 0.28459644317626953 + 50.0 * 6.251287937164307
Epoch 1080, val loss: 1.1554208993911743
Epoch 1090, training loss: 312.6749572753906 = 0.27716976404190063 + 50.0 * 6.247955799102783
Epoch 1090, val loss: 1.1588760614395142
Epoch 1100, training loss: 312.60833740234375 = 0.26993998885154724 + 50.0 * 6.246768474578857
Epoch 1100, val loss: 1.162913203239441
Epoch 1110, training loss: 312.5368957519531 = 0.26286569237709045 + 50.0 * 6.245480537414551
Epoch 1110, val loss: 1.166841983795166
Epoch 1120, training loss: 312.59954833984375 = 0.2560040354728699 + 50.0 * 6.246870994567871
Epoch 1120, val loss: 1.1709084510803223
Epoch 1130, training loss: 312.7677307128906 = 0.24928072094917297 + 50.0 * 6.250369071960449
Epoch 1130, val loss: 1.1748344898223877
Epoch 1140, training loss: 312.62542724609375 = 0.24260255694389343 + 50.0 * 6.247656345367432
Epoch 1140, val loss: 1.1786178350448608
Epoch 1150, training loss: 312.4420471191406 = 0.2361188381910324 + 50.0 * 6.244118690490723
Epoch 1150, val loss: 1.1827813386917114
Epoch 1160, training loss: 312.36981201171875 = 0.22989073395729065 + 50.0 * 6.242798328399658
Epoch 1160, val loss: 1.1870441436767578
Epoch 1170, training loss: 312.3775329589844 = 0.22381922602653503 + 50.0 * 6.243074417114258
Epoch 1170, val loss: 1.191371202468872
Epoch 1180, training loss: 312.6499938964844 = 0.21786734461784363 + 50.0 * 6.248642444610596
Epoch 1180, val loss: 1.1957778930664062
Epoch 1190, training loss: 312.4274597167969 = 0.21206146478652954 + 50.0 * 6.244307994842529
Epoch 1190, val loss: 1.1996424198150635
Epoch 1200, training loss: 312.3138732910156 = 0.20638838410377502 + 50.0 * 6.242149353027344
Epoch 1200, val loss: 1.2040071487426758
Epoch 1210, training loss: 312.2398986816406 = 0.2008797973394394 + 50.0 * 6.240780353546143
Epoch 1210, val loss: 1.2085751295089722
Epoch 1220, training loss: 312.3049621582031 = 0.1955449879169464 + 50.0 * 6.242187976837158
Epoch 1220, val loss: 1.2130537033081055
Epoch 1230, training loss: 312.3898010253906 = 0.19032461941242218 + 50.0 * 6.243988990783691
Epoch 1230, val loss: 1.217430830001831
Epoch 1240, training loss: 312.17724609375 = 0.18522080779075623 + 50.0 * 6.239840984344482
Epoch 1240, val loss: 1.2216731309890747
Epoch 1250, training loss: 312.1191711425781 = 0.18027092516422272 + 50.0 * 6.238778114318848
Epoch 1250, val loss: 1.226303219795227
Epoch 1260, training loss: 312.1730651855469 = 0.17548435926437378 + 50.0 * 6.2399516105651855
Epoch 1260, val loss: 1.2309526205062866
Epoch 1270, training loss: 312.229736328125 = 0.1707967072725296 + 50.0 * 6.241178512573242
Epoch 1270, val loss: 1.2355008125305176
Epoch 1280, training loss: 312.06591796875 = 0.166253924369812 + 50.0 * 6.237993240356445
Epoch 1280, val loss: 1.2399543523788452
Epoch 1290, training loss: 312.0195617675781 = 0.16181868314743042 + 50.0 * 6.237155437469482
Epoch 1290, val loss: 1.2447283267974854
Epoch 1300, training loss: 312.14886474609375 = 0.15757524967193604 + 50.0 * 6.239826202392578
Epoch 1300, val loss: 1.24934983253479
Epoch 1310, training loss: 312.0146179199219 = 0.1533474624156952 + 50.0 * 6.23722505569458
Epoch 1310, val loss: 1.2539467811584473
Epoch 1320, training loss: 311.9666748046875 = 0.14926140010356903 + 50.0 * 6.2363481521606445
Epoch 1320, val loss: 1.258622407913208
Epoch 1330, training loss: 311.9464416503906 = 0.14534111320972443 + 50.0 * 6.236021995544434
Epoch 1330, val loss: 1.263546109199524
Epoch 1340, training loss: 312.1136474609375 = 0.14153818786144257 + 50.0 * 6.239442348480225
Epoch 1340, val loss: 1.268239974975586
Epoch 1350, training loss: 311.93927001953125 = 0.1377810537815094 + 50.0 * 6.236029624938965
Epoch 1350, val loss: 1.2731226682662964
Epoch 1360, training loss: 311.9327087402344 = 0.13416045904159546 + 50.0 * 6.235970973968506
Epoch 1360, val loss: 1.277827262878418
Epoch 1370, training loss: 311.8685607910156 = 0.13063453137874603 + 50.0 * 6.234758377075195
Epoch 1370, val loss: 1.282886266708374
Epoch 1380, training loss: 311.8915100097656 = 0.12724609673023224 + 50.0 * 6.23528528213501
Epoch 1380, val loss: 1.2877739667892456
Epoch 1390, training loss: 311.9150085449219 = 0.12394043058156967 + 50.0 * 6.235821723937988
Epoch 1390, val loss: 1.292690634727478
Epoch 1400, training loss: 311.8547668457031 = 0.1207081750035286 + 50.0 * 6.234681606292725
Epoch 1400, val loss: 1.2974624633789062
Epoch 1410, training loss: 312.147705078125 = 0.11760333180427551 + 50.0 * 6.240602016448975
Epoch 1410, val loss: 1.3021020889282227
Epoch 1420, training loss: 311.8442077636719 = 0.11448731273412704 + 50.0 * 6.234594345092773
Epoch 1420, val loss: 1.3070008754730225
Epoch 1430, training loss: 311.7337951660156 = 0.11154098063707352 + 50.0 * 6.232444763183594
Epoch 1430, val loss: 1.3121287822723389
Epoch 1440, training loss: 311.69293212890625 = 0.10870970040559769 + 50.0 * 6.231684684753418
Epoch 1440, val loss: 1.3171864748001099
Epoch 1450, training loss: 311.73712158203125 = 0.10595241189002991 + 50.0 * 6.232623100280762
Epoch 1450, val loss: 1.3223060369491577
Epoch 1460, training loss: 311.8306884765625 = 0.10326668620109558 + 50.0 * 6.234548091888428
Epoch 1460, val loss: 1.32716703414917
Epoch 1470, training loss: 311.71014404296875 = 0.1006583571434021 + 50.0 * 6.232189655303955
Epoch 1470, val loss: 1.3318235874176025
Epoch 1480, training loss: 311.6874084472656 = 0.09811071306467056 + 50.0 * 6.231785774230957
Epoch 1480, val loss: 1.3369312286376953
Epoch 1490, training loss: 311.69281005859375 = 0.09567905217409134 + 50.0 * 6.231942653656006
Epoch 1490, val loss: 1.3419052362442017
Epoch 1500, training loss: 311.62530517578125 = 0.09329294413328171 + 50.0 * 6.230640411376953
Epoch 1500, val loss: 1.346746802330017
Epoch 1510, training loss: 311.587890625 = 0.09100794047117233 + 50.0 * 6.229937553405762
Epoch 1510, val loss: 1.3518874645233154
Epoch 1520, training loss: 312.0058288574219 = 0.08880160748958588 + 50.0 * 6.238340377807617
Epoch 1520, val loss: 1.3567659854888916
Epoch 1530, training loss: 311.6441955566406 = 0.08656500279903412 + 50.0 * 6.231152534484863
Epoch 1530, val loss: 1.361292839050293
Epoch 1540, training loss: 311.4998779296875 = 0.08442319929599762 + 50.0 * 6.228309154510498
Epoch 1540, val loss: 1.3660815954208374
Epoch 1550, training loss: 311.50396728515625 = 0.08238833397626877 + 50.0 * 6.228431701660156
Epoch 1550, val loss: 1.3714414834976196
Epoch 1560, training loss: 311.4682922363281 = 0.08042692393064499 + 50.0 * 6.227757453918457
Epoch 1560, val loss: 1.3764538764953613
Epoch 1570, training loss: 311.6478576660156 = 0.07853880524635315 + 50.0 * 6.231386184692383
Epoch 1570, val loss: 1.3815498352050781
Epoch 1580, training loss: 311.456298828125 = 0.07665924727916718 + 50.0 * 6.227592945098877
Epoch 1580, val loss: 1.386226773262024
Epoch 1590, training loss: 311.45196533203125 = 0.07484599202871323 + 50.0 * 6.227542400360107
Epoch 1590, val loss: 1.3912341594696045
Epoch 1600, training loss: 311.44708251953125 = 0.07310933619737625 + 50.0 * 6.227478981018066
Epoch 1600, val loss: 1.3962565660476685
Epoch 1610, training loss: 311.747314453125 = 0.07143527269363403 + 50.0 * 6.233517169952393
Epoch 1610, val loss: 1.4011597633361816
Epoch 1620, training loss: 311.548095703125 = 0.06974205374717712 + 50.0 * 6.229567050933838
Epoch 1620, val loss: 1.405887484550476
Epoch 1630, training loss: 311.5840148925781 = 0.06814542412757874 + 50.0 * 6.230317115783691
Epoch 1630, val loss: 1.4108768701553345
Epoch 1640, training loss: 311.4141845703125 = 0.0665832981467247 + 50.0 * 6.226952075958252
Epoch 1640, val loss: 1.415462851524353
Epoch 1650, training loss: 311.4326171875 = 0.06509307771921158 + 50.0 * 6.22735071182251
Epoch 1650, val loss: 1.4201631546020508
Epoch 1660, training loss: 311.459716796875 = 0.06362725794315338 + 50.0 * 6.227921962738037
Epoch 1660, val loss: 1.424925446510315
Epoch 1670, training loss: 311.3171081542969 = 0.06220803037285805 + 50.0 * 6.225098133087158
Epoch 1670, val loss: 1.4298728704452515
Epoch 1680, training loss: 311.30279541015625 = 0.060847263783216476 + 50.0 * 6.224838733673096
Epoch 1680, val loss: 1.4347072839736938
Epoch 1690, training loss: 311.3777770996094 = 0.059527188539505005 + 50.0 * 6.226364612579346
Epoch 1690, val loss: 1.4395577907562256
Epoch 1700, training loss: 311.48291015625 = 0.058231472969055176 + 50.0 * 6.228493690490723
Epoch 1700, val loss: 1.4442634582519531
Epoch 1710, training loss: 311.505126953125 = 0.05696864426136017 + 50.0 * 6.2289628982543945
Epoch 1710, val loss: 1.4482401609420776
Epoch 1720, training loss: 311.36810302734375 = 0.055713921785354614 + 50.0 * 6.226247787475586
Epoch 1720, val loss: 1.452761173248291
Epoch 1730, training loss: 311.2168884277344 = 0.054532624781131744 + 50.0 * 6.223247051239014
Epoch 1730, val loss: 1.4575302600860596
Epoch 1740, training loss: 311.1915588378906 = 0.053396109491586685 + 50.0 * 6.2227630615234375
Epoch 1740, val loss: 1.4622719287872314
Epoch 1750, training loss: 311.17791748046875 = 0.0522918626666069 + 50.0 * 6.222512722015381
Epoch 1750, val loss: 1.4669430255889893
Epoch 1760, training loss: 311.514892578125 = 0.051231104880571365 + 50.0 * 6.229273319244385
Epoch 1760, val loss: 1.4714528322219849
Epoch 1770, training loss: 311.2984313964844 = 0.05014808848500252 + 50.0 * 6.224965572357178
Epoch 1770, val loss: 1.4754430055618286
Epoch 1780, training loss: 311.2434997558594 = 0.04911369830369949 + 50.0 * 6.223887920379639
Epoch 1780, val loss: 1.4799200296401978
Epoch 1790, training loss: 311.19024658203125 = 0.048126351088285446 + 50.0 * 6.222842216491699
Epoch 1790, val loss: 1.4843418598175049
Epoch 1800, training loss: 311.34112548828125 = 0.047172024846076965 + 50.0 * 6.225879192352295
Epoch 1800, val loss: 1.488732933998108
Epoch 1810, training loss: 311.2295227050781 = 0.046211354434490204 + 50.0 * 6.223666667938232
Epoch 1810, val loss: 1.4935003519058228
Epoch 1820, training loss: 311.2768249511719 = 0.045302864164114 + 50.0 * 6.224630832672119
Epoch 1820, val loss: 1.4976491928100586
Epoch 1830, training loss: 311.1371765136719 = 0.04439525678753853 + 50.0 * 6.221855640411377
Epoch 1830, val loss: 1.5020805597305298
Epoch 1840, training loss: 311.0684509277344 = 0.04353528097271919 + 50.0 * 6.220498085021973
Epoch 1840, val loss: 1.5066134929656982
Epoch 1850, training loss: 311.1817932128906 = 0.042722590267658234 + 50.0 * 6.222781658172607
Epoch 1850, val loss: 1.5111266374588013
Epoch 1860, training loss: 311.2646789550781 = 0.04189397394657135 + 50.0 * 6.224455833435059
Epoch 1860, val loss: 1.5153330564498901
Epoch 1870, training loss: 311.1211853027344 = 0.04107377305626869 + 50.0 * 6.221602439880371
Epoch 1870, val loss: 1.5195468664169312
Epoch 1880, training loss: 311.07806396484375 = 0.0403045229613781 + 50.0 * 6.220755100250244
Epoch 1880, val loss: 1.5238656997680664
Epoch 1890, training loss: 311.1546936035156 = 0.03955796733498573 + 50.0 * 6.2223029136657715
Epoch 1890, val loss: 1.5281012058258057
Epoch 1900, training loss: 311.0184631347656 = 0.03881779685616493 + 50.0 * 6.219593048095703
Epoch 1900, val loss: 1.5327154397964478
Epoch 1910, training loss: 311.189208984375 = 0.03810839354991913 + 50.0 * 6.223021984100342
Epoch 1910, val loss: 1.5372728109359741
Epoch 1920, training loss: 311.21649169921875 = 0.03739660978317261 + 50.0 * 6.2235822677612305
Epoch 1920, val loss: 1.5404211282730103
Epoch 1930, training loss: 311.0381164550781 = 0.03670845180749893 + 50.0 * 6.220027923583984
Epoch 1930, val loss: 1.5451130867004395
Epoch 1940, training loss: 310.9564514160156 = 0.03604229539632797 + 50.0 * 6.218408107757568
Epoch 1940, val loss: 1.549067735671997
Epoch 1950, training loss: 310.9242248535156 = 0.03540736809372902 + 50.0 * 6.217776298522949
Epoch 1950, val loss: 1.5534769296646118
Epoch 1960, training loss: 310.9407653808594 = 0.03479091078042984 + 50.0 * 6.2181196212768555
Epoch 1960, val loss: 1.557698130607605
Epoch 1970, training loss: 311.416015625 = 0.03418963402509689 + 50.0 * 6.227636814117432
Epoch 1970, val loss: 1.5619257688522339
Epoch 1980, training loss: 311.0561828613281 = 0.03359246999025345 + 50.0 * 6.220451831817627
Epoch 1980, val loss: 1.5653598308563232
Epoch 1990, training loss: 310.89508056640625 = 0.03299154341220856 + 50.0 * 6.2172417640686035
Epoch 1990, val loss: 1.5694619417190552
Epoch 2000, training loss: 310.8976745605469 = 0.032432906329631805 + 50.0 * 6.217304706573486
Epoch 2000, val loss: 1.5735453367233276
Epoch 2010, training loss: 311.2518005371094 = 0.03189397603273392 + 50.0 * 6.224398136138916
Epoch 2010, val loss: 1.577431321144104
Epoch 2020, training loss: 311.0582275390625 = 0.03134599328041077 + 50.0 * 6.2205376625061035
Epoch 2020, val loss: 1.581519365310669
Epoch 2030, training loss: 311.03997802734375 = 0.03081689402461052 + 50.0 * 6.2201828956604
Epoch 2030, val loss: 1.5853092670440674
Epoch 2040, training loss: 311.0092468261719 = 0.030303725972771645 + 50.0 * 6.219578742980957
Epoch 2040, val loss: 1.5889112949371338
Epoch 2050, training loss: 310.8242492675781 = 0.029791541397571564 + 50.0 * 6.2158894538879395
Epoch 2050, val loss: 1.5930224657058716
Epoch 2060, training loss: 310.829833984375 = 0.029309019446372986 + 50.0 * 6.216010570526123
Epoch 2060, val loss: 1.5969774723052979
Epoch 2070, training loss: 310.87811279296875 = 0.028842046856880188 + 50.0 * 6.21698522567749
Epoch 2070, val loss: 1.6007498502731323
Epoch 2080, training loss: 311.10528564453125 = 0.02838338352739811 + 50.0 * 6.221538066864014
Epoch 2080, val loss: 1.6043503284454346
Epoch 2090, training loss: 310.9334716796875 = 0.027923518791794777 + 50.0 * 6.218111038208008
Epoch 2090, val loss: 1.6081981658935547
Epoch 2100, training loss: 310.89935302734375 = 0.02747802436351776 + 50.0 * 6.217437744140625
Epoch 2100, val loss: 1.6116702556610107
Epoch 2110, training loss: 310.8808898925781 = 0.02704821527004242 + 50.0 * 6.217076778411865
Epoch 2110, val loss: 1.6156742572784424
Epoch 2120, training loss: 310.93280029296875 = 0.026626037433743477 + 50.0 * 6.218123912811279
Epoch 2120, val loss: 1.6193214654922485
Epoch 2130, training loss: 310.7351989746094 = 0.026207517832517624 + 50.0 * 6.214179992675781
Epoch 2130, val loss: 1.6227256059646606
Epoch 2140, training loss: 310.7791442871094 = 0.025813374668359756 + 50.0 * 6.215066432952881
Epoch 2140, val loss: 1.626313328742981
Epoch 2150, training loss: 311.1332092285156 = 0.02543572708964348 + 50.0 * 6.222155570983887
Epoch 2150, val loss: 1.6297883987426758
Epoch 2160, training loss: 310.79974365234375 = 0.025025827810168266 + 50.0 * 6.215494155883789
Epoch 2160, val loss: 1.633378028869629
Epoch 2170, training loss: 310.7467956542969 = 0.024649454280734062 + 50.0 * 6.214442729949951
Epoch 2170, val loss: 1.636641502380371
Epoch 2180, training loss: 310.7206115722656 = 0.02428937703371048 + 50.0 * 6.213926315307617
Epoch 2180, val loss: 1.6403249502182007
Epoch 2190, training loss: 311.03759765625 = 0.02394830249249935 + 50.0 * 6.220273017883301
Epoch 2190, val loss: 1.643609642982483
Epoch 2200, training loss: 310.8467102050781 = 0.023586556315422058 + 50.0 * 6.216462135314941
Epoch 2200, val loss: 1.646849513053894
Epoch 2210, training loss: 310.8709411621094 = 0.023244906216859818 + 50.0 * 6.216953754425049
Epoch 2210, val loss: 1.6499971151351929
Epoch 2220, training loss: 310.7237243652344 = 0.02289271168410778 + 50.0 * 6.214016437530518
Epoch 2220, val loss: 1.6533430814743042
Epoch 2230, training loss: 310.7813720703125 = 0.022566353902220726 + 50.0 * 6.215176105499268
Epoch 2230, val loss: 1.6567002534866333
Epoch 2240, training loss: 310.7605895996094 = 0.022247064858675003 + 50.0 * 6.214766979217529
Epoch 2240, val loss: 1.6600066423416138
Epoch 2250, training loss: 310.7737121582031 = 0.02192990481853485 + 50.0 * 6.215035438537598
Epoch 2250, val loss: 1.6634513139724731
Epoch 2260, training loss: 310.77764892578125 = 0.021619625389575958 + 50.0 * 6.215120792388916
Epoch 2260, val loss: 1.666651964187622
Epoch 2270, training loss: 310.657958984375 = 0.021315595135092735 + 50.0 * 6.212733268737793
Epoch 2270, val loss: 1.6697592735290527
Epoch 2280, training loss: 310.68035888671875 = 0.02102270908653736 + 50.0 * 6.213186740875244
Epoch 2280, val loss: 1.6728330850601196
Epoch 2290, training loss: 310.7613220214844 = 0.02073400281369686 + 50.0 * 6.2148118019104
Epoch 2290, val loss: 1.6758877038955688
Epoch 2300, training loss: 310.7080383300781 = 0.0204617939889431 + 50.0 * 6.213751792907715
Epoch 2300, val loss: 1.6791871786117554
Epoch 2310, training loss: 310.7137451171875 = 0.020186772570014 + 50.0 * 6.213871479034424
Epoch 2310, val loss: 1.68204665184021
Epoch 2320, training loss: 310.7261657714844 = 0.0199150200933218 + 50.0 * 6.21412467956543
Epoch 2320, val loss: 1.6852895021438599
Epoch 2330, training loss: 310.6403503417969 = 0.019637368619441986 + 50.0 * 6.212414264678955
Epoch 2330, val loss: 1.6884644031524658
Epoch 2340, training loss: 310.6139221191406 = 0.019377680495381355 + 50.0 * 6.211890697479248
Epoch 2340, val loss: 1.6913293600082397
Epoch 2350, training loss: 310.6000671386719 = 0.019129928201436996 + 50.0 * 6.211618900299072
Epoch 2350, val loss: 1.6943790912628174
Epoch 2360, training loss: 310.6981201171875 = 0.01888546347618103 + 50.0 * 6.2135844230651855
Epoch 2360, val loss: 1.6973347663879395
Epoch 2370, training loss: 310.6957702636719 = 0.018640775233507156 + 50.0 * 6.213542461395264
Epoch 2370, val loss: 1.7002190351486206
Epoch 2380, training loss: 310.61346435546875 = 0.018396640196442604 + 50.0 * 6.2119011878967285
Epoch 2380, val loss: 1.7028236389160156
Epoch 2390, training loss: 310.5666198730469 = 0.018164921551942825 + 50.0 * 6.210968971252441
Epoch 2390, val loss: 1.7056692838668823
Epoch 2400, training loss: 310.6026916503906 = 0.017939448356628418 + 50.0 * 6.211694717407227
Epoch 2400, val loss: 1.7087432146072388
Epoch 2410, training loss: 310.568603515625 = 0.017708726227283478 + 50.0 * 6.211018085479736
Epoch 2410, val loss: 1.7115923166275024
Epoch 2420, training loss: 310.82861328125 = 0.017490873113274574 + 50.0 * 6.216222763061523
Epoch 2420, val loss: 1.7146282196044922
Epoch 2430, training loss: 310.5918273925781 = 0.017262548208236694 + 50.0 * 6.211491107940674
Epoch 2430, val loss: 1.7171995639801025
Epoch 2440, training loss: 310.5099182128906 = 0.01705271750688553 + 50.0 * 6.20985746383667
Epoch 2440, val loss: 1.720114827156067
Epoch 2450, training loss: 310.6430358886719 = 0.016846349462866783 + 50.0 * 6.212523937225342
Epoch 2450, val loss: 1.722695231437683
Epoch 2460, training loss: 310.6154479980469 = 0.01663946360349655 + 50.0 * 6.211976051330566
Epoch 2460, val loss: 1.7256643772125244
Epoch 2470, training loss: 310.4722595214844 = 0.016437532380223274 + 50.0 * 6.209116458892822
Epoch 2470, val loss: 1.7280186414718628
Epoch 2480, training loss: 310.442626953125 = 0.016241811215877533 + 50.0 * 6.208527565002441
Epoch 2480, val loss: 1.7309339046478271
Epoch 2490, training loss: 310.43707275390625 = 0.016054950654506683 + 50.0 * 6.208420276641846
Epoch 2490, val loss: 1.7337790727615356
Epoch 2500, training loss: 310.5503845214844 = 0.015875039622187614 + 50.0 * 6.210690498352051
Epoch 2500, val loss: 1.7361361980438232
Epoch 2510, training loss: 310.4855651855469 = 0.015685029327869415 + 50.0 * 6.209397315979004
Epoch 2510, val loss: 1.7389912605285645
Epoch 2520, training loss: 310.6479797363281 = 0.015499335713684559 + 50.0 * 6.212649822235107
Epoch 2520, val loss: 1.7413866519927979
Epoch 2530, training loss: 310.4432373046875 = 0.01531414408236742 + 50.0 * 6.208558082580566
Epoch 2530, val loss: 1.7440975904464722
Epoch 2540, training loss: 310.4454345703125 = 0.01514002401381731 + 50.0 * 6.208605766296387
Epoch 2540, val loss: 1.7465897798538208
Epoch 2550, training loss: 310.4912414550781 = 0.014969770796597004 + 50.0 * 6.209525108337402
Epoch 2550, val loss: 1.7495478391647339
Epoch 2560, training loss: 310.5623779296875 = 0.014798391610383987 + 50.0 * 6.210951805114746
Epoch 2560, val loss: 1.7519997358322144
Epoch 2570, training loss: 310.4761962890625 = 0.014636465348303318 + 50.0 * 6.209230899810791
Epoch 2570, val loss: 1.7541029453277588
Epoch 2580, training loss: 310.38165283203125 = 0.014471408911049366 + 50.0 * 6.207343578338623
Epoch 2580, val loss: 1.7568089962005615
Epoch 2590, training loss: 310.6064758300781 = 0.014324289746582508 + 50.0 * 6.211843490600586
Epoch 2590, val loss: 1.7591561079025269
Epoch 2600, training loss: 310.5088195800781 = 0.014165814034640789 + 50.0 * 6.209893226623535
Epoch 2600, val loss: 1.7617110013961792
Epoch 2610, training loss: 310.37139892578125 = 0.013999790884554386 + 50.0 * 6.20714807510376
Epoch 2610, val loss: 1.7640819549560547
Epoch 2620, training loss: 310.33441162109375 = 0.01385045051574707 + 50.0 * 6.206410884857178
Epoch 2620, val loss: 1.7665196657180786
Epoch 2630, training loss: 310.3412780761719 = 0.013707016594707966 + 50.0 * 6.206551551818848
Epoch 2630, val loss: 1.7690833806991577
Epoch 2640, training loss: 310.477783203125 = 0.013568531721830368 + 50.0 * 6.20928430557251
Epoch 2640, val loss: 1.7713221311569214
Epoch 2650, training loss: 310.5497741699219 = 0.013423633761703968 + 50.0 * 6.210726737976074
Epoch 2650, val loss: 1.7735456228256226
Epoch 2660, training loss: 310.4031982421875 = 0.013279099017381668 + 50.0 * 6.207798480987549
Epoch 2660, val loss: 1.7757686376571655
Epoch 2670, training loss: 310.45941162109375 = 0.013142707757651806 + 50.0 * 6.208925247192383
Epoch 2670, val loss: 1.7781704664230347
Epoch 2680, training loss: 310.3551330566406 = 0.013003431260585785 + 50.0 * 6.20684289932251
Epoch 2680, val loss: 1.7803058624267578
Epoch 2690, training loss: 310.3150634765625 = 0.012866680510342121 + 50.0 * 6.206043720245361
Epoch 2690, val loss: 1.7829983234405518
Epoch 2700, training loss: 310.458251953125 = 0.01273969653993845 + 50.0 * 6.2089104652404785
Epoch 2700, val loss: 1.7851325273513794
Epoch 2710, training loss: 310.4408264160156 = 0.012613601982593536 + 50.0 * 6.208564281463623
Epoch 2710, val loss: 1.7872179746627808
Epoch 2720, training loss: 310.26776123046875 = 0.012478409335017204 + 50.0 * 6.205105304718018
Epoch 2720, val loss: 1.789465069770813
Epoch 2730, training loss: 310.3350830078125 = 0.012356840074062347 + 50.0 * 6.206454277038574
Epoch 2730, val loss: 1.7917729616165161
Epoch 2740, training loss: 310.4170837402344 = 0.012240498326718807 + 50.0 * 6.208096981048584
Epoch 2740, val loss: 1.7937157154083252
Epoch 2750, training loss: 310.4846496582031 = 0.012119026854634285 + 50.0 * 6.209450721740723
Epoch 2750, val loss: 1.795767068862915
Epoch 2760, training loss: 310.3108825683594 = 0.011990251019597054 + 50.0 * 6.205977916717529
Epoch 2760, val loss: 1.7981951236724854
Epoch 2770, training loss: 310.2532043457031 = 0.011869627051055431 + 50.0 * 6.204826354980469
Epoch 2770, val loss: 1.8002985715866089
Epoch 2780, training loss: 310.23443603515625 = 0.011757655069231987 + 50.0 * 6.204453468322754
Epoch 2780, val loss: 1.8023648262023926
Epoch 2790, training loss: 310.2761535644531 = 0.011650529690086842 + 50.0 * 6.205289840698242
Epoch 2790, val loss: 1.8048005104064941
Epoch 2800, training loss: 310.4605407714844 = 0.011544317938387394 + 50.0 * 6.208980083465576
Epoch 2800, val loss: 1.8066588640213013
Epoch 2810, training loss: 310.31866455078125 = 0.011427486315369606 + 50.0 * 6.2061448097229
Epoch 2810, val loss: 1.8083223104476929
Epoch 2820, training loss: 310.27874755859375 = 0.011321624740958214 + 50.0 * 6.205348491668701
Epoch 2820, val loss: 1.8107572793960571
Epoch 2830, training loss: 310.51092529296875 = 0.011221310123801231 + 50.0 * 6.209993839263916
Epoch 2830, val loss: 1.8126827478408813
Epoch 2840, training loss: 310.5983581542969 = 0.011112228967249393 + 50.0 * 6.211745262145996
Epoch 2840, val loss: 1.8144445419311523
Epoch 2850, training loss: 310.2720947265625 = 0.010996876284480095 + 50.0 * 6.205222129821777
Epoch 2850, val loss: 1.8162944316864014
Epoch 2860, training loss: 310.20751953125 = 0.0108958650380373 + 50.0 * 6.203932762145996
Epoch 2860, val loss: 1.8184314966201782
Epoch 2870, training loss: 310.1714172363281 = 0.0107998913154006 + 50.0 * 6.203212261199951
Epoch 2870, val loss: 1.8205533027648926
Epoch 2880, training loss: 310.2982482910156 = 0.010706047527492046 + 50.0 * 6.205750942230225
Epoch 2880, val loss: 1.8228777647018433
Epoch 2890, training loss: 310.2425231933594 = 0.010606823489069939 + 50.0 * 6.2046380043029785
Epoch 2890, val loss: 1.8244469165802002
Epoch 2900, training loss: 310.2269592285156 = 0.010512610897421837 + 50.0 * 6.204329490661621
Epoch 2900, val loss: 1.8258589506149292
Epoch 2910, training loss: 310.2607727050781 = 0.010417398065328598 + 50.0 * 6.205007553100586
Epoch 2910, val loss: 1.82815420627594
Epoch 2920, training loss: 310.40509033203125 = 0.010327027179300785 + 50.0 * 6.207895278930664
Epoch 2920, val loss: 1.8296135663986206
Epoch 2930, training loss: 310.2091369628906 = 0.010235798545181751 + 50.0 * 6.203978061676025
Epoch 2930, val loss: 1.8316333293914795
Epoch 2940, training loss: 310.1603698730469 = 0.010148359462618828 + 50.0 * 6.203003883361816
Epoch 2940, val loss: 1.8332282304763794
Epoch 2950, training loss: 310.179443359375 = 0.010061092674732208 + 50.0 * 6.203387260437012
Epoch 2950, val loss: 1.835118055343628
Epoch 2960, training loss: 310.2527160644531 = 0.009981323964893818 + 50.0 * 6.204854488372803
Epoch 2960, val loss: 1.8366574048995972
Epoch 2970, training loss: 310.2300720214844 = 0.009895389899611473 + 50.0 * 6.204403400421143
Epoch 2970, val loss: 1.8386203050613403
Epoch 2980, training loss: 310.1858825683594 = 0.009804642759263515 + 50.0 * 6.203521728515625
Epoch 2980, val loss: 1.8404829502105713
Epoch 2990, training loss: 310.1994323730469 = 0.009722547605633736 + 50.0 * 6.203794002532959
Epoch 2990, val loss: 1.842224359512329
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 431.80145263671875 = 1.9576213359832764 + 50.0 * 8.596877098083496
Epoch 0, val loss: 1.9649473428726196
Epoch 10, training loss: 431.7668151855469 = 1.9486119747161865 + 50.0 * 8.59636402130127
Epoch 10, val loss: 1.954961895942688
Epoch 20, training loss: 431.55889892578125 = 1.937516212463379 + 50.0 * 8.592427253723145
Epoch 20, val loss: 1.942494511604309
Epoch 30, training loss: 430.1595764160156 = 1.9231302738189697 + 50.0 * 8.564728736877441
Epoch 30, val loss: 1.9259668588638306
Epoch 40, training loss: 422.97918701171875 = 1.9049458503723145 + 50.0 * 8.42148494720459
Epoch 40, val loss: 1.9055488109588623
Epoch 50, training loss: 396.0972900390625 = 1.885646104812622 + 50.0 * 7.884232997894287
Epoch 50, val loss: 1.8840746879577637
Epoch 60, training loss: 377.6539001464844 = 1.8694871664047241 + 50.0 * 7.515688419342041
Epoch 60, val loss: 1.8683339357376099
Epoch 70, training loss: 362.6139831542969 = 1.857460379600525 + 50.0 * 7.215130805969238
Epoch 70, val loss: 1.8570656776428223
Epoch 80, training loss: 352.4110107421875 = 1.845493197441101 + 50.0 * 7.011310577392578
Epoch 80, val loss: 1.8455500602722168
Epoch 90, training loss: 347.6486511230469 = 1.834498405456543 + 50.0 * 6.916283130645752
Epoch 90, val loss: 1.8348863124847412
Epoch 100, training loss: 344.5732421875 = 1.8243600130081177 + 50.0 * 6.854977607727051
Epoch 100, val loss: 1.8248534202575684
Epoch 110, training loss: 341.1158752441406 = 1.8153268098831177 + 50.0 * 6.7860107421875
Epoch 110, val loss: 1.816008448600769
Epoch 120, training loss: 338.1544189453125 = 1.8078221082687378 + 50.0 * 6.726931571960449
Epoch 120, val loss: 1.808666706085205
Epoch 130, training loss: 335.7906799316406 = 1.8003132343292236 + 50.0 * 6.679807186126709
Epoch 130, val loss: 1.801604151725769
Epoch 140, training loss: 333.72674560546875 = 1.7925262451171875 + 50.0 * 6.6386847496032715
Epoch 140, val loss: 1.7943682670593262
Epoch 150, training loss: 331.7981262207031 = 1.7844369411468506 + 50.0 * 6.600274085998535
Epoch 150, val loss: 1.7872151136398315
Epoch 160, training loss: 330.1299133300781 = 1.775892734527588 + 50.0 * 6.567080497741699
Epoch 160, val loss: 1.7797787189483643
Epoch 170, training loss: 328.62646484375 = 1.7666356563568115 + 50.0 * 6.537196636199951
Epoch 170, val loss: 1.7718698978424072
Epoch 180, training loss: 327.4276428222656 = 1.7566211223602295 + 50.0 * 6.513420581817627
Epoch 180, val loss: 1.7633566856384277
Epoch 190, training loss: 326.7217712402344 = 1.7454988956451416 + 50.0 * 6.49952507019043
Epoch 190, val loss: 1.7539267539978027
Epoch 200, training loss: 325.8140563964844 = 1.7333236932754517 + 50.0 * 6.481614589691162
Epoch 200, val loss: 1.7435643672943115
Epoch 210, training loss: 325.03521728515625 = 1.7200536727905273 + 50.0 * 6.46630334854126
Epoch 210, val loss: 1.7324198484420776
Epoch 220, training loss: 324.4107666015625 = 1.7056832313537598 + 50.0 * 6.4541015625
Epoch 220, val loss: 1.7204034328460693
Epoch 230, training loss: 323.8589172363281 = 1.6900241374969482 + 50.0 * 6.44337797164917
Epoch 230, val loss: 1.7073932886123657
Epoch 240, training loss: 323.5599365234375 = 1.6730926036834717 + 50.0 * 6.437736511230469
Epoch 240, val loss: 1.6933404207229614
Epoch 250, training loss: 323.0442199707031 = 1.6547555923461914 + 50.0 * 6.427789211273193
Epoch 250, val loss: 1.678261637687683
Epoch 260, training loss: 322.5311584472656 = 1.6351112127304077 + 50.0 * 6.41792106628418
Epoch 260, val loss: 1.6622309684753418
Epoch 270, training loss: 322.1450500488281 = 1.6142923831939697 + 50.0 * 6.410614967346191
Epoch 270, val loss: 1.6452932357788086
Epoch 280, training loss: 321.8069152832031 = 1.5922908782958984 + 50.0 * 6.404292583465576
Epoch 280, val loss: 1.6274865865707397
Epoch 290, training loss: 321.8188171386719 = 1.56907057762146 + 50.0 * 6.404994964599609
Epoch 290, val loss: 1.608849048614502
Epoch 300, training loss: 321.1639709472656 = 1.5448228120803833 + 50.0 * 6.392383098602295
Epoch 300, val loss: 1.5894027948379517
Epoch 310, training loss: 320.8608703613281 = 1.5199103355407715 + 50.0 * 6.386819362640381
Epoch 310, val loss: 1.5697269439697266
Epoch 320, training loss: 320.5578308105469 = 1.494234561920166 + 50.0 * 6.381271839141846
Epoch 320, val loss: 1.5496420860290527
Epoch 330, training loss: 320.2626037597656 = 1.468033790588379 + 50.0 * 6.375891208648682
Epoch 330, val loss: 1.5293487310409546
Epoch 340, training loss: 319.98828125 = 1.4413865804672241 + 50.0 * 6.370937824249268
Epoch 340, val loss: 1.5089303255081177
Epoch 350, training loss: 319.8483581542969 = 1.4143911600112915 + 50.0 * 6.368679046630859
Epoch 350, val loss: 1.4885164499282837
Epoch 360, training loss: 319.7272644042969 = 1.3867110013961792 + 50.0 * 6.3668107986450195
Epoch 360, val loss: 1.4676799774169922
Epoch 370, training loss: 319.306640625 = 1.3591078519821167 + 50.0 * 6.358950614929199
Epoch 370, val loss: 1.4470598697662354
Epoch 380, training loss: 319.0632629394531 = 1.3313993215560913 + 50.0 * 6.354637622833252
Epoch 380, val loss: 1.4265079498291016
Epoch 390, training loss: 318.83148193359375 = 1.3035749197006226 + 50.0 * 6.350557804107666
Epoch 390, val loss: 1.406114101409912
Epoch 400, training loss: 318.7971496582031 = 1.2756528854370117 + 50.0 * 6.350429534912109
Epoch 400, val loss: 1.3857955932617188
Epoch 410, training loss: 318.6671447753906 = 1.2476999759674072 + 50.0 * 6.348388671875
Epoch 410, val loss: 1.3656171560287476
Epoch 420, training loss: 318.278076171875 = 1.2196707725524902 + 50.0 * 6.34116792678833
Epoch 420, val loss: 1.3455102443695068
Epoch 430, training loss: 318.1170349121094 = 1.1917965412139893 + 50.0 * 6.338504791259766
Epoch 430, val loss: 1.3257948160171509
Epoch 440, training loss: 317.93646240234375 = 1.1641100645065308 + 50.0 * 6.335446834564209
Epoch 440, val loss: 1.3063753843307495
Epoch 450, training loss: 317.79132080078125 = 1.136531949043274 + 50.0 * 6.333095550537109
Epoch 450, val loss: 1.2871288061141968
Epoch 460, training loss: 317.75439453125 = 1.1089425086975098 + 50.0 * 6.332909107208252
Epoch 460, val loss: 1.2681350708007812
Epoch 470, training loss: 317.5096435546875 = 1.0816704034805298 + 50.0 * 6.328559875488281
Epoch 470, val loss: 1.2494069337844849
Epoch 480, training loss: 317.3912658691406 = 1.0546176433563232 + 50.0 * 6.326732635498047
Epoch 480, val loss: 1.2310093641281128
Epoch 490, training loss: 317.31201171875 = 1.0279548168182373 + 50.0 * 6.325681209564209
Epoch 490, val loss: 1.2131072282791138
Epoch 500, training loss: 317.1151428222656 = 1.001432180404663 + 50.0 * 6.322274208068848
Epoch 500, val loss: 1.1954976320266724
Epoch 510, training loss: 316.9322509765625 = 0.975572407245636 + 50.0 * 6.319133281707764
Epoch 510, val loss: 1.1785749197006226
Epoch 520, training loss: 316.9606018066406 = 0.9501513242721558 + 50.0 * 6.320208549499512
Epoch 520, val loss: 1.1621888875961304
Epoch 530, training loss: 316.8020935058594 = 0.9247626662254333 + 50.0 * 6.317546367645264
Epoch 530, val loss: 1.1460407972335815
Epoch 540, training loss: 316.5566101074219 = 0.9000208377838135 + 50.0 * 6.313131809234619
Epoch 540, val loss: 1.130513072013855
Epoch 550, training loss: 316.4321594238281 = 0.8758137226104736 + 50.0 * 6.311126708984375
Epoch 550, val loss: 1.1156913042068481
Epoch 560, training loss: 316.55010986328125 = 0.8520565032958984 + 50.0 * 6.313961029052734
Epoch 560, val loss: 1.10141921043396
Epoch 570, training loss: 316.2584228515625 = 0.8285723328590393 + 50.0 * 6.308597087860107
Epoch 570, val loss: 1.0875364542007446
Epoch 580, training loss: 316.32952880859375 = 0.8056445717811584 + 50.0 * 6.3104777336120605
Epoch 580, val loss: 1.0742394924163818
Epoch 590, training loss: 316.0249328613281 = 0.7835158109664917 + 50.0 * 6.304828643798828
Epoch 590, val loss: 1.0618385076522827
Epoch 600, training loss: 315.8395080566406 = 0.7616071105003357 + 50.0 * 6.301558494567871
Epoch 600, val loss: 1.049766182899475
Epoch 610, training loss: 315.86566162109375 = 0.7402334809303284 + 50.0 * 6.302508354187012
Epoch 610, val loss: 1.038298487663269
Epoch 620, training loss: 315.6209716796875 = 0.7194251418113708 + 50.0 * 6.298030853271484
Epoch 620, val loss: 1.0277175903320312
Epoch 630, training loss: 315.8035888671875 = 0.6991012096405029 + 50.0 * 6.302089691162109
Epoch 630, val loss: 1.0174874067306519
Epoch 640, training loss: 315.7966613769531 = 0.6790701746940613 + 50.0 * 6.302351951599121
Epoch 640, val loss: 1.0073652267456055
Epoch 650, training loss: 315.44659423828125 = 0.6594865918159485 + 50.0 * 6.295742034912109
Epoch 650, val loss: 0.9980289936065674
Epoch 660, training loss: 315.26788330078125 = 0.6406446695327759 + 50.0 * 6.292544364929199
Epoch 660, val loss: 0.9894904494285583
Epoch 670, training loss: 315.1982727050781 = 0.6223371028900146 + 50.0 * 6.291518688201904
Epoch 670, val loss: 0.981445848941803
Epoch 680, training loss: 315.2847900390625 = 0.6044743657112122 + 50.0 * 6.293606758117676
Epoch 680, val loss: 0.9738079905509949
Epoch 690, training loss: 315.0565185546875 = 0.5867472887039185 + 50.0 * 6.289394855499268
Epoch 690, val loss: 0.9665260314941406
Epoch 700, training loss: 314.9553527832031 = 0.5698643922805786 + 50.0 * 6.287710189819336
Epoch 700, val loss: 0.9600522518157959
Epoch 710, training loss: 315.09613037109375 = 0.5532835721969604 + 50.0 * 6.290856838226318
Epoch 710, val loss: 0.9538546800613403
Epoch 720, training loss: 314.8701171875 = 0.5372886061668396 + 50.0 * 6.286656379699707
Epoch 720, val loss: 0.9484420418739319
Epoch 730, training loss: 314.8450012207031 = 0.5216494798660278 + 50.0 * 6.286467552185059
Epoch 730, val loss: 0.9432984590530396
Epoch 740, training loss: 314.58197021484375 = 0.5062891840934753 + 50.0 * 6.281513690948486
Epoch 740, val loss: 0.9382837414741516
Epoch 750, training loss: 314.57666015625 = 0.49150702357292175 + 50.0 * 6.281702995300293
Epoch 750, val loss: 0.9340990781784058
Epoch 760, training loss: 314.4571533203125 = 0.4771643280982971 + 50.0 * 6.279599666595459
Epoch 760, val loss: 0.9302873015403748
Epoch 770, training loss: 314.4456481933594 = 0.4631898105144501 + 50.0 * 6.279648780822754
Epoch 770, val loss: 0.9269247055053711
Epoch 780, training loss: 314.39190673828125 = 0.44957613945007324 + 50.0 * 6.278846740722656
Epoch 780, val loss: 0.9239557385444641
Epoch 790, training loss: 314.3958435058594 = 0.4363117218017578 + 50.0 * 6.279190540313721
Epoch 790, val loss: 0.9213011860847473
Epoch 800, training loss: 314.1902770996094 = 0.4233447313308716 + 50.0 * 6.275338649749756
Epoch 800, val loss: 0.9191768169403076
Epoch 810, training loss: 314.12713623046875 = 0.4107321500778198 + 50.0 * 6.274328231811523
Epoch 810, val loss: 0.9175241589546204
Epoch 820, training loss: 314.20648193359375 = 0.39845967292785645 + 50.0 * 6.27616024017334
Epoch 820, val loss: 0.9161179661750793
Epoch 830, training loss: 314.0271301269531 = 0.3865790069103241 + 50.0 * 6.272811412811279
Epoch 830, val loss: 0.9152563214302063
Epoch 840, training loss: 313.97113037109375 = 0.3749878406524658 + 50.0 * 6.271922588348389
Epoch 840, val loss: 0.9147660136222839
Epoch 850, training loss: 314.12744140625 = 0.36372673511505127 + 50.0 * 6.27527379989624
Epoch 850, val loss: 0.9143703579902649
Epoch 860, training loss: 313.8884582519531 = 0.3528901934623718 + 50.0 * 6.2707109451293945
Epoch 860, val loss: 0.9145005345344543
Epoch 870, training loss: 313.8192138671875 = 0.3422793745994568 + 50.0 * 6.269538402557373
Epoch 870, val loss: 0.9148373007774353
Epoch 880, training loss: 313.8478698730469 = 0.3320045471191406 + 50.0 * 6.270317077636719
Epoch 880, val loss: 0.9155316352844238
Epoch 890, training loss: 313.6681823730469 = 0.3221468925476074 + 50.0 * 6.266921043395996
Epoch 890, val loss: 0.9167176485061646
Epoch 900, training loss: 313.5489501953125 = 0.3125242292881012 + 50.0 * 6.264728546142578
Epoch 900, val loss: 0.9180415868759155
Epoch 910, training loss: 313.66204833984375 = 0.3032568395137787 + 50.0 * 6.267176151275635
Epoch 910, val loss: 0.9196085929870605
Epoch 920, training loss: 313.6831970214844 = 0.29420483112335205 + 50.0 * 6.267779350280762
Epoch 920, val loss: 0.921574592590332
Epoch 930, training loss: 313.70098876953125 = 0.28540387749671936 + 50.0 * 6.268311500549316
Epoch 930, val loss: 0.9235604405403137
Epoch 940, training loss: 313.4186096191406 = 0.27677926421165466 + 50.0 * 6.262836456298828
Epoch 940, val loss: 0.9256226420402527
Epoch 950, training loss: 313.345703125 = 0.26856669783592224 + 50.0 * 6.261542797088623
Epoch 950, val loss: 0.9284112453460693
Epoch 960, training loss: 313.2603454589844 = 0.2606709897518158 + 50.0 * 6.259993553161621
Epoch 960, val loss: 0.9313703179359436
Epoch 970, training loss: 313.64227294921875 = 0.2529907524585724 + 50.0 * 6.267785549163818
Epoch 970, val loss: 0.9343530535697937
Epoch 980, training loss: 313.4626159667969 = 0.2454996109008789 + 50.0 * 6.264342308044434
Epoch 980, val loss: 0.9374244809150696
Epoch 990, training loss: 313.1397705078125 = 0.23830370604991913 + 50.0 * 6.258028984069824
Epoch 990, val loss: 0.9409065246582031
Epoch 1000, training loss: 313.1355895996094 = 0.23142361640930176 + 50.0 * 6.258083343505859
Epoch 1000, val loss: 0.9446749687194824
Epoch 1010, training loss: 313.2587585449219 = 0.22481437027454376 + 50.0 * 6.260678768157959
Epoch 1010, val loss: 0.9486004710197449
Epoch 1020, training loss: 313.0226745605469 = 0.21824343502521515 + 50.0 * 6.256088733673096
Epoch 1020, val loss: 0.9521979689598083
Epoch 1030, training loss: 313.0431213378906 = 0.21195775270462036 + 50.0 * 6.256623268127441
Epoch 1030, val loss: 0.9561202526092529
Epoch 1040, training loss: 313.06439208984375 = 0.20597516000270844 + 50.0 * 6.257168292999268
Epoch 1040, val loss: 0.9604762196540833
Epoch 1050, training loss: 312.9835510253906 = 0.20010772347450256 + 50.0 * 6.255668640136719
Epoch 1050, val loss: 0.9647020101547241
Epoch 1060, training loss: 312.90704345703125 = 0.19444413483142853 + 50.0 * 6.254251480102539
Epoch 1060, val loss: 0.9690744876861572
Epoch 1070, training loss: 312.8651428222656 = 0.18903453648090363 + 50.0 * 6.2535223960876465
Epoch 1070, val loss: 0.9736632108688354
Epoch 1080, training loss: 313.265869140625 = 0.1838521510362625 + 50.0 * 6.261640548706055
Epoch 1080, val loss: 0.9784083366394043
Epoch 1090, training loss: 312.8636779785156 = 0.1785682737827301 + 50.0 * 6.253702640533447
Epoch 1090, val loss: 0.9825478792190552
Epoch 1100, training loss: 312.7129211425781 = 0.1736983209848404 + 50.0 * 6.250784397125244
Epoch 1100, val loss: 0.987515389919281
Epoch 1110, training loss: 312.6828308105469 = 0.16896571218967438 + 50.0 * 6.250277042388916
Epoch 1110, val loss: 0.9925013780593872
Epoch 1120, training loss: 312.8101501464844 = 0.16441404819488525 + 50.0 * 6.2529144287109375
Epoch 1120, val loss: 0.9974009394645691
Epoch 1130, training loss: 312.88983154296875 = 0.15988965332508087 + 50.0 * 6.254599094390869
Epoch 1130, val loss: 1.002377986907959
Epoch 1140, training loss: 312.7425842285156 = 0.1555333137512207 + 50.0 * 6.251740455627441
Epoch 1140, val loss: 1.0072063207626343
Epoch 1150, training loss: 312.5334777832031 = 0.15134580433368683 + 50.0 * 6.247642993927002
Epoch 1150, val loss: 1.012317419052124
Epoch 1160, training loss: 312.5319519042969 = 0.1473415046930313 + 50.0 * 6.247692108154297
Epoch 1160, val loss: 1.0176925659179688
Epoch 1170, training loss: 312.8839111328125 = 0.14344888925552368 + 50.0 * 6.2548089027404785
Epoch 1170, val loss: 1.0227471590042114
Epoch 1180, training loss: 312.54949951171875 = 0.13968710601329803 + 50.0 * 6.248196125030518
Epoch 1180, val loss: 1.0283401012420654
Epoch 1190, training loss: 312.4495849609375 = 0.13602271676063538 + 50.0 * 6.246271133422852
Epoch 1190, val loss: 1.0335781574249268
Epoch 1200, training loss: 312.3757019042969 = 0.13251690566539764 + 50.0 * 6.244863986968994
Epoch 1200, val loss: 1.0391408205032349
Epoch 1210, training loss: 312.4964904785156 = 0.12912052869796753 + 50.0 * 6.247347354888916
Epoch 1210, val loss: 1.0444613695144653
Epoch 1220, training loss: 312.4411315917969 = 0.12580087780952454 + 50.0 * 6.246306896209717
Epoch 1220, val loss: 1.0500397682189941
Epoch 1230, training loss: 312.42291259765625 = 0.12258070707321167 + 50.0 * 6.246006488800049
Epoch 1230, val loss: 1.055275321006775
Epoch 1240, training loss: 312.28619384765625 = 0.11949056386947632 + 50.0 * 6.2433342933654785
Epoch 1240, val loss: 1.0610724687576294
Epoch 1250, training loss: 312.26263427734375 = 0.11650045961141586 + 50.0 * 6.242923259735107
Epoch 1250, val loss: 1.0666866302490234
Epoch 1260, training loss: 312.5187072753906 = 0.11359371989965439 + 50.0 * 6.248102188110352
Epoch 1260, val loss: 1.0720096826553345
Epoch 1270, training loss: 312.37139892578125 = 0.11080125719308853 + 50.0 * 6.245212078094482
Epoch 1270, val loss: 1.0780030488967896
Epoch 1280, training loss: 312.2059326171875 = 0.10804693400859833 + 50.0 * 6.241958141326904
Epoch 1280, val loss: 1.0836660861968994
Epoch 1290, training loss: 312.16143798828125 = 0.10543319582939148 + 50.0 * 6.241119861602783
Epoch 1290, val loss: 1.0892972946166992
Epoch 1300, training loss: 312.2940368652344 = 0.10288986563682556 + 50.0 * 6.243823051452637
Epoch 1300, val loss: 1.0951862335205078
Epoch 1310, training loss: 312.17364501953125 = 0.10037464648485184 + 50.0 * 6.2414655685424805
Epoch 1310, val loss: 1.1008079051971436
Epoch 1320, training loss: 312.0555114746094 = 0.09793446958065033 + 50.0 * 6.239151477813721
Epoch 1320, val loss: 1.1064188480377197
Epoch 1330, training loss: 312.0250244140625 = 0.09561564028263092 + 50.0 * 6.238588333129883
Epoch 1330, val loss: 1.1122503280639648
Epoch 1340, training loss: 312.2464294433594 = 0.09335571527481079 + 50.0 * 6.243061542510986
Epoch 1340, val loss: 1.1178098917007446
Epoch 1350, training loss: 312.20208740234375 = 0.09112744778394699 + 50.0 * 6.242218971252441
Epoch 1350, val loss: 1.1235769987106323
Epoch 1360, training loss: 312.0369873046875 = 0.08897086977958679 + 50.0 * 6.2389607429504395
Epoch 1360, val loss: 1.1293110847473145
Epoch 1370, training loss: 311.9250793457031 = 0.08690294623374939 + 50.0 * 6.2367634773254395
Epoch 1370, val loss: 1.135071873664856
Epoch 1380, training loss: 311.904541015625 = 0.08491673320531845 + 50.0 * 6.236392021179199
Epoch 1380, val loss: 1.1410233974456787
Epoch 1390, training loss: 311.89910888671875 = 0.08298853039741516 + 50.0 * 6.236322402954102
Epoch 1390, val loss: 1.146728754043579
Epoch 1400, training loss: 312.3968505859375 = 0.08112511783838272 + 50.0 * 6.246314525604248
Epoch 1400, val loss: 1.1526247262954712
Epoch 1410, training loss: 312.27685546875 = 0.07925756275653839 + 50.0 * 6.24395227432251
Epoch 1410, val loss: 1.1582667827606201
Epoch 1420, training loss: 311.8641662597656 = 0.07737637311220169 + 50.0 * 6.235735893249512
Epoch 1420, val loss: 1.1635414361953735
Epoch 1430, training loss: 311.8186340332031 = 0.07565609365701675 + 50.0 * 6.234859466552734
Epoch 1430, val loss: 1.169433832168579
Epoch 1440, training loss: 311.77777099609375 = 0.07399675250053406 + 50.0 * 6.23407506942749
Epoch 1440, val loss: 1.175321102142334
Epoch 1450, training loss: 311.8008728027344 = 0.07238458096981049 + 50.0 * 6.234569549560547
Epoch 1450, val loss: 1.181120753288269
Epoch 1460, training loss: 312.0181884765625 = 0.07082147896289825 + 50.0 * 6.23894739151001
Epoch 1460, val loss: 1.186888337135315
Epoch 1470, training loss: 311.7954406738281 = 0.06920702010393143 + 50.0 * 6.234524726867676
Epoch 1470, val loss: 1.192194938659668
Epoch 1480, training loss: 311.70574951171875 = 0.06770607829093933 + 50.0 * 6.232760906219482
Epoch 1480, val loss: 1.1979620456695557
Epoch 1490, training loss: 311.7712097167969 = 0.06626033037900925 + 50.0 * 6.234099388122559
Epoch 1490, val loss: 1.2036340236663818
Epoch 1500, training loss: 311.7942199707031 = 0.06482765823602676 + 50.0 * 6.234588146209717
Epoch 1500, val loss: 1.209127426147461
Epoch 1510, training loss: 311.85986328125 = 0.06344304233789444 + 50.0 * 6.235928535461426
Epoch 1510, val loss: 1.2146360874176025
Epoch 1520, training loss: 311.6698913574219 = 0.062069498002529144 + 50.0 * 6.232156276702881
Epoch 1520, val loss: 1.2200567722320557
Epoch 1530, training loss: 311.66729736328125 = 0.06075575202703476 + 50.0 * 6.232131004333496
Epoch 1530, val loss: 1.2256648540496826
Epoch 1540, training loss: 311.84698486328125 = 0.059476714581251144 + 50.0 * 6.235750198364258
Epoch 1540, val loss: 1.230844497680664
Epoch 1550, training loss: 311.6020202636719 = 0.058249685913324356 + 50.0 * 6.230875492095947
Epoch 1550, val loss: 1.2369800806045532
Epoch 1560, training loss: 311.5646667480469 = 0.057040855288505554 + 50.0 * 6.230152606964111
Epoch 1560, val loss: 1.2422163486480713
Epoch 1570, training loss: 311.7528381347656 = 0.055869899690151215 + 50.0 * 6.233939170837402
Epoch 1570, val loss: 1.2475882768630981
Epoch 1580, training loss: 311.5775146484375 = 0.054721761494874954 + 50.0 * 6.2304558753967285
Epoch 1580, val loss: 1.2532858848571777
Epoch 1590, training loss: 311.5199890136719 = 0.05361614003777504 + 50.0 * 6.22932767868042
Epoch 1590, val loss: 1.258554458618164
Epoch 1600, training loss: 311.557373046875 = 0.05252983048558235 + 50.0 * 6.23009729385376
Epoch 1600, val loss: 1.2639425992965698
Epoch 1610, training loss: 311.50726318359375 = 0.05148034170269966 + 50.0 * 6.2291154861450195
Epoch 1610, val loss: 1.2694090604782104
Epoch 1620, training loss: 311.6195373535156 = 0.05045301094651222 + 50.0 * 6.231381893157959
Epoch 1620, val loss: 1.2745139598846436
Epoch 1630, training loss: 311.6460876464844 = 0.04945524409413338 + 50.0 * 6.231932640075684
Epoch 1630, val loss: 1.279571533203125
Epoch 1640, training loss: 311.4757385253906 = 0.048495013266801834 + 50.0 * 6.228545188903809
Epoch 1640, val loss: 1.2852840423583984
Epoch 1650, training loss: 311.3883972167969 = 0.04753247648477554 + 50.0 * 6.2268171310424805
Epoch 1650, val loss: 1.2903616428375244
Epoch 1660, training loss: 311.42767333984375 = 0.04662022739648819 + 50.0 * 6.227621078491211
Epoch 1660, val loss: 1.2956831455230713
Epoch 1670, training loss: 311.6204528808594 = 0.04573100432753563 + 50.0 * 6.231494426727295
Epoch 1670, val loss: 1.3010185956954956
Epoch 1680, training loss: 311.59344482421875 = 0.044850993901491165 + 50.0 * 6.230971813201904
Epoch 1680, val loss: 1.306053638458252
Epoch 1690, training loss: 311.4563903808594 = 0.04398459941148758 + 50.0 * 6.228248596191406
Epoch 1690, val loss: 1.3111507892608643
Epoch 1700, training loss: 311.4040222167969 = 0.043144818395376205 + 50.0 * 6.227217197418213
Epoch 1700, val loss: 1.3163124322891235
Epoch 1710, training loss: 311.4022521972656 = 0.04234154894948006 + 50.0 * 6.227198600769043
Epoch 1710, val loss: 1.32155442237854
Epoch 1720, training loss: 311.4054870605469 = 0.041555050760507584 + 50.0 * 6.227278709411621
Epoch 1720, val loss: 1.3266072273254395
Epoch 1730, training loss: 311.3260803222656 = 0.040796391665935516 + 50.0 * 6.225705623626709
Epoch 1730, val loss: 1.3317973613739014
Epoch 1740, training loss: 311.3461608886719 = 0.04005290940403938 + 50.0 * 6.2261223793029785
Epoch 1740, val loss: 1.3372246026992798
Epoch 1750, training loss: 311.6123046875 = 0.039351437240839005 + 50.0 * 6.231459617614746
Epoch 1750, val loss: 1.34222412109375
Epoch 1760, training loss: 311.3004455566406 = 0.03858550637960434 + 50.0 * 6.2252373695373535
Epoch 1760, val loss: 1.3466620445251465
Epoch 1770, training loss: 311.22113037109375 = 0.037901319563388824 + 50.0 * 6.223664283752441
Epoch 1770, val loss: 1.3518837690353394
Epoch 1780, training loss: 311.28515625 = 0.03723523020744324 + 50.0 * 6.224958419799805
Epoch 1780, val loss: 1.356932520866394
Epoch 1790, training loss: 311.41021728515625 = 0.03656246140599251 + 50.0 * 6.227473258972168
Epoch 1790, val loss: 1.3617005348205566
Epoch 1800, training loss: 311.2575988769531 = 0.03592353314161301 + 50.0 * 6.224433422088623
Epoch 1800, val loss: 1.3666085004806519
Epoch 1810, training loss: 311.3212585449219 = 0.035293400287628174 + 50.0 * 6.225719451904297
Epoch 1810, val loss: 1.3713027238845825
Epoch 1820, training loss: 311.2221374511719 = 0.03467167168855667 + 50.0 * 6.22374963760376
Epoch 1820, val loss: 1.3762258291244507
Epoch 1830, training loss: 311.1803283691406 = 0.03406865894794464 + 50.0 * 6.222925662994385
Epoch 1830, val loss: 1.3809798955917358
Epoch 1840, training loss: 311.2913513183594 = 0.033486176282167435 + 50.0 * 6.225157260894775
Epoch 1840, val loss: 1.3854705095291138
Epoch 1850, training loss: 311.2727355957031 = 0.03290504217147827 + 50.0 * 6.224796772003174
Epoch 1850, val loss: 1.390503168106079
Epoch 1860, training loss: 311.17352294921875 = 0.032353002578020096 + 50.0 * 6.222823619842529
Epoch 1860, val loss: 1.395099401473999
Epoch 1870, training loss: 311.1053771972656 = 0.03180565685033798 + 50.0 * 6.221471786499023
Epoch 1870, val loss: 1.4000517129898071
Epoch 1880, training loss: 311.2379455566406 = 0.031290050595998764 + 50.0 * 6.224133014678955
Epoch 1880, val loss: 1.4049756526947021
Epoch 1890, training loss: 311.13629150390625 = 0.030760319903492928 + 50.0 * 6.222110748291016
Epoch 1890, val loss: 1.409254550933838
Epoch 1900, training loss: 311.0668640136719 = 0.030241766944527626 + 50.0 * 6.220732688903809
Epoch 1900, val loss: 1.4137606620788574
Epoch 1910, training loss: 311.13543701171875 = 0.029745256528258324 + 50.0 * 6.222113609313965
Epoch 1910, val loss: 1.4184458255767822
Epoch 1920, training loss: 311.1554870605469 = 0.029257148504257202 + 50.0 * 6.222524642944336
Epoch 1920, val loss: 1.4228596687316895
Epoch 1930, training loss: 311.08026123046875 = 0.028783781453967094 + 50.0 * 6.221029758453369
Epoch 1930, val loss: 1.427607536315918
Epoch 1940, training loss: 311.15594482421875 = 0.028328021988272667 + 50.0 * 6.222552299499512
Epoch 1940, val loss: 1.4321931600570679
Epoch 1950, training loss: 311.20794677734375 = 0.027880055829882622 + 50.0 * 6.2236008644104
Epoch 1950, val loss: 1.4366114139556885
Epoch 1960, training loss: 311.0861511230469 = 0.02740115113556385 + 50.0 * 6.221174716949463
Epoch 1960, val loss: 1.4404394626617432
Epoch 1970, training loss: 311.0121765136719 = 0.026978282257914543 + 50.0 * 6.2197041511535645
Epoch 1970, val loss: 1.4454561471939087
Epoch 1980, training loss: 311.11602783203125 = 0.026553817093372345 + 50.0 * 6.221789360046387
Epoch 1980, val loss: 1.4495092630386353
Epoch 1990, training loss: 310.9425048828125 = 0.02614593133330345 + 50.0 * 6.218327045440674
Epoch 1990, val loss: 1.4542136192321777
Epoch 2000, training loss: 310.9654541015625 = 0.025738932192325592 + 50.0 * 6.218794822692871
Epoch 2000, val loss: 1.4584091901779175
Epoch 2010, training loss: 311.1033935546875 = 0.0253597404807806 + 50.0 * 6.221560478210449
Epoch 2010, val loss: 1.462920069694519
Epoch 2020, training loss: 310.9504089355469 = 0.024957844987511635 + 50.0 * 6.218509197235107
Epoch 2020, val loss: 1.4671576023101807
Epoch 2030, training loss: 311.1033935546875 = 0.024582361802458763 + 50.0 * 6.221575736999512
Epoch 2030, val loss: 1.4712756872177124
Epoch 2040, training loss: 310.97698974609375 = 0.024194184690713882 + 50.0 * 6.219055652618408
Epoch 2040, val loss: 1.475093960762024
Epoch 2050, training loss: 310.8746337890625 = 0.023825041949748993 + 50.0 * 6.217016220092773
Epoch 2050, val loss: 1.4795445203781128
Epoch 2060, training loss: 310.9045715332031 = 0.02348320372402668 + 50.0 * 6.217621803283691
Epoch 2060, val loss: 1.4838064908981323
Epoch 2070, training loss: 311.15228271484375 = 0.023140978068113327 + 50.0 * 6.222583293914795
Epoch 2070, val loss: 1.4879472255706787
Epoch 2080, training loss: 310.94720458984375 = 0.022783804684877396 + 50.0 * 6.2184882164001465
Epoch 2080, val loss: 1.4916380643844604
Epoch 2090, training loss: 310.8854675292969 = 0.02245260775089264 + 50.0 * 6.217260360717773
Epoch 2090, val loss: 1.4960192441940308
Epoch 2100, training loss: 310.8497009277344 = 0.022132370620965958 + 50.0 * 6.216551303863525
Epoch 2100, val loss: 1.5000184774398804
Epoch 2110, training loss: 310.8371276855469 = 0.021812597289681435 + 50.0 * 6.216306209564209
Epoch 2110, val loss: 1.5040100812911987
Epoch 2120, training loss: 310.974853515625 = 0.021506451070308685 + 50.0 * 6.219066619873047
Epoch 2120, val loss: 1.508172631263733
Epoch 2130, training loss: 310.9107971191406 = 0.021199652925133705 + 50.0 * 6.21779203414917
Epoch 2130, val loss: 1.5119092464447021
Epoch 2140, training loss: 310.8648376464844 = 0.020902344956994057 + 50.0 * 6.216878414154053
Epoch 2140, val loss: 1.5161763429641724
Epoch 2150, training loss: 311.02166748046875 = 0.020602140575647354 + 50.0 * 6.2200212478637695
Epoch 2150, val loss: 1.519439697265625
Epoch 2160, training loss: 310.7663879394531 = 0.02031104266643524 + 50.0 * 6.214921474456787
Epoch 2160, val loss: 1.5240809917449951
Epoch 2170, training loss: 310.7592468261719 = 0.02003452554345131 + 50.0 * 6.214784622192383
Epoch 2170, val loss: 1.5277845859527588
Epoch 2180, training loss: 310.7436828613281 = 0.01975710690021515 + 50.0 * 6.214478492736816
Epoch 2180, val loss: 1.5315454006195068
Epoch 2190, training loss: 310.798095703125 = 0.019497035071253777 + 50.0 * 6.215571880340576
Epoch 2190, val loss: 1.5354009866714478
Epoch 2200, training loss: 311.0045471191406 = 0.019243936985731125 + 50.0 * 6.219706058502197
Epoch 2200, val loss: 1.5393627882003784
Epoch 2210, training loss: 310.8022155761719 = 0.01895967870950699 + 50.0 * 6.215665340423584
Epoch 2210, val loss: 1.54291832447052
Epoch 2220, training loss: 310.7487487792969 = 0.01870708167552948 + 50.0 * 6.214600563049316
Epoch 2220, val loss: 1.5465738773345947
Epoch 2230, training loss: 310.79071044921875 = 0.018461810424923897 + 50.0 * 6.215445041656494
Epoch 2230, val loss: 1.5507636070251465
Epoch 2240, training loss: 310.80157470703125 = 0.018215429037809372 + 50.0 * 6.215667247772217
Epoch 2240, val loss: 1.5541715621948242
Epoch 2250, training loss: 310.7069091796875 = 0.01797596924006939 + 50.0 * 6.213778972625732
Epoch 2250, val loss: 1.5580518245697021
Epoch 2260, training loss: 310.92279052734375 = 0.01775660365819931 + 50.0 * 6.218100547790527
Epoch 2260, val loss: 1.5618236064910889
Epoch 2270, training loss: 310.7634582519531 = 0.017507463693618774 + 50.0 * 6.214918613433838
Epoch 2270, val loss: 1.5651065111160278
Epoch 2280, training loss: 310.65313720703125 = 0.01727486401796341 + 50.0 * 6.212717533111572
Epoch 2280, val loss: 1.5685359239578247
Epoch 2290, training loss: 310.6380310058594 = 0.0170575063675642 + 50.0 * 6.212419509887695
Epoch 2290, val loss: 1.5723172426223755
Epoch 2300, training loss: 310.71514892578125 = 0.016842449083924294 + 50.0 * 6.213966369628906
Epoch 2300, val loss: 1.5756756067276
Epoch 2310, training loss: 310.7275085449219 = 0.016635436564683914 + 50.0 * 6.214217662811279
Epoch 2310, val loss: 1.57941472530365
Epoch 2320, training loss: 310.6985168457031 = 0.016423404216766357 + 50.0 * 6.213642120361328
Epoch 2320, val loss: 1.582777976989746
Epoch 2330, training loss: 310.7361145019531 = 0.016215555369853973 + 50.0 * 6.214398384094238
Epoch 2330, val loss: 1.5860276222229004
Epoch 2340, training loss: 310.65087890625 = 0.01601526513695717 + 50.0 * 6.2126970291137695
Epoch 2340, val loss: 1.5898115634918213
Epoch 2350, training loss: 310.59210205078125 = 0.015816705301404 + 50.0 * 6.211525917053223
Epoch 2350, val loss: 1.5929349660873413
Epoch 2360, training loss: 310.56884765625 = 0.01562498975545168 + 50.0 * 6.211064338684082
Epoch 2360, val loss: 1.5964938402175903
Epoch 2370, training loss: 310.9378356933594 = 0.015448088757693768 + 50.0 * 6.218447685241699
Epoch 2370, val loss: 1.600182056427002
Epoch 2380, training loss: 310.7033386230469 = 0.015246250666677952 + 50.0 * 6.213761806488037
Epoch 2380, val loss: 1.6024730205535889
Epoch 2390, training loss: 310.6420593261719 = 0.015058142133057117 + 50.0 * 6.212540149688721
Epoch 2390, val loss: 1.606364369392395
Epoch 2400, training loss: 310.86761474609375 = 0.014876573346555233 + 50.0 * 6.217054843902588
Epoch 2400, val loss: 1.6091125011444092
Epoch 2410, training loss: 310.6942138671875 = 0.014701424166560173 + 50.0 * 6.213590145111084
Epoch 2410, val loss: 1.6126469373703003
Epoch 2420, training loss: 310.5998840332031 = 0.014520064927637577 + 50.0 * 6.21170711517334
Epoch 2420, val loss: 1.6156021356582642
Epoch 2430, training loss: 310.58251953125 = 0.014354025013744831 + 50.0 * 6.211363792419434
Epoch 2430, val loss: 1.6190757751464844
Epoch 2440, training loss: 310.5654602050781 = 0.014185791835188866 + 50.0 * 6.211025238037109
Epoch 2440, val loss: 1.6222389936447144
Epoch 2450, training loss: 310.4942932128906 = 0.014024841599166393 + 50.0 * 6.2096052169799805
Epoch 2450, val loss: 1.6256755590438843
Epoch 2460, training loss: 310.60552978515625 = 0.01387129444628954 + 50.0 * 6.2118330001831055
Epoch 2460, val loss: 1.6288502216339111
Epoch 2470, training loss: 310.6028137207031 = 0.013707034289836884 + 50.0 * 6.211781978607178
Epoch 2470, val loss: 1.6317048072814941
Epoch 2480, training loss: 310.61505126953125 = 0.013547247275710106 + 50.0 * 6.212029933929443
Epoch 2480, val loss: 1.6347427368164062
Epoch 2490, training loss: 310.4898681640625 = 0.013395591638982296 + 50.0 * 6.209529399871826
Epoch 2490, val loss: 1.6379903554916382
Epoch 2500, training loss: 310.4270935058594 = 0.013245757669210434 + 50.0 * 6.208277225494385
Epoch 2500, val loss: 1.6412163972854614
Epoch 2510, training loss: 310.4414367675781 = 0.013106247410178185 + 50.0 * 6.208567142486572
Epoch 2510, val loss: 1.6443779468536377
Epoch 2520, training loss: 310.7444152832031 = 0.012973613105714321 + 50.0 * 6.21462869644165
Epoch 2520, val loss: 1.6477311849594116
Epoch 2530, training loss: 310.5243225097656 = 0.0128195034340024 + 50.0 * 6.210230350494385
Epoch 2530, val loss: 1.650153636932373
Epoch 2540, training loss: 310.4289855957031 = 0.012670711614191532 + 50.0 * 6.20832633972168
Epoch 2540, val loss: 1.6530232429504395
Epoch 2550, training loss: 310.42315673828125 = 0.012537560425698757 + 50.0 * 6.208212375640869
Epoch 2550, val loss: 1.6562480926513672
Epoch 2560, training loss: 310.7363586425781 = 0.01240671519190073 + 50.0 * 6.214478492736816
Epoch 2560, val loss: 1.6593470573425293
Epoch 2570, training loss: 310.4223937988281 = 0.012271067127585411 + 50.0 * 6.208202362060547
Epoch 2570, val loss: 1.6619433164596558
Epoch 2580, training loss: 310.38385009765625 = 0.012137701734900475 + 50.0 * 6.207434177398682
Epoch 2580, val loss: 1.664774775505066
Epoch 2590, training loss: 310.385498046875 = 0.012012437917292118 + 50.0 * 6.207469463348389
Epoch 2590, val loss: 1.6679939031600952
Epoch 2600, training loss: 310.6747131347656 = 0.011889508925378323 + 50.0 * 6.213256359100342
Epoch 2600, val loss: 1.670685052871704
Epoch 2610, training loss: 310.4514465332031 = 0.01176346093416214 + 50.0 * 6.208793640136719
Epoch 2610, val loss: 1.6735950708389282
Epoch 2620, training loss: 310.6100158691406 = 0.011638607829809189 + 50.0 * 6.211967468261719
Epoch 2620, val loss: 1.6761738061904907
Epoch 2630, training loss: 310.552978515625 = 0.011517655104398727 + 50.0 * 6.210829257965088
Epoch 2630, val loss: 1.6788992881774902
Epoch 2640, training loss: 310.3652038574219 = 0.011398044414818287 + 50.0 * 6.207076549530029
Epoch 2640, val loss: 1.6819353103637695
Epoch 2650, training loss: 310.32989501953125 = 0.011284464970231056 + 50.0 * 6.206371784210205
Epoch 2650, val loss: 1.6849943399429321
Epoch 2660, training loss: 310.3387756347656 = 0.011175720952451229 + 50.0 * 6.206552028656006
Epoch 2660, val loss: 1.6878230571746826
Epoch 2670, training loss: 310.6669921875 = 0.011076249182224274 + 50.0 * 6.213118553161621
Epoch 2670, val loss: 1.6907047033309937
Epoch 2680, training loss: 310.3048400878906 = 0.010951259173452854 + 50.0 * 6.205877780914307
Epoch 2680, val loss: 1.6928532123565674
Epoch 2690, training loss: 310.3236389160156 = 0.010841377079486847 + 50.0 * 6.20625638961792
Epoch 2690, val loss: 1.6956403255462646
Epoch 2700, training loss: 310.4669494628906 = 0.010743197053670883 + 50.0 * 6.2091240882873535
Epoch 2700, val loss: 1.6987348794937134
Epoch 2710, training loss: 310.3843688964844 = 0.010630524717271328 + 50.0 * 6.207475185394287
Epoch 2710, val loss: 1.7010345458984375
Epoch 2720, training loss: 310.39117431640625 = 0.010524149052798748 + 50.0 * 6.207612991333008
Epoch 2720, val loss: 1.7033251523971558
Epoch 2730, training loss: 310.3828430175781 = 0.01042433362454176 + 50.0 * 6.2074480056762695
Epoch 2730, val loss: 1.7061930894851685
Epoch 2740, training loss: 310.34515380859375 = 0.010324584320187569 + 50.0 * 6.2066969871521
Epoch 2740, val loss: 1.7083954811096191
Epoch 2750, training loss: 310.4627685546875 = 0.010232157073915005 + 50.0 * 6.20905065536499
Epoch 2750, val loss: 1.7113252878189087
Epoch 2760, training loss: 310.33966064453125 = 0.010130372829735279 + 50.0 * 6.20659065246582
Epoch 2760, val loss: 1.7141776084899902
Epoch 2770, training loss: 310.3652648925781 = 0.010032069869339466 + 50.0 * 6.207104682922363
Epoch 2770, val loss: 1.716248869895935
Epoch 2780, training loss: 310.28192138671875 = 0.009939118288457394 + 50.0 * 6.205439567565918
Epoch 2780, val loss: 1.719039797782898
Epoch 2790, training loss: 310.2545471191406 = 0.009846893139183521 + 50.0 * 6.204893589019775
Epoch 2790, val loss: 1.7216377258300781
Epoch 2800, training loss: 310.3536682128906 = 0.009758545085787773 + 50.0 * 6.206878185272217
Epoch 2800, val loss: 1.7240177392959595
Epoch 2810, training loss: 310.4980773925781 = 0.009666409343481064 + 50.0 * 6.209768295288086
Epoch 2810, val loss: 1.7262405157089233
Epoch 2820, training loss: 310.2835388183594 = 0.009576473385095596 + 50.0 * 6.205479145050049
Epoch 2820, val loss: 1.7291157245635986
Epoch 2830, training loss: 310.2390441894531 = 0.009491282515227795 + 50.0 * 6.204591274261475
Epoch 2830, val loss: 1.7315151691436768
Epoch 2840, training loss: 310.2365417480469 = 0.009407543577253819 + 50.0 * 6.204542636871338
Epoch 2840, val loss: 1.7340173721313477
Epoch 2850, training loss: 310.3953857421875 = 0.00932921189814806 + 50.0 * 6.207720756530762
Epoch 2850, val loss: 1.7366456985473633
Epoch 2860, training loss: 310.28204345703125 = 0.009238711558282375 + 50.0 * 6.205455780029297
Epoch 2860, val loss: 1.738708257675171
Epoch 2870, training loss: 310.2430725097656 = 0.009155315347015858 + 50.0 * 6.204678535461426
Epoch 2870, val loss: 1.7409753799438477
Epoch 2880, training loss: 310.2080993652344 = 0.009076017886400223 + 50.0 * 6.203980445861816
Epoch 2880, val loss: 1.7435189485549927
Epoch 2890, training loss: 310.2098693847656 = 0.008998502977192402 + 50.0 * 6.204017639160156
Epoch 2890, val loss: 1.7459864616394043
Epoch 2900, training loss: 310.56378173828125 = 0.008922699838876724 + 50.0 * 6.211097717285156
Epoch 2900, val loss: 1.7483246326446533
Epoch 2910, training loss: 310.2891540527344 = 0.00884199608117342 + 50.0 * 6.205605983734131
Epoch 2910, val loss: 1.7504664659500122
Epoch 2920, training loss: 310.1995544433594 = 0.008763676509261131 + 50.0 * 6.203815937042236
Epoch 2920, val loss: 1.7528889179229736
Epoch 2930, training loss: 310.1530456542969 = 0.0086884880438447 + 50.0 * 6.202887058258057
Epoch 2930, val loss: 1.7551406621932983
Epoch 2940, training loss: 310.1265563964844 = 0.008618388324975967 + 50.0 * 6.202358722686768
Epoch 2940, val loss: 1.7576183080673218
Epoch 2950, training loss: 310.2589111328125 = 0.008553821593523026 + 50.0 * 6.205007553100586
Epoch 2950, val loss: 1.7600255012512207
Epoch 2960, training loss: 310.2987365722656 = 0.008480844087898731 + 50.0 * 6.205804824829102
Epoch 2960, val loss: 1.762043833732605
Epoch 2970, training loss: 310.1499938964844 = 0.00840072613209486 + 50.0 * 6.202832221984863
Epoch 2970, val loss: 1.7640661001205444
Epoch 2980, training loss: 310.13262939453125 = 0.008326472714543343 + 50.0 * 6.202486038208008
Epoch 2980, val loss: 1.7659944295883179
Epoch 2990, training loss: 310.122802734375 = 0.008262134157121181 + 50.0 * 6.2022905349731445
Epoch 2990, val loss: 1.768355369567871
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 431.76776123046875 = 1.9245752096176147 + 50.0 * 8.596863746643066
Epoch 0, val loss: 1.9236748218536377
Epoch 10, training loss: 431.72772216796875 = 1.9160993099212646 + 50.0 * 8.596232414245605
Epoch 10, val loss: 1.9152501821517944
Epoch 20, training loss: 431.4837646484375 = 1.9056980609893799 + 50.0 * 8.591561317443848
Epoch 20, val loss: 1.9044842720031738
Epoch 30, training loss: 429.7915954589844 = 1.8926583528518677 + 50.0 * 8.557978630065918
Epoch 30, val loss: 1.8907475471496582
Epoch 40, training loss: 419.5072326660156 = 1.8767169713974 + 50.0 * 8.35261058807373
Epoch 40, val loss: 1.8743380308151245
Epoch 50, training loss: 386.4677734375 = 1.8601768016815186 + 50.0 * 7.69215202331543
Epoch 50, val loss: 1.857016682624817
Epoch 60, training loss: 376.2873840332031 = 1.8474868535995483 + 50.0 * 7.488798141479492
Epoch 60, val loss: 1.8458197116851807
Epoch 70, training loss: 366.09429931640625 = 1.83726167678833 + 50.0 * 7.2851409912109375
Epoch 70, val loss: 1.8368457555770874
Epoch 80, training loss: 357.3977355957031 = 1.826803207397461 + 50.0 * 7.111418724060059
Epoch 80, val loss: 1.826956033706665
Epoch 90, training loss: 352.3221740722656 = 1.81658935546875 + 50.0 * 7.0101118087768555
Epoch 90, val loss: 1.8172763586044312
Epoch 100, training loss: 347.10369873046875 = 1.8070071935653687 + 50.0 * 6.905933856964111
Epoch 100, val loss: 1.8084062337875366
Epoch 110, training loss: 343.5059509277344 = 1.797842025756836 + 50.0 * 6.83416223526001
Epoch 110, val loss: 1.8000843524932861
Epoch 120, training loss: 339.7477111816406 = 1.7883538007736206 + 50.0 * 6.7591872215271
Epoch 120, val loss: 1.7912436723709106
Epoch 130, training loss: 336.6909484863281 = 1.779031753540039 + 50.0 * 6.698238372802734
Epoch 130, val loss: 1.7825424671173096
Epoch 140, training loss: 333.9727783203125 = 1.7690131664276123 + 50.0 * 6.644075393676758
Epoch 140, val loss: 1.7731857299804688
Epoch 150, training loss: 331.8351745605469 = 1.7578142881393433 + 50.0 * 6.6015472412109375
Epoch 150, val loss: 1.762993335723877
Epoch 160, training loss: 329.9393615722656 = 1.7455545663833618 + 50.0 * 6.563876152038574
Epoch 160, val loss: 1.7515861988067627
Epoch 170, training loss: 328.6533508300781 = 1.7315988540649414 + 50.0 * 6.538434982299805
Epoch 170, val loss: 1.7388218641281128
Epoch 180, training loss: 327.6313171386719 = 1.7158591747283936 + 50.0 * 6.518309116363525
Epoch 180, val loss: 1.7247496843338013
Epoch 190, training loss: 326.8796691894531 = 1.6987907886505127 + 50.0 * 6.503617763519287
Epoch 190, val loss: 1.7094345092773438
Epoch 200, training loss: 326.10736083984375 = 1.6801718473434448 + 50.0 * 6.488543510437012
Epoch 200, val loss: 1.6930134296417236
Epoch 210, training loss: 325.4378967285156 = 1.6601430177688599 + 50.0 * 6.475554943084717
Epoch 210, val loss: 1.6755186319351196
Epoch 220, training loss: 324.893310546875 = 1.638596534729004 + 50.0 * 6.465094089508057
Epoch 220, val loss: 1.6567610502243042
Epoch 230, training loss: 324.6015625 = 1.6156597137451172 + 50.0 * 6.459717750549316
Epoch 230, val loss: 1.6366442441940308
Epoch 240, training loss: 323.9501953125 = 1.591216802597046 + 50.0 * 6.447179794311523
Epoch 240, val loss: 1.615496039390564
Epoch 250, training loss: 323.5423278808594 = 1.5656529664993286 + 50.0 * 6.439533710479736
Epoch 250, val loss: 1.5933804512023926
Epoch 260, training loss: 323.118896484375 = 1.5391013622283936 + 50.0 * 6.431595802307129
Epoch 260, val loss: 1.5705164670944214
Epoch 270, training loss: 323.0459899902344 = 1.5115760564804077 + 50.0 * 6.430688381195068
Epoch 270, val loss: 1.546974778175354
Epoch 280, training loss: 322.47705078125 = 1.4831759929656982 + 50.0 * 6.419877052307129
Epoch 280, val loss: 1.5230803489685059
Epoch 290, training loss: 322.0610656738281 = 1.4545788764953613 + 50.0 * 6.4121294021606445
Epoch 290, val loss: 1.4991559982299805
Epoch 300, training loss: 321.62078857421875 = 1.4256905317306519 + 50.0 * 6.403902053833008
Epoch 300, val loss: 1.4753625392913818
Epoch 310, training loss: 321.2248229980469 = 1.3966262340545654 + 50.0 * 6.396563529968262
Epoch 310, val loss: 1.4516563415527344
Epoch 320, training loss: 320.8534851074219 = 1.3675096035003662 + 50.0 * 6.389719009399414
Epoch 320, val loss: 1.428194284439087
Epoch 330, training loss: 321.06597900390625 = 1.3383541107177734 + 50.0 * 6.394552707672119
Epoch 330, val loss: 1.404669165611267
Epoch 340, training loss: 320.2715759277344 = 1.3086509704589844 + 50.0 * 6.379258155822754
Epoch 340, val loss: 1.381386637687683
Epoch 350, training loss: 319.9024963378906 = 1.2794337272644043 + 50.0 * 6.372461795806885
Epoch 350, val loss: 1.3585052490234375
Epoch 360, training loss: 319.5838928222656 = 1.2500815391540527 + 50.0 * 6.366675853729248
Epoch 360, val loss: 1.3359129428863525
Epoch 370, training loss: 319.3155212402344 = 1.220772624015808 + 50.0 * 6.3618950843811035
Epoch 370, val loss: 1.3134353160858154
Epoch 380, training loss: 319.4374084472656 = 1.1915284395217896 + 50.0 * 6.364917755126953
Epoch 380, val loss: 1.2912567853927612
Epoch 390, training loss: 318.8763427734375 = 1.1620572805404663 + 50.0 * 6.354285717010498
Epoch 390, val loss: 1.269217848777771
Epoch 400, training loss: 318.6097106933594 = 1.1330243349075317 + 50.0 * 6.349533557891846
Epoch 400, val loss: 1.2478052377700806
Epoch 410, training loss: 318.37139892578125 = 1.1041970252990723 + 50.0 * 6.345344543457031
Epoch 410, val loss: 1.2268553972244263
Epoch 420, training loss: 318.1926574707031 = 1.0757237672805786 + 50.0 * 6.342339038848877
Epoch 420, val loss: 1.2063547372817993
Epoch 430, training loss: 318.0113830566406 = 1.0474928617477417 + 50.0 * 6.339278221130371
Epoch 430, val loss: 1.1863632202148438
Epoch 440, training loss: 317.889892578125 = 1.0197861194610596 + 50.0 * 6.33740234375
Epoch 440, val loss: 1.1668455600738525
Epoch 450, training loss: 317.61468505859375 = 0.9926304221153259 + 50.0 * 6.3324408531188965
Epoch 450, val loss: 1.148388147354126
Epoch 460, training loss: 317.42822265625 = 0.9662275910377502 + 50.0 * 6.329239845275879
Epoch 460, val loss: 1.1306633949279785
Epoch 470, training loss: 317.5785217285156 = 0.9404191970825195 + 50.0 * 6.332761764526367
Epoch 470, val loss: 1.1136784553527832
Epoch 480, training loss: 317.225830078125 = 0.9153308868408203 + 50.0 * 6.326210021972656
Epoch 480, val loss: 1.097804307937622
Epoch 490, training loss: 316.9742431640625 = 0.8910768032073975 + 50.0 * 6.3216633796691895
Epoch 490, val loss: 1.0829944610595703
Epoch 500, training loss: 317.0222473144531 = 0.8676901459693909 + 50.0 * 6.323091506958008
Epoch 500, val loss: 1.0692873001098633
Epoch 510, training loss: 317.0356140136719 = 0.8449119925498962 + 50.0 * 6.3238139152526855
Epoch 510, val loss: 1.0560132265090942
Epoch 520, training loss: 316.599365234375 = 0.8230108618736267 + 50.0 * 6.315527439117432
Epoch 520, val loss: 1.044180989265442
Epoch 530, training loss: 316.422607421875 = 0.8020943999290466 + 50.0 * 6.312410354614258
Epoch 530, val loss: 1.0334843397140503
Epoch 540, training loss: 316.29437255859375 = 0.7819215059280396 + 50.0 * 6.310248851776123
Epoch 540, val loss: 1.0238118171691895
Epoch 550, training loss: 316.4779357910156 = 0.7623549103736877 + 50.0 * 6.314311504364014
Epoch 550, val loss: 1.0147908926010132
Epoch 560, training loss: 316.21612548828125 = 0.7437257170677185 + 50.0 * 6.3094482421875
Epoch 560, val loss: 1.00666081905365
Epoch 570, training loss: 315.9309387207031 = 0.725616991519928 + 50.0 * 6.30410623550415
Epoch 570, val loss: 0.9997133612632751
Epoch 580, training loss: 315.79296875 = 0.7083010077476501 + 50.0 * 6.301692962646484
Epoch 580, val loss: 0.9933810830116272
Epoch 590, training loss: 315.67840576171875 = 0.691612184047699 + 50.0 * 6.2997355461120605
Epoch 590, val loss: 0.9880529046058655
Epoch 600, training loss: 316.5992736816406 = 0.6753730773925781 + 50.0 * 6.318477630615234
Epoch 600, val loss: 0.9836834669113159
Epoch 610, training loss: 315.48486328125 = 0.6596645712852478 + 50.0 * 6.296504020690918
Epoch 610, val loss: 0.9792248606681824
Epoch 620, training loss: 315.4425354003906 = 0.6445867419242859 + 50.0 * 6.295958995819092
Epoch 620, val loss: 0.9758010506629944
Epoch 630, training loss: 315.3236083984375 = 0.6299024820327759 + 50.0 * 6.2938737869262695
Epoch 630, val loss: 0.9730449318885803
Epoch 640, training loss: 315.2199401855469 = 0.6155996918678284 + 50.0 * 6.292087078094482
Epoch 640, val loss: 0.9709758758544922
Epoch 650, training loss: 315.27392578125 = 0.6016497015953064 + 50.0 * 6.293445587158203
Epoch 650, val loss: 0.9695187211036682
Epoch 660, training loss: 315.1470947265625 = 0.5880253911018372 + 50.0 * 6.291181564331055
Epoch 660, val loss: 0.9680376052856445
Epoch 670, training loss: 314.9658203125 = 0.5746631026268005 + 50.0 * 6.287822723388672
Epoch 670, val loss: 0.9672006964683533
Epoch 680, training loss: 315.16302490234375 = 0.561521053314209 + 50.0 * 6.292030334472656
Epoch 680, val loss: 0.9665638208389282
Epoch 690, training loss: 314.8210754394531 = 0.5486200451850891 + 50.0 * 6.285449504852295
Epoch 690, val loss: 0.9663966298103333
Epoch 700, training loss: 315.0104675292969 = 0.5359047055244446 + 50.0 * 6.289490699768066
Epoch 700, val loss: 0.9663621783256531
Epoch 710, training loss: 314.7342834472656 = 0.5232943892478943 + 50.0 * 6.284219741821289
Epoch 710, val loss: 0.9663482904434204
Epoch 720, training loss: 314.6134338378906 = 0.5107700824737549 + 50.0 * 6.282053470611572
Epoch 720, val loss: 0.9662041664123535
Epoch 730, training loss: 314.6017150878906 = 0.4983929395675659 + 50.0 * 6.282066345214844
Epoch 730, val loss: 0.9664427638053894
Epoch 740, training loss: 314.50482177734375 = 0.48601770401000977 + 50.0 * 6.280375957489014
Epoch 740, val loss: 0.9662891626358032
Epoch 750, training loss: 314.39324951171875 = 0.47379934787750244 + 50.0 * 6.2783894538879395
Epoch 750, val loss: 0.966259777545929
Epoch 760, training loss: 314.3072204589844 = 0.46156227588653564 + 50.0 * 6.276913166046143
Epoch 760, val loss: 0.9661127328872681
Epoch 770, training loss: 314.2389221191406 = 0.4494135081768036 + 50.0 * 6.275790691375732
Epoch 770, val loss: 0.9660367369651794
Epoch 780, training loss: 314.3052062988281 = 0.4372844398021698 + 50.0 * 6.277358531951904
Epoch 780, val loss: 0.9660965800285339
Epoch 790, training loss: 314.1247863769531 = 0.4251733422279358 + 50.0 * 6.273992538452148
Epoch 790, val loss: 0.9655973315238953
Epoch 800, training loss: 314.13604736328125 = 0.4130963981151581 + 50.0 * 6.274459362030029
Epoch 800, val loss: 0.9654989838600159
Epoch 810, training loss: 314.1683349609375 = 0.4011848568916321 + 50.0 * 6.27534294128418
Epoch 810, val loss: 0.965201735496521
Epoch 820, training loss: 314.0993957519531 = 0.38923218846321106 + 50.0 * 6.274203300476074
Epoch 820, val loss: 0.9649959802627563
Epoch 830, training loss: 313.92535400390625 = 0.3773892819881439 + 50.0 * 6.270959377288818
Epoch 830, val loss: 0.9649106860160828
Epoch 840, training loss: 313.8779296875 = 0.36580994725227356 + 50.0 * 6.270242214202881
Epoch 840, val loss: 0.9646070599555969
Epoch 850, training loss: 314.08154296875 = 0.3542870283126831 + 50.0 * 6.274545192718506
Epoch 850, val loss: 0.9643948078155518
Epoch 860, training loss: 313.90313720703125 = 0.3430463373661041 + 50.0 * 6.2712016105651855
Epoch 860, val loss: 0.9648779034614563
Epoch 870, training loss: 313.6917724609375 = 0.33190083503723145 + 50.0 * 6.267197132110596
Epoch 870, val loss: 0.9646238088607788
Epoch 880, training loss: 313.6748352050781 = 0.321087509393692 + 50.0 * 6.267075061798096
Epoch 880, val loss: 0.9650129675865173
Epoch 890, training loss: 313.9390563964844 = 0.3104420304298401 + 50.0 * 6.272572040557861
Epoch 890, val loss: 0.9654882550239563
Epoch 900, training loss: 313.598876953125 = 0.30017679929733276 + 50.0 * 6.265974044799805
Epoch 900, val loss: 0.9655025005340576
Epoch 910, training loss: 313.4726867675781 = 0.2901207208633423 + 50.0 * 6.263651371002197
Epoch 910, val loss: 0.9665996432304382
Epoch 920, training loss: 313.40838623046875 = 0.28042152523994446 + 50.0 * 6.262558937072754
Epoch 920, val loss: 0.9675304293632507
Epoch 930, training loss: 313.3984680175781 = 0.27104848623275757 + 50.0 * 6.262547969818115
Epoch 930, val loss: 0.968691349029541
Epoch 940, training loss: 313.5423278808594 = 0.26195427775382996 + 50.0 * 6.2656073570251465
Epoch 940, val loss: 0.9701982140541077
Epoch 950, training loss: 313.3164367675781 = 0.25322243571281433 + 50.0 * 6.261264324188232
Epoch 950, val loss: 0.9716628193855286
Epoch 960, training loss: 313.53558349609375 = 0.24474474787712097 + 50.0 * 6.265816688537598
Epoch 960, val loss: 0.9736407399177551
Epoch 970, training loss: 313.3078918457031 = 0.23659533262252808 + 50.0 * 6.261425971984863
Epoch 970, val loss: 0.9751477241516113
Epoch 980, training loss: 313.1552734375 = 0.22881703078746796 + 50.0 * 6.258529186248779
Epoch 980, val loss: 0.9777492880821228
Epoch 990, training loss: 313.1013488769531 = 0.22135116159915924 + 50.0 * 6.2576003074646
Epoch 990, val loss: 0.9801574945449829
Epoch 1000, training loss: 313.0625305175781 = 0.2141963541507721 + 50.0 * 6.256966590881348
Epoch 1000, val loss: 0.9828683137893677
Epoch 1010, training loss: 313.1488952636719 = 0.20735634863376617 + 50.0 * 6.258830547332764
Epoch 1010, val loss: 0.9856593608856201
Epoch 1020, training loss: 313.14691162109375 = 0.2007303237915039 + 50.0 * 6.258923053741455
Epoch 1020, val loss: 0.9888694286346436
Epoch 1030, training loss: 313.07904052734375 = 0.19434717297554016 + 50.0 * 6.257694244384766
Epoch 1030, val loss: 0.9922123551368713
Epoch 1040, training loss: 312.9917297363281 = 0.18822616338729858 + 50.0 * 6.256069660186768
Epoch 1040, val loss: 0.995781660079956
Epoch 1050, training loss: 312.927001953125 = 0.1824425309896469 + 50.0 * 6.254891395568848
Epoch 1050, val loss: 0.9991542100906372
Epoch 1060, training loss: 312.8258972167969 = 0.17687036097049713 + 50.0 * 6.2529802322387695
Epoch 1060, val loss: 1.0031312704086304
Epoch 1070, training loss: 312.8991394042969 = 0.1715220957994461 + 50.0 * 6.254552364349365
Epoch 1070, val loss: 1.0071001052856445
Epoch 1080, training loss: 312.9400329589844 = 0.16641974449157715 + 50.0 * 6.255472660064697
Epoch 1080, val loss: 1.011087417602539
Epoch 1090, training loss: 312.75775146484375 = 0.16145367920398712 + 50.0 * 6.251925468444824
Epoch 1090, val loss: 1.0151766538619995
Epoch 1100, training loss: 312.7150573730469 = 0.1566981077194214 + 50.0 * 6.251166820526123
Epoch 1100, val loss: 1.019641637802124
Epoch 1110, training loss: 312.667724609375 = 0.15217015147209167 + 50.0 * 6.250311374664307
Epoch 1110, val loss: 1.0241448879241943
Epoch 1120, training loss: 312.9972229003906 = 0.14780239760875702 + 50.0 * 6.256988525390625
Epoch 1120, val loss: 1.0289943218231201
Epoch 1130, training loss: 312.8360900878906 = 0.14359764754772186 + 50.0 * 6.253849983215332
Epoch 1130, val loss: 1.0329715013504028
Epoch 1140, training loss: 312.6292419433594 = 0.1395459622144699 + 50.0 * 6.249793529510498
Epoch 1140, val loss: 1.03779935836792
Epoch 1150, training loss: 312.53985595703125 = 0.13566161692142487 + 50.0 * 6.24808406829834
Epoch 1150, val loss: 1.0424885749816895
Epoch 1160, training loss: 312.5222473144531 = 0.13192787766456604 + 50.0 * 6.247806549072266
Epoch 1160, val loss: 1.0473742485046387
Epoch 1170, training loss: 312.93743896484375 = 0.12834401428699493 + 50.0 * 6.256181716918945
Epoch 1170, val loss: 1.0519599914550781
Epoch 1180, training loss: 312.67425537109375 = 0.12483016401529312 + 50.0 * 6.250988483428955
Epoch 1180, val loss: 1.0574406385421753
Epoch 1190, training loss: 312.56390380859375 = 0.12144329398870468 + 50.0 * 6.248849391937256
Epoch 1190, val loss: 1.0620152950286865
Epoch 1200, training loss: 312.3941650390625 = 0.11816683411598206 + 50.0 * 6.245520114898682
Epoch 1200, val loss: 1.0671768188476562
Epoch 1210, training loss: 312.38232421875 = 0.1150452271103859 + 50.0 * 6.245345592498779
Epoch 1210, val loss: 1.0720913410186768
Epoch 1220, training loss: 312.4883728027344 = 0.11201948672533035 + 50.0 * 6.2475266456604
Epoch 1220, val loss: 1.0772559642791748
Epoch 1230, training loss: 312.7019348144531 = 0.10914549231529236 + 50.0 * 6.251856327056885
Epoch 1230, val loss: 1.082377314567566
Epoch 1240, training loss: 312.5222473144531 = 0.1062074825167656 + 50.0 * 6.248321056365967
Epoch 1240, val loss: 1.0875459909439087
Epoch 1250, training loss: 312.290283203125 = 0.10346076637506485 + 50.0 * 6.243736743927002
Epoch 1250, val loss: 1.0925501585006714
Epoch 1260, training loss: 312.1914978027344 = 0.10082214325666428 + 50.0 * 6.2418131828308105
Epoch 1260, val loss: 1.0979201793670654
Epoch 1270, training loss: 312.16705322265625 = 0.09827183187007904 + 50.0 * 6.24137544631958
Epoch 1270, val loss: 1.1032835245132446
Epoch 1280, training loss: 312.34991455078125 = 0.09580519050359726 + 50.0 * 6.245082378387451
Epoch 1280, val loss: 1.1087363958358765
Epoch 1290, training loss: 312.3168029785156 = 0.09334512054920197 + 50.0 * 6.244469165802002
Epoch 1290, val loss: 1.1134635210037231
Epoch 1300, training loss: 312.2219543457031 = 0.09104403853416443 + 50.0 * 6.242618560791016
Epoch 1300, val loss: 1.1193104982376099
Epoch 1310, training loss: 312.09405517578125 = 0.08875300735235214 + 50.0 * 6.240106105804443
Epoch 1310, val loss: 1.1242798566818237
Epoch 1320, training loss: 312.0319519042969 = 0.08657975494861603 + 50.0 * 6.238907337188721
Epoch 1320, val loss: 1.1296803951263428
Epoch 1330, training loss: 312.14581298828125 = 0.08448326587677002 + 50.0 * 6.241226673126221
Epoch 1330, val loss: 1.1349594593048096
Epoch 1340, training loss: 312.26031494140625 = 0.08242671191692352 + 50.0 * 6.243557929992676
Epoch 1340, val loss: 1.14058518409729
Epoch 1350, training loss: 312.1177062988281 = 0.0803886130452156 + 50.0 * 6.24074649810791
Epoch 1350, val loss: 1.145506501197815
Epoch 1360, training loss: 311.9649353027344 = 0.07843813300132751 + 50.0 * 6.237730026245117
Epoch 1360, val loss: 1.151024341583252
Epoch 1370, training loss: 311.9373779296875 = 0.07658117264509201 + 50.0 * 6.237216472625732
Epoch 1370, val loss: 1.1563845872879028
Epoch 1380, training loss: 311.9413757324219 = 0.07476737350225449 + 50.0 * 6.237331867218018
Epoch 1380, val loss: 1.161872386932373
Epoch 1390, training loss: 312.5183410644531 = 0.07299181073904037 + 50.0 * 6.24890661239624
Epoch 1390, val loss: 1.1671655178070068
Epoch 1400, training loss: 312.0828552246094 = 0.07132091373205185 + 50.0 * 6.240230560302734
Epoch 1400, val loss: 1.172194004058838
Epoch 1410, training loss: 311.90771484375 = 0.06960421800613403 + 50.0 * 6.236762046813965
Epoch 1410, val loss: 1.1777088642120361
Epoch 1420, training loss: 311.85418701171875 = 0.06801974773406982 + 50.0 * 6.235723495483398
Epoch 1420, val loss: 1.1829073429107666
Epoch 1430, training loss: 312.196044921875 = 0.0664563924074173 + 50.0 * 6.242591857910156
Epoch 1430, val loss: 1.1883796453475952
Epoch 1440, training loss: 311.9117431640625 = 0.0649237111210823 + 50.0 * 6.236936569213867
Epoch 1440, val loss: 1.1935147047042847
Epoch 1450, training loss: 311.8318176269531 = 0.06343325227499008 + 50.0 * 6.235367298126221
Epoch 1450, val loss: 1.1987453699111938
Epoch 1460, training loss: 311.9636535644531 = 0.06200525164604187 + 50.0 * 6.238032817840576
Epoch 1460, val loss: 1.2040581703186035
Epoch 1470, training loss: 311.76751708984375 = 0.060589760541915894 + 50.0 * 6.2341389656066895
Epoch 1470, val loss: 1.2091714143753052
Epoch 1480, training loss: 311.907958984375 = 0.0592346116900444 + 50.0 * 6.236974239349365
Epoch 1480, val loss: 1.214845895767212
Epoch 1490, training loss: 311.762451171875 = 0.05792006105184555 + 50.0 * 6.234090805053711
Epoch 1490, val loss: 1.2194881439208984
Epoch 1500, training loss: 311.7208251953125 = 0.05664513632655144 + 50.0 * 6.233283519744873
Epoch 1500, val loss: 1.2245756387710571
Epoch 1510, training loss: 311.7589111328125 = 0.05541974678635597 + 50.0 * 6.23406982421875
Epoch 1510, val loss: 1.2298812866210938
Epoch 1520, training loss: 312.0216369628906 = 0.054234057664871216 + 50.0 * 6.2393479347229
Epoch 1520, val loss: 1.2351243495941162
Epoch 1530, training loss: 311.7705078125 = 0.05299542844295502 + 50.0 * 6.234349727630615
Epoch 1530, val loss: 1.240172266960144
Epoch 1540, training loss: 311.6532897949219 = 0.05187563970685005 + 50.0 * 6.232028484344482
Epoch 1540, val loss: 1.2453103065490723
Epoch 1550, training loss: 311.5971984863281 = 0.05075419694185257 + 50.0 * 6.230928897857666
Epoch 1550, val loss: 1.2504208087921143
Epoch 1560, training loss: 311.8077087402344 = 0.049689095467329025 + 50.0 * 6.2351603507995605
Epoch 1560, val loss: 1.2559171915054321
Epoch 1570, training loss: 311.63421630859375 = 0.048622921109199524 + 50.0 * 6.2317118644714355
Epoch 1570, val loss: 1.260414719581604
Epoch 1580, training loss: 311.7555236816406 = 0.04759818688035011 + 50.0 * 6.234158515930176
Epoch 1580, val loss: 1.2658823728561401
Epoch 1590, training loss: 311.5712890625 = 0.046594973653554916 + 50.0 * 6.230494022369385
Epoch 1590, val loss: 1.2704426050186157
Epoch 1600, training loss: 311.5069580078125 = 0.04564538598060608 + 50.0 * 6.229226112365723
Epoch 1600, val loss: 1.2755697965621948
Epoch 1610, training loss: 311.6886291503906 = 0.04472624137997627 + 50.0 * 6.232877731323242
Epoch 1610, val loss: 1.280366063117981
Epoch 1620, training loss: 311.5338134765625 = 0.04380391538143158 + 50.0 * 6.229800224304199
Epoch 1620, val loss: 1.2854909896850586
Epoch 1630, training loss: 311.67431640625 = 0.0429115928709507 + 50.0 * 6.232627868652344
Epoch 1630, val loss: 1.2906692028045654
Epoch 1640, training loss: 311.50921630859375 = 0.042043838649988174 + 50.0 * 6.229343414306641
Epoch 1640, val loss: 1.2950749397277832
Epoch 1650, training loss: 311.41253662109375 = 0.041204873472452164 + 50.0 * 6.227426528930664
Epoch 1650, val loss: 1.3002454042434692
Epoch 1660, training loss: 311.7165832519531 = 0.040423423051834106 + 50.0 * 6.233522891998291
Epoch 1660, val loss: 1.3051787614822388
Epoch 1670, training loss: 311.5169982910156 = 0.03958806395530701 + 50.0 * 6.229548454284668
Epoch 1670, val loss: 1.309816837310791
Epoch 1680, training loss: 311.4552001953125 = 0.03880695626139641 + 50.0 * 6.228327751159668
Epoch 1680, val loss: 1.3147296905517578
Epoch 1690, training loss: 311.3880310058594 = 0.03805071488022804 + 50.0 * 6.226999282836914
Epoch 1690, val loss: 1.3194149732589722
Epoch 1700, training loss: 311.71685791015625 = 0.037345752120018005 + 50.0 * 6.233590126037598
Epoch 1700, val loss: 1.3240686655044556
Epoch 1710, training loss: 311.3798522949219 = 0.03659185394644737 + 50.0 * 6.226865291595459
Epoch 1710, val loss: 1.328784704208374
Epoch 1720, training loss: 311.36126708984375 = 0.03589547798037529 + 50.0 * 6.226507663726807
Epoch 1720, val loss: 1.3335415124893188
Epoch 1730, training loss: 311.31805419921875 = 0.03521568328142166 + 50.0 * 6.225656509399414
Epoch 1730, val loss: 1.3382048606872559
Epoch 1740, training loss: 311.293212890625 = 0.03456664830446243 + 50.0 * 6.225172519683838
Epoch 1740, val loss: 1.3430687189102173
Epoch 1750, training loss: 311.6564025878906 = 0.03393012285232544 + 50.0 * 6.232449054718018
Epoch 1750, val loss: 1.3479862213134766
Epoch 1760, training loss: 311.4964294433594 = 0.03328534588217735 + 50.0 * 6.229262828826904
Epoch 1760, val loss: 1.3519833087921143
Epoch 1770, training loss: 311.3694763183594 = 0.03267316892743111 + 50.0 * 6.226735591888428
Epoch 1770, val loss: 1.356816053390503
Epoch 1780, training loss: 311.22393798828125 = 0.03208241984248161 + 50.0 * 6.223837375640869
Epoch 1780, val loss: 1.3612909317016602
Epoch 1790, training loss: 311.2510986328125 = 0.03151494264602661 + 50.0 * 6.224391460418701
Epoch 1790, val loss: 1.3657530546188354
Epoch 1800, training loss: 311.55914306640625 = 0.03097018413245678 + 50.0 * 6.230563163757324
Epoch 1800, val loss: 1.370097041130066
Epoch 1810, training loss: 311.3974914550781 = 0.030386915430426598 + 50.0 * 6.227342128753662
Epoch 1810, val loss: 1.3741860389709473
Epoch 1820, training loss: 311.1983642578125 = 0.029850348830223083 + 50.0 * 6.22337007522583
Epoch 1820, val loss: 1.3790003061294556
Epoch 1830, training loss: 311.1895751953125 = 0.029321536421775818 + 50.0 * 6.223205089569092
Epoch 1830, val loss: 1.3833314180374146
Epoch 1840, training loss: 311.42413330078125 = 0.028817901387810707 + 50.0 * 6.227906703948975
Epoch 1840, val loss: 1.3876049518585205
Epoch 1850, training loss: 311.3095397949219 = 0.028326619416475296 + 50.0 * 6.2256245613098145
Epoch 1850, val loss: 1.3921942710876465
Epoch 1860, training loss: 311.1888427734375 = 0.027828261256217957 + 50.0 * 6.223220348358154
Epoch 1860, val loss: 1.3961652517318726
Epoch 1870, training loss: 311.1995544433594 = 0.027353301644325256 + 50.0 * 6.22344446182251
Epoch 1870, val loss: 1.4006186723709106
Epoch 1880, training loss: 311.1669006347656 = 0.02689650095999241 + 50.0 * 6.222800254821777
Epoch 1880, val loss: 1.4047244787216187
Epoch 1890, training loss: 311.4068603515625 = 0.02645888365805149 + 50.0 * 6.2276082038879395
Epoch 1890, val loss: 1.4090988636016846
Epoch 1900, training loss: 311.1293029785156 = 0.025991061702370644 + 50.0 * 6.2220659255981445
Epoch 1900, val loss: 1.4133837223052979
Epoch 1910, training loss: 311.0947570800781 = 0.025557050481438637 + 50.0 * 6.221384048461914
Epoch 1910, val loss: 1.4177716970443726
Epoch 1920, training loss: 311.0656433105469 = 0.025147434324026108 + 50.0 * 6.2208099365234375
Epoch 1920, val loss: 1.4217578172683716
Epoch 1930, training loss: 311.1480712890625 = 0.024747055023908615 + 50.0 * 6.222466468811035
Epoch 1930, val loss: 1.4257886409759521
Epoch 1940, training loss: 311.1775207519531 = 0.024343926459550858 + 50.0 * 6.2230634689331055
Epoch 1940, val loss: 1.4298455715179443
Epoch 1950, training loss: 311.29736328125 = 0.023944180458784103 + 50.0 * 6.225468158721924
Epoch 1950, val loss: 1.4342411756515503
Epoch 1960, training loss: 311.0768127441406 = 0.023558586835861206 + 50.0 * 6.221065044403076
Epoch 1960, val loss: 1.4377294778823853
Epoch 1970, training loss: 311.1127624511719 = 0.02318609319627285 + 50.0 * 6.2217912673950195
Epoch 1970, val loss: 1.4418607950210571
Epoch 1980, training loss: 311.1277770996094 = 0.022824468091130257 + 50.0 * 6.2220988273620605
Epoch 1980, val loss: 1.4458805322647095
Epoch 1990, training loss: 311.0380554199219 = 0.02246939204633236 + 50.0 * 6.220312118530273
Epoch 1990, val loss: 1.450086236000061
Epoch 2000, training loss: 311.1576843261719 = 0.022123068571090698 + 50.0 * 6.222711086273193
Epoch 2000, val loss: 1.4542231559753418
Epoch 2010, training loss: 311.027099609375 = 0.021764902397990227 + 50.0 * 6.220107078552246
Epoch 2010, val loss: 1.4577879905700684
Epoch 2020, training loss: 311.06170654296875 = 0.021435970440506935 + 50.0 * 6.220805644989014
Epoch 2020, val loss: 1.4618083238601685
Epoch 2030, training loss: 311.0162353515625 = 0.021112438291311264 + 50.0 * 6.219902515411377
Epoch 2030, val loss: 1.465761661529541
Epoch 2040, training loss: 310.89263916015625 = 0.020792659372091293 + 50.0 * 6.217437267303467
Epoch 2040, val loss: 1.4693970680236816
Epoch 2050, training loss: 310.9695129394531 = 0.020485589280724525 + 50.0 * 6.21898078918457
Epoch 2050, val loss: 1.473263144493103
Epoch 2060, training loss: 311.12860107421875 = 0.020182576030492783 + 50.0 * 6.222168445587158
Epoch 2060, val loss: 1.4773973226547241
Epoch 2070, training loss: 311.1944885253906 = 0.0198811162263155 + 50.0 * 6.22349214553833
Epoch 2070, val loss: 1.4804437160491943
Epoch 2080, training loss: 311.0281677246094 = 0.019579891115427017 + 50.0 * 6.2201714515686035
Epoch 2080, val loss: 1.484278917312622
Epoch 2090, training loss: 310.9217529296875 = 0.019302140921354294 + 50.0 * 6.2180495262146
Epoch 2090, val loss: 1.4882169961929321
Epoch 2100, training loss: 311.0733337402344 = 0.019022749736905098 + 50.0 * 6.221086502075195
Epoch 2100, val loss: 1.4920393228530884
Epoch 2110, training loss: 310.968994140625 = 0.018739458173513412 + 50.0 * 6.219005107879639
Epoch 2110, val loss: 1.4953947067260742
Epoch 2120, training loss: 310.8371276855469 = 0.018475869670510292 + 50.0 * 6.216373443603516
Epoch 2120, val loss: 1.4990462064743042
Epoch 2130, training loss: 310.84600830078125 = 0.018220866098999977 + 50.0 * 6.216556072235107
Epoch 2130, val loss: 1.5027457475662231
Epoch 2140, training loss: 310.8592529296875 = 0.01796742156147957 + 50.0 * 6.216825485229492
Epoch 2140, val loss: 1.5060389041900635
Epoch 2150, training loss: 311.06463623046875 = 0.01772027090191841 + 50.0 * 6.220938205718994
Epoch 2150, val loss: 1.5093902349472046
Epoch 2160, training loss: 310.8128662109375 = 0.017465148121118546 + 50.0 * 6.215908050537109
Epoch 2160, val loss: 1.5132502317428589
Epoch 2170, training loss: 310.81378173828125 = 0.01722530834376812 + 50.0 * 6.215931415557861
Epoch 2170, val loss: 1.5167776346206665
Epoch 2180, training loss: 311.0472412109375 = 0.016998128965497017 + 50.0 * 6.22060489654541
Epoch 2180, val loss: 1.5198266506195068
Epoch 2190, training loss: 310.9320373535156 = 0.016768107190728188 + 50.0 * 6.218305587768555
Epoch 2190, val loss: 1.5237990617752075
Epoch 2200, training loss: 310.7419128417969 = 0.016527215018868446 + 50.0 * 6.214507579803467
Epoch 2200, val loss: 1.5265945196151733
Epoch 2210, training loss: 310.7320251464844 = 0.01630634069442749 + 50.0 * 6.2143144607543945
Epoch 2210, val loss: 1.5304077863693237
Epoch 2220, training loss: 310.70135498046875 = 0.01609347201883793 + 50.0 * 6.213705539703369
Epoch 2220, val loss: 1.5336120128631592
Epoch 2230, training loss: 310.7601013183594 = 0.015884364023804665 + 50.0 * 6.2148847579956055
Epoch 2230, val loss: 1.5367813110351562
Epoch 2240, training loss: 311.143798828125 = 0.015679750591516495 + 50.0 * 6.222562313079834
Epoch 2240, val loss: 1.5406016111373901
Epoch 2250, training loss: 310.9495544433594 = 0.015477986074984074 + 50.0 * 6.218681335449219
Epoch 2250, val loss: 1.5432696342468262
Epoch 2260, training loss: 310.76611328125 = 0.015270487405359745 + 50.0 * 6.215017318725586
Epoch 2260, val loss: 1.5463439226150513
Epoch 2270, training loss: 310.88116455078125 = 0.015085119754076004 + 50.0 * 6.217321872711182
Epoch 2270, val loss: 1.5496641397476196
Epoch 2280, training loss: 310.7203674316406 = 0.014883286319673061 + 50.0 * 6.214109420776367
Epoch 2280, val loss: 1.5526553392410278
Epoch 2290, training loss: 310.7195739746094 = 0.014689709059894085 + 50.0 * 6.21409797668457
Epoch 2290, val loss: 1.555863618850708
Epoch 2300, training loss: 310.6331481933594 = 0.014508244581520557 + 50.0 * 6.21237325668335
Epoch 2300, val loss: 1.5590524673461914
Epoch 2310, training loss: 310.6974182128906 = 0.014331266283988953 + 50.0 * 6.2136616706848145
Epoch 2310, val loss: 1.562278151512146
Epoch 2320, training loss: 310.94732666015625 = 0.01415144745260477 + 50.0 * 6.218663215637207
Epoch 2320, val loss: 1.5649594068527222
Epoch 2330, training loss: 310.81915283203125 = 0.01398101169615984 + 50.0 * 6.216103553771973
Epoch 2330, val loss: 1.5678141117095947
Epoch 2340, training loss: 310.92193603515625 = 0.013807497918605804 + 50.0 * 6.218163013458252
Epoch 2340, val loss: 1.570935845375061
Epoch 2350, training loss: 310.6231994628906 = 0.013631191104650497 + 50.0 * 6.212191104888916
Epoch 2350, val loss: 1.5742237567901611
Epoch 2360, training loss: 310.6116027832031 = 0.0134670315310359 + 50.0 * 6.211963176727295
Epoch 2360, val loss: 1.5772554874420166
Epoch 2370, training loss: 310.57843017578125 = 0.013307171873748302 + 50.0 * 6.211302280426025
Epoch 2370, val loss: 1.580001711845398
Epoch 2380, training loss: 310.8779296875 = 0.013151459395885468 + 50.0 * 6.2172956466674805
Epoch 2380, val loss: 1.5827995538711548
Epoch 2390, training loss: 310.56256103515625 = 0.012994752265512943 + 50.0 * 6.210990905761719
Epoch 2390, val loss: 1.5857491493225098
Epoch 2400, training loss: 310.5489196777344 = 0.012839808128774166 + 50.0 * 6.210721492767334
Epoch 2400, val loss: 1.5888731479644775
Epoch 2410, training loss: 310.6540832519531 = 0.012693498283624649 + 50.0 * 6.212827682495117
Epoch 2410, val loss: 1.5910371541976929
Epoch 2420, training loss: 310.9795227050781 = 0.01255289651453495 + 50.0 * 6.219339847564697
Epoch 2420, val loss: 1.5942672491073608
Epoch 2430, training loss: 310.6341247558594 = 0.012393331155180931 + 50.0 * 6.212434768676758
Epoch 2430, val loss: 1.5969364643096924
Epoch 2440, training loss: 310.5482177734375 = 0.01225073728710413 + 50.0 * 6.210719585418701
Epoch 2440, val loss: 1.599974513053894
Epoch 2450, training loss: 310.4984130859375 = 0.012112593278288841 + 50.0 * 6.209725856781006
Epoch 2450, val loss: 1.6026283502578735
Epoch 2460, training loss: 310.6011962890625 = 0.0119782704859972 + 50.0 * 6.2117838859558105
Epoch 2460, val loss: 1.6052899360656738
Epoch 2470, training loss: 310.6944580078125 = 0.011842966079711914 + 50.0 * 6.21365213394165
Epoch 2470, val loss: 1.6081106662750244
Epoch 2480, training loss: 310.6200256347656 = 0.011711813509464264 + 50.0 * 6.2121663093566895
Epoch 2480, val loss: 1.611031413078308
Epoch 2490, training loss: 310.539306640625 = 0.011579692363739014 + 50.0 * 6.210554599761963
Epoch 2490, val loss: 1.613409161567688
Epoch 2500, training loss: 310.67535400390625 = 0.011453174985945225 + 50.0 * 6.213278293609619
Epoch 2500, val loss: 1.6160532236099243
Epoch 2510, training loss: 310.5061340332031 = 0.011329906061291695 + 50.0 * 6.209896087646484
Epoch 2510, val loss: 1.6187094449996948
Epoch 2520, training loss: 310.6782531738281 = 0.01121249794960022 + 50.0 * 6.213340759277344
Epoch 2520, val loss: 1.6211284399032593
Epoch 2530, training loss: 310.54193115234375 = 0.011085422709584236 + 50.0 * 6.2106170654296875
Epoch 2530, val loss: 1.6236591339111328
Epoch 2540, training loss: 310.46405029296875 = 0.010966355912387371 + 50.0 * 6.209062099456787
Epoch 2540, val loss: 1.626677393913269
Epoch 2550, training loss: 310.48480224609375 = 0.010850805789232254 + 50.0 * 6.209478855133057
Epoch 2550, val loss: 1.6292234659194946
Epoch 2560, training loss: 310.5245056152344 = 0.010737685486674309 + 50.0 * 6.210275173187256
Epoch 2560, val loss: 1.6314409971237183
Epoch 2570, training loss: 310.5693359375 = 0.010628937743604183 + 50.0 * 6.211174488067627
Epoch 2570, val loss: 1.6341098546981812
Epoch 2580, training loss: 310.6798400878906 = 0.010521112941205502 + 50.0 * 6.213386535644531
Epoch 2580, val loss: 1.6366496086120605
Epoch 2590, training loss: 310.4425964355469 = 0.010400346480309963 + 50.0 * 6.208643913269043
Epoch 2590, val loss: 1.6388556957244873
Epoch 2600, training loss: 310.3847351074219 = 0.010295229032635689 + 50.0 * 6.207489013671875
Epoch 2600, val loss: 1.641880750656128
Epoch 2610, training loss: 310.5416564941406 = 0.010195846669375896 + 50.0 * 6.210629463195801
Epoch 2610, val loss: 1.6443181037902832
Epoch 2620, training loss: 310.48773193359375 = 0.01008856762200594 + 50.0 * 6.209552764892578
Epoch 2620, val loss: 1.6462692022323608
Epoch 2630, training loss: 310.39068603515625 = 0.00998778361827135 + 50.0 * 6.207614421844482
Epoch 2630, val loss: 1.6485073566436768
Epoch 2640, training loss: 310.52716064453125 = 0.009894108399748802 + 50.0 * 6.210345268249512
Epoch 2640, val loss: 1.651227355003357
Epoch 2650, training loss: 310.50482177734375 = 0.009792041964828968 + 50.0 * 6.209900379180908
Epoch 2650, val loss: 1.6532280445098877
Epoch 2660, training loss: 310.4073486328125 = 0.009690449573099613 + 50.0 * 6.207952976226807
Epoch 2660, val loss: 1.655673861503601
Epoch 2670, training loss: 310.3541564941406 = 0.009599791839718819 + 50.0 * 6.206891059875488
Epoch 2670, val loss: 1.6581751108169556
Epoch 2680, training loss: 310.4547424316406 = 0.009505345486104488 + 50.0 * 6.20890474319458
Epoch 2680, val loss: 1.6602760553359985
Epoch 2690, training loss: 310.4720764160156 = 0.009412001818418503 + 50.0 * 6.209253787994385
Epoch 2690, val loss: 1.6626189947128296
Epoch 2700, training loss: 310.3688049316406 = 0.00932391732931137 + 50.0 * 6.207189083099365
Epoch 2700, val loss: 1.6646589040756226
Epoch 2710, training loss: 310.3106689453125 = 0.009232645854353905 + 50.0 * 6.206028461456299
Epoch 2710, val loss: 1.666871428489685
Epoch 2720, training loss: 310.3113098144531 = 0.009150111116468906 + 50.0 * 6.206043243408203
Epoch 2720, val loss: 1.669323444366455
Epoch 2730, training loss: 310.7484436035156 = 0.009066790342330933 + 50.0 * 6.214787483215332
Epoch 2730, val loss: 1.6716324090957642
Epoch 2740, training loss: 310.43927001953125 = 0.008980488404631615 + 50.0 * 6.208606243133545
Epoch 2740, val loss: 1.6729395389556885
Epoch 2750, training loss: 310.332763671875 = 0.008896167390048504 + 50.0 * 6.206477165222168
Epoch 2750, val loss: 1.6756494045257568
Epoch 2760, training loss: 310.35601806640625 = 0.008814788423478603 + 50.0 * 6.206943988800049
Epoch 2760, val loss: 1.677467703819275
Epoch 2770, training loss: 310.41192626953125 = 0.008734342642128468 + 50.0 * 6.208064079284668
Epoch 2770, val loss: 1.679736614227295
Epoch 2780, training loss: 310.29705810546875 = 0.008656099438667297 + 50.0 * 6.205767631530762
Epoch 2780, val loss: 1.681839942932129
Epoch 2790, training loss: 310.3304748535156 = 0.008582335896790028 + 50.0 * 6.206438064575195
Epoch 2790, val loss: 1.684308409690857
Epoch 2800, training loss: 310.29718017578125 = 0.00850517489016056 + 50.0 * 6.20577335357666
Epoch 2800, val loss: 1.686260461807251
Epoch 2810, training loss: 310.3870849609375 = 0.008426187559962273 + 50.0 * 6.207573413848877
Epoch 2810, val loss: 1.6877940893173218
Epoch 2820, training loss: 310.3887023925781 = 0.008352421224117279 + 50.0 * 6.207606792449951
Epoch 2820, val loss: 1.6904047727584839
Epoch 2830, training loss: 310.27008056640625 = 0.008282207883894444 + 50.0 * 6.205235958099365
Epoch 2830, val loss: 1.6921292543411255
Epoch 2840, training loss: 310.2836608886719 = 0.008208151906728745 + 50.0 * 6.205509185791016
Epoch 2840, val loss: 1.6939789056777954
Epoch 2850, training loss: 310.49981689453125 = 0.008141150698065758 + 50.0 * 6.209833145141602
Epoch 2850, val loss: 1.6963366270065308
Epoch 2860, training loss: 310.2481689453125 = 0.00806798879057169 + 50.0 * 6.204802513122559
Epoch 2860, val loss: 1.6974810361862183
Epoch 2870, training loss: 310.27935791015625 = 0.008002310991287231 + 50.0 * 6.205427169799805
Epoch 2870, val loss: 1.7000786066055298
Epoch 2880, training loss: 310.395751953125 = 0.00793427973985672 + 50.0 * 6.207756519317627
Epoch 2880, val loss: 1.7015914916992188
Epoch 2890, training loss: 310.2043762207031 = 0.007863801904022694 + 50.0 * 6.203929901123047
Epoch 2890, val loss: 1.703631043434143
Epoch 2900, training loss: 310.1684875488281 = 0.007796894758939743 + 50.0 * 6.203214168548584
Epoch 2900, val loss: 1.705498218536377
Epoch 2910, training loss: 310.1678466796875 = 0.007733436767011881 + 50.0 * 6.203202724456787
Epoch 2910, val loss: 1.7076245546340942
Epoch 2920, training loss: 310.33282470703125 = 0.0076706623658537865 + 50.0 * 6.206503391265869
Epoch 2920, val loss: 1.7093385457992554
Epoch 2930, training loss: 310.3011779785156 = 0.007606832310557365 + 50.0 * 6.20587158203125
Epoch 2930, val loss: 1.710785984992981
Epoch 2940, training loss: 310.43121337890625 = 0.007547792978584766 + 50.0 * 6.2084736824035645
Epoch 2940, val loss: 1.712798833847046
Epoch 2950, training loss: 310.1697082519531 = 0.007481998298317194 + 50.0 * 6.203244686126709
Epoch 2950, val loss: 1.7145938873291016
Epoch 2960, training loss: 310.1611633300781 = 0.0074249752797186375 + 50.0 * 6.203074932098389
Epoch 2960, val loss: 1.7165371179580688
Epoch 2970, training loss: 310.2109069824219 = 0.007367179729044437 + 50.0 * 6.204071044921875
Epoch 2970, val loss: 1.7182376384735107
Epoch 2980, training loss: 310.359619140625 = 0.007309488952159882 + 50.0 * 6.207046031951904
Epoch 2980, val loss: 1.720077633857727
Epoch 2990, training loss: 310.29669189453125 = 0.007246298249810934 + 50.0 * 6.205789089202881
Epoch 2990, val loss: 1.721386432647705
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.812335266209805
The final CL Acc:0.72469, 0.02937, The final GNN Acc:0.81339, 0.00188
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13242])
remove edge: torch.Size([2, 7866])
updated graph: torch.Size([2, 10552])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7804870605469 = 1.9369851350784302 + 50.0 * 8.596870422363281
Epoch 0, val loss: 1.9451560974121094
Epoch 10, training loss: 431.7407531738281 = 1.9280626773834229 + 50.0 * 8.596253395080566
Epoch 10, val loss: 1.935684084892273
Epoch 20, training loss: 431.5040283203125 = 1.9173762798309326 + 50.0 * 8.5917329788208
Epoch 20, val loss: 1.9242364168167114
Epoch 30, training loss: 429.7632141113281 = 1.9043042659759521 + 50.0 * 8.557178497314453
Epoch 30, val loss: 1.9101743698120117
Epoch 40, training loss: 416.4894714355469 = 1.8884903192520142 + 50.0 * 8.292019844055176
Epoch 40, val loss: 1.8934788703918457
Epoch 50, training loss: 375.7378234863281 = 1.8706324100494385 + 50.0 * 7.477344036102295
Epoch 50, val loss: 1.8752778768539429
Epoch 60, training loss: 366.500732421875 = 1.856495976448059 + 50.0 * 7.292884349822998
Epoch 60, val loss: 1.8617181777954102
Epoch 70, training loss: 357.5686340332031 = 1.8440018892288208 + 50.0 * 7.114492893218994
Epoch 70, val loss: 1.8488532304763794
Epoch 80, training loss: 352.0950927734375 = 1.8314018249511719 + 50.0 * 7.005273818969727
Epoch 80, val loss: 1.8354026079177856
Epoch 90, training loss: 347.9063720703125 = 1.8191126585006714 + 50.0 * 6.9217448234558105
Epoch 90, val loss: 1.8223508596420288
Epoch 100, training loss: 345.2077331542969 = 1.8071105480194092 + 50.0 * 6.868012428283691
Epoch 100, val loss: 1.809880256652832
Epoch 110, training loss: 342.2436218261719 = 1.795616626739502 + 50.0 * 6.8089599609375
Epoch 110, val loss: 1.7982031106948853
Epoch 120, training loss: 338.9046630859375 = 1.7853786945343018 + 50.0 * 6.7423858642578125
Epoch 120, val loss: 1.7879972457885742
Epoch 130, training loss: 336.2291259765625 = 1.7758264541625977 + 50.0 * 6.689066410064697
Epoch 130, val loss: 1.778280258178711
Epoch 140, training loss: 334.0675964355469 = 1.7655677795410156 + 50.0 * 6.646040439605713
Epoch 140, val loss: 1.7680842876434326
Epoch 150, training loss: 331.9931335449219 = 1.754522681236267 + 50.0 * 6.604772090911865
Epoch 150, val loss: 1.7573436498641968
Epoch 160, training loss: 330.1435852050781 = 1.7430583238601685 + 50.0 * 6.568010330200195
Epoch 160, val loss: 1.7464454174041748
Epoch 170, training loss: 328.8591613769531 = 1.7306902408599854 + 50.0 * 6.542569637298584
Epoch 170, val loss: 1.7348439693450928
Epoch 180, training loss: 327.70721435546875 = 1.716381549835205 + 50.0 * 6.519816875457764
Epoch 180, val loss: 1.7216826677322388
Epoch 190, training loss: 326.7171936035156 = 1.7005223035812378 + 50.0 * 6.500333309173584
Epoch 190, val loss: 1.707119107246399
Epoch 200, training loss: 326.0188293457031 = 1.6834728717803955 + 50.0 * 6.4867072105407715
Epoch 200, val loss: 1.691408395767212
Epoch 210, training loss: 325.1795959472656 = 1.6648544073104858 + 50.0 * 6.470294952392578
Epoch 210, val loss: 1.674578070640564
Epoch 220, training loss: 324.51885986328125 = 1.6449419260025024 + 50.0 * 6.4574785232543945
Epoch 220, val loss: 1.6568291187286377
Epoch 230, training loss: 323.9023132324219 = 1.6236562728881836 + 50.0 * 6.445573329925537
Epoch 230, val loss: 1.6380037069320679
Epoch 240, training loss: 323.4850158691406 = 1.6007150411605835 + 50.0 * 6.437686443328857
Epoch 240, val loss: 1.6180320978164673
Epoch 250, training loss: 322.8810729980469 = 1.5765970945358276 + 50.0 * 6.426089286804199
Epoch 250, val loss: 1.5970406532287598
Epoch 260, training loss: 322.500732421875 = 1.5512787103652954 + 50.0 * 6.418989181518555
Epoch 260, val loss: 1.575356125831604
Epoch 270, training loss: 322.09649658203125 = 1.5247565507888794 + 50.0 * 6.411434650421143
Epoch 270, val loss: 1.5529025793075562
Epoch 280, training loss: 321.6537780761719 = 1.4975411891937256 + 50.0 * 6.403125286102295
Epoch 280, val loss: 1.5300649404525757
Epoch 290, training loss: 321.3327941894531 = 1.4697976112365723 + 50.0 * 6.3972601890563965
Epoch 290, val loss: 1.5069657564163208
Epoch 300, training loss: 320.9619445800781 = 1.441543459892273 + 50.0 * 6.390408515930176
Epoch 300, val loss: 1.4841357469558716
Epoch 310, training loss: 320.6172790527344 = 1.413103699684143 + 50.0 * 6.384083271026611
Epoch 310, val loss: 1.4614222049713135
Epoch 320, training loss: 320.311767578125 = 1.384697437286377 + 50.0 * 6.378540992736816
Epoch 320, val loss: 1.439039707183838
Epoch 330, training loss: 320.0203552246094 = 1.3561711311340332 + 50.0 * 6.373283386230469
Epoch 330, val loss: 1.4170100688934326
Epoch 340, training loss: 320.1656188964844 = 1.3276697397232056 + 50.0 * 6.376759052276611
Epoch 340, val loss: 1.3954435586929321
Epoch 350, training loss: 319.4640808105469 = 1.2992092370986938 + 50.0 * 6.363297939300537
Epoch 350, val loss: 1.3740248680114746
Epoch 360, training loss: 319.0624694824219 = 1.271158218383789 + 50.0 * 6.355826377868652
Epoch 360, val loss: 1.3534055948257446
Epoch 370, training loss: 318.7936096191406 = 1.2433245182037354 + 50.0 * 6.351005554199219
Epoch 370, val loss: 1.3334020376205444
Epoch 380, training loss: 318.5292053222656 = 1.215687870979309 + 50.0 * 6.3462700843811035
Epoch 380, val loss: 1.3137537240982056
Epoch 390, training loss: 318.6002502441406 = 1.1880943775177002 + 50.0 * 6.348243236541748
Epoch 390, val loss: 1.2945445775985718
Epoch 400, training loss: 318.2149963378906 = 1.1604056358337402 + 50.0 * 6.341092109680176
Epoch 400, val loss: 1.2753106355667114
Epoch 410, training loss: 317.9356994628906 = 1.1330939531326294 + 50.0 * 6.336051940917969
Epoch 410, val loss: 1.2565619945526123
Epoch 420, training loss: 317.65728759765625 = 1.106093168258667 + 50.0 * 6.331024169921875
Epoch 420, val loss: 1.2382290363311768
Epoch 430, training loss: 317.520751953125 = 1.0793341398239136 + 50.0 * 6.328827857971191
Epoch 430, val loss: 1.2203912734985352
Epoch 440, training loss: 317.6610107421875 = 1.0525312423706055 + 50.0 * 6.332169532775879
Epoch 440, val loss: 1.2029564380645752
Epoch 450, training loss: 317.19921875 = 1.02620267868042 + 50.0 * 6.323460102081299
Epoch 450, val loss: 1.1858439445495605
Epoch 460, training loss: 316.9520568847656 = 1.0002528429031372 + 50.0 * 6.31903600692749
Epoch 460, val loss: 1.1690168380737305
Epoch 470, training loss: 316.7874450683594 = 0.9747803211212158 + 50.0 * 6.316253185272217
Epoch 470, val loss: 1.152824878692627
Epoch 480, training loss: 316.9891357421875 = 0.9497000575065613 + 50.0 * 6.320788383483887
Epoch 480, val loss: 1.1369892358779907
Epoch 490, training loss: 316.55059814453125 = 0.9247382283210754 + 50.0 * 6.312517166137695
Epoch 490, val loss: 1.121888279914856
Epoch 500, training loss: 316.32501220703125 = 0.9005066752433777 + 50.0 * 6.308489799499512
Epoch 500, val loss: 1.1071393489837646
Epoch 510, training loss: 316.67816162109375 = 0.8767037987709045 + 50.0 * 6.316029071807861
Epoch 510, val loss: 1.092807412147522
Epoch 520, training loss: 316.0849304199219 = 0.8532257676124573 + 50.0 * 6.304634094238281
Epoch 520, val loss: 1.0794142484664917
Epoch 530, training loss: 315.9551696777344 = 0.8305143713951111 + 50.0 * 6.302493572235107
Epoch 530, val loss: 1.06636643409729
Epoch 540, training loss: 315.81243896484375 = 0.8083537817001343 + 50.0 * 6.300081729888916
Epoch 540, val loss: 1.0540846586227417
Epoch 550, training loss: 315.7325134277344 = 0.7867273092269897 + 50.0 * 6.298915863037109
Epoch 550, val loss: 1.0424268245697021
Epoch 560, training loss: 315.6780700683594 = 0.7655543088912964 + 50.0 * 6.298250198364258
Epoch 560, val loss: 1.031139612197876
Epoch 570, training loss: 315.5610046386719 = 0.7448707222938538 + 50.0 * 6.296322345733643
Epoch 570, val loss: 1.020326852798462
Epoch 580, training loss: 315.50341796875 = 0.7248305678367615 + 50.0 * 6.295571804046631
Epoch 580, val loss: 1.0103538036346436
Epoch 590, training loss: 315.3089904785156 = 0.7054137587547302 + 50.0 * 6.292071342468262
Epoch 590, val loss: 1.0011117458343506
Epoch 600, training loss: 315.13427734375 = 0.686477780342102 + 50.0 * 6.2889556884765625
Epoch 600, val loss: 0.9923707246780396
Epoch 610, training loss: 315.1817626953125 = 0.668038547039032 + 50.0 * 6.290274620056152
Epoch 610, val loss: 0.9842684268951416
Epoch 620, training loss: 315.08990478515625 = 0.6500808000564575 + 50.0 * 6.288796424865723
Epoch 620, val loss: 0.9766311049461365
Epoch 630, training loss: 314.8875732421875 = 0.6325094103813171 + 50.0 * 6.285101413726807
Epoch 630, val loss: 0.9696007370948792
Epoch 640, training loss: 314.929443359375 = 0.6154661178588867 + 50.0 * 6.286279201507568
Epoch 640, val loss: 0.9632258415222168
Epoch 650, training loss: 314.7064208984375 = 0.5987868309020996 + 50.0 * 6.2821526527404785
Epoch 650, val loss: 0.9569680690765381
Epoch 660, training loss: 314.5680236816406 = 0.5826638340950012 + 50.0 * 6.279706954956055
Epoch 660, val loss: 0.9517043232917786
Epoch 670, training loss: 314.8014831542969 = 0.566917359828949 + 50.0 * 6.284690856933594
Epoch 670, val loss: 0.9469473361968994
Epoch 680, training loss: 314.5375061035156 = 0.5515487194061279 + 50.0 * 6.279719352722168
Epoch 680, val loss: 0.94208824634552
Epoch 690, training loss: 314.29925537109375 = 0.536577045917511 + 50.0 * 6.2752532958984375
Epoch 690, val loss: 0.9381094574928284
Epoch 700, training loss: 314.2498474121094 = 0.5220954418182373 + 50.0 * 6.274555206298828
Epoch 700, val loss: 0.9347575902938843
Epoch 710, training loss: 314.4068298339844 = 0.507954478263855 + 50.0 * 6.277976989746094
Epoch 710, val loss: 0.9314630627632141
Epoch 720, training loss: 314.20245361328125 = 0.4941055476665497 + 50.0 * 6.274167060852051
Epoch 720, val loss: 0.928886890411377
Epoch 730, training loss: 314.09918212890625 = 0.48065823316574097 + 50.0 * 6.2723708152771
Epoch 730, val loss: 0.926564633846283
Epoch 740, training loss: 314.1277160644531 = 0.4675177335739136 + 50.0 * 6.2732038497924805
Epoch 740, val loss: 0.9244717955589294
Epoch 750, training loss: 313.85040283203125 = 0.45473796129226685 + 50.0 * 6.267913341522217
Epoch 750, val loss: 0.9229981303215027
Epoch 760, training loss: 313.7947998046875 = 0.4423636794090271 + 50.0 * 6.2670488357543945
Epoch 760, val loss: 0.9219533801078796
Epoch 770, training loss: 313.7054138183594 = 0.43031948804855347 + 50.0 * 6.265501499176025
Epoch 770, val loss: 0.921146035194397
Epoch 780, training loss: 314.08489990234375 = 0.41858577728271484 + 50.0 * 6.273326396942139
Epoch 780, val loss: 0.9204261898994446
Epoch 790, training loss: 313.7572326660156 = 0.40710046887397766 + 50.0 * 6.267002582550049
Epoch 790, val loss: 0.920615017414093
Epoch 800, training loss: 313.5510559082031 = 0.39595940709114075 + 50.0 * 6.263102054595947
Epoch 800, val loss: 0.9205913543701172
Epoch 810, training loss: 313.4683837890625 = 0.38518986105918884 + 50.0 * 6.261663913726807
Epoch 810, val loss: 0.9212132692337036
Epoch 820, training loss: 313.8781433105469 = 0.3746991753578186 + 50.0 * 6.270069122314453
Epoch 820, val loss: 0.9220295548439026
Epoch 830, training loss: 313.3138122558594 = 0.36445415019989014 + 50.0 * 6.2589874267578125
Epoch 830, val loss: 0.9228874444961548
Epoch 840, training loss: 313.3337097167969 = 0.3545748293399811 + 50.0 * 6.25958251953125
Epoch 840, val loss: 0.9241973757743835
Epoch 850, training loss: 313.2658386230469 = 0.34499210119247437 + 50.0 * 6.258416652679443
Epoch 850, val loss: 0.9260284304618835
Epoch 860, training loss: 313.36993408203125 = 0.3356969356536865 + 50.0 * 6.260684967041016
Epoch 860, val loss: 0.9279798269271851
Epoch 870, training loss: 313.17608642578125 = 0.32665205001831055 + 50.0 * 6.256988525390625
Epoch 870, val loss: 0.9300448894500732
Epoch 880, training loss: 313.0812683105469 = 0.31788912415504456 + 50.0 * 6.255267143249512
Epoch 880, val loss: 0.9325964450836182
Epoch 890, training loss: 313.0291442871094 = 0.30940675735473633 + 50.0 * 6.25439453125
Epoch 890, val loss: 0.9353983998298645
Epoch 900, training loss: 313.39434814453125 = 0.30115580558776855 + 50.0 * 6.261864185333252
Epoch 900, val loss: 0.9382065534591675
Epoch 910, training loss: 312.94683837890625 = 0.2930363714694977 + 50.0 * 6.253076076507568
Epoch 910, val loss: 0.9414049386978149
Epoch 920, training loss: 312.9088439941406 = 0.28520283102989197 + 50.0 * 6.252472400665283
Epoch 920, val loss: 0.9446420669555664
Epoch 930, training loss: 312.8407897949219 = 0.27765849232673645 + 50.0 * 6.251262664794922
Epoch 930, val loss: 0.9483415484428406
Epoch 940, training loss: 313.1991271972656 = 0.27029645442962646 + 50.0 * 6.2585768699646
Epoch 940, val loss: 0.9520995020866394
Epoch 950, training loss: 312.8046875 = 0.263138085603714 + 50.0 * 6.25083065032959
Epoch 950, val loss: 0.9557580947875977
Epoch 960, training loss: 312.75244140625 = 0.25619080662727356 + 50.0 * 6.249924659729004
Epoch 960, val loss: 0.9598374962806702
Epoch 970, training loss: 312.7911376953125 = 0.24942819774150848 + 50.0 * 6.2508344650268555
Epoch 970, val loss: 0.9641815423965454
Epoch 980, training loss: 312.7358703613281 = 0.24289073050022125 + 50.0 * 6.249859809875488
Epoch 980, val loss: 0.9682619571685791
Epoch 990, training loss: 312.70416259765625 = 0.23649394512176514 + 50.0 * 6.249353408813477
Epoch 990, val loss: 0.9731352925300598
Epoch 1000, training loss: 312.5898132324219 = 0.23038777709007263 + 50.0 * 6.247188568115234
Epoch 1000, val loss: 0.9777776598930359
Epoch 1010, training loss: 312.494140625 = 0.2244170904159546 + 50.0 * 6.245394229888916
Epoch 1010, val loss: 0.9826552867889404
Epoch 1020, training loss: 312.4683532714844 = 0.21864698827266693 + 50.0 * 6.244993686676025
Epoch 1020, val loss: 0.9876008033752441
Epoch 1030, training loss: 312.79376220703125 = 0.21304073929786682 + 50.0 * 6.251614093780518
Epoch 1030, val loss: 0.9928539991378784
Epoch 1040, training loss: 312.6471862792969 = 0.20754912495613098 + 50.0 * 6.24879264831543
Epoch 1040, val loss: 0.9980068802833557
Epoch 1050, training loss: 312.4211730957031 = 0.20218630135059357 + 50.0 * 6.244379997253418
Epoch 1050, val loss: 1.0034078359603882
Epoch 1060, training loss: 312.4013977050781 = 0.1970600187778473 + 50.0 * 6.244087219238281
Epoch 1060, val loss: 1.0089867115020752
Epoch 1070, training loss: 312.4030456542969 = 0.19203482568264008 + 50.0 * 6.244219779968262
Epoch 1070, val loss: 1.0145947933197021
Epoch 1080, training loss: 312.2789306640625 = 0.18714705109596252 + 50.0 * 6.241836071014404
Epoch 1080, val loss: 1.019911289215088
Epoch 1090, training loss: 312.43560791015625 = 0.18244414031505585 + 50.0 * 6.245063781738281
Epoch 1090, val loss: 1.0257099866867065
Epoch 1100, training loss: 312.1969909667969 = 0.17784634232521057 + 50.0 * 6.240382671356201
Epoch 1100, val loss: 1.031491756439209
Epoch 1110, training loss: 312.1575012207031 = 0.1734064519405365 + 50.0 * 6.239681720733643
Epoch 1110, val loss: 1.0372573137283325
Epoch 1120, training loss: 312.1183166503906 = 0.16911548376083374 + 50.0 * 6.238983631134033
Epoch 1120, val loss: 1.0432000160217285
Epoch 1130, training loss: 312.2518005371094 = 0.16495810449123383 + 50.0 * 6.241737365722656
Epoch 1130, val loss: 1.049118995666504
Epoch 1140, training loss: 312.1492919921875 = 0.16085880994796753 + 50.0 * 6.2397685050964355
Epoch 1140, val loss: 1.0551345348358154
Epoch 1150, training loss: 312.1264953613281 = 0.15688370168209076 + 50.0 * 6.239391803741455
Epoch 1150, val loss: 1.0614612102508545
Epoch 1160, training loss: 312.02606201171875 = 0.15304163098335266 + 50.0 * 6.237460136413574
Epoch 1160, val loss: 1.0675023794174194
Epoch 1170, training loss: 311.9651184082031 = 0.14932413399219513 + 50.0 * 6.236315727233887
Epoch 1170, val loss: 1.0739312171936035
Epoch 1180, training loss: 312.118408203125 = 0.1457344889640808 + 50.0 * 6.2394537925720215
Epoch 1180, val loss: 1.0802537202835083
Epoch 1190, training loss: 312.0278015136719 = 0.14212697744369507 + 50.0 * 6.23771333694458
Epoch 1190, val loss: 1.0867440700531006
Epoch 1200, training loss: 312.0856018066406 = 0.13868014514446259 + 50.0 * 6.238938808441162
Epoch 1200, val loss: 1.0928934812545776
Epoch 1210, training loss: 311.8956604003906 = 0.13530951738357544 + 50.0 * 6.2352070808410645
Epoch 1210, val loss: 1.0993046760559082
Epoch 1220, training loss: 311.8772277832031 = 0.1320803016424179 + 50.0 * 6.234902858734131
Epoch 1220, val loss: 1.105949878692627
Epoch 1230, training loss: 311.9281311035156 = 0.12895353138446808 + 50.0 * 6.235983371734619
Epoch 1230, val loss: 1.1125589609146118
Epoch 1240, training loss: 311.8233337402344 = 0.12588323652744293 + 50.0 * 6.233949184417725
Epoch 1240, val loss: 1.1189841032028198
Epoch 1250, training loss: 311.8392333984375 = 0.12291040271520615 + 50.0 * 6.234325885772705
Epoch 1250, val loss: 1.1253619194030762
Epoch 1260, training loss: 312.16009521484375 = 0.12001320719718933 + 50.0 * 6.2408013343811035
Epoch 1260, val loss: 1.1318587064743042
Epoch 1270, training loss: 311.88934326171875 = 0.11716296523809433 + 50.0 * 6.235443592071533
Epoch 1270, val loss: 1.1387887001037598
Epoch 1280, training loss: 311.7146911621094 = 0.11441199481487274 + 50.0 * 6.232005596160889
Epoch 1280, val loss: 1.1451830863952637
Epoch 1290, training loss: 311.6435546875 = 0.11176081746816635 + 50.0 * 6.230636119842529
Epoch 1290, val loss: 1.1519553661346436
Epoch 1300, training loss: 311.64990234375 = 0.10918378084897995 + 50.0 * 6.230813980102539
Epoch 1300, val loss: 1.1585240364074707
Epoch 1310, training loss: 312.19403076171875 = 0.10666428506374359 + 50.0 * 6.2417473793029785
Epoch 1310, val loss: 1.165107011795044
Epoch 1320, training loss: 311.6681213378906 = 0.10414955765008926 + 50.0 * 6.231279373168945
Epoch 1320, val loss: 1.1713473796844482
Epoch 1330, training loss: 311.6289978027344 = 0.10175463557243347 + 50.0 * 6.2305450439453125
Epoch 1330, val loss: 1.17805814743042
Epoch 1340, training loss: 311.5716552734375 = 0.09943409264087677 + 50.0 * 6.22944450378418
Epoch 1340, val loss: 1.1848273277282715
Epoch 1350, training loss: 311.8257751464844 = 0.09717722982168198 + 50.0 * 6.234571933746338
Epoch 1350, val loss: 1.1914325952529907
Epoch 1360, training loss: 311.5153503417969 = 0.09496362507343292 + 50.0 * 6.228407382965088
Epoch 1360, val loss: 1.197530746459961
Epoch 1370, training loss: 311.4717712402344 = 0.09281142801046371 + 50.0 * 6.227579116821289
Epoch 1370, val loss: 1.204229474067688
Epoch 1380, training loss: 311.633544921875 = 0.0907188281416893 + 50.0 * 6.230856418609619
Epoch 1380, val loss: 1.2106311321258545
Epoch 1390, training loss: 311.5992126464844 = 0.08866690844297409 + 50.0 * 6.23021125793457
Epoch 1390, val loss: 1.2169818878173828
Epoch 1400, training loss: 311.5455322265625 = 0.08666108548641205 + 50.0 * 6.229177474975586
Epoch 1400, val loss: 1.2237707376480103
Epoch 1410, training loss: 311.44500732421875 = 0.08470571041107178 + 50.0 * 6.227205753326416
Epoch 1410, val loss: 1.2299220561981201
Epoch 1420, training loss: 311.38360595703125 = 0.0828225314617157 + 50.0 * 6.226015567779541
Epoch 1420, val loss: 1.2364652156829834
Epoch 1430, training loss: 311.40203857421875 = 0.08099326491355896 + 50.0 * 6.226420879364014
Epoch 1430, val loss: 1.2428269386291504
Epoch 1440, training loss: 311.7853088378906 = 0.0791984349489212 + 50.0 * 6.234122276306152
Epoch 1440, val loss: 1.2492338418960571
Epoch 1450, training loss: 311.4225769042969 = 0.07742563635110855 + 50.0 * 6.226902961730957
Epoch 1450, val loss: 1.2552063465118408
Epoch 1460, training loss: 311.36444091796875 = 0.07570440322160721 + 50.0 * 6.225774765014648
Epoch 1460, val loss: 1.2615504264831543
Epoch 1470, training loss: 311.4662170410156 = 0.07405883073806763 + 50.0 * 6.227843284606934
Epoch 1470, val loss: 1.2676061391830444
Epoch 1480, training loss: 311.32366943359375 = 0.0724225789308548 + 50.0 * 6.225025177001953
Epoch 1480, val loss: 1.2738333940505981
Epoch 1490, training loss: 311.3805847167969 = 0.07084093242883682 + 50.0 * 6.226195335388184
Epoch 1490, val loss: 1.2800620794296265
Epoch 1500, training loss: 311.24932861328125 = 0.06930744647979736 + 50.0 * 6.223600387573242
Epoch 1500, val loss: 1.2862913608551025
Epoch 1510, training loss: 311.25982666015625 = 0.06782149523496628 + 50.0 * 6.223840236663818
Epoch 1510, val loss: 1.2924649715423584
Epoch 1520, training loss: 311.62139892578125 = 0.06636533886194229 + 50.0 * 6.231100559234619
Epoch 1520, val loss: 1.2982319593429565
Epoch 1530, training loss: 311.3323059082031 = 0.06491480767726898 + 50.0 * 6.225347995758057
Epoch 1530, val loss: 1.3045545816421509
Epoch 1540, training loss: 311.24041748046875 = 0.06353104114532471 + 50.0 * 6.223537445068359
Epoch 1540, val loss: 1.3104112148284912
Epoch 1550, training loss: 311.4545593261719 = 0.062187422066926956 + 50.0 * 6.227847576141357
Epoch 1550, val loss: 1.316259503364563
Epoch 1560, training loss: 311.2530212402344 = 0.060846760869026184 + 50.0 * 6.223843097686768
Epoch 1560, val loss: 1.3225334882736206
Epoch 1570, training loss: 311.1914367675781 = 0.05955719202756882 + 50.0 * 6.222637176513672
Epoch 1570, val loss: 1.328485131263733
Epoch 1580, training loss: 311.2353210449219 = 0.05831170827150345 + 50.0 * 6.223540306091309
Epoch 1580, val loss: 1.3344264030456543
Epoch 1590, training loss: 311.1318664550781 = 0.057083360850811005 + 50.0 * 6.221495151519775
Epoch 1590, val loss: 1.340202808380127
Epoch 1600, training loss: 311.154541015625 = 0.055899478495121 + 50.0 * 6.221972465515137
Epoch 1600, val loss: 1.3458175659179688
Epoch 1610, training loss: 311.3140563964844 = 0.054747048765420914 + 50.0 * 6.225186347961426
Epoch 1610, val loss: 1.351685881614685
Epoch 1620, training loss: 311.1658935546875 = 0.053605105727910995 + 50.0 * 6.222245693206787
Epoch 1620, val loss: 1.3574532270431519
Epoch 1630, training loss: 311.10406494140625 = 0.05249423533678055 + 50.0 * 6.221031665802002
Epoch 1630, val loss: 1.3631662130355835
Epoch 1640, training loss: 311.1802673339844 = 0.05143129080533981 + 50.0 * 6.22257661819458
Epoch 1640, val loss: 1.3691831827163696
Epoch 1650, training loss: 311.091796875 = 0.05037706717848778 + 50.0 * 6.220828533172607
Epoch 1650, val loss: 1.374455213546753
Epoch 1660, training loss: 311.1277160644531 = 0.04935970902442932 + 50.0 * 6.221567153930664
Epoch 1660, val loss: 1.3799453973770142
Epoch 1670, training loss: 311.0899963378906 = 0.04836297407746315 + 50.0 * 6.220832824707031
Epoch 1670, val loss: 1.3856956958770752
Epoch 1680, training loss: 311.14111328125 = 0.047395505011081696 + 50.0 * 6.221874237060547
Epoch 1680, val loss: 1.3910053968429565
Epoch 1690, training loss: 311.0002746582031 = 0.046431466937065125 + 50.0 * 6.219077110290527
Epoch 1690, val loss: 1.3966314792633057
Epoch 1700, training loss: 310.9527587890625 = 0.04550457000732422 + 50.0 * 6.218145370483398
Epoch 1700, val loss: 1.401950478553772
Epoch 1710, training loss: 310.9330749511719 = 0.04461325332522392 + 50.0 * 6.217769145965576
Epoch 1710, val loss: 1.4074277877807617
Epoch 1720, training loss: 311.2409362792969 = 0.04374634101986885 + 50.0 * 6.223944187164307
Epoch 1720, val loss: 1.4131715297698975
Epoch 1730, training loss: 311.0719909667969 = 0.04287353530526161 + 50.0 * 6.220582008361816
Epoch 1730, val loss: 1.4175142049789429
Epoch 1740, training loss: 311.04364013671875 = 0.04203449562191963 + 50.0 * 6.220032215118408
Epoch 1740, val loss: 1.423262596130371
Epoch 1750, training loss: 311.01885986328125 = 0.04121355712413788 + 50.0 * 6.219553470611572
Epoch 1750, val loss: 1.42808198928833
Epoch 1760, training loss: 310.886474609375 = 0.04040999338030815 + 50.0 * 6.216921329498291
Epoch 1760, val loss: 1.4333528280258179
Epoch 1770, training loss: 310.9784240722656 = 0.039638224989175797 + 50.0 * 6.218775749206543
Epoch 1770, val loss: 1.4383429288864136
Epoch 1780, training loss: 310.89044189453125 = 0.038879334926605225 + 50.0 * 6.217031002044678
Epoch 1780, val loss: 1.4434432983398438
Epoch 1790, training loss: 311.10595703125 = 0.03814516216516495 + 50.0 * 6.22135591506958
Epoch 1790, val loss: 1.448865294456482
Epoch 1800, training loss: 310.83074951171875 = 0.037414636462926865 + 50.0 * 6.215866565704346
Epoch 1800, val loss: 1.4534671306610107
Epoch 1810, training loss: 310.7984619140625 = 0.036710213869810104 + 50.0 * 6.215234756469727
Epoch 1810, val loss: 1.458812952041626
Epoch 1820, training loss: 310.8856506347656 = 0.036034200340509415 + 50.0 * 6.216992378234863
Epoch 1820, val loss: 1.4638755321502686
Epoch 1830, training loss: 311.1817321777344 = 0.03538176417350769 + 50.0 * 6.222927093505859
Epoch 1830, val loss: 1.4683245420455933
Epoch 1840, training loss: 310.8877258300781 = 0.03469587862491608 + 50.0 * 6.217060565948486
Epoch 1840, val loss: 1.4732985496520996
Epoch 1850, training loss: 310.7665710449219 = 0.034058578312397 + 50.0 * 6.2146501541137695
Epoch 1850, val loss: 1.4783967733383179
Epoch 1860, training loss: 310.7347106933594 = 0.03343746066093445 + 50.0 * 6.214025497436523
Epoch 1860, val loss: 1.4831435680389404
Epoch 1870, training loss: 310.83416748046875 = 0.03283795714378357 + 50.0 * 6.216026306152344
Epoch 1870, val loss: 1.4879928827285767
Epoch 1880, training loss: 310.8960266113281 = 0.03224547579884529 + 50.0 * 6.217275619506836
Epoch 1880, val loss: 1.4925388097763062
Epoch 1890, training loss: 310.8289489746094 = 0.031668130308389664 + 50.0 * 6.215945720672607
Epoch 1890, val loss: 1.4972447156906128
Epoch 1900, training loss: 310.81475830078125 = 0.03109307959675789 + 50.0 * 6.215672969818115
Epoch 1900, val loss: 1.5017762184143066
Epoch 1910, training loss: 311.21124267578125 = 0.03054393269121647 + 50.0 * 6.223613739013672
Epoch 1910, val loss: 1.5063951015472412
Epoch 1920, training loss: 310.7623596191406 = 0.0299892146140337 + 50.0 * 6.21464729309082
Epoch 1920, val loss: 1.5112067461013794
Epoch 1930, training loss: 310.6449279785156 = 0.029456790536642075 + 50.0 * 6.212309837341309
Epoch 1930, val loss: 1.515749216079712
Epoch 1940, training loss: 310.6333312988281 = 0.028948260471224785 + 50.0 * 6.212087154388428
Epoch 1940, val loss: 1.5202182531356812
Epoch 1950, training loss: 310.62811279296875 = 0.028452236205339432 + 50.0 * 6.211993217468262
Epoch 1950, val loss: 1.5247018337249756
Epoch 1960, training loss: 311.20526123046875 = 0.027987411245703697 + 50.0 * 6.223545551300049
Epoch 1960, val loss: 1.5287046432495117
Epoch 1970, training loss: 310.8420104980469 = 0.027484184131026268 + 50.0 * 6.216290473937988
Epoch 1970, val loss: 1.5338897705078125
Epoch 1980, training loss: 310.665283203125 = 0.027011914178729057 + 50.0 * 6.212765216827393
Epoch 1980, val loss: 1.537756323814392
Epoch 1990, training loss: 310.65679931640625 = 0.02655666321516037 + 50.0 * 6.212604999542236
Epoch 1990, val loss: 1.542513132095337
Epoch 2000, training loss: 310.8037109375 = 0.026110613718628883 + 50.0 * 6.215551853179932
Epoch 2000, val loss: 1.5469645261764526
Epoch 2010, training loss: 310.6433410644531 = 0.02566886879503727 + 50.0 * 6.212353706359863
Epoch 2010, val loss: 1.5508290529251099
Epoch 2020, training loss: 310.63067626953125 = 0.025241216644644737 + 50.0 * 6.212108612060547
Epoch 2020, val loss: 1.555367350578308
Epoch 2030, training loss: 310.6562805175781 = 0.02482944168150425 + 50.0 * 6.2126288414001465
Epoch 2030, val loss: 1.559693694114685
Epoch 2040, training loss: 310.582763671875 = 0.02442188933491707 + 50.0 * 6.211166858673096
Epoch 2040, val loss: 1.5638554096221924
Epoch 2050, training loss: 310.70654296875 = 0.02402949519455433 + 50.0 * 6.213650703430176
Epoch 2050, val loss: 1.5679432153701782
Epoch 2060, training loss: 310.6044921875 = 0.023638905957341194 + 50.0 * 6.2116169929504395
Epoch 2060, val loss: 1.5721609592437744
Epoch 2070, training loss: 310.5792236328125 = 0.02325739711523056 + 50.0 * 6.211119651794434
Epoch 2070, val loss: 1.5762817859649658
Epoch 2080, training loss: 310.7611999511719 = 0.02288997545838356 + 50.0 * 6.214766502380371
Epoch 2080, val loss: 1.5799403190612793
Epoch 2090, training loss: 310.60345458984375 = 0.022516749799251556 + 50.0 * 6.211618900299072
Epoch 2090, val loss: 1.5843651294708252
Epoch 2100, training loss: 310.56475830078125 = 0.022161543369293213 + 50.0 * 6.210852146148682
Epoch 2100, val loss: 1.5882530212402344
Epoch 2110, training loss: 310.5187072753906 = 0.021815750747919083 + 50.0 * 6.209937572479248
Epoch 2110, val loss: 1.5924270153045654
Epoch 2120, training loss: 310.5526123046875 = 0.02147826738655567 + 50.0 * 6.210622787475586
Epoch 2120, val loss: 1.5963778495788574
Epoch 2130, training loss: 310.5422058105469 = 0.021148106083273888 + 50.0 * 6.210421085357666
Epoch 2130, val loss: 1.600338101387024
Epoch 2140, training loss: 310.5600891113281 = 0.0208242516964674 + 50.0 * 6.210785388946533
Epoch 2140, val loss: 1.6042954921722412
Epoch 2150, training loss: 310.65234375 = 0.020507346838712692 + 50.0 * 6.212636470794678
Epoch 2150, val loss: 1.608115315437317
Epoch 2160, training loss: 310.5744934082031 = 0.020195258781313896 + 50.0 * 6.211085796356201
Epoch 2160, val loss: 1.6119968891143799
Epoch 2170, training loss: 310.4836120605469 = 0.01989065855741501 + 50.0 * 6.2092742919921875
Epoch 2170, val loss: 1.6154779195785522
Epoch 2180, training loss: 310.5133361816406 = 0.019594434648752213 + 50.0 * 6.209875106811523
Epoch 2180, val loss: 1.6195323467254639
Epoch 2190, training loss: 310.50653076171875 = 0.019306445494294167 + 50.0 * 6.209744453430176
Epoch 2190, val loss: 1.6231712102890015
Epoch 2200, training loss: 310.4722595214844 = 0.019022800028324127 + 50.0 * 6.209064960479736
Epoch 2200, val loss: 1.6274157762527466
Epoch 2210, training loss: 310.8260498046875 = 0.018747584894299507 + 50.0 * 6.216145992279053
Epoch 2210, val loss: 1.6310863494873047
Epoch 2220, training loss: 310.480224609375 = 0.01846700720489025 + 50.0 * 6.209235191345215
Epoch 2220, val loss: 1.634488582611084
Epoch 2230, training loss: 310.4330749511719 = 0.01819780096411705 + 50.0 * 6.2082977294921875
Epoch 2230, val loss: 1.6384050846099854
Epoch 2240, training loss: 310.4794921875 = 0.01793932355940342 + 50.0 * 6.209230899810791
Epoch 2240, val loss: 1.6418097019195557
Epoch 2250, training loss: 310.3851623535156 = 0.01768156886100769 + 50.0 * 6.20734977722168
Epoch 2250, val loss: 1.6454131603240967
Epoch 2260, training loss: 310.4090270996094 = 0.01743296906352043 + 50.0 * 6.207831859588623
Epoch 2260, val loss: 1.6488690376281738
Epoch 2270, training loss: 310.57342529296875 = 0.01719154790043831 + 50.0 * 6.211124897003174
Epoch 2270, val loss: 1.6522496938705444
Epoch 2280, training loss: 310.4106750488281 = 0.016946615651249886 + 50.0 * 6.207874774932861
Epoch 2280, val loss: 1.6564929485321045
Epoch 2290, training loss: 310.3455505371094 = 0.01671190932393074 + 50.0 * 6.206576824188232
Epoch 2290, val loss: 1.6597437858581543
Epoch 2300, training loss: 310.36077880859375 = 0.016482988372445107 + 50.0 * 6.206886291503906
Epoch 2300, val loss: 1.6630152463912964
Epoch 2310, training loss: 310.56011962890625 = 0.016263624653220177 + 50.0 * 6.210876941680908
Epoch 2310, val loss: 1.6665370464324951
Epoch 2320, training loss: 310.3298034667969 = 0.01603260077536106 + 50.0 * 6.206275463104248
Epoch 2320, val loss: 1.669964075088501
Epoch 2330, training loss: 310.4397277832031 = 0.01581830345094204 + 50.0 * 6.2084784507751465
Epoch 2330, val loss: 1.6731164455413818
Epoch 2340, training loss: 310.6065368652344 = 0.015607893466949463 + 50.0 * 6.211818695068359
Epoch 2340, val loss: 1.676527976989746
Epoch 2350, training loss: 310.3822021484375 = 0.01538791973143816 + 50.0 * 6.20733642578125
Epoch 2350, val loss: 1.6800740957260132
Epoch 2360, training loss: 310.2833251953125 = 0.015185261145234108 + 50.0 * 6.205362319946289
Epoch 2360, val loss: 1.6830335855484009
Epoch 2370, training loss: 310.25537109375 = 0.014986180700361729 + 50.0 * 6.204807758331299
Epoch 2370, val loss: 1.686684489250183
Epoch 2380, training loss: 310.4684753417969 = 0.014797785319387913 + 50.0 * 6.209073543548584
Epoch 2380, val loss: 1.689357042312622
Epoch 2390, training loss: 310.2569580078125 = 0.01460198499262333 + 50.0 * 6.20484733581543
Epoch 2390, val loss: 1.693001627922058
Epoch 2400, training loss: 310.217041015625 = 0.014411122538149357 + 50.0 * 6.204052448272705
Epoch 2400, val loss: 1.6961888074874878
Epoch 2410, training loss: 310.2815856933594 = 0.014228221029043198 + 50.0 * 6.205347537994385
Epoch 2410, val loss: 1.6996471881866455
Epoch 2420, training loss: 310.48052978515625 = 0.014050781726837158 + 50.0 * 6.209329605102539
Epoch 2420, val loss: 1.7028732299804688
Epoch 2430, training loss: 310.3122253417969 = 0.013867856003344059 + 50.0 * 6.205967426300049
Epoch 2430, val loss: 1.7052147388458252
Epoch 2440, training loss: 310.2961730957031 = 0.013695141300559044 + 50.0 * 6.205649375915527
Epoch 2440, val loss: 1.7087023258209229
Epoch 2450, training loss: 310.4920959472656 = 0.01352633535861969 + 50.0 * 6.209571361541748
Epoch 2450, val loss: 1.7114630937576294
Epoch 2460, training loss: 310.25433349609375 = 0.013353431597352028 + 50.0 * 6.204819202423096
Epoch 2460, val loss: 1.7147496938705444
Epoch 2470, training loss: 310.282958984375 = 0.01318894699215889 + 50.0 * 6.205395221710205
Epoch 2470, val loss: 1.71770441532135
Epoch 2480, training loss: 310.2879943847656 = 0.013028927147388458 + 50.0 * 6.205499172210693
Epoch 2480, val loss: 1.7209330797195435
Epoch 2490, training loss: 310.247314453125 = 0.01286984607577324 + 50.0 * 6.204689025878906
Epoch 2490, val loss: 1.723637342453003
Epoch 2500, training loss: 310.3471984863281 = 0.012714631855487823 + 50.0 * 6.206689357757568
Epoch 2500, val loss: 1.727073311805725
Epoch 2510, training loss: 310.16143798828125 = 0.012560870498418808 + 50.0 * 6.202977180480957
Epoch 2510, val loss: 1.7295892238616943
Epoch 2520, training loss: 310.1532287597656 = 0.01241380162537098 + 50.0 * 6.202816009521484
Epoch 2520, val loss: 1.732528805732727
Epoch 2530, training loss: 310.4680480957031 = 0.012276915833353996 + 50.0 * 6.209115505218506
Epoch 2530, val loss: 1.7349867820739746
Epoch 2540, training loss: 310.1600646972656 = 0.012120254337787628 + 50.0 * 6.202959060668945
Epoch 2540, val loss: 1.7382375001907349
Epoch 2550, training loss: 310.1386413574219 = 0.011980514042079449 + 50.0 * 6.20253324508667
Epoch 2550, val loss: 1.7412054538726807
Epoch 2560, training loss: 310.129638671875 = 0.011842797510325909 + 50.0 * 6.202355861663818
Epoch 2560, val loss: 1.743629813194275
Epoch 2570, training loss: 310.4084167480469 = 0.011712047271430492 + 50.0 * 6.2079339027404785
Epoch 2570, val loss: 1.7464828491210938
Epoch 2580, training loss: 310.1700439453125 = 0.01157232653349638 + 50.0 * 6.203169822692871
Epoch 2580, val loss: 1.749426007270813
Epoch 2590, training loss: 310.0899353027344 = 0.011439167894423008 + 50.0 * 6.2015700340271
Epoch 2590, val loss: 1.752069115638733
Epoch 2600, training loss: 310.07183837890625 = 0.011309811845421791 + 50.0 * 6.2012104988098145
Epoch 2600, val loss: 1.7550066709518433
Epoch 2610, training loss: 310.24090576171875 = 0.011190451681613922 + 50.0 * 6.204594135284424
Epoch 2610, val loss: 1.7581087350845337
Epoch 2620, training loss: 310.1604919433594 = 0.011062179692089558 + 50.0 * 6.202988624572754
Epoch 2620, val loss: 1.7601282596588135
Epoch 2630, training loss: 310.1611328125 = 0.010937731713056564 + 50.0 * 6.203003883361816
Epoch 2630, val loss: 1.7629106044769287
Epoch 2640, training loss: 310.0920104980469 = 0.010816886089742184 + 50.0 * 6.201623916625977
Epoch 2640, val loss: 1.7654790878295898
Epoch 2650, training loss: 310.1079406738281 = 0.01069945190101862 + 50.0 * 6.201944828033447
Epoch 2650, val loss: 1.768188714981079
Epoch 2660, training loss: 310.3569641113281 = 0.010589213110506535 + 50.0 * 6.206927299499512
Epoch 2660, val loss: 1.7703453302383423
Epoch 2670, training loss: 310.12725830078125 = 0.010470399633049965 + 50.0 * 6.202335834503174
Epoch 2670, val loss: 1.7730354070663452
Epoch 2680, training loss: 310.052978515625 = 0.010356450453400612 + 50.0 * 6.200852394104004
Epoch 2680, val loss: 1.7759612798690796
Epoch 2690, training loss: 310.015869140625 = 0.010248241946101189 + 50.0 * 6.200112342834473
Epoch 2690, val loss: 1.7783602476119995
Epoch 2700, training loss: 310.1509704589844 = 0.010142533108592033 + 50.0 * 6.202816963195801
Epoch 2700, val loss: 1.7808778285980225
Epoch 2710, training loss: 310.3004150390625 = 0.010035532526671886 + 50.0 * 6.205807685852051
Epoch 2710, val loss: 1.783333659172058
Epoch 2720, training loss: 310.0826416015625 = 0.009930003434419632 + 50.0 * 6.2014546394348145
Epoch 2720, val loss: 1.785667061805725
Epoch 2730, training loss: 310.00189208984375 = 0.009823204018175602 + 50.0 * 6.199841022491455
Epoch 2730, val loss: 1.788051962852478
Epoch 2740, training loss: 309.98406982421875 = 0.009724253788590431 + 50.0 * 6.19948673248291
Epoch 2740, val loss: 1.7906012535095215
Epoch 2750, training loss: 310.0896301269531 = 0.009627447463572025 + 50.0 * 6.201600074768066
Epoch 2750, val loss: 1.7932448387145996
Epoch 2760, training loss: 310.1351013183594 = 0.009530930779874325 + 50.0 * 6.202511787414551
Epoch 2760, val loss: 1.7958073616027832
Epoch 2770, training loss: 310.1086120605469 = 0.00943225808441639 + 50.0 * 6.20198392868042
Epoch 2770, val loss: 1.7979228496551514
Epoch 2780, training loss: 310.02606201171875 = 0.009336956776678562 + 50.0 * 6.200334548950195
Epoch 2780, val loss: 1.800307035446167
Epoch 2790, training loss: 309.9912109375 = 0.009244187735021114 + 50.0 * 6.199639320373535
Epoch 2790, val loss: 1.8021832704544067
Epoch 2800, training loss: 310.0560607910156 = 0.009155218489468098 + 50.0 * 6.2009382247924805
Epoch 2800, val loss: 1.8042939901351929
Epoch 2810, training loss: 310.0314025878906 = 0.009065425023436546 + 50.0 * 6.200447082519531
Epoch 2810, val loss: 1.8065685033798218
Epoch 2820, training loss: 309.9350280761719 = 0.008975911885499954 + 50.0 * 6.198521137237549
Epoch 2820, val loss: 1.8091992139816284
Epoch 2830, training loss: 310.1576843261719 = 0.008893058635294437 + 50.0 * 6.202976226806641
Epoch 2830, val loss: 1.8111833333969116
Epoch 2840, training loss: 310.0911865234375 = 0.008805680088698864 + 50.0 * 6.2016472816467285
Epoch 2840, val loss: 1.8129395246505737
Epoch 2850, training loss: 310.0233459472656 = 0.008717500604689121 + 50.0 * 6.200292587280273
Epoch 2850, val loss: 1.815173625946045
Epoch 2860, training loss: 309.9252624511719 = 0.008634489960968494 + 50.0 * 6.198332786560059
Epoch 2860, val loss: 1.8178532123565674
Epoch 2870, training loss: 309.9116516113281 = 0.008552293293178082 + 50.0 * 6.198061943054199
Epoch 2870, val loss: 1.8199779987335205
Epoch 2880, training loss: 309.9719543457031 = 0.008473879657685757 + 50.0 * 6.1992692947387695
Epoch 2880, val loss: 1.8222198486328125
Epoch 2890, training loss: 310.2001037597656 = 0.008396008983254433 + 50.0 * 6.203834056854248
Epoch 2890, val loss: 1.8242466449737549
Epoch 2900, training loss: 310.02386474609375 = 0.008320089429616928 + 50.0 * 6.200310707092285
Epoch 2900, val loss: 1.825776219367981
Epoch 2910, training loss: 310.0857849121094 = 0.008241213858127594 + 50.0 * 6.2015509605407715
Epoch 2910, val loss: 1.828182578086853
Epoch 2920, training loss: 309.93817138671875 = 0.008165406994521618 + 50.0 * 6.198599815368652
Epoch 2920, val loss: 1.8298879861831665
Epoch 2930, training loss: 309.9366760253906 = 0.008091339841485023 + 50.0 * 6.198571681976318
Epoch 2930, val loss: 1.8327032327651978
Epoch 2940, training loss: 310.0329895019531 = 0.008021267130970955 + 50.0 * 6.200499534606934
Epoch 2940, val loss: 1.834639549255371
Epoch 2950, training loss: 309.9024658203125 = 0.007947714067995548 + 50.0 * 6.197890758514404
Epoch 2950, val loss: 1.8360828161239624
Epoch 2960, training loss: 309.9271240234375 = 0.00787813775241375 + 50.0 * 6.198384761810303
Epoch 2960, val loss: 1.837739109992981
Epoch 2970, training loss: 310.019775390625 = 0.007811507675796747 + 50.0 * 6.200239181518555
Epoch 2970, val loss: 1.8393564224243164
Epoch 2980, training loss: 309.94561767578125 = 0.007741208653897047 + 50.0 * 6.198757171630859
Epoch 2980, val loss: 1.841758131980896
Epoch 2990, training loss: 309.891357421875 = 0.007673666346818209 + 50.0 * 6.197673797607422
Epoch 2990, val loss: 1.8439661264419556
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7333333333333334
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 431.7996826171875 = 1.9571619033813477 + 50.0 * 8.596850395202637
Epoch 0, val loss: 1.961211919784546
Epoch 10, training loss: 431.7515869140625 = 1.947504997253418 + 50.0 * 8.596081733703613
Epoch 10, val loss: 1.951106071472168
Epoch 20, training loss: 431.43780517578125 = 1.9354636669158936 + 50.0 * 8.590046882629395
Epoch 20, val loss: 1.938322901725769
Epoch 30, training loss: 429.32659912109375 = 1.9201922416687012 + 50.0 * 8.548128128051758
Epoch 30, val loss: 1.922053337097168
Epoch 40, training loss: 418.0679931640625 = 1.9023324251174927 + 50.0 * 8.323312759399414
Epoch 40, val loss: 1.9034850597381592
Epoch 50, training loss: 390.95123291015625 = 1.8836324214935303 + 50.0 * 7.7813520431518555
Epoch 50, val loss: 1.88422429561615
Epoch 60, training loss: 371.7615966796875 = 1.8684195280075073 + 50.0 * 7.397863864898682
Epoch 60, val loss: 1.8689262866973877
Epoch 70, training loss: 355.5297546386719 = 1.8534384965896606 + 50.0 * 7.073526859283447
Epoch 70, val loss: 1.8542907238006592
Epoch 80, training loss: 347.8583679199219 = 1.8400734663009644 + 50.0 * 6.920365810394287
Epoch 80, val loss: 1.8414726257324219
Epoch 90, training loss: 343.8807067871094 = 1.8275262117385864 + 50.0 * 6.841063976287842
Epoch 90, val loss: 1.829374074935913
Epoch 100, training loss: 339.8143005371094 = 1.815818190574646 + 50.0 * 6.759969711303711
Epoch 100, val loss: 1.8182599544525146
Epoch 110, training loss: 336.604248046875 = 1.8049646615982056 + 50.0 * 6.695985794067383
Epoch 110, val loss: 1.8079824447631836
Epoch 120, training loss: 334.5525817871094 = 1.7944040298461914 + 50.0 * 6.655163764953613
Epoch 120, val loss: 1.7980451583862305
Epoch 130, training loss: 332.6084289550781 = 1.7838225364685059 + 50.0 * 6.61649227142334
Epoch 130, val loss: 1.7880381345748901
Epoch 140, training loss: 330.5831604003906 = 1.773075819015503 + 50.0 * 6.576201915740967
Epoch 140, val loss: 1.7779051065444946
Epoch 150, training loss: 328.94091796875 = 1.762068510055542 + 50.0 * 6.543577194213867
Epoch 150, val loss: 1.76759934425354
Epoch 160, training loss: 327.6623840332031 = 1.7503172159194946 + 50.0 * 6.5182414054870605
Epoch 160, val loss: 1.7565680742263794
Epoch 170, training loss: 326.5639953613281 = 1.7376471757888794 + 50.0 * 6.496527194976807
Epoch 170, val loss: 1.7447935342788696
Epoch 180, training loss: 325.6759338378906 = 1.7240593433380127 + 50.0 * 6.479037761688232
Epoch 180, val loss: 1.7323822975158691
Epoch 190, training loss: 324.8473205566406 = 1.7094461917877197 + 50.0 * 6.462757587432861
Epoch 190, val loss: 1.7190942764282227
Epoch 200, training loss: 324.1631774902344 = 1.6936924457550049 + 50.0 * 6.449389457702637
Epoch 200, val loss: 1.7048133611679077
Epoch 210, training loss: 323.77203369140625 = 1.676640510559082 + 50.0 * 6.44190788269043
Epoch 210, val loss: 1.6893903017044067
Epoch 220, training loss: 323.03839111328125 = 1.658249855041504 + 50.0 * 6.427602291107178
Epoch 220, val loss: 1.6727643013000488
Epoch 230, training loss: 322.52740478515625 = 1.6386150121688843 + 50.0 * 6.417776107788086
Epoch 230, val loss: 1.6551220417022705
Epoch 240, training loss: 322.46466064453125 = 1.6177358627319336 + 50.0 * 6.416938781738281
Epoch 240, val loss: 1.636297583580017
Epoch 250, training loss: 321.8289489746094 = 1.5954042673110962 + 50.0 * 6.4046711921691895
Epoch 250, val loss: 1.6164216995239258
Epoch 260, training loss: 321.233642578125 = 1.571995735168457 + 50.0 * 6.393232822418213
Epoch 260, val loss: 1.5957367420196533
Epoch 270, training loss: 320.8533935546875 = 1.5475937128067017 + 50.0 * 6.386115550994873
Epoch 270, val loss: 1.5742255449295044
Epoch 280, training loss: 320.5656433105469 = 1.5223013162612915 + 50.0 * 6.380866527557373
Epoch 280, val loss: 1.5521214008331299
Epoch 290, training loss: 320.2191162109375 = 1.4960975646972656 + 50.0 * 6.374460220336914
Epoch 290, val loss: 1.5292850732803345
Epoch 300, training loss: 319.8832702636719 = 1.469258189201355 + 50.0 * 6.368279933929443
Epoch 300, val loss: 1.506491780281067
Epoch 310, training loss: 319.5551452636719 = 1.4420217275619507 + 50.0 * 6.362262725830078
Epoch 310, val loss: 1.4834117889404297
Epoch 320, training loss: 319.2606506347656 = 1.4144587516784668 + 50.0 * 6.356923580169678
Epoch 320, val loss: 1.4603731632232666
Epoch 330, training loss: 318.9904479980469 = 1.386668086051941 + 50.0 * 6.352076053619385
Epoch 330, val loss: 1.4374359846115112
Epoch 340, training loss: 318.7376708984375 = 1.3586353063583374 + 50.0 * 6.347580432891846
Epoch 340, val loss: 1.414557695388794
Epoch 350, training loss: 318.49676513671875 = 1.3305672407150269 + 50.0 * 6.343324184417725
Epoch 350, val loss: 1.3920085430145264
Epoch 360, training loss: 318.4716796875 = 1.3025736808776855 + 50.0 * 6.343381881713867
Epoch 360, val loss: 1.369789719581604
Epoch 370, training loss: 318.21295166015625 = 1.2747867107391357 + 50.0 * 6.33876371383667
Epoch 370, val loss: 1.3478046655654907
Epoch 380, training loss: 317.9122009277344 = 1.2471553087234497 + 50.0 * 6.333300590515137
Epoch 380, val loss: 1.3263509273529053
Epoch 390, training loss: 317.662841796875 = 1.2198915481567383 + 50.0 * 6.328859329223633
Epoch 390, val loss: 1.3054494857788086
Epoch 400, training loss: 317.9596252441406 = 1.1929798126220703 + 50.0 * 6.33533239364624
Epoch 400, val loss: 1.2851769924163818
Epoch 410, training loss: 317.3169250488281 = 1.1665407419204712 + 50.0 * 6.323007583618164
Epoch 410, val loss: 1.2650444507598877
Epoch 420, training loss: 317.12823486328125 = 1.1406980752944946 + 50.0 * 6.319751262664795
Epoch 420, val loss: 1.2456581592559814
Epoch 430, training loss: 316.9719543457031 = 1.1155030727386475 + 50.0 * 6.317128658294678
Epoch 430, val loss: 1.226947546005249
Epoch 440, training loss: 316.8028869628906 = 1.0909680128097534 + 50.0 * 6.31423807144165
Epoch 440, val loss: 1.2089619636535645
Epoch 450, training loss: 316.6761779785156 = 1.066921591758728 + 50.0 * 6.312184810638428
Epoch 450, val loss: 1.1915175914764404
Epoch 460, training loss: 316.54974365234375 = 1.0436272621154785 + 50.0 * 6.310122013092041
Epoch 460, val loss: 1.1746115684509277
Epoch 470, training loss: 316.4083251953125 = 1.0210634469985962 + 50.0 * 6.307745456695557
Epoch 470, val loss: 1.1584423780441284
Epoch 480, training loss: 316.2190856933594 = 0.9992813467979431 + 50.0 * 6.304396152496338
Epoch 480, val loss: 1.14304780960083
Epoch 490, training loss: 316.0966491699219 = 0.9781869649887085 + 50.0 * 6.302369117736816
Epoch 490, val loss: 1.1282844543457031
Epoch 500, training loss: 315.99505615234375 = 0.9576691389083862 + 50.0 * 6.300747394561768
Epoch 500, val loss: 1.113966941833496
Epoch 510, training loss: 315.8706970214844 = 0.937798023223877 + 50.0 * 6.2986578941345215
Epoch 510, val loss: 1.1003612279891968
Epoch 520, training loss: 315.73681640625 = 0.9186604619026184 + 50.0 * 6.29636287689209
Epoch 520, val loss: 1.0875754356384277
Epoch 530, training loss: 315.6625061035156 = 0.9002180695533752 + 50.0 * 6.295246124267578
Epoch 530, val loss: 1.0754338502883911
Epoch 540, training loss: 315.5664978027344 = 0.8823339939117432 + 50.0 * 6.2936835289001465
Epoch 540, val loss: 1.063759207725525
Epoch 550, training loss: 315.37432861328125 = 0.8650632500648499 + 50.0 * 6.290185451507568
Epoch 550, val loss: 1.05266535282135
Epoch 560, training loss: 315.2665100097656 = 0.8483003377914429 + 50.0 * 6.288364410400391
Epoch 560, val loss: 1.0422018766403198
Epoch 570, training loss: 315.1754150390625 = 0.8320809006690979 + 50.0 * 6.286866188049316
Epoch 570, val loss: 1.0324207544326782
Epoch 580, training loss: 315.2469482421875 = 0.8162644505500793 + 50.0 * 6.288613796234131
Epoch 580, val loss: 1.0231574773788452
Epoch 590, training loss: 315.1341552734375 = 0.8009164333343506 + 50.0 * 6.286664962768555
Epoch 590, val loss: 1.0142905712127686
Epoch 600, training loss: 314.918701171875 = 0.7859305143356323 + 50.0 * 6.282655715942383
Epoch 600, val loss: 1.0058687925338745
Epoch 610, training loss: 314.77947998046875 = 0.7714263796806335 + 50.0 * 6.280160903930664
Epoch 610, val loss: 0.9980512857437134
Epoch 620, training loss: 314.7164001464844 = 0.7573192119598389 + 50.0 * 6.279181480407715
Epoch 620, val loss: 0.9906291961669922
Epoch 630, training loss: 314.8741455078125 = 0.743491530418396 + 50.0 * 6.2826128005981445
Epoch 630, val loss: 0.9835931658744812
Epoch 640, training loss: 314.7398986816406 = 0.7299239039421082 + 50.0 * 6.2801995277404785
Epoch 640, val loss: 0.9768322110176086
Epoch 650, training loss: 314.6729736328125 = 0.7165884375572205 + 50.0 * 6.279128074645996
Epoch 650, val loss: 0.9706010818481445
Epoch 660, training loss: 314.4211120605469 = 0.7034875154495239 + 50.0 * 6.274352550506592
Epoch 660, val loss: 0.9645261168479919
Epoch 670, training loss: 314.3257141113281 = 0.6907005906105042 + 50.0 * 6.272700309753418
Epoch 670, val loss: 0.9589760303497314
Epoch 680, training loss: 314.2817687988281 = 0.6781399250030518 + 50.0 * 6.272072792053223
Epoch 680, val loss: 0.9536879658699036
Epoch 690, training loss: 314.37493896484375 = 0.6657087802886963 + 50.0 * 6.274184703826904
Epoch 690, val loss: 0.9486687779426575
Epoch 700, training loss: 314.20599365234375 = 0.6534882187843323 + 50.0 * 6.271049976348877
Epoch 700, val loss: 0.943713366985321
Epoch 710, training loss: 314.1257019042969 = 0.641310453414917 + 50.0 * 6.269688129425049
Epoch 710, val loss: 0.9392619729042053
Epoch 720, training loss: 314.0172424316406 = 0.6293675303459167 + 50.0 * 6.267757415771484
Epoch 720, val loss: 0.9350199103355408
Epoch 730, training loss: 313.9398193359375 = 0.6175438761711121 + 50.0 * 6.266445159912109
Epoch 730, val loss: 0.9309691190719604
Epoch 740, training loss: 314.0752868652344 = 0.6058655977249146 + 50.0 * 6.269388198852539
Epoch 740, val loss: 0.92719966173172
Epoch 750, training loss: 314.1024169921875 = 0.5941678881645203 + 50.0 * 6.270164489746094
Epoch 750, val loss: 0.9236990809440613
Epoch 760, training loss: 313.8045959472656 = 0.5826932191848755 + 50.0 * 6.264437675476074
Epoch 760, val loss: 0.9200398921966553
Epoch 770, training loss: 313.7642822265625 = 0.5713072419166565 + 50.0 * 6.263859272003174
Epoch 770, val loss: 0.9169284701347351
Epoch 780, training loss: 313.61895751953125 = 0.5600805282592773 + 50.0 * 6.2611775398254395
Epoch 780, val loss: 0.9138875603675842
Epoch 790, training loss: 313.5285949707031 = 0.5489625930786133 + 50.0 * 6.2595930099487305
Epoch 790, val loss: 0.9111456871032715
Epoch 800, training loss: 313.52337646484375 = 0.5379977822303772 + 50.0 * 6.259707927703857
Epoch 800, val loss: 0.9086815118789673
Epoch 810, training loss: 313.7395935058594 = 0.5270768404006958 + 50.0 * 6.2642502784729
Epoch 810, val loss: 0.9062628149986267
Epoch 820, training loss: 313.4735412597656 = 0.516209065914154 + 50.0 * 6.259146690368652
Epoch 820, val loss: 0.9039154052734375
Epoch 830, training loss: 313.38330078125 = 0.5055302977561951 + 50.0 * 6.2575554847717285
Epoch 830, val loss: 0.9017975926399231
Epoch 840, training loss: 313.403076171875 = 0.49499037861824036 + 50.0 * 6.258161544799805
Epoch 840, val loss: 0.8999465107917786
Epoch 850, training loss: 313.2015380859375 = 0.4845082461833954 + 50.0 * 6.254341125488281
Epoch 850, val loss: 0.8984093070030212
Epoch 860, training loss: 313.200927734375 = 0.4741945266723633 + 50.0 * 6.25453519821167
Epoch 860, val loss: 0.8970128297805786
Epoch 870, training loss: 313.19927978515625 = 0.4639889597892761 + 50.0 * 6.254705905914307
Epoch 870, val loss: 0.895771861076355
Epoch 880, training loss: 313.467041015625 = 0.453901082277298 + 50.0 * 6.260262966156006
Epoch 880, val loss: 0.8945950865745544
Epoch 890, training loss: 313.1769104003906 = 0.4438501298427582 + 50.0 * 6.2546610832214355
Epoch 890, val loss: 0.893269956111908
Epoch 900, training loss: 312.9881896972656 = 0.43394485116004944 + 50.0 * 6.251084804534912
Epoch 900, val loss: 0.892548143863678
Epoch 910, training loss: 313.12017822265625 = 0.42420855164527893 + 50.0 * 6.25391960144043
Epoch 910, val loss: 0.891815185546875
Epoch 920, training loss: 312.9362487792969 = 0.41457706689834595 + 50.0 * 6.250433444976807
Epoch 920, val loss: 0.8912580013275146
Epoch 930, training loss: 312.8664245605469 = 0.4050908386707306 + 50.0 * 6.2492265701293945
Epoch 930, val loss: 0.8907821774482727
Epoch 940, training loss: 312.9937744140625 = 0.3957475423812866 + 50.0 * 6.251960277557373
Epoch 940, val loss: 0.8904171586036682
Epoch 950, training loss: 312.7723388671875 = 0.38642463088035583 + 50.0 * 6.247718811035156
Epoch 950, val loss: 0.8902236223220825
Epoch 960, training loss: 312.77838134765625 = 0.3772827088832855 + 50.0 * 6.248022079467773
Epoch 960, val loss: 0.8900550007820129
Epoch 970, training loss: 312.6905822753906 = 0.36829355359077454 + 50.0 * 6.246446132659912
Epoch 970, val loss: 0.889964759349823
Epoch 980, training loss: 312.6042785644531 = 0.35945531725883484 + 50.0 * 6.244896411895752
Epoch 980, val loss: 0.8901563882827759
Epoch 990, training loss: 312.731689453125 = 0.35077759623527527 + 50.0 * 6.247618198394775
Epoch 990, val loss: 0.8903466463088989
Epoch 1000, training loss: 312.8562316894531 = 0.3421720862388611 + 50.0 * 6.25028133392334
Epoch 1000, val loss: 0.8905448913574219
Epoch 1010, training loss: 312.72705078125 = 0.33364126086235046 + 50.0 * 6.247868061065674
Epoch 1010, val loss: 0.8909640312194824
Epoch 1020, training loss: 312.4899597167969 = 0.3253062963485718 + 50.0 * 6.243292808532715
Epoch 1020, val loss: 0.8913731575012207
Epoch 1030, training loss: 312.43597412109375 = 0.3171667754650116 + 50.0 * 6.24237585067749
Epoch 1030, val loss: 0.8919606804847717
Epoch 1040, training loss: 312.5018310546875 = 0.30921700596809387 + 50.0 * 6.243852615356445
Epoch 1040, val loss: 0.8926831483840942
Epoch 1050, training loss: 312.42034912109375 = 0.3013944625854492 + 50.0 * 6.242379188537598
Epoch 1050, val loss: 0.8935052752494812
Epoch 1060, training loss: 312.6014709472656 = 0.293731689453125 + 50.0 * 6.24615478515625
Epoch 1060, val loss: 0.894280195236206
Epoch 1070, training loss: 312.37176513671875 = 0.28620555996894836 + 50.0 * 6.241711139678955
Epoch 1070, val loss: 0.8948047757148743
Epoch 1080, training loss: 312.2763977050781 = 0.27890297770500183 + 50.0 * 6.239950180053711
Epoch 1080, val loss: 0.8959838151931763
Epoch 1090, training loss: 312.25555419921875 = 0.27177131175994873 + 50.0 * 6.239675998687744
Epoch 1090, val loss: 0.8969458937644958
Epoch 1100, training loss: 312.5111999511719 = 0.2648085951805115 + 50.0 * 6.244927883148193
Epoch 1100, val loss: 0.8980571031570435
Epoch 1110, training loss: 312.3250732421875 = 0.2579086124897003 + 50.0 * 6.2413434982299805
Epoch 1110, val loss: 0.8992879390716553
Epoch 1120, training loss: 312.20550537109375 = 0.2512884736061096 + 50.0 * 6.239084720611572
Epoch 1120, val loss: 0.9004707336425781
Epoch 1130, training loss: 312.09722900390625 = 0.24479034543037415 + 50.0 * 6.237049102783203
Epoch 1130, val loss: 0.9018552303314209
Epoch 1140, training loss: 312.2726135253906 = 0.23853179812431335 + 50.0 * 6.2406816482543945
Epoch 1140, val loss: 0.9031524062156677
Epoch 1150, training loss: 312.1357727050781 = 0.23238657414913177 + 50.0 * 6.238067626953125
Epoch 1150, val loss: 0.9047400951385498
Epoch 1160, training loss: 312.1026611328125 = 0.22639279067516327 + 50.0 * 6.237525463104248
Epoch 1160, val loss: 0.9061347246170044
Epoch 1170, training loss: 312.09710693359375 = 0.22056463360786438 + 50.0 * 6.2375311851501465
Epoch 1170, val loss: 0.9079890251159668
Epoch 1180, training loss: 311.9712219238281 = 0.21489939093589783 + 50.0 * 6.235126495361328
Epoch 1180, val loss: 0.9093843102455139
Epoch 1190, training loss: 311.9394226074219 = 0.20942114293575287 + 50.0 * 6.234600067138672
Epoch 1190, val loss: 0.9110797643661499
Epoch 1200, training loss: 311.8871154785156 = 0.20409676432609558 + 50.0 * 6.2336602210998535
Epoch 1200, val loss: 0.9129417538642883
Epoch 1210, training loss: 312.0487365722656 = 0.19894810020923615 + 50.0 * 6.236995697021484
Epoch 1210, val loss: 0.9149297475814819
Epoch 1220, training loss: 311.8981018066406 = 0.1938975304365158 + 50.0 * 6.234084129333496
Epoch 1220, val loss: 0.9164857864379883
Epoch 1230, training loss: 312.09735107421875 = 0.18898145854473114 + 50.0 * 6.2381672859191895
Epoch 1230, val loss: 0.918389618396759
Epoch 1240, training loss: 311.8972473144531 = 0.18421849608421326 + 50.0 * 6.2342610359191895
Epoch 1240, val loss: 0.9204201698303223
Epoch 1250, training loss: 311.7295837402344 = 0.17960214614868164 + 50.0 * 6.23099946975708
Epoch 1250, val loss: 0.9223171472549438
Epoch 1260, training loss: 311.728515625 = 0.17515896260738373 + 50.0 * 6.231067180633545
Epoch 1260, val loss: 0.9245014786720276
Epoch 1270, training loss: 311.9410095214844 = 0.17087210714817047 + 50.0 * 6.235402584075928
Epoch 1270, val loss: 0.926613986492157
Epoch 1280, training loss: 311.67138671875 = 0.1666148453950882 + 50.0 * 6.230094909667969
Epoch 1280, val loss: 0.9288263320922852
Epoch 1290, training loss: 311.6403503417969 = 0.16252657771110535 + 50.0 * 6.229556083679199
Epoch 1290, val loss: 0.9309702515602112
Epoch 1300, training loss: 311.8551940917969 = 0.15858635306358337 + 50.0 * 6.233932018280029
Epoch 1300, val loss: 0.9333965182304382
Epoch 1310, training loss: 311.6742248535156 = 0.1546812206506729 + 50.0 * 6.230390548706055
Epoch 1310, val loss: 0.9352487325668335
Epoch 1320, training loss: 311.6184387207031 = 0.15093384683132172 + 50.0 * 6.2293500900268555
Epoch 1320, val loss: 0.9376391768455505
Epoch 1330, training loss: 311.5672607421875 = 0.14729560911655426 + 50.0 * 6.22839879989624
Epoch 1330, val loss: 0.9400381445884705
Epoch 1340, training loss: 311.6387023925781 = 0.14378240704536438 + 50.0 * 6.229898929595947
Epoch 1340, val loss: 0.9424371123313904
Epoch 1350, training loss: 311.6523132324219 = 0.14033809304237366 + 50.0 * 6.230239391326904
Epoch 1350, val loss: 0.9450483322143555
Epoch 1360, training loss: 311.5652770996094 = 0.1369592696428299 + 50.0 * 6.2285661697387695
Epoch 1360, val loss: 0.9474003911018372
Epoch 1370, training loss: 311.4938049316406 = 0.13370636105537415 + 50.0 * 6.22720193862915
Epoch 1370, val loss: 0.9499214291572571
Epoch 1380, training loss: 311.4517517089844 = 0.13056182861328125 + 50.0 * 6.226423740386963
Epoch 1380, val loss: 0.9524766802787781
Epoch 1390, training loss: 311.56195068359375 = 0.1275150179862976 + 50.0 * 6.228688716888428
Epoch 1390, val loss: 0.9550699591636658
Epoch 1400, training loss: 311.5984802246094 = 0.12451884150505066 + 50.0 * 6.2294793128967285
Epoch 1400, val loss: 0.9577591419219971
Epoch 1410, training loss: 311.6188049316406 = 0.12159073352813721 + 50.0 * 6.229944229125977
Epoch 1410, val loss: 0.9605260491371155
Epoch 1420, training loss: 311.4246520996094 = 0.11874519288539886 + 50.0 * 6.226118087768555
Epoch 1420, val loss: 0.9627452492713928
Epoch 1430, training loss: 311.3583068847656 = 0.11599824577569962 + 50.0 * 6.224845886230469
Epoch 1430, val loss: 0.9655017852783203
Epoch 1440, training loss: 311.3103942871094 = 0.1133432686328888 + 50.0 * 6.223940849304199
Epoch 1440, val loss: 0.9683112502098083
Epoch 1450, training loss: 311.4222717285156 = 0.11075734347105026 + 50.0 * 6.226230621337891
Epoch 1450, val loss: 0.9709880948066711
Epoch 1460, training loss: 311.3143615722656 = 0.10820957273244858 + 50.0 * 6.224123001098633
Epoch 1460, val loss: 0.9736202955245972
Epoch 1470, training loss: 311.3140563964844 = 0.10574571788311005 + 50.0 * 6.224166393280029
Epoch 1470, val loss: 0.9763447046279907
Epoch 1480, training loss: 311.79156494140625 = 0.10335294157266617 + 50.0 * 6.233764171600342
Epoch 1480, val loss: 0.9788784384727478
Epoch 1490, training loss: 311.3594970703125 = 0.10095766931772232 + 50.0 * 6.225170612335205
Epoch 1490, val loss: 0.9818866848945618
Epoch 1500, training loss: 311.2249755859375 = 0.0986749529838562 + 50.0 * 6.2225260734558105
Epoch 1500, val loss: 0.9843960404396057
Epoch 1510, training loss: 311.1790466308594 = 0.09646447747945786 + 50.0 * 6.221651554107666
Epoch 1510, val loss: 0.98731929063797
Epoch 1520, training loss: 311.1932678222656 = 0.09431149810552597 + 50.0 * 6.22197961807251
Epoch 1520, val loss: 0.9900346398353577
Epoch 1530, training loss: 311.6952819824219 = 0.09219496697187424 + 50.0 * 6.232061862945557
Epoch 1530, val loss: 0.9925591349601746
Epoch 1540, training loss: 311.14776611328125 = 0.0900922492146492 + 50.0 * 6.221153259277344
Epoch 1540, val loss: 0.9954851865768433
Epoch 1550, training loss: 311.1449279785156 = 0.08806940913200378 + 50.0 * 6.221137046813965
Epoch 1550, val loss: 0.9982424974441528
Epoch 1560, training loss: 311.1034240722656 = 0.08611820638179779 + 50.0 * 6.220345973968506
Epoch 1560, val loss: 1.0010364055633545
Epoch 1570, training loss: 311.08074951171875 = 0.08423315733671188 + 50.0 * 6.219930648803711
Epoch 1570, val loss: 1.0039722919464111
Epoch 1580, training loss: 311.6026306152344 = 0.08239715546369553 + 50.0 * 6.230404376983643
Epoch 1580, val loss: 1.006737470626831
Epoch 1590, training loss: 311.25042724609375 = 0.08052853494882584 + 50.0 * 6.223397731781006
Epoch 1590, val loss: 1.0095163583755493
Epoch 1600, training loss: 311.08990478515625 = 0.07874133437871933 + 50.0 * 6.220223426818848
Epoch 1600, val loss: 1.0125449895858765
Epoch 1610, training loss: 311.0841979980469 = 0.0770070031285286 + 50.0 * 6.220144271850586
Epoch 1610, val loss: 1.015313982963562
Epoch 1620, training loss: 311.02532958984375 = 0.07531857490539551 + 50.0 * 6.219000339508057
Epoch 1620, val loss: 1.018264889717102
Epoch 1630, training loss: 311.075439453125 = 0.07368622720241547 + 50.0 * 6.220034599304199
Epoch 1630, val loss: 1.0212461948394775
Epoch 1640, training loss: 311.1483459472656 = 0.07207490503787994 + 50.0 * 6.221525192260742
Epoch 1640, val loss: 1.0240000486373901
Epoch 1650, training loss: 311.0515441894531 = 0.07048533111810684 + 50.0 * 6.219621181488037
Epoch 1650, val loss: 1.0267291069030762
Epoch 1660, training loss: 311.06842041015625 = 0.06895533204078674 + 50.0 * 6.219988822937012
Epoch 1660, val loss: 1.0297660827636719
Epoch 1670, training loss: 311.0763244628906 = 0.06745973974466324 + 50.0 * 6.220177173614502
Epoch 1670, val loss: 1.0324881076812744
Epoch 1680, training loss: 311.0882263183594 = 0.06598620116710663 + 50.0 * 6.220444679260254
Epoch 1680, val loss: 1.0352939367294312
Epoch 1690, training loss: 310.92352294921875 = 0.0645587369799614 + 50.0 * 6.217179775238037
Epoch 1690, val loss: 1.0385104417800903
Epoch 1700, training loss: 310.8902893066406 = 0.0631772056221962 + 50.0 * 6.2165422439575195
Epoch 1700, val loss: 1.0413399934768677
Epoch 1710, training loss: 310.9808654785156 = 0.061831969767808914 + 50.0 * 6.218380451202393
Epoch 1710, val loss: 1.0442014932632446
Epoch 1720, training loss: 311.0584411621094 = 0.06051143631339073 + 50.0 * 6.219958782196045
Epoch 1720, val loss: 1.0470701456069946
Epoch 1730, training loss: 310.9880065917969 = 0.059213340282440186 + 50.0 * 6.218575954437256
Epoch 1730, val loss: 1.0499268770217896
Epoch 1740, training loss: 310.8273620605469 = 0.05794162303209305 + 50.0 * 6.215388298034668
Epoch 1740, val loss: 1.0528745651245117
Epoch 1750, training loss: 310.9376220703125 = 0.05672996863722801 + 50.0 * 6.217617511749268
Epoch 1750, val loss: 1.0558497905731201
Epoch 1760, training loss: 310.8804931640625 = 0.05552499741315842 + 50.0 * 6.2164998054504395
Epoch 1760, val loss: 1.058637022972107
Epoch 1770, training loss: 310.7984619140625 = 0.05434468761086464 + 50.0 * 6.2148823738098145
Epoch 1770, val loss: 1.0615428686141968
Epoch 1780, training loss: 310.7964782714844 = 0.05321398749947548 + 50.0 * 6.214865207672119
Epoch 1780, val loss: 1.064582347869873
Epoch 1790, training loss: 310.82513427734375 = 0.052114591002464294 + 50.0 * 6.215460300445557
Epoch 1790, val loss: 1.0672963857650757
Epoch 1800, training loss: 311.00152587890625 = 0.05104229226708412 + 50.0 * 6.2190093994140625
Epoch 1800, val loss: 1.0700812339782715
Epoch 1810, training loss: 310.9389343261719 = 0.04998240992426872 + 50.0 * 6.217779159545898
Epoch 1810, val loss: 1.0730574131011963
Epoch 1820, training loss: 310.7525329589844 = 0.04893745854496956 + 50.0 * 6.214071750640869
Epoch 1820, val loss: 1.0759254693984985
Epoch 1830, training loss: 310.70782470703125 = 0.047942183911800385 + 50.0 * 6.213197708129883
Epoch 1830, val loss: 1.0789610147476196
Epoch 1840, training loss: 310.90277099609375 = 0.0469837449491024 + 50.0 * 6.21711540222168
Epoch 1840, val loss: 1.0817739963531494
Epoch 1850, training loss: 310.8436279296875 = 0.046020057052373886 + 50.0 * 6.215952396392822
Epoch 1850, val loss: 1.0842936038970947
Epoch 1860, training loss: 310.69622802734375 = 0.04508134722709656 + 50.0 * 6.2130231857299805
Epoch 1860, val loss: 1.0872447490692139
Epoch 1870, training loss: 310.7083435058594 = 0.04417634755373001 + 50.0 * 6.213283061981201
Epoch 1870, val loss: 1.0898914337158203
Epoch 1880, training loss: 310.8186950683594 = 0.04329923167824745 + 50.0 * 6.215507984161377
Epoch 1880, val loss: 1.092651128768921
Epoch 1890, training loss: 310.6775207519531 = 0.04243820160627365 + 50.0 * 6.212701320648193
Epoch 1890, val loss: 1.0956305265426636
Epoch 1900, training loss: 310.7154846191406 = 0.041608817875385284 + 50.0 * 6.213477611541748
Epoch 1900, val loss: 1.0983871221542358
Epoch 1910, training loss: 310.9537658691406 = 0.040794484317302704 + 50.0 * 6.218259334564209
Epoch 1910, val loss: 1.101081371307373
Epoch 1920, training loss: 310.6788024902344 = 0.039983730763196945 + 50.0 * 6.2127766609191895
Epoch 1920, val loss: 1.1039925813674927
Epoch 1930, training loss: 310.6155090332031 = 0.039212729781866074 + 50.0 * 6.211525917053223
Epoch 1930, val loss: 1.1068165302276611
Epoch 1940, training loss: 310.98486328125 = 0.038474757224321365 + 50.0 * 6.218927383422852
Epoch 1940, val loss: 1.1097748279571533
Epoch 1950, training loss: 310.7049255371094 = 0.03770968317985535 + 50.0 * 6.213344097137451
Epoch 1950, val loss: 1.1119043827056885
Epoch 1960, training loss: 310.593505859375 = 0.036991626024246216 + 50.0 * 6.211130619049072
Epoch 1960, val loss: 1.1150987148284912
Epoch 1970, training loss: 310.55511474609375 = 0.03629673644900322 + 50.0 * 6.210376739501953
Epoch 1970, val loss: 1.1176331043243408
Epoch 1980, training loss: 310.6643371582031 = 0.03562897816300392 + 50.0 * 6.212574481964111
Epoch 1980, val loss: 1.1206923723220825
Epoch 1990, training loss: 310.6907958984375 = 0.034955840557813644 + 50.0 * 6.2131171226501465
Epoch 1990, val loss: 1.1231058835983276
Epoch 2000, training loss: 310.5813293457031 = 0.03429647535085678 + 50.0 * 6.210940361022949
Epoch 2000, val loss: 1.125592589378357
Epoch 2010, training loss: 310.5089416503906 = 0.033655088394880295 + 50.0 * 6.209505558013916
Epoch 2010, val loss: 1.1281256675720215
Epoch 2020, training loss: 310.503662109375 = 0.0330454558134079 + 50.0 * 6.209412097930908
Epoch 2020, val loss: 1.1308958530426025
Epoch 2030, training loss: 310.63092041015625 = 0.03245284780859947 + 50.0 * 6.211969375610352
Epoch 2030, val loss: 1.133450984954834
Epoch 2040, training loss: 310.6214599609375 = 0.03186213970184326 + 50.0 * 6.2117919921875
Epoch 2040, val loss: 1.1362533569335938
Epoch 2050, training loss: 310.4953918457031 = 0.03127678111195564 + 50.0 * 6.209281921386719
Epoch 2050, val loss: 1.138617753982544
Epoch 2060, training loss: 310.47052001953125 = 0.030714990571141243 + 50.0 * 6.208796501159668
Epoch 2060, val loss: 1.1414309740066528
Epoch 2070, training loss: 310.54827880859375 = 0.030177872627973557 + 50.0 * 6.210361957550049
Epoch 2070, val loss: 1.1438939571380615
Epoch 2080, training loss: 310.6005859375 = 0.029646294191479683 + 50.0 * 6.211419105529785
Epoch 2080, val loss: 1.1467456817626953
Epoch 2090, training loss: 310.51556396484375 = 0.0291264858096838 + 50.0 * 6.209728717803955
Epoch 2090, val loss: 1.1490226984024048
Epoch 2100, training loss: 310.4715270996094 = 0.028617504984140396 + 50.0 * 6.208858013153076
Epoch 2100, val loss: 1.151513934135437
Epoch 2110, training loss: 310.4188537597656 = 0.028124770149588585 + 50.0 * 6.2078142166137695
Epoch 2110, val loss: 1.1541192531585693
Epoch 2120, training loss: 310.5752258300781 = 0.02765035629272461 + 50.0 * 6.210951805114746
Epoch 2120, val loss: 1.156566858291626
Epoch 2130, training loss: 310.5278015136719 = 0.027175260707736015 + 50.0 * 6.210012912750244
Epoch 2130, val loss: 1.159000039100647
Epoch 2140, training loss: 310.6185302734375 = 0.026705684140324593 + 50.0 * 6.211836814880371
Epoch 2140, val loss: 1.1614047288894653
Epoch 2150, training loss: 310.5010070800781 = 0.026244837790727615 + 50.0 * 6.2094950675964355
Epoch 2150, val loss: 1.1636210680007935
Epoch 2160, training loss: 310.3797912597656 = 0.025804491713643074 + 50.0 * 6.2070794105529785
Epoch 2160, val loss: 1.166252613067627
Epoch 2170, training loss: 310.36114501953125 = 0.025378981605172157 + 50.0 * 6.206715106964111
Epoch 2170, val loss: 1.1688200235366821
Epoch 2180, training loss: 310.3808288574219 = 0.024966981261968613 + 50.0 * 6.207117557525635
Epoch 2180, val loss: 1.1712392568588257
Epoch 2190, training loss: 310.699951171875 = 0.02456606738269329 + 50.0 * 6.213507652282715
Epoch 2190, val loss: 1.1739341020584106
Epoch 2200, training loss: 310.4493408203125 = 0.024156732484698296 + 50.0 * 6.208503246307373
Epoch 2200, val loss: 1.1756913661956787
Epoch 2210, training loss: 310.4345703125 = 0.023765286430716515 + 50.0 * 6.208216190338135
Epoch 2210, val loss: 1.178152322769165
Epoch 2220, training loss: 310.4601135253906 = 0.023381546139717102 + 50.0 * 6.208734512329102
Epoch 2220, val loss: 1.1805192232131958
Epoch 2230, training loss: 310.31866455078125 = 0.023004136979579926 + 50.0 * 6.205913066864014
Epoch 2230, val loss: 1.1828681230545044
Epoch 2240, training loss: 310.4900207519531 = 0.022650763392448425 + 50.0 * 6.209347248077393
Epoch 2240, val loss: 1.1854044198989868
Epoch 2250, training loss: 310.42303466796875 = 0.022291136905550957 + 50.0 * 6.208014965057373
Epoch 2250, val loss: 1.1874722242355347
Epoch 2260, training loss: 310.29901123046875 = 0.021939147263765335 + 50.0 * 6.205541133880615
Epoch 2260, val loss: 1.1898019313812256
Epoch 2270, training loss: 310.3296203613281 = 0.021601777523756027 + 50.0 * 6.206160068511963
Epoch 2270, val loss: 1.1920772790908813
Epoch 2280, training loss: 310.35284423828125 = 0.02127387560904026 + 50.0 * 6.206631660461426
Epoch 2280, val loss: 1.1944726705551147
Epoch 2290, training loss: 310.44268798828125 = 0.020950468257069588 + 50.0 * 6.208434581756592
Epoch 2290, val loss: 1.1968324184417725
Epoch 2300, training loss: 310.4248962402344 = 0.02062758430838585 + 50.0 * 6.208085060119629
Epoch 2300, val loss: 1.1988950967788696
Epoch 2310, training loss: 310.32904052734375 = 0.020312171429395676 + 50.0 * 6.206174373626709
Epoch 2310, val loss: 1.2011653184890747
Epoch 2320, training loss: 310.5614013671875 = 0.02001218870282173 + 50.0 * 6.210827827453613
Epoch 2320, val loss: 1.2034786939620972
Epoch 2330, training loss: 310.28643798828125 = 0.019709832966327667 + 50.0 * 6.205334186553955
Epoch 2330, val loss: 1.2056277990341187
Epoch 2340, training loss: 310.26593017578125 = 0.019422655925154686 + 50.0 * 6.204930305480957
Epoch 2340, val loss: 1.2077440023422241
Epoch 2350, training loss: 310.345458984375 = 0.01914280466735363 + 50.0 * 6.206526279449463
Epoch 2350, val loss: 1.2101703882217407
Epoch 2360, training loss: 310.3603820800781 = 0.01886862702667713 + 50.0 * 6.2068305015563965
Epoch 2360, val loss: 1.2123641967773438
Epoch 2370, training loss: 310.2268981933594 = 0.018590910360217094 + 50.0 * 6.204166412353516
Epoch 2370, val loss: 1.2144200801849365
Epoch 2380, training loss: 310.2615051269531 = 0.01833054982125759 + 50.0 * 6.20486307144165
Epoch 2380, val loss: 1.2166353464126587
Epoch 2390, training loss: 310.42059326171875 = 0.018074881285429 + 50.0 * 6.20805025100708
Epoch 2390, val loss: 1.218867301940918
Epoch 2400, training loss: 310.29412841796875 = 0.01781620644032955 + 50.0 * 6.205525875091553
Epoch 2400, val loss: 1.2205854654312134
Epoch 2410, training loss: 310.2964172363281 = 0.01756926253437996 + 50.0 * 6.2055768966674805
Epoch 2410, val loss: 1.222801923751831
Epoch 2420, training loss: 310.2138366699219 = 0.01732277311384678 + 50.0 * 6.203929901123047
Epoch 2420, val loss: 1.2248284816741943
Epoch 2430, training loss: 310.2431640625 = 0.017087405547499657 + 50.0 * 6.204521179199219
Epoch 2430, val loss: 1.2271809577941895
Epoch 2440, training loss: 310.2722473144531 = 0.016858289018273354 + 50.0 * 6.205108165740967
Epoch 2440, val loss: 1.229229211807251
Epoch 2450, training loss: 310.3529968261719 = 0.016628598794341087 + 50.0 * 6.206727504730225
Epoch 2450, val loss: 1.2315682172775269
Epoch 2460, training loss: 310.1485290527344 = 0.016394684091210365 + 50.0 * 6.202642917633057
Epoch 2460, val loss: 1.233324408531189
Epoch 2470, training loss: 310.13214111328125 = 0.01617819257080555 + 50.0 * 6.202319145202637
Epoch 2470, val loss: 1.2354053258895874
Epoch 2480, training loss: 310.258544921875 = 0.015972701832652092 + 50.0 * 6.2048516273498535
Epoch 2480, val loss: 1.2377827167510986
Epoch 2490, training loss: 310.24566650390625 = 0.015758350491523743 + 50.0 * 6.204598426818848
Epoch 2490, val loss: 1.2394993305206299
Epoch 2500, training loss: 310.2437438964844 = 0.015549282543361187 + 50.0 * 6.204563617706299
Epoch 2500, val loss: 1.2415624856948853
Epoch 2510, training loss: 310.1645202636719 = 0.01534885074943304 + 50.0 * 6.202983379364014
Epoch 2510, val loss: 1.2435041666030884
Epoch 2520, training loss: 310.1134033203125 = 0.01514910813421011 + 50.0 * 6.20196533203125
Epoch 2520, val loss: 1.2454469203948975
Epoch 2530, training loss: 310.1686706542969 = 0.014964073896408081 + 50.0 * 6.2030744552612305
Epoch 2530, val loss: 1.2474725246429443
Epoch 2540, training loss: 310.3996887207031 = 0.01477677933871746 + 50.0 * 6.207698345184326
Epoch 2540, val loss: 1.2495126724243164
Epoch 2550, training loss: 310.2031555175781 = 0.014579401351511478 + 50.0 * 6.203771114349365
Epoch 2550, val loss: 1.2516018152236938
Epoch 2560, training loss: 310.0809020996094 = 0.014397695660591125 + 50.0 * 6.201330184936523
Epoch 2560, val loss: 1.2533471584320068
Epoch 2570, training loss: 310.0704650878906 = 0.014221112243831158 + 50.0 * 6.201124668121338
Epoch 2570, val loss: 1.2553672790527344
Epoch 2580, training loss: 310.27825927734375 = 0.014052238315343857 + 50.0 * 6.205284118652344
Epoch 2580, val loss: 1.2572845220565796
Epoch 2590, training loss: 310.1138610839844 = 0.013877726159989834 + 50.0 * 6.201999664306641
Epoch 2590, val loss: 1.2594677209854126
Epoch 2600, training loss: 310.1933288574219 = 0.013708516024053097 + 50.0 * 6.203592777252197
Epoch 2600, val loss: 1.2614015340805054
Epoch 2610, training loss: 310.0639953613281 = 0.013540918938815594 + 50.0 * 6.2010087966918945
Epoch 2610, val loss: 1.2630865573883057
Epoch 2620, training loss: 310.1351623535156 = 0.01337865088135004 + 50.0 * 6.2024359703063965
Epoch 2620, val loss: 1.2648991346359253
Epoch 2630, training loss: 310.0291442871094 = 0.01322091929614544 + 50.0 * 6.200318336486816
Epoch 2630, val loss: 1.2668945789337158
Epoch 2640, training loss: 310.0606384277344 = 0.013067787513136864 + 50.0 * 6.20095157623291
Epoch 2640, val loss: 1.2689567804336548
Epoch 2650, training loss: 310.20050048828125 = 0.012919960543513298 + 50.0 * 6.203751564025879
Epoch 2650, val loss: 1.270731806755066
Epoch 2660, training loss: 310.1853332519531 = 0.012765159830451012 + 50.0 * 6.203451633453369
Epoch 2660, val loss: 1.2723222970962524
Epoch 2670, training loss: 310.1833801269531 = 0.012612193822860718 + 50.0 * 6.203415393829346
Epoch 2670, val loss: 1.274159550666809
Epoch 2680, training loss: 310.0044250488281 = 0.012464911676943302 + 50.0 * 6.1998395919799805
Epoch 2680, val loss: 1.2760415077209473
Epoch 2690, training loss: 310.0673828125 = 0.012326495721936226 + 50.0 * 6.201101303100586
Epoch 2690, val loss: 1.277942419052124
Epoch 2700, training loss: 310.21551513671875 = 0.01218970026820898 + 50.0 * 6.204066753387451
Epoch 2700, val loss: 1.2795737981796265
Epoch 2710, training loss: 310.0184326171875 = 0.012047172524034977 + 50.0 * 6.200127601623535
Epoch 2710, val loss: 1.2815524339675903
Epoch 2720, training loss: 309.97149658203125 = 0.011913910508155823 + 50.0 * 6.199192047119141
Epoch 2720, val loss: 1.2835088968276978
Epoch 2730, training loss: 309.9833679199219 = 0.011785835027694702 + 50.0 * 6.199431896209717
Epoch 2730, val loss: 1.2853314876556396
Epoch 2740, training loss: 310.18255615234375 = 0.01166114117950201 + 50.0 * 6.203417778015137
Epoch 2740, val loss: 1.2872850894927979
Epoch 2750, training loss: 310.0713806152344 = 0.011528418399393559 + 50.0 * 6.201197147369385
Epoch 2750, val loss: 1.2885103225708008
Epoch 2760, training loss: 309.9927978515625 = 0.011399324983358383 + 50.0 * 6.19962739944458
Epoch 2760, val loss: 1.2904025316238403
Epoch 2770, training loss: 310.0178527832031 = 0.011275731027126312 + 50.0 * 6.200131893157959
Epoch 2770, val loss: 1.292259931564331
Epoch 2780, training loss: 310.0542907714844 = 0.011154104955494404 + 50.0 * 6.200862884521484
Epoch 2780, val loss: 1.293977975845337
Epoch 2790, training loss: 309.9371643066406 = 0.011038276366889477 + 50.0 * 6.198522090911865
Epoch 2790, val loss: 1.2956585884094238
Epoch 2800, training loss: 309.969482421875 = 0.010924015194177628 + 50.0 * 6.19917106628418
Epoch 2800, val loss: 1.2974283695220947
Epoch 2810, training loss: 310.0537414550781 = 0.010811748914420605 + 50.0 * 6.2008585929870605
Epoch 2810, val loss: 1.299345850944519
Epoch 2820, training loss: 310.0669860839844 = 0.010698070749640465 + 50.0 * 6.201125621795654
Epoch 2820, val loss: 1.3009532690048218
Epoch 2830, training loss: 309.9735412597656 = 0.01058797724545002 + 50.0 * 6.199258804321289
Epoch 2830, val loss: 1.3024311065673828
Epoch 2840, training loss: 310.0381164550781 = 0.010477420873939991 + 50.0 * 6.200552940368652
Epoch 2840, val loss: 1.3040313720703125
Epoch 2850, training loss: 310.05279541015625 = 0.010368764400482178 + 50.0 * 6.20084810256958
Epoch 2850, val loss: 1.3058552742004395
Epoch 2860, training loss: 309.9050598144531 = 0.010260977782309055 + 50.0 * 6.1978960037231445
Epoch 2860, val loss: 1.3073623180389404
Epoch 2870, training loss: 309.87646484375 = 0.010159742087125778 + 50.0 * 6.197326183319092
Epoch 2870, val loss: 1.30910062789917
Epoch 2880, training loss: 309.92431640625 = 0.010063054971396923 + 50.0 * 6.198285102844238
Epoch 2880, val loss: 1.3109699487686157
Epoch 2890, training loss: 310.2003479003906 = 0.00996778067201376 + 50.0 * 6.203807353973389
Epoch 2890, val loss: 1.3125865459442139
Epoch 2900, training loss: 310.012939453125 = 0.009862414561212063 + 50.0 * 6.200061798095703
Epoch 2900, val loss: 1.3141119480133057
Epoch 2910, training loss: 309.8888244628906 = 0.009761875495314598 + 50.0 * 6.1975812911987305
Epoch 2910, val loss: 1.315643072128296
Epoch 2920, training loss: 310.00445556640625 = 0.00966882798820734 + 50.0 * 6.19989538192749
Epoch 2920, val loss: 1.3174853324890137
Epoch 2930, training loss: 309.898681640625 = 0.009573682211339474 + 50.0 * 6.197782039642334
Epoch 2930, val loss: 1.318954348564148
Epoch 2940, training loss: 309.8703308105469 = 0.00948181003332138 + 50.0 * 6.197216987609863
Epoch 2940, val loss: 1.3202720880508423
Epoch 2950, training loss: 309.83807373046875 = 0.009394418448209763 + 50.0 * 6.196573257446289
Epoch 2950, val loss: 1.3220040798187256
Epoch 2960, training loss: 310.1432800292969 = 0.00931180827319622 + 50.0 * 6.20267915725708
Epoch 2960, val loss: 1.3235353231430054
Epoch 2970, training loss: 309.87054443359375 = 0.009217618964612484 + 50.0 * 6.197226524353027
Epoch 2970, val loss: 1.3251694440841675
Epoch 2980, training loss: 309.80316162109375 = 0.00912911631166935 + 50.0 * 6.195880889892578
Epoch 2980, val loss: 1.3266538381576538
Epoch 2990, training loss: 309.8298645019531 = 0.009047304280102253 + 50.0 * 6.19641637802124
Epoch 2990, val loss: 1.3281886577606201
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 431.8056335449219 = 1.963087797164917 + 50.0 * 8.596851348876953
Epoch 0, val loss: 1.9539545774459839
Epoch 10, training loss: 431.7519836425781 = 1.953236699104309 + 50.0 * 8.595974922180176
Epoch 10, val loss: 1.9449026584625244
Epoch 20, training loss: 431.37982177734375 = 1.9414790868759155 + 50.0 * 8.588767051696777
Epoch 20, val loss: 1.9341554641723633
Epoch 30, training loss: 428.728515625 = 1.9265103340148926 + 50.0 * 8.536040306091309
Epoch 30, val loss: 1.9203602075576782
Epoch 40, training loss: 410.35491943359375 = 1.9076584577560425 + 50.0 * 8.1689453125
Epoch 40, val loss: 1.9030643701553345
Epoch 50, training loss: 377.1596984863281 = 1.8861459493637085 + 50.0 * 7.505471229553223
Epoch 50, val loss: 1.8844900131225586
Epoch 60, training loss: 365.6499938964844 = 1.871147632598877 + 50.0 * 7.275577068328857
Epoch 60, val loss: 1.8709734678268433
Epoch 70, training loss: 354.6777038574219 = 1.8567339181900024 + 50.0 * 7.056419849395752
Epoch 70, val loss: 1.857926607131958
Epoch 80, training loss: 347.4460754394531 = 1.843197226524353 + 50.0 * 6.912057399749756
Epoch 80, val loss: 1.8459713459014893
Epoch 90, training loss: 343.77008056640625 = 1.829590082168579 + 50.0 * 6.838809967041016
Epoch 90, val loss: 1.8336338996887207
Epoch 100, training loss: 340.55316162109375 = 1.8163278102874756 + 50.0 * 6.7747368812561035
Epoch 100, val loss: 1.8216073513031006
Epoch 110, training loss: 338.2286376953125 = 1.8041636943817139 + 50.0 * 6.728489398956299
Epoch 110, val loss: 1.810333013534546
Epoch 120, training loss: 335.8217468261719 = 1.7925001382827759 + 50.0 * 6.68058443069458
Epoch 120, val loss: 1.799499273300171
Epoch 130, training loss: 333.4815673828125 = 1.7816839218139648 + 50.0 * 6.633997917175293
Epoch 130, val loss: 1.7892814874649048
Epoch 140, training loss: 331.58447265625 = 1.7708877325057983 + 50.0 * 6.596271991729736
Epoch 140, val loss: 1.7790979146957397
Epoch 150, training loss: 330.1148376464844 = 1.7595984935760498 + 50.0 * 6.567104339599609
Epoch 150, val loss: 1.7683908939361572
Epoch 160, training loss: 328.8366394042969 = 1.7474939823150635 + 50.0 * 6.541782855987549
Epoch 160, val loss: 1.7570632696151733
Epoch 170, training loss: 327.80712890625 = 1.7343817949295044 + 50.0 * 6.521454811096191
Epoch 170, val loss: 1.7448976039886475
Epoch 180, training loss: 326.77947998046875 = 1.7202692031860352 + 50.0 * 6.501183986663818
Epoch 180, val loss: 1.731919288635254
Epoch 190, training loss: 325.89794921875 = 1.705199122428894 + 50.0 * 6.4838547706604
Epoch 190, val loss: 1.7180372476577759
Epoch 200, training loss: 324.9825439453125 = 1.6891450881958008 + 50.0 * 6.46586799621582
Epoch 200, val loss: 1.7032830715179443
Epoch 210, training loss: 324.229736328125 = 1.6719000339508057 + 50.0 * 6.4511566162109375
Epoch 210, val loss: 1.687502145767212
Epoch 220, training loss: 323.68084716796875 = 1.653327226638794 + 50.0 * 6.440550804138184
Epoch 220, val loss: 1.6704843044281006
Epoch 230, training loss: 322.92803955078125 = 1.633349061012268 + 50.0 * 6.425893783569336
Epoch 230, val loss: 1.6522654294967651
Epoch 240, training loss: 322.4149169921875 = 1.6122589111328125 + 50.0 * 6.416053771972656
Epoch 240, val loss: 1.6329644918441772
Epoch 250, training loss: 321.9967041015625 = 1.5899819135665894 + 50.0 * 6.4081339836120605
Epoch 250, val loss: 1.6126216650009155
Epoch 260, training loss: 321.57318115234375 = 1.5665233135223389 + 50.0 * 6.40013313293457
Epoch 260, val loss: 1.5913140773773193
Epoch 270, training loss: 321.1404724121094 = 1.542096495628357 + 50.0 * 6.3919677734375
Epoch 270, val loss: 1.5691255331039429
Epoch 280, training loss: 320.8040771484375 = 1.5169224739074707 + 50.0 * 6.385742664337158
Epoch 280, val loss: 1.5462993383407593
Epoch 290, training loss: 320.5301513671875 = 1.4908063411712646 + 50.0 * 6.380786895751953
Epoch 290, val loss: 1.5230178833007812
Epoch 300, training loss: 320.1466064453125 = 1.4642045497894287 + 50.0 * 6.373648166656494
Epoch 300, val loss: 1.4992684125900269
Epoch 310, training loss: 319.81243896484375 = 1.4371271133422852 + 50.0 * 6.36750602722168
Epoch 310, val loss: 1.4752026796340942
Epoch 320, training loss: 320.14373779296875 = 1.4096477031707764 + 50.0 * 6.3746819496154785
Epoch 320, val loss: 1.4510140419006348
Epoch 330, training loss: 319.44329833984375 = 1.381579875946045 + 50.0 * 6.361234188079834
Epoch 330, val loss: 1.4264997243881226
Epoch 340, training loss: 319.0559387207031 = 1.3535175323486328 + 50.0 * 6.354048728942871
Epoch 340, val loss: 1.4023617506027222
Epoch 350, training loss: 318.7738037109375 = 1.3253352642059326 + 50.0 * 6.348968982696533
Epoch 350, val loss: 1.3781863451004028
Epoch 360, training loss: 318.6896057128906 = 1.297067642211914 + 50.0 * 6.347850799560547
Epoch 360, val loss: 1.3539692163467407
Epoch 370, training loss: 318.5650939941406 = 1.268604040145874 + 50.0 * 6.3459296226501465
Epoch 370, val loss: 1.3301054239273071
Epoch 380, training loss: 318.1764831542969 = 1.2402950525283813 + 50.0 * 6.338723659515381
Epoch 380, val loss: 1.3062514066696167
Epoch 390, training loss: 317.9400329589844 = 1.2120380401611328 + 50.0 * 6.334559917449951
Epoch 390, val loss: 1.2826294898986816
Epoch 400, training loss: 317.7262878417969 = 1.1839460134506226 + 50.0 * 6.330846309661865
Epoch 400, val loss: 1.2592734098434448
Epoch 410, training loss: 318.1604919433594 = 1.1559215784072876 + 50.0 * 6.340091705322266
Epoch 410, val loss: 1.2360305786132812
Epoch 420, training loss: 317.5656433105469 = 1.127998948097229 + 50.0 * 6.3287529945373535
Epoch 420, val loss: 1.2130557298660278
Epoch 430, training loss: 317.5803527832031 = 1.1004831790924072 + 50.0 * 6.329597473144531
Epoch 430, val loss: 1.1904289722442627
Epoch 440, training loss: 317.19647216796875 = 1.0733357667922974 + 50.0 * 6.322462558746338
Epoch 440, val loss: 1.1684759855270386
Epoch 450, training loss: 316.9442443847656 = 1.0465933084487915 + 50.0 * 6.317952632904053
Epoch 450, val loss: 1.1468664407730103
Epoch 460, training loss: 316.75421142578125 = 1.0204036235809326 + 50.0 * 6.314675807952881
Epoch 460, val loss: 1.1258589029312134
Epoch 470, training loss: 316.8141784667969 = 0.9946926236152649 + 50.0 * 6.316390037536621
Epoch 470, val loss: 1.1053980588912964
Epoch 480, training loss: 316.6531677246094 = 0.9692192077636719 + 50.0 * 6.313679218292236
Epoch 480, val loss: 1.0854488611221313
Epoch 490, training loss: 316.3399658203125 = 0.9446249604225159 + 50.0 * 6.3079071044921875
Epoch 490, val loss: 1.0662956237792969
Epoch 500, training loss: 316.22576904296875 = 0.9205938577651978 + 50.0 * 6.306103706359863
Epoch 500, val loss: 1.047716498374939
Epoch 510, training loss: 316.100830078125 = 0.8971783518791199 + 50.0 * 6.304072856903076
Epoch 510, val loss: 1.030025839805603
Epoch 520, training loss: 316.10565185546875 = 0.8743705153465271 + 50.0 * 6.304625988006592
Epoch 520, val loss: 1.0130776166915894
Epoch 530, training loss: 316.0079650878906 = 0.8520783185958862 + 50.0 * 6.303117752075195
Epoch 530, val loss: 0.9965892434120178
Epoch 540, training loss: 315.744384765625 = 0.830463707447052 + 50.0 * 6.298278331756592
Epoch 540, val loss: 0.9810197353363037
Epoch 550, training loss: 315.5942077636719 = 0.8094688653945923 + 50.0 * 6.295694828033447
Epoch 550, val loss: 0.9662489295005798
Epoch 560, training loss: 315.4649658203125 = 0.7891402244567871 + 50.0 * 6.293516159057617
Epoch 560, val loss: 0.9522257447242737
Epoch 570, training loss: 315.8205261230469 = 0.7693562507629395 + 50.0 * 6.301023483276367
Epoch 570, val loss: 0.9389556646347046
Epoch 580, training loss: 315.2669982910156 = 0.7499077320098877 + 50.0 * 6.290341854095459
Epoch 580, val loss: 0.9260004162788391
Epoch 590, training loss: 315.1937255859375 = 0.7310883402824402 + 50.0 * 6.289252758026123
Epoch 590, val loss: 0.9137707352638245
Epoch 600, training loss: 315.2129211425781 = 0.7127903699874878 + 50.0 * 6.290002346038818
Epoch 600, val loss: 0.9023028612136841
Epoch 610, training loss: 314.9844055175781 = 0.6949222683906555 + 50.0 * 6.285789489746094
Epoch 610, val loss: 0.8913219571113586
Epoch 620, training loss: 314.83770751953125 = 0.6775549054145813 + 50.0 * 6.283203125
Epoch 620, val loss: 0.8810595273971558
Epoch 630, training loss: 314.79864501953125 = 0.6606299877166748 + 50.0 * 6.282760143280029
Epoch 630, val loss: 0.8713406920433044
Epoch 640, training loss: 315.0790100097656 = 0.6440571546554565 + 50.0 * 6.288699626922607
Epoch 640, val loss: 0.862044632434845
Epoch 650, training loss: 314.596435546875 = 0.627824604511261 + 50.0 * 6.279371738433838
Epoch 650, val loss: 0.8531010150909424
Epoch 660, training loss: 314.50933837890625 = 0.612019956111908 + 50.0 * 6.2779459953308105
Epoch 660, val loss: 0.8447157144546509
Epoch 670, training loss: 314.9977722167969 = 0.5965904593467712 + 50.0 * 6.288023948669434
Epoch 670, val loss: 0.8370437622070312
Epoch 680, training loss: 314.39739990234375 = 0.5814100503921509 + 50.0 * 6.27631950378418
Epoch 680, val loss: 0.8292824029922485
Epoch 690, training loss: 314.3158874511719 = 0.5666103363037109 + 50.0 * 6.274985313415527
Epoch 690, val loss: 0.8223006129264832
Epoch 700, training loss: 314.1909484863281 = 0.5521746277809143 + 50.0 * 6.272775173187256
Epoch 700, val loss: 0.8156521320343018
Epoch 710, training loss: 314.173583984375 = 0.5380486845970154 + 50.0 * 6.27271032333374
Epoch 710, val loss: 0.8094111680984497
Epoch 720, training loss: 314.74810791015625 = 0.5241257548332214 + 50.0 * 6.28447961807251
Epoch 720, val loss: 0.803142786026001
Epoch 730, training loss: 313.99615478515625 = 0.5104212164878845 + 50.0 * 6.269714832305908
Epoch 730, val loss: 0.7975238561630249
Epoch 740, training loss: 313.9646301269531 = 0.49708086252212524 + 50.0 * 6.269351005554199
Epoch 740, val loss: 0.7922343611717224
Epoch 750, training loss: 313.87139892578125 = 0.4840308427810669 + 50.0 * 6.267747402191162
Epoch 750, val loss: 0.787291944026947
Epoch 760, training loss: 313.8066101074219 = 0.47124016284942627 + 50.0 * 6.266706943511963
Epoch 760, val loss: 0.7826716303825378
Epoch 770, training loss: 313.80029296875 = 0.4586760103702545 + 50.0 * 6.26683235168457
Epoch 770, val loss: 0.7782140374183655
Epoch 780, training loss: 313.84259033203125 = 0.44623860716819763 + 50.0 * 6.267927169799805
Epoch 780, val loss: 0.7740830779075623
Epoch 790, training loss: 313.7430725097656 = 0.4340945780277252 + 50.0 * 6.266180038452148
Epoch 790, val loss: 0.7700352072715759
Epoch 800, training loss: 313.62457275390625 = 0.42219263315200806 + 50.0 * 6.264048099517822
Epoch 800, val loss: 0.7663825154304504
Epoch 810, training loss: 313.54052734375 = 0.41051605343818665 + 50.0 * 6.262599945068359
Epoch 810, val loss: 0.7629044651985168
Epoch 820, training loss: 313.9762268066406 = 0.3990432024002075 + 50.0 * 6.271543502807617
Epoch 820, val loss: 0.7595638632774353
Epoch 830, training loss: 313.62200927734375 = 0.3877755105495453 + 50.0 * 6.264684200286865
Epoch 830, val loss: 0.7566694021224976
Epoch 840, training loss: 313.39532470703125 = 0.37667831778526306 + 50.0 * 6.260372638702393
Epoch 840, val loss: 0.7537137866020203
Epoch 850, training loss: 313.3191223144531 = 0.36588168144226074 + 50.0 * 6.259064674377441
Epoch 850, val loss: 0.7511022686958313
Epoch 860, training loss: 313.27935791015625 = 0.3553425073623657 + 50.0 * 6.258480072021484
Epoch 860, val loss: 0.7487881183624268
Epoch 870, training loss: 313.5404052734375 = 0.3449788987636566 + 50.0 * 6.263908386230469
Epoch 870, val loss: 0.7464131712913513
Epoch 880, training loss: 313.4835510253906 = 0.3347906172275543 + 50.0 * 6.262975215911865
Epoch 880, val loss: 0.7445864677429199
Epoch 890, training loss: 313.12689208984375 = 0.3248376250267029 + 50.0 * 6.256041049957275
Epoch 890, val loss: 0.7426254749298096
Epoch 900, training loss: 313.1549377441406 = 0.31516435742378235 + 50.0 * 6.256795406341553
Epoch 900, val loss: 0.7409862279891968
Epoch 910, training loss: 313.17681884765625 = 0.30574116110801697 + 50.0 * 6.257421493530273
Epoch 910, val loss: 0.7396460771560669
Epoch 920, training loss: 313.19287109375 = 0.29655423760414124 + 50.0 * 6.2579264640808105
Epoch 920, val loss: 0.7385141849517822
Epoch 930, training loss: 313.0963134765625 = 0.2875542938709259 + 50.0 * 6.2561750411987305
Epoch 930, val loss: 0.7373335361480713
Epoch 940, training loss: 312.94384765625 = 0.2788267433643341 + 50.0 * 6.253300189971924
Epoch 940, val loss: 0.7364740967750549
Epoch 950, training loss: 312.91705322265625 = 0.27036210894584656 + 50.0 * 6.252933979034424
Epoch 950, val loss: 0.7358713150024414
Epoch 960, training loss: 313.06744384765625 = 0.26214519143104553 + 50.0 * 6.256105899810791
Epoch 960, val loss: 0.7355197668075562
Epoch 970, training loss: 312.8960876464844 = 0.254144549369812 + 50.0 * 6.252838611602783
Epoch 970, val loss: 0.735447108745575
Epoch 980, training loss: 312.76513671875 = 0.24634325504302979 + 50.0 * 6.250376224517822
Epoch 980, val loss: 0.7353429794311523
Epoch 990, training loss: 312.693603515625 = 0.23884537816047668 + 50.0 * 6.249095439910889
Epoch 990, val loss: 0.7356889247894287
Epoch 1000, training loss: 312.914794921875 = 0.23160074651241302 + 50.0 * 6.253664016723633
Epoch 1000, val loss: 0.7361853718757629
Epoch 1010, training loss: 312.7177734375 = 0.2244778722524643 + 50.0 * 6.249865531921387
Epoch 1010, val loss: 0.7367927432060242
Epoch 1020, training loss: 312.6152648925781 = 0.21766705811023712 + 50.0 * 6.247951507568359
Epoch 1020, val loss: 0.7374811172485352
Epoch 1030, training loss: 312.5624084472656 = 0.21105891466140747 + 50.0 * 6.2470269203186035
Epoch 1030, val loss: 0.7385989427566528
Epoch 1040, training loss: 312.86810302734375 = 0.20469988882541656 + 50.0 * 6.253267765045166
Epoch 1040, val loss: 0.7398159503936768
Epoch 1050, training loss: 312.6668701171875 = 0.19846759736537933 + 50.0 * 6.249368190765381
Epoch 1050, val loss: 0.7410414814949036
Epoch 1060, training loss: 312.6002502441406 = 0.1925027072429657 + 50.0 * 6.248155117034912
Epoch 1060, val loss: 0.742583155632019
Epoch 1070, training loss: 312.4410400390625 = 0.18671387434005737 + 50.0 * 6.245086669921875
Epoch 1070, val loss: 0.7441633939743042
Epoch 1080, training loss: 312.49102783203125 = 0.18114466965198517 + 50.0 * 6.246197700500488
Epoch 1080, val loss: 0.7461718916893005
Epoch 1090, training loss: 312.3484191894531 = 0.17577330768108368 + 50.0 * 6.243453025817871
Epoch 1090, val loss: 0.7482613921165466
Epoch 1100, training loss: 312.6356201171875 = 0.17059184610843658 + 50.0 * 6.249300479888916
Epoch 1100, val loss: 0.7506115436553955
Epoch 1110, training loss: 312.3912353515625 = 0.16551761329174042 + 50.0 * 6.244513988494873
Epoch 1110, val loss: 0.7528734803199768
Epoch 1120, training loss: 312.29248046875 = 0.16066724061965942 + 50.0 * 6.242636203765869
Epoch 1120, val loss: 0.7552538514137268
Epoch 1130, training loss: 312.2057189941406 = 0.15599322319030762 + 50.0 * 6.240994453430176
Epoch 1130, val loss: 0.7580605149269104
Epoch 1140, training loss: 312.2088928222656 = 0.1515166163444519 + 50.0 * 6.241147518157959
Epoch 1140, val loss: 0.7610519528388977
Epoch 1150, training loss: 312.40570068359375 = 0.1471506953239441 + 50.0 * 6.245170593261719
Epoch 1150, val loss: 0.7639091610908508
Epoch 1160, training loss: 312.3181457519531 = 0.14290477335453033 + 50.0 * 6.243504524230957
Epoch 1160, val loss: 0.7667675614356995
Epoch 1170, training loss: 312.0624084472656 = 0.13884329795837402 + 50.0 * 6.238471031188965
Epoch 1170, val loss: 0.7697378396987915
Epoch 1180, training loss: 312.0890197753906 = 0.134929820895195 + 50.0 * 6.239081859588623
Epoch 1180, val loss: 0.7733595967292786
Epoch 1190, training loss: 312.0564270019531 = 0.131166011095047 + 50.0 * 6.2385053634643555
Epoch 1190, val loss: 0.7765110731124878
Epoch 1200, training loss: 312.252197265625 = 0.12752696871757507 + 50.0 * 6.242493152618408
Epoch 1200, val loss: 0.7800655364990234
Epoch 1210, training loss: 312.00885009765625 = 0.12398909777402878 + 50.0 * 6.237697124481201
Epoch 1210, val loss: 0.7837510108947754
Epoch 1220, training loss: 312.0433044433594 = 0.12057100236415863 + 50.0 * 6.238454818725586
Epoch 1220, val loss: 0.7872995734214783
Epoch 1230, training loss: 311.9969482421875 = 0.11726989597082138 + 50.0 * 6.237593173980713
Epoch 1230, val loss: 0.7911518812179565
Epoch 1240, training loss: 311.8968811035156 = 0.11410503089427948 + 50.0 * 6.235655784606934
Epoch 1240, val loss: 0.7949362397193909
Epoch 1250, training loss: 312.0312805175781 = 0.11105658113956451 + 50.0 * 6.238404273986816
Epoch 1250, val loss: 0.7989083528518677
Epoch 1260, training loss: 311.90185546875 = 0.10808280110359192 + 50.0 * 6.235875129699707
Epoch 1260, val loss: 0.8027083873748779
Epoch 1270, training loss: 311.86083984375 = 0.1052154079079628 + 50.0 * 6.235112190246582
Epoch 1270, val loss: 0.8066885471343994
Epoch 1280, training loss: 311.7801208496094 = 0.10245612263679504 + 50.0 * 6.233553409576416
Epoch 1280, val loss: 0.8109408617019653
Epoch 1290, training loss: 311.9093017578125 = 0.09978866577148438 + 50.0 * 6.236190319061279
Epoch 1290, val loss: 0.8150327801704407
Epoch 1300, training loss: 312.0149230957031 = 0.09719093888998032 + 50.0 * 6.238354682922363
Epoch 1300, val loss: 0.8188688158988953
Epoch 1310, training loss: 311.7920227050781 = 0.09469269216060638 + 50.0 * 6.233946323394775
Epoch 1310, val loss: 0.8236113786697388
Epoch 1320, training loss: 311.71148681640625 = 0.09227259457111359 + 50.0 * 6.232384204864502
Epoch 1320, val loss: 0.8273778557777405
Epoch 1330, training loss: 311.7452087402344 = 0.08994504064321518 + 50.0 * 6.233105659484863
Epoch 1330, val loss: 0.8320184350013733
Epoch 1340, training loss: 311.9449157714844 = 0.08767765015363693 + 50.0 * 6.237144947052002
Epoch 1340, val loss: 0.8360506296157837
Epoch 1350, training loss: 311.77838134765625 = 0.08549688756465912 + 50.0 * 6.23385763168335
Epoch 1350, val loss: 0.8403187990188599
Epoch 1360, training loss: 311.67724609375 = 0.08336585015058517 + 50.0 * 6.231877326965332
Epoch 1360, val loss: 0.8446027636528015
Epoch 1370, training loss: 311.5773620605469 = 0.08132883906364441 + 50.0 * 6.229920387268066
Epoch 1370, val loss: 0.8492714166641235
Epoch 1380, training loss: 311.55963134765625 = 0.07936513423919678 + 50.0 * 6.229605197906494
Epoch 1380, val loss: 0.8536578416824341
Epoch 1390, training loss: 311.7394104003906 = 0.07747580111026764 + 50.0 * 6.233238220214844
Epoch 1390, val loss: 0.8582876324653625
Epoch 1400, training loss: 311.6280212402344 = 0.07559774070978165 + 50.0 * 6.231048583984375
Epoch 1400, val loss: 0.8621532320976257
Epoch 1410, training loss: 311.5135192871094 = 0.07380596548318863 + 50.0 * 6.228794574737549
Epoch 1410, val loss: 0.8665491342544556
Epoch 1420, training loss: 311.5202331542969 = 0.07206396013498306 + 50.0 * 6.228963375091553
Epoch 1420, val loss: 0.8710263967514038
Epoch 1430, training loss: 312.1772155761719 = 0.07037636637687683 + 50.0 * 6.2421369552612305
Epoch 1430, val loss: 0.8752716779708862
Epoch 1440, training loss: 311.5904541015625 = 0.06874486058950424 + 50.0 * 6.230433940887451
Epoch 1440, val loss: 0.8795594573020935
Epoch 1450, training loss: 311.4505615234375 = 0.067140594124794 + 50.0 * 6.227668762207031
Epoch 1450, val loss: 0.8839373588562012
Epoch 1460, training loss: 311.4027404785156 = 0.06561684608459473 + 50.0 * 6.226742744445801
Epoch 1460, val loss: 0.8882827162742615
Epoch 1470, training loss: 311.3749694824219 = 0.0641428753733635 + 50.0 * 6.2262163162231445
Epoch 1470, val loss: 0.8926999568939209
Epoch 1480, training loss: 311.44390869140625 = 0.0627131387591362 + 50.0 * 6.22762393951416
Epoch 1480, val loss: 0.8970873951911926
Epoch 1490, training loss: 311.6095886230469 = 0.06130943447351456 + 50.0 * 6.230965614318848
Epoch 1490, val loss: 0.9013245701789856
Epoch 1500, training loss: 311.3898620605469 = 0.05994119867682457 + 50.0 * 6.226598262786865
Epoch 1500, val loss: 0.9054924845695496
Epoch 1510, training loss: 311.3590393066406 = 0.05862603709101677 + 50.0 * 6.226008415222168
Epoch 1510, val loss: 0.909820020198822
Epoch 1520, training loss: 311.3336181640625 = 0.057355016469955444 + 50.0 * 6.225525379180908
Epoch 1520, val loss: 0.9141679406166077
Epoch 1530, training loss: 311.64324951171875 = 0.05612108111381531 + 50.0 * 6.2317423820495605
Epoch 1530, val loss: 0.9183384776115417
Epoch 1540, training loss: 311.47674560546875 = 0.05492127686738968 + 50.0 * 6.22843599319458
Epoch 1540, val loss: 0.922527551651001
Epoch 1550, training loss: 311.3569641113281 = 0.05374801903963089 + 50.0 * 6.226064682006836
Epoch 1550, val loss: 0.9267948269844055
Epoch 1560, training loss: 311.2532653808594 = 0.05261937528848648 + 50.0 * 6.224013328552246
Epoch 1560, val loss: 0.9310773611068726
Epoch 1570, training loss: 311.3117980957031 = 0.05153019726276398 + 50.0 * 6.225204944610596
Epoch 1570, val loss: 0.9355121850967407
Epoch 1580, training loss: 311.328369140625 = 0.05045996606349945 + 50.0 * 6.225558280944824
Epoch 1580, val loss: 0.9395131468772888
Epoch 1590, training loss: 311.24395751953125 = 0.04942784458398819 + 50.0 * 6.22389030456543
Epoch 1590, val loss: 0.9433103203773499
Epoch 1600, training loss: 311.5688781738281 = 0.04843343421816826 + 50.0 * 6.230408668518066
Epoch 1600, val loss: 0.9481219053268433
Epoch 1610, training loss: 311.20684814453125 = 0.04742332175374031 + 50.0 * 6.223188400268555
Epoch 1610, val loss: 0.95128333568573
Epoch 1620, training loss: 311.1697998046875 = 0.04646545276045799 + 50.0 * 6.222466468811035
Epoch 1620, val loss: 0.9559652805328369
Epoch 1630, training loss: 311.13482666015625 = 0.04554847627878189 + 50.0 * 6.221785545349121
Epoch 1630, val loss: 0.9597046971321106
Epoch 1640, training loss: 311.0968933105469 = 0.04465172439813614 + 50.0 * 6.221045017242432
Epoch 1640, val loss: 0.9639909267425537
Epoch 1650, training loss: 311.2794494628906 = 0.04378777742385864 + 50.0 * 6.224713325500488
Epoch 1650, val loss: 0.9675891995429993
Epoch 1660, training loss: 311.15716552734375 = 0.04293259233236313 + 50.0 * 6.22228479385376
Epoch 1660, val loss: 0.9722832441329956
Epoch 1670, training loss: 311.17742919921875 = 0.042096469551324844 + 50.0 * 6.2227067947387695
Epoch 1670, val loss: 0.9757057428359985
Epoch 1680, training loss: 311.0956726074219 = 0.04129308834671974 + 50.0 * 6.22108793258667
Epoch 1680, val loss: 0.9798532128334045
Epoch 1690, training loss: 311.3467712402344 = 0.040513839572668076 + 50.0 * 6.2261247634887695
Epoch 1690, val loss: 0.9836214184761047
Epoch 1700, training loss: 311.0630187988281 = 0.039747003465890884 + 50.0 * 6.220465660095215
Epoch 1700, val loss: 0.9877937436103821
Epoch 1710, training loss: 311.0567626953125 = 0.03900453448295593 + 50.0 * 6.22035551071167
Epoch 1710, val loss: 0.9916713237762451
Epoch 1720, training loss: 311.2480773925781 = 0.03828530013561249 + 50.0 * 6.22419548034668
Epoch 1720, val loss: 0.9955678582191467
Epoch 1730, training loss: 311.0242614746094 = 0.03757229447364807 + 50.0 * 6.219734191894531
Epoch 1730, val loss: 0.9994770288467407
Epoch 1740, training loss: 311.14215087890625 = 0.03688136115670204 + 50.0 * 6.222105026245117
Epoch 1740, val loss: 1.0031237602233887
Epoch 1750, training loss: 310.9883117675781 = 0.036215927451848984 + 50.0 * 6.21904182434082
Epoch 1750, val loss: 1.0070775747299194
Epoch 1760, training loss: 310.9656982421875 = 0.03556142747402191 + 50.0 * 6.218602657318115
Epoch 1760, val loss: 1.0106351375579834
Epoch 1770, training loss: 310.930419921875 = 0.03493087738752365 + 50.0 * 6.217909812927246
Epoch 1770, val loss: 1.0143951177597046
Epoch 1780, training loss: 311.17913818359375 = 0.034323014318943024 + 50.0 * 6.222896099090576
Epoch 1780, val loss: 1.0180046558380127
Epoch 1790, training loss: 311.2910461425781 = 0.03370176628232002 + 50.0 * 6.225147247314453
Epoch 1790, val loss: 1.0217993259429932
Epoch 1800, training loss: 310.9901123046875 = 0.0331108383834362 + 50.0 * 6.21914005279541
Epoch 1800, val loss: 1.0254310369491577
Epoch 1810, training loss: 310.8997497558594 = 0.032529015094041824 + 50.0 * 6.217344284057617
Epoch 1810, val loss: 1.029232144355774
Epoch 1820, training loss: 310.8524475097656 = 0.031970053911209106 + 50.0 * 6.216409206390381
Epoch 1820, val loss: 1.0327768325805664
Epoch 1830, training loss: 310.95269775390625 = 0.03143150359392166 + 50.0 * 6.218425273895264
Epoch 1830, val loss: 1.036176085472107
Epoch 1840, training loss: 311.1505126953125 = 0.03090137057006359 + 50.0 * 6.2223920822143555
Epoch 1840, val loss: 1.040094256401062
Epoch 1850, training loss: 310.8360900878906 = 0.03036782518029213 + 50.0 * 6.216114521026611
Epoch 1850, val loss: 1.0434715747833252
Epoch 1860, training loss: 310.90582275390625 = 0.029855305328965187 + 50.0 * 6.217519283294678
Epoch 1860, val loss: 1.0473730564117432
Epoch 1870, training loss: 310.9152526855469 = 0.02935885824263096 + 50.0 * 6.21771764755249
Epoch 1870, val loss: 1.0507773160934448
Epoch 1880, training loss: 310.8066711425781 = 0.028877614066004753 + 50.0 * 6.2155561447143555
Epoch 1880, val loss: 1.05433189868927
Epoch 1890, training loss: 310.8115539550781 = 0.028408696874976158 + 50.0 * 6.215662956237793
Epoch 1890, val loss: 1.0577841997146606
Epoch 1900, training loss: 311.1435241699219 = 0.02795160934329033 + 50.0 * 6.222311496734619
Epoch 1900, val loss: 1.061279535293579
Epoch 1910, training loss: 310.9213562011719 = 0.02749616652727127 + 50.0 * 6.217877388000488
Epoch 1910, val loss: 1.0648994445800781
Epoch 1920, training loss: 310.75543212890625 = 0.027050498872995377 + 50.0 * 6.214568138122559
Epoch 1920, val loss: 1.0680474042892456
Epoch 1930, training loss: 310.72552490234375 = 0.026621313765645027 + 50.0 * 6.213978290557861
Epoch 1930, val loss: 1.0715595483779907
Epoch 1940, training loss: 310.7984619140625 = 0.02620747499167919 + 50.0 * 6.215445041656494
Epoch 1940, val loss: 1.0749976634979248
Epoch 1950, training loss: 311.0782775878906 = 0.02579980157315731 + 50.0 * 6.221049785614014
Epoch 1950, val loss: 1.0787384510040283
Epoch 1960, training loss: 310.72747802734375 = 0.025382094085216522 + 50.0 * 6.214041709899902
Epoch 1960, val loss: 1.0816274881362915
Epoch 1970, training loss: 310.6981506347656 = 0.024990636855363846 + 50.0 * 6.213463306427002
Epoch 1970, val loss: 1.0849595069885254
Epoch 1980, training loss: 310.7511901855469 = 0.02460707537829876 + 50.0 * 6.214531898498535
Epoch 1980, val loss: 1.0881949663162231
Epoch 1990, training loss: 311.1573486328125 = 0.024231458082795143 + 50.0 * 6.222662448883057
Epoch 1990, val loss: 1.091649055480957
Epoch 2000, training loss: 310.8143005371094 = 0.02386835217475891 + 50.0 * 6.215808868408203
Epoch 2000, val loss: 1.094925045967102
Epoch 2010, training loss: 310.6809997558594 = 0.023503612726926804 + 50.0 * 6.2131500244140625
Epoch 2010, val loss: 1.0981147289276123
Epoch 2020, training loss: 310.6454772949219 = 0.023154811933636665 + 50.0 * 6.212446212768555
Epoch 2020, val loss: 1.1012048721313477
Epoch 2030, training loss: 310.7942810058594 = 0.022820070385932922 + 50.0 * 6.215429306030273
Epoch 2030, val loss: 1.1042492389678955
Epoch 2040, training loss: 310.7828369140625 = 0.022481853142380714 + 50.0 * 6.215207099914551
Epoch 2040, val loss: 1.1078027486801147
Epoch 2050, training loss: 310.70947265625 = 0.022150292992591858 + 50.0 * 6.213746070861816
Epoch 2050, val loss: 1.111094355583191
Epoch 2060, training loss: 310.6435241699219 = 0.021828241646289825 + 50.0 * 6.212433815002441
Epoch 2060, val loss: 1.113918662071228
Epoch 2070, training loss: 310.79296875 = 0.02152070216834545 + 50.0 * 6.215429306030273
Epoch 2070, val loss: 1.1174724102020264
Epoch 2080, training loss: 310.6319885253906 = 0.021205678582191467 + 50.0 * 6.212215423583984
Epoch 2080, val loss: 1.1205567121505737
Epoch 2090, training loss: 310.7929992675781 = 0.020907945930957794 + 50.0 * 6.215442180633545
Epoch 2090, val loss: 1.1237956285476685
Epoch 2100, training loss: 310.76336669921875 = 0.02060561813414097 + 50.0 * 6.214855194091797
Epoch 2100, val loss: 1.1260184049606323
Epoch 2110, training loss: 310.6011047363281 = 0.02031385712325573 + 50.0 * 6.211615562438965
Epoch 2110, val loss: 1.1295666694641113
Epoch 2120, training loss: 310.5458679199219 = 0.020026741549372673 + 50.0 * 6.210516929626465
Epoch 2120, val loss: 1.1322816610336304
Epoch 2130, training loss: 310.60693359375 = 0.019752323627471924 + 50.0 * 6.2117438316345215
Epoch 2130, val loss: 1.135343313217163
Epoch 2140, training loss: 310.8038330078125 = 0.01948307640850544 + 50.0 * 6.215687274932861
Epoch 2140, val loss: 1.138243317604065
Epoch 2150, training loss: 310.5213623046875 = 0.019214682281017303 + 50.0 * 6.210042953491211
Epoch 2150, val loss: 1.1413418054580688
Epoch 2160, training loss: 310.7677001953125 = 0.018957585096359253 + 50.0 * 6.214974880218506
Epoch 2160, val loss: 1.1442935466766357
Epoch 2170, training loss: 310.673828125 = 0.01869877427816391 + 50.0 * 6.213102340698242
Epoch 2170, val loss: 1.1471093893051147
Epoch 2180, training loss: 310.68597412109375 = 0.018447572365403175 + 50.0 * 6.213350772857666
Epoch 2180, val loss: 1.150099277496338
Epoch 2190, training loss: 310.50787353515625 = 0.01819656416773796 + 50.0 * 6.209793567657471
Epoch 2190, val loss: 1.1528457403182983
Epoch 2200, training loss: 310.494384765625 = 0.017956851050257683 + 50.0 * 6.209528923034668
Epoch 2200, val loss: 1.1557141542434692
Epoch 2210, training loss: 310.47039794921875 = 0.017721915617585182 + 50.0 * 6.2090535163879395
Epoch 2210, val loss: 1.1584676504135132
Epoch 2220, training loss: 310.8775329589844 = 0.017499219626188278 + 50.0 * 6.217200756072998
Epoch 2220, val loss: 1.1606323719024658
Epoch 2230, training loss: 310.7323303222656 = 0.017270922660827637 + 50.0 * 6.214301109313965
Epoch 2230, val loss: 1.1644282341003418
Epoch 2240, training loss: 310.5379638671875 = 0.017036544159054756 + 50.0 * 6.210418701171875
Epoch 2240, val loss: 1.1665986776351929
Epoch 2250, training loss: 310.45458984375 = 0.01681715063750744 + 50.0 * 6.2087554931640625
Epoch 2250, val loss: 1.1697832345962524
Epoch 2260, training loss: 310.41790771484375 = 0.016604330390691757 + 50.0 * 6.20802640914917
Epoch 2260, val loss: 1.172444462776184
Epoch 2270, training loss: 310.7980041503906 = 0.016400055959820747 + 50.0 * 6.215632438659668
Epoch 2270, val loss: 1.1750417947769165
Epoch 2280, training loss: 310.5959167480469 = 0.016189321875572205 + 50.0 * 6.211595058441162
Epoch 2280, val loss: 1.17772376537323
Epoch 2290, training loss: 310.52301025390625 = 0.015984296798706055 + 50.0 * 6.210140228271484
Epoch 2290, val loss: 1.1805753707885742
Epoch 2300, training loss: 310.40966796875 = 0.01578478328883648 + 50.0 * 6.2078776359558105
Epoch 2300, val loss: 1.1831153631210327
Epoch 2310, training loss: 310.3623962402344 = 0.015590444207191467 + 50.0 * 6.206935882568359
Epoch 2310, val loss: 1.1857277154922485
Epoch 2320, training loss: 310.372314453125 = 0.0154028395190835 + 50.0 * 6.2071380615234375
Epoch 2320, val loss: 1.1886115074157715
Epoch 2330, training loss: 310.8970947265625 = 0.015218722634017467 + 50.0 * 6.217637538909912
Epoch 2330, val loss: 1.1915686130523682
Epoch 2340, training loss: 310.61962890625 = 0.015038525685667992 + 50.0 * 6.212091445922852
Epoch 2340, val loss: 1.193395972251892
Epoch 2350, training loss: 310.3785400390625 = 0.014848162420094013 + 50.0 * 6.207273483276367
Epoch 2350, val loss: 1.1962711811065674
Epoch 2360, training loss: 310.3173522949219 = 0.01467189658433199 + 50.0 * 6.206053256988525
Epoch 2360, val loss: 1.1987236738204956
Epoch 2370, training loss: 310.4381103515625 = 0.014503562822937965 + 50.0 * 6.20847225189209
Epoch 2370, val loss: 1.2013477087020874
Epoch 2380, training loss: 310.6247863769531 = 0.014331396669149399 + 50.0 * 6.2122087478637695
Epoch 2380, val loss: 1.2039436101913452
Epoch 2390, training loss: 310.306396484375 = 0.014154323376715183 + 50.0 * 6.205844879150391
Epoch 2390, val loss: 1.2061742544174194
Epoch 2400, training loss: 310.29827880859375 = 0.013987602666020393 + 50.0 * 6.205686092376709
Epoch 2400, val loss: 1.2087265253067017
Epoch 2410, training loss: 310.28948974609375 = 0.01382835116237402 + 50.0 * 6.2055134773254395
Epoch 2410, val loss: 1.2112802267074585
Epoch 2420, training loss: 310.2924499511719 = 0.013672254048287868 + 50.0 * 6.205575466156006
Epoch 2420, val loss: 1.2138769626617432
Epoch 2430, training loss: 310.8363037109375 = 0.01352791115641594 + 50.0 * 6.216455459594727
Epoch 2430, val loss: 1.21673583984375
Epoch 2440, training loss: 310.5558776855469 = 0.01336371898651123 + 50.0 * 6.210850238800049
Epoch 2440, val loss: 1.2179216146469116
Epoch 2450, training loss: 310.3621826171875 = 0.013211913406848907 + 50.0 * 6.206979274749756
Epoch 2450, val loss: 1.221053957939148
Epoch 2460, training loss: 310.4195251464844 = 0.013062343001365662 + 50.0 * 6.208129405975342
Epoch 2460, val loss: 1.223465919494629
Epoch 2470, training loss: 310.3506164550781 = 0.012916161678731441 + 50.0 * 6.206754207611084
Epoch 2470, val loss: 1.2255932092666626
Epoch 2480, training loss: 310.3014221191406 = 0.012773443013429642 + 50.0 * 6.205772399902344
Epoch 2480, val loss: 1.2277706861495972
Epoch 2490, training loss: 310.2443542480469 = 0.012633124366402626 + 50.0 * 6.204634666442871
Epoch 2490, val loss: 1.230345606803894
Epoch 2500, training loss: 310.3529968261719 = 0.012498061172664165 + 50.0 * 6.2068095207214355
Epoch 2500, val loss: 1.232747197151184
Epoch 2510, training loss: 310.4864501953125 = 0.01236255094408989 + 50.0 * 6.209481716156006
Epoch 2510, val loss: 1.2348185777664185
Epoch 2520, training loss: 310.3597412109375 = 0.012228981591761112 + 50.0 * 6.2069501876831055
Epoch 2520, val loss: 1.2376641035079956
Epoch 2530, training loss: 310.23779296875 = 0.012094558216631413 + 50.0 * 6.204514026641846
Epoch 2530, val loss: 1.2393898963928223
Epoch 2540, training loss: 310.2078857421875 = 0.011966676451265812 + 50.0 * 6.20391845703125
Epoch 2540, val loss: 1.241761565208435
Epoch 2550, training loss: 310.29840087890625 = 0.011846105568110943 + 50.0 * 6.205731391906738
Epoch 2550, val loss: 1.2442021369934082
Epoch 2560, training loss: 310.3953857421875 = 0.011722452007234097 + 50.0 * 6.207673072814941
Epoch 2560, val loss: 1.2460225820541382
Epoch 2570, training loss: 310.4630432128906 = 0.011601014994084835 + 50.0 * 6.209029197692871
Epoch 2570, val loss: 1.2482357025146484
Epoch 2580, training loss: 310.2447204589844 = 0.011478127911686897 + 50.0 * 6.204664707183838
Epoch 2580, val loss: 1.2507262229919434
Epoch 2590, training loss: 310.1733093261719 = 0.011357512325048447 + 50.0 * 6.2032389640808105
Epoch 2590, val loss: 1.2527003288269043
Epoch 2600, training loss: 310.2040710449219 = 0.011244652792811394 + 50.0 * 6.203856945037842
Epoch 2600, val loss: 1.2549529075622559
Epoch 2610, training loss: 310.5693359375 = 0.011135273613035679 + 50.0 * 6.2111639976501465
Epoch 2610, val loss: 1.257042407989502
Epoch 2620, training loss: 310.3299255371094 = 0.011019425466656685 + 50.0 * 6.20637845993042
Epoch 2620, val loss: 1.2594703435897827
Epoch 2630, training loss: 310.2930908203125 = 0.010908709838986397 + 50.0 * 6.205643653869629
Epoch 2630, val loss: 1.2610888481140137
Epoch 2640, training loss: 310.2616271972656 = 0.010796915739774704 + 50.0 * 6.205016613006592
Epoch 2640, val loss: 1.2633024454116821
Epoch 2650, training loss: 310.1221008300781 = 0.010688935406506062 + 50.0 * 6.202228546142578
Epoch 2650, val loss: 1.2653756141662598
Epoch 2660, training loss: 310.1576232910156 = 0.010583668947219849 + 50.0 * 6.202940940856934
Epoch 2660, val loss: 1.2674230337142944
Epoch 2670, training loss: 310.3555603027344 = 0.010482870042324066 + 50.0 * 6.2069010734558105
Epoch 2670, val loss: 1.2691863775253296
Epoch 2680, training loss: 310.21624755859375 = 0.010381169617176056 + 50.0 * 6.204117298126221
Epoch 2680, val loss: 1.271274447441101
Epoch 2690, training loss: 310.1313781738281 = 0.010280370712280273 + 50.0 * 6.20242166519165
Epoch 2690, val loss: 1.273794412612915
Epoch 2700, training loss: 310.3001403808594 = 0.010182897560298443 + 50.0 * 6.205799102783203
Epoch 2700, val loss: 1.2758885622024536
Epoch 2710, training loss: 310.240478515625 = 0.010082936845719814 + 50.0 * 6.20460844039917
Epoch 2710, val loss: 1.2773821353912354
Epoch 2720, training loss: 310.12957763671875 = 0.009987740777432919 + 50.0 * 6.202392101287842
Epoch 2720, val loss: 1.2792905569076538
Epoch 2730, training loss: 310.08734130859375 = 0.009892404079437256 + 50.0 * 6.201549053192139
Epoch 2730, val loss: 1.2815645933151245
Epoch 2740, training loss: 310.05596923828125 = 0.009801586158573627 + 50.0 * 6.200923442840576
Epoch 2740, val loss: 1.2832269668579102
Epoch 2750, training loss: 310.1555480957031 = 0.0097136739641428 + 50.0 * 6.202917098999023
Epoch 2750, val loss: 1.2851123809814453
Epoch 2760, training loss: 310.34423828125 = 0.009625990875065327 + 50.0 * 6.206692218780518
Epoch 2760, val loss: 1.2874839305877686
Epoch 2770, training loss: 310.2474365234375 = 0.009536896832287312 + 50.0 * 6.2047576904296875
Epoch 2770, val loss: 1.289153814315796
Epoch 2780, training loss: 310.1438293457031 = 0.009446335956454277 + 50.0 * 6.2026872634887695
Epoch 2780, val loss: 1.2905938625335693
Epoch 2790, training loss: 310.1227111816406 = 0.009361355565488338 + 50.0 * 6.202266693115234
Epoch 2790, val loss: 1.2927876710891724
Epoch 2800, training loss: 310.2679138183594 = 0.00927787646651268 + 50.0 * 6.205172538757324
Epoch 2800, val loss: 1.2943100929260254
Epoch 2810, training loss: 310.09881591796875 = 0.00919676385819912 + 50.0 * 6.2017927169799805
Epoch 2810, val loss: 1.2961933612823486
Epoch 2820, training loss: 310.1078796386719 = 0.009115775115787983 + 50.0 * 6.201975345611572
Epoch 2820, val loss: 1.2981473207473755
Epoch 2830, training loss: 310.1125183105469 = 0.009033802896738052 + 50.0 * 6.2020697593688965
Epoch 2830, val loss: 1.300050973892212
Epoch 2840, training loss: 310.1288757324219 = 0.008957344107329845 + 50.0 * 6.20239782333374
Epoch 2840, val loss: 1.301810622215271
Epoch 2850, training loss: 310.18280029296875 = 0.00887774582952261 + 50.0 * 6.2034783363342285
Epoch 2850, val loss: 1.3035262823104858
Epoch 2860, training loss: 310.0249938964844 = 0.008798838593065739 + 50.0 * 6.200324058532715
Epoch 2860, val loss: 1.305457592010498
Epoch 2870, training loss: 309.9938049316406 = 0.008722729980945587 + 50.0 * 6.19970178604126
Epoch 2870, val loss: 1.3071125745773315
Epoch 2880, training loss: 309.9627685546875 = 0.008649202063679695 + 50.0 * 6.199082851409912
Epoch 2880, val loss: 1.3089077472686768
Epoch 2890, training loss: 310.0753173828125 = 0.008578970097005367 + 50.0 * 6.2013349533081055
Epoch 2890, val loss: 1.310556173324585
Epoch 2900, training loss: 310.3359680175781 = 0.008506495505571365 + 50.0 * 6.206549167633057
Epoch 2900, val loss: 1.312147617340088
Epoch 2910, training loss: 310.0455017089844 = 0.008433769457042217 + 50.0 * 6.200741767883301
Epoch 2910, val loss: 1.3139766454696655
Epoch 2920, training loss: 309.9353332519531 = 0.008361957035958767 + 50.0 * 6.1985392570495605
Epoch 2920, val loss: 1.315459132194519
Epoch 2930, training loss: 309.9358215332031 = 0.008293540216982365 + 50.0 * 6.198550701141357
Epoch 2930, val loss: 1.3172210454940796
Epoch 2940, training loss: 309.9540710449219 = 0.008228025399148464 + 50.0 * 6.198916912078857
Epoch 2940, val loss: 1.3190526962280273
Epoch 2950, training loss: 310.26373291015625 = 0.008166623301804066 + 50.0 * 6.205111026763916
Epoch 2950, val loss: 1.320919156074524
Epoch 2960, training loss: 309.9615173339844 = 0.008094447664916515 + 50.0 * 6.199068546295166
Epoch 2960, val loss: 1.32218337059021
Epoch 2970, training loss: 310.17596435546875 = 0.008031434379518032 + 50.0 * 6.2033586502075195
Epoch 2970, val loss: 1.323850154876709
Epoch 2980, training loss: 310.10467529296875 = 0.007964912801980972 + 50.0 * 6.201934337615967
Epoch 2980, val loss: 1.3245705366134644
Epoch 2990, training loss: 309.9208068847656 = 0.007898164913058281 + 50.0 * 6.198257923126221
Epoch 2990, val loss: 1.327148675918579
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7777777777777778
0.8371112282551397
The final CL Acc:0.76420, 0.02188, The final GNN Acc:0.83834, 0.00108
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11584])
remove edge: torch.Size([2, 9538])
updated graph: torch.Size([2, 10566])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7882385253906 = 1.9456311464309692 + 50.0 * 8.59685230255127
Epoch 0, val loss: 1.9458441734313965
Epoch 10, training loss: 431.7447204589844 = 1.9368689060211182 + 50.0 * 8.59615707397461
Epoch 10, val loss: 1.9374507665634155
Epoch 20, training loss: 431.4925537109375 = 1.9261810779571533 + 50.0 * 8.591327667236328
Epoch 20, val loss: 1.9269723892211914
Epoch 30, training loss: 429.8730163574219 = 1.9126583337783813 + 50.0 * 8.55920696258545
Epoch 30, val loss: 1.9132611751556396
Epoch 40, training loss: 420.58868408203125 = 1.8963024616241455 + 50.0 * 8.373847961425781
Epoch 40, val loss: 1.8968281745910645
Epoch 50, training loss: 390.1203308105469 = 1.8788208961486816 + 50.0 * 7.764830112457275
Epoch 50, val loss: 1.8792845010757446
Epoch 60, training loss: 366.9226989746094 = 1.865273356437683 + 50.0 * 7.301148414611816
Epoch 60, val loss: 1.8664761781692505
Epoch 70, training loss: 352.75994873046875 = 1.8537979125976562 + 50.0 * 7.018123149871826
Epoch 70, val loss: 1.8552093505859375
Epoch 80, training loss: 347.02935791015625 = 1.8430109024047852 + 50.0 * 6.903727054595947
Epoch 80, val loss: 1.8443084955215454
Epoch 90, training loss: 342.75311279296875 = 1.8327288627624512 + 50.0 * 6.8184075355529785
Epoch 90, val loss: 1.83376944065094
Epoch 100, training loss: 338.7378234863281 = 1.823499321937561 + 50.0 * 6.738286972045898
Epoch 100, val loss: 1.824397087097168
Epoch 110, training loss: 336.070556640625 = 1.8150992393493652 + 50.0 * 6.6851091384887695
Epoch 110, val loss: 1.8157155513763428
Epoch 120, training loss: 334.1188659667969 = 1.806560754776001 + 50.0 * 6.646246433258057
Epoch 120, val loss: 1.806858777999878
Epoch 130, training loss: 332.4468078613281 = 1.7980550527572632 + 50.0 * 6.612974643707275
Epoch 130, val loss: 1.7980613708496094
Epoch 140, training loss: 330.9534606933594 = 1.7896817922592163 + 50.0 * 6.58327579498291
Epoch 140, val loss: 1.7895069122314453
Epoch 150, training loss: 329.73089599609375 = 1.7811468839645386 + 50.0 * 6.558994770050049
Epoch 150, val loss: 1.781022071838379
Epoch 160, training loss: 328.5624694824219 = 1.7722676992416382 + 50.0 * 6.53580379486084
Epoch 160, val loss: 1.7723761796951294
Epoch 170, training loss: 327.6033020019531 = 1.7627742290496826 + 50.0 * 6.516810417175293
Epoch 170, val loss: 1.76333749294281
Epoch 180, training loss: 326.76495361328125 = 1.7524409294128418 + 50.0 * 6.500250339508057
Epoch 180, val loss: 1.7537773847579956
Epoch 190, training loss: 325.9920349121094 = 1.7412670850753784 + 50.0 * 6.485015392303467
Epoch 190, val loss: 1.7436975240707397
Epoch 200, training loss: 325.53839111328125 = 1.7290509939193726 + 50.0 * 6.476186752319336
Epoch 200, val loss: 1.7328004837036133
Epoch 210, training loss: 324.7263488769531 = 1.7157509326934814 + 50.0 * 6.460211753845215
Epoch 210, val loss: 1.7210311889648438
Epoch 220, training loss: 324.0581359863281 = 1.7013124227523804 + 50.0 * 6.447136402130127
Epoch 220, val loss: 1.7084057331085205
Epoch 230, training loss: 323.50726318359375 = 1.6857177019119263 + 50.0 * 6.436430931091309
Epoch 230, val loss: 1.6948853731155396
Epoch 240, training loss: 323.04864501953125 = 1.6688073873519897 + 50.0 * 6.4275970458984375
Epoch 240, val loss: 1.680336356163025
Epoch 250, training loss: 322.6726989746094 = 1.6506398916244507 + 50.0 * 6.420441150665283
Epoch 250, val loss: 1.6646794080734253
Epoch 260, training loss: 322.1507263183594 = 1.6311293840408325 + 50.0 * 6.410391807556152
Epoch 260, val loss: 1.6482349634170532
Epoch 270, training loss: 321.7431640625 = 1.6105114221572876 + 50.0 * 6.402653217315674
Epoch 270, val loss: 1.6308673620224
Epoch 280, training loss: 321.407470703125 = 1.5887491703033447 + 50.0 * 6.396374702453613
Epoch 280, val loss: 1.6125377416610718
Epoch 290, training loss: 321.06549072265625 = 1.5659923553466797 + 50.0 * 6.389990329742432
Epoch 290, val loss: 1.5937771797180176
Epoch 300, training loss: 320.7003173828125 = 1.5424436330795288 + 50.0 * 6.383157253265381
Epoch 300, val loss: 1.574358344078064
Epoch 310, training loss: 320.57275390625 = 1.5181775093078613 + 50.0 * 6.381091594696045
Epoch 310, val loss: 1.5546668767929077
Epoch 320, training loss: 320.1383056640625 = 1.4931964874267578 + 50.0 * 6.3729023933410645
Epoch 320, val loss: 1.534637212753296
Epoch 330, training loss: 319.8592224121094 = 1.4678000211715698 + 50.0 * 6.367828369140625
Epoch 330, val loss: 1.5142794847488403
Epoch 340, training loss: 319.5660095214844 = 1.4420336484909058 + 50.0 * 6.362479209899902
Epoch 340, val loss: 1.4940122365951538
Epoch 350, training loss: 319.37384033203125 = 1.4160422086715698 + 50.0 * 6.359156131744385
Epoch 350, val loss: 1.473717451095581
Epoch 360, training loss: 319.1054992675781 = 1.389809250831604 + 50.0 * 6.354313850402832
Epoch 360, val loss: 1.4535601139068604
Epoch 370, training loss: 318.838134765625 = 1.3633947372436523 + 50.0 * 6.349494457244873
Epoch 370, val loss: 1.4334660768508911
Epoch 380, training loss: 318.85894775390625 = 1.336824655532837 + 50.0 * 6.350442409515381
Epoch 380, val loss: 1.4137649536132812
Epoch 390, training loss: 318.4413757324219 = 1.3102177381515503 + 50.0 * 6.342623233795166
Epoch 390, val loss: 1.393653154373169
Epoch 400, training loss: 318.26580810546875 = 1.2835955619812012 + 50.0 * 6.339643955230713
Epoch 400, val loss: 1.3739694356918335
Epoch 410, training loss: 318.0478210449219 = 1.2571442127227783 + 50.0 * 6.335813522338867
Epoch 410, val loss: 1.3545366525650024
Epoch 420, training loss: 317.8528747558594 = 1.230834722518921 + 50.0 * 6.3324408531188965
Epoch 420, val loss: 1.3354384899139404
Epoch 430, training loss: 317.9499816894531 = 1.2046115398406982 + 50.0 * 6.334907531738281
Epoch 430, val loss: 1.3165374994277954
Epoch 440, training loss: 317.6161193847656 = 1.178492784500122 + 50.0 * 6.328752517700195
Epoch 440, val loss: 1.298003077507019
Epoch 450, training loss: 317.495361328125 = 1.1526451110839844 + 50.0 * 6.326854228973389
Epoch 450, val loss: 1.2797969579696655
Epoch 460, training loss: 317.2390441894531 = 1.1271740198135376 + 50.0 * 6.322237491607666
Epoch 460, val loss: 1.2622182369232178
Epoch 470, training loss: 317.1085205078125 = 1.1021270751953125 + 50.0 * 6.320127964019775
Epoch 470, val loss: 1.2451717853546143
Epoch 480, training loss: 317.0581359863281 = 1.0774744749069214 + 50.0 * 6.319612979888916
Epoch 480, val loss: 1.2286285161972046
Epoch 490, training loss: 316.8600158691406 = 1.0532987117767334 + 50.0 * 6.316134929656982
Epoch 490, val loss: 1.212447166442871
Epoch 500, training loss: 316.7348937988281 = 1.0295398235321045 + 50.0 * 6.3141069412231445
Epoch 500, val loss: 1.1970951557159424
Epoch 510, training loss: 316.7048645019531 = 1.006333589553833 + 50.0 * 6.313970565795898
Epoch 510, val loss: 1.1822638511657715
Epoch 520, training loss: 316.4136962890625 = 0.9837260842323303 + 50.0 * 6.308599472045898
Epoch 520, val loss: 1.1681994199752808
Epoch 530, training loss: 316.44561767578125 = 0.9615995287895203 + 50.0 * 6.309679985046387
Epoch 530, val loss: 1.1547470092773438
Epoch 540, training loss: 316.1806945800781 = 0.9401742219924927 + 50.0 * 6.304810047149658
Epoch 540, val loss: 1.1415408849716187
Epoch 550, training loss: 316.0387878417969 = 0.91922527551651 + 50.0 * 6.302391052246094
Epoch 550, val loss: 1.129376769065857
Epoch 560, training loss: 316.1395263671875 = 0.8989547491073608 + 50.0 * 6.304811477661133
Epoch 560, val loss: 1.1176810264587402
Epoch 570, training loss: 315.8559875488281 = 0.8791058659553528 + 50.0 * 6.299537181854248
Epoch 570, val loss: 1.1063820123672485
Epoch 580, training loss: 315.83697509765625 = 0.8598098158836365 + 50.0 * 6.299543380737305
Epoch 580, val loss: 1.0958720445632935
Epoch 590, training loss: 315.64892578125 = 0.8411990404129028 + 50.0 * 6.296154975891113
Epoch 590, val loss: 1.0858701467514038
Epoch 600, training loss: 315.45953369140625 = 0.822997510433197 + 50.0 * 6.292730808258057
Epoch 600, val loss: 1.0764493942260742
Epoch 610, training loss: 315.3925476074219 = 0.8053726553916931 + 50.0 * 6.291743755340576
Epoch 610, val loss: 1.0675890445709229
Epoch 620, training loss: 315.6764831542969 = 0.7882370948791504 + 50.0 * 6.297764778137207
Epoch 620, val loss: 1.059110403060913
Epoch 630, training loss: 315.2256774902344 = 0.7713853120803833 + 50.0 * 6.289085388183594
Epoch 630, val loss: 1.050666093826294
Epoch 640, training loss: 315.1365661621094 = 0.7550796866416931 + 50.0 * 6.287630081176758
Epoch 640, val loss: 1.0427931547164917
Epoch 650, training loss: 315.073974609375 = 0.7392480969429016 + 50.0 * 6.286694526672363
Epoch 650, val loss: 1.0354527235031128
Epoch 660, training loss: 314.9316101074219 = 0.7236530780792236 + 50.0 * 6.284158706665039
Epoch 660, val loss: 1.0284779071807861
Epoch 670, training loss: 314.8303527832031 = 0.7085056304931641 + 50.0 * 6.282436847686768
Epoch 670, val loss: 1.0217547416687012
Epoch 680, training loss: 314.7607116699219 = 0.6937680244445801 + 50.0 * 6.281339168548584
Epoch 680, val loss: 1.0154097080230713
Epoch 690, training loss: 315.0811767578125 = 0.6793169975280762 + 50.0 * 6.288037300109863
Epoch 690, val loss: 1.0092358589172363
Epoch 700, training loss: 314.6162109375 = 0.6651182174682617 + 50.0 * 6.279021739959717
Epoch 700, val loss: 1.0034924745559692
Epoch 710, training loss: 314.5386047363281 = 0.6513769626617432 + 50.0 * 6.277744770050049
Epoch 710, val loss: 0.997967541217804
Epoch 720, training loss: 314.4775390625 = 0.6379902362823486 + 50.0 * 6.276790618896484
Epoch 720, val loss: 0.9928981065750122
Epoch 730, training loss: 314.55267333984375 = 0.6248176693916321 + 50.0 * 6.278556823730469
Epoch 730, val loss: 0.9879705309867859
Epoch 740, training loss: 314.3507995605469 = 0.6118553280830383 + 50.0 * 6.274778842926025
Epoch 740, val loss: 0.9832311272621155
Epoch 750, training loss: 314.3998718261719 = 0.5992307662963867 + 50.0 * 6.276012420654297
Epoch 750, val loss: 0.9789393544197083
Epoch 760, training loss: 314.1855773925781 = 0.5869059562683105 + 50.0 * 6.271973133087158
Epoch 760, val loss: 0.9747353792190552
Epoch 770, training loss: 314.27484130859375 = 0.5748363137245178 + 50.0 * 6.27400016784668
Epoch 770, val loss: 0.9710109233856201
Epoch 780, training loss: 314.1861572265625 = 0.5629656314849854 + 50.0 * 6.272463798522949
Epoch 780, val loss: 0.9673057198524475
Epoch 790, training loss: 314.0166320800781 = 0.5514045357704163 + 50.0 * 6.2693047523498535
Epoch 790, val loss: 0.9637695550918579
Epoch 800, training loss: 313.9484558105469 = 0.5400729179382324 + 50.0 * 6.268167972564697
Epoch 800, val loss: 0.9605899453163147
Epoch 810, training loss: 314.02001953125 = 0.5289546251296997 + 50.0 * 6.2698211669921875
Epoch 810, val loss: 0.9575735330581665
Epoch 820, training loss: 314.0089416503906 = 0.517951250076294 + 50.0 * 6.269820213317871
Epoch 820, val loss: 0.9546143412590027
Epoch 830, training loss: 313.9028625488281 = 0.5070728063583374 + 50.0 * 6.267915725708008
Epoch 830, val loss: 0.951866865158081
Epoch 840, training loss: 313.791748046875 = 0.4964197278022766 + 50.0 * 6.26590633392334
Epoch 840, val loss: 0.9494301080703735
Epoch 850, training loss: 313.653564453125 = 0.4860157370567322 + 50.0 * 6.263350963592529
Epoch 850, val loss: 0.9472816586494446
Epoch 860, training loss: 313.62152099609375 = 0.4758349359035492 + 50.0 * 6.262913703918457
Epoch 860, val loss: 0.9452974200248718
Epoch 870, training loss: 313.811279296875 = 0.46585968136787415 + 50.0 * 6.266908645629883
Epoch 870, val loss: 0.9434345960617065
Epoch 880, training loss: 313.658447265625 = 0.45589494705200195 + 50.0 * 6.2640509605407715
Epoch 880, val loss: 0.9414708614349365
Epoch 890, training loss: 313.5835876464844 = 0.44614213705062866 + 50.0 * 6.262748718261719
Epoch 890, val loss: 0.9401264786720276
Epoch 900, training loss: 313.4619140625 = 0.4365215301513672 + 50.0 * 6.260507583618164
Epoch 900, val loss: 0.9388575553894043
Epoch 910, training loss: 313.36767578125 = 0.4272095561027527 + 50.0 * 6.2588090896606445
Epoch 910, val loss: 0.93769770860672
Epoch 920, training loss: 313.7107238769531 = 0.41797739267349243 + 50.0 * 6.265854835510254
Epoch 920, val loss: 0.9366089105606079
Epoch 930, training loss: 313.2932434082031 = 0.4088800549507141 + 50.0 * 6.257687091827393
Epoch 930, val loss: 0.935661256313324
Epoch 940, training loss: 313.24041748046875 = 0.400003045797348 + 50.0 * 6.256808280944824
Epoch 940, val loss: 0.9349390268325806
Epoch 950, training loss: 313.255615234375 = 0.39130356907844543 + 50.0 * 6.257286071777344
Epoch 950, val loss: 0.9341838359832764
Epoch 960, training loss: 313.15460205078125 = 0.38272324204444885 + 50.0 * 6.25543737411499
Epoch 960, val loss: 0.9336464405059814
Epoch 970, training loss: 313.1738586425781 = 0.37432342767715454 + 50.0 * 6.255990505218506
Epoch 970, val loss: 0.9332836866378784
Epoch 980, training loss: 313.0806579589844 = 0.36601772904396057 + 50.0 * 6.2542924880981445
Epoch 980, val loss: 0.9331601858139038
Epoch 990, training loss: 313.0421447753906 = 0.35785946249961853 + 50.0 * 6.25368595123291
Epoch 990, val loss: 0.933314859867096
Epoch 1000, training loss: 313.02972412109375 = 0.34995436668395996 + 50.0 * 6.25359582901001
Epoch 1000, val loss: 0.9333993792533875
Epoch 1010, training loss: 313.02777099609375 = 0.34216055274009705 + 50.0 * 6.253712177276611
Epoch 1010, val loss: 0.9334815144538879
Epoch 1020, training loss: 312.89874267578125 = 0.33441320061683655 + 50.0 * 6.251286506652832
Epoch 1020, val loss: 0.9340361952781677
Epoch 1030, training loss: 312.8643798828125 = 0.3269375264644623 + 50.0 * 6.250748634338379
Epoch 1030, val loss: 0.9345148205757141
Epoch 1040, training loss: 312.9550476074219 = 0.3196074664592743 + 50.0 * 6.252708911895752
Epoch 1040, val loss: 0.9351323246955872
Epoch 1050, training loss: 312.797607421875 = 0.3123229444026947 + 50.0 * 6.249705791473389
Epoch 1050, val loss: 0.9358293414115906
Epoch 1060, training loss: 312.7975158691406 = 0.3051817715167999 + 50.0 * 6.249846935272217
Epoch 1060, val loss: 0.9365620017051697
Epoch 1070, training loss: 312.7787780761719 = 0.29823464155197144 + 50.0 * 6.249610424041748
Epoch 1070, val loss: 0.937576949596405
Epoch 1080, training loss: 312.78839111328125 = 0.2914213240146637 + 50.0 * 6.249939441680908
Epoch 1080, val loss: 0.9385225772857666
Epoch 1090, training loss: 312.7311706542969 = 0.28464600443840027 + 50.0 * 6.24893045425415
Epoch 1090, val loss: 0.9395698308944702
Epoch 1100, training loss: 312.58514404296875 = 0.2780732810497284 + 50.0 * 6.24614143371582
Epoch 1100, val loss: 0.9408054947853088
Epoch 1110, training loss: 312.5455017089844 = 0.27170276641845703 + 50.0 * 6.245476245880127
Epoch 1110, val loss: 0.9420597553253174
Epoch 1120, training loss: 312.5779724121094 = 0.2655067443847656 + 50.0 * 6.246249198913574
Epoch 1120, val loss: 0.9434122443199158
Epoch 1130, training loss: 312.56463623046875 = 0.2593221366405487 + 50.0 * 6.2461066246032715
Epoch 1130, val loss: 0.9448898434638977
Epoch 1140, training loss: 312.5152282714844 = 0.25322988629341125 + 50.0 * 6.245239734649658
Epoch 1140, val loss: 0.9461316466331482
Epoch 1150, training loss: 312.43609619140625 = 0.24729831516742706 + 50.0 * 6.243776321411133
Epoch 1150, val loss: 0.9479157328605652
Epoch 1160, training loss: 312.364990234375 = 0.2416003942489624 + 50.0 * 6.242467880249023
Epoch 1160, val loss: 0.9496497511863708
Epoch 1170, training loss: 312.4227294921875 = 0.23601149022579193 + 50.0 * 6.243734359741211
Epoch 1170, val loss: 0.9513486623764038
Epoch 1180, training loss: 312.38458251953125 = 0.23045894503593445 + 50.0 * 6.243082523345947
Epoch 1180, val loss: 0.953144907951355
Epoch 1190, training loss: 312.3977355957031 = 0.2250276803970337 + 50.0 * 6.2434539794921875
Epoch 1190, val loss: 0.9548603296279907
Epoch 1200, training loss: 312.29345703125 = 0.21978473663330078 + 50.0 * 6.241473197937012
Epoch 1200, val loss: 0.956955075263977
Epoch 1210, training loss: 312.31451416015625 = 0.21468138694763184 + 50.0 * 6.2419962882995605
Epoch 1210, val loss: 0.9591078162193298
Epoch 1220, training loss: 312.25335693359375 = 0.20967352390289307 + 50.0 * 6.240873336791992
Epoch 1220, val loss: 0.9613386392593384
Epoch 1230, training loss: 312.4042663574219 = 0.20480583608150482 + 50.0 * 6.243988990783691
Epoch 1230, val loss: 0.9636296033859253
Epoch 1240, training loss: 312.2101745605469 = 0.19999298453330994 + 50.0 * 6.240203857421875
Epoch 1240, val loss: 0.9656178951263428
Epoch 1250, training loss: 312.14666748046875 = 0.19536274671554565 + 50.0 * 6.239025592803955
Epoch 1250, val loss: 0.9681447148323059
Epoch 1260, training loss: 312.4612121582031 = 0.19084107875823975 + 50.0 * 6.245407581329346
Epoch 1260, val loss: 0.9706993699073792
Epoch 1270, training loss: 312.26971435546875 = 0.18638384342193604 + 50.0 * 6.241666793823242
Epoch 1270, val loss: 0.9724798202514648
Epoch 1280, training loss: 312.3138732910156 = 0.18199004232883453 + 50.0 * 6.242637634277344
Epoch 1280, val loss: 0.975222110748291
Epoch 1290, training loss: 312.038818359375 = 0.17769767343997955 + 50.0 * 6.237222194671631
Epoch 1290, val loss: 0.9779780507087708
Epoch 1300, training loss: 312.0133972167969 = 0.1736130565404892 + 50.0 * 6.236795425415039
Epoch 1300, val loss: 0.9805130362510681
Epoch 1310, training loss: 311.9480285644531 = 0.16962559521198273 + 50.0 * 6.235568046569824
Epoch 1310, val loss: 0.9834277033805847
Epoch 1320, training loss: 312.0210266113281 = 0.16576668620109558 + 50.0 * 6.237105369567871
Epoch 1320, val loss: 0.9863036274909973
Epoch 1330, training loss: 312.0758056640625 = 0.16194377839565277 + 50.0 * 6.238276958465576
Epoch 1330, val loss: 0.9889043569564819
Epoch 1340, training loss: 311.9423828125 = 0.15814773738384247 + 50.0 * 6.235684871673584
Epoch 1340, val loss: 0.9919737577438354
Epoch 1350, training loss: 311.8949890136719 = 0.15452782809734344 + 50.0 * 6.234808921813965
Epoch 1350, val loss: 0.9952166676521301
Epoch 1360, training loss: 312.00213623046875 = 0.15101845562458038 + 50.0 * 6.2370219230651855
Epoch 1360, val loss: 0.9983684420585632
Epoch 1370, training loss: 312.0118713378906 = 0.14758887887001038 + 50.0 * 6.237285614013672
Epoch 1370, val loss: 1.0011845827102661
Epoch 1380, training loss: 311.8876037597656 = 0.1441229283809662 + 50.0 * 6.234869480133057
Epoch 1380, val loss: 1.0046532154083252
Epoch 1390, training loss: 311.8641052246094 = 0.14085336029529572 + 50.0 * 6.234465599060059
Epoch 1390, val loss: 1.0078842639923096
Epoch 1400, training loss: 311.7952880859375 = 0.13764062523841858 + 50.0 * 6.233152866363525
Epoch 1400, val loss: 1.0113575458526611
Epoch 1410, training loss: 311.7079772949219 = 0.13454023003578186 + 50.0 * 6.231468200683594
Epoch 1410, val loss: 1.014718770980835
Epoch 1420, training loss: 311.7693786621094 = 0.13152790069580078 + 50.0 * 6.232757091522217
Epoch 1420, val loss: 1.018322467803955
Epoch 1430, training loss: 311.7893371582031 = 0.1285582035779953 + 50.0 * 6.23321533203125
Epoch 1430, val loss: 1.021549105644226
Epoch 1440, training loss: 311.81121826171875 = 0.12566320598125458 + 50.0 * 6.233710765838623
Epoch 1440, val loss: 1.0248407125473022
Epoch 1450, training loss: 311.71148681640625 = 0.12279730290174484 + 50.0 * 6.231773376464844
Epoch 1450, val loss: 1.028616189956665
Epoch 1460, training loss: 311.62957763671875 = 0.12004119157791138 + 50.0 * 6.230190277099609
Epoch 1460, val loss: 1.0323628187179565
Epoch 1470, training loss: 311.7117614746094 = 0.11738122999668121 + 50.0 * 6.2318878173828125
Epoch 1470, val loss: 1.0359829664230347
Epoch 1480, training loss: 311.651123046875 = 0.11476732045412064 + 50.0 * 6.230726718902588
Epoch 1480, val loss: 1.0398180484771729
Epoch 1490, training loss: 311.72625732421875 = 0.11223013699054718 + 50.0 * 6.232280254364014
Epoch 1490, val loss: 1.0432300567626953
Epoch 1500, training loss: 311.5657043457031 = 0.1096998006105423 + 50.0 * 6.229119777679443
Epoch 1500, val loss: 1.047202229499817
Epoch 1510, training loss: 311.5838928222656 = 0.10727619379758835 + 50.0 * 6.229532241821289
Epoch 1510, val loss: 1.0509939193725586
Epoch 1520, training loss: 311.6513977050781 = 0.10493680834770203 + 50.0 * 6.230928897857666
Epoch 1520, val loss: 1.0547429323196411
Epoch 1530, training loss: 311.5718994140625 = 0.10262582451105118 + 50.0 * 6.2293853759765625
Epoch 1530, val loss: 1.0587149858474731
Epoch 1540, training loss: 311.4932861328125 = 0.1003609225153923 + 50.0 * 6.227858543395996
Epoch 1540, val loss: 1.0626908540725708
Epoch 1550, training loss: 311.4811096191406 = 0.09818357229232788 + 50.0 * 6.227658748626709
Epoch 1550, val loss: 1.066795825958252
Epoch 1560, training loss: 311.5457763671875 = 0.09606009721755981 + 50.0 * 6.228994369506836
Epoch 1560, val loss: 1.0708136558532715
Epoch 1570, training loss: 311.5300598144531 = 0.09400240331888199 + 50.0 * 6.2287211418151855
Epoch 1570, val loss: 1.0745928287506104
Epoch 1580, training loss: 311.4498291015625 = 0.0919264554977417 + 50.0 * 6.227158069610596
Epoch 1580, val loss: 1.0787944793701172
Epoch 1590, training loss: 311.4165344238281 = 0.08994431048631668 + 50.0 * 6.226531982421875
Epoch 1590, val loss: 1.0825459957122803
Epoch 1600, training loss: 311.5216369628906 = 0.08801943063735962 + 50.0 * 6.228672504425049
Epoch 1600, val loss: 1.0870245695114136
Epoch 1610, training loss: 311.4215393066406 = 0.08613798022270203 + 50.0 * 6.226707458496094
Epoch 1610, val loss: 1.0909333229064941
Epoch 1620, training loss: 311.354736328125 = 0.08429259806871414 + 50.0 * 6.225409030914307
Epoch 1620, val loss: 1.094984531402588
Epoch 1630, training loss: 311.3376770019531 = 0.08252672851085663 + 50.0 * 6.225103378295898
Epoch 1630, val loss: 1.0993226766586304
Epoch 1640, training loss: 311.4405517578125 = 0.08080244809389114 + 50.0 * 6.227194786071777
Epoch 1640, val loss: 1.1037653684616089
Epoch 1650, training loss: 311.29522705078125 = 0.07911001890897751 + 50.0 * 6.224322319030762
Epoch 1650, val loss: 1.1076910495758057
Epoch 1660, training loss: 311.2852783203125 = 0.07746689021587372 + 50.0 * 6.224156379699707
Epoch 1660, val loss: 1.1118998527526855
Epoch 1670, training loss: 311.225830078125 = 0.07587666809558868 + 50.0 * 6.222999572753906
Epoch 1670, val loss: 1.116334080696106
Epoch 1680, training loss: 311.3713684082031 = 0.07433407008647919 + 50.0 * 6.225940704345703
Epoch 1680, val loss: 1.1204673051834106
Epoch 1690, training loss: 311.4970397949219 = 0.07278093695640564 + 50.0 * 6.228485107421875
Epoch 1690, val loss: 1.1243484020233154
Epoch 1700, training loss: 311.3132019042969 = 0.07125608623027802 + 50.0 * 6.224838733673096
Epoch 1700, val loss: 1.129213809967041
Epoch 1710, training loss: 311.2383728027344 = 0.06978996843099594 + 50.0 * 6.223371505737305
Epoch 1710, val loss: 1.1331901550292969
Epoch 1720, training loss: 311.19537353515625 = 0.0683838427066803 + 50.0 * 6.22253942489624
Epoch 1720, val loss: 1.1378206014633179
Epoch 1730, training loss: 311.2695007324219 = 0.06701979786157608 + 50.0 * 6.2240495681762695
Epoch 1730, val loss: 1.1420315504074097
Epoch 1740, training loss: 311.3118591308594 = 0.0656755194067955 + 50.0 * 6.224923610687256
Epoch 1740, val loss: 1.1462284326553345
Epoch 1750, training loss: 311.1946105957031 = 0.064335897564888 + 50.0 * 6.2226057052612305
Epoch 1750, val loss: 1.1504828929901123
Epoch 1760, training loss: 311.12841796875 = 0.06305039674043655 + 50.0 * 6.221307277679443
Epoch 1760, val loss: 1.1548486948013306
Epoch 1770, training loss: 311.17022705078125 = 0.061810463666915894 + 50.0 * 6.222168445587158
Epoch 1770, val loss: 1.1592903137207031
Epoch 1780, training loss: 311.1112365722656 = 0.060599301010370255 + 50.0 * 6.221012592315674
Epoch 1780, val loss: 1.1634057760238647
Epoch 1790, training loss: 311.0865783691406 = 0.05942299962043762 + 50.0 * 6.220542907714844
Epoch 1790, val loss: 1.1676383018493652
Epoch 1800, training loss: 311.1236877441406 = 0.05828148126602173 + 50.0 * 6.22130823135376
Epoch 1800, val loss: 1.1719011068344116
Epoch 1810, training loss: 311.1857604980469 = 0.05715912580490112 + 50.0 * 6.222572326660156
Epoch 1810, val loss: 1.1760612726211548
Epoch 1820, training loss: 311.1661376953125 = 0.05605153739452362 + 50.0 * 6.222201824188232
Epoch 1820, val loss: 1.180540680885315
Epoch 1830, training loss: 311.24261474609375 = 0.054950471967458725 + 50.0 * 6.223752975463867
Epoch 1830, val loss: 1.1847724914550781
Epoch 1840, training loss: 311.0552978515625 = 0.05389517918229103 + 50.0 * 6.220027923583984
Epoch 1840, val loss: 1.1891202926635742
Epoch 1850, training loss: 311.0289001464844 = 0.05286809429526329 + 50.0 * 6.2195210456848145
Epoch 1850, val loss: 1.1932673454284668
Epoch 1860, training loss: 311.0474548339844 = 0.05188143998384476 + 50.0 * 6.219911575317383
Epoch 1860, val loss: 1.1977168321609497
Epoch 1870, training loss: 311.0821533203125 = 0.050912704318761826 + 50.0 * 6.220624923706055
Epoch 1870, val loss: 1.2021386623382568
Epoch 1880, training loss: 310.94244384765625 = 0.04996389523148537 + 50.0 * 6.2178497314453125
Epoch 1880, val loss: 1.206300973892212
Epoch 1890, training loss: 311.05364990234375 = 0.04905211925506592 + 50.0 * 6.220092296600342
Epoch 1890, val loss: 1.2107266187667847
Epoch 1900, training loss: 311.0252685546875 = 0.048146188259124756 + 50.0 * 6.219542026519775
Epoch 1900, val loss: 1.2146532535552979
Epoch 1910, training loss: 310.97247314453125 = 0.047252535820007324 + 50.0 * 6.218504428863525
Epoch 1910, val loss: 1.218845009803772
Epoch 1920, training loss: 310.94970703125 = 0.046389348804950714 + 50.0 * 6.218066215515137
Epoch 1920, val loss: 1.2229372262954712
Epoch 1930, training loss: 310.9652404785156 = 0.04555295780301094 + 50.0 * 6.218393802642822
Epoch 1930, val loss: 1.2273026704788208
Epoch 1940, training loss: 310.94537353515625 = 0.04473887011408806 + 50.0 * 6.218012809753418
Epoch 1940, val loss: 1.231410026550293
Epoch 1950, training loss: 311.2239990234375 = 0.04394907131791115 + 50.0 * 6.2236008644104
Epoch 1950, val loss: 1.2353706359863281
Epoch 1960, training loss: 310.9549560546875 = 0.04311686009168625 + 50.0 * 6.218236446380615
Epoch 1960, val loss: 1.2400704622268677
Epoch 1970, training loss: 310.86602783203125 = 0.042358849197626114 + 50.0 * 6.216473579406738
Epoch 1970, val loss: 1.2439322471618652
Epoch 1980, training loss: 310.8076477050781 = 0.04161518067121506 + 50.0 * 6.215320587158203
Epoch 1980, val loss: 1.2483776807785034
Epoch 1990, training loss: 310.8072814941406 = 0.040902622044086456 + 50.0 * 6.215327739715576
Epoch 1990, val loss: 1.252421259880066
Epoch 2000, training loss: 311.18963623046875 = 0.040219079703092575 + 50.0 * 6.222988128662109
Epoch 2000, val loss: 1.2566372156143188
Epoch 2010, training loss: 311.1504211425781 = 0.039482805877923965 + 50.0 * 6.2222185134887695
Epoch 2010, val loss: 1.2608307600021362
Epoch 2020, training loss: 310.80645751953125 = 0.03877793252468109 + 50.0 * 6.215353488922119
Epoch 2020, val loss: 1.2643659114837646
Epoch 2030, training loss: 310.76824951171875 = 0.038115400820970535 + 50.0 * 6.214602947235107
Epoch 2030, val loss: 1.2687926292419434
Epoch 2040, training loss: 310.7659606933594 = 0.03747288137674332 + 50.0 * 6.214569568634033
Epoch 2040, val loss: 1.2731635570526123
Epoch 2050, training loss: 311.00274658203125 = 0.03685057535767555 + 50.0 * 6.219317436218262
Epoch 2050, val loss: 1.277497410774231
Epoch 2060, training loss: 310.83056640625 = 0.03622335568070412 + 50.0 * 6.215887069702148
Epoch 2060, val loss: 1.2807079553604126
Epoch 2070, training loss: 310.7787170410156 = 0.03560130298137665 + 50.0 * 6.21486234664917
Epoch 2070, val loss: 1.2851524353027344
Epoch 2080, training loss: 310.96856689453125 = 0.0350196436047554 + 50.0 * 6.21867036819458
Epoch 2080, val loss: 1.2886967658996582
Epoch 2090, training loss: 310.78387451171875 = 0.03442182019352913 + 50.0 * 6.214989185333252
Epoch 2090, val loss: 1.2931289672851562
Epoch 2100, training loss: 310.7184753417969 = 0.033845338970422745 + 50.0 * 6.213692665100098
Epoch 2100, val loss: 1.297179102897644
Epoch 2110, training loss: 310.6854248046875 = 0.03330228477716446 + 50.0 * 6.213042736053467
Epoch 2110, val loss: 1.3012018203735352
Epoch 2120, training loss: 310.69677734375 = 0.032773297280073166 + 50.0 * 6.213280200958252
Epoch 2120, val loss: 1.3052488565444946
Epoch 2130, training loss: 310.8555908203125 = 0.032256729900836945 + 50.0 * 6.216466426849365
Epoch 2130, val loss: 1.3092869520187378
Epoch 2140, training loss: 310.79974365234375 = 0.03172144293785095 + 50.0 * 6.215360641479492
Epoch 2140, val loss: 1.313146710395813
Epoch 2150, training loss: 310.7323913574219 = 0.03120298683643341 + 50.0 * 6.214024066925049
Epoch 2150, val loss: 1.316932201385498
Epoch 2160, training loss: 310.6482849121094 = 0.030702903866767883 + 50.0 * 6.2123517990112305
Epoch 2160, val loss: 1.3205374479293823
Epoch 2170, training loss: 310.65179443359375 = 0.030211687088012695 + 50.0 * 6.212431907653809
Epoch 2170, val loss: 1.3248628377914429
Epoch 2180, training loss: 310.7494201660156 = 0.029751116409897804 + 50.0 * 6.214393615722656
Epoch 2180, val loss: 1.3283038139343262
Epoch 2190, training loss: 310.6636962890625 = 0.029278656467795372 + 50.0 * 6.212688446044922
Epoch 2190, val loss: 1.3325350284576416
Epoch 2200, training loss: 310.668701171875 = 0.028822321444749832 + 50.0 * 6.21279764175415
Epoch 2200, val loss: 1.3365201950073242
Epoch 2210, training loss: 310.6806640625 = 0.0283811092376709 + 50.0 * 6.213045597076416
Epoch 2210, val loss: 1.3401405811309814
Epoch 2220, training loss: 310.7611389160156 = 0.02794444002211094 + 50.0 * 6.214663505554199
Epoch 2220, val loss: 1.344023585319519
Epoch 2230, training loss: 310.75274658203125 = 0.02750813215970993 + 50.0 * 6.214505195617676
Epoch 2230, val loss: 1.3475570678710938
Epoch 2240, training loss: 310.59979248046875 = 0.02708667889237404 + 50.0 * 6.211453914642334
Epoch 2240, val loss: 1.3511600494384766
Epoch 2250, training loss: 310.5513916015625 = 0.02667844481766224 + 50.0 * 6.210494518280029
Epoch 2250, val loss: 1.3552422523498535
Epoch 2260, training loss: 310.5612487792969 = 0.02628730982542038 + 50.0 * 6.210699558258057
Epoch 2260, val loss: 1.3591687679290771
Epoch 2270, training loss: 310.8074645996094 = 0.025904806330800056 + 50.0 * 6.215631008148193
Epoch 2270, val loss: 1.3627700805664062
Epoch 2280, training loss: 310.6004638671875 = 0.02550750970840454 + 50.0 * 6.211498737335205
Epoch 2280, val loss: 1.3663114309310913
Epoch 2290, training loss: 310.6138000488281 = 0.025126179680228233 + 50.0 * 6.21177339553833
Epoch 2290, val loss: 1.369780421257019
Epoch 2300, training loss: 310.5513000488281 = 0.024755245074629784 + 50.0 * 6.210530757904053
Epoch 2300, val loss: 1.373100996017456
Epoch 2310, training loss: 310.5341491699219 = 0.024401143193244934 + 50.0 * 6.2101945877075195
Epoch 2310, val loss: 1.3768317699432373
Epoch 2320, training loss: 310.8229064941406 = 0.02405521646142006 + 50.0 * 6.215977191925049
Epoch 2320, val loss: 1.3807456493377686
Epoch 2330, training loss: 310.6114501953125 = 0.023687394335865974 + 50.0 * 6.211755275726318
Epoch 2330, val loss: 1.3840296268463135
Epoch 2340, training loss: 310.490234375 = 0.023350780829787254 + 50.0 * 6.2093377113342285
Epoch 2340, val loss: 1.3876620531082153
Epoch 2350, training loss: 310.484375 = 0.02302500233054161 + 50.0 * 6.209227085113525
Epoch 2350, val loss: 1.390889286994934
Epoch 2360, training loss: 310.94415283203125 = 0.022718675434589386 + 50.0 * 6.218429088592529
Epoch 2360, val loss: 1.3941482305526733
Epoch 2370, training loss: 310.7082214355469 = 0.022370807826519012 + 50.0 * 6.213716983795166
Epoch 2370, val loss: 1.397979736328125
Epoch 2380, training loss: 310.4947509765625 = 0.022048676386475563 + 50.0 * 6.209454536437988
Epoch 2380, val loss: 1.4010998010635376
Epoch 2390, training loss: 310.4434814453125 = 0.02174815908074379 + 50.0 * 6.208434581756592
Epoch 2390, val loss: 1.4050405025482178
Epoch 2400, training loss: 310.4563293457031 = 0.021455414593219757 + 50.0 * 6.208697319030762
Epoch 2400, val loss: 1.4084930419921875
Epoch 2410, training loss: 310.5904541015625 = 0.021165288984775543 + 50.0 * 6.211385250091553
Epoch 2410, val loss: 1.4119447469711304
Epoch 2420, training loss: 310.7516784667969 = 0.020874163135886192 + 50.0 * 6.214616298675537
Epoch 2420, val loss: 1.4153485298156738
Epoch 2430, training loss: 310.5339660644531 = 0.0205830130726099 + 50.0 * 6.210268020629883
Epoch 2430, val loss: 1.4181020259857178
Epoch 2440, training loss: 310.45068359375 = 0.020302141085267067 + 50.0 * 6.2086076736450195
Epoch 2440, val loss: 1.4216147661209106
Epoch 2450, training loss: 310.3929138183594 = 0.020031621679663658 + 50.0 * 6.207458019256592
Epoch 2450, val loss: 1.425127387046814
Epoch 2460, training loss: 310.41290283203125 = 0.0197742972522974 + 50.0 * 6.207862854003906
Epoch 2460, val loss: 1.4286115169525146
Epoch 2470, training loss: 310.7996826171875 = 0.019519353285431862 + 50.0 * 6.215602874755859
Epoch 2470, val loss: 1.4321342706680298
Epoch 2480, training loss: 310.5181579589844 = 0.019253244623541832 + 50.0 * 6.209978103637695
Epoch 2480, val loss: 1.4345108270645142
Epoch 2490, training loss: 310.38250732421875 = 0.018992137163877487 + 50.0 * 6.207270622253418
Epoch 2490, val loss: 1.4378782510757446
Epoch 2500, training loss: 310.36785888671875 = 0.01874837838113308 + 50.0 * 6.206982135772705
Epoch 2500, val loss: 1.4413810968399048
Epoch 2510, training loss: 310.66607666015625 = 0.018514104187488556 + 50.0 * 6.212951183319092
Epoch 2510, val loss: 1.4448856115341187
Epoch 2520, training loss: 310.4273376464844 = 0.018272683024406433 + 50.0 * 6.208180904388428
Epoch 2520, val loss: 1.4471391439437866
Epoch 2530, training loss: 310.5065612792969 = 0.018037401139736176 + 50.0 * 6.209770202636719
Epoch 2530, val loss: 1.4502990245819092
Epoch 2540, training loss: 310.3299560546875 = 0.017804859206080437 + 50.0 * 6.206243515014648
Epoch 2540, val loss: 1.453553318977356
Epoch 2550, training loss: 310.33343505859375 = 0.017584767192602158 + 50.0 * 6.206316947937012
Epoch 2550, val loss: 1.4569216966629028
Epoch 2560, training loss: 310.3990783691406 = 0.017373371869325638 + 50.0 * 6.207633972167969
Epoch 2560, val loss: 1.4604277610778809
Epoch 2570, training loss: 310.6391906738281 = 0.017159275710582733 + 50.0 * 6.2124409675598145
Epoch 2570, val loss: 1.4628123044967651
Epoch 2580, training loss: 310.36920166015625 = 0.016925500705838203 + 50.0 * 6.207045555114746
Epoch 2580, val loss: 1.466086745262146
Epoch 2590, training loss: 310.29718017578125 = 0.01671651378273964 + 50.0 * 6.205609321594238
Epoch 2590, val loss: 1.468641757965088
Epoch 2600, training loss: 310.2781066894531 = 0.016516724601387978 + 50.0 * 6.2052321434021
Epoch 2600, val loss: 1.4719717502593994
Epoch 2610, training loss: 310.2933044433594 = 0.016321474686264992 + 50.0 * 6.205539703369141
Epoch 2610, val loss: 1.4751532077789307
Epoch 2620, training loss: 310.564453125 = 0.016131235286593437 + 50.0 * 6.210966110229492
Epoch 2620, val loss: 1.478326678276062
Epoch 2630, training loss: 310.422119140625 = 0.01593111641705036 + 50.0 * 6.208123683929443
Epoch 2630, val loss: 1.4806452989578247
Epoch 2640, training loss: 310.3160400390625 = 0.015738019719719887 + 50.0 * 6.206005573272705
Epoch 2640, val loss: 1.4836232662200928
Epoch 2650, training loss: 310.3353271484375 = 0.01555542927235365 + 50.0 * 6.206395149230957
Epoch 2650, val loss: 1.4866125583648682
Epoch 2660, training loss: 310.5005187988281 = 0.01537462417036295 + 50.0 * 6.209702491760254
Epoch 2660, val loss: 1.4891581535339355
Epoch 2670, training loss: 310.3912353515625 = 0.015181437134742737 + 50.0 * 6.207521438598633
Epoch 2670, val loss: 1.4924652576446533
Epoch 2680, training loss: 310.3243408203125 = 0.015005843713879585 + 50.0 * 6.206186771392822
Epoch 2680, val loss: 1.4951982498168945
Epoch 2690, training loss: 310.2273864746094 = 0.014832974411547184 + 50.0 * 6.204251289367676
Epoch 2690, val loss: 1.497697114944458
Epoch 2700, training loss: 310.2208251953125 = 0.014668455347418785 + 50.0 * 6.204123020172119
Epoch 2700, val loss: 1.5010780096054077
Epoch 2710, training loss: 310.61029052734375 = 0.01451173797249794 + 50.0 * 6.211915493011475
Epoch 2710, val loss: 1.504253625869751
Epoch 2720, training loss: 310.3982849121094 = 0.01433445606380701 + 50.0 * 6.20767879486084
Epoch 2720, val loss: 1.5057135820388794
Epoch 2730, training loss: 310.26495361328125 = 0.014167108573019505 + 50.0 * 6.205016136169434
Epoch 2730, val loss: 1.5092554092407227
Epoch 2740, training loss: 310.218505859375 = 0.01401177141815424 + 50.0 * 6.204090118408203
Epoch 2740, val loss: 1.511418104171753
Epoch 2750, training loss: 310.2506103515625 = 0.013861164450645447 + 50.0 * 6.204734802246094
Epoch 2750, val loss: 1.5143260955810547
Epoch 2760, training loss: 310.3536682128906 = 0.013710232451558113 + 50.0 * 6.206799507141113
Epoch 2760, val loss: 1.5170296430587769
Epoch 2770, training loss: 310.5184326171875 = 0.013552507385611534 + 50.0 * 6.210097789764404
Epoch 2770, val loss: 1.5205228328704834
Epoch 2780, training loss: 310.4039611816406 = 0.013403248973190784 + 50.0 * 6.20781135559082
Epoch 2780, val loss: 1.5217958688735962
Epoch 2790, training loss: 310.18792724609375 = 0.013246501795947552 + 50.0 * 6.203493595123291
Epoch 2790, val loss: 1.5254597663879395
Epoch 2800, training loss: 310.1614990234375 = 0.013107914477586746 + 50.0 * 6.202967643737793
Epoch 2800, val loss: 1.5279074907302856
Epoch 2810, training loss: 310.14874267578125 = 0.012970739044249058 + 50.0 * 6.2027153968811035
Epoch 2810, val loss: 1.5308812856674194
Epoch 2820, training loss: 310.87213134765625 = 0.012841355986893177 + 50.0 * 6.2171854972839355
Epoch 2820, val loss: 1.533584475517273
Epoch 2830, training loss: 310.3622131347656 = 0.012691794894635677 + 50.0 * 6.2069902420043945
Epoch 2830, val loss: 1.5354294776916504
Epoch 2840, training loss: 310.1602478027344 = 0.012549523264169693 + 50.0 * 6.202953815460205
Epoch 2840, val loss: 1.5384546518325806
Epoch 2850, training loss: 310.1045227050781 = 0.012422701343894005 + 50.0 * 6.201842308044434
Epoch 2850, val loss: 1.541032314300537
Epoch 2860, training loss: 310.09356689453125 = 0.012297507375478745 + 50.0 * 6.201625347137451
Epoch 2860, val loss: 1.5440409183502197
Epoch 2870, training loss: 310.1268005371094 = 0.01217652764171362 + 50.0 * 6.202292442321777
Epoch 2870, val loss: 1.5464853048324585
Epoch 2880, training loss: 310.5084228515625 = 0.01205697562545538 + 50.0 * 6.209927558898926
Epoch 2880, val loss: 1.549068808555603
Epoch 2890, training loss: 310.34893798828125 = 0.011924409307539463 + 50.0 * 6.206739902496338
Epoch 2890, val loss: 1.5513405799865723
Epoch 2900, training loss: 310.2105407714844 = 0.011793144047260284 + 50.0 * 6.203975200653076
Epoch 2900, val loss: 1.5539195537567139
Epoch 2910, training loss: 310.1064758300781 = 0.011673682369291782 + 50.0 * 6.201895713806152
Epoch 2910, val loss: 1.5562034845352173
Epoch 2920, training loss: 310.11212158203125 = 0.01155799999833107 + 50.0 * 6.2020111083984375
Epoch 2920, val loss: 1.559323787689209
Epoch 2930, training loss: 310.4285583496094 = 0.011445464566349983 + 50.0 * 6.208342552185059
Epoch 2930, val loss: 1.5617369413375854
Epoch 2940, training loss: 310.18060302734375 = 0.011329314671456814 + 50.0 * 6.203385829925537
Epoch 2940, val loss: 1.564017653465271
Epoch 2950, training loss: 310.09564208984375 = 0.011215323582291603 + 50.0 * 6.201688289642334
Epoch 2950, val loss: 1.566299319267273
Epoch 2960, training loss: 310.05963134765625 = 0.01110805943608284 + 50.0 * 6.200970649719238
Epoch 2960, val loss: 1.5688648223876953
Epoch 2970, training loss: 310.0974426269531 = 0.011004626750946045 + 50.0 * 6.201728343963623
Epoch 2970, val loss: 1.5712997913360596
Epoch 2980, training loss: 310.3891296386719 = 0.010901564732193947 + 50.0 * 6.207564830780029
Epoch 2980, val loss: 1.5736544132232666
Epoch 2990, training loss: 310.2974853515625 = 0.010782518424093723 + 50.0 * 6.2057342529296875
Epoch 2990, val loss: 1.5758575201034546
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8139167105956774
=== training gcn model ===
Epoch 0, training loss: 431.783447265625 = 1.9418846368789673 + 50.0 * 8.596831321716309
Epoch 0, val loss: 1.9439932107925415
Epoch 10, training loss: 431.7337341308594 = 1.9334633350372314 + 50.0 * 8.5960054397583
Epoch 10, val loss: 1.9352588653564453
Epoch 20, training loss: 431.43524169921875 = 1.9227259159088135 + 50.0 * 8.590250015258789
Epoch 20, val loss: 1.9239965677261353
Epoch 30, training loss: 429.5230407714844 = 1.908511996269226 + 50.0 * 8.552290916442871
Epoch 30, val loss: 1.9092156887054443
Epoch 40, training loss: 418.26617431640625 = 1.891099214553833 + 50.0 * 8.32750129699707
Epoch 40, val loss: 1.8915300369262695
Epoch 50, training loss: 384.2330627441406 = 1.8720581531524658 + 50.0 * 7.647219657897949
Epoch 50, val loss: 1.872563123703003
Epoch 60, training loss: 368.4141845703125 = 1.8581149578094482 + 50.0 * 7.33112096786499
Epoch 60, val loss: 1.8599562644958496
Epoch 70, training loss: 355.4025573730469 = 1.8474102020263672 + 50.0 * 7.071102619171143
Epoch 70, val loss: 1.849353313446045
Epoch 80, training loss: 346.2502136230469 = 1.8381990194320679 + 50.0 * 6.888240337371826
Epoch 80, val loss: 1.8408387899398804
Epoch 90, training loss: 341.5753479003906 = 1.8299453258514404 + 50.0 * 6.794908046722412
Epoch 90, val loss: 1.8327293395996094
Epoch 100, training loss: 337.8303527832031 = 1.8217519521713257 + 50.0 * 6.720171928405762
Epoch 100, val loss: 1.8247071504592896
Epoch 110, training loss: 334.9109802246094 = 1.813884973526001 + 50.0 * 6.661942005157471
Epoch 110, val loss: 1.817064642906189
Epoch 120, training loss: 332.9645080566406 = 1.8065640926361084 + 50.0 * 6.623159408569336
Epoch 120, val loss: 1.809940218925476
Epoch 130, training loss: 331.5010070800781 = 1.7992358207702637 + 50.0 * 6.594035625457764
Epoch 130, val loss: 1.8028569221496582
Epoch 140, training loss: 330.180908203125 = 1.791643500328064 + 50.0 * 6.567785263061523
Epoch 140, val loss: 1.7957332134246826
Epoch 150, training loss: 329.0573425292969 = 1.7837104797363281 + 50.0 * 6.545472145080566
Epoch 150, val loss: 1.788495421409607
Epoch 160, training loss: 328.2842712402344 = 1.7753958702087402 + 50.0 * 6.530177593231201
Epoch 160, val loss: 1.78104567527771
Epoch 170, training loss: 327.300537109375 = 1.7664419412612915 + 50.0 * 6.510681629180908
Epoch 170, val loss: 1.773173451423645
Epoch 180, training loss: 326.57550048828125 = 1.7567578554153442 + 50.0 * 6.49637508392334
Epoch 180, val loss: 1.7647596597671509
Epoch 190, training loss: 325.88275146484375 = 1.7463152408599854 + 50.0 * 6.482728958129883
Epoch 190, val loss: 1.7558737993240356
Epoch 200, training loss: 325.2589416503906 = 1.7350479364395142 + 50.0 * 6.470478057861328
Epoch 200, val loss: 1.7463057041168213
Epoch 210, training loss: 324.88812255859375 = 1.7228374481201172 + 50.0 * 6.463305950164795
Epoch 210, val loss: 1.735980749130249
Epoch 220, training loss: 324.18939208984375 = 1.7094953060150146 + 50.0 * 6.4495978355407715
Epoch 220, val loss: 1.7249252796173096
Epoch 230, training loss: 323.6590270996094 = 1.6951547861099243 + 50.0 * 6.439277172088623
Epoch 230, val loss: 1.7130755186080933
Epoch 240, training loss: 323.29644775390625 = 1.6797153949737549 + 50.0 * 6.4323344230651855
Epoch 240, val loss: 1.7003653049468994
Epoch 250, training loss: 322.7646789550781 = 1.6631122827529907 + 50.0 * 6.422031402587891
Epoch 250, val loss: 1.6867202520370483
Epoch 260, training loss: 322.36749267578125 = 1.645450472831726 + 50.0 * 6.414441108703613
Epoch 260, val loss: 1.6722326278686523
Epoch 270, training loss: 322.0994873046875 = 1.6266616582870483 + 50.0 * 6.409456729888916
Epoch 270, val loss: 1.6569234132766724
Epoch 280, training loss: 321.61572265625 = 1.6068729162216187 + 50.0 * 6.400177001953125
Epoch 280, val loss: 1.6408112049102783
Epoch 290, training loss: 321.27301025390625 = 1.5861760377883911 + 50.0 * 6.393736362457275
Epoch 290, val loss: 1.6240426301956177
Epoch 300, training loss: 321.0303039550781 = 1.5646796226501465 + 50.0 * 6.389312744140625
Epoch 300, val loss: 1.6067166328430176
Epoch 310, training loss: 320.7093811035156 = 1.5423455238342285 + 50.0 * 6.383340358734131
Epoch 310, val loss: 1.5886831283569336
Epoch 320, training loss: 320.4275817871094 = 1.5193614959716797 + 50.0 * 6.378164768218994
Epoch 320, val loss: 1.570494532585144
Epoch 330, training loss: 320.1412048339844 = 1.4959710836410522 + 50.0 * 6.3729047775268555
Epoch 330, val loss: 1.5520046949386597
Epoch 340, training loss: 319.880126953125 = 1.472201943397522 + 50.0 * 6.36815881729126
Epoch 340, val loss: 1.533381462097168
Epoch 350, training loss: 319.6611022949219 = 1.4481196403503418 + 50.0 * 6.364259719848633
Epoch 350, val loss: 1.514784812927246
Epoch 360, training loss: 319.71856689453125 = 1.423614740371704 + 50.0 * 6.365899085998535
Epoch 360, val loss: 1.4959648847579956
Epoch 370, training loss: 319.3064270019531 = 1.399072289466858 + 50.0 * 6.358147144317627
Epoch 370, val loss: 1.477268099784851
Epoch 380, training loss: 319.0087585449219 = 1.3744637966156006 + 50.0 * 6.352685928344727
Epoch 380, val loss: 1.4588998556137085
Epoch 390, training loss: 318.7898864746094 = 1.349875569343567 + 50.0 * 6.348800182342529
Epoch 390, val loss: 1.440650463104248
Epoch 400, training loss: 318.6533508300781 = 1.325300693511963 + 50.0 * 6.346561431884766
Epoch 400, val loss: 1.42267644405365
Epoch 410, training loss: 318.78411865234375 = 1.3006752729415894 + 50.0 * 6.349668502807617
Epoch 410, val loss: 1.4045803546905518
Epoch 420, training loss: 318.3634948730469 = 1.276140570640564 + 50.0 * 6.341746807098389
Epoch 420, val loss: 1.3870588541030884
Epoch 430, training loss: 318.0863037109375 = 1.2518672943115234 + 50.0 * 6.336688995361328
Epoch 430, val loss: 1.3699012994766235
Epoch 440, training loss: 317.9132080078125 = 1.2278597354888916 + 50.0 * 6.333706378936768
Epoch 440, val loss: 1.3531651496887207
Epoch 450, training loss: 317.8027648925781 = 1.2040232419967651 + 50.0 * 6.331974983215332
Epoch 450, val loss: 1.3365298509597778
Epoch 460, training loss: 317.646728515625 = 1.1804618835449219 + 50.0 * 6.3293256759643555
Epoch 460, val loss: 1.3205645084381104
Epoch 470, training loss: 317.4586181640625 = 1.1572598218917847 + 50.0 * 6.3260273933410645
Epoch 470, val loss: 1.3050684928894043
Epoch 480, training loss: 317.3818054199219 = 1.1345241069793701 + 50.0 * 6.324945449829102
Epoch 480, val loss: 1.290122389793396
Epoch 490, training loss: 317.3760986328125 = 1.1120193004608154 + 50.0 * 6.325281143188477
Epoch 490, val loss: 1.2753809690475464
Epoch 500, training loss: 317.02825927734375 = 1.0900384187698364 + 50.0 * 6.318764686584473
Epoch 500, val loss: 1.2614037990570068
Epoch 510, training loss: 316.90374755859375 = 1.0686237812042236 + 50.0 * 6.316702365875244
Epoch 510, val loss: 1.2480310201644897
Epoch 520, training loss: 316.85821533203125 = 1.0476741790771484 + 50.0 * 6.316210746765137
Epoch 520, val loss: 1.2352501153945923
Epoch 530, training loss: 316.6492919921875 = 1.0272183418273926 + 50.0 * 6.312441349029541
Epoch 530, val loss: 1.22300124168396
Epoch 540, training loss: 316.72265625 = 1.0072927474975586 + 50.0 * 6.31430721282959
Epoch 540, val loss: 1.2112765312194824
Epoch 550, training loss: 316.4471130371094 = 0.9878423810005188 + 50.0 * 6.309185028076172
Epoch 550, val loss: 1.2002044916152954
Epoch 560, training loss: 316.27606201171875 = 0.9689972996711731 + 50.0 * 6.306141376495361
Epoch 560, val loss: 1.1897531747817993
Epoch 570, training loss: 316.2596130371094 = 0.9506635069847107 + 50.0 * 6.306179046630859
Epoch 570, val loss: 1.1798149347305298
Epoch 580, training loss: 316.4654235839844 = 0.9327471256256104 + 50.0 * 6.3106536865234375
Epoch 580, val loss: 1.1704920530319214
Epoch 590, training loss: 315.9797668457031 = 0.9152160286903381 + 50.0 * 6.301290988922119
Epoch 590, val loss: 1.1614115238189697
Epoch 600, training loss: 315.89056396484375 = 0.8982539176940918 + 50.0 * 6.299846172332764
Epoch 600, val loss: 1.1531609296798706
Epoch 610, training loss: 315.7523193359375 = 0.8817465901374817 + 50.0 * 6.2974114418029785
Epoch 610, val loss: 1.1453741788864136
Epoch 620, training loss: 315.70587158203125 = 0.8656271696090698 + 50.0 * 6.296804904937744
Epoch 620, val loss: 1.1380771398544312
Epoch 630, training loss: 315.73736572265625 = 0.8496912717819214 + 50.0 * 6.29775333404541
Epoch 630, val loss: 1.13107168674469
Epoch 640, training loss: 315.5840759277344 = 0.8340170383453369 + 50.0 * 6.295001029968262
Epoch 640, val loss: 1.1242690086364746
Epoch 650, training loss: 315.4252624511719 = 0.8187169432640076 + 50.0 * 6.292130470275879
Epoch 650, val loss: 1.118094801902771
Epoch 660, training loss: 315.31756591796875 = 0.8037611842155457 + 50.0 * 6.290275573730469
Epoch 660, val loss: 1.1122732162475586
Epoch 670, training loss: 315.4406433105469 = 0.7890962362289429 + 50.0 * 6.293030738830566
Epoch 670, val loss: 1.1069145202636719
Epoch 680, training loss: 315.3903503417969 = 0.7744540572166443 + 50.0 * 6.292317867279053
Epoch 680, val loss: 1.1014057397842407
Epoch 690, training loss: 315.07501220703125 = 0.7600533366203308 + 50.0 * 6.286299228668213
Epoch 690, val loss: 1.0964477062225342
Epoch 700, training loss: 315.030517578125 = 0.7459636926651001 + 50.0 * 6.285690784454346
Epoch 700, val loss: 1.091928243637085
Epoch 710, training loss: 315.05279541015625 = 0.7320732474327087 + 50.0 * 6.28641414642334
Epoch 710, val loss: 1.0875756740570068
Epoch 720, training loss: 314.856201171875 = 0.7182157039642334 + 50.0 * 6.282760143280029
Epoch 720, val loss: 1.0832972526550293
Epoch 730, training loss: 314.9266357421875 = 0.7046115398406982 + 50.0 * 6.284440517425537
Epoch 730, val loss: 1.079438328742981
Epoch 740, training loss: 314.8252868652344 = 0.6911563277244568 + 50.0 * 6.282682418823242
Epoch 740, val loss: 1.0757980346679688
Epoch 750, training loss: 314.7431335449219 = 0.6778118014335632 + 50.0 * 6.281306266784668
Epoch 750, val loss: 1.0722968578338623
Epoch 760, training loss: 314.6937561035156 = 0.6646833419799805 + 50.0 * 6.280581951141357
Epoch 760, val loss: 1.0691430568695068
Epoch 770, training loss: 314.5561218261719 = 0.6516963839530945 + 50.0 * 6.278088092803955
Epoch 770, val loss: 1.0661065578460693
Epoch 780, training loss: 314.61993408203125 = 0.6389267444610596 + 50.0 * 6.27962064743042
Epoch 780, val loss: 1.063429832458496
Epoch 790, training loss: 314.4986267089844 = 0.626161515712738 + 50.0 * 6.277449131011963
Epoch 790, val loss: 1.0608494281768799
Epoch 800, training loss: 314.43707275390625 = 0.6136333346366882 + 50.0 * 6.276468753814697
Epoch 800, val loss: 1.058541178703308
Epoch 810, training loss: 314.3047180175781 = 0.6012818813323975 + 50.0 * 6.274068355560303
Epoch 810, val loss: 1.0564922094345093
Epoch 820, training loss: 314.2656555175781 = 0.5892089009284973 + 50.0 * 6.273529052734375
Epoch 820, val loss: 1.054836392402649
Epoch 830, training loss: 314.2593078613281 = 0.5772538781166077 + 50.0 * 6.273641586303711
Epoch 830, val loss: 1.053160309791565
Epoch 840, training loss: 314.2099609375 = 0.5654157400131226 + 50.0 * 6.272890567779541
Epoch 840, val loss: 1.051877737045288
Epoch 850, training loss: 314.1123352050781 = 0.553793728351593 + 50.0 * 6.271170616149902
Epoch 850, val loss: 1.0507867336273193
Epoch 860, training loss: 313.9964599609375 = 0.5424270629882812 + 50.0 * 6.269081115722656
Epoch 860, val loss: 1.0499687194824219
Epoch 870, training loss: 313.9246826171875 = 0.5312774181365967 + 50.0 * 6.2678680419921875
Epoch 870, val loss: 1.0494904518127441
Epoch 880, training loss: 314.1012878417969 = 0.520306408405304 + 50.0 * 6.27161979675293
Epoch 880, val loss: 1.0491477251052856
Epoch 890, training loss: 313.9273681640625 = 0.5094714164733887 + 50.0 * 6.26835823059082
Epoch 890, val loss: 1.0489588975906372
Epoch 900, training loss: 314.0399475097656 = 0.49877867102622986 + 50.0 * 6.2708234786987305
Epoch 900, val loss: 1.0488275289535522
Epoch 910, training loss: 313.8289794921875 = 0.48824113607406616 + 50.0 * 6.266814708709717
Epoch 910, val loss: 1.0491447448730469
Epoch 920, training loss: 313.6874694824219 = 0.47797951102256775 + 50.0 * 6.264190196990967
Epoch 920, val loss: 1.0496091842651367
Epoch 930, training loss: 313.6114501953125 = 0.46801280975341797 + 50.0 * 6.262868404388428
Epoch 930, val loss: 1.0506231784820557
Epoch 940, training loss: 313.55859375 = 0.4582125246524811 + 50.0 * 6.262007713317871
Epoch 940, val loss: 1.0517042875289917
Epoch 950, training loss: 314.1700439453125 = 0.4486686587333679 + 50.0 * 6.27442741394043
Epoch 950, val loss: 1.053001046180725
Epoch 960, training loss: 313.5486755371094 = 0.4388085901737213 + 50.0 * 6.262197494506836
Epoch 960, val loss: 1.0538506507873535
Epoch 970, training loss: 313.4969787597656 = 0.4293875992298126 + 50.0 * 6.261352062225342
Epoch 970, val loss: 1.0551860332489014
Epoch 980, training loss: 313.4044494628906 = 0.4202546775341034 + 50.0 * 6.259684085845947
Epoch 980, val loss: 1.057039499282837
Epoch 990, training loss: 313.3406677246094 = 0.4113207757472992 + 50.0 * 6.258586883544922
Epoch 990, val loss: 1.0590307712554932
Epoch 1000, training loss: 313.3841857910156 = 0.4025452136993408 + 50.0 * 6.259632587432861
Epoch 1000, val loss: 1.0612311363220215
Epoch 1010, training loss: 313.2585754394531 = 0.39377301931381226 + 50.0 * 6.257296085357666
Epoch 1010, val loss: 1.0630806684494019
Epoch 1020, training loss: 313.3311462402344 = 0.38515380024909973 + 50.0 * 6.258919715881348
Epoch 1020, val loss: 1.065584659576416
Epoch 1030, training loss: 313.1693115234375 = 0.37682342529296875 + 50.0 * 6.255849361419678
Epoch 1030, val loss: 1.0681794881820679
Epoch 1040, training loss: 313.32342529296875 = 0.3686867952346802 + 50.0 * 6.259094715118408
Epoch 1040, val loss: 1.070953130722046
Epoch 1050, training loss: 313.197265625 = 0.36054953932762146 + 50.0 * 6.256734848022461
Epoch 1050, val loss: 1.0735979080200195
Epoch 1060, training loss: 313.2882080078125 = 0.35260850191116333 + 50.0 * 6.258712291717529
Epoch 1060, val loss: 1.076720118522644
Epoch 1070, training loss: 313.0328369140625 = 0.3448235094547272 + 50.0 * 6.25376033782959
Epoch 1070, val loss: 1.079686164855957
Epoch 1080, training loss: 313.03826904296875 = 0.33722084760665894 + 50.0 * 6.254020690917969
Epoch 1080, val loss: 1.0828721523284912
Epoch 1090, training loss: 313.0196838378906 = 0.3297792673110962 + 50.0 * 6.253798007965088
Epoch 1090, val loss: 1.086303949356079
Epoch 1100, training loss: 313.08697509765625 = 0.3224203288555145 + 50.0 * 6.255290985107422
Epoch 1100, val loss: 1.0897427797317505
Epoch 1110, training loss: 313.09259033203125 = 0.31514719128608704 + 50.0 * 6.25554895401001
Epoch 1110, val loss: 1.0935395956039429
Epoch 1120, training loss: 312.9818420410156 = 0.3080395460128784 + 50.0 * 6.253476142883301
Epoch 1120, val loss: 1.0969823598861694
Epoch 1130, training loss: 312.91033935546875 = 0.30109310150146484 + 50.0 * 6.252185344696045
Epoch 1130, val loss: 1.1010441780090332
Epoch 1140, training loss: 312.8798828125 = 0.29424843192100525 + 50.0 * 6.251712799072266
Epoch 1140, val loss: 1.1050115823745728
Epoch 1150, training loss: 312.8097839355469 = 0.28750449419021606 + 50.0 * 6.250445365905762
Epoch 1150, val loss: 1.1094980239868164
Epoch 1160, training loss: 312.9461669921875 = 0.28089281916618347 + 50.0 * 6.253305912017822
Epoch 1160, val loss: 1.113769292831421
Epoch 1170, training loss: 312.7529296875 = 0.2743096947669983 + 50.0 * 6.249572277069092
Epoch 1170, val loss: 1.1184108257293701
Epoch 1180, training loss: 312.6895751953125 = 0.26793262362480164 + 50.0 * 6.248432636260986
Epoch 1180, val loss: 1.1230721473693848
Epoch 1190, training loss: 312.8410339355469 = 0.261728972196579 + 50.0 * 6.251586437225342
Epoch 1190, val loss: 1.1278289556503296
Epoch 1200, training loss: 312.7093200683594 = 0.25557687878608704 + 50.0 * 6.249074459075928
Epoch 1200, val loss: 1.1327261924743652
Epoch 1210, training loss: 312.6070251464844 = 0.24954889714717865 + 50.0 * 6.247149467468262
Epoch 1210, val loss: 1.1374009847640991
Epoch 1220, training loss: 312.5601501464844 = 0.24372683465480804 + 50.0 * 6.246328830718994
Epoch 1220, val loss: 1.142610788345337
Epoch 1230, training loss: 312.5746154785156 = 0.2380300611257553 + 50.0 * 6.246731281280518
Epoch 1230, val loss: 1.1476348638534546
Epoch 1240, training loss: 312.7237243652344 = 0.23242294788360596 + 50.0 * 6.249825954437256
Epoch 1240, val loss: 1.1524920463562012
Epoch 1250, training loss: 312.5889892578125 = 0.2269103080034256 + 50.0 * 6.247241973876953
Epoch 1250, val loss: 1.1573302745819092
Epoch 1260, training loss: 312.539794921875 = 0.22151224315166473 + 50.0 * 6.246365547180176
Epoch 1260, val loss: 1.1627278327941895
Epoch 1270, training loss: 312.44305419921875 = 0.2162620723247528 + 50.0 * 6.24453592300415
Epoch 1270, val loss: 1.1679091453552246
Epoch 1280, training loss: 312.4765625 = 0.21117813885211945 + 50.0 * 6.245307445526123
Epoch 1280, val loss: 1.1731208562850952
Epoch 1290, training loss: 312.5093688964844 = 0.2061966508626938 + 50.0 * 6.246063232421875
Epoch 1290, val loss: 1.1786309480667114
Epoch 1300, training loss: 312.453369140625 = 0.20127129554748535 + 50.0 * 6.245041847229004
Epoch 1300, val loss: 1.1838006973266602
Epoch 1310, training loss: 312.3963928222656 = 0.19649210572242737 + 50.0 * 6.243997573852539
Epoch 1310, val loss: 1.1895160675048828
Epoch 1320, training loss: 312.3797302246094 = 0.19183573126792908 + 50.0 * 6.243758201599121
Epoch 1320, val loss: 1.1950191259384155
Epoch 1330, training loss: 312.30303955078125 = 0.18732665479183197 + 50.0 * 6.242314338684082
Epoch 1330, val loss: 1.2010425329208374
Epoch 1340, training loss: 312.2993469238281 = 0.1829351931810379 + 50.0 * 6.242328643798828
Epoch 1340, val loss: 1.206934928894043
Epoch 1350, training loss: 312.41015625 = 0.17861293256282806 + 50.0 * 6.244630813598633
Epoch 1350, val loss: 1.2126480340957642
Epoch 1360, training loss: 312.3069763183594 = 0.17436839640140533 + 50.0 * 6.24265193939209
Epoch 1360, val loss: 1.2183550596237183
Epoch 1370, training loss: 312.35400390625 = 0.17025874555110931 + 50.0 * 6.2436747550964355
Epoch 1370, val loss: 1.2244293689727783
Epoch 1380, training loss: 312.18182373046875 = 0.1662280261516571 + 50.0 * 6.240312099456787
Epoch 1380, val loss: 1.2299151420593262
Epoch 1390, training loss: 312.3140563964844 = 0.16234411299228668 + 50.0 * 6.2430338859558105
Epoch 1390, val loss: 1.2361721992492676
Epoch 1400, training loss: 312.1614685058594 = 0.15852399170398712 + 50.0 * 6.240058422088623
Epoch 1400, val loss: 1.242156744003296
Epoch 1410, training loss: 312.1332702636719 = 0.15482474863529205 + 50.0 * 6.239569187164307
Epoch 1410, val loss: 1.2482469081878662
Epoch 1420, training loss: 312.3313293457031 = 0.15123510360717773 + 50.0 * 6.2436017990112305
Epoch 1420, val loss: 1.2542657852172852
Epoch 1430, training loss: 312.1740417480469 = 0.14762534201145172 + 50.0 * 6.240528583526611
Epoch 1430, val loss: 1.2601386308670044
Epoch 1440, training loss: 312.04998779296875 = 0.14415226876735687 + 50.0 * 6.23811674118042
Epoch 1440, val loss: 1.2662005424499512
Epoch 1450, training loss: 312.0039367675781 = 0.14083562791347504 + 50.0 * 6.237261772155762
Epoch 1450, val loss: 1.2725425958633423
Epoch 1460, training loss: 311.9862060546875 = 0.13759706914424896 + 50.0 * 6.236971855163574
Epoch 1460, val loss: 1.278694748878479
Epoch 1470, training loss: 312.3309020996094 = 0.13447733223438263 + 50.0 * 6.2439284324646
Epoch 1470, val loss: 1.285189151763916
Epoch 1480, training loss: 312.09539794921875 = 0.13129621744155884 + 50.0 * 6.239282131195068
Epoch 1480, val loss: 1.290307641029358
Epoch 1490, training loss: 312.1380920410156 = 0.12826712429523468 + 50.0 * 6.240196228027344
Epoch 1490, val loss: 1.2968498468399048
Epoch 1500, training loss: 311.9884033203125 = 0.12531839311122894 + 50.0 * 6.237261772155762
Epoch 1500, val loss: 1.3029118776321411
Epoch 1510, training loss: 311.8865661621094 = 0.12244371324777603 + 50.0 * 6.2352824211120605
Epoch 1510, val loss: 1.3090580701828003
Epoch 1520, training loss: 311.9190368652344 = 0.11969005316495895 + 50.0 * 6.235986709594727
Epoch 1520, val loss: 1.3154032230377197
Epoch 1530, training loss: 312.0491027832031 = 0.11699441820383072 + 50.0 * 6.238641738891602
Epoch 1530, val loss: 1.3215059041976929
Epoch 1540, training loss: 311.9208984375 = 0.11430390924215317 + 50.0 * 6.23613166809082
Epoch 1540, val loss: 1.3279472589492798
Epoch 1550, training loss: 311.95635986328125 = 0.11175426840782166 + 50.0 * 6.236892223358154
Epoch 1550, val loss: 1.3338879346847534
Epoch 1560, training loss: 311.8673400878906 = 0.10922086983919144 + 50.0 * 6.235162258148193
Epoch 1560, val loss: 1.3401110172271729
Epoch 1570, training loss: 311.9658508300781 = 0.10675787180662155 + 50.0 * 6.237182140350342
Epoch 1570, val loss: 1.3459659814834595
Epoch 1580, training loss: 311.8116455078125 = 0.10434243083000183 + 50.0 * 6.2341461181640625
Epoch 1580, val loss: 1.3525173664093018
Epoch 1590, training loss: 311.75775146484375 = 0.10205471515655518 + 50.0 * 6.233114242553711
Epoch 1590, val loss: 1.3586817979812622
Epoch 1600, training loss: 311.8606872558594 = 0.09981030970811844 + 50.0 * 6.235217094421387
Epoch 1600, val loss: 1.3650178909301758
Epoch 1610, training loss: 311.719970703125 = 0.09759886562824249 + 50.0 * 6.232447624206543
Epoch 1610, val loss: 1.3711775541305542
Epoch 1620, training loss: 311.7666015625 = 0.09545034170150757 + 50.0 * 6.233422756195068
Epoch 1620, val loss: 1.3773009777069092
Epoch 1630, training loss: 311.8445129394531 = 0.09338115900754929 + 50.0 * 6.23502254486084
Epoch 1630, val loss: 1.3836065530776978
Epoch 1640, training loss: 311.79083251953125 = 0.09136024862527847 + 50.0 * 6.233989238739014
Epoch 1640, val loss: 1.3896604776382446
Epoch 1650, training loss: 311.79205322265625 = 0.08937446773052216 + 50.0 * 6.234053134918213
Epoch 1650, val loss: 1.3959037065505981
Epoch 1660, training loss: 311.71234130859375 = 0.08742152154445648 + 50.0 * 6.2324981689453125
Epoch 1660, val loss: 1.4020227193832397
Epoch 1670, training loss: 311.64752197265625 = 0.08558057993650436 + 50.0 * 6.231238842010498
Epoch 1670, val loss: 1.4081860780715942
Epoch 1680, training loss: 311.69195556640625 = 0.08376143872737885 + 50.0 * 6.232163429260254
Epoch 1680, val loss: 1.4143874645233154
Epoch 1690, training loss: 311.6767883300781 = 0.08199445903301239 + 50.0 * 6.231895923614502
Epoch 1690, val loss: 1.420233964920044
Epoch 1700, training loss: 311.6902770996094 = 0.0802481472492218 + 50.0 * 6.2322001457214355
Epoch 1700, val loss: 1.4260907173156738
Epoch 1710, training loss: 311.5759582519531 = 0.07856308668851852 + 50.0 * 6.229948043823242
Epoch 1710, val loss: 1.432413101196289
Epoch 1720, training loss: 311.60418701171875 = 0.07693726569414139 + 50.0 * 6.2305450439453125
Epoch 1720, val loss: 1.4385433197021484
Epoch 1730, training loss: 311.51727294921875 = 0.07534024119377136 + 50.0 * 6.2288384437561035
Epoch 1730, val loss: 1.4444459676742554
Epoch 1740, training loss: 311.587646484375 = 0.07382127642631531 + 50.0 * 6.230276584625244
Epoch 1740, val loss: 1.4503663778305054
Epoch 1750, training loss: 311.6153564453125 = 0.072298564016819 + 50.0 * 6.230861186981201
Epoch 1750, val loss: 1.4563795328140259
Epoch 1760, training loss: 311.5606384277344 = 0.0708281546831131 + 50.0 * 6.229796409606934
Epoch 1760, val loss: 1.4623531103134155
Epoch 1770, training loss: 311.618408203125 = 0.06937136501073837 + 50.0 * 6.23098087310791
Epoch 1770, val loss: 1.4679769277572632
Epoch 1780, training loss: 311.56158447265625 = 0.06796783208847046 + 50.0 * 6.229872703552246
Epoch 1780, val loss: 1.4747344255447388
Epoch 1790, training loss: 311.4350280761719 = 0.06659214198589325 + 50.0 * 6.2273688316345215
Epoch 1790, val loss: 1.4801636934280396
Epoch 1800, training loss: 311.3949890136719 = 0.06528744846582413 + 50.0 * 6.226593971252441
Epoch 1800, val loss: 1.486233115196228
Epoch 1810, training loss: 311.6479797363281 = 0.06402089446783066 + 50.0 * 6.2316789627075195
Epoch 1810, val loss: 1.4920644760131836
Epoch 1820, training loss: 311.4256286621094 = 0.06273271143436432 + 50.0 * 6.22725772857666
Epoch 1820, val loss: 1.4977682828903198
Epoch 1830, training loss: 311.44915771484375 = 0.061480842530727386 + 50.0 * 6.227753162384033
Epoch 1830, val loss: 1.5033012628555298
Epoch 1840, training loss: 311.60162353515625 = 0.060296762734651566 + 50.0 * 6.230826377868652
Epoch 1840, val loss: 1.5088998079299927
Epoch 1850, training loss: 311.3792419433594 = 0.05912007763981819 + 50.0 * 6.226402282714844
Epoch 1850, val loss: 1.5147989988327026
Epoch 1860, training loss: 311.3814697265625 = 0.05799577385187149 + 50.0 * 6.22646951675415
Epoch 1860, val loss: 1.5206574201583862
Epoch 1870, training loss: 311.42694091796875 = 0.05688249692320824 + 50.0 * 6.227401256561279
Epoch 1870, val loss: 1.52626371383667
Epoch 1880, training loss: 311.30535888671875 = 0.05579577758908272 + 50.0 * 6.224991321563721
Epoch 1880, val loss: 1.5319099426269531
Epoch 1890, training loss: 311.2705383300781 = 0.05475720018148422 + 50.0 * 6.224315643310547
Epoch 1890, val loss: 1.537797212600708
Epoch 1900, training loss: 311.32452392578125 = 0.053748179227113724 + 50.0 * 6.2254157066345215
Epoch 1900, val loss: 1.543723702430725
Epoch 1910, training loss: 311.4247741699219 = 0.0527392253279686 + 50.0 * 6.22744083404541
Epoch 1910, val loss: 1.5487520694732666
Epoch 1920, training loss: 311.4004821777344 = 0.05172756314277649 + 50.0 * 6.226974964141846
Epoch 1920, val loss: 1.5546823740005493
Epoch 1930, training loss: 311.25177001953125 = 0.05076144263148308 + 50.0 * 6.224020481109619
Epoch 1930, val loss: 1.5593287944793701
Epoch 1940, training loss: 311.2203063964844 = 0.04983450099825859 + 50.0 * 6.223409175872803
Epoch 1940, val loss: 1.5655162334442139
Epoch 1950, training loss: 311.1766357421875 = 0.04894673451781273 + 50.0 * 6.222553730010986
Epoch 1950, val loss: 1.570796012878418
Epoch 1960, training loss: 311.3712463378906 = 0.048086486756801605 + 50.0 * 6.2264628410339355
Epoch 1960, val loss: 1.5766042470932007
Epoch 1970, training loss: 311.4452209472656 = 0.047223739326000214 + 50.0 * 6.227960109710693
Epoch 1970, val loss: 1.5816593170166016
Epoch 1980, training loss: 311.25103759765625 = 0.04633522406220436 + 50.0 * 6.224094390869141
Epoch 1980, val loss: 1.5864660739898682
Epoch 1990, training loss: 311.15606689453125 = 0.04551716893911362 + 50.0 * 6.22221040725708
Epoch 1990, val loss: 1.5923677682876587
Epoch 2000, training loss: 311.1328125 = 0.04472431540489197 + 50.0 * 6.221761703491211
Epoch 2000, val loss: 1.5976516008377075
Epoch 2010, training loss: 311.4206237792969 = 0.04395781457424164 + 50.0 * 6.227533340454102
Epoch 2010, val loss: 1.6031895875930786
Epoch 2020, training loss: 311.2717590332031 = 0.04319269582629204 + 50.0 * 6.224571704864502
Epoch 2020, val loss: 1.6081470251083374
Epoch 2030, training loss: 311.1224670410156 = 0.042413629591464996 + 50.0 * 6.2216010093688965
Epoch 2030, val loss: 1.6131510734558105
Epoch 2040, training loss: 311.0801696777344 = 0.0416959673166275 + 50.0 * 6.22076940536499
Epoch 2040, val loss: 1.6186609268188477
Epoch 2050, training loss: 311.2206726074219 = 0.04099872335791588 + 50.0 * 6.223593711853027
Epoch 2050, val loss: 1.624045968055725
Epoch 2060, training loss: 311.1126708984375 = 0.04029471427202225 + 50.0 * 6.221447467803955
Epoch 2060, val loss: 1.628852128982544
Epoch 2070, training loss: 311.1830139160156 = 0.03960910812020302 + 50.0 * 6.222867965698242
Epoch 2070, val loss: 1.634012222290039
Epoch 2080, training loss: 311.2229919433594 = 0.03893788531422615 + 50.0 * 6.2236809730529785
Epoch 2080, val loss: 1.6390665769577026
Epoch 2090, training loss: 311.10150146484375 = 0.03827483952045441 + 50.0 * 6.221264362335205
Epoch 2090, val loss: 1.6438261270523071
Epoch 2100, training loss: 311.0350646972656 = 0.03763765096664429 + 50.0 * 6.219948768615723
Epoch 2100, val loss: 1.6492360830307007
Epoch 2110, training loss: 310.9863586425781 = 0.037022121250629425 + 50.0 * 6.218986988067627
Epoch 2110, val loss: 1.6543619632720947
Epoch 2120, training loss: 310.98577880859375 = 0.03643207252025604 + 50.0 * 6.218986511230469
Epoch 2120, val loss: 1.6593742370605469
Epoch 2130, training loss: 311.28631591796875 = 0.0358588732779026 + 50.0 * 6.225008964538574
Epoch 2130, val loss: 1.6644583940505981
Epoch 2140, training loss: 311.06182861328125 = 0.0352584570646286 + 50.0 * 6.220531463623047
Epoch 2140, val loss: 1.6689324378967285
Epoch 2150, training loss: 311.0435485839844 = 0.034689679741859436 + 50.0 * 6.220177173614502
Epoch 2150, val loss: 1.6736565828323364
Epoch 2160, training loss: 311.081787109375 = 0.03413505107164383 + 50.0 * 6.22095251083374
Epoch 2160, val loss: 1.6786378622055054
Epoch 2170, training loss: 311.233154296875 = 0.033591222018003464 + 50.0 * 6.2239909172058105
Epoch 2170, val loss: 1.6837257146835327
Epoch 2180, training loss: 310.9770812988281 = 0.0330522395670414 + 50.0 * 6.218880653381348
Epoch 2180, val loss: 1.6882883310317993
Epoch 2190, training loss: 310.89532470703125 = 0.03253858909010887 + 50.0 * 6.217255592346191
Epoch 2190, val loss: 1.6935323476791382
Epoch 2200, training loss: 310.93341064453125 = 0.032047126442193985 + 50.0 * 6.218027114868164
Epoch 2200, val loss: 1.6985334157943726
Epoch 2210, training loss: 311.32684326171875 = 0.03155765309929848 + 50.0 * 6.225905895233154
Epoch 2210, val loss: 1.7032229900360107
Epoch 2220, training loss: 311.02655029296875 = 0.03105483390390873 + 50.0 * 6.21990966796875
Epoch 2220, val loss: 1.707169771194458
Epoch 2230, training loss: 310.89837646484375 = 0.030569175258278847 + 50.0 * 6.217356204986572
Epoch 2230, val loss: 1.7124589681625366
Epoch 2240, training loss: 310.85687255859375 = 0.030115408822894096 + 50.0 * 6.2165350914001465
Epoch 2240, val loss: 1.7168748378753662
Epoch 2250, training loss: 310.9065856933594 = 0.02967054583132267 + 50.0 * 6.217538356781006
Epoch 2250, val loss: 1.721692681312561
Epoch 2260, training loss: 311.1336364746094 = 0.029229553416371346 + 50.0 * 6.222087860107422
Epoch 2260, val loss: 1.7264612913131714
Epoch 2270, training loss: 311.01104736328125 = 0.02879161387681961 + 50.0 * 6.2196455001831055
Epoch 2270, val loss: 1.7310854196548462
Epoch 2280, training loss: 310.9912414550781 = 0.02835574559867382 + 50.0 * 6.219257831573486
Epoch 2280, val loss: 1.7357079982757568
Epoch 2290, training loss: 310.90283203125 = 0.02794385515153408 + 50.0 * 6.217497825622559
Epoch 2290, val loss: 1.740336298942566
Epoch 2300, training loss: 310.929443359375 = 0.027529871091246605 + 50.0 * 6.218038082122803
Epoch 2300, val loss: 1.744558572769165
Epoch 2310, training loss: 310.88824462890625 = 0.027128366753458977 + 50.0 * 6.217222213745117
Epoch 2310, val loss: 1.7492791414260864
Epoch 2320, training loss: 310.84429931640625 = 0.026738645508885384 + 50.0 * 6.21635103225708
Epoch 2320, val loss: 1.753214955329895
Epoch 2330, training loss: 310.9032897949219 = 0.026361161842942238 + 50.0 * 6.217538356781006
Epoch 2330, val loss: 1.7577279806137085
Epoch 2340, training loss: 310.8868103027344 = 0.025973627343773842 + 50.0 * 6.217216491699219
Epoch 2340, val loss: 1.7625609636306763
Epoch 2350, training loss: 310.809326171875 = 0.025610066950321198 + 50.0 * 6.21567440032959
Epoch 2350, val loss: 1.7668908834457397
Epoch 2360, training loss: 310.77716064453125 = 0.025252802297472954 + 50.0 * 6.215038299560547
Epoch 2360, val loss: 1.771250605583191
Epoch 2370, training loss: 310.7515563964844 = 0.024900957942008972 + 50.0 * 6.214532852172852
Epoch 2370, val loss: 1.7755789756774902
Epoch 2380, training loss: 311.01971435546875 = 0.024580545723438263 + 50.0 * 6.219902992248535
Epoch 2380, val loss: 1.7799538373947144
Epoch 2390, training loss: 310.9227294921875 = 0.0242089182138443 + 50.0 * 6.217970371246338
Epoch 2390, val loss: 1.7830218076705933
Epoch 2400, training loss: 310.83123779296875 = 0.02387501858174801 + 50.0 * 6.216147422790527
Epoch 2400, val loss: 1.788766622543335
Epoch 2410, training loss: 310.76641845703125 = 0.02354418858885765 + 50.0 * 6.214858055114746
Epoch 2410, val loss: 1.7922124862670898
Epoch 2420, training loss: 310.7892150878906 = 0.02323584258556366 + 50.0 * 6.215319633483887
Epoch 2420, val loss: 1.796736240386963
Epoch 2430, training loss: 310.8253173828125 = 0.022919347509741783 + 50.0 * 6.216048240661621
Epoch 2430, val loss: 1.8010197877883911
Epoch 2440, training loss: 310.6971740722656 = 0.022614475339651108 + 50.0 * 6.213491439819336
Epoch 2440, val loss: 1.8049894571304321
Epoch 2450, training loss: 310.7579650878906 = 0.022321604192256927 + 50.0 * 6.214713096618652
Epoch 2450, val loss: 1.8087936639785767
Epoch 2460, training loss: 310.725341796875 = 0.022025717422366142 + 50.0 * 6.214066028594971
Epoch 2460, val loss: 1.81303870677948
Epoch 2470, training loss: 310.9132995605469 = 0.02173730544745922 + 50.0 * 6.217831134796143
Epoch 2470, val loss: 1.8176100254058838
Epoch 2480, training loss: 310.75372314453125 = 0.021448658779263496 + 50.0 * 6.2146453857421875
Epoch 2480, val loss: 1.8210599422454834
Epoch 2490, training loss: 310.6247253417969 = 0.021164346486330032 + 50.0 * 6.212070941925049
Epoch 2490, val loss: 1.8252465724945068
Epoch 2500, training loss: 310.6201477050781 = 0.020899461582303047 + 50.0 * 6.211984634399414
Epoch 2500, val loss: 1.829524040222168
Epoch 2510, training loss: 310.7079772949219 = 0.020646916702389717 + 50.0 * 6.213746070861816
Epoch 2510, val loss: 1.833464503288269
Epoch 2520, training loss: 310.8273010253906 = 0.02038107067346573 + 50.0 * 6.2161383628845215
Epoch 2520, val loss: 1.8370530605316162
Epoch 2530, training loss: 310.7082824707031 = 0.020105963572859764 + 50.0 * 6.21376371383667
Epoch 2530, val loss: 1.840792179107666
Epoch 2540, training loss: 310.6955871582031 = 0.019851023331284523 + 50.0 * 6.213515281677246
Epoch 2540, val loss: 1.844802975654602
Epoch 2550, training loss: 310.5940856933594 = 0.019599562510848045 + 50.0 * 6.211489677429199
Epoch 2550, val loss: 1.8486762046813965
Epoch 2560, training loss: 310.8062744140625 = 0.019367851316928864 + 50.0 * 6.215737819671631
Epoch 2560, val loss: 1.8526211977005005
Epoch 2570, training loss: 310.6123046875 = 0.019119640812277794 + 50.0 * 6.2118635177612305
Epoch 2570, val loss: 1.8565938472747803
Epoch 2580, training loss: 310.5557861328125 = 0.018890507519245148 + 50.0 * 6.210738182067871
Epoch 2580, val loss: 1.8604987859725952
Epoch 2590, training loss: 310.64874267578125 = 0.01867075078189373 + 50.0 * 6.212601184844971
Epoch 2590, val loss: 1.8640997409820557
Epoch 2600, training loss: 310.7160339355469 = 0.018443338572978973 + 50.0 * 6.21395206451416
Epoch 2600, val loss: 1.8678523302078247
Epoch 2610, training loss: 310.6468505859375 = 0.018213601782917976 + 50.0 * 6.2125725746154785
Epoch 2610, val loss: 1.8713456392288208
Epoch 2620, training loss: 310.56414794921875 = 0.017995819449424744 + 50.0 * 6.210922718048096
Epoch 2620, val loss: 1.8753583431243896
Epoch 2630, training loss: 310.6934814453125 = 0.017788155004382133 + 50.0 * 6.2135138511657715
Epoch 2630, val loss: 1.879372477531433
Epoch 2640, training loss: 310.54376220703125 = 0.017574971541762352 + 50.0 * 6.21052360534668
Epoch 2640, val loss: 1.8825546503067017
Epoch 2650, training loss: 310.59759521484375 = 0.017373692244291306 + 50.0 * 6.211604595184326
Epoch 2650, val loss: 1.8858124017715454
Epoch 2660, training loss: 310.71649169921875 = 0.017171479761600494 + 50.0 * 6.213985919952393
Epoch 2660, val loss: 1.890159249305725
Epoch 2670, training loss: 310.5804748535156 = 0.016968930140137672 + 50.0 * 6.211269855499268
Epoch 2670, val loss: 1.8930323123931885
Epoch 2680, training loss: 310.5098571777344 = 0.016773134469985962 + 50.0 * 6.2098612785339355
Epoch 2680, val loss: 1.8969604969024658
Epoch 2690, training loss: 310.6134948730469 = 0.016593726351857185 + 50.0 * 6.21193790435791
Epoch 2690, val loss: 1.9003719091415405
Epoch 2700, training loss: 310.50152587890625 = 0.016397781670093536 + 50.0 * 6.209702491760254
Epoch 2700, val loss: 1.9039387702941895
Epoch 2710, training loss: 310.80010986328125 = 0.01621420308947563 + 50.0 * 6.2156782150268555
Epoch 2710, val loss: 1.9071571826934814
Epoch 2720, training loss: 310.51611328125 = 0.01602255553007126 + 50.0 * 6.2100019454956055
Epoch 2720, val loss: 1.910435438156128
Epoch 2730, training loss: 310.4754943847656 = 0.01584511809051037 + 50.0 * 6.209193229675293
Epoch 2730, val loss: 1.9140089750289917
Epoch 2740, training loss: 310.5121154785156 = 0.015675552189350128 + 50.0 * 6.209928512573242
Epoch 2740, val loss: 1.9173810482025146
Epoch 2750, training loss: 310.6255798339844 = 0.01550502609461546 + 50.0 * 6.2122015953063965
Epoch 2750, val loss: 1.9208941459655762
Epoch 2760, training loss: 310.4637451171875 = 0.015336193144321442 + 50.0 * 6.208967685699463
Epoch 2760, val loss: 1.9244697093963623
Epoch 2770, training loss: 310.49298095703125 = 0.015174259431660175 + 50.0 * 6.2095561027526855
Epoch 2770, val loss: 1.9273537397384644
Epoch 2780, training loss: 310.56878662109375 = 0.015009892173111439 + 50.0 * 6.211075305938721
Epoch 2780, val loss: 1.9307719469070435
Epoch 2790, training loss: 310.47991943359375 = 0.014844045042991638 + 50.0 * 6.209301471710205
Epoch 2790, val loss: 1.934289813041687
Epoch 2800, training loss: 310.45843505859375 = 0.014684642665088177 + 50.0 * 6.2088751792907715
Epoch 2800, val loss: 1.9371496438980103
Epoch 2810, training loss: 310.457275390625 = 0.014532383531332016 + 50.0 * 6.208855152130127
Epoch 2810, val loss: 1.9404743909835815
Epoch 2820, training loss: 310.4709777832031 = 0.014380617067217827 + 50.0 * 6.209132194519043
Epoch 2820, val loss: 1.9441717863082886
Epoch 2830, training loss: 310.476806640625 = 0.014230532571673393 + 50.0 * 6.209251880645752
Epoch 2830, val loss: 1.9475798606872559
Epoch 2840, training loss: 310.46160888671875 = 0.014081806875765324 + 50.0 * 6.208950519561768
Epoch 2840, val loss: 1.950460433959961
Epoch 2850, training loss: 310.4200439453125 = 0.013937069103121758 + 50.0 * 6.2081217765808105
Epoch 2850, val loss: 1.9534649848937988
Epoch 2860, training loss: 310.51739501953125 = 0.013797658495604992 + 50.0 * 6.210072040557861
Epoch 2860, val loss: 1.9566700458526611
Epoch 2870, training loss: 310.4521179199219 = 0.013652767054736614 + 50.0 * 6.20876932144165
Epoch 2870, val loss: 1.9598047733306885
Epoch 2880, training loss: 310.4465637207031 = 0.013516333885490894 + 50.0 * 6.20866060256958
Epoch 2880, val loss: 1.9635968208312988
Epoch 2890, training loss: 310.44122314453125 = 0.01337866485118866 + 50.0 * 6.20855712890625
Epoch 2890, val loss: 1.9662855863571167
Epoch 2900, training loss: 310.367919921875 = 0.013245229609310627 + 50.0 * 6.207093238830566
Epoch 2900, val loss: 1.9691137075424194
Epoch 2910, training loss: 310.3448486328125 = 0.013119139708578587 + 50.0 * 6.206634521484375
Epoch 2910, val loss: 1.9725979566574097
Epoch 2920, training loss: 310.4574890136719 = 0.012996315024793148 + 50.0 * 6.208889961242676
Epoch 2920, val loss: 1.9756525754928589
Epoch 2930, training loss: 310.3837585449219 = 0.012872007675468922 + 50.0 * 6.2074174880981445
Epoch 2930, val loss: 1.97859787940979
Epoch 2940, training loss: 310.5011901855469 = 0.012749122455716133 + 50.0 * 6.209768772125244
Epoch 2940, val loss: 1.9811500310897827
Epoch 2950, training loss: 310.4761047363281 = 0.012617642991244793 + 50.0 * 6.209270000457764
Epoch 2950, val loss: 1.983987808227539
Epoch 2960, training loss: 310.3547668457031 = 0.012487138621509075 + 50.0 * 6.206845760345459
Epoch 2960, val loss: 1.9877395629882812
Epoch 2970, training loss: 310.3009948730469 = 0.012371109798550606 + 50.0 * 6.205772399902344
Epoch 2970, val loss: 1.9904643297195435
Epoch 2980, training loss: 310.3971862792969 = 0.012257370166480541 + 50.0 * 6.207698345184326
Epoch 2980, val loss: 1.9933993816375732
Epoch 2990, training loss: 310.334228515625 = 0.012142463587224483 + 50.0 * 6.206441879272461
Epoch 2990, val loss: 1.9962289333343506
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7000000000000001
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 431.7842102050781 = 1.942155361175537 + 50.0 * 8.596840858459473
Epoch 0, val loss: 1.9442577362060547
Epoch 10, training loss: 431.7361145019531 = 1.9334949254989624 + 50.0 * 8.596052169799805
Epoch 10, val loss: 1.935902714729309
Epoch 20, training loss: 431.42242431640625 = 1.9223713874816895 + 50.0 * 8.590001106262207
Epoch 20, val loss: 1.924729824066162
Epoch 30, training loss: 429.33026123046875 = 1.907693862915039 + 50.0 * 8.54845142364502
Epoch 30, val loss: 1.9095829725265503
Epoch 40, training loss: 418.59210205078125 = 1.8904386758804321 + 50.0 * 8.334033012390137
Epoch 40, val loss: 1.8920364379882812
Epoch 50, training loss: 396.2377014160156 = 1.8705980777740479 + 50.0 * 7.8873419761657715
Epoch 50, val loss: 1.8723759651184082
Epoch 60, training loss: 373.6097412109375 = 1.8565175533294678 + 50.0 * 7.435064792633057
Epoch 60, val loss: 1.8594505786895752
Epoch 70, training loss: 357.79864501953125 = 1.8467127084732056 + 50.0 * 7.1190385818481445
Epoch 70, val loss: 1.8496826887130737
Epoch 80, training loss: 350.10546875 = 1.837676763534546 + 50.0 * 6.96535587310791
Epoch 80, val loss: 1.8404841423034668
Epoch 90, training loss: 343.4097900390625 = 1.8274317979812622 + 50.0 * 6.8316473960876465
Epoch 90, val loss: 1.8302165269851685
Epoch 100, training loss: 338.7157897949219 = 1.8175712823867798 + 50.0 * 6.737964630126953
Epoch 100, val loss: 1.820233702659607
Epoch 110, training loss: 335.8199768066406 = 1.8079887628555298 + 50.0 * 6.680239677429199
Epoch 110, val loss: 1.8103437423706055
Epoch 120, training loss: 333.5270080566406 = 1.798285961151123 + 50.0 * 6.6345744132995605
Epoch 120, val loss: 1.8005446195602417
Epoch 130, training loss: 331.68267822265625 = 1.7887479066848755 + 50.0 * 6.597878456115723
Epoch 130, val loss: 1.7908490896224976
Epoch 140, training loss: 330.1479797363281 = 1.77891206741333 + 50.0 * 6.567381381988525
Epoch 140, val loss: 1.7810461521148682
Epoch 150, training loss: 328.7796630859375 = 1.768475890159607 + 50.0 * 6.540224075317383
Epoch 150, val loss: 1.7709367275238037
Epoch 160, training loss: 327.698486328125 = 1.7574005126953125 + 50.0 * 6.518822193145752
Epoch 160, val loss: 1.7604268789291382
Epoch 170, training loss: 326.6654968261719 = 1.745344877243042 + 50.0 * 6.498403072357178
Epoch 170, val loss: 1.7492576837539673
Epoch 180, training loss: 325.828857421875 = 1.7322853803634644 + 50.0 * 6.481931209564209
Epoch 180, val loss: 1.737337589263916
Epoch 190, training loss: 325.0966796875 = 1.7180991172790527 + 50.0 * 6.467571258544922
Epoch 190, val loss: 1.7245573997497559
Epoch 200, training loss: 324.3841552734375 = 1.7027080059051514 + 50.0 * 6.453629016876221
Epoch 200, val loss: 1.7107970714569092
Epoch 210, training loss: 323.69122314453125 = 1.6860698461532593 + 50.0 * 6.440103054046631
Epoch 210, val loss: 1.6959694623947144
Epoch 220, training loss: 323.219970703125 = 1.668089747428894 + 50.0 * 6.431037425994873
Epoch 220, val loss: 1.68012273311615
Epoch 230, training loss: 322.7240295410156 = 1.6486492156982422 + 50.0 * 6.421507835388184
Epoch 230, val loss: 1.6631250381469727
Epoch 240, training loss: 322.2882080078125 = 1.6279425621032715 + 50.0 * 6.413205623626709
Epoch 240, val loss: 1.6450978517532349
Epoch 250, training loss: 321.78692626953125 = 1.605953574180603 + 50.0 * 6.403619289398193
Epoch 250, val loss: 1.6262121200561523
Epoch 260, training loss: 321.385986328125 = 1.5828555822372437 + 50.0 * 6.396062850952148
Epoch 260, val loss: 1.6064398288726807
Epoch 270, training loss: 321.0950927734375 = 1.5587178468704224 + 50.0 * 6.390727519989014
Epoch 270, val loss: 1.5859732627868652
Epoch 280, training loss: 320.77978515625 = 1.5336464643478394 + 50.0 * 6.384922504425049
Epoch 280, val loss: 1.5649082660675049
Epoch 290, training loss: 320.39886474609375 = 1.5078520774841309 + 50.0 * 6.3778204917907715
Epoch 290, val loss: 1.5434967279434204
Epoch 300, training loss: 320.1026306152344 = 1.4815841913223267 + 50.0 * 6.372420787811279
Epoch 300, val loss: 1.5218596458435059
Epoch 310, training loss: 320.0340576171875 = 1.4548523426055908 + 50.0 * 6.371583938598633
Epoch 310, val loss: 1.5002189874649048
Epoch 320, training loss: 319.6004333496094 = 1.4280400276184082 + 50.0 * 6.363447666168213
Epoch 320, val loss: 1.4786375761032104
Epoch 330, training loss: 319.3392333984375 = 1.4011411666870117 + 50.0 * 6.358761787414551
Epoch 330, val loss: 1.4573956727981567
Epoch 340, training loss: 319.1334533691406 = 1.3743659257888794 + 50.0 * 6.355181694030762
Epoch 340, val loss: 1.436395525932312
Epoch 350, training loss: 318.9732360839844 = 1.347749948501587 + 50.0 * 6.352509498596191
Epoch 350, val loss: 1.415806770324707
Epoch 360, training loss: 318.69378662109375 = 1.3214524984359741 + 50.0 * 6.347446918487549
Epoch 360, val loss: 1.3954230546951294
Epoch 370, training loss: 318.4538269042969 = 1.2955799102783203 + 50.0 * 6.343164443969727
Epoch 370, val loss: 1.3757168054580688
Epoch 380, training loss: 318.29559326171875 = 1.2701188325881958 + 50.0 * 6.34050989151001
Epoch 380, val loss: 1.3563791513442993
Epoch 390, training loss: 318.1641540527344 = 1.2452058792114258 + 50.0 * 6.33837890625
Epoch 390, val loss: 1.3376142978668213
Epoch 400, training loss: 318.0129089355469 = 1.220665693283081 + 50.0 * 6.335844993591309
Epoch 400, val loss: 1.3190988302230835
Epoch 410, training loss: 317.75030517578125 = 1.1967847347259521 + 50.0 * 6.331070423126221
Epoch 410, val loss: 1.3013416528701782
Epoch 420, training loss: 317.5855407714844 = 1.1734462976455688 + 50.0 * 6.328242301940918
Epoch 420, val loss: 1.2842199802398682
Epoch 430, training loss: 317.4239501953125 = 1.1507123708724976 + 50.0 * 6.325464248657227
Epoch 430, val loss: 1.267591118812561
Epoch 440, training loss: 317.35955810546875 = 1.1284211874008179 + 50.0 * 6.324622631072998
Epoch 440, val loss: 1.2513405084609985
Epoch 450, training loss: 317.2153015136719 = 1.1067147254943848 + 50.0 * 6.322171688079834
Epoch 450, val loss: 1.2355879545211792
Epoch 460, training loss: 316.98779296875 = 1.0854750871658325 + 50.0 * 6.3180460929870605
Epoch 460, val loss: 1.220293402671814
Epoch 470, training loss: 316.8235778808594 = 1.0647873878479004 + 50.0 * 6.315175533294678
Epoch 470, val loss: 1.2055740356445312
Epoch 480, training loss: 316.7086486816406 = 1.0445979833602905 + 50.0 * 6.313281536102295
Epoch 480, val loss: 1.1913201808929443
Epoch 490, training loss: 316.5711975097656 = 1.0247552394866943 + 50.0 * 6.310928821563721
Epoch 490, val loss: 1.1774569749832153
Epoch 500, training loss: 316.47906494140625 = 1.0053246021270752 + 50.0 * 6.309474945068359
Epoch 500, val loss: 1.1639320850372314
Epoch 510, training loss: 316.32025146484375 = 0.9863244891166687 + 50.0 * 6.306678771972656
Epoch 510, val loss: 1.1508865356445312
Epoch 520, training loss: 316.45330810546875 = 0.967695951461792 + 50.0 * 6.3097124099731445
Epoch 520, val loss: 1.1381242275238037
Epoch 530, training loss: 316.333740234375 = 0.9494946002960205 + 50.0 * 6.307684898376465
Epoch 530, val loss: 1.1257466077804565
Epoch 540, training loss: 316.0797424316406 = 0.9314394593238831 + 50.0 * 6.302966594696045
Epoch 540, val loss: 1.1140261888504028
Epoch 550, training loss: 315.90863037109375 = 0.9139272570610046 + 50.0 * 6.299893856048584
Epoch 550, val loss: 1.1024158000946045
Epoch 560, training loss: 315.7848205566406 = 0.8967388868331909 + 50.0 * 6.297761917114258
Epoch 560, val loss: 1.0913571119308472
Epoch 570, training loss: 315.956298828125 = 0.879899263381958 + 50.0 * 6.301527500152588
Epoch 570, val loss: 1.0805613994598389
Epoch 580, training loss: 315.65277099609375 = 0.8634146451950073 + 50.0 * 6.295787334442139
Epoch 580, val loss: 1.070263385772705
Epoch 590, training loss: 315.4918518066406 = 0.8472470045089722 + 50.0 * 6.292891979217529
Epoch 590, val loss: 1.060334324836731
Epoch 600, training loss: 315.42333984375 = 0.8314324021339417 + 50.0 * 6.291838645935059
Epoch 600, val loss: 1.050803303718567
Epoch 610, training loss: 315.6771240234375 = 0.8159130811691284 + 50.0 * 6.297224044799805
Epoch 610, val loss: 1.0414941310882568
Epoch 620, training loss: 315.30950927734375 = 0.8004987239837646 + 50.0 * 6.290180206298828
Epoch 620, val loss: 1.032545566558838
Epoch 630, training loss: 315.17169189453125 = 0.7854169011116028 + 50.0 * 6.28772497177124
Epoch 630, val loss: 1.0241661071777344
Epoch 640, training loss: 315.0790100097656 = 0.7705720067024231 + 50.0 * 6.286168575286865
Epoch 640, val loss: 1.0161076784133911
Epoch 650, training loss: 315.1212158203125 = 0.7559406757354736 + 50.0 * 6.2873053550720215
Epoch 650, val loss: 1.0082789659500122
Epoch 660, training loss: 314.9344482421875 = 0.7414841055870056 + 50.0 * 6.2838592529296875
Epoch 660, val loss: 1.0004324913024902
Epoch 670, training loss: 315.0035400390625 = 0.727142333984375 + 50.0 * 6.28552770614624
Epoch 670, val loss: 0.993329644203186
Epoch 680, training loss: 314.8682861328125 = 0.7130657434463501 + 50.0 * 6.283104419708252
Epoch 680, val loss: 0.9862459301948547
Epoch 690, training loss: 314.69598388671875 = 0.6990456581115723 + 50.0 * 6.2799391746521
Epoch 690, val loss: 0.979500412940979
Epoch 700, training loss: 314.6329650878906 = 0.685280978679657 + 50.0 * 6.278953552246094
Epoch 700, val loss: 0.973159670829773
Epoch 710, training loss: 314.9277648925781 = 0.6716833710670471 + 50.0 * 6.285121440887451
Epoch 710, val loss: 0.9669538736343384
Epoch 720, training loss: 314.6977233886719 = 0.6580736041069031 + 50.0 * 6.280792713165283
Epoch 720, val loss: 0.9612774848937988
Epoch 730, training loss: 314.4328308105469 = 0.6447207927703857 + 50.0 * 6.27576208114624
Epoch 730, val loss: 0.9556968212127686
Epoch 740, training loss: 314.37060546875 = 0.6315839290618896 + 50.0 * 6.2747802734375
Epoch 740, val loss: 0.9503877758979797
Epoch 750, training loss: 314.61517333984375 = 0.6185996532440186 + 50.0 * 6.279931545257568
Epoch 750, val loss: 0.9452295303344727
Epoch 760, training loss: 314.3759460449219 = 0.6058124899864197 + 50.0 * 6.275402545928955
Epoch 760, val loss: 0.9410392045974731
Epoch 770, training loss: 314.190185546875 = 0.5931307077407837 + 50.0 * 6.2719407081604
Epoch 770, val loss: 0.9363869428634644
Epoch 780, training loss: 314.135498046875 = 0.5807454586029053 + 50.0 * 6.271095275878906
Epoch 780, val loss: 0.9323206543922424
Epoch 790, training loss: 314.2177734375 = 0.5685118436813354 + 50.0 * 6.272984981536865
Epoch 790, val loss: 0.9283674359321594
Epoch 800, training loss: 314.1226806640625 = 0.556343674659729 + 50.0 * 6.271327018737793
Epoch 800, val loss: 0.9247311353683472
Epoch 810, training loss: 313.9417724609375 = 0.5444418787956238 + 50.0 * 6.267946720123291
Epoch 810, val loss: 0.9213459491729736
Epoch 820, training loss: 313.958984375 = 0.5327743291854858 + 50.0 * 6.268524169921875
Epoch 820, val loss: 0.9182946681976318
Epoch 830, training loss: 313.9677734375 = 0.5212905406951904 + 50.0 * 6.268929481506348
Epoch 830, val loss: 0.9153636693954468
Epoch 840, training loss: 313.840087890625 = 0.5099180340766907 + 50.0 * 6.266603469848633
Epoch 840, val loss: 0.9126843214035034
Epoch 850, training loss: 313.7498779296875 = 0.49878063797950745 + 50.0 * 6.265021800994873
Epoch 850, val loss: 0.9103090763092041
Epoch 860, training loss: 313.7287292480469 = 0.4878402352333069 + 50.0 * 6.264817714691162
Epoch 860, val loss: 0.9080935716629028
Epoch 870, training loss: 313.7886047363281 = 0.47706905007362366 + 50.0 * 6.266230583190918
Epoch 870, val loss: 0.9060796499252319
Epoch 880, training loss: 313.60919189453125 = 0.46640825271606445 + 50.0 * 6.2628560066223145
Epoch 880, val loss: 0.904062032699585
Epoch 890, training loss: 313.7529602050781 = 0.4559836983680725 + 50.0 * 6.265939235687256
Epoch 890, val loss: 0.9022266864776611
Epoch 900, training loss: 313.74755859375 = 0.4457297623157501 + 50.0 * 6.266036510467529
Epoch 900, val loss: 0.9009713530540466
Epoch 910, training loss: 313.5273132324219 = 0.4355276823043823 + 50.0 * 6.261836051940918
Epoch 910, val loss: 0.899270236492157
Epoch 920, training loss: 313.4144287109375 = 0.4256161153316498 + 50.0 * 6.2597761154174805
Epoch 920, val loss: 0.8981614112854004
Epoch 930, training loss: 313.36260986328125 = 0.4158936142921448 + 50.0 * 6.258934497833252
Epoch 930, val loss: 0.8972105979919434
Epoch 940, training loss: 313.69085693359375 = 0.40633273124694824 + 50.0 * 6.265690326690674
Epoch 940, val loss: 0.8964823484420776
Epoch 950, training loss: 313.47930908203125 = 0.39691361784935 + 50.0 * 6.261648178100586
Epoch 950, val loss: 0.8952437043190002
Epoch 960, training loss: 313.2340087890625 = 0.3876800835132599 + 50.0 * 6.256926536560059
Epoch 960, val loss: 0.8948737978935242
Epoch 970, training loss: 313.183349609375 = 0.3786984384059906 + 50.0 * 6.2560930252075195
Epoch 970, val loss: 0.8944454789161682
Epoch 980, training loss: 313.42315673828125 = 0.36992189288139343 + 50.0 * 6.261064529418945
Epoch 980, val loss: 0.8941960334777832
Epoch 990, training loss: 313.5098571777344 = 0.3611389398574829 + 50.0 * 6.262974262237549
Epoch 990, val loss: 0.8936623334884644
Epoch 1000, training loss: 313.1424560546875 = 0.3526875674724579 + 50.0 * 6.255795001983643
Epoch 1000, val loss: 0.8935077786445618
Epoch 1010, training loss: 313.0621032714844 = 0.3444063067436218 + 50.0 * 6.2543535232543945
Epoch 1010, val loss: 0.8937570452690125
Epoch 1020, training loss: 313.0096740722656 = 0.33633607625961304 + 50.0 * 6.253467082977295
Epoch 1020, val loss: 0.8938913941383362
Epoch 1030, training loss: 313.2314758300781 = 0.3285081088542938 + 50.0 * 6.258059024810791
Epoch 1030, val loss: 0.8942996263504028
Epoch 1040, training loss: 313.0941467285156 = 0.320740282535553 + 50.0 * 6.255468368530273
Epoch 1040, val loss: 0.8944812417030334
Epoch 1050, training loss: 312.9493713378906 = 0.31319981813430786 + 50.0 * 6.252723217010498
Epoch 1050, val loss: 0.8951697945594788
Epoch 1060, training loss: 312.92205810546875 = 0.30583059787750244 + 50.0 * 6.25232458114624
Epoch 1060, val loss: 0.8956243395805359
Epoch 1070, training loss: 312.935546875 = 0.2986830174922943 + 50.0 * 6.252737522125244
Epoch 1070, val loss: 0.8964354395866394
Epoch 1080, training loss: 312.9363098144531 = 0.2916591763496399 + 50.0 * 6.252892971038818
Epoch 1080, val loss: 0.8972680568695068
Epoch 1090, training loss: 312.9438781738281 = 0.2848367393016815 + 50.0 * 6.253180503845215
Epoch 1090, val loss: 0.8981894254684448
Epoch 1100, training loss: 312.7970886230469 = 0.27814406156539917 + 50.0 * 6.2503790855407715
Epoch 1100, val loss: 0.8991987705230713
Epoch 1110, training loss: 312.7284240722656 = 0.2716403603553772 + 50.0 * 6.249135971069336
Epoch 1110, val loss: 0.9001299738883972
Epoch 1120, training loss: 312.7663879394531 = 0.2653332054615021 + 50.0 * 6.250021457672119
Epoch 1120, val loss: 0.9013575911521912
Epoch 1130, training loss: 312.6708679199219 = 0.25916334986686707 + 50.0 * 6.248234272003174
Epoch 1130, val loss: 0.9024699926376343
Epoch 1140, training loss: 312.7958984375 = 0.25312361121177673 + 50.0 * 6.250855445861816
Epoch 1140, val loss: 0.9038307666778564
Epoch 1150, training loss: 312.68212890625 = 0.2472572773694992 + 50.0 * 6.248697280883789
Epoch 1150, val loss: 0.9050944447517395
Epoch 1160, training loss: 312.5431213378906 = 0.2415456473827362 + 50.0 * 6.246031761169434
Epoch 1160, val loss: 0.9065279364585876
Epoch 1170, training loss: 312.5530090332031 = 0.2360001653432846 + 50.0 * 6.246340274810791
Epoch 1170, val loss: 0.9080340266227722
Epoch 1180, training loss: 312.606201171875 = 0.23061060905456543 + 50.0 * 6.247511386871338
Epoch 1180, val loss: 0.9095763564109802
Epoch 1190, training loss: 312.6401672363281 = 0.22531872987747192 + 50.0 * 6.248297214508057
Epoch 1190, val loss: 0.9113873243331909
Epoch 1200, training loss: 312.51580810546875 = 0.22017279267311096 + 50.0 * 6.245912551879883
Epoch 1200, val loss: 0.9128743410110474
Epoch 1210, training loss: 312.55120849609375 = 0.21516285836696625 + 50.0 * 6.246721267700195
Epoch 1210, val loss: 0.9147047400474548
Epoch 1220, training loss: 312.4303894042969 = 0.2102571576833725 + 50.0 * 6.2444024085998535
Epoch 1220, val loss: 0.9163925647735596
Epoch 1230, training loss: 312.37506103515625 = 0.20547904074192047 + 50.0 * 6.243391990661621
Epoch 1230, val loss: 0.9181336164474487
Epoch 1240, training loss: 312.37646484375 = 0.2008739411830902 + 50.0 * 6.24351167678833
Epoch 1240, val loss: 0.9201483130455017
Epoch 1250, training loss: 312.38909912109375 = 0.1963610202074051 + 50.0 * 6.243854999542236
Epoch 1250, val loss: 0.9220848083496094
Epoch 1260, training loss: 312.4937438964844 = 0.19195236265659332 + 50.0 * 6.246036052703857
Epoch 1260, val loss: 0.9241181015968323
Epoch 1270, training loss: 312.2604675292969 = 0.18760333955287933 + 50.0 * 6.241457462310791
Epoch 1270, val loss: 0.9260408282279968
Epoch 1280, training loss: 312.21990966796875 = 0.18342255055904388 + 50.0 * 6.240730285644531
Epoch 1280, val loss: 0.9281699061393738
Epoch 1290, training loss: 312.4567565917969 = 0.179353728890419 + 50.0 * 6.245548248291016
Epoch 1290, val loss: 0.9303361177444458
Epoch 1300, training loss: 312.192138671875 = 0.17536987364292145 + 50.0 * 6.240334987640381
Epoch 1300, val loss: 0.9323435425758362
Epoch 1310, training loss: 312.1339111328125 = 0.17149357497692108 + 50.0 * 6.239247798919678
Epoch 1310, val loss: 0.9347009658813477
Epoch 1320, training loss: 312.562744140625 = 0.16774243116378784 + 50.0 * 6.247899532318115
Epoch 1320, val loss: 0.9370575547218323
Epoch 1330, training loss: 312.4620361328125 = 0.164006769657135 + 50.0 * 6.245960712432861
Epoch 1330, val loss: 0.9388635754585266
Epoch 1340, training loss: 312.0693664550781 = 0.1603652983903885 + 50.0 * 6.238180160522461
Epoch 1340, val loss: 0.9412297606468201
Epoch 1350, training loss: 312.0580139160156 = 0.15685367584228516 + 50.0 * 6.238022804260254
Epoch 1350, val loss: 0.9436389803886414
Epoch 1360, training loss: 311.98321533203125 = 0.15344280004501343 + 50.0 * 6.236595630645752
Epoch 1360, val loss: 0.9460557103157043
Epoch 1370, training loss: 312.02978515625 = 0.15012356638908386 + 50.0 * 6.237593650817871
Epoch 1370, val loss: 0.948508620262146
Epoch 1380, training loss: 312.0788269042969 = 0.1468603014945984 + 50.0 * 6.2386393547058105
Epoch 1380, val loss: 0.9508848786354065
Epoch 1390, training loss: 312.13775634765625 = 0.14365769922733307 + 50.0 * 6.239882469177246
Epoch 1390, val loss: 0.9532871842384338
Epoch 1400, training loss: 311.9274597167969 = 0.1405038982629776 + 50.0 * 6.235739231109619
Epoch 1400, val loss: 0.9557417035102844
Epoch 1410, training loss: 312.0409851074219 = 0.137454554438591 + 50.0 * 6.238070487976074
Epoch 1410, val loss: 0.9584237337112427
Epoch 1420, training loss: 311.9706726074219 = 0.1344757378101349 + 50.0 * 6.23672342300415
Epoch 1420, val loss: 0.9609353542327881
Epoch 1430, training loss: 311.8719787597656 = 0.13156862556934357 + 50.0 * 6.234808444976807
Epoch 1430, val loss: 0.9636067748069763
Epoch 1440, training loss: 311.8351135253906 = 0.12873226404190063 + 50.0 * 6.234127998352051
Epoch 1440, val loss: 0.9662614464759827
Epoch 1450, training loss: 311.9184265136719 = 0.12597642838954926 + 50.0 * 6.235848903656006
Epoch 1450, val loss: 0.9689491391181946
Epoch 1460, training loss: 311.8370666503906 = 0.12324220687150955 + 50.0 * 6.23427677154541
Epoch 1460, val loss: 0.9714675545692444
Epoch 1470, training loss: 311.8493957519531 = 0.12056513875722885 + 50.0 * 6.234576225280762
Epoch 1470, val loss: 0.9743049740791321
Epoch 1480, training loss: 311.9057922363281 = 0.11796502023935318 + 50.0 * 6.235756874084473
Epoch 1480, val loss: 0.9769788980484009
Epoch 1490, training loss: 311.7416687011719 = 0.11543497443199158 + 50.0 * 6.232524394989014
Epoch 1490, val loss: 0.9798233509063721
Epoch 1500, training loss: 311.9063720703125 = 0.11298482120037079 + 50.0 * 6.235867977142334
Epoch 1500, val loss: 0.9825603365898132
Epoch 1510, training loss: 311.74676513671875 = 0.11053141206502914 + 50.0 * 6.232724666595459
Epoch 1510, val loss: 0.9854071736335754
Epoch 1520, training loss: 311.69647216796875 = 0.1081627756357193 + 50.0 * 6.231766223907471
Epoch 1520, val loss: 0.9881663918495178
Epoch 1530, training loss: 311.64422607421875 = 0.10585680603981018 + 50.0 * 6.230767250061035
Epoch 1530, val loss: 0.9912098050117493
Epoch 1540, training loss: 311.63690185546875 = 0.10361769795417786 + 50.0 * 6.230666160583496
Epoch 1540, val loss: 0.9940854907035828
Epoch 1550, training loss: 312.02288818359375 = 0.10142678022384644 + 50.0 * 6.238429069519043
Epoch 1550, val loss: 0.9969146847724915
Epoch 1560, training loss: 311.81903076171875 = 0.09926162660121918 + 50.0 * 6.2343950271606445
Epoch 1560, val loss: 0.9999737739562988
Epoch 1570, training loss: 311.8636779785156 = 0.09714017063379288 + 50.0 * 6.235331058502197
Epoch 1570, val loss: 1.0029276609420776
Epoch 1580, training loss: 311.7637634277344 = 0.09507870674133301 + 50.0 * 6.233373165130615
Epoch 1580, val loss: 1.0056899785995483
Epoch 1590, training loss: 311.576416015625 = 0.09304719418287277 + 50.0 * 6.2296671867370605
Epoch 1590, val loss: 1.0087242126464844
Epoch 1600, training loss: 311.5307922363281 = 0.09109955281019211 + 50.0 * 6.228794097900391
Epoch 1600, val loss: 1.0117075443267822
Epoch 1610, training loss: 311.54876708984375 = 0.08919792622327805 + 50.0 * 6.229191303253174
Epoch 1610, val loss: 1.014693260192871
Epoch 1620, training loss: 311.7134704589844 = 0.08733795583248138 + 50.0 * 6.232522487640381
Epoch 1620, val loss: 1.017660140991211
Epoch 1630, training loss: 311.5313415527344 = 0.08550098538398743 + 50.0 * 6.228916645050049
Epoch 1630, val loss: 1.0207834243774414
Epoch 1640, training loss: 311.5495910644531 = 0.08371418714523315 + 50.0 * 6.229317665100098
Epoch 1640, val loss: 1.0238251686096191
Epoch 1650, training loss: 311.6124267578125 = 0.08197145164012909 + 50.0 * 6.23060941696167
Epoch 1650, val loss: 1.0269147157669067
Epoch 1660, training loss: 311.46630859375 = 0.08025854825973511 + 50.0 * 6.227721214294434
Epoch 1660, val loss: 1.0300387144088745
Epoch 1670, training loss: 311.4417724609375 = 0.07859940826892853 + 50.0 * 6.227263450622559
Epoch 1670, val loss: 1.0331014394760132
Epoch 1680, training loss: 311.7660217285156 = 0.07698944211006165 + 50.0 * 6.233780384063721
Epoch 1680, val loss: 1.0363914966583252
Epoch 1690, training loss: 311.49456787109375 = 0.07539315521717072 + 50.0 * 6.228384017944336
Epoch 1690, val loss: 1.0390712022781372
Epoch 1700, training loss: 311.4812316894531 = 0.073847196996212 + 50.0 * 6.228147506713867
Epoch 1700, val loss: 1.0424038171768188
Epoch 1710, training loss: 311.4765319824219 = 0.07233341038227081 + 50.0 * 6.228084087371826
Epoch 1710, val loss: 1.0454260110855103
Epoch 1720, training loss: 311.3863220214844 = 0.07084866613149643 + 50.0 * 6.226309299468994
Epoch 1720, val loss: 1.0486787557601929
Epoch 1730, training loss: 311.65655517578125 = 0.06942009180784225 + 50.0 * 6.2317423820495605
Epoch 1730, val loss: 1.0516775846481323
Epoch 1740, training loss: 311.3875732421875 = 0.06799959391355515 + 50.0 * 6.226391315460205
Epoch 1740, val loss: 1.0549954175949097
Epoch 1750, training loss: 311.3213195800781 = 0.06661295890808105 + 50.0 * 6.225093841552734
Epoch 1750, val loss: 1.0580872297286987
Epoch 1760, training loss: 311.3963928222656 = 0.06527804583311081 + 50.0 * 6.226622104644775
Epoch 1760, val loss: 1.0613689422607422
Epoch 1770, training loss: 311.27349853515625 = 0.06396131962537766 + 50.0 * 6.224190711975098
Epoch 1770, val loss: 1.0644712448120117
Epoch 1780, training loss: 311.3865966796875 = 0.06268759816884995 + 50.0 * 6.226478576660156
Epoch 1780, val loss: 1.067694902420044
Epoch 1790, training loss: 311.2824401855469 = 0.06143403425812721 + 50.0 * 6.224420070648193
Epoch 1790, val loss: 1.0707855224609375
Epoch 1800, training loss: 311.2273864746094 = 0.06021425500512123 + 50.0 * 6.223343372344971
Epoch 1800, val loss: 1.0739071369171143
Epoch 1810, training loss: 311.273193359375 = 0.05902550369501114 + 50.0 * 6.224283218383789
Epoch 1810, val loss: 1.0772452354431152
Epoch 1820, training loss: 311.6103515625 = 0.057871293276548386 + 50.0 * 6.23105001449585
Epoch 1820, val loss: 1.0803297758102417
Epoch 1830, training loss: 311.1913146972656 = 0.05670877546072006 + 50.0 * 6.222692012786865
Epoch 1830, val loss: 1.0833868980407715
Epoch 1840, training loss: 311.15020751953125 = 0.05559443682432175 + 50.0 * 6.221892356872559
Epoch 1840, val loss: 1.0866332054138184
Epoch 1850, training loss: 311.1355895996094 = 0.05451915040612221 + 50.0 * 6.221621990203857
Epoch 1850, val loss: 1.0899568796157837
Epoch 1860, training loss: 311.2093505859375 = 0.0534798763692379 + 50.0 * 6.223117828369141
Epoch 1860, val loss: 1.0930742025375366
Epoch 1870, training loss: 311.34564208984375 = 0.05245284363627434 + 50.0 * 6.225863456726074
Epoch 1870, val loss: 1.0960878133773804
Epoch 1880, training loss: 311.1458435058594 = 0.05142435431480408 + 50.0 * 6.221888542175293
Epoch 1880, val loss: 1.0992008447647095
Epoch 1890, training loss: 311.136962890625 = 0.05043558031320572 + 50.0 * 6.2217302322387695
Epoch 1890, val loss: 1.1024466753005981
Epoch 1900, training loss: 311.2307434082031 = 0.04947993531823158 + 50.0 * 6.223625659942627
Epoch 1900, val loss: 1.1056652069091797
Epoch 1910, training loss: 311.3280334472656 = 0.048537496477365494 + 50.0 * 6.225590229034424
Epoch 1910, val loss: 1.1086817979812622
Epoch 1920, training loss: 311.078125 = 0.047619059681892395 + 50.0 * 6.22061014175415
Epoch 1920, val loss: 1.1115766763687134
Epoch 1930, training loss: 311.0220031738281 = 0.04671887680888176 + 50.0 * 6.219505786895752
Epoch 1930, val loss: 1.1148840188980103
Epoch 1940, training loss: 311.01495361328125 = 0.045855745673179626 + 50.0 * 6.219381809234619
Epoch 1940, val loss: 1.1180291175842285
Epoch 1950, training loss: 311.20703125 = 0.04502144828438759 + 50.0 * 6.223240375518799
Epoch 1950, val loss: 1.1211299896240234
Epoch 1960, training loss: 311.097900390625 = 0.04417378455400467 + 50.0 * 6.221075057983398
Epoch 1960, val loss: 1.1240036487579346
Epoch 1970, training loss: 311.005126953125 = 0.04335358738899231 + 50.0 * 6.219235420227051
Epoch 1970, val loss: 1.1271207332611084
Epoch 1980, training loss: 311.0337219238281 = 0.04255972057580948 + 50.0 * 6.219822883605957
Epoch 1980, val loss: 1.1302802562713623
Epoch 1990, training loss: 311.2206115722656 = 0.04179311916232109 + 50.0 * 6.223576545715332
Epoch 1990, val loss: 1.1333787441253662
Epoch 2000, training loss: 311.0474853515625 = 0.0410303920507431 + 50.0 * 6.220129489898682
Epoch 2000, val loss: 1.1361937522888184
Epoch 2010, training loss: 311.0841064453125 = 0.040289971977472305 + 50.0 * 6.220876216888428
Epoch 2010, val loss: 1.1392735242843628
Epoch 2020, training loss: 310.94415283203125 = 0.03955770283937454 + 50.0 * 6.21809196472168
Epoch 2020, val loss: 1.142378807067871
Epoch 2030, training loss: 310.94842529296875 = 0.03886052221059799 + 50.0 * 6.218191623687744
Epoch 2030, val loss: 1.1454217433929443
Epoch 2040, training loss: 310.9812316894531 = 0.03817371651530266 + 50.0 * 6.218861103057861
Epoch 2040, val loss: 1.148474097251892
Epoch 2050, training loss: 311.2012634277344 = 0.03751160949468613 + 50.0 * 6.223275184631348
Epoch 2050, val loss: 1.1513713598251343
Epoch 2060, training loss: 311.0188903808594 = 0.03683483973145485 + 50.0 * 6.219641208648682
Epoch 2060, val loss: 1.1543093919754028
Epoch 2070, training loss: 310.9037170410156 = 0.03617881238460541 + 50.0 * 6.217350482940674
Epoch 2070, val loss: 1.1573115587234497
Epoch 2080, training loss: 310.85205078125 = 0.03554984927177429 + 50.0 * 6.216330051422119
Epoch 2080, val loss: 1.1604176759719849
Epoch 2090, training loss: 310.8570556640625 = 0.03494188189506531 + 50.0 * 6.216442108154297
Epoch 2090, val loss: 1.1634776592254639
Epoch 2100, training loss: 311.23065185546875 = 0.03435123339295387 + 50.0 * 6.223925590515137
Epoch 2100, val loss: 1.1663668155670166
Epoch 2110, training loss: 311.09405517578125 = 0.033762264996767044 + 50.0 * 6.221206188201904
Epoch 2110, val loss: 1.1690500974655151
Epoch 2120, training loss: 310.9053955078125 = 0.03317316249012947 + 50.0 * 6.21744441986084
Epoch 2120, val loss: 1.1719943284988403
Epoch 2130, training loss: 310.8489685058594 = 0.03261249512434006 + 50.0 * 6.216326713562012
Epoch 2130, val loss: 1.1749402284622192
Epoch 2140, training loss: 310.8802795410156 = 0.03206845000386238 + 50.0 * 6.216964244842529
Epoch 2140, val loss: 1.1777809858322144
Epoch 2150, training loss: 310.8896789550781 = 0.0315355584025383 + 50.0 * 6.2171630859375
Epoch 2150, val loss: 1.1807183027267456
Epoch 2160, training loss: 310.9032897949219 = 0.03101370669901371 + 50.0 * 6.2174458503723145
Epoch 2160, val loss: 1.18351411819458
Epoch 2170, training loss: 310.8835144042969 = 0.0304899699985981 + 50.0 * 6.217060565948486
Epoch 2170, val loss: 1.1864492893218994
Epoch 2180, training loss: 310.88525390625 = 0.02999028190970421 + 50.0 * 6.217105388641357
Epoch 2180, val loss: 1.1891744136810303
Epoch 2190, training loss: 310.7344055175781 = 0.029496537521481514 + 50.0 * 6.21409797668457
Epoch 2190, val loss: 1.1920289993286133
Epoch 2200, training loss: 310.8424072265625 = 0.029027298092842102 + 50.0 * 6.2162675857543945
Epoch 2200, val loss: 1.1948676109313965
Epoch 2210, training loss: 311.0087585449219 = 0.0285593643784523 + 50.0 * 6.219604015350342
Epoch 2210, val loss: 1.1975388526916504
Epoch 2220, training loss: 310.765625 = 0.028086399659514427 + 50.0 * 6.214751243591309
Epoch 2220, val loss: 1.2002718448638916
Epoch 2230, training loss: 310.7151794433594 = 0.027638601139187813 + 50.0 * 6.21375036239624
Epoch 2230, val loss: 1.2031630277633667
Epoch 2240, training loss: 310.8067321777344 = 0.027204999700188637 + 50.0 * 6.215590953826904
Epoch 2240, val loss: 1.205956220626831
Epoch 2250, training loss: 310.8125915527344 = 0.026779118925333023 + 50.0 * 6.215716361999512
Epoch 2250, val loss: 1.208583116531372
Epoch 2260, training loss: 310.9018859863281 = 0.02635972946882248 + 50.0 * 6.217510223388672
Epoch 2260, val loss: 1.2111233472824097
Epoch 2270, training loss: 310.7657470703125 = 0.02594291977584362 + 50.0 * 6.21479606628418
Epoch 2270, val loss: 1.2138116359710693
Epoch 2280, training loss: 310.7741394042969 = 0.02554267831146717 + 50.0 * 6.214972019195557
Epoch 2280, val loss: 1.216469645500183
Epoch 2290, training loss: 310.7249755859375 = 0.025148333981633186 + 50.0 * 6.213996887207031
Epoch 2290, val loss: 1.219245433807373
Epoch 2300, training loss: 310.6457824707031 = 0.02476469799876213 + 50.0 * 6.212420463562012
Epoch 2300, val loss: 1.2218786478042603
Epoch 2310, training loss: 310.6436767578125 = 0.02439151331782341 + 50.0 * 6.212385654449463
Epoch 2310, val loss: 1.2245938777923584
Epoch 2320, training loss: 310.8514099121094 = 0.02403423935174942 + 50.0 * 6.216547012329102
Epoch 2320, val loss: 1.227213978767395
Epoch 2330, training loss: 310.68017578125 = 0.023666882887482643 + 50.0 * 6.213129997253418
Epoch 2330, val loss: 1.2295464277267456
Epoch 2340, training loss: 310.69354248046875 = 0.02331256866455078 + 50.0 * 6.213404655456543
Epoch 2340, val loss: 1.2321693897247314
Epoch 2350, training loss: 310.8918762207031 = 0.022973159328103065 + 50.0 * 6.21737813949585
Epoch 2350, val loss: 1.234603762626648
Epoch 2360, training loss: 310.691162109375 = 0.022618282586336136 + 50.0 * 6.2133708000183105
Epoch 2360, val loss: 1.2372186183929443
Epoch 2370, training loss: 310.7200927734375 = 0.022288961336016655 + 50.0 * 6.213956356048584
Epoch 2370, val loss: 1.2396899461746216
Epoch 2380, training loss: 310.65771484375 = 0.021964864805340767 + 50.0 * 6.212714672088623
Epoch 2380, val loss: 1.2423244714736938
Epoch 2390, training loss: 310.8034362792969 = 0.02164933830499649 + 50.0 * 6.215636253356934
Epoch 2390, val loss: 1.2447428703308105
Epoch 2400, training loss: 310.5704650878906 = 0.021333862096071243 + 50.0 * 6.210982799530029
Epoch 2400, val loss: 1.2473609447479248
Epoch 2410, training loss: 310.55084228515625 = 0.021030502393841743 + 50.0 * 6.210596084594727
Epoch 2410, val loss: 1.2499102354049683
Epoch 2420, training loss: 310.57440185546875 = 0.02073683775961399 + 50.0 * 6.21107292175293
Epoch 2420, val loss: 1.2524672746658325
Epoch 2430, training loss: 310.87896728515625 = 0.02045425958931446 + 50.0 * 6.217170238494873
Epoch 2430, val loss: 1.254854679107666
Epoch 2440, training loss: 310.6778259277344 = 0.020159728825092316 + 50.0 * 6.213152885437012
Epoch 2440, val loss: 1.2571159601211548
Epoch 2450, training loss: 310.6028747558594 = 0.01987295225262642 + 50.0 * 6.211659908294678
Epoch 2450, val loss: 1.2596468925476074
Epoch 2460, training loss: 310.6085510253906 = 0.019599003717303276 + 50.0 * 6.2117791175842285
Epoch 2460, val loss: 1.2620669603347778
Epoch 2470, training loss: 310.66619873046875 = 0.01933143474161625 + 50.0 * 6.212937355041504
Epoch 2470, val loss: 1.264559268951416
Epoch 2480, training loss: 310.7187194824219 = 0.01906871609389782 + 50.0 * 6.213993072509766
Epoch 2480, val loss: 1.2667407989501953
Epoch 2490, training loss: 310.6859130859375 = 0.018808992579579353 + 50.0 * 6.213342189788818
Epoch 2490, val loss: 1.2691404819488525
Epoch 2500, training loss: 310.5378112792969 = 0.018549207597970963 + 50.0 * 6.210384845733643
Epoch 2500, val loss: 1.2715901136398315
Epoch 2510, training loss: 310.5268859863281 = 0.018303873017430305 + 50.0 * 6.210171222686768
Epoch 2510, val loss: 1.27399742603302
Epoch 2520, training loss: 310.6985778808594 = 0.018063606694340706 + 50.0 * 6.2136101722717285
Epoch 2520, val loss: 1.276361346244812
Epoch 2530, training loss: 310.5762023925781 = 0.01782424934208393 + 50.0 * 6.211167335510254
Epoch 2530, val loss: 1.2785545587539673
Epoch 2540, training loss: 310.56268310546875 = 0.017587510868906975 + 50.0 * 6.210902214050293
Epoch 2540, val loss: 1.2808908224105835
Epoch 2550, training loss: 310.50048828125 = 0.017358927056193352 + 50.0 * 6.209662437438965
Epoch 2550, val loss: 1.2831937074661255
Epoch 2560, training loss: 310.52349853515625 = 0.01713484525680542 + 50.0 * 6.210127353668213
Epoch 2560, val loss: 1.2855205535888672
Epoch 2570, training loss: 310.554443359375 = 0.016916818916797638 + 50.0 * 6.210750579833984
Epoch 2570, val loss: 1.2876664400100708
Epoch 2580, training loss: 310.6187438964844 = 0.016698792576789856 + 50.0 * 6.212040901184082
Epoch 2580, val loss: 1.2899527549743652
Epoch 2590, training loss: 310.5098876953125 = 0.016484113410115242 + 50.0 * 6.20986795425415
Epoch 2590, val loss: 1.2922078371047974
Epoch 2600, training loss: 310.4299011230469 = 0.016275634989142418 + 50.0 * 6.208272457122803
Epoch 2600, val loss: 1.2944636344909668
Epoch 2610, training loss: 310.49053955078125 = 0.016075368970632553 + 50.0 * 6.209489345550537
Epoch 2610, val loss: 1.2966943979263306
Epoch 2620, training loss: 310.54705810546875 = 0.01587788760662079 + 50.0 * 6.210623741149902
Epoch 2620, val loss: 1.2987643480300903
Epoch 2630, training loss: 310.58331298828125 = 0.015682127326726913 + 50.0 * 6.211352825164795
Epoch 2630, val loss: 1.3007981777191162
Epoch 2640, training loss: 310.5184631347656 = 0.015484809875488281 + 50.0 * 6.210060119628906
Epoch 2640, val loss: 1.3031476736068726
Epoch 2650, training loss: 310.5255126953125 = 0.015296590514481068 + 50.0 * 6.210204601287842
Epoch 2650, val loss: 1.305269479751587
Epoch 2660, training loss: 310.5052185058594 = 0.015110142529010773 + 50.0 * 6.209802150726318
Epoch 2660, val loss: 1.3074769973754883
Epoch 2670, training loss: 310.558349609375 = 0.014929858036339283 + 50.0 * 6.2108683586120605
Epoch 2670, val loss: 1.30941641330719
Epoch 2680, training loss: 310.3912353515625 = 0.014745546504855156 + 50.0 * 6.2075300216674805
Epoch 2680, val loss: 1.3118152618408203
Epoch 2690, training loss: 310.53155517578125 = 0.014571803621947765 + 50.0 * 6.2103400230407715
Epoch 2690, val loss: 1.313981056213379
Epoch 2700, training loss: 310.4313049316406 = 0.014395052567124367 + 50.0 * 6.208337783813477
Epoch 2700, val loss: 1.3158966302871704
Epoch 2710, training loss: 310.5367736816406 = 0.014227611944079399 + 50.0 * 6.210451126098633
Epoch 2710, val loss: 1.3178952932357788
Epoch 2720, training loss: 310.3379821777344 = 0.014056184329092503 + 50.0 * 6.206478118896484
Epoch 2720, val loss: 1.3200925588607788
Epoch 2730, training loss: 310.345703125 = 0.013894551433622837 + 50.0 * 6.206636428833008
Epoch 2730, val loss: 1.3221495151519775
Epoch 2740, training loss: 310.3217468261719 = 0.013738180510699749 + 50.0 * 6.206160068511963
Epoch 2740, val loss: 1.3243627548217773
Epoch 2750, training loss: 310.57867431640625 = 0.013588900677859783 + 50.0 * 6.211301803588867
Epoch 2750, val loss: 1.326396107673645
Epoch 2760, training loss: 310.49298095703125 = 0.013429619371891022 + 50.0 * 6.209590911865234
Epoch 2760, val loss: 1.3280495405197144
Epoch 2770, training loss: 310.3790588378906 = 0.013269701041281223 + 50.0 * 6.207315444946289
Epoch 2770, val loss: 1.3303543329238892
Epoch 2780, training loss: 310.3767395019531 = 0.013119492679834366 + 50.0 * 6.207272052764893
Epoch 2780, val loss: 1.3322527408599854
Epoch 2790, training loss: 310.4075927734375 = 0.012975466437637806 + 50.0 * 6.207892417907715
Epoch 2790, val loss: 1.3343689441680908
Epoch 2800, training loss: 310.325927734375 = 0.012830355204641819 + 50.0 * 6.206262111663818
Epoch 2800, val loss: 1.3362197875976562
Epoch 2810, training loss: 310.3247375488281 = 0.012688134796917439 + 50.0 * 6.206240653991699
Epoch 2810, val loss: 1.3383222818374634
Epoch 2820, training loss: 310.3129577636719 = 0.012549256905913353 + 50.0 * 6.206008434295654
Epoch 2820, val loss: 1.340293526649475
Epoch 2830, training loss: 310.2891540527344 = 0.012414071708917618 + 50.0 * 6.2055344581604
Epoch 2830, val loss: 1.342309832572937
Epoch 2840, training loss: 310.6845703125 = 0.012285351753234863 + 50.0 * 6.21344518661499
Epoch 2840, val loss: 1.3442918062210083
Epoch 2850, training loss: 310.6446228027344 = 0.012150421738624573 + 50.0 * 6.212649822235107
Epoch 2850, val loss: 1.3457601070404053
Epoch 2860, training loss: 310.2884521484375 = 0.01200916338711977 + 50.0 * 6.205528259277344
Epoch 2860, val loss: 1.3477116823196411
Epoch 2870, training loss: 310.220703125 = 0.01188114657998085 + 50.0 * 6.204176425933838
Epoch 2870, val loss: 1.3497473001480103
Epoch 2880, training loss: 310.2383728027344 = 0.011758859269320965 + 50.0 * 6.204532623291016
Epoch 2880, val loss: 1.351778507232666
Epoch 2890, training loss: 310.5364990234375 = 0.011639554053544998 + 50.0 * 6.2104973793029785
Epoch 2890, val loss: 1.353658676147461
Epoch 2900, training loss: 310.2746276855469 = 0.01151683647185564 + 50.0 * 6.205262660980225
Epoch 2900, val loss: 1.3552645444869995
Epoch 2910, training loss: 310.276123046875 = 0.011394924484193325 + 50.0 * 6.205294609069824
Epoch 2910, val loss: 1.3571857213974
Epoch 2920, training loss: 310.27716064453125 = 0.011277223937213421 + 50.0 * 6.205317497253418
Epoch 2920, val loss: 1.3590295314788818
Epoch 2930, training loss: 310.23724365234375 = 0.011162476614117622 + 50.0 * 6.204521179199219
Epoch 2930, val loss: 1.3608912229537964
Epoch 2940, training loss: 310.2797546386719 = 0.01105156447738409 + 50.0 * 6.205374240875244
Epoch 2940, val loss: 1.3627263307571411
Epoch 2950, training loss: 310.2447509765625 = 0.010941177606582642 + 50.0 * 6.204675674438477
Epoch 2950, val loss: 1.3644071817398071
Epoch 2960, training loss: 310.3973693847656 = 0.010833431966602802 + 50.0 * 6.207730770111084
Epoch 2960, val loss: 1.3661463260650635
Epoch 2970, training loss: 310.31512451171875 = 0.010724019259214401 + 50.0 * 6.206088542938232
Epoch 2970, val loss: 1.3680216073989868
Epoch 2980, training loss: 310.3794250488281 = 0.010617558844387531 + 50.0 * 6.207376003265381
Epoch 2980, val loss: 1.3697071075439453
Epoch 2990, training loss: 310.2673645019531 = 0.010509038344025612 + 50.0 * 6.205137252807617
Epoch 2990, val loss: 1.3716071844100952
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8176067474960464
The final CL Acc:0.70123, 0.00462, The final GNN Acc:0.81638, 0.00174
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13202])
remove edge: torch.Size([2, 7872])
updated graph: torch.Size([2, 10518])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.779296875 = 1.9376918077468872 + 50.0 * 8.596832275390625
Epoch 0, val loss: 1.9333211183547974
Epoch 10, training loss: 431.7243347167969 = 1.9288249015808105 + 50.0 * 8.59591007232666
Epoch 10, val loss: 1.9243487119674683
Epoch 20, training loss: 431.3603820800781 = 1.918030858039856 + 50.0 * 8.588847160339355
Epoch 20, val loss: 1.913499116897583
Epoch 30, training loss: 428.8313903808594 = 1.9042668342590332 + 50.0 * 8.538542747497559
Epoch 30, val loss: 1.899769902229309
Epoch 40, training loss: 414.6658020019531 = 1.8871384859085083 + 50.0 * 8.255573272705078
Epoch 40, val loss: 1.8831050395965576
Epoch 50, training loss: 389.4070739746094 = 1.8660972118377686 + 50.0 * 7.750819683074951
Epoch 50, val loss: 1.8637702465057373
Epoch 60, training loss: 375.63104248046875 = 1.851867437362671 + 50.0 * 7.475583553314209
Epoch 60, val loss: 1.8517078161239624
Epoch 70, training loss: 364.8069763183594 = 1.8438135385513306 + 50.0 * 7.259263515472412
Epoch 70, val loss: 1.8443031311035156
Epoch 80, training loss: 354.62005615234375 = 1.8363233804702759 + 50.0 * 7.0556745529174805
Epoch 80, val loss: 1.8372479677200317
Epoch 90, training loss: 346.0694885253906 = 1.8293018341064453 + 50.0 * 6.884803771972656
Epoch 90, val loss: 1.8311336040496826
Epoch 100, training loss: 339.4385070800781 = 1.8238325119018555 + 50.0 * 6.752293586730957
Epoch 100, val loss: 1.8257548809051514
Epoch 110, training loss: 336.1032409667969 = 1.8167158365249634 + 50.0 * 6.685730457305908
Epoch 110, val loss: 1.8188912868499756
Epoch 120, training loss: 333.8045349121094 = 1.808722972869873 + 50.0 * 6.63991641998291
Epoch 120, val loss: 1.8114020824432373
Epoch 130, training loss: 332.08013916015625 = 1.8009238243103027 + 50.0 * 6.605584144592285
Epoch 130, val loss: 1.8041530847549438
Epoch 140, training loss: 330.7567138671875 = 1.7932935953140259 + 50.0 * 6.579267978668213
Epoch 140, val loss: 1.79710853099823
Epoch 150, training loss: 329.4006652832031 = 1.7856720685958862 + 50.0 * 6.552299976348877
Epoch 150, val loss: 1.7899855375289917
Epoch 160, training loss: 328.2702941894531 = 1.7778174877166748 + 50.0 * 6.529849052429199
Epoch 160, val loss: 1.7826745510101318
Epoch 170, training loss: 327.2197265625 = 1.769502878189087 + 50.0 * 6.509004592895508
Epoch 170, val loss: 1.7750576734542847
Epoch 180, training loss: 326.1957702636719 = 1.760660171508789 + 50.0 * 6.488702297210693
Epoch 180, val loss: 1.7671048641204834
Epoch 190, training loss: 325.408203125 = 1.7511578798294067 + 50.0 * 6.473140716552734
Epoch 190, val loss: 1.7586724758148193
Epoch 200, training loss: 324.54840087890625 = 1.740638017654419 + 50.0 * 6.456155300140381
Epoch 200, val loss: 1.7494261264801025
Epoch 210, training loss: 323.8531799316406 = 1.7291651964187622 + 50.0 * 6.442480564117432
Epoch 210, val loss: 1.7394260168075562
Epoch 220, training loss: 323.4876403808594 = 1.7165451049804688 + 50.0 * 6.435421943664551
Epoch 220, val loss: 1.7285269498825073
Epoch 230, training loss: 322.7115478515625 = 1.7026866674423218 + 50.0 * 6.420176982879639
Epoch 230, val loss: 1.7166156768798828
Epoch 240, training loss: 322.2115783691406 = 1.687694787979126 + 50.0 * 6.410477638244629
Epoch 240, val loss: 1.7037887573242188
Epoch 250, training loss: 322.240478515625 = 1.6713565587997437 + 50.0 * 6.41138219833374
Epoch 250, val loss: 1.6898609399795532
Epoch 260, training loss: 321.42095947265625 = 1.6535676717758179 + 50.0 * 6.395348072052002
Epoch 260, val loss: 1.6749404668807983
Epoch 270, training loss: 320.9947814941406 = 1.6346021890640259 + 50.0 * 6.387203216552734
Epoch 270, val loss: 1.6589628458023071
Epoch 280, training loss: 320.6414489746094 = 1.6144522428512573 + 50.0 * 6.380540370941162
Epoch 280, val loss: 1.6421087980270386
Epoch 290, training loss: 320.5445251464844 = 1.5931403636932373 + 50.0 * 6.379027843475342
Epoch 290, val loss: 1.6243441104888916
Epoch 300, training loss: 320.3027038574219 = 1.5705780982971191 + 50.0 * 6.374642372131348
Epoch 300, val loss: 1.6057904958724976
Epoch 310, training loss: 319.80938720703125 = 1.547440528869629 + 50.0 * 6.365238666534424
Epoch 310, val loss: 1.5869204998016357
Epoch 320, training loss: 319.5051574707031 = 1.5236971378326416 + 50.0 * 6.359628677368164
Epoch 320, val loss: 1.5676190853118896
Epoch 330, training loss: 319.26025390625 = 1.4995126724243164 + 50.0 * 6.355214595794678
Epoch 330, val loss: 1.5482317209243774
Epoch 340, training loss: 319.0211486816406 = 1.4749616384506226 + 50.0 * 6.350923538208008
Epoch 340, val loss: 1.5288618803024292
Epoch 350, training loss: 319.0933837890625 = 1.4502192735671997 + 50.0 * 6.352863311767578
Epoch 350, val loss: 1.5095701217651367
Epoch 360, training loss: 318.6983337402344 = 1.4253774881362915 + 50.0 * 6.345458984375
Epoch 360, val loss: 1.490471363067627
Epoch 370, training loss: 318.4230041503906 = 1.4006743431091309 + 50.0 * 6.340446472167969
Epoch 370, val loss: 1.4718458652496338
Epoch 380, training loss: 318.7532653808594 = 1.3761519193649292 + 50.0 * 6.3475422859191895
Epoch 380, val loss: 1.4537328481674194
Epoch 390, training loss: 318.18310546875 = 1.3516350984573364 + 50.0 * 6.336629867553711
Epoch 390, val loss: 1.4356187582015991
Epoch 400, training loss: 317.8741149902344 = 1.3275450468063354 + 50.0 * 6.330931186676025
Epoch 400, val loss: 1.4182255268096924
Epoch 410, training loss: 317.6578063964844 = 1.3037350177764893 + 50.0 * 6.327081203460693
Epoch 410, val loss: 1.4013779163360596
Epoch 420, training loss: 317.82440185546875 = 1.2801884412765503 + 50.0 * 6.3308844566345215
Epoch 420, val loss: 1.3850898742675781
Epoch 430, training loss: 317.3789367675781 = 1.256490707397461 + 50.0 * 6.32244873046875
Epoch 430, val loss: 1.3683773279190063
Epoch 440, training loss: 317.2119140625 = 1.233209252357483 + 50.0 * 6.319573879241943
Epoch 440, val loss: 1.3524364233016968
Epoch 450, training loss: 316.9911193847656 = 1.210216760635376 + 50.0 * 6.315618515014648
Epoch 450, val loss: 1.3368595838546753
Epoch 460, training loss: 316.961669921875 = 1.1874890327453613 + 50.0 * 6.315483570098877
Epoch 460, val loss: 1.321717381477356
Epoch 470, training loss: 317.1318054199219 = 1.164651870727539 + 50.0 * 6.319343090057373
Epoch 470, val loss: 1.3065136671066284
Epoch 480, training loss: 316.6417541503906 = 1.1421840190887451 + 50.0 * 6.309991359710693
Epoch 480, val loss: 1.2914396524429321
Epoch 490, training loss: 316.4332580566406 = 1.1201531887054443 + 50.0 * 6.306262493133545
Epoch 490, val loss: 1.2770417928695679
Epoch 500, training loss: 316.2878112792969 = 1.0984398126602173 + 50.0 * 6.3037872314453125
Epoch 500, val loss: 1.2630064487457275
Epoch 510, training loss: 316.56842041015625 = 1.0769282579421997 + 50.0 * 6.3098297119140625
Epoch 510, val loss: 1.2489107847213745
Epoch 520, training loss: 316.1986083984375 = 1.0556182861328125 + 50.0 * 6.302859783172607
Epoch 520, val loss: 1.2357202768325806
Epoch 530, training loss: 315.9886779785156 = 1.0346081256866455 + 50.0 * 6.299081325531006
Epoch 530, val loss: 1.2225710153579712
Epoch 540, training loss: 315.83819580078125 = 1.0140682458877563 + 50.0 * 6.296482563018799
Epoch 540, val loss: 1.2099469900131226
Epoch 550, training loss: 315.9947509765625 = 0.993859052658081 + 50.0 * 6.300017833709717
Epoch 550, val loss: 1.1978079080581665
Epoch 560, training loss: 315.7010498046875 = 0.9738889336585999 + 50.0 * 6.294543266296387
Epoch 560, val loss: 1.1854760646820068
Epoch 570, training loss: 315.4792785644531 = 0.9542554616928101 + 50.0 * 6.290500640869141
Epoch 570, val loss: 1.1739400625228882
Epoch 580, training loss: 315.4451599121094 = 0.9350460171699524 + 50.0 * 6.2902021408081055
Epoch 580, val loss: 1.1625829935073853
Epoch 590, training loss: 315.39349365234375 = 0.9160098433494568 + 50.0 * 6.289549827575684
Epoch 590, val loss: 1.1518867015838623
Epoch 600, training loss: 315.1872253417969 = 0.8973991274833679 + 50.0 * 6.285796642303467
Epoch 600, val loss: 1.141304850578308
Epoch 610, training loss: 315.1084289550781 = 0.8792279958724976 + 50.0 * 6.284583568572998
Epoch 610, val loss: 1.131164789199829
Epoch 620, training loss: 315.0106201171875 = 0.8614861965179443 + 50.0 * 6.28298282623291
Epoch 620, val loss: 1.121593952178955
Epoch 630, training loss: 314.9562683105469 = 0.8439275622367859 + 50.0 * 6.2822465896606445
Epoch 630, val loss: 1.1122157573699951
Epoch 640, training loss: 314.9269104003906 = 0.8267555832862854 + 50.0 * 6.282002925872803
Epoch 640, val loss: 1.1033987998962402
Epoch 650, training loss: 314.76666259765625 = 0.8100235462188721 + 50.0 * 6.279132843017578
Epoch 650, val loss: 1.0949170589447021
Epoch 660, training loss: 314.9652404785156 = 0.7936693429946899 + 50.0 * 6.283431529998779
Epoch 660, val loss: 1.0864949226379395
Epoch 670, training loss: 314.7305908203125 = 0.7774463295936584 + 50.0 * 6.2790632247924805
Epoch 670, val loss: 1.0784671306610107
Epoch 680, training loss: 314.5458984375 = 0.7616645693778992 + 50.0 * 6.275684833526611
Epoch 680, val loss: 1.0708427429199219
Epoch 690, training loss: 314.431640625 = 0.7463573813438416 + 50.0 * 6.27370548248291
Epoch 690, val loss: 1.0638549327850342
Epoch 700, training loss: 314.34356689453125 = 0.7314163446426392 + 50.0 * 6.272243022918701
Epoch 700, val loss: 1.0570827722549438
Epoch 710, training loss: 314.5165710449219 = 0.7167754173278809 + 50.0 * 6.275996208190918
Epoch 710, val loss: 1.050597906112671
Epoch 720, training loss: 314.4595947265625 = 0.7021032571792603 + 50.0 * 6.275149822235107
Epoch 720, val loss: 1.0440460443496704
Epoch 730, training loss: 314.1618347167969 = 0.6878588199615479 + 50.0 * 6.269479274749756
Epoch 730, val loss: 1.0380233526229858
Epoch 740, training loss: 314.10662841796875 = 0.6739470362663269 + 50.0 * 6.268653392791748
Epoch 740, val loss: 1.032306432723999
Epoch 750, training loss: 314.20257568359375 = 0.6602421998977661 + 50.0 * 6.270846843719482
Epoch 750, val loss: 1.0265737771987915
Epoch 760, training loss: 313.93304443359375 = 0.6467694044113159 + 50.0 * 6.265725612640381
Epoch 760, val loss: 1.0220527648925781
Epoch 770, training loss: 313.849853515625 = 0.6334965825080872 + 50.0 * 6.264327526092529
Epoch 770, val loss: 1.0168297290802002
Epoch 780, training loss: 313.8178405761719 = 0.6204493045806885 + 50.0 * 6.263947486877441
Epoch 780, val loss: 1.0120635032653809
Epoch 790, training loss: 314.1395568847656 = 0.6076036691665649 + 50.0 * 6.270638942718506
Epoch 790, val loss: 1.0077345371246338
Epoch 800, training loss: 313.8569030761719 = 0.5947815775871277 + 50.0 * 6.265242576599121
Epoch 800, val loss: 1.0033249855041504
Epoch 810, training loss: 313.6900939941406 = 0.5821799635887146 + 50.0 * 6.262158393859863
Epoch 810, val loss: 0.9993658065795898
Epoch 820, training loss: 313.584228515625 = 0.5698298215866089 + 50.0 * 6.260288238525391
Epoch 820, val loss: 0.9956454634666443
Epoch 830, training loss: 313.55499267578125 = 0.5576788187026978 + 50.0 * 6.259946346282959
Epoch 830, val loss: 0.9922237992286682
Epoch 840, training loss: 313.61273193359375 = 0.5455831289291382 + 50.0 * 6.261343002319336
Epoch 840, val loss: 0.9889222383499146
Epoch 850, training loss: 313.6157531738281 = 0.533549427986145 + 50.0 * 6.26164436340332
Epoch 850, val loss: 0.9853403568267822
Epoch 860, training loss: 313.41473388671875 = 0.5217132568359375 + 50.0 * 6.25786018371582
Epoch 860, val loss: 0.9830389022827148
Epoch 870, training loss: 313.30450439453125 = 0.5100892186164856 + 50.0 * 6.255887985229492
Epoch 870, val loss: 0.9801921844482422
Epoch 880, training loss: 313.24859619140625 = 0.49868762493133545 + 50.0 * 6.254998207092285
Epoch 880, val loss: 0.9778590202331543
Epoch 890, training loss: 313.30645751953125 = 0.48749208450317383 + 50.0 * 6.256379127502441
Epoch 890, val loss: 0.9758961796760559
Epoch 900, training loss: 313.2447509765625 = 0.47625821828842163 + 50.0 * 6.255370140075684
Epoch 900, val loss: 0.9739040732383728
Epoch 910, training loss: 313.1442565917969 = 0.46528246998786926 + 50.0 * 6.253579616546631
Epoch 910, val loss: 0.9724200367927551
Epoch 920, training loss: 313.1020202636719 = 0.45449721813201904 + 50.0 * 6.252950668334961
Epoch 920, val loss: 0.9710063934326172
Epoch 930, training loss: 313.335205078125 = 0.4439980983734131 + 50.0 * 6.257823944091797
Epoch 930, val loss: 0.9702568054199219
Epoch 940, training loss: 313.1218566894531 = 0.43336454033851624 + 50.0 * 6.253770351409912
Epoch 940, val loss: 0.9682840704917908
Epoch 950, training loss: 312.94793701171875 = 0.42307987809181213 + 50.0 * 6.250497341156006
Epoch 950, val loss: 0.9679440259933472
Epoch 960, training loss: 312.8849182128906 = 0.4130067527294159 + 50.0 * 6.249438762664795
Epoch 960, val loss: 0.9671903252601624
Epoch 970, training loss: 312.8723449707031 = 0.4031379818916321 + 50.0 * 6.249383926391602
Epoch 970, val loss: 0.9669216871261597
Epoch 980, training loss: 312.94110107421875 = 0.3933941721916199 + 50.0 * 6.2509541511535645
Epoch 980, val loss: 0.966596245765686
Epoch 990, training loss: 312.8890075683594 = 0.38382580876350403 + 50.0 * 6.250103950500488
Epoch 990, val loss: 0.9668819904327393
Epoch 1000, training loss: 312.7842712402344 = 0.3744555115699768 + 50.0 * 6.248196125030518
Epoch 1000, val loss: 0.9675440788269043
Epoch 1010, training loss: 312.6860656738281 = 0.3652653396129608 + 50.0 * 6.246416091918945
Epoch 1010, val loss: 0.9677684903144836
Epoch 1020, training loss: 312.6174011230469 = 0.35637789964675903 + 50.0 * 6.245220184326172
Epoch 1020, val loss: 0.9686797261238098
Epoch 1030, training loss: 312.9341735839844 = 0.34762394428253174 + 50.0 * 6.251730918884277
Epoch 1030, val loss: 0.9695974588394165
Epoch 1040, training loss: 312.837158203125 = 0.3389870226383209 + 50.0 * 6.249963283538818
Epoch 1040, val loss: 0.9699080586433411
Epoch 1050, training loss: 312.6626281738281 = 0.33054062724113464 + 50.0 * 6.246641635894775
Epoch 1050, val loss: 0.9715948104858398
Epoch 1060, training loss: 312.49237060546875 = 0.32237327098846436 + 50.0 * 6.243399620056152
Epoch 1060, val loss: 0.9730280041694641
Epoch 1070, training loss: 312.4283142089844 = 0.31443163752555847 + 50.0 * 6.242278099060059
Epoch 1070, val loss: 0.9744781255722046
Epoch 1080, training loss: 312.4840393066406 = 0.30668875575065613 + 50.0 * 6.243546962738037
Epoch 1080, val loss: 0.9762048721313477
Epoch 1090, training loss: 312.4526062011719 = 0.2990904152393341 + 50.0 * 6.243070125579834
Epoch 1090, val loss: 0.9782524704933167
Epoch 1100, training loss: 312.6105041503906 = 0.29164668917655945 + 50.0 * 6.246376991271973
Epoch 1100, val loss: 0.9803112745285034
Epoch 1110, training loss: 312.3565673828125 = 0.2842661440372467 + 50.0 * 6.241446018218994
Epoch 1110, val loss: 0.9817022085189819
Epoch 1120, training loss: 312.2643737792969 = 0.27726423740386963 + 50.0 * 6.239742279052734
Epoch 1120, val loss: 0.9842853546142578
Epoch 1130, training loss: 312.2250061035156 = 0.27041223645210266 + 50.0 * 6.239091873168945
Epoch 1130, val loss: 0.9867846369743347
Epoch 1140, training loss: 312.4192199707031 = 0.2637593150138855 + 50.0 * 6.243109226226807
Epoch 1140, val loss: 0.9894216060638428
Epoch 1150, training loss: 312.3924255371094 = 0.2570580840110779 + 50.0 * 6.242707252502441
Epoch 1150, val loss: 0.9914583563804626
Epoch 1160, training loss: 312.16998291015625 = 0.2506389915943146 + 50.0 * 6.238386631011963
Epoch 1160, val loss: 0.9944223761558533
Epoch 1170, training loss: 312.1087646484375 = 0.24443335831165314 + 50.0 * 6.237286567687988
Epoch 1170, val loss: 0.9976838231086731
Epoch 1180, training loss: 312.13543701171875 = 0.23842233419418335 + 50.0 * 6.237939834594727
Epoch 1180, val loss: 1.0006847381591797
Epoch 1190, training loss: 312.30938720703125 = 0.23254354298114777 + 50.0 * 6.241536617279053
Epoch 1190, val loss: 1.0040180683135986
Epoch 1200, training loss: 312.1531677246094 = 0.22672061622142792 + 50.0 * 6.238529205322266
Epoch 1200, val loss: 1.0068089962005615
Epoch 1210, training loss: 312.3126525878906 = 0.22116878628730774 + 50.0 * 6.241829872131348
Epoch 1210, val loss: 1.010316014289856
Epoch 1220, training loss: 311.9897155761719 = 0.21559731662273407 + 50.0 * 6.235482215881348
Epoch 1220, val loss: 1.0134168863296509
Epoch 1230, training loss: 311.91448974609375 = 0.21028876304626465 + 50.0 * 6.234084129333496
Epoch 1230, val loss: 1.0170886516571045
Epoch 1240, training loss: 311.908447265625 = 0.20515039563179016 + 50.0 * 6.234066009521484
Epoch 1240, val loss: 1.0209362506866455
Epoch 1250, training loss: 312.0393371582031 = 0.20012861490249634 + 50.0 * 6.236783981323242
Epoch 1250, val loss: 1.0249803066253662
Epoch 1260, training loss: 311.8889465332031 = 0.19517399370670319 + 50.0 * 6.233875751495361
Epoch 1260, val loss: 1.0286381244659424
Epoch 1270, training loss: 311.9184875488281 = 0.19035477936267853 + 50.0 * 6.234562397003174
Epoch 1270, val loss: 1.0327539443969727
Epoch 1280, training loss: 311.94775390625 = 0.18569153547286987 + 50.0 * 6.235240936279297
Epoch 1280, val loss: 1.0367447137832642
Epoch 1290, training loss: 311.78826904296875 = 0.1810951977968216 + 50.0 * 6.232143402099609
Epoch 1290, val loss: 1.040824294090271
Epoch 1300, training loss: 311.7799377441406 = 0.17668113112449646 + 50.0 * 6.232065677642822
Epoch 1300, val loss: 1.045045018196106
Epoch 1310, training loss: 311.9377746582031 = 0.1723763644695282 + 50.0 * 6.2353081703186035
Epoch 1310, val loss: 1.049250602722168
Epoch 1320, training loss: 311.9043884277344 = 0.16817303001880646 + 50.0 * 6.234724044799805
Epoch 1320, val loss: 1.0539053678512573
Epoch 1330, training loss: 311.7511901855469 = 0.16407401859760284 + 50.0 * 6.2317423820495605
Epoch 1330, val loss: 1.058678388595581
Epoch 1340, training loss: 311.8037109375 = 0.16009297966957092 + 50.0 * 6.232872009277344
Epoch 1340, val loss: 1.063248634338379
Epoch 1350, training loss: 311.7057800292969 = 0.15621089935302734 + 50.0 * 6.230991363525391
Epoch 1350, val loss: 1.0676875114440918
Epoch 1360, training loss: 311.6670837402344 = 0.15242797136306763 + 50.0 * 6.230292797088623
Epoch 1360, val loss: 1.0723072290420532
Epoch 1370, training loss: 311.6064453125 = 0.1487874686717987 + 50.0 * 6.229153156280518
Epoch 1370, val loss: 1.0772387981414795
Epoch 1380, training loss: 311.60260009765625 = 0.14524538815021515 + 50.0 * 6.229147434234619
Epoch 1380, val loss: 1.0821667909622192
Epoch 1390, training loss: 311.90606689453125 = 0.1418004333972931 + 50.0 * 6.23528528213501
Epoch 1390, val loss: 1.0874732732772827
Epoch 1400, training loss: 311.95013427734375 = 0.13842228055000305 + 50.0 * 6.236234188079834
Epoch 1400, val loss: 1.092116355895996
Epoch 1410, training loss: 311.5835266113281 = 0.13500317931175232 + 50.0 * 6.228970050811768
Epoch 1410, val loss: 1.0967280864715576
Epoch 1420, training loss: 311.5039367675781 = 0.13182203471660614 + 50.0 * 6.227442264556885
Epoch 1420, val loss: 1.1018720865249634
Epoch 1430, training loss: 311.5235290527344 = 0.1287456452846527 + 50.0 * 6.227895259857178
Epoch 1430, val loss: 1.1071393489837646
Epoch 1440, training loss: 311.7502136230469 = 0.12571491301059723 + 50.0 * 6.232490062713623
Epoch 1440, val loss: 1.1120188236236572
Epoch 1450, training loss: 311.52874755859375 = 0.12274817377328873 + 50.0 * 6.22812032699585
Epoch 1450, val loss: 1.1176217794418335
Epoch 1460, training loss: 311.50018310546875 = 0.11988857388496399 + 50.0 * 6.22760534286499
Epoch 1460, val loss: 1.1228580474853516
Epoch 1470, training loss: 311.60797119140625 = 0.11710035055875778 + 50.0 * 6.2298173904418945
Epoch 1470, val loss: 1.1281667947769165
Epoch 1480, training loss: 311.4561767578125 = 0.11438459903001785 + 50.0 * 6.22683572769165
Epoch 1480, val loss: 1.133562684059143
Epoch 1490, training loss: 311.4035949707031 = 0.11174163222312927 + 50.0 * 6.225836753845215
Epoch 1490, val loss: 1.1388323307037354
Epoch 1500, training loss: 311.79766845703125 = 0.10916576534509659 + 50.0 * 6.233770370483398
Epoch 1500, val loss: 1.1439931392669678
Epoch 1510, training loss: 311.46429443359375 = 0.10667742043733597 + 50.0 * 6.227152347564697
Epoch 1510, val loss: 1.1500768661499023
Epoch 1520, training loss: 311.33453369140625 = 0.10421706736087799 + 50.0 * 6.224606513977051
Epoch 1520, val loss: 1.1550136804580688
Epoch 1530, training loss: 311.2926940917969 = 0.10189206153154373 + 50.0 * 6.22381591796875
Epoch 1530, val loss: 1.1608535051345825
Epoch 1540, training loss: 311.88629150390625 = 0.09962321072816849 + 50.0 * 6.235733509063721
Epoch 1540, val loss: 1.165993571281433
Epoch 1550, training loss: 311.4367980957031 = 0.09733780473470688 + 50.0 * 6.2267889976501465
Epoch 1550, val loss: 1.1718817949295044
Epoch 1560, training loss: 311.2470703125 = 0.09515097737312317 + 50.0 * 6.223038196563721
Epoch 1560, val loss: 1.1772905588150024
Epoch 1570, training loss: 311.2139587402344 = 0.0930509865283966 + 50.0 * 6.222418308258057
Epoch 1570, val loss: 1.1830106973648071
Epoch 1580, training loss: 311.2476501464844 = 0.09101869910955429 + 50.0 * 6.223132133483887
Epoch 1580, val loss: 1.1884835958480835
Epoch 1590, training loss: 311.4964294433594 = 0.08901668339967728 + 50.0 * 6.228148460388184
Epoch 1590, val loss: 1.1942181587219238
Epoch 1600, training loss: 311.2822570800781 = 0.08706706017255783 + 50.0 * 6.223903656005859
Epoch 1600, val loss: 1.1999869346618652
Epoch 1610, training loss: 311.1966552734375 = 0.08515657484531403 + 50.0 * 6.222229957580566
Epoch 1610, val loss: 1.205445408821106
Epoch 1620, training loss: 311.5135192871094 = 0.08333636075258255 + 50.0 * 6.228603363037109
Epoch 1620, val loss: 1.2111949920654297
Epoch 1630, training loss: 311.2930908203125 = 0.08153916895389557 + 50.0 * 6.224231243133545
Epoch 1630, val loss: 1.217172384262085
Epoch 1640, training loss: 311.240234375 = 0.0797656998038292 + 50.0 * 6.223209381103516
Epoch 1640, val loss: 1.2225675582885742
Epoch 1650, training loss: 311.1580505371094 = 0.07806842774152756 + 50.0 * 6.221599578857422
Epoch 1650, val loss: 1.2284204959869385
Epoch 1660, training loss: 311.11993408203125 = 0.07642508298158646 + 50.0 * 6.220870494842529
Epoch 1660, val loss: 1.233978271484375
Epoch 1670, training loss: 311.2316589355469 = 0.07484400272369385 + 50.0 * 6.2231364250183105
Epoch 1670, val loss: 1.239905834197998
Epoch 1680, training loss: 311.23175048828125 = 0.07327363640069962 + 50.0 * 6.223169803619385
Epoch 1680, val loss: 1.2454233169555664
Epoch 1690, training loss: 311.1466979980469 = 0.07173022627830505 + 50.0 * 6.221499443054199
Epoch 1690, val loss: 1.251114010810852
Epoch 1700, training loss: 311.0697021484375 = 0.07022558152675629 + 50.0 * 6.219989776611328
Epoch 1700, val loss: 1.2566466331481934
Epoch 1710, training loss: 311.0244445800781 = 0.06878834217786789 + 50.0 * 6.219112873077393
Epoch 1710, val loss: 1.2622361183166504
Epoch 1720, training loss: 311.029541015625 = 0.06739548593759537 + 50.0 * 6.219243049621582
Epoch 1720, val loss: 1.2680227756500244
Epoch 1730, training loss: 311.4461364746094 = 0.0660669207572937 + 50.0 * 6.227601528167725
Epoch 1730, val loss: 1.2739089727401733
Epoch 1740, training loss: 311.0960693359375 = 0.06465210020542145 + 50.0 * 6.220627784729004
Epoch 1740, val loss: 1.278428554534912
Epoch 1750, training loss: 311.0316162109375 = 0.06335509568452835 + 50.0 * 6.219365119934082
Epoch 1750, val loss: 1.2845748662948608
Epoch 1760, training loss: 311.1439514160156 = 0.06209344044327736 + 50.0 * 6.221636772155762
Epoch 1760, val loss: 1.2897942066192627
Epoch 1770, training loss: 310.97674560546875 = 0.06083809584379196 + 50.0 * 6.218317985534668
Epoch 1770, val loss: 1.2956552505493164
Epoch 1780, training loss: 311.044921875 = 0.0596320815384388 + 50.0 * 6.219706058502197
Epoch 1780, val loss: 1.3010215759277344
Epoch 1790, training loss: 311.0506286621094 = 0.05844173952937126 + 50.0 * 6.219843864440918
Epoch 1790, val loss: 1.30656898021698
Epoch 1800, training loss: 310.9601745605469 = 0.057291507720947266 + 50.0 * 6.218057632446289
Epoch 1800, val loss: 1.3116490840911865
Epoch 1810, training loss: 311.0477294921875 = 0.056201666593551636 + 50.0 * 6.219830513000488
Epoch 1810, val loss: 1.3176127672195435
Epoch 1820, training loss: 310.8868713378906 = 0.055082887411117554 + 50.0 * 6.216635704040527
Epoch 1820, val loss: 1.3228957653045654
Epoch 1830, training loss: 310.9195251464844 = 0.054032374173402786 + 50.0 * 6.217309474945068
Epoch 1830, val loss: 1.3282965421676636
Epoch 1840, training loss: 311.0525817871094 = 0.053015947341918945 + 50.0 * 6.219991683959961
Epoch 1840, val loss: 1.3341014385223389
Epoch 1850, training loss: 311.0084228515625 = 0.051991384476423264 + 50.0 * 6.219128608703613
Epoch 1850, val loss: 1.3390028476715088
Epoch 1860, training loss: 310.86944580078125 = 0.0509876124560833 + 50.0 * 6.216369152069092
Epoch 1860, val loss: 1.3445031642913818
Epoch 1870, training loss: 310.8322448730469 = 0.05000511184334755 + 50.0 * 6.215644359588623
Epoch 1870, val loss: 1.3495757579803467
Epoch 1880, training loss: 310.81683349609375 = 0.04907830432057381 + 50.0 * 6.215354919433594
Epoch 1880, val loss: 1.355059027671814
Epoch 1890, training loss: 311.0509948730469 = 0.048178452998399734 + 50.0 * 6.220056056976318
Epoch 1890, val loss: 1.360621690750122
Epoch 1900, training loss: 310.9233093261719 = 0.04727572202682495 + 50.0 * 6.217520713806152
Epoch 1900, val loss: 1.3656384944915771
Epoch 1910, training loss: 310.804443359375 = 0.046373672783374786 + 50.0 * 6.215160846710205
Epoch 1910, val loss: 1.3710541725158691
Epoch 1920, training loss: 310.77728271484375 = 0.045514628291130066 + 50.0 * 6.214635848999023
Epoch 1920, val loss: 1.375845193862915
Epoch 1930, training loss: 310.9749755859375 = 0.04469490796327591 + 50.0 * 6.2186055183410645
Epoch 1930, val loss: 1.3810454607009888
Epoch 1940, training loss: 310.7430114746094 = 0.0438695065677166 + 50.0 * 6.213982582092285
Epoch 1940, val loss: 1.3864315748214722
Epoch 1950, training loss: 310.7623291015625 = 0.04308878630399704 + 50.0 * 6.214385032653809
Epoch 1950, val loss: 1.391785740852356
Epoch 1960, training loss: 310.7504577636719 = 0.042314812541007996 + 50.0 * 6.214162826538086
Epoch 1960, val loss: 1.3967418670654297
Epoch 1970, training loss: 310.9028625488281 = 0.04159299284219742 + 50.0 * 6.217225074768066
Epoch 1970, val loss: 1.4021750688552856
Epoch 1980, training loss: 310.6885986328125 = 0.040822915732860565 + 50.0 * 6.212955474853516
Epoch 1980, val loss: 1.4068979024887085
Epoch 1990, training loss: 310.7742004394531 = 0.040100887417793274 + 50.0 * 6.214682102203369
Epoch 1990, val loss: 1.4122109413146973
Epoch 2000, training loss: 310.7851257324219 = 0.039405275136232376 + 50.0 * 6.214914798736572
Epoch 2000, val loss: 1.4171369075775146
Epoch 2010, training loss: 310.7990417480469 = 0.03870435804128647 + 50.0 * 6.215206623077393
Epoch 2010, val loss: 1.4216712713241577
Epoch 2020, training loss: 310.6847839355469 = 0.03802867606282234 + 50.0 * 6.212935447692871
Epoch 2020, val loss: 1.4269838333129883
Epoch 2030, training loss: 310.6654052734375 = 0.03737317770719528 + 50.0 * 6.212560176849365
Epoch 2030, val loss: 1.431808590888977
Epoch 2040, training loss: 310.6593322753906 = 0.03674236685037613 + 50.0 * 6.212451934814453
Epoch 2040, val loss: 1.4369648694992065
Epoch 2050, training loss: 311.04779052734375 = 0.036140866577625275 + 50.0 * 6.220232963562012
Epoch 2050, val loss: 1.442008376121521
Epoch 2060, training loss: 310.78900146484375 = 0.03549296036362648 + 50.0 * 6.2150702476501465
Epoch 2060, val loss: 1.4465142488479614
Epoch 2070, training loss: 310.6687927246094 = 0.034889403730630875 + 50.0 * 6.2126784324646
Epoch 2070, val loss: 1.4512239694595337
Epoch 2080, training loss: 310.6584167480469 = 0.034308917820453644 + 50.0 * 6.212482452392578
Epoch 2080, val loss: 1.4561930894851685
Epoch 2090, training loss: 310.7185363769531 = 0.03373837471008301 + 50.0 * 6.213695526123047
Epoch 2090, val loss: 1.4605355262756348
Epoch 2100, training loss: 310.6307373046875 = 0.03317229077219963 + 50.0 * 6.21195125579834
Epoch 2100, val loss: 1.4656480550765991
Epoch 2110, training loss: 310.7003173828125 = 0.0326245091855526 + 50.0 * 6.213354110717773
Epoch 2110, val loss: 1.4702199697494507
Epoch 2120, training loss: 310.5982666015625 = 0.03209557384252548 + 50.0 * 6.211323261260986
Epoch 2120, val loss: 1.4751428365707397
Epoch 2130, training loss: 310.78582763671875 = 0.031572841107845306 + 50.0 * 6.215084552764893
Epoch 2130, val loss: 1.4795938730239868
Epoch 2140, training loss: 310.55242919921875 = 0.031055094674229622 + 50.0 * 6.210427284240723
Epoch 2140, val loss: 1.484256625175476
Epoch 2150, training loss: 310.5266418457031 = 0.03054579347372055 + 50.0 * 6.209921836853027
Epoch 2150, val loss: 1.488825798034668
Epoch 2160, training loss: 310.6795349121094 = 0.030073584988713264 + 50.0 * 6.212989807128906
Epoch 2160, val loss: 1.4934420585632324
Epoch 2170, training loss: 310.4974670410156 = 0.02957810088992119 + 50.0 * 6.209357738494873
Epoch 2170, val loss: 1.4982731342315674
Epoch 2180, training loss: 310.4960632324219 = 0.02911357767879963 + 50.0 * 6.209339141845703
Epoch 2180, val loss: 1.50286066532135
Epoch 2190, training loss: 310.4786682128906 = 0.028662720695137978 + 50.0 * 6.209000110626221
Epoch 2190, val loss: 1.5072555541992188
Epoch 2200, training loss: 310.87286376953125 = 0.028221067041158676 + 50.0 * 6.216892719268799
Epoch 2200, val loss: 1.5111836194992065
Epoch 2210, training loss: 310.594970703125 = 0.027777811512351036 + 50.0 * 6.211344242095947
Epoch 2210, val loss: 1.5165482759475708
Epoch 2220, training loss: 310.5093078613281 = 0.027327286079525948 + 50.0 * 6.209640026092529
Epoch 2220, val loss: 1.5205038785934448
Epoch 2230, training loss: 310.4571533203125 = 0.02691730298101902 + 50.0 * 6.20860481262207
Epoch 2230, val loss: 1.5253078937530518
Epoch 2240, training loss: 310.4390869140625 = 0.026510464027523994 + 50.0 * 6.208251476287842
Epoch 2240, val loss: 1.5296332836151123
Epoch 2250, training loss: 310.6383361816406 = 0.026124969124794006 + 50.0 * 6.212244033813477
Epoch 2250, val loss: 1.534178376197815
Epoch 2260, training loss: 310.4706115722656 = 0.025721821933984756 + 50.0 * 6.208897590637207
Epoch 2260, val loss: 1.5381243228912354
Epoch 2270, training loss: 310.5861511230469 = 0.025345368310809135 + 50.0 * 6.211215972900391
Epoch 2270, val loss: 1.5429702997207642
Epoch 2280, training loss: 310.42327880859375 = 0.02494625188410282 + 50.0 * 6.2079668045043945
Epoch 2280, val loss: 1.5467498302459717
Epoch 2290, training loss: 310.5182800292969 = 0.024580884724855423 + 50.0 * 6.209874153137207
Epoch 2290, val loss: 1.5512592792510986
Epoch 2300, training loss: 310.4979553222656 = 0.024219466373324394 + 50.0 * 6.209474563598633
Epoch 2300, val loss: 1.5552233457565308
Epoch 2310, training loss: 310.5141296386719 = 0.023856379091739655 + 50.0 * 6.209805011749268
Epoch 2310, val loss: 1.5596497058868408
Epoch 2320, training loss: 310.3891296386719 = 0.023502929136157036 + 50.0 * 6.20731258392334
Epoch 2320, val loss: 1.5638856887817383
Epoch 2330, training loss: 310.3524475097656 = 0.023171700537204742 + 50.0 * 6.20658540725708
Epoch 2330, val loss: 1.568310260772705
Epoch 2340, training loss: 310.338134765625 = 0.022848714143037796 + 50.0 * 6.206305503845215
Epoch 2340, val loss: 1.5725172758102417
Epoch 2350, training loss: 310.6375732421875 = 0.022537874057888985 + 50.0 * 6.2123003005981445
Epoch 2350, val loss: 1.5768516063690186
Epoch 2360, training loss: 310.38262939453125 = 0.02219451777637005 + 50.0 * 6.207208633422852
Epoch 2360, val loss: 1.5804343223571777
Epoch 2370, training loss: 310.4052429199219 = 0.021880947053432465 + 50.0 * 6.207667350769043
Epoch 2370, val loss: 1.584499716758728
Epoch 2380, training loss: 310.3830871582031 = 0.021572671830654144 + 50.0 * 6.207230091094971
Epoch 2380, val loss: 1.5885016918182373
Epoch 2390, training loss: 310.507568359375 = 0.02126910351216793 + 50.0 * 6.209725856781006
Epoch 2390, val loss: 1.5922514200210571
Epoch 2400, training loss: 310.3495178222656 = 0.02097197249531746 + 50.0 * 6.206571102142334
Epoch 2400, val loss: 1.5961757898330688
Epoch 2410, training loss: 310.26654052734375 = 0.0206807442009449 + 50.0 * 6.204916954040527
Epoch 2410, val loss: 1.6005223989486694
Epoch 2420, training loss: 310.3083801269531 = 0.020405283197760582 + 50.0 * 6.205759048461914
Epoch 2420, val loss: 1.6047348976135254
Epoch 2430, training loss: 310.70843505859375 = 0.02012796513736248 + 50.0 * 6.213766098022461
Epoch 2430, val loss: 1.6081525087356567
Epoch 2440, training loss: 310.4117431640625 = 0.019847912713885307 + 50.0 * 6.20783805847168
Epoch 2440, val loss: 1.6116080284118652
Epoch 2450, training loss: 310.2982482910156 = 0.01957380585372448 + 50.0 * 6.205574035644531
Epoch 2450, val loss: 1.616271734237671
Epoch 2460, training loss: 310.2460021972656 = 0.019316429272294044 + 50.0 * 6.204533576965332
Epoch 2460, val loss: 1.6196492910385132
Epoch 2470, training loss: 310.36517333984375 = 0.01906765252351761 + 50.0 * 6.2069220542907715
Epoch 2470, val loss: 1.623735785484314
Epoch 2480, training loss: 310.334716796875 = 0.018807271495461464 + 50.0 * 6.206318378448486
Epoch 2480, val loss: 1.6269117593765259
Epoch 2490, training loss: 310.4078674316406 = 0.01855851709842682 + 50.0 * 6.2077860832214355
Epoch 2490, val loss: 1.63078773021698
Epoch 2500, training loss: 310.2337646484375 = 0.018315579742193222 + 50.0 * 6.204308986663818
Epoch 2500, val loss: 1.634656310081482
Epoch 2510, training loss: 310.2541809082031 = 0.01808592490851879 + 50.0 * 6.204721927642822
Epoch 2510, val loss: 1.638409972190857
Epoch 2520, training loss: 310.44281005859375 = 0.017864083871245384 + 50.0 * 6.208499431610107
Epoch 2520, val loss: 1.6422686576843262
Epoch 2530, training loss: 310.2894287109375 = 0.01761680841445923 + 50.0 * 6.2054362297058105
Epoch 2530, val loss: 1.6453933715820312
Epoch 2540, training loss: 310.2337951660156 = 0.01739172823727131 + 50.0 * 6.2043280601501465
Epoch 2540, val loss: 1.649253010749817
Epoch 2550, training loss: 310.16094970703125 = 0.017174972221255302 + 50.0 * 6.202875137329102
Epoch 2550, val loss: 1.6526944637298584
Epoch 2560, training loss: 310.1921691894531 = 0.016960810869932175 + 50.0 * 6.2035040855407715
Epoch 2560, val loss: 1.656117558479309
Epoch 2570, training loss: 310.818603515625 = 0.016750384122133255 + 50.0 * 6.216036796569824
Epoch 2570, val loss: 1.6594011783599854
Epoch 2580, training loss: 310.3973083496094 = 0.01652645878493786 + 50.0 * 6.207615375518799
Epoch 2580, val loss: 1.6633121967315674
Epoch 2590, training loss: 310.1888732910156 = 0.01631593145430088 + 50.0 * 6.203450679779053
Epoch 2590, val loss: 1.6664646863937378
Epoch 2600, training loss: 310.1239013671875 = 0.016115430742502213 + 50.0 * 6.202156066894531
Epoch 2600, val loss: 1.6702556610107422
Epoch 2610, training loss: 310.116455078125 = 0.015928741544485092 + 50.0 * 6.202010631561279
Epoch 2610, val loss: 1.673582911491394
Epoch 2620, training loss: 310.39678955078125 = 0.015749333426356316 + 50.0 * 6.207621097564697
Epoch 2620, val loss: 1.6773539781570435
Epoch 2630, training loss: 310.1393737792969 = 0.015543402172625065 + 50.0 * 6.202476501464844
Epoch 2630, val loss: 1.680227518081665
Epoch 2640, training loss: 310.18768310546875 = 0.015352439135313034 + 50.0 * 6.203446388244629
Epoch 2640, val loss: 1.6841740608215332
Epoch 2650, training loss: 310.1877136230469 = 0.015166481956839561 + 50.0 * 6.203450679779053
Epoch 2650, val loss: 1.686900019645691
Epoch 2660, training loss: 310.2327575683594 = 0.014982713386416435 + 50.0 * 6.204355239868164
Epoch 2660, val loss: 1.6902271509170532
Epoch 2670, training loss: 310.1719665527344 = 0.014802458696067333 + 50.0 * 6.203143119812012
Epoch 2670, val loss: 1.693799376487732
Epoch 2680, training loss: 310.0911560058594 = 0.014631250873208046 + 50.0 * 6.201530933380127
Epoch 2680, val loss: 1.6971057653427124
Epoch 2690, training loss: 310.2300720214844 = 0.014472398906946182 + 50.0 * 6.204311847686768
Epoch 2690, val loss: 1.7005237340927124
Epoch 2700, training loss: 310.1520690917969 = 0.01429406926035881 + 50.0 * 6.202755451202393
Epoch 2700, val loss: 1.7036371231079102
Epoch 2710, training loss: 310.0862731933594 = 0.01412301603704691 + 50.0 * 6.201442718505859
Epoch 2710, val loss: 1.70651113986969
Epoch 2720, training loss: 310.0745849609375 = 0.013966375030577183 + 50.0 * 6.201211929321289
Epoch 2720, val loss: 1.709692120552063
Epoch 2730, training loss: 310.0606384277344 = 0.013808131217956543 + 50.0 * 6.200936794281006
Epoch 2730, val loss: 1.712847113609314
Epoch 2740, training loss: 310.316650390625 = 0.013657690957188606 + 50.0 * 6.206059455871582
Epoch 2740, val loss: 1.716033697128296
Epoch 2750, training loss: 310.1814880371094 = 0.013501194305717945 + 50.0 * 6.203360080718994
Epoch 2750, val loss: 1.71962308883667
Epoch 2760, training loss: 310.1950378417969 = 0.013342326506972313 + 50.0 * 6.203634262084961
Epoch 2760, val loss: 1.7224582433700562
Epoch 2770, training loss: 310.1282958984375 = 0.013193070888519287 + 50.0 * 6.2023024559021
Epoch 2770, val loss: 1.7253851890563965
Epoch 2780, training loss: 310.0140075683594 = 0.01303856447339058 + 50.0 * 6.200019359588623
Epoch 2780, val loss: 1.7283200025558472
Epoch 2790, training loss: 310.0382995605469 = 0.012898562476038933 + 50.0 * 6.200507640838623
Epoch 2790, val loss: 1.7313297986984253
Epoch 2800, training loss: 310.1700134277344 = 0.012761873193085194 + 50.0 * 6.2031450271606445
Epoch 2800, val loss: 1.7342575788497925
Epoch 2810, training loss: 310.2123718261719 = 0.012623048387467861 + 50.0 * 6.2039947509765625
Epoch 2810, val loss: 1.737715482711792
Epoch 2820, training loss: 310.1778259277344 = 0.012475213967263699 + 50.0 * 6.203307151794434
Epoch 2820, val loss: 1.7398967742919922
Epoch 2830, training loss: 309.9798583984375 = 0.01233723945915699 + 50.0 * 6.199350833892822
Epoch 2830, val loss: 1.743320345878601
Epoch 2840, training loss: 309.9439392089844 = 0.012207301333546638 + 50.0 * 6.198634624481201
Epoch 2840, val loss: 1.7459698915481567
Epoch 2850, training loss: 309.9270935058594 = 0.012082600966095924 + 50.0 * 6.198300361633301
Epoch 2850, val loss: 1.7490158081054688
Epoch 2860, training loss: 310.001953125 = 0.011961236596107483 + 50.0 * 6.19980001449585
Epoch 2860, val loss: 1.7518144845962524
Epoch 2870, training loss: 310.3445739746094 = 0.011832809075713158 + 50.0 * 6.2066545486450195
Epoch 2870, val loss: 1.7540165185928345
Epoch 2880, training loss: 309.9871826171875 = 0.011700206436216831 + 50.0 * 6.199510097503662
Epoch 2880, val loss: 1.7578016519546509
Epoch 2890, training loss: 309.9288635253906 = 0.011578256264328957 + 50.0 * 6.198346138000488
Epoch 2890, val loss: 1.7602559328079224
Epoch 2900, training loss: 309.8934631347656 = 0.011459250934422016 + 50.0 * 6.197640419006348
Epoch 2900, val loss: 1.7631453275680542
Epoch 2910, training loss: 310.0442810058594 = 0.011347591876983643 + 50.0 * 6.200658321380615
Epoch 2910, val loss: 1.766086459159851
Epoch 2920, training loss: 310.0263366699219 = 0.011228151619434357 + 50.0 * 6.2003021240234375
Epoch 2920, val loss: 1.7687100172042847
Epoch 2930, training loss: 309.9228820800781 = 0.011109534651041031 + 50.0 * 6.198235511779785
Epoch 2930, val loss: 1.7712681293487549
Epoch 2940, training loss: 309.89013671875 = 0.010995420627295971 + 50.0 * 6.197582721710205
Epoch 2940, val loss: 1.7740687131881714
Epoch 2950, training loss: 309.94189453125 = 0.010889960452914238 + 50.0 * 6.198619842529297
Epoch 2950, val loss: 1.7767161130905151
Epoch 2960, training loss: 310.27130126953125 = 0.010780188255012035 + 50.0 * 6.2052106857299805
Epoch 2960, val loss: 1.7792162895202637
Epoch 2970, training loss: 310.00384521484375 = 0.010674909688532352 + 50.0 * 6.199863433837891
Epoch 2970, val loss: 1.7823996543884277
Epoch 2980, training loss: 309.9109191894531 = 0.010563919320702553 + 50.0 * 6.198007106781006
Epoch 2980, val loss: 1.784877061843872
Epoch 2990, training loss: 309.91619873046875 = 0.010463306680321693 + 50.0 * 6.198114395141602
Epoch 2990, val loss: 1.787137508392334
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 431.8028869628906 = 1.9610329866409302 + 50.0 * 8.596837043762207
Epoch 0, val loss: 1.960814118385315
Epoch 10, training loss: 431.74822998046875 = 1.9515324831008911 + 50.0 * 8.59593391418457
Epoch 10, val loss: 1.9513877630233765
Epoch 20, training loss: 431.40936279296875 = 1.9393770694732666 + 50.0 * 8.589399337768555
Epoch 20, val loss: 1.93869948387146
Epoch 30, training loss: 429.32586669921875 = 1.9234628677368164 + 50.0 * 8.54804801940918
Epoch 30, val loss: 1.9219740629196167
Epoch 40, training loss: 418.7509460449219 = 1.905735731124878 + 50.0 * 8.336904525756836
Epoch 40, val loss: 1.9042986631393433
Epoch 50, training loss: 384.9832763671875 = 1.8874387741088867 + 50.0 * 7.661916732788086
Epoch 50, val loss: 1.885772705078125
Epoch 60, training loss: 366.600830078125 = 1.8709231615066528 + 50.0 * 7.29459810256958
Epoch 60, val loss: 1.869197964668274
Epoch 70, training loss: 355.042724609375 = 1.856972098350525 + 50.0 * 7.063714981079102
Epoch 70, val loss: 1.8549984693527222
Epoch 80, training loss: 348.308837890625 = 1.8438594341278076 + 50.0 * 6.929299354553223
Epoch 80, val loss: 1.8422622680664062
Epoch 90, training loss: 343.72186279296875 = 1.831639289855957 + 50.0 * 6.837804794311523
Epoch 90, val loss: 1.8307472467422485
Epoch 100, training loss: 340.36859130859375 = 1.8202660083770752 + 50.0 * 6.77096700668335
Epoch 100, val loss: 1.819966435432434
Epoch 110, training loss: 337.9898986816406 = 1.809845209121704 + 50.0 * 6.7236008644104
Epoch 110, val loss: 1.8099102973937988
Epoch 120, training loss: 335.9504089355469 = 1.799925446510315 + 50.0 * 6.683009624481201
Epoch 120, val loss: 1.800301194190979
Epoch 130, training loss: 334.3301086425781 = 1.7903094291687012 + 50.0 * 6.650795936584473
Epoch 130, val loss: 1.7912085056304932
Epoch 140, training loss: 332.8407287597656 = 1.7809089422225952 + 50.0 * 6.621196269989014
Epoch 140, val loss: 1.7822647094726562
Epoch 150, training loss: 331.4554443359375 = 1.7716857194900513 + 50.0 * 6.593674659729004
Epoch 150, val loss: 1.773502230644226
Epoch 160, training loss: 330.27117919921875 = 1.7626214027404785 + 50.0 * 6.570170879364014
Epoch 160, val loss: 1.764889121055603
Epoch 170, training loss: 329.14971923828125 = 1.7532525062561035 + 50.0 * 6.547929286956787
Epoch 170, val loss: 1.7560280561447144
Epoch 180, training loss: 328.1358337402344 = 1.743436574935913 + 50.0 * 6.527847766876221
Epoch 180, val loss: 1.7468643188476562
Epoch 190, training loss: 327.0439758300781 = 1.7329233884811401 + 50.0 * 6.506221294403076
Epoch 190, val loss: 1.7372318506240845
Epoch 200, training loss: 326.0514831542969 = 1.7219069004058838 + 50.0 * 6.486591815948486
Epoch 200, val loss: 1.7271231412887573
Epoch 210, training loss: 325.2652282714844 = 1.7100809812545776 + 50.0 * 6.471102714538574
Epoch 210, val loss: 1.7163641452789307
Epoch 220, training loss: 324.57904052734375 = 1.6968625783920288 + 50.0 * 6.457643508911133
Epoch 220, val loss: 1.7044020891189575
Epoch 230, training loss: 323.87127685546875 = 1.6825720071792603 + 50.0 * 6.4437737464904785
Epoch 230, val loss: 1.6915446519851685
Epoch 240, training loss: 323.2373352050781 = 1.667134165763855 + 50.0 * 6.431404113769531
Epoch 240, val loss: 1.6776983737945557
Epoch 250, training loss: 322.6724853515625 = 1.650565266609192 + 50.0 * 6.420438289642334
Epoch 250, val loss: 1.662945032119751
Epoch 260, training loss: 322.30401611328125 = 1.6328423023223877 + 50.0 * 6.413423538208008
Epoch 260, val loss: 1.6471222639083862
Epoch 270, training loss: 321.8960266113281 = 1.613648772239685 + 50.0 * 6.4056477546691895
Epoch 270, val loss: 1.6300277709960938
Epoch 280, training loss: 321.31866455078125 = 1.5933558940887451 + 50.0 * 6.394506454467773
Epoch 280, val loss: 1.6118979454040527
Epoch 290, training loss: 320.8713073730469 = 1.571881651878357 + 50.0 * 6.385988712310791
Epoch 290, val loss: 1.5928090810775757
Epoch 300, training loss: 320.6613464355469 = 1.5494261980056763 + 50.0 * 6.382238388061523
Epoch 300, val loss: 1.5727946758270264
Epoch 310, training loss: 320.2032165527344 = 1.5255500078201294 + 50.0 * 6.37355375289917
Epoch 310, val loss: 1.5517131090164185
Epoch 320, training loss: 319.9080505371094 = 1.5011025667190552 + 50.0 * 6.368139266967773
Epoch 320, val loss: 1.5300694704055786
Epoch 330, training loss: 319.5965270996094 = 1.4758061170578003 + 50.0 * 6.362414360046387
Epoch 330, val loss: 1.507940649986267
Epoch 340, training loss: 319.32989501953125 = 1.4499305486679077 + 50.0 * 6.357599258422852
Epoch 340, val loss: 1.4853421449661255
Epoch 350, training loss: 319.2416076660156 = 1.4233062267303467 + 50.0 * 6.35636568069458
Epoch 350, val loss: 1.462298035621643
Epoch 360, training loss: 318.89202880859375 = 1.3962795734405518 + 50.0 * 6.349915027618408
Epoch 360, val loss: 1.4389381408691406
Epoch 370, training loss: 318.6097412109375 = 1.369039535522461 + 50.0 * 6.344813823699951
Epoch 370, val loss: 1.415601372718811
Epoch 380, training loss: 318.3719482421875 = 1.3416656255722046 + 50.0 * 6.34060525894165
Epoch 380, val loss: 1.3923358917236328
Epoch 390, training loss: 318.6712646484375 = 1.313991665840149 + 50.0 * 6.3471455574035645
Epoch 390, val loss: 1.36900794506073
Epoch 400, training loss: 318.1278381347656 = 1.2862298488616943 + 50.0 * 6.336832523345947
Epoch 400, val loss: 1.3457932472229004
Epoch 410, training loss: 317.84747314453125 = 1.2586082220077515 + 50.0 * 6.331777095794678
Epoch 410, val loss: 1.3228332996368408
Epoch 420, training loss: 317.62652587890625 = 1.2311776876449585 + 50.0 * 6.327907085418701
Epoch 420, val loss: 1.3003476858139038
Epoch 430, training loss: 317.4466247558594 = 1.2039587497711182 + 50.0 * 6.324853420257568
Epoch 430, val loss: 1.2782902717590332
Epoch 440, training loss: 317.5226135253906 = 1.176934003829956 + 50.0 * 6.326913356781006
Epoch 440, val loss: 1.2566156387329102
Epoch 450, training loss: 317.21173095703125 = 1.1499712467193604 + 50.0 * 6.321235656738281
Epoch 450, val loss: 1.2352197170257568
Epoch 460, training loss: 317.0137634277344 = 1.123550534248352 + 50.0 * 6.317803859710693
Epoch 460, val loss: 1.2146154642105103
Epoch 470, training loss: 316.83953857421875 = 1.0976663827896118 + 50.0 * 6.31483793258667
Epoch 470, val loss: 1.1947050094604492
Epoch 480, training loss: 316.7261047363281 = 1.0723167657852173 + 50.0 * 6.313075542449951
Epoch 480, val loss: 1.1756041049957275
Epoch 490, training loss: 316.6722106933594 = 1.0474053621292114 + 50.0 * 6.312496185302734
Epoch 490, val loss: 1.1571500301361084
Epoch 500, training loss: 316.54962158203125 = 1.0230481624603271 + 50.0 * 6.3105316162109375
Epoch 500, val loss: 1.1392210721969604
Epoch 510, training loss: 316.4704284667969 = 0.9993715882301331 + 50.0 * 6.309421539306641
Epoch 510, val loss: 1.1222652196884155
Epoch 520, training loss: 316.1973571777344 = 0.9762917160987854 + 50.0 * 6.304421424865723
Epoch 520, val loss: 1.106096863746643
Epoch 530, training loss: 316.0503234863281 = 0.9539808630943298 + 50.0 * 6.301926612854004
Epoch 530, val loss: 1.0909103155136108
Epoch 540, training loss: 316.0975646972656 = 0.9322619438171387 + 50.0 * 6.3033061027526855
Epoch 540, val loss: 1.0764278173446655
Epoch 550, training loss: 315.86114501953125 = 0.911231279373169 + 50.0 * 6.2989983558654785
Epoch 550, val loss: 1.062699794769287
Epoch 560, training loss: 315.7139892578125 = 0.8907018303871155 + 50.0 * 6.2964653968811035
Epoch 560, val loss: 1.049622654914856
Epoch 570, training loss: 315.9799499511719 = 0.8710060715675354 + 50.0 * 6.302178859710693
Epoch 570, val loss: 1.0373451709747314
Epoch 580, training loss: 315.47308349609375 = 0.8514979481697083 + 50.0 * 6.292431831359863
Epoch 580, val loss: 1.0256173610687256
Epoch 590, training loss: 315.3448486328125 = 0.83273845911026 + 50.0 * 6.2902421951293945
Epoch 590, val loss: 1.0146455764770508
Epoch 600, training loss: 315.2668151855469 = 0.8146442770957947 + 50.0 * 6.289043426513672
Epoch 600, val loss: 1.0044842958450317
Epoch 610, training loss: 315.2856750488281 = 0.7969303727149963 + 50.0 * 6.2897748947143555
Epoch 610, val loss: 0.9947580099105835
Epoch 620, training loss: 315.10906982421875 = 0.7796387672424316 + 50.0 * 6.286588668823242
Epoch 620, val loss: 0.9856173992156982
Epoch 630, training loss: 315.0694580078125 = 0.7627200484275818 + 50.0 * 6.286134719848633
Epoch 630, val loss: 0.9768405556678772
Epoch 640, training loss: 314.8847961425781 = 0.7463158369064331 + 50.0 * 6.282769680023193
Epoch 640, val loss: 0.9688071012496948
Epoch 650, training loss: 314.79278564453125 = 0.7302583456039429 + 50.0 * 6.281250476837158
Epoch 650, val loss: 0.961191713809967
Epoch 660, training loss: 315.2364501953125 = 0.7145888209342957 + 50.0 * 6.2904372215271
Epoch 660, val loss: 0.9538630843162537
Epoch 670, training loss: 314.7001037597656 = 0.698778510093689 + 50.0 * 6.280026435852051
Epoch 670, val loss: 0.9468001127243042
Epoch 680, training loss: 314.5692138671875 = 0.6835525631904602 + 50.0 * 6.277712821960449
Epoch 680, val loss: 0.9401531219482422
Epoch 690, training loss: 314.42755126953125 = 0.6687625050544739 + 50.0 * 6.27517557144165
Epoch 690, val loss: 0.9340550303459167
Epoch 700, training loss: 314.35394287109375 = 0.6542137265205383 + 50.0 * 6.2739949226379395
Epoch 700, val loss: 0.92831951379776
Epoch 710, training loss: 314.81683349609375 = 0.6398651599884033 + 50.0 * 6.283539295196533
Epoch 710, val loss: 0.9227079153060913
Epoch 720, training loss: 314.3157653808594 = 0.6253520250320435 + 50.0 * 6.273808002471924
Epoch 720, val loss: 0.9170621037483215
Epoch 730, training loss: 314.1557922363281 = 0.6112671494483948 + 50.0 * 6.270890712738037
Epoch 730, val loss: 0.9119073748588562
Epoch 740, training loss: 314.0444030761719 = 0.5975139737129211 + 50.0 * 6.268938064575195
Epoch 740, val loss: 0.9071369767189026
Epoch 750, training loss: 314.00518798828125 = 0.5839611291885376 + 50.0 * 6.2684245109558105
Epoch 750, val loss: 0.9026907086372375
Epoch 760, training loss: 313.9493408203125 = 0.5703445076942444 + 50.0 * 6.267580032348633
Epoch 760, val loss: 0.8982243537902832
Epoch 770, training loss: 313.9739074707031 = 0.5569762587547302 + 50.0 * 6.268338680267334
Epoch 770, val loss: 0.8939326405525208
Epoch 780, training loss: 313.89630126953125 = 0.5438026785850525 + 50.0 * 6.267050266265869
Epoch 780, val loss: 0.8900273442268372
Epoch 790, training loss: 313.74273681640625 = 0.5308406352996826 + 50.0 * 6.264237403869629
Epoch 790, val loss: 0.8863449692726135
Epoch 800, training loss: 313.7315368652344 = 0.5181422233581543 + 50.0 * 6.264268398284912
Epoch 800, val loss: 0.8828717470169067
Epoch 810, training loss: 313.63079833984375 = 0.505495548248291 + 50.0 * 6.262506008148193
Epoch 810, val loss: 0.8795523643493652
Epoch 820, training loss: 313.63604736328125 = 0.4930281639099121 + 50.0 * 6.26285982131958
Epoch 820, val loss: 0.876316487789154
Epoch 830, training loss: 313.594970703125 = 0.4807380437850952 + 50.0 * 6.262284755706787
Epoch 830, val loss: 0.873216450214386
Epoch 840, training loss: 313.4563903808594 = 0.46858611702919006 + 50.0 * 6.259756088256836
Epoch 840, val loss: 0.8703742027282715
Epoch 850, training loss: 313.3612365722656 = 0.45672109723091125 + 50.0 * 6.258090019226074
Epoch 850, val loss: 0.8677505254745483
Epoch 860, training loss: 313.3813781738281 = 0.4450886845588684 + 50.0 * 6.258725643157959
Epoch 860, val loss: 0.8653561472892761
Epoch 870, training loss: 313.2678527832031 = 0.4335922300815582 + 50.0 * 6.256685256958008
Epoch 870, val loss: 0.8631628155708313
Epoch 880, training loss: 313.3574523925781 = 0.42234376072883606 + 50.0 * 6.258702278137207
Epoch 880, val loss: 0.8611182570457458
Epoch 890, training loss: 313.1676330566406 = 0.4112304151058197 + 50.0 * 6.255127906799316
Epoch 890, val loss: 0.8592615723609924
Epoch 900, training loss: 313.0622863769531 = 0.400447815656662 + 50.0 * 6.253236770629883
Epoch 900, val loss: 0.857740044593811
Epoch 910, training loss: 313.0790100097656 = 0.3899626135826111 + 50.0 * 6.253780841827393
Epoch 910, val loss: 0.8564460873603821
Epoch 920, training loss: 313.15362548828125 = 0.37964773178100586 + 50.0 * 6.25547981262207
Epoch 920, val loss: 0.855258047580719
Epoch 930, training loss: 313.14202880859375 = 0.3696773946285248 + 50.0 * 6.255446910858154
Epoch 930, val loss: 0.8541495203971863
Epoch 940, training loss: 312.8973693847656 = 0.35979199409484863 + 50.0 * 6.250751495361328
Epoch 940, val loss: 0.8533208966255188
Epoch 950, training loss: 312.8208312988281 = 0.3503059148788452 + 50.0 * 6.249410629272461
Epoch 950, val loss: 0.8528437614440918
Epoch 960, training loss: 312.7744445800781 = 0.34109681844711304 + 50.0 * 6.248667240142822
Epoch 960, val loss: 0.8526090979576111
Epoch 970, training loss: 313.09100341796875 = 0.3321405351161957 + 50.0 * 6.255177021026611
Epoch 970, val loss: 0.8523737192153931
Epoch 980, training loss: 312.8780517578125 = 0.3232792913913727 + 50.0 * 6.251095294952393
Epoch 980, val loss: 0.8523236513137817
Epoch 990, training loss: 312.6475830078125 = 0.3146989345550537 + 50.0 * 6.246657848358154
Epoch 990, val loss: 0.852554976940155
Epoch 1000, training loss: 312.6894836425781 = 0.3065240681171417 + 50.0 * 6.247659206390381
Epoch 1000, val loss: 0.8530365824699402
Epoch 1010, training loss: 312.666748046875 = 0.2984530031681061 + 50.0 * 6.247365951538086
Epoch 1010, val loss: 0.853659451007843
Epoch 1020, training loss: 312.5478820800781 = 0.29058837890625 + 50.0 * 6.245145797729492
Epoch 1020, val loss: 0.8544272780418396
Epoch 1030, training loss: 312.4909362792969 = 0.2830761671066284 + 50.0 * 6.244157314300537
Epoch 1030, val loss: 0.8554773926734924
Epoch 1040, training loss: 312.528076171875 = 0.2757626473903656 + 50.0 * 6.245046615600586
Epoch 1040, val loss: 0.8566917777061462
Epoch 1050, training loss: 312.3955078125 = 0.26860377192497253 + 50.0 * 6.242537975311279
Epoch 1050, val loss: 0.8579205870628357
Epoch 1060, training loss: 312.4068603515625 = 0.26168277859687805 + 50.0 * 6.242903709411621
Epoch 1060, val loss: 0.8593946099281311
Epoch 1070, training loss: 312.4232482910156 = 0.2549699544906616 + 50.0 * 6.24336576461792
Epoch 1070, val loss: 0.8609391450881958
Epoch 1080, training loss: 312.5779724121094 = 0.24841706454753876 + 50.0 * 6.246591091156006
Epoch 1080, val loss: 0.8625772595405579
Epoch 1090, training loss: 312.2558898925781 = 0.24204501509666443 + 50.0 * 6.240277290344238
Epoch 1090, val loss: 0.8644806742668152
Epoch 1100, training loss: 312.2179260253906 = 0.235945925116539 + 50.0 * 6.239639759063721
Epoch 1100, val loss: 0.8666536211967468
Epoch 1110, training loss: 312.1667785644531 = 0.23007439076900482 + 50.0 * 6.238734245300293
Epoch 1110, val loss: 0.8688666820526123
Epoch 1120, training loss: 312.37493896484375 = 0.22438272833824158 + 50.0 * 6.243010997772217
Epoch 1120, val loss: 0.8710877299308777
Epoch 1130, training loss: 312.1909484863281 = 0.21869252622127533 + 50.0 * 6.239445209503174
Epoch 1130, val loss: 0.8733784556388855
Epoch 1140, training loss: 312.1966247558594 = 0.21321417391300201 + 50.0 * 6.239667892456055
Epoch 1140, val loss: 0.8755990862846375
Epoch 1150, training loss: 312.07427978515625 = 0.20793133974075317 + 50.0 * 6.2373270988464355
Epoch 1150, val loss: 0.878284752368927
Epoch 1160, training loss: 312.00616455078125 = 0.2028665691614151 + 50.0 * 6.23606538772583
Epoch 1160, val loss: 0.8809723854064941
Epoch 1170, training loss: 312.14227294921875 = 0.19798435270786285 + 50.0 * 6.238885402679443
Epoch 1170, val loss: 0.8837975859642029
Epoch 1180, training loss: 312.0696105957031 = 0.1930481493473053 + 50.0 * 6.2375311851501465
Epoch 1180, val loss: 0.8863577842712402
Epoch 1190, training loss: 312.020751953125 = 0.18833957612514496 + 50.0 * 6.236648082733154
Epoch 1190, val loss: 0.8892849683761597
Epoch 1200, training loss: 311.900634765625 = 0.18377777934074402 + 50.0 * 6.234337329864502
Epoch 1200, val loss: 0.8922651410102844
Epoch 1210, training loss: 311.9309387207031 = 0.17939753830432892 + 50.0 * 6.2350311279296875
Epoch 1210, val loss: 0.8954390287399292
Epoch 1220, training loss: 311.93572998046875 = 0.17511118948459625 + 50.0 * 6.235212326049805
Epoch 1220, val loss: 0.8984911441802979
Epoch 1230, training loss: 311.8807678222656 = 0.17089539766311646 + 50.0 * 6.234197616577148
Epoch 1230, val loss: 0.9016600251197815
Epoch 1240, training loss: 311.9123840332031 = 0.16685797274112701 + 50.0 * 6.234910011291504
Epoch 1240, val loss: 0.904947817325592
Epoch 1250, training loss: 311.8329162597656 = 0.16292579472064972 + 50.0 * 6.233399868011475
Epoch 1250, val loss: 0.908405065536499
Epoch 1260, training loss: 311.7532653808594 = 0.15908460319042206 + 50.0 * 6.231883525848389
Epoch 1260, val loss: 0.9119076728820801
Epoch 1270, training loss: 311.688232421875 = 0.1553696244955063 + 50.0 * 6.230657577514648
Epoch 1270, val loss: 0.915458083152771
Epoch 1280, training loss: 311.7507629394531 = 0.15177100896835327 + 50.0 * 6.231979846954346
Epoch 1280, val loss: 0.9191709756851196
Epoch 1290, training loss: 311.8297119140625 = 0.14819273352622986 + 50.0 * 6.233630657196045
Epoch 1290, val loss: 0.9226715564727783
Epoch 1300, training loss: 311.6686706542969 = 0.14471657574176788 + 50.0 * 6.2304792404174805
Epoch 1300, val loss: 0.9261535406112671
Epoch 1310, training loss: 311.6573791503906 = 0.14138416945934296 + 50.0 * 6.230319976806641
Epoch 1310, val loss: 0.9299036860466003
Epoch 1320, training loss: 311.59478759765625 = 0.13815705478191376 + 50.0 * 6.229132652282715
Epoch 1320, val loss: 0.9339298605918884
Epoch 1330, training loss: 311.74835205078125 = 0.13505031168460846 + 50.0 * 6.232265949249268
Epoch 1330, val loss: 0.9378289580345154
Epoch 1340, training loss: 311.5662841796875 = 0.13195310533046722 + 50.0 * 6.228686332702637
Epoch 1340, val loss: 0.9416003227233887
Epoch 1350, training loss: 311.6242980957031 = 0.12894795835018158 + 50.0 * 6.229907512664795
Epoch 1350, val loss: 0.9454827308654785
Epoch 1360, training loss: 311.47308349609375 = 0.12603570520877838 + 50.0 * 6.226941108703613
Epoch 1360, val loss: 0.9493997097015381
Epoch 1370, training loss: 311.4754638671875 = 0.12321244925260544 + 50.0 * 6.22704553604126
Epoch 1370, val loss: 0.9534615874290466
Epoch 1380, training loss: 311.5050964355469 = 0.12050168961286545 + 50.0 * 6.227691650390625
Epoch 1380, val loss: 0.9574826955795288
Epoch 1390, training loss: 311.7935791015625 = 0.11780377477407455 + 50.0 * 6.233515739440918
Epoch 1390, val loss: 0.9615156650543213
Epoch 1400, training loss: 311.4654235839844 = 0.11510619521141052 + 50.0 * 6.227006435394287
Epoch 1400, val loss: 0.9650707244873047
Epoch 1410, training loss: 311.4075622558594 = 0.11256147176027298 + 50.0 * 6.225900173187256
Epoch 1410, val loss: 0.9694615006446838
Epoch 1420, training loss: 311.3337707519531 = 0.11012236028909683 + 50.0 * 6.224472999572754
Epoch 1420, val loss: 0.9736782312393188
Epoch 1430, training loss: 311.33551025390625 = 0.1077534556388855 + 50.0 * 6.224555015563965
Epoch 1430, val loss: 0.977834165096283
Epoch 1440, training loss: 311.8826904296875 = 0.10547289252281189 + 50.0 * 6.235544204711914
Epoch 1440, val loss: 0.9819052219390869
Epoch 1450, training loss: 311.4449768066406 = 0.10307008773088455 + 50.0 * 6.226838111877441
Epoch 1450, val loss: 0.9858001470565796
Epoch 1460, training loss: 311.28765869140625 = 0.10083254426717758 + 50.0 * 6.223736763000488
Epoch 1460, val loss: 0.9899194836616516
Epoch 1470, training loss: 311.30096435546875 = 0.09869284927845001 + 50.0 * 6.224045276641846
Epoch 1470, val loss: 0.9942458271980286
Epoch 1480, training loss: 311.422607421875 = 0.0965789407491684 + 50.0 * 6.226520538330078
Epoch 1480, val loss: 0.9984105825424194
Epoch 1490, training loss: 311.2100524902344 = 0.09453079104423523 + 50.0 * 6.2223100662231445
Epoch 1490, val loss: 1.002598524093628
Epoch 1500, training loss: 311.3863830566406 = 0.09255436807870865 + 50.0 * 6.225876331329346
Epoch 1500, val loss: 1.0068333148956299
Epoch 1510, training loss: 311.3302307128906 = 0.09056270122528076 + 50.0 * 6.224792957305908
Epoch 1510, val loss: 1.0110807418823242
Epoch 1520, training loss: 311.19158935546875 = 0.0886661633849144 + 50.0 * 6.2220587730407715
Epoch 1520, val loss: 1.0153980255126953
Epoch 1530, training loss: 311.1554870605469 = 0.08681819587945938 + 50.0 * 6.221373558044434
Epoch 1530, val loss: 1.0197149515151978
Epoch 1540, training loss: 311.1484680175781 = 0.08502987772226334 + 50.0 * 6.221268653869629
Epoch 1540, val loss: 1.0240867137908936
Epoch 1550, training loss: 311.54302978515625 = 0.0832904651761055 + 50.0 * 6.2291951179504395
Epoch 1550, val loss: 1.028242588043213
Epoch 1560, training loss: 311.1884460449219 = 0.08150723576545715 + 50.0 * 6.222138404846191
Epoch 1560, val loss: 1.0324188470840454
Epoch 1570, training loss: 311.07843017578125 = 0.07984047383069992 + 50.0 * 6.219971656799316
Epoch 1570, val loss: 1.0368300676345825
Epoch 1580, training loss: 311.08343505859375 = 0.07821954041719437 + 50.0 * 6.220104217529297
Epoch 1580, val loss: 1.0412747859954834
Epoch 1590, training loss: 311.24365234375 = 0.07664680480957031 + 50.0 * 6.223340034484863
Epoch 1590, val loss: 1.045676589012146
Epoch 1600, training loss: 311.27587890625 = 0.07509836554527283 + 50.0 * 6.224015712738037
Epoch 1600, val loss: 1.0498496294021606
Epoch 1610, training loss: 311.069580078125 = 0.07354075461626053 + 50.0 * 6.219920635223389
Epoch 1610, val loss: 1.0540319681167603
Epoch 1620, training loss: 310.9891662597656 = 0.07208193838596344 + 50.0 * 6.218341827392578
Epoch 1620, val loss: 1.0585707426071167
Epoch 1630, training loss: 310.9775695800781 = 0.07066112756729126 + 50.0 * 6.218138217926025
Epoch 1630, val loss: 1.0629491806030273
Epoch 1640, training loss: 311.1604309082031 = 0.06928613781929016 + 50.0 * 6.221823215484619
Epoch 1640, val loss: 1.067254900932312
Epoch 1650, training loss: 311.1880187988281 = 0.06788706034421921 + 50.0 * 6.222402095794678
Epoch 1650, val loss: 1.0713893175125122
Epoch 1660, training loss: 311.1478271484375 = 0.0665297880768776 + 50.0 * 6.221626281738281
Epoch 1660, val loss: 1.0757108926773071
Epoch 1670, training loss: 310.9786682128906 = 0.0652102679014206 + 50.0 * 6.218269348144531
Epoch 1670, val loss: 1.0799822807312012
Epoch 1680, training loss: 310.8905334472656 = 0.06395500898361206 + 50.0 * 6.216531276702881
Epoch 1680, val loss: 1.084315299987793
Epoch 1690, training loss: 310.924560546875 = 0.06274210661649704 + 50.0 * 6.217236518859863
Epoch 1690, val loss: 1.088708519935608
Epoch 1700, training loss: 311.04949951171875 = 0.06153815984725952 + 50.0 * 6.219759464263916
Epoch 1700, val loss: 1.092979907989502
Epoch 1710, training loss: 311.0877990722656 = 0.06034930795431137 + 50.0 * 6.220548629760742
Epoch 1710, val loss: 1.097591519355774
Epoch 1720, training loss: 311.0376892089844 = 0.05917360633611679 + 50.0 * 6.219570159912109
Epoch 1720, val loss: 1.1014562845230103
Epoch 1730, training loss: 310.873779296875 = 0.05803052708506584 + 50.0 * 6.216314792633057
Epoch 1730, val loss: 1.1057217121124268
Epoch 1740, training loss: 310.8139343261719 = 0.05694675073027611 + 50.0 * 6.215139865875244
Epoch 1740, val loss: 1.1101455688476562
Epoch 1750, training loss: 310.81549072265625 = 0.0559009350836277 + 50.0 * 6.21519136428833
Epoch 1750, val loss: 1.1146018505096436
Epoch 1760, training loss: 310.9656982421875 = 0.05488935485482216 + 50.0 * 6.2182159423828125
Epoch 1760, val loss: 1.1189384460449219
Epoch 1770, training loss: 310.77227783203125 = 0.05383412167429924 + 50.0 * 6.21436882019043
Epoch 1770, val loss: 1.1230098009109497
Epoch 1780, training loss: 310.9002990722656 = 0.05284399539232254 + 50.0 * 6.216948986053467
Epoch 1780, val loss: 1.1272071599960327
Epoch 1790, training loss: 310.9432373046875 = 0.05188191682100296 + 50.0 * 6.217826843261719
Epoch 1790, val loss: 1.1315144300460815
Epoch 1800, training loss: 310.78387451171875 = 0.05090940371155739 + 50.0 * 6.214659214019775
Epoch 1800, val loss: 1.1357142925262451
Epoch 1810, training loss: 310.7298583984375 = 0.0499836690723896 + 50.0 * 6.213597297668457
Epoch 1810, val loss: 1.1399821043014526
Epoch 1820, training loss: 310.7687072753906 = 0.04910018667578697 + 50.0 * 6.214392185211182
Epoch 1820, val loss: 1.144273042678833
Epoch 1830, training loss: 310.8763732910156 = 0.04822118207812309 + 50.0 * 6.2165632247924805
Epoch 1830, val loss: 1.1483008861541748
Epoch 1840, training loss: 310.8149719238281 = 0.04733848199248314 + 50.0 * 6.215353012084961
Epoch 1840, val loss: 1.152492642402649
Epoch 1850, training loss: 310.73223876953125 = 0.0464903824031353 + 50.0 * 6.213715076446533
Epoch 1850, val loss: 1.156759262084961
Epoch 1860, training loss: 310.6998291015625 = 0.04566570743918419 + 50.0 * 6.213083744049072
Epoch 1860, val loss: 1.1608374118804932
Epoch 1870, training loss: 310.78826904296875 = 0.04487598314881325 + 50.0 * 6.214868068695068
Epoch 1870, val loss: 1.1651967763900757
Epoch 1880, training loss: 310.6892395019531 = 0.04408174380660057 + 50.0 * 6.2129034996032715
Epoch 1880, val loss: 1.169082760810852
Epoch 1890, training loss: 310.73663330078125 = 0.043335314840078354 + 50.0 * 6.213866233825684
Epoch 1890, val loss: 1.1733161211013794
Epoch 1900, training loss: 310.6656799316406 = 0.0425679013133049 + 50.0 * 6.212461948394775
Epoch 1900, val loss: 1.1775051355361938
Epoch 1910, training loss: 310.70648193359375 = 0.04183945432305336 + 50.0 * 6.213293075561523
Epoch 1910, val loss: 1.1815837621688843
Epoch 1920, training loss: 310.7196350097656 = 0.04112227261066437 + 50.0 * 6.213570594787598
Epoch 1920, val loss: 1.1853421926498413
Epoch 1930, training loss: 310.6448669433594 = 0.04041826352477074 + 50.0 * 6.2120890617370605
Epoch 1930, val loss: 1.1895289421081543
Epoch 1940, training loss: 310.560302734375 = 0.03973250091075897 + 50.0 * 6.210411548614502
Epoch 1940, val loss: 1.193647861480713
Epoch 1950, training loss: 310.53704833984375 = 0.039074886590242386 + 50.0 * 6.209959983825684
Epoch 1950, val loss: 1.1977070569992065
Epoch 1960, training loss: 310.6383361816406 = 0.03844636678695679 + 50.0 * 6.2119975090026855
Epoch 1960, val loss: 1.2018376588821411
Epoch 1970, training loss: 310.64349365234375 = 0.037799496203660965 + 50.0 * 6.212113857269287
Epoch 1970, val loss: 1.205480694770813
Epoch 1980, training loss: 310.7362976074219 = 0.037157244980335236 + 50.0 * 6.213982582092285
Epoch 1980, val loss: 1.2094098329544067
Epoch 1990, training loss: 310.6947937011719 = 0.036519553512334824 + 50.0 * 6.213165283203125
Epoch 1990, val loss: 1.2132072448730469
Epoch 2000, training loss: 310.5533142089844 = 0.03591552749276161 + 50.0 * 6.210348129272461
Epoch 2000, val loss: 1.2170313596725464
Epoch 2010, training loss: 310.4892883300781 = 0.03532467782497406 + 50.0 * 6.209079265594482
Epoch 2010, val loss: 1.2210990190505981
Epoch 2020, training loss: 310.4686584472656 = 0.034767020493745804 + 50.0 * 6.208677768707275
Epoch 2020, val loss: 1.2250077724456787
Epoch 2030, training loss: 310.5147399902344 = 0.03422357887029648 + 50.0 * 6.209610462188721
Epoch 2030, val loss: 1.2288724184036255
Epoch 2040, training loss: 310.7445373535156 = 0.03367314860224724 + 50.0 * 6.214217662811279
Epoch 2040, val loss: 1.2325373888015747
Epoch 2050, training loss: 310.673828125 = 0.033141426742076874 + 50.0 * 6.212813854217529
Epoch 2050, val loss: 1.2364532947540283
Epoch 2060, training loss: 310.4792175292969 = 0.03259706124663353 + 50.0 * 6.208932399749756
Epoch 2060, val loss: 1.240132451057434
Epoch 2070, training loss: 310.4310302734375 = 0.03209446370601654 + 50.0 * 6.20797872543335
Epoch 2070, val loss: 1.244223713874817
Epoch 2080, training loss: 310.3995361328125 = 0.031602054834365845 + 50.0 * 6.207358360290527
Epoch 2080, val loss: 1.247971534729004
Epoch 2090, training loss: 310.5559387207031 = 0.031122518703341484 + 50.0 * 6.210495948791504
Epoch 2090, val loss: 1.2518229484558105
Epoch 2100, training loss: 310.5960998535156 = 0.03063216060400009 + 50.0 * 6.211309432983398
Epoch 2100, val loss: 1.2553462982177734
Epoch 2110, training loss: 310.3887023925781 = 0.03014315292239189 + 50.0 * 6.2071709632873535
Epoch 2110, val loss: 1.258743166923523
Epoch 2120, training loss: 310.3752746582031 = 0.029681673273444176 + 50.0 * 6.206911563873291
Epoch 2120, val loss: 1.2625371217727661
Epoch 2130, training loss: 310.41693115234375 = 0.02923511154949665 + 50.0 * 6.207753658294678
Epoch 2130, val loss: 1.2663148641586304
Epoch 2140, training loss: 310.53656005859375 = 0.028798040002584457 + 50.0 * 6.210155010223389
Epoch 2140, val loss: 1.269819736480713
Epoch 2150, training loss: 310.3818359375 = 0.028367117047309875 + 50.0 * 6.207069396972656
Epoch 2150, val loss: 1.273366928100586
Epoch 2160, training loss: 310.3489990234375 = 0.027946237474679947 + 50.0 * 6.2064208984375
Epoch 2160, val loss: 1.2770049571990967
Epoch 2170, training loss: 310.49932861328125 = 0.02753370814025402 + 50.0 * 6.209435939788818
Epoch 2170, val loss: 1.280541181564331
Epoch 2180, training loss: 310.3084411621094 = 0.027125410735607147 + 50.0 * 6.205626010894775
Epoch 2180, val loss: 1.2840449810028076
Epoch 2190, training loss: 310.4161682128906 = 0.026739658787846565 + 50.0 * 6.207788944244385
Epoch 2190, val loss: 1.2875293493270874
Epoch 2200, training loss: 310.4919738769531 = 0.026340140029788017 + 50.0 * 6.209312438964844
Epoch 2200, val loss: 1.2910197973251343
Epoch 2210, training loss: 310.3813781738281 = 0.0259501114487648 + 50.0 * 6.207108497619629
Epoch 2210, val loss: 1.294529676437378
Epoch 2220, training loss: 310.2920837402344 = 0.025575906038284302 + 50.0 * 6.2053303718566895
Epoch 2220, val loss: 1.2980488538742065
Epoch 2230, training loss: 310.2431945800781 = 0.02521423064172268 + 50.0 * 6.204359531402588
Epoch 2230, val loss: 1.3015867471694946
Epoch 2240, training loss: 310.3017883300781 = 0.024865642189979553 + 50.0 * 6.205538272857666
Epoch 2240, val loss: 1.3049348592758179
Epoch 2250, training loss: 310.6380615234375 = 0.024523582309484482 + 50.0 * 6.212270736694336
Epoch 2250, val loss: 1.3082289695739746
Epoch 2260, training loss: 310.3386535644531 = 0.024154972285032272 + 50.0 * 6.206289768218994
Epoch 2260, val loss: 1.3115328550338745
Epoch 2270, training loss: 310.28167724609375 = 0.02381119504570961 + 50.0 * 6.205157279968262
Epoch 2270, val loss: 1.314824104309082
Epoch 2280, training loss: 310.40008544921875 = 0.023485630750656128 + 50.0 * 6.207531452178955
Epoch 2280, val loss: 1.3181921243667603
Epoch 2290, training loss: 310.2302551269531 = 0.023159034550189972 + 50.0 * 6.204141616821289
Epoch 2290, val loss: 1.3216166496276855
Epoch 2300, training loss: 310.2527770996094 = 0.022850999608635902 + 50.0 * 6.204598426818848
Epoch 2300, val loss: 1.3250010013580322
Epoch 2310, training loss: 310.22540283203125 = 0.022537700831890106 + 50.0 * 6.204057216644287
Epoch 2310, val loss: 1.3283277750015259
Epoch 2320, training loss: 310.5747375488281 = 0.022239886224269867 + 50.0 * 6.211050033569336
Epoch 2320, val loss: 1.3316338062286377
Epoch 2330, training loss: 310.3577575683594 = 0.021933268755674362 + 50.0 * 6.206716537475586
Epoch 2330, val loss: 1.3346319198608398
Epoch 2340, training loss: 310.2315979003906 = 0.021629365161061287 + 50.0 * 6.204199314117432
Epoch 2340, val loss: 1.3377991914749146
Epoch 2350, training loss: 310.16436767578125 = 0.021344924345612526 + 50.0 * 6.2028608322143555
Epoch 2350, val loss: 1.3412082195281982
Epoch 2360, training loss: 310.1555480957031 = 0.02107585221529007 + 50.0 * 6.202689170837402
Epoch 2360, val loss: 1.3445783853530884
Epoch 2370, training loss: 310.40045166015625 = 0.020810581743717194 + 50.0 * 6.207592487335205
Epoch 2370, val loss: 1.3476876020431519
Epoch 2380, training loss: 310.2244567871094 = 0.020527517423033714 + 50.0 * 6.204078197479248
Epoch 2380, val loss: 1.350698709487915
Epoch 2390, training loss: 310.2682800292969 = 0.020255999639630318 + 50.0 * 6.204960346221924
Epoch 2390, val loss: 1.3539657592773438
Epoch 2400, training loss: 310.13909912109375 = 0.01998893916606903 + 50.0 * 6.2023820877075195
Epoch 2400, val loss: 1.3570690155029297
Epoch 2410, training loss: 310.167724609375 = 0.019739698618650436 + 50.0 * 6.2029595375061035
Epoch 2410, val loss: 1.3602324724197388
Epoch 2420, training loss: 310.19818115234375 = 0.019492320716381073 + 50.0 * 6.203573703765869
Epoch 2420, val loss: 1.3633085489273071
Epoch 2430, training loss: 310.2362365722656 = 0.01924910396337509 + 50.0 * 6.204339504241943
Epoch 2430, val loss: 1.3661905527114868
Epoch 2440, training loss: 310.1736755371094 = 0.019005434587597847 + 50.0 * 6.2030930519104
Epoch 2440, val loss: 1.3693093061447144
Epoch 2450, training loss: 310.169921875 = 0.018765002489089966 + 50.0 * 6.2030229568481445
Epoch 2450, val loss: 1.372339129447937
Epoch 2460, training loss: 310.1510314941406 = 0.0185296181589365 + 50.0 * 6.20265007019043
Epoch 2460, val loss: 1.3754429817199707
Epoch 2470, training loss: 310.3406066894531 = 0.018298255279660225 + 50.0 * 6.206445693969727
Epoch 2470, val loss: 1.3782833814620972
Epoch 2480, training loss: 310.1302185058594 = 0.018077369779348373 + 50.0 * 6.202243328094482
Epoch 2480, val loss: 1.3813661336898804
Epoch 2490, training loss: 310.054931640625 = 0.01785069704055786 + 50.0 * 6.200741291046143
Epoch 2490, val loss: 1.384271502494812
Epoch 2500, training loss: 310.0494384765625 = 0.017638524994254112 + 50.0 * 6.20063591003418
Epoch 2500, val loss: 1.3872942924499512
Epoch 2510, training loss: 310.21221923828125 = 0.01743808388710022 + 50.0 * 6.2038960456848145
Epoch 2510, val loss: 1.39017653465271
Epoch 2520, training loss: 310.0942687988281 = 0.017225690186023712 + 50.0 * 6.201540946960449
Epoch 2520, val loss: 1.3929377794265747
Epoch 2530, training loss: 310.2681884765625 = 0.01702125556766987 + 50.0 * 6.205023288726807
Epoch 2530, val loss: 1.395538330078125
Epoch 2540, training loss: 310.0210876464844 = 0.016803404316306114 + 50.0 * 6.200085639953613
Epoch 2540, val loss: 1.3985280990600586
Epoch 2550, training loss: 310.0229797363281 = 0.016607727855443954 + 50.0 * 6.200127601623535
Epoch 2550, val loss: 1.4015214443206787
Epoch 2560, training loss: 310.0616455078125 = 0.016423162072896957 + 50.0 * 6.200904846191406
Epoch 2560, val loss: 1.4042837619781494
Epoch 2570, training loss: 310.19903564453125 = 0.01623499020934105 + 50.0 * 6.203656196594238
Epoch 2570, val loss: 1.4068596363067627
Epoch 2580, training loss: 310.0459899902344 = 0.016036687418818474 + 50.0 * 6.200599670410156
Epoch 2580, val loss: 1.4098434448242188
Epoch 2590, training loss: 309.9830017089844 = 0.01585732027888298 + 50.0 * 6.199342727661133
Epoch 2590, val loss: 1.4125772714614868
Epoch 2600, training loss: 310.29620361328125 = 0.015689745545387268 + 50.0 * 6.205610275268555
Epoch 2600, val loss: 1.4153797626495361
Epoch 2610, training loss: 310.02093505859375 = 0.015501395799219608 + 50.0 * 6.200108528137207
Epoch 2610, val loss: 1.4178533554077148
Epoch 2620, training loss: 309.967529296875 = 0.015321225859224796 + 50.0 * 6.199044227600098
Epoch 2620, val loss: 1.4205318689346313
Epoch 2630, training loss: 309.94256591796875 = 0.01515511516481638 + 50.0 * 6.198547840118408
Epoch 2630, val loss: 1.423445701599121
Epoch 2640, training loss: 310.092529296875 = 0.014998977072536945 + 50.0 * 6.2015509605407715
Epoch 2640, val loss: 1.4263614416122437
Epoch 2650, training loss: 310.1073913574219 = 0.014821230433881283 + 50.0 * 6.2018513679504395
Epoch 2650, val loss: 1.4285236597061157
Epoch 2660, training loss: 309.9785461425781 = 0.01465178094804287 + 50.0 * 6.199277877807617
Epoch 2660, val loss: 1.4311025142669678
Epoch 2670, training loss: 309.96044921875 = 0.014489925466477871 + 50.0 * 6.19891881942749
Epoch 2670, val loss: 1.4335851669311523
Epoch 2680, training loss: 309.9044494628906 = 0.014339711517095566 + 50.0 * 6.1978020668029785
Epoch 2680, val loss: 1.4364312887191772
Epoch 2690, training loss: 309.9104919433594 = 0.014193128794431686 + 50.0 * 6.197926044464111
Epoch 2690, val loss: 1.4390603303909302
Epoch 2700, training loss: 310.4148254394531 = 0.014059683308005333 + 50.0 * 6.208015441894531
Epoch 2700, val loss: 1.441538691520691
Epoch 2710, training loss: 310.2341003417969 = 0.013882335275411606 + 50.0 * 6.204404354095459
Epoch 2710, val loss: 1.443650484085083
Epoch 2720, training loss: 309.98052978515625 = 0.013725637458264828 + 50.0 * 6.199336051940918
Epoch 2720, val loss: 1.4462110996246338
Epoch 2730, training loss: 309.8735046386719 = 0.013581790030002594 + 50.0 * 6.197198390960693
Epoch 2730, val loss: 1.4488611221313477
Epoch 2740, training loss: 309.8586730957031 = 0.013447333127260208 + 50.0 * 6.196904182434082
Epoch 2740, val loss: 1.4514613151550293
Epoch 2750, training loss: 309.9208984375 = 0.013320110738277435 + 50.0 * 6.1981520652771
Epoch 2750, val loss: 1.4539971351623535
Epoch 2760, training loss: 310.1611633300781 = 0.013187087140977383 + 50.0 * 6.2029595375061035
Epoch 2760, val loss: 1.456207036972046
Epoch 2770, training loss: 309.97955322265625 = 0.013033409602940083 + 50.0 * 6.1993303298950195
Epoch 2770, val loss: 1.458602786064148
Epoch 2780, training loss: 309.8913879394531 = 0.012905586510896683 + 50.0 * 6.197569370269775
Epoch 2780, val loss: 1.4610095024108887
Epoch 2790, training loss: 309.9981994628906 = 0.012777999974787235 + 50.0 * 6.199708461761475
Epoch 2790, val loss: 1.4633474349975586
Epoch 2800, training loss: 309.98681640625 = 0.012648141011595726 + 50.0 * 6.199483394622803
Epoch 2800, val loss: 1.4655919075012207
Epoch 2810, training loss: 309.99822998046875 = 0.012517871335148811 + 50.0 * 6.199714183807373
Epoch 2810, val loss: 1.4680259227752686
Epoch 2820, training loss: 309.84979248046875 = 0.012389779090881348 + 50.0 * 6.196747779846191
Epoch 2820, val loss: 1.4702365398406982
Epoch 2830, training loss: 309.8393249511719 = 0.012270402163267136 + 50.0 * 6.1965413093566895
Epoch 2830, val loss: 1.4729247093200684
Epoch 2840, training loss: 309.94854736328125 = 0.012155885808169842 + 50.0 * 6.198728084564209
Epoch 2840, val loss: 1.4751845598220825
Epoch 2850, training loss: 309.9497985839844 = 0.012036561965942383 + 50.0 * 6.198755741119385
Epoch 2850, val loss: 1.4772485494613647
Epoch 2860, training loss: 309.88800048828125 = 0.011915020644664764 + 50.0 * 6.197522163391113
Epoch 2860, val loss: 1.479277491569519
Epoch 2870, training loss: 309.8343811035156 = 0.011796563863754272 + 50.0 * 6.196451187133789
Epoch 2870, val loss: 1.4817581176757812
Epoch 2880, training loss: 309.83929443359375 = 0.01169000007212162 + 50.0 * 6.196552276611328
Epoch 2880, val loss: 1.4840173721313477
Epoch 2890, training loss: 310.11053466796875 = 0.011585453525185585 + 50.0 * 6.20197868347168
Epoch 2890, val loss: 1.4860501289367676
Epoch 2900, training loss: 309.86968994140625 = 0.011461159214377403 + 50.0 * 6.197164535522461
Epoch 2900, val loss: 1.4882166385650635
Epoch 2910, training loss: 309.77545166015625 = 0.011357069946825504 + 50.0 * 6.195281982421875
Epoch 2910, val loss: 1.4904042482376099
Epoch 2920, training loss: 309.76446533203125 = 0.011255326680839062 + 50.0 * 6.195064067840576
Epoch 2920, val loss: 1.4926387071609497
Epoch 2930, training loss: 309.9254150390625 = 0.011156199499964714 + 50.0 * 6.198285102844238
Epoch 2930, val loss: 1.4947389364242554
Epoch 2940, training loss: 309.8356628417969 = 0.011053136549890041 + 50.0 * 6.1964921951293945
Epoch 2940, val loss: 1.4968775510787964
Epoch 2950, training loss: 309.9106140136719 = 0.010946453548967838 + 50.0 * 6.197993278503418
Epoch 2950, val loss: 1.4988821744918823
Epoch 2960, training loss: 309.7884216308594 = 0.010840018279850483 + 50.0 * 6.195551872253418
Epoch 2960, val loss: 1.5007622241973877
Epoch 2970, training loss: 309.7456970214844 = 0.010745652951300144 + 50.0 * 6.194699287414551
Epoch 2970, val loss: 1.5029594898223877
Epoch 2980, training loss: 309.8250427246094 = 0.010654453188180923 + 50.0 * 6.196287631988525
Epoch 2980, val loss: 1.505071997642517
Epoch 2990, training loss: 309.8938293457031 = 0.01055870484560728 + 50.0 * 6.197665214538574
Epoch 2990, val loss: 1.507046103477478
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 431.7882385253906 = 1.9475620985031128 + 50.0 * 8.596813201904297
Epoch 0, val loss: 1.9416037797927856
Epoch 10, training loss: 431.73065185546875 = 1.9381202459335327 + 50.0 * 8.595850944519043
Epoch 10, val loss: 1.9317117929458618
Epoch 20, training loss: 431.3850402832031 = 1.9263617992401123 + 50.0 * 8.589173316955566
Epoch 20, val loss: 1.9194295406341553
Epoch 30, training loss: 429.06854248046875 = 1.9116158485412598 + 50.0 * 8.54313850402832
Epoch 30, val loss: 1.904301404953003
Epoch 40, training loss: 414.82781982421875 = 1.8946470022201538 + 50.0 * 8.258663177490234
Epoch 40, val loss: 1.887636661529541
Epoch 50, training loss: 376.857666015625 = 1.8761358261108398 + 50.0 * 7.499630451202393
Epoch 50, val loss: 1.869884729385376
Epoch 60, training loss: 365.8558044433594 = 1.8627668619155884 + 50.0 * 7.279860973358154
Epoch 60, val loss: 1.857698917388916
Epoch 70, training loss: 353.7137451171875 = 1.852112889289856 + 50.0 * 7.037232398986816
Epoch 70, val loss: 1.8473855257034302
Epoch 80, training loss: 344.68853759765625 = 1.8406333923339844 + 50.0 * 6.856957912445068
Epoch 80, val loss: 1.8368209600448608
Epoch 90, training loss: 339.35498046875 = 1.8311598300933838 + 50.0 * 6.750476837158203
Epoch 90, val loss: 1.8282297849655151
Epoch 100, training loss: 336.5953674316406 = 1.821608066558838 + 50.0 * 6.6954755783081055
Epoch 100, val loss: 1.8197094202041626
Epoch 110, training loss: 334.4107666015625 = 1.8113125562667847 + 50.0 * 6.651988983154297
Epoch 110, val loss: 1.8104945421218872
Epoch 120, training loss: 332.75714111328125 = 1.8013607263565063 + 50.0 * 6.619115829467773
Epoch 120, val loss: 1.8016635179519653
Epoch 130, training loss: 331.382568359375 = 1.7918788194656372 + 50.0 * 6.591814041137695
Epoch 130, val loss: 1.7933167219161987
Epoch 140, training loss: 329.9908447265625 = 1.7826268672943115 + 50.0 * 6.564164638519287
Epoch 140, val loss: 1.7851312160491943
Epoch 150, training loss: 328.72967529296875 = 1.7734330892562866 + 50.0 * 6.539124488830566
Epoch 150, val loss: 1.7768439054489136
Epoch 160, training loss: 327.6263122558594 = 1.763705849647522 + 50.0 * 6.517251968383789
Epoch 160, val loss: 1.768195390701294
Epoch 170, training loss: 326.62005615234375 = 1.7531496286392212 + 50.0 * 6.49733829498291
Epoch 170, val loss: 1.7587701082229614
Epoch 180, training loss: 325.6831359863281 = 1.7416094541549683 + 50.0 * 6.478830814361572
Epoch 180, val loss: 1.7485471963882446
Epoch 190, training loss: 324.8987121582031 = 1.7291789054870605 + 50.0 * 6.463390350341797
Epoch 190, val loss: 1.7374745607376099
Epoch 200, training loss: 324.2452087402344 = 1.7155325412750244 + 50.0 * 6.4505934715271
Epoch 200, val loss: 1.7253378629684448
Epoch 210, training loss: 323.6181640625 = 1.7008389234542847 + 50.0 * 6.4383463859558105
Epoch 210, val loss: 1.7122498750686646
Epoch 220, training loss: 323.0506896972656 = 1.6849339008331299 + 50.0 * 6.4273152351379395
Epoch 220, val loss: 1.6981035470962524
Epoch 230, training loss: 322.6263122558594 = 1.6677565574645996 + 50.0 * 6.41917085647583
Epoch 230, val loss: 1.682868480682373
Epoch 240, training loss: 322.135986328125 = 1.6490552425384521 + 50.0 * 6.409739017486572
Epoch 240, val loss: 1.6663833856582642
Epoch 250, training loss: 321.7363586425781 = 1.6290192604064941 + 50.0 * 6.402146816253662
Epoch 250, val loss: 1.6488208770751953
Epoch 260, training loss: 321.37091064453125 = 1.607582449913025 + 50.0 * 6.395267009735107
Epoch 260, val loss: 1.6301237344741821
Epoch 270, training loss: 321.0793151855469 = 1.5847034454345703 + 50.0 * 6.389892101287842
Epoch 270, val loss: 1.6103324890136719
Epoch 280, training loss: 320.7333679199219 = 1.560504674911499 + 50.0 * 6.383457183837891
Epoch 280, val loss: 1.5895135402679443
Epoch 290, training loss: 320.42352294921875 = 1.5351099967956543 + 50.0 * 6.377768516540527
Epoch 290, val loss: 1.5678309202194214
Epoch 300, training loss: 320.2425537109375 = 1.5086296796798706 + 50.0 * 6.374678134918213
Epoch 300, val loss: 1.545336127281189
Epoch 310, training loss: 319.82293701171875 = 1.480960488319397 + 50.0 * 6.36683988571167
Epoch 310, val loss: 1.522284746170044
Epoch 320, training loss: 319.6960144042969 = 1.452531099319458 + 50.0 * 6.364869117736816
Epoch 320, val loss: 1.498634696006775
Epoch 330, training loss: 319.2744140625 = 1.4233185052871704 + 50.0 * 6.357022285461426
Epoch 330, val loss: 1.4746867418289185
Epoch 340, training loss: 319.0290832519531 = 1.3935227394104004 + 50.0 * 6.352711200714111
Epoch 340, val loss: 1.450519323348999
Epoch 350, training loss: 319.0382385253906 = 1.3633757829666138 + 50.0 * 6.353497505187988
Epoch 350, val loss: 1.4262441396713257
Epoch 360, training loss: 318.61932373046875 = 1.3326873779296875 + 50.0 * 6.345733165740967
Epoch 360, val loss: 1.401959776878357
Epoch 370, training loss: 318.3653564453125 = 1.3020638227462769 + 50.0 * 6.341265678405762
Epoch 370, val loss: 1.3778085708618164
Epoch 380, training loss: 318.13189697265625 = 1.2714526653289795 + 50.0 * 6.3372087478637695
Epoch 380, val loss: 1.3539754152297974
Epoch 390, training loss: 317.9487609863281 = 1.241013765335083 + 50.0 * 6.3341546058654785
Epoch 390, val loss: 1.3304872512817383
Epoch 400, training loss: 317.81195068359375 = 1.2106564044952393 + 50.0 * 6.332025527954102
Epoch 400, val loss: 1.3072518110275269
Epoch 410, training loss: 317.6536865234375 = 1.1807788610458374 + 50.0 * 6.329457759857178
Epoch 410, val loss: 1.284583330154419
Epoch 420, training loss: 317.3927001953125 = 1.1511629819869995 + 50.0 * 6.324831008911133
Epoch 420, val loss: 1.2625361680984497
Epoch 430, training loss: 317.24468994140625 = 1.122220754623413 + 50.0 * 6.322449207305908
Epoch 430, val loss: 1.2411459684371948
Epoch 440, training loss: 317.1414489746094 = 1.0939449071884155 + 50.0 * 6.320950031280518
Epoch 440, val loss: 1.2203258275985718
Epoch 450, training loss: 317.0101013183594 = 1.066107153892517 + 50.0 * 6.318880081176758
Epoch 450, val loss: 1.2002873420715332
Epoch 460, training loss: 316.8499450683594 = 1.0390483140945435 + 50.0 * 6.31621789932251
Epoch 460, val loss: 1.1808072328567505
Epoch 470, training loss: 316.6734313964844 = 1.0126612186431885 + 50.0 * 6.313215255737305
Epoch 470, val loss: 1.162078857421875
Epoch 480, training loss: 316.6154479980469 = 0.9870492815971375 + 50.0 * 6.312567710876465
Epoch 480, val loss: 1.1443278789520264
Epoch 490, training loss: 316.3676452636719 = 0.962326169013977 + 50.0 * 6.308105945587158
Epoch 490, val loss: 1.1271564960479736
Epoch 500, training loss: 316.2044982910156 = 0.9382922649383545 + 50.0 * 6.305324077606201
Epoch 500, val loss: 1.1108797788619995
Epoch 510, training loss: 316.095947265625 = 0.9150930047035217 + 50.0 * 6.303617000579834
Epoch 510, val loss: 1.0953500270843506
Epoch 520, training loss: 316.22943115234375 = 0.8924462795257568 + 50.0 * 6.306739807128906
Epoch 520, val loss: 1.080242395401001
Epoch 530, training loss: 315.9468688964844 = 0.8704822659492493 + 50.0 * 6.301527500152588
Epoch 530, val loss: 1.0660042762756348
Epoch 540, training loss: 315.751953125 = 0.8492283225059509 + 50.0 * 6.298054218292236
Epoch 540, val loss: 1.0522618293762207
Epoch 550, training loss: 315.6065979003906 = 0.8286150097846985 + 50.0 * 6.295559406280518
Epoch 550, val loss: 1.0390700101852417
Epoch 560, training loss: 315.9716491699219 = 0.8084813356399536 + 50.0 * 6.3032636642456055
Epoch 560, val loss: 1.0265343189239502
Epoch 570, training loss: 315.5207824707031 = 0.7889328598976135 + 50.0 * 6.2946367263793945
Epoch 570, val loss: 1.014104962348938
Epoch 580, training loss: 315.3140563964844 = 0.76992267370224 + 50.0 * 6.290882587432861
Epoch 580, val loss: 1.0023812055587769
Epoch 590, training loss: 315.1859130859375 = 0.7514711618423462 + 50.0 * 6.288688659667969
Epoch 590, val loss: 0.991088330745697
Epoch 600, training loss: 315.1808776855469 = 0.7335342764854431 + 50.0 * 6.288947105407715
Epoch 600, val loss: 0.9802164435386658
Epoch 610, training loss: 315.1780700683594 = 0.715732216835022 + 50.0 * 6.289247035980225
Epoch 610, val loss: 0.9694933891296387
Epoch 620, training loss: 314.9409484863281 = 0.6984642744064331 + 50.0 * 6.284850120544434
Epoch 620, val loss: 0.959246039390564
Epoch 630, training loss: 314.93603515625 = 0.6816102862358093 + 50.0 * 6.285088539123535
Epoch 630, val loss: 0.9493598341941833
Epoch 640, training loss: 314.74658203125 = 0.6650811433792114 + 50.0 * 6.281630516052246
Epoch 640, val loss: 0.9396170377731323
Epoch 650, training loss: 314.66680908203125 = 0.6489298939704895 + 50.0 * 6.280357837677002
Epoch 650, val loss: 0.9303476214408875
Epoch 660, training loss: 314.8215637207031 = 0.6332339644432068 + 50.0 * 6.283766269683838
Epoch 660, val loss: 0.921480655670166
Epoch 670, training loss: 314.6971130371094 = 0.6176393032073975 + 50.0 * 6.281589508056641
Epoch 670, val loss: 0.912552535533905
Epoch 680, training loss: 314.474365234375 = 0.6025691628456116 + 50.0 * 6.277435779571533
Epoch 680, val loss: 0.9043521881103516
Epoch 690, training loss: 314.3525390625 = 0.5878591537475586 + 50.0 * 6.275293827056885
Epoch 690, val loss: 0.896369218826294
Epoch 700, training loss: 314.2771911621094 = 0.5735650062561035 + 50.0 * 6.274072170257568
Epoch 700, val loss: 0.8888770937919617
Epoch 710, training loss: 314.47418212890625 = 0.5596193671226501 + 50.0 * 6.27829122543335
Epoch 710, val loss: 0.8816629648208618
Epoch 720, training loss: 314.4177551269531 = 0.5459057092666626 + 50.0 * 6.277437210083008
Epoch 720, val loss: 0.8750038146972656
Epoch 730, training loss: 314.1765441894531 = 0.5325364470481873 + 50.0 * 6.2728800773620605
Epoch 730, val loss: 0.8686848282814026
Epoch 740, training loss: 314.0359802246094 = 0.519589364528656 + 50.0 * 6.270328044891357
Epoch 740, val loss: 0.8626919388771057
Epoch 750, training loss: 313.9437255859375 = 0.5071134567260742 + 50.0 * 6.268732070922852
Epoch 750, val loss: 0.8573072552680969
Epoch 760, training loss: 314.1537780761719 = 0.4950379431247711 + 50.0 * 6.27317476272583
Epoch 760, val loss: 0.8521873950958252
Epoch 770, training loss: 314.0422058105469 = 0.48297351598739624 + 50.0 * 6.27118444442749
Epoch 770, val loss: 0.8474628329277039
Epoch 780, training loss: 313.7743835449219 = 0.47141218185424805 + 50.0 * 6.266059875488281
Epoch 780, val loss: 0.8431790471076965
Epoch 790, training loss: 313.7047119140625 = 0.46030107140541077 + 50.0 * 6.264888286590576
Epoch 790, val loss: 0.8392361998558044
Epoch 800, training loss: 313.72216796875 = 0.44945356249809265 + 50.0 * 6.265453815460205
Epoch 800, val loss: 0.8357070088386536
Epoch 810, training loss: 313.7369384765625 = 0.4388444125652313 + 50.0 * 6.265961647033691
Epoch 810, val loss: 0.8325981497764587
Epoch 820, training loss: 313.61163330078125 = 0.42859309911727905 + 50.0 * 6.263660907745361
Epoch 820, val loss: 0.8297529816627502
Epoch 830, training loss: 313.5355224609375 = 0.4185827970504761 + 50.0 * 6.262339115142822
Epoch 830, val loss: 0.8272412419319153
Epoch 840, training loss: 313.4428405761719 = 0.4088502824306488 + 50.0 * 6.260680198669434
Epoch 840, val loss: 0.8249905109405518
Epoch 850, training loss: 313.4351501464844 = 0.3993677794933319 + 50.0 * 6.260715961456299
Epoch 850, val loss: 0.8229302167892456
Epoch 860, training loss: 313.40203857421875 = 0.3900694251060486 + 50.0 * 6.260239124298096
Epoch 860, val loss: 0.821077287197113
Epoch 870, training loss: 313.3408203125 = 0.3810490071773529 + 50.0 * 6.259195804595947
Epoch 870, val loss: 0.8197832703590393
Epoch 880, training loss: 313.66217041015625 = 0.3721950650215149 + 50.0 * 6.265799522399902
Epoch 880, val loss: 0.8186555504798889
Epoch 890, training loss: 313.2538146972656 = 0.36341366171836853 + 50.0 * 6.257808208465576
Epoch 890, val loss: 0.8173690438270569
Epoch 900, training loss: 313.1413269042969 = 0.35485562682151794 + 50.0 * 6.2557291984558105
Epoch 900, val loss: 0.8166390657424927
Epoch 910, training loss: 313.0526123046875 = 0.3465534448623657 + 50.0 * 6.254120826721191
Epoch 910, val loss: 0.816145122051239
Epoch 920, training loss: 313.03546142578125 = 0.3384273946285248 + 50.0 * 6.253940582275391
Epoch 920, val loss: 0.8158621788024902
Epoch 930, training loss: 313.2908630371094 = 0.3304171562194824 + 50.0 * 6.259208679199219
Epoch 930, val loss: 0.8155515789985657
Epoch 940, training loss: 312.9943542480469 = 0.32241949439048767 + 50.0 * 6.253438949584961
Epoch 940, val loss: 0.8153783082962036
Epoch 950, training loss: 312.88055419921875 = 0.314635694026947 + 50.0 * 6.251318454742432
Epoch 950, val loss: 0.8155215382575989
Epoch 960, training loss: 312.8563537597656 = 0.3070237934589386 + 50.0 * 6.250986576080322
Epoch 960, val loss: 0.8155704140663147
Epoch 970, training loss: 312.996337890625 = 0.2995416224002838 + 50.0 * 6.253936290740967
Epoch 970, val loss: 0.8159608840942383
Epoch 980, training loss: 312.8583984375 = 0.2921259105205536 + 50.0 * 6.251325607299805
Epoch 980, val loss: 0.8164535164833069
Epoch 990, training loss: 313.0664978027344 = 0.28485897183418274 + 50.0 * 6.2556328773498535
Epoch 990, val loss: 0.8167225122451782
Epoch 1000, training loss: 312.78680419921875 = 0.27778634428977966 + 50.0 * 6.250180244445801
Epoch 1000, val loss: 0.8179036378860474
Epoch 1010, training loss: 312.63275146484375 = 0.2707851827144623 + 50.0 * 6.247239589691162
Epoch 1010, val loss: 0.8186699151992798
Epoch 1020, training loss: 312.61322021484375 = 0.26400578022003174 + 50.0 * 6.246984481811523
Epoch 1020, val loss: 0.819689929485321
Epoch 1030, training loss: 312.923095703125 = 0.2573864459991455 + 50.0 * 6.25331449508667
Epoch 1030, val loss: 0.8208715915679932
Epoch 1040, training loss: 312.6678771972656 = 0.2508857846260071 + 50.0 * 6.248339653015137
Epoch 1040, val loss: 0.8222492337226868
Epoch 1050, training loss: 312.58111572265625 = 0.24448621273040771 + 50.0 * 6.246732711791992
Epoch 1050, val loss: 0.823654055595398
Epoch 1060, training loss: 312.4793701171875 = 0.23833227157592773 + 50.0 * 6.244820594787598
Epoch 1060, val loss: 0.8254494667053223
Epoch 1070, training loss: 312.5994567871094 = 0.232326477766037 + 50.0 * 6.247342586517334
Epoch 1070, val loss: 0.8271675705909729
Epoch 1080, training loss: 312.46453857421875 = 0.22639545798301697 + 50.0 * 6.244762420654297
Epoch 1080, val loss: 0.8289267420768738
Epoch 1090, training loss: 312.42608642578125 = 0.22064992785453796 + 50.0 * 6.2441086769104
Epoch 1090, val loss: 0.8308752179145813
Epoch 1100, training loss: 312.3445129394531 = 0.21507802605628967 + 50.0 * 6.242588520050049
Epoch 1100, val loss: 0.8331595063209534
Epoch 1110, training loss: 312.309326171875 = 0.20971357822418213 + 50.0 * 6.241992473602295
Epoch 1110, val loss: 0.8355526924133301
Epoch 1120, training loss: 312.6534118652344 = 0.20452572405338287 + 50.0 * 6.2489776611328125
Epoch 1120, val loss: 0.8380650281906128
Epoch 1130, training loss: 312.3753967285156 = 0.1993023157119751 + 50.0 * 6.243521690368652
Epoch 1130, val loss: 0.8402799367904663
Epoch 1140, training loss: 312.21307373046875 = 0.19435742497444153 + 50.0 * 6.2403740882873535
Epoch 1140, val loss: 0.8431383967399597
Epoch 1150, training loss: 312.2020568847656 = 0.18954318761825562 + 50.0 * 6.240250110626221
Epoch 1150, val loss: 0.845824658870697
Epoch 1160, training loss: 312.3896484375 = 0.18484793603420258 + 50.0 * 6.244096279144287
Epoch 1160, val loss: 0.848393440246582
Epoch 1170, training loss: 312.1552429199219 = 0.18030010163784027 + 50.0 * 6.239498615264893
Epoch 1170, val loss: 0.8520017266273499
Epoch 1180, training loss: 312.10894775390625 = 0.17587095499038696 + 50.0 * 6.238661766052246
Epoch 1180, val loss: 0.855025053024292
Epoch 1190, training loss: 312.1384582519531 = 0.17160743474960327 + 50.0 * 6.239336967468262
Epoch 1190, val loss: 0.8583108186721802
Epoch 1200, training loss: 312.20123291015625 = 0.16741116344928741 + 50.0 * 6.240676403045654
Epoch 1200, val loss: 0.8614703416824341
Epoch 1210, training loss: 312.04241943359375 = 0.1633358895778656 + 50.0 * 6.237581729888916
Epoch 1210, val loss: 0.8647249937057495
Epoch 1220, training loss: 312.0123596191406 = 0.15940557420253754 + 50.0 * 6.237059116363525
Epoch 1220, val loss: 0.8681688904762268
Epoch 1230, training loss: 311.97802734375 = 0.15560954809188843 + 50.0 * 6.236448287963867
Epoch 1230, val loss: 0.8718874454498291
Epoch 1240, training loss: 312.16839599609375 = 0.1519385427236557 + 50.0 * 6.240329265594482
Epoch 1240, val loss: 0.8754092454910278
Epoch 1250, training loss: 311.98895263671875 = 0.1482682228088379 + 50.0 * 6.236814022064209
Epoch 1250, val loss: 0.8788784146308899
Epoch 1260, training loss: 312.0498352050781 = 0.1447618007659912 + 50.0 * 6.238101482391357
Epoch 1260, val loss: 0.8827548623085022
Epoch 1270, training loss: 311.99371337890625 = 0.14133840799331665 + 50.0 * 6.2370476722717285
Epoch 1270, val loss: 0.8864375948905945
Epoch 1280, training loss: 311.90802001953125 = 0.13800722360610962 + 50.0 * 6.235400676727295
Epoch 1280, val loss: 0.8905506134033203
Epoch 1290, training loss: 311.8512268066406 = 0.1347968727350235 + 50.0 * 6.234328746795654
Epoch 1290, val loss: 0.8943971395492554
Epoch 1300, training loss: 311.8621826171875 = 0.1316785216331482 + 50.0 * 6.234610080718994
Epoch 1300, val loss: 0.89859539270401
Epoch 1310, training loss: 311.869873046875 = 0.12861238420009613 + 50.0 * 6.234825134277344
Epoch 1310, val loss: 0.9025040864944458
Epoch 1320, training loss: 311.7766418457031 = 0.12562930583953857 + 50.0 * 6.233020305633545
Epoch 1320, val loss: 0.9065590500831604
Epoch 1330, training loss: 311.7319641113281 = 0.12276744842529297 + 50.0 * 6.232183933258057
Epoch 1330, val loss: 0.9109410643577576
Epoch 1340, training loss: 312.0108337402344 = 0.12000492960214615 + 50.0 * 6.23781681060791
Epoch 1340, val loss: 0.9153010845184326
Epoch 1350, training loss: 311.9466247558594 = 0.11718510091304779 + 50.0 * 6.236588954925537
Epoch 1350, val loss: 0.9188359975814819
Epoch 1360, training loss: 311.7372741699219 = 0.1145227774977684 + 50.0 * 6.232454776763916
Epoch 1360, val loss: 0.9236842393875122
Epoch 1370, training loss: 311.6925354003906 = 0.1119253933429718 + 50.0 * 6.231611728668213
Epoch 1370, val loss: 0.9280214905738831
Epoch 1380, training loss: 311.66241455078125 = 0.10940834134817123 + 50.0 * 6.231060028076172
Epoch 1380, val loss: 0.9322609305381775
Epoch 1390, training loss: 311.60430908203125 = 0.10696130990982056 + 50.0 * 6.229946613311768
Epoch 1390, val loss: 0.9368330836296082
Epoch 1400, training loss: 311.6885986328125 = 0.10457871109247208 + 50.0 * 6.231680393218994
Epoch 1400, val loss: 0.9412945508956909
Epoch 1410, training loss: 311.77386474609375 = 0.1022690013051033 + 50.0 * 6.233432292938232
Epoch 1410, val loss: 0.9454147815704346
Epoch 1420, training loss: 311.6110534667969 = 0.09996923059225082 + 50.0 * 6.230221271514893
Epoch 1420, val loss: 0.9501643180847168
Epoch 1430, training loss: 311.55828857421875 = 0.09775892645120621 + 50.0 * 6.22921085357666
Epoch 1430, val loss: 0.9547185897827148
Epoch 1440, training loss: 311.50408935546875 = 0.09560039639472961 + 50.0 * 6.2281694412231445
Epoch 1440, val loss: 0.959403395652771
Epoch 1450, training loss: 311.4958190917969 = 0.09353489428758621 + 50.0 * 6.22804594039917
Epoch 1450, val loss: 0.9639195203781128
Epoch 1460, training loss: 311.7674255371094 = 0.09151902049779892 + 50.0 * 6.233518123626709
Epoch 1460, val loss: 0.9682299494743347
Epoch 1470, training loss: 311.54583740234375 = 0.08946753293275833 + 50.0 * 6.229126930236816
Epoch 1470, val loss: 0.9727711081504822
Epoch 1480, training loss: 311.4254455566406 = 0.08753170818090439 + 50.0 * 6.2267584800720215
Epoch 1480, val loss: 0.97770756483078
Epoch 1490, training loss: 311.4185791015625 = 0.08564735949039459 + 50.0 * 6.226658821105957
Epoch 1490, val loss: 0.9823170900344849
Epoch 1500, training loss: 311.4759826660156 = 0.08382295817136765 + 50.0 * 6.227843284606934
Epoch 1500, val loss: 0.9869824647903442
Epoch 1510, training loss: 311.470947265625 = 0.0820310115814209 + 50.0 * 6.227778434753418
Epoch 1510, val loss: 0.9917832016944885
Epoch 1520, training loss: 311.5061340332031 = 0.0802575871348381 + 50.0 * 6.228517532348633
Epoch 1520, val loss: 0.9962126612663269
Epoch 1530, training loss: 311.36065673828125 = 0.07855618000030518 + 50.0 * 6.225642204284668
Epoch 1530, val loss: 1.0014288425445557
Epoch 1540, training loss: 311.3086242675781 = 0.0768791064620018 + 50.0 * 6.224635124206543
Epoch 1540, val loss: 1.0061722993850708
Epoch 1550, training loss: 311.4879455566406 = 0.07529929280281067 + 50.0 * 6.22825288772583
Epoch 1550, val loss: 1.011154055595398
Epoch 1560, training loss: 311.3410949707031 = 0.07368892431259155 + 50.0 * 6.225347995758057
Epoch 1560, val loss: 1.0153805017471313
Epoch 1570, training loss: 311.274658203125 = 0.07214962691068649 + 50.0 * 6.224050521850586
Epoch 1570, val loss: 1.0204745531082153
Epoch 1580, training loss: 311.36212158203125 = 0.07065737247467041 + 50.0 * 6.225829601287842
Epoch 1580, val loss: 1.0251940488815308
Epoch 1590, training loss: 311.2032775878906 = 0.06918725371360779 + 50.0 * 6.222681999206543
Epoch 1590, val loss: 1.0300979614257812
Epoch 1600, training loss: 311.239501953125 = 0.06775657087564468 + 50.0 * 6.223434925079346
Epoch 1600, val loss: 1.0348870754241943
Epoch 1610, training loss: 311.6925964355469 = 0.06638845056295395 + 50.0 * 6.232524394989014
Epoch 1610, val loss: 1.0398927927017212
Epoch 1620, training loss: 311.2750549316406 = 0.06499183177947998 + 50.0 * 6.224201202392578
Epoch 1620, val loss: 1.0440702438354492
Epoch 1630, training loss: 311.1357116699219 = 0.06366157531738281 + 50.0 * 6.22144079208374
Epoch 1630, val loss: 1.0491387844085693
Epoch 1640, training loss: 311.1094665527344 = 0.06238291412591934 + 50.0 * 6.220941543579102
Epoch 1640, val loss: 1.05415678024292
Epoch 1650, training loss: 311.1089172363281 = 0.06114734709262848 + 50.0 * 6.2209553718566895
Epoch 1650, val loss: 1.0589056015014648
Epoch 1660, training loss: 311.41400146484375 = 0.059952910989522934 + 50.0 * 6.227080821990967
Epoch 1660, val loss: 1.0637940168380737
Epoch 1670, training loss: 311.490478515625 = 0.05871986225247383 + 50.0 * 6.228635311126709
Epoch 1670, val loss: 1.0684071779251099
Epoch 1680, training loss: 311.185302734375 = 0.05755220353603363 + 50.0 * 6.222555160522461
Epoch 1680, val loss: 1.0731818675994873
Epoch 1690, training loss: 311.07666015625 = 0.056399840861558914 + 50.0 * 6.220405101776123
Epoch 1690, val loss: 1.0783028602600098
Epoch 1700, training loss: 311.02337646484375 = 0.05529152229428291 + 50.0 * 6.219361305236816
Epoch 1700, val loss: 1.0830340385437012
Epoch 1710, training loss: 311.0502014160156 = 0.05422813445329666 + 50.0 * 6.219919681549072
Epoch 1710, val loss: 1.0876506567001343
Epoch 1720, training loss: 311.31280517578125 = 0.05318709462881088 + 50.0 * 6.225192070007324
Epoch 1720, val loss: 1.0923093557357788
Epoch 1730, training loss: 311.1091613769531 = 0.05215800553560257 + 50.0 * 6.221139907836914
Epoch 1730, val loss: 1.0976953506469727
Epoch 1740, training loss: 311.02978515625 = 0.051136743277311325 + 50.0 * 6.219573020935059
Epoch 1740, val loss: 1.1024397611618042
Epoch 1750, training loss: 310.9878234863281 = 0.050158485770225525 + 50.0 * 6.218752861022949
Epoch 1750, val loss: 1.1069257259368896
Epoch 1760, training loss: 310.9413146972656 = 0.049212876707315445 + 50.0 * 6.217841625213623
Epoch 1760, val loss: 1.1121097803115845
Epoch 1770, training loss: 311.0893859863281 = 0.04830455407500267 + 50.0 * 6.220821380615234
Epoch 1770, val loss: 1.1166632175445557
Epoch 1780, training loss: 311.0436706542969 = 0.0473942905664444 + 50.0 * 6.219925403594971
Epoch 1780, val loss: 1.1214451789855957
Epoch 1790, training loss: 310.9259338378906 = 0.04648986831307411 + 50.0 * 6.217589378356934
Epoch 1790, val loss: 1.1259359121322632
Epoch 1800, training loss: 310.97802734375 = 0.045618537813425064 + 50.0 * 6.2186479568481445
Epoch 1800, val loss: 1.1309376955032349
Epoch 1810, training loss: 310.9613342285156 = 0.0447777658700943 + 50.0 * 6.218331336975098
Epoch 1810, val loss: 1.1355390548706055
Epoch 1820, training loss: 310.8868103027344 = 0.04396014288067818 + 50.0 * 6.216857433319092
Epoch 1820, val loss: 1.1401631832122803
Epoch 1830, training loss: 310.9380187988281 = 0.04316854476928711 + 50.0 * 6.217896461486816
Epoch 1830, val loss: 1.1447234153747559
Epoch 1840, training loss: 310.9089050292969 = 0.04238969460129738 + 50.0 * 6.217330455780029
Epoch 1840, val loss: 1.1497260332107544
Epoch 1850, training loss: 311.02740478515625 = 0.041622672230005264 + 50.0 * 6.219715595245361
Epoch 1850, val loss: 1.1544067859649658
Epoch 1860, training loss: 311.0275573730469 = 0.040870361030101776 + 50.0 * 6.219733715057373
Epoch 1860, val loss: 1.1588647365570068
Epoch 1870, training loss: 310.8627014160156 = 0.04013800993561745 + 50.0 * 6.216451168060303
Epoch 1870, val loss: 1.1637409925460815
Epoch 1880, training loss: 310.7916259765625 = 0.039418913424015045 + 50.0 * 6.215044021606445
Epoch 1880, val loss: 1.1681207418441772
Epoch 1890, training loss: 310.8271484375 = 0.0387338250875473 + 50.0 * 6.215768337249756
Epoch 1890, val loss: 1.1726560592651367
Epoch 1900, training loss: 310.9275817871094 = 0.038063447922468185 + 50.0 * 6.217790603637695
Epoch 1900, val loss: 1.1772061586380005
Epoch 1910, training loss: 310.80450439453125 = 0.03740683197975159 + 50.0 * 6.215342044830322
Epoch 1910, val loss: 1.1820580959320068
Epoch 1920, training loss: 310.8876647949219 = 0.036766428500413895 + 50.0 * 6.217017650604248
Epoch 1920, val loss: 1.1864244937896729
Epoch 1930, training loss: 310.774658203125 = 0.03612472862005234 + 50.0 * 6.214770793914795
Epoch 1930, val loss: 1.1909922361373901
Epoch 1940, training loss: 310.8114318847656 = 0.03551151603460312 + 50.0 * 6.215518474578857
Epoch 1940, val loss: 1.1951031684875488
Epoch 1950, training loss: 311.0045166015625 = 0.03491739556193352 + 50.0 * 6.219391822814941
Epoch 1950, val loss: 1.19973623752594
Epoch 1960, training loss: 310.7492980957031 = 0.03430965915322304 + 50.0 * 6.21429967880249
Epoch 1960, val loss: 1.2041523456573486
Epoch 1970, training loss: 310.68377685546875 = 0.03374162316322327 + 50.0 * 6.213000774383545
Epoch 1970, val loss: 1.2088031768798828
Epoch 1980, training loss: 310.6738586425781 = 0.033187832683324814 + 50.0 * 6.212813854217529
Epoch 1980, val loss: 1.213154673576355
Epoch 1990, training loss: 310.7853698730469 = 0.03265269473195076 + 50.0 * 6.215054035186768
Epoch 1990, val loss: 1.2178566455841064
Epoch 2000, training loss: 310.7422180175781 = 0.03211734816431999 + 50.0 * 6.214202404022217
Epoch 2000, val loss: 1.2218565940856934
Epoch 2010, training loss: 310.6989440917969 = 0.0315849669277668 + 50.0 * 6.2133469581604
Epoch 2010, val loss: 1.2263338565826416
Epoch 2020, training loss: 310.72015380859375 = 0.03106033056974411 + 50.0 * 6.213781833648682
Epoch 2020, val loss: 1.2300758361816406
Epoch 2030, training loss: 310.6498107910156 = 0.03056606650352478 + 50.0 * 6.2123847007751465
Epoch 2030, val loss: 1.2347698211669922
Epoch 2040, training loss: 310.77313232421875 = 0.030083736404776573 + 50.0 * 6.214860916137695
Epoch 2040, val loss: 1.2386759519577026
Epoch 2050, training loss: 310.7626953125 = 0.029602134600281715 + 50.0 * 6.214662075042725
Epoch 2050, val loss: 1.2426836490631104
Epoch 2060, training loss: 310.64190673828125 = 0.029129434376955032 + 50.0 * 6.212255001068115
Epoch 2060, val loss: 1.2472898960113525
Epoch 2070, training loss: 310.6001892089844 = 0.0286684799939394 + 50.0 * 6.211430549621582
Epoch 2070, val loss: 1.251346230506897
Epoch 2080, training loss: 310.9481201171875 = 0.028227651491761208 + 50.0 * 6.218398094177246
Epoch 2080, val loss: 1.2555524110794067
Epoch 2090, training loss: 310.6624450683594 = 0.027793362736701965 + 50.0 * 6.212692737579346
Epoch 2090, val loss: 1.2597088813781738
Epoch 2100, training loss: 310.5812072753906 = 0.027354560792446136 + 50.0 * 6.2110772132873535
Epoch 2100, val loss: 1.2637383937835693
Epoch 2110, training loss: 310.56292724609375 = 0.02694571018218994 + 50.0 * 6.210719585418701
Epoch 2110, val loss: 1.2679263353347778
Epoch 2120, training loss: 310.96844482421875 = 0.026553601026535034 + 50.0 * 6.218837738037109
Epoch 2120, val loss: 1.27159583568573
Epoch 2130, training loss: 310.6426086425781 = 0.026139846071600914 + 50.0 * 6.212329387664795
Epoch 2130, val loss: 1.2760288715362549
Epoch 2140, training loss: 310.6094055175781 = 0.025749608874320984 + 50.0 * 6.211673259735107
Epoch 2140, val loss: 1.2800475358963013
Epoch 2150, training loss: 310.65069580078125 = 0.025369372218847275 + 50.0 * 6.2125067710876465
Epoch 2150, val loss: 1.2839387655258179
Epoch 2160, training loss: 310.5443115234375 = 0.024981437250971794 + 50.0 * 6.210386276245117
Epoch 2160, val loss: 1.2877472639083862
Epoch 2170, training loss: 310.61029052734375 = 0.024617385119199753 + 50.0 * 6.2117133140563965
Epoch 2170, val loss: 1.29205322265625
Epoch 2180, training loss: 310.7037658691406 = 0.024275116622447968 + 50.0 * 6.213590145111084
Epoch 2180, val loss: 1.2961173057556152
Epoch 2190, training loss: 310.5473327636719 = 0.023901693522930145 + 50.0 * 6.210468769073486
Epoch 2190, val loss: 1.2993555068969727
Epoch 2200, training loss: 310.4739990234375 = 0.02355927601456642 + 50.0 * 6.209008693695068
Epoch 2200, val loss: 1.3036924600601196
Epoch 2210, training loss: 310.4627685546875 = 0.023227840662002563 + 50.0 * 6.2087907791137695
Epoch 2210, val loss: 1.3074324131011963
Epoch 2220, training loss: 310.5406494140625 = 0.02290487103164196 + 50.0 * 6.210354804992676
Epoch 2220, val loss: 1.3113865852355957
Epoch 2230, training loss: 310.6542663574219 = 0.0225821603089571 + 50.0 * 6.2126336097717285
Epoch 2230, val loss: 1.3152366876602173
Epoch 2240, training loss: 310.6081848144531 = 0.022257810458540916 + 50.0 * 6.211718559265137
Epoch 2240, val loss: 1.3187036514282227
Epoch 2250, training loss: 310.4655456542969 = 0.02194192260503769 + 50.0 * 6.208871841430664
Epoch 2250, val loss: 1.3223717212677002
Epoch 2260, training loss: 310.48004150390625 = 0.02163662388920784 + 50.0 * 6.209167957305908
Epoch 2260, val loss: 1.326477289199829
Epoch 2270, training loss: 310.53228759765625 = 0.021341867744922638 + 50.0 * 6.210218906402588
Epoch 2270, val loss: 1.3299657106399536
Epoch 2280, training loss: 310.42315673828125 = 0.021053725853562355 + 50.0 * 6.208042144775391
Epoch 2280, val loss: 1.3337798118591309
Epoch 2290, training loss: 310.54443359375 = 0.020775457844138145 + 50.0 * 6.21047306060791
Epoch 2290, val loss: 1.337472915649414
Epoch 2300, training loss: 310.5094299316406 = 0.020486634224653244 + 50.0 * 6.209779262542725
Epoch 2300, val loss: 1.3408323526382446
Epoch 2310, training loss: 310.4261779785156 = 0.0202114786952734 + 50.0 * 6.2081193923950195
Epoch 2310, val loss: 1.344396948814392
Epoch 2320, training loss: 310.4061279296875 = 0.019946057349443436 + 50.0 * 6.207723617553711
Epoch 2320, val loss: 1.3481814861297607
Epoch 2330, training loss: 310.55352783203125 = 0.01968914084136486 + 50.0 * 6.210677146911621
Epoch 2330, val loss: 1.3512811660766602
Epoch 2340, training loss: 310.5338439941406 = 0.019429875537753105 + 50.0 * 6.210288047790527
Epoch 2340, val loss: 1.3554950952529907
Epoch 2350, training loss: 310.4201965332031 = 0.01917201280593872 + 50.0 * 6.2080206871032715
Epoch 2350, val loss: 1.358679175376892
Epoch 2360, training loss: 310.3822326660156 = 0.018913213163614273 + 50.0 * 6.207266330718994
Epoch 2360, val loss: 1.3621217012405396
Epoch 2370, training loss: 310.46905517578125 = 0.018675876781344414 + 50.0 * 6.209007263183594
Epoch 2370, val loss: 1.3657476902008057
Epoch 2380, training loss: 310.3643798828125 = 0.01844174601137638 + 50.0 * 6.206918716430664
Epoch 2380, val loss: 1.3689746856689453
Epoch 2390, training loss: 310.328857421875 = 0.01820402592420578 + 50.0 * 6.206212997436523
Epoch 2390, val loss: 1.3726121187210083
Epoch 2400, training loss: 310.3664245605469 = 0.017979055643081665 + 50.0 * 6.206969261169434
Epoch 2400, val loss: 1.376047134399414
Epoch 2410, training loss: 310.4779357910156 = 0.017759984359145164 + 50.0 * 6.209203720092773
Epoch 2410, val loss: 1.3795533180236816
Epoch 2420, training loss: 310.4677429199219 = 0.01752791367471218 + 50.0 * 6.2090044021606445
Epoch 2420, val loss: 1.3828009366989136
Epoch 2430, training loss: 310.30029296875 = 0.017305223271250725 + 50.0 * 6.205659866333008
Epoch 2430, val loss: 1.3859574794769287
Epoch 2440, training loss: 310.30126953125 = 0.017094995826482773 + 50.0 * 6.205683708190918
Epoch 2440, val loss: 1.3893972635269165
Epoch 2450, training loss: 310.6892395019531 = 0.01689804159104824 + 50.0 * 6.213446617126465
Epoch 2450, val loss: 1.3928639888763428
Epoch 2460, training loss: 310.3617858886719 = 0.016678158193826675 + 50.0 * 6.206902027130127
Epoch 2460, val loss: 1.395606279373169
Epoch 2470, training loss: 310.2710266113281 = 0.016477268189191818 + 50.0 * 6.2050909996032715
Epoch 2470, val loss: 1.3993024826049805
Epoch 2480, training loss: 310.2642822265625 = 0.016280941665172577 + 50.0 * 6.204960346221924
Epoch 2480, val loss: 1.4023561477661133
Epoch 2490, training loss: 310.4346923828125 = 0.01609428972005844 + 50.0 * 6.208372116088867
Epoch 2490, val loss: 1.4054991006851196
Epoch 2500, training loss: 310.2613525390625 = 0.015902947634458542 + 50.0 * 6.204909324645996
Epoch 2500, val loss: 1.4090416431427002
Epoch 2510, training loss: 310.40655517578125 = 0.015721069648861885 + 50.0 * 6.2078166007995605
Epoch 2510, val loss: 1.412016749382019
Epoch 2520, training loss: 310.2698059082031 = 0.015532394871115685 + 50.0 * 6.205085277557373
Epoch 2520, val loss: 1.4151450395584106
Epoch 2530, training loss: 310.2889099121094 = 0.0153494942933321 + 50.0 * 6.205471038818359
Epoch 2530, val loss: 1.4179377555847168
Epoch 2540, training loss: 310.2486877441406 = 0.015171102248132229 + 50.0 * 6.204670429229736
Epoch 2540, val loss: 1.421169638633728
Epoch 2550, training loss: 310.3832702636719 = 0.014996830374002457 + 50.0 * 6.207365989685059
Epoch 2550, val loss: 1.4241206645965576
Epoch 2560, training loss: 310.2220458984375 = 0.01482931524515152 + 50.0 * 6.20414400100708
Epoch 2560, val loss: 1.427344799041748
Epoch 2570, training loss: 310.2342529296875 = 0.014665132388472557 + 50.0 * 6.2043914794921875
Epoch 2570, val loss: 1.4305845499038696
Epoch 2580, training loss: 310.3192138671875 = 0.014503116719424725 + 50.0 * 6.206094264984131
Epoch 2580, val loss: 1.4335132837295532
Epoch 2590, training loss: 310.25518798828125 = 0.014337241649627686 + 50.0 * 6.204816818237305
Epoch 2590, val loss: 1.4365841150283813
Epoch 2600, training loss: 310.259033203125 = 0.014176150783896446 + 50.0 * 6.204896926879883
Epoch 2600, val loss: 1.4393551349639893
Epoch 2610, training loss: 310.189208984375 = 0.0140205267816782 + 50.0 * 6.2035040855407715
Epoch 2610, val loss: 1.4423561096191406
Epoch 2620, training loss: 310.3338928222656 = 0.013867502100765705 + 50.0 * 6.2064008712768555
Epoch 2620, val loss: 1.4452590942382812
Epoch 2630, training loss: 310.2121887207031 = 0.013711829669773579 + 50.0 * 6.203969478607178
Epoch 2630, val loss: 1.4480981826782227
Epoch 2640, training loss: 310.3382873535156 = 0.013568907976150513 + 50.0 * 6.206494331359863
Epoch 2640, val loss: 1.4513535499572754
Epoch 2650, training loss: 310.26068115234375 = 0.013421621173620224 + 50.0 * 6.204945087432861
Epoch 2650, val loss: 1.4537713527679443
Epoch 2660, training loss: 310.1579284667969 = 0.013271941803395748 + 50.0 * 6.202893257141113
Epoch 2660, val loss: 1.4563366174697876
Epoch 2670, training loss: 310.1224670410156 = 0.013130121864378452 + 50.0 * 6.2021870613098145
Epoch 2670, val loss: 1.4595130681991577
Epoch 2680, training loss: 310.1094055175781 = 0.012996306642889977 + 50.0 * 6.20192813873291
Epoch 2680, val loss: 1.462216854095459
Epoch 2690, training loss: 310.1884460449219 = 0.012866835109889507 + 50.0 * 6.2035112380981445
Epoch 2690, val loss: 1.4649344682693481
Epoch 2700, training loss: 310.3671875 = 0.012733259238302708 + 50.0 * 6.207089424133301
Epoch 2700, val loss: 1.4675683975219727
Epoch 2710, training loss: 310.24530029296875 = 0.01259703654795885 + 50.0 * 6.204653739929199
Epoch 2710, val loss: 1.4705854654312134
Epoch 2720, training loss: 310.09234619140625 = 0.012462323531508446 + 50.0 * 6.201598167419434
Epoch 2720, val loss: 1.4729703664779663
Epoch 2730, training loss: 310.07354736328125 = 0.012333330698311329 + 50.0 * 6.201224327087402
Epoch 2730, val loss: 1.4757747650146484
Epoch 2740, training loss: 310.0933837890625 = 0.012212402187287807 + 50.0 * 6.201623439788818
Epoch 2740, val loss: 1.4784293174743652
Epoch 2750, training loss: 310.587890625 = 0.01209927536547184 + 50.0 * 6.2115159034729
Epoch 2750, val loss: 1.4810404777526855
Epoch 2760, training loss: 310.1799621582031 = 0.01196768507361412 + 50.0 * 6.203360080718994
Epoch 2760, val loss: 1.4835515022277832
Epoch 2770, training loss: 310.0670471191406 = 0.01184383500367403 + 50.0 * 6.201104164123535
Epoch 2770, val loss: 1.4864912033081055
Epoch 2780, training loss: 310.0751953125 = 0.011731225065886974 + 50.0 * 6.201269626617432
Epoch 2780, val loss: 1.488991379737854
Epoch 2790, training loss: 310.3677978515625 = 0.011628069914877415 + 50.0 * 6.207123279571533
Epoch 2790, val loss: 1.491637110710144
Epoch 2800, training loss: 310.1260681152344 = 0.011498229578137398 + 50.0 * 6.202291011810303
Epoch 2800, val loss: 1.4942498207092285
Epoch 2810, training loss: 310.0477600097656 = 0.011389007791876793 + 50.0 * 6.200727462768555
Epoch 2810, val loss: 1.4966791868209839
Epoch 2820, training loss: 310.2158203125 = 0.011280018836259842 + 50.0 * 6.204090595245361
Epoch 2820, val loss: 1.4994655847549438
Epoch 2830, training loss: 310.0450134277344 = 0.011171907186508179 + 50.0 * 6.200676918029785
Epoch 2830, val loss: 1.5015138387680054
Epoch 2840, training loss: 310.062744140625 = 0.011065016500651836 + 50.0 * 6.201033115386963
Epoch 2840, val loss: 1.504112720489502
Epoch 2850, training loss: 310.0419616699219 = 0.010956667363643646 + 50.0 * 6.200620174407959
Epoch 2850, val loss: 1.5064356327056885
Epoch 2860, training loss: 310.1259765625 = 0.010858282446861267 + 50.0 * 6.202301979064941
Epoch 2860, val loss: 1.5085417032241821
Epoch 2870, training loss: 310.0541076660156 = 0.010755863972008228 + 50.0 * 6.200867176055908
Epoch 2870, val loss: 1.5115046501159668
Epoch 2880, training loss: 310.1314697265625 = 0.010661927983164787 + 50.0 * 6.20241641998291
Epoch 2880, val loss: 1.514190435409546
Epoch 2890, training loss: 310.0556335449219 = 0.010558086447417736 + 50.0 * 6.200901508331299
Epoch 2890, val loss: 1.5161423683166504
Epoch 2900, training loss: 310.0401306152344 = 0.01045930664986372 + 50.0 * 6.2005934715271
Epoch 2900, val loss: 1.5187658071517944
Epoch 2910, training loss: 310.1402587890625 = 0.010369877330958843 + 50.0 * 6.202597618103027
Epoch 2910, val loss: 1.5211451053619385
Epoch 2920, training loss: 309.9931945800781 = 0.010270721279084682 + 50.0 * 6.199658393859863
Epoch 2920, val loss: 1.5233839750289917
Epoch 2930, training loss: 310.01483154296875 = 0.010180598124861717 + 50.0 * 6.200092792510986
Epoch 2930, val loss: 1.5254945755004883
Epoch 2940, training loss: 310.1105041503906 = 0.010092358104884624 + 50.0 * 6.202008247375488
Epoch 2940, val loss: 1.528085470199585
Epoch 2950, training loss: 310.1677551269531 = 0.01000308245420456 + 50.0 * 6.203155040740967
Epoch 2950, val loss: 1.530186653137207
Epoch 2960, training loss: 309.97283935546875 = 0.009906020946800709 + 50.0 * 6.199258327484131
Epoch 2960, val loss: 1.532500147819519
Epoch 2970, training loss: 309.9328918457031 = 0.009819034487009048 + 50.0 * 6.198461532592773
Epoch 2970, val loss: 1.534759521484375
Epoch 2980, training loss: 309.9219055175781 = 0.009736099280416965 + 50.0 * 6.198243618011475
Epoch 2980, val loss: 1.5373531579971313
Epoch 2990, training loss: 310.1385192871094 = 0.009661120362579823 + 50.0 * 6.202576637268066
Epoch 2990, val loss: 1.5397902727127075
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8371112282551397
The final CL Acc:0.74815, 0.00524, The final GNN Acc:0.83869, 0.00129
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11672])
remove edge: torch.Size([2, 9492])
updated graph: torch.Size([2, 10608])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7821960449219 = 1.9427244663238525 + 50.0 * 8.596789360046387
Epoch 0, val loss: 1.9445871114730835
Epoch 10, training loss: 431.7174987792969 = 1.9343926906585693 + 50.0 * 8.595662117004395
Epoch 10, val loss: 1.9365549087524414
Epoch 20, training loss: 431.3192443847656 = 1.9237949848175049 + 50.0 * 8.587908744812012
Epoch 20, val loss: 1.9261318445205688
Epoch 30, training loss: 428.75177001953125 = 1.9101781845092773 + 50.0 * 8.536831855773926
Epoch 30, val loss: 1.912886381149292
Epoch 40, training loss: 413.8590393066406 = 1.893999457359314 + 50.0 * 8.239300727844238
Epoch 40, val loss: 1.89741051197052
Epoch 50, training loss: 381.6086730957031 = 1.8746395111083984 + 50.0 * 7.5946807861328125
Epoch 50, val loss: 1.8792102336883545
Epoch 60, training loss: 364.49932861328125 = 1.8615096807479858 + 50.0 * 7.252756595611572
Epoch 60, val loss: 1.8663145303726196
Epoch 70, training loss: 351.6845397949219 = 1.8513432741165161 + 50.0 * 6.996664047241211
Epoch 70, val loss: 1.8555548191070557
Epoch 80, training loss: 344.9810485839844 = 1.8417463302612305 + 50.0 * 6.862786293029785
Epoch 80, val loss: 1.845610499382019
Epoch 90, training loss: 340.7576599121094 = 1.832473874092102 + 50.0 * 6.77850341796875
Epoch 90, val loss: 1.8363105058670044
Epoch 100, training loss: 337.4973449707031 = 1.8244102001190186 + 50.0 * 6.713459014892578
Epoch 100, val loss: 1.8282828330993652
Epoch 110, training loss: 334.98388671875 = 1.8176547288894653 + 50.0 * 6.663324356079102
Epoch 110, val loss: 1.8212841749191284
Epoch 120, training loss: 332.80828857421875 = 1.8116552829742432 + 50.0 * 6.619932651519775
Epoch 120, val loss: 1.8149175643920898
Epoch 130, training loss: 330.87298583984375 = 1.8060345649719238 + 50.0 * 6.581338882446289
Epoch 130, val loss: 1.8090945482254028
Epoch 140, training loss: 329.2073669433594 = 1.800657868385315 + 50.0 * 6.5481343269348145
Epoch 140, val loss: 1.803675651550293
Epoch 150, training loss: 327.9132995605469 = 1.7954332828521729 + 50.0 * 6.522356986999512
Epoch 150, val loss: 1.7985724210739136
Epoch 160, training loss: 326.77447509765625 = 1.7900713682174683 + 50.0 * 6.499688148498535
Epoch 160, val loss: 1.7932795286178589
Epoch 170, training loss: 325.7934875488281 = 1.7843862771987915 + 50.0 * 6.480181694030762
Epoch 170, val loss: 1.7879523038864136
Epoch 180, training loss: 324.9664611816406 = 1.7784943580627441 + 50.0 * 6.463758945465088
Epoch 180, val loss: 1.782523274421692
Epoch 190, training loss: 324.20208740234375 = 1.7722543478012085 + 50.0 * 6.448596954345703
Epoch 190, val loss: 1.7768243551254272
Epoch 200, training loss: 323.692626953125 = 1.7656103372573853 + 50.0 * 6.438539981842041
Epoch 200, val loss: 1.7708728313446045
Epoch 210, training loss: 323.00421142578125 = 1.7582663297653198 + 50.0 * 6.4249186515808105
Epoch 210, val loss: 1.7645541429519653
Epoch 220, training loss: 322.49383544921875 = 1.7503833770751953 + 50.0 * 6.4148688316345215
Epoch 220, val loss: 1.7578068971633911
Epoch 230, training loss: 322.0772705078125 = 1.7418292760849 + 50.0 * 6.40670919418335
Epoch 230, val loss: 1.7505137920379639
Epoch 240, training loss: 321.6116027832031 = 1.7325353622436523 + 50.0 * 6.397581100463867
Epoch 240, val loss: 1.7426618337631226
Epoch 250, training loss: 321.2218322753906 = 1.7225123643875122 + 50.0 * 6.389986515045166
Epoch 250, val loss: 1.7342182397842407
Epoch 260, training loss: 321.0353088378906 = 1.7116491794586182 + 50.0 * 6.386473178863525
Epoch 260, val loss: 1.725016474723816
Epoch 270, training loss: 320.5900573730469 = 1.6997153759002686 + 50.0 * 6.377807140350342
Epoch 270, val loss: 1.7151012420654297
Epoch 280, training loss: 320.2254333496094 = 1.6869885921478271 + 50.0 * 6.370769023895264
Epoch 280, val loss: 1.7045354843139648
Epoch 290, training loss: 319.9436340332031 = 1.67333984375 + 50.0 * 6.365406036376953
Epoch 290, val loss: 1.6932063102722168
Epoch 300, training loss: 319.78338623046875 = 1.6586356163024902 + 50.0 * 6.362494945526123
Epoch 300, val loss: 1.6810636520385742
Epoch 310, training loss: 319.5030212402344 = 1.643086552619934 + 50.0 * 6.357198238372803
Epoch 310, val loss: 1.668028712272644
Epoch 320, training loss: 319.1717224121094 = 1.6264913082122803 + 50.0 * 6.35090446472168
Epoch 320, val loss: 1.6543220281600952
Epoch 330, training loss: 318.93316650390625 = 1.6090750694274902 + 50.0 * 6.346481800079346
Epoch 330, val loss: 1.6399081945419312
Epoch 340, training loss: 318.999755859375 = 1.5908440351486206 + 50.0 * 6.348178386688232
Epoch 340, val loss: 1.6248362064361572
Epoch 350, training loss: 318.5887145996094 = 1.5718109607696533 + 50.0 * 6.340338230133057
Epoch 350, val loss: 1.6091110706329346
Epoch 360, training loss: 318.3405456542969 = 1.551977515220642 + 50.0 * 6.335771560668945
Epoch 360, val loss: 1.5929867029190063
Epoch 370, training loss: 318.1111145019531 = 1.5316591262817383 + 50.0 * 6.331589221954346
Epoch 370, val loss: 1.5764613151550293
Epoch 380, training loss: 317.914794921875 = 1.5109243392944336 + 50.0 * 6.32807731628418
Epoch 380, val loss: 1.5597000122070312
Epoch 390, training loss: 317.8396911621094 = 1.489778757095337 + 50.0 * 6.326998233795166
Epoch 390, val loss: 1.5427237749099731
Epoch 400, training loss: 317.626953125 = 1.4682295322418213 + 50.0 * 6.323174476623535
Epoch 400, val loss: 1.5256175994873047
Epoch 410, training loss: 317.6982116699219 = 1.4465994834899902 + 50.0 * 6.3250322341918945
Epoch 410, val loss: 1.5081769227981567
Epoch 420, training loss: 317.2325439453125 = 1.4243602752685547 + 50.0 * 6.316164016723633
Epoch 420, val loss: 1.4909101724624634
Epoch 430, training loss: 317.0704345703125 = 1.4022890329360962 + 50.0 * 6.313363075256348
Epoch 430, val loss: 1.4738116264343262
Epoch 440, training loss: 316.90673828125 = 1.3802921772003174 + 50.0 * 6.3105292320251465
Epoch 440, val loss: 1.4569456577301025
Epoch 450, training loss: 316.7544250488281 = 1.3582814931869507 + 50.0 * 6.307922840118408
Epoch 450, val loss: 1.4402390718460083
Epoch 460, training loss: 317.17919921875 = 1.3361376523971558 + 50.0 * 6.316860675811768
Epoch 460, val loss: 1.423717737197876
Epoch 470, training loss: 316.5882873535156 = 1.3139339685440063 + 50.0 * 6.305487155914307
Epoch 470, val loss: 1.4071998596191406
Epoch 480, training loss: 316.3894348144531 = 1.291891098022461 + 50.0 * 6.301950454711914
Epoch 480, val loss: 1.391029715538025
Epoch 490, training loss: 316.26666259765625 = 1.270076870918274 + 50.0 * 6.299931526184082
Epoch 490, val loss: 1.3753390312194824
Epoch 500, training loss: 316.2001953125 = 1.248339056968689 + 50.0 * 6.299036979675293
Epoch 500, val loss: 1.3599298000335693
Epoch 510, training loss: 316.01226806640625 = 1.226851463317871 + 50.0 * 6.295708656311035
Epoch 510, val loss: 1.3447414636611938
Epoch 520, training loss: 316.0743713378906 = 1.2055292129516602 + 50.0 * 6.29737663269043
Epoch 520, val loss: 1.330073595046997
Epoch 530, training loss: 315.86932373046875 = 1.1843678951263428 + 50.0 * 6.293699264526367
Epoch 530, val loss: 1.3157436847686768
Epoch 540, training loss: 315.67578125 = 1.163515567779541 + 50.0 * 6.290245056152344
Epoch 540, val loss: 1.3017284870147705
Epoch 550, training loss: 315.5744323730469 = 1.1430208683013916 + 50.0 * 6.288628101348877
Epoch 550, val loss: 1.2883834838867188
Epoch 560, training loss: 315.69403076171875 = 1.1226140260696411 + 50.0 * 6.291428089141846
Epoch 560, val loss: 1.2752630710601807
Epoch 570, training loss: 315.47900390625 = 1.1026721000671387 + 50.0 * 6.287527084350586
Epoch 570, val loss: 1.2624545097351074
Epoch 580, training loss: 315.298828125 = 1.0830639600753784 + 50.0 * 6.28431510925293
Epoch 580, val loss: 1.2503008842468262
Epoch 590, training loss: 315.1966552734375 = 1.0639821290969849 + 50.0 * 6.282653331756592
Epoch 590, val loss: 1.2387645244598389
Epoch 600, training loss: 315.2554626464844 = 1.0451852083206177 + 50.0 * 6.284205436706543
Epoch 600, val loss: 1.2277441024780273
Epoch 610, training loss: 315.15106201171875 = 1.026758074760437 + 50.0 * 6.2824859619140625
Epoch 610, val loss: 1.216889500617981
Epoch 620, training loss: 315.0166015625 = 1.0084912776947021 + 50.0 * 6.280162334442139
Epoch 620, val loss: 1.2064704895019531
Epoch 630, training loss: 314.9061584472656 = 0.9907888770103455 + 50.0 * 6.2783074378967285
Epoch 630, val loss: 1.1967087984085083
Epoch 640, training loss: 314.793212890625 = 0.9734825491905212 + 50.0 * 6.276394367218018
Epoch 640, val loss: 1.1873277425765991
Epoch 650, training loss: 314.8779602050781 = 0.9565571546554565 + 50.0 * 6.278428554534912
Epoch 650, val loss: 1.1783117055892944
Epoch 660, training loss: 314.76800537109375 = 0.9395612478256226 + 50.0 * 6.27656888961792
Epoch 660, val loss: 1.1697142124176025
Epoch 670, training loss: 314.628173828125 = 0.9231446981430054 + 50.0 * 6.274100303649902
Epoch 670, val loss: 1.1615674495697021
Epoch 680, training loss: 314.5899963378906 = 0.9070588946342468 + 50.0 * 6.273658752441406
Epoch 680, val loss: 1.1537995338439941
Epoch 690, training loss: 314.4151611328125 = 0.8910820484161377 + 50.0 * 6.270481586456299
Epoch 690, val loss: 1.146526575088501
Epoch 700, training loss: 314.3819274902344 = 0.8755066394805908 + 50.0 * 6.27012825012207
Epoch 700, val loss: 1.139622449874878
Epoch 710, training loss: 314.4279479980469 = 0.8600913882255554 + 50.0 * 6.271357536315918
Epoch 710, val loss: 1.1330970525741577
Epoch 720, training loss: 314.39410400390625 = 0.8450021743774414 + 50.0 * 6.270982265472412
Epoch 720, val loss: 1.1263303756713867
Epoch 730, training loss: 314.1927185058594 = 0.8297878503799438 + 50.0 * 6.267259120941162
Epoch 730, val loss: 1.1203173398971558
Epoch 740, training loss: 314.08819580078125 = 0.8151044249534607 + 50.0 * 6.2654619216918945
Epoch 740, val loss: 1.1147661209106445
Epoch 750, training loss: 314.19354248046875 = 0.8005728125572205 + 50.0 * 6.26785945892334
Epoch 750, val loss: 1.1094679832458496
Epoch 760, training loss: 314.0193786621094 = 0.7860973477363586 + 50.0 * 6.264665603637695
Epoch 760, val loss: 1.1045942306518555
Epoch 770, training loss: 313.99249267578125 = 0.771784782409668 + 50.0 * 6.264413833618164
Epoch 770, val loss: 1.099872350692749
Epoch 780, training loss: 313.8609313964844 = 0.7576107978820801 + 50.0 * 6.26206636428833
Epoch 780, val loss: 1.0954357385635376
Epoch 790, training loss: 313.8299865722656 = 0.7435200810432434 + 50.0 * 6.2617292404174805
Epoch 790, val loss: 1.0914921760559082
Epoch 800, training loss: 313.7458801269531 = 0.7295919060707092 + 50.0 * 6.260325908660889
Epoch 800, val loss: 1.0877172946929932
Epoch 810, training loss: 313.87811279296875 = 0.7156074047088623 + 50.0 * 6.26324987411499
Epoch 810, val loss: 1.0843408107757568
Epoch 820, training loss: 313.6161193847656 = 0.7019162178039551 + 50.0 * 6.258284568786621
Epoch 820, val loss: 1.0809590816497803
Epoch 830, training loss: 313.5371398925781 = 0.6882886290550232 + 50.0 * 6.256977081298828
Epoch 830, val loss: 1.0783214569091797
Epoch 840, training loss: 313.4802551269531 = 0.6748849153518677 + 50.0 * 6.256107330322266
Epoch 840, val loss: 1.0760400295257568
Epoch 850, training loss: 313.7772216796875 = 0.6616378426551819 + 50.0 * 6.2623114585876465
Epoch 850, val loss: 1.073898434638977
Epoch 860, training loss: 313.4668884277344 = 0.6482401490211487 + 50.0 * 6.256372928619385
Epoch 860, val loss: 1.0718170404434204
Epoch 870, training loss: 313.3572998046875 = 0.6351817846298218 + 50.0 * 6.25444221496582
Epoch 870, val loss: 1.0702285766601562
Epoch 880, training loss: 313.4326171875 = 0.6222484111785889 + 50.0 * 6.256207466125488
Epoch 880, val loss: 1.0689538717269897
Epoch 890, training loss: 313.2614440917969 = 0.6092506051063538 + 50.0 * 6.2530436515808105
Epoch 890, val loss: 1.0677458047866821
Epoch 900, training loss: 313.1786193847656 = 0.5967044234275818 + 50.0 * 6.251638412475586
Epoch 900, val loss: 1.0670851469039917
Epoch 910, training loss: 313.33563232421875 = 0.5842342376708984 + 50.0 * 6.255028247833252
Epoch 910, val loss: 1.0664927959442139
Epoch 920, training loss: 313.19891357421875 = 0.5719887614250183 + 50.0 * 6.252538681030273
Epoch 920, val loss: 1.0658930540084839
Epoch 930, training loss: 313.1600341796875 = 0.5596898198127747 + 50.0 * 6.252007007598877
Epoch 930, val loss: 1.0655646324157715
Epoch 940, training loss: 313.0510559082031 = 0.5478209853172302 + 50.0 * 6.250064849853516
Epoch 940, val loss: 1.0653893947601318
Epoch 950, training loss: 313.01995849609375 = 0.5361392498016357 + 50.0 * 6.249676704406738
Epoch 950, val loss: 1.065522313117981
Epoch 960, training loss: 312.8741455078125 = 0.5246748328208923 + 50.0 * 6.2469892501831055
Epoch 960, val loss: 1.0658453702926636
Epoch 970, training loss: 312.88250732421875 = 0.5134761333465576 + 50.0 * 6.247380256652832
Epoch 970, val loss: 1.0665221214294434
Epoch 980, training loss: 313.19708251953125 = 0.502642810344696 + 50.0 * 6.2538886070251465
Epoch 980, val loss: 1.0672739744186401
Epoch 990, training loss: 312.9394836425781 = 0.49141964316368103 + 50.0 * 6.248961448669434
Epoch 990, val loss: 1.06809401512146
Epoch 1000, training loss: 312.8365173339844 = 0.4809354245662689 + 50.0 * 6.247111797332764
Epoch 1000, val loss: 1.0691888332366943
Epoch 1010, training loss: 312.65850830078125 = 0.47051241993904114 + 50.0 * 6.243759632110596
Epoch 1010, val loss: 1.0707067251205444
Epoch 1020, training loss: 312.6417236328125 = 0.46051347255706787 + 50.0 * 6.243624210357666
Epoch 1020, val loss: 1.0724358558654785
Epoch 1030, training loss: 312.80572509765625 = 0.450699120759964 + 50.0 * 6.247100353240967
Epoch 1030, val loss: 1.0741291046142578
Epoch 1040, training loss: 312.6991882324219 = 0.44088420271873474 + 50.0 * 6.245166301727295
Epoch 1040, val loss: 1.075868010520935
Epoch 1050, training loss: 312.52935791015625 = 0.4314797520637512 + 50.0 * 6.241957187652588
Epoch 1050, val loss: 1.0778690576553345
Epoch 1060, training loss: 312.4828186035156 = 0.42227014899253845 + 50.0 * 6.2412109375
Epoch 1060, val loss: 1.0804040431976318
Epoch 1070, training loss: 312.9579162597656 = 0.4133462607860565 + 50.0 * 6.250891208648682
Epoch 1070, val loss: 1.082861304283142
Epoch 1080, training loss: 312.5372314453125 = 0.40456417202949524 + 50.0 * 6.2426533699035645
Epoch 1080, val loss: 1.0853582620620728
Epoch 1090, training loss: 312.35626220703125 = 0.395959734916687 + 50.0 * 6.239205837249756
Epoch 1090, val loss: 1.088220238685608
Epoch 1100, training loss: 312.3355712890625 = 0.38772404193878174 + 50.0 * 6.238956928253174
Epoch 1100, val loss: 1.0913845300674438
Epoch 1110, training loss: 312.53369140625 = 0.37970495223999023 + 50.0 * 6.243080139160156
Epoch 1110, val loss: 1.0946390628814697
Epoch 1120, training loss: 312.3481140136719 = 0.37168365716934204 + 50.0 * 6.239528656005859
Epoch 1120, val loss: 1.0978227853775024
Epoch 1130, training loss: 312.26336669921875 = 0.3639660179615021 + 50.0 * 6.237988471984863
Epoch 1130, val loss: 1.1011916399002075
Epoch 1140, training loss: 312.30047607421875 = 0.35645154118537903 + 50.0 * 6.238880634307861
Epoch 1140, val loss: 1.1048004627227783
Epoch 1150, training loss: 312.1921081542969 = 0.34906214475631714 + 50.0 * 6.236861228942871
Epoch 1150, val loss: 1.1083290576934814
Epoch 1160, training loss: 312.2295837402344 = 0.3417809307575226 + 50.0 * 6.237756252288818
Epoch 1160, val loss: 1.1118221282958984
Epoch 1170, training loss: 312.1164245605469 = 0.3348250687122345 + 50.0 * 6.235631465911865
Epoch 1170, val loss: 1.11588716506958
Epoch 1180, training loss: 312.38623046875 = 0.32804128527641296 + 50.0 * 6.241163730621338
Epoch 1180, val loss: 1.119901418685913
Epoch 1190, training loss: 312.3778991699219 = 0.32140877842903137 + 50.0 * 6.2411298751831055
Epoch 1190, val loss: 1.1238490343093872
Epoch 1200, training loss: 312.05926513671875 = 0.31482261419296265 + 50.0 * 6.234889030456543
Epoch 1200, val loss: 1.127974271774292
Epoch 1210, training loss: 311.97528076171875 = 0.3085035979747772 + 50.0 * 6.233335494995117
Epoch 1210, val loss: 1.1323775053024292
Epoch 1220, training loss: 311.9502868652344 = 0.3023601174354553 + 50.0 * 6.2329583168029785
Epoch 1220, val loss: 1.1369333267211914
Epoch 1230, training loss: 311.9897155761719 = 0.29635676741600037 + 50.0 * 6.233867168426514
Epoch 1230, val loss: 1.141465187072754
Epoch 1240, training loss: 312.15887451171875 = 0.29040026664733887 + 50.0 * 6.237369537353516
Epoch 1240, val loss: 1.145948052406311
Epoch 1250, training loss: 311.91314697265625 = 0.28458932042121887 + 50.0 * 6.232571601867676
Epoch 1250, val loss: 1.150225281715393
Epoch 1260, training loss: 311.8616027832031 = 0.2788980007171631 + 50.0 * 6.231654167175293
Epoch 1260, val loss: 1.1551032066345215
Epoch 1270, training loss: 311.8322448730469 = 0.27341344952583313 + 50.0 * 6.231176853179932
Epoch 1270, val loss: 1.1599117517471313
Epoch 1280, training loss: 312.0205078125 = 0.26809385418891907 + 50.0 * 6.235048294067383
Epoch 1280, val loss: 1.1646671295166016
Epoch 1290, training loss: 311.8221740722656 = 0.26275187730789185 + 50.0 * 6.2311882972717285
Epoch 1290, val loss: 1.1695829629898071
Epoch 1300, training loss: 311.80560302734375 = 0.2576001286506653 + 50.0 * 6.230959892272949
Epoch 1300, val loss: 1.1743295192718506
Epoch 1310, training loss: 311.9445495605469 = 0.252562552690506 + 50.0 * 6.233839511871338
Epoch 1310, val loss: 1.1793439388275146
Epoch 1320, training loss: 311.7248229980469 = 0.24762143194675446 + 50.0 * 6.229544162750244
Epoch 1320, val loss: 1.1844124794006348
Epoch 1330, training loss: 311.654541015625 = 0.24282795190811157 + 50.0 * 6.22823429107666
Epoch 1330, val loss: 1.1895672082901
Epoch 1340, training loss: 311.8587646484375 = 0.23815669119358063 + 50.0 * 6.232411861419678
Epoch 1340, val loss: 1.1947532892227173
Epoch 1350, training loss: 311.6748352050781 = 0.23349665105342865 + 50.0 * 6.228826999664307
Epoch 1350, val loss: 1.199742078781128
Epoch 1360, training loss: 311.65057373046875 = 0.22893385589122772 + 50.0 * 6.228432655334473
Epoch 1360, val loss: 1.204917073249817
Epoch 1370, training loss: 311.6437072753906 = 0.2245883345603943 + 50.0 * 6.228382587432861
Epoch 1370, val loss: 1.210282802581787
Epoch 1380, training loss: 311.6916198730469 = 0.2202579826116562 + 50.0 * 6.229427337646484
Epoch 1380, val loss: 1.215336561203003
Epoch 1390, training loss: 311.8565673828125 = 0.21603934466838837 + 50.0 * 6.2328104972839355
Epoch 1390, val loss: 1.2206544876098633
Epoch 1400, training loss: 311.5708312988281 = 0.2118997424840927 + 50.0 * 6.22717809677124
Epoch 1400, val loss: 1.225762963294983
Epoch 1410, training loss: 311.4992980957031 = 0.2078484147787094 + 50.0 * 6.225828647613525
Epoch 1410, val loss: 1.231174349784851
Epoch 1420, training loss: 311.4568176269531 = 0.20396140217781067 + 50.0 * 6.225057125091553
Epoch 1420, val loss: 1.2366905212402344
Epoch 1430, training loss: 311.84674072265625 = 0.20018833875656128 + 50.0 * 6.232931137084961
Epoch 1430, val loss: 1.242093801498413
Epoch 1440, training loss: 311.64727783203125 = 0.19625845551490784 + 50.0 * 6.229020595550537
Epoch 1440, val loss: 1.2469074726104736
Epoch 1450, training loss: 311.4405822753906 = 0.19251716136932373 + 50.0 * 6.224961280822754
Epoch 1450, val loss: 1.252271294593811
Epoch 1460, training loss: 311.36346435546875 = 0.18891200423240662 + 50.0 * 6.223491191864014
Epoch 1460, val loss: 1.25775945186615
Epoch 1470, training loss: 311.43084716796875 = 0.1854051649570465 + 50.0 * 6.22490930557251
Epoch 1470, val loss: 1.2631127834320068
Epoch 1480, training loss: 311.5216369628906 = 0.18191705644130707 + 50.0 * 6.226794719696045
Epoch 1480, val loss: 1.2682585716247559
Epoch 1490, training loss: 311.3532409667969 = 0.17847710847854614 + 50.0 * 6.2234954833984375
Epoch 1490, val loss: 1.2736074924468994
Epoch 1500, training loss: 311.29046630859375 = 0.17512299120426178 + 50.0 * 6.222307205200195
Epoch 1500, val loss: 1.2789710760116577
Epoch 1510, training loss: 311.28546142578125 = 0.1719004362821579 + 50.0 * 6.222270965576172
Epoch 1510, val loss: 1.2843974828720093
Epoch 1520, training loss: 311.72454833984375 = 0.16872453689575195 + 50.0 * 6.23111629486084
Epoch 1520, val loss: 1.2896839380264282
Epoch 1530, training loss: 311.4064025878906 = 0.16557753086090088 + 50.0 * 6.22481632232666
Epoch 1530, val loss: 1.2949622869491577
Epoch 1540, training loss: 311.2586669921875 = 0.1624803990125656 + 50.0 * 6.221923828125
Epoch 1540, val loss: 1.3003156185150146
Epoch 1550, training loss: 311.3492736816406 = 0.15953673422336578 + 50.0 * 6.223794460296631
Epoch 1550, val loss: 1.3058232069015503
Epoch 1560, training loss: 311.21978759765625 = 0.15654057264328003 + 50.0 * 6.221264362335205
Epoch 1560, val loss: 1.3108031749725342
Epoch 1570, training loss: 311.2486267089844 = 0.15364383161067963 + 50.0 * 6.221899509429932
Epoch 1570, val loss: 1.3161641359329224
Epoch 1580, training loss: 311.17620849609375 = 0.15080751478672028 + 50.0 * 6.220508098602295
Epoch 1580, val loss: 1.3215805292129517
Epoch 1590, training loss: 311.1260681152344 = 0.14809872210025787 + 50.0 * 6.219559192657471
Epoch 1590, val loss: 1.327095627784729
Epoch 1600, training loss: 311.1403503417969 = 0.1454351395368576 + 50.0 * 6.219898223876953
Epoch 1600, val loss: 1.3326001167297363
Epoch 1610, training loss: 311.3473205566406 = 0.14281065762043 + 50.0 * 6.224090099334717
Epoch 1610, val loss: 1.3377246856689453
Epoch 1620, training loss: 311.2290954589844 = 0.14018216729164124 + 50.0 * 6.221778869628906
Epoch 1620, val loss: 1.3431720733642578
Epoch 1630, training loss: 311.1365661621094 = 0.13757629692554474 + 50.0 * 6.219979763031006
Epoch 1630, val loss: 1.3481833934783936
Epoch 1640, training loss: 311.0986022949219 = 0.13501568138599396 + 50.0 * 6.219272136688232
Epoch 1640, val loss: 1.3535655736923218
Epoch 1650, training loss: 311.02783203125 = 0.13261409103870392 + 50.0 * 6.217904567718506
Epoch 1650, val loss: 1.3591229915618896
Epoch 1660, training loss: 311.0167541503906 = 0.13025036454200745 + 50.0 * 6.2177300453186035
Epoch 1660, val loss: 1.364659070968628
Epoch 1670, training loss: 311.2362365722656 = 0.12794871628284454 + 50.0 * 6.222165584564209
Epoch 1670, val loss: 1.370023488998413
Epoch 1680, training loss: 311.0841064453125 = 0.12561479210853577 + 50.0 * 6.219169616699219
Epoch 1680, val loss: 1.375295639038086
Epoch 1690, training loss: 310.99896240234375 = 0.12332958728075027 + 50.0 * 6.217513084411621
Epoch 1690, val loss: 1.3807990550994873
Epoch 1700, training loss: 311.1460266113281 = 0.12118707597255707 + 50.0 * 6.220496654510498
Epoch 1700, val loss: 1.3864965438842773
Epoch 1710, training loss: 310.98626708984375 = 0.11895401030778885 + 50.0 * 6.21734619140625
Epoch 1710, val loss: 1.3913780450820923
Epoch 1720, training loss: 310.9266357421875 = 0.11681953072547913 + 50.0 * 6.216196060180664
Epoch 1720, val loss: 1.3969030380249023
Epoch 1730, training loss: 310.89385986328125 = 0.11477252095937729 + 50.0 * 6.21558141708374
Epoch 1730, val loss: 1.4024925231933594
Epoch 1740, training loss: 310.9351501464844 = 0.11278264969587326 + 50.0 * 6.216446876525879
Epoch 1740, val loss: 1.4079632759094238
Epoch 1750, training loss: 311.2065734863281 = 0.11083018034696579 + 50.0 * 6.221914768218994
Epoch 1750, val loss: 1.4132561683654785
Epoch 1760, training loss: 311.00244140625 = 0.108820840716362 + 50.0 * 6.217872619628906
Epoch 1760, val loss: 1.4184619188308716
Epoch 1770, training loss: 310.9487609863281 = 0.1069173663854599 + 50.0 * 6.216836929321289
Epoch 1770, val loss: 1.4240866899490356
Epoch 1780, training loss: 311.0070495605469 = 0.10503390431404114 + 50.0 * 6.2180399894714355
Epoch 1780, val loss: 1.4292198419570923
Epoch 1790, training loss: 311.0408020019531 = 0.10315554589033127 + 50.0 * 6.218752861022949
Epoch 1790, val loss: 1.4342635869979858
Epoch 1800, training loss: 310.8556213378906 = 0.10136711597442627 + 50.0 * 6.215084552764893
Epoch 1800, val loss: 1.4400554895401
Epoch 1810, training loss: 310.8015441894531 = 0.09961596876382828 + 50.0 * 6.214038848876953
Epoch 1810, val loss: 1.4454865455627441
Epoch 1820, training loss: 310.7633056640625 = 0.09791860729455948 + 50.0 * 6.2133073806762695
Epoch 1820, val loss: 1.4509594440460205
Epoch 1830, training loss: 310.8393249511719 = 0.09629138559103012 + 50.0 * 6.214860916137695
Epoch 1830, val loss: 1.4564589262008667
Epoch 1840, training loss: 310.94976806640625 = 0.09462536126375198 + 50.0 * 6.217102527618408
Epoch 1840, val loss: 1.461485505104065
Epoch 1850, training loss: 310.87847900390625 = 0.09293022006750107 + 50.0 * 6.2157111167907715
Epoch 1850, val loss: 1.4664767980575562
Epoch 1860, training loss: 310.8070373535156 = 0.09132268279790878 + 50.0 * 6.2143144607543945
Epoch 1860, val loss: 1.4717276096343994
Epoch 1870, training loss: 310.7616882324219 = 0.08975687623023987 + 50.0 * 6.213438510894775
Epoch 1870, val loss: 1.4770522117614746
Epoch 1880, training loss: 310.7135314941406 = 0.08826544135808945 + 50.0 * 6.212505340576172
Epoch 1880, val loss: 1.4824869632720947
Epoch 1890, training loss: 310.9697265625 = 0.08682309091091156 + 50.0 * 6.217658042907715
Epoch 1890, val loss: 1.4879122972488403
Epoch 1900, training loss: 310.7048645019531 = 0.08531162887811661 + 50.0 * 6.212391376495361
Epoch 1900, val loss: 1.4926947355270386
Epoch 1910, training loss: 310.6798400878906 = 0.08388638496398926 + 50.0 * 6.211918830871582
Epoch 1910, val loss: 1.4981932640075684
Epoch 1920, training loss: 310.82952880859375 = 0.082512266933918 + 50.0 * 6.214940071105957
Epoch 1920, val loss: 1.5034428834915161
Epoch 1930, training loss: 310.68878173828125 = 0.08110219240188599 + 50.0 * 6.212153434753418
Epoch 1930, val loss: 1.5084158182144165
Epoch 1940, training loss: 310.712158203125 = 0.07976400852203369 + 50.0 * 6.212647438049316
Epoch 1940, val loss: 1.5136741399765015
Epoch 1950, training loss: 310.62982177734375 = 0.07844369113445282 + 50.0 * 6.211028099060059
Epoch 1950, val loss: 1.5189725160598755
Epoch 1960, training loss: 310.6753234863281 = 0.07718262821435928 + 50.0 * 6.211963176727295
Epoch 1960, val loss: 1.524376630783081
Epoch 1970, training loss: 310.8499450683594 = 0.075917087495327 + 50.0 * 6.215480327606201
Epoch 1970, val loss: 1.529326319694519
Epoch 1980, training loss: 310.71697998046875 = 0.07460831105709076 + 50.0 * 6.2128472328186035
Epoch 1980, val loss: 1.5340198278427124
Epoch 1990, training loss: 310.5677795410156 = 0.07335082441568375 + 50.0 * 6.209888458251953
Epoch 1990, val loss: 1.5389537811279297
Epoch 2000, training loss: 310.5751647949219 = 0.07218026369810104 + 50.0 * 6.210060119628906
Epoch 2000, val loss: 1.5443694591522217
Epoch 2010, training loss: 310.54541015625 = 0.07103538513183594 + 50.0 * 6.209487438201904
Epoch 2010, val loss: 1.5496296882629395
Epoch 2020, training loss: 310.721923828125 = 0.0699303075671196 + 50.0 * 6.213040351867676
Epoch 2020, val loss: 1.5547221899032593
Epoch 2030, training loss: 310.5559997558594 = 0.06878536194562912 + 50.0 * 6.209744453430176
Epoch 2030, val loss: 1.559675693511963
Epoch 2040, training loss: 310.5897521972656 = 0.06768036633729935 + 50.0 * 6.2104411125183105
Epoch 2040, val loss: 1.5646134614944458
Epoch 2050, training loss: 310.5791015625 = 0.06660640984773636 + 50.0 * 6.210249423980713
Epoch 2050, val loss: 1.5696022510528564
Epoch 2060, training loss: 310.7806396484375 = 0.06557103246450424 + 50.0 * 6.214301109313965
Epoch 2060, val loss: 1.5749332904815674
Epoch 2070, training loss: 310.5325012207031 = 0.0644751489162445 + 50.0 * 6.209360599517822
Epoch 2070, val loss: 1.5793592929840088
Epoch 2080, training loss: 310.4693908691406 = 0.0634559765458107 + 50.0 * 6.208118915557861
Epoch 2080, val loss: 1.584391713142395
Epoch 2090, training loss: 310.43731689453125 = 0.062481772154569626 + 50.0 * 6.2074971199035645
Epoch 2090, val loss: 1.58957839012146
Epoch 2100, training loss: 310.6007385253906 = 0.06153784319758415 + 50.0 * 6.210784435272217
Epoch 2100, val loss: 1.5946917533874512
Epoch 2110, training loss: 310.49859619140625 = 0.06055227667093277 + 50.0 * 6.208760738372803
Epoch 2110, val loss: 1.5990499258041382
Epoch 2120, training loss: 310.43304443359375 = 0.05957762524485588 + 50.0 * 6.207469463348389
Epoch 2120, val loss: 1.6040546894073486
Epoch 2130, training loss: 310.41607666015625 = 0.05867193266749382 + 50.0 * 6.20714807510376
Epoch 2130, val loss: 1.6090632677078247
Epoch 2140, training loss: 310.4262390136719 = 0.057786695659160614 + 50.0 * 6.207368850708008
Epoch 2140, val loss: 1.614113211631775
Epoch 2150, training loss: 310.795166015625 = 0.05691017955541611 + 50.0 * 6.2147650718688965
Epoch 2150, val loss: 1.6184948682785034
Epoch 2160, training loss: 310.5554504394531 = 0.05602814629673958 + 50.0 * 6.209988117218018
Epoch 2160, val loss: 1.6232587099075317
Epoch 2170, training loss: 310.47589111328125 = 0.0551549531519413 + 50.0 * 6.2084150314331055
Epoch 2170, val loss: 1.6281030178070068
Epoch 2180, training loss: 310.4593505859375 = 0.05434801056981087 + 50.0 * 6.208099842071533
Epoch 2180, val loss: 1.6331837177276611
Epoch 2190, training loss: 310.4359436035156 = 0.05353380739688873 + 50.0 * 6.207648277282715
Epoch 2190, val loss: 1.6381195783615112
Epoch 2200, training loss: 310.3888244628906 = 0.05272349715232849 + 50.0 * 6.206721782684326
Epoch 2200, val loss: 1.6426912546157837
Epoch 2210, training loss: 310.46380615234375 = 0.05193523317575455 + 50.0 * 6.208237171173096
Epoch 2210, val loss: 1.6473734378814697
Epoch 2220, training loss: 310.38983154296875 = 0.051170092076063156 + 50.0 * 6.206772804260254
Epoch 2220, val loss: 1.6519830226898193
Epoch 2230, training loss: 310.4224548339844 = 0.05042136833071709 + 50.0 * 6.2074408531188965
Epoch 2230, val loss: 1.656975269317627
Epoch 2240, training loss: 310.4854736328125 = 0.04967615380883217 + 50.0 * 6.208715915679932
Epoch 2240, val loss: 1.6614506244659424
Epoch 2250, training loss: 310.3448486328125 = 0.04892345890402794 + 50.0 * 6.205918788909912
Epoch 2250, val loss: 1.6658587455749512
Epoch 2260, training loss: 310.4683837890625 = 0.0482114814221859 + 50.0 * 6.20840311050415
Epoch 2260, val loss: 1.6704602241516113
Epoch 2270, training loss: 310.3021240234375 = 0.04750844091176987 + 50.0 * 6.205092430114746
Epoch 2270, val loss: 1.675263524055481
Epoch 2280, training loss: 310.3230285644531 = 0.046833381056785583 + 50.0 * 6.205523490905762
Epoch 2280, val loss: 1.6799315214157104
Epoch 2290, training loss: 310.29473876953125 = 0.046160608530044556 + 50.0 * 6.2049713134765625
Epoch 2290, val loss: 1.684485912322998
Epoch 2300, training loss: 310.41796875 = 0.04550937935709953 + 50.0 * 6.207449436187744
Epoch 2300, val loss: 1.6889407634735107
Epoch 2310, training loss: 310.35638427734375 = 0.04485455900430679 + 50.0 * 6.206230640411377
Epoch 2310, val loss: 1.6934220790863037
Epoch 2320, training loss: 310.3602600097656 = 0.04420308396220207 + 50.0 * 6.2063212394714355
Epoch 2320, val loss: 1.6976302862167358
Epoch 2330, training loss: 310.33203125 = 0.0435713492333889 + 50.0 * 6.205769062042236
Epoch 2330, val loss: 1.7021546363830566
Epoch 2340, training loss: 310.25592041015625 = 0.042952653020620346 + 50.0 * 6.204259395599365
Epoch 2340, val loss: 1.7067893743515015
Epoch 2350, training loss: 310.2497863769531 = 0.04235976189374924 + 50.0 * 6.204148292541504
Epoch 2350, val loss: 1.7114096879959106
Epoch 2360, training loss: 310.37774658203125 = 0.04178296774625778 + 50.0 * 6.206719398498535
Epoch 2360, val loss: 1.715909719467163
Epoch 2370, training loss: 310.2584228515625 = 0.041178230196237564 + 50.0 * 6.204345226287842
Epoch 2370, val loss: 1.7197976112365723
Epoch 2380, training loss: 310.2522888183594 = 0.04060337692499161 + 50.0 * 6.2042341232299805
Epoch 2380, val loss: 1.7240488529205322
Epoch 2390, training loss: 310.3392028808594 = 0.04004904627799988 + 50.0 * 6.2059831619262695
Epoch 2390, val loss: 1.728542447090149
Epoch 2400, training loss: 310.2454833984375 = 0.03949551284313202 + 50.0 * 6.20412015914917
Epoch 2400, val loss: 1.7327021360397339
Epoch 2410, training loss: 310.2583312988281 = 0.03896297886967659 + 50.0 * 6.204387187957764
Epoch 2410, val loss: 1.7371031045913696
Epoch 2420, training loss: 310.2574157714844 = 0.03841778263449669 + 50.0 * 6.204380035400391
Epoch 2420, val loss: 1.7411850690841675
Epoch 2430, training loss: 310.196533203125 = 0.03789937123656273 + 50.0 * 6.20317268371582
Epoch 2430, val loss: 1.745673656463623
Epoch 2440, training loss: 310.27191162109375 = 0.03740246593952179 + 50.0 * 6.204689979553223
Epoch 2440, val loss: 1.7499370574951172
Epoch 2450, training loss: 310.1810302734375 = 0.03688469156622887 + 50.0 * 6.202882766723633
Epoch 2450, val loss: 1.7539869546890259
Epoch 2460, training loss: 310.20501708984375 = 0.03639061748981476 + 50.0 * 6.203372955322266
Epoch 2460, val loss: 1.7582170963287354
Epoch 2470, training loss: 310.13543701171875 = 0.03591509908437729 + 50.0 * 6.201990127563477
Epoch 2470, val loss: 1.762644648551941
Epoch 2480, training loss: 310.2016296386719 = 0.035458754748106 + 50.0 * 6.2033233642578125
Epoch 2480, val loss: 1.7669039964675903
Epoch 2490, training loss: 310.3363342285156 = 0.034989990293979645 + 50.0 * 6.206026554107666
Epoch 2490, val loss: 1.7709211111068726
Epoch 2500, training loss: 310.26171875 = 0.034502867609262466 + 50.0 * 6.2045440673828125
Epoch 2500, val loss: 1.7744662761688232
Epoch 2510, training loss: 310.1200256347656 = 0.034044913947582245 + 50.0 * 6.201719284057617
Epoch 2510, val loss: 1.778882384300232
Epoch 2520, training loss: 310.07354736328125 = 0.03360635042190552 + 50.0 * 6.200798988342285
Epoch 2520, val loss: 1.7830570936203003
Epoch 2530, training loss: 310.08062744140625 = 0.03318788483738899 + 50.0 * 6.200948715209961
Epoch 2530, val loss: 1.7872343063354492
Epoch 2540, training loss: 310.5278625488281 = 0.03276306763291359 + 50.0 * 6.209901809692383
Epoch 2540, val loss: 1.7906371355056763
Epoch 2550, training loss: 310.2580871582031 = 0.032330241054296494 + 50.0 * 6.20451545715332
Epoch 2550, val loss: 1.7950762510299683
Epoch 2560, training loss: 310.069091796875 = 0.0319056361913681 + 50.0 * 6.200744152069092
Epoch 2560, val loss: 1.7988712787628174
Epoch 2570, training loss: 310.02606201171875 = 0.03151577711105347 + 50.0 * 6.199890613555908
Epoch 2570, val loss: 1.8031467199325562
Epoch 2580, training loss: 310.2474060058594 = 0.031140586361289024 + 50.0 * 6.2043256759643555
Epoch 2580, val loss: 1.8070168495178223
Epoch 2590, training loss: 310.0224609375 = 0.03072448819875717 + 50.0 * 6.19983434677124
Epoch 2590, val loss: 1.8106821775436401
Epoch 2600, training loss: 310.06378173828125 = 0.03033580258488655 + 50.0 * 6.200668811798096
Epoch 2600, val loss: 1.8146405220031738
Epoch 2610, training loss: 310.0477600097656 = 0.029957251623272896 + 50.0 * 6.2003560066223145
Epoch 2610, val loss: 1.818552017211914
Epoch 2620, training loss: 310.283935546875 = 0.029599452391266823 + 50.0 * 6.205086708068848
Epoch 2620, val loss: 1.822498083114624
Epoch 2630, training loss: 310.0456237792969 = 0.02922236919403076 + 50.0 * 6.2003278732299805
Epoch 2630, val loss: 1.8262401819229126
Epoch 2640, training loss: 309.9649658203125 = 0.02885112538933754 + 50.0 * 6.1987223625183105
Epoch 2640, val loss: 1.8301362991333008
Epoch 2650, training loss: 309.9617919921875 = 0.028503229841589928 + 50.0 * 6.198665618896484
Epoch 2650, val loss: 1.8340867757797241
Epoch 2660, training loss: 309.9751892089844 = 0.028170129284262657 + 50.0 * 6.198940277099609
Epoch 2660, val loss: 1.8380805253982544
Epoch 2670, training loss: 310.26214599609375 = 0.027836255729198456 + 50.0 * 6.204686164855957
Epoch 2670, val loss: 1.8414528369903564
Epoch 2680, training loss: 309.9879455566406 = 0.027493691071867943 + 50.0 * 6.199209213256836
Epoch 2680, val loss: 1.8454986810684204
Epoch 2690, training loss: 310.1094665527344 = 0.02716846950352192 + 50.0 * 6.201646327972412
Epoch 2690, val loss: 1.849160075187683
Epoch 2700, training loss: 309.9798889160156 = 0.026821844279766083 + 50.0 * 6.199061393737793
Epoch 2700, val loss: 1.8525298833847046
Epoch 2710, training loss: 309.9508972167969 = 0.026494842022657394 + 50.0 * 6.198488235473633
Epoch 2710, val loss: 1.8564943075180054
Epoch 2720, training loss: 309.9205627441406 = 0.02618897520005703 + 50.0 * 6.197887420654297
Epoch 2720, val loss: 1.8604789972305298
Epoch 2730, training loss: 309.91876220703125 = 0.025890463963150978 + 50.0 * 6.197857856750488
Epoch 2730, val loss: 1.8641548156738281
Epoch 2740, training loss: 310.2598876953125 = 0.025598345324397087 + 50.0 * 6.204685688018799
Epoch 2740, val loss: 1.867533564567566
Epoch 2750, training loss: 310.0309753417969 = 0.025291385129094124 + 50.0 * 6.200113773345947
Epoch 2750, val loss: 1.8712621927261353
Epoch 2760, training loss: 309.9513244628906 = 0.02498393878340721 + 50.0 * 6.198526382446289
Epoch 2760, val loss: 1.8746886253356934
Epoch 2770, training loss: 309.99261474609375 = 0.02470097690820694 + 50.0 * 6.1993584632873535
Epoch 2770, val loss: 1.878519892692566
Epoch 2780, training loss: 309.98382568359375 = 0.024414854124188423 + 50.0 * 6.199188232421875
Epoch 2780, val loss: 1.8819972276687622
Epoch 2790, training loss: 309.8804016113281 = 0.024136865511536598 + 50.0 * 6.197125434875488
Epoch 2790, val loss: 1.8858152627944946
Epoch 2800, training loss: 309.9498291015625 = 0.023869844153523445 + 50.0 * 6.198519229888916
Epoch 2800, val loss: 1.8893165588378906
Epoch 2810, training loss: 310.05194091796875 = 0.0235991720110178 + 50.0 * 6.200567245483398
Epoch 2810, val loss: 1.8927421569824219
Epoch 2820, training loss: 309.99609375 = 0.02332133799791336 + 50.0 * 6.199455738067627
Epoch 2820, val loss: 1.896140694618225
Epoch 2830, training loss: 310.11651611328125 = 0.023056821897625923 + 50.0 * 6.201869010925293
Epoch 2830, val loss: 1.8993659019470215
Epoch 2840, training loss: 309.8883972167969 = 0.022786501795053482 + 50.0 * 6.197311878204346
Epoch 2840, val loss: 1.902922511100769
Epoch 2850, training loss: 309.8439636230469 = 0.022531773895025253 + 50.0 * 6.1964287757873535
Epoch 2850, val loss: 1.9063842296600342
Epoch 2860, training loss: 310.0119934082031 = 0.02228696271777153 + 50.0 * 6.199794292449951
Epoch 2860, val loss: 1.9097504615783691
Epoch 2870, training loss: 309.8725891113281 = 0.022039756178855896 + 50.0 * 6.1970109939575195
Epoch 2870, val loss: 1.913450837135315
Epoch 2880, training loss: 309.88043212890625 = 0.021800050511956215 + 50.0 * 6.19717264175415
Epoch 2880, val loss: 1.9169926643371582
Epoch 2890, training loss: 309.88616943359375 = 0.021559959277510643 + 50.0 * 6.197292327880859
Epoch 2890, val loss: 1.920195460319519
Epoch 2900, training loss: 309.9869079589844 = 0.021333931013941765 + 50.0 * 6.19931173324585
Epoch 2900, val loss: 1.9234868288040161
Epoch 2910, training loss: 309.9109191894531 = 0.02109808847308159 + 50.0 * 6.19779634475708
Epoch 2910, val loss: 1.9266445636749268
Epoch 2920, training loss: 309.8701477050781 = 0.020869074389338493 + 50.0 * 6.196985721588135
Epoch 2920, val loss: 1.9299883842468262
Epoch 2930, training loss: 310.01220703125 = 0.02065141685307026 + 50.0 * 6.199831008911133
Epoch 2930, val loss: 1.933278203010559
Epoch 2940, training loss: 309.8446350097656 = 0.020415818318724632 + 50.0 * 6.196484565734863
Epoch 2940, val loss: 1.93623948097229
Epoch 2950, training loss: 309.8264465332031 = 0.02019156701862812 + 50.0 * 6.196125030517578
Epoch 2950, val loss: 1.9394440650939941
Epoch 2960, training loss: 309.80328369140625 = 0.019982343539595604 + 50.0 * 6.1956658363342285
Epoch 2960, val loss: 1.9428385496139526
Epoch 2970, training loss: 309.8178405761719 = 0.019781753420829773 + 50.0 * 6.1959614753723145
Epoch 2970, val loss: 1.9463069438934326
Epoch 2980, training loss: 309.9875793457031 = 0.019584259018301964 + 50.0 * 6.199359893798828
Epoch 2980, val loss: 1.9494760036468506
Epoch 2990, training loss: 309.92767333984375 = 0.019360767677426338 + 50.0 * 6.198166370391846
Epoch 2990, val loss: 1.9520654678344727
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.662962962962963
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 431.81793212890625 = 1.9781203269958496 + 50.0 * 8.596796035766602
Epoch 0, val loss: 1.974990725517273
Epoch 10, training loss: 431.7505187988281 = 1.9671621322631836 + 50.0 * 8.595666885375977
Epoch 10, val loss: 1.964248776435852
Epoch 20, training loss: 431.31463623046875 = 1.9534169435501099 + 50.0 * 8.587224006652832
Epoch 20, val loss: 1.9503264427185059
Epoch 30, training loss: 428.24237060546875 = 1.935726284980774 + 50.0 * 8.526132583618164
Epoch 30, val loss: 1.9323893785476685
Epoch 40, training loss: 406.534912109375 = 1.9159198999404907 + 50.0 * 8.092379570007324
Epoch 40, val loss: 1.9126302003860474
Epoch 50, training loss: 376.6596374511719 = 1.896378993988037 + 50.0 * 7.495265007019043
Epoch 50, val loss: 1.8945558071136475
Epoch 60, training loss: 361.5216369628906 = 1.8842471837997437 + 50.0 * 7.192747592926025
Epoch 60, val loss: 1.8833591938018799
Epoch 70, training loss: 349.43768310546875 = 1.872897744178772 + 50.0 * 6.951295852661133
Epoch 70, val loss: 1.872269630432129
Epoch 80, training loss: 343.4490966796875 = 1.8629885911941528 + 50.0 * 6.831722259521484
Epoch 80, val loss: 1.8627808094024658
Epoch 90, training loss: 338.30767822265625 = 1.853091835975647 + 50.0 * 6.729091644287109
Epoch 90, val loss: 1.853168249130249
Epoch 100, training loss: 334.4632873535156 = 1.844071388244629 + 50.0 * 6.652383804321289
Epoch 100, val loss: 1.844468593597412
Epoch 110, training loss: 331.874267578125 = 1.8357346057891846 + 50.0 * 6.600770950317383
Epoch 110, val loss: 1.8365318775177002
Epoch 120, training loss: 329.8065185546875 = 1.828182578086853 + 50.0 * 6.559566497802734
Epoch 120, val loss: 1.8294024467468262
Epoch 130, training loss: 328.328369140625 = 1.8215134143829346 + 50.0 * 6.530137538909912
Epoch 130, val loss: 1.8229949474334717
Epoch 140, training loss: 327.0934143066406 = 1.815382957458496 + 50.0 * 6.505560874938965
Epoch 140, val loss: 1.8172063827514648
Epoch 150, training loss: 326.1011962890625 = 1.8097000122070312 + 50.0 * 6.485829830169678
Epoch 150, val loss: 1.811775803565979
Epoch 160, training loss: 325.4515686035156 = 1.804248332977295 + 50.0 * 6.4729461669921875
Epoch 160, val loss: 1.8066264390945435
Epoch 170, training loss: 324.70208740234375 = 1.7989304065704346 + 50.0 * 6.458063125610352
Epoch 170, val loss: 1.8015917539596558
Epoch 180, training loss: 324.05609130859375 = 1.7937018871307373 + 50.0 * 6.445247650146484
Epoch 180, val loss: 1.7967042922973633
Epoch 190, training loss: 323.4869384765625 = 1.788399338722229 + 50.0 * 6.433970928192139
Epoch 190, val loss: 1.7919251918792725
Epoch 200, training loss: 323.19744873046875 = 1.7830044031143188 + 50.0 * 6.428289413452148
Epoch 200, val loss: 1.787112832069397
Epoch 210, training loss: 322.5931396484375 = 1.7772893905639648 + 50.0 * 6.416316986083984
Epoch 210, val loss: 1.7822297811508179
Epoch 220, training loss: 322.14898681640625 = 1.7713582515716553 + 50.0 * 6.407552242279053
Epoch 220, val loss: 1.7772406339645386
Epoch 230, training loss: 321.7664794921875 = 1.7651499509811401 + 50.0 * 6.400026798248291
Epoch 230, val loss: 1.7721006870269775
Epoch 240, training loss: 321.5877380371094 = 1.7585700750350952 + 50.0 * 6.396583557128906
Epoch 240, val loss: 1.7667287588119507
Epoch 250, training loss: 321.1612243652344 = 1.7514374256134033 + 50.0 * 6.388195514678955
Epoch 250, val loss: 1.7610450983047485
Epoch 260, training loss: 320.7351989746094 = 1.7439630031585693 + 50.0 * 6.379825115203857
Epoch 260, val loss: 1.7551109790802002
Epoch 270, training loss: 320.44293212890625 = 1.7359874248504639 + 50.0 * 6.374138832092285
Epoch 270, val loss: 1.7488572597503662
Epoch 280, training loss: 320.39141845703125 = 1.7274665832519531 + 50.0 * 6.373279094696045
Epoch 280, val loss: 1.7422608137130737
Epoch 290, training loss: 320.0317077636719 = 1.7183557748794556 + 50.0 * 6.366267204284668
Epoch 290, val loss: 1.7351258993148804
Epoch 300, training loss: 319.6763916015625 = 1.7085375785827637 + 50.0 * 6.3593573570251465
Epoch 300, val loss: 1.7276480197906494
Epoch 310, training loss: 319.40899658203125 = 1.6981761455535889 + 50.0 * 6.354216575622559
Epoch 310, val loss: 1.7197569608688354
Epoch 320, training loss: 319.1606750488281 = 1.6872235536575317 + 50.0 * 6.349469184875488
Epoch 320, val loss: 1.7114312648773193
Epoch 330, training loss: 319.1839904785156 = 1.6755809783935547 + 50.0 * 6.350168704986572
Epoch 330, val loss: 1.7026331424713135
Epoch 340, training loss: 318.8293151855469 = 1.6632015705108643 + 50.0 * 6.343322277069092
Epoch 340, val loss: 1.6932735443115234
Epoch 350, training loss: 318.55194091796875 = 1.650217890739441 + 50.0 * 6.338034629821777
Epoch 350, val loss: 1.6835838556289673
Epoch 360, training loss: 318.5418395996094 = 1.6366475820541382 + 50.0 * 6.338103771209717
Epoch 360, val loss: 1.6734371185302734
Epoch 370, training loss: 318.2202453613281 = 1.6222225427627563 + 50.0 * 6.331960678100586
Epoch 370, val loss: 1.662938117980957
Epoch 380, training loss: 317.96142578125 = 1.6072955131530762 + 50.0 * 6.327082633972168
Epoch 380, val loss: 1.6519184112548828
Epoch 390, training loss: 317.7940368652344 = 1.59182608127594 + 50.0 * 6.324044227600098
Epoch 390, val loss: 1.6406885385513306
Epoch 400, training loss: 317.6427001953125 = 1.575864315032959 + 50.0 * 6.32133674621582
Epoch 400, val loss: 1.629194736480713
Epoch 410, training loss: 317.7351989746094 = 1.5592328310012817 + 50.0 * 6.323519229888916
Epoch 410, val loss: 1.6171865463256836
Epoch 420, training loss: 317.32208251953125 = 1.5421338081359863 + 50.0 * 6.315598487854004
Epoch 420, val loss: 1.6050690412521362
Epoch 430, training loss: 317.1126403808594 = 1.524728536605835 + 50.0 * 6.311758518218994
Epoch 430, val loss: 1.5927910804748535
Epoch 440, training loss: 316.97332763671875 = 1.507031798362732 + 50.0 * 6.309326171875
Epoch 440, val loss: 1.580514669418335
Epoch 450, training loss: 316.9714050292969 = 1.4889349937438965 + 50.0 * 6.309649467468262
Epoch 450, val loss: 1.5679208040237427
Epoch 460, training loss: 316.85888671875 = 1.4704092741012573 + 50.0 * 6.307769775390625
Epoch 460, val loss: 1.555403470993042
Epoch 470, training loss: 316.61895751953125 = 1.4516671895980835 + 50.0 * 6.303345680236816
Epoch 470, val loss: 1.5426945686340332
Epoch 480, training loss: 316.4421081542969 = 1.4328641891479492 + 50.0 * 6.300185203552246
Epoch 480, val loss: 1.5301727056503296
Epoch 490, training loss: 316.3897705078125 = 1.413886308670044 + 50.0 * 6.29951810836792
Epoch 490, val loss: 1.5176563262939453
Epoch 500, training loss: 316.3012390136719 = 1.3947468996047974 + 50.0 * 6.298130035400391
Epoch 500, val loss: 1.505028486251831
Epoch 510, training loss: 316.18792724609375 = 1.375455617904663 + 50.0 * 6.2962493896484375
Epoch 510, val loss: 1.4926033020019531
Epoch 520, training loss: 315.9776611328125 = 1.3561617136001587 + 50.0 * 6.2924299240112305
Epoch 520, val loss: 1.4804894924163818
Epoch 530, training loss: 315.85968017578125 = 1.3370040655136108 + 50.0 * 6.2904534339904785
Epoch 530, val loss: 1.4685571193695068
Epoch 540, training loss: 315.8343200683594 = 1.3178783655166626 + 50.0 * 6.2903289794921875
Epoch 540, val loss: 1.456907868385315
Epoch 550, training loss: 315.8578796386719 = 1.2987381219863892 + 50.0 * 6.291182994842529
Epoch 550, val loss: 1.445130467414856
Epoch 560, training loss: 315.6219177246094 = 1.2794508934020996 + 50.0 * 6.286849498748779
Epoch 560, val loss: 1.4335628747940063
Epoch 570, training loss: 315.4338073730469 = 1.2606295347213745 + 50.0 * 6.283463954925537
Epoch 570, val loss: 1.4223250150680542
Epoch 580, training loss: 315.3351745605469 = 1.2420004606246948 + 50.0 * 6.281863689422607
Epoch 580, val loss: 1.4114068746566772
Epoch 590, training loss: 315.9002685546875 = 1.223618984222412 + 50.0 * 6.293532848358154
Epoch 590, val loss: 1.4006423950195312
Epoch 600, training loss: 315.2208557128906 = 1.2050328254699707 + 50.0 * 6.280316352844238
Epoch 600, val loss: 1.3901422023773193
Epoch 610, training loss: 315.08587646484375 = 1.1869128942489624 + 50.0 * 6.277978897094727
Epoch 610, val loss: 1.380136489868164
Epoch 620, training loss: 314.982177734375 = 1.1692336797714233 + 50.0 * 6.276258945465088
Epoch 620, val loss: 1.370469331741333
Epoch 630, training loss: 314.9228210449219 = 1.151824951171875 + 50.0 * 6.275420188903809
Epoch 630, val loss: 1.3611103296279907
Epoch 640, training loss: 314.9166259765625 = 1.1345481872558594 + 50.0 * 6.275641441345215
Epoch 640, val loss: 1.3518439531326294
Epoch 650, training loss: 314.9361877441406 = 1.117382526397705 + 50.0 * 6.276376247406006
Epoch 650, val loss: 1.3427525758743286
Epoch 660, training loss: 314.7159423828125 = 1.1005594730377197 + 50.0 * 6.272307872772217
Epoch 660, val loss: 1.3340885639190674
Epoch 670, training loss: 314.627197265625 = 1.084210991859436 + 50.0 * 6.270859718322754
Epoch 670, val loss: 1.325817346572876
Epoch 680, training loss: 314.7207946777344 = 1.068174958229065 + 50.0 * 6.273052215576172
Epoch 680, val loss: 1.3178008794784546
Epoch 690, training loss: 314.6068420410156 = 1.0524024963378906 + 50.0 * 6.27108907699585
Epoch 690, val loss: 1.3105477094650269
Epoch 700, training loss: 314.4988708496094 = 1.0367488861083984 + 50.0 * 6.269242763519287
Epoch 700, val loss: 1.3027472496032715
Epoch 710, training loss: 314.4168701171875 = 1.021811842918396 + 50.0 * 6.2679009437561035
Epoch 710, val loss: 1.2957993745803833
Epoch 720, training loss: 314.3161926269531 = 1.0071114301681519 + 50.0 * 6.266181468963623
Epoch 720, val loss: 1.2892404794692993
Epoch 730, training loss: 314.2079772949219 = 0.9928577542304993 + 50.0 * 6.2643022537231445
Epoch 730, val loss: 1.2829598188400269
Epoch 740, training loss: 314.374755859375 = 0.9788920879364014 + 50.0 * 6.267917633056641
Epoch 740, val loss: 1.2773386240005493
Epoch 750, training loss: 314.1521301269531 = 0.9651508331298828 + 50.0 * 6.263739585876465
Epoch 750, val loss: 1.2709895372390747
Epoch 760, training loss: 314.04150390625 = 0.9518578052520752 + 50.0 * 6.26179313659668
Epoch 760, val loss: 1.2659118175506592
Epoch 770, training loss: 313.9555358886719 = 0.9389129877090454 + 50.0 * 6.2603325843811035
Epoch 770, val loss: 1.2607746124267578
Epoch 780, training loss: 314.1642761230469 = 0.9263118505477905 + 50.0 * 6.264759540557861
Epoch 780, val loss: 1.2559127807617188
Epoch 790, training loss: 314.2215881347656 = 0.9133896827697754 + 50.0 * 6.2661638259887695
Epoch 790, val loss: 1.2515064477920532
Epoch 800, training loss: 313.8731689453125 = 0.9010562300682068 + 50.0 * 6.259442329406738
Epoch 800, val loss: 1.2469723224639893
Epoch 810, training loss: 313.7310485839844 = 0.8891128301620483 + 50.0 * 6.256838798522949
Epoch 810, val loss: 1.2427432537078857
Epoch 820, training loss: 313.7888488769531 = 0.8774822354316711 + 50.0 * 6.258227825164795
Epoch 820, val loss: 1.2386934757232666
Epoch 830, training loss: 313.720703125 = 0.8658835887908936 + 50.0 * 6.257096290588379
Epoch 830, val loss: 1.235323190689087
Epoch 840, training loss: 313.68963623046875 = 0.8543314337730408 + 50.0 * 6.2567057609558105
Epoch 840, val loss: 1.231611728668213
Epoch 850, training loss: 313.5704650878906 = 0.8431569933891296 + 50.0 * 6.25454568862915
Epoch 850, val loss: 1.2281221151351929
Epoch 860, training loss: 313.4778137207031 = 0.8321593403816223 + 50.0 * 6.252913475036621
Epoch 860, val loss: 1.2249432802200317
Epoch 870, training loss: 313.58001708984375 = 0.82136070728302 + 50.0 * 6.255173206329346
Epoch 870, val loss: 1.22153639793396
Epoch 880, training loss: 313.4974670410156 = 0.810527503490448 + 50.0 * 6.253738880157471
Epoch 880, val loss: 1.2185697555541992
Epoch 890, training loss: 313.4928283691406 = 0.7996915578842163 + 50.0 * 6.2538628578186035
Epoch 890, val loss: 1.2156529426574707
Epoch 900, training loss: 313.3365478515625 = 0.7890795469284058 + 50.0 * 6.250948905944824
Epoch 900, val loss: 1.2126543521881104
Epoch 910, training loss: 313.26568603515625 = 0.7787020802497864 + 50.0 * 6.249739646911621
Epoch 910, val loss: 1.2098342180252075
Epoch 920, training loss: 313.35504150390625 = 0.7684274911880493 + 50.0 * 6.251732349395752
Epoch 920, val loss: 1.2074079513549805
Epoch 930, training loss: 313.20123291015625 = 0.7581586837768555 + 50.0 * 6.248861789703369
Epoch 930, val loss: 1.204627513885498
Epoch 940, training loss: 313.2652587890625 = 0.7479414939880371 + 50.0 * 6.2503461837768555
Epoch 940, val loss: 1.2019877433776855
Epoch 950, training loss: 313.0765075683594 = 0.7378473281860352 + 50.0 * 6.2467732429504395
Epoch 950, val loss: 1.1999109983444214
Epoch 960, training loss: 313.0276184082031 = 0.7278414964675903 + 50.0 * 6.24599552154541
Epoch 960, val loss: 1.1976238489151
Epoch 970, training loss: 313.3607482910156 = 0.7179703712463379 + 50.0 * 6.2528557777404785
Epoch 970, val loss: 1.1950182914733887
Epoch 980, training loss: 313.0297546386719 = 0.7076706290245056 + 50.0 * 6.24644136428833
Epoch 980, val loss: 1.1923956871032715
Epoch 990, training loss: 312.9373474121094 = 0.6977254748344421 + 50.0 * 6.244792461395264
Epoch 990, val loss: 1.1901540756225586
Epoch 1000, training loss: 312.8631286621094 = 0.6880360245704651 + 50.0 * 6.243501663208008
Epoch 1000, val loss: 1.188637375831604
Epoch 1010, training loss: 312.8678894042969 = 0.6783765554428101 + 50.0 * 6.243790149688721
Epoch 1010, val loss: 1.1865476369857788
Epoch 1020, training loss: 312.9177551269531 = 0.668641984462738 + 50.0 * 6.2449822425842285
Epoch 1020, val loss: 1.1845965385437012
Epoch 1030, training loss: 312.80804443359375 = 0.6588504910469055 + 50.0 * 6.242984294891357
Epoch 1030, val loss: 1.1832331418991089
Epoch 1040, training loss: 312.7959289550781 = 0.6492114663124084 + 50.0 * 6.242934703826904
Epoch 1040, val loss: 1.1814101934432983
Epoch 1050, training loss: 312.67803955078125 = 0.6396380662918091 + 50.0 * 6.240767955780029
Epoch 1050, val loss: 1.1796810626983643
Epoch 1060, training loss: 312.7574157714844 = 0.6302105188369751 + 50.0 * 6.242543697357178
Epoch 1060, val loss: 1.1777303218841553
Epoch 1070, training loss: 312.6408996582031 = 0.6206071972846985 + 50.0 * 6.240406036376953
Epoch 1070, val loss: 1.176648497581482
Epoch 1080, training loss: 312.5892028808594 = 0.6111090779304504 + 50.0 * 6.239562034606934
Epoch 1080, val loss: 1.1743643283843994
Epoch 1090, training loss: 312.5789489746094 = 0.6017764210700989 + 50.0 * 6.239543437957764
Epoch 1090, val loss: 1.1735466718673706
Epoch 1100, training loss: 312.66131591796875 = 0.5925348997116089 + 50.0 * 6.24137544631958
Epoch 1100, val loss: 1.1714884042739868
Epoch 1110, training loss: 312.5313415527344 = 0.5831652879714966 + 50.0 * 6.238963603973389
Epoch 1110, val loss: 1.171252727508545
Epoch 1120, training loss: 312.4381408691406 = 0.5739495158195496 + 50.0 * 6.237283706665039
Epoch 1120, val loss: 1.1695351600646973
Epoch 1130, training loss: 312.61553955078125 = 0.5648918151855469 + 50.0 * 6.241013050079346
Epoch 1130, val loss: 1.1685975790023804
Epoch 1140, training loss: 312.45489501953125 = 0.5557087659835815 + 50.0 * 6.2379841804504395
Epoch 1140, val loss: 1.167216181755066
Epoch 1150, training loss: 312.3321228027344 = 0.5467308163642883 + 50.0 * 6.235708236694336
Epoch 1150, val loss: 1.1667659282684326
Epoch 1160, training loss: 312.2760009765625 = 0.5379104614257812 + 50.0 * 6.234762191772461
Epoch 1160, val loss: 1.1658399105072021
Epoch 1170, training loss: 312.28411865234375 = 0.5292285680770874 + 50.0 * 6.235097408294678
Epoch 1170, val loss: 1.165337324142456
Epoch 1180, training loss: 312.591552734375 = 0.5205049514770508 + 50.0 * 6.241420745849609
Epoch 1180, val loss: 1.164597988128662
Epoch 1190, training loss: 312.2799987792969 = 0.51175856590271 + 50.0 * 6.23536491394043
Epoch 1190, val loss: 1.1649218797683716
Epoch 1200, training loss: 312.30328369140625 = 0.5031715631484985 + 50.0 * 6.236001968383789
Epoch 1200, val loss: 1.1642359495162964
Epoch 1210, training loss: 312.14501953125 = 0.4947851002216339 + 50.0 * 6.233004570007324
Epoch 1210, val loss: 1.164212703704834
Epoch 1220, training loss: 312.2488708496094 = 0.4865621030330658 + 50.0 * 6.235246181488037
Epoch 1220, val loss: 1.1645132303237915
Epoch 1230, training loss: 312.1510009765625 = 0.47833117842674255 + 50.0 * 6.233453273773193
Epoch 1230, val loss: 1.1654644012451172
Epoch 1240, training loss: 312.1908874511719 = 0.4703024923801422 + 50.0 * 6.234411716461182
Epoch 1240, val loss: 1.1660720109939575
Epoch 1250, training loss: 312.0828552246094 = 0.46231693029403687 + 50.0 * 6.232410907745361
Epoch 1250, val loss: 1.1660901308059692
Epoch 1260, training loss: 312.1422424316406 = 0.45448824763298035 + 50.0 * 6.233755111694336
Epoch 1260, val loss: 1.1673299074172974
Epoch 1270, training loss: 312.03521728515625 = 0.44679516553878784 + 50.0 * 6.2317681312561035
Epoch 1270, val loss: 1.1684545278549194
Epoch 1280, training loss: 311.9544677734375 = 0.4392363429069519 + 50.0 * 6.230304718017578
Epoch 1280, val loss: 1.1699532270431519
Epoch 1290, training loss: 311.9812927246094 = 0.4318503737449646 + 50.0 * 6.230988502502441
Epoch 1290, val loss: 1.1713002920150757
Epoch 1300, training loss: 312.0321960449219 = 0.42447608709335327 + 50.0 * 6.232154846191406
Epoch 1300, val loss: 1.1731524467468262
Epoch 1310, training loss: 311.926025390625 = 0.4170968532562256 + 50.0 * 6.2301788330078125
Epoch 1310, val loss: 1.1749342679977417
Epoch 1320, training loss: 311.98980712890625 = 0.4099157750606537 + 50.0 * 6.231597900390625
Epoch 1320, val loss: 1.1771948337554932
Epoch 1330, training loss: 311.8297424316406 = 0.40295690298080444 + 50.0 * 6.2285356521606445
Epoch 1330, val loss: 1.1787532567977905
Epoch 1340, training loss: 311.8005065917969 = 0.39615440368652344 + 50.0 * 6.228086948394775
Epoch 1340, val loss: 1.1812013387680054
Epoch 1350, training loss: 312.02587890625 = 0.38944539427757263 + 50.0 * 6.232728958129883
Epoch 1350, val loss: 1.1838011741638184
Epoch 1360, training loss: 311.9581604003906 = 0.3826567530632019 + 50.0 * 6.231510162353516
Epoch 1360, val loss: 1.1865146160125732
Epoch 1370, training loss: 311.7924499511719 = 0.3761196434497833 + 50.0 * 6.228326320648193
Epoch 1370, val loss: 1.1886128187179565
Epoch 1380, training loss: 311.69097900390625 = 0.36971989274024963 + 50.0 * 6.2264251708984375
Epoch 1380, val loss: 1.191787600517273
Epoch 1390, training loss: 311.6802673339844 = 0.3635327219963074 + 50.0 * 6.226334571838379
Epoch 1390, val loss: 1.1949386596679688
Epoch 1400, training loss: 312.01385498046875 = 0.35742998123168945 + 50.0 * 6.233128547668457
Epoch 1400, val loss: 1.1981691122055054
Epoch 1410, training loss: 311.8407287597656 = 0.35121050477027893 + 50.0 * 6.229790687561035
Epoch 1410, val loss: 1.2007663249969482
Epoch 1420, training loss: 311.705078125 = 0.3451574742794037 + 50.0 * 6.227198600769043
Epoch 1420, val loss: 1.2043137550354004
Epoch 1430, training loss: 311.58807373046875 = 0.33935678005218506 + 50.0 * 6.224974155426025
Epoch 1430, val loss: 1.2074394226074219
Epoch 1440, training loss: 311.68316650390625 = 0.33371010422706604 + 50.0 * 6.226989269256592
Epoch 1440, val loss: 1.2116039991378784
Epoch 1450, training loss: 311.7114562988281 = 0.327979177236557 + 50.0 * 6.227669715881348
Epoch 1450, val loss: 1.2147254943847656
Epoch 1460, training loss: 311.6100158691406 = 0.3223418891429901 + 50.0 * 6.2257537841796875
Epoch 1460, val loss: 1.2181527614593506
Epoch 1470, training loss: 311.52044677734375 = 0.31683334708213806 + 50.0 * 6.224072456359863
Epoch 1470, val loss: 1.2217236757278442
Epoch 1480, training loss: 311.465576171875 = 0.3115895688533783 + 50.0 * 6.223079681396484
Epoch 1480, val loss: 1.2258203029632568
Epoch 1490, training loss: 311.4765319824219 = 0.3064408302307129 + 50.0 * 6.22340202331543
Epoch 1490, val loss: 1.2294780015945435
Epoch 1500, training loss: 311.71343994140625 = 0.30134642124176025 + 50.0 * 6.228241443634033
Epoch 1500, val loss: 1.233529806137085
Epoch 1510, training loss: 311.574951171875 = 0.2960931658744812 + 50.0 * 6.225577354431152
Epoch 1510, val loss: 1.237931728363037
Epoch 1520, training loss: 311.42791748046875 = 0.29112961888313293 + 50.0 * 6.222735404968262
Epoch 1520, val loss: 1.2412006855010986
Epoch 1530, training loss: 311.4158935546875 = 0.2862820625305176 + 50.0 * 6.222591876983643
Epoch 1530, val loss: 1.2463082075119019
Epoch 1540, training loss: 311.60369873046875 = 0.281558096408844 + 50.0 * 6.226442813873291
Epoch 1540, val loss: 1.250044584274292
Epoch 1550, training loss: 311.42626953125 = 0.2767189145088196 + 50.0 * 6.222990989685059
Epoch 1550, val loss: 1.2545522451400757
Epoch 1560, training loss: 311.3602294921875 = 0.2720882296562195 + 50.0 * 6.221762657165527
Epoch 1560, val loss: 1.258632779121399
Epoch 1570, training loss: 311.4380187988281 = 0.2675594389438629 + 50.0 * 6.223409652709961
Epoch 1570, val loss: 1.263039469718933
Epoch 1580, training loss: 311.3348388671875 = 0.263071745634079 + 50.0 * 6.221435546875
Epoch 1580, val loss: 1.2677010297775269
Epoch 1590, training loss: 311.51910400390625 = 0.25861141085624695 + 50.0 * 6.225210189819336
Epoch 1590, val loss: 1.272196650505066
Epoch 1600, training loss: 311.2965087890625 = 0.2542145550251007 + 50.0 * 6.220845699310303
Epoch 1600, val loss: 1.2765815258026123
Epoch 1610, training loss: 311.24078369140625 = 0.25000816583633423 + 50.0 * 6.219815731048584
Epoch 1610, val loss: 1.2816847562789917
Epoch 1620, training loss: 311.3321228027344 = 0.2458806037902832 + 50.0 * 6.221724987030029
Epoch 1620, val loss: 1.286086916923523
Epoch 1630, training loss: 311.2572937011719 = 0.24171817302703857 + 50.0 * 6.220311164855957
Epoch 1630, val loss: 1.2917300462722778
Epoch 1640, training loss: 311.2052001953125 = 0.23764900863170624 + 50.0 * 6.219351291656494
Epoch 1640, val loss: 1.2960127592086792
Epoch 1650, training loss: 311.18792724609375 = 0.23370671272277832 + 50.0 * 6.219084739685059
Epoch 1650, val loss: 1.300933599472046
Epoch 1660, training loss: 311.4365539550781 = 0.2298453152179718 + 50.0 * 6.2241339683532715
Epoch 1660, val loss: 1.3061127662658691
Epoch 1670, training loss: 311.2430419921875 = 0.2259030044078827 + 50.0 * 6.220343112945557
Epoch 1670, val loss: 1.3108230829238892
Epoch 1680, training loss: 311.18634033203125 = 0.2221146672964096 + 50.0 * 6.219284534454346
Epoch 1680, val loss: 1.3164399862289429
Epoch 1690, training loss: 311.13128662109375 = 0.21839793026447296 + 50.0 * 6.218257904052734
Epoch 1690, val loss: 1.3203591108322144
Epoch 1700, training loss: 311.143798828125 = 0.21476627886295319 + 50.0 * 6.218581199645996
Epoch 1700, val loss: 1.326600193977356
Epoch 1710, training loss: 311.2848205566406 = 0.2111566662788391 + 50.0 * 6.221473217010498
Epoch 1710, val loss: 1.3313878774642944
Epoch 1720, training loss: 311.0765380859375 = 0.20756138861179352 + 50.0 * 6.217380046844482
Epoch 1720, val loss: 1.3365272283554077
Epoch 1730, training loss: 311.0433044433594 = 0.2040950357913971 + 50.0 * 6.2167840003967285
Epoch 1730, val loss: 1.3413063287734985
Epoch 1740, training loss: 311.0752868652344 = 0.2007344365119934 + 50.0 * 6.2174906730651855
Epoch 1740, val loss: 1.3471238613128662
Epoch 1750, training loss: 311.1073303222656 = 0.19736534357070923 + 50.0 * 6.218199729919434
Epoch 1750, val loss: 1.352232575416565
Epoch 1760, training loss: 311.03131103515625 = 0.19406923651695251 + 50.0 * 6.216744899749756
Epoch 1760, val loss: 1.3583824634552002
Epoch 1770, training loss: 310.97100830078125 = 0.19083929061889648 + 50.0 * 6.215603828430176
Epoch 1770, val loss: 1.3634189367294312
Epoch 1780, training loss: 311.1973876953125 = 0.1876826137304306 + 50.0 * 6.220193862915039
Epoch 1780, val loss: 1.3691608905792236
Epoch 1790, training loss: 311.00592041015625 = 0.18448780477046967 + 50.0 * 6.216428756713867
Epoch 1790, val loss: 1.3741683959960938
Epoch 1800, training loss: 311.1471862792969 = 0.18139705061912537 + 50.0 * 6.219315528869629
Epoch 1800, val loss: 1.3793476819992065
Epoch 1810, training loss: 310.982666015625 = 0.17830270528793335 + 50.0 * 6.2160868644714355
Epoch 1810, val loss: 1.3845239877700806
Epoch 1820, training loss: 310.91143798828125 = 0.1753469705581665 + 50.0 * 6.2147216796875
Epoch 1820, val loss: 1.3902292251586914
Epoch 1830, training loss: 310.9105224609375 = 0.17246297001838684 + 50.0 * 6.214761257171631
Epoch 1830, val loss: 1.3961880207061768
Epoch 1840, training loss: 310.9469299316406 = 0.16961051523685455 + 50.0 * 6.215546131134033
Epoch 1840, val loss: 1.401542067527771
Epoch 1850, training loss: 311.02484130859375 = 0.1667536199092865 + 50.0 * 6.217161655426025
Epoch 1850, val loss: 1.4072626829147339
Epoch 1860, training loss: 310.8994445800781 = 0.16395673155784607 + 50.0 * 6.214709281921387
Epoch 1860, val loss: 1.4130468368530273
Epoch 1870, training loss: 310.831787109375 = 0.1612085998058319 + 50.0 * 6.213411808013916
Epoch 1870, val loss: 1.41834557056427
Epoch 1880, training loss: 310.8900146484375 = 0.15853871405124664 + 50.0 * 6.214629650115967
Epoch 1880, val loss: 1.4242125749588013
Epoch 1890, training loss: 310.8522644042969 = 0.15590272843837738 + 50.0 * 6.213926792144775
Epoch 1890, val loss: 1.4298337697982788
Epoch 1900, training loss: 310.81451416015625 = 0.15331712365150452 + 50.0 * 6.213223934173584
Epoch 1900, val loss: 1.4355194568634033
Epoch 1910, training loss: 311.1111145019531 = 0.15078337490558624 + 50.0 * 6.2192063331604
Epoch 1910, val loss: 1.4413549900054932
Epoch 1920, training loss: 310.82867431640625 = 0.1482086032629013 + 50.0 * 6.21360969543457
Epoch 1920, val loss: 1.4463039636611938
Epoch 1930, training loss: 310.7476806640625 = 0.1457238346338272 + 50.0 * 6.212039470672607
Epoch 1930, val loss: 1.4525601863861084
Epoch 1940, training loss: 310.7029724121094 = 0.14334659278392792 + 50.0 * 6.211192607879639
Epoch 1940, val loss: 1.4583431482315063
Epoch 1950, training loss: 310.7907409667969 = 0.1410246044397354 + 50.0 * 6.212994575500488
Epoch 1950, val loss: 1.4641306400299072
Epoch 1960, training loss: 310.83575439453125 = 0.13865675032138824 + 50.0 * 6.21394157409668
Epoch 1960, val loss: 1.470061182975769
Epoch 1970, training loss: 310.73553466796875 = 0.13630054891109467 + 50.0 * 6.211985111236572
Epoch 1970, val loss: 1.4753447771072388
Epoch 1980, training loss: 310.7438659667969 = 0.1340388059616089 + 50.0 * 6.2121968269348145
Epoch 1980, val loss: 1.481619119644165
Epoch 1990, training loss: 310.6943359375 = 0.13183127343654633 + 50.0 * 6.211250305175781
Epoch 1990, val loss: 1.4879264831542969
Epoch 2000, training loss: 310.73309326171875 = 0.12969233095645905 + 50.0 * 6.212067604064941
Epoch 2000, val loss: 1.4941644668579102
Epoch 2010, training loss: 310.6715087890625 = 0.12754787504673004 + 50.0 * 6.210879802703857
Epoch 2010, val loss: 1.4996041059494019
Epoch 2020, training loss: 310.83050537109375 = 0.12547039985656738 + 50.0 * 6.2141008377075195
Epoch 2020, val loss: 1.505457878112793
Epoch 2030, training loss: 310.7412414550781 = 0.12336388230323792 + 50.0 * 6.212357997894287
Epoch 2030, val loss: 1.5118165016174316
Epoch 2040, training loss: 310.6624450683594 = 0.12130364775657654 + 50.0 * 6.210822582244873
Epoch 2040, val loss: 1.5166549682617188
Epoch 2050, training loss: 310.62274169921875 = 0.11932752281427383 + 50.0 * 6.210068225860596
Epoch 2050, val loss: 1.5234922170639038
Epoch 2060, training loss: 310.7695617675781 = 0.11741037666797638 + 50.0 * 6.213043212890625
Epoch 2060, val loss: 1.5289586782455444
Epoch 2070, training loss: 310.5809020996094 = 0.11547543853521347 + 50.0 * 6.209308624267578
Epoch 2070, val loss: 1.5350875854492188
Epoch 2080, training loss: 310.6317443847656 = 0.11359195411205292 + 50.0 * 6.210363388061523
Epoch 2080, val loss: 1.5410100221633911
Epoch 2090, training loss: 310.6589050292969 = 0.1117621511220932 + 50.0 * 6.210943222045898
Epoch 2090, val loss: 1.5471763610839844
Epoch 2100, training loss: 310.7289733886719 = 0.1099485456943512 + 50.0 * 6.212380409240723
Epoch 2100, val loss: 1.5527622699737549
Epoch 2110, training loss: 310.58319091796875 = 0.10808722674846649 + 50.0 * 6.209502220153809
Epoch 2110, val loss: 1.5588020086288452
Epoch 2120, training loss: 310.54766845703125 = 0.10631749033927917 + 50.0 * 6.208827018737793
Epoch 2120, val loss: 1.5645252466201782
Epoch 2130, training loss: 310.5052185058594 = 0.1046084389090538 + 50.0 * 6.208012104034424
Epoch 2130, val loss: 1.570959448814392
Epoch 2140, training loss: 310.5592041015625 = 0.10293643176555634 + 50.0 * 6.209125518798828
Epoch 2140, val loss: 1.576493740081787
Epoch 2150, training loss: 310.738525390625 = 0.10128965228796005 + 50.0 * 6.21274471282959
Epoch 2150, val loss: 1.5825556516647339
Epoch 2160, training loss: 310.65118408203125 = 0.09958010166883469 + 50.0 * 6.211031913757324
Epoch 2160, val loss: 1.589026927947998
Epoch 2170, training loss: 310.5194091796875 = 0.09795518964529037 + 50.0 * 6.208428859710693
Epoch 2170, val loss: 1.5944241285324097
Epoch 2180, training loss: 310.4603576660156 = 0.09639468044042587 + 50.0 * 6.207279205322266
Epoch 2180, val loss: 1.600874423980713
Epoch 2190, training loss: 310.4756164550781 = 0.09489782154560089 + 50.0 * 6.207613945007324
Epoch 2190, val loss: 1.6066043376922607
Epoch 2200, training loss: 310.7456970214844 = 0.09339789301156998 + 50.0 * 6.213046073913574
Epoch 2200, val loss: 1.6125181913375854
Epoch 2210, training loss: 310.5238342285156 = 0.09182974696159363 + 50.0 * 6.208640098571777
Epoch 2210, val loss: 1.6180994510650635
Epoch 2220, training loss: 310.5304260253906 = 0.09037226438522339 + 50.0 * 6.20880126953125
Epoch 2220, val loss: 1.6238703727722168
Epoch 2230, training loss: 310.5040588378906 = 0.08890867233276367 + 50.0 * 6.208303451538086
Epoch 2230, val loss: 1.629783272743225
Epoch 2240, training loss: 310.46075439453125 = 0.08748731762170792 + 50.0 * 6.207465171813965
Epoch 2240, val loss: 1.6358047723770142
Epoch 2250, training loss: 310.4719543457031 = 0.08611072599887848 + 50.0 * 6.207716464996338
Epoch 2250, val loss: 1.6414129734039307
Epoch 2260, training loss: 310.44158935546875 = 0.08474849909543991 + 50.0 * 6.207136631011963
Epoch 2260, val loss: 1.6468887329101562
Epoch 2270, training loss: 310.53424072265625 = 0.08343511074781418 + 50.0 * 6.209015846252441
Epoch 2270, val loss: 1.6530814170837402
Epoch 2280, training loss: 310.4095153808594 = 0.08207987248897552 + 50.0 * 6.206548690795898
Epoch 2280, val loss: 1.658202052116394
Epoch 2290, training loss: 310.4667663574219 = 0.08080784976482391 + 50.0 * 6.207719326019287
Epoch 2290, val loss: 1.6642695665359497
Epoch 2300, training loss: 310.3805847167969 = 0.07953152805566788 + 50.0 * 6.206020832061768
Epoch 2300, val loss: 1.670039176940918
Epoch 2310, training loss: 310.4154357910156 = 0.07831089943647385 + 50.0 * 6.206742763519287
Epoch 2310, val loss: 1.675516963005066
Epoch 2320, training loss: 310.3770751953125 = 0.07709085941314697 + 50.0 * 6.205999851226807
Epoch 2320, val loss: 1.6816034317016602
Epoch 2330, training loss: 310.3805236816406 = 0.07589951902627945 + 50.0 * 6.206092834472656
Epoch 2330, val loss: 1.687583327293396
Epoch 2340, training loss: 310.3984680175781 = 0.07471341639757156 + 50.0 * 6.206475257873535
Epoch 2340, val loss: 1.6928120851516724
Epoch 2350, training loss: 310.4263000488281 = 0.07357235997915268 + 50.0 * 6.207054138183594
Epoch 2350, val loss: 1.6983428001403809
Epoch 2360, training loss: 310.4081726074219 = 0.07242408394813538 + 50.0 * 6.206715106964111
Epoch 2360, val loss: 1.7037917375564575
Epoch 2370, training loss: 310.5736999511719 = 0.07129239290952682 + 50.0 * 6.210048198699951
Epoch 2370, val loss: 1.7092018127441406
Epoch 2380, training loss: 310.36700439453125 = 0.07016990333795547 + 50.0 * 6.205936908721924
Epoch 2380, val loss: 1.714722990989685
Epoch 2390, training loss: 310.3035888671875 = 0.06910166144371033 + 50.0 * 6.204689979553223
Epoch 2390, val loss: 1.7203750610351562
Epoch 2400, training loss: 310.3561096191406 = 0.06807912886142731 + 50.0 * 6.205760478973389
Epoch 2400, val loss: 1.726320743560791
Epoch 2410, training loss: 310.2879638671875 = 0.0670337975025177 + 50.0 * 6.204418182373047
Epoch 2410, val loss: 1.7313542366027832
Epoch 2420, training loss: 310.3523864746094 = 0.06603176146745682 + 50.0 * 6.2057271003723145
Epoch 2420, val loss: 1.736815094947815
Epoch 2430, training loss: 310.3078918457031 = 0.06502632051706314 + 50.0 * 6.204857349395752
Epoch 2430, val loss: 1.74225914478302
Epoch 2440, training loss: 310.2259826660156 = 0.06403528898954391 + 50.0 * 6.2032389640808105
Epoch 2440, val loss: 1.7476832866668701
Epoch 2450, training loss: 310.4388122558594 = 0.06309845298528671 + 50.0 * 6.20751428604126
Epoch 2450, val loss: 1.752496361732483
Epoch 2460, training loss: 310.3238830566406 = 0.062143757939338684 + 50.0 * 6.205235004425049
Epoch 2460, val loss: 1.7586381435394287
Epoch 2470, training loss: 310.31011962890625 = 0.061189599335193634 + 50.0 * 6.2049784660339355
Epoch 2470, val loss: 1.7638884782791138
Epoch 2480, training loss: 310.2412109375 = 0.06026212126016617 + 50.0 * 6.203619003295898
Epoch 2480, val loss: 1.7693874835968018
Epoch 2490, training loss: 310.21685791015625 = 0.059375129640102386 + 50.0 * 6.203149318695068
Epoch 2490, val loss: 1.774795651435852
Epoch 2500, training loss: 310.20562744140625 = 0.05851633846759796 + 50.0 * 6.202942371368408
Epoch 2500, val loss: 1.7801008224487305
Epoch 2510, training loss: 310.4288024902344 = 0.05768368020653725 + 50.0 * 6.207422256469727
Epoch 2510, val loss: 1.7851886749267578
Epoch 2520, training loss: 310.2322692871094 = 0.05680510401725769 + 50.0 * 6.203509330749512
Epoch 2520, val loss: 1.79057776927948
Epoch 2530, training loss: 310.23876953125 = 0.055981241166591644 + 50.0 * 6.203656196594238
Epoch 2530, val loss: 1.7954518795013428
Epoch 2540, training loss: 310.27691650390625 = 0.055164556950330734 + 50.0 * 6.204434871673584
Epoch 2540, val loss: 1.8010985851287842
Epoch 2550, training loss: 310.14813232421875 = 0.05434941127896309 + 50.0 * 6.201875686645508
Epoch 2550, val loss: 1.8064451217651367
Epoch 2560, training loss: 310.3443298339844 = 0.053568873554468155 + 50.0 * 6.205815315246582
Epoch 2560, val loss: 1.8117623329162598
Epoch 2570, training loss: 310.1614990234375 = 0.05278263986110687 + 50.0 * 6.202174186706543
Epoch 2570, val loss: 1.8161523342132568
Epoch 2580, training loss: 310.1299133300781 = 0.05199866741895676 + 50.0 * 6.2015581130981445
Epoch 2580, val loss: 1.821532130241394
Epoch 2590, training loss: 310.1123352050781 = 0.051269423216581345 + 50.0 * 6.201221466064453
Epoch 2590, val loss: 1.82659113407135
Epoch 2600, training loss: 310.17376708984375 = 0.05056484788656235 + 50.0 * 6.2024641036987305
Epoch 2600, val loss: 1.8315962553024292
Epoch 2610, training loss: 310.1672668457031 = 0.04985016584396362 + 50.0 * 6.202348709106445
Epoch 2610, val loss: 1.8367643356323242
Epoch 2620, training loss: 310.1325988769531 = 0.049142833799123764 + 50.0 * 6.201669216156006
Epoch 2620, val loss: 1.8414688110351562
Epoch 2630, training loss: 310.2986755371094 = 0.04845990613102913 + 50.0 * 6.2050042152404785
Epoch 2630, val loss: 1.846028208732605
Epoch 2640, training loss: 310.1636962890625 = 0.047761887311935425 + 50.0 * 6.2023186683654785
Epoch 2640, val loss: 1.851821780204773
Epoch 2650, training loss: 310.29931640625 = 0.04709608852863312 + 50.0 * 6.205044746398926
Epoch 2650, val loss: 1.8565688133239746
Epoch 2660, training loss: 310.1112060546875 = 0.04641735553741455 + 50.0 * 6.201295852661133
Epoch 2660, val loss: 1.861334204673767
Epoch 2670, training loss: 310.16217041015625 = 0.04576857388019562 + 50.0 * 6.202327728271484
Epoch 2670, val loss: 1.866203784942627
Epoch 2680, training loss: 310.1332092285156 = 0.04514298960566521 + 50.0 * 6.201761722564697
Epoch 2680, val loss: 1.8713217973709106
Epoch 2690, training loss: 310.041259765625 = 0.04451727494597435 + 50.0 * 6.199934959411621
Epoch 2690, val loss: 1.8754847049713135
Epoch 2700, training loss: 310.09130859375 = 0.04393178969621658 + 50.0 * 6.200947284698486
Epoch 2700, val loss: 1.880238652229309
Epoch 2710, training loss: 310.31103515625 = 0.04334985837340355 + 50.0 * 6.205353736877441
Epoch 2710, val loss: 1.8854414224624634
Epoch 2720, training loss: 310.1269226074219 = 0.0427246168255806 + 50.0 * 6.20168399810791
Epoch 2720, val loss: 1.8897596597671509
Epoch 2730, training loss: 310.0486755371094 = 0.04213332384824753 + 50.0 * 6.200130462646484
Epoch 2730, val loss: 1.894325613975525
Epoch 2740, training loss: 310.0717468261719 = 0.04157024994492531 + 50.0 * 6.200603485107422
Epoch 2740, val loss: 1.8997865915298462
Epoch 2750, training loss: 310.1730041503906 = 0.04101793095469475 + 50.0 * 6.202640056610107
Epoch 2750, val loss: 1.9037160873413086
Epoch 2760, training loss: 310.07666015625 = 0.040457796305418015 + 50.0 * 6.200723648071289
Epoch 2760, val loss: 1.9084588289260864
Epoch 2770, training loss: 310.0710144042969 = 0.03992675244808197 + 50.0 * 6.200622081756592
Epoch 2770, val loss: 1.9134989976882935
Epoch 2780, training loss: 310.0352783203125 = 0.039395883679389954 + 50.0 * 6.199917316436768
Epoch 2780, val loss: 1.9177377223968506
Epoch 2790, training loss: 310.0091247558594 = 0.03889067471027374 + 50.0 * 6.199404716491699
Epoch 2790, val loss: 1.9220633506774902
Epoch 2800, training loss: 310.2214660644531 = 0.03839873895049095 + 50.0 * 6.2036614418029785
Epoch 2800, val loss: 1.9263585805892944
Epoch 2810, training loss: 310.25604248046875 = 0.03785466402769089 + 50.0 * 6.204363822937012
Epoch 2810, val loss: 1.9310938119888306
Epoch 2820, training loss: 310.047607421875 = 0.03735437989234924 + 50.0 * 6.200205326080322
Epoch 2820, val loss: 1.9349067211151123
Epoch 2830, training loss: 309.9444580078125 = 0.03686027601361275 + 50.0 * 6.1981520652771
Epoch 2830, val loss: 1.9398740530014038
Epoch 2840, training loss: 309.9193115234375 = 0.036410003900527954 + 50.0 * 6.197658061981201
Epoch 2840, val loss: 1.9443833827972412
Epoch 2850, training loss: 309.9348449707031 = 0.035964399576187134 + 50.0 * 6.1979780197143555
Epoch 2850, val loss: 1.9490035772323608
Epoch 2860, training loss: 310.56219482421875 = 0.03552950918674469 + 50.0 * 6.210533618927002
Epoch 2860, val loss: 1.9532909393310547
Epoch 2870, training loss: 310.1744079589844 = 0.03503040596842766 + 50.0 * 6.202787399291992
Epoch 2870, val loss: 1.9566960334777832
Epoch 2880, training loss: 310.0263366699219 = 0.03456311300396919 + 50.0 * 6.199835300445557
Epoch 2880, val loss: 1.9610439538955688
Epoch 2890, training loss: 309.92327880859375 = 0.0341327041387558 + 50.0 * 6.197783470153809
Epoch 2890, val loss: 1.965272307395935
Epoch 2900, training loss: 309.9173583984375 = 0.03371956944465637 + 50.0 * 6.1976728439331055
Epoch 2900, val loss: 1.9696745872497559
Epoch 2910, training loss: 310.10321044921875 = 0.03332210332155228 + 50.0 * 6.20139741897583
Epoch 2910, val loss: 1.9731488227844238
Epoch 2920, training loss: 309.98516845703125 = 0.032902952283620834 + 50.0 * 6.199045658111572
Epoch 2920, val loss: 1.978027105331421
Epoch 2930, training loss: 309.9344482421875 = 0.03248395398259163 + 50.0 * 6.198039531707764
Epoch 2930, val loss: 1.9819953441619873
Epoch 2940, training loss: 309.9891662597656 = 0.03208054229617119 + 50.0 * 6.199141979217529
Epoch 2940, val loss: 1.9862017631530762
Epoch 2950, training loss: 309.9982604980469 = 0.03169294074177742 + 50.0 * 6.199331283569336
Epoch 2950, val loss: 1.9900603294372559
Epoch 2960, training loss: 309.8907470703125 = 0.03129681199789047 + 50.0 * 6.197188854217529
Epoch 2960, val loss: 1.9942271709442139
Epoch 2970, training loss: 309.955322265625 = 0.03093176893889904 + 50.0 * 6.198487281799316
Epoch 2970, val loss: 1.998655915260315
Epoch 2980, training loss: 310.0838928222656 = 0.03056361898779869 + 50.0 * 6.201066493988037
Epoch 2980, val loss: 2.0020484924316406
Epoch 2990, training loss: 309.90008544921875 = 0.030184827744960785 + 50.0 * 6.1973981857299805
Epoch 2990, val loss: 2.005927085876465
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.674074074074074
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 431.79010009765625 = 1.9493967294692993 + 50.0 * 8.596814155578613
Epoch 0, val loss: 1.9564597606658936
Epoch 10, training loss: 431.7326965332031 = 1.940838098526001 + 50.0 * 8.595837593078613
Epoch 10, val loss: 1.947153091430664
Epoch 20, training loss: 431.3740234375 = 1.9301834106445312 + 50.0 * 8.588876724243164
Epoch 20, val loss: 1.9355632066726685
Epoch 30, training loss: 428.958984375 = 1.9164412021636963 + 50.0 * 8.540850639343262
Epoch 30, val loss: 1.9203940629959106
Epoch 40, training loss: 414.521728515625 = 1.9008058309555054 + 50.0 * 8.252418518066406
Epoch 40, val loss: 1.90342116355896
Epoch 50, training loss: 382.6584777832031 = 1.8843762874603271 + 50.0 * 7.615482330322266
Epoch 50, val loss: 1.8866475820541382
Epoch 60, training loss: 362.0633544921875 = 1.8727837800979614 + 50.0 * 7.2038116455078125
Epoch 60, val loss: 1.8763002157211304
Epoch 70, training loss: 349.9619445800781 = 1.861130952835083 + 50.0 * 6.9620161056518555
Epoch 70, val loss: 1.8650569915771484
Epoch 80, training loss: 342.975830078125 = 1.8516517877578735 + 50.0 * 6.822483539581299
Epoch 80, val loss: 1.8557093143463135
Epoch 90, training loss: 338.442138671875 = 1.8423430919647217 + 50.0 * 6.731996059417725
Epoch 90, val loss: 1.8463112115859985
Epoch 100, training loss: 335.2176818847656 = 1.8336573839187622 + 50.0 * 6.667680740356445
Epoch 100, val loss: 1.8374252319335938
Epoch 110, training loss: 332.8451843261719 = 1.8259837627410889 + 50.0 * 6.6203837394714355
Epoch 110, val loss: 1.8294086456298828
Epoch 120, training loss: 331.1239318847656 = 1.819051742553711 + 50.0 * 6.586097717285156
Epoch 120, val loss: 1.821937918663025
Epoch 130, training loss: 329.7236633300781 = 1.8124196529388428 + 50.0 * 6.558225154876709
Epoch 130, val loss: 1.8147203922271729
Epoch 140, training loss: 328.61016845703125 = 1.8059049844741821 + 50.0 * 6.53608512878418
Epoch 140, val loss: 1.8077000379562378
Epoch 150, training loss: 327.5880432128906 = 1.799443244934082 + 50.0 * 6.515771865844727
Epoch 150, val loss: 1.800823450088501
Epoch 160, training loss: 326.7127990722656 = 1.7928446531295776 + 50.0 * 6.498399257659912
Epoch 160, val loss: 1.793962001800537
Epoch 170, training loss: 326.01129150390625 = 1.7859269380569458 + 50.0 * 6.4845075607299805
Epoch 170, val loss: 1.7869623899459839
Epoch 180, training loss: 325.2603454589844 = 1.778506875038147 + 50.0 * 6.469636917114258
Epoch 180, val loss: 1.7796837091445923
Epoch 190, training loss: 324.73101806640625 = 1.770482063293457 + 50.0 * 6.4592108726501465
Epoch 190, val loss: 1.7719682455062866
Epoch 200, training loss: 324.0924987792969 = 1.7617727518081665 + 50.0 * 6.4466142654418945
Epoch 200, val loss: 1.763695478439331
Epoch 210, training loss: 323.5780334472656 = 1.7522854804992676 + 50.0 * 6.436514854431152
Epoch 210, val loss: 1.7547467947006226
Epoch 220, training loss: 323.10296630859375 = 1.741898536682129 + 50.0 * 6.427221298217773
Epoch 220, val loss: 1.745035171508789
Epoch 230, training loss: 322.7376403808594 = 1.7305526733398438 + 50.0 * 6.420141696929932
Epoch 230, val loss: 1.7344225645065308
Epoch 240, training loss: 322.2207336425781 = 1.7180545330047607 + 50.0 * 6.410053730010986
Epoch 240, val loss: 1.7227879762649536
Epoch 250, training loss: 321.79803466796875 = 1.7044426202774048 + 50.0 * 6.401872158050537
Epoch 250, val loss: 1.7101293802261353
Epoch 260, training loss: 321.4471130371094 = 1.6896095275878906 + 50.0 * 6.395150184631348
Epoch 260, val loss: 1.6962774991989136
Epoch 270, training loss: 321.13836669921875 = 1.6734082698822021 + 50.0 * 6.389299392700195
Epoch 270, val loss: 1.6812494993209839
Epoch 280, training loss: 320.78668212890625 = 1.6559793949127197 + 50.0 * 6.3826141357421875
Epoch 280, val loss: 1.664933204650879
Epoch 290, training loss: 320.46026611328125 = 1.637247085571289 + 50.0 * 6.376460552215576
Epoch 290, val loss: 1.6475324630737305
Epoch 300, training loss: 320.1461181640625 = 1.617299199104309 + 50.0 * 6.37057638168335
Epoch 300, val loss: 1.6290098428726196
Epoch 310, training loss: 320.1280212402344 = 1.5961823463439941 + 50.0 * 6.370636463165283
Epoch 310, val loss: 1.609455943107605
Epoch 320, training loss: 319.7031555175781 = 1.574025273323059 + 50.0 * 6.362582206726074
Epoch 320, val loss: 1.589079737663269
Epoch 330, training loss: 319.4373474121094 = 1.5509250164031982 + 50.0 * 6.357728481292725
Epoch 330, val loss: 1.5680447816848755
Epoch 340, training loss: 319.1683349609375 = 1.5271573066711426 + 50.0 * 6.352823734283447
Epoch 340, val loss: 1.5465681552886963
Epoch 350, training loss: 319.36669921875 = 1.5028303861618042 + 50.0 * 6.3572773933410645
Epoch 350, val loss: 1.5247224569320679
Epoch 360, training loss: 318.8637390136719 = 1.4779272079467773 + 50.0 * 6.347715854644775
Epoch 360, val loss: 1.5026748180389404
Epoch 370, training loss: 318.6123962402344 = 1.452825903892517 + 50.0 * 6.343191623687744
Epoch 370, val loss: 1.48067045211792
Epoch 380, training loss: 318.4239501953125 = 1.4276713132858276 + 50.0 * 6.339925765991211
Epoch 380, val loss: 1.4589133262634277
Epoch 390, training loss: 318.1611633300781 = 1.402496337890625 + 50.0 * 6.335173606872559
Epoch 390, val loss: 1.437389850616455
Epoch 400, training loss: 318.09912109375 = 1.3772990703582764 + 50.0 * 6.334436416625977
Epoch 400, val loss: 1.4161875247955322
Epoch 410, training loss: 317.91339111328125 = 1.3523508310317993 + 50.0 * 6.331220626831055
Epoch 410, val loss: 1.3953578472137451
Epoch 420, training loss: 317.6571350097656 = 1.3272583484649658 + 50.0 * 6.326597213745117
Epoch 420, val loss: 1.3748987913131714
Epoch 430, training loss: 317.4912109375 = 1.3025380373001099 + 50.0 * 6.32377290725708
Epoch 430, val loss: 1.3549870252609253
Epoch 440, training loss: 317.7548522949219 = 1.2780959606170654 + 50.0 * 6.329535007476807
Epoch 440, val loss: 1.3354747295379639
Epoch 450, training loss: 317.2713623046875 = 1.2537339925765991 + 50.0 * 6.320352554321289
Epoch 450, val loss: 1.3162939548492432
Epoch 460, training loss: 317.0208435058594 = 1.2295747995376587 + 50.0 * 6.315825462341309
Epoch 460, val loss: 1.2977237701416016
Epoch 470, training loss: 316.8816223144531 = 1.205776572227478 + 50.0 * 6.313516616821289
Epoch 470, val loss: 1.2796132564544678
Epoch 480, training loss: 317.1949462890625 = 1.1822940111160278 + 50.0 * 6.320253372192383
Epoch 480, val loss: 1.261876106262207
Epoch 490, training loss: 316.6361389160156 = 1.1590754985809326 + 50.0 * 6.30954122543335
Epoch 490, val loss: 1.244802474975586
Epoch 500, training loss: 316.5183410644531 = 1.1363139152526855 + 50.0 * 6.307640075683594
Epoch 500, val loss: 1.228209137916565
Epoch 510, training loss: 316.5648498535156 = 1.1138980388641357 + 50.0 * 6.309019088745117
Epoch 510, val loss: 1.212142825126648
Epoch 520, training loss: 316.4718933105469 = 1.0918519496917725 + 50.0 * 6.307600975036621
Epoch 520, val loss: 1.19667387008667
Epoch 530, training loss: 316.1618957519531 = 1.0701438188552856 + 50.0 * 6.301835060119629
Epoch 530, val loss: 1.181596279144287
Epoch 540, training loss: 315.99713134765625 = 1.0489767789840698 + 50.0 * 6.2989630699157715
Epoch 540, val loss: 1.167372226715088
Epoch 550, training loss: 316.16021728515625 = 1.028244137763977 + 50.0 * 6.302639007568359
Epoch 550, val loss: 1.1537952423095703
Epoch 560, training loss: 316.0240478515625 = 1.0080692768096924 + 50.0 * 6.300319671630859
Epoch 560, val loss: 1.1407009363174438
Epoch 570, training loss: 315.6830139160156 = 0.9881765246391296 + 50.0 * 6.293896198272705
Epoch 570, val loss: 1.1283165216445923
Epoch 580, training loss: 315.60394287109375 = 0.9688758850097656 + 50.0 * 6.292701721191406
Epoch 580, val loss: 1.1166102886199951
Epoch 590, training loss: 315.7138671875 = 0.9500330686569214 + 50.0 * 6.295276641845703
Epoch 590, val loss: 1.105514407157898
Epoch 600, training loss: 315.41375732421875 = 0.9315811395645142 + 50.0 * 6.28964376449585
Epoch 600, val loss: 1.0950074195861816
Epoch 610, training loss: 315.3392028808594 = 0.9135705232620239 + 50.0 * 6.288512706756592
Epoch 610, val loss: 1.0852186679840088
Epoch 620, training loss: 315.2949523925781 = 0.8959878087043762 + 50.0 * 6.2879791259765625
Epoch 620, val loss: 1.075728178024292
Epoch 630, training loss: 315.2964782714844 = 0.8787931203842163 + 50.0 * 6.28835391998291
Epoch 630, val loss: 1.0668607950210571
Epoch 640, training loss: 315.0912780761719 = 0.8619568943977356 + 50.0 * 6.284586429595947
Epoch 640, val loss: 1.0587607622146606
Epoch 650, training loss: 315.0296325683594 = 0.8453956246376038 + 50.0 * 6.283684730529785
Epoch 650, val loss: 1.0506932735443115
Epoch 660, training loss: 314.96295166015625 = 0.8291890025138855 + 50.0 * 6.282675266265869
Epoch 660, val loss: 1.0432868003845215
Epoch 670, training loss: 314.7959899902344 = 0.8131670951843262 + 50.0 * 6.279656410217285
Epoch 670, val loss: 1.0362775325775146
Epoch 680, training loss: 314.75531005859375 = 0.7975810766220093 + 50.0 * 6.2791547775268555
Epoch 680, val loss: 1.0297781229019165
Epoch 690, training loss: 314.749755859375 = 0.7822198271751404 + 50.0 * 6.279350757598877
Epoch 690, val loss: 1.0236217975616455
Epoch 700, training loss: 314.8273620605469 = 0.7670773267745972 + 50.0 * 6.281205177307129
Epoch 700, val loss: 1.0175796747207642
Epoch 710, training loss: 314.5547790527344 = 0.7521711587905884 + 50.0 * 6.276052474975586
Epoch 710, val loss: 1.0123229026794434
Epoch 720, training loss: 314.418701171875 = 0.7375330328941345 + 50.0 * 6.273623943328857
Epoch 720, val loss: 1.0070518255233765
Epoch 730, training loss: 314.3271179199219 = 0.723253071308136 + 50.0 * 6.2720770835876465
Epoch 730, val loss: 1.002449870109558
Epoch 740, training loss: 314.31341552734375 = 0.709177553653717 + 50.0 * 6.272085189819336
Epoch 740, val loss: 0.9980117082595825
Epoch 750, training loss: 314.46661376953125 = 0.6951767802238464 + 50.0 * 6.275428771972656
Epoch 750, val loss: 0.9936716556549072
Epoch 760, training loss: 314.33837890625 = 0.6814330816268921 + 50.0 * 6.273138999938965
Epoch 760, val loss: 0.9898635149002075
Epoch 770, training loss: 314.0817565917969 = 0.6679440140724182 + 50.0 * 6.268276214599609
Epoch 770, val loss: 0.9862469434738159
Epoch 780, training loss: 314.0404357910156 = 0.6548108458518982 + 50.0 * 6.267712116241455
Epoch 780, val loss: 0.9829093813896179
Epoch 790, training loss: 314.0302734375 = 0.641924262046814 + 50.0 * 6.267766952514648
Epoch 790, val loss: 0.9800649881362915
Epoch 800, training loss: 313.935791015625 = 0.6292150616645813 + 50.0 * 6.266131401062012
Epoch 800, val loss: 0.9774289727210999
Epoch 810, training loss: 313.9526062011719 = 0.6167514324188232 + 50.0 * 6.266716957092285
Epoch 810, val loss: 0.9751701354980469
Epoch 820, training loss: 313.8483581542969 = 0.6043623685836792 + 50.0 * 6.2648797035217285
Epoch 820, val loss: 0.9725595116615295
Epoch 830, training loss: 313.77667236328125 = 0.5923728346824646 + 50.0 * 6.263686180114746
Epoch 830, val loss: 0.9709238409996033
Epoch 840, training loss: 313.65045166015625 = 0.5806237459182739 + 50.0 * 6.261396408081055
Epoch 840, val loss: 0.9693763852119446
Epoch 850, training loss: 313.6722412109375 = 0.5691887736320496 + 50.0 * 6.26206111907959
Epoch 850, val loss: 0.9680260419845581
Epoch 860, training loss: 313.635498046875 = 0.5578954815864563 + 50.0 * 6.261551856994629
Epoch 860, val loss: 0.966855525970459
Epoch 870, training loss: 313.602294921875 = 0.5467561483383179 + 50.0 * 6.261110782623291
Epoch 870, val loss: 0.9657068252563477
Epoch 880, training loss: 313.5312194824219 = 0.535831093788147 + 50.0 * 6.2599077224731445
Epoch 880, val loss: 0.9646638035774231
Epoch 890, training loss: 313.5260009765625 = 0.5251749753952026 + 50.0 * 6.260016441345215
Epoch 890, val loss: 0.9642401933670044
Epoch 900, training loss: 313.3819885253906 = 0.5147535800933838 + 50.0 * 6.257345199584961
Epoch 900, val loss: 0.9638590216636658
Epoch 910, training loss: 313.31390380859375 = 0.5046665668487549 + 50.0 * 6.2561845779418945
Epoch 910, val loss: 0.9638084173202515
Epoch 920, training loss: 313.4727478027344 = 0.49481984972953796 + 50.0 * 6.25955867767334
Epoch 920, val loss: 0.9640763998031616
Epoch 930, training loss: 313.23046875 = 0.4849555194377899 + 50.0 * 6.254909992218018
Epoch 930, val loss: 0.9641087055206299
Epoch 940, training loss: 313.35736083984375 = 0.4754656255245209 + 50.0 * 6.257637977600098
Epoch 940, val loss: 0.9646451473236084
Epoch 950, training loss: 313.1575927734375 = 0.46603628993034363 + 50.0 * 6.253830909729004
Epoch 950, val loss: 0.964962899684906
Epoch 960, training loss: 313.1112365722656 = 0.4569830894470215 + 50.0 * 6.253085136413574
Epoch 960, val loss: 0.9659634232521057
Epoch 970, training loss: 313.242431640625 = 0.44808685779571533 + 50.0 * 6.255886554718018
Epoch 970, val loss: 0.9669085741043091
Epoch 980, training loss: 313.0928039550781 = 0.4393167793750763 + 50.0 * 6.2530694007873535
Epoch 980, val loss: 0.9680548906326294
Epoch 990, training loss: 313.04443359375 = 0.43068623542785645 + 50.0 * 6.252274513244629
Epoch 990, val loss: 0.9691578149795532
Epoch 1000, training loss: 313.1310729980469 = 0.4223724901676178 + 50.0 * 6.25417423248291
Epoch 1000, val loss: 0.9706827998161316
Epoch 1010, training loss: 312.8874206542969 = 0.41419368982315063 + 50.0 * 6.249464511871338
Epoch 1010, val loss: 0.9725465178489685
Epoch 1020, training loss: 312.8422546386719 = 0.4061955511569977 + 50.0 * 6.248721122741699
Epoch 1020, val loss: 0.9743216037750244
Epoch 1030, training loss: 312.9639892578125 = 0.3984634280204773 + 50.0 * 6.251310348510742
Epoch 1030, val loss: 0.9763630032539368
Epoch 1040, training loss: 312.9595031738281 = 0.3906809091567993 + 50.0 * 6.251376628875732
Epoch 1040, val loss: 0.9781324863433838
Epoch 1050, training loss: 312.81463623046875 = 0.38314229249954224 + 50.0 * 6.248629570007324
Epoch 1050, val loss: 0.9803975224494934
Epoch 1060, training loss: 312.6825866699219 = 0.37583792209625244 + 50.0 * 6.246135234832764
Epoch 1060, val loss: 0.9827650189399719
Epoch 1070, training loss: 312.6590881347656 = 0.36869341135025024 + 50.0 * 6.245808124542236
Epoch 1070, val loss: 0.9852398633956909
Epoch 1080, training loss: 313.1013488769531 = 0.36169934272766113 + 50.0 * 6.254793167114258
Epoch 1080, val loss: 0.9875649809837341
Epoch 1090, training loss: 312.8053894042969 = 0.35479748249053955 + 50.0 * 6.249011993408203
Epoch 1090, val loss: 0.9905813932418823
Epoch 1100, training loss: 312.7607421875 = 0.3479491174221039 + 50.0 * 6.248255729675293
Epoch 1100, val loss: 0.9929017424583435
Epoch 1110, training loss: 312.58636474609375 = 0.3412957787513733 + 50.0 * 6.244901180267334
Epoch 1110, val loss: 0.9956961274147034
Epoch 1120, training loss: 312.4620666503906 = 0.33488139510154724 + 50.0 * 6.242543697357178
Epoch 1120, val loss: 0.9987197518348694
Epoch 1130, training loss: 312.5182189941406 = 0.3285975158214569 + 50.0 * 6.24379301071167
Epoch 1130, val loss: 1.002050518989563
Epoch 1140, training loss: 312.6844482421875 = 0.3223639130592346 + 50.0 * 6.247241973876953
Epoch 1140, val loss: 1.0051162242889404
Epoch 1150, training loss: 312.4475402832031 = 0.31614524126052856 + 50.0 * 6.24262809753418
Epoch 1150, val loss: 1.007904052734375
Epoch 1160, training loss: 312.35968017578125 = 0.310140997171402 + 50.0 * 6.24099063873291
Epoch 1160, val loss: 1.0111150741577148
Epoch 1170, training loss: 312.39349365234375 = 0.30432212352752686 + 50.0 * 6.241783618927002
Epoch 1170, val loss: 1.0147109031677246
Epoch 1180, training loss: 312.386962890625 = 0.2985580563545227 + 50.0 * 6.2417683601379395
Epoch 1180, val loss: 1.0179821252822876
Epoch 1190, training loss: 312.3542785644531 = 0.2928982377052307 + 50.0 * 6.241227626800537
Epoch 1190, val loss: 1.0215648412704468
Epoch 1200, training loss: 312.3119812011719 = 0.2873467206954956 + 50.0 * 6.240492343902588
Epoch 1200, val loss: 1.0251283645629883
Epoch 1210, training loss: 312.2277526855469 = 0.28189703822135925 + 50.0 * 6.238917350769043
Epoch 1210, val loss: 1.0290071964263916
Epoch 1220, training loss: 312.267333984375 = 0.27654725313186646 + 50.0 * 6.239815711975098
Epoch 1220, val loss: 1.0327224731445312
Epoch 1230, training loss: 312.2726135253906 = 0.27130451798439026 + 50.0 * 6.240026473999023
Epoch 1230, val loss: 1.0363386869430542
Epoch 1240, training loss: 312.2343444824219 = 0.26610851287841797 + 50.0 * 6.2393646240234375
Epoch 1240, val loss: 1.0398669242858887
Epoch 1250, training loss: 312.1455993652344 = 0.26104262471199036 + 50.0 * 6.2376909255981445
Epoch 1250, val loss: 1.0440669059753418
Epoch 1260, training loss: 312.3545837402344 = 0.25603413581848145 + 50.0 * 6.241971015930176
Epoch 1260, val loss: 1.047377347946167
Epoch 1270, training loss: 312.1309814453125 = 0.2510879635810852 + 50.0 * 6.237597465515137
Epoch 1270, val loss: 1.0515773296356201
Epoch 1280, training loss: 312.0597839355469 = 0.2462739497423172 + 50.0 * 6.236270427703857
Epoch 1280, val loss: 1.0554437637329102
Epoch 1290, training loss: 312.2167053222656 = 0.24155518412590027 + 50.0 * 6.239502906799316
Epoch 1290, val loss: 1.0590730905532837
Epoch 1300, training loss: 312.0513916015625 = 0.23687861859798431 + 50.0 * 6.236290454864502
Epoch 1300, val loss: 1.0639694929122925
Epoch 1310, training loss: 312.1041259765625 = 0.232258602976799 + 50.0 * 6.2374372482299805
Epoch 1310, val loss: 1.0674282312393188
Epoch 1320, training loss: 311.9166564941406 = 0.22772887349128723 + 50.0 * 6.233778476715088
Epoch 1320, val loss: 1.0719114542007446
Epoch 1330, training loss: 312.0490417480469 = 0.22332701086997986 + 50.0 * 6.236514091491699
Epoch 1330, val loss: 1.075954794883728
Epoch 1340, training loss: 311.9578857421875 = 0.21892701089382172 + 50.0 * 6.234778881072998
Epoch 1340, val loss: 1.0800009965896606
Epoch 1350, training loss: 311.8356018066406 = 0.214573934674263 + 50.0 * 6.232420444488525
Epoch 1350, val loss: 1.0839239358901978
Epoch 1360, training loss: 311.8292236328125 = 0.21036265790462494 + 50.0 * 6.232377052307129
Epoch 1360, val loss: 1.0881462097167969
Epoch 1370, training loss: 311.93505859375 = 0.20622368156909943 + 50.0 * 6.234576225280762
Epoch 1370, val loss: 1.0920312404632568
Epoch 1380, training loss: 311.938232421875 = 0.20213791728019714 + 50.0 * 6.234721660614014
Epoch 1380, val loss: 1.0966131687164307
Epoch 1390, training loss: 311.8165588378906 = 0.19803431630134583 + 50.0 * 6.232370376586914
Epoch 1390, val loss: 1.100266933441162
Epoch 1400, training loss: 311.791748046875 = 0.19409240782260895 + 50.0 * 6.231953144073486
Epoch 1400, val loss: 1.1049017906188965
Epoch 1410, training loss: 312.00146484375 = 0.1901727169752121 + 50.0 * 6.236225605010986
Epoch 1410, val loss: 1.1085048913955688
Epoch 1420, training loss: 311.9166259765625 = 0.18632321059703827 + 50.0 * 6.2346062660217285
Epoch 1420, val loss: 1.1132997274398804
Epoch 1430, training loss: 311.68902587890625 = 0.1825299859046936 + 50.0 * 6.230129718780518
Epoch 1430, val loss: 1.117256760597229
Epoch 1440, training loss: 311.6543273925781 = 0.17887288331985474 + 50.0 * 6.229509353637695
Epoch 1440, val loss: 1.1215026378631592
Epoch 1450, training loss: 311.7084655761719 = 0.17528383433818817 + 50.0 * 6.230663299560547
Epoch 1450, val loss: 1.1257190704345703
Epoch 1460, training loss: 311.7220153808594 = 0.17170476913452148 + 50.0 * 6.231006622314453
Epoch 1460, val loss: 1.1296899318695068
Epoch 1470, training loss: 311.74688720703125 = 0.1681932657957077 + 50.0 * 6.231574058532715
Epoch 1470, val loss: 1.1343141794204712
Epoch 1480, training loss: 311.69091796875 = 0.16474701464176178 + 50.0 * 6.230523586273193
Epoch 1480, val loss: 1.138595461845398
Epoch 1490, training loss: 311.7220458984375 = 0.1613861471414566 + 50.0 * 6.231213092803955
Epoch 1490, val loss: 1.1430892944335938
Epoch 1500, training loss: 311.5636291503906 = 0.15804262459278107 + 50.0 * 6.228111743927002
Epoch 1500, val loss: 1.146666407585144
Epoch 1510, training loss: 311.5181884765625 = 0.15481159090995789 + 50.0 * 6.227267742156982
Epoch 1510, val loss: 1.1513376235961914
Epoch 1520, training loss: 311.9193115234375 = 0.15168607234954834 + 50.0 * 6.235352993011475
Epoch 1520, val loss: 1.155848741531372
Epoch 1530, training loss: 311.67218017578125 = 0.14845484495162964 + 50.0 * 6.230474472045898
Epoch 1530, val loss: 1.1596068143844604
Epoch 1540, training loss: 311.48681640625 = 0.14538848400115967 + 50.0 * 6.226828575134277
Epoch 1540, val loss: 1.1640208959579468
Epoch 1550, training loss: 311.43121337890625 = 0.1424005776643753 + 50.0 * 6.225776195526123
Epoch 1550, val loss: 1.1680275201797485
Epoch 1560, training loss: 311.471923828125 = 0.13950157165527344 + 50.0 * 6.226648807525635
Epoch 1560, val loss: 1.172554612159729
Epoch 1570, training loss: 311.6553039550781 = 0.1366203874349594 + 50.0 * 6.230373382568359
Epoch 1570, val loss: 1.1767998933792114
Epoch 1580, training loss: 311.5575866699219 = 0.13374578952789307 + 50.0 * 6.228476524353027
Epoch 1580, val loss: 1.1810956001281738
Epoch 1590, training loss: 311.3783264160156 = 0.1309603452682495 + 50.0 * 6.224947452545166
Epoch 1590, val loss: 1.1849161386489868
Epoch 1600, training loss: 311.3131103515625 = 0.12827923893928528 + 50.0 * 6.223696708679199
Epoch 1600, val loss: 1.1893113851547241
Epoch 1610, training loss: 311.3011474609375 = 0.12567384541034698 + 50.0 * 6.223509311676025
Epoch 1610, val loss: 1.1936792135238647
Epoch 1620, training loss: 311.5544128417969 = 0.12312507629394531 + 50.0 * 6.228625297546387
Epoch 1620, val loss: 1.1977100372314453
Epoch 1630, training loss: 311.46282958984375 = 0.12053380161523819 + 50.0 * 6.226845741271973
Epoch 1630, val loss: 1.2021061182022095
Epoch 1640, training loss: 311.3349304199219 = 0.11803176254034042 + 50.0 * 6.224338054656982
Epoch 1640, val loss: 1.206316351890564
Epoch 1650, training loss: 311.2983093261719 = 0.11560937017202377 + 50.0 * 6.223654270172119
Epoch 1650, val loss: 1.21049964427948
Epoch 1660, training loss: 311.6654357910156 = 0.11328401416540146 + 50.0 * 6.231043338775635
Epoch 1660, val loss: 1.2153135538101196
Epoch 1670, training loss: 311.35369873046875 = 0.11089562624692917 + 50.0 * 6.224855899810791
Epoch 1670, val loss: 1.2183334827423096
Epoch 1680, training loss: 311.3200988769531 = 0.10863831639289856 + 50.0 * 6.224229335784912
Epoch 1680, val loss: 1.2231107950210571
Epoch 1690, training loss: 311.209716796875 = 0.10641223192214966 + 50.0 * 6.2220659255981445
Epoch 1690, val loss: 1.2271618843078613
Epoch 1700, training loss: 311.16912841796875 = 0.1042664647102356 + 50.0 * 6.221297264099121
Epoch 1700, val loss: 1.2317149639129639
Epoch 1710, training loss: 311.2464904785156 = 0.1021830290555954 + 50.0 * 6.222886085510254
Epoch 1710, val loss: 1.2358818054199219
Epoch 1720, training loss: 311.3179016113281 = 0.10010608285665512 + 50.0 * 6.224356174468994
Epoch 1720, val loss: 1.2401444911956787
Epoch 1730, training loss: 311.2297058105469 = 0.09804023057222366 + 50.0 * 6.222632884979248
Epoch 1730, val loss: 1.24402916431427
Epoch 1740, training loss: 311.29962158203125 = 0.09605927765369415 + 50.0 * 6.224071025848389
Epoch 1740, val loss: 1.2487568855285645
Epoch 1750, training loss: 311.09796142578125 = 0.09410133212804794 + 50.0 * 6.220077037811279
Epoch 1750, val loss: 1.2524338960647583
Epoch 1760, training loss: 311.0897216796875 = 0.09223262965679169 + 50.0 * 6.219950199127197
Epoch 1760, val loss: 1.2569708824157715
Epoch 1770, training loss: 311.056396484375 = 0.09040119498968124 + 50.0 * 6.219319820404053
Epoch 1770, val loss: 1.2614076137542725
Epoch 1780, training loss: 311.3530578613281 = 0.08862047642469406 + 50.0 * 6.2252888679504395
Epoch 1780, val loss: 1.2654660940170288
Epoch 1790, training loss: 311.0837097167969 = 0.08681832998991013 + 50.0 * 6.219937801361084
Epoch 1790, val loss: 1.2696971893310547
Epoch 1800, training loss: 311.0328063964844 = 0.08508016169071198 + 50.0 * 6.218954563140869
Epoch 1800, val loss: 1.2739267349243164
Epoch 1810, training loss: 311.1600341796875 = 0.083411805331707 + 50.0 * 6.221532344818115
Epoch 1810, val loss: 1.2784126996994019
Epoch 1820, training loss: 311.26678466796875 = 0.0817263126373291 + 50.0 * 6.223701000213623
Epoch 1820, val loss: 1.281492829322815
Epoch 1830, training loss: 311.0202941894531 = 0.08010152727365494 + 50.0 * 6.218803405761719
Epoch 1830, val loss: 1.286643147468567
Epoch 1840, training loss: 310.96661376953125 = 0.07851850986480713 + 50.0 * 6.217761993408203
Epoch 1840, val loss: 1.290297031402588
Epoch 1850, training loss: 310.9395751953125 = 0.07700631022453308 + 50.0 * 6.217251300811768
Epoch 1850, val loss: 1.2945815324783325
Epoch 1860, training loss: 310.9566345214844 = 0.07552935928106308 + 50.0 * 6.217621803283691
Epoch 1860, val loss: 1.2989779710769653
Epoch 1870, training loss: 311.3759460449219 = 0.07405664026737213 + 50.0 * 6.226037502288818
Epoch 1870, val loss: 1.3026468753814697
Epoch 1880, training loss: 310.99639892578125 = 0.07259009033441544 + 50.0 * 6.218475818634033
Epoch 1880, val loss: 1.3073076009750366
Epoch 1890, training loss: 310.9209289550781 = 0.07117915153503418 + 50.0 * 6.2169952392578125
Epoch 1890, val loss: 1.3112891912460327
Epoch 1900, training loss: 311.09674072265625 = 0.069822758436203 + 50.0 * 6.22053861618042
Epoch 1900, val loss: 1.3155136108398438
Epoch 1910, training loss: 310.9620056152344 = 0.06846605241298676 + 50.0 * 6.217871189117432
Epoch 1910, val loss: 1.3198940753936768
Epoch 1920, training loss: 310.95062255859375 = 0.06715230643749237 + 50.0 * 6.21766996383667
Epoch 1920, val loss: 1.3236618041992188
Epoch 1930, training loss: 310.9450378417969 = 0.06588153541088104 + 50.0 * 6.217582702636719
Epoch 1930, val loss: 1.3280153274536133
Epoch 1940, training loss: 310.9373474121094 = 0.0646398514509201 + 50.0 * 6.217454433441162
Epoch 1940, val loss: 1.3321211338043213
Epoch 1950, training loss: 310.9319763183594 = 0.06340684741735458 + 50.0 * 6.217370986938477
Epoch 1950, val loss: 1.336466908454895
Epoch 1960, training loss: 310.9559020996094 = 0.06220347061753273 + 50.0 * 6.217874050140381
Epoch 1960, val loss: 1.3402971029281616
Epoch 1970, training loss: 310.85565185546875 = 0.06102527678012848 + 50.0 * 6.215892314910889
Epoch 1970, val loss: 1.344057559967041
Epoch 1980, training loss: 311.0232849121094 = 0.059894271194934845 + 50.0 * 6.219267845153809
Epoch 1980, val loss: 1.3479094505310059
Epoch 1990, training loss: 310.87054443359375 = 0.05876917764544487 + 50.0 * 6.216235637664795
Epoch 1990, val loss: 1.3526071310043335
Epoch 2000, training loss: 310.7830505371094 = 0.05766988918185234 + 50.0 * 6.214507579803467
Epoch 2000, val loss: 1.3562967777252197
Epoch 2010, training loss: 310.8970947265625 = 0.05661415308713913 + 50.0 * 6.2168097496032715
Epoch 2010, val loss: 1.3602828979492188
Epoch 2020, training loss: 310.77130126953125 = 0.055577415972948074 + 50.0 * 6.2143144607543945
Epoch 2020, val loss: 1.3648083209991455
Epoch 2030, training loss: 310.9810791015625 = 0.0545814149081707 + 50.0 * 6.21852970123291
Epoch 2030, val loss: 1.3691155910491943
Epoch 2040, training loss: 311.0387878417969 = 0.05355389043688774 + 50.0 * 6.219704627990723
Epoch 2040, val loss: 1.3723214864730835
Epoch 2050, training loss: 310.8548278808594 = 0.05255383625626564 + 50.0 * 6.216045379638672
Epoch 2050, val loss: 1.3765901327133179
Epoch 2060, training loss: 310.73968505859375 = 0.05160169675946236 + 50.0 * 6.213761806488037
Epoch 2060, val loss: 1.3802903890609741
Epoch 2070, training loss: 310.6996154785156 = 0.05068774148821831 + 50.0 * 6.212978363037109
Epoch 2070, val loss: 1.384680986404419
Epoch 2080, training loss: 310.73309326171875 = 0.049802836030721664 + 50.0 * 6.213665962219238
Epoch 2080, val loss: 1.3887273073196411
Epoch 2090, training loss: 310.9982604980469 = 0.04892681539058685 + 50.0 * 6.218986988067627
Epoch 2090, val loss: 1.3928016424179077
Epoch 2100, training loss: 310.7709655761719 = 0.048029668629169464 + 50.0 * 6.214458465576172
Epoch 2100, val loss: 1.3960247039794922
Epoch 2110, training loss: 310.70184326171875 = 0.04718358814716339 + 50.0 * 6.213093280792236
Epoch 2110, val loss: 1.4001001119613647
Epoch 2120, training loss: 310.8898010253906 = 0.04636479914188385 + 50.0 * 6.216868877410889
Epoch 2120, val loss: 1.4037054777145386
Epoch 2130, training loss: 311.0207214355469 = 0.045559801161289215 + 50.0 * 6.219502925872803
Epoch 2130, val loss: 1.4086649417877197
Epoch 2140, training loss: 310.7208251953125 = 0.04471861943602562 + 50.0 * 6.213522434234619
Epoch 2140, val loss: 1.411522388458252
Epoch 2150, training loss: 310.6363220214844 = 0.04395370930433273 + 50.0 * 6.21184778213501
Epoch 2150, val loss: 1.416262149810791
Epoch 2160, training loss: 310.6502380371094 = 0.04320628568530083 + 50.0 * 6.2121405601501465
Epoch 2160, val loss: 1.4199620485305786
Epoch 2170, training loss: 310.8565673828125 = 0.04248111695051193 + 50.0 * 6.216281890869141
Epoch 2170, val loss: 1.423822045326233
Epoch 2180, training loss: 310.700439453125 = 0.04173840582370758 + 50.0 * 6.213173866271973
Epoch 2180, val loss: 1.427306890487671
Epoch 2190, training loss: 310.60345458984375 = 0.04102740064263344 + 50.0 * 6.211248874664307
Epoch 2190, val loss: 1.4314041137695312
Epoch 2200, training loss: 310.57110595703125 = 0.040346190333366394 + 50.0 * 6.210615158081055
Epoch 2200, val loss: 1.435434341430664
Epoch 2210, training loss: 310.6991882324219 = 0.039681367576122284 + 50.0 * 6.21319055557251
Epoch 2210, val loss: 1.4390009641647339
Epoch 2220, training loss: 310.7567138671875 = 0.0390121191740036 + 50.0 * 6.214354515075684
Epoch 2220, val loss: 1.4429965019226074
Epoch 2230, training loss: 310.6448974609375 = 0.03834961727261543 + 50.0 * 6.212131023406982
Epoch 2230, val loss: 1.4464915990829468
Epoch 2240, training loss: 310.5696105957031 = 0.03770599886775017 + 50.0 * 6.21063756942749
Epoch 2240, val loss: 1.4505550861358643
Epoch 2250, training loss: 310.6095886230469 = 0.037094846367836 + 50.0 * 6.21144962310791
Epoch 2250, val loss: 1.454133152961731
Epoch 2260, training loss: 310.6290283203125 = 0.036493364721536636 + 50.0 * 6.211850643157959
Epoch 2260, val loss: 1.4579930305480957
Epoch 2270, training loss: 310.55645751953125 = 0.03590355068445206 + 50.0 * 6.210411071777344
Epoch 2270, val loss: 1.4623278379440308
Epoch 2280, training loss: 310.58111572265625 = 0.03532971814274788 + 50.0 * 6.210915565490723
Epoch 2280, val loss: 1.4660025835037231
Epoch 2290, training loss: 310.7322082519531 = 0.03476618602871895 + 50.0 * 6.213949203491211
Epoch 2290, val loss: 1.4699848890304565
Epoch 2300, training loss: 310.56292724609375 = 0.03418800234794617 + 50.0 * 6.210575103759766
Epoch 2300, val loss: 1.4731611013412476
Epoch 2310, training loss: 310.46435546875 = 0.03362935408949852 + 50.0 * 6.208614349365234
Epoch 2310, val loss: 1.4769980907440186
Epoch 2320, training loss: 310.4505920410156 = 0.03310722857713699 + 50.0 * 6.208349704742432
Epoch 2320, val loss: 1.4807486534118652
Epoch 2330, training loss: 310.494140625 = 0.03260113671422005 + 50.0 * 6.209230899810791
Epoch 2330, val loss: 1.4844245910644531
Epoch 2340, training loss: 310.8876647949219 = 0.03209943324327469 + 50.0 * 6.217111110687256
Epoch 2340, val loss: 1.4878746271133423
Epoch 2350, training loss: 310.5894470214844 = 0.03158855438232422 + 50.0 * 6.211157321929932
Epoch 2350, val loss: 1.4925874471664429
Epoch 2360, training loss: 310.5252380371094 = 0.031096499413251877 + 50.0 * 6.209882736206055
Epoch 2360, val loss: 1.4957373142242432
Epoch 2370, training loss: 310.7461853027344 = 0.030640853568911552 + 50.0 * 6.214311122894287
Epoch 2370, val loss: 1.4999579191207886
Epoch 2380, training loss: 310.4900207519531 = 0.030152125284075737 + 50.0 * 6.209197521209717
Epoch 2380, val loss: 1.5029264688491821
Epoch 2390, training loss: 310.41400146484375 = 0.029703492298722267 + 50.0 * 6.207685947418213
Epoch 2390, val loss: 1.5069794654846191
Epoch 2400, training loss: 310.5498962402344 = 0.029267171397805214 + 50.0 * 6.210412502288818
Epoch 2400, val loss: 1.5103029012680054
Epoch 2410, training loss: 310.61962890625 = 0.028831128031015396 + 50.0 * 6.21181583404541
Epoch 2410, val loss: 1.5147508382797241
Epoch 2420, training loss: 310.4612731933594 = 0.028378285467624664 + 50.0 * 6.208657741546631
Epoch 2420, val loss: 1.5174424648284912
Epoch 2430, training loss: 310.375732421875 = 0.027957163751125336 + 50.0 * 6.206955432891846
Epoch 2430, val loss: 1.5213499069213867
Epoch 2440, training loss: 310.3681640625 = 0.0275567639619112 + 50.0 * 6.206812381744385
Epoch 2440, val loss: 1.5249184370040894
Epoch 2450, training loss: 310.52252197265625 = 0.02717018499970436 + 50.0 * 6.209907531738281
Epoch 2450, val loss: 1.5284624099731445
Epoch 2460, training loss: 310.43609619140625 = 0.026771241798996925 + 50.0 * 6.208186626434326
Epoch 2460, val loss: 1.5321612358093262
Epoch 2470, training loss: 310.4818420410156 = 0.026381229981780052 + 50.0 * 6.209109783172607
Epoch 2470, val loss: 1.53619384765625
Epoch 2480, training loss: 310.4015808105469 = 0.025999989360570908 + 50.0 * 6.2075114250183105
Epoch 2480, val loss: 1.5398249626159668
Epoch 2490, training loss: 310.4170227050781 = 0.02563251368701458 + 50.0 * 6.207827568054199
Epoch 2490, val loss: 1.5429847240447998
Epoch 2500, training loss: 310.32421875 = 0.025267815217375755 + 50.0 * 6.205978870391846
Epoch 2500, val loss: 1.546563982963562
Epoch 2510, training loss: 310.50604248046875 = 0.024921655654907227 + 50.0 * 6.209622383117676
Epoch 2510, val loss: 1.5497708320617676
Epoch 2520, training loss: 310.38397216796875 = 0.024567602202296257 + 50.0 * 6.207188129425049
Epoch 2520, val loss: 1.55374014377594
Epoch 2530, training loss: 310.55120849609375 = 0.0242299847304821 + 50.0 * 6.2105393409729
Epoch 2530, val loss: 1.5566169023513794
Epoch 2540, training loss: 310.3621520996094 = 0.023872602730989456 + 50.0 * 6.206765651702881
Epoch 2540, val loss: 1.5601650476455688
Epoch 2550, training loss: 310.28826904296875 = 0.023544101044535637 + 50.0 * 6.205294609069824
Epoch 2550, val loss: 1.5637212991714478
Epoch 2560, training loss: 310.27178955078125 = 0.023232804611325264 + 50.0 * 6.2049713134765625
Epoch 2560, val loss: 1.567521095275879
Epoch 2570, training loss: 310.39208984375 = 0.022931303828954697 + 50.0 * 6.207383155822754
Epoch 2570, val loss: 1.5712779760360718
Epoch 2580, training loss: 310.4712219238281 = 0.022609639912843704 + 50.0 * 6.208971977233887
Epoch 2580, val loss: 1.5737520456314087
Epoch 2590, training loss: 310.4203796386719 = 0.02229980193078518 + 50.0 * 6.207961559295654
Epoch 2590, val loss: 1.5773205757141113
Epoch 2600, training loss: 310.44134521484375 = 0.02200143039226532 + 50.0 * 6.2083868980407715
Epoch 2600, val loss: 1.5807543992996216
Epoch 2610, training loss: 310.25042724609375 = 0.021703938022255898 + 50.0 * 6.2045745849609375
Epoch 2610, val loss: 1.5848908424377441
Epoch 2620, training loss: 310.2664489746094 = 0.021426234394311905 + 50.0 * 6.204900741577148
Epoch 2620, val loss: 1.5883604288101196
Epoch 2630, training loss: 310.38201904296875 = 0.021155858412384987 + 50.0 * 6.207217693328857
Epoch 2630, val loss: 1.5917192697525024
Epoch 2640, training loss: 310.44830322265625 = 0.02087547816336155 + 50.0 * 6.208548545837402
Epoch 2640, val loss: 1.5948495864868164
Epoch 2650, training loss: 310.3277893066406 = 0.02059849537909031 + 50.0 * 6.206143856048584
Epoch 2650, val loss: 1.597874641418457
Epoch 2660, training loss: 310.2836608886719 = 0.020335452631115913 + 50.0 * 6.20526647567749
Epoch 2660, val loss: 1.602023959159851
Epoch 2670, training loss: 310.262451171875 = 0.02008715085685253 + 50.0 * 6.20484733581543
Epoch 2670, val loss: 1.6052498817443848
Epoch 2680, training loss: 310.51611328125 = 0.01984211429953575 + 50.0 * 6.209925651550293
Epoch 2680, val loss: 1.6082922220230103
Epoch 2690, training loss: 310.2817687988281 = 0.01957303285598755 + 50.0 * 6.205244064331055
Epoch 2690, val loss: 1.6115658283233643
Epoch 2700, training loss: 310.19873046875 = 0.019330359995365143 + 50.0 * 6.203588485717773
Epoch 2700, val loss: 1.6152664422988892
Epoch 2710, training loss: 310.364013671875 = 0.019098201766610146 + 50.0 * 6.206898212432861
Epoch 2710, val loss: 1.6183089017868042
Epoch 2720, training loss: 310.2389221191406 = 0.01885729655623436 + 50.0 * 6.20440149307251
Epoch 2720, val loss: 1.6216384172439575
Epoch 2730, training loss: 310.4692077636719 = 0.018627312034368515 + 50.0 * 6.209011554718018
Epoch 2730, val loss: 1.6240761280059814
Epoch 2740, training loss: 310.21978759765625 = 0.01838778145611286 + 50.0 * 6.2040276527404785
Epoch 2740, val loss: 1.6280790567398071
Epoch 2750, training loss: 310.17828369140625 = 0.018163517117500305 + 50.0 * 6.203202724456787
Epoch 2750, val loss: 1.6316194534301758
Epoch 2760, training loss: 310.1618347167969 = 0.01795184798538685 + 50.0 * 6.202877998352051
Epoch 2760, val loss: 1.634911060333252
Epoch 2770, training loss: 310.21868896484375 = 0.01774679683148861 + 50.0 * 6.204018592834473
Epoch 2770, val loss: 1.6385465860366821
Epoch 2780, training loss: 310.4398193359375 = 0.017539607360959053 + 50.0 * 6.2084455490112305
Epoch 2780, val loss: 1.6414668560028076
Epoch 2790, training loss: 310.302001953125 = 0.017315885052084923 + 50.0 * 6.20569372177124
Epoch 2790, val loss: 1.6439447402954102
Epoch 2800, training loss: 310.2580261230469 = 0.017115145921707153 + 50.0 * 6.204818248748779
Epoch 2800, val loss: 1.6481711864471436
Epoch 2810, training loss: 310.4898376464844 = 0.01691891811788082 + 50.0 * 6.209458827972412
Epoch 2810, val loss: 1.6511611938476562
Epoch 2820, training loss: 310.2425537109375 = 0.01671472191810608 + 50.0 * 6.204516887664795
Epoch 2820, val loss: 1.6542378664016724
Epoch 2830, training loss: 310.1287536621094 = 0.01651618629693985 + 50.0 * 6.202244758605957
Epoch 2830, val loss: 1.6572808027267456
Epoch 2840, training loss: 310.1190490722656 = 0.016333648934960365 + 50.0 * 6.202054500579834
Epoch 2840, val loss: 1.6605587005615234
Epoch 2850, training loss: 310.1785888671875 = 0.016154944896697998 + 50.0 * 6.203248977661133
Epoch 2850, val loss: 1.663519263267517
Epoch 2860, training loss: 310.39227294921875 = 0.01597859337925911 + 50.0 * 6.207525730133057
Epoch 2860, val loss: 1.6671037673950195
Epoch 2870, training loss: 310.3017272949219 = 0.015790395438671112 + 50.0 * 6.205718994140625
Epoch 2870, val loss: 1.6701686382293701
Epoch 2880, training loss: 310.1445617675781 = 0.01560244895517826 + 50.0 * 6.202579498291016
Epoch 2880, val loss: 1.6730396747589111
Epoch 2890, training loss: 310.1361999511719 = 0.015436673536896706 + 50.0 * 6.2024149894714355
Epoch 2890, val loss: 1.676469326019287
Epoch 2900, training loss: 310.2169189453125 = 0.015271863900125027 + 50.0 * 6.204032897949219
Epoch 2900, val loss: 1.6798896789550781
Epoch 2910, training loss: 310.2040710449219 = 0.015101942233741283 + 50.0 * 6.203779220581055
Epoch 2910, val loss: 1.682515263557434
Epoch 2920, training loss: 310.1920166015625 = 0.014931625686585903 + 50.0 * 6.2035417556762695
Epoch 2920, val loss: 1.684831976890564
Epoch 2930, training loss: 310.1438293457031 = 0.014764774590730667 + 50.0 * 6.20258092880249
Epoch 2930, val loss: 1.6881293058395386
Epoch 2940, training loss: 310.05999755859375 = 0.01460692286491394 + 50.0 * 6.2009077072143555
Epoch 2940, val loss: 1.691392183303833
Epoch 2950, training loss: 310.1455383300781 = 0.01445539016276598 + 50.0 * 6.2026214599609375
Epoch 2950, val loss: 1.6941636800765991
Epoch 2960, training loss: 310.28033447265625 = 0.014302190393209457 + 50.0 * 6.205320358276367
Epoch 2960, val loss: 1.6974149942398071
Epoch 2970, training loss: 310.1951904296875 = 0.014145164750516415 + 50.0 * 6.203620910644531
Epoch 2970, val loss: 1.7004919052124023
Epoch 2980, training loss: 310.0744934082031 = 0.013986923731863499 + 50.0 * 6.2012104988098145
Epoch 2980, val loss: 1.7030816078186035
Epoch 2990, training loss: 310.0432434082031 = 0.013846343383193016 + 50.0 * 6.200587749481201
Epoch 2990, val loss: 1.7065118551254272
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.8181338956246705
The final CL Acc:0.67284, 0.00761, The final GNN Acc:0.81251, 0.00407
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13202])
remove edge: torch.Size([2, 8002])
updated graph: torch.Size([2, 10648])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.79595947265625 = 1.9571377038955688 + 50.0 * 8.596776962280273
Epoch 0, val loss: 1.9574657678604126
Epoch 10, training loss: 431.72222900390625 = 1.9485292434692383 + 50.0 * 8.595474243164062
Epoch 10, val loss: 1.948034405708313
Epoch 20, training loss: 431.2674865722656 = 1.9382460117340088 + 50.0 * 8.58658504486084
Epoch 20, val loss: 1.9366613626480103
Epoch 30, training loss: 428.3535461425781 = 1.925228238105774 + 50.0 * 8.528566360473633
Epoch 30, val loss: 1.9220985174179077
Epoch 40, training loss: 411.805419921875 = 1.9098349809646606 + 50.0 * 8.197912216186523
Epoch 40, val loss: 1.90518319606781
Epoch 50, training loss: 373.4515075683594 = 1.8933653831481934 + 50.0 * 7.4311628341674805
Epoch 50, val loss: 1.887673258781433
Epoch 60, training loss: 357.3822326660156 = 1.8792171478271484 + 50.0 * 7.110060214996338
Epoch 60, val loss: 1.8735750913619995
Epoch 70, training loss: 349.342529296875 = 1.8655977249145508 + 50.0 * 6.949538707733154
Epoch 70, val loss: 1.8600397109985352
Epoch 80, training loss: 344.02545166015625 = 1.852973461151123 + 50.0 * 6.843449592590332
Epoch 80, val loss: 1.8478082418441772
Epoch 90, training loss: 339.3174743652344 = 1.8419984579086304 + 50.0 * 6.749509334564209
Epoch 90, val loss: 1.8371870517730713
Epoch 100, training loss: 335.9158630371094 = 1.832571268081665 + 50.0 * 6.681665897369385
Epoch 100, val loss: 1.8279330730438232
Epoch 110, training loss: 333.4838562011719 = 1.8234570026397705 + 50.0 * 6.63320779800415
Epoch 110, val loss: 1.818852424621582
Epoch 120, training loss: 331.5414123535156 = 1.8147207498550415 + 50.0 * 6.594533920288086
Epoch 120, val loss: 1.8102258443832397
Epoch 130, training loss: 329.9936218261719 = 1.8064581155776978 + 50.0 * 6.5637431144714355
Epoch 130, val loss: 1.802130103111267
Epoch 140, training loss: 328.7861022949219 = 1.7985575199127197 + 50.0 * 6.539751052856445
Epoch 140, val loss: 1.7943328619003296
Epoch 150, training loss: 327.50030517578125 = 1.7906773090362549 + 50.0 * 6.514192581176758
Epoch 150, val loss: 1.7865961790084839
Epoch 160, training loss: 326.4524841308594 = 1.782881736755371 + 50.0 * 6.493392467498779
Epoch 160, val loss: 1.7789218425750732
Epoch 170, training loss: 325.508056640625 = 1.7749937772750854 + 50.0 * 6.474661350250244
Epoch 170, val loss: 1.7712619304656982
Epoch 180, training loss: 324.7167053222656 = 1.7668144702911377 + 50.0 * 6.45899772644043
Epoch 180, val loss: 1.7633510828018188
Epoch 190, training loss: 324.0305480957031 = 1.7580976486206055 + 50.0 * 6.445449352264404
Epoch 190, val loss: 1.755375862121582
Epoch 200, training loss: 323.4117126464844 = 1.7489615678787231 + 50.0 * 6.433255195617676
Epoch 200, val loss: 1.7468948364257812
Epoch 210, training loss: 322.8830261230469 = 1.739180564880371 + 50.0 * 6.422877311706543
Epoch 210, val loss: 1.7381188869476318
Epoch 220, training loss: 322.4433288574219 = 1.728611707687378 + 50.0 * 6.414294719696045
Epoch 220, val loss: 1.7285937070846558
Epoch 230, training loss: 321.9559020996094 = 1.7172554731369019 + 50.0 * 6.404772758483887
Epoch 230, val loss: 1.7184809446334839
Epoch 240, training loss: 321.5274353027344 = 1.7049568891525269 + 50.0 * 6.396449565887451
Epoch 240, val loss: 1.7076258659362793
Epoch 250, training loss: 321.1300354003906 = 1.6917740106582642 + 50.0 * 6.388765335083008
Epoch 250, val loss: 1.6960303783416748
Epoch 260, training loss: 320.7628173828125 = 1.6776589155197144 + 50.0 * 6.381702899932861
Epoch 260, val loss: 1.6836515665054321
Epoch 270, training loss: 320.42578125 = 1.6624290943145752 + 50.0 * 6.375267505645752
Epoch 270, val loss: 1.6704316139221191
Epoch 280, training loss: 320.0985412597656 = 1.6462143659591675 + 50.0 * 6.369046688079834
Epoch 280, val loss: 1.6563912630081177
Epoch 290, training loss: 319.7968444824219 = 1.628981590270996 + 50.0 * 6.3633575439453125
Epoch 290, val loss: 1.6415284872055054
Epoch 300, training loss: 319.5435485839844 = 1.6106102466583252 + 50.0 * 6.358658790588379
Epoch 300, val loss: 1.625860571861267
Epoch 310, training loss: 319.3182067871094 = 1.5911922454833984 + 50.0 * 6.3545403480529785
Epoch 310, val loss: 1.6094063520431519
Epoch 320, training loss: 318.9855041503906 = 1.5709387063980103 + 50.0 * 6.348290920257568
Epoch 320, val loss: 1.5922825336456299
Epoch 330, training loss: 318.8514709472656 = 1.5497918128967285 + 50.0 * 6.346033096313477
Epoch 330, val loss: 1.5745103359222412
Epoch 340, training loss: 318.47393798828125 = 1.527777075767517 + 50.0 * 6.338923454284668
Epoch 340, val loss: 1.556315541267395
Epoch 350, training loss: 318.23785400390625 = 1.5052123069763184 + 50.0 * 6.334652900695801
Epoch 350, val loss: 1.537703037261963
Epoch 360, training loss: 318.0403747558594 = 1.4819345474243164 + 50.0 * 6.3311686515808105
Epoch 360, val loss: 1.518644094467163
Epoch 370, training loss: 317.78961181640625 = 1.4582250118255615 + 50.0 * 6.326627731323242
Epoch 370, val loss: 1.4994475841522217
Epoch 380, training loss: 317.60174560546875 = 1.4342231750488281 + 50.0 * 6.323350429534912
Epoch 380, val loss: 1.4801690578460693
Epoch 390, training loss: 317.53192138671875 = 1.4099700450897217 + 50.0 * 6.322438716888428
Epoch 390, val loss: 1.4608466625213623
Epoch 400, training loss: 317.3136901855469 = 1.3854132890701294 + 50.0 * 6.318565368652344
Epoch 400, val loss: 1.4416097402572632
Epoch 410, training loss: 317.06658935546875 = 1.3610941171646118 + 50.0 * 6.314109802246094
Epoch 410, val loss: 1.422704815864563
Epoch 420, training loss: 316.8886413574219 = 1.33689546585083 + 50.0 * 6.31103515625
Epoch 420, val loss: 1.4042142629623413
Epoch 430, training loss: 316.7478332519531 = 1.3129323720932007 + 50.0 * 6.3086981773376465
Epoch 430, val loss: 1.386076807975769
Epoch 440, training loss: 316.7839660644531 = 1.2889820337295532 + 50.0 * 6.309899806976318
Epoch 440, val loss: 1.3682889938354492
Epoch 450, training loss: 316.5693664550781 = 1.265377163887024 + 50.0 * 6.306079864501953
Epoch 450, val loss: 1.3507603406906128
Epoch 460, training loss: 316.3377685546875 = 1.2421759366989136 + 50.0 * 6.3019118309021
Epoch 460, val loss: 1.3339530229568481
Epoch 470, training loss: 316.177734375 = 1.219451904296875 + 50.0 * 6.299165725708008
Epoch 470, val loss: 1.3177316188812256
Epoch 480, training loss: 316.0553894042969 = 1.1971285343170166 + 50.0 * 6.2971649169921875
Epoch 480, val loss: 1.3020528554916382
Epoch 490, training loss: 315.9814147949219 = 1.1750916242599487 + 50.0 * 6.296126842498779
Epoch 490, val loss: 1.2866876125335693
Epoch 500, training loss: 316.0443420410156 = 1.1533950567245483 + 50.0 * 6.297819137573242
Epoch 500, val loss: 1.2718908786773682
Epoch 510, training loss: 315.7380065917969 = 1.1323553323745728 + 50.0 * 6.292113304138184
Epoch 510, val loss: 1.2575334310531616
Epoch 520, training loss: 315.5645751953125 = 1.1117327213287354 + 50.0 * 6.289056777954102
Epoch 520, val loss: 1.2438679933547974
Epoch 530, training loss: 315.5367126464844 = 1.0915976762771606 + 50.0 * 6.288902282714844
Epoch 530, val loss: 1.2307988405227661
Epoch 540, training loss: 315.48822021484375 = 1.0718103647232056 + 50.0 * 6.288328170776367
Epoch 540, val loss: 1.2180906534194946
Epoch 550, training loss: 315.31719970703125 = 1.0524306297302246 + 50.0 * 6.285295486450195
Epoch 550, val loss: 1.2056626081466675
Epoch 560, training loss: 315.1705322265625 = 1.0335893630981445 + 50.0 * 6.28273868560791
Epoch 560, val loss: 1.1940433979034424
Epoch 570, training loss: 315.6444396972656 = 1.0151935815811157 + 50.0 * 6.2925848960876465
Epoch 570, val loss: 1.182660698890686
Epoch 580, training loss: 315.0159912109375 = 0.9970464110374451 + 50.0 * 6.280378818511963
Epoch 580, val loss: 1.1716785430908203
Epoch 590, training loss: 314.8920593261719 = 0.9794920682907104 + 50.0 * 6.2782511711120605
Epoch 590, val loss: 1.1612627506256104
Epoch 600, training loss: 314.79742431640625 = 0.9624626040458679 + 50.0 * 6.276699066162109
Epoch 600, val loss: 1.1513285636901855
Epoch 610, training loss: 315.1012268066406 = 0.9458576440811157 + 50.0 * 6.283107280731201
Epoch 610, val loss: 1.1416711807250977
Epoch 620, training loss: 314.7034606933594 = 0.9293110370635986 + 50.0 * 6.275482654571533
Epoch 620, val loss: 1.1321585178375244
Epoch 630, training loss: 314.60345458984375 = 0.9132577776908875 + 50.0 * 6.2738037109375
Epoch 630, val loss: 1.123245358467102
Epoch 640, training loss: 314.4751892089844 = 0.8976959586143494 + 50.0 * 6.271549701690674
Epoch 640, val loss: 1.114701747894287
Epoch 650, training loss: 314.478759765625 = 0.8824787139892578 + 50.0 * 6.271925449371338
Epoch 650, val loss: 1.1064693927764893
Epoch 660, training loss: 314.3475341796875 = 0.8673588633537292 + 50.0 * 6.269603252410889
Epoch 660, val loss: 1.0983796119689941
Epoch 670, training loss: 314.5657958984375 = 0.8523985743522644 + 50.0 * 6.27426815032959
Epoch 670, val loss: 1.0904557704925537
Epoch 680, training loss: 314.2327575683594 = 0.8377135992050171 + 50.0 * 6.2679009437561035
Epoch 680, val loss: 1.082703948020935
Epoch 690, training loss: 314.13812255859375 = 0.8234476447105408 + 50.0 * 6.266293525695801
Epoch 690, val loss: 1.0752792358398438
Epoch 700, training loss: 314.0350341796875 = 0.809425950050354 + 50.0 * 6.264512538909912
Epoch 700, val loss: 1.068191647529602
Epoch 710, training loss: 314.0455017089844 = 0.7955361604690552 + 50.0 * 6.2649993896484375
Epoch 710, val loss: 1.061282992362976
Epoch 720, training loss: 313.95904541015625 = 0.7816223502159119 + 50.0 * 6.263548374176025
Epoch 720, val loss: 1.0541964769363403
Epoch 730, training loss: 313.947021484375 = 0.7678438425064087 + 50.0 * 6.263583660125732
Epoch 730, val loss: 1.0474040508270264
Epoch 740, training loss: 313.76422119140625 = 0.7542971968650818 + 50.0 * 6.26019811630249
Epoch 740, val loss: 1.0407757759094238
Epoch 750, training loss: 313.84222412109375 = 0.7409293055534363 + 50.0 * 6.262025833129883
Epoch 750, val loss: 1.0343519449234009
Epoch 760, training loss: 313.7010803222656 = 0.7274934649467468 + 50.0 * 6.259471416473389
Epoch 760, val loss: 1.027939796447754
Epoch 770, training loss: 313.66375732421875 = 0.7140823602676392 + 50.0 * 6.258993625640869
Epoch 770, val loss: 1.0216232538223267
Epoch 780, training loss: 313.64013671875 = 0.7009645700454712 + 50.0 * 6.25878381729126
Epoch 780, val loss: 1.0156538486480713
Epoch 790, training loss: 313.50799560546875 = 0.6878159046173096 + 50.0 * 6.256403923034668
Epoch 790, val loss: 1.0096137523651123
Epoch 800, training loss: 313.5308532714844 = 0.6748414635658264 + 50.0 * 6.257120132446289
Epoch 800, val loss: 1.003852128982544
Epoch 810, training loss: 313.4896240234375 = 0.6618412733078003 + 50.0 * 6.256556034088135
Epoch 810, val loss: 0.9980136156082153
Epoch 820, training loss: 313.4924621582031 = 0.6490278840065002 + 50.0 * 6.256868839263916
Epoch 820, val loss: 0.9924409985542297
Epoch 830, training loss: 313.3744201660156 = 0.6359963417053223 + 50.0 * 6.2547688484191895
Epoch 830, val loss: 0.9868432879447937
Epoch 840, training loss: 313.21099853515625 = 0.6233187317848206 + 50.0 * 6.251753330230713
Epoch 840, val loss: 0.9816103577613831
Epoch 850, training loss: 313.13787841796875 = 0.610826849937439 + 50.0 * 6.250540733337402
Epoch 850, val loss: 0.9766234159469604
Epoch 860, training loss: 313.1654052734375 = 0.5984093546867371 + 50.0 * 6.251339912414551
Epoch 860, val loss: 0.9718080759048462
Epoch 870, training loss: 313.1210021972656 = 0.5859696269035339 + 50.0 * 6.250700950622559
Epoch 870, val loss: 0.9669522643089294
Epoch 880, training loss: 313.069091796875 = 0.573652446269989 + 50.0 * 6.249908924102783
Epoch 880, val loss: 0.9623333811759949
Epoch 890, training loss: 312.9292297363281 = 0.5615757703781128 + 50.0 * 6.2473530769348145
Epoch 890, val loss: 0.9581696391105652
Epoch 900, training loss: 313.1441650390625 = 0.5497111082077026 + 50.0 * 6.251888751983643
Epoch 900, val loss: 0.9542377591133118
Epoch 910, training loss: 312.97509765625 = 0.5377755761146545 + 50.0 * 6.248746395111084
Epoch 910, val loss: 0.9500173926353455
Epoch 920, training loss: 312.86090087890625 = 0.5260593891143799 + 50.0 * 6.246696472167969
Epoch 920, val loss: 0.9463196396827698
Epoch 930, training loss: 312.7420959472656 = 0.5146238803863525 + 50.0 * 6.24454927444458
Epoch 930, val loss: 0.9429630041122437
Epoch 940, training loss: 312.75994873046875 = 0.5034121870994568 + 50.0 * 6.24513053894043
Epoch 940, val loss: 0.9399104714393616
Epoch 950, training loss: 312.6552734375 = 0.4921835958957672 + 50.0 * 6.243261814117432
Epoch 950, val loss: 0.9367080926895142
Epoch 960, training loss: 312.6837463378906 = 0.4812134802341461 + 50.0 * 6.244050979614258
Epoch 960, val loss: 0.9338887333869934
Epoch 970, training loss: 312.5834045410156 = 0.4704923927783966 + 50.0 * 6.242258548736572
Epoch 970, val loss: 0.9313424229621887
Epoch 980, training loss: 312.87884521484375 = 0.4599602520465851 + 50.0 * 6.248377799987793
Epoch 980, val loss: 0.9289161562919617
Epoch 990, training loss: 312.4885559082031 = 0.44953399896621704 + 50.0 * 6.240780353546143
Epoch 990, val loss: 0.9265570044517517
Epoch 1000, training loss: 312.433837890625 = 0.43941015005111694 + 50.0 * 6.2398881912231445
Epoch 1000, val loss: 0.9246655702590942
Epoch 1010, training loss: 312.4158630371094 = 0.42953959107398987 + 50.0 * 6.239726543426514
Epoch 1010, val loss: 0.9230770468711853
Epoch 1020, training loss: 312.7632751464844 = 0.419861763715744 + 50.0 * 6.246868133544922
Epoch 1020, val loss: 0.9216073751449585
Epoch 1030, training loss: 312.3917236328125 = 0.41016635298728943 + 50.0 * 6.239631175994873
Epoch 1030, val loss: 0.9200944304466248
Epoch 1040, training loss: 312.322998046875 = 0.40081968903541565 + 50.0 * 6.238443374633789
Epoch 1040, val loss: 0.9190747141838074
Epoch 1050, training loss: 312.354248046875 = 0.3917056620121002 + 50.0 * 6.239251136779785
Epoch 1050, val loss: 0.9181538224220276
Epoch 1060, training loss: 312.21417236328125 = 0.3827027678489685 + 50.0 * 6.236629486083984
Epoch 1060, val loss: 0.9173598885536194
Epoch 1070, training loss: 312.1632385253906 = 0.37400320172309875 + 50.0 * 6.235785007476807
Epoch 1070, val loss: 0.9169396162033081
Epoch 1080, training loss: 312.81048583984375 = 0.36545172333717346 + 50.0 * 6.248900890350342
Epoch 1080, val loss: 0.9164928793907166
Epoch 1090, training loss: 312.2764587402344 = 0.3567492663860321 + 50.0 * 6.238394260406494
Epoch 1090, val loss: 0.9157984852790833
Epoch 1100, training loss: 312.11358642578125 = 0.34844741225242615 + 50.0 * 6.235302448272705
Epoch 1100, val loss: 0.9156523942947388
Epoch 1110, training loss: 312.0125732421875 = 0.34044352173805237 + 50.0 * 6.233442306518555
Epoch 1110, val loss: 0.915960431098938
Epoch 1120, training loss: 311.9534606933594 = 0.3326319754123688 + 50.0 * 6.23241662979126
Epoch 1120, val loss: 0.9163836240768433
Epoch 1130, training loss: 311.93084716796875 = 0.32498806715011597 + 50.0 * 6.232117176055908
Epoch 1130, val loss: 0.9169177412986755
Epoch 1140, training loss: 312.52447509765625 = 0.317444771528244 + 50.0 * 6.244140625
Epoch 1140, val loss: 0.9173329472541809
Epoch 1150, training loss: 311.9016418457031 = 0.3098253607749939 + 50.0 * 6.231836318969727
Epoch 1150, val loss: 0.9175616502761841
Epoch 1160, training loss: 311.8780212402344 = 0.3025458753108978 + 50.0 * 6.231509208679199
Epoch 1160, val loss: 0.9181287288665771
Epoch 1170, training loss: 311.8049011230469 = 0.2955038249492645 + 50.0 * 6.230187892913818
Epoch 1170, val loss: 0.9191074967384338
Epoch 1180, training loss: 311.8179016113281 = 0.2886591851711273 + 50.0 * 6.230584621429443
Epoch 1180, val loss: 0.9202011823654175
Epoch 1190, training loss: 311.9170227050781 = 0.2818233072757721 + 50.0 * 6.232703685760498
Epoch 1190, val loss: 0.9209771752357483
Epoch 1200, training loss: 311.76983642578125 = 0.2751093804836273 + 50.0 * 6.229894638061523
Epoch 1200, val loss: 0.9220024347305298
Epoch 1210, training loss: 311.70751953125 = 0.2686212956905365 + 50.0 * 6.228777885437012
Epoch 1210, val loss: 0.9232863187789917
Epoch 1220, training loss: 311.6541748046875 = 0.2623555064201355 + 50.0 * 6.2278361320495605
Epoch 1220, val loss: 0.9248007535934448
Epoch 1230, training loss: 311.8887939453125 = 0.2562289237976074 + 50.0 * 6.232651233673096
Epoch 1230, val loss: 0.9264030456542969
Epoch 1240, training loss: 311.61083984375 = 0.25013113021850586 + 50.0 * 6.227214336395264
Epoch 1240, val loss: 0.927623987197876
Epoch 1250, training loss: 311.73382568359375 = 0.2442263960838318 + 50.0 * 6.22979211807251
Epoch 1250, val loss: 0.9292725920677185
Epoch 1260, training loss: 311.74310302734375 = 0.23838183283805847 + 50.0 * 6.2300944328308105
Epoch 1260, val loss: 0.9305647611618042
Epoch 1270, training loss: 311.601318359375 = 0.23273435235023499 + 50.0 * 6.227371692657471
Epoch 1270, val loss: 0.932451605796814
Epoch 1280, training loss: 311.4811706542969 = 0.22725850343704224 + 50.0 * 6.225078105926514
Epoch 1280, val loss: 0.9341182112693787
Epoch 1290, training loss: 311.4542541503906 = 0.2219669222831726 + 50.0 * 6.224646091461182
Epoch 1290, val loss: 0.9361810684204102
Epoch 1300, training loss: 311.4836120605469 = 0.2168022245168686 + 50.0 * 6.225336074829102
Epoch 1300, val loss: 0.9381332993507385
Epoch 1310, training loss: 311.69952392578125 = 0.2116760015487671 + 50.0 * 6.229757308959961
Epoch 1310, val loss: 0.940080463886261
Epoch 1320, training loss: 311.5832214355469 = 0.20665381848812103 + 50.0 * 6.2275309562683105
Epoch 1320, val loss: 0.9419888257980347
Epoch 1330, training loss: 311.3912048339844 = 0.2017470747232437 + 50.0 * 6.223789215087891
Epoch 1330, val loss: 0.943999171257019
Epoch 1340, training loss: 311.3202209472656 = 0.19705137610435486 + 50.0 * 6.222463607788086
Epoch 1340, val loss: 0.9462540745735168
Epoch 1350, training loss: 311.307373046875 = 0.19250831007957458 + 50.0 * 6.222297668457031
Epoch 1350, val loss: 0.948575496673584
Epoch 1360, training loss: 311.4508056640625 = 0.18807020783424377 + 50.0 * 6.225254535675049
Epoch 1360, val loss: 0.9508664608001709
Epoch 1370, training loss: 311.3611145019531 = 0.1836852878332138 + 50.0 * 6.223548889160156
Epoch 1370, val loss: 0.9530795812606812
Epoch 1380, training loss: 311.3185729980469 = 0.17934396862983704 + 50.0 * 6.222784519195557
Epoch 1380, val loss: 0.9552083611488342
Epoch 1390, training loss: 311.51092529296875 = 0.17518441379070282 + 50.0 * 6.226715087890625
Epoch 1390, val loss: 0.957577645778656
Epoch 1400, training loss: 311.263427734375 = 0.17108623683452606 + 50.0 * 6.221847057342529
Epoch 1400, val loss: 0.9599127769470215
Epoch 1410, training loss: 311.16156005859375 = 0.16715504229068756 + 50.0 * 6.219888210296631
Epoch 1410, val loss: 0.9624916315078735
Epoch 1420, training loss: 311.16680908203125 = 0.16339172422885895 + 50.0 * 6.220068454742432
Epoch 1420, val loss: 0.9651322960853577
Epoch 1430, training loss: 311.3600769042969 = 0.1597074568271637 + 50.0 * 6.224007606506348
Epoch 1430, val loss: 0.967856764793396
Epoch 1440, training loss: 311.2041320800781 = 0.15595743060112 + 50.0 * 6.220963954925537
Epoch 1440, val loss: 0.9699239134788513
Epoch 1450, training loss: 311.1596984863281 = 0.15234290063381195 + 50.0 * 6.220147132873535
Epoch 1450, val loss: 0.9724692106246948
Epoch 1460, training loss: 311.09637451171875 = 0.14891593158245087 + 50.0 * 6.218948841094971
Epoch 1460, val loss: 0.9752077460289001
Epoch 1470, training loss: 311.0470275878906 = 0.14560025930404663 + 50.0 * 6.218028545379639
Epoch 1470, val loss: 0.9781288504600525
Epoch 1480, training loss: 311.2010803222656 = 0.14238272607326508 + 50.0 * 6.2211737632751465
Epoch 1480, val loss: 0.9809116125106812
Epoch 1490, training loss: 311.0198669433594 = 0.13914115726947784 + 50.0 * 6.217614650726318
Epoch 1490, val loss: 0.9834991693496704
Epoch 1500, training loss: 311.07293701171875 = 0.13601791858673096 + 50.0 * 6.218738555908203
Epoch 1500, val loss: 0.9862082004547119
Epoch 1510, training loss: 311.010498046875 = 0.132981076836586 + 50.0 * 6.217550277709961
Epoch 1510, val loss: 0.9890368580818176
Epoch 1520, training loss: 311.1246643066406 = 0.1300358772277832 + 50.0 * 6.219892501831055
Epoch 1520, val loss: 0.992053210735321
Epoch 1530, training loss: 310.9988098144531 = 0.12717723846435547 + 50.0 * 6.217432975769043
Epoch 1530, val loss: 0.9949782490730286
Epoch 1540, training loss: 310.9263000488281 = 0.12438403815031052 + 50.0 * 6.216038227081299
Epoch 1540, val loss: 0.998110294342041
Epoch 1550, training loss: 310.9219665527344 = 0.12170008569955826 + 50.0 * 6.216005325317383
Epoch 1550, val loss: 1.0011675357818604
Epoch 1560, training loss: 311.1559143066406 = 0.11910154670476913 + 50.0 * 6.220736026763916
Epoch 1560, val loss: 1.00424063205719
Epoch 1570, training loss: 311.19854736328125 = 0.1164429560303688 + 50.0 * 6.221641540527344
Epoch 1570, val loss: 1.007058024406433
Epoch 1580, training loss: 310.9704895019531 = 0.11390810459852219 + 50.0 * 6.217131614685059
Epoch 1580, val loss: 1.0099194049835205
Epoch 1590, training loss: 310.86749267578125 = 0.1114523634314537 + 50.0 * 6.215120792388916
Epoch 1590, val loss: 1.0130122900009155
Epoch 1600, training loss: 310.9014587402344 = 0.10910085588693619 + 50.0 * 6.215847015380859
Epoch 1600, val loss: 1.0163174867630005
Epoch 1610, training loss: 310.9368896484375 = 0.10681430995464325 + 50.0 * 6.216601848602295
Epoch 1610, val loss: 1.0194368362426758
Epoch 1620, training loss: 310.84637451171875 = 0.10454785078763962 + 50.0 * 6.214836597442627
Epoch 1620, val loss: 1.0228735208511353
Epoch 1630, training loss: 310.79022216796875 = 0.10236001014709473 + 50.0 * 6.213757514953613
Epoch 1630, val loss: 1.0259957313537598
Epoch 1640, training loss: 311.13348388671875 = 0.10023189336061478 + 50.0 * 6.220664978027344
Epoch 1640, val loss: 1.0291191339492798
Epoch 1650, training loss: 310.85400390625 = 0.0981186255812645 + 50.0 * 6.215117931365967
Epoch 1650, val loss: 1.0322250127792358
Epoch 1660, training loss: 310.7583923339844 = 0.09605264663696289 + 50.0 * 6.213246822357178
Epoch 1660, val loss: 1.035434365272522
Epoch 1670, training loss: 310.7304382324219 = 0.09410598874092102 + 50.0 * 6.212726593017578
Epoch 1670, val loss: 1.0390417575836182
Epoch 1680, training loss: 310.86065673828125 = 0.09220265597105026 + 50.0 * 6.21536922454834
Epoch 1680, val loss: 1.0424039363861084
Epoch 1690, training loss: 310.8268737792969 = 0.09030529111623764 + 50.0 * 6.214731216430664
Epoch 1690, val loss: 1.0455350875854492
Epoch 1700, training loss: 310.6920471191406 = 0.08843749016523361 + 50.0 * 6.212072372436523
Epoch 1700, val loss: 1.0491348505020142
Epoch 1710, training loss: 310.6510009765625 = 0.08665882050991058 + 50.0 * 6.211286544799805
Epoch 1710, val loss: 1.052549958229065
Epoch 1720, training loss: 310.6462707519531 = 0.08494825661182404 + 50.0 * 6.211225986480713
Epoch 1720, val loss: 1.056276559829712
Epoch 1730, training loss: 310.7856750488281 = 0.08328147977590561 + 50.0 * 6.214047908782959
Epoch 1730, val loss: 1.0599086284637451
Epoch 1740, training loss: 310.86114501953125 = 0.08160959929227829 + 50.0 * 6.215590953826904
Epoch 1740, val loss: 1.063033938407898
Epoch 1750, training loss: 310.7199401855469 = 0.07993742823600769 + 50.0 * 6.2128005027771
Epoch 1750, val loss: 1.066399097442627
Epoch 1760, training loss: 310.6322937011719 = 0.07837346941232681 + 50.0 * 6.211078643798828
Epoch 1760, val loss: 1.0700730085372925
Epoch 1770, training loss: 310.57965087890625 = 0.0768604725599289 + 50.0 * 6.210055828094482
Epoch 1770, val loss: 1.0738701820373535
Epoch 1780, training loss: 310.5911865234375 = 0.07539842277765274 + 50.0 * 6.210315704345703
Epoch 1780, val loss: 1.0776209831237793
Epoch 1790, training loss: 311.0517272949219 = 0.07393569499254227 + 50.0 * 6.219555377960205
Epoch 1790, val loss: 1.0810972452163696
Epoch 1800, training loss: 310.7617492675781 = 0.07246000319719315 + 50.0 * 6.2137861251831055
Epoch 1800, val loss: 1.0843594074249268
Epoch 1810, training loss: 310.51751708984375 = 0.07104405015707016 + 50.0 * 6.208929538726807
Epoch 1810, val loss: 1.087905764579773
Epoch 1820, training loss: 310.52923583984375 = 0.0697016716003418 + 50.0 * 6.209190368652344
Epoch 1820, val loss: 1.092098593711853
Epoch 1830, training loss: 310.5628356933594 = 0.0684143453836441 + 50.0 * 6.209888458251953
Epoch 1830, val loss: 1.095832109451294
Epoch 1840, training loss: 310.68634033203125 = 0.06713322550058365 + 50.0 * 6.212384223937988
Epoch 1840, val loss: 1.0996360778808594
Epoch 1850, training loss: 310.5041198730469 = 0.06584834307432175 + 50.0 * 6.208765506744385
Epoch 1850, val loss: 1.1029529571533203
Epoch 1860, training loss: 310.64776611328125 = 0.06462306529283524 + 50.0 * 6.211662769317627
Epoch 1860, val loss: 1.1066125631332397
Epoch 1870, training loss: 310.54571533203125 = 0.06341755390167236 + 50.0 * 6.209646224975586
Epoch 1870, val loss: 1.1106494665145874
Epoch 1880, training loss: 310.5847473144531 = 0.062237322330474854 + 50.0 * 6.210450649261475
Epoch 1880, val loss: 1.1142443418502808
Epoch 1890, training loss: 310.60693359375 = 0.06108631193637848 + 50.0 * 6.210916996002197
Epoch 1890, val loss: 1.1179715394973755
Epoch 1900, training loss: 310.5376892089844 = 0.059964340180158615 + 50.0 * 6.209554195404053
Epoch 1900, val loss: 1.121706485748291
Epoch 1910, training loss: 310.4628601074219 = 0.058858633041381836 + 50.0 * 6.208079814910889
Epoch 1910, val loss: 1.1257035732269287
Epoch 1920, training loss: 310.43621826171875 = 0.05780588462948799 + 50.0 * 6.207568645477295
Epoch 1920, val loss: 1.1294586658477783
Epoch 1930, training loss: 310.4622497558594 = 0.05678245425224304 + 50.0 * 6.208109378814697
Epoch 1930, val loss: 1.1335997581481934
Epoch 1940, training loss: 310.5705261230469 = 0.05576647073030472 + 50.0 * 6.210295677185059
Epoch 1940, val loss: 1.1373378038406372
Epoch 1950, training loss: 310.60125732421875 = 0.05475606396794319 + 50.0 * 6.210930347442627
Epoch 1950, val loss: 1.140783429145813
Epoch 1960, training loss: 310.3966369628906 = 0.05376772955060005 + 50.0 * 6.206857204437256
Epoch 1960, val loss: 1.1447070837020874
Epoch 1970, training loss: 310.3455810546875 = 0.05282109975814819 + 50.0 * 6.205854892730713
Epoch 1970, val loss: 1.14861261844635
Epoch 1980, training loss: 310.3658142089844 = 0.051917143166065216 + 50.0 * 6.206278324127197
Epoch 1980, val loss: 1.1526527404785156
Epoch 1990, training loss: 310.56414794921875 = 0.05102526396512985 + 50.0 * 6.210262298583984
Epoch 1990, val loss: 1.1566756963729858
Epoch 2000, training loss: 310.3648986816406 = 0.050124235451221466 + 50.0 * 6.206295490264893
Epoch 2000, val loss: 1.1601312160491943
Epoch 2010, training loss: 310.3340148925781 = 0.049252718687057495 + 50.0 * 6.205695152282715
Epoch 2010, val loss: 1.1641526222229004
Epoch 2020, training loss: 310.5394287109375 = 0.04840952903032303 + 50.0 * 6.209820747375488
Epoch 2020, val loss: 1.1679861545562744
Epoch 2030, training loss: 310.2945251464844 = 0.047567807137966156 + 50.0 * 6.204938888549805
Epoch 2030, val loss: 1.171924114227295
Epoch 2040, training loss: 310.2999572753906 = 0.04676530137658119 + 50.0 * 6.205064296722412
Epoch 2040, val loss: 1.1759028434753418
Epoch 2050, training loss: 310.6415710449219 = 0.046001896262168884 + 50.0 * 6.211911678314209
Epoch 2050, val loss: 1.1797969341278076
Epoch 2060, training loss: 310.37005615234375 = 0.04519608989357948 + 50.0 * 6.2064971923828125
Epoch 2060, val loss: 1.1833837032318115
Epoch 2070, training loss: 310.2747497558594 = 0.04443877935409546 + 50.0 * 6.204606533050537
Epoch 2070, val loss: 1.1875249147415161
Epoch 2080, training loss: 310.2552185058594 = 0.04372129961848259 + 50.0 * 6.204229831695557
Epoch 2080, val loss: 1.1915029287338257
Epoch 2090, training loss: 310.4877624511719 = 0.043022461235523224 + 50.0 * 6.208894729614258
Epoch 2090, val loss: 1.195411205291748
Epoch 2100, training loss: 310.49749755859375 = 0.04228934273123741 + 50.0 * 6.209104061126709
Epoch 2100, val loss: 1.1988677978515625
Epoch 2110, training loss: 310.322265625 = 0.04158135503530502 + 50.0 * 6.205613136291504
Epoch 2110, val loss: 1.2028045654296875
Epoch 2120, training loss: 310.2290954589844 = 0.04089590162038803 + 50.0 * 6.203763961791992
Epoch 2120, val loss: 1.2067302465438843
Epoch 2130, training loss: 310.2075500488281 = 0.04025379568338394 + 50.0 * 6.203345775604248
Epoch 2130, val loss: 1.210878610610962
Epoch 2140, training loss: 310.2892761230469 = 0.0396304614841938 + 50.0 * 6.204992771148682
Epoch 2140, val loss: 1.2150441408157349
Epoch 2150, training loss: 310.32281494140625 = 0.03900257498025894 + 50.0 * 6.205676078796387
Epoch 2150, val loss: 1.2187731266021729
Epoch 2160, training loss: 310.2370910644531 = 0.03836812451481819 + 50.0 * 6.203974723815918
Epoch 2160, val loss: 1.2223223447799683
Epoch 2170, training loss: 310.30181884765625 = 0.03776958957314491 + 50.0 * 6.205280780792236
Epoch 2170, val loss: 1.226462960243225
Epoch 2180, training loss: 310.32928466796875 = 0.037180185317993164 + 50.0 * 6.2058424949646
Epoch 2180, val loss: 1.2302078008651733
Epoch 2190, training loss: 310.2879638671875 = 0.036595720797777176 + 50.0 * 6.2050275802612305
Epoch 2190, val loss: 1.2339993715286255
Epoch 2200, training loss: 310.2519226074219 = 0.03601851686835289 + 50.0 * 6.204318046569824
Epoch 2200, val loss: 1.237737774848938
Epoch 2210, training loss: 310.1606140136719 = 0.03546682745218277 + 50.0 * 6.202503204345703
Epoch 2210, val loss: 1.2416682243347168
Epoch 2220, training loss: 310.1840515136719 = 0.03493540734052658 + 50.0 * 6.202981948852539
Epoch 2220, val loss: 1.2456284761428833
Epoch 2230, training loss: 310.27789306640625 = 0.03440506383776665 + 50.0 * 6.204870223999023
Epoch 2230, val loss: 1.249485731124878
Epoch 2240, training loss: 310.2405090332031 = 0.03388281166553497 + 50.0 * 6.204132556915283
Epoch 2240, val loss: 1.2527271509170532
Epoch 2250, training loss: 310.1051940917969 = 0.033363357186317444 + 50.0 * 6.201436996459961
Epoch 2250, val loss: 1.2568495273590088
Epoch 2260, training loss: 310.2225646972656 = 0.032876402139663696 + 50.0 * 6.203794002532959
Epoch 2260, val loss: 1.2605979442596436
Epoch 2270, training loss: 310.2408447265625 = 0.03238251805305481 + 50.0 * 6.204169273376465
Epoch 2270, val loss: 1.26413893699646
Epoch 2280, training loss: 310.1305236816406 = 0.03189782053232193 + 50.0 * 6.201972484588623
Epoch 2280, val loss: 1.2682631015777588
Epoch 2290, training loss: 310.1064453125 = 0.03144149109721184 + 50.0 * 6.201499938964844
Epoch 2290, val loss: 1.2718865871429443
Epoch 2300, training loss: 310.0948486328125 = 0.03098425455391407 + 50.0 * 6.201277256011963
Epoch 2300, val loss: 1.2757720947265625
Epoch 2310, training loss: 310.51263427734375 = 0.030537741258740425 + 50.0 * 6.209641933441162
Epoch 2310, val loss: 1.2794053554534912
Epoch 2320, training loss: 310.1815490722656 = 0.030079152435064316 + 50.0 * 6.203029155731201
Epoch 2320, val loss: 1.2825247049331665
Epoch 2330, training loss: 310.0954895019531 = 0.02963779680430889 + 50.0 * 6.201317310333252
Epoch 2330, val loss: 1.28640878200531
Epoch 2340, training loss: 310.0804748535156 = 0.029222041368484497 + 50.0 * 6.201024532318115
Epoch 2340, val loss: 1.2903258800506592
Epoch 2350, training loss: 310.3362121582031 = 0.028820395469665527 + 50.0 * 6.206148147583008
Epoch 2350, val loss: 1.294130563735962
Epoch 2360, training loss: 310.1347351074219 = 0.028405040502548218 + 50.0 * 6.202126502990723
Epoch 2360, val loss: 1.2974625825881958
Epoch 2370, training loss: 310.0267028808594 = 0.027992604300379753 + 50.0 * 6.199974536895752
Epoch 2370, val loss: 1.301196575164795
Epoch 2380, training loss: 310.00396728515625 = 0.027608366683125496 + 50.0 * 6.199526786804199
Epoch 2380, val loss: 1.3048608303070068
Epoch 2390, training loss: 310.07684326171875 = 0.027238862589001656 + 50.0 * 6.200991630554199
Epoch 2390, val loss: 1.3086708784103394
Epoch 2400, training loss: 310.3190612792969 = 0.026856347918510437 + 50.0 * 6.205843925476074
Epoch 2400, val loss: 1.3117798566818237
Epoch 2410, training loss: 310.11614990234375 = 0.026467958465218544 + 50.0 * 6.201793670654297
Epoch 2410, val loss: 1.3150112628936768
Epoch 2420, training loss: 310.0123596191406 = 0.026091545820236206 + 50.0 * 6.199725151062012
Epoch 2420, val loss: 1.3185702562332153
Epoch 2430, training loss: 309.9877014160156 = 0.0257406085729599 + 50.0 * 6.199239730834961
Epoch 2430, val loss: 1.3224073648452759
Epoch 2440, training loss: 310.1335144042969 = 0.02540559321641922 + 50.0 * 6.202162742614746
Epoch 2440, val loss: 1.3257453441619873
Epoch 2450, training loss: 309.9757080078125 = 0.02505555935204029 + 50.0 * 6.1990132331848145
Epoch 2450, val loss: 1.3295865058898926
Epoch 2460, training loss: 310.0093078613281 = 0.024728301912546158 + 50.0 * 6.1996917724609375
Epoch 2460, val loss: 1.333032250404358
Epoch 2470, training loss: 310.1976013183594 = 0.024407071992754936 + 50.0 * 6.203464031219482
Epoch 2470, val loss: 1.3363897800445557
Epoch 2480, training loss: 309.9583740234375 = 0.024066712707281113 + 50.0 * 6.198686122894287
Epoch 2480, val loss: 1.339766263961792
Epoch 2490, training loss: 309.9217834472656 = 0.02374693937599659 + 50.0 * 6.19796085357666
Epoch 2490, val loss: 1.343356728553772
Epoch 2500, training loss: 309.9374694824219 = 0.02344825677573681 + 50.0 * 6.1982808113098145
Epoch 2500, val loss: 1.3468990325927734
Epoch 2510, training loss: 310.00457763671875 = 0.023156199604272842 + 50.0 * 6.1996283531188965
Epoch 2510, val loss: 1.3505139350891113
Epoch 2520, training loss: 310.0837097167969 = 0.022856486961245537 + 50.0 * 6.201217174530029
Epoch 2520, val loss: 1.3538084030151367
Epoch 2530, training loss: 310.0446472167969 = 0.022547384724020958 + 50.0 * 6.200441837310791
Epoch 2530, val loss: 1.3569198846817017
Epoch 2540, training loss: 309.98419189453125 = 0.022260386496782303 + 50.0 * 6.1992387771606445
Epoch 2540, val loss: 1.3604685068130493
Epoch 2550, training loss: 309.9364013671875 = 0.021981146186590195 + 50.0 * 6.198288440704346
Epoch 2550, val loss: 1.3636765480041504
Epoch 2560, training loss: 310.1465759277344 = 0.021705539897084236 + 50.0 * 6.202497482299805
Epoch 2560, val loss: 1.3670603036880493
Epoch 2570, training loss: 310.1091613769531 = 0.02142595313489437 + 50.0 * 6.201754570007324
Epoch 2570, val loss: 1.370521903038025
Epoch 2580, training loss: 310.0592956542969 = 0.021148288622498512 + 50.0 * 6.200762748718262
Epoch 2580, val loss: 1.3732694387435913
Epoch 2590, training loss: 309.9305419921875 = 0.020878130570054054 + 50.0 * 6.198193073272705
Epoch 2590, val loss: 1.376957654953003
Epoch 2600, training loss: 309.8710632324219 = 0.02062145061790943 + 50.0 * 6.1970086097717285
Epoch 2600, val loss: 1.3804266452789307
Epoch 2610, training loss: 309.915771484375 = 0.020382195711135864 + 50.0 * 6.197907447814941
Epoch 2610, val loss: 1.3837864398956299
Epoch 2620, training loss: 310.0230407714844 = 0.020138662308454514 + 50.0 * 6.2000579833984375
Epoch 2620, val loss: 1.3868622779846191
Epoch 2630, training loss: 309.9861755371094 = 0.01988999731838703 + 50.0 * 6.1993255615234375
Epoch 2630, val loss: 1.3899800777435303
Epoch 2640, training loss: 309.9167785644531 = 0.01964423432946205 + 50.0 * 6.19794225692749
Epoch 2640, val loss: 1.3932222127914429
Epoch 2650, training loss: 309.989013671875 = 0.019409431144595146 + 50.0 * 6.199391841888428
Epoch 2650, val loss: 1.3965482711791992
Epoch 2660, training loss: 309.8489685058594 = 0.019169945269823074 + 50.0 * 6.196596145629883
Epoch 2660, val loss: 1.3996556997299194
Epoch 2670, training loss: 309.9510498046875 = 0.01894419826567173 + 50.0 * 6.198641777038574
Epoch 2670, val loss: 1.4028679132461548
Epoch 2680, training loss: 310.1122131347656 = 0.018712712451815605 + 50.0 * 6.201869964599609
Epoch 2680, val loss: 1.4056841135025024
Epoch 2690, training loss: 309.88104248046875 = 0.01848171465098858 + 50.0 * 6.197250843048096
Epoch 2690, val loss: 1.408618688583374
Epoch 2700, training loss: 309.81829833984375 = 0.01826535351574421 + 50.0 * 6.196000576019287
Epoch 2700, val loss: 1.4120755195617676
Epoch 2710, training loss: 309.7992248535156 = 0.01806025393307209 + 50.0 * 6.195623397827148
Epoch 2710, val loss: 1.4152668714523315
Epoch 2720, training loss: 309.8696594238281 = 0.01786065101623535 + 50.0 * 6.197036266326904
Epoch 2720, val loss: 1.418657898902893
Epoch 2730, training loss: 310.0875549316406 = 0.01766066811978817 + 50.0 * 6.20139741897583
Epoch 2730, val loss: 1.4213048219680786
Epoch 2740, training loss: 309.8799133300781 = 0.017444785684347153 + 50.0 * 6.197249412536621
Epoch 2740, val loss: 1.4239286184310913
Epoch 2750, training loss: 309.8170471191406 = 0.017240867018699646 + 50.0 * 6.195996284484863
Epoch 2750, val loss: 1.4271011352539062
Epoch 2760, training loss: 309.843994140625 = 0.017054548487067223 + 50.0 * 6.19653844833374
Epoch 2760, val loss: 1.4301691055297852
Epoch 2770, training loss: 309.89739990234375 = 0.016864459961652756 + 50.0 * 6.197610378265381
Epoch 2770, val loss: 1.4331766366958618
Epoch 2780, training loss: 309.7987976074219 = 0.01667201891541481 + 50.0 * 6.195642471313477
Epoch 2780, val loss: 1.4359925985336304
Epoch 2790, training loss: 310.1347961425781 = 0.016492025926709175 + 50.0 * 6.202366352081299
Epoch 2790, val loss: 1.4390590190887451
Epoch 2800, training loss: 309.8957214355469 = 0.01630180887877941 + 50.0 * 6.1975884437561035
Epoch 2800, val loss: 1.441390037536621
Epoch 2810, training loss: 309.8307189941406 = 0.016114339232444763 + 50.0 * 6.196291923522949
Epoch 2810, val loss: 1.444637656211853
Epoch 2820, training loss: 309.8326110839844 = 0.01594523899257183 + 50.0 * 6.196333885192871
Epoch 2820, val loss: 1.4473201036453247
Epoch 2830, training loss: 309.81573486328125 = 0.015771377831697464 + 50.0 * 6.1959991455078125
Epoch 2830, val loss: 1.4504493474960327
Epoch 2840, training loss: 309.9771728515625 = 0.015606218948960304 + 50.0 * 6.1992316246032715
Epoch 2840, val loss: 1.4532469511032104
Epoch 2850, training loss: 309.8397521972656 = 0.015431099571287632 + 50.0 * 6.196485996246338
Epoch 2850, val loss: 1.4560770988464355
Epoch 2860, training loss: 309.85791015625 = 0.015262698754668236 + 50.0 * 6.196853160858154
Epoch 2860, val loss: 1.4584743976593018
Epoch 2870, training loss: 309.8839416503906 = 0.015096810646355152 + 50.0 * 6.197376728057861
Epoch 2870, val loss: 1.4615685939788818
Epoch 2880, training loss: 309.7333679199219 = 0.014929494820535183 + 50.0 * 6.194368839263916
Epoch 2880, val loss: 1.4641236066818237
Epoch 2890, training loss: 309.71929931640625 = 0.014777165837585926 + 50.0 * 6.194090366363525
Epoch 2890, val loss: 1.467031478881836
Epoch 2900, training loss: 309.6983642578125 = 0.014630468562245369 + 50.0 * 6.1936750411987305
Epoch 2900, val loss: 1.4700660705566406
Epoch 2910, training loss: 309.792724609375 = 0.014485226012766361 + 50.0 * 6.1955647468566895
Epoch 2910, val loss: 1.4728739261627197
Epoch 2920, training loss: 309.9133605957031 = 0.014329942874610424 + 50.0 * 6.1979804039001465
Epoch 2920, val loss: 1.4751911163330078
Epoch 2930, training loss: 309.8092956542969 = 0.014179971069097519 + 50.0 * 6.195902347564697
Epoch 2930, val loss: 1.4780917167663574
Epoch 2940, training loss: 309.863525390625 = 0.01403038576245308 + 50.0 * 6.196990013122559
Epoch 2940, val loss: 1.4807199239730835
Epoch 2950, training loss: 309.8009338378906 = 0.013883785344660282 + 50.0 * 6.195740699768066
Epoch 2950, val loss: 1.4834755659103394
Epoch 2960, training loss: 309.79083251953125 = 0.013740315102040768 + 50.0 * 6.195541858673096
Epoch 2960, val loss: 1.4861482381820679
Epoch 2970, training loss: 309.748779296875 = 0.013600887730717659 + 50.0 * 6.194703102111816
Epoch 2970, val loss: 1.4888077974319458
Epoch 2980, training loss: 309.8540954589844 = 0.013468099758028984 + 50.0 * 6.196812629699707
Epoch 2980, val loss: 1.4914976358413696
Epoch 2990, training loss: 309.69537353515625 = 0.013325738720595837 + 50.0 * 6.19364070892334
Epoch 2990, val loss: 1.4940153360366821
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 431.7830810546875 = 1.9413748979568481 + 50.0 * 8.596834182739258
Epoch 0, val loss: 1.944158911705017
Epoch 10, training loss: 431.7336120605469 = 1.9325916767120361 + 50.0 * 8.596020698547363
Epoch 10, val loss: 1.9348381757736206
Epoch 20, training loss: 431.4479675292969 = 1.9214372634887695 + 50.0 * 8.590530395507812
Epoch 20, val loss: 1.9230091571807861
Epoch 30, training loss: 429.4969177246094 = 1.9069693088531494 + 50.0 * 8.551798820495605
Epoch 30, val loss: 1.9074739217758179
Epoch 40, training loss: 417.49700927734375 = 1.8894000053405762 + 50.0 * 8.312151908874512
Epoch 40, val loss: 1.8894399404525757
Epoch 50, training loss: 375.9970397949219 = 1.8701502084732056 + 50.0 * 7.482537746429443
Epoch 50, val loss: 1.8696154356002808
Epoch 60, training loss: 364.32470703125 = 1.8550118207931519 + 50.0 * 7.249393939971924
Epoch 60, val loss: 1.8551467657089233
Epoch 70, training loss: 353.899658203125 = 1.8437256813049316 + 50.0 * 7.041118621826172
Epoch 70, val loss: 1.8437851667404175
Epoch 80, training loss: 347.5665283203125 = 1.8318772315979004 + 50.0 * 6.9146928787231445
Epoch 80, val loss: 1.8316645622253418
Epoch 90, training loss: 341.9888610839844 = 1.8212398290634155 + 50.0 * 6.8033528327941895
Epoch 90, val loss: 1.8209171295166016
Epoch 100, training loss: 337.603759765625 = 1.811518907546997 + 50.0 * 6.715844631195068
Epoch 100, val loss: 1.8110753297805786
Epoch 110, training loss: 334.4169616699219 = 1.8028819561004639 + 50.0 * 6.652281761169434
Epoch 110, val loss: 1.8022540807724
Epoch 120, training loss: 332.169189453125 = 1.7938883304595947 + 50.0 * 6.607506275177002
Epoch 120, val loss: 1.7930244207382202
Epoch 130, training loss: 330.3004150390625 = 1.784145712852478 + 50.0 * 6.5703253746032715
Epoch 130, val loss: 1.7831605672836304
Epoch 140, training loss: 328.7763366699219 = 1.7743405103683472 + 50.0 * 6.540039539337158
Epoch 140, val loss: 1.7731579542160034
Epoch 150, training loss: 327.51934814453125 = 1.7638550996780396 + 50.0 * 6.515110015869141
Epoch 150, val loss: 1.7625807523727417
Epoch 160, training loss: 326.4858703613281 = 1.7524563074111938 + 50.0 * 6.494668483734131
Epoch 160, val loss: 1.7513339519500732
Epoch 170, training loss: 325.6719055175781 = 1.7401494979858398 + 50.0 * 6.478635311126709
Epoch 170, val loss: 1.7392287254333496
Epoch 180, training loss: 324.907958984375 = 1.7266461849212646 + 50.0 * 6.463626384735107
Epoch 180, val loss: 1.7262910604476929
Epoch 190, training loss: 324.1261291503906 = 1.7119295597076416 + 50.0 * 6.448283672332764
Epoch 190, val loss: 1.7123469114303589
Epoch 200, training loss: 323.48577880859375 = 1.6960581541061401 + 50.0 * 6.435794830322266
Epoch 200, val loss: 1.697407603263855
Epoch 210, training loss: 323.0390930175781 = 1.678786277770996 + 50.0 * 6.427206516265869
Epoch 210, val loss: 1.6813961267471313
Epoch 220, training loss: 322.4169006347656 = 1.660137414932251 + 50.0 * 6.415135383605957
Epoch 220, val loss: 1.6641645431518555
Epoch 230, training loss: 321.8680419921875 = 1.6401822566986084 + 50.0 * 6.404557704925537
Epoch 230, val loss: 1.645997166633606
Epoch 240, training loss: 321.39453125 = 1.6189755201339722 + 50.0 * 6.395510673522949
Epoch 240, val loss: 1.62691068649292
Epoch 250, training loss: 321.14056396484375 = 1.59639310836792 + 50.0 * 6.390882968902588
Epoch 250, val loss: 1.6067314147949219
Epoch 260, training loss: 320.5942077636719 = 1.5728089809417725 + 50.0 * 6.380427837371826
Epoch 260, val loss: 1.5859653949737549
Epoch 270, training loss: 320.1805419921875 = 1.5483754873275757 + 50.0 * 6.37264347076416
Epoch 270, val loss: 1.5647867918014526
Epoch 280, training loss: 320.0120544433594 = 1.5232526063919067 + 50.0 * 6.369775772094727
Epoch 280, val loss: 1.543200969696045
Epoch 290, training loss: 319.52734375 = 1.497581958770752 + 50.0 * 6.360595226287842
Epoch 290, val loss: 1.5214111804962158
Epoch 300, training loss: 319.22125244140625 = 1.471585988998413 + 50.0 * 6.3549933433532715
Epoch 300, val loss: 1.4997715950012207
Epoch 310, training loss: 319.1051940917969 = 1.445468544960022 + 50.0 * 6.353194713592529
Epoch 310, val loss: 1.4783178567886353
Epoch 320, training loss: 318.7467346191406 = 1.4191429615020752 + 50.0 * 6.346551895141602
Epoch 320, val loss: 1.457108497619629
Epoch 330, training loss: 318.5234069824219 = 1.3928970098495483 + 50.0 * 6.3426103591918945
Epoch 330, val loss: 1.4361273050308228
Epoch 340, training loss: 318.227783203125 = 1.3668042421340942 + 50.0 * 6.337219715118408
Epoch 340, val loss: 1.4154367446899414
Epoch 350, training loss: 318.01043701171875 = 1.3409383296966553 + 50.0 * 6.333389759063721
Epoch 350, val loss: 1.3953303098678589
Epoch 360, training loss: 317.847900390625 = 1.3152759075164795 + 50.0 * 6.330652236938477
Epoch 360, val loss: 1.3756967782974243
Epoch 370, training loss: 317.692138671875 = 1.2896850109100342 + 50.0 * 6.328049182891846
Epoch 370, val loss: 1.3564218282699585
Epoch 380, training loss: 317.4705810546875 = 1.2643530368804932 + 50.0 * 6.324124813079834
Epoch 380, val loss: 1.337424635887146
Epoch 390, training loss: 317.2658386230469 = 1.2393115758895874 + 50.0 * 6.320530414581299
Epoch 390, val loss: 1.3191447257995605
Epoch 400, training loss: 317.0783996582031 = 1.2145904302597046 + 50.0 * 6.3172760009765625
Epoch 400, val loss: 1.3013865947723389
Epoch 410, training loss: 317.0931396484375 = 1.1901404857635498 + 50.0 * 6.31805944442749
Epoch 410, val loss: 1.2840372323989868
Epoch 420, training loss: 316.8388366699219 = 1.1660584211349487 + 50.0 * 6.313456058502197
Epoch 420, val loss: 1.2670706510543823
Epoch 430, training loss: 316.63262939453125 = 1.1423434019088745 + 50.0 * 6.309805870056152
Epoch 430, val loss: 1.2509031295776367
Epoch 440, training loss: 316.6286315917969 = 1.1189448833465576 + 50.0 * 6.3101935386657715
Epoch 440, val loss: 1.2351621389389038
Epoch 450, training loss: 316.33099365234375 = 1.0961894989013672 + 50.0 * 6.304696083068848
Epoch 450, val loss: 1.2202116250991821
Epoch 460, training loss: 316.17279052734375 = 1.0739916563034058 + 50.0 * 6.301975727081299
Epoch 460, val loss: 1.2060126066207886
Epoch 470, training loss: 316.1695861816406 = 1.052337408065796 + 50.0 * 6.302344799041748
Epoch 470, val loss: 1.1924479007720947
Epoch 480, training loss: 316.04376220703125 = 1.031170129776001 + 50.0 * 6.3002519607543945
Epoch 480, val loss: 1.1792739629745483
Epoch 490, training loss: 315.7673034667969 = 1.0105526447296143 + 50.0 * 6.295135021209717
Epoch 490, val loss: 1.1670386791229248
Epoch 500, training loss: 315.65179443359375 = 0.9905921220779419 + 50.0 * 6.293223857879639
Epoch 500, val loss: 1.1556775569915771
Epoch 510, training loss: 315.8738098144531 = 0.9710838794708252 + 50.0 * 6.2980546951293945
Epoch 510, val loss: 1.1447817087173462
Epoch 520, training loss: 315.5314025878906 = 0.9520515203475952 + 50.0 * 6.291586875915527
Epoch 520, val loss: 1.134325623512268
Epoch 530, training loss: 315.3275451660156 = 0.9336022138595581 + 50.0 * 6.28787899017334
Epoch 530, val loss: 1.124565601348877
Epoch 540, training loss: 315.21051025390625 = 0.9157131910324097 + 50.0 * 6.285896301269531
Epoch 540, val loss: 1.1153405904769897
Epoch 550, training loss: 315.3255310058594 = 0.8982326984405518 + 50.0 * 6.288546085357666
Epoch 550, val loss: 1.1065815687179565
Epoch 560, training loss: 315.1348571777344 = 0.8811682462692261 + 50.0 * 6.285073757171631
Epoch 560, val loss: 1.0982235670089722
Epoch 570, training loss: 315.0242919921875 = 0.8644136190414429 + 50.0 * 6.28319787979126
Epoch 570, val loss: 1.0903348922729492
Epoch 580, training loss: 314.88671875 = 0.8480995893478394 + 50.0 * 6.2807722091674805
Epoch 580, val loss: 1.0826886892318726
Epoch 590, training loss: 314.761474609375 = 0.8321500420570374 + 50.0 * 6.278586387634277
Epoch 590, val loss: 1.0753810405731201
Epoch 600, training loss: 314.7428894042969 = 0.8165654540061951 + 50.0 * 6.278526306152344
Epoch 600, val loss: 1.0685036182403564
Epoch 610, training loss: 314.5998840332031 = 0.8011505603790283 + 50.0 * 6.275974750518799
Epoch 610, val loss: 1.0616298913955688
Epoch 620, training loss: 314.511474609375 = 0.786050021648407 + 50.0 * 6.274508476257324
Epoch 620, val loss: 1.055262565612793
Epoch 630, training loss: 314.4090881347656 = 0.7712769508361816 + 50.0 * 6.272756576538086
Epoch 630, val loss: 1.0489795207977295
Epoch 640, training loss: 314.4423522949219 = 0.7566837072372437 + 50.0 * 6.273713111877441
Epoch 640, val loss: 1.0428247451782227
Epoch 650, training loss: 314.3261413574219 = 0.7422053813934326 + 50.0 * 6.271678447723389
Epoch 650, val loss: 1.0369116067886353
Epoch 660, training loss: 314.1874084472656 = 0.7280113697052002 + 50.0 * 6.269187927246094
Epoch 660, val loss: 1.031107783317566
Epoch 670, training loss: 314.2791748046875 = 0.714077889919281 + 50.0 * 6.271301746368408
Epoch 670, val loss: 1.025437593460083
Epoch 680, training loss: 314.06817626953125 = 0.7001703977584839 + 50.0 * 6.267360210418701
Epoch 680, val loss: 1.019883632659912
Epoch 690, training loss: 313.9671936035156 = 0.6865812540054321 + 50.0 * 6.2656121253967285
Epoch 690, val loss: 1.0145431756973267
Epoch 700, training loss: 313.95263671875 = 0.6731674075126648 + 50.0 * 6.265589714050293
Epoch 700, val loss: 1.0093127489089966
Epoch 710, training loss: 313.8805236816406 = 0.6597574353218079 + 50.0 * 6.264415264129639
Epoch 710, val loss: 1.004029631614685
Epoch 720, training loss: 313.9124450683594 = 0.6465178728103638 + 50.0 * 6.265318870544434
Epoch 720, val loss: 0.99885493516922
Epoch 730, training loss: 313.703125 = 0.6334975957870483 + 50.0 * 6.261392593383789
Epoch 730, val loss: 0.993886411190033
Epoch 740, training loss: 313.6671142578125 = 0.6206870079040527 + 50.0 * 6.260928630828857
Epoch 740, val loss: 0.9890434145927429
Epoch 750, training loss: 313.7087097167969 = 0.6080572605133057 + 50.0 * 6.262012958526611
Epoch 750, val loss: 0.9842869639396667
Epoch 760, training loss: 313.62518310546875 = 0.5954335927963257 + 50.0 * 6.260594844818115
Epoch 760, val loss: 0.9794040322303772
Epoch 770, training loss: 313.5462646484375 = 0.5829935073852539 + 50.0 * 6.259264945983887
Epoch 770, val loss: 0.9748584032058716
Epoch 780, training loss: 313.4904479980469 = 0.5707716345787048 + 50.0 * 6.25839376449585
Epoch 780, val loss: 0.970316469669342
Epoch 790, training loss: 313.4398498535156 = 0.5586926937103271 + 50.0 * 6.257623195648193
Epoch 790, val loss: 0.9661203026771545
Epoch 800, training loss: 313.3446960449219 = 0.5467687845230103 + 50.0 * 6.255958557128906
Epoch 800, val loss: 0.9619454741477966
Epoch 810, training loss: 313.30462646484375 = 0.5350777506828308 + 50.0 * 6.2553911209106445
Epoch 810, val loss: 0.9579910635948181
Epoch 820, training loss: 313.3544006347656 = 0.5235570669174194 + 50.0 * 6.256617069244385
Epoch 820, val loss: 0.9540484547615051
Epoch 830, training loss: 313.23236083984375 = 0.5121472477912903 + 50.0 * 6.254404544830322
Epoch 830, val loss: 0.9505100846290588
Epoch 840, training loss: 313.133056640625 = 0.5009229779243469 + 50.0 * 6.25264310836792
Epoch 840, val loss: 0.9469148516654968
Epoch 850, training loss: 313.099365234375 = 0.48996901512145996 + 50.0 * 6.252188205718994
Epoch 850, val loss: 0.9438086152076721
Epoch 860, training loss: 313.1263122558594 = 0.4791889786720276 + 50.0 * 6.2529425621032715
Epoch 860, val loss: 0.9404556751251221
Epoch 870, training loss: 313.1425476074219 = 0.4685383439064026 + 50.0 * 6.253480434417725
Epoch 870, val loss: 0.9372901320457458
Epoch 880, training loss: 313.087646484375 = 0.45802703499794006 + 50.0 * 6.252592086791992
Epoch 880, val loss: 0.9343705773353577
Epoch 890, training loss: 312.8878479003906 = 0.44776394963264465 + 50.0 * 6.2488017082214355
Epoch 890, val loss: 0.9315659999847412
Epoch 900, training loss: 312.808349609375 = 0.43778732419013977 + 50.0 * 6.247411251068115
Epoch 900, val loss: 0.9290632605552673
Epoch 910, training loss: 312.8723449707031 = 0.42803943157196045 + 50.0 * 6.2488861083984375
Epoch 910, val loss: 0.9266186952590942
Epoch 920, training loss: 312.755126953125 = 0.4184047281742096 + 50.0 * 6.246734619140625
Epoch 920, val loss: 0.9243676066398621
Epoch 930, training loss: 312.7693786621094 = 0.40892213582992554 + 50.0 * 6.247208595275879
Epoch 930, val loss: 0.9221490621566772
Epoch 940, training loss: 312.7786560058594 = 0.39973002672195435 + 50.0 * 6.2475786209106445
Epoch 940, val loss: 0.9202706217765808
Epoch 950, training loss: 312.606689453125 = 0.39070576429367065 + 50.0 * 6.244319438934326
Epoch 950, val loss: 0.9185408353805542
Epoch 960, training loss: 312.58868408203125 = 0.3819328546524048 + 50.0 * 6.244134902954102
Epoch 960, val loss: 0.9169683456420898
Epoch 970, training loss: 312.579833984375 = 0.3733464181423187 + 50.0 * 6.244129657745361
Epoch 970, val loss: 0.9155920743942261
Epoch 980, training loss: 312.77740478515625 = 0.3649122416973114 + 50.0 * 6.2482500076293945
Epoch 980, val loss: 0.9143024682998657
Epoch 990, training loss: 312.51556396484375 = 0.3565334379673004 + 50.0 * 6.243180751800537
Epoch 990, val loss: 0.9130858182907104
Epoch 1000, training loss: 312.44354248046875 = 0.34844279289245605 + 50.0 * 6.241901874542236
Epoch 1000, val loss: 0.9121271371841431
Epoch 1010, training loss: 312.4384765625 = 0.34061992168426514 + 50.0 * 6.241957187652588
Epoch 1010, val loss: 0.9113950133323669
Epoch 1020, training loss: 312.41815185546875 = 0.3329075574874878 + 50.0 * 6.241704940795898
Epoch 1020, val loss: 0.9107726216316223
Epoch 1030, training loss: 312.31097412109375 = 0.3253515362739563 + 50.0 * 6.239712238311768
Epoch 1030, val loss: 0.9102602601051331
Epoch 1040, training loss: 312.2689208984375 = 0.3180510997772217 + 50.0 * 6.239017486572266
Epoch 1040, val loss: 0.9100831151008606
Epoch 1050, training loss: 312.38775634765625 = 0.3109445869922638 + 50.0 * 6.2415361404418945
Epoch 1050, val loss: 0.9099549651145935
Epoch 1060, training loss: 312.3883972167969 = 0.30384036898612976 + 50.0 * 6.2416911125183105
Epoch 1060, val loss: 0.9097710251808167
Epoch 1070, training loss: 312.2663269042969 = 0.2969643771648407 + 50.0 * 6.239387512207031
Epoch 1070, val loss: 0.909748911857605
Epoch 1080, training loss: 312.1473693847656 = 0.2902790307998657 + 50.0 * 6.2371416091918945
Epoch 1080, val loss: 0.9099841713905334
Epoch 1090, training loss: 312.2356262207031 = 0.2837980389595032 + 50.0 * 6.239037036895752
Epoch 1090, val loss: 0.9104032516479492
Epoch 1100, training loss: 312.08209228515625 = 0.2774169147014618 + 50.0 * 6.236093521118164
Epoch 1100, val loss: 0.9107983112335205
Epoch 1110, training loss: 312.0780944824219 = 0.2712146043777466 + 50.0 * 6.236137390136719
Epoch 1110, val loss: 0.9113160967826843
Epoch 1120, training loss: 312.2433776855469 = 0.2651827335357666 + 50.0 * 6.239563465118408
Epoch 1120, val loss: 0.9120212197303772
Epoch 1130, training loss: 312.2201843261719 = 0.2591748833656311 + 50.0 * 6.239219665527344
Epoch 1130, val loss: 0.9127936959266663
Epoch 1140, training loss: 312.0123291015625 = 0.2533300220966339 + 50.0 * 6.235179901123047
Epoch 1140, val loss: 0.913517415523529
Epoch 1150, training loss: 311.9490661621094 = 0.24769896268844604 + 50.0 * 6.23402738571167
Epoch 1150, val loss: 0.9145569801330566
Epoch 1160, training loss: 311.907958984375 = 0.24226199090480804 + 50.0 * 6.233314514160156
Epoch 1160, val loss: 0.9157850742340088
Epoch 1170, training loss: 312.06494140625 = 0.23694546520709991 + 50.0 * 6.236560344696045
Epoch 1170, val loss: 0.9169567823410034
Epoch 1180, training loss: 312.0132751464844 = 0.2316301017999649 + 50.0 * 6.23563289642334
Epoch 1180, val loss: 0.9181681275367737
Epoch 1190, training loss: 311.8655090332031 = 0.22646498680114746 + 50.0 * 6.232780933380127
Epoch 1190, val loss: 0.9192839860916138
Epoch 1200, training loss: 311.8302917480469 = 0.22147685289382935 + 50.0 * 6.232176303863525
Epoch 1200, val loss: 0.9207628965377808
Epoch 1210, training loss: 311.9833984375 = 0.21664342284202576 + 50.0 * 6.235335350036621
Epoch 1210, val loss: 0.9223224520683289
Epoch 1220, training loss: 311.7929382324219 = 0.21184170246124268 + 50.0 * 6.231621742248535
Epoch 1220, val loss: 0.9237930774688721
Epoch 1230, training loss: 311.8187561035156 = 0.2071930468082428 + 50.0 * 6.232231140136719
Epoch 1230, val loss: 0.9254292845726013
Epoch 1240, training loss: 311.6873474121094 = 0.20267151296138763 + 50.0 * 6.22969388961792
Epoch 1240, val loss: 0.9270764589309692
Epoch 1250, training loss: 311.68768310546875 = 0.1983141452074051 + 50.0 * 6.229787826538086
Epoch 1250, val loss: 0.9289355278015137
Epoch 1260, training loss: 311.8193664550781 = 0.19402417540550232 + 50.0 * 6.23250675201416
Epoch 1260, val loss: 0.9307973384857178
Epoch 1270, training loss: 311.7498779296875 = 0.18976877629756927 + 50.0 * 6.231202125549316
Epoch 1270, val loss: 0.9326183199882507
Epoch 1280, training loss: 311.6511535644531 = 0.18563400208950043 + 50.0 * 6.229310512542725
Epoch 1280, val loss: 0.9344865679740906
Epoch 1290, training loss: 311.5815124511719 = 0.18165776133537292 + 50.0 * 6.227996826171875
Epoch 1290, val loss: 0.9366334080696106
Epoch 1300, training loss: 311.75555419921875 = 0.17780031263828278 + 50.0 * 6.231554985046387
Epoch 1300, val loss: 0.9387582540512085
Epoch 1310, training loss: 311.6128234863281 = 0.17392559349536896 + 50.0 * 6.228777885437012
Epoch 1310, val loss: 0.9408665299415588
Epoch 1320, training loss: 311.59326171875 = 0.17018067836761475 + 50.0 * 6.228462219238281
Epoch 1320, val loss: 0.9428842663764954
Epoch 1330, training loss: 311.57464599609375 = 0.1665688008069992 + 50.0 * 6.228161334991455
Epoch 1330, val loss: 0.945275068283081
Epoch 1340, training loss: 311.49755859375 = 0.1630389243364334 + 50.0 * 6.226690769195557
Epoch 1340, val loss: 0.9476208686828613
Epoch 1350, training loss: 311.4230041503906 = 0.15960381925106049 + 50.0 * 6.2252678871154785
Epoch 1350, val loss: 0.9499534368515015
Epoch 1360, training loss: 311.5602111816406 = 0.15628382563591003 + 50.0 * 6.228078365325928
Epoch 1360, val loss: 0.9522968530654907
Epoch 1370, training loss: 311.4060363769531 = 0.15298748016357422 + 50.0 * 6.225060939788818
Epoch 1370, val loss: 0.9547102451324463
Epoch 1380, training loss: 311.4748840332031 = 0.14977215230464935 + 50.0 * 6.226501941680908
Epoch 1380, val loss: 0.9571523070335388
Epoch 1390, training loss: 311.4403381347656 = 0.1466199904680252 + 50.0 * 6.225874423980713
Epoch 1390, val loss: 0.9597638249397278
Epoch 1400, training loss: 311.39556884765625 = 0.14357300102710724 + 50.0 * 6.225039958953857
Epoch 1400, val loss: 0.9622661471366882
Epoch 1410, training loss: 311.5130920410156 = 0.1405683308839798 + 50.0 * 6.227450847625732
Epoch 1410, val loss: 0.9648071527481079
Epoch 1420, training loss: 311.2882385253906 = 0.13759826123714447 + 50.0 * 6.223012924194336
Epoch 1420, val loss: 0.9671186804771423
Epoch 1430, training loss: 311.2713317871094 = 0.13477039337158203 + 50.0 * 6.222731113433838
Epoch 1430, val loss: 0.9700900316238403
Epoch 1440, training loss: 311.23614501953125 = 0.1320653110742569 + 50.0 * 6.222081184387207
Epoch 1440, val loss: 0.9728721380233765
Epoch 1450, training loss: 311.21063232421875 = 0.12939807772636414 + 50.0 * 6.221624851226807
Epoch 1450, val loss: 0.9757506251335144
Epoch 1460, training loss: 311.5281982421875 = 0.12680432200431824 + 50.0 * 6.228027820587158
Epoch 1460, val loss: 0.9786327481269836
Epoch 1470, training loss: 311.41900634765625 = 0.12416484206914902 + 50.0 * 6.225896835327148
Epoch 1470, val loss: 0.9812076091766357
Epoch 1480, training loss: 311.2425842285156 = 0.12160234153270721 + 50.0 * 6.222419738769531
Epoch 1480, val loss: 0.9840526580810547
Epoch 1490, training loss: 311.1640319824219 = 0.11915063858032227 + 50.0 * 6.220897674560547
Epoch 1490, val loss: 0.9870622158050537
Epoch 1500, training loss: 311.45770263671875 = 0.11676416546106339 + 50.0 * 6.226819038391113
Epoch 1500, val loss: 0.990084171295166
Epoch 1510, training loss: 311.2007141113281 = 0.11438612639904022 + 50.0 * 6.221726417541504
Epoch 1510, val loss: 0.9930840134620667
Epoch 1520, training loss: 311.1286315917969 = 0.11209879070520401 + 50.0 * 6.220330715179443
Epoch 1520, val loss: 0.9961090087890625
Epoch 1530, training loss: 311.096923828125 = 0.10987872630357742 + 50.0 * 6.219740867614746
Epoch 1530, val loss: 0.9992585182189941
Epoch 1540, training loss: 311.2050476074219 = 0.1077185645699501 + 50.0 * 6.2219462394714355
Epoch 1540, val loss: 1.0023995637893677
Epoch 1550, training loss: 311.0698547363281 = 0.10555688291788101 + 50.0 * 6.21928596496582
Epoch 1550, val loss: 1.005423903465271
Epoch 1560, training loss: 311.09442138671875 = 0.10346424579620361 + 50.0 * 6.21981954574585
Epoch 1560, val loss: 1.0085169076919556
Epoch 1570, training loss: 311.251708984375 = 0.10140789300203323 + 50.0 * 6.223005771636963
Epoch 1570, val loss: 1.011582612991333
Epoch 1580, training loss: 311.0574035644531 = 0.09937849640846252 + 50.0 * 6.219161033630371
Epoch 1580, val loss: 1.0144227743148804
Epoch 1590, training loss: 311.0013122558594 = 0.09743358939886093 + 50.0 * 6.218077659606934
Epoch 1590, val loss: 1.0177806615829468
Epoch 1600, training loss: 310.9759826660156 = 0.09555993974208832 + 50.0 * 6.21760892868042
Epoch 1600, val loss: 1.0209925174713135
Epoch 1610, training loss: 311.16082763671875 = 0.09372459352016449 + 50.0 * 6.221342086791992
Epoch 1610, val loss: 1.0242314338684082
Epoch 1620, training loss: 310.9866943359375 = 0.0918923020362854 + 50.0 * 6.217895984649658
Epoch 1620, val loss: 1.0270147323608398
Epoch 1630, training loss: 311.0472106933594 = 0.09009493887424469 + 50.0 * 6.219142436981201
Epoch 1630, val loss: 1.0302914381027222
Epoch 1640, training loss: 310.95916748046875 = 0.08831050992012024 + 50.0 * 6.217417240142822
Epoch 1640, val loss: 1.0336575508117676
Epoch 1650, training loss: 310.99267578125 = 0.0866212472319603 + 50.0 * 6.21812105178833
Epoch 1650, val loss: 1.0367940664291382
Epoch 1660, training loss: 310.96429443359375 = 0.08495448529720306 + 50.0 * 6.217586517333984
Epoch 1660, val loss: 1.0400009155273438
Epoch 1670, training loss: 310.85302734375 = 0.0833233967423439 + 50.0 * 6.215394020080566
Epoch 1670, val loss: 1.0433553457260132
Epoch 1680, training loss: 310.8596496582031 = 0.0817539244890213 + 50.0 * 6.21555757522583
Epoch 1680, val loss: 1.0466958284378052
Epoch 1690, training loss: 310.881103515625 = 0.08021870255470276 + 50.0 * 6.216017723083496
Epoch 1690, val loss: 1.050211787223816
Epoch 1700, training loss: 311.03753662109375 = 0.07869723439216614 + 50.0 * 6.219176769256592
Epoch 1700, val loss: 1.0533924102783203
Epoch 1710, training loss: 310.8673400878906 = 0.07716338336467743 + 50.0 * 6.215804100036621
Epoch 1710, val loss: 1.0563576221466064
Epoch 1720, training loss: 310.8149719238281 = 0.07570815831422806 + 50.0 * 6.214785099029541
Epoch 1720, val loss: 1.059669852256775
Epoch 1730, training loss: 310.79962158203125 = 0.07429947704076767 + 50.0 * 6.214506149291992
Epoch 1730, val loss: 1.0630409717559814
Epoch 1740, training loss: 310.8006896972656 = 0.07292085140943527 + 50.0 * 6.214555740356445
Epoch 1740, val loss: 1.0664575099945068
Epoch 1750, training loss: 310.92822265625 = 0.07157807052135468 + 50.0 * 6.217133045196533
Epoch 1750, val loss: 1.06959867477417
Epoch 1760, training loss: 310.97015380859375 = 0.07022292912006378 + 50.0 * 6.217998504638672
Epoch 1760, val loss: 1.0728697776794434
Epoch 1770, training loss: 310.7818603515625 = 0.06889288127422333 + 50.0 * 6.214259624481201
Epoch 1770, val loss: 1.0762439966201782
Epoch 1780, training loss: 310.7468566894531 = 0.06763733178377151 + 50.0 * 6.2135844230651855
Epoch 1780, val loss: 1.0794775485992432
Epoch 1790, training loss: 310.97198486328125 = 0.06639707088470459 + 50.0 * 6.218111515045166
Epoch 1790, val loss: 1.0830953121185303
Epoch 1800, training loss: 310.73046875 = 0.06517865508794785 + 50.0 * 6.213305950164795
Epoch 1800, val loss: 1.0859581232070923
Epoch 1810, training loss: 310.66131591796875 = 0.06399041414260864 + 50.0 * 6.211946487426758
Epoch 1810, val loss: 1.0895535945892334
Epoch 1820, training loss: 310.7544860839844 = 0.06285441666841507 + 50.0 * 6.213832378387451
Epoch 1820, val loss: 1.0929629802703857
Epoch 1830, training loss: 310.6804504394531 = 0.06169118732213974 + 50.0 * 6.212375640869141
Epoch 1830, val loss: 1.096109390258789
Epoch 1840, training loss: 310.6127624511719 = 0.060557156801223755 + 50.0 * 6.2110443115234375
Epoch 1840, val loss: 1.0990527868270874
Epoch 1850, training loss: 310.6167907714844 = 0.05947327986359596 + 50.0 * 6.211146354675293
Epoch 1850, val loss: 1.1026579141616821
Epoch 1860, training loss: 310.82110595703125 = 0.05843415483832359 + 50.0 * 6.2152533531188965
Epoch 1860, val loss: 1.10592782497406
Epoch 1870, training loss: 310.6422424316406 = 0.05737605690956116 + 50.0 * 6.211697578430176
Epoch 1870, val loss: 1.1087483167648315
Epoch 1880, training loss: 310.583251953125 = 0.05634109303355217 + 50.0 * 6.210538387298584
Epoch 1880, val loss: 1.1120610237121582
Epoch 1890, training loss: 310.5678405761719 = 0.05534449219703674 + 50.0 * 6.210249423980713
Epoch 1890, val loss: 1.115572452545166
Epoch 1900, training loss: 310.6365051269531 = 0.054388727992773056 + 50.0 * 6.211642742156982
Epoch 1900, val loss: 1.118789553642273
Epoch 1910, training loss: 310.61474609375 = 0.05342922732234001 + 50.0 * 6.211225986480713
Epoch 1910, val loss: 1.1220828294754028
Epoch 1920, training loss: 310.8814697265625 = 0.05248488113284111 + 50.0 * 6.216579437255859
Epoch 1920, val loss: 1.1249226331710815
Epoch 1930, training loss: 310.5748291015625 = 0.051530737429857254 + 50.0 * 6.210465908050537
Epoch 1930, val loss: 1.1281400918960571
Epoch 1940, training loss: 310.5339050292969 = 0.05064046382904053 + 50.0 * 6.209665775299072
Epoch 1940, val loss: 1.1312788724899292
Epoch 1950, training loss: 310.48016357421875 = 0.049779828637838364 + 50.0 * 6.2086076736450195
Epoch 1950, val loss: 1.134669303894043
Epoch 1960, training loss: 310.4607238769531 = 0.04894889518618584 + 50.0 * 6.208235740661621
Epoch 1960, val loss: 1.1379456520080566
Epoch 1970, training loss: 310.774658203125 = 0.04814084246754646 + 50.0 * 6.2145304679870605
Epoch 1970, val loss: 1.1410226821899414
Epoch 1980, training loss: 310.472900390625 = 0.047278992831707 + 50.0 * 6.208512783050537
Epoch 1980, val loss: 1.1441469192504883
Epoch 1990, training loss: 310.56451416015625 = 0.046471405774354935 + 50.0 * 6.210361003875732
Epoch 1990, val loss: 1.1473329067230225
Epoch 2000, training loss: 310.4548645019531 = 0.04567328840494156 + 50.0 * 6.208183765411377
Epoch 2000, val loss: 1.150447130203247
Epoch 2010, training loss: 310.4520568847656 = 0.04491657763719559 + 50.0 * 6.2081427574157715
Epoch 2010, val loss: 1.1536723375320435
Epoch 2020, training loss: 310.46270751953125 = 0.04416879266500473 + 50.0 * 6.208371162414551
Epoch 2020, val loss: 1.1570545434951782
Epoch 2030, training loss: 310.5472412109375 = 0.04343201220035553 + 50.0 * 6.210076332092285
Epoch 2030, val loss: 1.1601240634918213
Epoch 2040, training loss: 310.5019226074219 = 0.04270442575216293 + 50.0 * 6.209184646606445
Epoch 2040, val loss: 1.1631619930267334
Epoch 2050, training loss: 310.4993591308594 = 0.041994549334049225 + 50.0 * 6.2091474533081055
Epoch 2050, val loss: 1.1665385961532593
Epoch 2060, training loss: 310.3653259277344 = 0.04130953922867775 + 50.0 * 6.206480026245117
Epoch 2060, val loss: 1.1693923473358154
Epoch 2070, training loss: 310.3662414550781 = 0.04064540937542915 + 50.0 * 6.206511974334717
Epoch 2070, val loss: 1.1724356412887573
Epoch 2080, training loss: 310.6122131347656 = 0.04000304266810417 + 50.0 * 6.21144437789917
Epoch 2080, val loss: 1.1755672693252563
Epoch 2090, training loss: 310.4642028808594 = 0.03932266682386398 + 50.0 * 6.208497524261475
Epoch 2090, val loss: 1.1788631677627563
Epoch 2100, training loss: 310.4329833984375 = 0.03868567943572998 + 50.0 * 6.2078857421875
Epoch 2100, val loss: 1.1814061403274536
Epoch 2110, training loss: 310.44903564453125 = 0.03805742412805557 + 50.0 * 6.208219528198242
Epoch 2110, val loss: 1.1846773624420166
Epoch 2120, training loss: 310.3148498535156 = 0.037452332675457 + 50.0 * 6.205548286437988
Epoch 2120, val loss: 1.1877962350845337
Epoch 2130, training loss: 310.3454284667969 = 0.03686536103487015 + 50.0 * 6.20617151260376
Epoch 2130, val loss: 1.1911073923110962
Epoch 2140, training loss: 310.4625549316406 = 0.03628164529800415 + 50.0 * 6.208525657653809
Epoch 2140, val loss: 1.1939923763275146
Epoch 2150, training loss: 310.3255615234375 = 0.03569963201880455 + 50.0 * 6.20579719543457
Epoch 2150, val loss: 1.196707844734192
Epoch 2160, training loss: 310.3057861328125 = 0.0351472906768322 + 50.0 * 6.205412864685059
Epoch 2160, val loss: 1.2000558376312256
Epoch 2170, training loss: 310.39569091796875 = 0.03460496664047241 + 50.0 * 6.207221508026123
Epoch 2170, val loss: 1.202939748764038
Epoch 2180, training loss: 310.2691650390625 = 0.03406061977148056 + 50.0 * 6.204702377319336
Epoch 2180, val loss: 1.206050157546997
Epoch 2190, training loss: 310.3994445800781 = 0.033543661236763 + 50.0 * 6.207318305969238
Epoch 2190, val loss: 1.2093101739883423
Epoch 2200, training loss: 310.25921630859375 = 0.033017467707395554 + 50.0 * 6.204524040222168
Epoch 2200, val loss: 1.2116670608520508
Epoch 2210, training loss: 310.2420959472656 = 0.0325150303542614 + 50.0 * 6.204192161560059
Epoch 2210, val loss: 1.2147700786590576
Epoch 2220, training loss: 310.3655700683594 = 0.03202861547470093 + 50.0 * 6.20667028427124
Epoch 2220, val loss: 1.217766284942627
Epoch 2230, training loss: 310.26611328125 = 0.03154173120856285 + 50.0 * 6.204691410064697
Epoch 2230, val loss: 1.2208579778671265
Epoch 2240, training loss: 310.4245910644531 = 0.031063344329595566 + 50.0 * 6.2078704833984375
Epoch 2240, val loss: 1.2239458560943604
Epoch 2250, training loss: 310.21893310546875 = 0.030582739040255547 + 50.0 * 6.203766822814941
Epoch 2250, val loss: 1.226370096206665
Epoch 2260, training loss: 310.1706848144531 = 0.030132057145237923 + 50.0 * 6.202811241149902
Epoch 2260, val loss: 1.229676365852356
Epoch 2270, training loss: 310.2367858886719 = 0.029701314866542816 + 50.0 * 6.204141616821289
Epoch 2270, val loss: 1.2325212955474854
Epoch 2280, training loss: 310.3110046386719 = 0.029261352494359016 + 50.0 * 6.205634593963623
Epoch 2280, val loss: 1.2351980209350586
Epoch 2290, training loss: 310.2369079589844 = 0.028807932510972023 + 50.0 * 6.204162120819092
Epoch 2290, val loss: 1.2382136583328247
Epoch 2300, training loss: 310.31622314453125 = 0.028397519141435623 + 50.0 * 6.205756187438965
Epoch 2300, val loss: 1.2410448789596558
Epoch 2310, training loss: 310.11431884765625 = 0.027969377115368843 + 50.0 * 6.20172643661499
Epoch 2310, val loss: 1.2439230680465698
Epoch 2320, training loss: 310.1270751953125 = 0.02757636271417141 + 50.0 * 6.201989650726318
Epoch 2320, val loss: 1.2467848062515259
Epoch 2330, training loss: 310.1280212402344 = 0.027185913175344467 + 50.0 * 6.202016353607178
Epoch 2330, val loss: 1.249733328819275
Epoch 2340, training loss: 310.62396240234375 = 0.026813099160790443 + 50.0 * 6.211942672729492
Epoch 2340, val loss: 1.2524988651275635
Epoch 2350, training loss: 310.2640075683594 = 0.026394708082079887 + 50.0 * 6.204751968383789
Epoch 2350, val loss: 1.2550842761993408
Epoch 2360, training loss: 310.1004638671875 = 0.026009663939476013 + 50.0 * 6.201488971710205
Epoch 2360, val loss: 1.2579642534255981
Epoch 2370, training loss: 310.067626953125 = 0.0256508719176054 + 50.0 * 6.200839042663574
Epoch 2370, val loss: 1.2609251737594604
Epoch 2380, training loss: 310.1895446777344 = 0.025303073227405548 + 50.0 * 6.203285217285156
Epoch 2380, val loss: 1.263779640197754
Epoch 2390, training loss: 310.0722961425781 = 0.024946724995970726 + 50.0 * 6.200947284698486
Epoch 2390, val loss: 1.2662711143493652
Epoch 2400, training loss: 310.10247802734375 = 0.024596380069851875 + 50.0 * 6.201557636260986
Epoch 2400, val loss: 1.269010305404663
Epoch 2410, training loss: 310.2237854003906 = 0.024264050647616386 + 50.0 * 6.203990459442139
Epoch 2410, val loss: 1.2717119455337524
Epoch 2420, training loss: 310.13092041015625 = 0.023923804983496666 + 50.0 * 6.202139854431152
Epoch 2420, val loss: 1.2748022079467773
Epoch 2430, training loss: 310.1893310546875 = 0.023586727678775787 + 50.0 * 6.203314781188965
Epoch 2430, val loss: 1.2771472930908203
Epoch 2440, training loss: 310.04278564453125 = 0.023259181529283524 + 50.0 * 6.200390815734863
Epoch 2440, val loss: 1.2799423933029175
Epoch 2450, training loss: 310.00811767578125 = 0.022946229204535484 + 50.0 * 6.199703216552734
Epoch 2450, val loss: 1.282629132270813
Epoch 2460, training loss: 310.0042724609375 = 0.02264656312763691 + 50.0 * 6.19963264465332
Epoch 2460, val loss: 1.2853765487670898
Epoch 2470, training loss: 310.1676025390625 = 0.022354621440172195 + 50.0 * 6.20290470123291
Epoch 2470, val loss: 1.28801429271698
Epoch 2480, training loss: 310.0819396972656 = 0.022052450105547905 + 50.0 * 6.201197624206543
Epoch 2480, val loss: 1.2904853820800781
Epoch 2490, training loss: 310.021484375 = 0.021747764199972153 + 50.0 * 6.1999945640563965
Epoch 2490, val loss: 1.2932353019714355
Epoch 2500, training loss: 309.9855041503906 = 0.021455757319927216 + 50.0 * 6.199281215667725
Epoch 2500, val loss: 1.2960830926895142
Epoch 2510, training loss: 310.0978698730469 = 0.021185921505093575 + 50.0 * 6.201533794403076
Epoch 2510, val loss: 1.2987316846847534
Epoch 2520, training loss: 310.0099792480469 = 0.020901087671518326 + 50.0 * 6.19978141784668
Epoch 2520, val loss: 1.3013348579406738
Epoch 2530, training loss: 309.9905700683594 = 0.020624399185180664 + 50.0 * 6.199398994445801
Epoch 2530, val loss: 1.3036202192306519
Epoch 2540, training loss: 310.0435791015625 = 0.02036416344344616 + 50.0 * 6.200464248657227
Epoch 2540, val loss: 1.3065969944000244
Epoch 2550, training loss: 310.0252380371094 = 0.020100297406315804 + 50.0 * 6.200102806091309
Epoch 2550, val loss: 1.3091238737106323
Epoch 2560, training loss: 309.9488220214844 = 0.019852615892887115 + 50.0 * 6.198578834533691
Epoch 2560, val loss: 1.311611533164978
Epoch 2570, training loss: 310.0696716308594 = 0.01960752159357071 + 50.0 * 6.2010016441345215
Epoch 2570, val loss: 1.314398169517517
Epoch 2580, training loss: 310.00177001953125 = 0.019349239766597748 + 50.0 * 6.199648380279541
Epoch 2580, val loss: 1.316794753074646
Epoch 2590, training loss: 310.0699157714844 = 0.019105590879917145 + 50.0 * 6.201015949249268
Epoch 2590, val loss: 1.3190386295318604
Epoch 2600, training loss: 309.9297790527344 = 0.018863432109355927 + 50.0 * 6.19821834564209
Epoch 2600, val loss: 1.3216056823730469
Epoch 2610, training loss: 309.9862976074219 = 0.01863180100917816 + 50.0 * 6.199352741241455
Epoch 2610, val loss: 1.323910117149353
Epoch 2620, training loss: 310.00067138671875 = 0.018405159935355186 + 50.0 * 6.199645519256592
Epoch 2620, val loss: 1.326550841331482
Epoch 2630, training loss: 310.09234619140625 = 0.018177904188632965 + 50.0 * 6.201483249664307
Epoch 2630, val loss: 1.3290988206863403
Epoch 2640, training loss: 309.90460205078125 = 0.017951469868421555 + 50.0 * 6.197732925415039
Epoch 2640, val loss: 1.3314448595046997
Epoch 2650, training loss: 309.8805847167969 = 0.01773104816675186 + 50.0 * 6.197257041931152
Epoch 2650, val loss: 1.3337602615356445
Epoch 2660, training loss: 309.91558837890625 = 0.017522621899843216 + 50.0 * 6.197961330413818
Epoch 2660, val loss: 1.3365825414657593
Epoch 2670, training loss: 310.1132507324219 = 0.01731681451201439 + 50.0 * 6.201919078826904
Epoch 2670, val loss: 1.3390027284622192
Epoch 2680, training loss: 309.9186096191406 = 0.017105940729379654 + 50.0 * 6.198029518127441
Epoch 2680, val loss: 1.340812087059021
Epoch 2690, training loss: 309.8757019042969 = 0.016896585002541542 + 50.0 * 6.197175979614258
Epoch 2690, val loss: 1.3436684608459473
Epoch 2700, training loss: 309.846435546875 = 0.016704358160495758 + 50.0 * 6.196594715118408
Epoch 2700, val loss: 1.3458157777786255
Epoch 2710, training loss: 310.1195068359375 = 0.016518086194992065 + 50.0 * 6.202060222625732
Epoch 2710, val loss: 1.3483939170837402
Epoch 2720, training loss: 309.88067626953125 = 0.01631566882133484 + 50.0 * 6.197287082672119
Epoch 2720, val loss: 1.350311279296875
Epoch 2730, training loss: 309.8518981933594 = 0.016123924404382706 + 50.0 * 6.196715831756592
Epoch 2730, val loss: 1.3526456356048584
Epoch 2740, training loss: 309.9617004394531 = 0.015937061980366707 + 50.0 * 6.198915481567383
Epoch 2740, val loss: 1.3549388647079468
Epoch 2750, training loss: 309.9514465332031 = 0.015751909464597702 + 50.0 * 6.198714256286621
Epoch 2750, val loss: 1.3569941520690918
Epoch 2760, training loss: 309.887939453125 = 0.01556449756026268 + 50.0 * 6.197447299957275
Epoch 2760, val loss: 1.3593872785568237
Epoch 2770, training loss: 309.8219299316406 = 0.015388804487884045 + 50.0 * 6.196130752563477
Epoch 2770, val loss: 1.36185884475708
Epoch 2780, training loss: 309.81048583984375 = 0.015217617154121399 + 50.0 * 6.1959052085876465
Epoch 2780, val loss: 1.3640981912612915
Epoch 2790, training loss: 309.875 = 0.01505272462964058 + 50.0 * 6.19719934463501
Epoch 2790, val loss: 1.3664252758026123
Epoch 2800, training loss: 309.895263671875 = 0.014881315641105175 + 50.0 * 6.197607517242432
Epoch 2800, val loss: 1.3685764074325562
Epoch 2810, training loss: 309.9656677246094 = 0.014710675925016403 + 50.0 * 6.199018955230713
Epoch 2810, val loss: 1.370689034461975
Epoch 2820, training loss: 309.8658752441406 = 0.014549077488481998 + 50.0 * 6.197026252746582
Epoch 2820, val loss: 1.3728256225585938
Epoch 2830, training loss: 309.8295593261719 = 0.014390108175575733 + 50.0 * 6.196303367614746
Epoch 2830, val loss: 1.3749717473983765
Epoch 2840, training loss: 309.7613830566406 = 0.014232908375561237 + 50.0 * 6.194942951202393
Epoch 2840, val loss: 1.3772653341293335
Epoch 2850, training loss: 309.806640625 = 0.01408262550830841 + 50.0 * 6.1958513259887695
Epoch 2850, val loss: 1.379469633102417
Epoch 2860, training loss: 309.978271484375 = 0.013933753594756126 + 50.0 * 6.199286460876465
Epoch 2860, val loss: 1.3814257383346558
Epoch 2870, training loss: 309.86590576171875 = 0.013778799213469028 + 50.0 * 6.197041988372803
Epoch 2870, val loss: 1.383605718612671
Epoch 2880, training loss: 309.8477478027344 = 0.013627937994897366 + 50.0 * 6.196681976318359
Epoch 2880, val loss: 1.3857017755508423
Epoch 2890, training loss: 309.7607116699219 = 0.013481775298714638 + 50.0 * 6.194944381713867
Epoch 2890, val loss: 1.3879801034927368
Epoch 2900, training loss: 309.72674560546875 = 0.013340211473405361 + 50.0 * 6.194268226623535
Epoch 2900, val loss: 1.3901584148406982
Epoch 2910, training loss: 309.84295654296875 = 0.013205356895923615 + 50.0 * 6.196594715118408
Epoch 2910, val loss: 1.3922070264816284
Epoch 2920, training loss: 309.8368225097656 = 0.013066194020211697 + 50.0 * 6.196475505828857
Epoch 2920, val loss: 1.3940656185150146
Epoch 2930, training loss: 309.8327331542969 = 0.012925959192216396 + 50.0 * 6.1963958740234375
Epoch 2930, val loss: 1.3964886665344238
Epoch 2940, training loss: 309.916748046875 = 0.012791747227311134 + 50.0 * 6.1980791091918945
Epoch 2940, val loss: 1.3982690572738647
Epoch 2950, training loss: 309.73406982421875 = 0.012658808380365372 + 50.0 * 6.194427967071533
Epoch 2950, val loss: 1.4004405736923218
Epoch 2960, training loss: 309.7086486816406 = 0.012531844899058342 + 50.0 * 6.19392204284668
Epoch 2960, val loss: 1.402490496635437
Epoch 2970, training loss: 309.7310485839844 = 0.012406456284224987 + 50.0 * 6.194372653961182
Epoch 2970, val loss: 1.404450535774231
Epoch 2980, training loss: 309.9093017578125 = 0.012283365242183208 + 50.0 * 6.197940349578857
Epoch 2980, val loss: 1.4064112901687622
Epoch 2990, training loss: 309.7276916503906 = 0.012158341705799103 + 50.0 * 6.194311141967773
Epoch 2990, val loss: 1.4086389541625977
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 431.8135681152344 = 1.972667932510376 + 50.0 * 8.596817970275879
Epoch 0, val loss: 1.9739723205566406
Epoch 10, training loss: 431.7564392089844 = 1.9621834754943848 + 50.0 * 8.595885276794434
Epoch 10, val loss: 1.9629690647125244
Epoch 20, training loss: 431.41497802734375 = 1.9491451978683472 + 50.0 * 8.589316368103027
Epoch 20, val loss: 1.9493128061294556
Epoch 30, training loss: 429.2547607421875 = 1.9322234392166138 + 50.0 * 8.5464506149292
Epoch 30, val loss: 1.9317286014556885
Epoch 40, training loss: 417.4644470214844 = 1.9118596315383911 + 50.0 * 8.311051368713379
Epoch 40, val loss: 1.9111766815185547
Epoch 50, training loss: 387.20806884765625 = 1.8893942832946777 + 50.0 * 7.70637321472168
Epoch 50, val loss: 1.8893442153930664
Epoch 60, training loss: 368.2792663574219 = 1.8725550174713135 + 50.0 * 7.328134059906006
Epoch 60, val loss: 1.874525547027588
Epoch 70, training loss: 356.3684387207031 = 1.859955072402954 + 50.0 * 7.090169429779053
Epoch 70, val loss: 1.8626834154129028
Epoch 80, training loss: 349.1646728515625 = 1.8465583324432373 + 50.0 * 6.946362495422363
Epoch 80, val loss: 1.8501536846160889
Epoch 90, training loss: 343.98687744140625 = 1.8353420495986938 + 50.0 * 6.84303092956543
Epoch 90, val loss: 1.8395555019378662
Epoch 100, training loss: 339.6115417480469 = 1.826030969619751 + 50.0 * 6.755710601806641
Epoch 100, val loss: 1.8304946422576904
Epoch 110, training loss: 336.2732238769531 = 1.8185046911239624 + 50.0 * 6.689094543457031
Epoch 110, val loss: 1.822728157043457
Epoch 120, training loss: 333.3480529785156 = 1.8119043111801147 + 50.0 * 6.630722999572754
Epoch 120, val loss: 1.815703272819519
Epoch 130, training loss: 331.4859313964844 = 1.8053072690963745 + 50.0 * 6.5936126708984375
Epoch 130, val loss: 1.8087767362594604
Epoch 140, training loss: 330.11566162109375 = 1.7982032299041748 + 50.0 * 6.566349029541016
Epoch 140, val loss: 1.8016712665557861
Epoch 150, training loss: 328.82904052734375 = 1.7911710739135742 + 50.0 * 6.540757179260254
Epoch 150, val loss: 1.7948033809661865
Epoch 160, training loss: 327.61767578125 = 1.7842689752578735 + 50.0 * 6.51666784286499
Epoch 160, val loss: 1.7879313230514526
Epoch 170, training loss: 326.6251525878906 = 1.7772010564804077 + 50.0 * 6.496959209442139
Epoch 170, val loss: 1.7809139490127563
Epoch 180, training loss: 325.55572509765625 = 1.7696943283081055 + 50.0 * 6.4757208824157715
Epoch 180, val loss: 1.7734622955322266
Epoch 190, training loss: 324.7803955078125 = 1.761723518371582 + 50.0 * 6.460373401641846
Epoch 190, val loss: 1.7655959129333496
Epoch 200, training loss: 324.1111145019531 = 1.7531713247299194 + 50.0 * 6.4471588134765625
Epoch 200, val loss: 1.7572588920593262
Epoch 210, training loss: 323.56915283203125 = 1.7439135313034058 + 50.0 * 6.436504364013672
Epoch 210, val loss: 1.7483309507369995
Epoch 220, training loss: 323.0309753417969 = 1.7339400053024292 + 50.0 * 6.42594051361084
Epoch 220, val loss: 1.7388144731521606
Epoch 230, training loss: 322.51904296875 = 1.7232414484024048 + 50.0 * 6.4159159660339355
Epoch 230, val loss: 1.7286615371704102
Epoch 240, training loss: 322.0808410644531 = 1.7116179466247559 + 50.0 * 6.407384872436523
Epoch 240, val loss: 1.717788815498352
Epoch 250, training loss: 321.537841796875 = 1.6992954015731812 + 50.0 * 6.396770477294922
Epoch 250, val loss: 1.7062138319015503
Epoch 260, training loss: 321.185791015625 = 1.6860980987548828 + 50.0 * 6.389993667602539
Epoch 260, val loss: 1.6939362287521362
Epoch 270, training loss: 321.00152587890625 = 1.6719120740890503 + 50.0 * 6.386592388153076
Epoch 270, val loss: 1.6808682680130005
Epoch 280, training loss: 320.4735107421875 = 1.6568573713302612 + 50.0 * 6.376333236694336
Epoch 280, val loss: 1.666978120803833
Epoch 290, training loss: 320.11358642578125 = 1.6410675048828125 + 50.0 * 6.369450569152832
Epoch 290, val loss: 1.6524755954742432
Epoch 300, training loss: 319.84014892578125 = 1.6243655681610107 + 50.0 * 6.364315509796143
Epoch 300, val loss: 1.637258529663086
Epoch 310, training loss: 319.5428771972656 = 1.6070283651351929 + 50.0 * 6.35871696472168
Epoch 310, val loss: 1.6214466094970703
Epoch 320, training loss: 319.22344970703125 = 1.5890698432922363 + 50.0 * 6.352687358856201
Epoch 320, val loss: 1.6052091121673584
Epoch 330, training loss: 319.25506591796875 = 1.5706294775009155 + 50.0 * 6.353688716888428
Epoch 330, val loss: 1.5886033773422241
Epoch 340, training loss: 318.7948913574219 = 1.5514426231384277 + 50.0 * 6.3448686599731445
Epoch 340, val loss: 1.5715569257736206
Epoch 350, training loss: 318.5337219238281 = 1.532113790512085 + 50.0 * 6.34003210067749
Epoch 350, val loss: 1.5543566942214966
Epoch 360, training loss: 318.28765869140625 = 1.512519121170044 + 50.0 * 6.335503101348877
Epoch 360, val loss: 1.5371699333190918
Epoch 370, training loss: 318.14520263671875 = 1.492784023284912 + 50.0 * 6.333047866821289
Epoch 370, val loss: 1.520035743713379
Epoch 380, training loss: 318.11505126953125 = 1.472741723060608 + 50.0 * 6.332846164703369
Epoch 380, val loss: 1.5026776790618896
Epoch 390, training loss: 317.7521057128906 = 1.4528355598449707 + 50.0 * 6.325984954833984
Epoch 390, val loss: 1.4855536222457886
Epoch 400, training loss: 317.5357360839844 = 1.4329410791397095 + 50.0 * 6.322055816650391
Epoch 400, val loss: 1.4685933589935303
Epoch 410, training loss: 317.4076843261719 = 1.4132047891616821 + 50.0 * 6.319889545440674
Epoch 410, val loss: 1.4518481492996216
Epoch 420, training loss: 317.37408447265625 = 1.3932148218154907 + 50.0 * 6.31961727142334
Epoch 420, val loss: 1.4353634119033813
Epoch 430, training loss: 317.0690002441406 = 1.3736408948898315 + 50.0 * 6.313907623291016
Epoch 430, val loss: 1.4189574718475342
Epoch 440, training loss: 316.880126953125 = 1.3541499376296997 + 50.0 * 6.310519218444824
Epoch 440, val loss: 1.4028488397598267
Epoch 450, training loss: 316.80902099609375 = 1.3348335027694702 + 50.0 * 6.309483528137207
Epoch 450, val loss: 1.386981725692749
Epoch 460, training loss: 316.68048095703125 = 1.3154313564300537 + 50.0 * 6.307301044464111
Epoch 460, val loss: 1.3712416887283325
Epoch 470, training loss: 316.48663330078125 = 1.296237826347351 + 50.0 * 6.303808212280273
Epoch 470, val loss: 1.3556755781173706
Epoch 480, training loss: 316.3736572265625 = 1.2769670486450195 + 50.0 * 6.301933765411377
Epoch 480, val loss: 1.3402353525161743
Epoch 490, training loss: 316.285400390625 = 1.2579246759414673 + 50.0 * 6.300549030303955
Epoch 490, val loss: 1.3249715566635132
Epoch 500, training loss: 316.0795593261719 = 1.23896062374115 + 50.0 * 6.296812057495117
Epoch 500, val loss: 1.309874415397644
Epoch 510, training loss: 316.03533935546875 = 1.220125675201416 + 50.0 * 6.296304225921631
Epoch 510, val loss: 1.294897437095642
Epoch 520, training loss: 315.9677734375 = 1.2012450695037842 + 50.0 * 6.29533052444458
Epoch 520, val loss: 1.2800719738006592
Epoch 530, training loss: 315.7350158691406 = 1.1823827028274536 + 50.0 * 6.29105281829834
Epoch 530, val loss: 1.265170693397522
Epoch 540, training loss: 315.6746826171875 = 1.1636310815811157 + 50.0 * 6.290220737457275
Epoch 540, val loss: 1.250531554222107
Epoch 550, training loss: 315.5392761230469 = 1.1448613405227661 + 50.0 * 6.287888050079346
Epoch 550, val loss: 1.2359426021575928
Epoch 560, training loss: 315.556884765625 = 1.1260945796966553 + 50.0 * 6.288615703582764
Epoch 560, val loss: 1.2214329242706299
Epoch 570, training loss: 315.37255859375 = 1.1075985431671143 + 50.0 * 6.285298824310303
Epoch 570, val loss: 1.2069603204727173
Epoch 580, training loss: 315.185546875 = 1.0889891386032104 + 50.0 * 6.281930923461914
Epoch 580, val loss: 1.1927546262741089
Epoch 590, training loss: 315.1716613769531 = 1.0705209970474243 + 50.0 * 6.282022953033447
Epoch 590, val loss: 1.1787190437316895
Epoch 600, training loss: 315.1654052734375 = 1.0522373914718628 + 50.0 * 6.282263278961182
Epoch 600, val loss: 1.1646913290023804
Epoch 610, training loss: 315.09527587890625 = 1.033708095550537 + 50.0 * 6.281230926513672
Epoch 610, val loss: 1.1508588790893555
Epoch 620, training loss: 314.842529296875 = 1.0156548023223877 + 50.0 * 6.2765374183654785
Epoch 620, val loss: 1.1373249292373657
Epoch 630, training loss: 314.75537109375 = 0.9978969097137451 + 50.0 * 6.275149822235107
Epoch 630, val loss: 1.1241047382354736
Epoch 640, training loss: 314.9634094238281 = 0.98020339012146 + 50.0 * 6.279664516448975
Epoch 640, val loss: 1.111362099647522
Epoch 650, training loss: 314.8211975097656 = 0.9625917673110962 + 50.0 * 6.277172088623047
Epoch 650, val loss: 1.098345160484314
Epoch 660, training loss: 314.53155517578125 = 0.9452816843986511 + 50.0 * 6.271725654602051
Epoch 660, val loss: 1.0860300064086914
Epoch 670, training loss: 314.41778564453125 = 0.9283255934715271 + 50.0 * 6.269789218902588
Epoch 670, val loss: 1.074346899986267
Epoch 680, training loss: 314.3484191894531 = 0.9116883873939514 + 50.0 * 6.268734931945801
Epoch 680, val loss: 1.0629762411117554
Epoch 690, training loss: 314.5115966796875 = 0.8952053189277649 + 50.0 * 6.272327899932861
Epoch 690, val loss: 1.0519376993179321
Epoch 700, training loss: 314.3584289550781 = 0.8789544701576233 + 50.0 * 6.269588947296143
Epoch 700, val loss: 1.0412530899047852
Epoch 710, training loss: 314.1557922363281 = 0.8628566861152649 + 50.0 * 6.2658586502075195
Epoch 710, val loss: 1.030993103981018
Epoch 720, training loss: 314.08251953125 = 0.8472483158111572 + 50.0 * 6.264705181121826
Epoch 720, val loss: 1.0210661888122559
Epoch 730, training loss: 313.9666748046875 = 0.8319560289382935 + 50.0 * 6.262693881988525
Epoch 730, val loss: 1.011755347251892
Epoch 740, training loss: 314.0979309082031 = 0.8168485164642334 + 50.0 * 6.265622138977051
Epoch 740, val loss: 1.0028494596481323
Epoch 750, training loss: 314.0738220214844 = 0.8019057512283325 + 50.0 * 6.265438079833984
Epoch 750, val loss: 0.9939745664596558
Epoch 760, training loss: 313.7841796875 = 0.7871524095535278 + 50.0 * 6.2599406242370605
Epoch 760, val loss: 0.9855384826660156
Epoch 770, training loss: 313.7412414550781 = 0.7727241516113281 + 50.0 * 6.25937032699585
Epoch 770, val loss: 0.9776648879051208
Epoch 780, training loss: 313.8544616699219 = 0.7585697174072266 + 50.0 * 6.261917591094971
Epoch 780, val loss: 0.9700475335121155
Epoch 790, training loss: 313.7968444824219 = 0.7444555163383484 + 50.0 * 6.261047840118408
Epoch 790, val loss: 0.9623498916625977
Epoch 800, training loss: 313.55419921875 = 0.7304431796073914 + 50.0 * 6.25647497177124
Epoch 800, val loss: 0.9551593065261841
Epoch 810, training loss: 313.47772216796875 = 0.7167168259620667 + 50.0 * 6.255220413208008
Epoch 810, val loss: 0.948319137096405
Epoch 820, training loss: 313.4294738769531 = 0.7031599879264832 + 50.0 * 6.254526615142822
Epoch 820, val loss: 0.9415900111198425
Epoch 830, training loss: 313.6661071777344 = 0.6896353363990784 + 50.0 * 6.2595295906066895
Epoch 830, val loss: 0.9349808096885681
Epoch 840, training loss: 313.3395080566406 = 0.6759989261627197 + 50.0 * 6.253270149230957
Epoch 840, val loss: 0.928210437297821
Epoch 850, training loss: 313.2860412597656 = 0.6625478267669678 + 50.0 * 6.252470016479492
Epoch 850, val loss: 0.9218105673789978
Epoch 860, training loss: 313.2068786621094 = 0.6493725180625916 + 50.0 * 6.251149654388428
Epoch 860, val loss: 0.915761411190033
Epoch 870, training loss: 313.4259033203125 = 0.6363235712051392 + 50.0 * 6.255791664123535
Epoch 870, val loss: 0.9097645282745361
Epoch 880, training loss: 313.3118896484375 = 0.6229241490364075 + 50.0 * 6.253779411315918
Epoch 880, val loss: 0.9036781191825867
Epoch 890, training loss: 313.16693115234375 = 0.6099482178688049 + 50.0 * 6.2511396408081055
Epoch 890, val loss: 0.8978841304779053
Epoch 900, training loss: 312.9896545410156 = 0.5969913005828857 + 50.0 * 6.2478532791137695
Epoch 900, val loss: 0.8921851515769958
Epoch 910, training loss: 312.9588928222656 = 0.584250807762146 + 50.0 * 6.247492790222168
Epoch 910, val loss: 0.8867775201797485
Epoch 920, training loss: 313.0013732910156 = 0.5716596245765686 + 50.0 * 6.248594284057617
Epoch 920, val loss: 0.8815642595291138
Epoch 930, training loss: 312.8621826171875 = 0.5589538216590881 + 50.0 * 6.24606466293335
Epoch 930, val loss: 0.8761647343635559
Epoch 940, training loss: 312.82952880859375 = 0.5465626120567322 + 50.0 * 6.245659351348877
Epoch 940, val loss: 0.8712030649185181
Epoch 950, training loss: 313.0463562011719 = 0.5342937111854553 + 50.0 * 6.250240802764893
Epoch 950, val loss: 0.8664461374282837
Epoch 960, training loss: 312.7776184082031 = 0.5221338868141174 + 50.0 * 6.245110034942627
Epoch 960, val loss: 0.8615931272506714
Epoch 970, training loss: 312.6758117675781 = 0.5102599263191223 + 50.0 * 6.243310928344727
Epoch 970, val loss: 0.8573485612869263
Epoch 980, training loss: 312.6236572265625 = 0.4986497759819031 + 50.0 * 6.242500305175781
Epoch 980, val loss: 0.8532880544662476
Epoch 990, training loss: 312.9524841308594 = 0.4872443377971649 + 50.0 * 6.24930477142334
Epoch 990, val loss: 0.8492615818977356
Epoch 1000, training loss: 312.7538757324219 = 0.4757761061191559 + 50.0 * 6.2455620765686035
Epoch 1000, val loss: 0.845554530620575
Epoch 1010, training loss: 312.61016845703125 = 0.46464020013809204 + 50.0 * 6.242910861968994
Epoch 1010, val loss: 0.8420367240905762
Epoch 1020, training loss: 312.5066223144531 = 0.45386022329330444 + 50.0 * 6.241055011749268
Epoch 1020, val loss: 0.8389032483100891
Epoch 1030, training loss: 312.49468994140625 = 0.443356454372406 + 50.0 * 6.241026401519775
Epoch 1030, val loss: 0.8360060453414917
Epoch 1040, training loss: 312.48095703125 = 0.43298885226249695 + 50.0 * 6.240959644317627
Epoch 1040, val loss: 0.8332553505897522
Epoch 1050, training loss: 312.4405517578125 = 0.42282113432884216 + 50.0 * 6.240354537963867
Epoch 1050, val loss: 0.8306243419647217
Epoch 1060, training loss: 312.3475646972656 = 0.4129757285118103 + 50.0 * 6.238692283630371
Epoch 1060, val loss: 0.8286428451538086
Epoch 1070, training loss: 312.5884704589844 = 0.40329158306121826 + 50.0 * 6.243703365325928
Epoch 1070, val loss: 0.8265830874443054
Epoch 1080, training loss: 312.32293701171875 = 0.3938455879688263 + 50.0 * 6.238581657409668
Epoch 1080, val loss: 0.8247933983802795
Epoch 1090, training loss: 312.23681640625 = 0.38463929295539856 + 50.0 * 6.237043380737305
Epoch 1090, val loss: 0.8232662081718445
Epoch 1100, training loss: 312.1655578613281 = 0.3758532702922821 + 50.0 * 6.2357940673828125
Epoch 1100, val loss: 0.8222429156303406
Epoch 1110, training loss: 312.2085266113281 = 0.3672722280025482 + 50.0 * 6.236824989318848
Epoch 1110, val loss: 0.8213135600090027
Epoch 1120, training loss: 312.16259765625 = 0.3586766719818115 + 50.0 * 6.236078262329102
Epoch 1120, val loss: 0.8201644420623779
Epoch 1130, training loss: 312.2159729003906 = 0.35035738348960876 + 50.0 * 6.237311840057373
Epoch 1130, val loss: 0.8194335103034973
Epoch 1140, training loss: 312.0623474121094 = 0.34226059913635254 + 50.0 * 6.234401702880859
Epoch 1140, val loss: 0.8188521265983582
Epoch 1150, training loss: 312.0021667480469 = 0.3345361351966858 + 50.0 * 6.2333526611328125
Epoch 1150, val loss: 0.8186497688293457
Epoch 1160, training loss: 312.12506103515625 = 0.32701635360717773 + 50.0 * 6.235960960388184
Epoch 1160, val loss: 0.8186591267585754
Epoch 1170, training loss: 312.1123046875 = 0.31951284408569336 + 50.0 * 6.235855579376221
Epoch 1170, val loss: 0.8184999227523804
Epoch 1180, training loss: 312.0250244140625 = 0.3121876120567322 + 50.0 * 6.234256744384766
Epoch 1180, val loss: 0.8185420036315918
Epoch 1190, training loss: 311.95550537109375 = 0.3051537573337555 + 50.0 * 6.233007431030273
Epoch 1190, val loss: 0.8189194798469543
Epoch 1200, training loss: 311.9086608886719 = 0.29832273721694946 + 50.0 * 6.23220682144165
Epoch 1200, val loss: 0.8194289207458496
Epoch 1210, training loss: 311.8944396972656 = 0.29169386625289917 + 50.0 * 6.232055187225342
Epoch 1210, val loss: 0.8201647400856018
Epoch 1220, training loss: 311.9629211425781 = 0.2852081060409546 + 50.0 * 6.233553886413574
Epoch 1220, val loss: 0.820966362953186
Epoch 1230, training loss: 311.8247985839844 = 0.27882620692253113 + 50.0 * 6.230918884277344
Epoch 1230, val loss: 0.8221322298049927
Epoch 1240, training loss: 311.92291259765625 = 0.2726198732852936 + 50.0 * 6.233006000518799
Epoch 1240, val loss: 0.8231402039527893
Epoch 1250, training loss: 311.7586669921875 = 0.2665274441242218 + 50.0 * 6.229842662811279
Epoch 1250, val loss: 0.824124276638031
Epoch 1260, training loss: 311.70391845703125 = 0.2606792151927948 + 50.0 * 6.228864669799805
Epoch 1260, val loss: 0.8255187273025513
Epoch 1270, training loss: 311.6782531738281 = 0.2549985647201538 + 50.0 * 6.2284650802612305
Epoch 1270, val loss: 0.8270514011383057
Epoch 1280, training loss: 311.9778137207031 = 0.2494284063577652 + 50.0 * 6.234568119049072
Epoch 1280, val loss: 0.8285135626792908
Epoch 1290, training loss: 311.8057861328125 = 0.2439415603876114 + 50.0 * 6.231237411499023
Epoch 1290, val loss: 0.8303219676017761
Epoch 1300, training loss: 311.6465148925781 = 0.2385496199131012 + 50.0 * 6.228159427642822
Epoch 1300, val loss: 0.8320091962814331
Epoch 1310, training loss: 311.6214294433594 = 0.2334280163049698 + 50.0 * 6.227760314941406
Epoch 1310, val loss: 0.833949625492096
Epoch 1320, training loss: 311.9241027832031 = 0.22833910584449768 + 50.0 * 6.233915328979492
Epoch 1320, val loss: 0.8359317183494568
Epoch 1330, training loss: 311.6448059082031 = 0.22332601249217987 + 50.0 * 6.228429794311523
Epoch 1330, val loss: 0.8378720879554749
Epoch 1340, training loss: 311.5460205078125 = 0.21848437190055847 + 50.0 * 6.226551055908203
Epoch 1340, val loss: 0.8398445248603821
Epoch 1350, training loss: 311.5019836425781 = 0.21388289332389832 + 50.0 * 6.225761890411377
Epoch 1350, val loss: 0.8422774076461792
Epoch 1360, training loss: 311.4643249511719 = 0.20939704775810242 + 50.0 * 6.225098133087158
Epoch 1360, val loss: 0.8446581959724426
Epoch 1370, training loss: 311.72247314453125 = 0.20501935482025146 + 50.0 * 6.230349063873291
Epoch 1370, val loss: 0.8470624089241028
Epoch 1380, training loss: 311.4722595214844 = 0.20056144893169403 + 50.0 * 6.225433826446533
Epoch 1380, val loss: 0.8492629528045654
Epoch 1390, training loss: 311.4060974121094 = 0.19631238281726837 + 50.0 * 6.22419548034668
Epoch 1390, val loss: 0.8520467281341553
Epoch 1400, training loss: 311.4064636230469 = 0.19222892820835114 + 50.0 * 6.224284648895264
Epoch 1400, val loss: 0.8546391725540161
Epoch 1410, training loss: 311.6742858886719 = 0.18821007013320923 + 50.0 * 6.229721546173096
Epoch 1410, val loss: 0.8574446439743042
Epoch 1420, training loss: 311.4534912109375 = 0.18424850702285767 + 50.0 * 6.2253851890563965
Epoch 1420, val loss: 0.8601077795028687
Epoch 1430, training loss: 311.4017333984375 = 0.18041782081127167 + 50.0 * 6.22442626953125
Epoch 1430, val loss: 0.8630135655403137
Epoch 1440, training loss: 311.46954345703125 = 0.17672039568424225 + 50.0 * 6.225856304168701
Epoch 1440, val loss: 0.8660009503364563
Epoch 1450, training loss: 311.2856140136719 = 0.17301303148269653 + 50.0 * 6.222252368927002
Epoch 1450, val loss: 0.8686774969100952
Epoch 1460, training loss: 311.3188171386719 = 0.16948887705802917 + 50.0 * 6.222986221313477
Epoch 1460, val loss: 0.8715459704399109
Epoch 1470, training loss: 311.4393005371094 = 0.16604210436344147 + 50.0 * 6.225465297698975
Epoch 1470, val loss: 0.8747727870941162
Epoch 1480, training loss: 311.2974548339844 = 0.16257643699645996 + 50.0 * 6.222697734832764
Epoch 1480, val loss: 0.8776065111160278
Epoch 1490, training loss: 311.4713134765625 = 0.15921644866466522 + 50.0 * 6.2262420654296875
Epoch 1490, val loss: 0.8805484175682068
Epoch 1500, training loss: 311.2358093261719 = 0.15596091747283936 + 50.0 * 6.221596717834473
Epoch 1500, val loss: 0.8835970759391785
Epoch 1510, training loss: 311.170166015625 = 0.15280745923519135 + 50.0 * 6.2203474044799805
Epoch 1510, val loss: 0.886702299118042
Epoch 1520, training loss: 311.1537170410156 = 0.1497766077518463 + 50.0 * 6.220078468322754
Epoch 1520, val loss: 0.8899546265602112
Epoch 1530, training loss: 311.3658752441406 = 0.14679250121116638 + 50.0 * 6.224381446838379
Epoch 1530, val loss: 0.8928976058959961
Epoch 1540, training loss: 311.26873779296875 = 0.14380893111228943 + 50.0 * 6.222498893737793
Epoch 1540, val loss: 0.8962692618370056
Epoch 1550, training loss: 311.185546875 = 0.14084671437740326 + 50.0 * 6.2208943367004395
Epoch 1550, val loss: 0.8992500305175781
Epoch 1560, training loss: 311.0887145996094 = 0.13808543980121613 + 50.0 * 6.219012260437012
Epoch 1560, val loss: 0.9025803804397583
Epoch 1570, training loss: 311.06903076171875 = 0.13537819683551788 + 50.0 * 6.218673229217529
Epoch 1570, val loss: 0.9058440327644348
Epoch 1580, training loss: 311.4790344238281 = 0.13274137675762177 + 50.0 * 6.226925849914551
Epoch 1580, val loss: 0.9091464281082153
Epoch 1590, training loss: 311.2275695800781 = 0.13006052374839783 + 50.0 * 6.221950054168701
Epoch 1590, val loss: 0.9124386310577393
Epoch 1600, training loss: 311.0794677734375 = 0.12746602296829224 + 50.0 * 6.2190399169921875
Epoch 1600, val loss: 0.9156304001808167
Epoch 1610, training loss: 311.0174560546875 = 0.12498541921377182 + 50.0 * 6.217849254608154
Epoch 1610, val loss: 0.919097363948822
Epoch 1620, training loss: 311.0611267089844 = 0.12256263941526413 + 50.0 * 6.218771457672119
Epoch 1620, val loss: 0.9224387407302856
Epoch 1630, training loss: 311.0672607421875 = 0.12019166350364685 + 50.0 * 6.218941688537598
Epoch 1630, val loss: 0.9258547425270081
Epoch 1640, training loss: 311.1532897949219 = 0.11785276234149933 + 50.0 * 6.220708847045898
Epoch 1640, val loss: 0.9292885065078735
Epoch 1650, training loss: 310.9953308105469 = 0.11556616425514221 + 50.0 * 6.217595100402832
Epoch 1650, val loss: 0.9328332543373108
Epoch 1660, training loss: 311.0400390625 = 0.11335764825344086 + 50.0 * 6.218533515930176
Epoch 1660, val loss: 0.9362186789512634
Epoch 1670, training loss: 310.9446716308594 = 0.11116523295640945 + 50.0 * 6.216670036315918
Epoch 1670, val loss: 0.9396910071372986
Epoch 1680, training loss: 310.8822937011719 = 0.1090228259563446 + 50.0 * 6.215465545654297
Epoch 1680, val loss: 0.9429839849472046
Epoch 1690, training loss: 310.86968994140625 = 0.10697004199028015 + 50.0 * 6.215254783630371
Epoch 1690, val loss: 0.9466360211372375
Epoch 1700, training loss: 311.252685546875 = 0.10495942085981369 + 50.0 * 6.222954750061035
Epoch 1700, val loss: 0.9502211809158325
Epoch 1710, training loss: 311.0230712890625 = 0.10292395949363708 + 50.0 * 6.218402862548828
Epoch 1710, val loss: 0.9536910653114319
Epoch 1720, training loss: 310.86126708984375 = 0.10093403607606888 + 50.0 * 6.215207099914551
Epoch 1720, val loss: 0.9568772912025452
Epoch 1730, training loss: 310.85296630859375 = 0.09906311333179474 + 50.0 * 6.215077877044678
Epoch 1730, val loss: 0.9605885744094849
Epoch 1740, training loss: 311.0762634277344 = 0.09723084419965744 + 50.0 * 6.21958065032959
Epoch 1740, val loss: 0.9642044901847839
Epoch 1750, training loss: 310.8577575683594 = 0.09536923468112946 + 50.0 * 6.215247631072998
Epoch 1750, val loss: 0.9675958752632141
Epoch 1760, training loss: 310.75225830078125 = 0.093594491481781 + 50.0 * 6.2131733894348145
Epoch 1760, val loss: 0.9711543321609497
Epoch 1770, training loss: 310.8610534667969 = 0.09189283102750778 + 50.0 * 6.215383052825928
Epoch 1770, val loss: 0.9748650193214417
Epoch 1780, training loss: 310.8997497558594 = 0.09015011042356491 + 50.0 * 6.216192245483398
Epoch 1780, val loss: 0.9780682325363159
Epoch 1790, training loss: 310.7323303222656 = 0.08843935281038284 + 50.0 * 6.2128777503967285
Epoch 1790, val loss: 0.981511652469635
Epoch 1800, training loss: 310.697265625 = 0.08681682497262955 + 50.0 * 6.2122087478637695
Epoch 1800, val loss: 0.9849095940589905
Epoch 1810, training loss: 310.6977844238281 = 0.08525697141885757 + 50.0 * 6.212250232696533
Epoch 1810, val loss: 0.988610029220581
Epoch 1820, training loss: 310.8999938964844 = 0.08371799439191818 + 50.0 * 6.216325759887695
Epoch 1820, val loss: 0.991920530796051
Epoch 1830, training loss: 310.783447265625 = 0.08218429982662201 + 50.0 * 6.214025497436523
Epoch 1830, val loss: 0.9957334399223328
Epoch 1840, training loss: 310.7077941894531 = 0.08066505938768387 + 50.0 * 6.21254301071167
Epoch 1840, val loss: 0.9990781545639038
Epoch 1850, training loss: 310.6842956542969 = 0.0792234018445015 + 50.0 * 6.212101459503174
Epoch 1850, val loss: 1.0029398202896118
Epoch 1860, training loss: 310.703369140625 = 0.07780937850475311 + 50.0 * 6.21251106262207
Epoch 1860, val loss: 1.0062626600265503
Epoch 1870, training loss: 310.6616516113281 = 0.07642891258001328 + 50.0 * 6.211704730987549
Epoch 1870, val loss: 1.0099070072174072
Epoch 1880, training loss: 310.6349792480469 = 0.07507640868425369 + 50.0 * 6.211198329925537
Epoch 1880, val loss: 1.0134763717651367
Epoch 1890, training loss: 310.9471130371094 = 0.07377085089683533 + 50.0 * 6.217467308044434
Epoch 1890, val loss: 1.0170645713806152
Epoch 1900, training loss: 310.7722473144531 = 0.07242091000080109 + 50.0 * 6.213996887207031
Epoch 1900, val loss: 1.0206375122070312
Epoch 1910, training loss: 310.6355285644531 = 0.07112867385149002 + 50.0 * 6.211287975311279
Epoch 1910, val loss: 1.0239741802215576
Epoch 1920, training loss: 310.597412109375 = 0.06990128010511398 + 50.0 * 6.210549831390381
Epoch 1920, val loss: 1.0276376008987427
Epoch 1930, training loss: 310.8369445800781 = 0.06871576607227325 + 50.0 * 6.215364456176758
Epoch 1930, val loss: 1.0310869216918945
Epoch 1940, training loss: 310.5406188964844 = 0.06746752560138702 + 50.0 * 6.209463119506836
Epoch 1940, val loss: 1.034472107887268
Epoch 1950, training loss: 310.5434875488281 = 0.06630302965641022 + 50.0 * 6.209543704986572
Epoch 1950, val loss: 1.0381159782409668
Epoch 1960, training loss: 310.7559509277344 = 0.06518060714006424 + 50.0 * 6.213815212249756
Epoch 1960, val loss: 1.041663408279419
Epoch 1970, training loss: 310.54345703125 = 0.06404087692499161 + 50.0 * 6.209588527679443
Epoch 1970, val loss: 1.0450533628463745
Epoch 1980, training loss: 310.7259521484375 = 0.0629616528749466 + 50.0 * 6.213259696960449
Epoch 1980, val loss: 1.0484898090362549
Epoch 1990, training loss: 310.57525634765625 = 0.06186004728078842 + 50.0 * 6.210268020629883
Epoch 1990, val loss: 1.0518243312835693
Epoch 2000, training loss: 310.4913330078125 = 0.06080624461174011 + 50.0 * 6.208610534667969
Epoch 2000, val loss: 1.055003046989441
Epoch 2010, training loss: 310.4852294921875 = 0.059794504195451736 + 50.0 * 6.2085089683532715
Epoch 2010, val loss: 1.0584747791290283
Epoch 2020, training loss: 310.5582275390625 = 0.0588083490729332 + 50.0 * 6.209988117218018
Epoch 2020, val loss: 1.0620336532592773
Epoch 2030, training loss: 310.5442199707031 = 0.05782703682780266 + 50.0 * 6.209727764129639
Epoch 2030, val loss: 1.0652904510498047
Epoch 2040, training loss: 310.5220031738281 = 0.05685204267501831 + 50.0 * 6.20930290222168
Epoch 2040, val loss: 1.0684571266174316
Epoch 2050, training loss: 310.4901428222656 = 0.05590996518731117 + 50.0 * 6.20868444442749
Epoch 2050, val loss: 1.0717042684555054
Epoch 2060, training loss: 310.4270324707031 = 0.05499272793531418 + 50.0 * 6.2074408531188965
Epoch 2060, val loss: 1.0750758647918701
Epoch 2070, training loss: 310.58892822265625 = 0.054097454994916916 + 50.0 * 6.210696697235107
Epoch 2070, val loss: 1.0782601833343506
Epoch 2080, training loss: 310.63421630859375 = 0.05318494513630867 + 50.0 * 6.211620330810547
Epoch 2080, val loss: 1.0814398527145386
Epoch 2090, training loss: 310.46746826171875 = 0.05231740325689316 + 50.0 * 6.208303451538086
Epoch 2090, val loss: 1.0849192142486572
Epoch 2100, training loss: 310.3838195800781 = 0.05146403610706329 + 50.0 * 6.2066473960876465
Epoch 2100, val loss: 1.0881685018539429
Epoch 2110, training loss: 310.3439636230469 = 0.050655853003263474 + 50.0 * 6.20586633682251
Epoch 2110, val loss: 1.0915132761001587
Epoch 2120, training loss: 310.5672912597656 = 0.04986245185136795 + 50.0 * 6.210348606109619
Epoch 2120, val loss: 1.0948911905288696
Epoch 2130, training loss: 310.3764343261719 = 0.04904182255268097 + 50.0 * 6.206547737121582
Epoch 2130, val loss: 1.0975157022476196
Epoch 2140, training loss: 310.3335266113281 = 0.04824985936284065 + 50.0 * 6.205705642700195
Epoch 2140, val loss: 1.100856065750122
Epoch 2150, training loss: 310.30413818359375 = 0.047500014305114746 + 50.0 * 6.205132961273193
Epoch 2150, val loss: 1.1042563915252686
Epoch 2160, training loss: 310.4066467285156 = 0.046779025346040726 + 50.0 * 6.207197189331055
Epoch 2160, val loss: 1.1075983047485352
Epoch 2170, training loss: 310.4242248535156 = 0.046036332845687866 + 50.0 * 6.207563400268555
Epoch 2170, val loss: 1.1106643676757812
Epoch 2180, training loss: 310.42071533203125 = 0.04530024901032448 + 50.0 * 6.207508563995361
Epoch 2180, val loss: 1.1133148670196533
Epoch 2190, training loss: 310.46527099609375 = 0.04460727423429489 + 50.0 * 6.208413124084473
Epoch 2190, val loss: 1.1170622110366821
Epoch 2200, training loss: 310.4264831542969 = 0.04390719160437584 + 50.0 * 6.207651615142822
Epoch 2200, val loss: 1.119706630706787
Epoch 2210, training loss: 310.2857360839844 = 0.04322241246700287 + 50.0 * 6.204850673675537
Epoch 2210, val loss: 1.1227905750274658
Epoch 2220, training loss: 310.41790771484375 = 0.04256988316774368 + 50.0 * 6.2075066566467285
Epoch 2220, val loss: 1.125792384147644
Epoch 2230, training loss: 310.2398986816406 = 0.04191456735134125 + 50.0 * 6.203959941864014
Epoch 2230, val loss: 1.1291166543960571
Epoch 2240, training loss: 310.2472839355469 = 0.04128660634160042 + 50.0 * 6.204119682312012
Epoch 2240, val loss: 1.1320741176605225
Epoch 2250, training loss: 310.208251953125 = 0.04067791625857353 + 50.0 * 6.2033514976501465
Epoch 2250, val loss: 1.1352131366729736
Epoch 2260, training loss: 310.5068054199219 = 0.04009120538830757 + 50.0 * 6.209333896636963
Epoch 2260, val loss: 1.1382197141647339
Epoch 2270, training loss: 310.32171630859375 = 0.03947066888213158 + 50.0 * 6.2056450843811035
Epoch 2270, val loss: 1.1413440704345703
Epoch 2280, training loss: 310.38531494140625 = 0.03886996582150459 + 50.0 * 6.206928730010986
Epoch 2280, val loss: 1.1441292762756348
Epoch 2290, training loss: 310.2166442871094 = 0.038280636072158813 + 50.0 * 6.2035675048828125
Epoch 2290, val loss: 1.1470158100128174
Epoch 2300, training loss: 310.2541198730469 = 0.0377206914126873 + 50.0 * 6.2043280601501465
Epoch 2300, val loss: 1.1500800848007202
Epoch 2310, training loss: 310.43121337890625 = 0.03717326372861862 + 50.0 * 6.207880973815918
Epoch 2310, val loss: 1.1529229879379272
Epoch 2320, training loss: 310.2276306152344 = 0.03661126643419266 + 50.0 * 6.20382022857666
Epoch 2320, val loss: 1.1561509370803833
Epoch 2330, training loss: 310.15948486328125 = 0.036077070981264114 + 50.0 * 6.202468395233154
Epoch 2330, val loss: 1.158866286277771
Epoch 2340, training loss: 310.1510314941406 = 0.035569850355386734 + 50.0 * 6.2023091316223145
Epoch 2340, val loss: 1.1617847681045532
Epoch 2350, training loss: 310.355712890625 = 0.03507077693939209 + 50.0 * 6.2064127922058105
Epoch 2350, val loss: 1.1645793914794922
Epoch 2360, training loss: 310.27154541015625 = 0.03456166386604309 + 50.0 * 6.204739570617676
Epoch 2360, val loss: 1.1679121255874634
Epoch 2370, training loss: 310.2264404296875 = 0.03405602276325226 + 50.0 * 6.203847408294678
Epoch 2370, val loss: 1.1703107357025146
Epoch 2380, training loss: 310.1498107910156 = 0.03356480225920677 + 50.0 * 6.202324867248535
Epoch 2380, val loss: 1.173151969909668
Epoch 2390, training loss: 310.0871887207031 = 0.03309677168726921 + 50.0 * 6.2010817527771
Epoch 2390, val loss: 1.176038146018982
Epoch 2400, training loss: 310.1409912109375 = 0.03264762461185455 + 50.0 * 6.20216703414917
Epoch 2400, val loss: 1.1789921522140503
Epoch 2410, training loss: 310.4624938964844 = 0.03220100700855255 + 50.0 * 6.208606243133545
Epoch 2410, val loss: 1.1817331314086914
Epoch 2420, training loss: 310.127685546875 = 0.03170888498425484 + 50.0 * 6.2019195556640625
Epoch 2420, val loss: 1.184296727180481
Epoch 2430, training loss: 310.0535583496094 = 0.03125952556729317 + 50.0 * 6.200446128845215
Epoch 2430, val loss: 1.187002420425415
Epoch 2440, training loss: 310.045654296875 = 0.030842620879411697 + 50.0 * 6.200295925140381
Epoch 2440, val loss: 1.1899771690368652
Epoch 2450, training loss: 310.1783142089844 = 0.03043765015900135 + 50.0 * 6.202957630157471
Epoch 2450, val loss: 1.192883014678955
Epoch 2460, training loss: 310.0726318359375 = 0.030012592673301697 + 50.0 * 6.200852870941162
Epoch 2460, val loss: 1.1953526735305786
Epoch 2470, training loss: 310.0963439941406 = 0.029595108702778816 + 50.0 * 6.2013349533081055
Epoch 2470, val loss: 1.197697639465332
Epoch 2480, training loss: 310.4137268066406 = 0.02921002171933651 + 50.0 * 6.207690238952637
Epoch 2480, val loss: 1.2005492448806763
Epoch 2490, training loss: 310.1252746582031 = 0.0287966039031744 + 50.0 * 6.201929092407227
Epoch 2490, val loss: 1.203057050704956
Epoch 2500, training loss: 310.0339050292969 = 0.02840898185968399 + 50.0 * 6.200109958648682
Epoch 2500, val loss: 1.2056678533554077
Epoch 2510, training loss: 310.0032043457031 = 0.028035355731844902 + 50.0 * 6.199502944946289
Epoch 2510, val loss: 1.2084569931030273
Epoch 2520, training loss: 310.0913391113281 = 0.02768116071820259 + 50.0 * 6.201273441314697
Epoch 2520, val loss: 1.2109973430633545
Epoch 2530, training loss: 310.1358947753906 = 0.027312610298395157 + 50.0 * 6.202171325683594
Epoch 2530, val loss: 1.2135592699050903
Epoch 2540, training loss: 310.12310791015625 = 0.0269447173923254 + 50.0 * 6.201923370361328
Epoch 2540, val loss: 1.2161986827850342
Epoch 2550, training loss: 309.997314453125 = 0.02658868208527565 + 50.0 * 6.1994147300720215
Epoch 2550, val loss: 1.2186039686203003
Epoch 2560, training loss: 310.0287170410156 = 0.026248915120959282 + 50.0 * 6.20004940032959
Epoch 2560, val loss: 1.2213521003723145
Epoch 2570, training loss: 310.17999267578125 = 0.025912446901202202 + 50.0 * 6.2030816078186035
Epoch 2570, val loss: 1.2235127687454224
Epoch 2580, training loss: 310.0487365722656 = 0.025572191923856735 + 50.0 * 6.20046329498291
Epoch 2580, val loss: 1.2262561321258545
Epoch 2590, training loss: 309.99957275390625 = 0.02524067461490631 + 50.0 * 6.19948673248291
Epoch 2590, val loss: 1.228585124015808
Epoch 2600, training loss: 309.9753112792969 = 0.02491847798228264 + 50.0 * 6.199007511138916
Epoch 2600, val loss: 1.2310770750045776
Epoch 2610, training loss: 310.161376953125 = 0.024605819955468178 + 50.0 * 6.202735424041748
Epoch 2610, val loss: 1.2331076860427856
Epoch 2620, training loss: 310.05078125 = 0.02428874373435974 + 50.0 * 6.200530052185059
Epoch 2620, val loss: 1.2361918687820435
Epoch 2630, training loss: 309.96307373046875 = 0.023975325748324394 + 50.0 * 6.198781490325928
Epoch 2630, val loss: 1.2381583452224731
Epoch 2640, training loss: 309.9074401855469 = 0.023679014295339584 + 50.0 * 6.1976752281188965
Epoch 2640, val loss: 1.2407853603363037
Epoch 2650, training loss: 309.90875244140625 = 0.023396162316203117 + 50.0 * 6.197706699371338
Epoch 2650, val loss: 1.2431265115737915
Epoch 2660, training loss: 310.2910461425781 = 0.02312045358121395 + 50.0 * 6.205358028411865
Epoch 2660, val loss: 1.2451605796813965
Epoch 2670, training loss: 309.9905090332031 = 0.02281719632446766 + 50.0 * 6.1993536949157715
Epoch 2670, val loss: 1.247912883758545
Epoch 2680, training loss: 309.9895935058594 = 0.02253306284546852 + 50.0 * 6.199341297149658
Epoch 2680, val loss: 1.2500053644180298
Epoch 2690, training loss: 309.9728698730469 = 0.022255657240748405 + 50.0 * 6.199012279510498
Epoch 2690, val loss: 1.2525043487548828
Epoch 2700, training loss: 309.856201171875 = 0.021986374631524086 + 50.0 * 6.196684837341309
Epoch 2700, val loss: 1.2549363374710083
Epoch 2710, training loss: 309.9169006347656 = 0.02173074521124363 + 50.0 * 6.197903156280518
Epoch 2710, val loss: 1.2572214603424072
Epoch 2720, training loss: 310.12457275390625 = 0.02147352695465088 + 50.0 * 6.202061653137207
Epoch 2720, val loss: 1.259436845779419
Epoch 2730, training loss: 309.9987487792969 = 0.02120506390929222 + 50.0 * 6.199550628662109
Epoch 2730, val loss: 1.2614145278930664
Epoch 2740, training loss: 309.8587951660156 = 0.02095172367990017 + 50.0 * 6.196756362915039
Epoch 2740, val loss: 1.2639695405960083
Epoch 2750, training loss: 309.9414367675781 = 0.020713383331894875 + 50.0 * 6.198414325714111
Epoch 2750, val loss: 1.2662674188613892
Epoch 2760, training loss: 310.2469787597656 = 0.020463494583964348 + 50.0 * 6.204529762268066
Epoch 2760, val loss: 1.2682268619537354
Epoch 2770, training loss: 309.9274597167969 = 0.0202045701444149 + 50.0 * 6.198145389556885
Epoch 2770, val loss: 1.270361065864563
Epoch 2780, training loss: 309.8694152832031 = 0.01995919831097126 + 50.0 * 6.196989059448242
Epoch 2780, val loss: 1.2724348306655884
Epoch 2790, training loss: 309.81231689453125 = 0.019739530980587006 + 50.0 * 6.1958513259887695
Epoch 2790, val loss: 1.27499520778656
Epoch 2800, training loss: 309.8005676269531 = 0.019519725814461708 + 50.0 * 6.195621013641357
Epoch 2800, val loss: 1.2770965099334717
Epoch 2810, training loss: 310.2828063964844 = 0.019319530576467514 + 50.0 * 6.205269813537598
Epoch 2810, val loss: 1.2794524431228638
Epoch 2820, training loss: 309.9461669921875 = 0.019063550978899002 + 50.0 * 6.19854211807251
Epoch 2820, val loss: 1.2809207439422607
Epoch 2830, training loss: 309.83612060546875 = 0.018846148625016212 + 50.0 * 6.196345329284668
Epoch 2830, val loss: 1.2833771705627441
Epoch 2840, training loss: 309.7846374511719 = 0.018636932596564293 + 50.0 * 6.195319652557373
Epoch 2840, val loss: 1.2855693101882935
Epoch 2850, training loss: 309.78802490234375 = 0.01843665912747383 + 50.0 * 6.195391654968262
Epoch 2850, val loss: 1.2877223491668701
Epoch 2860, training loss: 310.15313720703125 = 0.018239272758364677 + 50.0 * 6.20269775390625
Epoch 2860, val loss: 1.289703369140625
Epoch 2870, training loss: 309.83624267578125 = 0.018022775650024414 + 50.0 * 6.196363925933838
Epoch 2870, val loss: 1.2918356657028198
Epoch 2880, training loss: 309.83428955078125 = 0.017821229994297028 + 50.0 * 6.196329116821289
Epoch 2880, val loss: 1.2939802408218384
Epoch 2890, training loss: 309.8602294921875 = 0.01762615144252777 + 50.0 * 6.19685173034668
Epoch 2890, val loss: 1.296157717704773
Epoch 2900, training loss: 309.8958435058594 = 0.017428075894713402 + 50.0 * 6.197568416595459
Epoch 2900, val loss: 1.2978533506393433
Epoch 2910, training loss: 309.732421875 = 0.017233246937394142 + 50.0 * 6.194303512573242
Epoch 2910, val loss: 1.2997791767120361
Epoch 2920, training loss: 309.7532958984375 = 0.017052123323082924 + 50.0 * 6.1947245597839355
Epoch 2920, val loss: 1.3019485473632812
Epoch 2930, training loss: 309.9474182128906 = 0.016876917332410812 + 50.0 * 6.198610782623291
Epoch 2930, val loss: 1.3039382696151733
Epoch 2940, training loss: 309.8141174316406 = 0.01668221317231655 + 50.0 * 6.195948600769043
Epoch 2940, val loss: 1.305899739265442
Epoch 2950, training loss: 309.7274169921875 = 0.016500916332006454 + 50.0 * 6.194218158721924
Epoch 2950, val loss: 1.3078250885009766
Epoch 2960, training loss: 309.700439453125 = 0.016324853524565697 + 50.0 * 6.1936821937561035
Epoch 2960, val loss: 1.3099970817565918
Epoch 2970, training loss: 309.7677307128906 = 0.016160083934664726 + 50.0 * 6.19503116607666
Epoch 2970, val loss: 1.3121452331542969
Epoch 2980, training loss: 309.9146423339844 = 0.015992406755685806 + 50.0 * 6.197973251342773
Epoch 2980, val loss: 1.3140385150909424
Epoch 2990, training loss: 309.9600830078125 = 0.015810346230864525 + 50.0 * 6.198885440826416
Epoch 2990, val loss: 1.3150875568389893
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8350026357406432
The final CL Acc:0.72469, 0.00462, The final GNN Acc:0.83764, 0.00215
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.2 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11564])
remove edge: torch.Size([2, 9504])
updated graph: torch.Size([2, 10512])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.80029296875 = 1.9572149515151978 + 50.0 * 8.596861839294434
Epoch 0, val loss: 1.9520593881607056
Epoch 10, training loss: 431.7606201171875 = 1.9472224712371826 + 50.0 * 8.596267700195312
Epoch 10, val loss: 1.9418445825576782
Epoch 20, training loss: 431.5572814941406 = 1.9350712299346924 + 50.0 * 8.59244441986084
Epoch 20, val loss: 1.9292349815368652
Epoch 30, training loss: 430.2946472167969 = 1.9195584058761597 + 50.0 * 8.56750202178955
Epoch 30, val loss: 1.9131288528442383
Epoch 40, training loss: 423.4511413574219 = 1.8995361328125 + 50.0 * 8.431032180786133
Epoch 40, val loss: 1.8925894498825073
Epoch 50, training loss: 398.6322937011719 = 1.8770171403884888 + 50.0 * 7.935105323791504
Epoch 50, val loss: 1.8694227933883667
Epoch 60, training loss: 378.35992431640625 = 1.8551925420761108 + 50.0 * 7.530094623565674
Epoch 60, val loss: 1.8479756116867065
Epoch 70, training loss: 367.7039794921875 = 1.842061996459961 + 50.0 * 7.3172383308410645
Epoch 70, val loss: 1.835348129272461
Epoch 80, training loss: 361.236572265625 = 1.8316611051559448 + 50.0 * 7.188098430633545
Epoch 80, val loss: 1.8251006603240967
Epoch 90, training loss: 356.82379150390625 = 1.819685459136963 + 50.0 * 7.1000823974609375
Epoch 90, val loss: 1.8138628005981445
Epoch 100, training loss: 351.27569580078125 = 1.8102035522460938 + 50.0 * 6.989309787750244
Epoch 100, val loss: 1.8054782152175903
Epoch 110, training loss: 345.412353515625 = 1.8028481006622314 + 50.0 * 6.872189998626709
Epoch 110, val loss: 1.7989202737808228
Epoch 120, training loss: 341.142578125 = 1.795885682106018 + 50.0 * 6.786933898925781
Epoch 120, val loss: 1.7925342321395874
Epoch 130, training loss: 338.03125 = 1.7883929014205933 + 50.0 * 6.724857330322266
Epoch 130, val loss: 1.7857178449630737
Epoch 140, training loss: 335.6634216308594 = 1.7806216478347778 + 50.0 * 6.677656173706055
Epoch 140, val loss: 1.7785451412200928
Epoch 150, training loss: 333.72625732421875 = 1.7726075649261475 + 50.0 * 6.639072895050049
Epoch 150, val loss: 1.771155834197998
Epoch 160, training loss: 332.2055358886719 = 1.7643259763717651 + 50.0 * 6.608824729919434
Epoch 160, val loss: 1.7634904384613037
Epoch 170, training loss: 330.89447021484375 = 1.7555097341537476 + 50.0 * 6.5827789306640625
Epoch 170, val loss: 1.755375862121582
Epoch 180, training loss: 329.93243408203125 = 1.7460521459579468 + 50.0 * 6.563727378845215
Epoch 180, val loss: 1.7468602657318115
Epoch 190, training loss: 328.8601989746094 = 1.735705852508545 + 50.0 * 6.542489528656006
Epoch 190, val loss: 1.7376489639282227
Epoch 200, training loss: 328.04571533203125 = 1.7245314121246338 + 50.0 * 6.526423931121826
Epoch 200, val loss: 1.7278718948364258
Epoch 210, training loss: 327.3047180175781 = 1.7123878002166748 + 50.0 * 6.51184606552124
Epoch 210, val loss: 1.7171937227249146
Epoch 220, training loss: 326.6527099609375 = 1.6992942094802856 + 50.0 * 6.499068737030029
Epoch 220, val loss: 1.7057135105133057
Epoch 230, training loss: 326.1202392578125 = 1.6851093769073486 + 50.0 * 6.488702297210693
Epoch 230, val loss: 1.6933746337890625
Epoch 240, training loss: 325.5071716308594 = 1.6697356700897217 + 50.0 * 6.476748943328857
Epoch 240, val loss: 1.6800436973571777
Epoch 250, training loss: 325.02276611328125 = 1.653236746788025 + 50.0 * 6.467391014099121
Epoch 250, val loss: 1.6658461093902588
Epoch 260, training loss: 324.5626220703125 = 1.6354528665542603 + 50.0 * 6.458542823791504
Epoch 260, val loss: 1.6505788564682007
Epoch 270, training loss: 324.0898132324219 = 1.61637282371521 + 50.0 * 6.449469089508057
Epoch 270, val loss: 1.6343677043914795
Epoch 280, training loss: 323.70263671875 = 1.5959713459014893 + 50.0 * 6.442132949829102
Epoch 280, val loss: 1.6171190738677979
Epoch 290, training loss: 323.3557434082031 = 1.5742474794387817 + 50.0 * 6.435629844665527
Epoch 290, val loss: 1.5988476276397705
Epoch 300, training loss: 322.9857482910156 = 1.5513666868209839 + 50.0 * 6.428687572479248
Epoch 300, val loss: 1.5797103643417358
Epoch 310, training loss: 322.936767578125 = 1.5273263454437256 + 50.0 * 6.428188800811768
Epoch 310, val loss: 1.5597096681594849
Epoch 320, training loss: 322.3836975097656 = 1.5020743608474731 + 50.0 * 6.417632579803467
Epoch 320, val loss: 1.538962960243225
Epoch 330, training loss: 322.04925537109375 = 1.4758952856063843 + 50.0 * 6.411467552185059
Epoch 330, val loss: 1.5176247358322144
Epoch 340, training loss: 321.8331604003906 = 1.44889497756958 + 50.0 * 6.407685279846191
Epoch 340, val loss: 1.495766282081604
Epoch 350, training loss: 321.6525573730469 = 1.4211220741271973 + 50.0 * 6.404628753662109
Epoch 350, val loss: 1.4735628366470337
Epoch 360, training loss: 321.23748779296875 = 1.3927395343780518 + 50.0 * 6.396895408630371
Epoch 360, val loss: 1.4510774612426758
Epoch 370, training loss: 320.9520568847656 = 1.3640660047531128 + 50.0 * 6.391759395599365
Epoch 370, val loss: 1.4286437034606934
Epoch 380, training loss: 320.6842041015625 = 1.3350961208343506 + 50.0 * 6.386982440948486
Epoch 380, val loss: 1.4062354564666748
Epoch 390, training loss: 320.4851989746094 = 1.306035041809082 + 50.0 * 6.3835835456848145
Epoch 390, val loss: 1.3839901685714722
Epoch 400, training loss: 320.2467041015625 = 1.2770905494689941 + 50.0 * 6.379392147064209
Epoch 400, val loss: 1.362139105796814
Epoch 410, training loss: 320.0144958496094 = 1.248380422592163 + 50.0 * 6.375322341918945
Epoch 410, val loss: 1.3406336307525635
Epoch 420, training loss: 319.7561340332031 = 1.2200039625167847 + 50.0 * 6.370722770690918
Epoch 420, val loss: 1.3197863101959229
Epoch 430, training loss: 319.5133361816406 = 1.192200779914856 + 50.0 * 6.366422653198242
Epoch 430, val loss: 1.299572229385376
Epoch 440, training loss: 319.7373352050781 = 1.1649315357208252 + 50.0 * 6.371448516845703
Epoch 440, val loss: 1.2800631523132324
Epoch 450, training loss: 319.2738037109375 = 1.1380456686019897 + 50.0 * 6.362715721130371
Epoch 450, val loss: 1.2611602544784546
Epoch 460, training loss: 318.94024658203125 = 1.1120367050170898 + 50.0 * 6.356564044952393
Epoch 460, val loss: 1.243112564086914
Epoch 470, training loss: 318.7297058105469 = 1.0868489742279053 + 50.0 * 6.3528571128845215
Epoch 470, val loss: 1.226042628288269
Epoch 480, training loss: 318.6805725097656 = 1.0624110698699951 + 50.0 * 6.352363109588623
Epoch 480, val loss: 1.209886908531189
Epoch 490, training loss: 318.4961242675781 = 1.0386736392974854 + 50.0 * 6.349149227142334
Epoch 490, val loss: 1.1946971416473389
Epoch 500, training loss: 318.2025146484375 = 1.015516996383667 + 50.0 * 6.343739986419678
Epoch 500, val loss: 1.1800545454025269
Epoch 510, training loss: 318.0613708496094 = 0.9931687712669373 + 50.0 * 6.341363906860352
Epoch 510, val loss: 1.1662174463272095
Epoch 520, training loss: 317.9720458984375 = 0.9714127779006958 + 50.0 * 6.340012550354004
Epoch 520, val loss: 1.153399109840393
Epoch 530, training loss: 317.8317565917969 = 0.9501436352729797 + 50.0 * 6.337632179260254
Epoch 530, val loss: 1.1409811973571777
Epoch 540, training loss: 317.6454162597656 = 0.9295251965522766 + 50.0 * 6.334317684173584
Epoch 540, val loss: 1.1294276714324951
Epoch 550, training loss: 317.5540771484375 = 0.9095253944396973 + 50.0 * 6.33289098739624
Epoch 550, val loss: 1.1187547445297241
Epoch 560, training loss: 317.4139099121094 = 0.8899139165878296 + 50.0 * 6.330479621887207
Epoch 560, val loss: 1.108747959136963
Epoch 570, training loss: 317.2264404296875 = 0.870810866355896 + 50.0 * 6.327112197875977
Epoch 570, val loss: 1.0990805625915527
Epoch 580, training loss: 317.1417236328125 = 0.852103054523468 + 50.0 * 6.32579231262207
Epoch 580, val loss: 1.0902774333953857
Epoch 590, training loss: 317.0057373046875 = 0.8338207602500916 + 50.0 * 6.3234381675720215
Epoch 590, val loss: 1.0818661451339722
Epoch 600, training loss: 316.9144592285156 = 0.8158995509147644 + 50.0 * 6.321971416473389
Epoch 600, val loss: 1.0741900205612183
Epoch 610, training loss: 316.8930969238281 = 0.7982676029205322 + 50.0 * 6.321896076202393
Epoch 610, val loss: 1.0666300058364868
Epoch 620, training loss: 316.6537780761719 = 0.7809785008430481 + 50.0 * 6.317456245422363
Epoch 620, val loss: 1.0599204301834106
Epoch 630, training loss: 316.60504150390625 = 0.7640199065208435 + 50.0 * 6.3168206214904785
Epoch 630, val loss: 1.0535913705825806
Epoch 640, training loss: 316.5777282714844 = 0.7472808361053467 + 50.0 * 6.316608905792236
Epoch 640, val loss: 1.047562837600708
Epoch 650, training loss: 316.3731994628906 = 0.7308141589164734 + 50.0 * 6.312848091125488
Epoch 650, val loss: 1.0421953201293945
Epoch 660, training loss: 316.2397766113281 = 0.714608371257782 + 50.0 * 6.3105034828186035
Epoch 660, val loss: 1.0371038913726807
Epoch 670, training loss: 316.3960266113281 = 0.6986116766929626 + 50.0 * 6.313948154449463
Epoch 670, val loss: 1.0323598384857178
Epoch 680, training loss: 316.07867431640625 = 0.6827379465103149 + 50.0 * 6.307918548583984
Epoch 680, val loss: 1.0279065370559692
Epoch 690, training loss: 316.01251220703125 = 0.6671245098114014 + 50.0 * 6.306908130645752
Epoch 690, val loss: 1.0239112377166748
Epoch 700, training loss: 315.87786865234375 = 0.6517506241798401 + 50.0 * 6.3045220375061035
Epoch 700, val loss: 1.0203689336776733
Epoch 710, training loss: 315.8262939453125 = 0.6365782618522644 + 50.0 * 6.3037943840026855
Epoch 710, val loss: 1.0172810554504395
Epoch 720, training loss: 315.7661437988281 = 0.6215696334838867 + 50.0 * 6.302891254425049
Epoch 720, val loss: 1.0144374370574951
Epoch 730, training loss: 315.6048583984375 = 0.6067774891853333 + 50.0 * 6.299961566925049
Epoch 730, val loss: 1.0119593143463135
Epoch 740, training loss: 315.80767822265625 = 0.5921432971954346 + 50.0 * 6.3043107986450195
Epoch 740, val loss: 1.0095752477645874
Epoch 750, training loss: 315.5505065917969 = 0.5777178406715393 + 50.0 * 6.299455642700195
Epoch 750, val loss: 1.0081144571304321
Epoch 760, training loss: 315.3558349609375 = 0.5634639859199524 + 50.0 * 6.295846939086914
Epoch 760, val loss: 1.0064653158187866
Epoch 770, training loss: 315.2601623535156 = 0.5495364665985107 + 50.0 * 6.294212818145752
Epoch 770, val loss: 1.005502462387085
Epoch 780, training loss: 315.230712890625 = 0.5357791185379028 + 50.0 * 6.293899059295654
Epoch 780, val loss: 1.0046191215515137
Epoch 790, training loss: 315.2251892089844 = 0.5222079157829285 + 50.0 * 6.2940592765808105
Epoch 790, val loss: 1.004066824913025
Epoch 800, training loss: 315.15087890625 = 0.5086847543716431 + 50.0 * 6.292843341827393
Epoch 800, val loss: 1.003571629524231
Epoch 810, training loss: 314.9827575683594 = 0.49552762508392334 + 50.0 * 6.289744853973389
Epoch 810, val loss: 1.003574252128601
Epoch 820, training loss: 314.91534423828125 = 0.48262354731559753 + 50.0 * 6.288654327392578
Epoch 820, val loss: 1.0037646293640137
Epoch 830, training loss: 315.193359375 = 0.4699828624725342 + 50.0 * 6.294467926025391
Epoch 830, val loss: 1.0041340589523315
Epoch 840, training loss: 314.7966003417969 = 0.4574173092842102 + 50.0 * 6.286783218383789
Epoch 840, val loss: 1.004900574684143
Epoch 850, training loss: 314.727783203125 = 0.4451993405818939 + 50.0 * 6.285652160644531
Epoch 850, val loss: 1.0058578252792358
Epoch 860, training loss: 314.8122863769531 = 0.4332734942436218 + 50.0 * 6.2875800132751465
Epoch 860, val loss: 1.0068403482437134
Epoch 870, training loss: 314.7082214355469 = 0.42147374153137207 + 50.0 * 6.2857346534729
Epoch 870, val loss: 1.0082037448883057
Epoch 880, training loss: 314.5954895019531 = 0.41001391410827637 + 50.0 * 6.283709526062012
Epoch 880, val loss: 1.0098316669464111
Epoch 890, training loss: 314.4879150390625 = 0.39879515767097473 + 50.0 * 6.281782150268555
Epoch 890, val loss: 1.0116181373596191
Epoch 900, training loss: 314.4739685058594 = 0.38787949085235596 + 50.0 * 6.281721591949463
Epoch 900, val loss: 1.0137354135513306
Epoch 910, training loss: 314.7566833496094 = 0.3771977424621582 + 50.0 * 6.2875895500183105
Epoch 910, val loss: 1.0159980058670044
Epoch 920, training loss: 314.3954772949219 = 0.3665538430213928 + 50.0 * 6.28057861328125
Epoch 920, val loss: 1.0179296731948853
Epoch 930, training loss: 314.2889404296875 = 0.35633212327957153 + 50.0 * 6.278652191162109
Epoch 930, val loss: 1.0203837156295776
Epoch 940, training loss: 314.1991882324219 = 0.34643498063087463 + 50.0 * 6.277055263519287
Epoch 940, val loss: 1.0233628749847412
Epoch 950, training loss: 314.1841125488281 = 0.3368239402770996 + 50.0 * 6.276946067810059
Epoch 950, val loss: 1.0265685319900513
Epoch 960, training loss: 314.2074890136719 = 0.32734596729278564 + 50.0 * 6.2776031494140625
Epoch 960, val loss: 1.0295392274856567
Epoch 970, training loss: 314.0861511230469 = 0.3180992007255554 + 50.0 * 6.27536153793335
Epoch 970, val loss: 1.0329774618148804
Epoch 980, training loss: 314.0225830078125 = 0.3091510534286499 + 50.0 * 6.274269104003906
Epoch 980, val loss: 1.0364779233932495
Epoch 990, training loss: 313.948974609375 = 0.30051276087760925 + 50.0 * 6.2729692459106445
Epoch 990, val loss: 1.040486454963684
Epoch 1000, training loss: 314.5085754394531 = 0.2921088635921478 + 50.0 * 6.284328937530518
Epoch 1000, val loss: 1.0444070100784302
Epoch 1010, training loss: 313.9674377441406 = 0.28377896547317505 + 50.0 * 6.273673057556152
Epoch 1010, val loss: 1.0486230850219727
Epoch 1020, training loss: 313.8523254394531 = 0.27576586604118347 + 50.0 * 6.271531105041504
Epoch 1020, val loss: 1.0530295372009277
Epoch 1030, training loss: 313.77105712890625 = 0.2680296301841736 + 50.0 * 6.2700605392456055
Epoch 1030, val loss: 1.0577596426010132
Epoch 1040, training loss: 313.9148254394531 = 0.26059406995773315 + 50.0 * 6.27308464050293
Epoch 1040, val loss: 1.0629644393920898
Epoch 1050, training loss: 313.950439453125 = 0.25316596031188965 + 50.0 * 6.273945331573486
Epoch 1050, val loss: 1.067170262336731
Epoch 1060, training loss: 313.6910705566406 = 0.24603413045406342 + 50.0 * 6.2689008712768555
Epoch 1060, val loss: 1.0722182989120483
Epoch 1070, training loss: 313.630126953125 = 0.2391698956489563 + 50.0 * 6.267818927764893
Epoch 1070, val loss: 1.0777539014816284
Epoch 1080, training loss: 313.5506286621094 = 0.232505202293396 + 50.0 * 6.266362190246582
Epoch 1080, val loss: 1.0831257104873657
Epoch 1090, training loss: 313.7797546386719 = 0.22608646750450134 + 50.0 * 6.271073818206787
Epoch 1090, val loss: 1.0886874198913574
Epoch 1100, training loss: 313.7718811035156 = 0.2197408676147461 + 50.0 * 6.271042823791504
Epoch 1100, val loss: 1.0940395593643188
Epoch 1110, training loss: 313.4703674316406 = 0.21360479295253754 + 50.0 * 6.265135765075684
Epoch 1110, val loss: 1.099968433380127
Epoch 1120, training loss: 313.4245300292969 = 0.20772747695446014 + 50.0 * 6.264336109161377
Epoch 1120, val loss: 1.1061078310012817
Epoch 1130, training loss: 313.418212890625 = 0.2020600438117981 + 50.0 * 6.2643232345581055
Epoch 1130, val loss: 1.1123319864273071
Epoch 1140, training loss: 313.60321044921875 = 0.19656984508037567 + 50.0 * 6.268133163452148
Epoch 1140, val loss: 1.1186238527297974
Epoch 1150, training loss: 313.43994140625 = 0.19109489023685455 + 50.0 * 6.264976978302002
Epoch 1150, val loss: 1.1245307922363281
Epoch 1160, training loss: 313.3998107910156 = 0.1859303116798401 + 50.0 * 6.264277458190918
Epoch 1160, val loss: 1.1312013864517212
Epoch 1170, training loss: 313.2640686035156 = 0.1808447390794754 + 50.0 * 6.261664390563965
Epoch 1170, val loss: 1.1375372409820557
Epoch 1180, training loss: 313.2016296386719 = 0.17601220309734344 + 50.0 * 6.260511875152588
Epoch 1180, val loss: 1.1444491147994995
Epoch 1190, training loss: 313.22784423828125 = 0.1713024526834488 + 50.0 * 6.2611308097839355
Epoch 1190, val loss: 1.1511824131011963
Epoch 1200, training loss: 313.1785583496094 = 0.16669055819511414 + 50.0 * 6.260237216949463
Epoch 1200, val loss: 1.15768301486969
Epoch 1210, training loss: 313.28399658203125 = 0.16222743690013885 + 50.0 * 6.262435436248779
Epoch 1210, val loss: 1.1645008325576782
Epoch 1220, training loss: 313.1190490722656 = 0.15787889063358307 + 50.0 * 6.259223937988281
Epoch 1220, val loss: 1.1712123155593872
Epoch 1230, training loss: 313.0348205566406 = 0.15372177958488464 + 50.0 * 6.257621765136719
Epoch 1230, val loss: 1.178284764289856
Epoch 1240, training loss: 313.31494140625 = 0.1497327834367752 + 50.0 * 6.263304710388184
Epoch 1240, val loss: 1.1853768825531006
Epoch 1250, training loss: 313.04534912109375 = 0.14577756822109222 + 50.0 * 6.257991313934326
Epoch 1250, val loss: 1.1924258470535278
Epoch 1260, training loss: 312.96539306640625 = 0.1419813185930252 + 50.0 * 6.256468772888184
Epoch 1260, val loss: 1.1994355916976929
Epoch 1270, training loss: 312.9129638671875 = 0.13833796977996826 + 50.0 * 6.255492687225342
Epoch 1270, val loss: 1.2067598104476929
Epoch 1280, training loss: 313.11627197265625 = 0.13479824364185333 + 50.0 * 6.259629726409912
Epoch 1280, val loss: 1.21376633644104
Epoch 1290, training loss: 312.97381591796875 = 0.13133573532104492 + 50.0 * 6.25684928894043
Epoch 1290, val loss: 1.2213512659072876
Epoch 1300, training loss: 312.89422607421875 = 0.12795096635818481 + 50.0 * 6.2553253173828125
Epoch 1300, val loss: 1.2283568382263184
Epoch 1310, training loss: 312.8165588378906 = 0.12474143505096436 + 50.0 * 6.253836154937744
Epoch 1310, val loss: 1.2359853982925415
Epoch 1320, training loss: 312.7726135253906 = 0.12160889804363251 + 50.0 * 6.2530198097229
Epoch 1320, val loss: 1.2434630393981934
Epoch 1330, training loss: 312.9355163574219 = 0.11860956996679306 + 50.0 * 6.256338119506836
Epoch 1330, val loss: 1.2510055303573608
Epoch 1340, training loss: 313.0608215332031 = 0.11560603976249695 + 50.0 * 6.258904457092285
Epoch 1340, val loss: 1.2580770254135132
Epoch 1350, training loss: 312.76531982421875 = 0.11271020025014877 + 50.0 * 6.253052234649658
Epoch 1350, val loss: 1.2655490636825562
Epoch 1360, training loss: 312.6906433105469 = 0.10991623997688293 + 50.0 * 6.251614093780518
Epoch 1360, val loss: 1.272878646850586
Epoch 1370, training loss: 312.6310729980469 = 0.10723225772380829 + 50.0 * 6.250476837158203
Epoch 1370, val loss: 1.2804820537567139
Epoch 1380, training loss: 312.8757629394531 = 0.10464604198932648 + 50.0 * 6.255422115325928
Epoch 1380, val loss: 1.287908911705017
Epoch 1390, training loss: 312.6855163574219 = 0.10206234455108643 + 50.0 * 6.251669406890869
Epoch 1390, val loss: 1.2950857877731323
Epoch 1400, training loss: 312.66650390625 = 0.09958706796169281 + 50.0 * 6.251338481903076
Epoch 1400, val loss: 1.3024531602859497
Epoch 1410, training loss: 312.5629577636719 = 0.0972033217549324 + 50.0 * 6.24931526184082
Epoch 1410, val loss: 1.3100714683532715
Epoch 1420, training loss: 312.55523681640625 = 0.09490323066711426 + 50.0 * 6.24920654296875
Epoch 1420, val loss: 1.3176305294036865
Epoch 1430, training loss: 312.8294372558594 = 0.09267107397317886 + 50.0 * 6.254734992980957
Epoch 1430, val loss: 1.324919581413269
Epoch 1440, training loss: 312.5758361816406 = 0.09044232964515686 + 50.0 * 6.2497076988220215
Epoch 1440, val loss: 1.3321255445480347
Epoch 1450, training loss: 312.5487976074219 = 0.08831354230642319 + 50.0 * 6.249209403991699
Epoch 1450, val loss: 1.3394205570220947
Epoch 1460, training loss: 312.58233642578125 = 0.0862516239285469 + 50.0 * 6.249921798706055
Epoch 1460, val loss: 1.3466635942459106
Epoch 1470, training loss: 312.468994140625 = 0.08426980674266815 + 50.0 * 6.247694969177246
Epoch 1470, val loss: 1.3543440103530884
Epoch 1480, training loss: 312.3901062011719 = 0.08231472223997116 + 50.0 * 6.246155738830566
Epoch 1480, val loss: 1.3614445924758911
Epoch 1490, training loss: 312.6314697265625 = 0.08045145869255066 + 50.0 * 6.251020431518555
Epoch 1490, val loss: 1.3687249422073364
Epoch 1500, training loss: 312.46649169921875 = 0.07858962565660477 + 50.0 * 6.247758388519287
Epoch 1500, val loss: 1.3761317729949951
Epoch 1510, training loss: 312.4056701660156 = 0.07678338140249252 + 50.0 * 6.246577739715576
Epoch 1510, val loss: 1.3832523822784424
Epoch 1520, training loss: 312.4401550292969 = 0.07504530251026154 + 50.0 * 6.247302532196045
Epoch 1520, val loss: 1.3905247449874878
Epoch 1530, training loss: 312.30712890625 = 0.07334520667791367 + 50.0 * 6.244675636291504
Epoch 1530, val loss: 1.3975645303726196
Epoch 1540, training loss: 312.34234619140625 = 0.07171431183815002 + 50.0 * 6.245412826538086
Epoch 1540, val loss: 1.4046835899353027
Epoch 1550, training loss: 312.3848876953125 = 0.07011830806732178 + 50.0 * 6.24629545211792
Epoch 1550, val loss: 1.411753535270691
Epoch 1560, training loss: 312.2395324707031 = 0.06856749206781387 + 50.0 * 6.243419170379639
Epoch 1560, val loss: 1.419000506401062
Epoch 1570, training loss: 312.3243713378906 = 0.0670699030160904 + 50.0 * 6.245145797729492
Epoch 1570, val loss: 1.4263335466384888
Epoch 1580, training loss: 312.4926452636719 = 0.0655951052904129 + 50.0 * 6.248541355133057
Epoch 1580, val loss: 1.432783603668213
Epoch 1590, training loss: 312.3166809082031 = 0.06415999680757523 + 50.0 * 6.24505090713501
Epoch 1590, val loss: 1.4400469064712524
Epoch 1600, training loss: 312.1870422363281 = 0.06275710463523865 + 50.0 * 6.242486000061035
Epoch 1600, val loss: 1.4467588663101196
Epoch 1610, training loss: 312.1551818847656 = 0.06142820417881012 + 50.0 * 6.241875171661377
Epoch 1610, val loss: 1.4539893865585327
Epoch 1620, training loss: 312.2335510253906 = 0.060141175985336304 + 50.0 * 6.243467807769775
Epoch 1620, val loss: 1.4610154628753662
Epoch 1630, training loss: 312.1434020996094 = 0.058855991810560226 + 50.0 * 6.241690635681152
Epoch 1630, val loss: 1.4676823616027832
Epoch 1640, training loss: 312.2452697753906 = 0.0576133131980896 + 50.0 * 6.243752956390381
Epoch 1640, val loss: 1.4744863510131836
Epoch 1650, training loss: 312.15234375 = 0.05638938024640083 + 50.0 * 6.241919040679932
Epoch 1650, val loss: 1.4811824560165405
Epoch 1660, training loss: 312.073974609375 = 0.05520094186067581 + 50.0 * 6.240375518798828
Epoch 1660, val loss: 1.4879391193389893
Epoch 1670, training loss: 312.097900390625 = 0.05406586825847626 + 50.0 * 6.240876197814941
Epoch 1670, val loss: 1.4949538707733154
Epoch 1680, training loss: 312.2966003417969 = 0.05296251177787781 + 50.0 * 6.244873046875
Epoch 1680, val loss: 1.501382827758789
Epoch 1690, training loss: 312.1185302734375 = 0.051858775317668915 + 50.0 * 6.241333484649658
Epoch 1690, val loss: 1.5080699920654297
Epoch 1700, training loss: 312.00299072265625 = 0.0507882758975029 + 50.0 * 6.239044189453125
Epoch 1700, val loss: 1.5147039890289307
Epoch 1710, training loss: 311.9915771484375 = 0.04976562038064003 + 50.0 * 6.238836288452148
Epoch 1710, val loss: 1.5215046405792236
Epoch 1720, training loss: 312.0417175292969 = 0.04877465218305588 + 50.0 * 6.239859104156494
Epoch 1720, val loss: 1.5280287265777588
Epoch 1730, training loss: 312.0278625488281 = 0.04780392348766327 + 50.0 * 6.2396016120910645
Epoch 1730, val loss: 1.5345853567123413
Epoch 1740, training loss: 312.0855712890625 = 0.0468713715672493 + 50.0 * 6.240773677825928
Epoch 1740, val loss: 1.541015625
Epoch 1750, training loss: 311.95587158203125 = 0.04592425376176834 + 50.0 * 6.238198757171631
Epoch 1750, val loss: 1.547302007675171
Epoch 1760, training loss: 311.9377746582031 = 0.04502595216035843 + 50.0 * 6.237855434417725
Epoch 1760, val loss: 1.5537359714508057
Epoch 1770, training loss: 312.0743103027344 = 0.04415247589349747 + 50.0 * 6.240602970123291
Epoch 1770, val loss: 1.5600749254226685
Epoch 1780, training loss: 312.10986328125 = 0.043289873749017715 + 50.0 * 6.241331100463867
Epoch 1780, val loss: 1.566420316696167
Epoch 1790, training loss: 311.8819274902344 = 0.04245218634605408 + 50.0 * 6.236789703369141
Epoch 1790, val loss: 1.5726652145385742
Epoch 1800, training loss: 311.839599609375 = 0.04164585843682289 + 50.0 * 6.235958576202393
Epoch 1800, val loss: 1.5790431499481201
Epoch 1810, training loss: 311.82568359375 = 0.040862224996089935 + 50.0 * 6.235696315765381
Epoch 1810, val loss: 1.5853042602539062
Epoch 1820, training loss: 312.01953125 = 0.040112245827913284 + 50.0 * 6.239588737487793
Epoch 1820, val loss: 1.5913684368133545
Epoch 1830, training loss: 311.785888671875 = 0.039347000420093536 + 50.0 * 6.234930992126465
Epoch 1830, val loss: 1.5976570844650269
Epoch 1840, training loss: 311.8462219238281 = 0.03861625865101814 + 50.0 * 6.236152172088623
Epoch 1840, val loss: 1.6037918329238892
Epoch 1850, training loss: 311.9892883300781 = 0.037920642644166946 + 50.0 * 6.23902702331543
Epoch 1850, val loss: 1.60969877243042
Epoch 1860, training loss: 311.8665466308594 = 0.037196651101112366 + 50.0 * 6.236587047576904
Epoch 1860, val loss: 1.6154271364212036
Epoch 1870, training loss: 311.7413635253906 = 0.03651583939790726 + 50.0 * 6.234096527099609
Epoch 1870, val loss: 1.6214878559112549
Epoch 1880, training loss: 311.7274169921875 = 0.035862408578395844 + 50.0 * 6.23383092880249
Epoch 1880, val loss: 1.6276671886444092
Epoch 1890, training loss: 312.055908203125 = 0.03523850440979004 + 50.0 * 6.240413188934326
Epoch 1890, val loss: 1.6337615251541138
Epoch 1900, training loss: 311.92852783203125 = 0.034580402076244354 + 50.0 * 6.237879276275635
Epoch 1900, val loss: 1.6390643119812012
Epoch 1910, training loss: 311.7164306640625 = 0.03395354375243187 + 50.0 * 6.233649253845215
Epoch 1910, val loss: 1.6449151039123535
Epoch 1920, training loss: 311.66064453125 = 0.033356644213199615 + 50.0 * 6.232545852661133
Epoch 1920, val loss: 1.6508938074111938
Epoch 1930, training loss: 311.6375732421875 = 0.03277873992919922 + 50.0 * 6.232095718383789
Epoch 1930, val loss: 1.6566294431686401
Epoch 1940, training loss: 311.8133544921875 = 0.032218966633081436 + 50.0 * 6.235622406005859
Epoch 1940, val loss: 1.6623672246932983
Epoch 1950, training loss: 311.9173583984375 = 0.03165934234857559 + 50.0 * 6.2377142906188965
Epoch 1950, val loss: 1.6678506135940552
Epoch 1960, training loss: 311.74212646484375 = 0.03109346702694893 + 50.0 * 6.234220504760742
Epoch 1960, val loss: 1.6732176542282104
Epoch 1970, training loss: 311.60858154296875 = 0.030561333522200584 + 50.0 * 6.231560707092285
Epoch 1970, val loss: 1.6790869235992432
Epoch 1980, training loss: 311.5792541503906 = 0.030048560351133347 + 50.0 * 6.230983734130859
Epoch 1980, val loss: 1.6846388578414917
Epoch 1990, training loss: 312.1670227050781 = 0.029556334018707275 + 50.0 * 6.242748737335205
Epoch 1990, val loss: 1.6905497312545776
Epoch 2000, training loss: 311.7942810058594 = 0.029049571603536606 + 50.0 * 6.235304355621338
Epoch 2000, val loss: 1.6950769424438477
Epoch 2010, training loss: 311.5641784667969 = 0.028554130345582962 + 50.0 * 6.230712413787842
Epoch 2010, val loss: 1.700755000114441
Epoch 2020, training loss: 311.517822265625 = 0.028088731691241264 + 50.0 * 6.229794979095459
Epoch 2020, val loss: 1.7062841653823853
Epoch 2030, training loss: 311.50762939453125 = 0.02763758972287178 + 50.0 * 6.229599475860596
Epoch 2030, val loss: 1.7115405797958374
Epoch 2040, training loss: 312.1332092285156 = 0.027211077511310577 + 50.0 * 6.242119789123535
Epoch 2040, val loss: 1.717116117477417
Epoch 2050, training loss: 311.7727355957031 = 0.02674724906682968 + 50.0 * 6.234920024871826
Epoch 2050, val loss: 1.7216064929962158
Epoch 2060, training loss: 311.51055908203125 = 0.026306908577680588 + 50.0 * 6.229684829711914
Epoch 2060, val loss: 1.7269259691238403
Epoch 2070, training loss: 311.5742492675781 = 0.025894099846482277 + 50.0 * 6.2309675216674805
Epoch 2070, val loss: 1.7323271036148071
Epoch 2080, training loss: 311.5357971191406 = 0.025480685755610466 + 50.0 * 6.23020601272583
Epoch 2080, val loss: 1.7373099327087402
Epoch 2090, training loss: 311.4906921386719 = 0.02507200837135315 + 50.0 * 6.229311943054199
Epoch 2090, val loss: 1.7423696517944336
Epoch 2100, training loss: 311.5474853515625 = 0.024688800796866417 + 50.0 * 6.2304558753967285
Epoch 2100, val loss: 1.7475212812423706
Epoch 2110, training loss: 311.55194091796875 = 0.024308031424880028 + 50.0 * 6.2305521965026855
Epoch 2110, val loss: 1.7524272203445435
Epoch 2120, training loss: 311.4337463378906 = 0.023927079513669014 + 50.0 * 6.228196144104004
Epoch 2120, val loss: 1.7575325965881348
Epoch 2130, training loss: 311.4079284667969 = 0.02355693094432354 + 50.0 * 6.227687358856201
Epoch 2130, val loss: 1.7624931335449219
Epoch 2140, training loss: 311.4375915527344 = 0.023201758041977882 + 50.0 * 6.228287696838379
Epoch 2140, val loss: 1.7674927711486816
Epoch 2150, training loss: 311.5362243652344 = 0.02285590022802353 + 50.0 * 6.230267524719238
Epoch 2150, val loss: 1.7723182439804077
Epoch 2160, training loss: 311.3692321777344 = 0.02251167595386505 + 50.0 * 6.226934432983398
Epoch 2160, val loss: 1.7773075103759766
Epoch 2170, training loss: 311.40765380859375 = 0.022186655551195145 + 50.0 * 6.2277092933654785
Epoch 2170, val loss: 1.7821667194366455
Epoch 2180, training loss: 311.7386474609375 = 0.021855417639017105 + 50.0 * 6.234335899353027
Epoch 2180, val loss: 1.786722183227539
Epoch 2190, training loss: 311.3981018066406 = 0.021521858870983124 + 50.0 * 6.227531909942627
Epoch 2190, val loss: 1.7912315130233765
Epoch 2200, training loss: 311.3976745605469 = 0.021206403151154518 + 50.0 * 6.227529525756836
Epoch 2200, val loss: 1.796044111251831
Epoch 2210, training loss: 311.364013671875 = 0.020899785682559013 + 50.0 * 6.22686243057251
Epoch 2210, val loss: 1.8006941080093384
Epoch 2220, training loss: 311.3173522949219 = 0.02059737779200077 + 50.0 * 6.225934982299805
Epoch 2220, val loss: 1.8054912090301514
Epoch 2230, training loss: 311.3959655761719 = 0.020312344655394554 + 50.0 * 6.227512836456299
Epoch 2230, val loss: 1.8101327419281006
Epoch 2240, training loss: 311.40289306640625 = 0.020027682185173035 + 50.0 * 6.227657318115234
Epoch 2240, val loss: 1.8146353960037231
Epoch 2250, training loss: 311.3352966308594 = 0.01973572000861168 + 50.0 * 6.226310729980469
Epoch 2250, val loss: 1.8192449808120728
Epoch 2260, training loss: 311.3411865234375 = 0.019459126517176628 + 50.0 * 6.226434230804443
Epoch 2260, val loss: 1.8236340284347534
Epoch 2270, training loss: 311.5318908691406 = 0.019193097949028015 + 50.0 * 6.23025369644165
Epoch 2270, val loss: 1.8280850648880005
Epoch 2280, training loss: 311.3606262207031 = 0.01892528310418129 + 50.0 * 6.226834297180176
Epoch 2280, val loss: 1.8325855731964111
Epoch 2290, training loss: 311.2701110839844 = 0.018655728548765182 + 50.0 * 6.225028991699219
Epoch 2290, val loss: 1.836713433265686
Epoch 2300, training loss: 311.2045593261719 = 0.01840398460626602 + 50.0 * 6.223723411560059
Epoch 2300, val loss: 1.8414454460144043
Epoch 2310, training loss: 311.3056945800781 = 0.018160194158554077 + 50.0 * 6.22575044631958
Epoch 2310, val loss: 1.8459151983261108
Epoch 2320, training loss: 311.362548828125 = 0.017916999757289886 + 50.0 * 6.226892471313477
Epoch 2320, val loss: 1.8498539924621582
Epoch 2330, training loss: 311.24517822265625 = 0.017672456800937653 + 50.0 * 6.224550247192383
Epoch 2330, val loss: 1.8541061878204346
Epoch 2340, training loss: 311.177978515625 = 0.017436420544981956 + 50.0 * 6.22321081161499
Epoch 2340, val loss: 1.8584367036819458
Epoch 2350, training loss: 311.1589050292969 = 0.017212362959980965 + 50.0 * 6.222833633422852
Epoch 2350, val loss: 1.8626905679702759
Epoch 2360, training loss: 311.7104187011719 = 0.017008135095238686 + 50.0 * 6.233868598937988
Epoch 2360, val loss: 1.8665997982025146
Epoch 2370, training loss: 311.4023132324219 = 0.01675582490861416 + 50.0 * 6.227711200714111
Epoch 2370, val loss: 1.8704607486724854
Epoch 2380, training loss: 311.1445007324219 = 0.016534773632884026 + 50.0 * 6.222559452056885
Epoch 2380, val loss: 1.8744593858718872
Epoch 2390, training loss: 311.1039733886719 = 0.016324777156114578 + 50.0 * 6.221753120422363
Epoch 2390, val loss: 1.8786193132400513
Epoch 2400, training loss: 311.10223388671875 = 0.016121486201882362 + 50.0 * 6.22172212600708
Epoch 2400, val loss: 1.8829882144927979
Epoch 2410, training loss: 311.3974914550781 = 0.015933606773614883 + 50.0 * 6.227631092071533
Epoch 2410, val loss: 1.8871873617172241
Epoch 2420, training loss: 311.2958679199219 = 0.015717819333076477 + 50.0 * 6.225603103637695
Epoch 2420, val loss: 1.8905237913131714
Epoch 2430, training loss: 311.2091369628906 = 0.015520218759775162 + 50.0 * 6.223872184753418
Epoch 2430, val loss: 1.894577145576477
Epoch 2440, training loss: 311.1133728027344 = 0.015323242172598839 + 50.0 * 6.22196102142334
Epoch 2440, val loss: 1.8982903957366943
Epoch 2450, training loss: 311.06451416015625 = 0.015139510855078697 + 50.0 * 6.220987796783447
Epoch 2450, val loss: 1.9022811651229858
Epoch 2460, training loss: 311.0888366699219 = 0.014955451712012291 + 50.0 * 6.221477508544922
Epoch 2460, val loss: 1.906275749206543
Epoch 2470, training loss: 311.3172912597656 = 0.014781870879232883 + 50.0 * 6.22605037689209
Epoch 2470, val loss: 1.9100215435028076
Epoch 2480, training loss: 311.17962646484375 = 0.014590871520340443 + 50.0 * 6.223300933837891
Epoch 2480, val loss: 1.9135717153549194
Epoch 2490, training loss: 311.1045837402344 = 0.014414630830287933 + 50.0 * 6.221803665161133
Epoch 2490, val loss: 1.9173780679702759
Epoch 2500, training loss: 311.1806945800781 = 0.014240953139960766 + 50.0 * 6.223329067230225
Epoch 2500, val loss: 1.9210275411605835
Epoch 2510, training loss: 311.037109375 = 0.014069836586713791 + 50.0 * 6.220460891723633
Epoch 2510, val loss: 1.92446768283844
Epoch 2520, training loss: 311.2974548339844 = 0.013905049301683903 + 50.0 * 6.22567081451416
Epoch 2520, val loss: 1.9279624223709106
Epoch 2530, training loss: 311.04779052734375 = 0.01373674999922514 + 50.0 * 6.220681190490723
Epoch 2530, val loss: 1.9319044351577759
Epoch 2540, training loss: 310.99859619140625 = 0.013577686622738838 + 50.0 * 6.219700336456299
Epoch 2540, val loss: 1.9353615045547485
Epoch 2550, training loss: 310.9582824707031 = 0.013424555771052837 + 50.0 * 6.218896865844727
Epoch 2550, val loss: 1.9391974210739136
Epoch 2560, training loss: 311.0589904785156 = 0.01327789667993784 + 50.0 * 6.220914363861084
Epoch 2560, val loss: 1.9425954818725586
Epoch 2570, training loss: 311.0858459472656 = 0.013126734644174576 + 50.0 * 6.221454620361328
Epoch 2570, val loss: 1.9459863901138306
Epoch 2580, training loss: 311.119140625 = 0.012972827069461346 + 50.0 * 6.222123622894287
Epoch 2580, val loss: 1.9496283531188965
Epoch 2590, training loss: 311.04248046875 = 0.012820717878639698 + 50.0 * 6.220593452453613
Epoch 2590, val loss: 1.952784776687622
Epoch 2600, training loss: 310.9445495605469 = 0.012675106525421143 + 50.0 * 6.218637466430664
Epoch 2600, val loss: 1.9564130306243896
Epoch 2610, training loss: 310.9266662597656 = 0.012538029812276363 + 50.0 * 6.218282699584961
Epoch 2610, val loss: 1.9599268436431885
Epoch 2620, training loss: 311.0242919921875 = 0.012403702363371849 + 50.0 * 6.220237731933594
Epoch 2620, val loss: 1.9633516073226929
Epoch 2630, training loss: 311.1369323730469 = 0.01226872019469738 + 50.0 * 6.2224931716918945
Epoch 2630, val loss: 1.9667325019836426
Epoch 2640, training loss: 310.9242248535156 = 0.012123947963118553 + 50.0 * 6.218242168426514
Epoch 2640, val loss: 1.9699368476867676
Epoch 2650, training loss: 310.8758544921875 = 0.01199163869023323 + 50.0 * 6.2172770500183105
Epoch 2650, val loss: 1.9731934070587158
Epoch 2660, training loss: 310.85516357421875 = 0.011863702908158302 + 50.0 * 6.2168660163879395
Epoch 2660, val loss: 1.9767440557479858
Epoch 2670, training loss: 311.20587158203125 = 0.011741705238819122 + 50.0 * 6.22388219833374
Epoch 2670, val loss: 1.980154275894165
Epoch 2680, training loss: 310.90435791015625 = 0.011611327528953552 + 50.0 * 6.217855453491211
Epoch 2680, val loss: 1.982771635055542
Epoch 2690, training loss: 310.888671875 = 0.01148456335067749 + 50.0 * 6.217544078826904
Epoch 2690, val loss: 1.986074447631836
Epoch 2700, training loss: 310.9029846191406 = 0.011367353610694408 + 50.0 * 6.217832565307617
Epoch 2700, val loss: 1.9891972541809082
Epoch 2710, training loss: 310.9044494628906 = 0.011249319650232792 + 50.0 * 6.217864036560059
Epoch 2710, val loss: 1.9925744533538818
Epoch 2720, training loss: 310.8302307128906 = 0.011132985353469849 + 50.0 * 6.216381549835205
Epoch 2720, val loss: 1.9958254098892212
Epoch 2730, training loss: 310.8976745605469 = 0.011023517698049545 + 50.0 * 6.217732906341553
Epoch 2730, val loss: 1.999109148979187
Epoch 2740, training loss: 310.84686279296875 = 0.010908334515988827 + 50.0 * 6.216719627380371
Epoch 2740, val loss: 2.002079725265503
Epoch 2750, training loss: 310.8794860839844 = 0.010798182338476181 + 50.0 * 6.217373847961426
Epoch 2750, val loss: 2.004929780960083
Epoch 2760, training loss: 310.94659423828125 = 0.010690254159271717 + 50.0 * 6.2187180519104
Epoch 2760, val loss: 2.0082144737243652
Epoch 2770, training loss: 310.85302734375 = 0.010576955042779446 + 50.0 * 6.216848850250244
Epoch 2770, val loss: 2.0110456943511963
Epoch 2780, training loss: 310.83563232421875 = 0.01047073770314455 + 50.0 * 6.216503143310547
Epoch 2780, val loss: 2.0138282775878906
Epoch 2790, training loss: 311.0833435058594 = 0.010371963493525982 + 50.0 * 6.22145938873291
Epoch 2790, val loss: 2.0169191360473633
Epoch 2800, training loss: 310.87109375 = 0.010267925448715687 + 50.0 * 6.217216491699219
Epoch 2800, val loss: 2.0196521282196045
Epoch 2810, training loss: 310.7756042480469 = 0.01016218587756157 + 50.0 * 6.215308666229248
Epoch 2810, val loss: 2.0226569175720215
Epoch 2820, training loss: 310.7227783203125 = 0.010064610280096531 + 50.0 * 6.214254379272461
Epoch 2820, val loss: 2.0257372856140137
Epoch 2830, training loss: 310.7248840332031 = 0.009969910606741905 + 50.0 * 6.214298248291016
Epoch 2830, val loss: 2.0288829803466797
Epoch 2840, training loss: 311.07647705078125 = 0.009880139492452145 + 50.0 * 6.22133207321167
Epoch 2840, val loss: 2.0318853855133057
Epoch 2850, training loss: 310.7802734375 = 0.009782910346984863 + 50.0 * 6.215409278869629
Epoch 2850, val loss: 2.0340466499328613
Epoch 2860, training loss: 310.7484130859375 = 0.009687074460089207 + 50.0 * 6.2147746086120605
Epoch 2860, val loss: 2.0370731353759766
Epoch 2870, training loss: 310.8598327636719 = 0.0095996567979455 + 50.0 * 6.217004299163818
Epoch 2870, val loss: 2.0398952960968018
Epoch 2880, training loss: 310.73529052734375 = 0.009507386945188046 + 50.0 * 6.214515686035156
Epoch 2880, val loss: 2.0426692962646484
Epoch 2890, training loss: 310.7569580078125 = 0.009422516450285912 + 50.0 * 6.2149505615234375
Epoch 2890, val loss: 2.045640468597412
Epoch 2900, training loss: 310.8342590332031 = 0.009337260387837887 + 50.0 * 6.216498374938965
Epoch 2900, val loss: 2.0484976768493652
Epoch 2910, training loss: 310.76824951171875 = 0.009251636452972889 + 50.0 * 6.215179920196533
Epoch 2910, val loss: 2.0506844520568848
Epoch 2920, training loss: 310.69183349609375 = 0.009162040427327156 + 50.0 * 6.213653564453125
Epoch 2920, val loss: 2.053597927093506
Epoch 2930, training loss: 310.6881103515625 = 0.009081846103072166 + 50.0 * 6.213580131530762
Epoch 2930, val loss: 2.056485891342163
Epoch 2940, training loss: 310.99090576171875 = 0.009007034823298454 + 50.0 * 6.219637870788574
Epoch 2940, val loss: 2.058800458908081
Epoch 2950, training loss: 310.7939758300781 = 0.008918315172195435 + 50.0 * 6.215701103210449
Epoch 2950, val loss: 2.0615177154541016
Epoch 2960, training loss: 310.69427490234375 = 0.00883388053625822 + 50.0 * 6.213708877563477
Epoch 2960, val loss: 2.06384539604187
Epoch 2970, training loss: 310.6120910644531 = 0.008756771683692932 + 50.0 * 6.212066650390625
Epoch 2970, val loss: 2.066706895828247
Epoch 2980, training loss: 310.6014404296875 = 0.008683545514941216 + 50.0 * 6.211854934692383
Epoch 2980, val loss: 2.0695250034332275
Epoch 2990, training loss: 310.5937805175781 = 0.008611948229372501 + 50.0 * 6.211703300476074
Epoch 2990, val loss: 2.0722503662109375
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8165524512387982
=== training gcn model ===
Epoch 0, training loss: 431.7890930175781 = 1.9459577798843384 + 50.0 * 8.59686279296875
Epoch 0, val loss: 1.9389731884002686
Epoch 10, training loss: 431.7503967285156 = 1.9370741844177246 + 50.0 * 8.596266746520996
Epoch 10, val loss: 1.9306877851486206
Epoch 20, training loss: 431.5362243652344 = 1.9264949560165405 + 50.0 * 8.592194557189941
Epoch 20, val loss: 1.9204601049423218
Epoch 30, training loss: 430.2177429199219 = 1.913219928741455 + 50.0 * 8.56609058380127
Epoch 30, val loss: 1.9073327779769897
Epoch 40, training loss: 423.56060791015625 = 1.8962255716323853 + 50.0 * 8.433287620544434
Epoch 40, val loss: 1.8907527923583984
Epoch 50, training loss: 399.5506286621094 = 1.877244234085083 + 50.0 * 7.95346736907959
Epoch 50, val loss: 1.8722933530807495
Epoch 60, training loss: 380.6642150878906 = 1.859745979309082 + 50.0 * 7.576089382171631
Epoch 60, val loss: 1.8557803630828857
Epoch 70, training loss: 367.8700256347656 = 1.8472102880477905 + 50.0 * 7.320456504821777
Epoch 70, val loss: 1.8439559936523438
Epoch 80, training loss: 356.32550048828125 = 1.8354371786117554 + 50.0 * 7.089800834655762
Epoch 80, val loss: 1.8327780961990356
Epoch 90, training loss: 349.1197204589844 = 1.8244599103927612 + 50.0 * 6.9459052085876465
Epoch 90, val loss: 1.8229904174804688
Epoch 100, training loss: 343.8583984375 = 1.8145713806152344 + 50.0 * 6.840876579284668
Epoch 100, val loss: 1.8136190176010132
Epoch 110, training loss: 339.6630554199219 = 1.8044477701187134 + 50.0 * 6.757172107696533
Epoch 110, val loss: 1.8041822910308838
Epoch 120, training loss: 336.2954406738281 = 1.794480800628662 + 50.0 * 6.690019130706787
Epoch 120, val loss: 1.7948464155197144
Epoch 130, training loss: 333.8072204589844 = 1.7847654819488525 + 50.0 * 6.640449523925781
Epoch 130, val loss: 1.7856523990631104
Epoch 140, training loss: 332.0507507324219 = 1.7744766473770142 + 50.0 * 6.605525493621826
Epoch 140, val loss: 1.7759757041931152
Epoch 150, training loss: 330.6387634277344 = 1.7634143829345703 + 50.0 * 6.5775065422058105
Epoch 150, val loss: 1.7657383680343628
Epoch 160, training loss: 329.5281982421875 = 1.7515872716903687 + 50.0 * 6.555532455444336
Epoch 160, val loss: 1.7548552751541138
Epoch 170, training loss: 328.6189880371094 = 1.7388721704483032 + 50.0 * 6.537602424621582
Epoch 170, val loss: 1.743256688117981
Epoch 180, training loss: 327.9368896484375 = 1.725189447402954 + 50.0 * 6.524233818054199
Epoch 180, val loss: 1.730830192565918
Epoch 190, training loss: 327.0613098144531 = 1.7103503942489624 + 50.0 * 6.50701904296875
Epoch 190, val loss: 1.7174493074417114
Epoch 200, training loss: 326.3212890625 = 1.6943132877349854 + 50.0 * 6.492539882659912
Epoch 200, val loss: 1.7031439542770386
Epoch 210, training loss: 325.7422180175781 = 1.677039623260498 + 50.0 * 6.481303691864014
Epoch 210, val loss: 1.6877636909484863
Epoch 220, training loss: 325.0882873535156 = 1.6584521532058716 + 50.0 * 6.468596935272217
Epoch 220, val loss: 1.6713199615478516
Epoch 230, training loss: 324.55706787109375 = 1.6385672092437744 + 50.0 * 6.458369731903076
Epoch 230, val loss: 1.6537529230117798
Epoch 240, training loss: 324.18548583984375 = 1.6173077821731567 + 50.0 * 6.451363563537598
Epoch 240, val loss: 1.6351064443588257
Epoch 250, training loss: 323.7177734375 = 1.5948255062103271 + 50.0 * 6.4424591064453125
Epoch 250, val loss: 1.6153563261032104
Epoch 260, training loss: 323.2931213378906 = 1.5711971521377563 + 50.0 * 6.434438228607178
Epoch 260, val loss: 1.5947972536087036
Epoch 270, training loss: 323.1981506347656 = 1.5466269254684448 + 50.0 * 6.433030128479004
Epoch 270, val loss: 1.5735729932785034
Epoch 280, training loss: 322.58575439453125 = 1.5209895372390747 + 50.0 * 6.421295166015625
Epoch 280, val loss: 1.5516314506530762
Epoch 290, training loss: 322.2417297363281 = 1.4947458505630493 + 50.0 * 6.4149394035339355
Epoch 290, val loss: 1.5294197797775269
Epoch 300, training loss: 321.9763488769531 = 1.4679546356201172 + 50.0 * 6.410167694091797
Epoch 300, val loss: 1.5070395469665527
Epoch 310, training loss: 321.6293029785156 = 1.4407715797424316 + 50.0 * 6.403770446777344
Epoch 310, val loss: 1.484597086906433
Epoch 320, training loss: 321.49591064453125 = 1.4132176637649536 + 50.0 * 6.401654243469238
Epoch 320, val loss: 1.4621623754501343
Epoch 330, training loss: 321.09344482421875 = 1.385435938835144 + 50.0 * 6.394160270690918
Epoch 330, val loss: 1.4398036003112793
Epoch 340, training loss: 320.7574768066406 = 1.3575732707977295 + 50.0 * 6.387998104095459
Epoch 340, val loss: 1.4177203178405762
Epoch 350, training loss: 320.5225830078125 = 1.3296947479248047 + 50.0 * 6.3838582038879395
Epoch 350, val loss: 1.3959071636199951
Epoch 360, training loss: 320.3809814453125 = 1.3016414642333984 + 50.0 * 6.381587028503418
Epoch 360, val loss: 1.3742986917495728
Epoch 370, training loss: 320.006103515625 = 1.2735497951507568 + 50.0 * 6.374650955200195
Epoch 370, val loss: 1.3529133796691895
Epoch 380, training loss: 319.75396728515625 = 1.2455500364303589 + 50.0 * 6.370168209075928
Epoch 380, val loss: 1.3318694829940796
Epoch 390, training loss: 319.6647033691406 = 1.2175772190093994 + 50.0 * 6.3689422607421875
Epoch 390, val loss: 1.31111478805542
Epoch 400, training loss: 319.3139343261719 = 1.1897705793380737 + 50.0 * 6.362483501434326
Epoch 400, val loss: 1.2906814813613892
Epoch 410, training loss: 319.15863037109375 = 1.1621261835098267 + 50.0 * 6.35992956161499
Epoch 410, val loss: 1.2706986665725708
Epoch 420, training loss: 318.898681640625 = 1.134652018547058 + 50.0 * 6.355280876159668
Epoch 420, val loss: 1.250966191291809
Epoch 430, training loss: 318.7474060058594 = 1.1075373888015747 + 50.0 * 6.352797031402588
Epoch 430, val loss: 1.2317136526107788
Epoch 440, training loss: 318.6314392089844 = 1.0808485746383667 + 50.0 * 6.351012229919434
Epoch 440, val loss: 1.2131060361862183
Epoch 450, training loss: 318.432861328125 = 1.0545742511749268 + 50.0 * 6.3475661277771
Epoch 450, val loss: 1.1947309970855713
Epoch 460, training loss: 318.21820068359375 = 1.0288286209106445 + 50.0 * 6.34378719329834
Epoch 460, val loss: 1.1772178411483765
Epoch 470, training loss: 318.0361022949219 = 1.0038152933120728 + 50.0 * 6.340645790100098
Epoch 470, val loss: 1.1603977680206299
Epoch 480, training loss: 317.90447998046875 = 0.9793625473976135 + 50.0 * 6.338501930236816
Epoch 480, val loss: 1.14414381980896
Epoch 490, training loss: 317.8337707519531 = 0.9555425047874451 + 50.0 * 6.337564468383789
Epoch 490, val loss: 1.128424882888794
Epoch 500, training loss: 317.6007080078125 = 0.9324451684951782 + 50.0 * 6.333365440368652
Epoch 500, val loss: 1.1136356592178345
Epoch 510, training loss: 317.43084716796875 = 0.9101113677024841 + 50.0 * 6.330414295196533
Epoch 510, val loss: 1.0994905233383179
Epoch 520, training loss: 317.6264343261719 = 0.8883786201477051 + 50.0 * 6.334761142730713
Epoch 520, val loss: 1.0859984159469604
Epoch 530, training loss: 317.1951904296875 = 0.8672310709953308 + 50.0 * 6.326559543609619
Epoch 530, val loss: 1.0730012655258179
Epoch 540, training loss: 317.06732177734375 = 0.8468149304389954 + 50.0 * 6.324410438537598
Epoch 540, val loss: 1.0607901811599731
Epoch 550, training loss: 316.8894348144531 = 0.8270794153213501 + 50.0 * 6.321247100830078
Epoch 550, val loss: 1.0494154691696167
Epoch 560, training loss: 317.2339782714844 = 0.8078914284706116 + 50.0 * 6.328521728515625
Epoch 560, val loss: 1.0385152101516724
Epoch 570, training loss: 316.80096435546875 = 0.7892467975616455 + 50.0 * 6.320234298706055
Epoch 570, val loss: 1.0282584428787231
Epoch 580, training loss: 316.6291198730469 = 0.7712303996086121 + 50.0 * 6.317157745361328
Epoch 580, val loss: 1.0185999870300293
Epoch 590, training loss: 316.4476013183594 = 0.7538580894470215 + 50.0 * 6.313875198364258
Epoch 590, val loss: 1.0097774267196655
Epoch 600, training loss: 316.4013671875 = 0.7370274066925049 + 50.0 * 6.313286781311035
Epoch 600, val loss: 1.0015697479248047
Epoch 610, training loss: 316.3854675292969 = 0.7206538319587708 + 50.0 * 6.313296794891357
Epoch 610, val loss: 0.9939775466918945
Epoch 620, training loss: 316.24188232421875 = 0.7046965956687927 + 50.0 * 6.310743808746338
Epoch 620, val loss: 0.9868846535682678
Epoch 630, training loss: 316.11834716796875 = 0.6892567276954651 + 50.0 * 6.308581352233887
Epoch 630, val loss: 0.9804626703262329
Epoch 640, training loss: 315.9228820800781 = 0.6742731332778931 + 50.0 * 6.304972171783447
Epoch 640, val loss: 0.9745237231254578
Epoch 650, training loss: 315.87060546875 = 0.6597334146499634 + 50.0 * 6.30421781539917
Epoch 650, val loss: 0.969171941280365
Epoch 660, training loss: 315.8829345703125 = 0.645537793636322 + 50.0 * 6.304748058319092
Epoch 660, val loss: 0.9643984436988831
Epoch 670, training loss: 315.72039794921875 = 0.6316447854042053 + 50.0 * 6.301774978637695
Epoch 670, val loss: 0.959750235080719
Epoch 680, training loss: 315.6030578613281 = 0.6181044578552246 + 50.0 * 6.299699306488037
Epoch 680, val loss: 0.9560633897781372
Epoch 690, training loss: 315.4917297363281 = 0.6048798561096191 + 50.0 * 6.297736644744873
Epoch 690, val loss: 0.952438235282898
Epoch 700, training loss: 315.72235107421875 = 0.5918912887573242 + 50.0 * 6.302608966827393
Epoch 700, val loss: 0.949170708656311
Epoch 710, training loss: 315.3970642089844 = 0.5790694355964661 + 50.0 * 6.296360015869141
Epoch 710, val loss: 0.9463452696800232
Epoch 720, training loss: 315.2550354003906 = 0.5664921998977661 + 50.0 * 6.293770790100098
Epoch 720, val loss: 0.9439126253128052
Epoch 730, training loss: 315.1925354003906 = 0.5541417598724365 + 50.0 * 6.2927680015563965
Epoch 730, val loss: 0.9415983557701111
Epoch 740, training loss: 315.3200988769531 = 0.5418936014175415 + 50.0 * 6.2955641746521
Epoch 740, val loss: 0.9395222067832947
Epoch 750, training loss: 315.0605773925781 = 0.5297738313674927 + 50.0 * 6.290616035461426
Epoch 750, val loss: 0.9377542734146118
Epoch 760, training loss: 315.0610046386719 = 0.5178041458129883 + 50.0 * 6.290863990783691
Epoch 760, val loss: 0.9361072778701782
Epoch 770, training loss: 314.8760986328125 = 0.5058897137641907 + 50.0 * 6.2874040603637695
Epoch 770, val loss: 0.9348728656768799
Epoch 780, training loss: 314.8082275390625 = 0.4941065013408661 + 50.0 * 6.286282539367676
Epoch 780, val loss: 0.9335885643959045
Epoch 790, training loss: 314.9739990234375 = 0.4824633300304413 + 50.0 * 6.289830684661865
Epoch 790, val loss: 0.9326371550559998
Epoch 800, training loss: 314.8183288574219 = 0.47073349356651306 + 50.0 * 6.286952018737793
Epoch 800, val loss: 0.9314831495285034
Epoch 810, training loss: 314.67822265625 = 0.4591731131076813 + 50.0 * 6.28438138961792
Epoch 810, val loss: 0.9306015968322754
Epoch 820, training loss: 314.5435791015625 = 0.4476641118526459 + 50.0 * 6.281918525695801
Epoch 820, val loss: 0.9299398064613342
Epoch 830, training loss: 314.55450439453125 = 0.43626391887664795 + 50.0 * 6.282364845275879
Epoch 830, val loss: 0.9294840097427368
Epoch 840, training loss: 314.5513000488281 = 0.4248709976673126 + 50.0 * 6.282528400421143
Epoch 840, val loss: 0.9290782809257507
Epoch 850, training loss: 314.40057373046875 = 0.413533091545105 + 50.0 * 6.279740333557129
Epoch 850, val loss: 0.9285472631454468
Epoch 860, training loss: 314.302734375 = 0.4023161232471466 + 50.0 * 6.278008460998535
Epoch 860, val loss: 0.9283861517906189
Epoch 870, training loss: 314.26361083984375 = 0.39123374223709106 + 50.0 * 6.277447700500488
Epoch 870, val loss: 0.9283350706100464
Epoch 880, training loss: 314.4671630859375 = 0.3802183270454407 + 50.0 * 6.281738758087158
Epoch 880, val loss: 0.9286739826202393
Epoch 890, training loss: 314.1610107421875 = 0.3693036437034607 + 50.0 * 6.275834083557129
Epoch 890, val loss: 0.9283103942871094
Epoch 900, training loss: 314.0701904296875 = 0.35858070850372314 + 50.0 * 6.274232387542725
Epoch 900, val loss: 0.9291101098060608
Epoch 910, training loss: 313.982177734375 = 0.34803029894828796 + 50.0 * 6.272683143615723
Epoch 910, val loss: 0.9295810461044312
Epoch 920, training loss: 314.4849853515625 = 0.3376922011375427 + 50.0 * 6.2829461097717285
Epoch 920, val loss: 0.9302315711975098
Epoch 930, training loss: 314.10675048828125 = 0.32735204696655273 + 50.0 * 6.275588035583496
Epoch 930, val loss: 0.9308697581291199
Epoch 940, training loss: 313.8827209472656 = 0.3172927796840668 + 50.0 * 6.271308898925781
Epoch 940, val loss: 0.9319443702697754
Epoch 950, training loss: 313.77294921875 = 0.30749019980430603 + 50.0 * 6.269309043884277
Epoch 950, val loss: 0.9332557320594788
Epoch 960, training loss: 313.847412109375 = 0.29793503880500793 + 50.0 * 6.270989418029785
Epoch 960, val loss: 0.934699535369873
Epoch 970, training loss: 313.7581481933594 = 0.28848031163215637 + 50.0 * 6.269393444061279
Epoch 970, val loss: 0.936048686504364
Epoch 980, training loss: 313.7463684082031 = 0.27931472659111023 + 50.0 * 6.269340515136719
Epoch 980, val loss: 0.9379693865776062
Epoch 990, training loss: 313.6319580078125 = 0.2704029977321625 + 50.0 * 6.267230987548828
Epoch 990, val loss: 0.9401748180389404
Epoch 1000, training loss: 313.7741394042969 = 0.2617548108100891 + 50.0 * 6.270247936248779
Epoch 1000, val loss: 0.9424939155578613
Epoch 1010, training loss: 313.6747131347656 = 0.2533380389213562 + 50.0 * 6.268427848815918
Epoch 1010, val loss: 0.9444672465324402
Epoch 1020, training loss: 313.5639953613281 = 0.24517332017421722 + 50.0 * 6.266376495361328
Epoch 1020, val loss: 0.9475802779197693
Epoch 1030, training loss: 313.4402160644531 = 0.2372780293226242 + 50.0 * 6.264059066772461
Epoch 1030, val loss: 0.9503591060638428
Epoch 1040, training loss: 313.4827575683594 = 0.2296472191810608 + 50.0 * 6.26506233215332
Epoch 1040, val loss: 0.9537131786346436
Epoch 1050, training loss: 313.43597412109375 = 0.22223757207393646 + 50.0 * 6.264274597167969
Epoch 1050, val loss: 0.9570394158363342
Epoch 1060, training loss: 313.4820251464844 = 0.21505147218704224 + 50.0 * 6.265339374542236
Epoch 1060, val loss: 0.9604160785675049
Epoch 1070, training loss: 313.5407409667969 = 0.20810213685035706 + 50.0 * 6.266652584075928
Epoch 1070, val loss: 0.9636794328689575
Epoch 1080, training loss: 313.297119140625 = 0.20137570798397064 + 50.0 * 6.2619147300720215
Epoch 1080, val loss: 0.9677682518959045
Epoch 1090, training loss: 313.2019958496094 = 0.1949177235364914 + 50.0 * 6.260141849517822
Epoch 1090, val loss: 0.9716376662254333
Epoch 1100, training loss: 313.171875 = 0.188718780875206 + 50.0 * 6.259663105010986
Epoch 1100, val loss: 0.9758875966072083
Epoch 1110, training loss: 313.3280334472656 = 0.18273228406906128 + 50.0 * 6.262905597686768
Epoch 1110, val loss: 0.9801777005195618
Epoch 1120, training loss: 313.3025817871094 = 0.1769089251756668 + 50.0 * 6.262513637542725
Epoch 1120, val loss: 0.9846680760383606
Epoch 1130, training loss: 313.1199035644531 = 0.17128588259220123 + 50.0 * 6.25897216796875
Epoch 1130, val loss: 0.9887316823005676
Epoch 1140, training loss: 313.33135986328125 = 0.1659061461687088 + 50.0 * 6.263309478759766
Epoch 1140, val loss: 0.9937470555305481
Epoch 1150, training loss: 313.0899658203125 = 0.16071239113807678 + 50.0 * 6.258585453033447
Epoch 1150, val loss: 0.9981914162635803
Epoch 1160, training loss: 312.9798889160156 = 0.15570765733718872 + 50.0 * 6.256484031677246
Epoch 1160, val loss: 1.003144383430481
Epoch 1170, training loss: 313.04254150390625 = 0.15091979503631592 + 50.0 * 6.2578325271606445
Epoch 1170, val loss: 1.0082142353057861
Epoch 1180, training loss: 312.88262939453125 = 0.14627337455749512 + 50.0 * 6.254726886749268
Epoch 1180, val loss: 1.013139247894287
Epoch 1190, training loss: 313.0283508300781 = 0.14182381331920624 + 50.0 * 6.257730960845947
Epoch 1190, val loss: 1.0184838771820068
Epoch 1200, training loss: 312.8751525878906 = 0.13750004768371582 + 50.0 * 6.2547526359558105
Epoch 1200, val loss: 1.0233194828033447
Epoch 1210, training loss: 312.826416015625 = 0.1333395391702652 + 50.0 * 6.253861427307129
Epoch 1210, val loss: 1.0287028551101685
Epoch 1220, training loss: 312.8738098144531 = 0.12935790419578552 + 50.0 * 6.254889011383057
Epoch 1220, val loss: 1.033787488937378
Epoch 1230, training loss: 312.832275390625 = 0.125497505068779 + 50.0 * 6.254135608673096
Epoch 1230, val loss: 1.0394424200057983
Epoch 1240, training loss: 312.8041687011719 = 0.12177414447069168 + 50.0 * 6.253647804260254
Epoch 1240, val loss: 1.0447478294372559
Epoch 1250, training loss: 312.7239990234375 = 0.11819566041231155 + 50.0 * 6.2521162033081055
Epoch 1250, val loss: 1.050065279006958
Epoch 1260, training loss: 312.6737365722656 = 0.11474484205245972 + 50.0 * 6.2511796951293945
Epoch 1260, val loss: 1.0553888082504272
Epoch 1270, training loss: 312.78515625 = 0.11143353581428528 + 50.0 * 6.253474712371826
Epoch 1270, val loss: 1.0607823133468628
Epoch 1280, training loss: 312.9056091308594 = 0.10823260247707367 + 50.0 * 6.255947113037109
Epoch 1280, val loss: 1.0666677951812744
Epoch 1290, training loss: 312.6961975097656 = 0.10507559031248093 + 50.0 * 6.251822471618652
Epoch 1290, val loss: 1.0713250637054443
Epoch 1300, training loss: 312.554443359375 = 0.10209665447473526 + 50.0 * 6.249046325683594
Epoch 1300, val loss: 1.0774168968200684
Epoch 1310, training loss: 312.5046691894531 = 0.09922123700380325 + 50.0 * 6.248108863830566
Epoch 1310, val loss: 1.0827330350875854
Epoch 1320, training loss: 312.7164306640625 = 0.09646141529083252 + 50.0 * 6.252399444580078
Epoch 1320, val loss: 1.0883820056915283
Epoch 1330, training loss: 312.4560546875 = 0.09375528991222382 + 50.0 * 6.247246265411377
Epoch 1330, val loss: 1.093928337097168
Epoch 1340, training loss: 312.55780029296875 = 0.09115933626890182 + 50.0 * 6.249332904815674
Epoch 1340, val loss: 1.0996389389038086
Epoch 1350, training loss: 312.3714599609375 = 0.08863414824008942 + 50.0 * 6.245656490325928
Epoch 1350, val loss: 1.1048469543457031
Epoch 1360, training loss: 312.42974853515625 = 0.08620709180831909 + 50.0 * 6.246870517730713
Epoch 1360, val loss: 1.1104487180709839
Epoch 1370, training loss: 312.4322509765625 = 0.0838751271367073 + 50.0 * 6.246967792510986
Epoch 1370, val loss: 1.1159565448760986
Epoch 1380, training loss: 312.3490295410156 = 0.08162399381399155 + 50.0 * 6.24534797668457
Epoch 1380, val loss: 1.1217113733291626
Epoch 1390, training loss: 312.6709899902344 = 0.0794667974114418 + 50.0 * 6.251830577850342
Epoch 1390, val loss: 1.1273736953735352
Epoch 1400, training loss: 312.39556884765625 = 0.07732664793729782 + 50.0 * 6.246364593505859
Epoch 1400, val loss: 1.1326583623886108
Epoch 1410, training loss: 312.3466491699219 = 0.07528737932443619 + 50.0 * 6.245427131652832
Epoch 1410, val loss: 1.1386007070541382
Epoch 1420, training loss: 312.227783203125 = 0.07332117855548859 + 50.0 * 6.243088722229004
Epoch 1420, val loss: 1.143809199333191
Epoch 1430, training loss: 312.17864990234375 = 0.07143381983041763 + 50.0 * 6.2421441078186035
Epoch 1430, val loss: 1.1496796607971191
Epoch 1440, training loss: 312.4540710449219 = 0.06961002200841904 + 50.0 * 6.247689247131348
Epoch 1440, val loss: 1.1549845933914185
Epoch 1450, training loss: 312.2479248046875 = 0.06782577931880951 + 50.0 * 6.2436017990112305
Epoch 1450, val loss: 1.1609055995941162
Epoch 1460, training loss: 312.193603515625 = 0.06608490645885468 + 50.0 * 6.242550849914551
Epoch 1460, val loss: 1.1660648584365845
Epoch 1470, training loss: 312.13995361328125 = 0.06443323194980621 + 50.0 * 6.24151086807251
Epoch 1470, val loss: 1.171891212463379
Epoch 1480, training loss: 312.47454833984375 = 0.06283104419708252 + 50.0 * 6.248234272003174
Epoch 1480, val loss: 1.1770786046981812
Epoch 1490, training loss: 312.13348388671875 = 0.06124982237815857 + 50.0 * 6.2414445877075195
Epoch 1490, val loss: 1.18233060836792
Epoch 1500, training loss: 312.0458984375 = 0.05974477902054787 + 50.0 * 6.239722728729248
Epoch 1500, val loss: 1.188131332397461
Epoch 1510, training loss: 312.1084899902344 = 0.05829932540655136 + 50.0 * 6.24100399017334
Epoch 1510, val loss: 1.1932710409164429
Epoch 1520, training loss: 312.2320556640625 = 0.05689217522740364 + 50.0 * 6.243503570556641
Epoch 1520, val loss: 1.1989670991897583
Epoch 1530, training loss: 312.0026550292969 = 0.05550932511687279 + 50.0 * 6.238943099975586
Epoch 1530, val loss: 1.2041898965835571
Epoch 1540, training loss: 311.927001953125 = 0.0541858933866024 + 50.0 * 6.23745584487915
Epoch 1540, val loss: 1.2094472646713257
Epoch 1550, training loss: 311.9216003417969 = 0.052920352667570114 + 50.0 * 6.2373738288879395
Epoch 1550, val loss: 1.214986801147461
Epoch 1560, training loss: 312.054931640625 = 0.05169130116701126 + 50.0 * 6.24006462097168
Epoch 1560, val loss: 1.2200851440429688
Epoch 1570, training loss: 312.15283203125 = 0.050486307591199875 + 50.0 * 6.242047309875488
Epoch 1570, val loss: 1.2255151271820068
Epoch 1580, training loss: 311.9238586425781 = 0.04930693283677101 + 50.0 * 6.237491130828857
Epoch 1580, val loss: 1.2305265665054321
Epoch 1590, training loss: 311.86639404296875 = 0.048181112855672836 + 50.0 * 6.236363887786865
Epoch 1590, val loss: 1.2360903024673462
Epoch 1600, training loss: 311.8356628417969 = 0.04709711670875549 + 50.0 * 6.235771179199219
Epoch 1600, val loss: 1.241083025932312
Epoch 1610, training loss: 311.88702392578125 = 0.04605204612016678 + 50.0 * 6.236819744110107
Epoch 1610, val loss: 1.2465038299560547
Epoch 1620, training loss: 312.0310974121094 = 0.04502877965569496 + 50.0 * 6.239721298217773
Epoch 1620, val loss: 1.2515943050384521
Epoch 1630, training loss: 311.7841491699219 = 0.04401271045207977 + 50.0 * 6.234802722930908
Epoch 1630, val loss: 1.25651216506958
Epoch 1640, training loss: 311.7784118652344 = 0.043046995997428894 + 50.0 * 6.234706878662109
Epoch 1640, val loss: 1.2617559432983398
Epoch 1650, training loss: 311.7343444824219 = 0.042120836675167084 + 50.0 * 6.233844757080078
Epoch 1650, val loss: 1.2668482065200806
Epoch 1660, training loss: 311.83837890625 = 0.041229572147130966 + 50.0 * 6.235942840576172
Epoch 1660, val loss: 1.2720288038253784
Epoch 1670, training loss: 311.8011169433594 = 0.04034143313765526 + 50.0 * 6.235215663909912
Epoch 1670, val loss: 1.2772161960601807
Epoch 1680, training loss: 311.7792663574219 = 0.03947428986430168 + 50.0 * 6.234796047210693
Epoch 1680, val loss: 1.281840443611145
Epoch 1690, training loss: 311.7091369628906 = 0.038644324988126755 + 50.0 * 6.233409881591797
Epoch 1690, val loss: 1.2870079278945923
Epoch 1700, training loss: 311.6350402832031 = 0.03784739226102829 + 50.0 * 6.2319440841674805
Epoch 1700, val loss: 1.2919409275054932
Epoch 1710, training loss: 311.7607116699219 = 0.03708324953913689 + 50.0 * 6.234472751617432
Epoch 1710, val loss: 1.2969430685043335
Epoch 1720, training loss: 311.6278076171875 = 0.036318957805633545 + 50.0 * 6.23183012008667
Epoch 1720, val loss: 1.3014967441558838
Epoch 1730, training loss: 311.6378479003906 = 0.035577211529016495 + 50.0 * 6.2320451736450195
Epoch 1730, val loss: 1.3061864376068115
Epoch 1740, training loss: 311.83111572265625 = 0.03487445414066315 + 50.0 * 6.23592472076416
Epoch 1740, val loss: 1.3113329410552979
Epoch 1750, training loss: 311.6346130371094 = 0.03416794165968895 + 50.0 * 6.232008457183838
Epoch 1750, val loss: 1.3158314228057861
Epoch 1760, training loss: 311.59130859375 = 0.03349379450082779 + 50.0 * 6.231155872344971
Epoch 1760, val loss: 1.320851445198059
Epoch 1770, training loss: 311.7425842285156 = 0.03284338116645813 + 50.0 * 6.234194755554199
Epoch 1770, val loss: 1.3252803087234497
Epoch 1780, training loss: 311.52813720703125 = 0.032200682908296585 + 50.0 * 6.229918956756592
Epoch 1780, val loss: 1.3298006057739258
Epoch 1790, training loss: 311.5030212402344 = 0.03157883882522583 + 50.0 * 6.229428768157959
Epoch 1790, val loss: 1.3345071077346802
Epoch 1800, training loss: 311.4571228027344 = 0.030980274081230164 + 50.0 * 6.228523254394531
Epoch 1800, val loss: 1.3391250371932983
Epoch 1810, training loss: 311.458740234375 = 0.030403221026062965 + 50.0 * 6.228566646575928
Epoch 1810, val loss: 1.3437072038650513
Epoch 1820, training loss: 312.00921630859375 = 0.029843663796782494 + 50.0 * 6.239587306976318
Epoch 1820, val loss: 1.3477016687393188
Epoch 1830, training loss: 311.6741638183594 = 0.02927307039499283 + 50.0 * 6.232897758483887
Epoch 1830, val loss: 1.3525053262710571
Epoch 1840, training loss: 311.5575866699219 = 0.028726495802402496 + 50.0 * 6.23057746887207
Epoch 1840, val loss: 1.3569469451904297
Epoch 1850, training loss: 311.4862365722656 = 0.028198398649692535 + 50.0 * 6.229160785675049
Epoch 1850, val loss: 1.3613942861557007
Epoch 1860, training loss: 311.4044189453125 = 0.02768721431493759 + 50.0 * 6.227534770965576
Epoch 1860, val loss: 1.3657116889953613
Epoch 1870, training loss: 311.4210205078125 = 0.02719646319746971 + 50.0 * 6.227876663208008
Epoch 1870, val loss: 1.3697940111160278
Epoch 1880, training loss: 311.66961669921875 = 0.026716580614447594 + 50.0 * 6.232858180999756
Epoch 1880, val loss: 1.3742332458496094
Epoch 1890, training loss: 311.4696350097656 = 0.026238098740577698 + 50.0 * 6.228867530822754
Epoch 1890, val loss: 1.3787834644317627
Epoch 1900, training loss: 311.5357360839844 = 0.02577655203640461 + 50.0 * 6.230198860168457
Epoch 1900, val loss: 1.3827036619186401
Epoch 1910, training loss: 311.3956298828125 = 0.025322984904050827 + 50.0 * 6.227406024932861
Epoch 1910, val loss: 1.386978030204773
Epoch 1920, training loss: 311.3275451660156 = 0.024884667247533798 + 50.0 * 6.226053714752197
Epoch 1920, val loss: 1.3911652565002441
Epoch 1930, training loss: 311.3019714355469 = 0.024463960900902748 + 50.0 * 6.225549697875977
Epoch 1930, val loss: 1.3954498767852783
Epoch 1940, training loss: 311.4928894042969 = 0.02405310794711113 + 50.0 * 6.229376792907715
Epoch 1940, val loss: 1.3993968963623047
Epoch 1950, training loss: 311.4708251953125 = 0.023642510175704956 + 50.0 * 6.228943347930908
Epoch 1950, val loss: 1.40330171585083
Epoch 1960, training loss: 311.31036376953125 = 0.02324114739894867 + 50.0 * 6.225742340087891
Epoch 1960, val loss: 1.4075398445129395
Epoch 1970, training loss: 311.251953125 = 0.022857334464788437 + 50.0 * 6.224581718444824
Epoch 1970, val loss: 1.4117013216018677
Epoch 1980, training loss: 311.32318115234375 = 0.02248505875468254 + 50.0 * 6.226013660430908
Epoch 1980, val loss: 1.4156187772750854
Epoch 1990, training loss: 311.3363342285156 = 0.022120213136076927 + 50.0 * 6.226284027099609
Epoch 1990, val loss: 1.419697642326355
Epoch 2000, training loss: 311.23114013671875 = 0.021760771051049232 + 50.0 * 6.22418737411499
Epoch 2000, val loss: 1.4233585596084595
Epoch 2010, training loss: 311.30133056640625 = 0.021415947005152702 + 50.0 * 6.225597858428955
Epoch 2010, val loss: 1.426924228668213
Epoch 2020, training loss: 311.46820068359375 = 0.021073685958981514 + 50.0 * 6.228942394256592
Epoch 2020, val loss: 1.4317388534545898
Epoch 2030, training loss: 311.2699279785156 = 0.020731600001454353 + 50.0 * 6.224984169006348
Epoch 2030, val loss: 1.4344793558120728
Epoch 2040, training loss: 311.1882629394531 = 0.020405184477567673 + 50.0 * 6.223357200622559
Epoch 2040, val loss: 1.4389704465866089
Epoch 2050, training loss: 311.10687255859375 = 0.020089616999030113 + 50.0 * 6.221735954284668
Epoch 2050, val loss: 1.4426807165145874
Epoch 2060, training loss: 311.11517333984375 = 0.019786832854151726 + 50.0 * 6.221908092498779
Epoch 2060, val loss: 1.4463658332824707
Epoch 2070, training loss: 311.67376708984375 = 0.01949012465775013 + 50.0 * 6.2330851554870605
Epoch 2070, val loss: 1.4498004913330078
Epoch 2080, training loss: 311.4366149902344 = 0.01918591931462288 + 50.0 * 6.228348255157471
Epoch 2080, val loss: 1.4539772272109985
Epoch 2090, training loss: 311.1418762207031 = 0.01888621784746647 + 50.0 * 6.22245979309082
Epoch 2090, val loss: 1.457245111465454
Epoch 2100, training loss: 311.0478210449219 = 0.01860673353075981 + 50.0 * 6.220583915710449
Epoch 2100, val loss: 1.461001992225647
Epoch 2110, training loss: 311.0457458496094 = 0.018336765468120575 + 50.0 * 6.220548152923584
Epoch 2110, val loss: 1.464753270149231
Epoch 2120, training loss: 311.3067321777344 = 0.018075071275234222 + 50.0 * 6.225773334503174
Epoch 2120, val loss: 1.4680709838867188
Epoch 2130, training loss: 311.1252136230469 = 0.017805246636271477 + 50.0 * 6.222148418426514
Epoch 2130, val loss: 1.4718618392944336
Epoch 2140, training loss: 311.0276794433594 = 0.017541654407978058 + 50.0 * 6.220202445983887
Epoch 2140, val loss: 1.4750968217849731
Epoch 2150, training loss: 311.0447692871094 = 0.017290975898504257 + 50.0 * 6.220549583435059
Epoch 2150, val loss: 1.4789084196090698
Epoch 2160, training loss: 311.26568603515625 = 0.017048794776201248 + 50.0 * 6.224972724914551
Epoch 2160, val loss: 1.4821405410766602
Epoch 2170, training loss: 311.3087158203125 = 0.016806289553642273 + 50.0 * 6.2258381843566895
Epoch 2170, val loss: 1.4858174324035645
Epoch 2180, training loss: 311.0653381347656 = 0.016559531912207603 + 50.0 * 6.220975399017334
Epoch 2180, val loss: 1.4891180992126465
Epoch 2190, training loss: 310.9835205078125 = 0.016330735757946968 + 50.0 * 6.219343662261963
Epoch 2190, val loss: 1.4927208423614502
Epoch 2200, training loss: 310.93438720703125 = 0.01610708050429821 + 50.0 * 6.218365669250488
Epoch 2200, val loss: 1.4961000680923462
Epoch 2210, training loss: 311.0237731933594 = 0.01589157246053219 + 50.0 * 6.220157623291016
Epoch 2210, val loss: 1.4994843006134033
Epoch 2220, training loss: 311.1303405761719 = 0.015674056485295296 + 50.0 * 6.222293376922607
Epoch 2220, val loss: 1.5026801824569702
Epoch 2230, training loss: 310.9794006347656 = 0.015455921180546284 + 50.0 * 6.2192792892456055
Epoch 2230, val loss: 1.505873680114746
Epoch 2240, training loss: 310.9069519042969 = 0.015245054848492146 + 50.0 * 6.217833995819092
Epoch 2240, val loss: 1.5092239379882812
Epoch 2250, training loss: 310.91131591796875 = 0.015044085681438446 + 50.0 * 6.217925548553467
Epoch 2250, val loss: 1.5126184225082397
Epoch 2260, training loss: 311.13079833984375 = 0.014849996194243431 + 50.0 * 6.222318649291992
Epoch 2260, val loss: 1.5157802104949951
Epoch 2270, training loss: 310.86773681640625 = 0.014650865457952023 + 50.0 * 6.217061996459961
Epoch 2270, val loss: 1.5189766883850098
Epoch 2280, training loss: 311.024169921875 = 0.01446062233299017 + 50.0 * 6.220194339752197
Epoch 2280, val loss: 1.5220133066177368
Epoch 2290, training loss: 310.90789794921875 = 0.01426956057548523 + 50.0 * 6.217872619628906
Epoch 2290, val loss: 1.525153398513794
Epoch 2300, training loss: 310.89398193359375 = 0.014085888862609863 + 50.0 * 6.217597484588623
Epoch 2300, val loss: 1.5286859273910522
Epoch 2310, training loss: 311.06817626953125 = 0.013908970169723034 + 50.0 * 6.221085071563721
Epoch 2310, val loss: 1.5314520597457886
Epoch 2320, training loss: 310.9236145019531 = 0.013729937374591827 + 50.0 * 6.218197345733643
Epoch 2320, val loss: 1.5344666242599487
Epoch 2330, training loss: 310.8949890136719 = 0.013556089252233505 + 50.0 * 6.2176289558410645
Epoch 2330, val loss: 1.537817358970642
Epoch 2340, training loss: 310.8370666503906 = 0.0133849261328578 + 50.0 * 6.216473579406738
Epoch 2340, val loss: 1.540677785873413
Epoch 2350, training loss: 310.86016845703125 = 0.013222834095358849 + 50.0 * 6.2169389724731445
Epoch 2350, val loss: 1.5439972877502441
Epoch 2360, training loss: 310.95086669921875 = 0.013061502948403358 + 50.0 * 6.218756198883057
Epoch 2360, val loss: 1.5467617511749268
Epoch 2370, training loss: 310.98602294921875 = 0.012898556888103485 + 50.0 * 6.2194623947143555
Epoch 2370, val loss: 1.5497021675109863
Epoch 2380, training loss: 310.8126220703125 = 0.012740585952997208 + 50.0 * 6.215997695922852
Epoch 2380, val loss: 1.552999496459961
Epoch 2390, training loss: 310.8470153808594 = 0.012587889097630978 + 50.0 * 6.216688632965088
Epoch 2390, val loss: 1.5559406280517578
Epoch 2400, training loss: 310.8271789550781 = 0.0124366395175457 + 50.0 * 6.216294765472412
Epoch 2400, val loss: 1.5585359334945679
Epoch 2410, training loss: 310.7704162597656 = 0.01228920929133892 + 50.0 * 6.21516227722168
Epoch 2410, val loss: 1.5614858865737915
Epoch 2420, training loss: 311.2368469238281 = 0.012146822176873684 + 50.0 * 6.224493980407715
Epoch 2420, val loss: 1.5640206336975098
Epoch 2430, training loss: 310.9089050292969 = 0.011999578215181828 + 50.0 * 6.217938423156738
Epoch 2430, val loss: 1.5672001838684082
Epoch 2440, training loss: 310.7464599609375 = 0.011854821816086769 + 50.0 * 6.214692115783691
Epoch 2440, val loss: 1.569826364517212
Epoch 2450, training loss: 310.69671630859375 = 0.011718923225998878 + 50.0 * 6.213699817657471
Epoch 2450, val loss: 1.572713017463684
Epoch 2460, training loss: 310.7184143066406 = 0.011588172055780888 + 50.0 * 6.214136600494385
Epoch 2460, val loss: 1.5754847526550293
Epoch 2470, training loss: 311.0349426269531 = 0.01145967748016119 + 50.0 * 6.2204694747924805
Epoch 2470, val loss: 1.5781856775283813
Epoch 2480, training loss: 310.935302734375 = 0.011328908614814281 + 50.0 * 6.218479633331299
Epoch 2480, val loss: 1.5813612937927246
Epoch 2490, training loss: 310.7574157714844 = 0.011194948107004166 + 50.0 * 6.214924335479736
Epoch 2490, val loss: 1.5831252336502075
Epoch 2500, training loss: 310.6856689453125 = 0.011072222143411636 + 50.0 * 6.213491916656494
Epoch 2500, val loss: 1.5865389108657837
Epoch 2510, training loss: 310.754638671875 = 0.01095154695212841 + 50.0 * 6.214873790740967
Epoch 2510, val loss: 1.589155673980713
Epoch 2520, training loss: 310.75836181640625 = 0.010832737199962139 + 50.0 * 6.2149505615234375
Epoch 2520, val loss: 1.5917586088180542
Epoch 2530, training loss: 310.8978271484375 = 0.010714744217693806 + 50.0 * 6.217742443084717
Epoch 2530, val loss: 1.5944135189056396
Epoch 2540, training loss: 310.6678771972656 = 0.010592799633741379 + 50.0 * 6.213145732879639
Epoch 2540, val loss: 1.5967028141021729
Epoch 2550, training loss: 310.6296691894531 = 0.010477966628968716 + 50.0 * 6.212384223937988
Epoch 2550, val loss: 1.5992552042007446
Epoch 2560, training loss: 310.5960998535156 = 0.01036834716796875 + 50.0 * 6.211714744567871
Epoch 2560, val loss: 1.601928472518921
Epoch 2570, training loss: 310.6092834472656 = 0.010261778719723225 + 50.0 * 6.21198034286499
Epoch 2570, val loss: 1.6044422388076782
Epoch 2580, training loss: 310.95379638671875 = 0.010157386772334576 + 50.0 * 6.218872547149658
Epoch 2580, val loss: 1.6066778898239136
Epoch 2590, training loss: 310.7534484863281 = 0.010049299336969852 + 50.0 * 6.214868068695068
Epoch 2590, val loss: 1.6095362901687622
Epoch 2600, training loss: 310.67041015625 = 0.009940401650965214 + 50.0 * 6.21320915222168
Epoch 2600, val loss: 1.61166250705719
Epoch 2610, training loss: 310.6107177734375 = 0.009837373159825802 + 50.0 * 6.212018013000488
Epoch 2610, val loss: 1.6146565675735474
Epoch 2620, training loss: 310.6361389160156 = 0.009737534448504448 + 50.0 * 6.212528228759766
Epoch 2620, val loss: 1.6168503761291504
Epoch 2630, training loss: 310.72601318359375 = 0.009642895311117172 + 50.0 * 6.214327335357666
Epoch 2630, val loss: 1.619450330734253
Epoch 2640, training loss: 310.925048828125 = 0.009543543681502342 + 50.0 * 6.2183098793029785
Epoch 2640, val loss: 1.6218863725662231
Epoch 2650, training loss: 310.6106262207031 = 0.009443365968763828 + 50.0 * 6.212023735046387
Epoch 2650, val loss: 1.6239264011383057
Epoch 2660, training loss: 310.5533447265625 = 0.009350176900625229 + 50.0 * 6.210880279541016
Epoch 2660, val loss: 1.6262707710266113
Epoch 2670, training loss: 310.5184631347656 = 0.009260711260139942 + 50.0 * 6.210184574127197
Epoch 2670, val loss: 1.629042625427246
Epoch 2680, training loss: 310.7135925292969 = 0.009172838181257248 + 50.0 * 6.214088439941406
Epoch 2680, val loss: 1.631301999092102
Epoch 2690, training loss: 310.5784912109375 = 0.009079542011022568 + 50.0 * 6.211387634277344
Epoch 2690, val loss: 1.6332168579101562
Epoch 2700, training loss: 310.5048828125 = 0.008989461697638035 + 50.0 * 6.2099175453186035
Epoch 2700, val loss: 1.6354491710662842
Epoch 2710, training loss: 310.5154113769531 = 0.008904615417122841 + 50.0 * 6.210129737854004
Epoch 2710, val loss: 1.6381176710128784
Epoch 2720, training loss: 310.77703857421875 = 0.008820932358503342 + 50.0 * 6.215364456176758
Epoch 2720, val loss: 1.6397204399108887
Epoch 2730, training loss: 310.5285339355469 = 0.008734402246773243 + 50.0 * 6.2103962898254395
Epoch 2730, val loss: 1.6425734758377075
Epoch 2740, training loss: 310.46807861328125 = 0.008652044460177422 + 50.0 * 6.209188461303711
Epoch 2740, val loss: 1.6446208953857422
Epoch 2750, training loss: 310.5350341796875 = 0.008573613129556179 + 50.0 * 6.210529327392578
Epoch 2750, val loss: 1.6468844413757324
Epoch 2760, training loss: 310.77313232421875 = 0.008494740352034569 + 50.0 * 6.215292930603027
Epoch 2760, val loss: 1.6488229036331177
Epoch 2770, training loss: 310.54779052734375 = 0.008411591872572899 + 50.0 * 6.210787296295166
Epoch 2770, val loss: 1.6511794328689575
Epoch 2780, training loss: 310.4750671386719 = 0.008335006423294544 + 50.0 * 6.209334850311279
Epoch 2780, val loss: 1.6535663604736328
Epoch 2790, training loss: 310.5159606933594 = 0.008259562775492668 + 50.0 * 6.210154056549072
Epoch 2790, val loss: 1.6555954217910767
Epoch 2800, training loss: 310.52362060546875 = 0.00818623322993517 + 50.0 * 6.21030855178833
Epoch 2800, val loss: 1.6579947471618652
Epoch 2810, training loss: 310.4920349121094 = 0.008113604038953781 + 50.0 * 6.2096781730651855
Epoch 2810, val loss: 1.6600775718688965
Epoch 2820, training loss: 310.556640625 = 0.008042620494961739 + 50.0 * 6.210971832275391
Epoch 2820, val loss: 1.6621185541152954
Epoch 2830, training loss: 310.6046447753906 = 0.007970601320266724 + 50.0 * 6.211933612823486
Epoch 2830, val loss: 1.6639986038208008
Epoch 2840, training loss: 310.4620056152344 = 0.007898913696408272 + 50.0 * 6.209082126617432
Epoch 2840, val loss: 1.6662025451660156
Epoch 2850, training loss: 310.3953857421875 = 0.00782786589115858 + 50.0 * 6.2077507972717285
Epoch 2850, val loss: 1.6679799556732178
Epoch 2860, training loss: 310.394775390625 = 0.007761572953313589 + 50.0 * 6.207740783691406
Epoch 2860, val loss: 1.6700196266174316
Epoch 2870, training loss: 310.5274353027344 = 0.007696774788200855 + 50.0 * 6.210394859313965
Epoch 2870, val loss: 1.6721463203430176
Epoch 2880, training loss: 310.6361389160156 = 0.007630129810422659 + 50.0 * 6.2125701904296875
Epoch 2880, val loss: 1.6737816333770752
Epoch 2890, training loss: 310.45159912109375 = 0.0075627281330525875 + 50.0 * 6.208880424499512
Epoch 2890, val loss: 1.6762654781341553
Epoch 2900, training loss: 310.4162292480469 = 0.007495480123907328 + 50.0 * 6.208174228668213
Epoch 2900, val loss: 1.6778137683868408
Epoch 2910, training loss: 310.4432373046875 = 0.007434363942593336 + 50.0 * 6.208715915679932
Epoch 2910, val loss: 1.6803019046783447
Epoch 2920, training loss: 310.57623291015625 = 0.0073735699988901615 + 50.0 * 6.211377143859863
Epoch 2920, val loss: 1.682371973991394
Epoch 2930, training loss: 310.42498779296875 = 0.007310454733669758 + 50.0 * 6.208353042602539
Epoch 2930, val loss: 1.6839604377746582
Epoch 2940, training loss: 310.3458251953125 = 0.007248711306601763 + 50.0 * 6.206771373748779
Epoch 2940, val loss: 1.6854848861694336
Epoch 2950, training loss: 310.6020812988281 = 0.007191199343651533 + 50.0 * 6.211897850036621
Epoch 2950, val loss: 1.687380313873291
Epoch 2960, training loss: 310.3680114746094 = 0.00713067501783371 + 50.0 * 6.207217216491699
Epoch 2960, val loss: 1.6896976232528687
Epoch 2970, training loss: 310.3196716308594 = 0.007070985157042742 + 50.0 * 6.206251621246338
Epoch 2970, val loss: 1.691255807876587
Epoch 2980, training loss: 310.29400634765625 = 0.007015579380095005 + 50.0 * 6.205739974975586
Epoch 2980, val loss: 1.6932425498962402
Epoch 2990, training loss: 310.3087463378906 = 0.0069623724557459354 + 50.0 * 6.206035614013672
Epoch 2990, val loss: 1.6953330039978027
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7037037037037037
0.8118081180811808
=== training gcn model ===
Epoch 0, training loss: 431.7828369140625 = 1.9414914846420288 + 50.0 * 8.596826553344727
Epoch 0, val loss: 1.9427205324172974
Epoch 10, training loss: 431.7320251464844 = 1.933037281036377 + 50.0 * 8.595979690551758
Epoch 10, val loss: 1.934325933456421
Epoch 20, training loss: 431.4151306152344 = 1.9226800203323364 + 50.0 * 8.589849472045898
Epoch 20, val loss: 1.923608660697937
Epoch 30, training loss: 429.4419250488281 = 1.9090179204940796 + 50.0 * 8.550658226013184
Epoch 30, val loss: 1.9090971946716309
Epoch 40, training loss: 419.7265319824219 = 1.8924314975738525 + 50.0 * 8.356681823730469
Epoch 40, val loss: 1.892325520515442
Epoch 50, training loss: 399.25341796875 = 1.8739615678787231 + 50.0 * 7.94758939743042
Epoch 50, val loss: 1.8741854429244995
Epoch 60, training loss: 382.9673767089844 = 1.8600996732711792 + 50.0 * 7.622145175933838
Epoch 60, val loss: 1.8615809679031372
Epoch 70, training loss: 366.8617858886719 = 1.8501293659210205 + 50.0 * 7.300232887268066
Epoch 70, val loss: 1.8521902561187744
Epoch 80, training loss: 354.88665771484375 = 1.8412226438522339 + 50.0 * 7.060908794403076
Epoch 80, val loss: 1.8434802293777466
Epoch 90, training loss: 347.5435485839844 = 1.831751823425293 + 50.0 * 6.914236068725586
Epoch 90, val loss: 1.8343080282211304
Epoch 100, training loss: 343.2112121582031 = 1.821675181388855 + 50.0 * 6.827790260314941
Epoch 100, val loss: 1.8243976831436157
Epoch 110, training loss: 339.42816162109375 = 1.8122687339782715 + 50.0 * 6.752317905426025
Epoch 110, val loss: 1.8151402473449707
Epoch 120, training loss: 336.3897705078125 = 1.8039100170135498 + 50.0 * 6.691717147827148
Epoch 120, val loss: 1.8066672086715698
Epoch 130, training loss: 334.1385192871094 = 1.7952946424484253 + 50.0 * 6.646864891052246
Epoch 130, val loss: 1.7979799509048462
Epoch 140, training loss: 332.0966796875 = 1.7860865592956543 + 50.0 * 6.606212139129639
Epoch 140, val loss: 1.7887588739395142
Epoch 150, training loss: 330.7850646972656 = 1.7764252424240112 + 50.0 * 6.580173015594482
Epoch 150, val loss: 1.779067039489746
Epoch 160, training loss: 329.405029296875 = 1.765911340713501 + 50.0 * 6.5527825355529785
Epoch 160, val loss: 1.7689073085784912
Epoch 170, training loss: 328.4382019042969 = 1.7547450065612793 + 50.0 * 6.533669471740723
Epoch 170, val loss: 1.7581909894943237
Epoch 180, training loss: 327.7353820800781 = 1.7428560256958008 + 50.0 * 6.519850730895996
Epoch 180, val loss: 1.7468358278274536
Epoch 190, training loss: 326.9762268066406 = 1.7299546003341675 + 50.0 * 6.504925727844238
Epoch 190, val loss: 1.7346699237823486
Epoch 200, training loss: 326.1443176269531 = 1.7161297798156738 + 50.0 * 6.4885640144348145
Epoch 200, val loss: 1.7217200994491577
Epoch 210, training loss: 325.4842834472656 = 1.701243281364441 + 50.0 * 6.475660800933838
Epoch 210, val loss: 1.7079370021820068
Epoch 220, training loss: 324.8794250488281 = 1.6852277517318726 + 50.0 * 6.463883876800537
Epoch 220, val loss: 1.6931551694869995
Epoch 230, training loss: 324.3366394042969 = 1.668049693107605 + 50.0 * 6.453371524810791
Epoch 230, val loss: 1.6774364709854126
Epoch 240, training loss: 323.80560302734375 = 1.649678111076355 + 50.0 * 6.443118572235107
Epoch 240, val loss: 1.6607611179351807
Epoch 250, training loss: 323.3044738769531 = 1.630051851272583 + 50.0 * 6.433488368988037
Epoch 250, val loss: 1.6430455446243286
Epoch 260, training loss: 323.0613708496094 = 1.6091206073760986 + 50.0 * 6.429044723510742
Epoch 260, val loss: 1.6243267059326172
Epoch 270, training loss: 322.5426940917969 = 1.5869132280349731 + 50.0 * 6.4191155433654785
Epoch 270, val loss: 1.6045199632644653
Epoch 280, training loss: 322.1040954589844 = 1.563515067100525 + 50.0 * 6.410811901092529
Epoch 280, val loss: 1.583860158920288
Epoch 290, training loss: 321.840087890625 = 1.5389683246612549 + 50.0 * 6.406022548675537
Epoch 290, val loss: 1.5623672008514404
Epoch 300, training loss: 321.419921875 = 1.5131394863128662 + 50.0 * 6.398135662078857
Epoch 300, val loss: 1.5399893522262573
Epoch 310, training loss: 321.0623474121094 = 1.4863992929458618 + 50.0 * 6.391519069671631
Epoch 310, val loss: 1.5169984102249146
Epoch 320, training loss: 320.8627014160156 = 1.4586181640625 + 50.0 * 6.3880815505981445
Epoch 320, val loss: 1.493411898612976
Epoch 330, training loss: 320.5090026855469 = 1.4299432039260864 + 50.0 * 6.3815813064575195
Epoch 330, val loss: 1.469323992729187
Epoch 340, training loss: 320.1449279785156 = 1.400497317314148 + 50.0 * 6.374888896942139
Epoch 340, val loss: 1.4448124170303345
Epoch 350, training loss: 320.48095703125 = 1.3702831268310547 + 50.0 * 6.382213592529297
Epoch 350, val loss: 1.4200624227523804
Epoch 360, training loss: 319.7863464355469 = 1.339555263519287 + 50.0 * 6.368935585021973
Epoch 360, val loss: 1.3950356245040894
Epoch 370, training loss: 319.4237976074219 = 1.3085533380508423 + 50.0 * 6.3623046875
Epoch 370, val loss: 1.3701659440994263
Epoch 380, training loss: 319.1448059082031 = 1.2773934602737427 + 50.0 * 6.3573479652404785
Epoch 380, val loss: 1.3454519510269165
Epoch 390, training loss: 319.2252197265625 = 1.246132731437683 + 50.0 * 6.35958194732666
Epoch 390, val loss: 1.3209258317947388
Epoch 400, training loss: 318.7351989746094 = 1.2150087356567383 + 50.0 * 6.350404262542725
Epoch 400, val loss: 1.2965947389602661
Epoch 410, training loss: 318.5905456542969 = 1.1840903759002686 + 50.0 * 6.3481292724609375
Epoch 410, val loss: 1.272773027420044
Epoch 420, training loss: 318.35107421875 = 1.1535638570785522 + 50.0 * 6.343950271606445
Epoch 420, val loss: 1.2496873140335083
Epoch 430, training loss: 318.1261291503906 = 1.123619794845581 + 50.0 * 6.340049743652344
Epoch 430, val loss: 1.2270398139953613
Epoch 440, training loss: 317.9879150390625 = 1.0944143533706665 + 50.0 * 6.337870121002197
Epoch 440, val loss: 1.205298900604248
Epoch 450, training loss: 317.8203430175781 = 1.0659499168395996 + 50.0 * 6.335087776184082
Epoch 450, val loss: 1.1843438148498535
Epoch 460, training loss: 317.7273864746094 = 1.0383093357086182 + 50.0 * 6.333781719207764
Epoch 460, val loss: 1.1643930673599243
Epoch 470, training loss: 317.6056213378906 = 1.0115433931350708 + 50.0 * 6.331881999969482
Epoch 470, val loss: 1.145265817642212
Epoch 480, training loss: 317.2945861816406 = 0.9857162833213806 + 50.0 * 6.326177597045898
Epoch 480, val loss: 1.1271456480026245
Epoch 490, training loss: 317.2843933105469 = 0.9607741236686707 + 50.0 * 6.326472282409668
Epoch 490, val loss: 1.1101090908050537
Epoch 500, training loss: 317.08709716796875 = 0.9367465972900391 + 50.0 * 6.323007106781006
Epoch 500, val loss: 1.0937178134918213
Epoch 510, training loss: 316.8498840332031 = 0.9136092662811279 + 50.0 * 6.3187255859375
Epoch 510, val loss: 1.0785609483718872
Epoch 520, training loss: 316.7660827636719 = 0.8913703560829163 + 50.0 * 6.3174943923950195
Epoch 520, val loss: 1.0642286539077759
Epoch 530, training loss: 316.67730712890625 = 0.8698893189430237 + 50.0 * 6.316147804260254
Epoch 530, val loss: 1.0506240129470825
Epoch 540, training loss: 316.56097412109375 = 0.8492110967636108 + 50.0 * 6.314235210418701
Epoch 540, val loss: 1.0381823778152466
Epoch 550, training loss: 316.3282775878906 = 0.8292561769485474 + 50.0 * 6.309980392456055
Epoch 550, val loss: 1.0261863470077515
Epoch 560, training loss: 316.2078857421875 = 0.8099925518035889 + 50.0 * 6.307957649230957
Epoch 560, val loss: 1.0150614976882935
Epoch 570, training loss: 316.3591613769531 = 0.7912668585777283 + 50.0 * 6.3113579750061035
Epoch 570, val loss: 1.0045769214630127
Epoch 580, training loss: 316.0609130859375 = 0.7729650139808655 + 50.0 * 6.305758953094482
Epoch 580, val loss: 0.9945295453071594
Epoch 590, training loss: 315.9259033203125 = 0.755290150642395 + 50.0 * 6.303412437438965
Epoch 590, val loss: 0.9853722453117371
Epoch 600, training loss: 315.7762451171875 = 0.7381329536437988 + 50.0 * 6.300762176513672
Epoch 600, val loss: 0.976844072341919
Epoch 610, training loss: 315.70501708984375 = 0.7213634848594666 + 50.0 * 6.299673080444336
Epoch 610, val loss: 0.9688099026679993
Epoch 620, training loss: 315.6349792480469 = 0.7049441933631897 + 50.0 * 6.298600673675537
Epoch 620, val loss: 0.9610514044761658
Epoch 630, training loss: 315.8965148925781 = 0.6887111067771912 + 50.0 * 6.3041558265686035
Epoch 630, val loss: 0.9537509679794312
Epoch 640, training loss: 315.48138427734375 = 0.6728015542030334 + 50.0 * 6.29617166519165
Epoch 640, val loss: 0.9467333555221558
Epoch 650, training loss: 315.2956237792969 = 0.6571680307388306 + 50.0 * 6.292769432067871
Epoch 650, val loss: 0.9402281641960144
Epoch 660, training loss: 315.1910095214844 = 0.6418499946594238 + 50.0 * 6.290983200073242
Epoch 660, val loss: 0.9341599345207214
Epoch 670, training loss: 315.1280212402344 = 0.6268014907836914 + 50.0 * 6.290024280548096
Epoch 670, val loss: 0.928442120552063
Epoch 680, training loss: 315.0735168457031 = 0.6118253469467163 + 50.0 * 6.289234161376953
Epoch 680, val loss: 0.9228419661521912
Epoch 690, training loss: 315.0171203613281 = 0.5970602631568909 + 50.0 * 6.2884016036987305
Epoch 690, val loss: 0.9177262187004089
Epoch 700, training loss: 314.9487609863281 = 0.5825065970420837 + 50.0 * 6.287324905395508
Epoch 700, val loss: 0.912904679775238
Epoch 710, training loss: 314.91827392578125 = 0.5681947469711304 + 50.0 * 6.287001132965088
Epoch 710, val loss: 0.9083945751190186
Epoch 720, training loss: 314.7396240234375 = 0.5540664792060852 + 50.0 * 6.283710956573486
Epoch 720, val loss: 0.9042991399765015
Epoch 730, training loss: 314.74530029296875 = 0.5401597619056702 + 50.0 * 6.284102916717529
Epoch 730, val loss: 0.9005163311958313
Epoch 740, training loss: 315.017822265625 = 0.52638840675354 + 50.0 * 6.289828300476074
Epoch 740, val loss: 0.8969357013702393
Epoch 750, training loss: 314.52239990234375 = 0.5128767490386963 + 50.0 * 6.280190467834473
Epoch 750, val loss: 0.8938808441162109
Epoch 760, training loss: 314.4706115722656 = 0.49958255887031555 + 50.0 * 6.279420852661133
Epoch 760, val loss: 0.8910462260246277
Epoch 770, training loss: 314.45025634765625 = 0.4865543246269226 + 50.0 * 6.2792744636535645
Epoch 770, val loss: 0.8885995149612427
Epoch 780, training loss: 314.44915771484375 = 0.47371217608451843 + 50.0 * 6.279508590698242
Epoch 780, val loss: 0.8864431977272034
Epoch 790, training loss: 314.28338623046875 = 0.46106666326522827 + 50.0 * 6.276446342468262
Epoch 790, val loss: 0.8845852613449097
Epoch 800, training loss: 314.21295166015625 = 0.4487476050853729 + 50.0 * 6.2752838134765625
Epoch 800, val loss: 0.8833046555519104
Epoch 810, training loss: 314.3822021484375 = 0.4366917908191681 + 50.0 * 6.278910160064697
Epoch 810, val loss: 0.8822926878929138
Epoch 820, training loss: 314.2208251953125 = 0.4248257577419281 + 50.0 * 6.2759199142456055
Epoch 820, val loss: 0.8818942308425903
Epoch 830, training loss: 314.14111328125 = 0.4132820665836334 + 50.0 * 6.274556636810303
Epoch 830, val loss: 0.8816480040550232
Epoch 840, training loss: 313.93829345703125 = 0.4020131230354309 + 50.0 * 6.270725727081299
Epoch 840, val loss: 0.8818865418434143
Epoch 850, training loss: 313.9031677246094 = 0.3910525143146515 + 50.0 * 6.270242214202881
Epoch 850, val loss: 0.8825861215591431
Epoch 860, training loss: 314.27398681640625 = 0.3803117275238037 + 50.0 * 6.277873516082764
Epoch 860, val loss: 0.8835462331771851
Epoch 870, training loss: 314.03570556640625 = 0.3698643147945404 + 50.0 * 6.273316860198975
Epoch 870, val loss: 0.8850618600845337
Epoch 880, training loss: 313.7508850097656 = 0.3596389591693878 + 50.0 * 6.267824649810791
Epoch 880, val loss: 0.8867397308349609
Epoch 890, training loss: 313.7119445800781 = 0.34975436329841614 + 50.0 * 6.267243385314941
Epoch 890, val loss: 0.8888007402420044
Epoch 900, training loss: 314.0583801269531 = 0.34012606739997864 + 50.0 * 6.274364948272705
Epoch 900, val loss: 0.8912158608436584
Epoch 910, training loss: 313.7494812011719 = 0.3306525647640228 + 50.0 * 6.268376350402832
Epoch 910, val loss: 0.8940659165382385
Epoch 920, training loss: 313.5662841796875 = 0.3215399980545044 + 50.0 * 6.264894962310791
Epoch 920, val loss: 0.8972139954566956
Epoch 930, training loss: 313.4977722167969 = 0.3126748204231262 + 50.0 * 6.263701915740967
Epoch 930, val loss: 0.9006809592247009
Epoch 940, training loss: 313.6798400878906 = 0.3041140139102936 + 50.0 * 6.267514705657959
Epoch 940, val loss: 0.9044547080993652
Epoch 950, training loss: 313.4295349121094 = 0.2956656813621521 + 50.0 * 6.2626776695251465
Epoch 950, val loss: 0.9082283973693848
Epoch 960, training loss: 313.45867919921875 = 0.2875712811946869 + 50.0 * 6.263422012329102
Epoch 960, val loss: 0.9124684929847717
Epoch 970, training loss: 313.3993835449219 = 0.27960965037345886 + 50.0 * 6.26239538192749
Epoch 970, val loss: 0.9166091084480286
Epoch 980, training loss: 313.410888671875 = 0.2719261646270752 + 50.0 * 6.262779712677002
Epoch 980, val loss: 0.9214320182800293
Epoch 990, training loss: 313.27569580078125 = 0.26440930366516113 + 50.0 * 6.260225772857666
Epoch 990, val loss: 0.9260379672050476
Epoch 1000, training loss: 313.2807922363281 = 0.25718313455581665 + 50.0 * 6.260472297668457
Epoch 1000, val loss: 0.9311263561248779
Epoch 1010, training loss: 313.1626281738281 = 0.2501384913921356 + 50.0 * 6.258249282836914
Epoch 1010, val loss: 0.9361737370491028
Epoch 1020, training loss: 313.36572265625 = 0.2433667629957199 + 50.0 * 6.262446880340576
Epoch 1020, val loss: 0.9415814876556396
Epoch 1030, training loss: 313.1477966308594 = 0.23669275641441345 + 50.0 * 6.2582221031188965
Epoch 1030, val loss: 0.9469462037086487
Epoch 1040, training loss: 313.0766906738281 = 0.23029787838459015 + 50.0 * 6.256927967071533
Epoch 1040, val loss: 0.9527325630187988
Epoch 1050, training loss: 313.32989501953125 = 0.22405920922756195 + 50.0 * 6.2621169090271
Epoch 1050, val loss: 0.9585885405540466
Epoch 1060, training loss: 313.1104431152344 = 0.21800249814987183 + 50.0 * 6.257848262786865
Epoch 1060, val loss: 0.9641109704971313
Epoch 1070, training loss: 312.9552307128906 = 0.21212410926818848 + 50.0 * 6.254861831665039
Epoch 1070, val loss: 0.9703563451766968
Epoch 1080, training loss: 312.87548828125 = 0.20648160576820374 + 50.0 * 6.253380298614502
Epoch 1080, val loss: 0.9765517711639404
Epoch 1090, training loss: 312.8887939453125 = 0.20101304352283478 + 50.0 * 6.253755569458008
Epoch 1090, val loss: 0.9827662706375122
Epoch 1100, training loss: 312.8724060058594 = 0.19565783441066742 + 50.0 * 6.253535270690918
Epoch 1100, val loss: 0.9892259240150452
Epoch 1110, training loss: 312.82275390625 = 0.19046184420585632 + 50.0 * 6.252645969390869
Epoch 1110, val loss: 0.9956110119819641
Epoch 1120, training loss: 312.8988037109375 = 0.185445636510849 + 50.0 * 6.254266738891602
Epoch 1120, val loss: 1.0023483037948608
Epoch 1130, training loss: 312.74853515625 = 0.1805495023727417 + 50.0 * 6.251359939575195
Epoch 1130, val loss: 1.0090904235839844
Epoch 1140, training loss: 312.8172607421875 = 0.17582471668720245 + 50.0 * 6.252829074859619
Epoch 1140, val loss: 1.0158823728561401
Epoch 1150, training loss: 312.9953308105469 = 0.1712108701467514 + 50.0 * 6.2564826011657715
Epoch 1150, val loss: 1.022801399230957
Epoch 1160, training loss: 312.79486083984375 = 0.16671226918697357 + 50.0 * 6.252562999725342
Epoch 1160, val loss: 1.0297918319702148
Epoch 1170, training loss: 312.63128662109375 = 0.16237030923366547 + 50.0 * 6.249378204345703
Epoch 1170, val loss: 1.0368961095809937
Epoch 1180, training loss: 312.6222839355469 = 0.15817664563655853 + 50.0 * 6.249282360076904
Epoch 1180, val loss: 1.0443257093429565
Epoch 1190, training loss: 312.75372314453125 = 0.15409334003925323 + 50.0 * 6.251992702484131
Epoch 1190, val loss: 1.0512810945510864
Epoch 1200, training loss: 312.6807556152344 = 0.150141641497612 + 50.0 * 6.250612258911133
Epoch 1200, val loss: 1.0589475631713867
Epoch 1210, training loss: 312.62677001953125 = 0.14624986052513123 + 50.0 * 6.249610424041748
Epoch 1210, val loss: 1.0662446022033691
Epoch 1220, training loss: 312.7529296875 = 0.1425030529499054 + 50.0 * 6.252208232879639
Epoch 1220, val loss: 1.0734398365020752
Epoch 1230, training loss: 312.4638671875 = 0.13887919485569 + 50.0 * 6.246499538421631
Epoch 1230, val loss: 1.0812950134277344
Epoch 1240, training loss: 312.39923095703125 = 0.13534954190254211 + 50.0 * 6.2452778816223145
Epoch 1240, val loss: 1.088709831237793
Epoch 1250, training loss: 312.36041259765625 = 0.1319453865289688 + 50.0 * 6.244568824768066
Epoch 1250, val loss: 1.0966088771820068
Epoch 1260, training loss: 312.82354736328125 = 0.12865938246250153 + 50.0 * 6.253897666931152
Epoch 1260, val loss: 1.1042368412017822
Epoch 1270, training loss: 312.5478210449219 = 0.12539377808570862 + 50.0 * 6.248448371887207
Epoch 1270, val loss: 1.1117722988128662
Epoch 1280, training loss: 312.3504638671875 = 0.12224133312702179 + 50.0 * 6.244564056396484
Epoch 1280, val loss: 1.1195931434631348
Epoch 1290, training loss: 312.2520751953125 = 0.11919035017490387 + 50.0 * 6.242657661437988
Epoch 1290, val loss: 1.1274189949035645
Epoch 1300, training loss: 312.5682678222656 = 0.11627022176980972 + 50.0 * 6.249040126800537
Epoch 1300, val loss: 1.1352652311325073
Epoch 1310, training loss: 312.3990173339844 = 0.11336673051118851 + 50.0 * 6.245712757110596
Epoch 1310, val loss: 1.1434489488601685
Epoch 1320, training loss: 312.3183898925781 = 0.11057817190885544 + 50.0 * 6.244156360626221
Epoch 1320, val loss: 1.1508609056472778
Epoch 1330, training loss: 312.376708984375 = 0.1078713983297348 + 50.0 * 6.2453765869140625
Epoch 1330, val loss: 1.1588190793991089
Epoch 1340, training loss: 312.1543884277344 = 0.10523267090320587 + 50.0 * 6.240983486175537
Epoch 1340, val loss: 1.1672881841659546
Epoch 1350, training loss: 312.12176513671875 = 0.10269375145435333 + 50.0 * 6.240381240844727
Epoch 1350, val loss: 1.1751093864440918
Epoch 1360, training loss: 312.4378967285156 = 0.10025501996278763 + 50.0 * 6.246752738952637
Epoch 1360, val loss: 1.183305263519287
Epoch 1370, training loss: 312.2000427246094 = 0.09780707210302353 + 50.0 * 6.242044448852539
Epoch 1370, val loss: 1.1910982131958008
Epoch 1380, training loss: 312.2038879394531 = 0.09548245370388031 + 50.0 * 6.242167949676514
Epoch 1380, val loss: 1.1990371942520142
Epoch 1390, training loss: 312.0828857421875 = 0.0932057574391365 + 50.0 * 6.23979377746582
Epoch 1390, val loss: 1.2074072360992432
Epoch 1400, training loss: 312.06585693359375 = 0.0910084918141365 + 50.0 * 6.239497184753418
Epoch 1400, val loss: 1.215501308441162
Epoch 1410, training loss: 312.3655090332031 = 0.08887022733688354 + 50.0 * 6.245532989501953
Epoch 1410, val loss: 1.2231518030166626
Epoch 1420, training loss: 312.0722961425781 = 0.08676772564649582 + 50.0 * 6.239710807800293
Epoch 1420, val loss: 1.2315466403961182
Epoch 1430, training loss: 311.99029541015625 = 0.08473935723304749 + 50.0 * 6.2381110191345215
Epoch 1430, val loss: 1.2396184206008911
Epoch 1440, training loss: 312.4794006347656 = 0.08277101814746857 + 50.0 * 6.2479329109191895
Epoch 1440, val loss: 1.2477399110794067
Epoch 1450, training loss: 312.0322570800781 = 0.08085004985332489 + 50.0 * 6.239028453826904
Epoch 1450, val loss: 1.2555701732635498
Epoch 1460, training loss: 311.8691711425781 = 0.07896916568279266 + 50.0 * 6.235803604125977
Epoch 1460, val loss: 1.263695240020752
Epoch 1470, training loss: 311.8515930175781 = 0.0771692618727684 + 50.0 * 6.235488414764404
Epoch 1470, val loss: 1.27155601978302
Epoch 1480, training loss: 312.13812255859375 = 0.07542455196380615 + 50.0 * 6.24125337600708
Epoch 1480, val loss: 1.2795590162277222
Epoch 1490, training loss: 311.86175537109375 = 0.07370895892381668 + 50.0 * 6.2357611656188965
Epoch 1490, val loss: 1.2872425317764282
Epoch 1500, training loss: 312.08636474609375 = 0.07205109298229218 + 50.0 * 6.240286350250244
Epoch 1500, val loss: 1.295196771621704
Epoch 1510, training loss: 311.8953857421875 = 0.0704147219657898 + 50.0 * 6.236499786376953
Epoch 1510, val loss: 1.302788496017456
Epoch 1520, training loss: 311.80511474609375 = 0.06884019076824188 + 50.0 * 6.234725475311279
Epoch 1520, val loss: 1.3105032444000244
Epoch 1530, training loss: 311.7283020019531 = 0.06732379645109177 + 50.0 * 6.233219623565674
Epoch 1530, val loss: 1.3183826208114624
Epoch 1540, training loss: 311.7411804199219 = 0.06585031747817993 + 50.0 * 6.233506679534912
Epoch 1540, val loss: 1.3263123035430908
Epoch 1550, training loss: 311.9956359863281 = 0.06443176418542862 + 50.0 * 6.238624572753906
Epoch 1550, val loss: 1.3343092203140259
Epoch 1560, training loss: 311.9005432128906 = 0.06302215158939362 + 50.0 * 6.236750602722168
Epoch 1560, val loss: 1.3411210775375366
Epoch 1570, training loss: 311.8473815917969 = 0.06164415553212166 + 50.0 * 6.235714912414551
Epoch 1570, val loss: 1.3491945266723633
Epoch 1580, training loss: 311.6976013183594 = 0.06029783934354782 + 50.0 * 6.232746124267578
Epoch 1580, val loss: 1.3566612005233765
Epoch 1590, training loss: 311.79388427734375 = 0.05901408940553665 + 50.0 * 6.234697341918945
Epoch 1590, val loss: 1.3647711277008057
Epoch 1600, training loss: 311.69989013671875 = 0.05776064470410347 + 50.0 * 6.232842445373535
Epoch 1600, val loss: 1.3718458414077759
Epoch 1610, training loss: 311.589111328125 = 0.05652165412902832 + 50.0 * 6.23065185546875
Epoch 1610, val loss: 1.3793753385543823
Epoch 1620, training loss: 311.5671081542969 = 0.05533497408032417 + 50.0 * 6.230235576629639
Epoch 1620, val loss: 1.3867429494857788
Epoch 1630, training loss: 311.658203125 = 0.05419176071882248 + 50.0 * 6.232079982757568
Epoch 1630, val loss: 1.3943543434143066
Epoch 1640, training loss: 311.7548828125 = 0.05306556448340416 + 50.0 * 6.234035968780518
Epoch 1640, val loss: 1.4012033939361572
Epoch 1650, training loss: 311.648681640625 = 0.05195726454257965 + 50.0 * 6.231934070587158
Epoch 1650, val loss: 1.4083129167556763
Epoch 1660, training loss: 311.55126953125 = 0.05088241025805473 + 50.0 * 6.230007648468018
Epoch 1660, val loss: 1.4158992767333984
Epoch 1670, training loss: 311.6372985839844 = 0.04984419420361519 + 50.0 * 6.231749534606934
Epoch 1670, val loss: 1.4227659702301025
Epoch 1680, training loss: 311.6477355957031 = 0.04883455112576485 + 50.0 * 6.231978416442871
Epoch 1680, val loss: 1.4302024841308594
Epoch 1690, training loss: 311.5496520996094 = 0.047850627452135086 + 50.0 * 6.230035781860352
Epoch 1690, val loss: 1.4368890523910522
Epoch 1700, training loss: 311.46044921875 = 0.04688301309943199 + 50.0 * 6.228271484375
Epoch 1700, val loss: 1.4442321062088013
Epoch 1710, training loss: 311.53411865234375 = 0.04596114903688431 + 50.0 * 6.229763031005859
Epoch 1710, val loss: 1.4510142803192139
Epoch 1720, training loss: 311.56512451171875 = 0.04506385698914528 + 50.0 * 6.230401039123535
Epoch 1720, val loss: 1.4581085443496704
Epoch 1730, training loss: 311.6180419921875 = 0.04415195807814598 + 50.0 * 6.231477737426758
Epoch 1730, val loss: 1.4648362398147583
Epoch 1740, training loss: 311.509033203125 = 0.04327964782714844 + 50.0 * 6.229315280914307
Epoch 1740, val loss: 1.4718550443649292
Epoch 1750, training loss: 311.39849853515625 = 0.04242558404803276 + 50.0 * 6.227121353149414
Epoch 1750, val loss: 1.4789042472839355
Epoch 1760, training loss: 311.5282897949219 = 0.041615258902311325 + 50.0 * 6.229733467102051
Epoch 1760, val loss: 1.4858770370483398
Epoch 1770, training loss: 311.4098815917969 = 0.040798917412757874 + 50.0 * 6.227381706237793
Epoch 1770, val loss: 1.491734266281128
Epoch 1780, training loss: 311.4440002441406 = 0.04002278298139572 + 50.0 * 6.228079795837402
Epoch 1780, val loss: 1.4986997842788696
Epoch 1790, training loss: 311.3556213378906 = 0.03924868255853653 + 50.0 * 6.226327419281006
Epoch 1790, val loss: 1.505081057548523
Epoch 1800, training loss: 311.2979736328125 = 0.0385056808590889 + 50.0 * 6.225189208984375
Epoch 1800, val loss: 1.512202262878418
Epoch 1810, training loss: 311.33575439453125 = 0.03778553754091263 + 50.0 * 6.225959300994873
Epoch 1810, val loss: 1.5187325477600098
Epoch 1820, training loss: 311.5628356933594 = 0.03708773851394653 + 50.0 * 6.230515003204346
Epoch 1820, val loss: 1.5255118608474731
Epoch 1830, training loss: 311.5313720703125 = 0.03639611974358559 + 50.0 * 6.2298994064331055
Epoch 1830, val loss: 1.5310757160186768
Epoch 1840, training loss: 311.4482727050781 = 0.03571227937936783 + 50.0 * 6.2282514572143555
Epoch 1840, val loss: 1.5376653671264648
Epoch 1850, training loss: 311.2978515625 = 0.03505971282720566 + 50.0 * 6.225255489349365
Epoch 1850, val loss: 1.5442543029785156
Epoch 1860, training loss: 311.5534973144531 = 0.03442471846938133 + 50.0 * 6.230381488800049
Epoch 1860, val loss: 1.5504250526428223
Epoch 1870, training loss: 311.31884765625 = 0.03378511592745781 + 50.0 * 6.225701332092285
Epoch 1870, val loss: 1.557060718536377
Epoch 1880, training loss: 311.23590087890625 = 0.033175211399793625 + 50.0 * 6.22405481338501
Epoch 1880, val loss: 1.5633201599121094
Epoch 1890, training loss: 311.1877746582031 = 0.032584793865680695 + 50.0 * 6.2231035232543945
Epoch 1890, val loss: 1.5692898035049438
Epoch 1900, training loss: 311.2200927734375 = 0.032009948045015335 + 50.0 * 6.223761558532715
Epoch 1900, val loss: 1.5755391120910645
Epoch 1910, training loss: 311.4816589355469 = 0.031453780829906464 + 50.0 * 6.22900390625
Epoch 1910, val loss: 1.5817795991897583
Epoch 1920, training loss: 311.3280029296875 = 0.03088257648050785 + 50.0 * 6.225942134857178
Epoch 1920, val loss: 1.5870307683944702
Epoch 1930, training loss: 311.23712158203125 = 0.030344298109412193 + 50.0 * 6.224135875701904
Epoch 1930, val loss: 1.59328031539917
Epoch 1940, training loss: 311.379638671875 = 0.02982548624277115 + 50.0 * 6.226996421813965
Epoch 1940, val loss: 1.598840355873108
Epoch 1950, training loss: 311.1147155761719 = 0.0292963907122612 + 50.0 * 6.221708297729492
Epoch 1950, val loss: 1.605520248413086
Epoch 1960, training loss: 311.15765380859375 = 0.02879253774881363 + 50.0 * 6.2225775718688965
Epoch 1960, val loss: 1.6113511323928833
Epoch 1970, training loss: 311.19097900390625 = 0.028310082852840424 + 50.0 * 6.22325325012207
Epoch 1970, val loss: 1.6173462867736816
Epoch 1980, training loss: 311.16058349609375 = 0.027828862890601158 + 50.0 * 6.222654819488525
Epoch 1980, val loss: 1.6224210262298584
Epoch 1990, training loss: 311.2561340332031 = 0.027354231104254723 + 50.0 * 6.224575996398926
Epoch 1990, val loss: 1.6279550790786743
Epoch 2000, training loss: 311.2255859375 = 0.026894597336649895 + 50.0 * 6.223973751068115
Epoch 2000, val loss: 1.6345436573028564
Epoch 2010, training loss: 311.1706848144531 = 0.026443690061569214 + 50.0 * 6.222884654998779
Epoch 2010, val loss: 1.639378309249878
Epoch 2020, training loss: 311.2069091796875 = 0.026005854830145836 + 50.0 * 6.223618030548096
Epoch 2020, val loss: 1.6451692581176758
Epoch 2030, training loss: 311.0216979980469 = 0.02557775378227234 + 50.0 * 6.2199225425720215
Epoch 2030, val loss: 1.6505671739578247
Epoch 2040, training loss: 311.0294494628906 = 0.02516324445605278 + 50.0 * 6.220085620880127
Epoch 2040, val loss: 1.6563764810562134
Epoch 2050, training loss: 311.1414794921875 = 0.024762414395809174 + 50.0 * 6.222334384918213
Epoch 2050, val loss: 1.6615742444992065
Epoch 2060, training loss: 311.0656433105469 = 0.02436154894530773 + 50.0 * 6.220825672149658
Epoch 2060, val loss: 1.6671571731567383
Epoch 2070, training loss: 311.1773681640625 = 0.02397473342716694 + 50.0 * 6.223067760467529
Epoch 2070, val loss: 1.6718413829803467
Epoch 2080, training loss: 311.0545349121094 = 0.023586492985486984 + 50.0 * 6.220618724822998
Epoch 2080, val loss: 1.6777877807617188
Epoch 2090, training loss: 311.13092041015625 = 0.0232133399695158 + 50.0 * 6.222154140472412
Epoch 2090, val loss: 1.6825977563858032
Epoch 2100, training loss: 310.940673828125 = 0.022845551371574402 + 50.0 * 6.218356132507324
Epoch 2100, val loss: 1.6885416507720947
Epoch 2110, training loss: 310.9659118652344 = 0.022490091621875763 + 50.0 * 6.218868255615234
Epoch 2110, val loss: 1.6936217546463013
Epoch 2120, training loss: 311.1180419921875 = 0.022145431488752365 + 50.0 * 6.221917629241943
Epoch 2120, val loss: 1.698426604270935
Epoch 2130, training loss: 310.99237060546875 = 0.021800672635436058 + 50.0 * 6.219411373138428
Epoch 2130, val loss: 1.7037748098373413
Epoch 2140, training loss: 310.9858093261719 = 0.021466262638568878 + 50.0 * 6.219287395477295
Epoch 2140, val loss: 1.7091097831726074
Epoch 2150, training loss: 311.06689453125 = 0.02114008367061615 + 50.0 * 6.220914840698242
Epoch 2150, val loss: 1.7141728401184082
Epoch 2160, training loss: 311.0592041015625 = 0.020816832780838013 + 50.0 * 6.220767974853516
Epoch 2160, val loss: 1.7189457416534424
Epoch 2170, training loss: 310.9064636230469 = 0.0205056332051754 + 50.0 * 6.217719078063965
Epoch 2170, val loss: 1.723907232284546
Epoch 2180, training loss: 310.83099365234375 = 0.020200228318572044 + 50.0 * 6.21621561050415
Epoch 2180, val loss: 1.7288631200790405
Epoch 2190, training loss: 310.8758239746094 = 0.019906653091311455 + 50.0 * 6.217118263244629
Epoch 2190, val loss: 1.733830451965332
Epoch 2200, training loss: 311.10137939453125 = 0.019618717953562737 + 50.0 * 6.221634864807129
Epoch 2200, val loss: 1.7379063367843628
Epoch 2210, training loss: 310.9715881347656 = 0.019329426810145378 + 50.0 * 6.219045639038086
Epoch 2210, val loss: 1.7442328929901123
Epoch 2220, training loss: 310.99652099609375 = 0.01904461532831192 + 50.0 * 6.219549655914307
Epoch 2220, val loss: 1.7483739852905273
Epoch 2230, training loss: 310.8409423828125 = 0.018769999966025352 + 50.0 * 6.2164435386657715
Epoch 2230, val loss: 1.753433346748352
Epoch 2240, training loss: 310.84228515625 = 0.01850079372525215 + 50.0 * 6.216475963592529
Epoch 2240, val loss: 1.7572951316833496
Epoch 2250, training loss: 310.8990478515625 = 0.01823907531797886 + 50.0 * 6.217616081237793
Epoch 2250, val loss: 1.76260244846344
Epoch 2260, training loss: 310.90570068359375 = 0.017980128526687622 + 50.0 * 6.217754364013672
Epoch 2260, val loss: 1.7676883935928345
Epoch 2270, training loss: 310.88671875 = 0.01773136854171753 + 50.0 * 6.217379570007324
Epoch 2270, val loss: 1.7719234228134155
Epoch 2280, training loss: 310.8776550292969 = 0.017481815069913864 + 50.0 * 6.217203617095947
Epoch 2280, val loss: 1.7763054370880127
Epoch 2290, training loss: 310.70654296875 = 0.017232274636626244 + 50.0 * 6.2137861251831055
Epoch 2290, val loss: 1.7808667421340942
Epoch 2300, training loss: 310.7355651855469 = 0.016996171325445175 + 50.0 * 6.214371204376221
Epoch 2300, val loss: 1.7856465578079224
Epoch 2310, training loss: 310.9734191894531 = 0.016769012436270714 + 50.0 * 6.219133377075195
Epoch 2310, val loss: 1.7899399995803833
Epoch 2320, training loss: 310.71209716796875 = 0.016537711024284363 + 50.0 * 6.213911056518555
Epoch 2320, val loss: 1.7946076393127441
Epoch 2330, training loss: 310.6883239746094 = 0.016313940286636353 + 50.0 * 6.21343994140625
Epoch 2330, val loss: 1.7986254692077637
Epoch 2340, training loss: 310.70458984375 = 0.016096128150820732 + 50.0 * 6.213770389556885
Epoch 2340, val loss: 1.803600788116455
Epoch 2350, training loss: 310.8676452636719 = 0.01589319109916687 + 50.0 * 6.217034816741943
Epoch 2350, val loss: 1.807692289352417
Epoch 2360, training loss: 310.7928466796875 = 0.01567607745528221 + 50.0 * 6.215543270111084
Epoch 2360, val loss: 1.8114005327224731
Epoch 2370, training loss: 310.8414306640625 = 0.015466573648154736 + 50.0 * 6.216518878936768
Epoch 2370, val loss: 1.815809726715088
Epoch 2380, training loss: 310.837890625 = 0.015261012129485607 + 50.0 * 6.216452598571777
Epoch 2380, val loss: 1.8203388452529907
Epoch 2390, training loss: 310.6499328613281 = 0.015061008743941784 + 50.0 * 6.2126970291137695
Epoch 2390, val loss: 1.8251657485961914
Epoch 2400, training loss: 310.63433837890625 = 0.014867273159325123 + 50.0 * 6.2123894691467285
Epoch 2400, val loss: 1.8294270038604736
Epoch 2410, training loss: 310.65106201171875 = 0.014680189080536366 + 50.0 * 6.2127275466918945
Epoch 2410, val loss: 1.8338425159454346
Epoch 2420, training loss: 310.8647155761719 = 0.014498216100037098 + 50.0 * 6.217004299163818
Epoch 2420, val loss: 1.8378303050994873
Epoch 2430, training loss: 310.93133544921875 = 0.014317882247269154 + 50.0 * 6.2183403968811035
Epoch 2430, val loss: 1.8406269550323486
Epoch 2440, training loss: 310.8276672363281 = 0.014132189564406872 + 50.0 * 6.216270446777344
Epoch 2440, val loss: 1.8457449674606323
Epoch 2450, training loss: 310.63043212890625 = 0.0139470798894763 + 50.0 * 6.212329864501953
Epoch 2450, val loss: 1.8489748239517212
Epoch 2460, training loss: 310.580322265625 = 0.013775289990007877 + 50.0 * 6.211331367492676
Epoch 2460, val loss: 1.8538875579833984
Epoch 2470, training loss: 310.5404357910156 = 0.013606518507003784 + 50.0 * 6.210536479949951
Epoch 2470, val loss: 1.8574378490447998
Epoch 2480, training loss: 310.7128601074219 = 0.013445891439914703 + 50.0 * 6.213988304138184
Epoch 2480, val loss: 1.8614715337753296
Epoch 2490, training loss: 310.6911926269531 = 0.013280493207275867 + 50.0 * 6.213558197021484
Epoch 2490, val loss: 1.864801049232483
Epoch 2500, training loss: 310.5557861328125 = 0.013115660287439823 + 50.0 * 6.210853576660156
Epoch 2500, val loss: 1.8691388368606567
Epoch 2510, training loss: 310.4978942871094 = 0.012957009486854076 + 50.0 * 6.20969820022583
Epoch 2510, val loss: 1.8730593919754028
Epoch 2520, training loss: 310.597900390625 = 0.01280894037336111 + 50.0 * 6.2117018699646
Epoch 2520, val loss: 1.8768945932388306
Epoch 2530, training loss: 310.7441711425781 = 0.012658143416047096 + 50.0 * 6.214630126953125
Epoch 2530, val loss: 1.880949854850769
Epoch 2540, training loss: 310.83563232421875 = 0.012504829093813896 + 50.0 * 6.2164626121521
Epoch 2540, val loss: 1.8840973377227783
Epoch 2550, training loss: 310.5723571777344 = 0.012353982776403427 + 50.0 * 6.211199760437012
Epoch 2550, val loss: 1.8880587816238403
Epoch 2560, training loss: 310.5047912597656 = 0.012206990271806717 + 50.0 * 6.2098517417907715
Epoch 2560, val loss: 1.89243483543396
Epoch 2570, training loss: 310.4479064941406 = 0.012069153599441051 + 50.0 * 6.208716869354248
Epoch 2570, val loss: 1.8955132961273193
Epoch 2580, training loss: 310.56500244140625 = 0.011937800794839859 + 50.0 * 6.211061477661133
Epoch 2580, val loss: 1.8991525173187256
Epoch 2590, training loss: 310.6332092285156 = 0.011799830012023449 + 50.0 * 6.212428092956543
Epoch 2590, val loss: 1.9027619361877441
Epoch 2600, training loss: 310.5380554199219 = 0.011662200093269348 + 50.0 * 6.2105278968811035
Epoch 2600, val loss: 1.9068000316619873
Epoch 2610, training loss: 310.42572021484375 = 0.011531227268278599 + 50.0 * 6.2082839012146
Epoch 2610, val loss: 1.90984046459198
Epoch 2620, training loss: 310.5389404296875 = 0.01140589825809002 + 50.0 * 6.210550785064697
Epoch 2620, val loss: 1.9138357639312744
Epoch 2630, training loss: 310.5317687988281 = 0.011278069578111172 + 50.0 * 6.210409641265869
Epoch 2630, val loss: 1.9170244932174683
Epoch 2640, training loss: 310.46551513671875 = 0.011151417158544064 + 50.0 * 6.209087371826172
Epoch 2640, val loss: 1.920655369758606
Epoch 2650, training loss: 310.3869323730469 = 0.011028780601918697 + 50.0 * 6.207518577575684
Epoch 2650, val loss: 1.9243807792663574
Epoch 2660, training loss: 310.4584655761719 = 0.010914119891822338 + 50.0 * 6.208950996398926
Epoch 2660, val loss: 1.9276329278945923
Epoch 2670, training loss: 310.6810302734375 = 0.010800533927977085 + 50.0 * 6.213404655456543
Epoch 2670, val loss: 1.9308634996414185
Epoch 2680, training loss: 310.5212707519531 = 0.010677670128643513 + 50.0 * 6.210211753845215
Epoch 2680, val loss: 1.934579610824585
Epoch 2690, training loss: 310.42291259765625 = 0.010564442723989487 + 50.0 * 6.208247184753418
Epoch 2690, val loss: 1.9377292394638062
Epoch 2700, training loss: 310.5033264160156 = 0.010452492162585258 + 50.0 * 6.209856986999512
Epoch 2700, val loss: 1.9412016868591309
Epoch 2710, training loss: 310.4986877441406 = 0.010340802371501923 + 50.0 * 6.209766864776611
Epoch 2710, val loss: 1.9446860551834106
Epoch 2720, training loss: 310.4283752441406 = 0.010233091190457344 + 50.0 * 6.208363056182861
Epoch 2720, val loss: 1.9479390382766724
Epoch 2730, training loss: 310.4043884277344 = 0.010125613771378994 + 50.0 * 6.207885265350342
Epoch 2730, val loss: 1.9509073495864868
Epoch 2740, training loss: 310.5942687988281 = 0.010028610937297344 + 50.0 * 6.211684703826904
Epoch 2740, val loss: 1.953338861465454
Epoch 2750, training loss: 310.37939453125 = 0.009918784722685814 + 50.0 * 6.2073893547058105
Epoch 2750, val loss: 1.9577949047088623
Epoch 2760, training loss: 310.3600769042969 = 0.009818417951464653 + 50.0 * 6.207005023956299
Epoch 2760, val loss: 1.9610446691513062
Epoch 2770, training loss: 310.367919921875 = 0.009721864014863968 + 50.0 * 6.2071638107299805
Epoch 2770, val loss: 1.9636698961257935
Epoch 2780, training loss: 310.4010009765625 = 0.009625855833292007 + 50.0 * 6.207828044891357
Epoch 2780, val loss: 1.9673789739608765
Epoch 2790, training loss: 310.4963073730469 = 0.009531819261610508 + 50.0 * 6.209735870361328
Epoch 2790, val loss: 1.9705634117126465
Epoch 2800, training loss: 310.3140869140625 = 0.009432462975382805 + 50.0 * 6.2060933113098145
Epoch 2800, val loss: 1.9733206033706665
Epoch 2810, training loss: 310.5379638671875 = 0.009343058802187443 + 50.0 * 6.210572242736816
Epoch 2810, val loss: 1.97678542137146
Epoch 2820, training loss: 310.4831237792969 = 0.009250594303011894 + 50.0 * 6.209477424621582
Epoch 2820, val loss: 1.979741096496582
Epoch 2830, training loss: 310.3355407714844 = 0.009157044813036919 + 50.0 * 6.2065277099609375
Epoch 2830, val loss: 1.982757568359375
Epoch 2840, training loss: 310.27239990234375 = 0.009067250415682793 + 50.0 * 6.20526647567749
Epoch 2840, val loss: 1.9861629009246826
Epoch 2850, training loss: 310.2682800292969 = 0.008982675150036812 + 50.0 * 6.205186367034912
Epoch 2850, val loss: 1.9896142482757568
Epoch 2860, training loss: 310.48651123046875 = 0.008899458684027195 + 50.0 * 6.209551811218262
Epoch 2860, val loss: 1.992414951324463
Epoch 2870, training loss: 310.2419128417969 = 0.008811788633465767 + 50.0 * 6.204661846160889
Epoch 2870, val loss: 1.9952514171600342
Epoch 2880, training loss: 310.4056701660156 = 0.008731240406632423 + 50.0 * 6.2079386711120605
Epoch 2880, val loss: 1.9982702732086182
Epoch 2890, training loss: 310.5069580078125 = 0.008648958057165146 + 50.0 * 6.209966659545898
Epoch 2890, val loss: 2.0008981227874756
Epoch 2900, training loss: 310.3313903808594 = 0.00856686383485794 + 50.0 * 6.206456184387207
Epoch 2900, val loss: 2.0035183429718018
Epoch 2910, training loss: 310.2490234375 = 0.008483510464429855 + 50.0 * 6.204811096191406
Epoch 2910, val loss: 2.0073459148406982
Epoch 2920, training loss: 310.24505615234375 = 0.008407559245824814 + 50.0 * 6.204733371734619
Epoch 2920, val loss: 2.010073184967041
Epoch 2930, training loss: 310.5577392578125 = 0.008334011770784855 + 50.0 * 6.2109880447387695
Epoch 2930, val loss: 2.0134410858154297
Epoch 2940, training loss: 310.3948669433594 = 0.008259573020040989 + 50.0 * 6.207732200622559
Epoch 2940, val loss: 2.0147314071655273
Epoch 2950, training loss: 310.2105712890625 = 0.0081810113042593 + 50.0 * 6.204048156738281
Epoch 2950, val loss: 2.018610715866089
Epoch 2960, training loss: 310.20172119140625 = 0.008106755092740059 + 50.0 * 6.203872203826904
Epoch 2960, val loss: 2.020862340927124
Epoch 2970, training loss: 310.4994812011719 = 0.008036518469452858 + 50.0 * 6.209829330444336
Epoch 2970, val loss: 2.023913621902466
Epoch 2980, training loss: 310.2188720703125 = 0.007963381707668304 + 50.0 * 6.20421838760376
Epoch 2980, val loss: 2.026427984237671
Epoch 2990, training loss: 310.1560974121094 = 0.007893773727118969 + 50.0 * 6.202963829040527
Epoch 2990, val loss: 2.0291507244110107
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.8128624143384291
The final CL Acc:0.70494, 0.00761, The final GNN Acc:0.81374, 0.00203
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13200])
remove edge: torch.Size([2, 7984])
updated graph: torch.Size([2, 10628])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7995300292969 = 1.957827091217041 + 50.0 * 8.596834182739258
Epoch 0, val loss: 1.9604417085647583
Epoch 10, training loss: 431.7479248046875 = 1.948513388633728 + 50.0 * 8.595988273620605
Epoch 10, val loss: 1.9504367113113403
Epoch 20, training loss: 431.4609069824219 = 1.936911940574646 + 50.0 * 8.590479850769043
Epoch 20, val loss: 1.937641978263855
Epoch 30, training loss: 429.59375 = 1.9220175743103027 + 50.0 * 8.553434371948242
Epoch 30, val loss: 1.921026349067688
Epoch 40, training loss: 417.3868103027344 = 1.903281331062317 + 50.0 * 8.309670448303223
Epoch 40, val loss: 1.900654673576355
Epoch 50, training loss: 382.5571594238281 = 1.879333257675171 + 50.0 * 7.613556385040283
Epoch 50, val loss: 1.8750499486923218
Epoch 60, training loss: 373.18572998046875 = 1.8588587045669556 + 50.0 * 7.42653751373291
Epoch 60, val loss: 1.854786992073059
Epoch 70, training loss: 360.8770446777344 = 1.8457441329956055 + 50.0 * 7.180626392364502
Epoch 70, val loss: 1.842161774635315
Epoch 80, training loss: 354.0830383300781 = 1.8320807218551636 + 50.0 * 7.045019149780273
Epoch 80, val loss: 1.8291988372802734
Epoch 90, training loss: 348.8017272949219 = 1.8188904523849487 + 50.0 * 6.939657211303711
Epoch 90, val loss: 1.8171172142028809
Epoch 100, training loss: 344.9338684082031 = 1.8075847625732422 + 50.0 * 6.862525463104248
Epoch 100, val loss: 1.8067235946655273
Epoch 110, training loss: 341.1085205078125 = 1.798424482345581 + 50.0 * 6.7862019538879395
Epoch 110, val loss: 1.7983416318893433
Epoch 120, training loss: 338.0176696777344 = 1.7895482778549194 + 50.0 * 6.724562168121338
Epoch 120, val loss: 1.7901204824447632
Epoch 130, training loss: 335.3709716796875 = 1.7806110382080078 + 50.0 * 6.671807289123535
Epoch 130, val loss: 1.7819615602493286
Epoch 140, training loss: 333.31646728515625 = 1.7714937925338745 + 50.0 * 6.630899429321289
Epoch 140, val loss: 1.7735251188278198
Epoch 150, training loss: 331.8172912597656 = 1.7613176107406616 + 50.0 * 6.601119518280029
Epoch 150, val loss: 1.7640873193740845
Epoch 160, training loss: 330.6307678222656 = 1.74998140335083 + 50.0 * 6.577615737915039
Epoch 160, val loss: 1.7535574436187744
Epoch 170, training loss: 329.51385498046875 = 1.7375810146331787 + 50.0 * 6.555525302886963
Epoch 170, val loss: 1.7422622442245483
Epoch 180, training loss: 328.4602355957031 = 1.724149465560913 + 50.0 * 6.534721851348877
Epoch 180, val loss: 1.7300680875778198
Epoch 190, training loss: 327.6272277832031 = 1.7094742059707642 + 50.0 * 6.518355369567871
Epoch 190, val loss: 1.7169313430786133
Epoch 200, training loss: 326.7889099121094 = 1.6934700012207031 + 50.0 * 6.501908302307129
Epoch 200, val loss: 1.702778935432434
Epoch 210, training loss: 326.08935546875 = 1.6761090755462646 + 50.0 * 6.488265037536621
Epoch 210, val loss: 1.6875035762786865
Epoch 220, training loss: 325.5218200683594 = 1.6572327613830566 + 50.0 * 6.477292060852051
Epoch 220, val loss: 1.6709256172180176
Epoch 230, training loss: 324.9754943847656 = 1.6366794109344482 + 50.0 * 6.466776371002197
Epoch 230, val loss: 1.6530883312225342
Epoch 240, training loss: 324.4632568359375 = 1.6145113706588745 + 50.0 * 6.456974983215332
Epoch 240, val loss: 1.6339911222457886
Epoch 250, training loss: 323.99786376953125 = 1.5908596515655518 + 50.0 * 6.4481401443481445
Epoch 250, val loss: 1.6137464046478271
Epoch 260, training loss: 323.60443115234375 = 1.5655573606491089 + 50.0 * 6.44077730178833
Epoch 260, val loss: 1.5924043655395508
Epoch 270, training loss: 323.20257568359375 = 1.5389752388000488 + 50.0 * 6.433271884918213
Epoch 270, val loss: 1.570181131362915
Epoch 280, training loss: 322.8785400390625 = 1.5111680030822754 + 50.0 * 6.427347660064697
Epoch 280, val loss: 1.5472360849380493
Epoch 290, training loss: 322.47320556640625 = 1.4824438095092773 + 50.0 * 6.4198150634765625
Epoch 290, val loss: 1.5237611532211304
Epoch 300, training loss: 322.1181945800781 = 1.4527831077575684 + 50.0 * 6.413308143615723
Epoch 300, val loss: 1.4999693632125854
Epoch 310, training loss: 321.7686462402344 = 1.4226332902908325 + 50.0 * 6.406920433044434
Epoch 310, val loss: 1.4760682582855225
Epoch 320, training loss: 321.430419921875 = 1.392086148262024 + 50.0 * 6.400766372680664
Epoch 320, val loss: 1.4522192478179932
Epoch 330, training loss: 321.3822326660156 = 1.3613590002059937 + 50.0 * 6.400417327880859
Epoch 330, val loss: 1.428379774093628
Epoch 340, training loss: 320.91644287109375 = 1.3302205801010132 + 50.0 * 6.391724109649658
Epoch 340, val loss: 1.4045464992523193
Epoch 350, training loss: 320.5524597167969 = 1.2992688417434692 + 50.0 * 6.385064125061035
Epoch 350, val loss: 1.381264567375183
Epoch 360, training loss: 320.2873840332031 = 1.2685126066207886 + 50.0 * 6.380377292633057
Epoch 360, val loss: 1.3583645820617676
Epoch 370, training loss: 320.10552978515625 = 1.2378777265548706 + 50.0 * 6.377353191375732
Epoch 370, val loss: 1.3356349468231201
Epoch 380, training loss: 319.81298828125 = 1.2077018022537231 + 50.0 * 6.372105598449707
Epoch 380, val loss: 1.3133227825164795
Epoch 390, training loss: 319.5647277832031 = 1.1780292987823486 + 50.0 * 6.367733955383301
Epoch 390, val loss: 1.2919127941131592
Epoch 400, training loss: 319.381103515625 = 1.1488851308822632 + 50.0 * 6.3646440505981445
Epoch 400, val loss: 1.2711366415023804
Epoch 410, training loss: 319.1773986816406 = 1.1203047037124634 + 50.0 * 6.361141681671143
Epoch 410, val loss: 1.250639796257019
Epoch 420, training loss: 318.9480895996094 = 1.0923973321914673 + 50.0 * 6.357113838195801
Epoch 420, val loss: 1.2311874628067017
Epoch 430, training loss: 318.75433349609375 = 1.0653045177459717 + 50.0 * 6.353780746459961
Epoch 430, val loss: 1.2125052213668823
Epoch 440, training loss: 318.8624267578125 = 1.0389983654022217 + 50.0 * 6.356468200683594
Epoch 440, val loss: 1.194506287574768
Epoch 450, training loss: 318.36737060546875 = 1.0132797956466675 + 50.0 * 6.347082138061523
Epoch 450, val loss: 1.1772103309631348
Epoch 460, training loss: 318.203369140625 = 0.9884585738182068 + 50.0 * 6.344297885894775
Epoch 460, val loss: 1.1607351303100586
Epoch 470, training loss: 318.02020263671875 = 0.9644072651863098 + 50.0 * 6.341115951538086
Epoch 470, val loss: 1.1450165510177612
Epoch 480, training loss: 317.8866882324219 = 0.941191554069519 + 50.0 * 6.338910102844238
Epoch 480, val loss: 1.1301920413970947
Epoch 490, training loss: 317.7455139160156 = 0.9186465740203857 + 50.0 * 6.3365373611450195
Epoch 490, val loss: 1.1160777807235718
Epoch 500, training loss: 317.58148193359375 = 0.8968325853347778 + 50.0 * 6.333693027496338
Epoch 500, val loss: 1.1028202772140503
Epoch 510, training loss: 317.36834716796875 = 0.8758108615875244 + 50.0 * 6.329850196838379
Epoch 510, val loss: 1.0902540683746338
Epoch 520, training loss: 317.637939453125 = 0.8554903864860535 + 50.0 * 6.335649013519287
Epoch 520, val loss: 1.0783348083496094
Epoch 530, training loss: 317.12200927734375 = 0.8355859518051147 + 50.0 * 6.325728893280029
Epoch 530, val loss: 1.0672271251678467
Epoch 540, training loss: 316.9761657714844 = 0.8164210915565491 + 50.0 * 6.323194980621338
Epoch 540, val loss: 1.0567628145217896
Epoch 550, training loss: 316.9662780761719 = 0.7979225516319275 + 50.0 * 6.323367595672607
Epoch 550, val loss: 1.0469756126403809
Epoch 560, training loss: 316.87139892578125 = 0.7798370122909546 + 50.0 * 6.321831226348877
Epoch 560, val loss: 1.037561297416687
Epoch 570, training loss: 316.61175537109375 = 0.7623818516731262 + 50.0 * 6.31698751449585
Epoch 570, val loss: 1.0289405584335327
Epoch 580, training loss: 316.62957763671875 = 0.7455142736434937 + 50.0 * 6.317681312561035
Epoch 580, val loss: 1.0208778381347656
Epoch 590, training loss: 316.4471435546875 = 0.728977620601654 + 50.0 * 6.314363479614258
Epoch 590, val loss: 1.0137176513671875
Epoch 600, training loss: 316.3254089355469 = 0.7130632400512695 + 50.0 * 6.312246799468994
Epoch 600, val loss: 1.0063788890838623
Epoch 610, training loss: 316.16290283203125 = 0.6974743604660034 + 50.0 * 6.3093085289001465
Epoch 610, val loss: 1.0000853538513184
Epoch 620, training loss: 316.0879821777344 = 0.6824058890342712 + 50.0 * 6.308111667633057
Epoch 620, val loss: 0.9940966367721558
Epoch 630, training loss: 315.9508056640625 = 0.6675565838813782 + 50.0 * 6.305665493011475
Epoch 630, val loss: 0.988466739654541
Epoch 640, training loss: 315.8760070800781 = 0.6531603336334229 + 50.0 * 6.30445671081543
Epoch 640, val loss: 0.9832213521003723
Epoch 650, training loss: 315.76824951171875 = 0.6390933394432068 + 50.0 * 6.302582740783691
Epoch 650, val loss: 0.9784333109855652
Epoch 660, training loss: 315.8808898925781 = 0.6253288388252258 + 50.0 * 6.305110931396484
Epoch 660, val loss: 0.9739457964897156
Epoch 670, training loss: 315.5371398925781 = 0.6117494702339172 + 50.0 * 6.2985076904296875
Epoch 670, val loss: 0.9699370265007019
Epoch 680, training loss: 315.4790344238281 = 0.5985133647918701 + 50.0 * 6.297610759735107
Epoch 680, val loss: 0.9661729335784912
Epoch 690, training loss: 315.750732421875 = 0.585584819316864 + 50.0 * 6.303303241729736
Epoch 690, val loss: 0.9624971747398376
Epoch 700, training loss: 315.2799377441406 = 0.5726099610328674 + 50.0 * 6.29414701461792
Epoch 700, val loss: 0.9594126343727112
Epoch 710, training loss: 315.197265625 = 0.5600019097328186 + 50.0 * 6.292745590209961
Epoch 710, val loss: 0.9565812945365906
Epoch 720, training loss: 315.3447265625 = 0.5476611256599426 + 50.0 * 6.295941352844238
Epoch 720, val loss: 0.9539051055908203
Epoch 730, training loss: 315.1582336425781 = 0.5353682637214661 + 50.0 * 6.292457103729248
Epoch 730, val loss: 0.9511021375656128
Epoch 740, training loss: 314.9603271484375 = 0.5232328772544861 + 50.0 * 6.2887420654296875
Epoch 740, val loss: 0.9489713311195374
Epoch 750, training loss: 314.9378662109375 = 0.5113273859024048 + 50.0 * 6.2885308265686035
Epoch 750, val loss: 0.9468492269515991
Epoch 760, training loss: 314.8893737792969 = 0.4995077848434448 + 50.0 * 6.287797451019287
Epoch 760, val loss: 0.94496750831604
Epoch 770, training loss: 314.7046203613281 = 0.487806499004364 + 50.0 * 6.284336566925049
Epoch 770, val loss: 0.9432528018951416
Epoch 780, training loss: 314.67352294921875 = 0.47625625133514404 + 50.0 * 6.283945083618164
Epoch 780, val loss: 0.9418036937713623
Epoch 790, training loss: 314.8318176269531 = 0.4648250937461853 + 50.0 * 6.28734016418457
Epoch 790, val loss: 0.9404107332229614
Epoch 800, training loss: 314.6357421875 = 0.45351442694664 + 50.0 * 6.283644199371338
Epoch 800, val loss: 0.938766360282898
Epoch 810, training loss: 314.41644287109375 = 0.4422902762889862 + 50.0 * 6.279482841491699
Epoch 810, val loss: 0.9375504851341248
Epoch 820, training loss: 314.3639831542969 = 0.4312649369239807 + 50.0 * 6.278654098510742
Epoch 820, val loss: 0.9365946650505066
Epoch 830, training loss: 314.5354919433594 = 0.420438677072525 + 50.0 * 6.28230094909668
Epoch 830, val loss: 0.9355257153511047
Epoch 840, training loss: 314.4285888671875 = 0.40952280163764954 + 50.0 * 6.280381679534912
Epoch 840, val loss: 0.934914767742157
Epoch 850, training loss: 314.24774169921875 = 0.3988491892814636 + 50.0 * 6.276978015899658
Epoch 850, val loss: 0.9340692162513733
Epoch 860, training loss: 314.1527099609375 = 0.3883123993873596 + 50.0 * 6.275288105010986
Epoch 860, val loss: 0.9336118102073669
Epoch 870, training loss: 314.044677734375 = 0.37797781825065613 + 50.0 * 6.273333549499512
Epoch 870, val loss: 0.933424711227417
Epoch 880, training loss: 314.23016357421875 = 0.3678283393383026 + 50.0 * 6.277246952056885
Epoch 880, val loss: 0.9335354566574097
Epoch 890, training loss: 314.19561767578125 = 0.35777291655540466 + 50.0 * 6.276756763458252
Epoch 890, val loss: 0.9331697225570679
Epoch 900, training loss: 313.9130554199219 = 0.3479222059249878 + 50.0 * 6.271302700042725
Epoch 900, val loss: 0.9332513809204102
Epoch 910, training loss: 313.87103271484375 = 0.3383072018623352 + 50.0 * 6.270654201507568
Epoch 910, val loss: 0.9336575865745544
Epoch 920, training loss: 313.9974365234375 = 0.328926146030426 + 50.0 * 6.273370265960693
Epoch 920, val loss: 0.9342479109764099
Epoch 930, training loss: 313.79327392578125 = 0.31965646147727966 + 50.0 * 6.269472122192383
Epoch 930, val loss: 0.9347851872444153
Epoch 940, training loss: 313.7561340332031 = 0.31067314743995667 + 50.0 * 6.268909454345703
Epoch 940, val loss: 0.9356526136398315
Epoch 950, training loss: 313.73858642578125 = 0.3018955588340759 + 50.0 * 6.268733501434326
Epoch 950, val loss: 0.9365667104721069
Epoch 960, training loss: 313.6068420410156 = 0.29337000846862793 + 50.0 * 6.266269683837891
Epoch 960, val loss: 0.9376406669616699
Epoch 970, training loss: 314.1133728027344 = 0.28514400124549866 + 50.0 * 6.276564121246338
Epoch 970, val loss: 0.9385970830917358
Epoch 980, training loss: 313.5516357421875 = 0.27688124775886536 + 50.0 * 6.2654948234558105
Epoch 980, val loss: 0.9402061104774475
Epoch 990, training loss: 313.4947814941406 = 0.2690145671367645 + 50.0 * 6.264515399932861
Epoch 990, val loss: 0.94180828332901
Epoch 1000, training loss: 313.39605712890625 = 0.2614229917526245 + 50.0 * 6.262692928314209
Epoch 1000, val loss: 0.9434911608695984
Epoch 1010, training loss: 313.35406494140625 = 0.25406017899513245 + 50.0 * 6.26200008392334
Epoch 1010, val loss: 0.9454010725021362
Epoch 1020, training loss: 313.734375 = 0.24691157042980194 + 50.0 * 6.269749164581299
Epoch 1020, val loss: 0.9473139047622681
Epoch 1030, training loss: 313.4909362792969 = 0.23985552787780762 + 50.0 * 6.265021324157715
Epoch 1030, val loss: 0.9493622779846191
Epoch 1040, training loss: 313.2256774902344 = 0.23308707773685455 + 50.0 * 6.259851932525635
Epoch 1040, val loss: 0.9513705968856812
Epoch 1050, training loss: 313.2292175292969 = 0.22656592726707458 + 50.0 * 6.2600531578063965
Epoch 1050, val loss: 0.9536861777305603
Epoch 1060, training loss: 313.329345703125 = 0.22024105489253998 + 50.0 * 6.262181758880615
Epoch 1060, val loss: 0.9561565518379211
Epoch 1070, training loss: 313.30072021484375 = 0.21407043933868408 + 50.0 * 6.261733055114746
Epoch 1070, val loss: 0.9585536122322083
Epoch 1080, training loss: 313.1675720214844 = 0.2081163376569748 + 50.0 * 6.259189128875732
Epoch 1080, val loss: 0.9608796238899231
Epoch 1090, training loss: 313.1066589355469 = 0.202340230345726 + 50.0 * 6.258086681365967
Epoch 1090, val loss: 0.963642954826355
Epoch 1100, training loss: 313.0869140625 = 0.19676785171031952 + 50.0 * 6.257802486419678
Epoch 1100, val loss: 0.9664019346237183
Epoch 1110, training loss: 313.080322265625 = 0.19136810302734375 + 50.0 * 6.257779121398926
Epoch 1110, val loss: 0.9693462252616882
Epoch 1120, training loss: 312.9840087890625 = 0.1861046999692917 + 50.0 * 6.255958557128906
Epoch 1120, val loss: 0.9723086953163147
Epoch 1130, training loss: 312.9393310546875 = 0.1810491383075714 + 50.0 * 6.2551655769348145
Epoch 1130, val loss: 0.975311815738678
Epoch 1140, training loss: 312.95947265625 = 0.1761239469051361 + 50.0 * 6.255667209625244
Epoch 1140, val loss: 0.9784560799598694
Epoch 1150, training loss: 312.86883544921875 = 0.1713726818561554 + 50.0 * 6.253949165344238
Epoch 1150, val loss: 0.9816408753395081
Epoch 1160, training loss: 313.0578918457031 = 0.166763573884964 + 50.0 * 6.257822513580322
Epoch 1160, val loss: 0.9849683046340942
Epoch 1170, training loss: 312.8364562988281 = 0.16230390965938568 + 50.0 * 6.253483295440674
Epoch 1170, val loss: 0.9882476925849915
Epoch 1180, training loss: 312.7596435546875 = 0.15796788036823273 + 50.0 * 6.252033710479736
Epoch 1180, val loss: 0.9916790723800659
Epoch 1190, training loss: 312.7334289550781 = 0.15379783511161804 + 50.0 * 6.25159215927124
Epoch 1190, val loss: 0.9952059388160706
Epoch 1200, training loss: 312.8450927734375 = 0.14973649382591248 + 50.0 * 6.253906726837158
Epoch 1200, val loss: 0.9986775517463684
Epoch 1210, training loss: 312.9911193847656 = 0.14577944576740265 + 50.0 * 6.256906986236572
Epoch 1210, val loss: 1.0023295879364014
Epoch 1220, training loss: 312.8283386230469 = 0.14195352792739868 + 50.0 * 6.253727436065674
Epoch 1220, val loss: 1.0053144693374634
Epoch 1230, training loss: 312.5695495605469 = 0.1382150650024414 + 50.0 * 6.248626708984375
Epoch 1230, val loss: 1.0089898109436035
Epoch 1240, training loss: 312.5645751953125 = 0.13464665412902832 + 50.0 * 6.248598575592041
Epoch 1240, val loss: 1.0127748250961304
Epoch 1250, training loss: 312.5075378417969 = 0.13121415674686432 + 50.0 * 6.247526168823242
Epoch 1250, val loss: 1.0165308713912964
Epoch 1260, training loss: 313.07623291015625 = 0.1278984546661377 + 50.0 * 6.258966445922852
Epoch 1260, val loss: 1.02006995677948
Epoch 1270, training loss: 312.56256103515625 = 0.12455736845731735 + 50.0 * 6.248760223388672
Epoch 1270, val loss: 1.0239508152008057
Epoch 1280, training loss: 312.4547119140625 = 0.12138377875089645 + 50.0 * 6.246666431427002
Epoch 1280, val loss: 1.0277413129806519
Epoch 1290, training loss: 312.444091796875 = 0.11834261566400528 + 50.0 * 6.246514797210693
Epoch 1290, val loss: 1.0316367149353027
Epoch 1300, training loss: 312.70037841796875 = 0.11538711935281754 + 50.0 * 6.251699924468994
Epoch 1300, val loss: 1.0354875326156616
Epoch 1310, training loss: 312.4574279785156 = 0.112460657954216 + 50.0 * 6.246899127960205
Epoch 1310, val loss: 1.0393142700195312
Epoch 1320, training loss: 312.3716735839844 = 0.10967062413692474 + 50.0 * 6.245239734649658
Epoch 1320, val loss: 1.0433012247085571
Epoch 1330, training loss: 312.8590087890625 = 0.106967993080616 + 50.0 * 6.255041122436523
Epoch 1330, val loss: 1.047219157218933
Epoch 1340, training loss: 312.40521240234375 = 0.10429375618696213 + 50.0 * 6.246018409729004
Epoch 1340, val loss: 1.050954818725586
Epoch 1350, training loss: 312.2391662597656 = 0.10172116011381149 + 50.0 * 6.242748737335205
Epoch 1350, val loss: 1.054985523223877
Epoch 1360, training loss: 312.2306823730469 = 0.09924966841936111 + 50.0 * 6.242628574371338
Epoch 1360, val loss: 1.059061050415039
Epoch 1370, training loss: 312.4828186035156 = 0.09685306251049042 + 50.0 * 6.2477192878723145
Epoch 1370, val loss: 1.0632166862487793
Epoch 1380, training loss: 312.2628479003906 = 0.0945112556219101 + 50.0 * 6.243366718292236
Epoch 1380, val loss: 1.066840648651123
Epoch 1390, training loss: 312.218994140625 = 0.09221144020557404 + 50.0 * 6.24253511428833
Epoch 1390, val loss: 1.070957899093628
Epoch 1400, training loss: 312.1504211425781 = 0.09002979844808578 + 50.0 * 6.241208076477051
Epoch 1400, val loss: 1.0749675035476685
Epoch 1410, training loss: 312.13616943359375 = 0.08789434283971786 + 50.0 * 6.240965366363525
Epoch 1410, val loss: 1.079115867614746
Epoch 1420, training loss: 312.51141357421875 = 0.08582496643066406 + 50.0 * 6.248511791229248
Epoch 1420, val loss: 1.083036184310913
Epoch 1430, training loss: 312.09820556640625 = 0.08378737419843674 + 50.0 * 6.240288257598877
Epoch 1430, val loss: 1.0868796110153198
Epoch 1440, training loss: 312.1138610839844 = 0.08181118220090866 + 50.0 * 6.240641117095947
Epoch 1440, val loss: 1.0909618139266968
Epoch 1450, training loss: 312.20587158203125 = 0.07990045100450516 + 50.0 * 6.242519378662109
Epoch 1450, val loss: 1.0948567390441895
Epoch 1460, training loss: 312.0504455566406 = 0.07804545015096664 + 50.0 * 6.239448070526123
Epoch 1460, val loss: 1.0987358093261719
Epoch 1470, training loss: 311.9822998046875 = 0.0762471854686737 + 50.0 * 6.238121509552002
Epoch 1470, val loss: 1.1027803421020508
Epoch 1480, training loss: 311.9549560546875 = 0.07451868057250977 + 50.0 * 6.237608909606934
Epoch 1480, val loss: 1.1068309545516968
Epoch 1490, training loss: 312.3251647949219 = 0.07285446673631668 + 50.0 * 6.245046615600586
Epoch 1490, val loss: 1.1108651161193848
Epoch 1500, training loss: 312.08392333984375 = 0.07116737961769104 + 50.0 * 6.240255355834961
Epoch 1500, val loss: 1.1143654584884644
Epoch 1510, training loss: 311.9775695800781 = 0.06955491006374359 + 50.0 * 6.238160133361816
Epoch 1510, val loss: 1.1184873580932617
Epoch 1520, training loss: 311.95147705078125 = 0.06799736618995667 + 50.0 * 6.237669467926025
Epoch 1520, val loss: 1.1224267482757568
Epoch 1530, training loss: 312.0080871582031 = 0.06648528575897217 + 50.0 * 6.238831996917725
Epoch 1530, val loss: 1.1262385845184326
Epoch 1540, training loss: 312.04364013671875 = 0.06504040956497192 + 50.0 * 6.239572048187256
Epoch 1540, val loss: 1.1301289796829224
Epoch 1550, training loss: 311.90966796875 = 0.06356139481067657 + 50.0 * 6.236921787261963
Epoch 1550, val loss: 1.133784532546997
Epoch 1560, training loss: 311.802001953125 = 0.062168143689632416 + 50.0 * 6.23479700088501
Epoch 1560, val loss: 1.1376301050186157
Epoch 1570, training loss: 311.7718200683594 = 0.06082628667354584 + 50.0 * 6.234220027923584
Epoch 1570, val loss: 1.1415002346038818
Epoch 1580, training loss: 311.76055908203125 = 0.05952555686235428 + 50.0 * 6.234020233154297
Epoch 1580, val loss: 1.1453102827072144
Epoch 1590, training loss: 312.11541748046875 = 0.058279503136873245 + 50.0 * 6.241142272949219
Epoch 1590, val loss: 1.149122953414917
Epoch 1600, training loss: 311.9305419921875 = 0.05699513852596283 + 50.0 * 6.237470626831055
Epoch 1600, val loss: 1.1526833772659302
Epoch 1610, training loss: 311.8216247558594 = 0.055774588137865067 + 50.0 * 6.235316753387451
Epoch 1610, val loss: 1.1564743518829346
Epoch 1620, training loss: 311.74688720703125 = 0.05459409952163696 + 50.0 * 6.2338457107543945
Epoch 1620, val loss: 1.160314679145813
Epoch 1630, training loss: 311.736572265625 = 0.0534571036696434 + 50.0 * 6.233662128448486
Epoch 1630, val loss: 1.1640454530715942
Epoch 1640, training loss: 311.720703125 = 0.052347421646118164 + 50.0 * 6.233367443084717
Epoch 1640, val loss: 1.1677900552749634
Epoch 1650, training loss: 311.8401184082031 = 0.05127420648932457 + 50.0 * 6.235776901245117
Epoch 1650, val loss: 1.1714537143707275
Epoch 1660, training loss: 311.8328552246094 = 0.05020122602581978 + 50.0 * 6.235652923583984
Epoch 1660, val loss: 1.1749310493469238
Epoch 1670, training loss: 311.6656188964844 = 0.04916009679436684 + 50.0 * 6.23232889175415
Epoch 1670, val loss: 1.1786623001098633
Epoch 1680, training loss: 311.5909729003906 = 0.048159919679164886 + 50.0 * 6.230856418609619
Epoch 1680, val loss: 1.1824378967285156
Epoch 1690, training loss: 311.680419921875 = 0.04720810055732727 + 50.0 * 6.232664108276367
Epoch 1690, val loss: 1.1861966848373413
Epoch 1700, training loss: 311.6549987792969 = 0.04625216871500015 + 50.0 * 6.232174873352051
Epoch 1700, val loss: 1.1896003484725952
Epoch 1710, training loss: 311.5738830566406 = 0.04531082510948181 + 50.0 * 6.230571269989014
Epoch 1710, val loss: 1.1930853128433228
Epoch 1720, training loss: 311.6002502441406 = 0.04442933574318886 + 50.0 * 6.23111629486084
Epoch 1720, val loss: 1.196681022644043
Epoch 1730, training loss: 311.6789245605469 = 0.04355420917272568 + 50.0 * 6.232707500457764
Epoch 1730, val loss: 1.200148582458496
Epoch 1740, training loss: 311.6163330078125 = 0.04269828647375107 + 50.0 * 6.231472492218018
Epoch 1740, val loss: 1.2037498950958252
Epoch 1750, training loss: 311.5570983886719 = 0.041874635964632034 + 50.0 * 6.230304718017578
Epoch 1750, val loss: 1.2072025537490845
Epoch 1760, training loss: 311.5247802734375 = 0.04106450453400612 + 50.0 * 6.229674339294434
Epoch 1760, val loss: 1.2106882333755493
Epoch 1770, training loss: 311.5751037597656 = 0.040279630571603775 + 50.0 * 6.230696678161621
Epoch 1770, val loss: 1.2141541242599487
Epoch 1780, training loss: 311.5959777832031 = 0.03951423242688179 + 50.0 * 6.231129169464111
Epoch 1780, val loss: 1.2176316976547241
Epoch 1790, training loss: 311.5093078613281 = 0.038755811750888824 + 50.0 * 6.2294111251831055
Epoch 1790, val loss: 1.2209727764129639
Epoch 1800, training loss: 311.46185302734375 = 0.0380275659263134 + 50.0 * 6.228476524353027
Epoch 1800, val loss: 1.2244205474853516
Epoch 1810, training loss: 311.43853759765625 = 0.03732025995850563 + 50.0 * 6.228024482727051
Epoch 1810, val loss: 1.227845311164856
Epoch 1820, training loss: 311.5835266113281 = 0.03663863241672516 + 50.0 * 6.230937480926514
Epoch 1820, val loss: 1.231208324432373
Epoch 1830, training loss: 311.3977355957031 = 0.035948704928159714 + 50.0 * 6.227235794067383
Epoch 1830, val loss: 1.2344269752502441
Epoch 1840, training loss: 311.4830322265625 = 0.03528997674584389 + 50.0 * 6.228955268859863
Epoch 1840, val loss: 1.2376315593719482
Epoch 1850, training loss: 311.5243835449219 = 0.034657008945941925 + 50.0 * 6.229794025421143
Epoch 1850, val loss: 1.2409648895263672
Epoch 1860, training loss: 311.3670959472656 = 0.03401251137256622 + 50.0 * 6.226661682128906
Epoch 1860, val loss: 1.2441582679748535
Epoch 1870, training loss: 311.3441467285156 = 0.03340188413858414 + 50.0 * 6.22621488571167
Epoch 1870, val loss: 1.2474383115768433
Epoch 1880, training loss: 311.53759765625 = 0.03282855451107025 + 50.0 * 6.230094909667969
Epoch 1880, val loss: 1.250808596611023
Epoch 1890, training loss: 311.3157653808594 = 0.03222750127315521 + 50.0 * 6.22567081451416
Epoch 1890, val loss: 1.2537988424301147
Epoch 1900, training loss: 311.3232116699219 = 0.0316600501537323 + 50.0 * 6.225831031799316
Epoch 1900, val loss: 1.2570312023162842
Epoch 1910, training loss: 311.3351135253906 = 0.03110525943338871 + 50.0 * 6.226080417633057
Epoch 1910, val loss: 1.2602388858795166
Epoch 1920, training loss: 311.4151611328125 = 0.030567610636353493 + 50.0 * 6.227691650390625
Epoch 1920, val loss: 1.2634388208389282
Epoch 1930, training loss: 311.3305358886719 = 0.030039390549063683 + 50.0 * 6.226009845733643
Epoch 1930, val loss: 1.266442894935608
Epoch 1940, training loss: 311.21697998046875 = 0.029521381482481956 + 50.0 * 6.22374963760376
Epoch 1940, val loss: 1.2697261571884155
Epoch 1950, training loss: 311.2782897949219 = 0.029028430581092834 + 50.0 * 6.224985122680664
Epoch 1950, val loss: 1.2728906869888306
Epoch 1960, training loss: 311.5029296875 = 0.028547054156661034 + 50.0 * 6.229487895965576
Epoch 1960, val loss: 1.2760374546051025
Epoch 1970, training loss: 311.2923583984375 = 0.028051959350705147 + 50.0 * 6.22528600692749
Epoch 1970, val loss: 1.2787097692489624
Epoch 1980, training loss: 311.2106628417969 = 0.02757805958390236 + 50.0 * 6.223661422729492
Epoch 1980, val loss: 1.2818528413772583
Epoch 1990, training loss: 311.218505859375 = 0.02712964080274105 + 50.0 * 6.223827362060547
Epoch 1990, val loss: 1.2849162817001343
Epoch 2000, training loss: 311.29644775390625 = 0.02668599784374237 + 50.0 * 6.225395202636719
Epoch 2000, val loss: 1.2878897190093994
Epoch 2010, training loss: 311.3162841796875 = 0.026245325803756714 + 50.0 * 6.225800514221191
Epoch 2010, val loss: 1.2908685207366943
Epoch 2020, training loss: 311.17510986328125 = 0.025817187502980232 + 50.0 * 6.222985744476318
Epoch 2020, val loss: 1.2936118841171265
Epoch 2030, training loss: 311.1814880371094 = 0.025403697043657303 + 50.0 * 6.2231221199035645
Epoch 2030, val loss: 1.2966248989105225
Epoch 2040, training loss: 311.2493591308594 = 0.024993812665343285 + 50.0 * 6.2244873046875
Epoch 2040, val loss: 1.2993823289871216
Epoch 2050, training loss: 311.216552734375 = 0.024594632908701897 + 50.0 * 6.223838806152344
Epoch 2050, val loss: 1.302356243133545
Epoch 2060, training loss: 311.31280517578125 = 0.02421453595161438 + 50.0 * 6.225771903991699
Epoch 2060, val loss: 1.305234432220459
Epoch 2070, training loss: 311.1531982421875 = 0.023818034678697586 + 50.0 * 6.222587585449219
Epoch 2070, val loss: 1.307984471321106
Epoch 2080, training loss: 311.0765075683594 = 0.02344832383096218 + 50.0 * 6.2210612297058105
Epoch 2080, val loss: 1.310868501663208
Epoch 2090, training loss: 311.0383605957031 = 0.023088639602065086 + 50.0 * 6.2203049659729
Epoch 2090, val loss: 1.3136636018753052
Epoch 2100, training loss: 311.0457458496094 = 0.022740546613931656 + 50.0 * 6.220459938049316
Epoch 2100, val loss: 1.3164565563201904
Epoch 2110, training loss: 311.3597717285156 = 0.022403636947274208 + 50.0 * 6.226747512817383
Epoch 2110, val loss: 1.3191534280776978
Epoch 2120, training loss: 311.21441650390625 = 0.022051645442843437 + 50.0 * 6.223846912384033
Epoch 2120, val loss: 1.3219306468963623
Epoch 2130, training loss: 311.1575927734375 = 0.021715937182307243 + 50.0 * 6.22271728515625
Epoch 2130, val loss: 1.3244574069976807
Epoch 2140, training loss: 311.03497314453125 = 0.021380269899964333 + 50.0 * 6.220271587371826
Epoch 2140, val loss: 1.3271249532699585
Epoch 2150, training loss: 311.068115234375 = 0.021069694310426712 + 50.0 * 6.220941066741943
Epoch 2150, val loss: 1.3299951553344727
Epoch 2160, training loss: 311.1233215332031 = 0.02075495757162571 + 50.0 * 6.222051620483398
Epoch 2160, val loss: 1.3325376510620117
Epoch 2170, training loss: 311.0408935546875 = 0.020444655790925026 + 50.0 * 6.220408916473389
Epoch 2170, val loss: 1.3349571228027344
Epoch 2180, training loss: 310.96435546875 = 0.020149873569607735 + 50.0 * 6.218883991241455
Epoch 2180, val loss: 1.3377727270126343
Epoch 2190, training loss: 311.13525390625 = 0.019861891865730286 + 50.0 * 6.2223076820373535
Epoch 2190, val loss: 1.3401970863342285
Epoch 2200, training loss: 310.9869689941406 = 0.019573207944631577 + 50.0 * 6.219348430633545
Epoch 2200, val loss: 1.3427711725234985
Epoch 2210, training loss: 310.9447021484375 = 0.019295550882816315 + 50.0 * 6.218508243560791
Epoch 2210, val loss: 1.3454759120941162
Epoch 2220, training loss: 310.954833984375 = 0.01902155391871929 + 50.0 * 6.218716621398926
Epoch 2220, val loss: 1.3479613065719604
Epoch 2230, training loss: 311.13238525390625 = 0.018755974248051643 + 50.0 * 6.2222723960876465
Epoch 2230, val loss: 1.3504611253738403
Epoch 2240, training loss: 311.0132751464844 = 0.0184930432587862 + 50.0 * 6.219895362854004
Epoch 2240, val loss: 1.352947473526001
Epoch 2250, training loss: 310.9275817871094 = 0.01823202520608902 + 50.0 * 6.21818733215332
Epoch 2250, val loss: 1.3553818464279175
Epoch 2260, training loss: 310.9932861328125 = 0.017980320379137993 + 50.0 * 6.21950626373291
Epoch 2260, val loss: 1.3579224348068237
Epoch 2270, training loss: 310.9349670410156 = 0.017735380679368973 + 50.0 * 6.218344688415527
Epoch 2270, val loss: 1.3602705001831055
Epoch 2280, training loss: 310.9029235839844 = 0.01749035343527794 + 50.0 * 6.217708587646484
Epoch 2280, val loss: 1.3627084493637085
Epoch 2290, training loss: 311.115966796875 = 0.017253482714295387 + 50.0 * 6.2219743728637695
Epoch 2290, val loss: 1.364992618560791
Epoch 2300, training loss: 310.91961669921875 = 0.01702146790921688 + 50.0 * 6.218051910400391
Epoch 2300, val loss: 1.367504358291626
Epoch 2310, training loss: 310.84393310546875 = 0.016789879649877548 + 50.0 * 6.216542720794678
Epoch 2310, val loss: 1.369922399520874
Epoch 2320, training loss: 310.8504333496094 = 0.01657450757920742 + 50.0 * 6.216677188873291
Epoch 2320, val loss: 1.3725244998931885
Epoch 2330, training loss: 311.08087158203125 = 0.01636253483593464 + 50.0 * 6.221290111541748
Epoch 2330, val loss: 1.374863624572754
Epoch 2340, training loss: 310.9378356933594 = 0.016139138489961624 + 50.0 * 6.218433856964111
Epoch 2340, val loss: 1.3767967224121094
Epoch 2350, training loss: 310.8558044433594 = 0.015928708016872406 + 50.0 * 6.216797351837158
Epoch 2350, val loss: 1.3792978525161743
Epoch 2360, training loss: 310.88616943359375 = 0.015724554657936096 + 50.0 * 6.217409133911133
Epoch 2360, val loss: 1.3815582990646362
Epoch 2370, training loss: 310.9129333496094 = 0.015527041628956795 + 50.0 * 6.217947959899902
Epoch 2370, val loss: 1.3838891983032227
Epoch 2380, training loss: 310.8208923339844 = 0.015320941805839539 + 50.0 * 6.216111660003662
Epoch 2380, val loss: 1.3859350681304932
Epoch 2390, training loss: 310.9243469238281 = 0.01512907911092043 + 50.0 * 6.218184471130371
Epoch 2390, val loss: 1.3884360790252686
Epoch 2400, training loss: 310.8674621582031 = 0.014937127940356731 + 50.0 * 6.217050552368164
Epoch 2400, val loss: 1.3905694484710693
Epoch 2410, training loss: 310.7598876953125 = 0.014744347892701626 + 50.0 * 6.214902877807617
Epoch 2410, val loss: 1.392659068107605
Epoch 2420, training loss: 310.7198181152344 = 0.014564386568963528 + 50.0 * 6.214105129241943
Epoch 2420, val loss: 1.39506995677948
Epoch 2430, training loss: 310.83038330078125 = 0.014387287199497223 + 50.0 * 6.216320037841797
Epoch 2430, val loss: 1.3971880674362183
Epoch 2440, training loss: 310.7577819824219 = 0.014211182482540607 + 50.0 * 6.214870929718018
Epoch 2440, val loss: 1.3992911577224731
Epoch 2450, training loss: 310.71453857421875 = 0.014042163267731667 + 50.0 * 6.214009761810303
Epoch 2450, val loss: 1.401685118675232
Epoch 2460, training loss: 310.9115295410156 = 0.013875684700906277 + 50.0 * 6.217952728271484
Epoch 2460, val loss: 1.4037314653396606
Epoch 2470, training loss: 310.7572937011719 = 0.013702323660254478 + 50.0 * 6.214871883392334
Epoch 2470, val loss: 1.4057985544204712
Epoch 2480, training loss: 310.6727294921875 = 0.013541193678975105 + 50.0 * 6.213183879852295
Epoch 2480, val loss: 1.408048391342163
Epoch 2490, training loss: 310.8094177246094 = 0.013381400145590305 + 50.0 * 6.215920925140381
Epoch 2490, val loss: 1.4101876020431519
Epoch 2500, training loss: 310.688232421875 = 0.013222197070717812 + 50.0 * 6.213500499725342
Epoch 2500, val loss: 1.4122259616851807
Epoch 2510, training loss: 310.65362548828125 = 0.013069875538349152 + 50.0 * 6.21281099319458
Epoch 2510, val loss: 1.4143376350402832
Epoch 2520, training loss: 310.6328125 = 0.012917245738208294 + 50.0 * 6.212398052215576
Epoch 2520, val loss: 1.4165031909942627
Epoch 2530, training loss: 310.8375244140625 = 0.012775135226547718 + 50.0 * 6.216494560241699
Epoch 2530, val loss: 1.418393850326538
Epoch 2540, training loss: 310.6467590332031 = 0.012625395320355892 + 50.0 * 6.212682247161865
Epoch 2540, val loss: 1.4205800294876099
Epoch 2550, training loss: 310.59466552734375 = 0.012479598633944988 + 50.0 * 6.211643695831299
Epoch 2550, val loss: 1.4224917888641357
Epoch 2560, training loss: 310.6169738769531 = 0.012339034117758274 + 50.0 * 6.212092876434326
Epoch 2560, val loss: 1.42465078830719
Epoch 2570, training loss: 310.72784423828125 = 0.012206433340907097 + 50.0 * 6.214312553405762
Epoch 2570, val loss: 1.4265270233154297
Epoch 2580, training loss: 310.5541687011719 = 0.012067064642906189 + 50.0 * 6.210842132568359
Epoch 2580, val loss: 1.4285409450531006
Epoch 2590, training loss: 310.74554443359375 = 0.011939920485019684 + 50.0 * 6.214672088623047
Epoch 2590, val loss: 1.430500864982605
Epoch 2600, training loss: 310.6645812988281 = 0.011805118061602116 + 50.0 * 6.21305513381958
Epoch 2600, val loss: 1.4324842691421509
Epoch 2610, training loss: 310.668701171875 = 0.011673765257000923 + 50.0 * 6.21314001083374
Epoch 2610, val loss: 1.4343448877334595
Epoch 2620, training loss: 310.58209228515625 = 0.011546011082828045 + 50.0 * 6.211410999298096
Epoch 2620, val loss: 1.4363303184509277
Epoch 2630, training loss: 310.5989074707031 = 0.011422290466725826 + 50.0 * 6.211750030517578
Epoch 2630, val loss: 1.4380617141723633
Epoch 2640, training loss: 310.5400085449219 = 0.011300737038254738 + 50.0 * 6.210574626922607
Epoch 2640, val loss: 1.440106749534607
Epoch 2650, training loss: 310.5840759277344 = 0.01118446234613657 + 50.0 * 6.211458206176758
Epoch 2650, val loss: 1.4419881105422974
Epoch 2660, training loss: 310.595947265625 = 0.011066604405641556 + 50.0 * 6.211697578430176
Epoch 2660, val loss: 1.4437754154205322
Epoch 2670, training loss: 310.55615234375 = 0.010948103852570057 + 50.0 * 6.210903644561768
Epoch 2670, val loss: 1.4456335306167603
Epoch 2680, training loss: 311.07562255859375 = 0.010842952877283096 + 50.0 * 6.2212958335876465
Epoch 2680, val loss: 1.447596549987793
Epoch 2690, training loss: 310.6605224609375 = 0.010714853182435036 + 50.0 * 6.212996482849121
Epoch 2690, val loss: 1.4490909576416016
Epoch 2700, training loss: 310.4978332519531 = 0.010605120100080967 + 50.0 * 6.209744453430176
Epoch 2700, val loss: 1.4510304927825928
Epoch 2710, training loss: 310.4546203613281 = 0.01049637421965599 + 50.0 * 6.2088823318481445
Epoch 2710, val loss: 1.4527943134307861
Epoch 2720, training loss: 310.5519714355469 = 0.010395807214081287 + 50.0 * 6.210831165313721
Epoch 2720, val loss: 1.4546318054199219
Epoch 2730, training loss: 310.59234619140625 = 0.010290682315826416 + 50.0 * 6.211641311645508
Epoch 2730, val loss: 1.4562110900878906
Epoch 2740, training loss: 310.5484924316406 = 0.010182133875787258 + 50.0 * 6.210765838623047
Epoch 2740, val loss: 1.457973837852478
Epoch 2750, training loss: 310.70330810546875 = 0.01008277665823698 + 50.0 * 6.213864803314209
Epoch 2750, val loss: 1.4598358869552612
Epoch 2760, training loss: 310.4430236816406 = 0.009979278780519962 + 50.0 * 6.208661079406738
Epoch 2760, val loss: 1.461348295211792
Epoch 2770, training loss: 310.4368896484375 = 0.009882200509309769 + 50.0 * 6.208539962768555
Epoch 2770, val loss: 1.4631648063659668
Epoch 2780, training loss: 310.3999328613281 = 0.009787097573280334 + 50.0 * 6.207802772521973
Epoch 2780, val loss: 1.4648957252502441
Epoch 2790, training loss: 310.6285705566406 = 0.009696152992546558 + 50.0 * 6.212377071380615
Epoch 2790, val loss: 1.4664767980575562
Epoch 2800, training loss: 310.5057067871094 = 0.009599142707884312 + 50.0 * 6.209921836853027
Epoch 2800, val loss: 1.4681930541992188
Epoch 2810, training loss: 310.4954528808594 = 0.009502221830189228 + 50.0 * 6.209719181060791
Epoch 2810, val loss: 1.4696660041809082
Epoch 2820, training loss: 310.4081115722656 = 0.009410593658685684 + 50.0 * 6.207973957061768
Epoch 2820, val loss: 1.4715402126312256
Epoch 2830, training loss: 310.40057373046875 = 0.009325253777205944 + 50.0 * 6.20782470703125
Epoch 2830, val loss: 1.4731318950653076
Epoch 2840, training loss: 310.6593933105469 = 0.009242020547389984 + 50.0 * 6.213003158569336
Epoch 2840, val loss: 1.4746562242507935
Epoch 2850, training loss: 310.42230224609375 = 0.009150411002337933 + 50.0 * 6.208262920379639
Epoch 2850, val loss: 1.47622549533844
Epoch 2860, training loss: 310.5295104980469 = 0.009065431542694569 + 50.0 * 6.210408687591553
Epoch 2860, val loss: 1.4778028726577759
Epoch 2870, training loss: 310.4132995605469 = 0.008977462537586689 + 50.0 * 6.2080864906311035
Epoch 2870, val loss: 1.4793579578399658
Epoch 2880, training loss: 310.4150390625 = 0.00889531522989273 + 50.0 * 6.208123207092285
Epoch 2880, val loss: 1.481025218963623
Epoch 2890, training loss: 310.352294921875 = 0.00881523173302412 + 50.0 * 6.206869602203369
Epoch 2890, val loss: 1.4826014041900635
Epoch 2900, training loss: 310.37872314453125 = 0.008737837895751 + 50.0 * 6.207399845123291
Epoch 2900, val loss: 1.4841605424880981
Epoch 2910, training loss: 310.4834899902344 = 0.008660677820444107 + 50.0 * 6.20949649810791
Epoch 2910, val loss: 1.4855424165725708
Epoch 2920, training loss: 310.4371643066406 = 0.008582072332501411 + 50.0 * 6.208571910858154
Epoch 2920, val loss: 1.4869918823242188
Epoch 2930, training loss: 310.4149169921875 = 0.008504930883646011 + 50.0 * 6.208127975463867
Epoch 2930, val loss: 1.4885284900665283
Epoch 2940, training loss: 310.4578552246094 = 0.008429108187556267 + 50.0 * 6.208988666534424
Epoch 2940, val loss: 1.4900643825531006
Epoch 2950, training loss: 310.3867492675781 = 0.008355836383998394 + 50.0 * 6.2075676918029785
Epoch 2950, val loss: 1.4916890859603882
Epoch 2960, training loss: 310.3666076660156 = 0.00828428566455841 + 50.0 * 6.20716667175293
Epoch 2960, val loss: 1.4930695295333862
Epoch 2970, training loss: 310.3125305175781 = 0.008212415501475334 + 50.0 * 6.206086158752441
Epoch 2970, val loss: 1.494465947151184
Epoch 2980, training loss: 310.3490295410156 = 0.008143313229084015 + 50.0 * 6.206817626953125
Epoch 2980, val loss: 1.4958360195159912
Epoch 2990, training loss: 310.4083557128906 = 0.00807582214474678 + 50.0 * 6.208005428314209
Epoch 2990, val loss: 1.4972022771835327
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 431.7843933105469 = 1.941088318824768 + 50.0 * 8.5968656539917
Epoch 0, val loss: 1.936427116394043
Epoch 10, training loss: 431.7458801269531 = 1.9319795370101929 + 50.0 * 8.596278190612793
Epoch 10, val loss: 1.9269664287567139
Epoch 20, training loss: 431.521484375 = 1.920791745185852 + 50.0 * 8.592013359069824
Epoch 20, val loss: 1.9153636693954468
Epoch 30, training loss: 429.9233703613281 = 1.9063478708267212 + 50.0 * 8.560340881347656
Epoch 30, val loss: 1.9005199670791626
Epoch 40, training loss: 419.61065673828125 = 1.8882578611373901 + 50.0 * 8.354448318481445
Epoch 40, val loss: 1.8826165199279785
Epoch 50, training loss: 381.7128601074219 = 1.8679335117340088 + 50.0 * 7.596898555755615
Epoch 50, val loss: 1.8630002737045288
Epoch 60, training loss: 365.2909851074219 = 1.8526588678359985 + 50.0 * 7.268766403198242
Epoch 60, val loss: 1.848752737045288
Epoch 70, training loss: 354.0653991699219 = 1.8399443626403809 + 50.0 * 7.044509410858154
Epoch 70, val loss: 1.836525321006775
Epoch 80, training loss: 348.74517822265625 = 1.8268502950668335 + 50.0 * 6.938366889953613
Epoch 80, val loss: 1.8242347240447998
Epoch 90, training loss: 345.3294677734375 = 1.8146679401397705 + 50.0 * 6.870296001434326
Epoch 90, val loss: 1.81277596950531
Epoch 100, training loss: 341.5562744140625 = 1.8025559186935425 + 50.0 * 6.795074462890625
Epoch 100, val loss: 1.8016626834869385
Epoch 110, training loss: 337.7698669433594 = 1.7921749353408813 + 50.0 * 6.7195539474487305
Epoch 110, val loss: 1.7921777963638306
Epoch 120, training loss: 334.8866271972656 = 1.782686471939087 + 50.0 * 6.662078857421875
Epoch 120, val loss: 1.7833057641983032
Epoch 130, training loss: 332.7861022949219 = 1.772438406944275 + 50.0 * 6.620273590087891
Epoch 130, val loss: 1.7736282348632812
Epoch 140, training loss: 331.0999450683594 = 1.7610589265823364 + 50.0 * 6.586778163909912
Epoch 140, val loss: 1.7630259990692139
Epoch 150, training loss: 329.5727233886719 = 1.7489420175552368 + 50.0 * 6.55647611618042
Epoch 150, val loss: 1.7517380714416504
Epoch 160, training loss: 328.353515625 = 1.736010193824768 + 50.0 * 6.532349586486816
Epoch 160, val loss: 1.7398247718811035
Epoch 170, training loss: 327.4014587402344 = 1.72206711769104 + 50.0 * 6.513587951660156
Epoch 170, val loss: 1.7270234823226929
Epoch 180, training loss: 326.52081298828125 = 1.7068309783935547 + 50.0 * 6.496280193328857
Epoch 180, val loss: 1.713145136833191
Epoch 190, training loss: 325.6844787597656 = 1.6902965307235718 + 50.0 * 6.479883193969727
Epoch 190, val loss: 1.6980998516082764
Epoch 200, training loss: 325.32080078125 = 1.6723567247390747 + 50.0 * 6.472969055175781
Epoch 200, val loss: 1.681929111480713
Epoch 210, training loss: 324.3717346191406 = 1.652376413345337 + 50.0 * 6.454387187957764
Epoch 210, val loss: 1.6641006469726562
Epoch 220, training loss: 323.80133056640625 = 1.63090980052948 + 50.0 * 6.443408489227295
Epoch 220, val loss: 1.6450676918029785
Epoch 230, training loss: 323.2601013183594 = 1.6078485250473022 + 50.0 * 6.433044910430908
Epoch 230, val loss: 1.6245989799499512
Epoch 240, training loss: 322.9514465332031 = 1.5831959247589111 + 50.0 * 6.427364826202393
Epoch 240, val loss: 1.6029529571533203
Epoch 250, training loss: 322.52490234375 = 1.556943416595459 + 50.0 * 6.41935920715332
Epoch 250, val loss: 1.5801037549972534
Epoch 260, training loss: 322.00250244140625 = 1.5295696258544922 + 50.0 * 6.409458637237549
Epoch 260, val loss: 1.5564651489257812
Epoch 270, training loss: 321.5978698730469 = 1.501114010810852 + 50.0 * 6.401934623718262
Epoch 270, val loss: 1.5322010517120361
Epoch 280, training loss: 321.27752685546875 = 1.471860647201538 + 50.0 * 6.396113395690918
Epoch 280, val loss: 1.5074729919433594
Epoch 290, training loss: 321.19122314453125 = 1.4417599439620972 + 50.0 * 6.394989013671875
Epoch 290, val loss: 1.4824192523956299
Epoch 300, training loss: 320.6840515136719 = 1.411497950553894 + 50.0 * 6.385450839996338
Epoch 300, val loss: 1.4574929475784302
Epoch 310, training loss: 320.389404296875 = 1.3811426162719727 + 50.0 * 6.3801655769348145
Epoch 310, val loss: 1.4326828718185425
Epoch 320, training loss: 320.1339416503906 = 1.350764274597168 + 50.0 * 6.3756632804870605
Epoch 320, val loss: 1.4083938598632812
Epoch 330, training loss: 319.9595642089844 = 1.3204271793365479 + 50.0 * 6.3727827072143555
Epoch 330, val loss: 1.3842378854751587
Epoch 340, training loss: 319.64520263671875 = 1.2903306484222412 + 50.0 * 6.3670973777771
Epoch 340, val loss: 1.3607243299484253
Epoch 350, training loss: 319.5283508300781 = 1.2605080604553223 + 50.0 * 6.365356922149658
Epoch 350, val loss: 1.3375890254974365
Epoch 360, training loss: 319.3175354003906 = 1.2310069799423218 + 50.0 * 6.361730575561523
Epoch 360, val loss: 1.314856767654419
Epoch 370, training loss: 318.9596252441406 = 1.2018550634384155 + 50.0 * 6.3551554679870605
Epoch 370, val loss: 1.292802333831787
Epoch 380, training loss: 318.8026123046875 = 1.1731740236282349 + 50.0 * 6.352588653564453
Epoch 380, val loss: 1.2712197303771973
Epoch 390, training loss: 318.5264587402344 = 1.1448875665664673 + 50.0 * 6.347631454467773
Epoch 390, val loss: 1.2498787641525269
Epoch 400, training loss: 318.35675048828125 = 1.1170369386672974 + 50.0 * 6.344794273376465
Epoch 400, val loss: 1.2292354106903076
Epoch 410, training loss: 318.3533020019531 = 1.0897578001022339 + 50.0 * 6.345271110534668
Epoch 410, val loss: 1.2092514038085938
Epoch 420, training loss: 317.9602355957031 = 1.0629706382751465 + 50.0 * 6.337945461273193
Epoch 420, val loss: 1.1896097660064697
Epoch 430, training loss: 317.8954772949219 = 1.0368807315826416 + 50.0 * 6.33717155456543
Epoch 430, val loss: 1.1706030368804932
Epoch 440, training loss: 317.7598876953125 = 1.0113801956176758 + 50.0 * 6.334969997406006
Epoch 440, val loss: 1.152382493019104
Epoch 450, training loss: 317.47100830078125 = 0.9865673184394836 + 50.0 * 6.329688549041748
Epoch 450, val loss: 1.1349352598190308
Epoch 460, training loss: 317.3489990234375 = 0.9625241756439209 + 50.0 * 6.32772970199585
Epoch 460, val loss: 1.1182094812393188
Epoch 470, training loss: 317.1539001464844 = 0.9392079710960388 + 50.0 * 6.324293613433838
Epoch 470, val loss: 1.1024446487426758
Epoch 480, training loss: 317.04180908203125 = 0.9167010188102722 + 50.0 * 6.322502136230469
Epoch 480, val loss: 1.0875450372695923
Epoch 490, training loss: 316.9727783203125 = 0.8949341773986816 + 50.0 * 6.32155704498291
Epoch 490, val loss: 1.0731996297836304
Epoch 500, training loss: 316.94024658203125 = 0.8737300038337708 + 50.0 * 6.321330547332764
Epoch 500, val loss: 1.0596282482147217
Epoch 510, training loss: 316.6352233886719 = 0.8532507419586182 + 50.0 * 6.315639495849609
Epoch 510, val loss: 1.046988606452942
Epoch 520, training loss: 316.4561767578125 = 0.8334736227989197 + 50.0 * 6.3124542236328125
Epoch 520, val loss: 1.035120964050293
Epoch 530, training loss: 316.39874267578125 = 0.8143325448036194 + 50.0 * 6.311688423156738
Epoch 530, val loss: 1.0239546298980713
Epoch 540, training loss: 316.4055480957031 = 0.7956461310386658 + 50.0 * 6.312198162078857
Epoch 540, val loss: 1.0134812593460083
Epoch 550, training loss: 316.1263122558594 = 0.7775588631629944 + 50.0 * 6.306975364685059
Epoch 550, val loss: 1.0036611557006836
Epoch 560, training loss: 315.984130859375 = 0.7600513100624084 + 50.0 * 6.3044819831848145
Epoch 560, val loss: 0.9945511221885681
Epoch 570, training loss: 315.89227294921875 = 0.7430683970451355 + 50.0 * 6.30298376083374
Epoch 570, val loss: 0.9861661791801453
Epoch 580, training loss: 316.0789489746094 = 0.7265154719352722 + 50.0 * 6.307048797607422
Epoch 580, val loss: 0.9783260822296143
Epoch 590, training loss: 315.7778015136719 = 0.7103848457336426 + 50.0 * 6.3013482093811035
Epoch 590, val loss: 0.9709010124206543
Epoch 600, training loss: 315.66546630859375 = 0.6945728659629822 + 50.0 * 6.299417972564697
Epoch 600, val loss: 0.9640313982963562
Epoch 610, training loss: 315.530517578125 = 0.6791471242904663 + 50.0 * 6.297027587890625
Epoch 610, val loss: 0.9575898051261902
Epoch 620, training loss: 315.410888671875 = 0.6641285419464111 + 50.0 * 6.29493522644043
Epoch 620, val loss: 0.9515716433525085
Epoch 630, training loss: 315.68829345703125 = 0.6493396162986755 + 50.0 * 6.300778865814209
Epoch 630, val loss: 0.9459344744682312
Epoch 640, training loss: 315.2971496582031 = 0.634909987449646 + 50.0 * 6.293244361877441
Epoch 640, val loss: 0.9404857754707336
Epoch 650, training loss: 315.1467590332031 = 0.6206718683242798 + 50.0 * 6.29052209854126
Epoch 650, val loss: 0.9356576204299927
Epoch 660, training loss: 315.0591125488281 = 0.6068184971809387 + 50.0 * 6.289046287536621
Epoch 660, val loss: 0.9311147332191467
Epoch 670, training loss: 314.9991760253906 = 0.5932471752166748 + 50.0 * 6.288118362426758
Epoch 670, val loss: 0.92673259973526
Epoch 680, training loss: 315.1899108886719 = 0.5797154903411865 + 50.0 * 6.292203903198242
Epoch 680, val loss: 0.9227096438407898
Epoch 690, training loss: 314.8539123535156 = 0.5665053129196167 + 50.0 * 6.285748481750488
Epoch 690, val loss: 0.9187387228012085
Epoch 700, training loss: 314.7352600097656 = 0.5534815788269043 + 50.0 * 6.28363561630249
Epoch 700, val loss: 0.9150701761245728
Epoch 710, training loss: 314.6647033691406 = 0.5407711863517761 + 50.0 * 6.2824788093566895
Epoch 710, val loss: 0.9118090271949768
Epoch 720, training loss: 314.89599609375 = 0.5282537341117859 + 50.0 * 6.287354469299316
Epoch 720, val loss: 0.908680260181427
Epoch 730, training loss: 314.75323486328125 = 0.5158423781394958 + 50.0 * 6.284748077392578
Epoch 730, val loss: 0.9055956602096558
Epoch 740, training loss: 314.46282958984375 = 0.5036130547523499 + 50.0 * 6.279184818267822
Epoch 740, val loss: 0.9027195572853088
Epoch 750, training loss: 314.4189758300781 = 0.4916285276412964 + 50.0 * 6.2785468101501465
Epoch 750, val loss: 0.9001333713531494
Epoch 760, training loss: 314.6171569824219 = 0.47986117005348206 + 50.0 * 6.282745838165283
Epoch 760, val loss: 0.8976556062698364
Epoch 770, training loss: 314.4937744140625 = 0.4681338965892792 + 50.0 * 6.280512809753418
Epoch 770, val loss: 0.8953136205673218
Epoch 780, training loss: 314.2434387207031 = 0.456603080034256 + 50.0 * 6.2757368087768555
Epoch 780, val loss: 0.8930083513259888
Epoch 790, training loss: 314.2683410644531 = 0.445264607667923 + 50.0 * 6.276462078094482
Epoch 790, val loss: 0.8908958435058594
Epoch 800, training loss: 314.0867614746094 = 0.4341217875480652 + 50.0 * 6.273053169250488
Epoch 800, val loss: 0.8888971209526062
Epoch 810, training loss: 314.0428466796875 = 0.42316681146621704 + 50.0 * 6.272393703460693
Epoch 810, val loss: 0.8871247172355652
Epoch 820, training loss: 314.25933837890625 = 0.41234758496284485 + 50.0 * 6.276939868927002
Epoch 820, val loss: 0.8853991031646729
Epoch 830, training loss: 314.1562194824219 = 0.40160930156707764 + 50.0 * 6.275092124938965
Epoch 830, val loss: 0.8838374018669128
Epoch 840, training loss: 313.9593505859375 = 0.3910200893878937 + 50.0 * 6.271366596221924
Epoch 840, val loss: 0.8822311758995056
Epoch 850, training loss: 313.9461669921875 = 0.3806683123111725 + 50.0 * 6.271309852600098
Epoch 850, val loss: 0.8810163736343384
Epoch 860, training loss: 313.77398681640625 = 0.3705174922943115 + 50.0 * 6.268069267272949
Epoch 860, val loss: 0.8798083662986755
Epoch 870, training loss: 313.706787109375 = 0.3605550229549408 + 50.0 * 6.2669243812561035
Epoch 870, val loss: 0.8788849711418152
Epoch 880, training loss: 313.72100830078125 = 0.35079678893089294 + 50.0 * 6.267404079437256
Epoch 880, val loss: 0.8780642747879028
Epoch 890, training loss: 313.9769287109375 = 0.34118300676345825 + 50.0 * 6.272714614868164
Epoch 890, val loss: 0.877191424369812
Epoch 900, training loss: 313.7547912597656 = 0.33159953355789185 + 50.0 * 6.268463611602783
Epoch 900, val loss: 0.8767794370651245
Epoch 910, training loss: 313.4987487792969 = 0.32233333587646484 + 50.0 * 6.263528347015381
Epoch 910, val loss: 0.8764042854309082
Epoch 920, training loss: 313.46649169921875 = 0.31334325671195984 + 50.0 * 6.263062953948975
Epoch 920, val loss: 0.876277506351471
Epoch 930, training loss: 313.4049377441406 = 0.3045720160007477 + 50.0 * 6.262007236480713
Epoch 930, val loss: 0.8764554262161255
Epoch 940, training loss: 313.7972106933594 = 0.2960200607776642 + 50.0 * 6.270023822784424
Epoch 940, val loss: 0.8766174912452698
Epoch 950, training loss: 313.51043701171875 = 0.28747743368148804 + 50.0 * 6.264459133148193
Epoch 950, val loss: 0.8769460320472717
Epoch 960, training loss: 313.3006286621094 = 0.2792713940143585 + 50.0 * 6.260427474975586
Epoch 960, val loss: 0.8774473071098328
Epoch 970, training loss: 313.229736328125 = 0.27127552032470703 + 50.0 * 6.259169578552246
Epoch 970, val loss: 0.8782933950424194
Epoch 980, training loss: 313.387451171875 = 0.2635267376899719 + 50.0 * 6.262478828430176
Epoch 980, val loss: 0.879211962223053
Epoch 990, training loss: 313.4100036621094 = 0.2559545040130615 + 50.0 * 6.263081073760986
Epoch 990, val loss: 0.8800740838050842
Epoch 1000, training loss: 313.1394348144531 = 0.24850638210773468 + 50.0 * 6.257818698883057
Epoch 1000, val loss: 0.8816521167755127
Epoch 1010, training loss: 313.0600280761719 = 0.24138690531253815 + 50.0 * 6.256372928619385
Epoch 1010, val loss: 0.8830670118331909
Epoch 1020, training loss: 313.0318298339844 = 0.23449291288852692 + 50.0 * 6.255946636199951
Epoch 1020, val loss: 0.8848918080329895
Epoch 1030, training loss: 313.2843322753906 = 0.22776424884796143 + 50.0 * 6.261131763458252
Epoch 1030, val loss: 0.8868817090988159
Epoch 1040, training loss: 313.1705627441406 = 0.22117778658866882 + 50.0 * 6.2589874267578125
Epoch 1040, val loss: 0.8888331055641174
Epoch 1050, training loss: 313.1372985839844 = 0.2148391604423523 + 50.0 * 6.258449077606201
Epoch 1050, val loss: 0.8909385800361633
Epoch 1060, training loss: 312.89013671875 = 0.20866659283638 + 50.0 * 6.253629207611084
Epoch 1060, val loss: 0.8932814002037048
Epoch 1070, training loss: 312.84112548828125 = 0.20272043347358704 + 50.0 * 6.252768039703369
Epoch 1070, val loss: 0.8959007859230042
Epoch 1080, training loss: 312.7857666015625 = 0.19700314104557037 + 50.0 * 6.251775741577148
Epoch 1080, val loss: 0.8986460566520691
Epoch 1090, training loss: 312.91668701171875 = 0.19147957861423492 + 50.0 * 6.254504203796387
Epoch 1090, val loss: 0.9014139175415039
Epoch 1100, training loss: 312.7878112792969 = 0.18598046898841858 + 50.0 * 6.2520365715026855
Epoch 1100, val loss: 0.9046469330787659
Epoch 1110, training loss: 312.8332824707031 = 0.18074287474155426 + 50.0 * 6.253050804138184
Epoch 1110, val loss: 0.9075399041175842
Epoch 1120, training loss: 312.66064453125 = 0.17563533782958984 + 50.0 * 6.249700546264648
Epoch 1120, val loss: 0.9110461473464966
Epoch 1130, training loss: 312.6241455078125 = 0.17075686156749725 + 50.0 * 6.249068260192871
Epoch 1130, val loss: 0.9145110249519348
Epoch 1140, training loss: 313.02410888671875 = 0.1660328507423401 + 50.0 * 6.2571611404418945
Epoch 1140, val loss: 0.9178863763809204
Epoch 1150, training loss: 312.72052001953125 = 0.16136787831783295 + 50.0 * 6.251182556152344
Epoch 1150, val loss: 0.9218060374259949
Epoch 1160, training loss: 312.59283447265625 = 0.1569048911333084 + 50.0 * 6.248718738555908
Epoch 1160, val loss: 0.9254591464996338
Epoch 1170, training loss: 312.8825988769531 = 0.1525752693414688 + 50.0 * 6.2546000480651855
Epoch 1170, val loss: 0.9295448064804077
Epoch 1180, training loss: 312.7064208984375 = 0.14837336540222168 + 50.0 * 6.251161098480225
Epoch 1180, val loss: 0.9330415725708008
Epoch 1190, training loss: 312.49652099609375 = 0.1442568600177765 + 50.0 * 6.247045040130615
Epoch 1190, val loss: 0.9373400807380676
Epoch 1200, training loss: 312.4075012207031 = 0.1403367817401886 + 50.0 * 6.24534273147583
Epoch 1200, val loss: 0.9415620565414429
Epoch 1210, training loss: 312.38189697265625 = 0.13654516637325287 + 50.0 * 6.244907379150391
Epoch 1210, val loss: 0.9459203481674194
Epoch 1220, training loss: 312.47235107421875 = 0.13287675380706787 + 50.0 * 6.246789455413818
Epoch 1220, val loss: 0.95028156042099
Epoch 1230, training loss: 312.4119567871094 = 0.1292618364095688 + 50.0 * 6.2456536293029785
Epoch 1230, val loss: 0.9548889994621277
Epoch 1240, training loss: 312.39599609375 = 0.12575027346611023 + 50.0 * 6.2454047203063965
Epoch 1240, val loss: 0.9590901732444763
Epoch 1250, training loss: 312.3502197265625 = 0.12232686579227448 + 50.0 * 6.244558334350586
Epoch 1250, val loss: 0.9640237092971802
Epoch 1260, training loss: 312.2471008300781 = 0.1190648078918457 + 50.0 * 6.242560386657715
Epoch 1260, val loss: 0.9687070250511169
Epoch 1270, training loss: 312.268798828125 = 0.1159142255783081 + 50.0 * 6.243057727813721
Epoch 1270, val loss: 0.9736620783805847
Epoch 1280, training loss: 312.51318359375 = 0.11282390356063843 + 50.0 * 6.248007297515869
Epoch 1280, val loss: 0.9786113500595093
Epoch 1290, training loss: 312.27447509765625 = 0.10984888672828674 + 50.0 * 6.243292331695557
Epoch 1290, val loss: 0.9830979108810425
Epoch 1300, training loss: 312.2024841308594 = 0.10694323480129242 + 50.0 * 6.241910934448242
Epoch 1300, val loss: 0.988192617893219
Epoch 1310, training loss: 312.2474365234375 = 0.10414857417345047 + 50.0 * 6.242865562438965
Epoch 1310, val loss: 0.9932790398597717
Epoch 1320, training loss: 312.1294250488281 = 0.1014171615242958 + 50.0 * 6.240560054779053
Epoch 1320, val loss: 0.9983100891113281
Epoch 1330, training loss: 312.19915771484375 = 0.09879059344530106 + 50.0 * 6.242007732391357
Epoch 1330, val loss: 1.0034611225128174
Epoch 1340, training loss: 312.1003723144531 = 0.0962272509932518 + 50.0 * 6.240082740783691
Epoch 1340, val loss: 1.0086637735366821
Epoch 1350, training loss: 312.24444580078125 = 0.09376010298728943 + 50.0 * 6.243013858795166
Epoch 1350, val loss: 1.0138760805130005
Epoch 1360, training loss: 312.13043212890625 = 0.09132002294063568 + 50.0 * 6.240782260894775
Epoch 1360, val loss: 1.0189530849456787
Epoch 1370, training loss: 312.2260437011719 = 0.08898884803056717 + 50.0 * 6.242741107940674
Epoch 1370, val loss: 1.0240329504013062
Epoch 1380, training loss: 312.2596740722656 = 0.08670934289693832 + 50.0 * 6.243459701538086
Epoch 1380, val loss: 1.0290879011154175
Epoch 1390, training loss: 312.01800537109375 = 0.0844661146402359 + 50.0 * 6.238670825958252
Epoch 1390, val loss: 1.034610629081726
Epoch 1400, training loss: 311.9400939941406 = 0.08234027028083801 + 50.0 * 6.237155437469482
Epoch 1400, val loss: 1.0399301052093506
Epoch 1410, training loss: 311.9491271972656 = 0.08028293401002884 + 50.0 * 6.237376689910889
Epoch 1410, val loss: 1.045436978340149
Epoch 1420, training loss: 312.13250732421875 = 0.0782737135887146 + 50.0 * 6.241084575653076
Epoch 1420, val loss: 1.050938606262207
Epoch 1430, training loss: 311.9584045410156 = 0.0763096809387207 + 50.0 * 6.237641334533691
Epoch 1430, val loss: 1.0560811758041382
Epoch 1440, training loss: 311.84381103515625 = 0.07441811263561249 + 50.0 * 6.235387325286865
Epoch 1440, val loss: 1.061432957649231
Epoch 1450, training loss: 311.99969482421875 = 0.07259278744459152 + 50.0 * 6.238542079925537
Epoch 1450, val loss: 1.066910982131958
Epoch 1460, training loss: 311.9232177734375 = 0.07078664004802704 + 50.0 * 6.237048625946045
Epoch 1460, val loss: 1.0719927549362183
Epoch 1470, training loss: 311.90655517578125 = 0.06902995705604553 + 50.0 * 6.236750602722168
Epoch 1470, val loss: 1.0775001049041748
Epoch 1480, training loss: 311.8108825683594 = 0.06734418869018555 + 50.0 * 6.234870910644531
Epoch 1480, val loss: 1.0828386545181274
Epoch 1490, training loss: 311.74560546875 = 0.06572765856981277 + 50.0 * 6.233597278594971
Epoch 1490, val loss: 1.088322639465332
Epoch 1500, training loss: 312.0957336425781 = 0.06419070065021515 + 50.0 * 6.240631103515625
Epoch 1500, val loss: 1.093485713005066
Epoch 1510, training loss: 311.8186950683594 = 0.06258504837751389 + 50.0 * 6.235122203826904
Epoch 1510, val loss: 1.0992058515548706
Epoch 1520, training loss: 311.8329772949219 = 0.06110268458724022 + 50.0 * 6.235437870025635
Epoch 1520, val loss: 1.1042283773422241
Epoch 1530, training loss: 311.85198974609375 = 0.059642285108566284 + 50.0 * 6.235847473144531
Epoch 1530, val loss: 1.1097707748413086
Epoch 1540, training loss: 311.67498779296875 = 0.058243054896593094 + 50.0 * 6.232334613800049
Epoch 1540, val loss: 1.1151665449142456
Epoch 1550, training loss: 311.79150390625 = 0.056884825229644775 + 50.0 * 6.234692096710205
Epoch 1550, val loss: 1.1205958127975464
Epoch 1560, training loss: 311.7850646972656 = 0.05555892735719681 + 50.0 * 6.234589576721191
Epoch 1560, val loss: 1.126050353050232
Epoch 1570, training loss: 311.622314453125 = 0.054257046431303024 + 50.0 * 6.231361389160156
Epoch 1570, val loss: 1.1310850381851196
Epoch 1580, training loss: 311.6043395996094 = 0.05300457775592804 + 50.0 * 6.231026649475098
Epoch 1580, val loss: 1.1366581916809082
Epoch 1590, training loss: 311.57769775390625 = 0.05181216076016426 + 50.0 * 6.230517864227295
Epoch 1590, val loss: 1.1418565511703491
Epoch 1600, training loss: 311.80377197265625 = 0.050655756145715714 + 50.0 * 6.235062122344971
Epoch 1600, val loss: 1.14717698097229
Epoch 1610, training loss: 311.696044921875 = 0.049494460225105286 + 50.0 * 6.232931137084961
Epoch 1610, val loss: 1.152090072631836
Epoch 1620, training loss: 311.74176025390625 = 0.0483698695898056 + 50.0 * 6.233867645263672
Epoch 1620, val loss: 1.1573787927627563
Epoch 1630, training loss: 311.51458740234375 = 0.04727267101407051 + 50.0 * 6.22934627532959
Epoch 1630, val loss: 1.162764310836792
Epoch 1640, training loss: 311.5272521972656 = 0.04623296484351158 + 50.0 * 6.229620456695557
Epoch 1640, val loss: 1.1680607795715332
Epoch 1650, training loss: 311.55615234375 = 0.045225780457258224 + 50.0 * 6.230218410491943
Epoch 1650, val loss: 1.1733112335205078
Epoch 1660, training loss: 311.6175231933594 = 0.04423925280570984 + 50.0 * 6.2314653396606445
Epoch 1660, val loss: 1.1784749031066895
Epoch 1670, training loss: 311.49798583984375 = 0.04327945411205292 + 50.0 * 6.2290940284729
Epoch 1670, val loss: 1.1833487749099731
Epoch 1680, training loss: 311.81817626953125 = 0.04235183820128441 + 50.0 * 6.235516548156738
Epoch 1680, val loss: 1.1882336139678955
Epoch 1690, training loss: 311.4811096191406 = 0.0414249524474144 + 50.0 * 6.228794097900391
Epoch 1690, val loss: 1.1931207180023193
Epoch 1700, training loss: 311.436279296875 = 0.0405350960791111 + 50.0 * 6.227915287017822
Epoch 1700, val loss: 1.198199987411499
Epoch 1710, training loss: 311.5478210449219 = 0.03969191014766693 + 50.0 * 6.230162620544434
Epoch 1710, val loss: 1.2030895948410034
Epoch 1720, training loss: 311.4166259765625 = 0.03885510191321373 + 50.0 * 6.227555751800537
Epoch 1720, val loss: 1.2080634832382202
Epoch 1730, training loss: 311.5454406738281 = 0.038056693971157074 + 50.0 * 6.230147838592529
Epoch 1730, val loss: 1.2129658460617065
Epoch 1740, training loss: 311.4666442871094 = 0.037270039319992065 + 50.0 * 6.228587627410889
Epoch 1740, val loss: 1.217519760131836
Epoch 1750, training loss: 311.3581237792969 = 0.03648773208260536 + 50.0 * 6.2264323234558105
Epoch 1750, val loss: 1.2225563526153564
Epoch 1760, training loss: 311.45654296875 = 0.0357508659362793 + 50.0 * 6.2284159660339355
Epoch 1760, val loss: 1.2273025512695312
Epoch 1770, training loss: 311.3886413574219 = 0.03502954542636871 + 50.0 * 6.227072238922119
Epoch 1770, val loss: 1.2318512201309204
Epoch 1780, training loss: 311.386474609375 = 0.03432786092162132 + 50.0 * 6.2270426750183105
Epoch 1780, val loss: 1.236643671989441
Epoch 1790, training loss: 311.61126708984375 = 0.03365447744727135 + 50.0 * 6.2315521240234375
Epoch 1790, val loss: 1.2411121129989624
Epoch 1800, training loss: 311.34326171875 = 0.032959576696157455 + 50.0 * 6.226206302642822
Epoch 1800, val loss: 1.2460705041885376
Epoch 1810, training loss: 311.3392333984375 = 0.03230920061469078 + 50.0 * 6.226138114929199
Epoch 1810, val loss: 1.2503694295883179
Epoch 1820, training loss: 311.2774658203125 = 0.03168077766895294 + 50.0 * 6.224915981292725
Epoch 1820, val loss: 1.2550890445709229
Epoch 1830, training loss: 311.2749938964844 = 0.031074844300746918 + 50.0 * 6.224878787994385
Epoch 1830, val loss: 1.2596157789230347
Epoch 1840, training loss: 311.4669189453125 = 0.030483068898320198 + 50.0 * 6.228728771209717
Epoch 1840, val loss: 1.2642379999160767
Epoch 1850, training loss: 311.334228515625 = 0.029905959963798523 + 50.0 * 6.226086139678955
Epoch 1850, val loss: 1.2686741352081299
Epoch 1860, training loss: 311.2589111328125 = 0.029337294399738312 + 50.0 * 6.2245917320251465
Epoch 1860, val loss: 1.272931456565857
Epoch 1870, training loss: 311.3686828613281 = 0.02879180759191513 + 50.0 * 6.226798057556152
Epoch 1870, val loss: 1.2774635553359985
Epoch 1880, training loss: 311.3183898925781 = 0.028246169909834862 + 50.0 * 6.225802898406982
Epoch 1880, val loss: 1.2817188501358032
Epoch 1890, training loss: 311.22705078125 = 0.02772671915590763 + 50.0 * 6.2239861488342285
Epoch 1890, val loss: 1.2858293056488037
Epoch 1900, training loss: 311.1734313964844 = 0.02721983939409256 + 50.0 * 6.22292423248291
Epoch 1900, val loss: 1.2901320457458496
Epoch 1910, training loss: 311.2048034667969 = 0.026739608496427536 + 50.0 * 6.2235612869262695
Epoch 1910, val loss: 1.294391393661499
Epoch 1920, training loss: 311.4217529296875 = 0.026267537847161293 + 50.0 * 6.227909564971924
Epoch 1920, val loss: 1.2983794212341309
Epoch 1930, training loss: 311.32318115234375 = 0.02577747218310833 + 50.0 * 6.225947856903076
Epoch 1930, val loss: 1.3027057647705078
Epoch 1940, training loss: 311.1961364746094 = 0.025319155305624008 + 50.0 * 6.223416328430176
Epoch 1940, val loss: 1.306672215461731
Epoch 1950, training loss: 311.12615966796875 = 0.02487199567258358 + 50.0 * 6.2220258712768555
Epoch 1950, val loss: 1.3110711574554443
Epoch 1960, training loss: 311.2604064941406 = 0.02444680966436863 + 50.0 * 6.224719524383545
Epoch 1960, val loss: 1.3152368068695068
Epoch 1970, training loss: 311.1859130859375 = 0.02401799149811268 + 50.0 * 6.223237991333008
Epoch 1970, val loss: 1.318929672241211
Epoch 1980, training loss: 311.1224670410156 = 0.02361133135855198 + 50.0 * 6.2219767570495605
Epoch 1980, val loss: 1.3227441310882568
Epoch 1990, training loss: 311.20758056640625 = 0.02320566214621067 + 50.0 * 6.223687648773193
Epoch 1990, val loss: 1.3268488645553589
Epoch 2000, training loss: 311.29437255859375 = 0.022809617221355438 + 50.0 * 6.225431442260742
Epoch 2000, val loss: 1.3306673765182495
Epoch 2010, training loss: 311.1191101074219 = 0.022422799840569496 + 50.0 * 6.221933364868164
Epoch 2010, val loss: 1.334524154663086
Epoch 2020, training loss: 311.0450134277344 = 0.022052060812711716 + 50.0 * 6.220458984375
Epoch 2020, val loss: 1.3381301164627075
Epoch 2030, training loss: 311.02203369140625 = 0.021692663431167603 + 50.0 * 6.220006942749023
Epoch 2030, val loss: 1.342118501663208
Epoch 2040, training loss: 311.15106201171875 = 0.021350210532546043 + 50.0 * 6.222594261169434
Epoch 2040, val loss: 1.3457461595535278
Epoch 2050, training loss: 311.1092834472656 = 0.020993757992982864 + 50.0 * 6.221765995025635
Epoch 2050, val loss: 1.349639892578125
Epoch 2060, training loss: 311.080810546875 = 0.0206476841121912 + 50.0 * 6.221202850341797
Epoch 2060, val loss: 1.3532768487930298
Epoch 2070, training loss: 310.98773193359375 = 0.020318733528256416 + 50.0 * 6.219348430633545
Epoch 2070, val loss: 1.3568428754806519
Epoch 2080, training loss: 310.9850158691406 = 0.020000452175736427 + 50.0 * 6.219300270080566
Epoch 2080, val loss: 1.3605279922485352
Epoch 2090, training loss: 311.3263854980469 = 0.019690847024321556 + 50.0 * 6.226133823394775
Epoch 2090, val loss: 1.3642797470092773
Epoch 2100, training loss: 311.0732421875 = 0.019374055787920952 + 50.0 * 6.2210774421691895
Epoch 2100, val loss: 1.3677653074264526
Epoch 2110, training loss: 310.9541320800781 = 0.019071200862526894 + 50.0 * 6.218700885772705
Epoch 2110, val loss: 1.3712276220321655
Epoch 2120, training loss: 311.03411865234375 = 0.018783651292324066 + 50.0 * 6.220306396484375
Epoch 2120, val loss: 1.374834418296814
Epoch 2130, training loss: 311.0610656738281 = 0.018495263531804085 + 50.0 * 6.220851421356201
Epoch 2130, val loss: 1.3783284425735474
Epoch 2140, training loss: 310.93463134765625 = 0.018213219940662384 + 50.0 * 6.218328475952148
Epoch 2140, val loss: 1.381602168083191
Epoch 2150, training loss: 310.8915100097656 = 0.017942965030670166 + 50.0 * 6.217471599578857
Epoch 2150, val loss: 1.3851364850997925
Epoch 2160, training loss: 310.9338073730469 = 0.017680006101727486 + 50.0 * 6.21832275390625
Epoch 2160, val loss: 1.3887724876403809
Epoch 2170, training loss: 311.1612854003906 = 0.017423294484615326 + 50.0 * 6.222877025604248
Epoch 2170, val loss: 1.3920865058898926
Epoch 2180, training loss: 311.044189453125 = 0.017158038914203644 + 50.0 * 6.220540523529053
Epoch 2180, val loss: 1.3950377702713013
Epoch 2190, training loss: 310.96392822265625 = 0.01690649800002575 + 50.0 * 6.218940258026123
Epoch 2190, val loss: 1.398481845855713
Epoch 2200, training loss: 310.8483581542969 = 0.016664328053593636 + 50.0 * 6.2166337966918945
Epoch 2200, val loss: 1.40157949924469
Epoch 2210, training loss: 310.8455810546875 = 0.01643126644194126 + 50.0 * 6.216583251953125
Epoch 2210, val loss: 1.4049619436264038
Epoch 2220, training loss: 311.0111999511719 = 0.0162091925740242 + 50.0 * 6.219899654388428
Epoch 2220, val loss: 1.4079951047897339
Epoch 2230, training loss: 310.8793029785156 = 0.01597472093999386 + 50.0 * 6.217267036437988
Epoch 2230, val loss: 1.4113508462905884
Epoch 2240, training loss: 310.90130615234375 = 0.01574701815843582 + 50.0 * 6.217711448669434
Epoch 2240, val loss: 1.4146233797073364
Epoch 2250, training loss: 310.9065246582031 = 0.015531268902122974 + 50.0 * 6.217819690704346
Epoch 2250, val loss: 1.417790174484253
Epoch 2260, training loss: 310.9145812988281 = 0.015317446552217007 + 50.0 * 6.217985153198242
Epoch 2260, val loss: 1.4208407402038574
Epoch 2270, training loss: 310.90631103515625 = 0.015108286403119564 + 50.0 * 6.2178239822387695
Epoch 2270, val loss: 1.4238810539245605
Epoch 2280, training loss: 310.87078857421875 = 0.014897461049258709 + 50.0 * 6.217117786407471
Epoch 2280, val loss: 1.4271891117095947
Epoch 2290, training loss: 310.79541015625 = 0.014699566178023815 + 50.0 * 6.215613842010498
Epoch 2290, val loss: 1.4301635026931763
Epoch 2300, training loss: 310.80938720703125 = 0.014510104432702065 + 50.0 * 6.215898036956787
Epoch 2300, val loss: 1.4329873323440552
Epoch 2310, training loss: 310.98150634765625 = 0.01432271208614111 + 50.0 * 6.219344139099121
Epoch 2310, val loss: 1.4360991716384888
Epoch 2320, training loss: 310.76373291015625 = 0.014127114787697792 + 50.0 * 6.214992046356201
Epoch 2320, val loss: 1.4391615390777588
Epoch 2330, training loss: 310.8563232421875 = 0.013946270570158958 + 50.0 * 6.2168474197387695
Epoch 2330, val loss: 1.4421087503433228
Epoch 2340, training loss: 310.94622802734375 = 0.013771655969321728 + 50.0 * 6.218649387359619
Epoch 2340, val loss: 1.444757342338562
Epoch 2350, training loss: 310.7898254394531 = 0.013587852008640766 + 50.0 * 6.215525150299072
Epoch 2350, val loss: 1.4477503299713135
Epoch 2360, training loss: 310.67926025390625 = 0.013416356407105923 + 50.0 * 6.213316917419434
Epoch 2360, val loss: 1.450637698173523
Epoch 2370, training loss: 310.93365478515625 = 0.013257615268230438 + 50.0 * 6.218408107757568
Epoch 2370, val loss: 1.4535305500030518
Epoch 2380, training loss: 310.7488708496094 = 0.013083129189908504 + 50.0 * 6.214715480804443
Epoch 2380, val loss: 1.4562771320343018
Epoch 2390, training loss: 310.7380676269531 = 0.012920648790895939 + 50.0 * 6.214503288269043
Epoch 2390, val loss: 1.4591373205184937
Epoch 2400, training loss: 310.8084411621094 = 0.012758343480527401 + 50.0 * 6.215913772583008
Epoch 2400, val loss: 1.4619297981262207
Epoch 2410, training loss: 310.6402587890625 = 0.012603738345205784 + 50.0 * 6.212553024291992
Epoch 2410, val loss: 1.464529037475586
Epoch 2420, training loss: 310.70587158203125 = 0.01245303824543953 + 50.0 * 6.213868618011475
Epoch 2420, val loss: 1.4673875570297241
Epoch 2430, training loss: 310.75592041015625 = 0.012305423617362976 + 50.0 * 6.214872360229492
Epoch 2430, val loss: 1.4701696634292603
Epoch 2440, training loss: 310.74664306640625 = 0.012160937301814556 + 50.0 * 6.214690208435059
Epoch 2440, val loss: 1.4724702835083008
Epoch 2450, training loss: 310.7410888671875 = 0.012013229541480541 + 50.0 * 6.21458101272583
Epoch 2450, val loss: 1.4751642942428589
Epoch 2460, training loss: 310.6618347167969 = 0.01187334768474102 + 50.0 * 6.21299934387207
Epoch 2460, val loss: 1.4775294065475464
Epoch 2470, training loss: 310.7237854003906 = 0.011735443957149982 + 50.0 * 6.214240550994873
Epoch 2470, val loss: 1.4801205396652222
Epoch 2480, training loss: 310.6090087890625 = 0.011597268283367157 + 50.0 * 6.211948394775391
Epoch 2480, val loss: 1.4828227758407593
Epoch 2490, training loss: 310.5981750488281 = 0.011465414427220821 + 50.0 * 6.211733818054199
Epoch 2490, val loss: 1.48545503616333
Epoch 2500, training loss: 310.8675537109375 = 0.011341625824570656 + 50.0 * 6.217123985290527
Epoch 2500, val loss: 1.4880094528198242
Epoch 2510, training loss: 310.6935119628906 = 0.011210381053388119 + 50.0 * 6.213646411895752
Epoch 2510, val loss: 1.4901059865951538
Epoch 2520, training loss: 310.64727783203125 = 0.011082260869443417 + 50.0 * 6.212723731994629
Epoch 2520, val loss: 1.4929040670394897
Epoch 2530, training loss: 310.7119140625 = 0.010957461781799793 + 50.0 * 6.214019298553467
Epoch 2530, val loss: 1.4954689741134644
Epoch 2540, training loss: 310.5869140625 = 0.010835017077624798 + 50.0 * 6.211521625518799
Epoch 2540, val loss: 1.4979714155197144
Epoch 2550, training loss: 310.5695495605469 = 0.01071804016828537 + 50.0 * 6.211176872253418
Epoch 2550, val loss: 1.500240683555603
Epoch 2560, training loss: 310.7189636230469 = 0.010604972951114178 + 50.0 * 6.21416711807251
Epoch 2560, val loss: 1.5027923583984375
Epoch 2570, training loss: 310.5276794433594 = 0.010487192310392857 + 50.0 * 6.210343837738037
Epoch 2570, val loss: 1.5050956010818481
Epoch 2580, training loss: 310.5940246582031 = 0.01037406362593174 + 50.0 * 6.211673259735107
Epoch 2580, val loss: 1.5074158906936646
Epoch 2590, training loss: 310.69256591796875 = 0.010270279832184315 + 50.0 * 6.2136454582214355
Epoch 2590, val loss: 1.5094997882843018
Epoch 2600, training loss: 310.6134338378906 = 0.010157597251236439 + 50.0 * 6.21206521987915
Epoch 2600, val loss: 1.5118682384490967
Epoch 2610, training loss: 310.53814697265625 = 0.010046319104731083 + 50.0 * 6.210562229156494
Epoch 2610, val loss: 1.514448881149292
Epoch 2620, training loss: 310.5103454589844 = 0.009943967685103416 + 50.0 * 6.210007667541504
Epoch 2620, val loss: 1.5165395736694336
Epoch 2630, training loss: 310.57208251953125 = 0.009843497537076473 + 50.0 * 6.211244583129883
Epoch 2630, val loss: 1.5190894603729248
Epoch 2640, training loss: 310.5474853515625 = 0.009742241352796555 + 50.0 * 6.210754871368408
Epoch 2640, val loss: 1.5212310552597046
Epoch 2650, training loss: 310.7867431640625 = 0.009643487632274628 + 50.0 * 6.215541839599609
Epoch 2650, val loss: 1.5233303308486938
Epoch 2660, training loss: 310.5076904296875 = 0.009541377425193787 + 50.0 * 6.209962844848633
Epoch 2660, val loss: 1.5257127285003662
Epoch 2670, training loss: 310.51031494140625 = 0.009444900788366795 + 50.0 * 6.210017681121826
Epoch 2670, val loss: 1.528017282485962
Epoch 2680, training loss: 310.75555419921875 = 0.009352026507258415 + 50.0 * 6.214924335479736
Epoch 2680, val loss: 1.5303890705108643
Epoch 2690, training loss: 310.4399108886719 = 0.009256230667233467 + 50.0 * 6.208613395690918
Epoch 2690, val loss: 1.5321271419525146
Epoch 2700, training loss: 310.4371643066406 = 0.00916675291955471 + 50.0 * 6.208559989929199
Epoch 2700, val loss: 1.5342897176742554
Epoch 2710, training loss: 310.429931640625 = 0.009078601375222206 + 50.0 * 6.2084174156188965
Epoch 2710, val loss: 1.5366169214248657
Epoch 2720, training loss: 310.7345886230469 = 0.00899254996329546 + 50.0 * 6.214511871337891
Epoch 2720, val loss: 1.538907766342163
Epoch 2730, training loss: 310.4676513671875 = 0.008906811475753784 + 50.0 * 6.209174633026123
Epoch 2730, val loss: 1.5404889583587646
Epoch 2740, training loss: 310.3969421386719 = 0.008819482289254665 + 50.0 * 6.207762241363525
Epoch 2740, val loss: 1.5425924062728882
Epoch 2750, training loss: 310.5048828125 = 0.008737873286008835 + 50.0 * 6.209923267364502
Epoch 2750, val loss: 1.544714331626892
Epoch 2760, training loss: 310.4146423339844 = 0.008655987679958344 + 50.0 * 6.2081193923950195
Epoch 2760, val loss: 1.546878695487976
Epoch 2770, training loss: 310.8036804199219 = 0.008575025014579296 + 50.0 * 6.215901851654053
Epoch 2770, val loss: 1.5489728450775146
Epoch 2780, training loss: 310.47979736328125 = 0.00849310401827097 + 50.0 * 6.209426403045654
Epoch 2780, val loss: 1.55027437210083
Epoch 2790, training loss: 310.3895263671875 = 0.008410053327679634 + 50.0 * 6.207622051239014
Epoch 2790, val loss: 1.552708387374878
Epoch 2800, training loss: 310.39788818359375 = 0.008337982930243015 + 50.0 * 6.207791328430176
Epoch 2800, val loss: 1.5544579029083252
Epoch 2810, training loss: 310.46331787109375 = 0.008263072930276394 + 50.0 * 6.20910120010376
Epoch 2810, val loss: 1.5564155578613281
Epoch 2820, training loss: 310.3564147949219 = 0.00818878598511219 + 50.0 * 6.20696496963501
Epoch 2820, val loss: 1.558449625968933
Epoch 2830, training loss: 310.4909362792969 = 0.008118350058794022 + 50.0 * 6.209656238555908
Epoch 2830, val loss: 1.5603840351104736
Epoch 2840, training loss: 310.5125732421875 = 0.008046353235840797 + 50.0 * 6.210090637207031
Epoch 2840, val loss: 1.5620546340942383
Epoch 2850, training loss: 310.322265625 = 0.00797173660248518 + 50.0 * 6.2062859535217285
Epoch 2850, val loss: 1.5640798807144165
Epoch 2860, training loss: 310.4482421875 = 0.007902514189481735 + 50.0 * 6.208806991577148
Epoch 2860, val loss: 1.56606125831604
Epoch 2870, training loss: 310.32000732421875 = 0.00783288199454546 + 50.0 * 6.206243515014648
Epoch 2870, val loss: 1.5678595304489136
Epoch 2880, training loss: 310.3952941894531 = 0.007766552735120058 + 50.0 * 6.2077507972717285
Epoch 2880, val loss: 1.5697938203811646
Epoch 2890, training loss: 310.4265441894531 = 0.007699126377701759 + 50.0 * 6.208376884460449
Epoch 2890, val loss: 1.5716627836227417
Epoch 2900, training loss: 310.353515625 = 0.007632795721292496 + 50.0 * 6.206917762756348
Epoch 2900, val loss: 1.5730863809585571
Epoch 2910, training loss: 310.3653259277344 = 0.007569420617073774 + 50.0 * 6.207155227661133
Epoch 2910, val loss: 1.5748218297958374
Epoch 2920, training loss: 310.6427307128906 = 0.007507918402552605 + 50.0 * 6.212704181671143
Epoch 2920, val loss: 1.5766243934631348
Epoch 2930, training loss: 310.40625 = 0.007441460620611906 + 50.0 * 6.2079758644104
Epoch 2930, val loss: 1.5782268047332764
Epoch 2940, training loss: 310.31219482421875 = 0.007375277113169432 + 50.0 * 6.206096172332764
Epoch 2940, val loss: 1.5801817178726196
Epoch 2950, training loss: 310.23583984375 = 0.0073177628219127655 + 50.0 * 6.204570293426514
Epoch 2950, val loss: 1.5817747116088867
Epoch 2960, training loss: 310.22760009765625 = 0.007260151207447052 + 50.0 * 6.20440673828125
Epoch 2960, val loss: 1.5834846496582031
Epoch 2970, training loss: 310.57574462890625 = 0.007204352878034115 + 50.0 * 6.211370944976807
Epoch 2970, val loss: 1.585099220275879
Epoch 2980, training loss: 310.43798828125 = 0.007144159637391567 + 50.0 * 6.208617210388184
Epoch 2980, val loss: 1.5869052410125732
Epoch 2990, training loss: 310.28985595703125 = 0.007082552183419466 + 50.0 * 6.205655574798584
Epoch 2990, val loss: 1.5878995656967163
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 431.796630859375 = 1.9538332223892212 + 50.0 * 8.596856117248535
Epoch 0, val loss: 1.960340976715088
Epoch 10, training loss: 431.748779296875 = 1.9444670677185059 + 50.0 * 8.596086502075195
Epoch 10, val loss: 1.9505149126052856
Epoch 20, training loss: 431.4609680175781 = 1.9334324598312378 + 50.0 * 8.590550422668457
Epoch 20, val loss: 1.938781499862671
Epoch 30, training loss: 429.21160888671875 = 1.9198061227798462 + 50.0 * 8.545836448669434
Epoch 30, val loss: 1.9242221117019653
Epoch 40, training loss: 409.98016357421875 = 1.903193712234497 + 50.0 * 8.161539077758789
Epoch 40, val loss: 1.906748652458191
Epoch 50, training loss: 371.5002746582031 = 1.884272813796997 + 50.0 * 7.392319679260254
Epoch 50, val loss: 1.8872430324554443
Epoch 60, training loss: 359.6047668457031 = 1.8707914352416992 + 50.0 * 7.154679775238037
Epoch 60, val loss: 1.873569369316101
Epoch 70, training loss: 351.7479553222656 = 1.8586212396621704 + 50.0 * 6.997786998748779
Epoch 70, val loss: 1.860945224761963
Epoch 80, training loss: 345.2008361816406 = 1.8463807106018066 + 50.0 * 6.86708927154541
Epoch 80, val loss: 1.8487869501113892
Epoch 90, training loss: 340.6808776855469 = 1.8350419998168945 + 50.0 * 6.77691650390625
Epoch 90, val loss: 1.8374230861663818
Epoch 100, training loss: 337.6153869628906 = 1.8236751556396484 + 50.0 * 6.715834617614746
Epoch 100, val loss: 1.8263272047042847
Epoch 110, training loss: 334.919677734375 = 1.8123903274536133 + 50.0 * 6.662146091461182
Epoch 110, val loss: 1.8150546550750732
Epoch 120, training loss: 332.8487854003906 = 1.80149507522583 + 50.0 * 6.620945930480957
Epoch 120, val loss: 1.8040728569030762
Epoch 130, training loss: 331.2142028808594 = 1.7904081344604492 + 50.0 * 6.588476181030273
Epoch 130, val loss: 1.7929307222366333
Epoch 140, training loss: 329.7935485839844 = 1.7789926528930664 + 50.0 * 6.560291290283203
Epoch 140, val loss: 1.7815824747085571
Epoch 150, training loss: 328.57489013671875 = 1.7673375606536865 + 50.0 * 6.53615140914917
Epoch 150, val loss: 1.7701237201690674
Epoch 160, training loss: 327.6855163574219 = 1.7551823854446411 + 50.0 * 6.518606662750244
Epoch 160, val loss: 1.758442759513855
Epoch 170, training loss: 326.61285400390625 = 1.7421801090240479 + 50.0 * 6.497413635253906
Epoch 170, val loss: 1.7461295127868652
Epoch 180, training loss: 325.7239685058594 = 1.728235125541687 + 50.0 * 6.479914665222168
Epoch 180, val loss: 1.7331287860870361
Epoch 190, training loss: 324.96728515625 = 1.7130435705184937 + 50.0 * 6.465084552764893
Epoch 190, val loss: 1.719182014465332
Epoch 200, training loss: 324.4564514160156 = 1.6963907480239868 + 50.0 * 6.455201148986816
Epoch 200, val loss: 1.704025387763977
Epoch 210, training loss: 323.8141784667969 = 1.6782077550888062 + 50.0 * 6.442718982696533
Epoch 210, val loss: 1.6876378059387207
Epoch 220, training loss: 323.2696228027344 = 1.6583898067474365 + 50.0 * 6.432224750518799
Epoch 220, val loss: 1.670050024986267
Epoch 230, training loss: 322.8190002441406 = 1.6368846893310547 + 50.0 * 6.423642635345459
Epoch 230, val loss: 1.6511192321777344
Epoch 240, training loss: 322.4794006347656 = 1.6135737895965576 + 50.0 * 6.417316436767578
Epoch 240, val loss: 1.6305986642837524
Epoch 250, training loss: 321.9895324707031 = 1.5883779525756836 + 50.0 * 6.408022880554199
Epoch 250, val loss: 1.6087744235992432
Epoch 260, training loss: 321.6026306152344 = 1.5613911151885986 + 50.0 * 6.400824546813965
Epoch 260, val loss: 1.585424780845642
Epoch 270, training loss: 321.198486328125 = 1.5327959060668945 + 50.0 * 6.393313884735107
Epoch 270, val loss: 1.560947299003601
Epoch 280, training loss: 320.8632507324219 = 1.5026428699493408 + 50.0 * 6.387211799621582
Epoch 280, val loss: 1.5352970361709595
Epoch 290, training loss: 320.6122131347656 = 1.4710017442703247 + 50.0 * 6.382823944091797
Epoch 290, val loss: 1.5085376501083374
Epoch 300, training loss: 320.3201599121094 = 1.438166856765747 + 50.0 * 6.3776397705078125
Epoch 300, val loss: 1.4810819625854492
Epoch 310, training loss: 319.989013671875 = 1.4044039249420166 + 50.0 * 6.371692180633545
Epoch 310, val loss: 1.4531644582748413
Epoch 320, training loss: 319.7256774902344 = 1.3699262142181396 + 50.0 * 6.367115020751953
Epoch 320, val loss: 1.4248684644699097
Epoch 330, training loss: 319.56964111328125 = 1.3348695039749146 + 50.0 * 6.3646955490112305
Epoch 330, val loss: 1.3961703777313232
Epoch 340, training loss: 319.27178955078125 = 1.2995072603225708 + 50.0 * 6.359445571899414
Epoch 340, val loss: 1.3676995038986206
Epoch 350, training loss: 319.0216369628906 = 1.2640653848648071 + 50.0 * 6.355151176452637
Epoch 350, val loss: 1.3395304679870605
Epoch 360, training loss: 318.87567138671875 = 1.2287383079528809 + 50.0 * 6.352938652038574
Epoch 360, val loss: 1.3116819858551025
Epoch 370, training loss: 318.63824462890625 = 1.1937862634658813 + 50.0 * 6.348889350891113
Epoch 370, val loss: 1.284502387046814
Epoch 380, training loss: 318.44232177734375 = 1.1593906879425049 + 50.0 * 6.345658779144287
Epoch 380, val loss: 1.2580108642578125
Epoch 390, training loss: 318.20489501953125 = 1.125670313835144 + 50.0 * 6.341584205627441
Epoch 390, val loss: 1.2324453592300415
Epoch 400, training loss: 318.01385498046875 = 1.0927810668945312 + 50.0 * 6.33842134475708
Epoch 400, val loss: 1.2077256441116333
Epoch 410, training loss: 317.8902587890625 = 1.0607823133468628 + 50.0 * 6.336589336395264
Epoch 410, val loss: 1.1842498779296875
Epoch 420, training loss: 317.7729797363281 = 1.0296916961669922 + 50.0 * 6.334865570068359
Epoch 420, val loss: 1.1614820957183838
Epoch 430, training loss: 317.5348815917969 = 0.9995360970497131 + 50.0 * 6.33070707321167
Epoch 430, val loss: 1.1400196552276611
Epoch 440, training loss: 317.3390808105469 = 0.970480740070343 + 50.0 * 6.327372074127197
Epoch 440, val loss: 1.11978280544281
Epoch 450, training loss: 317.1636962890625 = 0.9426194429397583 + 50.0 * 6.324421405792236
Epoch 450, val loss: 1.100618600845337
Epoch 460, training loss: 317.216064453125 = 0.9157500863075256 + 50.0 * 6.3260064125061035
Epoch 460, val loss: 1.082593560218811
Epoch 470, training loss: 317.0726013183594 = 0.889647901058197 + 50.0 * 6.3236589431762695
Epoch 470, val loss: 1.0656101703643799
Epoch 480, training loss: 316.80859375 = 0.8646630644798279 + 50.0 * 6.318878650665283
Epoch 480, val loss: 1.0496866703033447
Epoch 490, training loss: 316.6228942871094 = 0.8406099677085876 + 50.0 * 6.315645694732666
Epoch 490, val loss: 1.0348467826843262
Epoch 500, training loss: 316.46807861328125 = 0.8174067139625549 + 50.0 * 6.313013553619385
Epoch 500, val loss: 1.0209300518035889
Epoch 510, training loss: 316.45672607421875 = 0.7949604392051697 + 50.0 * 6.313235759735107
Epoch 510, val loss: 1.0077837705612183
Epoch 520, training loss: 316.30828857421875 = 0.7733182311058044 + 50.0 * 6.310699462890625
Epoch 520, val loss: 0.9955385327339172
Epoch 530, training loss: 316.0839538574219 = 0.7524091005325317 + 50.0 * 6.306630611419678
Epoch 530, val loss: 0.9841057658195496
Epoch 540, training loss: 316.1134948730469 = 0.732131838798523 + 50.0 * 6.3076276779174805
Epoch 540, val loss: 0.973513126373291
Epoch 550, training loss: 316.1594543457031 = 0.7123818397521973 + 50.0 * 6.30894136428833
Epoch 550, val loss: 0.9631137251853943
Epoch 560, training loss: 315.7950439453125 = 0.6932255625724792 + 50.0 * 6.302036285400391
Epoch 560, val loss: 0.9537424445152283
Epoch 570, training loss: 315.6407470703125 = 0.6745675206184387 + 50.0 * 6.299323558807373
Epoch 570, val loss: 0.9449865818023682
Epoch 580, training loss: 315.5214538574219 = 0.6564469933509827 + 50.0 * 6.297300338745117
Epoch 580, val loss: 0.936826229095459
Epoch 590, training loss: 315.838134765625 = 0.6387460231781006 + 50.0 * 6.303987979888916
Epoch 590, val loss: 0.9289748072624207
Epoch 600, training loss: 315.4500732421875 = 0.621248185634613 + 50.0 * 6.296576499938965
Epoch 600, val loss: 0.9213829636573792
Epoch 610, training loss: 315.2401123046875 = 0.6042448878288269 + 50.0 * 6.292716979980469
Epoch 610, val loss: 0.9144293665885925
Epoch 620, training loss: 315.1352233886719 = 0.5876744985580444 + 50.0 * 6.290950775146484
Epoch 620, val loss: 0.9079927206039429
Epoch 630, training loss: 315.192138671875 = 0.5714691877365112 + 50.0 * 6.292413234710693
Epoch 630, val loss: 0.901933491230011
Epoch 640, training loss: 315.2239990234375 = 0.5554401278495789 + 50.0 * 6.293371200561523
Epoch 640, val loss: 0.8959143757820129
Epoch 650, training loss: 314.90643310546875 = 0.5396574139595032 + 50.0 * 6.2873358726501465
Epoch 650, val loss: 0.8903315663337708
Epoch 660, training loss: 314.850830078125 = 0.5243037343025208 + 50.0 * 6.2865309715271
Epoch 660, val loss: 0.8851797580718994
Epoch 670, training loss: 314.71685791015625 = 0.5092434287071228 + 50.0 * 6.284152030944824
Epoch 670, val loss: 0.8803306818008423
Epoch 680, training loss: 314.78936767578125 = 0.49454542994499207 + 50.0 * 6.2858967781066895
Epoch 680, val loss: 0.8757479786872864
Epoch 690, training loss: 314.6903381347656 = 0.48000088334083557 + 50.0 * 6.284206390380859
Epoch 690, val loss: 0.8713750839233398
Epoch 700, training loss: 314.5722961425781 = 0.46581053733825684 + 50.0 * 6.282129764556885
Epoch 700, val loss: 0.8673107624053955
Epoch 710, training loss: 314.5160827636719 = 0.4518857002258301 + 50.0 * 6.281284332275391
Epoch 710, val loss: 0.8635292649269104
Epoch 720, training loss: 314.67572021484375 = 0.43829625844955444 + 50.0 * 6.284748554229736
Epoch 720, val loss: 0.860191285610199
Epoch 730, training loss: 314.4148864746094 = 0.4249482750892639 + 50.0 * 6.27979850769043
Epoch 730, val loss: 0.856810450553894
Epoch 740, training loss: 314.2088623046875 = 0.41201093792915344 + 50.0 * 6.275937080383301
Epoch 740, val loss: 0.8539760112762451
Epoch 750, training loss: 314.18084716796875 = 0.39940622448921204 + 50.0 * 6.275628566741943
Epoch 750, val loss: 0.8514803647994995
Epoch 760, training loss: 314.21942138671875 = 0.3871348798274994 + 50.0 * 6.276645660400391
Epoch 760, val loss: 0.8492162227630615
Epoch 770, training loss: 314.0811767578125 = 0.3751099109649658 + 50.0 * 6.274121284484863
Epoch 770, val loss: 0.8472561240196228
Epoch 780, training loss: 314.0269775390625 = 0.36354246735572815 + 50.0 * 6.273268699645996
Epoch 780, val loss: 0.8456923365592957
Epoch 790, training loss: 314.0138244628906 = 0.35225751996040344 + 50.0 * 6.273231029510498
Epoch 790, val loss: 0.8443995118141174
Epoch 800, training loss: 313.9603576660156 = 0.341268390417099 + 50.0 * 6.27238130569458
Epoch 800, val loss: 0.8433142900466919
Epoch 810, training loss: 313.84771728515625 = 0.33069586753845215 + 50.0 * 6.270340442657471
Epoch 810, val loss: 0.8426443934440613
Epoch 820, training loss: 313.7505798339844 = 0.3204217553138733 + 50.0 * 6.2686028480529785
Epoch 820, val loss: 0.842221200466156
Epoch 830, training loss: 313.7129821777344 = 0.31053414940834045 + 50.0 * 6.2680487632751465
Epoch 830, val loss: 0.8421608209609985
Epoch 840, training loss: 313.70355224609375 = 0.3009736239910126 + 50.0 * 6.268051624298096
Epoch 840, val loss: 0.8423540592193604
Epoch 850, training loss: 313.6824951171875 = 0.29173824191093445 + 50.0 * 6.267814636230469
Epoch 850, val loss: 0.8426977396011353
Epoch 860, training loss: 313.692138671875 = 0.28278595209121704 + 50.0 * 6.268187522888184
Epoch 860, val loss: 0.8434897065162659
Epoch 870, training loss: 313.6512756347656 = 0.2741606533527374 + 50.0 * 6.267541885375977
Epoch 870, val loss: 0.8443797826766968
Epoch 880, training loss: 313.499755859375 = 0.2657541036605835 + 50.0 * 6.264679908752441
Epoch 880, val loss: 0.8455885052680969
Epoch 890, training loss: 313.37518310546875 = 0.2577318847179413 + 50.0 * 6.2623491287231445
Epoch 890, val loss: 0.8471795320510864
Epoch 900, training loss: 313.32403564453125 = 0.25001367926597595 + 50.0 * 6.261480808258057
Epoch 900, val loss: 0.8489940762519836
Epoch 910, training loss: 313.3323669433594 = 0.24256631731987 + 50.0 * 6.261795997619629
Epoch 910, val loss: 0.851061224937439
Epoch 920, training loss: 313.2896423339844 = 0.2353678047657013 + 50.0 * 6.261085510253906
Epoch 920, val loss: 0.8532370328903198
Epoch 930, training loss: 313.3249816894531 = 0.2284001260995865 + 50.0 * 6.261931896209717
Epoch 930, val loss: 0.8557572960853577
Epoch 940, training loss: 313.152099609375 = 0.22164355218410492 + 50.0 * 6.258608818054199
Epoch 940, val loss: 0.8582493662834167
Epoch 950, training loss: 313.1402587890625 = 0.21516089141368866 + 50.0 * 6.258502006530762
Epoch 950, val loss: 0.8611546158790588
Epoch 960, training loss: 313.09466552734375 = 0.20888911187648773 + 50.0 * 6.257715225219727
Epoch 960, val loss: 0.8641384243965149
Epoch 970, training loss: 313.00921630859375 = 0.2028406858444214 + 50.0 * 6.25612735748291
Epoch 970, val loss: 0.8672367930412292
Epoch 980, training loss: 312.9668884277344 = 0.19704461097717285 + 50.0 * 6.255396842956543
Epoch 980, val loss: 0.8706647157669067
Epoch 990, training loss: 312.9287109375 = 0.19144339859485626 + 50.0 * 6.2547454833984375
Epoch 990, val loss: 0.8742913007736206
Epoch 1000, training loss: 313.18353271484375 = 0.18602947890758514 + 50.0 * 6.259950160980225
Epoch 1000, val loss: 0.8780139088630676
Epoch 1010, training loss: 312.94207763671875 = 0.1807783544063568 + 50.0 * 6.255226135253906
Epoch 1010, val loss: 0.8818594217300415
Epoch 1020, training loss: 312.8449401855469 = 0.17566631734371185 + 50.0 * 6.253385543823242
Epoch 1020, val loss: 0.8858775496482849
Epoch 1030, training loss: 312.7720031738281 = 0.17079350352287292 + 50.0 * 6.252023696899414
Epoch 1030, val loss: 0.8901184797286987
Epoch 1040, training loss: 312.84344482421875 = 0.1661105901002884 + 50.0 * 6.253546714782715
Epoch 1040, val loss: 0.8944292664527893
Epoch 1050, training loss: 312.8257751464844 = 0.1615578830242157 + 50.0 * 6.253284454345703
Epoch 1050, val loss: 0.8987622857093811
Epoch 1060, training loss: 312.69244384765625 = 0.15709108114242554 + 50.0 * 6.250706672668457
Epoch 1060, val loss: 0.903136670589447
Epoch 1070, training loss: 312.6612854003906 = 0.15282458066940308 + 50.0 * 6.250168800354004
Epoch 1070, val loss: 0.9077888131141663
Epoch 1080, training loss: 312.6920166015625 = 0.14872120320796967 + 50.0 * 6.250865936279297
Epoch 1080, val loss: 0.9124550223350525
Epoch 1090, training loss: 312.61163330078125 = 0.14474309980869293 + 50.0 * 6.249337673187256
Epoch 1090, val loss: 0.9172619581222534
Epoch 1100, training loss: 312.60504150390625 = 0.14089453220367432 + 50.0 * 6.2492828369140625
Epoch 1100, val loss: 0.9220209717750549
Epoch 1110, training loss: 312.87908935546875 = 0.1371912658214569 + 50.0 * 6.254837989807129
Epoch 1110, val loss: 0.9270229935646057
Epoch 1120, training loss: 312.5023193359375 = 0.13352560997009277 + 50.0 * 6.247375965118408
Epoch 1120, val loss: 0.9318450093269348
Epoch 1130, training loss: 312.4032287597656 = 0.13003720343112946 + 50.0 * 6.245463848114014
Epoch 1130, val loss: 0.9369961023330688
Epoch 1140, training loss: 312.384033203125 = 0.12668246030807495 + 50.0 * 6.245147228240967
Epoch 1140, val loss: 0.9423073530197144
Epoch 1150, training loss: 312.6927795410156 = 0.12346728146076202 + 50.0 * 6.2513861656188965
Epoch 1150, val loss: 0.9475635290145874
Epoch 1160, training loss: 312.6124572753906 = 0.1202264130115509 + 50.0 * 6.249844074249268
Epoch 1160, val loss: 0.9525319337844849
Epoch 1170, training loss: 312.3690490722656 = 0.11714915931224823 + 50.0 * 6.24503755569458
Epoch 1170, val loss: 0.9578857421875
Epoch 1180, training loss: 312.2994079589844 = 0.11418568342924118 + 50.0 * 6.243704319000244
Epoch 1180, val loss: 0.9632512927055359
Epoch 1190, training loss: 312.3098449707031 = 0.11132396012544632 + 50.0 * 6.2439703941345215
Epoch 1190, val loss: 0.9687493443489075
Epoch 1200, training loss: 312.37384033203125 = 0.10854735225439072 + 50.0 * 6.24530553817749
Epoch 1200, val loss: 0.9741438031196594
Epoch 1210, training loss: 312.408935546875 = 0.10582492500543594 + 50.0 * 6.2460618019104
Epoch 1210, val loss: 0.9796021580696106
Epoch 1220, training loss: 312.25592041015625 = 0.10320845991373062 + 50.0 * 6.243054389953613
Epoch 1220, val loss: 0.9850484132766724
Epoch 1230, training loss: 312.3350830078125 = 0.10067608952522278 + 50.0 * 6.244688034057617
Epoch 1230, val loss: 0.9905400276184082
Epoch 1240, training loss: 312.22149658203125 = 0.09820225834846497 + 50.0 * 6.242465972900391
Epoch 1240, val loss: 0.9962112903594971
Epoch 1250, training loss: 312.11358642578125 = 0.09580571204423904 + 50.0 * 6.240355968475342
Epoch 1250, val loss: 1.0016077756881714
Epoch 1260, training loss: 312.06597900390625 = 0.09350475668907166 + 50.0 * 6.239449501037598
Epoch 1260, val loss: 1.0073928833007812
Epoch 1270, training loss: 312.10638427734375 = 0.0912742167711258 + 50.0 * 6.240302085876465
Epoch 1270, val loss: 1.0130664110183716
Epoch 1280, training loss: 312.1929931640625 = 0.08911687880754471 + 50.0 * 6.242077827453613
Epoch 1280, val loss: 1.0187784433364868
Epoch 1290, training loss: 312.2684020996094 = 0.08696439117193222 + 50.0 * 6.24362850189209
Epoch 1290, val loss: 1.0242263078689575
Epoch 1300, training loss: 312.0768127441406 = 0.08489666134119034 + 50.0 * 6.239838123321533
Epoch 1300, val loss: 1.0300685167312622
Epoch 1310, training loss: 311.9919128417969 = 0.08288693428039551 + 50.0 * 6.238180637359619
Epoch 1310, val loss: 1.0356134176254272
Epoch 1320, training loss: 311.91888427734375 = 0.08097656816244125 + 50.0 * 6.236758708953857
Epoch 1320, val loss: 1.0415197610855103
Epoch 1330, training loss: 311.90765380859375 = 0.07910791784524918 + 50.0 * 6.236570835113525
Epoch 1330, val loss: 1.0472064018249512
Epoch 1340, training loss: 312.432861328125 = 0.07730825245380402 + 50.0 * 6.2471113204956055
Epoch 1340, val loss: 1.0529018640518188
Epoch 1350, training loss: 311.9712829589844 = 0.07550669461488724 + 50.0 * 6.237915515899658
Epoch 1350, val loss: 1.0584603548049927
Epoch 1360, training loss: 311.82757568359375 = 0.07377059757709503 + 50.0 * 6.235076427459717
Epoch 1360, val loss: 1.0641902685165405
Epoch 1370, training loss: 311.9052734375 = 0.07210754603147507 + 50.0 * 6.236663341522217
Epoch 1370, val loss: 1.0698491334915161
Epoch 1380, training loss: 311.89892578125 = 0.07047292590141296 + 50.0 * 6.236568927764893
Epoch 1380, val loss: 1.075545310974121
Epoch 1390, training loss: 311.8017883300781 = 0.0688890814781189 + 50.0 * 6.234658241271973
Epoch 1390, val loss: 1.0811231136322021
Epoch 1400, training loss: 311.7686462402344 = 0.0673600435256958 + 50.0 * 6.234025955200195
Epoch 1400, val loss: 1.0869234800338745
Epoch 1410, training loss: 311.9076232910156 = 0.06587845832109451 + 50.0 * 6.236834526062012
Epoch 1410, val loss: 1.0924822092056274
Epoch 1420, training loss: 311.7458190917969 = 0.0644284263253212 + 50.0 * 6.233627796173096
Epoch 1420, val loss: 1.0980852842330933
Epoch 1430, training loss: 312.0115661621094 = 0.0630251094698906 + 50.0 * 6.238970756530762
Epoch 1430, val loss: 1.1038217544555664
Epoch 1440, training loss: 311.8877868652344 = 0.06164116412401199 + 50.0 * 6.236522674560547
Epoch 1440, val loss: 1.1093872785568237
Epoch 1450, training loss: 311.72271728515625 = 0.06028749793767929 + 50.0 * 6.233248233795166
Epoch 1450, val loss: 1.1149041652679443
Epoch 1460, training loss: 311.6607360839844 = 0.05899379029870033 + 50.0 * 6.232035160064697
Epoch 1460, val loss: 1.1205672025680542
Epoch 1470, training loss: 311.6472473144531 = 0.05775005742907524 + 50.0 * 6.231790065765381
Epoch 1470, val loss: 1.1262420415878296
Epoch 1480, training loss: 311.8845520019531 = 0.05654493719339371 + 50.0 * 6.236560344696045
Epoch 1480, val loss: 1.1319576501846313
Epoch 1490, training loss: 311.78997802734375 = 0.055345568805933 + 50.0 * 6.234692096710205
Epoch 1490, val loss: 1.137261986732483
Epoch 1500, training loss: 311.5835876464844 = 0.05416573956608772 + 50.0 * 6.230588436126709
Epoch 1500, val loss: 1.1428512334823608
Epoch 1510, training loss: 311.5588684082031 = 0.053040824830532074 + 50.0 * 6.230116844177246
Epoch 1510, val loss: 1.1483476161956787
Epoch 1520, training loss: 311.57568359375 = 0.05195613577961922 + 50.0 * 6.230474948883057
Epoch 1520, val loss: 1.153860330581665
Epoch 1530, training loss: 311.7703552246094 = 0.050892844796180725 + 50.0 * 6.234389305114746
Epoch 1530, val loss: 1.159234642982483
Epoch 1540, training loss: 311.70989990234375 = 0.04986304044723511 + 50.0 * 6.233200550079346
Epoch 1540, val loss: 1.164900541305542
Epoch 1550, training loss: 311.5638122558594 = 0.04883153736591339 + 50.0 * 6.230299949645996
Epoch 1550, val loss: 1.1702673435211182
Epoch 1560, training loss: 311.5747375488281 = 0.04785154014825821 + 50.0 * 6.2305378913879395
Epoch 1560, val loss: 1.1757689714431763
Epoch 1570, training loss: 311.5389099121094 = 0.04689529538154602 + 50.0 * 6.22983980178833
Epoch 1570, val loss: 1.181287407875061
Epoch 1580, training loss: 311.46722412109375 = 0.045965272933244705 + 50.0 * 6.2284255027771
Epoch 1580, val loss: 1.1866158246994019
Epoch 1590, training loss: 311.52252197265625 = 0.04506160318851471 + 50.0 * 6.229548931121826
Epoch 1590, val loss: 1.1921004056930542
Epoch 1600, training loss: 311.60888671875 = 0.044177133589982986 + 50.0 * 6.231293678283691
Epoch 1600, val loss: 1.1974244117736816
Epoch 1610, training loss: 311.4707946777344 = 0.04331102594733238 + 50.0 * 6.228549957275391
Epoch 1610, val loss: 1.202688217163086
Epoch 1620, training loss: 311.39068603515625 = 0.042464666068553925 + 50.0 * 6.226964950561523
Epoch 1620, val loss: 1.2080601453781128
Epoch 1630, training loss: 311.37005615234375 = 0.04165205359458923 + 50.0 * 6.226568222045898
Epoch 1630, val loss: 1.2133417129516602
Epoch 1640, training loss: 311.4910888671875 = 0.04086766391992569 + 50.0 * 6.229004383087158
Epoch 1640, val loss: 1.2185766696929932
Epoch 1650, training loss: 311.535400390625 = 0.04010091349482536 + 50.0 * 6.22990608215332
Epoch 1650, val loss: 1.2238354682922363
Epoch 1660, training loss: 311.4658203125 = 0.03932483494281769 + 50.0 * 6.228529930114746
Epoch 1660, val loss: 1.2288960218429565
Epoch 1670, training loss: 311.38226318359375 = 0.038583073765039444 + 50.0 * 6.226873874664307
Epoch 1670, val loss: 1.2341861724853516
Epoch 1680, training loss: 311.3235168457031 = 0.03786325454711914 + 50.0 * 6.225712776184082
Epoch 1680, val loss: 1.2394616603851318
Epoch 1690, training loss: 311.5263671875 = 0.03717392683029175 + 50.0 * 6.22978401184082
Epoch 1690, val loss: 1.2445652484893799
Epoch 1700, training loss: 311.3811950683594 = 0.036478590220212936 + 50.0 * 6.226894378662109
Epoch 1700, val loss: 1.249648928642273
Epoch 1710, training loss: 311.2480773925781 = 0.03581055998802185 + 50.0 * 6.224245548248291
Epoch 1710, val loss: 1.254795789718628
Epoch 1720, training loss: 311.2152099609375 = 0.035162292420864105 + 50.0 * 6.2236008644104
Epoch 1720, val loss: 1.2600780725479126
Epoch 1730, training loss: 311.26617431640625 = 0.03453657403588295 + 50.0 * 6.224632263183594
Epoch 1730, val loss: 1.2651829719543457
Epoch 1740, training loss: 311.5069885253906 = 0.03392210230231285 + 50.0 * 6.229461193084717
Epoch 1740, val loss: 1.2701166868209839
Epoch 1750, training loss: 311.4681701660156 = 0.033308420330286026 + 50.0 * 6.228697299957275
Epoch 1750, val loss: 1.2746917009353638
Epoch 1760, training loss: 311.2434997558594 = 0.032713260501623154 + 50.0 * 6.224215507507324
Epoch 1760, val loss: 1.280051827430725
Epoch 1770, training loss: 311.1537780761719 = 0.03213616833090782 + 50.0 * 6.222433090209961
Epoch 1770, val loss: 1.2850933074951172
Epoch 1780, training loss: 311.2388916015625 = 0.03158438950777054 + 50.0 * 6.224146366119385
Epoch 1780, val loss: 1.2899669408798218
Epoch 1790, training loss: 311.338623046875 = 0.031035639345645905 + 50.0 * 6.226151466369629
Epoch 1790, val loss: 1.2948752641677856
Epoch 1800, training loss: 311.19921875 = 0.03050035983324051 + 50.0 * 6.223374366760254
Epoch 1800, val loss: 1.2997667789459229
Epoch 1810, training loss: 311.14019775390625 = 0.02997388131916523 + 50.0 * 6.222204685211182
Epoch 1810, val loss: 1.304581880569458
Epoch 1820, training loss: 311.3153991699219 = 0.02947819046676159 + 50.0 * 6.2257184982299805
Epoch 1820, val loss: 1.3093645572662354
Epoch 1830, training loss: 311.2529296875 = 0.02897638827562332 + 50.0 * 6.2244791984558105
Epoch 1830, val loss: 1.314165711402893
Epoch 1840, training loss: 311.153564453125 = 0.028483828529715538 + 50.0 * 6.222501754760742
Epoch 1840, val loss: 1.3187708854675293
Epoch 1850, training loss: 311.0918884277344 = 0.028006458654999733 + 50.0 * 6.221277236938477
Epoch 1850, val loss: 1.323737382888794
Epoch 1860, training loss: 311.0525207519531 = 0.027551470324397087 + 50.0 * 6.220499515533447
Epoch 1860, val loss: 1.3284602165222168
Epoch 1870, training loss: 311.21197509765625 = 0.027109064161777496 + 50.0 * 6.223697662353516
Epoch 1870, val loss: 1.3329432010650635
Epoch 1880, training loss: 311.166748046875 = 0.026661958545446396 + 50.0 * 6.222801208496094
Epoch 1880, val loss: 1.337706446647644
Epoch 1890, training loss: 311.0694580078125 = 0.026226116344332695 + 50.0 * 6.220864772796631
Epoch 1890, val loss: 1.342299222946167
Epoch 1900, training loss: 311.034912109375 = 0.02580028586089611 + 50.0 * 6.220182418823242
Epoch 1900, val loss: 1.3469717502593994
Epoch 1910, training loss: 310.99285888671875 = 0.02538980357348919 + 50.0 * 6.219349384307861
Epoch 1910, val loss: 1.3517723083496094
Epoch 1920, training loss: 311.13037109375 = 0.02499775029718876 + 50.0 * 6.222107410430908
Epoch 1920, val loss: 1.3563333749771118
Epoch 1930, training loss: 310.981201171875 = 0.024600178003311157 + 50.0 * 6.219131946563721
Epoch 1930, val loss: 1.3606727123260498
Epoch 1940, training loss: 311.0030822753906 = 0.024217935279011726 + 50.0 * 6.219577312469482
Epoch 1940, val loss: 1.3651692867279053
Epoch 1950, training loss: 311.3044128417969 = 0.02384699322283268 + 50.0 * 6.225611686706543
Epoch 1950, val loss: 1.3697632551193237
Epoch 1960, training loss: 311.1773376464844 = 0.02347067929804325 + 50.0 * 6.223077297210693
Epoch 1960, val loss: 1.3741300106048584
Epoch 1970, training loss: 310.9888916015625 = 0.023101383820176125 + 50.0 * 6.219315528869629
Epoch 1970, val loss: 1.378358244895935
Epoch 1980, training loss: 310.9587707519531 = 0.022748423740267754 + 50.0 * 6.21872091293335
Epoch 1980, val loss: 1.382983922958374
Epoch 1990, training loss: 310.9231872558594 = 0.022408392280340195 + 50.0 * 6.218015670776367
Epoch 1990, val loss: 1.3873802423477173
Epoch 2000, training loss: 310.9858093261719 = 0.022079333662986755 + 50.0 * 6.219274997711182
Epoch 2000, val loss: 1.391890048980713
Epoch 2010, training loss: 310.9131774902344 = 0.021748187020421028 + 50.0 * 6.217828273773193
Epoch 2010, val loss: 1.3960727453231812
Epoch 2020, training loss: 310.951904296875 = 0.021427344530820847 + 50.0 * 6.218609809875488
Epoch 2020, val loss: 1.400197982788086
Epoch 2030, training loss: 310.9514465332031 = 0.021110452711582184 + 50.0 * 6.218606472015381
Epoch 2030, val loss: 1.4043868780136108
Epoch 2040, training loss: 310.9768371582031 = 0.020801259204745293 + 50.0 * 6.219120502471924
Epoch 2040, val loss: 1.4085546731948853
Epoch 2050, training loss: 311.0013427734375 = 0.020504431799054146 + 50.0 * 6.219616889953613
Epoch 2050, val loss: 1.412805199623108
Epoch 2060, training loss: 310.9385681152344 = 0.020203661173582077 + 50.0 * 6.218367099761963
Epoch 2060, val loss: 1.4172070026397705
Epoch 2070, training loss: 311.0115661621094 = 0.0199107863008976 + 50.0 * 6.2198333740234375
Epoch 2070, val loss: 1.4211082458496094
Epoch 2080, training loss: 310.8713073730469 = 0.019621627405285835 + 50.0 * 6.217033386230469
Epoch 2080, val loss: 1.4252474308013916
Epoch 2090, training loss: 310.8094787597656 = 0.019346268847584724 + 50.0 * 6.2158026695251465
Epoch 2090, val loss: 1.429434061050415
Epoch 2100, training loss: 310.8035888671875 = 0.019077982753515244 + 50.0 * 6.2156901359558105
Epoch 2100, val loss: 1.433413028717041
Epoch 2110, training loss: 311.0198059082031 = 0.018816860392689705 + 50.0 * 6.220019817352295
Epoch 2110, val loss: 1.4373571872711182
Epoch 2120, training loss: 310.8516845703125 = 0.01855364255607128 + 50.0 * 6.216662406921387
Epoch 2120, val loss: 1.4416121244430542
Epoch 2130, training loss: 310.83587646484375 = 0.01829572208225727 + 50.0 * 6.21635103225708
Epoch 2130, val loss: 1.4454978704452515
Epoch 2140, training loss: 310.7352600097656 = 0.01804332062602043 + 50.0 * 6.214344501495361
Epoch 2140, val loss: 1.4495216608047485
Epoch 2150, training loss: 310.83087158203125 = 0.017804883420467377 + 50.0 * 6.216261386871338
Epoch 2150, val loss: 1.4534649848937988
Epoch 2160, training loss: 310.9004211425781 = 0.017565099522471428 + 50.0 * 6.217657089233398
Epoch 2160, val loss: 1.457253098487854
Epoch 2170, training loss: 310.8017578125 = 0.017324237152934074 + 50.0 * 6.215688705444336
Epoch 2170, val loss: 1.4613507986068726
Epoch 2180, training loss: 310.7789306640625 = 0.01709541119635105 + 50.0 * 6.215236663818359
Epoch 2180, val loss: 1.4654067754745483
Epoch 2190, training loss: 310.7856750488281 = 0.016872115433216095 + 50.0 * 6.215375900268555
Epoch 2190, val loss: 1.469316005706787
Epoch 2200, training loss: 310.7635498046875 = 0.01664689928293228 + 50.0 * 6.214938163757324
Epoch 2200, val loss: 1.4732266664505005
Epoch 2210, training loss: 310.804931640625 = 0.01642954722046852 + 50.0 * 6.215770244598389
Epoch 2210, val loss: 1.477024793624878
Epoch 2220, training loss: 310.73968505859375 = 0.016216421499848366 + 50.0 * 6.2144694328308105
Epoch 2220, val loss: 1.480851650238037
Epoch 2230, training loss: 310.6929016113281 = 0.016004566103219986 + 50.0 * 6.21353816986084
Epoch 2230, val loss: 1.484439492225647
Epoch 2240, training loss: 310.7397155761719 = 0.01580175757408142 + 50.0 * 6.214478015899658
Epoch 2240, val loss: 1.4882550239562988
Epoch 2250, training loss: 310.76385498046875 = 0.015602217987179756 + 50.0 * 6.214965343475342
Epoch 2250, val loss: 1.492051362991333
Epoch 2260, training loss: 310.8827819824219 = 0.015409641899168491 + 50.0 * 6.217347145080566
Epoch 2260, val loss: 1.495820164680481
Epoch 2270, training loss: 310.69183349609375 = 0.015208335593342781 + 50.0 * 6.2135329246521
Epoch 2270, val loss: 1.4997057914733887
Epoch 2280, training loss: 310.6703186035156 = 0.015018358826637268 + 50.0 * 6.213106155395508
Epoch 2280, val loss: 1.5033200979232788
Epoch 2290, training loss: 310.6635437011719 = 0.014833388850092888 + 50.0 * 6.2129740715026855
Epoch 2290, val loss: 1.5071063041687012
Epoch 2300, training loss: 310.7209777832031 = 0.014652160927653313 + 50.0 * 6.2141265869140625
Epoch 2300, val loss: 1.5104660987854004
Epoch 2310, training loss: 310.79241943359375 = 0.01447275746613741 + 50.0 * 6.215559005737305
Epoch 2310, val loss: 1.5139553546905518
Epoch 2320, training loss: 310.614990234375 = 0.014295924454927444 + 50.0 * 6.212014198303223
Epoch 2320, val loss: 1.517747163772583
Epoch 2330, training loss: 310.6125793457031 = 0.014122067019343376 + 50.0 * 6.211968898773193
Epoch 2330, val loss: 1.521585464477539
Epoch 2340, training loss: 310.6469421386719 = 0.013954262249171734 + 50.0 * 6.21265983581543
Epoch 2340, val loss: 1.5250967741012573
Epoch 2350, training loss: 310.59417724609375 = 0.013790706172585487 + 50.0 * 6.211607933044434
Epoch 2350, val loss: 1.5286275148391724
Epoch 2360, training loss: 310.65985107421875 = 0.013629895634949207 + 50.0 * 6.212924480438232
Epoch 2360, val loss: 1.532069206237793
Epoch 2370, training loss: 310.6426696777344 = 0.01346675306558609 + 50.0 * 6.212584495544434
Epoch 2370, val loss: 1.5354799032211304
Epoch 2380, training loss: 310.5636291503906 = 0.01330850925296545 + 50.0 * 6.2110066413879395
Epoch 2380, val loss: 1.5392050743103027
Epoch 2390, training loss: 310.5787658691406 = 0.013158047571778297 + 50.0 * 6.211312294006348
Epoch 2390, val loss: 1.5425894260406494
Epoch 2400, training loss: 310.8572998046875 = 0.013009347952902317 + 50.0 * 6.216886043548584
Epoch 2400, val loss: 1.5461465120315552
Epoch 2410, training loss: 310.6689147949219 = 0.012855440378189087 + 50.0 * 6.21312141418457
Epoch 2410, val loss: 1.5489270687103271
Epoch 2420, training loss: 310.639404296875 = 0.012707119807600975 + 50.0 * 6.212534427642822
Epoch 2420, val loss: 1.5528314113616943
Epoch 2430, training loss: 310.51678466796875 = 0.012561935931444168 + 50.0 * 6.210083961486816
Epoch 2430, val loss: 1.556012511253357
Epoch 2440, training loss: 310.56549072265625 = 0.012424021027982235 + 50.0 * 6.211061477661133
Epoch 2440, val loss: 1.5593889951705933
Epoch 2450, training loss: 310.5857849121094 = 0.012287747114896774 + 50.0 * 6.211469650268555
Epoch 2450, val loss: 1.5626838207244873
Epoch 2460, training loss: 310.5068664550781 = 0.012151164002716541 + 50.0 * 6.20989465713501
Epoch 2460, val loss: 1.5664377212524414
Epoch 2470, training loss: 310.5372619628906 = 0.012020372785627842 + 50.0 * 6.210504531860352
Epoch 2470, val loss: 1.569533348083496
Epoch 2480, training loss: 310.6546936035156 = 0.011890341527760029 + 50.0 * 6.212855815887451
Epoch 2480, val loss: 1.5729098320007324
Epoch 2490, training loss: 310.6193542480469 = 0.011760883964598179 + 50.0 * 6.212152004241943
Epoch 2490, val loss: 1.5761574506759644
Epoch 2500, training loss: 310.470703125 = 0.011627967469394207 + 50.0 * 6.209181308746338
Epoch 2500, val loss: 1.5791723728179932
Epoch 2510, training loss: 310.4377136230469 = 0.011504679918289185 + 50.0 * 6.208524227142334
Epoch 2510, val loss: 1.582585334777832
Epoch 2520, training loss: 310.4142150878906 = 0.01138585340231657 + 50.0 * 6.208056926727295
Epoch 2520, val loss: 1.5858796834945679
Epoch 2530, training loss: 310.6092224121094 = 0.011270696297287941 + 50.0 * 6.211959362030029
Epoch 2530, val loss: 1.5891920328140259
Epoch 2540, training loss: 310.6258544921875 = 0.011149733327329159 + 50.0 * 6.212294578552246
Epoch 2540, val loss: 1.592164158821106
Epoch 2550, training loss: 310.4361877441406 = 0.011028925888240337 + 50.0 * 6.208503723144531
Epoch 2550, val loss: 1.59530508518219
Epoch 2560, training loss: 310.4052734375 = 0.010915005579590797 + 50.0 * 6.207886695861816
Epoch 2560, val loss: 1.5983972549438477
Epoch 2570, training loss: 310.41839599609375 = 0.010804468765854836 + 50.0 * 6.208151817321777
Epoch 2570, val loss: 1.6017875671386719
Epoch 2580, training loss: 310.5984191894531 = 0.010697494260966778 + 50.0 * 6.211753845214844
Epoch 2580, val loss: 1.6047805547714233
Epoch 2590, training loss: 310.40728759765625 = 0.010588441044092178 + 50.0 * 6.2079339027404785
Epoch 2590, val loss: 1.607779860496521
Epoch 2600, training loss: 310.4397277832031 = 0.010483060963451862 + 50.0 * 6.208584785461426
Epoch 2600, val loss: 1.6109271049499512
Epoch 2610, training loss: 310.4781188964844 = 0.010377020575106144 + 50.0 * 6.209354877471924
Epoch 2610, val loss: 1.614136815071106
Epoch 2620, training loss: 310.41497802734375 = 0.010272571817040443 + 50.0 * 6.208094120025635
Epoch 2620, val loss: 1.6170470714569092
Epoch 2630, training loss: 310.3343200683594 = 0.010171089321374893 + 50.0 * 6.206482887268066
Epoch 2630, val loss: 1.620063066482544
Epoch 2640, training loss: 310.34735107421875 = 0.010073409415781498 + 50.0 * 6.206745624542236
Epoch 2640, val loss: 1.6232459545135498
Epoch 2650, training loss: 310.4776916503906 = 0.009978137910366058 + 50.0 * 6.209354400634766
Epoch 2650, val loss: 1.6260981559753418
Epoch 2660, training loss: 310.4386901855469 = 0.009879686869680882 + 50.0 * 6.208576202392578
Epoch 2660, val loss: 1.628892183303833
Epoch 2670, training loss: 310.3368835449219 = 0.00978451780974865 + 50.0 * 6.206542015075684
Epoch 2670, val loss: 1.632094144821167
Epoch 2680, training loss: 310.40155029296875 = 0.00969155877828598 + 50.0 * 6.207836627960205
Epoch 2680, val loss: 1.6347676515579224
Epoch 2690, training loss: 310.4657897949219 = 0.009600196033716202 + 50.0 * 6.209123611450195
Epoch 2690, val loss: 1.6374287605285645
Epoch 2700, training loss: 310.47918701171875 = 0.009509466588497162 + 50.0 * 6.20939302444458
Epoch 2700, val loss: 1.6407328844070435
Epoch 2710, training loss: 310.3146057128906 = 0.009416273795068264 + 50.0 * 6.206103801727295
Epoch 2710, val loss: 1.643641710281372
Epoch 2720, training loss: 310.27557373046875 = 0.009329432621598244 + 50.0 * 6.205324649810791
Epoch 2720, val loss: 1.646660566329956
Epoch 2730, training loss: 310.3207092285156 = 0.009246433153748512 + 50.0 * 6.206229209899902
Epoch 2730, val loss: 1.6495065689086914
Epoch 2740, training loss: 310.4585876464844 = 0.009163347072899342 + 50.0 * 6.208988666534424
Epoch 2740, val loss: 1.6523218154907227
Epoch 2750, training loss: 310.3337097167969 = 0.009078945964574814 + 50.0 * 6.206492900848389
Epoch 2750, val loss: 1.6551111936569214
Epoch 2760, training loss: 310.3839416503906 = 0.008998638950288296 + 50.0 * 6.207499027252197
Epoch 2760, val loss: 1.6579539775848389
Epoch 2770, training loss: 310.3735656738281 = 0.00891679897904396 + 50.0 * 6.2072930335998535
Epoch 2770, val loss: 1.6607356071472168
Epoch 2780, training loss: 310.2935791015625 = 0.008834609761834145 + 50.0 * 6.205695152282715
Epoch 2780, val loss: 1.663367509841919
Epoch 2790, training loss: 310.20648193359375 = 0.008757374249398708 + 50.0 * 6.203954219818115
Epoch 2790, val loss: 1.666375994682312
Epoch 2800, training loss: 310.44232177734375 = 0.008684882894158363 + 50.0 * 6.208672523498535
Epoch 2800, val loss: 1.6688114404678345
Epoch 2810, training loss: 310.245849609375 = 0.008605729788541794 + 50.0 * 6.204744815826416
Epoch 2810, val loss: 1.6718287467956543
Epoch 2820, training loss: 310.28533935546875 = 0.008530777879059315 + 50.0 * 6.205535888671875
Epoch 2820, val loss: 1.6744191646575928
Epoch 2830, training loss: 310.2498474121094 = 0.008457658812403679 + 50.0 * 6.204827785491943
Epoch 2830, val loss: 1.6773372888565063
Epoch 2840, training loss: 310.2290954589844 = 0.008384523913264275 + 50.0 * 6.204413890838623
Epoch 2840, val loss: 1.6798561811447144
Epoch 2850, training loss: 310.3236389160156 = 0.008315706625580788 + 50.0 * 6.2063069343566895
Epoch 2850, val loss: 1.6827369928359985
Epoch 2860, training loss: 310.22943115234375 = 0.008244628086686134 + 50.0 * 6.204423904418945
Epoch 2860, val loss: 1.6853444576263428
Epoch 2870, training loss: 310.2011413574219 = 0.008174709975719452 + 50.0 * 6.203859329223633
Epoch 2870, val loss: 1.6879056692123413
Epoch 2880, training loss: 310.31268310546875 = 0.00810842216014862 + 50.0 * 6.206091403961182
Epoch 2880, val loss: 1.6904103755950928
Epoch 2890, training loss: 310.269775390625 = 0.008039773441851139 + 50.0 * 6.205235004425049
Epoch 2890, val loss: 1.6930142641067505
Epoch 2900, training loss: 310.237060546875 = 0.00797248538583517 + 50.0 * 6.2045817375183105
Epoch 2900, val loss: 1.6958448886871338
Epoch 2910, training loss: 310.17864990234375 = 0.007906587794423103 + 50.0 * 6.2034149169921875
Epoch 2910, val loss: 1.698348879814148
Epoch 2920, training loss: 310.2842712402344 = 0.007846148684620857 + 50.0 * 6.205528259277344
Epoch 2920, val loss: 1.7012600898742676
Epoch 2930, training loss: 310.235107421875 = 0.00778120756149292 + 50.0 * 6.2045464515686035
Epoch 2930, val loss: 1.7035424709320068
Epoch 2940, training loss: 310.20703125 = 0.007717455271631479 + 50.0 * 6.203986167907715
Epoch 2940, val loss: 1.706048607826233
Epoch 2950, training loss: 310.3692932128906 = 0.007660253439098597 + 50.0 * 6.20723295211792
Epoch 2950, val loss: 1.708729863166809
Epoch 2960, training loss: 310.1275634765625 = 0.007594235707074404 + 50.0 * 6.202399253845215
Epoch 2960, val loss: 1.7112481594085693
Epoch 2970, training loss: 310.3153076171875 = 0.0075357332825660706 + 50.0 * 6.206155300140381
Epoch 2970, val loss: 1.7136040925979614
Epoch 2980, training loss: 310.1500244140625 = 0.007474754936993122 + 50.0 * 6.202850818634033
Epoch 2980, val loss: 1.7161396741867065
Epoch 2990, training loss: 310.1244201660156 = 0.0074171386659145355 + 50.0 * 6.202340126037598
Epoch 2990, val loss: 1.7188794612884521
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.838165524512388
The final CL Acc:0.75926, 0.00800, The final GNN Acc:0.83764, 0.00043
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11640])
remove edge: torch.Size([2, 9522])
updated graph: torch.Size([2, 10606])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7750549316406 = 1.9321634769439697 + 50.0 * 8.596858024597168
Epoch 0, val loss: 1.922804594039917
Epoch 10, training loss: 431.7351989746094 = 1.9239996671676636 + 50.0 * 8.596223831176758
Epoch 10, val loss: 1.9149855375289917
Epoch 20, training loss: 431.515380859375 = 1.9138463735580444 + 50.0 * 8.59203052520752
Epoch 20, val loss: 1.904969334602356
Epoch 30, training loss: 429.8592834472656 = 1.9006918668746948 + 50.0 * 8.559171676635742
Epoch 30, val loss: 1.8918794393539429
Epoch 40, training loss: 416.4309997558594 = 1.8844273090362549 + 50.0 * 8.290931701660156
Epoch 40, val loss: 1.8758655786514282
Epoch 50, training loss: 377.3061218261719 = 1.866181492805481 + 50.0 * 7.508799076080322
Epoch 50, val loss: 1.8584177494049072
Epoch 60, training loss: 362.5478820800781 = 1.8537216186523438 + 50.0 * 7.213882923126221
Epoch 60, val loss: 1.8469054698944092
Epoch 70, training loss: 352.4420471191406 = 1.8441309928894043 + 50.0 * 7.011958599090576
Epoch 70, val loss: 1.8375332355499268
Epoch 80, training loss: 345.1844177246094 = 1.8349568843841553 + 50.0 * 6.8669891357421875
Epoch 80, val loss: 1.8286645412445068
Epoch 90, training loss: 340.6444091796875 = 1.8258919715881348 + 50.0 * 6.776370048522949
Epoch 90, val loss: 1.820084810256958
Epoch 100, training loss: 337.12811279296875 = 1.8175941705703735 + 50.0 * 6.706210136413574
Epoch 100, val loss: 1.8122670650482178
Epoch 110, training loss: 334.3891906738281 = 1.8098809719085693 + 50.0 * 6.651586532592773
Epoch 110, val loss: 1.8048549890518188
Epoch 120, training loss: 332.34271240234375 = 1.8020579814910889 + 50.0 * 6.610813140869141
Epoch 120, val loss: 1.7972220182418823
Epoch 130, training loss: 330.6944274902344 = 1.793933629989624 + 50.0 * 6.578009605407715
Epoch 130, val loss: 1.7894002199172974
Epoch 140, training loss: 329.2866516113281 = 1.785640001296997 + 50.0 * 6.550020217895508
Epoch 140, val loss: 1.7814629077911377
Epoch 150, training loss: 328.04083251953125 = 1.776971459388733 + 50.0 * 6.525277137756348
Epoch 150, val loss: 1.773442029953003
Epoch 160, training loss: 327.0999755859375 = 1.767807126045227 + 50.0 * 6.506643295288086
Epoch 160, val loss: 1.7650281190872192
Epoch 170, training loss: 326.1748962402344 = 1.757773518562317 + 50.0 * 6.48834228515625
Epoch 170, val loss: 1.7560832500457764
Epoch 180, training loss: 325.4289855957031 = 1.746907114982605 + 50.0 * 6.473641395568848
Epoch 180, val loss: 1.7464253902435303
Epoch 190, training loss: 324.76513671875 = 1.7350386381149292 + 50.0 * 6.460601806640625
Epoch 190, val loss: 1.7359752655029297
Epoch 200, training loss: 324.2112121582031 = 1.7219810485839844 + 50.0 * 6.449784278869629
Epoch 200, val loss: 1.7244600057601929
Epoch 210, training loss: 323.6289367675781 = 1.7076407670974731 + 50.0 * 6.4384260177612305
Epoch 210, val loss: 1.711963176727295
Epoch 220, training loss: 323.14324951171875 = 1.6919759511947632 + 50.0 * 6.429025173187256
Epoch 220, val loss: 1.6983665227890015
Epoch 230, training loss: 322.90509033203125 = 1.674835443496704 + 50.0 * 6.424605369567871
Epoch 230, val loss: 1.6835191249847412
Epoch 240, training loss: 322.372314453125 = 1.6562553644180298 + 50.0 * 6.414321422576904
Epoch 240, val loss: 1.667540192604065
Epoch 250, training loss: 321.9326171875 = 1.636195421218872 + 50.0 * 6.405928134918213
Epoch 250, val loss: 1.6503907442092896
Epoch 260, training loss: 321.6676330566406 = 1.614747405052185 + 50.0 * 6.401057720184326
Epoch 260, val loss: 1.6321537494659424
Epoch 270, training loss: 321.39471435546875 = 1.591801643371582 + 50.0 * 6.396058559417725
Epoch 270, val loss: 1.6128607988357544
Epoch 280, training loss: 320.96246337890625 = 1.567718744277954 + 50.0 * 6.387895107269287
Epoch 280, val loss: 1.592626690864563
Epoch 290, training loss: 320.6695556640625 = 1.5425091981887817 + 50.0 * 6.382541179656982
Epoch 290, val loss: 1.5716946125030518
Epoch 300, training loss: 320.6617431640625 = 1.5162346363067627 + 50.0 * 6.382910251617432
Epoch 300, val loss: 1.550069808959961
Epoch 310, training loss: 320.2068176269531 = 1.4892315864562988 + 50.0 * 6.374351501464844
Epoch 310, val loss: 1.528045892715454
Epoch 320, training loss: 319.8969421386719 = 1.4616310596466064 + 50.0 * 6.368706226348877
Epoch 320, val loss: 1.5058224201202393
Epoch 330, training loss: 319.6856384277344 = 1.4336817264556885 + 50.0 * 6.365039348602295
Epoch 330, val loss: 1.4835305213928223
Epoch 340, training loss: 319.4787902832031 = 1.4053902626037598 + 50.0 * 6.3614678382873535
Epoch 340, val loss: 1.4612197875976562
Epoch 350, training loss: 319.1894836425781 = 1.3771013021469116 + 50.0 * 6.356247425079346
Epoch 350, val loss: 1.4390705823898315
Epoch 360, training loss: 319.00823974609375 = 1.3489032983779907 + 50.0 * 6.35318660736084
Epoch 360, val loss: 1.417260766029358
Epoch 370, training loss: 318.8193359375 = 1.3208460807800293 + 50.0 * 6.349969863891602
Epoch 370, val loss: 1.3957250118255615
Epoch 380, training loss: 318.6178894042969 = 1.2931193113327026 + 50.0 * 6.346495151519775
Epoch 380, val loss: 1.3745810985565186
Epoch 390, training loss: 318.3940734863281 = 1.2658814191818237 + 50.0 * 6.342564105987549
Epoch 390, val loss: 1.354004979133606
Epoch 400, training loss: 318.3763732910156 = 1.239158034324646 + 50.0 * 6.34274435043335
Epoch 400, val loss: 1.3338603973388672
Epoch 410, training loss: 318.1219177246094 = 1.2126530408859253 + 50.0 * 6.3381853103637695
Epoch 410, val loss: 1.314186930656433
Epoch 420, training loss: 317.8393859863281 = 1.1868617534637451 + 50.0 * 6.333050727844238
Epoch 420, val loss: 1.2950302362442017
Epoch 430, training loss: 317.79150390625 = 1.1615523099899292 + 50.0 * 6.332598686218262
Epoch 430, val loss: 1.276386022567749
Epoch 440, training loss: 317.5938415527344 = 1.1367566585540771 + 50.0 * 6.329141616821289
Epoch 440, val loss: 1.257899284362793
Epoch 450, training loss: 317.54656982421875 = 1.1125153303146362 + 50.0 * 6.328680992126465
Epoch 450, val loss: 1.2400593757629395
Epoch 460, training loss: 317.2610778808594 = 1.0887809991836548 + 50.0 * 6.323446273803711
Epoch 460, val loss: 1.2227859497070312
Epoch 470, training loss: 317.09881591796875 = 1.0657355785369873 + 50.0 * 6.320661544799805
Epoch 470, val loss: 1.2059776782989502
Epoch 480, training loss: 316.9188537597656 = 1.0432525873184204 + 50.0 * 6.317512512207031
Epoch 480, val loss: 1.1897529363632202
Epoch 490, training loss: 316.9472961425781 = 1.0214096307754517 + 50.0 * 6.318517684936523
Epoch 490, val loss: 1.1741634607315063
Epoch 500, training loss: 316.7990417480469 = 1.000096321105957 + 50.0 * 6.31597900390625
Epoch 500, val loss: 1.1590657234191895
Epoch 510, training loss: 316.5199279785156 = 0.9793528318405151 + 50.0 * 6.310811996459961
Epoch 510, val loss: 1.1445504426956177
Epoch 520, training loss: 316.73736572265625 = 0.9593009352684021 + 50.0 * 6.315561771392822
Epoch 520, val loss: 1.1304869651794434
Epoch 530, training loss: 316.4300537109375 = 0.9396380186080933 + 50.0 * 6.309808254241943
Epoch 530, val loss: 1.1175248622894287
Epoch 540, training loss: 316.1673889160156 = 0.9206489324569702 + 50.0 * 6.304934978485107
Epoch 540, val loss: 1.1047600507736206
Epoch 550, training loss: 316.173095703125 = 0.9021725654602051 + 50.0 * 6.305418491363525
Epoch 550, val loss: 1.092522144317627
Epoch 560, training loss: 316.2044372558594 = 0.8841010928153992 + 50.0 * 6.3064069747924805
Epoch 560, val loss: 1.0811454057693481
Epoch 570, training loss: 315.9281921386719 = 0.8663937449455261 + 50.0 * 6.301236152648926
Epoch 570, val loss: 1.0701146125793457
Epoch 580, training loss: 315.75628662109375 = 0.8492031693458557 + 50.0 * 6.2981414794921875
Epoch 580, val loss: 1.0596460103988647
Epoch 590, training loss: 315.69287109375 = 0.8324178457260132 + 50.0 * 6.297208786010742
Epoch 590, val loss: 1.0495736598968506
Epoch 600, training loss: 315.66998291015625 = 0.8158503770828247 + 50.0 * 6.297082424163818
Epoch 600, val loss: 1.0399919748306274
Epoch 610, training loss: 315.5555419921875 = 0.7994372844696045 + 50.0 * 6.295122146606445
Epoch 610, val loss: 1.0306729078292847
Epoch 620, training loss: 315.43890380859375 = 0.7833229899406433 + 50.0 * 6.293111324310303
Epoch 620, val loss: 1.0218770503997803
Epoch 630, training loss: 315.3524169921875 = 0.7675067186355591 + 50.0 * 6.291697978973389
Epoch 630, val loss: 1.0135207176208496
Epoch 640, training loss: 315.2994689941406 = 0.7518088817596436 + 50.0 * 6.290953636169434
Epoch 640, val loss: 1.0055325031280518
Epoch 650, training loss: 315.15533447265625 = 0.7362565398216248 + 50.0 * 6.288381576538086
Epoch 650, val loss: 0.9979444742202759
Epoch 660, training loss: 315.5003356933594 = 0.7208411693572998 + 50.0 * 6.295589447021484
Epoch 660, val loss: 0.9905678033828735
Epoch 670, training loss: 315.14617919921875 = 0.7056164145469666 + 50.0 * 6.288811206817627
Epoch 670, val loss: 0.9832631349563599
Epoch 680, training loss: 314.9012451171875 = 0.6904111504554749 + 50.0 * 6.28421688079834
Epoch 680, val loss: 0.9764001965522766
Epoch 690, training loss: 314.81829833984375 = 0.675446629524231 + 50.0 * 6.2828569412231445
Epoch 690, val loss: 0.9701168537139893
Epoch 700, training loss: 314.787841796875 = 0.6606264710426331 + 50.0 * 6.2825446128845215
Epoch 700, val loss: 0.9639974236488342
Epoch 710, training loss: 314.7973327636719 = 0.6458196640014648 + 50.0 * 6.2830305099487305
Epoch 710, val loss: 0.9581391215324402
Epoch 720, training loss: 314.74456787109375 = 0.6311553120613098 + 50.0 * 6.282268047332764
Epoch 720, val loss: 0.9520961046218872
Epoch 730, training loss: 314.5641784667969 = 0.6166988015174866 + 50.0 * 6.278949737548828
Epoch 730, val loss: 0.9468225836753845
Epoch 740, training loss: 314.4498596191406 = 0.6024438738822937 + 50.0 * 6.27694845199585
Epoch 740, val loss: 0.9417959451675415
Epoch 750, training loss: 314.849365234375 = 0.5883498787879944 + 50.0 * 6.285220146179199
Epoch 750, val loss: 0.936843991279602
Epoch 760, training loss: 314.4995422363281 = 0.5744181275367737 + 50.0 * 6.278501987457275
Epoch 760, val loss: 0.9326867461204529
Epoch 770, training loss: 314.3066101074219 = 0.5606437921524048 + 50.0 * 6.274919509887695
Epoch 770, val loss: 0.9285616874694824
Epoch 780, training loss: 314.1985778808594 = 0.547184944152832 + 50.0 * 6.2730278968811035
Epoch 780, val loss: 0.9246113896369934
Epoch 790, training loss: 314.124755859375 = 0.5340354442596436 + 50.0 * 6.271814346313477
Epoch 790, val loss: 0.9212462306022644
Epoch 800, training loss: 314.29510498046875 = 0.5211669206619263 + 50.0 * 6.275478363037109
Epoch 800, val loss: 0.918515145778656
Epoch 810, training loss: 314.362548828125 = 0.508346676826477 + 50.0 * 6.277083873748779
Epoch 810, val loss: 0.9150790572166443
Epoch 820, training loss: 313.98486328125 = 0.49590572714805603 + 50.0 * 6.269779205322266
Epoch 820, val loss: 0.9124234914779663
Epoch 830, training loss: 313.9012756347656 = 0.48379698395729065 + 50.0 * 6.268349647521973
Epoch 830, val loss: 0.910454511642456
Epoch 840, training loss: 314.1158142089844 = 0.47201040387153625 + 50.0 * 6.272876262664795
Epoch 840, val loss: 0.9085587859153748
Epoch 850, training loss: 313.8322448730469 = 0.46047237515449524 + 50.0 * 6.267435073852539
Epoch 850, val loss: 0.9069616794586182
Epoch 860, training loss: 313.7625427246094 = 0.4491795599460602 + 50.0 * 6.266266822814941
Epoch 860, val loss: 0.9054766893386841
Epoch 870, training loss: 313.7740173339844 = 0.4383082091808319 + 50.0 * 6.266714572906494
Epoch 870, val loss: 0.9043399095535278
Epoch 880, training loss: 313.69769287109375 = 0.42755889892578125 + 50.0 * 6.265402793884277
Epoch 880, val loss: 0.9035871624946594
Epoch 890, training loss: 313.623046875 = 0.41713470220565796 + 50.0 * 6.264118194580078
Epoch 890, val loss: 0.9028818607330322
Epoch 900, training loss: 313.6194763183594 = 0.4069412350654602 + 50.0 * 6.264250755310059
Epoch 900, val loss: 0.9026857614517212
Epoch 910, training loss: 313.60516357421875 = 0.39708346128463745 + 50.0 * 6.264161586761475
Epoch 910, val loss: 0.9025086164474487
Epoch 920, training loss: 313.44000244140625 = 0.3873702883720398 + 50.0 * 6.261053085327148
Epoch 920, val loss: 0.9026450514793396
Epoch 930, training loss: 313.4196472167969 = 0.37794098258018494 + 50.0 * 6.260834217071533
Epoch 930, val loss: 0.9032089114189148
Epoch 940, training loss: 313.61810302734375 = 0.36879637837409973 + 50.0 * 6.264986038208008
Epoch 940, val loss: 0.9040249586105347
Epoch 950, training loss: 313.4126892089844 = 0.3596675992012024 + 50.0 * 6.2610602378845215
Epoch 950, val loss: 0.9044403433799744
Epoch 960, training loss: 313.4522705078125 = 0.35083654522895813 + 50.0 * 6.262028694152832
Epoch 960, val loss: 0.905791163444519
Epoch 970, training loss: 313.2658386230469 = 0.3421451151371002 + 50.0 * 6.258474349975586
Epoch 970, val loss: 0.9068641066551208
Epoch 980, training loss: 313.2057800292969 = 0.3336954116821289 + 50.0 * 6.257441520690918
Epoch 980, val loss: 0.9085707664489746
Epoch 990, training loss: 313.2680969238281 = 0.3254402279853821 + 50.0 * 6.258852958679199
Epoch 990, val loss: 0.9102319478988647
Epoch 1000, training loss: 313.2402648925781 = 0.31730687618255615 + 50.0 * 6.258458614349365
Epoch 1000, val loss: 0.9118491411209106
Epoch 1010, training loss: 313.1981506347656 = 0.30942586064338684 + 50.0 * 6.257774353027344
Epoch 1010, val loss: 0.9140365719795227
Epoch 1020, training loss: 313.1149597167969 = 0.30153656005859375 + 50.0 * 6.25626802444458
Epoch 1020, val loss: 0.9160979986190796
Epoch 1030, training loss: 313.1202392578125 = 0.2939704358577728 + 50.0 * 6.25652551651001
Epoch 1030, val loss: 0.9187701344490051
Epoch 1040, training loss: 313.01824951171875 = 0.2864541709423065 + 50.0 * 6.254635810852051
Epoch 1040, val loss: 0.9212013483047485
Epoch 1050, training loss: 312.98504638671875 = 0.27913543581962585 + 50.0 * 6.254117965698242
Epoch 1050, val loss: 0.9234115481376648
Epoch 1060, training loss: 313.08258056640625 = 0.2719881236553192 + 50.0 * 6.256211757659912
Epoch 1060, val loss: 0.9263564944267273
Epoch 1070, training loss: 313.0146179199219 = 0.2649294137954712 + 50.0 * 6.254993915557861
Epoch 1070, val loss: 0.9291456341743469
Epoch 1080, training loss: 312.8619689941406 = 0.2580147683620453 + 50.0 * 6.252079010009766
Epoch 1080, val loss: 0.9320160150527954
Epoch 1090, training loss: 312.8829040527344 = 0.2512962222099304 + 50.0 * 6.2526326179504395
Epoch 1090, val loss: 0.9351219534873962
Epoch 1100, training loss: 312.7363586425781 = 0.24470798671245575 + 50.0 * 6.249832630157471
Epoch 1100, val loss: 0.9383904337882996
Epoch 1110, training loss: 312.7457580566406 = 0.23827527463436127 + 50.0 * 6.250149726867676
Epoch 1110, val loss: 0.9417263865470886
Epoch 1120, training loss: 313.01434326171875 = 0.2320515662431717 + 50.0 * 6.255645751953125
Epoch 1120, val loss: 0.9453980922698975
Epoch 1130, training loss: 312.8065185546875 = 0.22580179572105408 + 50.0 * 6.251614570617676
Epoch 1130, val loss: 0.9485254287719727
Epoch 1140, training loss: 312.6008605957031 = 0.2197694033384323 + 50.0 * 6.247622013092041
Epoch 1140, val loss: 0.9521418809890747
Epoch 1150, training loss: 312.55462646484375 = 0.21394309401512146 + 50.0 * 6.246814250946045
Epoch 1150, val loss: 0.9557850956916809
Epoch 1160, training loss: 312.53485107421875 = 0.20828427374362946 + 50.0 * 6.2465314865112305
Epoch 1160, val loss: 0.9597567915916443
Epoch 1170, training loss: 313.0179138183594 = 0.20275484025478363 + 50.0 * 6.256302833557129
Epoch 1170, val loss: 0.9638146162033081
Epoch 1180, training loss: 312.74066162109375 = 0.19732649624347687 + 50.0 * 6.250866889953613
Epoch 1180, val loss: 0.9674059748649597
Epoch 1190, training loss: 312.53680419921875 = 0.1920313984155655 + 50.0 * 6.246895790100098
Epoch 1190, val loss: 0.9711400866508484
Epoch 1200, training loss: 312.43988037109375 = 0.18692021071910858 + 50.0 * 6.245059490203857
Epoch 1200, val loss: 0.9755352735519409
Epoch 1210, training loss: 312.4137268066406 = 0.18198774755001068 + 50.0 * 6.244635105133057
Epoch 1210, val loss: 0.9798129796981812
Epoch 1220, training loss: 312.65960693359375 = 0.17721012234687805 + 50.0 * 6.249648094177246
Epoch 1220, val loss: 0.98430335521698
Epoch 1230, training loss: 312.4519348144531 = 0.1724739670753479 + 50.0 * 6.245588779449463
Epoch 1230, val loss: 0.9874302744865417
Epoch 1240, training loss: 312.3604431152344 = 0.1679094284772873 + 50.0 * 6.2438507080078125
Epoch 1240, val loss: 0.9924129843711853
Epoch 1250, training loss: 312.29302978515625 = 0.1634650081396103 + 50.0 * 6.242591381072998
Epoch 1250, val loss: 0.996466875076294
Epoch 1260, training loss: 312.3510437011719 = 0.1591903567314148 + 50.0 * 6.243837356567383
Epoch 1260, val loss: 1.0009528398513794
Epoch 1270, training loss: 312.3206787109375 = 0.15500718355178833 + 50.0 * 6.243313789367676
Epoch 1270, val loss: 1.0052566528320312
Epoch 1280, training loss: 312.42742919921875 = 0.15094543993473053 + 50.0 * 6.245529651641846
Epoch 1280, val loss: 1.0093220472335815
Epoch 1290, training loss: 312.4947509765625 = 0.1469597965478897 + 50.0 * 6.246955394744873
Epoch 1290, val loss: 1.0145598649978638
Epoch 1300, training loss: 312.1604919433594 = 0.14313963055610657 + 50.0 * 6.240347385406494
Epoch 1300, val loss: 1.0186196565628052
Epoch 1310, training loss: 312.14599609375 = 0.139409139752388 + 50.0 * 6.240131855010986
Epoch 1310, val loss: 1.0236644744873047
Epoch 1320, training loss: 312.1189270019531 = 0.1358376145362854 + 50.0 * 6.239661693572998
Epoch 1320, val loss: 1.0283912420272827
Epoch 1330, training loss: 312.14544677734375 = 0.13235296308994293 + 50.0 * 6.240261554718018
Epoch 1330, val loss: 1.0328854322433472
Epoch 1340, training loss: 312.30548095703125 = 0.12895254790782928 + 50.0 * 6.2435302734375
Epoch 1340, val loss: 1.0377280712127686
Epoch 1350, training loss: 312.0727844238281 = 0.12562786042690277 + 50.0 * 6.238943099975586
Epoch 1350, val loss: 1.0420194864273071
Epoch 1360, training loss: 312.0637512207031 = 0.12244301289319992 + 50.0 * 6.238826274871826
Epoch 1360, val loss: 1.0472768545150757
Epoch 1370, training loss: 312.0703430175781 = 0.11933951079845428 + 50.0 * 6.239019870758057
Epoch 1370, val loss: 1.0522273778915405
Epoch 1380, training loss: 312.2007751464844 = 0.11637174338102341 + 50.0 * 6.241688251495361
Epoch 1380, val loss: 1.0570459365844727
Epoch 1390, training loss: 312.1562805175781 = 0.11343017965555191 + 50.0 * 6.240856647491455
Epoch 1390, val loss: 1.0610136985778809
Epoch 1400, training loss: 312.0262145996094 = 0.1105620488524437 + 50.0 * 6.238312721252441
Epoch 1400, val loss: 1.0663917064666748
Epoch 1410, training loss: 311.9245910644531 = 0.10781653970479965 + 50.0 * 6.236335754394531
Epoch 1410, val loss: 1.0708941221237183
Epoch 1420, training loss: 311.8827819824219 = 0.10514708608388901 + 50.0 * 6.23555326461792
Epoch 1420, val loss: 1.0759094953536987
Epoch 1430, training loss: 312.16876220703125 = 0.10259028524160385 + 50.0 * 6.241323471069336
Epoch 1430, val loss: 1.0802720785140991
Epoch 1440, training loss: 312.0015869140625 = 0.1000434085726738 + 50.0 * 6.238030910491943
Epoch 1440, val loss: 1.0858917236328125
Epoch 1450, training loss: 311.9271240234375 = 0.09754651039838791 + 50.0 * 6.236591815948486
Epoch 1450, val loss: 1.0901103019714355
Epoch 1460, training loss: 312.0780944824219 = 0.09516102075576782 + 50.0 * 6.239658832550049
Epoch 1460, val loss: 1.0951985120773315
Epoch 1470, training loss: 311.9017639160156 = 0.09282584488391876 + 50.0 * 6.236178874969482
Epoch 1470, val loss: 1.100422739982605
Epoch 1480, training loss: 311.79351806640625 = 0.09056852757930756 + 50.0 * 6.234058856964111
Epoch 1480, val loss: 1.1050132513046265
Epoch 1490, training loss: 311.7607421875 = 0.08838686347007751 + 50.0 * 6.233447551727295
Epoch 1490, val loss: 1.1101033687591553
Epoch 1500, training loss: 311.7431335449219 = 0.08629493415355682 + 50.0 * 6.2331366539001465
Epoch 1500, val loss: 1.115416407585144
Epoch 1510, training loss: 311.82305908203125 = 0.0842491164803505 + 50.0 * 6.234776020050049
Epoch 1510, val loss: 1.1203275918960571
Epoch 1520, training loss: 311.81402587890625 = 0.0822414755821228 + 50.0 * 6.234635829925537
Epoch 1520, val loss: 1.1250336170196533
Epoch 1530, training loss: 311.9443664550781 = 0.0802897959947586 + 50.0 * 6.237281322479248
Epoch 1530, val loss: 1.1295078992843628
Epoch 1540, training loss: 311.9009704589844 = 0.07839695364236832 + 50.0 * 6.236451148986816
Epoch 1540, val loss: 1.13497793674469
Epoch 1550, training loss: 311.70867919921875 = 0.07653792202472687 + 50.0 * 6.232642650604248
Epoch 1550, val loss: 1.1397157907485962
Epoch 1560, training loss: 311.66180419921875 = 0.07475713640451431 + 50.0 * 6.231740951538086
Epoch 1560, val loss: 1.1452516317367554
Epoch 1570, training loss: 311.6250915527344 = 0.07303446531295776 + 50.0 * 6.231041431427002
Epoch 1570, val loss: 1.149958610534668
Epoch 1580, training loss: 311.7597351074219 = 0.07138291746377945 + 50.0 * 6.233767032623291
Epoch 1580, val loss: 1.1552119255065918
Epoch 1590, training loss: 311.67840576171875 = 0.06974194198846817 + 50.0 * 6.232173442840576
Epoch 1590, val loss: 1.159619927406311
Epoch 1600, training loss: 311.5956726074219 = 0.06813070178031921 + 50.0 * 6.230550765991211
Epoch 1600, val loss: 1.1644657850265503
Epoch 1610, training loss: 311.5833435058594 = 0.06660526990890503 + 50.0 * 6.230334281921387
Epoch 1610, val loss: 1.169764518737793
Epoch 1620, training loss: 311.6903076171875 = 0.06513023376464844 + 50.0 * 6.232503414154053
Epoch 1620, val loss: 1.1747654676437378
Epoch 1630, training loss: 311.84893798828125 = 0.06368172913789749 + 50.0 * 6.2357048988342285
Epoch 1630, val loss: 1.1803051233291626
Epoch 1640, training loss: 311.6037292480469 = 0.06220749393105507 + 50.0 * 6.230830669403076
Epoch 1640, val loss: 1.1847617626190186
Epoch 1650, training loss: 311.51025390625 = 0.06083497405052185 + 50.0 * 6.2289886474609375
Epoch 1650, val loss: 1.189477562904358
Epoch 1660, training loss: 311.49676513671875 = 0.05951327458024025 + 50.0 * 6.228744983673096
Epoch 1660, val loss: 1.1943728923797607
Epoch 1670, training loss: 311.5209045410156 = 0.05823322758078575 + 50.0 * 6.22925329208374
Epoch 1670, val loss: 1.199151635169983
Epoch 1680, training loss: 311.6924133300781 = 0.05699155479669571 + 50.0 * 6.232708930969238
Epoch 1680, val loss: 1.20374596118927
Epoch 1690, training loss: 311.6845397949219 = 0.05576465278863907 + 50.0 * 6.232575416564941
Epoch 1690, val loss: 1.2096171379089355
Epoch 1700, training loss: 311.5242004394531 = 0.054543785750865936 + 50.0 * 6.229393482208252
Epoch 1700, val loss: 1.213721752166748
Epoch 1710, training loss: 311.5145263671875 = 0.053390614688396454 + 50.0 * 6.229222297668457
Epoch 1710, val loss: 1.2192599773406982
Epoch 1720, training loss: 311.5183410644531 = 0.05226878449320793 + 50.0 * 6.229321002960205
Epoch 1720, val loss: 1.2234302759170532
Epoch 1730, training loss: 311.5653991699219 = 0.051187217235565186 + 50.0 * 6.230284214019775
Epoch 1730, val loss: 1.2280946969985962
Epoch 1740, training loss: 311.45947265625 = 0.0501263290643692 + 50.0 * 6.22818660736084
Epoch 1740, val loss: 1.233479380607605
Epoch 1750, training loss: 311.5440368652344 = 0.04909953474998474 + 50.0 * 6.229898929595947
Epoch 1750, val loss: 1.2380003929138184
Epoch 1760, training loss: 311.4224853515625 = 0.04807019978761673 + 50.0 * 6.2274885177612305
Epoch 1760, val loss: 1.243016004562378
Epoch 1770, training loss: 311.39849853515625 = 0.04709440842270851 + 50.0 * 6.2270283699035645
Epoch 1770, val loss: 1.2476381063461304
Epoch 1780, training loss: 311.45452880859375 = 0.04615369439125061 + 50.0 * 6.22816801071167
Epoch 1780, val loss: 1.2522114515304565
Epoch 1790, training loss: 311.3385925292969 = 0.04522743076086044 + 50.0 * 6.22586727142334
Epoch 1790, val loss: 1.2574312686920166
Epoch 1800, training loss: 311.30072021484375 = 0.04432803392410278 + 50.0 * 6.225127696990967
Epoch 1800, val loss: 1.2616872787475586
Epoch 1810, training loss: 311.5010070800781 = 0.04348435997962952 + 50.0 * 6.229150295257568
Epoch 1810, val loss: 1.2661315202713013
Epoch 1820, training loss: 311.4165344238281 = 0.04261753708124161 + 50.0 * 6.227478504180908
Epoch 1820, val loss: 1.270922064781189
Epoch 1830, training loss: 311.3603515625 = 0.041760921478271484 + 50.0 * 6.226371765136719
Epoch 1830, val loss: 1.2750940322875977
Epoch 1840, training loss: 311.4336853027344 = 0.04095000773668289 + 50.0 * 6.2278547286987305
Epoch 1840, val loss: 1.2800004482269287
Epoch 1850, training loss: 311.27978515625 = 0.040167611092329025 + 50.0 * 6.22479248046875
Epoch 1850, val loss: 1.2849290370941162
Epoch 1860, training loss: 311.2398376464844 = 0.03940873593091965 + 50.0 * 6.224008560180664
Epoch 1860, val loss: 1.2897709608078003
Epoch 1870, training loss: 311.2778015136719 = 0.038675788789987564 + 50.0 * 6.224782466888428
Epoch 1870, val loss: 1.2943400144577026
Epoch 1880, training loss: 311.3907775878906 = 0.03795783221721649 + 50.0 * 6.227056503295898
Epoch 1880, val loss: 1.2990049123764038
Epoch 1890, training loss: 311.5887756347656 = 0.03727014362812042 + 50.0 * 6.231030464172363
Epoch 1890, val loss: 1.303049921989441
Epoch 1900, training loss: 311.32989501953125 = 0.03653477132320404 + 50.0 * 6.22586727142334
Epoch 1900, val loss: 1.3072774410247803
Epoch 1910, training loss: 311.2563171386719 = 0.0358809232711792 + 50.0 * 6.2244086265563965
Epoch 1910, val loss: 1.3119333982467651
Epoch 1920, training loss: 311.2691345214844 = 0.035216424614191055 + 50.0 * 6.2246785163879395
Epoch 1920, val loss: 1.3167164325714111
Epoch 1930, training loss: 311.2649230957031 = 0.03458913043141365 + 50.0 * 6.224606990814209
Epoch 1930, val loss: 1.3202232122421265
Epoch 1940, training loss: 311.2256164550781 = 0.03396620973944664 + 50.0 * 6.223833084106445
Epoch 1940, val loss: 1.324769139289856
Epoch 1950, training loss: 311.21136474609375 = 0.033369485288858414 + 50.0 * 6.223560333251953
Epoch 1950, val loss: 1.3294209241867065
Epoch 1960, training loss: 311.1072082519531 = 0.0327836275100708 + 50.0 * 6.2214884757995605
Epoch 1960, val loss: 1.3337866067886353
Epoch 1970, training loss: 311.2366638183594 = 0.03222130611538887 + 50.0 * 6.224088668823242
Epoch 1970, val loss: 1.338452935218811
Epoch 1980, training loss: 311.2370300292969 = 0.031663667410612106 + 50.0 * 6.224107265472412
Epoch 1980, val loss: 1.3429652452468872
Epoch 1990, training loss: 311.12646484375 = 0.03110271506011486 + 50.0 * 6.221907615661621
Epoch 1990, val loss: 1.3469491004943848
Epoch 2000, training loss: 311.2762451171875 = 0.03058191016316414 + 50.0 * 6.224913597106934
Epoch 2000, val loss: 1.3517183065414429
Epoch 2010, training loss: 311.1711730957031 = 0.03005731850862503 + 50.0 * 6.222822189331055
Epoch 2010, val loss: 1.3547521829605103
Epoch 2020, training loss: 311.1395568847656 = 0.02955050766468048 + 50.0 * 6.222200393676758
Epoch 2020, val loss: 1.3589972257614136
Epoch 2030, training loss: 311.05731201171875 = 0.029047932475805283 + 50.0 * 6.220565319061279
Epoch 2030, val loss: 1.3637722730636597
Epoch 2040, training loss: 311.0545654296875 = 0.028570866212248802 + 50.0 * 6.22052001953125
Epoch 2040, val loss: 1.3675055503845215
Epoch 2050, training loss: 311.2466125488281 = 0.028112296015024185 + 50.0 * 6.224370002746582
Epoch 2050, val loss: 1.371484637260437
Epoch 2060, training loss: 311.1197204589844 = 0.027647247537970543 + 50.0 * 6.221841335296631
Epoch 2060, val loss: 1.375832200050354
Epoch 2070, training loss: 311.0888977050781 = 0.027188405394554138 + 50.0 * 6.22123384475708
Epoch 2070, val loss: 1.3803167343139648
Epoch 2080, training loss: 311.1786804199219 = 0.02675270102918148 + 50.0 * 6.223038196563721
Epoch 2080, val loss: 1.3842341899871826
Epoch 2090, training loss: 310.9775390625 = 0.026323474943637848 + 50.0 * 6.219024181365967
Epoch 2090, val loss: 1.3879523277282715
Epoch 2100, training loss: 310.96173095703125 = 0.02590988203883171 + 50.0 * 6.218716621398926
Epoch 2100, val loss: 1.392072081565857
Epoch 2110, training loss: 310.9693603515625 = 0.025510422885417938 + 50.0 * 6.218876838684082
Epoch 2110, val loss: 1.3962303400039673
Epoch 2120, training loss: 311.1269836425781 = 0.025126079097390175 + 50.0 * 6.222037315368652
Epoch 2120, val loss: 1.4000416994094849
Epoch 2130, training loss: 311.1676330566406 = 0.024738909676671028 + 50.0 * 6.222857475280762
Epoch 2130, val loss: 1.4038984775543213
Epoch 2140, training loss: 311.0960388183594 = 0.024335702881217003 + 50.0 * 6.221434116363525
Epoch 2140, val loss: 1.4084018468856812
Epoch 2150, training loss: 310.98309326171875 = 0.023951923474669456 + 50.0 * 6.21918249130249
Epoch 2150, val loss: 1.4111545085906982
Epoch 2160, training loss: 310.974609375 = 0.023597221821546555 + 50.0 * 6.219020366668701
Epoch 2160, val loss: 1.4152491092681885
Epoch 2170, training loss: 311.16888427734375 = 0.023255791515111923 + 50.0 * 6.222912788391113
Epoch 2170, val loss: 1.4188028573989868
Epoch 2180, training loss: 310.91796875 = 0.022894876077771187 + 50.0 * 6.217901706695557
Epoch 2180, val loss: 1.4235732555389404
Epoch 2190, training loss: 310.8588562011719 = 0.022561771795153618 + 50.0 * 6.216726303100586
Epoch 2190, val loss: 1.4269142150878906
Epoch 2200, training loss: 310.90838623046875 = 0.022236457094550133 + 50.0 * 6.2177228927612305
Epoch 2200, val loss: 1.4306048154830933
Epoch 2210, training loss: 311.3050231933594 = 0.021914871409535408 + 50.0 * 6.2256622314453125
Epoch 2210, val loss: 1.4349037408828735
Epoch 2220, training loss: 310.99603271484375 = 0.021592842414975166 + 50.0 * 6.219488620758057
Epoch 2220, val loss: 1.4377821683883667
Epoch 2230, training loss: 310.9297790527344 = 0.021268298849463463 + 50.0 * 6.218170166015625
Epoch 2230, val loss: 1.4415968656539917
Epoch 2240, training loss: 310.90118408203125 = 0.02096710540354252 + 50.0 * 6.217604637145996
Epoch 2240, val loss: 1.4452241659164429
Epoch 2250, training loss: 310.9091796875 = 0.020675573498010635 + 50.0 * 6.217770576477051
Epoch 2250, val loss: 1.4489727020263672
Epoch 2260, training loss: 310.8283386230469 = 0.020386341959238052 + 50.0 * 6.216159343719482
Epoch 2260, val loss: 1.4527873992919922
Epoch 2270, training loss: 310.93499755859375 = 0.020117513835430145 + 50.0 * 6.218297481536865
Epoch 2270, val loss: 1.4566901922225952
Epoch 2280, training loss: 310.8667907714844 = 0.019836915656924248 + 50.0 * 6.2169389724731445
Epoch 2280, val loss: 1.460138201713562
Epoch 2290, training loss: 311.00457763671875 = 0.019564446061849594 + 50.0 * 6.219700336456299
Epoch 2290, val loss: 1.46376371383667
Epoch 2300, training loss: 310.8673095703125 = 0.019288845360279083 + 50.0 * 6.216960430145264
Epoch 2300, val loss: 1.467011570930481
Epoch 2310, training loss: 311.002197265625 = 0.01903269812464714 + 50.0 * 6.219663143157959
Epoch 2310, val loss: 1.4711025953292847
Epoch 2320, training loss: 310.7839050292969 = 0.0187697634100914 + 50.0 * 6.21530294418335
Epoch 2320, val loss: 1.4735379219055176
Epoch 2330, training loss: 310.7555847167969 = 0.018522249534726143 + 50.0 * 6.214741230010986
Epoch 2330, val loss: 1.4775747060775757
Epoch 2340, training loss: 310.8277587890625 = 0.018285829573869705 + 50.0 * 6.216189384460449
Epoch 2340, val loss: 1.4812803268432617
Epoch 2350, training loss: 310.8837585449219 = 0.018047580495476723 + 50.0 * 6.217314720153809
Epoch 2350, val loss: 1.4845869541168213
Epoch 2360, training loss: 310.862060546875 = 0.017808910459280014 + 50.0 * 6.216884613037109
Epoch 2360, val loss: 1.4873559474945068
Epoch 2370, training loss: 311.01983642578125 = 0.017592228949069977 + 50.0 * 6.22004508972168
Epoch 2370, val loss: 1.4909776449203491
Epoch 2380, training loss: 310.868896484375 = 0.017345866188406944 + 50.0 * 6.217031002044678
Epoch 2380, val loss: 1.4929735660552979
Epoch 2390, training loss: 310.71893310546875 = 0.017117125913500786 + 50.0 * 6.214035987854004
Epoch 2390, val loss: 1.497576355934143
Epoch 2400, training loss: 310.6963806152344 = 0.01690390519797802 + 50.0 * 6.213589191436768
Epoch 2400, val loss: 1.5010783672332764
Epoch 2410, training loss: 310.751220703125 = 0.016697900369763374 + 50.0 * 6.214690685272217
Epoch 2410, val loss: 1.504349708557129
Epoch 2420, training loss: 310.9294128417969 = 0.016491971909999847 + 50.0 * 6.218258857727051
Epoch 2420, val loss: 1.5068320035934448
Epoch 2430, training loss: 310.8069152832031 = 0.016289612278342247 + 50.0 * 6.2158122062683105
Epoch 2430, val loss: 1.509572148323059
Epoch 2440, training loss: 310.69293212890625 = 0.01607925444841385 + 50.0 * 6.213536739349365
Epoch 2440, val loss: 1.5140658617019653
Epoch 2450, training loss: 310.6629638671875 = 0.015886038541793823 + 50.0 * 6.2129411697387695
Epoch 2450, val loss: 1.5168124437332153
Epoch 2460, training loss: 311.09490966796875 = 0.015696637332439423 + 50.0 * 6.221584320068359
Epoch 2460, val loss: 1.5201667547225952
Epoch 2470, training loss: 310.7868957519531 = 0.015504193492233753 + 50.0 * 6.215427875518799
Epoch 2470, val loss: 1.5229536294937134
Epoch 2480, training loss: 310.68170166015625 = 0.015314712189137936 + 50.0 * 6.213327407836914
Epoch 2480, val loss: 1.5259361267089844
Epoch 2490, training loss: 310.7455749511719 = 0.0151416826993227 + 50.0 * 6.214608669281006
Epoch 2490, val loss: 1.5300132036209106
Epoch 2500, training loss: 310.999267578125 = 0.01496952399611473 + 50.0 * 6.2196855545043945
Epoch 2500, val loss: 1.5321593284606934
Epoch 2510, training loss: 310.6727600097656 = 0.014773537404835224 + 50.0 * 6.213160037994385
Epoch 2510, val loss: 1.5348844528198242
Epoch 2520, training loss: 310.6045837402344 = 0.01460125483572483 + 50.0 * 6.2118000984191895
Epoch 2520, val loss: 1.5381994247436523
Epoch 2530, training loss: 310.5755920410156 = 0.01443939283490181 + 50.0 * 6.211223125457764
Epoch 2530, val loss: 1.541472315788269
Epoch 2540, training loss: 310.6284484863281 = 0.01428116112947464 + 50.0 * 6.212283134460449
Epoch 2540, val loss: 1.5447889566421509
Epoch 2550, training loss: 310.92041015625 = 0.014122502878308296 + 50.0 * 6.218125820159912
Epoch 2550, val loss: 1.5476469993591309
Epoch 2560, training loss: 310.73974609375 = 0.013952909037470818 + 50.0 * 6.2145161628723145
Epoch 2560, val loss: 1.5496467351913452
Epoch 2570, training loss: 310.6367492675781 = 0.013792889192700386 + 50.0 * 6.212459087371826
Epoch 2570, val loss: 1.553079605102539
Epoch 2580, training loss: 310.6476745605469 = 0.013644973747432232 + 50.0 * 6.212680816650391
Epoch 2580, val loss: 1.5563673973083496
Epoch 2590, training loss: 310.7103271484375 = 0.013497442938387394 + 50.0 * 6.213936805725098
Epoch 2590, val loss: 1.5598602294921875
Epoch 2600, training loss: 310.7037048339844 = 0.013351949863135815 + 50.0 * 6.213806629180908
Epoch 2600, val loss: 1.5624141693115234
Epoch 2610, training loss: 310.5856628417969 = 0.013195068575441837 + 50.0 * 6.21144962310791
Epoch 2610, val loss: 1.5647379159927368
Epoch 2620, training loss: 310.6991271972656 = 0.013051828369498253 + 50.0 * 6.21372127532959
Epoch 2620, val loss: 1.5680639743804932
Epoch 2630, training loss: 310.5848388671875 = 0.012913761660456657 + 50.0 * 6.2114386558532715
Epoch 2630, val loss: 1.5713239908218384
Epoch 2640, training loss: 310.5514221191406 = 0.012774346396327019 + 50.0 * 6.21077299118042
Epoch 2640, val loss: 1.5737224817276
Epoch 2650, training loss: 310.51031494140625 = 0.01264257449656725 + 50.0 * 6.209953784942627
Epoch 2650, val loss: 1.5764987468719482
Epoch 2660, training loss: 310.7746276855469 = 0.012520238757133484 + 50.0 * 6.215242385864258
Epoch 2660, val loss: 1.5790473222732544
Epoch 2670, training loss: 310.541748046875 = 0.012380704283714294 + 50.0 * 6.210587024688721
Epoch 2670, val loss: 1.5823363065719604
Epoch 2680, training loss: 310.60968017578125 = 0.012256050482392311 + 50.0 * 6.211948394775391
Epoch 2680, val loss: 1.5853204727172852
Epoch 2690, training loss: 310.55828857421875 = 0.012126754969358444 + 50.0 * 6.210923671722412
Epoch 2690, val loss: 1.5876796245574951
Epoch 2700, training loss: 310.6164245605469 = 0.012003130279481411 + 50.0 * 6.212088584899902
Epoch 2700, val loss: 1.5898393392562866
Epoch 2710, training loss: 310.53155517578125 = 0.011874957010149956 + 50.0 * 6.21039342880249
Epoch 2710, val loss: 1.5928136110305786
Epoch 2720, training loss: 310.4834289550781 = 0.011753208003938198 + 50.0 * 6.209433555603027
Epoch 2720, val loss: 1.5960668325424194
Epoch 2730, training loss: 310.49658203125 = 0.011638622730970383 + 50.0 * 6.2096991539001465
Epoch 2730, val loss: 1.598976731300354
Epoch 2740, training loss: 310.67303466796875 = 0.011523700319230556 + 50.0 * 6.213230133056641
Epoch 2740, val loss: 1.6017265319824219
Epoch 2750, training loss: 310.5077209472656 = 0.01140674576163292 + 50.0 * 6.209926128387451
Epoch 2750, val loss: 1.6035963296890259
Epoch 2760, training loss: 310.53875732421875 = 0.011300508864223957 + 50.0 * 6.210549354553223
Epoch 2760, val loss: 1.6065738201141357
Epoch 2770, training loss: 310.5907287597656 = 0.0111897187307477 + 50.0 * 6.21159029006958
Epoch 2770, val loss: 1.6091188192367554
Epoch 2780, training loss: 310.461669921875 = 0.011070286855101585 + 50.0 * 6.209011554718018
Epoch 2780, val loss: 1.6112576723098755
Epoch 2790, training loss: 310.5007629394531 = 0.01096840389072895 + 50.0 * 6.20979642868042
Epoch 2790, val loss: 1.6134144067764282
Epoch 2800, training loss: 310.5498046875 = 0.010869886726140976 + 50.0 * 6.210778713226318
Epoch 2800, val loss: 1.6163415908813477
Epoch 2810, training loss: 310.5994567871094 = 0.010759926401078701 + 50.0 * 6.21177339553833
Epoch 2810, val loss: 1.6193969249725342
Epoch 2820, training loss: 310.5000305175781 = 0.01065319124609232 + 50.0 * 6.209787845611572
Epoch 2820, val loss: 1.6214252710342407
Epoch 2830, training loss: 310.530029296875 = 0.010550118051469326 + 50.0 * 6.210389137268066
Epoch 2830, val loss: 1.6245461702346802
Epoch 2840, training loss: 310.4982604980469 = 0.010452332906425 + 50.0 * 6.209755897521973
Epoch 2840, val loss: 1.6267240047454834
Epoch 2850, training loss: 310.4454650878906 = 0.010356871411204338 + 50.0 * 6.208702087402344
Epoch 2850, val loss: 1.6293240785598755
Epoch 2860, training loss: 310.4141845703125 = 0.010262368246912956 + 50.0 * 6.208078384399414
Epoch 2860, val loss: 1.6320182085037231
Epoch 2870, training loss: 310.447998046875 = 0.010172476060688496 + 50.0 * 6.208756446838379
Epoch 2870, val loss: 1.6348059177398682
Epoch 2880, training loss: 310.4854736328125 = 0.010077465325593948 + 50.0 * 6.209507942199707
Epoch 2880, val loss: 1.636911392211914
Epoch 2890, training loss: 310.39117431640625 = 0.009983398951590061 + 50.0 * 6.2076239585876465
Epoch 2890, val loss: 1.639085292816162
Epoch 2900, training loss: 310.5097351074219 = 0.009899569675326347 + 50.0 * 6.209996700286865
Epoch 2900, val loss: 1.6417922973632812
Epoch 2910, training loss: 310.49053955078125 = 0.009809747338294983 + 50.0 * 6.2096147537231445
Epoch 2910, val loss: 1.644458532333374
Epoch 2920, training loss: 310.3668212890625 = 0.009717493318021297 + 50.0 * 6.207142353057861
Epoch 2920, val loss: 1.6460636854171753
Epoch 2930, training loss: 310.5723876953125 = 0.00963757373392582 + 50.0 * 6.211254596710205
Epoch 2930, val loss: 1.647741436958313
Epoch 2940, training loss: 310.35302734375 = 0.00954678189009428 + 50.0 * 6.206869602203369
Epoch 2940, val loss: 1.650901198387146
Epoch 2950, training loss: 310.31341552734375 = 0.009458525106310844 + 50.0 * 6.206079006195068
Epoch 2950, val loss: 1.6530170440673828
Epoch 2960, training loss: 310.2921447753906 = 0.009378773160278797 + 50.0 * 6.205655574798584
Epoch 2960, val loss: 1.6554653644561768
Epoch 2970, training loss: 310.27386474609375 = 0.009301898069679737 + 50.0 * 6.205291271209717
Epoch 2970, val loss: 1.6581215858459473
Epoch 2980, training loss: 310.5527038574219 = 0.009227162227034569 + 50.0 * 6.210869789123535
Epoch 2980, val loss: 1.6603989601135254
Epoch 2990, training loss: 310.3675537109375 = 0.009145976975560188 + 50.0 * 6.207168102264404
Epoch 2990, val loss: 1.6617701053619385
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7222222222222222
0.8044280442804429
=== training gcn model ===
Epoch 0, training loss: 431.7901306152344 = 1.9480316638946533 + 50.0 * 8.596841812133789
Epoch 0, val loss: 1.934804081916809
Epoch 10, training loss: 431.74456787109375 = 1.938820719718933 + 50.0 * 8.596115112304688
Epoch 10, val loss: 1.9258677959442139
Epoch 20, training loss: 431.5043029785156 = 1.9276782274246216 + 50.0 * 8.591532707214355
Epoch 20, val loss: 1.9147619009017944
Epoch 30, training loss: 429.9828186035156 = 1.913834571838379 + 50.0 * 8.561379432678223
Epoch 30, val loss: 1.9007381200790405
Epoch 40, training loss: 420.19403076171875 = 1.897301197052002 + 50.0 * 8.365934371948242
Epoch 40, val loss: 1.8844881057739258
Epoch 50, training loss: 384.7209777832031 = 1.8788466453552246 + 50.0 * 7.6568427085876465
Epoch 50, val loss: 1.8661577701568604
Epoch 60, training loss: 372.43505859375 = 1.8613287210464478 + 50.0 * 7.411474704742432
Epoch 60, val loss: 1.8499149084091187
Epoch 70, training loss: 358.55279541015625 = 1.8485603332519531 + 50.0 * 7.134084701538086
Epoch 70, val loss: 1.8381096124649048
Epoch 80, training loss: 351.21160888671875 = 1.8363850116729736 + 50.0 * 6.987504482269287
Epoch 80, val loss: 1.8273003101348877
Epoch 90, training loss: 347.2568664550781 = 1.825001835823059 + 50.0 * 6.908637046813965
Epoch 90, val loss: 1.8173187971115112
Epoch 100, training loss: 344.1999816894531 = 1.8149341344833374 + 50.0 * 6.847700595855713
Epoch 100, val loss: 1.8086696863174438
Epoch 110, training loss: 341.56964111328125 = 1.8060163259506226 + 50.0 * 6.795272350311279
Epoch 110, val loss: 1.8012460470199585
Epoch 120, training loss: 338.7810974121094 = 1.7981003522872925 + 50.0 * 6.739660263061523
Epoch 120, val loss: 1.7946422100067139
Epoch 130, training loss: 336.23553466796875 = 1.7905809879302979 + 50.0 * 6.688899040222168
Epoch 130, val loss: 1.7885568141937256
Epoch 140, training loss: 334.16143798828125 = 1.7829787731170654 + 50.0 * 6.647569179534912
Epoch 140, val loss: 1.7822130918502808
Epoch 150, training loss: 332.4295654296875 = 1.775059700012207 + 50.0 * 6.6130900382995605
Epoch 150, val loss: 1.7756648063659668
Epoch 160, training loss: 330.8511962890625 = 1.766723394393921 + 50.0 * 6.581689357757568
Epoch 160, val loss: 1.7688413858413696
Epoch 170, training loss: 329.5776672363281 = 1.7576090097427368 + 50.0 * 6.556401252746582
Epoch 170, val loss: 1.7614831924438477
Epoch 180, training loss: 328.6771240234375 = 1.7474825382232666 + 50.0 * 6.538592338562012
Epoch 180, val loss: 1.7534527778625488
Epoch 190, training loss: 327.74554443359375 = 1.7366020679473877 + 50.0 * 6.52017879486084
Epoch 190, val loss: 1.7447161674499512
Epoch 200, training loss: 326.8967590332031 = 1.724954605102539 + 50.0 * 6.503436088562012
Epoch 200, val loss: 1.7353737354278564
Epoch 210, training loss: 326.2217102050781 = 1.7123613357543945 + 50.0 * 6.49018669128418
Epoch 210, val loss: 1.7253025770187378
Epoch 220, training loss: 325.5146789550781 = 1.69846510887146 + 50.0 * 6.476324558258057
Epoch 220, val loss: 1.7142918109893799
Epoch 230, training loss: 325.0709533691406 = 1.68352210521698 + 50.0 * 6.467748641967773
Epoch 230, val loss: 1.702366828918457
Epoch 240, training loss: 324.4483337402344 = 1.6671035289764404 + 50.0 * 6.455624103546143
Epoch 240, val loss: 1.689530849456787
Epoch 250, training loss: 323.9533386230469 = 1.6495623588562012 + 50.0 * 6.446075439453125
Epoch 250, val loss: 1.6756868362426758
Epoch 260, training loss: 323.5210876464844 = 1.630725383758545 + 50.0 * 6.437807083129883
Epoch 260, val loss: 1.6608893871307373
Epoch 270, training loss: 323.1424865722656 = 1.610591173171997 + 50.0 * 6.430637836456299
Epoch 270, val loss: 1.645121455192566
Epoch 280, training loss: 322.849853515625 = 1.5889557600021362 + 50.0 * 6.425217628479004
Epoch 280, val loss: 1.628242015838623
Epoch 290, training loss: 322.43878173828125 = 1.5661389827728271 + 50.0 * 6.417452812194824
Epoch 290, val loss: 1.6105314493179321
Epoch 300, training loss: 322.06011962890625 = 1.542188286781311 + 50.0 * 6.410358905792236
Epoch 300, val loss: 1.5919761657714844
Epoch 310, training loss: 321.9283447265625 = 1.517215371131897 + 50.0 * 6.408222675323486
Epoch 310, val loss: 1.5726687908172607
Epoch 320, training loss: 321.4913024902344 = 1.4910252094268799 + 50.0 * 6.400005340576172
Epoch 320, val loss: 1.552821159362793
Epoch 330, training loss: 321.1426696777344 = 1.4642349481582642 + 50.0 * 6.393568992614746
Epoch 330, val loss: 1.532561182975769
Epoch 340, training loss: 320.916259765625 = 1.436798095703125 + 50.0 * 6.389589309692383
Epoch 340, val loss: 1.511799693107605
Epoch 350, training loss: 320.76812744140625 = 1.4089053869247437 + 50.0 * 6.3871846199035645
Epoch 350, val loss: 1.4912927150726318
Epoch 360, training loss: 320.3212890625 = 1.3807227611541748 + 50.0 * 6.378810882568359
Epoch 360, val loss: 1.4706116914749146
Epoch 370, training loss: 320.05224609375 = 1.352599024772644 + 50.0 * 6.373992919921875
Epoch 370, val loss: 1.4503204822540283
Epoch 380, training loss: 319.8782958984375 = 1.3244967460632324 + 50.0 * 6.3710761070251465
Epoch 380, val loss: 1.4302324056625366
Epoch 390, training loss: 319.6903076171875 = 1.2966643571853638 + 50.0 * 6.367873191833496
Epoch 390, val loss: 1.4105966091156006
Epoch 400, training loss: 319.36419677734375 = 1.2692131996154785 + 50.0 * 6.361899375915527
Epoch 400, val loss: 1.3916188478469849
Epoch 410, training loss: 319.1416931152344 = 1.2423210144042969 + 50.0 * 6.357987403869629
Epoch 410, val loss: 1.3731491565704346
Epoch 420, training loss: 318.9237060546875 = 1.21589994430542 + 50.0 * 6.354156017303467
Epoch 420, val loss: 1.3552632331848145
Epoch 430, training loss: 319.3537902832031 = 1.1900124549865723 + 50.0 * 6.363275527954102
Epoch 430, val loss: 1.33766508102417
Epoch 440, training loss: 318.6908874511719 = 1.164407730102539 + 50.0 * 6.350529670715332
Epoch 440, val loss: 1.3205809593200684
Epoch 450, training loss: 318.4236755371094 = 1.139405369758606 + 50.0 * 6.3456854820251465
Epoch 450, val loss: 1.304132103919983
Epoch 460, training loss: 318.2152099609375 = 1.1150389909744263 + 50.0 * 6.342002868652344
Epoch 460, val loss: 1.288172721862793
Epoch 470, training loss: 318.0288391113281 = 1.0910394191741943 + 50.0 * 6.338756084442139
Epoch 470, val loss: 1.2724858522415161
Epoch 480, training loss: 317.9363098144531 = 1.0674340724945068 + 50.0 * 6.337377071380615
Epoch 480, val loss: 1.257150650024414
Epoch 490, training loss: 317.80438232421875 = 1.044271469116211 + 50.0 * 6.335201740264893
Epoch 490, val loss: 1.2420026063919067
Epoch 500, training loss: 317.5810852050781 = 1.0214216709136963 + 50.0 * 6.331192970275879
Epoch 500, val loss: 1.2275629043579102
Epoch 510, training loss: 317.48675537109375 = 0.9989767074584961 + 50.0 * 6.329755783081055
Epoch 510, val loss: 1.2128465175628662
Epoch 520, training loss: 317.24884033203125 = 0.9766912460327148 + 50.0 * 6.325443267822266
Epoch 520, val loss: 1.19868004322052
Epoch 530, training loss: 317.19305419921875 = 0.9547370672225952 + 50.0 * 6.324766159057617
Epoch 530, val loss: 1.1846290826797485
Epoch 540, training loss: 317.0743103027344 = 0.9330843091011047 + 50.0 * 6.322824954986572
Epoch 540, val loss: 1.1706303358078003
Epoch 550, training loss: 316.96026611328125 = 0.9115673303604126 + 50.0 * 6.320974349975586
Epoch 550, val loss: 1.1569194793701172
Epoch 560, training loss: 316.7645263671875 = 0.8903248906135559 + 50.0 * 6.317484378814697
Epoch 560, val loss: 1.143523097038269
Epoch 570, training loss: 316.77801513671875 = 0.8693439960479736 + 50.0 * 6.318172931671143
Epoch 570, val loss: 1.1304467916488647
Epoch 580, training loss: 316.6404724121094 = 0.8486406803131104 + 50.0 * 6.3158369064331055
Epoch 580, val loss: 1.1176631450653076
Epoch 590, training loss: 316.4404602050781 = 0.828080415725708 + 50.0 * 6.312247276306152
Epoch 590, val loss: 1.1046864986419678
Epoch 600, training loss: 316.31304931640625 = 0.8078687191009521 + 50.0 * 6.310103893280029
Epoch 600, val loss: 1.0925358533859253
Epoch 610, training loss: 316.3161315917969 = 0.7879868745803833 + 50.0 * 6.310562610626221
Epoch 610, val loss: 1.0804259777069092
Epoch 620, training loss: 316.2726135253906 = 0.7680578231811523 + 50.0 * 6.310091018676758
Epoch 620, val loss: 1.0688508749008179
Epoch 630, training loss: 316.0511779785156 = 0.7485982775688171 + 50.0 * 6.306051731109619
Epoch 630, val loss: 1.0573053359985352
Epoch 640, training loss: 316.0040283203125 = 0.7294610738754272 + 50.0 * 6.3054914474487305
Epoch 640, val loss: 1.0463173389434814
Epoch 650, training loss: 315.9466247558594 = 0.7106626033782959 + 50.0 * 6.304718971252441
Epoch 650, val loss: 1.035852313041687
Epoch 660, training loss: 315.79400634765625 = 0.6919985413551331 + 50.0 * 6.3020405769348145
Epoch 660, val loss: 1.025742769241333
Epoch 670, training loss: 315.74444580078125 = 0.6738055348396301 + 50.0 * 6.301413059234619
Epoch 670, val loss: 1.0160205364227295
Epoch 680, training loss: 315.60247802734375 = 0.6559426188468933 + 50.0 * 6.298930644989014
Epoch 680, val loss: 1.0065488815307617
Epoch 690, training loss: 315.514892578125 = 0.638388991355896 + 50.0 * 6.297529697418213
Epoch 690, val loss: 0.9978672862052917
Epoch 700, training loss: 315.478759765625 = 0.6213008165359497 + 50.0 * 6.297149181365967
Epoch 700, val loss: 0.9895287156105042
Epoch 710, training loss: 315.3282470703125 = 0.6044411063194275 + 50.0 * 6.29447603225708
Epoch 710, val loss: 0.9817882180213928
Epoch 720, training loss: 315.40240478515625 = 0.5880144238471985 + 50.0 * 6.296288013458252
Epoch 720, val loss: 0.974479615688324
Epoch 730, training loss: 315.23480224609375 = 0.5720970034599304 + 50.0 * 6.293254375457764
Epoch 730, val loss: 0.9676303267478943
Epoch 740, training loss: 315.1357421875 = 0.5565385818481445 + 50.0 * 6.291584014892578
Epoch 740, val loss: 0.9613131880760193
Epoch 750, training loss: 315.0242004394531 = 0.5414348840713501 + 50.0 * 6.2896552085876465
Epoch 750, val loss: 0.9557971954345703
Epoch 760, training loss: 315.1291198730469 = 0.526810884475708 + 50.0 * 6.292045593261719
Epoch 760, val loss: 0.9508649110794067
Epoch 770, training loss: 315.115966796875 = 0.5124879479408264 + 50.0 * 6.292069911956787
Epoch 770, val loss: 0.9457271695137024
Epoch 780, training loss: 314.84771728515625 = 0.49850741028785706 + 50.0 * 6.286983966827393
Epoch 780, val loss: 0.9417673945426941
Epoch 790, training loss: 314.75006103515625 = 0.48509055376052856 + 50.0 * 6.285299777984619
Epoch 790, val loss: 0.938377857208252
Epoch 800, training loss: 315.2243347167969 = 0.47221702337265015 + 50.0 * 6.295042037963867
Epoch 800, val loss: 0.9354975819587708
Epoch 810, training loss: 314.8259582519531 = 0.45932894945144653 + 50.0 * 6.287333011627197
Epoch 810, val loss: 0.9324873089790344
Epoch 820, training loss: 314.6111145019531 = 0.4470917582511902 + 50.0 * 6.283280372619629
Epoch 820, val loss: 0.9304039478302002
Epoch 830, training loss: 314.49725341796875 = 0.43536221981048584 + 50.0 * 6.281237602233887
Epoch 830, val loss: 0.9288707971572876
Epoch 840, training loss: 314.43634033203125 = 0.42404064536094666 + 50.0 * 6.280245780944824
Epoch 840, val loss: 0.9278684258460999
Epoch 850, training loss: 314.8761901855469 = 0.41312292218208313 + 50.0 * 6.289261341094971
Epoch 850, val loss: 0.9269806146621704
Epoch 860, training loss: 314.4547424316406 = 0.40221497416496277 + 50.0 * 6.281050682067871
Epoch 860, val loss: 0.9266625046730042
Epoch 870, training loss: 314.5251770019531 = 0.3918722867965698 + 50.0 * 6.282666206359863
Epoch 870, val loss: 0.9270188212394714
Epoch 880, training loss: 314.2958984375 = 0.38178834319114685 + 50.0 * 6.278282642364502
Epoch 880, val loss: 0.9267340898513794
Epoch 890, training loss: 314.1853942871094 = 0.37212100625038147 + 50.0 * 6.2762651443481445
Epoch 890, val loss: 0.9276233911514282
Epoch 900, training loss: 314.10284423828125 = 0.3627682328224182 + 50.0 * 6.274801731109619
Epoch 900, val loss: 0.928629994392395
Epoch 910, training loss: 314.06182861328125 = 0.3537501096725464 + 50.0 * 6.274161338806152
Epoch 910, val loss: 0.9300104975700378
Epoch 920, training loss: 314.4663391113281 = 0.3450315296649933 + 50.0 * 6.282426357269287
Epoch 920, val loss: 0.9317730665206909
Epoch 930, training loss: 314.08123779296875 = 0.33625346422195435 + 50.0 * 6.274899959564209
Epoch 930, val loss: 0.9331340789794922
Epoch 940, training loss: 313.95489501953125 = 0.32792341709136963 + 50.0 * 6.2725396156311035
Epoch 940, val loss: 0.9349595904350281
Epoch 950, training loss: 313.9032897949219 = 0.3199077844619751 + 50.0 * 6.27166748046875
Epoch 950, val loss: 0.9374463558197021
Epoch 960, training loss: 314.0067138671875 = 0.3121170401573181 + 50.0 * 6.273891925811768
Epoch 960, val loss: 0.9398443698883057
Epoch 970, training loss: 313.80419921875 = 0.3044178783893585 + 50.0 * 6.26999568939209
Epoch 970, val loss: 0.9429547190666199
Epoch 980, training loss: 313.9158935546875 = 0.2970004379749298 + 50.0 * 6.272377967834473
Epoch 980, val loss: 0.9455600380897522
Epoch 990, training loss: 313.7066345214844 = 0.28979307413101196 + 50.0 * 6.268336772918701
Epoch 990, val loss: 0.9489827752113342
Epoch 1000, training loss: 313.6210632324219 = 0.2827441096305847 + 50.0 * 6.26676607131958
Epoch 1000, val loss: 0.9524095058441162
Epoch 1010, training loss: 313.70697021484375 = 0.27595746517181396 + 50.0 * 6.268620014190674
Epoch 1010, val loss: 0.9559606313705444
Epoch 1020, training loss: 313.6394958496094 = 0.26931673288345337 + 50.0 * 6.267403602600098
Epoch 1020, val loss: 0.9595025181770325
Epoch 1030, training loss: 313.6247863769531 = 0.26275834441185 + 50.0 * 6.267240524291992
Epoch 1030, val loss: 0.9634000658988953
Epoch 1040, training loss: 313.5196228027344 = 0.2564454972743988 + 50.0 * 6.265263557434082
Epoch 1040, val loss: 0.9675719141960144
Epoch 1050, training loss: 313.5562744140625 = 0.2503349184989929 + 50.0 * 6.266119003295898
Epoch 1050, val loss: 0.9717057943344116
Epoch 1060, training loss: 313.4133605957031 = 0.24433253705501556 + 50.0 * 6.263380527496338
Epoch 1060, val loss: 0.9760597348213196
Epoch 1070, training loss: 313.35516357421875 = 0.23851974308490753 + 50.0 * 6.262332916259766
Epoch 1070, val loss: 0.9803497195243835
Epoch 1080, training loss: 313.50567626953125 = 0.23289436101913452 + 50.0 * 6.265456199645996
Epoch 1080, val loss: 0.9846792817115784
Epoch 1090, training loss: 313.44561767578125 = 0.2272794395685196 + 50.0 * 6.26436710357666
Epoch 1090, val loss: 0.9898971915245056
Epoch 1100, training loss: 313.28338623046875 = 0.22182805836200714 + 50.0 * 6.261230945587158
Epoch 1100, val loss: 0.9939691424369812
Epoch 1110, training loss: 313.1929016113281 = 0.21657903492450714 + 50.0 * 6.259526252746582
Epoch 1110, val loss: 0.9993782043457031
Epoch 1120, training loss: 313.1235656738281 = 0.21152594685554504 + 50.0 * 6.258240699768066
Epoch 1120, val loss: 1.0044349431991577
Epoch 1130, training loss: 313.3951110839844 = 0.20656275749206543 + 50.0 * 6.263771057128906
Epoch 1130, val loss: 1.0093679428100586
Epoch 1140, training loss: 313.3843688964844 = 0.20173311233520508 + 50.0 * 6.263652801513672
Epoch 1140, val loss: 1.0145328044891357
Epoch 1150, training loss: 313.0318908691406 = 0.19689549505710602 + 50.0 * 6.256699562072754
Epoch 1150, val loss: 1.0195649862289429
Epoch 1160, training loss: 313.0166015625 = 0.1923123002052307 + 50.0 * 6.256485462188721
Epoch 1160, val loss: 1.0248888731002808
Epoch 1170, training loss: 312.9417419433594 = 0.18789903819561005 + 50.0 * 6.255076885223389
Epoch 1170, val loss: 1.0303945541381836
Epoch 1180, training loss: 313.2436828613281 = 0.18356703221797943 + 50.0 * 6.261202335357666
Epoch 1180, val loss: 1.0358957052230835
Epoch 1190, training loss: 313.1755676269531 = 0.17929363250732422 + 50.0 * 6.259925842285156
Epoch 1190, val loss: 1.0410757064819336
Epoch 1200, training loss: 312.900390625 = 0.17507846653461456 + 50.0 * 6.2545061111450195
Epoch 1200, val loss: 1.0464731454849243
Epoch 1210, training loss: 312.8356628417969 = 0.17107249796390533 + 50.0 * 6.253291606903076
Epoch 1210, val loss: 1.0522313117980957
Epoch 1220, training loss: 312.8256530761719 = 0.16718630492687225 + 50.0 * 6.253169536590576
Epoch 1220, val loss: 1.058009386062622
Epoch 1230, training loss: 312.96405029296875 = 0.16338661313056946 + 50.0 * 6.2560133934021
Epoch 1230, val loss: 1.063399076461792
Epoch 1240, training loss: 313.1686706542969 = 0.1596488654613495 + 50.0 * 6.260180950164795
Epoch 1240, val loss: 1.0686166286468506
Epoch 1250, training loss: 312.8411865234375 = 0.15594971179962158 + 50.0 * 6.253705024719238
Epoch 1250, val loss: 1.074572205543518
Epoch 1260, training loss: 312.7102355957031 = 0.15240025520324707 + 50.0 * 6.251156806945801
Epoch 1260, val loss: 1.080394983291626
Epoch 1270, training loss: 312.6399841308594 = 0.14899098873138428 + 50.0 * 6.249819755554199
Epoch 1270, val loss: 1.08619225025177
Epoch 1280, training loss: 312.6165466308594 = 0.1456609070301056 + 50.0 * 6.249417781829834
Epoch 1280, val loss: 1.0923256874084473
Epoch 1290, training loss: 313.3455810546875 = 0.14241649210453033 + 50.0 * 6.264062881469727
Epoch 1290, val loss: 1.098722219467163
Epoch 1300, training loss: 312.7191467285156 = 0.13918767869472504 + 50.0 * 6.251598834991455
Epoch 1300, val loss: 1.10294508934021
Epoch 1310, training loss: 312.5477600097656 = 0.13603635132312775 + 50.0 * 6.248234272003174
Epoch 1310, val loss: 1.108780026435852
Epoch 1320, training loss: 312.5186462402344 = 0.1330321580171585 + 50.0 * 6.247712135314941
Epoch 1320, val loss: 1.1150134801864624
Epoch 1330, training loss: 312.4891357421875 = 0.13013684749603271 + 50.0 * 6.247179985046387
Epoch 1330, val loss: 1.1206920146942139
Epoch 1340, training loss: 312.8842468261719 = 0.12730346620082855 + 50.0 * 6.255138874053955
Epoch 1340, val loss: 1.1264780759811401
Epoch 1350, training loss: 312.48638916015625 = 0.12442421168088913 + 50.0 * 6.247239112854004
Epoch 1350, val loss: 1.1319260597229004
Epoch 1360, training loss: 312.4278564453125 = 0.12166829407215118 + 50.0 * 6.246123790740967
Epoch 1360, val loss: 1.1373915672302246
Epoch 1370, training loss: 312.3665466308594 = 0.11904039978981018 + 50.0 * 6.244949817657471
Epoch 1370, val loss: 1.1434111595153809
Epoch 1380, training loss: 312.4228820800781 = 0.1164807453751564 + 50.0 * 6.246128082275391
Epoch 1380, val loss: 1.149121642112732
Epoch 1390, training loss: 312.442626953125 = 0.11394675821065903 + 50.0 * 6.246573448181152
Epoch 1390, val loss: 1.1545541286468506
Epoch 1400, training loss: 312.4035339355469 = 0.11149009317159653 + 50.0 * 6.245841026306152
Epoch 1400, val loss: 1.160610318183899
Epoch 1410, training loss: 312.6333312988281 = 0.10908584296703339 + 50.0 * 6.250484466552734
Epoch 1410, val loss: 1.165879249572754
Epoch 1420, training loss: 312.3025817871094 = 0.10665345191955566 + 50.0 * 6.243918418884277
Epoch 1420, val loss: 1.1716474294662476
Epoch 1430, training loss: 312.24566650390625 = 0.10439231246709824 + 50.0 * 6.242825508117676
Epoch 1430, val loss: 1.177444338798523
Epoch 1440, training loss: 312.19525146484375 = 0.10217996686697006 + 50.0 * 6.241861343383789
Epoch 1440, val loss: 1.1832741498947144
Epoch 1450, training loss: 312.16522216796875 = 0.10004127025604248 + 50.0 * 6.24130392074585
Epoch 1450, val loss: 1.189051866531372
Epoch 1460, training loss: 312.57098388671875 = 0.09795346856117249 + 50.0 * 6.249460220336914
Epoch 1460, val loss: 1.1941320896148682
Epoch 1470, training loss: 312.36322021484375 = 0.09586302191019058 + 50.0 * 6.245347023010254
Epoch 1470, val loss: 1.2002732753753662
Epoch 1480, training loss: 312.1329345703125 = 0.09381402283906937 + 50.0 * 6.240782260894775
Epoch 1480, val loss: 1.2054497003555298
Epoch 1490, training loss: 312.13153076171875 = 0.09187012910842896 + 50.0 * 6.240793228149414
Epoch 1490, val loss: 1.2111157178878784
Epoch 1500, training loss: 312.2939147949219 = 0.08996804058551788 + 50.0 * 6.244079113006592
Epoch 1500, val loss: 1.217195987701416
Epoch 1510, training loss: 312.3150939941406 = 0.0880868136882782 + 50.0 * 6.244540214538574
Epoch 1510, val loss: 1.2224596738815308
Epoch 1520, training loss: 312.1158752441406 = 0.08627254515886307 + 50.0 * 6.240592002868652
Epoch 1520, val loss: 1.2270294427871704
Epoch 1530, training loss: 312.06231689453125 = 0.08450169116258621 + 50.0 * 6.239556312561035
Epoch 1530, val loss: 1.2330044507980347
Epoch 1540, training loss: 312.2557678222656 = 0.08278922736644745 + 50.0 * 6.243459701538086
Epoch 1540, val loss: 1.2380778789520264
Epoch 1550, training loss: 312.0750427246094 = 0.08105204254388809 + 50.0 * 6.239879608154297
Epoch 1550, val loss: 1.2435721158981323
Epoch 1560, training loss: 311.96270751953125 = 0.07940692454576492 + 50.0 * 6.237666130065918
Epoch 1560, val loss: 1.249241590499878
Epoch 1570, training loss: 311.9482421875 = 0.07780919969081879 + 50.0 * 6.237408638000488
Epoch 1570, val loss: 1.254773497581482
Epoch 1580, training loss: 312.04510498046875 = 0.07624967396259308 + 50.0 * 6.239376544952393
Epoch 1580, val loss: 1.2599419355392456
Epoch 1590, training loss: 311.9696350097656 = 0.07472015917301178 + 50.0 * 6.237898826599121
Epoch 1590, val loss: 1.2648712396621704
Epoch 1600, training loss: 312.25201416015625 = 0.07324358075857162 + 50.0 * 6.243575572967529
Epoch 1600, val loss: 1.2697734832763672
Epoch 1610, training loss: 311.89141845703125 = 0.07173510640859604 + 50.0 * 6.236393451690674
Epoch 1610, val loss: 1.2757798433303833
Epoch 1620, training loss: 311.8116760253906 = 0.070332832634449 + 50.0 * 6.234826564788818
Epoch 1620, val loss: 1.2806792259216309
Epoch 1630, training loss: 311.8055419921875 = 0.06896734237670898 + 50.0 * 6.234731674194336
Epoch 1630, val loss: 1.2860716581344604
Epoch 1640, training loss: 311.9280090332031 = 0.06764059513807297 + 50.0 * 6.237207412719727
Epoch 1640, val loss: 1.2910572290420532
Epoch 1650, training loss: 311.95513916015625 = 0.06631762534379959 + 50.0 * 6.237776279449463
Epoch 1650, val loss: 1.2961167097091675
Epoch 1660, training loss: 311.8614196777344 = 0.06500591337680817 + 50.0 * 6.235928535461426
Epoch 1660, val loss: 1.3018090724945068
Epoch 1670, training loss: 311.7388916015625 = 0.06374434381723404 + 50.0 * 6.2335028648376465
Epoch 1670, val loss: 1.3063102960586548
Epoch 1680, training loss: 311.7419738769531 = 0.06254591047763824 + 50.0 * 6.233588218688965
Epoch 1680, val loss: 1.311653971672058
Epoch 1690, training loss: 312.012939453125 = 0.06137635558843613 + 50.0 * 6.2390313148498535
Epoch 1690, val loss: 1.3162580728530884
Epoch 1700, training loss: 311.7685241699219 = 0.06017380207777023 + 50.0 * 6.234166622161865
Epoch 1700, val loss: 1.3220064640045166
Epoch 1710, training loss: 311.8636779785156 = 0.059035610407590866 + 50.0 * 6.236093044281006
Epoch 1710, val loss: 1.3264847993850708
Epoch 1720, training loss: 311.76446533203125 = 0.05793138965964317 + 50.0 * 6.234130859375
Epoch 1720, val loss: 1.3313862085342407
Epoch 1730, training loss: 311.63006591796875 = 0.056826699525117874 + 50.0 * 6.231464862823486
Epoch 1730, val loss: 1.3365143537521362
Epoch 1740, training loss: 311.63287353515625 = 0.055783335119485855 + 50.0 * 6.231541633605957
Epoch 1740, val loss: 1.341303825378418
Epoch 1750, training loss: 311.984130859375 = 0.05476974695920944 + 50.0 * 6.238586902618408
Epoch 1750, val loss: 1.3461254835128784
Epoch 1760, training loss: 311.8070068359375 = 0.05376230552792549 + 50.0 * 6.235064506530762
Epoch 1760, val loss: 1.3507877588272095
Epoch 1770, training loss: 311.6830749511719 = 0.052754707634449005 + 50.0 * 6.232606410980225
Epoch 1770, val loss: 1.355444312095642
Epoch 1780, training loss: 311.6391296386719 = 0.05179206654429436 + 50.0 * 6.231746673583984
Epoch 1780, val loss: 1.360296368598938
Epoch 1790, training loss: 311.56121826171875 = 0.05086876079440117 + 50.0 * 6.2302069664001465
Epoch 1790, val loss: 1.3650915622711182
Epoch 1800, training loss: 311.6929626464844 = 0.04996717348694801 + 50.0 * 6.232860088348389
Epoch 1800, val loss: 1.369739055633545
Epoch 1810, training loss: 311.5962829589844 = 0.04905400052666664 + 50.0 * 6.230944633483887
Epoch 1810, val loss: 1.374584674835205
Epoch 1820, training loss: 311.59210205078125 = 0.04817822575569153 + 50.0 * 6.2308783531188965
Epoch 1820, val loss: 1.3795220851898193
Epoch 1830, training loss: 311.64691162109375 = 0.047322724014520645 + 50.0 * 6.231991767883301
Epoch 1830, val loss: 1.3839882612228394
Epoch 1840, training loss: 311.54669189453125 = 0.04647218808531761 + 50.0 * 6.23000431060791
Epoch 1840, val loss: 1.3887100219726562
Epoch 1850, training loss: 311.50311279296875 = 0.045664459466934204 + 50.0 * 6.229149341583252
Epoch 1850, val loss: 1.3925385475158691
Epoch 1860, training loss: 311.4310302734375 = 0.0448744110763073 + 50.0 * 6.227723598480225
Epoch 1860, val loss: 1.397669792175293
Epoch 1870, training loss: 311.4715576171875 = 0.04410713165998459 + 50.0 * 6.228549480438232
Epoch 1870, val loss: 1.401917576789856
Epoch 1880, training loss: 311.729736328125 = 0.043348073959350586 + 50.0 * 6.233727931976318
Epoch 1880, val loss: 1.4063043594360352
Epoch 1890, training loss: 311.523193359375 = 0.042592935264110565 + 50.0 * 6.229611873626709
Epoch 1890, val loss: 1.4108219146728516
Epoch 1900, training loss: 311.4063415527344 = 0.04185992851853371 + 50.0 * 6.227289199829102
Epoch 1900, val loss: 1.415350317955017
Epoch 1910, training loss: 311.4654846191406 = 0.04116354137659073 + 50.0 * 6.22848653793335
Epoch 1910, val loss: 1.420271635055542
Epoch 1920, training loss: 311.536376953125 = 0.04046270251274109 + 50.0 * 6.229918479919434
Epoch 1920, val loss: 1.4246338605880737
Epoch 1930, training loss: 311.4354553222656 = 0.039791785180568695 + 50.0 * 6.2279133796691895
Epoch 1930, val loss: 1.4281456470489502
Epoch 1940, training loss: 311.36798095703125 = 0.03912520036101341 + 50.0 * 6.226577281951904
Epoch 1940, val loss: 1.4329125881195068
Epoch 1950, training loss: 311.55126953125 = 0.038503967225551605 + 50.0 * 6.230255126953125
Epoch 1950, val loss: 1.4368669986724854
Epoch 1960, training loss: 311.4186706542969 = 0.037845317274332047 + 50.0 * 6.227616310119629
Epoch 1960, val loss: 1.4420291185379028
Epoch 1970, training loss: 311.40826416015625 = 0.03722642362117767 + 50.0 * 6.227420806884766
Epoch 1970, val loss: 1.445257544517517
Epoch 1980, training loss: 311.33221435546875 = 0.03660605847835541 + 50.0 * 6.225911617279053
Epoch 1980, val loss: 1.449736475944519
Epoch 1990, training loss: 311.2471923828125 = 0.036013130098581314 + 50.0 * 6.224223613739014
Epoch 1990, val loss: 1.453749418258667
Epoch 2000, training loss: 311.3031921386719 = 0.03544585779309273 + 50.0 * 6.22535514831543
Epoch 2000, val loss: 1.45845627784729
Epoch 2010, training loss: 311.4039306640625 = 0.03488346189260483 + 50.0 * 6.227380752563477
Epoch 2010, val loss: 1.462196946144104
Epoch 2020, training loss: 311.39251708984375 = 0.0343300998210907 + 50.0 * 6.227163791656494
Epoch 2020, val loss: 1.4659119844436646
Epoch 2030, training loss: 311.26898193359375 = 0.0337756872177124 + 50.0 * 6.224704265594482
Epoch 2030, val loss: 1.4702726602554321
Epoch 2040, training loss: 311.2025146484375 = 0.03324354439973831 + 50.0 * 6.223385810852051
Epoch 2040, val loss: 1.4745028018951416
Epoch 2050, training loss: 311.2865905761719 = 0.03273102268576622 + 50.0 * 6.225077152252197
Epoch 2050, val loss: 1.4786651134490967
Epoch 2060, training loss: 311.4231262207031 = 0.03221432864665985 + 50.0 * 6.227818012237549
Epoch 2060, val loss: 1.4827312231063843
Epoch 2070, training loss: 311.2754821777344 = 0.031715888530015945 + 50.0 * 6.224875450134277
Epoch 2070, val loss: 1.4863085746765137
Epoch 2080, training loss: 311.2005615234375 = 0.031228233128786087 + 50.0 * 6.223386764526367
Epoch 2080, val loss: 1.4903242588043213
Epoch 2090, training loss: 311.3262023925781 = 0.03076285310089588 + 50.0 * 6.2259087562561035
Epoch 2090, val loss: 1.4938616752624512
Epoch 2100, training loss: 311.21954345703125 = 0.030285248532891273 + 50.0 * 6.223785400390625
Epoch 2100, val loss: 1.498214602470398
Epoch 2110, training loss: 311.212646484375 = 0.029830999672412872 + 50.0 * 6.223655700683594
Epoch 2110, val loss: 1.5025429725646973
Epoch 2120, training loss: 311.2080993652344 = 0.029389072209596634 + 50.0 * 6.223574161529541
Epoch 2120, val loss: 1.505635380744934
Epoch 2130, training loss: 311.0819091796875 = 0.028944486752152443 + 50.0 * 6.221059322357178
Epoch 2130, val loss: 1.5097354650497437
Epoch 2140, training loss: 311.1322021484375 = 0.028524570167064667 + 50.0 * 6.222073554992676
Epoch 2140, val loss: 1.513600468635559
Epoch 2150, training loss: 311.2491760253906 = 0.028116105124354362 + 50.0 * 6.224421501159668
Epoch 2150, val loss: 1.5169881582260132
Epoch 2160, training loss: 311.20428466796875 = 0.027699248865246773 + 50.0 * 6.223531723022461
Epoch 2160, val loss: 1.520906925201416
Epoch 2170, training loss: 311.1934814453125 = 0.027287855744361877 + 50.0 * 6.223323822021484
Epoch 2170, val loss: 1.5250310897827148
Epoch 2180, training loss: 311.0549621582031 = 0.0268874354660511 + 50.0 * 6.220561504364014
Epoch 2180, val loss: 1.5283772945404053
Epoch 2190, training loss: 311.07830810546875 = 0.026498224586248398 + 50.0 * 6.221036434173584
Epoch 2190, val loss: 1.5321102142333984
Epoch 2200, training loss: 311.356689453125 = 0.026133062317967415 + 50.0 * 6.226611614227295
Epoch 2200, val loss: 1.5357153415679932
Epoch 2210, training loss: 311.0928039550781 = 0.02574787475168705 + 50.0 * 6.221340656280518
Epoch 2210, val loss: 1.5395469665527344
Epoch 2220, training loss: 311.1300964355469 = 0.025382691994309425 + 50.0 * 6.2220940589904785
Epoch 2220, val loss: 1.5435137748718262
Epoch 2230, training loss: 311.12335205078125 = 0.025016168132424355 + 50.0 * 6.221966743469238
Epoch 2230, val loss: 1.5468584299087524
Epoch 2240, training loss: 311.0447082519531 = 0.02466948889195919 + 50.0 * 6.220401287078857
Epoch 2240, val loss: 1.5498194694519043
Epoch 2250, training loss: 310.9648132324219 = 0.024324653670191765 + 50.0 * 6.218809604644775
Epoch 2250, val loss: 1.5538209676742554
Epoch 2260, training loss: 311.00042724609375 = 0.023996638134121895 + 50.0 * 6.219528675079346
Epoch 2260, val loss: 1.5573277473449707
Epoch 2270, training loss: 311.223388671875 = 0.023666223511099815 + 50.0 * 6.223994731903076
Epoch 2270, val loss: 1.5605779886245728
Epoch 2280, training loss: 311.151611328125 = 0.023341728374361992 + 50.0 * 6.222565174102783
Epoch 2280, val loss: 1.5644937753677368
Epoch 2290, training loss: 311.0216979980469 = 0.02301645278930664 + 50.0 * 6.219974040985107
Epoch 2290, val loss: 1.5678353309631348
Epoch 2300, training loss: 310.9884033203125 = 0.022714897990226746 + 50.0 * 6.219314098358154
Epoch 2300, val loss: 1.5706738233566284
Epoch 2310, training loss: 310.9581604003906 = 0.02241048403084278 + 50.0 * 6.218715190887451
Epoch 2310, val loss: 1.5744719505310059
Epoch 2320, training loss: 310.9969787597656 = 0.022112378850579262 + 50.0 * 6.219497203826904
Epoch 2320, val loss: 1.5781993865966797
Epoch 2330, training loss: 310.98846435546875 = 0.021820414811372757 + 50.0 * 6.219332695007324
Epoch 2330, val loss: 1.581256628036499
Epoch 2340, training loss: 310.95703125 = 0.02153480239212513 + 50.0 * 6.218709468841553
Epoch 2340, val loss: 1.5843228101730347
Epoch 2350, training loss: 310.97015380859375 = 0.02126123569905758 + 50.0 * 6.218977928161621
Epoch 2350, val loss: 1.5879120826721191
Epoch 2360, training loss: 311.0451965332031 = 0.02098514884710312 + 50.0 * 6.220483779907227
Epoch 2360, val loss: 1.5904406309127808
Epoch 2370, training loss: 311.1368713378906 = 0.020711001008749008 + 50.0 * 6.222322940826416
Epoch 2370, val loss: 1.593189001083374
Epoch 2380, training loss: 310.8935241699219 = 0.02044515311717987 + 50.0 * 6.217461585998535
Epoch 2380, val loss: 1.5970395803451538
Epoch 2390, training loss: 310.850341796875 = 0.020181654021143913 + 50.0 * 6.2166032791137695
Epoch 2390, val loss: 1.6004431247711182
Epoch 2400, training loss: 310.82635498046875 = 0.019928010180592537 + 50.0 * 6.216128349304199
Epoch 2400, val loss: 1.603449821472168
Epoch 2410, training loss: 311.2264709472656 = 0.019686643034219742 + 50.0 * 6.224135875701904
Epoch 2410, val loss: 1.607329249382019
Epoch 2420, training loss: 310.9564514160156 = 0.019434617832303047 + 50.0 * 6.218739986419678
Epoch 2420, val loss: 1.6094894409179688
Epoch 2430, training loss: 311.01336669921875 = 0.01918204128742218 + 50.0 * 6.219883441925049
Epoch 2430, val loss: 1.6131672859191895
Epoch 2440, training loss: 310.8682861328125 = 0.018945397809147835 + 50.0 * 6.216986656188965
Epoch 2440, val loss: 1.6156415939331055
Epoch 2450, training loss: 310.83135986328125 = 0.01871541142463684 + 50.0 * 6.21625280380249
Epoch 2450, val loss: 1.6183068752288818
Epoch 2460, training loss: 310.76513671875 = 0.01848730631172657 + 50.0 * 6.214932918548584
Epoch 2460, val loss: 1.621588110923767
Epoch 2470, training loss: 310.94610595703125 = 0.018272390589118004 + 50.0 * 6.2185564041137695
Epoch 2470, val loss: 1.6241912841796875
Epoch 2480, training loss: 310.82098388671875 = 0.018047474324703217 + 50.0 * 6.216058731079102
Epoch 2480, val loss: 1.628028392791748
Epoch 2490, training loss: 310.99261474609375 = 0.017830485478043556 + 50.0 * 6.21949577331543
Epoch 2490, val loss: 1.6309248208999634
Epoch 2500, training loss: 310.79998779296875 = 0.017608363181352615 + 50.0 * 6.2156476974487305
Epoch 2500, val loss: 1.633288025856018
Epoch 2510, training loss: 310.79827880859375 = 0.017404545098543167 + 50.0 * 6.215617656707764
Epoch 2510, val loss: 1.6361409425735474
Epoch 2520, training loss: 310.8207702636719 = 0.017203742638230324 + 50.0 * 6.216071128845215
Epoch 2520, val loss: 1.6389535665512085
Epoch 2530, training loss: 310.8476257324219 = 0.01700425148010254 + 50.0 * 6.216612815856934
Epoch 2530, val loss: 1.6419309377670288
Epoch 2540, training loss: 310.9278259277344 = 0.01679971255362034 + 50.0 * 6.2182207107543945
Epoch 2540, val loss: 1.6440824270248413
Epoch 2550, training loss: 310.6838684082031 = 0.016601603478193283 + 50.0 * 6.213345050811768
Epoch 2550, val loss: 1.6474632024765015
Epoch 2560, training loss: 310.7140197753906 = 0.016415491700172424 + 50.0 * 6.21395206451416
Epoch 2560, val loss: 1.6502528190612793
Epoch 2570, training loss: 310.8722839355469 = 0.016237378120422363 + 50.0 * 6.217121124267578
Epoch 2570, val loss: 1.6525709629058838
Epoch 2580, training loss: 310.74737548828125 = 0.01604519411921501 + 50.0 * 6.214626312255859
Epoch 2580, val loss: 1.6555087566375732
Epoch 2590, training loss: 310.83331298828125 = 0.015864117071032524 + 50.0 * 6.216348648071289
Epoch 2590, val loss: 1.6576807498931885
Epoch 2600, training loss: 310.7200927734375 = 0.015678437426686287 + 50.0 * 6.214087963104248
Epoch 2600, val loss: 1.6609541177749634
Epoch 2610, training loss: 310.7107238769531 = 0.015499548986554146 + 50.0 * 6.21390438079834
Epoch 2610, val loss: 1.663704752922058
Epoch 2620, training loss: 310.81817626953125 = 0.015333300456404686 + 50.0 * 6.216057300567627
Epoch 2620, val loss: 1.6658705472946167
Epoch 2630, training loss: 310.7355041503906 = 0.015162278898060322 + 50.0 * 6.214406490325928
Epoch 2630, val loss: 1.6689252853393555
Epoch 2640, training loss: 310.6322326660156 = 0.014992867596447468 + 50.0 * 6.212345123291016
Epoch 2640, val loss: 1.6717543601989746
Epoch 2650, training loss: 310.84954833984375 = 0.01483349408954382 + 50.0 * 6.216694355010986
Epoch 2650, val loss: 1.6750400066375732
Epoch 2660, training loss: 310.6457824707031 = 0.01466994546353817 + 50.0 * 6.212622165679932
Epoch 2660, val loss: 1.6766972541809082
Epoch 2670, training loss: 310.6976318359375 = 0.014514729380607605 + 50.0 * 6.213662147521973
Epoch 2670, val loss: 1.6789555549621582
Epoch 2680, training loss: 310.7346496582031 = 0.014354449696838856 + 50.0 * 6.2144060134887695
Epoch 2680, val loss: 1.6821242570877075
Epoch 2690, training loss: 310.73040771484375 = 0.014204143546521664 + 50.0 * 6.214324474334717
Epoch 2690, val loss: 1.684508204460144
Epoch 2700, training loss: 310.64794921875 = 0.014049281366169453 + 50.0 * 6.2126784324646
Epoch 2700, val loss: 1.6870603561401367
Epoch 2710, training loss: 310.6716613769531 = 0.013899985700845718 + 50.0 * 6.213155269622803
Epoch 2710, val loss: 1.689355492591858
Epoch 2720, training loss: 310.6982421875 = 0.013758998364210129 + 50.0 * 6.213689804077148
Epoch 2720, val loss: 1.6920419931411743
Epoch 2730, training loss: 310.6873474121094 = 0.013613065704703331 + 50.0 * 6.213474750518799
Epoch 2730, val loss: 1.694474697113037
Epoch 2740, training loss: 310.7355651855469 = 0.013477304019033909 + 50.0 * 6.214441299438477
Epoch 2740, val loss: 1.6961203813552856
Epoch 2750, training loss: 310.65411376953125 = 0.013331538066267967 + 50.0 * 6.212815284729004
Epoch 2750, val loss: 1.6995267868041992
Epoch 2760, training loss: 310.6202697753906 = 0.013190549798309803 + 50.0 * 6.212141990661621
Epoch 2760, val loss: 1.7016524076461792
Epoch 2770, training loss: 310.56597900390625 = 0.01305922120809555 + 50.0 * 6.211058616638184
Epoch 2770, val loss: 1.7038906812667847
Epoch 2780, training loss: 310.5670166015625 = 0.012931915931403637 + 50.0 * 6.211081504821777
Epoch 2780, val loss: 1.7068830728530884
Epoch 2790, training loss: 310.770263671875 = 0.01280631311237812 + 50.0 * 6.21514892578125
Epoch 2790, val loss: 1.70907461643219
Epoch 2800, training loss: 310.5837707519531 = 0.012672808952629566 + 50.0 * 6.211421966552734
Epoch 2800, val loss: 1.7102534770965576
Epoch 2810, training loss: 310.523193359375 = 0.012544435448944569 + 50.0 * 6.2102131843566895
Epoch 2810, val loss: 1.7133800983428955
Epoch 2820, training loss: 310.8596496582031 = 0.012431638315320015 + 50.0 * 6.216944694519043
Epoch 2820, val loss: 1.7155181169509888
Epoch 2830, training loss: 310.60546875 = 0.012296414002776146 + 50.0 * 6.2118635177612305
Epoch 2830, val loss: 1.7175284624099731
Epoch 2840, training loss: 310.5433044433594 = 0.012173870578408241 + 50.0 * 6.210622787475586
Epoch 2840, val loss: 1.7200034856796265
Epoch 2850, training loss: 310.48681640625 = 0.01205401960760355 + 50.0 * 6.2094950675964355
Epoch 2850, val loss: 1.7220005989074707
Epoch 2860, training loss: 310.5545349121094 = 0.011941278353333473 + 50.0 * 6.210852146148682
Epoch 2860, val loss: 1.7245986461639404
Epoch 2870, training loss: 310.5846252441406 = 0.011828259564936161 + 50.0 * 6.211455821990967
Epoch 2870, val loss: 1.7268213033676147
Epoch 2880, training loss: 310.6152038574219 = 0.0117172347381711 + 50.0 * 6.212069511413574
Epoch 2880, val loss: 1.7287814617156982
Epoch 2890, training loss: 310.4837646484375 = 0.011606041342020035 + 50.0 * 6.209443092346191
Epoch 2890, val loss: 1.730576992034912
Epoch 2900, training loss: 310.6007385253906 = 0.011499699205160141 + 50.0 * 6.211784839630127
Epoch 2900, val loss: 1.732383370399475
Epoch 2910, training loss: 310.48223876953125 = 0.01138732023537159 + 50.0 * 6.20941686630249
Epoch 2910, val loss: 1.734913945198059
Epoch 2920, training loss: 310.5703125 = 0.011282007209956646 + 50.0 * 6.211180210113525
Epoch 2920, val loss: 1.7369834184646606
Epoch 2930, training loss: 310.50921630859375 = 0.011176804080605507 + 50.0 * 6.2099609375
Epoch 2930, val loss: 1.7387326955795288
Epoch 2940, training loss: 310.5644226074219 = 0.011078701354563236 + 50.0 * 6.211067199707031
Epoch 2940, val loss: 1.740575909614563
Epoch 2950, training loss: 310.5238952636719 = 0.010973747819662094 + 50.0 * 6.2102580070495605
Epoch 2950, val loss: 1.7422958612442017
Epoch 2960, training loss: 310.59442138671875 = 0.010874180123209953 + 50.0 * 6.211670875549316
Epoch 2960, val loss: 1.7444065809249878
Epoch 2970, training loss: 310.4667053222656 = 0.010773617774248123 + 50.0 * 6.209118366241455
Epoch 2970, val loss: 1.7466479539871216
Epoch 2980, training loss: 310.39837646484375 = 0.010676251724362373 + 50.0 * 6.207753658294678
Epoch 2980, val loss: 1.749048113822937
Epoch 2990, training loss: 310.5726013183594 = 0.010583719238638878 + 50.0 * 6.211240291595459
Epoch 2990, val loss: 1.751219391822815
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 431.7889099121094 = 1.9475724697113037 + 50.0 * 8.596826553344727
Epoch 0, val loss: 1.9554861783981323
Epoch 10, training loss: 431.7365417480469 = 1.9386792182922363 + 50.0 * 8.595956802368164
Epoch 10, val loss: 1.9466891288757324
Epoch 20, training loss: 431.4131164550781 = 1.927170991897583 + 50.0 * 8.58971881866455
Epoch 20, val loss: 1.93483567237854
Epoch 30, training loss: 429.492919921875 = 1.9113759994506836 + 50.0 * 8.551630973815918
Epoch 30, val loss: 1.9183404445648193
Epoch 40, training loss: 420.3365173339844 = 1.8930370807647705 + 50.0 * 8.36886978149414
Epoch 40, val loss: 1.9000664949417114
Epoch 50, training loss: 401.8125915527344 = 1.8725987672805786 + 50.0 * 7.998800277709961
Epoch 50, val loss: 1.8803743124008179
Epoch 60, training loss: 377.22393798828125 = 1.8572885990142822 + 50.0 * 7.507332801818848
Epoch 60, val loss: 1.8668655157089233
Epoch 70, training loss: 356.35540771484375 = 1.848351001739502 + 50.0 * 7.0901408195495605
Epoch 70, val loss: 1.858178973197937
Epoch 80, training loss: 348.1337585449219 = 1.8402748107910156 + 50.0 * 6.925869941711426
Epoch 80, val loss: 1.8498778343200684
Epoch 90, training loss: 343.402099609375 = 1.8298180103302002 + 50.0 * 6.831446170806885
Epoch 90, val loss: 1.8394135236740112
Epoch 100, training loss: 339.5168151855469 = 1.8188070058822632 + 50.0 * 6.753959655761719
Epoch 100, val loss: 1.8286164999008179
Epoch 110, training loss: 336.5624084472656 = 1.8089406490325928 + 50.0 * 6.695069789886475
Epoch 110, val loss: 1.818731427192688
Epoch 120, training loss: 334.28387451171875 = 1.7997064590454102 + 50.0 * 6.649683475494385
Epoch 120, val loss: 1.8092687129974365
Epoch 130, training loss: 332.642578125 = 1.7902990579605103 + 50.0 * 6.6170454025268555
Epoch 130, val loss: 1.7996702194213867
Epoch 140, training loss: 331.3239440917969 = 1.7805814743041992 + 50.0 * 6.590867042541504
Epoch 140, val loss: 1.790073275566101
Epoch 150, training loss: 330.284423828125 = 1.7705941200256348 + 50.0 * 6.570276260375977
Epoch 150, val loss: 1.7805191278457642
Epoch 160, training loss: 329.4653015136719 = 1.7601947784423828 + 50.0 * 6.554101943969727
Epoch 160, val loss: 1.7708473205566406
Epoch 170, training loss: 328.4926452636719 = 1.7491854429244995 + 50.0 * 6.53486967086792
Epoch 170, val loss: 1.760861873626709
Epoch 180, training loss: 327.7456970214844 = 1.7374484539031982 + 50.0 * 6.520164489746094
Epoch 180, val loss: 1.7503595352172852
Epoch 190, training loss: 326.9356994628906 = 1.7248361110687256 + 50.0 * 6.504217624664307
Epoch 190, val loss: 1.7392675876617432
Epoch 200, training loss: 326.40240478515625 = 1.711256742477417 + 50.0 * 6.493823051452637
Epoch 200, val loss: 1.7273931503295898
Epoch 210, training loss: 325.6002502441406 = 1.696610450744629 + 50.0 * 6.478072643280029
Epoch 210, val loss: 1.7146767377853394
Epoch 220, training loss: 324.93707275390625 = 1.6809618473052979 + 50.0 * 6.465122222900391
Epoch 220, val loss: 1.7011312246322632
Epoch 230, training loss: 324.4212646484375 = 1.664143443107605 + 50.0 * 6.455142021179199
Epoch 230, val loss: 1.6866650581359863
Epoch 240, training loss: 323.78192138671875 = 1.6461082696914673 + 50.0 * 6.442716121673584
Epoch 240, val loss: 1.671366810798645
Epoch 250, training loss: 323.49505615234375 = 1.626988172531128 + 50.0 * 6.437361240386963
Epoch 250, val loss: 1.6551302671432495
Epoch 260, training loss: 322.8280029296875 = 1.6065914630889893 + 50.0 * 6.4244279861450195
Epoch 260, val loss: 1.638079285621643
Epoch 270, training loss: 322.3914794921875 = 1.585072636604309 + 50.0 * 6.416128158569336
Epoch 270, val loss: 1.6201430559158325
Epoch 280, training loss: 321.9506530761719 = 1.5624349117279053 + 50.0 * 6.407764434814453
Epoch 280, val loss: 1.601568579673767
Epoch 290, training loss: 321.5750427246094 = 1.5387669801712036 + 50.0 * 6.400725841522217
Epoch 290, val loss: 1.5822041034698486
Epoch 300, training loss: 321.1904296875 = 1.514106273651123 + 50.0 * 6.393526554107666
Epoch 300, val loss: 1.5623126029968262
Epoch 310, training loss: 320.8592834472656 = 1.4886256456375122 + 50.0 * 6.387413024902344
Epoch 310, val loss: 1.541941523551941
Epoch 320, training loss: 320.6996154785156 = 1.4624402523040771 + 50.0 * 6.384743690490723
Epoch 320, val loss: 1.5211408138275146
Epoch 330, training loss: 320.3521423339844 = 1.4355210065841675 + 50.0 * 6.378332614898682
Epoch 330, val loss: 1.5001165866851807
Epoch 340, training loss: 320.0628662109375 = 1.408188819885254 + 50.0 * 6.373093128204346
Epoch 340, val loss: 1.4789559841156006
Epoch 350, training loss: 319.74481201171875 = 1.3804829120635986 + 50.0 * 6.367286682128906
Epoch 350, val loss: 1.4578194618225098
Epoch 360, training loss: 319.54681396484375 = 1.3525515794754028 + 50.0 * 6.363885402679443
Epoch 360, val loss: 1.4367998838424683
Epoch 370, training loss: 319.4019775390625 = 1.324461579322815 + 50.0 * 6.361550331115723
Epoch 370, val loss: 1.4157335758209229
Epoch 380, training loss: 319.17059326171875 = 1.2961626052856445 + 50.0 * 6.357488632202148
Epoch 380, val loss: 1.3950763940811157
Epoch 390, training loss: 318.8670654296875 = 1.2680262327194214 + 50.0 * 6.351980686187744
Epoch 390, val loss: 1.3747889995574951
Epoch 400, training loss: 318.62359619140625 = 1.2400397062301636 + 50.0 * 6.347671031951904
Epoch 400, val loss: 1.354940414428711
Epoch 410, training loss: 318.6520080566406 = 1.2122033834457397 + 50.0 * 6.348796367645264
Epoch 410, val loss: 1.3354302644729614
Epoch 420, training loss: 318.2112121582031 = 1.1844021081924438 + 50.0 * 6.340536594390869
Epoch 420, val loss: 1.3162815570831299
Epoch 430, training loss: 318.1410217285156 = 1.1568901538848877 + 50.0 * 6.339682579040527
Epoch 430, val loss: 1.2975313663482666
Epoch 440, training loss: 317.9248352050781 = 1.1295641660690308 + 50.0 * 6.335905075073242
Epoch 440, val loss: 1.279227614402771
Epoch 450, training loss: 317.7641296386719 = 1.1025809049606323 + 50.0 * 6.333231449127197
Epoch 450, val loss: 1.2615256309509277
Epoch 460, training loss: 317.613525390625 = 1.0757862329483032 + 50.0 * 6.33075475692749
Epoch 460, val loss: 1.244210124015808
Epoch 470, training loss: 317.392822265625 = 1.0494794845581055 + 50.0 * 6.32686710357666
Epoch 470, val loss: 1.2274271249771118
Epoch 480, training loss: 317.2087707519531 = 1.0235456228256226 + 50.0 * 6.323704242706299
Epoch 480, val loss: 1.21140456199646
Epoch 490, training loss: 317.3480224609375 = 0.9979921579360962 + 50.0 * 6.327000617980957
Epoch 490, val loss: 1.1957792043685913
Epoch 500, training loss: 317.04193115234375 = 0.9728564023971558 + 50.0 * 6.321381092071533
Epoch 500, val loss: 1.1807653903961182
Epoch 510, training loss: 316.80548095703125 = 0.9484016299247742 + 50.0 * 6.317142009735107
Epoch 510, val loss: 1.1666269302368164
Epoch 520, training loss: 316.6422119140625 = 0.9246388673782349 + 50.0 * 6.3143510818481445
Epoch 520, val loss: 1.1532301902770996
Epoch 530, training loss: 316.8272705078125 = 0.9014593362808228 + 50.0 * 6.318516254425049
Epoch 530, val loss: 1.1405069828033447
Epoch 540, training loss: 316.4499816894531 = 0.8790122866630554 + 50.0 * 6.311419486999512
Epoch 540, val loss: 1.1287189722061157
Epoch 550, training loss: 316.2751159667969 = 0.8573416471481323 + 50.0 * 6.308355808258057
Epoch 550, val loss: 1.1177760362625122
Epoch 560, training loss: 316.4808654785156 = 0.8364070653915405 + 50.0 * 6.312889575958252
Epoch 560, val loss: 1.107594609260559
Epoch 570, training loss: 316.1907653808594 = 0.816074550151825 + 50.0 * 6.307493686676025
Epoch 570, val loss: 1.0981476306915283
Epoch 580, training loss: 315.94140625 = 0.7965553402900696 + 50.0 * 6.302896499633789
Epoch 580, val loss: 1.0896005630493164
Epoch 590, training loss: 315.82696533203125 = 0.7777746915817261 + 50.0 * 6.300983905792236
Epoch 590, val loss: 1.0817513465881348
Epoch 600, training loss: 316.09381103515625 = 0.7596355080604553 + 50.0 * 6.306683540344238
Epoch 600, val loss: 1.0745313167572021
Epoch 610, training loss: 315.6416320800781 = 0.7421144247055054 + 50.0 * 6.297989845275879
Epoch 610, val loss: 1.068110704421997
Epoch 620, training loss: 315.5072937011719 = 0.7252340912818909 + 50.0 * 6.2956414222717285
Epoch 620, val loss: 1.0621153116226196
Epoch 630, training loss: 315.91668701171875 = 0.7090343832969666 + 50.0 * 6.304152965545654
Epoch 630, val loss: 1.0566837787628174
Epoch 640, training loss: 315.409423828125 = 0.6931518316268921 + 50.0 * 6.294325828552246
Epoch 640, val loss: 1.0516120195388794
Epoch 650, training loss: 315.2779846191406 = 0.6779104471206665 + 50.0 * 6.292001247406006
Epoch 650, val loss: 1.0471572875976562
Epoch 660, training loss: 315.14019775390625 = 0.6632071733474731 + 50.0 * 6.289539813995361
Epoch 660, val loss: 1.0430243015289307
Epoch 670, training loss: 315.3192443847656 = 0.6489419341087341 + 50.0 * 6.293405532836914
Epoch 670, val loss: 1.0389889478683472
Epoch 680, training loss: 315.26861572265625 = 0.6350347995758057 + 50.0 * 6.2926716804504395
Epoch 680, val loss: 1.0357131958007812
Epoch 690, training loss: 314.9071350097656 = 0.6214926838874817 + 50.0 * 6.285712718963623
Epoch 690, val loss: 1.032471776008606
Epoch 700, training loss: 314.8522033691406 = 0.6084003448486328 + 50.0 * 6.284876346588135
Epoch 700, val loss: 1.0295088291168213
Epoch 710, training loss: 315.02947998046875 = 0.5956906676292419 + 50.0 * 6.288675785064697
Epoch 710, val loss: 1.0268893241882324
Epoch 720, training loss: 314.8629150390625 = 0.5831894278526306 + 50.0 * 6.285594463348389
Epoch 720, val loss: 1.0246241092681885
Epoch 730, training loss: 314.8061218261719 = 0.5710392594337463 + 50.0 * 6.284701824188232
Epoch 730, val loss: 1.0225279331207275
Epoch 740, training loss: 314.7087097167969 = 0.5591631531715393 + 50.0 * 6.2829909324646
Epoch 740, val loss: 1.0208115577697754
Epoch 750, training loss: 314.5401916503906 = 0.5476022362709045 + 50.0 * 6.27985143661499
Epoch 750, val loss: 1.0193586349487305
Epoch 760, training loss: 314.4263916015625 = 0.5363479852676392 + 50.0 * 6.277801036834717
Epoch 760, val loss: 1.0180809497833252
Epoch 770, training loss: 314.3993225097656 = 0.5253645181655884 + 50.0 * 6.27747917175293
Epoch 770, val loss: 1.0171663761138916
Epoch 780, training loss: 314.63641357421875 = 0.5145934820175171 + 50.0 * 6.282436370849609
Epoch 780, val loss: 1.0164684057235718
Epoch 790, training loss: 314.3393859863281 = 0.5039590001106262 + 50.0 * 6.276708126068115
Epoch 790, val loss: 1.015636920928955
Epoch 800, training loss: 314.2195739746094 = 0.4936807155609131 + 50.0 * 6.274518013000488
Epoch 800, val loss: 1.0156012773513794
Epoch 810, training loss: 314.09429931640625 = 0.4836638569831848 + 50.0 * 6.272212505340576
Epoch 810, val loss: 1.0156831741333008
Epoch 820, training loss: 314.0654296875 = 0.47389325499534607 + 50.0 * 6.2718305587768555
Epoch 820, val loss: 1.015958309173584
Epoch 830, training loss: 314.3864440917969 = 0.46430838108062744 + 50.0 * 6.278442859649658
Epoch 830, val loss: 1.0165091753005981
Epoch 840, training loss: 314.0658264160156 = 0.45486581325531006 + 50.0 * 6.272219181060791
Epoch 840, val loss: 1.017175555229187
Epoch 850, training loss: 314.22344970703125 = 0.44567856192588806 + 50.0 * 6.27555513381958
Epoch 850, val loss: 1.0182373523712158
Epoch 860, training loss: 313.9505920410156 = 0.4367436468601227 + 50.0 * 6.27027702331543
Epoch 860, val loss: 1.0194976329803467
Epoch 870, training loss: 313.8298645019531 = 0.4280085563659668 + 50.0 * 6.268036842346191
Epoch 870, val loss: 1.0210297107696533
Epoch 880, training loss: 313.7575988769531 = 0.41953200101852417 + 50.0 * 6.266761779785156
Epoch 880, val loss: 1.0228440761566162
Epoch 890, training loss: 314.23291015625 = 0.4112265110015869 + 50.0 * 6.276433944702148
Epoch 890, val loss: 1.0248405933380127
Epoch 900, training loss: 313.7035827636719 = 0.40301790833473206 + 50.0 * 6.2660112380981445
Epoch 900, val loss: 1.0269508361816406
Epoch 910, training loss: 313.578369140625 = 0.39506256580352783 + 50.0 * 6.263666152954102
Epoch 910, val loss: 1.0293786525726318
Epoch 920, training loss: 313.5406799316406 = 0.38733357191085815 + 50.0 * 6.263067245483398
Epoch 920, val loss: 1.0320085287094116
Epoch 930, training loss: 313.9513854980469 = 0.37977516651153564 + 50.0 * 6.271432399749756
Epoch 930, val loss: 1.0348509550094604
Epoch 940, training loss: 313.6701965332031 = 0.372290700674057 + 50.0 * 6.265958309173584
Epoch 940, val loss: 1.0377883911132812
Epoch 950, training loss: 313.391845703125 = 0.3649856150150299 + 50.0 * 6.260537147521973
Epoch 950, val loss: 1.0409376621246338
Epoch 960, training loss: 313.3934020996094 = 0.35788944363594055 + 50.0 * 6.2607102394104
Epoch 960, val loss: 1.0443841218948364
Epoch 970, training loss: 313.8599548339844 = 0.3509491980075836 + 50.0 * 6.2701802253723145
Epoch 970, val loss: 1.0479141473770142
Epoch 980, training loss: 313.3641662597656 = 0.3439975082874298 + 50.0 * 6.260403156280518
Epoch 980, val loss: 1.051315188407898
Epoch 990, training loss: 313.2913513183594 = 0.3372478485107422 + 50.0 * 6.259081840515137
Epoch 990, val loss: 1.055010437965393
Epoch 1000, training loss: 313.19293212890625 = 0.3306606411933899 + 50.0 * 6.2572455406188965
Epoch 1000, val loss: 1.059057593345642
Epoch 1010, training loss: 313.2316589355469 = 0.3241744637489319 + 50.0 * 6.258149147033691
Epoch 1010, val loss: 1.0630817413330078
Epoch 1020, training loss: 313.1767883300781 = 0.3177473545074463 + 50.0 * 6.257180690765381
Epoch 1020, val loss: 1.0670791864395142
Epoch 1030, training loss: 313.55059814453125 = 0.3114185929298401 + 50.0 * 6.2647833824157715
Epoch 1030, val loss: 1.0712757110595703
Epoch 1040, training loss: 313.22943115234375 = 0.30509334802627563 + 50.0 * 6.258487224578857
Epoch 1040, val loss: 1.0752875804901123
Epoch 1050, training loss: 313.0623474121094 = 0.29890257120132446 + 50.0 * 6.2552690505981445
Epoch 1050, val loss: 1.0797189474105835
Epoch 1060, training loss: 313.0376281738281 = 0.29281866550445557 + 50.0 * 6.25489616394043
Epoch 1060, val loss: 1.0842334032058716
Epoch 1070, training loss: 312.9768371582031 = 0.28677383065223694 + 50.0 * 6.253801345825195
Epoch 1070, val loss: 1.088680386543274
Epoch 1080, training loss: 312.9058532714844 = 0.2807917296886444 + 50.0 * 6.252501010894775
Epoch 1080, val loss: 1.0932908058166504
Epoch 1090, training loss: 312.8986511230469 = 0.27488023042678833 + 50.0 * 6.252475738525391
Epoch 1090, val loss: 1.097868800163269
Epoch 1100, training loss: 312.9447937011719 = 0.2690143585205078 + 50.0 * 6.253515720367432
Epoch 1100, val loss: 1.1025111675262451
Epoch 1110, training loss: 313.0785827636719 = 0.2631622850894928 + 50.0 * 6.256308555603027
Epoch 1110, val loss: 1.1070619821548462
Epoch 1120, training loss: 312.8590087890625 = 0.25731533765792847 + 50.0 * 6.252033710479736
Epoch 1120, val loss: 1.1116975545883179
Epoch 1130, training loss: 312.7266540527344 = 0.251569539308548 + 50.0 * 6.249502182006836
Epoch 1130, val loss: 1.1165274381637573
Epoch 1140, training loss: 312.8130798339844 = 0.24588391184806824 + 50.0 * 6.251344203948975
Epoch 1140, val loss: 1.121351718902588
Epoch 1150, training loss: 312.65374755859375 = 0.24022585153579712 + 50.0 * 6.248270511627197
Epoch 1150, val loss: 1.1260861158370972
Epoch 1160, training loss: 312.6840515136719 = 0.234636589884758 + 50.0 * 6.248988151550293
Epoch 1160, val loss: 1.130874752998352
Epoch 1170, training loss: 312.8580627441406 = 0.22909234464168549 + 50.0 * 6.252579212188721
Epoch 1170, val loss: 1.1357735395431519
Epoch 1180, training loss: 312.676025390625 = 0.2235737293958664 + 50.0 * 6.249049186706543
Epoch 1180, val loss: 1.1406164169311523
Epoch 1190, training loss: 312.57244873046875 = 0.21812622249126434 + 50.0 * 6.247086048126221
Epoch 1190, val loss: 1.1454488039016724
Epoch 1200, training loss: 312.8982238769531 = 0.2127935290336609 + 50.0 * 6.253708362579346
Epoch 1200, val loss: 1.1504459381103516
Epoch 1210, training loss: 312.57574462890625 = 0.20747293531894684 + 50.0 * 6.247365951538086
Epoch 1210, val loss: 1.1552597284317017
Epoch 1220, training loss: 312.4413757324219 = 0.20225302875041962 + 50.0 * 6.244782447814941
Epoch 1220, val loss: 1.160271167755127
Epoch 1230, training loss: 312.3918151855469 = 0.19713684916496277 + 50.0 * 6.243893623352051
Epoch 1230, val loss: 1.1653066873550415
Epoch 1240, training loss: 312.7294921875 = 0.1921055167913437 + 50.0 * 6.2507476806640625
Epoch 1240, val loss: 1.1703312397003174
Epoch 1250, training loss: 312.433837890625 = 0.18714343011379242 + 50.0 * 6.24493408203125
Epoch 1250, val loss: 1.1753007173538208
Epoch 1260, training loss: 312.3612976074219 = 0.18226273357868195 + 50.0 * 6.2435808181762695
Epoch 1260, val loss: 1.1804858446121216
Epoch 1270, training loss: 312.6543273925781 = 0.177537202835083 + 50.0 * 6.24953556060791
Epoch 1270, val loss: 1.1856569051742554
Epoch 1280, training loss: 312.4179992675781 = 0.17285026609897614 + 50.0 * 6.244903087615967
Epoch 1280, val loss: 1.1907131671905518
Epoch 1290, training loss: 312.3590087890625 = 0.16827699542045593 + 50.0 * 6.243814468383789
Epoch 1290, val loss: 1.1960111856460571
Epoch 1300, training loss: 312.2230224609375 = 0.16385406255722046 + 50.0 * 6.241183280944824
Epoch 1300, val loss: 1.2014843225479126
Epoch 1310, training loss: 312.1775207519531 = 0.15953649580478668 + 50.0 * 6.240359783172607
Epoch 1310, val loss: 1.2069404125213623
Epoch 1320, training loss: 312.4357604980469 = 0.1553470939397812 + 50.0 * 6.245608806610107
Epoch 1320, val loss: 1.2122966051101685
Epoch 1330, training loss: 312.1784362792969 = 0.15121977031230927 + 50.0 * 6.240544319152832
Epoch 1330, val loss: 1.2176634073257446
Epoch 1340, training loss: 312.2334289550781 = 0.1472153663635254 + 50.0 * 6.241724491119385
Epoch 1340, val loss: 1.2230980396270752
Epoch 1350, training loss: 312.0703430175781 = 0.1433187872171402 + 50.0 * 6.2385406494140625
Epoch 1350, val loss: 1.2285913228988647
Epoch 1360, training loss: 312.0719909667969 = 0.1395656019449234 + 50.0 * 6.238648414611816
Epoch 1360, val loss: 1.2342984676361084
Epoch 1370, training loss: 312.0447998046875 = 0.13592758774757385 + 50.0 * 6.238177299499512
Epoch 1370, val loss: 1.2398898601531982
Epoch 1380, training loss: 312.3340148925781 = 0.13239340484142303 + 50.0 * 6.244032382965088
Epoch 1380, val loss: 1.2455724477767944
Epoch 1390, training loss: 312.3336181640625 = 0.12892766296863556 + 50.0 * 6.244093418121338
Epoch 1390, val loss: 1.2512555122375488
Epoch 1400, training loss: 311.9696960449219 = 0.12556372582912445 + 50.0 * 6.236883163452148
Epoch 1400, val loss: 1.2569252252578735
Epoch 1410, training loss: 311.91778564453125 = 0.12234519422054291 + 50.0 * 6.2359089851379395
Epoch 1410, val loss: 1.2628991603851318
Epoch 1420, training loss: 311.89886474609375 = 0.11923203617334366 + 50.0 * 6.235592365264893
Epoch 1420, val loss: 1.2688268423080444
Epoch 1430, training loss: 312.45867919921875 = 0.11622458696365356 + 50.0 * 6.246849536895752
Epoch 1430, val loss: 1.274693250656128
Epoch 1440, training loss: 312.0430908203125 = 0.11324099451303482 + 50.0 * 6.2385969161987305
Epoch 1440, val loss: 1.2805839776992798
Epoch 1450, training loss: 311.84967041015625 = 0.11038487404584885 + 50.0 * 6.234786033630371
Epoch 1450, val loss: 1.2867354154586792
Epoch 1460, training loss: 311.85003662109375 = 0.10763760656118393 + 50.0 * 6.2348480224609375
Epoch 1460, val loss: 1.2928944826126099
Epoch 1470, training loss: 311.96990966796875 = 0.10496313869953156 + 50.0 * 6.237298965454102
Epoch 1470, val loss: 1.2989485263824463
Epoch 1480, training loss: 311.9183044433594 = 0.10237729549407959 + 50.0 * 6.236318111419678
Epoch 1480, val loss: 1.305083155632019
Epoch 1490, training loss: 311.901611328125 = 0.09985194355249405 + 50.0 * 6.236035346984863
Epoch 1490, val loss: 1.3110859394073486
Epoch 1500, training loss: 311.8581848144531 = 0.0974036231637001 + 50.0 * 6.235215663909912
Epoch 1500, val loss: 1.3172526359558105
Epoch 1510, training loss: 311.7266845703125 = 0.09505827724933624 + 50.0 * 6.232632160186768
Epoch 1510, val loss: 1.3235399723052979
Epoch 1520, training loss: 311.7208557128906 = 0.09278550744056702 + 50.0 * 6.2325615882873535
Epoch 1520, val loss: 1.3297449350357056
Epoch 1530, training loss: 311.87738037109375 = 0.09058438986539841 + 50.0 * 6.235735893249512
Epoch 1530, val loss: 1.3359534740447998
Epoch 1540, training loss: 311.7013854980469 = 0.08841903507709503 + 50.0 * 6.232259273529053
Epoch 1540, val loss: 1.342190146446228
Epoch 1550, training loss: 312.0639343261719 = 0.08634066581726074 + 50.0 * 6.239552021026611
Epoch 1550, val loss: 1.348461627960205
Epoch 1560, training loss: 311.7998352050781 = 0.08427812159061432 + 50.0 * 6.234310626983643
Epoch 1560, val loss: 1.3547053337097168
Epoch 1570, training loss: 311.6454162597656 = 0.08229873329401016 + 50.0 * 6.23126220703125
Epoch 1570, val loss: 1.3609230518341064
Epoch 1580, training loss: 311.5738830566406 = 0.08039723336696625 + 50.0 * 6.229869842529297
Epoch 1580, val loss: 1.3675087690353394
Epoch 1590, training loss: 311.5537414550781 = 0.07856100052595139 + 50.0 * 6.229503631591797
Epoch 1590, val loss: 1.373863935470581
Epoch 1600, training loss: 311.88092041015625 = 0.0767880529165268 + 50.0 * 6.236082553863525
Epoch 1600, val loss: 1.3800570964813232
Epoch 1610, training loss: 311.6375732421875 = 0.07500741630792618 + 50.0 * 6.231251239776611
Epoch 1610, val loss: 1.3864386081695557
Epoch 1620, training loss: 311.734375 = 0.07330983877182007 + 50.0 * 6.233221530914307
Epoch 1620, val loss: 1.3925570249557495
Epoch 1630, training loss: 311.50567626953125 = 0.07164057344198227 + 50.0 * 6.228680610656738
Epoch 1630, val loss: 1.3992972373962402
Epoch 1640, training loss: 311.5165100097656 = 0.0700482577085495 + 50.0 * 6.22892951965332
Epoch 1640, val loss: 1.4056596755981445
Epoch 1650, training loss: 311.69805908203125 = 0.06850229203701019 + 50.0 * 6.232590675354004
Epoch 1650, val loss: 1.411924123764038
Epoch 1660, training loss: 311.57330322265625 = 0.06696401536464691 + 50.0 * 6.230126857757568
Epoch 1660, val loss: 1.4180055856704712
Epoch 1670, training loss: 311.4921875 = 0.06548722088336945 + 50.0 * 6.22853422164917
Epoch 1670, val loss: 1.424373984336853
Epoch 1680, training loss: 311.704833984375 = 0.06405801326036453 + 50.0 * 6.232815742492676
Epoch 1680, val loss: 1.4306288957595825
Epoch 1690, training loss: 311.47894287109375 = 0.06263717263936996 + 50.0 * 6.228326320648193
Epoch 1690, val loss: 1.4368752241134644
Epoch 1700, training loss: 311.3896179199219 = 0.061289746314287186 + 50.0 * 6.226566791534424
Epoch 1700, val loss: 1.4433900117874146
Epoch 1710, training loss: 311.3466796875 = 0.05997886508703232 + 50.0 * 6.225734233856201
Epoch 1710, val loss: 1.4496512413024902
Epoch 1720, training loss: 311.6244812011719 = 0.058715857565402985 + 50.0 * 6.2313151359558105
Epoch 1720, val loss: 1.4558688402175903
Epoch 1730, training loss: 311.3966064453125 = 0.0574442557990551 + 50.0 * 6.226783752441406
Epoch 1730, val loss: 1.4619439840316772
Epoch 1740, training loss: 311.8132629394531 = 0.05622323229908943 + 50.0 * 6.235140800476074
Epoch 1740, val loss: 1.4680118560791016
Epoch 1750, training loss: 311.4576416015625 = 0.05501851812005043 + 50.0 * 6.228052616119385
Epoch 1750, val loss: 1.4742631912231445
Epoch 1760, training loss: 311.2984619140625 = 0.05385908856987953 + 50.0 * 6.2248921394348145
Epoch 1760, val loss: 1.4805320501327515
Epoch 1770, training loss: 311.2597961425781 = 0.052741970866918564 + 50.0 * 6.2241411209106445
Epoch 1770, val loss: 1.4868022203445435
Epoch 1780, training loss: 311.5658264160156 = 0.05166979879140854 + 50.0 * 6.230283260345459
Epoch 1780, val loss: 1.4928125143051147
Epoch 1790, training loss: 311.3340759277344 = 0.0505853146314621 + 50.0 * 6.2256693840026855
Epoch 1790, val loss: 1.499102234840393
Epoch 1800, training loss: 311.3187255859375 = 0.04954345524311066 + 50.0 * 6.225383758544922
Epoch 1800, val loss: 1.5050595998764038
Epoch 1810, training loss: 311.3216247558594 = 0.04853053390979767 + 50.0 * 6.225461959838867
Epoch 1810, val loss: 1.5113145112991333
Epoch 1820, training loss: 311.30499267578125 = 0.047546274960041046 + 50.0 * 6.225148677825928
Epoch 1820, val loss: 1.5173174142837524
Epoch 1830, training loss: 311.22613525390625 = 0.04658738896250725 + 50.0 * 6.223590850830078
Epoch 1830, val loss: 1.5234278440475464
Epoch 1840, training loss: 311.176025390625 = 0.04565857723355293 + 50.0 * 6.222607135772705
Epoch 1840, val loss: 1.5295319557189941
Epoch 1850, training loss: 311.3819885253906 = 0.04475806653499603 + 50.0 * 6.226744651794434
Epoch 1850, val loss: 1.5355496406555176
Epoch 1860, training loss: 311.2632751464844 = 0.04386087507009506 + 50.0 * 6.224388599395752
Epoch 1860, val loss: 1.5414965152740479
Epoch 1870, training loss: 311.1890563964844 = 0.0429941825568676 + 50.0 * 6.222921371459961
Epoch 1870, val loss: 1.547441005706787
Epoch 1880, training loss: 311.1802978515625 = 0.042155325412750244 + 50.0 * 6.2227630615234375
Epoch 1880, val loss: 1.553430438041687
Epoch 1890, training loss: 311.3628845214844 = 0.04134227707982063 + 50.0 * 6.226430416107178
Epoch 1890, val loss: 1.5593620538711548
Epoch 1900, training loss: 311.11993408203125 = 0.04053022712469101 + 50.0 * 6.221588134765625
Epoch 1900, val loss: 1.565244197845459
Epoch 1910, training loss: 311.1617431640625 = 0.03975660353899002 + 50.0 * 6.222439765930176
Epoch 1910, val loss: 1.571031093597412
Epoch 1920, training loss: 311.1591796875 = 0.039001092314720154 + 50.0 * 6.222403526306152
Epoch 1920, val loss: 1.577121615409851
Epoch 1930, training loss: 311.1385192871094 = 0.03826704993844032 + 50.0 * 6.2220048904418945
Epoch 1930, val loss: 1.5828354358673096
Epoch 1940, training loss: 311.1003723144531 = 0.03755035251379013 + 50.0 * 6.221256732940674
Epoch 1940, val loss: 1.5886927843093872
Epoch 1950, training loss: 311.2665100097656 = 0.036852091550827026 + 50.0 * 6.224593162536621
Epoch 1950, val loss: 1.5943176746368408
Epoch 1960, training loss: 311.0980529785156 = 0.0361504890024662 + 50.0 * 6.221237659454346
Epoch 1960, val loss: 1.6000016927719116
Epoch 1970, training loss: 311.1206359863281 = 0.0354890339076519 + 50.0 * 6.221702575683594
Epoch 1970, val loss: 1.6057401895523071
Epoch 1980, training loss: 311.13848876953125 = 0.034837521612644196 + 50.0 * 6.222072601318359
Epoch 1980, val loss: 1.6114413738250732
Epoch 1990, training loss: 311.0030517578125 = 0.034207724034786224 + 50.0 * 6.219376564025879
Epoch 1990, val loss: 1.6172401905059814
Epoch 2000, training loss: 311.1885986328125 = 0.03360079973936081 + 50.0 * 6.223100185394287
Epoch 2000, val loss: 1.6227818727493286
Epoch 2010, training loss: 311.19854736328125 = 0.03298160806298256 + 50.0 * 6.223310947418213
Epoch 2010, val loss: 1.628355622291565
Epoch 2020, training loss: 311.028076171875 = 0.03239025920629501 + 50.0 * 6.219913959503174
Epoch 2020, val loss: 1.6339832544326782
Epoch 2030, training loss: 310.9604187011719 = 0.031813859939575195 + 50.0 * 6.218572616577148
Epoch 2030, val loss: 1.6396517753601074
Epoch 2040, training loss: 310.93768310546875 = 0.031266000121831894 + 50.0 * 6.218128204345703
Epoch 2040, val loss: 1.6453205347061157
Epoch 2050, training loss: 311.01025390625 = 0.03072969615459442 + 50.0 * 6.219590187072754
Epoch 2050, val loss: 1.6507107019424438
Epoch 2060, training loss: 311.0979919433594 = 0.030197108164429665 + 50.0 * 6.22135591506958
Epoch 2060, val loss: 1.6561353206634521
Epoch 2070, training loss: 311.081787109375 = 0.02966933697462082 + 50.0 * 6.221042633056641
Epoch 2070, val loss: 1.6614038944244385
Epoch 2080, training loss: 310.9705505371094 = 0.029161905869841576 + 50.0 * 6.218827247619629
Epoch 2080, val loss: 1.6668784618377686
Epoch 2090, training loss: 310.9743957519531 = 0.028660442680120468 + 50.0 * 6.21891450881958
Epoch 2090, val loss: 1.6724319458007812
Epoch 2100, training loss: 311.1976318359375 = 0.0281854048371315 + 50.0 * 6.223388671875
Epoch 2100, val loss: 1.6775487661361694
Epoch 2110, training loss: 310.9782409667969 = 0.027696628123521805 + 50.0 * 6.219010353088379
Epoch 2110, val loss: 1.6830629110336304
Epoch 2120, training loss: 310.8597412109375 = 0.027238480746746063 + 50.0 * 6.216649532318115
Epoch 2120, val loss: 1.6884119510650635
Epoch 2130, training loss: 310.81549072265625 = 0.026791928336024284 + 50.0 * 6.215774059295654
Epoch 2130, val loss: 1.6939184665679932
Epoch 2140, training loss: 310.9195556640625 = 0.026361744850873947 + 50.0 * 6.217864036560059
Epoch 2140, val loss: 1.6991145610809326
Epoch 2150, training loss: 311.0271911621094 = 0.025932541117072105 + 50.0 * 6.220025062561035
Epoch 2150, val loss: 1.7041863203048706
Epoch 2160, training loss: 310.9312744140625 = 0.025504183024168015 + 50.0 * 6.218115329742432
Epoch 2160, val loss: 1.709412693977356
Epoch 2170, training loss: 310.8191223144531 = 0.025091465562582016 + 50.0 * 6.215880870819092
Epoch 2170, val loss: 1.714331030845642
Epoch 2180, training loss: 310.8060607910156 = 0.024698207154870033 + 50.0 * 6.215627670288086
Epoch 2180, val loss: 1.7196377515792847
Epoch 2190, training loss: 311.031982421875 = 0.02431502193212509 + 50.0 * 6.220153331756592
Epoch 2190, val loss: 1.7245745658874512
Epoch 2200, training loss: 310.8824462890625 = 0.02392520010471344 + 50.0 * 6.217170238494873
Epoch 2200, val loss: 1.7298601865768433
Epoch 2210, training loss: 310.85125732421875 = 0.023547925055027008 + 50.0 * 6.216553688049316
Epoch 2210, val loss: 1.7344845533370972
Epoch 2220, training loss: 310.91705322265625 = 0.023189222440123558 + 50.0 * 6.217877388000488
Epoch 2220, val loss: 1.7397102117538452
Epoch 2230, training loss: 310.9498596191406 = 0.022824347019195557 + 50.0 * 6.218540668487549
Epoch 2230, val loss: 1.7446426153182983
Epoch 2240, training loss: 310.7930908203125 = 0.02247828245162964 + 50.0 * 6.215412139892578
Epoch 2240, val loss: 1.7494587898254395
Epoch 2250, training loss: 310.7353515625 = 0.022137248888611794 + 50.0 * 6.214264392852783
Epoch 2250, val loss: 1.7545148134231567
Epoch 2260, training loss: 310.8252868652344 = 0.021809598430991173 + 50.0 * 6.216069221496582
Epoch 2260, val loss: 1.759268045425415
Epoch 2270, training loss: 310.8045654296875 = 0.021483490243554115 + 50.0 * 6.215661525726318
Epoch 2270, val loss: 1.7639391422271729
Epoch 2280, training loss: 310.8349304199219 = 0.021164504811167717 + 50.0 * 6.216275215148926
Epoch 2280, val loss: 1.7688478231430054
Epoch 2290, training loss: 310.7825012207031 = 0.020852105692029 + 50.0 * 6.215233325958252
Epoch 2290, val loss: 1.7733573913574219
Epoch 2300, training loss: 310.7663879394531 = 0.020545022562146187 + 50.0 * 6.214917182922363
Epoch 2300, val loss: 1.7780615091323853
Epoch 2310, training loss: 310.8882141113281 = 0.020243743434548378 + 50.0 * 6.21735954284668
Epoch 2310, val loss: 1.7826054096221924
Epoch 2320, training loss: 310.680419921875 = 0.019949844107031822 + 50.0 * 6.21320915222168
Epoch 2320, val loss: 1.7874208688735962
Epoch 2330, training loss: 310.6806945800781 = 0.019668349996209145 + 50.0 * 6.213220596313477
Epoch 2330, val loss: 1.7922930717468262
Epoch 2340, training loss: 310.77374267578125 = 0.019391832873225212 + 50.0 * 6.215087413787842
Epoch 2340, val loss: 1.796615719795227
Epoch 2350, training loss: 310.7503356933594 = 0.019115140661597252 + 50.0 * 6.214624404907227
Epoch 2350, val loss: 1.8012045621871948
Epoch 2360, training loss: 310.8013610839844 = 0.018850887194275856 + 50.0 * 6.2156500816345215
Epoch 2360, val loss: 1.805512547492981
Epoch 2370, training loss: 310.7067565917969 = 0.018583979457616806 + 50.0 * 6.21376371383667
Epoch 2370, val loss: 1.8100718259811401
Epoch 2380, training loss: 310.6867980957031 = 0.01832834631204605 + 50.0 * 6.213369369506836
Epoch 2380, val loss: 1.8146393299102783
Epoch 2390, training loss: 310.6676330566406 = 0.018079346045851707 + 50.0 * 6.212991237640381
Epoch 2390, val loss: 1.8191736936569214
Epoch 2400, training loss: 310.919677734375 = 0.017835458740592003 + 50.0 * 6.218037128448486
Epoch 2400, val loss: 1.8233357667922974
Epoch 2410, training loss: 310.66424560546875 = 0.01758594810962677 + 50.0 * 6.212933540344238
Epoch 2410, val loss: 1.827650785446167
Epoch 2420, training loss: 310.5669860839844 = 0.017347993329167366 + 50.0 * 6.210992813110352
Epoch 2420, val loss: 1.8322677612304688
Epoch 2430, training loss: 310.5594482421875 = 0.01712091453373432 + 50.0 * 6.210846424102783
Epoch 2430, val loss: 1.8366894721984863
Epoch 2440, training loss: 310.8775329589844 = 0.016903622075915337 + 50.0 * 6.217212677001953
Epoch 2440, val loss: 1.8410300016403198
Epoch 2450, training loss: 310.8353271484375 = 0.016679706051945686 + 50.0 * 6.216372489929199
Epoch 2450, val loss: 1.8446944952011108
Epoch 2460, training loss: 310.7058410644531 = 0.01645088382065296 + 50.0 * 6.21378755569458
Epoch 2460, val loss: 1.849123477935791
Epoch 2470, training loss: 310.5684814453125 = 0.016238130629062653 + 50.0 * 6.211044788360596
Epoch 2470, val loss: 1.8534148931503296
Epoch 2480, training loss: 310.5596008300781 = 0.01602945290505886 + 50.0 * 6.210871696472168
Epoch 2480, val loss: 1.857694387435913
Epoch 2490, training loss: 310.7134094238281 = 0.01583106815814972 + 50.0 * 6.213951587677002
Epoch 2490, val loss: 1.8617711067199707
Epoch 2500, training loss: 310.4880676269531 = 0.015626829117536545 + 50.0 * 6.20944881439209
Epoch 2500, val loss: 1.8657524585723877
Epoch 2510, training loss: 310.6067810058594 = 0.015436084009706974 + 50.0 * 6.211826801300049
Epoch 2510, val loss: 1.8698745965957642
Epoch 2520, training loss: 310.9033508300781 = 0.015247879549860954 + 50.0 * 6.217761993408203
Epoch 2520, val loss: 1.874049425125122
Epoch 2530, training loss: 310.6552429199219 = 0.01504499465227127 + 50.0 * 6.212803840637207
Epoch 2530, val loss: 1.8776804208755493
Epoch 2540, training loss: 310.5202941894531 = 0.014854982495307922 + 50.0 * 6.210108757019043
Epoch 2540, val loss: 1.8819036483764648
Epoch 2550, training loss: 310.4563903808594 = 0.014675203710794449 + 50.0 * 6.208834171295166
Epoch 2550, val loss: 1.8860681056976318
Epoch 2560, training loss: 310.4419860839844 = 0.014500513672828674 + 50.0 * 6.208549499511719
Epoch 2560, val loss: 1.8901143074035645
Epoch 2570, training loss: 310.6076965332031 = 0.014332173392176628 + 50.0 * 6.211866855621338
Epoch 2570, val loss: 1.8939988613128662
Epoch 2580, training loss: 310.6123962402344 = 0.014159989543259144 + 50.0 * 6.2119646072387695
Epoch 2580, val loss: 1.8976472616195679
Epoch 2590, training loss: 310.57550048828125 = 0.013980934396386147 + 50.0 * 6.211230278015137
Epoch 2590, val loss: 1.901376485824585
Epoch 2600, training loss: 310.504150390625 = 0.013817152939736843 + 50.0 * 6.209806442260742
Epoch 2600, val loss: 1.9054384231567383
Epoch 2610, training loss: 310.5434265136719 = 0.013656457886099815 + 50.0 * 6.210595607757568
Epoch 2610, val loss: 1.9093130826950073
Epoch 2620, training loss: 310.46405029296875 = 0.013498264364898205 + 50.0 * 6.209011077880859
Epoch 2620, val loss: 1.9129780530929565
Epoch 2630, training loss: 310.4410400390625 = 0.013340954668819904 + 50.0 * 6.208553791046143
Epoch 2630, val loss: 1.9167442321777344
Epoch 2640, training loss: 310.80828857421875 = 0.013194591738283634 + 50.0 * 6.215902328491211
Epoch 2640, val loss: 1.9206359386444092
Epoch 2650, training loss: 310.4949951171875 = 0.013036678545176983 + 50.0 * 6.209639072418213
Epoch 2650, val loss: 1.9239847660064697
Epoch 2660, training loss: 310.42803955078125 = 0.012886776588857174 + 50.0 * 6.208303451538086
Epoch 2660, val loss: 1.9277129173278809
Epoch 2670, training loss: 310.7567138671875 = 0.012748442590236664 + 50.0 * 6.214879035949707
Epoch 2670, val loss: 1.9314380884170532
Epoch 2680, training loss: 310.3789367675781 = 0.012595309875905514 + 50.0 * 6.207326889038086
Epoch 2680, val loss: 1.9347859621047974
Epoch 2690, training loss: 310.34857177734375 = 0.01245418842881918 + 50.0 * 6.206722259521484
Epoch 2690, val loss: 1.9385242462158203
Epoch 2700, training loss: 310.3462219238281 = 0.012319536879658699 + 50.0 * 6.2066779136657715
Epoch 2700, val loss: 1.9423909187316895
Epoch 2710, training loss: 310.37506103515625 = 0.012187895365059376 + 50.0 * 6.2072577476501465
Epoch 2710, val loss: 1.9459558725357056
Epoch 2720, training loss: 310.8275146484375 = 0.012060675770044327 + 50.0 * 6.216309070587158
Epoch 2720, val loss: 1.9492011070251465
Epoch 2730, training loss: 310.54083251953125 = 0.011924670077860355 + 50.0 * 6.210577964782715
Epoch 2730, val loss: 1.9530569314956665
Epoch 2740, training loss: 310.52435302734375 = 0.011792834848165512 + 50.0 * 6.210251331329346
Epoch 2740, val loss: 1.9561233520507812
Epoch 2750, training loss: 310.3881530761719 = 0.011665880680084229 + 50.0 * 6.2075300216674805
Epoch 2750, val loss: 1.9596978425979614
Epoch 2760, training loss: 310.3354797363281 = 0.011543585918843746 + 50.0 * 6.206478595733643
Epoch 2760, val loss: 1.9633533954620361
Epoch 2770, training loss: 310.4904479980469 = 0.011426355689764023 + 50.0 * 6.209580898284912
Epoch 2770, val loss: 1.9669524431228638
Epoch 2780, training loss: 310.40380859375 = 0.011303449049592018 + 50.0 * 6.207850456237793
Epoch 2780, val loss: 1.9700645208358765
Epoch 2790, training loss: 310.4102478027344 = 0.011187070049345493 + 50.0 * 6.207981109619141
Epoch 2790, val loss: 1.9732451438903809
Epoch 2800, training loss: 310.40521240234375 = 0.011072340421378613 + 50.0 * 6.207882881164551
Epoch 2800, val loss: 1.9766979217529297
Epoch 2810, training loss: 310.4286804199219 = 0.010962415486574173 + 50.0 * 6.208354473114014
Epoch 2810, val loss: 1.9799824953079224
Epoch 2820, training loss: 310.3002014160156 = 0.010846475139260292 + 50.0 * 6.205787658691406
Epoch 2820, val loss: 1.9833797216415405
Epoch 2830, training loss: 310.61138916015625 = 0.010740086436271667 + 50.0 * 6.212012767791748
Epoch 2830, val loss: 1.9868247509002686
Epoch 2840, training loss: 310.4747009277344 = 0.01062573678791523 + 50.0 * 6.2092814445495605
Epoch 2840, val loss: 1.9896348714828491
Epoch 2850, training loss: 310.3323669433594 = 0.010514131747186184 + 50.0 * 6.206436634063721
Epoch 2850, val loss: 1.9928525686264038
Epoch 2860, training loss: 310.2696533203125 = 0.010410980321466923 + 50.0 * 6.2051849365234375
Epoch 2860, val loss: 1.9963645935058594
Epoch 2870, training loss: 310.242431640625 = 0.010312510654330254 + 50.0 * 6.204642295837402
Epoch 2870, val loss: 1.9995883703231812
Epoch 2880, training loss: 310.374267578125 = 0.01021635439246893 + 50.0 * 6.20728063583374
Epoch 2880, val loss: 2.0027575492858887
Epoch 2890, training loss: 310.3122253417969 = 0.010114803910255432 + 50.0 * 6.206042289733887
Epoch 2890, val loss: 2.0055854320526123
Epoch 2900, training loss: 310.31390380859375 = 0.010015448555350304 + 50.0 * 6.206077575683594
Epoch 2900, val loss: 2.0089235305786133
Epoch 2910, training loss: 310.4056701660156 = 0.009921574965119362 + 50.0 * 6.207915306091309
Epoch 2910, val loss: 2.0119454860687256
Epoch 2920, training loss: 310.3860778808594 = 0.009825726971030235 + 50.0 * 6.207525253295898
Epoch 2920, val loss: 2.014630079269409
Epoch 2930, training loss: 310.3155822753906 = 0.009729597717523575 + 50.0 * 6.206116676330566
Epoch 2930, val loss: 2.0178062915802
Epoch 2940, training loss: 310.25970458984375 = 0.009639320895075798 + 50.0 * 6.205001354217529
Epoch 2940, val loss: 2.0210678577423096
Epoch 2950, training loss: 310.2098693847656 = 0.009550021961331367 + 50.0 * 6.204006195068359
Epoch 2950, val loss: 2.0240366458892822
Epoch 2960, training loss: 310.35430908203125 = 0.009465746581554413 + 50.0 * 6.206896781921387
Epoch 2960, val loss: 2.0270540714263916
Epoch 2970, training loss: 310.2772216796875 = 0.009375969879329205 + 50.0 * 6.205357074737549
Epoch 2970, val loss: 2.0299975872039795
Epoch 2980, training loss: 310.2880554199219 = 0.009288625791668892 + 50.0 * 6.205575466156006
Epoch 2980, val loss: 2.0327630043029785
Epoch 2990, training loss: 310.7513427734375 = 0.009210268035531044 + 50.0 * 6.214842796325684
Epoch 2990, val loss: 2.035752058029175
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8028465998945704
The final CL Acc:0.71728, 0.02444, The final GNN Acc:0.80548, 0.00269
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13252])
remove edge: torch.Size([2, 7864])
updated graph: torch.Size([2, 10560])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7804870605469 = 1.9380148649215698 + 50.0 * 8.59684944152832
Epoch 0, val loss: 1.9376102685928345
Epoch 10, training loss: 431.7389831542969 = 1.929348349571228 + 50.0 * 8.596192359924316
Epoch 10, val loss: 1.9291598796844482
Epoch 20, training loss: 431.5048522949219 = 1.9183927774429321 + 50.0 * 8.591729164123535
Epoch 20, val loss: 1.918070912361145
Epoch 30, training loss: 429.96820068359375 = 1.9038065671920776 + 50.0 * 8.561287879943848
Epoch 30, val loss: 1.9030617475509644
Epoch 40, training loss: 420.658935546875 = 1.8853877782821655 + 50.0 * 8.375471115112305
Epoch 40, val loss: 1.884902000427246
Epoch 50, training loss: 388.29486083984375 = 1.86420476436615 + 50.0 * 7.728613376617432
Epoch 50, val loss: 1.8651963472366333
Epoch 60, training loss: 371.7745666503906 = 1.8475089073181152 + 50.0 * 7.39854097366333
Epoch 60, val loss: 1.850003719329834
Epoch 70, training loss: 358.47149658203125 = 1.838857889175415 + 50.0 * 7.132652759552002
Epoch 70, val loss: 1.8413060903549194
Epoch 80, training loss: 350.7637939453125 = 1.8284155130386353 + 50.0 * 6.978707313537598
Epoch 80, val loss: 1.8306541442871094
Epoch 90, training loss: 345.3345642089844 = 1.8178627490997314 + 50.0 * 6.870334148406982
Epoch 90, val loss: 1.8204495906829834
Epoch 100, training loss: 340.5438537597656 = 1.8081414699554443 + 50.0 * 6.774714469909668
Epoch 100, val loss: 1.811192274093628
Epoch 110, training loss: 337.22650146484375 = 1.7995084524154663 + 50.0 * 6.708539962768555
Epoch 110, val loss: 1.8025230169296265
Epoch 120, training loss: 334.987060546875 = 1.7897746562957764 + 50.0 * 6.66394567489624
Epoch 120, val loss: 1.7929962873458862
Epoch 130, training loss: 333.1879577636719 = 1.7790820598602295 + 50.0 * 6.628177642822266
Epoch 130, val loss: 1.7830580472946167
Epoch 140, training loss: 331.6994934082031 = 1.7684742212295532 + 50.0 * 6.598620414733887
Epoch 140, val loss: 1.7731746435165405
Epoch 150, training loss: 330.410888671875 = 1.75745689868927 + 50.0 * 6.573069095611572
Epoch 150, val loss: 1.7629454135894775
Epoch 160, training loss: 329.3282165527344 = 1.7454570531845093 + 50.0 * 6.551655292510986
Epoch 160, val loss: 1.7519640922546387
Epoch 170, training loss: 328.1297302246094 = 1.7325392961502075 + 50.0 * 6.5279436111450195
Epoch 170, val loss: 1.7401987314224243
Epoch 180, training loss: 327.12017822265625 = 1.718747854232788 + 50.0 * 6.508028507232666
Epoch 180, val loss: 1.7276976108551025
Epoch 190, training loss: 326.1995849609375 = 1.70374596118927 + 50.0 * 6.489916801452637
Epoch 190, val loss: 1.714200496673584
Epoch 200, training loss: 325.5064392089844 = 1.6872690916061401 + 50.0 * 6.476383686065674
Epoch 200, val loss: 1.699297308921814
Epoch 210, training loss: 324.8282775878906 = 1.6691006422042847 + 50.0 * 6.463183403015137
Epoch 210, val loss: 1.6829818487167358
Epoch 220, training loss: 324.2492370605469 = 1.6493898630142212 + 50.0 * 6.451996803283691
Epoch 220, val loss: 1.6652449369430542
Epoch 230, training loss: 323.9266357421875 = 1.6281501054763794 + 50.0 * 6.445969581604004
Epoch 230, val loss: 1.6461302042007446
Epoch 240, training loss: 323.2681579589844 = 1.6052793264389038 + 50.0 * 6.433257579803467
Epoch 240, val loss: 1.6255995035171509
Epoch 250, training loss: 322.7970886230469 = 1.5809694528579712 + 50.0 * 6.424322605133057
Epoch 250, val loss: 1.603866457939148
Epoch 260, training loss: 322.3261413574219 = 1.5552932024002075 + 50.0 * 6.415416717529297
Epoch 260, val loss: 1.5807653665542603
Epoch 270, training loss: 322.0495300292969 = 1.528180480003357 + 50.0 * 6.410427093505859
Epoch 270, val loss: 1.5567989349365234
Epoch 280, training loss: 321.5957946777344 = 1.4999973773956299 + 50.0 * 6.401916027069092
Epoch 280, val loss: 1.5316241979599
Epoch 290, training loss: 321.2236022949219 = 1.470809817314148 + 50.0 * 6.395056247711182
Epoch 290, val loss: 1.5057103633880615
Epoch 300, training loss: 321.121337890625 = 1.440707802772522 + 50.0 * 6.393612384796143
Epoch 300, val loss: 1.4792487621307373
Epoch 310, training loss: 320.61566162109375 = 1.4098756313323975 + 50.0 * 6.384115695953369
Epoch 310, val loss: 1.4522864818572998
Epoch 320, training loss: 320.2933349609375 = 1.3785549402236938 + 50.0 * 6.3782958984375
Epoch 320, val loss: 1.425163745880127
Epoch 330, training loss: 320.11871337890625 = 1.3468259572982788 + 50.0 * 6.3754377365112305
Epoch 330, val loss: 1.3976922035217285
Epoch 340, training loss: 319.73388671875 = 1.3151212930679321 + 50.0 * 6.368375301361084
Epoch 340, val loss: 1.3704538345336914
Epoch 350, training loss: 319.4602355957031 = 1.283353567123413 + 50.0 * 6.363537311553955
Epoch 350, val loss: 1.34340238571167
Epoch 360, training loss: 319.3520202636719 = 1.2515432834625244 + 50.0 * 6.362009048461914
Epoch 360, val loss: 1.3164972066879272
Epoch 370, training loss: 319.088134765625 = 1.2202149629592896 + 50.0 * 6.357358455657959
Epoch 370, val loss: 1.2901763916015625
Epoch 380, training loss: 318.74560546875 = 1.1893432140350342 + 50.0 * 6.351125240325928
Epoch 380, val loss: 1.2644984722137451
Epoch 390, training loss: 318.6235046386719 = 1.1591477394104004 + 50.0 * 6.349287033081055
Epoch 390, val loss: 1.239751935005188
Epoch 400, training loss: 318.4980773925781 = 1.1295950412750244 + 50.0 * 6.34736967086792
Epoch 400, val loss: 1.2152447700500488
Epoch 410, training loss: 318.22967529296875 = 1.1008433103561401 + 50.0 * 6.34257698059082
Epoch 410, val loss: 1.1921383142471313
Epoch 420, training loss: 317.9814758300781 = 1.0730761289596558 + 50.0 * 6.338167667388916
Epoch 420, val loss: 1.1700302362442017
Epoch 430, training loss: 317.8257141113281 = 1.046233892440796 + 50.0 * 6.33558988571167
Epoch 430, val loss: 1.1489803791046143
Epoch 440, training loss: 317.6072082519531 = 1.0206212997436523 + 50.0 * 6.33173131942749
Epoch 440, val loss: 1.1292517185211182
Epoch 450, training loss: 317.6495666503906 = 0.9959543347358704 + 50.0 * 6.333072662353516
Epoch 450, val loss: 1.110297441482544
Epoch 460, training loss: 317.3615417480469 = 0.9721471071243286 + 50.0 * 6.32778787612915
Epoch 460, val loss: 1.0926960706710815
Epoch 470, training loss: 317.120849609375 = 0.9494414329528809 + 50.0 * 6.323428153991699
Epoch 470, val loss: 1.0763167142868042
Epoch 480, training loss: 316.9593811035156 = 0.927757978439331 + 50.0 * 6.320632457733154
Epoch 480, val loss: 1.0609465837478638
Epoch 490, training loss: 317.348876953125 = 0.9069070816040039 + 50.0 * 6.32883882522583
Epoch 490, val loss: 1.0467355251312256
Epoch 500, training loss: 316.69696044921875 = 0.8868387341499329 + 50.0 * 6.316202640533447
Epoch 500, val loss: 1.0329363346099854
Epoch 510, training loss: 316.57440185546875 = 0.8674854040145874 + 50.0 * 6.314138412475586
Epoch 510, val loss: 1.0200998783111572
Epoch 520, training loss: 316.44171142578125 = 0.8489278554916382 + 50.0 * 6.311855316162109
Epoch 520, val loss: 1.0082087516784668
Epoch 530, training loss: 316.6249084472656 = 0.8308965563774109 + 50.0 * 6.315880298614502
Epoch 530, val loss: 0.9970731139183044
Epoch 540, training loss: 316.3404235839844 = 0.8132947087287903 + 50.0 * 6.310542583465576
Epoch 540, val loss: 0.98619544506073
Epoch 550, training loss: 316.0971984863281 = 0.7962532043457031 + 50.0 * 6.306018829345703
Epoch 550, val loss: 0.9758493900299072
Epoch 560, training loss: 315.96636962890625 = 0.7796058654785156 + 50.0 * 6.303735256195068
Epoch 560, val loss: 0.9662051796913147
Epoch 570, training loss: 316.0279541015625 = 0.7633150219917297 + 50.0 * 6.305293083190918
Epoch 570, val loss: 0.9567999839782715
Epoch 580, training loss: 315.9156188964844 = 0.7472158074378967 + 50.0 * 6.303368091583252
Epoch 580, val loss: 0.9475501775741577
Epoch 590, training loss: 315.7004699707031 = 0.7313095927238464 + 50.0 * 6.299383163452148
Epoch 590, val loss: 0.9390047192573547
Epoch 600, training loss: 315.8002014160156 = 0.7156161069869995 + 50.0 * 6.301692008972168
Epoch 600, val loss: 0.930679976940155
Epoch 610, training loss: 315.5141906738281 = 0.7001095414161682 + 50.0 * 6.296281814575195
Epoch 610, val loss: 0.9221757650375366
Epoch 620, training loss: 315.3774719238281 = 0.6847470998764038 + 50.0 * 6.293854236602783
Epoch 620, val loss: 0.9144232273101807
Epoch 630, training loss: 315.3089294433594 = 0.6695519685745239 + 50.0 * 6.292787551879883
Epoch 630, val loss: 0.9065577387809753
Epoch 640, training loss: 315.3218078613281 = 0.6544324159622192 + 50.0 * 6.2933478355407715
Epoch 640, val loss: 0.8990930914878845
Epoch 650, training loss: 315.125244140625 = 0.6393410563468933 + 50.0 * 6.289718151092529
Epoch 650, val loss: 0.8917012810707092
Epoch 660, training loss: 315.2382507324219 = 0.6244284510612488 + 50.0 * 6.292276382446289
Epoch 660, val loss: 0.8844363689422607
Epoch 670, training loss: 314.9732666015625 = 0.6096895933151245 + 50.0 * 6.287271499633789
Epoch 670, val loss: 0.8775650858879089
Epoch 680, training loss: 314.8533935546875 = 0.5951007008552551 + 50.0 * 6.285166263580322
Epoch 680, val loss: 0.8708785176277161
Epoch 690, training loss: 315.5133361816406 = 0.5806623697280884 + 50.0 * 6.298653602600098
Epoch 690, val loss: 0.8647406697273254
Epoch 700, training loss: 314.74969482421875 = 0.5663162469863892 + 50.0 * 6.28366756439209
Epoch 700, val loss: 0.8579604625701904
Epoch 710, training loss: 314.61077880859375 = 0.5522224307060242 + 50.0 * 6.2811713218688965
Epoch 710, val loss: 0.8519214391708374
Epoch 720, training loss: 314.5365295410156 = 0.5384436249732971 + 50.0 * 6.279961585998535
Epoch 720, val loss: 0.8461412191390991
Epoch 730, training loss: 314.8244323730469 = 0.5248599648475647 + 50.0 * 6.285991191864014
Epoch 730, val loss: 0.8405219316482544
Epoch 740, training loss: 314.4781188964844 = 0.5114246010780334 + 50.0 * 6.27933406829834
Epoch 740, val loss: 0.8352710604667664
Epoch 750, training loss: 314.31488037109375 = 0.49828457832336426 + 50.0 * 6.276331901550293
Epoch 750, val loss: 0.8302108645439148
Epoch 760, training loss: 314.4202575683594 = 0.48543938994407654 + 50.0 * 6.278696537017822
Epoch 760, val loss: 0.8255724310874939
Epoch 770, training loss: 314.2894592285156 = 0.4728464186191559 + 50.0 * 6.276332378387451
Epoch 770, val loss: 0.8207004070281982
Epoch 780, training loss: 314.2850036621094 = 0.46045342087745667 + 50.0 * 6.276491165161133
Epoch 780, val loss: 0.8165971636772156
Epoch 790, training loss: 314.0860595703125 = 0.4483702480792999 + 50.0 * 6.272753715515137
Epoch 790, val loss: 0.8122923970222473
Epoch 800, training loss: 314.00750732421875 = 0.4366028606891632 + 50.0 * 6.27141809463501
Epoch 800, val loss: 0.8084730505943298
Epoch 810, training loss: 313.9388122558594 = 0.4251624345779419 + 50.0 * 6.270272731781006
Epoch 810, val loss: 0.8050632476806641
Epoch 820, training loss: 314.0205993652344 = 0.4140007793903351 + 50.0 * 6.27213191986084
Epoch 820, val loss: 0.801715075969696
Epoch 830, training loss: 313.9169616699219 = 0.40300238132476807 + 50.0 * 6.2702789306640625
Epoch 830, val loss: 0.7986063361167908
Epoch 840, training loss: 314.0752868652344 = 0.39224112033843994 + 50.0 * 6.273661136627197
Epoch 840, val loss: 0.7959198355674744
Epoch 850, training loss: 313.7562561035156 = 0.3817788064479828 + 50.0 * 6.267489910125732
Epoch 850, val loss: 0.792802631855011
Epoch 860, training loss: 313.6685485839844 = 0.37167149782180786 + 50.0 * 6.265937805175781
Epoch 860, val loss: 0.7905734181404114
Epoch 870, training loss: 313.588134765625 = 0.3618609309196472 + 50.0 * 6.264525890350342
Epoch 870, val loss: 0.788472592830658
Epoch 880, training loss: 313.67401123046875 = 0.35230380296707153 + 50.0 * 6.266434192657471
Epoch 880, val loss: 0.7866136431694031
Epoch 890, training loss: 313.540771484375 = 0.3428422212600708 + 50.0 * 6.263958930969238
Epoch 890, val loss: 0.7845684885978699
Epoch 900, training loss: 313.5164794921875 = 0.33371251821517944 + 50.0 * 6.263655662536621
Epoch 900, val loss: 0.7830091118812561
Epoch 910, training loss: 313.3968505859375 = 0.3248792886734009 + 50.0 * 6.261439323425293
Epoch 910, val loss: 0.7816733121871948
Epoch 920, training loss: 313.78326416015625 = 0.3163640797138214 + 50.0 * 6.2693376541137695
Epoch 920, val loss: 0.7804463505744934
Epoch 930, training loss: 313.5025939941406 = 0.30785611271858215 + 50.0 * 6.263894557952881
Epoch 930, val loss: 0.7793678641319275
Epoch 940, training loss: 313.354736328125 = 0.2997491657733917 + 50.0 * 6.261099815368652
Epoch 940, val loss: 0.7784797549247742
Epoch 950, training loss: 313.20599365234375 = 0.2918657660484314 + 50.0 * 6.258282661437988
Epoch 950, val loss: 0.7779474854469299
Epoch 960, training loss: 313.1663513183594 = 0.2842312753200531 + 50.0 * 6.2576422691345215
Epoch 960, val loss: 0.7775793075561523
Epoch 970, training loss: 313.5639953613281 = 0.2768246531486511 + 50.0 * 6.265743255615234
Epoch 970, val loss: 0.777327299118042
Epoch 980, training loss: 313.176513671875 = 0.26955273747444153 + 50.0 * 6.258139133453369
Epoch 980, val loss: 0.7770369648933411
Epoch 990, training loss: 313.1571960449219 = 0.2625419497489929 + 50.0 * 6.257893085479736
Epoch 990, val loss: 0.7771027684211731
Epoch 1000, training loss: 313.05047607421875 = 0.25574812293052673 + 50.0 * 6.255894660949707
Epoch 1000, val loss: 0.7774925231933594
Epoch 1010, training loss: 313.03350830078125 = 0.249172642827034 + 50.0 * 6.2556867599487305
Epoch 1010, val loss: 0.7779824733734131
Epoch 1020, training loss: 312.9875183105469 = 0.24279731512069702 + 50.0 * 6.254894256591797
Epoch 1020, val loss: 0.7783811092376709
Epoch 1030, training loss: 313.0663146972656 = 0.23661662638187408 + 50.0 * 6.256594181060791
Epoch 1030, val loss: 0.7792454361915588
Epoch 1040, training loss: 312.9290771484375 = 0.23054900765419006 + 50.0 * 6.253970146179199
Epoch 1040, val loss: 0.7800156474113464
Epoch 1050, training loss: 312.8157043457031 = 0.22468625009059906 + 50.0 * 6.251820087432861
Epoch 1050, val loss: 0.781097948551178
Epoch 1060, training loss: 312.7624816894531 = 0.219047412276268 + 50.0 * 6.250868797302246
Epoch 1060, val loss: 0.782315731048584
Epoch 1070, training loss: 312.79766845703125 = 0.21358636021614075 + 50.0 * 6.251681327819824
Epoch 1070, val loss: 0.7836053371429443
Epoch 1080, training loss: 312.8299865722656 = 0.20823906362056732 + 50.0 * 6.252434730529785
Epoch 1080, val loss: 0.7851121425628662
Epoch 1090, training loss: 312.7916259765625 = 0.20298516750335693 + 50.0 * 6.251772880554199
Epoch 1090, val loss: 0.7864542007446289
Epoch 1100, training loss: 312.7909240722656 = 0.19797636568546295 + 50.0 * 6.251859188079834
Epoch 1100, val loss: 0.7882134318351746
Epoch 1110, training loss: 312.5943298339844 = 0.19307860732078552 + 50.0 * 6.248024940490723
Epoch 1110, val loss: 0.7901065349578857
Epoch 1120, training loss: 312.59161376953125 = 0.18835951387882233 + 50.0 * 6.24806547164917
Epoch 1120, val loss: 0.7921670079231262
Epoch 1130, training loss: 312.6766052246094 = 0.18375462293624878 + 50.0 * 6.249856948852539
Epoch 1130, val loss: 0.7941675782203674
Epoch 1140, training loss: 312.5495300292969 = 0.17927873134613037 + 50.0 * 6.247405052185059
Epoch 1140, val loss: 0.7960888743400574
Epoch 1150, training loss: 312.53656005859375 = 0.17491032183170319 + 50.0 * 6.2472333908081055
Epoch 1150, val loss: 0.798251748085022
Epoch 1160, training loss: 312.6434631347656 = 0.17063802480697632 + 50.0 * 6.249456882476807
Epoch 1160, val loss: 0.8005264401435852
Epoch 1170, training loss: 312.4617614746094 = 0.16647212207317352 + 50.0 * 6.245905876159668
Epoch 1170, val loss: 0.8027699589729309
Epoch 1180, training loss: 312.36578369140625 = 0.16244938969612122 + 50.0 * 6.2440667152404785
Epoch 1180, val loss: 0.8051773905754089
Epoch 1190, training loss: 312.3233642578125 = 0.15858329832553864 + 50.0 * 6.243296146392822
Epoch 1190, val loss: 0.8077107667922974
Epoch 1200, training loss: 312.36578369140625 = 0.15481999516487122 + 50.0 * 6.2442193031311035
Epoch 1200, val loss: 0.8102607727050781
Epoch 1210, training loss: 312.47601318359375 = 0.15110862255096436 + 50.0 * 6.246497631072998
Epoch 1210, val loss: 0.8128392696380615
Epoch 1220, training loss: 312.2935485839844 = 0.14747312664985657 + 50.0 * 6.242921829223633
Epoch 1220, val loss: 0.8151800632476807
Epoch 1230, training loss: 312.53802490234375 = 0.143996462225914 + 50.0 * 6.247880935668945
Epoch 1230, val loss: 0.8179089426994324
Epoch 1240, training loss: 312.2259521484375 = 0.14054550230503082 + 50.0 * 6.241708278656006
Epoch 1240, val loss: 0.8205858469009399
Epoch 1250, training loss: 312.16204833984375 = 0.1372401863336563 + 50.0 * 6.2404961585998535
Epoch 1250, val loss: 0.8234375715255737
Epoch 1260, training loss: 312.1524658203125 = 0.13405512273311615 + 50.0 * 6.240367889404297
Epoch 1260, val loss: 0.8262003064155579
Epoch 1270, training loss: 312.5939636230469 = 0.13093885779380798 + 50.0 * 6.249260425567627
Epoch 1270, val loss: 0.8288587927818298
Epoch 1280, training loss: 312.1791687011719 = 0.12778706848621368 + 50.0 * 6.24102783203125
Epoch 1280, val loss: 0.8315762877464294
Epoch 1290, training loss: 312.0843811035156 = 0.12480416148900986 + 50.0 * 6.23919153213501
Epoch 1290, val loss: 0.8344550132751465
Epoch 1300, training loss: 312.0549011230469 = 0.12193593382835388 + 50.0 * 6.238658905029297
Epoch 1300, val loss: 0.8374449610710144
Epoch 1310, training loss: 312.1687927246094 = 0.11913354694843292 + 50.0 * 6.240993022918701
Epoch 1310, val loss: 0.84033203125
Epoch 1320, training loss: 312.0014953613281 = 0.11635806411504745 + 50.0 * 6.2377028465271
Epoch 1320, val loss: 0.8433191776275635
Epoch 1330, training loss: 312.4512939453125 = 0.11365015804767609 + 50.0 * 6.246752738952637
Epoch 1330, val loss: 0.8463598489761353
Epoch 1340, training loss: 312.0336608886719 = 0.11101279407739639 + 50.0 * 6.238452911376953
Epoch 1340, val loss: 0.8489087820053101
Epoch 1350, training loss: 311.9091796875 = 0.10844247043132782 + 50.0 * 6.2360148429870605
Epoch 1350, val loss: 0.8521565794944763
Epoch 1360, training loss: 311.8795471191406 = 0.10598831623792648 + 50.0 * 6.235471248626709
Epoch 1360, val loss: 0.8552007079124451
Epoch 1370, training loss: 311.94891357421875 = 0.1035904735326767 + 50.0 * 6.2369065284729
Epoch 1370, val loss: 0.8584136962890625
Epoch 1380, training loss: 311.95855712890625 = 0.10120802372694016 + 50.0 * 6.237147331237793
Epoch 1380, val loss: 0.8613004684448242
Epoch 1390, training loss: 311.8660888671875 = 0.09887758642435074 + 50.0 * 6.235344409942627
Epoch 1390, val loss: 0.8643850684165955
Epoch 1400, training loss: 312.1268005371094 = 0.09663765877485275 + 50.0 * 6.240602970123291
Epoch 1400, val loss: 0.8673560619354248
Epoch 1410, training loss: 311.84295654296875 = 0.09442172199487686 + 50.0 * 6.234970569610596
Epoch 1410, val loss: 0.8704426288604736
Epoch 1420, training loss: 311.7686767578125 = 0.09230472147464752 + 50.0 * 6.233527183532715
Epoch 1420, val loss: 0.8738110661506653
Epoch 1430, training loss: 311.71527099609375 = 0.09024928510189056 + 50.0 * 6.2325005531311035
Epoch 1430, val loss: 0.8769937753677368
Epoch 1440, training loss: 311.94195556640625 = 0.08824459463357925 + 50.0 * 6.23707389831543
Epoch 1440, val loss: 0.8804649114608765
Epoch 1450, training loss: 311.7814636230469 = 0.08622632175683975 + 50.0 * 6.233904838562012
Epoch 1450, val loss: 0.8830909132957458
Epoch 1460, training loss: 311.8061218261719 = 0.0842650905251503 + 50.0 * 6.234436988830566
Epoch 1460, val loss: 0.8864565491676331
Epoch 1470, training loss: 311.66387939453125 = 0.08240623772144318 + 50.0 * 6.231629848480225
Epoch 1470, val loss: 0.8897075653076172
Epoch 1480, training loss: 311.7923278808594 = 0.08060210198163986 + 50.0 * 6.234234809875488
Epoch 1480, val loss: 0.8930949568748474
Epoch 1490, training loss: 311.6246643066406 = 0.07881484180688858 + 50.0 * 6.230916500091553
Epoch 1490, val loss: 0.8962307572364807
Epoch 1500, training loss: 311.78564453125 = 0.07709232717752457 + 50.0 * 6.234171390533447
Epoch 1500, val loss: 0.8993794918060303
Epoch 1510, training loss: 311.5618896484375 = 0.07537803798913956 + 50.0 * 6.229730129241943
Epoch 1510, val loss: 0.9026300311088562
Epoch 1520, training loss: 311.557861328125 = 0.07372558116912842 + 50.0 * 6.229682445526123
Epoch 1520, val loss: 0.905951738357544
Epoch 1530, training loss: 311.53326416015625 = 0.07214554399251938 + 50.0 * 6.229222297668457
Epoch 1530, val loss: 0.9091828465461731
Epoch 1540, training loss: 311.5658874511719 = 0.07060297578573227 + 50.0 * 6.229905128479004
Epoch 1540, val loss: 0.9124430418014526
Epoch 1550, training loss: 311.7135314941406 = 0.06906873732805252 + 50.0 * 6.232889175415039
Epoch 1550, val loss: 0.9157014489173889
Epoch 1560, training loss: 311.5339050292969 = 0.06756467372179031 + 50.0 * 6.2293267250061035
Epoch 1560, val loss: 0.9190297722816467
Epoch 1570, training loss: 311.5074462890625 = 0.06611762195825577 + 50.0 * 6.228826522827148
Epoch 1570, val loss: 0.9225119352340698
Epoch 1580, training loss: 311.5041198730469 = 0.06471653282642365 + 50.0 * 6.228787899017334
Epoch 1580, val loss: 0.9256672263145447
Epoch 1590, training loss: 311.51220703125 = 0.06335750967264175 + 50.0 * 6.228977203369141
Epoch 1590, val loss: 0.9290525317192078
Epoch 1600, training loss: 311.6074523925781 = 0.062029846012592316 + 50.0 * 6.230907917022705
Epoch 1600, val loss: 0.9320124387741089
Epoch 1610, training loss: 311.5932312011719 = 0.06071372702717781 + 50.0 * 6.230650901794434
Epoch 1610, val loss: 0.9353533983230591
Epoch 1620, training loss: 311.5359191894531 = 0.05942052975296974 + 50.0 * 6.229530334472656
Epoch 1620, val loss: 0.9388600587844849
Epoch 1630, training loss: 311.4188537597656 = 0.05817807838320732 + 50.0 * 6.2272138595581055
Epoch 1630, val loss: 0.9420861601829529
Epoch 1640, training loss: 311.38714599609375 = 0.05697593092918396 + 50.0 * 6.2266035079956055
Epoch 1640, val loss: 0.9455186724662781
Epoch 1650, training loss: 311.43890380859375 = 0.05581055209040642 + 50.0 * 6.227661609649658
Epoch 1650, val loss: 0.948824942111969
Epoch 1660, training loss: 311.4117126464844 = 0.054662588983774185 + 50.0 * 6.2271409034729
Epoch 1660, val loss: 0.9520612955093384
Epoch 1670, training loss: 311.40130615234375 = 0.053550608456134796 + 50.0 * 6.226954936981201
Epoch 1670, val loss: 0.9553500413894653
Epoch 1680, training loss: 311.5025329589844 = 0.052449338138103485 + 50.0 * 6.229001522064209
Epoch 1680, val loss: 0.9585601687431335
Epoch 1690, training loss: 311.32342529296875 = 0.051369957625865936 + 50.0 * 6.2254414558410645
Epoch 1690, val loss: 0.9616971611976624
Epoch 1700, training loss: 311.3346862792969 = 0.05033388361334801 + 50.0 * 6.225687503814697
Epoch 1700, val loss: 0.9650607705116272
Epoch 1710, training loss: 311.5116882324219 = 0.04933640733361244 + 50.0 * 6.229246616363525
Epoch 1710, val loss: 0.9682608246803284
Epoch 1720, training loss: 311.24822998046875 = 0.048321470618247986 + 50.0 * 6.223998546600342
Epoch 1720, val loss: 0.9718364477157593
Epoch 1730, training loss: 311.2103271484375 = 0.047371018677949905 + 50.0 * 6.223258972167969
Epoch 1730, val loss: 0.9752646088600159
Epoch 1740, training loss: 311.2563781738281 = 0.04644789919257164 + 50.0 * 6.224198341369629
Epoch 1740, val loss: 0.9786752462387085
Epoch 1750, training loss: 311.3659362792969 = 0.045534100383520126 + 50.0 * 6.226408004760742
Epoch 1750, val loss: 0.9821174740791321
Epoch 1760, training loss: 311.3363342285156 = 0.04463678598403931 + 50.0 * 6.225833892822266
Epoch 1760, val loss: 0.9848946928977966
Epoch 1770, training loss: 311.3116760253906 = 0.04376580938696861 + 50.0 * 6.225358009338379
Epoch 1770, val loss: 0.9883002638816833
Epoch 1780, training loss: 311.33197021484375 = 0.04290724918246269 + 50.0 * 6.225781440734863
Epoch 1780, val loss: 0.9919277429580688
Epoch 1790, training loss: 311.203125 = 0.04207717627286911 + 50.0 * 6.2232208251953125
Epoch 1790, val loss: 0.9949889183044434
Epoch 1800, training loss: 311.15472412109375 = 0.041258689016103745 + 50.0 * 6.222269535064697
Epoch 1800, val loss: 0.9984448552131653
Epoch 1810, training loss: 311.0849609375 = 0.040484633296728134 + 50.0 * 6.220889091491699
Epoch 1810, val loss: 1.0018388032913208
Epoch 1820, training loss: 311.1137390136719 = 0.03972974047064781 + 50.0 * 6.221480369567871
Epoch 1820, val loss: 1.0053386688232422
Epoch 1830, training loss: 311.3925476074219 = 0.038982465863227844 + 50.0 * 6.227071762084961
Epoch 1830, val loss: 1.008594274520874
Epoch 1840, training loss: 311.36895751953125 = 0.038236092776060104 + 50.0 * 6.226614475250244
Epoch 1840, val loss: 1.01170814037323
Epoch 1850, training loss: 311.1795654296875 = 0.037506330758333206 + 50.0 * 6.222841262817383
Epoch 1850, val loss: 1.015035629272461
Epoch 1860, training loss: 311.0523376464844 = 0.036803387105464935 + 50.0 * 6.220310688018799
Epoch 1860, val loss: 1.0184661149978638
Epoch 1870, training loss: 311.1234436035156 = 0.03613817319273949 + 50.0 * 6.22174596786499
Epoch 1870, val loss: 1.0216214656829834
Epoch 1880, training loss: 311.1684265136719 = 0.03546755015850067 + 50.0 * 6.222659587860107
Epoch 1880, val loss: 1.0250763893127441
Epoch 1890, training loss: 311.0775146484375 = 0.03481568023562431 + 50.0 * 6.220853805541992
Epoch 1890, val loss: 1.0284662246704102
Epoch 1900, training loss: 311.1064758300781 = 0.034187767654657364 + 50.0 * 6.2214460372924805
Epoch 1900, val loss: 1.0319410562515259
Epoch 1910, training loss: 311.0920104980469 = 0.03356492519378662 + 50.0 * 6.2211689949035645
Epoch 1910, val loss: 1.0351356267929077
Epoch 1920, training loss: 310.97125244140625 = 0.03296619653701782 + 50.0 * 6.218765735626221
Epoch 1920, val loss: 1.0381498336791992
Epoch 1930, training loss: 311.0030212402344 = 0.03238895907998085 + 50.0 * 6.219412803649902
Epoch 1930, val loss: 1.041511058807373
Epoch 1940, training loss: 311.0516052246094 = 0.031814735382795334 + 50.0 * 6.220395565032959
Epoch 1940, val loss: 1.0449856519699097
Epoch 1950, training loss: 311.06427001953125 = 0.031243767589330673 + 50.0 * 6.220660209655762
Epoch 1950, val loss: 1.048317313194275
Epoch 1960, training loss: 311.0535888671875 = 0.030692148953676224 + 50.0 * 6.220457553863525
Epoch 1960, val loss: 1.05123770236969
Epoch 1970, training loss: 311.0648193359375 = 0.03014915995299816 + 50.0 * 6.220693111419678
Epoch 1970, val loss: 1.0542885065078735
Epoch 1980, training loss: 310.9075622558594 = 0.029620299115777016 + 50.0 * 6.21755838394165
Epoch 1980, val loss: 1.0580471754074097
Epoch 1990, training loss: 310.8829040527344 = 0.02911473996937275 + 50.0 * 6.217075824737549
Epoch 1990, val loss: 1.0614641904830933
Epoch 2000, training loss: 310.93927001953125 = 0.028625700622797012 + 50.0 * 6.218213081359863
Epoch 2000, val loss: 1.064738392829895
Epoch 2010, training loss: 311.07513427734375 = 0.028134256601333618 + 50.0 * 6.220939636230469
Epoch 2010, val loss: 1.0678114891052246
Epoch 2020, training loss: 310.98291015625 = 0.027650849893689156 + 50.0 * 6.219105243682861
Epoch 2020, val loss: 1.0709408521652222
Epoch 2030, training loss: 310.8694152832031 = 0.027189740911126137 + 50.0 * 6.21684455871582
Epoch 2030, val loss: 1.0744084119796753
Epoch 2040, training loss: 311.0273742675781 = 0.02674272283911705 + 50.0 * 6.220012664794922
Epoch 2040, val loss: 1.077641248703003
Epoch 2050, training loss: 310.9339904785156 = 0.02629169449210167 + 50.0 * 6.218153953552246
Epoch 2050, val loss: 1.0805901288986206
Epoch 2060, training loss: 310.90826416015625 = 0.025853753089904785 + 50.0 * 6.217648029327393
Epoch 2060, val loss: 1.0837903022766113
Epoch 2070, training loss: 310.8037414550781 = 0.025418907403945923 + 50.0 * 6.215566158294678
Epoch 2070, val loss: 1.087207317352295
Epoch 2080, training loss: 310.7974548339844 = 0.025011073797941208 + 50.0 * 6.21544885635376
Epoch 2080, val loss: 1.090407133102417
Epoch 2090, training loss: 310.8225402832031 = 0.024612227454781532 + 50.0 * 6.215958595275879
Epoch 2090, val loss: 1.0934687852859497
Epoch 2100, training loss: 310.89990234375 = 0.024223139509558678 + 50.0 * 6.217513561248779
Epoch 2100, val loss: 1.0965169668197632
Epoch 2110, training loss: 311.06378173828125 = 0.023839721456170082 + 50.0 * 6.220798969268799
Epoch 2110, val loss: 1.0997698307037354
Epoch 2120, training loss: 310.91973876953125 = 0.02344452403485775 + 50.0 * 6.217926025390625
Epoch 2120, val loss: 1.1030025482177734
Epoch 2130, training loss: 310.7898254394531 = 0.02306997962296009 + 50.0 * 6.215335369110107
Epoch 2130, val loss: 1.1061642169952393
Epoch 2140, training loss: 310.7615661621094 = 0.02271285094320774 + 50.0 * 6.21477746963501
Epoch 2140, val loss: 1.1094456911087036
Epoch 2150, training loss: 310.7900085449219 = 0.022362710908055305 + 50.0 * 6.215353012084961
Epoch 2150, val loss: 1.1125874519348145
Epoch 2160, training loss: 310.8326110839844 = 0.022022515535354614 + 50.0 * 6.216211795806885
Epoch 2160, val loss: 1.1155626773834229
Epoch 2170, training loss: 310.87359619140625 = 0.02168513648211956 + 50.0 * 6.217037677764893
Epoch 2170, val loss: 1.1183619499206543
Epoch 2180, training loss: 310.7353820800781 = 0.02134014666080475 + 50.0 * 6.21428108215332
Epoch 2180, val loss: 1.1217217445373535
Epoch 2190, training loss: 310.6794738769531 = 0.021018948405981064 + 50.0 * 6.213169097900391
Epoch 2190, val loss: 1.1250174045562744
Epoch 2200, training loss: 310.72308349609375 = 0.02071286179125309 + 50.0 * 6.214047431945801
Epoch 2200, val loss: 1.1279278993606567
Epoch 2210, training loss: 310.8780822753906 = 0.020404867827892303 + 50.0 * 6.217153072357178
Epoch 2210, val loss: 1.1308180093765259
Epoch 2220, training loss: 310.8014831542969 = 0.020089760422706604 + 50.0 * 6.215628147125244
Epoch 2220, val loss: 1.1337192058563232
Epoch 2230, training loss: 310.6980895996094 = 0.019790228456258774 + 50.0 * 6.213566303253174
Epoch 2230, val loss: 1.136844277381897
Epoch 2240, training loss: 310.8104553222656 = 0.019500913098454475 + 50.0 * 6.215819358825684
Epoch 2240, val loss: 1.1397312879562378
Epoch 2250, training loss: 310.68768310546875 = 0.01921689882874489 + 50.0 * 6.213368892669678
Epoch 2250, val loss: 1.1426851749420166
Epoch 2260, training loss: 310.6241760253906 = 0.01893593743443489 + 50.0 * 6.2121052742004395
Epoch 2260, val loss: 1.1459871530532837
Epoch 2270, training loss: 310.7895812988281 = 0.01866975985467434 + 50.0 * 6.215417861938477
Epoch 2270, val loss: 1.1490179300308228
Epoch 2280, training loss: 310.7863464355469 = 0.01838894933462143 + 50.0 * 6.215358734130859
Epoch 2280, val loss: 1.1515181064605713
Epoch 2290, training loss: 310.66351318359375 = 0.018121222034096718 + 50.0 * 6.212907791137695
Epoch 2290, val loss: 1.1545019149780273
Epoch 2300, training loss: 310.5687561035156 = 0.01786443218588829 + 50.0 * 6.211018085479736
Epoch 2300, val loss: 1.1574413776397705
Epoch 2310, training loss: 310.5333251953125 = 0.01762080378830433 + 50.0 * 6.2103142738342285
Epoch 2310, val loss: 1.1604942083358765
Epoch 2320, training loss: 310.6046447753906 = 0.01738291233778 + 50.0 * 6.211745262145996
Epoch 2320, val loss: 1.163542628288269
Epoch 2330, training loss: 310.8712158203125 = 0.017138758674263954 + 50.0 * 6.217081069946289
Epoch 2330, val loss: 1.1664416790008545
Epoch 2340, training loss: 310.604248046875 = 0.016887353733181953 + 50.0 * 6.211747169494629
Epoch 2340, val loss: 1.1687498092651367
Epoch 2350, training loss: 310.5767822265625 = 0.01665624789893627 + 50.0 * 6.211202621459961
Epoch 2350, val loss: 1.1713299751281738
Epoch 2360, training loss: 310.52496337890625 = 0.016429070383310318 + 50.0 * 6.210170745849609
Epoch 2360, val loss: 1.1746686697006226
Epoch 2370, training loss: 310.4950256347656 = 0.016215477138757706 + 50.0 * 6.209576606750488
Epoch 2370, val loss: 1.1773396730422974
Epoch 2380, training loss: 310.5698547363281 = 0.016006186604499817 + 50.0 * 6.2110772132873535
Epoch 2380, val loss: 1.180126667022705
Epoch 2390, training loss: 310.6843566894531 = 0.015791308134794235 + 50.0 * 6.213371753692627
Epoch 2390, val loss: 1.1824703216552734
Epoch 2400, training loss: 310.5616149902344 = 0.015571734867990017 + 50.0 * 6.210921287536621
Epoch 2400, val loss: 1.1854466199874878
Epoch 2410, training loss: 310.4856872558594 = 0.015366376377642155 + 50.0 * 6.20940637588501
Epoch 2410, val loss: 1.1881061792373657
Epoch 2420, training loss: 310.5810546875 = 0.015171135775744915 + 50.0 * 6.211317539215088
Epoch 2420, val loss: 1.1911711692810059
Epoch 2430, training loss: 310.5684509277344 = 0.014971839264035225 + 50.0 * 6.211069583892822
Epoch 2430, val loss: 1.1936317682266235
Epoch 2440, training loss: 310.5679016113281 = 0.01477636955678463 + 50.0 * 6.211062908172607
Epoch 2440, val loss: 1.1962777376174927
Epoch 2450, training loss: 310.7717590332031 = 0.014585759490728378 + 50.0 * 6.21514368057251
Epoch 2450, val loss: 1.199235200881958
Epoch 2460, training loss: 310.4566650390625 = 0.014396511018276215 + 50.0 * 6.208845138549805
Epoch 2460, val loss: 1.2013691663742065
Epoch 2470, training loss: 310.4405822753906 = 0.014217864722013474 + 50.0 * 6.208527088165283
Epoch 2470, val loss: 1.2040144205093384
Epoch 2480, training loss: 310.67071533203125 = 0.014044119976460934 + 50.0 * 6.213133335113525
Epoch 2480, val loss: 1.2067221403121948
Epoch 2490, training loss: 310.4323425292969 = 0.013860022649168968 + 50.0 * 6.208369731903076
Epoch 2490, val loss: 1.2093465328216553
Epoch 2500, training loss: 310.4010314941406 = 0.013686192221939564 + 50.0 * 6.207747459411621
Epoch 2500, val loss: 1.211868405342102
Epoch 2510, training loss: 310.4056091308594 = 0.013521050103008747 + 50.0 * 6.207841873168945
Epoch 2510, val loss: 1.2143588066101074
Epoch 2520, training loss: 310.3921203613281 = 0.013361761346459389 + 50.0 * 6.207574844360352
Epoch 2520, val loss: 1.2170459032058716
Epoch 2530, training loss: 310.8294677734375 = 0.013208726420998573 + 50.0 * 6.216325283050537
Epoch 2530, val loss: 1.2195650339126587
Epoch 2540, training loss: 310.5986328125 = 0.013035479001700878 + 50.0 * 6.211711883544922
Epoch 2540, val loss: 1.2220609188079834
Epoch 2550, training loss: 310.4476623535156 = 0.01287382747977972 + 50.0 * 6.208695888519287
Epoch 2550, val loss: 1.2243961095809937
Epoch 2560, training loss: 310.3810119628906 = 0.012718736194074154 + 50.0 * 6.207365989685059
Epoch 2560, val loss: 1.2267816066741943
Epoch 2570, training loss: 310.473388671875 = 0.01257838774472475 + 50.0 * 6.209216594696045
Epoch 2570, val loss: 1.2289657592773438
Epoch 2580, training loss: 310.7109680175781 = 0.012426100671291351 + 50.0 * 6.213971138000488
Epoch 2580, val loss: 1.2315821647644043
Epoch 2590, training loss: 310.3572998046875 = 0.012266376987099648 + 50.0 * 6.206900596618652
Epoch 2590, val loss: 1.2339365482330322
Epoch 2600, training loss: 310.319580078125 = 0.01212237123399973 + 50.0 * 6.206149101257324
Epoch 2600, val loss: 1.2367569208145142
Epoch 2610, training loss: 310.3164367675781 = 0.011986878700554371 + 50.0 * 6.206089019775391
Epoch 2610, val loss: 1.239030361175537
Epoch 2620, training loss: 310.3463134765625 = 0.011854980140924454 + 50.0 * 6.206689357757568
Epoch 2620, val loss: 1.2416930198669434
Epoch 2630, training loss: 310.66387939453125 = 0.011725679971277714 + 50.0 * 6.213043212890625
Epoch 2630, val loss: 1.2439907789230347
Epoch 2640, training loss: 310.40594482421875 = 0.011584201827645302 + 50.0 * 6.207886695861816
Epoch 2640, val loss: 1.246076226234436
Epoch 2650, training loss: 310.3460693359375 = 0.011453702114522457 + 50.0 * 6.206692218780518
Epoch 2650, val loss: 1.248915433883667
Epoch 2660, training loss: 310.5545349121094 = 0.011328778229653835 + 50.0 * 6.210864543914795
Epoch 2660, val loss: 1.2510210275650024
Epoch 2670, training loss: 310.35186767578125 = 0.0111989825963974 + 50.0 * 6.206813335418701
Epoch 2670, val loss: 1.253066062927246
Epoch 2680, training loss: 310.37713623046875 = 0.011074231006205082 + 50.0 * 6.2073211669921875
Epoch 2680, val loss: 1.2552653551101685
Epoch 2690, training loss: 310.3301086425781 = 0.010951233096420765 + 50.0 * 6.206383228302002
Epoch 2690, val loss: 1.2579392194747925
Epoch 2700, training loss: 310.2948913574219 = 0.010833077132701874 + 50.0 * 6.205680847167969
Epoch 2700, val loss: 1.2601078748703003
Epoch 2710, training loss: 310.3520812988281 = 0.010720677673816681 + 50.0 * 6.206827640533447
Epoch 2710, val loss: 1.262150764465332
Epoch 2720, training loss: 310.46142578125 = 0.010607442818582058 + 50.0 * 6.209015846252441
Epoch 2720, val loss: 1.264121651649475
Epoch 2730, training loss: 310.5306701660156 = 0.010484323836863041 + 50.0 * 6.2104034423828125
Epoch 2730, val loss: 1.2666020393371582
Epoch 2740, training loss: 310.33966064453125 = 0.010367536917328835 + 50.0 * 6.20658540725708
Epoch 2740, val loss: 1.2689253091812134
Epoch 2750, training loss: 310.25439453125 = 0.010259459726512432 + 50.0 * 6.204883098602295
Epoch 2750, val loss: 1.270983099937439
Epoch 2760, training loss: 310.2473449707031 = 0.010153399780392647 + 50.0 * 6.204743385314941
Epoch 2760, val loss: 1.2733241319656372
Epoch 2770, training loss: 310.47418212890625 = 0.010049994103610516 + 50.0 * 6.209282875061035
Epoch 2770, val loss: 1.2755011320114136
Epoch 2780, training loss: 310.2834777832031 = 0.00994498934596777 + 50.0 * 6.205470561981201
Epoch 2780, val loss: 1.2776938676834106
Epoch 2790, training loss: 310.332763671875 = 0.009841693565249443 + 50.0 * 6.206458568572998
Epoch 2790, val loss: 1.2797818183898926
Epoch 2800, training loss: 310.24395751953125 = 0.009738605469465256 + 50.0 * 6.204684734344482
Epoch 2800, val loss: 1.281895637512207
Epoch 2810, training loss: 310.3098449707031 = 0.009643547236919403 + 50.0 * 6.2060041427612305
Epoch 2810, val loss: 1.2838701009750366
Epoch 2820, training loss: 310.25054931640625 = 0.009544076398015022 + 50.0 * 6.204819679260254
Epoch 2820, val loss: 1.2861508131027222
Epoch 2830, training loss: 310.6018371582031 = 0.009450055658817291 + 50.0 * 6.21184778213501
Epoch 2830, val loss: 1.287994384765625
Epoch 2840, training loss: 310.34930419921875 = 0.009351184591650963 + 50.0 * 6.206799030303955
Epoch 2840, val loss: 1.2898041009902954
Epoch 2850, training loss: 310.24664306640625 = 0.0092538520693779 + 50.0 * 6.204747676849365
Epoch 2850, val loss: 1.292230486869812
Epoch 2860, training loss: 310.2869873046875 = 0.009165856055915356 + 50.0 * 6.205556392669678
Epoch 2860, val loss: 1.2941124439239502
Epoch 2870, training loss: 310.2624816894531 = 0.009074793197214603 + 50.0 * 6.205068111419678
Epoch 2870, val loss: 1.2961599826812744
Epoch 2880, training loss: 310.1805114746094 = 0.00898496899753809 + 50.0 * 6.203430652618408
Epoch 2880, val loss: 1.2984708547592163
Epoch 2890, training loss: 310.4015197753906 = 0.008901701308786869 + 50.0 * 6.207851886749268
Epoch 2890, val loss: 1.3006821870803833
Epoch 2900, training loss: 310.2246398925781 = 0.008814063854515553 + 50.0 * 6.204316139221191
Epoch 2900, val loss: 1.302369236946106
Epoch 2910, training loss: 310.1376953125 = 0.008728907443583012 + 50.0 * 6.202579498291016
Epoch 2910, val loss: 1.304439663887024
Epoch 2920, training loss: 310.1669616699219 = 0.008647274225950241 + 50.0 * 6.203166484832764
Epoch 2920, val loss: 1.3064217567443848
Epoch 2930, training loss: 310.35882568359375 = 0.00856947060674429 + 50.0 * 6.207005023956299
Epoch 2930, val loss: 1.308444857597351
Epoch 2940, training loss: 310.2205505371094 = 0.008486040867865086 + 50.0 * 6.2042412757873535
Epoch 2940, val loss: 1.3107129335403442
Epoch 2950, training loss: 310.1844787597656 = 0.008403124287724495 + 50.0 * 6.203521728515625
Epoch 2950, val loss: 1.3121247291564941
Epoch 2960, training loss: 310.1246337890625 = 0.00832638330757618 + 50.0 * 6.202325820922852
Epoch 2960, val loss: 1.3140742778778076
Epoch 2970, training loss: 310.1365661621094 = 0.008253353647887707 + 50.0 * 6.202566623687744
Epoch 2970, val loss: 1.3159838914871216
Epoch 2980, training loss: 310.3448791503906 = 0.008181178011000156 + 50.0 * 6.2067341804504395
Epoch 2980, val loss: 1.3178493976593018
Epoch 2990, training loss: 310.1473388671875 = 0.008103971369564533 + 50.0 * 6.202784538269043
Epoch 2990, val loss: 1.3198553323745728
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 431.7917785644531 = 1.9494727849960327 + 50.0 * 8.596846580505371
Epoch 0, val loss: 1.9512749910354614
Epoch 10, training loss: 431.748046875 = 1.9406278133392334 + 50.0 * 8.596148490905762
Epoch 10, val loss: 1.9419270753860474
Epoch 20, training loss: 431.49407958984375 = 1.9297430515289307 + 50.0 * 8.591286659240723
Epoch 20, val loss: 1.930281400680542
Epoch 30, training loss: 429.82110595703125 = 1.9154765605926514 + 50.0 * 8.558113098144531
Epoch 30, val loss: 1.915130615234375
Epoch 40, training loss: 420.72784423828125 = 1.896740436553955 + 50.0 * 8.376622200012207
Epoch 40, val loss: 1.8961551189422607
Epoch 50, training loss: 394.8804016113281 = 1.875610589981079 + 50.0 * 7.860095977783203
Epoch 50, val loss: 1.8758845329284668
Epoch 60, training loss: 373.8623352050781 = 1.8593206405639648 + 50.0 * 7.440060138702393
Epoch 60, val loss: 1.8612176179885864
Epoch 70, training loss: 356.7026672363281 = 1.8485591411590576 + 50.0 * 7.097082138061523
Epoch 70, val loss: 1.8508965969085693
Epoch 80, training loss: 347.90576171875 = 1.8390743732452393 + 50.0 * 6.9213337898254395
Epoch 80, val loss: 1.8413807153701782
Epoch 90, training loss: 343.0799255371094 = 1.82993745803833 + 50.0 * 6.824999809265137
Epoch 90, val loss: 1.8324238061904907
Epoch 100, training loss: 339.3714904785156 = 1.8198639154434204 + 50.0 * 6.751032829284668
Epoch 100, val loss: 1.8232061862945557
Epoch 110, training loss: 336.2627258300781 = 1.810134768486023 + 50.0 * 6.689052104949951
Epoch 110, val loss: 1.8144038915634155
Epoch 120, training loss: 334.0557861328125 = 1.8014899492263794 + 50.0 * 6.645086288452148
Epoch 120, val loss: 1.8060778379440308
Epoch 130, training loss: 332.27044677734375 = 1.792823076248169 + 50.0 * 6.609552383422852
Epoch 130, val loss: 1.7976133823394775
Epoch 140, training loss: 330.6953430175781 = 1.7836958169937134 + 50.0 * 6.578233242034912
Epoch 140, val loss: 1.7887624502182007
Epoch 150, training loss: 329.4598083496094 = 1.774200677871704 + 50.0 * 6.553712368011475
Epoch 150, val loss: 1.7797502279281616
Epoch 160, training loss: 328.29766845703125 = 1.7640838623046875 + 50.0 * 6.5306715965271
Epoch 160, val loss: 1.7703529596328735
Epoch 170, training loss: 327.2562255859375 = 1.7533842325210571 + 50.0 * 6.510056972503662
Epoch 170, val loss: 1.7604682445526123
Epoch 180, training loss: 326.4263000488281 = 1.7420283555984497 + 50.0 * 6.493685245513916
Epoch 180, val loss: 1.750016689300537
Epoch 190, training loss: 325.67132568359375 = 1.7296605110168457 + 50.0 * 6.478832721710205
Epoch 190, val loss: 1.7387927770614624
Epoch 200, training loss: 324.93829345703125 = 1.7163101434707642 + 50.0 * 6.464439868927002
Epoch 200, val loss: 1.7267299890518188
Epoch 210, training loss: 324.26837158203125 = 1.7018098831176758 + 50.0 * 6.45133113861084
Epoch 210, val loss: 1.713680624961853
Epoch 220, training loss: 323.92938232421875 = 1.6860246658325195 + 50.0 * 6.44486665725708
Epoch 220, val loss: 1.6997132301330566
Epoch 230, training loss: 323.27545166015625 = 1.6689677238464355 + 50.0 * 6.432129383087158
Epoch 230, val loss: 1.6845943927764893
Epoch 240, training loss: 322.7961730957031 = 1.6505963802337646 + 50.0 * 6.422911167144775
Epoch 240, val loss: 1.668427586555481
Epoch 250, training loss: 322.3811340332031 = 1.6308542490005493 + 50.0 * 6.415005683898926
Epoch 250, val loss: 1.6512506008148193
Epoch 260, training loss: 321.96160888671875 = 1.6098343133926392 + 50.0 * 6.4070353507995605
Epoch 260, val loss: 1.6330169439315796
Epoch 270, training loss: 321.8074645996094 = 1.587664008140564 + 50.0 * 6.404396057128906
Epoch 270, val loss: 1.6139072179794312
Epoch 280, training loss: 321.251953125 = 1.5644721984863281 + 50.0 * 6.393749237060547
Epoch 280, val loss: 1.5939362049102783
Epoch 290, training loss: 320.90643310546875 = 1.5404243469238281 + 50.0 * 6.387320041656494
Epoch 290, val loss: 1.573386788368225
Epoch 300, training loss: 320.6515808105469 = 1.5157301425933838 + 50.0 * 6.382717132568359
Epoch 300, val loss: 1.5523990392684937
Epoch 310, training loss: 320.4032897949219 = 1.490465521812439 + 50.0 * 6.378256320953369
Epoch 310, val loss: 1.5311442613601685
Epoch 320, training loss: 320.0629577636719 = 1.46498703956604 + 50.0 * 6.371959209442139
Epoch 320, val loss: 1.5098272562026978
Epoch 330, training loss: 319.8009338378906 = 1.439417839050293 + 50.0 * 6.367230415344238
Epoch 330, val loss: 1.488606572151184
Epoch 340, training loss: 319.6895446777344 = 1.4139121770858765 + 50.0 * 6.365512847900391
Epoch 340, val loss: 1.4676415920257568
Epoch 350, training loss: 319.3790283203125 = 1.388615608215332 + 50.0 * 6.359808444976807
Epoch 350, val loss: 1.4470399618148804
Epoch 360, training loss: 319.1100158691406 = 1.3637222051620483 + 50.0 * 6.354926109313965
Epoch 360, val loss: 1.426970362663269
Epoch 370, training loss: 318.8963623046875 = 1.3393362760543823 + 50.0 * 6.351140975952148
Epoch 370, val loss: 1.4074804782867432
Epoch 380, training loss: 318.6921691894531 = 1.3152832984924316 + 50.0 * 6.347537994384766
Epoch 380, val loss: 1.3885166645050049
Epoch 390, training loss: 318.537109375 = 1.2918473482131958 + 50.0 * 6.344905376434326
Epoch 390, val loss: 1.3701088428497314
Epoch 400, training loss: 318.3133544921875 = 1.2688896656036377 + 50.0 * 6.3408894538879395
Epoch 400, val loss: 1.3523460626602173
Epoch 410, training loss: 318.1532287597656 = 1.246524691581726 + 50.0 * 6.338134288787842
Epoch 410, val loss: 1.335160732269287
Epoch 420, training loss: 317.9883117675781 = 1.2246499061584473 + 50.0 * 6.335273742675781
Epoch 420, val loss: 1.318319320678711
Epoch 430, training loss: 317.8277282714844 = 1.203213095664978 + 50.0 * 6.332489967346191
Epoch 430, val loss: 1.301940679550171
Epoch 440, training loss: 317.73114013671875 = 1.1822538375854492 + 50.0 * 6.330977916717529
Epoch 440, val loss: 1.2859375476837158
Epoch 450, training loss: 317.4958190917969 = 1.1617647409439087 + 50.0 * 6.326681137084961
Epoch 450, val loss: 1.2706443071365356
Epoch 460, training loss: 317.3124084472656 = 1.1416420936584473 + 50.0 * 6.323415279388428
Epoch 460, val loss: 1.2554782629013062
Epoch 470, training loss: 317.46728515625 = 1.1219052076339722 + 50.0 * 6.326907634735107
Epoch 470, val loss: 1.2408456802368164
Epoch 480, training loss: 317.0877380371094 = 1.1023664474487305 + 50.0 * 6.31970739364624
Epoch 480, val loss: 1.2261724472045898
Epoch 490, training loss: 316.908447265625 = 1.083204984664917 + 50.0 * 6.316505432128906
Epoch 490, val loss: 1.2121069431304932
Epoch 500, training loss: 316.80511474609375 = 1.064348578453064 + 50.0 * 6.314815044403076
Epoch 500, val loss: 1.198257565498352
Epoch 510, training loss: 316.79449462890625 = 1.0456511974334717 + 50.0 * 6.314976692199707
Epoch 510, val loss: 1.1845451593399048
Epoch 520, training loss: 316.58197021484375 = 1.0272270441055298 + 50.0 * 6.311094760894775
Epoch 520, val loss: 1.1709799766540527
Epoch 530, training loss: 316.42864990234375 = 1.0089279413223267 + 50.0 * 6.308393955230713
Epoch 530, val loss: 1.1577256917953491
Epoch 540, training loss: 316.49798583984375 = 0.9908491373062134 + 50.0 * 6.310142993927002
Epoch 540, val loss: 1.1445187330245972
Epoch 550, training loss: 316.228515625 = 0.9729219675064087 + 50.0 * 6.305111408233643
Epoch 550, val loss: 1.13164484500885
Epoch 560, training loss: 316.0837707519531 = 0.9551343321800232 + 50.0 * 6.302572727203369
Epoch 560, val loss: 1.1188490390777588
Epoch 570, training loss: 316.2060852050781 = 0.9373700618743896 + 50.0 * 6.3053741455078125
Epoch 570, val loss: 1.1060309410095215
Epoch 580, training loss: 315.9228210449219 = 0.9198470115661621 + 50.0 * 6.3000593185424805
Epoch 580, val loss: 1.093461513519287
Epoch 590, training loss: 315.7640075683594 = 0.9024497270584106 + 50.0 * 6.297231197357178
Epoch 590, val loss: 1.081107497215271
Epoch 600, training loss: 315.79473876953125 = 0.8852998614311218 + 50.0 * 6.298188209533691
Epoch 600, val loss: 1.0688000917434692
Epoch 610, training loss: 315.7472229003906 = 0.8680621981620789 + 50.0 * 6.297583103179932
Epoch 610, val loss: 1.0566205978393555
Epoch 620, training loss: 315.47015380859375 = 0.8510642051696777 + 50.0 * 6.292381763458252
Epoch 620, val loss: 1.0445032119750977
Epoch 630, training loss: 315.3966979980469 = 0.8342309594154358 + 50.0 * 6.2912492752075195
Epoch 630, val loss: 1.0325334072113037
Epoch 640, training loss: 315.32977294921875 = 0.8175117373466492 + 50.0 * 6.290245056152344
Epoch 640, val loss: 1.0207443237304688
Epoch 650, training loss: 315.2247314453125 = 0.8007537126541138 + 50.0 * 6.288479804992676
Epoch 650, val loss: 1.0090107917785645
Epoch 660, training loss: 315.1986999511719 = 0.7841042280197144 + 50.0 * 6.2882914543151855
Epoch 660, val loss: 0.9972356557846069
Epoch 670, training loss: 315.113525390625 = 0.7676073908805847 + 50.0 * 6.2869181632995605
Epoch 670, val loss: 0.9855920672416687
Epoch 680, training loss: 315.02386474609375 = 0.7511935830116272 + 50.0 * 6.2854533195495605
Epoch 680, val loss: 0.9740964770317078
Epoch 690, training loss: 314.9390869140625 = 0.734870433807373 + 50.0 * 6.284084320068359
Epoch 690, val loss: 0.9627951383590698
Epoch 700, training loss: 314.9667053222656 = 0.7185774445533752 + 50.0 * 6.2849626541137695
Epoch 700, val loss: 0.9516125917434692
Epoch 710, training loss: 314.8178405761719 = 0.702268123626709 + 50.0 * 6.28231143951416
Epoch 710, val loss: 0.9401512145996094
Epoch 720, training loss: 314.6942138671875 = 0.6861719489097595 + 50.0 * 6.280161380767822
Epoch 720, val loss: 0.9293027520179749
Epoch 730, training loss: 314.62286376953125 = 0.6701328158378601 + 50.0 * 6.279054641723633
Epoch 730, val loss: 0.9182450175285339
Epoch 740, training loss: 314.7055969238281 = 0.654198944568634 + 50.0 * 6.281027793884277
Epoch 740, val loss: 0.907575249671936
Epoch 750, training loss: 314.4930419921875 = 0.6384376287460327 + 50.0 * 6.277091979980469
Epoch 750, val loss: 0.8967387080192566
Epoch 760, training loss: 314.39385986328125 = 0.6227349042892456 + 50.0 * 6.275422096252441
Epoch 760, val loss: 0.8863718509674072
Epoch 770, training loss: 314.4574890136719 = 0.6072494983673096 + 50.0 * 6.277005195617676
Epoch 770, val loss: 0.876069188117981
Epoch 780, training loss: 314.2588806152344 = 0.5918629765510559 + 50.0 * 6.273340702056885
Epoch 780, val loss: 0.8659021854400635
Epoch 790, training loss: 314.2386474609375 = 0.5767391920089722 + 50.0 * 6.273238182067871
Epoch 790, val loss: 0.8560425639152527
Epoch 800, training loss: 314.1481018066406 = 0.5617722868919373 + 50.0 * 6.271726608276367
Epoch 800, val loss: 0.8464894890785217
Epoch 810, training loss: 314.12762451171875 = 0.5470104813575745 + 50.0 * 6.271612644195557
Epoch 810, val loss: 0.8372122049331665
Epoch 820, training loss: 314.0210876464844 = 0.5324584245681763 + 50.0 * 6.269772052764893
Epoch 820, val loss: 0.8282799124717712
Epoch 830, training loss: 314.02398681640625 = 0.5181544423103333 + 50.0 * 6.270116806030273
Epoch 830, val loss: 0.8195251226425171
Epoch 840, training loss: 313.9548645019531 = 0.5041058659553528 + 50.0 * 6.269014835357666
Epoch 840, val loss: 0.8112857341766357
Epoch 850, training loss: 313.8314514160156 = 0.49031734466552734 + 50.0 * 6.266822338104248
Epoch 850, val loss: 0.8033685088157654
Epoch 860, training loss: 313.86181640625 = 0.4768410921096802 + 50.0 * 6.267699718475342
Epoch 860, val loss: 0.7957605719566345
Epoch 870, training loss: 313.9495849609375 = 0.4636036157608032 + 50.0 * 6.269720077514648
Epoch 870, val loss: 0.7882923483848572
Epoch 880, training loss: 313.7994079589844 = 0.4504329562187195 + 50.0 * 6.266979217529297
Epoch 880, val loss: 0.7813957929611206
Epoch 890, training loss: 313.6090087890625 = 0.4376877248287201 + 50.0 * 6.263426303863525
Epoch 890, val loss: 0.7747290134429932
Epoch 900, training loss: 313.529296875 = 0.4252060353755951 + 50.0 * 6.262081623077393
Epoch 900, val loss: 0.7685748338699341
Epoch 910, training loss: 313.5864562988281 = 0.41304728388786316 + 50.0 * 6.263468265533447
Epoch 910, val loss: 0.7628926634788513
Epoch 920, training loss: 313.4932556152344 = 0.40113282203674316 + 50.0 * 6.261842727661133
Epoch 920, val loss: 0.7573679685592651
Epoch 930, training loss: 313.4017333984375 = 0.3894444704055786 + 50.0 * 6.2602458000183105
Epoch 930, val loss: 0.7523499131202698
Epoch 940, training loss: 313.4123229980469 = 0.37809979915618896 + 50.0 * 6.260684490203857
Epoch 940, val loss: 0.7478439807891846
Epoch 950, training loss: 313.42626953125 = 0.3670231103897095 + 50.0 * 6.2611846923828125
Epoch 950, val loss: 0.7434861660003662
Epoch 960, training loss: 313.32281494140625 = 0.35615774989128113 + 50.0 * 6.259332656860352
Epoch 960, val loss: 0.7396350502967834
Epoch 970, training loss: 313.2779235839844 = 0.3456321954727173 + 50.0 * 6.258645534515381
Epoch 970, val loss: 0.7361134886741638
Epoch 980, training loss: 313.21710205078125 = 0.33540695905685425 + 50.0 * 6.257633686065674
Epoch 980, val loss: 0.7330577373504639
Epoch 990, training loss: 313.28155517578125 = 0.32540273666381836 + 50.0 * 6.259122848510742
Epoch 990, val loss: 0.73031085729599
Epoch 1000, training loss: 313.15655517578125 = 0.3156943619251251 + 50.0 * 6.256816864013672
Epoch 1000, val loss: 0.7279707789421082
Epoch 1010, training loss: 313.1146545410156 = 0.3062710464000702 + 50.0 * 6.256167411804199
Epoch 1010, val loss: 0.7259489297866821
Epoch 1020, training loss: 313.0805358886719 = 0.29709312319755554 + 50.0 * 6.255668640136719
Epoch 1020, val loss: 0.7242129445075989
Epoch 1030, training loss: 312.93017578125 = 0.2881821393966675 + 50.0 * 6.252840042114258
Epoch 1030, val loss: 0.722934901714325
Epoch 1040, training loss: 312.8701171875 = 0.27959689497947693 + 50.0 * 6.251810073852539
Epoch 1040, val loss: 0.7220087051391602
Epoch 1050, training loss: 312.8470458984375 = 0.27132466435432434 + 50.0 * 6.251514434814453
Epoch 1050, val loss: 0.7213611006736755
Epoch 1060, training loss: 313.36358642578125 = 0.26336759328842163 + 50.0 * 6.262004375457764
Epoch 1060, val loss: 0.7208553552627563
Epoch 1070, training loss: 312.82781982421875 = 0.2553550601005554 + 50.0 * 6.2514495849609375
Epoch 1070, val loss: 0.720725417137146
Epoch 1080, training loss: 312.7501525878906 = 0.2477698177099228 + 50.0 * 6.25004768371582
Epoch 1080, val loss: 0.7209725379943848
Epoch 1090, training loss: 312.69525146484375 = 0.24053560197353363 + 50.0 * 6.249094009399414
Epoch 1090, val loss: 0.7213989496231079
Epoch 1100, training loss: 312.8205261230469 = 0.2334877848625183 + 50.0 * 6.2517409324646
Epoch 1100, val loss: 0.7221393585205078
Epoch 1110, training loss: 312.9678039550781 = 0.22660262882709503 + 50.0 * 6.254824161529541
Epoch 1110, val loss: 0.7230141758918762
Epoch 1120, training loss: 312.6056213378906 = 0.21989020705223083 + 50.0 * 6.247714996337891
Epoch 1120, val loss: 0.7240540385246277
Epoch 1130, training loss: 312.5744934082031 = 0.21351227164268494 + 50.0 * 6.247220039367676
Epoch 1130, val loss: 0.7253946661949158
Epoch 1140, training loss: 312.5938415527344 = 0.207391619682312 + 50.0 * 6.2477288246154785
Epoch 1140, val loss: 0.7270292639732361
Epoch 1150, training loss: 312.5807800292969 = 0.20145057141780853 + 50.0 * 6.247586727142334
Epoch 1150, val loss: 0.7287885546684265
Epoch 1160, training loss: 312.55548095703125 = 0.19572219252586365 + 50.0 * 6.247195720672607
Epoch 1160, val loss: 0.730867862701416
Epoch 1170, training loss: 312.4504089355469 = 0.19014182686805725 + 50.0 * 6.245204925537109
Epoch 1170, val loss: 0.7328447699546814
Epoch 1180, training loss: 312.36761474609375 = 0.18477372825145721 + 50.0 * 6.243656635284424
Epoch 1180, val loss: 0.7351376414299011
Epoch 1190, training loss: 312.3787841796875 = 0.17963184416294098 + 50.0 * 6.243983268737793
Epoch 1190, val loss: 0.7375075817108154
Epoch 1200, training loss: 312.7897644042969 = 0.17463158071041107 + 50.0 * 6.252302646636963
Epoch 1200, val loss: 0.73978191614151
Epoch 1210, training loss: 312.40350341796875 = 0.1697467714548111 + 50.0 * 6.244675159454346
Epoch 1210, val loss: 0.7423062920570374
Epoch 1220, training loss: 312.2708740234375 = 0.16505785286426544 + 50.0 * 6.2421159744262695
Epoch 1220, val loss: 0.7450584769248962
Epoch 1230, training loss: 312.24090576171875 = 0.160591721534729 + 50.0 * 6.24160623550415
Epoch 1230, val loss: 0.7478227615356445
Epoch 1240, training loss: 312.4703063964844 = 0.15630213916301727 + 50.0 * 6.246279716491699
Epoch 1240, val loss: 0.7506731748580933
Epoch 1250, training loss: 312.26776123046875 = 0.15202659368515015 + 50.0 * 6.242314338684082
Epoch 1250, val loss: 0.7537102699279785
Epoch 1260, training loss: 312.2012023925781 = 0.1479637771844864 + 50.0 * 6.24106502532959
Epoch 1260, val loss: 0.7565814256668091
Epoch 1270, training loss: 312.1431884765625 = 0.1440441906452179 + 50.0 * 6.239983081817627
Epoch 1270, val loss: 0.7598748207092285
Epoch 1280, training loss: 312.4324035644531 = 0.1402980089187622 + 50.0 * 6.245842456817627
Epoch 1280, val loss: 0.7630177140235901
Epoch 1290, training loss: 312.126708984375 = 0.1365591287612915 + 50.0 * 6.239802837371826
Epoch 1290, val loss: 0.7660382390022278
Epoch 1300, training loss: 312.04254150390625 = 0.13301602005958557 + 50.0 * 6.238190174102783
Epoch 1300, val loss: 0.7694117426872253
Epoch 1310, training loss: 312.2425231933594 = 0.12961237132549286 + 50.0 * 6.242258548736572
Epoch 1310, val loss: 0.772826611995697
Epoch 1320, training loss: 312.02899169921875 = 0.12624387443065643 + 50.0 * 6.2380547523498535
Epoch 1320, val loss: 0.7760435342788696
Epoch 1330, training loss: 311.9926452636719 = 0.12301432341337204 + 50.0 * 6.237392425537109
Epoch 1330, val loss: 0.77951580286026
Epoch 1340, training loss: 311.9648132324219 = 0.11990735679864883 + 50.0 * 6.236897945404053
Epoch 1340, val loss: 0.783016562461853
Epoch 1350, training loss: 312.1655578613281 = 0.11692238599061966 + 50.0 * 6.240972995758057
Epoch 1350, val loss: 0.7864621877670288
Epoch 1360, training loss: 311.94000244140625 = 0.1139507070183754 + 50.0 * 6.236521244049072
Epoch 1360, val loss: 0.7900736331939697
Epoch 1370, training loss: 312.0414733886719 = 0.11111704260110855 + 50.0 * 6.238606929779053
Epoch 1370, val loss: 0.7936328053474426
Epoch 1380, training loss: 311.85040283203125 = 0.10837279260158539 + 50.0 * 6.2348408699035645
Epoch 1380, val loss: 0.7973262071609497
Epoch 1390, training loss: 311.9636535644531 = 0.10573068261146545 + 50.0 * 6.237158298492432
Epoch 1390, val loss: 0.8011653423309326
Epoch 1400, training loss: 311.9246826171875 = 0.10312459617853165 + 50.0 * 6.236431121826172
Epoch 1400, val loss: 0.8046286106109619
Epoch 1410, training loss: 311.8121337890625 = 0.10061302781105042 + 50.0 * 6.2342305183410645
Epoch 1410, val loss: 0.8085631132125854
Epoch 1420, training loss: 311.7707214355469 = 0.09821717441082001 + 50.0 * 6.233450412750244
Epoch 1420, val loss: 0.8123857378959656
Epoch 1430, training loss: 311.9039306640625 = 0.09588833898305893 + 50.0 * 6.236160755157471
Epoch 1430, val loss: 0.8162244558334351
Epoch 1440, training loss: 311.80340576171875 = 0.09359532594680786 + 50.0 * 6.234196186065674
Epoch 1440, val loss: 0.8198832273483276
Epoch 1450, training loss: 311.8079528808594 = 0.09136352688074112 + 50.0 * 6.2343316078186035
Epoch 1450, val loss: 0.8237847685813904
Epoch 1460, training loss: 311.74407958984375 = 0.0892125815153122 + 50.0 * 6.233097553253174
Epoch 1460, val loss: 0.8274729251861572
Epoch 1470, training loss: 311.70379638671875 = 0.08714132010936737 + 50.0 * 6.232333183288574
Epoch 1470, val loss: 0.8313701748847961
Epoch 1480, training loss: 311.7270812988281 = 0.08514819294214249 + 50.0 * 6.2328386306762695
Epoch 1480, val loss: 0.8353536128997803
Epoch 1490, training loss: 311.7442321777344 = 0.08319135755300522 + 50.0 * 6.233221054077148
Epoch 1490, val loss: 0.8393533229827881
Epoch 1500, training loss: 311.6344299316406 = 0.08128458261489868 + 50.0 * 6.231062412261963
Epoch 1500, val loss: 0.8429955244064331
Epoch 1510, training loss: 311.6256103515625 = 0.07944390177726746 + 50.0 * 6.230923175811768
Epoch 1510, val loss: 0.8469242453575134
Epoch 1520, training loss: 311.7705383300781 = 0.07766986638307571 + 50.0 * 6.233857154846191
Epoch 1520, val loss: 0.8506612181663513
Epoch 1530, training loss: 311.726806640625 = 0.07592416554689407 + 50.0 * 6.233017444610596
Epoch 1530, val loss: 0.8545410633087158
Epoch 1540, training loss: 311.53875732421875 = 0.07421254366636276 + 50.0 * 6.229290962219238
Epoch 1540, val loss: 0.8582988381385803
Epoch 1550, training loss: 311.59033203125 = 0.07257979363203049 + 50.0 * 6.230355262756348
Epoch 1550, val loss: 0.8622733950614929
Epoch 1560, training loss: 311.6331481933594 = 0.07099337875843048 + 50.0 * 6.231243133544922
Epoch 1560, val loss: 0.865932285785675
Epoch 1570, training loss: 311.5197448730469 = 0.06944922357797623 + 50.0 * 6.229005813598633
Epoch 1570, val loss: 0.8701100945472717
Epoch 1580, training loss: 311.4529113769531 = 0.06795325875282288 + 50.0 * 6.227699279785156
Epoch 1580, val loss: 0.8740250468254089
Epoch 1590, training loss: 311.703125 = 0.06652221083641052 + 50.0 * 6.232731819152832
Epoch 1590, val loss: 0.877869188785553
Epoch 1600, training loss: 311.5413818359375 = 0.06506909430027008 + 50.0 * 6.229526519775391
Epoch 1600, val loss: 0.8815036416053772
Epoch 1610, training loss: 311.46002197265625 = 0.06367570906877518 + 50.0 * 6.227926731109619
Epoch 1610, val loss: 0.8854677677154541
Epoch 1620, training loss: 311.4839172363281 = 0.06234276667237282 + 50.0 * 6.228431701660156
Epoch 1620, val loss: 0.8891940116882324
Epoch 1630, training loss: 311.52301025390625 = 0.06102685630321503 + 50.0 * 6.229239463806152
Epoch 1630, val loss: 0.8928900361061096
Epoch 1640, training loss: 311.369873046875 = 0.05974258854985237 + 50.0 * 6.226202487945557
Epoch 1640, val loss: 0.8967605233192444
Epoch 1650, training loss: 311.3477478027344 = 0.05850863829255104 + 50.0 * 6.225784778594971
Epoch 1650, val loss: 0.9006305932998657
Epoch 1660, training loss: 311.4650573730469 = 0.057321060448884964 + 50.0 * 6.22815465927124
Epoch 1660, val loss: 0.9043840765953064
Epoch 1670, training loss: 311.4015808105469 = 0.05615324154496193 + 50.0 * 6.2269086837768555
Epoch 1670, val loss: 0.9079622626304626
Epoch 1680, training loss: 311.34820556640625 = 0.05500137060880661 + 50.0 * 6.225864410400391
Epoch 1680, val loss: 0.9117100238800049
Epoch 1690, training loss: 311.29998779296875 = 0.05390238016843796 + 50.0 * 6.224922180175781
Epoch 1690, val loss: 0.915532112121582
Epoch 1700, training loss: 311.2760314941406 = 0.05283501744270325 + 50.0 * 6.224464416503906
Epoch 1700, val loss: 0.9193977117538452
Epoch 1710, training loss: 311.34515380859375 = 0.05180028825998306 + 50.0 * 6.22586727142334
Epoch 1710, val loss: 0.9230884313583374
Epoch 1720, training loss: 311.334228515625 = 0.05078338831663132 + 50.0 * 6.225668907165527
Epoch 1720, val loss: 0.9266077876091003
Epoch 1730, training loss: 311.48614501953125 = 0.049773745238780975 + 50.0 * 6.228727340698242
Epoch 1730, val loss: 0.9302754998207092
Epoch 1740, training loss: 311.37518310546875 = 0.04879547283053398 + 50.0 * 6.226527690887451
Epoch 1740, val loss: 0.9337944984436035
Epoch 1750, training loss: 311.2264709472656 = 0.04784897342324257 + 50.0 * 6.223572254180908
Epoch 1750, val loss: 0.9379438757896423
Epoch 1760, training loss: 311.1816101074219 = 0.0469360314309597 + 50.0 * 6.22269344329834
Epoch 1760, val loss: 0.9416112303733826
Epoch 1770, training loss: 311.2556457519531 = 0.04605931043624878 + 50.0 * 6.224192142486572
Epoch 1770, val loss: 0.945464551448822
Epoch 1780, training loss: 311.2684326171875 = 0.04518436640501022 + 50.0 * 6.2244648933410645
Epoch 1780, val loss: 0.9489993453025818
Epoch 1790, training loss: 311.28546142578125 = 0.04433309659361839 + 50.0 * 6.224822521209717
Epoch 1790, val loss: 0.9521793127059937
Epoch 1800, training loss: 311.2397766113281 = 0.04349081963300705 + 50.0 * 6.223926067352295
Epoch 1800, val loss: 0.9559510350227356
Epoch 1810, training loss: 311.2406005859375 = 0.042678698897361755 + 50.0 * 6.223958492279053
Epoch 1810, val loss: 0.9593319296836853
Epoch 1820, training loss: 311.1451110839844 = 0.04188740998506546 + 50.0 * 6.22206449508667
Epoch 1820, val loss: 0.9631485342979431
Epoch 1830, training loss: 311.1845703125 = 0.041121624410152435 + 50.0 * 6.222869396209717
Epoch 1830, val loss: 0.9667595624923706
Epoch 1840, training loss: 311.1407775878906 = 0.04036856070160866 + 50.0 * 6.222008228302002
Epoch 1840, val loss: 0.9701871275901794
Epoch 1850, training loss: 311.16748046875 = 0.03965352103114128 + 50.0 * 6.2225565910339355
Epoch 1850, val loss: 0.9737436771392822
Epoch 1860, training loss: 311.1568908691406 = 0.0389350987970829 + 50.0 * 6.2223591804504395
Epoch 1860, val loss: 0.9773366451263428
Epoch 1870, training loss: 311.040283203125 = 0.03823934867978096 + 50.0 * 6.220040798187256
Epoch 1870, val loss: 0.9811850190162659
Epoch 1880, training loss: 311.1264343261719 = 0.03758039325475693 + 50.0 * 6.221777439117432
Epoch 1880, val loss: 0.9846723675727844
Epoch 1890, training loss: 311.1721496582031 = 0.03691429644823074 + 50.0 * 6.2227044105529785
Epoch 1890, val loss: 0.9881474375724792
Epoch 1900, training loss: 311.1351013183594 = 0.03624691814184189 + 50.0 * 6.2219767570495605
Epoch 1900, val loss: 0.9915065169334412
Epoch 1910, training loss: 311.125732421875 = 0.03562410920858383 + 50.0 * 6.221802234649658
Epoch 1910, val loss: 0.9951215982437134
Epoch 1920, training loss: 310.99066162109375 = 0.03499823436141014 + 50.0 * 6.219113349914551
Epoch 1920, val loss: 0.998476505279541
Epoch 1930, training loss: 311.0149841308594 = 0.034409262239933014 + 50.0 * 6.219611167907715
Epoch 1930, val loss: 1.0021352767944336
Epoch 1940, training loss: 311.1263122558594 = 0.03382475674152374 + 50.0 * 6.2218499183654785
Epoch 1940, val loss: 1.0056997537612915
Epoch 1950, training loss: 310.9955749511719 = 0.03324885666370392 + 50.0 * 6.219246864318848
Epoch 1950, val loss: 1.008812427520752
Epoch 1960, training loss: 310.97900390625 = 0.03268761187791824 + 50.0 * 6.218926429748535
Epoch 1960, val loss: 1.0122312307357788
Epoch 1970, training loss: 310.9388122558594 = 0.03214984014630318 + 50.0 * 6.218133449554443
Epoch 1970, val loss: 1.0157290697097778
Epoch 1980, training loss: 311.1617126464844 = 0.03162538260221481 + 50.0 * 6.222601890563965
Epoch 1980, val loss: 1.0192068815231323
Epoch 1990, training loss: 311.0422058105469 = 0.031087730079889297 + 50.0 * 6.220222473144531
Epoch 1990, val loss: 1.0223170518875122
Epoch 2000, training loss: 310.9405517578125 = 0.03057519719004631 + 50.0 * 6.218199729919434
Epoch 2000, val loss: 1.025507926940918
Epoch 2010, training loss: 310.8961181640625 = 0.030077537521719933 + 50.0 * 6.217320442199707
Epoch 2010, val loss: 1.0291987657546997
Epoch 2020, training loss: 311.2373352050781 = 0.029612088575959206 + 50.0 * 6.224154949188232
Epoch 2020, val loss: 1.0321826934814453
Epoch 2030, training loss: 311.0116882324219 = 0.0291074700653553 + 50.0 * 6.219651222229004
Epoch 2030, val loss: 1.0356310606002808
Epoch 2040, training loss: 310.8573913574219 = 0.02864418551325798 + 50.0 * 6.216574668884277
Epoch 2040, val loss: 1.0387630462646484
Epoch 2050, training loss: 310.81414794921875 = 0.028192615136504173 + 50.0 * 6.215719223022461
Epoch 2050, val loss: 1.042134165763855
Epoch 2060, training loss: 310.92608642578125 = 0.027764374390244484 + 50.0 * 6.217966079711914
Epoch 2060, val loss: 1.045190691947937
Epoch 2070, training loss: 310.84393310546875 = 0.027324052527546883 + 50.0 * 6.21633243560791
Epoch 2070, val loss: 1.0485692024230957
Epoch 2080, training loss: 311.1006774902344 = 0.026896493509411812 + 50.0 * 6.221476078033447
Epoch 2080, val loss: 1.0516891479492188
Epoch 2090, training loss: 310.80224609375 = 0.026473037898540497 + 50.0 * 6.215515613555908
Epoch 2090, val loss: 1.0551248788833618
Epoch 2100, training loss: 310.7763366699219 = 0.026064710691571236 + 50.0 * 6.215005397796631
Epoch 2100, val loss: 1.0584837198257446
Epoch 2110, training loss: 310.8079528808594 = 0.025675807148218155 + 50.0 * 6.215645790100098
Epoch 2110, val loss: 1.0615888833999634
Epoch 2120, training loss: 311.0110168457031 = 0.025284288451075554 + 50.0 * 6.219714164733887
Epoch 2120, val loss: 1.064752221107483
Epoch 2130, training loss: 310.836669921875 = 0.024909624829888344 + 50.0 * 6.216235637664795
Epoch 2130, val loss: 1.0679607391357422
Epoch 2140, training loss: 310.765625 = 0.02453017421066761 + 50.0 * 6.214821815490723
Epoch 2140, val loss: 1.071117639541626
Epoch 2150, training loss: 310.7413330078125 = 0.024175306782126427 + 50.0 * 6.214343070983887
Epoch 2150, val loss: 1.074284315109253
Epoch 2160, training loss: 310.8948059082031 = 0.023821745067834854 + 50.0 * 6.217419147491455
Epoch 2160, val loss: 1.077388048171997
Epoch 2170, training loss: 310.88568115234375 = 0.023471694439649582 + 50.0 * 6.2172441482543945
Epoch 2170, val loss: 1.0799888372421265
Epoch 2180, training loss: 310.7182922363281 = 0.02311919815838337 + 50.0 * 6.213902950286865
Epoch 2180, val loss: 1.0837584733963013
Epoch 2190, training loss: 310.7050476074219 = 0.02278641052544117 + 50.0 * 6.213644981384277
Epoch 2190, val loss: 1.086472749710083
Epoch 2200, training loss: 310.65728759765625 = 0.02245933935046196 + 50.0 * 6.212696552276611
Epoch 2200, val loss: 1.0896700620651245
Epoch 2210, training loss: 310.7538757324219 = 0.022146644070744514 + 50.0 * 6.214634418487549
Epoch 2210, val loss: 1.092524766921997
Epoch 2220, training loss: 310.7688293457031 = 0.02183033712208271 + 50.0 * 6.214940071105957
Epoch 2220, val loss: 1.0955113172531128
Epoch 2230, training loss: 310.699951171875 = 0.021512692794203758 + 50.0 * 6.213568687438965
Epoch 2230, val loss: 1.0987796783447266
Epoch 2240, training loss: 310.6597595214844 = 0.0212156530469656 + 50.0 * 6.212770938873291
Epoch 2240, val loss: 1.101535439491272
Epoch 2250, training loss: 310.6569519042969 = 0.020920882001519203 + 50.0 * 6.2127203941345215
Epoch 2250, val loss: 1.1044859886169434
Epoch 2260, training loss: 310.9332275390625 = 0.02063532918691635 + 50.0 * 6.218251705169678
Epoch 2260, val loss: 1.1073697805404663
Epoch 2270, training loss: 310.7026672363281 = 0.020353589206933975 + 50.0 * 6.213646411895752
Epoch 2270, val loss: 1.1104726791381836
Epoch 2280, training loss: 310.6919860839844 = 0.020061826333403587 + 50.0 * 6.213438510894775
Epoch 2280, val loss: 1.1132763624191284
Epoch 2290, training loss: 310.61517333984375 = 0.019795753061771393 + 50.0 * 6.211907386779785
Epoch 2290, val loss: 1.1162959337234497
Epoch 2300, training loss: 310.5543518066406 = 0.01953122764825821 + 50.0 * 6.210696697235107
Epoch 2300, val loss: 1.1191960573196411
Epoch 2310, training loss: 310.606689453125 = 0.01927574910223484 + 50.0 * 6.211748123168945
Epoch 2310, val loss: 1.1221132278442383
Epoch 2320, training loss: 310.74810791015625 = 0.01902798004448414 + 50.0 * 6.21458101272583
Epoch 2320, val loss: 1.1250519752502441
Epoch 2330, training loss: 310.6308898925781 = 0.018771041184663773 + 50.0 * 6.212242603302002
Epoch 2330, val loss: 1.1276164054870605
Epoch 2340, training loss: 310.57843017578125 = 0.01852189004421234 + 50.0 * 6.211198329925537
Epoch 2340, val loss: 1.130797028541565
Epoch 2350, training loss: 310.85333251953125 = 0.018289828673005104 + 50.0 * 6.216701030731201
Epoch 2350, val loss: 1.1335370540618896
Epoch 2360, training loss: 310.58819580078125 = 0.01804204098880291 + 50.0 * 6.2114033699035645
Epoch 2360, val loss: 1.1361955404281616
Epoch 2370, training loss: 310.51568603515625 = 0.017806582152843475 + 50.0 * 6.209958076477051
Epoch 2370, val loss: 1.1389461755752563
Epoch 2380, training loss: 310.4940490722656 = 0.017582718282938004 + 50.0 * 6.209529399871826
Epoch 2380, val loss: 1.1421618461608887
Epoch 2390, training loss: 310.8558349609375 = 0.01736750826239586 + 50.0 * 6.216769218444824
Epoch 2390, val loss: 1.1450750827789307
Epoch 2400, training loss: 310.6955261230469 = 0.01714450679719448 + 50.0 * 6.21356725692749
Epoch 2400, val loss: 1.1472028493881226
Epoch 2410, training loss: 310.5089111328125 = 0.01692027412354946 + 50.0 * 6.209840297698975
Epoch 2410, val loss: 1.1501699686050415
Epoch 2420, training loss: 310.4415588378906 = 0.016711261123418808 + 50.0 * 6.208496570587158
Epoch 2420, val loss: 1.1529216766357422
Epoch 2430, training loss: 310.45880126953125 = 0.01651180163025856 + 50.0 * 6.208845615386963
Epoch 2430, val loss: 1.1557962894439697
Epoch 2440, training loss: 310.67431640625 = 0.01631413772702217 + 50.0 * 6.213160037994385
Epoch 2440, val loss: 1.1582731008529663
Epoch 2450, training loss: 310.5181884765625 = 0.01610829494893551 + 50.0 * 6.210041522979736
Epoch 2450, val loss: 1.1610581874847412
Epoch 2460, training loss: 310.5358581542969 = 0.015911253169178963 + 50.0 * 6.210399150848389
Epoch 2460, val loss: 1.1632639169692993
Epoch 2470, training loss: 310.4487609863281 = 0.015718767419457436 + 50.0 * 6.208661079406738
Epoch 2470, val loss: 1.1663907766342163
Epoch 2480, training loss: 310.4498291015625 = 0.015531444922089577 + 50.0 * 6.208685874938965
Epoch 2480, val loss: 1.1692547798156738
Epoch 2490, training loss: 310.6437072753906 = 0.015348492190241814 + 50.0 * 6.212567329406738
Epoch 2490, val loss: 1.172040343284607
Epoch 2500, training loss: 310.4268798828125 = 0.015163656324148178 + 50.0 * 6.2082343101501465
Epoch 2500, val loss: 1.1742972135543823
Epoch 2510, training loss: 310.3779296875 = 0.014985835179686546 + 50.0 * 6.207259178161621
Epoch 2510, val loss: 1.1771667003631592
Epoch 2520, training loss: 310.49273681640625 = 0.014814167283475399 + 50.0 * 6.209558486938477
Epoch 2520, val loss: 1.1796575784683228
Epoch 2530, training loss: 310.5602722167969 = 0.014643268659710884 + 50.0 * 6.210912704467773
Epoch 2530, val loss: 1.1818879842758179
Epoch 2540, training loss: 310.3877258300781 = 0.014468914829194546 + 50.0 * 6.207465171813965
Epoch 2540, val loss: 1.185002088546753
Epoch 2550, training loss: 310.3988952636719 = 0.01430588774383068 + 50.0 * 6.207691669464111
Epoch 2550, val loss: 1.1875779628753662
Epoch 2560, training loss: 310.5438232421875 = 0.014149492606520653 + 50.0 * 6.210593223571777
Epoch 2560, val loss: 1.1899381875991821
Epoch 2570, training loss: 310.3673400878906 = 0.013981915079057217 + 50.0 * 6.207067012786865
Epoch 2570, val loss: 1.1927653551101685
Epoch 2580, training loss: 310.4012145996094 = 0.01382465846836567 + 50.0 * 6.207747936248779
Epoch 2580, val loss: 1.1954365968704224
Epoch 2590, training loss: 310.4002990722656 = 0.013674070127308369 + 50.0 * 6.207732677459717
Epoch 2590, val loss: 1.197655200958252
Epoch 2600, training loss: 310.43145751953125 = 0.013523105531930923 + 50.0 * 6.2083587646484375
Epoch 2600, val loss: 1.200284481048584
Epoch 2610, training loss: 310.35235595703125 = 0.013368850573897362 + 50.0 * 6.206779956817627
Epoch 2610, val loss: 1.2026747465133667
Epoch 2620, training loss: 310.418212890625 = 0.013226361013948917 + 50.0 * 6.208099842071533
Epoch 2620, val loss: 1.2053951025009155
Epoch 2630, training loss: 310.53009033203125 = 0.013082542456686497 + 50.0 * 6.2103400230407715
Epoch 2630, val loss: 1.2078378200531006
Epoch 2640, training loss: 310.3559875488281 = 0.012926959432661533 + 50.0 * 6.2068610191345215
Epoch 2640, val loss: 1.2097011804580688
Epoch 2650, training loss: 310.3869323730469 = 0.012788841500878334 + 50.0 * 6.207482814788818
Epoch 2650, val loss: 1.2125623226165771
Epoch 2660, training loss: 310.28900146484375 = 0.012652615085244179 + 50.0 * 6.205526828765869
Epoch 2660, val loss: 1.2148784399032593
Epoch 2670, training loss: 310.2388000488281 = 0.012519360519945621 + 50.0 * 6.204525470733643
Epoch 2670, val loss: 1.2174066305160522
Epoch 2680, training loss: 310.2763671875 = 0.012392890639603138 + 50.0 * 6.20527982711792
Epoch 2680, val loss: 1.2200579643249512
Epoch 2690, training loss: 310.6709289550781 = 0.012266954407095909 + 50.0 * 6.2131733894348145
Epoch 2690, val loss: 1.22221040725708
Epoch 2700, training loss: 310.4073791503906 = 0.012129807844758034 + 50.0 * 6.207905292510986
Epoch 2700, val loss: 1.2243902683258057
Epoch 2710, training loss: 310.3831787109375 = 0.012000838294625282 + 50.0 * 6.207423686981201
Epoch 2710, val loss: 1.226770043373108
Epoch 2720, training loss: 310.3201904296875 = 0.011875464580953121 + 50.0 * 6.2061662673950195
Epoch 2720, val loss: 1.228968858718872
Epoch 2730, training loss: 310.24798583984375 = 0.011754529550671577 + 50.0 * 6.2047247886657715
Epoch 2730, val loss: 1.2316151857376099
Epoch 2740, training loss: 310.2441101074219 = 0.011638226918876171 + 50.0 * 6.204649448394775
Epoch 2740, val loss: 1.2340033054351807
Epoch 2750, training loss: 310.5411376953125 = 0.011527519673109055 + 50.0 * 6.210591793060303
Epoch 2750, val loss: 1.236236333847046
Epoch 2760, training loss: 310.2967529296875 = 0.01139870472252369 + 50.0 * 6.20570707321167
Epoch 2760, val loss: 1.2384387254714966
Epoch 2770, training loss: 310.2104187011719 = 0.01128509920090437 + 50.0 * 6.203982353210449
Epoch 2770, val loss: 1.2405214309692383
Epoch 2780, training loss: 310.1875305175781 = 0.011172846890985966 + 50.0 * 6.203527450561523
Epoch 2780, val loss: 1.2430447340011597
Epoch 2790, training loss: 310.37432861328125 = 0.011068659834563732 + 50.0 * 6.2072649002075195
Epoch 2790, val loss: 1.2448168992996216
Epoch 2800, training loss: 310.2439270019531 = 0.010953308083117008 + 50.0 * 6.204659461975098
Epoch 2800, val loss: 1.247349739074707
Epoch 2810, training loss: 310.1698303222656 = 0.010844838805496693 + 50.0 * 6.203179836273193
Epoch 2810, val loss: 1.249802827835083
Epoch 2820, training loss: 310.1863708496094 = 0.010742876678705215 + 50.0 * 6.203512668609619
Epoch 2820, val loss: 1.2519152164459229
Epoch 2830, training loss: 310.3724365234375 = 0.010646666400134563 + 50.0 * 6.207235813140869
Epoch 2830, val loss: 1.254294753074646
Epoch 2840, training loss: 310.21112060546875 = 0.01053586881607771 + 50.0 * 6.204011917114258
Epoch 2840, val loss: 1.2561042308807373
Epoch 2850, training loss: 310.1383972167969 = 0.010433044284582138 + 50.0 * 6.202559471130371
Epoch 2850, val loss: 1.2583932876586914
Epoch 2860, training loss: 310.1585998535156 = 0.01033738162368536 + 50.0 * 6.202964782714844
Epoch 2860, val loss: 1.2605805397033691
Epoch 2870, training loss: 310.5361328125 = 0.010242307558655739 + 50.0 * 6.210517406463623
Epoch 2870, val loss: 1.2624861001968384
Epoch 2880, training loss: 310.27935791015625 = 0.010144817642867565 + 50.0 * 6.205384731292725
Epoch 2880, val loss: 1.264932632446289
Epoch 2890, training loss: 310.1556091308594 = 0.010047136805951595 + 50.0 * 6.202911376953125
Epoch 2890, val loss: 1.2668004035949707
Epoch 2900, training loss: 310.1390075683594 = 0.009958118200302124 + 50.0 * 6.20258092880249
Epoch 2900, val loss: 1.269191026687622
Epoch 2910, training loss: 310.4047546386719 = 0.009870978072285652 + 50.0 * 6.207898139953613
Epoch 2910, val loss: 1.2707823514938354
Epoch 2920, training loss: 310.16082763671875 = 0.009775212965905666 + 50.0 * 6.20302152633667
Epoch 2920, val loss: 1.273380994796753
Epoch 2930, training loss: 310.1219787597656 = 0.009684556163847446 + 50.0 * 6.202246189117432
Epoch 2930, val loss: 1.275264024734497
Epoch 2940, training loss: 310.1504821777344 = 0.009601522237062454 + 50.0 * 6.202817440032959
Epoch 2940, val loss: 1.2773686647415161
Epoch 2950, training loss: 310.4252014160156 = 0.009519963525235653 + 50.0 * 6.208313465118408
Epoch 2950, val loss: 1.2792595624923706
Epoch 2960, training loss: 310.1806945800781 = 0.009426821954548359 + 50.0 * 6.203425407409668
Epoch 2960, val loss: 1.2814128398895264
Epoch 2970, training loss: 310.1114501953125 = 0.009341930970549583 + 50.0 * 6.202042102813721
Epoch 2970, val loss: 1.2832154035568237
Epoch 2980, training loss: 310.1499938964844 = 0.009263060055673122 + 50.0 * 6.20281457901001
Epoch 2980, val loss: 1.2851910591125488
Epoch 2990, training loss: 310.2885437011719 = 0.009181185625493526 + 50.0 * 6.205587387084961
Epoch 2990, val loss: 1.2870361804962158
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 431.7709655761719 = 1.9308844804763794 + 50.0 * 8.5968017578125
Epoch 0, val loss: 1.92868173122406
Epoch 10, training loss: 431.7082214355469 = 1.9225537776947021 + 50.0 * 8.59571361541748
Epoch 10, val loss: 1.9199130535125732
Epoch 20, training loss: 431.3042297363281 = 1.9122567176818848 + 50.0 * 8.587839126586914
Epoch 20, val loss: 1.9089834690093994
Epoch 30, training loss: 428.52203369140625 = 1.8995507955551147 + 50.0 * 8.532449722290039
Epoch 30, val loss: 1.895448923110962
Epoch 40, training loss: 409.1314697265625 = 1.8852421045303345 + 50.0 * 8.14492416381836
Epoch 40, val loss: 1.8803213834762573
Epoch 50, training loss: 377.50616455078125 = 1.8707093000411987 + 50.0 * 7.512709617614746
Epoch 50, val loss: 1.8661342859268188
Epoch 60, training loss: 360.30328369140625 = 1.8597198724746704 + 50.0 * 7.1688714027404785
Epoch 60, val loss: 1.8554952144622803
Epoch 70, training loss: 349.9910583496094 = 1.8479121923446655 + 50.0 * 6.962862968444824
Epoch 70, val loss: 1.8438165187835693
Epoch 80, training loss: 344.2397766113281 = 1.8380157947540283 + 50.0 * 6.8480353355407715
Epoch 80, val loss: 1.834244966506958
Epoch 90, training loss: 340.0218811035156 = 1.829843521118164 + 50.0 * 6.763841152191162
Epoch 90, val loss: 1.8266370296478271
Epoch 100, training loss: 336.54339599609375 = 1.8226608037948608 + 50.0 * 6.6944146156311035
Epoch 100, val loss: 1.8201076984405518
Epoch 110, training loss: 333.71630859375 = 1.8156837224960327 + 50.0 * 6.638012409210205
Epoch 110, val loss: 1.8136937618255615
Epoch 120, training loss: 331.6181945800781 = 1.8089067935943604 + 50.0 * 6.59618616104126
Epoch 120, val loss: 1.8074638843536377
Epoch 130, training loss: 329.8412780761719 = 1.8020858764648438 + 50.0 * 6.560783863067627
Epoch 130, val loss: 1.801056981086731
Epoch 140, training loss: 328.37677001953125 = 1.7951480150222778 + 50.0 * 6.531632900238037
Epoch 140, val loss: 1.7946107387542725
Epoch 150, training loss: 327.27667236328125 = 1.78769850730896 + 50.0 * 6.509779930114746
Epoch 150, val loss: 1.787756085395813
Epoch 160, training loss: 326.28900146484375 = 1.7798632383346558 + 50.0 * 6.490182399749756
Epoch 160, val loss: 1.7805250883102417
Epoch 170, training loss: 325.5985107421875 = 1.7715870141983032 + 50.0 * 6.47653865814209
Epoch 170, val loss: 1.7728639841079712
Epoch 180, training loss: 324.7426452636719 = 1.7626279592514038 + 50.0 * 6.45959997177124
Epoch 180, val loss: 1.7645498514175415
Epoch 190, training loss: 324.0090026855469 = 1.753027319908142 + 50.0 * 6.445119857788086
Epoch 190, val loss: 1.7557216882705688
Epoch 200, training loss: 323.41473388671875 = 1.7425527572631836 + 50.0 * 6.433443546295166
Epoch 200, val loss: 1.7461717128753662
Epoch 210, training loss: 322.84283447265625 = 1.7312666177749634 + 50.0 * 6.422231674194336
Epoch 210, val loss: 1.7359868288040161
Epoch 220, training loss: 322.3266906738281 = 1.7190824747085571 + 50.0 * 6.41215181350708
Epoch 220, val loss: 1.7250481843948364
Epoch 230, training loss: 321.99078369140625 = 1.7056357860565186 + 50.0 * 6.405703067779541
Epoch 230, val loss: 1.7132381200790405
Epoch 240, training loss: 321.4312744140625 = 1.6912387609481812 + 50.0 * 6.394800186157227
Epoch 240, val loss: 1.7004854679107666
Epoch 250, training loss: 320.9924011230469 = 1.6756622791290283 + 50.0 * 6.3863348960876465
Epoch 250, val loss: 1.6868305206298828
Epoch 260, training loss: 320.62347412109375 = 1.6588506698608398 + 50.0 * 6.3792924880981445
Epoch 260, val loss: 1.6721458435058594
Epoch 270, training loss: 320.2389221191406 = 1.6407963037490845 + 50.0 * 6.371962070465088
Epoch 270, val loss: 1.6565074920654297
Epoch 280, training loss: 319.89520263671875 = 1.6215910911560059 + 50.0 * 6.365472316741943
Epoch 280, val loss: 1.639851689338684
Epoch 290, training loss: 319.83172607421875 = 1.6011390686035156 + 50.0 * 6.364611625671387
Epoch 290, val loss: 1.6222994327545166
Epoch 300, training loss: 319.3749694824219 = 1.5792804956436157 + 50.0 * 6.3559136390686035
Epoch 300, val loss: 1.603724479675293
Epoch 310, training loss: 319.09417724609375 = 1.5564860105514526 + 50.0 * 6.3507537841796875
Epoch 310, val loss: 1.584466814994812
Epoch 320, training loss: 318.8258972167969 = 1.532759428024292 + 50.0 * 6.345863342285156
Epoch 320, val loss: 1.5645123720169067
Epoch 330, training loss: 318.8642578125 = 1.5081496238708496 + 50.0 * 6.3471221923828125
Epoch 330, val loss: 1.543932557106018
Epoch 340, training loss: 318.46527099609375 = 1.4824692010879517 + 50.0 * 6.339655876159668
Epoch 340, val loss: 1.5229090452194214
Epoch 350, training loss: 318.1981201171875 = 1.4563343524932861 + 50.0 * 6.334835529327393
Epoch 350, val loss: 1.5015391111373901
Epoch 360, training loss: 318.02099609375 = 1.42979097366333 + 50.0 * 6.33182430267334
Epoch 360, val loss: 1.4800420999526978
Epoch 370, training loss: 317.8939208984375 = 1.4029210805892944 + 50.0 * 6.329819679260254
Epoch 370, val loss: 1.458534836769104
Epoch 380, training loss: 317.7521057128906 = 1.3757671117782593 + 50.0 * 6.327527046203613
Epoch 380, val loss: 1.4370133876800537
Epoch 390, training loss: 317.576904296875 = 1.3483015298843384 + 50.0 * 6.3245720863342285
Epoch 390, val loss: 1.415596842765808
Epoch 400, training loss: 317.28759765625 = 1.3210196495056152 + 50.0 * 6.319331645965576
Epoch 400, val loss: 1.3944411277770996
Epoch 410, training loss: 317.1447448730469 = 1.2939506769180298 + 50.0 * 6.317016124725342
Epoch 410, val loss: 1.3737319707870483
Epoch 420, training loss: 317.33990478515625 = 1.2669318914413452 + 50.0 * 6.3214592933654785
Epoch 420, val loss: 1.3533533811569214
Epoch 430, training loss: 316.9796447753906 = 1.2400743961334229 + 50.0 * 6.314791202545166
Epoch 430, val loss: 1.3332867622375488
Epoch 440, training loss: 316.7971496582031 = 1.2136245965957642 + 50.0 * 6.311670303344727
Epoch 440, val loss: 1.3137762546539307
Epoch 450, training loss: 316.68841552734375 = 1.1873868703842163 + 50.0 * 6.310020446777344
Epoch 450, val loss: 1.2946864366531372
Epoch 460, training loss: 316.46038818359375 = 1.161820888519287 + 50.0 * 6.305971145629883
Epoch 460, val loss: 1.2761284112930298
Epoch 470, training loss: 316.3288879394531 = 1.1365300416946411 + 50.0 * 6.303847312927246
Epoch 470, val loss: 1.2581393718719482
Epoch 480, training loss: 316.27886962890625 = 1.1115870475769043 + 50.0 * 6.303345680236816
Epoch 480, val loss: 1.2406989336013794
Epoch 490, training loss: 316.4317321777344 = 1.0868898630142212 + 50.0 * 6.306896686553955
Epoch 490, val loss: 1.2237460613250732
Epoch 500, training loss: 315.9566955566406 = 1.0627514123916626 + 50.0 * 6.297878742218018
Epoch 500, val loss: 1.2071181535720825
Epoch 510, training loss: 315.9033508300781 = 1.0391236543655396 + 50.0 * 6.2972846031188965
Epoch 510, val loss: 1.191081166267395
Epoch 520, training loss: 315.7536315917969 = 1.0156947374343872 + 50.0 * 6.2947587966918945
Epoch 520, val loss: 1.1755262613296509
Epoch 530, training loss: 315.6274719238281 = 0.9927402138710022 + 50.0 * 6.292694568634033
Epoch 530, val loss: 1.1604877710342407
Epoch 540, training loss: 315.5278625488281 = 0.9702438116073608 + 50.0 * 6.291152477264404
Epoch 540, val loss: 1.146116852760315
Epoch 550, training loss: 315.5379943847656 = 0.9482879042625427 + 50.0 * 6.291794300079346
Epoch 550, val loss: 1.132180094718933
Epoch 560, training loss: 315.4012451171875 = 0.9266067147254944 + 50.0 * 6.289493083953857
Epoch 560, val loss: 1.1186883449554443
Epoch 570, training loss: 315.2369689941406 = 0.9054335951805115 + 50.0 * 6.286631107330322
Epoch 570, val loss: 1.1055853366851807
Epoch 580, training loss: 315.3914489746094 = 0.88465416431427 + 50.0 * 6.290136337280273
Epoch 580, val loss: 1.093142032623291
Epoch 590, training loss: 315.26055908203125 = 0.8640173673629761 + 50.0 * 6.287930965423584
Epoch 590, val loss: 1.0809205770492554
Epoch 600, training loss: 315.019775390625 = 0.8440480828285217 + 50.0 * 6.283514499664307
Epoch 600, val loss: 1.0693358182907104
Epoch 610, training loss: 314.8724060058594 = 0.8245705366134644 + 50.0 * 6.280956268310547
Epoch 610, val loss: 1.0584574937820435
Epoch 620, training loss: 314.76611328125 = 0.8055983185768127 + 50.0 * 6.279210090637207
Epoch 620, val loss: 1.0481228828430176
Epoch 630, training loss: 315.575439453125 = 0.7871785163879395 + 50.0 * 6.295765399932861
Epoch 630, val loss: 1.0380630493164062
Epoch 640, training loss: 314.638916015625 = 0.7684649229049683 + 50.0 * 6.277409076690674
Epoch 640, val loss: 1.0284117460250854
Epoch 650, training loss: 314.55841064453125 = 0.7504667043685913 + 50.0 * 6.276159286499023
Epoch 650, val loss: 1.0193901062011719
Epoch 660, training loss: 314.47491455078125 = 0.7330788373947144 + 50.0 * 6.274836540222168
Epoch 660, val loss: 1.0110504627227783
Epoch 670, training loss: 314.38275146484375 = 0.7161474227905273 + 50.0 * 6.273332118988037
Epoch 670, val loss: 1.0031945705413818
Epoch 680, training loss: 314.4256591796875 = 0.699589729309082 + 50.0 * 6.274521350860596
Epoch 680, val loss: 0.9956732988357544
Epoch 690, training loss: 314.5179138183594 = 0.6830512285232544 + 50.0 * 6.276697158813477
Epoch 690, val loss: 0.9884079694747925
Epoch 700, training loss: 314.2319641113281 = 0.6670207381248474 + 50.0 * 6.271298885345459
Epoch 700, val loss: 0.9815680384635925
Epoch 710, training loss: 314.1154479980469 = 0.651445746421814 + 50.0 * 6.269279956817627
Epoch 710, val loss: 0.9753279089927673
Epoch 720, training loss: 314.0889892578125 = 0.6362574100494385 + 50.0 * 6.269054412841797
Epoch 720, val loss: 0.9695464968681335
Epoch 730, training loss: 314.03985595703125 = 0.6213231682777405 + 50.0 * 6.268370151519775
Epoch 730, val loss: 0.9639357924461365
Epoch 740, training loss: 314.0963439941406 = 0.6067426204681396 + 50.0 * 6.269791603088379
Epoch 740, val loss: 0.9586370587348938
Epoch 750, training loss: 313.8656921386719 = 0.592444896697998 + 50.0 * 6.265464782714844
Epoch 750, val loss: 0.9538241624832153
Epoch 760, training loss: 313.8290710449219 = 0.5786060690879822 + 50.0 * 6.265008926391602
Epoch 760, val loss: 0.9493919014930725
Epoch 770, training loss: 314.27655029296875 = 0.5651639699935913 + 50.0 * 6.274227619171143
Epoch 770, val loss: 0.9452496767044067
Epoch 780, training loss: 313.8802795410156 = 0.5515475869178772 + 50.0 * 6.266574859619141
Epoch 780, val loss: 0.9410660266876221
Epoch 790, training loss: 313.6982727050781 = 0.5385046601295471 + 50.0 * 6.263195037841797
Epoch 790, val loss: 0.9373332858085632
Epoch 800, training loss: 313.5723876953125 = 0.5259112119674683 + 50.0 * 6.260929584503174
Epoch 800, val loss: 0.9341384768486023
Epoch 810, training loss: 313.75714111328125 = 0.5136123895645142 + 50.0 * 6.264870643615723
Epoch 810, val loss: 0.9312060475349426
Epoch 820, training loss: 313.57763671875 = 0.5014600157737732 + 50.0 * 6.261523723602295
Epoch 820, val loss: 0.9283134937286377
Epoch 830, training loss: 313.5390625 = 0.48961004614830017 + 50.0 * 6.260988712310791
Epoch 830, val loss: 0.9257100820541382
Epoch 840, training loss: 313.47991943359375 = 0.47813135385513306 + 50.0 * 6.260035991668701
Epoch 840, val loss: 0.923401415348053
Epoch 850, training loss: 313.3723449707031 = 0.46700602769851685 + 50.0 * 6.258106708526611
Epoch 850, val loss: 0.921403169631958
Epoch 860, training loss: 313.2911682128906 = 0.4561769962310791 + 50.0 * 6.256700038909912
Epoch 860, val loss: 0.9196844100952148
Epoch 870, training loss: 313.36859130859375 = 0.44564950466156006 + 50.0 * 6.258459091186523
Epoch 870, val loss: 0.9181823134422302
Epoch 880, training loss: 313.1949157714844 = 0.4352109134197235 + 50.0 * 6.255194187164307
Epoch 880, val loss: 0.9167380332946777
Epoch 890, training loss: 313.158935546875 = 0.4251071512699127 + 50.0 * 6.254676342010498
Epoch 890, val loss: 0.9155402183532715
Epoch 900, training loss: 313.2017822265625 = 0.41539135575294495 + 50.0 * 6.255727291107178
Epoch 900, val loss: 0.9146458506584167
Epoch 910, training loss: 313.2979736328125 = 0.4057144522666931 + 50.0 * 6.257845401763916
Epoch 910, val loss: 0.9138277173042297
Epoch 920, training loss: 313.19195556640625 = 0.3962898254394531 + 50.0 * 6.255912780761719
Epoch 920, val loss: 0.9128910899162292
Epoch 930, training loss: 312.979248046875 = 0.38721850514411926 + 50.0 * 6.251840591430664
Epoch 930, val loss: 0.9125303030014038
Epoch 940, training loss: 312.9343566894531 = 0.3784787356853485 + 50.0 * 6.251117706298828
Epoch 940, val loss: 0.9123790264129639
Epoch 950, training loss: 312.8858337402344 = 0.36998796463012695 + 50.0 * 6.250316619873047
Epoch 950, val loss: 0.9123541712760925
Epoch 960, training loss: 313.42864990234375 = 0.3616240620613098 + 50.0 * 6.261340618133545
Epoch 960, val loss: 0.9123028516769409
Epoch 970, training loss: 313.0069274902344 = 0.3535105884075165 + 50.0 * 6.253067970275879
Epoch 970, val loss: 0.9124191403388977
Epoch 980, training loss: 312.77984619140625 = 0.34548628330230713 + 50.0 * 6.248687267303467
Epoch 980, val loss: 0.9127321839332581
Epoch 990, training loss: 312.9752197265625 = 0.33785971999168396 + 50.0 * 6.252747058868408
Epoch 990, val loss: 0.9133502244949341
Epoch 1000, training loss: 312.7886047363281 = 0.3303041160106659 + 50.0 * 6.249166011810303
Epoch 1000, val loss: 0.9137544631958008
Epoch 1010, training loss: 312.7178649902344 = 0.322964072227478 + 50.0 * 6.247898101806641
Epoch 1010, val loss: 0.9145506024360657
Epoch 1020, training loss: 312.6407165527344 = 0.3159256875514984 + 50.0 * 6.246496200561523
Epoch 1020, val loss: 0.9154635667800903
Epoch 1030, training loss: 313.0002136230469 = 0.3090301752090454 + 50.0 * 6.253823757171631
Epoch 1030, val loss: 0.9164323210716248
Epoch 1040, training loss: 312.6871643066406 = 0.3022156059741974 + 50.0 * 6.24769926071167
Epoch 1040, val loss: 0.9172738194465637
Epoch 1050, training loss: 312.6074523925781 = 0.29562219977378845 + 50.0 * 6.246236324310303
Epoch 1050, val loss: 0.9184015393257141
Epoch 1060, training loss: 312.5662841796875 = 0.28922170400619507 + 50.0 * 6.245541095733643
Epoch 1060, val loss: 0.9196434617042542
Epoch 1070, training loss: 312.4703369140625 = 0.28293851017951965 + 50.0 * 6.243748188018799
Epoch 1070, val loss: 0.9209302067756653
Epoch 1080, training loss: 312.52392578125 = 0.2768650949001312 + 50.0 * 6.244941234588623
Epoch 1080, val loss: 0.9223681092262268
Epoch 1090, training loss: 312.4891052246094 = 0.2709314227104187 + 50.0 * 6.244363307952881
Epoch 1090, val loss: 0.9237973690032959
Epoch 1100, training loss: 312.51470947265625 = 0.2651136517524719 + 50.0 * 6.244991779327393
Epoch 1100, val loss: 0.9250826239585876
Epoch 1110, training loss: 312.4222106933594 = 0.25938680768013 + 50.0 * 6.243256092071533
Epoch 1110, val loss: 0.9266879558563232
Epoch 1120, training loss: 312.3788757324219 = 0.2538612484931946 + 50.0 * 6.242499828338623
Epoch 1120, val loss: 0.9282811880111694
Epoch 1130, training loss: 312.3111877441406 = 0.248510479927063 + 50.0 * 6.241253852844238
Epoch 1130, val loss: 0.9299469590187073
Epoch 1140, training loss: 312.3955383300781 = 0.24328996241092682 + 50.0 * 6.243044853210449
Epoch 1140, val loss: 0.9316844940185547
Epoch 1150, training loss: 312.2535095214844 = 0.2380705624818802 + 50.0 * 6.24030876159668
Epoch 1150, val loss: 0.933286190032959
Epoch 1160, training loss: 312.2088928222656 = 0.23303329944610596 + 50.0 * 6.2395172119140625
Epoch 1160, val loss: 0.9351723194122314
Epoch 1170, training loss: 312.203857421875 = 0.22820252180099487 + 50.0 * 6.239512920379639
Epoch 1170, val loss: 0.9372091889381409
Epoch 1180, training loss: 312.3385009765625 = 0.22343425452709198 + 50.0 * 6.2423014640808105
Epoch 1180, val loss: 0.9392516016960144
Epoch 1190, training loss: 312.146240234375 = 0.21872828900814056 + 50.0 * 6.238550662994385
Epoch 1190, val loss: 0.9410430192947388
Epoch 1200, training loss: 312.1208801269531 = 0.21416828036308289 + 50.0 * 6.238133907318115
Epoch 1200, val loss: 0.943202793598175
Epoch 1210, training loss: 312.17303466796875 = 0.20970670878887177 + 50.0 * 6.239266395568848
Epoch 1210, val loss: 0.9453909397125244
Epoch 1220, training loss: 312.0886535644531 = 0.2053694874048233 + 50.0 * 6.237665176391602
Epoch 1220, val loss: 0.9475616812705994
Epoch 1230, training loss: 312.3389892578125 = 0.20110869407653809 + 50.0 * 6.242757320404053
Epoch 1230, val loss: 0.949714720249176
Epoch 1240, training loss: 311.9955139160156 = 0.19685164093971252 + 50.0 * 6.235973358154297
Epoch 1240, val loss: 0.9516410827636719
Epoch 1250, training loss: 311.965087890625 = 0.19277885556221008 + 50.0 * 6.235446453094482
Epoch 1250, val loss: 0.9540590643882751
Epoch 1260, training loss: 311.9187927246094 = 0.18885891139507294 + 50.0 * 6.234598636627197
Epoch 1260, val loss: 0.9564390778541565
Epoch 1270, training loss: 311.9776611328125 = 0.18503525853157043 + 50.0 * 6.2358527183532715
Epoch 1270, val loss: 0.958884596824646
Epoch 1280, training loss: 312.0728759765625 = 0.18119552731513977 + 50.0 * 6.2378339767456055
Epoch 1280, val loss: 0.9610856771469116
Epoch 1290, training loss: 311.869384765625 = 0.17742617428302765 + 50.0 * 6.23383903503418
Epoch 1290, val loss: 0.9633691906929016
Epoch 1300, training loss: 311.864501953125 = 0.1737951636314392 + 50.0 * 6.233814239501953
Epoch 1300, val loss: 0.9658519625663757
Epoch 1310, training loss: 311.9185791015625 = 0.17027583718299866 + 50.0 * 6.234965801239014
Epoch 1310, val loss: 0.9683941602706909
Epoch 1320, training loss: 311.8785705566406 = 0.1668306291103363 + 50.0 * 6.234234809875488
Epoch 1320, val loss: 0.9709302186965942
Epoch 1330, training loss: 311.78643798828125 = 0.1634129136800766 + 50.0 * 6.2324604988098145
Epoch 1330, val loss: 0.9735941290855408
Epoch 1340, training loss: 311.9831848144531 = 0.16015268862247467 + 50.0 * 6.2364606857299805
Epoch 1340, val loss: 0.9763303399085999
Epoch 1350, training loss: 311.8522644042969 = 0.15680286288261414 + 50.0 * 6.2339091300964355
Epoch 1350, val loss: 0.978354811668396
Epoch 1360, training loss: 311.7854919433594 = 0.15356090664863586 + 50.0 * 6.232638835906982
Epoch 1360, val loss: 0.9811537861824036
Epoch 1370, training loss: 311.6837158203125 = 0.15049375593662262 + 50.0 * 6.2306647300720215
Epoch 1370, val loss: 0.9838114380836487
Epoch 1380, training loss: 311.6311950683594 = 0.14749833941459656 + 50.0 * 6.229674339294434
Epoch 1380, val loss: 0.9867413640022278
Epoch 1390, training loss: 311.8323974609375 = 0.14458774030208588 + 50.0 * 6.233756065368652
Epoch 1390, val loss: 0.9895190000534058
Epoch 1400, training loss: 311.6893005371094 = 0.1416364461183548 + 50.0 * 6.230953216552734
Epoch 1400, val loss: 0.9918997287750244
Epoch 1410, training loss: 311.6639404296875 = 0.1387360394001007 + 50.0 * 6.230504035949707
Epoch 1410, val loss: 0.9946682453155518
Epoch 1420, training loss: 311.652587890625 = 0.13595888018608093 + 50.0 * 6.230332851409912
Epoch 1420, val loss: 0.9973745942115784
Epoch 1430, training loss: 311.6461486816406 = 0.13323496282100677 + 50.0 * 6.230257987976074
Epoch 1430, val loss: 1.0000970363616943
Epoch 1440, training loss: 311.61572265625 = 0.13060133159160614 + 50.0 * 6.229701995849609
Epoch 1440, val loss: 1.0030279159545898
Epoch 1450, training loss: 311.86773681640625 = 0.1280038207769394 + 50.0 * 6.234794616699219
Epoch 1450, val loss: 1.0057098865509033
Epoch 1460, training loss: 311.5954895019531 = 0.1253633201122284 + 50.0 * 6.229402542114258
Epoch 1460, val loss: 1.008309006690979
Epoch 1470, training loss: 311.52032470703125 = 0.12291274964809418 + 50.0 * 6.22794771194458
Epoch 1470, val loss: 1.0114085674285889
Epoch 1480, training loss: 311.6062316894531 = 0.12047551572322845 + 50.0 * 6.229714870452881
Epoch 1480, val loss: 1.0143177509307861
Epoch 1490, training loss: 311.4730224609375 = 0.11809399724006653 + 50.0 * 6.22709846496582
Epoch 1490, val loss: 1.0170371532440186
Epoch 1500, training loss: 311.51959228515625 = 0.1157669872045517 + 50.0 * 6.228076934814453
Epoch 1500, val loss: 1.019905924797058
Epoch 1510, training loss: 311.43292236328125 = 0.11348377913236618 + 50.0 * 6.226388454437256
Epoch 1510, val loss: 1.0230594873428345
Epoch 1520, training loss: 311.3891906738281 = 0.11128301173448563 + 50.0 * 6.225557804107666
Epoch 1520, val loss: 1.0260086059570312
Epoch 1530, training loss: 311.6378173828125 = 0.10910768806934357 + 50.0 * 6.230574131011963
Epoch 1530, val loss: 1.028836727142334
Epoch 1540, training loss: 311.6534729003906 = 0.10694950073957443 + 50.0 * 6.230930328369141
Epoch 1540, val loss: 1.0318341255187988
Epoch 1550, training loss: 311.4139099121094 = 0.10483428835868835 + 50.0 * 6.226181507110596
Epoch 1550, val loss: 1.0347520112991333
Epoch 1560, training loss: 311.3532409667969 = 0.10279211401939392 + 50.0 * 6.225008964538574
Epoch 1560, val loss: 1.0381004810333252
Epoch 1570, training loss: 311.56597900390625 = 0.10082868486642838 + 50.0 * 6.229302883148193
Epoch 1570, val loss: 1.0411250591278076
Epoch 1580, training loss: 311.2916564941406 = 0.09883256256580353 + 50.0 * 6.223856449127197
Epoch 1580, val loss: 1.0441778898239136
Epoch 1590, training loss: 311.3265075683594 = 0.09691042453050613 + 50.0 * 6.2245917320251465
Epoch 1590, val loss: 1.0471947193145752
Epoch 1600, training loss: 311.2685241699219 = 0.09502915292978287 + 50.0 * 6.2234697341918945
Epoch 1600, val loss: 1.0502511262893677
Epoch 1610, training loss: 311.1886901855469 = 0.09322591125965118 + 50.0 * 6.221909046173096
Epoch 1610, val loss: 1.053573727607727
Epoch 1620, training loss: 311.2202453613281 = 0.09148036688566208 + 50.0 * 6.2225751876831055
Epoch 1620, val loss: 1.0567220449447632
Epoch 1630, training loss: 311.522705078125 = 0.08973924070596695 + 50.0 * 6.228659152984619
Epoch 1630, val loss: 1.0597306489944458
Epoch 1640, training loss: 311.27978515625 = 0.08797191083431244 + 50.0 * 6.2238359451293945
Epoch 1640, val loss: 1.062943696975708
Epoch 1650, training loss: 311.51580810546875 = 0.08628957718610764 + 50.0 * 6.22859001159668
Epoch 1650, val loss: 1.0659489631652832
Epoch 1660, training loss: 311.2775573730469 = 0.08460963517427444 + 50.0 * 6.2238593101501465
Epoch 1660, val loss: 1.0689175128936768
Epoch 1670, training loss: 311.1752014160156 = 0.08296509087085724 + 50.0 * 6.22184419631958
Epoch 1670, val loss: 1.072356939315796
Epoch 1680, training loss: 311.1145324707031 = 0.08142171055078506 + 50.0 * 6.2206621170043945
Epoch 1680, val loss: 1.0755314826965332
Epoch 1690, training loss: 311.081787109375 = 0.07990981638431549 + 50.0 * 6.220037937164307
Epoch 1690, val loss: 1.0790284872055054
Epoch 1700, training loss: 311.3236389160156 = 0.07844775915145874 + 50.0 * 6.224903583526611
Epoch 1700, val loss: 1.082180380821228
Epoch 1710, training loss: 311.0891418457031 = 0.07693209499120712 + 50.0 * 6.220244407653809
Epoch 1710, val loss: 1.0853805541992188
Epoch 1720, training loss: 311.1004333496094 = 0.07548442482948303 + 50.0 * 6.220499515533447
Epoch 1720, val loss: 1.0886791944503784
Epoch 1730, training loss: 311.16650390625 = 0.0740661472082138 + 50.0 * 6.221848487854004
Epoch 1730, val loss: 1.0918834209442139
Epoch 1740, training loss: 311.13665771484375 = 0.0726771429181099 + 50.0 * 6.221280097961426
Epoch 1740, val loss: 1.094855785369873
Epoch 1750, training loss: 311.25811767578125 = 0.07128643244504929 + 50.0 * 6.223736763000488
Epoch 1750, val loss: 1.0981686115264893
Epoch 1760, training loss: 311.0536193847656 = 0.06995903700590134 + 50.0 * 6.2196736335754395
Epoch 1760, val loss: 1.1014634370803833
Epoch 1770, training loss: 310.9671325683594 = 0.06866687536239624 + 50.0 * 6.2179694175720215
Epoch 1770, val loss: 1.1047747135162354
Epoch 1780, training loss: 310.93792724609375 = 0.06743863970041275 + 50.0 * 6.217409610748291
Epoch 1780, val loss: 1.1082770824432373
Epoch 1790, training loss: 310.99188232421875 = 0.06622935831546783 + 50.0 * 6.218513488769531
Epoch 1790, val loss: 1.1116474866867065
Epoch 1800, training loss: 311.2583923339844 = 0.06501280516386032 + 50.0 * 6.223867893218994
Epoch 1800, val loss: 1.1149135828018188
Epoch 1810, training loss: 310.99249267578125 = 0.06378981471061707 + 50.0 * 6.218574047088623
Epoch 1810, val loss: 1.1177833080291748
Epoch 1820, training loss: 310.88427734375 = 0.06261652708053589 + 50.0 * 6.216433048248291
Epoch 1820, val loss: 1.1211379766464233
Epoch 1830, training loss: 310.890380859375 = 0.06150215119123459 + 50.0 * 6.216577529907227
Epoch 1830, val loss: 1.1245360374450684
Epoch 1840, training loss: 311.017822265625 = 0.06041756272315979 + 50.0 * 6.2191481590271
Epoch 1840, val loss: 1.127690315246582
Epoch 1850, training loss: 310.945556640625 = 0.05931994691491127 + 50.0 * 6.217724800109863
Epoch 1850, val loss: 1.1308796405792236
Epoch 1860, training loss: 311.0497741699219 = 0.058253295719623566 + 50.0 * 6.219830513000488
Epoch 1860, val loss: 1.1342039108276367
Epoch 1870, training loss: 310.98388671875 = 0.057208139449357986 + 50.0 * 6.218533515930176
Epoch 1870, val loss: 1.1372500658035278
Epoch 1880, training loss: 310.8824462890625 = 0.056182846426963806 + 50.0 * 6.216525554656982
Epoch 1880, val loss: 1.1407512426376343
Epoch 1890, training loss: 310.85015869140625 = 0.05520156770944595 + 50.0 * 6.2158989906311035
Epoch 1890, val loss: 1.1441112756729126
Epoch 1900, training loss: 310.7936096191406 = 0.05423828214406967 + 50.0 * 6.214787483215332
Epoch 1900, val loss: 1.1472446918487549
Epoch 1910, training loss: 310.885498046875 = 0.05330539122223854 + 50.0 * 6.216643810272217
Epoch 1910, val loss: 1.1505961418151855
Epoch 1920, training loss: 311.00164794921875 = 0.052363622933626175 + 50.0 * 6.218985557556152
Epoch 1920, val loss: 1.1534091234207153
Epoch 1930, training loss: 310.76409912109375 = 0.051424816250801086 + 50.0 * 6.2142534255981445
Epoch 1930, val loss: 1.1565968990325928
Epoch 1940, training loss: 310.7393493652344 = 0.05053281411528587 + 50.0 * 6.213776111602783
Epoch 1940, val loss: 1.1599723100662231
Epoch 1950, training loss: 310.9212951660156 = 0.04968470707535744 + 50.0 * 6.217432022094727
Epoch 1950, val loss: 1.1632267236709595
Epoch 1960, training loss: 310.7861633300781 = 0.0488152876496315 + 50.0 * 6.214746475219727
Epoch 1960, val loss: 1.166390061378479
Epoch 1970, training loss: 310.7507629394531 = 0.04796585068106651 + 50.0 * 6.214056015014648
Epoch 1970, val loss: 1.1697382926940918
Epoch 1980, training loss: 310.7738037109375 = 0.047161318361759186 + 50.0 * 6.21453332901001
Epoch 1980, val loss: 1.1730924844741821
Epoch 1990, training loss: 310.7179870605469 = 0.04636463150382042 + 50.0 * 6.213432788848877
Epoch 1990, val loss: 1.1763137578964233
Epoch 2000, training loss: 310.6697998046875 = 0.045590370893478394 + 50.0 * 6.212483882904053
Epoch 2000, val loss: 1.1794354915618896
Epoch 2010, training loss: 310.7338562011719 = 0.04484402388334274 + 50.0 * 6.213780403137207
Epoch 2010, val loss: 1.182664394378662
Epoch 2020, training loss: 310.7962646484375 = 0.04408438503742218 + 50.0 * 6.215043544769287
Epoch 2020, val loss: 1.1855322122573853
Epoch 2030, training loss: 310.9160461425781 = 0.043323736637830734 + 50.0 * 6.217454433441162
Epoch 2030, val loss: 1.1885517835617065
Epoch 2040, training loss: 310.7629699707031 = 0.04259414225816727 + 50.0 * 6.214407444000244
Epoch 2040, val loss: 1.191636562347412
Epoch 2050, training loss: 310.6983642578125 = 0.04188179969787598 + 50.0 * 6.213129997253418
Epoch 2050, val loss: 1.195046305656433
Epoch 2060, training loss: 310.6255187988281 = 0.04119174927473068 + 50.0 * 6.211686611175537
Epoch 2060, val loss: 1.1982485055923462
Epoch 2070, training loss: 310.5745544433594 = 0.04053816571831703 + 50.0 * 6.2106804847717285
Epoch 2070, val loss: 1.2016820907592773
Epoch 2080, training loss: 310.81060791015625 = 0.039906106889247894 + 50.0 * 6.215414047241211
Epoch 2080, val loss: 1.2050515413284302
Epoch 2090, training loss: 310.58099365234375 = 0.03923119604587555 + 50.0 * 6.210834980010986
Epoch 2090, val loss: 1.2077149152755737
Epoch 2100, training loss: 310.6706848144531 = 0.038587771356105804 + 50.0 * 6.212642192840576
Epoch 2100, val loss: 1.2106704711914062
Epoch 2110, training loss: 310.6007080078125 = 0.03796718269586563 + 50.0 * 6.211254596710205
Epoch 2110, val loss: 1.2140024900436401
Epoch 2120, training loss: 310.5441589355469 = 0.03737816959619522 + 50.0 * 6.210135459899902
Epoch 2120, val loss: 1.2172493934631348
Epoch 2130, training loss: 310.7843322753906 = 0.03681173548102379 + 50.0 * 6.2149505615234375
Epoch 2130, val loss: 1.2204985618591309
Epoch 2140, training loss: 310.5185852050781 = 0.03620024770498276 + 50.0 * 6.2096476554870605
Epoch 2140, val loss: 1.2234323024749756
Epoch 2150, training loss: 310.5055236816406 = 0.03562283515930176 + 50.0 * 6.20939826965332
Epoch 2150, val loss: 1.2265264987945557
Epoch 2160, training loss: 310.50408935546875 = 0.03508435934782028 + 50.0 * 6.20937967300415
Epoch 2160, val loss: 1.229988694190979
Epoch 2170, training loss: 310.8006286621094 = 0.034562740474939346 + 50.0 * 6.215321063995361
Epoch 2170, val loss: 1.233283519744873
Epoch 2180, training loss: 310.612548828125 = 0.03400428593158722 + 50.0 * 6.211570739746094
Epoch 2180, val loss: 1.235723614692688
Epoch 2190, training loss: 310.5171813964844 = 0.03348859027028084 + 50.0 * 6.209673881530762
Epoch 2190, val loss: 1.2390283346176147
Epoch 2200, training loss: 310.5316162109375 = 0.03298293054103851 + 50.0 * 6.209972381591797
Epoch 2200, val loss: 1.242080569267273
Epoch 2210, training loss: 310.6318664550781 = 0.03249770775437355 + 50.0 * 6.211987495422363
Epoch 2210, val loss: 1.245194911956787
Epoch 2220, training loss: 310.4757995605469 = 0.03199199214577675 + 50.0 * 6.208876609802246
Epoch 2220, val loss: 1.2479925155639648
Epoch 2230, training loss: 310.6755065917969 = 0.03152693435549736 + 50.0 * 6.212879657745361
Epoch 2230, val loss: 1.2510935068130493
Epoch 2240, training loss: 310.5872802734375 = 0.031034624204039574 + 50.0 * 6.211124897003174
Epoch 2240, val loss: 1.2541143894195557
Epoch 2250, training loss: 310.4337463378906 = 0.03055083565413952 + 50.0 * 6.208064079284668
Epoch 2250, val loss: 1.2565428018569946
Epoch 2260, training loss: 310.392333984375 = 0.030110392719507217 + 50.0 * 6.207244396209717
Epoch 2260, val loss: 1.2599588632583618
Epoch 2270, training loss: 310.3733215332031 = 0.029687250033020973 + 50.0 * 6.206872463226318
Epoch 2270, val loss: 1.2630313634872437
Epoch 2280, training loss: 310.4719543457031 = 0.02927860990166664 + 50.0 * 6.208853721618652
Epoch 2280, val loss: 1.2660119533538818
Epoch 2290, training loss: 310.54132080078125 = 0.028852300718426704 + 50.0 * 6.210249900817871
Epoch 2290, val loss: 1.2687262296676636
Epoch 2300, training loss: 310.4914245605469 = 0.028413843363523483 + 50.0 * 6.209259986877441
Epoch 2300, val loss: 1.2715171575546265
Epoch 2310, training loss: 310.5039978027344 = 0.028013838455080986 + 50.0 * 6.209519386291504
Epoch 2310, val loss: 1.2744132280349731
Epoch 2320, training loss: 310.4872131347656 = 0.02759774588048458 + 50.0 * 6.209192752838135
Epoch 2320, val loss: 1.277313232421875
Epoch 2330, training loss: 310.38043212890625 = 0.02721088007092476 + 50.0 * 6.207064151763916
Epoch 2330, val loss: 1.280264139175415
Epoch 2340, training loss: 310.29510498046875 = 0.02682872675359249 + 50.0 * 6.2053656578063965
Epoch 2340, val loss: 1.2832461595535278
Epoch 2350, training loss: 310.3484802246094 = 0.026470685377717018 + 50.0 * 6.206440448760986
Epoch 2350, val loss: 1.2862828969955444
Epoch 2360, training loss: 310.6627197265625 = 0.0261133573949337 + 50.0 * 6.212731838226318
Epoch 2360, val loss: 1.2891017198562622
Epoch 2370, training loss: 310.4117126464844 = 0.025741510093212128 + 50.0 * 6.207719802856445
Epoch 2370, val loss: 1.2914239168167114
Epoch 2380, training loss: 310.34967041015625 = 0.025379687547683716 + 50.0 * 6.206485748291016
Epoch 2380, val loss: 1.2942276000976562
Epoch 2390, training loss: 310.58819580078125 = 0.02503695897758007 + 50.0 * 6.211263179779053
Epoch 2390, val loss: 1.2968591451644897
Epoch 2400, training loss: 310.3788146972656 = 0.024695103988051414 + 50.0 * 6.207082271575928
Epoch 2400, val loss: 1.2998580932617188
Epoch 2410, training loss: 310.2789611816406 = 0.024359090253710747 + 50.0 * 6.205092430114746
Epoch 2410, val loss: 1.3027464151382446
Epoch 2420, training loss: 310.363037109375 = 0.02403845079243183 + 50.0 * 6.206779956817627
Epoch 2420, val loss: 1.305565357208252
Epoch 2430, training loss: 310.3169250488281 = 0.023724650964140892 + 50.0 * 6.205863952636719
Epoch 2430, val loss: 1.3083146810531616
Epoch 2440, training loss: 310.3634948730469 = 0.0234141293913126 + 50.0 * 6.206801891326904
Epoch 2440, val loss: 1.3108210563659668
Epoch 2450, training loss: 310.4324645996094 = 0.023100227117538452 + 50.0 * 6.208187103271484
Epoch 2450, val loss: 1.3136935234069824
Epoch 2460, training loss: 310.3567810058594 = 0.02278413064777851 + 50.0 * 6.206679821014404
Epoch 2460, val loss: 1.3162702322006226
Epoch 2470, training loss: 310.2407531738281 = 0.022480279207229614 + 50.0 * 6.204365253448486
Epoch 2470, val loss: 1.3186203241348267
Epoch 2480, training loss: 310.18963623046875 = 0.022200359031558037 + 50.0 * 6.2033491134643555
Epoch 2480, val loss: 1.3216469287872314
Epoch 2490, training loss: 310.225830078125 = 0.021927017718553543 + 50.0 * 6.20407772064209
Epoch 2490, val loss: 1.3243138790130615
Epoch 2500, training loss: 310.4658203125 = 0.021657036617398262 + 50.0 * 6.208883285522461
Epoch 2500, val loss: 1.3266955614089966
Epoch 2510, training loss: 310.3236083984375 = 0.021363208070397377 + 50.0 * 6.206045150756836
Epoch 2510, val loss: 1.3291230201721191
Epoch 2520, training loss: 310.1962890625 = 0.02108362503349781 + 50.0 * 6.2035040855407715
Epoch 2520, val loss: 1.3319710493087769
Epoch 2530, training loss: 310.147705078125 = 0.020821109414100647 + 50.0 * 6.202538013458252
Epoch 2530, val loss: 1.3344839811325073
Epoch 2540, training loss: 310.5265197753906 = 0.02056984230875969 + 50.0 * 6.210119247436523
Epoch 2540, val loss: 1.3368040323257446
Epoch 2550, training loss: 310.2610778808594 = 0.020297957584261894 + 50.0 * 6.20481538772583
Epoch 2550, val loss: 1.339552402496338
Epoch 2560, training loss: 310.206298828125 = 0.020030401647090912 + 50.0 * 6.203725814819336
Epoch 2560, val loss: 1.3417600393295288
Epoch 2570, training loss: 310.1202697753906 = 0.0197890717536211 + 50.0 * 6.202009677886963
Epoch 2570, val loss: 1.3444918394088745
Epoch 2580, training loss: 310.0887451171875 = 0.019554106518626213 + 50.0 * 6.201383590698242
Epoch 2580, val loss: 1.347217082977295
Epoch 2590, training loss: 310.07763671875 = 0.01932988315820694 + 50.0 * 6.201166152954102
Epoch 2590, val loss: 1.349869728088379
Epoch 2600, training loss: 310.3641662597656 = 0.019117509946227074 + 50.0 * 6.2069010734558105
Epoch 2600, val loss: 1.3519046306610107
Epoch 2610, training loss: 310.2809143066406 = 0.0188691858202219 + 50.0 * 6.2052412033081055
Epoch 2610, val loss: 1.3548243045806885
Epoch 2620, training loss: 310.1859130859375 = 0.01862911321222782 + 50.0 * 6.203346252441406
Epoch 2620, val loss: 1.3566932678222656
Epoch 2630, training loss: 310.0906982421875 = 0.01840144395828247 + 50.0 * 6.201446056365967
Epoch 2630, val loss: 1.3595460653305054
Epoch 2640, training loss: 310.0395812988281 = 0.01819349266588688 + 50.0 * 6.200428009033203
Epoch 2640, val loss: 1.362047791481018
Epoch 2650, training loss: 310.1803894042969 = 0.01799248903989792 + 50.0 * 6.203247547149658
Epoch 2650, val loss: 1.3644440174102783
Epoch 2660, training loss: 310.17864990234375 = 0.01777581125497818 + 50.0 * 6.20321798324585
Epoch 2660, val loss: 1.3669131994247437
Epoch 2670, training loss: 310.1671447753906 = 0.017566192895174026 + 50.0 * 6.202991485595703
Epoch 2670, val loss: 1.3691788911819458
Epoch 2680, training loss: 310.0505065917969 = 0.017354203388094902 + 50.0 * 6.200663089752197
Epoch 2680, val loss: 1.3715020418167114
Epoch 2690, training loss: 310.05767822265625 = 0.017160754650831223 + 50.0 * 6.200810432434082
Epoch 2690, val loss: 1.3740391731262207
Epoch 2700, training loss: 310.22723388671875 = 0.01697065308690071 + 50.0 * 6.204205513000488
Epoch 2700, val loss: 1.3765212297439575
Epoch 2710, training loss: 310.0753173828125 = 0.016774408519268036 + 50.0 * 6.201170921325684
Epoch 2710, val loss: 1.3787190914154053
Epoch 2720, training loss: 310.04034423828125 = 0.016584210097789764 + 50.0 * 6.200475215911865
Epoch 2720, val loss: 1.381075143814087
Epoch 2730, training loss: 310.0459289550781 = 0.01640212908387184 + 50.0 * 6.20059061050415
Epoch 2730, val loss: 1.3834130764007568
Epoch 2740, training loss: 310.2091979980469 = 0.016224874183535576 + 50.0 * 6.203859329223633
Epoch 2740, val loss: 1.3855841159820557
Epoch 2750, training loss: 310.164306640625 = 0.01604151912033558 + 50.0 * 6.202964782714844
Epoch 2750, val loss: 1.3879728317260742
Epoch 2760, training loss: 310.0953369140625 = 0.015860509127378464 + 50.0 * 6.201589584350586
Epoch 2760, val loss: 1.390070915222168
Epoch 2770, training loss: 310.0292663574219 = 0.015686318278312683 + 50.0 * 6.2002716064453125
Epoch 2770, val loss: 1.3924015760421753
Epoch 2780, training loss: 310.11431884765625 = 0.0155217619612813 + 50.0 * 6.2019758224487305
Epoch 2780, val loss: 1.3947558403015137
Epoch 2790, training loss: 309.9656066894531 = 0.015346463769674301 + 50.0 * 6.199005126953125
Epoch 2790, val loss: 1.3969087600708008
Epoch 2800, training loss: 309.966064453125 = 0.015185478143393993 + 50.0 * 6.199017524719238
Epoch 2800, val loss: 1.3994213342666626
Epoch 2810, training loss: 310.1346130371094 = 0.015035722404718399 + 50.0 * 6.202391147613525
Epoch 2810, val loss: 1.401928424835205
Epoch 2820, training loss: 310.0332946777344 = 0.014868570491671562 + 50.0 * 6.200368881225586
Epoch 2820, val loss: 1.403540015220642
Epoch 2830, training loss: 310.00311279296875 = 0.014703062362968922 + 50.0 * 6.19976806640625
Epoch 2830, val loss: 1.4053912162780762
Epoch 2840, training loss: 309.9928894042969 = 0.014551478438079357 + 50.0 * 6.199566841125488
Epoch 2840, val loss: 1.407867670059204
Epoch 2850, training loss: 310.1368103027344 = 0.014406776987016201 + 50.0 * 6.20244836807251
Epoch 2850, val loss: 1.4099677801132202
Epoch 2860, training loss: 309.9807434082031 = 0.014249151572585106 + 50.0 * 6.199329853057861
Epoch 2860, val loss: 1.4122520685195923
Epoch 2870, training loss: 309.9207458496094 = 0.014099878259003162 + 50.0 * 6.1981329917907715
Epoch 2870, val loss: 1.4141547679901123
Epoch 2880, training loss: 310.0566101074219 = 0.0139639712870121 + 50.0 * 6.200852870941162
Epoch 2880, val loss: 1.4164409637451172
Epoch 2890, training loss: 310.0460205078125 = 0.01381620392203331 + 50.0 * 6.200644016265869
Epoch 2890, val loss: 1.418370008468628
Epoch 2900, training loss: 309.9426574707031 = 0.013668983243405819 + 50.0 * 6.198579788208008
Epoch 2900, val loss: 1.4203649759292603
Epoch 2910, training loss: 309.9283752441406 = 0.013533984310925007 + 50.0 * 6.198297023773193
Epoch 2910, val loss: 1.4227213859558105
Epoch 2920, training loss: 310.0517883300781 = 0.01340335700660944 + 50.0 * 6.200767993927002
Epoch 2920, val loss: 1.4247077703475952
Epoch 2930, training loss: 309.8667907714844 = 0.01326401811093092 + 50.0 * 6.197070598602295
Epoch 2930, val loss: 1.4267611503601074
Epoch 2940, training loss: 309.9787902832031 = 0.013135798275470734 + 50.0 * 6.199313640594482
Epoch 2940, val loss: 1.4289381504058838
Epoch 2950, training loss: 310.11773681640625 = 0.013007326982915401 + 50.0 * 6.202095031738281
Epoch 2950, val loss: 1.4309208393096924
Epoch 2960, training loss: 309.9684753417969 = 0.012874453328549862 + 50.0 * 6.1991119384765625
Epoch 2960, val loss: 1.4326775074005127
Epoch 2970, training loss: 309.8595275878906 = 0.012747415341436863 + 50.0 * 6.196935176849365
Epoch 2970, val loss: 1.4346975088119507
Epoch 2980, training loss: 309.84088134765625 = 0.012629211880266666 + 50.0 * 6.1965651512146
Epoch 2980, val loss: 1.436905026435852
Epoch 2990, training loss: 310.0965576171875 = 0.01251461822539568 + 50.0 * 6.201681137084961
Epoch 2990, val loss: 1.4382600784301758
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8460727464417502
The final CL Acc:0.72099, 0.01848, The final GNN Acc:0.84080, 0.00375
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11616])
remove edge: torch.Size([2, 9530])
updated graph: torch.Size([2, 10590])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7897644042969 = 1.9485630989074707 + 50.0 * 8.596823692321777
Epoch 0, val loss: 1.9409340620040894
Epoch 10, training loss: 431.738525390625 = 1.938822865486145 + 50.0 * 8.595993995666504
Epoch 10, val loss: 1.9309965372085571
Epoch 20, training loss: 431.44769287109375 = 1.9266091585159302 + 50.0 * 8.590421676635742
Epoch 20, val loss: 1.9185296297073364
Epoch 30, training loss: 429.75439453125 = 1.9105265140533447 + 50.0 * 8.556877136230469
Epoch 30, val loss: 1.902226209640503
Epoch 40, training loss: 421.12335205078125 = 1.8913706541061401 + 50.0 * 8.384639739990234
Epoch 40, val loss: 1.8837954998016357
Epoch 50, training loss: 394.1525573730469 = 1.8685858249664307 + 50.0 * 7.84567928314209
Epoch 50, val loss: 1.8623466491699219
Epoch 60, training loss: 377.05047607421875 = 1.8496066331863403 + 50.0 * 7.504017353057861
Epoch 60, val loss: 1.8464996814727783
Epoch 70, training loss: 365.58184814453125 = 1.839501976966858 + 50.0 * 7.27484655380249
Epoch 70, val loss: 1.8384336233139038
Epoch 80, training loss: 360.2985534667969 = 1.8288650512695312 + 50.0 * 7.169394016265869
Epoch 80, val loss: 1.8294117450714111
Epoch 90, training loss: 355.1001281738281 = 1.8180407285690308 + 50.0 * 7.065641403198242
Epoch 90, val loss: 1.820947527885437
Epoch 100, training loss: 349.2638244628906 = 1.810876488685608 + 50.0 * 6.949058532714844
Epoch 100, val loss: 1.8149940967559814
Epoch 110, training loss: 344.9772033691406 = 1.8053982257843018 + 50.0 * 6.863436222076416
Epoch 110, val loss: 1.809682846069336
Epoch 120, training loss: 342.01287841796875 = 1.8000123500823975 + 50.0 * 6.804256916046143
Epoch 120, val loss: 1.8046112060546875
Epoch 130, training loss: 339.14666748046875 = 1.7939703464508057 + 50.0 * 6.747054100036621
Epoch 130, val loss: 1.7994152307510376
Epoch 140, training loss: 336.47503662109375 = 1.7885985374450684 + 50.0 * 6.693728446960449
Epoch 140, val loss: 1.7947087287902832
Epoch 150, training loss: 334.59075927734375 = 1.783585786819458 + 50.0 * 6.6561431884765625
Epoch 150, val loss: 1.7902952432632446
Epoch 160, training loss: 332.8111572265625 = 1.777899980545044 + 50.0 * 6.620665073394775
Epoch 160, val loss: 1.7854779958724976
Epoch 170, training loss: 331.4393310546875 = 1.7720394134521484 + 50.0 * 6.593346118927002
Epoch 170, val loss: 1.7807140350341797
Epoch 180, training loss: 330.0746765136719 = 1.7659727334976196 + 50.0 * 6.566174030303955
Epoch 180, val loss: 1.7757811546325684
Epoch 190, training loss: 329.0126647949219 = 1.759582281112671 + 50.0 * 6.5450615882873535
Epoch 190, val loss: 1.7704315185546875
Epoch 200, training loss: 327.9518127441406 = 1.752685308456421 + 50.0 * 6.523982524871826
Epoch 200, val loss: 1.764755368232727
Epoch 210, training loss: 327.2447204589844 = 1.7452552318572998 + 50.0 * 6.509989261627197
Epoch 210, val loss: 1.7586565017700195
Epoch 220, training loss: 326.30267333984375 = 1.7370736598968506 + 50.0 * 6.491312503814697
Epoch 220, val loss: 1.7521374225616455
Epoch 230, training loss: 325.5894470214844 = 1.7283780574798584 + 50.0 * 6.477221488952637
Epoch 230, val loss: 1.745168924331665
Epoch 240, training loss: 324.9832763671875 = 1.7190221548080444 + 50.0 * 6.465284824371338
Epoch 240, val loss: 1.7376714944839478
Epoch 250, training loss: 324.3616638183594 = 1.708966851234436 + 50.0 * 6.453053951263428
Epoch 250, val loss: 1.729756236076355
Epoch 260, training loss: 323.7774963378906 = 1.6983015537261963 + 50.0 * 6.441583633422852
Epoch 260, val loss: 1.7212731838226318
Epoch 270, training loss: 323.3740234375 = 1.6868056058883667 + 50.0 * 6.433744430541992
Epoch 270, val loss: 1.7122657299041748
Epoch 280, training loss: 322.90618896484375 = 1.6745350360870361 + 50.0 * 6.424633026123047
Epoch 280, val loss: 1.7025460004806519
Epoch 290, training loss: 322.3856506347656 = 1.6614652872085571 + 50.0 * 6.414483547210693
Epoch 290, val loss: 1.6922968626022339
Epoch 300, training loss: 321.9521484375 = 1.6477608680725098 + 50.0 * 6.406087398529053
Epoch 300, val loss: 1.6815814971923828
Epoch 310, training loss: 321.97601318359375 = 1.6332793235778809 + 50.0 * 6.40685510635376
Epoch 310, val loss: 1.6703824996948242
Epoch 320, training loss: 321.35430908203125 = 1.6180932521820068 + 50.0 * 6.394723892211914
Epoch 320, val loss: 1.6585732698440552
Epoch 330, training loss: 320.90045166015625 = 1.6023998260498047 + 50.0 * 6.385961532592773
Epoch 330, val loss: 1.6463353633880615
Epoch 340, training loss: 320.7134094238281 = 1.5862418413162231 + 50.0 * 6.382543563842773
Epoch 340, val loss: 1.6339284181594849
Epoch 350, training loss: 320.48138427734375 = 1.5695465803146362 + 50.0 * 6.378236770629883
Epoch 350, val loss: 1.6211283206939697
Epoch 360, training loss: 320.0854187011719 = 1.5523788928985596 + 50.0 * 6.370660781860352
Epoch 360, val loss: 1.608090877532959
Epoch 370, training loss: 320.0924987792969 = 1.5349206924438477 + 50.0 * 6.371151447296143
Epoch 370, val loss: 1.5949714183807373
Epoch 380, training loss: 319.6733093261719 = 1.5171161890029907 + 50.0 * 6.363123893737793
Epoch 380, val loss: 1.581644892692566
Epoch 390, training loss: 319.3514099121094 = 1.4992890357971191 + 50.0 * 6.35704231262207
Epoch 390, val loss: 1.5684789419174194
Epoch 400, training loss: 319.12255859375 = 1.4813799858093262 + 50.0 * 6.352823734283447
Epoch 400, val loss: 1.5554168224334717
Epoch 410, training loss: 318.9841003417969 = 1.4633857011795044 + 50.0 * 6.350414276123047
Epoch 410, val loss: 1.5424931049346924
Epoch 420, training loss: 319.0001525878906 = 1.4450143575668335 + 50.0 * 6.351102828979492
Epoch 420, val loss: 1.5293906927108765
Epoch 430, training loss: 318.5633239746094 = 1.4267901182174683 + 50.0 * 6.342730522155762
Epoch 430, val loss: 1.5166126489639282
Epoch 440, training loss: 318.3347473144531 = 1.4086766242980957 + 50.0 * 6.3385210037231445
Epoch 440, val loss: 1.5041923522949219
Epoch 450, training loss: 318.2225341796875 = 1.3906151056289673 + 50.0 * 6.336638450622559
Epoch 450, val loss: 1.4920909404754639
Epoch 460, training loss: 318.2360534667969 = 1.3724275827407837 + 50.0 * 6.3372721672058105
Epoch 460, val loss: 1.4800077676773071
Epoch 470, training loss: 317.92718505859375 = 1.3543258905410767 + 50.0 * 6.331457138061523
Epoch 470, val loss: 1.468268632888794
Epoch 480, training loss: 317.72235107421875 = 1.3365192413330078 + 50.0 * 6.327716827392578
Epoch 480, val loss: 1.4570891857147217
Epoch 490, training loss: 317.5867004394531 = 1.3188443183898926 + 50.0 * 6.325356960296631
Epoch 490, val loss: 1.446380376815796
Epoch 500, training loss: 317.4291076660156 = 1.3011014461517334 + 50.0 * 6.3225603103637695
Epoch 500, val loss: 1.4356509447097778
Epoch 510, training loss: 317.3846130371094 = 1.2835719585418701 + 50.0 * 6.322021007537842
Epoch 510, val loss: 1.4256219863891602
Epoch 520, training loss: 317.22186279296875 = 1.2661702632904053 + 50.0 * 6.319113731384277
Epoch 520, val loss: 1.4157042503356934
Epoch 530, training loss: 317.1835632324219 = 1.2489371299743652 + 50.0 * 6.318692684173584
Epoch 530, val loss: 1.4062539339065552
Epoch 540, training loss: 316.8981628417969 = 1.2317837476730347 + 50.0 * 6.313327789306641
Epoch 540, val loss: 1.39723539352417
Epoch 550, training loss: 316.7916259765625 = 1.2149779796600342 + 50.0 * 6.311533451080322
Epoch 550, val loss: 1.3887323141098022
Epoch 560, training loss: 317.0841979980469 = 1.198244571685791 + 50.0 * 6.317718982696533
Epoch 560, val loss: 1.3802627325057983
Epoch 570, training loss: 316.6399230957031 = 1.1815048456192017 + 50.0 * 6.309167861938477
Epoch 570, val loss: 1.3720899820327759
Epoch 580, training loss: 316.44500732421875 = 1.1651942729949951 + 50.0 * 6.305596351623535
Epoch 580, val loss: 1.3646804094314575
Epoch 590, training loss: 316.4726257324219 = 1.149161458015442 + 50.0 * 6.306468963623047
Epoch 590, val loss: 1.3577020168304443
Epoch 600, training loss: 316.394775390625 = 1.1329830884933472 + 50.0 * 6.305235385894775
Epoch 600, val loss: 1.3502284288406372
Epoch 610, training loss: 316.1624450683594 = 1.1172419786453247 + 50.0 * 6.300903797149658
Epoch 610, val loss: 1.3440542221069336
Epoch 620, training loss: 316.2039489746094 = 1.1017131805419922 + 50.0 * 6.302044868469238
Epoch 620, val loss: 1.3377435207366943
Epoch 630, training loss: 315.9765930175781 = 1.0863542556762695 + 50.0 * 6.297804355621338
Epoch 630, val loss: 1.332069993019104
Epoch 640, training loss: 315.8966369628906 = 1.0712379217147827 + 50.0 * 6.296508312225342
Epoch 640, val loss: 1.3263920545578003
Epoch 650, training loss: 315.9035339355469 = 1.0564109086990356 + 50.0 * 6.296942710876465
Epoch 650, val loss: 1.321466088294983
Epoch 660, training loss: 315.75048828125 = 1.0416786670684814 + 50.0 * 6.29417610168457
Epoch 660, val loss: 1.3163152933120728
Epoch 670, training loss: 315.67987060546875 = 1.027343988418579 + 50.0 * 6.293050765991211
Epoch 670, val loss: 1.3117841482162476
Epoch 680, training loss: 315.5245666503906 = 1.013185739517212 + 50.0 * 6.290227890014648
Epoch 680, val loss: 1.307386040687561
Epoch 690, training loss: 315.63323974609375 = 0.999469518661499 + 50.0 * 6.292675018310547
Epoch 690, val loss: 1.3035308122634888
Epoch 700, training loss: 315.55450439453125 = 0.9856233596801758 + 50.0 * 6.291377544403076
Epoch 700, val loss: 1.2994714975357056
Epoch 710, training loss: 315.292724609375 = 0.9721550941467285 + 50.0 * 6.286411285400391
Epoch 710, val loss: 1.29568350315094
Epoch 720, training loss: 315.2505187988281 = 0.9589985013008118 + 50.0 * 6.285830497741699
Epoch 720, val loss: 1.2923035621643066
Epoch 730, training loss: 315.1656494140625 = 0.9460031390190125 + 50.0 * 6.284392833709717
Epoch 730, val loss: 1.2891812324523926
Epoch 740, training loss: 315.05291748046875 = 0.9332687258720398 + 50.0 * 6.282392978668213
Epoch 740, val loss: 1.2861844301223755
Epoch 750, training loss: 314.9833068847656 = 0.9207415580749512 + 50.0 * 6.281251430511475
Epoch 750, val loss: 1.283371925354004
Epoch 760, training loss: 315.0796813964844 = 0.9085271954536438 + 50.0 * 6.283422946929932
Epoch 760, val loss: 1.2806956768035889
Epoch 770, training loss: 314.9171142578125 = 0.8964549899101257 + 50.0 * 6.2804131507873535
Epoch 770, val loss: 1.2789154052734375
Epoch 780, training loss: 315.072021484375 = 0.8845025300979614 + 50.0 * 6.283750534057617
Epoch 780, val loss: 1.276387333869934
Epoch 790, training loss: 314.79833984375 = 0.8726919889450073 + 50.0 * 6.278512954711914
Epoch 790, val loss: 1.2742713689804077
Epoch 800, training loss: 314.6165466308594 = 0.861225962638855 + 50.0 * 6.275106430053711
Epoch 800, val loss: 1.2726038694381714
Epoch 810, training loss: 314.5568542480469 = 0.8499589562416077 + 50.0 * 6.274138450622559
Epoch 810, val loss: 1.2707899808883667
Epoch 820, training loss: 314.7333068847656 = 0.8388677835464478 + 50.0 * 6.277888774871826
Epoch 820, val loss: 1.2693438529968262
Epoch 830, training loss: 314.57391357421875 = 0.827823281288147 + 50.0 * 6.274921894073486
Epoch 830, val loss: 1.2680143117904663
Epoch 840, training loss: 314.3830261230469 = 0.8169246912002563 + 50.0 * 6.271321773529053
Epoch 840, val loss: 1.2665424346923828
Epoch 850, training loss: 314.5437316894531 = 0.8063389658927917 + 50.0 * 6.274747848510742
Epoch 850, val loss: 1.265230655670166
Epoch 860, training loss: 314.3589172363281 = 0.7958193421363831 + 50.0 * 6.271262168884277
Epoch 860, val loss: 1.264617919921875
Epoch 870, training loss: 314.24102783203125 = 0.7854255437850952 + 50.0 * 6.2691121101379395
Epoch 870, val loss: 1.2634897232055664
Epoch 880, training loss: 314.3645935058594 = 0.7752882242202759 + 50.0 * 6.271785736083984
Epoch 880, val loss: 1.2628533840179443
Epoch 890, training loss: 314.1919860839844 = 0.7650612592697144 + 50.0 * 6.268538475036621
Epoch 890, val loss: 1.2621071338653564
Epoch 900, training loss: 314.0730285644531 = 0.7551103234291077 + 50.0 * 6.266358375549316
Epoch 900, val loss: 1.261400580406189
Epoch 910, training loss: 314.1271057128906 = 0.7452888488769531 + 50.0 * 6.267635822296143
Epoch 910, val loss: 1.2607673406600952
Epoch 920, training loss: 313.9546203613281 = 0.735626220703125 + 50.0 * 6.264379978179932
Epoch 920, val loss: 1.2607561349868774
Epoch 930, training loss: 313.8954772949219 = 0.7260215282440186 + 50.0 * 6.2633891105651855
Epoch 930, val loss: 1.260283350944519
Epoch 940, training loss: 313.8637390136719 = 0.7165713906288147 + 50.0 * 6.262943267822266
Epoch 940, val loss: 1.2602273225784302
Epoch 950, training loss: 313.841796875 = 0.7071735262870789 + 50.0 * 6.262692451477051
Epoch 950, val loss: 1.2599232196807861
Epoch 960, training loss: 313.8050231933594 = 0.6978407502174377 + 50.0 * 6.262143611907959
Epoch 960, val loss: 1.2598953247070312
Epoch 970, training loss: 313.6982421875 = 0.6886324286460876 + 50.0 * 6.260192394256592
Epoch 970, val loss: 1.2598798274993896
Epoch 980, training loss: 313.6844787597656 = 0.6795417666435242 + 50.0 * 6.260098934173584
Epoch 980, val loss: 1.2600575685501099
Epoch 990, training loss: 313.6453552246094 = 0.6705430746078491 + 50.0 * 6.259496688842773
Epoch 990, val loss: 1.2603373527526855
Epoch 1000, training loss: 313.6707458496094 = 0.661500096321106 + 50.0 * 6.2601847648620605
Epoch 1000, val loss: 1.2605103254318237
Epoch 1010, training loss: 313.5144348144531 = 0.6526009440422058 + 50.0 * 6.257236957550049
Epoch 1010, val loss: 1.2609069347381592
Epoch 1020, training loss: 313.6035461425781 = 0.6438441872596741 + 50.0 * 6.259194374084473
Epoch 1020, val loss: 1.261495590209961
Epoch 1030, training loss: 313.42193603515625 = 0.6350154876708984 + 50.0 * 6.255738258361816
Epoch 1030, val loss: 1.2615102529525757
Epoch 1040, training loss: 313.40155029296875 = 0.6264058947563171 + 50.0 * 6.255503177642822
Epoch 1040, val loss: 1.2624832391738892
Epoch 1050, training loss: 313.4489440917969 = 0.6177796125411987 + 50.0 * 6.2566237449646
Epoch 1050, val loss: 1.2629934549331665
Epoch 1060, training loss: 313.3659973144531 = 0.6090925931930542 + 50.0 * 6.255137920379639
Epoch 1060, val loss: 1.2636315822601318
Epoch 1070, training loss: 313.2548828125 = 0.6005948781967163 + 50.0 * 6.253086090087891
Epoch 1070, val loss: 1.2646205425262451
Epoch 1080, training loss: 313.26446533203125 = 0.5920859575271606 + 50.0 * 6.253448009490967
Epoch 1080, val loss: 1.265406608581543
Epoch 1090, training loss: 313.37261962890625 = 0.5836247205734253 + 50.0 * 6.25577974319458
Epoch 1090, val loss: 1.2665619850158691
Epoch 1100, training loss: 313.1260986328125 = 0.5751420259475708 + 50.0 * 6.25101900100708
Epoch 1100, val loss: 1.26750648021698
Epoch 1110, training loss: 313.1051025390625 = 0.5667478442192078 + 50.0 * 6.250767230987549
Epoch 1110, val loss: 1.268992304801941
Epoch 1120, training loss: 313.09234619140625 = 0.5584482550621033 + 50.0 * 6.250678062438965
Epoch 1120, val loss: 1.2701129913330078
Epoch 1130, training loss: 313.02276611328125 = 0.5501704812049866 + 50.0 * 6.249451637268066
Epoch 1130, val loss: 1.27175772190094
Epoch 1140, training loss: 313.273193359375 = 0.5419483184814453 + 50.0 * 6.254624366760254
Epoch 1140, val loss: 1.273194670677185
Epoch 1150, training loss: 313.034423828125 = 0.5334990620613098 + 50.0 * 6.250018119812012
Epoch 1150, val loss: 1.2742433547973633
Epoch 1160, training loss: 312.89312744140625 = 0.5253147482872009 + 50.0 * 6.247355937957764
Epoch 1160, val loss: 1.2765839099884033
Epoch 1170, training loss: 312.8110046386719 = 0.5171459913253784 + 50.0 * 6.245877265930176
Epoch 1170, val loss: 1.2781661748886108
Epoch 1180, training loss: 312.78704833984375 = 0.5090554356575012 + 50.0 * 6.2455596923828125
Epoch 1180, val loss: 1.2802752256393433
Epoch 1190, training loss: 313.29840087890625 = 0.5009291172027588 + 50.0 * 6.255949974060059
Epoch 1190, val loss: 1.2824456691741943
Epoch 1200, training loss: 312.8215637207031 = 0.49263641238212585 + 50.0 * 6.246578216552734
Epoch 1200, val loss: 1.2842504978179932
Epoch 1210, training loss: 312.74542236328125 = 0.48449838161468506 + 50.0 * 6.245218753814697
Epoch 1210, val loss: 1.2864477634429932
Epoch 1220, training loss: 312.6697998046875 = 0.4764685332775116 + 50.0 * 6.243866443634033
Epoch 1220, val loss: 1.2889975309371948
Epoch 1230, training loss: 312.8432922363281 = 0.4684560000896454 + 50.0 * 6.247497081756592
Epoch 1230, val loss: 1.2912441492080688
Epoch 1240, training loss: 312.6081848144531 = 0.46043968200683594 + 50.0 * 6.242954730987549
Epoch 1240, val loss: 1.294206976890564
Epoch 1250, training loss: 312.6025695800781 = 0.4524986743927002 + 50.0 * 6.243001461029053
Epoch 1250, val loss: 1.2969081401824951
Epoch 1260, training loss: 312.5506286621094 = 0.4445476233959198 + 50.0 * 6.242121696472168
Epoch 1260, val loss: 1.299605131149292
Epoch 1270, training loss: 312.54754638671875 = 0.4367097318172455 + 50.0 * 6.242217063903809
Epoch 1270, val loss: 1.3024033308029175
Epoch 1280, training loss: 312.8155822753906 = 0.4288327991962433 + 50.0 * 6.247735023498535
Epoch 1280, val loss: 1.305236577987671
Epoch 1290, training loss: 312.5631103515625 = 0.4210754930973053 + 50.0 * 6.242840766906738
Epoch 1290, val loss: 1.3086835145950317
Epoch 1300, training loss: 312.39715576171875 = 0.4132136404514313 + 50.0 * 6.239678859710693
Epoch 1300, val loss: 1.3117746114730835
Epoch 1310, training loss: 312.3592834472656 = 0.40560296177864075 + 50.0 * 6.239073753356934
Epoch 1310, val loss: 1.3150062561035156
Epoch 1320, training loss: 312.3515930175781 = 0.39809301495552063 + 50.0 * 6.239069938659668
Epoch 1320, val loss: 1.318533182144165
Epoch 1330, training loss: 312.5829162597656 = 0.39056965708732605 + 50.0 * 6.243846893310547
Epoch 1330, val loss: 1.3217886686325073
Epoch 1340, training loss: 312.4043884277344 = 0.38308125734329224 + 50.0 * 6.240426063537598
Epoch 1340, val loss: 1.3256765604019165
Epoch 1350, training loss: 312.4246520996094 = 0.3756349980831146 + 50.0 * 6.24098014831543
Epoch 1350, val loss: 1.3294509649276733
Epoch 1360, training loss: 312.3781433105469 = 0.3683040142059326 + 50.0 * 6.240196704864502
Epoch 1360, val loss: 1.3331187963485718
Epoch 1370, training loss: 312.23779296875 = 0.3610790967941284 + 50.0 * 6.237534046173096
Epoch 1370, val loss: 1.3372976779937744
Epoch 1380, training loss: 312.13848876953125 = 0.3539498448371887 + 50.0 * 6.235691070556641
Epoch 1380, val loss: 1.3412889242172241
Epoch 1390, training loss: 312.1361999511719 = 0.3469906449317932 + 50.0 * 6.23578405380249
Epoch 1390, val loss: 1.34552001953125
Epoch 1400, training loss: 312.2349853515625 = 0.34007006883621216 + 50.0 * 6.237898826599121
Epoch 1400, val loss: 1.3495770692825317
Epoch 1410, training loss: 312.3013610839844 = 0.33320480585098267 + 50.0 * 6.239363193511963
Epoch 1410, val loss: 1.354367971420288
Epoch 1420, training loss: 312.30877685546875 = 0.32639822363853455 + 50.0 * 6.23964786529541
Epoch 1420, val loss: 1.3585652112960815
Epoch 1430, training loss: 312.0701904296875 = 0.3196205794811249 + 50.0 * 6.235011577606201
Epoch 1430, val loss: 1.3628129959106445
Epoch 1440, training loss: 311.99176025390625 = 0.3131074011325836 + 50.0 * 6.233572959899902
Epoch 1440, val loss: 1.3677095174789429
Epoch 1450, training loss: 311.9606018066406 = 0.3067028820514679 + 50.0 * 6.2330780029296875
Epoch 1450, val loss: 1.3725097179412842
Epoch 1460, training loss: 312.0853271484375 = 0.300430566072464 + 50.0 * 6.2356977462768555
Epoch 1460, val loss: 1.377342700958252
Epoch 1470, training loss: 311.99554443359375 = 0.29410257935523987 + 50.0 * 6.2340288162231445
Epoch 1470, val loss: 1.3819137811660767
Epoch 1480, training loss: 311.9803771972656 = 0.28786155581474304 + 50.0 * 6.233850002288818
Epoch 1480, val loss: 1.3863765001296997
Epoch 1490, training loss: 311.9262390136719 = 0.28177669644355774 + 50.0 * 6.232889175415039
Epoch 1490, val loss: 1.3914858102798462
Epoch 1500, training loss: 312.0092468261719 = 0.2758016586303711 + 50.0 * 6.234669208526611
Epoch 1500, val loss: 1.396356225013733
Epoch 1510, training loss: 311.8782958984375 = 0.26988762617111206 + 50.0 * 6.232167720794678
Epoch 1510, val loss: 1.4014068841934204
Epoch 1520, training loss: 311.8167724609375 = 0.26418566703796387 + 50.0 * 6.231051445007324
Epoch 1520, val loss: 1.4066530466079712
Epoch 1530, training loss: 311.784912109375 = 0.2585770785808563 + 50.0 * 6.230526447296143
Epoch 1530, val loss: 1.411835789680481
Epoch 1540, training loss: 312.053466796875 = 0.2530951201915741 + 50.0 * 6.2360076904296875
Epoch 1540, val loss: 1.4171051979064941
Epoch 1550, training loss: 311.96221923828125 = 0.24760408699512482 + 50.0 * 6.234292507171631
Epoch 1550, val loss: 1.4223229885101318
Epoch 1560, training loss: 311.788330078125 = 0.24217043817043304 + 50.0 * 6.230923652648926
Epoch 1560, val loss: 1.4274204969406128
Epoch 1570, training loss: 311.7277526855469 = 0.23694944381713867 + 50.0 * 6.229816436767578
Epoch 1570, val loss: 1.432844638824463
Epoch 1580, training loss: 311.8516845703125 = 0.2318449318408966 + 50.0 * 6.232397079467773
Epoch 1580, val loss: 1.4378422498703003
Epoch 1590, training loss: 311.6637878417969 = 0.2268126755952835 + 50.0 * 6.2287397384643555
Epoch 1590, val loss: 1.4442174434661865
Epoch 1600, training loss: 311.6607666015625 = 0.22186464071273804 + 50.0 * 6.228777885437012
Epoch 1600, val loss: 1.4493072032928467
Epoch 1610, training loss: 311.7421569824219 = 0.21705999970436096 + 50.0 * 6.230501651763916
Epoch 1610, val loss: 1.4551383256912231
Epoch 1620, training loss: 311.5785827636719 = 0.21233771741390228 + 50.0 * 6.227324962615967
Epoch 1620, val loss: 1.4607150554656982
Epoch 1630, training loss: 311.7025146484375 = 0.20778140425682068 + 50.0 * 6.229894638061523
Epoch 1630, val loss: 1.4666322469711304
Epoch 1640, training loss: 311.7353515625 = 0.2031896561384201 + 50.0 * 6.230643272399902
Epoch 1640, val loss: 1.472203254699707
Epoch 1650, training loss: 311.54351806640625 = 0.19875174760818481 + 50.0 * 6.226894855499268
Epoch 1650, val loss: 1.4781410694122314
Epoch 1660, training loss: 311.49884033203125 = 0.1944226622581482 + 50.0 * 6.226088523864746
Epoch 1660, val loss: 1.483899712562561
Epoch 1670, training loss: 311.5550842285156 = 0.19024941325187683 + 50.0 * 6.227296829223633
Epoch 1670, val loss: 1.4897675514221191
Epoch 1680, training loss: 311.68829345703125 = 0.18610934913158417 + 50.0 * 6.230043888092041
Epoch 1680, val loss: 1.4956177473068237
Epoch 1690, training loss: 311.5472717285156 = 0.18203601241111755 + 50.0 * 6.227304458618164
Epoch 1690, val loss: 1.5017690658569336
Epoch 1700, training loss: 311.470947265625 = 0.17807766795158386 + 50.0 * 6.225857734680176
Epoch 1700, val loss: 1.5076524019241333
Epoch 1710, training loss: 311.48065185546875 = 0.17424719035625458 + 50.0 * 6.226128101348877
Epoch 1710, val loss: 1.5139731168746948
Epoch 1720, training loss: 311.5623779296875 = 0.17049375176429749 + 50.0 * 6.227837562561035
Epoch 1720, val loss: 1.5199388265609741
Epoch 1730, training loss: 311.4452209472656 = 0.1668226420879364 + 50.0 * 6.2255682945251465
Epoch 1730, val loss: 1.5261598825454712
Epoch 1740, training loss: 311.6181945800781 = 0.16325537860393524 + 50.0 * 6.229098796844482
Epoch 1740, val loss: 1.5326611995697021
Epoch 1750, training loss: 311.4322509765625 = 0.15964552760124207 + 50.0 * 6.225452423095703
Epoch 1750, val loss: 1.5382976531982422
Epoch 1760, training loss: 311.35089111328125 = 0.15621396899223328 + 50.0 * 6.223893642425537
Epoch 1760, val loss: 1.5448973178863525
Epoch 1770, training loss: 311.3068542480469 = 0.1528552621603012 + 50.0 * 6.223079681396484
Epoch 1770, val loss: 1.5510081052780151
Epoch 1780, training loss: 311.33099365234375 = 0.14963442087173462 + 50.0 * 6.223627090454102
Epoch 1780, val loss: 1.5573457479476929
Epoch 1790, training loss: 311.603271484375 = 0.14647692441940308 + 50.0 * 6.229135990142822
Epoch 1790, val loss: 1.563430666923523
Epoch 1800, training loss: 311.388916015625 = 0.1432514488697052 + 50.0 * 6.224913597106934
Epoch 1800, val loss: 1.5690696239471436
Epoch 1810, training loss: 311.4493408203125 = 0.14021432399749756 + 50.0 * 6.226182460784912
Epoch 1810, val loss: 1.5754905939102173
Epoch 1820, training loss: 311.24627685546875 = 0.13720113039016724 + 50.0 * 6.22218132019043
Epoch 1820, val loss: 1.581742525100708
Epoch 1830, training loss: 311.37030029296875 = 0.1343289464712143 + 50.0 * 6.224719524383545
Epoch 1830, val loss: 1.5879082679748535
Epoch 1840, training loss: 311.245361328125 = 0.13146618008613586 + 50.0 * 6.222278118133545
Epoch 1840, val loss: 1.5941686630249023
Epoch 1850, training loss: 311.2323303222656 = 0.1287243366241455 + 50.0 * 6.222072124481201
Epoch 1850, val loss: 1.6006661653518677
Epoch 1860, training loss: 311.29541015625 = 0.12605087459087372 + 50.0 * 6.223387241363525
Epoch 1860, val loss: 1.606878638267517
Epoch 1870, training loss: 311.2457275390625 = 0.1233966127038002 + 50.0 * 6.222446918487549
Epoch 1870, val loss: 1.6128791570663452
Epoch 1880, training loss: 311.19683837890625 = 0.1207965686917305 + 50.0 * 6.221520900726318
Epoch 1880, val loss: 1.6192938089370728
Epoch 1890, training loss: 311.3118896484375 = 0.11833456158638 + 50.0 * 6.223870754241943
Epoch 1890, val loss: 1.6255466938018799
Epoch 1900, training loss: 311.1916809082031 = 0.11584053188562393 + 50.0 * 6.2215166091918945
Epoch 1900, val loss: 1.6316571235656738
Epoch 1910, training loss: 311.3632507324219 = 0.1134638711810112 + 50.0 * 6.2249956130981445
Epoch 1910, val loss: 1.638161540031433
Epoch 1920, training loss: 311.1436462402344 = 0.11108989268541336 + 50.0 * 6.220651149749756
Epoch 1920, val loss: 1.6440597772598267
Epoch 1930, training loss: 311.1081237792969 = 0.10882105678319931 + 50.0 * 6.2199859619140625
Epoch 1930, val loss: 1.6507701873779297
Epoch 1940, training loss: 311.0941162109375 = 0.10661743581295013 + 50.0 * 6.219749450683594
Epoch 1940, val loss: 1.656975507736206
Epoch 1950, training loss: 311.37982177734375 = 0.10450106114149094 + 50.0 * 6.22550630569458
Epoch 1950, val loss: 1.663394808769226
Epoch 1960, training loss: 311.15875244140625 = 0.10233231633901596 + 50.0 * 6.221128463745117
Epoch 1960, val loss: 1.6697361469268799
Epoch 1970, training loss: 311.07452392578125 = 0.10025240480899811 + 50.0 * 6.219485759735107
Epoch 1970, val loss: 1.6762007474899292
Epoch 1980, training loss: 311.06927490234375 = 0.09826917201280594 + 50.0 * 6.219420433044434
Epoch 1980, val loss: 1.6825562715530396
Epoch 1990, training loss: 311.21270751953125 = 0.0963498055934906 + 50.0 * 6.22232723236084
Epoch 1990, val loss: 1.6888206005096436
Epoch 2000, training loss: 311.3257141113281 = 0.09437945485115051 + 50.0 * 6.224626541137695
Epoch 2000, val loss: 1.6944392919540405
Epoch 2010, training loss: 311.0448303222656 = 0.09246473014354706 + 50.0 * 6.2190470695495605
Epoch 2010, val loss: 1.7010449171066284
Epoch 2020, training loss: 310.990478515625 = 0.0906279906630516 + 50.0 * 6.217997074127197
Epoch 2020, val loss: 1.7070244550704956
Epoch 2030, training loss: 310.958740234375 = 0.08889065682888031 + 50.0 * 6.2173967361450195
Epoch 2030, val loss: 1.7135863304138184
Epoch 2040, training loss: 311.2145080566406 = 0.08717671036720276 + 50.0 * 6.222546577453613
Epoch 2040, val loss: 1.720031976699829
Epoch 2050, training loss: 310.96917724609375 = 0.0854363739490509 + 50.0 * 6.217674732208252
Epoch 2050, val loss: 1.7256323099136353
Epoch 2060, training loss: 310.9621276855469 = 0.08376424014568329 + 50.0 * 6.217566967010498
Epoch 2060, val loss: 1.7318496704101562
Epoch 2070, training loss: 311.04400634765625 = 0.08216522634029388 + 50.0 * 6.219236850738525
Epoch 2070, val loss: 1.73811936378479
Epoch 2080, training loss: 310.907958984375 = 0.08058776706457138 + 50.0 * 6.216547012329102
Epoch 2080, val loss: 1.7447359561920166
Epoch 2090, training loss: 310.9101867675781 = 0.07906387001276016 + 50.0 * 6.216622352600098
Epoch 2090, val loss: 1.7508951425552368
Epoch 2100, training loss: 311.01959228515625 = 0.0775739923119545 + 50.0 * 6.2188401222229
Epoch 2100, val loss: 1.7566829919815063
Epoch 2110, training loss: 310.90863037109375 = 0.07606986910104752 + 50.0 * 6.21665096282959
Epoch 2110, val loss: 1.7625099420547485
Epoch 2120, training loss: 310.8774719238281 = 0.07461681962013245 + 50.0 * 6.216056823730469
Epoch 2120, val loss: 1.7687524557113647
Epoch 2130, training loss: 310.80908203125 = 0.07322394102811813 + 50.0 * 6.214717388153076
Epoch 2130, val loss: 1.774658203125
Epoch 2140, training loss: 310.9649353027344 = 0.07190938293933868 + 50.0 * 6.217860698699951
Epoch 2140, val loss: 1.7809680700302124
Epoch 2150, training loss: 310.8965759277344 = 0.0705169141292572 + 50.0 * 6.216521263122559
Epoch 2150, val loss: 1.7863065004348755
Epoch 2160, training loss: 310.79412841796875 = 0.069179467856884 + 50.0 * 6.214498996734619
Epoch 2160, val loss: 1.7923359870910645
Epoch 2170, training loss: 310.7781982421875 = 0.06790228933095932 + 50.0 * 6.214206218719482
Epoch 2170, val loss: 1.7983096837997437
Epoch 2180, training loss: 310.79400634765625 = 0.06668443977832794 + 50.0 * 6.2145466804504395
Epoch 2180, val loss: 1.804606318473816
Epoch 2190, training loss: 310.99749755859375 = 0.06548695266246796 + 50.0 * 6.218640327453613
Epoch 2190, val loss: 1.8104785680770874
Epoch 2200, training loss: 310.74151611328125 = 0.06425189226865768 + 50.0 * 6.213545322418213
Epoch 2200, val loss: 1.8160144090652466
Epoch 2210, training loss: 310.9308166503906 = 0.06309973448514938 + 50.0 * 6.2173542976379395
Epoch 2210, val loss: 1.8215999603271484
Epoch 2220, training loss: 310.8424072265625 = 0.06193659454584122 + 50.0 * 6.215609073638916
Epoch 2220, val loss: 1.8277630805969238
Epoch 2230, training loss: 310.71343994140625 = 0.06078782677650452 + 50.0 * 6.213052749633789
Epoch 2230, val loss: 1.833182692527771
Epoch 2240, training loss: 310.7199401855469 = 0.05970399081707001 + 50.0 * 6.213204860687256
Epoch 2240, val loss: 1.8391520977020264
Epoch 2250, training loss: 310.8748474121094 = 0.05867450311779976 + 50.0 * 6.216323375701904
Epoch 2250, val loss: 1.8448129892349243
Epoch 2260, training loss: 310.6861877441406 = 0.05761728435754776 + 50.0 * 6.212571620941162
Epoch 2260, val loss: 1.8508384227752686
Epoch 2270, training loss: 310.78729248046875 = 0.056631989777088165 + 50.0 * 6.21461296081543
Epoch 2270, val loss: 1.8566997051239014
Epoch 2280, training loss: 310.8203430175781 = 0.055629607290029526 + 50.0 * 6.215293884277344
Epoch 2280, val loss: 1.862089991569519
Epoch 2290, training loss: 310.7469482421875 = 0.054638464003801346 + 50.0 * 6.213846683502197
Epoch 2290, val loss: 1.8677873611450195
Epoch 2300, training loss: 310.6839904785156 = 0.053695254027843475 + 50.0 * 6.212606430053711
Epoch 2300, val loss: 1.8736940622329712
Epoch 2310, training loss: 310.69189453125 = 0.05277654528617859 + 50.0 * 6.212782382965088
Epoch 2310, val loss: 1.8794987201690674
Epoch 2320, training loss: 310.7297058105469 = 0.05187463015317917 + 50.0 * 6.21355676651001
Epoch 2320, val loss: 1.8854235410690308
Epoch 2330, training loss: 310.7255859375 = 0.05098339542746544 + 50.0 * 6.213491916656494
Epoch 2330, val loss: 1.8906605243682861
Epoch 2340, training loss: 310.6884460449219 = 0.050103746354579926 + 50.0 * 6.212766647338867
Epoch 2340, val loss: 1.896342396736145
Epoch 2350, training loss: 310.6282958984375 = 0.049253035336732864 + 50.0 * 6.211580753326416
Epoch 2350, val loss: 1.902247667312622
Epoch 2360, training loss: 310.74969482421875 = 0.048437874764204025 + 50.0 * 6.214025020599365
Epoch 2360, val loss: 1.9074103832244873
Epoch 2370, training loss: 310.6046447753906 = 0.047620318830013275 + 50.0 * 6.2111406326293945
Epoch 2370, val loss: 1.913360595703125
Epoch 2380, training loss: 310.74273681640625 = 0.04683572053909302 + 50.0 * 6.2139177322387695
Epoch 2380, val loss: 1.9189785718917847
Epoch 2390, training loss: 310.54925537109375 = 0.04603245109319687 + 50.0 * 6.210064888000488
Epoch 2390, val loss: 1.9243718385696411
Epoch 2400, training loss: 310.6639404296875 = 0.045279428362846375 + 50.0 * 6.21237325668335
Epoch 2400, val loss: 1.9296294450759888
Epoch 2410, training loss: 310.6590576171875 = 0.04454304650425911 + 50.0 * 6.212290287017822
Epoch 2410, val loss: 1.9356626272201538
Epoch 2420, training loss: 310.58355712890625 = 0.04381692409515381 + 50.0 * 6.210794448852539
Epoch 2420, val loss: 1.9412670135498047
Epoch 2430, training loss: 310.55517578125 = 0.04311011731624603 + 50.0 * 6.210241317749023
Epoch 2430, val loss: 1.9468305110931396
Epoch 2440, training loss: 310.6723327636719 = 0.04242755472660065 + 50.0 * 6.2125983238220215
Epoch 2440, val loss: 1.9518308639526367
Epoch 2450, training loss: 310.6099548339844 = 0.041748885065317154 + 50.0 * 6.211364269256592
Epoch 2450, val loss: 1.9573429822921753
Epoch 2460, training loss: 310.61761474609375 = 0.041060302406549454 + 50.0 * 6.211531639099121
Epoch 2460, val loss: 1.9622031450271606
Epoch 2470, training loss: 310.55181884765625 = 0.04040993005037308 + 50.0 * 6.210228443145752
Epoch 2470, val loss: 1.967738151550293
Epoch 2480, training loss: 310.5425109863281 = 0.03978251293301582 + 50.0 * 6.210054397583008
Epoch 2480, val loss: 1.973082423210144
Epoch 2490, training loss: 310.6231689453125 = 0.03915484622120857 + 50.0 * 6.2116804122924805
Epoch 2490, val loss: 1.9781378507614136
Epoch 2500, training loss: 310.5045166015625 = 0.03853233531117439 + 50.0 * 6.209319591522217
Epoch 2500, val loss: 1.9832816123962402
Epoch 2510, training loss: 310.4903259277344 = 0.03793414309620857 + 50.0 * 6.209047794342041
Epoch 2510, val loss: 1.9887161254882812
Epoch 2520, training loss: 310.4494323730469 = 0.037353236228227615 + 50.0 * 6.2082414627075195
Epoch 2520, val loss: 1.9939414262771606
Epoch 2530, training loss: 310.7210693359375 = 0.036810509860515594 + 50.0 * 6.213685512542725
Epoch 2530, val loss: 1.9988667964935303
Epoch 2540, training loss: 310.6752014160156 = 0.03622034564614296 + 50.0 * 6.212779521942139
Epoch 2540, val loss: 2.0041935443878174
Epoch 2550, training loss: 310.5712585449219 = 0.035637278109788895 + 50.0 * 6.210712432861328
Epoch 2550, val loss: 2.0083298683166504
Epoch 2560, training loss: 310.4849548339844 = 0.0350927971303463 + 50.0 * 6.2089972496032715
Epoch 2560, val loss: 2.0141592025756836
Epoch 2570, training loss: 310.3915710449219 = 0.03457331657409668 + 50.0 * 6.20713996887207
Epoch 2570, val loss: 2.019345760345459
Epoch 2580, training loss: 310.3863525390625 = 0.034065838903188705 + 50.0 * 6.207046031951904
Epoch 2580, val loss: 2.0245361328125
Epoch 2590, training loss: 310.600341796875 = 0.0335858017206192 + 50.0 * 6.211334705352783
Epoch 2590, val loss: 2.0296216011047363
Epoch 2600, training loss: 310.40338134765625 = 0.03307027369737625 + 50.0 * 6.207406044006348
Epoch 2600, val loss: 2.0343761444091797
Epoch 2610, training loss: 310.52301025390625 = 0.032574642449617386 + 50.0 * 6.209808826446533
Epoch 2610, val loss: 2.0387589931488037
Epoch 2620, training loss: 310.39971923828125 = 0.03207625076174736 + 50.0 * 6.207352638244629
Epoch 2620, val loss: 2.043673276901245
Epoch 2630, training loss: 310.4031677246094 = 0.03162052109837532 + 50.0 * 6.207431316375732
Epoch 2630, val loss: 2.0485713481903076
Epoch 2640, training loss: 310.4411926269531 = 0.03115796484053135 + 50.0 * 6.208200454711914
Epoch 2640, val loss: 2.0532774925231934
Epoch 2650, training loss: 310.4700012207031 = 0.030712014064192772 + 50.0 * 6.2087860107421875
Epoch 2650, val loss: 2.0581836700439453
Epoch 2660, training loss: 310.40216064453125 = 0.030275341123342514 + 50.0 * 6.207437992095947
Epoch 2660, val loss: 2.0633552074432373
Epoch 2670, training loss: 310.376220703125 = 0.029840722680091858 + 50.0 * 6.20692777633667
Epoch 2670, val loss: 2.0675089359283447
Epoch 2680, training loss: 310.42059326171875 = 0.02943163737654686 + 50.0 * 6.207823753356934
Epoch 2680, val loss: 2.0728843212127686
Epoch 2690, training loss: 310.3522033691406 = 0.029011178761720657 + 50.0 * 6.20646333694458
Epoch 2690, val loss: 2.0770936012268066
Epoch 2700, training loss: 310.42047119140625 = 0.028603801503777504 + 50.0 * 6.2078375816345215
Epoch 2700, val loss: 2.0816400051116943
Epoch 2710, training loss: 310.3904724121094 = 0.028204498812556267 + 50.0 * 6.207245349884033
Epoch 2710, val loss: 2.0863494873046875
Epoch 2720, training loss: 310.33306884765625 = 0.02779989317059517 + 50.0 * 6.2061052322387695
Epoch 2720, val loss: 2.0907838344573975
Epoch 2730, training loss: 310.38580322265625 = 0.027419615536928177 + 50.0 * 6.207168102264404
Epoch 2730, val loss: 2.0954220294952393
Epoch 2740, training loss: 310.2978210449219 = 0.02704351209104061 + 50.0 * 6.205415725708008
Epoch 2740, val loss: 2.0999326705932617
Epoch 2750, training loss: 310.2988586425781 = 0.026671595871448517 + 50.0 * 6.205443859100342
Epoch 2750, val loss: 2.1042628288269043
Epoch 2760, training loss: 310.4665222167969 = 0.026318278163671494 + 50.0 * 6.208804130554199
Epoch 2760, val loss: 2.108786106109619
Epoch 2770, training loss: 310.38726806640625 = 0.025969425216317177 + 50.0 * 6.207225799560547
Epoch 2770, val loss: 2.113037586212158
Epoch 2780, training loss: 310.3388671875 = 0.02560793235898018 + 50.0 * 6.206264972686768
Epoch 2780, val loss: 2.117342710494995
Epoch 2790, training loss: 310.32830810546875 = 0.025265511125326157 + 50.0 * 6.206060886383057
Epoch 2790, val loss: 2.1219074726104736
Epoch 2800, training loss: 310.2671813964844 = 0.02492154762148857 + 50.0 * 6.204844951629639
Epoch 2800, val loss: 2.1263532638549805
Epoch 2810, training loss: 310.3968811035156 = 0.0245957188308239 + 50.0 * 6.2074456214904785
Epoch 2810, val loss: 2.1306421756744385
Epoch 2820, training loss: 310.2393798828125 = 0.024279270321130753 + 50.0 * 6.204301834106445
Epoch 2820, val loss: 2.1350748538970947
Epoch 2830, training loss: 310.2422180175781 = 0.023963740095496178 + 50.0 * 6.204365253448486
Epoch 2830, val loss: 2.1391053199768066
Epoch 2840, training loss: 310.2734375 = 0.02365926094353199 + 50.0 * 6.204995632171631
Epoch 2840, val loss: 2.143345832824707
Epoch 2850, training loss: 310.3633117675781 = 0.02335192821919918 + 50.0 * 6.206799507141113
Epoch 2850, val loss: 2.1472201347351074
Epoch 2860, training loss: 310.4609680175781 = 0.023050012066960335 + 50.0 * 6.208758354187012
Epoch 2860, val loss: 2.1511001586914062
Epoch 2870, training loss: 310.34222412109375 = 0.022752758115530014 + 50.0 * 6.206389427185059
Epoch 2870, val loss: 2.155747413635254
Epoch 2880, training loss: 310.2132568359375 = 0.022445887327194214 + 50.0 * 6.203815937042236
Epoch 2880, val loss: 2.15963077545166
Epoch 2890, training loss: 310.1564636230469 = 0.02216721884906292 + 50.0 * 6.202686309814453
Epoch 2890, val loss: 2.1635677814483643
Epoch 2900, training loss: 310.2120666503906 = 0.021896686404943466 + 50.0 * 6.203803062438965
Epoch 2900, val loss: 2.167506217956543
Epoch 2910, training loss: 310.2878723144531 = 0.021627075970172882 + 50.0 * 6.205324649810791
Epoch 2910, val loss: 2.1715638637542725
Epoch 2920, training loss: 310.18670654296875 = 0.021354828029870987 + 50.0 * 6.203307151794434
Epoch 2920, val loss: 2.175614595413208
Epoch 2930, training loss: 310.41552734375 = 0.02110982872545719 + 50.0 * 6.207888126373291
Epoch 2930, val loss: 2.1796181201934814
Epoch 2940, training loss: 310.1813049316406 = 0.020823121070861816 + 50.0 * 6.20320987701416
Epoch 2940, val loss: 2.1828925609588623
Epoch 2950, training loss: 310.18133544921875 = 0.020561382174491882 + 50.0 * 6.203215599060059
Epoch 2950, val loss: 2.1865406036376953
Epoch 2960, training loss: 310.3763122558594 = 0.020317766815423965 + 50.0 * 6.207119941711426
Epoch 2960, val loss: 2.190627336502075
Epoch 2970, training loss: 310.22296142578125 = 0.02006208710372448 + 50.0 * 6.2040581703186035
Epoch 2970, val loss: 2.193443536758423
Epoch 2980, training loss: 310.111572265625 = 0.019818080589175224 + 50.0 * 6.2018351554870605
Epoch 2980, val loss: 2.1977508068084717
Epoch 2990, training loss: 310.0871887207031 = 0.01958257146179676 + 50.0 * 6.201352119445801
Epoch 2990, val loss: 2.2014763355255127
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6333333333333333
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 431.7669982910156 = 1.925462007522583 + 50.0 * 8.596830368041992
Epoch 0, val loss: 1.9166951179504395
Epoch 10, training loss: 431.7183837890625 = 1.9169400930404663 + 50.0 * 8.596029281616211
Epoch 10, val loss: 1.9087330102920532
Epoch 20, training loss: 431.4345397949219 = 1.9063513278961182 + 50.0 * 8.590563774108887
Epoch 20, val loss: 1.8985729217529297
Epoch 30, training loss: 429.56536865234375 = 1.8925580978393555 + 50.0 * 8.55345630645752
Epoch 30, val loss: 1.8850746154785156
Epoch 40, training loss: 417.5966796875 = 1.876261830329895 + 50.0 * 8.314408302307129
Epoch 40, val loss: 1.8693251609802246
Epoch 50, training loss: 382.4464416503906 = 1.8578367233276367 + 50.0 * 7.611772060394287
Epoch 50, val loss: 1.8518098592758179
Epoch 60, training loss: 371.7270202636719 = 1.8443905115127563 + 50.0 * 7.397652626037598
Epoch 60, val loss: 1.8398892879486084
Epoch 70, training loss: 359.4218444824219 = 1.835660696029663 + 50.0 * 7.151723384857178
Epoch 70, val loss: 1.831841230392456
Epoch 80, training loss: 352.1285095214844 = 1.8264507055282593 + 50.0 * 7.006041526794434
Epoch 80, val loss: 1.8232711553573608
Epoch 90, training loss: 346.2660827636719 = 1.8188817501068115 + 50.0 * 6.888944149017334
Epoch 90, val loss: 1.8167262077331543
Epoch 100, training loss: 342.1110534667969 = 1.8126883506774902 + 50.0 * 6.805967330932617
Epoch 100, val loss: 1.811301589012146
Epoch 110, training loss: 338.9110412597656 = 1.8069981336593628 + 50.0 * 6.7420806884765625
Epoch 110, val loss: 1.8059744834899902
Epoch 120, training loss: 335.9740905761719 = 1.8012216091156006 + 50.0 * 6.683457851409912
Epoch 120, val loss: 1.8004276752471924
Epoch 130, training loss: 333.70458984375 = 1.795669674873352 + 50.0 * 6.638177871704102
Epoch 130, val loss: 1.7951017618179321
Epoch 140, training loss: 332.03204345703125 = 1.7900241613388062 + 50.0 * 6.60483980178833
Epoch 140, val loss: 1.78951096534729
Epoch 150, training loss: 330.5816345214844 = 1.7838839292526245 + 50.0 * 6.575955390930176
Epoch 150, val loss: 1.7836530208587646
Epoch 160, training loss: 329.3485107421875 = 1.7774690389633179 + 50.0 * 6.55142068862915
Epoch 160, val loss: 1.777509093284607
Epoch 170, training loss: 328.2782287597656 = 1.7707053422927856 + 50.0 * 6.530150890350342
Epoch 170, val loss: 1.7711125612258911
Epoch 180, training loss: 327.43841552734375 = 1.7632807493209839 + 50.0 * 6.513503074645996
Epoch 180, val loss: 1.7641652822494507
Epoch 190, training loss: 326.7018127441406 = 1.7551301717758179 + 50.0 * 6.498933792114258
Epoch 190, val loss: 1.7566031217575073
Epoch 200, training loss: 326.0060119628906 = 1.746220588684082 + 50.0 * 6.485195636749268
Epoch 200, val loss: 1.7484381198883057
Epoch 210, training loss: 325.45880126953125 = 1.7365449666976929 + 50.0 * 6.474445343017578
Epoch 210, val loss: 1.7395923137664795
Epoch 220, training loss: 324.9207763671875 = 1.7260469198226929 + 50.0 * 6.463894367218018
Epoch 220, val loss: 1.730139136314392
Epoch 230, training loss: 324.306396484375 = 1.7148480415344238 + 50.0 * 6.451830863952637
Epoch 230, val loss: 1.720155954360962
Epoch 240, training loss: 323.8121643066406 = 1.7029027938842773 + 50.0 * 6.442184925079346
Epoch 240, val loss: 1.7095460891723633
Epoch 250, training loss: 323.3924560546875 = 1.6899313926696777 + 50.0 * 6.4340500831604
Epoch 250, val loss: 1.6981825828552246
Epoch 260, training loss: 322.94903564453125 = 1.676201343536377 + 50.0 * 6.425456523895264
Epoch 260, val loss: 1.6862596273422241
Epoch 270, training loss: 322.4964294433594 = 1.661752462387085 + 50.0 * 6.416693687438965
Epoch 270, val loss: 1.6737537384033203
Epoch 280, training loss: 322.07415771484375 = 1.6466065645217896 + 50.0 * 6.408551216125488
Epoch 280, val loss: 1.6608487367630005
Epoch 290, training loss: 321.6946716308594 = 1.6307889223098755 + 50.0 * 6.401277542114258
Epoch 290, val loss: 1.6475154161453247
Epoch 300, training loss: 321.37261962890625 = 1.6143842935562134 + 50.0 * 6.395164489746094
Epoch 300, val loss: 1.6338708400726318
Epoch 310, training loss: 321.3001403808594 = 1.5972003936767578 + 50.0 * 6.394058704376221
Epoch 310, val loss: 1.6198817491531372
Epoch 320, training loss: 320.7404479980469 = 1.579727053642273 + 50.0 * 6.383214950561523
Epoch 320, val loss: 1.6057592630386353
Epoch 330, training loss: 320.4764404296875 = 1.5620249509811401 + 50.0 * 6.378288745880127
Epoch 330, val loss: 1.5917755365371704
Epoch 340, training loss: 320.52264404296875 = 1.5440481901168823 + 50.0 * 6.37957239151001
Epoch 340, val loss: 1.5779778957366943
Epoch 350, training loss: 319.96417236328125 = 1.5259696245193481 + 50.0 * 6.3687639236450195
Epoch 350, val loss: 1.564277172088623
Epoch 360, training loss: 319.72064208984375 = 1.5079106092453003 + 50.0 * 6.364254951477051
Epoch 360, val loss: 1.5510517358779907
Epoch 370, training loss: 319.4572448730469 = 1.489944577217102 + 50.0 * 6.359345436096191
Epoch 370, val loss: 1.5382561683654785
Epoch 380, training loss: 319.2498779296875 = 1.472063422203064 + 50.0 * 6.355556011199951
Epoch 380, val loss: 1.5258451700210571
Epoch 390, training loss: 319.1705322265625 = 1.454160213470459 + 50.0 * 6.35432767868042
Epoch 390, val loss: 1.5137124061584473
Epoch 400, training loss: 318.8858642578125 = 1.4362621307373047 + 50.0 * 6.348992347717285
Epoch 400, val loss: 1.5020229816436768
Epoch 410, training loss: 318.6581115722656 = 1.4185429811477661 + 50.0 * 6.344791412353516
Epoch 410, val loss: 1.490753173828125
Epoch 420, training loss: 318.6259765625 = 1.4009087085723877 + 50.0 * 6.344501495361328
Epoch 420, val loss: 1.4797941446304321
Epoch 430, training loss: 318.2547302246094 = 1.383258581161499 + 50.0 * 6.337429046630859
Epoch 430, val loss: 1.4690214395523071
Epoch 440, training loss: 318.49554443359375 = 1.365610957145691 + 50.0 * 6.342598915100098
Epoch 440, val loss: 1.4586137533187866
Epoch 450, training loss: 318.01458740234375 = 1.348007321357727 + 50.0 * 6.33333158493042
Epoch 450, val loss: 1.448119044303894
Epoch 460, training loss: 317.79388427734375 = 1.3304524421691895 + 50.0 * 6.329268932342529
Epoch 460, val loss: 1.438024878501892
Epoch 470, training loss: 317.6165466308594 = 1.3129298686981201 + 50.0 * 6.3260722160339355
Epoch 470, val loss: 1.4282504320144653
Epoch 480, training loss: 317.6805419921875 = 1.2954280376434326 + 50.0 * 6.327702045440674
Epoch 480, val loss: 1.4183697700500488
Epoch 490, training loss: 317.3536376953125 = 1.2776440382003784 + 50.0 * 6.32151985168457
Epoch 490, val loss: 1.408652663230896
Epoch 500, training loss: 317.1421813964844 = 1.2600189447402954 + 50.0 * 6.317643642425537
Epoch 500, val loss: 1.3990060091018677
Epoch 510, training loss: 317.1549987792969 = 1.2424192428588867 + 50.0 * 6.318251132965088
Epoch 510, val loss: 1.3894487619400024
Epoch 520, training loss: 317.01727294921875 = 1.2247514724731445 + 50.0 * 6.315850257873535
Epoch 520, val loss: 1.3800803422927856
Epoch 530, training loss: 316.7551574707031 = 1.206946849822998 + 50.0 * 6.310964584350586
Epoch 530, val loss: 1.3706459999084473
Epoch 540, training loss: 316.857421875 = 1.1892839670181274 + 50.0 * 6.313363075256348
Epoch 540, val loss: 1.3616151809692383
Epoch 550, training loss: 316.5999450683594 = 1.1717302799224854 + 50.0 * 6.30856466293335
Epoch 550, val loss: 1.352299451828003
Epoch 560, training loss: 316.4019470214844 = 1.1541125774383545 + 50.0 * 6.304956912994385
Epoch 560, val loss: 1.3435570001602173
Epoch 570, training loss: 316.2832336425781 = 1.1365978717803955 + 50.0 * 6.3029327392578125
Epoch 570, val loss: 1.3346796035766602
Epoch 580, training loss: 316.4197692871094 = 1.1191565990447998 + 50.0 * 6.306012153625488
Epoch 580, val loss: 1.3260854482650757
Epoch 590, training loss: 316.1210021972656 = 1.1015516519546509 + 50.0 * 6.300388813018799
Epoch 590, val loss: 1.3174383640289307
Epoch 600, training loss: 315.9593200683594 = 1.0841219425201416 + 50.0 * 6.29750394821167
Epoch 600, val loss: 1.3092341423034668
Epoch 610, training loss: 315.8560485839844 = 1.0668408870697021 + 50.0 * 6.2957844734191895
Epoch 610, val loss: 1.3011749982833862
Epoch 620, training loss: 315.91754150390625 = 1.0495492219924927 + 50.0 * 6.297359466552734
Epoch 620, val loss: 1.2931787967681885
Epoch 630, training loss: 315.7053527832031 = 1.0323594808578491 + 50.0 * 6.293459892272949
Epoch 630, val loss: 1.285333514213562
Epoch 640, training loss: 315.56683349609375 = 1.0152653455734253 + 50.0 * 6.291031360626221
Epoch 640, val loss: 1.2777329683303833
Epoch 650, training loss: 315.4436950683594 = 0.9983424544334412 + 50.0 * 6.288906574249268
Epoch 650, val loss: 1.2703654766082764
Epoch 660, training loss: 315.5144958496094 = 0.9815949201583862 + 50.0 * 6.290657997131348
Epoch 660, val loss: 1.2634137868881226
Epoch 670, training loss: 315.53314208984375 = 0.9646764397621155 + 50.0 * 6.2913689613342285
Epoch 670, val loss: 1.2556527853012085
Epoch 680, training loss: 315.1956481933594 = 0.9478053450584412 + 50.0 * 6.284956932067871
Epoch 680, val loss: 1.2486435174942017
Epoch 690, training loss: 315.12005615234375 = 0.9313104152679443 + 50.0 * 6.2837748527526855
Epoch 690, val loss: 1.2419836521148682
Epoch 700, training loss: 315.0269470214844 = 0.9150387048721313 + 50.0 * 6.282238006591797
Epoch 700, val loss: 1.2354426383972168
Epoch 710, training loss: 315.1982421875 = 0.8988279700279236 + 50.0 * 6.2859883308410645
Epoch 710, val loss: 1.2288087606430054
Epoch 720, training loss: 314.9110412597656 = 0.8825032711029053 + 50.0 * 6.2805705070495605
Epoch 720, val loss: 1.2224594354629517
Epoch 730, training loss: 314.97119140625 = 0.8664525151252747 + 50.0 * 6.282094955444336
Epoch 730, val loss: 1.2163305282592773
Epoch 740, training loss: 314.79510498046875 = 0.8506860136985779 + 50.0 * 6.278888702392578
Epoch 740, val loss: 1.210415244102478
Epoch 750, training loss: 314.6424255371094 = 0.8350128531455994 + 50.0 * 6.276147842407227
Epoch 750, val loss: 1.2046664953231812
Epoch 760, training loss: 314.69647216796875 = 0.8196403980255127 + 50.0 * 6.277536869049072
Epoch 760, val loss: 1.1992979049682617
Epoch 770, training loss: 314.54571533203125 = 0.8043831586837769 + 50.0 * 6.274826526641846
Epoch 770, val loss: 1.193785548210144
Epoch 780, training loss: 314.45831298828125 = 0.7892351746559143 + 50.0 * 6.273381233215332
Epoch 780, val loss: 1.1887657642364502
Epoch 790, training loss: 314.6033020019531 = 0.7744430303573608 + 50.0 * 6.276576995849609
Epoch 790, val loss: 1.1835867166519165
Epoch 800, training loss: 314.4134826660156 = 0.759758472442627 + 50.0 * 6.273074626922607
Epoch 800, val loss: 1.1792174577713013
Epoch 810, training loss: 314.31683349609375 = 0.7452890276908875 + 50.0 * 6.271430969238281
Epoch 810, val loss: 1.1744920015335083
Epoch 820, training loss: 314.22430419921875 = 0.7311086058616638 + 50.0 * 6.269863605499268
Epoch 820, val loss: 1.170163869857788
Epoch 830, training loss: 314.2330627441406 = 0.7171717286109924 + 50.0 * 6.270318031311035
Epoch 830, val loss: 1.1662147045135498
Epoch 840, training loss: 314.1115417480469 = 0.7035380601882935 + 50.0 * 6.268159866333008
Epoch 840, val loss: 1.1622601747512817
Epoch 850, training loss: 314.1475524902344 = 0.6900222897529602 + 50.0 * 6.269150257110596
Epoch 850, val loss: 1.1584832668304443
Epoch 860, training loss: 313.9970397949219 = 0.6767411828041077 + 50.0 * 6.266406536102295
Epoch 860, val loss: 1.1548067331314087
Epoch 870, training loss: 314.1051330566406 = 0.6638636589050293 + 50.0 * 6.268825531005859
Epoch 870, val loss: 1.1516168117523193
Epoch 880, training loss: 313.8826904296875 = 0.6510270237922668 + 50.0 * 6.2646331787109375
Epoch 880, val loss: 1.1485518217086792
Epoch 890, training loss: 313.90924072265625 = 0.638616681098938 + 50.0 * 6.2654128074646
Epoch 890, val loss: 1.1458362340927124
Epoch 900, training loss: 313.74884033203125 = 0.6263043880462646 + 50.0 * 6.262450695037842
Epoch 900, val loss: 1.1431480646133423
Epoch 910, training loss: 313.677001953125 = 0.6143293976783752 + 50.0 * 6.261253356933594
Epoch 910, val loss: 1.1408787965774536
Epoch 920, training loss: 313.6347961425781 = 0.6026462912559509 + 50.0 * 6.2606425285339355
Epoch 920, val loss: 1.138979196548462
Epoch 930, training loss: 313.90673828125 = 0.5911687016487122 + 50.0 * 6.2663116455078125
Epoch 930, val loss: 1.1371411085128784
Epoch 940, training loss: 313.6858215332031 = 0.5797554850578308 + 50.0 * 6.262121677398682
Epoch 940, val loss: 1.1358115673065186
Epoch 950, training loss: 313.4729919433594 = 0.5686593055725098 + 50.0 * 6.258086681365967
Epoch 950, val loss: 1.1345463991165161
Epoch 960, training loss: 313.4830017089844 = 0.5579034090042114 + 50.0 * 6.258502006530762
Epoch 960, val loss: 1.1339367628097534
Epoch 970, training loss: 313.5364685058594 = 0.5472707748413086 + 50.0 * 6.25978422164917
Epoch 970, val loss: 1.1331499814987183
Epoch 980, training loss: 313.3822021484375 = 0.5367893576622009 + 50.0 * 6.256907939910889
Epoch 980, val loss: 1.1326569318771362
Epoch 990, training loss: 313.3255920410156 = 0.5266505479812622 + 50.0 * 6.255979061126709
Epoch 990, val loss: 1.132566213607788
Epoch 1000, training loss: 313.24591064453125 = 0.5168038010597229 + 50.0 * 6.254581928253174
Epoch 1000, val loss: 1.1326457262039185
Epoch 1010, training loss: 313.443115234375 = 0.5071640610694885 + 50.0 * 6.258718967437744
Epoch 1010, val loss: 1.1329209804534912
Epoch 1020, training loss: 313.3406982421875 = 0.49750980734825134 + 50.0 * 6.256864070892334
Epoch 1020, val loss: 1.1329940557479858
Epoch 1030, training loss: 313.23541259765625 = 0.48810523748397827 + 50.0 * 6.254946231842041
Epoch 1030, val loss: 1.1332967281341553
Epoch 1040, training loss: 313.1406555175781 = 0.4789266288280487 + 50.0 * 6.25323486328125
Epoch 1040, val loss: 1.1339526176452637
Epoch 1050, training loss: 313.0811462402344 = 0.46994706988334656 + 50.0 * 6.252223968505859
Epoch 1050, val loss: 1.1345586776733398
Epoch 1060, training loss: 313.2159729003906 = 0.4612228274345398 + 50.0 * 6.255095481872559
Epoch 1060, val loss: 1.1355421543121338
Epoch 1070, training loss: 313.1459045410156 = 0.45260941982269287 + 50.0 * 6.253865718841553
Epoch 1070, val loss: 1.1364191770553589
Epoch 1080, training loss: 313.0238037109375 = 0.44401815533638 + 50.0 * 6.251595497131348
Epoch 1080, val loss: 1.137336254119873
Epoch 1090, training loss: 312.9287414550781 = 0.4358277916908264 + 50.0 * 6.249858379364014
Epoch 1090, val loss: 1.1387995481491089
Epoch 1100, training loss: 312.90576171875 = 0.42779088020324707 + 50.0 * 6.24955940246582
Epoch 1100, val loss: 1.140058159828186
Epoch 1110, training loss: 312.987060546875 = 0.41993993520736694 + 50.0 * 6.251342296600342
Epoch 1110, val loss: 1.1417051553726196
Epoch 1120, training loss: 312.9469299316406 = 0.41211754083633423 + 50.0 * 6.250696659088135
Epoch 1120, val loss: 1.1431812047958374
Epoch 1130, training loss: 312.83306884765625 = 0.4044799208641052 + 50.0 * 6.248571872711182
Epoch 1130, val loss: 1.144629716873169
Epoch 1140, training loss: 312.77606201171875 = 0.3969976603984833 + 50.0 * 6.2475810050964355
Epoch 1140, val loss: 1.1460416316986084
Epoch 1150, training loss: 312.71905517578125 = 0.38978973031044006 + 50.0 * 6.246585369110107
Epoch 1150, val loss: 1.1484529972076416
Epoch 1160, training loss: 312.73992919921875 = 0.3827448785305023 + 50.0 * 6.247143745422363
Epoch 1160, val loss: 1.1504156589508057
Epoch 1170, training loss: 312.7130432128906 = 0.37575528025627136 + 50.0 * 6.246745586395264
Epoch 1170, val loss: 1.1523486375808716
Epoch 1180, training loss: 312.7900390625 = 0.36885038018226624 + 50.0 * 6.248424053192139
Epoch 1180, val loss: 1.1544748544692993
Epoch 1190, training loss: 312.66168212890625 = 0.3621116578578949 + 50.0 * 6.245991230010986
Epoch 1190, val loss: 1.1567578315734863
Epoch 1200, training loss: 312.6394958496094 = 0.3555584251880646 + 50.0 * 6.245678424835205
Epoch 1200, val loss: 1.1591283082962036
Epoch 1210, training loss: 312.57476806640625 = 0.34915095567703247 + 50.0 * 6.24451208114624
Epoch 1210, val loss: 1.1617802381515503
Epoch 1220, training loss: 312.4990539550781 = 0.3428557813167572 + 50.0 * 6.243123531341553
Epoch 1220, val loss: 1.1643632650375366
Epoch 1230, training loss: 312.5618896484375 = 0.33675462007522583 + 50.0 * 6.244502544403076
Epoch 1230, val loss: 1.1670782566070557
Epoch 1240, training loss: 312.5379638671875 = 0.33066996932029724 + 50.0 * 6.244146347045898
Epoch 1240, val loss: 1.170064926147461
Epoch 1250, training loss: 312.4023742675781 = 0.3246847093105316 + 50.0 * 6.241554260253906
Epoch 1250, val loss: 1.173154592514038
Epoch 1260, training loss: 312.4071350097656 = 0.3189079463481903 + 50.0 * 6.241764545440674
Epoch 1260, val loss: 1.1762027740478516
Epoch 1270, training loss: 312.51080322265625 = 0.3132137358188629 + 50.0 * 6.24395227432251
Epoch 1270, val loss: 1.1795028448104858
Epoch 1280, training loss: 312.43951416015625 = 0.3076271116733551 + 50.0 * 6.242637634277344
Epoch 1280, val loss: 1.1830552816390991
Epoch 1290, training loss: 312.31158447265625 = 0.30210739374160767 + 50.0 * 6.240189552307129
Epoch 1290, val loss: 1.186339259147644
Epoch 1300, training loss: 312.2604064941406 = 0.2967773973941803 + 50.0 * 6.239272594451904
Epoch 1300, val loss: 1.1899211406707764
Epoch 1310, training loss: 312.291015625 = 0.29156333208084106 + 50.0 * 6.239988803863525
Epoch 1310, val loss: 1.1937203407287598
Epoch 1320, training loss: 312.31103515625 = 0.28637251257896423 + 50.0 * 6.240493297576904
Epoch 1320, val loss: 1.1971241235733032
Epoch 1330, training loss: 312.2668762207031 = 0.28125280141830444 + 50.0 * 6.239712238311768
Epoch 1330, val loss: 1.2008875608444214
Epoch 1340, training loss: 312.28369140625 = 0.2762955129146576 + 50.0 * 6.240147590637207
Epoch 1340, val loss: 1.2050385475158691
Epoch 1350, training loss: 312.14544677734375 = 0.2713469862937927 + 50.0 * 6.237482070922852
Epoch 1350, val loss: 1.2083783149719238
Epoch 1360, training loss: 312.3720397949219 = 0.266605019569397 + 50.0 * 6.2421088218688965
Epoch 1360, val loss: 1.2123700380325317
Epoch 1370, training loss: 312.300537109375 = 0.261784166097641 + 50.0 * 6.240775108337402
Epoch 1370, val loss: 1.2166327238082886
Epoch 1380, training loss: 312.0709228515625 = 0.2570643424987793 + 50.0 * 6.2362775802612305
Epoch 1380, val loss: 1.2203856706619263
Epoch 1390, training loss: 312.0216064453125 = 0.2525690495967865 + 50.0 * 6.235381126403809
Epoch 1390, val loss: 1.2247369289398193
Epoch 1400, training loss: 311.9928894042969 = 0.24816355109214783 + 50.0 * 6.234894275665283
Epoch 1400, val loss: 1.2290281057357788
Epoch 1410, training loss: 312.0916748046875 = 0.24384233355522156 + 50.0 * 6.23695707321167
Epoch 1410, val loss: 1.233201265335083
Epoch 1420, training loss: 312.01007080078125 = 0.23947788774967194 + 50.0 * 6.235412120819092
Epoch 1420, val loss: 1.2377291917800903
Epoch 1430, training loss: 312.01898193359375 = 0.23520131409168243 + 50.0 * 6.235675811767578
Epoch 1430, val loss: 1.2417176961898804
Epoch 1440, training loss: 312.0547180175781 = 0.23104074597358704 + 50.0 * 6.236473560333252
Epoch 1440, val loss: 1.2461175918579102
Epoch 1450, training loss: 311.9259948730469 = 0.2269418090581894 + 50.0 * 6.233981609344482
Epoch 1450, val loss: 1.2507151365280151
Epoch 1460, training loss: 311.8833923339844 = 0.22292578220367432 + 50.0 * 6.233209133148193
Epoch 1460, val loss: 1.2552313804626465
Epoch 1470, training loss: 311.9377746582031 = 0.21902215480804443 + 50.0 * 6.234375
Epoch 1470, val loss: 1.2595688104629517
Epoch 1480, training loss: 311.90869140625 = 0.21511723101139069 + 50.0 * 6.2338714599609375
Epoch 1480, val loss: 1.2643089294433594
Epoch 1490, training loss: 311.84869384765625 = 0.2112504243850708 + 50.0 * 6.232748985290527
Epoch 1490, val loss: 1.268903136253357
Epoch 1500, training loss: 311.8417053222656 = 0.20749583840370178 + 50.0 * 6.232684135437012
Epoch 1500, val loss: 1.2734142541885376
Epoch 1510, training loss: 311.940673828125 = 0.20377331972122192 + 50.0 * 6.234738349914551
Epoch 1510, val loss: 1.2780972719192505
Epoch 1520, training loss: 311.7788391113281 = 0.20010949671268463 + 50.0 * 6.231575012207031
Epoch 1520, val loss: 1.2829222679138184
Epoch 1530, training loss: 311.7328186035156 = 0.19655472040176392 + 50.0 * 6.230725288391113
Epoch 1530, val loss: 1.2877962589263916
Epoch 1540, training loss: 311.86895751953125 = 0.1931021362543106 + 50.0 * 6.233516693115234
Epoch 1540, val loss: 1.2927908897399902
Epoch 1550, training loss: 311.7217712402344 = 0.18960663676261902 + 50.0 * 6.230643272399902
Epoch 1550, val loss: 1.297353744506836
Epoch 1560, training loss: 311.7511901855469 = 0.1862100511789322 + 50.0 * 6.23129940032959
Epoch 1560, val loss: 1.3024241924285889
Epoch 1570, training loss: 311.6654357910156 = 0.18285846710205078 + 50.0 * 6.22965145111084
Epoch 1570, val loss: 1.3072444200515747
Epoch 1580, training loss: 311.690673828125 = 0.17959608137607574 + 50.0 * 6.230221271514893
Epoch 1580, val loss: 1.3122656345367432
Epoch 1590, training loss: 311.76483154296875 = 0.1763601005077362 + 50.0 * 6.231769561767578
Epoch 1590, val loss: 1.3174514770507812
Epoch 1600, training loss: 311.78656005859375 = 0.1731717884540558 + 50.0 * 6.232268333435059
Epoch 1600, val loss: 1.3228927850723267
Epoch 1610, training loss: 311.598388671875 = 0.17002010345458984 + 50.0 * 6.228567600250244
Epoch 1610, val loss: 1.3277524709701538
Epoch 1620, training loss: 311.58563232421875 = 0.16697874665260315 + 50.0 * 6.228372573852539
Epoch 1620, val loss: 1.3335014581680298
Epoch 1630, training loss: 311.72418212890625 = 0.16397792100906372 + 50.0 * 6.231204509735107
Epoch 1630, val loss: 1.3384791612625122
Epoch 1640, training loss: 311.7297058105469 = 0.16097283363342285 + 50.0 * 6.231374740600586
Epoch 1640, val loss: 1.3430026769638062
Epoch 1650, training loss: 311.508056640625 = 0.15800727903842926 + 50.0 * 6.227000713348389
Epoch 1650, val loss: 1.3492510318756104
Epoch 1660, training loss: 311.47259521484375 = 0.1551600694656372 + 50.0 * 6.226348876953125
Epoch 1660, val loss: 1.3543766736984253
Epoch 1670, training loss: 311.4525146484375 = 0.15238502621650696 + 50.0 * 6.2260026931762695
Epoch 1670, val loss: 1.3599696159362793
Epoch 1680, training loss: 311.4764099121094 = 0.14965856075286865 + 50.0 * 6.226534843444824
Epoch 1680, val loss: 1.3653481006622314
Epoch 1690, training loss: 311.7897644042969 = 0.14692550897598267 + 50.0 * 6.2328572273254395
Epoch 1690, val loss: 1.3710254430770874
Epoch 1700, training loss: 311.4576721191406 = 0.14415201544761658 + 50.0 * 6.2262701988220215
Epoch 1700, val loss: 1.3755437135696411
Epoch 1710, training loss: 311.3797607421875 = 0.1415073722600937 + 50.0 * 6.224764823913574
Epoch 1710, val loss: 1.3820070028305054
Epoch 1720, training loss: 311.3788757324219 = 0.13897691667079926 + 50.0 * 6.224798202514648
Epoch 1720, val loss: 1.3875092267990112
Epoch 1730, training loss: 311.6182556152344 = 0.13651703298091888 + 50.0 * 6.229634761810303
Epoch 1730, val loss: 1.3934228420257568
Epoch 1740, training loss: 311.53363037109375 = 0.1339322030544281 + 50.0 * 6.227993488311768
Epoch 1740, val loss: 1.3983196020126343
Epoch 1750, training loss: 311.41754150390625 = 0.1314595341682434 + 50.0 * 6.22572135925293
Epoch 1750, val loss: 1.4042885303497314
Epoch 1760, training loss: 311.31134033203125 = 0.12907925248146057 + 50.0 * 6.223644733428955
Epoch 1760, val loss: 1.409982681274414
Epoch 1770, training loss: 311.27459716796875 = 0.12677721679210663 + 50.0 * 6.222956657409668
Epoch 1770, val loss: 1.4159842729568481
Epoch 1780, training loss: 311.5498046875 = 0.12452195584774017 + 50.0 * 6.228505611419678
Epoch 1780, val loss: 1.4218909740447998
Epoch 1790, training loss: 311.4295959472656 = 0.12217404693365097 + 50.0 * 6.22614860534668
Epoch 1790, val loss: 1.4267009496688843
Epoch 1800, training loss: 311.3135986328125 = 0.11991133540868759 + 50.0 * 6.223873615264893
Epoch 1800, val loss: 1.432816743850708
Epoch 1810, training loss: 311.2436828613281 = 0.11774170398712158 + 50.0 * 6.2225189208984375
Epoch 1810, val loss: 1.4388924837112427
Epoch 1820, training loss: 311.2759094238281 = 0.11563004553318024 + 50.0 * 6.22320556640625
Epoch 1820, val loss: 1.444766640663147
Epoch 1830, training loss: 311.3942565917969 = 0.1135263666510582 + 50.0 * 6.225614547729492
Epoch 1830, val loss: 1.4504505395889282
Epoch 1840, training loss: 311.2366638183594 = 0.11141546815633774 + 50.0 * 6.222504615783691
Epoch 1840, val loss: 1.4555785655975342
Epoch 1850, training loss: 311.1895751953125 = 0.10936322808265686 + 50.0 * 6.221603870391846
Epoch 1850, val loss: 1.4617444276809692
Epoch 1860, training loss: 311.14227294921875 = 0.10738634318113327 + 50.0 * 6.22069787979126
Epoch 1860, val loss: 1.4679310321807861
Epoch 1870, training loss: 311.13458251953125 = 0.10547465085983276 + 50.0 * 6.220582008361816
Epoch 1870, val loss: 1.4740558862686157
Epoch 1880, training loss: 311.5158996582031 = 0.1036185547709465 + 50.0 * 6.228245735168457
Epoch 1880, val loss: 1.4800403118133545
Epoch 1890, training loss: 311.2242736816406 = 0.10166214406490326 + 50.0 * 6.222452640533447
Epoch 1890, val loss: 1.484817385673523
Epoch 1900, training loss: 311.2243347167969 = 0.09983045607805252 + 50.0 * 6.222490310668945
Epoch 1900, val loss: 1.4910858869552612
Epoch 1910, training loss: 311.232421875 = 0.09800796210765839 + 50.0 * 6.222687721252441
Epoch 1910, val loss: 1.4965177774429321
Epoch 1920, training loss: 311.1302185058594 = 0.09622067958116531 + 50.0 * 6.220679759979248
Epoch 1920, val loss: 1.5029613971710205
Epoch 1930, training loss: 311.07354736328125 = 0.09450767934322357 + 50.0 * 6.21958065032959
Epoch 1930, val loss: 1.5086556673049927
Epoch 1940, training loss: 311.31048583984375 = 0.09284462779760361 + 50.0 * 6.224353313446045
Epoch 1940, val loss: 1.5145014524459839
Epoch 1950, training loss: 311.0597229003906 = 0.09113001823425293 + 50.0 * 6.219371795654297
Epoch 1950, val loss: 1.5208909511566162
Epoch 1960, training loss: 311.07025146484375 = 0.0894913449883461 + 50.0 * 6.219615459442139
Epoch 1960, val loss: 1.52637779712677
Epoch 1970, training loss: 311.04833984375 = 0.08790156990289688 + 50.0 * 6.21920919418335
Epoch 1970, val loss: 1.5328294038772583
Epoch 1980, training loss: 310.9835205078125 = 0.08633703738451004 + 50.0 * 6.2179436683654785
Epoch 1980, val loss: 1.538082242012024
Epoch 1990, training loss: 311.2108459472656 = 0.08483565598726273 + 50.0 * 6.222519874572754
Epoch 1990, val loss: 1.5437767505645752
Epoch 2000, training loss: 311.0720520019531 = 0.08328881859779358 + 50.0 * 6.219775676727295
Epoch 2000, val loss: 1.5497928857803345
Epoch 2010, training loss: 311.01361083984375 = 0.08181705325841904 + 50.0 * 6.2186360359191895
Epoch 2010, val loss: 1.555796504020691
Epoch 2020, training loss: 311.0010070800781 = 0.08038297295570374 + 50.0 * 6.218412399291992
Epoch 2020, val loss: 1.5617320537567139
Epoch 2030, training loss: 311.05621337890625 = 0.07899229973554611 + 50.0 * 6.219544887542725
Epoch 2030, val loss: 1.5678598880767822
Epoch 2040, training loss: 311.0119934082031 = 0.07758191972970963 + 50.0 * 6.218688488006592
Epoch 2040, val loss: 1.5729011297225952
Epoch 2050, training loss: 311.0183410644531 = 0.07622772455215454 + 50.0 * 6.218842029571533
Epoch 2050, val loss: 1.5784541368484497
Epoch 2060, training loss: 311.1551208496094 = 0.074901282787323 + 50.0 * 6.221604347229004
Epoch 2060, val loss: 1.584306001663208
Epoch 2070, training loss: 310.957763671875 = 0.07356292009353638 + 50.0 * 6.217683792114258
Epoch 2070, val loss: 1.5905078649520874
Epoch 2080, training loss: 310.8894348144531 = 0.072306327521801 + 50.0 * 6.216342926025391
Epoch 2080, val loss: 1.5962172746658325
Epoch 2090, training loss: 310.9816589355469 = 0.0710742399096489 + 50.0 * 6.218211650848389
Epoch 2090, val loss: 1.6017887592315674
Epoch 2100, training loss: 310.8421630859375 = 0.06983815878629684 + 50.0 * 6.215446472167969
Epoch 2100, val loss: 1.607666015625
Epoch 2110, training loss: 310.8629150390625 = 0.06865736842155457 + 50.0 * 6.215885162353516
Epoch 2110, val loss: 1.613862156867981
Epoch 2120, training loss: 310.84930419921875 = 0.06749436259269714 + 50.0 * 6.215635776519775
Epoch 2120, val loss: 1.619599461555481
Epoch 2130, training loss: 311.3816223144531 = 0.06636014580726624 + 50.0 * 6.2263054847717285
Epoch 2130, val loss: 1.6246185302734375
Epoch 2140, training loss: 310.8497314453125 = 0.06513052433729172 + 50.0 * 6.215692043304443
Epoch 2140, val loss: 1.6297059059143066
Epoch 2150, training loss: 310.8544616699219 = 0.06401225924491882 + 50.0 * 6.215808868408203
Epoch 2150, val loss: 1.635443091392517
Epoch 2160, training loss: 310.7695007324219 = 0.06295479089021683 + 50.0 * 6.214130878448486
Epoch 2160, val loss: 1.641680121421814
Epoch 2170, training loss: 310.7547912597656 = 0.06192855164408684 + 50.0 * 6.213857173919678
Epoch 2170, val loss: 1.6475847959518433
Epoch 2180, training loss: 311.04931640625 = 0.060926392674446106 + 50.0 * 6.219768047332764
Epoch 2180, val loss: 1.6531028747558594
Epoch 2190, training loss: 310.98541259765625 = 0.059882089495658875 + 50.0 * 6.218510627746582
Epoch 2190, val loss: 1.6577234268188477
Epoch 2200, training loss: 310.7961730957031 = 0.05884247645735741 + 50.0 * 6.214746952056885
Epoch 2200, val loss: 1.6631208658218384
Epoch 2210, training loss: 310.7439880371094 = 0.05787644162774086 + 50.0 * 6.2137227058410645
Epoch 2210, val loss: 1.6689380407333374
Epoch 2220, training loss: 310.71466064453125 = 0.056944090873003006 + 50.0 * 6.213154315948486
Epoch 2220, val loss: 1.6748182773590088
Epoch 2230, training loss: 310.81390380859375 = 0.05603618174791336 + 50.0 * 6.215157508850098
Epoch 2230, val loss: 1.6806364059448242
Epoch 2240, training loss: 310.8453063964844 = 0.05510886386036873 + 50.0 * 6.215804100036621
Epoch 2240, val loss: 1.6855744123458862
Epoch 2250, training loss: 310.7260437011719 = 0.05419040843844414 + 50.0 * 6.213437080383301
Epoch 2250, val loss: 1.6902979612350464
Epoch 2260, training loss: 311.0152893066406 = 0.05333247408270836 + 50.0 * 6.219238758087158
Epoch 2260, val loss: 1.6959129571914673
Epoch 2270, training loss: 310.779296875 = 0.05242612212896347 + 50.0 * 6.214537620544434
Epoch 2270, val loss: 1.7011620998382568
Epoch 2280, training loss: 310.6944885253906 = 0.05158185586333275 + 50.0 * 6.212858200073242
Epoch 2280, val loss: 1.7064135074615479
Epoch 2290, training loss: 310.6419372558594 = 0.050772007554769516 + 50.0 * 6.211822986602783
Epoch 2290, val loss: 1.7118645906448364
Epoch 2300, training loss: 310.7222595214844 = 0.04998167231678963 + 50.0 * 6.21344518661499
Epoch 2300, val loss: 1.7173120975494385
Epoch 2310, training loss: 310.76593017578125 = 0.049190498888492584 + 50.0 * 6.214334964752197
Epoch 2310, val loss: 1.7219316959381104
Epoch 2320, training loss: 310.7381286621094 = 0.04840880259871483 + 50.0 * 6.213794708251953
Epoch 2320, val loss: 1.7280182838439941
Epoch 2330, training loss: 310.7947692871094 = 0.04764338210225105 + 50.0 * 6.214942455291748
Epoch 2330, val loss: 1.7327831983566284
Epoch 2340, training loss: 310.6279296875 = 0.04688588157296181 + 50.0 * 6.211621284484863
Epoch 2340, val loss: 1.737439751625061
Epoch 2350, training loss: 310.63641357421875 = 0.04616690054535866 + 50.0 * 6.2118048667907715
Epoch 2350, val loss: 1.74283766746521
Epoch 2360, training loss: 310.6981506347656 = 0.04546535760164261 + 50.0 * 6.2130537033081055
Epoch 2360, val loss: 1.7480413913726807
Epoch 2370, training loss: 310.70458984375 = 0.0447600893676281 + 50.0 * 6.213196277618408
Epoch 2370, val loss: 1.7525687217712402
Epoch 2380, training loss: 310.7455139160156 = 0.04406067356467247 + 50.0 * 6.214028835296631
Epoch 2380, val loss: 1.7572500705718994
Epoch 2390, training loss: 310.61627197265625 = 0.043367765843868256 + 50.0 * 6.211458206176758
Epoch 2390, val loss: 1.761834979057312
Epoch 2400, training loss: 310.54595947265625 = 0.04271250218153 + 50.0 * 6.210064888000488
Epoch 2400, val loss: 1.76772940158844
Epoch 2410, training loss: 310.5205993652344 = 0.042079728096723557 + 50.0 * 6.209570407867432
Epoch 2410, val loss: 1.772762656211853
Epoch 2420, training loss: 310.74688720703125 = 0.041469670832157135 + 50.0 * 6.214108467102051
Epoch 2420, val loss: 1.7779648303985596
Epoch 2430, training loss: 310.57110595703125 = 0.04082130640745163 + 50.0 * 6.210605621337891
Epoch 2430, val loss: 1.7810983657836914
Epoch 2440, training loss: 310.5417785644531 = 0.0402054488658905 + 50.0 * 6.210031986236572
Epoch 2440, val loss: 1.7862058877944946
Epoch 2450, training loss: 310.554931640625 = 0.039607785642147064 + 50.0 * 6.210306167602539
Epoch 2450, val loss: 1.791103482246399
Epoch 2460, training loss: 310.5362548828125 = 0.03902950510382652 + 50.0 * 6.209944725036621
Epoch 2460, val loss: 1.7956398725509644
Epoch 2470, training loss: 310.73858642578125 = 0.038460809737443924 + 50.0 * 6.21400260925293
Epoch 2470, val loss: 1.8002009391784668
Epoch 2480, training loss: 310.5025634765625 = 0.03788495808839798 + 50.0 * 6.209293842315674
Epoch 2480, val loss: 1.8052979707717896
Epoch 2490, training loss: 310.487548828125 = 0.037332601845264435 + 50.0 * 6.2090044021606445
Epoch 2490, val loss: 1.8096461296081543
Epoch 2500, training loss: 310.59613037109375 = 0.03680460527539253 + 50.0 * 6.211186408996582
Epoch 2500, val loss: 1.8143049478530884
Epoch 2510, training loss: 310.53460693359375 = 0.03626324608922005 + 50.0 * 6.209967136383057
Epoch 2510, val loss: 1.818494200706482
Epoch 2520, training loss: 310.5923156738281 = 0.03574180230498314 + 50.0 * 6.211131572723389
Epoch 2520, val loss: 1.822989821434021
Epoch 2530, training loss: 310.5215759277344 = 0.035232435911893845 + 50.0 * 6.2097272872924805
Epoch 2530, val loss: 1.8278485536575317
Epoch 2540, training loss: 310.4244384765625 = 0.03473099321126938 + 50.0 * 6.207794189453125
Epoch 2540, val loss: 1.8321479558944702
Epoch 2550, training loss: 310.45172119140625 = 0.034252967685461044 + 50.0 * 6.208349704742432
Epoch 2550, val loss: 1.8371148109436035
Epoch 2560, training loss: 310.5136413574219 = 0.03378327190876007 + 50.0 * 6.209597110748291
Epoch 2560, val loss: 1.8417648077011108
Epoch 2570, training loss: 310.58099365234375 = 0.033309970051050186 + 50.0 * 6.210953235626221
Epoch 2570, val loss: 1.8450753688812256
Epoch 2580, training loss: 310.5888671875 = 0.03282894566655159 + 50.0 * 6.21112060546875
Epoch 2580, val loss: 1.849818229675293
Epoch 2590, training loss: 310.42364501953125 = 0.03234775364398956 + 50.0 * 6.207826137542725
Epoch 2590, val loss: 1.8529304265975952
Epoch 2600, training loss: 310.38800048828125 = 0.03190929442644119 + 50.0 * 6.207121849060059
Epoch 2600, val loss: 1.8585093021392822
Epoch 2610, training loss: 310.4708251953125 = 0.03148871287703514 + 50.0 * 6.208786487579346
Epoch 2610, val loss: 1.862246036529541
Epoch 2620, training loss: 310.4651794433594 = 0.03105330653488636 + 50.0 * 6.208682537078857
Epoch 2620, val loss: 1.8665333986282349
Epoch 2630, training loss: 310.3678283691406 = 0.03061007149517536 + 50.0 * 6.20674467086792
Epoch 2630, val loss: 1.8703042268753052
Epoch 2640, training loss: 310.3369140625 = 0.03019862435758114 + 50.0 * 6.206133842468262
Epoch 2640, val loss: 1.874851942062378
Epoch 2650, training loss: 310.447021484375 = 0.029809055849909782 + 50.0 * 6.208343982696533
Epoch 2650, val loss: 1.8786163330078125
Epoch 2660, training loss: 310.4867858886719 = 0.029406795278191566 + 50.0 * 6.2091474533081055
Epoch 2660, val loss: 1.8828614950180054
Epoch 2670, training loss: 310.3866882324219 = 0.02899804152548313 + 50.0 * 6.207153797149658
Epoch 2670, val loss: 1.887142300605774
Epoch 2680, training loss: 310.34332275390625 = 0.02861398458480835 + 50.0 * 6.206294059753418
Epoch 2680, val loss: 1.891430139541626
Epoch 2690, training loss: 310.3753967285156 = 0.028251376003026962 + 50.0 * 6.206942558288574
Epoch 2690, val loss: 1.895692229270935
Epoch 2700, training loss: 310.5242004394531 = 0.027887770906090736 + 50.0 * 6.209926128387451
Epoch 2700, val loss: 1.8995391130447388
Epoch 2710, training loss: 310.3540954589844 = 0.02751098945736885 + 50.0 * 6.206532001495361
Epoch 2710, val loss: 1.9032753705978394
Epoch 2720, training loss: 310.344970703125 = 0.027158720418810844 + 50.0 * 6.206356048583984
Epoch 2720, val loss: 1.9070204496383667
Epoch 2730, training loss: 310.4820861816406 = 0.02681533433496952 + 50.0 * 6.209105491638184
Epoch 2730, val loss: 1.911035418510437
Epoch 2740, training loss: 310.3482666015625 = 0.026454493403434753 + 50.0 * 6.2064361572265625
Epoch 2740, val loss: 1.9149858951568604
Epoch 2750, training loss: 310.4073791503906 = 0.02611868642270565 + 50.0 * 6.207625389099121
Epoch 2750, val loss: 1.9190818071365356
Epoch 2760, training loss: 310.2938537597656 = 0.02578073926270008 + 50.0 * 6.205361366271973
Epoch 2760, val loss: 1.9234215021133423
Epoch 2770, training loss: 310.2998962402344 = 0.025457262992858887 + 50.0 * 6.205489158630371
Epoch 2770, val loss: 1.9265408515930176
Epoch 2780, training loss: 310.3340759277344 = 0.025146961212158203 + 50.0 * 6.206178665161133
Epoch 2780, val loss: 1.9305695295333862
Epoch 2790, training loss: 310.38818359375 = 0.024839691817760468 + 50.0 * 6.207266807556152
Epoch 2790, val loss: 1.934335470199585
Epoch 2800, training loss: 310.3522644042969 = 0.02452019974589348 + 50.0 * 6.206554889678955
Epoch 2800, val loss: 1.938522458076477
Epoch 2810, training loss: 310.2469787597656 = 0.02421852946281433 + 50.0 * 6.2044548988342285
Epoch 2810, val loss: 1.9419512748718262
Epoch 2820, training loss: 310.37689208984375 = 0.02393263950943947 + 50.0 * 6.207059383392334
Epoch 2820, val loss: 1.9457374811172485
Epoch 2830, training loss: 310.2479248046875 = 0.02362532913684845 + 50.0 * 6.20448637008667
Epoch 2830, val loss: 1.9491584300994873
Epoch 2840, training loss: 310.276611328125 = 0.02334272302687168 + 50.0 * 6.2050652503967285
Epoch 2840, val loss: 1.9526138305664062
Epoch 2850, training loss: 310.362548828125 = 0.02306482382118702 + 50.0 * 6.206789493560791
Epoch 2850, val loss: 1.9562138319015503
Epoch 2860, training loss: 310.37115478515625 = 0.02278059907257557 + 50.0 * 6.206967830657959
Epoch 2860, val loss: 1.9596807956695557
Epoch 2870, training loss: 310.346435546875 = 0.022504232823848724 + 50.0 * 6.206478595733643
Epoch 2870, val loss: 1.9634103775024414
Epoch 2880, training loss: 310.24896240234375 = 0.02223092131316662 + 50.0 * 6.204535007476807
Epoch 2880, val loss: 1.9665123224258423
Epoch 2890, training loss: 310.46722412109375 = 0.02197052165865898 + 50.0 * 6.20890474319458
Epoch 2890, val loss: 1.970128059387207
Epoch 2900, training loss: 310.2401123046875 = 0.021703476086258888 + 50.0 * 6.2043681144714355
Epoch 2900, val loss: 1.9737604856491089
Epoch 2910, training loss: 310.1873474121094 = 0.021439746022224426 + 50.0 * 6.203317642211914
Epoch 2910, val loss: 1.9774084091186523
Epoch 2920, training loss: 310.1535949707031 = 0.021194322034716606 + 50.0 * 6.202648162841797
Epoch 2920, val loss: 1.9809993505477905
Epoch 2930, training loss: 310.17095947265625 = 0.020960353314876556 + 50.0 * 6.203000068664551
Epoch 2930, val loss: 1.9846512079238892
Epoch 2940, training loss: 310.4485168457031 = 0.020733976736664772 + 50.0 * 6.208555698394775
Epoch 2940, val loss: 1.9884512424468994
Epoch 2950, training loss: 310.2109069824219 = 0.020476028323173523 + 50.0 * 6.203808784484863
Epoch 2950, val loss: 1.9901046752929688
Epoch 2960, training loss: 310.1910400390625 = 0.020238421857357025 + 50.0 * 6.203416347503662
Epoch 2960, val loss: 1.9938386678695679
Epoch 2970, training loss: 310.317626953125 = 0.020012790337204933 + 50.0 * 6.205952167510986
Epoch 2970, val loss: 1.9964544773101807
Epoch 2980, training loss: 310.1266174316406 = 0.019779043272137642 + 50.0 * 6.202136993408203
Epoch 2980, val loss: 2.0009608268737793
Epoch 2990, training loss: 310.12615966796875 = 0.01956043392419815 + 50.0 * 6.202131748199463
Epoch 2990, val loss: 2.0042004585266113
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6444444444444445
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 431.7933349609375 = 1.9519894123077393 + 50.0 * 8.596826553344727
Epoch 0, val loss: 1.9511042833328247
Epoch 10, training loss: 431.74127197265625 = 1.9424371719360352 + 50.0 * 8.595976829528809
Epoch 10, val loss: 1.9418801069259644
Epoch 20, training loss: 431.4483337402344 = 1.930296778678894 + 50.0 * 8.590360641479492
Epoch 20, val loss: 1.9297142028808594
Epoch 30, training loss: 429.51702880859375 = 1.9142042398452759 + 50.0 * 8.552056312561035
Epoch 30, val loss: 1.9132286310195923
Epoch 40, training loss: 416.4458312988281 = 1.894891381263733 + 50.0 * 8.29101848602295
Epoch 40, val loss: 1.8940157890319824
Epoch 50, training loss: 380.2073669433594 = 1.8727601766586304 + 50.0 * 7.566691875457764
Epoch 50, val loss: 1.872674822807312
Epoch 60, training loss: 367.0891418457031 = 1.8592413663864136 + 50.0 * 7.304597854614258
Epoch 60, val loss: 1.860790491104126
Epoch 70, training loss: 356.3869323730469 = 1.8503884077072144 + 50.0 * 7.090730667114258
Epoch 70, val loss: 1.8524088859558105
Epoch 80, training loss: 348.4461364746094 = 1.8414466381072998 + 50.0 * 6.932093620300293
Epoch 80, val loss: 1.844070315361023
Epoch 90, training loss: 342.7369384765625 = 1.8342857360839844 + 50.0 * 6.818053245544434
Epoch 90, val loss: 1.8373806476593018
Epoch 100, training loss: 339.2027282714844 = 1.8277966976165771 + 50.0 * 6.747498512268066
Epoch 100, val loss: 1.8314225673675537
Epoch 110, training loss: 336.08367919921875 = 1.8217616081237793 + 50.0 * 6.685238838195801
Epoch 110, val loss: 1.8256442546844482
Epoch 120, training loss: 333.71771240234375 = 1.8163948059082031 + 50.0 * 6.638026237487793
Epoch 120, val loss: 1.820290446281433
Epoch 130, training loss: 331.6990966796875 = 1.8113341331481934 + 50.0 * 6.597755432128906
Epoch 130, val loss: 1.8153177499771118
Epoch 140, training loss: 330.0467224121094 = 1.8063232898712158 + 50.0 * 6.564807891845703
Epoch 140, val loss: 1.8105673789978027
Epoch 150, training loss: 328.9452209472656 = 1.8011319637298584 + 50.0 * 6.542881965637207
Epoch 150, val loss: 1.805690050125122
Epoch 160, training loss: 327.8697204589844 = 1.7954541444778442 + 50.0 * 6.521485805511475
Epoch 160, val loss: 1.8005661964416504
Epoch 170, training loss: 327.0177001953125 = 1.7893720865249634 + 50.0 * 6.504566669464111
Epoch 170, val loss: 1.7950499057769775
Epoch 180, training loss: 326.5140380859375 = 1.782776117324829 + 50.0 * 6.494625091552734
Epoch 180, val loss: 1.7891353368759155
Epoch 190, training loss: 325.5342712402344 = 1.7756779193878174 + 50.0 * 6.47517204284668
Epoch 190, val loss: 1.7828198671340942
Epoch 200, training loss: 324.92431640625 = 1.7679486274719238 + 50.0 * 6.463127136230469
Epoch 200, val loss: 1.775996446609497
Epoch 210, training loss: 324.3672180175781 = 1.759421944618225 + 50.0 * 6.452156066894531
Epoch 210, val loss: 1.7685884237289429
Epoch 220, training loss: 323.7352294921875 = 1.7499891519546509 + 50.0 * 6.439704895019531
Epoch 220, val loss: 1.7604236602783203
Epoch 230, training loss: 323.2192687988281 = 1.7396374940872192 + 50.0 * 6.429593086242676
Epoch 230, val loss: 1.7514503002166748
Epoch 240, training loss: 322.9542541503906 = 1.7282615900039673 + 50.0 * 6.4245195388793945
Epoch 240, val loss: 1.7415817975997925
Epoch 250, training loss: 322.3540954589844 = 1.7156596183776855 + 50.0 * 6.412768363952637
Epoch 250, val loss: 1.7307616472244263
Epoch 260, training loss: 321.9053039550781 = 1.701844334602356 + 50.0 * 6.404068946838379
Epoch 260, val loss: 1.7189502716064453
Epoch 270, training loss: 321.5395202636719 = 1.6868809461593628 + 50.0 * 6.397052764892578
Epoch 270, val loss: 1.706129550933838
Epoch 280, training loss: 321.1715393066406 = 1.6706714630126953 + 50.0 * 6.390017032623291
Epoch 280, val loss: 1.6923078298568726
Epoch 290, training loss: 321.00250244140625 = 1.65321946144104 + 50.0 * 6.3869853019714355
Epoch 290, val loss: 1.6775596141815186
Epoch 300, training loss: 320.5882568359375 = 1.6347014904022217 + 50.0 * 6.37907075881958
Epoch 300, val loss: 1.6619186401367188
Epoch 310, training loss: 320.26641845703125 = 1.6151126623153687 + 50.0 * 6.373026371002197
Epoch 310, val loss: 1.6455132961273193
Epoch 320, training loss: 320.02349853515625 = 1.5946519374847412 + 50.0 * 6.368576526641846
Epoch 320, val loss: 1.6285344362258911
Epoch 330, training loss: 319.82647705078125 = 1.5734937191009521 + 50.0 * 6.365059852600098
Epoch 330, val loss: 1.6111481189727783
Epoch 340, training loss: 319.56866455078125 = 1.5517139434814453 + 50.0 * 6.3603386878967285
Epoch 340, val loss: 1.5934020280838013
Epoch 350, training loss: 319.4040222167969 = 1.5295500755310059 + 50.0 * 6.357489585876465
Epoch 350, val loss: 1.5755841732025146
Epoch 360, training loss: 319.0940856933594 = 1.507234811782837 + 50.0 * 6.351737022399902
Epoch 360, val loss: 1.5578361749649048
Epoch 370, training loss: 318.85101318359375 = 1.4848295450210571 + 50.0 * 6.347323417663574
Epoch 370, val loss: 1.540188193321228
Epoch 380, training loss: 318.6669616699219 = 1.4625178575515747 + 50.0 * 6.344089031219482
Epoch 380, val loss: 1.522849202156067
Epoch 390, training loss: 318.6819152832031 = 1.440259337425232 + 50.0 * 6.3448333740234375
Epoch 390, val loss: 1.5057955980300903
Epoch 400, training loss: 318.32110595703125 = 1.4179840087890625 + 50.0 * 6.338062286376953
Epoch 400, val loss: 1.488897681236267
Epoch 410, training loss: 318.09869384765625 = 1.3961414098739624 + 50.0 * 6.33405065536499
Epoch 410, val loss: 1.4725635051727295
Epoch 420, training loss: 318.1127624511719 = 1.3745990991592407 + 50.0 * 6.334763050079346
Epoch 420, val loss: 1.4566831588745117
Epoch 430, training loss: 317.79107666015625 = 1.3532198667526245 + 50.0 * 6.328757286071777
Epoch 430, val loss: 1.4411317110061646
Epoch 440, training loss: 317.5859069824219 = 1.3321908712387085 + 50.0 * 6.325074195861816
Epoch 440, val loss: 1.426028847694397
Epoch 450, training loss: 317.6947326660156 = 1.3114280700683594 + 50.0 * 6.327666282653809
Epoch 450, val loss: 1.4113322496414185
Epoch 460, training loss: 317.3098449707031 = 1.2905564308166504 + 50.0 * 6.320385932922363
Epoch 460, val loss: 1.396758794784546
Epoch 470, training loss: 317.16400146484375 = 1.270065426826477 + 50.0 * 6.317878246307373
Epoch 470, val loss: 1.3827439546585083
Epoch 480, training loss: 317.02313232421875 = 1.2498160600662231 + 50.0 * 6.315466403961182
Epoch 480, val loss: 1.3691365718841553
Epoch 490, training loss: 317.1402282714844 = 1.229735016822815 + 50.0 * 6.318210124969482
Epoch 490, val loss: 1.3558381795883179
Epoch 500, training loss: 316.73211669921875 = 1.2097060680389404 + 50.0 * 6.310448169708252
Epoch 500, val loss: 1.3425370454788208
Epoch 510, training loss: 316.6469421386719 = 1.1899290084838867 + 50.0 * 6.309139728546143
Epoch 510, val loss: 1.329810619354248
Epoch 520, training loss: 316.732177734375 = 1.1703507900238037 + 50.0 * 6.311236381530762
Epoch 520, val loss: 1.317434310913086
Epoch 530, training loss: 316.3782958984375 = 1.150843858718872 + 50.0 * 6.304548740386963
Epoch 530, val loss: 1.3053892850875854
Epoch 540, training loss: 316.2743225097656 = 1.1315782070159912 + 50.0 * 6.302854537963867
Epoch 540, val loss: 1.2937726974487305
Epoch 550, training loss: 316.49267578125 = 1.112627387046814 + 50.0 * 6.307600975036621
Epoch 550, val loss: 1.28253972530365
Epoch 560, training loss: 316.13165283203125 = 1.0933493375778198 + 50.0 * 6.3007659912109375
Epoch 560, val loss: 1.271299958229065
Epoch 570, training loss: 315.98828125 = 1.0745699405670166 + 50.0 * 6.298274040222168
Epoch 570, val loss: 1.2607451677322388
Epoch 580, training loss: 315.897216796875 = 1.056050181388855 + 50.0 * 6.296823024749756
Epoch 580, val loss: 1.2505686283111572
Epoch 590, training loss: 315.83782958984375 = 1.0375638008117676 + 50.0 * 6.2960052490234375
Epoch 590, val loss: 1.2407249212265015
Epoch 600, training loss: 315.72027587890625 = 1.019194483757019 + 50.0 * 6.2940216064453125
Epoch 600, val loss: 1.231103539466858
Epoch 610, training loss: 315.5892333984375 = 1.001169204711914 + 50.0 * 6.29176139831543
Epoch 610, val loss: 1.2222288846969604
Epoch 620, training loss: 315.58795166015625 = 0.9833905696868896 + 50.0 * 6.292091369628906
Epoch 620, val loss: 1.2137552499771118
Epoch 630, training loss: 315.3857116699219 = 0.9656884074211121 + 50.0 * 6.288400173187256
Epoch 630, val loss: 1.2054047584533691
Epoch 640, training loss: 315.2991943359375 = 0.948239803314209 + 50.0 * 6.2870192527771
Epoch 640, val loss: 1.1975069046020508
Epoch 650, training loss: 315.55401611328125 = 0.9310407638549805 + 50.0 * 6.292459964752197
Epoch 650, val loss: 1.189958930015564
Epoch 660, training loss: 315.2215881347656 = 0.9137381315231323 + 50.0 * 6.286157131195068
Epoch 660, val loss: 1.1824193000793457
Epoch 670, training loss: 315.07366943359375 = 0.8969207406044006 + 50.0 * 6.283535003662109
Epoch 670, val loss: 1.1754690408706665
Epoch 680, training loss: 315.0142822265625 = 0.8803225159645081 + 50.0 * 6.282679557800293
Epoch 680, val loss: 1.168906807899475
Epoch 690, training loss: 314.9647521972656 = 0.8638368844985962 + 50.0 * 6.282018661499023
Epoch 690, val loss: 1.1622369289398193
Epoch 700, training loss: 314.8808898925781 = 0.8474442362785339 + 50.0 * 6.280669212341309
Epoch 700, val loss: 1.15599524974823
Epoch 710, training loss: 314.8289794921875 = 0.8314029574394226 + 50.0 * 6.279951572418213
Epoch 710, val loss: 1.1499366760253906
Epoch 720, training loss: 314.72283935546875 = 0.8154770135879517 + 50.0 * 6.278147220611572
Epoch 720, val loss: 1.1439560651779175
Epoch 730, training loss: 314.6733093261719 = 0.7997779846191406 + 50.0 * 6.277470588684082
Epoch 730, val loss: 1.1383603811264038
Epoch 740, training loss: 314.679443359375 = 0.7842382192611694 + 50.0 * 6.277904033660889
Epoch 740, val loss: 1.1327617168426514
Epoch 750, training loss: 314.55364990234375 = 0.7689067125320435 + 50.0 * 6.275694847106934
Epoch 750, val loss: 1.127261757850647
Epoch 760, training loss: 314.44427490234375 = 0.7538177967071533 + 50.0 * 6.27380895614624
Epoch 760, val loss: 1.1220862865447998
Epoch 770, training loss: 314.49578857421875 = 0.7390526533126831 + 50.0 * 6.275135040283203
Epoch 770, val loss: 1.1170947551727295
Epoch 780, training loss: 314.4159851074219 = 0.7242304086685181 + 50.0 * 6.273834705352783
Epoch 780, val loss: 1.1116546392440796
Epoch 790, training loss: 314.3183288574219 = 0.7096765637397766 + 50.0 * 6.272172927856445
Epoch 790, val loss: 1.1067869663238525
Epoch 800, training loss: 314.1738586425781 = 0.6953631043434143 + 50.0 * 6.2695698738098145
Epoch 800, val loss: 1.10198175907135
Epoch 810, training loss: 314.5151672363281 = 0.6814088821411133 + 50.0 * 6.276675701141357
Epoch 810, val loss: 1.0974894762039185
Epoch 820, training loss: 314.1614990234375 = 0.6674710512161255 + 50.0 * 6.269880294799805
Epoch 820, val loss: 1.092599868774414
Epoch 830, training loss: 314.0071105957031 = 0.6538650989532471 + 50.0 * 6.267064571380615
Epoch 830, val loss: 1.0884666442871094
Epoch 840, training loss: 313.9777526855469 = 0.6406514644622803 + 50.0 * 6.26674222946167
Epoch 840, val loss: 1.0842922925949097
Epoch 850, training loss: 314.099609375 = 0.6275320053100586 + 50.0 * 6.269441604614258
Epoch 850, val loss: 1.0800176858901978
Epoch 860, training loss: 313.830322265625 = 0.6146222949028015 + 50.0 * 6.264313697814941
Epoch 860, val loss: 1.0760635137557983
Epoch 870, training loss: 313.8089904785156 = 0.6020815968513489 + 50.0 * 6.264138221740723
Epoch 870, val loss: 1.0722522735595703
Epoch 880, training loss: 313.96466064453125 = 0.5898202657699585 + 50.0 * 6.2674970626831055
Epoch 880, val loss: 1.0688815116882324
Epoch 890, training loss: 313.75970458984375 = 0.5776463150978088 + 50.0 * 6.263641357421875
Epoch 890, val loss: 1.0650070905685425
Epoch 900, training loss: 313.6849365234375 = 0.5658157467842102 + 50.0 * 6.2623820304870605
Epoch 900, val loss: 1.0620566606521606
Epoch 910, training loss: 313.77471923828125 = 0.5542575716972351 + 50.0 * 6.264409065246582
Epoch 910, val loss: 1.0590722560882568
Epoch 920, training loss: 313.64013671875 = 0.5428656339645386 + 50.0 * 6.2619452476501465
Epoch 920, val loss: 1.0561496019363403
Epoch 930, training loss: 313.5338439941406 = 0.5316941142082214 + 50.0 * 6.260042667388916
Epoch 930, val loss: 1.0534193515777588
Epoch 940, training loss: 313.6490783691406 = 0.5208902359008789 + 50.0 * 6.262563705444336
Epoch 940, val loss: 1.0511114597320557
Epoch 950, training loss: 313.44647216796875 = 0.5101470351219177 + 50.0 * 6.258727073669434
Epoch 950, val loss: 1.049051284790039
Epoch 960, training loss: 313.3683776855469 = 0.499701589345932 + 50.0 * 6.257373809814453
Epoch 960, val loss: 1.0467907190322876
Epoch 970, training loss: 313.4921569824219 = 0.48952192068099976 + 50.0 * 6.260052680969238
Epoch 970, val loss: 1.0451136827468872
Epoch 980, training loss: 313.34588623046875 = 0.47941941022872925 + 50.0 * 6.257328987121582
Epoch 980, val loss: 1.0432597398757935
Epoch 990, training loss: 313.3064270019531 = 0.46957144141197205 + 50.0 * 6.256737232208252
Epoch 990, val loss: 1.042146921157837
Epoch 1000, training loss: 313.2099609375 = 0.46002551913261414 + 50.0 * 6.254998683929443
Epoch 1000, val loss: 1.0411014556884766
Epoch 1010, training loss: 313.46246337890625 = 0.45069438219070435 + 50.0 * 6.260235786437988
Epoch 1010, val loss: 1.0406370162963867
Epoch 1020, training loss: 313.2099609375 = 0.44136127829551697 + 50.0 * 6.255371570587158
Epoch 1020, val loss: 1.0399659872055054
Epoch 1030, training loss: 313.10845947265625 = 0.43239450454711914 + 50.0 * 6.253520965576172
Epoch 1030, val loss: 1.039688229560852
Epoch 1040, training loss: 313.138916015625 = 0.42362168431282043 + 50.0 * 6.254305839538574
Epoch 1040, val loss: 1.0399054288864136
Epoch 1050, training loss: 313.10357666015625 = 0.4149313271045685 + 50.0 * 6.253773212432861
Epoch 1050, val loss: 1.0396779775619507
Epoch 1060, training loss: 313.0913391113281 = 0.4063328504562378 + 50.0 * 6.253699779510498
Epoch 1060, val loss: 1.039433240890503
Epoch 1070, training loss: 312.96075439453125 = 0.3979979455471039 + 50.0 * 6.251255035400391
Epoch 1070, val loss: 1.039953589439392
Epoch 1080, training loss: 312.9622802734375 = 0.389853298664093 + 50.0 * 6.251448154449463
Epoch 1080, val loss: 1.040631890296936
Epoch 1090, training loss: 312.9379577636719 = 0.3818797469139099 + 50.0 * 6.251121997833252
Epoch 1090, val loss: 1.0414358377456665
Epoch 1100, training loss: 312.8069152832031 = 0.3740891218185425 + 50.0 * 6.248656749725342
Epoch 1100, val loss: 1.0424134731292725
Epoch 1110, training loss: 313.066162109375 = 0.36644992232322693 + 50.0 * 6.253993988037109
Epoch 1110, val loss: 1.0435863733291626
Epoch 1120, training loss: 312.8771057128906 = 0.3588210940361023 + 50.0 * 6.250365734100342
Epoch 1120, val loss: 1.0446841716766357
Epoch 1130, training loss: 312.76263427734375 = 0.3514036536216736 + 50.0 * 6.248224258422852
Epoch 1130, val loss: 1.0457199811935425
Epoch 1140, training loss: 312.6779479980469 = 0.3442196547985077 + 50.0 * 6.24667501449585
Epoch 1140, val loss: 1.0473670959472656
Epoch 1150, training loss: 313.1015930175781 = 0.3371311128139496 + 50.0 * 6.255289554595947
Epoch 1150, val loss: 1.0484576225280762
Epoch 1160, training loss: 312.7488708496094 = 0.33002644777297974 + 50.0 * 6.248376846313477
Epoch 1160, val loss: 1.050383448600769
Epoch 1170, training loss: 312.59613037109375 = 0.3231469392776489 + 50.0 * 6.24545955657959
Epoch 1170, val loss: 1.0522980690002441
Epoch 1180, training loss: 312.5718688964844 = 0.31649941205978394 + 50.0 * 6.245107173919678
Epoch 1180, val loss: 1.0542395114898682
Epoch 1190, training loss: 312.7554931640625 = 0.30993494391441345 + 50.0 * 6.248910903930664
Epoch 1190, val loss: 1.056137204170227
Epoch 1200, training loss: 312.6198425292969 = 0.3033905327320099 + 50.0 * 6.246328830718994
Epoch 1200, val loss: 1.0584216117858887
Epoch 1210, training loss: 312.6414489746094 = 0.29706114530563354 + 50.0 * 6.246887683868408
Epoch 1210, val loss: 1.061086893081665
Epoch 1220, training loss: 312.51837158203125 = 0.2907997965812683 + 50.0 * 6.244551658630371
Epoch 1220, val loss: 1.0635955333709717
Epoch 1230, training loss: 312.4397888183594 = 0.28473442792892456 + 50.0 * 6.243101119995117
Epoch 1230, val loss: 1.0665225982666016
Epoch 1240, training loss: 312.4382629394531 = 0.2787916362285614 + 50.0 * 6.243189811706543
Epoch 1240, val loss: 1.0692545175552368
Epoch 1250, training loss: 312.4482727050781 = 0.27294790744781494 + 50.0 * 6.24350643157959
Epoch 1250, val loss: 1.071797490119934
Epoch 1260, training loss: 312.41937255859375 = 0.2671375274658203 + 50.0 * 6.243044376373291
Epoch 1260, val loss: 1.074133038520813
Epoch 1270, training loss: 312.3620910644531 = 0.26150745153427124 + 50.0 * 6.242011547088623
Epoch 1270, val loss: 1.0777181386947632
Epoch 1280, training loss: 312.4073791503906 = 0.25598618388175964 + 50.0 * 6.243028163909912
Epoch 1280, val loss: 1.080502986907959
Epoch 1290, training loss: 312.3122253417969 = 0.25054195523262024 + 50.0 * 6.2412333488464355
Epoch 1290, val loss: 1.0835150480270386
Epoch 1300, training loss: 312.36480712890625 = 0.24527406692504883 + 50.0 * 6.2423906326293945
Epoch 1300, val loss: 1.086888313293457
Epoch 1310, training loss: 312.3129577636719 = 0.2400708943605423 + 50.0 * 6.241457462310791
Epoch 1310, val loss: 1.089571237564087
Epoch 1320, training loss: 312.19134521484375 = 0.23498915135860443 + 50.0 * 6.239127159118652
Epoch 1320, val loss: 1.0935444831848145
Epoch 1330, training loss: 312.13970947265625 = 0.23006568849086761 + 50.0 * 6.238193035125732
Epoch 1330, val loss: 1.0969324111938477
Epoch 1340, training loss: 312.12164306640625 = 0.22525428235530853 + 50.0 * 6.2379279136657715
Epoch 1340, val loss: 1.1002774238586426
Epoch 1350, training loss: 312.3271789550781 = 0.22055265307426453 + 50.0 * 6.242132663726807
Epoch 1350, val loss: 1.1041789054870605
Epoch 1360, training loss: 312.3119201660156 = 0.21581104397773743 + 50.0 * 6.241921901702881
Epoch 1360, val loss: 1.1079630851745605
Epoch 1370, training loss: 312.1211853027344 = 0.2111939787864685 + 50.0 * 6.2382001876831055
Epoch 1370, val loss: 1.1104737520217896
Epoch 1380, training loss: 312.0433044433594 = 0.20678846538066864 + 50.0 * 6.236730575561523
Epoch 1380, val loss: 1.1153819561004639
Epoch 1390, training loss: 312.3319396972656 = 0.20246140658855438 + 50.0 * 6.242589950561523
Epoch 1390, val loss: 1.118779182434082
Epoch 1400, training loss: 312.0514221191406 = 0.19814839959144592 + 50.0 * 6.237065315246582
Epoch 1400, val loss: 1.1227339506149292
Epoch 1410, training loss: 311.9510498046875 = 0.1940002143383026 + 50.0 * 6.235140800476074
Epoch 1410, val loss: 1.1268365383148193
Epoch 1420, training loss: 312.1429748535156 = 0.1899973601102829 + 50.0 * 6.2390594482421875
Epoch 1420, val loss: 1.1311732530593872
Epoch 1430, training loss: 312.0574645996094 = 0.1859358251094818 + 50.0 * 6.237430572509766
Epoch 1430, val loss: 1.1352444887161255
Epoch 1440, training loss: 311.92987060546875 = 0.18198353052139282 + 50.0 * 6.234958171844482
Epoch 1440, val loss: 1.138768196105957
Epoch 1450, training loss: 311.87158203125 = 0.17821720242500305 + 50.0 * 6.233867168426514
Epoch 1450, val loss: 1.1432452201843262
Epoch 1460, training loss: 311.8417663574219 = 0.17455962300300598 + 50.0 * 6.233344078063965
Epoch 1460, val loss: 1.1478201150894165
Epoch 1470, training loss: 312.054931640625 = 0.171005979180336 + 50.0 * 6.237678050994873
Epoch 1470, val loss: 1.152295708656311
Epoch 1480, training loss: 312.0159912109375 = 0.16734984517097473 + 50.0 * 6.236972808837891
Epoch 1480, val loss: 1.1558129787445068
Epoch 1490, training loss: 311.9769592285156 = 0.16385464370250702 + 50.0 * 6.236262321472168
Epoch 1490, val loss: 1.1607482433319092
Epoch 1500, training loss: 311.7852478027344 = 0.1603972464799881 + 50.0 * 6.232496738433838
Epoch 1500, val loss: 1.1649112701416016
Epoch 1510, training loss: 311.69915771484375 = 0.1571296900510788 + 50.0 * 6.230840682983398
Epoch 1510, val loss: 1.1694467067718506
Epoch 1520, training loss: 311.72430419921875 = 0.15394407510757446 + 50.0 * 6.231407642364502
Epoch 1520, val loss: 1.1741600036621094
Epoch 1530, training loss: 312.00146484375 = 0.1507977694272995 + 50.0 * 6.237013339996338
Epoch 1530, val loss: 1.178186058998108
Epoch 1540, training loss: 311.8386535644531 = 0.14766369760036469 + 50.0 * 6.233819484710693
Epoch 1540, val loss: 1.1840990781784058
Epoch 1550, training loss: 311.70458984375 = 0.14459878206253052 + 50.0 * 6.231199741363525
Epoch 1550, val loss: 1.187843918800354
Epoch 1560, training loss: 311.81549072265625 = 0.14169371128082275 + 50.0 * 6.233476161956787
Epoch 1560, val loss: 1.192978858947754
Epoch 1570, training loss: 311.6913757324219 = 0.138797789812088 + 50.0 * 6.231051445007324
Epoch 1570, val loss: 1.1981443166732788
Epoch 1580, training loss: 311.6110534667969 = 0.13597528636455536 + 50.0 * 6.229501247406006
Epoch 1580, val loss: 1.202717900276184
Epoch 1590, training loss: 311.6088562011719 = 0.13324296474456787 + 50.0 * 6.2295122146606445
Epoch 1590, val loss: 1.206916332244873
Epoch 1600, training loss: 311.9172058105469 = 0.13060255348682404 + 50.0 * 6.235732078552246
Epoch 1600, val loss: 1.2122639417648315
Epoch 1610, training loss: 311.6556091308594 = 0.1279229372739792 + 50.0 * 6.23055362701416
Epoch 1610, val loss: 1.2176908254623413
Epoch 1620, training loss: 311.5278625488281 = 0.12536871433258057 + 50.0 * 6.2280497550964355
Epoch 1620, val loss: 1.2222715616226196
Epoch 1630, training loss: 311.47308349609375 = 0.12290840595960617 + 50.0 * 6.227003574371338
Epoch 1630, val loss: 1.2275424003601074
Epoch 1640, training loss: 311.70489501953125 = 0.12051508575677872 + 50.0 * 6.231687545776367
Epoch 1640, val loss: 1.2323527336120605
Epoch 1650, training loss: 311.62677001953125 = 0.11809386312961578 + 50.0 * 6.230173587799072
Epoch 1650, val loss: 1.2381553649902344
Epoch 1660, training loss: 311.53857421875 = 0.11569449305534363 + 50.0 * 6.228457927703857
Epoch 1660, val loss: 1.2423666715621948
Epoch 1670, training loss: 311.45611572265625 = 0.11344014108181 + 50.0 * 6.226853847503662
Epoch 1670, val loss: 1.2481809854507446
Epoch 1680, training loss: 311.4253234863281 = 0.11126922070980072 + 50.0 * 6.22628116607666
Epoch 1680, val loss: 1.2532252073287964
Epoch 1690, training loss: 311.834228515625 = 0.10912977159023285 + 50.0 * 6.234501838684082
Epoch 1690, val loss: 1.2581450939178467
Epoch 1700, training loss: 311.5282287597656 = 0.10695049911737442 + 50.0 * 6.2284255027771
Epoch 1700, val loss: 1.2641092538833618
Epoch 1710, training loss: 311.38677978515625 = 0.10486644506454468 + 50.0 * 6.225638389587402
Epoch 1710, val loss: 1.268814206123352
Epoch 1720, training loss: 311.3407897949219 = 0.10289408266544342 + 50.0 * 6.224757671356201
Epoch 1720, val loss: 1.2743862867355347
Epoch 1730, training loss: 311.5164794921875 = 0.10096503794193268 + 50.0 * 6.228310585021973
Epoch 1730, val loss: 1.2787861824035645
Epoch 1740, training loss: 311.34332275390625 = 0.09899971634149551 + 50.0 * 6.224886417388916
Epoch 1740, val loss: 1.2850912809371948
Epoch 1750, training loss: 311.3614196777344 = 0.09709852933883667 + 50.0 * 6.225286483764648
Epoch 1750, val loss: 1.289501667022705
Epoch 1760, training loss: 311.38458251953125 = 0.0952632799744606 + 50.0 * 6.225786209106445
Epoch 1760, val loss: 1.2957335710525513
Epoch 1770, training loss: 311.2996826171875 = 0.09346504509449005 + 50.0 * 6.224124431610107
Epoch 1770, val loss: 1.3006101846694946
Epoch 1780, training loss: 311.31256103515625 = 0.0917416661977768 + 50.0 * 6.224416732788086
Epoch 1780, val loss: 1.3068430423736572
Epoch 1790, training loss: 311.4722595214844 = 0.0900334045290947 + 50.0 * 6.227644920349121
Epoch 1790, val loss: 1.3113682270050049
Epoch 1800, training loss: 311.3297424316406 = 0.08831112086772919 + 50.0 * 6.224828720092773
Epoch 1800, val loss: 1.317407250404358
Epoch 1810, training loss: 311.2710876464844 = 0.08663512766361237 + 50.0 * 6.223689079284668
Epoch 1810, val loss: 1.3225150108337402
Epoch 1820, training loss: 311.2127990722656 = 0.08506366610527039 + 50.0 * 6.222555160522461
Epoch 1820, val loss: 1.3282119035720825
Epoch 1830, training loss: 311.1710510253906 = 0.083530493080616 + 50.0 * 6.221750259399414
Epoch 1830, val loss: 1.3336048126220703
Epoch 1840, training loss: 311.7774963378906 = 0.08204048871994019 + 50.0 * 6.2339091300964355
Epoch 1840, val loss: 1.3382863998413086
Epoch 1850, training loss: 311.5209655761719 = 0.0804690420627594 + 50.0 * 6.228809833526611
Epoch 1850, val loss: 1.3449788093566895
Epoch 1860, training loss: 311.156982421875 = 0.07896961271762848 + 50.0 * 6.221560001373291
Epoch 1860, val loss: 1.3499075174331665
Epoch 1870, training loss: 311.1066589355469 = 0.07755738496780396 + 50.0 * 6.220582485198975
Epoch 1870, val loss: 1.3553160429000854
Epoch 1880, training loss: 311.1190490722656 = 0.07619884610176086 + 50.0 * 6.220856666564941
Epoch 1880, val loss: 1.3614192008972168
Epoch 1890, training loss: 311.4730224609375 = 0.0748644471168518 + 50.0 * 6.227962970733643
Epoch 1890, val loss: 1.3663549423217773
Epoch 1900, training loss: 311.25341796875 = 0.07346859574317932 + 50.0 * 6.223598957061768
Epoch 1900, val loss: 1.3719338178634644
Epoch 1910, training loss: 311.09063720703125 = 0.07215733826160431 + 50.0 * 6.220369815826416
Epoch 1910, val loss: 1.377486228942871
Epoch 1920, training loss: 311.04718017578125 = 0.07088650017976761 + 50.0 * 6.2195258140563965
Epoch 1920, val loss: 1.3830513954162598
Epoch 1930, training loss: 311.1427917480469 = 0.06966926157474518 + 50.0 * 6.221462249755859
Epoch 1930, val loss: 1.3883029222488403
Epoch 1940, training loss: 311.10845947265625 = 0.06841625273227692 + 50.0 * 6.220800876617432
Epoch 1940, val loss: 1.3933982849121094
Epoch 1950, training loss: 311.06781005859375 = 0.06719447672367096 + 50.0 * 6.220012187957764
Epoch 1950, val loss: 1.3987040519714355
Epoch 1960, training loss: 311.01654052734375 = 0.06602422893047333 + 50.0 * 6.219010829925537
Epoch 1960, val loss: 1.4048362970352173
Epoch 1970, training loss: 311.0015869140625 = 0.06491196900606155 + 50.0 * 6.218733787536621
Epoch 1970, val loss: 1.4105188846588135
Epoch 1980, training loss: 311.3120422363281 = 0.0638355165719986 + 50.0 * 6.224964141845703
Epoch 1980, val loss: 1.4158762693405151
Epoch 1990, training loss: 311.07061767578125 = 0.0626707449555397 + 50.0 * 6.220158576965332
Epoch 1990, val loss: 1.4200544357299805
Epoch 2000, training loss: 311.0359802246094 = 0.06160222738981247 + 50.0 * 6.219487190246582
Epoch 2000, val loss: 1.4263064861297607
Epoch 2010, training loss: 311.1249694824219 = 0.06055283918976784 + 50.0 * 6.221288681030273
Epoch 2010, val loss: 1.4308242797851562
Epoch 2020, training loss: 310.983154296875 = 0.059532828629016876 + 50.0 * 6.218472003936768
Epoch 2020, val loss: 1.4377267360687256
Epoch 2030, training loss: 311.1712341308594 = 0.0585348904132843 + 50.0 * 6.222253799438477
Epoch 2030, val loss: 1.4422439336776733
Epoch 2040, training loss: 311.0142517089844 = 0.05752721056342125 + 50.0 * 6.21913480758667
Epoch 2040, val loss: 1.4479659795761108
Epoch 2050, training loss: 310.9390869140625 = 0.056557245552539825 + 50.0 * 6.217650890350342
Epoch 2050, val loss: 1.4534832239151
Epoch 2060, training loss: 310.8876953125 = 0.05563107877969742 + 50.0 * 6.216640949249268
Epoch 2060, val loss: 1.4588778018951416
Epoch 2070, training loss: 310.8847961425781 = 0.05472544953227043 + 50.0 * 6.216601848602295
Epoch 2070, val loss: 1.4643540382385254
Epoch 2080, training loss: 311.4136657714844 = 0.05386263132095337 + 50.0 * 6.227196216583252
Epoch 2080, val loss: 1.4705824851989746
Epoch 2090, training loss: 311.0970458984375 = 0.052909109741449356 + 50.0 * 6.220882415771484
Epoch 2090, val loss: 1.4746992588043213
Epoch 2100, training loss: 310.9253845214844 = 0.05203546956181526 + 50.0 * 6.217467308044434
Epoch 2100, val loss: 1.4804402589797974
Epoch 2110, training loss: 310.9446105957031 = 0.051194462925195694 + 50.0 * 6.217868328094482
Epoch 2110, val loss: 1.485843539237976
Epoch 2120, training loss: 310.8019714355469 = 0.05036541447043419 + 50.0 * 6.21503210067749
Epoch 2120, val loss: 1.4908955097198486
Epoch 2130, training loss: 310.8160400390625 = 0.049573127180337906 + 50.0 * 6.215329647064209
Epoch 2130, val loss: 1.4962130784988403
Epoch 2140, training loss: 311.0109558105469 = 0.04880467429757118 + 50.0 * 6.219243049621582
Epoch 2140, val loss: 1.5020588636398315
Epoch 2150, training loss: 311.0058288574219 = 0.047997307032346725 + 50.0 * 6.219156742095947
Epoch 2150, val loss: 1.5066572427749634
Epoch 2160, training loss: 310.95123291015625 = 0.047218602150678635 + 50.0 * 6.218080520629883
Epoch 2160, val loss: 1.5108578205108643
Epoch 2170, training loss: 310.81103515625 = 0.04645654559135437 + 50.0 * 6.215291500091553
Epoch 2170, val loss: 1.5167739391326904
Epoch 2180, training loss: 310.7698669433594 = 0.04573783278465271 + 50.0 * 6.214482307434082
Epoch 2180, val loss: 1.5215415954589844
Epoch 2190, training loss: 310.849365234375 = 0.045038964599370956 + 50.0 * 6.216086387634277
Epoch 2190, val loss: 1.5269429683685303
Epoch 2200, training loss: 310.9501647949219 = 0.04434264823794365 + 50.0 * 6.218116760253906
Epoch 2200, val loss: 1.5321736335754395
Epoch 2210, training loss: 310.87738037109375 = 0.043640993535518646 + 50.0 * 6.2166748046875
Epoch 2210, val loss: 1.5378960371017456
Epoch 2220, training loss: 310.8151550292969 = 0.0429476834833622 + 50.0 * 6.215444564819336
Epoch 2220, val loss: 1.5424561500549316
Epoch 2230, training loss: 310.727294921875 = 0.04229631647467613 + 50.0 * 6.213699817657471
Epoch 2230, val loss: 1.5479241609573364
Epoch 2240, training loss: 310.6976623535156 = 0.041665952652692795 + 50.0 * 6.213119983673096
Epoch 2240, val loss: 1.5530195236206055
Epoch 2250, training loss: 310.96453857421875 = 0.04107638821005821 + 50.0 * 6.218469142913818
Epoch 2250, val loss: 1.5593132972717285
Epoch 2260, training loss: 310.75885009765625 = 0.04040282592177391 + 50.0 * 6.21436882019043
Epoch 2260, val loss: 1.5621200799942017
Epoch 2270, training loss: 310.7347412109375 = 0.03979603201150894 + 50.0 * 6.213898658752441
Epoch 2270, val loss: 1.56802499294281
Epoch 2280, training loss: 310.78387451171875 = 0.0392005629837513 + 50.0 * 6.214893341064453
Epoch 2280, val loss: 1.5720990896224976
Epoch 2290, training loss: 310.68865966796875 = 0.038622479885816574 + 50.0 * 6.213000297546387
Epoch 2290, val loss: 1.5779387950897217
Epoch 2300, training loss: 310.7770080566406 = 0.03805986046791077 + 50.0 * 6.214778900146484
Epoch 2300, val loss: 1.5820504426956177
Epoch 2310, training loss: 310.6584777832031 = 0.03749381750822067 + 50.0 * 6.212419509887695
Epoch 2310, val loss: 1.58803391456604
Epoch 2320, training loss: 310.9118347167969 = 0.036952804774045944 + 50.0 * 6.217497825622559
Epoch 2320, val loss: 1.5924530029296875
Epoch 2330, training loss: 310.8230895996094 = 0.036403998732566833 + 50.0 * 6.215733528137207
Epoch 2330, val loss: 1.5977426767349243
Epoch 2340, training loss: 310.6651306152344 = 0.03585085645318031 + 50.0 * 6.21258544921875
Epoch 2340, val loss: 1.6023842096328735
Epoch 2350, training loss: 310.62042236328125 = 0.0353475883603096 + 50.0 * 6.2117018699646
Epoch 2350, val loss: 1.6076769828796387
Epoch 2360, training loss: 310.6255187988281 = 0.03484483063220978 + 50.0 * 6.211813449859619
Epoch 2360, val loss: 1.6125017404556274
Epoch 2370, training loss: 310.8335266113281 = 0.03435921669006348 + 50.0 * 6.2159833908081055
Epoch 2370, val loss: 1.616995930671692
Epoch 2380, training loss: 310.7580871582031 = 0.03385142609477043 + 50.0 * 6.214485168457031
Epoch 2380, val loss: 1.6212741136550903
Epoch 2390, training loss: 310.58502197265625 = 0.03336231783032417 + 50.0 * 6.211033344268799
Epoch 2390, val loss: 1.6270208358764648
Epoch 2400, training loss: 310.61676025390625 = 0.03289734944701195 + 50.0 * 6.211677551269531
Epoch 2400, val loss: 1.6315698623657227
Epoch 2410, training loss: 310.6570739746094 = 0.03243640437722206 + 50.0 * 6.2124924659729
Epoch 2410, val loss: 1.636130928993225
Epoch 2420, training loss: 310.5715637207031 = 0.031985875219106674 + 50.0 * 6.21079158782959
Epoch 2420, val loss: 1.640868067741394
Epoch 2430, training loss: 310.5560607910156 = 0.03154592588543892 + 50.0 * 6.2104902267456055
Epoch 2430, val loss: 1.6452945470809937
Epoch 2440, training loss: 310.63592529296875 = 0.031117606908082962 + 50.0 * 6.212096214294434
Epoch 2440, val loss: 1.6495722532272339
Epoch 2450, training loss: 310.7380676269531 = 0.030687522143125534 + 50.0 * 6.214147090911865
Epoch 2450, val loss: 1.654412031173706
Epoch 2460, training loss: 310.5296630859375 = 0.030255520716309547 + 50.0 * 6.209988117218018
Epoch 2460, val loss: 1.6598161458969116
Epoch 2470, training loss: 310.469970703125 = 0.02983110584318638 + 50.0 * 6.208802700042725
Epoch 2470, val loss: 1.6639584302902222
Epoch 2480, training loss: 310.4712219238281 = 0.02943502739071846 + 50.0 * 6.208835601806641
Epoch 2480, val loss: 1.668771505355835
Epoch 2490, training loss: 310.66094970703125 = 0.029056411236524582 + 50.0 * 6.212637901306152
Epoch 2490, val loss: 1.6734740734100342
Epoch 2500, training loss: 310.477294921875 = 0.02865196391940117 + 50.0 * 6.208972930908203
Epoch 2500, val loss: 1.6781971454620361
Epoch 2510, training loss: 310.54705810546875 = 0.02827191725373268 + 50.0 * 6.210376262664795
Epoch 2510, val loss: 1.6825950145721436
Epoch 2520, training loss: 310.5458679199219 = 0.02789897471666336 + 50.0 * 6.210359573364258
Epoch 2520, val loss: 1.687508463859558
Epoch 2530, training loss: 310.5181884765625 = 0.027533039450645447 + 50.0 * 6.209813117980957
Epoch 2530, val loss: 1.6915621757507324
Epoch 2540, training loss: 310.5206298828125 = 0.02717154659330845 + 50.0 * 6.209869384765625
Epoch 2540, val loss: 1.695730447769165
Epoch 2550, training loss: 310.7362365722656 = 0.02681782655417919 + 50.0 * 6.214188098907471
Epoch 2550, val loss: 1.700073480606079
Epoch 2560, training loss: 310.55413818359375 = 0.02645784057676792 + 50.0 * 6.2105536460876465
Epoch 2560, val loss: 1.7048897743225098
Epoch 2570, training loss: 310.4453125 = 0.026110129430890083 + 50.0 * 6.208384037017822
Epoch 2570, val loss: 1.7089089155197144
Epoch 2580, training loss: 310.50201416015625 = 0.025778580456972122 + 50.0 * 6.209524631500244
Epoch 2580, val loss: 1.7128106355667114
Epoch 2590, training loss: 310.5713806152344 = 0.025453243404626846 + 50.0 * 6.210918426513672
Epoch 2590, val loss: 1.7172014713287354
Epoch 2600, training loss: 310.4838562011719 = 0.025127967819571495 + 50.0 * 6.209174633026123
Epoch 2600, val loss: 1.7221254110336304
Epoch 2610, training loss: 310.55389404296875 = 0.02480880543589592 + 50.0 * 6.2105817794799805
Epoch 2610, val loss: 1.7257397174835205
Epoch 2620, training loss: 310.45404052734375 = 0.02449757419526577 + 50.0 * 6.208590507507324
Epoch 2620, val loss: 1.7304701805114746
Epoch 2630, training loss: 310.4365234375 = 0.02419576793909073 + 50.0 * 6.208246231079102
Epoch 2630, val loss: 1.7355819940567017
Epoch 2640, training loss: 310.3837585449219 = 0.02389298938214779 + 50.0 * 6.207197189331055
Epoch 2640, val loss: 1.7395473718643188
Epoch 2650, training loss: 310.5940246582031 = 0.023614512756466866 + 50.0 * 6.2114081382751465
Epoch 2650, val loss: 1.744568943977356
Epoch 2660, training loss: 310.4342346191406 = 0.023303069174289703 + 50.0 * 6.208218097686768
Epoch 2660, val loss: 1.7474374771118164
Epoch 2670, training loss: 310.3480224609375 = 0.023006681352853775 + 50.0 * 6.206500053405762
Epoch 2670, val loss: 1.752200722694397
Epoch 2680, training loss: 310.3176574707031 = 0.02272825315594673 + 50.0 * 6.205898284912109
Epoch 2680, val loss: 1.755603313446045
Epoch 2690, training loss: 310.3253479003906 = 0.022464057430624962 + 50.0 * 6.206057548522949
Epoch 2690, val loss: 1.7601970434188843
Epoch 2700, training loss: 310.6305236816406 = 0.02220848575234413 + 50.0 * 6.2121663093566895
Epoch 2700, val loss: 1.7644827365875244
Epoch 2710, training loss: 310.39154052734375 = 0.02192491665482521 + 50.0 * 6.207392692565918
Epoch 2710, val loss: 1.7678827047348022
Epoch 2720, training loss: 310.4193115234375 = 0.02166149951517582 + 50.0 * 6.207952976226807
Epoch 2720, val loss: 1.772649884223938
Epoch 2730, training loss: 310.3282165527344 = 0.021402748301625252 + 50.0 * 6.206136703491211
Epoch 2730, val loss: 1.7765012979507446
Epoch 2740, training loss: 310.33050537109375 = 0.021150849759578705 + 50.0 * 6.2061872482299805
Epoch 2740, val loss: 1.7803746461868286
Epoch 2750, training loss: 310.6648864746094 = 0.02091139182448387 + 50.0 * 6.212879657745361
Epoch 2750, val loss: 1.7843438386917114
Epoch 2760, training loss: 310.4131774902344 = 0.020659349858760834 + 50.0 * 6.207850456237793
Epoch 2760, val loss: 1.788747787475586
Epoch 2770, training loss: 310.3303527832031 = 0.020416421815752983 + 50.0 * 6.206198692321777
Epoch 2770, val loss: 1.7928117513656616
Epoch 2780, training loss: 310.3138122558594 = 0.02018660306930542 + 50.0 * 6.205873012542725
Epoch 2780, val loss: 1.7964825630187988
Epoch 2790, training loss: 310.3245544433594 = 0.01995871402323246 + 50.0 * 6.20609188079834
Epoch 2790, val loss: 1.8001981973648071
Epoch 2800, training loss: 310.3283996582031 = 0.019731098785996437 + 50.0 * 6.206172943115234
Epoch 2800, val loss: 1.8030511140823364
Epoch 2810, training loss: 310.4890441894531 = 0.019509604200720787 + 50.0 * 6.209391117095947
Epoch 2810, val loss: 1.8060343265533447
Epoch 2820, training loss: 310.3446960449219 = 0.019286178052425385 + 50.0 * 6.206508159637451
Epoch 2820, val loss: 1.8120195865631104
Epoch 2830, training loss: 310.3576965332031 = 0.0190675500780344 + 50.0 * 6.206772327423096
Epoch 2830, val loss: 1.8149833679199219
Epoch 2840, training loss: 310.3108825683594 = 0.01884770393371582 + 50.0 * 6.205840587615967
Epoch 2840, val loss: 1.8182547092437744
Epoch 2850, training loss: 310.3039855957031 = 0.018636668100953102 + 50.0 * 6.20570707321167
Epoch 2850, val loss: 1.8219455480575562
Epoch 2860, training loss: 310.2406921386719 = 0.01843935437500477 + 50.0 * 6.2044453620910645
Epoch 2860, val loss: 1.8266160488128662
Epoch 2870, training loss: 310.25787353515625 = 0.018243752419948578 + 50.0 * 6.204792499542236
Epoch 2870, val loss: 1.8305021524429321
Epoch 2880, training loss: 310.4693908691406 = 0.018050167709589005 + 50.0 * 6.209027290344238
Epoch 2880, val loss: 1.8335285186767578
Epoch 2890, training loss: 310.3436584472656 = 0.017842940986156464 + 50.0 * 6.206516265869141
Epoch 2890, val loss: 1.8370429277420044
Epoch 2900, training loss: 310.21697998046875 = 0.017650529742240906 + 50.0 * 6.203986644744873
Epoch 2900, val loss: 1.841478943824768
Epoch 2910, training loss: 310.20574951171875 = 0.01746562123298645 + 50.0 * 6.203765869140625
Epoch 2910, val loss: 1.8452303409576416
Epoch 2920, training loss: 310.3907470703125 = 0.017283780500292778 + 50.0 * 6.207469463348389
Epoch 2920, val loss: 1.8478929996490479
Epoch 2930, training loss: 310.28216552734375 = 0.017088303342461586 + 50.0 * 6.205301761627197
Epoch 2930, val loss: 1.8512110710144043
Epoch 2940, training loss: 310.2314147949219 = 0.016906052827835083 + 50.0 * 6.204290390014648
Epoch 2940, val loss: 1.8556196689605713
Epoch 2950, training loss: 310.31707763671875 = 0.016737746074795723 + 50.0 * 6.20600700378418
Epoch 2950, val loss: 1.8596636056900024
Epoch 2960, training loss: 310.2904968261719 = 0.01655573956668377 + 50.0 * 6.205479145050049
Epoch 2960, val loss: 1.8617116212844849
Epoch 2970, training loss: 310.21856689453125 = 0.01638372801244259 + 50.0 * 6.204043865203857
Epoch 2970, val loss: 1.8654104471206665
Epoch 2980, training loss: 310.16485595703125 = 0.016219547018408775 + 50.0 * 6.202972888946533
Epoch 2980, val loss: 1.8694846630096436
Epoch 2990, training loss: 310.23712158203125 = 0.016058314591646194 + 50.0 * 6.204421520233154
Epoch 2990, val loss: 1.8731251955032349
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6518518518518519
0.8081180811808119
The final CL Acc:0.64321, 0.00761, The final GNN Acc:0.80706, 0.00075
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13110])
remove edge: torch.Size([2, 7848])
updated graph: torch.Size([2, 10402])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7976989746094 = 1.9587955474853516 + 50.0 * 8.59677791595459
Epoch 0, val loss: 1.9578872919082642
Epoch 10, training loss: 431.7322082519531 = 1.9484953880310059 + 50.0 * 8.595674514770508
Epoch 10, val loss: 1.9470350742340088
Epoch 20, training loss: 431.37322998046875 = 1.9355502128601074 + 50.0 * 8.588753700256348
Epoch 20, val loss: 1.9334388971328735
Epoch 30, training loss: 429.2878112792969 = 1.9190813302993774 + 50.0 * 8.547374725341797
Epoch 30, val loss: 1.9163264036178589
Epoch 40, training loss: 418.32098388671875 = 1.8989943265914917 + 50.0 * 8.328439712524414
Epoch 40, val loss: 1.896244764328003
Epoch 50, training loss: 382.5855407714844 = 1.8753215074539185 + 50.0 * 7.614204406738281
Epoch 50, val loss: 1.8736822605133057
Epoch 60, training loss: 362.7282409667969 = 1.8597394227981567 + 50.0 * 7.21737003326416
Epoch 60, val loss: 1.8603641986846924
Epoch 70, training loss: 349.7285461425781 = 1.8499423265457153 + 50.0 * 6.957571983337402
Epoch 70, val loss: 1.851333737373352
Epoch 80, training loss: 343.4705505371094 = 1.840067982673645 + 50.0 * 6.832610130310059
Epoch 80, val loss: 1.8420774936676025
Epoch 90, training loss: 339.7759704589844 = 1.8295210599899292 + 50.0 * 6.7589287757873535
Epoch 90, val loss: 1.8321912288665771
Epoch 100, training loss: 336.7305603027344 = 1.8195542097091675 + 50.0 * 6.698220252990723
Epoch 100, val loss: 1.8231993913650513
Epoch 110, training loss: 334.38726806640625 = 1.8110096454620361 + 50.0 * 6.651525020599365
Epoch 110, val loss: 1.8155635595321655
Epoch 120, training loss: 332.28668212890625 = 1.8035054206848145 + 50.0 * 6.609663963317871
Epoch 120, val loss: 1.8087698221206665
Epoch 130, training loss: 330.5150451660156 = 1.796470046043396 + 50.0 * 6.574371337890625
Epoch 130, val loss: 1.8023784160614014
Epoch 140, training loss: 329.17120361328125 = 1.789524793624878 + 50.0 * 6.547633647918701
Epoch 140, val loss: 1.7959983348846436
Epoch 150, training loss: 328.11614990234375 = 1.7823081016540527 + 50.0 * 6.526676654815674
Epoch 150, val loss: 1.7894823551177979
Epoch 160, training loss: 327.1133117675781 = 1.774834394454956 + 50.0 * 6.50676965713501
Epoch 160, val loss: 1.7829838991165161
Epoch 170, training loss: 326.2176208496094 = 1.7672467231750488 + 50.0 * 6.489007472991943
Epoch 170, val loss: 1.7763373851776123
Epoch 180, training loss: 325.66461181640625 = 1.7592685222625732 + 50.0 * 6.478106498718262
Epoch 180, val loss: 1.7694532871246338
Epoch 190, training loss: 324.842529296875 = 1.750464916229248 + 50.0 * 6.461841583251953
Epoch 190, val loss: 1.7620989084243774
Epoch 200, training loss: 324.267578125 = 1.7410849332809448 + 50.0 * 6.450530052185059
Epoch 200, val loss: 1.7543268203735352
Epoch 210, training loss: 323.7263488769531 = 1.731056809425354 + 50.0 * 6.439906120300293
Epoch 210, val loss: 1.7460734844207764
Epoch 220, training loss: 323.26214599609375 = 1.7202261686325073 + 50.0 * 6.430838584899902
Epoch 220, val loss: 1.7372066974639893
Epoch 230, training loss: 322.8494873046875 = 1.7085936069488525 + 50.0 * 6.422818183898926
Epoch 230, val loss: 1.7277936935424805
Epoch 240, training loss: 322.4283752441406 = 1.6960424184799194 + 50.0 * 6.414646625518799
Epoch 240, val loss: 1.717736005783081
Epoch 250, training loss: 322.03521728515625 = 1.6826059818267822 + 50.0 * 6.407052040100098
Epoch 250, val loss: 1.7070337533950806
Epoch 260, training loss: 321.6796875 = 1.6681534051895142 + 50.0 * 6.400230407714844
Epoch 260, val loss: 1.69561767578125
Epoch 270, training loss: 321.31011962890625 = 1.652710199356079 + 50.0 * 6.393148422241211
Epoch 270, val loss: 1.683503270149231
Epoch 280, training loss: 321.03192138671875 = 1.636246681213379 + 50.0 * 6.387913227081299
Epoch 280, val loss: 1.6706435680389404
Epoch 290, training loss: 320.68316650390625 = 1.6188933849334717 + 50.0 * 6.381285667419434
Epoch 290, val loss: 1.6569923162460327
Epoch 300, training loss: 320.3666687011719 = 1.6003202199935913 + 50.0 * 6.375327110290527
Epoch 300, val loss: 1.642715573310852
Epoch 310, training loss: 320.0394287109375 = 1.5809093713760376 + 50.0 * 6.369170665740967
Epoch 310, val loss: 1.6277203559875488
Epoch 320, training loss: 319.85955810546875 = 1.5604065656661987 + 50.0 * 6.365983009338379
Epoch 320, val loss: 1.6122249364852905
Epoch 330, training loss: 319.4989013671875 = 1.539101481437683 + 50.0 * 6.359196186065674
Epoch 330, val loss: 1.5958585739135742
Epoch 340, training loss: 319.2071228027344 = 1.516852855682373 + 50.0 * 6.3538055419921875
Epoch 340, val loss: 1.5792474746704102
Epoch 350, training loss: 318.943359375 = 1.4938913583755493 + 50.0 * 6.348989486694336
Epoch 350, val loss: 1.562220573425293
Epoch 360, training loss: 318.7080078125 = 1.4702564477920532 + 50.0 * 6.344755172729492
Epoch 360, val loss: 1.5448448657989502
Epoch 370, training loss: 318.6111145019531 = 1.4457848072052002 + 50.0 * 6.343307018280029
Epoch 370, val loss: 1.5270112752914429
Epoch 380, training loss: 318.41217041015625 = 1.420915126800537 + 50.0 * 6.339824676513672
Epoch 380, val loss: 1.5089812278747559
Epoch 390, training loss: 318.0924377441406 = 1.3954479694366455 + 50.0 * 6.333939552307129
Epoch 390, val loss: 1.4908729791641235
Epoch 400, training loss: 317.8916931152344 = 1.3696811199188232 + 50.0 * 6.330440044403076
Epoch 400, val loss: 1.4726632833480835
Epoch 410, training loss: 317.75927734375 = 1.3435192108154297 + 50.0 * 6.328315258026123
Epoch 410, val loss: 1.454322338104248
Epoch 420, training loss: 317.51483154296875 = 1.317260980606079 + 50.0 * 6.323951244354248
Epoch 420, val loss: 1.436219573020935
Epoch 430, training loss: 317.4515380859375 = 1.2907731533050537 + 50.0 * 6.323215484619141
Epoch 430, val loss: 1.4181822538375854
Epoch 440, training loss: 317.3636779785156 = 1.2643831968307495 + 50.0 * 6.321986198425293
Epoch 440, val loss: 1.400020956993103
Epoch 450, training loss: 317.0518798828125 = 1.238020420074463 + 50.0 * 6.316277503967285
Epoch 450, val loss: 1.3824387788772583
Epoch 460, training loss: 316.85870361328125 = 1.212064504623413 + 50.0 * 6.31293249130249
Epoch 460, val loss: 1.3652780055999756
Epoch 470, training loss: 316.7071228027344 = 1.1863853931427002 + 50.0 * 6.310415267944336
Epoch 470, val loss: 1.3484817743301392
Epoch 480, training loss: 316.62542724609375 = 1.1609656810760498 + 50.0 * 6.30928897857666
Epoch 480, val loss: 1.3319599628448486
Epoch 490, training loss: 316.6390380859375 = 1.1360219717025757 + 50.0 * 6.310060501098633
Epoch 490, val loss: 1.3158527612686157
Epoch 500, training loss: 316.369873046875 = 1.111297607421875 + 50.0 * 6.305171489715576
Epoch 500, val loss: 1.3001344203948975
Epoch 510, training loss: 316.1876525878906 = 1.0871922969818115 + 50.0 * 6.302009582519531
Epoch 510, val loss: 1.285124659538269
Epoch 520, training loss: 316.1354064941406 = 1.0636851787567139 + 50.0 * 6.301434516906738
Epoch 520, val loss: 1.2703912258148193
Epoch 530, training loss: 316.1217041015625 = 1.0403376817703247 + 50.0 * 6.301627159118652
Epoch 530, val loss: 1.2564315795898438
Epoch 540, training loss: 315.86529541015625 = 1.0177791118621826 + 50.0 * 6.296949863433838
Epoch 540, val loss: 1.242724061012268
Epoch 550, training loss: 315.701904296875 = 0.9958416223526001 + 50.0 * 6.294121265411377
Epoch 550, val loss: 1.229707956314087
Epoch 560, training loss: 315.7060546875 = 0.9744102358818054 + 50.0 * 6.294633388519287
Epoch 560, val loss: 1.2169842720031738
Epoch 570, training loss: 315.6138000488281 = 0.9534130096435547 + 50.0 * 6.293208122253418
Epoch 570, val loss: 1.2050577402114868
Epoch 580, training loss: 315.3990783691406 = 0.9329835176467896 + 50.0 * 6.2893218994140625
Epoch 580, val loss: 1.1932318210601807
Epoch 590, training loss: 315.2593688964844 = 0.913175642490387 + 50.0 * 6.286923885345459
Epoch 590, val loss: 1.1822772026062012
Epoch 600, training loss: 315.21600341796875 = 0.8938790559768677 + 50.0 * 6.286442279815674
Epoch 600, val loss: 1.1716692447662354
Epoch 610, training loss: 315.1450500488281 = 0.8748360872268677 + 50.0 * 6.285404205322266
Epoch 610, val loss: 1.1612428426742554
Epoch 620, training loss: 315.0386047363281 = 0.8564148545265198 + 50.0 * 6.28364372253418
Epoch 620, val loss: 1.151402473449707
Epoch 630, training loss: 314.85943603515625 = 0.8383494019508362 + 50.0 * 6.280421733856201
Epoch 630, val loss: 1.1422467231750488
Epoch 640, training loss: 314.7897033691406 = 0.8207836747169495 + 50.0 * 6.279378890991211
Epoch 640, val loss: 1.1334489583969116
Epoch 650, training loss: 314.77301025390625 = 0.8037084937095642 + 50.0 * 6.279386043548584
Epoch 650, val loss: 1.1246798038482666
Epoch 660, training loss: 314.5795593261719 = 0.7870778441429138 + 50.0 * 6.275849342346191
Epoch 660, val loss: 1.1168594360351562
Epoch 670, training loss: 314.55511474609375 = 0.7708259224891663 + 50.0 * 6.275686264038086
Epoch 670, val loss: 1.1089227199554443
Epoch 680, training loss: 314.5860595703125 = 0.7548717856407166 + 50.0 * 6.276623725891113
Epoch 680, val loss: 1.1020454168319702
Epoch 690, training loss: 314.3384094238281 = 0.739172637462616 + 50.0 * 6.271984577178955
Epoch 690, val loss: 1.0946139097213745
Epoch 700, training loss: 314.26226806640625 = 0.7239811420440674 + 50.0 * 6.270765781402588
Epoch 700, val loss: 1.0879297256469727
Epoch 710, training loss: 314.16754150390625 = 0.7092334032058716 + 50.0 * 6.269165992736816
Epoch 710, val loss: 1.08184015750885
Epoch 720, training loss: 314.2317810058594 = 0.6948389410972595 + 50.0 * 6.2707390785217285
Epoch 720, val loss: 1.0761206150054932
Epoch 730, training loss: 314.2874450683594 = 0.6805688738822937 + 50.0 * 6.27213716506958
Epoch 730, val loss: 1.070211410522461
Epoch 740, training loss: 313.98822021484375 = 0.6664421558380127 + 50.0 * 6.266435623168945
Epoch 740, val loss: 1.064914345741272
Epoch 750, training loss: 313.884521484375 = 0.6528417468070984 + 50.0 * 6.264633655548096
Epoch 750, val loss: 1.0600309371948242
Epoch 760, training loss: 313.81488037109375 = 0.6396528482437134 + 50.0 * 6.263504505157471
Epoch 760, val loss: 1.055523157119751
Epoch 770, training loss: 314.1266784667969 = 0.6266711950302124 + 50.0 * 6.269999980926514
Epoch 770, val loss: 1.0514148473739624
Epoch 780, training loss: 313.68902587890625 = 0.6138127446174622 + 50.0 * 6.261504650115967
Epoch 780, val loss: 1.0469945669174194
Epoch 790, training loss: 313.68341064453125 = 0.6013340950012207 + 50.0 * 6.261641502380371
Epoch 790, val loss: 1.043359637260437
Epoch 800, training loss: 313.6072082519531 = 0.5891472697257996 + 50.0 * 6.260361194610596
Epoch 800, val loss: 1.0399813652038574
Epoch 810, training loss: 313.821044921875 = 0.5773146152496338 + 50.0 * 6.2648749351501465
Epoch 810, val loss: 1.0374253988265991
Epoch 820, training loss: 313.5591125488281 = 0.5653451681137085 + 50.0 * 6.259875297546387
Epoch 820, val loss: 1.0337802171707153
Epoch 830, training loss: 313.3669738769531 = 0.553959310054779 + 50.0 * 6.256260395050049
Epoch 830, val loss: 1.031419277191162
Epoch 840, training loss: 313.3323059082031 = 0.5428721904754639 + 50.0 * 6.255788803100586
Epoch 840, val loss: 1.0294631719589233
Epoch 850, training loss: 313.5975646972656 = 0.5319613814353943 + 50.0 * 6.261312007904053
Epoch 850, val loss: 1.0271426439285278
Epoch 860, training loss: 313.3382263183594 = 0.5212033987045288 + 50.0 * 6.256340503692627
Epoch 860, val loss: 1.0255458354949951
Epoch 870, training loss: 313.1599426269531 = 0.5107665061950684 + 50.0 * 6.252983570098877
Epoch 870, val loss: 1.024185299873352
Epoch 880, training loss: 313.12054443359375 = 0.5006311535835266 + 50.0 * 6.2523980140686035
Epoch 880, val loss: 1.0228183269500732
Epoch 890, training loss: 313.3352355957031 = 0.4908207952976227 + 50.0 * 6.256888389587402
Epoch 890, val loss: 1.0218470096588135
Epoch 900, training loss: 313.18408203125 = 0.48092374205589294 + 50.0 * 6.254063129425049
Epoch 900, val loss: 1.0209753513336182
Epoch 910, training loss: 312.98968505859375 = 0.47143158316612244 + 50.0 * 6.250364780426025
Epoch 910, val loss: 1.0204232931137085
Epoch 920, training loss: 312.9127502441406 = 0.46221715211868286 + 50.0 * 6.2490105628967285
Epoch 920, val loss: 1.0200804471969604
Epoch 930, training loss: 312.92388916015625 = 0.4532565772533417 + 50.0 * 6.249413013458252
Epoch 930, val loss: 1.020018458366394
Epoch 940, training loss: 312.8695983886719 = 0.44445866346359253 + 50.0 * 6.248502731323242
Epoch 940, val loss: 1.0201666355133057
Epoch 950, training loss: 312.9239196777344 = 0.435806542634964 + 50.0 * 6.249762058258057
Epoch 950, val loss: 1.0202608108520508
Epoch 960, training loss: 312.77484130859375 = 0.42737582325935364 + 50.0 * 6.246949195861816
Epoch 960, val loss: 1.0206869840621948
Epoch 970, training loss: 312.7113342285156 = 0.4192676246166229 + 50.0 * 6.245841026306152
Epoch 970, val loss: 1.0215606689453125
Epoch 980, training loss: 312.85540771484375 = 0.4113130271434784 + 50.0 * 6.248881816864014
Epoch 980, val loss: 1.0222694873809814
Epoch 990, training loss: 312.6410217285156 = 0.4035547971725464 + 50.0 * 6.244749069213867
Epoch 990, val loss: 1.0233135223388672
Epoch 1000, training loss: 312.5864562988281 = 0.39601680636405945 + 50.0 * 6.243808746337891
Epoch 1000, val loss: 1.0246951580047607
Epoch 1010, training loss: 312.76470947265625 = 0.3887332081794739 + 50.0 * 6.247519493103027
Epoch 1010, val loss: 1.0262956619262695
Epoch 1020, training loss: 312.58013916015625 = 0.3813595771789551 + 50.0 * 6.24397611618042
Epoch 1020, val loss: 1.0268349647521973
Epoch 1030, training loss: 312.5433044433594 = 0.37438109517097473 + 50.0 * 6.243378162384033
Epoch 1030, val loss: 1.029051423072815
Epoch 1040, training loss: 312.44659423828125 = 0.36753934621810913 + 50.0 * 6.241580963134766
Epoch 1040, val loss: 1.0305352210998535
Epoch 1050, training loss: 312.4630126953125 = 0.3609435558319092 + 50.0 * 6.24204158782959
Epoch 1050, val loss: 1.0324656963348389
Epoch 1060, training loss: 312.3705749511719 = 0.35443973541259766 + 50.0 * 6.240323066711426
Epoch 1060, val loss: 1.0345603227615356
Epoch 1070, training loss: 312.386474609375 = 0.34808221459388733 + 50.0 * 6.240767955780029
Epoch 1070, val loss: 1.036643624305725
Epoch 1080, training loss: 312.33612060546875 = 0.3419172465801239 + 50.0 * 6.239883899688721
Epoch 1080, val loss: 1.039060354232788
Epoch 1090, training loss: 312.34747314453125 = 0.33587318658828735 + 50.0 * 6.240231990814209
Epoch 1090, val loss: 1.0414561033248901
Epoch 1100, training loss: 312.26214599609375 = 0.32993167638778687 + 50.0 * 6.238644599914551
Epoch 1100, val loss: 1.0436838865280151
Epoch 1110, training loss: 312.2740478515625 = 0.3241877257823944 + 50.0 * 6.238997459411621
Epoch 1110, val loss: 1.0462840795516968
Epoch 1120, training loss: 312.2086486816406 = 0.31854447722435 + 50.0 * 6.237802028656006
Epoch 1120, val loss: 1.049072265625
Epoch 1130, training loss: 312.18426513671875 = 0.3130018413066864 + 50.0 * 6.237425804138184
Epoch 1130, val loss: 1.0517061948776245
Epoch 1140, training loss: 312.10498046875 = 0.30761244893074036 + 50.0 * 6.235947132110596
Epoch 1140, val loss: 1.054662823677063
Epoch 1150, training loss: 312.122314453125 = 0.3023545444011688 + 50.0 * 6.236398696899414
Epoch 1150, val loss: 1.057667851448059
Epoch 1160, training loss: 312.1097106933594 = 0.2972029745578766 + 50.0 * 6.236249923706055
Epoch 1160, val loss: 1.0606131553649902
Epoch 1170, training loss: 312.1239318847656 = 0.29218029975891113 + 50.0 * 6.236635208129883
Epoch 1170, val loss: 1.0641422271728516
Epoch 1180, training loss: 312.1097412109375 = 0.28719210624694824 + 50.0 * 6.236450672149658
Epoch 1180, val loss: 1.066886305809021
Epoch 1190, training loss: 312.0647888183594 = 0.282346248626709 + 50.0 * 6.2356486320495605
Epoch 1190, val loss: 1.0704822540283203
Epoch 1200, training loss: 311.9559631347656 = 0.27759721875190735 + 50.0 * 6.233567714691162
Epoch 1200, val loss: 1.0741461515426636
Epoch 1210, training loss: 311.9332580566406 = 0.27299582958221436 + 50.0 * 6.2332048416137695
Epoch 1210, val loss: 1.0777522325515747
Epoch 1220, training loss: 311.94866943359375 = 0.26845839619636536 + 50.0 * 6.2336039543151855
Epoch 1220, val loss: 1.0809941291809082
Epoch 1230, training loss: 311.9251403808594 = 0.26400840282440186 + 50.0 * 6.233222484588623
Epoch 1230, val loss: 1.0848073959350586
Epoch 1240, training loss: 311.9059753417969 = 0.259640097618103 + 50.0 * 6.232926845550537
Epoch 1240, val loss: 1.0885064601898193
Epoch 1250, training loss: 311.83721923828125 = 0.25531044602394104 + 50.0 * 6.231637954711914
Epoch 1250, val loss: 1.0923709869384766
Epoch 1260, training loss: 311.82574462890625 = 0.25113487243652344 + 50.0 * 6.231492042541504
Epoch 1260, val loss: 1.0961531400680542
Epoch 1270, training loss: 311.7564392089844 = 0.24699752032756805 + 50.0 * 6.230188846588135
Epoch 1270, val loss: 1.099828839302063
Epoch 1280, training loss: 311.7787170410156 = 0.24296240508556366 + 50.0 * 6.230715274810791
Epoch 1280, val loss: 1.104057788848877
Epoch 1290, training loss: 311.73175048828125 = 0.23896998167037964 + 50.0 * 6.229855537414551
Epoch 1290, val loss: 1.1078269481658936
Epoch 1300, training loss: 311.77825927734375 = 0.23506073653697968 + 50.0 * 6.23086404800415
Epoch 1300, val loss: 1.1120394468307495
Epoch 1310, training loss: 311.71710205078125 = 0.23118405044078827 + 50.0 * 6.2297186851501465
Epoch 1310, val loss: 1.1158626079559326
Epoch 1320, training loss: 311.6207580566406 = 0.2273818701505661 + 50.0 * 6.227867603302002
Epoch 1320, val loss: 1.120133876800537
Epoch 1330, training loss: 311.5946960449219 = 0.2236824482679367 + 50.0 * 6.227419853210449
Epoch 1330, val loss: 1.1241825819015503
Epoch 1340, training loss: 311.61920166015625 = 0.22007474303245544 + 50.0 * 6.227982997894287
Epoch 1340, val loss: 1.1283378601074219
Epoch 1350, training loss: 311.78009033203125 = 0.2164766937494278 + 50.0 * 6.231272220611572
Epoch 1350, val loss: 1.1324604749679565
Epoch 1360, training loss: 311.60369873046875 = 0.21293705701828003 + 50.0 * 6.2278151512146
Epoch 1360, val loss: 1.1368167400360107
Epoch 1370, training loss: 311.5228576660156 = 0.20944446325302124 + 50.0 * 6.226268291473389
Epoch 1370, val loss: 1.140952229499817
Epoch 1380, training loss: 311.595947265625 = 0.20608903467655182 + 50.0 * 6.227797031402588
Epoch 1380, val loss: 1.145405888557434
Epoch 1390, training loss: 311.536865234375 = 0.20269055664539337 + 50.0 * 6.226683616638184
Epoch 1390, val loss: 1.1492061614990234
Epoch 1400, training loss: 311.539306640625 = 0.19935300946235657 + 50.0 * 6.226799011230469
Epoch 1400, val loss: 1.1538983583450317
Epoch 1410, training loss: 311.4444580078125 = 0.19609767198562622 + 50.0 * 6.224967002868652
Epoch 1410, val loss: 1.1582800149917603
Epoch 1420, training loss: 311.4043273925781 = 0.1929163634777069 + 50.0 * 6.224228382110596
Epoch 1420, val loss: 1.162792682647705
Epoch 1430, training loss: 311.5321350097656 = 0.18979740142822266 + 50.0 * 6.226846694946289
Epoch 1430, val loss: 1.1670724153518677
Epoch 1440, training loss: 311.5579833984375 = 0.18665023148059845 + 50.0 * 6.227426528930664
Epoch 1440, val loss: 1.17169988155365
Epoch 1450, training loss: 311.4090270996094 = 0.18354366719722748 + 50.0 * 6.2245097160339355
Epoch 1450, val loss: 1.1758705377578735
Epoch 1460, training loss: 311.3384704589844 = 0.1805438995361328 + 50.0 * 6.223158836364746
Epoch 1460, val loss: 1.1807302236557007
Epoch 1470, training loss: 311.29254150390625 = 0.17761242389678955 + 50.0 * 6.222298622131348
Epoch 1470, val loss: 1.1852809190750122
Epoch 1480, training loss: 311.47015380859375 = 0.17476244270801544 + 50.0 * 6.225907325744629
Epoch 1480, val loss: 1.1902763843536377
Epoch 1490, training loss: 311.316650390625 = 0.1717834323644638 + 50.0 * 6.222897052764893
Epoch 1490, val loss: 1.1935293674468994
Epoch 1500, training loss: 311.3099670410156 = 0.1689252406358719 + 50.0 * 6.222821235656738
Epoch 1500, val loss: 1.1987147331237793
Epoch 1510, training loss: 311.2347412109375 = 0.16613180935382843 + 50.0 * 6.221372127532959
Epoch 1510, val loss: 1.2028416395187378
Epoch 1520, training loss: 311.2391357421875 = 0.1634422093629837 + 50.0 * 6.221513748168945
Epoch 1520, val loss: 1.207706093788147
Epoch 1530, training loss: 311.3919982910156 = 0.16074863076210022 + 50.0 * 6.224625110626221
Epoch 1530, val loss: 1.2121561765670776
Epoch 1540, training loss: 311.29595947265625 = 0.15802668035030365 + 50.0 * 6.222758769989014
Epoch 1540, val loss: 1.216524362564087
Epoch 1550, training loss: 311.2728271484375 = 0.15540726482868195 + 50.0 * 6.222348690032959
Epoch 1550, val loss: 1.2213573455810547
Epoch 1560, training loss: 311.18182373046875 = 0.15281005203723907 + 50.0 * 6.220580577850342
Epoch 1560, val loss: 1.225659966468811
Epoch 1570, training loss: 311.13800048828125 = 0.15029507875442505 + 50.0 * 6.219753742218018
Epoch 1570, val loss: 1.2305461168289185
Epoch 1580, training loss: 311.1409606933594 = 0.14780676364898682 + 50.0 * 6.219863414764404
Epoch 1580, val loss: 1.2350424528121948
Epoch 1590, training loss: 311.2899169921875 = 0.1453685313463211 + 50.0 * 6.222891330718994
Epoch 1590, val loss: 1.2397750616073608
Epoch 1600, training loss: 311.116943359375 = 0.14288386702537537 + 50.0 * 6.219480991363525
Epoch 1600, val loss: 1.2442463636398315
Epoch 1610, training loss: 311.0577087402344 = 0.14047899842262268 + 50.0 * 6.218344688415527
Epoch 1610, val loss: 1.2489805221557617
Epoch 1620, training loss: 311.0491027832031 = 0.1381605565547943 + 50.0 * 6.218218803405762
Epoch 1620, val loss: 1.2538028955459595
Epoch 1630, training loss: 311.3114013671875 = 0.1358802318572998 + 50.0 * 6.223510265350342
Epoch 1630, val loss: 1.2582988739013672
Epoch 1640, training loss: 311.11590576171875 = 0.13355472683906555 + 50.0 * 6.21964693069458
Epoch 1640, val loss: 1.2627906799316406
Epoch 1650, training loss: 311.04925537109375 = 0.13128957152366638 + 50.0 * 6.218359470367432
Epoch 1650, val loss: 1.267675757408142
Epoch 1660, training loss: 311.01239013671875 = 0.12910883128643036 + 50.0 * 6.217665672302246
Epoch 1660, val loss: 1.272294044494629
Epoch 1670, training loss: 311.25146484375 = 0.1269599199295044 + 50.0 * 6.222490310668945
Epoch 1670, val loss: 1.2769845724105835
Epoch 1680, training loss: 311.04534912109375 = 0.12478792667388916 + 50.0 * 6.218410968780518
Epoch 1680, val loss: 1.281479835510254
Epoch 1690, training loss: 310.9766540527344 = 0.1226748526096344 + 50.0 * 6.2170796394348145
Epoch 1690, val loss: 1.286110281944275
Epoch 1700, training loss: 311.1190185546875 = 0.12062635272741318 + 50.0 * 6.219967365264893
Epoch 1700, val loss: 1.2908111810684204
Epoch 1710, training loss: 310.9107666015625 = 0.11859998106956482 + 50.0 * 6.215843200683594
Epoch 1710, val loss: 1.295653223991394
Epoch 1720, training loss: 310.8959655761719 = 0.11663082242012024 + 50.0 * 6.2155866622924805
Epoch 1720, val loss: 1.300406575202942
Epoch 1730, training loss: 310.9178771972656 = 0.11468871682882309 + 50.0 * 6.216063976287842
Epoch 1730, val loss: 1.3052021265029907
Epoch 1740, training loss: 311.0400085449219 = 0.11276911944150925 + 50.0 * 6.218544960021973
Epoch 1740, val loss: 1.3099006414413452
Epoch 1750, training loss: 311.0089416503906 = 0.11084109544754028 + 50.0 * 6.217962265014648
Epoch 1750, val loss: 1.314150094985962
Epoch 1760, training loss: 310.9049072265625 = 0.10895203053951263 + 50.0 * 6.215919494628906
Epoch 1760, val loss: 1.3187726736068726
Epoch 1770, training loss: 310.91827392578125 = 0.10714010894298553 + 50.0 * 6.216222763061523
Epoch 1770, val loss: 1.3237463235855103
Epoch 1780, training loss: 310.8246765136719 = 0.10534004867076874 + 50.0 * 6.214386463165283
Epoch 1780, val loss: 1.3285315036773682
Epoch 1790, training loss: 310.86383056640625 = 0.10357546806335449 + 50.0 * 6.215205192565918
Epoch 1790, val loss: 1.3333455324172974
Epoch 1800, training loss: 310.8359680175781 = 0.1018471047282219 + 50.0 * 6.214682579040527
Epoch 1800, val loss: 1.3380621671676636
Epoch 1810, training loss: 310.8570251464844 = 0.10014304518699646 + 50.0 * 6.215137958526611
Epoch 1810, val loss: 1.3427306413650513
Epoch 1820, training loss: 310.91888427734375 = 0.09845291823148727 + 50.0 * 6.216408729553223
Epoch 1820, val loss: 1.3474047183990479
Epoch 1830, training loss: 310.81640625 = 0.09677962213754654 + 50.0 * 6.21439266204834
Epoch 1830, val loss: 1.352074146270752
Epoch 1840, training loss: 310.73236083984375 = 0.09514438360929489 + 50.0 * 6.212744235992432
Epoch 1840, val loss: 1.3570129871368408
Epoch 1850, training loss: 310.73236083984375 = 0.09358242154121399 + 50.0 * 6.212775230407715
Epoch 1850, val loss: 1.3619918823242188
Epoch 1860, training loss: 310.7574462890625 = 0.09204467386007309 + 50.0 * 6.213308334350586
Epoch 1860, val loss: 1.3668599128723145
Epoch 1870, training loss: 310.89349365234375 = 0.09052463620901108 + 50.0 * 6.216059684753418
Epoch 1870, val loss: 1.3713974952697754
Epoch 1880, training loss: 310.76666259765625 = 0.08898116648197174 + 50.0 * 6.213553428649902
Epoch 1880, val loss: 1.3761307001113892
Epoch 1890, training loss: 310.8467712402344 = 0.08749116957187653 + 50.0 * 6.215185642242432
Epoch 1890, val loss: 1.3805242776870728
Epoch 1900, training loss: 310.7134704589844 = 0.0860147774219513 + 50.0 * 6.212548732757568
Epoch 1900, val loss: 1.3858168125152588
Epoch 1910, training loss: 310.70257568359375 = 0.0845981314778328 + 50.0 * 6.212359428405762
Epoch 1910, val loss: 1.3903486728668213
Epoch 1920, training loss: 310.9530944824219 = 0.08320169895887375 + 50.0 * 6.217398166656494
Epoch 1920, val loss: 1.3949322700500488
Epoch 1930, training loss: 310.7137451171875 = 0.08179453760385513 + 50.0 * 6.212638854980469
Epoch 1930, val loss: 1.39995539188385
Epoch 1940, training loss: 310.63555908203125 = 0.08044268190860748 + 50.0 * 6.21110200881958
Epoch 1940, val loss: 1.404735803604126
Epoch 1950, training loss: 310.7024230957031 = 0.07913100719451904 + 50.0 * 6.212465763092041
Epoch 1950, val loss: 1.4095876216888428
Epoch 1960, training loss: 310.67718505859375 = 0.07782154530286789 + 50.0 * 6.211987495422363
Epoch 1960, val loss: 1.414216160774231
Epoch 1970, training loss: 310.63201904296875 = 0.07652732729911804 + 50.0 * 6.211109638214111
Epoch 1970, val loss: 1.4189181327819824
Epoch 1980, training loss: 310.6330871582031 = 0.07526884973049164 + 50.0 * 6.211156845092773
Epoch 1980, val loss: 1.4237245321273804
Epoch 1990, training loss: 310.71759033203125 = 0.07402653247117996 + 50.0 * 6.212871074676514
Epoch 1990, val loss: 1.4280986785888672
Epoch 2000, training loss: 310.61395263671875 = 0.07279941439628601 + 50.0 * 6.2108235359191895
Epoch 2000, val loss: 1.4329885244369507
Epoch 2010, training loss: 310.6861877441406 = 0.07160627096891403 + 50.0 * 6.212291717529297
Epoch 2010, val loss: 1.437651515007019
Epoch 2020, training loss: 310.6459045410156 = 0.07042766362428665 + 50.0 * 6.2115092277526855
Epoch 2020, val loss: 1.4421476125717163
Epoch 2030, training loss: 310.5459899902344 = 0.0692468136548996 + 50.0 * 6.209535121917725
Epoch 2030, val loss: 1.4468821287155151
Epoch 2040, training loss: 310.5326232910156 = 0.0681256577372551 + 50.0 * 6.209290027618408
Epoch 2040, val loss: 1.4516416788101196
Epoch 2050, training loss: 310.58831787109375 = 0.06703480333089828 + 50.0 * 6.21042537689209
Epoch 2050, val loss: 1.4563469886779785
Epoch 2060, training loss: 310.5752258300781 = 0.06592482328414917 + 50.0 * 6.210186004638672
Epoch 2060, val loss: 1.4607490301132202
Epoch 2070, training loss: 310.48883056640625 = 0.06483782082796097 + 50.0 * 6.208479404449463
Epoch 2070, val loss: 1.4654395580291748
Epoch 2080, training loss: 310.47174072265625 = 0.06378915905952454 + 50.0 * 6.20815896987915
Epoch 2080, val loss: 1.470366358757019
Epoch 2090, training loss: 310.7361145019531 = 0.06279202550649643 + 50.0 * 6.213466167449951
Epoch 2090, val loss: 1.4749908447265625
Epoch 2100, training loss: 310.596923828125 = 0.06173616647720337 + 50.0 * 6.2107038497924805
Epoch 2100, val loss: 1.4793213605880737
Epoch 2110, training loss: 310.5348205566406 = 0.06072726473212242 + 50.0 * 6.209481716156006
Epoch 2110, val loss: 1.4838817119598389
Epoch 2120, training loss: 310.45343017578125 = 0.059742797166109085 + 50.0 * 6.207873344421387
Epoch 2120, val loss: 1.4885982275009155
Epoch 2130, training loss: 310.507080078125 = 0.058803990483284 + 50.0 * 6.208965301513672
Epoch 2130, val loss: 1.4932209253311157
Epoch 2140, training loss: 310.5113220214844 = 0.05785733088850975 + 50.0 * 6.20906925201416
Epoch 2140, val loss: 1.4976521730422974
Epoch 2150, training loss: 310.4984436035156 = 0.056932806968688965 + 50.0 * 6.208829879760742
Epoch 2150, val loss: 1.5023000240325928
Epoch 2160, training loss: 310.5250244140625 = 0.05602657422423363 + 50.0 * 6.20937967300415
Epoch 2160, val loss: 1.5069031715393066
Epoch 2170, training loss: 310.6124267578125 = 0.05512837693095207 + 50.0 * 6.211146354675293
Epoch 2170, val loss: 1.5109643936157227
Epoch 2180, training loss: 310.4920654296875 = 0.054225578904151917 + 50.0 * 6.208756446838379
Epoch 2180, val loss: 1.5152652263641357
Epoch 2190, training loss: 310.41943359375 = 0.05336495861411095 + 50.0 * 6.2073211669921875
Epoch 2190, val loss: 1.520058035850525
Epoch 2200, training loss: 310.3786926269531 = 0.05253308638930321 + 50.0 * 6.206523418426514
Epoch 2200, val loss: 1.5245904922485352
Epoch 2210, training loss: 310.3703918457031 = 0.051725517958402634 + 50.0 * 6.20637321472168
Epoch 2210, val loss: 1.5292083024978638
Epoch 2220, training loss: 310.6231994628906 = 0.05094364285469055 + 50.0 * 6.211445331573486
Epoch 2220, val loss: 1.5334641933441162
Epoch 2230, training loss: 310.4007263183594 = 0.05011555925011635 + 50.0 * 6.207012176513672
Epoch 2230, val loss: 1.5376298427581787
Epoch 2240, training loss: 310.3471374511719 = 0.04932873696088791 + 50.0 * 6.20595645904541
Epoch 2240, val loss: 1.5422329902648926
Epoch 2250, training loss: 310.3817443847656 = 0.048576295375823975 + 50.0 * 6.206663131713867
Epoch 2250, val loss: 1.5466519594192505
Epoch 2260, training loss: 310.4765930175781 = 0.047831181436777115 + 50.0 * 6.208575248718262
Epoch 2260, val loss: 1.5508254766464233
Epoch 2270, training loss: 310.3885498046875 = 0.0470774844288826 + 50.0 * 6.206829071044922
Epoch 2270, val loss: 1.5551034212112427
Epoch 2280, training loss: 310.4561767578125 = 0.04635274410247803 + 50.0 * 6.20819616317749
Epoch 2280, val loss: 1.5595712661743164
Epoch 2290, training loss: 310.3489990234375 = 0.04562637209892273 + 50.0 * 6.2060675621032715
Epoch 2290, val loss: 1.5634685754776
Epoch 2300, training loss: 310.318603515625 = 0.04492303729057312 + 50.0 * 6.20547342300415
Epoch 2300, val loss: 1.5680019855499268
Epoch 2310, training loss: 310.30072021484375 = 0.04424683004617691 + 50.0 * 6.205129146575928
Epoch 2310, val loss: 1.5723627805709839
Epoch 2320, training loss: 310.4720458984375 = 0.043591152876615524 + 50.0 * 6.208569049835205
Epoch 2320, val loss: 1.5764062404632568
Epoch 2330, training loss: 310.2759094238281 = 0.04292312264442444 + 50.0 * 6.204659461975098
Epoch 2330, val loss: 1.5808757543563843
Epoch 2340, training loss: 310.2580871582031 = 0.042272429913282394 + 50.0 * 6.204316139221191
Epoch 2340, val loss: 1.5850688219070435
Epoch 2350, training loss: 310.3236389160156 = 0.0416523739695549 + 50.0 * 6.205639362335205
Epoch 2350, val loss: 1.5893291234970093
Epoch 2360, training loss: 310.3894348144531 = 0.04103400185704231 + 50.0 * 6.206967830657959
Epoch 2360, val loss: 1.5935330390930176
Epoch 2370, training loss: 310.3655090332031 = 0.04041347652673721 + 50.0 * 6.2065019607543945
Epoch 2370, val loss: 1.5975199937820435
Epoch 2380, training loss: 310.25750732421875 = 0.039817024022340775 + 50.0 * 6.2043538093566895
Epoch 2380, val loss: 1.6016802787780762
Epoch 2390, training loss: 310.2403564453125 = 0.03923288732767105 + 50.0 * 6.20402193069458
Epoch 2390, val loss: 1.6058194637298584
Epoch 2400, training loss: 310.31341552734375 = 0.03866644948720932 + 50.0 * 6.2054948806762695
Epoch 2400, val loss: 1.609915018081665
Epoch 2410, training loss: 310.2521667480469 = 0.038097310811281204 + 50.0 * 6.204281806945801
Epoch 2410, val loss: 1.6139711141586304
Epoch 2420, training loss: 310.2803955078125 = 0.03754428029060364 + 50.0 * 6.204857349395752
Epoch 2420, val loss: 1.6180698871612549
Epoch 2430, training loss: 310.19384765625 = 0.03700268268585205 + 50.0 * 6.203136444091797
Epoch 2430, val loss: 1.6222188472747803
Epoch 2440, training loss: 310.3988952636719 = 0.036491069942712784 + 50.0 * 6.207248210906982
Epoch 2440, val loss: 1.6263409852981567
Epoch 2450, training loss: 310.2616882324219 = 0.03593182936310768 + 50.0 * 6.20451545715332
Epoch 2450, val loss: 1.6296809911727905
Epoch 2460, training loss: 310.201416015625 = 0.03540473431348801 + 50.0 * 6.203320503234863
Epoch 2460, val loss: 1.6339366436004639
Epoch 2470, training loss: 310.1695251464844 = 0.03490122780203819 + 50.0 * 6.202692031860352
Epoch 2470, val loss: 1.637942910194397
Epoch 2480, training loss: 310.2423095703125 = 0.034423258155584335 + 50.0 * 6.204157829284668
Epoch 2480, val loss: 1.6418849229812622
Epoch 2490, training loss: 310.2591247558594 = 0.03393245115876198 + 50.0 * 6.204504013061523
Epoch 2490, val loss: 1.6455626487731934
Epoch 2500, training loss: 310.22113037109375 = 0.03345674276351929 + 50.0 * 6.20375394821167
Epoch 2500, val loss: 1.6498805284500122
Epoch 2510, training loss: 310.1629333496094 = 0.032983291894197464 + 50.0 * 6.202599048614502
Epoch 2510, val loss: 1.6536940336227417
Epoch 2520, training loss: 310.30072021484375 = 0.032540690153837204 + 50.0 * 6.205363750457764
Epoch 2520, val loss: 1.6576781272888184
Epoch 2530, training loss: 310.10748291015625 = 0.03207758069038391 + 50.0 * 6.201508045196533
Epoch 2530, val loss: 1.661288857460022
Epoch 2540, training loss: 310.12310791015625 = 0.03163895383477211 + 50.0 * 6.201829433441162
Epoch 2540, val loss: 1.665260672569275
Epoch 2550, training loss: 310.3087158203125 = 0.031215423718094826 + 50.0 * 6.205549716949463
Epoch 2550, val loss: 1.6690438985824585
Epoch 2560, training loss: 310.13543701171875 = 0.030773380771279335 + 50.0 * 6.202093601226807
Epoch 2560, val loss: 1.6725293397903442
Epoch 2570, training loss: 310.1432800292969 = 0.03035254217684269 + 50.0 * 6.202258586883545
Epoch 2570, val loss: 1.6764060258865356
Epoch 2580, training loss: 310.1073913574219 = 0.029944082722067833 + 50.0 * 6.201549053192139
Epoch 2580, val loss: 1.6802302598953247
Epoch 2590, training loss: 310.2773132324219 = 0.029553858563303947 + 50.0 * 6.204955577850342
Epoch 2590, val loss: 1.6840811967849731
Epoch 2600, training loss: 310.14752197265625 = 0.029139354825019836 + 50.0 * 6.202367782592773
Epoch 2600, val loss: 1.6872693300247192
Epoch 2610, training loss: 310.077880859375 = 0.028742073103785515 + 50.0 * 6.200982570648193
Epoch 2610, val loss: 1.6910096406936646
Epoch 2620, training loss: 310.0547790527344 = 0.028368273749947548 + 50.0 * 6.200527667999268
Epoch 2620, val loss: 1.694855809211731
Epoch 2630, training loss: 310.0688171386719 = 0.02800624631345272 + 50.0 * 6.2008161544799805
Epoch 2630, val loss: 1.6985390186309814
Epoch 2640, training loss: 310.3265075683594 = 0.02765277586877346 + 50.0 * 6.205977439880371
Epoch 2640, val loss: 1.7020394802093506
Epoch 2650, training loss: 310.2352600097656 = 0.027269840240478516 + 50.0 * 6.204159736633301
Epoch 2650, val loss: 1.705216646194458
Epoch 2660, training loss: 310.0478210449219 = 0.026898780837655067 + 50.0 * 6.200418949127197
Epoch 2660, val loss: 1.70902419090271
Epoch 2670, training loss: 310.0291748046875 = 0.02655351720750332 + 50.0 * 6.200052261352539
Epoch 2670, val loss: 1.712735652923584
Epoch 2680, training loss: 310.1705627441406 = 0.026221998035907745 + 50.0 * 6.202887058258057
Epoch 2680, val loss: 1.7163035869598389
Epoch 2690, training loss: 310.2403869628906 = 0.02587665431201458 + 50.0 * 6.204290390014648
Epoch 2690, val loss: 1.719483733177185
Epoch 2700, training loss: 310.10986328125 = 0.02552688866853714 + 50.0 * 6.201686859130859
Epoch 2700, val loss: 1.7227845191955566
Epoch 2710, training loss: 310.0101318359375 = 0.025191470980644226 + 50.0 * 6.1996989250183105
Epoch 2710, val loss: 1.7265162467956543
Epoch 2720, training loss: 309.97125244140625 = 0.024877622723579407 + 50.0 * 6.198927402496338
Epoch 2720, val loss: 1.7301416397094727
Epoch 2730, training loss: 309.96234130859375 = 0.02457309328019619 + 50.0 * 6.198755741119385
Epoch 2730, val loss: 1.7338148355484009
Epoch 2740, training loss: 310.1181335449219 = 0.02428189106285572 + 50.0 * 6.201877117156982
Epoch 2740, val loss: 1.7371503114700317
Epoch 2750, training loss: 310.0420837402344 = 0.02396734617650509 + 50.0 * 6.200362682342529
Epoch 2750, val loss: 1.7404717206954956
Epoch 2760, training loss: 310.08477783203125 = 0.023659879341721535 + 50.0 * 6.2012224197387695
Epoch 2760, val loss: 1.743643879890442
Epoch 2770, training loss: 309.9612121582031 = 0.023343488574028015 + 50.0 * 6.198757171630859
Epoch 2770, val loss: 1.7471517324447632
Epoch 2780, training loss: 309.9505920410156 = 0.023059967905282974 + 50.0 * 6.198550224304199
Epoch 2780, val loss: 1.7505590915679932
Epoch 2790, training loss: 309.93365478515625 = 0.022788209840655327 + 50.0 * 6.198217391967773
Epoch 2790, val loss: 1.7540282011032104
Epoch 2800, training loss: 310.0210266113281 = 0.022518886253237724 + 50.0 * 6.199970245361328
Epoch 2800, val loss: 1.7571303844451904
Epoch 2810, training loss: 310.0111389160156 = 0.02223978005349636 + 50.0 * 6.199777603149414
Epoch 2810, val loss: 1.7602473497390747
Epoch 2820, training loss: 309.97698974609375 = 0.021967347711324692 + 50.0 * 6.199100494384766
Epoch 2820, val loss: 1.7639107704162598
Epoch 2830, training loss: 310.2008361816406 = 0.021706368774175644 + 50.0 * 6.203582763671875
Epoch 2830, val loss: 1.7670631408691406
Epoch 2840, training loss: 309.97772216796875 = 0.021436486393213272 + 50.0 * 6.199126243591309
Epoch 2840, val loss: 1.7699203491210938
Epoch 2850, training loss: 309.9177551269531 = 0.02117271162569523 + 50.0 * 6.19793176651001
Epoch 2850, val loss: 1.7734965085983276
Epoch 2860, training loss: 309.92559814453125 = 0.02092890441417694 + 50.0 * 6.198093414306641
Epoch 2860, val loss: 1.7768383026123047
Epoch 2870, training loss: 310.1314392089844 = 0.020693108439445496 + 50.0 * 6.20221471786499
Epoch 2870, val loss: 1.7799862623214722
Epoch 2880, training loss: 309.9439392089844 = 0.02043531835079193 + 50.0 * 6.198470115661621
Epoch 2880, val loss: 1.78305983543396
Epoch 2890, training loss: 309.8904724121094 = 0.02019755356013775 + 50.0 * 6.1974053382873535
Epoch 2890, val loss: 1.7862646579742432
Epoch 2900, training loss: 309.9524841308594 = 0.0199727825820446 + 50.0 * 6.198650360107422
Epoch 2900, val loss: 1.7893913984298706
Epoch 2910, training loss: 309.9579772949219 = 0.01973666064441204 + 50.0 * 6.198764801025391
Epoch 2910, val loss: 1.7924635410308838
Epoch 2920, training loss: 310.0490417480469 = 0.019506601616740227 + 50.0 * 6.20059061050415
Epoch 2920, val loss: 1.7952250242233276
Epoch 2930, training loss: 309.90594482421875 = 0.019273346289992332 + 50.0 * 6.197732925415039
Epoch 2930, val loss: 1.7983615398406982
Epoch 2940, training loss: 309.8537902832031 = 0.01905595138669014 + 50.0 * 6.196694850921631
Epoch 2940, val loss: 1.8016047477722168
Epoch 2950, training loss: 309.9188232421875 = 0.018848108127713203 + 50.0 * 6.197999000549316
Epoch 2950, val loss: 1.8045579195022583
Epoch 2960, training loss: 310.05084228515625 = 0.01863241195678711 + 50.0 * 6.200644016265869
Epoch 2960, val loss: 1.8071789741516113
Epoch 2970, training loss: 309.8844909667969 = 0.018408119678497314 + 50.0 * 6.197321891784668
Epoch 2970, val loss: 1.8103151321411133
Epoch 2980, training loss: 309.8177490234375 = 0.01820480450987816 + 50.0 * 6.195990562438965
Epoch 2980, val loss: 1.813391089439392
Epoch 2990, training loss: 309.82305908203125 = 0.01801290176808834 + 50.0 * 6.196101188659668
Epoch 2990, val loss: 1.8164634704589844
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 431.7905578613281 = 1.9503040313720703 + 50.0 * 8.59680461883545
Epoch 0, val loss: 1.9538352489471436
Epoch 10, training loss: 431.72259521484375 = 1.9415075778961182 + 50.0 * 8.595622062683105
Epoch 10, val loss: 1.9455076456069946
Epoch 20, training loss: 431.27789306640625 = 1.9306056499481201 + 50.0 * 8.586945533752441
Epoch 20, val loss: 1.9348962306976318
Epoch 30, training loss: 428.08740234375 = 1.916535496711731 + 50.0 * 8.523417472839355
Epoch 30, val loss: 1.9210277795791626
Epoch 40, training loss: 405.70062255859375 = 1.8988120555877686 + 50.0 * 8.07603645324707
Epoch 40, val loss: 1.9036610126495361
Epoch 50, training loss: 373.4631652832031 = 1.8788444995880127 + 50.0 * 7.4316864013671875
Epoch 50, val loss: 1.8850723505020142
Epoch 60, training loss: 360.2328186035156 = 1.8646776676177979 + 50.0 * 7.167362689971924
Epoch 60, val loss: 1.8715052604675293
Epoch 70, training loss: 352.73126220703125 = 1.8519134521484375 + 50.0 * 7.017586708068848
Epoch 70, val loss: 1.8589794635772705
Epoch 80, training loss: 346.3182678222656 = 1.8406896591186523 + 50.0 * 6.889551162719727
Epoch 80, val loss: 1.8478556871414185
Epoch 90, training loss: 341.105712890625 = 1.8307266235351562 + 50.0 * 6.7855000495910645
Epoch 90, val loss: 1.8379077911376953
Epoch 100, training loss: 337.5665588378906 = 1.8221025466918945 + 50.0 * 6.714889049530029
Epoch 100, val loss: 1.8288965225219727
Epoch 110, training loss: 334.7409973144531 = 1.8141298294067383 + 50.0 * 6.658537864685059
Epoch 110, val loss: 1.8203520774841309
Epoch 120, training loss: 332.3141784667969 = 1.8068721294403076 + 50.0 * 6.6101460456848145
Epoch 120, val loss: 1.8123337030410767
Epoch 130, training loss: 330.34326171875 = 1.800101399421692 + 50.0 * 6.570863246917725
Epoch 130, val loss: 1.8047024011611938
Epoch 140, training loss: 328.70458984375 = 1.7933472394943237 + 50.0 * 6.538225173950195
Epoch 140, val loss: 1.7972943782806396
Epoch 150, training loss: 327.3868713378906 = 1.786663293838501 + 50.0 * 6.512004375457764
Epoch 150, val loss: 1.7901040315628052
Epoch 160, training loss: 326.3020324707031 = 1.7798677682876587 + 50.0 * 6.490443229675293
Epoch 160, val loss: 1.783055305480957
Epoch 170, training loss: 325.3633728027344 = 1.772765874862671 + 50.0 * 6.4718122482299805
Epoch 170, val loss: 1.7758843898773193
Epoch 180, training loss: 324.5428466796875 = 1.765296220779419 + 50.0 * 6.4555511474609375
Epoch 180, val loss: 1.7686083316802979
Epoch 190, training loss: 323.96527099609375 = 1.7573609352111816 + 50.0 * 6.444158554077148
Epoch 190, val loss: 1.760998010635376
Epoch 200, training loss: 323.2357482910156 = 1.7488025426864624 + 50.0 * 6.429738521575928
Epoch 200, val loss: 1.7530205249786377
Epoch 210, training loss: 322.6817626953125 = 1.7395949363708496 + 50.0 * 6.4188432693481445
Epoch 210, val loss: 1.7445613145828247
Epoch 220, training loss: 322.4168701171875 = 1.729602336883545 + 50.0 * 6.413744926452637
Epoch 220, val loss: 1.7356019020080566
Epoch 230, training loss: 321.8193359375 = 1.7187373638153076 + 50.0 * 6.402011871337891
Epoch 230, val loss: 1.7258809804916382
Epoch 240, training loss: 321.3829345703125 = 1.7070107460021973 + 50.0 * 6.393518447875977
Epoch 240, val loss: 1.7154957056045532
Epoch 250, training loss: 320.96990966796875 = 1.6942843198776245 + 50.0 * 6.385512828826904
Epoch 250, val loss: 1.7043336629867554
Epoch 260, training loss: 320.6887512207031 = 1.6806098222732544 + 50.0 * 6.380163192749023
Epoch 260, val loss: 1.6923738718032837
Epoch 270, training loss: 320.4023742675781 = 1.6657365560531616 + 50.0 * 6.374732494354248
Epoch 270, val loss: 1.6794260740280151
Epoch 280, training loss: 320.02178955078125 = 1.6498396396636963 + 50.0 * 6.367439270019531
Epoch 280, val loss: 1.6656651496887207
Epoch 290, training loss: 319.7114562988281 = 1.632947325706482 + 50.0 * 6.361570358276367
Epoch 290, val loss: 1.6511632204055786
Epoch 300, training loss: 319.4490051269531 = 1.615052342414856 + 50.0 * 6.3566789627075195
Epoch 300, val loss: 1.6358981132507324
Epoch 310, training loss: 319.4196472167969 = 1.5961064100265503 + 50.0 * 6.356471061706543
Epoch 310, val loss: 1.6198774576187134
Epoch 320, training loss: 319.0271301269531 = 1.5761619806289673 + 50.0 * 6.3490190505981445
Epoch 320, val loss: 1.6029999256134033
Epoch 330, training loss: 318.7488708496094 = 1.555442214012146 + 50.0 * 6.343868255615234
Epoch 330, val loss: 1.5856659412384033
Epoch 340, training loss: 318.5224609375 = 1.5340194702148438 + 50.0 * 6.339768409729004
Epoch 340, val loss: 1.5678722858428955
Epoch 350, training loss: 318.47259521484375 = 1.5119808912277222 + 50.0 * 6.339211940765381
Epoch 350, val loss: 1.5495295524597168
Epoch 360, training loss: 318.3189392089844 = 1.4891839027404785 + 50.0 * 6.336595058441162
Epoch 360, val loss: 1.5312541723251343
Epoch 370, training loss: 317.9806823730469 = 1.465946912765503 + 50.0 * 6.330295085906982
Epoch 370, val loss: 1.512385368347168
Epoch 380, training loss: 317.8353576660156 = 1.4425321817398071 + 50.0 * 6.327856540679932
Epoch 380, val loss: 1.4937413930892944
Epoch 390, training loss: 317.634765625 = 1.4187744855880737 + 50.0 * 6.324320316314697
Epoch 390, val loss: 1.4750133752822876
Epoch 400, training loss: 317.46002197265625 = 1.394988775253296 + 50.0 * 6.321300506591797
Epoch 400, val loss: 1.4563062191009521
Epoch 410, training loss: 317.2733154296875 = 1.3710885047912598 + 50.0 * 6.318044185638428
Epoch 410, val loss: 1.4381005764007568
Epoch 420, training loss: 317.5120849609375 = 1.3471636772155762 + 50.0 * 6.323298454284668
Epoch 420, val loss: 1.419805645942688
Epoch 430, training loss: 317.0894470214844 = 1.3230886459350586 + 50.0 * 6.315327167510986
Epoch 430, val loss: 1.4016865491867065
Epoch 440, training loss: 316.84442138671875 = 1.299256682395935 + 50.0 * 6.310903072357178
Epoch 440, val loss: 1.3840304613113403
Epoch 450, training loss: 316.72308349609375 = 1.2757070064544678 + 50.0 * 6.308947563171387
Epoch 450, val loss: 1.366970181465149
Epoch 460, training loss: 316.8197326660156 = 1.2523000240325928 + 50.0 * 6.311348915100098
Epoch 460, val loss: 1.3494993448257446
Epoch 470, training loss: 316.4300842285156 = 1.2289718389511108 + 50.0 * 6.304022312164307
Epoch 470, val loss: 1.332979679107666
Epoch 480, training loss: 316.2855224609375 = 1.206153392791748 + 50.0 * 6.3015875816345215
Epoch 480, val loss: 1.3171422481536865
Epoch 490, training loss: 316.148681640625 = 1.1838117837905884 + 50.0 * 6.299297332763672
Epoch 490, val loss: 1.301912546157837
Epoch 500, training loss: 316.16265869140625 = 1.1618351936340332 + 50.0 * 6.300016403198242
Epoch 500, val loss: 1.2873177528381348
Epoch 510, training loss: 316.0772399902344 = 1.140161156654358 + 50.0 * 6.298741340637207
Epoch 510, val loss: 1.2725579738616943
Epoch 520, training loss: 315.8561096191406 = 1.1188888549804688 + 50.0 * 6.294744491577148
Epoch 520, val loss: 1.258878469467163
Epoch 530, training loss: 315.7611999511719 = 1.098109483718872 + 50.0 * 6.293262004852295
Epoch 530, val loss: 1.2458451986312866
Epoch 540, training loss: 315.6446533203125 = 1.0778311491012573 + 50.0 * 6.291336536407471
Epoch 540, val loss: 1.2334544658660889
Epoch 550, training loss: 315.5144958496094 = 1.058149814605713 + 50.0 * 6.289127349853516
Epoch 550, val loss: 1.2216819524765015
Epoch 560, training loss: 315.5135498046875 = 1.0388882160186768 + 50.0 * 6.289493560791016
Epoch 560, val loss: 1.2103052139282227
Epoch 570, training loss: 315.3841552734375 = 1.0200213193893433 + 50.0 * 6.287282466888428
Epoch 570, val loss: 1.199514627456665
Epoch 580, training loss: 315.1947326660156 = 1.0017167329788208 + 50.0 * 6.283860683441162
Epoch 580, val loss: 1.1892462968826294
Epoch 590, training loss: 315.3168640136719 = 0.9839486479759216 + 50.0 * 6.28665828704834
Epoch 590, val loss: 1.1798207759857178
Epoch 600, training loss: 315.18756103515625 = 0.9664598703384399 + 50.0 * 6.284421920776367
Epoch 600, val loss: 1.17072331905365
Epoch 610, training loss: 314.9490661621094 = 0.949468731880188 + 50.0 * 6.27999210357666
Epoch 610, val loss: 1.1621028184890747
Epoch 620, training loss: 315.0863342285156 = 0.932974100112915 + 50.0 * 6.283067226409912
Epoch 620, val loss: 1.1539651155471802
Epoch 630, training loss: 314.8112487792969 = 0.9169520139694214 + 50.0 * 6.277885913848877
Epoch 630, val loss: 1.146389126777649
Epoch 640, training loss: 314.7349548339844 = 0.9012941718101501 + 50.0 * 6.276672840118408
Epoch 640, val loss: 1.139125108718872
Epoch 650, training loss: 314.70599365234375 = 0.8859948515892029 + 50.0 * 6.276400089263916
Epoch 650, val loss: 1.132332682609558
Epoch 660, training loss: 314.5260314941406 = 0.8709884881973267 + 50.0 * 6.27310037612915
Epoch 660, val loss: 1.125795841217041
Epoch 670, training loss: 314.45806884765625 = 0.8564376831054688 + 50.0 * 6.272032260894775
Epoch 670, val loss: 1.119774580001831
Epoch 680, training loss: 314.71221923828125 = 0.842228889465332 + 50.0 * 6.277400016784668
Epoch 680, val loss: 1.1138043403625488
Epoch 690, training loss: 314.3225402832031 = 0.8280580043792725 + 50.0 * 6.2698893547058105
Epoch 690, val loss: 1.108381986618042
Epoch 700, training loss: 314.2713928222656 = 0.8143575191497803 + 50.0 * 6.269140720367432
Epoch 700, val loss: 1.1031116247177124
Epoch 710, training loss: 314.1650390625 = 0.80106121301651 + 50.0 * 6.267279624938965
Epoch 710, val loss: 1.0981330871582031
Epoch 720, training loss: 314.4685974121094 = 0.7879205942153931 + 50.0 * 6.273613452911377
Epoch 720, val loss: 1.0931142568588257
Epoch 730, training loss: 314.0867614746094 = 0.7749040126800537 + 50.0 * 6.266237258911133
Epoch 730, val loss: 1.0886081457138062
Epoch 740, training loss: 314.1173400878906 = 0.7621872425079346 + 50.0 * 6.26710319519043
Epoch 740, val loss: 1.0844091176986694
Epoch 750, training loss: 313.94287109375 = 0.7496819496154785 + 50.0 * 6.263863563537598
Epoch 750, val loss: 1.0797854661941528
Epoch 760, training loss: 313.8746643066406 = 0.7374455332756042 + 50.0 * 6.262744426727295
Epoch 760, val loss: 1.075736165046692
Epoch 770, training loss: 313.9657897949219 = 0.7254614233970642 + 50.0 * 6.264806747436523
Epoch 770, val loss: 1.0721325874328613
Epoch 780, training loss: 313.7979431152344 = 0.7135211229324341 + 50.0 * 6.261688232421875
Epoch 780, val loss: 1.0677080154418945
Epoch 790, training loss: 313.73614501953125 = 0.7017567753791809 + 50.0 * 6.260687828063965
Epoch 790, val loss: 1.0642478466033936
Epoch 800, training loss: 313.9692077636719 = 0.6901825666427612 + 50.0 * 6.265580654144287
Epoch 800, val loss: 1.0602045059204102
Epoch 810, training loss: 313.6551208496094 = 0.6786913275718689 + 50.0 * 6.259529113769531
Epoch 810, val loss: 1.0565704107284546
Epoch 820, training loss: 313.5087585449219 = 0.6674460768699646 + 50.0 * 6.256826400756836
Epoch 820, val loss: 1.0532286167144775
Epoch 830, training loss: 313.5012512207031 = 0.6564099788665771 + 50.0 * 6.25689697265625
Epoch 830, val loss: 1.04987370967865
Epoch 840, training loss: 313.4580993652344 = 0.6453670263290405 + 50.0 * 6.25625467300415
Epoch 840, val loss: 1.046298861503601
Epoch 850, training loss: 313.446533203125 = 0.6344135999679565 + 50.0 * 6.256242752075195
Epoch 850, val loss: 1.0428913831710815
Epoch 860, training loss: 313.3669738769531 = 0.6235699653625488 + 50.0 * 6.254868030548096
Epoch 860, val loss: 1.0397108793258667
Epoch 870, training loss: 313.3924255371094 = 0.6129090189933777 + 50.0 * 6.255590438842773
Epoch 870, val loss: 1.0363309383392334
Epoch 880, training loss: 313.2646484375 = 0.6023858189582825 + 50.0 * 6.2532453536987305
Epoch 880, val loss: 1.0328941345214844
Epoch 890, training loss: 313.20928955078125 = 0.5919354557991028 + 50.0 * 6.252346992492676
Epoch 890, val loss: 1.0300239324569702
Epoch 900, training loss: 313.18536376953125 = 0.5816686749458313 + 50.0 * 6.252074241638184
Epoch 900, val loss: 1.0268977880477905
Epoch 910, training loss: 313.1806945800781 = 0.5714446902275085 + 50.0 * 6.252185344696045
Epoch 910, val loss: 1.02376389503479
Epoch 920, training loss: 313.2301330566406 = 0.561268150806427 + 50.0 * 6.2533769607543945
Epoch 920, val loss: 1.0214512348175049
Epoch 930, training loss: 313.0287170410156 = 0.5512256622314453 + 50.0 * 6.249549865722656
Epoch 930, val loss: 1.0177905559539795
Epoch 940, training loss: 312.9849853515625 = 0.5413672924041748 + 50.0 * 6.248871803283691
Epoch 940, val loss: 1.0151855945587158
Epoch 950, training loss: 312.9514465332031 = 0.5316401124000549 + 50.0 * 6.248395919799805
Epoch 950, val loss: 1.012600302696228
Epoch 960, training loss: 312.91778564453125 = 0.5220469832420349 + 50.0 * 6.247915267944336
Epoch 960, val loss: 1.010219693183899
Epoch 970, training loss: 312.9515075683594 = 0.5125576257705688 + 50.0 * 6.248779296875
Epoch 970, val loss: 1.0077486038208008
Epoch 980, training loss: 312.7692565917969 = 0.5032303929328918 + 50.0 * 6.2453203201293945
Epoch 980, val loss: 1.005350112915039
Epoch 990, training loss: 312.77911376953125 = 0.49411043524742126 + 50.0 * 6.245699882507324
Epoch 990, val loss: 1.0033341646194458
Epoch 1000, training loss: 312.8164367675781 = 0.48501232266426086 + 50.0 * 6.246628284454346
Epoch 1000, val loss: 1.0012760162353516
Epoch 1010, training loss: 312.65106201171875 = 0.4760378301143646 + 50.0 * 6.243500232696533
Epoch 1010, val loss: 0.9993299841880798
Epoch 1020, training loss: 312.7752685546875 = 0.4672503173351288 + 50.0 * 6.24616003036499
Epoch 1020, val loss: 0.9975839257240295
Epoch 1030, training loss: 312.55474853515625 = 0.45859795808792114 + 50.0 * 6.2419233322143555
Epoch 1030, val loss: 0.9956606030464172
Epoch 1040, training loss: 312.52056884765625 = 0.4501541256904602 + 50.0 * 6.241407871246338
Epoch 1040, val loss: 0.9940984845161438
Epoch 1050, training loss: 312.633056640625 = 0.4419122338294983 + 50.0 * 6.2438225746154785
Epoch 1050, val loss: 0.9926818609237671
Epoch 1060, training loss: 312.4941711425781 = 0.4336712062358856 + 50.0 * 6.241209506988525
Epoch 1060, val loss: 0.9917564988136292
Epoch 1070, training loss: 312.79412841796875 = 0.4256328046321869 + 50.0 * 6.24737024307251
Epoch 1070, val loss: 0.9899737238883972
Epoch 1080, training loss: 312.5013427734375 = 0.41763320565223694 + 50.0 * 6.241674423217773
Epoch 1080, val loss: 0.9896172285079956
Epoch 1090, training loss: 312.3692321777344 = 0.40989208221435547 + 50.0 * 6.239187240600586
Epoch 1090, val loss: 0.9883444309234619
Epoch 1100, training loss: 312.5085144042969 = 0.4023360311985016 + 50.0 * 6.242123126983643
Epoch 1100, val loss: 0.9877300262451172
Epoch 1110, training loss: 312.3707275390625 = 0.39487457275390625 + 50.0 * 6.2395172119140625
Epoch 1110, val loss: 0.9870953559875488
Epoch 1120, training loss: 312.30303955078125 = 0.3874833583831787 + 50.0 * 6.238311290740967
Epoch 1120, val loss: 0.986374020576477
Epoch 1130, training loss: 312.2135314941406 = 0.38039836287498474 + 50.0 * 6.236662864685059
Epoch 1130, val loss: 0.9860779047012329
Epoch 1140, training loss: 312.1726989746094 = 0.37344470620155334 + 50.0 * 6.235984802246094
Epoch 1140, val loss: 0.9857765436172485
Epoch 1150, training loss: 312.3994445800781 = 0.36662283539772034 + 50.0 * 6.240656852722168
Epoch 1150, val loss: 0.9854849576950073
Epoch 1160, training loss: 312.2212219238281 = 0.35988086462020874 + 50.0 * 6.237226486206055
Epoch 1160, val loss: 0.9854682087898254
Epoch 1170, training loss: 312.34808349609375 = 0.3531675934791565 + 50.0 * 6.239898204803467
Epoch 1170, val loss: 0.9849408268928528
Epoch 1180, training loss: 312.05706787109375 = 0.3465511202812195 + 50.0 * 6.23421049118042
Epoch 1180, val loss: 0.9854313731193542
Epoch 1190, training loss: 312.0394287109375 = 0.3402359187602997 + 50.0 * 6.233983993530273
Epoch 1190, val loss: 0.9860974550247192
Epoch 1200, training loss: 311.99652099609375 = 0.33409053087234497 + 50.0 * 6.233249187469482
Epoch 1200, val loss: 0.9864735007286072
Epoch 1210, training loss: 312.2600402832031 = 0.3280947208404541 + 50.0 * 6.238638877868652
Epoch 1210, val loss: 0.9871602654457092
Epoch 1220, training loss: 311.98724365234375 = 0.3220248222351074 + 50.0 * 6.233304500579834
Epoch 1220, val loss: 0.9872575402259827
Epoch 1230, training loss: 311.9498596191406 = 0.31615445017814636 + 50.0 * 6.2326741218566895
Epoch 1230, val loss: 0.9879793524742126
Epoch 1240, training loss: 311.9402770996094 = 0.3104465901851654 + 50.0 * 6.232596397399902
Epoch 1240, val loss: 0.9886927008628845
Epoch 1250, training loss: 311.9335021972656 = 0.3048389256000519 + 50.0 * 6.23257303237915
Epoch 1250, val loss: 0.9897116422653198
Epoch 1260, training loss: 312.1329040527344 = 0.2993057668209076 + 50.0 * 6.2366719245910645
Epoch 1260, val loss: 0.9909837245941162
Epoch 1270, training loss: 311.91571044921875 = 0.29382380843162537 + 50.0 * 6.232437610626221
Epoch 1270, val loss: 0.9913763999938965
Epoch 1280, training loss: 311.79302978515625 = 0.28852275013923645 + 50.0 * 6.230090618133545
Epoch 1280, val loss: 0.9931271076202393
Epoch 1290, training loss: 311.755859375 = 0.2833656072616577 + 50.0 * 6.229450225830078
Epoch 1290, val loss: 0.994255781173706
Epoch 1300, training loss: 311.982421875 = 0.27832233905792236 + 50.0 * 6.234082221984863
Epoch 1300, val loss: 0.9960642457008362
Epoch 1310, training loss: 311.7497253417969 = 0.2733131945133209 + 50.0 * 6.229527950286865
Epoch 1310, val loss: 0.9967366456985474
Epoch 1320, training loss: 311.6916198730469 = 0.2684197723865509 + 50.0 * 6.228463649749756
Epoch 1320, val loss: 0.998587965965271
Epoch 1330, training loss: 311.9689636230469 = 0.26367318630218506 + 50.0 * 6.234106063842773
Epoch 1330, val loss: 0.9993072748184204
Epoch 1340, training loss: 311.75482177734375 = 0.25887200236320496 + 50.0 * 6.229918956756592
Epoch 1340, val loss: 1.0018951892852783
Epoch 1350, training loss: 311.6126403808594 = 0.25425904989242554 + 50.0 * 6.22716760635376
Epoch 1350, val loss: 1.003313660621643
Epoch 1360, training loss: 311.61163330078125 = 0.24978485703468323 + 50.0 * 6.227236747741699
Epoch 1360, val loss: 1.0052400827407837
Epoch 1370, training loss: 311.7184753417969 = 0.24537841975688934 + 50.0 * 6.229461669921875
Epoch 1370, val loss: 1.0072230100631714
Epoch 1380, training loss: 311.678955078125 = 0.2409842610359192 + 50.0 * 6.228759288787842
Epoch 1380, val loss: 1.0093014240264893
Epoch 1390, training loss: 311.7210388183594 = 0.23666530847549438 + 50.0 * 6.229687690734863
Epoch 1390, val loss: 1.0113751888275146
Epoch 1400, training loss: 311.62548828125 = 0.232401505112648 + 50.0 * 6.2278618812561035
Epoch 1400, val loss: 1.0136220455169678
Epoch 1410, training loss: 311.4895324707031 = 0.2283170074224472 + 50.0 * 6.225224494934082
Epoch 1410, val loss: 1.0154175758361816
Epoch 1420, training loss: 311.4985046386719 = 0.22431428730487823 + 50.0 * 6.2254838943481445
Epoch 1420, val loss: 1.0180414915084839
Epoch 1430, training loss: 311.69500732421875 = 0.2203795462846756 + 50.0 * 6.229492664337158
Epoch 1430, val loss: 1.0204664468765259
Epoch 1440, training loss: 311.5075378417969 = 0.21645858883857727 + 50.0 * 6.225821495056152
Epoch 1440, val loss: 1.022635579109192
Epoch 1450, training loss: 311.44232177734375 = 0.21268108487129211 + 50.0 * 6.224593162536621
Epoch 1450, val loss: 1.0254284143447876
Epoch 1460, training loss: 311.6240539550781 = 0.20898868143558502 + 50.0 * 6.228301525115967
Epoch 1460, val loss: 1.0279631614685059
Epoch 1470, training loss: 311.38690185546875 = 0.20526231825351715 + 50.0 * 6.2236328125
Epoch 1470, val loss: 1.0302629470825195
Epoch 1480, training loss: 311.3840026855469 = 0.20168040692806244 + 50.0 * 6.22364616394043
Epoch 1480, val loss: 1.0330179929733276
Epoch 1490, training loss: 311.4143981933594 = 0.19814562797546387 + 50.0 * 6.224325180053711
Epoch 1490, val loss: 1.035945177078247
Epoch 1500, training loss: 311.42071533203125 = 0.19465813040733337 + 50.0 * 6.224520683288574
Epoch 1500, val loss: 1.038826823234558
Epoch 1510, training loss: 311.33197021484375 = 0.1912415474653244 + 50.0 * 6.222814559936523
Epoch 1510, val loss: 1.0415935516357422
Epoch 1520, training loss: 311.3785400390625 = 0.1879112720489502 + 50.0 * 6.223813056945801
Epoch 1520, val loss: 1.0447373390197754
Epoch 1530, training loss: 311.228271484375 = 0.18460613489151 + 50.0 * 6.2208733558654785
Epoch 1530, val loss: 1.0474754571914673
Epoch 1540, training loss: 311.2289123535156 = 0.18142634630203247 + 50.0 * 6.220949649810791
Epoch 1540, val loss: 1.0504930019378662
Epoch 1550, training loss: 311.32916259765625 = 0.17830869555473328 + 50.0 * 6.223016738891602
Epoch 1550, val loss: 1.0537774562835693
Epoch 1560, training loss: 311.32830810546875 = 0.17515873908996582 + 50.0 * 6.223062992095947
Epoch 1560, val loss: 1.0572559833526611
Epoch 1570, training loss: 311.3067321777344 = 0.17204345762729645 + 50.0 * 6.22269344329834
Epoch 1570, val loss: 1.059843897819519
Epoch 1580, training loss: 311.2469787597656 = 0.16903521120548248 + 50.0 * 6.221558570861816
Epoch 1580, val loss: 1.0628604888916016
Epoch 1590, training loss: 311.1899108886719 = 0.16608253121376038 + 50.0 * 6.2204766273498535
Epoch 1590, val loss: 1.0665305852890015
Epoch 1600, training loss: 311.22808837890625 = 0.1632244884967804 + 50.0 * 6.221297264099121
Epoch 1600, val loss: 1.0700256824493408
Epoch 1610, training loss: 311.19671630859375 = 0.1603899449110031 + 50.0 * 6.220726013183594
Epoch 1610, val loss: 1.072735071182251
Epoch 1620, training loss: 311.1790771484375 = 0.15760980546474457 + 50.0 * 6.220428943634033
Epoch 1620, val loss: 1.0763473510742188
Epoch 1630, training loss: 311.1435546875 = 0.15485726296901703 + 50.0 * 6.21977424621582
Epoch 1630, val loss: 1.0798368453979492
Epoch 1640, training loss: 311.0769958496094 = 0.15217876434326172 + 50.0 * 6.218495845794678
Epoch 1640, val loss: 1.083315134048462
Epoch 1650, training loss: 311.17803955078125 = 0.14957189559936523 + 50.0 * 6.220569610595703
Epoch 1650, val loss: 1.08655846118927
Epoch 1660, training loss: 311.2512512207031 = 0.14692825078964233 + 50.0 * 6.222086429595947
Epoch 1660, val loss: 1.0909353494644165
Epoch 1670, training loss: 311.0679931640625 = 0.1443624198436737 + 50.0 * 6.218472957611084
Epoch 1670, val loss: 1.093988299369812
Epoch 1680, training loss: 310.9831848144531 = 0.14184468984603882 + 50.0 * 6.216826915740967
Epoch 1680, val loss: 1.0979045629501343
Epoch 1690, training loss: 310.9736633300781 = 0.1394270807504654 + 50.0 * 6.216684341430664
Epoch 1690, val loss: 1.1017050743103027
Epoch 1700, training loss: 311.3102722167969 = 0.13708984851837158 + 50.0 * 6.223464012145996
Epoch 1700, val loss: 1.1053929328918457
Epoch 1710, training loss: 311.082275390625 = 0.13463769853115082 + 50.0 * 6.218952655792236
Epoch 1710, val loss: 1.1095068454742432
Epoch 1720, training loss: 310.9792785644531 = 0.13230395317077637 + 50.0 * 6.216939449310303
Epoch 1720, val loss: 1.1130048036575317
Epoch 1730, training loss: 310.916259765625 = 0.13001762330532074 + 50.0 * 6.215724945068359
Epoch 1730, val loss: 1.1172536611557007
Epoch 1740, training loss: 310.9112548828125 = 0.12782208621501923 + 50.0 * 6.215668678283691
Epoch 1740, val loss: 1.1209301948547363
Epoch 1750, training loss: 311.29742431640625 = 0.1256365329027176 + 50.0 * 6.223435878753662
Epoch 1750, val loss: 1.1246862411499023
Epoch 1760, training loss: 311.0085754394531 = 0.12343573570251465 + 50.0 * 6.217702865600586
Epoch 1760, val loss: 1.1290944814682007
Epoch 1770, training loss: 310.8936462402344 = 0.12130355089902878 + 50.0 * 6.215446472167969
Epoch 1770, val loss: 1.1327688694000244
Epoch 1780, training loss: 310.9490051269531 = 0.11923794448375702 + 50.0 * 6.216595649719238
Epoch 1780, val loss: 1.1370766162872314
Epoch 1790, training loss: 310.9158630371094 = 0.11720573157072067 + 50.0 * 6.215972900390625
Epoch 1790, val loss: 1.1409223079681396
Epoch 1800, training loss: 310.9144287109375 = 0.11523428559303284 + 50.0 * 6.215983867645264
Epoch 1800, val loss: 1.1450221538543701
Epoch 1810, training loss: 310.84857177734375 = 0.11326579749584198 + 50.0 * 6.2147064208984375
Epoch 1810, val loss: 1.1493295431137085
Epoch 1820, training loss: 310.88995361328125 = 0.11136897653341293 + 50.0 * 6.215571880340576
Epoch 1820, val loss: 1.1534322500228882
Epoch 1830, training loss: 311.006591796875 = 0.10947863012552261 + 50.0 * 6.217942237854004
Epoch 1830, val loss: 1.1579515933990479
Epoch 1840, training loss: 310.80999755859375 = 0.10759176313877106 + 50.0 * 6.214047908782959
Epoch 1840, val loss: 1.1616154909133911
Epoch 1850, training loss: 310.7449645996094 = 0.10577332973480225 + 50.0 * 6.2127838134765625
Epoch 1850, val loss: 1.1661399602890015
Epoch 1860, training loss: 310.7339172363281 = 0.1040249764919281 + 50.0 * 6.212597846984863
Epoch 1860, val loss: 1.1704708337783813
Epoch 1870, training loss: 310.80450439453125 = 0.10233175754547119 + 50.0 * 6.214043617248535
Epoch 1870, val loss: 1.174704909324646
Epoch 1880, training loss: 310.9186096191406 = 0.10061842203140259 + 50.0 * 6.216359615325928
Epoch 1880, val loss: 1.1790251731872559
Epoch 1890, training loss: 310.8864440917969 = 0.09888828545808792 + 50.0 * 6.2157511711120605
Epoch 1890, val loss: 1.1833972930908203
Epoch 1900, training loss: 310.73046875 = 0.09722532331943512 + 50.0 * 6.212664604187012
Epoch 1900, val loss: 1.187293291091919
Epoch 1910, training loss: 310.7733154296875 = 0.09562033414840698 + 50.0 * 6.2135539054870605
Epoch 1910, val loss: 1.1917520761489868
Epoch 1920, training loss: 310.7012939453125 = 0.09404098987579346 + 50.0 * 6.21214485168457
Epoch 1920, val loss: 1.1963170766830444
Epoch 1930, training loss: 310.724365234375 = 0.09250881522893906 + 50.0 * 6.212637424468994
Epoch 1930, val loss: 1.2008488178253174
Epoch 1940, training loss: 310.80316162109375 = 0.09099289029836655 + 50.0 * 6.214243412017822
Epoch 1940, val loss: 1.2049554586410522
Epoch 1950, training loss: 310.7316589355469 = 0.08948551118373871 + 50.0 * 6.212843418121338
Epoch 1950, val loss: 1.2089927196502686
Epoch 1960, training loss: 310.6712341308594 = 0.08802530169487 + 50.0 * 6.211664199829102
Epoch 1960, val loss: 1.2137902975082397
Epoch 1970, training loss: 310.67584228515625 = 0.08660323172807693 + 50.0 * 6.211784839630127
Epoch 1970, val loss: 1.2182341814041138
Epoch 1980, training loss: 310.8663330078125 = 0.08521662652492523 + 50.0 * 6.215622425079346
Epoch 1980, val loss: 1.2223936319351196
Epoch 1990, training loss: 310.71209716796875 = 0.08379948884248734 + 50.0 * 6.212565898895264
Epoch 1990, val loss: 1.226875901222229
Epoch 2000, training loss: 310.7794494628906 = 0.0824420303106308 + 50.0 * 6.213940143585205
Epoch 2000, val loss: 1.2313885688781738
Epoch 2010, training loss: 310.65814208984375 = 0.0811069905757904 + 50.0 * 6.211540222167969
Epoch 2010, val loss: 1.2354223728179932
Epoch 2020, training loss: 310.5731506347656 = 0.07980244606733322 + 50.0 * 6.209867000579834
Epoch 2020, val loss: 1.2401071786880493
Epoch 2030, training loss: 310.56414794921875 = 0.0785515308380127 + 50.0 * 6.209712028503418
Epoch 2030, val loss: 1.2445857524871826
Epoch 2040, training loss: 310.6334228515625 = 0.07732988148927689 + 50.0 * 6.211122035980225
Epoch 2040, val loss: 1.2493928670883179
Epoch 2050, training loss: 310.7948913574219 = 0.07609990239143372 + 50.0 * 6.2143754959106445
Epoch 2050, val loss: 1.2534829378128052
Epoch 2060, training loss: 310.59893798828125 = 0.07485347241163254 + 50.0 * 6.210481643676758
Epoch 2060, val loss: 1.2575044631958008
Epoch 2070, training loss: 310.5192565917969 = 0.0736708790063858 + 50.0 * 6.208911895751953
Epoch 2070, val loss: 1.2620006799697876
Epoch 2080, training loss: 310.5137634277344 = 0.07252893596887589 + 50.0 * 6.208824157714844
Epoch 2080, val loss: 1.2665013074874878
Epoch 2090, training loss: 310.83392333984375 = 0.07142581045627594 + 50.0 * 6.215250492095947
Epoch 2090, val loss: 1.2707078456878662
Epoch 2100, training loss: 310.5288391113281 = 0.07029938697814941 + 50.0 * 6.209170818328857
Epoch 2100, val loss: 1.2751291990280151
Epoch 2110, training loss: 310.5166931152344 = 0.06920292228460312 + 50.0 * 6.208949565887451
Epoch 2110, val loss: 1.2795605659484863
Epoch 2120, training loss: 310.5597229003906 = 0.06814640015363693 + 50.0 * 6.209831714630127
Epoch 2120, val loss: 1.283931016921997
Epoch 2130, training loss: 310.4971008300781 = 0.06709505617618561 + 50.0 * 6.208600044250488
Epoch 2130, val loss: 1.2886172533035278
Epoch 2140, training loss: 310.8038635253906 = 0.06609362363815308 + 50.0 * 6.214755058288574
Epoch 2140, val loss: 1.2925492525100708
Epoch 2150, training loss: 310.5782775878906 = 0.06504011899232864 + 50.0 * 6.210265159606934
Epoch 2150, val loss: 1.2970831394195557
Epoch 2160, training loss: 310.4556579589844 = 0.06404682993888855 + 50.0 * 6.207831859588623
Epoch 2160, val loss: 1.3017271757125854
Epoch 2170, training loss: 310.4377136230469 = 0.06308870762586594 + 50.0 * 6.207492828369141
Epoch 2170, val loss: 1.3061858415603638
Epoch 2180, training loss: 310.5923767089844 = 0.062155794352293015 + 50.0 * 6.210604190826416
Epoch 2180, val loss: 1.3105474710464478
Epoch 2190, training loss: 310.39569091796875 = 0.06122122332453728 + 50.0 * 6.206689357757568
Epoch 2190, val loss: 1.3148610591888428
Epoch 2200, training loss: 310.6015930175781 = 0.0603315606713295 + 50.0 * 6.210825443267822
Epoch 2200, val loss: 1.3188639879226685
Epoch 2210, training loss: 310.4370422363281 = 0.05940062552690506 + 50.0 * 6.207553386688232
Epoch 2210, val loss: 1.323338508605957
Epoch 2220, training loss: 310.4069519042969 = 0.05851949378848076 + 50.0 * 6.206968307495117
Epoch 2220, val loss: 1.3278818130493164
Epoch 2230, training loss: 310.49432373046875 = 0.05767529085278511 + 50.0 * 6.208733081817627
Epoch 2230, val loss: 1.3322969675064087
Epoch 2240, training loss: 310.5180358886719 = 0.05681183561682701 + 50.0 * 6.209224224090576
Epoch 2240, val loss: 1.3364195823669434
Epoch 2250, training loss: 310.40008544921875 = 0.055969126522541046 + 50.0 * 6.206882476806641
Epoch 2250, val loss: 1.3405824899673462
Epoch 2260, training loss: 310.34063720703125 = 0.055156927555799484 + 50.0 * 6.205709934234619
Epoch 2260, val loss: 1.3448983430862427
Epoch 2270, training loss: 310.3279113769531 = 0.054375022649765015 + 50.0 * 6.205470561981201
Epoch 2270, val loss: 1.349366545677185
Epoch 2280, training loss: 310.3866271972656 = 0.05360989645123482 + 50.0 * 6.206660270690918
Epoch 2280, val loss: 1.3537123203277588
Epoch 2290, training loss: 310.5595397949219 = 0.05283603072166443 + 50.0 * 6.210134029388428
Epoch 2290, val loss: 1.3580268621444702
Epoch 2300, training loss: 310.40679931640625 = 0.05205387622117996 + 50.0 * 6.207094669342041
Epoch 2300, val loss: 1.361925482749939
Epoch 2310, training loss: 310.36572265625 = 0.051309533417224884 + 50.0 * 6.2062883377075195
Epoch 2310, val loss: 1.3662563562393188
Epoch 2320, training loss: 310.4264831542969 = 0.0505819097161293 + 50.0 * 6.207518577575684
Epoch 2320, val loss: 1.3705638647079468
Epoch 2330, training loss: 310.4734191894531 = 0.04987800493836403 + 50.0 * 6.208470821380615
Epoch 2330, val loss: 1.3749985694885254
Epoch 2340, training loss: 310.2612609863281 = 0.04915608465671539 + 50.0 * 6.20424222946167
Epoch 2340, val loss: 1.3786778450012207
Epoch 2350, training loss: 310.2437744140625 = 0.04846668243408203 + 50.0 * 6.203906536102295
Epoch 2350, val loss: 1.383039116859436
Epoch 2360, training loss: 310.2491455078125 = 0.047808971256017685 + 50.0 * 6.204026222229004
Epoch 2360, val loss: 1.3874874114990234
Epoch 2370, training loss: 310.4685974121094 = 0.04716581851243973 + 50.0 * 6.208428382873535
Epoch 2370, val loss: 1.391776204109192
Epoch 2380, training loss: 310.2970886230469 = 0.04649736359715462 + 50.0 * 6.20501184463501
Epoch 2380, val loss: 1.3949552774429321
Epoch 2390, training loss: 310.2817077636719 = 0.04583359882235527 + 50.0 * 6.20471715927124
Epoch 2390, val loss: 1.399210810661316
Epoch 2400, training loss: 310.273681640625 = 0.045214418321847916 + 50.0 * 6.204569339752197
Epoch 2400, val loss: 1.4032942056655884
Epoch 2410, training loss: 310.5314025878906 = 0.044607799500226974 + 50.0 * 6.209735870361328
Epoch 2410, val loss: 1.4073290824890137
Epoch 2420, training loss: 310.2471008300781 = 0.04397691413760185 + 50.0 * 6.204062461853027
Epoch 2420, val loss: 1.4111888408660889
Epoch 2430, training loss: 310.2208251953125 = 0.043380025774240494 + 50.0 * 6.203548908233643
Epoch 2430, val loss: 1.4153579473495483
Epoch 2440, training loss: 310.4056701660156 = 0.04281231015920639 + 50.0 * 6.20725679397583
Epoch 2440, val loss: 1.4192177057266235
Epoch 2450, training loss: 310.3678283691406 = 0.042219195514917374 + 50.0 * 6.206512451171875
Epoch 2450, val loss: 1.4232254028320312
Epoch 2460, training loss: 310.2169189453125 = 0.04164210334420204 + 50.0 * 6.203505516052246
Epoch 2460, val loss: 1.426802396774292
Epoch 2470, training loss: 310.177001953125 = 0.04108450189232826 + 50.0 * 6.202718734741211
Epoch 2470, val loss: 1.4308016300201416
Epoch 2480, training loss: 310.18133544921875 = 0.040548842400312424 + 50.0 * 6.202815532684326
Epoch 2480, val loss: 1.4348275661468506
Epoch 2490, training loss: 310.4410705566406 = 0.0400361530482769 + 50.0 * 6.2080206871032715
Epoch 2490, val loss: 1.4384077787399292
Epoch 2500, training loss: 310.3027648925781 = 0.03948262706398964 + 50.0 * 6.205265522003174
Epoch 2500, val loss: 1.442294955253601
Epoch 2510, training loss: 310.2171630859375 = 0.038946982473134995 + 50.0 * 6.203564167022705
Epoch 2510, val loss: 1.4461561441421509
Epoch 2520, training loss: 310.1920166015625 = 0.03844030573964119 + 50.0 * 6.203071594238281
Epoch 2520, val loss: 1.4500023126602173
Epoch 2530, training loss: 310.2340393066406 = 0.037952303886413574 + 50.0 * 6.203921318054199
Epoch 2530, val loss: 1.4535948038101196
Epoch 2540, training loss: 310.33038330078125 = 0.037461310625076294 + 50.0 * 6.20585823059082
Epoch 2540, val loss: 1.4575271606445312
Epoch 2550, training loss: 310.13787841796875 = 0.03696407005190849 + 50.0 * 6.2020182609558105
Epoch 2550, val loss: 1.4612929821014404
Epoch 2560, training loss: 310.1109619140625 = 0.036483652889728546 + 50.0 * 6.2014899253845215
Epoch 2560, val loss: 1.465032696723938
Epoch 2570, training loss: 310.1170349121094 = 0.036026157438755035 + 50.0 * 6.201619625091553
Epoch 2570, val loss: 1.4688059091567993
Epoch 2580, training loss: 310.42376708984375 = 0.03558702394366264 + 50.0 * 6.207763671875
Epoch 2580, val loss: 1.4725261926651
Epoch 2590, training loss: 310.1270446777344 = 0.035110052675008774 + 50.0 * 6.201838970184326
Epoch 2590, val loss: 1.4761759042739868
Epoch 2600, training loss: 310.07745361328125 = 0.03465902805328369 + 50.0 * 6.200855731964111
Epoch 2600, val loss: 1.4795832633972168
Epoch 2610, training loss: 310.1031799316406 = 0.03422774374485016 + 50.0 * 6.20137882232666
Epoch 2610, val loss: 1.4835151433944702
Epoch 2620, training loss: 310.26556396484375 = 0.03380396217107773 + 50.0 * 6.204635143280029
Epoch 2620, val loss: 1.487092137336731
Epoch 2630, training loss: 310.31280517578125 = 0.03337002918124199 + 50.0 * 6.2055888175964355
Epoch 2630, val loss: 1.4904038906097412
Epoch 2640, training loss: 310.10675048828125 = 0.032952215522527695 + 50.0 * 6.201475620269775
Epoch 2640, val loss: 1.493769645690918
Epoch 2650, training loss: 310.0323181152344 = 0.0325397290289402 + 50.0 * 6.199995994567871
Epoch 2650, val loss: 1.497444748878479
Epoch 2660, training loss: 310.0173034667969 = 0.03215005248785019 + 50.0 * 6.199703216552734
Epoch 2660, val loss: 1.5011118650436401
Epoch 2670, training loss: 310.1064147949219 = 0.031774796545505524 + 50.0 * 6.201492786407471
Epoch 2670, val loss: 1.5042543411254883
Epoch 2680, training loss: 310.181640625 = 0.03137589246034622 + 50.0 * 6.203005313873291
Epoch 2680, val loss: 1.5075187683105469
Epoch 2690, training loss: 310.0272521972656 = 0.030973121523857117 + 50.0 * 6.199925422668457
Epoch 2690, val loss: 1.510921835899353
Epoch 2700, training loss: 310.0266418457031 = 0.030593575909733772 + 50.0 * 6.199921131134033
Epoch 2700, val loss: 1.5146790742874146
Epoch 2710, training loss: 310.0169677734375 = 0.030235374346375465 + 50.0 * 6.199734210968018
Epoch 2710, val loss: 1.5178465843200684
Epoch 2720, training loss: 310.33837890625 = 0.02988855540752411 + 50.0 * 6.206170082092285
Epoch 2720, val loss: 1.5210883617401123
Epoch 2730, training loss: 310.0741882324219 = 0.029518788680434227 + 50.0 * 6.200893402099609
Epoch 2730, val loss: 1.5245287418365479
Epoch 2740, training loss: 310.04998779296875 = 0.029161745682358742 + 50.0 * 6.200416088104248
Epoch 2740, val loss: 1.5278353691101074
Epoch 2750, training loss: 310.0459289550781 = 0.028818996623158455 + 50.0 * 6.200342178344727
Epoch 2750, val loss: 1.5313122272491455
Epoch 2760, training loss: 310.216552734375 = 0.028479810804128647 + 50.0 * 6.203761577606201
Epoch 2760, val loss: 1.5347391366958618
Epoch 2770, training loss: 310.02740478515625 = 0.028135238215327263 + 50.0 * 6.199985504150391
Epoch 2770, val loss: 1.537404179573059
Epoch 2780, training loss: 309.97381591796875 = 0.02780590020120144 + 50.0 * 6.198920249938965
Epoch 2780, val loss: 1.5406162738800049
Epoch 2790, training loss: 310.0611267089844 = 0.02748749777674675 + 50.0 * 6.200672626495361
Epoch 2790, val loss: 1.5442538261413574
Epoch 2800, training loss: 310.06451416015625 = 0.02717003785073757 + 50.0 * 6.200747013092041
Epoch 2800, val loss: 1.547271490097046
Epoch 2810, training loss: 310.14617919921875 = 0.026861324906349182 + 50.0 * 6.202386379241943
Epoch 2810, val loss: 1.5503637790679932
Epoch 2820, training loss: 309.9820861816406 = 0.026539133861660957 + 50.0 * 6.199110984802246
Epoch 2820, val loss: 1.5532907247543335
Epoch 2830, training loss: 309.92333984375 = 0.026225654408335686 + 50.0 * 6.19794225692749
Epoch 2830, val loss: 1.556450605392456
Epoch 2840, training loss: 309.9089050292969 = 0.025932947173714638 + 50.0 * 6.197659492492676
Epoch 2840, val loss: 1.5597846508026123
Epoch 2850, training loss: 309.95086669921875 = 0.025646492838859558 + 50.0 * 6.198504447937012
Epoch 2850, val loss: 1.5628505945205688
Epoch 2860, training loss: 310.1850280761719 = 0.025360271334648132 + 50.0 * 6.203193187713623
Epoch 2860, val loss: 1.5656667947769165
Epoch 2870, training loss: 310.1068420410156 = 0.02508172206580639 + 50.0 * 6.201634883880615
Epoch 2870, val loss: 1.5684667825698853
Epoch 2880, training loss: 310.05010986328125 = 0.024782756343483925 + 50.0 * 6.200506687164307
Epoch 2880, val loss: 1.5715298652648926
Epoch 2890, training loss: 310.01654052734375 = 0.024504918605089188 + 50.0 * 6.199840545654297
Epoch 2890, val loss: 1.5750577449798584
Epoch 2900, training loss: 309.9346618652344 = 0.02422788366675377 + 50.0 * 6.198208808898926
Epoch 2900, val loss: 1.5777390003204346
Epoch 2910, training loss: 309.9515380859375 = 0.02396412566304207 + 50.0 * 6.198551654815674
Epoch 2910, val loss: 1.5803691148757935
Epoch 2920, training loss: 310.05755615234375 = 0.02370389550924301 + 50.0 * 6.200676918029785
Epoch 2920, val loss: 1.5835155248641968
Epoch 2930, training loss: 309.93768310546875 = 0.02343447506427765 + 50.0 * 6.198285102844238
Epoch 2930, val loss: 1.5864311456680298
Epoch 2940, training loss: 309.919677734375 = 0.023175735026597977 + 50.0 * 6.197930335998535
Epoch 2940, val loss: 1.58926260471344
Epoch 2950, training loss: 309.93231201171875 = 0.022931167855858803 + 50.0 * 6.198187828063965
Epoch 2950, val loss: 1.5921155214309692
Epoch 2960, training loss: 309.97454833984375 = 0.022685837000608444 + 50.0 * 6.199037551879883
Epoch 2960, val loss: 1.5952355861663818
Epoch 2970, training loss: 309.852294921875 = 0.022433999925851822 + 50.0 * 6.196597576141357
Epoch 2970, val loss: 1.5981134176254272
Epoch 2980, training loss: 310.06744384765625 = 0.022204605862498283 + 50.0 * 6.200904846191406
Epoch 2980, val loss: 1.6007879972457886
Epoch 2990, training loss: 310.0457458496094 = 0.021961759775877 + 50.0 * 6.200475215911865
Epoch 2990, val loss: 1.6031790971755981
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8423827095413812
=== training gcn model ===
Epoch 0, training loss: 431.77685546875 = 1.9370405673980713 + 50.0 * 8.596796035766602
Epoch 0, val loss: 1.9376211166381836
Epoch 10, training loss: 431.71783447265625 = 1.9285742044448853 + 50.0 * 8.595785140991211
Epoch 10, val loss: 1.92843759059906
Epoch 20, training loss: 431.3846740722656 = 1.9181039333343506 + 50.0 * 8.58933162689209
Epoch 20, val loss: 1.9168651103973389
Epoch 30, training loss: 429.2278747558594 = 1.9045642614364624 + 50.0 * 8.546465873718262
Epoch 30, val loss: 1.901887059211731
Epoch 40, training loss: 415.3295593261719 = 1.888253092765808 + 50.0 * 8.268826484680176
Epoch 40, val loss: 1.8847391605377197
Epoch 50, training loss: 374.5284423828125 = 1.8695130348205566 + 50.0 * 7.453178405761719
Epoch 50, val loss: 1.8657883405685425
Epoch 60, training loss: 361.583251953125 = 1.8563904762268066 + 50.0 * 7.194537162780762
Epoch 60, val loss: 1.8536568880081177
Epoch 70, training loss: 351.86444091796875 = 1.8454256057739258 + 50.0 * 7.000380516052246
Epoch 70, val loss: 1.843094825744629
Epoch 80, training loss: 345.1965637207031 = 1.8352537155151367 + 50.0 * 6.8672261238098145
Epoch 80, val loss: 1.8333467245101929
Epoch 90, training loss: 339.9190368652344 = 1.827833890914917 + 50.0 * 6.761824131011963
Epoch 90, val loss: 1.8261709213256836
Epoch 100, training loss: 336.254150390625 = 1.8215242624282837 + 50.0 * 6.688652515411377
Epoch 100, val loss: 1.8200874328613281
Epoch 110, training loss: 333.8613586425781 = 1.8156195878982544 + 50.0 * 6.6409149169921875
Epoch 110, val loss: 1.8141703605651855
Epoch 120, training loss: 331.92462158203125 = 1.8094940185546875 + 50.0 * 6.602302551269531
Epoch 120, val loss: 1.8078612089157104
Epoch 130, training loss: 330.1835021972656 = 1.8035422563552856 + 50.0 * 6.567599773406982
Epoch 130, val loss: 1.8017148971557617
Epoch 140, training loss: 328.7637023925781 = 1.797886848449707 + 50.0 * 6.539316177368164
Epoch 140, val loss: 1.795838713645935
Epoch 150, training loss: 327.6817626953125 = 1.7921017408370972 + 50.0 * 6.517792701721191
Epoch 150, val loss: 1.7898839712142944
Epoch 160, training loss: 326.5809326171875 = 1.785861849784851 + 50.0 * 6.495901584625244
Epoch 160, val loss: 1.783593773841858
Epoch 170, training loss: 325.6705627441406 = 1.7792255878448486 + 50.0 * 6.4778265953063965
Epoch 170, val loss: 1.7770147323608398
Epoch 180, training loss: 325.0192565917969 = 1.7720351219177246 + 50.0 * 6.464944362640381
Epoch 180, val loss: 1.7701005935668945
Epoch 190, training loss: 324.30377197265625 = 1.7641130685806274 + 50.0 * 6.450793743133545
Epoch 190, val loss: 1.7625102996826172
Epoch 200, training loss: 323.6722412109375 = 1.7555056810379028 + 50.0 * 6.438334941864014
Epoch 200, val loss: 1.7543509006500244
Epoch 210, training loss: 323.198486328125 = 1.746113657951355 + 50.0 * 6.429047107696533
Epoch 210, val loss: 1.7454546689987183
Epoch 220, training loss: 322.6630554199219 = 1.735805869102478 + 50.0 * 6.418544769287109
Epoch 220, val loss: 1.735732913017273
Epoch 230, training loss: 322.18597412109375 = 1.7245254516601562 + 50.0 * 6.409229278564453
Epoch 230, val loss: 1.725177526473999
Epoch 240, training loss: 321.8515930175781 = 1.712225079536438 + 50.0 * 6.402787685394287
Epoch 240, val loss: 1.7136611938476562
Epoch 250, training loss: 321.3706970214844 = 1.6987988948822021 + 50.0 * 6.39343786239624
Epoch 250, val loss: 1.7011713981628418
Epoch 260, training loss: 320.97943115234375 = 1.6842217445373535 + 50.0 * 6.385903835296631
Epoch 260, val loss: 1.6876630783081055
Epoch 270, training loss: 320.8267822265625 = 1.6684792041778564 + 50.0 * 6.3831658363342285
Epoch 270, val loss: 1.6730270385742188
Epoch 280, training loss: 320.3456726074219 = 1.6513135433197021 + 50.0 * 6.373887538909912
Epoch 280, val loss: 1.6573106050491333
Epoch 290, training loss: 319.9725646972656 = 1.6330175399780273 + 50.0 * 6.366790771484375
Epoch 290, val loss: 1.640636682510376
Epoch 300, training loss: 319.7472229003906 = 1.6134730577468872 + 50.0 * 6.362675189971924
Epoch 300, val loss: 1.6229296922683716
Epoch 310, training loss: 319.4407043457031 = 1.5927863121032715 + 50.0 * 6.356958866119385
Epoch 310, val loss: 1.6041619777679443
Epoch 320, training loss: 319.1606750488281 = 1.5709733963012695 + 50.0 * 6.3517937660217285
Epoch 320, val loss: 1.5846362113952637
Epoch 330, training loss: 318.8798828125 = 1.5482045412063599 + 50.0 * 6.346633434295654
Epoch 330, val loss: 1.564342975616455
Epoch 340, training loss: 318.69879150390625 = 1.5245386362075806 + 50.0 * 6.343485355377197
Epoch 340, val loss: 1.5433632135391235
Epoch 350, training loss: 318.4485778808594 = 1.500193476676941 + 50.0 * 6.338967800140381
Epoch 350, val loss: 1.5220662355422974
Epoch 360, training loss: 318.23858642578125 = 1.4754077196121216 + 50.0 * 6.335263729095459
Epoch 360, val loss: 1.5004565715789795
Epoch 370, training loss: 318.0824279785156 = 1.4500343799591064 + 50.0 * 6.332647800445557
Epoch 370, val loss: 1.4786018133163452
Epoch 380, training loss: 317.8157653808594 = 1.42463219165802 + 50.0 * 6.327823162078857
Epoch 380, val loss: 1.4567575454711914
Epoch 390, training loss: 317.6139221191406 = 1.399161696434021 + 50.0 * 6.3242950439453125
Epoch 390, val loss: 1.4351216554641724
Epoch 400, training loss: 317.5758361816406 = 1.3737937211990356 + 50.0 * 6.324041366577148
Epoch 400, val loss: 1.413779377937317
Epoch 410, training loss: 317.36993408203125 = 1.348489761352539 + 50.0 * 6.32042932510376
Epoch 410, val loss: 1.3926939964294434
Epoch 420, training loss: 317.10748291015625 = 1.3235291242599487 + 50.0 * 6.31567907333374
Epoch 420, val loss: 1.3719772100448608
Epoch 430, training loss: 316.91900634765625 = 1.298973560333252 + 50.0 * 6.3124003410339355
Epoch 430, val loss: 1.3518779277801514
Epoch 440, training loss: 316.8487854003906 = 1.2749146223068237 + 50.0 * 6.3114776611328125
Epoch 440, val loss: 1.3324017524719238
Epoch 450, training loss: 316.8007507324219 = 1.2512784004211426 + 50.0 * 6.3109893798828125
Epoch 450, val loss: 1.3133233785629272
Epoch 460, training loss: 316.48199462890625 = 1.2279871702194214 + 50.0 * 6.305079936981201
Epoch 460, val loss: 1.2948797941207886
Epoch 470, training loss: 316.31890869140625 = 1.2054194211959839 + 50.0 * 6.30226993560791
Epoch 470, val loss: 1.2771953344345093
Epoch 480, training loss: 316.3426818847656 = 1.1833722591400146 + 50.0 * 6.303185939788818
Epoch 480, val loss: 1.2601615190505981
Epoch 490, training loss: 316.0531921386719 = 1.1617767810821533 + 50.0 * 6.297828197479248
Epoch 490, val loss: 1.2436543703079224
Epoch 500, training loss: 315.8931884765625 = 1.1408259868621826 + 50.0 * 6.295047283172607
Epoch 500, val loss: 1.2279573678970337
Epoch 510, training loss: 315.982666015625 = 1.1203328371047974 + 50.0 * 6.297246932983398
Epoch 510, val loss: 1.2128905057907104
Epoch 520, training loss: 315.7256164550781 = 1.100433349609375 + 50.0 * 6.292503356933594
Epoch 520, val loss: 1.198330044746399
Epoch 530, training loss: 315.53265380859375 = 1.0812127590179443 + 50.0 * 6.289029121398926
Epoch 530, val loss: 1.1846051216125488
Epoch 540, training loss: 315.4464111328125 = 1.0625874996185303 + 50.0 * 6.2876763343811035
Epoch 540, val loss: 1.171583652496338
Epoch 550, training loss: 315.4317932128906 = 1.0443626642227173 + 50.0 * 6.287748336791992
Epoch 550, val loss: 1.159123182296753
Epoch 560, training loss: 315.3700866699219 = 1.0264915227890015 + 50.0 * 6.286871910095215
Epoch 560, val loss: 1.1471065282821655
Epoch 570, training loss: 315.1080322265625 = 1.0091739892959595 + 50.0 * 6.281976699829102
Epoch 570, val loss: 1.1357972621917725
Epoch 580, training loss: 315.0018005371094 = 0.992444634437561 + 50.0 * 6.280187606811523
Epoch 580, val loss: 1.125180721282959
Epoch 590, training loss: 315.30303955078125 = 0.9760820269584656 + 50.0 * 6.286539554595947
Epoch 590, val loss: 1.11504328250885
Epoch 600, training loss: 314.85455322265625 = 0.9601635336875916 + 50.0 * 6.277887344360352
Epoch 600, val loss: 1.105190396308899
Epoch 610, training loss: 314.7631530761719 = 0.9446406364440918 + 50.0 * 6.276370048522949
Epoch 610, val loss: 1.0960338115692139
Epoch 620, training loss: 314.69036865234375 = 0.929564356803894 + 50.0 * 6.275216102600098
Epoch 620, val loss: 1.0874149799346924
Epoch 630, training loss: 314.56103515625 = 0.9147084355354309 + 50.0 * 6.2729268074035645
Epoch 630, val loss: 1.0791531801223755
Epoch 640, training loss: 314.5830993652344 = 0.9002353549003601 + 50.0 * 6.273657321929932
Epoch 640, val loss: 1.0712356567382812
Epoch 650, training loss: 314.43426513671875 = 0.8859357833862305 + 50.0 * 6.27096700668335
Epoch 650, val loss: 1.0636743307113647
Epoch 660, training loss: 314.3548278808594 = 0.8720622658729553 + 50.0 * 6.269655227661133
Epoch 660, val loss: 1.05657958984375
Epoch 670, training loss: 314.30059814453125 = 0.8584378361701965 + 50.0 * 6.268843650817871
Epoch 670, val loss: 1.0497764348983765
Epoch 680, training loss: 314.1942138671875 = 0.8450213074684143 + 50.0 * 6.266983509063721
Epoch 680, val loss: 1.0433897972106934
Epoch 690, training loss: 314.13751220703125 = 0.8319161534309387 + 50.0 * 6.266112327575684
Epoch 690, val loss: 1.0372902154922485
Epoch 700, training loss: 314.23321533203125 = 0.8189204335212708 + 50.0 * 6.268286228179932
Epoch 700, val loss: 1.0312378406524658
Epoch 710, training loss: 314.01434326171875 = 0.8058192729949951 + 50.0 * 6.2641706466674805
Epoch 710, val loss: 1.0254652500152588
Epoch 720, training loss: 313.9111022949219 = 0.7932021617889404 + 50.0 * 6.262357711791992
Epoch 720, val loss: 1.0201687812805176
Epoch 730, training loss: 313.8406982421875 = 0.7808069586753845 + 50.0 * 6.261198043823242
Epoch 730, val loss: 1.0151945352554321
Epoch 740, training loss: 314.21038818359375 = 0.7685543298721313 + 50.0 * 6.268836498260498
Epoch 740, val loss: 1.0103278160095215
Epoch 750, training loss: 313.7498779296875 = 0.7561845183372498 + 50.0 * 6.259873867034912
Epoch 750, val loss: 1.0054292678833008
Epoch 760, training loss: 313.66796875 = 0.7441110014915466 + 50.0 * 6.258477210998535
Epoch 760, val loss: 1.0010119676589966
Epoch 770, training loss: 313.605712890625 = 0.7322369813919067 + 50.0 * 6.257469177246094
Epoch 770, val loss: 0.9969298243522644
Epoch 780, training loss: 313.7850036621094 = 0.7204287648200989 + 50.0 * 6.26129150390625
Epoch 780, val loss: 0.9928749203681946
Epoch 790, training loss: 313.57855224609375 = 0.7085685133934021 + 50.0 * 6.257400035858154
Epoch 790, val loss: 0.9889315962791443
Epoch 800, training loss: 313.4898681640625 = 0.6969128847122192 + 50.0 * 6.255859375
Epoch 800, val loss: 0.9851635098457336
Epoch 810, training loss: 313.39862060546875 = 0.6853649616241455 + 50.0 * 6.254265308380127
Epoch 810, val loss: 0.9816296696662903
Epoch 820, training loss: 313.37054443359375 = 0.6738978028297424 + 50.0 * 6.253932952880859
Epoch 820, val loss: 0.9783229231834412
Epoch 830, training loss: 313.30645751953125 = 0.6624979972839355 + 50.0 * 6.2528791427612305
Epoch 830, val loss: 0.9751487374305725
Epoch 840, training loss: 313.2193298339844 = 0.6512757539749146 + 50.0 * 6.25136137008667
Epoch 840, val loss: 0.9721251130104065
Epoch 850, training loss: 313.1596374511719 = 0.6401862502098083 + 50.0 * 6.2503886222839355
Epoch 850, val loss: 0.9694182276725769
Epoch 860, training loss: 313.24761962890625 = 0.6292677521705627 + 50.0 * 6.25236701965332
Epoch 860, val loss: 0.9667967557907104
Epoch 870, training loss: 313.22332763671875 = 0.6181076169013977 + 50.0 * 6.25210428237915
Epoch 870, val loss: 0.964203417301178
Epoch 880, training loss: 313.0714416503906 = 0.6071712970733643 + 50.0 * 6.2492852210998535
Epoch 880, val loss: 0.961708128452301
Epoch 890, training loss: 312.9617614746094 = 0.5964542627334595 + 50.0 * 6.247305870056152
Epoch 890, val loss: 0.9596611857414246
Epoch 900, training loss: 312.92047119140625 = 0.5858639478683472 + 50.0 * 6.246692180633545
Epoch 900, val loss: 0.9578374624252319
Epoch 910, training loss: 313.1449279785156 = 0.5753470063209534 + 50.0 * 6.251391887664795
Epoch 910, val loss: 0.9559998512268066
Epoch 920, training loss: 313.0594177246094 = 0.5647490620613098 + 50.0 * 6.2498931884765625
Epoch 920, val loss: 0.9541135430335999
Epoch 930, training loss: 312.8132019042969 = 0.5542149543762207 + 50.0 * 6.245179176330566
Epoch 930, val loss: 0.952492356300354
Epoch 940, training loss: 312.7361145019531 = 0.5439542531967163 + 50.0 * 6.2438435554504395
Epoch 940, val loss: 0.9511495232582092
Epoch 950, training loss: 312.76318359375 = 0.533875048160553 + 50.0 * 6.244585990905762
Epoch 950, val loss: 0.9499186277389526
Epoch 960, training loss: 312.74420166015625 = 0.5237014889717102 + 50.0 * 6.244410037994385
Epoch 960, val loss: 0.948580265045166
Epoch 970, training loss: 312.78564453125 = 0.5135716795921326 + 50.0 * 6.245441436767578
Epoch 970, val loss: 0.9472784996032715
Epoch 980, training loss: 312.6423645019531 = 0.5035001039505005 + 50.0 * 6.242777347564697
Epoch 980, val loss: 0.946198582649231
Epoch 990, training loss: 312.5384216308594 = 0.4937884509563446 + 50.0 * 6.2408928871154785
Epoch 990, val loss: 0.9453607797622681
Epoch 1000, training loss: 312.48577880859375 = 0.4841481149196625 + 50.0 * 6.240032196044922
Epoch 1000, val loss: 0.9446157813072205
Epoch 1010, training loss: 312.5899658203125 = 0.474601149559021 + 50.0 * 6.242307186126709
Epoch 1010, val loss: 0.9439005255699158
Epoch 1020, training loss: 312.4418029785156 = 0.46497082710266113 + 50.0 * 6.239536762237549
Epoch 1020, val loss: 0.9431437849998474
Epoch 1030, training loss: 312.4303283691406 = 0.45552071928977966 + 50.0 * 6.239495754241943
Epoch 1030, val loss: 0.9425172209739685
Epoch 1040, training loss: 312.5519104003906 = 0.44623929262161255 + 50.0 * 6.2421135902404785
Epoch 1040, val loss: 0.9420751929283142
Epoch 1050, training loss: 312.43658447265625 = 0.4369720220565796 + 50.0 * 6.239992141723633
Epoch 1050, val loss: 0.9415579438209534
Epoch 1060, training loss: 312.2898864746094 = 0.42780521512031555 + 50.0 * 6.237241744995117
Epoch 1060, val loss: 0.9412857294082642
Epoch 1070, training loss: 312.23486328125 = 0.41895920038223267 + 50.0 * 6.236318111419678
Epoch 1070, val loss: 0.9413663744926453
Epoch 1080, training loss: 312.177978515625 = 0.4101944863796234 + 50.0 * 6.235355854034424
Epoch 1080, val loss: 0.9413880705833435
Epoch 1090, training loss: 312.3174743652344 = 0.4015573561191559 + 50.0 * 6.23831844329834
Epoch 1090, val loss: 0.9415345788002014
Epoch 1100, training loss: 312.2342224121094 = 0.3928751051425934 + 50.0 * 6.2368268966674805
Epoch 1100, val loss: 0.941386342048645
Epoch 1110, training loss: 312.19873046875 = 0.3843016028404236 + 50.0 * 6.236288547515869
Epoch 1110, val loss: 0.9415026307106018
Epoch 1120, training loss: 312.08929443359375 = 0.37603840231895447 + 50.0 * 6.234265327453613
Epoch 1120, val loss: 0.9419483542442322
Epoch 1130, training loss: 312.1056213378906 = 0.36791765689849854 + 50.0 * 6.2347540855407715
Epoch 1130, val loss: 0.9425069093704224
Epoch 1140, training loss: 311.998779296875 = 0.35994774103164673 + 50.0 * 6.232776641845703
Epoch 1140, val loss: 0.9430513978004456
Epoch 1150, training loss: 312.2001647949219 = 0.3521706759929657 + 50.0 * 6.236959934234619
Epoch 1150, val loss: 0.9437375664710999
Epoch 1160, training loss: 312.13616943359375 = 0.34436023235321045 + 50.0 * 6.235836029052734
Epoch 1160, val loss: 0.944496750831604
Epoch 1170, training loss: 311.947998046875 = 0.33673223853111267 + 50.0 * 6.23222541809082
Epoch 1170, val loss: 0.945320725440979
Epoch 1180, training loss: 311.8546447753906 = 0.3293517529964447 + 50.0 * 6.23050594329834
Epoch 1180, val loss: 0.9465621113777161
Epoch 1190, training loss: 311.90106201171875 = 0.32216817140579224 + 50.0 * 6.2315778732299805
Epoch 1190, val loss: 0.9479170441627502
Epoch 1200, training loss: 311.86932373046875 = 0.31506067514419556 + 50.0 * 6.231085300445557
Epoch 1200, val loss: 0.9490079283714294
Epoch 1210, training loss: 311.8079528808594 = 0.3080873191356659 + 50.0 * 6.229997634887695
Epoch 1210, val loss: 0.9505467414855957
Epoch 1220, training loss: 311.9317626953125 = 0.30136242508888245 + 50.0 * 6.232607841491699
Epoch 1220, val loss: 0.952103316783905
Epoch 1230, training loss: 311.75555419921875 = 0.2946378290653229 + 50.0 * 6.229218006134033
Epoch 1230, val loss: 0.9537936449050903
Epoch 1240, training loss: 311.99493408203125 = 0.2882232666015625 + 50.0 * 6.234134197235107
Epoch 1240, val loss: 0.9556491374969482
Epoch 1250, training loss: 311.754638671875 = 0.2817588448524475 + 50.0 * 6.229457378387451
Epoch 1250, val loss: 0.9574208855628967
Epoch 1260, training loss: 311.6651916503906 = 0.27563610672950745 + 50.0 * 6.2277913093566895
Epoch 1260, val loss: 0.9596460461616516
Epoch 1270, training loss: 311.6116638183594 = 0.2696722745895386 + 50.0 * 6.226839542388916
Epoch 1270, val loss: 0.9620658755302429
Epoch 1280, training loss: 311.6422119140625 = 0.26386722922325134 + 50.0 * 6.227567195892334
Epoch 1280, val loss: 0.9644268155097961
Epoch 1290, training loss: 311.718017578125 = 0.2580432891845703 + 50.0 * 6.229199409484863
Epoch 1290, val loss: 0.9666286706924438
Epoch 1300, training loss: 311.658935546875 = 0.2523430585861206 + 50.0 * 6.2281317710876465
Epoch 1300, val loss: 0.9688428044319153
Epoch 1310, training loss: 311.50238037109375 = 0.24688749015331268 + 50.0 * 6.225109577178955
Epoch 1310, val loss: 0.9716131091117859
Epoch 1320, training loss: 311.48699951171875 = 0.24161911010742188 + 50.0 * 6.224907875061035
Epoch 1320, val loss: 0.9746686220169067
Epoch 1330, training loss: 311.5018615722656 = 0.23648640513420105 + 50.0 * 6.225307464599609
Epoch 1330, val loss: 0.9775723218917847
Epoch 1340, training loss: 311.61712646484375 = 0.2314063161611557 + 50.0 * 6.2277140617370605
Epoch 1340, val loss: 0.9804229736328125
Epoch 1350, training loss: 311.4988098144531 = 0.2263990342617035 + 50.0 * 6.225448131561279
Epoch 1350, val loss: 0.9832472205162048
Epoch 1360, training loss: 311.4053955078125 = 0.22152791917324066 + 50.0 * 6.223677635192871
Epoch 1360, val loss: 0.986521303653717
Epoch 1370, training loss: 311.362548828125 = 0.21682946383953094 + 50.0 * 6.222914218902588
Epoch 1370, val loss: 0.9900078773498535
Epoch 1380, training loss: 311.3520812988281 = 0.2122965157032013 + 50.0 * 6.222795486450195
Epoch 1380, val loss: 0.9936217069625854
Epoch 1390, training loss: 311.68255615234375 = 0.2078092247247696 + 50.0 * 6.229495048522949
Epoch 1390, val loss: 0.9970744848251343
Epoch 1400, training loss: 311.50567626953125 = 0.2033388763666153 + 50.0 * 6.226046562194824
Epoch 1400, val loss: 1.0000457763671875
Epoch 1410, training loss: 311.317138671875 = 0.19896818697452545 + 50.0 * 6.222363471984863
Epoch 1410, val loss: 1.0037896633148193
Epoch 1420, training loss: 311.3855895996094 = 0.19480621814727783 + 50.0 * 6.22381591796875
Epoch 1420, val loss: 1.0077357292175293
Epoch 1430, training loss: 311.2457275390625 = 0.19074685871601105 + 50.0 * 6.221099853515625
Epoch 1430, val loss: 1.0112780332565308
Epoch 1440, training loss: 311.23846435546875 = 0.18681220710277557 + 50.0 * 6.221033096313477
Epoch 1440, val loss: 1.0151861906051636
Epoch 1450, training loss: 311.38665771484375 = 0.1829732358455658 + 50.0 * 6.22407341003418
Epoch 1450, val loss: 1.0191034078598022
Epoch 1460, training loss: 311.367431640625 = 0.17910291254520416 + 50.0 * 6.223766326904297
Epoch 1460, val loss: 1.0228723287582397
Epoch 1470, training loss: 311.2174072265625 = 0.17533889412879944 + 50.0 * 6.220840930938721
Epoch 1470, val loss: 1.0266157388687134
Epoch 1480, training loss: 311.15576171875 = 0.17175185680389404 + 50.0 * 6.219680309295654
Epoch 1480, val loss: 1.0309444665908813
Epoch 1490, training loss: 311.12042236328125 = 0.1682712584733963 + 50.0 * 6.219043254852295
Epoch 1490, val loss: 1.0351921319961548
Epoch 1500, training loss: 311.3079833984375 = 0.16485849022865295 + 50.0 * 6.222862243652344
Epoch 1500, val loss: 1.0392663478851318
Epoch 1510, training loss: 311.1813659667969 = 0.16143055260181427 + 50.0 * 6.220398426055908
Epoch 1510, val loss: 1.0432578325271606
Epoch 1520, training loss: 311.1626281738281 = 0.15807685256004333 + 50.0 * 6.220090866088867
Epoch 1520, val loss: 1.0474311113357544
Epoch 1530, training loss: 311.09661865234375 = 0.1548803150653839 + 50.0 * 6.21883487701416
Epoch 1530, val loss: 1.051903486251831
Epoch 1540, training loss: 311.15057373046875 = 0.1517573744058609 + 50.0 * 6.21997594833374
Epoch 1540, val loss: 1.0561555624008179
Epoch 1550, training loss: 311.21954345703125 = 0.14867469668388367 + 50.0 * 6.22141695022583
Epoch 1550, val loss: 1.0604084730148315
Epoch 1560, training loss: 311.03387451171875 = 0.14556556940078735 + 50.0 * 6.217766284942627
Epoch 1560, val loss: 1.064591884613037
Epoch 1570, training loss: 310.98248291015625 = 0.1426496058702469 + 50.0 * 6.216796875
Epoch 1570, val loss: 1.0691869258880615
Epoch 1580, training loss: 311.1029357910156 = 0.1398196965456009 + 50.0 * 6.21926212310791
Epoch 1580, val loss: 1.0735238790512085
Epoch 1590, training loss: 311.0083312988281 = 0.13693740963935852 + 50.0 * 6.217427730560303
Epoch 1590, val loss: 1.0779249668121338
Epoch 1600, training loss: 310.9570007324219 = 0.13418743014335632 + 50.0 * 6.216456413269043
Epoch 1600, val loss: 1.0824000835418701
Epoch 1610, training loss: 311.0882263183594 = 0.13150067627429962 + 50.0 * 6.21913480758667
Epoch 1610, val loss: 1.0868390798568726
Epoch 1620, training loss: 310.9166564941406 = 0.12883339822292328 + 50.0 * 6.215755939483643
Epoch 1620, val loss: 1.0912928581237793
Epoch 1630, training loss: 310.88629150390625 = 0.1262788474559784 + 50.0 * 6.215200424194336
Epoch 1630, val loss: 1.0958759784698486
Epoch 1640, training loss: 310.9085388183594 = 0.12378640472888947 + 50.0 * 6.215694904327393
Epoch 1640, val loss: 1.1005502939224243
Epoch 1650, training loss: 311.1339111328125 = 0.12132242321968079 + 50.0 * 6.22025203704834
Epoch 1650, val loss: 1.104849934577942
Epoch 1660, training loss: 310.9916076660156 = 0.11880472302436829 + 50.0 * 6.217455863952637
Epoch 1660, val loss: 1.1091665029525757
Epoch 1670, training loss: 310.84063720703125 = 0.11643294990062714 + 50.0 * 6.214484214782715
Epoch 1670, val loss: 1.1140040159225464
Epoch 1680, training loss: 310.8263854980469 = 0.11414191871881485 + 50.0 * 6.214244842529297
Epoch 1680, val loss: 1.118593692779541
Epoch 1690, training loss: 311.1328125 = 0.11188661307096481 + 50.0 * 6.220418930053711
Epoch 1690, val loss: 1.1230379343032837
Epoch 1700, training loss: 310.8374328613281 = 0.10960064828395844 + 50.0 * 6.214556694030762
Epoch 1700, val loss: 1.1276081800460815
Epoch 1710, training loss: 310.7607727050781 = 0.10743885487318039 + 50.0 * 6.213066577911377
Epoch 1710, val loss: 1.1323047876358032
Epoch 1720, training loss: 310.804931640625 = 0.1053646057844162 + 50.0 * 6.213991165161133
Epoch 1720, val loss: 1.1370846033096313
Epoch 1730, training loss: 310.9278564453125 = 0.10327950865030289 + 50.0 * 6.21649169921875
Epoch 1730, val loss: 1.1416330337524414
Epoch 1740, training loss: 310.870849609375 = 0.1011950820684433 + 50.0 * 6.21539306640625
Epoch 1740, val loss: 1.1461924314498901
Epoch 1750, training loss: 310.76361083984375 = 0.09918604046106339 + 50.0 * 6.2132887840271
Epoch 1750, val loss: 1.1506882905960083
Epoch 1760, training loss: 310.7356262207031 = 0.09723900258541107 + 50.0 * 6.212768077850342
Epoch 1760, val loss: 1.1553384065628052
Epoch 1770, training loss: 310.74365234375 = 0.09535645693540573 + 50.0 * 6.212965488433838
Epoch 1770, val loss: 1.1600959300994873
Epoch 1780, training loss: 310.7256774902344 = 0.09350079298019409 + 50.0 * 6.212643146514893
Epoch 1780, val loss: 1.1647274494171143
Epoch 1790, training loss: 310.9664306640625 = 0.09168718010187149 + 50.0 * 6.217494964599609
Epoch 1790, val loss: 1.169575572013855
Epoch 1800, training loss: 310.7680969238281 = 0.08985549211502075 + 50.0 * 6.213565349578857
Epoch 1800, val loss: 1.1731270551681519
Epoch 1810, training loss: 310.63885498046875 = 0.08810370415449142 + 50.0 * 6.211015224456787
Epoch 1810, val loss: 1.1782019138336182
Epoch 1820, training loss: 310.625732421875 = 0.08642565459012985 + 50.0 * 6.210785865783691
Epoch 1820, val loss: 1.1829713582992554
Epoch 1830, training loss: 311.0429382324219 = 0.08478639274835587 + 50.0 * 6.219162940979004
Epoch 1830, val loss: 1.1871590614318848
Epoch 1840, training loss: 310.74176025390625 = 0.08310681581497192 + 50.0 * 6.2131733894348145
Epoch 1840, val loss: 1.192107915878296
Epoch 1850, training loss: 310.5966796875 = 0.08148281276226044 + 50.0 * 6.210304260253906
Epoch 1850, val loss: 1.196319341659546
Epoch 1860, training loss: 310.5614929199219 = 0.07996523380279541 + 50.0 * 6.209630966186523
Epoch 1860, val loss: 1.201135516166687
Epoch 1870, training loss: 310.77801513671875 = 0.07849234342575073 + 50.0 * 6.213990211486816
Epoch 1870, val loss: 1.2058806419372559
Epoch 1880, training loss: 310.63818359375 = 0.07692320644855499 + 50.0 * 6.2112250328063965
Epoch 1880, val loss: 1.209987759590149
Epoch 1890, training loss: 310.630859375 = 0.07545632123947144 + 50.0 * 6.2111077308654785
Epoch 1890, val loss: 1.2145448923110962
Epoch 1900, training loss: 310.5895080566406 = 0.07401115447282791 + 50.0 * 6.210309982299805
Epoch 1900, val loss: 1.2192630767822266
Epoch 1910, training loss: 310.5188903808594 = 0.0726507231593132 + 50.0 * 6.208924293518066
Epoch 1910, val loss: 1.2239446640014648
Epoch 1920, training loss: 310.56463623046875 = 0.07131804525852203 + 50.0 * 6.209866523742676
Epoch 1920, val loss: 1.2286475896835327
Epoch 1930, training loss: 310.6209411621094 = 0.06999091058969498 + 50.0 * 6.211019039154053
Epoch 1930, val loss: 1.2327890396118164
Epoch 1940, training loss: 310.5835876464844 = 0.06867555528879166 + 50.0 * 6.210298538208008
Epoch 1940, val loss: 1.2372599840164185
Epoch 1950, training loss: 310.6008605957031 = 0.06741320341825485 + 50.0 * 6.210669040679932
Epoch 1950, val loss: 1.241775393486023
Epoch 1960, training loss: 310.5444641113281 = 0.06615160405635834 + 50.0 * 6.209566116333008
Epoch 1960, val loss: 1.246218204498291
Epoch 1970, training loss: 310.5659484863281 = 0.0649397224187851 + 50.0 * 6.210020065307617
Epoch 1970, val loss: 1.2506576776504517
Epoch 1980, training loss: 310.57672119140625 = 0.06375032663345337 + 50.0 * 6.210259437561035
Epoch 1980, val loss: 1.2548961639404297
Epoch 1990, training loss: 310.44232177734375 = 0.06257802993059158 + 50.0 * 6.207594394683838
Epoch 1990, val loss: 1.259617805480957
Epoch 2000, training loss: 310.4104309082031 = 0.06146297976374626 + 50.0 * 6.206979274749756
Epoch 2000, val loss: 1.263843059539795
Epoch 2010, training loss: 310.42919921875 = 0.060392431914806366 + 50.0 * 6.207376003265381
Epoch 2010, val loss: 1.2682803869247437
Epoch 2020, training loss: 310.58404541015625 = 0.059326495975255966 + 50.0 * 6.210494518280029
Epoch 2020, val loss: 1.2725043296813965
Epoch 2030, training loss: 310.53759765625 = 0.05823386088013649 + 50.0 * 6.209587097167969
Epoch 2030, val loss: 1.2771916389465332
Epoch 2040, training loss: 310.4790344238281 = 0.057204023003578186 + 50.0 * 6.208436965942383
Epoch 2040, val loss: 1.280998706817627
Epoch 2050, training loss: 310.5498046875 = 0.05618574470281601 + 50.0 * 6.209872245788574
Epoch 2050, val loss: 1.2854682207107544
Epoch 2060, training loss: 310.5415344238281 = 0.05517100915312767 + 50.0 * 6.2097272872924805
Epoch 2060, val loss: 1.2893065214157104
Epoch 2070, training loss: 310.4072265625 = 0.05418655648827553 + 50.0 * 6.207060813903809
Epoch 2070, val loss: 1.2933021783828735
Epoch 2080, training loss: 310.40814208984375 = 0.053263697773218155 + 50.0 * 6.20709753036499
Epoch 2080, val loss: 1.2978070974349976
Epoch 2090, training loss: 310.4682312011719 = 0.05234423652291298 + 50.0 * 6.208317756652832
Epoch 2090, val loss: 1.3015556335449219
Epoch 2100, training loss: 310.38690185546875 = 0.051430974155664444 + 50.0 * 6.206709384918213
Epoch 2100, val loss: 1.3064534664154053
Epoch 2110, training loss: 310.4329833984375 = 0.05056380853056908 + 50.0 * 6.207648277282715
Epoch 2110, val loss: 1.3101383447647095
Epoch 2120, training loss: 310.3038635253906 = 0.04968781769275665 + 50.0 * 6.205083847045898
Epoch 2120, val loss: 1.3143755197525024
Epoch 2130, training loss: 310.3232421875 = 0.048852816224098206 + 50.0 * 6.2054877281188965
Epoch 2130, val loss: 1.318244457244873
Epoch 2140, training loss: 310.65020751953125 = 0.048026930540800095 + 50.0 * 6.212043762207031
Epoch 2140, val loss: 1.322188138961792
Epoch 2150, training loss: 310.3865661621094 = 0.04721144959330559 + 50.0 * 6.206787109375
Epoch 2150, val loss: 1.3262975215911865
Epoch 2160, training loss: 310.28167724609375 = 0.04640420526266098 + 50.0 * 6.204705238342285
Epoch 2160, val loss: 1.3301408290863037
Epoch 2170, training loss: 310.25445556640625 = 0.045667193830013275 + 50.0 * 6.20417594909668
Epoch 2170, val loss: 1.3343974351882935
Epoch 2180, training loss: 310.37713623046875 = 0.04493463784456253 + 50.0 * 6.206644535064697
Epoch 2180, val loss: 1.3381296396255493
Epoch 2190, training loss: 310.4190979003906 = 0.04416962340474129 + 50.0 * 6.207498550415039
Epoch 2190, val loss: 1.3422060012817383
Epoch 2200, training loss: 310.2848205566406 = 0.04340757802128792 + 50.0 * 6.204828262329102
Epoch 2200, val loss: 1.3450130224227905
Epoch 2210, training loss: 310.2398681640625 = 0.042712897062301636 + 50.0 * 6.203942775726318
Epoch 2210, val loss: 1.349870204925537
Epoch 2220, training loss: 310.2084655761719 = 0.04204435274004936 + 50.0 * 6.2033281326293945
Epoch 2220, val loss: 1.3538563251495361
Epoch 2230, training loss: 310.2791442871094 = 0.04140351340174675 + 50.0 * 6.204754829406738
Epoch 2230, val loss: 1.3579438924789429
Epoch 2240, training loss: 310.27496337890625 = 0.04073617607355118 + 50.0 * 6.204684734344482
Epoch 2240, val loss: 1.3615657091140747
Epoch 2250, training loss: 310.2690734863281 = 0.04008343815803528 + 50.0 * 6.204579830169678
Epoch 2250, val loss: 1.3653770685195923
Epoch 2260, training loss: 310.58538818359375 = 0.03946920856833458 + 50.0 * 6.210918426513672
Epoch 2260, val loss: 1.369381308555603
Epoch 2270, training loss: 310.3080139160156 = 0.03881207853555679 + 50.0 * 6.205383777618408
Epoch 2270, val loss: 1.372615098953247
Epoch 2280, training loss: 310.1871337890625 = 0.03820466250181198 + 50.0 * 6.202978610992432
Epoch 2280, val loss: 1.3764623403549194
Epoch 2290, training loss: 310.19537353515625 = 0.037630900740623474 + 50.0 * 6.203155040740967
Epoch 2290, val loss: 1.3806378841400146
Epoch 2300, training loss: 310.284912109375 = 0.037060387432575226 + 50.0 * 6.204957008361816
Epoch 2300, val loss: 1.384215235710144
Epoch 2310, training loss: 310.1748962402344 = 0.036486297845840454 + 50.0 * 6.202767848968506
Epoch 2310, val loss: 1.3876128196716309
Epoch 2320, training loss: 310.2023620605469 = 0.035945549607276917 + 50.0 * 6.2033281326293945
Epoch 2320, val loss: 1.3915435075759888
Epoch 2330, training loss: 310.2646179199219 = 0.03538971021771431 + 50.0 * 6.204584121704102
Epoch 2330, val loss: 1.3952631950378418
Epoch 2340, training loss: 310.1734313964844 = 0.034840986132621765 + 50.0 * 6.2027716636657715
Epoch 2340, val loss: 1.398244023323059
Epoch 2350, training loss: 310.1114501953125 = 0.03432425484061241 + 50.0 * 6.201542377471924
Epoch 2350, val loss: 1.4024418592453003
Epoch 2360, training loss: 310.10223388671875 = 0.03382951393723488 + 50.0 * 6.2013678550720215
Epoch 2360, val loss: 1.4060810804367065
Epoch 2370, training loss: 310.39898681640625 = 0.03334132954478264 + 50.0 * 6.20731258392334
Epoch 2370, val loss: 1.4094902276992798
Epoch 2380, training loss: 310.14202880859375 = 0.032834600657224655 + 50.0 * 6.202183723449707
Epoch 2380, val loss: 1.4125832319259644
Epoch 2390, training loss: 310.1499938964844 = 0.032352298498153687 + 50.0 * 6.202353000640869
Epoch 2390, val loss: 1.4161189794540405
Epoch 2400, training loss: 310.0913391113281 = 0.03188372030854225 + 50.0 * 6.201189041137695
Epoch 2400, val loss: 1.4198474884033203
Epoch 2410, training loss: 310.0574035644531 = 0.031432345509529114 + 50.0 * 6.200519561767578
Epoch 2410, val loss: 1.4234074354171753
Epoch 2420, training loss: 310.2557067871094 = 0.030995970591902733 + 50.0 * 6.204493999481201
Epoch 2420, val loss: 1.4267514944076538
Epoch 2430, training loss: 310.1411437988281 = 0.03054187260568142 + 50.0 * 6.202211856842041
Epoch 2430, val loss: 1.4295263290405273
Epoch 2440, training loss: 310.0811767578125 = 0.030093371868133545 + 50.0 * 6.201021671295166
Epoch 2440, val loss: 1.4331276416778564
Epoch 2450, training loss: 310.0855712890625 = 0.02967195212841034 + 50.0 * 6.201118469238281
Epoch 2450, val loss: 1.4366092681884766
Epoch 2460, training loss: 310.0182800292969 = 0.02925972267985344 + 50.0 * 6.199779987335205
Epoch 2460, val loss: 1.4401347637176514
Epoch 2470, training loss: 310.1401062011719 = 0.028872566297650337 + 50.0 * 6.2022247314453125
Epoch 2470, val loss: 1.4433493614196777
Epoch 2480, training loss: 310.06915283203125 = 0.028457297012209892 + 50.0 * 6.200814247131348
Epoch 2480, val loss: 1.4467476606369019
Epoch 2490, training loss: 310.0640563964844 = 0.028060751035809517 + 50.0 * 6.200720310211182
Epoch 2490, val loss: 1.4495995044708252
Epoch 2500, training loss: 310.18170166015625 = 0.027681250125169754 + 50.0 * 6.203080177307129
Epoch 2500, val loss: 1.4522106647491455
Epoch 2510, training loss: 310.0433349609375 = 0.027297817170619965 + 50.0 * 6.200321197509766
Epoch 2510, val loss: 1.4566230773925781
Epoch 2520, training loss: 310.0721740722656 = 0.026939239352941513 + 50.0 * 6.200904846191406
Epoch 2520, val loss: 1.4593158960342407
Epoch 2530, training loss: 310.1606750488281 = 0.026570599526166916 + 50.0 * 6.202682018280029
Epoch 2530, val loss: 1.4626775979995728
Epoch 2540, training loss: 310.1309814453125 = 0.026218298822641373 + 50.0 * 6.2020955085754395
Epoch 2540, val loss: 1.4656602144241333
Epoch 2550, training loss: 310.03546142578125 = 0.025854239240288734 + 50.0 * 6.200192451477051
Epoch 2550, val loss: 1.468955397605896
Epoch 2560, training loss: 310.04998779296875 = 0.02551349252462387 + 50.0 * 6.200489521026611
Epoch 2560, val loss: 1.471604585647583
Epoch 2570, training loss: 309.94854736328125 = 0.02517538145184517 + 50.0 * 6.198467254638672
Epoch 2570, val loss: 1.4752370119094849
Epoch 2580, training loss: 309.9569091796875 = 0.024857094511389732 + 50.0 * 6.198640823364258
Epoch 2580, val loss: 1.4782507419586182
Epoch 2590, training loss: 310.13092041015625 = 0.024544481188058853 + 50.0 * 6.202127933502197
Epoch 2590, val loss: 1.4816161394119263
Epoch 2600, training loss: 310.0478820800781 = 0.024221021682024002 + 50.0 * 6.200472831726074
Epoch 2600, val loss: 1.4840842485427856
Epoch 2610, training loss: 309.9237365722656 = 0.023895544931292534 + 50.0 * 6.197996616363525
Epoch 2610, val loss: 1.487141489982605
Epoch 2620, training loss: 309.9392395019531 = 0.023597318679094315 + 50.0 * 6.198313236236572
Epoch 2620, val loss: 1.490439534187317
Epoch 2630, training loss: 310.20465087890625 = 0.023306196555495262 + 50.0 * 6.20362663269043
Epoch 2630, val loss: 1.493789792060852
Epoch 2640, training loss: 310.0299072265625 = 0.022998405620455742 + 50.0 * 6.200138092041016
Epoch 2640, val loss: 1.4956064224243164
Epoch 2650, training loss: 309.92144775390625 = 0.02269916981458664 + 50.0 * 6.197975158691406
Epoch 2650, val loss: 1.4993114471435547
Epoch 2660, training loss: 309.8888854980469 = 0.02243112586438656 + 50.0 * 6.197329044342041
Epoch 2660, val loss: 1.5021334886550903
Epoch 2670, training loss: 309.9334411621094 = 0.022166382521390915 + 50.0 * 6.198225975036621
Epoch 2670, val loss: 1.505629301071167
Epoch 2680, training loss: 310.0522766113281 = 0.02189469151198864 + 50.0 * 6.200607776641846
Epoch 2680, val loss: 1.5080797672271729
Epoch 2690, training loss: 310.0071105957031 = 0.02160947397351265 + 50.0 * 6.199709892272949
Epoch 2690, val loss: 1.5102903842926025
Epoch 2700, training loss: 310.0640563964844 = 0.021346230059862137 + 50.0 * 6.200854301452637
Epoch 2700, val loss: 1.5135756731033325
Epoch 2710, training loss: 309.8861389160156 = 0.021077940240502357 + 50.0 * 6.197300910949707
Epoch 2710, val loss: 1.516426682472229
Epoch 2720, training loss: 309.8407897949219 = 0.02083190344274044 + 50.0 * 6.196399211883545
Epoch 2720, val loss: 1.5194834470748901
Epoch 2730, training loss: 310.0082092285156 = 0.020597204566001892 + 50.0 * 6.199752330780029
Epoch 2730, val loss: 1.5221284627914429
Epoch 2740, training loss: 309.8639221191406 = 0.02033589407801628 + 50.0 * 6.196871757507324
Epoch 2740, val loss: 1.524903655052185
Epoch 2750, training loss: 309.8333740234375 = 0.02008538320660591 + 50.0 * 6.196265697479248
Epoch 2750, val loss: 1.52744722366333
Epoch 2760, training loss: 309.8224182128906 = 0.019860876724123955 + 50.0 * 6.196051120758057
Epoch 2760, val loss: 1.5307644605636597
Epoch 2770, training loss: 309.86578369140625 = 0.019639624282717705 + 50.0 * 6.196922779083252
Epoch 2770, val loss: 1.533412218093872
Epoch 2780, training loss: 310.0421142578125 = 0.01941530965268612 + 50.0 * 6.200454235076904
Epoch 2780, val loss: 1.5358792543411255
Epoch 2790, training loss: 310.00555419921875 = 0.01917932741343975 + 50.0 * 6.199728012084961
Epoch 2790, val loss: 1.5386087894439697
Epoch 2800, training loss: 309.8706359863281 = 0.018952207639813423 + 50.0 * 6.197033405303955
Epoch 2800, val loss: 1.5415372848510742
Epoch 2810, training loss: 309.8089904785156 = 0.01873824931681156 + 50.0 * 6.195805072784424
Epoch 2810, val loss: 1.5442194938659668
Epoch 2820, training loss: 309.8740234375 = 0.018536334857344627 + 50.0 * 6.197109699249268
Epoch 2820, val loss: 1.5473480224609375
Epoch 2830, training loss: 309.85284423828125 = 0.018325842916965485 + 50.0 * 6.196690082550049
Epoch 2830, val loss: 1.5500956773757935
Epoch 2840, training loss: 309.8124694824219 = 0.018120525404810905 + 50.0 * 6.195886611938477
Epoch 2840, val loss: 1.5528981685638428
Epoch 2850, training loss: 309.8925476074219 = 0.01792173460125923 + 50.0 * 6.197492599487305
Epoch 2850, val loss: 1.555757761001587
Epoch 2860, training loss: 309.8547058105469 = 0.017725110054016113 + 50.0 * 6.196739196777344
Epoch 2860, val loss: 1.5577884912490845
Epoch 2870, training loss: 310.02679443359375 = 0.017523888498544693 + 50.0 * 6.200185775756836
Epoch 2870, val loss: 1.5604746341705322
Epoch 2880, training loss: 309.8101501464844 = 0.017322273924946785 + 50.0 * 6.195856094360352
Epoch 2880, val loss: 1.5629637241363525
Epoch 2890, training loss: 309.75238037109375 = 0.017135266214609146 + 50.0 * 6.194705009460449
Epoch 2890, val loss: 1.565573811531067
Epoch 2900, training loss: 309.7458801269531 = 0.01696002669632435 + 50.0 * 6.194578170776367
Epoch 2900, val loss: 1.5687798261642456
Epoch 2910, training loss: 309.8141784667969 = 0.016791004687547684 + 50.0 * 6.195947647094727
Epoch 2910, val loss: 1.5715854167938232
Epoch 2920, training loss: 309.9109802246094 = 0.016611753031611443 + 50.0 * 6.197887420654297
Epoch 2920, val loss: 1.5736669301986694
Epoch 2930, training loss: 309.8204650878906 = 0.01642153225839138 + 50.0 * 6.196081161499023
Epoch 2930, val loss: 1.5755771398544312
Epoch 2940, training loss: 309.9577941894531 = 0.016252119094133377 + 50.0 * 6.198830604553223
Epoch 2940, val loss: 1.5786261558532715
Epoch 2950, training loss: 309.7908630371094 = 0.01607021875679493 + 50.0 * 6.19549560546875
Epoch 2950, val loss: 1.580930471420288
Epoch 2960, training loss: 309.7555236816406 = 0.015902869403362274 + 50.0 * 6.1947922706604
Epoch 2960, val loss: 1.5836114883422852
Epoch 2970, training loss: 309.7193298339844 = 0.01573934778571129 + 50.0 * 6.1940717697143555
Epoch 2970, val loss: 1.586169958114624
Epoch 2980, training loss: 309.71392822265625 = 0.01558815035969019 + 50.0 * 6.193966388702393
Epoch 2980, val loss: 1.5889592170715332
Epoch 2990, training loss: 309.8567810058594 = 0.015437198802828789 + 50.0 * 6.196826934814453
Epoch 2990, val loss: 1.591328740119934
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8392198207696363
The final CL Acc:0.73704, 0.02978, The final GNN Acc:0.84010, 0.00163
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.3 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.3, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11642])
remove edge: torch.Size([2, 9562])
updated graph: torch.Size([2, 10648])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.79632568359375 = 1.9528335332870483 + 50.0 * 8.596870422363281
Epoch 0, val loss: 1.9465430974960327
Epoch 10, training loss: 431.76226806640625 = 1.943264126777649 + 50.0 * 8.596380233764648
Epoch 10, val loss: 1.9373347759246826
Epoch 20, training loss: 431.57061767578125 = 1.931668996810913 + 50.0 * 8.592779159545898
Epoch 20, val loss: 1.925744652748108
Epoch 30, training loss: 430.3104553222656 = 1.9170756340026855 + 50.0 * 8.567867279052734
Epoch 30, val loss: 1.910811185836792
Epoch 40, training loss: 423.3389892578125 = 1.8986753225326538 + 50.0 * 8.42880630493164
Epoch 40, val loss: 1.8924058675765991
Epoch 50, training loss: 401.2666931152344 = 1.879186987876892 + 50.0 * 7.987750053405762
Epoch 50, val loss: 1.873265027999878
Epoch 60, training loss: 383.45367431640625 = 1.8631244897842407 + 50.0 * 7.631811141967773
Epoch 60, val loss: 1.858701467514038
Epoch 70, training loss: 369.0638427734375 = 1.8501728773117065 + 50.0 * 7.344273567199707
Epoch 70, val loss: 1.8465722799301147
Epoch 80, training loss: 357.3072814941406 = 1.8380632400512695 + 50.0 * 7.109384059906006
Epoch 80, val loss: 1.8353219032287598
Epoch 90, training loss: 348.4974670410156 = 1.8290538787841797 + 50.0 * 6.933368682861328
Epoch 90, val loss: 1.826879858970642
Epoch 100, training loss: 343.73907470703125 = 1.8190035820007324 + 50.0 * 6.8384013175964355
Epoch 100, val loss: 1.8174840211868286
Epoch 110, training loss: 340.0721435546875 = 1.8084242343902588 + 50.0 * 6.765274524688721
Epoch 110, val loss: 1.8075546026229858
Epoch 120, training loss: 337.1865539550781 = 1.7985215187072754 + 50.0 * 6.707760810852051
Epoch 120, val loss: 1.798112154006958
Epoch 130, training loss: 334.786376953125 = 1.7891870737075806 + 50.0 * 6.659943580627441
Epoch 130, val loss: 1.7890431880950928
Epoch 140, training loss: 332.9495849609375 = 1.7794995307922363 + 50.0 * 6.623401641845703
Epoch 140, val loss: 1.779677152633667
Epoch 150, training loss: 331.3254699707031 = 1.7690976858139038 + 50.0 * 6.591127395629883
Epoch 150, val loss: 1.769663691520691
Epoch 160, training loss: 330.1693115234375 = 1.7579920291900635 + 50.0 * 6.568226337432861
Epoch 160, val loss: 1.7588995695114136
Epoch 170, training loss: 329.05230712890625 = 1.7458604574203491 + 50.0 * 6.54612922668457
Epoch 170, val loss: 1.7473570108413696
Epoch 180, training loss: 328.2019348144531 = 1.7328567504882812 + 50.0 * 6.52938175201416
Epoch 180, val loss: 1.7350289821624756
Epoch 190, training loss: 327.529296875 = 1.7188928127288818 + 50.0 * 6.516207695007324
Epoch 190, val loss: 1.7217952013015747
Epoch 200, training loss: 326.7362365722656 = 1.7037067413330078 + 50.0 * 6.500650405883789
Epoch 200, val loss: 1.7075576782226562
Epoch 210, training loss: 326.0432434082031 = 1.687444806098938 + 50.0 * 6.48711633682251
Epoch 210, val loss: 1.6923452615737915
Epoch 220, training loss: 325.81451416015625 = 1.6699936389923096 + 50.0 * 6.482890605926514
Epoch 220, val loss: 1.6761748790740967
Epoch 230, training loss: 324.9096984863281 = 1.6514461040496826 + 50.0 * 6.465164661407471
Epoch 230, val loss: 1.658835530281067
Epoch 240, training loss: 324.4146728515625 = 1.6318327188491821 + 50.0 * 6.4556565284729
Epoch 240, val loss: 1.640638828277588
Epoch 250, training loss: 323.9200439453125 = 1.611206293106079 + 50.0 * 6.446177005767822
Epoch 250, val loss: 1.6216130256652832
Epoch 260, training loss: 323.57171630859375 = 1.5896310806274414 + 50.0 * 6.439641952514648
Epoch 260, val loss: 1.6017652750015259
Epoch 270, training loss: 323.1234436035156 = 1.567279577255249 + 50.0 * 6.43112325668335
Epoch 270, val loss: 1.581203818321228
Epoch 280, training loss: 322.8609313964844 = 1.544350028038025 + 50.0 * 6.426331996917725
Epoch 280, val loss: 1.5601317882537842
Epoch 290, training loss: 322.4751281738281 = 1.5207124948501587 + 50.0 * 6.419087886810303
Epoch 290, val loss: 1.538756251335144
Epoch 300, training loss: 321.9973449707031 = 1.4968819618225098 + 50.0 * 6.410008907318115
Epoch 300, val loss: 1.5172667503356934
Epoch 310, training loss: 321.6767883300781 = 1.472832202911377 + 50.0 * 6.404078960418701
Epoch 310, val loss: 1.4958983659744263
Epoch 320, training loss: 321.5277404785156 = 1.4486627578735352 + 50.0 * 6.401581287384033
Epoch 320, val loss: 1.4746806621551514
Epoch 330, training loss: 321.1412658691406 = 1.4243990182876587 + 50.0 * 6.394337177276611
Epoch 330, val loss: 1.453523874282837
Epoch 340, training loss: 320.8403015136719 = 1.40012526512146 + 50.0 * 6.388803958892822
Epoch 340, val loss: 1.4327441453933716
Epoch 350, training loss: 320.5600280761719 = 1.3759127855300903 + 50.0 * 6.3836822509765625
Epoch 350, val loss: 1.412373423576355
Epoch 360, training loss: 320.62677001953125 = 1.351697564125061 + 50.0 * 6.385501861572266
Epoch 360, val loss: 1.3923535346984863
Epoch 370, training loss: 320.0889587402344 = 1.327588438987732 + 50.0 * 6.375227928161621
Epoch 370, val loss: 1.372485637664795
Epoch 380, training loss: 319.923095703125 = 1.3035027980804443 + 50.0 * 6.372391700744629
Epoch 380, val loss: 1.3530293703079224
Epoch 390, training loss: 319.66070556640625 = 1.2794075012207031 + 50.0 * 6.367625713348389
Epoch 390, val loss: 1.33393394947052
Epoch 400, training loss: 319.5874328613281 = 1.2552374601364136 + 50.0 * 6.36664342880249
Epoch 400, val loss: 1.3149324655532837
Epoch 410, training loss: 319.3214111328125 = 1.2311956882476807 + 50.0 * 6.361804008483887
Epoch 410, val loss: 1.2961773872375488
Epoch 420, training loss: 319.0369873046875 = 1.2071939706802368 + 50.0 * 6.356595993041992
Epoch 420, val loss: 1.2779395580291748
Epoch 430, training loss: 318.8995666503906 = 1.1833330392837524 + 50.0 * 6.354324817657471
Epoch 430, val loss: 1.2600233554840088
Epoch 440, training loss: 318.84869384765625 = 1.1595728397369385 + 50.0 * 6.3537821769714355
Epoch 440, val loss: 1.2428576946258545
Epoch 450, training loss: 318.5459289550781 = 1.13600754737854 + 50.0 * 6.348198413848877
Epoch 450, val loss: 1.2255295515060425
Epoch 460, training loss: 318.36328125 = 1.1128108501434326 + 50.0 * 6.3450093269348145
Epoch 460, val loss: 1.2089673280715942
Epoch 470, training loss: 318.30682373046875 = 1.0901066064834595 + 50.0 * 6.344334125518799
Epoch 470, val loss: 1.1927968263626099
Epoch 480, training loss: 318.1856994628906 = 1.067798376083374 + 50.0 * 6.342357635498047
Epoch 480, val loss: 1.1777921915054321
Epoch 490, training loss: 317.9493408203125 = 1.046007513999939 + 50.0 * 6.338066577911377
Epoch 490, val loss: 1.1629486083984375
Epoch 500, training loss: 317.79962158203125 = 1.0249595642089844 + 50.0 * 6.335493087768555
Epoch 500, val loss: 1.149106502532959
Epoch 510, training loss: 317.80560302734375 = 1.0045831203460693 + 50.0 * 6.336020469665527
Epoch 510, val loss: 1.1360290050506592
Epoch 520, training loss: 317.5914001464844 = 0.984691321849823 + 50.0 * 6.332134246826172
Epoch 520, val loss: 1.1237107515335083
Epoch 530, training loss: 317.434326171875 = 0.9655588269233704 + 50.0 * 6.329375743865967
Epoch 530, val loss: 1.1119089126586914
Epoch 540, training loss: 317.2735595703125 = 0.9471436738967896 + 50.0 * 6.326528549194336
Epoch 540, val loss: 1.1009513139724731
Epoch 550, training loss: 317.2822265625 = 0.9293661713600159 + 50.0 * 6.327057361602783
Epoch 550, val loss: 1.0908914804458618
Epoch 560, training loss: 317.2551574707031 = 0.9120792746543884 + 50.0 * 6.326861381530762
Epoch 560, val loss: 1.0810742378234863
Epoch 570, training loss: 316.9719543457031 = 0.8953294157981873 + 50.0 * 6.321532726287842
Epoch 570, val loss: 1.0719877481460571
Epoch 580, training loss: 316.89599609375 = 0.8790763020515442 + 50.0 * 6.320338249206543
Epoch 580, val loss: 1.0633143186569214
Epoch 590, training loss: 316.7369689941406 = 0.8633129000663757 + 50.0 * 6.317473411560059
Epoch 590, val loss: 1.054865837097168
Epoch 600, training loss: 316.724853515625 = 0.8479763269424438 + 50.0 * 6.317537784576416
Epoch 600, val loss: 1.0470216274261475
Epoch 610, training loss: 316.6906433105469 = 0.8328210711479187 + 50.0 * 6.3171563148498535
Epoch 610, val loss: 1.0394471883773804
Epoch 620, training loss: 316.47052001953125 = 0.8179222941398621 + 50.0 * 6.313051700592041
Epoch 620, val loss: 1.0321205854415894
Epoch 630, training loss: 316.3498229980469 = 0.8033486604690552 + 50.0 * 6.310929775238037
Epoch 630, val loss: 1.0250359773635864
Epoch 640, training loss: 316.4062805175781 = 0.7890579700469971 + 50.0 * 6.312344074249268
Epoch 640, val loss: 1.0184664726257324
Epoch 650, training loss: 316.30389404296875 = 0.7746837735176086 + 50.0 * 6.31058406829834
Epoch 650, val loss: 1.0115022659301758
Epoch 660, training loss: 316.1091613769531 = 0.7606213688850403 + 50.0 * 6.306970596313477
Epoch 660, val loss: 1.0049787759780884
Epoch 670, training loss: 316.2986145019531 = 0.7467551231384277 + 50.0 * 6.311037063598633
Epoch 670, val loss: 0.9991480112075806
Epoch 680, training loss: 316.0834655761719 = 0.7328461408615112 + 50.0 * 6.30701208114624
Epoch 680, val loss: 0.9930258393287659
Epoch 690, training loss: 315.8528137207031 = 0.7190464735031128 + 50.0 * 6.302675247192383
Epoch 690, val loss: 0.9870416522026062
Epoch 700, training loss: 315.7725830078125 = 0.7054358720779419 + 50.0 * 6.301342487335205
Epoch 700, val loss: 0.981499433517456
Epoch 710, training loss: 315.79888916015625 = 0.6919213533401489 + 50.0 * 6.3021392822265625
Epoch 710, val loss: 0.9761393070220947
Epoch 720, training loss: 315.8888244628906 = 0.6783884167671204 + 50.0 * 6.304209232330322
Epoch 720, val loss: 0.9708142280578613
Epoch 730, training loss: 315.6441955566406 = 0.6649855375289917 + 50.0 * 6.29958438873291
Epoch 730, val loss: 0.9654865264892578
Epoch 740, training loss: 315.48333740234375 = 0.6517181396484375 + 50.0 * 6.296632766723633
Epoch 740, val loss: 0.960846483707428
Epoch 750, training loss: 315.4687805175781 = 0.6386282444000244 + 50.0 * 6.296602725982666
Epoch 750, val loss: 0.9562761187553406
Epoch 760, training loss: 315.38446044921875 = 0.6255377531051636 + 50.0 * 6.295178413391113
Epoch 760, val loss: 0.951529860496521
Epoch 770, training loss: 315.3329162597656 = 0.6125780940055847 + 50.0 * 6.294406890869141
Epoch 770, val loss: 0.9476720690727234
Epoch 780, training loss: 315.3464050292969 = 0.5997330546379089 + 50.0 * 6.294933319091797
Epoch 780, val loss: 0.9436845183372498
Epoch 790, training loss: 315.2322692871094 = 0.5870440602302551 + 50.0 * 6.292904376983643
Epoch 790, val loss: 0.9399023056030273
Epoch 800, training loss: 315.1429748535156 = 0.5744838714599609 + 50.0 * 6.291369438171387
Epoch 800, val loss: 0.9363850355148315
Epoch 810, training loss: 315.2574157714844 = 0.5620543956756592 + 50.0 * 6.293907642364502
Epoch 810, val loss: 0.933424174785614
Epoch 820, training loss: 315.03570556640625 = 0.5497623085975647 + 50.0 * 6.2897186279296875
Epoch 820, val loss: 0.9305587410926819
Epoch 830, training loss: 314.91204833984375 = 0.5376271605491638 + 50.0 * 6.2874884605407715
Epoch 830, val loss: 0.9278473854064941
Epoch 840, training loss: 314.9210205078125 = 0.5256802439689636 + 50.0 * 6.287907123565674
Epoch 840, val loss: 0.9256705045700073
Epoch 850, training loss: 314.8797607421875 = 0.5138092041015625 + 50.0 * 6.287319183349609
Epoch 850, val loss: 0.9235296249389648
Epoch 860, training loss: 314.7987976074219 = 0.5021210312843323 + 50.0 * 6.285933494567871
Epoch 860, val loss: 0.9216611981391907
Epoch 870, training loss: 314.7850341796875 = 0.4905969202518463 + 50.0 * 6.285888671875
Epoch 870, val loss: 0.9198063015937805
Epoch 880, training loss: 314.6295166015625 = 0.47929245233535767 + 50.0 * 6.2830047607421875
Epoch 880, val loss: 0.918659508228302
Epoch 890, training loss: 314.5961608886719 = 0.46815305948257446 + 50.0 * 6.282560348510742
Epoch 890, val loss: 0.9176797270774841
Epoch 900, training loss: 314.778076171875 = 0.45715975761413574 + 50.0 * 6.286418437957764
Epoch 900, val loss: 0.9167307615280151
Epoch 910, training loss: 314.53070068359375 = 0.44628092646598816 + 50.0 * 6.281688213348389
Epoch 910, val loss: 0.9153038263320923
Epoch 920, training loss: 314.4311828613281 = 0.4356521964073181 + 50.0 * 6.279911041259766
Epoch 920, val loss: 0.9152778387069702
Epoch 930, training loss: 314.3930358886719 = 0.4251849949359894 + 50.0 * 6.279357433319092
Epoch 930, val loss: 0.9145055413246155
Epoch 940, training loss: 314.5888977050781 = 0.41495245695114136 + 50.0 * 6.283478736877441
Epoch 940, val loss: 0.9149706959724426
Epoch 950, training loss: 314.3808288574219 = 0.40480169653892517 + 50.0 * 6.279520511627197
Epoch 950, val loss: 0.9144048094749451
Epoch 960, training loss: 314.63665771484375 = 0.3949194848537445 + 50.0 * 6.284834384918213
Epoch 960, val loss: 0.9153087735176086
Epoch 970, training loss: 314.2758483886719 = 0.3851649761199951 + 50.0 * 6.277813911437988
Epoch 970, val loss: 0.915192186832428
Epoch 980, training loss: 314.1273193359375 = 0.37565526366233826 + 50.0 * 6.275033473968506
Epoch 980, val loss: 0.9157509803771973
Epoch 990, training loss: 314.1153869628906 = 0.3663921356201172 + 50.0 * 6.274979591369629
Epoch 990, val loss: 0.9166460633277893
Epoch 1000, training loss: 314.1913146972656 = 0.3572326898574829 + 50.0 * 6.276681423187256
Epoch 1000, val loss: 0.9173998832702637
Epoch 1010, training loss: 314.0734558105469 = 0.34830838441848755 + 50.0 * 6.274503231048584
Epoch 1010, val loss: 0.9181164503097534
Epoch 1020, training loss: 313.94293212890625 = 0.3396340310573578 + 50.0 * 6.272066116333008
Epoch 1020, val loss: 0.9194751977920532
Epoch 1030, training loss: 313.8794250488281 = 0.3311818540096283 + 50.0 * 6.270965099334717
Epoch 1030, val loss: 0.9205881953239441
Epoch 1040, training loss: 313.9913635253906 = 0.32293030619621277 + 50.0 * 6.2733683586120605
Epoch 1040, val loss: 0.9215869903564453
Epoch 1050, training loss: 314.0334777832031 = 0.3147878646850586 + 50.0 * 6.274373531341553
Epoch 1050, val loss: 0.9235758185386658
Epoch 1060, training loss: 313.8384704589844 = 0.3068346679210663 + 50.0 * 6.270633220672607
Epoch 1060, val loss: 0.925061047077179
Epoch 1070, training loss: 313.814208984375 = 0.29914596676826477 + 50.0 * 6.270301342010498
Epoch 1070, val loss: 0.926320493221283
Epoch 1080, training loss: 313.9024658203125 = 0.29169872403144836 + 50.0 * 6.272215366363525
Epoch 1080, val loss: 0.9283326864242554
Epoch 1090, training loss: 313.84783935546875 = 0.28437650203704834 + 50.0 * 6.27126932144165
Epoch 1090, val loss: 0.9307027459144592
Epoch 1100, training loss: 313.6661376953125 = 0.27723899483680725 + 50.0 * 6.267777919769287
Epoch 1100, val loss: 0.9328051805496216
Epoch 1110, training loss: 313.5914001464844 = 0.2703240215778351 + 50.0 * 6.266421794891357
Epoch 1110, val loss: 0.9353970289230347
Epoch 1120, training loss: 313.76580810546875 = 0.26361268758773804 + 50.0 * 6.270043849945068
Epoch 1120, val loss: 0.9374331831932068
Epoch 1130, training loss: 313.79010009765625 = 0.2570064067840576 + 50.0 * 6.2706618309021
Epoch 1130, val loss: 0.9406622052192688
Epoch 1140, training loss: 313.5579528808594 = 0.25053277611732483 + 50.0 * 6.266148567199707
Epoch 1140, val loss: 0.9429309964179993
Epoch 1150, training loss: 313.41534423828125 = 0.24432632327079773 + 50.0 * 6.263420581817627
Epoch 1150, val loss: 0.9458615183830261
Epoch 1160, training loss: 313.3992919921875 = 0.23830604553222656 + 50.0 * 6.263219356536865
Epoch 1160, val loss: 0.9491350054740906
Epoch 1170, training loss: 313.6966552734375 = 0.23244865238666534 + 50.0 * 6.269284248352051
Epoch 1170, val loss: 0.9521252512931824
Epoch 1180, training loss: 313.5039367675781 = 0.2266642153263092 + 50.0 * 6.26554536819458
Epoch 1180, val loss: 0.9547587037086487
Epoch 1190, training loss: 313.4517517089844 = 0.22105993330478668 + 50.0 * 6.264613628387451
Epoch 1190, val loss: 0.9582653045654297
Epoch 1200, training loss: 313.349365234375 = 0.21564173698425293 + 50.0 * 6.262674808502197
Epoch 1200, val loss: 0.9619676470756531
Epoch 1210, training loss: 313.3204650878906 = 0.21037524938583374 + 50.0 * 6.262201309204102
Epoch 1210, val loss: 0.9652711153030396
Epoch 1220, training loss: 313.3741760253906 = 0.20523805916309357 + 50.0 * 6.263379096984863
Epoch 1220, val loss: 0.9684866666793823
Epoch 1230, training loss: 313.2447204589844 = 0.20020923018455505 + 50.0 * 6.260890483856201
Epoch 1230, val loss: 0.9725183844566345
Epoch 1240, training loss: 313.1678466796875 = 0.19536328315734863 + 50.0 * 6.259449481964111
Epoch 1240, val loss: 0.9764361381530762
Epoch 1250, training loss: 313.3887939453125 = 0.19063535332679749 + 50.0 * 6.263963222503662
Epoch 1250, val loss: 0.9799302816390991
Epoch 1260, training loss: 313.0903015136719 = 0.18596704304218292 + 50.0 * 6.258086681365967
Epoch 1260, val loss: 0.9837145805358887
Epoch 1270, training loss: 313.0379333496094 = 0.18149732053279877 + 50.0 * 6.257128715515137
Epoch 1270, val loss: 0.9876388907432556
Epoch 1280, training loss: 313.30419921875 = 0.17715850472450256 + 50.0 * 6.262540817260742
Epoch 1280, val loss: 0.9909107089042664
Epoch 1290, training loss: 313.51849365234375 = 0.17289796471595764 + 50.0 * 6.266911506652832
Epoch 1290, val loss: 0.9963396787643433
Epoch 1300, training loss: 313.02294921875 = 0.1686779409646988 + 50.0 * 6.25708532333374
Epoch 1300, val loss: 1.000058889389038
Epoch 1310, training loss: 312.9547119140625 = 0.16463559865951538 + 50.0 * 6.255801677703857
Epoch 1310, val loss: 1.003563404083252
Epoch 1320, training loss: 312.88995361328125 = 0.16073913872241974 + 50.0 * 6.254584312438965
Epoch 1320, val loss: 1.0082981586456299
Epoch 1330, training loss: 312.9075622558594 = 0.15693902969360352 + 50.0 * 6.255012512207031
Epoch 1330, val loss: 1.012434482574463
Epoch 1340, training loss: 313.0660705566406 = 0.15319876372814178 + 50.0 * 6.2582573890686035
Epoch 1340, val loss: 1.0162683725357056
Epoch 1350, training loss: 312.9162902832031 = 0.1495068520307541 + 50.0 * 6.255335807800293
Epoch 1350, val loss: 1.020707130432129
Epoch 1360, training loss: 312.9386901855469 = 0.14593729376792908 + 50.0 * 6.255855083465576
Epoch 1360, val loss: 1.0259310007095337
Epoch 1370, training loss: 312.7620544433594 = 0.1424853801727295 + 50.0 * 6.252391338348389
Epoch 1370, val loss: 1.030381679534912
Epoch 1380, training loss: 312.82928466796875 = 0.13914595544338226 + 50.0 * 6.253802299499512
Epoch 1380, val loss: 1.0350055694580078
Epoch 1390, training loss: 312.87896728515625 = 0.13585497438907623 + 50.0 * 6.254861831665039
Epoch 1390, val loss: 1.0391411781311035
Epoch 1400, training loss: 312.85687255859375 = 0.13262371718883514 + 50.0 * 6.2544846534729
Epoch 1400, val loss: 1.042864441871643
Epoch 1410, training loss: 312.7753601074219 = 0.12949003279209137 + 50.0 * 6.252917289733887
Epoch 1410, val loss: 1.0488779544830322
Epoch 1420, training loss: 312.7752380371094 = 0.12643086910247803 + 50.0 * 6.252975940704346
Epoch 1420, val loss: 1.0527372360229492
Epoch 1430, training loss: 312.63409423828125 = 0.1234511062502861 + 50.0 * 6.250213146209717
Epoch 1430, val loss: 1.0581774711608887
Epoch 1440, training loss: 312.78265380859375 = 0.12055797129869461 + 50.0 * 6.253242015838623
Epoch 1440, val loss: 1.0627230405807495
Epoch 1450, training loss: 312.6712646484375 = 0.11771396547555923 + 50.0 * 6.251071453094482
Epoch 1450, val loss: 1.0676542520523071
Epoch 1460, training loss: 312.5612487792969 = 0.11494214087724686 + 50.0 * 6.248926639556885
Epoch 1460, val loss: 1.0717419385910034
Epoch 1470, training loss: 312.5843505859375 = 0.11226657778024673 + 50.0 * 6.249441623687744
Epoch 1470, val loss: 1.0766476392745972
Epoch 1480, training loss: 312.7913818359375 = 0.10965307056903839 + 50.0 * 6.253634929656982
Epoch 1480, val loss: 1.0815004110336304
Epoch 1490, training loss: 312.7430419921875 = 0.10706521570682526 + 50.0 * 6.252719879150391
Epoch 1490, val loss: 1.0870634317398071
Epoch 1500, training loss: 312.61334228515625 = 0.10455737262964249 + 50.0 * 6.250175952911377
Epoch 1500, val loss: 1.0906596183776855
Epoch 1510, training loss: 312.4830627441406 = 0.1021108329296112 + 50.0 * 6.247619152069092
Epoch 1510, val loss: 1.0958893299102783
Epoch 1520, training loss: 312.5350341796875 = 0.09975426644086838 + 50.0 * 6.2487053871154785
Epoch 1520, val loss: 1.1014111042022705
Epoch 1530, training loss: 312.58612060546875 = 0.09741759300231934 + 50.0 * 6.249773979187012
Epoch 1530, val loss: 1.1057571172714233
Epoch 1540, training loss: 312.4296875 = 0.09515812247991562 + 50.0 * 6.24669075012207
Epoch 1540, val loss: 1.1106051206588745
Epoch 1550, training loss: 312.42572021484375 = 0.0929504930973053 + 50.0 * 6.246654987335205
Epoch 1550, val loss: 1.115254521369934
Epoch 1560, training loss: 312.5303039550781 = 0.09081597626209259 + 50.0 * 6.2487897872924805
Epoch 1560, val loss: 1.1198402643203735
Epoch 1570, training loss: 312.3974914550781 = 0.08870556950569153 + 50.0 * 6.246175289154053
Epoch 1570, val loss: 1.1260594129562378
Epoch 1580, training loss: 312.3292541503906 = 0.08666497468948364 + 50.0 * 6.244851589202881
Epoch 1580, val loss: 1.130659580230713
Epoch 1590, training loss: 312.28125 = 0.08467822521924973 + 50.0 * 6.243931293487549
Epoch 1590, val loss: 1.134909749031067
Epoch 1600, training loss: 312.6122131347656 = 0.08275334537029266 + 50.0 * 6.250588893890381
Epoch 1600, val loss: 1.1403268575668335
Epoch 1610, training loss: 312.4641418457031 = 0.08084867894649506 + 50.0 * 6.247665882110596
Epoch 1610, val loss: 1.1460554599761963
Epoch 1620, training loss: 312.3058776855469 = 0.07898686081171036 + 50.0 * 6.244537830352783
Epoch 1620, val loss: 1.14962637424469
Epoch 1630, training loss: 312.42279052734375 = 0.0772135853767395 + 50.0 * 6.246911525726318
Epoch 1630, val loss: 1.1552525758743286
Epoch 1640, training loss: 312.2119140625 = 0.07543191313743591 + 50.0 * 6.242729187011719
Epoch 1640, val loss: 1.1602908372879028
Epoch 1650, training loss: 312.1739196777344 = 0.07371378690004349 + 50.0 * 6.24200439453125
Epoch 1650, val loss: 1.1650744676589966
Epoch 1660, training loss: 312.14971923828125 = 0.07207679748535156 + 50.0 * 6.241552829742432
Epoch 1660, val loss: 1.1699905395507812
Epoch 1670, training loss: 312.1945495605469 = 0.07047439366579056 + 50.0 * 6.242481708526611
Epoch 1670, val loss: 1.1743031740188599
Epoch 1680, training loss: 312.2588806152344 = 0.06889799982309341 + 50.0 * 6.243799209594727
Epoch 1680, val loss: 1.1799521446228027
Epoch 1690, training loss: 312.2752990722656 = 0.06736067682504654 + 50.0 * 6.24415922164917
Epoch 1690, val loss: 1.1856271028518677
Epoch 1700, training loss: 312.1395263671875 = 0.06585118919610977 + 50.0 * 6.241473197937012
Epoch 1700, val loss: 1.1897518634796143
Epoch 1710, training loss: 312.0997009277344 = 0.0644044354557991 + 50.0 * 6.240705966949463
Epoch 1710, val loss: 1.1953307390213013
Epoch 1720, training loss: 312.3917236328125 = 0.06300218403339386 + 50.0 * 6.246574878692627
Epoch 1720, val loss: 1.1997687816619873
Epoch 1730, training loss: 312.16571044921875 = 0.06159141659736633 + 50.0 * 6.242082595825195
Epoch 1730, val loss: 1.204085350036621
Epoch 1740, training loss: 312.0568542480469 = 0.06024458259344101 + 50.0 * 6.239932537078857
Epoch 1740, val loss: 1.2106285095214844
Epoch 1750, training loss: 311.99090576171875 = 0.05894250050187111 + 50.0 * 6.2386393547058105
Epoch 1750, val loss: 1.2143577337265015
Epoch 1760, training loss: 312.0040588378906 = 0.05768876522779465 + 50.0 * 6.238927364349365
Epoch 1760, val loss: 1.2203454971313477
Epoch 1770, training loss: 312.4046325683594 = 0.056451600044965744 + 50.0 * 6.2469635009765625
Epoch 1770, val loss: 1.2249747514724731
Epoch 1780, training loss: 312.0577697753906 = 0.05521517992019653 + 50.0 * 6.24005126953125
Epoch 1780, val loss: 1.229654312133789
Epoch 1790, training loss: 311.9197082519531 = 0.05403236672282219 + 50.0 * 6.237313270568848
Epoch 1790, val loss: 1.2337366342544556
Epoch 1800, training loss: 311.9261779785156 = 0.052905261516571045 + 50.0 * 6.2374653816223145
Epoch 1800, val loss: 1.23936927318573
Epoch 1810, training loss: 312.0657653808594 = 0.051807232201099396 + 50.0 * 6.240279197692871
Epoch 1810, val loss: 1.2436186075210571
Epoch 1820, training loss: 312.00982666015625 = 0.05071575939655304 + 50.0 * 6.239181995391846
Epoch 1820, val loss: 1.248153805732727
Epoch 1830, training loss: 311.92181396484375 = 0.04964534193277359 + 50.0 * 6.237442970275879
Epoch 1830, val loss: 1.253218173980713
Epoch 1840, training loss: 312.00262451171875 = 0.048621222376823425 + 50.0 * 6.239080429077148
Epoch 1840, val loss: 1.2583361864089966
Epoch 1850, training loss: 311.88043212890625 = 0.0476117804646492 + 50.0 * 6.236656665802002
Epoch 1850, val loss: 1.2630345821380615
Epoch 1860, training loss: 311.93701171875 = 0.04664262756705284 + 50.0 * 6.237807750701904
Epoch 1860, val loss: 1.2679736614227295
Epoch 1870, training loss: 311.857666015625 = 0.04569659382104874 + 50.0 * 6.236239433288574
Epoch 1870, val loss: 1.2721120119094849
Epoch 1880, training loss: 312.0394287109375 = 0.044787608087062836 + 50.0 * 6.239892482757568
Epoch 1880, val loss: 1.276632308959961
Epoch 1890, training loss: 311.92364501953125 = 0.043864842504262924 + 50.0 * 6.237596035003662
Epoch 1890, val loss: 1.2803064584732056
Epoch 1900, training loss: 311.7769775390625 = 0.042972076684236526 + 50.0 * 6.23468017578125
Epoch 1900, val loss: 1.2856327295303345
Epoch 1910, training loss: 311.7347717285156 = 0.04212023317813873 + 50.0 * 6.233852863311768
Epoch 1910, val loss: 1.290196180343628
Epoch 1920, training loss: 311.7433776855469 = 0.04129886254668236 + 50.0 * 6.234041690826416
Epoch 1920, val loss: 1.2949539422988892
Epoch 1930, training loss: 312.0106506347656 = 0.04050392284989357 + 50.0 * 6.239402770996094
Epoch 1930, val loss: 1.300073504447937
Epoch 1940, training loss: 311.78619384765625 = 0.03970084711909294 + 50.0 * 6.234930038452148
Epoch 1940, val loss: 1.302704930305481
Epoch 1950, training loss: 311.7781982421875 = 0.038929130882024765 + 50.0 * 6.234785079956055
Epoch 1950, val loss: 1.3080748319625854
Epoch 1960, training loss: 311.79168701171875 = 0.038181815296411514 + 50.0 * 6.23507022857666
Epoch 1960, val loss: 1.3111364841461182
Epoch 1970, training loss: 311.66436767578125 = 0.03744690865278244 + 50.0 * 6.23253870010376
Epoch 1970, val loss: 1.3164875507354736
Epoch 1980, training loss: 311.6930236816406 = 0.03674257919192314 + 50.0 * 6.233125686645508
Epoch 1980, val loss: 1.3211183547973633
Epoch 1990, training loss: 312.0342102050781 = 0.036062467843294144 + 50.0 * 6.239963054656982
Epoch 1990, val loss: 1.324884295463562
Epoch 2000, training loss: 311.746826171875 = 0.035360001027584076 + 50.0 * 6.23422908782959
Epoch 2000, val loss: 1.329249382019043
Epoch 2010, training loss: 311.66290283203125 = 0.034699201583862305 + 50.0 * 6.2325639724731445
Epoch 2010, val loss: 1.3335278034210205
Epoch 2020, training loss: 311.9579772949219 = 0.03406904637813568 + 50.0 * 6.238478660583496
Epoch 2020, val loss: 1.3370399475097656
Epoch 2030, training loss: 311.7265625 = 0.03341974690556526 + 50.0 * 6.23386287689209
Epoch 2030, val loss: 1.3412891626358032
Epoch 2040, training loss: 311.60650634765625 = 0.03280076012015343 + 50.0 * 6.231473922729492
Epoch 2040, val loss: 1.346068263053894
Epoch 2050, training loss: 311.555419921875 = 0.03221121057868004 + 50.0 * 6.230464458465576
Epoch 2050, val loss: 1.3498495817184448
Epoch 2060, training loss: 311.5464172363281 = 0.03163890913128853 + 50.0 * 6.230295658111572
Epoch 2060, val loss: 1.3540716171264648
Epoch 2070, training loss: 311.78228759765625 = 0.031088929623365402 + 50.0 * 6.2350239753723145
Epoch 2070, val loss: 1.3581448793411255
Epoch 2080, training loss: 311.7834167480469 = 0.030525708571076393 + 50.0 * 6.235057830810547
Epoch 2080, val loss: 1.3627022504806519
Epoch 2090, training loss: 311.8317565917969 = 0.029968783259391785 + 50.0 * 6.2360358238220215
Epoch 2090, val loss: 1.365370750427246
Epoch 2100, training loss: 311.5428466796875 = 0.029434965923428535 + 50.0 * 6.2302680015563965
Epoch 2100, val loss: 1.3694591522216797
Epoch 2110, training loss: 311.4964294433594 = 0.028925085440278053 + 50.0 * 6.2293500900268555
Epoch 2110, val loss: 1.3738068342208862
Epoch 2120, training loss: 311.50421142578125 = 0.02843356877565384 + 50.0 * 6.229515075683594
Epoch 2120, val loss: 1.3773056268692017
Epoch 2130, training loss: 311.80072021484375 = 0.027958393096923828 + 50.0 * 6.235455513000488
Epoch 2130, val loss: 1.3809876441955566
Epoch 2140, training loss: 311.6111145019531 = 0.027471009641885757 + 50.0 * 6.231673240661621
Epoch 2140, val loss: 1.3849819898605347
Epoch 2150, training loss: 311.55322265625 = 0.026999587193131447 + 50.0 * 6.230524063110352
Epoch 2150, val loss: 1.388123631477356
Epoch 2160, training loss: 311.4993896484375 = 0.026542499661445618 + 50.0 * 6.229456901550293
Epoch 2160, val loss: 1.3925988674163818
Epoch 2170, training loss: 311.4942321777344 = 0.026102401316165924 + 50.0 * 6.229362964630127
Epoch 2170, val loss: 1.3967994451522827
Epoch 2180, training loss: 311.5415954589844 = 0.02567068673670292 + 50.0 * 6.230318546295166
Epoch 2180, val loss: 1.4001795053482056
Epoch 2190, training loss: 311.58087158203125 = 0.02524656616151333 + 50.0 * 6.231112480163574
Epoch 2190, val loss: 1.4032782316207886
Epoch 2200, training loss: 311.5162658691406 = 0.02483355440199375 + 50.0 * 6.229828357696533
Epoch 2200, val loss: 1.4074159860610962
Epoch 2210, training loss: 311.40032958984375 = 0.024428708478808403 + 50.0 * 6.227518558502197
Epoch 2210, val loss: 1.4107269048690796
Epoch 2220, training loss: 311.4332275390625 = 0.024044377729296684 + 50.0 * 6.228183746337891
Epoch 2220, val loss: 1.4138864278793335
Epoch 2230, training loss: 311.7537536621094 = 0.023671112954616547 + 50.0 * 6.2346014976501465
Epoch 2230, val loss: 1.4164073467254639
Epoch 2240, training loss: 311.55419921875 = 0.02327917143702507 + 50.0 * 6.230618476867676
Epoch 2240, val loss: 1.4214601516723633
Epoch 2250, training loss: 311.42828369140625 = 0.022908052429556847 + 50.0 * 6.228107452392578
Epoch 2250, val loss: 1.4242284297943115
Epoch 2260, training loss: 311.4001770019531 = 0.022553972899913788 + 50.0 * 6.22755241394043
Epoch 2260, val loss: 1.4278379678726196
Epoch 2270, training loss: 311.5758972167969 = 0.022209415212273598 + 50.0 * 6.23107385635376
Epoch 2270, val loss: 1.4309829473495483
Epoch 2280, training loss: 311.4239501953125 = 0.021865908056497574 + 50.0 * 6.228041648864746
Epoch 2280, val loss: 1.4355050325393677
Epoch 2290, training loss: 311.3876647949219 = 0.021532537415623665 + 50.0 * 6.227322578430176
Epoch 2290, val loss: 1.4379674196243286
Epoch 2300, training loss: 311.6020812988281 = 0.021208681166172028 + 50.0 * 6.231617450714111
Epoch 2300, val loss: 1.4405478239059448
Epoch 2310, training loss: 311.4504089355469 = 0.02087918110191822 + 50.0 * 6.228590488433838
Epoch 2310, val loss: 1.4452226161956787
Epoch 2320, training loss: 311.32696533203125 = 0.02056356705725193 + 50.0 * 6.226128101348877
Epoch 2320, val loss: 1.4478442668914795
Epoch 2330, training loss: 311.2451171875 = 0.02026044763624668 + 50.0 * 6.224497318267822
Epoch 2330, val loss: 1.451263666152954
Epoch 2340, training loss: 311.29681396484375 = 0.019968174397945404 + 50.0 * 6.225536823272705
Epoch 2340, val loss: 1.4548876285552979
Epoch 2350, training loss: 311.7286071777344 = 0.01968071050941944 + 50.0 * 6.23417854309082
Epoch 2350, val loss: 1.4583642482757568
Epoch 2360, training loss: 311.4317932128906 = 0.019389623776078224 + 50.0 * 6.228248119354248
Epoch 2360, val loss: 1.460940957069397
Epoch 2370, training loss: 311.28680419921875 = 0.019104324281215668 + 50.0 * 6.225353717803955
Epoch 2370, val loss: 1.4644789695739746
Epoch 2380, training loss: 311.293212890625 = 0.018839970231056213 + 50.0 * 6.22548770904541
Epoch 2380, val loss: 1.4680991172790527
Epoch 2390, training loss: 311.321044921875 = 0.018575435504317284 + 50.0 * 6.226049423217773
Epoch 2390, val loss: 1.4710103273391724
Epoch 2400, training loss: 311.41357421875 = 0.018313273787498474 + 50.0 * 6.2279052734375
Epoch 2400, val loss: 1.4730619192123413
Epoch 2410, training loss: 311.2179870605469 = 0.0180504247546196 + 50.0 * 6.2239990234375
Epoch 2410, val loss: 1.4765417575836182
Epoch 2420, training loss: 311.1978454589844 = 0.017799247056245804 + 50.0 * 6.2236008644104
Epoch 2420, val loss: 1.4792379140853882
Epoch 2430, training loss: 311.1972351074219 = 0.017557941377162933 + 50.0 * 6.223593711853027
Epoch 2430, val loss: 1.4818881750106812
Epoch 2440, training loss: 311.7302551269531 = 0.01732689142227173 + 50.0 * 6.23425817489624
Epoch 2440, val loss: 1.4843064546585083
Epoch 2450, training loss: 311.3299865722656 = 0.017081240192055702 + 50.0 * 6.226257801055908
Epoch 2450, val loss: 1.4885952472686768
Epoch 2460, training loss: 311.179931640625 = 0.016842694953083992 + 50.0 * 6.223261833190918
Epoch 2460, val loss: 1.4907606840133667
Epoch 2470, training loss: 311.18914794921875 = 0.016621893271803856 + 50.0 * 6.223450183868408
Epoch 2470, val loss: 1.493780493736267
Epoch 2480, training loss: 311.3328857421875 = 0.016405923292040825 + 50.0 * 6.226329326629639
Epoch 2480, val loss: 1.4963010549545288
Epoch 2490, training loss: 311.17559814453125 = 0.016185512766242027 + 50.0 * 6.223188400268555
Epoch 2490, val loss: 1.5001808404922485
Epoch 2500, training loss: 311.34307861328125 = 0.01597963273525238 + 50.0 * 6.226541996002197
Epoch 2500, val loss: 1.5022178888320923
Epoch 2510, training loss: 311.1541748046875 = 0.015763234347105026 + 50.0 * 6.2227678298950195
Epoch 2510, val loss: 1.5046424865722656
Epoch 2520, training loss: 311.1798095703125 = 0.015558436512947083 + 50.0 * 6.22328519821167
Epoch 2520, val loss: 1.5076762437820435
Epoch 2530, training loss: 311.1403503417969 = 0.015361602418124676 + 50.0 * 6.222499847412109
Epoch 2530, val loss: 1.5109102725982666
Epoch 2540, training loss: 311.16290283203125 = 0.015170023776590824 + 50.0 * 6.222954750061035
Epoch 2540, val loss: 1.5135234594345093
Epoch 2550, training loss: 311.0941162109375 = 0.01497670728713274 + 50.0 * 6.221582889556885
Epoch 2550, val loss: 1.5157032012939453
Epoch 2560, training loss: 311.1744689941406 = 0.014791222289204597 + 50.0 * 6.223193645477295
Epoch 2560, val loss: 1.518825888633728
Epoch 2570, training loss: 311.2424011230469 = 0.014611628837883472 + 50.0 * 6.224555969238281
Epoch 2570, val loss: 1.5222175121307373
Epoch 2580, training loss: 311.167724609375 = 0.014424849301576614 + 50.0 * 6.2230658531188965
Epoch 2580, val loss: 1.5248057842254639
Epoch 2590, training loss: 311.0652770996094 = 0.014244351536035538 + 50.0 * 6.221020221710205
Epoch 2590, val loss: 1.5268677473068237
Epoch 2600, training loss: 311.0061950683594 = 0.014072892256081104 + 50.0 * 6.219842433929443
Epoch 2600, val loss: 1.5297071933746338
Epoch 2610, training loss: 311.0486755371094 = 0.013908786699175835 + 50.0 * 6.2206950187683105
Epoch 2610, val loss: 1.531881332397461
Epoch 2620, training loss: 311.4356384277344 = 0.013750730082392693 + 50.0 * 6.228437423706055
Epoch 2620, val loss: 1.5352259874343872
Epoch 2630, training loss: 311.2540588378906 = 0.013579408638179302 + 50.0 * 6.224809646606445
Epoch 2630, val loss: 1.5371627807617188
Epoch 2640, training loss: 311.1897888183594 = 0.013417183421552181 + 50.0 * 6.223527431488037
Epoch 2640, val loss: 1.5397872924804688
Epoch 2650, training loss: 311.1672668457031 = 0.013257390819489956 + 50.0 * 6.223080158233643
Epoch 2650, val loss: 1.5408525466918945
Epoch 2660, training loss: 311.0679016113281 = 0.013100818730890751 + 50.0 * 6.221096038818359
Epoch 2660, val loss: 1.5431349277496338
Epoch 2670, training loss: 310.9935302734375 = 0.012949902564287186 + 50.0 * 6.219611644744873
Epoch 2670, val loss: 1.5471032857894897
Epoch 2680, training loss: 310.95147705078125 = 0.012803034856915474 + 50.0 * 6.218772888183594
Epoch 2680, val loss: 1.5493929386138916
Epoch 2690, training loss: 311.2019958496094 = 0.012664446607232094 + 50.0 * 6.2237868309021
Epoch 2690, val loss: 1.5515145063400269
Epoch 2700, training loss: 310.9703369140625 = 0.012515944428741932 + 50.0 * 6.219156742095947
Epoch 2700, val loss: 1.5537519454956055
Epoch 2710, training loss: 310.9621887207031 = 0.01237581018358469 + 50.0 * 6.218996047973633
Epoch 2710, val loss: 1.5557799339294434
Epoch 2720, training loss: 311.0317687988281 = 0.012239367701113224 + 50.0 * 6.220390796661377
Epoch 2720, val loss: 1.5574103593826294
Epoch 2730, training loss: 311.13922119140625 = 0.012108340859413147 + 50.0 * 6.2225422859191895
Epoch 2730, val loss: 1.5598655939102173
Epoch 2740, training loss: 311.095458984375 = 0.011973163112998009 + 50.0 * 6.221669673919678
Epoch 2740, val loss: 1.563112735748291
Epoch 2750, training loss: 311.0285339355469 = 0.011842812411487103 + 50.0 * 6.220333576202393
Epoch 2750, val loss: 1.5652661323547363
Epoch 2760, training loss: 310.8960876464844 = 0.011711709201335907 + 50.0 * 6.217687606811523
Epoch 2760, val loss: 1.5674083232879639
Epoch 2770, training loss: 311.082275390625 = 0.011593026109039783 + 50.0 * 6.221413612365723
Epoch 2770, val loss: 1.569308876991272
Epoch 2780, training loss: 311.1346740722656 = 0.011466491967439651 + 50.0 * 6.222464084625244
Epoch 2780, val loss: 1.5719150304794312
Epoch 2790, training loss: 310.9195861816406 = 0.011338618583977222 + 50.0 * 6.218164443969727
Epoch 2790, val loss: 1.5743416547775269
Epoch 2800, training loss: 310.8435974121094 = 0.011220252141356468 + 50.0 * 6.216647624969482
Epoch 2800, val loss: 1.5764104127883911
Epoch 2810, training loss: 310.891845703125 = 0.011108357459306717 + 50.0 * 6.217614650726318
Epoch 2810, val loss: 1.5790590047836304
Epoch 2820, training loss: 311.17681884765625 = 0.01099521853029728 + 50.0 * 6.223316669464111
Epoch 2820, val loss: 1.58137047290802
Epoch 2830, training loss: 311.0873107910156 = 0.010880024172365665 + 50.0 * 6.221528053283691
Epoch 2830, val loss: 1.5824922323226929
Epoch 2840, training loss: 310.9732971191406 = 0.01076719630509615 + 50.0 * 6.219250202178955
Epoch 2840, val loss: 1.5855158567428589
Epoch 2850, training loss: 310.98095703125 = 0.010660341009497643 + 50.0 * 6.2194061279296875
Epoch 2850, val loss: 1.58744215965271
Epoch 2860, training loss: 310.9189147949219 = 0.010552172549068928 + 50.0 * 6.218167304992676
Epoch 2860, val loss: 1.5895462036132812
Epoch 2870, training loss: 310.82470703125 = 0.010444237850606441 + 50.0 * 6.216285228729248
Epoch 2870, val loss: 1.591246485710144
Epoch 2880, training loss: 310.8995056152344 = 0.010344283655285835 + 50.0 * 6.217783451080322
Epoch 2880, val loss: 1.5929449796676636
Epoch 2890, training loss: 310.9212951660156 = 0.010243810713291168 + 50.0 * 6.2182207107543945
Epoch 2890, val loss: 1.594669222831726
Epoch 2900, training loss: 311.1921691894531 = 0.010148311965167522 + 50.0 * 6.223639965057373
Epoch 2900, val loss: 1.5962798595428467
Epoch 2910, training loss: 310.9132385253906 = 0.010037929750978947 + 50.0 * 6.218063831329346
Epoch 2910, val loss: 1.5989376306533813
Epoch 2920, training loss: 310.8431396484375 = 0.009941471740603447 + 50.0 * 6.216663837432861
Epoch 2920, val loss: 1.6014413833618164
Epoch 2930, training loss: 310.88531494140625 = 0.009848725982010365 + 50.0 * 6.2175092697143555
Epoch 2930, val loss: 1.602675199508667
Epoch 2940, training loss: 310.9239807128906 = 0.009754846803843975 + 50.0 * 6.2182841300964355
Epoch 2940, val loss: 1.6048270463943481
Epoch 2950, training loss: 310.812744140625 = 0.00966316182166338 + 50.0 * 6.216061592102051
Epoch 2950, val loss: 1.6077104806900024
Epoch 2960, training loss: 310.8516845703125 = 0.009573142975568771 + 50.0 * 6.216842174530029
Epoch 2960, val loss: 1.6088684797286987
Epoch 2970, training loss: 310.8570251464844 = 0.0094847297295928 + 50.0 * 6.2169508934021
Epoch 2970, val loss: 1.6106318235397339
Epoch 2980, training loss: 310.7771911621094 = 0.009397357702255249 + 50.0 * 6.21535587310791
Epoch 2980, val loss: 1.6133219003677368
Epoch 2990, training loss: 310.8591613769531 = 0.009313295595347881 + 50.0 * 6.216997146606445
Epoch 2990, val loss: 1.6142654418945312
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 431.77032470703125 = 1.9273444414138794 + 50.0 * 8.5968599319458
Epoch 0, val loss: 1.9253169298171997
Epoch 10, training loss: 431.73126220703125 = 1.9185649156570435 + 50.0 * 8.596253395080566
Epoch 10, val loss: 1.9170390367507935
Epoch 20, training loss: 431.4972229003906 = 1.9075462818145752 + 50.0 * 8.59179401397705
Epoch 20, val loss: 1.9062434434890747
Epoch 30, training loss: 429.9572448730469 = 1.8933534622192383 + 50.0 * 8.561278343200684
Epoch 30, val loss: 1.8919605016708374
Epoch 40, training loss: 421.9576721191406 = 1.8761539459228516 + 50.0 * 8.401630401611328
Epoch 40, val loss: 1.8747984170913696
Epoch 50, training loss: 403.5285949707031 = 1.8575931787490845 + 50.0 * 8.033419609069824
Epoch 50, val loss: 1.8567454814910889
Epoch 60, training loss: 383.30389404296875 = 1.8430709838867188 + 50.0 * 7.629216194152832
Epoch 60, val loss: 1.8436087369918823
Epoch 70, training loss: 362.4994812011719 = 1.8354551792144775 + 50.0 * 7.21328067779541
Epoch 70, val loss: 1.8360625505447388
Epoch 80, training loss: 354.4155578613281 = 1.827942132949829 + 50.0 * 7.051752090454102
Epoch 80, val loss: 1.8278913497924805
Epoch 90, training loss: 347.3439025878906 = 1.8184298276901245 + 50.0 * 6.9105095863342285
Epoch 90, val loss: 1.8188037872314453
Epoch 100, training loss: 343.5180358886719 = 1.8093379735946655 + 50.0 * 6.834174156188965
Epoch 100, val loss: 1.8105058670043945
Epoch 110, training loss: 340.197021484375 = 1.800389289855957 + 50.0 * 6.767932891845703
Epoch 110, val loss: 1.8021905422210693
Epoch 120, training loss: 337.68603515625 = 1.791582703590393 + 50.0 * 6.717888832092285
Epoch 120, val loss: 1.793882131576538
Epoch 130, training loss: 335.83099365234375 = 1.7824252843856812 + 50.0 * 6.680971145629883
Epoch 130, val loss: 1.7853916883468628
Epoch 140, training loss: 334.2509460449219 = 1.7724363803863525 + 50.0 * 6.649570465087891
Epoch 140, val loss: 1.7763755321502686
Epoch 150, training loss: 332.840576171875 = 1.7616064548492432 + 50.0 * 6.621579647064209
Epoch 150, val loss: 1.7667369842529297
Epoch 160, training loss: 331.8858642578125 = 1.7499171495437622 + 50.0 * 6.602719306945801
Epoch 160, val loss: 1.7562857866287231
Epoch 170, training loss: 330.6537170410156 = 1.7369428873062134 + 50.0 * 6.578335285186768
Epoch 170, val loss: 1.7447600364685059
Epoch 180, training loss: 329.73138427734375 = 1.7228182554244995 + 50.0 * 6.560171604156494
Epoch 180, val loss: 1.732239007949829
Epoch 190, training loss: 328.8607177734375 = 1.707470178604126 + 50.0 * 6.543065071105957
Epoch 190, val loss: 1.7186647653579712
Epoch 200, training loss: 328.1597595214844 = 1.6908131837844849 + 50.0 * 6.529378890991211
Epoch 200, val loss: 1.703993797302246
Epoch 210, training loss: 327.4223937988281 = 1.6727389097213745 + 50.0 * 6.514993190765381
Epoch 210, val loss: 1.6881433725357056
Epoch 220, training loss: 326.7369384765625 = 1.6532968282699585 + 50.0 * 6.501673221588135
Epoch 220, val loss: 1.6713190078735352
Epoch 230, training loss: 326.06903076171875 = 1.632676362991333 + 50.0 * 6.488726615905762
Epoch 230, val loss: 1.653508186340332
Epoch 240, training loss: 325.4806213378906 = 1.6108959913253784 + 50.0 * 6.4773945808410645
Epoch 240, val loss: 1.6348649263381958
Epoch 250, training loss: 324.908935546875 = 1.587910532951355 + 50.0 * 6.4664201736450195
Epoch 250, val loss: 1.6153926849365234
Epoch 260, training loss: 324.42694091796875 = 1.5641647577285767 + 50.0 * 6.4572553634643555
Epoch 260, val loss: 1.5954487323760986
Epoch 270, training loss: 323.9735412597656 = 1.5396648645401 + 50.0 * 6.4486775398254395
Epoch 270, val loss: 1.5751267671585083
Epoch 280, training loss: 323.52484130859375 = 1.5146708488464355 + 50.0 * 6.4402031898498535
Epoch 280, val loss: 1.5547043085098267
Epoch 290, training loss: 323.4175109863281 = 1.489229440689087 + 50.0 * 6.438565731048584
Epoch 290, val loss: 1.5340691804885864
Epoch 300, training loss: 322.8364562988281 = 1.463532567024231 + 50.0 * 6.427458763122559
Epoch 300, val loss: 1.5136700868606567
Epoch 310, training loss: 322.4143371582031 = 1.437807559967041 + 50.0 * 6.419530868530273
Epoch 310, val loss: 1.4935444593429565
Epoch 320, training loss: 322.1156005859375 = 1.412035584449768 + 50.0 * 6.414071083068848
Epoch 320, val loss: 1.4736533164978027
Epoch 330, training loss: 321.9313049316406 = 1.3862535953521729 + 50.0 * 6.410900592803955
Epoch 330, val loss: 1.454066276550293
Epoch 340, training loss: 321.4576416015625 = 1.3605551719665527 + 50.0 * 6.401941299438477
Epoch 340, val loss: 1.4347643852233887
Epoch 350, training loss: 321.16815185546875 = 1.334957480430603 + 50.0 * 6.396663665771484
Epoch 350, val loss: 1.415686011314392
Epoch 360, training loss: 320.87384033203125 = 1.309433102607727 + 50.0 * 6.391287803649902
Epoch 360, val loss: 1.396837830543518
Epoch 370, training loss: 320.6238098144531 = 1.2838695049285889 + 50.0 * 6.386798858642578
Epoch 370, val loss: 1.3782508373260498
Epoch 380, training loss: 320.4178771972656 = 1.2583706378936768 + 50.0 * 6.383190155029297
Epoch 380, val loss: 1.3597679138183594
Epoch 390, training loss: 320.1610107421875 = 1.2328574657440186 + 50.0 * 6.378562927246094
Epoch 390, val loss: 1.3414698839187622
Epoch 400, training loss: 319.93865966796875 = 1.2073713541030884 + 50.0 * 6.3746256828308105
Epoch 400, val loss: 1.3233585357666016
Epoch 410, training loss: 320.1358642578125 = 1.1818894147872925 + 50.0 * 6.379079341888428
Epoch 410, val loss: 1.3054887056350708
Epoch 420, training loss: 319.6357727050781 = 1.1562179327011108 + 50.0 * 6.369591236114502
Epoch 420, val loss: 1.2873706817626953
Epoch 430, training loss: 319.31463623046875 = 1.1307014226913452 + 50.0 * 6.363678455352783
Epoch 430, val loss: 1.2695554494857788
Epoch 440, training loss: 319.1236267089844 = 1.105333924293518 + 50.0 * 6.360365867614746
Epoch 440, val loss: 1.2521495819091797
Epoch 450, training loss: 318.97991943359375 = 1.0800588130950928 + 50.0 * 6.357997417449951
Epoch 450, val loss: 1.234810709953308
Epoch 460, training loss: 318.7976379394531 = 1.0549347400665283 + 50.0 * 6.354854106903076
Epoch 460, val loss: 1.218018889427185
Epoch 470, training loss: 318.6831970214844 = 1.030056357383728 + 50.0 * 6.353062629699707
Epoch 470, val loss: 1.201242208480835
Epoch 480, training loss: 318.42669677734375 = 1.0055912733078003 + 50.0 * 6.348422050476074
Epoch 480, val loss: 1.185115933418274
Epoch 490, training loss: 318.5557556152344 = 0.981471836566925 + 50.0 * 6.351485729217529
Epoch 490, val loss: 1.1691644191741943
Epoch 500, training loss: 318.29815673828125 = 0.9577831029891968 + 50.0 * 6.34680700302124
Epoch 500, val loss: 1.1540627479553223
Epoch 510, training loss: 318.0601501464844 = 0.9346817135810852 + 50.0 * 6.3425092697143555
Epoch 510, val loss: 1.1398173570632935
Epoch 520, training loss: 317.846923828125 = 0.9120716452598572 + 50.0 * 6.3386969566345215
Epoch 520, val loss: 1.1256465911865234
Epoch 530, training loss: 317.6988830566406 = 0.8900902271270752 + 50.0 * 6.336175918579102
Epoch 530, val loss: 1.1121044158935547
Epoch 540, training loss: 317.8296813964844 = 0.8686598539352417 + 50.0 * 6.3392205238342285
Epoch 540, val loss: 1.0990084409713745
Epoch 550, training loss: 317.4921875 = 0.8478462100028992 + 50.0 * 6.332886695861816
Epoch 550, val loss: 1.0874111652374268
Epoch 560, training loss: 317.3235168457031 = 0.8276075124740601 + 50.0 * 6.329917907714844
Epoch 560, val loss: 1.0758466720581055
Epoch 570, training loss: 317.3020324707031 = 0.8079502582550049 + 50.0 * 6.32988166809082
Epoch 570, val loss: 1.065026044845581
Epoch 580, training loss: 317.11041259765625 = 0.7889148592948914 + 50.0 * 6.326429843902588
Epoch 580, val loss: 1.0550581216812134
Epoch 590, training loss: 316.95208740234375 = 0.7704930305480957 + 50.0 * 6.323631763458252
Epoch 590, val loss: 1.0456119775772095
Epoch 600, training loss: 317.1066589355469 = 0.7525650858879089 + 50.0 * 6.32708215713501
Epoch 600, val loss: 1.0368130207061768
Epoch 610, training loss: 316.8823547363281 = 0.7351089119911194 + 50.0 * 6.3229451179504395
Epoch 610, val loss: 1.0288708209991455
Epoch 620, training loss: 316.6588134765625 = 0.7181113362312317 + 50.0 * 6.318814277648926
Epoch 620, val loss: 1.0212266445159912
Epoch 630, training loss: 316.6166076660156 = 0.7015891671180725 + 50.0 * 6.318300247192383
Epoch 630, val loss: 1.0140243768692017
Epoch 640, training loss: 316.52880859375 = 0.6854881644248962 + 50.0 * 6.316866397857666
Epoch 640, val loss: 1.0072531700134277
Epoch 650, training loss: 316.39837646484375 = 0.669767439365387 + 50.0 * 6.314571857452393
Epoch 650, val loss: 1.0009182691574097
Epoch 660, training loss: 316.43841552734375 = 0.6543864011764526 + 50.0 * 6.315680503845215
Epoch 660, val loss: 0.9957346320152283
Epoch 670, training loss: 316.145751953125 = 0.6393293142318726 + 50.0 * 6.310128211975098
Epoch 670, val loss: 0.9902021884918213
Epoch 680, training loss: 316.053955078125 = 0.6246393918991089 + 50.0 * 6.308586597442627
Epoch 680, val loss: 0.9851789474487305
Epoch 690, training loss: 315.9543151855469 = 0.6102872490882874 + 50.0 * 6.306880474090576
Epoch 690, val loss: 0.9806074500083923
Epoch 700, training loss: 315.9886169433594 = 0.5962058901786804 + 50.0 * 6.3078484535217285
Epoch 700, val loss: 0.9763646721839905
Epoch 710, training loss: 316.1123352050781 = 0.5822533369064331 + 50.0 * 6.310601711273193
Epoch 710, val loss: 0.9728542566299438
Epoch 720, training loss: 315.71978759765625 = 0.5685228705406189 + 50.0 * 6.303025722503662
Epoch 720, val loss: 0.9691917300224304
Epoch 730, training loss: 315.6253356933594 = 0.5550677180290222 + 50.0 * 6.301405906677246
Epoch 730, val loss: 0.9656098484992981
Epoch 740, training loss: 315.6039123535156 = 0.5418540835380554 + 50.0 * 6.301241397857666
Epoch 740, val loss: 0.9623888731002808
Epoch 750, training loss: 315.74273681640625 = 0.5288348197937012 + 50.0 * 6.3042778968811035
Epoch 750, val loss: 0.9596565365791321
Epoch 760, training loss: 315.4305725097656 = 0.5158132314682007 + 50.0 * 6.298295021057129
Epoch 760, val loss: 0.956479549407959
Epoch 770, training loss: 315.3634338378906 = 0.5030903816223145 + 50.0 * 6.297206878662109
Epoch 770, val loss: 0.9540238976478577
Epoch 780, training loss: 315.2578430175781 = 0.4905455410480499 + 50.0 * 6.295345783233643
Epoch 780, val loss: 0.9515846371650696
Epoch 790, training loss: 315.3470764160156 = 0.4781922996044159 + 50.0 * 6.297378063201904
Epoch 790, val loss: 0.9495273232460022
Epoch 800, training loss: 315.3871154785156 = 0.4658867120742798 + 50.0 * 6.29842472076416
Epoch 800, val loss: 0.9472752213478088
Epoch 810, training loss: 315.0567321777344 = 0.4537876844406128 + 50.0 * 6.292058944702148
Epoch 810, val loss: 0.9455069899559021
Epoch 820, training loss: 314.976806640625 = 0.4419177770614624 + 50.0 * 6.2906975746154785
Epoch 820, val loss: 0.9438298344612122
Epoch 830, training loss: 315.3717956542969 = 0.4302380681037903 + 50.0 * 6.298830986022949
Epoch 830, val loss: 0.9420643448829651
Epoch 840, training loss: 315.0299072265625 = 0.4187760353088379 + 50.0 * 6.29222297668457
Epoch 840, val loss: 0.9415465593338013
Epoch 850, training loss: 314.8171081542969 = 0.4074456989765167 + 50.0 * 6.288193225860596
Epoch 850, val loss: 0.9403867125511169
Epoch 860, training loss: 314.7759704589844 = 0.39642295241355896 + 50.0 * 6.287590980529785
Epoch 860, val loss: 0.9399659037590027
Epoch 870, training loss: 314.81671142578125 = 0.3855458199977875 + 50.0 * 6.288623332977295
Epoch 870, val loss: 0.9392241835594177
Epoch 880, training loss: 314.7077331542969 = 0.37482979893684387 + 50.0 * 6.28665828704834
Epoch 880, val loss: 0.9388461112976074
Epoch 890, training loss: 314.6151123046875 = 0.3643888235092163 + 50.0 * 6.285014629364014
Epoch 890, val loss: 0.9385532140731812
Epoch 900, training loss: 314.49066162109375 = 0.35417962074279785 + 50.0 * 6.282729625701904
Epoch 900, val loss: 0.9387831687927246
Epoch 910, training loss: 314.47247314453125 = 0.3441946804523468 + 50.0 * 6.282565593719482
Epoch 910, val loss: 0.9390166997909546
Epoch 920, training loss: 314.6437683105469 = 0.3343849182128906 + 50.0 * 6.286187648773193
Epoch 920, val loss: 0.939834713935852
Epoch 930, training loss: 314.5291442871094 = 0.3247475028038025 + 50.0 * 6.284088134765625
Epoch 930, val loss: 0.9403246641159058
Epoch 940, training loss: 314.2936706542969 = 0.31536129117012024 + 50.0 * 6.279566287994385
Epoch 940, val loss: 0.941088855266571
Epoch 950, training loss: 314.2160949707031 = 0.3062605559825897 + 50.0 * 6.278196334838867
Epoch 950, val loss: 0.9423657059669495
Epoch 960, training loss: 314.60186767578125 = 0.29733434319496155 + 50.0 * 6.286090850830078
Epoch 960, val loss: 0.9433535933494568
Epoch 970, training loss: 314.3624267578125 = 0.28853628039360046 + 50.0 * 6.281477928161621
Epoch 970, val loss: 0.9448266625404358
Epoch 980, training loss: 314.0690612792969 = 0.27997875213623047 + 50.0 * 6.275782108306885
Epoch 980, val loss: 0.9466274380683899
Epoch 990, training loss: 314.0389404296875 = 0.2716505229473114 + 50.0 * 6.275345802307129
Epoch 990, val loss: 0.9485236406326294
Epoch 1000, training loss: 314.19976806640625 = 0.26354077458381653 + 50.0 * 6.278724193572998
Epoch 1000, val loss: 0.9506078362464905
Epoch 1010, training loss: 314.0692443847656 = 0.2555848956108093 + 50.0 * 6.276273250579834
Epoch 1010, val loss: 0.9522252082824707
Epoch 1020, training loss: 313.9372863769531 = 0.24781401455402374 + 50.0 * 6.273789405822754
Epoch 1020, val loss: 0.9545764923095703
Epoch 1030, training loss: 314.0636901855469 = 0.24029529094696045 + 50.0 * 6.276467800140381
Epoch 1030, val loss: 0.956635057926178
Epoch 1040, training loss: 313.97705078125 = 0.23291830718517303 + 50.0 * 6.274882793426514
Epoch 1040, val loss: 0.9592788219451904
Epoch 1050, training loss: 313.8242492675781 = 0.22573010623455048 + 50.0 * 6.271970272064209
Epoch 1050, val loss: 0.9612691402435303
Epoch 1060, training loss: 313.7212219238281 = 0.2188243269920349 + 50.0 * 6.270048141479492
Epoch 1060, val loss: 0.9643369317054749
Epoch 1070, training loss: 313.6661682128906 = 0.21210837364196777 + 50.0 * 6.2690815925598145
Epoch 1070, val loss: 0.9671160578727722
Epoch 1080, training loss: 314.1076965332031 = 0.20560021698474884 + 50.0 * 6.278041839599609
Epoch 1080, val loss: 0.9705820679664612
Epoch 1090, training loss: 313.96343994140625 = 0.19918721914291382 + 50.0 * 6.275285243988037
Epoch 1090, val loss: 0.9731687307357788
Epoch 1100, training loss: 313.5890197753906 = 0.19296970963478088 + 50.0 * 6.267921447753906
Epoch 1100, val loss: 0.9761213660240173
Epoch 1110, training loss: 313.5322265625 = 0.18702293932437897 + 50.0 * 6.266904354095459
Epoch 1110, val loss: 0.9795554876327515
Epoch 1120, training loss: 313.5113830566406 = 0.1812923401594162 + 50.0 * 6.2666015625
Epoch 1120, val loss: 0.9830801486968994
Epoch 1130, training loss: 313.6748046875 = 0.1757352203130722 + 50.0 * 6.269981384277344
Epoch 1130, val loss: 0.9865292906761169
Epoch 1140, training loss: 313.8470764160156 = 0.1703294813632965 + 50.0 * 6.273535251617432
Epoch 1140, val loss: 0.9907761216163635
Epoch 1150, training loss: 313.43707275390625 = 0.16505104303359985 + 50.0 * 6.265440464019775
Epoch 1150, val loss: 0.9936560988426208
Epoch 1160, training loss: 313.3631896972656 = 0.16002501547336578 + 50.0 * 6.264062881469727
Epoch 1160, val loss: 0.9976135492324829
Epoch 1170, training loss: 313.3177795410156 = 0.15519362688064575 + 50.0 * 6.263251781463623
Epoch 1170, val loss: 1.0014512538909912
Epoch 1180, training loss: 313.4541015625 = 0.15053457021713257 + 50.0 * 6.266071319580078
Epoch 1180, val loss: 1.005265712738037
Epoch 1190, training loss: 313.257568359375 = 0.1460190862417221 + 50.0 * 6.26223087310791
Epoch 1190, val loss: 1.0098366737365723
Epoch 1200, training loss: 313.3975830078125 = 0.14166420698165894 + 50.0 * 6.265118598937988
Epoch 1200, val loss: 1.0140340328216553
Epoch 1210, training loss: 313.20635986328125 = 0.13744041323661804 + 50.0 * 6.261378288269043
Epoch 1210, val loss: 1.0184351205825806
Epoch 1220, training loss: 313.2064208984375 = 0.13340727984905243 + 50.0 * 6.261460304260254
Epoch 1220, val loss: 1.0225764513015747
Epoch 1230, training loss: 313.3398742675781 = 0.12952309846878052 + 50.0 * 6.264206886291504
Epoch 1230, val loss: 1.0272003412246704
Epoch 1240, training loss: 313.2172546386719 = 0.12574893236160278 + 50.0 * 6.261829853057861
Epoch 1240, val loss: 1.0310965776443481
Epoch 1250, training loss: 313.1458740234375 = 0.1221391037106514 + 50.0 * 6.260475158691406
Epoch 1250, val loss: 1.0363129377365112
Epoch 1260, training loss: 313.08563232421875 = 0.11864231526851654 + 50.0 * 6.259339809417725
Epoch 1260, val loss: 1.0409260988235474
Epoch 1270, training loss: 313.284423828125 = 0.11528772860765457 + 50.0 * 6.263382434844971
Epoch 1270, val loss: 1.0460543632507324
Epoch 1280, training loss: 313.062255859375 = 0.11204611510038376 + 50.0 * 6.259003639221191
Epoch 1280, val loss: 1.050647497177124
Epoch 1290, training loss: 313.0044860839844 = 0.10891234129667282 + 50.0 * 6.257911682128906
Epoch 1290, val loss: 1.0557266473770142
Epoch 1300, training loss: 313.1465759277344 = 0.10592591762542725 + 50.0 * 6.260812759399414
Epoch 1300, val loss: 1.060941457748413
Epoch 1310, training loss: 313.04217529296875 = 0.10299607366323471 + 50.0 * 6.258783340454102
Epoch 1310, val loss: 1.0655856132507324
Epoch 1320, training loss: 312.9151611328125 = 0.10017147660255432 + 50.0 * 6.25629997253418
Epoch 1320, val loss: 1.0702649354934692
Epoch 1330, training loss: 313.1668395996094 = 0.09747989475727081 + 50.0 * 6.261387348175049
Epoch 1330, val loss: 1.0756168365478516
Epoch 1340, training loss: 312.85687255859375 = 0.09483559429645538 + 50.0 * 6.255240440368652
Epoch 1340, val loss: 1.0799392461776733
Epoch 1350, training loss: 312.7580261230469 = 0.0923011526465416 + 50.0 * 6.25331449508667
Epoch 1350, val loss: 1.0850123167037964
Epoch 1360, training loss: 312.7312927246094 = 0.08988111466169357 + 50.0 * 6.252828598022461
Epoch 1360, val loss: 1.0900256633758545
Epoch 1370, training loss: 312.90826416015625 = 0.08754794299602509 + 50.0 * 6.256414413452148
Epoch 1370, val loss: 1.094544529914856
Epoch 1380, training loss: 312.9533996582031 = 0.08525833487510681 + 50.0 * 6.2573628425598145
Epoch 1380, val loss: 1.1002581119537354
Epoch 1390, training loss: 312.8033752441406 = 0.08303625136613846 + 50.0 * 6.254406452178955
Epoch 1390, val loss: 1.1051830053329468
Epoch 1400, training loss: 312.7437438964844 = 0.0809057205915451 + 50.0 * 6.253256797790527
Epoch 1400, val loss: 1.1105186939239502
Epoch 1410, training loss: 312.9464416503906 = 0.0788506269454956 + 50.0 * 6.257351398468018
Epoch 1410, val loss: 1.1156619787216187
Epoch 1420, training loss: 312.71148681640625 = 0.07686205953359604 + 50.0 * 6.252692222595215
Epoch 1420, val loss: 1.1206482648849487
Epoch 1430, training loss: 312.6089172363281 = 0.07494118809700012 + 50.0 * 6.2506794929504395
Epoch 1430, val loss: 1.1260267496109009
Epoch 1440, training loss: 312.55511474609375 = 0.07309063524007797 + 50.0 * 6.249640464782715
Epoch 1440, val loss: 1.131036400794983
Epoch 1450, training loss: 313.08282470703125 = 0.07130715250968933 + 50.0 * 6.26023006439209
Epoch 1450, val loss: 1.1367460489273071
Epoch 1460, training loss: 312.6294860839844 = 0.06954375654459 + 50.0 * 6.251198768615723
Epoch 1460, val loss: 1.141295075416565
Epoch 1470, training loss: 312.4970703125 = 0.06784392893314362 + 50.0 * 6.248584747314453
Epoch 1470, val loss: 1.1464393138885498
Epoch 1480, training loss: 312.50732421875 = 0.0662311241030693 + 50.0 * 6.248822212219238
Epoch 1480, val loss: 1.1521333456039429
Epoch 1490, training loss: 312.8140869140625 = 0.06466436386108398 + 50.0 * 6.254988193511963
Epoch 1490, val loss: 1.1576220989227295
Epoch 1500, training loss: 312.6242370605469 = 0.06309982389211655 + 50.0 * 6.251222610473633
Epoch 1500, val loss: 1.1615673303604126
Epoch 1510, training loss: 312.50189208984375 = 0.06161879748106003 + 50.0 * 6.248805522918701
Epoch 1510, val loss: 1.167521595954895
Epoch 1520, training loss: 312.4125671386719 = 0.060175821185112 + 50.0 * 6.2470479011535645
Epoch 1520, val loss: 1.1718723773956299
Epoch 1530, training loss: 312.7953796386719 = 0.05879918485879898 + 50.0 * 6.254731178283691
Epoch 1530, val loss: 1.1775416135787964
Epoch 1540, training loss: 312.4454040527344 = 0.0574163943529129 + 50.0 * 6.24776029586792
Epoch 1540, val loss: 1.1821099519729614
Epoch 1550, training loss: 312.3247985839844 = 0.05610807240009308 + 50.0 * 6.245373249053955
Epoch 1550, val loss: 1.1875817775726318
Epoch 1560, training loss: 312.3207092285156 = 0.05484604835510254 + 50.0 * 6.245317459106445
Epoch 1560, val loss: 1.1924768686294556
Epoch 1570, training loss: 312.6719665527344 = 0.05363145098090172 + 50.0 * 6.25236701965332
Epoch 1570, val loss: 1.1981395483016968
Epoch 1580, training loss: 312.6039733886719 = 0.052411675453186035 + 50.0 * 6.251031398773193
Epoch 1580, val loss: 1.2016103267669678
Epoch 1590, training loss: 312.35565185546875 = 0.05123468115925789 + 50.0 * 6.246088027954102
Epoch 1590, val loss: 1.2075637578964233
Epoch 1600, training loss: 312.25146484375 = 0.050109583884477615 + 50.0 * 6.244027137756348
Epoch 1600, val loss: 1.212406873703003
Epoch 1610, training loss: 312.23089599609375 = 0.049025021493434906 + 50.0 * 6.243637561798096
Epoch 1610, val loss: 1.21742582321167
Epoch 1620, training loss: 312.2754821777344 = 0.04797886312007904 + 50.0 * 6.2445502281188965
Epoch 1620, val loss: 1.222598910331726
Epoch 1630, training loss: 312.28759765625 = 0.04694930464029312 + 50.0 * 6.244813442230225
Epoch 1630, val loss: 1.2275829315185547
Epoch 1640, training loss: 312.4778747558594 = 0.04595036804676056 + 50.0 * 6.248638153076172
Epoch 1640, val loss: 1.2329790592193604
Epoch 1650, training loss: 312.2283020019531 = 0.04496537894010544 + 50.0 * 6.243667125701904
Epoch 1650, val loss: 1.2371151447296143
Epoch 1660, training loss: 312.2930603027344 = 0.04401765018701553 + 50.0 * 6.244980812072754
Epoch 1660, val loss: 1.2417112588882446
Epoch 1670, training loss: 312.17498779296875 = 0.04310010001063347 + 50.0 * 6.242637634277344
Epoch 1670, val loss: 1.2469029426574707
Epoch 1680, training loss: 312.1703186035156 = 0.04220646992325783 + 50.0 * 6.242562294006348
Epoch 1680, val loss: 1.2520614862442017
Epoch 1690, training loss: 312.239501953125 = 0.04134484753012657 + 50.0 * 6.243963241577148
Epoch 1690, val loss: 1.2563316822052002
Epoch 1700, training loss: 312.0497741699219 = 0.04050038009881973 + 50.0 * 6.240185260772705
Epoch 1700, val loss: 1.2610424757003784
Epoch 1710, training loss: 312.10565185546875 = 0.039690520614385605 + 50.0 * 6.241319179534912
Epoch 1710, val loss: 1.2655344009399414
Epoch 1720, training loss: 312.11993408203125 = 0.038897253572940826 + 50.0 * 6.2416205406188965
Epoch 1720, val loss: 1.2700589895248413
Epoch 1730, training loss: 312.13507080078125 = 0.038121502846479416 + 50.0 * 6.241939067840576
Epoch 1730, val loss: 1.2751930952072144
Epoch 1740, training loss: 312.11407470703125 = 0.037373702973127365 + 50.0 * 6.2415337562561035
Epoch 1740, val loss: 1.2804445028305054
Epoch 1750, training loss: 312.0055236816406 = 0.03663667291402817 + 50.0 * 6.239377498626709
Epoch 1750, val loss: 1.284165859222412
Epoch 1760, training loss: 312.1841735839844 = 0.0359405055642128 + 50.0 * 6.242964267730713
Epoch 1760, val loss: 1.2886728048324585
Epoch 1770, training loss: 311.9758605957031 = 0.03523509204387665 + 50.0 * 6.23881196975708
Epoch 1770, val loss: 1.2938286066055298
Epoch 1780, training loss: 311.9340515136719 = 0.034555986523628235 + 50.0 * 6.237990379333496
Epoch 1780, val loss: 1.298153042793274
Epoch 1790, training loss: 311.9373474121094 = 0.033902090042829514 + 50.0 * 6.2380690574646
Epoch 1790, val loss: 1.3026912212371826
Epoch 1800, training loss: 312.0824279785156 = 0.03327629342675209 + 50.0 * 6.240983009338379
Epoch 1800, val loss: 1.307267665863037
Epoch 1810, training loss: 311.90435791015625 = 0.032648369669914246 + 50.0 * 6.237434387207031
Epoch 1810, val loss: 1.3120193481445312
Epoch 1820, training loss: 311.97125244140625 = 0.0320456363260746 + 50.0 * 6.238784313201904
Epoch 1820, val loss: 1.3161206245422363
Epoch 1830, training loss: 312.1327209472656 = 0.03146176040172577 + 50.0 * 6.242024898529053
Epoch 1830, val loss: 1.319855809211731
Epoch 1840, training loss: 311.96649169921875 = 0.030882421880960464 + 50.0 * 6.238712310791016
Epoch 1840, val loss: 1.325264573097229
Epoch 1850, training loss: 311.9327087402344 = 0.030320709571242332 + 50.0 * 6.2380475997924805
Epoch 1850, val loss: 1.3286242485046387
Epoch 1860, training loss: 312.0557861328125 = 0.029780041426420212 + 50.0 * 6.240520000457764
Epoch 1860, val loss: 1.3331133127212524
Epoch 1870, training loss: 311.818115234375 = 0.02924785017967224 + 50.0 * 6.235777854919434
Epoch 1870, val loss: 1.3379483222961426
Epoch 1880, training loss: 311.8184509277344 = 0.028736760839819908 + 50.0 * 6.2357940673828125
Epoch 1880, val loss: 1.3424673080444336
Epoch 1890, training loss: 311.9736633300781 = 0.028237856924533844 + 50.0 * 6.238908767700195
Epoch 1890, val loss: 1.346470832824707
Epoch 1900, training loss: 311.8063659667969 = 0.02774033136665821 + 50.0 * 6.235572338104248
Epoch 1900, val loss: 1.3499897718429565
Epoch 1910, training loss: 311.8438720703125 = 0.027263464406132698 + 50.0 * 6.236332416534424
Epoch 1910, val loss: 1.3543614149093628
Epoch 1920, training loss: 311.8586120605469 = 0.026798158884048462 + 50.0 * 6.236636638641357
Epoch 1920, val loss: 1.3586325645446777
Epoch 1930, training loss: 311.800537109375 = 0.02634538523852825 + 50.0 * 6.2354841232299805
Epoch 1930, val loss: 1.3634364604949951
Epoch 1940, training loss: 311.75946044921875 = 0.025899386033415794 + 50.0 * 6.234671115875244
Epoch 1940, val loss: 1.367467999458313
Epoch 1950, training loss: 311.81304931640625 = 0.025468220934271812 + 50.0 * 6.235751152038574
Epoch 1950, val loss: 1.3715132474899292
Epoch 1960, training loss: 311.8599853515625 = 0.025054410099983215 + 50.0 * 6.236698627471924
Epoch 1960, val loss: 1.3753641843795776
Epoch 1970, training loss: 311.71630859375 = 0.024628043174743652 + 50.0 * 6.2338337898254395
Epoch 1970, val loss: 1.379090428352356
Epoch 1980, training loss: 311.658447265625 = 0.024222686886787415 + 50.0 * 6.232684135437012
Epoch 1980, val loss: 1.3826793432235718
Epoch 1990, training loss: 311.6347351074219 = 0.023833254352211952 + 50.0 * 6.232218265533447
Epoch 1990, val loss: 1.3871978521347046
Epoch 2000, training loss: 311.69573974609375 = 0.023458367213606834 + 50.0 * 6.233445167541504
Epoch 2000, val loss: 1.39069664478302
Epoch 2010, training loss: 311.7164001464844 = 0.023087915033102036 + 50.0 * 6.233866214752197
Epoch 2010, val loss: 1.3947525024414062
Epoch 2020, training loss: 311.88385009765625 = 0.022725291550159454 + 50.0 * 6.237222194671631
Epoch 2020, val loss: 1.39832603931427
Epoch 2030, training loss: 311.6446533203125 = 0.022363632917404175 + 50.0 * 6.23244571685791
Epoch 2030, val loss: 1.403123140335083
Epoch 2040, training loss: 311.6942443847656 = 0.022014878690242767 + 50.0 * 6.233444690704346
Epoch 2040, val loss: 1.4065736532211304
Epoch 2050, training loss: 311.59466552734375 = 0.021676642820239067 + 50.0 * 6.231460094451904
Epoch 2050, val loss: 1.4104050397872925
Epoch 2060, training loss: 311.6018371582031 = 0.021345723420381546 + 50.0 * 6.23160982131958
Epoch 2060, val loss: 1.4138078689575195
Epoch 2070, training loss: 312.0308532714844 = 0.021024296060204506 + 50.0 * 6.240196228027344
Epoch 2070, val loss: 1.4173765182495117
Epoch 2080, training loss: 311.55450439453125 = 0.020696377381682396 + 50.0 * 6.230676174163818
Epoch 2080, val loss: 1.4220637083053589
Epoch 2090, training loss: 311.48577880859375 = 0.020385824143886566 + 50.0 * 6.229308128356934
Epoch 2090, val loss: 1.4256423711776733
Epoch 2100, training loss: 311.47796630859375 = 0.020088639110326767 + 50.0 * 6.229157447814941
Epoch 2100, val loss: 1.4292763471603394
Epoch 2110, training loss: 311.45855712890625 = 0.019800186157226562 + 50.0 * 6.2287750244140625
Epoch 2110, val loss: 1.4333875179290771
Epoch 2120, training loss: 311.7152099609375 = 0.019522525370121002 + 50.0 * 6.233913421630859
Epoch 2120, val loss: 1.4377634525299072
Epoch 2130, training loss: 311.60028076171875 = 0.019229333847761154 + 50.0 * 6.231621265411377
Epoch 2130, val loss: 1.439440131187439
Epoch 2140, training loss: 311.58343505859375 = 0.018946602940559387 + 50.0 * 6.231289386749268
Epoch 2140, val loss: 1.4447129964828491
Epoch 2150, training loss: 311.50372314453125 = 0.018676050007343292 + 50.0 * 6.229701042175293
Epoch 2150, val loss: 1.4474103450775146
Epoch 2160, training loss: 311.43170166015625 = 0.01841508224606514 + 50.0 * 6.228265762329102
Epoch 2160, val loss: 1.4515769481658936
Epoch 2170, training loss: 311.5676574707031 = 0.01816345378756523 + 50.0 * 6.230989933013916
Epoch 2170, val loss: 1.4546983242034912
Epoch 2180, training loss: 311.43133544921875 = 0.017906395718455315 + 50.0 * 6.228268623352051
Epoch 2180, val loss: 1.4587249755859375
Epoch 2190, training loss: 311.441650390625 = 0.0176562387496233 + 50.0 * 6.228479385375977
Epoch 2190, val loss: 1.4622310400009155
Epoch 2200, training loss: 311.5621643066406 = 0.017412422224879265 + 50.0 * 6.230895042419434
Epoch 2200, val loss: 1.4651238918304443
Epoch 2210, training loss: 311.42938232421875 = 0.017172854393720627 + 50.0 * 6.228244304656982
Epoch 2210, val loss: 1.4687142372131348
Epoch 2220, training loss: 311.4012756347656 = 0.016940157860517502 + 50.0 * 6.227686882019043
Epoch 2220, val loss: 1.4722609519958496
Epoch 2230, training loss: 311.49530029296875 = 0.016717301681637764 + 50.0 * 6.229571342468262
Epoch 2230, val loss: 1.475492000579834
Epoch 2240, training loss: 311.5281677246094 = 0.016492612659931183 + 50.0 * 6.230233669281006
Epoch 2240, val loss: 1.4788941144943237
Epoch 2250, training loss: 311.3622741699219 = 0.016271863132715225 + 50.0 * 6.226920127868652
Epoch 2250, val loss: 1.482839822769165
Epoch 2260, training loss: 311.33856201171875 = 0.01606309600174427 + 50.0 * 6.226450443267822
Epoch 2260, val loss: 1.4862785339355469
Epoch 2270, training loss: 311.32611083984375 = 0.01585799641907215 + 50.0 * 6.226204872131348
Epoch 2270, val loss: 1.4896526336669922
Epoch 2280, training loss: 311.7673645019531 = 0.015667948871850967 + 50.0 * 6.235033988952637
Epoch 2280, val loss: 1.4928245544433594
Epoch 2290, training loss: 311.5268249511719 = 0.015450696460902691 + 50.0 * 6.230227947235107
Epoch 2290, val loss: 1.495768427848816
Epoch 2300, training loss: 311.3808898925781 = 0.015253815799951553 + 50.0 * 6.2273125648498535
Epoch 2300, val loss: 1.499173879623413
Epoch 2310, training loss: 311.311767578125 = 0.015060722827911377 + 50.0 * 6.22593355178833
Epoch 2310, val loss: 1.5024282932281494
Epoch 2320, training loss: 311.3144836425781 = 0.014875129796564579 + 50.0 * 6.225992679595947
Epoch 2320, val loss: 1.505568265914917
Epoch 2330, training loss: 311.3465270996094 = 0.014693235047161579 + 50.0 * 6.22663688659668
Epoch 2330, val loss: 1.5090864896774292
Epoch 2340, training loss: 311.3836975097656 = 0.014513587579131126 + 50.0 * 6.227383136749268
Epoch 2340, val loss: 1.5124791860580444
Epoch 2350, training loss: 311.5304870605469 = 0.014340026304125786 + 50.0 * 6.23032283782959
Epoch 2350, val loss: 1.5162606239318848
Epoch 2360, training loss: 311.58331298828125 = 0.014158624224364758 + 50.0 * 6.231383323669434
Epoch 2360, val loss: 1.5190284252166748
Epoch 2370, training loss: 311.24591064453125 = 0.013980753719806671 + 50.0 * 6.224638938903809
Epoch 2370, val loss: 1.5214847326278687
Epoch 2380, training loss: 311.2025146484375 = 0.013815420679748058 + 50.0 * 6.223773956298828
Epoch 2380, val loss: 1.5246342420578003
Epoch 2390, training loss: 311.1992492675781 = 0.01365505438297987 + 50.0 * 6.223711967468262
Epoch 2390, val loss: 1.5280020236968994
Epoch 2400, training loss: 311.3748474121094 = 0.013499845750629902 + 50.0 * 6.227227210998535
Epoch 2400, val loss: 1.531080961227417
Epoch 2410, training loss: 311.40374755859375 = 0.013338495045900345 + 50.0 * 6.227808475494385
Epoch 2410, val loss: 1.5330290794372559
Epoch 2420, training loss: 311.3179016113281 = 0.013180086389183998 + 50.0 * 6.2260942459106445
Epoch 2420, val loss: 1.5376375913619995
Epoch 2430, training loss: 311.1811828613281 = 0.013025542721152306 + 50.0 * 6.223362922668457
Epoch 2430, val loss: 1.5400800704956055
Epoch 2440, training loss: 311.171142578125 = 0.012878376059234142 + 50.0 * 6.223165512084961
Epoch 2440, val loss: 1.5428495407104492
Epoch 2450, training loss: 311.1729431152344 = 0.012736144475638866 + 50.0 * 6.223204135894775
Epoch 2450, val loss: 1.5458332300186157
Epoch 2460, training loss: 311.3203430175781 = 0.012597580440342426 + 50.0 * 6.226154804229736
Epoch 2460, val loss: 1.5491211414337158
Epoch 2470, training loss: 311.3565673828125 = 0.012454744428396225 + 50.0 * 6.226882457733154
Epoch 2470, val loss: 1.5522911548614502
Epoch 2480, training loss: 311.1310119628906 = 0.01230848953127861 + 50.0 * 6.222373962402344
Epoch 2480, val loss: 1.554664134979248
Epoch 2490, training loss: 311.1368408203125 = 0.012171868234872818 + 50.0 * 6.2224931716918945
Epoch 2490, val loss: 1.557227611541748
Epoch 2500, training loss: 311.18878173828125 = 0.012040393427014351 + 50.0 * 6.22353458404541
Epoch 2500, val loss: 1.5603818893432617
Epoch 2510, training loss: 311.1972961425781 = 0.01191036682575941 + 50.0 * 6.223707675933838
Epoch 2510, val loss: 1.5630028247833252
Epoch 2520, training loss: 311.07537841796875 = 0.011782861314713955 + 50.0 * 6.221271991729736
Epoch 2520, val loss: 1.5668455362319946
Epoch 2530, training loss: 311.337646484375 = 0.011664076708257198 + 50.0 * 6.226519584655762
Epoch 2530, val loss: 1.5701062679290771
Epoch 2540, training loss: 311.0765380859375 = 0.011532886885106564 + 50.0 * 6.22130012512207
Epoch 2540, val loss: 1.5717657804489136
Epoch 2550, training loss: 311.20440673828125 = 0.011412480846047401 + 50.0 * 6.223859786987305
Epoch 2550, val loss: 1.574620008468628
Epoch 2560, training loss: 311.14263916015625 = 0.011290213093161583 + 50.0 * 6.222626686096191
Epoch 2560, val loss: 1.5772656202316284
Epoch 2570, training loss: 311.12823486328125 = 0.011173427104949951 + 50.0 * 6.222341537475586
Epoch 2570, val loss: 1.5807342529296875
Epoch 2580, training loss: 311.0781555175781 = 0.011057829484343529 + 50.0 * 6.221342086791992
Epoch 2580, val loss: 1.582757830619812
Epoch 2590, training loss: 311.12738037109375 = 0.010949353687465191 + 50.0 * 6.2223286628723145
Epoch 2590, val loss: 1.5857237577438354
Epoch 2600, training loss: 311.1579895019531 = 0.010837169364094734 + 50.0 * 6.222943305969238
Epoch 2600, val loss: 1.5881582498550415
Epoch 2610, training loss: 311.2140197753906 = 0.010728909634053707 + 50.0 * 6.22406530380249
Epoch 2610, val loss: 1.5925986766815186
Epoch 2620, training loss: 311.1141662597656 = 0.010617498308420181 + 50.0 * 6.222070693969727
Epoch 2620, val loss: 1.5938489437103271
Epoch 2630, training loss: 311.0749206542969 = 0.010510578751564026 + 50.0 * 6.221288681030273
Epoch 2630, val loss: 1.5969033241271973
Epoch 2640, training loss: 311.201416015625 = 0.010407558642327785 + 50.0 * 6.223820209503174
Epoch 2640, val loss: 1.599014163017273
Epoch 2650, training loss: 311.06280517578125 = 0.010306527838110924 + 50.0 * 6.221049785614014
Epoch 2650, val loss: 1.6023468971252441
Epoch 2660, training loss: 311.1048278808594 = 0.01020675990730524 + 50.0 * 6.221892833709717
Epoch 2660, val loss: 1.6051721572875977
Epoch 2670, training loss: 311.0171203613281 = 0.010106476955115795 + 50.0 * 6.22014045715332
Epoch 2670, val loss: 1.6078764200210571
Epoch 2680, training loss: 310.9996032714844 = 0.010011865757405758 + 50.0 * 6.219791889190674
Epoch 2680, val loss: 1.6103858947753906
Epoch 2690, training loss: 311.0804138183594 = 0.00991891697049141 + 50.0 * 6.221409797668457
Epoch 2690, val loss: 1.6131751537322998
Epoch 2700, training loss: 311.0609130859375 = 0.009824058972299099 + 50.0 * 6.22102165222168
Epoch 2700, val loss: 1.6158782243728638
Epoch 2710, training loss: 311.0946960449219 = 0.009732764214277267 + 50.0 * 6.221699237823486
Epoch 2710, val loss: 1.61766517162323
Epoch 2720, training loss: 311.1070861816406 = 0.009641696698963642 + 50.0 * 6.221949100494385
Epoch 2720, val loss: 1.6204721927642822
Epoch 2730, training loss: 311.18994140625 = 0.009550531394779682 + 50.0 * 6.223608016967773
Epoch 2730, val loss: 1.6227235794067383
Epoch 2740, training loss: 311.01763916015625 = 0.009459207765758038 + 50.0 * 6.220163345336914
Epoch 2740, val loss: 1.625896692276001
Epoch 2750, training loss: 310.9420166015625 = 0.009373700246214867 + 50.0 * 6.218653202056885
Epoch 2750, val loss: 1.6284847259521484
Epoch 2760, training loss: 310.92547607421875 = 0.009289707988500595 + 50.0 * 6.218324184417725
Epoch 2760, val loss: 1.631083607673645
Epoch 2770, training loss: 310.94207763671875 = 0.009210036136209965 + 50.0 * 6.218657493591309
Epoch 2770, val loss: 1.6335917711257935
Epoch 2780, training loss: 311.2632751464844 = 0.009130935184657574 + 50.0 * 6.225082874298096
Epoch 2780, val loss: 1.636267066001892
Epoch 2790, training loss: 311.21575927734375 = 0.009043260477483273 + 50.0 * 6.22413444519043
Epoch 2790, val loss: 1.6372143030166626
Epoch 2800, training loss: 310.95068359375 = 0.008962206542491913 + 50.0 * 6.218834400177002
Epoch 2800, val loss: 1.6414673328399658
Epoch 2810, training loss: 310.895751953125 = 0.008883149363100529 + 50.0 * 6.217737197875977
Epoch 2810, val loss: 1.6432440280914307
Epoch 2820, training loss: 310.9049072265625 = 0.008809772320091724 + 50.0 * 6.217921733856201
Epoch 2820, val loss: 1.6462244987487793
Epoch 2830, training loss: 311.0916748046875 = 0.008736716583371162 + 50.0 * 6.221658706665039
Epoch 2830, val loss: 1.6482423543930054
Epoch 2840, training loss: 311.1717529296875 = 0.008658205159008503 + 50.0 * 6.223261833190918
Epoch 2840, val loss: 1.6501739025115967
Epoch 2850, training loss: 310.9719543457031 = 0.008582009002566338 + 50.0 * 6.219267845153809
Epoch 2850, val loss: 1.6525975465774536
Epoch 2860, training loss: 310.8680725097656 = 0.008508755825459957 + 50.0 * 6.217191219329834
Epoch 2860, val loss: 1.654882788658142
Epoch 2870, training loss: 310.9322509765625 = 0.008438429795205593 + 50.0 * 6.218475818634033
Epoch 2870, val loss: 1.6567970514297485
Epoch 2880, training loss: 310.86285400390625 = 0.008368379436433315 + 50.0 * 6.217090129852295
Epoch 2880, val loss: 1.659743309020996
Epoch 2890, training loss: 310.9029846191406 = 0.008302021771669388 + 50.0 * 6.217893600463867
Epoch 2890, val loss: 1.6621372699737549
Epoch 2900, training loss: 311.09210205078125 = 0.008233555592596531 + 50.0 * 6.221677303314209
Epoch 2900, val loss: 1.6639798879623413
Epoch 2910, training loss: 310.957275390625 = 0.008161721751093864 + 50.0 * 6.218982696533203
Epoch 2910, val loss: 1.6661548614501953
Epoch 2920, training loss: 310.8074951171875 = 0.008094104938209057 + 50.0 * 6.2159881591796875
Epoch 2920, val loss: 1.6694056987762451
Epoch 2930, training loss: 310.8740539550781 = 0.008031636476516724 + 50.0 * 6.217320442199707
Epoch 2930, val loss: 1.6717593669891357
Epoch 2940, training loss: 310.905517578125 = 0.00796808023005724 + 50.0 * 6.217950820922852
Epoch 2940, val loss: 1.6742236614227295
Epoch 2950, training loss: 310.83221435546875 = 0.007904134690761566 + 50.0 * 6.21648645401001
Epoch 2950, val loss: 1.6753840446472168
Epoch 2960, training loss: 310.8664245605469 = 0.00784320943057537 + 50.0 * 6.217171669006348
Epoch 2960, val loss: 1.6778390407562256
Epoch 2970, training loss: 310.9163513183594 = 0.007782855071127415 + 50.0 * 6.2181715965271
Epoch 2970, val loss: 1.6807955503463745
Epoch 2980, training loss: 311.0442810058594 = 0.0077238744124770164 + 50.0 * 6.220731258392334
Epoch 2980, val loss: 1.682665467262268
Epoch 2990, training loss: 310.75457763671875 = 0.007657622918486595 + 50.0 * 6.214938163757324
Epoch 2990, val loss: 1.6845566034317017
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6444444444444445
0.816025303110174
=== training gcn model ===
Epoch 0, training loss: 431.8057556152344 = 1.9626063108444214 + 50.0 * 8.59686279296875
Epoch 0, val loss: 1.964605689048767
Epoch 10, training loss: 431.7671203613281 = 1.9527802467346191 + 50.0 * 8.59628677368164
Epoch 10, val loss: 1.9541047811508179
Epoch 20, training loss: 431.54547119140625 = 1.9405211210250854 + 50.0 * 8.5920991897583
Epoch 20, val loss: 1.940684199333191
Epoch 30, training loss: 429.9936218261719 = 1.9242140054702759 + 50.0 * 8.56138801574707
Epoch 30, val loss: 1.9225823879241943
Epoch 40, training loss: 421.11376953125 = 1.9035786390304565 + 50.0 * 8.384203910827637
Epoch 40, val loss: 1.900171160697937
Epoch 50, training loss: 396.7677307128906 = 1.8817187547683716 + 50.0 * 7.8977203369140625
Epoch 50, val loss: 1.8776015043258667
Epoch 60, training loss: 368.15020751953125 = 1.8677648305892944 + 50.0 * 7.325648784637451
Epoch 60, val loss: 1.8645691871643066
Epoch 70, training loss: 351.1373291015625 = 1.8553247451782227 + 50.0 * 6.985640048980713
Epoch 70, val loss: 1.852234959602356
Epoch 80, training loss: 345.1288146972656 = 1.8437901735305786 + 50.0 * 6.865700721740723
Epoch 80, val loss: 1.840506672859192
Epoch 90, training loss: 341.4386901855469 = 1.831499695777893 + 50.0 * 6.79214334487915
Epoch 90, val loss: 1.8283294439315796
Epoch 100, training loss: 338.2622375488281 = 1.8197449445724487 + 50.0 * 6.728850364685059
Epoch 100, val loss: 1.8168354034423828
Epoch 110, training loss: 335.8272705078125 = 1.8092421293258667 + 50.0 * 6.680360794067383
Epoch 110, val loss: 1.8065872192382812
Epoch 120, training loss: 333.8899841308594 = 1.7996139526367188 + 50.0 * 6.6418070793151855
Epoch 120, val loss: 1.7972193956375122
Epoch 130, training loss: 332.292236328125 = 1.7904130220413208 + 50.0 * 6.610036849975586
Epoch 130, val loss: 1.788310170173645
Epoch 140, training loss: 331.0323486328125 = 1.781125783920288 + 50.0 * 6.585024356842041
Epoch 140, val loss: 1.7795430421829224
Epoch 150, training loss: 329.85028076171875 = 1.7714359760284424 + 50.0 * 6.561576843261719
Epoch 150, val loss: 1.7706797122955322
Epoch 160, training loss: 328.99578857421875 = 1.7611950635910034 + 50.0 * 6.544691562652588
Epoch 160, val loss: 1.7614665031433105
Epoch 170, training loss: 327.97967529296875 = 1.7502048015594482 + 50.0 * 6.5245890617370605
Epoch 170, val loss: 1.7518665790557861
Epoch 180, training loss: 327.1110534667969 = 1.7384774684906006 + 50.0 * 6.50745153427124
Epoch 180, val loss: 1.7416932582855225
Epoch 190, training loss: 326.61566162109375 = 1.7258038520812988 + 50.0 * 6.497797012329102
Epoch 190, val loss: 1.7307727336883545
Epoch 200, training loss: 325.7690734863281 = 1.7119858264923096 + 50.0 * 6.481142044067383
Epoch 200, val loss: 1.7190254926681519
Epoch 210, training loss: 325.11383056640625 = 1.6970208883285522 + 50.0 * 6.46833610534668
Epoch 210, val loss: 1.706344723701477
Epoch 220, training loss: 324.7455749511719 = 1.6808240413665771 + 50.0 * 6.461295127868652
Epoch 220, val loss: 1.692682147026062
Epoch 230, training loss: 324.1953430175781 = 1.663217306137085 + 50.0 * 6.4506425857543945
Epoch 230, val loss: 1.6778846979141235
Epoch 240, training loss: 323.58984375 = 1.6443116664886475 + 50.0 * 6.438910484313965
Epoch 240, val loss: 1.661976933479309
Epoch 250, training loss: 323.1682434082031 = 1.6241521835327148 + 50.0 * 6.430881977081299
Epoch 250, val loss: 1.645218849182129
Epoch 260, training loss: 322.9604797363281 = 1.6026808023452759 + 50.0 * 6.4271559715271
Epoch 260, val loss: 1.6274577379226685
Epoch 270, training loss: 322.4439697265625 = 1.5800217390060425 + 50.0 * 6.417279243469238
Epoch 270, val loss: 1.6087676286697388
Epoch 280, training loss: 322.0541076660156 = 1.5562664270401 + 50.0 * 6.409956932067871
Epoch 280, val loss: 1.589353322982788
Epoch 290, training loss: 321.7702941894531 = 1.5316017866134644 + 50.0 * 6.404773712158203
Epoch 290, val loss: 1.569337010383606
Epoch 300, training loss: 321.6519470214844 = 1.5059653520584106 + 50.0 * 6.402919769287109
Epoch 300, val loss: 1.5489020347595215
Epoch 310, training loss: 321.16131591796875 = 1.4796595573425293 + 50.0 * 6.3936333656311035
Epoch 310, val loss: 1.5280059576034546
Epoch 320, training loss: 320.86651611328125 = 1.4528201818466187 + 50.0 * 6.3882737159729
Epoch 320, val loss: 1.5069611072540283
Epoch 330, training loss: 320.6627197265625 = 1.4255975484848022 + 50.0 * 6.384742259979248
Epoch 330, val loss: 1.4859918355941772
Epoch 340, training loss: 320.5713806152344 = 1.397882342338562 + 50.0 * 6.383470058441162
Epoch 340, val loss: 1.4647353887557983
Epoch 350, training loss: 320.1953430175781 = 1.3700380325317383 + 50.0 * 6.376506328582764
Epoch 350, val loss: 1.44384765625
Epoch 360, training loss: 319.929931640625 = 1.3420964479446411 + 50.0 * 6.371756553649902
Epoch 360, val loss: 1.423133373260498
Epoch 370, training loss: 319.8648376464844 = 1.3141127824783325 + 50.0 * 6.37101411819458
Epoch 370, val loss: 1.4027220010757446
Epoch 380, training loss: 319.6129150390625 = 1.2859643697738647 + 50.0 * 6.366539478302002
Epoch 380, val loss: 1.3825165033340454
Epoch 390, training loss: 319.3165283203125 = 1.257856011390686 + 50.0 * 6.361173629760742
Epoch 390, val loss: 1.3627924919128418
Epoch 400, training loss: 319.11566162109375 = 1.2299257516860962 + 50.0 * 6.357714653015137
Epoch 400, val loss: 1.3434375524520874
Epoch 410, training loss: 319.4560852050781 = 1.2021840810775757 + 50.0 * 6.365077972412109
Epoch 410, val loss: 1.3246700763702393
Epoch 420, training loss: 318.8238525390625 = 1.1743088960647583 + 50.0 * 6.352990627288818
Epoch 420, val loss: 1.3059576749801636
Epoch 430, training loss: 318.62060546875 = 1.146846055984497 + 50.0 * 6.349474906921387
Epoch 430, val loss: 1.287721037864685
Epoch 440, training loss: 318.4167175292969 = 1.119735598564148 + 50.0 * 6.345939636230469
Epoch 440, val loss: 1.2701153755187988
Epoch 450, training loss: 318.2635803222656 = 1.0930095911026 + 50.0 * 6.343411445617676
Epoch 450, val loss: 1.25310218334198
Epoch 460, training loss: 318.1935729980469 = 1.0666426420211792 + 50.0 * 6.342538356781006
Epoch 460, val loss: 1.2363871335983276
Epoch 470, training loss: 317.9743347167969 = 1.0406444072723389 + 50.0 * 6.3386735916137695
Epoch 470, val loss: 1.2203837633132935
Epoch 480, training loss: 317.8231506347656 = 1.0153528451919556 + 50.0 * 6.336155891418457
Epoch 480, val loss: 1.205095648765564
Epoch 490, training loss: 317.6504821777344 = 0.99072265625 + 50.0 * 6.333195209503174
Epoch 490, val loss: 1.1905124187469482
Epoch 500, training loss: 317.5621337890625 = 0.9667119383811951 + 50.0 * 6.331908702850342
Epoch 500, val loss: 1.1767451763153076
Epoch 510, training loss: 317.7383117675781 = 0.943245530128479 + 50.0 * 6.335901260375977
Epoch 510, val loss: 1.1633886098861694
Epoch 520, training loss: 317.3219299316406 = 0.9203992486000061 + 50.0 * 6.328030586242676
Epoch 520, val loss: 1.1508104801177979
Epoch 530, training loss: 317.1164855957031 = 0.898297905921936 + 50.0 * 6.324363708496094
Epoch 530, val loss: 1.1392115354537964
Epoch 540, training loss: 317.0003662109375 = 0.8769101500511169 + 50.0 * 6.3224687576293945
Epoch 540, val loss: 1.128330945968628
Epoch 550, training loss: 317.2595520019531 = 0.8561016917228699 + 50.0 * 6.328068733215332
Epoch 550, val loss: 1.1180205345153809
Epoch 560, training loss: 316.8033752441406 = 0.8359006643295288 + 50.0 * 6.31934928894043
Epoch 560, val loss: 1.10860013961792
Epoch 570, training loss: 316.6477355957031 = 0.8162452578544617 + 50.0 * 6.316629886627197
Epoch 570, val loss: 1.099666714668274
Epoch 580, training loss: 316.57220458984375 = 0.7971929907798767 + 50.0 * 6.315500259399414
Epoch 580, val loss: 1.0910909175872803
Epoch 590, training loss: 316.4807434082031 = 0.7786357402801514 + 50.0 * 6.314042091369629
Epoch 590, val loss: 1.0836961269378662
Epoch 600, training loss: 316.3863220214844 = 0.7605204582214355 + 50.0 * 6.312515735626221
Epoch 600, val loss: 1.0762361288070679
Epoch 610, training loss: 316.328369140625 = 0.7429647445678711 + 50.0 * 6.311708450317383
Epoch 610, val loss: 1.0699005126953125
Epoch 620, training loss: 316.21954345703125 = 0.7258379459381104 + 50.0 * 6.309874057769775
Epoch 620, val loss: 1.064063549041748
Epoch 630, training loss: 316.5439147949219 = 0.7092161178588867 + 50.0 * 6.3166937828063965
Epoch 630, val loss: 1.0583761930465698
Epoch 640, training loss: 316.0354919433594 = 0.6927406787872314 + 50.0 * 6.306854724884033
Epoch 640, val loss: 1.0531022548675537
Epoch 650, training loss: 315.8482666015625 = 0.6768585443496704 + 50.0 * 6.3034281730651855
Epoch 650, val loss: 1.0486984252929688
Epoch 660, training loss: 315.7691955566406 = 0.6614056825637817 + 50.0 * 6.3021559715271
Epoch 660, val loss: 1.0450842380523682
Epoch 670, training loss: 315.6943359375 = 0.6463361382484436 + 50.0 * 6.300960063934326
Epoch 670, val loss: 1.0414457321166992
Epoch 680, training loss: 315.8583679199219 = 0.6315532922744751 + 50.0 * 6.304535865783691
Epoch 680, val loss: 1.038397192955017
Epoch 690, training loss: 315.69451904296875 = 0.6170470714569092 + 50.0 * 6.301549911499023
Epoch 690, val loss: 1.0358598232269287
Epoch 700, training loss: 315.498046875 = 0.6030042171478271 + 50.0 * 6.297901153564453
Epoch 700, val loss: 1.0338244438171387
Epoch 710, training loss: 315.3465270996094 = 0.5893848538398743 + 50.0 * 6.295142650604248
Epoch 710, val loss: 1.0324374437332153
Epoch 720, training loss: 315.3982849121094 = 0.5761135816574097 + 50.0 * 6.296443462371826
Epoch 720, val loss: 1.0314459800720215
Epoch 730, training loss: 315.31866455078125 = 0.5630801320075989 + 50.0 * 6.295111656188965
Epoch 730, val loss: 1.0309418439865112
Epoch 740, training loss: 315.2121276855469 = 0.5503612756729126 + 50.0 * 6.2932353019714355
Epoch 740, val loss: 1.0305688381195068
Epoch 750, training loss: 315.130859375 = 0.5380520820617676 + 50.0 * 6.291856288909912
Epoch 750, val loss: 1.030984878540039
Epoch 760, training loss: 315.0547790527344 = 0.5260291695594788 + 50.0 * 6.29057502746582
Epoch 760, val loss: 1.0317480564117432
Epoch 770, training loss: 314.962646484375 = 0.514301061630249 + 50.0 * 6.288966655731201
Epoch 770, val loss: 1.0325764417648315
Epoch 780, training loss: 314.8718566894531 = 0.5029400587081909 + 50.0 * 6.287378311157227
Epoch 780, val loss: 1.0342488288879395
Epoch 790, training loss: 315.0410461425781 = 0.4919290840625763 + 50.0 * 6.290981769561768
Epoch 790, val loss: 1.035964846611023
Epoch 800, training loss: 314.90093994140625 = 0.48112350702285767 + 50.0 * 6.288396835327148
Epoch 800, val loss: 1.0385825634002686
Epoch 810, training loss: 314.8238830566406 = 0.47064241766929626 + 50.0 * 6.287065029144287
Epoch 810, val loss: 1.0408577919006348
Epoch 820, training loss: 314.67852783203125 = 0.4604364335536957 + 50.0 * 6.284361362457275
Epoch 820, val loss: 1.044104814529419
Epoch 830, training loss: 314.5767822265625 = 0.4505140781402588 + 50.0 * 6.282525539398193
Epoch 830, val loss: 1.047389268875122
Epoch 840, training loss: 314.4841613769531 = 0.44092506170272827 + 50.0 * 6.280864715576172
Epoch 840, val loss: 1.0510845184326172
Epoch 850, training loss: 314.4513854980469 = 0.4315909743309021 + 50.0 * 6.280395984649658
Epoch 850, val loss: 1.0551751852035522
Epoch 860, training loss: 314.49908447265625 = 0.42242932319641113 + 50.0 * 6.281533241271973
Epoch 860, val loss: 1.0591150522232056
Epoch 870, training loss: 314.51214599609375 = 0.4134557545185089 + 50.0 * 6.281973838806152
Epoch 870, val loss: 1.0637004375457764
Epoch 880, training loss: 314.2949523925781 = 0.4047166407108307 + 50.0 * 6.277804374694824
Epoch 880, val loss: 1.068114161491394
Epoch 890, training loss: 314.24090576171875 = 0.39627501368522644 + 50.0 * 6.27689266204834
Epoch 890, val loss: 1.0730221271514893
Epoch 900, training loss: 314.35009765625 = 0.3880137503147125 + 50.0 * 6.279242038726807
Epoch 900, val loss: 1.0781714916229248
Epoch 910, training loss: 314.1709899902344 = 0.3799252510070801 + 50.0 * 6.275821685791016
Epoch 910, val loss: 1.083420991897583
Epoch 920, training loss: 314.2061462402344 = 0.3720191419124603 + 50.0 * 6.2766828536987305
Epoch 920, val loss: 1.0889787673950195
Epoch 930, training loss: 314.1407775878906 = 0.36425331234931946 + 50.0 * 6.2755303382873535
Epoch 930, val loss: 1.0944490432739258
Epoch 940, training loss: 314.05535888671875 = 0.35667508840560913 + 50.0 * 6.27397346496582
Epoch 940, val loss: 1.1005158424377441
Epoch 950, training loss: 313.96929931640625 = 0.34927040338516235 + 50.0 * 6.272400379180908
Epoch 950, val loss: 1.1063541173934937
Epoch 960, training loss: 313.895263671875 = 0.3420364260673523 + 50.0 * 6.271064281463623
Epoch 960, val loss: 1.1127623319625854
Epoch 970, training loss: 313.93280029296875 = 0.33494260907173157 + 50.0 * 6.2719573974609375
Epoch 970, val loss: 1.1189604997634888
Epoch 980, training loss: 314.01177978515625 = 0.3279392719268799 + 50.0 * 6.273676872253418
Epoch 980, val loss: 1.12545907497406
Epoch 990, training loss: 313.8885498046875 = 0.3210563659667969 + 50.0 * 6.271349906921387
Epoch 990, val loss: 1.1323046684265137
Epoch 1000, training loss: 313.7840270996094 = 0.3143247961997986 + 50.0 * 6.2693939208984375
Epoch 1000, val loss: 1.138940691947937
Epoch 1010, training loss: 313.67242431640625 = 0.30772948265075684 + 50.0 * 6.267293930053711
Epoch 1010, val loss: 1.145933747291565
Epoch 1020, training loss: 313.553955078125 = 0.3012738525867462 + 50.0 * 6.265053749084473
Epoch 1020, val loss: 1.1529728174209595
Epoch 1030, training loss: 313.5879211425781 = 0.29496896266937256 + 50.0 * 6.2658586502075195
Epoch 1030, val loss: 1.1601252555847168
Epoch 1040, training loss: 313.69781494140625 = 0.2887575924396515 + 50.0 * 6.268180847167969
Epoch 1040, val loss: 1.1675692796707153
Epoch 1050, training loss: 313.6661682128906 = 0.2826235592365265 + 50.0 * 6.26767110824585
Epoch 1050, val loss: 1.1749203205108643
Epoch 1060, training loss: 313.4868469238281 = 0.27657926082611084 + 50.0 * 6.264205455780029
Epoch 1060, val loss: 1.1825892925262451
Epoch 1070, training loss: 313.39111328125 = 0.27070286870002747 + 50.0 * 6.26240873336792
Epoch 1070, val loss: 1.190306544303894
Epoch 1080, training loss: 313.3222961425781 = 0.2649748623371124 + 50.0 * 6.261146068572998
Epoch 1080, val loss: 1.1983469724655151
Epoch 1090, training loss: 313.5296936035156 = 0.2593381106853485 + 50.0 * 6.265407085418701
Epoch 1090, val loss: 1.2062532901763916
Epoch 1100, training loss: 313.4552001953125 = 0.2537152171134949 + 50.0 * 6.264029502868652
Epoch 1100, val loss: 1.214087963104248
Epoch 1110, training loss: 313.324951171875 = 0.24822574853897095 + 50.0 * 6.261534690856934
Epoch 1110, val loss: 1.222337245941162
Epoch 1120, training loss: 313.2210998535156 = 0.24286508560180664 + 50.0 * 6.2595648765563965
Epoch 1120, val loss: 1.2306514978408813
Epoch 1130, training loss: 313.41497802734375 = 0.23763343691825867 + 50.0 * 6.263546466827393
Epoch 1130, val loss: 1.238664150238037
Epoch 1140, training loss: 313.19830322265625 = 0.23242679238319397 + 50.0 * 6.259317398071289
Epoch 1140, val loss: 1.2474374771118164
Epoch 1150, training loss: 313.1607666015625 = 0.22736519575119019 + 50.0 * 6.258668422698975
Epoch 1150, val loss: 1.2556787729263306
Epoch 1160, training loss: 313.13287353515625 = 0.2224101424217224 + 50.0 * 6.258209228515625
Epoch 1160, val loss: 1.2643661499023438
Epoch 1170, training loss: 313.1034240722656 = 0.2175462692975998 + 50.0 * 6.257717132568359
Epoch 1170, val loss: 1.272830843925476
Epoch 1180, training loss: 313.0252380371094 = 0.21276122331619263 + 50.0 * 6.25624942779541
Epoch 1180, val loss: 1.2815544605255127
Epoch 1190, training loss: 313.1310729980469 = 0.2080729603767395 + 50.0 * 6.25846004486084
Epoch 1190, val loss: 1.2902098894119263
Epoch 1200, training loss: 313.02581787109375 = 0.2034810334444046 + 50.0 * 6.256446838378906
Epoch 1200, val loss: 1.299153208732605
Epoch 1210, training loss: 312.9971008300781 = 0.19899316132068634 + 50.0 * 6.255961894989014
Epoch 1210, val loss: 1.3075639009475708
Epoch 1220, training loss: 313.0065002441406 = 0.19459357857704163 + 50.0 * 6.2562384605407715
Epoch 1220, val loss: 1.316877841949463
Epoch 1230, training loss: 312.8606262207031 = 0.1902851015329361 + 50.0 * 6.253407001495361
Epoch 1230, val loss: 1.3255807161331177
Epoch 1240, training loss: 312.78692626953125 = 0.1860915571451187 + 50.0 * 6.252016544342041
Epoch 1240, val loss: 1.3348408937454224
Epoch 1250, training loss: 312.8609924316406 = 0.1819981038570404 + 50.0 * 6.253579616546631
Epoch 1250, val loss: 1.3437968492507935
Epoch 1260, training loss: 312.9505310058594 = 0.1779806762933731 + 50.0 * 6.255451202392578
Epoch 1260, val loss: 1.3528684377670288
Epoch 1270, training loss: 312.7806701660156 = 0.1740126609802246 + 50.0 * 6.252133369445801
Epoch 1270, val loss: 1.3616924285888672
Epoch 1280, training loss: 312.68121337890625 = 0.17015941441059113 + 50.0 * 6.250221252441406
Epoch 1280, val loss: 1.371069312095642
Epoch 1290, training loss: 312.7972412109375 = 0.1664241999387741 + 50.0 * 6.2526164054870605
Epoch 1290, val loss: 1.3801097869873047
Epoch 1300, training loss: 312.8019714355469 = 0.1627131849527359 + 50.0 * 6.2527852058410645
Epoch 1300, val loss: 1.3895094394683838
Epoch 1310, training loss: 312.6401062011719 = 0.1590883433818817 + 50.0 * 6.24962043762207
Epoch 1310, val loss: 1.3984465599060059
Epoch 1320, training loss: 312.5769958496094 = 0.15557417273521423 + 50.0 * 6.2484283447265625
Epoch 1320, val loss: 1.4080512523651123
Epoch 1330, training loss: 312.5368347167969 = 0.1521560549736023 + 50.0 * 6.2476935386657715
Epoch 1330, val loss: 1.4173247814178467
Epoch 1340, training loss: 312.9429626464844 = 0.14883728325366974 + 50.0 * 6.255882263183594
Epoch 1340, val loss: 1.4267457723617554
Epoch 1350, training loss: 312.71844482421875 = 0.145492285490036 + 50.0 * 6.25145959854126
Epoch 1350, val loss: 1.436269760131836
Epoch 1360, training loss: 312.6051330566406 = 0.1422697901725769 + 50.0 * 6.2492570877075195
Epoch 1360, val loss: 1.4454771280288696
Epoch 1370, training loss: 312.4454650878906 = 0.13912466168403625 + 50.0 * 6.246126651763916
Epoch 1370, val loss: 1.4548158645629883
Epoch 1380, training loss: 312.421630859375 = 0.1360766440629959 + 50.0 * 6.245711326599121
Epoch 1380, val loss: 1.4642990827560425
Epoch 1390, training loss: 312.6737976074219 = 0.13311147689819336 + 50.0 * 6.2508134841918945
Epoch 1390, val loss: 1.4737138748168945
Epoch 1400, training loss: 312.478515625 = 0.1301766335964203 + 50.0 * 6.24696683883667
Epoch 1400, val loss: 1.4832532405853271
Epoch 1410, training loss: 312.4142150878906 = 0.12731555104255676 + 50.0 * 6.2457380294799805
Epoch 1410, val loss: 1.4926080703735352
Epoch 1420, training loss: 312.4190368652344 = 0.12453732639551163 + 50.0 * 6.245890140533447
Epoch 1420, val loss: 1.5019700527191162
Epoch 1430, training loss: 312.3441467285156 = 0.12181544303894043 + 50.0 * 6.244446277618408
Epoch 1430, val loss: 1.5111477375030518
Epoch 1440, training loss: 312.37567138671875 = 0.11915881931781769 + 50.0 * 6.2451300621032715
Epoch 1440, val loss: 1.5204393863677979
Epoch 1450, training loss: 312.38018798828125 = 0.11657299846410751 + 50.0 * 6.245272159576416
Epoch 1450, val loss: 1.530013918876648
Epoch 1460, training loss: 312.32232666015625 = 0.11403708159923553 + 50.0 * 6.244165897369385
Epoch 1460, val loss: 1.5393397808074951
Epoch 1470, training loss: 312.24749755859375 = 0.1115695908665657 + 50.0 * 6.24271821975708
Epoch 1470, val loss: 1.5486912727355957
Epoch 1480, training loss: 312.18243408203125 = 0.1091499999165535 + 50.0 * 6.2414655685424805
Epoch 1480, val loss: 1.5579482316970825
Epoch 1490, training loss: 312.2384948730469 = 0.10681043565273285 + 50.0 * 6.242633819580078
Epoch 1490, val loss: 1.5672857761383057
Epoch 1500, training loss: 312.50787353515625 = 0.10452677309513092 + 50.0 * 6.2480669021606445
Epoch 1500, val loss: 1.5763169527053833
Epoch 1510, training loss: 312.2615051269531 = 0.10222797840833664 + 50.0 * 6.243185520172119
Epoch 1510, val loss: 1.5854202508926392
Epoch 1520, training loss: 312.17938232421875 = 0.10003108531236649 + 50.0 * 6.241586685180664
Epoch 1520, val loss: 1.59470534324646
Epoch 1530, training loss: 312.1310119628906 = 0.0978882759809494 + 50.0 * 6.240662097930908
Epoch 1530, val loss: 1.6040102243423462
Epoch 1540, training loss: 312.04827880859375 = 0.09579908847808838 + 50.0 * 6.239049911499023
Epoch 1540, val loss: 1.6131616830825806
Epoch 1550, training loss: 312.1333923339844 = 0.09376882761716843 + 50.0 * 6.240792274475098
Epoch 1550, val loss: 1.6221423149108887
Epoch 1560, training loss: 312.12957763671875 = 0.09177051484584808 + 50.0 * 6.240756511688232
Epoch 1560, val loss: 1.6315128803253174
Epoch 1570, training loss: 312.2427673339844 = 0.0898129865527153 + 50.0 * 6.243059158325195
Epoch 1570, val loss: 1.6408206224441528
Epoch 1580, training loss: 311.949462890625 = 0.0878831297159195 + 50.0 * 6.237231731414795
Epoch 1580, val loss: 1.6495202779769897
Epoch 1590, training loss: 311.9576110839844 = 0.08602537959814072 + 50.0 * 6.237431526184082
Epoch 1590, val loss: 1.658586025238037
Epoch 1600, training loss: 311.9698791503906 = 0.08422710746526718 + 50.0 * 6.237712860107422
Epoch 1600, val loss: 1.667960524559021
Epoch 1610, training loss: 312.3401794433594 = 0.08247026801109314 + 50.0 * 6.24515438079834
Epoch 1610, val loss: 1.6767442226409912
Epoch 1620, training loss: 312.1291198730469 = 0.08071447908878326 + 50.0 * 6.240967750549316
Epoch 1620, val loss: 1.6858242750167847
Epoch 1630, training loss: 311.9701232910156 = 0.07901730388402939 + 50.0 * 6.237822532653809
Epoch 1630, val loss: 1.6948233842849731
Epoch 1640, training loss: 311.85546875 = 0.07737034559249878 + 50.0 * 6.235561847686768
Epoch 1640, val loss: 1.7037776708602905
Epoch 1650, training loss: 311.8680114746094 = 0.0757712572813034 + 50.0 * 6.235844612121582
Epoch 1650, val loss: 1.7127509117126465
Epoch 1660, training loss: 312.0532531738281 = 0.07421539723873138 + 50.0 * 6.2395806312561035
Epoch 1660, val loss: 1.7215813398361206
Epoch 1670, training loss: 312.1748046875 = 0.0726764053106308 + 50.0 * 6.2420430183410645
Epoch 1670, val loss: 1.7303252220153809
Epoch 1680, training loss: 312.043212890625 = 0.07115182280540466 + 50.0 * 6.23944091796875
Epoch 1680, val loss: 1.7388640642166138
Epoch 1690, training loss: 311.8294982910156 = 0.06967821717262268 + 50.0 * 6.235196590423584
Epoch 1690, val loss: 1.7479475736618042
Epoch 1700, training loss: 311.7691345214844 = 0.06825125217437744 + 50.0 * 6.234017848968506
Epoch 1700, val loss: 1.7567272186279297
Epoch 1710, training loss: 311.7693176269531 = 0.06687420606613159 + 50.0 * 6.234048843383789
Epoch 1710, val loss: 1.7656079530715942
Epoch 1720, training loss: 312.1480407714844 = 0.0655331164598465 + 50.0 * 6.241650104522705
Epoch 1720, val loss: 1.7744485139846802
Epoch 1730, training loss: 311.8568420410156 = 0.06417780369520187 + 50.0 * 6.23585319519043
Epoch 1730, val loss: 1.7825927734375
Epoch 1740, training loss: 311.75787353515625 = 0.06287258863449097 + 50.0 * 6.23390007019043
Epoch 1740, val loss: 1.7914353609085083
Epoch 1750, training loss: 311.827392578125 = 0.06161315739154816 + 50.0 * 6.235315322875977
Epoch 1750, val loss: 1.8001518249511719
Epoch 1760, training loss: 311.74627685546875 = 0.060364797711372375 + 50.0 * 6.233718395233154
Epoch 1760, val loss: 1.8084440231323242
Epoch 1770, training loss: 311.7145080566406 = 0.05915825441479683 + 50.0 * 6.233107089996338
Epoch 1770, val loss: 1.8166370391845703
Epoch 1780, training loss: 311.73980712890625 = 0.05798929184675217 + 50.0 * 6.233636379241943
Epoch 1780, val loss: 1.8253839015960693
Epoch 1790, training loss: 311.9100646972656 = 0.0568467378616333 + 50.0 * 6.237064361572266
Epoch 1790, val loss: 1.8333535194396973
Epoch 1800, training loss: 311.7459411621094 = 0.05569775030016899 + 50.0 * 6.233805179595947
Epoch 1800, val loss: 1.8415249586105347
Epoch 1810, training loss: 311.6153564453125 = 0.05460076406598091 + 50.0 * 6.231215000152588
Epoch 1810, val loss: 1.850212812423706
Epoch 1820, training loss: 311.59063720703125 = 0.0535346083343029 + 50.0 * 6.23074197769165
Epoch 1820, val loss: 1.8582682609558105
Epoch 1830, training loss: 312.0619201660156 = 0.05250973254442215 + 50.0 * 6.240188121795654
Epoch 1830, val loss: 1.866476058959961
Epoch 1840, training loss: 311.7345275878906 = 0.051470186561346054 + 50.0 * 6.233660697937012
Epoch 1840, val loss: 1.8742815256118774
Epoch 1850, training loss: 311.5958557128906 = 0.050460085272789 + 50.0 * 6.230908393859863
Epoch 1850, val loss: 1.8822548389434814
Epoch 1860, training loss: 311.6197509765625 = 0.049500904977321625 + 50.0 * 6.231404781341553
Epoch 1860, val loss: 1.8903433084487915
Epoch 1870, training loss: 311.63702392578125 = 0.04855165258049965 + 50.0 * 6.231769561767578
Epoch 1870, val loss: 1.8982828855514526
Epoch 1880, training loss: 311.587646484375 = 0.047622572630643845 + 50.0 * 6.230800628662109
Epoch 1880, val loss: 1.9062923192977905
Epoch 1890, training loss: 311.5746154785156 = 0.0467229038476944 + 50.0 * 6.230557918548584
Epoch 1890, val loss: 1.9140539169311523
Epoch 1900, training loss: 311.58038330078125 = 0.045835498720407486 + 50.0 * 6.230690956115723
Epoch 1900, val loss: 1.9219032526016235
Epoch 1910, training loss: 311.5592346191406 = 0.044967133551836014 + 50.0 * 6.23028564453125
Epoch 1910, val loss: 1.929743766784668
Epoch 1920, training loss: 311.6049499511719 = 0.04413296654820442 + 50.0 * 6.2312164306640625
Epoch 1920, val loss: 1.9375534057617188
Epoch 1930, training loss: 311.5540771484375 = 0.0433071032166481 + 50.0 * 6.230215549468994
Epoch 1930, val loss: 1.9451375007629395
Epoch 1940, training loss: 311.614013671875 = 0.042503587901592255 + 50.0 * 6.2314300537109375
Epoch 1940, val loss: 1.9527047872543335
Epoch 1950, training loss: 311.4869689941406 = 0.041714102029800415 + 50.0 * 6.228905200958252
Epoch 1950, val loss: 1.9599957466125488
Epoch 1960, training loss: 311.4677429199219 = 0.040949076414108276 + 50.0 * 6.2285356521606445
Epoch 1960, val loss: 1.9674960374832153
Epoch 1970, training loss: 311.4477233886719 = 0.04020572081208229 + 50.0 * 6.228150367736816
Epoch 1970, val loss: 1.974885106086731
Epoch 1980, training loss: 311.5450744628906 = 0.03947848826646805 + 50.0 * 6.230111598968506
Epoch 1980, val loss: 1.9825866222381592
Epoch 1990, training loss: 311.5272521972656 = 0.03876810520887375 + 50.0 * 6.229769706726074
Epoch 1990, val loss: 1.9902225732803345
Epoch 2000, training loss: 311.3690185546875 = 0.03806068375706673 + 50.0 * 6.226619243621826
Epoch 2000, val loss: 1.9973182678222656
Epoch 2010, training loss: 311.5701904296875 = 0.03738601505756378 + 50.0 * 6.230656147003174
Epoch 2010, val loss: 2.004314422607422
Epoch 2020, training loss: 311.42523193359375 = 0.036713216453790665 + 50.0 * 6.2277703285217285
Epoch 2020, val loss: 2.0118460655212402
Epoch 2030, training loss: 311.3835754394531 = 0.03606010228395462 + 50.0 * 6.226950168609619
Epoch 2030, val loss: 2.018817663192749
Epoch 2040, training loss: 311.3398132324219 = 0.03543228283524513 + 50.0 * 6.22608757019043
Epoch 2040, val loss: 2.0259594917297363
Epoch 2050, training loss: 311.40179443359375 = 0.03482064604759216 + 50.0 * 6.227339744567871
Epoch 2050, val loss: 2.0329270362854004
Epoch 2060, training loss: 311.4719543457031 = 0.03421418368816376 + 50.0 * 6.228754997253418
Epoch 2060, val loss: 2.040207624435425
Epoch 2070, training loss: 311.5092468261719 = 0.033622708171606064 + 50.0 * 6.2295122146606445
Epoch 2070, val loss: 2.047091007232666
Epoch 2080, training loss: 311.36773681640625 = 0.03303566202521324 + 50.0 * 6.226693630218506
Epoch 2080, val loss: 2.054006338119507
Epoch 2090, training loss: 311.3460998535156 = 0.03247184306383133 + 50.0 * 6.2262725830078125
Epoch 2090, val loss: 2.0611720085144043
Epoch 2100, training loss: 311.3365478515625 = 0.031927336007356644 + 50.0 * 6.226092338562012
Epoch 2100, val loss: 2.0679385662078857
Epoch 2110, training loss: 311.2633972167969 = 0.03138723224401474 + 50.0 * 6.224640369415283
Epoch 2110, val loss: 2.0747835636138916
Epoch 2120, training loss: 311.55426025390625 = 0.030872788280248642 + 50.0 * 6.230467319488525
Epoch 2120, val loss: 2.0812952518463135
Epoch 2130, training loss: 311.3495788574219 = 0.030350062996149063 + 50.0 * 6.226384162902832
Epoch 2130, val loss: 2.088160753250122
Epoch 2140, training loss: 311.2258605957031 = 0.029839126393198967 + 50.0 * 6.2239203453063965
Epoch 2140, val loss: 2.094835042953491
Epoch 2150, training loss: 311.32562255859375 = 0.02935691550374031 + 50.0 * 6.225925445556641
Epoch 2150, val loss: 2.101292610168457
Epoch 2160, training loss: 311.25604248046875 = 0.028874250128865242 + 50.0 * 6.224543571472168
Epoch 2160, val loss: 2.1075100898742676
Epoch 2170, training loss: 311.3122253417969 = 0.028402669355273247 + 50.0 * 6.225676536560059
Epoch 2170, val loss: 2.1136417388916016
Epoch 2180, training loss: 311.32965087890625 = 0.027940932661294937 + 50.0 * 6.226033687591553
Epoch 2180, val loss: 2.120455265045166
Epoch 2190, training loss: 311.1979675292969 = 0.027492990717291832 + 50.0 * 6.223409175872803
Epoch 2190, val loss: 2.127030849456787
Epoch 2200, training loss: 311.40777587890625 = 0.027057867497205734 + 50.0 * 6.227613925933838
Epoch 2200, val loss: 2.1335158348083496
Epoch 2210, training loss: 311.1882019042969 = 0.026620542630553246 + 50.0 * 6.223231792449951
Epoch 2210, val loss: 2.1394784450531006
Epoch 2220, training loss: 311.1085205078125 = 0.026202816516160965 + 50.0 * 6.221646308898926
Epoch 2220, val loss: 2.1460442543029785
Epoch 2230, training loss: 311.1316833496094 = 0.02580070123076439 + 50.0 * 6.222117900848389
Epoch 2230, val loss: 2.1524980068206787
Epoch 2240, training loss: 311.2845153808594 = 0.02540842816233635 + 50.0 * 6.225182056427002
Epoch 2240, val loss: 2.158785581588745
Epoch 2250, training loss: 311.2198791503906 = 0.025008559226989746 + 50.0 * 6.223897933959961
Epoch 2250, val loss: 2.1642003059387207
Epoch 2260, training loss: 311.07073974609375 = 0.024617496877908707 + 50.0 * 6.220922470092773
Epoch 2260, val loss: 2.1701607704162598
Epoch 2270, training loss: 311.1055908203125 = 0.02424723654985428 + 50.0 * 6.2216267585754395
Epoch 2270, val loss: 2.1763010025024414
Epoch 2280, training loss: 311.3094482421875 = 0.023883715271949768 + 50.0 * 6.225711345672607
Epoch 2280, val loss: 2.181818962097168
Epoch 2290, training loss: 311.2686462402344 = 0.02352120168507099 + 50.0 * 6.224902629852295
Epoch 2290, val loss: 2.187964677810669
Epoch 2300, training loss: 311.148681640625 = 0.02316412888467312 + 50.0 * 6.22251033782959
Epoch 2300, val loss: 2.193241834640503
Epoch 2310, training loss: 311.099365234375 = 0.022820234298706055 + 50.0 * 6.221530914306641
Epoch 2310, val loss: 2.1997230052948
Epoch 2320, training loss: 311.1134033203125 = 0.02248476818203926 + 50.0 * 6.221817970275879
Epoch 2320, val loss: 2.205164670944214
Epoch 2330, training loss: 311.159912109375 = 0.022159311920404434 + 50.0 * 6.222755432128906
Epoch 2330, val loss: 2.210975408554077
Epoch 2340, training loss: 311.15093994140625 = 0.021835239604115486 + 50.0 * 6.2225823402404785
Epoch 2340, val loss: 2.2168185710906982
Epoch 2350, training loss: 311.0710144042969 = 0.02151375077664852 + 50.0 * 6.220990180969238
Epoch 2350, val loss: 2.2221622467041016
Epoch 2360, training loss: 311.0253601074219 = 0.021204626187682152 + 50.0 * 6.220082759857178
Epoch 2360, val loss: 2.2274227142333984
Epoch 2370, training loss: 311.0496826171875 = 0.02090505324304104 + 50.0 * 6.220575332641602
Epoch 2370, val loss: 2.2328264713287354
Epoch 2380, training loss: 311.1602478027344 = 0.020610002800822258 + 50.0 * 6.222793102264404
Epoch 2380, val loss: 2.2383899688720703
Epoch 2390, training loss: 310.9974365234375 = 0.02031913958489895 + 50.0 * 6.219542026519775
Epoch 2390, val loss: 2.2438364028930664
Epoch 2400, training loss: 311.1105041503906 = 0.020036378875374794 + 50.0 * 6.221809387207031
Epoch 2400, val loss: 2.248771905899048
Epoch 2410, training loss: 311.0592041015625 = 0.019754532724618912 + 50.0 * 6.220789432525635
Epoch 2410, val loss: 2.2545387744903564
Epoch 2420, training loss: 311.0780029296875 = 0.019476203247904778 + 50.0 * 6.221170902252197
Epoch 2420, val loss: 2.259631872177124
Epoch 2430, training loss: 310.96002197265625 = 0.01920773647725582 + 50.0 * 6.21881628036499
Epoch 2430, val loss: 2.264756202697754
Epoch 2440, training loss: 310.9944152832031 = 0.01895085722208023 + 50.0 * 6.219509124755859
Epoch 2440, val loss: 2.2699825763702393
Epoch 2450, training loss: 310.9930114746094 = 0.018693754449486732 + 50.0 * 6.219486236572266
Epoch 2450, val loss: 2.2749269008636475
Epoch 2460, training loss: 310.9286193847656 = 0.018444472923874855 + 50.0 * 6.218204021453857
Epoch 2460, val loss: 2.2804627418518066
Epoch 2470, training loss: 311.0528564453125 = 0.01820322312414646 + 50.0 * 6.220693111419678
Epoch 2470, val loss: 2.2854223251342773
Epoch 2480, training loss: 311.0599060058594 = 0.017960794270038605 + 50.0 * 6.22083854675293
Epoch 2480, val loss: 2.2907845973968506
Epoch 2490, training loss: 310.8941650390625 = 0.017717674374580383 + 50.0 * 6.217528820037842
Epoch 2490, val loss: 2.295475959777832
Epoch 2500, training loss: 310.89447021484375 = 0.017486581578850746 + 50.0 * 6.2175397872924805
Epoch 2500, val loss: 2.300541400909424
Epoch 2510, training loss: 311.10015869140625 = 0.017269007861614227 + 50.0 * 6.221657752990723
Epoch 2510, val loss: 2.3058667182922363
Epoch 2520, training loss: 310.85443115234375 = 0.0170365609228611 + 50.0 * 6.216747760772705
Epoch 2520, val loss: 2.310056209564209
Epoch 2530, training loss: 310.9227294921875 = 0.016820259392261505 + 50.0 * 6.218118190765381
Epoch 2530, val loss: 2.314831495285034
Epoch 2540, training loss: 311.040283203125 = 0.01661018654704094 + 50.0 * 6.220473766326904
Epoch 2540, val loss: 2.319673776626587
Epoch 2550, training loss: 310.8610534667969 = 0.016393855214118958 + 50.0 * 6.216893196105957
Epoch 2550, val loss: 2.3246185779571533
Epoch 2560, training loss: 310.8492431640625 = 0.01619143970310688 + 50.0 * 6.216660976409912
Epoch 2560, val loss: 2.329413414001465
Epoch 2570, training loss: 311.06988525390625 = 0.015995461493730545 + 50.0 * 6.221077919006348
Epoch 2570, val loss: 2.333719491958618
Epoch 2580, training loss: 311.1235656738281 = 0.015792759135365486 + 50.0 * 6.222155570983887
Epoch 2580, val loss: 2.339043617248535
Epoch 2590, training loss: 310.8879699707031 = 0.015591196715831757 + 50.0 * 6.217447280883789
Epoch 2590, val loss: 2.3430354595184326
Epoch 2600, training loss: 310.784912109375 = 0.015401404350996017 + 50.0 * 6.215390205383301
Epoch 2600, val loss: 2.347933769226074
Epoch 2610, training loss: 310.7537841796875 = 0.015219958499073982 + 50.0 * 6.214771270751953
Epoch 2610, val loss: 2.35239839553833
Epoch 2620, training loss: 311.002197265625 = 0.01504755113273859 + 50.0 * 6.219743251800537
Epoch 2620, val loss: 2.35697340965271
Epoch 2630, training loss: 310.7804260253906 = 0.014858152717351913 + 50.0 * 6.215311050415039
Epoch 2630, val loss: 2.3611743450164795
Epoch 2640, training loss: 310.7449035644531 = 0.014680864289402962 + 50.0 * 6.214604377746582
Epoch 2640, val loss: 2.365858793258667
Epoch 2650, training loss: 310.81829833984375 = 0.014512477442622185 + 50.0 * 6.216075420379639
Epoch 2650, val loss: 2.370234251022339
Epoch 2660, training loss: 310.95672607421875 = 0.014342457987368107 + 50.0 * 6.218847751617432
Epoch 2660, val loss: 2.373976230621338
Epoch 2670, training loss: 310.8438720703125 = 0.014168393798172474 + 50.0 * 6.216594219207764
Epoch 2670, val loss: 2.3781018257141113
Epoch 2680, training loss: 310.85589599609375 = 0.014008156023919582 + 50.0 * 6.2168378829956055
Epoch 2680, val loss: 2.3824970722198486
Epoch 2690, training loss: 310.720458984375 = 0.013844951055943966 + 50.0 * 6.214132308959961
Epoch 2690, val loss: 2.3869264125823975
Epoch 2700, training loss: 310.7348937988281 = 0.013691156171262264 + 50.0 * 6.214423656463623
Epoch 2700, val loss: 2.39148211479187
Epoch 2710, training loss: 310.9496765136719 = 0.013540063984692097 + 50.0 * 6.218722343444824
Epoch 2710, val loss: 2.3955297470092773
Epoch 2720, training loss: 310.82763671875 = 0.013383406214416027 + 50.0 * 6.21628475189209
Epoch 2720, val loss: 2.3990352153778076
Epoch 2730, training loss: 310.6806335449219 = 0.013234018348157406 + 50.0 * 6.213347911834717
Epoch 2730, val loss: 2.4034488201141357
Epoch 2740, training loss: 310.7617492675781 = 0.013090898282825947 + 50.0 * 6.214973449707031
Epoch 2740, val loss: 2.4073944091796875
Epoch 2750, training loss: 310.8531494140625 = 0.012948504649102688 + 50.0 * 6.216804504394531
Epoch 2750, val loss: 2.411348581314087
Epoch 2760, training loss: 310.7766418457031 = 0.012805107049643993 + 50.0 * 6.21527624130249
Epoch 2760, val loss: 2.4156880378723145
Epoch 2770, training loss: 310.7984924316406 = 0.012666885741055012 + 50.0 * 6.215716361999512
Epoch 2770, val loss: 2.419552803039551
Epoch 2780, training loss: 310.7713928222656 = 0.01253132801502943 + 50.0 * 6.215177059173584
Epoch 2780, val loss: 2.423405885696411
Epoch 2790, training loss: 310.7242126464844 = 0.012396934442222118 + 50.0 * 6.214236259460449
Epoch 2790, val loss: 2.4264042377471924
Epoch 2800, training loss: 310.7019958496094 = 0.012266184203326702 + 50.0 * 6.213794708251953
Epoch 2800, val loss: 2.4306585788726807
Epoch 2810, training loss: 310.6829833984375 = 0.012136784382164478 + 50.0 * 6.213417053222656
Epoch 2810, val loss: 2.4347548484802246
Epoch 2820, training loss: 310.72174072265625 = 0.012012217193841934 + 50.0 * 6.214194297790527
Epoch 2820, val loss: 2.438328742980957
Epoch 2830, training loss: 310.77685546875 = 0.011886007152497768 + 50.0 * 6.215299606323242
Epoch 2830, val loss: 2.441997528076172
Epoch 2840, training loss: 310.75140380859375 = 0.011759722605347633 + 50.0 * 6.2147932052612305
Epoch 2840, val loss: 2.4454283714294434
Epoch 2850, training loss: 310.6763610839844 = 0.011640504002571106 + 50.0 * 6.213294982910156
Epoch 2850, val loss: 2.449232339859009
Epoch 2860, training loss: 310.69549560546875 = 0.011523131281137466 + 50.0 * 6.213679313659668
Epoch 2860, val loss: 2.4526546001434326
Epoch 2870, training loss: 310.68341064453125 = 0.011406750418245792 + 50.0 * 6.21343994140625
Epoch 2870, val loss: 2.456036329269409
Epoch 2880, training loss: 310.6234130859375 = 0.011291954666376114 + 50.0 * 6.212242126464844
Epoch 2880, val loss: 2.460042953491211
Epoch 2890, training loss: 310.7779235839844 = 0.011181249283254147 + 50.0 * 6.215335369110107
Epoch 2890, val loss: 2.4639008045196533
Epoch 2900, training loss: 310.723388671875 = 0.01106912549585104 + 50.0 * 6.2142462730407715
Epoch 2900, val loss: 2.4666054248809814
Epoch 2910, training loss: 310.5398864746094 = 0.010955698788166046 + 50.0 * 6.210578918457031
Epoch 2910, val loss: 2.470524549484253
Epoch 2920, training loss: 310.56781005859375 = 0.010851893573999405 + 50.0 * 6.21113920211792
Epoch 2920, val loss: 2.4742016792297363
Epoch 2930, training loss: 310.75738525390625 = 0.010752669535577297 + 50.0 * 6.214932918548584
Epoch 2930, val loss: 2.477540969848633
Epoch 2940, training loss: 310.5881652832031 = 0.010644195601344109 + 50.0 * 6.211550235748291
Epoch 2940, val loss: 2.4807939529418945
Epoch 2950, training loss: 310.61468505859375 = 0.010542083531618118 + 50.0 * 6.212082862854004
Epoch 2950, val loss: 2.484431028366089
Epoch 2960, training loss: 310.695556640625 = 0.01044384390115738 + 50.0 * 6.21370267868042
Epoch 2960, val loss: 2.487980365753174
Epoch 2970, training loss: 310.57666015625 = 0.010342356748878956 + 50.0 * 6.2113261222839355
Epoch 2970, val loss: 2.4904367923736572
Epoch 2980, training loss: 310.47930908203125 = 0.010244103148579597 + 50.0 * 6.209381103515625
Epoch 2980, val loss: 2.493992328643799
Epoch 2990, training loss: 310.5321350097656 = 0.010152413509786129 + 50.0 * 6.210439682006836
Epoch 2990, val loss: 2.49729061126709
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6518518518518519
0.8139167105956774
The final CL Acc:0.67531, 0.03853, The final GNN Acc:0.81567, 0.00131
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13178])
remove edge: torch.Size([2, 7900])
updated graph: torch.Size([2, 10522])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7774963378906 = 1.9351669549942017 + 50.0 * 8.596846580505371
Epoch 0, val loss: 1.9308191537857056
Epoch 10, training loss: 431.7322998046875 = 1.9269779920578003 + 50.0 * 8.59610652923584
Epoch 10, val loss: 1.9232250452041626
Epoch 20, training loss: 431.4601745605469 = 1.9169358015060425 + 50.0 * 8.590865135192871
Epoch 20, val loss: 1.9136040210723877
Epoch 30, training loss: 429.6271667480469 = 1.9042975902557373 + 50.0 * 8.554457664489746
Epoch 30, val loss: 1.9014602899551392
Epoch 40, training loss: 418.0414733886719 = 1.8893550634384155 + 50.0 * 8.323042869567871
Epoch 40, val loss: 1.887497067451477
Epoch 50, training loss: 383.9804382324219 = 1.8717454671859741 + 50.0 * 7.642174243927002
Epoch 50, val loss: 1.8712692260742188
Epoch 60, training loss: 364.7734680175781 = 1.857829213142395 + 50.0 * 7.258312702178955
Epoch 60, val loss: 1.8583334684371948
Epoch 70, training loss: 352.1030578613281 = 1.8458747863769531 + 50.0 * 7.005143165588379
Epoch 70, val loss: 1.8467990159988403
Epoch 80, training loss: 344.3742370605469 = 1.8345527648925781 + 50.0 * 6.850793361663818
Epoch 80, val loss: 1.8361653089523315
Epoch 90, training loss: 340.0262451171875 = 1.82318913936615 + 50.0 * 6.764060974121094
Epoch 90, val loss: 1.825439214706421
Epoch 100, training loss: 337.05364990234375 = 1.8123083114624023 + 50.0 * 6.704826831817627
Epoch 100, val loss: 1.815274953842163
Epoch 110, training loss: 334.7661437988281 = 1.8017200231552124 + 50.0 * 6.65928840637207
Epoch 110, val loss: 1.8053803443908691
Epoch 120, training loss: 332.77093505859375 = 1.7917085886001587 + 50.0 * 6.619584083557129
Epoch 120, val loss: 1.795989751815796
Epoch 130, training loss: 331.0853271484375 = 1.781727910041809 + 50.0 * 6.586071491241455
Epoch 130, val loss: 1.7867447137832642
Epoch 140, training loss: 329.7060241699219 = 1.7712223529815674 + 50.0 * 6.558696269989014
Epoch 140, val loss: 1.7771871089935303
Epoch 150, training loss: 328.7109069824219 = 1.7599138021469116 + 50.0 * 6.539019584655762
Epoch 150, val loss: 1.7670297622680664
Epoch 160, training loss: 327.6042175292969 = 1.7476807832717896 + 50.0 * 6.5171308517456055
Epoch 160, val loss: 1.7560726404190063
Epoch 170, training loss: 326.7296447753906 = 1.734548807144165 + 50.0 * 6.49990177154541
Epoch 170, val loss: 1.7444332838058472
Epoch 180, training loss: 326.0516662597656 = 1.7203025817871094 + 50.0 * 6.486627101898193
Epoch 180, val loss: 1.7320219278335571
Epoch 190, training loss: 325.3680114746094 = 1.704719066619873 + 50.0 * 6.473266124725342
Epoch 190, val loss: 1.718448519706726
Epoch 200, training loss: 324.6732482910156 = 1.6877671480178833 + 50.0 * 6.459709167480469
Epoch 200, val loss: 1.7037211656570435
Epoch 210, training loss: 324.1116027832031 = 1.6693488359451294 + 50.0 * 6.448844909667969
Epoch 210, val loss: 1.687806248664856
Epoch 220, training loss: 323.66668701171875 = 1.6494250297546387 + 50.0 * 6.440345764160156
Epoch 220, val loss: 1.6707251071929932
Epoch 230, training loss: 323.2748107910156 = 1.627873182296753 + 50.0 * 6.432938575744629
Epoch 230, val loss: 1.6522977352142334
Epoch 240, training loss: 322.7874450683594 = 1.6047710180282593 + 50.0 * 6.423653602600098
Epoch 240, val loss: 1.6325281858444214
Epoch 250, training loss: 322.3359680175781 = 1.580187439918518 + 50.0 * 6.4151153564453125
Epoch 250, val loss: 1.611567735671997
Epoch 260, training loss: 321.92462158203125 = 1.5540887117385864 + 50.0 * 6.407411098480225
Epoch 260, val loss: 1.5893758535385132
Epoch 270, training loss: 321.63165283203125 = 1.5266891717910767 + 50.0 * 6.402099132537842
Epoch 270, val loss: 1.566102147102356
Epoch 280, training loss: 321.2449951171875 = 1.497908353805542 + 50.0 * 6.394942283630371
Epoch 280, val loss: 1.5418519973754883
Epoch 290, training loss: 320.9452819824219 = 1.4680585861206055 + 50.0 * 6.389544486999512
Epoch 290, val loss: 1.5166946649551392
Epoch 300, training loss: 320.5766296386719 = 1.437269926071167 + 50.0 * 6.382787704467773
Epoch 300, val loss: 1.4908828735351562
Epoch 310, training loss: 320.2471923828125 = 1.4057958126068115 + 50.0 * 6.376827716827393
Epoch 310, val loss: 1.4646661281585693
Epoch 320, training loss: 320.14874267578125 = 1.3738000392913818 + 50.0 * 6.3754987716674805
Epoch 320, val loss: 1.4381252527236938
Epoch 330, training loss: 319.82904052734375 = 1.3414403200149536 + 50.0 * 6.369751930236816
Epoch 330, val loss: 1.4114497900009155
Epoch 340, training loss: 319.5094909667969 = 1.3088470697402954 + 50.0 * 6.364013195037842
Epoch 340, val loss: 1.3849105834960938
Epoch 350, training loss: 319.21417236328125 = 1.276423692703247 + 50.0 * 6.358755111694336
Epoch 350, val loss: 1.3585243225097656
Epoch 360, training loss: 318.96405029296875 = 1.244222640991211 + 50.0 * 6.354396343231201
Epoch 360, val loss: 1.3327312469482422
Epoch 370, training loss: 318.8041687011719 = 1.2123329639434814 + 50.0 * 6.351836681365967
Epoch 370, val loss: 1.3073267936706543
Epoch 380, training loss: 318.5730285644531 = 1.1808464527130127 + 50.0 * 6.347843647003174
Epoch 380, val loss: 1.282670259475708
Epoch 390, training loss: 318.31256103515625 = 1.1499594449996948 + 50.0 * 6.343252182006836
Epoch 390, val loss: 1.258811593055725
Epoch 400, training loss: 318.15924072265625 = 1.119767665863037 + 50.0 * 6.340789318084717
Epoch 400, val loss: 1.2357956171035767
Epoch 410, training loss: 318.1557312011719 = 1.0901495218276978 + 50.0 * 6.341311931610107
Epoch 410, val loss: 1.21337890625
Epoch 420, training loss: 317.8036804199219 = 1.0613102912902832 + 50.0 * 6.334847450256348
Epoch 420, val loss: 1.1917643547058105
Epoch 430, training loss: 317.5474548339844 = 1.0332764387130737 + 50.0 * 6.3302836418151855
Epoch 430, val loss: 1.1714303493499756
Epoch 440, training loss: 317.3499755859375 = 1.0061315298080444 + 50.0 * 6.326877117156982
Epoch 440, val loss: 1.152101755142212
Epoch 450, training loss: 317.23187255859375 = 0.9798786044120789 + 50.0 * 6.325039386749268
Epoch 450, val loss: 1.1337724924087524
Epoch 460, training loss: 317.16522216796875 = 0.9542551040649414 + 50.0 * 6.324219226837158
Epoch 460, val loss: 1.116259217262268
Epoch 470, training loss: 316.90618896484375 = 0.9295832514762878 + 50.0 * 6.3195319175720215
Epoch 470, val loss: 1.0995742082595825
Epoch 480, training loss: 316.7561340332031 = 0.9057812690734863 + 50.0 * 6.317007064819336
Epoch 480, val loss: 1.0840063095092773
Epoch 490, training loss: 316.8101806640625 = 0.8828442692756653 + 50.0 * 6.318546772003174
Epoch 490, val loss: 1.0693252086639404
Epoch 500, training loss: 316.60992431640625 = 0.8607093691825867 + 50.0 * 6.31498384475708
Epoch 500, val loss: 1.0555239915847778
Epoch 510, training loss: 316.43817138671875 = 0.8392518162727356 + 50.0 * 6.311977863311768
Epoch 510, val loss: 1.0424683094024658
Epoch 520, training loss: 316.30780029296875 = 0.8184751272201538 + 50.0 * 6.309786319732666
Epoch 520, val loss: 1.030457854270935
Epoch 530, training loss: 316.13714599609375 = 0.798525869846344 + 50.0 * 6.306772708892822
Epoch 530, val loss: 1.019132375717163
Epoch 540, training loss: 316.0223693847656 = 0.7792707681655884 + 50.0 * 6.304862022399902
Epoch 540, val loss: 1.0088059902191162
Epoch 550, training loss: 316.0265197753906 = 0.7605680227279663 + 50.0 * 6.305319309234619
Epoch 550, val loss: 0.9990386962890625
Epoch 560, training loss: 316.0646057128906 = 0.7424184679985046 + 50.0 * 6.306443691253662
Epoch 560, val loss: 0.9897871017456055
Epoch 570, training loss: 315.6978454589844 = 0.7248335480690002 + 50.0 * 6.299460411071777
Epoch 570, val loss: 0.9812766909599304
Epoch 580, training loss: 315.6016845703125 = 0.7077333331108093 + 50.0 * 6.297878742218018
Epoch 580, val loss: 0.9735647439956665
Epoch 590, training loss: 315.5582275390625 = 0.6911989450454712 + 50.0 * 6.2973408699035645
Epoch 590, val loss: 0.9665203094482422
Epoch 600, training loss: 315.51153564453125 = 0.6750609874725342 + 50.0 * 6.296730041503906
Epoch 600, val loss: 0.959811270236969
Epoch 610, training loss: 315.29931640625 = 0.6593334674835205 + 50.0 * 6.292799949645996
Epoch 610, val loss: 0.9538534879684448
Epoch 620, training loss: 315.2646179199219 = 0.644027054309845 + 50.0 * 6.292411804199219
Epoch 620, val loss: 0.9484019875526428
Epoch 630, training loss: 315.2130432128906 = 0.6290938854217529 + 50.0 * 6.2916789054870605
Epoch 630, val loss: 0.9432022571563721
Epoch 640, training loss: 315.11895751953125 = 0.6144371628761292 + 50.0 * 6.290090084075928
Epoch 640, val loss: 0.9385370016098022
Epoch 650, training loss: 315.0565185546875 = 0.600275993347168 + 50.0 * 6.289124488830566
Epoch 650, val loss: 0.934537947177887
Epoch 660, training loss: 314.9870300292969 = 0.5862592458724976 + 50.0 * 6.288015365600586
Epoch 660, val loss: 0.9307465553283691
Epoch 670, training loss: 314.8235778808594 = 0.5726916790008545 + 50.0 * 6.285017490386963
Epoch 670, val loss: 0.9274085164070129
Epoch 680, training loss: 314.7574157714844 = 0.5594573020935059 + 50.0 * 6.28395938873291
Epoch 680, val loss: 0.9247121214866638
Epoch 690, training loss: 314.9415283203125 = 0.5465491414070129 + 50.0 * 6.287899494171143
Epoch 690, val loss: 0.9223025441169739
Epoch 700, training loss: 314.7937316894531 = 0.533769428730011 + 50.0 * 6.285199165344238
Epoch 700, val loss: 0.920186460018158
Epoch 710, training loss: 314.53680419921875 = 0.5213510990142822 + 50.0 * 6.280308723449707
Epoch 710, val loss: 0.9183540940284729
Epoch 720, training loss: 314.6780700683594 = 0.5092589855194092 + 50.0 * 6.283376693725586
Epoch 720, val loss: 0.9170464873313904
Epoch 730, training loss: 314.4788818359375 = 0.4973224103450775 + 50.0 * 6.279631614685059
Epoch 730, val loss: 0.9158066511154175
Epoch 740, training loss: 314.4202880859375 = 0.48575708270072937 + 50.0 * 6.278690814971924
Epoch 740, val loss: 0.9152964949607849
Epoch 750, training loss: 314.3533935546875 = 0.4743536710739136 + 50.0 * 6.277580261230469
Epoch 750, val loss: 0.9146177172660828
Epoch 760, training loss: 314.2142639160156 = 0.4632161855697632 + 50.0 * 6.275020599365234
Epoch 760, val loss: 0.9144397974014282
Epoch 770, training loss: 314.14642333984375 = 0.45236295461654663 + 50.0 * 6.273880958557129
Epoch 770, val loss: 0.9146242141723633
Epoch 780, training loss: 314.33612060546875 = 0.44173529744148254 + 50.0 * 6.277887344360352
Epoch 780, val loss: 0.915103018283844
Epoch 790, training loss: 314.1698913574219 = 0.4312981963157654 + 50.0 * 6.274771690368652
Epoch 790, val loss: 0.9154653549194336
Epoch 800, training loss: 314.0250549316406 = 0.4210400879383087 + 50.0 * 6.272079944610596
Epoch 800, val loss: 0.9164960980415344
Epoch 810, training loss: 313.9952392578125 = 0.41104602813720703 + 50.0 * 6.271684169769287
Epoch 810, val loss: 0.9176404476165771
Epoch 820, training loss: 313.92431640625 = 0.40129536390304565 + 50.0 * 6.27046012878418
Epoch 820, val loss: 0.9189175963401794
Epoch 830, training loss: 313.8933410644531 = 0.39171257615089417 + 50.0 * 6.2700324058532715
Epoch 830, val loss: 0.9205494523048401
Epoch 840, training loss: 313.93499755859375 = 0.3823052644729614 + 50.0 * 6.271053791046143
Epoch 840, val loss: 0.9224103689193726
Epoch 850, training loss: 313.85205078125 = 0.3730606734752655 + 50.0 * 6.269580364227295
Epoch 850, val loss: 0.9243372082710266
Epoch 860, training loss: 313.676025390625 = 0.3639479875564575 + 50.0 * 6.266241550445557
Epoch 860, val loss: 0.9261981844902039
Epoch 870, training loss: 313.64276123046875 = 0.35508546233177185 + 50.0 * 6.265753746032715
Epoch 870, val loss: 0.928728461265564
Epoch 880, training loss: 313.7939453125 = 0.346366286277771 + 50.0 * 6.268951416015625
Epoch 880, val loss: 0.931149423122406
Epoch 890, training loss: 313.5652770996094 = 0.3377590477466583 + 50.0 * 6.26455020904541
Epoch 890, val loss: 0.9336270689964294
Epoch 900, training loss: 313.57952880859375 = 0.32933762669563293 + 50.0 * 6.265003681182861
Epoch 900, val loss: 0.9365723729133606
Epoch 910, training loss: 313.4617919921875 = 0.32107725739479065 + 50.0 * 6.262814044952393
Epoch 910, val loss: 0.9396207332611084
Epoch 920, training loss: 313.3543395996094 = 0.31298550963401794 + 50.0 * 6.26082706451416
Epoch 920, val loss: 0.9426625370979309
Epoch 930, training loss: 313.5093994140625 = 0.3050670027732849 + 50.0 * 6.264087200164795
Epoch 930, val loss: 0.9460620880126953
Epoch 940, training loss: 313.4084777832031 = 0.29720059037208557 + 50.0 * 6.262225151062012
Epoch 940, val loss: 0.9490585923194885
Epoch 950, training loss: 313.33416748046875 = 0.2895219027996063 + 50.0 * 6.260892868041992
Epoch 950, val loss: 0.9525694847106934
Epoch 960, training loss: 313.40655517578125 = 0.2819695472717285 + 50.0 * 6.262491226196289
Epoch 960, val loss: 0.9560273289680481
Epoch 970, training loss: 313.1761169433594 = 0.27455011010169983 + 50.0 * 6.258031845092773
Epoch 970, val loss: 0.959549605846405
Epoch 980, training loss: 313.1077575683594 = 0.2673339545726776 + 50.0 * 6.256808280944824
Epoch 980, val loss: 0.9632734656333923
Epoch 990, training loss: 313.6179504394531 = 0.260226845741272 + 50.0 * 6.267154693603516
Epoch 990, val loss: 0.9667441844940186
Epoch 1000, training loss: 313.1361389160156 = 0.2532978951931 + 50.0 * 6.257656574249268
Epoch 1000, val loss: 0.9707096815109253
Epoch 1010, training loss: 312.986328125 = 0.24646151065826416 + 50.0 * 6.254797458648682
Epoch 1010, val loss: 0.9747223854064941
Epoch 1020, training loss: 313.1430358886719 = 0.23981373012065887 + 50.0 * 6.2580647468566895
Epoch 1020, val loss: 0.9786170125007629
Epoch 1030, training loss: 312.93994140625 = 0.2333202064037323 + 50.0 * 6.2541327476501465
Epoch 1030, val loss: 0.9825637340545654
Epoch 1040, training loss: 312.96466064453125 = 0.22696419060230255 + 50.0 * 6.254754066467285
Epoch 1040, val loss: 0.9869060516357422
Epoch 1050, training loss: 312.9106140136719 = 0.22077463567256927 + 50.0 * 6.2537970542907715
Epoch 1050, val loss: 0.9909381866455078
Epoch 1060, training loss: 312.7967529296875 = 0.21473544836044312 + 50.0 * 6.251640796661377
Epoch 1060, val loss: 0.9954482316970825
Epoch 1070, training loss: 312.8035583496094 = 0.20886103808879852 + 50.0 * 6.251893997192383
Epoch 1070, val loss: 0.9997537136077881
Epoch 1080, training loss: 312.95758056640625 = 0.2031446099281311 + 50.0 * 6.2550883293151855
Epoch 1080, val loss: 1.0040885210037231
Epoch 1090, training loss: 312.7532958984375 = 0.19757072627544403 + 50.0 * 6.251114368438721
Epoch 1090, val loss: 1.0086013078689575
Epoch 1100, training loss: 312.6695556640625 = 0.19215567409992218 + 50.0 * 6.249547481536865
Epoch 1100, val loss: 1.0133482217788696
Epoch 1110, training loss: 312.64190673828125 = 0.18691770732402802 + 50.0 * 6.2490997314453125
Epoch 1110, val loss: 1.0180245637893677
Epoch 1120, training loss: 312.8723449707031 = 0.1818823665380478 + 50.0 * 6.253809452056885
Epoch 1120, val loss: 1.0230082273483276
Epoch 1130, training loss: 312.7239074707031 = 0.17688196897506714 + 50.0 * 6.250940322875977
Epoch 1130, val loss: 1.0273361206054688
Epoch 1140, training loss: 312.5839538574219 = 0.17208659648895264 + 50.0 * 6.248237133026123
Epoch 1140, val loss: 1.032467246055603
Epoch 1150, training loss: 312.5275573730469 = 0.16742043197155 + 50.0 * 6.2472028732299805
Epoch 1150, val loss: 1.0374537706375122
Epoch 1160, training loss: 312.9598693847656 = 0.16292671859264374 + 50.0 * 6.255938529968262
Epoch 1160, val loss: 1.0424869060516357
Epoch 1170, training loss: 312.5108337402344 = 0.15850454568862915 + 50.0 * 6.24704647064209
Epoch 1170, val loss: 1.0477192401885986
Epoch 1180, training loss: 312.41314697265625 = 0.15423890948295593 + 50.0 * 6.24517822265625
Epoch 1180, val loss: 1.0527170896530151
Epoch 1190, training loss: 312.358154296875 = 0.15014606714248657 + 50.0 * 6.244160175323486
Epoch 1190, val loss: 1.0580930709838867
Epoch 1200, training loss: 312.41107177734375 = 0.14619484543800354 + 50.0 * 6.245297431945801
Epoch 1200, val loss: 1.0635064840316772
Epoch 1210, training loss: 312.3471984863281 = 0.14229954779148102 + 50.0 * 6.244097709655762
Epoch 1210, val loss: 1.0686472654342651
Epoch 1220, training loss: 312.39239501953125 = 0.1385452002286911 + 50.0 * 6.245076656341553
Epoch 1220, val loss: 1.074036955833435
Epoch 1230, training loss: 312.3460998535156 = 0.13488920032978058 + 50.0 * 6.2442240715026855
Epoch 1230, val loss: 1.0796105861663818
Epoch 1240, training loss: 312.3218078613281 = 0.1313742846250534 + 50.0 * 6.243808746337891
Epoch 1240, val loss: 1.0852519273757935
Epoch 1250, training loss: 312.32769775390625 = 0.1279587298631668 + 50.0 * 6.24399471282959
Epoch 1250, val loss: 1.0907008647918701
Epoch 1260, training loss: 312.3659362792969 = 0.12462764233350754 + 50.0 * 6.244825839996338
Epoch 1260, val loss: 1.0962436199188232
Epoch 1270, training loss: 312.2230224609375 = 0.12141342461109161 + 50.0 * 6.242032527923584
Epoch 1270, val loss: 1.101943850517273
Epoch 1280, training loss: 312.2074890136719 = 0.11829359084367752 + 50.0 * 6.24178409576416
Epoch 1280, val loss: 1.107656478881836
Epoch 1290, training loss: 312.3048400878906 = 0.11526434868574142 + 50.0 * 6.243791580200195
Epoch 1290, val loss: 1.1131993532180786
Epoch 1300, training loss: 312.2099609375 = 0.1123206689953804 + 50.0 * 6.241952419281006
Epoch 1300, val loss: 1.1190682649612427
Epoch 1310, training loss: 312.1019287109375 = 0.10945362597703934 + 50.0 * 6.239849090576172
Epoch 1310, val loss: 1.1249855756759644
Epoch 1320, training loss: 312.0567626953125 = 0.10670708119869232 + 50.0 * 6.2390007972717285
Epoch 1320, val loss: 1.130774736404419
Epoch 1330, training loss: 312.1199645996094 = 0.10404348373413086 + 50.0 * 6.240318775177002
Epoch 1330, val loss: 1.1367466449737549
Epoch 1340, training loss: 312.1304016113281 = 0.10143318772315979 + 50.0 * 6.240579128265381
Epoch 1340, val loss: 1.1425611972808838
Epoch 1350, training loss: 312.1429138183594 = 0.09889154136180878 + 50.0 * 6.240880966186523
Epoch 1350, val loss: 1.14841890335083
Epoch 1360, training loss: 312.11212158203125 = 0.09644932299852371 + 50.0 * 6.240313529968262
Epoch 1360, val loss: 1.154059886932373
Epoch 1370, training loss: 311.9655456542969 = 0.0940612405538559 + 50.0 * 6.237430095672607
Epoch 1370, val loss: 1.160210371017456
Epoch 1380, training loss: 311.94696044921875 = 0.09176994115114212 + 50.0 * 6.2371039390563965
Epoch 1380, val loss: 1.1664481163024902
Epoch 1390, training loss: 312.18316650390625 = 0.08954308927059174 + 50.0 * 6.241872310638428
Epoch 1390, val loss: 1.1722376346588135
Epoch 1400, training loss: 312.05706787109375 = 0.08735031634569168 + 50.0 * 6.239394664764404
Epoch 1400, val loss: 1.1782938241958618
Epoch 1410, training loss: 312.0392150878906 = 0.08523454517126083 + 50.0 * 6.239079475402832
Epoch 1410, val loss: 1.1844792366027832
Epoch 1420, training loss: 311.89862060546875 = 0.08318347483873367 + 50.0 * 6.236308574676514
Epoch 1420, val loss: 1.1903326511383057
Epoch 1430, training loss: 311.8362731933594 = 0.08117683231830597 + 50.0 * 6.235101699829102
Epoch 1430, val loss: 1.1963039636611938
Epoch 1440, training loss: 311.7982482910156 = 0.07925425469875336 + 50.0 * 6.234379768371582
Epoch 1440, val loss: 1.202409267425537
Epoch 1450, training loss: 311.8702087402344 = 0.07739301025867462 + 50.0 * 6.235856533050537
Epoch 1450, val loss: 1.208714485168457
Epoch 1460, training loss: 312.06304931640625 = 0.07558920979499817 + 50.0 * 6.239749431610107
Epoch 1460, val loss: 1.2145720720291138
Epoch 1470, training loss: 311.80029296875 = 0.07377078384160995 + 50.0 * 6.234530448913574
Epoch 1470, val loss: 1.2202874422073364
Epoch 1480, training loss: 311.74810791015625 = 0.0720478743314743 + 50.0 * 6.233520984649658
Epoch 1480, val loss: 1.2264900207519531
Epoch 1490, training loss: 311.711181640625 = 0.07037409394979477 + 50.0 * 6.232816219329834
Epoch 1490, val loss: 1.232440710067749
Epoch 1500, training loss: 311.71728515625 = 0.06876950711011887 + 50.0 * 6.232970714569092
Epoch 1500, val loss: 1.2385128736495972
Epoch 1510, training loss: 311.9853515625 = 0.06719521433115005 + 50.0 * 6.238362789154053
Epoch 1510, val loss: 1.244646668434143
Epoch 1520, training loss: 311.85968017578125 = 0.06566095352172852 + 50.0 * 6.235879898071289
Epoch 1520, val loss: 1.2501734495162964
Epoch 1530, training loss: 311.6291809082031 = 0.06413302570581436 + 50.0 * 6.2313008308410645
Epoch 1530, val loss: 1.2560296058654785
Epoch 1540, training loss: 311.63262939453125 = 0.06269241124391556 + 50.0 * 6.231399059295654
Epoch 1540, val loss: 1.2618985176086426
Epoch 1550, training loss: 311.5892639160156 = 0.061302974820137024 + 50.0 * 6.230559349060059
Epoch 1550, val loss: 1.2678834199905396
Epoch 1560, training loss: 312.1845703125 = 0.05995083600282669 + 50.0 * 6.24249267578125
Epoch 1560, val loss: 1.2736815214157104
Epoch 1570, training loss: 311.6985168457031 = 0.058612383902072906 + 50.0 * 6.232797622680664
Epoch 1570, val loss: 1.2795692682266235
Epoch 1580, training loss: 311.5195617675781 = 0.057313669472932816 + 50.0 * 6.229245185852051
Epoch 1580, val loss: 1.2854152917861938
Epoch 1590, training loss: 311.5052795410156 = 0.05607064813375473 + 50.0 * 6.228984355926514
Epoch 1590, val loss: 1.2912267446517944
Epoch 1600, training loss: 311.675537109375 = 0.054872483015060425 + 50.0 * 6.232413291931152
Epoch 1600, val loss: 1.2970765829086304
Epoch 1610, training loss: 311.5281677246094 = 0.053677696734666824 + 50.0 * 6.229489803314209
Epoch 1610, val loss: 1.3028674125671387
Epoch 1620, training loss: 311.5463562011719 = 0.052513252943754196 + 50.0 * 6.22987699508667
Epoch 1620, val loss: 1.3082773685455322
Epoch 1630, training loss: 311.5473327636719 = 0.051392294466495514 + 50.0 * 6.229918956756592
Epoch 1630, val loss: 1.3143149614334106
Epoch 1640, training loss: 311.5429382324219 = 0.05031565949320793 + 50.0 * 6.229852199554443
Epoch 1640, val loss: 1.3199386596679688
Epoch 1650, training loss: 311.4834899902344 = 0.04926103726029396 + 50.0 * 6.228684425354004
Epoch 1650, val loss: 1.3256785869598389
Epoch 1660, training loss: 311.50048828125 = 0.04823514074087143 + 50.0 * 6.2290449142456055
Epoch 1660, val loss: 1.331235647201538
Epoch 1670, training loss: 311.5774230957031 = 0.04722883552312851 + 50.0 * 6.2306036949157715
Epoch 1670, val loss: 1.3366554975509644
Epoch 1680, training loss: 311.4276123046875 = 0.046245962381362915 + 50.0 * 6.227627754211426
Epoch 1680, val loss: 1.3422446250915527
Epoch 1690, training loss: 311.4044494628906 = 0.045304182916879654 + 50.0 * 6.227182865142822
Epoch 1690, val loss: 1.3478790521621704
Epoch 1700, training loss: 311.4421691894531 = 0.04438590258359909 + 50.0 * 6.2279558181762695
Epoch 1700, val loss: 1.3534948825836182
Epoch 1710, training loss: 311.6535339355469 = 0.04350249841809273 + 50.0 * 6.232201099395752
Epoch 1710, val loss: 1.3587037324905396
Epoch 1720, training loss: 311.45947265625 = 0.04261596128344536 + 50.0 * 6.228337287902832
Epoch 1720, val loss: 1.3638911247253418
Epoch 1730, training loss: 311.3692321777344 = 0.04176279902458191 + 50.0 * 6.2265496253967285
Epoch 1730, val loss: 1.369640827178955
Epoch 1740, training loss: 311.3963317871094 = 0.04094121232628822 + 50.0 * 6.227107524871826
Epoch 1740, val loss: 1.3747652769088745
Epoch 1750, training loss: 311.4000244140625 = 0.04014233127236366 + 50.0 * 6.227197647094727
Epoch 1750, val loss: 1.3804552555084229
Epoch 1760, training loss: 311.39764404296875 = 0.03936709463596344 + 50.0 * 6.227165222167969
Epoch 1760, val loss: 1.3858405351638794
Epoch 1770, training loss: 311.3048400878906 = 0.03859047219157219 + 50.0 * 6.225324630737305
Epoch 1770, val loss: 1.390778660774231
Epoch 1780, training loss: 311.34417724609375 = 0.037852391600608826 + 50.0 * 6.226126670837402
Epoch 1780, val loss: 1.396308422088623
Epoch 1790, training loss: 311.62939453125 = 0.03713398054242134 + 50.0 * 6.231844902038574
Epoch 1790, val loss: 1.4011329412460327
Epoch 1800, training loss: 311.3814697265625 = 0.03641875460743904 + 50.0 * 6.226901531219482
Epoch 1800, val loss: 1.4065005779266357
Epoch 1810, training loss: 311.2372741699219 = 0.03572295233607292 + 50.0 * 6.2240309715271
Epoch 1810, val loss: 1.4114810228347778
Epoch 1820, training loss: 311.1893615722656 = 0.035059813410043716 + 50.0 * 6.223085880279541
Epoch 1820, val loss: 1.416940689086914
Epoch 1830, training loss: 311.30938720703125 = 0.034416306763887405 + 50.0 * 6.225499153137207
Epoch 1830, val loss: 1.4218286275863647
Epoch 1840, training loss: 311.2293395996094 = 0.03377613425254822 + 50.0 * 6.223911285400391
Epoch 1840, val loss: 1.4268771409988403
Epoch 1850, training loss: 311.30780029296875 = 0.033155810087919235 + 50.0 * 6.225493431091309
Epoch 1850, val loss: 1.4319583177566528
Epoch 1860, training loss: 311.2419128417969 = 0.03254583105444908 + 50.0 * 6.224187850952148
Epoch 1860, val loss: 1.436927318572998
Epoch 1870, training loss: 311.2577819824219 = 0.03195074200630188 + 50.0 * 6.224516868591309
Epoch 1870, val loss: 1.4419807195663452
Epoch 1880, training loss: 311.2116394042969 = 0.03137827664613724 + 50.0 * 6.223605155944824
Epoch 1880, val loss: 1.4467476606369019
Epoch 1890, training loss: 311.1478271484375 = 0.030820991843938828 + 50.0 * 6.222340106964111
Epoch 1890, val loss: 1.4519224166870117
Epoch 1900, training loss: 311.1077880859375 = 0.030279705300927162 + 50.0 * 6.221550464630127
Epoch 1900, val loss: 1.4567428827285767
Epoch 1910, training loss: 311.2559509277344 = 0.029756521806120872 + 50.0 * 6.224524021148682
Epoch 1910, val loss: 1.461523175239563
Epoch 1920, training loss: 311.178466796875 = 0.029231544584035873 + 50.0 * 6.222984790802002
Epoch 1920, val loss: 1.4663246870040894
Epoch 1930, training loss: 311.2373962402344 = 0.028715822845697403 + 50.0 * 6.224173545837402
Epoch 1930, val loss: 1.470900297164917
Epoch 1940, training loss: 311.0462341308594 = 0.02822292409837246 + 50.0 * 6.220360279083252
Epoch 1940, val loss: 1.4759864807128906
Epoch 1950, training loss: 311.0760192871094 = 0.02774745225906372 + 50.0 * 6.220965385437012
Epoch 1950, val loss: 1.4807250499725342
Epoch 1960, training loss: 311.1719970703125 = 0.027282139286398888 + 50.0 * 6.222894191741943
Epoch 1960, val loss: 1.4856334924697876
Epoch 1970, training loss: 311.101806640625 = 0.026814084500074387 + 50.0 * 6.221499919891357
Epoch 1970, val loss: 1.4898942708969116
Epoch 1980, training loss: 311.14300537109375 = 0.026364721357822418 + 50.0 * 6.222332954406738
Epoch 1980, val loss: 1.4942835569381714
Epoch 1990, training loss: 311.1363830566406 = 0.02592509053647518 + 50.0 * 6.2222089767456055
Epoch 1990, val loss: 1.499213695526123
Epoch 2000, training loss: 311.1473388671875 = 0.025496626272797585 + 50.0 * 6.222437381744385
Epoch 2000, val loss: 1.5036859512329102
Epoch 2010, training loss: 311.0853271484375 = 0.025081563740968704 + 50.0 * 6.22120475769043
Epoch 2010, val loss: 1.508275032043457
Epoch 2020, training loss: 310.9718017578125 = 0.024669447913765907 + 50.0 * 6.218942642211914
Epoch 2020, val loss: 1.512669563293457
Epoch 2030, training loss: 311.0072021484375 = 0.024272670969367027 + 50.0 * 6.219658851623535
Epoch 2030, val loss: 1.5173773765563965
Epoch 2040, training loss: 311.15374755859375 = 0.02389076165854931 + 50.0 * 6.222597122192383
Epoch 2040, val loss: 1.5213875770568848
Epoch 2050, training loss: 311.0818786621094 = 0.023503826931118965 + 50.0 * 6.22116756439209
Epoch 2050, val loss: 1.5261129140853882
Epoch 2060, training loss: 311.00872802734375 = 0.023125475272536278 + 50.0 * 6.219711780548096
Epoch 2060, val loss: 1.5304433107376099
Epoch 2070, training loss: 311.0726623535156 = 0.022765029221773148 + 50.0 * 6.2209978103637695
Epoch 2070, val loss: 1.534759521484375
Epoch 2080, training loss: 310.9324035644531 = 0.022408582270145416 + 50.0 * 6.218200206756592
Epoch 2080, val loss: 1.5392252206802368
Epoch 2090, training loss: 311.0211486816406 = 0.022064588963985443 + 50.0 * 6.219981670379639
Epoch 2090, val loss: 1.543680191040039
Epoch 2100, training loss: 310.956787109375 = 0.021719150245189667 + 50.0 * 6.218701362609863
Epoch 2100, val loss: 1.5477474927902222
Epoch 2110, training loss: 310.9375915527344 = 0.021386640146374702 + 50.0 * 6.218324184417725
Epoch 2110, val loss: 1.5516085624694824
Epoch 2120, training loss: 310.9917297363281 = 0.021062767133116722 + 50.0 * 6.2194132804870605
Epoch 2120, val loss: 1.556131362915039
Epoch 2130, training loss: 310.8843078613281 = 0.02074762061238289 + 50.0 * 6.217271327972412
Epoch 2130, val loss: 1.5602178573608398
Epoch 2140, training loss: 311.0818176269531 = 0.020445315167307854 + 50.0 * 6.221227169036865
Epoch 2140, val loss: 1.5647634267807007
Epoch 2150, training loss: 310.9232482910156 = 0.020134514197707176 + 50.0 * 6.218061923980713
Epoch 2150, val loss: 1.5684629678726196
Epoch 2160, training loss: 311.0859375 = 0.019839264452457428 + 50.0 * 6.221322059631348
Epoch 2160, val loss: 1.5725934505462646
Epoch 2170, training loss: 310.8644104003906 = 0.01954035833477974 + 50.0 * 6.216897487640381
Epoch 2170, val loss: 1.576209545135498
Epoch 2180, training loss: 310.84698486328125 = 0.0192571934312582 + 50.0 * 6.216554641723633
Epoch 2180, val loss: 1.5808181762695312
Epoch 2190, training loss: 310.8224792480469 = 0.018982086330652237 + 50.0 * 6.21606969833374
Epoch 2190, val loss: 1.5846424102783203
Epoch 2200, training loss: 311.0716247558594 = 0.018715698271989822 + 50.0 * 6.221058368682861
Epoch 2200, val loss: 1.5884531736373901
Epoch 2210, training loss: 310.833251953125 = 0.01844373717904091 + 50.0 * 6.216296195983887
Epoch 2210, val loss: 1.5924793481826782
Epoch 2220, training loss: 310.83905029296875 = 0.018185526132583618 + 50.0 * 6.21641731262207
Epoch 2220, val loss: 1.5964505672454834
Epoch 2230, training loss: 310.8626708984375 = 0.017930490896105766 + 50.0 * 6.216894626617432
Epoch 2230, val loss: 1.600118637084961
Epoch 2240, training loss: 310.8448791503906 = 0.017676332965493202 + 50.0 * 6.216544151306152
Epoch 2240, val loss: 1.604174017906189
Epoch 2250, training loss: 310.8944091796875 = 0.017433704808354378 + 50.0 * 6.2175397872924805
Epoch 2250, val loss: 1.608068823814392
Epoch 2260, training loss: 310.840576171875 = 0.017194783315062523 + 50.0 * 6.21646785736084
Epoch 2260, val loss: 1.6119011640548706
Epoch 2270, training loss: 310.7671203613281 = 0.01695665530860424 + 50.0 * 6.21500301361084
Epoch 2270, val loss: 1.6154979467391968
Epoch 2280, training loss: 310.73724365234375 = 0.01672758162021637 + 50.0 * 6.214410305023193
Epoch 2280, val loss: 1.6191179752349854
Epoch 2290, training loss: 310.86981201171875 = 0.016504770144820213 + 50.0 * 6.217066287994385
Epoch 2290, val loss: 1.6227836608886719
Epoch 2300, training loss: 310.8329772949219 = 0.01628175750374794 + 50.0 * 6.216333866119385
Epoch 2300, val loss: 1.6267828941345215
Epoch 2310, training loss: 310.8662109375 = 0.016062159091234207 + 50.0 * 6.217002868652344
Epoch 2310, val loss: 1.629608154296875
Epoch 2320, training loss: 310.7916259765625 = 0.015842292457818985 + 50.0 * 6.215515613555908
Epoch 2320, val loss: 1.6340206861495972
Epoch 2330, training loss: 310.68597412109375 = 0.015634387731552124 + 50.0 * 6.213407039642334
Epoch 2330, val loss: 1.6374835968017578
Epoch 2340, training loss: 310.6911315917969 = 0.01542956754565239 + 50.0 * 6.2135138511657715
Epoch 2340, val loss: 1.6414341926574707
Epoch 2350, training loss: 311.0689697265625 = 0.015232913196086884 + 50.0 * 6.22107458114624
Epoch 2350, val loss: 1.6447778940200806
Epoch 2360, training loss: 310.7462463378906 = 0.015027042478322983 + 50.0 * 6.214624881744385
Epoch 2360, val loss: 1.6481767892837524
Epoch 2370, training loss: 310.64459228515625 = 0.014830810017883778 + 50.0 * 6.212594985961914
Epoch 2370, val loss: 1.6517914533615112
Epoch 2380, training loss: 310.6314697265625 = 0.014642723836004734 + 50.0 * 6.212336540222168
Epoch 2380, val loss: 1.655484676361084
Epoch 2390, training loss: 310.9950256347656 = 0.014463203959167004 + 50.0 * 6.219611167907715
Epoch 2390, val loss: 1.658738374710083
Epoch 2400, training loss: 310.61822509765625 = 0.014274737797677517 + 50.0 * 6.21207857131958
Epoch 2400, val loss: 1.662235140800476
Epoch 2410, training loss: 310.57733154296875 = 0.014094657264649868 + 50.0 * 6.211264610290527
Epoch 2410, val loss: 1.6658984422683716
Epoch 2420, training loss: 310.6568298339844 = 0.013924883678555489 + 50.0 * 6.212858200073242
Epoch 2420, val loss: 1.6693543195724487
Epoch 2430, training loss: 310.9191589355469 = 0.01375727728009224 + 50.0 * 6.218108177185059
Epoch 2430, val loss: 1.673058032989502
Epoch 2440, training loss: 310.6744384765625 = 0.01357452291995287 + 50.0 * 6.213217258453369
Epoch 2440, val loss: 1.67569100856781
Epoch 2450, training loss: 310.60418701171875 = 0.013411343097686768 + 50.0 * 6.21181583404541
Epoch 2450, val loss: 1.679353952407837
Epoch 2460, training loss: 310.6063537597656 = 0.01324861403554678 + 50.0 * 6.211862087249756
Epoch 2460, val loss: 1.682744026184082
Epoch 2470, training loss: 310.5635681152344 = 0.013091152533888817 + 50.0 * 6.211009502410889
Epoch 2470, val loss: 1.685965895652771
Epoch 2480, training loss: 310.8030700683594 = 0.01293833926320076 + 50.0 * 6.2158026695251465
Epoch 2480, val loss: 1.6890983581542969
Epoch 2490, training loss: 310.62542724609375 = 0.012783833779394627 + 50.0 * 6.212253093719482
Epoch 2490, val loss: 1.6923733949661255
Epoch 2500, training loss: 310.58026123046875 = 0.012630780227482319 + 50.0 * 6.211352825164795
Epoch 2500, val loss: 1.6955204010009766
Epoch 2510, training loss: 310.6491394042969 = 0.012483320198953152 + 50.0 * 6.212733268737793
Epoch 2510, val loss: 1.6987230777740479
Epoch 2520, training loss: 310.59686279296875 = 0.01233724132180214 + 50.0 * 6.211690425872803
Epoch 2520, val loss: 1.701802372932434
Epoch 2530, training loss: 310.71881103515625 = 0.012199169024825096 + 50.0 * 6.214132308959961
Epoch 2530, val loss: 1.7052291631698608
Epoch 2540, training loss: 310.6129455566406 = 0.012058837339282036 + 50.0 * 6.212018013000488
Epoch 2540, val loss: 1.7084838151931763
Epoch 2550, training loss: 310.5075378417969 = 0.011918379925191402 + 50.0 * 6.209911823272705
Epoch 2550, val loss: 1.7114754915237427
Epoch 2560, training loss: 310.5176696777344 = 0.011785791255533695 + 50.0 * 6.210117816925049
Epoch 2560, val loss: 1.7145521640777588
Epoch 2570, training loss: 310.593017578125 = 0.011659097857773304 + 50.0 * 6.21162748336792
Epoch 2570, val loss: 1.7179228067398071
Epoch 2580, training loss: 310.59051513671875 = 0.01152767799794674 + 50.0 * 6.2115797996521
Epoch 2580, val loss: 1.7206192016601562
Epoch 2590, training loss: 310.5695495605469 = 0.01139876525849104 + 50.0 * 6.211162567138672
Epoch 2590, val loss: 1.7235772609710693
Epoch 2600, training loss: 310.60528564453125 = 0.011275775730609894 + 50.0 * 6.211880207061768
Epoch 2600, val loss: 1.7262603044509888
Epoch 2610, training loss: 310.702392578125 = 0.011151698417961597 + 50.0 * 6.213825225830078
Epoch 2610, val loss: 1.7291102409362793
Epoch 2620, training loss: 310.5392761230469 = 0.011030687019228935 + 50.0 * 6.210565090179443
Epoch 2620, val loss: 1.7326375246047974
Epoch 2630, training loss: 310.49090576171875 = 0.010910846292972565 + 50.0 * 6.209599494934082
Epoch 2630, val loss: 1.735237956047058
Epoch 2640, training loss: 310.59173583984375 = 0.01079584937542677 + 50.0 * 6.211618423461914
Epoch 2640, val loss: 1.738482117652893
Epoch 2650, training loss: 310.451171875 = 0.010680880397558212 + 50.0 * 6.208809852600098
Epoch 2650, val loss: 1.741026759147644
Epoch 2660, training loss: 310.4921875 = 0.010570264421403408 + 50.0 * 6.209632873535156
Epoch 2660, val loss: 1.7440288066864014
Epoch 2670, training loss: 310.51702880859375 = 0.010461266152560711 + 50.0 * 6.2101311683654785
Epoch 2670, val loss: 1.7469929456710815
Epoch 2680, training loss: 310.4884948730469 = 0.010354626923799515 + 50.0 * 6.209563255310059
Epoch 2680, val loss: 1.7500172853469849
Epoch 2690, training loss: 310.5697937011719 = 0.010250141844153404 + 50.0 * 6.211190700531006
Epoch 2690, val loss: 1.7529559135437012
Epoch 2700, training loss: 310.487548828125 = 0.010141627863049507 + 50.0 * 6.209548473358154
Epoch 2700, val loss: 1.754912257194519
Epoch 2710, training loss: 310.46661376953125 = 0.010037166997790337 + 50.0 * 6.209131240844727
Epoch 2710, val loss: 1.7582802772521973
Epoch 2720, training loss: 310.5154724121094 = 0.009939274750649929 + 50.0 * 6.210110664367676
Epoch 2720, val loss: 1.7608307600021362
Epoch 2730, training loss: 310.41107177734375 = 0.009838195517659187 + 50.0 * 6.208024978637695
Epoch 2730, val loss: 1.7637935876846313
Epoch 2740, training loss: 310.560791015625 = 0.00974350981414318 + 50.0 * 6.2110209465026855
Epoch 2740, val loss: 1.7666809558868408
Epoch 2750, training loss: 310.37646484375 = 0.009643164463341236 + 50.0 * 6.20733642578125
Epoch 2750, val loss: 1.7692137956619263
Epoch 2760, training loss: 310.4490661621094 = 0.00955037772655487 + 50.0 * 6.208790302276611
Epoch 2760, val loss: 1.7717808485031128
Epoch 2770, training loss: 310.46142578125 = 0.009459446184337139 + 50.0 * 6.209039211273193
Epoch 2770, val loss: 1.7745193243026733
Epoch 2780, training loss: 310.421142578125 = 0.009367762133479118 + 50.0 * 6.208235740661621
Epoch 2780, val loss: 1.7775269746780396
Epoch 2790, training loss: 310.585693359375 = 0.009279723279178143 + 50.0 * 6.211528301239014
Epoch 2790, val loss: 1.7799007892608643
Epoch 2800, training loss: 310.4370422363281 = 0.009187674149870872 + 50.0 * 6.20855712890625
Epoch 2800, val loss: 1.7825536727905273
Epoch 2810, training loss: 310.33477783203125 = 0.009098767302930355 + 50.0 * 6.20651388168335
Epoch 2810, val loss: 1.7853593826293945
Epoch 2820, training loss: 310.3062438964844 = 0.00901428610086441 + 50.0 * 6.205945014953613
Epoch 2820, val loss: 1.7879478931427002
Epoch 2830, training loss: 310.3136901855469 = 0.008932635188102722 + 50.0 * 6.206094741821289
Epoch 2830, val loss: 1.7905640602111816
Epoch 2840, training loss: 310.543701171875 = 0.008853991515934467 + 50.0 * 6.210697174072266
Epoch 2840, val loss: 1.7927908897399902
Epoch 2850, training loss: 310.47332763671875 = 0.008771151304244995 + 50.0 * 6.209291458129883
Epoch 2850, val loss: 1.7953345775604248
Epoch 2860, training loss: 310.3030700683594 = 0.008687833324074745 + 50.0 * 6.205887317657471
Epoch 2860, val loss: 1.797996997833252
Epoch 2870, training loss: 310.2772216796875 = 0.008608940988779068 + 50.0 * 6.205372333526611
Epoch 2870, val loss: 1.8005269765853882
Epoch 2880, training loss: 310.2646484375 = 0.00853276252746582 + 50.0 * 6.205121994018555
Epoch 2880, val loss: 1.8031078577041626
Epoch 2890, training loss: 310.4835205078125 = 0.00846286304295063 + 50.0 * 6.209501266479492
Epoch 2890, val loss: 1.8056844472885132
Epoch 2900, training loss: 310.3425598144531 = 0.008385183289647102 + 50.0 * 6.20668363571167
Epoch 2900, val loss: 1.8081814050674438
Epoch 2910, training loss: 310.3108215332031 = 0.008310128934681416 + 50.0 * 6.206050395965576
Epoch 2910, val loss: 1.8106093406677246
Epoch 2920, training loss: 310.36846923828125 = 0.008236445486545563 + 50.0 * 6.207204341888428
Epoch 2920, val loss: 1.8131675720214844
Epoch 2930, training loss: 310.2377014160156 = 0.008164222352206707 + 50.0 * 6.204590320587158
Epoch 2930, val loss: 1.8154963254928589
Epoch 2940, training loss: 310.4079284667969 = 0.00809531845152378 + 50.0 * 6.207996845245361
Epoch 2940, val loss: 1.8176859617233276
Epoch 2950, training loss: 310.2670593261719 = 0.00802376214414835 + 50.0 * 6.205180644989014
Epoch 2950, val loss: 1.820225477218628
Epoch 2960, training loss: 310.2584533691406 = 0.007956018671393394 + 50.0 * 6.205009460449219
Epoch 2960, val loss: 1.8227521181106567
Epoch 2970, training loss: 310.2615966796875 = 0.00788951851427555 + 50.0 * 6.205073833465576
Epoch 2970, val loss: 1.8249332904815674
Epoch 2980, training loss: 310.4146728515625 = 0.00782631617039442 + 50.0 * 6.208137512207031
Epoch 2980, val loss: 1.8272895812988281
Epoch 2990, training loss: 310.28363037109375 = 0.007759890053421259 + 50.0 * 6.205517768859863
Epoch 2990, val loss: 1.8291804790496826
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 431.800048828125 = 1.9564929008483887 + 50.0 * 8.596871376037598
Epoch 0, val loss: 1.9574897289276123
Epoch 10, training loss: 431.76190185546875 = 1.9474955797195435 + 50.0 * 8.596287727355957
Epoch 10, val loss: 1.9487272500991821
Epoch 20, training loss: 431.5374755859375 = 1.936511754989624 + 50.0 * 8.592019081115723
Epoch 20, val loss: 1.9374873638153076
Epoch 30, training loss: 430.0509338378906 = 1.9225515127182007 + 50.0 * 8.562567710876465
Epoch 30, val loss: 1.9229027032852173
Epoch 40, training loss: 421.1952819824219 = 1.9053871631622314 + 50.0 * 8.385797500610352
Epoch 40, val loss: 1.9052106142044067
Epoch 50, training loss: 392.8820495605469 = 1.8873103857040405 + 50.0 * 7.819895267486572
Epoch 50, val loss: 1.8866239786148071
Epoch 60, training loss: 380.6680908203125 = 1.8682972192764282 + 50.0 * 7.575995922088623
Epoch 60, val loss: 1.8685449361801147
Epoch 70, training loss: 368.32843017578125 = 1.8528608083724976 + 50.0 * 7.3295111656188965
Epoch 70, val loss: 1.8550533056259155
Epoch 80, training loss: 359.2644958496094 = 1.839455485343933 + 50.0 * 7.148500919342041
Epoch 80, val loss: 1.8430143594741821
Epoch 90, training loss: 353.51141357421875 = 1.8284715414047241 + 50.0 * 7.033658981323242
Epoch 90, val loss: 1.8332552909851074
Epoch 100, training loss: 348.12744140625 = 1.8172062635421753 + 50.0 * 6.926204681396484
Epoch 100, val loss: 1.8232520818710327
Epoch 110, training loss: 344.95281982421875 = 1.8055980205535889 + 50.0 * 6.86294412612915
Epoch 110, val loss: 1.8130621910095215
Epoch 120, training loss: 342.27349853515625 = 1.794751763343811 + 50.0 * 6.809575080871582
Epoch 120, val loss: 1.8035246133804321
Epoch 130, training loss: 339.78070068359375 = 1.7836912870407104 + 50.0 * 6.759940147399902
Epoch 130, val loss: 1.7939964532852173
Epoch 140, training loss: 337.58807373046875 = 1.7728009223937988 + 50.0 * 6.716305732727051
Epoch 140, val loss: 1.7844430208206177
Epoch 150, training loss: 335.86724853515625 = 1.761767029762268 + 50.0 * 6.682109355926514
Epoch 150, val loss: 1.774674654006958
Epoch 160, training loss: 334.4201354980469 = 1.7498775720596313 + 50.0 * 6.65340518951416
Epoch 160, val loss: 1.7644429206848145
Epoch 170, training loss: 332.50860595703125 = 1.7374564409255981 + 50.0 * 6.615423202514648
Epoch 170, val loss: 1.7536804676055908
Epoch 180, training loss: 330.9980163574219 = 1.7245194911956787 + 50.0 * 6.585470199584961
Epoch 180, val loss: 1.742627501487732
Epoch 190, training loss: 329.6609191894531 = 1.7105498313903809 + 50.0 * 6.55900764465332
Epoch 190, val loss: 1.730630874633789
Epoch 200, training loss: 328.8905334472656 = 1.6950280666351318 + 50.0 * 6.543910026550293
Epoch 200, val loss: 1.7174640893936157
Epoch 210, training loss: 327.7474060058594 = 1.677828311920166 + 50.0 * 6.521391868591309
Epoch 210, val loss: 1.70289146900177
Epoch 220, training loss: 327.0326843261719 = 1.659084677696228 + 50.0 * 6.507472038269043
Epoch 220, val loss: 1.6870629787445068
Epoch 230, training loss: 326.3582458496094 = 1.6388143301010132 + 50.0 * 6.494388580322266
Epoch 230, val loss: 1.6700351238250732
Epoch 240, training loss: 325.7478332519531 = 1.6171456575393677 + 50.0 * 6.482613563537598
Epoch 240, val loss: 1.6518820524215698
Epoch 250, training loss: 325.3946228027344 = 1.5942217111587524 + 50.0 * 6.476008415222168
Epoch 250, val loss: 1.6326228380203247
Epoch 260, training loss: 324.7288818359375 = 1.5696467161178589 + 50.0 * 6.463184833526611
Epoch 260, val loss: 1.6123355627059937
Epoch 270, training loss: 324.18572998046875 = 1.544203519821167 + 50.0 * 6.452830791473389
Epoch 270, val loss: 1.5911667346954346
Epoch 280, training loss: 323.70843505859375 = 1.5176069736480713 + 50.0 * 6.443816661834717
Epoch 280, val loss: 1.5692635774612427
Epoch 290, training loss: 323.5821228027344 = 1.490100383758545 + 50.0 * 6.441840171813965
Epoch 290, val loss: 1.5466593503952026
Epoch 300, training loss: 322.9235534667969 = 1.4613773822784424 + 50.0 * 6.429244041442871
Epoch 300, val loss: 1.5232795476913452
Epoch 310, training loss: 322.4548645019531 = 1.4319984912872314 + 50.0 * 6.420456886291504
Epoch 310, val loss: 1.4995415210723877
Epoch 320, training loss: 322.29638671875 = 1.4020048379898071 + 50.0 * 6.4178876876831055
Epoch 320, val loss: 1.4754709005355835
Epoch 330, training loss: 321.70318603515625 = 1.3715837001800537 + 50.0 * 6.406631946563721
Epoch 330, val loss: 1.4512474536895752
Epoch 340, training loss: 321.34906005859375 = 1.3407540321350098 + 50.0 * 6.400166034698486
Epoch 340, val loss: 1.426966667175293
Epoch 350, training loss: 321.1249084472656 = 1.3096352815628052 + 50.0 * 6.396305561065674
Epoch 350, val loss: 1.4026888608932495
Epoch 360, training loss: 320.8987121582031 = 1.2783719301223755 + 50.0 * 6.392406463623047
Epoch 360, val loss: 1.3786301612854004
Epoch 370, training loss: 320.48870849609375 = 1.247112512588501 + 50.0 * 6.384832382202148
Epoch 370, val loss: 1.3547848463058472
Epoch 380, training loss: 320.1877746582031 = 1.2159605026245117 + 50.0 * 6.379436016082764
Epoch 380, val loss: 1.331261157989502
Epoch 390, training loss: 319.9353332519531 = 1.1850659847259521 + 50.0 * 6.375005722045898
Epoch 390, val loss: 1.308220624923706
Epoch 400, training loss: 320.1541442871094 = 1.1539711952209473 + 50.0 * 6.380003929138184
Epoch 400, val loss: 1.2849282026290894
Epoch 410, training loss: 319.4879455566406 = 1.1235425472259521 + 50.0 * 6.367288112640381
Epoch 410, val loss: 1.2626796960830688
Epoch 420, training loss: 319.2861022949219 = 1.0934711694717407 + 50.0 * 6.363852500915527
Epoch 420, val loss: 1.2407541275024414
Epoch 430, training loss: 319.0606689453125 = 1.0636780261993408 + 50.0 * 6.3599395751953125
Epoch 430, val loss: 1.2193589210510254
Epoch 440, training loss: 318.8580627441406 = 1.0344609022140503 + 50.0 * 6.356472015380859
Epoch 440, val loss: 1.198576807975769
Epoch 450, training loss: 318.9978942871094 = 1.0058183670043945 + 50.0 * 6.359841346740723
Epoch 450, val loss: 1.1787128448486328
Epoch 460, training loss: 318.5575866699219 = 0.9776976704597473 + 50.0 * 6.351597785949707
Epoch 460, val loss: 1.1588356494903564
Epoch 470, training loss: 318.3827209472656 = 0.9504292011260986 + 50.0 * 6.3486456871032715
Epoch 470, val loss: 1.139851689338684
Epoch 480, training loss: 318.22796630859375 = 0.9239232540130615 + 50.0 * 6.346080780029297
Epoch 480, val loss: 1.1217938661575317
Epoch 490, training loss: 318.1169738769531 = 0.8980730772018433 + 50.0 * 6.3443779945373535
Epoch 490, val loss: 1.104194164276123
Epoch 500, training loss: 317.9051818847656 = 0.8728798031806946 + 50.0 * 6.340645790100098
Epoch 500, val loss: 1.0874581336975098
Epoch 510, training loss: 317.7183532714844 = 0.8485785722732544 + 50.0 * 6.337395668029785
Epoch 510, val loss: 1.0714002847671509
Epoch 520, training loss: 317.7551574707031 = 0.8250169157981873 + 50.0 * 6.3386030197143555
Epoch 520, val loss: 1.056026816368103
Epoch 530, training loss: 317.5162658691406 = 0.8022788166999817 + 50.0 * 6.334280014038086
Epoch 530, val loss: 1.0415167808532715
Epoch 540, training loss: 317.299072265625 = 0.7802287936210632 + 50.0 * 6.330376625061035
Epoch 540, val loss: 1.0277245044708252
Epoch 550, training loss: 317.1348571777344 = 0.7589841485023499 + 50.0 * 6.327517509460449
Epoch 550, val loss: 1.0146030187606812
Epoch 560, training loss: 317.2780456542969 = 0.7385190725326538 + 50.0 * 6.3307905197143555
Epoch 560, val loss: 1.0024206638336182
Epoch 570, training loss: 317.0332946777344 = 0.7184455394744873 + 50.0 * 6.326297283172607
Epoch 570, val loss: 0.9899057149887085
Epoch 580, training loss: 316.8127746582031 = 0.6992174386978149 + 50.0 * 6.322271347045898
Epoch 580, val loss: 0.9786127209663391
Epoch 590, training loss: 316.6192932128906 = 0.6806778907775879 + 50.0 * 6.318772315979004
Epoch 590, val loss: 0.9680702686309814
Epoch 600, training loss: 316.5169677734375 = 0.6627378463745117 + 50.0 * 6.317084312438965
Epoch 600, val loss: 0.9580325484275818
Epoch 610, training loss: 316.9877624511719 = 0.6452351212501526 + 50.0 * 6.326850414276123
Epoch 610, val loss: 0.9480677843093872
Epoch 620, training loss: 316.338134765625 = 0.6283297538757324 + 50.0 * 6.3141961097717285
Epoch 620, val loss: 0.9390026330947876
Epoch 630, training loss: 316.250732421875 = 0.6118425130844116 + 50.0 * 6.312777519226074
Epoch 630, val loss: 0.9303980469703674
Epoch 640, training loss: 316.0769958496094 = 0.5958569645881653 + 50.0 * 6.309622764587402
Epoch 640, val loss: 0.9221603870391846
Epoch 650, training loss: 316.3959655761719 = 0.5803421139717102 + 50.0 * 6.316312313079834
Epoch 650, val loss: 0.9145129919052124
Epoch 660, training loss: 315.8780517578125 = 0.565056562423706 + 50.0 * 6.306259632110596
Epoch 660, val loss: 0.9069015979766846
Epoch 670, training loss: 315.78863525390625 = 0.5502792596817017 + 50.0 * 6.304766654968262
Epoch 670, val loss: 0.9000179767608643
Epoch 680, training loss: 315.755615234375 = 0.53590327501297 + 50.0 * 6.304394721984863
Epoch 680, val loss: 0.8935750126838684
Epoch 690, training loss: 315.6268005371094 = 0.5218100547790527 + 50.0 * 6.302099704742432
Epoch 690, val loss: 0.8871887922286987
Epoch 700, training loss: 315.7211608886719 = 0.5079993009567261 + 50.0 * 6.304263114929199
Epoch 700, val loss: 0.8811609745025635
Epoch 710, training loss: 315.466064453125 = 0.49464645981788635 + 50.0 * 6.2994279861450195
Epoch 710, val loss: 0.875708281993866
Epoch 720, training loss: 315.3876647949219 = 0.4816318452358246 + 50.0 * 6.298120975494385
Epoch 720, val loss: 0.8706191182136536
Epoch 730, training loss: 315.5368347167969 = 0.46894434094429016 + 50.0 * 6.301358222961426
Epoch 730, val loss: 0.8657913208007812
Epoch 740, training loss: 315.3206481933594 = 0.45636793971061707 + 50.0 * 6.297286033630371
Epoch 740, val loss: 0.8607993721961975
Epoch 750, training loss: 315.143798828125 = 0.4442174732685089 + 50.0 * 6.293991565704346
Epoch 750, val loss: 0.8563941121101379
Epoch 760, training loss: 315.0456237792969 = 0.4324268698692322 + 50.0 * 6.292263984680176
Epoch 760, val loss: 0.8524798154830933
Epoch 770, training loss: 315.0903015136719 = 0.4209079146385193 + 50.0 * 6.2933878898620605
Epoch 770, val loss: 0.8486089706420898
Epoch 780, training loss: 314.9637451171875 = 0.40969207882881165 + 50.0 * 6.291080951690674
Epoch 780, val loss: 0.8453895449638367
Epoch 790, training loss: 314.9359130859375 = 0.39865830540657043 + 50.0 * 6.290745258331299
Epoch 790, val loss: 0.8418957591056824
Epoch 800, training loss: 314.7542724609375 = 0.38807377219200134 + 50.0 * 6.287323951721191
Epoch 800, val loss: 0.839162290096283
Epoch 810, training loss: 314.739990234375 = 0.37781333923339844 + 50.0 * 6.287243366241455
Epoch 810, val loss: 0.8368790745735168
Epoch 820, training loss: 314.8185119628906 = 0.36775869131088257 + 50.0 * 6.28901481628418
Epoch 820, val loss: 0.834454357624054
Epoch 830, training loss: 314.6855163574219 = 0.3578941226005554 + 50.0 * 6.286552429199219
Epoch 830, val loss: 0.8322960734367371
Epoch 840, training loss: 314.73834228515625 = 0.3483808934688568 + 50.0 * 6.287798881530762
Epoch 840, val loss: 0.8303371667861938
Epoch 850, training loss: 314.525146484375 = 0.3391297161579132 + 50.0 * 6.283720016479492
Epoch 850, val loss: 0.829079806804657
Epoch 860, training loss: 314.4275207519531 = 0.33020177483558655 + 50.0 * 6.281946659088135
Epoch 860, val loss: 0.8278110027313232
Epoch 870, training loss: 314.33050537109375 = 0.3215391933917999 + 50.0 * 6.280179500579834
Epoch 870, val loss: 0.826965868473053
Epoch 880, training loss: 314.4692077636719 = 0.31314051151275635 + 50.0 * 6.283121585845947
Epoch 880, val loss: 0.8264555931091309
Epoch 890, training loss: 314.4434814453125 = 0.3047938942909241 + 50.0 * 6.282773971557617
Epoch 890, val loss: 0.8254072666168213
Epoch 900, training loss: 314.2471008300781 = 0.29676443338394165 + 50.0 * 6.2790069580078125
Epoch 900, val loss: 0.8253158926963806
Epoch 910, training loss: 314.151123046875 = 0.2890107035636902 + 50.0 * 6.277242660522461
Epoch 910, val loss: 0.825232982635498
Epoch 920, training loss: 314.07244873046875 = 0.281503289937973 + 50.0 * 6.275818824768066
Epoch 920, val loss: 0.8254562020301819
Epoch 930, training loss: 314.4567565917969 = 0.27422165870666504 + 50.0 * 6.2836503982543945
Epoch 930, val loss: 0.8257594704627991
Epoch 940, training loss: 314.04345703125 = 0.2670031487941742 + 50.0 * 6.275528907775879
Epoch 940, val loss: 0.8263189792633057
Epoch 950, training loss: 314.0109558105469 = 0.26010632514953613 + 50.0 * 6.275016784667969
Epoch 950, val loss: 0.827104926109314
Epoch 960, training loss: 313.9151611328125 = 0.25334054231643677 + 50.0 * 6.2732367515563965
Epoch 960, val loss: 0.8277725577354431
Epoch 970, training loss: 313.850830078125 = 0.24675072729587555 + 50.0 * 6.27208137512207
Epoch 970, val loss: 0.8287208676338196
Epoch 980, training loss: 313.78472900390625 = 0.24042969942092896 + 50.0 * 6.270886421203613
Epoch 980, val loss: 0.8299975395202637
Epoch 990, training loss: 313.88287353515625 = 0.23426450788974762 + 50.0 * 6.272972106933594
Epoch 990, val loss: 0.8313935399055481
Epoch 1000, training loss: 313.85205078125 = 0.2281736433506012 + 50.0 * 6.272477626800537
Epoch 1000, val loss: 0.8325679302215576
Epoch 1010, training loss: 313.7394104003906 = 0.222273051738739 + 50.0 * 6.27034330368042
Epoch 1010, val loss: 0.8343064785003662
Epoch 1020, training loss: 313.613037109375 = 0.21658238768577576 + 50.0 * 6.2679290771484375
Epoch 1020, val loss: 0.8361856341362
Epoch 1030, training loss: 313.646240234375 = 0.21103055775165558 + 50.0 * 6.268703937530518
Epoch 1030, val loss: 0.8381338119506836
Epoch 1040, training loss: 313.6649475097656 = 0.2056151032447815 + 50.0 * 6.269186496734619
Epoch 1040, val loss: 0.8399239778518677
Epoch 1050, training loss: 313.67120361328125 = 0.20026108622550964 + 50.0 * 6.269419193267822
Epoch 1050, val loss: 0.8418023586273193
Epoch 1060, training loss: 313.45123291015625 = 0.1950923651456833 + 50.0 * 6.265122890472412
Epoch 1060, val loss: 0.8441873788833618
Epoch 1070, training loss: 313.392578125 = 0.1901041567325592 + 50.0 * 6.264049530029297
Epoch 1070, val loss: 0.8465608954429626
Epoch 1080, training loss: 313.4441223144531 = 0.18525052070617676 + 50.0 * 6.2651777267456055
Epoch 1080, val loss: 0.8490720987319946
Epoch 1090, training loss: 313.509765625 = 0.18051297962665558 + 50.0 * 6.266585350036621
Epoch 1090, val loss: 0.8515523076057434
Epoch 1100, training loss: 313.4075927734375 = 0.17581041157245636 + 50.0 * 6.2646355628967285
Epoch 1100, val loss: 0.8538464307785034
Epoch 1110, training loss: 313.2378234863281 = 0.17126908898353577 + 50.0 * 6.261331081390381
Epoch 1110, val loss: 0.8565621376037598
Epoch 1120, training loss: 313.2107849121094 = 0.16688686609268188 + 50.0 * 6.26087760925293
Epoch 1120, val loss: 0.8594781160354614
Epoch 1130, training loss: 313.4750061035156 = 0.16261449456214905 + 50.0 * 6.266247272491455
Epoch 1130, val loss: 0.8620986938476562
Epoch 1140, training loss: 313.34295654296875 = 0.15846015512943268 + 50.0 * 6.263689994812012
Epoch 1140, val loss: 0.8651605844497681
Epoch 1150, training loss: 313.15972900390625 = 0.15435060858726501 + 50.0 * 6.260107517242432
Epoch 1150, val loss: 0.8682307600975037
Epoch 1160, training loss: 313.07464599609375 = 0.15041176974773407 + 50.0 * 6.258484363555908
Epoch 1160, val loss: 0.8713167905807495
Epoch 1170, training loss: 313.10498046875 = 0.14657913148403168 + 50.0 * 6.2591681480407715
Epoch 1170, val loss: 0.8745458722114563
Epoch 1180, training loss: 313.06866455078125 = 0.14283843338489532 + 50.0 * 6.258516311645508
Epoch 1180, val loss: 0.8777844309806824
Epoch 1190, training loss: 313.1002197265625 = 0.13921838998794556 + 50.0 * 6.259220123291016
Epoch 1190, val loss: 0.8812066316604614
Epoch 1200, training loss: 313.2528991699219 = 0.1356634795665741 + 50.0 * 6.262344837188721
Epoch 1200, val loss: 0.8845142722129822
Epoch 1210, training loss: 312.9422912597656 = 0.13215848803520203 + 50.0 * 6.256202697753906
Epoch 1210, val loss: 0.8876680731773376
Epoch 1220, training loss: 312.8729248046875 = 0.12879905104637146 + 50.0 * 6.2548828125
Epoch 1220, val loss: 0.8913087248802185
Epoch 1230, training loss: 312.87640380859375 = 0.12555381655693054 + 50.0 * 6.255016803741455
Epoch 1230, val loss: 0.8949102163314819
Epoch 1240, training loss: 312.9509582519531 = 0.12238401919603348 + 50.0 * 6.2565717697143555
Epoch 1240, val loss: 0.8984665870666504
Epoch 1250, training loss: 313.05963134765625 = 0.11927510797977448 + 50.0 * 6.258807182312012
Epoch 1250, val loss: 0.9019098877906799
Epoch 1260, training loss: 312.7786560058594 = 0.11627664417028427 + 50.0 * 6.2532477378845215
Epoch 1260, val loss: 0.9058853983879089
Epoch 1270, training loss: 312.8402404785156 = 0.11336980015039444 + 50.0 * 6.254537105560303
Epoch 1270, val loss: 0.9096870422363281
Epoch 1280, training loss: 312.8304443359375 = 0.11051442474126816 + 50.0 * 6.254398822784424
Epoch 1280, val loss: 0.9132868647575378
Epoch 1290, training loss: 312.6893615722656 = 0.10776650905609131 + 50.0 * 6.251632213592529
Epoch 1290, val loss: 0.9175716638565063
Epoch 1300, training loss: 312.7376708984375 = 0.10511205345392227 + 50.0 * 6.252651214599609
Epoch 1300, val loss: 0.9215311408042908
Epoch 1310, training loss: 312.7658386230469 = 0.10251399129629135 + 50.0 * 6.253266334533691
Epoch 1310, val loss: 0.9252957105636597
Epoch 1320, training loss: 312.65576171875 = 0.09995610266923904 + 50.0 * 6.2511162757873535
Epoch 1320, val loss: 0.9292178750038147
Epoch 1330, training loss: 312.5435791015625 = 0.09752041846513748 + 50.0 * 6.248920917510986
Epoch 1330, val loss: 0.9333080649375916
Epoch 1340, training loss: 312.6922607421875 = 0.09516903758049011 + 50.0 * 6.251942157745361
Epoch 1340, val loss: 0.9372414350509644
Epoch 1350, training loss: 312.6688537597656 = 0.09282220900058746 + 50.0 * 6.251520156860352
Epoch 1350, val loss: 0.9414143562316895
Epoch 1360, training loss: 312.53619384765625 = 0.09057661890983582 + 50.0 * 6.248912334442139
Epoch 1360, val loss: 0.9454350471496582
Epoch 1370, training loss: 312.5893859863281 = 0.08838348090648651 + 50.0 * 6.2500200271606445
Epoch 1370, val loss: 0.9495862126350403
Epoch 1380, training loss: 312.50830078125 = 0.08626192063093185 + 50.0 * 6.248440742492676
Epoch 1380, val loss: 0.9536257982254028
Epoch 1390, training loss: 312.50152587890625 = 0.08420763164758682 + 50.0 * 6.24834680557251
Epoch 1390, val loss: 0.9580686092376709
Epoch 1400, training loss: 312.4649963378906 = 0.08221232891082764 + 50.0 * 6.247655868530273
Epoch 1400, val loss: 0.9621998071670532
Epoch 1410, training loss: 312.4179382324219 = 0.08026619255542755 + 50.0 * 6.246753692626953
Epoch 1410, val loss: 0.9665572643280029
Epoch 1420, training loss: 312.4183349609375 = 0.07839273661375046 + 50.0 * 6.246798515319824
Epoch 1420, val loss: 0.9708905220031738
Epoch 1430, training loss: 312.66656494140625 = 0.0765642449259758 + 50.0 * 6.251800060272217
Epoch 1430, val loss: 0.975082278251648
Epoch 1440, training loss: 312.38262939453125 = 0.07473212480545044 + 50.0 * 6.246157646179199
Epoch 1440, val loss: 0.9789867401123047
Epoch 1450, training loss: 312.270263671875 = 0.07301024347543716 + 50.0 * 6.243945598602295
Epoch 1450, val loss: 0.9835594892501831
Epoch 1460, training loss: 312.24072265625 = 0.07133582979440689 + 50.0 * 6.243387699127197
Epoch 1460, val loss: 0.9877488017082214
Epoch 1470, training loss: 312.32904052734375 = 0.06972620636224747 + 50.0 * 6.2451863288879395
Epoch 1470, val loss: 0.9922273755073547
Epoch 1480, training loss: 312.3919677734375 = 0.0681116133928299 + 50.0 * 6.246477127075195
Epoch 1480, val loss: 0.9960458278656006
Epoch 1490, training loss: 312.3177185058594 = 0.06654412299394608 + 50.0 * 6.245023250579834
Epoch 1490, val loss: 1.0007154941558838
Epoch 1500, training loss: 312.15460205078125 = 0.06501421332359314 + 50.0 * 6.24179220199585
Epoch 1500, val loss: 1.004791021347046
Epoch 1510, training loss: 312.1380310058594 = 0.06356580555438995 + 50.0 * 6.241489410400391
Epoch 1510, val loss: 1.009071946144104
Epoch 1520, training loss: 312.2440185546875 = 0.0621611550450325 + 50.0 * 6.2436370849609375
Epoch 1520, val loss: 1.0134886503219604
Epoch 1530, training loss: 312.1322021484375 = 0.060775671154260635 + 50.0 * 6.241428852081299
Epoch 1530, val loss: 1.0176355838775635
Epoch 1540, training loss: 312.2890625 = 0.059441640973091125 + 50.0 * 6.244592189788818
Epoch 1540, val loss: 1.0219579935073853
Epoch 1550, training loss: 312.07098388671875 = 0.058115921914577484 + 50.0 * 6.240257263183594
Epoch 1550, val loss: 1.026123046875
Epoch 1560, training loss: 312.0673828125 = 0.05683629959821701 + 50.0 * 6.240211009979248
Epoch 1560, val loss: 1.0303512811660767
Epoch 1570, training loss: 312.0167236328125 = 0.05561110004782677 + 50.0 * 6.239222526550293
Epoch 1570, val loss: 1.034819483757019
Epoch 1580, training loss: 312.1700134277344 = 0.05442285165190697 + 50.0 * 6.242311954498291
Epoch 1580, val loss: 1.038939118385315
Epoch 1590, training loss: 312.2408752441406 = 0.053243983536958694 + 50.0 * 6.243752479553223
Epoch 1590, val loss: 1.042961597442627
Epoch 1600, training loss: 312.0527648925781 = 0.052103836089372635 + 50.0 * 6.240013599395752
Epoch 1600, val loss: 1.0476188659667969
Epoch 1610, training loss: 311.9701843261719 = 0.0509970486164093 + 50.0 * 6.238383769989014
Epoch 1610, val loss: 1.0516074895858765
Epoch 1620, training loss: 311.9451904296875 = 0.049930013716220856 + 50.0 * 6.237905502319336
Epoch 1620, val loss: 1.0560863018035889
Epoch 1630, training loss: 312.2761535644531 = 0.04891568049788475 + 50.0 * 6.244544982910156
Epoch 1630, val loss: 1.0602223873138428
Epoch 1640, training loss: 312.0426330566406 = 0.047855496406555176 + 50.0 * 6.239895820617676
Epoch 1640, val loss: 1.064344048500061
Epoch 1650, training loss: 311.940673828125 = 0.04686816781759262 + 50.0 * 6.237875938415527
Epoch 1650, val loss: 1.0683352947235107
Epoch 1660, training loss: 311.9127197265625 = 0.04590268060564995 + 50.0 * 6.237336158752441
Epoch 1660, val loss: 1.0727367401123047
Epoch 1670, training loss: 311.9483337402344 = 0.04497111588716507 + 50.0 * 6.238067150115967
Epoch 1670, val loss: 1.0767664909362793
Epoch 1680, training loss: 311.921142578125 = 0.04405367374420166 + 50.0 * 6.237541198730469
Epoch 1680, val loss: 1.0809953212738037
Epoch 1690, training loss: 311.8985290527344 = 0.043166544288396835 + 50.0 * 6.237107753753662
Epoch 1690, val loss: 1.0852056741714478
Epoch 1700, training loss: 311.8864440917969 = 0.04230422526597977 + 50.0 * 6.236883163452148
Epoch 1700, val loss: 1.0894895792007446
Epoch 1710, training loss: 311.9255676269531 = 0.04146668687462807 + 50.0 * 6.237681865692139
Epoch 1710, val loss: 1.0935287475585938
Epoch 1720, training loss: 311.9139709472656 = 0.04063037782907486 + 50.0 * 6.237466812133789
Epoch 1720, val loss: 1.0977694988250732
Epoch 1730, training loss: 311.899658203125 = 0.03983735665678978 + 50.0 * 6.237196922302246
Epoch 1730, val loss: 1.1017918586730957
Epoch 1740, training loss: 311.76873779296875 = 0.03905288875102997 + 50.0 * 6.234593391418457
Epoch 1740, val loss: 1.1057031154632568
Epoch 1750, training loss: 311.7152099609375 = 0.038289379328489304 + 50.0 * 6.2335381507873535
Epoch 1750, val loss: 1.109812617301941
Epoch 1760, training loss: 311.77447509765625 = 0.03755585476756096 + 50.0 * 6.234737873077393
Epoch 1760, val loss: 1.1137133836746216
Epoch 1770, training loss: 311.76953125 = 0.036837074905633926 + 50.0 * 6.234653949737549
Epoch 1770, val loss: 1.117687702178955
Epoch 1780, training loss: 311.9053649902344 = 0.03613942489027977 + 50.0 * 6.237384796142578
Epoch 1780, val loss: 1.1212959289550781
Epoch 1790, training loss: 311.7300109863281 = 0.03544015809893608 + 50.0 * 6.233891487121582
Epoch 1790, val loss: 1.1254853010177612
Epoch 1800, training loss: 311.6400451660156 = 0.03477271646261215 + 50.0 * 6.232105731964111
Epoch 1800, val loss: 1.1293903589248657
Epoch 1810, training loss: 311.6327819824219 = 0.03412532061338425 + 50.0 * 6.231973171234131
Epoch 1810, val loss: 1.1334491968154907
Epoch 1820, training loss: 311.8063659667969 = 0.033503346145153046 + 50.0 * 6.235456943511963
Epoch 1820, val loss: 1.1371715068817139
Epoch 1830, training loss: 311.5857238769531 = 0.032881785184144974 + 50.0 * 6.231057167053223
Epoch 1830, val loss: 1.1411317586898804
Epoch 1840, training loss: 311.5697326660156 = 0.032283078879117966 + 50.0 * 6.230748653411865
Epoch 1840, val loss: 1.145010232925415
Epoch 1850, training loss: 311.7352294921875 = 0.03170562908053398 + 50.0 * 6.234070301055908
Epoch 1850, val loss: 1.148637056350708
Epoch 1860, training loss: 311.7728271484375 = 0.031139757484197617 + 50.0 * 6.234833717346191
Epoch 1860, val loss: 1.152662992477417
Epoch 1870, training loss: 311.8275146484375 = 0.030569009482860565 + 50.0 * 6.235938549041748
Epoch 1870, val loss: 1.1563405990600586
Epoch 1880, training loss: 311.5661926269531 = 0.030006365850567818 + 50.0 * 6.230723857879639
Epoch 1880, val loss: 1.1598045825958252
Epoch 1890, training loss: 311.5074768066406 = 0.029480719938874245 + 50.0 * 6.229559898376465
Epoch 1890, val loss: 1.1639550924301147
Epoch 1900, training loss: 311.4865417480469 = 0.028968889266252518 + 50.0 * 6.229151725769043
Epoch 1900, val loss: 1.1674160957336426
Epoch 1910, training loss: 311.77490234375 = 0.028479240834712982 + 50.0 * 6.234928607940674
Epoch 1910, val loss: 1.170821189880371
Epoch 1920, training loss: 311.5956726074219 = 0.027975892648100853 + 50.0 * 6.231353759765625
Epoch 1920, val loss: 1.1748440265655518
Epoch 1930, training loss: 311.5415344238281 = 0.027491053566336632 + 50.0 * 6.230280876159668
Epoch 1930, val loss: 1.1782785654067993
Epoch 1940, training loss: 311.546630859375 = 0.027029292657971382 + 50.0 * 6.230391979217529
Epoch 1940, val loss: 1.1820634603500366
Epoch 1950, training loss: 311.5186767578125 = 0.026571696624159813 + 50.0 * 6.229841709136963
Epoch 1950, val loss: 1.1856340169906616
Epoch 1960, training loss: 311.46014404296875 = 0.02612045779824257 + 50.0 * 6.228680610656738
Epoch 1960, val loss: 1.189223051071167
Epoch 1970, training loss: 311.66668701171875 = 0.025686290115118027 + 50.0 * 6.2328200340271
Epoch 1970, val loss: 1.1923962831497192
Epoch 1980, training loss: 311.4271240234375 = 0.025253623723983765 + 50.0 * 6.228037357330322
Epoch 1980, val loss: 1.1961039304733276
Epoch 1990, training loss: 311.3631591796875 = 0.02483457885682583 + 50.0 * 6.226766586303711
Epoch 1990, val loss: 1.1997778415679932
Epoch 2000, training loss: 311.45123291015625 = 0.024439996108412743 + 50.0 * 6.2285356521606445
Epoch 2000, val loss: 1.2033264636993408
Epoch 2010, training loss: 311.5755615234375 = 0.024045811966061592 + 50.0 * 6.231029987335205
Epoch 2010, val loss: 1.2065527439117432
Epoch 2020, training loss: 311.37786865234375 = 0.023635543882846832 + 50.0 * 6.227085113525391
Epoch 2020, val loss: 1.2097996473312378
Epoch 2030, training loss: 311.32281494140625 = 0.02326235920190811 + 50.0 * 6.225991249084473
Epoch 2030, val loss: 1.2134402990341187
Epoch 2040, training loss: 311.3865661621094 = 0.02289462648332119 + 50.0 * 6.227273464202881
Epoch 2040, val loss: 1.2168586254119873
Epoch 2050, training loss: 311.45135498046875 = 0.022536808624863625 + 50.0 * 6.22857666015625
Epoch 2050, val loss: 1.220235824584961
Epoch 2060, training loss: 311.3472900390625 = 0.02217460796236992 + 50.0 * 6.226501941680908
Epoch 2060, val loss: 1.2234454154968262
Epoch 2070, training loss: 311.35986328125 = 0.021831141784787178 + 50.0 * 6.2267608642578125
Epoch 2070, val loss: 1.2268929481506348
Epoch 2080, training loss: 311.4749450683594 = 0.021495865657925606 + 50.0 * 6.229069232940674
Epoch 2080, val loss: 1.2301603555679321
Epoch 2090, training loss: 311.405029296875 = 0.02116241306066513 + 50.0 * 6.227677345275879
Epoch 2090, val loss: 1.2329509258270264
Epoch 2100, training loss: 311.2433166503906 = 0.020834263414144516 + 50.0 * 6.224449634552002
Epoch 2100, val loss: 1.2364583015441895
Epoch 2110, training loss: 311.2511901855469 = 0.020522980019450188 + 50.0 * 6.224613666534424
Epoch 2110, val loss: 1.2398152351379395
Epoch 2120, training loss: 311.25982666015625 = 0.020216798409819603 + 50.0 * 6.22479248046875
Epoch 2120, val loss: 1.2428394556045532
Epoch 2130, training loss: 311.53369140625 = 0.019917285069823265 + 50.0 * 6.2302751541137695
Epoch 2130, val loss: 1.2457739114761353
Epoch 2140, training loss: 311.30633544921875 = 0.01961859129369259 + 50.0 * 6.225734233856201
Epoch 2140, val loss: 1.2491806745529175
Epoch 2150, training loss: 311.23382568359375 = 0.01933014951646328 + 50.0 * 6.224290370941162
Epoch 2150, val loss: 1.2523794174194336
Epoch 2160, training loss: 311.23236083984375 = 0.019047435373067856 + 50.0 * 6.224266052246094
Epoch 2160, val loss: 1.255340814590454
Epoch 2170, training loss: 311.43426513671875 = 0.018775666132569313 + 50.0 * 6.2283101081848145
Epoch 2170, val loss: 1.2584810256958008
Epoch 2180, training loss: 311.26446533203125 = 0.018500937148928642 + 50.0 * 6.224919319152832
Epoch 2180, val loss: 1.26127290725708
Epoch 2190, training loss: 311.2297668457031 = 0.018230920657515526 + 50.0 * 6.224230766296387
Epoch 2190, val loss: 1.2643324136734009
Epoch 2200, training loss: 311.2525939941406 = 0.017970556393265724 + 50.0 * 6.224692344665527
Epoch 2200, val loss: 1.2675014734268188
Epoch 2210, training loss: 311.1892395019531 = 0.017715126276016235 + 50.0 * 6.223430633544922
Epoch 2210, val loss: 1.270699143409729
Epoch 2220, training loss: 311.1844787597656 = 0.01746884547173977 + 50.0 * 6.2233405113220215
Epoch 2220, val loss: 1.2735928297042847
Epoch 2230, training loss: 311.1406555175781 = 0.017229776829481125 + 50.0 * 6.222468376159668
Epoch 2230, val loss: 1.2764723300933838
Epoch 2240, training loss: 311.5166015625 = 0.01699984446167946 + 50.0 * 6.229991912841797
Epoch 2240, val loss: 1.2795077562332153
Epoch 2250, training loss: 311.158935546875 = 0.016747621819376945 + 50.0 * 6.222843647003174
Epoch 2250, val loss: 1.282112717628479
Epoch 2260, training loss: 311.0756530761719 = 0.016522744670510292 + 50.0 * 6.221182823181152
Epoch 2260, val loss: 1.2851203680038452
Epoch 2270, training loss: 311.09234619140625 = 0.016298072412610054 + 50.0 * 6.221520900726318
Epoch 2270, val loss: 1.2880275249481201
Epoch 2280, training loss: 311.4450988769531 = 0.01608382724225521 + 50.0 * 6.228580474853516
Epoch 2280, val loss: 1.2903836965560913
Epoch 2290, training loss: 311.13848876953125 = 0.01586560346186161 + 50.0 * 6.222452163696289
Epoch 2290, val loss: 1.2938942909240723
Epoch 2300, training loss: 311.0572509765625 = 0.0156522449105978 + 50.0 * 6.220831871032715
Epoch 2300, val loss: 1.296299934387207
Epoch 2310, training loss: 311.1880798339844 = 0.015449837781488895 + 50.0 * 6.223453044891357
Epoch 2310, val loss: 1.2993721961975098
Epoch 2320, training loss: 310.97784423828125 = 0.0152443777769804 + 50.0 * 6.21925163269043
Epoch 2320, val loss: 1.301991581916809
Epoch 2330, training loss: 311.01226806640625 = 0.01504775695502758 + 50.0 * 6.219944477081299
Epoch 2330, val loss: 1.3048330545425415
Epoch 2340, training loss: 311.0848693847656 = 0.01485726609826088 + 50.0 * 6.221400260925293
Epoch 2340, val loss: 1.3076339960098267
Epoch 2350, training loss: 311.16473388671875 = 0.014669512398540974 + 50.0 * 6.223001003265381
Epoch 2350, val loss: 1.3099435567855835
Epoch 2360, training loss: 311.14599609375 = 0.014482068829238415 + 50.0 * 6.222630023956299
Epoch 2360, val loss: 1.3128254413604736
Epoch 2370, training loss: 311.0184326171875 = 0.01429163571447134 + 50.0 * 6.220082759857178
Epoch 2370, val loss: 1.3151289224624634
Epoch 2380, training loss: 310.9803161621094 = 0.01410920824855566 + 50.0 * 6.219324588775635
Epoch 2380, val loss: 1.3181449174880981
Epoch 2390, training loss: 310.9576721191406 = 0.013936135917901993 + 50.0 * 6.218874454498291
Epoch 2390, val loss: 1.3207645416259766
Epoch 2400, training loss: 311.1634521484375 = 0.01376681961119175 + 50.0 * 6.222993850708008
Epoch 2400, val loss: 1.3231643438339233
Epoch 2410, training loss: 310.9389953613281 = 0.013598411343991756 + 50.0 * 6.218507766723633
Epoch 2410, val loss: 1.326054573059082
Epoch 2420, training loss: 311.1666564941406 = 0.013437952846288681 + 50.0 * 6.223064422607422
Epoch 2420, val loss: 1.3285365104675293
Epoch 2430, training loss: 310.9761047363281 = 0.013265173882246017 + 50.0 * 6.219256401062012
Epoch 2430, val loss: 1.3308974504470825
Epoch 2440, training loss: 310.94195556640625 = 0.013106927275657654 + 50.0 * 6.2185773849487305
Epoch 2440, val loss: 1.333485722541809
Epoch 2450, training loss: 310.89599609375 = 0.012950532138347626 + 50.0 * 6.217660903930664
Epoch 2450, val loss: 1.3359142541885376
Epoch 2460, training loss: 311.11602783203125 = 0.012804687954485416 + 50.0 * 6.22206449508667
Epoch 2460, val loss: 1.338194727897644
Epoch 2470, training loss: 310.9465026855469 = 0.012648174539208412 + 50.0 * 6.218677520751953
Epoch 2470, val loss: 1.3411718606948853
Epoch 2480, training loss: 310.86358642578125 = 0.012494436465203762 + 50.0 * 6.217021942138672
Epoch 2480, val loss: 1.3432849645614624
Epoch 2490, training loss: 310.8438720703125 = 0.01234914269298315 + 50.0 * 6.216629981994629
Epoch 2490, val loss: 1.3460159301757812
Epoch 2500, training loss: 310.8451843261719 = 0.012210046872496605 + 50.0 * 6.2166595458984375
Epoch 2500, val loss: 1.3483558893203735
Epoch 2510, training loss: 311.1072692871094 = 0.012076751329004765 + 50.0 * 6.2219038009643555
Epoch 2510, val loss: 1.3507229089736938
Epoch 2520, training loss: 311.1027526855469 = 0.011938607320189476 + 50.0 * 6.221816539764404
Epoch 2520, val loss: 1.353167176246643
Epoch 2530, training loss: 311.0133361816406 = 0.01179568376392126 + 50.0 * 6.220030784606934
Epoch 2530, val loss: 1.3556225299835205
Epoch 2540, training loss: 310.859619140625 = 0.011659334413707256 + 50.0 * 6.216958999633789
Epoch 2540, val loss: 1.3579896688461304
Epoch 2550, training loss: 310.7912292480469 = 0.011530025862157345 + 50.0 * 6.2155938148498535
Epoch 2550, val loss: 1.3602746725082397
Epoch 2560, training loss: 310.81500244140625 = 0.011405443772673607 + 50.0 * 6.216072082519531
Epoch 2560, val loss: 1.3624733686447144
Epoch 2570, training loss: 311.343017578125 = 0.01128408033400774 + 50.0 * 6.226634502410889
Epoch 2570, val loss: 1.3643674850463867
Epoch 2580, training loss: 310.9476318359375 = 0.011158520355820656 + 50.0 * 6.218729496002197
Epoch 2580, val loss: 1.3671855926513672
Epoch 2590, training loss: 310.8126525878906 = 0.0110336197540164 + 50.0 * 6.216032028198242
Epoch 2590, val loss: 1.3692312240600586
Epoch 2600, training loss: 310.7560119628906 = 0.010915922001004219 + 50.0 * 6.214901924133301
Epoch 2600, val loss: 1.3718315362930298
Epoch 2610, training loss: 310.75396728515625 = 0.010801790282130241 + 50.0 * 6.214863300323486
Epoch 2610, val loss: 1.3740028142929077
Epoch 2620, training loss: 311.3143615722656 = 0.010698903352022171 + 50.0 * 6.226073265075684
Epoch 2620, val loss: 1.3758538961410522
Epoch 2630, training loss: 310.88031005859375 = 0.010569753125309944 + 50.0 * 6.217395305633545
Epoch 2630, val loss: 1.3784767389297485
Epoch 2640, training loss: 310.7856140136719 = 0.01045860443264246 + 50.0 * 6.215502738952637
Epoch 2640, val loss: 1.3803776502609253
Epoch 2650, training loss: 310.79595947265625 = 0.01035027764737606 + 50.0 * 6.215712547302246
Epoch 2650, val loss: 1.3829187154769897
Epoch 2660, training loss: 310.8628234863281 = 0.01024409756064415 + 50.0 * 6.2170515060424805
Epoch 2660, val loss: 1.3848155736923218
Epoch 2670, training loss: 310.7532958984375 = 0.0101395882666111 + 50.0 * 6.214863300323486
Epoch 2670, val loss: 1.3869624137878418
Epoch 2680, training loss: 310.8860778808594 = 0.010037899017333984 + 50.0 * 6.217520713806152
Epoch 2680, val loss: 1.3887490034103394
Epoch 2690, training loss: 310.9104309082031 = 0.009932252578437328 + 50.0 * 6.218009948730469
Epoch 2690, val loss: 1.3913731575012207
Epoch 2700, training loss: 310.7339782714844 = 0.009834302589297295 + 50.0 * 6.21448278427124
Epoch 2700, val loss: 1.3933171033859253
Epoch 2710, training loss: 310.6799011230469 = 0.009730968624353409 + 50.0 * 6.213403224945068
Epoch 2710, val loss: 1.395776391029358
Epoch 2720, training loss: 310.6566162109375 = 0.00963609479367733 + 50.0 * 6.212939739227295
Epoch 2720, val loss: 1.3977640867233276
Epoch 2730, training loss: 310.9271545410156 = 0.00954704824835062 + 50.0 * 6.218352317810059
Epoch 2730, val loss: 1.3996251821517944
Epoch 2740, training loss: 310.7372131347656 = 0.00944715179502964 + 50.0 * 6.214555263519287
Epoch 2740, val loss: 1.4014922380447388
Epoch 2750, training loss: 310.7280578613281 = 0.00935609731823206 + 50.0 * 6.214373588562012
Epoch 2750, val loss: 1.4036539793014526
Epoch 2760, training loss: 310.86474609375 = 0.00926356390118599 + 50.0 * 6.217109203338623
Epoch 2760, val loss: 1.4054489135742188
Epoch 2770, training loss: 310.71337890625 = 0.009176408872008324 + 50.0 * 6.214084148406982
Epoch 2770, val loss: 1.4076015949249268
Epoch 2780, training loss: 310.74298095703125 = 0.009087644517421722 + 50.0 * 6.214677810668945
Epoch 2780, val loss: 1.4092473983764648
Epoch 2790, training loss: 310.62847900390625 = 0.00899918470531702 + 50.0 * 6.2123894691467285
Epoch 2790, val loss: 1.4116289615631104
Epoch 2800, training loss: 310.6954040527344 = 0.008917059749364853 + 50.0 * 6.2137298583984375
Epoch 2800, val loss: 1.4135290384292603
Epoch 2810, training loss: 310.78472900390625 = 0.008834446780383587 + 50.0 * 6.215518474578857
Epoch 2810, val loss: 1.4150819778442383
Epoch 2820, training loss: 310.68756103515625 = 0.00875157117843628 + 50.0 * 6.213575839996338
Epoch 2820, val loss: 1.4172422885894775
Epoch 2830, training loss: 310.62921142578125 = 0.008669732138514519 + 50.0 * 6.212410926818848
Epoch 2830, val loss: 1.4194262027740479
Epoch 2840, training loss: 310.6713562011719 = 0.008590392768383026 + 50.0 * 6.213255405426025
Epoch 2840, val loss: 1.421259880065918
Epoch 2850, training loss: 310.7774658203125 = 0.008511235006153584 + 50.0 * 6.215378761291504
Epoch 2850, val loss: 1.4229398965835571
Epoch 2860, training loss: 310.6201171875 = 0.00843311008065939 + 50.0 * 6.212234020233154
Epoch 2860, val loss: 1.4246397018432617
Epoch 2870, training loss: 310.55560302734375 = 0.00835451576858759 + 50.0 * 6.210945129394531
Epoch 2870, val loss: 1.426853895187378
Epoch 2880, training loss: 310.5425720214844 = 0.008281053975224495 + 50.0 * 6.210686206817627
Epoch 2880, val loss: 1.4288047552108765
Epoch 2890, training loss: 310.6468200683594 = 0.008210336789488792 + 50.0 * 6.212772369384766
Epoch 2890, val loss: 1.4307498931884766
Epoch 2900, training loss: 310.8202209472656 = 0.008139403536915779 + 50.0 * 6.216241359710693
Epoch 2900, val loss: 1.4322739839553833
Epoch 2910, training loss: 310.6449279785156 = 0.008064158260822296 + 50.0 * 6.212737560272217
Epoch 2910, val loss: 1.4335790872573853
Epoch 2920, training loss: 310.5558776855469 = 0.007990234531462193 + 50.0 * 6.2109575271606445
Epoch 2920, val loss: 1.4358311891555786
Epoch 2930, training loss: 310.5509338378906 = 0.007923041470348835 + 50.0 * 6.210860252380371
Epoch 2930, val loss: 1.4374638795852661
Epoch 2940, training loss: 311.1362609863281 = 0.007864764891564846 + 50.0 * 6.222567558288574
Epoch 2940, val loss: 1.4393996000289917
Epoch 2950, training loss: 310.6509094238281 = 0.0077849808149039745 + 50.0 * 6.212862491607666
Epoch 2950, val loss: 1.4406182765960693
Epoch 2960, training loss: 310.54046630859375 = 0.0077206967398524284 + 50.0 * 6.2106547355651855
Epoch 2960, val loss: 1.4426119327545166
Epoch 2970, training loss: 310.4815368652344 = 0.007654127664864063 + 50.0 * 6.209477424621582
Epoch 2970, val loss: 1.4443588256835938
Epoch 2980, training loss: 310.4683532714844 = 0.00759206572547555 + 50.0 * 6.20921516418457
Epoch 2980, val loss: 1.4460958242416382
Epoch 2990, training loss: 310.8291931152344 = 0.007534063886851072 + 50.0 * 6.216433048248291
Epoch 2990, val loss: 1.4480634927749634
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.762962962962963
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 431.7983093261719 = 1.9564100503921509 + 50.0 * 8.596837997436523
Epoch 0, val loss: 1.9557533264160156
Epoch 10, training loss: 431.7498474121094 = 1.9467017650604248 + 50.0 * 8.596062660217285
Epoch 10, val loss: 1.9457944631576538
Epoch 20, training loss: 431.4624328613281 = 1.9342409372329712 + 50.0 * 8.590563774108887
Epoch 20, val loss: 1.932560682296753
Epoch 30, training loss: 429.6718444824219 = 1.918065071105957 + 50.0 * 8.555075645446777
Epoch 30, val loss: 1.9151381254196167
Epoch 40, training loss: 420.3316955566406 = 1.8991674184799194 + 50.0 * 8.368650436401367
Epoch 40, val loss: 1.8955427408218384
Epoch 50, training loss: 397.1722106933594 = 1.8777436017990112 + 50.0 * 7.90588903427124
Epoch 50, val loss: 1.8725515604019165
Epoch 60, training loss: 378.2942199707031 = 1.859025001525879 + 50.0 * 7.528703689575195
Epoch 60, val loss: 1.8538273572921753
Epoch 70, training loss: 363.39642333984375 = 1.8461098670959473 + 50.0 * 7.231006622314453
Epoch 70, val loss: 1.8412836790084839
Epoch 80, training loss: 352.8155212402344 = 1.8347846269607544 + 50.0 * 7.0196146965026855
Epoch 80, val loss: 1.8301050662994385
Epoch 90, training loss: 347.8307800292969 = 1.8224114179611206 + 50.0 * 6.920167446136475
Epoch 90, val loss: 1.8181263208389282
Epoch 100, training loss: 342.71307373046875 = 1.8110644817352295 + 50.0 * 6.818040370941162
Epoch 100, val loss: 1.8077449798583984
Epoch 110, training loss: 338.8151550292969 = 1.801735758781433 + 50.0 * 6.740268707275391
Epoch 110, val loss: 1.799189567565918
Epoch 120, training loss: 336.2523498535156 = 1.7925437688827515 + 50.0 * 6.6891961097717285
Epoch 120, val loss: 1.790518879890442
Epoch 130, training loss: 333.786865234375 = 1.7828928232192993 + 50.0 * 6.640079498291016
Epoch 130, val loss: 1.7813212871551514
Epoch 140, training loss: 331.9283752441406 = 1.7731363773345947 + 50.0 * 6.603104591369629
Epoch 140, val loss: 1.772153377532959
Epoch 150, training loss: 330.3669738769531 = 1.7631664276123047 + 50.0 * 6.572076320648193
Epoch 150, val loss: 1.7628440856933594
Epoch 160, training loss: 329.1506042480469 = 1.7525274753570557 + 50.0 * 6.547961711883545
Epoch 160, val loss: 1.7529629468917847
Epoch 170, training loss: 328.11859130859375 = 1.740917682647705 + 50.0 * 6.527553558349609
Epoch 170, val loss: 1.742239236831665
Epoch 180, training loss: 327.0910949707031 = 1.7284469604492188 + 50.0 * 6.5072526931762695
Epoch 180, val loss: 1.730781078338623
Epoch 190, training loss: 326.2749938964844 = 1.714873194694519 + 50.0 * 6.491202354431152
Epoch 190, val loss: 1.7184590101242065
Epoch 200, training loss: 325.5414123535156 = 1.7001169919967651 + 50.0 * 6.476826190948486
Epoch 200, val loss: 1.705167293548584
Epoch 210, training loss: 324.8460388183594 = 1.6841665506362915 + 50.0 * 6.463237285614014
Epoch 210, val loss: 1.6909239292144775
Epoch 220, training loss: 324.250732421875 = 1.6668134927749634 + 50.0 * 6.451678276062012
Epoch 220, val loss: 1.6754300594329834
Epoch 230, training loss: 323.7018737792969 = 1.6479398012161255 + 50.0 * 6.4410786628723145
Epoch 230, val loss: 1.6587226390838623
Epoch 240, training loss: 323.22216796875 = 1.627551794052124 + 50.0 * 6.431892395019531
Epoch 240, val loss: 1.6407846212387085
Epoch 250, training loss: 322.9689025878906 = 1.605478048324585 + 50.0 * 6.4272685050964355
Epoch 250, val loss: 1.6213188171386719
Epoch 260, training loss: 322.4215087890625 = 1.5817840099334717 + 50.0 * 6.416794300079346
Epoch 260, val loss: 1.6007436513900757
Epoch 270, training loss: 322.0281982421875 = 1.5566140413284302 + 50.0 * 6.4094319343566895
Epoch 270, val loss: 1.5789191722869873
Epoch 280, training loss: 321.6614685058594 = 1.5299996137619019 + 50.0 * 6.402629375457764
Epoch 280, val loss: 1.5559459924697876
Epoch 290, training loss: 321.6883239746094 = 1.5020164251327515 + 50.0 * 6.403726100921631
Epoch 290, val loss: 1.5319421291351318
Epoch 300, training loss: 321.13726806640625 = 1.472601056098938 + 50.0 * 6.393293380737305
Epoch 300, val loss: 1.5069574117660522
Epoch 310, training loss: 320.7915954589844 = 1.442188024520874 + 50.0 * 6.386988162994385
Epoch 310, val loss: 1.4813863039016724
Epoch 320, training loss: 320.70355224609375 = 1.410840630531311 + 50.0 * 6.385854721069336
Epoch 320, val loss: 1.4552806615829468
Epoch 330, training loss: 320.28411865234375 = 1.3787052631378174 + 50.0 * 6.378108501434326
Epoch 330, val loss: 1.4288314580917358
Epoch 340, training loss: 319.9925231933594 = 1.3460859060287476 + 50.0 * 6.372928619384766
Epoch 340, val loss: 1.4023245573043823
Epoch 350, training loss: 319.859619140625 = 1.313252329826355 + 50.0 * 6.370926856994629
Epoch 350, val loss: 1.3760465383529663
Epoch 360, training loss: 319.5613098144531 = 1.2802705764770508 + 50.0 * 6.3656206130981445
Epoch 360, val loss: 1.3499089479446411
Epoch 370, training loss: 319.37847900390625 = 1.2475446462631226 + 50.0 * 6.362618446350098
Epoch 370, val loss: 1.324373722076416
Epoch 380, training loss: 319.0682067871094 = 1.2152336835861206 + 50.0 * 6.357059478759766
Epoch 380, val loss: 1.2995187044143677
Epoch 390, training loss: 318.8515625 = 1.1835741996765137 + 50.0 * 6.353359699249268
Epoch 390, val loss: 1.2755991220474243
Epoch 400, training loss: 318.7593688964844 = 1.1526225805282593 + 50.0 * 6.352135181427002
Epoch 400, val loss: 1.2525144815444946
Epoch 410, training loss: 318.6247253417969 = 1.1223928928375244 + 50.0 * 6.350046157836914
Epoch 410, val loss: 1.2303892374038696
Epoch 420, training loss: 318.306640625 = 1.093308925628662 + 50.0 * 6.344266414642334
Epoch 420, val loss: 1.2095307111740112
Epoch 430, training loss: 318.08843994140625 = 1.065399169921875 + 50.0 * 6.340460777282715
Epoch 430, val loss: 1.1899250745773315
Epoch 440, training loss: 318.2261047363281 = 1.0385334491729736 + 50.0 * 6.343751430511475
Epoch 440, val loss: 1.1713858842849731
Epoch 450, training loss: 317.80694580078125 = 1.0128066539764404 + 50.0 * 6.335882663726807
Epoch 450, val loss: 1.1539868116378784
Epoch 460, training loss: 317.59613037109375 = 0.9883078932762146 + 50.0 * 6.332156658172607
Epoch 460, val loss: 1.1377977132797241
Epoch 470, training loss: 317.6007385253906 = 0.9649146199226379 + 50.0 * 6.332716464996338
Epoch 470, val loss: 1.1226521730422974
Epoch 480, training loss: 317.3177185058594 = 0.942560076713562 + 50.0 * 6.327503204345703
Epoch 480, val loss: 1.108748435974121
Epoch 490, training loss: 317.3342590332031 = 0.921212375164032 + 50.0 * 6.328261375427246
Epoch 490, val loss: 1.0956828594207764
Epoch 500, training loss: 317.0863952636719 = 0.9006948471069336 + 50.0 * 6.323714256286621
Epoch 500, val loss: 1.083411693572998
Epoch 510, training loss: 316.8999938964844 = 0.8811279535293579 + 50.0 * 6.320377349853516
Epoch 510, val loss: 1.0721360445022583
Epoch 520, training loss: 317.0212097167969 = 0.8623455762863159 + 50.0 * 6.323177337646484
Epoch 520, val loss: 1.0615448951721191
Epoch 530, training loss: 316.7283020019531 = 0.844136118888855 + 50.0 * 6.317683219909668
Epoch 530, val loss: 1.0516858100891113
Epoch 540, training loss: 316.7614440917969 = 0.8266502618789673 + 50.0 * 6.318695545196533
Epoch 540, val loss: 1.0424219369888306
Epoch 550, training loss: 316.5453186035156 = 0.8095301389694214 + 50.0 * 6.314715385437012
Epoch 550, val loss: 1.0334501266479492
Epoch 560, training loss: 316.3203430175781 = 0.7930704951286316 + 50.0 * 6.310545444488525
Epoch 560, val loss: 1.0252662897109985
Epoch 570, training loss: 316.1665954589844 = 0.7770684361457825 + 50.0 * 6.307790279388428
Epoch 570, val loss: 1.0174704790115356
Epoch 580, training loss: 316.1235656738281 = 0.7614852786064148 + 50.0 * 6.307241916656494
Epoch 580, val loss: 1.0100958347320557
Epoch 590, training loss: 316.14227294921875 = 0.7459912300109863 + 50.0 * 6.307925701141357
Epoch 590, val loss: 1.0027555227279663
Epoch 600, training loss: 315.8801574707031 = 0.7309483885765076 + 50.0 * 6.30298376083374
Epoch 600, val loss: 0.9959675073623657
Epoch 610, training loss: 315.7612609863281 = 0.7162092924118042 + 50.0 * 6.300900936126709
Epoch 610, val loss: 0.9894284605979919
Epoch 620, training loss: 315.7066650390625 = 0.7017917633056641 + 50.0 * 6.300097942352295
Epoch 620, val loss: 0.9831454753875732
Epoch 630, training loss: 315.6600036621094 = 0.6874505877494812 + 50.0 * 6.2994513511657715
Epoch 630, val loss: 0.9772012233734131
Epoch 640, training loss: 315.5036315917969 = 0.6733702421188354 + 50.0 * 6.296605110168457
Epoch 640, val loss: 0.9712780117988586
Epoch 650, training loss: 315.43963623046875 = 0.659552276134491 + 50.0 * 6.295601844787598
Epoch 650, val loss: 0.965774416923523
Epoch 660, training loss: 315.5598449707031 = 0.6459319591522217 + 50.0 * 6.298278331756592
Epoch 660, val loss: 0.9605371356010437
Epoch 670, training loss: 315.4910583496094 = 0.6325353384017944 + 50.0 * 6.297170639038086
Epoch 670, val loss: 0.9556238055229187
Epoch 680, training loss: 315.2019348144531 = 0.6191697716712952 + 50.0 * 6.29165506362915
Epoch 680, val loss: 0.9507175087928772
Epoch 690, training loss: 315.06744384765625 = 0.6061590313911438 + 50.0 * 6.2892255783081055
Epoch 690, val loss: 0.9463239908218384
Epoch 700, training loss: 314.987060546875 = 0.5932878851890564 + 50.0 * 6.287875175476074
Epoch 700, val loss: 0.9420186877250671
Epoch 710, training loss: 315.04473876953125 = 0.5806626677513123 + 50.0 * 6.289281845092773
Epoch 710, val loss: 0.9380124807357788
Epoch 720, training loss: 314.90643310546875 = 0.5680391192436218 + 50.0 * 6.286767482757568
Epoch 720, val loss: 0.9341048002243042
Epoch 730, training loss: 314.83001708984375 = 0.5557140707969666 + 50.0 * 6.285485744476318
Epoch 730, val loss: 0.9305437207221985
Epoch 740, training loss: 315.1097106933594 = 0.5435211062431335 + 50.0 * 6.291323661804199
Epoch 740, val loss: 0.9270551204681396
Epoch 750, training loss: 314.8247985839844 = 0.5313706398010254 + 50.0 * 6.2858686447143555
Epoch 750, val loss: 0.9237974882125854
Epoch 760, training loss: 314.5864562988281 = 0.5195637345314026 + 50.0 * 6.281337738037109
Epoch 760, val loss: 0.9208869934082031
Epoch 770, training loss: 314.49609375 = 0.5079508423805237 + 50.0 * 6.2797627449035645
Epoch 770, val loss: 0.9182330369949341
Epoch 780, training loss: 314.4423522949219 = 0.4965367317199707 + 50.0 * 6.278915882110596
Epoch 780, val loss: 0.9156603813171387
Epoch 790, training loss: 314.8856201171875 = 0.4853101670742035 + 50.0 * 6.288005828857422
Epoch 790, val loss: 0.9132261872291565
Epoch 800, training loss: 314.3117980957031 = 0.4740656316280365 + 50.0 * 6.276754856109619
Epoch 800, val loss: 0.9108757972717285
Epoch 810, training loss: 314.2821044921875 = 0.4631652534008026 + 50.0 * 6.276378631591797
Epoch 810, val loss: 0.9089003801345825
Epoch 820, training loss: 314.2213439941406 = 0.4524681866168976 + 50.0 * 6.2753777503967285
Epoch 820, val loss: 0.9070197939872742
Epoch 830, training loss: 314.40216064453125 = 0.44186297059059143 + 50.0 * 6.279205799102783
Epoch 830, val loss: 0.9051291942596436
Epoch 840, training loss: 314.0861511230469 = 0.43144190311431885 + 50.0 * 6.273094177246094
Epoch 840, val loss: 0.9034466743469238
Epoch 850, training loss: 314.1244201660156 = 0.4212196469306946 + 50.0 * 6.274063587188721
Epoch 850, val loss: 0.9018625020980835
Epoch 860, training loss: 314.0140686035156 = 0.4111219644546509 + 50.0 * 6.272058486938477
Epoch 860, val loss: 0.900519847869873
Epoch 870, training loss: 313.9534912109375 = 0.4012574255466461 + 50.0 * 6.271045207977295
Epoch 870, val loss: 0.8992044925689697
Epoch 880, training loss: 314.1360168457031 = 0.39150071144104004 + 50.0 * 6.274889945983887
Epoch 880, val loss: 0.8980534672737122
Epoch 890, training loss: 313.9046936035156 = 0.38185620307922363 + 50.0 * 6.270456314086914
Epoch 890, val loss: 0.8968579173088074
Epoch 900, training loss: 313.80938720703125 = 0.3724951446056366 + 50.0 * 6.26873779296875
Epoch 900, val loss: 0.8959418535232544
Epoch 910, training loss: 313.9129943847656 = 0.36324217915534973 + 50.0 * 6.270995140075684
Epoch 910, val loss: 0.8950070142745972
Epoch 920, training loss: 313.7736511230469 = 0.3541383445262909 + 50.0 * 6.268390655517578
Epoch 920, val loss: 0.8941034078598022
Epoch 930, training loss: 313.808837890625 = 0.3451678454875946 + 50.0 * 6.26927375793457
Epoch 930, val loss: 0.8932892680168152
Epoch 940, training loss: 313.5731506347656 = 0.3364218473434448 + 50.0 * 6.264734745025635
Epoch 940, val loss: 0.8926510810852051
Epoch 950, training loss: 313.57379150390625 = 0.32784712314605713 + 50.0 * 6.264918804168701
Epoch 950, val loss: 0.892042338848114
Epoch 960, training loss: 313.65692138671875 = 0.31942394375801086 + 50.0 * 6.266749858856201
Epoch 960, val loss: 0.891438901424408
Epoch 970, training loss: 313.67877197265625 = 0.31111645698547363 + 50.0 * 6.267353057861328
Epoch 970, val loss: 0.890777051448822
Epoch 980, training loss: 313.6244201660156 = 0.30293068289756775 + 50.0 * 6.266429901123047
Epoch 980, val loss: 0.8903897404670715
Epoch 990, training loss: 313.4140319824219 = 0.2949165105819702 + 50.0 * 6.2623820304870605
Epoch 990, val loss: 0.8898742198944092
Epoch 1000, training loss: 313.32568359375 = 0.28713423013687134 + 50.0 * 6.260770797729492
Epoch 1000, val loss: 0.8896893262863159
Epoch 1010, training loss: 313.28961181640625 = 0.27951332926750183 + 50.0 * 6.260201930999756
Epoch 1010, val loss: 0.8894131183624268
Epoch 1020, training loss: 313.58868408203125 = 0.27203091979026794 + 50.0 * 6.266333103179932
Epoch 1020, val loss: 0.8891828656196594
Epoch 1030, training loss: 313.5646057128906 = 0.26470866799354553 + 50.0 * 6.265997886657715
Epoch 1030, val loss: 0.8891444206237793
Epoch 1040, training loss: 313.1976318359375 = 0.25749075412750244 + 50.0 * 6.258803367614746
Epoch 1040, val loss: 0.8890931010246277
Epoch 1050, training loss: 313.1510925292969 = 0.2504904568195343 + 50.0 * 6.258012294769287
Epoch 1050, val loss: 0.8891683220863342
Epoch 1060, training loss: 313.41656494140625 = 0.2436848282814026 + 50.0 * 6.263457775115967
Epoch 1060, val loss: 0.8893143534660339
Epoch 1070, training loss: 313.2396545410156 = 0.23704791069030762 + 50.0 * 6.260051727294922
Epoch 1070, val loss: 0.8895835876464844
Epoch 1080, training loss: 313.0425720214844 = 0.23050378262996674 + 50.0 * 6.256241321563721
Epoch 1080, val loss: 0.890002965927124
Epoch 1090, training loss: 312.9736328125 = 0.22422119975090027 + 50.0 * 6.254988670349121
Epoch 1090, val loss: 0.8904855847358704
Epoch 1100, training loss: 313.1564636230469 = 0.21811309456825256 + 50.0 * 6.258767127990723
Epoch 1100, val loss: 0.8910794258117676
Epoch 1110, training loss: 312.9297180175781 = 0.21204937994480133 + 50.0 * 6.2543535232543945
Epoch 1110, val loss: 0.8918048143386841
Epoch 1120, training loss: 312.9131164550781 = 0.2062104493379593 + 50.0 * 6.254138469696045
Epoch 1120, val loss: 0.8927249908447266
Epoch 1130, training loss: 312.91436767578125 = 0.2005375772714615 + 50.0 * 6.254276752471924
Epoch 1130, val loss: 0.8937709927558899
Epoch 1140, training loss: 312.92535400390625 = 0.19503208994865417 + 50.0 * 6.254606246948242
Epoch 1140, val loss: 0.8948608636856079
Epoch 1150, training loss: 312.9577331542969 = 0.189667746424675 + 50.0 * 6.255361557006836
Epoch 1150, val loss: 0.8961489796638489
Epoch 1160, training loss: 312.8186340332031 = 0.18444198369979858 + 50.0 * 6.252683639526367
Epoch 1160, val loss: 0.8974822163581848
Epoch 1170, training loss: 312.7526550292969 = 0.1793922483921051 + 50.0 * 6.251465320587158
Epoch 1170, val loss: 0.8990041017532349
Epoch 1180, training loss: 312.7835693359375 = 0.17448453605175018 + 50.0 * 6.252181529998779
Epoch 1180, val loss: 0.9004500508308411
Epoch 1190, training loss: 312.7732238769531 = 0.16973330080509186 + 50.0 * 6.25206995010376
Epoch 1190, val loss: 0.9018874168395996
Epoch 1200, training loss: 312.59637451171875 = 0.16511449217796326 + 50.0 * 6.2486252784729
Epoch 1200, val loss: 0.9037649631500244
Epoch 1210, training loss: 312.60113525390625 = 0.1606442928314209 + 50.0 * 6.248809814453125
Epoch 1210, val loss: 0.9056092500686646
Epoch 1220, training loss: 312.7462158203125 = 0.15630872547626495 + 50.0 * 6.251798152923584
Epoch 1220, val loss: 0.9074989557266235
Epoch 1230, training loss: 312.6910705566406 = 0.15205955505371094 + 50.0 * 6.25078010559082
Epoch 1230, val loss: 0.9096457362174988
Epoch 1240, training loss: 312.54437255859375 = 0.1480039358139038 + 50.0 * 6.247927188873291
Epoch 1240, val loss: 0.9117059111595154
Epoch 1250, training loss: 312.5731201171875 = 0.14403793215751648 + 50.0 * 6.248581409454346
Epoch 1250, val loss: 0.9140373468399048
Epoch 1260, training loss: 312.5080261230469 = 0.14018090069293976 + 50.0 * 6.247357368469238
Epoch 1260, val loss: 0.9163616299629211
Epoch 1270, training loss: 312.56695556640625 = 0.1364770084619522 + 50.0 * 6.24860954284668
Epoch 1270, val loss: 0.9188879132270813
Epoch 1280, training loss: 312.4185791015625 = 0.1328703612089157 + 50.0 * 6.24571418762207
Epoch 1280, val loss: 0.9213314652442932
Epoch 1290, training loss: 312.3714904785156 = 0.1293872594833374 + 50.0 * 6.244842052459717
Epoch 1290, val loss: 0.9239707589149475
Epoch 1300, training loss: 312.50726318359375 = 0.12602025270462036 + 50.0 * 6.24762487411499
Epoch 1300, val loss: 0.9266610145568848
Epoch 1310, training loss: 312.3943786621094 = 0.12274790555238724 + 50.0 * 6.2454328536987305
Epoch 1310, val loss: 0.929368257522583
Epoch 1320, training loss: 312.2908020019531 = 0.1195608600974083 + 50.0 * 6.243424892425537
Epoch 1320, val loss: 0.9321784377098083
Epoch 1330, training loss: 312.3597717285156 = 0.11648696660995483 + 50.0 * 6.244865894317627
Epoch 1330, val loss: 0.9349913597106934
Epoch 1340, training loss: 312.40673828125 = 0.11352113634347916 + 50.0 * 6.245864391326904
Epoch 1340, val loss: 0.937730073928833
Epoch 1350, training loss: 312.29736328125 = 0.11060597747564316 + 50.0 * 6.243735313415527
Epoch 1350, val loss: 0.9406863451004028
Epoch 1360, training loss: 312.1971435546875 = 0.1078072264790535 + 50.0 * 6.241786479949951
Epoch 1360, val loss: 0.9437139630317688
Epoch 1370, training loss: 312.1466064453125 = 0.10511615127325058 + 50.0 * 6.240829944610596
Epoch 1370, val loss: 0.9467903971672058
Epoch 1380, training loss: 312.3673400878906 = 0.10251766443252563 + 50.0 * 6.245296478271484
Epoch 1380, val loss: 0.9498766660690308
Epoch 1390, training loss: 312.20489501953125 = 0.09995070844888687 + 50.0 * 6.242098808288574
Epoch 1390, val loss: 0.9528531432151794
Epoch 1400, training loss: 312.2428283691406 = 0.09746942669153214 + 50.0 * 6.2429070472717285
Epoch 1400, val loss: 0.9559004902839661
Epoch 1410, training loss: 312.1770324707031 = 0.09507220983505249 + 50.0 * 6.241639137268066
Epoch 1410, val loss: 0.9591816067695618
Epoch 1420, training loss: 312.0428466796875 = 0.092753566801548 + 50.0 * 6.239002227783203
Epoch 1420, val loss: 0.9625229239463806
Epoch 1430, training loss: 312.0299377441406 = 0.09053108841180801 + 50.0 * 6.238787651062012
Epoch 1430, val loss: 0.9657968282699585
Epoch 1440, training loss: 312.13555908203125 = 0.08837719261646271 + 50.0 * 6.240943908691406
Epoch 1440, val loss: 0.9690221548080444
Epoch 1450, training loss: 312.1689758300781 = 0.0862361416220665 + 50.0 * 6.241654396057129
Epoch 1450, val loss: 0.9723950624465942
Epoch 1460, training loss: 311.9694519042969 = 0.08416204154491425 + 50.0 * 6.237705707550049
Epoch 1460, val loss: 0.9759381413459778
Epoch 1470, training loss: 311.9759826660156 = 0.08217687159776688 + 50.0 * 6.237875938415527
Epoch 1470, val loss: 0.9792746305465698
Epoch 1480, training loss: 311.9830322265625 = 0.08025234192609787 + 50.0 * 6.238055229187012
Epoch 1480, val loss: 0.9827947616577148
Epoch 1490, training loss: 311.985107421875 = 0.07839643955230713 + 50.0 * 6.238133907318115
Epoch 1490, val loss: 0.9864256978034973
Epoch 1500, training loss: 311.9089660644531 = 0.07657966762781143 + 50.0 * 6.236648082733154
Epoch 1500, val loss: 0.9898061156272888
Epoch 1510, training loss: 311.91033935546875 = 0.074830062687397 + 50.0 * 6.236710071563721
Epoch 1510, val loss: 0.9932293891906738
Epoch 1520, training loss: 312.06231689453125 = 0.07312053442001343 + 50.0 * 6.239784240722656
Epoch 1520, val loss: 0.9967479705810547
Epoch 1530, training loss: 311.89385986328125 = 0.07141686230897903 + 50.0 * 6.236449241638184
Epoch 1530, val loss: 1.0002918243408203
Epoch 1540, training loss: 311.8081359863281 = 0.06981278955936432 + 50.0 * 6.234766006469727
Epoch 1540, val loss: 1.0038232803344727
Epoch 1550, training loss: 311.7626037597656 = 0.06824976205825806 + 50.0 * 6.233887195587158
Epoch 1550, val loss: 1.0075355768203735
Epoch 1560, training loss: 311.9113464355469 = 0.0667540580034256 + 50.0 * 6.236892223358154
Epoch 1560, val loss: 1.011273980140686
Epoch 1570, training loss: 311.780029296875 = 0.06523659825325012 + 50.0 * 6.23429536819458
Epoch 1570, val loss: 1.0142666101455688
Epoch 1580, training loss: 311.8498229980469 = 0.06379429250955582 + 50.0 * 6.235720634460449
Epoch 1580, val loss: 1.0182660818099976
Epoch 1590, training loss: 311.7781066894531 = 0.062382716685533524 + 50.0 * 6.234314441680908
Epoch 1590, val loss: 1.0217355489730835
Epoch 1600, training loss: 311.7198791503906 = 0.061034563928842545 + 50.0 * 6.2331767082214355
Epoch 1600, val loss: 1.0252677202224731
Epoch 1610, training loss: 311.7235107421875 = 0.05972553789615631 + 50.0 * 6.233275890350342
Epoch 1610, val loss: 1.028995156288147
Epoch 1620, training loss: 311.70928955078125 = 0.0584426075220108 + 50.0 * 6.2330169677734375
Epoch 1620, val loss: 1.032530665397644
Epoch 1630, training loss: 311.8189697265625 = 0.05720577761530876 + 50.0 * 6.235235214233398
Epoch 1630, val loss: 1.0362684726715088
Epoch 1640, training loss: 311.6566467285156 = 0.0559769831597805 + 50.0 * 6.232013702392578
Epoch 1640, val loss: 1.0396814346313477
Epoch 1650, training loss: 311.6097412109375 = 0.05478397384285927 + 50.0 * 6.2310991287231445
Epoch 1650, val loss: 1.0433754920959473
Epoch 1660, training loss: 311.6488342285156 = 0.05363283306360245 + 50.0 * 6.231904029846191
Epoch 1660, val loss: 1.047054409980774
Epoch 1670, training loss: 311.7361145019531 = 0.052511412650346756 + 50.0 * 6.23367166519165
Epoch 1670, val loss: 1.0507515668869019
Epoch 1680, training loss: 311.6444091796875 = 0.051415033638477325 + 50.0 * 6.2318596839904785
Epoch 1680, val loss: 1.0542577505111694
Epoch 1690, training loss: 311.5502014160156 = 0.05035839229822159 + 50.0 * 6.229997158050537
Epoch 1690, val loss: 1.0578739643096924
Epoch 1700, training loss: 311.5475158691406 = 0.049331530928611755 + 50.0 * 6.229963779449463
Epoch 1700, val loss: 1.061482548713684
Epoch 1710, training loss: 311.6197814941406 = 0.04833094775676727 + 50.0 * 6.231429100036621
Epoch 1710, val loss: 1.0650193691253662
Epoch 1720, training loss: 311.60479736328125 = 0.04735279455780983 + 50.0 * 6.231148719787598
Epoch 1720, val loss: 1.0686053037643433
Epoch 1730, training loss: 311.8500061035156 = 0.04639406502246857 + 50.0 * 6.236072540283203
Epoch 1730, val loss: 1.0725862979888916
Epoch 1740, training loss: 311.5866394042969 = 0.04545767605304718 + 50.0 * 6.230823516845703
Epoch 1740, val loss: 1.0755207538604736
Epoch 1750, training loss: 311.5264892578125 = 0.04453982040286064 + 50.0 * 6.229639053344727
Epoch 1750, val loss: 1.0795810222625732
Epoch 1760, training loss: 311.5983581542969 = 0.043663859367370605 + 50.0 * 6.231093883514404
Epoch 1760, val loss: 1.082977056503296
Epoch 1770, training loss: 311.5384826660156 = 0.04280094429850578 + 50.0 * 6.229913234710693
Epoch 1770, val loss: 1.0862706899642944
Epoch 1780, training loss: 311.4246826171875 = 0.041953038424253464 + 50.0 * 6.227654457092285
Epoch 1780, val loss: 1.0900795459747314
Epoch 1790, training loss: 311.4344482421875 = 0.04114774987101555 + 50.0 * 6.227866172790527
Epoch 1790, val loss: 1.0937169790267944
Epoch 1800, training loss: 311.45465087890625 = 0.04035810008645058 + 50.0 * 6.228286266326904
Epoch 1800, val loss: 1.097091794013977
Epoch 1810, training loss: 311.5034484863281 = 0.03958385810256004 + 50.0 * 6.22927713394165
Epoch 1810, val loss: 1.1005057096481323
Epoch 1820, training loss: 311.46673583984375 = 0.038820136338472366 + 50.0 * 6.228558540344238
Epoch 1820, val loss: 1.1042869091033936
Epoch 1830, training loss: 311.5177307128906 = 0.03809294104576111 + 50.0 * 6.229592800140381
Epoch 1830, val loss: 1.1079282760620117
Epoch 1840, training loss: 311.40521240234375 = 0.03736451640725136 + 50.0 * 6.227357387542725
Epoch 1840, val loss: 1.11116623878479
Epoch 1850, training loss: 311.6464538574219 = 0.036679208278656006 + 50.0 * 6.2321953773498535
Epoch 1850, val loss: 1.1148778200149536
Epoch 1860, training loss: 311.3858642578125 = 0.0359705314040184 + 50.0 * 6.2269978523254395
Epoch 1860, val loss: 1.1183717250823975
Epoch 1870, training loss: 311.3027038574219 = 0.035318322479724884 + 50.0 * 6.225347995758057
Epoch 1870, val loss: 1.1219568252563477
Epoch 1880, training loss: 311.2669677734375 = 0.03467300534248352 + 50.0 * 6.224646091461182
Epoch 1880, val loss: 1.1253286600112915
Epoch 1890, training loss: 311.4637756347656 = 0.03405879810452461 + 50.0 * 6.2285943031311035
Epoch 1890, val loss: 1.1288052797317505
Epoch 1900, training loss: 311.3065490722656 = 0.03342527896165848 + 50.0 * 6.225462436676025
Epoch 1900, val loss: 1.1323179006576538
Epoch 1910, training loss: 311.2859191894531 = 0.03281896561384201 + 50.0 * 6.225062370300293
Epoch 1910, val loss: 1.1356219053268433
Epoch 1920, training loss: 311.3376159667969 = 0.032236404716968536 + 50.0 * 6.226108074188232
Epoch 1920, val loss: 1.1392441987991333
Epoch 1930, training loss: 311.2774353027344 = 0.03166009485721588 + 50.0 * 6.224915981292725
Epoch 1930, val loss: 1.1426680088043213
Epoch 1940, training loss: 311.29132080078125 = 0.031101008877158165 + 50.0 * 6.2252044677734375
Epoch 1940, val loss: 1.1462537050247192
Epoch 1950, training loss: 311.29022216796875 = 0.030559275299310684 + 50.0 * 6.225193500518799
Epoch 1950, val loss: 1.1494429111480713
Epoch 1960, training loss: 311.2711486816406 = 0.030026856809854507 + 50.0 * 6.224822521209717
Epoch 1960, val loss: 1.152665138244629
Epoch 1970, training loss: 311.3746643066406 = 0.02950977347791195 + 50.0 * 6.226902961730957
Epoch 1970, val loss: 1.1560395956039429
Epoch 1980, training loss: 311.2060852050781 = 0.028998777270317078 + 50.0 * 6.223541736602783
Epoch 1980, val loss: 1.1592155694961548
Epoch 1990, training loss: 311.1952209472656 = 0.028506804257631302 + 50.0 * 6.223334312438965
Epoch 1990, val loss: 1.162489652633667
Epoch 2000, training loss: 311.2098693847656 = 0.028028953820466995 + 50.0 * 6.223637104034424
Epoch 2000, val loss: 1.1656913757324219
Epoch 2010, training loss: 311.2478332519531 = 0.027559630572795868 + 50.0 * 6.224405288696289
Epoch 2010, val loss: 1.168993353843689
Epoch 2020, training loss: 311.0905456542969 = 0.027092691510915756 + 50.0 * 6.221268653869629
Epoch 2020, val loss: 1.172482967376709
Epoch 2030, training loss: 311.2467041015625 = 0.026650074869394302 + 50.0 * 6.224401473999023
Epoch 2030, val loss: 1.1757513284683228
Epoch 2040, training loss: 311.1919860839844 = 0.026203777641057968 + 50.0 * 6.223315238952637
Epoch 2040, val loss: 1.1786291599273682
Epoch 2050, training loss: 311.26373291015625 = 0.025775959715247154 + 50.0 * 6.224759101867676
Epoch 2050, val loss: 1.181875467300415
Epoch 2060, training loss: 311.2282409667969 = 0.02535233460366726 + 50.0 * 6.224057674407959
Epoch 2060, val loss: 1.1853758096694946
Epoch 2070, training loss: 311.1009826660156 = 0.02494247630238533 + 50.0 * 6.221520900726318
Epoch 2070, val loss: 1.1882787942886353
Epoch 2080, training loss: 311.1667785644531 = 0.024546189233660698 + 50.0 * 6.222845077514648
Epoch 2080, val loss: 1.1914751529693604
Epoch 2090, training loss: 311.11944580078125 = 0.02415579743683338 + 50.0 * 6.22190523147583
Epoch 2090, val loss: 1.1944926977157593
Epoch 2100, training loss: 311.10406494140625 = 0.02376960590481758 + 50.0 * 6.2216057777404785
Epoch 2100, val loss: 1.1979142427444458
Epoch 2110, training loss: 311.1733093261719 = 0.023404238745570183 + 50.0 * 6.222998142242432
Epoch 2110, val loss: 1.2007392644882202
Epoch 2120, training loss: 311.04071044921875 = 0.023028407245874405 + 50.0 * 6.220353126525879
Epoch 2120, val loss: 1.203948974609375
Epoch 2130, training loss: 311.0264587402344 = 0.022672666236758232 + 50.0 * 6.220075607299805
Epoch 2130, val loss: 1.2071667909622192
Epoch 2140, training loss: 311.010986328125 = 0.02232874184846878 + 50.0 * 6.219772815704346
Epoch 2140, val loss: 1.210157871246338
Epoch 2150, training loss: 311.22265625 = 0.021988537162542343 + 50.0 * 6.224013328552246
Epoch 2150, val loss: 1.2130323648452759
Epoch 2160, training loss: 311.04974365234375 = 0.021655656397342682 + 50.0 * 6.220561504364014
Epoch 2160, val loss: 1.2161791324615479
Epoch 2170, training loss: 310.9840393066406 = 0.021318882703781128 + 50.0 * 6.219254016876221
Epoch 2170, val loss: 1.2190295457839966
Epoch 2180, training loss: 310.94171142578125 = 0.021005287766456604 + 50.0 * 6.218414306640625
Epoch 2180, val loss: 1.2222099304199219
Epoch 2190, training loss: 310.9370422363281 = 0.02069484069943428 + 50.0 * 6.218327045440674
Epoch 2190, val loss: 1.2252014875411987
Epoch 2200, training loss: 311.19549560546875 = 0.020397810265421867 + 50.0 * 6.223502159118652
Epoch 2200, val loss: 1.2283093929290771
Epoch 2210, training loss: 310.99810791015625 = 0.020090699195861816 + 50.0 * 6.219560623168945
Epoch 2210, val loss: 1.2308026552200317
Epoch 2220, training loss: 310.96258544921875 = 0.019798357039690018 + 50.0 * 6.218855381011963
Epoch 2220, val loss: 1.233886957168579
Epoch 2230, training loss: 310.9905700683594 = 0.019514720886945724 + 50.0 * 6.21942138671875
Epoch 2230, val loss: 1.2366397380828857
Epoch 2240, training loss: 310.8551025390625 = 0.019229978322982788 + 50.0 * 6.21671724319458
Epoch 2240, val loss: 1.2397388219833374
Epoch 2250, training loss: 310.8739929199219 = 0.01896011084318161 + 50.0 * 6.217100620269775
Epoch 2250, val loss: 1.2426717281341553
Epoch 2260, training loss: 311.1741027832031 = 0.018703818321228027 + 50.0 * 6.223107814788818
Epoch 2260, val loss: 1.2453035116195679
Epoch 2270, training loss: 311.0372009277344 = 0.0184317696839571 + 50.0 * 6.2203755378723145
Epoch 2270, val loss: 1.2479442358016968
Epoch 2280, training loss: 310.9729309082031 = 0.018165554851293564 + 50.0 * 6.219095230102539
Epoch 2280, val loss: 1.2511812448501587
Epoch 2290, training loss: 310.95965576171875 = 0.017914611846208572 + 50.0 * 6.21883487701416
Epoch 2290, val loss: 1.2536840438842773
Epoch 2300, training loss: 310.8206787109375 = 0.017666425555944443 + 50.0 * 6.216060161590576
Epoch 2300, val loss: 1.2565711736679077
Epoch 2310, training loss: 310.80950927734375 = 0.017428899183869362 + 50.0 * 6.215841770172119
Epoch 2310, val loss: 1.259393334388733
Epoch 2320, training loss: 310.8163146972656 = 0.017198577523231506 + 50.0 * 6.215981960296631
Epoch 2320, val loss: 1.262120246887207
Epoch 2330, training loss: 311.2154541015625 = 0.016972143203020096 + 50.0 * 6.223969459533691
Epoch 2330, val loss: 1.2648507356643677
Epoch 2340, training loss: 310.8957214355469 = 0.016730783507227898 + 50.0 * 6.2175798416137695
Epoch 2340, val loss: 1.2673929929733276
Epoch 2350, training loss: 310.77716064453125 = 0.016508132219314575 + 50.0 * 6.215212821960449
Epoch 2350, val loss: 1.2701913118362427
Epoch 2360, training loss: 310.7333068847656 = 0.016289660707116127 + 50.0 * 6.2143402099609375
Epoch 2360, val loss: 1.2729191780090332
Epoch 2370, training loss: 310.7622375488281 = 0.01608298532664776 + 50.0 * 6.214922904968262
Epoch 2370, val loss: 1.275614857673645
Epoch 2380, training loss: 311.1286926269531 = 0.015878718346357346 + 50.0 * 6.222256660461426
Epoch 2380, val loss: 1.278247594833374
Epoch 2390, training loss: 310.9355773925781 = 0.015670888125896454 + 50.0 * 6.218398094177246
Epoch 2390, val loss: 1.2804464101791382
Epoch 2400, training loss: 310.9886779785156 = 0.015464534051716328 + 50.0 * 6.21946382522583
Epoch 2400, val loss: 1.2831169366836548
Epoch 2410, training loss: 310.7815856933594 = 0.015259564854204655 + 50.0 * 6.21532678604126
Epoch 2410, val loss: 1.2856605052947998
Epoch 2420, training loss: 310.8193359375 = 0.015065822750329971 + 50.0 * 6.216085433959961
Epoch 2420, val loss: 1.2882676124572754
Epoch 2430, training loss: 310.7364196777344 = 0.01487646996974945 + 50.0 * 6.214431285858154
Epoch 2430, val loss: 1.2909095287322998
Epoch 2440, training loss: 310.7017822265625 = 0.014690926298499107 + 50.0 * 6.213741779327393
Epoch 2440, val loss: 1.293614149093628
Epoch 2450, training loss: 310.6847229003906 = 0.014513392001390457 + 50.0 * 6.213404178619385
Epoch 2450, val loss: 1.2960962057113647
Epoch 2460, training loss: 310.8731994628906 = 0.014340298250317574 + 50.0 * 6.217177391052246
Epoch 2460, val loss: 1.298572063446045
Epoch 2470, training loss: 310.695556640625 = 0.014160930179059505 + 50.0 * 6.213627815246582
Epoch 2470, val loss: 1.3008989095687866
Epoch 2480, training loss: 310.82208251953125 = 0.013986729085445404 + 50.0 * 6.216162204742432
Epoch 2480, val loss: 1.3032230138778687
Epoch 2490, training loss: 310.74224853515625 = 0.013811099343001842 + 50.0 * 6.214568614959717
Epoch 2490, val loss: 1.305729627609253
Epoch 2500, training loss: 310.7009582519531 = 0.013647185638546944 + 50.0 * 6.213746070861816
Epoch 2500, val loss: 1.3083804845809937
Epoch 2510, training loss: 310.722412109375 = 0.013486090116202831 + 50.0 * 6.214178562164307
Epoch 2510, val loss: 1.3105219602584839
Epoch 2520, training loss: 310.7365417480469 = 0.013327008113265038 + 50.0 * 6.21446418762207
Epoch 2520, val loss: 1.3129504919052124
Epoch 2530, training loss: 310.67547607421875 = 0.013168582692742348 + 50.0 * 6.213245868682861
Epoch 2530, val loss: 1.3156378269195557
Epoch 2540, training loss: 310.68267822265625 = 0.013014481402933598 + 50.0 * 6.213393688201904
Epoch 2540, val loss: 1.3179086446762085
Epoch 2550, training loss: 310.7684631347656 = 0.012867230921983719 + 50.0 * 6.21511173248291
Epoch 2550, val loss: 1.3202348947525024
Epoch 2560, training loss: 310.7423400878906 = 0.01271909475326538 + 50.0 * 6.214592456817627
Epoch 2560, val loss: 1.3224862813949585
Epoch 2570, training loss: 310.6282653808594 = 0.012568212114274502 + 50.0 * 6.212313652038574
Epoch 2570, val loss: 1.3248045444488525
Epoch 2580, training loss: 310.6161193847656 = 0.012429509311914444 + 50.0 * 6.212074279785156
Epoch 2580, val loss: 1.327075719833374
Epoch 2590, training loss: 310.9195556640625 = 0.012290893122553825 + 50.0 * 6.218145370483398
Epoch 2590, val loss: 1.329092264175415
Epoch 2600, training loss: 310.6121520996094 = 0.012149110436439514 + 50.0 * 6.211999893188477
Epoch 2600, val loss: 1.3314486742019653
Epoch 2610, training loss: 310.5523986816406 = 0.012015057727694511 + 50.0 * 6.2108073234558105
Epoch 2610, val loss: 1.3336164951324463
Epoch 2620, training loss: 310.7215576171875 = 0.011888500303030014 + 50.0 * 6.214192867279053
Epoch 2620, val loss: 1.3356343507766724
Epoch 2630, training loss: 310.5926818847656 = 0.01175156980752945 + 50.0 * 6.211618423461914
Epoch 2630, val loss: 1.338079810142517
Epoch 2640, training loss: 310.6423034667969 = 0.01162294764071703 + 50.0 * 6.212613582611084
Epoch 2640, val loss: 1.340488076210022
Epoch 2650, training loss: 310.70709228515625 = 0.011500165797770023 + 50.0 * 6.213912010192871
Epoch 2650, val loss: 1.342386245727539
Epoch 2660, training loss: 310.4941101074219 = 0.011373034678399563 + 50.0 * 6.209654331207275
Epoch 2660, val loss: 1.344595193862915
Epoch 2670, training loss: 310.54315185546875 = 0.011254078708589077 + 50.0 * 6.21063756942749
Epoch 2670, val loss: 1.3467191457748413
Epoch 2680, training loss: 310.75860595703125 = 0.01114251185208559 + 50.0 * 6.214949607849121
Epoch 2680, val loss: 1.348733901977539
Epoch 2690, training loss: 310.64666748046875 = 0.011017887853085995 + 50.0 * 6.212713241577148
Epoch 2690, val loss: 1.3508176803588867
Epoch 2700, training loss: 310.87030029296875 = 0.010900615714490414 + 50.0 * 6.217187881469727
Epoch 2700, val loss: 1.3527644872665405
Epoch 2710, training loss: 310.5819091796875 = 0.010782547295093536 + 50.0 * 6.211422920227051
Epoch 2710, val loss: 1.3550790548324585
Epoch 2720, training loss: 310.48284912109375 = 0.01066870242357254 + 50.0 * 6.209443092346191
Epoch 2720, val loss: 1.3570114374160767
Epoch 2730, training loss: 310.4511413574219 = 0.010561411269009113 + 50.0 * 6.2088117599487305
Epoch 2730, val loss: 1.359209418296814
Epoch 2740, training loss: 310.4966735839844 = 0.010458817705512047 + 50.0 * 6.209724426269531
Epoch 2740, val loss: 1.3611549139022827
Epoch 2750, training loss: 310.8141784667969 = 0.010357151739299297 + 50.0 * 6.216076850891113
Epoch 2750, val loss: 1.362843632698059
Epoch 2760, training loss: 310.6379699707031 = 0.010244033299386501 + 50.0 * 6.212554454803467
Epoch 2760, val loss: 1.3653064966201782
Epoch 2770, training loss: 310.5703125 = 0.010145096108317375 + 50.0 * 6.211203575134277
Epoch 2770, val loss: 1.3671014308929443
Epoch 2780, training loss: 310.5506591796875 = 0.010041297413408756 + 50.0 * 6.210812091827393
Epoch 2780, val loss: 1.3691506385803223
Epoch 2790, training loss: 310.44610595703125 = 0.009941253811120987 + 50.0 * 6.208723068237305
Epoch 2790, val loss: 1.3711163997650146
Epoch 2800, training loss: 310.5069885253906 = 0.009845225140452385 + 50.0 * 6.209942817687988
Epoch 2800, val loss: 1.373430609703064
Epoch 2810, training loss: 310.61102294921875 = 0.009750708937644958 + 50.0 * 6.212025165557861
Epoch 2810, val loss: 1.375214695930481
Epoch 2820, training loss: 310.5234069824219 = 0.009656017646193504 + 50.0 * 6.210275173187256
Epoch 2820, val loss: 1.3768807649612427
Epoch 2830, training loss: 310.5066833496094 = 0.009562177583575249 + 50.0 * 6.209942817687988
Epoch 2830, val loss: 1.3789852857589722
Epoch 2840, training loss: 310.6125183105469 = 0.009471544995903969 + 50.0 * 6.212060928344727
Epoch 2840, val loss: 1.38068687915802
Epoch 2850, training loss: 310.45245361328125 = 0.009376123547554016 + 50.0 * 6.208861827850342
Epoch 2850, val loss: 1.3826682567596436
Epoch 2860, training loss: 310.39422607421875 = 0.00929026585072279 + 50.0 * 6.207698822021484
Epoch 2860, val loss: 1.3846330642700195
Epoch 2870, training loss: 310.48553466796875 = 0.00920507125556469 + 50.0 * 6.209526538848877
Epoch 2870, val loss: 1.386607050895691
Epoch 2880, training loss: 310.5586853027344 = 0.009120018221437931 + 50.0 * 6.210991382598877
Epoch 2880, val loss: 1.3882871866226196
Epoch 2890, training loss: 310.6416015625 = 0.009034681133925915 + 50.0 * 6.212651252746582
Epoch 2890, val loss: 1.3899328708648682
Epoch 2900, training loss: 310.412109375 = 0.008947627618908882 + 50.0 * 6.208063125610352
Epoch 2900, val loss: 1.3921856880187988
Epoch 2910, training loss: 310.4034423828125 = 0.008867534808814526 + 50.0 * 6.207891464233398
Epoch 2910, val loss: 1.3940120935440063
Epoch 2920, training loss: 310.42156982421875 = 0.008788942359387875 + 50.0 * 6.208255767822266
Epoch 2920, val loss: 1.3958271741867065
Epoch 2930, training loss: 310.4671630859375 = 0.00871031079441309 + 50.0 * 6.209169387817383
Epoch 2930, val loss: 1.3975319862365723
Epoch 2940, training loss: 310.38116455078125 = 0.008633062243461609 + 50.0 * 6.2074503898620605
Epoch 2940, val loss: 1.3993128538131714
Epoch 2950, training loss: 310.4283752441406 = 0.008559499867260456 + 50.0 * 6.2083964347839355
Epoch 2950, val loss: 1.4009650945663452
Epoch 2960, training loss: 310.4376525878906 = 0.008482517674565315 + 50.0 * 6.208583354949951
Epoch 2960, val loss: 1.4028542041778564
Epoch 2970, training loss: 310.53594970703125 = 0.008408879861235619 + 50.0 * 6.210550785064697
Epoch 2970, val loss: 1.4047695398330688
Epoch 2980, training loss: 310.36041259765625 = 0.008331336081027985 + 50.0 * 6.2070417404174805
Epoch 2980, val loss: 1.406545877456665
Epoch 2990, training loss: 310.3287658691406 = 0.008258674293756485 + 50.0 * 6.206409931182861
Epoch 2990, val loss: 1.40812087059021
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8408012651555088
The final CL Acc:0.75185, 0.00800, The final GNN Acc:0.83922, 0.00262
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11650])
remove edge: torch.Size([2, 9462])
updated graph: torch.Size([2, 10556])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7822265625 = 1.9407988786697388 + 50.0 * 8.59682846069336
Epoch 0, val loss: 1.9434491395950317
Epoch 10, training loss: 431.73284912109375 = 1.9312561750411987 + 50.0 * 8.59603214263916
Epoch 10, val loss: 1.9341181516647339
Epoch 20, training loss: 431.4743957519531 = 1.9192885160446167 + 50.0 * 8.591102600097656
Epoch 20, val loss: 1.9220200777053833
Epoch 30, training loss: 429.89288330078125 = 1.9038408994674683 + 50.0 * 8.559781074523926
Epoch 30, val loss: 1.906224250793457
Epoch 40, training loss: 419.5342712402344 = 1.8848538398742676 + 50.0 * 8.352988243103027
Epoch 40, val loss: 1.8877384662628174
Epoch 50, training loss: 378.77130126953125 = 1.8631149530410767 + 50.0 * 7.538163661956787
Epoch 50, val loss: 1.8670696020126343
Epoch 60, training loss: 363.7752990722656 = 1.8487484455108643 + 50.0 * 7.23853063583374
Epoch 60, val loss: 1.85360848903656
Epoch 70, training loss: 356.7682189941406 = 1.836931824684143 + 50.0 * 7.098625659942627
Epoch 70, val loss: 1.8421400785446167
Epoch 80, training loss: 351.63958740234375 = 1.8256866931915283 + 50.0 * 6.996278285980225
Epoch 80, val loss: 1.8317564725875854
Epoch 90, training loss: 347.1991882324219 = 1.8166424036026 + 50.0 * 6.907650470733643
Epoch 90, val loss: 1.8232401609420776
Epoch 100, training loss: 342.99591064453125 = 1.8090920448303223 + 50.0 * 6.823736667633057
Epoch 100, val loss: 1.8158553838729858
Epoch 110, training loss: 339.2757568359375 = 1.8025985956192017 + 50.0 * 6.749462604522705
Epoch 110, val loss: 1.8093085289001465
Epoch 120, training loss: 336.3395080566406 = 1.7966381311416626 + 50.0 * 6.690857410430908
Epoch 120, val loss: 1.8031530380249023
Epoch 130, training loss: 334.0855407714844 = 1.7902798652648926 + 50.0 * 6.645905017852783
Epoch 130, val loss: 1.7967877388000488
Epoch 140, training loss: 332.4164123535156 = 1.783347249031067 + 50.0 * 6.612661361694336
Epoch 140, val loss: 1.790069580078125
Epoch 150, training loss: 330.97369384765625 = 1.775843858718872 + 50.0 * 6.583956718444824
Epoch 150, val loss: 1.7830147743225098
Epoch 160, training loss: 329.7091064453125 = 1.7678252458572388 + 50.0 * 6.558825969696045
Epoch 160, val loss: 1.775683879852295
Epoch 170, training loss: 328.64044189453125 = 1.7593379020690918 + 50.0 * 6.537621974945068
Epoch 170, val loss: 1.7681323289871216
Epoch 180, training loss: 327.9624328613281 = 1.7501448392868042 + 50.0 * 6.524245738983154
Epoch 180, val loss: 1.760141134262085
Epoch 190, training loss: 326.9321594238281 = 1.7401303052902222 + 50.0 * 6.503840446472168
Epoch 190, val loss: 1.751531958580017
Epoch 200, training loss: 326.171875 = 1.7292566299438477 + 50.0 * 6.488852500915527
Epoch 200, val loss: 1.7423774003982544
Epoch 210, training loss: 325.5269470214844 = 1.7174488306045532 + 50.0 * 6.476190090179443
Epoch 210, val loss: 1.7324817180633545
Epoch 220, training loss: 324.9615173339844 = 1.7046061754226685 + 50.0 * 6.465137958526611
Epoch 220, val loss: 1.721837043762207
Epoch 230, training loss: 324.4713134765625 = 1.6905748844146729 + 50.0 * 6.4556145668029785
Epoch 230, val loss: 1.7102998495101929
Epoch 240, training loss: 323.9712219238281 = 1.6753904819488525 + 50.0 * 6.445916652679443
Epoch 240, val loss: 1.697914958000183
Epoch 250, training loss: 323.52288818359375 = 1.6590713262557983 + 50.0 * 6.437276840209961
Epoch 250, val loss: 1.6846318244934082
Epoch 260, training loss: 323.12213134765625 = 1.641615867614746 + 50.0 * 6.429610729217529
Epoch 260, val loss: 1.670594573020935
Epoch 270, training loss: 322.7377014160156 = 1.6231765747070312 + 50.0 * 6.422290802001953
Epoch 270, val loss: 1.6559317111968994
Epoch 280, training loss: 322.39215087890625 = 1.6038224697113037 + 50.0 * 6.415766716003418
Epoch 280, val loss: 1.6407028436660767
Epoch 290, training loss: 322.0037536621094 = 1.58364999294281 + 50.0 * 6.408401966094971
Epoch 290, val loss: 1.6249785423278809
Epoch 300, training loss: 321.735595703125 = 1.5628362894058228 + 50.0 * 6.4034552574157715
Epoch 300, val loss: 1.6089160442352295
Epoch 310, training loss: 321.4214782714844 = 1.5414901971817017 + 50.0 * 6.397599220275879
Epoch 310, val loss: 1.592719316482544
Epoch 320, training loss: 321.33428955078125 = 1.5197504758834839 + 50.0 * 6.3962907791137695
Epoch 320, val loss: 1.5763756036758423
Epoch 330, training loss: 320.811767578125 = 1.4978069067001343 + 50.0 * 6.386279582977295
Epoch 330, val loss: 1.5600621700286865
Epoch 340, training loss: 320.522216796875 = 1.4758150577545166 + 50.0 * 6.380927562713623
Epoch 340, val loss: 1.5439521074295044
Epoch 350, training loss: 320.4814453125 = 1.4538533687591553 + 50.0 * 6.380551815032959
Epoch 350, val loss: 1.5279929637908936
Epoch 360, training loss: 320.0189208984375 = 1.4317880868911743 + 50.0 * 6.3717427253723145
Epoch 360, val loss: 1.512265682220459
Epoch 370, training loss: 319.8641357421875 = 1.4098405838012695 + 50.0 * 6.369085788726807
Epoch 370, val loss: 1.496836543083191
Epoch 380, training loss: 319.571533203125 = 1.3879867792129517 + 50.0 * 6.363670825958252
Epoch 380, val loss: 1.4816725254058838
Epoch 390, training loss: 319.3350524902344 = 1.3662294149398804 + 50.0 * 6.359376430511475
Epoch 390, val loss: 1.466709852218628
Epoch 400, training loss: 319.5246276855469 = 1.3444234132766724 + 50.0 * 6.3636040687561035
Epoch 400, val loss: 1.4518009424209595
Epoch 410, training loss: 319.0699462890625 = 1.3225549459457397 + 50.0 * 6.354948043823242
Epoch 410, val loss: 1.4369908571243286
Epoch 420, training loss: 318.789306640625 = 1.3007042407989502 + 50.0 * 6.3497724533081055
Epoch 420, val loss: 1.4224646091461182
Epoch 430, training loss: 318.5777282714844 = 1.2788598537445068 + 50.0 * 6.345977306365967
Epoch 430, val loss: 1.4080408811569214
Epoch 440, training loss: 318.79974365234375 = 1.2569259405136108 + 50.0 * 6.350856304168701
Epoch 440, val loss: 1.3937824964523315
Epoch 450, training loss: 318.3653869628906 = 1.2347906827926636 + 50.0 * 6.342611789703369
Epoch 450, val loss: 1.3790920972824097
Epoch 460, training loss: 318.0963134765625 = 1.21260666847229 + 50.0 * 6.337674140930176
Epoch 460, val loss: 1.364790439605713
Epoch 470, training loss: 317.9095458984375 = 1.1904935836791992 + 50.0 * 6.334381103515625
Epoch 470, val loss: 1.3505419492721558
Epoch 480, training loss: 318.23193359375 = 1.1683813333511353 + 50.0 * 6.341270446777344
Epoch 480, val loss: 1.3363854885101318
Epoch 490, training loss: 317.7064514160156 = 1.1458799839019775 + 50.0 * 6.331211566925049
Epoch 490, val loss: 1.3221800327301025
Epoch 500, training loss: 317.49371337890625 = 1.1235888004302979 + 50.0 * 6.327402114868164
Epoch 500, val loss: 1.3081510066986084
Epoch 510, training loss: 317.329345703125 = 1.1013933420181274 + 50.0 * 6.324559211730957
Epoch 510, val loss: 1.2943432331085205
Epoch 520, training loss: 317.590087890625 = 1.0791982412338257 + 50.0 * 6.3302178382873535
Epoch 520, val loss: 1.2806869745254517
Epoch 530, training loss: 317.20025634765625 = 1.056990385055542 + 50.0 * 6.3228654861450195
Epoch 530, val loss: 1.2671451568603516
Epoch 540, training loss: 316.9747314453125 = 1.0350263118743896 + 50.0 * 6.318794250488281
Epoch 540, val loss: 1.2539125680923462
Epoch 550, training loss: 316.82147216796875 = 1.013309359550476 + 50.0 * 6.316163063049316
Epoch 550, val loss: 1.2410569190979004
Epoch 560, training loss: 316.7816162109375 = 0.9918129444122314 + 50.0 * 6.3157958984375
Epoch 560, val loss: 1.228665828704834
Epoch 570, training loss: 316.91851806640625 = 0.9704984426498413 + 50.0 * 6.318960666656494
Epoch 570, val loss: 1.2158843278884888
Epoch 580, training loss: 316.5863342285156 = 0.9492911696434021 + 50.0 * 6.312740802764893
Epoch 580, val loss: 1.2038227319717407
Epoch 590, training loss: 316.3955993652344 = 0.9286134839057922 + 50.0 * 6.30933952331543
Epoch 590, val loss: 1.1923351287841797
Epoch 600, training loss: 316.2613830566406 = 0.9083653092384338 + 50.0 * 6.307060241699219
Epoch 600, val loss: 1.181249976158142
Epoch 610, training loss: 316.3481140136719 = 0.8885118961334229 + 50.0 * 6.309191703796387
Epoch 610, val loss: 1.1705429553985596
Epoch 620, training loss: 316.2928161621094 = 0.8688213229179382 + 50.0 * 6.3084797859191895
Epoch 620, val loss: 1.1600518226623535
Epoch 630, training loss: 315.9649963378906 = 0.8495793342590332 + 50.0 * 6.302308082580566
Epoch 630, val loss: 1.1499993801116943
Epoch 640, training loss: 315.9310302734375 = 0.8308824300765991 + 50.0 * 6.302002906799316
Epoch 640, val loss: 1.1404881477355957
Epoch 650, training loss: 315.8945617675781 = 0.8126015663146973 + 50.0 * 6.301639556884766
Epoch 650, val loss: 1.131372332572937
Epoch 660, training loss: 315.69061279296875 = 0.7947925925254822 + 50.0 * 6.297916412353516
Epoch 660, val loss: 1.122731328010559
Epoch 670, training loss: 315.59136962890625 = 0.7774344086647034 + 50.0 * 6.296278953552246
Epoch 670, val loss: 1.114566445350647
Epoch 680, training loss: 315.7976379394531 = 0.7604686617851257 + 50.0 * 6.300743103027344
Epoch 680, val loss: 1.1066629886627197
Epoch 690, training loss: 315.5069274902344 = 0.7439637780189514 + 50.0 * 6.295259475708008
Epoch 690, val loss: 1.0993714332580566
Epoch 700, training loss: 315.3565368652344 = 0.7277712225914001 + 50.0 * 6.292575359344482
Epoch 700, val loss: 1.0922489166259766
Epoch 710, training loss: 315.243408203125 = 0.7120245695114136 + 50.0 * 6.290627479553223
Epoch 710, val loss: 1.085594654083252
Epoch 720, training loss: 315.4894104003906 = 0.6966636180877686 + 50.0 * 6.2958550453186035
Epoch 720, val loss: 1.0792515277862549
Epoch 730, training loss: 315.164306640625 = 0.6814849376678467 + 50.0 * 6.289656639099121
Epoch 730, val loss: 1.0730822086334229
Epoch 740, training loss: 315.0555114746094 = 0.6667537093162537 + 50.0 * 6.28777551651001
Epoch 740, val loss: 1.0673673152923584
Epoch 750, training loss: 315.0820617675781 = 0.6522877812385559 + 50.0 * 6.288595676422119
Epoch 750, val loss: 1.0619064569473267
Epoch 760, training loss: 314.9422607421875 = 0.6381396651268005 + 50.0 * 6.2860822677612305
Epoch 760, val loss: 1.056707501411438
Epoch 770, training loss: 314.8491516113281 = 0.624228298664093 + 50.0 * 6.28449821472168
Epoch 770, val loss: 1.051808476448059
Epoch 780, training loss: 314.74786376953125 = 0.6105151772499084 + 50.0 * 6.282747268676758
Epoch 780, val loss: 1.0470819473266602
Epoch 790, training loss: 314.778076171875 = 0.5970693826675415 + 50.0 * 6.2836198806762695
Epoch 790, val loss: 1.0427955389022827
Epoch 800, training loss: 314.72406005859375 = 0.5837500691413879 + 50.0 * 6.282806396484375
Epoch 800, val loss: 1.038176417350769
Epoch 810, training loss: 314.5928955078125 = 0.5705655813217163 + 50.0 * 6.280447006225586
Epoch 810, val loss: 1.0341085195541382
Epoch 820, training loss: 314.4427795410156 = 0.557716429233551 + 50.0 * 6.277701377868652
Epoch 820, val loss: 1.0302186012268066
Epoch 830, training loss: 314.51214599609375 = 0.5450792908668518 + 50.0 * 6.279341697692871
Epoch 830, val loss: 1.0264314413070679
Epoch 840, training loss: 314.3401184082031 = 0.532385528087616 + 50.0 * 6.276154518127441
Epoch 840, val loss: 1.0228155851364136
Epoch 850, training loss: 314.3327941894531 = 0.5199312567710876 + 50.0 * 6.276257514953613
Epoch 850, val loss: 1.0192619562149048
Epoch 860, training loss: 314.3656005859375 = 0.5076159238815308 + 50.0 * 6.277159690856934
Epoch 860, val loss: 1.0161230564117432
Epoch 870, training loss: 314.2324523925781 = 0.495475172996521 + 50.0 * 6.2747392654418945
Epoch 870, val loss: 1.012813687324524
Epoch 880, training loss: 314.0953063964844 = 0.48356011509895325 + 50.0 * 6.272234916687012
Epoch 880, val loss: 1.009908676147461
Epoch 890, training loss: 314.0367126464844 = 0.4718055725097656 + 50.0 * 6.271297931671143
Epoch 890, val loss: 1.0072410106658936
Epoch 900, training loss: 314.1322021484375 = 0.46025779843330383 + 50.0 * 6.273438930511475
Epoch 900, val loss: 1.004542350769043
Epoch 910, training loss: 314.2150573730469 = 0.44890743494033813 + 50.0 * 6.275322914123535
Epoch 910, val loss: 1.0021682977676392
Epoch 920, training loss: 313.9744873046875 = 0.4376044273376465 + 50.0 * 6.270737648010254
Epoch 920, val loss: 0.999932050704956
Epoch 930, training loss: 313.9664001464844 = 0.4266741871833801 + 50.0 * 6.270794868469238
Epoch 930, val loss: 0.9979438185691833
Epoch 940, training loss: 313.8558349609375 = 0.4159370958805084 + 50.0 * 6.268798351287842
Epoch 940, val loss: 0.9963757395744324
Epoch 950, training loss: 313.78729248046875 = 0.40537765622138977 + 50.0 * 6.267638683319092
Epoch 950, val loss: 0.994667112827301
Epoch 960, training loss: 313.6787109375 = 0.39507412910461426 + 50.0 * 6.26567268371582
Epoch 960, val loss: 0.9936153888702393
Epoch 970, training loss: 314.0086975097656 = 0.3850516080856323 + 50.0 * 6.272472858428955
Epoch 970, val loss: 0.9925054311752319
Epoch 980, training loss: 313.6916809082031 = 0.37523844838142395 + 50.0 * 6.266328811645508
Epoch 980, val loss: 0.9918015003204346
Epoch 990, training loss: 313.5888977050781 = 0.36569708585739136 + 50.0 * 6.264463901519775
Epoch 990, val loss: 0.9913938045501709
Epoch 1000, training loss: 313.5956726074219 = 0.3564101755619049 + 50.0 * 6.264785289764404
Epoch 1000, val loss: 0.9910597205162048
Epoch 1010, training loss: 313.5118713378906 = 0.34728819131851196 + 50.0 * 6.263291835784912
Epoch 1010, val loss: 0.99102783203125
Epoch 1020, training loss: 313.60858154296875 = 0.33856746554374695 + 50.0 * 6.265400409698486
Epoch 1020, val loss: 0.991516649723053
Epoch 1030, training loss: 313.54754638671875 = 0.3299647867679596 + 50.0 * 6.264351844787598
Epoch 1030, val loss: 0.9916585087776184
Epoch 1040, training loss: 313.3354187011719 = 0.3216003179550171 + 50.0 * 6.2602763175964355
Epoch 1040, val loss: 0.9924896359443665
Epoch 1050, training loss: 313.2804260253906 = 0.31356412172317505 + 50.0 * 6.259336948394775
Epoch 1050, val loss: 0.9934579133987427
Epoch 1060, training loss: 313.2298583984375 = 0.305788516998291 + 50.0 * 6.258481502532959
Epoch 1060, val loss: 0.9945911765098572
Epoch 1070, training loss: 313.3521728515625 = 0.2982483506202698 + 50.0 * 6.261078357696533
Epoch 1070, val loss: 0.9958059787750244
Epoch 1080, training loss: 313.4937438964844 = 0.2908592224121094 + 50.0 * 6.264057636260986
Epoch 1080, val loss: 0.9975096583366394
Epoch 1090, training loss: 313.1411437988281 = 0.2834801971912384 + 50.0 * 6.257153034210205
Epoch 1090, val loss: 0.9988968372344971
Epoch 1100, training loss: 313.1264953613281 = 0.27657008171081543 + 50.0 * 6.256998062133789
Epoch 1100, val loss: 1.0007812976837158
Epoch 1110, training loss: 313.04010009765625 = 0.26987403631210327 + 50.0 * 6.255404949188232
Epoch 1110, val loss: 1.0029035806655884
Epoch 1120, training loss: 313.1383972167969 = 0.26340746879577637 + 50.0 * 6.257500171661377
Epoch 1120, val loss: 1.005195140838623
Epoch 1130, training loss: 313.01495361328125 = 0.25700506567955017 + 50.0 * 6.255158424377441
Epoch 1130, val loss: 1.007455587387085
Epoch 1140, training loss: 312.9735107421875 = 0.25079023838043213 + 50.0 * 6.254454135894775
Epoch 1140, val loss: 1.0097694396972656
Epoch 1150, training loss: 313.19842529296875 = 0.24475574493408203 + 50.0 * 6.259073734283447
Epoch 1150, val loss: 1.0125569105148315
Epoch 1160, training loss: 312.99053955078125 = 0.23893176019191742 + 50.0 * 6.255032539367676
Epoch 1160, val loss: 1.014930009841919
Epoch 1170, training loss: 312.86541748046875 = 0.23326455056667328 + 50.0 * 6.252642631530762
Epoch 1170, val loss: 1.0178849697113037
Epoch 1180, training loss: 312.80584716796875 = 0.22780664265155792 + 50.0 * 6.251560688018799
Epoch 1180, val loss: 1.0209311246871948
Epoch 1190, training loss: 313.12628173828125 = 0.2224777489900589 + 50.0 * 6.258076190948486
Epoch 1190, val loss: 1.0239547491073608
Epoch 1200, training loss: 312.95196533203125 = 0.21727322041988373 + 50.0 * 6.254693508148193
Epoch 1200, val loss: 1.0271247625350952
Epoch 1210, training loss: 312.74932861328125 = 0.21217525005340576 + 50.0 * 6.2507429122924805
Epoch 1210, val loss: 1.0302637815475464
Epoch 1220, training loss: 312.6890563964844 = 0.20730598270893097 + 50.0 * 6.249634742736816
Epoch 1220, val loss: 1.033747911453247
Epoch 1230, training loss: 313.01800537109375 = 0.20256055891513824 + 50.0 * 6.256308555603027
Epoch 1230, val loss: 1.0371936559677124
Epoch 1240, training loss: 312.7915954589844 = 0.1978602409362793 + 50.0 * 6.251874923706055
Epoch 1240, val loss: 1.0406056642532349
Epoch 1250, training loss: 312.620849609375 = 0.19336064159870148 + 50.0 * 6.248549938201904
Epoch 1250, val loss: 1.0442758798599243
Epoch 1260, training loss: 312.56622314453125 = 0.1889868676662445 + 50.0 * 6.247544765472412
Epoch 1260, val loss: 1.0480430126190186
Epoch 1270, training loss: 312.9012145996094 = 0.18472597002983093 + 50.0 * 6.254329681396484
Epoch 1270, val loss: 1.0518261194229126
Epoch 1280, training loss: 312.65985107421875 = 0.18054822087287903 + 50.0 * 6.24958610534668
Epoch 1280, val loss: 1.055616855621338
Epoch 1290, training loss: 312.4971618652344 = 0.17649777233600616 + 50.0 * 6.246413707733154
Epoch 1290, val loss: 1.059492588043213
Epoch 1300, training loss: 312.4547119140625 = 0.17259779572486877 + 50.0 * 6.245642185211182
Epoch 1300, val loss: 1.0635764598846436
Epoch 1310, training loss: 312.5306396484375 = 0.16882924735546112 + 50.0 * 6.247236251831055
Epoch 1310, val loss: 1.0677404403686523
Epoch 1320, training loss: 312.4082946777344 = 0.16506411135196686 + 50.0 * 6.244864463806152
Epoch 1320, val loss: 1.071614384651184
Epoch 1330, training loss: 312.4797058105469 = 0.1614150106906891 + 50.0 * 6.246366024017334
Epoch 1330, val loss: 1.0757511854171753
Epoch 1340, training loss: 312.42724609375 = 0.15784744918346405 + 50.0 * 6.245388031005859
Epoch 1340, val loss: 1.079879641532898
Epoch 1350, training loss: 312.3363342285156 = 0.15442611277103424 + 50.0 * 6.243638515472412
Epoch 1350, val loss: 1.0842204093933105
Epoch 1360, training loss: 312.353271484375 = 0.1510930359363556 + 50.0 * 6.244043827056885
Epoch 1360, val loss: 1.088545799255371
Epoch 1370, training loss: 312.45977783203125 = 0.1478293389081955 + 50.0 * 6.246239185333252
Epoch 1370, val loss: 1.0927799940109253
Epoch 1380, training loss: 312.29150390625 = 0.14458847045898438 + 50.0 * 6.242938041687012
Epoch 1380, val loss: 1.0975788831710815
Epoch 1390, training loss: 312.2236022949219 = 0.1415119618177414 + 50.0 * 6.241641998291016
Epoch 1390, val loss: 1.1020649671554565
Epoch 1400, training loss: 312.28924560546875 = 0.138508602976799 + 50.0 * 6.243014812469482
Epoch 1400, val loss: 1.1066887378692627
Epoch 1410, training loss: 312.2730712890625 = 0.13555419445037842 + 50.0 * 6.24275016784668
Epoch 1410, val loss: 1.1111940145492554
Epoch 1420, training loss: 312.1726989746094 = 0.13261616230010986 + 50.0 * 6.2408013343811035
Epoch 1420, val loss: 1.1158682107925415
Epoch 1430, training loss: 312.1761474609375 = 0.12982545793056488 + 50.0 * 6.240926742553711
Epoch 1430, val loss: 1.1208622455596924
Epoch 1440, training loss: 312.3354187011719 = 0.12709219753742218 + 50.0 * 6.244166374206543
Epoch 1440, val loss: 1.125504732131958
Epoch 1450, training loss: 312.13116455078125 = 0.12440694868564606 + 50.0 * 6.2401347160339355
Epoch 1450, val loss: 1.1302740573883057
Epoch 1460, training loss: 312.1015319824219 = 0.12181837111711502 + 50.0 * 6.239593982696533
Epoch 1460, val loss: 1.1352349519729614
Epoch 1470, training loss: 312.4299011230469 = 0.11930758506059647 + 50.0 * 6.246212005615234
Epoch 1470, val loss: 1.139878511428833
Epoch 1480, training loss: 312.21240234375 = 0.11673295497894287 + 50.0 * 6.241913318634033
Epoch 1480, val loss: 1.1448471546173096
Epoch 1490, training loss: 312.0100402832031 = 0.11431106925010681 + 50.0 * 6.237914562225342
Epoch 1490, val loss: 1.1497443914413452
Epoch 1500, training loss: 312.0050048828125 = 0.11196749657392502 + 50.0 * 6.237860679626465
Epoch 1500, val loss: 1.1548439264297485
Epoch 1510, training loss: 312.3514404296875 = 0.1097007542848587 + 50.0 * 6.2448344230651855
Epoch 1510, val loss: 1.159422755241394
Epoch 1520, training loss: 312.08428955078125 = 0.10739585757255554 + 50.0 * 6.239537715911865
Epoch 1520, val loss: 1.164792537689209
Epoch 1530, training loss: 311.97369384765625 = 0.1051858440041542 + 50.0 * 6.237370014190674
Epoch 1530, val loss: 1.1696308851242065
Epoch 1540, training loss: 311.9028625488281 = 0.10306490212678909 + 50.0 * 6.235996246337891
Epoch 1540, val loss: 1.1748744249343872
Epoch 1550, training loss: 311.87652587890625 = 0.10099785029888153 + 50.0 * 6.23551082611084
Epoch 1550, val loss: 1.1798510551452637
Epoch 1560, training loss: 312.1907958984375 = 0.09900765866041183 + 50.0 * 6.241836071014404
Epoch 1560, val loss: 1.18495512008667
Epoch 1570, training loss: 312.23504638671875 = 0.09694688022136688 + 50.0 * 6.242762088775635
Epoch 1570, val loss: 1.1897680759429932
Epoch 1580, training loss: 311.8808898925781 = 0.09494268894195557 + 50.0 * 6.235718727111816
Epoch 1580, val loss: 1.1944124698638916
Epoch 1590, training loss: 311.8142395019531 = 0.09306339174509048 + 50.0 * 6.2344231605529785
Epoch 1590, val loss: 1.1998684406280518
Epoch 1600, training loss: 311.7794494628906 = 0.09122335910797119 + 50.0 * 6.2337646484375
Epoch 1600, val loss: 1.204949140548706
Epoch 1610, training loss: 312.0762023925781 = 0.08945801854133606 + 50.0 * 6.239735126495361
Epoch 1610, val loss: 1.2098321914672852
Epoch 1620, training loss: 311.9283142089844 = 0.08762563019990921 + 50.0 * 6.236814022064209
Epoch 1620, val loss: 1.214942216873169
Epoch 1630, training loss: 311.83245849609375 = 0.08586709946393967 + 50.0 * 6.234931468963623
Epoch 1630, val loss: 1.2197643518447876
Epoch 1640, training loss: 311.6933898925781 = 0.08418292552232742 + 50.0 * 6.232183933258057
Epoch 1640, val loss: 1.2250248193740845
Epoch 1650, training loss: 311.6941223144531 = 0.08256401121616364 + 50.0 * 6.232231140136719
Epoch 1650, val loss: 1.2302988767623901
Epoch 1660, training loss: 312.115966796875 = 0.08099989593029022 + 50.0 * 6.240699291229248
Epoch 1660, val loss: 1.2349567413330078
Epoch 1670, training loss: 311.8085021972656 = 0.07934990525245667 + 50.0 * 6.234583377838135
Epoch 1670, val loss: 1.2404606342315674
Epoch 1680, training loss: 311.7920837402344 = 0.07783956825733185 + 50.0 * 6.2342848777771
Epoch 1680, val loss: 1.245410680770874
Epoch 1690, training loss: 311.7356262207031 = 0.07632168382406235 + 50.0 * 6.2331862449646
Epoch 1690, val loss: 1.2506905794143677
Epoch 1700, training loss: 311.6750793457031 = 0.07484586536884308 + 50.0 * 6.232004642486572
Epoch 1700, val loss: 1.2556445598602295
Epoch 1710, training loss: 311.7210693359375 = 0.07343265414237976 + 50.0 * 6.23295259475708
Epoch 1710, val loss: 1.261245608329773
Epoch 1720, training loss: 311.6321716308594 = 0.07201755791902542 + 50.0 * 6.231203079223633
Epoch 1720, val loss: 1.2658852338790894
Epoch 1730, training loss: 311.7056579589844 = 0.0706401839852333 + 50.0 * 6.232700347900391
Epoch 1730, val loss: 1.2710243463516235
Epoch 1740, training loss: 311.5987854003906 = 0.06930308789014816 + 50.0 * 6.230589866638184
Epoch 1740, val loss: 1.2762401103973389
Epoch 1750, training loss: 311.66461181640625 = 0.06798902899026871 + 50.0 * 6.231932163238525
Epoch 1750, val loss: 1.281333327293396
Epoch 1760, training loss: 311.5149230957031 = 0.06671647727489471 + 50.0 * 6.228964328765869
Epoch 1760, val loss: 1.2864164113998413
Epoch 1770, training loss: 311.5384216308594 = 0.0654836967587471 + 50.0 * 6.229458808898926
Epoch 1770, val loss: 1.2916061878204346
Epoch 1780, training loss: 311.5918884277344 = 0.06427183002233505 + 50.0 * 6.2305521965026855
Epoch 1780, val loss: 1.296820878982544
Epoch 1790, training loss: 311.67279052734375 = 0.06307349354028702 + 50.0 * 6.232194423675537
Epoch 1790, val loss: 1.301721215248108
Epoch 1800, training loss: 311.5780334472656 = 0.06189887970685959 + 50.0 * 6.23032283782959
Epoch 1800, val loss: 1.3064539432525635
Epoch 1810, training loss: 311.5335388183594 = 0.06076112017035484 + 50.0 * 6.229455471038818
Epoch 1810, val loss: 1.3120359182357788
Epoch 1820, training loss: 311.5531921386719 = 0.05964208021759987 + 50.0 * 6.2298712730407715
Epoch 1820, val loss: 1.316917896270752
Epoch 1830, training loss: 311.572998046875 = 0.05854948237538338 + 50.0 * 6.230288505554199
Epoch 1830, val loss: 1.322022795677185
Epoch 1840, training loss: 311.4474792480469 = 0.057491861283779144 + 50.0 * 6.227799892425537
Epoch 1840, val loss: 1.326903223991394
Epoch 1850, training loss: 311.42596435546875 = 0.05646209418773651 + 50.0 * 6.227390289306641
Epoch 1850, val loss: 1.332149863243103
Epoch 1860, training loss: 311.57757568359375 = 0.05546000972390175 + 50.0 * 6.230442523956299
Epoch 1860, val loss: 1.3371984958648682
Epoch 1870, training loss: 311.461669921875 = 0.054454393684864044 + 50.0 * 6.228144645690918
Epoch 1870, val loss: 1.3417330980300903
Epoch 1880, training loss: 311.3987121582031 = 0.053486306220293045 + 50.0 * 6.226904392242432
Epoch 1880, val loss: 1.3466999530792236
Epoch 1890, training loss: 311.35968017578125 = 0.05254759266972542 + 50.0 * 6.226142406463623
Epoch 1890, val loss: 1.3519889116287231
Epoch 1900, training loss: 311.6432189941406 = 0.05163801461458206 + 50.0 * 6.2318315505981445
Epoch 1900, val loss: 1.3570319414138794
Epoch 1910, training loss: 311.5111999511719 = 0.05070662871003151 + 50.0 * 6.2292094230651855
Epoch 1910, val loss: 1.3609800338745117
Epoch 1920, training loss: 311.4507751464844 = 0.0498042069375515 + 50.0 * 6.2280192375183105
Epoch 1920, val loss: 1.366273045539856
Epoch 1930, training loss: 311.31182861328125 = 0.048941243439912796 + 50.0 * 6.225257873535156
Epoch 1930, val loss: 1.3709596395492554
Epoch 1940, training loss: 311.30328369140625 = 0.048104144632816315 + 50.0 * 6.225103855133057
Epoch 1940, val loss: 1.376132845878601
Epoch 1950, training loss: 311.3826599121094 = 0.04728582128882408 + 50.0 * 6.226707935333252
Epoch 1950, val loss: 1.3809640407562256
Epoch 1960, training loss: 311.3508605957031 = 0.04647315293550491 + 50.0 * 6.22608757019043
Epoch 1960, val loss: 1.385388731956482
Epoch 1970, training loss: 311.2594299316406 = 0.045687686651945114 + 50.0 * 6.224274635314941
Epoch 1970, val loss: 1.389846920967102
Epoch 1980, training loss: 311.3228454589844 = 0.04492512345314026 + 50.0 * 6.225558280944824
Epoch 1980, val loss: 1.3948026895523071
Epoch 1990, training loss: 311.4862976074219 = 0.04415683075785637 + 50.0 * 6.228842735290527
Epoch 1990, val loss: 1.3993408679962158
Epoch 2000, training loss: 311.29852294921875 = 0.0434151366353035 + 50.0 * 6.225101947784424
Epoch 2000, val loss: 1.4040929079055786
Epoch 2010, training loss: 311.2707214355469 = 0.04269132390618324 + 50.0 * 6.224560260772705
Epoch 2010, val loss: 1.4086482524871826
Epoch 2020, training loss: 311.2357482910156 = 0.04198887199163437 + 50.0 * 6.223875045776367
Epoch 2020, val loss: 1.4134376049041748
Epoch 2030, training loss: 311.1940002441406 = 0.04130169376730919 + 50.0 * 6.2230544090271
Epoch 2030, val loss: 1.4181703329086304
Epoch 2040, training loss: 311.35968017578125 = 0.04063502699136734 + 50.0 * 6.226380825042725
Epoch 2040, val loss: 1.422843098640442
Epoch 2050, training loss: 311.233642578125 = 0.03996241092681885 + 50.0 * 6.223873615264893
Epoch 2050, val loss: 1.4269858598709106
Epoch 2060, training loss: 311.19354248046875 = 0.03931550681591034 + 50.0 * 6.223084449768066
Epoch 2060, val loss: 1.4316942691802979
Epoch 2070, training loss: 311.3399658203125 = 0.03869399428367615 + 50.0 * 6.226025104522705
Epoch 2070, val loss: 1.4359166622161865
Epoch 2080, training loss: 311.1087646484375 = 0.03805223107337952 + 50.0 * 6.221414089202881
Epoch 2080, val loss: 1.4408661127090454
Epoch 2090, training loss: 311.1318054199219 = 0.037447426468133926 + 50.0 * 6.221887111663818
Epoch 2090, val loss: 1.4454658031463623
Epoch 2100, training loss: 311.1739196777344 = 0.03686552867293358 + 50.0 * 6.22274112701416
Epoch 2100, val loss: 1.4497936964035034
Epoch 2110, training loss: 311.29315185546875 = 0.03628943860530853 + 50.0 * 6.225137233734131
Epoch 2110, val loss: 1.4544100761413574
Epoch 2120, training loss: 311.1548767089844 = 0.03570631891489029 + 50.0 * 6.222383499145508
Epoch 2120, val loss: 1.4585826396942139
Epoch 2130, training loss: 311.0654602050781 = 0.03514445200562477 + 50.0 * 6.220606327056885
Epoch 2130, val loss: 1.4630050659179688
Epoch 2140, training loss: 311.0827331542969 = 0.034609511494636536 + 50.0 * 6.2209625244140625
Epoch 2140, val loss: 1.467393159866333
Epoch 2150, training loss: 311.2289733886719 = 0.03408822417259216 + 50.0 * 6.223897933959961
Epoch 2150, val loss: 1.4716280698776245
Epoch 2160, training loss: 311.06536865234375 = 0.033551495522260666 + 50.0 * 6.22063684463501
Epoch 2160, val loss: 1.4766031503677368
Epoch 2170, training loss: 311.0035095214844 = 0.0330418199300766 + 50.0 * 6.219408988952637
Epoch 2170, val loss: 1.4807333946228027
Epoch 2180, training loss: 311.17327880859375 = 0.03256548196077347 + 50.0 * 6.222814083099365
Epoch 2180, val loss: 1.4848449230194092
Epoch 2190, training loss: 311.1049499511719 = 0.03205306455492973 + 50.0 * 6.2214579582214355
Epoch 2190, val loss: 1.4892913103103638
Epoch 2200, training loss: 311.0086669921875 = 0.031559549272060394 + 50.0 * 6.219542503356934
Epoch 2200, val loss: 1.4934113025665283
Epoch 2210, training loss: 311.1397705078125 = 0.031094640493392944 + 50.0 * 6.222173690795898
Epoch 2210, val loss: 1.4980671405792236
Epoch 2220, training loss: 310.9794616699219 = 0.030627569183707237 + 50.0 * 6.2189764976501465
Epoch 2220, val loss: 1.5017187595367432
Epoch 2230, training loss: 311.0232849121094 = 0.030182240530848503 + 50.0 * 6.21986198425293
Epoch 2230, val loss: 1.506144642829895
Epoch 2240, training loss: 311.0285949707031 = 0.029741883277893066 + 50.0 * 6.219976902008057
Epoch 2240, val loss: 1.5104553699493408
Epoch 2250, training loss: 310.9573059082031 = 0.02931133285164833 + 50.0 * 6.218560218811035
Epoch 2250, val loss: 1.5148131847381592
Epoch 2260, training loss: 311.0804138183594 = 0.028898386284708977 + 50.0 * 6.221030235290527
Epoch 2260, val loss: 1.518616795539856
Epoch 2270, training loss: 310.9475402832031 = 0.0284744780510664 + 50.0 * 6.218381404876709
Epoch 2270, val loss: 1.5225763320922852
Epoch 2280, training loss: 311.0455627441406 = 0.028064493089914322 + 50.0 * 6.2203497886657715
Epoch 2280, val loss: 1.526639461517334
Epoch 2290, training loss: 310.94573974609375 = 0.027660900726914406 + 50.0 * 6.218361854553223
Epoch 2290, val loss: 1.5310674905776978
Epoch 2300, training loss: 310.9852600097656 = 0.027274619787931442 + 50.0 * 6.2191596031188965
Epoch 2300, val loss: 1.5352579355239868
Epoch 2310, training loss: 311.0016784667969 = 0.02688789553940296 + 50.0 * 6.21949577331543
Epoch 2310, val loss: 1.5392917394638062
Epoch 2320, training loss: 310.9437561035156 = 0.0265109334141016 + 50.0 * 6.218344688415527
Epoch 2320, val loss: 1.5429531335830688
Epoch 2330, training loss: 310.9058837890625 = 0.026142707094550133 + 50.0 * 6.217594623565674
Epoch 2330, val loss: 1.5468943119049072
Epoch 2340, training loss: 310.8605041503906 = 0.025783758610486984 + 50.0 * 6.216694355010986
Epoch 2340, val loss: 1.5509206056594849
Epoch 2350, training loss: 310.8755187988281 = 0.025439273566007614 + 50.0 * 6.217001438140869
Epoch 2350, val loss: 1.5547868013381958
Epoch 2360, training loss: 311.1438903808594 = 0.025105250999331474 + 50.0 * 6.222375392913818
Epoch 2360, val loss: 1.5585176944732666
Epoch 2370, training loss: 310.9984130859375 = 0.02474871091544628 + 50.0 * 6.219473361968994
Epoch 2370, val loss: 1.562208890914917
Epoch 2380, training loss: 310.93536376953125 = 0.024404283612966537 + 50.0 * 6.218218803405762
Epoch 2380, val loss: 1.56558358669281
Epoch 2390, training loss: 310.8559265136719 = 0.02406945638358593 + 50.0 * 6.216636657714844
Epoch 2390, val loss: 1.5698559284210205
Epoch 2400, training loss: 310.78466796875 = 0.023752486333251 + 50.0 * 6.215218544006348
Epoch 2400, val loss: 1.5740830898284912
Epoch 2410, training loss: 310.79168701171875 = 0.023448744788765907 + 50.0 * 6.215364933013916
Epoch 2410, val loss: 1.5781867504119873
Epoch 2420, training loss: 311.08197021484375 = 0.023155689239501953 + 50.0 * 6.2211761474609375
Epoch 2420, val loss: 1.5824006795883179
Epoch 2430, training loss: 310.95611572265625 = 0.02283361367881298 + 50.0 * 6.218666076660156
Epoch 2430, val loss: 1.584776520729065
Epoch 2440, training loss: 310.9261779785156 = 0.022533584386110306 + 50.0 * 6.21807336807251
Epoch 2440, val loss: 1.5888539552688599
Epoch 2450, training loss: 310.8585510253906 = 0.02223925106227398 + 50.0 * 6.216726303100586
Epoch 2450, val loss: 1.592324137687683
Epoch 2460, training loss: 310.78814697265625 = 0.021955719217658043 + 50.0 * 6.2153239250183105
Epoch 2460, val loss: 1.5962200164794922
Epoch 2470, training loss: 310.7717590332031 = 0.021678315475583076 + 50.0 * 6.215001583099365
Epoch 2470, val loss: 1.599653959274292
Epoch 2480, training loss: 310.7960510253906 = 0.0214083231985569 + 50.0 * 6.2154927253723145
Epoch 2480, val loss: 1.6036014556884766
Epoch 2490, training loss: 310.86572265625 = 0.021142886951565742 + 50.0 * 6.216891765594482
Epoch 2490, val loss: 1.6072289943695068
Epoch 2500, training loss: 310.805908203125 = 0.02087155543267727 + 50.0 * 6.215700626373291
Epoch 2500, val loss: 1.6106081008911133
Epoch 2510, training loss: 310.8826904296875 = 0.020613497123122215 + 50.0 * 6.2172417640686035
Epoch 2510, val loss: 1.6145449876785278
Epoch 2520, training loss: 310.8004150390625 = 0.020349131897091866 + 50.0 * 6.215601444244385
Epoch 2520, val loss: 1.6175134181976318
Epoch 2530, training loss: 310.7774658203125 = 0.020093820989131927 + 50.0 * 6.215147495269775
Epoch 2530, val loss: 1.6215112209320068
Epoch 2540, training loss: 310.86090087890625 = 0.019847461953759193 + 50.0 * 6.216821193695068
Epoch 2540, val loss: 1.6248074769973755
Epoch 2550, training loss: 310.7986145019531 = 0.019609803333878517 + 50.0 * 6.215579986572266
Epoch 2550, val loss: 1.6279819011688232
Epoch 2560, training loss: 310.6756591796875 = 0.019370799884200096 + 50.0 * 6.213125705718994
Epoch 2560, val loss: 1.6317241191864014
Epoch 2570, training loss: 310.66986083984375 = 0.01914241537451744 + 50.0 * 6.213014602661133
Epoch 2570, val loss: 1.6351275444030762
Epoch 2580, training loss: 310.75689697265625 = 0.01892486959695816 + 50.0 * 6.214759826660156
Epoch 2580, val loss: 1.6382462978363037
Epoch 2590, training loss: 310.82366943359375 = 0.018699610605835915 + 50.0 * 6.216099262237549
Epoch 2590, val loss: 1.64139723777771
Epoch 2600, training loss: 310.7453308105469 = 0.018463313579559326 + 50.0 * 6.214537620544434
Epoch 2600, val loss: 1.6453138589859009
Epoch 2610, training loss: 310.8055114746094 = 0.018250687047839165 + 50.0 * 6.215744972229004
Epoch 2610, val loss: 1.6481664180755615
Epoch 2620, training loss: 310.6615295410156 = 0.0180350411683321 + 50.0 * 6.212870121002197
Epoch 2620, val loss: 1.65200936794281
Epoch 2630, training loss: 310.79327392578125 = 0.01783951185643673 + 50.0 * 6.215508460998535
Epoch 2630, val loss: 1.655172348022461
Epoch 2640, training loss: 310.68023681640625 = 0.017621302977204323 + 50.0 * 6.213252544403076
Epoch 2640, val loss: 1.6583393812179565
Epoch 2650, training loss: 310.6410827636719 = 0.017415767535567284 + 50.0 * 6.212473392486572
Epoch 2650, val loss: 1.6615482568740845
Epoch 2660, training loss: 310.6186828613281 = 0.017223529517650604 + 50.0 * 6.212029457092285
Epoch 2660, val loss: 1.6652166843414307
Epoch 2670, training loss: 310.6131591796875 = 0.017029035836458206 + 50.0 * 6.211922645568848
Epoch 2670, val loss: 1.6683692932128906
Epoch 2680, training loss: 310.80950927734375 = 0.01684701442718506 + 50.0 * 6.215853214263916
Epoch 2680, val loss: 1.6712199449539185
Epoch 2690, training loss: 310.7566223144531 = 0.016652844846248627 + 50.0 * 6.214799404144287
Epoch 2690, val loss: 1.6741516590118408
Epoch 2700, training loss: 310.70892333984375 = 0.01646018959581852 + 50.0 * 6.2138495445251465
Epoch 2700, val loss: 1.6769276857376099
Epoch 2710, training loss: 310.6531066894531 = 0.016276687383651733 + 50.0 * 6.212737083435059
Epoch 2710, val loss: 1.680760383605957
Epoch 2720, training loss: 310.6260986328125 = 0.016100382432341576 + 50.0 * 6.212199687957764
Epoch 2720, val loss: 1.683524250984192
Epoch 2730, training loss: 310.6048278808594 = 0.015924513339996338 + 50.0 * 6.211778163909912
Epoch 2730, val loss: 1.6867836713790894
Epoch 2740, training loss: 310.55450439453125 = 0.015752429142594337 + 50.0 * 6.210774898529053
Epoch 2740, val loss: 1.6902738809585571
Epoch 2750, training loss: 310.7434387207031 = 0.015588469803333282 + 50.0 * 6.214556694030762
Epoch 2750, val loss: 1.6932629346847534
Epoch 2760, training loss: 310.62548828125 = 0.015412109903991222 + 50.0 * 6.2122015953063965
Epoch 2760, val loss: 1.6961004734039307
Epoch 2770, training loss: 310.7065734863281 = 0.015248822048306465 + 50.0 * 6.2138261795043945
Epoch 2770, val loss: 1.699755311012268
Epoch 2780, training loss: 310.5722351074219 = 0.015076970681548119 + 50.0 * 6.2111430168151855
Epoch 2780, val loss: 1.7017827033996582
Epoch 2790, training loss: 310.4962463378906 = 0.014917154796421528 + 50.0 * 6.2096266746521
Epoch 2790, val loss: 1.7052056789398193
Epoch 2800, training loss: 310.5155029296875 = 0.014767011627554893 + 50.0 * 6.210014820098877
Epoch 2800, val loss: 1.708442211151123
Epoch 2810, training loss: 310.7086181640625 = 0.014622218906879425 + 50.0 * 6.2138800621032715
Epoch 2810, val loss: 1.7110025882720947
Epoch 2820, training loss: 310.6557922363281 = 0.014464574865996838 + 50.0 * 6.212826251983643
Epoch 2820, val loss: 1.7131954431533813
Epoch 2830, training loss: 310.4990539550781 = 0.014303501695394516 + 50.0 * 6.209694862365723
Epoch 2830, val loss: 1.716689109802246
Epoch 2840, training loss: 310.47113037109375 = 0.014155247248709202 + 50.0 * 6.209139347076416
Epoch 2840, val loss: 1.7195947170257568
Epoch 2850, training loss: 310.4493408203125 = 0.014016414061188698 + 50.0 * 6.208706378936768
Epoch 2850, val loss: 1.722652554512024
Epoch 2860, training loss: 310.6196594238281 = 0.013883278705179691 + 50.0 * 6.21211576461792
Epoch 2860, val loss: 1.725318431854248
Epoch 2870, training loss: 310.50152587890625 = 0.01373863872140646 + 50.0 * 6.209755897521973
Epoch 2870, val loss: 1.7280552387237549
Epoch 2880, training loss: 310.4601135253906 = 0.013594618998467922 + 50.0 * 6.208930969238281
Epoch 2880, val loss: 1.7305291891098022
Epoch 2890, training loss: 310.58697509765625 = 0.013463018462061882 + 50.0 * 6.211470603942871
Epoch 2890, val loss: 1.7333613634109497
Epoch 2900, training loss: 310.42529296875 = 0.013323384337127209 + 50.0 * 6.2082390785217285
Epoch 2900, val loss: 1.7365819215774536
Epoch 2910, training loss: 310.5069274902344 = 0.013195367529988289 + 50.0 * 6.209875106811523
Epoch 2910, val loss: 1.7398583889007568
Epoch 2920, training loss: 310.657470703125 = 0.0130656398832798 + 50.0 * 6.212888240814209
Epoch 2920, val loss: 1.742188811302185
Epoch 2930, training loss: 310.45391845703125 = 0.012936301529407501 + 50.0 * 6.20881986618042
Epoch 2930, val loss: 1.744521141052246
Epoch 2940, training loss: 310.4363708496094 = 0.012812349945306778 + 50.0 * 6.208471298217773
Epoch 2940, val loss: 1.7480624914169312
Epoch 2950, training loss: 310.6622314453125 = 0.012692689895629883 + 50.0 * 6.212990760803223
Epoch 2950, val loss: 1.7507609128952026
Epoch 2960, training loss: 310.4449157714844 = 0.01256641186773777 + 50.0 * 6.208646774291992
Epoch 2960, val loss: 1.7523720264434814
Epoch 2970, training loss: 310.4142761230469 = 0.012446681037545204 + 50.0 * 6.208036422729492
Epoch 2970, val loss: 1.7557017803192139
Epoch 2980, training loss: 310.5557556152344 = 0.012333812192082405 + 50.0 * 6.2108683586120605
Epoch 2980, val loss: 1.7581547498703003
Epoch 2990, training loss: 310.5104064941406 = 0.012214642949402332 + 50.0 * 6.209963798522949
Epoch 2990, val loss: 1.761083960533142
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8128624143384291
=== training gcn model ===
Epoch 0, training loss: 431.7867126464844 = 1.9459582567214966 + 50.0 * 8.59681510925293
Epoch 0, val loss: 1.9435467720031738
Epoch 10, training loss: 431.7311706542969 = 1.9380203485488892 + 50.0 * 8.595863342285156
Epoch 10, val loss: 1.9357688426971436
Epoch 20, training loss: 431.3970947265625 = 1.9278002977371216 + 50.0 * 8.589385986328125
Epoch 20, val loss: 1.9253313541412354
Epoch 30, training loss: 429.0422058105469 = 1.9143660068511963 + 50.0 * 8.542556762695312
Epoch 30, val loss: 1.9115620851516724
Epoch 40, training loss: 412.1481018066406 = 1.898343801498413 + 50.0 * 8.204995155334473
Epoch 40, val loss: 1.8953800201416016
Epoch 50, training loss: 382.508056640625 = 1.879634141921997 + 50.0 * 7.612568378448486
Epoch 50, val loss: 1.877808690071106
Epoch 60, training loss: 364.7400207519531 = 1.8673019409179688 + 50.0 * 7.2574543952941895
Epoch 60, val loss: 1.8668510913848877
Epoch 70, training loss: 353.9344177246094 = 1.8589673042297363 + 50.0 * 7.041508674621582
Epoch 70, val loss: 1.8589212894439697
Epoch 80, training loss: 346.63995361328125 = 1.8493773937225342 + 50.0 * 6.895812034606934
Epoch 80, val loss: 1.8499752283096313
Epoch 90, training loss: 341.2906494140625 = 1.8395366668701172 + 50.0 * 6.789021968841553
Epoch 90, val loss: 1.840983510017395
Epoch 100, training loss: 337.45391845703125 = 1.8304857015609741 + 50.0 * 6.712469100952148
Epoch 100, val loss: 1.832682490348816
Epoch 110, training loss: 334.8581237792969 = 1.821813702583313 + 50.0 * 6.660726547241211
Epoch 110, val loss: 1.8245067596435547
Epoch 120, training loss: 332.9332275390625 = 1.8131954669952393 + 50.0 * 6.622400283813477
Epoch 120, val loss: 1.8163294792175293
Epoch 130, training loss: 331.4477233886719 = 1.804734706878662 + 50.0 * 6.592859268188477
Epoch 130, val loss: 1.8081471920013428
Epoch 140, training loss: 330.1886901855469 = 1.796465277671814 + 50.0 * 6.567844390869141
Epoch 140, val loss: 1.800115704536438
Epoch 150, training loss: 329.2364807128906 = 1.7884405851364136 + 50.0 * 6.5489606857299805
Epoch 150, val loss: 1.7921944856643677
Epoch 160, training loss: 328.2560119628906 = 1.780207872390747 + 50.0 * 6.529516220092773
Epoch 160, val loss: 1.7842317819595337
Epoch 170, training loss: 327.48919677734375 = 1.771697998046875 + 50.0 * 6.514349937438965
Epoch 170, val loss: 1.776166319847107
Epoch 180, training loss: 326.74822998046875 = 1.7627190351486206 + 50.0 * 6.4997100830078125
Epoch 180, val loss: 1.7678909301757812
Epoch 190, training loss: 326.1424560546875 = 1.7531447410583496 + 50.0 * 6.487786293029785
Epoch 190, val loss: 1.7592806816101074
Epoch 200, training loss: 325.4828186035156 = 1.7428009510040283 + 50.0 * 6.4748005867004395
Epoch 200, val loss: 1.7502503395080566
Epoch 210, training loss: 325.0058898925781 = 1.7317354679107666 + 50.0 * 6.465482711791992
Epoch 210, val loss: 1.7407196760177612
Epoch 220, training loss: 324.3756103515625 = 1.7197537422180176 + 50.0 * 6.4531168937683105
Epoch 220, val loss: 1.7304922342300415
Epoch 230, training loss: 323.9081726074219 = 1.7067992687225342 + 50.0 * 6.444027900695801
Epoch 230, val loss: 1.7195215225219727
Epoch 240, training loss: 323.3932189941406 = 1.6928554773330688 + 50.0 * 6.43400764465332
Epoch 240, val loss: 1.7077397108078003
Epoch 250, training loss: 322.9566345214844 = 1.6778886318206787 + 50.0 * 6.425574779510498
Epoch 250, val loss: 1.6951521635055542
Epoch 260, training loss: 322.71051025390625 = 1.6618067026138306 + 50.0 * 6.420974254608154
Epoch 260, val loss: 1.6816922426223755
Epoch 270, training loss: 322.1755065917969 = 1.6446155309677124 + 50.0 * 6.410617828369141
Epoch 270, val loss: 1.6672894954681396
Epoch 280, training loss: 321.83740234375 = 1.6263941526412964 + 50.0 * 6.404220104217529
Epoch 280, val loss: 1.652034878730774
Epoch 290, training loss: 321.4831237792969 = 1.6072564125061035 + 50.0 * 6.397517204284668
Epoch 290, val loss: 1.63608717918396
Epoch 300, training loss: 321.13458251953125 = 1.5872831344604492 + 50.0 * 6.390945911407471
Epoch 300, val loss: 1.6194730997085571
Epoch 310, training loss: 321.22479248046875 = 1.5665313005447388 + 50.0 * 6.393165111541748
Epoch 310, val loss: 1.602234959602356
Epoch 320, training loss: 320.735595703125 = 1.544945240020752 + 50.0 * 6.38381290435791
Epoch 320, val loss: 1.584434986114502
Epoch 330, training loss: 320.2561950683594 = 1.5229190587997437 + 50.0 * 6.374665260314941
Epoch 330, val loss: 1.5663881301879883
Epoch 340, training loss: 320.0111389160156 = 1.5006035566329956 + 50.0 * 6.370210647583008
Epoch 340, val loss: 1.5481808185577393
Epoch 350, training loss: 319.87432861328125 = 1.4778242111206055 + 50.0 * 6.3679304122924805
Epoch 350, val loss: 1.529836893081665
Epoch 360, training loss: 319.53533935546875 = 1.4548765420913696 + 50.0 * 6.36160945892334
Epoch 360, val loss: 1.5113369226455688
Epoch 370, training loss: 319.27337646484375 = 1.4318803548812866 + 50.0 * 6.35683012008667
Epoch 370, val loss: 1.4930232763290405
Epoch 380, training loss: 319.1739196777344 = 1.4088956117630005 + 50.0 * 6.355300426483154
Epoch 380, val loss: 1.474934458732605
Epoch 390, training loss: 319.0646057128906 = 1.3858156204223633 + 50.0 * 6.353576183319092
Epoch 390, val loss: 1.4569528102874756
Epoch 400, training loss: 318.71624755859375 = 1.3627140522003174 + 50.0 * 6.347070693969727
Epoch 400, val loss: 1.4392462968826294
Epoch 410, training loss: 318.4310607910156 = 1.3396387100219727 + 50.0 * 6.341828346252441
Epoch 410, val loss: 1.4217791557312012
Epoch 420, training loss: 318.3840637207031 = 1.3166396617889404 + 50.0 * 6.341348171234131
Epoch 420, val loss: 1.4045451879501343
Epoch 430, training loss: 318.28265380859375 = 1.2934685945510864 + 50.0 * 6.339783668518066
Epoch 430, val loss: 1.3874285221099854
Epoch 440, training loss: 317.951904296875 = 1.2702983617782593 + 50.0 * 6.333632469177246
Epoch 440, val loss: 1.3704560995101929
Epoch 450, training loss: 317.80413818359375 = 1.2472103834152222 + 50.0 * 6.3311381340026855
Epoch 450, val loss: 1.3538154363632202
Epoch 460, training loss: 317.6157531738281 = 1.2240586280822754 + 50.0 * 6.327833652496338
Epoch 460, val loss: 1.3372676372528076
Epoch 470, training loss: 317.5057678222656 = 1.2008662223815918 + 50.0 * 6.3260979652404785
Epoch 470, val loss: 1.320914387702942
Epoch 480, training loss: 317.3218994140625 = 1.177833080291748 + 50.0 * 6.32288122177124
Epoch 480, val loss: 1.3048608303070068
Epoch 490, training loss: 317.3348388671875 = 1.1548969745635986 + 50.0 * 6.323598384857178
Epoch 490, val loss: 1.2889710664749146
Epoch 500, training loss: 317.19818115234375 = 1.13189697265625 + 50.0 * 6.321325778961182
Epoch 500, val loss: 1.2732526063919067
Epoch 510, training loss: 316.9670715332031 = 1.1091026067733765 + 50.0 * 6.317159175872803
Epoch 510, val loss: 1.257771611213684
Epoch 520, training loss: 316.7645263671875 = 1.086716890335083 + 50.0 * 6.313555717468262
Epoch 520, val loss: 1.2427736520767212
Epoch 530, training loss: 316.69024658203125 = 1.0646030902862549 + 50.0 * 6.3125128746032715
Epoch 530, val loss: 1.228268027305603
Epoch 540, training loss: 316.7599792480469 = 1.0426511764526367 + 50.0 * 6.3143463134765625
Epoch 540, val loss: 1.2138464450836182
Epoch 550, training loss: 316.5589904785156 = 1.021057367324829 + 50.0 * 6.310758590698242
Epoch 550, val loss: 1.1998544931411743
Epoch 560, training loss: 316.3018493652344 = 0.999904215335846 + 50.0 * 6.306038856506348
Epoch 560, val loss: 1.1864069700241089
Epoch 570, training loss: 316.2235412597656 = 0.979293942451477 + 50.0 * 6.304884910583496
Epoch 570, val loss: 1.1735831499099731
Epoch 580, training loss: 316.38104248046875 = 0.959079921245575 + 50.0 * 6.308439254760742
Epoch 580, val loss: 1.1611675024032593
Epoch 590, training loss: 316.0997619628906 = 0.939244270324707 + 50.0 * 6.303210258483887
Epoch 590, val loss: 1.149271845817566
Epoch 600, training loss: 315.90228271484375 = 0.9199129939079285 + 50.0 * 6.299647331237793
Epoch 600, val loss: 1.1378684043884277
Epoch 610, training loss: 315.7987976074219 = 0.9011521339416504 + 50.0 * 6.297953128814697
Epoch 610, val loss: 1.127043604850769
Epoch 620, training loss: 315.9572448730469 = 0.8828358054161072 + 50.0 * 6.301487922668457
Epoch 620, val loss: 1.1168434619903564
Epoch 630, training loss: 315.8654479980469 = 0.8648455142974854 + 50.0 * 6.300012111663818
Epoch 630, val loss: 1.1067477464675903
Epoch 640, training loss: 315.489501953125 = 0.847382128238678 + 50.0 * 6.292842388153076
Epoch 640, val loss: 1.0973303318023682
Epoch 650, training loss: 315.65093994140625 = 0.8304473757743835 + 50.0 * 6.296409606933594
Epoch 650, val loss: 1.0883936882019043
Epoch 660, training loss: 315.4221496582031 = 0.8138038516044617 + 50.0 * 6.292166709899902
Epoch 660, val loss: 1.0800182819366455
Epoch 670, training loss: 315.2870788574219 = 0.7977467775344849 + 50.0 * 6.289786338806152
Epoch 670, val loss: 1.0719951391220093
Epoch 680, training loss: 315.3743591308594 = 0.7820481061935425 + 50.0 * 6.29184627532959
Epoch 680, val loss: 1.0646071434020996
Epoch 690, training loss: 315.2430419921875 = 0.7665963172912598 + 50.0 * 6.289528846740723
Epoch 690, val loss: 1.0572433471679688
Epoch 700, training loss: 315.02862548828125 = 0.7516438961029053 + 50.0 * 6.285539627075195
Epoch 700, val loss: 1.0505571365356445
Epoch 710, training loss: 314.98028564453125 = 0.7371328473091125 + 50.0 * 6.284863471984863
Epoch 710, val loss: 1.0442484617233276
Epoch 720, training loss: 314.9721984863281 = 0.7228177785873413 + 50.0 * 6.284987926483154
Epoch 720, val loss: 1.0382046699523926
Epoch 730, training loss: 314.77880859375 = 0.7089055180549622 + 50.0 * 6.281398296356201
Epoch 730, val loss: 1.0325701236724854
Epoch 740, training loss: 314.76214599609375 = 0.6953341364860535 + 50.0 * 6.281336307525635
Epoch 740, val loss: 1.027411699295044
Epoch 750, training loss: 314.776123046875 = 0.6820847392082214 + 50.0 * 6.2818803787231445
Epoch 750, val loss: 1.0223520994186401
Epoch 760, training loss: 314.6254577636719 = 0.6690399646759033 + 50.0 * 6.279128551483154
Epoch 760, val loss: 1.017736792564392
Epoch 770, training loss: 314.58612060546875 = 0.6563947796821594 + 50.0 * 6.278594493865967
Epoch 770, val loss: 1.013473391532898
Epoch 780, training loss: 314.5832214355469 = 0.6439309120178223 + 50.0 * 6.2787861824035645
Epoch 780, val loss: 1.009458065032959
Epoch 790, training loss: 314.4335021972656 = 0.631655216217041 + 50.0 * 6.276037216186523
Epoch 790, val loss: 1.005281686782837
Epoch 800, training loss: 314.286376953125 = 0.6196914911270142 + 50.0 * 6.273333549499512
Epoch 800, val loss: 1.0017297267913818
Epoch 810, training loss: 314.23382568359375 = 0.608045220375061 + 50.0 * 6.272515773773193
Epoch 810, val loss: 0.9985244274139404
Epoch 820, training loss: 314.56292724609375 = 0.5966019034385681 + 50.0 * 6.279326915740967
Epoch 820, val loss: 0.9954696297645569
Epoch 830, training loss: 314.34246826171875 = 0.5852112770080566 + 50.0 * 6.275145053863525
Epoch 830, val loss: 0.9923734068870544
Epoch 840, training loss: 314.048828125 = 0.574051558971405 + 50.0 * 6.269495010375977
Epoch 840, val loss: 0.9896687865257263
Epoch 850, training loss: 314.5242004394531 = 0.5631387829780579 + 50.0 * 6.279221057891846
Epoch 850, val loss: 0.987058162689209
Epoch 860, training loss: 314.1422424316406 = 0.5523949861526489 + 50.0 * 6.271797180175781
Epoch 860, val loss: 0.9849852323532104
Epoch 870, training loss: 313.9034729003906 = 0.5417948365211487 + 50.0 * 6.267233371734619
Epoch 870, val loss: 0.9829651713371277
Epoch 880, training loss: 313.8454895019531 = 0.5314382910728455 + 50.0 * 6.2662811279296875
Epoch 880, val loss: 0.9812303185462952
Epoch 890, training loss: 314.0675964355469 = 0.5212883353233337 + 50.0 * 6.270925998687744
Epoch 890, val loss: 0.9798445105552673
Epoch 900, training loss: 313.8785400390625 = 0.5110827684402466 + 50.0 * 6.2673492431640625
Epoch 900, val loss: 0.9780417084693909
Epoch 910, training loss: 313.8290100097656 = 0.5010865926742554 + 50.0 * 6.2665581703186035
Epoch 910, val loss: 0.9768955707550049
Epoch 920, training loss: 313.739501953125 = 0.4912029206752777 + 50.0 * 6.264965534210205
Epoch 920, val loss: 0.9757428169250488
Epoch 930, training loss: 313.6729736328125 = 0.4814609885215759 + 50.0 * 6.263830184936523
Epoch 930, val loss: 0.9749465584754944
Epoch 940, training loss: 313.77471923828125 = 0.4718460142612457 + 50.0 * 6.26605749130249
Epoch 940, val loss: 0.9742803573608398
Epoch 950, training loss: 313.5291748046875 = 0.46226590871810913 + 50.0 * 6.261337757110596
Epoch 950, val loss: 0.9737422466278076
Epoch 960, training loss: 313.4433898925781 = 0.4528730809688568 + 50.0 * 6.259810447692871
Epoch 960, val loss: 0.973548948764801
Epoch 970, training loss: 313.5747375488281 = 0.44360771775245667 + 50.0 * 6.262622833251953
Epoch 970, val loss: 0.9734569191932678
Epoch 980, training loss: 313.47698974609375 = 0.4343292713165283 + 50.0 * 6.260853290557861
Epoch 980, val loss: 0.9734830260276794
Epoch 990, training loss: 313.57989501953125 = 0.4251466989517212 + 50.0 * 6.263095378875732
Epoch 990, val loss: 0.9734456539154053
Epoch 1000, training loss: 313.3802185058594 = 0.41599392890930176 + 50.0 * 6.259284973144531
Epoch 1000, val loss: 0.9738794565200806
Epoch 1010, training loss: 313.3006591796875 = 0.4071207046508789 + 50.0 * 6.257870197296143
Epoch 1010, val loss: 0.9744990468025208
Epoch 1020, training loss: 313.5097351074219 = 0.39825645089149475 + 50.0 * 6.2622294425964355
Epoch 1020, val loss: 0.9751088619232178
Epoch 1030, training loss: 313.2169494628906 = 0.3894849717617035 + 50.0 * 6.256548881530762
Epoch 1030, val loss: 0.976046085357666
Epoch 1040, training loss: 313.1284484863281 = 0.38087475299835205 + 50.0 * 6.254951000213623
Epoch 1040, val loss: 0.9770880937576294
Epoch 1050, training loss: 313.1329345703125 = 0.3723791837692261 + 50.0 * 6.255211353302002
Epoch 1050, val loss: 0.9783660769462585
Epoch 1060, training loss: 313.3866271972656 = 0.3639996647834778 + 50.0 * 6.2604522705078125
Epoch 1060, val loss: 0.9795687198638916
Epoch 1070, training loss: 313.075927734375 = 0.3555142879486084 + 50.0 * 6.254408836364746
Epoch 1070, val loss: 0.9807400107383728
Epoch 1080, training loss: 313.1077880859375 = 0.3473004102706909 + 50.0 * 6.255209922790527
Epoch 1080, val loss: 0.9822628498077393
Epoch 1090, training loss: 313.0437316894531 = 0.339269757270813 + 50.0 * 6.25408935546875
Epoch 1090, val loss: 0.9841014742851257
Epoch 1100, training loss: 312.94232177734375 = 0.3312652111053467 + 50.0 * 6.25222110748291
Epoch 1100, val loss: 0.9858103394508362
Epoch 1110, training loss: 312.89825439453125 = 0.3234446048736572 + 50.0 * 6.251495838165283
Epoch 1110, val loss: 0.9877914786338806
Epoch 1120, training loss: 313.09326171875 = 0.3158312439918518 + 50.0 * 6.25554895401001
Epoch 1120, val loss: 0.990063488483429
Epoch 1130, training loss: 312.9234313964844 = 0.30816373229026794 + 50.0 * 6.252305030822754
Epoch 1130, val loss: 0.9921348094940186
Epoch 1140, training loss: 312.7724609375 = 0.3007456660270691 + 50.0 * 6.249434471130371
Epoch 1140, val loss: 0.9945569038391113
Epoch 1150, training loss: 312.8599853515625 = 0.29347890615463257 + 50.0 * 6.2513298988342285
Epoch 1150, val loss: 0.9971277117729187
Epoch 1160, training loss: 312.7175598144531 = 0.2862977087497711 + 50.0 * 6.248625755310059
Epoch 1160, val loss: 0.9996702075004578
Epoch 1170, training loss: 312.7681579589844 = 0.2792772650718689 + 50.0 * 6.249777793884277
Epoch 1170, val loss: 1.0023117065429688
Epoch 1180, training loss: 312.7455749511719 = 0.27241790294647217 + 50.0 * 6.249463081359863
Epoch 1180, val loss: 1.005172610282898
Epoch 1190, training loss: 312.7514343261719 = 0.26566198468208313 + 50.0 * 6.249715805053711
Epoch 1190, val loss: 1.0079443454742432
Epoch 1200, training loss: 312.7066345214844 = 0.25909557938575745 + 50.0 * 6.248950958251953
Epoch 1200, val loss: 1.0109466314315796
Epoch 1210, training loss: 312.6050109863281 = 0.25264760851860046 + 50.0 * 6.247046947479248
Epoch 1210, val loss: 1.0140424966812134
Epoch 1220, training loss: 312.56781005859375 = 0.24635732173919678 + 50.0 * 6.246428966522217
Epoch 1220, val loss: 1.0172481536865234
Epoch 1230, training loss: 312.5791931152344 = 0.24024473130702972 + 50.0 * 6.246778964996338
Epoch 1230, val loss: 1.0205438137054443
Epoch 1240, training loss: 312.5738525390625 = 0.23424698412418365 + 50.0 * 6.246791839599609
Epoch 1240, val loss: 1.0238491296768188
Epoch 1250, training loss: 312.6049499511719 = 0.22837160527706146 + 50.0 * 6.247531890869141
Epoch 1250, val loss: 1.0272506475448608
Epoch 1260, training loss: 312.4518737792969 = 0.22259894013404846 + 50.0 * 6.2445855140686035
Epoch 1260, val loss: 1.030651330947876
Epoch 1270, training loss: 312.3648681640625 = 0.21706144511699677 + 50.0 * 6.242955684661865
Epoch 1270, val loss: 1.0344730615615845
Epoch 1280, training loss: 312.4344177246094 = 0.21167080104351044 + 50.0 * 6.244454860687256
Epoch 1280, val loss: 1.0383119583129883
Epoch 1290, training loss: 312.61187744140625 = 0.2063494622707367 + 50.0 * 6.248110294342041
Epoch 1290, val loss: 1.041922688484192
Epoch 1300, training loss: 312.399658203125 = 0.20112858712673187 + 50.0 * 6.2439703941345215
Epoch 1300, val loss: 1.045597791671753
Epoch 1310, training loss: 312.2900390625 = 0.19610871374607086 + 50.0 * 6.241878509521484
Epoch 1310, val loss: 1.0495857000350952
Epoch 1320, training loss: 312.4393310546875 = 0.1912107616662979 + 50.0 * 6.244962215423584
Epoch 1320, val loss: 1.053674578666687
Epoch 1330, training loss: 312.3301696777344 = 0.1864038109779358 + 50.0 * 6.242875576019287
Epoch 1330, val loss: 1.0577119588851929
Epoch 1340, training loss: 312.3833312988281 = 0.1816917061805725 + 50.0 * 6.244032382965088
Epoch 1340, val loss: 1.061684489250183
Epoch 1350, training loss: 312.170166015625 = 0.17713744938373566 + 50.0 * 6.239860534667969
Epoch 1350, val loss: 1.0660223960876465
Epoch 1360, training loss: 312.16455078125 = 0.17272242903709412 + 50.0 * 6.239836692810059
Epoch 1360, val loss: 1.0702183246612549
Epoch 1370, training loss: 312.25823974609375 = 0.16843818128108978 + 50.0 * 6.241796493530273
Epoch 1370, val loss: 1.074692726135254
Epoch 1380, training loss: 312.3072814941406 = 0.16419968008995056 + 50.0 * 6.242861747741699
Epoch 1380, val loss: 1.078871726989746
Epoch 1390, training loss: 312.1023864746094 = 0.16003552079200745 + 50.0 * 6.238846778869629
Epoch 1390, val loss: 1.0830347537994385
Epoch 1400, training loss: 312.0455322265625 = 0.15606942772865295 + 50.0 * 6.237789154052734
Epoch 1400, val loss: 1.087674617767334
Epoch 1410, training loss: 312.0111389160156 = 0.1522413045167923 + 50.0 * 6.237177848815918
Epoch 1410, val loss: 1.0923056602478027
Epoch 1420, training loss: 312.32220458984375 = 0.14850984513759613 + 50.0 * 6.243474006652832
Epoch 1420, val loss: 1.0967639684677124
Epoch 1430, training loss: 312.1735534667969 = 0.14476735889911652 + 50.0 * 6.240575313568115
Epoch 1430, val loss: 1.1011875867843628
Epoch 1440, training loss: 312.0350036621094 = 0.14117641746997833 + 50.0 * 6.2378764152526855
Epoch 1440, val loss: 1.10566246509552
Epoch 1450, training loss: 311.9244079589844 = 0.13773080706596375 + 50.0 * 6.235733509063721
Epoch 1450, val loss: 1.110429286956787
Epoch 1460, training loss: 312.2420959472656 = 0.13441288471221924 + 50.0 * 6.242154121398926
Epoch 1460, val loss: 1.1152087450027466
Epoch 1470, training loss: 311.9664611816406 = 0.13104765117168427 + 50.0 * 6.236708164215088
Epoch 1470, val loss: 1.1195310354232788
Epoch 1480, training loss: 311.8855285644531 = 0.12786120176315308 + 50.0 * 6.2351531982421875
Epoch 1480, val loss: 1.1243958473205566
Epoch 1490, training loss: 311.8699035644531 = 0.12476129084825516 + 50.0 * 6.234902858734131
Epoch 1490, val loss: 1.129243016242981
Epoch 1500, training loss: 312.07257080078125 = 0.12177055329084396 + 50.0 * 6.239016056060791
Epoch 1500, val loss: 1.1340595483779907
Epoch 1510, training loss: 311.8673095703125 = 0.11876256763935089 + 50.0 * 6.234970569610596
Epoch 1510, val loss: 1.1387715339660645
Epoch 1520, training loss: 312.00408935546875 = 0.11591868102550507 + 50.0 * 6.237763404846191
Epoch 1520, val loss: 1.1436916589736938
Epoch 1530, training loss: 311.74658203125 = 0.11308325082063675 + 50.0 * 6.232669830322266
Epoch 1530, val loss: 1.148249626159668
Epoch 1540, training loss: 311.7420349121094 = 0.11036257445812225 + 50.0 * 6.232633590698242
Epoch 1540, val loss: 1.1530652046203613
Epoch 1550, training loss: 311.8548583984375 = 0.10774563997983932 + 50.0 * 6.2349419593811035
Epoch 1550, val loss: 1.1579806804656982
Epoch 1560, training loss: 311.9008483886719 = 0.10512230545282364 + 50.0 * 6.23591423034668
Epoch 1560, val loss: 1.162420630455017
Epoch 1570, training loss: 311.7810974121094 = 0.10259298235177994 + 50.0 * 6.233570098876953
Epoch 1570, val loss: 1.1672580242156982
Epoch 1580, training loss: 311.68511962890625 = 0.1001463383436203 + 50.0 * 6.231698989868164
Epoch 1580, val loss: 1.172109842300415
Epoch 1590, training loss: 311.6369323730469 = 0.0978054329752922 + 50.0 * 6.230782508850098
Epoch 1590, val loss: 1.1772139072418213
Epoch 1600, training loss: 312.1005554199219 = 0.09551700949668884 + 50.0 * 6.240100860595703
Epoch 1600, val loss: 1.182045578956604
Epoch 1610, training loss: 311.9247131347656 = 0.09322577714920044 + 50.0 * 6.236629486083984
Epoch 1610, val loss: 1.1865367889404297
Epoch 1620, training loss: 311.6730041503906 = 0.09100370854139328 + 50.0 * 6.231639862060547
Epoch 1620, val loss: 1.1914135217666626
Epoch 1630, training loss: 311.5764465332031 = 0.08889301866292953 + 50.0 * 6.229751110076904
Epoch 1630, val loss: 1.196290373802185
Epoch 1640, training loss: 311.62957763671875 = 0.08684206753969193 + 50.0 * 6.230854511260986
Epoch 1640, val loss: 1.20114004611969
Epoch 1650, training loss: 311.7008056640625 = 0.08483584970235825 + 50.0 * 6.232319355010986
Epoch 1650, val loss: 1.2060590982437134
Epoch 1660, training loss: 311.9076232910156 = 0.08287515491247177 + 50.0 * 6.236494541168213
Epoch 1660, val loss: 1.2109663486480713
Epoch 1670, training loss: 311.69342041015625 = 0.08091369271278381 + 50.0 * 6.232250213623047
Epoch 1670, val loss: 1.2151849269866943
Epoch 1680, training loss: 311.5841369628906 = 0.07906389236450195 + 50.0 * 6.230101585388184
Epoch 1680, val loss: 1.2203290462493896
Epoch 1690, training loss: 311.789306640625 = 0.07727362960577011 + 50.0 * 6.234240531921387
Epoch 1690, val loss: 1.225113868713379
Epoch 1700, training loss: 311.50927734375 = 0.07549082487821579 + 50.0 * 6.228675842285156
Epoch 1700, val loss: 1.2297751903533936
Epoch 1710, training loss: 311.4554138183594 = 0.07379938662052155 + 50.0 * 6.227632522583008
Epoch 1710, val loss: 1.2347420454025269
Epoch 1720, training loss: 311.5517883300781 = 0.07215502113103867 + 50.0 * 6.229592800140381
Epoch 1720, val loss: 1.2395782470703125
Epoch 1730, training loss: 311.5157775878906 = 0.0705350935459137 + 50.0 * 6.228905200958252
Epoch 1730, val loss: 1.2444177865982056
Epoch 1740, training loss: 311.4976501464844 = 0.06898336112499237 + 50.0 * 6.228573322296143
Epoch 1740, val loss: 1.249505877494812
Epoch 1750, training loss: 311.5359191894531 = 0.06743549555540085 + 50.0 * 6.229369640350342
Epoch 1750, val loss: 1.2541218996047974
Epoch 1760, training loss: 311.3707580566406 = 0.0659286230802536 + 50.0 * 6.2260966300964355
Epoch 1760, val loss: 1.2589263916015625
Epoch 1770, training loss: 311.39935302734375 = 0.06451239436864853 + 50.0 * 6.226696491241455
Epoch 1770, val loss: 1.264112114906311
Epoch 1780, training loss: 311.6276550292969 = 0.06311207264661789 + 50.0 * 6.231290817260742
Epoch 1780, val loss: 1.268831491470337
Epoch 1790, training loss: 311.3484191894531 = 0.06171615421772003 + 50.0 * 6.225734233856201
Epoch 1790, val loss: 1.2734516859054565
Epoch 1800, training loss: 311.2950439453125 = 0.06038343533873558 + 50.0 * 6.2246928215026855
Epoch 1800, val loss: 1.2782931327819824
Epoch 1810, training loss: 311.37078857421875 = 0.05910484492778778 + 50.0 * 6.22623348236084
Epoch 1810, val loss: 1.2831591367721558
Epoch 1820, training loss: 311.7161560058594 = 0.05784266069531441 + 50.0 * 6.233166217803955
Epoch 1820, val loss: 1.2878397703170776
Epoch 1830, training loss: 311.3375549316406 = 0.05655042082071304 + 50.0 * 6.225620269775391
Epoch 1830, val loss: 1.29231595993042
Epoch 1840, training loss: 311.2868347167969 = 0.055345114320516586 + 50.0 * 6.2246294021606445
Epoch 1840, val loss: 1.297245979309082
Epoch 1850, training loss: 311.241455078125 = 0.05420016124844551 + 50.0 * 6.223745346069336
Epoch 1850, val loss: 1.3022252321243286
Epoch 1860, training loss: 311.3067932128906 = 0.053089022636413574 + 50.0 * 6.22507381439209
Epoch 1860, val loss: 1.3072484731674194
Epoch 1870, training loss: 311.42181396484375 = 0.05197988450527191 + 50.0 * 6.2273969650268555
Epoch 1870, val loss: 1.3117942810058594
Epoch 1880, training loss: 311.2206726074219 = 0.05087592452764511 + 50.0 * 6.223396301269531
Epoch 1880, val loss: 1.316273808479309
Epoch 1890, training loss: 311.2129211425781 = 0.049838587641716 + 50.0 * 6.223261833190918
Epoch 1890, val loss: 1.3211148977279663
Epoch 1900, training loss: 311.4444580078125 = 0.04883389174938202 + 50.0 * 6.227912425994873
Epoch 1900, val loss: 1.3259292840957642
Epoch 1910, training loss: 311.3105773925781 = 0.047841548919677734 + 50.0 * 6.225254535675049
Epoch 1910, val loss: 1.3309986591339111
Epoch 1920, training loss: 311.1973571777344 = 0.046851616352796555 + 50.0 * 6.223010063171387
Epoch 1920, val loss: 1.3351893424987793
Epoch 1930, training loss: 311.1434631347656 = 0.045924946665763855 + 50.0 * 6.221950531005859
Epoch 1930, val loss: 1.340294599533081
Epoch 1940, training loss: 311.2770690917969 = 0.0450340136885643 + 50.0 * 6.224640369415283
Epoch 1940, val loss: 1.345023274421692
Epoch 1950, training loss: 311.1684265136719 = 0.044119033962488174 + 50.0 * 6.2224860191345215
Epoch 1950, val loss: 1.3492828607559204
Epoch 1960, training loss: 311.1979675292969 = 0.04323848336935043 + 50.0 * 6.223094463348389
Epoch 1960, val loss: 1.3537150621414185
Epoch 1970, training loss: 311.3340148925781 = 0.04239969328045845 + 50.0 * 6.225832462310791
Epoch 1970, val loss: 1.3584234714508057
Epoch 1980, training loss: 311.1308288574219 = 0.041571952402591705 + 50.0 * 6.221785545349121
Epoch 1980, val loss: 1.3630752563476562
Epoch 1990, training loss: 311.1166076660156 = 0.04077114537358284 + 50.0 * 6.2215166091918945
Epoch 1990, val loss: 1.3677000999450684
Epoch 2000, training loss: 311.1502685546875 = 0.039995282888412476 + 50.0 * 6.22220516204834
Epoch 2000, val loss: 1.3722953796386719
Epoch 2010, training loss: 311.1060791015625 = 0.03923860192298889 + 50.0 * 6.221336841583252
Epoch 2010, val loss: 1.3766984939575195
Epoch 2020, training loss: 311.2447204589844 = 0.03850284963846207 + 50.0 * 6.224124431610107
Epoch 2020, val loss: 1.3811482191085815
Epoch 2030, training loss: 311.2485656738281 = 0.037777818739414215 + 50.0 * 6.224215984344482
Epoch 2030, val loss: 1.3857944011688232
Epoch 2040, training loss: 311.03802490234375 = 0.03706234693527222 + 50.0 * 6.220019817352295
Epoch 2040, val loss: 1.389875054359436
Epoch 2050, training loss: 311.0013427734375 = 0.03638767451047897 + 50.0 * 6.21929931640625
Epoch 2050, val loss: 1.3948781490325928
Epoch 2060, training loss: 311.1612548828125 = 0.035725660622119904 + 50.0 * 6.22251033782959
Epoch 2060, val loss: 1.3990262746810913
Epoch 2070, training loss: 311.0332946777344 = 0.03506635129451752 + 50.0 * 6.219964504241943
Epoch 2070, val loss: 1.4034605026245117
Epoch 2080, training loss: 311.0261535644531 = 0.034433215856552124 + 50.0 * 6.219834804534912
Epoch 2080, val loss: 1.4080148935317993
Epoch 2090, training loss: 310.977783203125 = 0.03380471467971802 + 50.0 * 6.218879699707031
Epoch 2090, val loss: 1.412320852279663
Epoch 2100, training loss: 311.08697509765625 = 0.033219460397958755 + 50.0 * 6.221075057983398
Epoch 2100, val loss: 1.4170377254486084
Epoch 2110, training loss: 311.06793212890625 = 0.0326186902821064 + 50.0 * 6.220706462860107
Epoch 2110, val loss: 1.4212825298309326
Epoch 2120, training loss: 311.0502624511719 = 0.03203326836228371 + 50.0 * 6.220364093780518
Epoch 2120, val loss: 1.425531029701233
Epoch 2130, training loss: 311.1708984375 = 0.03146805241703987 + 50.0 * 6.2227888107299805
Epoch 2130, val loss: 1.429680347442627
Epoch 2140, training loss: 310.9613037109375 = 0.030918706208467484 + 50.0 * 6.2186079025268555
Epoch 2140, val loss: 1.4340925216674805
Epoch 2150, training loss: 310.89288330078125 = 0.03038714826107025 + 50.0 * 6.217249870300293
Epoch 2150, val loss: 1.4385151863098145
Epoch 2160, training loss: 311.09564208984375 = 0.029872188344597816 + 50.0 * 6.221315383911133
Epoch 2160, val loss: 1.4428210258483887
Epoch 2170, training loss: 310.8963928222656 = 0.029353272169828415 + 50.0 * 6.217340469360352
Epoch 2170, val loss: 1.4467462301254272
Epoch 2180, training loss: 311.0361633300781 = 0.028856186196208 + 50.0 * 6.2201457023620605
Epoch 2180, val loss: 1.4508740901947021
Epoch 2190, training loss: 310.8660583496094 = 0.0283669363707304 + 50.0 * 6.2167534828186035
Epoch 2190, val loss: 1.455330491065979
Epoch 2200, training loss: 310.94134521484375 = 0.027900399640202522 + 50.0 * 6.218269348144531
Epoch 2200, val loss: 1.4594106674194336
Epoch 2210, training loss: 310.9693298339844 = 0.027441024780273438 + 50.0 * 6.218837738037109
Epoch 2210, val loss: 1.4636940956115723
Epoch 2220, training loss: 310.87359619140625 = 0.026984814554452896 + 50.0 * 6.21693229675293
Epoch 2220, val loss: 1.4673253297805786
Epoch 2230, training loss: 310.9393005371094 = 0.026555011048913002 + 50.0 * 6.218255043029785
Epoch 2230, val loss: 1.4719372987747192
Epoch 2240, training loss: 310.8076171875 = 0.02611522562801838 + 50.0 * 6.215629577636719
Epoch 2240, val loss: 1.4758076667785645
Epoch 2250, training loss: 310.9047546386719 = 0.02570110373198986 + 50.0 * 6.217581272125244
Epoch 2250, val loss: 1.4799085855484009
Epoch 2260, training loss: 310.951416015625 = 0.025293072685599327 + 50.0 * 6.218522548675537
Epoch 2260, val loss: 1.483852505683899
Epoch 2270, training loss: 310.8771057128906 = 0.024876000359654427 + 50.0 * 6.217044830322266
Epoch 2270, val loss: 1.4874523878097534
Epoch 2280, training loss: 310.7591552734375 = 0.024485768750309944 + 50.0 * 6.214693546295166
Epoch 2280, val loss: 1.4916081428527832
Epoch 2290, training loss: 310.7852478027344 = 0.024110354483127594 + 50.0 * 6.2152228355407715
Epoch 2290, val loss: 1.4955012798309326
Epoch 2300, training loss: 311.0630798339844 = 0.023740513250231743 + 50.0 * 6.2207865715026855
Epoch 2300, val loss: 1.4992445707321167
Epoch 2310, training loss: 310.8141174316406 = 0.02336595021188259 + 50.0 * 6.215814590454102
Epoch 2310, val loss: 1.5032833814620972
Epoch 2320, training loss: 310.7315673828125 = 0.023006629198789597 + 50.0 * 6.214171409606934
Epoch 2320, val loss: 1.5070768594741821
Epoch 2330, training loss: 310.9156494140625 = 0.022666186094284058 + 50.0 * 6.217859268188477
Epoch 2330, val loss: 1.5110433101654053
Epoch 2340, training loss: 310.7013244628906 = 0.022323310375213623 + 50.0 * 6.213580131530762
Epoch 2340, val loss: 1.5148539543151855
Epoch 2350, training loss: 310.7264099121094 = 0.02199441008269787 + 50.0 * 6.214087963104248
Epoch 2350, val loss: 1.5186618566513062
Epoch 2360, training loss: 310.9761962890625 = 0.02167893946170807 + 50.0 * 6.219090461730957
Epoch 2360, val loss: 1.52248215675354
Epoch 2370, training loss: 310.7666320800781 = 0.02133251167833805 + 50.0 * 6.214905738830566
Epoch 2370, val loss: 1.5253201723098755
Epoch 2380, training loss: 310.8536376953125 = 0.02102847211062908 + 50.0 * 6.2166523933410645
Epoch 2380, val loss: 1.5297491550445557
Epoch 2390, training loss: 310.6881103515625 = 0.020712081342935562 + 50.0 * 6.213347911834717
Epoch 2390, val loss: 1.5327796936035156
Epoch 2400, training loss: 310.6995544433594 = 0.020421398803591728 + 50.0 * 6.213582992553711
Epoch 2400, val loss: 1.5369150638580322
Epoch 2410, training loss: 310.7058410644531 = 0.0201300997287035 + 50.0 * 6.213714122772217
Epoch 2410, val loss: 1.5403820276260376
Epoch 2420, training loss: 310.7162170410156 = 0.01984640397131443 + 50.0 * 6.213927745819092
Epoch 2420, val loss: 1.5439680814743042
Epoch 2430, training loss: 310.6779479980469 = 0.019562922418117523 + 50.0 * 6.213167667388916
Epoch 2430, val loss: 1.5473811626434326
Epoch 2440, training loss: 310.76971435546875 = 0.019297845661640167 + 50.0 * 6.21500825881958
Epoch 2440, val loss: 1.5512702465057373
Epoch 2450, training loss: 310.6990966796875 = 0.019017646089196205 + 50.0 * 6.213601589202881
Epoch 2450, val loss: 1.554426670074463
Epoch 2460, training loss: 310.6130676269531 = 0.018751051276922226 + 50.0 * 6.211886405944824
Epoch 2460, val loss: 1.557909607887268
Epoch 2470, training loss: 310.6635437011719 = 0.018501944839954376 + 50.0 * 6.2129011154174805
Epoch 2470, val loss: 1.5618239641189575
Epoch 2480, training loss: 310.6895751953125 = 0.01824786327779293 + 50.0 * 6.21342658996582
Epoch 2480, val loss: 1.565123438835144
Epoch 2490, training loss: 310.6177978515625 = 0.01799011416733265 + 50.0 * 6.211995601654053
Epoch 2490, val loss: 1.5682092905044556
Epoch 2500, training loss: 310.79205322265625 = 0.017750004306435585 + 50.0 * 6.2154860496521
Epoch 2500, val loss: 1.5715631246566772
Epoch 2510, training loss: 310.64544677734375 = 0.017507025972008705 + 50.0 * 6.212558746337891
Epoch 2510, val loss: 1.5747196674346924
Epoch 2520, training loss: 310.60321044921875 = 0.017271848395466805 + 50.0 * 6.211719036102295
Epoch 2520, val loss: 1.5781974792480469
Epoch 2530, training loss: 310.61480712890625 = 0.01704699546098709 + 50.0 * 6.2119550704956055
Epoch 2530, val loss: 1.5814785957336426
Epoch 2540, training loss: 310.7009582519531 = 0.01683039963245392 + 50.0 * 6.213683128356934
Epoch 2540, val loss: 1.585141897201538
Epoch 2550, training loss: 310.54522705078125 = 0.016603466123342514 + 50.0 * 6.210572242736816
Epoch 2550, val loss: 1.5882729291915894
Epoch 2560, training loss: 310.54046630859375 = 0.016386494040489197 + 50.0 * 6.210481643676758
Epoch 2560, val loss: 1.5914396047592163
Epoch 2570, training loss: 310.6392822265625 = 0.01618463359773159 + 50.0 * 6.212461948394775
Epoch 2570, val loss: 1.5947225093841553
Epoch 2580, training loss: 310.519775390625 = 0.015972169116139412 + 50.0 * 6.210076332092285
Epoch 2580, val loss: 1.5977630615234375
Epoch 2590, training loss: 310.6459045410156 = 0.015771279111504555 + 50.0 * 6.212602615356445
Epoch 2590, val loss: 1.6009260416030884
Epoch 2600, training loss: 310.5643005371094 = 0.015570295043289661 + 50.0 * 6.21097469329834
Epoch 2600, val loss: 1.60402250289917
Epoch 2610, training loss: 310.5781555175781 = 0.015370753593742847 + 50.0 * 6.2112555503845215
Epoch 2610, val loss: 1.6071569919586182
Epoch 2620, training loss: 310.486328125 = 0.015184312127530575 + 50.0 * 6.209422588348389
Epoch 2620, val loss: 1.6106857061386108
Epoch 2630, training loss: 310.4862976074219 = 0.01500058826059103 + 50.0 * 6.209425449371338
Epoch 2630, val loss: 1.6138105392456055
Epoch 2640, training loss: 310.6488037109375 = 0.014819913543760777 + 50.0 * 6.212679386138916
Epoch 2640, val loss: 1.6169872283935547
Epoch 2650, training loss: 310.52947998046875 = 0.014627925120294094 + 50.0 * 6.210297107696533
Epoch 2650, val loss: 1.6194430589675903
Epoch 2660, training loss: 310.47125244140625 = 0.014453844167292118 + 50.0 * 6.20913553237915
Epoch 2660, val loss: 1.622634768486023
Epoch 2670, training loss: 310.4738464355469 = 0.014283639378845692 + 50.0 * 6.20919132232666
Epoch 2670, val loss: 1.6258214712142944
Epoch 2680, training loss: 310.6033935546875 = 0.01411416381597519 + 50.0 * 6.211785793304443
Epoch 2680, val loss: 1.6285303831100464
Epoch 2690, training loss: 310.543212890625 = 0.013940835371613503 + 50.0 * 6.210585594177246
Epoch 2690, val loss: 1.6310111284255981
Epoch 2700, training loss: 310.4538879394531 = 0.01377410814166069 + 50.0 * 6.208802700042725
Epoch 2700, val loss: 1.6340476274490356
Epoch 2710, training loss: 310.42303466796875 = 0.013616849668323994 + 50.0 * 6.208188533782959
Epoch 2710, val loss: 1.6371582746505737
Epoch 2720, training loss: 310.485107421875 = 0.013457848690450191 + 50.0 * 6.209433078765869
Epoch 2720, val loss: 1.6397732496261597
Epoch 2730, training loss: 310.4510498046875 = 0.013302529230713844 + 50.0 * 6.208755016326904
Epoch 2730, val loss: 1.6426795721054077
Epoch 2740, training loss: 310.44573974609375 = 0.01314990222454071 + 50.0 * 6.208651542663574
Epoch 2740, val loss: 1.6456979513168335
Epoch 2750, training loss: 310.45245361328125 = 0.013002206571400166 + 50.0 * 6.208789348602295
Epoch 2750, val loss: 1.6485581398010254
Epoch 2760, training loss: 310.4338073730469 = 0.012852678075432777 + 50.0 * 6.208419322967529
Epoch 2760, val loss: 1.651081919670105
Epoch 2770, training loss: 310.520751953125 = 0.012710020877420902 + 50.0 * 6.210161209106445
Epoch 2770, val loss: 1.6537104845046997
Epoch 2780, training loss: 310.3646240234375 = 0.01256093755364418 + 50.0 * 6.207041263580322
Epoch 2780, val loss: 1.6560626029968262
Epoch 2790, training loss: 310.3619689941406 = 0.012421534396708012 + 50.0 * 6.206990718841553
Epoch 2790, val loss: 1.6587973833084106
Epoch 2800, training loss: 310.3937683105469 = 0.01228985097259283 + 50.0 * 6.207629680633545
Epoch 2800, val loss: 1.6617653369903564
Epoch 2810, training loss: 310.4748229980469 = 0.012152750976383686 + 50.0 * 6.209253787994385
Epoch 2810, val loss: 1.6642121076583862
Epoch 2820, training loss: 310.44415283203125 = 0.012025079689919949 + 50.0 * 6.208642482757568
Epoch 2820, val loss: 1.667136311531067
Epoch 2830, training loss: 310.4385986328125 = 0.011890506371855736 + 50.0 * 6.208534240722656
Epoch 2830, val loss: 1.6695212125778198
Epoch 2840, training loss: 310.4751281738281 = 0.011762823909521103 + 50.0 * 6.209267616271973
Epoch 2840, val loss: 1.671892523765564
Epoch 2850, training loss: 310.3041687011719 = 0.011630142107605934 + 50.0 * 6.205850601196289
Epoch 2850, val loss: 1.6743634939193726
Epoch 2860, training loss: 310.36492919921875 = 0.011508774943649769 + 50.0 * 6.20706844329834
Epoch 2860, val loss: 1.6772440671920776
Epoch 2870, training loss: 310.37982177734375 = 0.011387046426534653 + 50.0 * 6.207368850708008
Epoch 2870, val loss: 1.679394006729126
Epoch 2880, training loss: 310.29888916015625 = 0.011266727931797504 + 50.0 * 6.205752849578857
Epoch 2880, val loss: 1.6819539070129395
Epoch 2890, training loss: 310.32513427734375 = 0.011148769408464432 + 50.0 * 6.206279754638672
Epoch 2890, val loss: 1.6843065023422241
Epoch 2900, training loss: 310.36834716796875 = 0.011034789495170116 + 50.0 * 6.207145690917969
Epoch 2900, val loss: 1.6868977546691895
Epoch 2910, training loss: 310.2674255371094 = 0.010920647531747818 + 50.0 * 6.205130100250244
Epoch 2910, val loss: 1.6895629167556763
Epoch 2920, training loss: 310.38409423828125 = 0.010814804583787918 + 50.0 * 6.207466125488281
Epoch 2920, val loss: 1.6921143531799316
Epoch 2930, training loss: 310.2841796875 = 0.010701583698391914 + 50.0 * 6.205469608306885
Epoch 2930, val loss: 1.6940902471542358
Epoch 2940, training loss: 310.2249755859375 = 0.010592399165034294 + 50.0 * 6.204288005828857
Epoch 2940, val loss: 1.696536660194397
Epoch 2950, training loss: 310.3567199707031 = 0.010491171851754189 + 50.0 * 6.2069244384765625
Epoch 2950, val loss: 1.6991742849349976
Epoch 2960, training loss: 310.41571044921875 = 0.01038243155926466 + 50.0 * 6.208106994628906
Epoch 2960, val loss: 1.701339602470398
Epoch 2970, training loss: 310.3175964355469 = 0.010283004492521286 + 50.0 * 6.206146240234375
Epoch 2970, val loss: 1.7037692070007324
Epoch 2980, training loss: 310.1964111328125 = 0.010170790366828442 + 50.0 * 6.2037248611450195
Epoch 2980, val loss: 1.7056056261062622
Epoch 2990, training loss: 310.1490783691406 = 0.010075577534735203 + 50.0 * 6.202780246734619
Epoch 2990, val loss: 1.7085037231445312
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 431.7840270996094 = 1.9425592422485352 + 50.0 * 8.596829414367676
Epoch 0, val loss: 1.9421770572662354
Epoch 10, training loss: 431.7345886230469 = 1.934528112411499 + 50.0 * 8.596000671386719
Epoch 10, val loss: 1.9335168600082397
Epoch 20, training loss: 431.4524841308594 = 1.9247816801071167 + 50.0 * 8.590554237365723
Epoch 20, val loss: 1.9231640100479126
Epoch 30, training loss: 429.5191955566406 = 1.9127652645111084 + 50.0 * 8.552128791809082
Epoch 30, val loss: 1.9103020429611206
Epoch 40, training loss: 415.7275085449219 = 1.8979889154434204 + 50.0 * 8.276590347290039
Epoch 40, val loss: 1.8947288990020752
Epoch 50, training loss: 381.3195495605469 = 1.8813825845718384 + 50.0 * 7.58876371383667
Epoch 50, val loss: 1.8785614967346191
Epoch 60, training loss: 365.11004638671875 = 1.869638442993164 + 50.0 * 7.264808177947998
Epoch 60, val loss: 1.867842197418213
Epoch 70, training loss: 352.9214172363281 = 1.859548568725586 + 50.0 * 7.021237373352051
Epoch 70, val loss: 1.858276605606079
Epoch 80, training loss: 345.7850341796875 = 1.8494960069656372 + 50.0 * 6.878711223602295
Epoch 80, val loss: 1.8489946126937866
Epoch 90, training loss: 340.8435974121094 = 1.8397008180618286 + 50.0 * 6.780078411102295
Epoch 90, val loss: 1.8396536111831665
Epoch 100, training loss: 337.4336242675781 = 1.8300161361694336 + 50.0 * 6.712072372436523
Epoch 100, val loss: 1.8301775455474854
Epoch 110, training loss: 335.1236267089844 = 1.8205318450927734 + 50.0 * 6.666061878204346
Epoch 110, val loss: 1.8208383321762085
Epoch 120, training loss: 333.2464294433594 = 1.811537504196167 + 50.0 * 6.628698348999023
Epoch 120, val loss: 1.8121238946914673
Epoch 130, training loss: 331.888671875 = 1.802909255027771 + 50.0 * 6.601715087890625
Epoch 130, val loss: 1.8036547899246216
Epoch 140, training loss: 330.5702819824219 = 1.7941652536392212 + 50.0 * 6.575522422790527
Epoch 140, val loss: 1.7950087785720825
Epoch 150, training loss: 329.4505615234375 = 1.7851325273513794 + 50.0 * 6.553308486938477
Epoch 150, val loss: 1.7861264944076538
Epoch 160, training loss: 328.611083984375 = 1.7756212949752808 + 50.0 * 6.536708831787109
Epoch 160, val loss: 1.7769064903259277
Epoch 170, training loss: 327.6452331542969 = 1.765425443649292 + 50.0 * 6.51759672164917
Epoch 170, val loss: 1.7671079635620117
Epoch 180, training loss: 326.9246826171875 = 1.7543443441390991 + 50.0 * 6.503407001495361
Epoch 180, val loss: 1.7566273212432861
Epoch 190, training loss: 326.2374572753906 = 1.7422553300857544 + 50.0 * 6.489903926849365
Epoch 190, val loss: 1.7453099489212036
Epoch 200, training loss: 325.6793518066406 = 1.7290077209472656 + 50.0 * 6.479007244110107
Epoch 200, val loss: 1.73295259475708
Epoch 210, training loss: 325.010009765625 = 1.7144687175750732 + 50.0 * 6.4659104347229
Epoch 210, val loss: 1.7194628715515137
Epoch 220, training loss: 324.5474548339844 = 1.6986497640609741 + 50.0 * 6.456976413726807
Epoch 220, val loss: 1.704774260520935
Epoch 230, training loss: 324.07373046875 = 1.6813569068908691 + 50.0 * 6.447847366333008
Epoch 230, val loss: 1.6888792514801025
Epoch 240, training loss: 323.5350646972656 = 1.6627612113952637 + 50.0 * 6.437446594238281
Epoch 240, val loss: 1.6716201305389404
Epoch 250, training loss: 323.1248474121094 = 1.6427942514419556 + 50.0 * 6.429641246795654
Epoch 250, val loss: 1.6531776189804077
Epoch 260, training loss: 322.68853759765625 = 1.6214207410812378 + 50.0 * 6.421342372894287
Epoch 260, val loss: 1.6335699558258057
Epoch 270, training loss: 322.3348388671875 = 1.5988529920578003 + 50.0 * 6.414719581604004
Epoch 270, val loss: 1.612828254699707
Epoch 280, training loss: 322.0979919433594 = 1.5751948356628418 + 50.0 * 6.41045618057251
Epoch 280, val loss: 1.59125816822052
Epoch 290, training loss: 321.6350402832031 = 1.5505566596984863 + 50.0 * 6.401689529418945
Epoch 290, val loss: 1.5691027641296387
Epoch 300, training loss: 321.3169250488281 = 1.5252265930175781 + 50.0 * 6.395833492279053
Epoch 300, val loss: 1.546487808227539
Epoch 310, training loss: 321.00927734375 = 1.499252200126648 + 50.0 * 6.390200614929199
Epoch 310, val loss: 1.5236103534698486
Epoch 320, training loss: 320.6881103515625 = 1.4730205535888672 + 50.0 * 6.384301662445068
Epoch 320, val loss: 1.50080144405365
Epoch 330, training loss: 320.4261474609375 = 1.4465694427490234 + 50.0 * 6.379591464996338
Epoch 330, val loss: 1.4782549142837524
Epoch 340, training loss: 320.1689758300781 = 1.4201264381408691 + 50.0 * 6.374976634979248
Epoch 340, val loss: 1.4559454917907715
Epoch 350, training loss: 319.9521179199219 = 1.3937804698944092 + 50.0 * 6.371167182922363
Epoch 350, val loss: 1.4341541528701782
Epoch 360, training loss: 319.70587158203125 = 1.3675827980041504 + 50.0 * 6.36676549911499
Epoch 360, val loss: 1.4126875400543213
Epoch 370, training loss: 319.39361572265625 = 1.3417634963989258 + 50.0 * 6.361037254333496
Epoch 370, val loss: 1.3920438289642334
Epoch 380, training loss: 319.1809387207031 = 1.3162789344787598 + 50.0 * 6.357293128967285
Epoch 380, val loss: 1.3721153736114502
Epoch 390, training loss: 319.3577880859375 = 1.291118860244751 + 50.0 * 6.361333847045898
Epoch 390, val loss: 1.3528752326965332
Epoch 400, training loss: 318.9294128417969 = 1.2662734985351562 + 50.0 * 6.353262901306152
Epoch 400, val loss: 1.3339091539382935
Epoch 410, training loss: 318.6556701660156 = 1.2417696714401245 + 50.0 * 6.348278045654297
Epoch 410, val loss: 1.315800428390503
Epoch 420, training loss: 318.44732666015625 = 1.2176740169525146 + 50.0 * 6.344593048095703
Epoch 420, val loss: 1.2982509136199951
Epoch 430, training loss: 318.29803466796875 = 1.1939585208892822 + 50.0 * 6.342081069946289
Epoch 430, val loss: 1.2812069654464722
Epoch 440, training loss: 318.43585205078125 = 1.1706053018569946 + 50.0 * 6.3453049659729
Epoch 440, val loss: 1.2648603916168213
Epoch 450, training loss: 317.8749084472656 = 1.147501826286316 + 50.0 * 6.334548473358154
Epoch 450, val loss: 1.2485750913619995
Epoch 460, training loss: 317.7454528808594 = 1.1248151063919067 + 50.0 * 6.3324127197265625
Epoch 460, val loss: 1.2329635620117188
Epoch 470, training loss: 317.5635070800781 = 1.1024577617645264 + 50.0 * 6.329221248626709
Epoch 470, val loss: 1.217875361442566
Epoch 480, training loss: 317.7147216796875 = 1.0804517269134521 + 50.0 * 6.332685470581055
Epoch 480, val loss: 1.2032396793365479
Epoch 490, training loss: 317.37493896484375 = 1.0587033033370972 + 50.0 * 6.326324462890625
Epoch 490, val loss: 1.1887515783309937
Epoch 500, training loss: 317.1946716308594 = 1.0373356342315674 + 50.0 * 6.323146820068359
Epoch 500, val loss: 1.1747931241989136
Epoch 510, training loss: 317.06097412109375 = 1.0162653923034668 + 50.0 * 6.320894241333008
Epoch 510, val loss: 1.1613900661468506
Epoch 520, training loss: 317.06781005859375 = 0.995599627494812 + 50.0 * 6.321444034576416
Epoch 520, val loss: 1.1482348442077637
Epoch 530, training loss: 316.7981872558594 = 0.9751778841018677 + 50.0 * 6.316459655761719
Epoch 530, val loss: 1.1356148719787598
Epoch 540, training loss: 316.6643371582031 = 0.9551682472229004 + 50.0 * 6.314183235168457
Epoch 540, val loss: 1.1233019828796387
Epoch 550, training loss: 316.5461120605469 = 0.9354692101478577 + 50.0 * 6.312212944030762
Epoch 550, val loss: 1.1116974353790283
Epoch 560, training loss: 316.8099670410156 = 0.916181743144989 + 50.0 * 6.317875862121582
Epoch 560, val loss: 1.1003843545913696
Epoch 570, training loss: 316.4026184082031 = 0.8969255685806274 + 50.0 * 6.310113906860352
Epoch 570, val loss: 1.0892802476882935
Epoch 580, training loss: 316.24053955078125 = 0.8781616687774658 + 50.0 * 6.307247161865234
Epoch 580, val loss: 1.0785444974899292
Epoch 590, training loss: 316.1367492675781 = 0.859792947769165 + 50.0 * 6.305538654327393
Epoch 590, val loss: 1.0684418678283691
Epoch 600, training loss: 316.24072265625 = 0.8417179584503174 + 50.0 * 6.307980537414551
Epoch 600, val loss: 1.0588346719741821
Epoch 610, training loss: 316.06085205078125 = 0.8239105343818665 + 50.0 * 6.304738521575928
Epoch 610, val loss: 1.049695611000061
Epoch 620, training loss: 315.8481750488281 = 0.8064394593238831 + 50.0 * 6.300835132598877
Epoch 620, val loss: 1.0405563116073608
Epoch 630, training loss: 316.0088806152344 = 0.7893559336662292 + 50.0 * 6.3043904304504395
Epoch 630, val loss: 1.0319794416427612
Epoch 640, training loss: 315.71331787109375 = 0.7724677920341492 + 50.0 * 6.298817157745361
Epoch 640, val loss: 1.024282455444336
Epoch 650, training loss: 315.60504150390625 = 0.756007969379425 + 50.0 * 6.296980857849121
Epoch 650, val loss: 1.016509771347046
Epoch 660, training loss: 315.6138916015625 = 0.7398588061332703 + 50.0 * 6.297480583190918
Epoch 660, val loss: 1.0094120502471924
Epoch 670, training loss: 315.4128723144531 = 0.7239124178886414 + 50.0 * 6.293779373168945
Epoch 670, val loss: 1.0024397373199463
Epoch 680, training loss: 315.3095397949219 = 0.7082598805427551 + 50.0 * 6.292026042938232
Epoch 680, val loss: 0.9960277676582336
Epoch 690, training loss: 315.5451354980469 = 0.6929326057434082 + 50.0 * 6.297044277191162
Epoch 690, val loss: 0.9902492761611938
Epoch 700, training loss: 315.405517578125 = 0.67783123254776 + 50.0 * 6.294553756713867
Epoch 700, val loss: 0.9839274287223816
Epoch 710, training loss: 315.161865234375 = 0.6629364490509033 + 50.0 * 6.289978504180908
Epoch 710, val loss: 0.9785592555999756
Epoch 720, training loss: 315.0586853027344 = 0.6484162211418152 + 50.0 * 6.288205623626709
Epoch 720, val loss: 0.9737533926963806
Epoch 730, training loss: 315.08697509765625 = 0.6341375112533569 + 50.0 * 6.289056777954102
Epoch 730, val loss: 0.9691219925880432
Epoch 740, training loss: 314.92523193359375 = 0.6201030015945435 + 50.0 * 6.286102294921875
Epoch 740, val loss: 0.9648680686950684
Epoch 750, training loss: 314.87945556640625 = 0.6063446998596191 + 50.0 * 6.285461902618408
Epoch 750, val loss: 0.9607077240943909
Epoch 760, training loss: 314.86260986328125 = 0.5928773283958435 + 50.0 * 6.285394668579102
Epoch 760, val loss: 0.957236647605896
Epoch 770, training loss: 314.8411865234375 = 0.5796267986297607 + 50.0 * 6.285231113433838
Epoch 770, val loss: 0.953511118888855
Epoch 780, training loss: 314.75115966796875 = 0.566487729549408 + 50.0 * 6.283693313598633
Epoch 780, val loss: 0.9508005976676941
Epoch 790, training loss: 314.63720703125 = 0.5537112355232239 + 50.0 * 6.281669616699219
Epoch 790, val loss: 0.9477114677429199
Epoch 800, training loss: 314.75054931640625 = 0.5411422252655029 + 50.0 * 6.284188270568848
Epoch 800, val loss: 0.9456696510314941
Epoch 810, training loss: 314.50213623046875 = 0.5287305116653442 + 50.0 * 6.279468536376953
Epoch 810, val loss: 0.9430177211761475
Epoch 820, training loss: 314.40289306640625 = 0.516590416431427 + 50.0 * 6.277725696563721
Epoch 820, val loss: 0.9412893652915955
Epoch 830, training loss: 314.3822021484375 = 0.5047140121459961 + 50.0 * 6.277549743652344
Epoch 830, val loss: 0.9394915103912354
Epoch 840, training loss: 314.4004821777344 = 0.4929898679256439 + 50.0 * 6.2781500816345215
Epoch 840, val loss: 0.9381194710731506
Epoch 850, training loss: 314.3016357421875 = 0.4813787043094635 + 50.0 * 6.276405334472656
Epoch 850, val loss: 0.9371147155761719
Epoch 860, training loss: 314.2505187988281 = 0.47005292773246765 + 50.0 * 6.275609016418457
Epoch 860, val loss: 0.9358230233192444
Epoch 870, training loss: 314.1737060546875 = 0.45891931653022766 + 50.0 * 6.274295806884766
Epoch 870, val loss: 0.9349392652511597
Epoch 880, training loss: 314.20391845703125 = 0.447957307100296 + 50.0 * 6.275119304656982
Epoch 880, val loss: 0.9347583055496216
Epoch 890, training loss: 313.98760986328125 = 0.437191367149353 + 50.0 * 6.271008014678955
Epoch 890, val loss: 0.9341970086097717
Epoch 900, training loss: 313.9538879394531 = 0.4266860783100128 + 50.0 * 6.270543575286865
Epoch 900, val loss: 0.9343656301498413
Epoch 910, training loss: 314.0182800292969 = 0.4163673520088196 + 50.0 * 6.272037982940674
Epoch 910, val loss: 0.9343603849411011
Epoch 920, training loss: 313.87542724609375 = 0.40617066621780396 + 50.0 * 6.26938533782959
Epoch 920, val loss: 0.9350053071975708
Epoch 930, training loss: 313.9820556640625 = 0.39620277285575867 + 50.0 * 6.271717071533203
Epoch 930, val loss: 0.9354881048202515
Epoch 940, training loss: 313.90985107421875 = 0.38633888959884644 + 50.0 * 6.270470142364502
Epoch 940, val loss: 0.9362583756446838
Epoch 950, training loss: 313.7678527832031 = 0.3767458200454712 + 50.0 * 6.267822265625
Epoch 950, val loss: 0.9372645020484924
Epoch 960, training loss: 313.6650695800781 = 0.36741504073143005 + 50.0 * 6.265953063964844
Epoch 960, val loss: 0.9384877681732178
Epoch 970, training loss: 313.71142578125 = 0.35828685760498047 + 50.0 * 6.267063140869141
Epoch 970, val loss: 0.9403718709945679
Epoch 980, training loss: 313.5858154296875 = 0.3492998778820038 + 50.0 * 6.264730453491211
Epoch 980, val loss: 0.9412742853164673
Epoch 990, training loss: 313.62335205078125 = 0.34050559997558594 + 50.0 * 6.265656471252441
Epoch 990, val loss: 0.9433937072753906
Epoch 1000, training loss: 313.5782775878906 = 0.331974059343338 + 50.0 * 6.264925956726074
Epoch 1000, val loss: 0.9448297023773193
Epoch 1010, training loss: 313.51416015625 = 0.32360610365867615 + 50.0 * 6.263811111450195
Epoch 1010, val loss: 0.9470162987709045
Epoch 1020, training loss: 313.58258056640625 = 0.3154660165309906 + 50.0 * 6.2653422355651855
Epoch 1020, val loss: 0.9493501782417297
Epoch 1030, training loss: 313.43475341796875 = 0.3074505925178528 + 50.0 * 6.262546062469482
Epoch 1030, val loss: 0.9516804814338684
Epoch 1040, training loss: 313.3276062011719 = 0.2996504008769989 + 50.0 * 6.26055908203125
Epoch 1040, val loss: 0.9541498422622681
Epoch 1050, training loss: 313.29827880859375 = 0.29212382435798645 + 50.0 * 6.260123252868652
Epoch 1050, val loss: 0.957021176815033
Epoch 1060, training loss: 313.65814208984375 = 0.28479576110839844 + 50.0 * 6.267467021942139
Epoch 1060, val loss: 0.9598000049591064
Epoch 1070, training loss: 313.3670349121094 = 0.2774941921234131 + 50.0 * 6.261790752410889
Epoch 1070, val loss: 0.9626880884170532
Epoch 1080, training loss: 313.2449645996094 = 0.27051475644111633 + 50.0 * 6.259489059448242
Epoch 1080, val loss: 0.9655725359916687
Epoch 1090, training loss: 313.3234558105469 = 0.2636882960796356 + 50.0 * 6.261195182800293
Epoch 1090, val loss: 0.968971848487854
Epoch 1100, training loss: 313.16436767578125 = 0.25703057646751404 + 50.0 * 6.258147239685059
Epoch 1100, val loss: 0.9722491502761841
Epoch 1110, training loss: 313.0978698730469 = 0.2505906820297241 + 50.0 * 6.256945610046387
Epoch 1110, val loss: 0.975809633731842
Epoch 1120, training loss: 313.2866516113281 = 0.24434809386730194 + 50.0 * 6.260846138000488
Epoch 1120, val loss: 0.9793362021446228
Epoch 1130, training loss: 313.2406921386719 = 0.23817773163318634 + 50.0 * 6.260049819946289
Epoch 1130, val loss: 0.9826602339744568
Epoch 1140, training loss: 313.02783203125 = 0.23223941028118134 + 50.0 * 6.255911827087402
Epoch 1140, val loss: 0.9862062931060791
Epoch 1150, training loss: 313.08294677734375 = 0.22649337351322174 + 50.0 * 6.257129192352295
Epoch 1150, val loss: 0.9896810054779053
Epoch 1160, training loss: 312.97027587890625 = 0.22084948420524597 + 50.0 * 6.254988670349121
Epoch 1160, val loss: 0.993786633014679
Epoch 1170, training loss: 312.91717529296875 = 0.21538496017456055 + 50.0 * 6.254035949707031
Epoch 1170, val loss: 0.997986376285553
Epoch 1180, training loss: 312.8725280761719 = 0.21011106669902802 + 50.0 * 6.25324821472168
Epoch 1180, val loss: 1.0018352270126343
Epoch 1190, training loss: 313.0345458984375 = 0.2049708366394043 + 50.0 * 6.256591796875
Epoch 1190, val loss: 1.0063668489456177
Epoch 1200, training loss: 312.9720153808594 = 0.19992081820964813 + 50.0 * 6.255442142486572
Epoch 1200, val loss: 1.0094326734542847
Epoch 1210, training loss: 312.89410400390625 = 0.19497443735599518 + 50.0 * 6.2539825439453125
Epoch 1210, val loss: 1.0136433839797974
Epoch 1220, training loss: 312.78656005859375 = 0.19023416936397552 + 50.0 * 6.251926422119141
Epoch 1220, val loss: 1.0178964138031006
Epoch 1230, training loss: 313.0242004394531 = 0.18563154339790344 + 50.0 * 6.256771087646484
Epoch 1230, val loss: 1.0217500925064087
Epoch 1240, training loss: 312.8023681640625 = 0.1811276525259018 + 50.0 * 6.252425193786621
Epoch 1240, val loss: 1.026344895362854
Epoch 1250, training loss: 312.695068359375 = 0.17676754295825958 + 50.0 * 6.2503662109375
Epoch 1250, val loss: 1.0305991172790527
Epoch 1260, training loss: 312.7327880859375 = 0.1725742369890213 + 50.0 * 6.251204490661621
Epoch 1260, val loss: 1.0348758697509766
Epoch 1270, training loss: 312.6706237792969 = 0.1684209406375885 + 50.0 * 6.250043869018555
Epoch 1270, val loss: 1.0391114950180054
Epoch 1280, training loss: 312.6669616699219 = 0.16438932716846466 + 50.0 * 6.250051021575928
Epoch 1280, val loss: 1.0434904098510742
Epoch 1290, training loss: 312.6044921875 = 0.16050216555595398 + 50.0 * 6.248879909515381
Epoch 1290, val loss: 1.048367977142334
Epoch 1300, training loss: 312.5726013183594 = 0.15671220421791077 + 50.0 * 6.248317718505859
Epoch 1300, val loss: 1.0530951023101807
Epoch 1310, training loss: 312.6022033691406 = 0.15304267406463623 + 50.0 * 6.248982906341553
Epoch 1310, val loss: 1.0576051473617554
Epoch 1320, training loss: 312.6074523925781 = 0.1494581252336502 + 50.0 * 6.249160289764404
Epoch 1320, val loss: 1.0614383220672607
Epoch 1330, training loss: 312.6049499511719 = 0.14597758650779724 + 50.0 * 6.249179840087891
Epoch 1330, val loss: 1.0665900707244873
Epoch 1340, training loss: 312.5173034667969 = 0.14256541430950165 + 50.0 * 6.247494220733643
Epoch 1340, val loss: 1.0711041688919067
Epoch 1350, training loss: 312.41644287109375 = 0.13927429914474487 + 50.0 * 6.245543479919434
Epoch 1350, val loss: 1.0762146711349487
Epoch 1360, training loss: 312.5287780761719 = 0.1360773742198944 + 50.0 * 6.247854232788086
Epoch 1360, val loss: 1.0808312892913818
Epoch 1370, training loss: 312.3427734375 = 0.13295917212963104 + 50.0 * 6.244195938110352
Epoch 1370, val loss: 1.0856175422668457
Epoch 1380, training loss: 312.39544677734375 = 0.12994399666786194 + 50.0 * 6.245310306549072
Epoch 1380, val loss: 1.090189814567566
Epoch 1390, training loss: 312.52532958984375 = 0.12698175013065338 + 50.0 * 6.247966766357422
Epoch 1390, val loss: 1.0950285196304321
Epoch 1400, training loss: 312.4522399902344 = 0.12411005795001984 + 50.0 * 6.246562480926514
Epoch 1400, val loss: 1.0995537042617798
Epoch 1410, training loss: 312.44122314453125 = 0.1212821826338768 + 50.0 * 6.24639892578125
Epoch 1410, val loss: 1.1043349504470825
Epoch 1420, training loss: 312.3207092285156 = 0.11852940171957016 + 50.0 * 6.244043827056885
Epoch 1420, val loss: 1.1099985837936401
Epoch 1430, training loss: 312.2819519042969 = 0.11589250713586807 + 50.0 * 6.243320941925049
Epoch 1430, val loss: 1.114750862121582
Epoch 1440, training loss: 312.2633056640625 = 0.11330309510231018 + 50.0 * 6.243000030517578
Epoch 1440, val loss: 1.1198538541793823
Epoch 1450, training loss: 312.23114013671875 = 0.11077143996953964 + 50.0 * 6.242407321929932
Epoch 1450, val loss: 1.1245863437652588
Epoch 1460, training loss: 312.4344177246094 = 0.10831670463085175 + 50.0 * 6.246521949768066
Epoch 1460, val loss: 1.1291311979293823
Epoch 1470, training loss: 312.18304443359375 = 0.10590367764234543 + 50.0 * 6.241542816162109
Epoch 1470, val loss: 1.1342839002609253
Epoch 1480, training loss: 312.12841796875 = 0.10357379913330078 + 50.0 * 6.240496635437012
Epoch 1480, val loss: 1.1391053199768066
Epoch 1490, training loss: 312.22918701171875 = 0.10131756216287613 + 50.0 * 6.242557525634766
Epoch 1490, val loss: 1.1441333293914795
Epoch 1500, training loss: 312.04644775390625 = 0.09909959882497787 + 50.0 * 6.23894739151001
Epoch 1500, val loss: 1.149357795715332
Epoch 1510, training loss: 312.18109130859375 = 0.096965491771698 + 50.0 * 6.241682529449463
Epoch 1510, val loss: 1.1547553539276123
Epoch 1520, training loss: 312.1529541015625 = 0.09486310929059982 + 50.0 * 6.241161823272705
Epoch 1520, val loss: 1.1587224006652832
Epoch 1530, training loss: 312.2657165527344 = 0.09280343353748322 + 50.0 * 6.243458271026611
Epoch 1530, val loss: 1.1638174057006836
Epoch 1540, training loss: 312.0114440917969 = 0.09079094976186752 + 50.0 * 6.238413333892822
Epoch 1540, val loss: 1.1687299013137817
Epoch 1550, training loss: 311.9822692871094 = 0.08884768933057785 + 50.0 * 6.237868785858154
Epoch 1550, val loss: 1.1742236614227295
Epoch 1560, training loss: 311.92437744140625 = 0.08698282390832901 + 50.0 * 6.236747741699219
Epoch 1560, val loss: 1.179125189781189
Epoch 1570, training loss: 312.1915283203125 = 0.08516859263181686 + 50.0 * 6.242126941680908
Epoch 1570, val loss: 1.1850286722183228
Epoch 1580, training loss: 312.0378723144531 = 0.08334488421678543 + 50.0 * 6.239090442657471
Epoch 1580, val loss: 1.1885358095169067
Epoch 1590, training loss: 312.1169738769531 = 0.08159118890762329 + 50.0 * 6.2407073974609375
Epoch 1590, val loss: 1.194291114807129
Epoch 1600, training loss: 311.8707275390625 = 0.07987207919359207 + 50.0 * 6.2358174324035645
Epoch 1600, val loss: 1.1990752220153809
Epoch 1610, training loss: 311.90032958984375 = 0.07822941243648529 + 50.0 * 6.2364420890808105
Epoch 1610, val loss: 1.2036640644073486
Epoch 1620, training loss: 312.02398681640625 = 0.07662222534418106 + 50.0 * 6.23894739151001
Epoch 1620, val loss: 1.2089711427688599
Epoch 1630, training loss: 312.0302734375 = 0.07503709942102432 + 50.0 * 6.239104747772217
Epoch 1630, val loss: 1.2145062685012817
Epoch 1640, training loss: 311.87060546875 = 0.07349158078432083 + 50.0 * 6.235942363739014
Epoch 1640, val loss: 1.2187589406967163
Epoch 1650, training loss: 311.9062194824219 = 0.07198616117238998 + 50.0 * 6.236684322357178
Epoch 1650, val loss: 1.2242636680603027
Epoch 1660, training loss: 311.7988586425781 = 0.07051119208335876 + 50.0 * 6.234566688537598
Epoch 1660, val loss: 1.229235291481018
Epoch 1670, training loss: 311.90533447265625 = 0.069089874625206 + 50.0 * 6.236724853515625
Epoch 1670, val loss: 1.234291434288025
Epoch 1680, training loss: 311.7648010253906 = 0.06769534200429916 + 50.0 * 6.233942031860352
Epoch 1680, val loss: 1.2395416498184204
Epoch 1690, training loss: 311.84503173828125 = 0.06634677201509476 + 50.0 * 6.235573768615723
Epoch 1690, val loss: 1.244925618171692
Epoch 1700, training loss: 311.8670654296875 = 0.06501474231481552 + 50.0 * 6.236041069030762
Epoch 1700, val loss: 1.2497187852859497
Epoch 1710, training loss: 311.7481994628906 = 0.06373181939125061 + 50.0 * 6.233689785003662
Epoch 1710, val loss: 1.2543131113052368
Epoch 1720, training loss: 311.7624206542969 = 0.06247904524207115 + 50.0 * 6.233999252319336
Epoch 1720, val loss: 1.2595856189727783
Epoch 1730, training loss: 311.97906494140625 = 0.061263926327228546 + 50.0 * 6.238356113433838
Epoch 1730, val loss: 1.264648199081421
Epoch 1740, training loss: 311.7847595214844 = 0.06004507839679718 + 50.0 * 6.234493732452393
Epoch 1740, val loss: 1.2686729431152344
Epoch 1750, training loss: 311.64202880859375 = 0.05885782092809677 + 50.0 * 6.231663227081299
Epoch 1750, val loss: 1.2746011018753052
Epoch 1760, training loss: 311.6278381347656 = 0.05773187801241875 + 50.0 * 6.2314019203186035
Epoch 1760, val loss: 1.2792621850967407
Epoch 1770, training loss: 311.92529296875 = 0.0566413514316082 + 50.0 * 6.237372875213623
Epoch 1770, val loss: 1.283707618713379
Epoch 1780, training loss: 311.7245788574219 = 0.05553489923477173 + 50.0 * 6.2333807945251465
Epoch 1780, val loss: 1.2888814210891724
Epoch 1790, training loss: 311.6658630371094 = 0.05445263534784317 + 50.0 * 6.2322282791137695
Epoch 1790, val loss: 1.2940489053726196
Epoch 1800, training loss: 311.66156005859375 = 0.05342075601220131 + 50.0 * 6.232162952423096
Epoch 1800, val loss: 1.2990251779556274
Epoch 1810, training loss: 311.5721740722656 = 0.0524153895676136 + 50.0 * 6.2303948402404785
Epoch 1810, val loss: 1.3038779497146606
Epoch 1820, training loss: 311.6607971191406 = 0.051431991159915924 + 50.0 * 6.232187271118164
Epoch 1820, val loss: 1.3089991807937622
Epoch 1830, training loss: 311.74658203125 = 0.050464991480112076 + 50.0 * 6.233922004699707
Epoch 1830, val loss: 1.3139472007751465
Epoch 1840, training loss: 311.56292724609375 = 0.04951658099889755 + 50.0 * 6.2302680015563965
Epoch 1840, val loss: 1.3182114362716675
Epoch 1850, training loss: 311.501220703125 = 0.04859384521842003 + 50.0 * 6.229053020477295
Epoch 1850, val loss: 1.3237446546554565
Epoch 1860, training loss: 311.4764709472656 = 0.0476967953145504 + 50.0 * 6.228575229644775
Epoch 1860, val loss: 1.328801155090332
Epoch 1870, training loss: 311.5168151855469 = 0.046837784349918365 + 50.0 * 6.229399681091309
Epoch 1870, val loss: 1.3338563442230225
Epoch 1880, training loss: 311.6565246582031 = 0.04599377140402794 + 50.0 * 6.232210636138916
Epoch 1880, val loss: 1.3382458686828613
Epoch 1890, training loss: 311.7068786621094 = 0.04515085369348526 + 50.0 * 6.233234405517578
Epoch 1890, val loss: 1.3420227766036987
Epoch 1900, training loss: 311.5011901855469 = 0.044309984892606735 + 50.0 * 6.229137420654297
Epoch 1900, val loss: 1.3483939170837402
Epoch 1910, training loss: 311.61700439453125 = 0.04352208599448204 + 50.0 * 6.231469631195068
Epoch 1910, val loss: 1.3534135818481445
Epoch 1920, training loss: 311.3909912109375 = 0.0427340604364872 + 50.0 * 6.226965427398682
Epoch 1920, val loss: 1.3568888902664185
Epoch 1930, training loss: 311.40643310546875 = 0.041980016976594925 + 50.0 * 6.227288722991943
Epoch 1930, val loss: 1.3619565963745117
Epoch 1940, training loss: 311.5438232421875 = 0.041249118745326996 + 50.0 * 6.230051517486572
Epoch 1940, val loss: 1.3669902086257935
Epoch 1950, training loss: 311.3866271972656 = 0.04051033779978752 + 50.0 * 6.226922512054443
Epoch 1950, val loss: 1.3715412616729736
Epoch 1960, training loss: 311.38714599609375 = 0.039790742099285126 + 50.0 * 6.22694730758667
Epoch 1960, val loss: 1.3759931325912476
Epoch 1970, training loss: 311.486328125 = 0.039106275886297226 + 50.0 * 6.228944778442383
Epoch 1970, val loss: 1.3804701566696167
Epoch 1980, training loss: 311.4444274902344 = 0.038416724652051926 + 50.0 * 6.22812032699585
Epoch 1980, val loss: 1.3852232694625854
Epoch 1990, training loss: 311.38824462890625 = 0.037751395255327225 + 50.0 * 6.2270097732543945
Epoch 1990, val loss: 1.3904201984405518
Epoch 2000, training loss: 311.4037170410156 = 0.03710372373461723 + 50.0 * 6.22733211517334
Epoch 2000, val loss: 1.3946717977523804
Epoch 2010, training loss: 311.3663024902344 = 0.03647414594888687 + 50.0 * 6.226596832275391
Epoch 2010, val loss: 1.3990252017974854
Epoch 2020, training loss: 311.6318054199219 = 0.035876065492630005 + 50.0 * 6.2319183349609375
Epoch 2020, val loss: 1.4028593301773071
Epoch 2030, training loss: 311.35101318359375 = 0.035239022225141525 + 50.0 * 6.226315498352051
Epoch 2030, val loss: 1.4084203243255615
Epoch 2040, training loss: 311.35546875 = 0.034648727625608444 + 50.0 * 6.22641658782959
Epoch 2040, val loss: 1.4122095108032227
Epoch 2050, training loss: 311.2984924316406 = 0.0340636782348156 + 50.0 * 6.2252888679504395
Epoch 2050, val loss: 1.4171175956726074
Epoch 2060, training loss: 311.2327575683594 = 0.033504389226436615 + 50.0 * 6.223984718322754
Epoch 2060, val loss: 1.421647310256958
Epoch 2070, training loss: 311.33978271484375 = 0.032959457486867905 + 50.0 * 6.226136684417725
Epoch 2070, val loss: 1.4255897998809814
Epoch 2080, training loss: 311.2749938964844 = 0.03241313248872757 + 50.0 * 6.224851608276367
Epoch 2080, val loss: 1.430287480354309
Epoch 2090, training loss: 311.3443603515625 = 0.031879786401987076 + 50.0 * 6.2262492179870605
Epoch 2090, val loss: 1.43528413772583
Epoch 2100, training loss: 311.19061279296875 = 0.03135230392217636 + 50.0 * 6.2231855392456055
Epoch 2100, val loss: 1.4388490915298462
Epoch 2110, training loss: 311.3110046386719 = 0.030854996293783188 + 50.0 * 6.225603103637695
Epoch 2110, val loss: 1.4438565969467163
Epoch 2120, training loss: 311.3288879394531 = 0.03036021627485752 + 50.0 * 6.225970268249512
Epoch 2120, val loss: 1.4480293989181519
Epoch 2130, training loss: 311.2406921386719 = 0.02985534444451332 + 50.0 * 6.224216938018799
Epoch 2130, val loss: 1.4526865482330322
Epoch 2140, training loss: 311.25726318359375 = 0.02938879281282425 + 50.0 * 6.224557399749756
Epoch 2140, val loss: 1.4568681716918945
Epoch 2150, training loss: 311.25482177734375 = 0.02892198972404003 + 50.0 * 6.224517822265625
Epoch 2150, val loss: 1.4615293741226196
Epoch 2160, training loss: 311.1920471191406 = 0.02847263216972351 + 50.0 * 6.223271369934082
Epoch 2160, val loss: 1.4649136066436768
Epoch 2170, training loss: 311.2232666015625 = 0.028026467189192772 + 50.0 * 6.223905086517334
Epoch 2170, val loss: 1.4697450399398804
Epoch 2180, training loss: 311.3191833496094 = 0.027589064091444016 + 50.0 * 6.225831985473633
Epoch 2180, val loss: 1.4738348722457886
Epoch 2190, training loss: 311.25299072265625 = 0.027156855911016464 + 50.0 * 6.22451639175415
Epoch 2190, val loss: 1.4771467447280884
Epoch 2200, training loss: 311.1087646484375 = 0.026730885729193687 + 50.0 * 6.221640586853027
Epoch 2200, val loss: 1.4820945262908936
Epoch 2210, training loss: 311.0613708496094 = 0.0263233482837677 + 50.0 * 6.220700740814209
Epoch 2210, val loss: 1.4863635301589966
Epoch 2220, training loss: 311.04541015625 = 0.025932036340236664 + 50.0 * 6.220389366149902
Epoch 2220, val loss: 1.4906628131866455
Epoch 2230, training loss: 311.20855712890625 = 0.025548584759235382 + 50.0 * 6.223659992218018
Epoch 2230, val loss: 1.495734453201294
Epoch 2240, training loss: 311.0926208496094 = 0.0251590758562088 + 50.0 * 6.221349239349365
Epoch 2240, val loss: 1.4983534812927246
Epoch 2250, training loss: 311.24334716796875 = 0.024778585880994797 + 50.0 * 6.224371433258057
Epoch 2250, val loss: 1.5029511451721191
Epoch 2260, training loss: 311.1515197753906 = 0.024404551833868027 + 50.0 * 6.2225422859191895
Epoch 2260, val loss: 1.5067187547683716
Epoch 2270, training loss: 311.10406494140625 = 0.024040766060352325 + 50.0 * 6.22160005569458
Epoch 2270, val loss: 1.511366605758667
Epoch 2280, training loss: 311.07720947265625 = 0.023692024871706963 + 50.0 * 6.221070766448975
Epoch 2280, val loss: 1.5146408081054688
Epoch 2290, training loss: 311.0354919433594 = 0.023349367082118988 + 50.0 * 6.220242977142334
Epoch 2290, val loss: 1.5188792943954468
Epoch 2300, training loss: 310.98193359375 = 0.023008231073617935 + 50.0 * 6.219178199768066
Epoch 2300, val loss: 1.5231554508209229
Epoch 2310, training loss: 311.1759033203125 = 0.022688696160912514 + 50.0 * 6.223064422607422
Epoch 2310, val loss: 1.5272256135940552
Epoch 2320, training loss: 311.0508728027344 = 0.022362062707543373 + 50.0 * 6.220570087432861
Epoch 2320, val loss: 1.5302330255508423
Epoch 2330, training loss: 311.07586669921875 = 0.022040603682398796 + 50.0 * 6.221076488494873
Epoch 2330, val loss: 1.5339829921722412
Epoch 2340, training loss: 311.1021423339844 = 0.02172142267227173 + 50.0 * 6.2216081619262695
Epoch 2340, val loss: 1.5378299951553345
Epoch 2350, training loss: 310.9870910644531 = 0.021410087123513222 + 50.0 * 6.219313144683838
Epoch 2350, val loss: 1.5418931245803833
Epoch 2360, training loss: 311.03558349609375 = 0.021113155409693718 + 50.0 * 6.22028923034668
Epoch 2360, val loss: 1.546000599861145
Epoch 2370, training loss: 311.0937194824219 = 0.020821988582611084 + 50.0 * 6.2214579582214355
Epoch 2370, val loss: 1.5499460697174072
Epoch 2380, training loss: 311.03472900390625 = 0.020529229193925858 + 50.0 * 6.2202839851379395
Epoch 2380, val loss: 1.5542099475860596
Epoch 2390, training loss: 310.89569091796875 = 0.020246895030140877 + 50.0 * 6.2175092697143555
Epoch 2390, val loss: 1.5575608015060425
Epoch 2400, training loss: 310.9423828125 = 0.019977521151304245 + 50.0 * 6.218447685241699
Epoch 2400, val loss: 1.561376690864563
Epoch 2410, training loss: 311.1080017089844 = 0.01971277967095375 + 50.0 * 6.221765518188477
Epoch 2410, val loss: 1.5657776594161987
Epoch 2420, training loss: 310.913818359375 = 0.019437963142991066 + 50.0 * 6.2178874015808105
Epoch 2420, val loss: 1.5693169832229614
Epoch 2430, training loss: 311.05712890625 = 0.01918233372271061 + 50.0 * 6.220758438110352
Epoch 2430, val loss: 1.5724847316741943
Epoch 2440, training loss: 310.93756103515625 = 0.018921539187431335 + 50.0 * 6.218372821807861
Epoch 2440, val loss: 1.5763487815856934
Epoch 2450, training loss: 310.9159240722656 = 0.01866976171731949 + 50.0 * 6.217945098876953
Epoch 2450, val loss: 1.5794346332550049
Epoch 2460, training loss: 311.27630615234375 = 0.018445495516061783 + 50.0 * 6.225157260894775
Epoch 2460, val loss: 1.5826932191848755
Epoch 2470, training loss: 311.0267028808594 = 0.018177075311541557 + 50.0 * 6.220170497894287
Epoch 2470, val loss: 1.5875134468078613
Epoch 2480, training loss: 311.0111999511719 = 0.01794079691171646 + 50.0 * 6.219864845275879
Epoch 2480, val loss: 1.5900063514709473
Epoch 2490, training loss: 310.8348388671875 = 0.01770506612956524 + 50.0 * 6.216342926025391
Epoch 2490, val loss: 1.5944263935089111
Epoch 2500, training loss: 310.82293701171875 = 0.017487160861492157 + 50.0 * 6.216108798980713
Epoch 2500, val loss: 1.597760558128357
Epoch 2510, training loss: 310.92987060546875 = 0.017269989475607872 + 50.0 * 6.218252182006836
Epoch 2510, val loss: 1.6014188528060913
Epoch 2520, training loss: 310.90045166015625 = 0.017049381509423256 + 50.0 * 6.217668056488037
Epoch 2520, val loss: 1.605040192604065
Epoch 2530, training loss: 311.08929443359375 = 0.016833573579788208 + 50.0 * 6.22144889831543
Epoch 2530, val loss: 1.6089215278625488
Epoch 2540, training loss: 310.9825439453125 = 0.016618357971310616 + 50.0 * 6.219318389892578
Epoch 2540, val loss: 1.6119858026504517
Epoch 2550, training loss: 310.9530029296875 = 0.016406530514359474 + 50.0 * 6.218731880187988
Epoch 2550, val loss: 1.615163803100586
Epoch 2560, training loss: 310.8804626464844 = 0.016201727092266083 + 50.0 * 6.21728515625
Epoch 2560, val loss: 1.618059515953064
Epoch 2570, training loss: 310.77490234375 = 0.01599944569170475 + 50.0 * 6.215178489685059
Epoch 2570, val loss: 1.6220122575759888
Epoch 2580, training loss: 310.7474670410156 = 0.015810683369636536 + 50.0 * 6.214632987976074
Epoch 2580, val loss: 1.6253995895385742
Epoch 2590, training loss: 311.0653381347656 = 0.01562953181564808 + 50.0 * 6.220994472503662
Epoch 2590, val loss: 1.6277450323104858
Epoch 2600, training loss: 310.8112487792969 = 0.015426001511514187 + 50.0 * 6.215916633605957
Epoch 2600, val loss: 1.632149577140808
Epoch 2610, training loss: 310.8150329589844 = 0.015243161469697952 + 50.0 * 6.215996265411377
Epoch 2610, val loss: 1.634961485862732
Epoch 2620, training loss: 310.8150634765625 = 0.01506154052913189 + 50.0 * 6.215999603271484
Epoch 2620, val loss: 1.638641595840454
Epoch 2630, training loss: 310.9334716796875 = 0.0148856732994318 + 50.0 * 6.218371868133545
Epoch 2630, val loss: 1.6432360410690308
Epoch 2640, training loss: 310.816650390625 = 0.014703839085996151 + 50.0 * 6.216038703918457
Epoch 2640, val loss: 1.6455564498901367
Epoch 2650, training loss: 310.7510070800781 = 0.014533576555550098 + 50.0 * 6.2147297859191895
Epoch 2650, val loss: 1.64821195602417
Epoch 2660, training loss: 310.7098693847656 = 0.014366579242050648 + 50.0 * 6.213910102844238
Epoch 2660, val loss: 1.6519161462783813
Epoch 2670, training loss: 310.7802734375 = 0.014208273030817509 + 50.0 * 6.215321063995361
Epoch 2670, val loss: 1.6553534269332886
Epoch 2680, training loss: 310.88134765625 = 0.014045383781194687 + 50.0 * 6.21734619140625
Epoch 2680, val loss: 1.6581265926361084
Epoch 2690, training loss: 310.80267333984375 = 0.01387810055166483 + 50.0 * 6.215775966644287
Epoch 2690, val loss: 1.6614800691604614
Epoch 2700, training loss: 310.9003601074219 = 0.013721967115998268 + 50.0 * 6.2177324295043945
Epoch 2700, val loss: 1.66497802734375
Epoch 2710, training loss: 310.8702697753906 = 0.013564463704824448 + 50.0 * 6.217134475708008
Epoch 2710, val loss: 1.6670161485671997
Epoch 2720, training loss: 310.7283935546875 = 0.013408493250608444 + 50.0 * 6.21429967880249
Epoch 2720, val loss: 1.6701786518096924
Epoch 2730, training loss: 310.647705078125 = 0.013261931017041206 + 50.0 * 6.212688446044922
Epoch 2730, val loss: 1.6739270687103271
Epoch 2740, training loss: 310.6577453613281 = 0.013120177201926708 + 50.0 * 6.212892532348633
Epoch 2740, val loss: 1.6772100925445557
Epoch 2750, training loss: 310.739501953125 = 0.012982753105461597 + 50.0 * 6.2145304679870605
Epoch 2750, val loss: 1.6805670261383057
Epoch 2760, training loss: 310.86212158203125 = 0.012838292866945267 + 50.0 * 6.216985702514648
Epoch 2760, val loss: 1.6839206218719482
Epoch 2770, training loss: 310.78143310546875 = 0.012691528536379337 + 50.0 * 6.21537446975708
Epoch 2770, val loss: 1.686049222946167
Epoch 2780, training loss: 310.7711486816406 = 0.0125561049208045 + 50.0 * 6.215171813964844
Epoch 2780, val loss: 1.6898301839828491
Epoch 2790, training loss: 310.7357482910156 = 0.012419801205396652 + 50.0 * 6.214466571807861
Epoch 2790, val loss: 1.6927282810211182
Epoch 2800, training loss: 310.6853942871094 = 0.012291472405195236 + 50.0 * 6.213461875915527
Epoch 2800, val loss: 1.694827914237976
Epoch 2810, training loss: 310.7280578613281 = 0.0121631333604455 + 50.0 * 6.214317321777344
Epoch 2810, val loss: 1.6980680227279663
Epoch 2820, training loss: 310.6300048828125 = 0.012032105587422848 + 50.0 * 6.212359428405762
Epoch 2820, val loss: 1.7012821435928345
Epoch 2830, training loss: 310.6325378417969 = 0.011909212917089462 + 50.0 * 6.2124128341674805
Epoch 2830, val loss: 1.7032794952392578
Epoch 2840, training loss: 311.0586853027344 = 0.011793926358222961 + 50.0 * 6.220938205718994
Epoch 2840, val loss: 1.7062528133392334
Epoch 2850, training loss: 310.643310546875 = 0.011653941124677658 + 50.0 * 6.21263313293457
Epoch 2850, val loss: 1.7096941471099854
Epoch 2860, training loss: 310.5316162109375 = 0.011534545570611954 + 50.0 * 6.21040153503418
Epoch 2860, val loss: 1.7132076025009155
Epoch 2870, training loss: 310.5406188964844 = 0.011422431096434593 + 50.0 * 6.2105841636657715
Epoch 2870, val loss: 1.7153278589248657
Epoch 2880, training loss: 310.62823486328125 = 0.011311753652989864 + 50.0 * 6.212337970733643
Epoch 2880, val loss: 1.7190929651260376
Epoch 2890, training loss: 310.70556640625 = 0.011198559775948524 + 50.0 * 6.2138872146606445
Epoch 2890, val loss: 1.7210772037506104
Epoch 2900, training loss: 310.8064880371094 = 0.01108571607619524 + 50.0 * 6.215908050537109
Epoch 2900, val loss: 1.724838376045227
Epoch 2910, training loss: 310.6318054199219 = 0.010970189236104488 + 50.0 * 6.212417125701904
Epoch 2910, val loss: 1.7268116474151611
Epoch 2920, training loss: 310.5885009765625 = 0.010865011252462864 + 50.0 * 6.211552619934082
Epoch 2920, val loss: 1.7298303842544556
Epoch 2930, training loss: 310.7112731933594 = 0.010762957856059074 + 50.0 * 6.214009761810303
Epoch 2930, val loss: 1.7332313060760498
Epoch 2940, training loss: 310.5333251953125 = 0.010653668083250523 + 50.0 * 6.210453510284424
Epoch 2940, val loss: 1.735653281211853
Epoch 2950, training loss: 310.6462097167969 = 0.01055356115102768 + 50.0 * 6.212713241577148
Epoch 2950, val loss: 1.738397240638733
Epoch 2960, training loss: 310.5072937011719 = 0.010448712855577469 + 50.0 * 6.20993709564209
Epoch 2960, val loss: 1.7410881519317627
Epoch 2970, training loss: 310.4942932128906 = 0.010348090901970863 + 50.0 * 6.209678649902344
Epoch 2970, val loss: 1.7434662580490112
Epoch 2980, training loss: 310.53070068359375 = 0.010254133492708206 + 50.0 * 6.210408687591553
Epoch 2980, val loss: 1.7464524507522583
Epoch 2990, training loss: 310.7583312988281 = 0.010162373073399067 + 50.0 * 6.214963436126709
Epoch 2990, val loss: 1.748962640762329
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7481481481481482
0.8128624143384291
The final CL Acc:0.72593, 0.03143, The final GNN Acc:0.81304, 0.00025
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13148])
remove edge: torch.Size([2, 7890])
updated graph: torch.Size([2, 10482])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.793212890625 = 1.9508440494537354 + 50.0 * 8.596847534179688
Epoch 0, val loss: 1.9441232681274414
Epoch 10, training loss: 431.74456787109375 = 1.9414678812026978 + 50.0 * 8.596061706542969
Epoch 10, val loss: 1.9351718425750732
Epoch 20, training loss: 431.4584045410156 = 1.9300868511199951 + 50.0 * 8.590566635131836
Epoch 20, val loss: 1.9240846633911133
Epoch 30, training loss: 429.5216064453125 = 1.915934443473816 + 50.0 * 8.55211353302002
Epoch 30, val loss: 1.9101287126541138
Epoch 40, training loss: 417.5246276855469 = 1.8997836112976074 + 50.0 * 8.31249713897705
Epoch 40, val loss: 1.894492745399475
Epoch 50, training loss: 390.09783935546875 = 1.8817644119262695 + 50.0 * 7.764321327209473
Epoch 50, val loss: 1.8765089511871338
Epoch 60, training loss: 373.5988464355469 = 1.8660300970077515 + 50.0 * 7.434656620025635
Epoch 60, val loss: 1.8616710901260376
Epoch 70, training loss: 360.42071533203125 = 1.8536542654037476 + 50.0 * 7.1713409423828125
Epoch 70, val loss: 1.850383996963501
Epoch 80, training loss: 350.426025390625 = 1.842646598815918 + 50.0 * 6.971667289733887
Epoch 80, val loss: 1.8401676416397095
Epoch 90, training loss: 344.4759216308594 = 1.8336272239685059 + 50.0 * 6.852846145629883
Epoch 90, val loss: 1.8314927816390991
Epoch 100, training loss: 340.7068786621094 = 1.8243387937545776 + 50.0 * 6.777650833129883
Epoch 100, val loss: 1.8228733539581299
Epoch 110, training loss: 337.76080322265625 = 1.8147823810577393 + 50.0 * 6.7189202308654785
Epoch 110, val loss: 1.814229965209961
Epoch 120, training loss: 335.3691711425781 = 1.8058885335922241 + 50.0 * 6.671266078948975
Epoch 120, val loss: 1.806035041809082
Epoch 130, training loss: 333.3856201171875 = 1.7975777387619019 + 50.0 * 6.631760597229004
Epoch 130, val loss: 1.7980732917785645
Epoch 140, training loss: 331.7919616699219 = 1.7891968488693237 + 50.0 * 6.600055694580078
Epoch 140, val loss: 1.7898235321044922
Epoch 150, training loss: 330.481689453125 = 1.780379295349121 + 50.0 * 6.574026584625244
Epoch 150, val loss: 1.7811100482940674
Epoch 160, training loss: 329.3185729980469 = 1.7710570096969604 + 50.0 * 6.550950050354004
Epoch 160, val loss: 1.7721542119979858
Epoch 170, training loss: 328.2770080566406 = 1.7611004114151 + 50.0 * 6.530318260192871
Epoch 170, val loss: 1.762650489807129
Epoch 180, training loss: 327.4192199707031 = 1.7504030466079712 + 50.0 * 6.513376235961914
Epoch 180, val loss: 1.7525427341461182
Epoch 190, training loss: 326.6323547363281 = 1.7387945652008057 + 50.0 * 6.497870922088623
Epoch 190, val loss: 1.741685152053833
Epoch 200, training loss: 325.90185546875 = 1.7262022495269775 + 50.0 * 6.483512878417969
Epoch 200, val loss: 1.7300333976745605
Epoch 210, training loss: 325.2760314941406 = 1.7125720977783203 + 50.0 * 6.471268653869629
Epoch 210, val loss: 1.7174538373947144
Epoch 220, training loss: 324.6781921386719 = 1.6977829933166504 + 50.0 * 6.45960807800293
Epoch 220, val loss: 1.7037955522537231
Epoch 230, training loss: 324.1343688964844 = 1.681694507598877 + 50.0 * 6.4490532875061035
Epoch 230, val loss: 1.689159631729126
Epoch 240, training loss: 323.72314453125 = 1.6643118858337402 + 50.0 * 6.441176891326904
Epoch 240, val loss: 1.6733332872390747
Epoch 250, training loss: 323.19781494140625 = 1.6454380750656128 + 50.0 * 6.431047439575195
Epoch 250, val loss: 1.6562668085098267
Epoch 260, training loss: 322.7497863769531 = 1.6252450942993164 + 50.0 * 6.42249059677124
Epoch 260, val loss: 1.6381334066390991
Epoch 270, training loss: 322.4708251953125 = 1.6037033796310425 + 50.0 * 6.417342662811279
Epoch 270, val loss: 1.6190298795700073
Epoch 280, training loss: 322.0585021972656 = 1.5810004472732544 + 50.0 * 6.409550189971924
Epoch 280, val loss: 1.5985018014907837
Epoch 290, training loss: 321.7093200683594 = 1.557117223739624 + 50.0 * 6.403043746948242
Epoch 290, val loss: 1.5773637294769287
Epoch 300, training loss: 321.3150634765625 = 1.5321896076202393 + 50.0 * 6.395657062530518
Epoch 300, val loss: 1.555533528327942
Epoch 310, training loss: 320.99029541015625 = 1.506439447402954 + 50.0 * 6.389677047729492
Epoch 310, val loss: 1.533177137374878
Epoch 320, training loss: 320.7112121582031 = 1.48006010055542 + 50.0 * 6.384622573852539
Epoch 320, val loss: 1.5103737115859985
Epoch 330, training loss: 320.5284118652344 = 1.4530177116394043 + 50.0 * 6.3815083503723145
Epoch 330, val loss: 1.4871673583984375
Epoch 340, training loss: 320.2696228027344 = 1.4256510734558105 + 50.0 * 6.3768792152404785
Epoch 340, val loss: 1.4638900756835938
Epoch 350, training loss: 319.91717529296875 = 1.3979213237762451 + 50.0 * 6.37038516998291
Epoch 350, val loss: 1.4405766725540161
Epoch 360, training loss: 319.6546325683594 = 1.370123028755188 + 50.0 * 6.365690231323242
Epoch 360, val loss: 1.4173754453659058
Epoch 370, training loss: 319.5303649902344 = 1.3422952890396118 + 50.0 * 6.3637614250183105
Epoch 370, val loss: 1.3941733837127686
Epoch 380, training loss: 319.3161926269531 = 1.3144162893295288 + 50.0 * 6.360035419464111
Epoch 380, val loss: 1.3711843490600586
Epoch 390, training loss: 319.0992431640625 = 1.2864774465560913 + 50.0 * 6.356255531311035
Epoch 390, val loss: 1.348360300064087
Epoch 400, training loss: 318.9153747558594 = 1.2588473558425903 + 50.0 * 6.353130340576172
Epoch 400, val loss: 1.3253482580184937
Epoch 410, training loss: 318.64080810546875 = 1.231317400932312 + 50.0 * 6.348189830780029
Epoch 410, val loss: 1.3030329942703247
Epoch 420, training loss: 318.58465576171875 = 1.204166054725647 + 50.0 * 6.347609996795654
Epoch 420, val loss: 1.2810001373291016
Epoch 430, training loss: 318.3000793457031 = 1.1772795915603638 + 50.0 * 6.342455863952637
Epoch 430, val loss: 1.259048342704773
Epoch 440, training loss: 318.0857849121094 = 1.1507128477096558 + 50.0 * 6.338701248168945
Epoch 440, val loss: 1.2376885414123535
Epoch 450, training loss: 318.0939025878906 = 1.124586820602417 + 50.0 * 6.339386463165283
Epoch 450, val loss: 1.2165600061416626
Epoch 460, training loss: 317.6871643066406 = 1.0988702774047852 + 50.0 * 6.331765651702881
Epoch 460, val loss: 1.1958376169204712
Epoch 470, training loss: 317.71478271484375 = 1.0736126899719238 + 50.0 * 6.332823276519775
Epoch 470, val loss: 1.1756956577301025
Epoch 480, training loss: 317.4771728515625 = 1.0489598512649536 + 50.0 * 6.328564643859863
Epoch 480, val loss: 1.1558012962341309
Epoch 490, training loss: 317.3802490234375 = 1.0247070789337158 + 50.0 * 6.327110767364502
Epoch 490, val loss: 1.136799931526184
Epoch 500, training loss: 317.2173767089844 = 1.0011566877365112 + 50.0 * 6.324324131011963
Epoch 500, val loss: 1.1181308031082153
Epoch 510, training loss: 317.0200500488281 = 0.9782450795173645 + 50.0 * 6.320836067199707
Epoch 510, val loss: 1.10016930103302
Epoch 520, training loss: 317.0050964355469 = 0.95576012134552 + 50.0 * 6.320987224578857
Epoch 520, val loss: 1.0828607082366943
Epoch 530, training loss: 316.74688720703125 = 0.9339255690574646 + 50.0 * 6.316258907318115
Epoch 530, val loss: 1.0661031007766724
Epoch 540, training loss: 316.5946960449219 = 0.912578284740448 + 50.0 * 6.313642501831055
Epoch 540, val loss: 1.050202488899231
Epoch 550, training loss: 316.5693359375 = 0.8919615745544434 + 50.0 * 6.313547134399414
Epoch 550, val loss: 1.0346630811691284
Epoch 560, training loss: 316.55035400390625 = 0.8716700673103333 + 50.0 * 6.313573837280273
Epoch 560, val loss: 1.0198942422866821
Epoch 570, training loss: 316.26715087890625 = 0.8519150018692017 + 50.0 * 6.308304309844971
Epoch 570, val loss: 1.005529761314392
Epoch 580, training loss: 316.0988464355469 = 0.832632303237915 + 50.0 * 6.305324077606201
Epoch 580, val loss: 0.9918429851531982
Epoch 590, training loss: 316.02374267578125 = 0.8138332366943359 + 50.0 * 6.304197788238525
Epoch 590, val loss: 0.9787790775299072
Epoch 600, training loss: 316.0313415527344 = 0.7953484654426575 + 50.0 * 6.304719924926758
Epoch 600, val loss: 0.9659016728401184
Epoch 610, training loss: 315.91796875 = 0.7772933840751648 + 50.0 * 6.302813529968262
Epoch 610, val loss: 0.953768789768219
Epoch 620, training loss: 315.7176208496094 = 0.7596247792243958 + 50.0 * 6.299160003662109
Epoch 620, val loss: 0.9419615268707275
Epoch 630, training loss: 315.8463439941406 = 0.7422956228256226 + 50.0 * 6.3020806312561035
Epoch 630, val loss: 0.9305892586708069
Epoch 640, training loss: 315.50921630859375 = 0.7252458930015564 + 50.0 * 6.295679569244385
Epoch 640, val loss: 0.9197194576263428
Epoch 650, training loss: 315.427001953125 = 0.708534300327301 + 50.0 * 6.294369220733643
Epoch 650, val loss: 0.9093576669692993
Epoch 660, training loss: 315.7599792480469 = 0.6922031044960022 + 50.0 * 6.301355838775635
Epoch 660, val loss: 0.8992598652839661
Epoch 670, training loss: 315.2452392578125 = 0.6758753657341003 + 50.0 * 6.291387557983398
Epoch 670, val loss: 0.889578640460968
Epoch 680, training loss: 315.2030334472656 = 0.6599662899971008 + 50.0 * 6.290861129760742
Epoch 680, val loss: 0.8804132342338562
Epoch 690, training loss: 315.4053039550781 = 0.644451916217804 + 50.0 * 6.295217514038086
Epoch 690, val loss: 0.8714702725410461
Epoch 700, training loss: 315.1570129394531 = 0.629026472568512 + 50.0 * 6.290559768676758
Epoch 700, val loss: 0.8631998300552368
Epoch 710, training loss: 314.92156982421875 = 0.6139414310455322 + 50.0 * 6.286152362823486
Epoch 710, val loss: 0.8549249172210693
Epoch 720, training loss: 314.86474609375 = 0.5991520881652832 + 50.0 * 6.285311698913574
Epoch 720, val loss: 0.8471312522888184
Epoch 730, training loss: 314.9071350097656 = 0.5845978260040283 + 50.0 * 6.2864508628845215
Epoch 730, val loss: 0.8395495414733887
Epoch 740, training loss: 314.7704162597656 = 0.5702246427536011 + 50.0 * 6.284004211425781
Epoch 740, val loss: 0.8323929905891418
Epoch 750, training loss: 314.6627502441406 = 0.5561447143554688 + 50.0 * 6.282132148742676
Epoch 750, val loss: 0.8255999684333801
Epoch 760, training loss: 314.59735107421875 = 0.5423073172569275 + 50.0 * 6.281101226806641
Epoch 760, val loss: 0.8192197680473328
Epoch 770, training loss: 314.5569763183594 = 0.5288154482841492 + 50.0 * 6.2805633544921875
Epoch 770, val loss: 0.8131002187728882
Epoch 780, training loss: 314.4616394042969 = 0.5156373977661133 + 50.0 * 6.2789201736450195
Epoch 780, val loss: 0.8075937628746033
Epoch 790, training loss: 314.51336669921875 = 0.5026990175247192 + 50.0 * 6.280213356018066
Epoch 790, val loss: 0.802457869052887
Epoch 800, training loss: 314.3141784667969 = 0.4901081919670105 + 50.0 * 6.2764811515808105
Epoch 800, val loss: 0.7973267436027527
Epoch 810, training loss: 314.507568359375 = 0.4776862859725952 + 50.0 * 6.280597686767578
Epoch 810, val loss: 0.7929498553276062
Epoch 820, training loss: 314.1920166015625 = 0.4655851721763611 + 50.0 * 6.274528503417969
Epoch 820, val loss: 0.7884398698806763
Epoch 830, training loss: 314.10125732421875 = 0.45372873544692993 + 50.0 * 6.272950649261475
Epoch 830, val loss: 0.7847816944122314
Epoch 840, training loss: 314.050537109375 = 0.44224631786346436 + 50.0 * 6.272165298461914
Epoch 840, val loss: 0.7812908291816711
Epoch 850, training loss: 314.37603759765625 = 0.43103674054145813 + 50.0 * 6.278900146484375
Epoch 850, val loss: 0.7779476642608643
Epoch 860, training loss: 314.07147216796875 = 0.41990870237350464 + 50.0 * 6.273030757904053
Epoch 860, val loss: 0.7752175331115723
Epoch 870, training loss: 313.9009704589844 = 0.40921324491500854 + 50.0 * 6.269835472106934
Epoch 870, val loss: 0.7726979851722717
Epoch 880, training loss: 313.8262023925781 = 0.3988478481769562 + 50.0 * 6.268547534942627
Epoch 880, val loss: 0.7705609798431396
Epoch 890, training loss: 314.0708312988281 = 0.3887418210506439 + 50.0 * 6.273642063140869
Epoch 890, val loss: 0.7686774730682373
Epoch 900, training loss: 313.8220520019531 = 0.37890008091926575 + 50.0 * 6.268863201141357
Epoch 900, val loss: 0.76725172996521
Epoch 910, training loss: 313.7008361816406 = 0.36930498480796814 + 50.0 * 6.266631126403809
Epoch 910, val loss: 0.7658937573432922
Epoch 920, training loss: 313.8930969238281 = 0.3599676489830017 + 50.0 * 6.270662784576416
Epoch 920, val loss: 0.7651124596595764
Epoch 930, training loss: 313.8330993652344 = 0.35099077224731445 + 50.0 * 6.269642353057861
Epoch 930, val loss: 0.763892650604248
Epoch 940, training loss: 313.56207275390625 = 0.3420960605144501 + 50.0 * 6.264399528503418
Epoch 940, val loss: 0.7635233402252197
Epoch 950, training loss: 313.4681701660156 = 0.3336257338523865 + 50.0 * 6.262691020965576
Epoch 950, val loss: 0.7632774114608765
Epoch 960, training loss: 313.4303283691406 = 0.3253925144672394 + 50.0 * 6.262098789215088
Epoch 960, val loss: 0.7632893919944763
Epoch 970, training loss: 313.89202880859375 = 0.3173450529575348 + 50.0 * 6.271493434906006
Epoch 970, val loss: 0.7634961605072021
Epoch 980, training loss: 313.44586181640625 = 0.30940401554107666 + 50.0 * 6.262729167938232
Epoch 980, val loss: 0.763841986656189
Epoch 990, training loss: 313.36517333984375 = 0.30177775025367737 + 50.0 * 6.26126766204834
Epoch 990, val loss: 0.7644906640052795
Epoch 1000, training loss: 313.2685852050781 = 0.294371098279953 + 50.0 * 6.25948429107666
Epoch 1000, val loss: 0.7652745842933655
Epoch 1010, training loss: 313.4176025390625 = 0.2871827781200409 + 50.0 * 6.262608528137207
Epoch 1010, val loss: 0.7663222551345825
Epoch 1020, training loss: 313.3462829589844 = 0.2801578640937805 + 50.0 * 6.261322498321533
Epoch 1020, val loss: 0.767564058303833
Epoch 1030, training loss: 313.2703552246094 = 0.27335643768310547 + 50.0 * 6.259940147399902
Epoch 1030, val loss: 0.7687082886695862
Epoch 1040, training loss: 313.1098937988281 = 0.26671871542930603 + 50.0 * 6.256863117218018
Epoch 1040, val loss: 0.7703063488006592
Epoch 1050, training loss: 313.0957946777344 = 0.2603144347667694 + 50.0 * 6.256709575653076
Epoch 1050, val loss: 0.772072434425354
Epoch 1060, training loss: 313.14678955078125 = 0.25408855080604553 + 50.0 * 6.257853984832764
Epoch 1060, val loss: 0.7739267945289612
Epoch 1070, training loss: 313.1904602050781 = 0.24804911017417908 + 50.0 * 6.258848190307617
Epoch 1070, val loss: 0.7758468985557556
Epoch 1080, training loss: 312.9858093261719 = 0.24204346537590027 + 50.0 * 6.254875659942627
Epoch 1080, val loss: 0.7780387997627258
Epoch 1090, training loss: 312.9276428222656 = 0.23629264533519745 + 50.0 * 6.25382661819458
Epoch 1090, val loss: 0.7803786396980286
Epoch 1100, training loss: 313.265869140625 = 0.230717733502388 + 50.0 * 6.260703086853027
Epoch 1100, val loss: 0.7827253937721252
Epoch 1110, training loss: 312.988037109375 = 0.2252170443534851 + 50.0 * 6.255256175994873
Epoch 1110, val loss: 0.7854663729667664
Epoch 1120, training loss: 312.8244323730469 = 0.21988381445407867 + 50.0 * 6.252090930938721
Epoch 1120, val loss: 0.7880887985229492
Epoch 1130, training loss: 312.8853759765625 = 0.214744433760643 + 50.0 * 6.25341272354126
Epoch 1130, val loss: 0.7910858392715454
Epoch 1140, training loss: 312.7679443359375 = 0.20973657071590424 + 50.0 * 6.251163959503174
Epoch 1140, val loss: 0.7939213514328003
Epoch 1150, training loss: 312.7662048339844 = 0.20489059388637543 + 50.0 * 6.25122594833374
Epoch 1150, val loss: 0.796949028968811
Epoch 1160, training loss: 312.8742980957031 = 0.2001277506351471 + 50.0 * 6.253483295440674
Epoch 1160, val loss: 0.8001465201377869
Epoch 1170, training loss: 313.04400634765625 = 0.19548708200454712 + 50.0 * 6.256969928741455
Epoch 1170, val loss: 0.8032004833221436
Epoch 1180, training loss: 312.7306823730469 = 0.19093486666679382 + 50.0 * 6.250794887542725
Epoch 1180, val loss: 0.8065434098243713
Epoch 1190, training loss: 312.56561279296875 = 0.18653659522533417 + 50.0 * 6.247581481933594
Epoch 1190, val loss: 0.8099965453147888
Epoch 1200, training loss: 312.5135192871094 = 0.18229271471500397 + 50.0 * 6.24662446975708
Epoch 1200, val loss: 0.8136020302772522
Epoch 1210, training loss: 312.5580749511719 = 0.1781664937734604 + 50.0 * 6.247598171234131
Epoch 1210, val loss: 0.8172981142997742
Epoch 1220, training loss: 312.5770263671875 = 0.17405685782432556 + 50.0 * 6.2480597496032715
Epoch 1220, val loss: 0.8207730054855347
Epoch 1230, training loss: 312.5830383300781 = 0.170047789812088 + 50.0 * 6.248260021209717
Epoch 1230, val loss: 0.8245062828063965
Epoch 1240, training loss: 312.3979797363281 = 0.16622555255889893 + 50.0 * 6.244635105133057
Epoch 1240, val loss: 0.8282468914985657
Epoch 1250, training loss: 312.3714294433594 = 0.16251038014888763 + 50.0 * 6.244178771972656
Epoch 1250, val loss: 0.8321054577827454
Epoch 1260, training loss: 312.56268310546875 = 0.15889781713485718 + 50.0 * 6.248075485229492
Epoch 1260, val loss: 0.8361178636550903
Epoch 1270, training loss: 312.3363952636719 = 0.1552969068288803 + 50.0 * 6.243621826171875
Epoch 1270, val loss: 0.8399713635444641
Epoch 1280, training loss: 312.4114685058594 = 0.15178696811199188 + 50.0 * 6.2451934814453125
Epoch 1280, val loss: 0.8440075516700745
Epoch 1290, training loss: 312.41912841796875 = 0.14840830862522125 + 50.0 * 6.2454142570495605
Epoch 1290, val loss: 0.8480644226074219
Epoch 1300, training loss: 312.27703857421875 = 0.14513170719146729 + 50.0 * 6.242637634277344
Epoch 1300, val loss: 0.852277398109436
Epoch 1310, training loss: 312.40240478515625 = 0.1419323980808258 + 50.0 * 6.245209217071533
Epoch 1310, val loss: 0.8563991189002991
Epoch 1320, training loss: 312.2763671875 = 0.1387815773487091 + 50.0 * 6.242751598358154
Epoch 1320, val loss: 0.8604864478111267
Epoch 1330, training loss: 312.19232177734375 = 0.1357278823852539 + 50.0 * 6.24113130569458
Epoch 1330, val loss: 0.8647646903991699
Epoch 1340, training loss: 312.1334533691406 = 0.13277825713157654 + 50.0 * 6.240013599395752
Epoch 1340, val loss: 0.8690224885940552
Epoch 1350, training loss: 312.1448974609375 = 0.12993328273296356 + 50.0 * 6.240299224853516
Epoch 1350, val loss: 0.8732779026031494
Epoch 1360, training loss: 312.508056640625 = 0.1271408349275589 + 50.0 * 6.247618198394775
Epoch 1360, val loss: 0.8775357007980347
Epoch 1370, training loss: 312.3066101074219 = 0.12432549893856049 + 50.0 * 6.243645668029785
Epoch 1370, val loss: 0.8817150592803955
Epoch 1380, training loss: 312.1454772949219 = 0.1216418668627739 + 50.0 * 6.240476608276367
Epoch 1380, val loss: 0.8860172629356384
Epoch 1390, training loss: 312.113037109375 = 0.1190461814403534 + 50.0 * 6.239879608154297
Epoch 1390, val loss: 0.8905307054519653
Epoch 1400, training loss: 312.09735107421875 = 0.11651363968849182 + 50.0 * 6.239616870880127
Epoch 1400, val loss: 0.8949196338653564
Epoch 1410, training loss: 311.998291015625 = 0.11403096467256546 + 50.0 * 6.237685203552246
Epoch 1410, val loss: 0.8994134068489075
Epoch 1420, training loss: 312.14801025390625 = 0.11164310574531555 + 50.0 * 6.240727424621582
Epoch 1420, val loss: 0.9037936925888062
Epoch 1430, training loss: 312.07958984375 = 0.10927363485097885 + 50.0 * 6.239406108856201
Epoch 1430, val loss: 0.9079471826553345
Epoch 1440, training loss: 311.989013671875 = 0.10695382952690125 + 50.0 * 6.237640857696533
Epoch 1440, val loss: 0.9125810861587524
Epoch 1450, training loss: 311.9189147949219 = 0.10472368448972702 + 50.0 * 6.236283302307129
Epoch 1450, val loss: 0.9170272946357727
Epoch 1460, training loss: 311.9104919433594 = 0.10255210101604462 + 50.0 * 6.236159324645996
Epoch 1460, val loss: 0.9215589165687561
Epoch 1470, training loss: 312.10662841796875 = 0.10044120997190475 + 50.0 * 6.240123748779297
Epoch 1470, val loss: 0.9258740544319153
Epoch 1480, training loss: 311.9764404296875 = 0.09829220175743103 + 50.0 * 6.237562656402588
Epoch 1480, val loss: 0.9304512143135071
Epoch 1490, training loss: 311.8843994140625 = 0.09627039730548859 + 50.0 * 6.235762596130371
Epoch 1490, val loss: 0.93483966588974
Epoch 1500, training loss: 311.8025817871094 = 0.09429723769426346 + 50.0 * 6.234165668487549
Epoch 1500, val loss: 0.9393622875213623
Epoch 1510, training loss: 311.8089599609375 = 0.09237809479236603 + 50.0 * 6.2343316078186035
Epoch 1510, val loss: 0.9437952637672424
Epoch 1520, training loss: 312.0798645019531 = 0.09050846844911575 + 50.0 * 6.2397871017456055
Epoch 1520, val loss: 0.9483575820922852
Epoch 1530, training loss: 312.05938720703125 = 0.08868499845266342 + 50.0 * 6.239414215087891
Epoch 1530, val loss: 0.9525208473205566
Epoch 1540, training loss: 311.7879333496094 = 0.08684603124856949 + 50.0 * 6.2340216636657715
Epoch 1540, val loss: 0.9571703672409058
Epoch 1550, training loss: 311.7318115234375 = 0.08510609716176987 + 50.0 * 6.23293399810791
Epoch 1550, val loss: 0.9615916013717651
Epoch 1560, training loss: 311.7213134765625 = 0.0834164023399353 + 50.0 * 6.232758045196533
Epoch 1560, val loss: 0.9661794304847717
Epoch 1570, training loss: 312.0048522949219 = 0.08178246766328812 + 50.0 * 6.238461494445801
Epoch 1570, val loss: 0.9705687165260315
Epoch 1580, training loss: 311.8433532714844 = 0.08011876046657562 + 50.0 * 6.235264778137207
Epoch 1580, val loss: 0.9747880697250366
Epoch 1590, training loss: 311.7386779785156 = 0.07852227985858917 + 50.0 * 6.233203411102295
Epoch 1590, val loss: 0.97945636510849
Epoch 1600, training loss: 311.66241455078125 = 0.07696132361888885 + 50.0 * 6.231709003448486
Epoch 1600, val loss: 0.983759343624115
Epoch 1610, training loss: 311.741455078125 = 0.07544329762458801 + 50.0 * 6.233320236206055
Epoch 1610, val loss: 0.9883087277412415
Epoch 1620, training loss: 311.6164855957031 = 0.07395166158676147 + 50.0 * 6.230850696563721
Epoch 1620, val loss: 0.9927207827568054
Epoch 1630, training loss: 311.71240234375 = 0.0725146234035492 + 50.0 * 6.232797622680664
Epoch 1630, val loss: 0.997124433517456
Epoch 1640, training loss: 311.5805358886719 = 0.07110770046710968 + 50.0 * 6.230188846588135
Epoch 1640, val loss: 1.001609444618225
Epoch 1650, training loss: 311.6787109375 = 0.06972681730985641 + 50.0 * 6.232179641723633
Epoch 1650, val loss: 1.0060157775878906
Epoch 1660, training loss: 311.6783142089844 = 0.06838717311620712 + 50.0 * 6.232198238372803
Epoch 1660, val loss: 1.0103639364242554
Epoch 1670, training loss: 311.5266418457031 = 0.06706734001636505 + 50.0 * 6.229191303253174
Epoch 1670, val loss: 1.0144845247268677
Epoch 1680, training loss: 311.4803161621094 = 0.06578131020069122 + 50.0 * 6.228290557861328
Epoch 1680, val loss: 1.0189284086227417
Epoch 1690, training loss: 311.6186218261719 = 0.06454428285360336 + 50.0 * 6.231081485748291
Epoch 1690, val loss: 1.0233001708984375
Epoch 1700, training loss: 311.4653625488281 = 0.06331881135702133 + 50.0 * 6.22804069519043
Epoch 1700, val loss: 1.0275554656982422
Epoch 1710, training loss: 311.51910400390625 = 0.06212441250681877 + 50.0 * 6.22913932800293
Epoch 1710, val loss: 1.031870722770691
Epoch 1720, training loss: 311.6529846191406 = 0.060952078551054 + 50.0 * 6.23184061050415
Epoch 1720, val loss: 1.0362133979797363
Epoch 1730, training loss: 311.43157958984375 = 0.05981401726603508 + 50.0 * 6.22743558883667
Epoch 1730, val loss: 1.0404070615768433
Epoch 1740, training loss: 311.448974609375 = 0.058695096522569656 + 50.0 * 6.2278056144714355
Epoch 1740, val loss: 1.044639229774475
Epoch 1750, training loss: 311.5914001464844 = 0.057619646191596985 + 50.0 * 6.23067569732666
Epoch 1750, val loss: 1.0490883588790894
Epoch 1760, training loss: 311.4222412109375 = 0.056547366082668304 + 50.0 * 6.227313995361328
Epoch 1760, val loss: 1.053121566772461
Epoch 1770, training loss: 311.3999328613281 = 0.055514056235551834 + 50.0 * 6.226888179779053
Epoch 1770, val loss: 1.0573140382766724
Epoch 1780, training loss: 311.4916687011719 = 0.05450762063264847 + 50.0 * 6.228743553161621
Epoch 1780, val loss: 1.0616191625595093
Epoch 1790, training loss: 311.40521240234375 = 0.053505729883909225 + 50.0 * 6.227034568786621
Epoch 1790, val loss: 1.0656803846359253
Epoch 1800, training loss: 311.5032043457031 = 0.052524227648973465 + 50.0 * 6.229013919830322
Epoch 1800, val loss: 1.0698667764663696
Epoch 1810, training loss: 311.3585205078125 = 0.051571715623140335 + 50.0 * 6.226139068603516
Epoch 1810, val loss: 1.0740026235580444
Epoch 1820, training loss: 311.29364013671875 = 0.050651803612709045 + 50.0 * 6.224859714508057
Epoch 1820, val loss: 1.07810378074646
Epoch 1830, training loss: 311.3046875 = 0.04975932091474533 + 50.0 * 6.225098133087158
Epoch 1830, val loss: 1.0822336673736572
Epoch 1840, training loss: 311.5832214355469 = 0.048891838639974594 + 50.0 * 6.230686664581299
Epoch 1840, val loss: 1.0862302780151367
Epoch 1850, training loss: 311.3877868652344 = 0.047980330884456635 + 50.0 * 6.2267961502075195
Epoch 1850, val loss: 1.0903352499008179
Epoch 1860, training loss: 311.25201416015625 = 0.04714030399918556 + 50.0 * 6.22409725189209
Epoch 1860, val loss: 1.0943418741226196
Epoch 1870, training loss: 311.2470703125 = 0.04631510376930237 + 50.0 * 6.224014759063721
Epoch 1870, val loss: 1.0984704494476318
Epoch 1880, training loss: 311.4391174316406 = 0.04551314562559128 + 50.0 * 6.227872371673584
Epoch 1880, val loss: 1.1024526357650757
Epoch 1890, training loss: 311.4852600097656 = 0.044714298099279404 + 50.0 * 6.228811264038086
Epoch 1890, val loss: 1.106318712234497
Epoch 1900, training loss: 311.228515625 = 0.04391740635037422 + 50.0 * 6.223691940307617
Epoch 1900, val loss: 1.1103144884109497
Epoch 1910, training loss: 311.1827392578125 = 0.043155405670404434 + 50.0 * 6.22279167175293
Epoch 1910, val loss: 1.1143327951431274
Epoch 1920, training loss: 311.251220703125 = 0.042428385466337204 + 50.0 * 6.224175930023193
Epoch 1920, val loss: 1.1184049844741821
Epoch 1930, training loss: 311.3091735839844 = 0.041696351021528244 + 50.0 * 6.2253499031066895
Epoch 1930, val loss: 1.1220707893371582
Epoch 1940, training loss: 311.2736511230469 = 0.040984027087688446 + 50.0 * 6.224653244018555
Epoch 1940, val loss: 1.1257296800613403
Epoch 1950, training loss: 311.20166015625 = 0.04028315097093582 + 50.0 * 6.223227500915527
Epoch 1950, val loss: 1.1298573017120361
Epoch 1960, training loss: 311.17510986328125 = 0.03961048275232315 + 50.0 * 6.222710132598877
Epoch 1960, val loss: 1.1335647106170654
Epoch 1970, training loss: 311.14947509765625 = 0.03894897922873497 + 50.0 * 6.22221040725708
Epoch 1970, val loss: 1.137490153312683
Epoch 1980, training loss: 311.25872802734375 = 0.03831034526228905 + 50.0 * 6.2244086265563965
Epoch 1980, val loss: 1.1411592960357666
Epoch 1990, training loss: 311.1812438964844 = 0.0376780666410923 + 50.0 * 6.222870826721191
Epoch 1990, val loss: 1.144911527633667
Epoch 2000, training loss: 311.2073059082031 = 0.03704001381993294 + 50.0 * 6.223404884338379
Epoch 2000, val loss: 1.148682951927185
Epoch 2010, training loss: 311.0936279296875 = 0.03642234206199646 + 50.0 * 6.221144199371338
Epoch 2010, val loss: 1.152445912361145
Epoch 2020, training loss: 311.0350646972656 = 0.035831548273563385 + 50.0 * 6.219984531402588
Epoch 2020, val loss: 1.1562248468399048
Epoch 2030, training loss: 311.239013671875 = 0.035252027213573456 + 50.0 * 6.2240753173828125
Epoch 2030, val loss: 1.1599912643432617
Epoch 2040, training loss: 311.1062927246094 = 0.03467235714197159 + 50.0 * 6.221432685852051
Epoch 2040, val loss: 1.1634769439697266
Epoch 2050, training loss: 311.0218811035156 = 0.034107279032468796 + 50.0 * 6.219755172729492
Epoch 2050, val loss: 1.1671326160430908
Epoch 2060, training loss: 310.9973449707031 = 0.03357010334730148 + 50.0 * 6.21927547454834
Epoch 2060, val loss: 1.1708104610443115
Epoch 2070, training loss: 311.0265808105469 = 0.03304345905780792 + 50.0 * 6.219870567321777
Epoch 2070, val loss: 1.1744829416275024
Epoch 2080, training loss: 311.32135009765625 = 0.03252579644322395 + 50.0 * 6.225776195526123
Epoch 2080, val loss: 1.1779050827026367
Epoch 2090, training loss: 311.18798828125 = 0.0319986529648304 + 50.0 * 6.223119258880615
Epoch 2090, val loss: 1.1817175149917603
Epoch 2100, training loss: 311.03985595703125 = 0.03149363771080971 + 50.0 * 6.22016716003418
Epoch 2100, val loss: 1.185200572013855
Epoch 2110, training loss: 310.9571228027344 = 0.03100784868001938 + 50.0 * 6.218522548675537
Epoch 2110, val loss: 1.1889222860336304
Epoch 2120, training loss: 310.93548583984375 = 0.030536308884620667 + 50.0 * 6.2180986404418945
Epoch 2120, val loss: 1.1924822330474854
Epoch 2130, training loss: 311.2714538574219 = 0.030078517273068428 + 50.0 * 6.224827289581299
Epoch 2130, val loss: 1.196246862411499
Epoch 2140, training loss: 311.1287841796875 = 0.029595179483294487 + 50.0 * 6.221983909606934
Epoch 2140, val loss: 1.1989648342132568
Epoch 2150, training loss: 310.99542236328125 = 0.029142070561647415 + 50.0 * 6.219325542449951
Epoch 2150, val loss: 1.2028864622116089
Epoch 2160, training loss: 311.01263427734375 = 0.028694959357380867 + 50.0 * 6.21967887878418
Epoch 2160, val loss: 1.2063740491867065
Epoch 2170, training loss: 311.05316162109375 = 0.0282664205878973 + 50.0 * 6.220498085021973
Epoch 2170, val loss: 1.209820032119751
Epoch 2180, training loss: 310.93316650390625 = 0.027844881638884544 + 50.0 * 6.218106746673584
Epoch 2180, val loss: 1.2131304740905762
Epoch 2190, training loss: 310.9359436035156 = 0.027432255446910858 + 50.0 * 6.218170166015625
Epoch 2190, val loss: 1.2166306972503662
Epoch 2200, training loss: 310.9452819824219 = 0.027024878188967705 + 50.0 * 6.218364715576172
Epoch 2200, val loss: 1.220156192779541
Epoch 2210, training loss: 310.9156799316406 = 0.026630273088812828 + 50.0 * 6.217780590057373
Epoch 2210, val loss: 1.2234463691711426
Epoch 2220, training loss: 311.0116271972656 = 0.02624867856502533 + 50.0 * 6.219707489013672
Epoch 2220, val loss: 1.2266722917556763
Epoch 2230, training loss: 311.0257263183594 = 0.025861242786049843 + 50.0 * 6.219997406005859
Epoch 2230, val loss: 1.2300143241882324
Epoch 2240, training loss: 310.90374755859375 = 0.025473758578300476 + 50.0 * 6.217565536499023
Epoch 2240, val loss: 1.2332162857055664
Epoch 2250, training loss: 310.85833740234375 = 0.025096524506807327 + 50.0 * 6.216665267944336
Epoch 2250, val loss: 1.2366632223129272
Epoch 2260, training loss: 310.8243103027344 = 0.024744490161538124 + 50.0 * 6.215991020202637
Epoch 2260, val loss: 1.2400377988815308
Epoch 2270, training loss: 310.88214111328125 = 0.024399397894740105 + 50.0 * 6.217154502868652
Epoch 2270, val loss: 1.2431741952896118
Epoch 2280, training loss: 310.8476257324219 = 0.02405160665512085 + 50.0 * 6.2164716720581055
Epoch 2280, val loss: 1.2464090585708618
Epoch 2290, training loss: 311.0196838378906 = 0.023714641109108925 + 50.0 * 6.219919681549072
Epoch 2290, val loss: 1.24959397315979
Epoch 2300, training loss: 310.91131591796875 = 0.023377951234579086 + 50.0 * 6.217758655548096
Epoch 2300, val loss: 1.2530274391174316
Epoch 2310, training loss: 310.81317138671875 = 0.023043330758810043 + 50.0 * 6.2158026695251465
Epoch 2310, val loss: 1.2560359239578247
Epoch 2320, training loss: 310.78204345703125 = 0.022725384682416916 + 50.0 * 6.21518611907959
Epoch 2320, val loss: 1.2595274448394775
Epoch 2330, training loss: 310.8608093261719 = 0.022419413551688194 + 50.0 * 6.21676778793335
Epoch 2330, val loss: 1.262630581855774
Epoch 2340, training loss: 310.7659912109375 = 0.02210419811308384 + 50.0 * 6.214878082275391
Epoch 2340, val loss: 1.2657166719436646
Epoch 2350, training loss: 310.8350524902344 = 0.02180381491780281 + 50.0 * 6.2162652015686035
Epoch 2350, val loss: 1.2689054012298584
Epoch 2360, training loss: 310.88067626953125 = 0.02150566130876541 + 50.0 * 6.2171831130981445
Epoch 2360, val loss: 1.271986722946167
Epoch 2370, training loss: 310.8898620605469 = 0.021209536120295525 + 50.0 * 6.217372894287109
Epoch 2370, val loss: 1.2751976251602173
Epoch 2380, training loss: 310.7804870605469 = 0.02091769501566887 + 50.0 * 6.215191841125488
Epoch 2380, val loss: 1.2780662775039673
Epoch 2390, training loss: 310.7503356933594 = 0.020636040717363358 + 50.0 * 6.214593887329102
Epoch 2390, val loss: 1.281276822090149
Epoch 2400, training loss: 310.7859191894531 = 0.020364265888929367 + 50.0 * 6.215311050415039
Epoch 2400, val loss: 1.2842603921890259
Epoch 2410, training loss: 310.66949462890625 = 0.02009393461048603 + 50.0 * 6.212988376617432
Epoch 2410, val loss: 1.2874329090118408
Epoch 2420, training loss: 310.7277526855469 = 0.019834410399198532 + 50.0 * 6.214158535003662
Epoch 2420, val loss: 1.2904787063598633
Epoch 2430, training loss: 310.8543701171875 = 0.01957705244421959 + 50.0 * 6.216695785522461
Epoch 2430, val loss: 1.2934688329696655
Epoch 2440, training loss: 310.8262023925781 = 0.019319718703627586 + 50.0 * 6.216137409210205
Epoch 2440, val loss: 1.2962363958358765
Epoch 2450, training loss: 310.70904541015625 = 0.01906738057732582 + 50.0 * 6.213799476623535
Epoch 2450, val loss: 1.2990989685058594
Epoch 2460, training loss: 310.6802978515625 = 0.018822357058525085 + 50.0 * 6.213229656219482
Epoch 2460, val loss: 1.3022335767745972
Epoch 2470, training loss: 310.95513916015625 = 0.018590610474348068 + 50.0 * 6.218730926513672
Epoch 2470, val loss: 1.3049635887145996
Epoch 2480, training loss: 310.7205810546875 = 0.018336595967411995 + 50.0 * 6.21404504776001
Epoch 2480, val loss: 1.3082573413848877
Epoch 2490, training loss: 310.64654541015625 = 0.018104998394846916 + 50.0 * 6.212568759918213
Epoch 2490, val loss: 1.3109710216522217
Epoch 2500, training loss: 310.595458984375 = 0.017882561311125755 + 50.0 * 6.211551666259766
Epoch 2500, val loss: 1.314099669456482
Epoch 2510, training loss: 310.57440185546875 = 0.01766710914671421 + 50.0 * 6.211134910583496
Epoch 2510, val loss: 1.3170171976089478
Epoch 2520, training loss: 310.924072265625 = 0.01746539957821369 + 50.0 * 6.218132495880127
Epoch 2520, val loss: 1.3198304176330566
Epoch 2530, training loss: 310.7155456542969 = 0.01723107136785984 + 50.0 * 6.213966369628906
Epoch 2530, val loss: 1.3222699165344238
Epoch 2540, training loss: 310.690185546875 = 0.017013927921652794 + 50.0 * 6.213463306427002
Epoch 2540, val loss: 1.3252547979354858
Epoch 2550, training loss: 310.58612060546875 = 0.016803449019789696 + 50.0 * 6.211386203765869
Epoch 2550, val loss: 1.328213095664978
Epoch 2560, training loss: 310.6295471191406 = 0.016605114564299583 + 50.0 * 6.212258815765381
Epoch 2560, val loss: 1.3310472965240479
Epoch 2570, training loss: 310.7760009765625 = 0.01640820875763893 + 50.0 * 6.215191841125488
Epoch 2570, val loss: 1.333853840827942
Epoch 2580, training loss: 310.743408203125 = 0.016216732561588287 + 50.0 * 6.21454381942749
Epoch 2580, val loss: 1.3363287448883057
Epoch 2590, training loss: 310.5885925292969 = 0.016009612008929253 + 50.0 * 6.211451530456543
Epoch 2590, val loss: 1.3392513990402222
Epoch 2600, training loss: 310.6711120605469 = 0.01582738570868969 + 50.0 * 6.213105201721191
Epoch 2600, val loss: 1.3421995639801025
Epoch 2610, training loss: 310.50384521484375 = 0.01563299633562565 + 50.0 * 6.20976448059082
Epoch 2610, val loss: 1.3448967933654785
Epoch 2620, training loss: 310.57659912109375 = 0.015453331172466278 + 50.0 * 6.211223125457764
Epoch 2620, val loss: 1.347680926322937
Epoch 2630, training loss: 310.7980651855469 = 0.01528241764754057 + 50.0 * 6.21565580368042
Epoch 2630, val loss: 1.3503395318984985
Epoch 2640, training loss: 310.6235046386719 = 0.015093783847987652 + 50.0 * 6.212167739868164
Epoch 2640, val loss: 1.3531887531280518
Epoch 2650, training loss: 310.5546569824219 = 0.014917655847966671 + 50.0 * 6.210794448852539
Epoch 2650, val loss: 1.355866551399231
Epoch 2660, training loss: 310.6346435546875 = 0.014751358889043331 + 50.0 * 6.212398052215576
Epoch 2660, val loss: 1.3589980602264404
Epoch 2670, training loss: 310.5524597167969 = 0.014580603688955307 + 50.0 * 6.210757255554199
Epoch 2670, val loss: 1.3611423969268799
Epoch 2680, training loss: 310.56121826171875 = 0.014414472505450249 + 50.0 * 6.210936069488525
Epoch 2680, val loss: 1.363547444343567
Epoch 2690, training loss: 310.64666748046875 = 0.014256206341087818 + 50.0 * 6.212648391723633
Epoch 2690, val loss: 1.3665895462036133
Epoch 2700, training loss: 310.48211669921875 = 0.014091623947024345 + 50.0 * 6.209360122680664
Epoch 2700, val loss: 1.3689426183700562
Epoch 2710, training loss: 310.517822265625 = 0.01393523532897234 + 50.0 * 6.21007776260376
Epoch 2710, val loss: 1.3717176914215088
Epoch 2720, training loss: 310.5943908691406 = 0.01378454826772213 + 50.0 * 6.211612224578857
Epoch 2720, val loss: 1.3741377592086792
Epoch 2730, training loss: 310.5352783203125 = 0.013636020943522453 + 50.0 * 6.210432529449463
Epoch 2730, val loss: 1.376524806022644
Epoch 2740, training loss: 310.4808044433594 = 0.013484171591699123 + 50.0 * 6.209346294403076
Epoch 2740, val loss: 1.3794069290161133
Epoch 2750, training loss: 310.5775146484375 = 0.013339897617697716 + 50.0 * 6.2112836837768555
Epoch 2750, val loss: 1.381675362586975
Epoch 2760, training loss: 310.4382629394531 = 0.013191470876336098 + 50.0 * 6.208501815795898
Epoch 2760, val loss: 1.3841251134872437
Epoch 2770, training loss: 310.4563293457031 = 0.01305024977773428 + 50.0 * 6.208865165710449
Epoch 2770, val loss: 1.3867329359054565
Epoch 2780, training loss: 310.82208251953125 = 0.012914607301354408 + 50.0 * 6.216183662414551
Epoch 2780, val loss: 1.3891745805740356
Epoch 2790, training loss: 310.5009460449219 = 0.01277107372879982 + 50.0 * 6.209764003753662
Epoch 2790, val loss: 1.3914393186569214
Epoch 2800, training loss: 310.4176025390625 = 0.012635917402803898 + 50.0 * 6.208099365234375
Epoch 2800, val loss: 1.3940060138702393
Epoch 2810, training loss: 310.65753173828125 = 0.012508356012403965 + 50.0 * 6.212900638580322
Epoch 2810, val loss: 1.396489143371582
Epoch 2820, training loss: 310.4280700683594 = 0.012369291856884956 + 50.0 * 6.208314418792725
Epoch 2820, val loss: 1.3987317085266113
Epoch 2830, training loss: 310.3889465332031 = 0.012235802598297596 + 50.0 * 6.207534313201904
Epoch 2830, val loss: 1.4013903141021729
Epoch 2840, training loss: 310.33660888671875 = 0.01211321260780096 + 50.0 * 6.2064900398254395
Epoch 2840, val loss: 1.4037762880325317
Epoch 2850, training loss: 310.3487854003906 = 0.011994127184152603 + 50.0 * 6.206735610961914
Epoch 2850, val loss: 1.4062052965164185
Epoch 2860, training loss: 310.7743835449219 = 0.011879781261086464 + 50.0 * 6.215250492095947
Epoch 2860, val loss: 1.4084975719451904
Epoch 2870, training loss: 310.58428955078125 = 0.011756740510463715 + 50.0 * 6.211451053619385
Epoch 2870, val loss: 1.410517930984497
Epoch 2880, training loss: 310.4054260253906 = 0.011631825938820839 + 50.0 * 6.207876205444336
Epoch 2880, val loss: 1.4128758907318115
Epoch 2890, training loss: 310.3721618652344 = 0.011515566147863865 + 50.0 * 6.207213401794434
Epoch 2890, val loss: 1.4153562784194946
Epoch 2900, training loss: 310.4226379394531 = 0.011407747864723206 + 50.0 * 6.208224296569824
Epoch 2900, val loss: 1.417751431465149
Epoch 2910, training loss: 310.4131164550781 = 0.011295984499156475 + 50.0 * 6.208036422729492
Epoch 2910, val loss: 1.4197742938995361
Epoch 2920, training loss: 310.4595642089844 = 0.011183423921465874 + 50.0 * 6.208967685699463
Epoch 2920, val loss: 1.4221765995025635
Epoch 2930, training loss: 310.5007629394531 = 0.011069156229496002 + 50.0 * 6.209793567657471
Epoch 2930, val loss: 1.4245344400405884
Epoch 2940, training loss: 310.3927917480469 = 0.010958811268210411 + 50.0 * 6.207636833190918
Epoch 2940, val loss: 1.4263266324996948
Epoch 2950, training loss: 310.3215026855469 = 0.010850775986909866 + 50.0 * 6.206212997436523
Epoch 2950, val loss: 1.4286123514175415
Epoch 2960, training loss: 310.27886962890625 = 0.01075204461812973 + 50.0 * 6.205362319946289
Epoch 2960, val loss: 1.4309360980987549
Epoch 2970, training loss: 310.4527893066406 = 0.010658147744834423 + 50.0 * 6.208842754364014
Epoch 2970, val loss: 1.4328747987747192
Epoch 2980, training loss: 310.3425598144531 = 0.010549699887633324 + 50.0 * 6.206640243530273
Epoch 2980, val loss: 1.435129165649414
Epoch 2990, training loss: 310.294921875 = 0.010445566847920418 + 50.0 * 6.205689430236816
Epoch 2990, val loss: 1.4371918439865112
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 431.79986572265625 = 1.957550048828125 + 50.0 * 8.596846580505371
Epoch 0, val loss: 1.9557099342346191
Epoch 10, training loss: 431.754638671875 = 1.9487327337265015 + 50.0 * 8.596117973327637
Epoch 10, val loss: 1.9469236135482788
Epoch 20, training loss: 431.48052978515625 = 1.9375699758529663 + 50.0 * 8.590859413146973
Epoch 20, val loss: 1.9359126091003418
Epoch 30, training loss: 429.68560791015625 = 1.923100471496582 + 50.0 * 8.55525016784668
Epoch 30, val loss: 1.921525239944458
Epoch 40, training loss: 419.1575622558594 = 1.906402349472046 + 50.0 * 8.345023155212402
Epoch 40, val loss: 1.905333399772644
Epoch 50, training loss: 386.6982727050781 = 1.888782024383545 + 50.0 * 7.6961894035339355
Epoch 50, val loss: 1.8883569240570068
Epoch 60, training loss: 369.0611267089844 = 1.8736776113510132 + 50.0 * 7.343748569488525
Epoch 60, val loss: 1.8737947940826416
Epoch 70, training loss: 355.8728942871094 = 1.8597766160964966 + 50.0 * 7.080262660980225
Epoch 70, val loss: 1.8602508306503296
Epoch 80, training loss: 347.8626708984375 = 1.8470112085342407 + 50.0 * 6.920312881469727
Epoch 80, val loss: 1.8480162620544434
Epoch 90, training loss: 342.6224670410156 = 1.835364818572998 + 50.0 * 6.815742492675781
Epoch 90, val loss: 1.8370589017868042
Epoch 100, training loss: 338.85394287109375 = 1.8246146440505981 + 50.0 * 6.740586280822754
Epoch 100, val loss: 1.827077865600586
Epoch 110, training loss: 336.1696472167969 = 1.8142770528793335 + 50.0 * 6.687107563018799
Epoch 110, val loss: 1.8176405429840088
Epoch 120, training loss: 334.2546691894531 = 1.8045074939727783 + 50.0 * 6.649003028869629
Epoch 120, val loss: 1.8087469339370728
Epoch 130, training loss: 332.6164855957031 = 1.7954044342041016 + 50.0 * 6.616421222686768
Epoch 130, val loss: 1.800302267074585
Epoch 140, training loss: 331.38250732421875 = 1.786689043045044 + 50.0 * 6.591916561126709
Epoch 140, val loss: 1.7920582294464111
Epoch 150, training loss: 330.12347412109375 = 1.7780064344406128 + 50.0 * 6.566909313201904
Epoch 150, val loss: 1.783828854560852
Epoch 160, training loss: 329.08343505859375 = 1.769345998764038 + 50.0 * 6.546281814575195
Epoch 160, val loss: 1.7755591869354248
Epoch 170, training loss: 328.19891357421875 = 1.7603137493133545 + 50.0 * 6.528771877288818
Epoch 170, val loss: 1.7670990228652954
Epoch 180, training loss: 327.3357849121094 = 1.7507801055908203 + 50.0 * 6.511699676513672
Epoch 180, val loss: 1.758444905281067
Epoch 190, training loss: 326.726806640625 = 1.7406744956970215 + 50.0 * 6.499722957611084
Epoch 190, val loss: 1.749509334564209
Epoch 200, training loss: 325.85400390625 = 1.7298028469085693 + 50.0 * 6.482483863830566
Epoch 200, val loss: 1.7399814128875732
Epoch 210, training loss: 325.1526184082031 = 1.7182409763336182 + 50.0 * 6.468688011169434
Epoch 210, val loss: 1.7299946546554565
Epoch 220, training loss: 324.4817199707031 = 1.705844521522522 + 50.0 * 6.455517292022705
Epoch 220, val loss: 1.7193721532821655
Epoch 230, training loss: 324.4248046875 = 1.6924235820770264 + 50.0 * 6.454648017883301
Epoch 230, val loss: 1.707902193069458
Epoch 240, training loss: 323.42572021484375 = 1.6777058839797974 + 50.0 * 6.43496036529541
Epoch 240, val loss: 1.6955087184906006
Epoch 250, training loss: 322.9159240722656 = 1.6620298624038696 + 50.0 * 6.425078392028809
Epoch 250, val loss: 1.6821998357772827
Epoch 260, training loss: 322.4406433105469 = 1.6451125144958496 + 50.0 * 6.415910720825195
Epoch 260, val loss: 1.6679730415344238
Epoch 270, training loss: 322.08294677734375 = 1.626987338066101 + 50.0 * 6.409119129180908
Epoch 270, val loss: 1.6527355909347534
Epoch 280, training loss: 321.91815185546875 = 1.6075515747070312 + 50.0 * 6.406211853027344
Epoch 280, val loss: 1.6364002227783203
Epoch 290, training loss: 321.31610107421875 = 1.5867950916290283 + 50.0 * 6.394586086273193
Epoch 290, val loss: 1.6190423965454102
Epoch 300, training loss: 320.9635925292969 = 1.5650346279144287 + 50.0 * 6.387970924377441
Epoch 300, val loss: 1.6008671522140503
Epoch 310, training loss: 320.60400390625 = 1.5421724319458008 + 50.0 * 6.381236553192139
Epoch 310, val loss: 1.5818201303482056
Epoch 320, training loss: 320.6820068359375 = 1.5183659791946411 + 50.0 * 6.383272647857666
Epoch 320, val loss: 1.5621432065963745
Epoch 330, training loss: 320.2296447753906 = 1.493560791015625 + 50.0 * 6.374721527099609
Epoch 330, val loss: 1.5416756868362427
Epoch 340, training loss: 319.75445556640625 = 1.4681748151779175 + 50.0 * 6.365725994110107
Epoch 340, val loss: 1.5209016799926758
Epoch 350, training loss: 319.50286865234375 = 1.4422165155410767 + 50.0 * 6.361212730407715
Epoch 350, val loss: 1.4998278617858887
Epoch 360, training loss: 319.2384338378906 = 1.4159131050109863 + 50.0 * 6.356450080871582
Epoch 360, val loss: 1.4786481857299805
Epoch 370, training loss: 319.0160217285156 = 1.3891974687576294 + 50.0 * 6.352536678314209
Epoch 370, val loss: 1.4573187828063965
Epoch 380, training loss: 318.9722900390625 = 1.3620682954788208 + 50.0 * 6.352204322814941
Epoch 380, val loss: 1.4358683824539185
Epoch 390, training loss: 318.6339111328125 = 1.3348331451416016 + 50.0 * 6.345981597900391
Epoch 390, val loss: 1.4144822359085083
Epoch 400, training loss: 318.3952331542969 = 1.3076084852218628 + 50.0 * 6.341752529144287
Epoch 400, val loss: 1.3933099508285522
Epoch 410, training loss: 318.387451171875 = 1.280509114265442 + 50.0 * 6.342138767242432
Epoch 410, val loss: 1.3723266124725342
Epoch 420, training loss: 318.0462951660156 = 1.2533533573150635 + 50.0 * 6.3358588218688965
Epoch 420, val loss: 1.3516669273376465
Epoch 430, training loss: 317.80224609375 = 1.226565957069397 + 50.0 * 6.331513404846191
Epoch 430, val loss: 1.3313393592834473
Epoch 440, training loss: 317.6239318847656 = 1.200217366218567 + 50.0 * 6.328474044799805
Epoch 440, val loss: 1.311561942100525
Epoch 450, training loss: 318.1115417480469 = 1.1743342876434326 + 50.0 * 6.338743686676025
Epoch 450, val loss: 1.2922303676605225
Epoch 460, training loss: 317.3255920410156 = 1.148341417312622 + 50.0 * 6.323544979095459
Epoch 460, val loss: 1.2731006145477295
Epoch 470, training loss: 317.15447998046875 = 1.1230751276016235 + 50.0 * 6.3206281661987305
Epoch 470, val loss: 1.2547016143798828
Epoch 480, training loss: 317.0099792480469 = 1.0985205173492432 + 50.0 * 6.3182291984558105
Epoch 480, val loss: 1.2370827198028564
Epoch 490, training loss: 316.9052429199219 = 1.0745913982391357 + 50.0 * 6.31661319732666
Epoch 490, val loss: 1.2200695276260376
Epoch 500, training loss: 316.827392578125 = 1.0509506464004517 + 50.0 * 6.315528869628906
Epoch 500, val loss: 1.2033013105392456
Epoch 510, training loss: 316.6053466796875 = 1.0280402898788452 + 50.0 * 6.3115458488464355
Epoch 510, val loss: 1.1874160766601562
Epoch 520, training loss: 316.4737548828125 = 1.0057826042175293 + 50.0 * 6.309359550476074
Epoch 520, val loss: 1.1722493171691895
Epoch 530, training loss: 316.4516296386719 = 0.9842833876609802 + 50.0 * 6.309346675872803
Epoch 530, val loss: 1.157844066619873
Epoch 540, training loss: 316.655517578125 = 0.9631056785583496 + 50.0 * 6.313848495483398
Epoch 540, val loss: 1.1440211534500122
Epoch 550, training loss: 316.2430725097656 = 0.9425607323646545 + 50.0 * 6.3060102462768555
Epoch 550, val loss: 1.1304343938827515
Epoch 560, training loss: 316.0782775878906 = 0.9227480888366699 + 50.0 * 6.303110599517822
Epoch 560, val loss: 1.117737889289856
Epoch 570, training loss: 315.9444580078125 = 0.9036036729812622 + 50.0 * 6.300817012786865
Epoch 570, val loss: 1.1058294773101807
Epoch 580, training loss: 315.901123046875 = 0.8849982619285583 + 50.0 * 6.300322532653809
Epoch 580, val loss: 1.0945546627044678
Epoch 590, training loss: 315.8571472167969 = 0.8667711615562439 + 50.0 * 6.299808025360107
Epoch 590, val loss: 1.0833818912506104
Epoch 600, training loss: 315.7762756347656 = 0.848970890045166 + 50.0 * 6.298545837402344
Epoch 600, val loss: 1.0730654001235962
Epoch 610, training loss: 315.6263427734375 = 0.831780731678009 + 50.0 * 6.295891284942627
Epoch 610, val loss: 1.0632195472717285
Epoch 620, training loss: 315.4624328613281 = 0.8150987029075623 + 50.0 * 6.292946815490723
Epoch 620, val loss: 1.0538829565048218
Epoch 630, training loss: 315.5076904296875 = 0.7988397479057312 + 50.0 * 6.294177532196045
Epoch 630, val loss: 1.0450186729431152
Epoch 640, training loss: 315.4148864746094 = 0.7828336954116821 + 50.0 * 6.2926411628723145
Epoch 640, val loss: 1.0365686416625977
Epoch 650, training loss: 315.3150329589844 = 0.7671424150466919 + 50.0 * 6.290957927703857
Epoch 650, val loss: 1.0284096002578735
Epoch 660, training loss: 315.1470031738281 = 0.7519854307174683 + 50.0 * 6.287900447845459
Epoch 660, val loss: 1.0207737684249878
Epoch 670, training loss: 315.1154479980469 = 0.7371755838394165 + 50.0 * 6.287565231323242
Epoch 670, val loss: 1.0136700868606567
Epoch 680, training loss: 315.02069091796875 = 0.7224944233894348 + 50.0 * 6.285964012145996
Epoch 680, val loss: 1.0068751573562622
Epoch 690, training loss: 315.128662109375 = 0.708062469959259 + 50.0 * 6.288411617279053
Epoch 690, val loss: 1.0000979900360107
Epoch 700, training loss: 314.97772216796875 = 0.6939329504966736 + 50.0 * 6.285675525665283
Epoch 700, val loss: 0.9937348365783691
Epoch 710, training loss: 314.83056640625 = 0.6800316572189331 + 50.0 * 6.283010959625244
Epoch 710, val loss: 0.9878711700439453
Epoch 720, training loss: 314.6997985839844 = 0.6664555072784424 + 50.0 * 6.280667304992676
Epoch 720, val loss: 0.9824140667915344
Epoch 730, training loss: 314.6506042480469 = 0.6531152725219727 + 50.0 * 6.279950141906738
Epoch 730, val loss: 0.9770471453666687
Epoch 740, training loss: 314.7022399902344 = 0.6399532556533813 + 50.0 * 6.281245708465576
Epoch 740, val loss: 0.9721485376358032
Epoch 750, training loss: 315.38116455078125 = 0.6269462704658508 + 50.0 * 6.295083999633789
Epoch 750, val loss: 0.967455267906189
Epoch 760, training loss: 314.55352783203125 = 0.6137064099311829 + 50.0 * 6.278796195983887
Epoch 760, val loss: 0.9626457095146179
Epoch 770, training loss: 314.3645324707031 = 0.6008981466293335 + 50.0 * 6.275272846221924
Epoch 770, val loss: 0.9580577611923218
Epoch 780, training loss: 314.2953796386719 = 0.5884461402893066 + 50.0 * 6.274138927459717
Epoch 780, val loss: 0.9539811611175537
Epoch 790, training loss: 314.2685546875 = 0.576191246509552 + 50.0 * 6.2738471031188965
Epoch 790, val loss: 0.950187623500824
Epoch 800, training loss: 314.3379821777344 = 0.5639424324035645 + 50.0 * 6.275481224060059
Epoch 800, val loss: 0.9463784694671631
Epoch 810, training loss: 314.1811218261719 = 0.5517318248748779 + 50.0 * 6.272587776184082
Epoch 810, val loss: 0.9428995251655579
Epoch 820, training loss: 314.0816650390625 = 0.5397549271583557 + 50.0 * 6.270837783813477
Epoch 820, val loss: 0.9396629929542542
Epoch 830, training loss: 314.0989685058594 = 0.5279861688613892 + 50.0 * 6.271419525146484
Epoch 830, val loss: 0.9366514682769775
Epoch 840, training loss: 313.9483337402344 = 0.5162978172302246 + 50.0 * 6.268640995025635
Epoch 840, val loss: 0.9337750673294067
Epoch 850, training loss: 313.9999084472656 = 0.5047004222869873 + 50.0 * 6.269904136657715
Epoch 850, val loss: 0.9312387108802795
Epoch 860, training loss: 313.8867492675781 = 0.4932609796524048 + 50.0 * 6.26786994934082
Epoch 860, val loss: 0.9284284710884094
Epoch 870, training loss: 313.83026123046875 = 0.4820125699043274 + 50.0 * 6.266964435577393
Epoch 870, val loss: 0.9262945652008057
Epoch 880, training loss: 314.1434020996094 = 0.4709351360797882 + 50.0 * 6.273448944091797
Epoch 880, val loss: 0.9242166876792908
Epoch 890, training loss: 313.8096008300781 = 0.4598073363304138 + 50.0 * 6.266995906829834
Epoch 890, val loss: 0.922056257724762
Epoch 900, training loss: 313.6031188964844 = 0.4489632844924927 + 50.0 * 6.263082981109619
Epoch 900, val loss: 0.9203664660453796
Epoch 910, training loss: 313.57763671875 = 0.43834611773490906 + 50.0 * 6.262785911560059
Epoch 910, val loss: 0.9189746975898743
Epoch 920, training loss: 313.62432861328125 = 0.4278414249420166 + 50.0 * 6.26392936706543
Epoch 920, val loss: 0.9176508188247681
Epoch 930, training loss: 313.7751159667969 = 0.4172947406768799 + 50.0 * 6.26715612411499
Epoch 930, val loss: 0.9162222743034363
Epoch 940, training loss: 313.478515625 = 0.4069153070449829 + 50.0 * 6.261431694030762
Epoch 940, val loss: 0.9151684641838074
Epoch 950, training loss: 313.4165954589844 = 0.3968850374221802 + 50.0 * 6.26039457321167
Epoch 950, val loss: 0.9144673943519592
Epoch 960, training loss: 313.3088073730469 = 0.38707786798477173 + 50.0 * 6.258434295654297
Epoch 960, val loss: 0.9140918850898743
Epoch 970, training loss: 313.2648620605469 = 0.3774649202823639 + 50.0 * 6.257747650146484
Epoch 970, val loss: 0.9139472246170044
Epoch 980, training loss: 313.8478088378906 = 0.3679219186306 + 50.0 * 6.26959753036499
Epoch 980, val loss: 0.9138607382774353
Epoch 990, training loss: 313.2165222167969 = 0.3584497272968292 + 50.0 * 6.2571611404418945
Epoch 990, val loss: 0.9135048985481262
Epoch 1000, training loss: 313.1822814941406 = 0.34926167130470276 + 50.0 * 6.256659984588623
Epoch 1000, val loss: 0.9137043356895447
Epoch 1010, training loss: 313.1284484863281 = 0.3403310179710388 + 50.0 * 6.255762577056885
Epoch 1010, val loss: 0.9142010807991028
Epoch 1020, training loss: 313.2850036621094 = 0.33155226707458496 + 50.0 * 6.259068965911865
Epoch 1020, val loss: 0.914817214012146
Epoch 1030, training loss: 313.05078125 = 0.32297977805137634 + 50.0 * 6.254556179046631
Epoch 1030, val loss: 0.915773332118988
Epoch 1040, training loss: 312.9783630371094 = 0.31457439064979553 + 50.0 * 6.2532758712768555
Epoch 1040, val loss: 0.9165796041488647
Epoch 1050, training loss: 313.1832580566406 = 0.30633193254470825 + 50.0 * 6.257538318634033
Epoch 1050, val loss: 0.9175887703895569
Epoch 1060, training loss: 312.9117126464844 = 0.2982816994190216 + 50.0 * 6.2522687911987305
Epoch 1060, val loss: 0.9190003275871277
Epoch 1070, training loss: 312.89886474609375 = 0.2904599905014038 + 50.0 * 6.252167701721191
Epoch 1070, val loss: 0.920689582824707
Epoch 1080, training loss: 313.1378173828125 = 0.28279030323028564 + 50.0 * 6.257100582122803
Epoch 1080, val loss: 0.9221064448356628
Epoch 1090, training loss: 312.88714599609375 = 0.27540484070777893 + 50.0 * 6.252234935760498
Epoch 1090, val loss: 0.923844039440155
Epoch 1100, training loss: 312.7584228515625 = 0.2680929899215698 + 50.0 * 6.2498064041137695
Epoch 1100, val loss: 0.925855278968811
Epoch 1110, training loss: 312.7275390625 = 0.2610839307308197 + 50.0 * 6.249329090118408
Epoch 1110, val loss: 0.9279729723930359
Epoch 1120, training loss: 312.8664245605469 = 0.2542644143104553 + 50.0 * 6.2522430419921875
Epoch 1120, val loss: 0.9302749633789062
Epoch 1130, training loss: 312.78765869140625 = 0.24753297865390778 + 50.0 * 6.250802516937256
Epoch 1130, val loss: 0.9323022365570068
Epoch 1140, training loss: 312.6326904296875 = 0.2409670203924179 + 50.0 * 6.247834205627441
Epoch 1140, val loss: 0.9347530603408813
Epoch 1150, training loss: 312.5806579589844 = 0.23463782668113708 + 50.0 * 6.246920108795166
Epoch 1150, val loss: 0.9373133182525635
Epoch 1160, training loss: 312.6446533203125 = 0.2285156399011612 + 50.0 * 6.248322486877441
Epoch 1160, val loss: 0.9400873780250549
Epoch 1170, training loss: 312.6514587402344 = 0.22251562774181366 + 50.0 * 6.248579025268555
Epoch 1170, val loss: 0.9427118897438049
Epoch 1180, training loss: 312.7010803222656 = 0.21673713624477386 + 50.0 * 6.2496867179870605
Epoch 1180, val loss: 0.9454544186592102
Epoch 1190, training loss: 312.4936828613281 = 0.21104124188423157 + 50.0 * 6.24565315246582
Epoch 1190, val loss: 0.9484569430351257
Epoch 1200, training loss: 312.4898376464844 = 0.205617293715477 + 50.0 * 6.2456841468811035
Epoch 1200, val loss: 0.9515284895896912
Epoch 1210, training loss: 312.39727783203125 = 0.2002364695072174 + 50.0 * 6.243941307067871
Epoch 1210, val loss: 0.9546017050743103
Epoch 1220, training loss: 312.3585510253906 = 0.19506226480007172 + 50.0 * 6.243269443511963
Epoch 1220, val loss: 0.9576727747917175
Epoch 1230, training loss: 312.4117736816406 = 0.19004422426223755 + 50.0 * 6.244434833526611
Epoch 1230, val loss: 0.9610278010368347
Epoch 1240, training loss: 312.4035339355469 = 0.18516775965690613 + 50.0 * 6.2443671226501465
Epoch 1240, val loss: 0.964203417301178
Epoch 1250, training loss: 312.34625244140625 = 0.1804559826850891 + 50.0 * 6.243316173553467
Epoch 1250, val loss: 0.9674974083900452
Epoch 1260, training loss: 312.3291931152344 = 0.17585720121860504 + 50.0 * 6.243067264556885
Epoch 1260, val loss: 0.97087162733078
Epoch 1270, training loss: 312.20361328125 = 0.17141616344451904 + 50.0 * 6.2406439781188965
Epoch 1270, val loss: 0.974479079246521
Epoch 1280, training loss: 312.3576354980469 = 0.167128324508667 + 50.0 * 6.243810176849365
Epoch 1280, val loss: 0.9779552221298218
Epoch 1290, training loss: 312.3485412597656 = 0.16286076605319977 + 50.0 * 6.24371337890625
Epoch 1290, val loss: 0.9811697602272034
Epoch 1300, training loss: 312.15570068359375 = 0.15875400602817535 + 50.0 * 6.239939212799072
Epoch 1300, val loss: 0.9848101735115051
Epoch 1310, training loss: 312.0901794433594 = 0.15477091073989868 + 50.0 * 6.238708019256592
Epoch 1310, val loss: 0.9884678721427917
Epoch 1320, training loss: 312.130859375 = 0.15093445777893066 + 50.0 * 6.239598274230957
Epoch 1320, val loss: 0.9921625256538391
Epoch 1330, training loss: 312.3030090332031 = 0.1471719592809677 + 50.0 * 6.24311637878418
Epoch 1330, val loss: 0.9956141710281372
Epoch 1340, training loss: 312.12811279296875 = 0.14353932440280914 + 50.0 * 6.239691734313965
Epoch 1340, val loss: 0.9992280006408691
Epoch 1350, training loss: 312.0332946777344 = 0.13996943831443787 + 50.0 * 6.237865924835205
Epoch 1350, val loss: 1.0028760433197021
Epoch 1360, training loss: 312.00201416015625 = 0.13656125962734222 + 50.0 * 6.237308979034424
Epoch 1360, val loss: 1.0068013668060303
Epoch 1370, training loss: 312.0748291015625 = 0.1332521289587021 + 50.0 * 6.238831996917725
Epoch 1370, val loss: 1.0105189085006714
Epoch 1380, training loss: 311.9434509277344 = 0.13001982867717743 + 50.0 * 6.236268997192383
Epoch 1380, val loss: 1.0144507884979248
Epoch 1390, training loss: 312.2087707519531 = 0.12689721584320068 + 50.0 * 6.241637706756592
Epoch 1390, val loss: 1.0183478593826294
Epoch 1400, training loss: 311.9303283691406 = 0.12379872798919678 + 50.0 * 6.236130237579346
Epoch 1400, val loss: 1.0218133926391602
Epoch 1410, training loss: 311.95037841796875 = 0.12085259705781937 + 50.0 * 6.236590385437012
Epoch 1410, val loss: 1.0258516073226929
Epoch 1420, training loss: 311.937744140625 = 0.11795688420534134 + 50.0 * 6.236395835876465
Epoch 1420, val loss: 1.0296173095703125
Epoch 1430, training loss: 311.8275451660156 = 0.11511445045471191 + 50.0 * 6.234248638153076
Epoch 1430, val loss: 1.0336822271347046
Epoch 1440, training loss: 311.7838134765625 = 0.11241012811660767 + 50.0 * 6.233428478240967
Epoch 1440, val loss: 1.037733793258667
Epoch 1450, training loss: 312.1072998046875 = 0.10979824513196945 + 50.0 * 6.239950180053711
Epoch 1450, val loss: 1.0415544509887695
Epoch 1460, training loss: 311.8912658691406 = 0.10719963908195496 + 50.0 * 6.235681056976318
Epoch 1460, val loss: 1.0457948446273804
Epoch 1470, training loss: 311.8013610839844 = 0.1046699658036232 + 50.0 * 6.233933448791504
Epoch 1470, val loss: 1.04970383644104
Epoch 1480, training loss: 311.8265380859375 = 0.10223263502120972 + 50.0 * 6.234486103057861
Epoch 1480, val loss: 1.053924322128296
Epoch 1490, training loss: 311.77593994140625 = 0.09984136372804642 + 50.0 * 6.233521461486816
Epoch 1490, val loss: 1.0579265356063843
Epoch 1500, training loss: 311.6134948730469 = 0.09754354506731033 + 50.0 * 6.230319499969482
Epoch 1500, val loss: 1.0620617866516113
Epoch 1510, training loss: 311.6425476074219 = 0.09531988203525543 + 50.0 * 6.230944633483887
Epoch 1510, val loss: 1.066348910331726
Epoch 1520, training loss: 312.03961181640625 = 0.09314075857400894 + 50.0 * 6.238929748535156
Epoch 1520, val loss: 1.0704320669174194
Epoch 1530, training loss: 311.708251953125 = 0.09101824462413788 + 50.0 * 6.232345104217529
Epoch 1530, val loss: 1.0742225646972656
Epoch 1540, training loss: 311.57861328125 = 0.08893638849258423 + 50.0 * 6.229793548583984
Epoch 1540, val loss: 1.0784244537353516
Epoch 1550, training loss: 311.5182189941406 = 0.08694315701723099 + 50.0 * 6.228625297546387
Epoch 1550, val loss: 1.0826767683029175
Epoch 1560, training loss: 311.61090087890625 = 0.08502902835607529 + 50.0 * 6.230517864227295
Epoch 1560, val loss: 1.0869108438491821
Epoch 1570, training loss: 311.6380920410156 = 0.0831194818019867 + 50.0 * 6.2310991287231445
Epoch 1570, val loss: 1.0909174680709839
Epoch 1580, training loss: 311.480224609375 = 0.08123624324798584 + 50.0 * 6.22797966003418
Epoch 1580, val loss: 1.0949006080627441
Epoch 1590, training loss: 311.4767761230469 = 0.07945564389228821 + 50.0 * 6.2279462814331055
Epoch 1590, val loss: 1.0992083549499512
Epoch 1600, training loss: 311.7352294921875 = 0.0777435302734375 + 50.0 * 6.233150005340576
Epoch 1600, val loss: 1.1034172773361206
Epoch 1610, training loss: 311.5394592285156 = 0.07603103667497635 + 50.0 * 6.229268550872803
Epoch 1610, val loss: 1.1074199676513672
Epoch 1620, training loss: 311.469482421875 = 0.07437536120414734 + 50.0 * 6.227902412414551
Epoch 1620, val loss: 1.1115515232086182
Epoch 1630, training loss: 311.4107666015625 = 0.07276925444602966 + 50.0 * 6.226759910583496
Epoch 1630, val loss: 1.1157246828079224
Epoch 1640, training loss: 311.5592346191406 = 0.07122112810611725 + 50.0 * 6.22976016998291
Epoch 1640, val loss: 1.1199427843093872
Epoch 1650, training loss: 311.4834289550781 = 0.06966263055801392 + 50.0 * 6.228275299072266
Epoch 1650, val loss: 1.1238081455230713
Epoch 1660, training loss: 311.3923034667969 = 0.06816627085208893 + 50.0 * 6.226482391357422
Epoch 1660, val loss: 1.1278119087219238
Epoch 1670, training loss: 311.33258056640625 = 0.0667441263794899 + 50.0 * 6.225317001342773
Epoch 1670, val loss: 1.1320157051086426
Epoch 1680, training loss: 311.3144836425781 = 0.0653543770313263 + 50.0 * 6.224982261657715
Epoch 1680, val loss: 1.1361099481582642
Epoch 1690, training loss: 311.4678039550781 = 0.06401567906141281 + 50.0 * 6.2280755043029785
Epoch 1690, val loss: 1.1401875019073486
Epoch 1700, training loss: 311.2776184082031 = 0.06266951560974121 + 50.0 * 6.224298477172852
Epoch 1700, val loss: 1.1442105770111084
Epoch 1710, training loss: 311.4501647949219 = 0.06137591972947121 + 50.0 * 6.227775573730469
Epoch 1710, val loss: 1.1482932567596436
Epoch 1720, training loss: 311.3983154296875 = 0.0601169690489769 + 50.0 * 6.226763725280762
Epoch 1720, val loss: 1.1522114276885986
Epoch 1730, training loss: 311.2667236328125 = 0.058845408260822296 + 50.0 * 6.224157810211182
Epoch 1730, val loss: 1.1560978889465332
Epoch 1740, training loss: 311.2384033203125 = 0.057664547115564346 + 50.0 * 6.223614692687988
Epoch 1740, val loss: 1.1604548692703247
Epoch 1750, training loss: 311.21490478515625 = 0.056518882513046265 + 50.0 * 6.223167419433594
Epoch 1750, val loss: 1.1645421981811523
Epoch 1760, training loss: 311.5491027832031 = 0.05539868399500847 + 50.0 * 6.229874134063721
Epoch 1760, val loss: 1.168580174446106
Epoch 1770, training loss: 311.2423095703125 = 0.05429685488343239 + 50.0 * 6.22376012802124
Epoch 1770, val loss: 1.1724913120269775
Epoch 1780, training loss: 311.2473449707031 = 0.05321415513753891 + 50.0 * 6.22388219833374
Epoch 1780, val loss: 1.1765440702438354
Epoch 1790, training loss: 311.2257995605469 = 0.05216667428612709 + 50.0 * 6.223472595214844
Epoch 1790, val loss: 1.1805099248886108
Epoch 1800, training loss: 311.12548828125 = 0.05116082355380058 + 50.0 * 6.221487045288086
Epoch 1800, val loss: 1.1845577955245972
Epoch 1810, training loss: 311.242431640625 = 0.050187788903713226 + 50.0 * 6.223844528198242
Epoch 1810, val loss: 1.1885783672332764
Epoch 1820, training loss: 311.2872009277344 = 0.04920657351613045 + 50.0 * 6.224760055541992
Epoch 1820, val loss: 1.1924399137496948
Epoch 1830, training loss: 311.1852111816406 = 0.048251401633024216 + 50.0 * 6.222739219665527
Epoch 1830, val loss: 1.1962831020355225
Epoch 1840, training loss: 311.0902404785156 = 0.04732169583439827 + 50.0 * 6.220858097076416
Epoch 1840, val loss: 1.2002075910568237
Epoch 1850, training loss: 311.06829833984375 = 0.04643050581216812 + 50.0 * 6.220437526702881
Epoch 1850, val loss: 1.2042531967163086
Epoch 1860, training loss: 311.2449645996094 = 0.04556386172771454 + 50.0 * 6.223988056182861
Epoch 1860, val loss: 1.2082411050796509
Epoch 1870, training loss: 311.1407470703125 = 0.044704586267471313 + 50.0 * 6.221920490264893
Epoch 1870, val loss: 1.211975336074829
Epoch 1880, training loss: 311.05340576171875 = 0.04386800155043602 + 50.0 * 6.22019100189209
Epoch 1880, val loss: 1.2157641649246216
Epoch 1890, training loss: 311.04949951171875 = 0.04304797574877739 + 50.0 * 6.220129013061523
Epoch 1890, val loss: 1.219622015953064
Epoch 1900, training loss: 311.2284240722656 = 0.04226250573992729 + 50.0 * 6.223723411560059
Epoch 1900, val loss: 1.2234880924224854
Epoch 1910, training loss: 311.03515625 = 0.041485708206892014 + 50.0 * 6.219873428344727
Epoch 1910, val loss: 1.2272670269012451
Epoch 1920, training loss: 311.0235595703125 = 0.04073803871870041 + 50.0 * 6.219656467437744
Epoch 1920, val loss: 1.2310841083526611
Epoch 1930, training loss: 311.040283203125 = 0.04000278562307358 + 50.0 * 6.220005512237549
Epoch 1930, val loss: 1.2349025011062622
Epoch 1940, training loss: 311.083251953125 = 0.03929143399000168 + 50.0 * 6.220879077911377
Epoch 1940, val loss: 1.2387566566467285
Epoch 1950, training loss: 310.9981689453125 = 0.03858082741498947 + 50.0 * 6.219192028045654
Epoch 1950, val loss: 1.242531180381775
Epoch 1960, training loss: 310.9720153808594 = 0.03789907693862915 + 50.0 * 6.218682289123535
Epoch 1960, val loss: 1.2462983131408691
Epoch 1970, training loss: 311.13232421875 = 0.03723684698343277 + 50.0 * 6.221901893615723
Epoch 1970, val loss: 1.2499492168426514
Epoch 1980, training loss: 310.9216003417969 = 0.03656988590955734 + 50.0 * 6.217700958251953
Epoch 1980, val loss: 1.2535043954849243
Epoch 1990, training loss: 310.9046325683594 = 0.03593634441494942 + 50.0 * 6.217373847961426
Epoch 1990, val loss: 1.2572211027145386
Epoch 2000, training loss: 310.99090576171875 = 0.03531859442591667 + 50.0 * 6.219111919403076
Epoch 2000, val loss: 1.2608885765075684
Epoch 2010, training loss: 311.2309265136719 = 0.03470627963542938 + 50.0 * 6.22392463684082
Epoch 2010, val loss: 1.2642748355865479
Epoch 2020, training loss: 310.8797607421875 = 0.034103408455848694 + 50.0 * 6.216912746429443
Epoch 2020, val loss: 1.2677505016326904
Epoch 2030, training loss: 310.83050537109375 = 0.0335206501185894 + 50.0 * 6.215939998626709
Epoch 2030, val loss: 1.2714548110961914
Epoch 2040, training loss: 310.8033447265625 = 0.032969534397125244 + 50.0 * 6.215407848358154
Epoch 2040, val loss: 1.2751219272613525
Epoch 2050, training loss: 310.7938537597656 = 0.03243598714470863 + 50.0 * 6.21522855758667
Epoch 2050, val loss: 1.2787715196609497
Epoch 2060, training loss: 310.9589538574219 = 0.031917303800582886 + 50.0 * 6.218540668487549
Epoch 2060, val loss: 1.2823865413665771
Epoch 2070, training loss: 311.0165100097656 = 0.031368862837553024 + 50.0 * 6.21970272064209
Epoch 2070, val loss: 1.2854657173156738
Epoch 2080, training loss: 310.90081787109375 = 0.03084726072847843 + 50.0 * 6.2173991203308105
Epoch 2080, val loss: 1.2888703346252441
Epoch 2090, training loss: 310.8762512207031 = 0.030334610491991043 + 50.0 * 6.216918468475342
Epoch 2090, val loss: 1.2923483848571777
Epoch 2100, training loss: 310.8713073730469 = 0.029854783788323402 + 50.0 * 6.216829299926758
Epoch 2100, val loss: 1.2958484888076782
Epoch 2110, training loss: 310.97564697265625 = 0.029382500797510147 + 50.0 * 6.2189249992370605
Epoch 2110, val loss: 1.299181342124939
Epoch 2120, training loss: 310.7609558105469 = 0.02889294922351837 + 50.0 * 6.214641094207764
Epoch 2120, val loss: 1.3025217056274414
Epoch 2130, training loss: 310.7447509765625 = 0.028437219560146332 + 50.0 * 6.21432638168335
Epoch 2130, val loss: 1.3060029745101929
Epoch 2140, training loss: 310.7595520019531 = 0.027998678386211395 + 50.0 * 6.2146315574646
Epoch 2140, val loss: 1.309391736984253
Epoch 2150, training loss: 310.94989013671875 = 0.02756410650908947 + 50.0 * 6.218446731567383
Epoch 2150, val loss: 1.3125853538513184
Epoch 2160, training loss: 310.97320556640625 = 0.027130160480737686 + 50.0 * 6.218921661376953
Epoch 2160, val loss: 1.315765619277954
Epoch 2170, training loss: 310.84259033203125 = 0.02669873833656311 + 50.0 * 6.216317653656006
Epoch 2170, val loss: 1.3190057277679443
Epoch 2180, training loss: 310.7088928222656 = 0.026283154264092445 + 50.0 * 6.21365213394165
Epoch 2180, val loss: 1.3223135471343994
Epoch 2190, training loss: 310.6916809082031 = 0.02588794380426407 + 50.0 * 6.213315963745117
Epoch 2190, val loss: 1.3257042169570923
Epoch 2200, training loss: 310.98455810546875 = 0.025499969720840454 + 50.0 * 6.219181060791016
Epoch 2200, val loss: 1.3288586139678955
Epoch 2210, training loss: 310.7085876464844 = 0.025109417736530304 + 50.0 * 6.213669300079346
Epoch 2210, val loss: 1.331951379776001
Epoch 2220, training loss: 310.71527099609375 = 0.02473423257470131 + 50.0 * 6.213810920715332
Epoch 2220, val loss: 1.335130214691162
Epoch 2230, training loss: 310.7962646484375 = 0.024371810257434845 + 50.0 * 6.215437889099121
Epoch 2230, val loss: 1.3383058309555054
Epoch 2240, training loss: 310.76593017578125 = 0.024008216336369514 + 50.0 * 6.214838027954102
Epoch 2240, val loss: 1.341418743133545
Epoch 2250, training loss: 310.7155456542969 = 0.023647237569093704 + 50.0 * 6.21383810043335
Epoch 2250, val loss: 1.3445656299591064
Epoch 2260, training loss: 310.68927001953125 = 0.023305175825953484 + 50.0 * 6.213318824768066
Epoch 2260, val loss: 1.3477115631103516
Epoch 2270, training loss: 310.5922546386719 = 0.022968387231230736 + 50.0 * 6.211385250091553
Epoch 2270, val loss: 1.3508834838867188
Epoch 2280, training loss: 310.7712707519531 = 0.0226521547883749 + 50.0 * 6.214972496032715
Epoch 2280, val loss: 1.3540380001068115
Epoch 2290, training loss: 310.7061767578125 = 0.022317497059702873 + 50.0 * 6.213677406311035
Epoch 2290, val loss: 1.3568317890167236
Epoch 2300, training loss: 310.5670471191406 = 0.02198210544884205 + 50.0 * 6.210901260375977
Epoch 2300, val loss: 1.3598263263702393
Epoch 2310, training loss: 310.5990295410156 = 0.021679071709513664 + 50.0 * 6.211547374725342
Epoch 2310, val loss: 1.3630094528198242
Epoch 2320, training loss: 310.8266906738281 = 0.021377796307206154 + 50.0 * 6.216105937957764
Epoch 2320, val loss: 1.3658775091171265
Epoch 2330, training loss: 310.5964050292969 = 0.021075570955872536 + 50.0 * 6.211506366729736
Epoch 2330, val loss: 1.3687831163406372
Epoch 2340, training loss: 310.7237243652344 = 0.020785877481102943 + 50.0 * 6.214058876037598
Epoch 2340, val loss: 1.3717657327651978
Epoch 2350, training loss: 310.6338806152344 = 0.020495794713497162 + 50.0 * 6.2122673988342285
Epoch 2350, val loss: 1.3745646476745605
Epoch 2360, training loss: 310.53717041015625 = 0.020209627225995064 + 50.0 * 6.210339546203613
Epoch 2360, val loss: 1.3773921728134155
Epoch 2370, training loss: 310.517822265625 = 0.01993866264820099 + 50.0 * 6.209958076477051
Epoch 2370, val loss: 1.380514144897461
Epoch 2380, training loss: 310.5697937011719 = 0.019680580124258995 + 50.0 * 6.211002349853516
Epoch 2380, val loss: 1.3834407329559326
Epoch 2390, training loss: 310.697998046875 = 0.019420547410845757 + 50.0 * 6.213571548461914
Epoch 2390, val loss: 1.3860934972763062
Epoch 2400, training loss: 310.6622009277344 = 0.01914534904062748 + 50.0 * 6.21286153793335
Epoch 2400, val loss: 1.3888386487960815
Epoch 2410, training loss: 310.5223388671875 = 0.018892593681812286 + 50.0 * 6.210069179534912
Epoch 2410, val loss: 1.3917988538742065
Epoch 2420, training loss: 310.55999755859375 = 0.018643677234649658 + 50.0 * 6.210826873779297
Epoch 2420, val loss: 1.394629716873169
Epoch 2430, training loss: 310.4707946777344 = 0.018398165702819824 + 50.0 * 6.209047794342041
Epoch 2430, val loss: 1.3973978757858276
Epoch 2440, training loss: 310.5563049316406 = 0.018162919208407402 + 50.0 * 6.210762977600098
Epoch 2440, val loss: 1.400209665298462
Epoch 2450, training loss: 310.5942077636719 = 0.01792657934129238 + 50.0 * 6.211525917053223
Epoch 2450, val loss: 1.402841567993164
Epoch 2460, training loss: 310.4762268066406 = 0.017690885812044144 + 50.0 * 6.209170341491699
Epoch 2460, val loss: 1.4055627584457397
Epoch 2470, training loss: 310.4342041015625 = 0.0174661073833704 + 50.0 * 6.208334922790527
Epoch 2470, val loss: 1.408370852470398
Epoch 2480, training loss: 310.5870666503906 = 0.0172547847032547 + 50.0 * 6.21139669418335
Epoch 2480, val loss: 1.4111108779907227
Epoch 2490, training loss: 310.5083923339844 = 0.017031347379088402 + 50.0 * 6.209827423095703
Epoch 2490, val loss: 1.4136362075805664
Epoch 2500, training loss: 310.5020751953125 = 0.016810348257422447 + 50.0 * 6.209705352783203
Epoch 2500, val loss: 1.4163436889648438
Epoch 2510, training loss: 310.45599365234375 = 0.01660449430346489 + 50.0 * 6.20878791809082
Epoch 2510, val loss: 1.4189612865447998
Epoch 2520, training loss: 310.516845703125 = 0.016397513449192047 + 50.0 * 6.2100090980529785
Epoch 2520, val loss: 1.4216816425323486
Epoch 2530, training loss: 310.4023742675781 = 0.016192767769098282 + 50.0 * 6.207723617553711
Epoch 2530, val loss: 1.4242804050445557
Epoch 2540, training loss: 310.5601806640625 = 0.01600468158721924 + 50.0 * 6.210884094238281
Epoch 2540, val loss: 1.4268401861190796
Epoch 2550, training loss: 310.4819030761719 = 0.01580207236111164 + 50.0 * 6.209321975708008
Epoch 2550, val loss: 1.4292736053466797
Epoch 2560, training loss: 310.374755859375 = 0.015605944208800793 + 50.0 * 6.207183361053467
Epoch 2560, val loss: 1.4318557977676392
Epoch 2570, training loss: 310.3262939453125 = 0.015421414747834206 + 50.0 * 6.2062177658081055
Epoch 2570, val loss: 1.4344732761383057
Epoch 2580, training loss: 310.3586730957031 = 0.015243865549564362 + 50.0 * 6.2068681716918945
Epoch 2580, val loss: 1.4370590448379517
Epoch 2590, training loss: 310.52679443359375 = 0.015065702609717846 + 50.0 * 6.21023416519165
Epoch 2590, val loss: 1.4394429922103882
Epoch 2600, training loss: 310.6539306640625 = 0.014881201088428497 + 50.0 * 6.212780952453613
Epoch 2600, val loss: 1.4417461156845093
Epoch 2610, training loss: 310.4615478515625 = 0.0147053562104702 + 50.0 * 6.20893669128418
Epoch 2610, val loss: 1.4442156553268433
Epoch 2620, training loss: 310.38671875 = 0.014531209133565426 + 50.0 * 6.207443714141846
Epoch 2620, val loss: 1.4467109441757202
Epoch 2630, training loss: 310.3713684082031 = 0.014365908689796925 + 50.0 * 6.20713996887207
Epoch 2630, val loss: 1.4490867853164673
Epoch 2640, training loss: 310.4135437011719 = 0.014201139099895954 + 50.0 * 6.207987308502197
Epoch 2640, val loss: 1.4514816999435425
Epoch 2650, training loss: 310.4127502441406 = 0.014038854278624058 + 50.0 * 6.207974433898926
Epoch 2650, val loss: 1.4538192749023438
Epoch 2660, training loss: 310.3078918457031 = 0.013882804661989212 + 50.0 * 6.205880165100098
Epoch 2660, val loss: 1.456317663192749
Epoch 2670, training loss: 310.47015380859375 = 0.013728187419474125 + 50.0 * 6.209128379821777
Epoch 2670, val loss: 1.4586256742477417
Epoch 2680, training loss: 310.3157043457031 = 0.013565011322498322 + 50.0 * 6.206043243408203
Epoch 2680, val loss: 1.4609081745147705
Epoch 2690, training loss: 310.2860412597656 = 0.013411059975624084 + 50.0 * 6.205452919006348
Epoch 2690, val loss: 1.4631998538970947
Epoch 2700, training loss: 310.2110595703125 = 0.013267169706523418 + 50.0 * 6.20395565032959
Epoch 2700, val loss: 1.4655814170837402
Epoch 2710, training loss: 310.2115173339844 = 0.01312762126326561 + 50.0 * 6.203968048095703
Epoch 2710, val loss: 1.467934489250183
Epoch 2720, training loss: 310.4762878417969 = 0.012997315265238285 + 50.0 * 6.20926570892334
Epoch 2720, val loss: 1.470144510269165
Epoch 2730, training loss: 310.2332763671875 = 0.012848114594817162 + 50.0 * 6.204408645629883
Epoch 2730, val loss: 1.4722845554351807
Epoch 2740, training loss: 310.3468017578125 = 0.012706413865089417 + 50.0 * 6.206682205200195
Epoch 2740, val loss: 1.474488377571106
Epoch 2750, training loss: 310.2671203613281 = 0.012563553638756275 + 50.0 * 6.2050909996032715
Epoch 2750, val loss: 1.476621389389038
Epoch 2760, training loss: 310.25360107421875 = 0.012429333291947842 + 50.0 * 6.2048234939575195
Epoch 2760, val loss: 1.4788745641708374
Epoch 2770, training loss: 310.3171691894531 = 0.012305776588618755 + 50.0 * 6.206097602844238
Epoch 2770, val loss: 1.4812599420547485
Epoch 2780, training loss: 310.1982727050781 = 0.012172114104032516 + 50.0 * 6.20372200012207
Epoch 2780, val loss: 1.4834033250808716
Epoch 2790, training loss: 310.20977783203125 = 0.012044692412018776 + 50.0 * 6.203954219818115
Epoch 2790, val loss: 1.4855749607086182
Epoch 2800, training loss: 310.2810363769531 = 0.011926069855690002 + 50.0 * 6.205382347106934
Epoch 2800, val loss: 1.4877556562423706
Epoch 2810, training loss: 310.200927734375 = 0.011801742017269135 + 50.0 * 6.203782558441162
Epoch 2810, val loss: 1.4898685216903687
Epoch 2820, training loss: 310.35455322265625 = 0.011679847724735737 + 50.0 * 6.206857204437256
Epoch 2820, val loss: 1.4919663667678833
Epoch 2830, training loss: 310.4236755371094 = 0.011556683108210564 + 50.0 * 6.208241939544678
Epoch 2830, val loss: 1.493886947631836
Epoch 2840, training loss: 310.2501220703125 = 0.011433611623942852 + 50.0 * 6.204773426055908
Epoch 2840, val loss: 1.496057152748108
Epoch 2850, training loss: 310.1521301269531 = 0.01132030226290226 + 50.0 * 6.202816009521484
Epoch 2850, val loss: 1.4982725381851196
Epoch 2860, training loss: 310.1883544921875 = 0.011207750998437405 + 50.0 * 6.203543186187744
Epoch 2860, val loss: 1.5004545450210571
Epoch 2870, training loss: 310.3446350097656 = 0.011098623275756836 + 50.0 * 6.20667028427124
Epoch 2870, val loss: 1.5024598836898804
Epoch 2880, training loss: 310.25537109375 = 0.010989541187882423 + 50.0 * 6.204887390136719
Epoch 2880, val loss: 1.5042823553085327
Epoch 2890, training loss: 310.2014465332031 = 0.010877230204641819 + 50.0 * 6.2038116455078125
Epoch 2890, val loss: 1.50638747215271
Epoch 2900, training loss: 310.1673278808594 = 0.01077363733202219 + 50.0 * 6.203131198883057
Epoch 2900, val loss: 1.5084712505340576
Epoch 2910, training loss: 310.17242431640625 = 0.010667478665709496 + 50.0 * 6.203235149383545
Epoch 2910, val loss: 1.5104455947875977
Epoch 2920, training loss: 310.0896911621094 = 0.010563550516963005 + 50.0 * 6.201582908630371
Epoch 2920, val loss: 1.512406587600708
Epoch 2930, training loss: 310.1587829589844 = 0.010463666170835495 + 50.0 * 6.202966213226318
Epoch 2930, val loss: 1.5144363641738892
Epoch 2940, training loss: 310.2825012207031 = 0.01036110520362854 + 50.0 * 6.205442428588867
Epoch 2940, val loss: 1.5163520574569702
Epoch 2950, training loss: 310.1671447753906 = 0.01026375312358141 + 50.0 * 6.2031378746032715
Epoch 2950, val loss: 1.5181095600128174
Epoch 2960, training loss: 310.1521301269531 = 0.010163092985749245 + 50.0 * 6.202839374542236
Epoch 2960, val loss: 1.5200591087341309
Epoch 2970, training loss: 310.1171569824219 = 0.010066849179565907 + 50.0 * 6.202141761779785
Epoch 2970, val loss: 1.5220599174499512
Epoch 2980, training loss: 310.0882873535156 = 0.00997481681406498 + 50.0 * 6.201566219329834
Epoch 2980, val loss: 1.524037480354309
Epoch 2990, training loss: 310.043701171875 = 0.009882335551083088 + 50.0 * 6.200676441192627
Epoch 2990, val loss: 1.5258902311325073
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8413284132841329
=== training gcn model ===
Epoch 0, training loss: 431.8063049316406 = 1.9671704769134521 + 50.0 * 8.596782684326172
Epoch 0, val loss: 1.9619892835617065
Epoch 10, training loss: 431.7409362792969 = 1.9574439525604248 + 50.0 * 8.595669746398926
Epoch 10, val loss: 1.9525307416915894
Epoch 20, training loss: 431.3602600097656 = 1.9453142881393433 + 50.0 * 8.588298797607422
Epoch 20, val loss: 1.940322995185852
Epoch 30, training loss: 428.98492431640625 = 1.929503321647644 + 50.0 * 8.541108131408691
Epoch 30, val loss: 1.9240503311157227
Epoch 40, training loss: 414.8153991699219 = 1.9104163646697998 + 50.0 * 8.258099555969238
Epoch 40, val loss: 1.9046605825424194
Epoch 50, training loss: 382.6815185546875 = 1.887050986289978 + 50.0 * 7.615889072418213
Epoch 50, val loss: 1.8812967538833618
Epoch 60, training loss: 369.60394287109375 = 1.8690969944000244 + 50.0 * 7.354696750640869
Epoch 60, val loss: 1.8646540641784668
Epoch 70, training loss: 357.4826965332031 = 1.8552526235580444 + 50.0 * 7.112548828125
Epoch 70, val loss: 1.8514708280563354
Epoch 80, training loss: 349.93499755859375 = 1.8417341709136963 + 50.0 * 6.961864948272705
Epoch 80, val loss: 1.8387089967727661
Epoch 90, training loss: 344.201904296875 = 1.8286008834838867 + 50.0 * 6.847465515136719
Epoch 90, val loss: 1.8262014389038086
Epoch 100, training loss: 340.53411865234375 = 1.816171646118164 + 50.0 * 6.774359226226807
Epoch 100, val loss: 1.8143649101257324
Epoch 110, training loss: 337.7154541015625 = 1.8048609495162964 + 50.0 * 6.718211650848389
Epoch 110, val loss: 1.8037216663360596
Epoch 120, training loss: 335.4975280761719 = 1.7945119142532349 + 50.0 * 6.674060344696045
Epoch 120, val loss: 1.794190526008606
Epoch 130, training loss: 333.7100830078125 = 1.784588098526001 + 50.0 * 6.638510227203369
Epoch 130, val loss: 1.7850830554962158
Epoch 140, training loss: 332.1163635253906 = 1.7750722169876099 + 50.0 * 6.606825351715088
Epoch 140, val loss: 1.776252269744873
Epoch 150, training loss: 330.6615295410156 = 1.7655843496322632 + 50.0 * 6.577918529510498
Epoch 150, val loss: 1.767508625984192
Epoch 160, training loss: 329.2876892089844 = 1.7558627128601074 + 50.0 * 6.5506367683410645
Epoch 160, val loss: 1.7586579322814941
Epoch 170, training loss: 328.1641845703125 = 1.7456145286560059 + 50.0 * 6.528371334075928
Epoch 170, val loss: 1.7495665550231934
Epoch 180, training loss: 327.1513366699219 = 1.7346729040145874 + 50.0 * 6.508333206176758
Epoch 180, val loss: 1.7399213314056396
Epoch 190, training loss: 326.4810485839844 = 1.7228771448135376 + 50.0 * 6.495163440704346
Epoch 190, val loss: 1.7296383380889893
Epoch 200, training loss: 325.61553955078125 = 1.710005521774292 + 50.0 * 6.4781107902526855
Epoch 200, val loss: 1.7185533046722412
Epoch 210, training loss: 324.9770202636719 = 1.6961338520050049 + 50.0 * 6.465617656707764
Epoch 210, val loss: 1.7066973447799683
Epoch 220, training loss: 324.4019775390625 = 1.6811938285827637 + 50.0 * 6.454415798187256
Epoch 220, val loss: 1.693987250328064
Epoch 230, training loss: 323.85858154296875 = 1.6651782989501953 + 50.0 * 6.4438676834106445
Epoch 230, val loss: 1.6804394721984863
Epoch 240, training loss: 323.50787353515625 = 1.6480450630187988 + 50.0 * 6.437196731567383
Epoch 240, val loss: 1.665961503982544
Epoch 250, training loss: 322.9050598144531 = 1.6296465396881104 + 50.0 * 6.425508499145508
Epoch 250, val loss: 1.6504836082458496
Epoch 260, training loss: 322.4903259277344 = 1.6101164817810059 + 50.0 * 6.417604446411133
Epoch 260, val loss: 1.6341723203659058
Epoch 270, training loss: 322.1850891113281 = 1.5893796682357788 + 50.0 * 6.411913871765137
Epoch 270, val loss: 1.6169365644454956
Epoch 280, training loss: 321.8349304199219 = 1.5674318075180054 + 50.0 * 6.4053497314453125
Epoch 280, val loss: 1.5987907648086548
Epoch 290, training loss: 321.409423828125 = 1.5444599390029907 + 50.0 * 6.397299289703369
Epoch 290, val loss: 1.5798625946044922
Epoch 300, training loss: 321.06927490234375 = 1.5205037593841553 + 50.0 * 6.390975475311279
Epoch 300, val loss: 1.560259222984314
Epoch 310, training loss: 320.8016662597656 = 1.4956144094467163 + 50.0 * 6.3861212730407715
Epoch 310, val loss: 1.5400192737579346
Epoch 320, training loss: 320.6936950683594 = 1.469849705696106 + 50.0 * 6.384477138519287
Epoch 320, val loss: 1.5191593170166016
Epoch 330, training loss: 320.2552795410156 = 1.4434770345687866 + 50.0 * 6.3762359619140625
Epoch 330, val loss: 1.4979348182678223
Epoch 340, training loss: 319.9622802734375 = 1.4165996313095093 + 50.0 * 6.370913982391357
Epoch 340, val loss: 1.476471185684204
Epoch 350, training loss: 319.9424743652344 = 1.389367699623108 + 50.0 * 6.3710618019104
Epoch 350, val loss: 1.4548671245574951
Epoch 360, training loss: 319.5384826660156 = 1.3617446422576904 + 50.0 * 6.363534450531006
Epoch 360, val loss: 1.4331556558609009
Epoch 370, training loss: 319.2392578125 = 1.3341120481491089 + 50.0 * 6.358103275299072
Epoch 370, val loss: 1.4116413593292236
Epoch 380, training loss: 319.0416259765625 = 1.3065259456634521 + 50.0 * 6.354701995849609
Epoch 380, val loss: 1.3903595209121704
Epoch 390, training loss: 318.840087890625 = 1.279028296470642 + 50.0 * 6.351221084594727
Epoch 390, val loss: 1.3692283630371094
Epoch 400, training loss: 318.8386535644531 = 1.251741886138916 + 50.0 * 6.351738452911377
Epoch 400, val loss: 1.3485071659088135
Epoch 410, training loss: 318.46380615234375 = 1.2248389720916748 + 50.0 * 6.344779014587402
Epoch 410, val loss: 1.3282537460327148
Epoch 420, training loss: 318.2181396484375 = 1.1983332633972168 + 50.0 * 6.340395927429199
Epoch 420, val loss: 1.3085556030273438
Epoch 430, training loss: 318.0469665527344 = 1.1723322868347168 + 50.0 * 6.3374924659729
Epoch 430, val loss: 1.289438009262085
Epoch 440, training loss: 317.8916931152344 = 1.1467630863189697 + 50.0 * 6.334898948669434
Epoch 440, val loss: 1.2709286212921143
Epoch 450, training loss: 317.7081604003906 = 1.1217023134231567 + 50.0 * 6.331729412078857
Epoch 450, val loss: 1.2529046535491943
Epoch 460, training loss: 317.594482421875 = 1.0972411632537842 + 50.0 * 6.329945087432861
Epoch 460, val loss: 1.2355875968933105
Epoch 470, training loss: 317.3707275390625 = 1.0733001232147217 + 50.0 * 6.325948715209961
Epoch 470, val loss: 1.2188740968704224
Epoch 480, training loss: 317.3170166015625 = 1.049931287765503 + 50.0 * 6.325341701507568
Epoch 480, val loss: 1.202763557434082
Epoch 490, training loss: 317.3008117675781 = 1.027181625366211 + 50.0 * 6.325472354888916
Epoch 490, val loss: 1.187305212020874
Epoch 500, training loss: 316.9264221191406 = 1.004866600036621 + 50.0 * 6.318431377410889
Epoch 500, val loss: 1.1724296808242798
Epoch 510, training loss: 316.8220520019531 = 0.9831669330596924 + 50.0 * 6.316778182983398
Epoch 510, val loss: 1.1581372022628784
Epoch 520, training loss: 316.68927001953125 = 0.9620194435119629 + 50.0 * 6.314545154571533
Epoch 520, val loss: 1.1444844007492065
Epoch 530, training loss: 316.566650390625 = 0.9414443969726562 + 50.0 * 6.312504291534424
Epoch 530, val loss: 1.1313682794570923
Epoch 540, training loss: 316.41552734375 = 0.9214500784873962 + 50.0 * 6.309881687164307
Epoch 540, val loss: 1.1188604831695557
Epoch 550, training loss: 316.3788757324219 = 0.9020327925682068 + 50.0 * 6.309536933898926
Epoch 550, val loss: 1.1068768501281738
Epoch 560, training loss: 316.3398742675781 = 0.8829547166824341 + 50.0 * 6.309138298034668
Epoch 560, val loss: 1.0954066514968872
Epoch 570, training loss: 316.09942626953125 = 0.8644764423370361 + 50.0 * 6.304698944091797
Epoch 570, val loss: 1.084359884262085
Epoch 580, training loss: 315.94183349609375 = 0.8465149402618408 + 50.0 * 6.301906108856201
Epoch 580, val loss: 1.0738133192062378
Epoch 590, training loss: 315.87164306640625 = 0.8290915489196777 + 50.0 * 6.300850868225098
Epoch 590, val loss: 1.0637279748916626
Epoch 600, training loss: 315.892578125 = 0.8119533061981201 + 50.0 * 6.301612854003906
Epoch 600, val loss: 1.0540602207183838
Epoch 610, training loss: 315.63812255859375 = 0.7952708601951599 + 50.0 * 6.2968573570251465
Epoch 610, val loss: 1.0446466207504272
Epoch 620, training loss: 315.5243835449219 = 0.7791128158569336 + 50.0 * 6.294905662536621
Epoch 620, val loss: 1.0358163118362427
Epoch 630, training loss: 315.4314270019531 = 0.7633440494537354 + 50.0 * 6.293361663818359
Epoch 630, val loss: 1.0273478031158447
Epoch 640, training loss: 315.6629333496094 = 0.7479227781295776 + 50.0 * 6.298300266265869
Epoch 640, val loss: 1.0193537473678589
Epoch 650, training loss: 315.4239807128906 = 0.7327433228492737 + 50.0 * 6.293824195861816
Epoch 650, val loss: 1.0113106966018677
Epoch 660, training loss: 315.2314453125 = 0.717937707901001 + 50.0 * 6.2902703285217285
Epoch 660, val loss: 1.0038095712661743
Epoch 670, training loss: 315.0768127441406 = 0.7034735679626465 + 50.0 * 6.287467002868652
Epoch 670, val loss: 0.9966599941253662
Epoch 680, training loss: 315.1921691894531 = 0.6893539428710938 + 50.0 * 6.290056228637695
Epoch 680, val loss: 0.9899399280548096
Epoch 690, training loss: 315.0435791015625 = 0.675301730632782 + 50.0 * 6.287365436553955
Epoch 690, val loss: 0.9830435514450073
Epoch 700, training loss: 314.9794616699219 = 0.6616314053535461 + 50.0 * 6.2863569259643555
Epoch 700, val loss: 0.9766391515731812
Epoch 710, training loss: 315.076904296875 = 0.648093581199646 + 50.0 * 6.288576126098633
Epoch 710, val loss: 0.9704356789588928
Epoch 720, training loss: 314.75958251953125 = 0.6346559524536133 + 50.0 * 6.282498836517334
Epoch 720, val loss: 0.96434485912323
Epoch 730, training loss: 314.608642578125 = 0.621525764465332 + 50.0 * 6.279742240905762
Epoch 730, val loss: 0.9583690762519836
Epoch 740, training loss: 314.572021484375 = 0.6086504459381104 + 50.0 * 6.279267311096191
Epoch 740, val loss: 0.9527572989463806
Epoch 750, training loss: 314.65234375 = 0.5958865880966187 + 50.0 * 6.281128883361816
Epoch 750, val loss: 0.9472945332527161
Epoch 760, training loss: 314.39898681640625 = 0.5832825899124146 + 50.0 * 6.2763142585754395
Epoch 760, val loss: 0.9418731927871704
Epoch 770, training loss: 314.31585693359375 = 0.5708760023117065 + 50.0 * 6.274899959564209
Epoch 770, val loss: 0.936553955078125
Epoch 780, training loss: 314.3582763671875 = 0.5587354898452759 + 50.0 * 6.2759904861450195
Epoch 780, val loss: 0.9315698146820068
Epoch 790, training loss: 314.1901550292969 = 0.5465728640556335 + 50.0 * 6.272871971130371
Epoch 790, val loss: 0.9265369176864624
Epoch 800, training loss: 314.1529541015625 = 0.5346720218658447 + 50.0 * 6.272365570068359
Epoch 800, val loss: 0.9218490123748779
Epoch 810, training loss: 314.05322265625 = 0.5228938460350037 + 50.0 * 6.270606517791748
Epoch 810, val loss: 0.917043149471283
Epoch 820, training loss: 314.4590759277344 = 0.5113817453384399 + 50.0 * 6.278953552246094
Epoch 820, val loss: 0.912510871887207
Epoch 830, training loss: 314.1041259765625 = 0.4998405873775482 + 50.0 * 6.272085666656494
Epoch 830, val loss: 0.9081029295921326
Epoch 840, training loss: 313.9043884277344 = 0.48856791853904724 + 50.0 * 6.268316745758057
Epoch 840, val loss: 0.9038776755332947
Epoch 850, training loss: 313.7870178222656 = 0.4775594174861908 + 50.0 * 6.266189098358154
Epoch 850, val loss: 0.8998977541923523
Epoch 860, training loss: 314.0564880371094 = 0.4667430520057678 + 50.0 * 6.271795272827148
Epoch 860, val loss: 0.8962168097496033
Epoch 870, training loss: 313.9021301269531 = 0.4560149908065796 + 50.0 * 6.268922328948975
Epoch 870, val loss: 0.8923321962356567
Epoch 880, training loss: 313.64190673828125 = 0.44548994302749634 + 50.0 * 6.263927936553955
Epoch 880, val loss: 0.8889385461807251
Epoch 890, training loss: 313.60284423828125 = 0.43520113825798035 + 50.0 * 6.263352870941162
Epoch 890, val loss: 0.8856366872787476
Epoch 900, training loss: 313.6345520019531 = 0.42513397336006165 + 50.0 * 6.264188289642334
Epoch 900, val loss: 0.882549524307251
Epoch 910, training loss: 313.6446533203125 = 0.41518792510032654 + 50.0 * 6.264589309692383
Epoch 910, val loss: 0.8796625733375549
Epoch 920, training loss: 313.5673828125 = 0.4054289162158966 + 50.0 * 6.263238906860352
Epoch 920, val loss: 0.8768870830535889
Epoch 930, training loss: 313.4056396484375 = 0.3958154618740082 + 50.0 * 6.260196685791016
Epoch 930, val loss: 0.8742659091949463
Epoch 940, training loss: 313.32940673828125 = 0.38646724820137024 + 50.0 * 6.258858680725098
Epoch 940, val loss: 0.8719586730003357
Epoch 950, training loss: 313.4855041503906 = 0.3773239850997925 + 50.0 * 6.2621636390686035
Epoch 950, val loss: 0.869774580001831
Epoch 960, training loss: 313.3629150390625 = 0.36830008029937744 + 50.0 * 6.259892463684082
Epoch 960, val loss: 0.8677584528923035
Epoch 970, training loss: 313.2406311035156 = 0.35945940017700195 + 50.0 * 6.257623195648193
Epoch 970, val loss: 0.865981936454773
Epoch 980, training loss: 313.1966857910156 = 0.35078665614128113 + 50.0 * 6.256917953491211
Epoch 980, val loss: 0.8643210530281067
Epoch 990, training loss: 313.26373291015625 = 0.3422994315624237 + 50.0 * 6.258429050445557
Epoch 990, val loss: 0.8628698587417603
Epoch 1000, training loss: 313.11859130859375 = 0.3340512812137604 + 50.0 * 6.255691051483154
Epoch 1000, val loss: 0.8616428375244141
Epoch 1010, training loss: 313.0689392089844 = 0.32591161131858826 + 50.0 * 6.254860877990723
Epoch 1010, val loss: 0.8606027960777283
Epoch 1020, training loss: 313.1078186035156 = 0.3180173337459564 + 50.0 * 6.255795955657959
Epoch 1020, val loss: 0.8596948981285095
Epoch 1030, training loss: 312.9924621582031 = 0.3102589249610901 + 50.0 * 6.25364351272583
Epoch 1030, val loss: 0.8589473366737366
Epoch 1040, training loss: 312.92718505859375 = 0.30268651247024536 + 50.0 * 6.252490520477295
Epoch 1040, val loss: 0.8585205078125
Epoch 1050, training loss: 312.8810119628906 = 0.2953231632709503 + 50.0 * 6.251713752746582
Epoch 1050, val loss: 0.8581651449203491
Epoch 1060, training loss: 312.8677062988281 = 0.28812745213508606 + 50.0 * 6.251591682434082
Epoch 1060, val loss: 0.8580443859100342
Epoch 1070, training loss: 312.9400939941406 = 0.281083345413208 + 50.0 * 6.253180027008057
Epoch 1070, val loss: 0.8580276966094971
Epoch 1080, training loss: 312.77020263671875 = 0.2742708623409271 + 50.0 * 6.2499189376831055
Epoch 1080, val loss: 0.8582376837730408
Epoch 1090, training loss: 312.82623291015625 = 0.2676062285900116 + 50.0 * 6.2511725425720215
Epoch 1090, val loss: 0.85850590467453
Epoch 1100, training loss: 312.8270568847656 = 0.26110756397247314 + 50.0 * 6.25131893157959
Epoch 1100, val loss: 0.8590365648269653
Epoch 1110, training loss: 312.6628723144531 = 0.2548006772994995 + 50.0 * 6.248161315917969
Epoch 1110, val loss: 0.8596785664558411
Epoch 1120, training loss: 312.5828857421875 = 0.24864736199378967 + 50.0 * 6.246684551239014
Epoch 1120, val loss: 0.8604930639266968
Epoch 1130, training loss: 312.5997009277344 = 0.24273861944675446 + 50.0 * 6.2471394538879395
Epoch 1130, val loss: 0.8615761995315552
Epoch 1140, training loss: 312.7059020996094 = 0.23692110180854797 + 50.0 * 6.249379634857178
Epoch 1140, val loss: 0.8627215623855591
Epoch 1150, training loss: 312.49945068359375 = 0.23122289776802063 + 50.0 * 6.245364665985107
Epoch 1150, val loss: 0.8640121817588806
Epoch 1160, training loss: 312.4455261230469 = 0.2257339358329773 + 50.0 * 6.244395732879639
Epoch 1160, val loss: 0.8655411005020142
Epoch 1170, training loss: 312.4521179199219 = 0.22043541073799133 + 50.0 * 6.244633674621582
Epoch 1170, val loss: 0.8670547008514404
Epoch 1180, training loss: 312.5389099121094 = 0.21526481211185455 + 50.0 * 6.2464728355407715
Epoch 1180, val loss: 0.8688122630119324
Epoch 1190, training loss: 312.4438781738281 = 0.2102155089378357 + 50.0 * 6.244673728942871
Epoch 1190, val loss: 0.8705994486808777
Epoch 1200, training loss: 312.5257568359375 = 0.2052983194589615 + 50.0 * 6.2464094161987305
Epoch 1200, val loss: 0.8724602460861206
Epoch 1210, training loss: 312.46307373046875 = 0.20049072802066803 + 50.0 * 6.245251178741455
Epoch 1210, val loss: 0.8745871186256409
Epoch 1220, training loss: 312.2751159667969 = 0.1958225667476654 + 50.0 * 6.241585731506348
Epoch 1220, val loss: 0.8767740726470947
Epoch 1230, training loss: 312.2278137207031 = 0.19132621586322784 + 50.0 * 6.240729808807373
Epoch 1230, val loss: 0.879146933555603
Epoch 1240, training loss: 312.3100280761719 = 0.1869649738073349 + 50.0 * 6.242461681365967
Epoch 1240, val loss: 0.881631076335907
Epoch 1250, training loss: 312.21478271484375 = 0.18266071379184723 + 50.0 * 6.240642547607422
Epoch 1250, val loss: 0.8841432332992554
Epoch 1260, training loss: 312.20452880859375 = 0.17850936949253082 + 50.0 * 6.240520477294922
Epoch 1260, val loss: 0.8868293762207031
Epoch 1270, training loss: 312.25225830078125 = 0.17447993159294128 + 50.0 * 6.241555690765381
Epoch 1270, val loss: 0.8895174264907837
Epoch 1280, training loss: 312.2396545410156 = 0.17054124176502228 + 50.0 * 6.241382598876953
Epoch 1280, val loss: 0.8925595283508301
Epoch 1290, training loss: 312.0843200683594 = 0.1667039394378662 + 50.0 * 6.238351821899414
Epoch 1290, val loss: 0.8953096270561218
Epoch 1300, training loss: 312.044677734375 = 0.16301082074642181 + 50.0 * 6.237633228302002
Epoch 1300, val loss: 0.8983988165855408
Epoch 1310, training loss: 312.095703125 = 0.15945889055728912 + 50.0 * 6.238725185394287
Epoch 1310, val loss: 0.9016271829605103
Epoch 1320, training loss: 312.1106262207031 = 0.1559639722108841 + 50.0 * 6.23909330368042
Epoch 1320, val loss: 0.9048018455505371
Epoch 1330, training loss: 312.04351806640625 = 0.15253111720085144 + 50.0 * 6.237819671630859
Epoch 1330, val loss: 0.9081773161888123
Epoch 1340, training loss: 311.95941162109375 = 0.14924700558185577 + 50.0 * 6.236202716827393
Epoch 1340, val loss: 0.911516547203064
Epoch 1350, training loss: 311.9128112792969 = 0.14604419469833374 + 50.0 * 6.235334873199463
Epoch 1350, val loss: 0.9150417447090149
Epoch 1360, training loss: 312.0251159667969 = 0.14295397698879242 + 50.0 * 6.237643718719482
Epoch 1360, val loss: 0.9187126755714417
Epoch 1370, training loss: 311.9237060546875 = 0.13985653221607208 + 50.0 * 6.2356767654418945
Epoch 1370, val loss: 0.9222578406333923
Epoch 1380, training loss: 311.860107421875 = 0.1368977427482605 + 50.0 * 6.234464168548584
Epoch 1380, val loss: 0.9259189367294312
Epoch 1390, training loss: 311.862060546875 = 0.13400806486606598 + 50.0 * 6.234560966491699
Epoch 1390, val loss: 0.9295710325241089
Epoch 1400, training loss: 312.07989501953125 = 0.13122887909412384 + 50.0 * 6.238973617553711
Epoch 1400, val loss: 0.933420717716217
Epoch 1410, training loss: 311.8110656738281 = 0.12843728065490723 + 50.0 * 6.233652114868164
Epoch 1410, val loss: 0.9371792078018188
Epoch 1420, training loss: 311.74200439453125 = 0.12576153874397278 + 50.0 * 6.232325077056885
Epoch 1420, val loss: 0.9411467909812927
Epoch 1430, training loss: 311.7666015625 = 0.1231885626912117 + 50.0 * 6.232868194580078
Epoch 1430, val loss: 0.94515061378479
Epoch 1440, training loss: 311.94622802734375 = 0.12064606696367264 + 50.0 * 6.236511707305908
Epoch 1440, val loss: 0.9490727186203003
Epoch 1450, training loss: 311.7099609375 = 0.11813993752002716 + 50.0 * 6.231836318969727
Epoch 1450, val loss: 0.9533159136772156
Epoch 1460, training loss: 311.64385986328125 = 0.11573997884988785 + 50.0 * 6.230562210083008
Epoch 1460, val loss: 0.9574404358863831
Epoch 1470, training loss: 311.84765625 = 0.11342193931341171 + 50.0 * 6.234684467315674
Epoch 1470, val loss: 0.9615879654884338
Epoch 1480, training loss: 311.6353759765625 = 0.11109504848718643 + 50.0 * 6.230485916137695
Epoch 1480, val loss: 0.9659973382949829
Epoch 1490, training loss: 311.7820129394531 = 0.10884835571050644 + 50.0 * 6.233463287353516
Epoch 1490, val loss: 0.9703834652900696
Epoch 1500, training loss: 311.6277160644531 = 0.10665252804756165 + 50.0 * 6.23042106628418
Epoch 1500, val loss: 0.974563479423523
Epoch 1510, training loss: 311.6791076660156 = 0.10451573878526688 + 50.0 * 6.231491565704346
Epoch 1510, val loss: 0.9789466857910156
Epoch 1520, training loss: 311.5528869628906 = 0.102435402572155 + 50.0 * 6.229008674621582
Epoch 1520, val loss: 0.9834049940109253
Epoch 1530, training loss: 311.5277099609375 = 0.10042128711938858 + 50.0 * 6.228545665740967
Epoch 1530, val loss: 0.9878517389297485
Epoch 1540, training loss: 311.6312561035156 = 0.09845056384801865 + 50.0 * 6.230656147003174
Epoch 1540, val loss: 0.9924097061157227
Epoch 1550, training loss: 311.8822937011719 = 0.09650443494319916 + 50.0 * 6.235715866088867
Epoch 1550, val loss: 0.9967131614685059
Epoch 1560, training loss: 311.59088134765625 = 0.09457831084728241 + 50.0 * 6.229926109313965
Epoch 1560, val loss: 1.001435399055481
Epoch 1570, training loss: 311.4524230957031 = 0.092719666659832 + 50.0 * 6.227194309234619
Epoch 1570, val loss: 1.0060516595840454
Epoch 1580, training loss: 311.4045715332031 = 0.09093740582466125 + 50.0 * 6.2262725830078125
Epoch 1580, val loss: 1.0107694864273071
Epoch 1590, training loss: 311.5979919433594 = 0.08920840919017792 + 50.0 * 6.230175971984863
Epoch 1590, val loss: 1.0154472589492798
Epoch 1600, training loss: 311.5601806640625 = 0.08746753633022308 + 50.0 * 6.229454517364502
Epoch 1600, val loss: 1.0199471712112427
Epoch 1610, training loss: 311.5145263671875 = 0.08574743568897247 + 50.0 * 6.228575229644775
Epoch 1610, val loss: 1.0247371196746826
Epoch 1620, training loss: 311.3916931152344 = 0.08409523218870163 + 50.0 * 6.226151466369629
Epoch 1620, val loss: 1.0292731523513794
Epoch 1630, training loss: 311.31060791015625 = 0.0824948325753212 + 50.0 * 6.224562168121338
Epoch 1630, val loss: 1.034101128578186
Epoch 1640, training loss: 311.39837646484375 = 0.08094713091850281 + 50.0 * 6.226348876953125
Epoch 1640, val loss: 1.0390485525131226
Epoch 1650, training loss: 311.34478759765625 = 0.0794006809592247 + 50.0 * 6.225307464599609
Epoch 1650, val loss: 1.043434500694275
Epoch 1660, training loss: 311.2767028808594 = 0.07788781076669693 + 50.0 * 6.2239766120910645
Epoch 1660, val loss: 1.0480878353118896
Epoch 1670, training loss: 311.2635498046875 = 0.0764240175485611 + 50.0 * 6.223742485046387
Epoch 1670, val loss: 1.0527706146240234
Epoch 1680, training loss: 311.471435546875 = 0.07500340044498444 + 50.0 * 6.227928638458252
Epoch 1680, val loss: 1.0576173067092896
Epoch 1690, training loss: 311.3094787597656 = 0.07359223067760468 + 50.0 * 6.22471809387207
Epoch 1690, val loss: 1.0623149871826172
Epoch 1700, training loss: 311.2376708984375 = 0.07220267504453659 + 50.0 * 6.223309516906738
Epoch 1700, val loss: 1.067030429840088
Epoch 1710, training loss: 311.1786804199219 = 0.07086421549320221 + 50.0 * 6.222156524658203
Epoch 1710, val loss: 1.071675419807434
Epoch 1720, training loss: 311.19635009765625 = 0.06956740468740463 + 50.0 * 6.222535133361816
Epoch 1720, val loss: 1.0765758752822876
Epoch 1730, training loss: 311.4063720703125 = 0.06830277293920517 + 50.0 * 6.226761341094971
Epoch 1730, val loss: 1.0812585353851318
Epoch 1740, training loss: 311.2734680175781 = 0.06703655421733856 + 50.0 * 6.224128723144531
Epoch 1740, val loss: 1.0860079526901245
Epoch 1750, training loss: 311.19903564453125 = 0.06578384339809418 + 50.0 * 6.222664833068848
Epoch 1750, val loss: 1.0907810926437378
Epoch 1760, training loss: 311.2649230957031 = 0.06457836180925369 + 50.0 * 6.2240071296691895
Epoch 1760, val loss: 1.0954902172088623
Epoch 1770, training loss: 311.1501770019531 = 0.06339982151985168 + 50.0 * 6.221735954284668
Epoch 1770, val loss: 1.1002259254455566
Epoch 1780, training loss: 311.1684875488281 = 0.06225767731666565 + 50.0 * 6.2221245765686035
Epoch 1780, val loss: 1.1050653457641602
Epoch 1790, training loss: 311.2680969238281 = 0.06113369017839432 + 50.0 * 6.224139213562012
Epoch 1790, val loss: 1.1097891330718994
Epoch 1800, training loss: 311.09423828125 = 0.06002698466181755 + 50.0 * 6.220684051513672
Epoch 1800, val loss: 1.114623785018921
Epoch 1810, training loss: 311.03497314453125 = 0.05895521491765976 + 50.0 * 6.219520092010498
Epoch 1810, val loss: 1.119332194328308
Epoch 1820, training loss: 311.2548522949219 = 0.05791604146361351 + 50.0 * 6.223938465118408
Epoch 1820, val loss: 1.1241436004638672
Epoch 1830, training loss: 311.1523132324219 = 0.0568685345351696 + 50.0 * 6.221909046173096
Epoch 1830, val loss: 1.1289725303649902
Epoch 1840, training loss: 311.0747375488281 = 0.05583828315138817 + 50.0 * 6.2203779220581055
Epoch 1840, val loss: 1.1334632635116577
Epoch 1850, training loss: 311.0242004394531 = 0.05485955998301506 + 50.0 * 6.219386577606201
Epoch 1850, val loss: 1.138351559638977
Epoch 1860, training loss: 311.0873718261719 = 0.05390811339020729 + 50.0 * 6.220669746398926
Epoch 1860, val loss: 1.143316388130188
Epoch 1870, training loss: 311.05560302734375 = 0.0529540479183197 + 50.0 * 6.220053195953369
Epoch 1870, val loss: 1.1479301452636719
Epoch 1880, training loss: 310.9561767578125 = 0.052015677094459534 + 50.0 * 6.218083381652832
Epoch 1880, val loss: 1.1524426937103271
Epoch 1890, training loss: 310.9313049316406 = 0.05110956355929375 + 50.0 * 6.21760368347168
Epoch 1890, val loss: 1.1572803258895874
Epoch 1900, training loss: 311.14190673828125 = 0.05023907497525215 + 50.0 * 6.2218337059021
Epoch 1900, val loss: 1.1621993780136108
Epoch 1910, training loss: 310.97296142578125 = 0.04936670884490013 + 50.0 * 6.218471527099609
Epoch 1910, val loss: 1.1666074991226196
Epoch 1920, training loss: 310.9744873046875 = 0.048509631305933 + 50.0 * 6.21851921081543
Epoch 1920, val loss: 1.1714035272598267
Epoch 1930, training loss: 311.0914001464844 = 0.04768349602818489 + 50.0 * 6.220874786376953
Epoch 1930, val loss: 1.17595636844635
Epoch 1940, training loss: 310.90081787109375 = 0.04685774818062782 + 50.0 * 6.2170796394348145
Epoch 1940, val loss: 1.1808061599731445
Epoch 1950, training loss: 310.8978576660156 = 0.04606332629919052 + 50.0 * 6.217036247253418
Epoch 1950, val loss: 1.185533881187439
Epoch 1960, training loss: 310.9790344238281 = 0.045295748859643936 + 50.0 * 6.218674659729004
Epoch 1960, val loss: 1.1903635263442993
Epoch 1970, training loss: 311.0262145996094 = 0.044530097395181656 + 50.0 * 6.21963357925415
Epoch 1970, val loss: 1.1949715614318848
Epoch 1980, training loss: 310.88763427734375 = 0.04376942291855812 + 50.0 * 6.216877460479736
Epoch 1980, val loss: 1.1995102167129517
Epoch 1990, training loss: 310.8437805175781 = 0.043029118329286575 + 50.0 * 6.216014862060547
Epoch 1990, val loss: 1.2040307521820068
Epoch 2000, training loss: 310.9100036621094 = 0.04231967777013779 + 50.0 * 6.217353343963623
Epoch 2000, val loss: 1.2088603973388672
Epoch 2010, training loss: 310.89862060546875 = 0.04161844030022621 + 50.0 * 6.217140197753906
Epoch 2010, val loss: 1.213499665260315
Epoch 2020, training loss: 310.83258056640625 = 0.040928080677986145 + 50.0 * 6.2158331871032715
Epoch 2020, val loss: 1.2179882526397705
Epoch 2030, training loss: 310.8824768066406 = 0.04026518762111664 + 50.0 * 6.21684455871582
Epoch 2030, val loss: 1.2226378917694092
Epoch 2040, training loss: 310.9571228027344 = 0.03960002586245537 + 50.0 * 6.218350410461426
Epoch 2040, val loss: 1.2273253202438354
Epoch 2050, training loss: 310.8519287109375 = 0.038935281336307526 + 50.0 * 6.216259479522705
Epoch 2050, val loss: 1.2318496704101562
Epoch 2060, training loss: 310.7864074707031 = 0.03831121698021889 + 50.0 * 6.214962005615234
Epoch 2060, val loss: 1.236472487449646
Epoch 2070, training loss: 310.96435546875 = 0.03770081698894501 + 50.0 * 6.218533515930176
Epoch 2070, val loss: 1.241255760192871
Epoch 2080, training loss: 310.76715087890625 = 0.03707559034228325 + 50.0 * 6.214601516723633
Epoch 2080, val loss: 1.245766520500183
Epoch 2090, training loss: 310.84515380859375 = 0.03647979721426964 + 50.0 * 6.2161736488342285
Epoch 2090, val loss: 1.2505252361297607
Epoch 2100, training loss: 310.7027893066406 = 0.03588993102312088 + 50.0 * 6.2133378982543945
Epoch 2100, val loss: 1.2547248601913452
Epoch 2110, training loss: 310.792724609375 = 0.03532465919852257 + 50.0 * 6.215147972106934
Epoch 2110, val loss: 1.2592345476150513
Epoch 2120, training loss: 310.8912353515625 = 0.03476547449827194 + 50.0 * 6.217129230499268
Epoch 2120, val loss: 1.2639321088790894
Epoch 2130, training loss: 310.7480163574219 = 0.03420605510473251 + 50.0 * 6.21427583694458
Epoch 2130, val loss: 1.2680658102035522
Epoch 2140, training loss: 310.71380615234375 = 0.033666301518678665 + 50.0 * 6.2136030197143555
Epoch 2140, val loss: 1.272558331489563
Epoch 2150, training loss: 311.06268310546875 = 0.03314816951751709 + 50.0 * 6.220590591430664
Epoch 2150, val loss: 1.2771320343017578
Epoch 2160, training loss: 310.8088073730469 = 0.03260963410139084 + 50.0 * 6.215523719787598
Epoch 2160, val loss: 1.2816461324691772
Epoch 2170, training loss: 310.70318603515625 = 0.032096099108457565 + 50.0 * 6.21342134475708
Epoch 2170, val loss: 1.2858396768569946
Epoch 2180, training loss: 310.64837646484375 = 0.03160325437784195 + 50.0 * 6.212335109710693
Epoch 2180, val loss: 1.29031240940094
Epoch 2190, training loss: 310.6741943359375 = 0.031127337366342545 + 50.0 * 6.21286153793335
Epoch 2190, val loss: 1.2947245836257935
Epoch 2200, training loss: 310.9030456542969 = 0.030657587572932243 + 50.0 * 6.217447280883789
Epoch 2200, val loss: 1.2989177703857422
Epoch 2210, training loss: 310.7132568359375 = 0.030168691650032997 + 50.0 * 6.2136616706848145
Epoch 2210, val loss: 1.3033579587936401
Epoch 2220, training loss: 310.6364440917969 = 0.029706943780183792 + 50.0 * 6.212134838104248
Epoch 2220, val loss: 1.3074874877929688
Epoch 2230, training loss: 310.68878173828125 = 0.02926178276538849 + 50.0 * 6.21319055557251
Epoch 2230, val loss: 1.3118993043899536
Epoch 2240, training loss: 310.64202880859375 = 0.02881976030766964 + 50.0 * 6.212264537811279
Epoch 2240, val loss: 1.3162885904312134
Epoch 2250, training loss: 310.61810302734375 = 0.028394630178809166 + 50.0 * 6.211794376373291
Epoch 2250, val loss: 1.3206533193588257
Epoch 2260, training loss: 310.7230529785156 = 0.02797854319214821 + 50.0 * 6.213901519775391
Epoch 2260, val loss: 1.3249130249023438
Epoch 2270, training loss: 310.6757507324219 = 0.027547040954232216 + 50.0 * 6.212964057922363
Epoch 2270, val loss: 1.3290791511535645
Epoch 2280, training loss: 310.6843566894531 = 0.02713850326836109 + 50.0 * 6.213144302368164
Epoch 2280, val loss: 1.3333206176757812
Epoch 2290, training loss: 310.67486572265625 = 0.026734614744782448 + 50.0 * 6.212962627410889
Epoch 2290, val loss: 1.3373019695281982
Epoch 2300, training loss: 310.6138000488281 = 0.026334477588534355 + 50.0 * 6.21174955368042
Epoch 2300, val loss: 1.3414751291275024
Epoch 2310, training loss: 310.5653076171875 = 0.025941988453269005 + 50.0 * 6.210787296295166
Epoch 2310, val loss: 1.3459198474884033
Epoch 2320, training loss: 310.5125732421875 = 0.02556924894452095 + 50.0 * 6.209740161895752
Epoch 2320, val loss: 1.3501205444335938
Epoch 2330, training loss: 310.5169372558594 = 0.025206249207258224 + 50.0 * 6.209834575653076
Epoch 2330, val loss: 1.3542675971984863
Epoch 2340, training loss: 310.76824951171875 = 0.02485543116927147 + 50.0 * 6.214868068695068
Epoch 2340, val loss: 1.358413815498352
Epoch 2350, training loss: 310.80682373046875 = 0.024489156901836395 + 50.0 * 6.215647220611572
Epoch 2350, val loss: 1.3620538711547852
Epoch 2360, training loss: 310.5755920410156 = 0.024123776704072952 + 50.0 * 6.211029529571533
Epoch 2360, val loss: 1.366364598274231
Epoch 2370, training loss: 310.5039367675781 = 0.023775354027748108 + 50.0 * 6.209603309631348
Epoch 2370, val loss: 1.3702259063720703
Epoch 2380, training loss: 310.48223876953125 = 0.023443862795829773 + 50.0 * 6.209176063537598
Epoch 2380, val loss: 1.3745741844177246
Epoch 2390, training loss: 310.5116882324219 = 0.023120909929275513 + 50.0 * 6.209771156311035
Epoch 2390, val loss: 1.378480076789856
Epoch 2400, training loss: 310.7491760253906 = 0.02280333824455738 + 50.0 * 6.214527606964111
Epoch 2400, val loss: 1.3822294473648071
Epoch 2410, training loss: 310.66400146484375 = 0.02248172089457512 + 50.0 * 6.212830066680908
Epoch 2410, val loss: 1.3859039545059204
Epoch 2420, training loss: 310.666259765625 = 0.022162314504384995 + 50.0 * 6.212882041931152
Epoch 2420, val loss: 1.3898682594299316
Epoch 2430, training loss: 310.5312194824219 = 0.021846886724233627 + 50.0 * 6.2101874351501465
Epoch 2430, val loss: 1.3939745426177979
Epoch 2440, training loss: 310.43017578125 = 0.021549109369516373 + 50.0 * 6.20817232131958
Epoch 2440, val loss: 1.3977088928222656
Epoch 2450, training loss: 310.47784423828125 = 0.021262098103761673 + 50.0 * 6.209131240844727
Epoch 2450, val loss: 1.4015411138534546
Epoch 2460, training loss: 310.4847717285156 = 0.020976165309548378 + 50.0 * 6.20927619934082
Epoch 2460, val loss: 1.4054949283599854
Epoch 2470, training loss: 310.4903259277344 = 0.0206924956291914 + 50.0 * 6.209392547607422
Epoch 2470, val loss: 1.4094523191452026
Epoch 2480, training loss: 310.5030212402344 = 0.02041732333600521 + 50.0 * 6.209651947021484
Epoch 2480, val loss: 1.4128655195236206
Epoch 2490, training loss: 310.6138610839844 = 0.02015228196978569 + 50.0 * 6.211874485015869
Epoch 2490, val loss: 1.4165592193603516
Epoch 2500, training loss: 310.5856628417969 = 0.019871333613991737 + 50.0 * 6.211316108703613
Epoch 2500, val loss: 1.4207412004470825
Epoch 2510, training loss: 310.4153137207031 = 0.01960773766040802 + 50.0 * 6.207913875579834
Epoch 2510, val loss: 1.4240086078643799
Epoch 2520, training loss: 310.4400634765625 = 0.019353117793798447 + 50.0 * 6.208414554595947
Epoch 2520, val loss: 1.428132176399231
Epoch 2530, training loss: 310.4827575683594 = 0.019106222316622734 + 50.0 * 6.209272861480713
Epoch 2530, val loss: 1.4314299821853638
Epoch 2540, training loss: 310.39056396484375 = 0.018853016197681427 + 50.0 * 6.207434177398682
Epoch 2540, val loss: 1.4352223873138428
Epoch 2550, training loss: 310.4879150390625 = 0.01861460506916046 + 50.0 * 6.209385871887207
Epoch 2550, val loss: 1.4387412071228027
Epoch 2560, training loss: 310.38873291015625 = 0.018373779952526093 + 50.0 * 6.207407474517822
Epoch 2560, val loss: 1.4423562288284302
Epoch 2570, training loss: 310.5600280761719 = 0.018140146508812904 + 50.0 * 6.2108378410339355
Epoch 2570, val loss: 1.4460095167160034
Epoch 2580, training loss: 310.3961486816406 = 0.017900235950946808 + 50.0 * 6.207564830780029
Epoch 2580, val loss: 1.4494844675064087
Epoch 2590, training loss: 310.33612060546875 = 0.017670338973402977 + 50.0 * 6.206368923187256
Epoch 2590, val loss: 1.4531166553497314
Epoch 2600, training loss: 310.3088073730469 = 0.017451422289013863 + 50.0 * 6.205826759338379
Epoch 2600, val loss: 1.4566270112991333
Epoch 2610, training loss: 310.37493896484375 = 0.01723930612206459 + 50.0 * 6.207153797149658
Epoch 2610, val loss: 1.4602491855621338
Epoch 2620, training loss: 310.4928894042969 = 0.01702680066227913 + 50.0 * 6.209517478942871
Epoch 2620, val loss: 1.4637291431427002
Epoch 2630, training loss: 310.40032958984375 = 0.016807807609438896 + 50.0 * 6.207670211791992
Epoch 2630, val loss: 1.4668991565704346
Epoch 2640, training loss: 310.3173522949219 = 0.016598138958215714 + 50.0 * 6.206015110015869
Epoch 2640, val loss: 1.4703545570373535
Epoch 2650, training loss: 310.37652587890625 = 0.01639780029654503 + 50.0 * 6.207202911376953
Epoch 2650, val loss: 1.473869800567627
Epoch 2660, training loss: 310.3952331542969 = 0.016197310760617256 + 50.0 * 6.20758056640625
Epoch 2660, val loss: 1.4771620035171509
Epoch 2670, training loss: 310.4214172363281 = 0.016002438962459564 + 50.0 * 6.208108425140381
Epoch 2670, val loss: 1.4801461696624756
Epoch 2680, training loss: 310.3703308105469 = 0.015803830698132515 + 50.0 * 6.207090377807617
Epoch 2680, val loss: 1.4839166402816772
Epoch 2690, training loss: 310.2647705078125 = 0.01561624277383089 + 50.0 * 6.204982757568359
Epoch 2690, val loss: 1.4871371984481812
Epoch 2700, training loss: 310.24237060546875 = 0.015430270694196224 + 50.0 * 6.204538345336914
Epoch 2700, val loss: 1.4904860258102417
Epoch 2710, training loss: 310.2830505371094 = 0.015253744088113308 + 50.0 * 6.205355644226074
Epoch 2710, val loss: 1.493879795074463
Epoch 2720, training loss: 310.5202941894531 = 0.015077762305736542 + 50.0 * 6.210104465484619
Epoch 2720, val loss: 1.4968681335449219
Epoch 2730, training loss: 310.3421325683594 = 0.014897974207997322 + 50.0 * 6.206544876098633
Epoch 2730, val loss: 1.4999111890792847
Epoch 2740, training loss: 310.3297424316406 = 0.014725771732628345 + 50.0 * 6.206299781799316
Epoch 2740, val loss: 1.5031044483184814
Epoch 2750, training loss: 310.2822570800781 = 0.014553799293935299 + 50.0 * 6.205353736877441
Epoch 2750, val loss: 1.506409764289856
Epoch 2760, training loss: 310.2933654785156 = 0.014387184754014015 + 50.0 * 6.20557975769043
Epoch 2760, val loss: 1.5096807479858398
Epoch 2770, training loss: 310.4244689941406 = 0.01422423031181097 + 50.0 * 6.208205223083496
Epoch 2770, val loss: 1.51262629032135
Epoch 2780, training loss: 310.2889404296875 = 0.0140630342066288 + 50.0 * 6.2054972648620605
Epoch 2780, val loss: 1.5156348943710327
Epoch 2790, training loss: 310.3013000488281 = 0.01390063390135765 + 50.0 * 6.205748558044434
Epoch 2790, val loss: 1.518860101699829
Epoch 2800, training loss: 310.28076171875 = 0.013746222481131554 + 50.0 * 6.205340385437012
Epoch 2800, val loss: 1.5219217538833618
Epoch 2810, training loss: 310.2117614746094 = 0.013593795709311962 + 50.0 * 6.203963756561279
Epoch 2810, val loss: 1.525202989578247
Epoch 2820, training loss: 310.28704833984375 = 0.01344752311706543 + 50.0 * 6.205471992492676
Epoch 2820, val loss: 1.5282875299453735
Epoch 2830, training loss: 310.28350830078125 = 0.013297989964485168 + 50.0 * 6.205403804779053
Epoch 2830, val loss: 1.5311137437820435
Epoch 2840, training loss: 310.2157897949219 = 0.013150990009307861 + 50.0 * 6.204052448272705
Epoch 2840, val loss: 1.53382408618927
Epoch 2850, training loss: 310.2746887207031 = 0.013010608963668346 + 50.0 * 6.205233573913574
Epoch 2850, val loss: 1.5373313426971436
Epoch 2860, training loss: 310.2403259277344 = 0.012870541773736477 + 50.0 * 6.2045488357543945
Epoch 2860, val loss: 1.539862871170044
Epoch 2870, training loss: 310.2574157714844 = 0.012730833142995834 + 50.0 * 6.204893589019775
Epoch 2870, val loss: 1.5425329208374023
Epoch 2880, training loss: 310.2807312011719 = 0.012596351094543934 + 50.0 * 6.205362319946289
Epoch 2880, val loss: 1.5456109046936035
Epoch 2890, training loss: 310.2047424316406 = 0.012457779608666897 + 50.0 * 6.203845977783203
Epoch 2890, val loss: 1.548242449760437
Epoch 2900, training loss: 310.14910888671875 = 0.01232491247355938 + 50.0 * 6.202735900878906
Epoch 2900, val loss: 1.5513447523117065
Epoch 2910, training loss: 310.114990234375 = 0.012199359014630318 + 50.0 * 6.202055931091309
Epoch 2910, val loss: 1.554276943206787
Epoch 2920, training loss: 310.2696533203125 = 0.012079516425728798 + 50.0 * 6.205151557922363
Epoch 2920, val loss: 1.5570590496063232
Epoch 2930, training loss: 310.1974182128906 = 0.011950518935918808 + 50.0 * 6.203709125518799
Epoch 2930, val loss: 1.5595873594284058
Epoch 2940, training loss: 310.1346435546875 = 0.011823033913969994 + 50.0 * 6.202456474304199
Epoch 2940, val loss: 1.5624370574951172
Epoch 2950, training loss: 310.16485595703125 = 0.011704978533089161 + 50.0 * 6.203063011169434
Epoch 2950, val loss: 1.5654324293136597
Epoch 2960, training loss: 310.34698486328125 = 0.011587655171751976 + 50.0 * 6.206707954406738
Epoch 2960, val loss: 1.5681753158569336
Epoch 2970, training loss: 310.2219543457031 = 0.01146728452295065 + 50.0 * 6.204209804534912
Epoch 2970, val loss: 1.5708203315734863
Epoch 2980, training loss: 310.11553955078125 = 0.011349556967616081 + 50.0 * 6.202083587646484
Epoch 2980, val loss: 1.5734968185424805
Epoch 2990, training loss: 310.1282958984375 = 0.011238106526434422 + 50.0 * 6.202341556549072
Epoch 2990, val loss: 1.5761643648147583
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7518518518518519
0.8402741170268846
The final CL Acc:0.74074, 0.00907, The final GNN Acc:0.83992, 0.00131
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11570])
remove edge: torch.Size([2, 9524])
updated graph: torch.Size([2, 10538])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7829895019531 = 1.9412940740585327 + 50.0 * 8.596834182739258
Epoch 0, val loss: 1.9281392097473145
Epoch 10, training loss: 431.7345275878906 = 1.9331659078598022 + 50.0 * 8.596027374267578
Epoch 10, val loss: 1.9200563430786133
Epoch 20, training loss: 431.4630126953125 = 1.9231523275375366 + 50.0 * 8.590797424316406
Epoch 20, val loss: 1.9096245765686035
Epoch 30, training loss: 429.58551025390625 = 1.9102765321731567 + 50.0 * 8.553504943847656
Epoch 30, val loss: 1.8959771394729614
Epoch 40, training loss: 414.69091796875 = 1.8946179151535034 + 50.0 * 8.255926132202148
Epoch 40, val loss: 1.8795047998428345
Epoch 50, training loss: 372.7200012207031 = 1.875871181488037 + 50.0 * 7.416882514953613
Epoch 50, val loss: 1.8604246377944946
Epoch 60, training loss: 358.6145935058594 = 1.8627656698226929 + 50.0 * 7.135036468505859
Epoch 60, val loss: 1.8490012884140015
Epoch 70, training loss: 350.9351501464844 = 1.851129174232483 + 50.0 * 6.981680393218994
Epoch 70, val loss: 1.8383196592330933
Epoch 80, training loss: 345.9444580078125 = 1.841289758682251 + 50.0 * 6.882063865661621
Epoch 80, val loss: 1.829521656036377
Epoch 90, training loss: 341.3967590332031 = 1.8320715427398682 + 50.0 * 6.791294097900391
Epoch 90, val loss: 1.8212003707885742
Epoch 100, training loss: 338.1517028808594 = 1.823653221130371 + 50.0 * 6.726561069488525
Epoch 100, val loss: 1.8136610984802246
Epoch 110, training loss: 335.6073303222656 = 1.8158053159713745 + 50.0 * 6.675830841064453
Epoch 110, val loss: 1.8067067861557007
Epoch 120, training loss: 333.61572265625 = 1.8085739612579346 + 50.0 * 6.636143207550049
Epoch 120, val loss: 1.800392746925354
Epoch 130, training loss: 332.11712646484375 = 1.8016763925552368 + 50.0 * 6.606309413909912
Epoch 130, val loss: 1.7943593263626099
Epoch 140, training loss: 330.6893005371094 = 1.7947313785552979 + 50.0 * 6.5778913497924805
Epoch 140, val loss: 1.788297176361084
Epoch 150, training loss: 329.5500793457031 = 1.7877633571624756 + 50.0 * 6.555246829986572
Epoch 150, val loss: 1.7821376323699951
Epoch 160, training loss: 328.5718078613281 = 1.7806082963943481 + 50.0 * 6.535823822021484
Epoch 160, val loss: 1.775718331336975
Epoch 170, training loss: 327.56890869140625 = 1.773101806640625 + 50.0 * 6.515916347503662
Epoch 170, val loss: 1.7690154314041138
Epoch 180, training loss: 326.6685791015625 = 1.7651777267456055 + 50.0 * 6.498068332672119
Epoch 180, val loss: 1.761948823928833
Epoch 190, training loss: 325.8885192871094 = 1.7566184997558594 + 50.0 * 6.482637882232666
Epoch 190, val loss: 1.7542710304260254
Epoch 200, training loss: 325.21160888671875 = 1.7473576068878174 + 50.0 * 6.469285011291504
Epoch 200, val loss: 1.7460721731185913
Epoch 210, training loss: 324.5823974609375 = 1.737337589263916 + 50.0 * 6.4569010734558105
Epoch 210, val loss: 1.7371621131896973
Epoch 220, training loss: 324.064453125 = 1.7265011072158813 + 50.0 * 6.446759223937988
Epoch 220, val loss: 1.7275104522705078
Epoch 230, training loss: 323.7060852050781 = 1.7146769762039185 + 50.0 * 6.439827919006348
Epoch 230, val loss: 1.7170156240463257
Epoch 240, training loss: 323.12225341796875 = 1.701884150505066 + 50.0 * 6.428407669067383
Epoch 240, val loss: 1.7056294679641724
Epoch 250, training loss: 322.6968078613281 = 1.6882314682006836 + 50.0 * 6.42017126083374
Epoch 250, val loss: 1.6935834884643555
Epoch 260, training loss: 322.30413818359375 = 1.6736619472503662 + 50.0 * 6.412609100341797
Epoch 260, val loss: 1.6807184219360352
Epoch 270, training loss: 321.9317626953125 = 1.6581567525863647 + 50.0 * 6.405472278594971
Epoch 270, val loss: 1.6670873165130615
Epoch 280, training loss: 322.0078430175781 = 1.6417043209075928 + 50.0 * 6.407322883605957
Epoch 280, val loss: 1.6525717973709106
Epoch 290, training loss: 321.50091552734375 = 1.6241320371627808 + 50.0 * 6.39753532409668
Epoch 290, val loss: 1.6373053789138794
Epoch 300, training loss: 321.02197265625 = 1.6058638095855713 + 50.0 * 6.388321876525879
Epoch 300, val loss: 1.6214650869369507
Epoch 310, training loss: 320.710205078125 = 1.5868844985961914 + 50.0 * 6.3824663162231445
Epoch 310, val loss: 1.6051232814788818
Epoch 320, training loss: 320.444091796875 = 1.5672968626022339 + 50.0 * 6.377535820007324
Epoch 320, val loss: 1.5883843898773193
Epoch 330, training loss: 320.38848876953125 = 1.5470961332321167 + 50.0 * 6.376827716827393
Epoch 330, val loss: 1.5712443590164185
Epoch 340, training loss: 320.0589599609375 = 1.526287317276001 + 50.0 * 6.3706536293029785
Epoch 340, val loss: 1.553761601448059
Epoch 350, training loss: 319.7294921875 = 1.505136489868164 + 50.0 * 6.364487171173096
Epoch 350, val loss: 1.5363322496414185
Epoch 360, training loss: 319.50457763671875 = 1.4837321043014526 + 50.0 * 6.360416889190674
Epoch 360, val loss: 1.518929362297058
Epoch 370, training loss: 319.3280029296875 = 1.4619886875152588 + 50.0 * 6.357320785522461
Epoch 370, val loss: 1.501309871673584
Epoch 380, training loss: 319.0760498046875 = 1.4400631189346313 + 50.0 * 6.352719783782959
Epoch 380, val loss: 1.4840087890625
Epoch 390, training loss: 319.0155334472656 = 1.4180768728256226 + 50.0 * 6.3519487380981445
Epoch 390, val loss: 1.4668259620666504
Epoch 400, training loss: 318.698974609375 = 1.3960020542144775 + 50.0 * 6.346059799194336
Epoch 400, val loss: 1.4497218132019043
Epoch 410, training loss: 318.51666259765625 = 1.3739734888076782 + 50.0 * 6.342854022979736
Epoch 410, val loss: 1.4330195188522339
Epoch 420, training loss: 318.5336608886719 = 1.3519238233566284 + 50.0 * 6.343634605407715
Epoch 420, val loss: 1.4164316654205322
Epoch 430, training loss: 318.2437744140625 = 1.329802393913269 + 50.0 * 6.3382792472839355
Epoch 430, val loss: 1.4000533819198608
Epoch 440, training loss: 318.0501403808594 = 1.3077770471572876 + 50.0 * 6.334847450256348
Epoch 440, val loss: 1.3840901851654053
Epoch 450, training loss: 317.8473815917969 = 1.285978078842163 + 50.0 * 6.331228256225586
Epoch 450, val loss: 1.3684017658233643
Epoch 460, training loss: 317.8124694824219 = 1.2643855810165405 + 50.0 * 6.330962181091309
Epoch 460, val loss: 1.3530538082122803
Epoch 470, training loss: 317.61822509765625 = 1.24288010597229 + 50.0 * 6.3275065422058105
Epoch 470, val loss: 1.337852120399475
Epoch 480, training loss: 317.5120544433594 = 1.2216757535934448 + 50.0 * 6.325807571411133
Epoch 480, val loss: 1.3234695196151733
Epoch 490, training loss: 317.3326416015625 = 1.2007166147232056 + 50.0 * 6.322638511657715
Epoch 490, val loss: 1.3090440034866333
Epoch 500, training loss: 317.15008544921875 = 1.180082082748413 + 50.0 * 6.319399833679199
Epoch 500, val loss: 1.2953382730484009
Epoch 510, training loss: 317.01300048828125 = 1.159826636314392 + 50.0 * 6.317063808441162
Epoch 510, val loss: 1.2819857597351074
Epoch 520, training loss: 316.9184875488281 = 1.1400262117385864 + 50.0 * 6.3155694007873535
Epoch 520, val loss: 1.2695002555847168
Epoch 530, training loss: 316.8336486816406 = 1.120479702949524 + 50.0 * 6.314263343811035
Epoch 530, val loss: 1.2570934295654297
Epoch 540, training loss: 316.68035888671875 = 1.101283073425293 + 50.0 * 6.311581134796143
Epoch 540, val loss: 1.2453155517578125
Epoch 550, training loss: 316.6402282714844 = 1.0825648307800293 + 50.0 * 6.311153411865234
Epoch 550, val loss: 1.234316349029541
Epoch 560, training loss: 316.4874267578125 = 1.0641919374465942 + 50.0 * 6.308465003967285
Epoch 560, val loss: 1.2233942747116089
Epoch 570, training loss: 316.2969665527344 = 1.046234369277954 + 50.0 * 6.305014610290527
Epoch 570, val loss: 1.213309645652771
Epoch 580, training loss: 316.2162170410156 = 1.02877938747406 + 50.0 * 6.303749084472656
Epoch 580, val loss: 1.2038205862045288
Epoch 590, training loss: 316.25323486328125 = 1.0115907192230225 + 50.0 * 6.304832935333252
Epoch 590, val loss: 1.1945278644561768
Epoch 600, training loss: 316.1737976074219 = 0.9947521090507507 + 50.0 * 6.3035807609558105
Epoch 600, val loss: 1.1859835386276245
Epoch 610, training loss: 315.9805908203125 = 0.9782419204711914 + 50.0 * 6.300046920776367
Epoch 610, val loss: 1.177431344985962
Epoch 620, training loss: 315.8616943359375 = 0.9621676802635193 + 50.0 * 6.297990798950195
Epoch 620, val loss: 1.1698602437973022
Epoch 630, training loss: 315.69110107421875 = 0.9464206099510193 + 50.0 * 6.294893741607666
Epoch 630, val loss: 1.1624360084533691
Epoch 640, training loss: 315.6727294921875 = 0.9310086965560913 + 50.0 * 6.294834613800049
Epoch 640, val loss: 1.1554991006851196
Epoch 650, training loss: 315.65484619140625 = 0.9157707095146179 + 50.0 * 6.294781684875488
Epoch 650, val loss: 1.1490527391433716
Epoch 660, training loss: 315.44482421875 = 0.9007496237754822 + 50.0 * 6.290881156921387
Epoch 660, val loss: 1.1424999237060547
Epoch 670, training loss: 315.5138244628906 = 0.8860631585121155 + 50.0 * 6.29255485534668
Epoch 670, val loss: 1.1367729902267456
Epoch 680, training loss: 315.2734069824219 = 0.8715022206306458 + 50.0 * 6.28803825378418
Epoch 680, val loss: 1.1307454109191895
Epoch 690, training loss: 315.1647033691406 = 0.8572694063186646 + 50.0 * 6.286148548126221
Epoch 690, val loss: 1.1256228685379028
Epoch 700, training loss: 315.3141784667969 = 0.8431735634803772 + 50.0 * 6.289420127868652
Epoch 700, val loss: 1.1201986074447632
Epoch 710, training loss: 315.0570068359375 = 0.8292319774627686 + 50.0 * 6.284555912017822
Epoch 710, val loss: 1.1157288551330566
Epoch 720, training loss: 314.9953918457031 = 0.8154672384262085 + 50.0 * 6.28359842300415
Epoch 720, val loss: 1.1109660863876343
Epoch 730, training loss: 314.8731689453125 = 0.8017802834510803 + 50.0 * 6.281427383422852
Epoch 730, val loss: 1.1063660383224487
Epoch 740, training loss: 314.82281494140625 = 0.7882673740386963 + 50.0 * 6.280691146850586
Epoch 740, val loss: 1.1021170616149902
Epoch 750, training loss: 315.1776428222656 = 0.7748598456382751 + 50.0 * 6.288055419921875
Epoch 750, val loss: 1.097874641418457
Epoch 760, training loss: 314.7039794921875 = 0.7614846229553223 + 50.0 * 6.278850078582764
Epoch 760, val loss: 1.0938557386398315
Epoch 770, training loss: 314.57403564453125 = 0.7483639717102051 + 50.0 * 6.276513576507568
Epoch 770, val loss: 1.0901944637298584
Epoch 780, training loss: 314.5400085449219 = 0.735442578792572 + 50.0 * 6.276091575622559
Epoch 780, val loss: 1.086762547492981
Epoch 790, training loss: 314.7144470214844 = 0.7225537896156311 + 50.0 * 6.279837608337402
Epoch 790, val loss: 1.0832316875457764
Epoch 800, training loss: 314.39886474609375 = 0.7095902562141418 + 50.0 * 6.27378511428833
Epoch 800, val loss: 1.079906940460205
Epoch 810, training loss: 314.2936706542969 = 0.6969403028488159 + 50.0 * 6.271934986114502
Epoch 810, val loss: 1.0770153999328613
Epoch 820, training loss: 314.4930725097656 = 0.6844748258590698 + 50.0 * 6.276172161102295
Epoch 820, val loss: 1.0743969678878784
Epoch 830, training loss: 314.3861389160156 = 0.6717909574508667 + 50.0 * 6.274287223815918
Epoch 830, val loss: 1.071144700050354
Epoch 840, training loss: 314.15234375 = 0.6594118475914001 + 50.0 * 6.269858360290527
Epoch 840, val loss: 1.0688968896865845
Epoch 850, training loss: 314.25830078125 = 0.6471896767616272 + 50.0 * 6.27222204208374
Epoch 850, val loss: 1.0666186809539795
Epoch 860, training loss: 314.02947998046875 = 0.63503098487854 + 50.0 * 6.267889022827148
Epoch 860, val loss: 1.0640462636947632
Epoch 870, training loss: 314.0238342285156 = 0.6230812072753906 + 50.0 * 6.268014907836914
Epoch 870, val loss: 1.0621111392974854
Epoch 880, training loss: 314.0022277832031 = 0.6112177968025208 + 50.0 * 6.267820358276367
Epoch 880, val loss: 1.060530185699463
Epoch 890, training loss: 313.9022216796875 = 0.5994660258293152 + 50.0 * 6.266055583953857
Epoch 890, val loss: 1.0588741302490234
Epoch 900, training loss: 314.0722351074219 = 0.5878758430480957 + 50.0 * 6.269686698913574
Epoch 900, val loss: 1.057456374168396
Epoch 910, training loss: 313.771728515625 = 0.5762958526611328 + 50.0 * 6.263908863067627
Epoch 910, val loss: 1.056100606918335
Epoch 920, training loss: 313.67242431640625 = 0.5649693012237549 + 50.0 * 6.262149333953857
Epoch 920, val loss: 1.0550867319107056
Epoch 930, training loss: 313.67724609375 = 0.5538694858551025 + 50.0 * 6.262467384338379
Epoch 930, val loss: 1.0545241832733154
Epoch 940, training loss: 313.8758239746094 = 0.5426931977272034 + 50.0 * 6.26666259765625
Epoch 940, val loss: 1.0530511140823364
Epoch 950, training loss: 313.6947326660156 = 0.5316262245178223 + 50.0 * 6.2632622718811035
Epoch 950, val loss: 1.052856683731079
Epoch 960, training loss: 313.49700927734375 = 0.5208492875099182 + 50.0 * 6.259523391723633
Epoch 960, val loss: 1.0525776147842407
Epoch 970, training loss: 313.4355773925781 = 0.5103407502174377 + 50.0 * 6.258504867553711
Epoch 970, val loss: 1.0523465871810913
Epoch 980, training loss: 313.4042053222656 = 0.5000026226043701 + 50.0 * 6.258084297180176
Epoch 980, val loss: 1.0526117086410522
Epoch 990, training loss: 313.8390808105469 = 0.48967698216438293 + 50.0 * 6.2669878005981445
Epoch 990, val loss: 1.0528154373168945
Epoch 1000, training loss: 313.3705139160156 = 0.47930777072906494 + 50.0 * 6.257823944091797
Epoch 1000, val loss: 1.052785873413086
Epoch 1010, training loss: 313.2669677734375 = 0.46928226947784424 + 50.0 * 6.255954265594482
Epoch 1010, val loss: 1.053085207939148
Epoch 1020, training loss: 313.23370361328125 = 0.4595395624637604 + 50.0 * 6.255483627319336
Epoch 1020, val loss: 1.0539065599441528
Epoch 1030, training loss: 313.19610595703125 = 0.4499503970146179 + 50.0 * 6.254922866821289
Epoch 1030, val loss: 1.0548568964004517
Epoch 1040, training loss: 313.3738098144531 = 0.4404733180999756 + 50.0 * 6.2586669921875
Epoch 1040, val loss: 1.0557422637939453
Epoch 1050, training loss: 313.47589111328125 = 0.43107789754867554 + 50.0 * 6.2608962059021
Epoch 1050, val loss: 1.0566635131835938
Epoch 1060, training loss: 313.14080810546875 = 0.42169222235679626 + 50.0 * 6.254382133483887
Epoch 1060, val loss: 1.057357668876648
Epoch 1070, training loss: 313.01312255859375 = 0.4126458764076233 + 50.0 * 6.252009391784668
Epoch 1070, val loss: 1.0588173866271973
Epoch 1080, training loss: 313.146728515625 = 0.40383827686309814 + 50.0 * 6.254858016967773
Epoch 1080, val loss: 1.0602362155914307
Epoch 1090, training loss: 312.97344970703125 = 0.395049512386322 + 50.0 * 6.251567840576172
Epoch 1090, val loss: 1.0615463256835938
Epoch 1100, training loss: 312.9039611816406 = 0.3864913582801819 + 50.0 * 6.250349044799805
Epoch 1100, val loss: 1.0634269714355469
Epoch 1110, training loss: 312.8596496582031 = 0.3781365156173706 + 50.0 * 6.249629974365234
Epoch 1110, val loss: 1.0652859210968018
Epoch 1120, training loss: 313.0481872558594 = 0.3699531555175781 + 50.0 * 6.253564357757568
Epoch 1120, val loss: 1.0671733617782593
Epoch 1130, training loss: 312.8782958984375 = 0.36186233162879944 + 50.0 * 6.250328063964844
Epoch 1130, val loss: 1.0694661140441895
Epoch 1140, training loss: 312.98846435546875 = 0.35392633080482483 + 50.0 * 6.25269079208374
Epoch 1140, val loss: 1.071438193321228
Epoch 1150, training loss: 312.91156005859375 = 0.34613990783691406 + 50.0 * 6.251308441162109
Epoch 1150, val loss: 1.0736720561981201
Epoch 1160, training loss: 312.7860412597656 = 0.3385147750377655 + 50.0 * 6.248950958251953
Epoch 1160, val loss: 1.0761133432388306
Epoch 1170, training loss: 312.6913757324219 = 0.3310876190662384 + 50.0 * 6.24720573425293
Epoch 1170, val loss: 1.0788240432739258
Epoch 1180, training loss: 312.836181640625 = 0.3239022493362427 + 50.0 * 6.250245094299316
Epoch 1180, val loss: 1.0815106630325317
Epoch 1190, training loss: 312.6003723144531 = 0.31676575541496277 + 50.0 * 6.24567174911499
Epoch 1190, val loss: 1.0845104455947876
Epoch 1200, training loss: 312.56060791015625 = 0.3098237216472626 + 50.0 * 6.245016098022461
Epoch 1200, val loss: 1.087446689605713
Epoch 1210, training loss: 312.73089599609375 = 0.3031095862388611 + 50.0 * 6.248556137084961
Epoch 1210, val loss: 1.0906199216842651
Epoch 1220, training loss: 312.54998779296875 = 0.29644477367401123 + 50.0 * 6.245070934295654
Epoch 1220, val loss: 1.0937553644180298
Epoch 1230, training loss: 312.5458068847656 = 0.2899394631385803 + 50.0 * 6.2451171875
Epoch 1230, val loss: 1.0968539714813232
Epoch 1240, training loss: 312.62652587890625 = 0.2836438715457916 + 50.0 * 6.2468581199646
Epoch 1240, val loss: 1.100327730178833
Epoch 1250, training loss: 312.4310607910156 = 0.2774890661239624 + 50.0 * 6.243071556091309
Epoch 1250, val loss: 1.1041529178619385
Epoch 1260, training loss: 312.4821472167969 = 0.2715138792991638 + 50.0 * 6.244212627410889
Epoch 1260, val loss: 1.1078815460205078
Epoch 1270, training loss: 312.60540771484375 = 0.2656019926071167 + 50.0 * 6.246796131134033
Epoch 1270, val loss: 1.1112605333328247
Epoch 1280, training loss: 312.3713684082031 = 0.2597913444042206 + 50.0 * 6.242231369018555
Epoch 1280, val loss: 1.1151384115219116
Epoch 1290, training loss: 312.3125915527344 = 0.2541854679584503 + 50.0 * 6.241168022155762
Epoch 1290, val loss: 1.1191874742507935
Epoch 1300, training loss: 312.5323486328125 = 0.24875181913375854 + 50.0 * 6.245672225952148
Epoch 1300, val loss: 1.1233155727386475
Epoch 1310, training loss: 312.3639831542969 = 0.24330094456672668 + 50.0 * 6.2424139976501465
Epoch 1310, val loss: 1.1271934509277344
Epoch 1320, training loss: 312.3115539550781 = 0.23804675042629242 + 50.0 * 6.2414703369140625
Epoch 1320, val loss: 1.1316189765930176
Epoch 1330, training loss: 312.33746337890625 = 0.23294900357723236 + 50.0 * 6.242090702056885
Epoch 1330, val loss: 1.1358842849731445
Epoch 1340, training loss: 312.3490295410156 = 0.2279129922389984 + 50.0 * 6.242422580718994
Epoch 1340, val loss: 1.1396836042404175
Epoch 1350, training loss: 312.1634826660156 = 0.2230278104543686 + 50.0 * 6.238809108734131
Epoch 1350, val loss: 1.1443828344345093
Epoch 1360, training loss: 312.1030578613281 = 0.21826650202274323 + 50.0 * 6.237695693969727
Epoch 1360, val loss: 1.1490172147750854
Epoch 1370, training loss: 312.0943298339844 = 0.21367311477661133 + 50.0 * 6.237613201141357
Epoch 1370, val loss: 1.153621792793274
Epoch 1380, training loss: 312.296142578125 = 0.20918874442577362 + 50.0 * 6.241738796234131
Epoch 1380, val loss: 1.1583501100540161
Epoch 1390, training loss: 312.3171691894531 = 0.2046307772397995 + 50.0 * 6.242250919342041
Epoch 1390, val loss: 1.1624341011047363
Epoch 1400, training loss: 312.1298828125 = 0.20024947822093964 + 50.0 * 6.238592624664307
Epoch 1400, val loss: 1.1671439409255981
Epoch 1410, training loss: 312.0281677246094 = 0.1959816962480545 + 50.0 * 6.2366437911987305
Epoch 1410, val loss: 1.171851396560669
Epoch 1420, training loss: 312.1982421875 = 0.191853865981102 + 50.0 * 6.2401275634765625
Epoch 1420, val loss: 1.1764960289001465
Epoch 1430, training loss: 311.9712829589844 = 0.18780401349067688 + 50.0 * 6.2356696128845215
Epoch 1430, val loss: 1.1815992593765259
Epoch 1440, training loss: 312.1825256347656 = 0.18386442959308624 + 50.0 * 6.239973068237305
Epoch 1440, val loss: 1.1865856647491455
Epoch 1450, training loss: 311.9421691894531 = 0.1799214482307434 + 50.0 * 6.2352447509765625
Epoch 1450, val loss: 1.1905994415283203
Epoch 1460, training loss: 311.9228210449219 = 0.17613135278224945 + 50.0 * 6.234933853149414
Epoch 1460, val loss: 1.195719599723816
Epoch 1470, training loss: 312.0870361328125 = 0.17247165739536285 + 50.0 * 6.238291263580322
Epoch 1470, val loss: 1.200310230255127
Epoch 1480, training loss: 311.8564453125 = 0.16884511709213257 + 50.0 * 6.2337517738342285
Epoch 1480, val loss: 1.2054749727249146
Epoch 1490, training loss: 311.8257141113281 = 0.16533240675926208 + 50.0 * 6.2332072257995605
Epoch 1490, val loss: 1.210250973701477
Epoch 1500, training loss: 311.8641662597656 = 0.16192176938056946 + 50.0 * 6.234044551849365
Epoch 1500, val loss: 1.2154020071029663
Epoch 1510, training loss: 311.9738464355469 = 0.1585480123758316 + 50.0 * 6.236306190490723
Epoch 1510, val loss: 1.2200678586959839
Epoch 1520, training loss: 311.7881774902344 = 0.15517836809158325 + 50.0 * 6.232659816741943
Epoch 1520, val loss: 1.2247995138168335
Epoch 1530, training loss: 311.771484375 = 0.15197069942951202 + 50.0 * 6.2323899269104
Epoch 1530, val loss: 1.2299270629882812
Epoch 1540, training loss: 311.74102783203125 = 0.14886268973350525 + 50.0 * 6.2318434715271
Epoch 1540, val loss: 1.2349311113357544
Epoch 1550, training loss: 311.8152160644531 = 0.14586040377616882 + 50.0 * 6.233386993408203
Epoch 1550, val loss: 1.2399811744689941
Epoch 1560, training loss: 311.71856689453125 = 0.14284849166870117 + 50.0 * 6.2315144538879395
Epoch 1560, val loss: 1.244955062866211
Epoch 1570, training loss: 312.0560302734375 = 0.13990555703639984 + 50.0 * 6.238322734832764
Epoch 1570, val loss: 1.2496649026870728
Epoch 1580, training loss: 311.8512878417969 = 0.13702766597270966 + 50.0 * 6.234285354614258
Epoch 1580, val loss: 1.2549397945404053
Epoch 1590, training loss: 311.6349182128906 = 0.13420403003692627 + 50.0 * 6.230014324188232
Epoch 1590, val loss: 1.2599563598632812
Epoch 1600, training loss: 311.6723937988281 = 0.13150708377361298 + 50.0 * 6.230817794799805
Epoch 1600, val loss: 1.2650971412658691
Epoch 1610, training loss: 311.7154235839844 = 0.12887881696224213 + 50.0 * 6.231730937957764
Epoch 1610, val loss: 1.2703006267547607
Epoch 1620, training loss: 311.608642578125 = 0.12626898288726807 + 50.0 * 6.229647159576416
Epoch 1620, val loss: 1.2750592231750488
Epoch 1630, training loss: 311.6090087890625 = 0.12376326322555542 + 50.0 * 6.229705333709717
Epoch 1630, val loss: 1.2801941633224487
Epoch 1640, training loss: 311.64593505859375 = 0.12131112068891525 + 50.0 * 6.23049259185791
Epoch 1640, val loss: 1.2852529287338257
Epoch 1650, training loss: 311.79107666015625 = 0.11889642477035522 + 50.0 * 6.233443737030029
Epoch 1650, val loss: 1.290261149406433
Epoch 1660, training loss: 311.5576477050781 = 0.11646541953086853 + 50.0 * 6.228824138641357
Epoch 1660, val loss: 1.2954373359680176
Epoch 1670, training loss: 311.5578918457031 = 0.11417478322982788 + 50.0 * 6.228874683380127
Epoch 1670, val loss: 1.3006095886230469
Epoch 1680, training loss: 311.5545349121094 = 0.11192617565393448 + 50.0 * 6.228851795196533
Epoch 1680, val loss: 1.305490255355835
Epoch 1690, training loss: 311.51007080078125 = 0.10972963273525238 + 50.0 * 6.228006839752197
Epoch 1690, val loss: 1.3104076385498047
Epoch 1700, training loss: 311.48162841796875 = 0.10758557170629501 + 50.0 * 6.227481365203857
Epoch 1700, val loss: 1.3154809474945068
Epoch 1710, training loss: 311.46917724609375 = 0.10552140325307846 + 50.0 * 6.227272987365723
Epoch 1710, val loss: 1.321032166481018
Epoch 1720, training loss: 311.8019714355469 = 0.10349871963262558 + 50.0 * 6.233969688415527
Epoch 1720, val loss: 1.3258209228515625
Epoch 1730, training loss: 311.48602294921875 = 0.10142727196216583 + 50.0 * 6.227691650390625
Epoch 1730, val loss: 1.330419659614563
Epoch 1740, training loss: 311.3795471191406 = 0.09945567697286606 + 50.0 * 6.225601673126221
Epoch 1740, val loss: 1.3355672359466553
Epoch 1750, training loss: 311.3297119140625 = 0.09758330881595612 + 50.0 * 6.224642276763916
Epoch 1750, val loss: 1.340932011604309
Epoch 1760, training loss: 311.3396911621094 = 0.0957622155547142 + 50.0 * 6.224878311157227
Epoch 1760, val loss: 1.346064805984497
Epoch 1770, training loss: 311.7987060546875 = 0.09396837651729584 + 50.0 * 6.234095096588135
Epoch 1770, val loss: 1.3508832454681396
Epoch 1780, training loss: 311.44378662109375 = 0.09212558716535568 + 50.0 * 6.2270331382751465
Epoch 1780, val loss: 1.3552757501602173
Epoch 1790, training loss: 311.279541015625 = 0.09036938846111298 + 50.0 * 6.223783493041992
Epoch 1790, val loss: 1.3606438636779785
Epoch 1800, training loss: 311.2790222167969 = 0.08870650827884674 + 50.0 * 6.223805904388428
Epoch 1800, val loss: 1.3658146858215332
Epoch 1810, training loss: 311.52484130859375 = 0.08708207309246063 + 50.0 * 6.228754997253418
Epoch 1810, val loss: 1.3705281019210815
Epoch 1820, training loss: 311.3307800292969 = 0.08546100556850433 + 50.0 * 6.2249064445495605
Epoch 1820, val loss: 1.3758223056793213
Epoch 1830, training loss: 311.3441162109375 = 0.08386650681495667 + 50.0 * 6.225204944610596
Epoch 1830, val loss: 1.380495309829712
Epoch 1840, training loss: 311.5173645019531 = 0.0823592022061348 + 50.0 * 6.228700160980225
Epoch 1840, val loss: 1.3855911493301392
Epoch 1850, training loss: 311.26116943359375 = 0.08078917115926743 + 50.0 * 6.223608016967773
Epoch 1850, val loss: 1.3903796672821045
Epoch 1860, training loss: 311.2245178222656 = 0.07931540906429291 + 50.0 * 6.222904205322266
Epoch 1860, val loss: 1.3952901363372803
Epoch 1870, training loss: 311.24859619140625 = 0.07790356874465942 + 50.0 * 6.223413467407227
Epoch 1870, val loss: 1.400406837463379
Epoch 1880, training loss: 311.2364807128906 = 0.07650591433048248 + 50.0 * 6.223199367523193
Epoch 1880, val loss: 1.4052066802978516
Epoch 1890, training loss: 311.1324157714844 = 0.07514799386262894 + 50.0 * 6.2211456298828125
Epoch 1890, val loss: 1.4107463359832764
Epoch 1900, training loss: 311.1324157714844 = 0.07383313775062561 + 50.0 * 6.221171855926514
Epoch 1900, val loss: 1.415796160697937
Epoch 1910, training loss: 311.6821594238281 = 0.0725613385438919 + 50.0 * 6.232191562652588
Epoch 1910, val loss: 1.4208011627197266
Epoch 1920, training loss: 311.3881530761719 = 0.07120151817798615 + 50.0 * 6.226338863372803
Epoch 1920, val loss: 1.4246175289154053
Epoch 1930, training loss: 311.1304626464844 = 0.06991284340620041 + 50.0 * 6.221210956573486
Epoch 1930, val loss: 1.429868221282959
Epoch 1940, training loss: 311.1235656738281 = 0.0686996802687645 + 50.0 * 6.221097469329834
Epoch 1940, val loss: 1.4349498748779297
Epoch 1950, training loss: 311.37982177734375 = 0.0675259530544281 + 50.0 * 6.226245880126953
Epoch 1950, val loss: 1.4393504858016968
Epoch 1960, training loss: 311.1599426269531 = 0.06632377207279205 + 50.0 * 6.221872806549072
Epoch 1960, val loss: 1.4449125528335571
Epoch 1970, training loss: 311.05987548828125 = 0.0651850700378418 + 50.0 * 6.219893932342529
Epoch 1970, val loss: 1.449696660041809
Epoch 1980, training loss: 311.2254638671875 = 0.0640949010848999 + 50.0 * 6.223227500915527
Epoch 1980, val loss: 1.4547371864318848
Epoch 1990, training loss: 310.9892272949219 = 0.06296590715646744 + 50.0 * 6.218525409698486
Epoch 1990, val loss: 1.4591554403305054
Epoch 2000, training loss: 310.9849548339844 = 0.06188393011689186 + 50.0 * 6.218461513519287
Epoch 2000, val loss: 1.464297890663147
Epoch 2010, training loss: 311.111572265625 = 0.06085828319191933 + 50.0 * 6.221014499664307
Epoch 2010, val loss: 1.4692679643630981
Epoch 2020, training loss: 311.03631591796875 = 0.059819549322128296 + 50.0 * 6.21953010559082
Epoch 2020, val loss: 1.4738115072250366
Epoch 2030, training loss: 311.0464172363281 = 0.05881141126155853 + 50.0 * 6.219752311706543
Epoch 2030, val loss: 1.4790534973144531
Epoch 2040, training loss: 311.0892028808594 = 0.05782770738005638 + 50.0 * 6.220627307891846
Epoch 2040, val loss: 1.483742594718933
Epoch 2050, training loss: 311.1535949707031 = 0.05686211585998535 + 50.0 * 6.221934795379639
Epoch 2050, val loss: 1.4883754253387451
Epoch 2060, training loss: 311.04296875 = 0.05591630935668945 + 50.0 * 6.219741344451904
Epoch 2060, val loss: 1.4931483268737793
Epoch 2070, training loss: 310.941162109375 = 0.05499635636806488 + 50.0 * 6.217723369598389
Epoch 2070, val loss: 1.4985864162445068
Epoch 2080, training loss: 310.9888000488281 = 0.05411609262228012 + 50.0 * 6.218693733215332
Epoch 2080, val loss: 1.5034103393554688
Epoch 2090, training loss: 311.1261291503906 = 0.05324465036392212 + 50.0 * 6.221457481384277
Epoch 2090, val loss: 1.5078893899917603
Epoch 2100, training loss: 310.9416809082031 = 0.05234147980809212 + 50.0 * 6.21778678894043
Epoch 2100, val loss: 1.512689471244812
Epoch 2110, training loss: 310.920654296875 = 0.05150815472006798 + 50.0 * 6.217382907867432
Epoch 2110, val loss: 1.5181939601898193
Epoch 2120, training loss: 311.2303466796875 = 0.05071285367012024 + 50.0 * 6.223592281341553
Epoch 2120, val loss: 1.5230013132095337
Epoch 2130, training loss: 310.96893310546875 = 0.0498616099357605 + 50.0 * 6.218381404876709
Epoch 2130, val loss: 1.5268210172653198
Epoch 2140, training loss: 310.8882141113281 = 0.049066368490457535 + 50.0 * 6.216783046722412
Epoch 2140, val loss: 1.5321987867355347
Epoch 2150, training loss: 310.82794189453125 = 0.04830550402402878 + 50.0 * 6.215592384338379
Epoch 2150, val loss: 1.5371474027633667
Epoch 2160, training loss: 311.0442810058594 = 0.047568123787641525 + 50.0 * 6.219933986663818
Epoch 2160, val loss: 1.5421340465545654
Epoch 2170, training loss: 310.8926086425781 = 0.046793822199106216 + 50.0 * 6.216916561126709
Epoch 2170, val loss: 1.5461546182632446
Epoch 2180, training loss: 310.9023132324219 = 0.04604683443903923 + 50.0 * 6.217125415802002
Epoch 2180, val loss: 1.551432728767395
Epoch 2190, training loss: 310.9188232421875 = 0.045331355184316635 + 50.0 * 6.217470169067383
Epoch 2190, val loss: 1.556082844734192
Epoch 2200, training loss: 310.79559326171875 = 0.04462379217147827 + 50.0 * 6.215019702911377
Epoch 2200, val loss: 1.5607630014419556
Epoch 2210, training loss: 310.84881591796875 = 0.043958548456430435 + 50.0 * 6.216097354888916
Epoch 2210, val loss: 1.5656710863113403
Epoch 2220, training loss: 310.9421691894531 = 0.043290287256240845 + 50.0 * 6.217977523803711
Epoch 2220, val loss: 1.570160984992981
Epoch 2230, training loss: 311.022216796875 = 0.042608585208654404 + 50.0 * 6.219592094421387
Epoch 2230, val loss: 1.5746248960494995
Epoch 2240, training loss: 310.8139343261719 = 0.04195162653923035 + 50.0 * 6.215439319610596
Epoch 2240, val loss: 1.5795369148254395
Epoch 2250, training loss: 310.7689514160156 = 0.04132358357310295 + 50.0 * 6.214552879333496
Epoch 2250, val loss: 1.5844858884811401
Epoch 2260, training loss: 310.962646484375 = 0.04072755575180054 + 50.0 * 6.218438148498535
Epoch 2260, val loss: 1.5895106792449951
Epoch 2270, training loss: 310.72674560546875 = 0.040095288306474686 + 50.0 * 6.213732719421387
Epoch 2270, val loss: 1.593532919883728
Epoch 2280, training loss: 310.7659912109375 = 0.03950037807226181 + 50.0 * 6.214529991149902
Epoch 2280, val loss: 1.598173975944519
Epoch 2290, training loss: 310.8699645996094 = 0.03891811892390251 + 50.0 * 6.216620922088623
Epoch 2290, val loss: 1.6026309728622437
Epoch 2300, training loss: 310.68707275390625 = 0.03833958879113197 + 50.0 * 6.212975025177002
Epoch 2300, val loss: 1.607499122619629
Epoch 2310, training loss: 310.9872131347656 = 0.0378027968108654 + 50.0 * 6.218987941741943
Epoch 2310, val loss: 1.612484097480774
Epoch 2320, training loss: 310.76239013671875 = 0.03721927851438522 + 50.0 * 6.214503288269043
Epoch 2320, val loss: 1.6162264347076416
Epoch 2330, training loss: 310.67559814453125 = 0.036669254302978516 + 50.0 * 6.212778091430664
Epoch 2330, val loss: 1.6211216449737549
Epoch 2340, training loss: 310.640380859375 = 0.036143958568573 + 50.0 * 6.212084770202637
Epoch 2340, val loss: 1.625630259513855
Epoch 2350, training loss: 310.75665283203125 = 0.03564772009849548 + 50.0 * 6.214420318603516
Epoch 2350, val loss: 1.6302133798599243
Epoch 2360, training loss: 310.79803466796875 = 0.035121429711580276 + 50.0 * 6.2152581214904785
Epoch 2360, val loss: 1.6340938806533813
Epoch 2370, training loss: 310.65374755859375 = 0.034589339047670364 + 50.0 * 6.212383270263672
Epoch 2370, val loss: 1.6387286186218262
Epoch 2380, training loss: 310.60693359375 = 0.034102488309144974 + 50.0 * 6.211456775665283
Epoch 2380, val loss: 1.6433751583099365
Epoch 2390, training loss: 310.57861328125 = 0.03363288938999176 + 50.0 * 6.210899353027344
Epoch 2390, val loss: 1.648005723953247
Epoch 2400, training loss: 310.6740417480469 = 0.03318054601550102 + 50.0 * 6.212817668914795
Epoch 2400, val loss: 1.6525378227233887
Epoch 2410, training loss: 310.769287109375 = 0.03271159902215004 + 50.0 * 6.214731216430664
Epoch 2410, val loss: 1.6567507982254028
Epoch 2420, training loss: 310.6604919433594 = 0.032243188470602036 + 50.0 * 6.212564468383789
Epoch 2420, val loss: 1.6610525846481323
Epoch 2430, training loss: 310.5425109863281 = 0.03178396075963974 + 50.0 * 6.210214614868164
Epoch 2430, val loss: 1.6656887531280518
Epoch 2440, training loss: 310.78363037109375 = 0.03136727586388588 + 50.0 * 6.21504545211792
Epoch 2440, val loss: 1.6705154180526733
Epoch 2450, training loss: 310.588623046875 = 0.030916960909962654 + 50.0 * 6.211153984069824
Epoch 2450, val loss: 1.6742221117019653
Epoch 2460, training loss: 310.55224609375 = 0.030475309118628502 + 50.0 * 6.210435390472412
Epoch 2460, val loss: 1.6784449815750122
Epoch 2470, training loss: 310.5647277832031 = 0.030071550980210304 + 50.0 * 6.210693359375
Epoch 2470, val loss: 1.6829431056976318
Epoch 2480, training loss: 310.6706848144531 = 0.029676062986254692 + 50.0 * 6.212820529937744
Epoch 2480, val loss: 1.6873313188552856
Epoch 2490, training loss: 310.5571594238281 = 0.029274502769112587 + 50.0 * 6.21055793762207
Epoch 2490, val loss: 1.6916038990020752
Epoch 2500, training loss: 310.60528564453125 = 0.02888217195868492 + 50.0 * 6.211528301239014
Epoch 2500, val loss: 1.6956313848495483
Epoch 2510, training loss: 310.62762451171875 = 0.028504397720098495 + 50.0 * 6.211982250213623
Epoch 2510, val loss: 1.700232744216919
Epoch 2520, training loss: 310.56597900390625 = 0.02811935916543007 + 50.0 * 6.210757255554199
Epoch 2520, val loss: 1.7043607234954834
Epoch 2530, training loss: 310.5372009277344 = 0.027743468061089516 + 50.0 * 6.210189342498779
Epoch 2530, val loss: 1.7081201076507568
Epoch 2540, training loss: 310.5145263671875 = 0.027387969195842743 + 50.0 * 6.209743022918701
Epoch 2540, val loss: 1.712865948677063
Epoch 2550, training loss: 310.78564453125 = 0.027041170746088028 + 50.0 * 6.215172290802002
Epoch 2550, val loss: 1.7167677879333496
Epoch 2560, training loss: 310.6094665527344 = 0.026666628196835518 + 50.0 * 6.211655616760254
Epoch 2560, val loss: 1.7205708026885986
Epoch 2570, training loss: 310.47552490234375 = 0.026312706992030144 + 50.0 * 6.208984375
Epoch 2570, val loss: 1.725151538848877
Epoch 2580, training loss: 310.7788391113281 = 0.025990765541791916 + 50.0 * 6.215056896209717
Epoch 2580, val loss: 1.7292251586914062
Epoch 2590, training loss: 310.4906311035156 = 0.025635624304413795 + 50.0 * 6.2093000411987305
Epoch 2590, val loss: 1.7329431772232056
Epoch 2600, training loss: 310.4246520996094 = 0.025305328890681267 + 50.0 * 6.207987308502197
Epoch 2600, val loss: 1.7374262809753418
Epoch 2610, training loss: 310.4067077636719 = 0.024991916492581367 + 50.0 * 6.207633972167969
Epoch 2610, val loss: 1.7416170835494995
Epoch 2620, training loss: 310.6080322265625 = 0.024690726771950722 + 50.0 * 6.211667060852051
Epoch 2620, val loss: 1.7452175617218018
Epoch 2630, training loss: 310.54290771484375 = 0.024375637993216515 + 50.0 * 6.2103705406188965
Epoch 2630, val loss: 1.749611735343933
Epoch 2640, training loss: 310.49859619140625 = 0.024054814130067825 + 50.0 * 6.20949125289917
Epoch 2640, val loss: 1.7531557083129883
Epoch 2650, training loss: 310.410400390625 = 0.023758241906762123 + 50.0 * 6.207732677459717
Epoch 2650, val loss: 1.7578421831130981
Epoch 2660, training loss: 310.4594421386719 = 0.023473354056477547 + 50.0 * 6.208719730377197
Epoch 2660, val loss: 1.7619681358337402
Epoch 2670, training loss: 310.66888427734375 = 0.02318562939763069 + 50.0 * 6.212913513183594
Epoch 2670, val loss: 1.765482783317566
Epoch 2680, training loss: 310.52587890625 = 0.022892234846949577 + 50.0 * 6.210060119628906
Epoch 2680, val loss: 1.769452452659607
Epoch 2690, training loss: 310.4629211425781 = 0.02261272817850113 + 50.0 * 6.208806037902832
Epoch 2690, val loss: 1.7739334106445312
Epoch 2700, training loss: 310.50323486328125 = 0.022344304248690605 + 50.0 * 6.209617614746094
Epoch 2700, val loss: 1.77767014503479
Epoch 2710, training loss: 310.4181823730469 = 0.02206757850944996 + 50.0 * 6.207922458648682
Epoch 2710, val loss: 1.7814756631851196
Epoch 2720, training loss: 310.36993408203125 = 0.021804997697472572 + 50.0 * 6.2069621086120605
Epoch 2720, val loss: 1.785435438156128
Epoch 2730, training loss: 310.40093994140625 = 0.02155429869890213 + 50.0 * 6.207587718963623
Epoch 2730, val loss: 1.789456844329834
Epoch 2740, training loss: 310.4814758300781 = 0.021302444860339165 + 50.0 * 6.209203720092773
Epoch 2740, val loss: 1.7933212518692017
Epoch 2750, training loss: 310.4361572265625 = 0.02105014957487583 + 50.0 * 6.208302021026611
Epoch 2750, val loss: 1.7973837852478027
Epoch 2760, training loss: 310.38665771484375 = 0.020798681303858757 + 50.0 * 6.207316875457764
Epoch 2760, val loss: 1.801403522491455
Epoch 2770, training loss: 310.3956298828125 = 0.020555919036269188 + 50.0 * 6.207501411437988
Epoch 2770, val loss: 1.8051486015319824
Epoch 2780, training loss: 310.3168029785156 = 0.020316915586590767 + 50.0 * 6.205929279327393
Epoch 2780, val loss: 1.8090919256210327
Epoch 2790, training loss: 310.4215393066406 = 0.020090902224183083 + 50.0 * 6.208029270172119
Epoch 2790, val loss: 1.8127104043960571
Epoch 2800, training loss: 310.3985290527344 = 0.01985827647149563 + 50.0 * 6.207572937011719
Epoch 2800, val loss: 1.816502571105957
Epoch 2810, training loss: 310.4499206542969 = 0.01962665282189846 + 50.0 * 6.208605766296387
Epoch 2810, val loss: 1.8200852870941162
Epoch 2820, training loss: 310.4882507324219 = 0.019391752779483795 + 50.0 * 6.209377288818359
Epoch 2820, val loss: 1.823623538017273
Epoch 2830, training loss: 310.2935791015625 = 0.01916935108602047 + 50.0 * 6.205488204956055
Epoch 2830, val loss: 1.8278868198394775
Epoch 2840, training loss: 310.363037109375 = 0.018962936475872993 + 50.0 * 6.206881999969482
Epoch 2840, val loss: 1.8320050239562988
Epoch 2850, training loss: 310.423583984375 = 0.018748117610812187 + 50.0 * 6.208096981048584
Epoch 2850, val loss: 1.8356386423110962
Epoch 2860, training loss: 310.4029541015625 = 0.018537718802690506 + 50.0 * 6.207688808441162
Epoch 2860, val loss: 1.8392962217330933
Epoch 2870, training loss: 310.2936096191406 = 0.018324490636587143 + 50.0 * 6.205505847930908
Epoch 2870, val loss: 1.8427156209945679
Epoch 2880, training loss: 310.2239990234375 = 0.018123971298336983 + 50.0 * 6.204117298126221
Epoch 2880, val loss: 1.8469411134719849
Epoch 2890, training loss: 310.2413024902344 = 0.017934449017047882 + 50.0 * 6.204467296600342
Epoch 2890, val loss: 1.8508135080337524
Epoch 2900, training loss: 310.4060974121094 = 0.0177480299025774 + 50.0 * 6.207767009735107
Epoch 2900, val loss: 1.8543248176574707
Epoch 2910, training loss: 310.3736267089844 = 0.017546987161040306 + 50.0 * 6.207121849060059
Epoch 2910, val loss: 1.857472538948059
Epoch 2920, training loss: 310.2419128417969 = 0.01734626479446888 + 50.0 * 6.20449161529541
Epoch 2920, val loss: 1.8613793849945068
Epoch 2930, training loss: 310.1867370605469 = 0.017158638685941696 + 50.0 * 6.2033915519714355
Epoch 2930, val loss: 1.8651763200759888
Epoch 2940, training loss: 310.2897644042969 = 0.016982007771730423 + 50.0 * 6.205455780029297
Epoch 2940, val loss: 1.8685686588287354
Epoch 2950, training loss: 310.3454284667969 = 0.01680499128997326 + 50.0 * 6.206572532653809
Epoch 2950, val loss: 1.8721749782562256
Epoch 2960, training loss: 310.3970031738281 = 0.016618233174085617 + 50.0 * 6.207607269287109
Epoch 2960, val loss: 1.8754531145095825
Epoch 2970, training loss: 310.2545471191406 = 0.016439424827694893 + 50.0 * 6.204761981964111
Epoch 2970, val loss: 1.8793076276779175
Epoch 2980, training loss: 310.23065185546875 = 0.016268637031316757 + 50.0 * 6.204287528991699
Epoch 2980, val loss: 1.8833695650100708
Epoch 2990, training loss: 310.2354736328125 = 0.016101393848657608 + 50.0 * 6.204387187957764
Epoch 2990, val loss: 1.8865290880203247
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6666666666666667
0.8149710068529257
=== training gcn model ===
Epoch 0, training loss: 431.7767333984375 = 1.9362143278121948 + 50.0 * 8.596810340881348
Epoch 0, val loss: 1.939041018486023
Epoch 10, training loss: 431.72509765625 = 1.9277998208999634 + 50.0 * 8.595946311950684
Epoch 10, val loss: 1.9309039115905762
Epoch 20, training loss: 431.4585266113281 = 1.9170141220092773 + 50.0 * 8.590829849243164
Epoch 20, val loss: 1.9199031591415405
Epoch 30, training loss: 429.9097595214844 = 1.9030624628067017 + 50.0 * 8.560133934020996
Epoch 30, val loss: 1.9052921533584595
Epoch 40, training loss: 421.52630615234375 = 1.8865536451339722 + 50.0 * 8.392794609069824
Epoch 40, val loss: 1.8882238864898682
Epoch 50, training loss: 397.3415832519531 = 1.86626136302948 + 50.0 * 7.909506320953369
Epoch 50, val loss: 1.8671188354492188
Epoch 60, training loss: 378.0535583496094 = 1.850257158279419 + 50.0 * 7.52406644821167
Epoch 60, val loss: 1.8524160385131836
Epoch 70, training loss: 361.95391845703125 = 1.8418481349945068 + 50.0 * 7.20224142074585
Epoch 70, val loss: 1.8444054126739502
Epoch 80, training loss: 352.2479553222656 = 1.8340423107147217 + 50.0 * 7.0082783699035645
Epoch 80, val loss: 1.8363107442855835
Epoch 90, training loss: 346.1343078613281 = 1.825927495956421 + 50.0 * 6.886167526245117
Epoch 90, val loss: 1.8280322551727295
Epoch 100, training loss: 341.602783203125 = 1.8190326690673828 + 50.0 * 6.795675277709961
Epoch 100, val loss: 1.820945143699646
Epoch 110, training loss: 338.6242370605469 = 1.813004732131958 + 50.0 * 6.736224174499512
Epoch 110, val loss: 1.8145027160644531
Epoch 120, training loss: 336.0042419433594 = 1.8070265054702759 + 50.0 * 6.683944225311279
Epoch 120, val loss: 1.8081331253051758
Epoch 130, training loss: 334.0256652832031 = 1.80148446559906 + 50.0 * 6.64448356628418
Epoch 130, val loss: 1.8023208379745483
Epoch 140, training loss: 332.2449951171875 = 1.7960327863693237 + 50.0 * 6.60897970199585
Epoch 140, val loss: 1.796661615371704
Epoch 150, training loss: 330.9775695800781 = 1.7902228832244873 + 50.0 * 6.583746910095215
Epoch 150, val loss: 1.7908579111099243
Epoch 160, training loss: 329.724853515625 = 1.7839081287384033 + 50.0 * 6.558818817138672
Epoch 160, val loss: 1.7848658561706543
Epoch 170, training loss: 328.70916748046875 = 1.7772784233093262 + 50.0 * 6.538637638092041
Epoch 170, val loss: 1.7787420749664307
Epoch 180, training loss: 328.0562744140625 = 1.7702330350875854 + 50.0 * 6.525720596313477
Epoch 180, val loss: 1.7723485231399536
Epoch 190, training loss: 327.0702209472656 = 1.762467384338379 + 50.0 * 6.506155014038086
Epoch 190, val loss: 1.7654651403427124
Epoch 200, training loss: 326.3124084472656 = 1.7541594505310059 + 50.0 * 6.4911651611328125
Epoch 200, val loss: 1.7581583261489868
Epoch 210, training loss: 325.6616516113281 = 1.7451666593551636 + 50.0 * 6.478329181671143
Epoch 210, val loss: 1.7503212690353394
Epoch 220, training loss: 324.9737548828125 = 1.7354024648666382 + 50.0 * 6.464766979217529
Epoch 220, val loss: 1.7419097423553467
Epoch 230, training loss: 324.44049072265625 = 1.7248412370681763 + 50.0 * 6.454312801361084
Epoch 230, val loss: 1.7328097820281982
Epoch 240, training loss: 323.9177551269531 = 1.7133662700653076 + 50.0 * 6.444087505340576
Epoch 240, val loss: 1.7229688167572021
Epoch 250, training loss: 323.4037780761719 = 1.7009185552597046 + 50.0 * 6.434057235717773
Epoch 250, val loss: 1.712372064590454
Epoch 260, training loss: 323.00238037109375 = 1.687568187713623 + 50.0 * 6.426296234130859
Epoch 260, val loss: 1.7009804248809814
Epoch 270, training loss: 322.5390625 = 1.6732475757598877 + 50.0 * 6.417316436767578
Epoch 270, val loss: 1.6888357400894165
Epoch 280, training loss: 322.1191101074219 = 1.6580419540405273 + 50.0 * 6.409221172332764
Epoch 280, val loss: 1.6759520769119263
Epoch 290, training loss: 321.807373046875 = 1.6417940855026245 + 50.0 * 6.403311729431152
Epoch 290, val loss: 1.6623066663742065
Epoch 300, training loss: 321.4159851074219 = 1.6246594190597534 + 50.0 * 6.39582633972168
Epoch 300, val loss: 1.6478928327560425
Epoch 310, training loss: 321.156005859375 = 1.6067200899124146 + 50.0 * 6.390985488891602
Epoch 310, val loss: 1.6328868865966797
Epoch 320, training loss: 320.8033752441406 = 1.5879592895507812 + 50.0 * 6.384308338165283
Epoch 320, val loss: 1.6173537969589233
Epoch 330, training loss: 320.54620361328125 = 1.5685268640518188 + 50.0 * 6.37955379486084
Epoch 330, val loss: 1.6013082265853882
Epoch 340, training loss: 320.30499267578125 = 1.5485202074050903 + 50.0 * 6.375129699707031
Epoch 340, val loss: 1.584912896156311
Epoch 350, training loss: 319.9576110839844 = 1.528096318244934 + 50.0 * 6.368590354919434
Epoch 350, val loss: 1.5684378147125244
Epoch 360, training loss: 319.700439453125 = 1.5073827505111694 + 50.0 * 6.363861083984375
Epoch 360, val loss: 1.5518025159835815
Epoch 370, training loss: 319.8182678222656 = 1.48639976978302 + 50.0 * 6.366637706756592
Epoch 370, val loss: 1.535182237625122
Epoch 380, training loss: 319.3607482910156 = 1.464993953704834 + 50.0 * 6.357914924621582
Epoch 380, val loss: 1.5183860063552856
Epoch 390, training loss: 319.08355712890625 = 1.4436858892440796 + 50.0 * 6.352797031402588
Epoch 390, val loss: 1.5018876791000366
Epoch 400, training loss: 318.8287048339844 = 1.4224638938903809 + 50.0 * 6.348124980926514
Epoch 400, val loss: 1.4856711626052856
Epoch 410, training loss: 319.1236877441406 = 1.4011807441711426 + 50.0 * 6.354450225830078
Epoch 410, val loss: 1.469511866569519
Epoch 420, training loss: 318.54571533203125 = 1.3799396753311157 + 50.0 * 6.343315124511719
Epoch 420, val loss: 1.4537094831466675
Epoch 430, training loss: 318.2954406738281 = 1.3589377403259277 + 50.0 * 6.3387298583984375
Epoch 430, val loss: 1.438366413116455
Epoch 440, training loss: 318.08184814453125 = 1.3382177352905273 + 50.0 * 6.334872245788574
Epoch 440, val loss: 1.4234899282455444
Epoch 450, training loss: 318.53662109375 = 1.317665457725525 + 50.0 * 6.344379425048828
Epoch 450, val loss: 1.4089304208755493
Epoch 460, training loss: 317.96844482421875 = 1.296997308731079 + 50.0 * 6.333428859710693
Epoch 460, val loss: 1.3943136930465698
Epoch 470, training loss: 317.6347351074219 = 1.2766807079315186 + 50.0 * 6.3271613121032715
Epoch 470, val loss: 1.3802876472473145
Epoch 480, training loss: 317.4450378417969 = 1.256753921508789 + 50.0 * 6.323765754699707
Epoch 480, val loss: 1.3667526245117188
Epoch 490, training loss: 317.5105895996094 = 1.2371039390563965 + 50.0 * 6.325469970703125
Epoch 490, val loss: 1.3533248901367188
Epoch 500, training loss: 317.2997741699219 = 1.2173898220062256 + 50.0 * 6.321648120880127
Epoch 500, val loss: 1.3406018018722534
Epoch 510, training loss: 317.072021484375 = 1.1979975700378418 + 50.0 * 6.317480564117432
Epoch 510, val loss: 1.327884554862976
Epoch 520, training loss: 316.97589111328125 = 1.178943157196045 + 50.0 * 6.315938949584961
Epoch 520, val loss: 1.315433144569397
Epoch 530, training loss: 316.83642578125 = 1.160071611404419 + 50.0 * 6.3135271072387695
Epoch 530, val loss: 1.3032468557357788
Epoch 540, training loss: 316.6551513671875 = 1.1412936449050903 + 50.0 * 6.310276985168457
Epoch 540, val loss: 1.291483998298645
Epoch 550, training loss: 316.5549621582031 = 1.1228219270706177 + 50.0 * 6.308642864227295
Epoch 550, val loss: 1.2799131870269775
Epoch 560, training loss: 316.53466796875 = 1.104608416557312 + 50.0 * 6.308601379394531
Epoch 560, val loss: 1.2688030004501343
Epoch 570, training loss: 316.43585205078125 = 1.0866494178771973 + 50.0 * 6.3069844245910645
Epoch 570, val loss: 1.2576180696487427
Epoch 580, training loss: 316.2315673828125 = 1.0686582326889038 + 50.0 * 6.303257942199707
Epoch 580, val loss: 1.2469133138656616
Epoch 590, training loss: 316.10552978515625 = 1.0511343479156494 + 50.0 * 6.301087856292725
Epoch 590, val loss: 1.236562728881836
Epoch 600, training loss: 316.1203308105469 = 1.0340169668197632 + 50.0 * 6.3017258644104
Epoch 600, val loss: 1.2264552116394043
Epoch 610, training loss: 315.954833984375 = 1.0168837308883667 + 50.0 * 6.2987589836120605
Epoch 610, val loss: 1.2164841890335083
Epoch 620, training loss: 315.8504638671875 = 1.0000892877578735 + 50.0 * 6.2970075607299805
Epoch 620, val loss: 1.2070320844650269
Epoch 630, training loss: 315.8485107421875 = 0.9836433529853821 + 50.0 * 6.297297477722168
Epoch 630, val loss: 1.1976090669631958
Epoch 640, training loss: 315.65789794921875 = 0.9673858880996704 + 50.0 * 6.2938103675842285
Epoch 640, val loss: 1.188165545463562
Epoch 650, training loss: 315.5837097167969 = 0.9513936638832092 + 50.0 * 6.292646408081055
Epoch 650, val loss: 1.1796671152114868
Epoch 660, training loss: 315.5996398925781 = 0.9356347322463989 + 50.0 * 6.293280124664307
Epoch 660, val loss: 1.1711779832839966
Epoch 670, training loss: 315.48974609375 = 0.920230507850647 + 50.0 * 6.291390419006348
Epoch 670, val loss: 1.163124918937683
Epoch 680, training loss: 315.2533264160156 = 0.9049914479255676 + 50.0 * 6.286966323852539
Epoch 680, val loss: 1.1552377939224243
Epoch 690, training loss: 315.1639404296875 = 0.8901876211166382 + 50.0 * 6.28547477722168
Epoch 690, val loss: 1.1478490829467773
Epoch 700, training loss: 315.1047668457031 = 0.875682532787323 + 50.0 * 6.284582138061523
Epoch 700, val loss: 1.1408385038375854
Epoch 710, training loss: 315.20208740234375 = 0.8612608909606934 + 50.0 * 6.286816596984863
Epoch 710, val loss: 1.1337069272994995
Epoch 720, training loss: 315.0945739746094 = 0.846953272819519 + 50.0 * 6.284952640533447
Epoch 720, val loss: 1.1270167827606201
Epoch 730, training loss: 314.87237548828125 = 0.8330981135368347 + 50.0 * 6.28078556060791
Epoch 730, val loss: 1.1205576658248901
Epoch 740, training loss: 314.8214111328125 = 0.8195624947547913 + 50.0 * 6.2800374031066895
Epoch 740, val loss: 1.1144756078720093
Epoch 750, training loss: 315.0136413574219 = 0.8061874508857727 + 50.0 * 6.284149169921875
Epoch 750, val loss: 1.1084825992584229
Epoch 760, training loss: 314.65673828125 = 0.7927912473678589 + 50.0 * 6.277278900146484
Epoch 760, val loss: 1.102874517440796
Epoch 770, training loss: 314.6380310058594 = 0.7797794342041016 + 50.0 * 6.277164936065674
Epoch 770, val loss: 1.09736967086792
Epoch 780, training loss: 314.5207214355469 = 0.7670324444770813 + 50.0 * 6.275074005126953
Epoch 780, val loss: 1.0926862955093384
Epoch 790, training loss: 314.6653747558594 = 0.7544939517974854 + 50.0 * 6.278217792510986
Epoch 790, val loss: 1.0884757041931152
Epoch 800, training loss: 314.5126037597656 = 0.7419202923774719 + 50.0 * 6.275413513183594
Epoch 800, val loss: 1.0827289819717407
Epoch 810, training loss: 314.4022216796875 = 0.7295604944229126 + 50.0 * 6.273453235626221
Epoch 810, val loss: 1.078905463218689
Epoch 820, training loss: 314.5181884765625 = 0.7173440456390381 + 50.0 * 6.276016712188721
Epoch 820, val loss: 1.0748915672302246
Epoch 830, training loss: 314.25579833984375 = 0.7052062749862671 + 50.0 * 6.271011829376221
Epoch 830, val loss: 1.0704424381256104
Epoch 840, training loss: 314.2061767578125 = 0.6933721899986267 + 50.0 * 6.270256042480469
Epoch 840, val loss: 1.0668565034866333
Epoch 850, training loss: 314.14398193359375 = 0.681675910949707 + 50.0 * 6.2692461013793945
Epoch 850, val loss: 1.0639053583145142
Epoch 860, training loss: 314.2154541015625 = 0.6700842380523682 + 50.0 * 6.270907402038574
Epoch 860, val loss: 1.060746192932129
Epoch 870, training loss: 314.1307067871094 = 0.6585826277732849 + 50.0 * 6.269443035125732
Epoch 870, val loss: 1.057112216949463
Epoch 880, training loss: 314.0227355957031 = 0.6471395492553711 + 50.0 * 6.267512321472168
Epoch 880, val loss: 1.0548185110092163
Epoch 890, training loss: 313.9029541015625 = 0.6359158754348755 + 50.0 * 6.265340805053711
Epoch 890, val loss: 1.0520024299621582
Epoch 900, training loss: 313.92620849609375 = 0.6248584389686584 + 50.0 * 6.266027450561523
Epoch 900, val loss: 1.0501763820648193
Epoch 910, training loss: 313.8406066894531 = 0.6137388944625854 + 50.0 * 6.264537334442139
Epoch 910, val loss: 1.0474244356155396
Epoch 920, training loss: 313.8316650390625 = 0.6027868390083313 + 50.0 * 6.264577865600586
Epoch 920, val loss: 1.0459120273590088
Epoch 930, training loss: 313.7285461425781 = 0.5919817686080933 + 50.0 * 6.262731075286865
Epoch 930, val loss: 1.0440102815628052
Epoch 940, training loss: 314.1943359375 = 0.5814056396484375 + 50.0 * 6.272258758544922
Epoch 940, val loss: 1.0426709651947021
Epoch 950, training loss: 313.6753845214844 = 0.5705971121788025 + 50.0 * 6.262095928192139
Epoch 950, val loss: 1.0417969226837158
Epoch 960, training loss: 313.5784912109375 = 0.5601779818534851 + 50.0 * 6.260366439819336
Epoch 960, val loss: 1.0404245853424072
Epoch 970, training loss: 313.4954528808594 = 0.5499529242515564 + 50.0 * 6.258910179138184
Epoch 970, val loss: 1.0402668714523315
Epoch 980, training loss: 313.4390563964844 = 0.5398283004760742 + 50.0 * 6.257984638214111
Epoch 980, val loss: 1.0400723218917847
Epoch 990, training loss: 313.976318359375 = 0.5297700762748718 + 50.0 * 6.268930912017822
Epoch 990, val loss: 1.039964199066162
Epoch 1000, training loss: 313.44195556640625 = 0.5196415185928345 + 50.0 * 6.258445739746094
Epoch 1000, val loss: 1.0394145250320435
Epoch 1010, training loss: 313.4812927246094 = 0.5097346305847168 + 50.0 * 6.259430885314941
Epoch 1010, val loss: 1.0396490097045898
Epoch 1020, training loss: 313.2729187011719 = 0.5000649094581604 + 50.0 * 6.255457401275635
Epoch 1020, val loss: 1.0401725769042969
Epoch 1030, training loss: 313.26287841796875 = 0.49057289958000183 + 50.0 * 6.255445957183838
Epoch 1030, val loss: 1.041199803352356
Epoch 1040, training loss: 313.4250183105469 = 0.48115187883377075 + 50.0 * 6.258877754211426
Epoch 1040, val loss: 1.0423344373703003
Epoch 1050, training loss: 313.3191223144531 = 0.471818745136261 + 50.0 * 6.256945610046387
Epoch 1050, val loss: 1.0428684949874878
Epoch 1060, training loss: 313.2994689941406 = 0.4626045227050781 + 50.0 * 6.256737232208252
Epoch 1060, val loss: 1.0434577465057373
Epoch 1070, training loss: 313.091552734375 = 0.4535064995288849 + 50.0 * 6.252760887145996
Epoch 1070, val loss: 1.0456874370574951
Epoch 1080, training loss: 313.07403564453125 = 0.4445934295654297 + 50.0 * 6.252589225769043
Epoch 1080, val loss: 1.04719877243042
Epoch 1090, training loss: 313.1141662597656 = 0.43579480051994324 + 50.0 * 6.253567695617676
Epoch 1090, val loss: 1.0489064455032349
Epoch 1100, training loss: 312.96807861328125 = 0.42706912755966187 + 50.0 * 6.250820159912109
Epoch 1100, val loss: 1.051734209060669
Epoch 1110, training loss: 313.0423278808594 = 0.41857874393463135 + 50.0 * 6.252475261688232
Epoch 1110, val loss: 1.053803563117981
Epoch 1120, training loss: 312.94488525390625 = 0.41013410687446594 + 50.0 * 6.25069522857666
Epoch 1120, val loss: 1.0556912422180176
Epoch 1130, training loss: 312.9129333496094 = 0.4018268883228302 + 50.0 * 6.250222206115723
Epoch 1130, val loss: 1.058538556098938
Epoch 1140, training loss: 312.94915771484375 = 0.3936975300312042 + 50.0 * 6.2511091232299805
Epoch 1140, val loss: 1.0609509944915771
Epoch 1150, training loss: 312.7905578613281 = 0.38567647337913513 + 50.0 * 6.2480974197387695
Epoch 1150, val loss: 1.064077377319336
Epoch 1160, training loss: 312.7677917480469 = 0.37790054082870483 + 50.0 * 6.247797966003418
Epoch 1160, val loss: 1.0669924020767212
Epoch 1170, training loss: 312.9637145996094 = 0.3701726794242859 + 50.0 * 6.251870632171631
Epoch 1170, val loss: 1.0703922510147095
Epoch 1180, training loss: 312.8833923339844 = 0.3624851107597351 + 50.0 * 6.250418186187744
Epoch 1180, val loss: 1.0729016065597534
Epoch 1190, training loss: 312.69677734375 = 0.35499775409698486 + 50.0 * 6.246835231781006
Epoch 1190, val loss: 1.0763241052627563
Epoch 1200, training loss: 312.7314758300781 = 0.34771817922592163 + 50.0 * 6.24767541885376
Epoch 1200, val loss: 1.0793893337249756
Epoch 1210, training loss: 312.6330871582031 = 0.3405204713344574 + 50.0 * 6.245851516723633
Epoch 1210, val loss: 1.083148717880249
Epoch 1220, training loss: 312.58642578125 = 0.3334822952747345 + 50.0 * 6.245058536529541
Epoch 1220, val loss: 1.0871689319610596
Epoch 1230, training loss: 312.7026062011719 = 0.32665199041366577 + 50.0 * 6.247519016265869
Epoch 1230, val loss: 1.0908082723617554
Epoch 1240, training loss: 312.5910339355469 = 0.3198103606700897 + 50.0 * 6.245424270629883
Epoch 1240, val loss: 1.0947149991989136
Epoch 1250, training loss: 312.5311279296875 = 0.3131788372993469 + 50.0 * 6.244359016418457
Epoch 1250, val loss: 1.0988315343856812
Epoch 1260, training loss: 312.4456481933594 = 0.30663517117500305 + 50.0 * 6.2427802085876465
Epoch 1260, val loss: 1.1030843257904053
Epoch 1270, training loss: 312.6119079589844 = 0.30029964447021484 + 50.0 * 6.246232032775879
Epoch 1270, val loss: 1.10765540599823
Epoch 1280, training loss: 312.4142761230469 = 0.2939487099647522 + 50.0 * 6.242406845092773
Epoch 1280, val loss: 1.111096739768982
Epoch 1290, training loss: 312.364990234375 = 0.2878098487854004 + 50.0 * 6.241543292999268
Epoch 1290, val loss: 1.1154890060424805
Epoch 1300, training loss: 312.6122741699219 = 0.2818344831466675 + 50.0 * 6.246608734130859
Epoch 1300, val loss: 1.119126558303833
Epoch 1310, training loss: 312.46343994140625 = 0.27582570910453796 + 50.0 * 6.243752479553223
Epoch 1310, val loss: 1.1236873865127563
Epoch 1320, training loss: 312.2886047363281 = 0.26999786496162415 + 50.0 * 6.240372180938721
Epoch 1320, val loss: 1.1278810501098633
Epoch 1330, training loss: 312.2156677246094 = 0.26436522603034973 + 50.0 * 6.239025592803955
Epoch 1330, val loss: 1.1323298215866089
Epoch 1340, training loss: 312.236083984375 = 0.2588762044906616 + 50.0 * 6.239543914794922
Epoch 1340, val loss: 1.136810064315796
Epoch 1350, training loss: 312.4111633300781 = 0.253461092710495 + 50.0 * 6.243154048919678
Epoch 1350, val loss: 1.1409521102905273
Epoch 1360, training loss: 312.255859375 = 0.2481057345867157 + 50.0 * 6.24015474319458
Epoch 1360, val loss: 1.145447850227356
Epoch 1370, training loss: 312.3763122558594 = 0.24291136860847473 + 50.0 * 6.2426676750183105
Epoch 1370, val loss: 1.149423599243164
Epoch 1380, training loss: 312.1390075683594 = 0.2377796620130539 + 50.0 * 6.2380242347717285
Epoch 1380, val loss: 1.1548783779144287
Epoch 1390, training loss: 312.1167297363281 = 0.23282501101493835 + 50.0 * 6.237678050994873
Epoch 1390, val loss: 1.1591566801071167
Epoch 1400, training loss: 312.47503662109375 = 0.2279762327671051 + 50.0 * 6.244941711425781
Epoch 1400, val loss: 1.1637572050094604
Epoch 1410, training loss: 312.1759948730469 = 0.22313274443149567 + 50.0 * 6.2390570640563965
Epoch 1410, val loss: 1.1683707237243652
Epoch 1420, training loss: 312.0052490234375 = 0.21843931078910828 + 50.0 * 6.23573637008667
Epoch 1420, val loss: 1.173282504081726
Epoch 1430, training loss: 311.9638671875 = 0.21389542520046234 + 50.0 * 6.234999656677246
Epoch 1430, val loss: 1.1783185005187988
Epoch 1440, training loss: 312.321044921875 = 0.20943540334701538 + 50.0 * 6.242232322692871
Epoch 1440, val loss: 1.1833765506744385
Epoch 1450, training loss: 312.3614807128906 = 0.2049536556005478 + 50.0 * 6.243130683898926
Epoch 1450, val loss: 1.1878315210342407
Epoch 1460, training loss: 311.96600341796875 = 0.20059968531131744 + 50.0 * 6.2353081703186035
Epoch 1460, val loss: 1.1920427083969116
Epoch 1470, training loss: 311.8756103515625 = 0.19639472663402557 + 50.0 * 6.233584880828857
Epoch 1470, val loss: 1.197324275970459
Epoch 1480, training loss: 311.8698425292969 = 0.19233323633670807 + 50.0 * 6.233550548553467
Epoch 1480, val loss: 1.2019374370574951
Epoch 1490, training loss: 312.1178283691406 = 0.18834708631038666 + 50.0 * 6.238589763641357
Epoch 1490, val loss: 1.2064327001571655
Epoch 1500, training loss: 311.8282165527344 = 0.18432895839214325 + 50.0 * 6.232877731323242
Epoch 1500, val loss: 1.2123961448669434
Epoch 1510, training loss: 311.8069152832031 = 0.18046334385871887 + 50.0 * 6.232529163360596
Epoch 1510, val loss: 1.2166402339935303
Epoch 1520, training loss: 311.99078369140625 = 0.1766875684261322 + 50.0 * 6.236281871795654
Epoch 1520, val loss: 1.221889853477478
Epoch 1530, training loss: 311.92645263671875 = 0.17293347418308258 + 50.0 * 6.23507022857666
Epoch 1530, val loss: 1.2268000841140747
Epoch 1540, training loss: 311.76971435546875 = 0.1692814975976944 + 50.0 * 6.232008457183838
Epoch 1540, val loss: 1.2318452596664429
Epoch 1550, training loss: 311.7911376953125 = 0.1657484620809555 + 50.0 * 6.232508182525635
Epoch 1550, val loss: 1.236871600151062
Epoch 1560, training loss: 311.8101501464844 = 0.1622830480337143 + 50.0 * 6.232956886291504
Epoch 1560, val loss: 1.2417023181915283
Epoch 1570, training loss: 311.71209716796875 = 0.15888462960720062 + 50.0 * 6.231064319610596
Epoch 1570, val loss: 1.2469500303268433
Epoch 1580, training loss: 311.88299560546875 = 0.15557348728179932 + 50.0 * 6.234548568725586
Epoch 1580, val loss: 1.2513418197631836
Epoch 1590, training loss: 311.6617736816406 = 0.1522468626499176 + 50.0 * 6.230190277099609
Epoch 1590, val loss: 1.2572232484817505
Epoch 1600, training loss: 311.5858154296875 = 0.14906743168830872 + 50.0 * 6.228734970092773
Epoch 1600, val loss: 1.2619613409042358
Epoch 1610, training loss: 311.5927734375 = 0.14598415791988373 + 50.0 * 6.228935241699219
Epoch 1610, val loss: 1.2673691511154175
Epoch 1620, training loss: 311.83953857421875 = 0.1429431140422821 + 50.0 * 6.233932018280029
Epoch 1620, val loss: 1.2724941968917847
Epoch 1630, training loss: 311.7452392578125 = 0.13989779353141785 + 50.0 * 6.232107162475586
Epoch 1630, val loss: 1.2768372297286987
Epoch 1640, training loss: 311.57354736328125 = 0.1369369626045227 + 50.0 * 6.228732109069824
Epoch 1640, val loss: 1.282212734222412
Epoch 1650, training loss: 311.51873779296875 = 0.13409148156642914 + 50.0 * 6.2276930809021
Epoch 1650, val loss: 1.287693738937378
Epoch 1660, training loss: 311.5408020019531 = 0.13132347166538239 + 50.0 * 6.228189468383789
Epoch 1660, val loss: 1.2929742336273193
Epoch 1670, training loss: 311.63201904296875 = 0.12858226895332336 + 50.0 * 6.230069160461426
Epoch 1670, val loss: 1.2979393005371094
Epoch 1680, training loss: 311.48638916015625 = 0.12587524950504303 + 50.0 * 6.22721004486084
Epoch 1680, val loss: 1.3029401302337646
Epoch 1690, training loss: 311.45111083984375 = 0.12324632704257965 + 50.0 * 6.22655725479126
Epoch 1690, val loss: 1.3081109523773193
Epoch 1700, training loss: 311.5271301269531 = 0.1206945851445198 + 50.0 * 6.228128910064697
Epoch 1700, val loss: 1.3133840560913086
Epoch 1710, training loss: 311.39642333984375 = 0.11816628277301788 + 50.0 * 6.225565433502197
Epoch 1710, val loss: 1.3187382221221924
Epoch 1720, training loss: 311.7757873535156 = 0.1157679408788681 + 50.0 * 6.233200550079346
Epoch 1720, val loss: 1.3246525526046753
Epoch 1730, training loss: 311.4848937988281 = 0.11327680200338364 + 50.0 * 6.2274322509765625
Epoch 1730, val loss: 1.3287827968597412
Epoch 1740, training loss: 311.3476867675781 = 0.11092379689216614 + 50.0 * 6.224735260009766
Epoch 1740, val loss: 1.3344144821166992
Epoch 1750, training loss: 311.2967224121094 = 0.1086651161313057 + 50.0 * 6.223761081695557
Epoch 1750, val loss: 1.3399593830108643
Epoch 1760, training loss: 311.3250732421875 = 0.10645735263824463 + 50.0 * 6.224372863769531
Epoch 1760, val loss: 1.3455255031585693
Epoch 1770, training loss: 311.5315246582031 = 0.10425318032503128 + 50.0 * 6.228545665740967
Epoch 1770, val loss: 1.350792646408081
Epoch 1780, training loss: 311.27752685546875 = 0.10206662863492966 + 50.0 * 6.223508834838867
Epoch 1780, val loss: 1.355774164199829
Epoch 1790, training loss: 311.2886047363281 = 0.09998910874128342 + 50.0 * 6.2237725257873535
Epoch 1790, val loss: 1.361392855644226
Epoch 1800, training loss: 311.4291076660156 = 0.09796319901943207 + 50.0 * 6.226623058319092
Epoch 1800, val loss: 1.3672453165054321
Epoch 1810, training loss: 311.1929931640625 = 0.09596289694309235 + 50.0 * 6.221940517425537
Epoch 1810, val loss: 1.3721966743469238
Epoch 1820, training loss: 311.2154541015625 = 0.09403703361749649 + 50.0 * 6.222428798675537
Epoch 1820, val loss: 1.3780087232589722
Epoch 1830, training loss: 311.376953125 = 0.09216150641441345 + 50.0 * 6.225695610046387
Epoch 1830, val loss: 1.3838376998901367
Epoch 1840, training loss: 311.2641906738281 = 0.09023146331310272 + 50.0 * 6.223479270935059
Epoch 1840, val loss: 1.3888392448425293
Epoch 1850, training loss: 311.1692199707031 = 0.08839843422174454 + 50.0 * 6.221616268157959
Epoch 1850, val loss: 1.3946058750152588
Epoch 1860, training loss: 311.1239013671875 = 0.08661850541830063 + 50.0 * 6.22074556350708
Epoch 1860, val loss: 1.4003537893295288
Epoch 1870, training loss: 311.3874816894531 = 0.08490467071533203 + 50.0 * 6.2260518074035645
Epoch 1870, val loss: 1.405921459197998
Epoch 1880, training loss: 311.1623229980469 = 0.08317642658948898 + 50.0 * 6.221582412719727
Epoch 1880, val loss: 1.4119349718093872
Epoch 1890, training loss: 311.0884704589844 = 0.08150815963745117 + 50.0 * 6.220139026641846
Epoch 1890, val loss: 1.417720079421997
Epoch 1900, training loss: 311.1570129394531 = 0.07989433407783508 + 50.0 * 6.2215423583984375
Epoch 1900, val loss: 1.423538088798523
Epoch 1910, training loss: 311.1910705566406 = 0.07830903679132462 + 50.0 * 6.222255229949951
Epoch 1910, val loss: 1.4292913675308228
Epoch 1920, training loss: 311.1754455566406 = 0.07674317806959152 + 50.0 * 6.221973896026611
Epoch 1920, val loss: 1.4347074031829834
Epoch 1930, training loss: 311.10736083984375 = 0.07520616799592972 + 50.0 * 6.220643043518066
Epoch 1930, val loss: 1.4408584833145142
Epoch 1940, training loss: 311.0563049316406 = 0.07373584061861038 + 50.0 * 6.219651222229004
Epoch 1940, val loss: 1.4471228122711182
Epoch 1950, training loss: 311.1023254394531 = 0.07228945195674896 + 50.0 * 6.220600605010986
Epoch 1950, val loss: 1.4530854225158691
Epoch 1960, training loss: 311.0602111816406 = 0.07086506485939026 + 50.0 * 6.219787120819092
Epoch 1960, val loss: 1.4585888385772705
Epoch 1970, training loss: 311.0732116699219 = 0.06947475671768188 + 50.0 * 6.22007417678833
Epoch 1970, val loss: 1.4645916223526
Epoch 1980, training loss: 311.22125244140625 = 0.06812981516122818 + 50.0 * 6.223062992095947
Epoch 1980, val loss: 1.4706878662109375
Epoch 1990, training loss: 310.9580078125 = 0.06678947806358337 + 50.0 * 6.2178239822387695
Epoch 1990, val loss: 1.4763580560684204
Epoch 2000, training loss: 310.90679931640625 = 0.06551354378461838 + 50.0 * 6.216825485229492
Epoch 2000, val loss: 1.4824438095092773
Epoch 2010, training loss: 310.97662353515625 = 0.06427866965532303 + 50.0 * 6.218246936798096
Epoch 2010, val loss: 1.4885035753250122
Epoch 2020, training loss: 310.97662353515625 = 0.06302818655967712 + 50.0 * 6.2182722091674805
Epoch 2020, val loss: 1.494201421737671
Epoch 2030, training loss: 310.9173583984375 = 0.06181265786290169 + 50.0 * 6.217111110687256
Epoch 2030, val loss: 1.5001753568649292
Epoch 2040, training loss: 311.0523681640625 = 0.06064224988222122 + 50.0 * 6.219834804534912
Epoch 2040, val loss: 1.5060151815414429
Epoch 2050, training loss: 310.9460144042969 = 0.05948786810040474 + 50.0 * 6.217730522155762
Epoch 2050, val loss: 1.511552095413208
Epoch 2060, training loss: 310.8336181640625 = 0.05835383012890816 + 50.0 * 6.215505599975586
Epoch 2060, val loss: 1.5179119110107422
Epoch 2070, training loss: 310.82916259765625 = 0.057275522500276566 + 50.0 * 6.215437889099121
Epoch 2070, val loss: 1.5238994359970093
Epoch 2080, training loss: 311.036865234375 = 0.056229040026664734 + 50.0 * 6.2196125984191895
Epoch 2080, val loss: 1.53030526638031
Epoch 2090, training loss: 310.7725524902344 = 0.0551660992205143 + 50.0 * 6.2143473625183105
Epoch 2090, val loss: 1.5352369546890259
Epoch 2100, training loss: 310.7592468261719 = 0.0541546493768692 + 50.0 * 6.214101314544678
Epoch 2100, val loss: 1.54155433177948
Epoch 2110, training loss: 310.8714599609375 = 0.05317779257893562 + 50.0 * 6.216365337371826
Epoch 2110, val loss: 1.5475897789001465
Epoch 2120, training loss: 310.9994201660156 = 0.05219786986708641 + 50.0 * 6.218944549560547
Epoch 2120, val loss: 1.5532379150390625
Epoch 2130, training loss: 310.7475280761719 = 0.05120539292693138 + 50.0 * 6.213926315307617
Epoch 2130, val loss: 1.5587056875228882
Epoch 2140, training loss: 310.7011413574219 = 0.050274793058633804 + 50.0 * 6.213017463684082
Epoch 2140, val loss: 1.56486976146698
Epoch 2150, training loss: 310.6842346191406 = 0.049388591200113297 + 50.0 * 6.2126970291137695
Epoch 2150, val loss: 1.5709621906280518
Epoch 2160, training loss: 311.0331726074219 = 0.04853740707039833 + 50.0 * 6.219692707061768
Epoch 2160, val loss: 1.5772228240966797
Epoch 2170, training loss: 310.7681884765625 = 0.04763191565871239 + 50.0 * 6.214410781860352
Epoch 2170, val loss: 1.5818672180175781
Epoch 2180, training loss: 310.7207336425781 = 0.0467730276286602 + 50.0 * 6.213479042053223
Epoch 2180, val loss: 1.5884170532226562
Epoch 2190, training loss: 310.7431945800781 = 0.04595925286412239 + 50.0 * 6.213944435119629
Epoch 2190, val loss: 1.594091773033142
Epoch 2200, training loss: 310.75921630859375 = 0.04515906423330307 + 50.0 * 6.21428108215332
Epoch 2200, val loss: 1.6000396013259888
Epoch 2210, training loss: 310.99566650390625 = 0.044365059584379196 + 50.0 * 6.219025611877441
Epoch 2210, val loss: 1.6049543619155884
Epoch 2220, training loss: 310.7298583984375 = 0.04356789216399193 + 50.0 * 6.213725566864014
Epoch 2220, val loss: 1.6111650466918945
Epoch 2230, training loss: 310.6162414550781 = 0.04281384125351906 + 50.0 * 6.21146821975708
Epoch 2230, val loss: 1.6168512105941772
Epoch 2240, training loss: 310.6080627441406 = 0.04209084063768387 + 50.0 * 6.211319446563721
Epoch 2240, val loss: 1.6227977275848389
Epoch 2250, training loss: 310.8628234863281 = 0.04138600081205368 + 50.0 * 6.216428756713867
Epoch 2250, val loss: 1.6282001733779907
Epoch 2260, training loss: 310.640380859375 = 0.04065777361392975 + 50.0 * 6.211994647979736
Epoch 2260, val loss: 1.6340885162353516
Epoch 2270, training loss: 310.67901611328125 = 0.03996296599507332 + 50.0 * 6.212780952453613
Epoch 2270, val loss: 1.6397889852523804
Epoch 2280, training loss: 310.6528015136719 = 0.0392889603972435 + 50.0 * 6.212270259857178
Epoch 2280, val loss: 1.6454005241394043
Epoch 2290, training loss: 310.6460876464844 = 0.038624972105026245 + 50.0 * 6.212149143218994
Epoch 2290, val loss: 1.650562047958374
Epoch 2300, training loss: 310.5534973144531 = 0.03797978162765503 + 50.0 * 6.210309982299805
Epoch 2300, val loss: 1.6566457748413086
Epoch 2310, training loss: 310.55389404296875 = 0.037357304245233536 + 50.0 * 6.210330963134766
Epoch 2310, val loss: 1.6623369455337524
Epoch 2320, training loss: 310.53857421875 = 0.036749787628650665 + 50.0 * 6.210036754608154
Epoch 2320, val loss: 1.667770504951477
Epoch 2330, training loss: 310.76336669921875 = 0.03615906089544296 + 50.0 * 6.214544296264648
Epoch 2330, val loss: 1.6733484268188477
Epoch 2340, training loss: 310.7177734375 = 0.035571109503507614 + 50.0 * 6.213644027709961
Epoch 2340, val loss: 1.6786749362945557
Epoch 2350, training loss: 310.6124267578125 = 0.034955523908138275 + 50.0 * 6.211549758911133
Epoch 2350, val loss: 1.6838279962539673
Epoch 2360, training loss: 310.53021240234375 = 0.03439520299434662 + 50.0 * 6.209916114807129
Epoch 2360, val loss: 1.689821720123291
Epoch 2370, training loss: 310.4623107910156 = 0.033848099410533905 + 50.0 * 6.208569049835205
Epoch 2370, val loss: 1.6949068307876587
Epoch 2380, training loss: 310.48260498046875 = 0.033326029777526855 + 50.0 * 6.208985805511475
Epoch 2380, val loss: 1.700819730758667
Epoch 2390, training loss: 310.6697692871094 = 0.03281811252236366 + 50.0 * 6.212738990783691
Epoch 2390, val loss: 1.7060260772705078
Epoch 2400, training loss: 310.59637451171875 = 0.032270148396492004 + 50.0 * 6.211282253265381
Epoch 2400, val loss: 1.711408257484436
Epoch 2410, training loss: 310.4330749511719 = 0.03176035359501839 + 50.0 * 6.208025932312012
Epoch 2410, val loss: 1.7165755033493042
Epoch 2420, training loss: 310.4320373535156 = 0.031267501413822174 + 50.0 * 6.208014965057373
Epoch 2420, val loss: 1.7219668626785278
Epoch 2430, training loss: 310.6038818359375 = 0.0307975672185421 + 50.0 * 6.211461544036865
Epoch 2430, val loss: 1.7272783517837524
Epoch 2440, training loss: 310.4997253417969 = 0.030312098562717438 + 50.0 * 6.209388256072998
Epoch 2440, val loss: 1.7322735786437988
Epoch 2450, training loss: 310.5145263671875 = 0.02984398789703846 + 50.0 * 6.209693908691406
Epoch 2450, val loss: 1.7376114130020142
Epoch 2460, training loss: 310.366943359375 = 0.0293838232755661 + 50.0 * 6.206751346588135
Epoch 2460, val loss: 1.7428250312805176
Epoch 2470, training loss: 310.358154296875 = 0.02895248308777809 + 50.0 * 6.2065839767456055
Epoch 2470, val loss: 1.7483229637145996
Epoch 2480, training loss: 310.51312255859375 = 0.028529733419418335 + 50.0 * 6.209692001342773
Epoch 2480, val loss: 1.753330111503601
Epoch 2490, training loss: 310.4142150878906 = 0.028086410835385323 + 50.0 * 6.2077226638793945
Epoch 2490, val loss: 1.758131504058838
Epoch 2500, training loss: 310.3502197265625 = 0.027660813182592392 + 50.0 * 6.206451416015625
Epoch 2500, val loss: 1.7635539770126343
Epoch 2510, training loss: 310.38031005859375 = 0.027260050177574158 + 50.0 * 6.207061290740967
Epoch 2510, val loss: 1.7687584161758423
Epoch 2520, training loss: 310.625 = 0.026861116290092468 + 50.0 * 6.211963176727295
Epoch 2520, val loss: 1.7732830047607422
Epoch 2530, training loss: 310.43023681640625 = 0.026458293199539185 + 50.0 * 6.208075523376465
Epoch 2530, val loss: 1.7788041830062866
Epoch 2540, training loss: 310.30792236328125 = 0.02607368491590023 + 50.0 * 6.205637454986572
Epoch 2540, val loss: 1.7837063074111938
Epoch 2550, training loss: 310.2664489746094 = 0.025702210143208504 + 50.0 * 6.204814910888672
Epoch 2550, val loss: 1.789025902748108
Epoch 2560, training loss: 310.3541259765625 = 0.025348545983433723 + 50.0 * 6.206575393676758
Epoch 2560, val loss: 1.7938960790634155
Epoch 2570, training loss: 310.4642333984375 = 0.024992913007736206 + 50.0 * 6.208785057067871
Epoch 2570, val loss: 1.798221230506897
Epoch 2580, training loss: 310.40496826171875 = 0.02461680956184864 + 50.0 * 6.207606792449951
Epoch 2580, val loss: 1.8032703399658203
Epoch 2590, training loss: 310.3243408203125 = 0.024264521896839142 + 50.0 * 6.2060017585754395
Epoch 2590, val loss: 1.8082435131072998
Epoch 2600, training loss: 310.2398986816406 = 0.023922940716147423 + 50.0 * 6.204319477081299
Epoch 2600, val loss: 1.8131014108657837
Epoch 2610, training loss: 310.23809814453125 = 0.023599056527018547 + 50.0 * 6.20428991317749
Epoch 2610, val loss: 1.8178415298461914
Epoch 2620, training loss: 310.39178466796875 = 0.02328542247414589 + 50.0 * 6.207370281219482
Epoch 2620, val loss: 1.822605848312378
Epoch 2630, training loss: 310.40576171875 = 0.022959033027291298 + 50.0 * 6.207656383514404
Epoch 2630, val loss: 1.8272103071212769
Epoch 2640, training loss: 310.2100830078125 = 0.02263657934963703 + 50.0 * 6.20374870300293
Epoch 2640, val loss: 1.83223557472229
Epoch 2650, training loss: 310.1934814453125 = 0.022327322512865067 + 50.0 * 6.203422546386719
Epoch 2650, val loss: 1.8370975255966187
Epoch 2660, training loss: 310.2535095214844 = 0.022039877250790596 + 50.0 * 6.204629421234131
Epoch 2660, val loss: 1.8420815467834473
Epoch 2670, training loss: 310.3802490234375 = 0.0217480156570673 + 50.0 * 6.207169532775879
Epoch 2670, val loss: 1.8464213609695435
Epoch 2680, training loss: 310.3017883300781 = 0.02145250514149666 + 50.0 * 6.205606460571289
Epoch 2680, val loss: 1.851286768913269
Epoch 2690, training loss: 310.3163757324219 = 0.021165145561099052 + 50.0 * 6.205904006958008
Epoch 2690, val loss: 1.8558681011199951
Epoch 2700, training loss: 310.1804504394531 = 0.0208873488008976 + 50.0 * 6.20319128036499
Epoch 2700, val loss: 1.8603519201278687
Epoch 2710, training loss: 310.23846435546875 = 0.020622357726097107 + 50.0 * 6.204356670379639
Epoch 2710, val loss: 1.86489999294281
Epoch 2720, training loss: 310.2356262207031 = 0.020355109125375748 + 50.0 * 6.204305648803711
Epoch 2720, val loss: 1.8696410655975342
Epoch 2730, training loss: 310.228271484375 = 0.02008657529950142 + 50.0 * 6.204164028167725
Epoch 2730, val loss: 1.873844027519226
Epoch 2740, training loss: 310.21405029296875 = 0.01982671581208706 + 50.0 * 6.203884124755859
Epoch 2740, val loss: 1.8780968189239502
Epoch 2750, training loss: 310.20648193359375 = 0.019582869485020638 + 50.0 * 6.203737735748291
Epoch 2750, val loss: 1.883048176765442
Epoch 2760, training loss: 310.2582702636719 = 0.019333042204380035 + 50.0 * 6.20477819442749
Epoch 2760, val loss: 1.8871500492095947
Epoch 2770, training loss: 310.1903076171875 = 0.019085664302110672 + 50.0 * 6.20342493057251
Epoch 2770, val loss: 1.8911874294281006
Epoch 2780, training loss: 310.2079162597656 = 0.018847782164812088 + 50.0 * 6.2037811279296875
Epoch 2780, val loss: 1.8955285549163818
Epoch 2790, training loss: 310.1134033203125 = 0.01861017756164074 + 50.0 * 6.201895713806152
Epoch 2790, val loss: 1.8999449014663696
Epoch 2800, training loss: 310.2332763671875 = 0.018392950296401978 + 50.0 * 6.2042975425720215
Epoch 2800, val loss: 1.9038585424423218
Epoch 2810, training loss: 310.2205505371094 = 0.018157893791794777 + 50.0 * 6.204048156738281
Epoch 2810, val loss: 1.9083136320114136
Epoch 2820, training loss: 310.2613830566406 = 0.01793169230222702 + 50.0 * 6.204868793487549
Epoch 2820, val loss: 1.9129916429519653
Epoch 2830, training loss: 310.13739013671875 = 0.01771426573395729 + 50.0 * 6.202393531799316
Epoch 2830, val loss: 1.9168897867202759
Epoch 2840, training loss: 310.17633056640625 = 0.017498325556516647 + 50.0 * 6.203176975250244
Epoch 2840, val loss: 1.9205968379974365
Epoch 2850, training loss: 310.04638671875 = 0.017283054068684578 + 50.0 * 6.200582504272461
Epoch 2850, val loss: 1.9254730939865112
Epoch 2860, training loss: 310.06683349609375 = 0.017086807638406754 + 50.0 * 6.200994968414307
Epoch 2860, val loss: 1.9296385049819946
Epoch 2870, training loss: 310.37884521484375 = 0.016891928389668465 + 50.0 * 6.207238674163818
Epoch 2870, val loss: 1.933287262916565
Epoch 2880, training loss: 310.0694274902344 = 0.0166765246540308 + 50.0 * 6.20105504989624
Epoch 2880, val loss: 1.937468409538269
Epoch 2890, training loss: 310.0107116699219 = 0.016479359939694405 + 50.0 * 6.199884414672852
Epoch 2890, val loss: 1.941552996635437
Epoch 2900, training loss: 310.0511474609375 = 0.016294211149215698 + 50.0 * 6.20069694519043
Epoch 2900, val loss: 1.9457650184631348
Epoch 2910, training loss: 310.31292724609375 = 0.016117211431264877 + 50.0 * 6.205936431884766
Epoch 2910, val loss: 1.9496785402297974
Epoch 2920, training loss: 310.0845642089844 = 0.015910495072603226 + 50.0 * 6.20137357711792
Epoch 2920, val loss: 1.952998399734497
Epoch 2930, training loss: 310.08441162109375 = 0.01573287509381771 + 50.0 * 6.20137357711792
Epoch 2930, val loss: 1.9573891162872314
Epoch 2940, training loss: 310.1813049316406 = 0.015551533550024033 + 50.0 * 6.203314781188965
Epoch 2940, val loss: 1.9610517024993896
Epoch 2950, training loss: 310.0827941894531 = 0.01536854449659586 + 50.0 * 6.201348304748535
Epoch 2950, val loss: 1.9648162126541138
Epoch 2960, training loss: 309.9732360839844 = 0.015193785540759563 + 50.0 * 6.199161052703857
Epoch 2960, val loss: 1.9687305688858032
Epoch 2970, training loss: 309.9670715332031 = 0.015027153305709362 + 50.0 * 6.199041366577148
Epoch 2970, val loss: 1.9729892015457153
Epoch 2980, training loss: 310.0223083496094 = 0.014870959334075451 + 50.0 * 6.200149059295654
Epoch 2980, val loss: 1.9769870042800903
Epoch 2990, training loss: 310.19775390625 = 0.014714661054313183 + 50.0 * 6.20366096496582
Epoch 2990, val loss: 1.9800878763198853
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.808645229309436
=== training gcn model ===
Epoch 0, training loss: 431.788330078125 = 1.946729063987732 + 50.0 * 8.596832275390625
Epoch 0, val loss: 1.940637469291687
Epoch 10, training loss: 431.7422790527344 = 1.9375543594360352 + 50.0 * 8.596094131469727
Epoch 10, val loss: 1.932108998298645
Epoch 20, training loss: 431.506103515625 = 1.9260362386703491 + 50.0 * 8.591601371765137
Epoch 20, val loss: 1.9211393594741821
Epoch 30, training loss: 430.02874755859375 = 1.9111298322677612 + 50.0 * 8.562352180480957
Epoch 30, val loss: 1.9067156314849854
Epoch 40, training loss: 420.5484619140625 = 1.892812967300415 + 50.0 * 8.373112678527832
Epoch 40, val loss: 1.8893377780914307
Epoch 50, training loss: 383.53948974609375 = 1.8709003925323486 + 50.0 * 7.633371829986572
Epoch 50, val loss: 1.8684399127960205
Epoch 60, training loss: 372.74884033203125 = 1.8530091047286987 + 50.0 * 7.417916774749756
Epoch 60, val loss: 1.8523207902908325
Epoch 70, training loss: 360.030517578125 = 1.8434467315673828 + 50.0 * 7.163741588592529
Epoch 70, val loss: 1.8426111936569214
Epoch 80, training loss: 353.22222900390625 = 1.8330671787261963 + 50.0 * 7.027783393859863
Epoch 80, val loss: 1.8322176933288574
Epoch 90, training loss: 348.7870178222656 = 1.8234559297561646 + 50.0 * 6.939271450042725
Epoch 90, val loss: 1.8228687047958374
Epoch 100, training loss: 344.3778381347656 = 1.8153812885284424 + 50.0 * 6.8512492179870605
Epoch 100, val loss: 1.814606785774231
Epoch 110, training loss: 341.1681213378906 = 1.8093299865722656 + 50.0 * 6.787176132202148
Epoch 110, val loss: 1.8077250719070435
Epoch 120, training loss: 338.41766357421875 = 1.804124355316162 + 50.0 * 6.732270240783691
Epoch 120, val loss: 1.8018308877944946
Epoch 130, training loss: 336.0440979003906 = 1.799236536026001 + 50.0 * 6.684897422790527
Epoch 130, val loss: 1.7968932390213013
Epoch 140, training loss: 333.93072509765625 = 1.7945516109466553 + 50.0 * 6.642723560333252
Epoch 140, val loss: 1.7920347452163696
Epoch 150, training loss: 332.1560974121094 = 1.7897049188613892 + 50.0 * 6.607327938079834
Epoch 150, val loss: 1.7870444059371948
Epoch 160, training loss: 330.78131103515625 = 1.7843466997146606 + 50.0 * 6.579939365386963
Epoch 160, val loss: 1.7815032005310059
Epoch 170, training loss: 329.45367431640625 = 1.7783724069595337 + 50.0 * 6.553505897521973
Epoch 170, val loss: 1.7757073640823364
Epoch 180, training loss: 328.3616943359375 = 1.7720961570739746 + 50.0 * 6.531792163848877
Epoch 180, val loss: 1.769784927368164
Epoch 190, training loss: 327.3804626464844 = 1.7653840780258179 + 50.0 * 6.512301921844482
Epoch 190, val loss: 1.7635784149169922
Epoch 200, training loss: 326.5228271484375 = 1.7582521438598633 + 50.0 * 6.495291709899902
Epoch 200, val loss: 1.7570419311523438
Epoch 210, training loss: 325.9666748046875 = 1.7505810260772705 + 50.0 * 6.4843220710754395
Epoch 210, val loss: 1.7499521970748901
Epoch 220, training loss: 325.1867370605469 = 1.7422212362289429 + 50.0 * 6.46889066696167
Epoch 220, val loss: 1.7423627376556396
Epoch 230, training loss: 324.5479736328125 = 1.7331784963607788 + 50.0 * 6.456295490264893
Epoch 230, val loss: 1.7342634201049805
Epoch 240, training loss: 324.1468505859375 = 1.7234538793563843 + 50.0 * 6.44846773147583
Epoch 240, val loss: 1.725418210029602
Epoch 250, training loss: 323.6195373535156 = 1.7128053903579712 + 50.0 * 6.438134670257568
Epoch 250, val loss: 1.7159247398376465
Epoch 260, training loss: 323.1468505859375 = 1.7013766765594482 + 50.0 * 6.4289093017578125
Epoch 260, val loss: 1.7057658433914185
Epoch 270, training loss: 322.7619934082031 = 1.68911874294281 + 50.0 * 6.421457767486572
Epoch 270, val loss: 1.6949131488800049
Epoch 280, training loss: 322.6513671875 = 1.6760427951812744 + 50.0 * 6.419506072998047
Epoch 280, val loss: 1.6831868886947632
Epoch 290, training loss: 322.1544494628906 = 1.661885142326355 + 50.0 * 6.40985107421875
Epoch 290, val loss: 1.6708287000656128
Epoch 300, training loss: 321.75103759765625 = 1.6469017267227173 + 50.0 * 6.402082443237305
Epoch 300, val loss: 1.657741904258728
Epoch 310, training loss: 321.42449951171875 = 1.631185531616211 + 50.0 * 6.3958659172058105
Epoch 310, val loss: 1.644001841545105
Epoch 320, training loss: 321.16009521484375 = 1.6146275997161865 + 50.0 * 6.390909671783447
Epoch 320, val loss: 1.6296486854553223
Epoch 330, training loss: 320.8315124511719 = 1.5971513986587524 + 50.0 * 6.384687423706055
Epoch 330, val loss: 1.6146591901779175
Epoch 340, training loss: 320.6214294433594 = 1.5790904760360718 + 50.0 * 6.3808465003967285
Epoch 340, val loss: 1.5992227792739868
Epoch 350, training loss: 320.4356689453125 = 1.560361623764038 + 50.0 * 6.377506256103516
Epoch 350, val loss: 1.583128809928894
Epoch 360, training loss: 320.0738525390625 = 1.5410223007202148 + 50.0 * 6.370656490325928
Epoch 360, val loss: 1.5666306018829346
Epoch 370, training loss: 319.78515625 = 1.5212453603744507 + 50.0 * 6.365278244018555
Epoch 370, val loss: 1.5500208139419556
Epoch 380, training loss: 319.5581970214844 = 1.501072883605957 + 50.0 * 6.361142635345459
Epoch 380, val loss: 1.5332218408584595
Epoch 390, training loss: 319.349609375 = 1.480464220046997 + 50.0 * 6.357382774353027
Epoch 390, val loss: 1.5163835287094116
Epoch 400, training loss: 319.1980285644531 = 1.459536075592041 + 50.0 * 6.354769706726074
Epoch 400, val loss: 1.4990234375
Epoch 410, training loss: 318.9300231933594 = 1.4384453296661377 + 50.0 * 6.349831581115723
Epoch 410, val loss: 1.4817148447036743
Epoch 420, training loss: 318.7068786621094 = 1.4172548055648804 + 50.0 * 6.345792293548584
Epoch 420, val loss: 1.4644877910614014
Epoch 430, training loss: 318.7120056152344 = 1.3959617614746094 + 50.0 * 6.346321105957031
Epoch 430, val loss: 1.447213888168335
Epoch 440, training loss: 318.3895263671875 = 1.3745548725128174 + 50.0 * 6.340299606323242
Epoch 440, val loss: 1.430033802986145
Epoch 450, training loss: 318.2305603027344 = 1.3533291816711426 + 50.0 * 6.3375444412231445
Epoch 450, val loss: 1.4130067825317383
Epoch 460, training loss: 318.0751953125 = 1.3318705558776855 + 50.0 * 6.334866523742676
Epoch 460, val loss: 1.3964945077896118
Epoch 470, training loss: 317.8179931640625 = 1.3107562065124512 + 50.0 * 6.33014440536499
Epoch 470, val loss: 1.379887342453003
Epoch 480, training loss: 317.6693420410156 = 1.2898458242416382 + 50.0 * 6.327589511871338
Epoch 480, val loss: 1.3636823892593384
Epoch 490, training loss: 317.6037292480469 = 1.2689170837402344 + 50.0 * 6.326695919036865
Epoch 490, val loss: 1.3478169441223145
Epoch 500, training loss: 317.4347229003906 = 1.248274326324463 + 50.0 * 6.323729515075684
Epoch 500, val loss: 1.3319158554077148
Epoch 510, training loss: 317.23553466796875 = 1.2279126644134521 + 50.0 * 6.320152282714844
Epoch 510, val loss: 1.3167510032653809
Epoch 520, training loss: 317.266357421875 = 1.207812786102295 + 50.0 * 6.321170806884766
Epoch 520, val loss: 1.3019506931304932
Epoch 530, training loss: 316.9757080078125 = 1.1879056692123413 + 50.0 * 6.315756320953369
Epoch 530, val loss: 1.2874011993408203
Epoch 540, training loss: 316.88031005859375 = 1.168454885482788 + 50.0 * 6.314237117767334
Epoch 540, val loss: 1.2735002040863037
Epoch 550, training loss: 316.8397216796875 = 1.149230718612671 + 50.0 * 6.313809871673584
Epoch 550, val loss: 1.2599265575408936
Epoch 560, training loss: 316.610107421875 = 1.1303606033325195 + 50.0 * 6.309594631195068
Epoch 560, val loss: 1.2465208768844604
Epoch 570, training loss: 316.5517883300781 = 1.1118886470794678 + 50.0 * 6.308798313140869
Epoch 570, val loss: 1.233852505683899
Epoch 580, training loss: 316.37359619140625 = 1.0935676097869873 + 50.0 * 6.305600643157959
Epoch 580, val loss: 1.2218917608261108
Epoch 590, training loss: 316.232666015625 = 1.0757323503494263 + 50.0 * 6.303138256072998
Epoch 590, val loss: 1.2101510763168335
Epoch 600, training loss: 316.1804504394531 = 1.0582753419876099 + 50.0 * 6.302443027496338
Epoch 600, val loss: 1.1990517377853394
Epoch 610, training loss: 316.16790771484375 = 1.040945291519165 + 50.0 * 6.302539348602295
Epoch 610, val loss: 1.1884247064590454
Epoch 620, training loss: 315.9503479003906 = 1.0239228010177612 + 50.0 * 6.29852819442749
Epoch 620, val loss: 1.1778684854507446
Epoch 630, training loss: 315.8356628417969 = 1.0073620080947876 + 50.0 * 6.296566009521484
Epoch 630, val loss: 1.1681158542633057
Epoch 640, training loss: 315.8428955078125 = 0.9911525249481201 + 50.0 * 6.297035217285156
Epoch 640, val loss: 1.1587156057357788
Epoch 650, training loss: 315.7331848144531 = 0.9752350449562073 + 50.0 * 6.295158386230469
Epoch 650, val loss: 1.1492935419082642
Epoch 660, training loss: 315.6314697265625 = 0.9594549536705017 + 50.0 * 6.293440341949463
Epoch 660, val loss: 1.1409952640533447
Epoch 670, training loss: 315.5272216796875 = 0.9440191388130188 + 50.0 * 6.291664123535156
Epoch 670, val loss: 1.13242769241333
Epoch 680, training loss: 315.4638671875 = 0.9288387894630432 + 50.0 * 6.290700435638428
Epoch 680, val loss: 1.1246358156204224
Epoch 690, training loss: 315.3116149902344 = 0.9138484001159668 + 50.0 * 6.287955284118652
Epoch 690, val loss: 1.116859793663025
Epoch 700, training loss: 315.2352294921875 = 0.8990972638130188 + 50.0 * 6.286722660064697
Epoch 700, val loss: 1.1097052097320557
Epoch 710, training loss: 315.2761535644531 = 0.8844910264015198 + 50.0 * 6.287833213806152
Epoch 710, val loss: 1.102992057800293
Epoch 720, training loss: 315.1146240234375 = 0.8700395226478577 + 50.0 * 6.2848920822143555
Epoch 720, val loss: 1.096242904663086
Epoch 730, training loss: 314.9593200683594 = 0.855905294418335 + 50.0 * 6.282068252563477
Epoch 730, val loss: 1.0898106098175049
Epoch 740, training loss: 314.8753967285156 = 0.8419644236564636 + 50.0 * 6.280669212341309
Epoch 740, val loss: 1.083954930305481
Epoch 750, training loss: 315.08721923828125 = 0.8281894326210022 + 50.0 * 6.285180568695068
Epoch 750, val loss: 1.0784525871276855
Epoch 760, training loss: 314.83551025390625 = 0.8143762946128845 + 50.0 * 6.280422687530518
Epoch 760, val loss: 1.0725038051605225
Epoch 770, training loss: 314.8024597167969 = 0.800820529460907 + 50.0 * 6.280033111572266
Epoch 770, val loss: 1.0670053958892822
Epoch 780, training loss: 314.61322021484375 = 0.7873841524124146 + 50.0 * 6.276516914367676
Epoch 780, val loss: 1.0621562004089355
Epoch 790, training loss: 314.5430908203125 = 0.7741798758506775 + 50.0 * 6.275378227233887
Epoch 790, val loss: 1.0570440292358398
Epoch 800, training loss: 314.5489196777344 = 0.7611684799194336 + 50.0 * 6.275754928588867
Epoch 800, val loss: 1.052502155303955
Epoch 810, training loss: 314.4442443847656 = 0.7481371164321899 + 50.0 * 6.273921966552734
Epoch 810, val loss: 1.0482876300811768
Epoch 820, training loss: 314.38641357421875 = 0.7352409362792969 + 50.0 * 6.27302360534668
Epoch 820, val loss: 1.0435452461242676
Epoch 830, training loss: 314.40960693359375 = 0.7225225567817688 + 50.0 * 6.273741722106934
Epoch 830, val loss: 1.0397436618804932
Epoch 840, training loss: 314.27587890625 = 0.7099397778511047 + 50.0 * 6.2713189125061035
Epoch 840, val loss: 1.0352462530136108
Epoch 850, training loss: 314.1952819824219 = 0.6974483728408813 + 50.0 * 6.269956588745117
Epoch 850, val loss: 1.031675100326538
Epoch 860, training loss: 314.1798095703125 = 0.6850918531417847 + 50.0 * 6.269894123077393
Epoch 860, val loss: 1.0279817581176758
Epoch 870, training loss: 314.1553649902344 = 0.6728677153587341 + 50.0 * 6.269649505615234
Epoch 870, val loss: 1.0243446826934814
Epoch 880, training loss: 313.99261474609375 = 0.6606757044792175 + 50.0 * 6.26663875579834
Epoch 880, val loss: 1.0211209058761597
Epoch 890, training loss: 314.133056640625 = 0.6486508250236511 + 50.0 * 6.269688129425049
Epoch 890, val loss: 1.0179213285446167
Epoch 900, training loss: 313.90802001953125 = 0.6366477012634277 + 50.0 * 6.265427112579346
Epoch 900, val loss: 1.014643669128418
Epoch 910, training loss: 313.82501220703125 = 0.6249268054962158 + 50.0 * 6.264001369476318
Epoch 910, val loss: 1.0117347240447998
Epoch 920, training loss: 313.9288635253906 = 0.6133309006690979 + 50.0 * 6.266310691833496
Epoch 920, val loss: 1.0093233585357666
Epoch 930, training loss: 313.8214111328125 = 0.6017214059829712 + 50.0 * 6.2643938064575195
Epoch 930, val loss: 1.005805492401123
Epoch 940, training loss: 313.94024658203125 = 0.5901872515678406 + 50.0 * 6.267001628875732
Epoch 940, val loss: 1.0036351680755615
Epoch 950, training loss: 313.6553955078125 = 0.5788534879684448 + 50.0 * 6.261530876159668
Epoch 950, val loss: 1.0008463859558105
Epoch 960, training loss: 313.5816650390625 = 0.56781405210495 + 50.0 * 6.260276794433594
Epoch 960, val loss: 0.9985306262969971
Epoch 970, training loss: 313.5303039550781 = 0.5569280982017517 + 50.0 * 6.259467601776123
Epoch 970, val loss: 0.9967188239097595
Epoch 980, training loss: 313.7559509277344 = 0.5461453199386597 + 50.0 * 6.264195919036865
Epoch 980, val loss: 0.9947112798690796
Epoch 990, training loss: 313.5557861328125 = 0.5354112982749939 + 50.0 * 6.2604079246521
Epoch 990, val loss: 0.9923577308654785
Epoch 1000, training loss: 313.4467468261719 = 0.5248046517372131 + 50.0 * 6.258438587188721
Epoch 1000, val loss: 0.9909346103668213
Epoch 1010, training loss: 313.4754638671875 = 0.5144850611686707 + 50.0 * 6.259219646453857
Epoch 1010, val loss: 0.9890658855438232
Epoch 1020, training loss: 313.34759521484375 = 0.5042256116867065 + 50.0 * 6.256867408752441
Epoch 1020, val loss: 0.9876565337181091
Epoch 1030, training loss: 313.44677734375 = 0.49422410130500793 + 50.0 * 6.2590508460998535
Epoch 1030, val loss: 0.9864788055419922
Epoch 1040, training loss: 313.2423400878906 = 0.48427751660346985 + 50.0 * 6.255161285400391
Epoch 1040, val loss: 0.9846404790878296
Epoch 1050, training loss: 313.17657470703125 = 0.47455742955207825 + 50.0 * 6.254040241241455
Epoch 1050, val loss: 0.9840818047523499
Epoch 1060, training loss: 313.1146545410156 = 0.46514421701431274 + 50.0 * 6.252990245819092
Epoch 1060, val loss: 0.9830685257911682
Epoch 1070, training loss: 313.11212158203125 = 0.45590099692344666 + 50.0 * 6.253124237060547
Epoch 1070, val loss: 0.9823172092437744
Epoch 1080, training loss: 313.1766357421875 = 0.4467925727367401 + 50.0 * 6.254597187042236
Epoch 1080, val loss: 0.9814720153808594
Epoch 1090, training loss: 313.4004821777344 = 0.43765494227409363 + 50.0 * 6.259256362915039
Epoch 1090, val loss: 0.9806369543075562
Epoch 1100, training loss: 313.06591796875 = 0.4285881519317627 + 50.0 * 6.25274658203125
Epoch 1100, val loss: 0.9801931977272034
Epoch 1110, training loss: 312.94708251953125 = 0.41997700929641724 + 50.0 * 6.250542163848877
Epoch 1110, val loss: 0.9795063138008118
Epoch 1120, training loss: 312.8904724121094 = 0.41166090965270996 + 50.0 * 6.249576568603516
Epoch 1120, val loss: 0.9794749021530151
Epoch 1130, training loss: 312.9709167480469 = 0.4034963846206665 + 50.0 * 6.25134801864624
Epoch 1130, val loss: 0.979304313659668
Epoch 1140, training loss: 312.8254699707031 = 0.39530298113822937 + 50.0 * 6.248603343963623
Epoch 1140, val loss: 0.9789589047431946
Epoch 1150, training loss: 312.8293151855469 = 0.38730236887931824 + 50.0 * 6.24884033203125
Epoch 1150, val loss: 0.9787704944610596
Epoch 1160, training loss: 312.7721862792969 = 0.3795950710773468 + 50.0 * 6.247851848602295
Epoch 1160, val loss: 0.9789871573448181
Epoch 1170, training loss: 312.9935607910156 = 0.3720567226409912 + 50.0 * 6.252429962158203
Epoch 1170, val loss: 0.9792846441268921
Epoch 1180, training loss: 312.7547607421875 = 0.36452341079711914 + 50.0 * 6.247804641723633
Epoch 1180, val loss: 0.978947639465332
Epoch 1190, training loss: 312.6924133300781 = 0.35729265213012695 + 50.0 * 6.246702194213867
Epoch 1190, val loss: 0.9792664647102356
Epoch 1200, training loss: 312.8632507324219 = 0.3502109944820404 + 50.0 * 6.250260829925537
Epoch 1200, val loss: 0.9795090556144714
Epoch 1210, training loss: 312.74468994140625 = 0.34316346049308777 + 50.0 * 6.248030662536621
Epoch 1210, val loss: 0.9804585576057434
Epoch 1220, training loss: 312.6029357910156 = 0.3363191485404968 + 50.0 * 6.245331764221191
Epoch 1220, val loss: 0.9804845452308655
Epoch 1230, training loss: 312.5137939453125 = 0.3296348452568054 + 50.0 * 6.243683338165283
Epoch 1230, val loss: 0.9815260171890259
Epoch 1240, training loss: 312.58746337890625 = 0.3231602907180786 + 50.0 * 6.245285987854004
Epoch 1240, val loss: 0.9819603562355042
Epoch 1250, training loss: 312.4982604980469 = 0.31666743755340576 + 50.0 * 6.243631839752197
Epoch 1250, val loss: 0.983230471611023
Epoch 1260, training loss: 312.5021667480469 = 0.31030401587486267 + 50.0 * 6.243837356567383
Epoch 1260, val loss: 0.9836901426315308
Epoch 1270, training loss: 312.46099853515625 = 0.30416107177734375 + 50.0 * 6.243136405944824
Epoch 1270, val loss: 0.9850156307220459
Epoch 1280, training loss: 312.54534912109375 = 0.29816725850105286 + 50.0 * 6.244944095611572
Epoch 1280, val loss: 0.9857287406921387
Epoch 1290, training loss: 312.3437194824219 = 0.2921787202358246 + 50.0 * 6.241031169891357
Epoch 1290, val loss: 0.9871768355369568
Epoch 1300, training loss: 312.3589782714844 = 0.28645649552345276 + 50.0 * 6.241450309753418
Epoch 1300, val loss: 0.988652229309082
Epoch 1310, training loss: 312.5179138183594 = 0.280807763338089 + 50.0 * 6.244741916656494
Epoch 1310, val loss: 0.9896727204322815
Epoch 1320, training loss: 312.3305358886719 = 0.27517685294151306 + 50.0 * 6.24110746383667
Epoch 1320, val loss: 0.9911506772041321
Epoch 1330, training loss: 312.2474670410156 = 0.2697604298591614 + 50.0 * 6.239553928375244
Epoch 1330, val loss: 0.9926615357398987
Epoch 1340, training loss: 312.2544860839844 = 0.2645009756088257 + 50.0 * 6.239799976348877
Epoch 1340, val loss: 0.9945003986358643
Epoch 1350, training loss: 312.3995056152344 = 0.2592492997646332 + 50.0 * 6.242805480957031
Epoch 1350, val loss: 0.9955264329910278
Epoch 1360, training loss: 312.26702880859375 = 0.25409871339797974 + 50.0 * 6.240258693695068
Epoch 1360, val loss: 0.9969227910041809
Epoch 1370, training loss: 312.1319885253906 = 0.2490243911743164 + 50.0 * 6.237659454345703
Epoch 1370, val loss: 0.9988239407539368
Epoch 1380, training loss: 312.2189636230469 = 0.24416705965995789 + 50.0 * 6.239495754241943
Epoch 1380, val loss: 1.0007957220077515
Epoch 1390, training loss: 312.129150390625 = 0.2393404245376587 + 50.0 * 6.237796306610107
Epoch 1390, val loss: 1.002261996269226
Epoch 1400, training loss: 312.0843200683594 = 0.23460739850997925 + 50.0 * 6.23699426651001
Epoch 1400, val loss: 1.0040775537490845
Epoch 1410, training loss: 312.0690612792969 = 0.23005792498588562 + 50.0 * 6.236779689788818
Epoch 1410, val loss: 1.0062909126281738
Epoch 1420, training loss: 312.3340759277344 = 0.22558578848838806 + 50.0 * 6.2421698570251465
Epoch 1420, val loss: 1.0086833238601685
Epoch 1430, training loss: 312.1005859375 = 0.22107750177383423 + 50.0 * 6.237590312957764
Epoch 1430, val loss: 1.0099905729293823
Epoch 1440, training loss: 312.0070495605469 = 0.21674486994743347 + 50.0 * 6.235806465148926
Epoch 1440, val loss: 1.01253342628479
Epoch 1450, training loss: 312.06884765625 = 0.2125488817691803 + 50.0 * 6.237125873565674
Epoch 1450, val loss: 1.0146050453186035
Epoch 1460, training loss: 311.95220947265625 = 0.20836976170539856 + 50.0 * 6.23487663269043
Epoch 1460, val loss: 1.0166133642196655
Epoch 1470, training loss: 311.93603515625 = 0.20430779457092285 + 50.0 * 6.2346343994140625
Epoch 1470, val loss: 1.0188484191894531
Epoch 1480, training loss: 311.8849792480469 = 0.20034611225128174 + 50.0 * 6.233692646026611
Epoch 1480, val loss: 1.0214927196502686
Epoch 1490, training loss: 311.9173278808594 = 0.19650016725063324 + 50.0 * 6.234416484832764
Epoch 1490, val loss: 1.023983359336853
Epoch 1500, training loss: 312.0975036621094 = 0.19264501333236694 + 50.0 * 6.238096714019775
Epoch 1500, val loss: 1.0267612934112549
Epoch 1510, training loss: 311.9234924316406 = 0.18879742920398712 + 50.0 * 6.23469352722168
Epoch 1510, val loss: 1.028044581413269
Epoch 1520, training loss: 311.7842102050781 = 0.1851567178964615 + 50.0 * 6.23198127746582
Epoch 1520, val loss: 1.0310860872268677
Epoch 1530, training loss: 311.74700927734375 = 0.1816641390323639 + 50.0 * 6.231306552886963
Epoch 1530, val loss: 1.0340211391448975
Epoch 1540, training loss: 311.7322998046875 = 0.17824919521808624 + 50.0 * 6.231081008911133
Epoch 1540, val loss: 1.0367927551269531
Epoch 1550, training loss: 311.9727478027344 = 0.1749373823404312 + 50.0 * 6.23595666885376
Epoch 1550, val loss: 1.0395370721817017
Epoch 1560, training loss: 311.8849182128906 = 0.17145857214927673 + 50.0 * 6.234269618988037
Epoch 1560, val loss: 1.041608214378357
Epoch 1570, training loss: 311.89825439453125 = 0.16812804341316223 + 50.0 * 6.234602928161621
Epoch 1570, val loss: 1.0441073179244995
Epoch 1580, training loss: 311.67547607421875 = 0.16487917304039001 + 50.0 * 6.2302117347717285
Epoch 1580, val loss: 1.0473395586013794
Epoch 1590, training loss: 311.64324951171875 = 0.16179317235946655 + 50.0 * 6.229629039764404
Epoch 1590, val loss: 1.0502408742904663
Epoch 1600, training loss: 311.64013671875 = 0.1587957888841629 + 50.0 * 6.2296271324157715
Epoch 1600, val loss: 1.0531753301620483
Epoch 1610, training loss: 311.977294921875 = 0.15583942830562592 + 50.0 * 6.236428737640381
Epoch 1610, val loss: 1.0561290979385376
Epoch 1620, training loss: 311.7406005859375 = 0.15280777215957642 + 50.0 * 6.231756210327148
Epoch 1620, val loss: 1.0583128929138184
Epoch 1630, training loss: 311.5697937011719 = 0.1498899757862091 + 50.0 * 6.228397846221924
Epoch 1630, val loss: 1.061170220375061
Epoch 1640, training loss: 311.5351867675781 = 0.14713379740715027 + 50.0 * 6.227761268615723
Epoch 1640, val loss: 1.0642269849777222
Epoch 1650, training loss: 311.52520751953125 = 0.14443911612033844 + 50.0 * 6.2276153564453125
Epoch 1650, val loss: 1.067412257194519
Epoch 1660, training loss: 311.8843688964844 = 0.14181667566299438 + 50.0 * 6.234850883483887
Epoch 1660, val loss: 1.0713906288146973
Epoch 1670, training loss: 311.788818359375 = 0.13901387155056 + 50.0 * 6.232996463775635
Epoch 1670, val loss: 1.0718249082565308
Epoch 1680, training loss: 311.4981994628906 = 0.13636323809623718 + 50.0 * 6.227237224578857
Epoch 1680, val loss: 1.0751099586486816
Epoch 1690, training loss: 311.4841613769531 = 0.1338595300912857 + 50.0 * 6.227006435394287
Epoch 1690, val loss: 1.0788320302963257
Epoch 1700, training loss: 311.43011474609375 = 0.13144777715206146 + 50.0 * 6.225973606109619
Epoch 1700, val loss: 1.0817533731460571
Epoch 1710, training loss: 311.43414306640625 = 0.12907733023166656 + 50.0 * 6.226100921630859
Epoch 1710, val loss: 1.0848875045776367
Epoch 1720, training loss: 312.15496826171875 = 0.1267434060573578 + 50.0 * 6.240564346313477
Epoch 1720, val loss: 1.087499976158142
Epoch 1730, training loss: 311.43280029296875 = 0.12426915764808655 + 50.0 * 6.226170539855957
Epoch 1730, val loss: 1.0900822877883911
Epoch 1740, training loss: 311.43359375 = 0.12197907269001007 + 50.0 * 6.226232051849365
Epoch 1740, val loss: 1.0933167934417725
Epoch 1750, training loss: 311.3696594238281 = 0.11980480700731277 + 50.0 * 6.224997043609619
Epoch 1750, val loss: 1.0968559980392456
Epoch 1760, training loss: 311.41461181640625 = 0.11769337207078934 + 50.0 * 6.225937843322754
Epoch 1760, val loss: 1.1002140045166016
Epoch 1770, training loss: 311.45111083984375 = 0.11555489897727966 + 50.0 * 6.226710796356201
Epoch 1770, val loss: 1.1030582189559937
Epoch 1780, training loss: 311.3604431152344 = 0.11344624310731888 + 50.0 * 6.224940299987793
Epoch 1780, val loss: 1.105610966682434
Epoch 1790, training loss: 311.36199951171875 = 0.11142835021018982 + 50.0 * 6.225011825561523
Epoch 1790, val loss: 1.109083890914917
Epoch 1800, training loss: 311.53387451171875 = 0.10945658385753632 + 50.0 * 6.228487968444824
Epoch 1800, val loss: 1.1118640899658203
Epoch 1810, training loss: 311.334228515625 = 0.10745986551046371 + 50.0 * 6.2245354652404785
Epoch 1810, val loss: 1.115146279335022
Epoch 1820, training loss: 311.2969970703125 = 0.10556940734386444 + 50.0 * 6.2238287925720215
Epoch 1820, val loss: 1.1185530424118042
Epoch 1830, training loss: 311.5301513671875 = 0.10371924936771393 + 50.0 * 6.2285284996032715
Epoch 1830, val loss: 1.1216793060302734
Epoch 1840, training loss: 311.3405456542969 = 0.10184703022241592 + 50.0 * 6.22477388381958
Epoch 1840, val loss: 1.1239391565322876
Epoch 1850, training loss: 311.3712463378906 = 0.10004010796546936 + 50.0 * 6.225424289703369
Epoch 1850, val loss: 1.1272358894348145
Epoch 1860, training loss: 311.2511291503906 = 0.09827443212270737 + 50.0 * 6.223057270050049
Epoch 1860, val loss: 1.1304140090942383
Epoch 1870, training loss: 311.2370300292969 = 0.09657541662454605 + 50.0 * 6.222808837890625
Epoch 1870, val loss: 1.1338926553726196
Epoch 1880, training loss: 311.3073425292969 = 0.09490984678268433 + 50.0 * 6.22424840927124
Epoch 1880, val loss: 1.137083888053894
Epoch 1890, training loss: 311.2615966796875 = 0.0932171419262886 + 50.0 * 6.223367214202881
Epoch 1890, val loss: 1.1397148370742798
Epoch 1900, training loss: 311.2020263671875 = 0.09159030020236969 + 50.0 * 6.2222089767456055
Epoch 1900, val loss: 1.1426830291748047
Epoch 1910, training loss: 311.1702880859375 = 0.09002021700143814 + 50.0 * 6.22160530090332
Epoch 1910, val loss: 1.1465928554534912
Epoch 1920, training loss: 311.4714050292969 = 0.08847874402999878 + 50.0 * 6.227658748626709
Epoch 1920, val loss: 1.149897575378418
Epoch 1930, training loss: 311.2251281738281 = 0.08690456300973892 + 50.0 * 6.222764492034912
Epoch 1930, val loss: 1.1513036489486694
Epoch 1940, training loss: 311.1352844238281 = 0.08541270345449448 + 50.0 * 6.220997333526611
Epoch 1940, val loss: 1.1556254625320435
Epoch 1950, training loss: 311.1746826171875 = 0.08399112522602081 + 50.0 * 6.221813678741455
Epoch 1950, val loss: 1.1580911874771118
Epoch 1960, training loss: 311.2320861816406 = 0.08253491669893265 + 50.0 * 6.222990989685059
Epoch 1960, val loss: 1.1615793704986572
Epoch 1970, training loss: 311.0670166015625 = 0.0810612291097641 + 50.0 * 6.219719409942627
Epoch 1970, val loss: 1.1639899015426636
Epoch 1980, training loss: 311.0712890625 = 0.07969197630882263 + 50.0 * 6.219831943511963
Epoch 1980, val loss: 1.1676340103149414
Epoch 1990, training loss: 311.0982360839844 = 0.07838630676269531 + 50.0 * 6.220396518707275
Epoch 1990, val loss: 1.1708002090454102
Epoch 2000, training loss: 311.2217712402344 = 0.0770641416311264 + 50.0 * 6.222894191741943
Epoch 2000, val loss: 1.173933744430542
Epoch 2010, training loss: 311.0968017578125 = 0.07575222104787827 + 50.0 * 6.220420837402344
Epoch 2010, val loss: 1.1763837337493896
Epoch 2020, training loss: 311.276611328125 = 0.07448196411132812 + 50.0 * 6.2240424156188965
Epoch 2020, val loss: 1.179552674293518
Epoch 2030, training loss: 311.0480041503906 = 0.07321255654096603 + 50.0 * 6.21949577331543
Epoch 2030, val loss: 1.1828055381774902
Epoch 2040, training loss: 311.0228271484375 = 0.07199712842702866 + 50.0 * 6.2190165519714355
Epoch 2040, val loss: 1.1855043172836304
Epoch 2050, training loss: 311.0893249511719 = 0.07082226872444153 + 50.0 * 6.220369815826416
Epoch 2050, val loss: 1.188673734664917
Epoch 2060, training loss: 310.9571838378906 = 0.06965311616659164 + 50.0 * 6.2177510261535645
Epoch 2060, val loss: 1.1917887926101685
Epoch 2070, training loss: 310.94818115234375 = 0.06852524727582932 + 50.0 * 6.217593669891357
Epoch 2070, val loss: 1.195374846458435
Epoch 2080, training loss: 311.0950927734375 = 0.06742221117019653 + 50.0 * 6.220553874969482
Epoch 2080, val loss: 1.19844388961792
Epoch 2090, training loss: 310.99676513671875 = 0.06629343330860138 + 50.0 * 6.218609809875488
Epoch 2090, val loss: 1.2005703449249268
Epoch 2100, training loss: 311.0110778808594 = 0.06520234793424606 + 50.0 * 6.218917369842529
Epoch 2100, val loss: 1.204002022743225
Epoch 2110, training loss: 310.9024353027344 = 0.06412253528833389 + 50.0 * 6.216766357421875
Epoch 2110, val loss: 1.2070037126541138
Epoch 2120, training loss: 310.8963623046875 = 0.06309524178504944 + 50.0 * 6.216664791107178
Epoch 2120, val loss: 1.2101447582244873
Epoch 2130, training loss: 311.10003662109375 = 0.062122706323862076 + 50.0 * 6.220757961273193
Epoch 2130, val loss: 1.2136640548706055
Epoch 2140, training loss: 310.9003601074219 = 0.06108789145946503 + 50.0 * 6.216785430908203
Epoch 2140, val loss: 1.2158691883087158
Epoch 2150, training loss: 310.85711669921875 = 0.06010482832789421 + 50.0 * 6.215939998626709
Epoch 2150, val loss: 1.21913480758667
Epoch 2160, training loss: 310.8563537597656 = 0.05916912853717804 + 50.0 * 6.215943336486816
Epoch 2160, val loss: 1.2222238779067993
Epoch 2170, training loss: 311.0272216796875 = 0.05825353413820267 + 50.0 * 6.219379425048828
Epoch 2170, val loss: 1.225649356842041
Epoch 2180, training loss: 310.8414611816406 = 0.05730805918574333 + 50.0 * 6.2156829833984375
Epoch 2180, val loss: 1.2281863689422607
Epoch 2190, training loss: 310.8951416015625 = 0.05641903728246689 + 50.0 * 6.2167744636535645
Epoch 2190, val loss: 1.2315213680267334
Epoch 2200, training loss: 310.9271240234375 = 0.05553051456809044 + 50.0 * 6.217431545257568
Epoch 2200, val loss: 1.2346996068954468
Epoch 2210, training loss: 310.85546875 = 0.05465295910835266 + 50.0 * 6.2160162925720215
Epoch 2210, val loss: 1.2368172407150269
Epoch 2220, training loss: 310.8493347167969 = 0.0537990927696228 + 50.0 * 6.215910911560059
Epoch 2220, val loss: 1.2398337125778198
Epoch 2230, training loss: 310.802490234375 = 0.052978433668613434 + 50.0 * 6.214990139007568
Epoch 2230, val loss: 1.2435444593429565
Epoch 2240, training loss: 310.7904052734375 = 0.05218234658241272 + 50.0 * 6.21476411819458
Epoch 2240, val loss: 1.2460845708847046
Epoch 2250, training loss: 310.85662841796875 = 0.051395609974861145 + 50.0 * 6.216104507446289
Epoch 2250, val loss: 1.248758316040039
Epoch 2260, training loss: 310.87835693359375 = 0.050596967339515686 + 50.0 * 6.216555118560791
Epoch 2260, val loss: 1.2513352632522583
Epoch 2270, training loss: 310.7540588378906 = 0.04980427399277687 + 50.0 * 6.214085102081299
Epoch 2270, val loss: 1.2548127174377441
Epoch 2280, training loss: 310.81005859375 = 0.04906089976429939 + 50.0 * 6.215219497680664
Epoch 2280, val loss: 1.257390022277832
Epoch 2290, training loss: 310.75439453125 = 0.04831797257065773 + 50.0 * 6.2141218185424805
Epoch 2290, val loss: 1.2605159282684326
Epoch 2300, training loss: 310.7663879394531 = 0.04760720580816269 + 50.0 * 6.2143754959106445
Epoch 2300, val loss: 1.2637444734573364
Epoch 2310, training loss: 310.8874816894531 = 0.04689477011561394 + 50.0 * 6.216811656951904
Epoch 2310, val loss: 1.266932725906372
Epoch 2320, training loss: 310.76171875 = 0.04617856815457344 + 50.0 * 6.214311122894287
Epoch 2320, val loss: 1.2697675228118896
Epoch 2330, training loss: 310.7440185546875 = 0.04550923407077789 + 50.0 * 6.213970184326172
Epoch 2330, val loss: 1.272772192955017
Epoch 2340, training loss: 310.82647705078125 = 0.04484931752085686 + 50.0 * 6.215632438659668
Epoch 2340, val loss: 1.2752859592437744
Epoch 2350, training loss: 310.6761779785156 = 0.04416721686720848 + 50.0 * 6.212640285491943
Epoch 2350, val loss: 1.277502179145813
Epoch 2360, training loss: 310.6651611328125 = 0.04353634640574455 + 50.0 * 6.212432384490967
Epoch 2360, val loss: 1.280430555343628
Epoch 2370, training loss: 310.93536376953125 = 0.04291608929634094 + 50.0 * 6.217849254608154
Epoch 2370, val loss: 1.28300940990448
Epoch 2380, training loss: 310.6963195800781 = 0.04225229099392891 + 50.0 * 6.213080883026123
Epoch 2380, val loss: 1.2866159677505493
Epoch 2390, training loss: 310.6388244628906 = 0.041656743735075 + 50.0 * 6.211943626403809
Epoch 2390, val loss: 1.2893781661987305
Epoch 2400, training loss: 310.760009765625 = 0.041074808686971664 + 50.0 * 6.214378356933594
Epoch 2400, val loss: 1.292878270149231
Epoch 2410, training loss: 310.5965576171875 = 0.04047277197241783 + 50.0 * 6.211122035980225
Epoch 2410, val loss: 1.2946090698242188
Epoch 2420, training loss: 310.6505432128906 = 0.03990762680768967 + 50.0 * 6.212212562561035
Epoch 2420, val loss: 1.2972993850708008
Epoch 2430, training loss: 310.68878173828125 = 0.03935430571436882 + 50.0 * 6.212988376617432
Epoch 2430, val loss: 1.3008921146392822
Epoch 2440, training loss: 310.5959777832031 = 0.03878970071673393 + 50.0 * 6.211143493652344
Epoch 2440, val loss: 1.3033621311187744
Epoch 2450, training loss: 310.6646423339844 = 0.038243457674980164 + 50.0 * 6.212528228759766
Epoch 2450, val loss: 1.3058117628097534
Epoch 2460, training loss: 310.68359375 = 0.037713535130023956 + 50.0 * 6.212917327880859
Epoch 2460, val loss: 1.307745099067688
Epoch 2470, training loss: 310.5492858886719 = 0.037174783647060394 + 50.0 * 6.21024227142334
Epoch 2470, val loss: 1.3105332851409912
Epoch 2480, training loss: 310.5847473144531 = 0.03666985407471657 + 50.0 * 6.21096134185791
Epoch 2480, val loss: 1.313916563987732
Epoch 2490, training loss: 310.5904541015625 = 0.03616974875330925 + 50.0 * 6.211085796356201
Epoch 2490, val loss: 1.3159173727035522
Epoch 2500, training loss: 310.559326171875 = 0.035676490515470505 + 50.0 * 6.21047306060791
Epoch 2500, val loss: 1.3185842037200928
Epoch 2510, training loss: 310.5704650878906 = 0.03519304096698761 + 50.0 * 6.210705280303955
Epoch 2510, val loss: 1.3217970132827759
Epoch 2520, training loss: 310.5639953613281 = 0.03471655026078224 + 50.0 * 6.210585594177246
Epoch 2520, val loss: 1.3247774839401245
Epoch 2530, training loss: 310.7034912109375 = 0.034252069890499115 + 50.0 * 6.213385105133057
Epoch 2530, val loss: 1.32709538936615
Epoch 2540, training loss: 310.5666198730469 = 0.03375885263085365 + 50.0 * 6.210657596588135
Epoch 2540, val loss: 1.3297275304794312
Epoch 2550, training loss: 310.5164794921875 = 0.03330506756901741 + 50.0 * 6.2096638679504395
Epoch 2550, val loss: 1.330775260925293
Epoch 2560, training loss: 310.48553466796875 = 0.03286772593855858 + 50.0 * 6.2090535163879395
Epoch 2560, val loss: 1.3345509767532349
Epoch 2570, training loss: 310.5570983886719 = 0.03244945779442787 + 50.0 * 6.210493087768555
Epoch 2570, val loss: 1.3362739086151123
Epoch 2580, training loss: 310.4442443847656 = 0.03200205788016319 + 50.0 * 6.208244800567627
Epoch 2580, val loss: 1.3391536474227905
Epoch 2590, training loss: 310.5497741699219 = 0.03158817067742348 + 50.0 * 6.210363864898682
Epoch 2590, val loss: 1.3415285348892212
Epoch 2600, training loss: 310.5040283203125 = 0.031169863417744637 + 50.0 * 6.2094573974609375
Epoch 2600, val loss: 1.3440344333648682
Epoch 2610, training loss: 310.4366455078125 = 0.030756155028939247 + 50.0 * 6.208117485046387
Epoch 2610, val loss: 1.346617579460144
Epoch 2620, training loss: 310.5708312988281 = 0.030367663130164146 + 50.0 * 6.210809230804443
Epoch 2620, val loss: 1.3488266468048096
Epoch 2630, training loss: 310.5645751953125 = 0.029964474961161613 + 50.0 * 6.210691928863525
Epoch 2630, val loss: 1.3504414558410645
Epoch 2640, training loss: 310.4056701660156 = 0.029554542154073715 + 50.0 * 6.207522392272949
Epoch 2640, val loss: 1.3538306951522827
Epoch 2650, training loss: 310.3675231933594 = 0.029190029948949814 + 50.0 * 6.2067670822143555
Epoch 2650, val loss: 1.3562171459197998
Epoch 2660, training loss: 310.39703369140625 = 0.028832949697971344 + 50.0 * 6.207363605499268
Epoch 2660, val loss: 1.3581870794296265
Epoch 2670, training loss: 310.665771484375 = 0.02847817912697792 + 50.0 * 6.2127461433410645
Epoch 2670, val loss: 1.3599622249603271
Epoch 2680, training loss: 310.4981994628906 = 0.028092199936509132 + 50.0 * 6.209402084350586
Epoch 2680, val loss: 1.3631914854049683
Epoch 2690, training loss: 310.4298095703125 = 0.027726778760552406 + 50.0 * 6.208041667938232
Epoch 2690, val loss: 1.3654297590255737
Epoch 2700, training loss: 310.3411560058594 = 0.027384066954255104 + 50.0 * 6.206275939941406
Epoch 2700, val loss: 1.3682315349578857
Epoch 2710, training loss: 310.358642578125 = 0.0270551610738039 + 50.0 * 6.206631660461426
Epoch 2710, val loss: 1.3699052333831787
Epoch 2720, training loss: 310.41796875 = 0.026726193726062775 + 50.0 * 6.20782470703125
Epoch 2720, val loss: 1.3724250793457031
Epoch 2730, training loss: 310.36309814453125 = 0.026391223073005676 + 50.0 * 6.2067341804504395
Epoch 2730, val loss: 1.3757439851760864
Epoch 2740, training loss: 310.46588134765625 = 0.02606596238911152 + 50.0 * 6.208796501159668
Epoch 2740, val loss: 1.377752661705017
Epoch 2750, training loss: 310.4310607910156 = 0.025729816406965256 + 50.0 * 6.208106994628906
Epoch 2750, val loss: 1.3791124820709229
Epoch 2760, training loss: 310.3860778808594 = 0.025417519733309746 + 50.0 * 6.207213401794434
Epoch 2760, val loss: 1.3817856311798096
Epoch 2770, training loss: 310.3908386230469 = 0.02510753832757473 + 50.0 * 6.207314491271973
Epoch 2770, val loss: 1.3840187788009644
Epoch 2780, training loss: 310.3788146972656 = 0.024807017296552658 + 50.0 * 6.207080364227295
Epoch 2780, val loss: 1.3869081735610962
Epoch 2790, training loss: 310.25836181640625 = 0.0245054941624403 + 50.0 * 6.204677104949951
Epoch 2790, val loss: 1.388475775718689
Epoch 2800, training loss: 310.2950439453125 = 0.024223584681749344 + 50.0 * 6.205416202545166
Epoch 2800, val loss: 1.3906761407852173
Epoch 2810, training loss: 310.2640686035156 = 0.023944586515426636 + 50.0 * 6.204802513122559
Epoch 2810, val loss: 1.3934564590454102
Epoch 2820, training loss: 310.4684753417969 = 0.023676669225096703 + 50.0 * 6.208896160125732
Epoch 2820, val loss: 1.3957778215408325
Epoch 2830, training loss: 310.3518981933594 = 0.02338070049881935 + 50.0 * 6.206570148468018
Epoch 2830, val loss: 1.3965582847595215
Epoch 2840, training loss: 310.2738037109375 = 0.02309439145028591 + 50.0 * 6.205013751983643
Epoch 2840, val loss: 1.3989347219467163
Epoch 2850, training loss: 310.3179626464844 = 0.02282625064253807 + 50.0 * 6.205902576446533
Epoch 2850, val loss: 1.4007881879806519
Epoch 2860, training loss: 310.226806640625 = 0.022557677701115608 + 50.0 * 6.204085350036621
Epoch 2860, val loss: 1.4033013582229614
Epoch 2870, training loss: 310.24737548828125 = 0.02230178751051426 + 50.0 * 6.204501152038574
Epoch 2870, val loss: 1.4061733484268188
Epoch 2880, training loss: 310.348388671875 = 0.022054355591535568 + 50.0 * 6.206526756286621
Epoch 2880, val loss: 1.4075895547866821
Epoch 2890, training loss: 310.30841064453125 = 0.021793927997350693 + 50.0 * 6.205732345581055
Epoch 2890, val loss: 1.40940260887146
Epoch 2900, training loss: 310.17864990234375 = 0.02153892070055008 + 50.0 * 6.203142166137695
Epoch 2900, val loss: 1.4118496179580688
Epoch 2910, training loss: 310.15887451171875 = 0.02129666693508625 + 50.0 * 6.202751159667969
Epoch 2910, val loss: 1.4140201807022095
Epoch 2920, training loss: 310.1602478027344 = 0.02107386104762554 + 50.0 * 6.202783107757568
Epoch 2920, val loss: 1.4156817197799683
Epoch 2930, training loss: 310.5563659667969 = 0.020860839635133743 + 50.0 * 6.210709571838379
Epoch 2930, val loss: 1.4163293838500977
Epoch 2940, training loss: 310.3526916503906 = 0.02059388905763626 + 50.0 * 6.206641674041748
Epoch 2940, val loss: 1.4200048446655273
Epoch 2950, training loss: 310.22406005859375 = 0.020345740020275116 + 50.0 * 6.204073905944824
Epoch 2950, val loss: 1.4203389883041382
Epoch 2960, training loss: 310.1531677246094 = 0.0201274286955595 + 50.0 * 6.20266056060791
Epoch 2960, val loss: 1.4240224361419678
Epoch 2970, training loss: 310.2012023925781 = 0.01992463693022728 + 50.0 * 6.203625202178955
Epoch 2970, val loss: 1.4255119562149048
Epoch 2980, training loss: 310.2531433105469 = 0.019708041101694107 + 50.0 * 6.2046685218811035
Epoch 2980, val loss: 1.4272080659866333
Epoch 2990, training loss: 310.21807861328125 = 0.01948569342494011 + 50.0 * 6.2039713859558105
Epoch 2990, val loss: 1.4287278652191162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.816025303110174
The final CL Acc:0.67901, 0.00924, The final GNN Acc:0.81321, 0.00326
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13222])
remove edge: torch.Size([2, 7900])
updated graph: torch.Size([2, 10566])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.8036804199219 = 1.962927222251892 + 50.0 * 8.59681510925293
Epoch 0, val loss: 1.9563060998916626
Epoch 10, training loss: 431.7519226074219 = 1.9543009996414185 + 50.0 * 8.595952033996582
Epoch 10, val loss: 1.9478825330734253
Epoch 20, training loss: 431.4759521484375 = 1.9431639909744263 + 50.0 * 8.590655326843262
Epoch 20, val loss: 1.9365720748901367
Epoch 30, training loss: 429.74920654296875 = 1.9285292625427246 + 50.0 * 8.556413650512695
Epoch 30, val loss: 1.9215564727783203
Epoch 40, training loss: 417.4831848144531 = 1.9113516807556152 + 50.0 * 8.311436653137207
Epoch 40, val loss: 1.9041752815246582
Epoch 50, training loss: 375.81146240234375 = 1.891291618347168 + 50.0 * 7.478403568267822
Epoch 50, val loss: 1.8839340209960938
Epoch 60, training loss: 364.2898254394531 = 1.87432062625885 + 50.0 * 7.248310089111328
Epoch 60, val loss: 1.868098258972168
Epoch 70, training loss: 354.2059631347656 = 1.8609310388565063 + 50.0 * 7.046900749206543
Epoch 70, val loss: 1.855716586112976
Epoch 80, training loss: 347.2265625 = 1.848670482635498 + 50.0 * 6.907557964324951
Epoch 80, val loss: 1.8443223237991333
Epoch 90, training loss: 341.457275390625 = 1.8385502099990845 + 50.0 * 6.792374134063721
Epoch 90, val loss: 1.8349395990371704
Epoch 100, training loss: 338.2222900390625 = 1.8295475244522095 + 50.0 * 6.7278547286987305
Epoch 100, val loss: 1.8265304565429688
Epoch 110, training loss: 335.85162353515625 = 1.8208550214767456 + 50.0 * 6.680614948272705
Epoch 110, val loss: 1.8185272216796875
Epoch 120, training loss: 333.91021728515625 = 1.8130298852920532 + 50.0 * 6.64194393157959
Epoch 120, val loss: 1.8113240003585815
Epoch 130, training loss: 332.24786376953125 = 1.806130051612854 + 50.0 * 6.608834743499756
Epoch 130, val loss: 1.8050646781921387
Epoch 140, training loss: 330.7739562988281 = 1.7997608184814453 + 50.0 * 6.579483509063721
Epoch 140, val loss: 1.799180269241333
Epoch 150, training loss: 329.51385498046875 = 1.7934576272964478 + 50.0 * 6.554408073425293
Epoch 150, val loss: 1.7932429313659668
Epoch 160, training loss: 328.4216613769531 = 1.7869551181793213 + 50.0 * 6.532693862915039
Epoch 160, val loss: 1.7870839834213257
Epoch 170, training loss: 327.4828796386719 = 1.7801547050476074 + 50.0 * 6.514054775238037
Epoch 170, val loss: 1.7806663513183594
Epoch 180, training loss: 326.71533203125 = 1.7729568481445312 + 50.0 * 6.498847961425781
Epoch 180, val loss: 1.7739390134811401
Epoch 190, training loss: 326.0257568359375 = 1.7652075290679932 + 50.0 * 6.485211372375488
Epoch 190, val loss: 1.7668135166168213
Epoch 200, training loss: 325.2801208496094 = 1.7570301294326782 + 50.0 * 6.470462322235107
Epoch 200, val loss: 1.7593328952789307
Epoch 210, training loss: 324.6412658691406 = 1.748374104499817 + 50.0 * 6.457857608795166
Epoch 210, val loss: 1.75153386592865
Epoch 220, training loss: 324.3306884765625 = 1.7390837669372559 + 50.0 * 6.451832294464111
Epoch 220, val loss: 1.743272066116333
Epoch 230, training loss: 323.5856018066406 = 1.7289326190948486 + 50.0 * 6.437133312225342
Epoch 230, val loss: 1.734305500984192
Epoch 240, training loss: 323.07159423828125 = 1.7180256843566895 + 50.0 * 6.427071571350098
Epoch 240, val loss: 1.724812626838684
Epoch 250, training loss: 322.58782958984375 = 1.7062736749649048 + 50.0 * 6.417631149291992
Epoch 250, val loss: 1.7146050930023193
Epoch 260, training loss: 322.4465026855469 = 1.693644642829895 + 50.0 * 6.41505765914917
Epoch 260, val loss: 1.7035659551620483
Epoch 270, training loss: 321.88018798828125 = 1.6796810626983643 + 50.0 * 6.404009819030762
Epoch 270, val loss: 1.6917093992233276
Epoch 280, training loss: 321.4647216796875 = 1.6648221015930176 + 50.0 * 6.395998001098633
Epoch 280, val loss: 1.6791077852249146
Epoch 290, training loss: 321.07366943359375 = 1.6490702629089355 + 50.0 * 6.388491630554199
Epoch 290, val loss: 1.6655983924865723
Epoch 300, training loss: 320.80572509765625 = 1.6322892904281616 + 50.0 * 6.3834686279296875
Epoch 300, val loss: 1.6513224840164185
Epoch 310, training loss: 320.53155517578125 = 1.6142743825912476 + 50.0 * 6.378345489501953
Epoch 310, val loss: 1.6362532377243042
Epoch 320, training loss: 320.1793212890625 = 1.5952727794647217 + 50.0 * 6.371680736541748
Epoch 320, val loss: 1.620278239250183
Epoch 330, training loss: 319.9119873046875 = 1.5753334760665894 + 50.0 * 6.366733074188232
Epoch 330, val loss: 1.6036664247512817
Epoch 340, training loss: 319.5790710449219 = 1.554537057876587 + 50.0 * 6.360490798950195
Epoch 340, val loss: 1.5862772464752197
Epoch 350, training loss: 319.3124084472656 = 1.532976746559143 + 50.0 * 6.355588436126709
Epoch 350, val loss: 1.5685923099517822
Epoch 360, training loss: 319.11773681640625 = 1.510669231414795 + 50.0 * 6.3521409034729
Epoch 360, val loss: 1.5501362085342407
Epoch 370, training loss: 318.9808349609375 = 1.4876049757003784 + 50.0 * 6.349864482879639
Epoch 370, val loss: 1.531484842300415
Epoch 380, training loss: 318.62457275390625 = 1.464163064956665 + 50.0 * 6.343208312988281
Epoch 380, val loss: 1.5125566720962524
Epoch 390, training loss: 318.40045166015625 = 1.440411925315857 + 50.0 * 6.339200973510742
Epoch 390, val loss: 1.4935743808746338
Epoch 400, training loss: 318.2522277832031 = 1.4162940979003906 + 50.0 * 6.336719036102295
Epoch 400, val loss: 1.4745208024978638
Epoch 410, training loss: 318.0084228515625 = 1.3920670747756958 + 50.0 * 6.332327365875244
Epoch 410, val loss: 1.455430030822754
Epoch 420, training loss: 317.81109619140625 = 1.3677479028701782 + 50.0 * 6.328866958618164
Epoch 420, val loss: 1.4366984367370605
Epoch 430, training loss: 317.6000061035156 = 1.3434120416641235 + 50.0 * 6.325131893157959
Epoch 430, val loss: 1.4179763793945312
Epoch 440, training loss: 317.4269104003906 = 1.3192565441131592 + 50.0 * 6.322153568267822
Epoch 440, val loss: 1.3997126817703247
Epoch 450, training loss: 317.4661865234375 = 1.2952020168304443 + 50.0 * 6.323419570922852
Epoch 450, val loss: 1.3817270994186401
Epoch 460, training loss: 317.0899353027344 = 1.2714238166809082 + 50.0 * 6.316370010375977
Epoch 460, val loss: 1.36373770236969
Epoch 470, training loss: 316.898681640625 = 1.2479385137557983 + 50.0 * 6.313014984130859
Epoch 470, val loss: 1.3465607166290283
Epoch 480, training loss: 316.8911437988281 = 1.2248737812042236 + 50.0 * 6.313324928283691
Epoch 480, val loss: 1.3299201726913452
Epoch 490, training loss: 316.7491149902344 = 1.2020999193191528 + 50.0 * 6.310940742492676
Epoch 490, val loss: 1.3133193254470825
Epoch 500, training loss: 316.4691162109375 = 1.1796900033950806 + 50.0 * 6.305788516998291
Epoch 500, val loss: 1.2976070642471313
Epoch 510, training loss: 316.42266845703125 = 1.1579368114471436 + 50.0 * 6.305294513702393
Epoch 510, val loss: 1.2825530767440796
Epoch 520, training loss: 316.3265380859375 = 1.1363412141799927 + 50.0 * 6.30380392074585
Epoch 520, val loss: 1.2677021026611328
Epoch 530, training loss: 316.0733337402344 = 1.1153905391693115 + 50.0 * 6.299159049987793
Epoch 530, val loss: 1.2535041570663452
Epoch 540, training loss: 316.3337707519531 = 1.0950146913528442 + 50.0 * 6.304775238037109
Epoch 540, val loss: 1.2398518323898315
Epoch 550, training loss: 315.8783874511719 = 1.074884057044983 + 50.0 * 6.296070098876953
Epoch 550, val loss: 1.2267156839370728
Epoch 560, training loss: 315.7472229003906 = 1.0554403066635132 + 50.0 * 6.293835639953613
Epoch 560, val loss: 1.2143523693084717
Epoch 570, training loss: 315.66705322265625 = 1.036600947380066 + 50.0 * 6.292609214782715
Epoch 570, val loss: 1.2024524211883545
Epoch 580, training loss: 315.54290771484375 = 1.0179961919784546 + 50.0 * 6.290497779846191
Epoch 580, val loss: 1.1913396120071411
Epoch 590, training loss: 315.4737243652344 = 0.9999001622200012 + 50.0 * 6.28947639465332
Epoch 590, val loss: 1.1801987886428833
Epoch 600, training loss: 315.3541564941406 = 0.9823492169380188 + 50.0 * 6.287436008453369
Epoch 600, val loss: 1.170106053352356
Epoch 610, training loss: 315.3037414550781 = 0.9650436639785767 + 50.0 * 6.286773681640625
Epoch 610, val loss: 1.1600459814071655
Epoch 620, training loss: 315.1951599121094 = 0.9481900930404663 + 50.0 * 6.284939765930176
Epoch 620, val loss: 1.1506500244140625
Epoch 630, training loss: 315.0971374511719 = 0.9319021105766296 + 50.0 * 6.283304691314697
Epoch 630, val loss: 1.1415691375732422
Epoch 640, training loss: 315.0382995605469 = 0.9158516526222229 + 50.0 * 6.282448768615723
Epoch 640, val loss: 1.1330492496490479
Epoch 650, training loss: 314.8809814453125 = 0.9001617431640625 + 50.0 * 6.279616832733154
Epoch 650, val loss: 1.1248881816864014
Epoch 660, training loss: 314.81591796875 = 0.8849080204963684 + 50.0 * 6.27862024307251
Epoch 660, val loss: 1.1171014308929443
Epoch 670, training loss: 314.8774719238281 = 0.8699731230735779 + 50.0 * 6.280149936676025
Epoch 670, val loss: 1.1097559928894043
Epoch 680, training loss: 314.75335693359375 = 0.8552101254463196 + 50.0 * 6.277962684631348
Epoch 680, val loss: 1.102556586265564
Epoch 690, training loss: 314.6157531738281 = 0.8407409191131592 + 50.0 * 6.275500297546387
Epoch 690, val loss: 1.0961437225341797
Epoch 700, training loss: 314.5290832519531 = 0.8267443776130676 + 50.0 * 6.274046897888184
Epoch 700, val loss: 1.0898220539093018
Epoch 710, training loss: 314.64788818359375 = 0.8129856586456299 + 50.0 * 6.276698112487793
Epoch 710, val loss: 1.0836482048034668
Epoch 720, training loss: 314.35595703125 = 0.7993456125259399 + 50.0 * 6.271132469177246
Epoch 720, val loss: 1.0781307220458984
Epoch 730, training loss: 314.30218505859375 = 0.7861081957817078 + 50.0 * 6.270321369171143
Epoch 730, val loss: 1.0726571083068848
Epoch 740, training loss: 314.4172058105469 = 0.7731763124465942 + 50.0 * 6.272880554199219
Epoch 740, val loss: 1.0674186944961548
Epoch 750, training loss: 314.2696228027344 = 0.7602656483650208 + 50.0 * 6.2701873779296875
Epoch 750, val loss: 1.0626763105392456
Epoch 760, training loss: 314.1539611816406 = 0.7477312684059143 + 50.0 * 6.268124103546143
Epoch 760, val loss: 1.0581073760986328
Epoch 770, training loss: 314.0532531738281 = 0.7353795766830444 + 50.0 * 6.266357421875
Epoch 770, val loss: 1.053856611251831
Epoch 780, training loss: 313.996826171875 = 0.7232590317726135 + 50.0 * 6.265471458435059
Epoch 780, val loss: 1.0497097969055176
Epoch 790, training loss: 313.9887390136719 = 0.711409866809845 + 50.0 * 6.265546798706055
Epoch 790, val loss: 1.045876145362854
Epoch 800, training loss: 313.86065673828125 = 0.6997125148773193 + 50.0 * 6.263218879699707
Epoch 800, val loss: 1.0423028469085693
Epoch 810, training loss: 313.7974548339844 = 0.6883463859558105 + 50.0 * 6.262181758880615
Epoch 810, val loss: 1.0388294458389282
Epoch 820, training loss: 313.8338317871094 = 0.6770033240318298 + 50.0 * 6.263136386871338
Epoch 820, val loss: 1.0356239080429077
Epoch 830, training loss: 313.7393493652344 = 0.665779709815979 + 50.0 * 6.261471748352051
Epoch 830, val loss: 1.0321341753005981
Epoch 840, training loss: 313.630615234375 = 0.6548259854316711 + 50.0 * 6.259515762329102
Epoch 840, val loss: 1.0294710397720337
Epoch 850, training loss: 313.52655029296875 = 0.6441882252693176 + 50.0 * 6.2576470375061035
Epoch 850, val loss: 1.026821494102478
Epoch 860, training loss: 313.6976318359375 = 0.6337552070617676 + 50.0 * 6.261277198791504
Epoch 860, val loss: 1.0243639945983887
Epoch 870, training loss: 313.57891845703125 = 0.6233105063438416 + 50.0 * 6.2591118812561035
Epoch 870, val loss: 1.0218857526779175
Epoch 880, training loss: 313.46441650390625 = 0.6129285097122192 + 50.0 * 6.257030010223389
Epoch 880, val loss: 1.01949143409729
Epoch 890, training loss: 313.4292907714844 = 0.6028124690055847 + 50.0 * 6.256529331207275
Epoch 890, val loss: 1.0170897245407104
Epoch 900, training loss: 313.2962646484375 = 0.5928794741630554 + 50.0 * 6.254067897796631
Epoch 900, val loss: 1.0150535106658936
Epoch 910, training loss: 313.26812744140625 = 0.5831381678581238 + 50.0 * 6.253699779510498
Epoch 910, val loss: 1.0132758617401123
Epoch 920, training loss: 313.2909240722656 = 0.5735370516777039 + 50.0 * 6.254347324371338
Epoch 920, val loss: 1.011271357536316
Epoch 930, training loss: 313.21551513671875 = 0.5639649629592896 + 50.0 * 6.253031253814697
Epoch 930, val loss: 1.0095064640045166
Epoch 940, training loss: 313.1081237792969 = 0.5545561909675598 + 50.0 * 6.251071453094482
Epoch 940, val loss: 1.0079779624938965
Epoch 950, training loss: 313.0680847167969 = 0.5453547835350037 + 50.0 * 6.250454425811768
Epoch 950, val loss: 1.0062674283981323
Epoch 960, training loss: 313.0738220214844 = 0.5361573100090027 + 50.0 * 6.250753402709961
Epoch 960, val loss: 1.0047590732574463
Epoch 970, training loss: 312.9961242675781 = 0.5270602107048035 + 50.0 * 6.249381065368652
Epoch 970, val loss: 1.0031877756118774
Epoch 980, training loss: 312.9642028808594 = 0.5180922150611877 + 50.0 * 6.248922348022461
Epoch 980, val loss: 1.0017565488815308
Epoch 990, training loss: 312.8779296875 = 0.5093302130699158 + 50.0 * 6.247371673583984
Epoch 990, val loss: 1.0005311965942383
Epoch 1000, training loss: 312.9944152832031 = 0.5006639957427979 + 50.0 * 6.249874591827393
Epoch 1000, val loss: 0.9991240501403809
Epoch 1010, training loss: 312.811279296875 = 0.4918956458568573 + 50.0 * 6.246387958526611
Epoch 1010, val loss: 0.9979400634765625
Epoch 1020, training loss: 312.7686767578125 = 0.48347142338752747 + 50.0 * 6.245704174041748
Epoch 1020, val loss: 0.9967373609542847
Epoch 1030, training loss: 312.86090087890625 = 0.47505849599838257 + 50.0 * 6.247716426849365
Epoch 1030, val loss: 0.9959113001823425
Epoch 1040, training loss: 312.6538391113281 = 0.46665915846824646 + 50.0 * 6.243743896484375
Epoch 1040, val loss: 0.9945816397666931
Epoch 1050, training loss: 312.6097106933594 = 0.4585810899734497 + 50.0 * 6.243022441864014
Epoch 1050, val loss: 0.9938225746154785
Epoch 1060, training loss: 312.5520324707031 = 0.4506533741950989 + 50.0 * 6.242027282714844
Epoch 1060, val loss: 0.9931744337081909
Epoch 1070, training loss: 312.5527648925781 = 0.442817360162735 + 50.0 * 6.242198944091797
Epoch 1070, val loss: 0.9923742413520813
Epoch 1080, training loss: 312.82196044921875 = 0.43485790491104126 + 50.0 * 6.247742176055908
Epoch 1080, val loss: 0.9913346767425537
Epoch 1090, training loss: 312.62884521484375 = 0.42686185240745544 + 50.0 * 6.244040012359619
Epoch 1090, val loss: 0.9902170896530151
Epoch 1100, training loss: 312.42010498046875 = 0.419192910194397 + 50.0 * 6.240018367767334
Epoch 1100, val loss: 0.9895520210266113
Epoch 1110, training loss: 312.3658752441406 = 0.41176819801330566 + 50.0 * 6.239081859588623
Epoch 1110, val loss: 0.989260733127594
Epoch 1120, training loss: 312.32427978515625 = 0.40446004271507263 + 50.0 * 6.238396644592285
Epoch 1120, val loss: 0.9888682961463928
Epoch 1130, training loss: 312.5385437011719 = 0.3972712755203247 + 50.0 * 6.242825031280518
Epoch 1130, val loss: 0.988288938999176
Epoch 1140, training loss: 312.4027404785156 = 0.389852911233902 + 50.0 * 6.240257263183594
Epoch 1140, val loss: 0.9877524971961975
Epoch 1150, training loss: 312.3247375488281 = 0.3827096223831177 + 50.0 * 6.238840579986572
Epoch 1150, val loss: 0.9870321750640869
Epoch 1160, training loss: 312.2377624511719 = 0.3756979703903198 + 50.0 * 6.237241268157959
Epoch 1160, val loss: 0.9868686199188232
Epoch 1170, training loss: 312.1968688964844 = 0.3688432574272156 + 50.0 * 6.236560821533203
Epoch 1170, val loss: 0.9867036938667297
Epoch 1180, training loss: 312.3695068359375 = 0.36208316683769226 + 50.0 * 6.240148544311523
Epoch 1180, val loss: 0.9863719940185547
Epoch 1190, training loss: 312.2013854980469 = 0.3552999794483185 + 50.0 * 6.236922264099121
Epoch 1190, val loss: 0.9859631657600403
Epoch 1200, training loss: 312.09918212890625 = 0.3487400710582733 + 50.0 * 6.235008239746094
Epoch 1200, val loss: 0.9859610199928284
Epoch 1210, training loss: 312.2591552734375 = 0.3422870337963104 + 50.0 * 6.238337516784668
Epoch 1210, val loss: 0.9860559701919556
Epoch 1220, training loss: 312.0658264160156 = 0.3359491229057312 + 50.0 * 6.234597682952881
Epoch 1220, val loss: 0.9857868552207947
Epoch 1230, training loss: 312.0724792480469 = 0.32971692085266113 + 50.0 * 6.2348551750183105
Epoch 1230, val loss: 0.9860665798187256
Epoch 1240, training loss: 312.0188293457031 = 0.3235912621021271 + 50.0 * 6.23390531539917
Epoch 1240, val loss: 0.9858958125114441
Epoch 1250, training loss: 312.09979248046875 = 0.3175697326660156 + 50.0 * 6.235644340515137
Epoch 1250, val loss: 0.9859973192214966
Epoch 1260, training loss: 311.9757995605469 = 0.31160277128219604 + 50.0 * 6.233283519744873
Epoch 1260, val loss: 0.986358642578125
Epoch 1270, training loss: 312.1072692871094 = 0.3057730793952942 + 50.0 * 6.236029624938965
Epoch 1270, val loss: 0.9863815903663635
Epoch 1280, training loss: 311.9309997558594 = 0.3000345230102539 + 50.0 * 6.232619285583496
Epoch 1280, val loss: 0.9868820905685425
Epoch 1290, training loss: 311.82574462890625 = 0.2944703698158264 + 50.0 * 6.230625629425049
Epoch 1290, val loss: 0.9872236251831055
Epoch 1300, training loss: 311.78875732421875 = 0.28907591104507446 + 50.0 * 6.22999382019043
Epoch 1300, val loss: 0.9880567789077759
Epoch 1310, training loss: 311.9969787597656 = 0.28375208377838135 + 50.0 * 6.234264373779297
Epoch 1310, val loss: 0.9885319471359253
Epoch 1320, training loss: 311.79681396484375 = 0.27841657400131226 + 50.0 * 6.230368137359619
Epoch 1320, val loss: 0.9891387820243835
Epoch 1330, training loss: 311.806640625 = 0.2732515037059784 + 50.0 * 6.230667591094971
Epoch 1330, val loss: 0.9899911284446716
Epoch 1340, training loss: 311.9062805175781 = 0.26816293597221375 + 50.0 * 6.232762336730957
Epoch 1340, val loss: 0.9906874895095825
Epoch 1350, training loss: 311.67730712890625 = 0.26312312483787537 + 50.0 * 6.228283405303955
Epoch 1350, val loss: 0.9914991855621338
Epoch 1360, training loss: 311.6272888183594 = 0.25828224420547485 + 50.0 * 6.227380275726318
Epoch 1360, val loss: 0.9925820827484131
Epoch 1370, training loss: 311.60028076171875 = 0.2536163330078125 + 50.0 * 6.226933002471924
Epoch 1370, val loss: 0.9940142631530762
Epoch 1380, training loss: 311.8215637207031 = 0.2490309327840805 + 50.0 * 6.231451034545898
Epoch 1380, val loss: 0.9952333569526672
Epoch 1390, training loss: 311.7660827636719 = 0.2443842589855194 + 50.0 * 6.230433940887451
Epoch 1390, val loss: 0.9958935976028442
Epoch 1400, training loss: 311.6446228027344 = 0.23982661962509155 + 50.0 * 6.228095531463623
Epoch 1400, val loss: 0.9973980784416199
Epoch 1410, training loss: 311.612060546875 = 0.23545300960540771 + 50.0 * 6.227532386779785
Epoch 1410, val loss: 0.9987297058105469
Epoch 1420, training loss: 311.5074157714844 = 0.2311965972185135 + 50.0 * 6.225524425506592
Epoch 1420, val loss: 1.0004324913024902
Epoch 1430, training loss: 311.5020751953125 = 0.22704291343688965 + 50.0 * 6.225500583648682
Epoch 1430, val loss: 1.0021235942840576
Epoch 1440, training loss: 311.56500244140625 = 0.2229560911655426 + 50.0 * 6.226840972900391
Epoch 1440, val loss: 1.0038222074508667
Epoch 1450, training loss: 311.5189208984375 = 0.21892216801643372 + 50.0 * 6.22599983215332
Epoch 1450, val loss: 1.0053961277008057
Epoch 1460, training loss: 311.51239013671875 = 0.21494585275650024 + 50.0 * 6.225948810577393
Epoch 1460, val loss: 1.007197380065918
Epoch 1470, training loss: 311.4205322265625 = 0.21106016635894775 + 50.0 * 6.224189281463623
Epoch 1470, val loss: 1.0087634325027466
Epoch 1480, training loss: 311.43621826171875 = 0.20728933811187744 + 50.0 * 6.224578857421875
Epoch 1480, val loss: 1.0109796524047852
Epoch 1490, training loss: 311.55450439453125 = 0.2035079151391983 + 50.0 * 6.227019786834717
Epoch 1490, val loss: 1.0128082036972046
Epoch 1500, training loss: 311.3544921875 = 0.19979384541511536 + 50.0 * 6.2230939865112305
Epoch 1500, val loss: 1.0145219564437866
Epoch 1510, training loss: 311.3266906738281 = 0.19624750316143036 + 50.0 * 6.22260856628418
Epoch 1510, val loss: 1.0166884660720825
Epoch 1520, training loss: 311.27435302734375 = 0.19281882047653198 + 50.0 * 6.221631050109863
Epoch 1520, val loss: 1.019059658050537
Epoch 1530, training loss: 311.31512451171875 = 0.1894589215517044 + 50.0 * 6.222513198852539
Epoch 1530, val loss: 1.0212533473968506
Epoch 1540, training loss: 311.4198303222656 = 0.18609724938869476 + 50.0 * 6.224674701690674
Epoch 1540, val loss: 1.023152470588684
Epoch 1550, training loss: 311.2182312011719 = 0.1827232837677002 + 50.0 * 6.220710277557373
Epoch 1550, val loss: 1.0253986120224
Epoch 1560, training loss: 311.2188720703125 = 0.17952117323875427 + 50.0 * 6.2207865715026855
Epoch 1560, val loss: 1.0277831554412842
Epoch 1570, training loss: 311.1895446777344 = 0.17641296982765198 + 50.0 * 6.22026252746582
Epoch 1570, val loss: 1.0304360389709473
Epoch 1580, training loss: 311.32501220703125 = 0.1733950674533844 + 50.0 * 6.223032474517822
Epoch 1580, val loss: 1.0328361988067627
Epoch 1590, training loss: 311.26971435546875 = 0.17030911147594452 + 50.0 * 6.221987724304199
Epoch 1590, val loss: 1.0348994731903076
Epoch 1600, training loss: 311.1675109863281 = 0.16727809607982635 + 50.0 * 6.220005035400391
Epoch 1600, val loss: 1.037469744682312
Epoch 1610, training loss: 311.15673828125 = 0.16438083350658417 + 50.0 * 6.219847679138184
Epoch 1610, val loss: 1.0399374961853027
Epoch 1620, training loss: 311.23345947265625 = 0.16156260669231415 + 50.0 * 6.221437931060791
Epoch 1620, val loss: 1.0427296161651611
Epoch 1630, training loss: 311.08160400390625 = 0.15874384343624115 + 50.0 * 6.218457221984863
Epoch 1630, val loss: 1.0450670719146729
Epoch 1640, training loss: 311.1869201660156 = 0.15603135526180267 + 50.0 * 6.220617771148682
Epoch 1640, val loss: 1.0474166870117188
Epoch 1650, training loss: 311.1005554199219 = 0.1533096432685852 + 50.0 * 6.218944549560547
Epoch 1650, val loss: 1.0503106117248535
Epoch 1660, training loss: 311.08135986328125 = 0.15065065026283264 + 50.0 * 6.218614101409912
Epoch 1660, val loss: 1.0529828071594238
Epoch 1670, training loss: 311.04071044921875 = 0.1480715274810791 + 50.0 * 6.217852592468262
Epoch 1670, val loss: 1.0555636882781982
Epoch 1680, training loss: 311.0638732910156 = 0.14557696878910065 + 50.0 * 6.2183661460876465
Epoch 1680, val loss: 1.058588981628418
Epoch 1690, training loss: 311.0772705078125 = 0.14309062063694 + 50.0 * 6.21868371963501
Epoch 1690, val loss: 1.060894250869751
Epoch 1700, training loss: 311.05731201171875 = 0.14063246548175812 + 50.0 * 6.218333721160889
Epoch 1700, val loss: 1.0637158155441284
Epoch 1710, training loss: 310.9844970703125 = 0.13824200630187988 + 50.0 * 6.216925144195557
Epoch 1710, val loss: 1.0665428638458252
Epoch 1720, training loss: 311.16973876953125 = 0.135915145277977 + 50.0 * 6.220676422119141
Epoch 1720, val loss: 1.0689972639083862
Epoch 1730, training loss: 311.1252746582031 = 0.1335439383983612 + 50.0 * 6.219834804534912
Epoch 1730, val loss: 1.071895718574524
Epoch 1740, training loss: 310.96044921875 = 0.13123278319835663 + 50.0 * 6.216584205627441
Epoch 1740, val loss: 1.0744538307189941
Epoch 1750, training loss: 310.8845520019531 = 0.12903818488121033 + 50.0 * 6.2151103019714355
Epoch 1750, val loss: 1.0775630474090576
Epoch 1760, training loss: 310.9324645996094 = 0.1269158124923706 + 50.0 * 6.216110706329346
Epoch 1760, val loss: 1.0805613994598389
Epoch 1770, training loss: 311.0042419433594 = 0.12477252632379532 + 50.0 * 6.217589378356934
Epoch 1770, val loss: 1.0833165645599365
Epoch 1780, training loss: 310.89642333984375 = 0.1226363405585289 + 50.0 * 6.215475559234619
Epoch 1780, val loss: 1.0862997770309448
Epoch 1790, training loss: 310.87530517578125 = 0.120587557554245 + 50.0 * 6.215094566345215
Epoch 1790, val loss: 1.0891149044036865
Epoch 1800, training loss: 310.8992004394531 = 0.11858795583248138 + 50.0 * 6.215611934661865
Epoch 1800, val loss: 1.092176914215088
Epoch 1810, training loss: 310.8341979980469 = 0.11661118268966675 + 50.0 * 6.214351654052734
Epoch 1810, val loss: 1.0950342416763306
Epoch 1820, training loss: 310.8937683105469 = 0.11467958986759186 + 50.0 * 6.21558141708374
Epoch 1820, val loss: 1.0982998609542847
Epoch 1830, training loss: 310.9450988769531 = 0.1127510592341423 + 50.0 * 6.216646671295166
Epoch 1830, val loss: 1.1013489961624146
Epoch 1840, training loss: 310.8437194824219 = 0.11082661896944046 + 50.0 * 6.214657306671143
Epoch 1840, val loss: 1.1041216850280762
Epoch 1850, training loss: 310.7627868652344 = 0.10895667970180511 + 50.0 * 6.213076591491699
Epoch 1850, val loss: 1.1073709726333618
Epoch 1860, training loss: 310.7300720214844 = 0.10717777907848358 + 50.0 * 6.212457656860352
Epoch 1860, val loss: 1.1104427576065063
Epoch 1870, training loss: 310.8133850097656 = 0.10544327646493912 + 50.0 * 6.21415901184082
Epoch 1870, val loss: 1.1135748624801636
Epoch 1880, training loss: 310.7589416503906 = 0.10367360711097717 + 50.0 * 6.213105201721191
Epoch 1880, val loss: 1.1164202690124512
Epoch 1890, training loss: 310.7095642089844 = 0.10194087028503418 + 50.0 * 6.212152481079102
Epoch 1890, val loss: 1.1194603443145752
Epoch 1900, training loss: 310.7388916015625 = 0.1002708300948143 + 50.0 * 6.212772369384766
Epoch 1900, val loss: 1.1224610805511475
Epoch 1910, training loss: 310.759033203125 = 0.09863041341304779 + 50.0 * 6.213207721710205
Epoch 1910, val loss: 1.1259411573410034
Epoch 1920, training loss: 310.66156005859375 = 0.09701886028051376 + 50.0 * 6.2112908363342285
Epoch 1920, val loss: 1.1293307542800903
Epoch 1930, training loss: 310.7773132324219 = 0.0954594761133194 + 50.0 * 6.213636875152588
Epoch 1930, val loss: 1.1327804327011108
Epoch 1940, training loss: 310.7310485839844 = 0.09387552738189697 + 50.0 * 6.212743282318115
Epoch 1940, val loss: 1.1354337930679321
Epoch 1950, training loss: 310.7112121582031 = 0.0923343375325203 + 50.0 * 6.212377071380615
Epoch 1950, val loss: 1.1386312246322632
Epoch 1960, training loss: 310.74542236328125 = 0.09081149846315384 + 50.0 * 6.21309232711792
Epoch 1960, val loss: 1.1423888206481934
Epoch 1970, training loss: 310.6446533203125 = 0.08932066708803177 + 50.0 * 6.211106300354004
Epoch 1970, val loss: 1.1452555656433105
Epoch 1980, training loss: 310.6836242675781 = 0.0878879502415657 + 50.0 * 6.211914539337158
Epoch 1980, val loss: 1.1488468647003174
Epoch 1990, training loss: 310.7169189453125 = 0.08646020293235779 + 50.0 * 6.21260929107666
Epoch 1990, val loss: 1.1518115997314453
Epoch 2000, training loss: 310.7482604980469 = 0.08504801243543625 + 50.0 * 6.213263988494873
Epoch 2000, val loss: 1.1551647186279297
Epoch 2010, training loss: 310.65606689453125 = 0.08364427089691162 + 50.0 * 6.2114481925964355
Epoch 2010, val loss: 1.1584961414337158
Epoch 2020, training loss: 310.5914001464844 = 0.08229664713144302 + 50.0 * 6.210181713104248
Epoch 2020, val loss: 1.1619375944137573
Epoch 2030, training loss: 310.5318603515625 = 0.08098441362380981 + 50.0 * 6.209017276763916
Epoch 2030, val loss: 1.165191411972046
Epoch 2040, training loss: 310.67218017578125 = 0.07970330119132996 + 50.0 * 6.211849212646484
Epoch 2040, val loss: 1.1688650846481323
Epoch 2050, training loss: 310.6559143066406 = 0.0784054547548294 + 50.0 * 6.211550235748291
Epoch 2050, val loss: 1.171825647354126
Epoch 2060, training loss: 310.5255126953125 = 0.0771215483546257 + 50.0 * 6.208968162536621
Epoch 2060, val loss: 1.174864649772644
Epoch 2070, training loss: 310.4798889160156 = 0.07589486986398697 + 50.0 * 6.208079814910889
Epoch 2070, val loss: 1.1782265901565552
Epoch 2080, training loss: 310.501708984375 = 0.07471080124378204 + 50.0 * 6.208539962768555
Epoch 2080, val loss: 1.181730031967163
Epoch 2090, training loss: 310.6504211425781 = 0.07354287803173065 + 50.0 * 6.2115373611450195
Epoch 2090, val loss: 1.1849195957183838
Epoch 2100, training loss: 310.4930419921875 = 0.07233822345733643 + 50.0 * 6.208414554595947
Epoch 2100, val loss: 1.1885210275650024
Epoch 2110, training loss: 310.51873779296875 = 0.07119879871606827 + 50.0 * 6.208950996398926
Epoch 2110, val loss: 1.1919255256652832
Epoch 2120, training loss: 310.5139465332031 = 0.07007822394371033 + 50.0 * 6.2088775634765625
Epoch 2120, val loss: 1.1949423551559448
Epoch 2130, training loss: 310.6340026855469 = 0.06897268444299698 + 50.0 * 6.211300373077393
Epoch 2130, val loss: 1.1984089612960815
Epoch 2140, training loss: 310.57177734375 = 0.06785773485898972 + 50.0 * 6.210078239440918
Epoch 2140, val loss: 1.2018723487854004
Epoch 2150, training loss: 310.4323425292969 = 0.06676609069108963 + 50.0 * 6.207311630249023
Epoch 2150, val loss: 1.2053650617599487
Epoch 2160, training loss: 310.3935241699219 = 0.0657387375831604 + 50.0 * 6.2065558433532715
Epoch 2160, val loss: 1.2086740732192993
Epoch 2170, training loss: 310.3712463378906 = 0.06473369896411896 + 50.0 * 6.206130504608154
Epoch 2170, val loss: 1.2124263048171997
Epoch 2180, training loss: 310.4830322265625 = 0.06376265734434128 + 50.0 * 6.208385467529297
Epoch 2180, val loss: 1.2155340909957886
Epoch 2190, training loss: 310.47381591796875 = 0.06276320666074753 + 50.0 * 6.208220958709717
Epoch 2190, val loss: 1.2192045450210571
Epoch 2200, training loss: 310.4075622558594 = 0.061750203371047974 + 50.0 * 6.206916332244873
Epoch 2200, val loss: 1.2222148180007935
Epoch 2210, training loss: 310.3849792480469 = 0.06078791990876198 + 50.0 * 6.206483840942383
Epoch 2210, val loss: 1.2256935834884644
Epoch 2220, training loss: 310.32379150390625 = 0.059863585978746414 + 50.0 * 6.205278396606445
Epoch 2220, val loss: 1.2293106317520142
Epoch 2230, training loss: 310.3340759277344 = 0.05896930396556854 + 50.0 * 6.205502033233643
Epoch 2230, val loss: 1.2329978942871094
Epoch 2240, training loss: 310.50714111328125 = 0.05808597430586815 + 50.0 * 6.208981513977051
Epoch 2240, val loss: 1.236668348312378
Epoch 2250, training loss: 310.37762451171875 = 0.05717935785651207 + 50.0 * 6.206408500671387
Epoch 2250, val loss: 1.2393279075622559
Epoch 2260, training loss: 310.3368225097656 = 0.05629788339138031 + 50.0 * 6.205610275268555
Epoch 2260, val loss: 1.2428925037384033
Epoch 2270, training loss: 310.5185241699219 = 0.05544896051287651 + 50.0 * 6.209261417388916
Epoch 2270, val loss: 1.2464693784713745
Epoch 2280, training loss: 310.33258056640625 = 0.054581642150878906 + 50.0 * 6.205559730529785
Epoch 2280, val loss: 1.2494691610336304
Epoch 2290, training loss: 310.3357238769531 = 0.05374818295240402 + 50.0 * 6.205639839172363
Epoch 2290, val loss: 1.2531267404556274
Epoch 2300, training loss: 310.265380859375 = 0.05295482650399208 + 50.0 * 6.204248428344727
Epoch 2300, val loss: 1.256542444229126
Epoch 2310, training loss: 310.4822082519531 = 0.052183203399181366 + 50.0 * 6.2086005210876465
Epoch 2310, val loss: 1.2602169513702393
Epoch 2320, training loss: 310.2524108886719 = 0.05138665437698364 + 50.0 * 6.2040205001831055
Epoch 2320, val loss: 1.2635966539382935
Epoch 2330, training loss: 310.2178039550781 = 0.05062622204422951 + 50.0 * 6.203343391418457
Epoch 2330, val loss: 1.267072081565857
Epoch 2340, training loss: 310.2452087402344 = 0.04989900067448616 + 50.0 * 6.203906536102295
Epoch 2340, val loss: 1.2708193063735962
Epoch 2350, training loss: 310.5702819824219 = 0.049178171902894974 + 50.0 * 6.210422515869141
Epoch 2350, val loss: 1.2741121053695679
Epoch 2360, training loss: 310.30523681640625 = 0.04843574017286301 + 50.0 * 6.205136299133301
Epoch 2360, val loss: 1.2773653268814087
Epoch 2370, training loss: 310.3336181640625 = 0.0477214977145195 + 50.0 * 6.20571756362915
Epoch 2370, val loss: 1.2811007499694824
Epoch 2380, training loss: 310.3357849121094 = 0.047018103301525116 + 50.0 * 6.205775260925293
Epoch 2380, val loss: 1.2843945026397705
Epoch 2390, training loss: 310.2805480957031 = 0.046336688101291656 + 50.0 * 6.204684734344482
Epoch 2390, val loss: 1.287125825881958
Epoch 2400, training loss: 310.2096862792969 = 0.04565753787755966 + 50.0 * 6.203280448913574
Epoch 2400, val loss: 1.291148066520691
Epoch 2410, training loss: 310.22637939453125 = 0.045022185891866684 + 50.0 * 6.203627586364746
Epoch 2410, val loss: 1.2943997383117676
Epoch 2420, training loss: 310.3475341796875 = 0.04438665136694908 + 50.0 * 6.206063270568848
Epoch 2420, val loss: 1.2974241971969604
Epoch 2430, training loss: 310.2510070800781 = 0.04374537244439125 + 50.0 * 6.204145431518555
Epoch 2430, val loss: 1.3013396263122559
Epoch 2440, training loss: 310.14788818359375 = 0.043117474764585495 + 50.0 * 6.2020955085754395
Epoch 2440, val loss: 1.3044613599777222
Epoch 2450, training loss: 310.1668701171875 = 0.0425158254802227 + 50.0 * 6.202487468719482
Epoch 2450, val loss: 1.3077993392944336
Epoch 2460, training loss: 310.61138916015625 = 0.04192930832505226 + 50.0 * 6.211389064788818
Epoch 2460, val loss: 1.3109984397888184
Epoch 2470, training loss: 310.25579833984375 = 0.04129770025610924 + 50.0 * 6.20428991317749
Epoch 2470, val loss: 1.3140047788619995
Epoch 2480, training loss: 310.1134338378906 = 0.040707312524318695 + 50.0 * 6.2014546394348145
Epoch 2480, val loss: 1.3174254894256592
Epoch 2490, training loss: 310.0847473144531 = 0.04015406593680382 + 50.0 * 6.200891971588135
Epoch 2490, val loss: 1.3210103511810303
Epoch 2500, training loss: 310.1043701171875 = 0.039621852338314056 + 50.0 * 6.201294898986816
Epoch 2500, val loss: 1.3242058753967285
Epoch 2510, training loss: 310.473876953125 = 0.03909466415643692 + 50.0 * 6.208695888519287
Epoch 2510, val loss: 1.327488660812378
Epoch 2520, training loss: 310.2392272949219 = 0.03851960599422455 + 50.0 * 6.204014301300049
Epoch 2520, val loss: 1.3302730321884155
Epoch 2530, training loss: 310.20654296875 = 0.03798336535692215 + 50.0 * 6.203371047973633
Epoch 2530, val loss: 1.333838939666748
Epoch 2540, training loss: 310.11865234375 = 0.03746459633111954 + 50.0 * 6.201623439788818
Epoch 2540, val loss: 1.3371872901916504
Epoch 2550, training loss: 310.1712646484375 = 0.036964286118745804 + 50.0 * 6.202686309814453
Epoch 2550, val loss: 1.340394139289856
Epoch 2560, training loss: 310.187744140625 = 0.03646452724933624 + 50.0 * 6.2030253410339355
Epoch 2560, val loss: 1.3430625200271606
Epoch 2570, training loss: 310.0729675292969 = 0.035964805632829666 + 50.0 * 6.200740337371826
Epoch 2570, val loss: 1.3465404510498047
Epoch 2580, training loss: 310.2146911621094 = 0.035486772656440735 + 50.0 * 6.20358419418335
Epoch 2580, val loss: 1.3499512672424316
Epoch 2590, training loss: 310.1047058105469 = 0.03500550240278244 + 50.0 * 6.201394081115723
Epoch 2590, val loss: 1.3530908823013306
Epoch 2600, training loss: 310.0513610839844 = 0.03453861549496651 + 50.0 * 6.200336456298828
Epoch 2600, val loss: 1.356013298034668
Epoch 2610, training loss: 310.1284484863281 = 0.03409305587410927 + 50.0 * 6.201887130737305
Epoch 2610, val loss: 1.3592666387557983
Epoch 2620, training loss: 310.08135986328125 = 0.03364551439881325 + 50.0 * 6.200954437255859
Epoch 2620, val loss: 1.3622478246688843
Epoch 2630, training loss: 310.122802734375 = 0.033203862607479095 + 50.0 * 6.201792240142822
Epoch 2630, val loss: 1.3652209043502808
Epoch 2640, training loss: 310.06353759765625 = 0.03276268020272255 + 50.0 * 6.200614929199219
Epoch 2640, val loss: 1.3681913614273071
Epoch 2650, training loss: 310.1392822265625 = 0.032342080026865005 + 50.0 * 6.202138423919678
Epoch 2650, val loss: 1.3711028099060059
Epoch 2660, training loss: 310.04339599609375 = 0.031916286796331406 + 50.0 * 6.200229644775391
Epoch 2660, val loss: 1.374651312828064
Epoch 2670, training loss: 310.0765075683594 = 0.03150340914726257 + 50.0 * 6.200900077819824
Epoch 2670, val loss: 1.3775265216827393
Epoch 2680, training loss: 310.13836669921875 = 0.031095730140805244 + 50.0 * 6.202145576477051
Epoch 2680, val loss: 1.3806706666946411
Epoch 2690, training loss: 310.17822265625 = 0.0306883305311203 + 50.0 * 6.202950477600098
Epoch 2690, val loss: 1.3842581510543823
Epoch 2700, training loss: 309.9975891113281 = 0.03028588369488716 + 50.0 * 6.19934606552124
Epoch 2700, val loss: 1.3860044479370117
Epoch 2710, training loss: 309.9776916503906 = 0.029906587675213814 + 50.0 * 6.198955535888672
Epoch 2710, val loss: 1.3893595933914185
Epoch 2720, training loss: 309.97369384765625 = 0.029545392841100693 + 50.0 * 6.198883056640625
Epoch 2720, val loss: 1.392510175704956
Epoch 2730, training loss: 310.1369934082031 = 0.029191244393587112 + 50.0 * 6.202155590057373
Epoch 2730, val loss: 1.395505428314209
Epoch 2740, training loss: 309.97027587890625 = 0.028807535767555237 + 50.0 * 6.198829174041748
Epoch 2740, val loss: 1.3984712362289429
Epoch 2750, training loss: 310.0072021484375 = 0.028452040627598763 + 50.0 * 6.199575424194336
Epoch 2750, val loss: 1.401339054107666
Epoch 2760, training loss: 310.0127258300781 = 0.02810506522655487 + 50.0 * 6.199692249298096
Epoch 2760, val loss: 1.4043630361557007
Epoch 2770, training loss: 309.9861145019531 = 0.027756650000810623 + 50.0 * 6.199166774749756
Epoch 2770, val loss: 1.407142162322998
Epoch 2780, training loss: 310.1175842285156 = 0.027418503537774086 + 50.0 * 6.201803684234619
Epoch 2780, val loss: 1.4093000888824463
Epoch 2790, training loss: 310.0916748046875 = 0.027066214010119438 + 50.0 * 6.201292037963867
Epoch 2790, val loss: 1.4125779867172241
Epoch 2800, training loss: 309.95538330078125 = 0.02673012763261795 + 50.0 * 6.198573112487793
Epoch 2800, val loss: 1.4154317378997803
Epoch 2810, training loss: 309.90972900390625 = 0.026412324979901314 + 50.0 * 6.197666645050049
Epoch 2810, val loss: 1.4183473587036133
Epoch 2820, training loss: 309.9673156738281 = 0.026110632345080376 + 50.0 * 6.198823928833008
Epoch 2820, val loss: 1.4211814403533936
Epoch 2830, training loss: 309.9858093261719 = 0.025797029957175255 + 50.0 * 6.199200630187988
Epoch 2830, val loss: 1.4239579439163208
Epoch 2840, training loss: 309.92523193359375 = 0.025484928861260414 + 50.0 * 6.197995185852051
Epoch 2840, val loss: 1.4269548654556274
Epoch 2850, training loss: 309.99090576171875 = 0.025191837921738625 + 50.0 * 6.199314594268799
Epoch 2850, val loss: 1.4294697046279907
Epoch 2860, training loss: 309.9203186035156 = 0.024886641651391983 + 50.0 * 6.197908878326416
Epoch 2860, val loss: 1.432191252708435
Epoch 2870, training loss: 310.0401916503906 = 0.02459978312253952 + 50.0 * 6.20031213760376
Epoch 2870, val loss: 1.4347652196884155
Epoch 2880, training loss: 309.8869934082031 = 0.024297120049595833 + 50.0 * 6.197254180908203
Epoch 2880, val loss: 1.4375957250595093
Epoch 2890, training loss: 309.86248779296875 = 0.024011807516217232 + 50.0 * 6.1967692375183105
Epoch 2890, val loss: 1.440958857536316
Epoch 2900, training loss: 310.03558349609375 = 0.02374485693871975 + 50.0 * 6.200236797332764
Epoch 2900, val loss: 1.4434500932693481
Epoch 2910, training loss: 309.8456115722656 = 0.023461120203137398 + 50.0 * 6.1964430809021
Epoch 2910, val loss: 1.4460548162460327
Epoch 2920, training loss: 309.876220703125 = 0.023196851834654808 + 50.0 * 6.197060585021973
Epoch 2920, val loss: 1.4481765031814575
Epoch 2930, training loss: 310.0422058105469 = 0.022930476814508438 + 50.0 * 6.200385570526123
Epoch 2930, val loss: 1.4514400959014893
Epoch 2940, training loss: 309.8707275390625 = 0.02266649529337883 + 50.0 * 6.196960926055908
Epoch 2940, val loss: 1.454285979270935
Epoch 2950, training loss: 309.82208251953125 = 0.022412806749343872 + 50.0 * 6.195993423461914
Epoch 2950, val loss: 1.456874132156372
Epoch 2960, training loss: 309.9286193847656 = 0.022171076387166977 + 50.0 * 6.198128700256348
Epoch 2960, val loss: 1.4595226049423218
Epoch 2970, training loss: 309.809814453125 = 0.021919330582022667 + 50.0 * 6.195757865905762
Epoch 2970, val loss: 1.461902379989624
Epoch 2980, training loss: 309.8808898925781 = 0.02167791686952114 + 50.0 * 6.1971845626831055
Epoch 2980, val loss: 1.4640963077545166
Epoch 2990, training loss: 309.8674621582031 = 0.021439602598547935 + 50.0 * 6.196919918060303
Epoch 2990, val loss: 1.4669888019561768
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6962962962962963
0.8402741170268846
=== training gcn model ===
Epoch 0, training loss: 431.788330078125 = 1.9498347043991089 + 50.0 * 8.596770286560059
Epoch 0, val loss: 1.9573442935943604
Epoch 10, training loss: 431.7165832519531 = 1.9412841796875 + 50.0 * 8.595505714416504
Epoch 10, val loss: 1.9489037990570068
Epoch 20, training loss: 431.26800537109375 = 1.9304472208023071 + 50.0 * 8.586750984191895
Epoch 20, val loss: 1.937819242477417
Epoch 30, training loss: 428.4147033691406 = 1.9165704250335693 + 50.0 * 8.529962539672852
Epoch 30, val loss: 1.9233509302139282
Epoch 40, training loss: 412.0267639160156 = 1.9018861055374146 + 50.0 * 8.202497482299805
Epoch 40, val loss: 1.9083740711212158
Epoch 50, training loss: 381.6098327636719 = 1.8866288661956787 + 50.0 * 7.59446382522583
Epoch 50, val loss: 1.8931766748428345
Epoch 60, training loss: 359.6600341796875 = 1.87631356716156 + 50.0 * 7.155674457550049
Epoch 60, val loss: 1.8822969198226929
Epoch 70, training loss: 347.2800598144531 = 1.8651851415634155 + 50.0 * 6.908298015594482
Epoch 70, val loss: 1.8707741498947144
Epoch 80, training loss: 341.2589416503906 = 1.855466365814209 + 50.0 * 6.788069725036621
Epoch 80, val loss: 1.8605810403823853
Epoch 90, training loss: 337.3308410644531 = 1.8455783128738403 + 50.0 * 6.709705352783203
Epoch 90, val loss: 1.850412130355835
Epoch 100, training loss: 334.48138427734375 = 1.836256980895996 + 50.0 * 6.652903079986572
Epoch 100, val loss: 1.840570330619812
Epoch 110, training loss: 332.1947021484375 = 1.8275812864303589 + 50.0 * 6.60734224319458
Epoch 110, val loss: 1.831274390220642
Epoch 120, training loss: 330.4945373535156 = 1.819382905960083 + 50.0 * 6.573502540588379
Epoch 120, val loss: 1.8224843740463257
Epoch 130, training loss: 329.181396484375 = 1.811395525932312 + 50.0 * 6.547399997711182
Epoch 130, val loss: 1.814049482345581
Epoch 140, training loss: 327.96490478515625 = 1.8036491870880127 + 50.0 * 6.5232253074646
Epoch 140, val loss: 1.805883765220642
Epoch 150, training loss: 327.1168212890625 = 1.7961245775222778 + 50.0 * 6.50641393661499
Epoch 150, val loss: 1.7979326248168945
Epoch 160, training loss: 326.0547180175781 = 1.788454294204712 + 50.0 * 6.485325336456299
Epoch 160, val loss: 1.7900609970092773
Epoch 170, training loss: 325.2673034667969 = 1.7805472612380981 + 50.0 * 6.469735145568848
Epoch 170, val loss: 1.7820992469787598
Epoch 180, training loss: 324.66693115234375 = 1.7722406387329102 + 50.0 * 6.4578938484191895
Epoch 180, val loss: 1.7739299535751343
Epoch 190, training loss: 324.00311279296875 = 1.763252854347229 + 50.0 * 6.444797515869141
Epoch 190, val loss: 1.7654277086257935
Epoch 200, training loss: 323.3741760253906 = 1.7535725831985474 + 50.0 * 6.432412147521973
Epoch 200, val loss: 1.7564136981964111
Epoch 210, training loss: 322.8966064453125 = 1.7431163787841797 + 50.0 * 6.423069953918457
Epoch 210, val loss: 1.7467530965805054
Epoch 220, training loss: 322.4671936035156 = 1.7316538095474243 + 50.0 * 6.414710998535156
Epoch 220, val loss: 1.7363543510437012
Epoch 230, training loss: 321.97821044921875 = 1.7192962169647217 + 50.0 * 6.405178070068359
Epoch 230, val loss: 1.7251194715499878
Epoch 240, training loss: 321.5525817871094 = 1.7058430910110474 + 50.0 * 6.396934509277344
Epoch 240, val loss: 1.7130632400512695
Epoch 250, training loss: 321.47003173828125 = 1.6913195848464966 + 50.0 * 6.39557409286499
Epoch 250, val loss: 1.7000150680541992
Epoch 260, training loss: 320.87176513671875 = 1.6755318641662598 + 50.0 * 6.38392448425293
Epoch 260, val loss: 1.6859023571014404
Epoch 270, training loss: 320.4621887207031 = 1.6585298776626587 + 50.0 * 6.376072883605957
Epoch 270, val loss: 1.6708101034164429
Epoch 280, training loss: 320.1781005859375 = 1.6403355598449707 + 50.0 * 6.370754718780518
Epoch 280, val loss: 1.6546409130096436
Epoch 290, training loss: 319.9843444824219 = 1.620781660079956 + 50.0 * 6.3672709465026855
Epoch 290, val loss: 1.6373703479766846
Epoch 300, training loss: 319.628173828125 = 1.60006582736969 + 50.0 * 6.360561847686768
Epoch 300, val loss: 1.618990182876587
Epoch 310, training loss: 319.33203125 = 1.5782026052474976 + 50.0 * 6.355076313018799
Epoch 310, val loss: 1.5997847318649292
Epoch 320, training loss: 319.1237487792969 = 1.5552340745925903 + 50.0 * 6.351370334625244
Epoch 320, val loss: 1.5797055959701538
Epoch 330, training loss: 318.9708557128906 = 1.5313175916671753 + 50.0 * 6.348790645599365
Epoch 330, val loss: 1.5585289001464844
Epoch 340, training loss: 318.67364501953125 = 1.50626540184021 + 50.0 * 6.343347549438477
Epoch 340, val loss: 1.5368742942810059
Epoch 350, training loss: 318.4790344238281 = 1.480555772781372 + 50.0 * 6.339969635009766
Epoch 350, val loss: 1.5146217346191406
Epoch 360, training loss: 318.1949157714844 = 1.4542193412780762 + 50.0 * 6.334813594818115
Epoch 360, val loss: 1.4917571544647217
Epoch 370, training loss: 318.06512451171875 = 1.4273658990859985 + 50.0 * 6.332755088806152
Epoch 370, val loss: 1.4686857461929321
Epoch 380, training loss: 317.89678955078125 = 1.3999645709991455 + 50.0 * 6.329936504364014
Epoch 380, val loss: 1.4452996253967285
Epoch 390, training loss: 317.67034912109375 = 1.3723857402801514 + 50.0 * 6.3259596824646
Epoch 390, val loss: 1.4217387437820435
Epoch 400, training loss: 317.46600341796875 = 1.3445813655853271 + 50.0 * 6.3224287033081055
Epoch 400, val loss: 1.3982412815093994
Epoch 410, training loss: 317.377197265625 = 1.3167082071304321 + 50.0 * 6.32120943069458
Epoch 410, val loss: 1.3748779296875
Epoch 420, training loss: 317.29364013671875 = 1.2888996601104736 + 50.0 * 6.320094585418701
Epoch 420, val loss: 1.351766586303711
Epoch 430, training loss: 317.04669189453125 = 1.261197566986084 + 50.0 * 6.315710067749023
Epoch 430, val loss: 1.3284615278244019
Epoch 440, training loss: 316.84051513671875 = 1.2336997985839844 + 50.0 * 6.312136173248291
Epoch 440, val loss: 1.3058280944824219
Epoch 450, training loss: 316.6552734375 = 1.206642746925354 + 50.0 * 6.3089728355407715
Epoch 450, val loss: 1.2837746143341064
Epoch 460, training loss: 316.859619140625 = 1.1800116300582886 + 50.0 * 6.313591957092285
Epoch 460, val loss: 1.2621480226516724
Epoch 470, training loss: 316.4105529785156 = 1.1536526679992676 + 50.0 * 6.305137634277344
Epoch 470, val loss: 1.2409493923187256
Epoch 480, training loss: 316.2469177246094 = 1.127920150756836 + 50.0 * 6.302379608154297
Epoch 480, val loss: 1.22057044506073
Epoch 490, training loss: 316.1235656738281 = 1.1028648614883423 + 50.0 * 6.300414085388184
Epoch 490, val loss: 1.201002836227417
Epoch 500, training loss: 316.3788146972656 = 1.0785105228424072 + 50.0 * 6.306005954742432
Epoch 500, val loss: 1.1820508241653442
Epoch 510, training loss: 315.98492431640625 = 1.054421067237854 + 50.0 * 6.298610210418701
Epoch 510, val loss: 1.1639562845230103
Epoch 520, training loss: 315.8021545410156 = 1.0312153100967407 + 50.0 * 6.295418739318848
Epoch 520, val loss: 1.1464576721191406
Epoch 530, training loss: 315.6866149902344 = 1.0087655782699585 + 50.0 * 6.293557167053223
Epoch 530, val loss: 1.1297959089279175
Epoch 540, training loss: 315.60809326171875 = 0.9869910478591919 + 50.0 * 6.292421817779541
Epoch 540, val loss: 1.114020586013794
Epoch 550, training loss: 315.49658203125 = 0.9658482074737549 + 50.0 * 6.290614604949951
Epoch 550, val loss: 1.0989351272583008
Epoch 560, training loss: 315.6379089355469 = 0.9452898502349854 + 50.0 * 6.293852806091309
Epoch 560, val loss: 1.0845052003860474
Epoch 570, training loss: 315.3387145996094 = 0.9251111745834351 + 50.0 * 6.288271903991699
Epoch 570, val loss: 1.070683479309082
Epoch 580, training loss: 315.32464599609375 = 0.9056950807571411 + 50.0 * 6.288378715515137
Epoch 580, val loss: 1.0575703382492065
Epoch 590, training loss: 315.13037109375 = 0.8869136571884155 + 50.0 * 6.284869194030762
Epoch 590, val loss: 1.0447964668273926
Epoch 600, training loss: 315.04229736328125 = 0.8686714172363281 + 50.0 * 6.283472537994385
Epoch 600, val loss: 1.0329622030258179
Epoch 610, training loss: 314.96893310546875 = 0.8508409857749939 + 50.0 * 6.28236198425293
Epoch 610, val loss: 1.021619439125061
Epoch 620, training loss: 314.843505859375 = 0.8335423469543457 + 50.0 * 6.28019905090332
Epoch 620, val loss: 1.0108507871627808
Epoch 630, training loss: 314.8551330566406 = 0.8167480230331421 + 50.0 * 6.280767917633057
Epoch 630, val loss: 1.0005825757980347
Epoch 640, training loss: 314.8207092285156 = 0.8004364371299744 + 50.0 * 6.280405521392822
Epoch 640, val loss: 0.9903709292411804
Epoch 650, training loss: 314.6407165527344 = 0.7843654751777649 + 50.0 * 6.277127265930176
Epoch 650, val loss: 0.9812179207801819
Epoch 660, training loss: 314.52508544921875 = 0.768825113773346 + 50.0 * 6.275125026702881
Epoch 660, val loss: 0.9720678329467773
Epoch 670, training loss: 314.4423828125 = 0.7536873817443848 + 50.0 * 6.273773670196533
Epoch 670, val loss: 0.9638086557388306
Epoch 680, training loss: 314.735107421875 = 0.7388962507247925 + 50.0 * 6.279924392700195
Epoch 680, val loss: 0.9557438492774963
Epoch 690, training loss: 314.4303894042969 = 0.7243505120277405 + 50.0 * 6.274120330810547
Epoch 690, val loss: 0.947605311870575
Epoch 700, training loss: 314.29510498046875 = 0.7101790308952332 + 50.0 * 6.271698474884033
Epoch 700, val loss: 0.9399913549423218
Epoch 710, training loss: 314.1352233886719 = 0.6963606476783752 + 50.0 * 6.268777370452881
Epoch 710, val loss: 0.9330495595932007
Epoch 720, training loss: 314.101806640625 = 0.6829304695129395 + 50.0 * 6.268377780914307
Epoch 720, val loss: 0.9263392090797424
Epoch 730, training loss: 314.11260986328125 = 0.6696637868881226 + 50.0 * 6.268858909606934
Epoch 730, val loss: 0.9197894930839539
Epoch 740, training loss: 314.056640625 = 0.65652996301651 + 50.0 * 6.268002033233643
Epoch 740, val loss: 0.9135564565658569
Epoch 750, training loss: 314.0477600097656 = 0.6437937021255493 + 50.0 * 6.2680792808532715
Epoch 750, val loss: 0.9078436493873596
Epoch 760, training loss: 313.8326110839844 = 0.6312023997306824 + 50.0 * 6.264028072357178
Epoch 760, val loss: 0.9023606777191162
Epoch 770, training loss: 313.77886962890625 = 0.6189744472503662 + 50.0 * 6.263197898864746
Epoch 770, val loss: 0.897014856338501
Epoch 780, training loss: 313.9303283691406 = 0.607053279876709 + 50.0 * 6.266465663909912
Epoch 780, val loss: 0.8920886516571045
Epoch 790, training loss: 313.7117614746094 = 0.5951727032661438 + 50.0 * 6.262331485748291
Epoch 790, val loss: 0.8875320553779602
Epoch 800, training loss: 313.6218566894531 = 0.5836912989616394 + 50.0 * 6.260763645172119
Epoch 800, val loss: 0.8832002282142639
Epoch 810, training loss: 313.79254150390625 = 0.5724238157272339 + 50.0 * 6.264402389526367
Epoch 810, val loss: 0.879115879535675
Epoch 820, training loss: 313.7293701171875 = 0.561377227306366 + 50.0 * 6.263360023498535
Epoch 820, val loss: 0.8749127388000488
Epoch 830, training loss: 313.4124450683594 = 0.5503919720649719 + 50.0 * 6.257241249084473
Epoch 830, val loss: 0.8715441226959229
Epoch 840, training loss: 313.4064025878906 = 0.5397703647613525 + 50.0 * 6.257332801818848
Epoch 840, val loss: 0.8682283163070679
Epoch 850, training loss: 313.3205261230469 = 0.529435932636261 + 50.0 * 6.255821228027344
Epoch 850, val loss: 0.8654410243034363
Epoch 860, training loss: 313.52215576171875 = 0.5192390084266663 + 50.0 * 6.260058403015137
Epoch 860, val loss: 0.8631241321563721
Epoch 870, training loss: 313.5283203125 = 0.509188711643219 + 50.0 * 6.260382652282715
Epoch 870, val loss: 0.8597441911697388
Epoch 880, training loss: 313.2115173339844 = 0.4992648661136627 + 50.0 * 6.254245281219482
Epoch 880, val loss: 0.8574409484863281
Epoch 890, training loss: 313.1756896972656 = 0.48966243863105774 + 50.0 * 6.253720760345459
Epoch 890, val loss: 0.8554590940475464
Epoch 900, training loss: 313.2201843261719 = 0.48032835125923157 + 50.0 * 6.254797458648682
Epoch 900, val loss: 0.8534510135650635
Epoch 910, training loss: 313.156005859375 = 0.47099488973617554 + 50.0 * 6.253699779510498
Epoch 910, val loss: 0.8520122170448303
Epoch 920, training loss: 313.0117492675781 = 0.4619164764881134 + 50.0 * 6.2509965896606445
Epoch 920, val loss: 0.8502134084701538
Epoch 930, training loss: 312.93841552734375 = 0.4530583918094635 + 50.0 * 6.249707221984863
Epoch 930, val loss: 0.8491073250770569
Epoch 940, training loss: 313.0063171386719 = 0.44437000155448914 + 50.0 * 6.251238822937012
Epoch 940, val loss: 0.8481593132019043
Epoch 950, training loss: 312.9077453613281 = 0.43582725524902344 + 50.0 * 6.249438762664795
Epoch 950, val loss: 0.8468842506408691
Epoch 960, training loss: 312.9361572265625 = 0.42738208174705505 + 50.0 * 6.250175952911377
Epoch 960, val loss: 0.8460858464241028
Epoch 970, training loss: 312.8632507324219 = 0.4192182719707489 + 50.0 * 6.248880386352539
Epoch 970, val loss: 0.8453799486160278
Epoch 980, training loss: 312.7441711425781 = 0.4111956059932709 + 50.0 * 6.246659755706787
Epoch 980, val loss: 0.8450130224227905
Epoch 990, training loss: 313.1336364746094 = 0.4033948481082916 + 50.0 * 6.254604816436768
Epoch 990, val loss: 0.8443740606307983
Epoch 1000, training loss: 312.79254150390625 = 0.39554309844970703 + 50.0 * 6.2479400634765625
Epoch 1000, val loss: 0.8445175886154175
Epoch 1010, training loss: 312.6246032714844 = 0.387973427772522 + 50.0 * 6.244732856750488
Epoch 1010, val loss: 0.8442899584770203
Epoch 1020, training loss: 312.6214904785156 = 0.3806012272834778 + 50.0 * 6.24481725692749
Epoch 1020, val loss: 0.8443955183029175
Epoch 1030, training loss: 312.8753967285156 = 0.3733763098716736 + 50.0 * 6.250040054321289
Epoch 1030, val loss: 0.8446661233901978
Epoch 1040, training loss: 312.66748046875 = 0.36612439155578613 + 50.0 * 6.24602746963501
Epoch 1040, val loss: 0.8454115986824036
Epoch 1050, training loss: 312.5697021484375 = 0.35912638902664185 + 50.0 * 6.244211196899414
Epoch 1050, val loss: 0.8458399176597595
Epoch 1060, training loss: 312.4610290527344 = 0.35228946805000305 + 50.0 * 6.2421746253967285
Epoch 1060, val loss: 0.8464856743812561
Epoch 1070, training loss: 312.44232177734375 = 0.3456071615219116 + 50.0 * 6.241934299468994
Epoch 1070, val loss: 0.8474751114845276
Epoch 1080, training loss: 312.54168701171875 = 0.33906757831573486 + 50.0 * 6.244052410125732
Epoch 1080, val loss: 0.8484829664230347
Epoch 1090, training loss: 312.6117248535156 = 0.3325693607330322 + 50.0 * 6.2455830574035645
Epoch 1090, val loss: 0.8498252630233765
Epoch 1100, training loss: 312.38165283203125 = 0.32616472244262695 + 50.0 * 6.241109848022461
Epoch 1100, val loss: 0.8505493998527527
Epoch 1110, training loss: 312.3097839355469 = 0.3199552297592163 + 50.0 * 6.2397966384887695
Epoch 1110, val loss: 0.8520488142967224
Epoch 1120, training loss: 312.27178955078125 = 0.31392693519592285 + 50.0 * 6.239157199859619
Epoch 1120, val loss: 0.8536423444747925
Epoch 1130, training loss: 312.7205505371094 = 0.3080544173717499 + 50.0 * 6.2482500076293945
Epoch 1130, val loss: 0.8549661636352539
Epoch 1140, training loss: 312.2852478027344 = 0.3020888566970825 + 50.0 * 6.239663124084473
Epoch 1140, val loss: 0.8568734526634216
Epoch 1150, training loss: 312.2353820800781 = 0.29638785123825073 + 50.0 * 6.2387800216674805
Epoch 1150, val loss: 0.8586159944534302
Epoch 1160, training loss: 312.27276611328125 = 0.29078441858291626 + 50.0 * 6.239639759063721
Epoch 1160, val loss: 0.8605021238327026
Epoch 1170, training loss: 312.1396179199219 = 0.285225510597229 + 50.0 * 6.237088203430176
Epoch 1170, val loss: 0.8625403642654419
Epoch 1180, training loss: 312.08642578125 = 0.2798878848552704 + 50.0 * 6.236131191253662
Epoch 1180, val loss: 0.8646902441978455
Epoch 1190, training loss: 312.03662109375 = 0.27464622259140015 + 50.0 * 6.235239505767822
Epoch 1190, val loss: 0.8671162724494934
Epoch 1200, training loss: 312.2823486328125 = 0.2694895565509796 + 50.0 * 6.2402567863464355
Epoch 1200, val loss: 0.8696261048316956
Epoch 1210, training loss: 312.2725524902344 = 0.2643636167049408 + 50.0 * 6.240163803100586
Epoch 1210, val loss: 0.8717873096466064
Epoch 1220, training loss: 312.05645751953125 = 0.25930678844451904 + 50.0 * 6.235942840576172
Epoch 1220, val loss: 0.8738049268722534
Epoch 1230, training loss: 311.9874572753906 = 0.25437501072883606 + 50.0 * 6.234662055969238
Epoch 1230, val loss: 0.8765314221382141
Epoch 1240, training loss: 311.8904113769531 = 0.24964146316051483 + 50.0 * 6.232815742492676
Epoch 1240, val loss: 0.8795114159584045
Epoch 1250, training loss: 311.8565368652344 = 0.24499943852424622 + 50.0 * 6.2322306632995605
Epoch 1250, val loss: 0.8826062083244324
Epoch 1260, training loss: 312.22747802734375 = 0.24043789505958557 + 50.0 * 6.23974084854126
Epoch 1260, val loss: 0.8856440782546997
Epoch 1270, training loss: 312.0811767578125 = 0.2358451634645462 + 50.0 * 6.2369065284729
Epoch 1270, val loss: 0.8885751962661743
Epoch 1280, training loss: 311.80364990234375 = 0.231332927942276 + 50.0 * 6.231446743011475
Epoch 1280, val loss: 0.8915678262710571
Epoch 1290, training loss: 311.77001953125 = 0.22701263427734375 + 50.0 * 6.230859756469727
Epoch 1290, val loss: 0.895076334476471
Epoch 1300, training loss: 311.77130126953125 = 0.22281357645988464 + 50.0 * 6.2309699058532715
Epoch 1300, val loss: 0.8987487554550171
Epoch 1310, training loss: 312.0626220703125 = 0.2186393290758133 + 50.0 * 6.236879825592041
Epoch 1310, val loss: 0.9020963311195374
Epoch 1320, training loss: 311.7147216796875 = 0.21441569924354553 + 50.0 * 6.230006217956543
Epoch 1320, val loss: 0.905217170715332
Epoch 1330, training loss: 311.7068176269531 = 0.21039488911628723 + 50.0 * 6.229928493499756
Epoch 1330, val loss: 0.9095844626426697
Epoch 1340, training loss: 311.6228332519531 = 0.20649927854537964 + 50.0 * 6.228326320648193
Epoch 1340, val loss: 0.9136202931404114
Epoch 1350, training loss: 311.61309814453125 = 0.20268434286117554 + 50.0 * 6.228208065032959
Epoch 1350, val loss: 0.9175634384155273
Epoch 1360, training loss: 312.1163330078125 = 0.19895002245903015 + 50.0 * 6.23834753036499
Epoch 1360, val loss: 0.9217138886451721
Epoch 1370, training loss: 311.78570556640625 = 0.19507548213005066 + 50.0 * 6.231812477111816
Epoch 1370, val loss: 0.925542950630188
Epoch 1380, training loss: 311.636962890625 = 0.19135744869709015 + 50.0 * 6.228912353515625
Epoch 1380, val loss: 0.9293941259384155
Epoch 1390, training loss: 311.54412841796875 = 0.18779946863651276 + 50.0 * 6.227126598358154
Epoch 1390, val loss: 0.9339361190795898
Epoch 1400, training loss: 311.49322509765625 = 0.18433120846748352 + 50.0 * 6.226178169250488
Epoch 1400, val loss: 0.938615620136261
Epoch 1410, training loss: 311.6649475097656 = 0.18096709251403809 + 50.0 * 6.229679584503174
Epoch 1410, val loss: 0.9429858922958374
Epoch 1420, training loss: 311.45697021484375 = 0.1775284856557846 + 50.0 * 6.225588798522949
Epoch 1420, val loss: 0.9476118087768555
Epoch 1430, training loss: 311.44659423828125 = 0.17422053217887878 + 50.0 * 6.225447177886963
Epoch 1430, val loss: 0.9520247578620911
Epoch 1440, training loss: 311.4114685058594 = 0.17099052667617798 + 50.0 * 6.224809646606445
Epoch 1440, val loss: 0.957140326499939
Epoch 1450, training loss: 311.3993835449219 = 0.16785113513469696 + 50.0 * 6.224630832672119
Epoch 1450, val loss: 0.9620442986488342
Epoch 1460, training loss: 311.91400146484375 = 0.1647614687681198 + 50.0 * 6.234984874725342
Epoch 1460, val loss: 0.9667233824729919
Epoch 1470, training loss: 311.4078369140625 = 0.1616257280111313 + 50.0 * 6.224924564361572
Epoch 1470, val loss: 0.9711623787879944
Epoch 1480, training loss: 311.37060546875 = 0.158610001206398 + 50.0 * 6.224239826202393
Epoch 1480, val loss: 0.976438045501709
Epoch 1490, training loss: 311.3492431640625 = 0.1557071954011917 + 50.0 * 6.223870754241943
Epoch 1490, val loss: 0.981503963470459
Epoch 1500, training loss: 311.59136962890625 = 0.15288856625556946 + 50.0 * 6.228769302368164
Epoch 1500, val loss: 0.9863750338554382
Epoch 1510, training loss: 311.4551696777344 = 0.14999637007713318 + 50.0 * 6.226103782653809
Epoch 1510, val loss: 0.9915692210197449
Epoch 1520, training loss: 311.3077697753906 = 0.14721550047397614 + 50.0 * 6.22321081161499
Epoch 1520, val loss: 0.9969412088394165
Epoch 1530, training loss: 311.35467529296875 = 0.1445169448852539 + 50.0 * 6.224202632904053
Epoch 1530, val loss: 1.0023283958435059
Epoch 1540, training loss: 311.2939758300781 = 0.14185188710689545 + 50.0 * 6.2230424880981445
Epoch 1540, val loss: 1.0073856115341187
Epoch 1550, training loss: 311.2058410644531 = 0.139226034283638 + 50.0 * 6.221332550048828
Epoch 1550, val loss: 1.0127978324890137
Epoch 1560, training loss: 311.1838073730469 = 0.1367000788450241 + 50.0 * 6.220942497253418
Epoch 1560, val loss: 1.0181812047958374
Epoch 1570, training loss: 311.2719421386719 = 0.13424518704414368 + 50.0 * 6.222754001617432
Epoch 1570, val loss: 1.02367103099823
Epoch 1580, training loss: 311.20703125 = 0.13177506625652313 + 50.0 * 6.221505165100098
Epoch 1580, val loss: 1.0289742946624756
Epoch 1590, training loss: 311.132568359375 = 0.12935252487659454 + 50.0 * 6.220064163208008
Epoch 1590, val loss: 1.0344130992889404
Epoch 1600, training loss: 311.1304931640625 = 0.12702350318431854 + 50.0 * 6.220069885253906
Epoch 1600, val loss: 1.0399940013885498
Epoch 1610, training loss: 311.327392578125 = 0.12475869059562683 + 50.0 * 6.224052429199219
Epoch 1610, val loss: 1.0453541278839111
Epoch 1620, training loss: 311.130859375 = 0.1224702000617981 + 50.0 * 6.220167636871338
Epoch 1620, val loss: 1.0509923696517944
Epoch 1630, training loss: 311.09295654296875 = 0.12026778608560562 + 50.0 * 6.219453811645508
Epoch 1630, val loss: 1.0562384128570557
Epoch 1640, training loss: 311.240478515625 = 0.11814215779304504 + 50.0 * 6.222446918487549
Epoch 1640, val loss: 1.0617852210998535
Epoch 1650, training loss: 311.08245849609375 = 0.11597627401351929 + 50.0 * 6.219329833984375
Epoch 1650, val loss: 1.067453145980835
Epoch 1660, training loss: 311.0728759765625 = 0.11389677971601486 + 50.0 * 6.219179630279541
Epoch 1660, val loss: 1.0732024908065796
Epoch 1670, training loss: 311.0771484375 = 0.11189337819814682 + 50.0 * 6.21930456161499
Epoch 1670, val loss: 1.078843355178833
Epoch 1680, training loss: 311.1276550292969 = 0.10991070419549942 + 50.0 * 6.2203545570373535
Epoch 1680, val loss: 1.0848655700683594
Epoch 1690, training loss: 310.9917297363281 = 0.10796047002077103 + 50.0 * 6.21767520904541
Epoch 1690, val loss: 1.090077519416809
Epoch 1700, training loss: 311.25042724609375 = 0.10608001798391342 + 50.0 * 6.22288703918457
Epoch 1700, val loss: 1.095616340637207
Epoch 1710, training loss: 311.0416564941406 = 0.10416970402002335 + 50.0 * 6.21875
Epoch 1710, val loss: 1.101023554801941
Epoch 1720, training loss: 310.97259521484375 = 0.10233186930418015 + 50.0 * 6.217405319213867
Epoch 1720, val loss: 1.1068363189697266
Epoch 1730, training loss: 311.02716064453125 = 0.10055650025606155 + 50.0 * 6.218532085418701
Epoch 1730, val loss: 1.1127277612686157
Epoch 1740, training loss: 311.0414733886719 = 0.09878554195165634 + 50.0 * 6.218853950500488
Epoch 1740, val loss: 1.1181714534759521
Epoch 1750, training loss: 310.96087646484375 = 0.09702850133180618 + 50.0 * 6.2172770500183105
Epoch 1750, val loss: 1.1240335702896118
Epoch 1760, training loss: 310.9433288574219 = 0.09534494578838348 + 50.0 * 6.2169599533081055
Epoch 1760, val loss: 1.1301169395446777
Epoch 1770, training loss: 310.9302978515625 = 0.0936884731054306 + 50.0 * 6.216732025146484
Epoch 1770, val loss: 1.1355772018432617
Epoch 1780, training loss: 311.03515625 = 0.09208160638809204 + 50.0 * 6.2188615798950195
Epoch 1780, val loss: 1.140919804573059
Epoch 1790, training loss: 311.0268249511719 = 0.09047140926122665 + 50.0 * 6.218726634979248
Epoch 1790, val loss: 1.1459544897079468
Epoch 1800, training loss: 310.8952941894531 = 0.08886226266622543 + 50.0 * 6.216128826141357
Epoch 1800, val loss: 1.1520178318023682
Epoch 1810, training loss: 310.8407287597656 = 0.08734356611967087 + 50.0 * 6.2150678634643555
Epoch 1810, val loss: 1.1576772928237915
Epoch 1820, training loss: 310.9981384277344 = 0.08587077260017395 + 50.0 * 6.218245029449463
Epoch 1820, val loss: 1.162984013557434
Epoch 1830, training loss: 310.80572509765625 = 0.08436200767755508 + 50.0 * 6.214427471160889
Epoch 1830, val loss: 1.1687136888504028
Epoch 1840, training loss: 310.7919921875 = 0.08291119337081909 + 50.0 * 6.214181423187256
Epoch 1840, val loss: 1.1740849018096924
Epoch 1850, training loss: 310.8265075683594 = 0.08151084929704666 + 50.0 * 6.214900016784668
Epoch 1850, val loss: 1.1797618865966797
Epoch 1860, training loss: 310.967041015625 = 0.08012043684720993 + 50.0 * 6.217738628387451
Epoch 1860, val loss: 1.1855860948562622
Epoch 1870, training loss: 310.8216247558594 = 0.07874135673046112 + 50.0 * 6.214858055114746
Epoch 1870, val loss: 1.190799355506897
Epoch 1880, training loss: 310.7453308105469 = 0.07740896940231323 + 50.0 * 6.213357925415039
Epoch 1880, val loss: 1.1962469816207886
Epoch 1890, training loss: 310.9503173828125 = 0.07612647861242294 + 50.0 * 6.2174835205078125
Epoch 1890, val loss: 1.2016552686691284
Epoch 1900, training loss: 310.7729797363281 = 0.07482617348432541 + 50.0 * 6.213963031768799
Epoch 1900, val loss: 1.206822156906128
Epoch 1910, training loss: 310.7825622558594 = 0.0735659971833229 + 50.0 * 6.214179992675781
Epoch 1910, val loss: 1.2117938995361328
Epoch 1920, training loss: 310.7350158691406 = 0.07233194261789322 + 50.0 * 6.213253498077393
Epoch 1920, val loss: 1.2175493240356445
Epoch 1930, training loss: 310.7181701660156 = 0.07111886143684387 + 50.0 * 6.2129411697387695
Epoch 1930, val loss: 1.2230503559112549
Epoch 1940, training loss: 310.9211730957031 = 0.06994272768497467 + 50.0 * 6.217024803161621
Epoch 1940, val loss: 1.228562831878662
Epoch 1950, training loss: 310.7041320800781 = 0.06874514371156693 + 50.0 * 6.21270751953125
Epoch 1950, val loss: 1.2331278324127197
Epoch 1960, training loss: 310.63720703125 = 0.06759974360466003 + 50.0 * 6.211391925811768
Epoch 1960, val loss: 1.2386404275894165
Epoch 1970, training loss: 310.6551818847656 = 0.066496342420578 + 50.0 * 6.211773872375488
Epoch 1970, val loss: 1.243963599205017
Epoch 1980, training loss: 310.927978515625 = 0.06540107727050781 + 50.0 * 6.217251300811768
Epoch 1980, val loss: 1.2493051290512085
Epoch 1990, training loss: 310.75323486328125 = 0.06431367248296738 + 50.0 * 6.213778972625732
Epoch 1990, val loss: 1.2539925575256348
Epoch 2000, training loss: 310.67327880859375 = 0.06324220448732376 + 50.0 * 6.212201118469238
Epoch 2000, val loss: 1.259086012840271
Epoch 2010, training loss: 310.71978759765625 = 0.06221046298742294 + 50.0 * 6.213151454925537
Epoch 2010, val loss: 1.264082670211792
Epoch 2020, training loss: 310.5709228515625 = 0.061186786741018295 + 50.0 * 6.2101945877075195
Epoch 2020, val loss: 1.2689764499664307
Epoch 2030, training loss: 310.6590270996094 = 0.06020817160606384 + 50.0 * 6.211976051330566
Epoch 2030, val loss: 1.2742078304290771
Epoch 2040, training loss: 310.6208801269531 = 0.05921996012330055 + 50.0 * 6.211233139038086
Epoch 2040, val loss: 1.2790178060531616
Epoch 2050, training loss: 310.6112976074219 = 0.058256667107343674 + 50.0 * 6.211060523986816
Epoch 2050, val loss: 1.2839523553848267
Epoch 2060, training loss: 310.65020751953125 = 0.057316381484270096 + 50.0 * 6.211857795715332
Epoch 2060, val loss: 1.2893495559692383
Epoch 2070, training loss: 310.6539306640625 = 0.05637512728571892 + 50.0 * 6.21195125579834
Epoch 2070, val loss: 1.2940418720245361
Epoch 2080, training loss: 310.6231994628906 = 0.055469486862421036 + 50.0 * 6.2113542556762695
Epoch 2080, val loss: 1.2985610961914062
Epoch 2090, training loss: 310.5960388183594 = 0.05456385388970375 + 50.0 * 6.210829734802246
Epoch 2090, val loss: 1.303309440612793
Epoch 2100, training loss: 310.5512390136719 = 0.05369209498167038 + 50.0 * 6.209950923919678
Epoch 2100, val loss: 1.3083003759384155
Epoch 2110, training loss: 310.5546875 = 0.05283503234386444 + 50.0 * 6.2100372314453125
Epoch 2110, val loss: 1.313231110572815
Epoch 2120, training loss: 310.5326843261719 = 0.05199921876192093 + 50.0 * 6.209613800048828
Epoch 2120, val loss: 1.3178824186325073
Epoch 2130, training loss: 310.5517578125 = 0.051173605024814606 + 50.0 * 6.2100114822387695
Epoch 2130, val loss: 1.3224713802337646
Epoch 2140, training loss: 310.55206298828125 = 0.050361014902591705 + 50.0 * 6.210034370422363
Epoch 2140, val loss: 1.3270491361618042
Epoch 2150, training loss: 310.61383056640625 = 0.04954918846487999 + 50.0 * 6.21128511428833
Epoch 2150, val loss: 1.3315750360488892
Epoch 2160, training loss: 310.5354919433594 = 0.04876203462481499 + 50.0 * 6.2097344398498535
Epoch 2160, val loss: 1.3361263275146484
Epoch 2170, training loss: 310.5142517089844 = 0.048004284501075745 + 50.0 * 6.209324836730957
Epoch 2170, val loss: 1.3405526876449585
Epoch 2180, training loss: 310.77435302734375 = 0.04726867750287056 + 50.0 * 6.214541912078857
Epoch 2180, val loss: 1.3447474241256714
Epoch 2190, training loss: 310.5207824707031 = 0.046472206711769104 + 50.0 * 6.20948600769043
Epoch 2190, val loss: 1.350090742111206
Epoch 2200, training loss: 310.4273681640625 = 0.04576195776462555 + 50.0 * 6.207632064819336
Epoch 2200, val loss: 1.3539412021636963
Epoch 2210, training loss: 310.4012756347656 = 0.04505405202507973 + 50.0 * 6.207124710083008
Epoch 2210, val loss: 1.3591045141220093
Epoch 2220, training loss: 310.6016540527344 = 0.0443778820335865 + 50.0 * 6.211145877838135
Epoch 2220, val loss: 1.363352656364441
Epoch 2230, training loss: 310.3817138671875 = 0.04366717487573624 + 50.0 * 6.206760883331299
Epoch 2230, val loss: 1.3676172494888306
Epoch 2240, training loss: 310.42974853515625 = 0.04299518093466759 + 50.0 * 6.207735061645508
Epoch 2240, val loss: 1.3718628883361816
Epoch 2250, training loss: 310.49932861328125 = 0.042333293706178665 + 50.0 * 6.209139823913574
Epoch 2250, val loss: 1.376125454902649
Epoch 2260, training loss: 310.3565979003906 = 0.041684556752443314 + 50.0 * 6.206298351287842
Epoch 2260, val loss: 1.3802814483642578
Epoch 2270, training loss: 310.3926696777344 = 0.04105774685740471 + 50.0 * 6.207032680511475
Epoch 2270, val loss: 1.3848124742507935
Epoch 2280, training loss: 310.512939453125 = 0.04043453559279442 + 50.0 * 6.2094502449035645
Epoch 2280, val loss: 1.3889364004135132
Epoch 2290, training loss: 310.40057373046875 = 0.03980633616447449 + 50.0 * 6.207215785980225
Epoch 2290, val loss: 1.3932520151138306
Epoch 2300, training loss: 310.4190673828125 = 0.03920380026102066 + 50.0 * 6.207596778869629
Epoch 2300, val loss: 1.3974902629852295
Epoch 2310, training loss: 310.3100280761719 = 0.038615286350250244 + 50.0 * 6.205428600311279
Epoch 2310, val loss: 1.4015902280807495
Epoch 2320, training loss: 310.3125 = 0.03804772347211838 + 50.0 * 6.205489158630371
Epoch 2320, val loss: 1.4059494733810425
Epoch 2330, training loss: 310.5452880859375 = 0.03750186786055565 + 50.0 * 6.210155487060547
Epoch 2330, val loss: 1.409783959388733
Epoch 2340, training loss: 310.4351806640625 = 0.03690900281071663 + 50.0 * 6.207965850830078
Epoch 2340, val loss: 1.4142751693725586
Epoch 2350, training loss: 310.3394470214844 = 0.0363590344786644 + 50.0 * 6.206061840057373
Epoch 2350, val loss: 1.4180974960327148
Epoch 2360, training loss: 310.28009033203125 = 0.03581462800502777 + 50.0 * 6.204885482788086
Epoch 2360, val loss: 1.4225547313690186
Epoch 2370, training loss: 310.248291015625 = 0.035298679023981094 + 50.0 * 6.204259395599365
Epoch 2370, val loss: 1.4266160726547241
Epoch 2380, training loss: 310.40972900390625 = 0.0348009318113327 + 50.0 * 6.207498550415039
Epoch 2380, val loss: 1.4305038452148438
Epoch 2390, training loss: 310.40618896484375 = 0.03429613262414932 + 50.0 * 6.207437992095947
Epoch 2390, val loss: 1.4349067211151123
Epoch 2400, training loss: 310.29583740234375 = 0.03377050533890724 + 50.0 * 6.2052412033081055
Epoch 2400, val loss: 1.438486933708191
Epoch 2410, training loss: 310.2941589355469 = 0.033279962837696075 + 50.0 * 6.205217361450195
Epoch 2410, val loss: 1.4423646926879883
Epoch 2420, training loss: 310.29766845703125 = 0.032801900058984756 + 50.0 * 6.205297470092773
Epoch 2420, val loss: 1.4464040994644165
Epoch 2430, training loss: 310.2185974121094 = 0.032331354916095734 + 50.0 * 6.203725814819336
Epoch 2430, val loss: 1.4504941701889038
Epoch 2440, training loss: 310.37255859375 = 0.03187757730484009 + 50.0 * 6.206813335418701
Epoch 2440, val loss: 1.4547038078308105
Epoch 2450, training loss: 310.2581787109375 = 0.03142157569527626 + 50.0 * 6.204535007476807
Epoch 2450, val loss: 1.4579178094863892
Epoch 2460, training loss: 310.2618713378906 = 0.030974047258496284 + 50.0 * 6.204617977142334
Epoch 2460, val loss: 1.461596131324768
Epoch 2470, training loss: 310.2991638183594 = 0.03053976036608219 + 50.0 * 6.205372333526611
Epoch 2470, val loss: 1.4657336473464966
Epoch 2480, training loss: 310.264404296875 = 0.030115768313407898 + 50.0 * 6.204685688018799
Epoch 2480, val loss: 1.4693331718444824
Epoch 2490, training loss: 310.35430908203125 = 0.029686685651540756 + 50.0 * 6.2064924240112305
Epoch 2490, val loss: 1.4732110500335693
Epoch 2500, training loss: 310.2264709472656 = 0.029266824945807457 + 50.0 * 6.203944206237793
Epoch 2500, val loss: 1.4770307540893555
Epoch 2510, training loss: 310.17962646484375 = 0.028866257518529892 + 50.0 * 6.203015327453613
Epoch 2510, val loss: 1.4807628393173218
Epoch 2520, training loss: 310.371337890625 = 0.028478022664785385 + 50.0 * 6.206857204437256
Epoch 2520, val loss: 1.4849939346313477
Epoch 2530, training loss: 310.1401672363281 = 0.02807946316897869 + 50.0 * 6.202241897583008
Epoch 2530, val loss: 1.4879429340362549
Epoch 2540, training loss: 310.2380676269531 = 0.027703287079930305 + 50.0 * 6.204206943511963
Epoch 2540, val loss: 1.4916939735412598
Epoch 2550, training loss: 310.23834228515625 = 0.0273158960044384 + 50.0 * 6.204220294952393
Epoch 2550, val loss: 1.4951996803283691
Epoch 2560, training loss: 310.1800842285156 = 0.026957262307405472 + 50.0 * 6.203063011169434
Epoch 2560, val loss: 1.499011516571045
Epoch 2570, training loss: 310.29290771484375 = 0.026605183258652687 + 50.0 * 6.205326080322266
Epoch 2570, val loss: 1.5024054050445557
Epoch 2580, training loss: 310.13568115234375 = 0.026234524324536324 + 50.0 * 6.202188491821289
Epoch 2580, val loss: 1.5059232711791992
Epoch 2590, training loss: 310.1835021972656 = 0.025892067700624466 + 50.0 * 6.203152179718018
Epoch 2590, val loss: 1.5098921060562134
Epoch 2600, training loss: 310.2214660644531 = 0.02555118501186371 + 50.0 * 6.20391845703125
Epoch 2600, val loss: 1.5130189657211304
Epoch 2610, training loss: 310.1399841308594 = 0.025211622938513756 + 50.0 * 6.202295303344727
Epoch 2610, val loss: 1.5168792009353638
Epoch 2620, training loss: 310.2627258300781 = 0.024884743615984917 + 50.0 * 6.204757213592529
Epoch 2620, val loss: 1.5201905965805054
Epoch 2630, training loss: 310.1222229003906 = 0.0245553869754076 + 50.0 * 6.201953411102295
Epoch 2630, val loss: 1.5237587690353394
Epoch 2640, training loss: 310.1407470703125 = 0.0242388267070055 + 50.0 * 6.202330112457275
Epoch 2640, val loss: 1.526691198348999
Epoch 2650, training loss: 310.1627197265625 = 0.023925824090838432 + 50.0 * 6.202775955200195
Epoch 2650, val loss: 1.5302338600158691
Epoch 2660, training loss: 310.0706787109375 = 0.023615702986717224 + 50.0 * 6.20094108581543
Epoch 2660, val loss: 1.533437728881836
Epoch 2670, training loss: 310.1986999511719 = 0.023326124995946884 + 50.0 * 6.203507900238037
Epoch 2670, val loss: 1.5369441509246826
Epoch 2680, training loss: 310.2850036621094 = 0.02302386797964573 + 50.0 * 6.205239772796631
Epoch 2680, val loss: 1.539997935295105
Epoch 2690, training loss: 310.0718688964844 = 0.022710422053933144 + 50.0 * 6.20098352432251
Epoch 2690, val loss: 1.5430175065994263
Epoch 2700, training loss: 310.03802490234375 = 0.02242599055171013 + 50.0 * 6.20031213760376
Epoch 2700, val loss: 1.546461820602417
Epoch 2710, training loss: 310.0924987792969 = 0.0221527311950922 + 50.0 * 6.201406955718994
Epoch 2710, val loss: 1.5504117012023926
Epoch 2720, training loss: 310.2502746582031 = 0.021877417340874672 + 50.0 * 6.204567909240723
Epoch 2720, val loss: 1.5535035133361816
Epoch 2730, training loss: 310.1033020019531 = 0.021608712151646614 + 50.0 * 6.201633930206299
Epoch 2730, val loss: 1.5559377670288086
Epoch 2740, training loss: 310.0667724609375 = 0.02134224772453308 + 50.0 * 6.200908660888672
Epoch 2740, val loss: 1.5598045587539673
Epoch 2750, training loss: 310.1726989746094 = 0.0210863146930933 + 50.0 * 6.203032493591309
Epoch 2750, val loss: 1.563157558441162
Epoch 2760, training loss: 310.0694274902344 = 0.020827338099479675 + 50.0 * 6.200972557067871
Epoch 2760, val loss: 1.5656135082244873
Epoch 2770, training loss: 310.0537109375 = 0.020576883107423782 + 50.0 * 6.200662612915039
Epoch 2770, val loss: 1.569231629371643
Epoch 2780, training loss: 310.1081848144531 = 0.020338965579867363 + 50.0 * 6.201757431030273
Epoch 2780, val loss: 1.571785569190979
Epoch 2790, training loss: 310.136474609375 = 0.02009422332048416 + 50.0 * 6.202327728271484
Epoch 2790, val loss: 1.5748857259750366
Epoch 2800, training loss: 309.9929504394531 = 0.019841982051730156 + 50.0 * 6.199462413787842
Epoch 2800, val loss: 1.577834963798523
Epoch 2810, training loss: 309.98260498046875 = 0.019606681540608406 + 50.0 * 6.199260234832764
Epoch 2810, val loss: 1.5811814069747925
Epoch 2820, training loss: 309.9747009277344 = 0.01938137412071228 + 50.0 * 6.199106216430664
Epoch 2820, val loss: 1.58449387550354
Epoch 2830, training loss: 310.22735595703125 = 0.01916269212961197 + 50.0 * 6.204164028167725
Epoch 2830, val loss: 1.587435245513916
Epoch 2840, training loss: 310.2132568359375 = 0.01893521659076214 + 50.0 * 6.20388650894165
Epoch 2840, val loss: 1.5889912843704224
Epoch 2850, training loss: 310.0516357421875 = 0.018695421516895294 + 50.0 * 6.200658798217773
Epoch 2850, val loss: 1.592542290687561
Epoch 2860, training loss: 309.96221923828125 = 0.018481986597180367 + 50.0 * 6.198874473571777
Epoch 2860, val loss: 1.5956377983093262
Epoch 2870, training loss: 309.9188537597656 = 0.018274499103426933 + 50.0 * 6.19801139831543
Epoch 2870, val loss: 1.5988284349441528
Epoch 2880, training loss: 309.9801940917969 = 0.018079908564686775 + 50.0 * 6.19924259185791
Epoch 2880, val loss: 1.6019954681396484
Epoch 2890, training loss: 310.1802673339844 = 0.017882876098155975 + 50.0 * 6.203247547149658
Epoch 2890, val loss: 1.6047178506851196
Epoch 2900, training loss: 310.0388488769531 = 0.017661411315202713 + 50.0 * 6.200423717498779
Epoch 2900, val loss: 1.6068538427352905
Epoch 2910, training loss: 309.98236083984375 = 0.017465081065893173 + 50.0 * 6.199297904968262
Epoch 2910, val loss: 1.6102302074432373
Epoch 2920, training loss: 310.13031005859375 = 0.017269708216190338 + 50.0 * 6.202260971069336
Epoch 2920, val loss: 1.6131105422973633
Epoch 2930, training loss: 310.0432434082031 = 0.017073145136237144 + 50.0 * 6.200523376464844
Epoch 2930, val loss: 1.6156996488571167
Epoch 2940, training loss: 309.9273986816406 = 0.016884293407201767 + 50.0 * 6.1982102394104
Epoch 2940, val loss: 1.618444800376892
Epoch 2950, training loss: 309.9144592285156 = 0.016698328778147697 + 50.0 * 6.197955131530762
Epoch 2950, val loss: 1.621384859085083
Epoch 2960, training loss: 310.0618896484375 = 0.01652439683675766 + 50.0 * 6.2009077072143555
Epoch 2960, val loss: 1.6240086555480957
Epoch 2970, training loss: 309.8818359375 = 0.016338687390089035 + 50.0 * 6.197310447692871
Epoch 2970, val loss: 1.6268023252487183
Epoch 2980, training loss: 309.9186706542969 = 0.016163459047675133 + 50.0 * 6.198050022125244
Epoch 2980, val loss: 1.6295583248138428
Epoch 2990, training loss: 309.9504699707031 = 0.015991700813174248 + 50.0 * 6.1986894607543945
Epoch 2990, val loss: 1.6323304176330566
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.725925925925926
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 431.7730407714844 = 1.9328128099441528 + 50.0 * 8.59680461883545
Epoch 0, val loss: 1.9329370260238647
Epoch 10, training loss: 431.7156066894531 = 1.9245173931121826 + 50.0 * 8.595821380615234
Epoch 10, val loss: 1.9240361452102661
Epoch 20, training loss: 431.37335205078125 = 1.914122462272644 + 50.0 * 8.589184761047363
Epoch 20, val loss: 1.9127883911132812
Epoch 30, training loss: 429.09356689453125 = 1.9003937244415283 + 50.0 * 8.543863296508789
Epoch 30, val loss: 1.8978513479232788
Epoch 40, training loss: 412.373779296875 = 1.884034514427185 + 50.0 * 8.209794998168945
Epoch 40, val loss: 1.8803526163101196
Epoch 50, training loss: 373.7580871582031 = 1.866592288017273 + 50.0 * 7.437829971313477
Epoch 50, val loss: 1.8623993396759033
Epoch 60, training loss: 356.0937805175781 = 1.8552545309066772 + 50.0 * 7.084770202636719
Epoch 60, val loss: 1.851204752922058
Epoch 70, training loss: 348.6924133300781 = 1.8446381092071533 + 50.0 * 6.936955451965332
Epoch 70, val loss: 1.840341567993164
Epoch 80, training loss: 344.0186462402344 = 1.834848165512085 + 50.0 * 6.8436760902404785
Epoch 80, val loss: 1.8303555250167847
Epoch 90, training loss: 339.457763671875 = 1.8256101608276367 + 50.0 * 6.752642631530762
Epoch 90, val loss: 1.821183681488037
Epoch 100, training loss: 335.90911865234375 = 1.8176422119140625 + 50.0 * 6.681829452514648
Epoch 100, val loss: 1.8132195472717285
Epoch 110, training loss: 333.4428405761719 = 1.8102130889892578 + 50.0 * 6.632652282714844
Epoch 110, val loss: 1.8055508136749268
Epoch 120, training loss: 331.4796447753906 = 1.802984356880188 + 50.0 * 6.593533515930176
Epoch 120, val loss: 1.798033595085144
Epoch 130, training loss: 329.9440612792969 = 1.795913577079773 + 50.0 * 6.562963008880615
Epoch 130, val loss: 1.7906761169433594
Epoch 140, training loss: 328.6781921386719 = 1.7888273000717163 + 50.0 * 6.537787437438965
Epoch 140, val loss: 1.7833800315856934
Epoch 150, training loss: 327.5661926269531 = 1.7814934253692627 + 50.0 * 6.5156941413879395
Epoch 150, val loss: 1.7759208679199219
Epoch 160, training loss: 326.6488037109375 = 1.773801565170288 + 50.0 * 6.497499942779541
Epoch 160, val loss: 1.7682439088821411
Epoch 170, training loss: 325.9632263183594 = 1.7656868696212769 + 50.0 * 6.483950614929199
Epoch 170, val loss: 1.7602506875991821
Epoch 180, training loss: 325.09808349609375 = 1.7568796873092651 + 50.0 * 6.466824054718018
Epoch 180, val loss: 1.7517579793930054
Epoch 190, training loss: 324.39801025390625 = 1.7473845481872559 + 50.0 * 6.453012466430664
Epoch 190, val loss: 1.7426962852478027
Epoch 200, training loss: 323.8398742675781 = 1.737045168876648 + 50.0 * 6.442056655883789
Epoch 200, val loss: 1.7329405546188354
Epoch 210, training loss: 323.29364013671875 = 1.7256888151168823 + 50.0 * 6.43135929107666
Epoch 210, val loss: 1.7223023176193237
Epoch 220, training loss: 322.7216491699219 = 1.7133700847625732 + 50.0 * 6.420165538787842
Epoch 220, val loss: 1.7107336521148682
Epoch 230, training loss: 322.3269348144531 = 1.6999236345291138 + 50.0 * 6.412540435791016
Epoch 230, val loss: 1.6981663703918457
Epoch 240, training loss: 321.9126281738281 = 1.6851834058761597 + 50.0 * 6.4045491218566895
Epoch 240, val loss: 1.6844104528427124
Epoch 250, training loss: 321.4970397949219 = 1.6692321300506592 + 50.0 * 6.396556377410889
Epoch 250, val loss: 1.669572114944458
Epoch 260, training loss: 321.1190490722656 = 1.6520721912384033 + 50.0 * 6.389339447021484
Epoch 260, val loss: 1.6535552740097046
Epoch 270, training loss: 320.8415222167969 = 1.6337058544158936 + 50.0 * 6.384156703948975
Epoch 270, val loss: 1.6365015506744385
Epoch 280, training loss: 320.5368347167969 = 1.6141116619110107 + 50.0 * 6.378454685211182
Epoch 280, val loss: 1.6182680130004883
Epoch 290, training loss: 320.1906433105469 = 1.5933781862258911 + 50.0 * 6.371944904327393
Epoch 290, val loss: 1.5990941524505615
Epoch 300, training loss: 319.8588562011719 = 1.5716955661773682 + 50.0 * 6.365743637084961
Epoch 300, val loss: 1.579174518585205
Epoch 310, training loss: 319.8289489746094 = 1.5491679906845093 + 50.0 * 6.365595817565918
Epoch 310, val loss: 1.5586777925491333
Epoch 320, training loss: 319.4073181152344 = 1.5257279872894287 + 50.0 * 6.357631683349609
Epoch 320, val loss: 1.5373166799545288
Epoch 330, training loss: 319.1136474609375 = 1.501737356185913 + 50.0 * 6.352238178253174
Epoch 330, val loss: 1.5156605243682861
Epoch 340, training loss: 318.8550720214844 = 1.4773973226547241 + 50.0 * 6.347553730010986
Epoch 340, val loss: 1.49391508102417
Epoch 350, training loss: 318.62188720703125 = 1.4527924060821533 + 50.0 * 6.343381881713867
Epoch 350, val loss: 1.4721558094024658
Epoch 360, training loss: 318.5174865722656 = 1.4280661344528198 + 50.0 * 6.341788291931152
Epoch 360, val loss: 1.450522541999817
Epoch 370, training loss: 318.2801818847656 = 1.403106927871704 + 50.0 * 6.337541580200195
Epoch 370, val loss: 1.4289391040802002
Epoch 380, training loss: 317.9871826171875 = 1.378281593322754 + 50.0 * 6.332177639007568
Epoch 380, val loss: 1.4076075553894043
Epoch 390, training loss: 317.784912109375 = 1.3536508083343506 + 50.0 * 6.328625202178955
Epoch 390, val loss: 1.3866877555847168
Epoch 400, training loss: 317.6879577636719 = 1.329162359237671 + 50.0 * 6.327175617218018
Epoch 400, val loss: 1.3660945892333984
Epoch 410, training loss: 317.5928649902344 = 1.3046565055847168 + 50.0 * 6.325764179229736
Epoch 410, val loss: 1.3459374904632568
Epoch 420, training loss: 317.3385314941406 = 1.2804619073867798 + 50.0 * 6.321161270141602
Epoch 420, val loss: 1.3259971141815186
Epoch 430, training loss: 317.1933898925781 = 1.2564200162887573 + 50.0 * 6.318739414215088
Epoch 430, val loss: 1.3063956499099731
Epoch 440, training loss: 316.9825134277344 = 1.232703685760498 + 50.0 * 6.314996242523193
Epoch 440, val loss: 1.2872889041900635
Epoch 450, training loss: 316.8134765625 = 1.2093536853790283 + 50.0 * 6.312082767486572
Epoch 450, val loss: 1.2686233520507812
Epoch 460, training loss: 316.71026611328125 = 1.1862818002700806 + 50.0 * 6.310479640960693
Epoch 460, val loss: 1.250447154045105
Epoch 470, training loss: 316.5898742675781 = 1.1634156703948975 + 50.0 * 6.308528900146484
Epoch 470, val loss: 1.2325029373168945
Epoch 480, training loss: 316.439208984375 = 1.1408109664916992 + 50.0 * 6.305968284606934
Epoch 480, val loss: 1.2149345874786377
Epoch 490, training loss: 316.5115661621094 = 1.1185897588729858 + 50.0 * 6.307859420776367
Epoch 490, val loss: 1.1979033946990967
Epoch 500, training loss: 316.18829345703125 = 1.0967512130737305 + 50.0 * 6.301830768585205
Epoch 500, val loss: 1.181311011314392
Epoch 510, training loss: 316.0307922363281 = 1.0753885507583618 + 50.0 * 6.299108028411865
Epoch 510, val loss: 1.1652363538742065
Epoch 520, training loss: 316.1399841308594 = 1.054530143737793 + 50.0 * 6.301708698272705
Epoch 520, val loss: 1.1495791673660278
Epoch 530, training loss: 315.9302062988281 = 1.0336731672286987 + 50.0 * 6.297930717468262
Epoch 530, val loss: 1.1345734596252441
Epoch 540, training loss: 315.7445983886719 = 1.0134191513061523 + 50.0 * 6.294623374938965
Epoch 540, val loss: 1.119799256324768
Epoch 550, training loss: 315.600341796875 = 0.9937352538108826 + 50.0 * 6.2921319007873535
Epoch 550, val loss: 1.1057270765304565
Epoch 560, training loss: 315.6534729003906 = 0.9745784401893616 + 50.0 * 6.293578147888184
Epoch 560, val loss: 1.0922279357910156
Epoch 570, training loss: 315.5592346191406 = 0.9555885195732117 + 50.0 * 6.292072772979736
Epoch 570, val loss: 1.0790059566497803
Epoch 580, training loss: 315.2884521484375 = 0.9371153712272644 + 50.0 * 6.287026882171631
Epoch 580, val loss: 1.06636643409729
Epoch 590, training loss: 315.2177429199219 = 0.9191614389419556 + 50.0 * 6.285971641540527
Epoch 590, val loss: 1.0543544292449951
Epoch 600, training loss: 315.3119812011719 = 0.9015536904335022 + 50.0 * 6.288208484649658
Epoch 600, val loss: 1.0427411794662476
Epoch 610, training loss: 315.0592346191406 = 0.8841648101806641 + 50.0 * 6.283501625061035
Epoch 610, val loss: 1.0314183235168457
Epoch 620, training loss: 314.9749450683594 = 0.8673040270805359 + 50.0 * 6.2821526527404785
Epoch 620, val loss: 1.0205811262130737
Epoch 630, training loss: 314.9426574707031 = 0.8508132696151733 + 50.0 * 6.281836986541748
Epoch 630, val loss: 1.0103553533554077
Epoch 640, training loss: 314.8354187011719 = 0.8345940709114075 + 50.0 * 6.2800164222717285
Epoch 640, val loss: 1.000443935394287
Epoch 650, training loss: 314.7077941894531 = 0.8186803460121155 + 50.0 * 6.277781963348389
Epoch 650, val loss: 0.9908725023269653
Epoch 660, training loss: 314.6611022949219 = 0.8031212687492371 + 50.0 * 6.277159214019775
Epoch 660, val loss: 0.981709361076355
Epoch 670, training loss: 314.7356262207031 = 0.7878872752189636 + 50.0 * 6.278954982757568
Epoch 670, val loss: 0.9730032682418823
Epoch 680, training loss: 314.55133056640625 = 0.7727672457695007 + 50.0 * 6.275571346282959
Epoch 680, val loss: 0.9644206762313843
Epoch 690, training loss: 314.4603271484375 = 0.7580936551094055 + 50.0 * 6.274044513702393
Epoch 690, val loss: 0.9563040137290955
Epoch 700, training loss: 314.3601989746094 = 0.7436668872833252 + 50.0 * 6.2723307609558105
Epoch 700, val loss: 0.9485946297645569
Epoch 710, training loss: 314.2303161621094 = 0.7294063568115234 + 50.0 * 6.270018577575684
Epoch 710, val loss: 0.940991222858429
Epoch 720, training loss: 314.39093017578125 = 0.7155216932296753 + 50.0 * 6.273508071899414
Epoch 720, val loss: 0.9339295625686646
Epoch 730, training loss: 314.24761962890625 = 0.7016836404800415 + 50.0 * 6.270918846130371
Epoch 730, val loss: 0.9267541766166687
Epoch 740, training loss: 314.0848388671875 = 0.6880554556846619 + 50.0 * 6.267935752868652
Epoch 740, val loss: 0.9200619459152222
Epoch 750, training loss: 314.0481262207031 = 0.6747859716415405 + 50.0 * 6.267467021942139
Epoch 750, val loss: 0.9136708974838257
Epoch 760, training loss: 313.9468994140625 = 0.6616353988647461 + 50.0 * 6.265705585479736
Epoch 760, val loss: 0.9073274731636047
Epoch 770, training loss: 314.0580139160156 = 0.6486874222755432 + 50.0 * 6.268186569213867
Epoch 770, val loss: 0.9012710452079773
Epoch 780, training loss: 313.8575439453125 = 0.6358712315559387 + 50.0 * 6.26443338394165
Epoch 780, val loss: 0.8953689336776733
Epoch 790, training loss: 313.7377014160156 = 0.623185932636261 + 50.0 * 6.262290000915527
Epoch 790, val loss: 0.889815628528595
Epoch 800, training loss: 313.86444091796875 = 0.610763430595398 + 50.0 * 6.265073776245117
Epoch 800, val loss: 0.8842903971672058
Epoch 810, training loss: 313.81622314453125 = 0.5984618663787842 + 50.0 * 6.264355659484863
Epoch 810, val loss: 0.879037082195282
Epoch 820, training loss: 313.6209411621094 = 0.5861896276473999 + 50.0 * 6.260694980621338
Epoch 820, val loss: 0.8738765716552734
Epoch 830, training loss: 313.5173645019531 = 0.5743104219436646 + 50.0 * 6.258861064910889
Epoch 830, val loss: 0.8688989877700806
Epoch 840, training loss: 313.5932312011719 = 0.5625681281089783 + 50.0 * 6.260613441467285
Epoch 840, val loss: 0.8642187118530273
Epoch 850, training loss: 313.5785217285156 = 0.550813615322113 + 50.0 * 6.260554313659668
Epoch 850, val loss: 0.8595215678215027
Epoch 860, training loss: 313.40118408203125 = 0.5391957759857178 + 50.0 * 6.257239818572998
Epoch 860, val loss: 0.8546704053878784
Epoch 870, training loss: 313.3006286621094 = 0.5278193950653076 + 50.0 * 6.25545597076416
Epoch 870, val loss: 0.8504204750061035
Epoch 880, training loss: 313.2581481933594 = 0.5167025327682495 + 50.0 * 6.254829406738281
Epoch 880, val loss: 0.8463646769523621
Epoch 890, training loss: 313.4967346191406 = 0.5056747794151306 + 50.0 * 6.259820938110352
Epoch 890, val loss: 0.8423512578010559
Epoch 900, training loss: 313.31365966796875 = 0.4946594834327698 + 50.0 * 6.256380081176758
Epoch 900, val loss: 0.837841808795929
Epoch 910, training loss: 313.1770324707031 = 0.4838853180408478 + 50.0 * 6.2538628578186035
Epoch 910, val loss: 0.8342001438140869
Epoch 920, training loss: 313.1574401855469 = 0.4733000099658966 + 50.0 * 6.253682613372803
Epoch 920, val loss: 0.8303412795066833
Epoch 930, training loss: 313.09783935546875 = 0.46287962794303894 + 50.0 * 6.25269889831543
Epoch 930, val loss: 0.826970100402832
Epoch 940, training loss: 313.052001953125 = 0.452625572681427 + 50.0 * 6.251987457275391
Epoch 940, val loss: 0.8235133290290833
Epoch 950, training loss: 313.03143310546875 = 0.44251060485839844 + 50.0 * 6.251778602600098
Epoch 950, val loss: 0.8202384114265442
Epoch 960, training loss: 312.9013977050781 = 0.43257129192352295 + 50.0 * 6.2493767738342285
Epoch 960, val loss: 0.8170201182365417
Epoch 970, training loss: 312.851806640625 = 0.4229224622249603 + 50.0 * 6.24857759475708
Epoch 970, val loss: 0.8142127394676208
Epoch 980, training loss: 312.84716796875 = 0.41346675157546997 + 50.0 * 6.248674392700195
Epoch 980, val loss: 0.8115345239639282
Epoch 990, training loss: 312.9776611328125 = 0.4041455090045929 + 50.0 * 6.251470565795898
Epoch 990, val loss: 0.808770477771759
Epoch 1000, training loss: 312.87890625 = 0.3948042690753937 + 50.0 * 6.2496819496154785
Epoch 1000, val loss: 0.8061646223068237
Epoch 1010, training loss: 312.8253173828125 = 0.3858853280544281 + 50.0 * 6.248788356781006
Epoch 1010, val loss: 0.8039309978485107
Epoch 1020, training loss: 312.66339111328125 = 0.3769809305667877 + 50.0 * 6.245728015899658
Epoch 1020, val loss: 0.80173659324646
Epoch 1030, training loss: 312.7168884277344 = 0.36841657757759094 + 50.0 * 6.246969699859619
Epoch 1030, val loss: 0.7998031377792358
Epoch 1040, training loss: 312.6112060546875 = 0.36003628373146057 + 50.0 * 6.245023250579834
Epoch 1040, val loss: 0.7978750467300415
Epoch 1050, training loss: 312.5304870605469 = 0.35185712575912476 + 50.0 * 6.243572235107422
Epoch 1050, val loss: 0.7962454557418823
Epoch 1060, training loss: 312.54608154296875 = 0.34395599365234375 + 50.0 * 6.24404239654541
Epoch 1060, val loss: 0.794838547706604
Epoch 1070, training loss: 312.50030517578125 = 0.33614447712898254 + 50.0 * 6.243282794952393
Epoch 1070, val loss: 0.793350338935852
Epoch 1080, training loss: 312.6195373535156 = 0.3285129964351654 + 50.0 * 6.245820045471191
Epoch 1080, val loss: 0.7919110059738159
Epoch 1090, training loss: 312.529052734375 = 0.32103076577186584 + 50.0 * 6.244160175323486
Epoch 1090, val loss: 0.7912218570709229
Epoch 1100, training loss: 312.372314453125 = 0.3136642873287201 + 50.0 * 6.241173267364502
Epoch 1100, val loss: 0.7901684045791626
Epoch 1110, training loss: 312.37640380859375 = 0.3066403567790985 + 50.0 * 6.241395473480225
Epoch 1110, val loss: 0.7895962595939636
Epoch 1120, training loss: 312.5305480957031 = 0.2997073829174042 + 50.0 * 6.244616508483887
Epoch 1120, val loss: 0.7888639569282532
Epoch 1130, training loss: 312.3076477050781 = 0.29282867908477783 + 50.0 * 6.240296840667725
Epoch 1130, val loss: 0.7882769703865051
Epoch 1140, training loss: 312.2068176269531 = 0.286271870136261 + 50.0 * 6.238410949707031
Epoch 1140, val loss: 0.787844717502594
Epoch 1150, training loss: 312.15643310546875 = 0.27992701530456543 + 50.0 * 6.237529754638672
Epoch 1150, val loss: 0.7879044413566589
Epoch 1160, training loss: 312.1582336425781 = 0.2737829089164734 + 50.0 * 6.237689018249512
Epoch 1160, val loss: 0.7879786491394043
Epoch 1170, training loss: 312.4378662109375 = 0.2677035331726074 + 50.0 * 6.243403434753418
Epoch 1170, val loss: 0.787858784198761
Epoch 1180, training loss: 312.0911560058594 = 0.2616032660007477 + 50.0 * 6.236591339111328
Epoch 1180, val loss: 0.7881072759628296
Epoch 1190, training loss: 312.07989501953125 = 0.25579923391342163 + 50.0 * 6.2364821434021
Epoch 1190, val loss: 0.7883102893829346
Epoch 1200, training loss: 311.9994201660156 = 0.2502422034740448 + 50.0 * 6.234983444213867
Epoch 1200, val loss: 0.7888866662979126
Epoch 1210, training loss: 312.1629943847656 = 0.2448500543832779 + 50.0 * 6.238363265991211
Epoch 1210, val loss: 0.7895100116729736
Epoch 1220, training loss: 312.0307922363281 = 0.2393590360879898 + 50.0 * 6.235828876495361
Epoch 1220, val loss: 0.7902867794036865
Epoch 1230, training loss: 312.17864990234375 = 0.23410312831401825 + 50.0 * 6.238891124725342
Epoch 1230, val loss: 0.7908164858818054
Epoch 1240, training loss: 311.9000244140625 = 0.22894366085529327 + 50.0 * 6.233421325683594
Epoch 1240, val loss: 0.7918022871017456
Epoch 1250, training loss: 311.8883361816406 = 0.22403982281684875 + 50.0 * 6.233285903930664
Epoch 1250, val loss: 0.792930543422699
Epoch 1260, training loss: 311.9244384765625 = 0.21927106380462646 + 50.0 * 6.234103679656982
Epoch 1260, val loss: 0.7941526174545288
Epoch 1270, training loss: 311.8970031738281 = 0.21453362703323364 + 50.0 * 6.233649253845215
Epoch 1270, val loss: 0.7955395579338074
Epoch 1280, training loss: 311.94775390625 = 0.20990201830863953 + 50.0 * 6.234756946563721
Epoch 1280, val loss: 0.7969936728477478
Epoch 1290, training loss: 311.8523254394531 = 0.20541596412658691 + 50.0 * 6.232938289642334
Epoch 1290, val loss: 0.7983489036560059
Epoch 1300, training loss: 311.8895263671875 = 0.2010161131620407 + 50.0 * 6.233770370483398
Epoch 1300, val loss: 0.7997809052467346
Epoch 1310, training loss: 311.84112548828125 = 0.19670411944389343 + 50.0 * 6.232888221740723
Epoch 1310, val loss: 0.8014999032020569
Epoch 1320, training loss: 311.77105712890625 = 0.19254854321479797 + 50.0 * 6.231570720672607
Epoch 1320, val loss: 0.8032025694847107
Epoch 1330, training loss: 311.7137756347656 = 0.18846845626831055 + 50.0 * 6.23050594329834
Epoch 1330, val loss: 0.8050984740257263
Epoch 1340, training loss: 311.6867370605469 = 0.18452239036560059 + 50.0 * 6.230044364929199
Epoch 1340, val loss: 0.8070319294929504
Epoch 1350, training loss: 311.7742004394531 = 0.18067394196987152 + 50.0 * 6.231870651245117
Epoch 1350, val loss: 0.8089584112167358
Epoch 1360, training loss: 311.5917053222656 = 0.176889568567276 + 50.0 * 6.228296756744385
Epoch 1360, val loss: 0.8111373782157898
Epoch 1370, training loss: 311.8023986816406 = 0.17321404814720154 + 50.0 * 6.232583522796631
Epoch 1370, val loss: 0.8132584095001221
Epoch 1380, training loss: 311.8704833984375 = 0.16957393288612366 + 50.0 * 6.234017848968506
Epoch 1380, val loss: 0.815502405166626
Epoch 1390, training loss: 311.630126953125 = 0.1659960299730301 + 50.0 * 6.229282855987549
Epoch 1390, val loss: 0.8174033761024475
Epoch 1400, training loss: 311.5130615234375 = 0.16257254779338837 + 50.0 * 6.2270097732543945
Epoch 1400, val loss: 0.8198986053466797
Epoch 1410, training loss: 311.45928955078125 = 0.15928828716278076 + 50.0 * 6.22599983215332
Epoch 1410, val loss: 0.8224586248397827
Epoch 1420, training loss: 311.5766296386719 = 0.15608280897140503 + 50.0 * 6.228410720825195
Epoch 1420, val loss: 0.8249914646148682
Epoch 1430, training loss: 311.522216796875 = 0.15281927585601807 + 50.0 * 6.227387428283691
Epoch 1430, val loss: 0.8270910382270813
Epoch 1440, training loss: 311.4782409667969 = 0.14962299168109894 + 50.0 * 6.226572513580322
Epoch 1440, val loss: 0.8298432230949402
Epoch 1450, training loss: 311.4261779785156 = 0.1466168314218521 + 50.0 * 6.225591659545898
Epoch 1450, val loss: 0.8324962258338928
Epoch 1460, training loss: 311.3704833984375 = 0.14370469748973846 + 50.0 * 6.2245354652404785
Epoch 1460, val loss: 0.8353290557861328
Epoch 1470, training loss: 311.6617126464844 = 0.14086231589317322 + 50.0 * 6.230416774749756
Epoch 1470, val loss: 0.8380898237228394
Epoch 1480, training loss: 311.53887939453125 = 0.13801003992557526 + 50.0 * 6.228017807006836
Epoch 1480, val loss: 0.8409778475761414
Epoch 1490, training loss: 311.3666687011719 = 0.1351865530014038 + 50.0 * 6.2246294021606445
Epoch 1490, val loss: 0.8435162305831909
Epoch 1500, training loss: 311.36859130859375 = 0.13250814378261566 + 50.0 * 6.224721908569336
Epoch 1500, val loss: 0.846603274345398
Epoch 1510, training loss: 311.3780212402344 = 0.12988102436065674 + 50.0 * 6.2249627113342285
Epoch 1510, val loss: 0.8496827483177185
Epoch 1520, training loss: 311.3787536621094 = 0.12732002139091492 + 50.0 * 6.2250285148620605
Epoch 1520, val loss: 0.852733314037323
Epoch 1530, training loss: 311.4306335449219 = 0.12480087578296661 + 50.0 * 6.22611665725708
Epoch 1530, val loss: 0.8554438948631287
Epoch 1540, training loss: 311.22479248046875 = 0.12233443558216095 + 50.0 * 6.222048759460449
Epoch 1540, val loss: 0.8584829568862915
Epoch 1550, training loss: 311.209228515625 = 0.11997880041599274 + 50.0 * 6.221785545349121
Epoch 1550, val loss: 0.8616840243339539
Epoch 1560, training loss: 311.2856750488281 = 0.11766836792230606 + 50.0 * 6.223360061645508
Epoch 1560, val loss: 0.8648300766944885
Epoch 1570, training loss: 311.29925537109375 = 0.11535851657390594 + 50.0 * 6.223678112030029
Epoch 1570, val loss: 0.8681190609931946
Epoch 1580, training loss: 311.1587219238281 = 0.11311689764261246 + 50.0 * 6.220911979675293
Epoch 1580, val loss: 0.8709274530410767
Epoch 1590, training loss: 311.181884765625 = 0.110958993434906 + 50.0 * 6.221418380737305
Epoch 1590, val loss: 0.874291718006134
Epoch 1600, training loss: 311.3283996582031 = 0.10883591324090958 + 50.0 * 6.224391460418701
Epoch 1600, val loss: 0.8774617314338684
Epoch 1610, training loss: 311.1712951660156 = 0.10671424120664597 + 50.0 * 6.221291542053223
Epoch 1610, val loss: 0.8808565139770508
Epoch 1620, training loss: 311.1079406738281 = 0.10471553355455399 + 50.0 * 6.220064640045166
Epoch 1620, val loss: 0.884058952331543
Epoch 1630, training loss: 311.104736328125 = 0.10277118533849716 + 50.0 * 6.220039367675781
Epoch 1630, val loss: 0.8875604867935181
Epoch 1640, training loss: 311.39544677734375 = 0.10087033361196518 + 50.0 * 6.225891590118408
Epoch 1640, val loss: 0.8908514976501465
Epoch 1650, training loss: 311.1335144042969 = 0.09890013933181763 + 50.0 * 6.220692157745361
Epoch 1650, val loss: 0.8940237760543823
Epoch 1660, training loss: 311.0699462890625 = 0.09706559032201767 + 50.0 * 6.219457626342773
Epoch 1660, val loss: 0.8973199129104614
Epoch 1670, training loss: 311.32861328125 = 0.09525009244680405 + 50.0 * 6.224667072296143
Epoch 1670, val loss: 0.9008316397666931
Epoch 1680, training loss: 311.0947265625 = 0.09346472471952438 + 50.0 * 6.220025062561035
Epoch 1680, val loss: 0.9040996432304382
Epoch 1690, training loss: 310.9837341308594 = 0.09174489229917526 + 50.0 * 6.21783971786499
Epoch 1690, val loss: 0.9075309634208679
Epoch 1700, training loss: 310.96051025390625 = 0.09010554105043411 + 50.0 * 6.217407703399658
Epoch 1700, val loss: 0.910927414894104
Epoch 1710, training loss: 311.2362976074219 = 0.08850301802158356 + 50.0 * 6.22295618057251
Epoch 1710, val loss: 0.914055585861206
Epoch 1720, training loss: 311.0145568847656 = 0.08683503419160843 + 50.0 * 6.218554973602295
Epoch 1720, val loss: 0.9179015755653381
Epoch 1730, training loss: 311.1306457519531 = 0.08526207506656647 + 50.0 * 6.220907688140869
Epoch 1730, val loss: 0.9210759401321411
Epoch 1740, training loss: 310.9383239746094 = 0.08370852470397949 + 50.0 * 6.217092514038086
Epoch 1740, val loss: 0.9246940612792969
Epoch 1750, training loss: 310.9151306152344 = 0.08223956823348999 + 50.0 * 6.216657638549805
Epoch 1750, val loss: 0.9280813336372375
Epoch 1760, training loss: 311.00787353515625 = 0.08079475164413452 + 50.0 * 6.218542098999023
Epoch 1760, val loss: 0.9316702485084534
Epoch 1770, training loss: 310.95782470703125 = 0.07933393120765686 + 50.0 * 6.217569828033447
Epoch 1770, val loss: 0.9349265098571777
Epoch 1780, training loss: 310.89349365234375 = 0.07790037989616394 + 50.0 * 6.216311454772949
Epoch 1780, val loss: 0.9386010766029358
Epoch 1790, training loss: 310.9029541015625 = 0.07654359191656113 + 50.0 * 6.216528415679932
Epoch 1790, val loss: 0.9421681761741638
Epoch 1800, training loss: 310.9052734375 = 0.0752071961760521 + 50.0 * 6.216601848602295
Epoch 1800, val loss: 0.9455826282501221
Epoch 1810, training loss: 310.8227844238281 = 0.07390506565570831 + 50.0 * 6.214977264404297
Epoch 1810, val loss: 0.9491857290267944
Epoch 1820, training loss: 310.94659423828125 = 0.07263880223035812 + 50.0 * 6.217479228973389
Epoch 1820, val loss: 0.9527211785316467
Epoch 1830, training loss: 310.7973327636719 = 0.0713663324713707 + 50.0 * 6.214519023895264
Epoch 1830, val loss: 0.956260085105896
Epoch 1840, training loss: 310.8758544921875 = 0.07014009356498718 + 50.0 * 6.216114521026611
Epoch 1840, val loss: 0.9597164392471313
Epoch 1850, training loss: 310.8325500488281 = 0.06892750412225723 + 50.0 * 6.215271949768066
Epoch 1850, val loss: 0.9632633924484253
Epoch 1860, training loss: 310.865478515625 = 0.06775422394275665 + 50.0 * 6.215954303741455
Epoch 1860, val loss: 0.9666544795036316
Epoch 1870, training loss: 310.8612365722656 = 0.06660496443510056 + 50.0 * 6.215892314910889
Epoch 1870, val loss: 0.9700949192047119
Epoch 1880, training loss: 310.77490234375 = 0.06546512246131897 + 50.0 * 6.214188575744629
Epoch 1880, val loss: 0.9736628532409668
Epoch 1890, training loss: 310.8023986816406 = 0.06437630206346512 + 50.0 * 6.214760780334473
Epoch 1890, val loss: 0.9772030115127563
Epoch 1900, training loss: 310.8201599121094 = 0.06331148743629456 + 50.0 * 6.215136528015137
Epoch 1900, val loss: 0.9808566570281982
Epoch 1910, training loss: 310.720703125 = 0.062241051346063614 + 50.0 * 6.213169097900391
Epoch 1910, val loss: 0.9843902587890625
Epoch 1920, training loss: 310.867431640625 = 0.06122388690710068 + 50.0 * 6.216124057769775
Epoch 1920, val loss: 0.9880495667457581
Epoch 1930, training loss: 310.7364807128906 = 0.06020386144518852 + 50.0 * 6.213525295257568
Epoch 1930, val loss: 0.9911301136016846
Epoch 1940, training loss: 310.8237609863281 = 0.05920744314789772 + 50.0 * 6.2152910232543945
Epoch 1940, val loss: 0.9945662617683411
Epoch 1950, training loss: 310.8831481933594 = 0.05822131410241127 + 50.0 * 6.216498374938965
Epoch 1950, val loss: 0.9982638955116272
Epoch 1960, training loss: 310.6787109375 = 0.05725279077887535 + 50.0 * 6.212429046630859
Epoch 1960, val loss: 1.0015637874603271
Epoch 1970, training loss: 310.6444396972656 = 0.05634014680981636 + 50.0 * 6.211761951446533
Epoch 1970, val loss: 1.0052831172943115
Epoch 1980, training loss: 310.62457275390625 = 0.05544453114271164 + 50.0 * 6.2113823890686035
Epoch 1980, val loss: 1.0087121725082397
Epoch 1990, training loss: 310.9671630859375 = 0.05457654222846031 + 50.0 * 6.218252182006836
Epoch 1990, val loss: 1.012074589729309
Epoch 2000, training loss: 310.7247619628906 = 0.05367911234498024 + 50.0 * 6.21342134475708
Epoch 2000, val loss: 1.0156587362289429
Epoch 2010, training loss: 310.6610107421875 = 0.052812010049819946 + 50.0 * 6.21216344833374
Epoch 2010, val loss: 1.0189682245254517
Epoch 2020, training loss: 310.6805114746094 = 0.05198720470070839 + 50.0 * 6.2125701904296875
Epoch 2020, val loss: 1.0227056741714478
Epoch 2030, training loss: 310.8871154785156 = 0.05116722360253334 + 50.0 * 6.216718673706055
Epoch 2030, val loss: 1.0257446765899658
Epoch 2040, training loss: 310.6256408691406 = 0.05033140257000923 + 50.0 * 6.211506366729736
Epoch 2040, val loss: 1.0292329788208008
Epoch 2050, training loss: 310.55462646484375 = 0.04954487457871437 + 50.0 * 6.210102081298828
Epoch 2050, val loss: 1.0327659845352173
Epoch 2060, training loss: 310.538330078125 = 0.04879891872406006 + 50.0 * 6.2097907066345215
Epoch 2060, val loss: 1.0362365245819092
Epoch 2070, training loss: 310.6769714355469 = 0.04806391894817352 + 50.0 * 6.212578296661377
Epoch 2070, val loss: 1.0396854877471924
Epoch 2080, training loss: 310.63360595703125 = 0.047332968562841415 + 50.0 * 6.21172571182251
Epoch 2080, val loss: 1.0429247617721558
Epoch 2090, training loss: 310.6147155761719 = 0.046583082526922226 + 50.0 * 6.211362838745117
Epoch 2090, val loss: 1.0463964939117432
Epoch 2100, training loss: 310.6953430175781 = 0.04587411880493164 + 50.0 * 6.212989807128906
Epoch 2100, val loss: 1.0498266220092773
Epoch 2110, training loss: 310.51544189453125 = 0.045155227184295654 + 50.0 * 6.209405422210693
Epoch 2110, val loss: 1.0529965162277222
Epoch 2120, training loss: 310.522705078125 = 0.04449828714132309 + 50.0 * 6.209564208984375
Epoch 2120, val loss: 1.056357979774475
Epoch 2130, training loss: 310.64019775390625 = 0.04384394735097885 + 50.0 * 6.2119269371032715
Epoch 2130, val loss: 1.059758186340332
Epoch 2140, training loss: 310.5113220214844 = 0.04317107051610947 + 50.0 * 6.209362983703613
Epoch 2140, val loss: 1.0632404088974
Epoch 2150, training loss: 310.51116943359375 = 0.04251760616898537 + 50.0 * 6.2093729972839355
Epoch 2150, val loss: 1.0666629076004028
Epoch 2160, training loss: 310.5281982421875 = 0.04190714657306671 + 50.0 * 6.209725856781006
Epoch 2160, val loss: 1.069950819015503
Epoch 2170, training loss: 310.5963439941406 = 0.041291557252407074 + 50.0 * 6.211101055145264
Epoch 2170, val loss: 1.0732372999191284
Epoch 2180, training loss: 310.5456237792969 = 0.0406850203871727 + 50.0 * 6.210098743438721
Epoch 2180, val loss: 1.0768377780914307
Epoch 2190, training loss: 310.4980163574219 = 0.04008250683546066 + 50.0 * 6.209158897399902
Epoch 2190, val loss: 1.079780101776123
Epoch 2200, training loss: 310.5326232910156 = 0.03950018063187599 + 50.0 * 6.20986270904541
Epoch 2200, val loss: 1.0830621719360352
Epoch 2210, training loss: 310.427978515625 = 0.03891995921730995 + 50.0 * 6.2077813148498535
Epoch 2210, val loss: 1.0864448547363281
Epoch 2220, training loss: 310.43292236328125 = 0.0383652001619339 + 50.0 * 6.207891464233398
Epoch 2220, val loss: 1.089989423751831
Epoch 2230, training loss: 310.4236755371094 = 0.03783047944307327 + 50.0 * 6.207716464996338
Epoch 2230, val loss: 1.0931857824325562
Epoch 2240, training loss: 310.57025146484375 = 0.03729938343167305 + 50.0 * 6.210659027099609
Epoch 2240, val loss: 1.0963690280914307
Epoch 2250, training loss: 310.53387451171875 = 0.03675203397870064 + 50.0 * 6.209942817687988
Epoch 2250, val loss: 1.0995452404022217
Epoch 2260, training loss: 310.4217529296875 = 0.036223962903022766 + 50.0 * 6.2077107429504395
Epoch 2260, val loss: 1.1028844118118286
Epoch 2270, training loss: 310.3636169433594 = 0.035714443773031235 + 50.0 * 6.2065582275390625
Epoch 2270, val loss: 1.1060199737548828
Epoch 2280, training loss: 310.5135803222656 = 0.0352255217730999 + 50.0 * 6.209567070007324
Epoch 2280, val loss: 1.1092274188995361
Epoch 2290, training loss: 310.4933776855469 = 0.034734029322862625 + 50.0 * 6.209173202514648
Epoch 2290, val loss: 1.1123594045639038
Epoch 2300, training loss: 310.3761901855469 = 0.034239914268255234 + 50.0 * 6.206839084625244
Epoch 2300, val loss: 1.115417242050171
Epoch 2310, training loss: 310.3565673828125 = 0.03376823663711548 + 50.0 * 6.206455707550049
Epoch 2310, val loss: 1.1186643838882446
Epoch 2320, training loss: 310.47308349609375 = 0.03332412242889404 + 50.0 * 6.208795070648193
Epoch 2320, val loss: 1.1219420433044434
Epoch 2330, training loss: 310.3589782714844 = 0.03285535052418709 + 50.0 * 6.206521987915039
Epoch 2330, val loss: 1.1249829530715942
Epoch 2340, training loss: 310.51708984375 = 0.032410938292741776 + 50.0 * 6.209693431854248
Epoch 2340, val loss: 1.1285606622695923
Epoch 2350, training loss: 310.340087890625 = 0.03196173533797264 + 50.0 * 6.206162929534912
Epoch 2350, val loss: 1.130964994430542
Epoch 2360, training loss: 310.3033752441406 = 0.031528376042842865 + 50.0 * 6.205437183380127
Epoch 2360, val loss: 1.1343622207641602
Epoch 2370, training loss: 310.2781677246094 = 0.031120674684643745 + 50.0 * 6.2049407958984375
Epoch 2370, val loss: 1.1377601623535156
Epoch 2380, training loss: 310.3609924316406 = 0.030729293823242188 + 50.0 * 6.206605434417725
Epoch 2380, val loss: 1.1407630443572998
Epoch 2390, training loss: 310.52099609375 = 0.030311929062008858 + 50.0 * 6.209813594818115
Epoch 2390, val loss: 1.1436930894851685
Epoch 2400, training loss: 310.4415588378906 = 0.02988947555422783 + 50.0 * 6.20823335647583
Epoch 2400, val loss: 1.1462936401367188
Epoch 2410, training loss: 310.2755126953125 = 0.029485758394002914 + 50.0 * 6.204920768737793
Epoch 2410, val loss: 1.1496385335922241
Epoch 2420, training loss: 310.2597351074219 = 0.029109885916113853 + 50.0 * 6.2046122550964355
Epoch 2420, val loss: 1.1526494026184082
Epoch 2430, training loss: 310.2395324707031 = 0.02874349243938923 + 50.0 * 6.2042155265808105
Epoch 2430, val loss: 1.1558958292007446
Epoch 2440, training loss: 310.4358215332031 = 0.02838945761322975 + 50.0 * 6.208148956298828
Epoch 2440, val loss: 1.1588460206985474
Epoch 2450, training loss: 310.2396545410156 = 0.028009498491883278 + 50.0 * 6.204232692718506
Epoch 2450, val loss: 1.161907434463501
Epoch 2460, training loss: 310.385986328125 = 0.027649376541376114 + 50.0 * 6.20716667175293
Epoch 2460, val loss: 1.1648168563842773
Epoch 2470, training loss: 310.37054443359375 = 0.027295056730508804 + 50.0 * 6.206865310668945
Epoch 2470, val loss: 1.1677260398864746
Epoch 2480, training loss: 310.2300720214844 = 0.02693338133394718 + 50.0 * 6.204062461853027
Epoch 2480, val loss: 1.1704120635986328
Epoch 2490, training loss: 310.1893310546875 = 0.026601914316415787 + 50.0 * 6.203254699707031
Epoch 2490, val loss: 1.173981785774231
Epoch 2500, training loss: 310.27069091796875 = 0.026280157268047333 + 50.0 * 6.204888343811035
Epoch 2500, val loss: 1.1769713163375854
Epoch 2510, training loss: 310.3225402832031 = 0.025950094684958458 + 50.0 * 6.205932140350342
Epoch 2510, val loss: 1.179602026939392
Epoch 2520, training loss: 310.233154296875 = 0.02562832459807396 + 50.0 * 6.204150676727295
Epoch 2520, val loss: 1.182323694229126
Epoch 2530, training loss: 310.3231201171875 = 0.02531627006828785 + 50.0 * 6.205955982208252
Epoch 2530, val loss: 1.1853665113449097
Epoch 2540, training loss: 310.2411193847656 = 0.024996934458613396 + 50.0 * 6.204322338104248
Epoch 2540, val loss: 1.1882998943328857
Epoch 2550, training loss: 310.19451904296875 = 0.024684295058250427 + 50.0 * 6.203396797180176
Epoch 2550, val loss: 1.191102147102356
Epoch 2560, training loss: 310.1363830566406 = 0.024390673264861107 + 50.0 * 6.202239990234375
Epoch 2560, val loss: 1.1941627264022827
Epoch 2570, training loss: 310.23956298828125 = 0.024111585691571236 + 50.0 * 6.204308986663818
Epoch 2570, val loss: 1.1972590684890747
Epoch 2580, training loss: 310.2622985839844 = 0.023820826783776283 + 50.0 * 6.204769134521484
Epoch 2580, val loss: 1.1996638774871826
Epoch 2590, training loss: 310.1858215332031 = 0.023530246689915657 + 50.0 * 6.203246116638184
Epoch 2590, val loss: 1.2021490335464478
Epoch 2600, training loss: 310.30657958984375 = 0.023255251348018646 + 50.0 * 6.205666542053223
Epoch 2600, val loss: 1.2051931619644165
Epoch 2610, training loss: 310.15087890625 = 0.022970624268054962 + 50.0 * 6.2025580406188965
Epoch 2610, val loss: 1.207723617553711
Epoch 2620, training loss: 310.16510009765625 = 0.02269403263926506 + 50.0 * 6.202847957611084
Epoch 2620, val loss: 1.2107971906661987
Epoch 2630, training loss: 310.212646484375 = 0.022431377321481705 + 50.0 * 6.2038044929504395
Epoch 2630, val loss: 1.2135788202285767
Epoch 2640, training loss: 310.1339111328125 = 0.02217184565961361 + 50.0 * 6.202234268188477
Epoch 2640, val loss: 1.2165926694869995
Epoch 2650, training loss: 310.3492736816406 = 0.021925119683146477 + 50.0 * 6.206547260284424
Epoch 2650, val loss: 1.2192401885986328
Epoch 2660, training loss: 310.1532287597656 = 0.021657869219779968 + 50.0 * 6.20263147354126
Epoch 2660, val loss: 1.2216371297836304
Epoch 2670, training loss: 310.07843017578125 = 0.02141127735376358 + 50.0 * 6.2011399269104
Epoch 2670, val loss: 1.2244726419448853
Epoch 2680, training loss: 310.08929443359375 = 0.02117733471095562 + 50.0 * 6.201362133026123
Epoch 2680, val loss: 1.2273374795913696
Epoch 2690, training loss: 310.4353332519531 = 0.020950043573975563 + 50.0 * 6.208288192749023
Epoch 2690, val loss: 1.229609489440918
Epoch 2700, training loss: 310.2567443847656 = 0.02069263719022274 + 50.0 * 6.204720973968506
Epoch 2700, val loss: 1.232405424118042
Epoch 2710, training loss: 310.13189697265625 = 0.020452992990612984 + 50.0 * 6.202229022979736
Epoch 2710, val loss: 1.2350561618804932
Epoch 2720, training loss: 310.0820007324219 = 0.02022436633706093 + 50.0 * 6.201235294342041
Epoch 2720, val loss: 1.2379206418991089
Epoch 2730, training loss: 310.1606140136719 = 0.020005032420158386 + 50.0 * 6.2028117179870605
Epoch 2730, val loss: 1.2405133247375488
Epoch 2740, training loss: 310.10540771484375 = 0.019784865900874138 + 50.0 * 6.201712608337402
Epoch 2740, val loss: 1.243361234664917
Epoch 2750, training loss: 310.1965026855469 = 0.01957232505083084 + 50.0 * 6.20353889465332
Epoch 2750, val loss: 1.2456024885177612
Epoch 2760, training loss: 310.01904296875 = 0.019351471215486526 + 50.0 * 6.199994087219238
Epoch 2760, val loss: 1.2482922077178955
Epoch 2770, training loss: 310.0555725097656 = 0.019148552790284157 + 50.0 * 6.200728893280029
Epoch 2770, val loss: 1.2509318590164185
Epoch 2780, training loss: 310.26287841796875 = 0.018949225544929504 + 50.0 * 6.204878807067871
Epoch 2780, val loss: 1.2535524368286133
Epoch 2790, training loss: 310.1888427734375 = 0.018730059266090393 + 50.0 * 6.203402042388916
Epoch 2790, val loss: 1.256343126296997
Epoch 2800, training loss: 310.0640563964844 = 0.018524063751101494 + 50.0 * 6.200910568237305
Epoch 2800, val loss: 1.2583389282226562
Epoch 2810, training loss: 310.002197265625 = 0.018325354903936386 + 50.0 * 6.199677467346191
Epoch 2810, val loss: 1.261128306388855
Epoch 2820, training loss: 310.01873779296875 = 0.018139608204364777 + 50.0 * 6.20001220703125
Epoch 2820, val loss: 1.2636619806289673
Epoch 2830, training loss: 310.24127197265625 = 0.01795669086277485 + 50.0 * 6.204466342926025
Epoch 2830, val loss: 1.2664426565170288
Epoch 2840, training loss: 310.0953674316406 = 0.017754198983311653 + 50.0 * 6.201551914215088
Epoch 2840, val loss: 1.2688111066818237
Epoch 2850, training loss: 309.9501037597656 = 0.017563505098223686 + 50.0 * 6.198650360107422
Epoch 2850, val loss: 1.271033525466919
Epoch 2860, training loss: 309.9917297363281 = 0.017385512590408325 + 50.0 * 6.19948673248291
Epoch 2860, val loss: 1.273733377456665
Epoch 2870, training loss: 310.2991943359375 = 0.017211532220244408 + 50.0 * 6.205639362335205
Epoch 2870, val loss: 1.276308536529541
Epoch 2880, training loss: 310.08953857421875 = 0.017023496329784393 + 50.0 * 6.201450347900391
Epoch 2880, val loss: 1.2786657810211182
Epoch 2890, training loss: 310.0459289550781 = 0.016842760145664215 + 50.0 * 6.2005815505981445
Epoch 2890, val loss: 1.2808761596679688
Epoch 2900, training loss: 310.0140686035156 = 0.016674896702170372 + 50.0 * 6.199947834014893
Epoch 2900, val loss: 1.2832660675048828
Epoch 2910, training loss: 309.9770202636719 = 0.01650792546570301 + 50.0 * 6.199210166931152
Epoch 2910, val loss: 1.2855621576309204
Epoch 2920, training loss: 309.9476013183594 = 0.016343625262379646 + 50.0 * 6.198625087738037
Epoch 2920, val loss: 1.2880442142486572
Epoch 2930, training loss: 310.0883483886719 = 0.016184695065021515 + 50.0 * 6.201443672180176
Epoch 2930, val loss: 1.2903225421905518
Epoch 2940, training loss: 310.1032409667969 = 0.016020089387893677 + 50.0 * 6.201744556427002
Epoch 2940, val loss: 1.2931346893310547
Epoch 2950, training loss: 309.9593200683594 = 0.01585301011800766 + 50.0 * 6.198869705200195
Epoch 2950, val loss: 1.2951104640960693
Epoch 2960, training loss: 309.9041442871094 = 0.01569409854710102 + 50.0 * 6.1977691650390625
Epoch 2960, val loss: 1.297432541847229
Epoch 2970, training loss: 310.0091247558594 = 0.015545381233096123 + 50.0 * 6.199872016906738
Epoch 2970, val loss: 1.2998805046081543
Epoch 2980, training loss: 310.0455627441406 = 0.015388642437756062 + 50.0 * 6.200603485107422
Epoch 2980, val loss: 1.302298903465271
Epoch 2990, training loss: 310.0164794921875 = 0.015236112289130688 + 50.0 * 6.2000250816345215
Epoch 2990, val loss: 1.3048428297042847
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7814814814814816
0.8408012651555088
The final CL Acc:0.73457, 0.03531, The final GNN Acc:0.83992, 0.00090
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.3 der2:0.4 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.4, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11662])
remove edge: torch.Size([2, 9554])
updated graph: torch.Size([2, 10660])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7763977050781 = 1.9331926107406616 + 50.0 * 8.596863746643066
Epoch 0, val loss: 1.9367743730545044
Epoch 10, training loss: 431.74114990234375 = 1.924979329109192 + 50.0 * 8.596323013305664
Epoch 10, val loss: 1.9290552139282227
Epoch 20, training loss: 431.5469055175781 = 1.914412021636963 + 50.0 * 8.592650413513184
Epoch 20, val loss: 1.9187655448913574
Epoch 30, training loss: 430.24932861328125 = 1.9005959033966064 + 50.0 * 8.566974639892578
Epoch 30, val loss: 1.9051884412765503
Epoch 40, training loss: 422.6258544921875 = 1.8833266496658325 + 50.0 * 8.414850234985352
Epoch 40, val loss: 1.8884800672531128
Epoch 50, training loss: 399.1162109375 = 1.8638054132461548 + 50.0 * 7.9450483322143555
Epoch 50, val loss: 1.869289517402649
Epoch 60, training loss: 383.9623107910156 = 1.8470617532730103 + 50.0 * 7.642304420471191
Epoch 60, val loss: 1.853411078453064
Epoch 70, training loss: 370.8578796386719 = 1.8377078771591187 + 50.0 * 7.380403518676758
Epoch 70, val loss: 1.8444815874099731
Epoch 80, training loss: 358.0981750488281 = 1.8302664756774902 + 50.0 * 7.1253581047058105
Epoch 80, val loss: 1.8370311260223389
Epoch 90, training loss: 352.2236328125 = 1.8219696283340454 + 50.0 * 7.008033752441406
Epoch 90, val loss: 1.828903079032898
Epoch 100, training loss: 347.52630615234375 = 1.8125938177108765 + 50.0 * 6.914274215698242
Epoch 100, val loss: 1.8196446895599365
Epoch 110, training loss: 343.370361328125 = 1.8042652606964111 + 50.0 * 6.831322193145752
Epoch 110, val loss: 1.8111876249313354
Epoch 120, training loss: 340.1153564453125 = 1.7965501546859741 + 50.0 * 6.766376495361328
Epoch 120, val loss: 1.8031193017959595
Epoch 130, training loss: 337.5979309082031 = 1.7879523038864136 + 50.0 * 6.7161993980407715
Epoch 130, val loss: 1.7945832014083862
Epoch 140, training loss: 335.4476318359375 = 1.778817892074585 + 50.0 * 6.673376560211182
Epoch 140, val loss: 1.785720705986023
Epoch 150, training loss: 333.81201171875 = 1.76966392993927 + 50.0 * 6.640847206115723
Epoch 150, val loss: 1.7769702672958374
Epoch 160, training loss: 332.2776794433594 = 1.7596755027770996 + 50.0 * 6.610360145568848
Epoch 160, val loss: 1.7679047584533691
Epoch 170, training loss: 330.9613952636719 = 1.748719573020935 + 50.0 * 6.584253311157227
Epoch 170, val loss: 1.7582217454910278
Epoch 180, training loss: 329.8637390136719 = 1.7368930578231812 + 50.0 * 6.562536716461182
Epoch 180, val loss: 1.7478916645050049
Epoch 190, training loss: 328.9717102050781 = 1.7239410877227783 + 50.0 * 6.544955730438232
Epoch 190, val loss: 1.7367397546768188
Epoch 200, training loss: 328.096435546875 = 1.7099038362503052 + 50.0 * 6.527730941772461
Epoch 200, val loss: 1.7247625589370728
Epoch 210, training loss: 327.2958984375 = 1.6946954727172852 + 50.0 * 6.51202392578125
Epoch 210, val loss: 1.7118804454803467
Epoch 220, training loss: 326.6568603515625 = 1.6782251596450806 + 50.0 * 6.49957275390625
Epoch 220, val loss: 1.6980093717575073
Epoch 230, training loss: 326.1136474609375 = 1.660579800605774 + 50.0 * 6.48906135559082
Epoch 230, val loss: 1.6832636594772339
Epoch 240, training loss: 325.4391174316406 = 1.6417971849441528 + 50.0 * 6.475946426391602
Epoch 240, val loss: 1.6676231622695923
Epoch 250, training loss: 324.979248046875 = 1.6220636367797852 + 50.0 * 6.467143535614014
Epoch 250, val loss: 1.6512128114700317
Epoch 260, training loss: 324.51116943359375 = 1.6014642715454102 + 50.0 * 6.458194255828857
Epoch 260, val loss: 1.6341874599456787
Epoch 270, training loss: 323.96490478515625 = 1.5800838470458984 + 50.0 * 6.447696685791016
Epoch 270, val loss: 1.6166385412216187
Epoch 280, training loss: 323.7666015625 = 1.5580904483795166 + 50.0 * 6.444169998168945
Epoch 280, val loss: 1.5986988544464111
Epoch 290, training loss: 323.2326965332031 = 1.5356237888336182 + 50.0 * 6.43394136428833
Epoch 290, val loss: 1.5805953741073608
Epoch 300, training loss: 322.9320373535156 = 1.5129340887069702 + 50.0 * 6.42838191986084
Epoch 300, val loss: 1.5625413656234741
Epoch 310, training loss: 322.5121765136719 = 1.4901920557022095 + 50.0 * 6.420439720153809
Epoch 310, val loss: 1.5446499586105347
Epoch 320, training loss: 322.1498107910156 = 1.467387318611145 + 50.0 * 6.41364860534668
Epoch 320, val loss: 1.527030348777771
Epoch 330, training loss: 322.04144287109375 = 1.4446258544921875 + 50.0 * 6.4119367599487305
Epoch 330, val loss: 1.5097557306289673
Epoch 340, training loss: 321.7145080566406 = 1.4216402769088745 + 50.0 * 6.405857563018799
Epoch 340, val loss: 1.4925626516342163
Epoch 350, training loss: 321.3468017578125 = 1.3988854885101318 + 50.0 * 6.398958206176758
Epoch 350, val loss: 1.4757362604141235
Epoch 360, training loss: 321.0275573730469 = 1.376366376876831 + 50.0 * 6.393023490905762
Epoch 360, val loss: 1.4595181941986084
Epoch 370, training loss: 320.7730712890625 = 1.353826642036438 + 50.0 * 6.38838529586792
Epoch 370, val loss: 1.4437456130981445
Epoch 380, training loss: 320.4944152832031 = 1.3310552835464478 + 50.0 * 6.383266925811768
Epoch 380, val loss: 1.428151249885559
Epoch 390, training loss: 320.3585205078125 = 1.3082952499389648 + 50.0 * 6.381004333496094
Epoch 390, val loss: 1.412957787513733
Epoch 400, training loss: 320.1300048828125 = 1.285471796989441 + 50.0 * 6.376891136169434
Epoch 400, val loss: 1.3980958461761475
Epoch 410, training loss: 320.0367736816406 = 1.2624202966690063 + 50.0 * 6.375487327575684
Epoch 410, val loss: 1.383492350578308
Epoch 420, training loss: 319.71539306640625 = 1.2394061088562012 + 50.0 * 6.3695197105407715
Epoch 420, val loss: 1.3692747354507446
Epoch 430, training loss: 319.43878173828125 = 1.2164759635925293 + 50.0 * 6.364446640014648
Epoch 430, val loss: 1.3555479049682617
Epoch 440, training loss: 319.6461486816406 = 1.1935559511184692 + 50.0 * 6.369052410125732
Epoch 440, val loss: 1.3421778678894043
Epoch 450, training loss: 319.1209411621094 = 1.1705783605575562 + 50.0 * 6.359006881713867
Epoch 450, val loss: 1.3291836977005005
Epoch 460, training loss: 318.9179382324219 = 1.147830843925476 + 50.0 * 6.35540246963501
Epoch 460, val loss: 1.3167065382003784
Epoch 470, training loss: 319.2282409667969 = 1.125279188156128 + 50.0 * 6.362059116363525
Epoch 470, val loss: 1.3047640323638916
Epoch 480, training loss: 318.60162353515625 = 1.1029990911483765 + 50.0 * 6.349972724914551
Epoch 480, val loss: 1.2932919263839722
Epoch 490, training loss: 318.47412109375 = 1.0811426639556885 + 50.0 * 6.3478593826293945
Epoch 490, val loss: 1.2825490236282349
Epoch 500, training loss: 318.29779052734375 = 1.0597405433654785 + 50.0 * 6.344760894775391
Epoch 500, val loss: 1.2726117372512817
Epoch 510, training loss: 318.2109069824219 = 1.0386697053909302 + 50.0 * 6.34344482421875
Epoch 510, val loss: 1.2630174160003662
Epoch 520, training loss: 318.1739807128906 = 1.018057107925415 + 50.0 * 6.343118190765381
Epoch 520, val loss: 1.2542446851730347
Epoch 530, training loss: 317.8724060058594 = 0.9979891180992126 + 50.0 * 6.337488174438477
Epoch 530, val loss: 1.2460482120513916
Epoch 540, training loss: 317.81500244140625 = 0.9785248637199402 + 50.0 * 6.336729526519775
Epoch 540, val loss: 1.2384746074676514
Epoch 550, training loss: 317.6652526855469 = 0.959557056427002 + 50.0 * 6.334114074707031
Epoch 550, val loss: 1.2314273118972778
Epoch 560, training loss: 317.6038818359375 = 0.9410315752029419 + 50.0 * 6.333256721496582
Epoch 560, val loss: 1.2251118421554565
Epoch 570, training loss: 317.5646057128906 = 0.922994077205658 + 50.0 * 6.332831859588623
Epoch 570, val loss: 1.2191416025161743
Epoch 580, training loss: 317.3130798339844 = 0.9055987596511841 + 50.0 * 6.328149318695068
Epoch 580, val loss: 1.2141462564468384
Epoch 590, training loss: 317.1363525390625 = 0.8886905908584595 + 50.0 * 6.324953079223633
Epoch 590, val loss: 1.209700107574463
Epoch 600, training loss: 317.1505126953125 = 0.8723028302192688 + 50.0 * 6.325563907623291
Epoch 600, val loss: 1.2056092023849487
Epoch 610, training loss: 317.0970764160156 = 0.8562201261520386 + 50.0 * 6.324817180633545
Epoch 610, val loss: 1.202101230621338
Epoch 620, training loss: 316.93524169921875 = 0.8405820727348328 + 50.0 * 6.321893215179443
Epoch 620, val loss: 1.1989156007766724
Epoch 630, training loss: 316.7716369628906 = 0.8253859877586365 + 50.0 * 6.318925380706787
Epoch 630, val loss: 1.1961426734924316
Epoch 640, training loss: 316.7002868652344 = 0.8106655478477478 + 50.0 * 6.3177924156188965
Epoch 640, val loss: 1.1941311359405518
Epoch 650, training loss: 316.6372985839844 = 0.7962893843650818 + 50.0 * 6.31682014465332
Epoch 650, val loss: 1.1923911571502686
Epoch 660, training loss: 316.4559631347656 = 0.7822043895721436 + 50.0 * 6.313475131988525
Epoch 660, val loss: 1.191072940826416
Epoch 670, training loss: 316.4273986816406 = 0.7684842944145203 + 50.0 * 6.313178062438965
Epoch 670, val loss: 1.1901525259017944
Epoch 680, training loss: 316.39422607421875 = 0.7549975514411926 + 50.0 * 6.312784194946289
Epoch 680, val loss: 1.1893653869628906
Epoch 690, training loss: 316.2391052246094 = 0.7418010234832764 + 50.0 * 6.309946060180664
Epoch 690, val loss: 1.1890701055526733
Epoch 700, training loss: 316.1006774902344 = 0.7289157509803772 + 50.0 * 6.307435512542725
Epoch 700, val loss: 1.1892309188842773
Epoch 710, training loss: 316.3263244628906 = 0.7163729071617126 + 50.0 * 6.312199115753174
Epoch 710, val loss: 1.1894793510437012
Epoch 720, training loss: 316.19293212890625 = 0.703673779964447 + 50.0 * 6.309784889221191
Epoch 720, val loss: 1.1894768476486206
Epoch 730, training loss: 315.85052490234375 = 0.6913966536521912 + 50.0 * 6.303182125091553
Epoch 730, val loss: 1.1902621984481812
Epoch 740, training loss: 315.7720947265625 = 0.6793144345283508 + 50.0 * 6.301855564117432
Epoch 740, val loss: 1.1912713050842285
Epoch 750, training loss: 315.98345947265625 = 0.6673832535743713 + 50.0 * 6.306321620941162
Epoch 750, val loss: 1.192358374595642
Epoch 760, training loss: 315.7795104980469 = 0.6554725766181946 + 50.0 * 6.302480220794678
Epoch 760, val loss: 1.1931170225143433
Epoch 770, training loss: 315.6202392578125 = 0.6436269879341125 + 50.0 * 6.299532413482666
Epoch 770, val loss: 1.1943570375442505
Epoch 780, training loss: 315.5196533203125 = 0.631987988948822 + 50.0 * 6.29775333404541
Epoch 780, val loss: 1.1959999799728394
Epoch 790, training loss: 315.61883544921875 = 0.6204407811164856 + 50.0 * 6.2999677658081055
Epoch 790, val loss: 1.1976052522659302
Epoch 800, training loss: 315.6075439453125 = 0.6088481545448303 + 50.0 * 6.299973487854004
Epoch 800, val loss: 1.198661208152771
Epoch 810, training loss: 315.2706298828125 = 0.5972616076469421 + 50.0 * 6.2934675216674805
Epoch 810, val loss: 1.2002129554748535
Epoch 820, training loss: 315.21142578125 = 0.585851788520813 + 50.0 * 6.292511463165283
Epoch 820, val loss: 1.2021760940551758
Epoch 830, training loss: 315.10919189453125 = 0.5744814872741699 + 50.0 * 6.290693759918213
Epoch 830, val loss: 1.2040067911148071
Epoch 840, training loss: 315.284912109375 = 0.5631290674209595 + 50.0 * 6.294435501098633
Epoch 840, val loss: 1.2058695554733276
Epoch 850, training loss: 315.5677490234375 = 0.5515328049659729 + 50.0 * 6.300323963165283
Epoch 850, val loss: 1.2068383693695068
Epoch 860, training loss: 315.106201171875 = 0.5400039553642273 + 50.0 * 6.291324138641357
Epoch 860, val loss: 1.2085468769073486
Epoch 870, training loss: 314.9375 = 0.5285252928733826 + 50.0 * 6.288179397583008
Epoch 870, val loss: 1.2102875709533691
Epoch 880, training loss: 314.8124694824219 = 0.5171362161636353 + 50.0 * 6.2859063148498535
Epoch 880, val loss: 1.2122007608413696
Epoch 890, training loss: 314.75299072265625 = 0.5056416392326355 + 50.0 * 6.284946918487549
Epoch 890, val loss: 1.2139337062835693
Epoch 900, training loss: 314.9045715332031 = 0.49414828419685364 + 50.0 * 6.288208484649658
Epoch 900, val loss: 1.215695858001709
Epoch 910, training loss: 314.7886657714844 = 0.4824002683162689 + 50.0 * 6.286125659942627
Epoch 910, val loss: 1.2168042659759521
Epoch 920, training loss: 315.061279296875 = 0.4707512855529785 + 50.0 * 6.291810512542725
Epoch 920, val loss: 1.2182414531707764
Epoch 930, training loss: 314.6448669433594 = 0.45903006196022034 + 50.0 * 6.283717155456543
Epoch 930, val loss: 1.219910740852356
Epoch 940, training loss: 314.5113220214844 = 0.4473840594291687 + 50.0 * 6.281278610229492
Epoch 940, val loss: 1.2215183973312378
Epoch 950, training loss: 314.48748779296875 = 0.43584704399108887 + 50.0 * 6.281032562255859
Epoch 950, val loss: 1.2232701778411865
Epoch 960, training loss: 314.4720764160156 = 0.424241304397583 + 50.0 * 6.280956268310547
Epoch 960, val loss: 1.224526286125183
Epoch 970, training loss: 314.38372802734375 = 0.4126378893852234 + 50.0 * 6.279422283172607
Epoch 970, val loss: 1.226162314414978
Epoch 980, training loss: 314.2970275878906 = 0.40121275186538696 + 50.0 * 6.277916431427002
Epoch 980, val loss: 1.2277973890304565
Epoch 990, training loss: 314.3105163574219 = 0.3898926079273224 + 50.0 * 6.278412342071533
Epoch 990, val loss: 1.22957181930542
Epoch 1000, training loss: 314.1885070800781 = 0.378607839345932 + 50.0 * 6.276198387145996
Epoch 1000, val loss: 1.231319785118103
Epoch 1010, training loss: 314.1275329589844 = 0.36741194128990173 + 50.0 * 6.275202751159668
Epoch 1010, val loss: 1.2331069707870483
Epoch 1020, training loss: 314.0732727050781 = 0.35648030042648315 + 50.0 * 6.274335861206055
Epoch 1020, val loss: 1.2354378700256348
Epoch 1030, training loss: 314.0502014160156 = 0.3457281291484833 + 50.0 * 6.274089336395264
Epoch 1030, val loss: 1.2374308109283447
Epoch 1040, training loss: 314.3114318847656 = 0.3351092040538788 + 50.0 * 6.279526233673096
Epoch 1040, val loss: 1.2396272420883179
Epoch 1050, training loss: 314.03411865234375 = 0.32457542419433594 + 50.0 * 6.274190425872803
Epoch 1050, val loss: 1.2414494752883911
Epoch 1060, training loss: 314.0137634277344 = 0.3143562078475952 + 50.0 * 6.273988246917725
Epoch 1060, val loss: 1.2440857887268066
Epoch 1070, training loss: 313.8860778808594 = 0.3043830990791321 + 50.0 * 6.271634101867676
Epoch 1070, val loss: 1.2471542358398438
Epoch 1080, training loss: 313.79296875 = 0.2947132885456085 + 50.0 * 6.269965171813965
Epoch 1080, val loss: 1.2500090599060059
Epoch 1090, training loss: 313.8660888671875 = 0.2853075861930847 + 50.0 * 6.271615505218506
Epoch 1090, val loss: 1.2530677318572998
Epoch 1100, training loss: 313.7896728515625 = 0.27610892057418823 + 50.0 * 6.270271301269531
Epoch 1100, val loss: 1.2561060190200806
Epoch 1110, training loss: 313.8775634765625 = 0.26721546053886414 + 50.0 * 6.272206783294678
Epoch 1110, val loss: 1.2595854997634888
Epoch 1120, training loss: 313.7721252441406 = 0.25854840874671936 + 50.0 * 6.2702717781066895
Epoch 1120, val loss: 1.2629988193511963
Epoch 1130, training loss: 313.58355712890625 = 0.2501521408557892 + 50.0 * 6.26666784286499
Epoch 1130, val loss: 1.2670663595199585
Epoch 1140, training loss: 313.548828125 = 0.2421133816242218 + 50.0 * 6.266134262084961
Epoch 1140, val loss: 1.2712455987930298
Epoch 1150, training loss: 313.6507263183594 = 0.23437824845314026 + 50.0 * 6.268327236175537
Epoch 1150, val loss: 1.275565505027771
Epoch 1160, training loss: 313.6561584472656 = 0.22685343027114868 + 50.0 * 6.268585681915283
Epoch 1160, val loss: 1.2797366380691528
Epoch 1170, training loss: 313.6025695800781 = 0.2195720225572586 + 50.0 * 6.267660140991211
Epoch 1170, val loss: 1.2845550775527954
Epoch 1180, training loss: 313.4317626953125 = 0.21254892647266388 + 50.0 * 6.2643842697143555
Epoch 1180, val loss: 1.2895243167877197
Epoch 1190, training loss: 313.3499755859375 = 0.20585215091705322 + 50.0 * 6.262882709503174
Epoch 1190, val loss: 1.294795036315918
Epoch 1200, training loss: 313.47442626953125 = 0.19941866397857666 + 50.0 * 6.265499591827393
Epoch 1200, val loss: 1.3000586032867432
Epoch 1210, training loss: 313.3348388671875 = 0.19313786923885345 + 50.0 * 6.262833595275879
Epoch 1210, val loss: 1.305312156677246
Epoch 1220, training loss: 313.4971008300781 = 0.18714328110218048 + 50.0 * 6.266199588775635
Epoch 1220, val loss: 1.3111190795898438
Epoch 1230, training loss: 313.3569030761719 = 0.18131598830223083 + 50.0 * 6.263511657714844
Epoch 1230, val loss: 1.3165674209594727
Epoch 1240, training loss: 313.24395751953125 = 0.17574220895767212 + 50.0 * 6.261363983154297
Epoch 1240, val loss: 1.322737693786621
Epoch 1250, training loss: 313.170166015625 = 0.17040905356407166 + 50.0 * 6.259994983673096
Epoch 1250, val loss: 1.3289545774459839
Epoch 1260, training loss: 313.25616455078125 = 0.165283203125 + 50.0 * 6.261817932128906
Epoch 1260, val loss: 1.3352550268173218
Epoch 1270, training loss: 313.1942443847656 = 0.16031379997730255 + 50.0 * 6.260678768157959
Epoch 1270, val loss: 1.341375470161438
Epoch 1280, training loss: 313.1544494628906 = 0.1555553376674652 + 50.0 * 6.259978294372559
Epoch 1280, val loss: 1.3477369546890259
Epoch 1290, training loss: 313.1434326171875 = 0.15095481276512146 + 50.0 * 6.259850025177002
Epoch 1290, val loss: 1.3545116186141968
Epoch 1300, training loss: 313.11981201171875 = 0.14652879536151886 + 50.0 * 6.25946569442749
Epoch 1300, val loss: 1.3608542680740356
Epoch 1310, training loss: 313.06005859375 = 0.14223334193229675 + 50.0 * 6.258356094360352
Epoch 1310, val loss: 1.367135763168335
Epoch 1320, training loss: 312.9792785644531 = 0.13813064992427826 + 50.0 * 6.2568230628967285
Epoch 1320, val loss: 1.374018907546997
Epoch 1330, training loss: 312.96087646484375 = 0.13419511914253235 + 50.0 * 6.256534099578857
Epoch 1330, val loss: 1.3808526992797852
Epoch 1340, training loss: 313.3140869140625 = 0.13040491938591003 + 50.0 * 6.263673782348633
Epoch 1340, val loss: 1.387597680091858
Epoch 1350, training loss: 313.1738586425781 = 0.12666644155979156 + 50.0 * 6.26094388961792
Epoch 1350, val loss: 1.3938045501708984
Epoch 1360, training loss: 312.8825378417969 = 0.1231129989027977 + 50.0 * 6.255188465118408
Epoch 1360, val loss: 1.4008816480636597
Epoch 1370, training loss: 312.8774719238281 = 0.11970565468072891 + 50.0 * 6.255155086517334
Epoch 1370, val loss: 1.4080857038497925
Epoch 1380, training loss: 313.1202392578125 = 0.11642274260520935 + 50.0 * 6.260076522827148
Epoch 1380, val loss: 1.4149072170257568
Epoch 1390, training loss: 312.93914794921875 = 0.11318698525428772 + 50.0 * 6.256519317626953
Epoch 1390, val loss: 1.4213472604751587
Epoch 1400, training loss: 312.75909423828125 = 0.11008275300264359 + 50.0 * 6.2529802322387695
Epoch 1400, val loss: 1.4286885261535645
Epoch 1410, training loss: 312.733154296875 = 0.10712850093841553 + 50.0 * 6.252520561218262
Epoch 1410, val loss: 1.4357882738113403
Epoch 1420, training loss: 312.7580261230469 = 0.1042807325720787 + 50.0 * 6.253074645996094
Epoch 1420, val loss: 1.4426681995391846
Epoch 1430, training loss: 312.9563293457031 = 0.1015041247010231 + 50.0 * 6.257096767425537
Epoch 1430, val loss: 1.4495607614517212
Epoch 1440, training loss: 312.80108642578125 = 0.09882264584302902 + 50.0 * 6.254045486450195
Epoch 1440, val loss: 1.4568432569503784
Epoch 1450, training loss: 312.8121643066406 = 0.09622236341238022 + 50.0 * 6.254318714141846
Epoch 1450, val loss: 1.4637374877929688
Epoch 1460, training loss: 312.89263916015625 = 0.09369207918643951 + 50.0 * 6.255979061126709
Epoch 1460, val loss: 1.470542073249817
Epoch 1470, training loss: 312.68853759765625 = 0.09125690907239914 + 50.0 * 6.251945972442627
Epoch 1470, val loss: 1.4772213697433472
Epoch 1480, training loss: 312.6222839355469 = 0.08892015367746353 + 50.0 * 6.250667095184326
Epoch 1480, val loss: 1.4846121072769165
Epoch 1490, training loss: 312.6228332519531 = 0.08666937053203583 + 50.0 * 6.250723361968994
Epoch 1490, val loss: 1.4914369583129883
Epoch 1500, training loss: 312.7996826171875 = 0.08446677029132843 + 50.0 * 6.2543044090271
Epoch 1500, val loss: 1.4981958866119385
Epoch 1510, training loss: 312.59686279296875 = 0.08229786157608032 + 50.0 * 6.250291347503662
Epoch 1510, val loss: 1.5053790807724
Epoch 1520, training loss: 312.50640869140625 = 0.08022547513246536 + 50.0 * 6.248523712158203
Epoch 1520, val loss: 1.5122179985046387
Epoch 1530, training loss: 312.611328125 = 0.07823862880468369 + 50.0 * 6.250661849975586
Epoch 1530, val loss: 1.5190207958221436
Epoch 1540, training loss: 312.4809875488281 = 0.07629480212926865 + 50.0 * 6.248093605041504
Epoch 1540, val loss: 1.5260382890701294
Epoch 1550, training loss: 312.57696533203125 = 0.07443848997354507 + 50.0 * 6.2500505447387695
Epoch 1550, val loss: 1.5328463315963745
Epoch 1560, training loss: 312.5740966796875 = 0.07260598987340927 + 50.0 * 6.250030040740967
Epoch 1560, val loss: 1.5391699075698853
Epoch 1570, training loss: 312.5127868652344 = 0.07084298133850098 + 50.0 * 6.248838901519775
Epoch 1570, val loss: 1.5462744235992432
Epoch 1580, training loss: 312.5492858886719 = 0.06912458688020706 + 50.0 * 6.249603271484375
Epoch 1580, val loss: 1.552803635597229
Epoch 1590, training loss: 312.46710205078125 = 0.06744866818189621 + 50.0 * 6.247993469238281
Epoch 1590, val loss: 1.5597196817398071
Epoch 1600, training loss: 312.41461181640625 = 0.06584244221448898 + 50.0 * 6.246974945068359
Epoch 1600, val loss: 1.5663422346115112
Epoch 1610, training loss: 312.4104919433594 = 0.06429007649421692 + 50.0 * 6.246923923492432
Epoch 1610, val loss: 1.573030710220337
Epoch 1620, training loss: 312.40020751953125 = 0.06277325004339218 + 50.0 * 6.246748447418213
Epoch 1620, val loss: 1.5796236991882324
Epoch 1630, training loss: 312.62677001953125 = 0.06130406633019447 + 50.0 * 6.251308917999268
Epoch 1630, val loss: 1.5863770246505737
Epoch 1640, training loss: 312.51995849609375 = 0.05985104292631149 + 50.0 * 6.249202251434326
Epoch 1640, val loss: 1.5925986766815186
Epoch 1650, training loss: 312.3188171386719 = 0.058445874601602554 + 50.0 * 6.245207786560059
Epoch 1650, val loss: 1.5996737480163574
Epoch 1660, training loss: 312.2316589355469 = 0.0571000762283802 + 50.0 * 6.243491172790527
Epoch 1660, val loss: 1.606210708618164
Epoch 1670, training loss: 312.22607421875 = 0.05581187829375267 + 50.0 * 6.243404865264893
Epoch 1670, val loss: 1.6129933595657349
Epoch 1680, training loss: 312.71771240234375 = 0.05455777421593666 + 50.0 * 6.253262996673584
Epoch 1680, val loss: 1.619493007659912
Epoch 1690, training loss: 312.41912841796875 = 0.05330129712820053 + 50.0 * 6.247316360473633
Epoch 1690, val loss: 1.6255158185958862
Epoch 1700, training loss: 312.2442321777344 = 0.05207950994372368 + 50.0 * 6.243842601776123
Epoch 1700, val loss: 1.6320745944976807
Epoch 1710, training loss: 312.4578857421875 = 0.05092142894864082 + 50.0 * 6.248138904571533
Epoch 1710, val loss: 1.6385854482650757
Epoch 1720, training loss: 312.1707458496094 = 0.049775369465351105 + 50.0 * 6.242419719696045
Epoch 1720, val loss: 1.6449869871139526
Epoch 1730, training loss: 312.13238525390625 = 0.048673246055841446 + 50.0 * 6.241673946380615
Epoch 1730, val loss: 1.651368260383606
Epoch 1740, training loss: 312.154296875 = 0.04761762171983719 + 50.0 * 6.242134094238281
Epoch 1740, val loss: 1.6581493616104126
Epoch 1750, training loss: 312.3409729003906 = 0.04659872502088547 + 50.0 * 6.245887279510498
Epoch 1750, val loss: 1.6646231412887573
Epoch 1760, training loss: 312.28131103515625 = 0.04555928707122803 + 50.0 * 6.244715213775635
Epoch 1760, val loss: 1.6699613332748413
Epoch 1770, training loss: 312.1263122558594 = 0.0445624478161335 + 50.0 * 6.241634845733643
Epoch 1770, val loss: 1.6763637065887451
Epoch 1780, training loss: 312.0842590332031 = 0.043615859001874924 + 50.0 * 6.240813255310059
Epoch 1780, val loss: 1.6826701164245605
Epoch 1790, training loss: 312.3204040527344 = 0.04270530864596367 + 50.0 * 6.245553970336914
Epoch 1790, val loss: 1.6890487670898438
Epoch 1800, training loss: 312.1394348144531 = 0.041781604290008545 + 50.0 * 6.241952896118164
Epoch 1800, val loss: 1.6946113109588623
Epoch 1810, training loss: 312.0310974121094 = 0.040898460894823074 + 50.0 * 6.239804267883301
Epoch 1810, val loss: 1.7010071277618408
Epoch 1820, training loss: 312.02325439453125 = 0.040055546909570694 + 50.0 * 6.239663600921631
Epoch 1820, val loss: 1.7067291736602783
Epoch 1830, training loss: 312.28289794921875 = 0.03923768177628517 + 50.0 * 6.244873046875
Epoch 1830, val loss: 1.7126092910766602
Epoch 1840, training loss: 312.0487976074219 = 0.03841472044587135 + 50.0 * 6.240207672119141
Epoch 1840, val loss: 1.718890905380249
Epoch 1850, training loss: 311.9667663574219 = 0.03763097897171974 + 50.0 * 6.238582611083984
Epoch 1850, val loss: 1.7248603105545044
Epoch 1860, training loss: 312.0526123046875 = 0.0368783101439476 + 50.0 * 6.240314960479736
Epoch 1860, val loss: 1.731238603591919
Epoch 1870, training loss: 312.0580749511719 = 0.036134351044893265 + 50.0 * 6.240438938140869
Epoch 1870, val loss: 1.736416220664978
Epoch 1880, training loss: 312.0399475097656 = 0.03540164604783058 + 50.0 * 6.240090847015381
Epoch 1880, val loss: 1.7420543432235718
Epoch 1890, training loss: 311.89373779296875 = 0.034704044461250305 + 50.0 * 6.237180709838867
Epoch 1890, val loss: 1.7482781410217285
Epoch 1900, training loss: 311.896240234375 = 0.03403021767735481 + 50.0 * 6.237244129180908
Epoch 1900, val loss: 1.7539304494857788
Epoch 1910, training loss: 312.23284912109375 = 0.03339435160160065 + 50.0 * 6.243988990783691
Epoch 1910, val loss: 1.7600809335708618
Epoch 1920, training loss: 312.2362976074219 = 0.03271682187914848 + 50.0 * 6.2440714836120605
Epoch 1920, val loss: 1.764847993850708
Epoch 1930, training loss: 311.9583435058594 = 0.03206636384129524 + 50.0 * 6.238525390625
Epoch 1930, val loss: 1.7704325914382935
Epoch 1940, training loss: 311.83612060546875 = 0.03145759925246239 + 50.0 * 6.236093044281006
Epoch 1940, val loss: 1.7763304710388184
Epoch 1950, training loss: 311.80401611328125 = 0.030871350318193436 + 50.0 * 6.235462665557861
Epoch 1950, val loss: 1.7818595170974731
Epoch 1960, training loss: 311.9487609863281 = 0.030304204672574997 + 50.0 * 6.238368988037109
Epoch 1960, val loss: 1.7875711917877197
Epoch 1970, training loss: 311.8522033691406 = 0.029728055000305176 + 50.0 * 6.236449718475342
Epoch 1970, val loss: 1.7925095558166504
Epoch 1980, training loss: 311.9180603027344 = 0.029176555573940277 + 50.0 * 6.2377777099609375
Epoch 1980, val loss: 1.797998070716858
Epoch 1990, training loss: 312.0303039550781 = 0.02863740362226963 + 50.0 * 6.2400336265563965
Epoch 1990, val loss: 1.8032571077346802
Epoch 2000, training loss: 311.8005065917969 = 0.028108082711696625 + 50.0 * 6.235447883605957
Epoch 2000, val loss: 1.8086718320846558
Epoch 2010, training loss: 311.7320556640625 = 0.027594808489084244 + 50.0 * 6.234089374542236
Epoch 2010, val loss: 1.8138922452926636
Epoch 2020, training loss: 311.7306823730469 = 0.02710791677236557 + 50.0 * 6.234071731567383
Epoch 2020, val loss: 1.819337010383606
Epoch 2030, training loss: 311.7712707519531 = 0.02663477137684822 + 50.0 * 6.234892845153809
Epoch 2030, val loss: 1.8243870735168457
Epoch 2040, training loss: 312.1084289550781 = 0.0261656753718853 + 50.0 * 6.241645812988281
Epoch 2040, val loss: 1.829284906387329
Epoch 2050, training loss: 311.85247802734375 = 0.025690313428640366 + 50.0 * 6.236535549163818
Epoch 2050, val loss: 1.8349446058273315
Epoch 2060, training loss: 311.7855529785156 = 0.025230105966329575 + 50.0 * 6.235206604003906
Epoch 2060, val loss: 1.839620590209961
Epoch 2070, training loss: 311.67193603515625 = 0.024808965623378754 + 50.0 * 6.232942581176758
Epoch 2070, val loss: 1.845228672027588
Epoch 2080, training loss: 311.7173156738281 = 0.024395212531089783 + 50.0 * 6.233858585357666
Epoch 2080, val loss: 1.850319504737854
Epoch 2090, training loss: 311.9109191894531 = 0.02399025671184063 + 50.0 * 6.237738609313965
Epoch 2090, val loss: 1.8554174900054932
Epoch 2100, training loss: 311.7767028808594 = 0.02356928400695324 + 50.0 * 6.235063076019287
Epoch 2100, val loss: 1.8601559400558472
Epoch 2110, training loss: 311.7901306152344 = 0.023185180500149727 + 50.0 * 6.2353386878967285
Epoch 2110, val loss: 1.8653430938720703
Epoch 2120, training loss: 311.7882385253906 = 0.022791936993598938 + 50.0 * 6.235308647155762
Epoch 2120, val loss: 1.8699043989181519
Epoch 2130, training loss: 311.7033996582031 = 0.02241591550409794 + 50.0 * 6.233619213104248
Epoch 2130, val loss: 1.874786376953125
Epoch 2140, training loss: 311.5926513671875 = 0.022051425650715828 + 50.0 * 6.231411933898926
Epoch 2140, val loss: 1.8799508810043335
Epoch 2150, training loss: 311.6239929199219 = 0.021702945232391357 + 50.0 * 6.232046127319336
Epoch 2150, val loss: 1.8848457336425781
Epoch 2160, training loss: 311.8365173339844 = 0.021358300000429153 + 50.0 * 6.236303329467773
Epoch 2160, val loss: 1.8893829584121704
Epoch 2170, training loss: 311.8759765625 = 0.02102048136293888 + 50.0 * 6.2370991706848145
Epoch 2170, val loss: 1.8944330215454102
Epoch 2180, training loss: 311.6207580566406 = 0.020674023777246475 + 50.0 * 6.232001781463623
Epoch 2180, val loss: 1.898809552192688
Epoch 2190, training loss: 311.595947265625 = 0.020350074395537376 + 50.0 * 6.23151159286499
Epoch 2190, val loss: 1.9035056829452515
Epoch 2200, training loss: 311.8376159667969 = 0.020046688616275787 + 50.0 * 6.236351013183594
Epoch 2200, val loss: 1.9081133604049683
Epoch 2210, training loss: 311.6817626953125 = 0.01973132975399494 + 50.0 * 6.233240127563477
Epoch 2210, val loss: 1.9135348796844482
Epoch 2220, training loss: 311.6144714355469 = 0.019414929673075676 + 50.0 * 6.231901168823242
Epoch 2220, val loss: 1.9175817966461182
Epoch 2230, training loss: 311.49163818359375 = 0.0191210750490427 + 50.0 * 6.229450225830078
Epoch 2230, val loss: 1.922302007675171
Epoch 2240, training loss: 311.4869689941406 = 0.018835892900824547 + 50.0 * 6.229362964630127
Epoch 2240, val loss: 1.926794171333313
Epoch 2250, training loss: 311.6974792480469 = 0.018560616299510002 + 50.0 * 6.233578681945801
Epoch 2250, val loss: 1.9311089515686035
Epoch 2260, training loss: 311.4997863769531 = 0.018279096111655235 + 50.0 * 6.229629993438721
Epoch 2260, val loss: 1.9359042644500732
Epoch 2270, training loss: 311.5583190917969 = 0.018015211448073387 + 50.0 * 6.230806350708008
Epoch 2270, val loss: 1.9405189752578735
Epoch 2280, training loss: 311.6560974121094 = 0.017747731879353523 + 50.0 * 6.232766628265381
Epoch 2280, val loss: 1.944671630859375
Epoch 2290, training loss: 311.5469665527344 = 0.017488768324255943 + 50.0 * 6.230589866638184
Epoch 2290, val loss: 1.9490888118743896
Epoch 2300, training loss: 311.84539794921875 = 0.017239868640899658 + 50.0 * 6.236563205718994
Epoch 2300, val loss: 1.9528383016586304
Epoch 2310, training loss: 311.4924011230469 = 0.016983147710561752 + 50.0 * 6.229507923126221
Epoch 2310, val loss: 1.9576950073242188
Epoch 2320, training loss: 311.38232421875 = 0.016742492094635963 + 50.0 * 6.227311611175537
Epoch 2320, val loss: 1.961976408958435
Epoch 2330, training loss: 311.37945556640625 = 0.01651543565094471 + 50.0 * 6.227259159088135
Epoch 2330, val loss: 1.9663822650909424
Epoch 2340, training loss: 311.7903137207031 = 0.01629387028515339 + 50.0 * 6.235480308532715
Epoch 2340, val loss: 1.9701061248779297
Epoch 2350, training loss: 311.4614562988281 = 0.016057217493653297 + 50.0 * 6.228908061981201
Epoch 2350, val loss: 1.975297451019287
Epoch 2360, training loss: 311.4618225097656 = 0.0158371739089489 + 50.0 * 6.228919506072998
Epoch 2360, val loss: 1.9789597988128662
Epoch 2370, training loss: 311.5544738769531 = 0.01562785916030407 + 50.0 * 6.230776786804199
Epoch 2370, val loss: 1.9834059476852417
Epoch 2380, training loss: 311.44622802734375 = 0.01540796086192131 + 50.0 * 6.228616237640381
Epoch 2380, val loss: 1.987375020980835
Epoch 2390, training loss: 311.46075439453125 = 0.015205850824713707 + 50.0 * 6.228911399841309
Epoch 2390, val loss: 1.9915995597839355
Epoch 2400, training loss: 311.4862976074219 = 0.015001670457422733 + 50.0 * 6.22942590713501
Epoch 2400, val loss: 1.9955071210861206
Epoch 2410, training loss: 311.4486999511719 = 0.014800523407757282 + 50.0 * 6.228677749633789
Epoch 2410, val loss: 1.999297857284546
Epoch 2420, training loss: 311.3917236328125 = 0.01460599061101675 + 50.0 * 6.227541923522949
Epoch 2420, val loss: 2.0029900074005127
Epoch 2430, training loss: 311.34063720703125 = 0.014414451085031033 + 50.0 * 6.226524353027344
Epoch 2430, val loss: 2.007084846496582
Epoch 2440, training loss: 311.393310546875 = 0.014231621287763119 + 50.0 * 6.22758150100708
Epoch 2440, val loss: 2.0110056400299072
Epoch 2450, training loss: 311.4091491699219 = 0.014050805941224098 + 50.0 * 6.227902412414551
Epoch 2450, val loss: 2.015188694000244
Epoch 2460, training loss: 311.5766296386719 = 0.01387547142803669 + 50.0 * 6.231254577636719
Epoch 2460, val loss: 2.0193965435028076
Epoch 2470, training loss: 311.2830810546875 = 0.013692711479961872 + 50.0 * 6.2253875732421875
Epoch 2470, val loss: 2.022761106491089
Epoch 2480, training loss: 311.3533935546875 = 0.01352366991341114 + 50.0 * 6.226797580718994
Epoch 2480, val loss: 2.027062177658081
Epoch 2490, training loss: 311.2938232421875 = 0.01335491519421339 + 50.0 * 6.225608825683594
Epoch 2490, val loss: 2.0305886268615723
Epoch 2500, training loss: 311.2530822753906 = 0.013193581253290176 + 50.0 * 6.224798202514648
Epoch 2500, val loss: 2.034303903579712
Epoch 2510, training loss: 311.3145751953125 = 0.013035775162279606 + 50.0 * 6.2260308265686035
Epoch 2510, val loss: 2.0383756160736084
Epoch 2520, training loss: 311.4942626953125 = 0.012880813330411911 + 50.0 * 6.22962760925293
Epoch 2520, val loss: 2.0423784255981445
Epoch 2530, training loss: 311.35931396484375 = 0.012717034667730331 + 50.0 * 6.226931571960449
Epoch 2530, val loss: 2.0452191829681396
Epoch 2540, training loss: 311.22216796875 = 0.01256133895367384 + 50.0 * 6.224192142486572
Epoch 2540, val loss: 2.049236536026001
Epoch 2550, training loss: 311.1773376464844 = 0.012416260316967964 + 50.0 * 6.223298072814941
Epoch 2550, val loss: 2.052546262741089
Epoch 2560, training loss: 311.3120422363281 = 0.012275965884327888 + 50.0 * 6.2259955406188965
Epoch 2560, val loss: 2.0561985969543457
Epoch 2570, training loss: 311.312255859375 = 0.012129846960306168 + 50.0 * 6.2260026931762695
Epoch 2570, val loss: 2.0600996017456055
Epoch 2580, training loss: 311.3034973144531 = 0.0119834765791893 + 50.0 * 6.225830078125
Epoch 2580, val loss: 2.0632972717285156
Epoch 2590, training loss: 311.1811218261719 = 0.011841025203466415 + 50.0 * 6.223385810852051
Epoch 2590, val loss: 2.0669667720794678
Epoch 2600, training loss: 311.1948547363281 = 0.011708916164934635 + 50.0 * 6.223662853240967
Epoch 2600, val loss: 2.070162773132324
Epoch 2610, training loss: 311.20013427734375 = 0.011578491888940334 + 50.0 * 6.223771095275879
Epoch 2610, val loss: 2.0738017559051514
Epoch 2620, training loss: 311.2206115722656 = 0.011449768207967281 + 50.0 * 6.224183559417725
Epoch 2620, val loss: 2.0772769451141357
Epoch 2630, training loss: 311.27935791015625 = 0.011322485283017159 + 50.0 * 6.225360870361328
Epoch 2630, val loss: 2.0809710025787354
Epoch 2640, training loss: 311.205322265625 = 0.011199453845620155 + 50.0 * 6.22388219833374
Epoch 2640, val loss: 2.0847654342651367
Epoch 2650, training loss: 311.18035888671875 = 0.011073634959757328 + 50.0 * 6.223385810852051
Epoch 2650, val loss: 2.087758779525757
Epoch 2660, training loss: 311.24102783203125 = 0.010954051278531551 + 50.0 * 6.2246012687683105
Epoch 2660, val loss: 2.0910706520080566
Epoch 2670, training loss: 311.1897888183594 = 0.010835851542651653 + 50.0 * 6.223579406738281
Epoch 2670, val loss: 2.0941848754882812
Epoch 2680, training loss: 311.1151123046875 = 0.010719242505729198 + 50.0 * 6.222087860107422
Epoch 2680, val loss: 2.097782611846924
Epoch 2690, training loss: 311.19781494140625 = 0.010606744326651096 + 50.0 * 6.223743915557861
Epoch 2690, val loss: 2.100950241088867
Epoch 2700, training loss: 311.30120849609375 = 0.010493508540093899 + 50.0 * 6.225814342498779
Epoch 2700, val loss: 2.104656219482422
Epoch 2710, training loss: 311.18316650390625 = 0.010381358675658703 + 50.0 * 6.223455905914307
Epoch 2710, val loss: 2.107746124267578
Epoch 2720, training loss: 311.1343688964844 = 0.010270685888826847 + 50.0 * 6.222481727600098
Epoch 2720, val loss: 2.1103017330169678
Epoch 2730, training loss: 311.1064147949219 = 0.010166982188820839 + 50.0 * 6.221924781799316
Epoch 2730, val loss: 2.114203453063965
Epoch 2740, training loss: 311.0776062011719 = 0.010063240304589272 + 50.0 * 6.22135066986084
Epoch 2740, val loss: 2.117025136947632
Epoch 2750, training loss: 311.0958251953125 = 0.009961968287825584 + 50.0 * 6.221717834472656
Epoch 2750, val loss: 2.1203105449676514
Epoch 2760, training loss: 311.1593322753906 = 0.009862121194601059 + 50.0 * 6.222989559173584
Epoch 2760, val loss: 2.1236419677734375
Epoch 2770, training loss: 311.1587829589844 = 0.009759483858942986 + 50.0 * 6.222980499267578
Epoch 2770, val loss: 2.125905752182007
Epoch 2780, training loss: 311.2750549316406 = 0.009663015604019165 + 50.0 * 6.225307464599609
Epoch 2780, val loss: 2.1292145252227783
Epoch 2790, training loss: 311.0736389160156 = 0.009559974074363708 + 50.0 * 6.2212815284729
Epoch 2790, val loss: 2.131915807723999
Epoch 2800, training loss: 310.9820861816406 = 0.009467105381190777 + 50.0 * 6.219452381134033
Epoch 2800, val loss: 2.135079860687256
Epoch 2810, training loss: 311.04254150390625 = 0.009377216920256615 + 50.0 * 6.220663547515869
Epoch 2810, val loss: 2.1378958225250244
Epoch 2820, training loss: 311.1900329589844 = 0.009288892149925232 + 50.0 * 6.2236151695251465
Epoch 2820, val loss: 2.1406004428863525
Epoch 2830, training loss: 311.0478820800781 = 0.009195155464112759 + 50.0 * 6.220773696899414
Epoch 2830, val loss: 2.1440682411193848
Epoch 2840, training loss: 311.1201171875 = 0.009106618352234364 + 50.0 * 6.222220420837402
Epoch 2840, val loss: 2.1461234092712402
Epoch 2850, training loss: 311.0012512207031 = 0.009017844684422016 + 50.0 * 6.219844818115234
Epoch 2850, val loss: 2.1494388580322266
Epoch 2860, training loss: 311.0119323730469 = 0.008932951837778091 + 50.0 * 6.220059871673584
Epoch 2860, val loss: 2.1522767543792725
Epoch 2870, training loss: 311.00750732421875 = 0.008849885314702988 + 50.0 * 6.219973087310791
Epoch 2870, val loss: 2.154827356338501
Epoch 2880, training loss: 311.1377258300781 = 0.008768182247877121 + 50.0 * 6.222579479217529
Epoch 2880, val loss: 2.1580474376678467
Epoch 2890, training loss: 311.1976623535156 = 0.00868651270866394 + 50.0 * 6.223779201507568
Epoch 2890, val loss: 2.161264419555664
Epoch 2900, training loss: 310.9501953125 = 0.00860083568841219 + 50.0 * 6.218832015991211
Epoch 2900, val loss: 2.1631622314453125
Epoch 2910, training loss: 310.87677001953125 = 0.008522676303982735 + 50.0 * 6.217365264892578
Epoch 2910, val loss: 2.1667239665985107
Epoch 2920, training loss: 310.92352294921875 = 0.008448455482721329 + 50.0 * 6.218301296234131
Epoch 2920, val loss: 2.169149875640869
Epoch 2930, training loss: 311.210205078125 = 0.008374384604394436 + 50.0 * 6.224036693572998
Epoch 2930, val loss: 2.171781301498413
Epoch 2940, training loss: 310.9588317871094 = 0.008295801468193531 + 50.0 * 6.219010829925537
Epoch 2940, val loss: 2.1750667095184326
Epoch 2950, training loss: 310.90826416015625 = 0.008219349198043346 + 50.0 * 6.218001365661621
Epoch 2950, val loss: 2.1773109436035156
Epoch 2960, training loss: 310.9842834472656 = 0.008147728629410267 + 50.0 * 6.219522476196289
Epoch 2960, val loss: 2.180246114730835
Epoch 2970, training loss: 310.88641357421875 = 0.008074883371591568 + 50.0 * 6.21756649017334
Epoch 2970, val loss: 2.182981252670288
Epoch 2980, training loss: 310.9848937988281 = 0.008007053285837173 + 50.0 * 6.21953821182251
Epoch 2980, val loss: 2.185554027557373
Epoch 2990, training loss: 310.95794677734375 = 0.007936416193842888 + 50.0 * 6.219000339508057
Epoch 2990, val loss: 2.1881954669952393
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6037037037037037
0.8039008961518187
=== training gcn model ===
Epoch 0, training loss: 431.7979431152344 = 1.9569076299667358 + 50.0 * 8.596820831298828
Epoch 0, val loss: 1.9533897638320923
Epoch 10, training loss: 431.7471618652344 = 1.9479146003723145 + 50.0 * 8.595985412597656
Epoch 10, val loss: 1.9441564083099365
Epoch 20, training loss: 431.4667053222656 = 1.9364652633666992 + 50.0 * 8.590604782104492
Epoch 20, val loss: 1.9323455095291138
Epoch 30, training loss: 429.6500244140625 = 1.9211511611938477 + 50.0 * 8.554577827453613
Epoch 30, val loss: 1.9167389869689941
Epoch 40, training loss: 417.50201416015625 = 1.9016855955123901 + 50.0 * 8.312006950378418
Epoch 40, val loss: 1.897538423538208
Epoch 50, training loss: 385.03753662109375 = 1.876894235610962 + 50.0 * 7.663212776184082
Epoch 50, val loss: 1.8738212585449219
Epoch 60, training loss: 369.88116455078125 = 1.8609825372695923 + 50.0 * 7.360403537750244
Epoch 60, val loss: 1.8596552610397339
Epoch 70, training loss: 360.255859375 = 1.850598692893982 + 50.0 * 7.168105602264404
Epoch 70, val loss: 1.8497639894485474
Epoch 80, training loss: 353.6197814941406 = 1.838718056678772 + 50.0 * 7.035621166229248
Epoch 80, val loss: 1.838928461074829
Epoch 90, training loss: 348.9534606933594 = 1.8280324935913086 + 50.0 * 6.942508697509766
Epoch 90, val loss: 1.8292367458343506
Epoch 100, training loss: 344.92254638671875 = 1.8189897537231445 + 50.0 * 6.8620710372924805
Epoch 100, val loss: 1.8210618495941162
Epoch 110, training loss: 341.7874450683594 = 1.8112281560897827 + 50.0 * 6.799524784088135
Epoch 110, val loss: 1.8137397766113281
Epoch 120, training loss: 338.75335693359375 = 1.8036949634552002 + 50.0 * 6.7389936447143555
Epoch 120, val loss: 1.8065210580825806
Epoch 130, training loss: 336.3866271972656 = 1.7964906692504883 + 50.0 * 6.691802978515625
Epoch 130, val loss: 1.7996749877929688
Epoch 140, training loss: 334.6075134277344 = 1.7888659238815308 + 50.0 * 6.656372547149658
Epoch 140, val loss: 1.7926793098449707
Epoch 150, training loss: 333.05841064453125 = 1.780578374862671 + 50.0 * 6.625556468963623
Epoch 150, val loss: 1.7852751016616821
Epoch 160, training loss: 331.6358337402344 = 1.771706461906433 + 50.0 * 6.597282409667969
Epoch 160, val loss: 1.7774858474731445
Epoch 170, training loss: 330.622314453125 = 1.7621996402740479 + 50.0 * 6.577201843261719
Epoch 170, val loss: 1.769135594367981
Epoch 180, training loss: 329.47979736328125 = 1.7516894340515137 + 50.0 * 6.554562091827393
Epoch 180, val loss: 1.7600548267364502
Epoch 190, training loss: 328.48333740234375 = 1.7402371168136597 + 50.0 * 6.534862041473389
Epoch 190, val loss: 1.7501951456069946
Epoch 200, training loss: 327.67877197265625 = 1.7277188301086426 + 50.0 * 6.519021034240723
Epoch 200, val loss: 1.7394990921020508
Epoch 210, training loss: 326.8673400878906 = 1.7139270305633545 + 50.0 * 6.503067970275879
Epoch 210, val loss: 1.727747917175293
Epoch 220, training loss: 326.1431884765625 = 1.6988840103149414 + 50.0 * 6.48888635635376
Epoch 220, val loss: 1.7150133848190308
Epoch 230, training loss: 325.6568298339844 = 1.682482361793518 + 50.0 * 6.47948694229126
Epoch 230, val loss: 1.701149821281433
Epoch 240, training loss: 324.9923400878906 = 1.6647591590881348 + 50.0 * 6.466551303863525
Epoch 240, val loss: 1.6862000226974487
Epoch 250, training loss: 324.4780578613281 = 1.6458477973937988 + 50.0 * 6.456644535064697
Epoch 250, val loss: 1.6702316999435425
Epoch 260, training loss: 324.06707763671875 = 1.625772476196289 + 50.0 * 6.448826313018799
Epoch 260, val loss: 1.6533869504928589
Epoch 270, training loss: 323.5546875 = 1.6047946214675903 + 50.0 * 6.438997745513916
Epoch 270, val loss: 1.635957956314087
Epoch 280, training loss: 323.0970153808594 = 1.5831506252288818 + 50.0 * 6.430277347564697
Epoch 280, val loss: 1.618134617805481
Epoch 290, training loss: 322.73284912109375 = 1.561063289642334 + 50.0 * 6.423435688018799
Epoch 290, val loss: 1.6000843048095703
Epoch 300, training loss: 322.389892578125 = 1.5385777950286865 + 50.0 * 6.417026519775391
Epoch 300, val loss: 1.5820448398590088
Epoch 310, training loss: 321.9845886230469 = 1.5160194635391235 + 50.0 * 6.409371376037598
Epoch 310, val loss: 1.5640842914581299
Epoch 320, training loss: 321.623291015625 = 1.4935333728790283 + 50.0 * 6.402595520019531
Epoch 320, val loss: 1.5464510917663574
Epoch 330, training loss: 321.3033142089844 = 1.4713191986083984 + 50.0 * 6.396639823913574
Epoch 330, val loss: 1.5292478799819946
Epoch 340, training loss: 321.4398498535156 = 1.4494383335113525 + 50.0 * 6.399808406829834
Epoch 340, val loss: 1.5125679969787598
Epoch 350, training loss: 320.851318359375 = 1.4277757406234741 + 50.0 * 6.3884711265563965
Epoch 350, val loss: 1.4963432550430298
Epoch 360, training loss: 320.49285888671875 = 1.4066147804260254 + 50.0 * 6.381724834442139
Epoch 360, val loss: 1.4807310104370117
Epoch 370, training loss: 320.251220703125 = 1.3858885765075684 + 50.0 * 6.3773064613342285
Epoch 370, val loss: 1.4656230211257935
Epoch 380, training loss: 320.16131591796875 = 1.3653868436813354 + 50.0 * 6.375918865203857
Epoch 380, val loss: 1.4508756399154663
Epoch 390, training loss: 319.795654296875 = 1.3452129364013672 + 50.0 * 6.369009017944336
Epoch 390, val loss: 1.436543345451355
Epoch 400, training loss: 319.5599060058594 = 1.325309157371521 + 50.0 * 6.364691734313965
Epoch 400, val loss: 1.4225980043411255
Epoch 410, training loss: 319.3897705078125 = 1.3054959774017334 + 50.0 * 6.361685752868652
Epoch 410, val loss: 1.4087799787521362
Epoch 420, training loss: 319.1183776855469 = 1.285689115524292 + 50.0 * 6.356654167175293
Epoch 420, val loss: 1.3950603008270264
Epoch 430, training loss: 319.0082702636719 = 1.2658950090408325 + 50.0 * 6.354847431182861
Epoch 430, val loss: 1.3814020156860352
Epoch 440, training loss: 318.97857666015625 = 1.2458891868591309 + 50.0 * 6.354653835296631
Epoch 440, val loss: 1.3675967454910278
Epoch 450, training loss: 318.6390380859375 = 1.2257417440414429 + 50.0 * 6.348266124725342
Epoch 450, val loss: 1.3538084030151367
Epoch 460, training loss: 318.3702697753906 = 1.2053430080413818 + 50.0 * 6.343298435211182
Epoch 460, val loss: 1.3397216796875
Epoch 470, training loss: 318.208251953125 = 1.184746265411377 + 50.0 * 6.340469837188721
Epoch 470, val loss: 1.3254817724227905
Epoch 480, training loss: 318.36761474609375 = 1.163783073425293 + 50.0 * 6.344076633453369
Epoch 480, val loss: 1.310975193977356
Epoch 490, training loss: 318.04241943359375 = 1.1424764394760132 + 50.0 * 6.337998867034912
Epoch 490, val loss: 1.2962243556976318
Epoch 500, training loss: 317.7864074707031 = 1.1209217309951782 + 50.0 * 6.333309650421143
Epoch 500, val loss: 1.281250238418579
Epoch 510, training loss: 317.6363220214844 = 1.099109411239624 + 50.0 * 6.33074426651001
Epoch 510, val loss: 1.2660683393478394
Epoch 520, training loss: 317.4794616699219 = 1.076955795288086 + 50.0 * 6.328049659729004
Epoch 520, val loss: 1.2506697177886963
Epoch 530, training loss: 317.4045104980469 = 1.054581642150879 + 50.0 * 6.326998233795166
Epoch 530, val loss: 1.2352502346038818
Epoch 540, training loss: 317.4119567871094 = 1.0320261716842651 + 50.0 * 6.327599048614502
Epoch 540, val loss: 1.2197246551513672
Epoch 550, training loss: 317.1188659667969 = 1.009228229522705 + 50.0 * 6.322193145751953
Epoch 550, val loss: 1.2040061950683594
Epoch 560, training loss: 316.95733642578125 = 0.9864526987075806 + 50.0 * 6.319417476654053
Epoch 560, val loss: 1.1885000467300415
Epoch 570, training loss: 316.8236083984375 = 0.9636993408203125 + 50.0 * 6.317198276519775
Epoch 570, val loss: 1.1731884479522705
Epoch 580, training loss: 317.3330383300781 = 0.9409058094024658 + 50.0 * 6.3278422355651855
Epoch 580, val loss: 1.1577619314193726
Epoch 590, training loss: 316.6946716308594 = 0.9181612133979797 + 50.0 * 6.315530300140381
Epoch 590, val loss: 1.1428102254867554
Epoch 600, training loss: 316.479248046875 = 0.895700216293335 + 50.0 * 6.311671257019043
Epoch 600, val loss: 1.128110408782959
Epoch 610, training loss: 316.3487548828125 = 0.8736268281936646 + 50.0 * 6.309502601623535
Epoch 610, val loss: 1.1138653755187988
Epoch 620, training loss: 316.26080322265625 = 0.8518818020820618 + 50.0 * 6.308178424835205
Epoch 620, val loss: 1.1000666618347168
Epoch 630, training loss: 316.219482421875 = 0.8304367065429688 + 50.0 * 6.307780742645264
Epoch 630, val loss: 1.086829423904419
Epoch 640, training loss: 316.2595520019531 = 0.8093951344490051 + 50.0 * 6.3090033531188965
Epoch 640, val loss: 1.0739942789077759
Epoch 650, training loss: 315.9825744628906 = 0.7889471650123596 + 50.0 * 6.303872585296631
Epoch 650, val loss: 1.0620713233947754
Epoch 660, training loss: 315.8360900878906 = 0.7690634727478027 + 50.0 * 6.301340579986572
Epoch 660, val loss: 1.0508710145950317
Epoch 670, training loss: 315.7945556640625 = 0.749738335609436 + 50.0 * 6.300896644592285
Epoch 670, val loss: 1.0404008626937866
Epoch 680, training loss: 315.6939697265625 = 0.7308758497238159 + 50.0 * 6.299262046813965
Epoch 680, val loss: 1.0305756330490112
Epoch 690, training loss: 315.7943420410156 = 0.7125569581985474 + 50.0 * 6.3016357421875
Epoch 690, val loss: 1.0214484930038452
Epoch 700, training loss: 315.5960388183594 = 0.6948434114456177 + 50.0 * 6.298023700714111
Epoch 700, val loss: 1.0128235816955566
Epoch 710, training loss: 315.40411376953125 = 0.6776556968688965 + 50.0 * 6.294529438018799
Epoch 710, val loss: 1.0051486492156982
Epoch 720, training loss: 315.41107177734375 = 0.6610504388809204 + 50.0 * 6.2950005531311035
Epoch 720, val loss: 0.9979872107505798
Epoch 730, training loss: 315.3656921386719 = 0.6448442935943604 + 50.0 * 6.294416904449463
Epoch 730, val loss: 0.9918721318244934
Epoch 740, training loss: 315.23187255859375 = 0.6290620565414429 + 50.0 * 6.292056083679199
Epoch 740, val loss: 0.9856360554695129
Epoch 750, training loss: 315.09295654296875 = 0.6138326525688171 + 50.0 * 6.289582252502441
Epoch 750, val loss: 0.9804247617721558
Epoch 760, training loss: 315.1086730957031 = 0.5990274548530579 + 50.0 * 6.290192604064941
Epoch 760, val loss: 0.9758132100105286
Epoch 770, training loss: 315.06219482421875 = 0.5845937728881836 + 50.0 * 6.289552211761475
Epoch 770, val loss: 0.9718142151832581
Epoch 780, training loss: 314.9002685546875 = 0.5704693794250488 + 50.0 * 6.286595821380615
Epoch 780, val loss: 0.9678618311882019
Epoch 790, training loss: 314.85504150390625 = 0.556775689125061 + 50.0 * 6.285965442657471
Epoch 790, val loss: 0.9647791981697083
Epoch 800, training loss: 314.92059326171875 = 0.5433682203292847 + 50.0 * 6.2875447273254395
Epoch 800, val loss: 0.961959719657898
Epoch 810, training loss: 314.6902770996094 = 0.530241847038269 + 50.0 * 6.283200740814209
Epoch 810, val loss: 0.9592914581298828
Epoch 820, training loss: 314.64019775390625 = 0.5174739956855774 + 50.0 * 6.282454490661621
Epoch 820, val loss: 0.9572961926460266
Epoch 830, training loss: 314.685302734375 = 0.5049949884414673 + 50.0 * 6.283606052398682
Epoch 830, val loss: 0.9555781483650208
Epoch 840, training loss: 314.6266174316406 = 0.4927615225315094 + 50.0 * 6.282676696777344
Epoch 840, val loss: 0.953987181186676
Epoch 850, training loss: 314.5860595703125 = 0.48071205615997314 + 50.0 * 6.282106876373291
Epoch 850, val loss: 0.9527950882911682
Epoch 860, training loss: 314.3885498046875 = 0.4689796566963196 + 50.0 * 6.278390884399414
Epoch 860, val loss: 0.9518458843231201
Epoch 870, training loss: 314.3082275390625 = 0.4574942886829376 + 50.0 * 6.27701473236084
Epoch 870, val loss: 0.9515291452407837
Epoch 880, training loss: 314.3997802734375 = 0.4462721347808838 + 50.0 * 6.2790703773498535
Epoch 880, val loss: 0.9511221647262573
Epoch 890, training loss: 314.332763671875 = 0.4351518750190735 + 50.0 * 6.277952194213867
Epoch 890, val loss: 0.9506579637527466
Epoch 900, training loss: 314.2154846191406 = 0.4242677092552185 + 50.0 * 6.275824546813965
Epoch 900, val loss: 0.9509525895118713
Epoch 910, training loss: 314.20513916015625 = 0.41362130641937256 + 50.0 * 6.275829792022705
Epoch 910, val loss: 0.9509233236312866
Epoch 920, training loss: 314.00872802734375 = 0.403177410364151 + 50.0 * 6.272110939025879
Epoch 920, val loss: 0.9515995979309082
Epoch 930, training loss: 314.0178527832031 = 0.392949640750885 + 50.0 * 6.27249813079834
Epoch 930, val loss: 0.9522643089294434
Epoch 940, training loss: 314.0897521972656 = 0.3828990161418915 + 50.0 * 6.274137020111084
Epoch 940, val loss: 0.9528354406356812
Epoch 950, training loss: 314.0417785644531 = 0.3730665147304535 + 50.0 * 6.273374080657959
Epoch 950, val loss: 0.9541749358177185
Epoch 960, training loss: 313.9624938964844 = 0.36333736777305603 + 50.0 * 6.2719831466674805
Epoch 960, val loss: 0.9550358653068542
Epoch 970, training loss: 313.7938232421875 = 0.3538114130496979 + 50.0 * 6.268799781799316
Epoch 970, val loss: 0.9564470052719116
Epoch 980, training loss: 313.72857666015625 = 0.34453368186950684 + 50.0 * 6.267680644989014
Epoch 980, val loss: 0.9580644369125366
Epoch 990, training loss: 313.76739501953125 = 0.3354564309120178 + 50.0 * 6.268639087677002
Epoch 990, val loss: 0.9596570134162903
Epoch 1000, training loss: 313.8866882324219 = 0.32654255628585815 + 50.0 * 6.27120304107666
Epoch 1000, val loss: 0.9612266421318054
Epoch 1010, training loss: 313.6595153808594 = 0.31772932410240173 + 50.0 * 6.266835689544678
Epoch 1010, val loss: 0.9630938768386841
Epoch 1020, training loss: 313.5327453613281 = 0.3092151880264282 + 50.0 * 6.264471054077148
Epoch 1020, val loss: 0.9651774168014526
Epoch 1030, training loss: 313.4946594238281 = 0.3009006381034851 + 50.0 * 6.2638750076293945
Epoch 1030, val loss: 0.9672316908836365
Epoch 1040, training loss: 313.7512512207031 = 0.2928096652030945 + 50.0 * 6.269168853759766
Epoch 1040, val loss: 0.9692372679710388
Epoch 1050, training loss: 313.6329345703125 = 0.2848297953605652 + 50.0 * 6.266962051391602
Epoch 1050, val loss: 0.9720567464828491
Epoch 1060, training loss: 313.4510192871094 = 0.27703210711479187 + 50.0 * 6.263479709625244
Epoch 1060, val loss: 0.973998486995697
Epoch 1070, training loss: 313.3418273925781 = 0.2694832384586334 + 50.0 * 6.261447429656982
Epoch 1070, val loss: 0.9766799211502075
Epoch 1080, training loss: 313.29718017578125 = 0.2621646523475647 + 50.0 * 6.260700225830078
Epoch 1080, val loss: 0.9795602560043335
Epoch 1090, training loss: 313.40570068359375 = 0.2550320625305176 + 50.0 * 6.2630133628845215
Epoch 1090, val loss: 0.9825255274772644
Epoch 1100, training loss: 313.2903747558594 = 0.2479991763830185 + 50.0 * 6.260848045349121
Epoch 1100, val loss: 0.9848452210426331
Epoch 1110, training loss: 313.3077087402344 = 0.24115681648254395 + 50.0 * 6.261331081390381
Epoch 1110, val loss: 0.9879645705223083
Epoch 1120, training loss: 313.3159484863281 = 0.23449812829494476 + 50.0 * 6.261629104614258
Epoch 1120, val loss: 0.9905799031257629
Epoch 1130, training loss: 313.1452941894531 = 0.22799554467201233 + 50.0 * 6.258346080780029
Epoch 1130, val loss: 0.993357241153717
Epoch 1140, training loss: 313.0492858886719 = 0.22175103425979614 + 50.0 * 6.2565507888793945
Epoch 1140, val loss: 0.9966387748718262
Epoch 1150, training loss: 313.0387878417969 = 0.21567659080028534 + 50.0 * 6.256462097167969
Epoch 1150, val loss: 0.9999203085899353
Epoch 1160, training loss: 313.47930908203125 = 0.20976844429969788 + 50.0 * 6.265390396118164
Epoch 1160, val loss: 1.0030816793441772
Epoch 1170, training loss: 313.0270080566406 = 0.20388826727867126 + 50.0 * 6.256462097167969
Epoch 1170, val loss: 1.0058963298797607
Epoch 1180, training loss: 312.9140930175781 = 0.1982530951499939 + 50.0 * 6.254317283630371
Epoch 1180, val loss: 1.0094250440597534
Epoch 1190, training loss: 312.8995361328125 = 0.19280605018138885 + 50.0 * 6.254134654998779
Epoch 1190, val loss: 1.012772560119629
Epoch 1200, training loss: 313.0600891113281 = 0.1875256896018982 + 50.0 * 6.257451057434082
Epoch 1200, val loss: 1.0160465240478516
Epoch 1210, training loss: 313.1041564941406 = 0.18236209452152252 + 50.0 * 6.2584357261657715
Epoch 1210, val loss: 1.0195019245147705
Epoch 1220, training loss: 312.9405822753906 = 0.17727471888065338 + 50.0 * 6.255266189575195
Epoch 1220, val loss: 1.0226049423217773
Epoch 1230, training loss: 312.7780456542969 = 0.17243324220180511 + 50.0 * 6.25211238861084
Epoch 1230, val loss: 1.0263195037841797
Epoch 1240, training loss: 312.7174072265625 = 0.16773955523967743 + 50.0 * 6.250993728637695
Epoch 1240, val loss: 1.0300209522247314
Epoch 1250, training loss: 313.2052001953125 = 0.16319358348846436 + 50.0 * 6.260839939117432
Epoch 1250, val loss: 1.0335596799850464
Epoch 1260, training loss: 312.8745422363281 = 0.1587582528591156 + 50.0 * 6.2543158531188965
Epoch 1260, val loss: 1.0371332168579102
Epoch 1270, training loss: 312.6554260253906 = 0.15442311763763428 + 50.0 * 6.2500200271606445
Epoch 1270, val loss: 1.0408023595809937
Epoch 1280, training loss: 312.6031188964844 = 0.15028852224349976 + 50.0 * 6.249056339263916
Epoch 1280, val loss: 1.0446131229400635
Epoch 1290, training loss: 312.8977966308594 = 0.14628709852695465 + 50.0 * 6.255029678344727
Epoch 1290, val loss: 1.0488653182983398
Epoch 1300, training loss: 312.84869384765625 = 0.14234058558940887 + 50.0 * 6.254127025604248
Epoch 1300, val loss: 1.0514392852783203
Epoch 1310, training loss: 312.5802917480469 = 0.13851851224899292 + 50.0 * 6.248835563659668
Epoch 1310, val loss: 1.0555535554885864
Epoch 1320, training loss: 312.49627685546875 = 0.13484947383403778 + 50.0 * 6.247228145599365
Epoch 1320, val loss: 1.0592150688171387
Epoch 1330, training loss: 312.61785888671875 = 0.13130733370780945 + 50.0 * 6.249731063842773
Epoch 1330, val loss: 1.0624775886535645
Epoch 1340, training loss: 312.5814514160156 = 0.12784451246261597 + 50.0 * 6.249072551727295
Epoch 1340, val loss: 1.066628336906433
Epoch 1350, training loss: 312.4234313964844 = 0.12447327375411987 + 50.0 * 6.245978832244873
Epoch 1350, val loss: 1.0700201988220215
Epoch 1360, training loss: 312.4043884277344 = 0.12122882157564163 + 50.0 * 6.245663642883301
Epoch 1360, val loss: 1.0738143920898438
Epoch 1370, training loss: 312.9744567871094 = 0.11810784786939621 + 50.0 * 6.257127285003662
Epoch 1370, val loss: 1.0774933099746704
Epoch 1380, training loss: 312.63214111328125 = 0.11502189934253693 + 50.0 * 6.25034236907959
Epoch 1380, val loss: 1.080870270729065
Epoch 1390, training loss: 312.347412109375 = 0.11202294379472733 + 50.0 * 6.2447075843811035
Epoch 1390, val loss: 1.0846478939056396
Epoch 1400, training loss: 312.2932434082031 = 0.10917392373085022 + 50.0 * 6.243681907653809
Epoch 1400, val loss: 1.0883233547210693
Epoch 1410, training loss: 312.27044677734375 = 0.10641852021217346 + 50.0 * 6.24328088760376
Epoch 1410, val loss: 1.0921882390975952
Epoch 1420, training loss: 312.3822937011719 = 0.10375786572694778 + 50.0 * 6.245570659637451
Epoch 1420, val loss: 1.0958515405654907
Epoch 1430, training loss: 312.30242919921875 = 0.10112348198890686 + 50.0 * 6.244025707244873
Epoch 1430, val loss: 1.0993902683258057
Epoch 1440, training loss: 312.4195556640625 = 0.09856829792261124 + 50.0 * 6.246419429779053
Epoch 1440, val loss: 1.1030651330947876
Epoch 1450, training loss: 312.23626708984375 = 0.09610041230916977 + 50.0 * 6.24280309677124
Epoch 1450, val loss: 1.1066186428070068
Epoch 1460, training loss: 312.18902587890625 = 0.09373192489147186 + 50.0 * 6.24190616607666
Epoch 1460, val loss: 1.1105704307556152
Epoch 1470, training loss: 312.2120666503906 = 0.09142794460058212 + 50.0 * 6.242412567138672
Epoch 1470, val loss: 1.1143451929092407
Epoch 1480, training loss: 312.27313232421875 = 0.0891798809170723 + 50.0 * 6.243679046630859
Epoch 1480, val loss: 1.1180909872055054
Epoch 1490, training loss: 312.1250915527344 = 0.0869869738817215 + 50.0 * 6.240762233734131
Epoch 1490, val loss: 1.121716022491455
Epoch 1500, training loss: 312.1173400878906 = 0.08488170802593231 + 50.0 * 6.240649700164795
Epoch 1500, val loss: 1.1255722045898438
Epoch 1510, training loss: 312.389404296875 = 0.08286972343921661 + 50.0 * 6.24613094329834
Epoch 1510, val loss: 1.1295102834701538
Epoch 1520, training loss: 312.15948486328125 = 0.08083543926477432 + 50.0 * 6.241572856903076
Epoch 1520, val loss: 1.1326302289962769
Epoch 1530, training loss: 312.09735107421875 = 0.07891499996185303 + 50.0 * 6.240368366241455
Epoch 1530, val loss: 1.1368122100830078
Epoch 1540, training loss: 312.2729187011719 = 0.07704828679561615 + 50.0 * 6.243916988372803
Epoch 1540, val loss: 1.1404565572738647
Epoch 1550, training loss: 312.08837890625 = 0.07523445039987564 + 50.0 * 6.240262985229492
Epoch 1550, val loss: 1.1443361043930054
Epoch 1560, training loss: 312.012939453125 = 0.07346019148826599 + 50.0 * 6.2387895584106445
Epoch 1560, val loss: 1.1480668783187866
Epoch 1570, training loss: 312.0839538574219 = 0.07175206393003464 + 50.0 * 6.240244388580322
Epoch 1570, val loss: 1.1518239974975586
Epoch 1580, training loss: 312.1206970214844 = 0.07008128613233566 + 50.0 * 6.2410125732421875
Epoch 1580, val loss: 1.1554064750671387
Epoch 1590, training loss: 311.9206848144531 = 0.06845219433307648 + 50.0 * 6.237044811248779
Epoch 1590, val loss: 1.1589596271514893
Epoch 1600, training loss: 311.885986328125 = 0.06688909232616425 + 50.0 * 6.236381530761719
Epoch 1600, val loss: 1.1628552675247192
Epoch 1610, training loss: 311.9281005859375 = 0.06538096070289612 + 50.0 * 6.237254619598389
Epoch 1610, val loss: 1.1664568185806274
Epoch 1620, training loss: 312.1363525390625 = 0.06391312927007675 + 50.0 * 6.241448879241943
Epoch 1620, val loss: 1.1699188947677612
Epoch 1630, training loss: 312.03228759765625 = 0.062464434653520584 + 50.0 * 6.239396095275879
Epoch 1630, val loss: 1.174115777015686
Epoch 1640, training loss: 312.14422607421875 = 0.06104903295636177 + 50.0 * 6.241663932800293
Epoch 1640, val loss: 1.1773749589920044
Epoch 1650, training loss: 311.86737060546875 = 0.059674669057130814 + 50.0 * 6.236154079437256
Epoch 1650, val loss: 1.1814504861831665
Epoch 1660, training loss: 311.7855529785156 = 0.05835475027561188 + 50.0 * 6.234544277191162
Epoch 1660, val loss: 1.1851179599761963
Epoch 1670, training loss: 311.8057556152344 = 0.05708413943648338 + 50.0 * 6.234973430633545
Epoch 1670, val loss: 1.1888574361801147
Epoch 1680, training loss: 312.1643981933594 = 0.05586197227239609 + 50.0 * 6.242170810699463
Epoch 1680, val loss: 1.1929155588150024
Epoch 1690, training loss: 311.9662170410156 = 0.05460799112915993 + 50.0 * 6.238232612609863
Epoch 1690, val loss: 1.1956762075424194
Epoch 1700, training loss: 311.76263427734375 = 0.05341357737779617 + 50.0 * 6.234184265136719
Epoch 1700, val loss: 1.199530005455017
Epoch 1710, training loss: 311.7502746582031 = 0.052268173545598984 + 50.0 * 6.233959674835205
Epoch 1710, val loss: 1.2031182050704956
Epoch 1720, training loss: 312.0964050292969 = 0.051166024059057236 + 50.0 * 6.240904331207275
Epoch 1720, val loss: 1.2067536115646362
Epoch 1730, training loss: 311.8355407714844 = 0.050076622515916824 + 50.0 * 6.235709190368652
Epoch 1730, val loss: 1.2102782726287842
Epoch 1740, training loss: 311.7100524902344 = 0.04901416599750519 + 50.0 * 6.233221054077148
Epoch 1740, val loss: 1.213911771774292
Epoch 1750, training loss: 311.7044677734375 = 0.048006121069192886 + 50.0 * 6.233129501342773
Epoch 1750, val loss: 1.2176402807235718
Epoch 1760, training loss: 311.9250793457031 = 0.04702087864279747 + 50.0 * 6.237561225891113
Epoch 1760, val loss: 1.221017837524414
Epoch 1770, training loss: 311.7721862792969 = 0.04602833464741707 + 50.0 * 6.234523296356201
Epoch 1770, val loss: 1.2240519523620605
Epoch 1780, training loss: 311.6903076171875 = 0.045089241117239 + 50.0 * 6.232904434204102
Epoch 1780, val loss: 1.2280325889587402
Epoch 1790, training loss: 311.6819152832031 = 0.04417390376329422 + 50.0 * 6.232755184173584
Epoch 1790, val loss: 1.2313623428344727
Epoch 1800, training loss: 311.7324523925781 = 0.04329494759440422 + 50.0 * 6.23378324508667
Epoch 1800, val loss: 1.234925389289856
Epoch 1810, training loss: 311.7640686035156 = 0.04241977259516716 + 50.0 * 6.234433174133301
Epoch 1810, val loss: 1.2377421855926514
Epoch 1820, training loss: 311.5748596191406 = 0.04155735671520233 + 50.0 * 6.230666160583496
Epoch 1820, val loss: 1.2418183088302612
Epoch 1830, training loss: 311.52783203125 = 0.04074188321828842 + 50.0 * 6.22974157333374
Epoch 1830, val loss: 1.2451164722442627
Epoch 1840, training loss: 311.5309143066406 = 0.039952076971530914 + 50.0 * 6.229819297790527
Epoch 1840, val loss: 1.2485759258270264
Epoch 1850, training loss: 311.67254638671875 = 0.039193857461214066 + 50.0 * 6.232666969299316
Epoch 1850, val loss: 1.252089500427246
Epoch 1860, training loss: 311.6497497558594 = 0.038426123559474945 + 50.0 * 6.232226848602295
Epoch 1860, val loss: 1.2554383277893066
Epoch 1870, training loss: 311.6102294921875 = 0.037682365626096725 + 50.0 * 6.231451034545898
Epoch 1870, val loss: 1.258722186088562
Epoch 1880, training loss: 311.51214599609375 = 0.03695511445403099 + 50.0 * 6.229503631591797
Epoch 1880, val loss: 1.261788249015808
Epoch 1890, training loss: 311.5225524902344 = 0.03626321256160736 + 50.0 * 6.2297258377075195
Epoch 1890, val loss: 1.2653101682662964
Epoch 1900, training loss: 311.551025390625 = 0.03558403626084328 + 50.0 * 6.230308532714844
Epoch 1900, val loss: 1.2685273885726929
Epoch 1910, training loss: 311.5726318359375 = 0.034925445914268494 + 50.0 * 6.230754375457764
Epoch 1910, val loss: 1.2721129655838013
Epoch 1920, training loss: 311.69793701171875 = 0.034278031438589096 + 50.0 * 6.233273029327393
Epoch 1920, val loss: 1.2754888534545898
Epoch 1930, training loss: 311.5177917480469 = 0.03364121913909912 + 50.0 * 6.2296833992004395
Epoch 1930, val loss: 1.2781727313995361
Epoch 1940, training loss: 311.4356689453125 = 0.03303040564060211 + 50.0 * 6.228053092956543
Epoch 1940, val loss: 1.2817827463150024
Epoch 1950, training loss: 311.6308898925781 = 0.03245444968342781 + 50.0 * 6.231968879699707
Epoch 1950, val loss: 1.2847474813461304
Epoch 1960, training loss: 311.4337463378906 = 0.031852394342422485 + 50.0 * 6.2280378341674805
Epoch 1960, val loss: 1.2879928350448608
Epoch 1970, training loss: 311.4261474609375 = 0.03127879276871681 + 50.0 * 6.2278971672058105
Epoch 1970, val loss: 1.2908140420913696
Epoch 1980, training loss: 311.53216552734375 = 0.030727876350283623 + 50.0 * 6.2300286293029785
Epoch 1980, val loss: 1.2940959930419922
Epoch 1990, training loss: 311.3854064941406 = 0.030185379087924957 + 50.0 * 6.227104187011719
Epoch 1990, val loss: 1.2973101139068604
Epoch 2000, training loss: 311.4322814941406 = 0.029658880084753036 + 50.0 * 6.228052616119385
Epoch 2000, val loss: 1.300624132156372
Epoch 2010, training loss: 311.5682678222656 = 0.02914811111986637 + 50.0 * 6.230782508850098
Epoch 2010, val loss: 1.3036186695098877
Epoch 2020, training loss: 311.5624084472656 = 0.028641046956181526 + 50.0 * 6.230674743652344
Epoch 2020, val loss: 1.3066402673721313
Epoch 2030, training loss: 311.3634033203125 = 0.028142116963863373 + 50.0 * 6.226705074310303
Epoch 2030, val loss: 1.309482216835022
Epoch 2040, training loss: 311.3005676269531 = 0.02766476944088936 + 50.0 * 6.225457668304443
Epoch 2040, val loss: 1.3125572204589844
Epoch 2050, training loss: 311.3001403808594 = 0.02720281109213829 + 50.0 * 6.225459098815918
Epoch 2050, val loss: 1.3155083656311035
Epoch 2060, training loss: 311.6378173828125 = 0.026756053790450096 + 50.0 * 6.2322211265563965
Epoch 2060, val loss: 1.3178789615631104
Epoch 2070, training loss: 311.5051574707031 = 0.026309167966246605 + 50.0 * 6.22957706451416
Epoch 2070, val loss: 1.3214308023452759
Epoch 2080, training loss: 311.32476806640625 = 0.02586335688829422 + 50.0 * 6.225978374481201
Epoch 2080, val loss: 1.3242342472076416
Epoch 2090, training loss: 311.33514404296875 = 0.025449197739362717 + 50.0 * 6.226193904876709
Epoch 2090, val loss: 1.3273136615753174
Epoch 2100, training loss: 311.38104248046875 = 0.02503642998635769 + 50.0 * 6.227120399475098
Epoch 2100, val loss: 1.3300130367279053
Epoch 2110, training loss: 311.3285827636719 = 0.024625465273857117 + 50.0 * 6.226078987121582
Epoch 2110, val loss: 1.3329012393951416
Epoch 2120, training loss: 311.41680908203125 = 0.024238958954811096 + 50.0 * 6.227851867675781
Epoch 2120, val loss: 1.335533618927002
Epoch 2130, training loss: 311.2638244628906 = 0.023840460926294327 + 50.0 * 6.224799633026123
Epoch 2130, val loss: 1.3384389877319336
Epoch 2140, training loss: 311.2650451660156 = 0.023467227816581726 + 50.0 * 6.224831581115723
Epoch 2140, val loss: 1.341315746307373
Epoch 2150, training loss: 311.1826477050781 = 0.023100698366761208 + 50.0 * 6.223190784454346
Epoch 2150, val loss: 1.34416663646698
Epoch 2160, training loss: 311.54766845703125 = 0.022756308317184448 + 50.0 * 6.230498313903809
Epoch 2160, val loss: 1.347115159034729
Epoch 2170, training loss: 311.2360534667969 = 0.022386357188224792 + 50.0 * 6.224273204803467
Epoch 2170, val loss: 1.349343180656433
Epoch 2180, training loss: 311.18505859375 = 0.022046327590942383 + 50.0 * 6.223260402679443
Epoch 2180, val loss: 1.3525867462158203
Epoch 2190, training loss: 311.47039794921875 = 0.021721821278333664 + 50.0 * 6.228973388671875
Epoch 2190, val loss: 1.3552485704421997
Epoch 2200, training loss: 311.1805114746094 = 0.021382467821240425 + 50.0 * 6.223182678222656
Epoch 2200, val loss: 1.3573410511016846
Epoch 2210, training loss: 311.1617126464844 = 0.02106299065053463 + 50.0 * 6.222813129425049
Epoch 2210, val loss: 1.3602269887924194
Epoch 2220, training loss: 311.2669982910156 = 0.02075575478374958 + 50.0 * 6.2249250411987305
Epoch 2220, val loss: 1.363133192062378
Epoch 2230, training loss: 311.21844482421875 = 0.020441431552171707 + 50.0 * 6.223959922790527
Epoch 2230, val loss: 1.3654142618179321
Epoch 2240, training loss: 311.2699279785156 = 0.020137278363108635 + 50.0 * 6.2249956130981445
Epoch 2240, val loss: 1.3679161071777344
Epoch 2250, training loss: 311.146728515625 = 0.019841982051730156 + 50.0 * 6.222537994384766
Epoch 2250, val loss: 1.3702781200408936
Epoch 2260, training loss: 311.09033203125 = 0.01955527439713478 + 50.0 * 6.2214155197143555
Epoch 2260, val loss: 1.3728559017181396
Epoch 2270, training loss: 311.0922546386719 = 0.01928265392780304 + 50.0 * 6.22145938873291
Epoch 2270, val loss: 1.375478982925415
Epoch 2280, training loss: 311.3526916503906 = 0.019015584141016006 + 50.0 * 6.226673603057861
Epoch 2280, val loss: 1.3775004148483276
Epoch 2290, training loss: 311.1651306152344 = 0.018733233213424683 + 50.0 * 6.222928047180176
Epoch 2290, val loss: 1.3802154064178467
Epoch 2300, training loss: 311.19525146484375 = 0.018474740907549858 + 50.0 * 6.223536014556885
Epoch 2300, val loss: 1.3826380968093872
Epoch 2310, training loss: 311.07763671875 = 0.018207887187600136 + 50.0 * 6.221188545227051
Epoch 2310, val loss: 1.3850218057632446
Epoch 2320, training loss: 311.2073669433594 = 0.01795935444533825 + 50.0 * 6.223788261413574
Epoch 2320, val loss: 1.3874852657318115
Epoch 2330, training loss: 311.12139892578125 = 0.017710525542497635 + 50.0 * 6.222074031829834
Epoch 2330, val loss: 1.3896979093551636
Epoch 2340, training loss: 311.1057434082031 = 0.017467888072133064 + 50.0 * 6.221765995025635
Epoch 2340, val loss: 1.3921329975128174
Epoch 2350, training loss: 311.16351318359375 = 0.017232714220881462 + 50.0 * 6.222925186157227
Epoch 2350, val loss: 1.3946483135223389
Epoch 2360, training loss: 311.1668395996094 = 0.017002707347273827 + 50.0 * 6.222996711730957
Epoch 2360, val loss: 1.3969721794128418
Epoch 2370, training loss: 311.029541015625 = 0.016765357926487923 + 50.0 * 6.2202558517456055
Epoch 2370, val loss: 1.3986341953277588
Epoch 2380, training loss: 310.9684143066406 = 0.016545819118618965 + 50.0 * 6.2190375328063965
Epoch 2380, val loss: 1.4013400077819824
Epoch 2390, training loss: 310.97625732421875 = 0.01633182168006897 + 50.0 * 6.219198703765869
Epoch 2390, val loss: 1.4037208557128906
Epoch 2400, training loss: 311.2138977050781 = 0.01612710766494274 + 50.0 * 6.2239556312561035
Epoch 2400, val loss: 1.4060089588165283
Epoch 2410, training loss: 311.206787109375 = 0.015907805413007736 + 50.0 * 6.223817825317383
Epoch 2410, val loss: 1.4078404903411865
Epoch 2420, training loss: 311.09295654296875 = 0.015701036900281906 + 50.0 * 6.221545219421387
Epoch 2420, val loss: 1.4098865985870361
Epoch 2430, training loss: 310.9942626953125 = 0.015493337996304035 + 50.0 * 6.219574928283691
Epoch 2430, val loss: 1.4116458892822266
Epoch 2440, training loss: 310.9710998535156 = 0.015300695784389973 + 50.0 * 6.2191162109375
Epoch 2440, val loss: 1.4141730070114136
Epoch 2450, training loss: 311.1900329589844 = 0.015117238275706768 + 50.0 * 6.223498344421387
Epoch 2450, val loss: 1.41631281375885
Epoch 2460, training loss: 311.0406799316406 = 0.014915390871465206 + 50.0 * 6.220515251159668
Epoch 2460, val loss: 1.4183133840560913
Epoch 2470, training loss: 310.8925476074219 = 0.01472537498921156 + 50.0 * 6.217555999755859
Epoch 2470, val loss: 1.4201067686080933
Epoch 2480, training loss: 310.87799072265625 = 0.01454614382237196 + 50.0 * 6.217268466949463
Epoch 2480, val loss: 1.4222357273101807
Epoch 2490, training loss: 310.9309387207031 = 0.014371723867952824 + 50.0 * 6.218331336975098
Epoch 2490, val loss: 1.4241567850112915
Epoch 2500, training loss: 311.37890625 = 0.01420257892459631 + 50.0 * 6.227294445037842
Epoch 2500, val loss: 1.4256621599197388
Epoch 2510, training loss: 311.046875 = 0.014024368487298489 + 50.0 * 6.220656871795654
Epoch 2510, val loss: 1.428101658821106
Epoch 2520, training loss: 310.8995361328125 = 0.013848107308149338 + 50.0 * 6.217713356018066
Epoch 2520, val loss: 1.429741382598877
Epoch 2530, training loss: 310.9355773925781 = 0.013687043450772762 + 50.0 * 6.218438148498535
Epoch 2530, val loss: 1.4318814277648926
Epoch 2540, training loss: 310.9887390136719 = 0.013525770977139473 + 50.0 * 6.219504356384277
Epoch 2540, val loss: 1.4335881471633911
Epoch 2550, training loss: 310.88818359375 = 0.013365582562983036 + 50.0 * 6.217496395111084
Epoch 2550, val loss: 1.43601655960083
Epoch 2560, training loss: 310.9193420410156 = 0.013210631906986237 + 50.0 * 6.218122482299805
Epoch 2560, val loss: 1.4375578165054321
Epoch 2570, training loss: 310.9903869628906 = 0.013057784177362919 + 50.0 * 6.219546318054199
Epoch 2570, val loss: 1.439134120941162
Epoch 2580, training loss: 310.9808654785156 = 0.012905334122478962 + 50.0 * 6.219359397888184
Epoch 2580, val loss: 1.441213846206665
Epoch 2590, training loss: 310.89825439453125 = 0.012759001925587654 + 50.0 * 6.217710018157959
Epoch 2590, val loss: 1.443162441253662
Epoch 2600, training loss: 310.89599609375 = 0.012612631544470787 + 50.0 * 6.217668056488037
Epoch 2600, val loss: 1.444709300994873
Epoch 2610, training loss: 310.9311828613281 = 0.012474589049816132 + 50.0 * 6.218374252319336
Epoch 2610, val loss: 1.4468410015106201
Epoch 2620, training loss: 310.8255310058594 = 0.012330522760748863 + 50.0 * 6.216264247894287
Epoch 2620, val loss: 1.4483470916748047
Epoch 2630, training loss: 310.8082580566406 = 0.01219470240175724 + 50.0 * 6.215920925140381
Epoch 2630, val loss: 1.4502921104431152
Epoch 2640, training loss: 310.8310852050781 = 0.012064078822731972 + 50.0 * 6.216380596160889
Epoch 2640, val loss: 1.4519134759902954
Epoch 2650, training loss: 311.03570556640625 = 0.011936329305171967 + 50.0 * 6.220475673675537
Epoch 2650, val loss: 1.4533544778823853
Epoch 2660, training loss: 310.8313903808594 = 0.011795645579695702 + 50.0 * 6.216391563415527
Epoch 2660, val loss: 1.45503830909729
Epoch 2670, training loss: 310.7756652832031 = 0.011668250896036625 + 50.0 * 6.215280055999756
Epoch 2670, val loss: 1.456736445426941
Epoch 2680, training loss: 310.9379577636719 = 0.011546541936695576 + 50.0 * 6.2185282707214355
Epoch 2680, val loss: 1.4581985473632812
Epoch 2690, training loss: 310.853515625 = 0.011420496739447117 + 50.0 * 6.216842174530029
Epoch 2690, val loss: 1.4599437713623047
Epoch 2700, training loss: 310.756103515625 = 0.011298983357846737 + 50.0 * 6.214896202087402
Epoch 2700, val loss: 1.461669921875
Epoch 2710, training loss: 310.8298034667969 = 0.011178635992109776 + 50.0 * 6.216372489929199
Epoch 2710, val loss: 1.463099718093872
Epoch 2720, training loss: 310.74761962890625 = 0.011061420664191246 + 50.0 * 6.214731216430664
Epoch 2720, val loss: 1.4647561311721802
Epoch 2730, training loss: 310.7724609375 = 0.010950706899166107 + 50.0 * 6.2152299880981445
Epoch 2730, val loss: 1.4665595293045044
Epoch 2740, training loss: 310.8823547363281 = 0.010838366113603115 + 50.0 * 6.217430591583252
Epoch 2740, val loss: 1.4677598476409912
Epoch 2750, training loss: 310.9837646484375 = 0.01072697527706623 + 50.0 * 6.219460487365723
Epoch 2750, val loss: 1.4695849418640137
Epoch 2760, training loss: 310.75091552734375 = 0.010612266138195992 + 50.0 * 6.214806079864502
Epoch 2760, val loss: 1.4710227251052856
Epoch 2770, training loss: 310.6624450683594 = 0.010506408289074898 + 50.0 * 6.213038921356201
Epoch 2770, val loss: 1.4726942777633667
Epoch 2780, training loss: 310.6578369140625 = 0.010404708795249462 + 50.0 * 6.212948322296143
Epoch 2780, val loss: 1.4740923643112183
Epoch 2790, training loss: 310.89630126953125 = 0.010307319462299347 + 50.0 * 6.217720031738281
Epoch 2790, val loss: 1.475258708000183
Epoch 2800, training loss: 310.7591552734375 = 0.010204365476965904 + 50.0 * 6.21497917175293
Epoch 2800, val loss: 1.477018117904663
Epoch 2810, training loss: 310.6918640136719 = 0.010098420083522797 + 50.0 * 6.213634967803955
Epoch 2810, val loss: 1.4781767129898071
Epoch 2820, training loss: 310.627197265625 = 0.010001118294894695 + 50.0 * 6.212343692779541
Epoch 2820, val loss: 1.4801024198532104
Epoch 2830, training loss: 310.7330627441406 = 0.009911254048347473 + 50.0 * 6.214462757110596
Epoch 2830, val loss: 1.4817872047424316
Epoch 2840, training loss: 310.7037658691406 = 0.00981313269585371 + 50.0 * 6.213878631591797
Epoch 2840, val loss: 1.4828667640686035
Epoch 2850, training loss: 310.6556091308594 = 0.009714861400425434 + 50.0 * 6.212917804718018
Epoch 2850, val loss: 1.4840556383132935
Epoch 2860, training loss: 310.6507568359375 = 0.00962627213448286 + 50.0 * 6.212822914123535
Epoch 2860, val loss: 1.4857673645019531
Epoch 2870, training loss: 310.8492431640625 = 0.00953750591725111 + 50.0 * 6.216794013977051
Epoch 2870, val loss: 1.4872276782989502
Epoch 2880, training loss: 310.6744689941406 = 0.009445883333683014 + 50.0 * 6.2133002281188965
Epoch 2880, val loss: 1.487900972366333
Epoch 2890, training loss: 310.7273864746094 = 0.009357529692351818 + 50.0 * 6.214360237121582
Epoch 2890, val loss: 1.4891204833984375
Epoch 2900, training loss: 310.7106628417969 = 0.009271292015910149 + 50.0 * 6.2140278816223145
Epoch 2900, val loss: 1.4908356666564941
Epoch 2910, training loss: 310.6626892089844 = 0.00918610580265522 + 50.0 * 6.213069915771484
Epoch 2910, val loss: 1.4918256998062134
Epoch 2920, training loss: 310.77105712890625 = 0.00910341739654541 + 50.0 * 6.21523904800415
Epoch 2920, val loss: 1.4931286573410034
Epoch 2930, training loss: 310.5833740234375 = 0.009019367396831512 + 50.0 * 6.21148681640625
Epoch 2930, val loss: 1.4943885803222656
Epoch 2940, training loss: 310.573974609375 = 0.008941172622144222 + 50.0 * 6.211300849914551
Epoch 2940, val loss: 1.495902419090271
Epoch 2950, training loss: 310.546630859375 = 0.008862383663654327 + 50.0 * 6.210755825042725
Epoch 2950, val loss: 1.4971376657485962
Epoch 2960, training loss: 310.67919921875 = 0.008788519538939 + 50.0 * 6.213408470153809
Epoch 2960, val loss: 1.498447299003601
Epoch 2970, training loss: 310.69122314453125 = 0.008712020702660084 + 50.0 * 6.213650703430176
Epoch 2970, val loss: 1.4996148347854614
Epoch 2980, training loss: 310.69580078125 = 0.008633687160909176 + 50.0 * 6.213743209838867
Epoch 2980, val loss: 1.5005491971969604
Epoch 2990, training loss: 310.5481872558594 = 0.008553382009267807 + 50.0 * 6.2107930183410645
Epoch 2990, val loss: 1.5013710260391235
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8107538218239325
=== training gcn model ===
Epoch 0, training loss: 431.79266357421875 = 1.9516680240631104 + 50.0 * 8.596819877624512
Epoch 0, val loss: 1.9581576585769653
Epoch 10, training loss: 431.7379150390625 = 1.9432734251022339 + 50.0 * 8.595892906188965
Epoch 10, val loss: 1.9489524364471436
Epoch 20, training loss: 431.4012756347656 = 1.9328287839889526 + 50.0 * 8.58936882019043
Epoch 20, val loss: 1.9372293949127197
Epoch 30, training loss: 429.07427978515625 = 1.9194682836532593 + 50.0 * 8.543096542358398
Epoch 30, val loss: 1.9220043420791626
Epoch 40, training loss: 415.92987060546875 = 1.9030815362930298 + 50.0 * 8.280535697937012
Epoch 40, val loss: 1.9033472537994385
Epoch 50, training loss: 389.4704284667969 = 1.8842614889144897 + 50.0 * 7.751723766326904
Epoch 50, val loss: 1.8825970888137817
Epoch 60, training loss: 369.0378723144531 = 1.8710293769836426 + 50.0 * 7.343336582183838
Epoch 60, val loss: 1.8695675134658813
Epoch 70, training loss: 354.7738952636719 = 1.860939621925354 + 50.0 * 7.058259010314941
Epoch 70, val loss: 1.8595284223556519
Epoch 80, training loss: 347.3141784667969 = 1.8504501581192017 + 50.0 * 6.909274101257324
Epoch 80, val loss: 1.8493653535842896
Epoch 90, training loss: 342.4518737792969 = 1.8395178318023682 + 50.0 * 6.812247276306152
Epoch 90, val loss: 1.8386247158050537
Epoch 100, training loss: 339.4163513183594 = 1.829240083694458 + 50.0 * 6.751741886138916
Epoch 100, val loss: 1.8284779787063599
Epoch 110, training loss: 337.03619384765625 = 1.819689154624939 + 50.0 * 6.704329967498779
Epoch 110, val loss: 1.8190444707870483
Epoch 120, training loss: 335.14898681640625 = 1.8106454610824585 + 50.0 * 6.666767120361328
Epoch 120, val loss: 1.8101896047592163
Epoch 130, training loss: 333.67816162109375 = 1.801855206489563 + 50.0 * 6.637526512145996
Epoch 130, val loss: 1.8016340732574463
Epoch 140, training loss: 332.4967956542969 = 1.7930039167404175 + 50.0 * 6.614076137542725
Epoch 140, val loss: 1.7931092977523804
Epoch 150, training loss: 331.23590087890625 = 1.7838612794876099 + 50.0 * 6.589040756225586
Epoch 150, val loss: 1.7843812704086304
Epoch 160, training loss: 330.1179504394531 = 1.7743375301361084 + 50.0 * 6.566872596740723
Epoch 160, val loss: 1.775351881980896
Epoch 170, training loss: 329.11175537109375 = 1.764252781867981 + 50.0 * 6.546949863433838
Epoch 170, val loss: 1.7658969163894653
Epoch 180, training loss: 328.4447021484375 = 1.7534537315368652 + 50.0 * 6.533824920654297
Epoch 180, val loss: 1.7558481693267822
Epoch 190, training loss: 327.61590576171875 = 1.741642713546753 + 50.0 * 6.517485618591309
Epoch 190, val loss: 1.7449913024902344
Epoch 200, training loss: 326.8701171875 = 1.7288060188293457 + 50.0 * 6.502825736999512
Epoch 200, val loss: 1.733286738395691
Epoch 210, training loss: 326.2223815917969 = 1.7148773670196533 + 50.0 * 6.490150451660156
Epoch 210, val loss: 1.7206363677978516
Epoch 220, training loss: 325.8839416503906 = 1.699695348739624 + 50.0 * 6.483684539794922
Epoch 220, val loss: 1.7068710327148438
Epoch 230, training loss: 325.3229675292969 = 1.683104157447815 + 50.0 * 6.472797393798828
Epoch 230, val loss: 1.6919351816177368
Epoch 240, training loss: 324.75054931640625 = 1.6650891304016113 + 50.0 * 6.461709022521973
Epoch 240, val loss: 1.675872802734375
Epoch 250, training loss: 324.2753601074219 = 1.6456695795059204 + 50.0 * 6.452593803405762
Epoch 250, val loss: 1.6585294008255005
Epoch 260, training loss: 324.078857421875 = 1.6248557567596436 + 50.0 * 6.449079990386963
Epoch 260, val loss: 1.6399595737457275
Epoch 270, training loss: 323.7259216308594 = 1.6024069786071777 + 50.0 * 6.442470073699951
Epoch 270, val loss: 1.6201884746551514
Epoch 280, training loss: 323.1307373046875 = 1.5784727334976196 + 50.0 * 6.4310455322265625
Epoch 280, val loss: 1.5990322828292847
Epoch 290, training loss: 322.7232666015625 = 1.5530868768692017 + 50.0 * 6.423403263092041
Epoch 290, val loss: 1.5768142938613892
Epoch 300, training loss: 322.478515625 = 1.5263116359710693 + 50.0 * 6.419044494628906
Epoch 300, val loss: 1.5535640716552734
Epoch 310, training loss: 322.14453125 = 1.4980977773666382 + 50.0 * 6.412928581237793
Epoch 310, val loss: 1.52925705909729
Epoch 320, training loss: 321.7483825683594 = 1.4687496423721313 + 50.0 * 6.405592918395996
Epoch 320, val loss: 1.5041775703430176
Epoch 330, training loss: 321.45513916015625 = 1.4384244680404663 + 50.0 * 6.400334358215332
Epoch 330, val loss: 1.4785540103912354
Epoch 340, training loss: 321.1810302734375 = 1.4071789979934692 + 50.0 * 6.395477294921875
Epoch 340, val loss: 1.4525349140167236
Epoch 350, training loss: 320.8987121582031 = 1.3752888441085815 + 50.0 * 6.390468597412109
Epoch 350, val loss: 1.4262293577194214
Epoch 360, training loss: 320.6085510253906 = 1.342950463294983 + 50.0 * 6.385312080383301
Epoch 360, val loss: 1.399972677230835
Epoch 370, training loss: 320.4991455078125 = 1.3103959560394287 + 50.0 * 6.383775234222412
Epoch 370, val loss: 1.3739492893218994
Epoch 380, training loss: 320.3226623535156 = 1.2776139974594116 + 50.0 * 6.380900859832764
Epoch 380, val loss: 1.3479660749435425
Epoch 390, training loss: 319.94854736328125 = 1.2450282573699951 + 50.0 * 6.374070167541504
Epoch 390, val loss: 1.3226791620254517
Epoch 400, training loss: 319.6759948730469 = 1.2127162218093872 + 50.0 * 6.369266033172607
Epoch 400, val loss: 1.2980492115020752
Epoch 410, training loss: 319.4515075683594 = 1.1808964014053345 + 50.0 * 6.36541223526001
Epoch 410, val loss: 1.2741862535476685
Epoch 420, training loss: 319.23089599609375 = 1.14968740940094 + 50.0 * 6.361624240875244
Epoch 420, val loss: 1.251221776008606
Epoch 430, training loss: 319.0895690917969 = 1.1190969944000244 + 50.0 * 6.359409332275391
Epoch 430, val loss: 1.2293776273727417
Epoch 440, training loss: 319.11993408203125 = 1.0891611576080322 + 50.0 * 6.360615253448486
Epoch 440, val loss: 1.2081342935562134
Epoch 450, training loss: 318.77838134765625 = 1.0601521730422974 + 50.0 * 6.354364395141602
Epoch 450, val loss: 1.1882470846176147
Epoch 460, training loss: 318.5019836425781 = 1.0320321321487427 + 50.0 * 6.349398612976074
Epoch 460, val loss: 1.1694018840789795
Epoch 470, training loss: 318.34197998046875 = 1.0048364400863647 + 50.0 * 6.346743106842041
Epoch 470, val loss: 1.1515510082244873
Epoch 480, training loss: 318.1685791015625 = 0.9783293604850769 + 50.0 * 6.343804836273193
Epoch 480, val loss: 1.1347367763519287
Epoch 490, training loss: 318.0091857910156 = 0.9526646733283997 + 50.0 * 6.341130256652832
Epoch 490, val loss: 1.1188336610794067
Epoch 500, training loss: 317.8496398925781 = 0.9278683066368103 + 50.0 * 6.338435649871826
Epoch 500, val loss: 1.1038947105407715
Epoch 510, training loss: 318.0699462890625 = 0.9037989377975464 + 50.0 * 6.34332275390625
Epoch 510, val loss: 1.089823603630066
Epoch 520, training loss: 317.5920715332031 = 0.8802945017814636 + 50.0 * 6.334236145019531
Epoch 520, val loss: 1.0764418840408325
Epoch 530, training loss: 317.48773193359375 = 0.8575620055198669 + 50.0 * 6.3326029777526855
Epoch 530, val loss: 1.0639079809188843
Epoch 540, training loss: 317.3336181640625 = 0.8354505300521851 + 50.0 * 6.329963207244873
Epoch 540, val loss: 1.0520904064178467
Epoch 550, training loss: 317.2000427246094 = 0.8139055371284485 + 50.0 * 6.327722549438477
Epoch 550, val loss: 1.0409876108169556
Epoch 560, training loss: 317.094970703125 = 0.7929143905639648 + 50.0 * 6.326041221618652
Epoch 560, val loss: 1.0305732488632202
Epoch 570, training loss: 316.9081115722656 = 0.7724475860595703 + 50.0 * 6.3227128982543945
Epoch 570, val loss: 1.0205529928207397
Epoch 580, training loss: 317.41595458984375 = 0.7524471879005432 + 50.0 * 6.333270072937012
Epoch 580, val loss: 1.0112303495407104
Epoch 590, training loss: 316.65313720703125 = 0.7328054308891296 + 50.0 * 6.318406105041504
Epoch 590, val loss: 1.0026206970214844
Epoch 600, training loss: 316.5593566894531 = 0.7137278318405151 + 50.0 * 6.316912651062012
Epoch 600, val loss: 0.9945406317710876
Epoch 610, training loss: 316.50518798828125 = 0.6951329708099365 + 50.0 * 6.316201210021973
Epoch 610, val loss: 0.9870896935462952
Epoch 620, training loss: 316.36883544921875 = 0.6768642067909241 + 50.0 * 6.313839912414551
Epoch 620, val loss: 0.9798753261566162
Epoch 630, training loss: 316.3926086425781 = 0.658980131149292 + 50.0 * 6.314672946929932
Epoch 630, val loss: 0.9733049273490906
Epoch 640, training loss: 316.22015380859375 = 0.6415287852287292 + 50.0 * 6.311572551727295
Epoch 640, val loss: 0.9672574400901794
Epoch 650, training loss: 316.0959167480469 = 0.6244792342185974 + 50.0 * 6.309428691864014
Epoch 650, val loss: 0.9617490172386169
Epoch 660, training loss: 315.97369384765625 = 0.6077725887298584 + 50.0 * 6.307318687438965
Epoch 660, val loss: 0.9565607309341431
Epoch 670, training loss: 315.9647521972656 = 0.5914011597633362 + 50.0 * 6.307466983795166
Epoch 670, val loss: 0.9518150091171265
Epoch 680, training loss: 315.97235107421875 = 0.5752851963043213 + 50.0 * 6.307941436767578
Epoch 680, val loss: 0.9475881457328796
Epoch 690, training loss: 315.692626953125 = 0.5596088171005249 + 50.0 * 6.3026604652404785
Epoch 690, val loss: 0.9440608024597168
Epoch 700, training loss: 315.5995178222656 = 0.5443729162216187 + 50.0 * 6.301102638244629
Epoch 700, val loss: 0.9408760070800781
Epoch 710, training loss: 315.5335998535156 = 0.5295060276985168 + 50.0 * 6.300081729888916
Epoch 710, val loss: 0.9382089972496033
Epoch 720, training loss: 315.6066589355469 = 0.5149610638618469 + 50.0 * 6.3018341064453125
Epoch 720, val loss: 0.9358218312263489
Epoch 730, training loss: 315.3687438964844 = 0.5007632970809937 + 50.0 * 6.297359466552734
Epoch 730, val loss: 0.9338831305503845
Epoch 740, training loss: 315.4580383300781 = 0.4869460463523865 + 50.0 * 6.299422264099121
Epoch 740, val loss: 0.9322196841239929
Epoch 750, training loss: 315.2391357421875 = 0.47347143292427063 + 50.0 * 6.295312881469727
Epoch 750, val loss: 0.9308872818946838
Epoch 760, training loss: 315.24945068359375 = 0.4603431820869446 + 50.0 * 6.295782089233398
Epoch 760, val loss: 0.9299613237380981
Epoch 770, training loss: 315.14007568359375 = 0.4475088119506836 + 50.0 * 6.293851375579834
Epoch 770, val loss: 0.929685652256012
Epoch 780, training loss: 315.05584716796875 = 0.43502387404441833 + 50.0 * 6.292416095733643
Epoch 780, val loss: 0.9293261170387268
Epoch 790, training loss: 315.4296875 = 0.4228432774543762 + 50.0 * 6.300136566162109
Epoch 790, val loss: 0.9291818737983704
Epoch 800, training loss: 314.98492431640625 = 0.41090965270996094 + 50.0 * 6.29148006439209
Epoch 800, val loss: 0.9294732809066772
Epoch 810, training loss: 314.8060607910156 = 0.39933228492736816 + 50.0 * 6.288135051727295
Epoch 810, val loss: 0.9302471876144409
Epoch 820, training loss: 314.7458190917969 = 0.3881302773952484 + 50.0 * 6.287154197692871
Epoch 820, val loss: 0.9311050772666931
Epoch 830, training loss: 314.9196472167969 = 0.37722599506378174 + 50.0 * 6.290848255157471
Epoch 830, val loss: 0.9324502944946289
Epoch 840, training loss: 314.95941162109375 = 0.3665279448032379 + 50.0 * 6.291857719421387
Epoch 840, val loss: 0.9333560466766357
Epoch 850, training loss: 314.593994140625 = 0.35609087347984314 + 50.0 * 6.2847580909729
Epoch 850, val loss: 0.9350847601890564
Epoch 860, training loss: 314.5241394042969 = 0.3460138440132141 + 50.0 * 6.283562660217285
Epoch 860, val loss: 0.9368712306022644
Epoch 870, training loss: 314.4393615722656 = 0.33625903725624084 + 50.0 * 6.28206205368042
Epoch 870, val loss: 0.938916027545929
Epoch 880, training loss: 314.79656982421875 = 0.3267790973186493 + 50.0 * 6.289395809173584
Epoch 880, val loss: 0.9415517449378967
Epoch 890, training loss: 314.6357116699219 = 0.3174387514591217 + 50.0 * 6.286365509033203
Epoch 890, val loss: 0.9433098435401917
Epoch 900, training loss: 314.30523681640625 = 0.3083887994289398 + 50.0 * 6.279937267303467
Epoch 900, val loss: 0.9457281231880188
Epoch 910, training loss: 314.2187194824219 = 0.29967552423477173 + 50.0 * 6.278380870819092
Epoch 910, val loss: 0.9486330151557922
Epoch 920, training loss: 314.296630859375 = 0.29121845960617065 + 50.0 * 6.2801079750061035
Epoch 920, val loss: 0.9516304135322571
Epoch 930, training loss: 314.2637634277344 = 0.2829349935054779 + 50.0 * 6.279616832733154
Epoch 930, val loss: 0.954681396484375
Epoch 940, training loss: 314.18597412109375 = 0.2748606503009796 + 50.0 * 6.27822208404541
Epoch 940, val loss: 0.9575545787811279
Epoch 950, training loss: 314.0563659667969 = 0.2670550048351288 + 50.0 * 6.27578592300415
Epoch 950, val loss: 0.9608737230300903
Epoch 960, training loss: 314.087158203125 = 0.25949740409851074 + 50.0 * 6.276553153991699
Epoch 960, val loss: 0.9644609093666077
Epoch 970, training loss: 314.0624694824219 = 0.2521098852157593 + 50.0 * 6.276206970214844
Epoch 970, val loss: 0.9680666327476501
Epoch 980, training loss: 313.9273376464844 = 0.24489696323871613 + 50.0 * 6.273648738861084
Epoch 980, val loss: 0.9716187715530396
Epoch 990, training loss: 313.9010314941406 = 0.23794688284397125 + 50.0 * 6.273262023925781
Epoch 990, val loss: 0.9756792783737183
Epoch 1000, training loss: 313.99609375 = 0.2311960607767105 + 50.0 * 6.275298118591309
Epoch 1000, val loss: 0.979275643825531
Epoch 1010, training loss: 313.78778076171875 = 0.22460025548934937 + 50.0 * 6.271263599395752
Epoch 1010, val loss: 0.9834079146385193
Epoch 1020, training loss: 313.74493408203125 = 0.21824821829795837 + 50.0 * 6.270533561706543
Epoch 1020, val loss: 0.9874451756477356
Epoch 1030, training loss: 314.14019775390625 = 0.2120777815580368 + 50.0 * 6.278562545776367
Epoch 1030, val loss: 0.9913747310638428
Epoch 1040, training loss: 313.7062072753906 = 0.20600511133670807 + 50.0 * 6.2700042724609375
Epoch 1040, val loss: 0.9960358738899231
Epoch 1050, training loss: 313.6074523925781 = 0.2001790553331375 + 50.0 * 6.268145561218262
Epoch 1050, val loss: 1.0004197359085083
Epoch 1060, training loss: 313.6033020019531 = 0.19455818831920624 + 50.0 * 6.26817512512207
Epoch 1060, val loss: 1.0048961639404297
Epoch 1070, training loss: 313.8974914550781 = 0.1890927255153656 + 50.0 * 6.274168014526367
Epoch 1070, val loss: 1.0096313953399658
Epoch 1080, training loss: 313.5640869140625 = 0.18375195562839508 + 50.0 * 6.267606735229492
Epoch 1080, val loss: 1.014122486114502
Epoch 1090, training loss: 313.5388488769531 = 0.1785915344953537 + 50.0 * 6.267205238342285
Epoch 1090, val loss: 1.0189317464828491
Epoch 1100, training loss: 313.52154541015625 = 0.1735851913690567 + 50.0 * 6.266959190368652
Epoch 1100, val loss: 1.023762822151184
Epoch 1110, training loss: 313.4434814453125 = 0.1687438040971756 + 50.0 * 6.2654948234558105
Epoch 1110, val loss: 1.0284779071807861
Epoch 1120, training loss: 313.54107666015625 = 0.16405314207077026 + 50.0 * 6.267539978027344
Epoch 1120, val loss: 1.0337653160095215
Epoch 1130, training loss: 313.5135498046875 = 0.1594768762588501 + 50.0 * 6.267081260681152
Epoch 1130, val loss: 1.0383650064468384
Epoch 1140, training loss: 313.3838195800781 = 0.1550227850675583 + 50.0 * 6.264575958251953
Epoch 1140, val loss: 1.0437915325164795
Epoch 1150, training loss: 313.4137878417969 = 0.15075033903121948 + 50.0 * 6.265260696411133
Epoch 1150, val loss: 1.0490753650665283
Epoch 1160, training loss: 313.2255859375 = 0.14658045768737793 + 50.0 * 6.261579990386963
Epoch 1160, val loss: 1.0539137125015259
Epoch 1170, training loss: 313.34222412109375 = 0.14255326986312866 + 50.0 * 6.263993263244629
Epoch 1170, val loss: 1.0593674182891846
Epoch 1180, training loss: 313.19976806640625 = 0.13864965736865997 + 50.0 * 6.2612223625183105
Epoch 1180, val loss: 1.0648335218429565
Epoch 1190, training loss: 313.1630554199219 = 0.1348678320646286 + 50.0 * 6.260563850402832
Epoch 1190, val loss: 1.0701202154159546
Epoch 1200, training loss: 313.2668762207031 = 0.13120664656162262 + 50.0 * 6.26271390914917
Epoch 1200, val loss: 1.0757540464401245
Epoch 1210, training loss: 313.2015075683594 = 0.12763214111328125 + 50.0 * 6.261477947235107
Epoch 1210, val loss: 1.0806729793548584
Epoch 1220, training loss: 313.23785400390625 = 0.12418176233768463 + 50.0 * 6.262273788452148
Epoch 1220, val loss: 1.0863006114959717
Epoch 1230, training loss: 313.06494140625 = 0.12082847207784653 + 50.0 * 6.258882522583008
Epoch 1230, val loss: 1.0917567014694214
Epoch 1240, training loss: 313.0120544433594 = 0.11760928481817245 + 50.0 * 6.2578887939453125
Epoch 1240, val loss: 1.0973448753356934
Epoch 1250, training loss: 313.1942138671875 = 0.11448662728071213 + 50.0 * 6.261594772338867
Epoch 1250, val loss: 1.1028484106063843
Epoch 1260, training loss: 313.21136474609375 = 0.11143242567777634 + 50.0 * 6.261999130249023
Epoch 1260, val loss: 1.1087566614151
Epoch 1270, training loss: 312.97723388671875 = 0.10847268998622894 + 50.0 * 6.257375240325928
Epoch 1270, val loss: 1.1139973402023315
Epoch 1280, training loss: 312.8565368652344 = 0.10562210530042648 + 50.0 * 6.25501823425293
Epoch 1280, val loss: 1.1198982000350952
Epoch 1290, training loss: 312.8674621582031 = 0.10288884490728378 + 50.0 * 6.25529146194458
Epoch 1290, val loss: 1.1256335973739624
Epoch 1300, training loss: 313.1195983886719 = 0.10022531449794769 + 50.0 * 6.260387420654297
Epoch 1300, val loss: 1.1312050819396973
Epoch 1310, training loss: 312.9638366699219 = 0.09762473404407501 + 50.0 * 6.25732421875
Epoch 1310, val loss: 1.1368589401245117
Epoch 1320, training loss: 312.7837829589844 = 0.09511152654886246 + 50.0 * 6.253773212432861
Epoch 1320, val loss: 1.1426409482955933
Epoch 1330, training loss: 312.783935546875 = 0.09270250052213669 + 50.0 * 6.253824710845947
Epoch 1330, val loss: 1.1482892036437988
Epoch 1340, training loss: 312.9294738769531 = 0.09035799652338028 + 50.0 * 6.256782531738281
Epoch 1340, val loss: 1.1540716886520386
Epoch 1350, training loss: 312.8439636230469 = 0.08807236701250076 + 50.0 * 6.255117893218994
Epoch 1350, val loss: 1.160084843635559
Epoch 1360, training loss: 312.80316162109375 = 0.08586137741804123 + 50.0 * 6.254345893859863
Epoch 1360, val loss: 1.1653867959976196
Epoch 1370, training loss: 312.78369140625 = 0.08372326195240021 + 50.0 * 6.253999710083008
Epoch 1370, val loss: 1.1714086532592773
Epoch 1380, training loss: 312.6240234375 = 0.08163435012102127 + 50.0 * 6.250847816467285
Epoch 1380, val loss: 1.1770641803741455
Epoch 1390, training loss: 312.6645812988281 = 0.07964258641004562 + 50.0 * 6.2516984939575195
Epoch 1390, val loss: 1.1828582286834717
Epoch 1400, training loss: 312.8504638671875 = 0.07770194113254547 + 50.0 * 6.255455493927002
Epoch 1400, val loss: 1.1881935596466064
Epoch 1410, training loss: 312.7333984375 = 0.07581017911434174 + 50.0 * 6.253151893615723
Epoch 1410, val loss: 1.1945511102676392
Epoch 1420, training loss: 312.59661865234375 = 0.07396702468395233 + 50.0 * 6.250452995300293
Epoch 1420, val loss: 1.19979989528656
Epoch 1430, training loss: 312.7671203613281 = 0.07220932096242905 + 50.0 * 6.2538981437683105
Epoch 1430, val loss: 1.2057478427886963
Epoch 1440, training loss: 312.56561279296875 = 0.07047196477651596 + 50.0 * 6.249903202056885
Epoch 1440, val loss: 1.2117146253585815
Epoch 1450, training loss: 312.4979248046875 = 0.06880936771631241 + 50.0 * 6.248581886291504
Epoch 1450, val loss: 1.217287302017212
Epoch 1460, training loss: 312.55059814453125 = 0.06720103323459625 + 50.0 * 6.249668121337891
Epoch 1460, val loss: 1.2229883670806885
Epoch 1470, training loss: 312.5202331542969 = 0.06563747674226761 + 50.0 * 6.249091625213623
Epoch 1470, val loss: 1.229080080986023
Epoch 1480, training loss: 312.5622253417969 = 0.0641137957572937 + 50.0 * 6.249962329864502
Epoch 1480, val loss: 1.2340617179870605
Epoch 1490, training loss: 312.7089538574219 = 0.06264317780733109 + 50.0 * 6.252925872802734
Epoch 1490, val loss: 1.239682674407959
Epoch 1500, training loss: 312.3602600097656 = 0.06118481233716011 + 50.0 * 6.245981693267822
Epoch 1500, val loss: 1.2457128763198853
Epoch 1510, training loss: 312.3037109375 = 0.05980519577860832 + 50.0 * 6.244877815246582
Epoch 1510, val loss: 1.2513047456741333
Epoch 1520, training loss: 312.288330078125 = 0.058476220816373825 + 50.0 * 6.2445969581604
Epoch 1520, val loss: 1.2573074102401733
Epoch 1530, training loss: 312.7626953125 = 0.05718797445297241 + 50.0 * 6.254110336303711
Epoch 1530, val loss: 1.2626731395721436
Epoch 1540, training loss: 312.5647888183594 = 0.05589789152145386 + 50.0 * 6.250177383422852
Epoch 1540, val loss: 1.268141746520996
Epoch 1550, training loss: 312.2412109375 = 0.05464916303753853 + 50.0 * 6.2437310218811035
Epoch 1550, val loss: 1.2736732959747314
Epoch 1560, training loss: 312.2206115722656 = 0.05346358194947243 + 50.0 * 6.243342876434326
Epoch 1560, val loss: 1.2795766592025757
Epoch 1570, training loss: 312.4679870605469 = 0.05231158807873726 + 50.0 * 6.2483134269714355
Epoch 1570, val loss: 1.284965991973877
Epoch 1580, training loss: 312.2773132324219 = 0.05118035897612572 + 50.0 * 6.244522571563721
Epoch 1580, val loss: 1.2903006076812744
Epoch 1590, training loss: 312.1892395019531 = 0.05007714033126831 + 50.0 * 6.242783069610596
Epoch 1590, val loss: 1.2960588932037354
Epoch 1600, training loss: 312.4232482910156 = 0.04902724176645279 + 50.0 * 6.24748420715332
Epoch 1600, val loss: 1.3015718460083008
Epoch 1610, training loss: 312.1219177246094 = 0.0479859821498394 + 50.0 * 6.24147891998291
Epoch 1610, val loss: 1.3067307472229004
Epoch 1620, training loss: 312.124267578125 = 0.04698188975453377 + 50.0 * 6.241545677185059
Epoch 1620, val loss: 1.3124912977218628
Epoch 1630, training loss: 312.3747863769531 = 0.04601668566465378 + 50.0 * 6.246575355529785
Epoch 1630, val loss: 1.3179903030395508
Epoch 1640, training loss: 312.1365051269531 = 0.04506031051278114 + 50.0 * 6.241828918457031
Epoch 1640, val loss: 1.3228845596313477
Epoch 1650, training loss: 312.1358642578125 = 0.04414105415344238 + 50.0 * 6.24183464050293
Epoch 1650, val loss: 1.3286592960357666
Epoch 1660, training loss: 312.18408203125 = 0.04324870556592941 + 50.0 * 6.242816925048828
Epoch 1660, val loss: 1.3337196111679077
Epoch 1670, training loss: 312.2293701171875 = 0.042380254715681076 + 50.0 * 6.243739604949951
Epoch 1670, val loss: 1.3390909433364868
Epoch 1680, training loss: 312.10791015625 = 0.04152033105492592 + 50.0 * 6.24132776260376
Epoch 1680, val loss: 1.3446722030639648
Epoch 1690, training loss: 311.99267578125 = 0.040695108473300934 + 50.0 * 6.239039897918701
Epoch 1690, val loss: 1.3496904373168945
Epoch 1700, training loss: 312.007080078125 = 0.03989824280142784 + 50.0 * 6.239344120025635
Epoch 1700, val loss: 1.355244517326355
Epoch 1710, training loss: 312.2400207519531 = 0.039122842252254486 + 50.0 * 6.244018077850342
Epoch 1710, val loss: 1.3600187301635742
Epoch 1720, training loss: 312.07928466796875 = 0.03835860267281532 + 50.0 * 6.240818500518799
Epoch 1720, val loss: 1.3655208349227905
Epoch 1730, training loss: 311.9625244140625 = 0.037617407739162445 + 50.0 * 6.238498210906982
Epoch 1730, val loss: 1.3704670667648315
Epoch 1740, training loss: 312.0858154296875 = 0.03691258281469345 + 50.0 * 6.240977764129639
Epoch 1740, val loss: 1.3756669759750366
Epoch 1750, training loss: 311.9751281738281 = 0.03620424121618271 + 50.0 * 6.238778591156006
Epoch 1750, val loss: 1.3809813261032104
Epoch 1760, training loss: 311.9703063964844 = 0.03551901504397392 + 50.0 * 6.2386956214904785
Epoch 1760, val loss: 1.3863855600357056
Epoch 1770, training loss: 311.89208984375 = 0.03485248237848282 + 50.0 * 6.237144947052002
Epoch 1770, val loss: 1.3912476301193237
Epoch 1780, training loss: 311.9823913574219 = 0.03421534597873688 + 50.0 * 6.238963603973389
Epoch 1780, val loss: 1.3965044021606445
Epoch 1790, training loss: 312.1701965332031 = 0.03358347713947296 + 50.0 * 6.242732524871826
Epoch 1790, val loss: 1.4014095067977905
Epoch 1800, training loss: 312.0155334472656 = 0.03296015039086342 + 50.0 * 6.239651679992676
Epoch 1800, val loss: 1.4061918258666992
Epoch 1810, training loss: 311.8818359375 = 0.03235888108611107 + 50.0 * 6.236989974975586
Epoch 1810, val loss: 1.411422610282898
Epoch 1820, training loss: 311.8345947265625 = 0.03178343549370766 + 50.0 * 6.236056327819824
Epoch 1820, val loss: 1.4166052341461182
Epoch 1830, training loss: 311.9923400878906 = 0.0312187597155571 + 50.0 * 6.239222526550293
Epoch 1830, val loss: 1.4215762615203857
Epoch 1840, training loss: 312.0904541015625 = 0.030664557591080666 + 50.0 * 6.2411956787109375
Epoch 1840, val loss: 1.426182508468628
Epoch 1850, training loss: 311.8034362792969 = 0.03010481595993042 + 50.0 * 6.235466957092285
Epoch 1850, val loss: 1.4314841032028198
Epoch 1860, training loss: 311.7674865722656 = 0.029578834772109985 + 50.0 * 6.234758377075195
Epoch 1860, val loss: 1.4360113143920898
Epoch 1870, training loss: 311.7799377441406 = 0.029070179909467697 + 50.0 * 6.235016822814941
Epoch 1870, val loss: 1.4413684606552124
Epoch 1880, training loss: 311.8852844238281 = 0.028569402173161507 + 50.0 * 6.2371344566345215
Epoch 1880, val loss: 1.445966124534607
Epoch 1890, training loss: 311.834716796875 = 0.02807893604040146 + 50.0 * 6.236133098602295
Epoch 1890, val loss: 1.450453281402588
Epoch 1900, training loss: 311.8598937988281 = 0.02759435959160328 + 50.0 * 6.2366461753845215
Epoch 1900, val loss: 1.455361008644104
Epoch 1910, training loss: 311.7223815917969 = 0.027132853865623474 + 50.0 * 6.233904838562012
Epoch 1910, val loss: 1.4600579738616943
Epoch 1920, training loss: 311.767333984375 = 0.02667892724275589 + 50.0 * 6.234813213348389
Epoch 1920, val loss: 1.465080738067627
Epoch 1930, training loss: 311.7129211425781 = 0.026234911754727364 + 50.0 * 6.233733654022217
Epoch 1930, val loss: 1.4696530103683472
Epoch 1940, training loss: 311.6331787109375 = 0.02579675056040287 + 50.0 * 6.232147693634033
Epoch 1940, val loss: 1.4738341569900513
Epoch 1950, training loss: 311.71197509765625 = 0.025378769263625145 + 50.0 * 6.233731746673584
Epoch 1950, val loss: 1.4786813259124756
Epoch 1960, training loss: 311.7637939453125 = 0.024967405945062637 + 50.0 * 6.234776496887207
Epoch 1960, val loss: 1.4832357168197632
Epoch 1970, training loss: 311.670654296875 = 0.0245543010532856 + 50.0 * 6.232921600341797
Epoch 1970, val loss: 1.4877575635910034
Epoch 1980, training loss: 311.7330017089844 = 0.0241664070636034 + 50.0 * 6.2341766357421875
Epoch 1980, val loss: 1.492211937904358
Epoch 1990, training loss: 311.6266784667969 = 0.023775344714522362 + 50.0 * 6.232058048248291
Epoch 1990, val loss: 1.4967228174209595
Epoch 2000, training loss: 311.6932678222656 = 0.023402241989970207 + 50.0 * 6.233397006988525
Epoch 2000, val loss: 1.5014634132385254
Epoch 2010, training loss: 311.6700134277344 = 0.023032069206237793 + 50.0 * 6.232939720153809
Epoch 2010, val loss: 1.5057964324951172
Epoch 2020, training loss: 311.5614318847656 = 0.022669680416584015 + 50.0 * 6.230775356292725
Epoch 2020, val loss: 1.51032292842865
Epoch 2030, training loss: 311.7168273925781 = 0.022324230521917343 + 50.0 * 6.233889579772949
Epoch 2030, val loss: 1.5145957469940186
Epoch 2040, training loss: 311.64794921875 = 0.02198241651058197 + 50.0 * 6.232519626617432
Epoch 2040, val loss: 1.5189447402954102
Epoch 2050, training loss: 311.54132080078125 = 0.02163602039217949 + 50.0 * 6.230393409729004
Epoch 2050, val loss: 1.5229698419570923
Epoch 2060, training loss: 311.56109619140625 = 0.021311920136213303 + 50.0 * 6.230795860290527
Epoch 2060, val loss: 1.5273183584213257
Epoch 2070, training loss: 311.6363830566406 = 0.02098879963159561 + 50.0 * 6.2323079109191895
Epoch 2070, val loss: 1.5312656164169312
Epoch 2080, training loss: 311.5867919921875 = 0.020673280581831932 + 50.0 * 6.231322765350342
Epoch 2080, val loss: 1.5363754034042358
Epoch 2090, training loss: 311.61981201171875 = 0.020367661491036415 + 50.0 * 6.231988906860352
Epoch 2090, val loss: 1.53993821144104
Epoch 2100, training loss: 311.4705505371094 = 0.020061587914824486 + 50.0 * 6.229010105133057
Epoch 2100, val loss: 1.5446102619171143
Epoch 2110, training loss: 311.5662841796875 = 0.019766520708799362 + 50.0 * 6.230930328369141
Epoch 2110, val loss: 1.5487061738967896
Epoch 2120, training loss: 311.6003112792969 = 0.019480202347040176 + 50.0 * 6.231616973876953
Epoch 2120, val loss: 1.552976369857788
Epoch 2130, training loss: 311.5638427734375 = 0.0191950686275959 + 50.0 * 6.230892658233643
Epoch 2130, val loss: 1.556858777999878
Epoch 2140, training loss: 311.4362487792969 = 0.01892109215259552 + 50.0 * 6.228346824645996
Epoch 2140, val loss: 1.5609219074249268
Epoch 2150, training loss: 311.4046936035156 = 0.018651016056537628 + 50.0 * 6.227720737457275
Epoch 2150, val loss: 1.565385103225708
Epoch 2160, training loss: 311.56146240234375 = 0.018391137942671776 + 50.0 * 6.230861186981201
Epoch 2160, val loss: 1.5697141885757446
Epoch 2170, training loss: 311.5743103027344 = 0.018134254962205887 + 50.0 * 6.231123447418213
Epoch 2170, val loss: 1.5729087591171265
Epoch 2180, training loss: 311.603271484375 = 0.017878498882055283 + 50.0 * 6.231707572937012
Epoch 2180, val loss: 1.5772558450698853
Epoch 2190, training loss: 311.39324951171875 = 0.017623260617256165 + 50.0 * 6.227512836456299
Epoch 2190, val loss: 1.5807018280029297
Epoch 2200, training loss: 311.3727111816406 = 0.01738063432276249 + 50.0 * 6.227106094360352
Epoch 2200, val loss: 1.58491849899292
Epoch 2210, training loss: 311.4081726074219 = 0.01714613102376461 + 50.0 * 6.22782039642334
Epoch 2210, val loss: 1.5888172388076782
Epoch 2220, training loss: 311.5623779296875 = 0.016917888075113297 + 50.0 * 6.23090934753418
Epoch 2220, val loss: 1.592814564704895
Epoch 2230, training loss: 311.4976501464844 = 0.016680298373103142 + 50.0 * 6.229619026184082
Epoch 2230, val loss: 1.596657633781433
Epoch 2240, training loss: 311.36322021484375 = 0.016453813761472702 + 50.0 * 6.226935386657715
Epoch 2240, val loss: 1.6001707315444946
Epoch 2250, training loss: 311.27764892578125 = 0.016234250739216805 + 50.0 * 6.225228309631348
Epoch 2250, val loss: 1.604337453842163
Epoch 2260, training loss: 311.4002380371094 = 0.01602628268301487 + 50.0 * 6.227684497833252
Epoch 2260, val loss: 1.6082963943481445
Epoch 2270, training loss: 311.3537902832031 = 0.015809910371899605 + 50.0 * 6.226759433746338
Epoch 2270, val loss: 1.6116172075271606
Epoch 2280, training loss: 311.3639221191406 = 0.015604621730744839 + 50.0 * 6.226966857910156
Epoch 2280, val loss: 1.6155091524124146
Epoch 2290, training loss: 311.45294189453125 = 0.015402221120893955 + 50.0 * 6.228750705718994
Epoch 2290, val loss: 1.619144082069397
Epoch 2300, training loss: 311.3085632324219 = 0.015200935304164886 + 50.0 * 6.22586727142334
Epoch 2300, val loss: 1.6226482391357422
Epoch 2310, training loss: 311.2566223144531 = 0.015004890039563179 + 50.0 * 6.224832057952881
Epoch 2310, val loss: 1.62625253200531
Epoch 2320, training loss: 311.25311279296875 = 0.014815484173595905 + 50.0 * 6.224766254425049
Epoch 2320, val loss: 1.6301062107086182
Epoch 2330, training loss: 311.33984375 = 0.014630925841629505 + 50.0 * 6.226504802703857
Epoch 2330, val loss: 1.6334642171859741
Epoch 2340, training loss: 311.5997314453125 = 0.014448811300098896 + 50.0 * 6.231706142425537
Epoch 2340, val loss: 1.6367870569229126
Epoch 2350, training loss: 311.3391418457031 = 0.014262569136917591 + 50.0 * 6.226497650146484
Epoch 2350, val loss: 1.6410630941390991
Epoch 2360, training loss: 311.2044372558594 = 0.014081683941185474 + 50.0 * 6.223807334899902
Epoch 2360, val loss: 1.6441775560379028
Epoch 2370, training loss: 311.1572570800781 = 0.013912388123571873 + 50.0 * 6.222866535186768
Epoch 2370, val loss: 1.6480917930603027
Epoch 2380, training loss: 311.4440612792969 = 0.013749832287430763 + 50.0 * 6.2286057472229
Epoch 2380, val loss: 1.6512653827667236
Epoch 2390, training loss: 311.1492004394531 = 0.013573886826634407 + 50.0 * 6.222712516784668
Epoch 2390, val loss: 1.6547119617462158
Epoch 2400, training loss: 311.2050476074219 = 0.01341125089675188 + 50.0 * 6.223833084106445
Epoch 2400, val loss: 1.658179521560669
Epoch 2410, training loss: 311.32244873046875 = 0.01325265783816576 + 50.0 * 6.226184368133545
Epoch 2410, val loss: 1.661330223083496
Epoch 2420, training loss: 311.1840515136719 = 0.013090627267956734 + 50.0 * 6.223419189453125
Epoch 2420, val loss: 1.6653509140014648
Epoch 2430, training loss: 311.1882019042969 = 0.012937438674271107 + 50.0 * 6.223505020141602
Epoch 2430, val loss: 1.6683995723724365
Epoch 2440, training loss: 311.37933349609375 = 0.012786531820893288 + 50.0 * 6.227330684661865
Epoch 2440, val loss: 1.6715561151504517
Epoch 2450, training loss: 311.173828125 = 0.012635334394872189 + 50.0 * 6.223223686218262
Epoch 2450, val loss: 1.6754634380340576
Epoch 2460, training loss: 311.12408447265625 = 0.012490320019423962 + 50.0 * 6.222231864929199
Epoch 2460, val loss: 1.6782728433609009
Epoch 2470, training loss: 311.1947326660156 = 0.01234850287437439 + 50.0 * 6.223647594451904
Epoch 2470, val loss: 1.6817046403884888
Epoch 2480, training loss: 311.15972900390625 = 0.012205979786813259 + 50.0 * 6.222950458526611
Epoch 2480, val loss: 1.684810757637024
Epoch 2490, training loss: 311.4164123535156 = 0.012067661620676517 + 50.0 * 6.228086948394775
Epoch 2490, val loss: 1.6879222393035889
Epoch 2500, training loss: 311.17559814453125 = 0.011928628198802471 + 50.0 * 6.223273277282715
Epoch 2500, val loss: 1.691461443901062
Epoch 2510, training loss: 311.06671142578125 = 0.011792616918683052 + 50.0 * 6.22109842300415
Epoch 2510, val loss: 1.6947253942489624
Epoch 2520, training loss: 311.2938537597656 = 0.011668886989355087 + 50.0 * 6.225643634796143
Epoch 2520, val loss: 1.6979259252548218
Epoch 2530, training loss: 311.1031494140625 = 0.011533192358911037 + 50.0 * 6.221832275390625
Epoch 2530, val loss: 1.7006975412368774
Epoch 2540, training loss: 311.01751708984375 = 0.011403868906199932 + 50.0 * 6.220122337341309
Epoch 2540, val loss: 1.7041904926300049
Epoch 2550, training loss: 311.0372314453125 = 0.011279662139713764 + 50.0 * 6.220518589019775
Epoch 2550, val loss: 1.707331895828247
Epoch 2560, training loss: 311.11907958984375 = 0.011160191148519516 + 50.0 * 6.222157955169678
Epoch 2560, val loss: 1.7104687690734863
Epoch 2570, training loss: 311.06903076171875 = 0.01104030292481184 + 50.0 * 6.2211594581604
Epoch 2570, val loss: 1.7138187885284424
Epoch 2580, training loss: 311.1881408691406 = 0.010924449190497398 + 50.0 * 6.223544597625732
Epoch 2580, val loss: 1.7165509462356567
Epoch 2590, training loss: 311.24505615234375 = 0.010806120000779629 + 50.0 * 6.224685192108154
Epoch 2590, val loss: 1.7193723917007446
Epoch 2600, training loss: 310.993896484375 = 0.010688255541026592 + 50.0 * 6.219664573669434
Epoch 2600, val loss: 1.7226463556289673
Epoch 2610, training loss: 311.0360412597656 = 0.010575534775853157 + 50.0 * 6.220509052276611
Epoch 2610, val loss: 1.7260990142822266
Epoch 2620, training loss: 311.08441162109375 = 0.01046685315668583 + 50.0 * 6.2214789390563965
Epoch 2620, val loss: 1.7289243936538696
Epoch 2630, training loss: 311.0422668457031 = 0.010357561521232128 + 50.0 * 6.220638275146484
Epoch 2630, val loss: 1.7317779064178467
Epoch 2640, training loss: 311.0005187988281 = 0.010250437073409557 + 50.0 * 6.2198052406311035
Epoch 2640, val loss: 1.734800100326538
Epoch 2650, training loss: 311.183837890625 = 0.010145650245249271 + 50.0 * 6.223474025726318
Epoch 2650, val loss: 1.7375174760818481
Epoch 2660, training loss: 310.9931335449219 = 0.01004161685705185 + 50.0 * 6.219661712646484
Epoch 2660, val loss: 1.7400459051132202
Epoch 2670, training loss: 310.9611511230469 = 0.009938609786331654 + 50.0 * 6.219024181365967
Epoch 2670, val loss: 1.7432347536087036
Epoch 2680, training loss: 311.2865905761719 = 0.009841802529990673 + 50.0 * 6.2255353927612305
Epoch 2680, val loss: 1.7462376356124878
Epoch 2690, training loss: 310.93359375 = 0.009739814326167107 + 50.0 * 6.218477249145508
Epoch 2690, val loss: 1.7488210201263428
Epoch 2700, training loss: 311.0592346191406 = 0.009646113961935043 + 50.0 * 6.220991611480713
Epoch 2700, val loss: 1.752220630645752
Epoch 2710, training loss: 310.96673583984375 = 0.009547739289700985 + 50.0 * 6.219143867492676
Epoch 2710, val loss: 1.7546952962875366
Epoch 2720, training loss: 310.91741943359375 = 0.009455176070332527 + 50.0 * 6.218159198760986
Epoch 2720, val loss: 1.7567960023880005
Epoch 2730, training loss: 310.9755859375 = 0.009365747682750225 + 50.0 * 6.219324111938477
Epoch 2730, val loss: 1.7598165273666382
Epoch 2740, training loss: 311.0399475097656 = 0.009274506941437721 + 50.0 * 6.220613479614258
Epoch 2740, val loss: 1.7627630233764648
Epoch 2750, training loss: 310.9391174316406 = 0.009183457121253014 + 50.0 * 6.218598365783691
Epoch 2750, val loss: 1.7652490139007568
Epoch 2760, training loss: 311.0807800292969 = 0.009096946567296982 + 50.0 * 6.221433639526367
Epoch 2760, val loss: 1.7682329416275024
Epoch 2770, training loss: 310.9012145996094 = 0.009008030407130718 + 50.0 * 6.217844009399414
Epoch 2770, val loss: 1.770626187324524
Epoch 2780, training loss: 310.9193420410156 = 0.00892429519444704 + 50.0 * 6.2182087898254395
Epoch 2780, val loss: 1.7735207080841064
Epoch 2790, training loss: 310.8844909667969 = 0.008841536939144135 + 50.0 * 6.217513084411621
Epoch 2790, val loss: 1.7759913206100464
Epoch 2800, training loss: 311.0464782714844 = 0.008760384283959866 + 50.0 * 6.220754146575928
Epoch 2800, val loss: 1.7785475254058838
Epoch 2810, training loss: 310.87042236328125 = 0.008677593432366848 + 50.0 * 6.217235088348389
Epoch 2810, val loss: 1.7809362411499023
Epoch 2820, training loss: 310.94061279296875 = 0.00859717559069395 + 50.0 * 6.218640327453613
Epoch 2820, val loss: 1.783621072769165
Epoch 2830, training loss: 310.9618835449219 = 0.008519312366843224 + 50.0 * 6.219067096710205
Epoch 2830, val loss: 1.7865732908248901
Epoch 2840, training loss: 310.9017028808594 = 0.008441291749477386 + 50.0 * 6.217864990234375
Epoch 2840, val loss: 1.7889149188995361
Epoch 2850, training loss: 310.8036804199219 = 0.0083660539239645 + 50.0 * 6.215906620025635
Epoch 2850, val loss: 1.7913798093795776
Epoch 2860, training loss: 310.9108581542969 = 0.00829287339001894 + 50.0 * 6.218050956726074
Epoch 2860, val loss: 1.7936793565750122
Epoch 2870, training loss: 310.83917236328125 = 0.008218088187277317 + 50.0 * 6.216619491577148
Epoch 2870, val loss: 1.7961485385894775
Epoch 2880, training loss: 311.0016784667969 = 0.008146786130964756 + 50.0 * 6.219870567321777
Epoch 2880, val loss: 1.7995648384094238
Epoch 2890, training loss: 310.921630859375 = 0.008073624223470688 + 50.0 * 6.218270778656006
Epoch 2890, val loss: 1.8012590408325195
Epoch 2900, training loss: 310.8000183105469 = 0.00800261739641428 + 50.0 * 6.2158403396606445
Epoch 2900, val loss: 1.8039686679840088
Epoch 2910, training loss: 310.78460693359375 = 0.007934107445180416 + 50.0 * 6.215533256530762
Epoch 2910, val loss: 1.806273341178894
Epoch 2920, training loss: 310.92236328125 = 0.007866030558943748 + 50.0 * 6.218289852142334
Epoch 2920, val loss: 1.8089524507522583
Epoch 2930, training loss: 310.9814453125 = 0.007797340862452984 + 50.0 * 6.219472408294678
Epoch 2930, val loss: 1.8109937906265259
Epoch 2940, training loss: 310.8153381347656 = 0.007728821597993374 + 50.0 * 6.216152191162109
Epoch 2940, val loss: 1.813354730606079
Epoch 2950, training loss: 310.8139953613281 = 0.007664586883038282 + 50.0 * 6.216126918792725
Epoch 2950, val loss: 1.8157618045806885
Epoch 2960, training loss: 310.69012451171875 = 0.007599493023008108 + 50.0 * 6.213650703430176
Epoch 2960, val loss: 1.8178859949111938
Epoch 2970, training loss: 310.7179870605469 = 0.0075397370383143425 + 50.0 * 6.214209079742432
Epoch 2970, val loss: 1.8203593492507935
Epoch 2980, training loss: 310.93426513671875 = 0.007480085361748934 + 50.0 * 6.218535900115967
Epoch 2980, val loss: 1.8220245838165283
Epoch 2990, training loss: 310.8125 = 0.007415900006890297 + 50.0 * 6.21610164642334
Epoch 2990, val loss: 1.8247504234313965
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6925925925925926
0.808645229309436
The final CL Acc:0.66049, 0.04027, The final GNN Acc:0.80777, 0.00287
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13156])
remove edge: torch.Size([2, 7996])
updated graph: torch.Size([2, 10596])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.8044128417969 = 1.9621344804763794 + 50.0 * 8.596845626831055
Epoch 0, val loss: 1.9533926248550415
Epoch 10, training loss: 431.7561950683594 = 1.9519063234329224 + 50.0 * 8.596085548400879
Epoch 10, val loss: 1.9435341358184814
Epoch 20, training loss: 431.4874267578125 = 1.9392216205596924 + 50.0 * 8.590964317321777
Epoch 20, val loss: 1.9309765100479126
Epoch 30, training loss: 429.6758117675781 = 1.9231239557266235 + 50.0 * 8.5550537109375
Epoch 30, val loss: 1.9149199724197388
Epoch 40, training loss: 417.1192626953125 = 1.903146743774414 + 50.0 * 8.304322242736816
Epoch 40, val loss: 1.8953635692596436
Epoch 50, training loss: 379.8272705078125 = 1.8789048194885254 + 50.0 * 7.558967113494873
Epoch 50, val loss: 1.8723011016845703
Epoch 60, training loss: 368.9747619628906 = 1.8616230487823486 + 50.0 * 7.3422627449035645
Epoch 60, val loss: 1.8568339347839355
Epoch 70, training loss: 362.0314025878906 = 1.849326729774475 + 50.0 * 7.203641414642334
Epoch 70, val loss: 1.8453881740570068
Epoch 80, training loss: 356.12078857421875 = 1.8347958326339722 + 50.0 * 7.085719585418701
Epoch 80, val loss: 1.8323261737823486
Epoch 90, training loss: 349.544921875 = 1.8237149715423584 + 50.0 * 6.9544243812561035
Epoch 90, val loss: 1.822651743888855
Epoch 100, training loss: 343.120849609375 = 1.8160176277160645 + 50.0 * 6.826096534729004
Epoch 100, val loss: 1.8160043954849243
Epoch 110, training loss: 339.0562438964844 = 1.8085206747055054 + 50.0 * 6.7449541091918945
Epoch 110, val loss: 1.809320330619812
Epoch 120, training loss: 336.3099060058594 = 1.79959237575531 + 50.0 * 6.690206050872803
Epoch 120, val loss: 1.801361083984375
Epoch 130, training loss: 334.1177978515625 = 1.7896965742111206 + 50.0 * 6.646562099456787
Epoch 130, val loss: 1.7927577495574951
Epoch 140, training loss: 332.3850402832031 = 1.7802600860595703 + 50.0 * 6.612095355987549
Epoch 140, val loss: 1.7844794988632202
Epoch 150, training loss: 331.0281982421875 = 1.7710442543029785 + 50.0 * 6.585143089294434
Epoch 150, val loss: 1.7762961387634277
Epoch 160, training loss: 329.8434753417969 = 1.7612509727478027 + 50.0 * 6.561644554138184
Epoch 160, val loss: 1.7676060199737549
Epoch 170, training loss: 328.9565124511719 = 1.7507433891296387 + 50.0 * 6.5441155433654785
Epoch 170, val loss: 1.758392572402954
Epoch 180, training loss: 328.03924560546875 = 1.7396533489227295 + 50.0 * 6.525991916656494
Epoch 180, val loss: 1.7488422393798828
Epoch 190, training loss: 327.36651611328125 = 1.727879285812378 + 50.0 * 6.512773036956787
Epoch 190, val loss: 1.738815426826477
Epoch 200, training loss: 326.62738037109375 = 1.7153372764587402 + 50.0 * 6.498240947723389
Epoch 200, val loss: 1.7281997203826904
Epoch 210, training loss: 326.0282897949219 = 1.7018941640853882 + 50.0 * 6.486527919769287
Epoch 210, val loss: 1.7169345617294312
Epoch 220, training loss: 325.44976806640625 = 1.687564730644226 + 50.0 * 6.475244045257568
Epoch 220, val loss: 1.7049546241760254
Epoch 230, training loss: 324.9004211425781 = 1.6721447706222534 + 50.0 * 6.464565277099609
Epoch 230, val loss: 1.6922229528427124
Epoch 240, training loss: 324.3805236816406 = 1.655809760093689 + 50.0 * 6.454493999481201
Epoch 240, val loss: 1.6786860227584839
Epoch 250, training loss: 324.0315246582031 = 1.6384021043777466 + 50.0 * 6.44786262512207
Epoch 250, val loss: 1.6644867658615112
Epoch 260, training loss: 323.5169982910156 = 1.6199777126312256 + 50.0 * 6.43794059753418
Epoch 260, val loss: 1.649462342262268
Epoch 270, training loss: 323.0467224121094 = 1.6005163192749023 + 50.0 * 6.428924083709717
Epoch 270, val loss: 1.6337368488311768
Epoch 280, training loss: 322.625 = 1.5802842378616333 + 50.0 * 6.420894145965576
Epoch 280, val loss: 1.6174319982528687
Epoch 290, training loss: 322.3957214355469 = 1.559274673461914 + 50.0 * 6.416728973388672
Epoch 290, val loss: 1.600633144378662
Epoch 300, training loss: 322.0744934082031 = 1.537318229675293 + 50.0 * 6.410743236541748
Epoch 300, val loss: 1.5833481550216675
Epoch 310, training loss: 321.6739807128906 = 1.514760971069336 + 50.0 * 6.403184413909912
Epoch 310, val loss: 1.565542221069336
Epoch 320, training loss: 321.28173828125 = 1.4917458295822144 + 50.0 * 6.39579963684082
Epoch 320, val loss: 1.5475808382034302
Epoch 330, training loss: 320.96148681640625 = 1.468315601348877 + 50.0 * 6.389863014221191
Epoch 330, val loss: 1.529489278793335
Epoch 340, training loss: 320.9219055175781 = 1.4445081949234009 + 50.0 * 6.389547824859619
Epoch 340, val loss: 1.511123776435852
Epoch 350, training loss: 320.4883728027344 = 1.4202462434768677 + 50.0 * 6.381362438201904
Epoch 350, val loss: 1.49271559715271
Epoch 360, training loss: 320.15643310546875 = 1.3958618640899658 + 50.0 * 6.375211238861084
Epoch 360, val loss: 1.4743080139160156
Epoch 370, training loss: 320.42376708984375 = 1.3713552951812744 + 50.0 * 6.381048202514648
Epoch 370, val loss: 1.4560656547546387
Epoch 380, training loss: 319.8387451171875 = 1.3464608192443848 + 50.0 * 6.369845867156982
Epoch 380, val loss: 1.437382459640503
Epoch 390, training loss: 319.47882080078125 = 1.3216290473937988 + 50.0 * 6.3631439208984375
Epoch 390, val loss: 1.4190410375595093
Epoch 400, training loss: 319.27813720703125 = 1.2968652248382568 + 50.0 * 6.359625339508057
Epoch 400, val loss: 1.4008244276046753
Epoch 410, training loss: 319.3255920410156 = 1.2721831798553467 + 50.0 * 6.361068248748779
Epoch 410, val loss: 1.382785677909851
Epoch 420, training loss: 319.0093078613281 = 1.2471644878387451 + 50.0 * 6.355242729187012
Epoch 420, val loss: 1.3646113872528076
Epoch 430, training loss: 318.7661437988281 = 1.2224016189575195 + 50.0 * 6.350874900817871
Epoch 430, val loss: 1.3467963933944702
Epoch 440, training loss: 318.5642395019531 = 1.197883129119873 + 50.0 * 6.34732723236084
Epoch 440, val loss: 1.3293002843856812
Epoch 450, training loss: 318.3816223144531 = 1.1735812425613403 + 50.0 * 6.344161033630371
Epoch 450, val loss: 1.3121744394302368
Epoch 460, training loss: 318.2627868652344 = 1.1493451595306396 + 50.0 * 6.342268943786621
Epoch 460, val loss: 1.2951810359954834
Epoch 470, training loss: 318.0876159667969 = 1.1253612041473389 + 50.0 * 6.339244842529297
Epoch 470, val loss: 1.2785496711730957
Epoch 480, training loss: 317.99456787109375 = 1.1017628908157349 + 50.0 * 6.337855815887451
Epoch 480, val loss: 1.262511968612671
Epoch 490, training loss: 317.7896423339844 = 1.0784592628479004 + 50.0 * 6.334223747253418
Epoch 490, val loss: 1.2467776536941528
Epoch 500, training loss: 317.6611022949219 = 1.0554980039596558 + 50.0 * 6.332111835479736
Epoch 500, val loss: 1.2316632270812988
Epoch 510, training loss: 317.53643798828125 = 1.0330082178115845 + 50.0 * 6.330068111419678
Epoch 510, val loss: 1.216933012008667
Epoch 520, training loss: 317.4975280761719 = 1.0107827186584473 + 50.0 * 6.329735279083252
Epoch 520, val loss: 1.2027126550674438
Epoch 530, training loss: 317.3957824707031 = 0.9888883233070374 + 50.0 * 6.3281378746032715
Epoch 530, val loss: 1.1887366771697998
Epoch 540, training loss: 317.16387939453125 = 0.9674519300460815 + 50.0 * 6.3239288330078125
Epoch 540, val loss: 1.1755635738372803
Epoch 550, training loss: 317.1336975097656 = 0.9465003609657288 + 50.0 * 6.32374382019043
Epoch 550, val loss: 1.162934422492981
Epoch 560, training loss: 316.9092712402344 = 0.9257134199142456 + 50.0 * 6.319671154022217
Epoch 560, val loss: 1.1506328582763672
Epoch 570, training loss: 316.88970947265625 = 0.9053834676742554 + 50.0 * 6.319686412811279
Epoch 570, val loss: 1.138728380203247
Epoch 580, training loss: 316.8619689941406 = 0.8851054906845093 + 50.0 * 6.31953763961792
Epoch 580, val loss: 1.1270265579223633
Epoch 590, training loss: 316.57171630859375 = 0.8653531074523926 + 50.0 * 6.314126968383789
Epoch 590, val loss: 1.1158068180084229
Epoch 600, training loss: 316.5013732910156 = 0.8459989428520203 + 50.0 * 6.313107013702393
Epoch 600, val loss: 1.1050978899002075
Epoch 610, training loss: 316.49920654296875 = 0.8269465565681458 + 50.0 * 6.313445568084717
Epoch 610, val loss: 1.0946825742721558
Epoch 620, training loss: 316.3497009277344 = 0.8080275654792786 + 50.0 * 6.310833930969238
Epoch 620, val loss: 1.0846350193023682
Epoch 630, training loss: 316.2420959472656 = 0.7894504070281982 + 50.0 * 6.309052467346191
Epoch 630, val loss: 1.0748307704925537
Epoch 640, training loss: 316.2490539550781 = 0.7711287140846252 + 50.0 * 6.309558868408203
Epoch 640, val loss: 1.0655248165130615
Epoch 650, training loss: 316.0287170410156 = 0.7530676126480103 + 50.0 * 6.305512428283691
Epoch 650, val loss: 1.0562481880187988
Epoch 660, training loss: 315.9552917480469 = 0.7353505492210388 + 50.0 * 6.304399013519287
Epoch 660, val loss: 1.0477557182312012
Epoch 670, training loss: 316.0485534667969 = 0.7177603840827942 + 50.0 * 6.306615352630615
Epoch 670, val loss: 1.0392637252807617
Epoch 680, training loss: 315.7371826171875 = 0.7004215121269226 + 50.0 * 6.3007354736328125
Epoch 680, val loss: 1.0309821367263794
Epoch 690, training loss: 315.686279296875 = 0.6834380030632019 + 50.0 * 6.3000569343566895
Epoch 690, val loss: 1.0234954357147217
Epoch 700, training loss: 315.7260437011719 = 0.6666666269302368 + 50.0 * 6.301187992095947
Epoch 700, val loss: 1.0161428451538086
Epoch 710, training loss: 315.48956298828125 = 0.6500723361968994 + 50.0 * 6.296789646148682
Epoch 710, val loss: 1.0087456703186035
Epoch 720, training loss: 315.39013671875 = 0.6338170170783997 + 50.0 * 6.295126438140869
Epoch 720, val loss: 1.002118706703186
Epoch 730, training loss: 315.77923583984375 = 0.6178773641586304 + 50.0 * 6.303226947784424
Epoch 730, val loss: 0.9957473278045654
Epoch 740, training loss: 315.3218078613281 = 0.6018094420433044 + 50.0 * 6.294399738311768
Epoch 740, val loss: 0.9896165728569031
Epoch 750, training loss: 315.2453308105469 = 0.58616042137146 + 50.0 * 6.293183326721191
Epoch 750, val loss: 0.9835249185562134
Epoch 760, training loss: 315.2253112792969 = 0.5708757638931274 + 50.0 * 6.293088912963867
Epoch 760, val loss: 0.978217363357544
Epoch 770, training loss: 315.0263671875 = 0.5557622909545898 + 50.0 * 6.289412021636963
Epoch 770, val loss: 0.9732455611228943
Epoch 780, training loss: 314.97186279296875 = 0.5409392714500427 + 50.0 * 6.288619041442871
Epoch 780, val loss: 0.9686232805252075
Epoch 790, training loss: 314.9452819824219 = 0.5262734293937683 + 50.0 * 6.288380146026611
Epoch 790, val loss: 0.9640901684761047
Epoch 800, training loss: 314.8595275878906 = 0.5118482112884521 + 50.0 * 6.286953449249268
Epoch 800, val loss: 0.9598196744918823
Epoch 810, training loss: 314.737060546875 = 0.49781927466392517 + 50.0 * 6.28478479385376
Epoch 810, val loss: 0.955977737903595
Epoch 820, training loss: 314.8134460449219 = 0.4840731620788574 + 50.0 * 6.286587238311768
Epoch 820, val loss: 0.9525875449180603
Epoch 830, training loss: 314.637451171875 = 0.4704725742340088 + 50.0 * 6.283339977264404
Epoch 830, val loss: 0.9493854641914368
Epoch 840, training loss: 314.6355285644531 = 0.4571993947029114 + 50.0 * 6.283566474914551
Epoch 840, val loss: 0.9463911652565002
Epoch 850, training loss: 314.5113830566406 = 0.4442651569843292 + 50.0 * 6.281342029571533
Epoch 850, val loss: 0.9438303709030151
Epoch 860, training loss: 314.4353942871094 = 0.4316534399986267 + 50.0 * 6.2800750732421875
Epoch 860, val loss: 0.9416637420654297
Epoch 870, training loss: 314.6499938964844 = 0.4193640351295471 + 50.0 * 6.28461217880249
Epoch 870, val loss: 0.9397795796394348
Epoch 880, training loss: 314.4436340332031 = 0.40723711252212524 + 50.0 * 6.280727863311768
Epoch 880, val loss: 0.9378865957260132
Epoch 890, training loss: 314.3442077636719 = 0.39547988772392273 + 50.0 * 6.278974533081055
Epoch 890, val loss: 0.936482846736908
Epoch 900, training loss: 314.2935485839844 = 0.38404980301856995 + 50.0 * 6.278189659118652
Epoch 900, val loss: 0.9353252649307251
Epoch 910, training loss: 314.1556701660156 = 0.3729517459869385 + 50.0 * 6.275654315948486
Epoch 910, val loss: 0.9341914653778076
Epoch 920, training loss: 314.0805969238281 = 0.36226892471313477 + 50.0 * 6.27436637878418
Epoch 920, val loss: 0.9336141347885132
Epoch 930, training loss: 314.1262512207031 = 0.3519021272659302 + 50.0 * 6.275486946105957
Epoch 930, val loss: 0.9331607818603516
Epoch 940, training loss: 314.1541748046875 = 0.3417874872684479 + 50.0 * 6.276247501373291
Epoch 940, val loss: 0.9328267574310303
Epoch 950, training loss: 314.12847900390625 = 0.33197885751724243 + 50.0 * 6.275929927825928
Epoch 950, val loss: 0.9329341650009155
Epoch 960, training loss: 314.0249328613281 = 0.322452574968338 + 50.0 * 6.274049758911133
Epoch 960, val loss: 0.9332411289215088
Epoch 970, training loss: 313.8604736328125 = 0.3132278621196747 + 50.0 * 6.270944595336914
Epoch 970, val loss: 0.9332307577133179
Epoch 980, training loss: 313.8361511230469 = 0.30439814925193787 + 50.0 * 6.270634651184082
Epoch 980, val loss: 0.9340764880180359
Epoch 990, training loss: 313.9895324707031 = 0.2957920432090759 + 50.0 * 6.273874282836914
Epoch 990, val loss: 0.9348188042640686
Epoch 1000, training loss: 313.8691711425781 = 0.28746652603149414 + 50.0 * 6.271634101867676
Epoch 1000, val loss: 0.9357500076293945
Epoch 1010, training loss: 313.7568054199219 = 0.27933207154273987 + 50.0 * 6.26954984664917
Epoch 1010, val loss: 0.936805248260498
Epoch 1020, training loss: 313.6380920410156 = 0.27155548334121704 + 50.0 * 6.267330646514893
Epoch 1020, val loss: 0.9380411505699158
Epoch 1030, training loss: 313.5738525390625 = 0.2640206217765808 + 50.0 * 6.2661967277526855
Epoch 1030, val loss: 0.9394969940185547
Epoch 1040, training loss: 313.8609619140625 = 0.25675931572914124 + 50.0 * 6.2720842361450195
Epoch 1040, val loss: 0.941037654876709
Epoch 1050, training loss: 313.7332458496094 = 0.24963660538196564 + 50.0 * 6.269672393798828
Epoch 1050, val loss: 0.9428821802139282
Epoch 1060, training loss: 313.50421142578125 = 0.24273334443569183 + 50.0 * 6.26522970199585
Epoch 1060, val loss: 0.9449254274368286
Epoch 1070, training loss: 313.4540100097656 = 0.23614314198493958 + 50.0 * 6.264357089996338
Epoch 1070, val loss: 0.9470711946487427
Epoch 1080, training loss: 313.6295471191406 = 0.22975800931453705 + 50.0 * 6.267995834350586
Epoch 1080, val loss: 0.9492995738983154
Epoch 1090, training loss: 313.6480407714844 = 0.22348079085350037 + 50.0 * 6.268491268157959
Epoch 1090, val loss: 0.9514552354812622
Epoch 1100, training loss: 313.2955322265625 = 0.2173851579427719 + 50.0 * 6.261562824249268
Epoch 1100, val loss: 0.9539401531219482
Epoch 1110, training loss: 313.28125 = 0.21161286532878876 + 50.0 * 6.261392593383789
Epoch 1110, val loss: 0.9566085338592529
Epoch 1120, training loss: 313.23175048828125 = 0.20601767301559448 + 50.0 * 6.260514736175537
Epoch 1120, val loss: 0.9594919681549072
Epoch 1130, training loss: 313.86480712890625 = 0.20060612261295319 + 50.0 * 6.273284435272217
Epoch 1130, val loss: 0.9624029994010925
Epoch 1140, training loss: 313.38616943359375 = 0.19524207711219788 + 50.0 * 6.263818264007568
Epoch 1140, val loss: 0.9651852250099182
Epoch 1150, training loss: 313.1148986816406 = 0.190086230635643 + 50.0 * 6.258496284484863
Epoch 1150, val loss: 0.9684162139892578
Epoch 1160, training loss: 313.10845947265625 = 0.1851896345615387 + 50.0 * 6.258465766906738
Epoch 1160, val loss: 0.9716516733169556
Epoch 1170, training loss: 313.06256103515625 = 0.1804434359073639 + 50.0 * 6.2576422691345215
Epoch 1170, val loss: 0.9751932621002197
Epoch 1180, training loss: 313.8569030761719 = 0.17588576674461365 + 50.0 * 6.27362060546875
Epoch 1180, val loss: 0.9788326621055603
Epoch 1190, training loss: 313.1842041015625 = 0.17119264602661133 + 50.0 * 6.260260105133057
Epoch 1190, val loss: 0.9819858074188232
Epoch 1200, training loss: 313.0400390625 = 0.16681316494941711 + 50.0 * 6.25746488571167
Epoch 1200, val loss: 0.9856870174407959
Epoch 1210, training loss: 312.9337463378906 = 0.1626199632883072 + 50.0 * 6.255422115325928
Epoch 1210, val loss: 0.9896745681762695
Epoch 1220, training loss: 313.15289306640625 = 0.1585715264081955 + 50.0 * 6.259886741638184
Epoch 1220, val loss: 0.9938104748725891
Epoch 1230, training loss: 312.91510009765625 = 0.1545325666666031 + 50.0 * 6.255210876464844
Epoch 1230, val loss: 0.997128963470459
Epoch 1240, training loss: 312.8868103027344 = 0.15066775679588318 + 50.0 * 6.254723072052002
Epoch 1240, val loss: 1.0015897750854492
Epoch 1250, training loss: 312.8307189941406 = 0.1469498574733734 + 50.0 * 6.25367546081543
Epoch 1250, val loss: 1.0056034326553345
Epoch 1260, training loss: 312.88037109375 = 0.14337879419326782 + 50.0 * 6.254739761352539
Epoch 1260, val loss: 1.0101569890975952
Epoch 1270, training loss: 312.8331298828125 = 0.13982586562633514 + 50.0 * 6.253865718841553
Epoch 1270, val loss: 1.0142959356307983
Epoch 1280, training loss: 312.928466796875 = 0.13639751076698303 + 50.0 * 6.2558417320251465
Epoch 1280, val loss: 1.0187853574752808
Epoch 1290, training loss: 312.8062438964844 = 0.1330772340297699 + 50.0 * 6.253463268280029
Epoch 1290, val loss: 1.0231295824050903
Epoch 1300, training loss: 312.7139892578125 = 0.12990166246891022 + 50.0 * 6.251681327819824
Epoch 1300, val loss: 1.0278911590576172
Epoch 1310, training loss: 312.7548828125 = 0.12682698667049408 + 50.0 * 6.252561092376709
Epoch 1310, val loss: 1.032487392425537
Epoch 1320, training loss: 312.8123474121094 = 0.123835988342762 + 50.0 * 6.253770351409912
Epoch 1320, val loss: 1.037292718887329
Epoch 1330, training loss: 312.7425842285156 = 0.12089396268129349 + 50.0 * 6.252434253692627
Epoch 1330, val loss: 1.0419682264328003
Epoch 1340, training loss: 312.66925048828125 = 0.1180419772863388 + 50.0 * 6.25102424621582
Epoch 1340, val loss: 1.0467822551727295
Epoch 1350, training loss: 312.6121826171875 = 0.11530835181474686 + 50.0 * 6.249938011169434
Epoch 1350, val loss: 1.0516881942749023
Epoch 1360, training loss: 312.6116943359375 = 0.11264964193105698 + 50.0 * 6.249980926513672
Epoch 1360, val loss: 1.056768536567688
Epoch 1370, training loss: 312.64129638671875 = 0.11008046567440033 + 50.0 * 6.250624656677246
Epoch 1370, val loss: 1.061420202255249
Epoch 1380, training loss: 312.5806884765625 = 0.10754378139972687 + 50.0 * 6.249463081359863
Epoch 1380, val loss: 1.0662968158721924
Epoch 1390, training loss: 312.5550537109375 = 0.10508023947477341 + 50.0 * 6.24899959564209
Epoch 1390, val loss: 1.071242094039917
Epoch 1400, training loss: 312.4709167480469 = 0.10270018875598907 + 50.0 * 6.247364521026611
Epoch 1400, val loss: 1.0761969089508057
Epoch 1410, training loss: 312.5349426269531 = 0.10042276233434677 + 50.0 * 6.248690128326416
Epoch 1410, val loss: 1.081173062324524
Epoch 1420, training loss: 312.53289794921875 = 0.0981559157371521 + 50.0 * 6.248695373535156
Epoch 1420, val loss: 1.0862337350845337
Epoch 1430, training loss: 312.45648193359375 = 0.09600596874952316 + 50.0 * 6.247209548950195
Epoch 1430, val loss: 1.0913810729980469
Epoch 1440, training loss: 312.382568359375 = 0.09388870000839233 + 50.0 * 6.2457733154296875
Epoch 1440, val loss: 1.0964516401290894
Epoch 1450, training loss: 312.8487548828125 = 0.09186205267906189 + 50.0 * 6.255137920379639
Epoch 1450, val loss: 1.1013808250427246
Epoch 1460, training loss: 312.3836364746094 = 0.0897749811410904 + 50.0 * 6.245877265930176
Epoch 1460, val loss: 1.1065629720687866
Epoch 1470, training loss: 312.2895202636719 = 0.08783488720655441 + 50.0 * 6.2440338134765625
Epoch 1470, val loss: 1.111697793006897
Epoch 1480, training loss: 312.5670471191406 = 0.08597847819328308 + 50.0 * 6.249621868133545
Epoch 1480, val loss: 1.1168965101242065
Epoch 1490, training loss: 312.2884521484375 = 0.08409979939460754 + 50.0 * 6.244087219238281
Epoch 1490, val loss: 1.122025966644287
Epoch 1500, training loss: 312.2480773925781 = 0.08230181038379669 + 50.0 * 6.24331521987915
Epoch 1500, val loss: 1.127275824546814
Epoch 1510, training loss: 312.233154296875 = 0.08058122545480728 + 50.0 * 6.243051528930664
Epoch 1510, val loss: 1.1325479745864868
Epoch 1520, training loss: 312.5428771972656 = 0.07888850569725037 + 50.0 * 6.249279499053955
Epoch 1520, val loss: 1.1377424001693726
Epoch 1530, training loss: 312.326416015625 = 0.077214814722538 + 50.0 * 6.244984149932861
Epoch 1530, val loss: 1.1426950693130493
Epoch 1540, training loss: 312.31451416015625 = 0.07560289651155472 + 50.0 * 6.244778633117676
Epoch 1540, val loss: 1.1480982303619385
Epoch 1550, training loss: 312.15484619140625 = 0.07402286678552628 + 50.0 * 6.241616249084473
Epoch 1550, val loss: 1.1531277894973755
Epoch 1560, training loss: 312.1754150390625 = 0.07252317667007446 + 50.0 * 6.242058277130127
Epoch 1560, val loss: 1.1584508419036865
Epoch 1570, training loss: 312.24432373046875 = 0.07103206217288971 + 50.0 * 6.243465423583984
Epoch 1570, val loss: 1.1636098623275757
Epoch 1580, training loss: 312.19415283203125 = 0.06957723200321198 + 50.0 * 6.242491722106934
Epoch 1580, val loss: 1.1686064004898071
Epoch 1590, training loss: 312.2634582519531 = 0.06817352026700974 + 50.0 * 6.243905544281006
Epoch 1590, val loss: 1.1737090349197388
Epoch 1600, training loss: 312.33416748046875 = 0.06678315252065659 + 50.0 * 6.24534797668457
Epoch 1600, val loss: 1.1786872148513794
Epoch 1610, training loss: 312.1634216308594 = 0.06542006134986877 + 50.0 * 6.241960048675537
Epoch 1610, val loss: 1.1838452816009521
Epoch 1620, training loss: 312.0282287597656 = 0.06409810483455658 + 50.0 * 6.239283084869385
Epoch 1620, val loss: 1.1888139247894287
Epoch 1630, training loss: 311.97467041015625 = 0.06284061074256897 + 50.0 * 6.238236427307129
Epoch 1630, val loss: 1.194261074066162
Epoch 1640, training loss: 311.98126220703125 = 0.061621006578207016 + 50.0 * 6.2383928298950195
Epoch 1640, val loss: 1.1991057395935059
Epoch 1650, training loss: 312.3179931640625 = 0.0604250505566597 + 50.0 * 6.245151519775391
Epoch 1650, val loss: 1.2042782306671143
Epoch 1660, training loss: 312.0644226074219 = 0.05919833481311798 + 50.0 * 6.2401041984558105
Epoch 1660, val loss: 1.2092962265014648
Epoch 1670, training loss: 311.9617004394531 = 0.05803989619016647 + 50.0 * 6.238072872161865
Epoch 1670, val loss: 1.2141815423965454
Epoch 1680, training loss: 312.0423278808594 = 0.05692969262599945 + 50.0 * 6.239708423614502
Epoch 1680, val loss: 1.2190322875976562
Epoch 1690, training loss: 311.97052001953125 = 0.05582285672426224 + 50.0 * 6.2382941246032715
Epoch 1690, val loss: 1.2240204811096191
Epoch 1700, training loss: 311.9184265136719 = 0.05476599931716919 + 50.0 * 6.2372727394104
Epoch 1700, val loss: 1.2290421724319458
Epoch 1710, training loss: 311.8639221191406 = 0.053720276802778244 + 50.0 * 6.236204147338867
Epoch 1710, val loss: 1.234032154083252
Epoch 1720, training loss: 312.17816162109375 = 0.05271419510245323 + 50.0 * 6.242508888244629
Epoch 1720, val loss: 1.2388354539871216
Epoch 1730, training loss: 312.0555419921875 = 0.05170062929391861 + 50.0 * 6.240077018737793
Epoch 1730, val loss: 1.2434158325195312
Epoch 1740, training loss: 312.0302734375 = 0.05071549117565155 + 50.0 * 6.239591121673584
Epoch 1740, val loss: 1.248644232749939
Epoch 1750, training loss: 311.8165283203125 = 0.04977124184370041 + 50.0 * 6.235335350036621
Epoch 1750, val loss: 1.2532341480255127
Epoch 1760, training loss: 311.7878723144531 = 0.04884681850671768 + 50.0 * 6.234780311584473
Epoch 1760, val loss: 1.2582529783248901
Epoch 1770, training loss: 311.7926940917969 = 0.04796129837632179 + 50.0 * 6.234894275665283
Epoch 1770, val loss: 1.2629876136779785
Epoch 1780, training loss: 312.038818359375 = 0.04708874970674515 + 50.0 * 6.239834785461426
Epoch 1780, val loss: 1.2676852941513062
Epoch 1790, training loss: 312.0507507324219 = 0.046210531145334244 + 50.0 * 6.240090847015381
Epoch 1790, val loss: 1.272295355796814
Epoch 1800, training loss: 311.76776123046875 = 0.04534602910280228 + 50.0 * 6.234448432922363
Epoch 1800, val loss: 1.276878833770752
Epoch 1810, training loss: 311.6983337402344 = 0.044527165591716766 + 50.0 * 6.233076095581055
Epoch 1810, val loss: 1.2816951274871826
Epoch 1820, training loss: 311.64874267578125 = 0.04373881593346596 + 50.0 * 6.232100009918213
Epoch 1820, val loss: 1.2865101099014282
Epoch 1830, training loss: 311.81451416015625 = 0.04297703504562378 + 50.0 * 6.235430717468262
Epoch 1830, val loss: 1.2912565469741821
Epoch 1840, training loss: 311.65460205078125 = 0.04219099506735802 + 50.0 * 6.232247829437256
Epoch 1840, val loss: 1.2954342365264893
Epoch 1850, training loss: 311.896484375 = 0.04143968224525452 + 50.0 * 6.237101078033447
Epoch 1850, val loss: 1.3000935316085815
Epoch 1860, training loss: 311.7528991699219 = 0.040686286985874176 + 50.0 * 6.234244346618652
Epoch 1860, val loss: 1.3044663667678833
Epoch 1870, training loss: 311.62750244140625 = 0.039963673800230026 + 50.0 * 6.23175048828125
Epoch 1870, val loss: 1.308896541595459
Epoch 1880, training loss: 311.56573486328125 = 0.03926840052008629 + 50.0 * 6.230529308319092
Epoch 1880, val loss: 1.3136247396469116
Epoch 1890, training loss: 311.5433349609375 = 0.03860453888773918 + 50.0 * 6.2300944328308105
Epoch 1890, val loss: 1.3181891441345215
Epoch 1900, training loss: 311.7142639160156 = 0.037956271320581436 + 50.0 * 6.23352575302124
Epoch 1900, val loss: 1.3226022720336914
Epoch 1910, training loss: 311.6893310546875 = 0.03727727383375168 + 50.0 * 6.233040809631348
Epoch 1910, val loss: 1.3267351388931274
Epoch 1920, training loss: 311.6408996582031 = 0.036613497883081436 + 50.0 * 6.232085704803467
Epoch 1920, val loss: 1.3308402299880981
Epoch 1930, training loss: 311.52490234375 = 0.035986386239528656 + 50.0 * 6.229778289794922
Epoch 1930, val loss: 1.3351576328277588
Epoch 1940, training loss: 311.4761657714844 = 0.03539123013615608 + 50.0 * 6.22881555557251
Epoch 1940, val loss: 1.339685320854187
Epoch 1950, training loss: 311.5739440917969 = 0.03480753302574158 + 50.0 * 6.230782508850098
Epoch 1950, val loss: 1.3439109325408936
Epoch 1960, training loss: 311.6009826660156 = 0.034217361360788345 + 50.0 * 6.231335639953613
Epoch 1960, val loss: 1.3479845523834229
Epoch 1970, training loss: 311.731201171875 = 0.03364717587828636 + 50.0 * 6.233950614929199
Epoch 1970, val loss: 1.352152943611145
Epoch 1980, training loss: 311.4797668457031 = 0.03307220712304115 + 50.0 * 6.228933811187744
Epoch 1980, val loss: 1.356232762336731
Epoch 1990, training loss: 311.5032653808594 = 0.03253430873155594 + 50.0 * 6.229414939880371
Epoch 1990, val loss: 1.3605467081069946
Epoch 2000, training loss: 311.5672912597656 = 0.032009340822696686 + 50.0 * 6.230705261230469
Epoch 2000, val loss: 1.364640474319458
Epoch 2010, training loss: 311.4873046875 = 0.03147900477051735 + 50.0 * 6.229116439819336
Epoch 2010, val loss: 1.3685590028762817
Epoch 2020, training loss: 311.52349853515625 = 0.03096483275294304 + 50.0 * 6.2298502922058105
Epoch 2020, val loss: 1.3726997375488281
Epoch 2030, training loss: 311.4217834472656 = 0.030472934246063232 + 50.0 * 6.227826118469238
Epoch 2030, val loss: 1.3768649101257324
Epoch 2040, training loss: 311.3891296386719 = 0.02998587116599083 + 50.0 * 6.227182865142822
Epoch 2040, val loss: 1.3809388875961304
Epoch 2050, training loss: 311.5321960449219 = 0.02952023223042488 + 50.0 * 6.230053901672363
Epoch 2050, val loss: 1.385188341140747
Epoch 2060, training loss: 311.697998046875 = 0.02904381975531578 + 50.0 * 6.233378887176514
Epoch 2060, val loss: 1.388581395149231
Epoch 2070, training loss: 311.4132995605469 = 0.028567874804139137 + 50.0 * 6.227694511413574
Epoch 2070, val loss: 1.3926860094070435
Epoch 2080, training loss: 311.34619140625 = 0.0281185582280159 + 50.0 * 6.2263617515563965
Epoch 2080, val loss: 1.3965742588043213
Epoch 2090, training loss: 311.43463134765625 = 0.02768777869641781 + 50.0 * 6.2281389236450195
Epoch 2090, val loss: 1.4005872011184692
Epoch 2100, training loss: 311.3565979003906 = 0.027253910899162292 + 50.0 * 6.226586818695068
Epoch 2100, val loss: 1.4043911695480347
Epoch 2110, training loss: 311.3183898925781 = 0.02683550864458084 + 50.0 * 6.225831031799316
Epoch 2110, val loss: 1.4083603620529175
Epoch 2120, training loss: 311.285400390625 = 0.026430826634168625 + 50.0 * 6.225179195404053
Epoch 2120, val loss: 1.4120689630508423
Epoch 2130, training loss: 311.5021057128906 = 0.026035936549305916 + 50.0 * 6.229521751403809
Epoch 2130, val loss: 1.4158177375793457
Epoch 2140, training loss: 311.2678527832031 = 0.025631364434957504 + 50.0 * 6.224844455718994
Epoch 2140, val loss: 1.4197022914886475
Epoch 2150, training loss: 311.2276306152344 = 0.025244221091270447 + 50.0 * 6.224048137664795
Epoch 2150, val loss: 1.4233571290969849
Epoch 2160, training loss: 311.3732604980469 = 0.02487734518945217 + 50.0 * 6.226967811584473
Epoch 2160, val loss: 1.4270684719085693
Epoch 2170, training loss: 311.3674011230469 = 0.024487607181072235 + 50.0 * 6.226858615875244
Epoch 2170, val loss: 1.4305756092071533
Epoch 2180, training loss: 311.2483825683594 = 0.024116383865475655 + 50.0 * 6.224485397338867
Epoch 2180, val loss: 1.4341245889663696
Epoch 2190, training loss: 311.2217712402344 = 0.02375999465584755 + 50.0 * 6.223959922790527
Epoch 2190, val loss: 1.4379220008850098
Epoch 2200, training loss: 311.1794128417969 = 0.023424742743372917 + 50.0 * 6.223119258880615
Epoch 2200, val loss: 1.4417544603347778
Epoch 2210, training loss: 311.4790344238281 = 0.023093799129128456 + 50.0 * 6.229118347167969
Epoch 2210, val loss: 1.4453661441802979
Epoch 2220, training loss: 311.2357177734375 = 0.022751951590180397 + 50.0 * 6.224258899688721
Epoch 2220, val loss: 1.4485245943069458
Epoch 2230, training loss: 311.2070617675781 = 0.02241189032793045 + 50.0 * 6.223693370819092
Epoch 2230, val loss: 1.452317476272583
Epoch 2240, training loss: 311.2412414550781 = 0.022106273099780083 + 50.0 * 6.2243828773498535
Epoch 2240, val loss: 1.4558875560760498
Epoch 2250, training loss: 311.3210754394531 = 0.02179136872291565 + 50.0 * 6.225985527038574
Epoch 2250, val loss: 1.4593331813812256
Epoch 2260, training loss: 311.2120056152344 = 0.021482037380337715 + 50.0 * 6.223810195922852
Epoch 2260, val loss: 1.4627954959869385
Epoch 2270, training loss: 311.14385986328125 = 0.021184658631682396 + 50.0 * 6.222453594207764
Epoch 2270, val loss: 1.4663773775100708
Epoch 2280, training loss: 311.19696044921875 = 0.020889177918434143 + 50.0 * 6.223521709442139
Epoch 2280, val loss: 1.4696608781814575
Epoch 2290, training loss: 311.15277099609375 = 0.02060095965862274 + 50.0 * 6.2226433753967285
Epoch 2290, val loss: 1.4733422994613647
Epoch 2300, training loss: 311.31573486328125 = 0.020321303978562355 + 50.0 * 6.225908279418945
Epoch 2300, val loss: 1.4764480590820312
Epoch 2310, training loss: 311.2191467285156 = 0.020037569105625153 + 50.0 * 6.223981857299805
Epoch 2310, val loss: 1.4797565937042236
Epoch 2320, training loss: 311.1201477050781 = 0.019757503643631935 + 50.0 * 6.222008228302002
Epoch 2320, val loss: 1.4831405878067017
Epoch 2330, training loss: 311.0489807128906 = 0.019496748223900795 + 50.0 * 6.220589637756348
Epoch 2330, val loss: 1.4866199493408203
Epoch 2340, training loss: 311.03863525390625 = 0.019239405170083046 + 50.0 * 6.220388412475586
Epoch 2340, val loss: 1.4898844957351685
Epoch 2350, training loss: 311.3271179199219 = 0.018993543460965157 + 50.0 * 6.226162910461426
Epoch 2350, val loss: 1.4930318593978882
Epoch 2360, training loss: 311.1366882324219 = 0.018735183402895927 + 50.0 * 6.2223591804504395
Epoch 2360, val loss: 1.4962855577468872
Epoch 2370, training loss: 311.3078918457031 = 0.018476946279406548 + 50.0 * 6.225788593292236
Epoch 2370, val loss: 1.4993526935577393
Epoch 2380, training loss: 311.1025695800781 = 0.01823381520807743 + 50.0 * 6.221686840057373
Epoch 2380, val loss: 1.502684473991394
Epoch 2390, training loss: 310.9964294433594 = 0.017997004091739655 + 50.0 * 6.219568252563477
Epoch 2390, val loss: 1.5059105157852173
Epoch 2400, training loss: 311.0730895996094 = 0.01777711883187294 + 50.0 * 6.221106052398682
Epoch 2400, val loss: 1.5090246200561523
Epoch 2410, training loss: 311.2091979980469 = 0.01754470355808735 + 50.0 * 6.223833084106445
Epoch 2410, val loss: 1.5120337009429932
Epoch 2420, training loss: 311.08843994140625 = 0.017310485243797302 + 50.0 * 6.2214226722717285
Epoch 2420, val loss: 1.5151257514953613
Epoch 2430, training loss: 310.97998046875 = 0.01709604263305664 + 50.0 * 6.219257831573486
Epoch 2430, val loss: 1.5183964967727661
Epoch 2440, training loss: 310.9552917480469 = 0.01688305102288723 + 50.0 * 6.218768119812012
Epoch 2440, val loss: 1.5215706825256348
Epoch 2450, training loss: 311.2443542480469 = 0.016685958951711655 + 50.0 * 6.224553108215332
Epoch 2450, val loss: 1.5246171951293945
Epoch 2460, training loss: 311.00775146484375 = 0.016466181725263596 + 50.0 * 6.219825267791748
Epoch 2460, val loss: 1.5273690223693848
Epoch 2470, training loss: 310.98345947265625 = 0.016261234879493713 + 50.0 * 6.219343662261963
Epoch 2470, val loss: 1.5303428173065186
Epoch 2480, training loss: 311.0137634277344 = 0.016060341149568558 + 50.0 * 6.219954490661621
Epoch 2480, val loss: 1.533311128616333
Epoch 2490, training loss: 311.0990295410156 = 0.015865512192249298 + 50.0 * 6.221663475036621
Epoch 2490, val loss: 1.5362286567687988
Epoch 2500, training loss: 310.9555969238281 = 0.015668313950300217 + 50.0 * 6.218799114227295
Epoch 2500, val loss: 1.5392718315124512
Epoch 2510, training loss: 310.9106750488281 = 0.015484894625842571 + 50.0 * 6.217904090881348
Epoch 2510, val loss: 1.5422829389572144
Epoch 2520, training loss: 310.9251708984375 = 0.015301505103707314 + 50.0 * 6.218197345733643
Epoch 2520, val loss: 1.5452007055282593
Epoch 2530, training loss: 311.0770568847656 = 0.015125662088394165 + 50.0 * 6.221238136291504
Epoch 2530, val loss: 1.5479828119277954
Epoch 2540, training loss: 311.0643310546875 = 0.014940734021365643 + 50.0 * 6.220987796783447
Epoch 2540, val loss: 1.5508390665054321
Epoch 2550, training loss: 310.85302734375 = 0.014760738238692284 + 50.0 * 6.2167649269104
Epoch 2550, val loss: 1.5536924600601196
Epoch 2560, training loss: 310.95721435546875 = 0.01459085289388895 + 50.0 * 6.218852519989014
Epoch 2560, val loss: 1.5564181804656982
Epoch 2570, training loss: 311.0669250488281 = 0.01442140806466341 + 50.0 * 6.221049785614014
Epoch 2570, val loss: 1.5592074394226074
Epoch 2580, training loss: 310.907958984375 = 0.014250933192670345 + 50.0 * 6.217874050140381
Epoch 2580, val loss: 1.5619381666183472
Epoch 2590, training loss: 310.8682861328125 = 0.014090173877775669 + 50.0 * 6.217083930969238
Epoch 2590, val loss: 1.564799189567566
Epoch 2600, training loss: 311.03076171875 = 0.013939158990979195 + 50.0 * 6.220336437225342
Epoch 2600, val loss: 1.567442774772644
Epoch 2610, training loss: 310.79803466796875 = 0.013773457147181034 + 50.0 * 6.2156853675842285
Epoch 2610, val loss: 1.5702545642852783
Epoch 2620, training loss: 310.83477783203125 = 0.013623065315186977 + 50.0 * 6.216423034667969
Epoch 2620, val loss: 1.5727297067642212
Epoch 2630, training loss: 311.2701721191406 = 0.013478328473865986 + 50.0 * 6.225133419036865
Epoch 2630, val loss: 1.5752573013305664
Epoch 2640, training loss: 310.95538330078125 = 0.013320048339664936 + 50.0 * 6.218841552734375
Epoch 2640, val loss: 1.5777863264083862
Epoch 2650, training loss: 310.8434753417969 = 0.01316925324499607 + 50.0 * 6.2166056632995605
Epoch 2650, val loss: 1.5805221796035767
Epoch 2660, training loss: 310.92498779296875 = 0.013031375594437122 + 50.0 * 6.2182393074035645
Epoch 2660, val loss: 1.5833892822265625
Epoch 2670, training loss: 310.8026428222656 = 0.01288454607129097 + 50.0 * 6.215795040130615
Epoch 2670, val loss: 1.5858741998672485
Epoch 2680, training loss: 310.7833557128906 = 0.012747661210596561 + 50.0 * 6.215412139892578
Epoch 2680, val loss: 1.588415265083313
Epoch 2690, training loss: 310.965087890625 = 0.012618137523531914 + 50.0 * 6.21904993057251
Epoch 2690, val loss: 1.5910197496414185
Epoch 2700, training loss: 310.8077392578125 = 0.012475795112550259 + 50.0 * 6.21590518951416
Epoch 2700, val loss: 1.5935243368148804
Epoch 2710, training loss: 310.7540283203125 = 0.012340675108134747 + 50.0 * 6.214833736419678
Epoch 2710, val loss: 1.5958977937698364
Epoch 2720, training loss: 310.78704833984375 = 0.012215869501233101 + 50.0 * 6.215497016906738
Epoch 2720, val loss: 1.5985645055770874
Epoch 2730, training loss: 311.10125732421875 = 0.012092064134776592 + 50.0 * 6.221783638000488
Epoch 2730, val loss: 1.6010067462921143
Epoch 2740, training loss: 310.8052978515625 = 0.011956552974879742 + 50.0 * 6.215866565704346
Epoch 2740, val loss: 1.6032780408859253
Epoch 2750, training loss: 310.71026611328125 = 0.01183677650988102 + 50.0 * 6.213968753814697
Epoch 2750, val loss: 1.6059306859970093
Epoch 2760, training loss: 310.8374938964844 = 0.011722488328814507 + 50.0 * 6.21651554107666
Epoch 2760, val loss: 1.608384609222412
Epoch 2770, training loss: 310.7917785644531 = 0.011598233133554459 + 50.0 * 6.215603828430176
Epoch 2770, val loss: 1.6106085777282715
Epoch 2780, training loss: 310.8503723144531 = 0.01147816888988018 + 50.0 * 6.216777801513672
Epoch 2780, val loss: 1.613150715827942
Epoch 2790, training loss: 310.709228515625 = 0.011360139586031437 + 50.0 * 6.213957786560059
Epoch 2790, val loss: 1.6153532266616821
Epoch 2800, training loss: 310.8013916015625 = 0.011251351796090603 + 50.0 * 6.2158026695251465
Epoch 2800, val loss: 1.6179603338241577
Epoch 2810, training loss: 310.6973571777344 = 0.01113899052143097 + 50.0 * 6.213724136352539
Epoch 2810, val loss: 1.620241403579712
Epoch 2820, training loss: 310.75701904296875 = 0.011031202971935272 + 50.0 * 6.2149200439453125
Epoch 2820, val loss: 1.6226726770401
Epoch 2830, training loss: 310.6532897949219 = 0.010922238230705261 + 50.0 * 6.2128472328186035
Epoch 2830, val loss: 1.6249244213104248
Epoch 2840, training loss: 310.89300537109375 = 0.010821537114679813 + 50.0 * 6.2176432609558105
Epoch 2840, val loss: 1.6271182298660278
Epoch 2850, training loss: 310.6128234863281 = 0.010710186325013638 + 50.0 * 6.212042331695557
Epoch 2850, val loss: 1.6294447183609009
Epoch 2860, training loss: 310.7489013671875 = 0.010611592791974545 + 50.0 * 6.214765548706055
Epoch 2860, val loss: 1.6316567659378052
Epoch 2870, training loss: 310.8370666503906 = 0.010507653467357159 + 50.0 * 6.216531276702881
Epoch 2870, val loss: 1.6336307525634766
Epoch 2880, training loss: 310.6844177246094 = 0.01040386501699686 + 50.0 * 6.213480472564697
Epoch 2880, val loss: 1.6360713243484497
Epoch 2890, training loss: 310.6152038574219 = 0.010308390483260155 + 50.0 * 6.212097644805908
Epoch 2890, val loss: 1.638250708580017
Epoch 2900, training loss: 310.781982421875 = 0.010215570218861103 + 50.0 * 6.215435028076172
Epoch 2900, val loss: 1.6405961513519287
Epoch 2910, training loss: 310.7060241699219 = 0.010119058191776276 + 50.0 * 6.2139177322387695
Epoch 2910, val loss: 1.6424953937530518
Epoch 2920, training loss: 310.6653747558594 = 0.010023601353168488 + 50.0 * 6.213107109069824
Epoch 2920, val loss: 1.644783616065979
Epoch 2930, training loss: 310.81378173828125 = 0.009935506619513035 + 50.0 * 6.216076850891113
Epoch 2930, val loss: 1.6468008756637573
Epoch 2940, training loss: 310.67315673828125 = 0.009838944301009178 + 50.0 * 6.213266372680664
Epoch 2940, val loss: 1.6488229036331177
Epoch 2950, training loss: 310.73834228515625 = 0.00974777527153492 + 50.0 * 6.214572429656982
Epoch 2950, val loss: 1.6509077548980713
Epoch 2960, training loss: 310.6600341796875 = 0.00965932384133339 + 50.0 * 6.21300745010376
Epoch 2960, val loss: 1.6531219482421875
Epoch 2970, training loss: 310.5907287597656 = 0.00957091897726059 + 50.0 * 6.211622714996338
Epoch 2970, val loss: 1.6552445888519287
Epoch 2980, training loss: 310.5445251464844 = 0.009489847347140312 + 50.0 * 6.210700988769531
Epoch 2980, val loss: 1.657530665397644
Epoch 2990, training loss: 310.7095947265625 = 0.009411880746483803 + 50.0 * 6.214004039764404
Epoch 2990, val loss: 1.6595553159713745
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7185185185185186
0.8397469688982605
=== training gcn model ===
Epoch 0, training loss: 431.7819519042969 = 1.939067006111145 + 50.0 * 8.596858024597168
Epoch 0, val loss: 1.9395955801010132
Epoch 10, training loss: 431.74407958984375 = 1.9304264783859253 + 50.0 * 8.596273422241211
Epoch 10, val loss: 1.9302923679351807
Epoch 20, training loss: 431.539794921875 = 1.9195820093154907 + 50.0 * 8.59240436553955
Epoch 20, val loss: 1.9185518026351929
Epoch 30, training loss: 430.2125244140625 = 1.905442237854004 + 50.0 * 8.566141128540039
Epoch 30, val loss: 1.9032708406448364
Epoch 40, training loss: 421.2730712890625 = 1.8881276845932007 + 50.0 * 8.387699127197266
Epoch 40, val loss: 1.885326623916626
Epoch 50, training loss: 385.9115295410156 = 1.8688879013061523 + 50.0 * 7.680852890014648
Epoch 50, val loss: 1.865993618965149
Epoch 60, training loss: 376.4280700683594 = 1.851639986038208 + 50.0 * 7.491528034210205
Epoch 60, val loss: 1.8494187593460083
Epoch 70, training loss: 363.3653259277344 = 1.8393696546554565 + 50.0 * 7.2305192947387695
Epoch 70, val loss: 1.8374360799789429
Epoch 80, training loss: 355.2274475097656 = 1.826875925064087 + 50.0 * 7.06801176071167
Epoch 80, val loss: 1.8250559568405151
Epoch 90, training loss: 349.189208984375 = 1.8140804767608643 + 50.0 * 6.947502136230469
Epoch 90, val loss: 1.8126778602600098
Epoch 100, training loss: 345.6420593261719 = 1.8019155263900757 + 50.0 * 6.876802921295166
Epoch 100, val loss: 1.8012111186981201
Epoch 110, training loss: 343.31170654296875 = 1.7901862859725952 + 50.0 * 6.830430030822754
Epoch 110, val loss: 1.7901543378829956
Epoch 120, training loss: 341.2802429199219 = 1.7781360149383545 + 50.0 * 6.790041923522949
Epoch 120, val loss: 1.7787394523620605
Epoch 130, training loss: 339.3939514160156 = 1.766007900238037 + 50.0 * 6.752558708190918
Epoch 130, val loss: 1.7674446105957031
Epoch 140, training loss: 337.6493225097656 = 1.75365149974823 + 50.0 * 6.7179131507873535
Epoch 140, val loss: 1.7563304901123047
Epoch 150, training loss: 335.8034973144531 = 1.740828514099121 + 50.0 * 6.681253910064697
Epoch 150, val loss: 1.7448118925094604
Epoch 160, training loss: 333.9652099609375 = 1.727124810218811 + 50.0 * 6.64476203918457
Epoch 160, val loss: 1.7326265573501587
Epoch 170, training loss: 332.37567138671875 = 1.7124184370040894 + 50.0 * 6.613264560699463
Epoch 170, val loss: 1.7193125486373901
Epoch 180, training loss: 331.0055847167969 = 1.6958104372024536 + 50.0 * 6.586195468902588
Epoch 180, val loss: 1.7045443058013916
Epoch 190, training loss: 329.8374328613281 = 1.6774555444717407 + 50.0 * 6.563199520111084
Epoch 190, val loss: 1.6882107257843018
Epoch 200, training loss: 328.845947265625 = 1.6574708223342896 + 50.0 * 6.543769836425781
Epoch 200, val loss: 1.6704094409942627
Epoch 210, training loss: 328.29248046875 = 1.6358788013458252 + 50.0 * 6.533132076263428
Epoch 210, val loss: 1.6511485576629639
Epoch 220, training loss: 327.32440185546875 = 1.612233281135559 + 50.0 * 6.514243125915527
Epoch 220, val loss: 1.6304532289505005
Epoch 230, training loss: 326.5911865234375 = 1.5871529579162598 + 50.0 * 6.500080585479736
Epoch 230, val loss: 1.608617901802063
Epoch 240, training loss: 326.202392578125 = 1.5605690479278564 + 50.0 * 6.4928364753723145
Epoch 240, val loss: 1.585635781288147
Epoch 250, training loss: 325.3957214355469 = 1.532677412033081 + 50.0 * 6.477260589599609
Epoch 250, val loss: 1.5615520477294922
Epoch 260, training loss: 324.8083801269531 = 1.5037671327590942 + 50.0 * 6.466092586517334
Epoch 260, val loss: 1.5369149446487427
Epoch 270, training loss: 324.3477783203125 = 1.4740145206451416 + 50.0 * 6.457475185394287
Epoch 270, val loss: 1.5117827653884888
Epoch 280, training loss: 323.9105529785156 = 1.4434250593185425 + 50.0 * 6.449342727661133
Epoch 280, val loss: 1.486015796661377
Epoch 290, training loss: 323.3367614746094 = 1.4127191305160522 + 50.0 * 6.438480854034424
Epoch 290, val loss: 1.4603970050811768
Epoch 300, training loss: 322.9246520996094 = 1.3819193840026855 + 50.0 * 6.430854320526123
Epoch 300, val loss: 1.4347649812698364
Epoch 310, training loss: 322.6012268066406 = 1.351256251335144 + 50.0 * 6.424999237060547
Epoch 310, val loss: 1.409420371055603
Epoch 320, training loss: 322.23260498046875 = 1.320624589920044 + 50.0 * 6.418239593505859
Epoch 320, val loss: 1.384479284286499
Epoch 330, training loss: 321.95654296875 = 1.2906240224838257 + 50.0 * 6.413318634033203
Epoch 330, val loss: 1.3601125478744507
Epoch 340, training loss: 321.4884948730469 = 1.2610405683517456 + 50.0 * 6.4045491218566895
Epoch 340, val loss: 1.3361806869506836
Epoch 350, training loss: 321.1690979003906 = 1.232161521911621 + 50.0 * 6.398738861083984
Epoch 350, val loss: 1.312865138053894
Epoch 360, training loss: 321.2680358886719 = 1.203800916671753 + 50.0 * 6.401284694671631
Epoch 360, val loss: 1.2902027368545532
Epoch 370, training loss: 320.71417236328125 = 1.1761949062347412 + 50.0 * 6.390758991241455
Epoch 370, val loss: 1.2681381702423096
Epoch 380, training loss: 320.36083984375 = 1.1492044925689697 + 50.0 * 6.384232997894287
Epoch 380, val loss: 1.2467083930969238
Epoch 390, training loss: 320.0952453613281 = 1.1229033470153809 + 50.0 * 6.379446983337402
Epoch 390, val loss: 1.2258177995681763
Epoch 400, training loss: 319.8825988769531 = 1.0969734191894531 + 50.0 * 6.3757123947143555
Epoch 400, val loss: 1.2053357362747192
Epoch 410, training loss: 319.60894775390625 = 1.0716534852981567 + 50.0 * 6.37074613571167
Epoch 410, val loss: 1.1853702068328857
Epoch 420, training loss: 319.40472412109375 = 1.0468746423721313 + 50.0 * 6.367156982421875
Epoch 420, val loss: 1.1658958196640015
Epoch 430, training loss: 319.2245788574219 = 1.0225310325622559 + 50.0 * 6.364041328430176
Epoch 430, val loss: 1.1466903686523438
Epoch 440, training loss: 319.0604248046875 = 0.9986012578010559 + 50.0 * 6.361236572265625
Epoch 440, val loss: 1.127864122390747
Epoch 450, training loss: 318.8333740234375 = 0.9749614596366882 + 50.0 * 6.357167720794678
Epoch 450, val loss: 1.109528660774231
Epoch 460, training loss: 318.5852966308594 = 0.9519274234771729 + 50.0 * 6.352667331695557
Epoch 460, val loss: 1.0915467739105225
Epoch 470, training loss: 318.5843505859375 = 0.9292018413543701 + 50.0 * 6.353103160858154
Epoch 470, val loss: 1.0739364624023438
Epoch 480, training loss: 318.3356018066406 = 0.9068750739097595 + 50.0 * 6.348575115203857
Epoch 480, val loss: 1.0564967393875122
Epoch 490, training loss: 318.0833740234375 = 0.8849644660949707 + 50.0 * 6.343967914581299
Epoch 490, val loss: 1.0395838022232056
Epoch 500, training loss: 318.0794372558594 = 0.8634187579154968 + 50.0 * 6.344319820404053
Epoch 500, val loss: 1.0231455564498901
Epoch 510, training loss: 317.7822265625 = 0.8423216938972473 + 50.0 * 6.3387980461120605
Epoch 510, val loss: 1.0069321393966675
Epoch 520, training loss: 317.61767578125 = 0.8216792941093445 + 50.0 * 6.3359198570251465
Epoch 520, val loss: 0.9914509057998657
Epoch 530, training loss: 317.4876403808594 = 0.8014907836914062 + 50.0 * 6.333723068237305
Epoch 530, val loss: 0.9764024615287781
Epoch 540, training loss: 317.54248046875 = 0.7816851139068604 + 50.0 * 6.335216045379639
Epoch 540, val loss: 0.961819589138031
Epoch 550, training loss: 317.29290771484375 = 0.7621997594833374 + 50.0 * 6.33061408996582
Epoch 550, val loss: 0.9474648833274841
Epoch 560, training loss: 317.04925537109375 = 0.7431087493896484 + 50.0 * 6.326122760772705
Epoch 560, val loss: 0.9337992072105408
Epoch 570, training loss: 316.9017028808594 = 0.7244825959205627 + 50.0 * 6.323544025421143
Epoch 570, val loss: 0.920678436756134
Epoch 580, training loss: 316.8543395996094 = 0.7062448263168335 + 50.0 * 6.322962284088135
Epoch 580, val loss: 0.9081860184669495
Epoch 590, training loss: 317.0899658203125 = 0.6882140636444092 + 50.0 * 6.328035354614258
Epoch 590, val loss: 0.8958325386047363
Epoch 600, training loss: 316.6583251953125 = 0.6706782579421997 + 50.0 * 6.3197526931762695
Epoch 600, val loss: 0.8840014934539795
Epoch 610, training loss: 316.4744567871094 = 0.6535137891769409 + 50.0 * 6.3164191246032715
Epoch 610, val loss: 0.87282794713974
Epoch 620, training loss: 316.3208923339844 = 0.6368260979652405 + 50.0 * 6.313681125640869
Epoch 620, val loss: 0.8621246218681335
Epoch 630, training loss: 316.2486877441406 = 0.6204934120178223 + 50.0 * 6.312563896179199
Epoch 630, val loss: 0.8519056439399719
Epoch 640, training loss: 316.1248474121094 = 0.6044304370880127 + 50.0 * 6.310408115386963
Epoch 640, val loss: 0.8419722318649292
Epoch 650, training loss: 316.1114501953125 = 0.5887426733970642 + 50.0 * 6.310454368591309
Epoch 650, val loss: 0.8325923681259155
Epoch 660, training loss: 315.9246520996094 = 0.57359379529953 + 50.0 * 6.307021141052246
Epoch 660, val loss: 0.823745846748352
Epoch 670, training loss: 315.9471740722656 = 0.5587357878684998 + 50.0 * 6.30776834487915
Epoch 670, val loss: 0.8153080344200134
Epoch 680, training loss: 315.7226867675781 = 0.5441668033599854 + 50.0 * 6.30357027053833
Epoch 680, val loss: 0.8072762489318848
Epoch 690, training loss: 315.87640380859375 = 0.5300504565238953 + 50.0 * 6.306926727294922
Epoch 690, val loss: 0.7995511293411255
Epoch 700, training loss: 315.6083984375 = 0.5162561535835266 + 50.0 * 6.30184268951416
Epoch 700, val loss: 0.7925394177436829
Epoch 710, training loss: 315.4674072265625 = 0.5028331875801086 + 50.0 * 6.299291133880615
Epoch 710, val loss: 0.7857421040534973
Epoch 720, training loss: 315.4820556640625 = 0.4897480607032776 + 50.0 * 6.299846172332764
Epoch 720, val loss: 0.7793681621551514
Epoch 730, training loss: 315.3048095703125 = 0.47702568769454956 + 50.0 * 6.296555519104004
Epoch 730, val loss: 0.7733639478683472
Epoch 740, training loss: 315.3548278808594 = 0.4646391272544861 + 50.0 * 6.29780387878418
Epoch 740, val loss: 0.7676941752433777
Epoch 750, training loss: 315.1316833496094 = 0.45242899656295776 + 50.0 * 6.293585300445557
Epoch 750, val loss: 0.7623811960220337
Epoch 760, training loss: 315.0904846191406 = 0.44059908390045166 + 50.0 * 6.292997360229492
Epoch 760, val loss: 0.7572529911994934
Epoch 770, training loss: 315.1043395996094 = 0.42906320095062256 + 50.0 * 6.2935051918029785
Epoch 770, val loss: 0.7525660991668701
Epoch 780, training loss: 314.9820556640625 = 0.4177386462688446 + 50.0 * 6.291286468505859
Epoch 780, val loss: 0.7480424046516418
Epoch 790, training loss: 314.88726806640625 = 0.40674686431884766 + 50.0 * 6.289610385894775
Epoch 790, val loss: 0.7438676357269287
Epoch 800, training loss: 314.8081970214844 = 0.3960452973842621 + 50.0 * 6.288242816925049
Epoch 800, val loss: 0.739936351776123
Epoch 810, training loss: 314.8131408691406 = 0.3856099247932434 + 50.0 * 6.28855037689209
Epoch 810, val loss: 0.7362082004547119
Epoch 820, training loss: 314.6411437988281 = 0.37536486983299255 + 50.0 * 6.28531551361084
Epoch 820, val loss: 0.7327920198440552
Epoch 830, training loss: 314.7261657714844 = 0.36542415618896484 + 50.0 * 6.287215232849121
Epoch 830, val loss: 0.7295665144920349
Epoch 840, training loss: 314.6240234375 = 0.3556712567806244 + 50.0 * 6.285366535186768
Epoch 840, val loss: 0.7263930439949036
Epoch 850, training loss: 314.46612548828125 = 0.34614256024360657 + 50.0 * 6.282399654388428
Epoch 850, val loss: 0.7236458659172058
Epoch 860, training loss: 314.4342041015625 = 0.33688944578170776 + 50.0 * 6.281946659088135
Epoch 860, val loss: 0.7209693193435669
Epoch 870, training loss: 314.4089660644531 = 0.3278813064098358 + 50.0 * 6.281621932983398
Epoch 870, val loss: 0.7185763716697693
Epoch 880, training loss: 314.2877502441406 = 0.3191439211368561 + 50.0 * 6.279371738433838
Epoch 880, val loss: 0.7163334488868713
Epoch 890, training loss: 314.40087890625 = 0.3106323778629303 + 50.0 * 6.28180456161499
Epoch 890, val loss: 0.7142693996429443
Epoch 900, training loss: 314.302734375 = 0.3022063672542572 + 50.0 * 6.280010223388672
Epoch 900, val loss: 0.7124317288398743
Epoch 910, training loss: 314.1280517578125 = 0.29408013820648193 + 50.0 * 6.276679515838623
Epoch 910, val loss: 0.7107105851173401
Epoch 920, training loss: 314.0066223144531 = 0.28620803356170654 + 50.0 * 6.274408340454102
Epoch 920, val loss: 0.7093347907066345
Epoch 930, training loss: 314.0315856933594 = 0.2785617411136627 + 50.0 * 6.275060176849365
Epoch 930, val loss: 0.7080966234207153
Epoch 940, training loss: 314.0184020996094 = 0.2710809111595154 + 50.0 * 6.274946212768555
Epoch 940, val loss: 0.7069246172904968
Epoch 950, training loss: 314.0118103027344 = 0.2637841999530792 + 50.0 * 6.274960517883301
Epoch 950, val loss: 0.706055760383606
Epoch 960, training loss: 313.8443908691406 = 0.25672605633735657 + 50.0 * 6.271753311157227
Epoch 960, val loss: 0.7052324414253235
Epoch 970, training loss: 313.8044128417969 = 0.24987903237342834 + 50.0 * 6.271090507507324
Epoch 970, val loss: 0.704587996006012
Epoch 980, training loss: 313.7869567871094 = 0.24325770139694214 + 50.0 * 6.2708740234375
Epoch 980, val loss: 0.70417720079422
Epoch 990, training loss: 313.8916320800781 = 0.23677745461463928 + 50.0 * 6.273097038269043
Epoch 990, val loss: 0.7038977742195129
Epoch 1000, training loss: 313.7381286621094 = 0.23042811453342438 + 50.0 * 6.270153522491455
Epoch 1000, val loss: 0.7037017345428467
Epoch 1010, training loss: 313.63519287109375 = 0.22428303956985474 + 50.0 * 6.268218517303467
Epoch 1010, val loss: 0.7036886215209961
Epoch 1020, training loss: 313.556396484375 = 0.21833735704421997 + 50.0 * 6.266761779785156
Epoch 1020, val loss: 0.7037808895111084
Epoch 1030, training loss: 313.5312805175781 = 0.21257713437080383 + 50.0 * 6.266373634338379
Epoch 1030, val loss: 0.7041099071502686
Epoch 1040, training loss: 313.8610534667969 = 0.20699487626552582 + 50.0 * 6.273081302642822
Epoch 1040, val loss: 0.7044503688812256
Epoch 1050, training loss: 313.5557861328125 = 0.20144836604595184 + 50.0 * 6.267086982727051
Epoch 1050, val loss: 0.7050803303718567
Epoch 1060, training loss: 313.4766845703125 = 0.19617493450641632 + 50.0 * 6.265610218048096
Epoch 1060, val loss: 0.7057044506072998
Epoch 1070, training loss: 313.47027587890625 = 0.19102437794208527 + 50.0 * 6.265585422515869
Epoch 1070, val loss: 0.7065446376800537
Epoch 1080, training loss: 313.4258728027344 = 0.18604062497615814 + 50.0 * 6.264796733856201
Epoch 1080, val loss: 0.7075011730194092
Epoch 1090, training loss: 313.33648681640625 = 0.18117395043373108 + 50.0 * 6.263106346130371
Epoch 1090, val loss: 0.7085766792297363
Epoch 1100, training loss: 313.43511962890625 = 0.17646470665931702 + 50.0 * 6.265173435211182
Epoch 1100, val loss: 0.7097963094711304
Epoch 1110, training loss: 313.2490539550781 = 0.1718491017818451 + 50.0 * 6.261544227600098
Epoch 1110, val loss: 0.7110887169837952
Epoch 1120, training loss: 313.1875915527344 = 0.16742457449436188 + 50.0 * 6.260403633117676
Epoch 1120, val loss: 0.7124692797660828
Epoch 1130, training loss: 313.1574401855469 = 0.16311578452587128 + 50.0 * 6.259886741638184
Epoch 1130, val loss: 0.7140534520149231
Epoch 1140, training loss: 313.5191955566406 = 0.15895108878612518 + 50.0 * 6.267204284667969
Epoch 1140, val loss: 0.7155695557594299
Epoch 1150, training loss: 313.1294860839844 = 0.1548355221748352 + 50.0 * 6.259492874145508
Epoch 1150, val loss: 0.7174435257911682
Epoch 1160, training loss: 313.0168762207031 = 0.15086330473423004 + 50.0 * 6.257320404052734
Epoch 1160, val loss: 0.7192730903625488
Epoch 1170, training loss: 313.05487060546875 = 0.14704960584640503 + 50.0 * 6.2581562995910645
Epoch 1170, val loss: 0.7212209701538086
Epoch 1180, training loss: 313.1759033203125 = 0.14333941042423248 + 50.0 * 6.260651111602783
Epoch 1180, val loss: 0.7233604192733765
Epoch 1190, training loss: 313.17156982421875 = 0.13970182836055756 + 50.0 * 6.260637283325195
Epoch 1190, val loss: 0.7253089547157288
Epoch 1200, training loss: 312.907958984375 = 0.13613510131835938 + 50.0 * 6.255436420440674
Epoch 1200, val loss: 0.7276265025138855
Epoch 1210, training loss: 312.898193359375 = 0.13272640109062195 + 50.0 * 6.255309581756592
Epoch 1210, val loss: 0.7299035787582397
Epoch 1220, training loss: 312.889892578125 = 0.12944164872169495 + 50.0 * 6.255208492279053
Epoch 1220, val loss: 0.7323172092437744
Epoch 1230, training loss: 312.9940490722656 = 0.1262234002351761 + 50.0 * 6.257356643676758
Epoch 1230, val loss: 0.7348067164421082
Epoch 1240, training loss: 312.8788146972656 = 0.12306949496269226 + 50.0 * 6.255115032196045
Epoch 1240, val loss: 0.7372276186943054
Epoch 1250, training loss: 312.8846435546875 = 0.12004078179597855 + 50.0 * 6.2552924156188965
Epoch 1250, val loss: 0.7398452758789062
Epoch 1260, training loss: 312.7935791015625 = 0.11706642806529999 + 50.0 * 6.253530502319336
Epoch 1260, val loss: 0.7424604296684265
Epoch 1270, training loss: 312.9320373535156 = 0.11419663578271866 + 50.0 * 6.256356716156006
Epoch 1270, val loss: 0.7451837658882141
Epoch 1280, training loss: 312.9479675292969 = 0.1113596111536026 + 50.0 * 6.256731986999512
Epoch 1280, val loss: 0.7480457425117493
Epoch 1290, training loss: 312.6861572265625 = 0.10861342400312424 + 50.0 * 6.251550674438477
Epoch 1290, val loss: 0.7506693005561829
Epoch 1300, training loss: 312.6201171875 = 0.10598956793546677 + 50.0 * 6.2502827644348145
Epoch 1300, val loss: 0.753688633441925
Epoch 1310, training loss: 312.58233642578125 = 0.10343823581933975 + 50.0 * 6.249578475952148
Epoch 1310, val loss: 0.7566888332366943
Epoch 1320, training loss: 312.5433349609375 = 0.10096343606710434 + 50.0 * 6.248847484588623
Epoch 1320, val loss: 0.7597313523292542
Epoch 1330, training loss: 312.8196105957031 = 0.09855150431394577 + 50.0 * 6.254421234130859
Epoch 1330, val loss: 0.7628615498542786
Epoch 1340, training loss: 312.7989501953125 = 0.09615668654441833 + 50.0 * 6.254055976867676
Epoch 1340, val loss: 0.7658461332321167
Epoch 1350, training loss: 312.5828552246094 = 0.09381896257400513 + 50.0 * 6.249780654907227
Epoch 1350, val loss: 0.7689275145530701
Epoch 1360, training loss: 312.4980773925781 = 0.09158085286617279 + 50.0 * 6.248129844665527
Epoch 1360, val loss: 0.7721424698829651
Epoch 1370, training loss: 312.5094909667969 = 0.08941537886857986 + 50.0 * 6.248401641845703
Epoch 1370, val loss: 0.7753628492355347
Epoch 1380, training loss: 312.7109069824219 = 0.08729048818349838 + 50.0 * 6.252472400665283
Epoch 1380, val loss: 0.7785341143608093
Epoch 1390, training loss: 312.4562072753906 = 0.08519401401281357 + 50.0 * 6.247419834136963
Epoch 1390, val loss: 0.7819770574569702
Epoch 1400, training loss: 312.38555908203125 = 0.08319275826215744 + 50.0 * 6.246047496795654
Epoch 1400, val loss: 0.7852877974510193
Epoch 1410, training loss: 312.4773254394531 = 0.08124848455190659 + 50.0 * 6.247921466827393
Epoch 1410, val loss: 0.7887445092201233
Epoch 1420, training loss: 312.43408203125 = 0.0793297216296196 + 50.0 * 6.247095584869385
Epoch 1420, val loss: 0.7920028567314148
Epoch 1430, training loss: 312.38653564453125 = 0.0774630606174469 + 50.0 * 6.246181488037109
Epoch 1430, val loss: 0.795290470123291
Epoch 1440, training loss: 312.4552001953125 = 0.07564828544855118 + 50.0 * 6.247591018676758
Epoch 1440, val loss: 0.7987343668937683
Epoch 1450, training loss: 312.33258056640625 = 0.07390911132097244 + 50.0 * 6.245173454284668
Epoch 1450, val loss: 0.802280604839325
Epoch 1460, training loss: 312.2673034667969 = 0.07220617681741714 + 50.0 * 6.24390172958374
Epoch 1460, val loss: 0.8056637644767761
Epoch 1470, training loss: 312.3799743652344 = 0.07055160403251648 + 50.0 * 6.246188163757324
Epoch 1470, val loss: 0.8091533780097961
Epoch 1480, training loss: 312.23004150390625 = 0.0689321979880333 + 50.0 * 6.243221759796143
Epoch 1480, val loss: 0.8126946091651917
Epoch 1490, training loss: 312.2549133300781 = 0.06736064702272415 + 50.0 * 6.243751049041748
Epoch 1490, val loss: 0.8161841034889221
Epoch 1500, training loss: 312.5638122558594 = 0.06583625078201294 + 50.0 * 6.249959468841553
Epoch 1500, val loss: 0.8196071982383728
Epoch 1510, training loss: 312.3282775878906 = 0.06429905444383621 + 50.0 * 6.245279312133789
Epoch 1510, val loss: 0.8232240676879883
Epoch 1520, training loss: 312.16864013671875 = 0.06282278150320053 + 50.0 * 6.2421159744262695
Epoch 1520, val loss: 0.8269124627113342
Epoch 1530, training loss: 312.1226501464844 = 0.06141402944922447 + 50.0 * 6.241225242614746
Epoch 1530, val loss: 0.8305338025093079
Epoch 1540, training loss: 312.1792907714844 = 0.06004359573125839 + 50.0 * 6.242384910583496
Epoch 1540, val loss: 0.8342253565788269
Epoch 1550, training loss: 312.1928405761719 = 0.05868286266922951 + 50.0 * 6.242683410644531
Epoch 1550, val loss: 0.8377802968025208
Epoch 1560, training loss: 312.0455627441406 = 0.05734742432832718 + 50.0 * 6.239764213562012
Epoch 1560, val loss: 0.8414430618286133
Epoch 1570, training loss: 312.01446533203125 = 0.05607590451836586 + 50.0 * 6.239168167114258
Epoch 1570, val loss: 0.8451899290084839
Epoch 1580, training loss: 312.0470886230469 = 0.05484192073345184 + 50.0 * 6.239844799041748
Epoch 1580, val loss: 0.8488532900810242
Epoch 1590, training loss: 312.25799560546875 = 0.05362319201231003 + 50.0 * 6.2440876960754395
Epoch 1590, val loss: 0.8525159358978271
Epoch 1600, training loss: 312.0502624511719 = 0.052441030740737915 + 50.0 * 6.239956378936768
Epoch 1600, val loss: 0.8562815189361572
Epoch 1610, training loss: 312.14752197265625 = 0.05129065364599228 + 50.0 * 6.241924285888672
Epoch 1610, val loss: 0.8599496483802795
Epoch 1620, training loss: 312.1544189453125 = 0.05015755444765091 + 50.0 * 6.242084980010986
Epoch 1620, val loss: 0.8637624382972717
Epoch 1630, training loss: 311.971435546875 = 0.04905659332871437 + 50.0 * 6.238447666168213
Epoch 1630, val loss: 0.8672945499420166
Epoch 1640, training loss: 311.9517517089844 = 0.04799859970808029 + 50.0 * 6.238074779510498
Epoch 1640, val loss: 0.8711057901382446
Epoch 1650, training loss: 311.8832702636719 = 0.046975813806056976 + 50.0 * 6.2367262840271
Epoch 1650, val loss: 0.8748440742492676
Epoch 1660, training loss: 311.90911865234375 = 0.04598420858383179 + 50.0 * 6.237262725830078
Epoch 1660, val loss: 0.8785837888717651
Epoch 1670, training loss: 312.24755859375 = 0.04500802233815193 + 50.0 * 6.244050979614258
Epoch 1670, val loss: 0.8823277354240417
Epoch 1680, training loss: 311.93780517578125 = 0.0440540611743927 + 50.0 * 6.237874984741211
Epoch 1680, val loss: 0.8858362436294556
Epoch 1690, training loss: 312.02496337890625 = 0.0431249663233757 + 50.0 * 6.2396368980407715
Epoch 1690, val loss: 0.8897372484207153
Epoch 1700, training loss: 311.88592529296875 = 0.04220874607563019 + 50.0 * 6.236874580383301
Epoch 1700, val loss: 0.8930500149726868
Epoch 1710, training loss: 311.8293151855469 = 0.04132406413555145 + 50.0 * 6.235759735107422
Epoch 1710, val loss: 0.8969810605049133
Epoch 1720, training loss: 311.78765869140625 = 0.04047900810837746 + 50.0 * 6.234943866729736
Epoch 1720, val loss: 0.9005241990089417
Epoch 1730, training loss: 311.7899169921875 = 0.039655979722738266 + 50.0 * 6.2350053787231445
Epoch 1730, val loss: 0.9042460918426514
Epoch 1740, training loss: 312.2168884277344 = 0.038858562707901 + 50.0 * 6.243560791015625
Epoch 1740, val loss: 0.9076692461967468
Epoch 1750, training loss: 312.056884765625 = 0.038041260093450546 + 50.0 * 6.2403764724731445
Epoch 1750, val loss: 0.91138756275177
Epoch 1760, training loss: 311.78973388671875 = 0.03725983947515488 + 50.0 * 6.235049724578857
Epoch 1760, val loss: 0.9149869680404663
Epoch 1770, training loss: 311.7102966308594 = 0.03651203215122223 + 50.0 * 6.233476161956787
Epoch 1770, val loss: 0.9186157584190369
Epoch 1780, training loss: 311.79052734375 = 0.035787709057331085 + 50.0 * 6.2350945472717285
Epoch 1780, val loss: 0.9222598671913147
Epoch 1790, training loss: 311.8752746582031 = 0.035069845616817474 + 50.0 * 6.236804008483887
Epoch 1790, val loss: 0.925722599029541
Epoch 1800, training loss: 311.789794921875 = 0.034373849630355835 + 50.0 * 6.235108375549316
Epoch 1800, val loss: 0.9293361902236938
Epoch 1810, training loss: 311.9994812011719 = 0.03370038419961929 + 50.0 * 6.239315509796143
Epoch 1810, val loss: 0.9328970313072205
Epoch 1820, training loss: 311.7370300292969 = 0.03303593024611473 + 50.0 * 6.234079360961914
Epoch 1820, val loss: 0.9360728859901428
Epoch 1830, training loss: 311.66680908203125 = 0.03239244967699051 + 50.0 * 6.2326884269714355
Epoch 1830, val loss: 0.9397806525230408
Epoch 1840, training loss: 311.8597717285156 = 0.03177673742175102 + 50.0 * 6.236560344696045
Epoch 1840, val loss: 0.9431480169296265
Epoch 1850, training loss: 311.6204833984375 = 0.03116065077483654 + 50.0 * 6.231786251068115
Epoch 1850, val loss: 0.9467599987983704
Epoch 1860, training loss: 311.6339416503906 = 0.03056703507900238 + 50.0 * 6.232067108154297
Epoch 1860, val loss: 0.9502232074737549
Epoch 1870, training loss: 311.89947509765625 = 0.02998880110681057 + 50.0 * 6.23738956451416
Epoch 1870, val loss: 0.9536356329917908
Epoch 1880, training loss: 311.605224609375 = 0.029426803812384605 + 50.0 * 6.231515884399414
Epoch 1880, val loss: 0.9568639397621155
Epoch 1890, training loss: 311.5690612792969 = 0.028879912570118904 + 50.0 * 6.230803966522217
Epoch 1890, val loss: 0.9603545069694519
Epoch 1900, training loss: 311.5760498046875 = 0.02834964357316494 + 50.0 * 6.230954170227051
Epoch 1900, val loss: 0.9637247920036316
Epoch 1910, training loss: 311.86932373046875 = 0.0278326366096735 + 50.0 * 6.23682975769043
Epoch 1910, val loss: 0.9669349789619446
Epoch 1920, training loss: 311.6465148925781 = 0.02731030248105526 + 50.0 * 6.232384204864502
Epoch 1920, val loss: 0.9703009724617004
Epoch 1930, training loss: 311.58319091796875 = 0.02681635692715645 + 50.0 * 6.2311272621154785
Epoch 1930, val loss: 0.9735882878303528
Epoch 1940, training loss: 311.6636962890625 = 0.026332078501582146 + 50.0 * 6.2327470779418945
Epoch 1940, val loss: 0.9768311381340027
Epoch 1950, training loss: 311.6956481933594 = 0.025858918204903603 + 50.0 * 6.233396053314209
Epoch 1950, val loss: 0.9800735116004944
Epoch 1960, training loss: 311.515869140625 = 0.02539629302918911 + 50.0 * 6.229809284210205
Epoch 1960, val loss: 0.9833490252494812
Epoch 1970, training loss: 311.4925537109375 = 0.02494833804666996 + 50.0 * 6.229351997375488
Epoch 1970, val loss: 0.9864825010299683
Epoch 1980, training loss: 311.50616455078125 = 0.024518003687262535 + 50.0 * 6.229633331298828
Epoch 1980, val loss: 0.9896115064620972
Epoch 1990, training loss: 311.65380859375 = 0.024095920845866203 + 50.0 * 6.232594013214111
Epoch 1990, val loss: 0.9928345680236816
Epoch 2000, training loss: 311.5022277832031 = 0.02366468869149685 + 50.0 * 6.229571342468262
Epoch 2000, val loss: 0.9960488677024841
Epoch 2010, training loss: 311.5575256347656 = 0.023256147280335426 + 50.0 * 6.230685234069824
Epoch 2010, val loss: 0.9989827275276184
Epoch 2020, training loss: 311.4198913574219 = 0.022854860872030258 + 50.0 * 6.227940559387207
Epoch 2020, val loss: 1.0021615028381348
Epoch 2030, training loss: 311.4039306640625 = 0.02247317135334015 + 50.0 * 6.227629661560059
Epoch 2030, val loss: 1.005162000656128
Epoch 2040, training loss: 311.6033630371094 = 0.02210678532719612 + 50.0 * 6.231625556945801
Epoch 2040, val loss: 1.0082460641860962
Epoch 2050, training loss: 311.4814758300781 = 0.02172558195888996 + 50.0 * 6.2291951179504395
Epoch 2050, val loss: 1.0111898183822632
Epoch 2060, training loss: 311.3681640625 = 0.021358145400881767 + 50.0 * 6.226935863494873
Epoch 2060, val loss: 1.014206051826477
Epoch 2070, training loss: 311.4093322753906 = 0.021007688716053963 + 50.0 * 6.227766990661621
Epoch 2070, val loss: 1.017130732536316
Epoch 2080, training loss: 311.591552734375 = 0.02066228538751602 + 50.0 * 6.231417655944824
Epoch 2080, val loss: 1.0202014446258545
Epoch 2090, training loss: 311.4065856933594 = 0.020329225808382034 + 50.0 * 6.227725505828857
Epoch 2090, val loss: 1.023004412651062
Epoch 2100, training loss: 311.3352355957031 = 0.019998542964458466 + 50.0 * 6.22630500793457
Epoch 2100, val loss: 1.026010274887085
Epoch 2110, training loss: 311.4613342285156 = 0.01968287117779255 + 50.0 * 6.228832721710205
Epoch 2110, val loss: 1.0287737846374512
Epoch 2120, training loss: 311.3089599609375 = 0.019360117614269257 + 50.0 * 6.225791931152344
Epoch 2120, val loss: 1.0317126512527466
Epoch 2130, training loss: 311.26580810546875 = 0.01905090920627117 + 50.0 * 6.224935054779053
Epoch 2130, val loss: 1.034480094909668
Epoch 2140, training loss: 311.2712707519531 = 0.01875239796936512 + 50.0 * 6.225050449371338
Epoch 2140, val loss: 1.037449598312378
Epoch 2150, training loss: 311.6172790527344 = 0.018465518951416016 + 50.0 * 6.23197603225708
Epoch 2150, val loss: 1.0402581691741943
Epoch 2160, training loss: 311.2662658691406 = 0.01817343384027481 + 50.0 * 6.224961757659912
Epoch 2160, val loss: 1.0428063869476318
Epoch 2170, training loss: 311.25238037109375 = 0.017893623560667038 + 50.0 * 6.224689960479736
Epoch 2170, val loss: 1.0455584526062012
Epoch 2180, training loss: 311.45892333984375 = 0.017626887187361717 + 50.0 * 6.228825569152832
Epoch 2180, val loss: 1.0483191013336182
Epoch 2190, training loss: 311.2610778808594 = 0.017353210598230362 + 50.0 * 6.224874496459961
Epoch 2190, val loss: 1.0509060621261597
Epoch 2200, training loss: 311.2951354980469 = 0.017091060057282448 + 50.0 * 6.225561141967773
Epoch 2200, val loss: 1.0537676811218262
Epoch 2210, training loss: 311.3359680175781 = 0.016841083765029907 + 50.0 * 6.226382255554199
Epoch 2210, val loss: 1.0562639236450195
Epoch 2220, training loss: 311.35052490234375 = 0.016591906547546387 + 50.0 * 6.226678371429443
Epoch 2220, val loss: 1.0589901208877563
Epoch 2230, training loss: 311.3275451660156 = 0.016342202201485634 + 50.0 * 6.226223945617676
Epoch 2230, val loss: 1.0615841150283813
Epoch 2240, training loss: 311.1955261230469 = 0.0160993542522192 + 50.0 * 6.223588466644287
Epoch 2240, val loss: 1.0641754865646362
Epoch 2250, training loss: 311.2112121582031 = 0.015866640955209732 + 50.0 * 6.223906993865967
Epoch 2250, val loss: 1.0668246746063232
Epoch 2260, training loss: 311.2236022949219 = 0.015642834827303886 + 50.0 * 6.224159240722656
Epoch 2260, val loss: 1.0694390535354614
Epoch 2270, training loss: 311.3099060058594 = 0.015417724847793579 + 50.0 * 6.225890159606934
Epoch 2270, val loss: 1.0719696283340454
Epoch 2280, training loss: 311.2252197265625 = 0.015195763669908047 + 50.0 * 6.224200248718262
Epoch 2280, val loss: 1.0743104219436646
Epoch 2290, training loss: 311.27923583984375 = 0.014979226514697075 + 50.0 * 6.225285053253174
Epoch 2290, val loss: 1.076906442642212
Epoch 2300, training loss: 311.24053955078125 = 0.014765565283596516 + 50.0 * 6.224515438079834
Epoch 2300, val loss: 1.079262614250183
Epoch 2310, training loss: 311.2980651855469 = 0.014561343006789684 + 50.0 * 6.225670337677002
Epoch 2310, val loss: 1.081834316253662
Epoch 2320, training loss: 311.20330810546875 = 0.014356057159602642 + 50.0 * 6.223779201507568
Epoch 2320, val loss: 1.0843604803085327
Epoch 2330, training loss: 311.2350158691406 = 0.014161976054310799 + 50.0 * 6.224417209625244
Epoch 2330, val loss: 1.0867613554000854
Epoch 2340, training loss: 311.0828552246094 = 0.013967028819024563 + 50.0 * 6.221377372741699
Epoch 2340, val loss: 1.088987112045288
Epoch 2350, training loss: 311.21319580078125 = 0.0137830451130867 + 50.0 * 6.223988056182861
Epoch 2350, val loss: 1.091378092765808
Epoch 2360, training loss: 311.3599853515625 = 0.013600703328847885 + 50.0 * 6.226927280426025
Epoch 2360, val loss: 1.093705177307129
Epoch 2370, training loss: 311.20489501953125 = 0.013409590348601341 + 50.0 * 6.223830223083496
Epoch 2370, val loss: 1.0960314273834229
Epoch 2380, training loss: 311.1860046386719 = 0.013229750096797943 + 50.0 * 6.223455429077148
Epoch 2380, val loss: 1.0982941389083862
Epoch 2390, training loss: 311.1474609375 = 0.013056734576821327 + 50.0 * 6.222687721252441
Epoch 2390, val loss: 1.1007088422775269
Epoch 2400, training loss: 311.0683288574219 = 0.01288687065243721 + 50.0 * 6.221108913421631
Epoch 2400, val loss: 1.1028634309768677
Epoch 2410, training loss: 311.162353515625 = 0.012723632156848907 + 50.0 * 6.222992420196533
Epoch 2410, val loss: 1.1051667928695679
Epoch 2420, training loss: 311.10858154296875 = 0.012557880952954292 + 50.0 * 6.221920967102051
Epoch 2420, val loss: 1.1075270175933838
Epoch 2430, training loss: 311.0744323730469 = 0.01239863783121109 + 50.0 * 6.221240997314453
Epoch 2430, val loss: 1.1096616983413696
Epoch 2440, training loss: 311.334228515625 = 0.012246271595358849 + 50.0 * 6.226439952850342
Epoch 2440, val loss: 1.1118454933166504
Epoch 2450, training loss: 311.0343322753906 = 0.01208264660090208 + 50.0 * 6.220444679260254
Epoch 2450, val loss: 1.1139973402023315
Epoch 2460, training loss: 311.00823974609375 = 0.011929746717214584 + 50.0 * 6.219926357269287
Epoch 2460, val loss: 1.1161315441131592
Epoch 2470, training loss: 311.0149230957031 = 0.011785032227635384 + 50.0 * 6.220062732696533
Epoch 2470, val loss: 1.1184130907058716
Epoch 2480, training loss: 311.2726745605469 = 0.011640522629022598 + 50.0 * 6.225220680236816
Epoch 2480, val loss: 1.120558261871338
Epoch 2490, training loss: 311.24310302734375 = 0.01149582490324974 + 50.0 * 6.2246317863464355
Epoch 2490, val loss: 1.122647762298584
Epoch 2500, training loss: 311.11761474609375 = 0.011349542066454887 + 50.0 * 6.222125053405762
Epoch 2500, val loss: 1.1247278451919556
Epoch 2510, training loss: 311.02789306640625 = 0.011214659549295902 + 50.0 * 6.220333576202393
Epoch 2510, val loss: 1.1266828775405884
Epoch 2520, training loss: 310.93609619140625 = 0.011080224066972733 + 50.0 * 6.218500137329102
Epoch 2520, val loss: 1.1289278268814087
Epoch 2530, training loss: 310.9568176269531 = 0.010952557437121868 + 50.0 * 6.218917369842529
Epoch 2530, val loss: 1.1309523582458496
Epoch 2540, training loss: 311.2913513183594 = 0.010830377228558064 + 50.0 * 6.225610256195068
Epoch 2540, val loss: 1.1327875852584839
Epoch 2550, training loss: 311.05810546875 = 0.01069548912346363 + 50.0 * 6.220948219299316
Epoch 2550, val loss: 1.1350657939910889
Epoch 2560, training loss: 310.9090881347656 = 0.01056673564016819 + 50.0 * 6.217970848083496
Epoch 2560, val loss: 1.1368764638900757
Epoch 2570, training loss: 311.03814697265625 = 0.010446630418300629 + 50.0 * 6.220554351806641
Epoch 2570, val loss: 1.1389648914337158
Epoch 2580, training loss: 310.9800109863281 = 0.010325421579182148 + 50.0 * 6.219393730163574
Epoch 2580, val loss: 1.140880823135376
Epoch 2590, training loss: 310.9400329589844 = 0.01020834594964981 + 50.0 * 6.218596458435059
Epoch 2590, val loss: 1.1430482864379883
Epoch 2600, training loss: 310.992431640625 = 0.01009336393326521 + 50.0 * 6.219646453857422
Epoch 2600, val loss: 1.1449981927871704
Epoch 2610, training loss: 311.06512451171875 = 0.009982126764953136 + 50.0 * 6.221102714538574
Epoch 2610, val loss: 1.146898865699768
Epoch 2620, training loss: 310.9683532714844 = 0.00986657477915287 + 50.0 * 6.219169616699219
Epoch 2620, val loss: 1.1486564874649048
Epoch 2630, training loss: 310.9287109375 = 0.009758980013430119 + 50.0 * 6.218379020690918
Epoch 2630, val loss: 1.150651454925537
Epoch 2640, training loss: 311.15240478515625 = 0.009653185494244099 + 50.0 * 6.222855091094971
Epoch 2640, val loss: 1.1523672342300415
Epoch 2650, training loss: 311.0304260253906 = 0.00954260490834713 + 50.0 * 6.220417499542236
Epoch 2650, val loss: 1.1542749404907227
Epoch 2660, training loss: 310.8882751464844 = 0.00943510327488184 + 50.0 * 6.21757698059082
Epoch 2660, val loss: 1.1563371419906616
Epoch 2670, training loss: 310.8398742675781 = 0.009335932321846485 + 50.0 * 6.216610431671143
Epoch 2670, val loss: 1.1580886840820312
Epoch 2680, training loss: 310.8067321777344 = 0.009239465929567814 + 50.0 * 6.215950012207031
Epoch 2680, val loss: 1.1599562168121338
Epoch 2690, training loss: 310.947509765625 = 0.009146320633590221 + 50.0 * 6.218767166137695
Epoch 2690, val loss: 1.1617718935012817
Epoch 2700, training loss: 310.9741516113281 = 0.009045321494340897 + 50.0 * 6.219302177429199
Epoch 2700, val loss: 1.1635266542434692
Epoch 2710, training loss: 310.85260009765625 = 0.008943304419517517 + 50.0 * 6.2168731689453125
Epoch 2710, val loss: 1.1651631593704224
Epoch 2720, training loss: 310.8207092285156 = 0.008850052021443844 + 50.0 * 6.2162370681762695
Epoch 2720, val loss: 1.167057991027832
Epoch 2730, training loss: 310.7959289550781 = 0.008760903961956501 + 50.0 * 6.215743541717529
Epoch 2730, val loss: 1.16878342628479
Epoch 2740, training loss: 310.8680725097656 = 0.008673926815390587 + 50.0 * 6.217187881469727
Epoch 2740, val loss: 1.1705477237701416
Epoch 2750, training loss: 311.037353515625 = 0.008587188087403774 + 50.0 * 6.220575332641602
Epoch 2750, val loss: 1.172223687171936
Epoch 2760, training loss: 311.0029602050781 = 0.008494424633681774 + 50.0 * 6.2198896408081055
Epoch 2760, val loss: 1.1741387844085693
Epoch 2770, training loss: 310.83349609375 = 0.00840845238417387 + 50.0 * 6.216501235961914
Epoch 2770, val loss: 1.1755400896072388
Epoch 2780, training loss: 310.85601806640625 = 0.008325351402163506 + 50.0 * 6.216953754425049
Epoch 2780, val loss: 1.1775202751159668
Epoch 2790, training loss: 310.9434509277344 = 0.008243135176599026 + 50.0 * 6.2187042236328125
Epoch 2790, val loss: 1.1789753437042236
Epoch 2800, training loss: 310.773193359375 = 0.008161379024386406 + 50.0 * 6.215301036834717
Epoch 2800, val loss: 1.1808922290802002
Epoch 2810, training loss: 310.7861022949219 = 0.008083322085440159 + 50.0 * 6.215560436248779
Epoch 2810, val loss: 1.182451844215393
Epoch 2820, training loss: 310.94732666015625 = 0.008006677031517029 + 50.0 * 6.218786716461182
Epoch 2820, val loss: 1.1840202808380127
Epoch 2830, training loss: 310.90435791015625 = 0.007928413338959217 + 50.0 * 6.217928409576416
Epoch 2830, val loss: 1.1857181787490845
Epoch 2840, training loss: 310.74822998046875 = 0.007850509136915207 + 50.0 * 6.214807510375977
Epoch 2840, val loss: 1.1870925426483154
Epoch 2850, training loss: 310.8067932128906 = 0.007775221951305866 + 50.0 * 6.215980529785156
Epoch 2850, val loss: 1.1889973878860474
Epoch 2860, training loss: 310.8533020019531 = 0.007704087998718023 + 50.0 * 6.216912269592285
Epoch 2860, val loss: 1.1903842687606812
Epoch 2870, training loss: 310.72882080078125 = 0.007630467414855957 + 50.0 * 6.214423656463623
Epoch 2870, val loss: 1.1919505596160889
Epoch 2880, training loss: 310.7425842285156 = 0.007560498081147671 + 50.0 * 6.214700222015381
Epoch 2880, val loss: 1.1935290098190308
Epoch 2890, training loss: 310.8204650878906 = 0.00749141676351428 + 50.0 * 6.216259956359863
Epoch 2890, val loss: 1.1950905323028564
Epoch 2900, training loss: 310.8820495605469 = 0.007422494702041149 + 50.0 * 6.217492580413818
Epoch 2900, val loss: 1.196549654006958
Epoch 2910, training loss: 310.7431335449219 = 0.007351709064096212 + 50.0 * 6.214715480804443
Epoch 2910, val loss: 1.198085904121399
Epoch 2920, training loss: 310.77276611328125 = 0.007285634987056255 + 50.0 * 6.2153096199035645
Epoch 2920, val loss: 1.1996163129806519
Epoch 2930, training loss: 310.8318786621094 = 0.007219200022518635 + 50.0 * 6.216493129730225
Epoch 2930, val loss: 1.201014518737793
Epoch 2940, training loss: 310.7300720214844 = 0.007156723644584417 + 50.0 * 6.214457988739014
Epoch 2940, val loss: 1.2025855779647827
Epoch 2950, training loss: 310.9372863769531 = 0.00709260068833828 + 50.0 * 6.21860408782959
Epoch 2950, val loss: 1.2039415836334229
Epoch 2960, training loss: 310.702392578125 = 0.007027663756161928 + 50.0 * 6.213907241821289
Epoch 2960, val loss: 1.205517292022705
Epoch 2970, training loss: 310.62615966796875 = 0.006966886110603809 + 50.0 * 6.212384223937988
Epoch 2970, val loss: 1.2068997621536255
Epoch 2980, training loss: 310.645263671875 = 0.006909512914717197 + 50.0 * 6.212767124176025
Epoch 2980, val loss: 1.2084252834320068
Epoch 2990, training loss: 310.7994689941406 = 0.006852170452475548 + 50.0 * 6.215851783752441
Epoch 2990, val loss: 1.2098437547683716
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 431.79248046875 = 1.9500226974487305 + 50.0 * 8.59684944152832
Epoch 0, val loss: 1.953338861465454
Epoch 10, training loss: 431.74688720703125 = 1.939846396446228 + 50.0 * 8.59614086151123
Epoch 10, val loss: 1.9425561428070068
Epoch 20, training loss: 431.48760986328125 = 1.9272680282592773 + 50.0 * 8.591206550598145
Epoch 20, val loss: 1.9290688037872314
Epoch 30, training loss: 429.9478759765625 = 1.9112935066223145 + 50.0 * 8.560731887817383
Epoch 30, val loss: 1.9118856191635132
Epoch 40, training loss: 422.3102111816406 = 1.8931828737258911 + 50.0 * 8.408340454101562
Epoch 40, val loss: 1.8930541276931763
Epoch 50, training loss: 394.71978759765625 = 1.87479829788208 + 50.0 * 7.856899738311768
Epoch 50, val loss: 1.8734902143478394
Epoch 60, training loss: 374.4455261230469 = 1.858967900276184 + 50.0 * 7.451731204986572
Epoch 60, val loss: 1.8578205108642578
Epoch 70, training loss: 362.89263916015625 = 1.8473225831985474 + 50.0 * 7.2209062576293945
Epoch 70, val loss: 1.8458958864212036
Epoch 80, training loss: 356.03021240234375 = 1.833219289779663 + 50.0 * 7.083939552307129
Epoch 80, val loss: 1.83149254322052
Epoch 90, training loss: 350.35467529296875 = 1.820785403251648 + 50.0 * 6.970678329467773
Epoch 90, val loss: 1.8194106817245483
Epoch 100, training loss: 345.7761535644531 = 1.810786485671997 + 50.0 * 6.879307270050049
Epoch 100, val loss: 1.8097718954086304
Epoch 110, training loss: 342.0555419921875 = 1.8004599809646606 + 50.0 * 6.8051018714904785
Epoch 110, val loss: 1.799680471420288
Epoch 120, training loss: 339.3780517578125 = 1.7894130945205688 + 50.0 * 6.751772880554199
Epoch 120, val loss: 1.7892001867294312
Epoch 130, training loss: 337.3122863769531 = 1.7781838178634644 + 50.0 * 6.710681915283203
Epoch 130, val loss: 1.7786219120025635
Epoch 140, training loss: 335.66741943359375 = 1.7664098739624023 + 50.0 * 6.678020000457764
Epoch 140, val loss: 1.7675753831863403
Epoch 150, training loss: 334.32342529296875 = 1.7538126707077026 + 50.0 * 6.651392459869385
Epoch 150, val loss: 1.7560594081878662
Epoch 160, training loss: 333.24090576171875 = 1.7403596639633179 + 50.0 * 6.630011081695557
Epoch 160, val loss: 1.7438561916351318
Epoch 170, training loss: 332.02734375 = 1.725854516029358 + 50.0 * 6.606029510498047
Epoch 170, val loss: 1.730653166770935
Epoch 180, training loss: 331.0628967285156 = 1.7100727558135986 + 50.0 * 6.5870561599731445
Epoch 180, val loss: 1.7164387702941895
Epoch 190, training loss: 330.3457946777344 = 1.692896842956543 + 50.0 * 6.573057651519775
Epoch 190, val loss: 1.7010518312454224
Epoch 200, training loss: 329.4515380859375 = 1.6739386320114136 + 50.0 * 6.555551528930664
Epoch 200, val loss: 1.684336543083191
Epoch 210, training loss: 328.6700744628906 = 1.6533381938934326 + 50.0 * 6.540334701538086
Epoch 210, val loss: 1.6662392616271973
Epoch 220, training loss: 327.96112060546875 = 1.631003737449646 + 50.0 * 6.526602268218994
Epoch 220, val loss: 1.6467214822769165
Epoch 230, training loss: 327.3992919921875 = 1.6066981554031372 + 50.0 * 6.515851974487305
Epoch 230, val loss: 1.6258111000061035
Epoch 240, training loss: 326.67327880859375 = 1.5806734561920166 + 50.0 * 6.501852035522461
Epoch 240, val loss: 1.6034153699874878
Epoch 250, training loss: 326.0299377441406 = 1.5528132915496826 + 50.0 * 6.489542484283447
Epoch 250, val loss: 1.57978093624115
Epoch 260, training loss: 325.5542907714844 = 1.5232212543487549 + 50.0 * 6.480621337890625
Epoch 260, val loss: 1.554923415184021
Epoch 270, training loss: 325.04266357421875 = 1.4920017719268799 + 50.0 * 6.471013069152832
Epoch 270, val loss: 1.5287224054336548
Epoch 280, training loss: 324.47442626953125 = 1.4591113328933716 + 50.0 * 6.460306167602539
Epoch 280, val loss: 1.5018612146377563
Epoch 290, training loss: 324.0088195800781 = 1.425288438796997 + 50.0 * 6.4516706466674805
Epoch 290, val loss: 1.4744586944580078
Epoch 300, training loss: 323.6875 = 1.3905166387557983 + 50.0 * 6.445940017700195
Epoch 300, val loss: 1.4468064308166504
Epoch 310, training loss: 323.3188171386719 = 1.3551100492477417 + 50.0 * 6.439274311065674
Epoch 310, val loss: 1.4191207885742188
Epoch 320, training loss: 322.9107666015625 = 1.319319248199463 + 50.0 * 6.431829452514648
Epoch 320, val loss: 1.3916720151901245
Epoch 330, training loss: 322.5613098144531 = 1.2836241722106934 + 50.0 * 6.425553321838379
Epoch 330, val loss: 1.3647032976150513
Epoch 340, training loss: 322.2232666015625 = 1.2482037544250488 + 50.0 * 6.419501304626465
Epoch 340, val loss: 1.3383716344833374
Epoch 350, training loss: 322.1957702636719 = 1.2133194208145142 + 50.0 * 6.419649124145508
Epoch 350, val loss: 1.3127700090408325
Epoch 360, training loss: 321.73809814453125 = 1.178766131401062 + 50.0 * 6.411186218261719
Epoch 360, val loss: 1.2879937887191772
Epoch 370, training loss: 321.3402099609375 = 1.1452022790908813 + 50.0 * 6.403900146484375
Epoch 370, val loss: 1.264102578163147
Epoch 380, training loss: 321.0459289550781 = 1.1125181913375854 + 50.0 * 6.39866828918457
Epoch 380, val loss: 1.241229772567749
Epoch 390, training loss: 320.9639587402344 = 1.0807803869247437 + 50.0 * 6.397663593292236
Epoch 390, val loss: 1.2194246053695679
Epoch 400, training loss: 320.5326232910156 = 1.0498369932174683 + 50.0 * 6.389655590057373
Epoch 400, val loss: 1.1984376907348633
Epoch 410, training loss: 320.3638916015625 = 1.020107626914978 + 50.0 * 6.386875629425049
Epoch 410, val loss: 1.1785475015640259
Epoch 420, training loss: 320.03363037109375 = 0.9914314150810242 + 50.0 * 6.3808441162109375
Epoch 420, val loss: 1.1597497463226318
Epoch 430, training loss: 320.144775390625 = 0.9639105200767517 + 50.0 * 6.383617401123047
Epoch 430, val loss: 1.1419999599456787
Epoch 440, training loss: 319.7205810546875 = 0.9372944235801697 + 50.0 * 6.375665664672852
Epoch 440, val loss: 1.125289797782898
Epoch 450, training loss: 319.422607421875 = 0.9119146466255188 + 50.0 * 6.370213508605957
Epoch 450, val loss: 1.1095852851867676
Epoch 460, training loss: 319.1947937011719 = 0.8876317739486694 + 50.0 * 6.366143226623535
Epoch 460, val loss: 1.0949549674987793
Epoch 470, training loss: 319.0457763671875 = 0.864470362663269 + 50.0 * 6.363626003265381
Epoch 470, val loss: 1.081262469291687
Epoch 480, training loss: 318.7962951660156 = 0.8423125743865967 + 50.0 * 6.359079360961914
Epoch 480, val loss: 1.0683417320251465
Epoch 490, training loss: 318.72808837890625 = 0.8210892677307129 + 50.0 * 6.358139991760254
Epoch 490, val loss: 1.0563217401504517
Epoch 500, training loss: 318.41522216796875 = 0.8008335828781128 + 50.0 * 6.352287292480469
Epoch 500, val loss: 1.045176386833191
Epoch 510, training loss: 318.296630859375 = 0.7815438508987427 + 50.0 * 6.350301742553711
Epoch 510, val loss: 1.0347800254821777
Epoch 520, training loss: 318.2706298828125 = 0.7629310488700867 + 50.0 * 6.350153923034668
Epoch 520, val loss: 1.024998664855957
Epoch 530, training loss: 317.9803771972656 = 0.7449893951416016 + 50.0 * 6.344707489013672
Epoch 530, val loss: 1.015733242034912
Epoch 540, training loss: 317.7589416503906 = 0.72781902551651 + 50.0 * 6.340622425079346
Epoch 540, val loss: 1.0071340799331665
Epoch 550, training loss: 317.60308837890625 = 0.7112542390823364 + 50.0 * 6.337837219238281
Epoch 550, val loss: 0.9990972280502319
Epoch 560, training loss: 317.6620788574219 = 0.6951158046722412 + 50.0 * 6.339339256286621
Epoch 560, val loss: 0.9913104176521301
Epoch 570, training loss: 317.4068298339844 = 0.6794665455818176 + 50.0 * 6.33454704284668
Epoch 570, val loss: 0.9839459657669067
Epoch 580, training loss: 317.28887939453125 = 0.6641656160354614 + 50.0 * 6.332494258880615
Epoch 580, val loss: 0.9767795205116272
Epoch 590, training loss: 317.03314208984375 = 0.6492600440979004 + 50.0 * 6.3276777267456055
Epoch 590, val loss: 0.969967782497406
Epoch 600, training loss: 316.941162109375 = 0.6347214579582214 + 50.0 * 6.3261284828186035
Epoch 600, val loss: 0.963442325592041
Epoch 610, training loss: 316.90325927734375 = 0.62038254737854 + 50.0 * 6.325657367706299
Epoch 610, val loss: 0.9571128487586975
Epoch 620, training loss: 316.7888488769531 = 0.6063085198402405 + 50.0 * 6.323650360107422
Epoch 620, val loss: 0.9509035348892212
Epoch 630, training loss: 316.7072448730469 = 0.5923886299133301 + 50.0 * 6.322297096252441
Epoch 630, val loss: 0.9448659420013428
Epoch 640, training loss: 316.47601318359375 = 0.578681468963623 + 50.0 * 6.317946910858154
Epoch 640, val loss: 0.9389736652374268
Epoch 650, training loss: 316.3397521972656 = 0.5652429461479187 + 50.0 * 6.315490245819092
Epoch 650, val loss: 0.9334288239479065
Epoch 660, training loss: 316.3457336425781 = 0.5519906282424927 + 50.0 * 6.3158745765686035
Epoch 660, val loss: 0.9280477166175842
Epoch 670, training loss: 316.15948486328125 = 0.5389188528060913 + 50.0 * 6.312411785125732
Epoch 670, val loss: 0.9230400919914246
Epoch 680, training loss: 316.0691223144531 = 0.5259954333305359 + 50.0 * 6.3108625411987305
Epoch 680, val loss: 0.9180065393447876
Epoch 690, training loss: 316.27239990234375 = 0.5133652687072754 + 50.0 * 6.315180778503418
Epoch 690, val loss: 0.913411557674408
Epoch 700, training loss: 316.05780029296875 = 0.5008112788200378 + 50.0 * 6.3111395835876465
Epoch 700, val loss: 0.9089847207069397
Epoch 710, training loss: 315.82855224609375 = 0.4885411262512207 + 50.0 * 6.30679988861084
Epoch 710, val loss: 0.904870867729187
Epoch 720, training loss: 315.6513366699219 = 0.4765447974205017 + 50.0 * 6.30349588394165
Epoch 720, val loss: 0.901141345500946
Epoch 730, training loss: 315.550048828125 = 0.46481558680534363 + 50.0 * 6.3017048835754395
Epoch 730, val loss: 0.8977550268173218
Epoch 740, training loss: 315.971923828125 = 0.45326465368270874 + 50.0 * 6.310372829437256
Epoch 740, val loss: 0.8944615125656128
Epoch 750, training loss: 315.4924621582031 = 0.4419282376766205 + 50.0 * 6.301010608673096
Epoch 750, val loss: 0.8915830850601196
Epoch 760, training loss: 315.3736877441406 = 0.43084779381752014 + 50.0 * 6.298856735229492
Epoch 760, val loss: 0.8891652822494507
Epoch 770, training loss: 315.5414123535156 = 0.42001965641975403 + 50.0 * 6.302428245544434
Epoch 770, val loss: 0.8868914246559143
Epoch 780, training loss: 315.319091796875 = 0.40943509340286255 + 50.0 * 6.298193454742432
Epoch 780, val loss: 0.8849191665649414
Epoch 790, training loss: 315.1362609863281 = 0.399059534072876 + 50.0 * 6.294744491577148
Epoch 790, val loss: 0.8833755254745483
Epoch 800, training loss: 315.0341796875 = 0.38897955417633057 + 50.0 * 6.292903900146484
Epoch 800, val loss: 0.8821544051170349
Epoch 810, training loss: 315.61456298828125 = 0.3791242837905884 + 50.0 * 6.304708957672119
Epoch 810, val loss: 0.8813470005989075
Epoch 820, training loss: 314.9294128417969 = 0.3693157732486725 + 50.0 * 6.291201591491699
Epoch 820, val loss: 0.8801953196525574
Epoch 830, training loss: 314.854736328125 = 0.35980144143104553 + 50.0 * 6.289898872375488
Epoch 830, val loss: 0.8795562386512756
Epoch 840, training loss: 314.73681640625 = 0.35055211186408997 + 50.0 * 6.28772497177124
Epoch 840, val loss: 0.8794680833816528
Epoch 850, training loss: 315.1177673339844 = 0.3414963483810425 + 50.0 * 6.295525550842285
Epoch 850, val loss: 0.8792806267738342
Epoch 860, training loss: 314.8981628417969 = 0.3324882388114929 + 50.0 * 6.291313171386719
Epoch 860, val loss: 0.8794819712638855
Epoch 870, training loss: 314.6518249511719 = 0.3237314224243164 + 50.0 * 6.286561965942383
Epoch 870, val loss: 0.8798706531524658
Epoch 880, training loss: 314.5000305175781 = 0.3152120113372803 + 50.0 * 6.283696174621582
Epoch 880, val loss: 0.8804898858070374
Epoch 890, training loss: 314.6248474121094 = 0.3068767786026001 + 50.0 * 6.2863593101501465
Epoch 890, val loss: 0.8813292384147644
Epoch 900, training loss: 314.39373779296875 = 0.29865697026252747 + 50.0 * 6.281901836395264
Epoch 900, val loss: 0.8822208046913147
Epoch 910, training loss: 314.4488220214844 = 0.29061320424079895 + 50.0 * 6.283164024353027
Epoch 910, val loss: 0.8834239840507507
Epoch 920, training loss: 314.2674865722656 = 0.2827417552471161 + 50.0 * 6.2796950340271
Epoch 920, val loss: 0.8846960067749023
Epoch 930, training loss: 314.3932189941406 = 0.27504095435142517 + 50.0 * 6.282363414764404
Epoch 930, val loss: 0.8861373662948608
Epoch 940, training loss: 314.2427673339844 = 0.2674999535083771 + 50.0 * 6.279505729675293
Epoch 940, val loss: 0.8879312872886658
Epoch 950, training loss: 314.11932373046875 = 0.2601240575313568 + 50.0 * 6.277184009552002
Epoch 950, val loss: 0.889740526676178
Epoch 960, training loss: 314.0435791015625 = 0.2529308795928955 + 50.0 * 6.275813102722168
Epoch 960, val loss: 0.8918617367744446
Epoch 970, training loss: 314.3437805175781 = 0.24593910574913025 + 50.0 * 6.281956672668457
Epoch 970, val loss: 0.8943061828613281
Epoch 980, training loss: 314.0787048339844 = 0.23903250694274902 + 50.0 * 6.276793479919434
Epoch 980, val loss: 0.8960376381874084
Epoch 990, training loss: 313.92791748046875 = 0.23234230279922485 + 50.0 * 6.273911952972412
Epoch 990, val loss: 0.8988045454025269
Epoch 1000, training loss: 314.14630126953125 = 0.22581911087036133 + 50.0 * 6.278409481048584
Epoch 1000, val loss: 0.9014521837234497
Epoch 1010, training loss: 313.89984130859375 = 0.21945367753505707 + 50.0 * 6.2736077308654785
Epoch 1010, val loss: 0.9039446711540222
Epoch 1020, training loss: 313.7900085449219 = 0.21325235068798065 + 50.0 * 6.2715349197387695
Epoch 1020, val loss: 0.9070838689804077
Epoch 1030, training loss: 313.7579040527344 = 0.2072305977344513 + 50.0 * 6.271013259887695
Epoch 1030, val loss: 0.9099323749542236
Epoch 1040, training loss: 313.79388427734375 = 0.20136409997940063 + 50.0 * 6.2718505859375
Epoch 1040, val loss: 0.9131653904914856
Epoch 1050, training loss: 313.6597595214844 = 0.19566375017166138 + 50.0 * 6.269281387329102
Epoch 1050, val loss: 0.9165490865707397
Epoch 1060, training loss: 313.7135925292969 = 0.19010870158672333 + 50.0 * 6.270470142364502
Epoch 1060, val loss: 0.919978678226471
Epoch 1070, training loss: 313.6751708984375 = 0.18467575311660767 + 50.0 * 6.269810199737549
Epoch 1070, val loss: 0.923522412776947
Epoch 1080, training loss: 313.47412109375 = 0.17940828204154968 + 50.0 * 6.265894412994385
Epoch 1080, val loss: 0.927031934261322
Epoch 1090, training loss: 313.517822265625 = 0.174293652176857 + 50.0 * 6.266870975494385
Epoch 1090, val loss: 0.9306308031082153
Epoch 1100, training loss: 313.53765869140625 = 0.16931194067001343 + 50.0 * 6.267366886138916
Epoch 1100, val loss: 0.9347622990608215
Epoch 1110, training loss: 313.4173278808594 = 0.16448678076267242 + 50.0 * 6.265056610107422
Epoch 1110, val loss: 0.9385108947753906
Epoch 1120, training loss: 313.3375549316406 = 0.15978236496448517 + 50.0 * 6.26355504989624
Epoch 1120, val loss: 0.9425471425056458
Epoch 1130, training loss: 313.360107421875 = 0.15525409579277039 + 50.0 * 6.264097213745117
Epoch 1130, val loss: 0.9465463161468506
Epoch 1140, training loss: 313.46478271484375 = 0.15082810819149017 + 50.0 * 6.266279220581055
Epoch 1140, val loss: 0.9506767988204956
Epoch 1150, training loss: 313.4237365722656 = 0.14654164016246796 + 50.0 * 6.2655439376831055
Epoch 1150, val loss: 0.9548947811126709
Epoch 1160, training loss: 313.16497802734375 = 0.14236098527908325 + 50.0 * 6.2604522705078125
Epoch 1160, val loss: 0.9592595100402832
Epoch 1170, training loss: 313.11138916015625 = 0.13833315670490265 + 50.0 * 6.259460926055908
Epoch 1170, val loss: 0.9637627601623535
Epoch 1180, training loss: 313.0884094238281 = 0.13445612788200378 + 50.0 * 6.2590789794921875
Epoch 1180, val loss: 0.9682524800300598
Epoch 1190, training loss: 313.4081726074219 = 0.13069087266921997 + 50.0 * 6.265549659729004
Epoch 1190, val loss: 0.9729739427566528
Epoch 1200, training loss: 313.34942626953125 = 0.12699803709983826 + 50.0 * 6.264449119567871
Epoch 1200, val loss: 0.9768425822257996
Epoch 1210, training loss: 312.9925231933594 = 0.12342014908790588 + 50.0 * 6.257382392883301
Epoch 1210, val loss: 0.9818143248558044
Epoch 1220, training loss: 313.0782470703125 = 0.11998654156923294 + 50.0 * 6.259165287017822
Epoch 1220, val loss: 0.9867015480995178
Epoch 1230, training loss: 313.02716064453125 = 0.11665288358926773 + 50.0 * 6.2582106590271
Epoch 1230, val loss: 0.991317629814148
Epoch 1240, training loss: 313.0091247558594 = 0.11342360079288483 + 50.0 * 6.257914066314697
Epoch 1240, val loss: 0.9958046078681946
Epoch 1250, training loss: 312.9458923339844 = 0.11029177904129028 + 50.0 * 6.256711959838867
Epoch 1250, val loss: 1.0007129907608032
Epoch 1260, training loss: 312.8590087890625 = 0.10727225989103317 + 50.0 * 6.255034923553467
Epoch 1260, val loss: 1.0057038068771362
Epoch 1270, training loss: 313.15692138671875 = 0.10434500128030777 + 50.0 * 6.261051654815674
Epoch 1270, val loss: 1.010643720626831
Epoch 1280, training loss: 312.8453674316406 = 0.10149179399013519 + 50.0 * 6.254877090454102
Epoch 1280, val loss: 1.0152029991149902
Epoch 1290, training loss: 312.77960205078125 = 0.09874337911605835 + 50.0 * 6.253616809844971
Epoch 1290, val loss: 1.0203921794891357
Epoch 1300, training loss: 313.0581970214844 = 0.09608551114797592 + 50.0 * 6.259242057800293
Epoch 1300, val loss: 1.0250829458236694
Epoch 1310, training loss: 312.7795104980469 = 0.09349828213453293 + 50.0 * 6.253719806671143
Epoch 1310, val loss: 1.030145287513733
Epoch 1320, training loss: 312.8529052734375 = 0.09102295339107513 + 50.0 * 6.255237579345703
Epoch 1320, val loss: 1.0351591110229492
Epoch 1330, training loss: 312.66015625 = 0.08859864622354507 + 50.0 * 6.251431465148926
Epoch 1330, val loss: 1.039910912513733
Epoch 1340, training loss: 312.6225891113281 = 0.08625955134630203 + 50.0 * 6.250726222991943
Epoch 1340, val loss: 1.0447551012039185
Epoch 1350, training loss: 312.59454345703125 = 0.08401758968830109 + 50.0 * 6.250210285186768
Epoch 1350, val loss: 1.0500088930130005
Epoch 1360, training loss: 312.8266296386719 = 0.08183940500020981 + 50.0 * 6.2548956871032715
Epoch 1360, val loss: 1.0550791025161743
Epoch 1370, training loss: 312.6297607421875 = 0.07971609383821487 + 50.0 * 6.251000881195068
Epoch 1370, val loss: 1.059460163116455
Epoch 1380, training loss: 312.5301208496094 = 0.0776715874671936 + 50.0 * 6.249049186706543
Epoch 1380, val loss: 1.0648032426834106
Epoch 1390, training loss: 312.4732360839844 = 0.07568924129009247 + 50.0 * 6.247951030731201
Epoch 1390, val loss: 1.069723129272461
Epoch 1400, training loss: 312.8494873046875 = 0.0737830400466919 + 50.0 * 6.255513668060303
Epoch 1400, val loss: 1.074724793434143
Epoch 1410, training loss: 312.805908203125 = 0.07190842181444168 + 50.0 * 6.254680156707764
Epoch 1410, val loss: 1.0794878005981445
Epoch 1420, training loss: 312.4286193847656 = 0.07008329033851624 + 50.0 * 6.247170925140381
Epoch 1420, val loss: 1.084765911102295
Epoch 1430, training loss: 312.3927917480469 = 0.0683431625366211 + 50.0 * 6.24648904800415
Epoch 1430, val loss: 1.0897589921951294
Epoch 1440, training loss: 312.7568664550781 = 0.06668002158403397 + 50.0 * 6.253803730010986
Epoch 1440, val loss: 1.0949934720993042
Epoch 1450, training loss: 312.4769287109375 = 0.06500742584466934 + 50.0 * 6.248238563537598
Epoch 1450, val loss: 1.0993095636367798
Epoch 1460, training loss: 312.3825378417969 = 0.06342585384845734 + 50.0 * 6.246382236480713
Epoch 1460, val loss: 1.104570746421814
Epoch 1470, training loss: 312.3068542480469 = 0.06188652291893959 + 50.0 * 6.244899272918701
Epoch 1470, val loss: 1.1095117330551147
Epoch 1480, training loss: 312.337890625 = 0.06041070073843002 + 50.0 * 6.245549201965332
Epoch 1480, val loss: 1.114524483680725
Epoch 1490, training loss: 312.4125061035156 = 0.05895116552710533 + 50.0 * 6.247070789337158
Epoch 1490, val loss: 1.1192984580993652
Epoch 1500, training loss: 312.28717041015625 = 0.05754418671131134 + 50.0 * 6.244592189788818
Epoch 1500, val loss: 1.124451756477356
Epoch 1510, training loss: 312.4337158203125 = 0.056174736469984055 + 50.0 * 6.2475504875183105
Epoch 1510, val loss: 1.1295613050460815
Epoch 1520, training loss: 312.2127685546875 = 0.05485256388783455 + 50.0 * 6.243158340454102
Epoch 1520, val loss: 1.1343342065811157
Epoch 1530, training loss: 312.19158935546875 = 0.053573038429021835 + 50.0 * 6.24276065826416
Epoch 1530, val loss: 1.1393892765045166
Epoch 1540, training loss: 312.162353515625 = 0.05233735963702202 + 50.0 * 6.2422003746032715
Epoch 1540, val loss: 1.1443806886672974
Epoch 1550, training loss: 312.5201416015625 = 0.05114229768514633 + 50.0 * 6.249379634857178
Epoch 1550, val loss: 1.1497262716293335
Epoch 1560, training loss: 312.19677734375 = 0.04995204880833626 + 50.0 * 6.242936134338379
Epoch 1560, val loss: 1.1541274785995483
Epoch 1570, training loss: 312.1048583984375 = 0.048808757215738297 + 50.0 * 6.241121292114258
Epoch 1570, val loss: 1.1587159633636475
Epoch 1580, training loss: 312.08367919921875 = 0.04770953208208084 + 50.0 * 6.240719795227051
Epoch 1580, val loss: 1.1637680530548096
Epoch 1590, training loss: 312.51873779296875 = 0.046653758734464645 + 50.0 * 6.249441623687744
Epoch 1590, val loss: 1.1688566207885742
Epoch 1600, training loss: 312.3125 = 0.045595306903123856 + 50.0 * 6.245337963104248
Epoch 1600, val loss: 1.1734592914581299
Epoch 1610, training loss: 312.0804443359375 = 0.04457446187734604 + 50.0 * 6.240716934204102
Epoch 1610, val loss: 1.1781193017959595
Epoch 1620, training loss: 311.9901123046875 = 0.04359572380781174 + 50.0 * 6.2389302253723145
Epoch 1620, val loss: 1.1831884384155273
Epoch 1630, training loss: 311.97998046875 = 0.042649734765291214 + 50.0 * 6.238746166229248
Epoch 1630, val loss: 1.1880145072937012
Epoch 1640, training loss: 312.50439453125 = 0.041741449385881424 + 50.0 * 6.249252796173096
Epoch 1640, val loss: 1.1929163932800293
Epoch 1650, training loss: 312.12298583984375 = 0.04081638902425766 + 50.0 * 6.24164342880249
Epoch 1650, val loss: 1.1972260475158691
Epoch 1660, training loss: 312.04388427734375 = 0.03994140028953552 + 50.0 * 6.240078449249268
Epoch 1660, val loss: 1.2021760940551758
Epoch 1670, training loss: 312.0849914550781 = 0.039080794900655746 + 50.0 * 6.240918159484863
Epoch 1670, val loss: 1.2068841457366943
Epoch 1680, training loss: 311.93572998046875 = 0.03824461251497269 + 50.0 * 6.237949848175049
Epoch 1680, val loss: 1.2113502025604248
Epoch 1690, training loss: 311.8935241699219 = 0.03744284436106682 + 50.0 * 6.23712158203125
Epoch 1690, val loss: 1.2160478830337524
Epoch 1700, training loss: 311.8680114746094 = 0.036664023995399475 + 50.0 * 6.236627101898193
Epoch 1700, val loss: 1.2206059694290161
Epoch 1710, training loss: 312.36474609375 = 0.03591912239789963 + 50.0 * 6.24657678604126
Epoch 1710, val loss: 1.2255268096923828
Epoch 1720, training loss: 312.0130310058594 = 0.03516398370265961 + 50.0 * 6.23955774307251
Epoch 1720, val loss: 1.2295429706573486
Epoch 1730, training loss: 311.919677734375 = 0.03444533795118332 + 50.0 * 6.237704277038574
Epoch 1730, val loss: 1.2344388961791992
Epoch 1740, training loss: 312.1944274902344 = 0.0337437242269516 + 50.0 * 6.243213653564453
Epoch 1740, val loss: 1.238928198814392
Epoch 1750, training loss: 311.8843688964844 = 0.03305504098534584 + 50.0 * 6.237026214599609
Epoch 1750, val loss: 1.2427098751068115
Epoch 1760, training loss: 311.77288818359375 = 0.03239122033119202 + 50.0 * 6.2348103523254395
Epoch 1760, val loss: 1.2475782632827759
Epoch 1770, training loss: 311.7474060058594 = 0.031750474125146866 + 50.0 * 6.234313488006592
Epoch 1770, val loss: 1.2517220973968506
Epoch 1780, training loss: 312.1667785644531 = 0.031130898743867874 + 50.0 * 6.24271297454834
Epoch 1780, val loss: 1.2554855346679688
Epoch 1790, training loss: 311.84771728515625 = 0.03051203303039074 + 50.0 * 6.236343860626221
Epoch 1790, val loss: 1.2608039379119873
Epoch 1800, training loss: 311.7480773925781 = 0.02991372160613537 + 50.0 * 6.234363555908203
Epoch 1800, val loss: 1.2645493745803833
Epoch 1810, training loss: 311.8953552246094 = 0.029340727254748344 + 50.0 * 6.237320423126221
Epoch 1810, val loss: 1.26924467086792
Epoch 1820, training loss: 311.8492126464844 = 0.028773486614227295 + 50.0 * 6.236408710479736
Epoch 1820, val loss: 1.2732501029968262
Epoch 1830, training loss: 311.7489013671875 = 0.02821439318358898 + 50.0 * 6.2344136238098145
Epoch 1830, val loss: 1.2775121927261353
Epoch 1840, training loss: 311.68585205078125 = 0.027677251026034355 + 50.0 * 6.233163356781006
Epoch 1840, val loss: 1.2816039323806763
Epoch 1850, training loss: 311.6419982910156 = 0.027158426120877266 + 50.0 * 6.232296466827393
Epoch 1850, val loss: 1.2856783866882324
Epoch 1860, training loss: 311.6941833496094 = 0.0266562569886446 + 50.0 * 6.23335075378418
Epoch 1860, val loss: 1.2897859811782837
Epoch 1870, training loss: 311.7817077636719 = 0.026162143796682358 + 50.0 * 6.235111236572266
Epoch 1870, val loss: 1.2937854528427124
Epoch 1880, training loss: 311.79010009765625 = 0.025679923593997955 + 50.0 * 6.235288619995117
Epoch 1880, val loss: 1.2982568740844727
Epoch 1890, training loss: 311.7061767578125 = 0.02520490065217018 + 50.0 * 6.233619213104248
Epoch 1890, val loss: 1.3019022941589355
Epoch 1900, training loss: 311.63446044921875 = 0.024746229872107506 + 50.0 * 6.232194423675537
Epoch 1900, val loss: 1.3061360120773315
Epoch 1910, training loss: 311.673583984375 = 0.024303482845425606 + 50.0 * 6.232985973358154
Epoch 1910, val loss: 1.3098185062408447
Epoch 1920, training loss: 311.7718200683594 = 0.023864170536398888 + 50.0 * 6.234959125518799
Epoch 1920, val loss: 1.3137588500976562
Epoch 1930, training loss: 311.6116943359375 = 0.02344275265932083 + 50.0 * 6.231765270233154
Epoch 1930, val loss: 1.3180110454559326
Epoch 1940, training loss: 311.5389709472656 = 0.023025644943118095 + 50.0 * 6.230318546295166
Epoch 1940, val loss: 1.321927547454834
Epoch 1950, training loss: 311.5343017578125 = 0.022625761106610298 + 50.0 * 6.230233669281006
Epoch 1950, val loss: 1.3256429433822632
Epoch 1960, training loss: 311.6878356933594 = 0.02224041149020195 + 50.0 * 6.233311653137207
Epoch 1960, val loss: 1.3294315338134766
Epoch 1970, training loss: 311.6542663574219 = 0.021854955703020096 + 50.0 * 6.2326483726501465
Epoch 1970, val loss: 1.3335819244384766
Epoch 1980, training loss: 311.6224670410156 = 0.02147505059838295 + 50.0 * 6.232019424438477
Epoch 1980, val loss: 1.336534857749939
Epoch 1990, training loss: 311.5038146972656 = 0.021108165383338928 + 50.0 * 6.229653835296631
Epoch 1990, val loss: 1.3406907320022583
Epoch 2000, training loss: 311.4974365234375 = 0.020754672586917877 + 50.0 * 6.229533672332764
Epoch 2000, val loss: 1.3445314168930054
Epoch 2010, training loss: 311.4971008300781 = 0.020410653203725815 + 50.0 * 6.229533672332764
Epoch 2010, val loss: 1.3481032848358154
Epoch 2020, training loss: 311.65191650390625 = 0.02007383666932583 + 50.0 * 6.232636451721191
Epoch 2020, val loss: 1.3515745401382446
Epoch 2030, training loss: 311.65484619140625 = 0.01974046602845192 + 50.0 * 6.232701778411865
Epoch 2030, val loss: 1.35520601272583
Epoch 2040, training loss: 311.6644287109375 = 0.01942053623497486 + 50.0 * 6.232900619506836
Epoch 2040, val loss: 1.3590385913848877
Epoch 2050, training loss: 311.4998474121094 = 0.01909833401441574 + 50.0 * 6.229614734649658
Epoch 2050, val loss: 1.3624705076217651
Epoch 2060, training loss: 311.5882568359375 = 0.018794581294059753 + 50.0 * 6.231389045715332
Epoch 2060, val loss: 1.3660968542099
Epoch 2070, training loss: 311.40545654296875 = 0.018491091206669807 + 50.0 * 6.227739334106445
Epoch 2070, val loss: 1.369295358657837
Epoch 2080, training loss: 311.37103271484375 = 0.018198413774371147 + 50.0 * 6.227056980133057
Epoch 2080, val loss: 1.3727641105651855
Epoch 2090, training loss: 311.4906311035156 = 0.017915066331624985 + 50.0 * 6.229454517364502
Epoch 2090, val loss: 1.3759493827819824
Epoch 2100, training loss: 311.4405212402344 = 0.017636220902204514 + 50.0 * 6.228457927703857
Epoch 2100, val loss: 1.3790910243988037
Epoch 2110, training loss: 311.54376220703125 = 0.017361681908369064 + 50.0 * 6.230527877807617
Epoch 2110, val loss: 1.3825573921203613
Epoch 2120, training loss: 311.6000061035156 = 0.01709037646651268 + 50.0 * 6.231658458709717
Epoch 2120, val loss: 1.3860769271850586
Epoch 2130, training loss: 311.31744384765625 = 0.01682635210454464 + 50.0 * 6.226012706756592
Epoch 2130, val loss: 1.389478087425232
Epoch 2140, training loss: 311.2882995605469 = 0.0165727399289608 + 50.0 * 6.225434303283691
Epoch 2140, val loss: 1.3927196264266968
Epoch 2150, training loss: 311.2655944824219 = 0.01632578670978546 + 50.0 * 6.224985599517822
Epoch 2150, val loss: 1.3959779739379883
Epoch 2160, training loss: 311.40362548828125 = 0.016086529940366745 + 50.0 * 6.227750778198242
Epoch 2160, val loss: 1.399129033088684
Epoch 2170, training loss: 311.4460754394531 = 0.01584756001830101 + 50.0 * 6.228604793548584
Epoch 2170, val loss: 1.4021756649017334
Epoch 2180, training loss: 311.3528747558594 = 0.015608207322657108 + 50.0 * 6.22674560546875
Epoch 2180, val loss: 1.4054819345474243
Epoch 2190, training loss: 311.3107604980469 = 0.015378894284367561 + 50.0 * 6.225907325744629
Epoch 2190, val loss: 1.4089921712875366
Epoch 2200, training loss: 311.284912109375 = 0.015155436471104622 + 50.0 * 6.2253947257995605
Epoch 2200, val loss: 1.4118696451187134
Epoch 2210, training loss: 311.3226623535156 = 0.014943912625312805 + 50.0 * 6.226154327392578
Epoch 2210, val loss: 1.4152508974075317
Epoch 2220, training loss: 311.4266357421875 = 0.014731664210557938 + 50.0 * 6.228237628936768
Epoch 2220, val loss: 1.418330430984497
Epoch 2230, training loss: 311.3547058105469 = 0.014517211355268955 + 50.0 * 6.226803779602051
Epoch 2230, val loss: 1.4206575155258179
Epoch 2240, training loss: 311.31817626953125 = 0.014314856380224228 + 50.0 * 6.226077079772949
Epoch 2240, val loss: 1.4240427017211914
Epoch 2250, training loss: 311.3865051269531 = 0.014114368706941605 + 50.0 * 6.227447986602783
Epoch 2250, val loss: 1.4264509677886963
Epoch 2260, training loss: 311.2162170410156 = 0.013917586766183376 + 50.0 * 6.224045753479004
Epoch 2260, val loss: 1.4302912950515747
Epoch 2270, training loss: 311.197509765625 = 0.013727344572544098 + 50.0 * 6.22367525100708
Epoch 2270, val loss: 1.4329636096954346
Epoch 2280, training loss: 311.46563720703125 = 0.013542153872549534 + 50.0 * 6.229042053222656
Epoch 2280, val loss: 1.435674786567688
Epoch 2290, training loss: 311.3018798828125 = 0.013356965035200119 + 50.0 * 6.225770473480225
Epoch 2290, val loss: 1.438634991645813
Epoch 2300, training loss: 311.2779846191406 = 0.013177760876715183 + 50.0 * 6.2252960205078125
Epoch 2300, val loss: 1.4411550760269165
Epoch 2310, training loss: 311.1699523925781 = 0.013001295737922192 + 50.0 * 6.223138809204102
Epoch 2310, val loss: 1.4445452690124512
Epoch 2320, training loss: 311.39337158203125 = 0.012831696309149265 + 50.0 * 6.227611064910889
Epoch 2320, val loss: 1.447341799736023
Epoch 2330, training loss: 311.2015075683594 = 0.01266279723495245 + 50.0 * 6.223776817321777
Epoch 2330, val loss: 1.4496058225631714
Epoch 2340, training loss: 311.1276550292969 = 0.012496219016611576 + 50.0 * 6.22230339050293
Epoch 2340, val loss: 1.4528162479400635
Epoch 2350, training loss: 311.115966796875 = 0.012335543520748615 + 50.0 * 6.222072601318359
Epoch 2350, val loss: 1.4555234909057617
Epoch 2360, training loss: 311.4388732910156 = 0.012186362408101559 + 50.0 * 6.22853422164917
Epoch 2360, val loss: 1.4582408666610718
Epoch 2370, training loss: 311.1290283203125 = 0.012022166512906551 + 50.0 * 6.222340106964111
Epoch 2370, val loss: 1.4605587720870972
Epoch 2380, training loss: 311.138427734375 = 0.01187082752585411 + 50.0 * 6.222531318664551
Epoch 2380, val loss: 1.4630379676818848
Epoch 2390, training loss: 311.34991455078125 = 0.011723272502422333 + 50.0 * 6.22676420211792
Epoch 2390, val loss: 1.4658896923065186
Epoch 2400, training loss: 311.1748352050781 = 0.011574048548936844 + 50.0 * 6.223265647888184
Epoch 2400, val loss: 1.4684102535247803
Epoch 2410, training loss: 311.0658264160156 = 0.011429177597165108 + 50.0 * 6.22108793258667
Epoch 2410, val loss: 1.4709454774856567
Epoch 2420, training loss: 311.0531311035156 = 0.01129088643938303 + 50.0 * 6.220836639404297
Epoch 2420, val loss: 1.4733842611312866
Epoch 2430, training loss: 311.07257080078125 = 0.011154573410749435 + 50.0 * 6.221228122711182
Epoch 2430, val loss: 1.4759373664855957
Epoch 2440, training loss: 311.3408508300781 = 0.011023682542145252 + 50.0 * 6.226596832275391
Epoch 2440, val loss: 1.4785858392715454
Epoch 2450, training loss: 311.2325744628906 = 0.010890811681747437 + 50.0 * 6.224433422088623
Epoch 2450, val loss: 1.4814400672912598
Epoch 2460, training loss: 311.15667724609375 = 0.010756139643490314 + 50.0 * 6.222918510437012
Epoch 2460, val loss: 1.4830397367477417
Epoch 2470, training loss: 311.1470947265625 = 0.010627138428390026 + 50.0 * 6.222729682922363
Epoch 2470, val loss: 1.4860049486160278
Epoch 2480, training loss: 310.9939270019531 = 0.01050336379557848 + 50.0 * 6.219668865203857
Epoch 2480, val loss: 1.4881511926651
Epoch 2490, training loss: 310.9683532714844 = 0.010382157750427723 + 50.0 * 6.2191596031188965
Epoch 2490, val loss: 1.4908732175827026
Epoch 2500, training loss: 311.2864074707031 = 0.01026849914342165 + 50.0 * 6.225522994995117
Epoch 2500, val loss: 1.4931279420852661
Epoch 2510, training loss: 310.9966125488281 = 0.010146220214664936 + 50.0 * 6.219729423522949
Epoch 2510, val loss: 1.4953235387802124
Epoch 2520, training loss: 311.059326171875 = 0.010028382763266563 + 50.0 * 6.2209858894348145
Epoch 2520, val loss: 1.4974539279937744
Epoch 2530, training loss: 311.0840148925781 = 0.009914240799844265 + 50.0 * 6.221481800079346
Epoch 2530, val loss: 1.4999403953552246
Epoch 2540, training loss: 311.1070251464844 = 0.009803742170333862 + 50.0 * 6.221944808959961
Epoch 2540, val loss: 1.5022597312927246
Epoch 2550, training loss: 310.9942932128906 = 0.009692286141216755 + 50.0 * 6.219691753387451
Epoch 2550, val loss: 1.5045809745788574
Epoch 2560, training loss: 310.91632080078125 = 0.009585893712937832 + 50.0 * 6.218134880065918
Epoch 2560, val loss: 1.506744384765625
Epoch 2570, training loss: 310.9459533691406 = 0.00948228221386671 + 50.0 * 6.218729019165039
Epoch 2570, val loss: 1.5091016292572021
Epoch 2580, training loss: 311.1295471191406 = 0.009382140822708607 + 50.0 * 6.222403526306152
Epoch 2580, val loss: 1.5114092826843262
Epoch 2590, training loss: 311.0439453125 = 0.009279752150177956 + 50.0 * 6.220693111419678
Epoch 2590, val loss: 1.5129570960998535
Epoch 2600, training loss: 310.9303894042969 = 0.0091770114377141 + 50.0 * 6.218423843383789
Epoch 2600, val loss: 1.5154067277908325
Epoch 2610, training loss: 310.9011535644531 = 0.009079339914023876 + 50.0 * 6.217841148376465
Epoch 2610, val loss: 1.5174795389175415
Epoch 2620, training loss: 311.0635681152344 = 0.008985455147922039 + 50.0 * 6.2210917472839355
Epoch 2620, val loss: 1.519354224205017
Epoch 2630, training loss: 311.2659606933594 = 0.008888842537999153 + 50.0 * 6.225141525268555
Epoch 2630, val loss: 1.5214648246765137
Epoch 2640, training loss: 310.94110107421875 = 0.008793478831648827 + 50.0 * 6.21864652633667
Epoch 2640, val loss: 1.5236499309539795
Epoch 2650, training loss: 310.8602294921875 = 0.008699337020516396 + 50.0 * 6.2170305252075195
Epoch 2650, val loss: 1.5255683660507202
Epoch 2660, training loss: 310.83966064453125 = 0.008611267432570457 + 50.0 * 6.216621398925781
Epoch 2660, val loss: 1.527479887008667
Epoch 2670, training loss: 310.8541259765625 = 0.008524080738425255 + 50.0 * 6.216912269592285
Epoch 2670, val loss: 1.5296525955200195
Epoch 2680, training loss: 311.1938781738281 = 0.00844008382409811 + 50.0 * 6.223708629608154
Epoch 2680, val loss: 1.5311927795410156
Epoch 2690, training loss: 310.8202209472656 = 0.008352968841791153 + 50.0 * 6.2162370681762695
Epoch 2690, val loss: 1.5337110757827759
Epoch 2700, training loss: 310.79437255859375 = 0.008269819431006908 + 50.0 * 6.21572208404541
Epoch 2700, val loss: 1.5355110168457031
Epoch 2710, training loss: 310.8688659667969 = 0.008188353851437569 + 50.0 * 6.2172136306762695
Epoch 2710, val loss: 1.5374122858047485
Epoch 2720, training loss: 311.0818176269531 = 0.008109565824270248 + 50.0 * 6.2214741706848145
Epoch 2720, val loss: 1.5396041870117188
Epoch 2730, training loss: 310.9245300292969 = 0.008028845302760601 + 50.0 * 6.218329906463623
Epoch 2730, val loss: 1.5410211086273193
Epoch 2740, training loss: 311.00433349609375 = 0.007949150167405605 + 50.0 * 6.21992826461792
Epoch 2740, val loss: 1.5430786609649658
Epoch 2750, training loss: 310.79608154296875 = 0.007872425019741058 + 50.0 * 6.215764045715332
Epoch 2750, val loss: 1.5449793338775635
Epoch 2760, training loss: 310.93621826171875 = 0.007798278704285622 + 50.0 * 6.218568325042725
Epoch 2760, val loss: 1.5471248626708984
Epoch 2770, training loss: 310.8666687011719 = 0.007723895367234945 + 50.0 * 6.217178821563721
Epoch 2770, val loss: 1.548675298690796
Epoch 2780, training loss: 310.81005859375 = 0.00764880096539855 + 50.0 * 6.216048240661621
Epoch 2780, val loss: 1.5504858493804932
Epoch 2790, training loss: 310.87994384765625 = 0.007577287033200264 + 50.0 * 6.217447280883789
Epoch 2790, val loss: 1.552298665046692
Epoch 2800, training loss: 310.78839111328125 = 0.007507264148443937 + 50.0 * 6.215617656707764
Epoch 2800, val loss: 1.5540120601654053
Epoch 2810, training loss: 310.84893798828125 = 0.007438128348439932 + 50.0 * 6.216829776763916
Epoch 2810, val loss: 1.5555604696273804
Epoch 2820, training loss: 310.8427429199219 = 0.007370134349912405 + 50.0 * 6.216707229614258
Epoch 2820, val loss: 1.55721914768219
Epoch 2830, training loss: 310.9031066894531 = 0.007303234189748764 + 50.0 * 6.217916011810303
Epoch 2830, val loss: 1.5586166381835938
Epoch 2840, training loss: 310.8445739746094 = 0.007237071171402931 + 50.0 * 6.216746807098389
Epoch 2840, val loss: 1.560793161392212
Epoch 2850, training loss: 310.763916015625 = 0.007170164491981268 + 50.0 * 6.215135097503662
Epoch 2850, val loss: 1.5623699426651
Epoch 2860, training loss: 310.693115234375 = 0.0071068573743104935 + 50.0 * 6.213719844818115
Epoch 2860, val loss: 1.5641916990280151
Epoch 2870, training loss: 310.8938293457031 = 0.007044877856969833 + 50.0 * 6.217735767364502
Epoch 2870, val loss: 1.5659594535827637
Epoch 2880, training loss: 310.73406982421875 = 0.00698262732475996 + 50.0 * 6.214541435241699
Epoch 2880, val loss: 1.5672041177749634
Epoch 2890, training loss: 310.79541015625 = 0.006921217776834965 + 50.0 * 6.2157697677612305
Epoch 2890, val loss: 1.5688729286193848
Epoch 2900, training loss: 310.6883850097656 = 0.006861008703708649 + 50.0 * 6.213630676269531
Epoch 2900, val loss: 1.570428729057312
Epoch 2910, training loss: 310.8645324707031 = 0.00680279778316617 + 50.0 * 6.217154502868652
Epoch 2910, val loss: 1.571908950805664
Epoch 2920, training loss: 310.7313232421875 = 0.0067429193295538425 + 50.0 * 6.214491844177246
Epoch 2920, val loss: 1.5737606287002563
Epoch 2930, training loss: 310.8538818359375 = 0.0066860816441476345 + 50.0 * 6.216943740844727
Epoch 2930, val loss: 1.575346827507019
Epoch 2940, training loss: 310.72772216796875 = 0.006629513576626778 + 50.0 * 6.21442174911499
Epoch 2940, val loss: 1.5770896673202515
Epoch 2950, training loss: 310.7080993652344 = 0.006575309205800295 + 50.0 * 6.214030742645264
Epoch 2950, val loss: 1.5786535739898682
Epoch 2960, training loss: 310.6579895019531 = 0.00651858514174819 + 50.0 * 6.213028907775879
Epoch 2960, val loss: 1.5801712274551392
Epoch 2970, training loss: 310.72528076171875 = 0.0064665162935853004 + 50.0 * 6.214376449584961
Epoch 2970, val loss: 1.5814845561981201
Epoch 2980, training loss: 310.8319091796875 = 0.006413314491510391 + 50.0 * 6.216509819030762
Epoch 2980, val loss: 1.5828832387924194
Epoch 2990, training loss: 310.68634033203125 = 0.0063608549535274506 + 50.0 * 6.213600158691406
Epoch 2990, val loss: 1.5842914581298828
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7703703703703704
0.8371112282551397
The final CL Acc:0.75185, 0.02362, The final GNN Acc:0.83869, 0.00114
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9460])
updated graph: torch.Size([2, 10478])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7716369628906 = 1.9289311170578003 + 50.0 * 8.596854209899902
Epoch 0, val loss: 1.9275919198989868
Epoch 10, training loss: 431.732666015625 = 1.9211890697479248 + 50.0 * 8.596229553222656
Epoch 10, val loss: 1.9193137884140015
Epoch 20, training loss: 431.5096130371094 = 1.9117614030838013 + 50.0 * 8.591957092285156
Epoch 20, val loss: 1.9090871810913086
Epoch 30, training loss: 430.0357360839844 = 1.8997102975845337 + 50.0 * 8.56272029876709
Epoch 30, val loss: 1.8960295915603638
Epoch 40, training loss: 421.3103942871094 = 1.8841650485992432 + 50.0 * 8.388525009155273
Epoch 40, val loss: 1.8797982931137085
Epoch 50, training loss: 391.7294006347656 = 1.8657134771347046 + 50.0 * 7.797273635864258
Epoch 50, val loss: 1.8613840341567993
Epoch 60, training loss: 371.6609191894531 = 1.8518528938293457 + 50.0 * 7.396181106567383
Epoch 60, val loss: 1.848450779914856
Epoch 70, training loss: 354.8988037109375 = 1.8414660692214966 + 50.0 * 7.0611467361450195
Epoch 70, val loss: 1.8380175828933716
Epoch 80, training loss: 346.7984924316406 = 1.8322665691375732 + 50.0 * 6.899324417114258
Epoch 80, val loss: 1.828705906867981
Epoch 90, training loss: 341.79962158203125 = 1.8226312398910522 + 50.0 * 6.799540042877197
Epoch 90, val loss: 1.819337010383606
Epoch 100, training loss: 338.52044677734375 = 1.8133920431137085 + 50.0 * 6.7341413497924805
Epoch 100, val loss: 1.810379981994629
Epoch 110, training loss: 336.0909118652344 = 1.8046058416366577 + 50.0 * 6.685726165771484
Epoch 110, val loss: 1.8019218444824219
Epoch 120, training loss: 334.319091796875 = 1.7963517904281616 + 50.0 * 6.650454521179199
Epoch 120, val loss: 1.7938098907470703
Epoch 130, training loss: 332.7841491699219 = 1.7879528999328613 + 50.0 * 6.6199235916137695
Epoch 130, val loss: 1.7856651544570923
Epoch 140, training loss: 331.32904052734375 = 1.7792032957077026 + 50.0 * 6.590996742248535
Epoch 140, val loss: 1.777377724647522
Epoch 150, training loss: 329.9211120605469 = 1.7700903415679932 + 50.0 * 6.563020706176758
Epoch 150, val loss: 1.768864631652832
Epoch 160, training loss: 328.620361328125 = 1.760375738143921 + 50.0 * 6.537199974060059
Epoch 160, val loss: 1.7599478960037231
Epoch 170, training loss: 327.5500183105469 = 1.7498942613601685 + 50.0 * 6.516002178192139
Epoch 170, val loss: 1.750397801399231
Epoch 180, training loss: 326.5119323730469 = 1.7384573221206665 + 50.0 * 6.495469570159912
Epoch 180, val loss: 1.7399847507476807
Epoch 190, training loss: 325.69720458984375 = 1.7257646322250366 + 50.0 * 6.479428768157959
Epoch 190, val loss: 1.7286252975463867
Epoch 200, training loss: 324.9136047363281 = 1.7117996215820312 + 50.0 * 6.464036464691162
Epoch 200, val loss: 1.716217041015625
Epoch 210, training loss: 324.26031494140625 = 1.696448802947998 + 50.0 * 6.451277256011963
Epoch 210, val loss: 1.702789068222046
Epoch 220, training loss: 323.8189697265625 = 1.6797302961349487 + 50.0 * 6.442785263061523
Epoch 220, val loss: 1.6883240938186646
Epoch 230, training loss: 323.18414306640625 = 1.6616672277450562 + 50.0 * 6.43044900894165
Epoch 230, val loss: 1.6727569103240967
Epoch 240, training loss: 322.72808837890625 = 1.6422410011291504 + 50.0 * 6.421716690063477
Epoch 240, val loss: 1.656270146369934
Epoch 250, training loss: 322.4396057128906 = 1.6214243173599243 + 50.0 * 6.41636323928833
Epoch 250, val loss: 1.6389306783676147
Epoch 260, training loss: 322.0194396972656 = 1.5993365049362183 + 50.0 * 6.408401966094971
Epoch 260, val loss: 1.6208142042160034
Epoch 270, training loss: 321.59027099609375 = 1.5762214660644531 + 50.0 * 6.400280952453613
Epoch 270, val loss: 1.6020530462265015
Epoch 280, training loss: 321.2494201660156 = 1.552244782447815 + 50.0 * 6.3939433097839355
Epoch 280, val loss: 1.5829719305038452
Epoch 290, training loss: 320.9555969238281 = 1.527408242225647 + 50.0 * 6.388563632965088
Epoch 290, val loss: 1.5637649297714233
Epoch 300, training loss: 320.66558837890625 = 1.502008318901062 + 50.0 * 6.383271217346191
Epoch 300, val loss: 1.5444605350494385
Epoch 310, training loss: 320.38330078125 = 1.4762948751449585 + 50.0 * 6.378139972686768
Epoch 310, val loss: 1.5252511501312256
Epoch 320, training loss: 320.5892028808594 = 1.4503241777420044 + 50.0 * 6.382777690887451
Epoch 320, val loss: 1.5061557292938232
Epoch 330, training loss: 319.9754333496094 = 1.424182653427124 + 50.0 * 6.3710246086120605
Epoch 330, val loss: 1.4874827861785889
Epoch 340, training loss: 319.68157958984375 = 1.3982384204864502 + 50.0 * 6.36566686630249
Epoch 340, val loss: 1.4692867994308472
Epoch 350, training loss: 319.42730712890625 = 1.372489333152771 + 50.0 * 6.361095905303955
Epoch 350, val loss: 1.4515933990478516
Epoch 360, training loss: 319.1844177246094 = 1.3468700647354126 + 50.0 * 6.356750965118408
Epoch 360, val loss: 1.434285283088684
Epoch 370, training loss: 318.9743957519531 = 1.3215835094451904 + 50.0 * 6.353055953979492
Epoch 370, val loss: 1.4175339937210083
Epoch 380, training loss: 318.7922668457031 = 1.296585202217102 + 50.0 * 6.349913597106934
Epoch 380, val loss: 1.4013084173202515
Epoch 390, training loss: 318.6372375488281 = 1.2718298435211182 + 50.0 * 6.34730863571167
Epoch 390, val loss: 1.3853203058242798
Epoch 400, training loss: 318.3285827636719 = 1.2474522590637207 + 50.0 * 6.341622352600098
Epoch 400, val loss: 1.3697773218154907
Epoch 410, training loss: 318.1562194824219 = 1.2234293222427368 + 50.0 * 6.338655948638916
Epoch 410, val loss: 1.3544833660125732
Epoch 420, training loss: 317.9882507324219 = 1.1997472047805786 + 50.0 * 6.335770130157471
Epoch 420, val loss: 1.3397974967956543
Epoch 430, training loss: 317.8282775878906 = 1.176468849182129 + 50.0 * 6.333035945892334
Epoch 430, val loss: 1.3252042531967163
Epoch 440, training loss: 317.7025451660156 = 1.1535837650299072 + 50.0 * 6.330978870391846
Epoch 440, val loss: 1.3114327192306519
Epoch 450, training loss: 317.48260498046875 = 1.1311157941818237 + 50.0 * 6.327030181884766
Epoch 450, val loss: 1.2976739406585693
Epoch 460, training loss: 317.2750244140625 = 1.1092512607574463 + 50.0 * 6.323315620422363
Epoch 460, val loss: 1.2849427461624146
Epoch 470, training loss: 317.1916809082031 = 1.0879359245300293 + 50.0 * 6.322074890136719
Epoch 470, val loss: 1.272425889968872
Epoch 480, training loss: 317.10614013671875 = 1.0670338869094849 + 50.0 * 6.320781707763672
Epoch 480, val loss: 1.2607486248016357
Epoch 490, training loss: 316.83642578125 = 1.0467915534973145 + 50.0 * 6.315792560577393
Epoch 490, val loss: 1.2496145963668823
Epoch 500, training loss: 316.64190673828125 = 1.027105450630188 + 50.0 * 6.312295913696289
Epoch 500, val loss: 1.238957166671753
Epoch 510, training loss: 316.5890197753906 = 1.0080361366271973 + 50.0 * 6.311619758605957
Epoch 510, val loss: 1.2293148040771484
Epoch 520, training loss: 316.65533447265625 = 0.9893907308578491 + 50.0 * 6.313319206237793
Epoch 520, val loss: 1.2200103998184204
Epoch 530, training loss: 316.2850036621094 = 0.9712116122245789 + 50.0 * 6.306275367736816
Epoch 530, val loss: 1.2109874486923218
Epoch 540, training loss: 316.1076965332031 = 0.953660786151886 + 50.0 * 6.3030805587768555
Epoch 540, val loss: 1.203249454498291
Epoch 550, training loss: 316.15179443359375 = 0.9366046786308289 + 50.0 * 6.3043036460876465
Epoch 550, val loss: 1.1958547830581665
Epoch 560, training loss: 316.0725402832031 = 0.919948160648346 + 50.0 * 6.303051471710205
Epoch 560, val loss: 1.1888419389724731
Epoch 570, training loss: 315.7882385253906 = 0.9036640524864197 + 50.0 * 6.297691345214844
Epoch 570, val loss: 1.1824657917022705
Epoch 580, training loss: 315.6888427734375 = 0.8877953290939331 + 50.0 * 6.296020984649658
Epoch 580, val loss: 1.1766304969787598
Epoch 590, training loss: 316.01629638671875 = 0.8723223805427551 + 50.0 * 6.302879810333252
Epoch 590, val loss: 1.1715693473815918
Epoch 600, training loss: 315.50054931640625 = 0.8570321202278137 + 50.0 * 6.29287052154541
Epoch 600, val loss: 1.166278600692749
Epoch 610, training loss: 315.3955383300781 = 0.84206223487854 + 50.0 * 6.291069030761719
Epoch 610, val loss: 1.161586046218872
Epoch 620, training loss: 315.3119812011719 = 0.8274146914482117 + 50.0 * 6.28969144821167
Epoch 620, val loss: 1.157478928565979
Epoch 630, training loss: 315.2454833984375 = 0.8130010366439819 + 50.0 * 6.288650035858154
Epoch 630, val loss: 1.1539000272750854
Epoch 640, training loss: 315.1101379394531 = 0.7988305687904358 + 50.0 * 6.286226272583008
Epoch 640, val loss: 1.1506277322769165
Epoch 650, training loss: 315.1435546875 = 0.7848339080810547 + 50.0 * 6.287174701690674
Epoch 650, val loss: 1.1475911140441895
Epoch 660, training loss: 315.0332336425781 = 0.7710232138633728 + 50.0 * 6.285243988037109
Epoch 660, val loss: 1.1450953483581543
Epoch 670, training loss: 314.9298095703125 = 0.7573229670524597 + 50.0 * 6.283449649810791
Epoch 670, val loss: 1.1425737142562866
Epoch 680, training loss: 314.7884826660156 = 0.743876039981842 + 50.0 * 6.280892372131348
Epoch 680, val loss: 1.140856385231018
Epoch 690, training loss: 314.91162109375 = 0.7305527329444885 + 50.0 * 6.283621311187744
Epoch 690, val loss: 1.1391993761062622
Epoch 700, training loss: 314.7679748535156 = 0.7173797488212585 + 50.0 * 6.281012058258057
Epoch 700, val loss: 1.1377135515213013
Epoch 710, training loss: 314.56707763671875 = 0.7043030858039856 + 50.0 * 6.277255058288574
Epoch 710, val loss: 1.1368112564086914
Epoch 720, training loss: 314.499267578125 = 0.6914775371551514 + 50.0 * 6.276155948638916
Epoch 720, val loss: 1.13601553440094
Epoch 730, training loss: 314.6023864746094 = 0.678736686706543 + 50.0 * 6.278472900390625
Epoch 730, val loss: 1.1355589628219604
Epoch 740, training loss: 314.3559265136719 = 0.6660939455032349 + 50.0 * 6.273796558380127
Epoch 740, val loss: 1.135420560836792
Epoch 750, training loss: 314.24298095703125 = 0.6536211371421814 + 50.0 * 6.271787166595459
Epoch 750, val loss: 1.1354738473892212
Epoch 760, training loss: 314.25323486328125 = 0.6413024663925171 + 50.0 * 6.272238731384277
Epoch 760, val loss: 1.1358911991119385
Epoch 770, training loss: 314.2457275390625 = 0.6290358901023865 + 50.0 * 6.272334098815918
Epoch 770, val loss: 1.1364411115646362
Epoch 780, training loss: 314.12213134765625 = 0.616852879524231 + 50.0 * 6.270105361938477
Epoch 780, val loss: 1.1367501020431519
Epoch 790, training loss: 314.14410400390625 = 0.6047888398170471 + 50.0 * 6.270786285400391
Epoch 790, val loss: 1.1377875804901123
Epoch 800, training loss: 313.9967041015625 = 0.5928840637207031 + 50.0 * 6.268076419830322
Epoch 800, val loss: 1.139377236366272
Epoch 810, training loss: 313.970703125 = 0.5810869932174683 + 50.0 * 6.267792224884033
Epoch 810, val loss: 1.1405112743377686
Epoch 820, training loss: 313.8968200683594 = 0.569405734539032 + 50.0 * 6.2665486335754395
Epoch 820, val loss: 1.142609715461731
Epoch 830, training loss: 313.76739501953125 = 0.5578233003616333 + 50.0 * 6.264191150665283
Epoch 830, val loss: 1.1445674896240234
Epoch 840, training loss: 313.74005126953125 = 0.5463809370994568 + 50.0 * 6.26387357711792
Epoch 840, val loss: 1.1469582319259644
Epoch 850, training loss: 313.807373046875 = 0.5350082516670227 + 50.0 * 6.265447616577148
Epoch 850, val loss: 1.1494286060333252
Epoch 860, training loss: 313.74603271484375 = 0.523669958114624 + 50.0 * 6.264447212219238
Epoch 860, val loss: 1.1517523527145386
Epoch 870, training loss: 313.6463317871094 = 0.5124905705451965 + 50.0 * 6.262677192687988
Epoch 870, val loss: 1.15519380569458
Epoch 880, training loss: 313.51470947265625 = 0.501396656036377 + 50.0 * 6.260265827178955
Epoch 880, val loss: 1.1581733226776123
Epoch 890, training loss: 313.51409912109375 = 0.4904913902282715 + 50.0 * 6.260472297668457
Epoch 890, val loss: 1.1616352796554565
Epoch 900, training loss: 313.4576721191406 = 0.47965729236602783 + 50.0 * 6.259560585021973
Epoch 900, val loss: 1.1654672622680664
Epoch 910, training loss: 313.3841247558594 = 0.4689362049102783 + 50.0 * 6.258303642272949
Epoch 910, val loss: 1.1696220636367798
Epoch 920, training loss: 313.4128723144531 = 0.4583940804004669 + 50.0 * 6.259089469909668
Epoch 920, val loss: 1.1740083694458008
Epoch 930, training loss: 313.3462829589844 = 0.44795897603034973 + 50.0 * 6.257966041564941
Epoch 930, val loss: 1.1786279678344727
Epoch 940, training loss: 313.391845703125 = 0.437622994184494 + 50.0 * 6.259084701538086
Epoch 940, val loss: 1.1832996606826782
Epoch 950, training loss: 313.1558532714844 = 0.4274008572101593 + 50.0 * 6.254569053649902
Epoch 950, val loss: 1.1880111694335938
Epoch 960, training loss: 313.0939025878906 = 0.4174344837665558 + 50.0 * 6.2535295486450195
Epoch 960, val loss: 1.1932837963104248
Epoch 970, training loss: 313.1892395019531 = 0.40765416622161865 + 50.0 * 6.255631446838379
Epoch 970, val loss: 1.1990363597869873
Epoch 980, training loss: 313.15411376953125 = 0.3979194760322571 + 50.0 * 6.255123615264893
Epoch 980, val loss: 1.204637050628662
Epoch 990, training loss: 313.0237731933594 = 0.38839906454086304 + 50.0 * 6.252707481384277
Epoch 990, val loss: 1.2103291749954224
Epoch 1000, training loss: 312.97186279296875 = 0.3790644407272339 + 50.0 * 6.251855850219727
Epoch 1000, val loss: 1.2169280052185059
Epoch 1010, training loss: 313.0920104980469 = 0.3699261248111725 + 50.0 * 6.254441261291504
Epoch 1010, val loss: 1.223481297492981
Epoch 1020, training loss: 312.88671875 = 0.36085236072540283 + 50.0 * 6.250517845153809
Epoch 1020, val loss: 1.2294974327087402
Epoch 1030, training loss: 312.85888671875 = 0.3520699441432953 + 50.0 * 6.250136375427246
Epoch 1030, val loss: 1.2367748022079468
Epoch 1040, training loss: 313.0001525878906 = 0.34343090653419495 + 50.0 * 6.253134250640869
Epoch 1040, val loss: 1.2435846328735352
Epoch 1050, training loss: 312.8279724121094 = 0.33492693305015564 + 50.0 * 6.249860763549805
Epoch 1050, val loss: 1.250387191772461
Epoch 1060, training loss: 312.7835998535156 = 0.3266746699810028 + 50.0 * 6.249138832092285
Epoch 1060, val loss: 1.2578305006027222
Epoch 1070, training loss: 312.7397155761719 = 0.3185632526874542 + 50.0 * 6.248423099517822
Epoch 1070, val loss: 1.2652003765106201
Epoch 1080, training loss: 312.73101806640625 = 0.31064239144325256 + 50.0 * 6.248407363891602
Epoch 1080, val loss: 1.2727223634719849
Epoch 1090, training loss: 312.6532897949219 = 0.30287471413612366 + 50.0 * 6.247008323669434
Epoch 1090, val loss: 1.2803620100021362
Epoch 1100, training loss: 312.5888366699219 = 0.29533031582832336 + 50.0 * 6.245870590209961
Epoch 1100, val loss: 1.2880216836929321
Epoch 1110, training loss: 312.64202880859375 = 0.28794801235198975 + 50.0 * 6.247081756591797
Epoch 1110, val loss: 1.29600191116333
Epoch 1120, training loss: 312.51470947265625 = 0.2807161808013916 + 50.0 * 6.2446794509887695
Epoch 1120, val loss: 1.3043314218521118
Epoch 1130, training loss: 312.5287780761719 = 0.2736614942550659 + 50.0 * 6.245102405548096
Epoch 1130, val loss: 1.3126928806304932
Epoch 1140, training loss: 312.5707092285156 = 0.26677680015563965 + 50.0 * 6.2460784912109375
Epoch 1140, val loss: 1.3203774690628052
Epoch 1150, training loss: 312.3919372558594 = 0.26003938913345337 + 50.0 * 6.242638111114502
Epoch 1150, val loss: 1.3283747434616089
Epoch 1160, training loss: 312.3576965332031 = 0.2535196840763092 + 50.0 * 6.24208402633667
Epoch 1160, val loss: 1.336713194847107
Epoch 1170, training loss: 312.36492919921875 = 0.2471788227558136 + 50.0 * 6.242354869842529
Epoch 1170, val loss: 1.3447619676589966
Epoch 1180, training loss: 312.5168151855469 = 0.2409639209508896 + 50.0 * 6.245517253875732
Epoch 1180, val loss: 1.3531838655471802
Epoch 1190, training loss: 312.4021911621094 = 0.2349015772342682 + 50.0 * 6.243345737457275
Epoch 1190, val loss: 1.3622685670852661
Epoch 1200, training loss: 312.3559875488281 = 0.2289554327726364 + 50.0 * 6.2425408363342285
Epoch 1200, val loss: 1.3705302476882935
Epoch 1210, training loss: 312.218994140625 = 0.22318999469280243 + 50.0 * 6.2399163246154785
Epoch 1210, val loss: 1.3792285919189453
Epoch 1220, training loss: 312.1700744628906 = 0.21759431064128876 + 50.0 * 6.239049911499023
Epoch 1220, val loss: 1.3878575563430786
Epoch 1230, training loss: 312.2681884765625 = 0.21217384934425354 + 50.0 * 6.241119861602783
Epoch 1230, val loss: 1.3966587781906128
Epoch 1240, training loss: 312.19024658203125 = 0.20682550966739655 + 50.0 * 6.239668846130371
Epoch 1240, val loss: 1.4058785438537598
Epoch 1250, training loss: 312.1128845214844 = 0.20160600543022156 + 50.0 * 6.23822546005249
Epoch 1250, val loss: 1.414617657661438
Epoch 1260, training loss: 312.3410949707031 = 0.19654443860054016 + 50.0 * 6.242891311645508
Epoch 1260, val loss: 1.423736572265625
Epoch 1270, training loss: 312.0982971191406 = 0.1916009485721588 + 50.0 * 6.238133907318115
Epoch 1270, val loss: 1.432742714881897
Epoch 1280, training loss: 312.0339050292969 = 0.18681283295154572 + 50.0 * 6.236942291259766
Epoch 1280, val loss: 1.4419739246368408
Epoch 1290, training loss: 312.1381530761719 = 0.18216286599636078 + 50.0 * 6.239120006561279
Epoch 1290, val loss: 1.4512544870376587
Epoch 1300, training loss: 311.996826171875 = 0.17758484184741974 + 50.0 * 6.236384868621826
Epoch 1300, val loss: 1.4604004621505737
Epoch 1310, training loss: 311.9566345214844 = 0.17315828800201416 + 50.0 * 6.2356696128845215
Epoch 1310, val loss: 1.4700030088424683
Epoch 1320, training loss: 311.9574279785156 = 0.16886286437511444 + 50.0 * 6.235771179199219
Epoch 1320, val loss: 1.4796562194824219
Epoch 1330, training loss: 312.04827880859375 = 0.16467973589897156 + 50.0 * 6.237671852111816
Epoch 1330, val loss: 1.4890161752700806
Epoch 1340, training loss: 312.0707092285156 = 0.160600945353508 + 50.0 * 6.23820161819458
Epoch 1340, val loss: 1.4985711574554443
Epoch 1350, training loss: 312.0419006347656 = 0.15657439827919006 + 50.0 * 6.237706184387207
Epoch 1350, val loss: 1.5087518692016602
Epoch 1360, training loss: 311.8765563964844 = 0.15269148349761963 + 50.0 * 6.234477519989014
Epoch 1360, val loss: 1.518317461013794
Epoch 1370, training loss: 311.8047180175781 = 0.1489245742559433 + 50.0 * 6.2331156730651855
Epoch 1370, val loss: 1.5286884307861328
Epoch 1380, training loss: 311.8640441894531 = 0.14529931545257568 + 50.0 * 6.234375
Epoch 1380, val loss: 1.5385117530822754
Epoch 1390, training loss: 311.9186706542969 = 0.1417362242937088 + 50.0 * 6.235538959503174
Epoch 1390, val loss: 1.548797369003296
Epoch 1400, training loss: 311.8079528808594 = 0.13823126256465912 + 50.0 * 6.233394145965576
Epoch 1400, val loss: 1.5578235387802124
Epoch 1410, training loss: 311.7290954589844 = 0.13484808802604675 + 50.0 * 6.231884956359863
Epoch 1410, val loss: 1.5680848360061646
Epoch 1420, training loss: 311.8240966796875 = 0.13158538937568665 + 50.0 * 6.233850002288818
Epoch 1420, val loss: 1.577976107597351
Epoch 1430, training loss: 311.724365234375 = 0.12838783860206604 + 50.0 * 6.231919765472412
Epoch 1430, val loss: 1.588486671447754
Epoch 1440, training loss: 311.72271728515625 = 0.12529055774211884 + 50.0 * 6.231948375701904
Epoch 1440, val loss: 1.5988836288452148
Epoch 1450, training loss: 311.6922912597656 = 0.12226133048534393 + 50.0 * 6.231400966644287
Epoch 1450, val loss: 1.6086900234222412
Epoch 1460, training loss: 311.74066162109375 = 0.11934629082679749 + 50.0 * 6.232426166534424
Epoch 1460, val loss: 1.6190530061721802
Epoch 1470, training loss: 311.86663818359375 = 0.11646527051925659 + 50.0 * 6.23500394821167
Epoch 1470, val loss: 1.628810167312622
Epoch 1480, training loss: 311.6257629394531 = 0.11365916579961777 + 50.0 * 6.2302422523498535
Epoch 1480, val loss: 1.6386157274246216
Epoch 1490, training loss: 311.5716857910156 = 0.11095372587442398 + 50.0 * 6.229214191436768
Epoch 1490, val loss: 1.6492334604263306
Epoch 1500, training loss: 311.5504455566406 = 0.10834288597106934 + 50.0 * 6.228842258453369
Epoch 1500, val loss: 1.6590110063552856
Epoch 1510, training loss: 311.84405517578125 = 0.10581141710281372 + 50.0 * 6.23476505279541
Epoch 1510, val loss: 1.6690540313720703
Epoch 1520, training loss: 311.6131591796875 = 0.1033083125948906 + 50.0 * 6.230197429656982
Epoch 1520, val loss: 1.6795766353607178
Epoch 1530, training loss: 311.5340270996094 = 0.10086880624294281 + 50.0 * 6.228663444519043
Epoch 1530, val loss: 1.68956458568573
Epoch 1540, training loss: 311.49560546875 = 0.0985376387834549 + 50.0 * 6.227941513061523
Epoch 1540, val loss: 1.6997034549713135
Epoch 1550, training loss: 311.8156433105469 = 0.0962643027305603 + 50.0 * 6.2343878746032715
Epoch 1550, val loss: 1.7096070051193237
Epoch 1560, training loss: 311.5846252441406 = 0.0940222293138504 + 50.0 * 6.229812145233154
Epoch 1560, val loss: 1.7207303047180176
Epoch 1570, training loss: 311.5011291503906 = 0.09184489399194717 + 50.0 * 6.228185653686523
Epoch 1570, val loss: 1.7302303314208984
Epoch 1580, training loss: 311.43408203125 = 0.08975334465503693 + 50.0 * 6.226886749267578
Epoch 1580, val loss: 1.7407723665237427
Epoch 1590, training loss: 311.50439453125 = 0.08772870153188705 + 50.0 * 6.228332996368408
Epoch 1590, val loss: 1.7509173154830933
Epoch 1600, training loss: 311.4281311035156 = 0.08573798090219498 + 50.0 * 6.226848125457764
Epoch 1600, val loss: 1.7611809968948364
Epoch 1610, training loss: 311.4752502441406 = 0.08379817754030228 + 50.0 * 6.2278289794921875
Epoch 1610, val loss: 1.7709074020385742
Epoch 1620, training loss: 311.5228271484375 = 0.08192224055528641 + 50.0 * 6.228818416595459
Epoch 1620, val loss: 1.7810341119766235
Epoch 1630, training loss: 311.40814208984375 = 0.08006716519594193 + 50.0 * 6.226561069488525
Epoch 1630, val loss: 1.790157675743103
Epoch 1640, training loss: 311.4320373535156 = 0.07829879224300385 + 50.0 * 6.22707462310791
Epoch 1640, val loss: 1.8008257150650024
Epoch 1650, training loss: 311.4364929199219 = 0.07656803727149963 + 50.0 * 6.227198600769043
Epoch 1650, val loss: 1.8105326890945435
Epoch 1660, training loss: 311.2760925292969 = 0.07489021122455597 + 50.0 * 6.224024295806885
Epoch 1660, val loss: 1.8212330341339111
Epoch 1670, training loss: 311.3258361816406 = 0.07326974719762802 + 50.0 * 6.225051403045654
Epoch 1670, val loss: 1.8312792778015137
Epoch 1680, training loss: 311.410400390625 = 0.07168795168399811 + 50.0 * 6.226774215698242
Epoch 1680, val loss: 1.8407261371612549
Epoch 1690, training loss: 311.24896240234375 = 0.0701267421245575 + 50.0 * 6.223576545715332
Epoch 1690, val loss: 1.8504278659820557
Epoch 1700, training loss: 311.2957458496094 = 0.06863154470920563 + 50.0 * 6.224542140960693
Epoch 1700, val loss: 1.860703468322754
Epoch 1710, training loss: 311.27667236328125 = 0.06715516000986099 + 50.0 * 6.2241902351379395
Epoch 1710, val loss: 1.8700703382492065
Epoch 1720, training loss: 311.2685852050781 = 0.0657271072268486 + 50.0 * 6.224056720733643
Epoch 1720, val loss: 1.8790595531463623
Epoch 1730, training loss: 311.5428161621094 = 0.06434542685747147 + 50.0 * 6.229569911956787
Epoch 1730, val loss: 1.8886382579803467
Epoch 1740, training loss: 311.27642822265625 = 0.06298062205314636 + 50.0 * 6.224268913269043
Epoch 1740, val loss: 1.8990942239761353
Epoch 1750, training loss: 311.1676940917969 = 0.06165260076522827 + 50.0 * 6.222120761871338
Epoch 1750, val loss: 1.9082924127578735
Epoch 1760, training loss: 311.1494445800781 = 0.060385409742593765 + 50.0 * 6.221781253814697
Epoch 1760, val loss: 1.9179760217666626
Epoch 1770, training loss: 311.4537048339844 = 0.059145938605070114 + 50.0 * 6.227891445159912
Epoch 1770, val loss: 1.927042007446289
Epoch 1780, training loss: 311.2922058105469 = 0.05792960152029991 + 50.0 * 6.2246856689453125
Epoch 1780, val loss: 1.9363077878952026
Epoch 1790, training loss: 311.1896057128906 = 0.05673157423734665 + 50.0 * 6.222657680511475
Epoch 1790, val loss: 1.9464095830917358
Epoch 1800, training loss: 311.1224670410156 = 0.055583760142326355 + 50.0 * 6.221337795257568
Epoch 1800, val loss: 1.9551138877868652
Epoch 1810, training loss: 311.0661315917969 = 0.054470013827085495 + 50.0 * 6.22023344039917
Epoch 1810, val loss: 1.96470046043396
Epoch 1820, training loss: 311.0690002441406 = 0.053391724824905396 + 50.0 * 6.220312118530273
Epoch 1820, val loss: 1.9739208221435547
Epoch 1830, training loss: 311.3500061035156 = 0.05234229564666748 + 50.0 * 6.225953578948975
Epoch 1830, val loss: 1.9822279214859009
Epoch 1840, training loss: 311.2375793457031 = 0.051293909549713135 + 50.0 * 6.22372579574585
Epoch 1840, val loss: 1.9917787313461304
Epoch 1850, training loss: 311.1148376464844 = 0.05026048421859741 + 50.0 * 6.221291542053223
Epoch 1850, val loss: 2.0008020401000977
Epoch 1860, training loss: 311.1755065917969 = 0.049288418143987656 + 50.0 * 6.222524642944336
Epoch 1860, val loss: 2.010282516479492
Epoch 1870, training loss: 311.0269775390625 = 0.04831703379750252 + 50.0 * 6.219573497772217
Epoch 1870, val loss: 2.019011974334717
Epoch 1880, training loss: 311.03497314453125 = 0.04738758131861687 + 50.0 * 6.219751834869385
Epoch 1880, val loss: 2.0276684761047363
Epoch 1890, training loss: 310.96942138671875 = 0.046488258987665176 + 50.0 * 6.218458652496338
Epoch 1890, val loss: 2.0368635654449463
Epoch 1900, training loss: 311.205322265625 = 0.04562870040535927 + 50.0 * 6.223194122314453
Epoch 1900, val loss: 2.046358585357666
Epoch 1910, training loss: 311.0229187011719 = 0.044741395860910416 + 50.0 * 6.2195634841918945
Epoch 1910, val loss: 2.0534892082214355
Epoch 1920, training loss: 310.9768981933594 = 0.04389515146613121 + 50.0 * 6.218660354614258
Epoch 1920, val loss: 2.0625245571136475
Epoch 1930, training loss: 311.0313415527344 = 0.04307815805077553 + 50.0 * 6.2197651863098145
Epoch 1930, val loss: 2.070856809616089
Epoch 1940, training loss: 310.9896240234375 = 0.042267732322216034 + 50.0 * 6.218947410583496
Epoch 1940, val loss: 2.0790820121765137
Epoch 1950, training loss: 311.05419921875 = 0.04148724302649498 + 50.0 * 6.220254421234131
Epoch 1950, val loss: 2.088381767272949
Epoch 1960, training loss: 310.9266052246094 = 0.040727391839027405 + 50.0 * 6.217717170715332
Epoch 1960, val loss: 2.096524238586426
Epoch 1970, training loss: 310.9457092285156 = 0.03998972848057747 + 50.0 * 6.218114376068115
Epoch 1970, val loss: 2.105168342590332
Epoch 1980, training loss: 310.9882507324219 = 0.039263978600502014 + 50.0 * 6.218979358673096
Epoch 1980, val loss: 2.11372709274292
Epoch 1990, training loss: 310.9662780761719 = 0.03855467215180397 + 50.0 * 6.218554496765137
Epoch 1990, val loss: 2.122022867202759
Epoch 2000, training loss: 311.0730285644531 = 0.03787154331803322 + 50.0 * 6.220703125
Epoch 2000, val loss: 2.1302993297576904
Epoch 2010, training loss: 311.0943908691406 = 0.03718971833586693 + 50.0 * 6.22114372253418
Epoch 2010, val loss: 2.138024091720581
Epoch 2020, training loss: 310.9125671386719 = 0.036516666412353516 + 50.0 * 6.217520713806152
Epoch 2020, val loss: 2.1460797786712646
Epoch 2030, training loss: 310.8590087890625 = 0.03588366135954857 + 50.0 * 6.2164626121521
Epoch 2030, val loss: 2.154151201248169
Epoch 2040, training loss: 310.94537353515625 = 0.03527071699500084 + 50.0 * 6.218201637268066
Epoch 2040, val loss: 2.163212537765503
Epoch 2050, training loss: 310.8604736328125 = 0.03465031832456589 + 50.0 * 6.216516971588135
Epoch 2050, val loss: 2.1704647541046143
Epoch 2060, training loss: 310.8653869628906 = 0.03405348211526871 + 50.0 * 6.2166266441345215
Epoch 2060, val loss: 2.1776840686798096
Epoch 2070, training loss: 310.84136962890625 = 0.03347821161150932 + 50.0 * 6.216157913208008
Epoch 2070, val loss: 2.1856930255889893
Epoch 2080, training loss: 310.8239440917969 = 0.032914649695158005 + 50.0 * 6.2158203125
Epoch 2080, val loss: 2.1934316158294678
Epoch 2090, training loss: 310.8732604980469 = 0.03236348181962967 + 50.0 * 6.216818332672119
Epoch 2090, val loss: 2.2017946243286133
Epoch 2100, training loss: 310.87542724609375 = 0.03182541951537132 + 50.0 * 6.216871738433838
Epoch 2100, val loss: 2.209519624710083
Epoch 2110, training loss: 310.7939147949219 = 0.031294338405132294 + 50.0 * 6.21525239944458
Epoch 2110, val loss: 2.217318296432495
Epoch 2120, training loss: 311.03436279296875 = 0.03078331984579563 + 50.0 * 6.220071315765381
Epoch 2120, val loss: 2.224595069885254
Epoch 2130, training loss: 310.75439453125 = 0.03025800921022892 + 50.0 * 6.21448278427124
Epoch 2130, val loss: 2.2317962646484375
Epoch 2140, training loss: 310.6743469238281 = 0.02976296655833721 + 50.0 * 6.212892055511475
Epoch 2140, val loss: 2.2402853965759277
Epoch 2150, training loss: 310.6808166503906 = 0.029292814433574677 + 50.0 * 6.2130303382873535
Epoch 2150, val loss: 2.247921943664551
Epoch 2160, training loss: 310.8867492675781 = 0.028842667117714882 + 50.0 * 6.217158317565918
Epoch 2160, val loss: 2.256258487701416
Epoch 2170, training loss: 310.71905517578125 = 0.028368398547172546 + 50.0 * 6.213813781738281
Epoch 2170, val loss: 2.2614197731018066
Epoch 2180, training loss: 310.73248291015625 = 0.02791544981300831 + 50.0 * 6.2140913009643555
Epoch 2180, val loss: 2.269211769104004
Epoch 2190, training loss: 310.716552734375 = 0.027479270473122597 + 50.0 * 6.213781833648682
Epoch 2190, val loss: 2.2761647701263428
Epoch 2200, training loss: 310.70294189453125 = 0.027056269347667694 + 50.0 * 6.213517189025879
Epoch 2200, val loss: 2.284130811691284
Epoch 2210, training loss: 310.64642333984375 = 0.0266426969319582 + 50.0 * 6.212395668029785
Epoch 2210, val loss: 2.291454792022705
Epoch 2220, training loss: 310.76544189453125 = 0.026247292757034302 + 50.0 * 6.214783668518066
Epoch 2220, val loss: 2.298490285873413
Epoch 2230, training loss: 310.7588195800781 = 0.025841863825917244 + 50.0 * 6.214659690856934
Epoch 2230, val loss: 2.3057847023010254
Epoch 2240, training loss: 310.72833251953125 = 0.02543829195201397 + 50.0 * 6.214057445526123
Epoch 2240, val loss: 2.312068223953247
Epoch 2250, training loss: 310.7354431152344 = 0.025049986317753792 + 50.0 * 6.214207649230957
Epoch 2250, val loss: 2.318875551223755
Epoch 2260, training loss: 310.65484619140625 = 0.02466900274157524 + 50.0 * 6.212603569030762
Epoch 2260, val loss: 2.3254895210266113
Epoch 2270, training loss: 310.57562255859375 = 0.024303093552589417 + 50.0 * 6.211026668548584
Epoch 2270, val loss: 2.332688808441162
Epoch 2280, training loss: 310.5547790527344 = 0.023953067138791084 + 50.0 * 6.210616588592529
Epoch 2280, val loss: 2.33992600440979
Epoch 2290, training loss: 310.6682434082031 = 0.023613987490534782 + 50.0 * 6.212892532348633
Epoch 2290, val loss: 2.3469417095184326
Epoch 2300, training loss: 310.6273193359375 = 0.023262791335582733 + 50.0 * 6.212081432342529
Epoch 2300, val loss: 2.3526697158813477
Epoch 2310, training loss: 310.7413024902344 = 0.022923385724425316 + 50.0 * 6.214367389678955
Epoch 2310, val loss: 2.3590807914733887
Epoch 2320, training loss: 310.728759765625 = 0.022591786459088326 + 50.0 * 6.214123725891113
Epoch 2320, val loss: 2.3662893772125244
Epoch 2330, training loss: 310.5378112792969 = 0.022255845367908478 + 50.0 * 6.210311412811279
Epoch 2330, val loss: 2.3721985816955566
Epoch 2340, training loss: 310.5434265136719 = 0.021939592435956 + 50.0 * 6.210429668426514
Epoch 2340, val loss: 2.378570795059204
Epoch 2350, training loss: 310.6515197753906 = 0.02164315991103649 + 50.0 * 6.212597846984863
Epoch 2350, val loss: 2.385978937149048
Epoch 2360, training loss: 310.6385803222656 = 0.021334314718842506 + 50.0 * 6.212345123291016
Epoch 2360, val loss: 2.3913419246673584
Epoch 2370, training loss: 310.5594177246094 = 0.02103198878467083 + 50.0 * 6.21076774597168
Epoch 2370, val loss: 2.3975110054016113
Epoch 2380, training loss: 310.46563720703125 = 0.020743906497955322 + 50.0 * 6.208897590637207
Epoch 2380, val loss: 2.404144048690796
Epoch 2390, training loss: 310.48309326171875 = 0.020467109978199005 + 50.0 * 6.20925235748291
Epoch 2390, val loss: 2.4107272624969482
Epoch 2400, training loss: 310.69122314453125 = 0.020199934020638466 + 50.0 * 6.213420391082764
Epoch 2400, val loss: 2.417231559753418
Epoch 2410, training loss: 310.60028076171875 = 0.019911838695406914 + 50.0 * 6.211607933044434
Epoch 2410, val loss: 2.4213247299194336
Epoch 2420, training loss: 310.55413818359375 = 0.019637426361441612 + 50.0 * 6.210690021514893
Epoch 2420, val loss: 2.428168296813965
Epoch 2430, training loss: 310.8109130859375 = 0.01937824860215187 + 50.0 * 6.2158308029174805
Epoch 2430, val loss: 2.4339051246643066
Epoch 2440, training loss: 310.5201110839844 = 0.01910707913339138 + 50.0 * 6.210020065307617
Epoch 2440, val loss: 2.440059185028076
Epoch 2450, training loss: 310.412353515625 = 0.018854528665542603 + 50.0 * 6.207870006561279
Epoch 2450, val loss: 2.446380853652954
Epoch 2460, training loss: 310.42108154296875 = 0.018616732209920883 + 50.0 * 6.208049297332764
Epoch 2460, val loss: 2.4522879123687744
Epoch 2470, training loss: 310.4256591796875 = 0.018381299450993538 + 50.0 * 6.208145618438721
Epoch 2470, val loss: 2.4577832221984863
Epoch 2480, training loss: 310.8531494140625 = 0.01815723441541195 + 50.0 * 6.216700077056885
Epoch 2480, val loss: 2.46291184425354
Epoch 2490, training loss: 310.6834411621094 = 0.01791108399629593 + 50.0 * 6.213310241699219
Epoch 2490, val loss: 2.468773603439331
Epoch 2500, training loss: 310.4263000488281 = 0.017669957131147385 + 50.0 * 6.20817232131958
Epoch 2500, val loss: 2.474748373031616
Epoch 2510, training loss: 310.3717041015625 = 0.01744525320827961 + 50.0 * 6.207084655761719
Epoch 2510, val loss: 2.480419635772705
Epoch 2520, training loss: 310.3973083496094 = 0.017230719327926636 + 50.0 * 6.207601547241211
Epoch 2520, val loss: 2.485718011856079
Epoch 2530, training loss: 310.86761474609375 = 0.017022840678691864 + 50.0 * 6.217011451721191
Epoch 2530, val loss: 2.489814281463623
Epoch 2540, training loss: 310.5542297363281 = 0.016807230189442635 + 50.0 * 6.210748195648193
Epoch 2540, val loss: 2.4971237182617188
Epoch 2550, training loss: 310.41790771484375 = 0.016590263694524765 + 50.0 * 6.20802640914917
Epoch 2550, val loss: 2.5021259784698486
Epoch 2560, training loss: 310.455078125 = 0.016396377235651016 + 50.0 * 6.208773612976074
Epoch 2560, val loss: 2.508561372756958
Epoch 2570, training loss: 310.58355712890625 = 0.016194770112633705 + 50.0 * 6.2113471031188965
Epoch 2570, val loss: 2.512890338897705
Epoch 2580, training loss: 310.484375 = 0.015993069857358932 + 50.0 * 6.209367752075195
Epoch 2580, val loss: 2.518087387084961
Epoch 2590, training loss: 310.3470458984375 = 0.015798818320035934 + 50.0 * 6.206624507904053
Epoch 2590, val loss: 2.5238289833068848
Epoch 2600, training loss: 310.31622314453125 = 0.015613095834851265 + 50.0 * 6.206011772155762
Epoch 2600, val loss: 2.5290801525115967
Epoch 2610, training loss: 310.4494323730469 = 0.01543471496552229 + 50.0 * 6.208679676055908
Epoch 2610, val loss: 2.5346262454986572
Epoch 2620, training loss: 310.4067077636719 = 0.015251163393259048 + 50.0 * 6.207828998565674
Epoch 2620, val loss: 2.5394845008850098
Epoch 2630, training loss: 310.48724365234375 = 0.01507201511412859 + 50.0 * 6.209443092346191
Epoch 2630, val loss: 2.5436887741088867
Epoch 2640, training loss: 310.35308837890625 = 0.01488874014467001 + 50.0 * 6.206764221191406
Epoch 2640, val loss: 2.5488362312316895
Epoch 2650, training loss: 310.34454345703125 = 0.014718965627253056 + 50.0 * 6.206596851348877
Epoch 2650, val loss: 2.554353713989258
Epoch 2660, training loss: 310.5801086425781 = 0.014557174406945705 + 50.0 * 6.211310863494873
Epoch 2660, val loss: 2.55972957611084
Epoch 2670, training loss: 310.3470153808594 = 0.014382840134203434 + 50.0 * 6.206653118133545
Epoch 2670, val loss: 2.5628857612609863
Epoch 2680, training loss: 310.27410888671875 = 0.01421835646033287 + 50.0 * 6.205197811126709
Epoch 2680, val loss: 2.569307327270508
Epoch 2690, training loss: 310.2947082519531 = 0.014063134789466858 + 50.0 * 6.205612659454346
Epoch 2690, val loss: 2.573310375213623
Epoch 2700, training loss: 310.61566162109375 = 0.013909345492720604 + 50.0 * 6.212035179138184
Epoch 2700, val loss: 2.57820200920105
Epoch 2710, training loss: 310.3675537109375 = 0.01375096756964922 + 50.0 * 6.207076072692871
Epoch 2710, val loss: 2.5829877853393555
Epoch 2720, training loss: 310.2785949707031 = 0.013595151714980602 + 50.0 * 6.205300331115723
Epoch 2720, val loss: 2.5872254371643066
Epoch 2730, training loss: 310.27911376953125 = 0.013447661884129047 + 50.0 * 6.205313205718994
Epoch 2730, val loss: 2.591921806335449
Epoch 2740, training loss: 310.3991394042969 = 0.013303663581609726 + 50.0 * 6.207716464996338
Epoch 2740, val loss: 2.5963330268859863
Epoch 2750, training loss: 310.4718933105469 = 0.013156411238014698 + 50.0 * 6.209174633026123
Epoch 2750, val loss: 2.6014175415039062
Epoch 2760, training loss: 310.30419921875 = 0.013010446913540363 + 50.0 * 6.20582389831543
Epoch 2760, val loss: 2.6061911582946777
Epoch 2770, training loss: 310.2279052734375 = 0.0128696383908391 + 50.0 * 6.204300403594971
Epoch 2770, val loss: 2.610922336578369
Epoch 2780, training loss: 310.2278747558594 = 0.012738552875816822 + 50.0 * 6.20430326461792
Epoch 2780, val loss: 2.616217851638794
Epoch 2790, training loss: 310.3236999511719 = 0.012607540935277939 + 50.0 * 6.206222057342529
Epoch 2790, val loss: 2.620715618133545
Epoch 2800, training loss: 310.21856689453125 = 0.012473039329051971 + 50.0 * 6.2041215896606445
Epoch 2800, val loss: 2.623598337173462
Epoch 2810, training loss: 310.431396484375 = 0.012346640229225159 + 50.0 * 6.208380699157715
Epoch 2810, val loss: 2.628275156021118
Epoch 2820, training loss: 310.2062072753906 = 0.012212587520480156 + 50.0 * 6.2038798332214355
Epoch 2820, val loss: 2.6336774826049805
Epoch 2830, training loss: 310.1588439941406 = 0.012083329260349274 + 50.0 * 6.202935218811035
Epoch 2830, val loss: 2.6375317573547363
Epoch 2840, training loss: 310.157958984375 = 0.01196234580129385 + 50.0 * 6.202919960021973
Epoch 2840, val loss: 2.6417179107666016
Epoch 2850, training loss: 310.1805419921875 = 0.011845475062727928 + 50.0 * 6.203373908996582
Epoch 2850, val loss: 2.646130323410034
Epoch 2860, training loss: 310.5361633300781 = 0.011730491183698177 + 50.0 * 6.210488796234131
Epoch 2860, val loss: 2.6498801708221436
Epoch 2870, training loss: 310.3689880371094 = 0.01160496100783348 + 50.0 * 6.20714807510376
Epoch 2870, val loss: 2.6545536518096924
Epoch 2880, training loss: 310.1553039550781 = 0.011479342356324196 + 50.0 * 6.202876567840576
Epoch 2880, val loss: 2.658642530441284
Epoch 2890, training loss: 310.1134338378906 = 0.011367524042725563 + 50.0 * 6.2020416259765625
Epoch 2890, val loss: 2.6632378101348877
Epoch 2900, training loss: 310.0997619628906 = 0.0112605020403862 + 50.0 * 6.201770305633545
Epoch 2900, val loss: 2.667592763900757
Epoch 2910, training loss: 310.3014831542969 = 0.011159063316881657 + 50.0 * 6.205806255340576
Epoch 2910, val loss: 2.6720468997955322
Epoch 2920, training loss: 310.12213134765625 = 0.011045406572520733 + 50.0 * 6.202221870422363
Epoch 2920, val loss: 2.6751835346221924
Epoch 2930, training loss: 310.2125549316406 = 0.010938636027276516 + 50.0 * 6.2040324211120605
Epoch 2930, val loss: 2.679624319076538
Epoch 2940, training loss: 310.0658874511719 = 0.010828239843249321 + 50.0 * 6.201101303100586
Epoch 2940, val loss: 2.6836860179901123
Epoch 2950, training loss: 310.10906982421875 = 0.010727634653449059 + 50.0 * 6.201966762542725
Epoch 2950, val loss: 2.6878857612609863
Epoch 2960, training loss: 310.436767578125 = 0.010633553378283978 + 50.0 * 6.208522796630859
Epoch 2960, val loss: 2.6912269592285156
Epoch 2970, training loss: 310.12786865234375 = 0.010524624027311802 + 50.0 * 6.2023468017578125
Epoch 2970, val loss: 2.695155382156372
Epoch 2980, training loss: 310.054443359375 = 0.010421388782560825 + 50.0 * 6.200881004333496
Epoch 2980, val loss: 2.6990997791290283
Epoch 2990, training loss: 310.0423583984375 = 0.010326670482754707 + 50.0 * 6.200640678405762
Epoch 2990, val loss: 2.7032530307769775
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6407407407407407
0.8017923036373221
=== training gcn model ===
Epoch 0, training loss: 431.7706604003906 = 1.9276264905929565 + 50.0 * 8.596860885620117
Epoch 0, val loss: 1.9284183979034424
Epoch 10, training loss: 431.7331237792969 = 1.919983983039856 + 50.0 * 8.59626293182373
Epoch 10, val loss: 1.9204508066177368
Epoch 20, training loss: 431.50579833984375 = 1.910617709159851 + 50.0 * 8.591903686523438
Epoch 20, val loss: 1.9106513261795044
Epoch 30, training loss: 429.86346435546875 = 1.898420810699463 + 50.0 * 8.559301376342773
Epoch 30, val loss: 1.8978341817855835
Epoch 40, training loss: 419.0586853027344 = 1.8828890323638916 + 50.0 * 8.343515396118164
Epoch 40, val loss: 1.8821040391921997
Epoch 50, training loss: 380.9016418457031 = 1.8645436763763428 + 50.0 * 7.580742359161377
Epoch 50, val loss: 1.863947868347168
Epoch 60, training loss: 368.204345703125 = 1.8499224185943604 + 50.0 * 7.327088356018066
Epoch 60, val loss: 1.8503074645996094
Epoch 70, training loss: 356.31671142578125 = 1.840069055557251 + 50.0 * 7.08953332901001
Epoch 70, val loss: 1.8404841423034668
Epoch 80, training loss: 349.2864990234375 = 1.8302088975906372 + 50.0 * 6.949126243591309
Epoch 80, val loss: 1.8307504653930664
Epoch 90, training loss: 345.3086242675781 = 1.8215510845184326 + 50.0 * 6.869741439819336
Epoch 90, val loss: 1.822203516960144
Epoch 100, training loss: 341.6368103027344 = 1.812472939491272 + 50.0 * 6.796486854553223
Epoch 100, val loss: 1.8138521909713745
Epoch 110, training loss: 339.30377197265625 = 1.8046079874038696 + 50.0 * 6.749983787536621
Epoch 110, val loss: 1.8063633441925049
Epoch 120, training loss: 337.41632080078125 = 1.7963777780532837 + 50.0 * 6.712398529052734
Epoch 120, val loss: 1.7984980344772339
Epoch 130, training loss: 335.6315612792969 = 1.7877646684646606 + 50.0 * 6.676876068115234
Epoch 130, val loss: 1.7903817892074585
Epoch 140, training loss: 333.89984130859375 = 1.7792351245880127 + 50.0 * 6.642412185668945
Epoch 140, val loss: 1.7825008630752563
Epoch 150, training loss: 332.58740234375 = 1.7706469297409058 + 50.0 * 6.616334915161133
Epoch 150, val loss: 1.774705171585083
Epoch 160, training loss: 331.1205749511719 = 1.761185884475708 + 50.0 * 6.587187767028809
Epoch 160, val loss: 1.7663679122924805
Epoch 170, training loss: 329.8588562011719 = 1.7510918378829956 + 50.0 * 6.562155246734619
Epoch 170, val loss: 1.757332444190979
Epoch 180, training loss: 328.74896240234375 = 1.740114688873291 + 50.0 * 6.540176868438721
Epoch 180, val loss: 1.7476270198822021
Epoch 190, training loss: 328.0175476074219 = 1.727899432182312 + 50.0 * 6.525793075561523
Epoch 190, val loss: 1.7369009256362915
Epoch 200, training loss: 327.0539245605469 = 1.7146474123001099 + 50.0 * 6.5067853927612305
Epoch 200, val loss: 1.7249871492385864
Epoch 210, training loss: 326.2338562011719 = 1.7002166509628296 + 50.0 * 6.490672588348389
Epoch 210, val loss: 1.7122982740402222
Epoch 220, training loss: 325.5495300292969 = 1.6846977472305298 + 50.0 * 6.477296829223633
Epoch 220, val loss: 1.698642373085022
Epoch 230, training loss: 325.1177062988281 = 1.6680775880813599 + 50.0 * 6.468992233276367
Epoch 230, val loss: 1.6839555501937866
Epoch 240, training loss: 324.4527587890625 = 1.6499505043029785 + 50.0 * 6.456056118011475
Epoch 240, val loss: 1.6682453155517578
Epoch 250, training loss: 323.8790588378906 = 1.6308858394622803 + 50.0 * 6.444963455200195
Epoch 250, val loss: 1.6516982316970825
Epoch 260, training loss: 323.3955078125 = 1.6108158826828003 + 50.0 * 6.435693740844727
Epoch 260, val loss: 1.6343883275985718
Epoch 270, training loss: 323.0384521484375 = 1.5898650884628296 + 50.0 * 6.428971767425537
Epoch 270, val loss: 1.6164337396621704
Epoch 280, training loss: 322.6907958984375 = 1.5679056644439697 + 50.0 * 6.422457695007324
Epoch 280, val loss: 1.5976182222366333
Epoch 290, training loss: 322.1683349609375 = 1.545249104499817 + 50.0 * 6.412461280822754
Epoch 290, val loss: 1.578433871269226
Epoch 300, training loss: 321.81536865234375 = 1.5222163200378418 + 50.0 * 6.405863285064697
Epoch 300, val loss: 1.559138298034668
Epoch 310, training loss: 321.58282470703125 = 1.4988511800765991 + 50.0 * 6.401679992675781
Epoch 310, val loss: 1.5398370027542114
Epoch 320, training loss: 321.1995544433594 = 1.4746590852737427 + 50.0 * 6.394497871398926
Epoch 320, val loss: 1.5199772119522095
Epoch 330, training loss: 320.91156005859375 = 1.4506598711013794 + 50.0 * 6.389218330383301
Epoch 330, val loss: 1.5002206563949585
Epoch 340, training loss: 320.5400695800781 = 1.4265084266662598 + 50.0 * 6.3822712898254395
Epoch 340, val loss: 1.4806575775146484
Epoch 350, training loss: 320.2467956542969 = 1.4023222923278809 + 50.0 * 6.376889705657959
Epoch 350, val loss: 1.461370587348938
Epoch 360, training loss: 320.0562744140625 = 1.3780434131622314 + 50.0 * 6.373564720153809
Epoch 360, val loss: 1.4422391653060913
Epoch 370, training loss: 320.0946044921875 = 1.3536044359207153 + 50.0 * 6.374819755554199
Epoch 370, val loss: 1.4229731559753418
Epoch 380, training loss: 319.60302734375 = 1.329126238822937 + 50.0 * 6.365478038787842
Epoch 380, val loss: 1.4039463996887207
Epoch 390, training loss: 319.2484436035156 = 1.304455280303955 + 50.0 * 6.358880043029785
Epoch 390, val loss: 1.3851062059402466
Epoch 400, training loss: 319.0124816894531 = 1.2798705101013184 + 50.0 * 6.354652404785156
Epoch 400, val loss: 1.3664103746414185
Epoch 410, training loss: 318.91943359375 = 1.2550290822982788 + 50.0 * 6.353287696838379
Epoch 410, val loss: 1.3474000692367554
Epoch 420, training loss: 318.6099853515625 = 1.2301143407821655 + 50.0 * 6.347597599029541
Epoch 420, val loss: 1.3286569118499756
Epoch 430, training loss: 318.3842468261719 = 1.205194354057312 + 50.0 * 6.343581199645996
Epoch 430, val loss: 1.3101017475128174
Epoch 440, training loss: 318.5010070800781 = 1.1801873445510864 + 50.0 * 6.346416473388672
Epoch 440, val loss: 1.291519284248352
Epoch 450, training loss: 318.08203125 = 1.155400276184082 + 50.0 * 6.338532447814941
Epoch 450, val loss: 1.273046851158142
Epoch 460, training loss: 317.8499450683594 = 1.1305385828018188 + 50.0 * 6.334388256072998
Epoch 460, val loss: 1.2548879384994507
Epoch 470, training loss: 317.6493835449219 = 1.1059322357177734 + 50.0 * 6.330869197845459
Epoch 470, val loss: 1.2369755506515503
Epoch 480, training loss: 317.59173583984375 = 1.0816847085952759 + 50.0 * 6.330200672149658
Epoch 480, val loss: 1.219498634338379
Epoch 490, training loss: 317.3974609375 = 1.0573488473892212 + 50.0 * 6.3268022537231445
Epoch 490, val loss: 1.2016633749008179
Epoch 500, training loss: 317.457763671875 = 1.033683180809021 + 50.0 * 6.328481674194336
Epoch 500, val loss: 1.1851110458374023
Epoch 510, training loss: 317.1253967285156 = 1.0101782083511353 + 50.0 * 6.3223042488098145
Epoch 510, val loss: 1.1686588525772095
Epoch 520, training loss: 316.9599609375 = 0.9872540235519409 + 50.0 * 6.319454193115234
Epoch 520, val loss: 1.152755618095398
Epoch 530, training loss: 316.8012390136719 = 0.9648501873016357 + 50.0 * 6.316727638244629
Epoch 530, val loss: 1.137209415435791
Epoch 540, training loss: 316.6586608886719 = 0.9431124925613403 + 50.0 * 6.3143110275268555
Epoch 540, val loss: 1.1224396228790283
Epoch 550, training loss: 316.7582092285156 = 0.9218931794166565 + 50.0 * 6.316726207733154
Epoch 550, val loss: 1.1085691452026367
Epoch 560, training loss: 316.4294738769531 = 0.9009124040603638 + 50.0 * 6.310571193695068
Epoch 560, val loss: 1.0947505235671997
Epoch 570, training loss: 316.29205322265625 = 0.8807013630867004 + 50.0 * 6.308227062225342
Epoch 570, val loss: 1.0819103717803955
Epoch 580, training loss: 316.1668395996094 = 0.861122190952301 + 50.0 * 6.306114196777344
Epoch 580, val loss: 1.0695613622665405
Epoch 590, training loss: 316.36590576171875 = 0.8420107960700989 + 50.0 * 6.3104777336120605
Epoch 590, val loss: 1.0578789710998535
Epoch 600, training loss: 316.0443115234375 = 0.8231529593467712 + 50.0 * 6.3044233322143555
Epoch 600, val loss: 1.0464494228363037
Epoch 610, training loss: 315.8291931152344 = 0.8049598932266235 + 50.0 * 6.300484657287598
Epoch 610, val loss: 1.0358502864837646
Epoch 620, training loss: 315.796630859375 = 0.7873746752738953 + 50.0 * 6.300184726715088
Epoch 620, val loss: 1.0259958505630493
Epoch 630, training loss: 315.6927185058594 = 0.7700987458229065 + 50.0 * 6.298452377319336
Epoch 630, val loss: 1.0162739753723145
Epoch 640, training loss: 315.7138366699219 = 0.7532089352607727 + 50.0 * 6.29921293258667
Epoch 640, val loss: 1.0072687864303589
Epoch 650, training loss: 315.487548828125 = 0.7367760539054871 + 50.0 * 6.295015335083008
Epoch 650, val loss: 0.9984500408172607
Epoch 660, training loss: 315.3873291015625 = 0.7208445072174072 + 50.0 * 6.293329238891602
Epoch 660, val loss: 0.9906211495399475
Epoch 670, training loss: 315.33349609375 = 0.7052672505378723 + 50.0 * 6.292564868927002
Epoch 670, val loss: 0.9834110736846924
Epoch 680, training loss: 315.27996826171875 = 0.689994752407074 + 50.0 * 6.291799545288086
Epoch 680, val loss: 0.9759173393249512
Epoch 690, training loss: 315.21478271484375 = 0.6751200556755066 + 50.0 * 6.290793418884277
Epoch 690, val loss: 0.9690732359886169
Epoch 700, training loss: 315.22723388671875 = 0.6604588031768799 + 50.0 * 6.291335582733154
Epoch 700, val loss: 0.9627317190170288
Epoch 710, training loss: 315.0029296875 = 0.6461287140846252 + 50.0 * 6.287136077880859
Epoch 710, val loss: 0.9567395448684692
Epoch 720, training loss: 314.9034729003906 = 0.6322535872459412 + 50.0 * 6.28542423248291
Epoch 720, val loss: 0.9512414932250977
Epoch 730, training loss: 314.8172302246094 = 0.618760347366333 + 50.0 * 6.283968925476074
Epoch 730, val loss: 0.9463184475898743
Epoch 740, training loss: 315.309814453125 = 0.6055964231491089 + 50.0 * 6.294084548950195
Epoch 740, val loss: 0.941764771938324
Epoch 750, training loss: 314.75531005859375 = 0.5923571586608887 + 50.0 * 6.283259391784668
Epoch 750, val loss: 0.9369905591011047
Epoch 760, training loss: 314.6621398925781 = 0.5796541571617126 + 50.0 * 6.281649589538574
Epoch 760, val loss: 0.9330003261566162
Epoch 770, training loss: 314.5652770996094 = 0.5673553943634033 + 50.0 * 6.279958724975586
Epoch 770, val loss: 0.9296544194221497
Epoch 780, training loss: 314.9681701660156 = 0.5553707480430603 + 50.0 * 6.2882561683654785
Epoch 780, val loss: 0.926395833492279
Epoch 790, training loss: 314.56256103515625 = 0.543339192867279 + 50.0 * 6.280384540557861
Epoch 790, val loss: 0.9234566688537598
Epoch 800, training loss: 314.37628173828125 = 0.5318163633346558 + 50.0 * 6.276889324188232
Epoch 800, val loss: 0.9208640456199646
Epoch 810, training loss: 314.4234313964844 = 0.5205725431442261 + 50.0 * 6.278057098388672
Epoch 810, val loss: 0.9186028242111206
Epoch 820, training loss: 314.27252197265625 = 0.5095883011817932 + 50.0 * 6.275259017944336
Epoch 820, val loss: 0.9171270132064819
Epoch 830, training loss: 314.2569580078125 = 0.49889713525772095 + 50.0 * 6.275161266326904
Epoch 830, val loss: 0.9158241152763367
Epoch 840, training loss: 314.3047790527344 = 0.4883856177330017 + 50.0 * 6.276328086853027
Epoch 840, val loss: 0.9142536520957947
Epoch 850, training loss: 314.1670837402344 = 0.4781128168106079 + 50.0 * 6.273779392242432
Epoch 850, val loss: 0.9130983352661133
Epoch 860, training loss: 314.168701171875 = 0.46816855669021606 + 50.0 * 6.27401065826416
Epoch 860, val loss: 0.9126594066619873
Epoch 870, training loss: 314.19354248046875 = 0.45834097266197205 + 50.0 * 6.2747039794921875
Epoch 870, val loss: 0.9118226766586304
Epoch 880, training loss: 313.9703674316406 = 0.44874030351638794 + 50.0 * 6.270432472229004
Epoch 880, val loss: 0.9116078615188599
Epoch 890, training loss: 313.8696594238281 = 0.4394682049751282 + 50.0 * 6.268604278564453
Epoch 890, val loss: 0.9115775227546692
Epoch 900, training loss: 313.8857727050781 = 0.4304487109184265 + 50.0 * 6.269106388092041
Epoch 900, val loss: 0.9121473431587219
Epoch 910, training loss: 314.0085754394531 = 0.4214780926704407 + 50.0 * 6.27174186706543
Epoch 910, val loss: 0.9122348427772522
Epoch 920, training loss: 313.80157470703125 = 0.41268685460090637 + 50.0 * 6.267777919769287
Epoch 920, val loss: 0.9124848246574402
Epoch 930, training loss: 313.73626708984375 = 0.40413805842399597 + 50.0 * 6.2666425704956055
Epoch 930, val loss: 0.9129752516746521
Epoch 940, training loss: 313.6671447753906 = 0.3958730697631836 + 50.0 * 6.265425682067871
Epoch 940, val loss: 0.9142143130302429
Epoch 950, training loss: 313.6664123535156 = 0.38776370882987976 + 50.0 * 6.265573024749756
Epoch 950, val loss: 0.9152262806892395
Epoch 960, training loss: 313.5693664550781 = 0.37980085611343384 + 50.0 * 6.263791561126709
Epoch 960, val loss: 0.9165504574775696
Epoch 970, training loss: 313.9804382324219 = 0.37195855379104614 + 50.0 * 6.272169589996338
Epoch 970, val loss: 0.9176220893859863
Epoch 980, training loss: 313.49237060546875 = 0.36425068974494934 + 50.0 * 6.262562274932861
Epoch 980, val loss: 0.9192641973495483
Epoch 990, training loss: 313.434814453125 = 0.35674864053726196 + 50.0 * 6.261561393737793
Epoch 990, val loss: 0.9208324551582336
Epoch 1000, training loss: 313.3673400878906 = 0.34947338700294495 + 50.0 * 6.260356903076172
Epoch 1000, val loss: 0.9226760864257812
Epoch 1010, training loss: 313.3544616699219 = 0.3423601984977722 + 50.0 * 6.260242462158203
Epoch 1010, val loss: 0.9248188734054565
Epoch 1020, training loss: 313.5631103515625 = 0.33532196283340454 + 50.0 * 6.264555931091309
Epoch 1020, val loss: 0.9268744587898254
Epoch 1030, training loss: 313.3468017578125 = 0.3283758759498596 + 50.0 * 6.260368347167969
Epoch 1030, val loss: 0.9286434650421143
Epoch 1040, training loss: 313.26202392578125 = 0.321593314409256 + 50.0 * 6.258808612823486
Epoch 1040, val loss: 0.931004524230957
Epoch 1050, training loss: 313.3805236816406 = 0.31502091884613037 + 50.0 * 6.261309623718262
Epoch 1050, val loss: 0.9339480400085449
Epoch 1060, training loss: 313.1623229980469 = 0.30842143297195435 + 50.0 * 6.257078170776367
Epoch 1060, val loss: 0.9353778958320618
Epoch 1070, training loss: 313.0870056152344 = 0.3020426630973816 + 50.0 * 6.255699157714844
Epoch 1070, val loss: 0.938193678855896
Epoch 1080, training loss: 313.0818176269531 = 0.2958168685436249 + 50.0 * 6.255720138549805
Epoch 1080, val loss: 0.9408368468284607
Epoch 1090, training loss: 313.5619812011719 = 0.28967058658599854 + 50.0 * 6.265446186065674
Epoch 1090, val loss: 0.9432986974716187
Epoch 1100, training loss: 313.0976257324219 = 0.28356775641441345 + 50.0 * 6.25628137588501
Epoch 1100, val loss: 0.9461473822593689
Epoch 1110, training loss: 312.9748229980469 = 0.2775830924510956 + 50.0 * 6.2539448738098145
Epoch 1110, val loss: 0.9486130475997925
Epoch 1120, training loss: 312.9974670410156 = 0.2717910408973694 + 50.0 * 6.254513263702393
Epoch 1120, val loss: 0.9515397548675537
Epoch 1130, training loss: 312.9543762207031 = 0.2660785913467407 + 50.0 * 6.25376558303833
Epoch 1130, val loss: 0.9544066190719604
Epoch 1140, training loss: 312.8960876464844 = 0.26048165559768677 + 50.0 * 6.252712249755859
Epoch 1140, val loss: 0.9577216506004333
Epoch 1150, training loss: 313.062255859375 = 0.2550065517425537 + 50.0 * 6.256145000457764
Epoch 1150, val loss: 0.9611173272132874
Epoch 1160, training loss: 312.7914733886719 = 0.24949724972248077 + 50.0 * 6.2508392333984375
Epoch 1160, val loss: 0.9634471535682678
Epoch 1170, training loss: 312.74163818359375 = 0.24418053030967712 + 50.0 * 6.2499494552612305
Epoch 1170, val loss: 0.9668769240379333
Epoch 1180, training loss: 312.7037658691406 = 0.23898230493068695 + 50.0 * 6.249295711517334
Epoch 1180, val loss: 0.9701444506645203
Epoch 1190, training loss: 313.062255859375 = 0.23387695848941803 + 50.0 * 6.256567478179932
Epoch 1190, val loss: 0.9734106063842773
Epoch 1200, training loss: 312.77508544921875 = 0.22878766059875488 + 50.0 * 6.2509260177612305
Epoch 1200, val loss: 0.9770597219467163
Epoch 1210, training loss: 312.724609375 = 0.2238267958164215 + 50.0 * 6.250015735626221
Epoch 1210, val loss: 0.9806085824966431
Epoch 1220, training loss: 312.744140625 = 0.21893107891082764 + 50.0 * 6.250504016876221
Epoch 1220, val loss: 0.9835399389266968
Epoch 1230, training loss: 312.64453125 = 0.21410585939884186 + 50.0 * 6.248608112335205
Epoch 1230, val loss: 0.9875134229660034
Epoch 1240, training loss: 312.5361022949219 = 0.2094559520483017 + 50.0 * 6.246533393859863
Epoch 1240, val loss: 0.9914260506629944
Epoch 1250, training loss: 312.4981994628906 = 0.20489919185638428 + 50.0 * 6.245865821838379
Epoch 1250, val loss: 0.9953667521476746
Epoch 1260, training loss: 312.7762451171875 = 0.20042350888252258 + 50.0 * 6.251516819000244
Epoch 1260, val loss: 0.9991494417190552
Epoch 1270, training loss: 312.5428466796875 = 0.19591960310935974 + 50.0 * 6.246938705444336
Epoch 1270, val loss: 1.0024845600128174
Epoch 1280, training loss: 312.547119140625 = 0.1915941834449768 + 50.0 * 6.247110366821289
Epoch 1280, val loss: 1.0067076683044434
Epoch 1290, training loss: 312.4020690917969 = 0.18734093010425568 + 50.0 * 6.2442946434021
Epoch 1290, val loss: 1.010926604270935
Epoch 1300, training loss: 312.4316711425781 = 0.18318745493888855 + 50.0 * 6.244969367980957
Epoch 1300, val loss: 1.0146900415420532
Epoch 1310, training loss: 312.4292297363281 = 0.179128497838974 + 50.0 * 6.245001792907715
Epoch 1310, val loss: 1.0187139511108398
Epoch 1320, training loss: 312.3129577636719 = 0.17512838542461395 + 50.0 * 6.242756366729736
Epoch 1320, val loss: 1.0226490497589111
Epoch 1330, training loss: 312.31024169921875 = 0.1712343841791153 + 50.0 * 6.2427802085876465
Epoch 1330, val loss: 1.027037262916565
Epoch 1340, training loss: 312.480712890625 = 0.16744090616703033 + 50.0 * 6.246265411376953
Epoch 1340, val loss: 1.0304042100906372
Epoch 1350, training loss: 312.275146484375 = 0.1637013703584671 + 50.0 * 6.242228984832764
Epoch 1350, val loss: 1.0358234643936157
Epoch 1360, training loss: 312.2858581542969 = 0.16005323827266693 + 50.0 * 6.242516040802002
Epoch 1360, val loss: 1.0395634174346924
Epoch 1370, training loss: 312.3304748535156 = 0.15650594234466553 + 50.0 * 6.2434797286987305
Epoch 1370, val loss: 1.0441802740097046
Epoch 1380, training loss: 312.5249938964844 = 0.1530408412218094 + 50.0 * 6.247438907623291
Epoch 1380, val loss: 1.0487604141235352
Epoch 1390, training loss: 312.2049865722656 = 0.14957267045974731 + 50.0 * 6.241108417510986
Epoch 1390, val loss: 1.0526281595230103
Epoch 1400, training loss: 312.1116027832031 = 0.14626480638980865 + 50.0 * 6.239306926727295
Epoch 1400, val loss: 1.0575391054153442
Epoch 1410, training loss: 312.0569152832031 = 0.14305229485034943 + 50.0 * 6.238276958465576
Epoch 1410, val loss: 1.062063217163086
Epoch 1420, training loss: 312.03216552734375 = 0.1399197280406952 + 50.0 * 6.237844944000244
Epoch 1420, val loss: 1.0668480396270752
Epoch 1430, training loss: 312.2809753417969 = 0.13686950504779816 + 50.0 * 6.242881774902344
Epoch 1430, val loss: 1.0717076063156128
Epoch 1440, training loss: 312.135498046875 = 0.13380542397499084 + 50.0 * 6.2400336265563965
Epoch 1440, val loss: 1.0760301351547241
Epoch 1450, training loss: 312.2542419433594 = 0.13084951043128967 + 50.0 * 6.242467403411865
Epoch 1450, val loss: 1.08135986328125
Epoch 1460, training loss: 311.9870300292969 = 0.1279354691505432 + 50.0 * 6.237182140350342
Epoch 1460, val loss: 1.085902214050293
Epoch 1470, training loss: 311.95794677734375 = 0.12513165175914764 + 50.0 * 6.236656665802002
Epoch 1470, val loss: 1.0907686948776245
Epoch 1480, training loss: 311.95501708984375 = 0.12242240458726883 + 50.0 * 6.23665189743042
Epoch 1480, val loss: 1.0959842205047607
Epoch 1490, training loss: 312.1656188964844 = 0.11978305876255035 + 50.0 * 6.240916728973389
Epoch 1490, val loss: 1.1010608673095703
Epoch 1500, training loss: 311.9195251464844 = 0.11712265014648438 + 50.0 * 6.236048221588135
Epoch 1500, val loss: 1.105603575706482
Epoch 1510, training loss: 311.84869384765625 = 0.11459270119667053 + 50.0 * 6.234682083129883
Epoch 1510, val loss: 1.1108747720718384
Epoch 1520, training loss: 311.9285583496094 = 0.11216144263744354 + 50.0 * 6.236328125
Epoch 1520, val loss: 1.1164921522140503
Epoch 1530, training loss: 312.1564025878906 = 0.10973286628723145 + 50.0 * 6.240932941436768
Epoch 1530, val loss: 1.1208271980285645
Epoch 1540, training loss: 311.9527893066406 = 0.10734480619430542 + 50.0 * 6.23690938949585
Epoch 1540, val loss: 1.1260279417037964
Epoch 1550, training loss: 311.7987365722656 = 0.10503241419792175 + 50.0 * 6.2338738441467285
Epoch 1550, val loss: 1.1313494443893433
Epoch 1560, training loss: 311.75494384765625 = 0.10281404107809067 + 50.0 * 6.2330427169799805
Epoch 1560, val loss: 1.1367090940475464
Epoch 1570, training loss: 311.7600402832031 = 0.10066164284944534 + 50.0 * 6.233187675476074
Epoch 1570, val loss: 1.1422598361968994
Epoch 1580, training loss: 312.01202392578125 = 0.0985526293516159 + 50.0 * 6.238269805908203
Epoch 1580, val loss: 1.1475294828414917
Epoch 1590, training loss: 311.7641296386719 = 0.09643611311912537 + 50.0 * 6.233353614807129
Epoch 1590, val loss: 1.1525694131851196
Epoch 1600, training loss: 311.8132019042969 = 0.09442170709371567 + 50.0 * 6.234375476837158
Epoch 1600, val loss: 1.1580498218536377
Epoch 1610, training loss: 311.7550354003906 = 0.09241844713687897 + 50.0 * 6.23325252532959
Epoch 1610, val loss: 1.1633555889129639
Epoch 1620, training loss: 311.6593322753906 = 0.09045613557100296 + 50.0 * 6.231377601623535
Epoch 1620, val loss: 1.1685442924499512
Epoch 1630, training loss: 311.6126708984375 = 0.08859150111675262 + 50.0 * 6.2304816246032715
Epoch 1630, val loss: 1.1740365028381348
Epoch 1640, training loss: 311.6018981933594 = 0.08677776902914047 + 50.0 * 6.230301856994629
Epoch 1640, val loss: 1.179452896118164
Epoch 1650, training loss: 312.03399658203125 = 0.08501085638999939 + 50.0 * 6.238979339599609
Epoch 1650, val loss: 1.1846352815628052
Epoch 1660, training loss: 311.7137451171875 = 0.08323069661855698 + 50.0 * 6.23261022567749
Epoch 1660, val loss: 1.1899577379226685
Epoch 1670, training loss: 311.5993957519531 = 0.08151224255561829 + 50.0 * 6.230357646942139
Epoch 1670, val loss: 1.1955450773239136
Epoch 1680, training loss: 311.59088134765625 = 0.07987247407436371 + 50.0 * 6.230220317840576
Epoch 1680, val loss: 1.200955867767334
Epoch 1690, training loss: 311.9048767089844 = 0.07825980335474014 + 50.0 * 6.236532688140869
Epoch 1690, val loss: 1.2061810493469238
Epoch 1700, training loss: 311.5850524902344 = 0.07665096968412399 + 50.0 * 6.230167865753174
Epoch 1700, val loss: 1.2122021913528442
Epoch 1710, training loss: 311.4917297363281 = 0.07510937750339508 + 50.0 * 6.22833251953125
Epoch 1710, val loss: 1.217560887336731
Epoch 1720, training loss: 311.5516357421875 = 0.07363060116767883 + 50.0 * 6.229559898376465
Epoch 1720, val loss: 1.2232919931411743
Epoch 1730, training loss: 311.65869140625 = 0.07215700298547745 + 50.0 * 6.231730937957764
Epoch 1730, val loss: 1.228211760520935
Epoch 1740, training loss: 311.6946716308594 = 0.07068943977355957 + 50.0 * 6.232480049133301
Epoch 1740, val loss: 1.233160376548767
Epoch 1750, training loss: 311.46734619140625 = 0.06928792595863342 + 50.0 * 6.227961540222168
Epoch 1750, val loss: 1.239153504371643
Epoch 1760, training loss: 311.4415588378906 = 0.06793832033872604 + 50.0 * 6.22747278213501
Epoch 1760, val loss: 1.2448445558547974
Epoch 1770, training loss: 311.4039611816406 = 0.06662141531705856 + 50.0 * 6.226747035980225
Epoch 1770, val loss: 1.2503035068511963
Epoch 1780, training loss: 311.6186828613281 = 0.06534876674413681 + 50.0 * 6.231067180633545
Epoch 1780, val loss: 1.255832552909851
Epoch 1790, training loss: 311.3624572753906 = 0.06405501067638397 + 50.0 * 6.225967884063721
Epoch 1790, val loss: 1.2611855268478394
Epoch 1800, training loss: 311.3611755371094 = 0.06281229108572006 + 50.0 * 6.2259674072265625
Epoch 1800, val loss: 1.2666429281234741
Epoch 1810, training loss: 311.4459533691406 = 0.06162185221910477 + 50.0 * 6.227686882019043
Epoch 1810, val loss: 1.2722713947296143
Epoch 1820, training loss: 311.4701232910156 = 0.06044764444231987 + 50.0 * 6.228193283081055
Epoch 1820, val loss: 1.2778255939483643
Epoch 1830, training loss: 311.473388671875 = 0.05929100140929222 + 50.0 * 6.2282819747924805
Epoch 1830, val loss: 1.2833364009857178
Epoch 1840, training loss: 311.54595947265625 = 0.05816413834691048 + 50.0 * 6.229755878448486
Epoch 1840, val loss: 1.2888226509094238
Epoch 1850, training loss: 311.3942565917969 = 0.05703176185488701 + 50.0 * 6.226744174957275
Epoch 1850, val loss: 1.293450951576233
Epoch 1860, training loss: 311.300537109375 = 0.05597187578678131 + 50.0 * 6.224891185760498
Epoch 1860, val loss: 1.299392580986023
Epoch 1870, training loss: 311.2690734863281 = 0.05493026599287987 + 50.0 * 6.224282741546631
Epoch 1870, val loss: 1.3046271800994873
Epoch 1880, training loss: 311.54840087890625 = 0.05392623692750931 + 50.0 * 6.229889392852783
Epoch 1880, val loss: 1.3097363710403442
Epoch 1890, training loss: 311.2761535644531 = 0.0529128760099411 + 50.0 * 6.2244648933410645
Epoch 1890, val loss: 1.3157786130905151
Epoch 1900, training loss: 311.21826171875 = 0.051930975168943405 + 50.0 * 6.223326683044434
Epoch 1900, val loss: 1.3209267854690552
Epoch 1910, training loss: 311.29998779296875 = 0.050992533564567566 + 50.0 * 6.224979877471924
Epoch 1910, val loss: 1.3265670537948608
Epoch 1920, training loss: 311.3429260253906 = 0.05007357895374298 + 50.0 * 6.225856781005859
Epoch 1920, val loss: 1.3322300910949707
Epoch 1930, training loss: 311.5472412109375 = 0.04916003718972206 + 50.0 * 6.229961395263672
Epoch 1930, val loss: 1.3371667861938477
Epoch 1940, training loss: 311.2919616699219 = 0.048242732882499695 + 50.0 * 6.224874496459961
Epoch 1940, val loss: 1.3421930074691772
Epoch 1950, training loss: 311.2375183105469 = 0.04736834391951561 + 50.0 * 6.2238030433654785
Epoch 1950, val loss: 1.347409963607788
Epoch 1960, training loss: 311.4451904296875 = 0.04654406011104584 + 50.0 * 6.227972984313965
Epoch 1960, val loss: 1.3533210754394531
Epoch 1970, training loss: 311.1856384277344 = 0.045679397881031036 + 50.0 * 6.222798824310303
Epoch 1970, val loss: 1.3576022386550903
Epoch 1980, training loss: 311.1372985839844 = 0.044874053448438644 + 50.0 * 6.221848487854004
Epoch 1980, val loss: 1.3633686304092407
Epoch 1990, training loss: 311.1337890625 = 0.044091325253248215 + 50.0 * 6.2217936515808105
Epoch 1990, val loss: 1.3685758113861084
Epoch 2000, training loss: 311.12689208984375 = 0.04332495108246803 + 50.0 * 6.221671104431152
Epoch 2000, val loss: 1.373847484588623
Epoch 2010, training loss: 311.4254455566406 = 0.04257436841726303 + 50.0 * 6.227657318115234
Epoch 2010, val loss: 1.3785467147827148
Epoch 2020, training loss: 311.351318359375 = 0.041820354759693146 + 50.0 * 6.226190090179443
Epoch 2020, val loss: 1.3837584257125854
Epoch 2030, training loss: 311.2370300292969 = 0.041083890944719315 + 50.0 * 6.223918914794922
Epoch 2030, val loss: 1.388994574546814
Epoch 2040, training loss: 311.2616271972656 = 0.040362872183322906 + 50.0 * 6.224424839019775
Epoch 2040, val loss: 1.3934204578399658
Epoch 2050, training loss: 311.1315612792969 = 0.03966689854860306 + 50.0 * 6.221837997436523
Epoch 2050, val loss: 1.3989933729171753
Epoch 2060, training loss: 311.06195068359375 = 0.038986653089523315 + 50.0 * 6.220458984375
Epoch 2060, val loss: 1.404396414756775
Epoch 2070, training loss: 311.04095458984375 = 0.03832818940281868 + 50.0 * 6.220052242279053
Epoch 2070, val loss: 1.4093050956726074
Epoch 2080, training loss: 311.01788330078125 = 0.03769507259130478 + 50.0 * 6.219604015350342
Epoch 2080, val loss: 1.41460382938385
Epoch 2090, training loss: 311.3374938964844 = 0.03708536550402641 + 50.0 * 6.226008415222168
Epoch 2090, val loss: 1.4197189807891846
Epoch 2100, training loss: 311.029541015625 = 0.03643692657351494 + 50.0 * 6.21986198425293
Epoch 2100, val loss: 1.4241760969161987
Epoch 2110, training loss: 311.0421447753906 = 0.03583464026451111 + 50.0 * 6.220126152038574
Epoch 2110, val loss: 1.4292112588882446
Epoch 2120, training loss: 311.17822265625 = 0.03525058180093765 + 50.0 * 6.2228593826293945
Epoch 2120, val loss: 1.4344818592071533
Epoch 2130, training loss: 311.0883483886719 = 0.03466331213712692 + 50.0 * 6.221073627471924
Epoch 2130, val loss: 1.4391069412231445
Epoch 2140, training loss: 311.001953125 = 0.03409166261553764 + 50.0 * 6.219357013702393
Epoch 2140, val loss: 1.4437397718429565
Epoch 2150, training loss: 310.9307861328125 = 0.03354107588529587 + 50.0 * 6.217945098876953
Epoch 2150, val loss: 1.4488577842712402
Epoch 2160, training loss: 311.09674072265625 = 0.033013273030519485 + 50.0 * 6.221274375915527
Epoch 2160, val loss: 1.4538178443908691
Epoch 2170, training loss: 310.9792175292969 = 0.03247851878404617 + 50.0 * 6.218935012817383
Epoch 2170, val loss: 1.4585421085357666
Epoch 2180, training loss: 310.9674377441406 = 0.03195619210600853 + 50.0 * 6.218709468841553
Epoch 2180, val loss: 1.4634449481964111
Epoch 2190, training loss: 310.9765319824219 = 0.03144962713122368 + 50.0 * 6.21890115737915
Epoch 2190, val loss: 1.468183994293213
Epoch 2200, training loss: 310.92327880859375 = 0.03095151297748089 + 50.0 * 6.217846393585205
Epoch 2200, val loss: 1.4727280139923096
Epoch 2210, training loss: 310.9059143066406 = 0.030468860641121864 + 50.0 * 6.2175092697143555
Epoch 2210, val loss: 1.4772275686264038
Epoch 2220, training loss: 311.26861572265625 = 0.030001986771821976 + 50.0 * 6.2247724533081055
Epoch 2220, val loss: 1.48110032081604
Epoch 2230, training loss: 311.2181091308594 = 0.02952372282743454 + 50.0 * 6.223772048950195
Epoch 2230, val loss: 1.4861661195755005
Epoch 2240, training loss: 310.87200927734375 = 0.029047256335616112 + 50.0 * 6.216858863830566
Epoch 2240, val loss: 1.4910317659378052
Epoch 2250, training loss: 310.8445129394531 = 0.028607577085494995 + 50.0 * 6.216318607330322
Epoch 2250, val loss: 1.495713710784912
Epoch 2260, training loss: 310.8522644042969 = 0.028178198263049126 + 50.0 * 6.216482162475586
Epoch 2260, val loss: 1.5000108480453491
Epoch 2270, training loss: 311.29144287109375 = 0.02776082418859005 + 50.0 * 6.225273609161377
Epoch 2270, val loss: 1.504076361656189
Epoch 2280, training loss: 310.9797668457031 = 0.027338489890098572 + 50.0 * 6.219048500061035
Epoch 2280, val loss: 1.5092459917068481
Epoch 2290, training loss: 310.8473815917969 = 0.02691427804529667 + 50.0 * 6.216409206390381
Epoch 2290, val loss: 1.5131635665893555
Epoch 2300, training loss: 310.8345031738281 = 0.026519574224948883 + 50.0 * 6.216159820556641
Epoch 2300, val loss: 1.5177851915359497
Epoch 2310, training loss: 311.05950927734375 = 0.026133766397833824 + 50.0 * 6.220667839050293
Epoch 2310, val loss: 1.5218168497085571
Epoch 2320, training loss: 310.8188171386719 = 0.025743896141648293 + 50.0 * 6.2158613204956055
Epoch 2320, val loss: 1.5271233320236206
Epoch 2330, training loss: 310.8140563964844 = 0.025369156152009964 + 50.0 * 6.215774059295654
Epoch 2330, val loss: 1.5312532186508179
Epoch 2340, training loss: 310.8877868652344 = 0.024997765198349953 + 50.0 * 6.217255592346191
Epoch 2340, val loss: 1.5349371433258057
Epoch 2350, training loss: 310.9166259765625 = 0.024640396237373352 + 50.0 * 6.21783971786499
Epoch 2350, val loss: 1.539591908454895
Epoch 2360, training loss: 310.7685852050781 = 0.024275612086057663 + 50.0 * 6.214886665344238
Epoch 2360, val loss: 1.5437475442886353
Epoch 2370, training loss: 310.7276611328125 = 0.023930398747324944 + 50.0 * 6.214074611663818
Epoch 2370, val loss: 1.5479824542999268
Epoch 2380, training loss: 310.7489318847656 = 0.02360088750720024 + 50.0 * 6.21450662612915
Epoch 2380, val loss: 1.5523133277893066
Epoch 2390, training loss: 311.05035400390625 = 0.023282799869775772 + 50.0 * 6.220541477203369
Epoch 2390, val loss: 1.5567561388015747
Epoch 2400, training loss: 310.9856872558594 = 0.02294062450528145 + 50.0 * 6.219254970550537
Epoch 2400, val loss: 1.5603373050689697
Epoch 2410, training loss: 310.7937316894531 = 0.02260909602046013 + 50.0 * 6.2154221534729
Epoch 2410, val loss: 1.5643125772476196
Epoch 2420, training loss: 310.73651123046875 = 0.022291649132966995 + 50.0 * 6.214284896850586
Epoch 2420, val loss: 1.568240761756897
Epoch 2430, training loss: 310.6999206542969 = 0.021992679685354233 + 50.0 * 6.213558197021484
Epoch 2430, val loss: 1.5723509788513184
Epoch 2440, training loss: 310.85845947265625 = 0.021702725440263748 + 50.0 * 6.216735363006592
Epoch 2440, val loss: 1.5764561891555786
Epoch 2450, training loss: 310.69366455078125 = 0.021402668207883835 + 50.0 * 6.213445663452148
Epoch 2450, val loss: 1.5806249380111694
Epoch 2460, training loss: 310.91558837890625 = 0.021120352670550346 + 50.0 * 6.217889308929443
Epoch 2460, val loss: 1.5848901271820068
Epoch 2470, training loss: 310.68841552734375 = 0.02081979811191559 + 50.0 * 6.213352203369141
Epoch 2470, val loss: 1.5877399444580078
Epoch 2480, training loss: 310.78167724609375 = 0.020544398576021194 + 50.0 * 6.2152228355407715
Epoch 2480, val loss: 1.5914602279663086
Epoch 2490, training loss: 310.713134765625 = 0.02027512528002262 + 50.0 * 6.213857650756836
Epoch 2490, val loss: 1.5959360599517822
Epoch 2500, training loss: 310.6347351074219 = 0.020005861297249794 + 50.0 * 6.212294578552246
Epoch 2500, val loss: 1.5998703241348267
Epoch 2510, training loss: 310.6357116699219 = 0.019747978076338768 + 50.0 * 6.212319374084473
Epoch 2510, val loss: 1.6035277843475342
Epoch 2520, training loss: 310.876953125 = 0.019498450681567192 + 50.0 * 6.217148780822754
Epoch 2520, val loss: 1.6070407629013062
Epoch 2530, training loss: 310.771484375 = 0.019240034744143486 + 50.0 * 6.21504545211792
Epoch 2530, val loss: 1.6103793382644653
Epoch 2540, training loss: 310.5992431640625 = 0.01898225024342537 + 50.0 * 6.211605072021484
Epoch 2540, val loss: 1.6145509481430054
Epoch 2550, training loss: 310.6080322265625 = 0.018742788583040237 + 50.0 * 6.211785793304443
Epoch 2550, val loss: 1.618412971496582
Epoch 2560, training loss: 310.8977966308594 = 0.01851492002606392 + 50.0 * 6.217585563659668
Epoch 2560, val loss: 1.6222100257873535
Epoch 2570, training loss: 310.581298828125 = 0.018268194049596786 + 50.0 * 6.2112603187561035
Epoch 2570, val loss: 1.6252238750457764
Epoch 2580, training loss: 310.5515441894531 = 0.018037090077996254 + 50.0 * 6.210670471191406
Epoch 2580, val loss: 1.6292245388031006
Epoch 2590, training loss: 310.55548095703125 = 0.017818337306380272 + 50.0 * 6.210752964019775
Epoch 2590, val loss: 1.6327861547470093
Epoch 2600, training loss: 310.6475830078125 = 0.017605489119887352 + 50.0 * 6.212599277496338
Epoch 2600, val loss: 1.6357625722885132
Epoch 2610, training loss: 310.7554931640625 = 0.017386088147759438 + 50.0 * 6.214762210845947
Epoch 2610, val loss: 1.6391372680664062
Epoch 2620, training loss: 310.5728454589844 = 0.017167672514915466 + 50.0 * 6.211113452911377
Epoch 2620, val loss: 1.6434872150421143
Epoch 2630, training loss: 310.5136413574219 = 0.01695607416331768 + 50.0 * 6.209933280944824
Epoch 2630, val loss: 1.6468594074249268
Epoch 2640, training loss: 310.5350646972656 = 0.016758399084210396 + 50.0 * 6.210366249084473
Epoch 2640, val loss: 1.6504682302474976
Epoch 2650, training loss: 310.8116149902344 = 0.016565172001719475 + 50.0 * 6.215900897979736
Epoch 2650, val loss: 1.6532402038574219
Epoch 2660, training loss: 310.6485290527344 = 0.016363397240638733 + 50.0 * 6.212643623352051
Epoch 2660, val loss: 1.6570594310760498
Epoch 2670, training loss: 310.56298828125 = 0.01616239733994007 + 50.0 * 6.210936069488525
Epoch 2670, val loss: 1.6600967645645142
Epoch 2680, training loss: 310.5461120605469 = 0.015975914895534515 + 50.0 * 6.2106032371521
Epoch 2680, val loss: 1.664156198501587
Epoch 2690, training loss: 310.6114196777344 = 0.015791330486536026 + 50.0 * 6.211912631988525
Epoch 2690, val loss: 1.6667546033859253
Epoch 2700, training loss: 310.5182189941406 = 0.015606259927153587 + 50.0 * 6.210052490234375
Epoch 2700, val loss: 1.6702609062194824
Epoch 2710, training loss: 310.44122314453125 = 0.015426762402057648 + 50.0 * 6.208515644073486
Epoch 2710, val loss: 1.6739438772201538
Epoch 2720, training loss: 310.5059509277344 = 0.015256463550031185 + 50.0 * 6.209813594818115
Epoch 2720, val loss: 1.677278757095337
Epoch 2730, training loss: 310.8592224121094 = 0.015089955180883408 + 50.0 * 6.216883182525635
Epoch 2730, val loss: 1.6800682544708252
Epoch 2740, training loss: 310.6148376464844 = 0.014907054603099823 + 50.0 * 6.21199893951416
Epoch 2740, val loss: 1.6827890872955322
Epoch 2750, training loss: 310.4560241699219 = 0.014732862822711468 + 50.0 * 6.208825588226318
Epoch 2750, val loss: 1.6867461204528809
Epoch 2760, training loss: 310.45098876953125 = 0.014571704901754856 + 50.0 * 6.208728790283203
Epoch 2760, val loss: 1.689855694770813
Epoch 2770, training loss: 310.6608581542969 = 0.014418198727071285 + 50.0 * 6.2129292488098145
Epoch 2770, val loss: 1.693145990371704
Epoch 2780, training loss: 310.4124755859375 = 0.01425099279731512 + 50.0 * 6.2079644203186035
Epoch 2780, val loss: 1.6956610679626465
Epoch 2790, training loss: 310.498291015625 = 0.014097374863922596 + 50.0 * 6.209683895111084
Epoch 2790, val loss: 1.6987695693969727
Epoch 2800, training loss: 310.57611083984375 = 0.013947187922894955 + 50.0 * 6.211243152618408
Epoch 2800, val loss: 1.7017455101013184
Epoch 2810, training loss: 310.3660888671875 = 0.013791468925774097 + 50.0 * 6.207046031951904
Epoch 2810, val loss: 1.7049875259399414
Epoch 2820, training loss: 310.4248046875 = 0.013648333959281445 + 50.0 * 6.208223342895508
Epoch 2820, val loss: 1.7082139253616333
Epoch 2830, training loss: 310.5802307128906 = 0.013510780408978462 + 50.0 * 6.211334228515625
Epoch 2830, val loss: 1.7115435600280762
Epoch 2840, training loss: 310.7061767578125 = 0.013363870792090893 + 50.0 * 6.213856220245361
Epoch 2840, val loss: 1.7140800952911377
Epoch 2850, training loss: 310.4381103515625 = 0.013207721523940563 + 50.0 * 6.208498001098633
Epoch 2850, val loss: 1.7166557312011719
Epoch 2860, training loss: 310.3758850097656 = 0.013066896237432957 + 50.0 * 6.207256317138672
Epoch 2860, val loss: 1.7193279266357422
Epoch 2870, training loss: 310.35150146484375 = 0.012935546226799488 + 50.0 * 6.206771373748779
Epoch 2870, val loss: 1.722508430480957
Epoch 2880, training loss: 310.395751953125 = 0.012807253748178482 + 50.0 * 6.207658767700195
Epoch 2880, val loss: 1.7252211570739746
Epoch 2890, training loss: 310.59539794921875 = 0.012680906802415848 + 50.0 * 6.211654186248779
Epoch 2890, val loss: 1.7276883125305176
Epoch 2900, training loss: 310.4722595214844 = 0.012543627060949802 + 50.0 * 6.209194183349609
Epoch 2900, val loss: 1.7303783893585205
Epoch 2910, training loss: 310.464599609375 = 0.012415426783263683 + 50.0 * 6.209043502807617
Epoch 2910, val loss: 1.7332760095596313
Epoch 2920, training loss: 310.4181213378906 = 0.012288039550185204 + 50.0 * 6.20811653137207
Epoch 2920, val loss: 1.7363862991333008
Epoch 2930, training loss: 310.3744812011719 = 0.012166076339781284 + 50.0 * 6.207245826721191
Epoch 2930, val loss: 1.7389832735061646
Epoch 2940, training loss: 310.4147644042969 = 0.012044721283018589 + 50.0 * 6.208054065704346
Epoch 2940, val loss: 1.7412097454071045
Epoch 2950, training loss: 310.3486328125 = 0.01192396879196167 + 50.0 * 6.2067341804504395
Epoch 2950, val loss: 1.74398672580719
Epoch 2960, training loss: 310.3904113769531 = 0.01180942077189684 + 50.0 * 6.207571983337402
Epoch 2960, val loss: 1.7468154430389404
Epoch 2970, training loss: 310.3369445800781 = 0.011694766581058502 + 50.0 * 6.206504821777344
Epoch 2970, val loss: 1.7494264841079712
Epoch 2980, training loss: 310.3511047363281 = 0.011583758518099785 + 50.0 * 6.206790447235107
Epoch 2980, val loss: 1.7522352933883667
Epoch 2990, training loss: 310.4791564941406 = 0.011477013118565083 + 50.0 * 6.209353446960449
Epoch 2990, val loss: 1.754849910736084
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6481481481481481
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 431.785888671875 = 1.9447041749954224 + 50.0 * 8.596823692321777
Epoch 0, val loss: 1.9350095987319946
Epoch 10, training loss: 431.73297119140625 = 1.9359408617019653 + 50.0 * 8.595940589904785
Epoch 10, val loss: 1.9259727001190186
Epoch 20, training loss: 431.4194641113281 = 1.925165057182312 + 50.0 * 8.589885711669922
Epoch 20, val loss: 1.9148231744766235
Epoch 30, training loss: 429.3284912109375 = 1.911244511604309 + 50.0 * 8.548344612121582
Epoch 30, val loss: 1.9006445407867432
Epoch 40, training loss: 415.9587707519531 = 1.8946833610534668 + 50.0 * 8.281281471252441
Epoch 40, val loss: 1.8844298124313354
Epoch 50, training loss: 379.2894287109375 = 1.8763829469680786 + 50.0 * 7.5482611656188965
Epoch 50, val loss: 1.8674798011779785
Epoch 60, training loss: 359.5557861328125 = 1.864665150642395 + 50.0 * 7.153822898864746
Epoch 60, val loss: 1.8563315868377686
Epoch 70, training loss: 348.3486328125 = 1.854198932647705 + 50.0 * 6.929888725280762
Epoch 70, val loss: 1.8461588621139526
Epoch 80, training loss: 342.1914367675781 = 1.8439958095550537 + 50.0 * 6.806949138641357
Epoch 80, val loss: 1.8364500999450684
Epoch 90, training loss: 338.9279479980469 = 1.8336089849472046 + 50.0 * 6.741886615753174
Epoch 90, val loss: 1.8269091844558716
Epoch 100, training loss: 335.8092041015625 = 1.8237009048461914 + 50.0 * 6.6797099113464355
Epoch 100, val loss: 1.8180714845657349
Epoch 110, training loss: 333.36419677734375 = 1.8150993585586548 + 50.0 * 6.630981922149658
Epoch 110, val loss: 1.8106967210769653
Epoch 120, training loss: 331.56732177734375 = 1.8074873685836792 + 50.0 * 6.595196723937988
Epoch 120, val loss: 1.8041768074035645
Epoch 130, training loss: 330.1539001464844 = 1.7998675107955933 + 50.0 * 6.567080974578857
Epoch 130, val loss: 1.7976776361465454
Epoch 140, training loss: 328.8643493652344 = 1.792136311531067 + 50.0 * 6.541443824768066
Epoch 140, val loss: 1.7910754680633545
Epoch 150, training loss: 327.7821350097656 = 1.7842509746551514 + 50.0 * 6.519958019256592
Epoch 150, val loss: 1.7843693494796753
Epoch 160, training loss: 326.9299621582031 = 1.776068091392517 + 50.0 * 6.503077983856201
Epoch 160, val loss: 1.7774373292922974
Epoch 170, training loss: 326.0244445800781 = 1.7673792839050293 + 50.0 * 6.485141754150391
Epoch 170, val loss: 1.7701295614242554
Epoch 180, training loss: 325.3393859863281 = 1.758091926574707 + 50.0 * 6.471626281738281
Epoch 180, val loss: 1.7624495029449463
Epoch 190, training loss: 324.5891418457031 = 1.7480723857879639 + 50.0 * 6.456821441650391
Epoch 190, val loss: 1.7541831731796265
Epoch 200, training loss: 323.98828125 = 1.7372232675552368 + 50.0 * 6.445021152496338
Epoch 200, val loss: 1.7452809810638428
Epoch 210, training loss: 323.7122802734375 = 1.7254644632339478 + 50.0 * 6.439736366271973
Epoch 210, val loss: 1.73568856716156
Epoch 220, training loss: 323.0401611328125 = 1.7124491930007935 + 50.0 * 6.426554203033447
Epoch 220, val loss: 1.7251499891281128
Epoch 230, training loss: 322.5340881347656 = 1.6983641386032104 + 50.0 * 6.416714191436768
Epoch 230, val loss: 1.713826060295105
Epoch 240, training loss: 322.2448425292969 = 1.683075189590454 + 50.0 * 6.411235332489014
Epoch 240, val loss: 1.7015711069107056
Epoch 250, training loss: 321.7027587890625 = 1.6665570735931396 + 50.0 * 6.400723934173584
Epoch 250, val loss: 1.6884297132492065
Epoch 260, training loss: 321.3318786621094 = 1.6487407684326172 + 50.0 * 6.393662929534912
Epoch 260, val loss: 1.6744056940078735
Epoch 270, training loss: 321.147705078125 = 1.6296662092208862 + 50.0 * 6.3903608322143555
Epoch 270, val loss: 1.6592848300933838
Epoch 280, training loss: 320.67962646484375 = 1.609205961227417 + 50.0 * 6.38140869140625
Epoch 280, val loss: 1.6433392763137817
Epoch 290, training loss: 320.3179931640625 = 1.5876044034957886 + 50.0 * 6.374607563018799
Epoch 290, val loss: 1.6266602277755737
Epoch 300, training loss: 320.00701904296875 = 1.5649060010910034 + 50.0 * 6.368842124938965
Epoch 300, val loss: 1.6094043254852295
Epoch 310, training loss: 319.733154296875 = 1.54103422164917 + 50.0 * 6.363842010498047
Epoch 310, val loss: 1.591390609741211
Epoch 320, training loss: 319.58837890625 = 1.516192078590393 + 50.0 * 6.361443519592285
Epoch 320, val loss: 1.5730059146881104
Epoch 330, training loss: 319.2144775390625 = 1.4904993772506714 + 50.0 * 6.3544793128967285
Epoch 330, val loss: 1.5541881322860718
Epoch 340, training loss: 318.9561462402344 = 1.464199423789978 + 50.0 * 6.349838733673096
Epoch 340, val loss: 1.5352096557617188
Epoch 350, training loss: 318.7970886230469 = 1.4373443126678467 + 50.0 * 6.347194671630859
Epoch 350, val loss: 1.5161460638046265
Epoch 360, training loss: 318.53509521484375 = 1.410122036933899 + 50.0 * 6.342499256134033
Epoch 360, val loss: 1.497460126876831
Epoch 370, training loss: 318.3607177734375 = 1.382648229598999 + 50.0 * 6.3395609855651855
Epoch 370, val loss: 1.4786052703857422
Epoch 380, training loss: 318.0998840332031 = 1.35508131980896 + 50.0 * 6.334896087646484
Epoch 380, val loss: 1.4603508710861206
Epoch 390, training loss: 317.9126892089844 = 1.3277184963226318 + 50.0 * 6.331699371337891
Epoch 390, val loss: 1.4425407648086548
Epoch 400, training loss: 318.0747985839844 = 1.3006197214126587 + 50.0 * 6.335483551025391
Epoch 400, val loss: 1.4253534078598022
Epoch 410, training loss: 317.59271240234375 = 1.2735378742218018 + 50.0 * 6.326383590698242
Epoch 410, val loss: 1.4082859754562378
Epoch 420, training loss: 317.4328308105469 = 1.2469111680984497 + 50.0 * 6.323718070983887
Epoch 420, val loss: 1.39202082157135
Epoch 430, training loss: 317.3232727050781 = 1.2207385301589966 + 50.0 * 6.32205057144165
Epoch 430, val loss: 1.3762378692626953
Epoch 440, training loss: 317.25616455078125 = 1.194960117340088 + 50.0 * 6.321224212646484
Epoch 440, val loss: 1.3612171411514282
Epoch 450, training loss: 316.9866943359375 = 1.1696288585662842 + 50.0 * 6.316341400146484
Epoch 450, val loss: 1.3466888666152954
Epoch 460, training loss: 316.8491516113281 = 1.144835352897644 + 50.0 * 6.314086437225342
Epoch 460, val loss: 1.3327196836471558
Epoch 470, training loss: 316.930419921875 = 1.1205881834030151 + 50.0 * 6.316196918487549
Epoch 470, val loss: 1.3194129467010498
Epoch 480, training loss: 316.64276123046875 = 1.096630573272705 + 50.0 * 6.310923099517822
Epoch 480, val loss: 1.3061803579330444
Epoch 490, training loss: 316.4678955078125 = 1.073319435119629 + 50.0 * 6.307891368865967
Epoch 490, val loss: 1.2941508293151855
Epoch 500, training loss: 316.5001525878906 = 1.0504696369171143 + 50.0 * 6.308993339538574
Epoch 500, val loss: 1.2823693752288818
Epoch 510, training loss: 316.373046875 = 1.0281434059143066 + 50.0 * 6.30689811706543
Epoch 510, val loss: 1.2707808017730713
Epoch 520, training loss: 316.2266540527344 = 1.0060888528823853 + 50.0 * 6.304410934448242
Epoch 520, val loss: 1.259643793106079
Epoch 530, training loss: 315.9688415527344 = 0.9847745895385742 + 50.0 * 6.299681186676025
Epoch 530, val loss: 1.2497557401657104
Epoch 540, training loss: 315.8526306152344 = 0.9640578627586365 + 50.0 * 6.297771453857422
Epoch 540, val loss: 1.2399215698242188
Epoch 550, training loss: 315.73004150390625 = 0.9438963532447815 + 50.0 * 6.295722484588623
Epoch 550, val loss: 1.2307684421539307
Epoch 560, training loss: 316.152099609375 = 0.9243208169937134 + 50.0 * 6.304555892944336
Epoch 560, val loss: 1.222256064414978
Epoch 570, training loss: 315.5488586425781 = 0.9049277305603027 + 50.0 * 6.2928786277771
Epoch 570, val loss: 1.2134532928466797
Epoch 580, training loss: 315.443603515625 = 0.8862472176551819 + 50.0 * 6.291146755218506
Epoch 580, val loss: 1.205214500427246
Epoch 590, training loss: 315.359375 = 0.8682122230529785 + 50.0 * 6.289823055267334
Epoch 590, val loss: 1.197671890258789
Epoch 600, training loss: 315.3909606933594 = 0.8506373167037964 + 50.0 * 6.290806293487549
Epoch 600, val loss: 1.1907044649124146
Epoch 610, training loss: 315.1654357910156 = 0.833526611328125 + 50.0 * 6.286638259887695
Epoch 610, val loss: 1.1842976808547974
Epoch 620, training loss: 315.0621643066406 = 0.816974937915802 + 50.0 * 6.284903526306152
Epoch 620, val loss: 1.178391933441162
Epoch 630, training loss: 315.09051513671875 = 0.8009210228919983 + 50.0 * 6.285791397094727
Epoch 630, val loss: 1.1729158163070679
Epoch 640, training loss: 315.1024169921875 = 0.7852861881256104 + 50.0 * 6.286342620849609
Epoch 640, val loss: 1.167549967765808
Epoch 650, training loss: 314.81695556640625 = 0.7699785232543945 + 50.0 * 6.28093957901001
Epoch 650, val loss: 1.1630232334136963
Epoch 660, training loss: 314.7366943359375 = 0.75520259141922 + 50.0 * 6.279630184173584
Epoch 660, val loss: 1.158875823020935
Epoch 670, training loss: 314.6724548339844 = 0.7409043908119202 + 50.0 * 6.278631210327148
Epoch 670, val loss: 1.1553034782409668
Epoch 680, training loss: 314.7484130859375 = 0.7268359661102295 + 50.0 * 6.280431270599365
Epoch 680, val loss: 1.1520209312438965
Epoch 690, training loss: 314.63519287109375 = 0.7129963040351868 + 50.0 * 6.278444290161133
Epoch 690, val loss: 1.1487305164337158
Epoch 700, training loss: 314.4687805175781 = 0.6996544003486633 + 50.0 * 6.2753825187683105
Epoch 700, val loss: 1.146416425704956
Epoch 710, training loss: 314.3497009277344 = 0.6867166757583618 + 50.0 * 6.273260116577148
Epoch 710, val loss: 1.1443219184875488
Epoch 720, training loss: 314.50262451171875 = 0.6740963459014893 + 50.0 * 6.2765703201293945
Epoch 720, val loss: 1.14240562915802
Epoch 730, training loss: 314.4166564941406 = 0.6616097092628479 + 50.0 * 6.2751007080078125
Epoch 730, val loss: 1.1411415338516235
Epoch 740, training loss: 314.211669921875 = 0.6494144201278687 + 50.0 * 6.271245002746582
Epoch 740, val loss: 1.1398624181747437
Epoch 750, training loss: 314.0857849121094 = 0.6375890374183655 + 50.0 * 6.26896333694458
Epoch 750, val loss: 1.1393444538116455
Epoch 760, training loss: 314.11846923828125 = 0.6260458827018738 + 50.0 * 6.269848346710205
Epoch 760, val loss: 1.1393284797668457
Epoch 770, training loss: 314.12158203125 = 0.6146544814109802 + 50.0 * 6.270138263702393
Epoch 770, val loss: 1.1387643814086914
Epoch 780, training loss: 313.9696960449219 = 0.6034091711044312 + 50.0 * 6.267325401306152
Epoch 780, val loss: 1.1390386819839478
Epoch 790, training loss: 313.8772277832031 = 0.5924916863441467 + 50.0 * 6.265694618225098
Epoch 790, val loss: 1.1394153833389282
Epoch 800, training loss: 313.7872314453125 = 0.581800103187561 + 50.0 * 6.264108657836914
Epoch 800, val loss: 1.1404149532318115
Epoch 810, training loss: 314.1336975097656 = 0.5712921023368835 + 50.0 * 6.2712483406066895
Epoch 810, val loss: 1.1413518190383911
Epoch 820, training loss: 313.7904357910156 = 0.5607873797416687 + 50.0 * 6.26459264755249
Epoch 820, val loss: 1.1425108909606934
Epoch 830, training loss: 313.68817138671875 = 0.5504971146583557 + 50.0 * 6.262753486633301
Epoch 830, val loss: 1.1440078020095825
Epoch 840, training loss: 313.5664367675781 = 0.5404825210571289 + 50.0 * 6.260518550872803
Epoch 840, val loss: 1.145981788635254
Epoch 850, training loss: 313.9450378417969 = 0.5306620597839355 + 50.0 * 6.268287181854248
Epoch 850, val loss: 1.148455023765564
Epoch 860, training loss: 313.6792907714844 = 0.5207420587539673 + 50.0 * 6.2631707191467285
Epoch 860, val loss: 1.1499078273773193
Epoch 870, training loss: 313.413330078125 = 0.5110796689987183 + 50.0 * 6.258045196533203
Epoch 870, val loss: 1.152587652206421
Epoch 880, training loss: 313.3707275390625 = 0.5016118884086609 + 50.0 * 6.257382392883301
Epoch 880, val loss: 1.1554772853851318
Epoch 890, training loss: 313.45281982421875 = 0.4922778606414795 + 50.0 * 6.25921106338501
Epoch 890, val loss: 1.1584488153457642
Epoch 900, training loss: 313.3508605957031 = 0.48298338055610657 + 50.0 * 6.257358074188232
Epoch 900, val loss: 1.1614378690719604
Epoch 910, training loss: 313.23516845703125 = 0.4737897515296936 + 50.0 * 6.255227565765381
Epoch 910, val loss: 1.1647231578826904
Epoch 920, training loss: 313.250244140625 = 0.4647732079029083 + 50.0 * 6.255709171295166
Epoch 920, val loss: 1.1684643030166626
Epoch 930, training loss: 313.2161560058594 = 0.45584636926651 + 50.0 * 6.25520658493042
Epoch 930, val loss: 1.172014832496643
Epoch 940, training loss: 313.0899353027344 = 0.4470010995864868 + 50.0 * 6.252859115600586
Epoch 940, val loss: 1.1760269403457642
Epoch 950, training loss: 313.0812683105469 = 0.4383689761161804 + 50.0 * 6.2528581619262695
Epoch 950, val loss: 1.1803520917892456
Epoch 960, training loss: 313.45294189453125 = 0.4298456311225891 + 50.0 * 6.260462284088135
Epoch 960, val loss: 1.1850576400756836
Epoch 970, training loss: 313.033447265625 = 0.42125648260116577 + 50.0 * 6.252243518829346
Epoch 970, val loss: 1.1887047290802002
Epoch 980, training loss: 312.92974853515625 = 0.4129312038421631 + 50.0 * 6.250336170196533
Epoch 980, val loss: 1.1937963962554932
Epoch 990, training loss: 313.0054016113281 = 0.4047275483608246 + 50.0 * 6.252013683319092
Epoch 990, val loss: 1.19865882396698
Epoch 1000, training loss: 312.85284423828125 = 0.3966398239135742 + 50.0 * 6.249124050140381
Epoch 1000, val loss: 1.2036561965942383
Epoch 1010, training loss: 312.8456115722656 = 0.38868483901023865 + 50.0 * 6.249138832092285
Epoch 1010, val loss: 1.2090178728103638
Epoch 1020, training loss: 312.8703918457031 = 0.38079702854156494 + 50.0 * 6.249791622161865
Epoch 1020, val loss: 1.2144025564193726
Epoch 1030, training loss: 312.7815246582031 = 0.37298884987831116 + 50.0 * 6.248170852661133
Epoch 1030, val loss: 1.2199736833572388
Epoch 1040, training loss: 312.8427734375 = 0.36528483033180237 + 50.0 * 6.249549865722656
Epoch 1040, val loss: 1.2253164052963257
Epoch 1050, training loss: 312.7302551269531 = 0.3576459288597107 + 50.0 * 6.247452259063721
Epoch 1050, val loss: 1.230129599571228
Epoch 1060, training loss: 312.71563720703125 = 0.3501495122909546 + 50.0 * 6.247309684753418
Epoch 1060, val loss: 1.2360081672668457
Epoch 1070, training loss: 312.6249694824219 = 0.3428029716014862 + 50.0 * 6.245643615722656
Epoch 1070, val loss: 1.2423112392425537
Epoch 1080, training loss: 312.5467529296875 = 0.33558446168899536 + 50.0 * 6.244223594665527
Epoch 1080, val loss: 1.247851014137268
Epoch 1090, training loss: 312.62030029296875 = 0.32847920060157776 + 50.0 * 6.24583625793457
Epoch 1090, val loss: 1.254263162612915
Epoch 1100, training loss: 312.5171203613281 = 0.3214046359062195 + 50.0 * 6.2439141273498535
Epoch 1100, val loss: 1.2601674795150757
Epoch 1110, training loss: 312.44964599609375 = 0.3144592344760895 + 50.0 * 6.242703914642334
Epoch 1110, val loss: 1.2657318115234375
Epoch 1120, training loss: 312.5103454589844 = 0.30765989422798157 + 50.0 * 6.244053840637207
Epoch 1120, val loss: 1.272147536277771
Epoch 1130, training loss: 312.39581298828125 = 0.3009136915206909 + 50.0 * 6.241898059844971
Epoch 1130, val loss: 1.2784873247146606
Epoch 1140, training loss: 312.348876953125 = 0.29431459307670593 + 50.0 * 6.241091251373291
Epoch 1140, val loss: 1.2853407859802246
Epoch 1150, training loss: 312.3368225097656 = 0.2878149747848511 + 50.0 * 6.24098014831543
Epoch 1150, val loss: 1.2915323972702026
Epoch 1160, training loss: 312.3694763183594 = 0.28145843744277954 + 50.0 * 6.24176025390625
Epoch 1160, val loss: 1.298141360282898
Epoch 1170, training loss: 312.31298828125 = 0.27513769268989563 + 50.0 * 6.240756988525391
Epoch 1170, val loss: 1.3053981065750122
Epoch 1180, training loss: 312.2637634277344 = 0.26897045969963074 + 50.0 * 6.239895343780518
Epoch 1180, val loss: 1.311661720275879
Epoch 1190, training loss: 312.1702880859375 = 0.2628975808620453 + 50.0 * 6.238147735595703
Epoch 1190, val loss: 1.3182830810546875
Epoch 1200, training loss: 312.2109680175781 = 0.25699862837791443 + 50.0 * 6.239079475402832
Epoch 1200, val loss: 1.3252243995666504
Epoch 1210, training loss: 312.2379150390625 = 0.2511622905731201 + 50.0 * 6.239735126495361
Epoch 1210, val loss: 1.3322899341583252
Epoch 1220, training loss: 312.18670654296875 = 0.24538324773311615 + 50.0 * 6.238826274871826
Epoch 1220, val loss: 1.3390319347381592
Epoch 1230, training loss: 312.09490966796875 = 0.2397230565547943 + 50.0 * 6.2371039390563965
Epoch 1230, val loss: 1.3453668355941772
Epoch 1240, training loss: 312.1153259277344 = 0.23423248529434204 + 50.0 * 6.237621784210205
Epoch 1240, val loss: 1.3523656129837036
Epoch 1250, training loss: 312.00823974609375 = 0.22881710529327393 + 50.0 * 6.235588073730469
Epoch 1250, val loss: 1.3595842123031616
Epoch 1260, training loss: 311.9627990722656 = 0.22352930903434753 + 50.0 * 6.234785079956055
Epoch 1260, val loss: 1.366556167602539
Epoch 1270, training loss: 312.189697265625 = 0.21836471557617188 + 50.0 * 6.239426612854004
Epoch 1270, val loss: 1.372804880142212
Epoch 1280, training loss: 312.02056884765625 = 0.21322497725486755 + 50.0 * 6.236146926879883
Epoch 1280, val loss: 1.380861759185791
Epoch 1290, training loss: 312.0142822265625 = 0.20819532871246338 + 50.0 * 6.236121654510498
Epoch 1290, val loss: 1.3871179819107056
Epoch 1300, training loss: 312.1864929199219 = 0.20334893465042114 + 50.0 * 6.239663124084473
Epoch 1300, val loss: 1.3941526412963867
Epoch 1310, training loss: 311.8809509277344 = 0.19851776957511902 + 50.0 * 6.233648777008057
Epoch 1310, val loss: 1.4021159410476685
Epoch 1320, training loss: 311.814697265625 = 0.1938731074333191 + 50.0 * 6.23241662979126
Epoch 1320, val loss: 1.4087918996810913
Epoch 1330, training loss: 311.76190185546875 = 0.18937040865421295 + 50.0 * 6.231451034545898
Epoch 1330, val loss: 1.4164162874221802
Epoch 1340, training loss: 311.84417724609375 = 0.18494874238967896 + 50.0 * 6.233184814453125
Epoch 1340, val loss: 1.423890233039856
Epoch 1350, training loss: 311.8377990722656 = 0.1805723011493683 + 50.0 * 6.233144283294678
Epoch 1350, val loss: 1.4307481050491333
Epoch 1360, training loss: 311.7423400878906 = 0.1762758493423462 + 50.0 * 6.231321334838867
Epoch 1360, val loss: 1.4381219148635864
Epoch 1370, training loss: 311.8309631347656 = 0.17210786044597626 + 50.0 * 6.2331767082214355
Epoch 1370, val loss: 1.4458907842636108
Epoch 1380, training loss: 311.6717224121094 = 0.16802619397640228 + 50.0 * 6.230073928833008
Epoch 1380, val loss: 1.4531084299087524
Epoch 1390, training loss: 311.6399230957031 = 0.1640838235616684 + 50.0 * 6.229516506195068
Epoch 1390, val loss: 1.4606090784072876
Epoch 1400, training loss: 311.8283386230469 = 0.16024917364120483 + 50.0 * 6.233361721038818
Epoch 1400, val loss: 1.4689267873764038
Epoch 1410, training loss: 311.6954040527344 = 0.1563989371061325 + 50.0 * 6.230780124664307
Epoch 1410, val loss: 1.4756395816802979
Epoch 1420, training loss: 311.6501770019531 = 0.15269015729427338 + 50.0 * 6.229949951171875
Epoch 1420, val loss: 1.4835165739059448
Epoch 1430, training loss: 311.6043395996094 = 0.14909178018569946 + 50.0 * 6.229105472564697
Epoch 1430, val loss: 1.490832805633545
Epoch 1440, training loss: 311.6490173339844 = 0.14561739563941956 + 50.0 * 6.230067729949951
Epoch 1440, val loss: 1.4987505674362183
Epoch 1450, training loss: 311.6372375488281 = 0.14218075573444366 + 50.0 * 6.22990083694458
Epoch 1450, val loss: 1.5058903694152832
Epoch 1460, training loss: 311.6313781738281 = 0.13881388306617737 + 50.0 * 6.229851245880127
Epoch 1460, val loss: 1.5146098136901855
Epoch 1470, training loss: 311.50689697265625 = 0.13554508984088898 + 50.0 * 6.227426528930664
Epoch 1470, val loss: 1.5220786333084106
Epoch 1480, training loss: 311.45770263671875 = 0.13237795233726501 + 50.0 * 6.226506233215332
Epoch 1480, val loss: 1.5300086736679077
Epoch 1490, training loss: 311.6250305175781 = 0.12931540608406067 + 50.0 * 6.229914665222168
Epoch 1490, val loss: 1.5383450984954834
Epoch 1500, training loss: 311.4613342285156 = 0.12627442181110382 + 50.0 * 6.226700782775879
Epoch 1500, val loss: 1.5453623533248901
Epoch 1510, training loss: 311.5198974609375 = 0.12332721799612045 + 50.0 * 6.227931499481201
Epoch 1510, val loss: 1.5539475679397583
Epoch 1520, training loss: 311.41064453125 = 0.12044233083724976 + 50.0 * 6.225803852081299
Epoch 1520, val loss: 1.5615460872650146
Epoch 1530, training loss: 311.35528564453125 = 0.11765319108963013 + 50.0 * 6.224752902984619
Epoch 1530, val loss: 1.5706247091293335
Epoch 1540, training loss: 311.5856018066406 = 0.11496776342391968 + 50.0 * 6.22941255569458
Epoch 1540, val loss: 1.5785692930221558
Epoch 1550, training loss: 311.4160461425781 = 0.11227616667747498 + 50.0 * 6.226075649261475
Epoch 1550, val loss: 1.5863405466079712
Epoch 1560, training loss: 311.3818054199219 = 0.10967878252267838 + 50.0 * 6.225442409515381
Epoch 1560, val loss: 1.5947431325912476
Epoch 1570, training loss: 311.3663635253906 = 0.10716317594051361 + 50.0 * 6.225183486938477
Epoch 1570, val loss: 1.6029648780822754
Epoch 1580, training loss: 311.4043273925781 = 0.10471826046705246 + 50.0 * 6.225992679595947
Epoch 1580, val loss: 1.6110833883285522
Epoch 1590, training loss: 311.4449768066406 = 0.10232848674058914 + 50.0 * 6.226852893829346
Epoch 1590, val loss: 1.619988203048706
Epoch 1600, training loss: 311.2434997558594 = 0.09997455030679703 + 50.0 * 6.222870349884033
Epoch 1600, val loss: 1.6282931566238403
Epoch 1610, training loss: 311.2165222167969 = 0.09771516919136047 + 50.0 * 6.222376346588135
Epoch 1610, val loss: 1.6366602182388306
Epoch 1620, training loss: 311.35784912109375 = 0.0955372229218483 + 50.0 * 6.225245952606201
Epoch 1620, val loss: 1.6442115306854248
Epoch 1630, training loss: 311.24151611328125 = 0.09336455911397934 + 50.0 * 6.222963333129883
Epoch 1630, val loss: 1.652802586555481
Epoch 1640, training loss: 311.2087097167969 = 0.09127085655927658 + 50.0 * 6.222348690032959
Epoch 1640, val loss: 1.661644458770752
Epoch 1650, training loss: 311.15374755859375 = 0.08924716711044312 + 50.0 * 6.221290111541748
Epoch 1650, val loss: 1.6695536375045776
Epoch 1660, training loss: 311.3153381347656 = 0.08730120211839676 + 50.0 * 6.224560260772705
Epoch 1660, val loss: 1.677891731262207
Epoch 1670, training loss: 311.3314514160156 = 0.08534523844718933 + 50.0 * 6.224922180175781
Epoch 1670, val loss: 1.6865172386169434
Epoch 1680, training loss: 311.2235107421875 = 0.08343980461359024 + 50.0 * 6.222801685333252
Epoch 1680, val loss: 1.6943022012710571
Epoch 1690, training loss: 311.1367492675781 = 0.08159693330526352 + 50.0 * 6.221102714538574
Epoch 1690, val loss: 1.7029333114624023
Epoch 1700, training loss: 311.1119689941406 = 0.07984434068202972 + 50.0 * 6.220642566680908
Epoch 1700, val loss: 1.7112632989883423
Epoch 1710, training loss: 311.30194091796875 = 0.07812763005495071 + 50.0 * 6.224476337432861
Epoch 1710, val loss: 1.7197693586349487
Epoch 1720, training loss: 311.0933837890625 = 0.07642626762390137 + 50.0 * 6.220339298248291
Epoch 1720, val loss: 1.7264524698257446
Epoch 1730, training loss: 311.17236328125 = 0.07478959858417511 + 50.0 * 6.221951484680176
Epoch 1730, val loss: 1.735162615776062
Epoch 1740, training loss: 311.0783996582031 = 0.07317443192005157 + 50.0 * 6.220104217529297
Epoch 1740, val loss: 1.7430002689361572
Epoch 1750, training loss: 311.04998779296875 = 0.07160474359989166 + 50.0 * 6.219567775726318
Epoch 1750, val loss: 1.750921607017517
Epoch 1760, training loss: 311.0845031738281 = 0.07010210305452347 + 50.0 * 6.220288276672363
Epoch 1760, val loss: 1.7586026191711426
Epoch 1770, training loss: 311.0351257324219 = 0.06862937659025192 + 50.0 * 6.219329833984375
Epoch 1770, val loss: 1.7663588523864746
Epoch 1780, training loss: 311.04998779296875 = 0.06719460338354111 + 50.0 * 6.219655990600586
Epoch 1780, val loss: 1.7748736143112183
Epoch 1790, training loss: 311.05718994140625 = 0.06579585373401642 + 50.0 * 6.219828128814697
Epoch 1790, val loss: 1.7821800708770752
Epoch 1800, training loss: 311.3325500488281 = 0.06441214680671692 + 50.0 * 6.225362777709961
Epoch 1800, val loss: 1.7902030944824219
Epoch 1810, training loss: 311.00689697265625 = 0.06306347250938416 + 50.0 * 6.218876838684082
Epoch 1810, val loss: 1.7982500791549683
Epoch 1820, training loss: 310.91436767578125 = 0.06176333874464035 + 50.0 * 6.217051982879639
Epoch 1820, val loss: 1.80587637424469
Epoch 1830, training loss: 310.90423583984375 = 0.06052704527974129 + 50.0 * 6.216874122619629
Epoch 1830, val loss: 1.8137699365615845
Epoch 1840, training loss: 311.21746826171875 = 0.05932304263114929 + 50.0 * 6.223162651062012
Epoch 1840, val loss: 1.8215315341949463
Epoch 1850, training loss: 310.998779296875 = 0.05809548497200012 + 50.0 * 6.218813419342041
Epoch 1850, val loss: 1.8280959129333496
Epoch 1860, training loss: 310.9110412597656 = 0.05692470818758011 + 50.0 * 6.217082500457764
Epoch 1860, val loss: 1.8362897634506226
Epoch 1870, training loss: 310.85595703125 = 0.0557999350130558 + 50.0 * 6.21600341796875
Epoch 1870, val loss: 1.8435207605361938
Epoch 1880, training loss: 310.8930969238281 = 0.05470940098166466 + 50.0 * 6.216767311096191
Epoch 1880, val loss: 1.8507930040359497
Epoch 1890, training loss: 311.0841064453125 = 0.05364286154508591 + 50.0 * 6.220609188079834
Epoch 1890, val loss: 1.857995867729187
Epoch 1900, training loss: 311.0918273925781 = 0.05256936326622963 + 50.0 * 6.220785140991211
Epoch 1900, val loss: 1.8657654523849487
Epoch 1910, training loss: 310.9574890136719 = 0.05153138190507889 + 50.0 * 6.218118667602539
Epoch 1910, val loss: 1.8728270530700684
Epoch 1920, training loss: 310.8669738769531 = 0.050529297441244125 + 50.0 * 6.2163286209106445
Epoch 1920, val loss: 1.8810354471206665
Epoch 1930, training loss: 310.8521728515625 = 0.04957013577222824 + 50.0 * 6.216052532196045
Epoch 1930, val loss: 1.887995958328247
Epoch 1940, training loss: 310.8060302734375 = 0.048630792647600174 + 50.0 * 6.215147972106934
Epoch 1940, val loss: 1.8948062658309937
Epoch 1950, training loss: 310.889892578125 = 0.04772069305181503 + 50.0 * 6.216843128204346
Epoch 1950, val loss: 1.901408314704895
Epoch 1960, training loss: 310.8040771484375 = 0.04681426286697388 + 50.0 * 6.215145111083984
Epoch 1960, val loss: 1.908900260925293
Epoch 1970, training loss: 310.7759094238281 = 0.0459388867020607 + 50.0 * 6.214599609375
Epoch 1970, val loss: 1.9163036346435547
Epoch 1980, training loss: 310.8470153808594 = 0.04509344696998596 + 50.0 * 6.216038227081299
Epoch 1980, val loss: 1.9231817722320557
Epoch 1990, training loss: 310.86334228515625 = 0.04424331337213516 + 50.0 * 6.216381549835205
Epoch 1990, val loss: 1.929403305053711
Epoch 2000, training loss: 310.7095642089844 = 0.04341816529631615 + 50.0 * 6.213322639465332
Epoch 2000, val loss: 1.9371991157531738
Epoch 2010, training loss: 310.68975830078125 = 0.042625319212675095 + 50.0 * 6.212942600250244
Epoch 2010, val loss: 1.943904161453247
Epoch 2020, training loss: 310.7444763183594 = 0.04186857119202614 + 50.0 * 6.214052200317383
Epoch 2020, val loss: 1.9508568048477173
Epoch 2030, training loss: 310.8393249511719 = 0.04111828655004501 + 50.0 * 6.215964317321777
Epoch 2030, val loss: 1.9574698209762573
Epoch 2040, training loss: 310.8453369140625 = 0.04038379713892937 + 50.0 * 6.216099262237549
Epoch 2040, val loss: 1.9642548561096191
Epoch 2050, training loss: 310.768310546875 = 0.039653897285461426 + 50.0 * 6.214573383331299
Epoch 2050, val loss: 1.9706650972366333
Epoch 2060, training loss: 310.6710205078125 = 0.038951944559812546 + 50.0 * 6.212641716003418
Epoch 2060, val loss: 1.9777342081069946
Epoch 2070, training loss: 310.8556823730469 = 0.03828272968530655 + 50.0 * 6.216348171234131
Epoch 2070, val loss: 1.9847220182418823
Epoch 2080, training loss: 310.7403259277344 = 0.03760610520839691 + 50.0 * 6.214054584503174
Epoch 2080, val loss: 1.9902704954147339
Epoch 2090, training loss: 310.6267395019531 = 0.03694972023367882 + 50.0 * 6.211795806884766
Epoch 2090, val loss: 1.9968162775039673
Epoch 2100, training loss: 310.7035827636719 = 0.03633023425936699 + 50.0 * 6.213345050811768
Epoch 2100, val loss: 2.0037193298339844
Epoch 2110, training loss: 310.6556701660156 = 0.035703469067811966 + 50.0 * 6.212399482727051
Epoch 2110, val loss: 2.009855270385742
Epoch 2120, training loss: 310.6280517578125 = 0.03509637713432312 + 50.0 * 6.211859226226807
Epoch 2120, val loss: 2.0154614448547363
Epoch 2130, training loss: 310.7700500488281 = 0.03451158106327057 + 50.0 * 6.214710712432861
Epoch 2130, val loss: 2.0218734741210938
Epoch 2140, training loss: 310.6372375488281 = 0.033923614770174026 + 50.0 * 6.212066173553467
Epoch 2140, val loss: 2.0278515815734863
Epoch 2150, training loss: 310.5890197753906 = 0.03335735946893692 + 50.0 * 6.211113452911377
Epoch 2150, val loss: 2.034013509750366
Epoch 2160, training loss: 310.56512451171875 = 0.03280959650874138 + 50.0 * 6.210646629333496
Epoch 2160, val loss: 2.040189743041992
Epoch 2170, training loss: 310.6750793457031 = 0.03228374570608139 + 50.0 * 6.212855815887451
Epoch 2170, val loss: 2.0458688735961914
Epoch 2180, training loss: 310.5630187988281 = 0.03175030276179314 + 50.0 * 6.210625648498535
Epoch 2180, val loss: 2.0523784160614014
Epoch 2190, training loss: 310.5234680175781 = 0.031244218349456787 + 50.0 * 6.209844589233398
Epoch 2190, val loss: 2.0586018562316895
Epoch 2200, training loss: 310.6038818359375 = 0.030749496072530746 + 50.0 * 6.211462497711182
Epoch 2200, val loss: 2.0649044513702393
Epoch 2210, training loss: 310.6388244628906 = 0.030258072540163994 + 50.0 * 6.21217155456543
Epoch 2210, val loss: 2.0706746578216553
Epoch 2220, training loss: 310.492431640625 = 0.029756873846054077 + 50.0 * 6.209253787994385
Epoch 2220, val loss: 2.0758562088012695
Epoch 2230, training loss: 310.52691650390625 = 0.029291005805134773 + 50.0 * 6.209952354431152
Epoch 2230, val loss: 2.0816001892089844
Epoch 2240, training loss: 310.6411437988281 = 0.028838878497481346 + 50.0 * 6.212245941162109
Epoch 2240, val loss: 2.0879738330841064
Epoch 2250, training loss: 310.46697998046875 = 0.028387969359755516 + 50.0 * 6.208771705627441
Epoch 2250, val loss: 2.092907428741455
Epoch 2260, training loss: 310.7210998535156 = 0.02795722521841526 + 50.0 * 6.213862895965576
Epoch 2260, val loss: 2.097949981689453
Epoch 2270, training loss: 310.5076904296875 = 0.02751757577061653 + 50.0 * 6.209603309631348
Epoch 2270, val loss: 2.1045773029327393
Epoch 2280, training loss: 310.4424743652344 = 0.027097774669528008 + 50.0 * 6.20830774307251
Epoch 2280, val loss: 2.1101887226104736
Epoch 2290, training loss: 310.4150390625 = 0.02669481560587883 + 50.0 * 6.207767009735107
Epoch 2290, val loss: 2.1156675815582275
Epoch 2300, training loss: 310.5062255859375 = 0.02631121501326561 + 50.0 * 6.209598541259766
Epoch 2300, val loss: 2.121464967727661
Epoch 2310, training loss: 310.5632629394531 = 0.025916164740920067 + 50.0 * 6.210746765136719
Epoch 2310, val loss: 2.1264455318450928
Epoch 2320, training loss: 310.55810546875 = 0.025520525872707367 + 50.0 * 6.210651874542236
Epoch 2320, val loss: 2.1315436363220215
Epoch 2330, training loss: 310.3997497558594 = 0.025139551609754562 + 50.0 * 6.207491874694824
Epoch 2330, val loss: 2.137418270111084
Epoch 2340, training loss: 310.4277038574219 = 0.024777710437774658 + 50.0 * 6.2080583572387695
Epoch 2340, val loss: 2.143329620361328
Epoch 2350, training loss: 310.5086975097656 = 0.02442646026611328 + 50.0 * 6.209685802459717
Epoch 2350, val loss: 2.1487691402435303
Epoch 2360, training loss: 310.4986877441406 = 0.02406935766339302 + 50.0 * 6.209492206573486
Epoch 2360, val loss: 2.1531832218170166
Epoch 2370, training loss: 310.435791015625 = 0.023720314726233482 + 50.0 * 6.2082414627075195
Epoch 2370, val loss: 2.1583943367004395
Epoch 2380, training loss: 310.39801025390625 = 0.02338225021958351 + 50.0 * 6.207492828369141
Epoch 2380, val loss: 2.1637487411499023
Epoch 2390, training loss: 310.4555358886719 = 0.023055849596858025 + 50.0 * 6.208649635314941
Epoch 2390, val loss: 2.1689910888671875
Epoch 2400, training loss: 310.4898376464844 = 0.02273404598236084 + 50.0 * 6.209342002868652
Epoch 2400, val loss: 2.174581527709961
Epoch 2410, training loss: 310.3283386230469 = 0.02240830846130848 + 50.0 * 6.206118583679199
Epoch 2410, val loss: 2.178884506225586
Epoch 2420, training loss: 310.33160400390625 = 0.022102685645222664 + 50.0 * 6.20619010925293
Epoch 2420, val loss: 2.1837334632873535
Epoch 2430, training loss: 310.3336181640625 = 0.021804312244057655 + 50.0 * 6.206236362457275
Epoch 2430, val loss: 2.188887357711792
Epoch 2440, training loss: 310.46453857421875 = 0.021511828526854515 + 50.0 * 6.208860397338867
Epoch 2440, val loss: 2.193732261657715
Epoch 2450, training loss: 310.6211242675781 = 0.02121664397418499 + 50.0 * 6.211998462677002
Epoch 2450, val loss: 2.198718786239624
Epoch 2460, training loss: 310.37994384765625 = 0.020916055887937546 + 50.0 * 6.207180976867676
Epoch 2460, val loss: 2.2036380767822266
Epoch 2470, training loss: 310.29656982421875 = 0.02063310518860817 + 50.0 * 6.20551872253418
Epoch 2470, val loss: 2.208343267440796
Epoch 2480, training loss: 310.3466796875 = 0.020366892218589783 + 50.0 * 6.206526279449463
Epoch 2480, val loss: 2.213914394378662
Epoch 2490, training loss: 310.44451904296875 = 0.020099371671676636 + 50.0 * 6.2084879875183105
Epoch 2490, val loss: 2.218417167663574
Epoch 2500, training loss: 310.34429931640625 = 0.019825713708996773 + 50.0 * 6.206489562988281
Epoch 2500, val loss: 2.222517251968384
Epoch 2510, training loss: 310.4676818847656 = 0.019571980461478233 + 50.0 * 6.208962440490723
Epoch 2510, val loss: 2.2275760173797607
Epoch 2520, training loss: 310.24053955078125 = 0.019310355186462402 + 50.0 * 6.2044243812561035
Epoch 2520, val loss: 2.2310612201690674
Epoch 2530, training loss: 310.3177490234375 = 0.01906556263566017 + 50.0 * 6.2059736251831055
Epoch 2530, val loss: 2.2361674308776855
Epoch 2540, training loss: 310.31787109375 = 0.018822910264134407 + 50.0 * 6.2059807777404785
Epoch 2540, val loss: 2.2404894828796387
Epoch 2550, training loss: 310.18878173828125 = 0.018582208082079887 + 50.0 * 6.203403949737549
Epoch 2550, val loss: 2.2458996772766113
Epoch 2560, training loss: 310.23419189453125 = 0.01835615187883377 + 50.0 * 6.204317092895508
Epoch 2560, val loss: 2.2506632804870605
Epoch 2570, training loss: 310.493896484375 = 0.018137751147150993 + 50.0 * 6.20951509475708
Epoch 2570, val loss: 2.2548937797546387
Epoch 2580, training loss: 310.4241027832031 = 0.017897721379995346 + 50.0 * 6.20812463760376
Epoch 2580, val loss: 2.258589506149292
Epoch 2590, training loss: 310.231201171875 = 0.01766480877995491 + 50.0 * 6.204270839691162
Epoch 2590, val loss: 2.263453245162964
Epoch 2600, training loss: 310.1966552734375 = 0.01744680292904377 + 50.0 * 6.203583717346191
Epoch 2600, val loss: 2.2679691314697266
Epoch 2610, training loss: 310.2572326660156 = 0.01723836176097393 + 50.0 * 6.204799652099609
Epoch 2610, val loss: 2.272156000137329
Epoch 2620, training loss: 310.3586120605469 = 0.017033107578754425 + 50.0 * 6.206831455230713
Epoch 2620, val loss: 2.2756195068359375
Epoch 2630, training loss: 310.1876220703125 = 0.016818244010210037 + 50.0 * 6.203416347503662
Epoch 2630, val loss: 2.2812728881835938
Epoch 2640, training loss: 310.1670227050781 = 0.016616467386484146 + 50.0 * 6.20300817489624
Epoch 2640, val loss: 2.2855374813079834
Epoch 2650, training loss: 310.2118225097656 = 0.016425637528300285 + 50.0 * 6.2039079666137695
Epoch 2650, val loss: 2.289482355117798
Epoch 2660, training loss: 310.3196716308594 = 0.01623128540813923 + 50.0 * 6.206068992614746
Epoch 2660, val loss: 2.293978691101074
Epoch 2670, training loss: 310.4581298828125 = 0.01603979617357254 + 50.0 * 6.208841323852539
Epoch 2670, val loss: 2.2981791496276855
Epoch 2680, training loss: 310.19427490234375 = 0.0158417746424675 + 50.0 * 6.203568935394287
Epoch 2680, val loss: 2.301703929901123
Epoch 2690, training loss: 310.1142883300781 = 0.015659119933843613 + 50.0 * 6.201972484588623
Epoch 2690, val loss: 2.306222915649414
Epoch 2700, training loss: 310.0890808105469 = 0.015482629649341106 + 50.0 * 6.201472282409668
Epoch 2700, val loss: 2.3104424476623535
Epoch 2710, training loss: 310.1468200683594 = 0.015312263742089272 + 50.0 * 6.202630043029785
Epoch 2710, val loss: 2.314220428466797
Epoch 2720, training loss: 310.3399658203125 = 0.015140047296881676 + 50.0 * 6.206496715545654
Epoch 2720, val loss: 2.317871332168579
Epoch 2730, training loss: 310.3059387207031 = 0.014968457631766796 + 50.0 * 6.205819606781006
Epoch 2730, val loss: 2.3224668502807617
Epoch 2740, training loss: 310.3384094238281 = 0.014790774323046207 + 50.0 * 6.206472396850586
Epoch 2740, val loss: 2.3262276649475098
Epoch 2750, training loss: 310.13232421875 = 0.014622388407588005 + 50.0 * 6.2023539543151855
Epoch 2750, val loss: 2.3306961059570312
Epoch 2760, training loss: 310.09796142578125 = 0.014460866339504719 + 50.0 * 6.201670169830322
Epoch 2760, val loss: 2.3347535133361816
Epoch 2770, training loss: 310.22674560546875 = 0.014311057515442371 + 50.0 * 6.204248428344727
Epoch 2770, val loss: 2.3391761779785156
Epoch 2780, training loss: 310.15850830078125 = 0.014146865345537663 + 50.0 * 6.202887058258057
Epoch 2780, val loss: 2.3423407077789307
Epoch 2790, training loss: 310.1438293457031 = 0.013990326784551144 + 50.0 * 6.202597141265869
Epoch 2790, val loss: 2.3452537059783936
Epoch 2800, training loss: 310.1782531738281 = 0.013841086998581886 + 50.0 * 6.2032880783081055
Epoch 2800, val loss: 2.3502094745635986
Epoch 2810, training loss: 310.1659851074219 = 0.013692211359739304 + 50.0 * 6.20304536819458
Epoch 2810, val loss: 2.353370189666748
Epoch 2820, training loss: 310.03839111328125 = 0.013544819317758083 + 50.0 * 6.200496673583984
Epoch 2820, val loss: 2.3574399948120117
Epoch 2830, training loss: 310.03448486328125 = 0.01340547762811184 + 50.0 * 6.2004218101501465
Epoch 2830, val loss: 2.361062526702881
Epoch 2840, training loss: 310.18157958984375 = 0.013270115479826927 + 50.0 * 6.203365802764893
Epoch 2840, val loss: 2.3640499114990234
Epoch 2850, training loss: 310.228271484375 = 0.013125707395374775 + 50.0 * 6.20430326461792
Epoch 2850, val loss: 2.3677351474761963
Epoch 2860, training loss: 310.0184631347656 = 0.012979123741388321 + 50.0 * 6.200109958648682
Epoch 2860, val loss: 2.3717827796936035
Epoch 2870, training loss: 309.9801940917969 = 0.012844819575548172 + 50.0 * 6.199347019195557
Epoch 2870, val loss: 2.3756425380706787
Epoch 2880, training loss: 309.98065185546875 = 0.012719951570034027 + 50.0 * 6.1993584632873535
Epoch 2880, val loss: 2.379359722137451
Epoch 2890, training loss: 310.1085510253906 = 0.012598341330885887 + 50.0 * 6.201919078826904
Epoch 2890, val loss: 2.3831586837768555
Epoch 2900, training loss: 310.1059265136719 = 0.012466983869671822 + 50.0 * 6.201869010925293
Epoch 2900, val loss: 2.38622784614563
Epoch 2910, training loss: 310.0080261230469 = 0.012334427796304226 + 50.0 * 6.19991397857666
Epoch 2910, val loss: 2.389465808868408
Epoch 2920, training loss: 310.07293701171875 = 0.01221487671136856 + 50.0 * 6.201214790344238
Epoch 2920, val loss: 2.3932995796203613
Epoch 2930, training loss: 310.12396240234375 = 0.012093453668057919 + 50.0 * 6.202237606048584
Epoch 2930, val loss: 2.3971943855285645
Epoch 2940, training loss: 310.0545349121094 = 0.01197168231010437 + 50.0 * 6.2008514404296875
Epoch 2940, val loss: 2.4002318382263184
Epoch 2950, training loss: 309.9642028808594 = 0.011852601543068886 + 50.0 * 6.199047088623047
Epoch 2950, val loss: 2.4035356044769287
Epoch 2960, training loss: 310.0137023925781 = 0.011742069385945797 + 50.0 * 6.200038909912109
Epoch 2960, val loss: 2.4072940349578857
Epoch 2970, training loss: 310.2601318359375 = 0.011632814072072506 + 50.0 * 6.204970359802246
Epoch 2970, val loss: 2.411088466644287
Epoch 2980, training loss: 310.15679931640625 = 0.011512364260852337 + 50.0 * 6.202905654907227
Epoch 2980, val loss: 2.4122400283813477
Epoch 2990, training loss: 309.9945983886719 = 0.01139515545219183 + 50.0 * 6.199664115905762
Epoch 2990, val loss: 2.416471004486084
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6592592592592593
0.8070637849235636
The final CL Acc:0.64938, 0.00761, The final GNN Acc:0.80601, 0.00310
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13228])
remove edge: torch.Size([2, 7862])
updated graph: torch.Size([2, 10534])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7805480957031 = 1.9389910697937012 + 50.0 * 8.596831321716309
Epoch 0, val loss: 1.9402117729187012
Epoch 10, training loss: 431.7299499511719 = 1.9300329685211182 + 50.0 * 8.595998764038086
Epoch 10, val loss: 1.9306292533874512
Epoch 20, training loss: 431.41510009765625 = 1.9187699556350708 + 50.0 * 8.589926719665527
Epoch 20, val loss: 1.9186480045318604
Epoch 30, training loss: 429.3309631347656 = 1.9039936065673828 + 50.0 * 8.548539161682129
Epoch 30, val loss: 1.9029407501220703
Epoch 40, training loss: 417.9260559082031 = 1.8855217695236206 + 50.0 * 8.320810317993164
Epoch 40, val loss: 1.8840488195419312
Epoch 50, training loss: 394.3968200683594 = 1.8619205951690674 + 50.0 * 7.850698471069336
Epoch 50, val loss: 1.8605865240097046
Epoch 60, training loss: 378.8481140136719 = 1.8419550657272339 + 50.0 * 7.540123462677002
Epoch 60, val loss: 1.8428068161010742
Epoch 70, training loss: 362.57537841796875 = 1.8317651748657227 + 50.0 * 7.214872360229492
Epoch 70, val loss: 1.834180235862732
Epoch 80, training loss: 352.4236145019531 = 1.8233989477157593 + 50.0 * 7.012004375457764
Epoch 80, val loss: 1.8259209394454956
Epoch 90, training loss: 345.1086120605469 = 1.8137168884277344 + 50.0 * 6.8658976554870605
Epoch 90, val loss: 1.817167043685913
Epoch 100, training loss: 340.5228271484375 = 1.8036558628082275 + 50.0 * 6.774383544921875
Epoch 100, val loss: 1.8083466291427612
Epoch 110, training loss: 337.1385803222656 = 1.7933731079101562 + 50.0 * 6.706904411315918
Epoch 110, val loss: 1.7992725372314453
Epoch 120, training loss: 334.8677978515625 = 1.7832703590393066 + 50.0 * 6.661690711975098
Epoch 120, val loss: 1.790274739265442
Epoch 130, training loss: 332.8056335449219 = 1.772919774055481 + 50.0 * 6.620654582977295
Epoch 130, val loss: 1.781250238418579
Epoch 140, training loss: 331.11700439453125 = 1.7621567249298096 + 50.0 * 6.58709716796875
Epoch 140, val loss: 1.771966576576233
Epoch 150, training loss: 329.64337158203125 = 1.7507479190826416 + 50.0 * 6.557852268218994
Epoch 150, val loss: 1.7622119188308716
Epoch 160, training loss: 328.509033203125 = 1.7381993532180786 + 50.0 * 6.535416603088379
Epoch 160, val loss: 1.7514655590057373
Epoch 170, training loss: 327.57293701171875 = 1.724355697631836 + 50.0 * 6.516971588134766
Epoch 170, val loss: 1.739733338356018
Epoch 180, training loss: 326.67822265625 = 1.7091730833053589 + 50.0 * 6.499381065368652
Epoch 180, val loss: 1.7269911766052246
Epoch 190, training loss: 325.8962097167969 = 1.6926816701889038 + 50.0 * 6.484070301055908
Epoch 190, val loss: 1.713178277015686
Epoch 200, training loss: 325.1258850097656 = 1.674790859222412 + 50.0 * 6.469021797180176
Epoch 200, val loss: 1.6983680725097656
Epoch 210, training loss: 324.46234130859375 = 1.6555689573287964 + 50.0 * 6.456135272979736
Epoch 210, val loss: 1.6824029684066772
Epoch 220, training loss: 323.9497985839844 = 1.6349139213562012 + 50.0 * 6.446297645568848
Epoch 220, val loss: 1.6652981042861938
Epoch 230, training loss: 323.3289794921875 = 1.6128143072128296 + 50.0 * 6.434322834014893
Epoch 230, val loss: 1.647159457206726
Epoch 240, training loss: 322.8956298828125 = 1.5894685983657837 + 50.0 * 6.426123142242432
Epoch 240, val loss: 1.6279999017715454
Epoch 250, training loss: 322.4071350097656 = 1.5649782419204712 + 50.0 * 6.416843414306641
Epoch 250, val loss: 1.6078147888183594
Epoch 260, training loss: 322.0296630859375 = 1.5397436618804932 + 50.0 * 6.409798622131348
Epoch 260, val loss: 1.5872087478637695
Epoch 270, training loss: 321.6487121582031 = 1.513811469078064 + 50.0 * 6.402697563171387
Epoch 270, val loss: 1.5661019086837769
Epoch 280, training loss: 321.2572326660156 = 1.4874565601348877 + 50.0 * 6.395395278930664
Epoch 280, val loss: 1.5446199178695679
Epoch 290, training loss: 320.90087890625 = 1.4610894918441772 + 50.0 * 6.388795852661133
Epoch 290, val loss: 1.5234196186065674
Epoch 300, training loss: 320.5355224609375 = 1.4347847700119019 + 50.0 * 6.382014751434326
Epoch 300, val loss: 1.5023287534713745
Epoch 310, training loss: 320.2548828125 = 1.4085936546325684 + 50.0 * 6.376925468444824
Epoch 310, val loss: 1.4816604852676392
Epoch 320, training loss: 319.8934020996094 = 1.3827613592147827 + 50.0 * 6.370213031768799
Epoch 320, val loss: 1.4613646268844604
Epoch 330, training loss: 319.6242370605469 = 1.3573312759399414 + 50.0 * 6.365338325500488
Epoch 330, val loss: 1.4416286945343018
Epoch 340, training loss: 319.3944396972656 = 1.3321667909622192 + 50.0 * 6.361245632171631
Epoch 340, val loss: 1.4221467971801758
Epoch 350, training loss: 319.1065673828125 = 1.3074746131896973 + 50.0 * 6.355982303619385
Epoch 350, val loss: 1.4031908512115479
Epoch 360, training loss: 318.84527587890625 = 1.283233880996704 + 50.0 * 6.351240634918213
Epoch 360, val loss: 1.3848744630813599
Epoch 370, training loss: 318.5819396972656 = 1.2594648599624634 + 50.0 * 6.346449375152588
Epoch 370, val loss: 1.367004632949829
Epoch 380, training loss: 318.73553466796875 = 1.2361067533493042 + 50.0 * 6.3499884605407715
Epoch 380, val loss: 1.349900245666504
Epoch 390, training loss: 318.5500793457031 = 1.212694525718689 + 50.0 * 6.346747398376465
Epoch 390, val loss: 1.3322917222976685
Epoch 400, training loss: 318.0574951171875 = 1.1899672746658325 + 50.0 * 6.337350368499756
Epoch 400, val loss: 1.3150023221969604
Epoch 410, training loss: 317.7938232421875 = 1.167603611946106 + 50.0 * 6.332524299621582
Epoch 410, val loss: 1.2987229824066162
Epoch 420, training loss: 317.5945739746094 = 1.1456258296966553 + 50.0 * 6.328979015350342
Epoch 420, val loss: 1.282589077949524
Epoch 430, training loss: 317.418212890625 = 1.1239093542099 + 50.0 * 6.325886249542236
Epoch 430, val loss: 1.2667360305786133
Epoch 440, training loss: 317.6291198730469 = 1.102493166923523 + 50.0 * 6.330533027648926
Epoch 440, val loss: 1.2510343790054321
Epoch 450, training loss: 317.1156311035156 = 1.0810880661010742 + 50.0 * 6.320691108703613
Epoch 450, val loss: 1.2355797290802002
Epoch 460, training loss: 317.0295715332031 = 1.0602048635482788 + 50.0 * 6.319386959075928
Epoch 460, val loss: 1.2203713655471802
Epoch 470, training loss: 316.81085205078125 = 1.0395885705947876 + 50.0 * 6.315425395965576
Epoch 470, val loss: 1.2055811882019043
Epoch 480, training loss: 316.7657470703125 = 1.0192748308181763 + 50.0 * 6.314929008483887
Epoch 480, val loss: 1.1909908056259155
Epoch 490, training loss: 316.78375244140625 = 0.9991245269775391 + 50.0 * 6.315692901611328
Epoch 490, val loss: 1.1769133806228638
Epoch 500, training loss: 316.41253662109375 = 0.979303777217865 + 50.0 * 6.308664798736572
Epoch 500, val loss: 1.1625924110412598
Epoch 510, training loss: 316.2325134277344 = 0.9598423838615417 + 50.0 * 6.305453300476074
Epoch 510, val loss: 1.1488012075424194
Epoch 520, training loss: 316.1216735839844 = 0.9407520890235901 + 50.0 * 6.30361795425415
Epoch 520, val loss: 1.135450839996338
Epoch 530, training loss: 316.3189392089844 = 0.9220039248466492 + 50.0 * 6.307938575744629
Epoch 530, val loss: 1.1222894191741943
Epoch 540, training loss: 315.9233703613281 = 0.9035060405731201 + 50.0 * 6.3003973960876465
Epoch 540, val loss: 1.1093961000442505
Epoch 550, training loss: 315.78143310546875 = 0.8855156898498535 + 50.0 * 6.297918319702148
Epoch 550, val loss: 1.097025752067566
Epoch 560, training loss: 315.6768493652344 = 0.8679755926132202 + 50.0 * 6.296177387237549
Epoch 560, val loss: 1.0849758386611938
Epoch 570, training loss: 315.6387939453125 = 0.8509025573730469 + 50.0 * 6.29575777053833
Epoch 570, val loss: 1.0733758211135864
Epoch 580, training loss: 315.4656066894531 = 0.8342464566230774 + 50.0 * 6.292626857757568
Epoch 580, val loss: 1.0623462200164795
Epoch 590, training loss: 315.66754150390625 = 0.8179581761360168 + 50.0 * 6.29699182510376
Epoch 590, val loss: 1.0515130758285522
Epoch 600, training loss: 315.3420715332031 = 0.8020802736282349 + 50.0 * 6.290799617767334
Epoch 600, val loss: 1.0409353971481323
Epoch 610, training loss: 315.1611022949219 = 0.7866286635398865 + 50.0 * 6.287489414215088
Epoch 610, val loss: 1.0309988260269165
Epoch 620, training loss: 315.0886535644531 = 0.7716435194015503 + 50.0 * 6.286340236663818
Epoch 620, val loss: 1.0214341878890991
Epoch 630, training loss: 315.2422180175781 = 0.756994366645813 + 50.0 * 6.289704322814941
Epoch 630, val loss: 1.012612223625183
Epoch 640, training loss: 315.0377502441406 = 0.7424220442771912 + 50.0 * 6.2859063148498535
Epoch 640, val loss: 1.0031996965408325
Epoch 650, training loss: 314.8813781738281 = 0.7282417416572571 + 50.0 * 6.28306245803833
Epoch 650, val loss: 0.994929313659668
Epoch 660, training loss: 314.7580871582031 = 0.7142694592475891 + 50.0 * 6.280876159667969
Epoch 660, val loss: 0.98648601770401
Epoch 670, training loss: 315.0030212402344 = 0.7005024552345276 + 50.0 * 6.286050319671631
Epoch 670, val loss: 0.9785647988319397
Epoch 680, training loss: 314.7268981933594 = 0.6867535710334778 + 50.0 * 6.2808027267456055
Epoch 680, val loss: 0.9707518219947815
Epoch 690, training loss: 314.59417724609375 = 0.6731109619140625 + 50.0 * 6.278420925140381
Epoch 690, val loss: 0.9629533290863037
Epoch 700, training loss: 314.6619873046875 = 0.6597061157226562 + 50.0 * 6.280045509338379
Epoch 700, val loss: 0.9557125568389893
Epoch 710, training loss: 314.470703125 = 0.6462199687957764 + 50.0 * 6.276489734649658
Epoch 710, val loss: 0.9485184550285339
Epoch 720, training loss: 314.3554382324219 = 0.632871150970459 + 50.0 * 6.27445125579834
Epoch 720, val loss: 0.9415689706802368
Epoch 730, training loss: 314.29254150390625 = 0.6195911169052124 + 50.0 * 6.273458957672119
Epoch 730, val loss: 0.9349314570426941
Epoch 740, training loss: 314.4883117675781 = 0.6063261032104492 + 50.0 * 6.277639865875244
Epoch 740, val loss: 0.9283333420753479
Epoch 750, training loss: 314.1950378417969 = 0.593017041683197 + 50.0 * 6.272040367126465
Epoch 750, val loss: 0.9220200777053833
Epoch 760, training loss: 314.1318664550781 = 0.5798454880714417 + 50.0 * 6.271040916442871
Epoch 760, val loss: 0.9157795310020447
Epoch 770, training loss: 314.3063049316406 = 0.5666965246200562 + 50.0 * 6.274791717529297
Epoch 770, val loss: 0.909807562828064
Epoch 780, training loss: 314.0603942871094 = 0.5536945462226868 + 50.0 * 6.270133972167969
Epoch 780, val loss: 0.9043676853179932
Epoch 790, training loss: 313.913818359375 = 0.5406869053840637 + 50.0 * 6.267462730407715
Epoch 790, val loss: 0.8989803194999695
Epoch 800, training loss: 313.87945556640625 = 0.5278511643409729 + 50.0 * 6.267032146453857
Epoch 800, val loss: 0.8938783407211304
Epoch 810, training loss: 313.9010925292969 = 0.5150523781776428 + 50.0 * 6.267720699310303
Epoch 810, val loss: 0.8890537023544312
Epoch 820, training loss: 313.8243408203125 = 0.5023741126060486 + 50.0 * 6.266438961029053
Epoch 820, val loss: 0.8845044374465942
Epoch 830, training loss: 313.7962341308594 = 0.48986905813217163 + 50.0 * 6.266127586364746
Epoch 830, val loss: 0.8805438280105591
Epoch 840, training loss: 313.6649475097656 = 0.47753065824508667 + 50.0 * 6.2637481689453125
Epoch 840, val loss: 0.8766433000564575
Epoch 850, training loss: 313.6089172363281 = 0.46542832255363464 + 50.0 * 6.262869834899902
Epoch 850, val loss: 0.8731493353843689
Epoch 860, training loss: 313.7824401855469 = 0.45353132486343384 + 50.0 * 6.266578197479248
Epoch 860, val loss: 0.8704068660736084
Epoch 870, training loss: 313.663818359375 = 0.441674143075943 + 50.0 * 6.2644429206848145
Epoch 870, val loss: 0.8671467900276184
Epoch 880, training loss: 313.42913818359375 = 0.43010571599006653 + 50.0 * 6.259980201721191
Epoch 880, val loss: 0.8647493124008179
Epoch 890, training loss: 313.3994140625 = 0.4188750684261322 + 50.0 * 6.259610652923584
Epoch 890, val loss: 0.862450897693634
Epoch 900, training loss: 313.33660888671875 = 0.40788304805755615 + 50.0 * 6.25857400894165
Epoch 900, val loss: 0.8607607483863831
Epoch 910, training loss: 313.63787841796875 = 0.3970937728881836 + 50.0 * 6.264815807342529
Epoch 910, val loss: 0.8590682744979858
Epoch 920, training loss: 313.468017578125 = 0.3866127133369446 + 50.0 * 6.261627674102783
Epoch 920, val loss: 0.8578015565872192
Epoch 930, training loss: 313.23138427734375 = 0.37633830308914185 + 50.0 * 6.257101058959961
Epoch 930, val loss: 0.8571013808250427
Epoch 940, training loss: 313.1709289550781 = 0.366465300321579 + 50.0 * 6.256089210510254
Epoch 940, val loss: 0.856757402420044
Epoch 950, training loss: 313.4442138671875 = 0.35684433579444885 + 50.0 * 6.261747360229492
Epoch 950, val loss: 0.8565989136695862
Epoch 960, training loss: 313.24560546875 = 0.347344309091568 + 50.0 * 6.257965087890625
Epoch 960, val loss: 0.8562819957733154
Epoch 970, training loss: 313.0169372558594 = 0.3382202386856079 + 50.0 * 6.253574371337891
Epoch 970, val loss: 0.8566204905509949
Epoch 980, training loss: 312.9891052246094 = 0.3293896019458771 + 50.0 * 6.253194808959961
Epoch 980, val loss: 0.8575236201286316
Epoch 990, training loss: 312.9859619140625 = 0.3208145797252655 + 50.0 * 6.253303050994873
Epoch 990, val loss: 0.8582615852355957
Epoch 1000, training loss: 313.0924987792969 = 0.31245338916778564 + 50.0 * 6.255600929260254
Epoch 1000, val loss: 0.8591547012329102
Epoch 1010, training loss: 312.85760498046875 = 0.3043476641178131 + 50.0 * 6.251065254211426
Epoch 1010, val loss: 0.8604749441146851
Epoch 1020, training loss: 312.78961181640625 = 0.2964857518672943 + 50.0 * 6.2498626708984375
Epoch 1020, val loss: 0.8619295358657837
Epoch 1030, training loss: 312.74713134765625 = 0.2888970375061035 + 50.0 * 6.249164581298828
Epoch 1030, val loss: 0.8635865449905396
Epoch 1040, training loss: 312.8269958496094 = 0.28157180547714233 + 50.0 * 6.250907897949219
Epoch 1040, val loss: 0.8655651807785034
Epoch 1050, training loss: 312.788330078125 = 0.2743214964866638 + 50.0 * 6.250279903411865
Epoch 1050, val loss: 0.86763995885849
Epoch 1060, training loss: 312.747314453125 = 0.26730459928512573 + 50.0 * 6.249600410461426
Epoch 1060, val loss: 0.8696291446685791
Epoch 1070, training loss: 312.5981750488281 = 0.2605409026145935 + 50.0 * 6.246752738952637
Epoch 1070, val loss: 0.8721125721931458
Epoch 1080, training loss: 312.5771484375 = 0.25400510430336 + 50.0 * 6.246462821960449
Epoch 1080, val loss: 0.8748093247413635
Epoch 1090, training loss: 312.88067626953125 = 0.24762944877147675 + 50.0 * 6.252661228179932
Epoch 1090, val loss: 0.8775103092193604
Epoch 1100, training loss: 312.7513732910156 = 0.24136631190776825 + 50.0 * 6.250200271606445
Epoch 1100, val loss: 0.8796902894973755
Epoch 1110, training loss: 312.5391540527344 = 0.2352864295244217 + 50.0 * 6.246077060699463
Epoch 1110, val loss: 0.8829696774482727
Epoch 1120, training loss: 312.4639587402344 = 0.22945410013198853 + 50.0 * 6.24468994140625
Epoch 1120, val loss: 0.8864539861679077
Epoch 1130, training loss: 312.4621887207031 = 0.22379250824451447 + 50.0 * 6.244768142700195
Epoch 1130, val loss: 0.8894557356834412
Epoch 1140, training loss: 312.4955749511719 = 0.21827946603298187 + 50.0 * 6.245545387268066
Epoch 1140, val loss: 0.89300137758255
Epoch 1150, training loss: 312.3897705078125 = 0.21288932859897614 + 50.0 * 6.243537425994873
Epoch 1150, val loss: 0.8962783217430115
Epoch 1160, training loss: 312.68768310546875 = 0.20766089856624603 + 50.0 * 6.249600410461426
Epoch 1160, val loss: 0.8998492360115051
Epoch 1170, training loss: 312.3838806152344 = 0.2025049477815628 + 50.0 * 6.243627071380615
Epoch 1170, val loss: 0.9032129049301147
Epoch 1180, training loss: 312.3322448730469 = 0.1975531131029129 + 50.0 * 6.242693901062012
Epoch 1180, val loss: 0.9065960645675659
Epoch 1190, training loss: 312.38275146484375 = 0.1927439272403717 + 50.0 * 6.243800163269043
Epoch 1190, val loss: 0.9106647372245789
Epoch 1200, training loss: 312.18951416015625 = 0.1880425363779068 + 50.0 * 6.240029335021973
Epoch 1200, val loss: 0.9144030809402466
Epoch 1210, training loss: 312.2742614746094 = 0.18350739777088165 + 50.0 * 6.241815090179443
Epoch 1210, val loss: 0.9183530807495117
Epoch 1220, training loss: 312.3949890136719 = 0.17907576262950897 + 50.0 * 6.24431848526001
Epoch 1220, val loss: 0.9227031469345093
Epoch 1230, training loss: 312.16558837890625 = 0.17469602823257446 + 50.0 * 6.239818096160889
Epoch 1230, val loss: 0.9265773296356201
Epoch 1240, training loss: 312.11614990234375 = 0.17049306631088257 + 50.0 * 6.238913059234619
Epoch 1240, val loss: 0.9308077096939087
Epoch 1250, training loss: 312.130859375 = 0.16642485558986664 + 50.0 * 6.239288806915283
Epoch 1250, val loss: 0.9351746439933777
Epoch 1260, training loss: 312.151123046875 = 0.16245019435882568 + 50.0 * 6.239773273468018
Epoch 1260, val loss: 0.9395065307617188
Epoch 1270, training loss: 312.0662536621094 = 0.15854696929454803 + 50.0 * 6.238154411315918
Epoch 1270, val loss: 0.9438424110412598
Epoch 1280, training loss: 312.0702819824219 = 0.15476004779338837 + 50.0 * 6.238310813903809
Epoch 1280, val loss: 0.9482729434967041
Epoch 1290, training loss: 312.09417724609375 = 0.1510692983865738 + 50.0 * 6.23886251449585
Epoch 1290, val loss: 0.9525827765464783
Epoch 1300, training loss: 312.0712585449219 = 0.14745184779167175 + 50.0 * 6.238475799560547
Epoch 1300, val loss: 0.9571987390518188
Epoch 1310, training loss: 312.02801513671875 = 0.14393532276153564 + 50.0 * 6.237681865692139
Epoch 1310, val loss: 0.9617059826850891
Epoch 1320, training loss: 311.9519958496094 = 0.14054886996746063 + 50.0 * 6.2362284660339355
Epoch 1320, val loss: 0.9667438864707947
Epoch 1330, training loss: 311.9377746582031 = 0.13723652064800262 + 50.0 * 6.236010551452637
Epoch 1330, val loss: 0.9714586138725281
Epoch 1340, training loss: 311.9190368652344 = 0.13398590683937073 + 50.0 * 6.235701084136963
Epoch 1340, val loss: 0.9762766361236572
Epoch 1350, training loss: 311.89263916015625 = 0.1308145970106125 + 50.0 * 6.235236167907715
Epoch 1350, val loss: 0.9809784889221191
Epoch 1360, training loss: 311.9592590332031 = 0.12772338092327118 + 50.0 * 6.236630916595459
Epoch 1360, val loss: 0.9857282638549805
Epoch 1370, training loss: 312.0238952636719 = 0.12469620257616043 + 50.0 * 6.2379841804504395
Epoch 1370, val loss: 0.9910288453102112
Epoch 1380, training loss: 311.81793212890625 = 0.12174902856349945 + 50.0 * 6.23392391204834
Epoch 1380, val loss: 0.995577871799469
Epoch 1390, training loss: 311.7269287109375 = 0.11889548599720001 + 50.0 * 6.232160568237305
Epoch 1390, val loss: 1.0009499788284302
Epoch 1400, training loss: 311.692138671875 = 0.11613757163286209 + 50.0 * 6.23151969909668
Epoch 1400, val loss: 1.0059547424316406
Epoch 1410, training loss: 311.9866638183594 = 0.11344778537750244 + 50.0 * 6.237464904785156
Epoch 1410, val loss: 1.0107684135437012
Epoch 1420, training loss: 311.7388610839844 = 0.11078748106956482 + 50.0 * 6.2325615882873535
Epoch 1420, val loss: 1.0164672136306763
Epoch 1430, training loss: 311.7318115234375 = 0.10818877816200256 + 50.0 * 6.2324724197387695
Epoch 1430, val loss: 1.021079421043396
Epoch 1440, training loss: 311.7433166503906 = 0.1057111844420433 + 50.0 * 6.232751846313477
Epoch 1440, val loss: 1.026689887046814
Epoch 1450, training loss: 311.6944885253906 = 0.10325653851032257 + 50.0 * 6.2318243980407715
Epoch 1450, val loss: 1.0317022800445557
Epoch 1460, training loss: 311.6881408691406 = 0.10086160898208618 + 50.0 * 6.231745719909668
Epoch 1460, val loss: 1.0368402004241943
Epoch 1470, training loss: 311.67974853515625 = 0.09855510294437408 + 50.0 * 6.231624126434326
Epoch 1470, val loss: 1.0420408248901367
Epoch 1480, training loss: 311.54937744140625 = 0.09627898782491684 + 50.0 * 6.229062080383301
Epoch 1480, val loss: 1.0472450256347656
Epoch 1490, training loss: 311.6338806152344 = 0.09408694505691528 + 50.0 * 6.230795860290527
Epoch 1490, val loss: 1.0524530410766602
Epoch 1500, training loss: 311.72955322265625 = 0.09196387976408005 + 50.0 * 6.232751846313477
Epoch 1500, val loss: 1.0576756000518799
Epoch 1510, training loss: 311.5061340332031 = 0.08983367681503296 + 50.0 * 6.228325843811035
Epoch 1510, val loss: 1.0625255107879639
Epoch 1520, training loss: 311.4848327636719 = 0.0878165140748024 + 50.0 * 6.227940082550049
Epoch 1520, val loss: 1.0675632953643799
Epoch 1530, training loss: 311.4949035644531 = 0.08585896342992783 + 50.0 * 6.2281813621521
Epoch 1530, val loss: 1.0728263854980469
Epoch 1540, training loss: 311.8529968261719 = 0.08393044024705887 + 50.0 * 6.235381603240967
Epoch 1540, val loss: 1.078288197517395
Epoch 1550, training loss: 311.4085998535156 = 0.08199610561132431 + 50.0 * 6.226531982421875
Epoch 1550, val loss: 1.082937479019165
Epoch 1560, training loss: 311.42822265625 = 0.08016428351402283 + 50.0 * 6.226961135864258
Epoch 1560, val loss: 1.088324785232544
Epoch 1570, training loss: 311.3699645996094 = 0.07840218394994736 + 50.0 * 6.225831031799316
Epoch 1570, val loss: 1.0934648513793945
Epoch 1580, training loss: 311.5101013183594 = 0.07668207585811615 + 50.0 * 6.228668212890625
Epoch 1580, val loss: 1.0985493659973145
Epoch 1590, training loss: 311.4101257324219 = 0.07496523857116699 + 50.0 * 6.226703643798828
Epoch 1590, val loss: 1.1033211946487427
Epoch 1600, training loss: 311.3377380371094 = 0.07330945879220963 + 50.0 * 6.2252888679504395
Epoch 1600, val loss: 1.1084816455841064
Epoch 1610, training loss: 311.3414001464844 = 0.07171908020973206 + 50.0 * 6.225393772125244
Epoch 1610, val loss: 1.1134450435638428
Epoch 1620, training loss: 311.458740234375 = 0.07017523795366287 + 50.0 * 6.227771282196045
Epoch 1620, val loss: 1.1187175512313843
Epoch 1630, training loss: 311.3358154296875 = 0.06862977147102356 + 50.0 * 6.225343704223633
Epoch 1630, val loss: 1.1233501434326172
Epoch 1640, training loss: 311.3736267089844 = 0.06712748855352402 + 50.0 * 6.226129531860352
Epoch 1640, val loss: 1.1282988786697388
Epoch 1650, training loss: 311.3160705566406 = 0.06569211930036545 + 50.0 * 6.2250075340271
Epoch 1650, val loss: 1.1332982778549194
Epoch 1660, training loss: 311.280517578125 = 0.06427916139364243 + 50.0 * 6.224325180053711
Epoch 1660, val loss: 1.1380259990692139
Epoch 1670, training loss: 311.53021240234375 = 0.0629054456949234 + 50.0 * 6.22934627532959
Epoch 1670, val loss: 1.1430338621139526
Epoch 1680, training loss: 311.314453125 = 0.061561618000268936 + 50.0 * 6.225058078765869
Epoch 1680, val loss: 1.147892951965332
Epoch 1690, training loss: 311.1949768066406 = 0.060244377702474594 + 50.0 * 6.2226948738098145
Epoch 1690, val loss: 1.1526691913604736
Epoch 1700, training loss: 311.1599426269531 = 0.058980587869882584 + 50.0 * 6.222019195556641
Epoch 1700, val loss: 1.1577316522598267
Epoch 1710, training loss: 311.19561767578125 = 0.05775841325521469 + 50.0 * 6.222756862640381
Epoch 1710, val loss: 1.1627141237258911
Epoch 1720, training loss: 311.47076416015625 = 0.05656493082642555 + 50.0 * 6.228283405303955
Epoch 1720, val loss: 1.167418122291565
Epoch 1730, training loss: 311.24432373046875 = 0.05534130707383156 + 50.0 * 6.223779678344727
Epoch 1730, val loss: 1.171736717224121
Epoch 1740, training loss: 311.1060791015625 = 0.05417867749929428 + 50.0 * 6.221038341522217
Epoch 1740, val loss: 1.1765460968017578
Epoch 1750, training loss: 311.0783996582031 = 0.05306857079267502 + 50.0 * 6.22050666809082
Epoch 1750, val loss: 1.181284785270691
Epoch 1760, training loss: 311.0736389160156 = 0.05198870971798897 + 50.0 * 6.220432758331299
Epoch 1760, val loss: 1.1859759092330933
Epoch 1770, training loss: 311.6005554199219 = 0.05093447491526604 + 50.0 * 6.230992317199707
Epoch 1770, val loss: 1.190569519996643
Epoch 1780, training loss: 311.30828857421875 = 0.049866024404764175 + 50.0 * 6.225168704986572
Epoch 1780, val loss: 1.1948838233947754
Epoch 1790, training loss: 311.07611083984375 = 0.048843227326869965 + 50.0 * 6.220545768737793
Epoch 1790, val loss: 1.199678659439087
Epoch 1800, training loss: 311.0142822265625 = 0.04785604402422905 + 50.0 * 6.2193284034729
Epoch 1800, val loss: 1.2041230201721191
Epoch 1810, training loss: 311.00531005859375 = 0.046906281262636185 + 50.0 * 6.219168186187744
Epoch 1810, val loss: 1.2090134620666504
Epoch 1820, training loss: 311.54296875 = 0.04600338637828827 + 50.0 * 6.2299394607543945
Epoch 1820, val loss: 1.2137970924377441
Epoch 1830, training loss: 311.2604064941406 = 0.0450458899140358 + 50.0 * 6.224307537078857
Epoch 1830, val loss: 1.2174612283706665
Epoch 1840, training loss: 311.0492248535156 = 0.044144999235868454 + 50.0 * 6.220101356506348
Epoch 1840, val loss: 1.2221717834472656
Epoch 1850, training loss: 310.973876953125 = 0.04327154532074928 + 50.0 * 6.218612194061279
Epoch 1850, val loss: 1.2263658046722412
Epoch 1860, training loss: 311.0851135253906 = 0.04243360459804535 + 50.0 * 6.220853805541992
Epoch 1860, val loss: 1.231103777885437
Epoch 1870, training loss: 310.95233154296875 = 0.04160946607589722 + 50.0 * 6.218214511871338
Epoch 1870, val loss: 1.23512864112854
Epoch 1880, training loss: 311.31256103515625 = 0.04082481190562248 + 50.0 * 6.225434303283691
Epoch 1880, val loss: 1.2398900985717773
Epoch 1890, training loss: 310.96905517578125 = 0.0399935282766819 + 50.0 * 6.218581199645996
Epoch 1890, val loss: 1.2433055639266968
Epoch 1900, training loss: 310.9167785644531 = 0.039225995540618896 + 50.0 * 6.217551231384277
Epoch 1900, val loss: 1.248060703277588
Epoch 1910, training loss: 310.8855285644531 = 0.03848211094737053 + 50.0 * 6.216940879821777
Epoch 1910, val loss: 1.2521605491638184
Epoch 1920, training loss: 310.8847351074219 = 0.037760935723781586 + 50.0 * 6.216939449310303
Epoch 1920, val loss: 1.256412148475647
Epoch 1930, training loss: 311.3290710449219 = 0.037066567689180374 + 50.0 * 6.225839614868164
Epoch 1930, val loss: 1.2609788179397583
Epoch 1940, training loss: 310.9805603027344 = 0.03634904697537422 + 50.0 * 6.218883991241455
Epoch 1940, val loss: 1.264173984527588
Epoch 1950, training loss: 310.90045166015625 = 0.035672929137945175 + 50.0 * 6.2172956466674805
Epoch 1950, val loss: 1.2688136100769043
Epoch 1960, training loss: 310.87188720703125 = 0.0350104495882988 + 50.0 * 6.216737747192383
Epoch 1960, val loss: 1.2725458145141602
Epoch 1970, training loss: 311.15850830078125 = 0.03437070548534393 + 50.0 * 6.222483158111572
Epoch 1970, val loss: 1.2767435312271118
Epoch 1980, training loss: 310.9204406738281 = 0.03373964875936508 + 50.0 * 6.217733860015869
Epoch 1980, val loss: 1.2804949283599854
Epoch 1990, training loss: 310.92193603515625 = 0.033121995627880096 + 50.0 * 6.217776298522949
Epoch 1990, val loss: 1.2847976684570312
Epoch 2000, training loss: 310.7841491699219 = 0.032522134482860565 + 50.0 * 6.215032577514648
Epoch 2000, val loss: 1.2884331941604614
Epoch 2010, training loss: 310.83038330078125 = 0.03194757550954819 + 50.0 * 6.215968608856201
Epoch 2010, val loss: 1.29233980178833
Epoch 2020, training loss: 311.166259765625 = 0.03138699382543564 + 50.0 * 6.222697734832764
Epoch 2020, val loss: 1.29642653465271
Epoch 2030, training loss: 310.871826171875 = 0.030809681862592697 + 50.0 * 6.216819763183594
Epoch 2030, val loss: 1.3002623319625854
Epoch 2040, training loss: 310.81182861328125 = 0.030267398804426193 + 50.0 * 6.215631008148193
Epoch 2040, val loss: 1.3038301467895508
Epoch 2050, training loss: 310.9743957519531 = 0.029752735048532486 + 50.0 * 6.218892574310303
Epoch 2050, val loss: 1.308100938796997
Epoch 2060, training loss: 310.7450866699219 = 0.02922002412378788 + 50.0 * 6.214317321777344
Epoch 2060, val loss: 1.311352252960205
Epoch 2070, training loss: 310.7732238769531 = 0.028709713369607925 + 50.0 * 6.214890003204346
Epoch 2070, val loss: 1.3152172565460205
Epoch 2080, training loss: 310.7589416503906 = 0.028224440291523933 + 50.0 * 6.214614391326904
Epoch 2080, val loss: 1.3191512823104858
Epoch 2090, training loss: 310.8843994140625 = 0.02774888649582863 + 50.0 * 6.217133045196533
Epoch 2090, val loss: 1.3227803707122803
Epoch 2100, training loss: 310.7686462402344 = 0.027270769700407982 + 50.0 * 6.214827537536621
Epoch 2100, val loss: 1.3260807991027832
Epoch 2110, training loss: 310.7331848144531 = 0.02680901810526848 + 50.0 * 6.214128017425537
Epoch 2110, val loss: 1.329955816268921
Epoch 2120, training loss: 310.9228210449219 = 0.026366980746388435 + 50.0 * 6.217928886413574
Epoch 2120, val loss: 1.3334912061691284
Epoch 2130, training loss: 310.7331848144531 = 0.025907928124070168 + 50.0 * 6.214145660400391
Epoch 2130, val loss: 1.3366625308990479
Epoch 2140, training loss: 310.7285461425781 = 0.025476545095443726 + 50.0 * 6.214061260223389
Epoch 2140, val loss: 1.340167760848999
Epoch 2150, training loss: 310.7804870605469 = 0.02505480870604515 + 50.0 * 6.215108394622803
Epoch 2150, val loss: 1.3438665866851807
Epoch 2160, training loss: 310.6918640136719 = 0.02464720606803894 + 50.0 * 6.213344097137451
Epoch 2160, val loss: 1.3472108840942383
Epoch 2170, training loss: 310.720947265625 = 0.024252448230981827 + 50.0 * 6.213933944702148
Epoch 2170, val loss: 1.351164698600769
Epoch 2180, training loss: 310.6349182128906 = 0.023858025670051575 + 50.0 * 6.212221145629883
Epoch 2180, val loss: 1.354446291923523
Epoch 2190, training loss: 310.6366882324219 = 0.023473722860217094 + 50.0 * 6.212264537811279
Epoch 2190, val loss: 1.3574626445770264
Epoch 2200, training loss: 310.9931945800781 = 0.02311510406434536 + 50.0 * 6.219401836395264
Epoch 2200, val loss: 1.3610665798187256
Epoch 2210, training loss: 310.6180725097656 = 0.022724464535713196 + 50.0 * 6.211906909942627
Epoch 2210, val loss: 1.3641993999481201
Epoch 2220, training loss: 310.5693054199219 = 0.022361809387803078 + 50.0 * 6.210938930511475
Epoch 2220, val loss: 1.367331624031067
Epoch 2230, training loss: 310.58111572265625 = 0.02201964147388935 + 50.0 * 6.211181640625
Epoch 2230, val loss: 1.3708164691925049
Epoch 2240, training loss: 310.90582275390625 = 0.021680667996406555 + 50.0 * 6.2176833152771
Epoch 2240, val loss: 1.3740577697753906
Epoch 2250, training loss: 310.56488037109375 = 0.021340729668736458 + 50.0 * 6.21087121963501
Epoch 2250, val loss: 1.3773354291915894
Epoch 2260, training loss: 310.5322265625 = 0.021009039133787155 + 50.0 * 6.210224628448486
Epoch 2260, val loss: 1.3804179430007935
Epoch 2270, training loss: 310.6514587402344 = 0.020695030689239502 + 50.0 * 6.212615489959717
Epoch 2270, val loss: 1.3835855722427368
Epoch 2280, training loss: 310.6848449707031 = 0.020380087196826935 + 50.0 * 6.213289260864258
Epoch 2280, val loss: 1.38601553440094
Epoch 2290, training loss: 310.5801696777344 = 0.02006441168487072 + 50.0 * 6.211202621459961
Epoch 2290, val loss: 1.3899922370910645
Epoch 2300, training loss: 310.5270690917969 = 0.01976669579744339 + 50.0 * 6.210145950317383
Epoch 2300, val loss: 1.3929378986358643
Epoch 2310, training loss: 310.53143310546875 = 0.019475387409329414 + 50.0 * 6.210239410400391
Epoch 2310, val loss: 1.395948886871338
Epoch 2320, training loss: 310.6129150390625 = 0.019194506108760834 + 50.0 * 6.211874485015869
Epoch 2320, val loss: 1.3992639780044556
Epoch 2330, training loss: 310.5562744140625 = 0.01890617050230503 + 50.0 * 6.210746765136719
Epoch 2330, val loss: 1.4020156860351562
Epoch 2340, training loss: 310.6221618652344 = 0.018628323450684547 + 50.0 * 6.212070941925049
Epoch 2340, val loss: 1.404873251914978
Epoch 2350, training loss: 310.5850830078125 = 0.01835654489696026 + 50.0 * 6.211334228515625
Epoch 2350, val loss: 1.4078459739685059
Epoch 2360, training loss: 310.5177307128906 = 0.018089456483721733 + 50.0 * 6.209992408752441
Epoch 2360, val loss: 1.4105494022369385
Epoch 2370, training loss: 310.4408874511719 = 0.017829574644565582 + 50.0 * 6.208461284637451
Epoch 2370, val loss: 1.4138118028640747
Epoch 2380, training loss: 310.4891662597656 = 0.01758132502436638 + 50.0 * 6.2094316482543945
Epoch 2380, val loss: 1.4169245958328247
Epoch 2390, training loss: 310.76910400390625 = 0.01733285002410412 + 50.0 * 6.215035438537598
Epoch 2390, val loss: 1.4195605516433716
Epoch 2400, training loss: 310.44256591796875 = 0.017087513580918312 + 50.0 * 6.20850944519043
Epoch 2400, val loss: 1.4221067428588867
Epoch 2410, training loss: 310.4297790527344 = 0.016846461221575737 + 50.0 * 6.208258628845215
Epoch 2410, val loss: 1.425132155418396
Epoch 2420, training loss: 310.6376953125 = 0.016614222899079323 + 50.0 * 6.212421894073486
Epoch 2420, val loss: 1.4277777671813965
Epoch 2430, training loss: 310.5830383300781 = 0.016387639567255974 + 50.0 * 6.21133279800415
Epoch 2430, val loss: 1.4302566051483154
Epoch 2440, training loss: 310.42401123046875 = 0.01615588366985321 + 50.0 * 6.208157539367676
Epoch 2440, val loss: 1.433389663696289
Epoch 2450, training loss: 310.3858642578125 = 0.0159393772482872 + 50.0 * 6.207398414611816
Epoch 2450, val loss: 1.4359623193740845
Epoch 2460, training loss: 310.4006652832031 = 0.015729589387774467 + 50.0 * 6.207698822021484
Epoch 2460, val loss: 1.438955307006836
Epoch 2470, training loss: 310.6727600097656 = 0.015526309609413147 + 50.0 * 6.213144302368164
Epoch 2470, val loss: 1.4421638250350952
Epoch 2480, training loss: 310.4765930175781 = 0.015311019495129585 + 50.0 * 6.209225177764893
Epoch 2480, val loss: 1.4435248374938965
Epoch 2490, training loss: 310.45782470703125 = 0.015107505954802036 + 50.0 * 6.2088541984558105
Epoch 2490, val loss: 1.4466664791107178
Epoch 2500, training loss: 310.4278564453125 = 0.014913367107510567 + 50.0 * 6.208258628845215
Epoch 2500, val loss: 1.4489861726760864
Epoch 2510, training loss: 310.5680236816406 = 0.014725998044013977 + 50.0 * 6.211065769195557
Epoch 2510, val loss: 1.4517412185668945
Epoch 2520, training loss: 310.4730529785156 = 0.014524636790156364 + 50.0 * 6.209170341491699
Epoch 2520, val loss: 1.4538028240203857
Epoch 2530, training loss: 310.3518981933594 = 0.014333355240523815 + 50.0 * 6.206751346588135
Epoch 2530, val loss: 1.4567511081695557
Epoch 2540, training loss: 310.3203430175781 = 0.014152349904179573 + 50.0 * 6.2061238288879395
Epoch 2540, val loss: 1.4590178728103638
Epoch 2550, training loss: 310.30975341796875 = 0.013974563218653202 + 50.0 * 6.205915451049805
Epoch 2550, val loss: 1.4615249633789062
Epoch 2560, training loss: 310.6798095703125 = 0.01380379218608141 + 50.0 * 6.213320255279541
Epoch 2560, val loss: 1.4637541770935059
Epoch 2570, training loss: 310.3783264160156 = 0.013627639040350914 + 50.0 * 6.207293510437012
Epoch 2570, val loss: 1.4661149978637695
Epoch 2580, training loss: 310.49237060546875 = 0.013452834449708462 + 50.0 * 6.209578037261963
Epoch 2580, val loss: 1.4684497117996216
Epoch 2590, training loss: 310.30426025390625 = 0.013287516310811043 + 50.0 * 6.205819606781006
Epoch 2590, val loss: 1.4707790613174438
Epoch 2600, training loss: 310.3058776855469 = 0.013126079924404621 + 50.0 * 6.205854892730713
Epoch 2600, val loss: 1.4734797477722168
Epoch 2610, training loss: 310.29656982421875 = 0.01296806801110506 + 50.0 * 6.205672264099121
Epoch 2610, val loss: 1.4755890369415283
Epoch 2620, training loss: 310.4617614746094 = 0.012816238217055798 + 50.0 * 6.208978652954102
Epoch 2620, val loss: 1.4782379865646362
Epoch 2630, training loss: 310.3516540527344 = 0.012656946666538715 + 50.0 * 6.206779956817627
Epoch 2630, val loss: 1.4802337884902954
Epoch 2640, training loss: 310.29071044921875 = 0.012502308934926987 + 50.0 * 6.205564022064209
Epoch 2640, val loss: 1.4820921421051025
Epoch 2650, training loss: 310.2814025878906 = 0.012353803031146526 + 50.0 * 6.205380916595459
Epoch 2650, val loss: 1.4846163988113403
Epoch 2660, training loss: 310.2869567871094 = 0.012209891341626644 + 50.0 * 6.2054948806762695
Epoch 2660, val loss: 1.4866557121276855
Epoch 2670, training loss: 310.60003662109375 = 0.012069585733115673 + 50.0 * 6.211759567260742
Epoch 2670, val loss: 1.4885293245315552
Epoch 2680, training loss: 310.37286376953125 = 0.011930875480175018 + 50.0 * 6.207218647003174
Epoch 2680, val loss: 1.4908967018127441
Epoch 2690, training loss: 310.2501525878906 = 0.011785435490310192 + 50.0 * 6.20476770401001
Epoch 2690, val loss: 1.4929864406585693
Epoch 2700, training loss: 310.2612609863281 = 0.011654829606413841 + 50.0 * 6.204992294311523
Epoch 2700, val loss: 1.4953629970550537
Epoch 2710, training loss: 310.4295349121094 = 0.011524489149451256 + 50.0 * 6.208359718322754
Epoch 2710, val loss: 1.497667908668518
Epoch 2720, training loss: 310.27386474609375 = 0.011390216648578644 + 50.0 * 6.205249786376953
Epoch 2720, val loss: 1.4990627765655518
Epoch 2730, training loss: 310.1830749511719 = 0.01125923078507185 + 50.0 * 6.203436374664307
Epoch 2730, val loss: 1.5013493299484253
Epoch 2740, training loss: 310.41998291015625 = 0.011135420762002468 + 50.0 * 6.208176612854004
Epoch 2740, val loss: 1.5034213066101074
Epoch 2750, training loss: 310.1850891113281 = 0.011008856818079948 + 50.0 * 6.203481674194336
Epoch 2750, val loss: 1.5048712491989136
Epoch 2760, training loss: 310.21600341796875 = 0.010887873359024525 + 50.0 * 6.204102039337158
Epoch 2760, val loss: 1.5071073770523071
Epoch 2770, training loss: 310.327392578125 = 0.010768886655569077 + 50.0 * 6.206332206726074
Epoch 2770, val loss: 1.50871741771698
Epoch 2780, training loss: 310.14892578125 = 0.010650034993886948 + 50.0 * 6.202765464782715
Epoch 2780, val loss: 1.511167287826538
Epoch 2790, training loss: 310.1860656738281 = 0.010538822039961815 + 50.0 * 6.203510761260986
Epoch 2790, val loss: 1.5133788585662842
Epoch 2800, training loss: 310.244140625 = 0.010425982996821404 + 50.0 * 6.204674243927002
Epoch 2800, val loss: 1.5149074792861938
Epoch 2810, training loss: 310.2540283203125 = 0.010312739759683609 + 50.0 * 6.204874515533447
Epoch 2810, val loss: 1.5168046951293945
Epoch 2820, training loss: 310.2057189941406 = 0.010203293524682522 + 50.0 * 6.2039103507995605
Epoch 2820, val loss: 1.518802523612976
Epoch 2830, training loss: 310.2684326171875 = 0.010096793062984943 + 50.0 * 6.205166816711426
Epoch 2830, val loss: 1.5203275680541992
Epoch 2840, training loss: 310.3513488769531 = 0.00998925045132637 + 50.0 * 6.206827640533447
Epoch 2840, val loss: 1.5225461721420288
Epoch 2850, training loss: 310.28167724609375 = 0.009885341860353947 + 50.0 * 6.205435752868652
Epoch 2850, val loss: 1.5241025686264038
Epoch 2860, training loss: 310.159423828125 = 0.009778836742043495 + 50.0 * 6.202993392944336
Epoch 2860, val loss: 1.5261260271072388
Epoch 2870, training loss: 310.0973205566406 = 0.00967937521636486 + 50.0 * 6.20175313949585
Epoch 2870, val loss: 1.5279420614242554
Epoch 2880, training loss: 310.0782470703125 = 0.009583190083503723 + 50.0 * 6.20137357711792
Epoch 2880, val loss: 1.5295794010162354
Epoch 2890, training loss: 310.4817810058594 = 0.009488098323345184 + 50.0 * 6.209445953369141
Epoch 2890, val loss: 1.5309889316558838
Epoch 2900, training loss: 310.2575988769531 = 0.0093942079693079 + 50.0 * 6.2049641609191895
Epoch 2900, val loss: 1.5335005521774292
Epoch 2910, training loss: 310.1607666015625 = 0.00929404329508543 + 50.0 * 6.203029155731201
Epoch 2910, val loss: 1.5342707633972168
Epoch 2920, training loss: 310.0838623046875 = 0.00920145120471716 + 50.0 * 6.201492786407471
Epoch 2920, val loss: 1.5366493463516235
Epoch 2930, training loss: 310.053466796875 = 0.009112359024584293 + 50.0 * 6.2008867263793945
Epoch 2930, val loss: 1.5379817485809326
Epoch 2940, training loss: 310.2051696777344 = 0.009029100649058819 + 50.0 * 6.203922748565674
Epoch 2940, val loss: 1.5399330854415894
Epoch 2950, training loss: 310.16815185546875 = 0.008938298560678959 + 50.0 * 6.203184127807617
Epoch 2950, val loss: 1.5410352945327759
Epoch 2960, training loss: 310.2524108886719 = 0.008850404992699623 + 50.0 * 6.20487117767334
Epoch 2960, val loss: 1.5429250001907349
Epoch 2970, training loss: 310.0738220214844 = 0.008758331649005413 + 50.0 * 6.201301574707031
Epoch 2970, val loss: 1.5445175170898438
Epoch 2980, training loss: 310.04315185546875 = 0.008675813674926758 + 50.0 * 6.200689792633057
Epoch 2980, val loss: 1.546059250831604
Epoch 2990, training loss: 310.0471496582031 = 0.008595967665314674 + 50.0 * 6.200770854949951
Epoch 2990, val loss: 1.5475022792816162
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.737037037037037
0.8328940432261466
=== training gcn model ===
Epoch 0, training loss: 431.80426025390625 = 1.96384596824646 + 50.0 * 8.596808433532715
Epoch 0, val loss: 1.958276629447937
Epoch 10, training loss: 431.7379150390625 = 1.954186201095581 + 50.0 * 8.595674514770508
Epoch 10, val loss: 1.9482609033584595
Epoch 20, training loss: 431.2942199707031 = 1.942700743675232 + 50.0 * 8.587030410766602
Epoch 20, val loss: 1.9363127946853638
Epoch 30, training loss: 428.26727294921875 = 1.9286459684371948 + 50.0 * 8.526772499084473
Epoch 30, val loss: 1.9216417074203491
Epoch 40, training loss: 409.6430358886719 = 1.9130216836929321 + 50.0 * 8.154600143432617
Epoch 40, val loss: 1.9057255983352661
Epoch 50, training loss: 370.8755798339844 = 1.8955062627792358 + 50.0 * 7.37960147857666
Epoch 50, val loss: 1.887959599494934
Epoch 60, training loss: 360.364013671875 = 1.8819844722747803 + 50.0 * 7.16964054107666
Epoch 60, val loss: 1.8751435279846191
Epoch 70, training loss: 355.1411437988281 = 1.8693732023239136 + 50.0 * 7.065435409545898
Epoch 70, val loss: 1.862578272819519
Epoch 80, training loss: 350.7811279296875 = 1.85508131980896 + 50.0 * 6.978521347045898
Epoch 80, val loss: 1.8488004207611084
Epoch 90, training loss: 346.67181396484375 = 1.842698574066162 + 50.0 * 6.896582126617432
Epoch 90, val loss: 1.8372606039047241
Epoch 100, training loss: 342.9573669433594 = 1.831152319908142 + 50.0 * 6.822524547576904
Epoch 100, val loss: 1.8268251419067383
Epoch 110, training loss: 338.9400329589844 = 1.8209604024887085 + 50.0 * 6.742381572723389
Epoch 110, val loss: 1.8180279731750488
Epoch 120, training loss: 335.1561584472656 = 1.8121737241744995 + 50.0 * 6.666879653930664
Epoch 120, val loss: 1.8107049465179443
Epoch 130, training loss: 332.7010803222656 = 1.803831934928894 + 50.0 * 6.617945194244385
Epoch 130, val loss: 1.8036404848098755
Epoch 140, training loss: 330.74163818359375 = 1.7951488494873047 + 50.0 * 6.578929901123047
Epoch 140, val loss: 1.7962726354599
Epoch 150, training loss: 329.24908447265625 = 1.7863352298736572 + 50.0 * 6.549254894256592
Epoch 150, val loss: 1.7887500524520874
Epoch 160, training loss: 327.9526672363281 = 1.7771574258804321 + 50.0 * 6.523509979248047
Epoch 160, val loss: 1.7809641361236572
Epoch 170, training loss: 326.9093017578125 = 1.7673747539520264 + 50.0 * 6.502838611602783
Epoch 170, val loss: 1.7725802659988403
Epoch 180, training loss: 326.1683349609375 = 1.7568562030792236 + 50.0 * 6.488229274749756
Epoch 180, val loss: 1.7636613845825195
Epoch 190, training loss: 325.2295837402344 = 1.7456270456314087 + 50.0 * 6.46967887878418
Epoch 190, val loss: 1.7540650367736816
Epoch 200, training loss: 324.5059814453125 = 1.733498454093933 + 50.0 * 6.455450057983398
Epoch 200, val loss: 1.7439497709274292
Epoch 210, training loss: 323.9608459472656 = 1.7202143669128418 + 50.0 * 6.444812774658203
Epoch 210, val loss: 1.7327824831008911
Epoch 220, training loss: 323.4184875488281 = 1.7057218551635742 + 50.0 * 6.434255599975586
Epoch 220, val loss: 1.7207071781158447
Epoch 230, training loss: 322.8963928222656 = 1.689979076385498 + 50.0 * 6.424128532409668
Epoch 230, val loss: 1.7075897455215454
Epoch 240, training loss: 322.6459045410156 = 1.672823429107666 + 50.0 * 6.419461727142334
Epoch 240, val loss: 1.6934146881103516
Epoch 250, training loss: 322.0950927734375 = 1.6542224884033203 + 50.0 * 6.408817291259766
Epoch 250, val loss: 1.678061842918396
Epoch 260, training loss: 321.6821594238281 = 1.6342034339904785 + 50.0 * 6.400959014892578
Epoch 260, val loss: 1.6616878509521484
Epoch 270, training loss: 321.4922180175781 = 1.6126923561096191 + 50.0 * 6.397590637207031
Epoch 270, val loss: 1.6440727710723877
Epoch 280, training loss: 321.0472412109375 = 1.5896244049072266 + 50.0 * 6.3891520500183105
Epoch 280, val loss: 1.625362753868103
Epoch 290, training loss: 320.6766662597656 = 1.5651918649673462 + 50.0 * 6.382229804992676
Epoch 290, val loss: 1.6056820154190063
Epoch 300, training loss: 320.3915100097656 = 1.5394612550735474 + 50.0 * 6.377040863037109
Epoch 300, val loss: 1.5848853588104248
Epoch 310, training loss: 320.53094482421875 = 1.512534260749817 + 50.0 * 6.380368232727051
Epoch 310, val loss: 1.5632250308990479
Epoch 320, training loss: 319.90863037109375 = 1.4844231605529785 + 50.0 * 6.368484020233154
Epoch 320, val loss: 1.5407835245132446
Epoch 330, training loss: 319.60272216796875 = 1.455549716949463 + 50.0 * 6.362943649291992
Epoch 330, val loss: 1.5178872346878052
Epoch 340, training loss: 319.3584289550781 = 1.4260143041610718 + 50.0 * 6.35864782333374
Epoch 340, val loss: 1.4946163892745972
Epoch 350, training loss: 320.5587158203125 = 1.39566969871521 + 50.0 * 6.383261203765869
Epoch 350, val loss: 1.4709439277648926
Epoch 360, training loss: 319.1273498535156 = 1.3650208711624146 + 50.0 * 6.355246543884277
Epoch 360, val loss: 1.4466835260391235
Epoch 370, training loss: 318.79730224609375 = 1.334281086921692 + 50.0 * 6.349260330200195
Epoch 370, val loss: 1.4231796264648438
Epoch 380, training loss: 318.5275573730469 = 1.3035805225372314 + 50.0 * 6.344479560852051
Epoch 380, val loss: 1.3996244668960571
Epoch 390, training loss: 318.3000793457031 = 1.2730531692504883 + 50.0 * 6.340540885925293
Epoch 390, val loss: 1.3766149282455444
Epoch 400, training loss: 318.0900573730469 = 1.2427196502685547 + 50.0 * 6.336946964263916
Epoch 400, val loss: 1.3539364337921143
Epoch 410, training loss: 317.8922424316406 = 1.2126573324203491 + 50.0 * 6.333591938018799
Epoch 410, val loss: 1.3317538499832153
Epoch 420, training loss: 317.9598693847656 = 1.182999610900879 + 50.0 * 6.335536956787109
Epoch 420, val loss: 1.310126781463623
Epoch 430, training loss: 317.61016845703125 = 1.153596043586731 + 50.0 * 6.329131603240967
Epoch 430, val loss: 1.2888892889022827
Epoch 440, training loss: 317.4339599609375 = 1.1249929666519165 + 50.0 * 6.326179027557373
Epoch 440, val loss: 1.2688473463058472
Epoch 450, training loss: 317.2234191894531 = 1.0971163511276245 + 50.0 * 6.322526454925537
Epoch 450, val loss: 1.2495976686477661
Epoch 460, training loss: 317.1412658691406 = 1.0699715614318848 + 50.0 * 6.321425914764404
Epoch 460, val loss: 1.2312169075012207
Epoch 470, training loss: 316.9339294433594 = 1.0435218811035156 + 50.0 * 6.317808151245117
Epoch 470, val loss: 1.2135847806930542
Epoch 480, training loss: 316.7306213378906 = 1.0179548263549805 + 50.0 * 6.314253330230713
Epoch 480, val loss: 1.1971843242645264
Epoch 490, training loss: 316.6534423828125 = 0.9931005239486694 + 50.0 * 6.313206672668457
Epoch 490, val loss: 1.1814543008804321
Epoch 500, training loss: 316.5552062988281 = 0.9689540863037109 + 50.0 * 6.311724662780762
Epoch 500, val loss: 1.1668651103973389
Epoch 510, training loss: 316.2735595703125 = 0.9455909729003906 + 50.0 * 6.3065595626831055
Epoch 510, val loss: 1.1530096530914307
Epoch 520, training loss: 316.1378173828125 = 0.92303067445755 + 50.0 * 6.304295539855957
Epoch 520, val loss: 1.140044927597046
Epoch 530, training loss: 316.0025634765625 = 0.901212751865387 + 50.0 * 6.302027225494385
Epoch 530, val loss: 1.128286361694336
Epoch 540, training loss: 315.9763488769531 = 0.8800649642944336 + 50.0 * 6.3019256591796875
Epoch 540, val loss: 1.1173427104949951
Epoch 550, training loss: 316.0142822265625 = 0.8594737648963928 + 50.0 * 6.303096294403076
Epoch 550, val loss: 1.1065689325332642
Epoch 560, training loss: 315.7552490234375 = 0.8394319415092468 + 50.0 * 6.29831600189209
Epoch 560, val loss: 1.097205638885498
Epoch 570, training loss: 315.56024169921875 = 0.8199692368507385 + 50.0 * 6.29480504989624
Epoch 570, val loss: 1.088296890258789
Epoch 580, training loss: 315.4102783203125 = 0.8011758327484131 + 50.0 * 6.292181968688965
Epoch 580, val loss: 1.0800596475601196
Epoch 590, training loss: 315.6716003417969 = 0.7828822731971741 + 50.0 * 6.297774791717529
Epoch 590, val loss: 1.072489619255066
Epoch 600, training loss: 315.3704833984375 = 0.7647272944450378 + 50.0 * 6.292114734649658
Epoch 600, val loss: 1.0655901432037354
Epoch 610, training loss: 315.10504150390625 = 0.7472284436225891 + 50.0 * 6.287156105041504
Epoch 610, val loss: 1.0593266487121582
Epoch 620, training loss: 314.9994812011719 = 0.7301995754241943 + 50.0 * 6.285385608673096
Epoch 620, val loss: 1.0534900426864624
Epoch 630, training loss: 314.9530029296875 = 0.7135274410247803 + 50.0 * 6.284789562225342
Epoch 630, val loss: 1.048396110534668
Epoch 640, training loss: 314.998779296875 = 0.697049617767334 + 50.0 * 6.28603458404541
Epoch 640, val loss: 1.043116807937622
Epoch 650, training loss: 314.7623596191406 = 0.6809426546096802 + 50.0 * 6.281628608703613
Epoch 650, val loss: 1.0387240648269653
Epoch 660, training loss: 314.666015625 = 0.6652575135231018 + 50.0 * 6.280014991760254
Epoch 660, val loss: 1.034896969795227
Epoch 670, training loss: 314.57293701171875 = 0.6500312685966492 + 50.0 * 6.278458118438721
Epoch 670, val loss: 1.0315889120101929
Epoch 680, training loss: 314.6614990234375 = 0.6351455450057983 + 50.0 * 6.280527114868164
Epoch 680, val loss: 1.028574824333191
Epoch 690, training loss: 314.4249267578125 = 0.6202850341796875 + 50.0 * 6.276093006134033
Epoch 690, val loss: 1.0257844924926758
Epoch 700, training loss: 314.37017822265625 = 0.6059214472770691 + 50.0 * 6.275285243988037
Epoch 700, val loss: 1.023440957069397
Epoch 710, training loss: 314.2929992675781 = 0.5917707681655884 + 50.0 * 6.274024486541748
Epoch 710, val loss: 1.021471619606018
Epoch 720, training loss: 314.48675537109375 = 0.5779646635055542 + 50.0 * 6.2781758308410645
Epoch 720, val loss: 1.0198016166687012
Epoch 730, training loss: 314.2250671386719 = 0.5643678307533264 + 50.0 * 6.273213863372803
Epoch 730, val loss: 1.018648386001587
Epoch 740, training loss: 314.1073303222656 = 0.5510871410369873 + 50.0 * 6.271124839782715
Epoch 740, val loss: 1.0174899101257324
Epoch 750, training loss: 314.1909484863281 = 0.5380783081054688 + 50.0 * 6.273057460784912
Epoch 750, val loss: 1.0167300701141357
Epoch 760, training loss: 313.98468017578125 = 0.5252562761306763 + 50.0 * 6.269188404083252
Epoch 760, val loss: 1.0161659717559814
Epoch 770, training loss: 313.90728759765625 = 0.5128099918365479 + 50.0 * 6.267889499664307
Epoch 770, val loss: 1.0158066749572754
Epoch 780, training loss: 313.84210205078125 = 0.5006445050239563 + 50.0 * 6.266829490661621
Epoch 780, val loss: 1.016069769859314
Epoch 790, training loss: 314.21148681640625 = 0.4886561930179596 + 50.0 * 6.274456977844238
Epoch 790, val loss: 1.0162630081176758
Epoch 800, training loss: 313.9031982421875 = 0.4770476222038269 + 50.0 * 6.2685227394104
Epoch 800, val loss: 1.0169247388839722
Epoch 810, training loss: 313.68609619140625 = 0.46552008390426636 + 50.0 * 6.264411926269531
Epoch 810, val loss: 1.0176732540130615
Epoch 820, training loss: 313.59661865234375 = 0.4544234275817871 + 50.0 * 6.262843608856201
Epoch 820, val loss: 1.0187269449234009
Epoch 830, training loss: 313.57037353515625 = 0.44351717829704285 + 50.0 * 6.262537002563477
Epoch 830, val loss: 1.0200984477996826
Epoch 840, training loss: 313.6923522949219 = 0.432739794254303 + 50.0 * 6.265192031860352
Epoch 840, val loss: 1.0216320753097534
Epoch 850, training loss: 313.5531005859375 = 0.4223146438598633 + 50.0 * 6.26261568069458
Epoch 850, val loss: 1.023316502571106
Epoch 860, training loss: 313.36639404296875 = 0.4120868444442749 + 50.0 * 6.2590861320495605
Epoch 860, val loss: 1.025256872177124
Epoch 870, training loss: 313.34136962890625 = 0.40213721990585327 + 50.0 * 6.258784770965576
Epoch 870, val loss: 1.0275193452835083
Epoch 880, training loss: 313.5879821777344 = 0.39238741993904114 + 50.0 * 6.263911724090576
Epoch 880, val loss: 1.0298175811767578
Epoch 890, training loss: 313.5050964355469 = 0.3827686905860901 + 50.0 * 6.262446403503418
Epoch 890, val loss: 1.0323758125305176
Epoch 900, training loss: 313.18817138671875 = 0.3733445703983307 + 50.0 * 6.256296157836914
Epoch 900, val loss: 1.0350568294525146
Epoch 910, training loss: 313.2195129394531 = 0.36418285965919495 + 50.0 * 6.257106304168701
Epoch 910, val loss: 1.0380074977874756
Epoch 920, training loss: 313.2904052734375 = 0.3551492691040039 + 50.0 * 6.258705139160156
Epoch 920, val loss: 1.0409091711044312
Epoch 930, training loss: 313.0785217285156 = 0.3464234173297882 + 50.0 * 6.254642009735107
Epoch 930, val loss: 1.0442050695419312
Epoch 940, training loss: 313.0166931152344 = 0.3378438353538513 + 50.0 * 6.25357723236084
Epoch 940, val loss: 1.04746675491333
Epoch 950, training loss: 312.95880126953125 = 0.32947733998298645 + 50.0 * 6.252586364746094
Epoch 950, val loss: 1.0511237382888794
Epoch 960, training loss: 312.94000244140625 = 0.32129019498825073 + 50.0 * 6.252374172210693
Epoch 960, val loss: 1.0545856952667236
Epoch 970, training loss: 313.34429931640625 = 0.3132200539112091 + 50.0 * 6.260621070861816
Epoch 970, val loss: 1.0580848455429077
Epoch 980, training loss: 312.8684387207031 = 0.3053326904773712 + 50.0 * 6.251262187957764
Epoch 980, val loss: 1.0620917081832886
Epoch 990, training loss: 312.8423156738281 = 0.2976110279560089 + 50.0 * 6.250894069671631
Epoch 990, val loss: 1.0664374828338623
Epoch 1000, training loss: 312.9971008300781 = 0.29009518027305603 + 50.0 * 6.2541399002075195
Epoch 1000, val loss: 1.070470929145813
Epoch 1010, training loss: 312.743408203125 = 0.28274983167648315 + 50.0 * 6.249213218688965
Epoch 1010, val loss: 1.0745576620101929
Epoch 1020, training loss: 312.6864318847656 = 0.27556589245796204 + 50.0 * 6.2482171058654785
Epoch 1020, val loss: 1.0792902708053589
Epoch 1030, training loss: 312.6408386230469 = 0.26858627796173096 + 50.0 * 6.247445106506348
Epoch 1030, val loss: 1.083741545677185
Epoch 1040, training loss: 312.70648193359375 = 0.26177269220352173 + 50.0 * 6.248894214630127
Epoch 1040, val loss: 1.0886067152023315
Epoch 1050, training loss: 313.0047912597656 = 0.25495702028274536 + 50.0 * 6.2549967765808105
Epoch 1050, val loss: 1.0930960178375244
Epoch 1060, training loss: 312.6399841308594 = 0.2484625279903412 + 50.0 * 6.247830390930176
Epoch 1060, val loss: 1.097670316696167
Epoch 1070, training loss: 312.5595703125 = 0.2420797348022461 + 50.0 * 6.246350288391113
Epoch 1070, val loss: 1.1029410362243652
Epoch 1080, training loss: 312.46966552734375 = 0.23591218888759613 + 50.0 * 6.244675159454346
Epoch 1080, val loss: 1.1082773208618164
Epoch 1090, training loss: 312.4224548339844 = 0.22992382943630219 + 50.0 * 6.2438507080078125
Epoch 1090, val loss: 1.1134629249572754
Epoch 1100, training loss: 312.37945556640625 = 0.224073126912117 + 50.0 * 6.243107795715332
Epoch 1100, val loss: 1.118895173072815
Epoch 1110, training loss: 312.5725402832031 = 0.2183704972267151 + 50.0 * 6.2470831871032715
Epoch 1110, val loss: 1.1243141889572144
Epoch 1120, training loss: 312.6331787109375 = 0.21274618804454803 + 50.0 * 6.248408794403076
Epoch 1120, val loss: 1.129745364189148
Epoch 1130, training loss: 312.3515625 = 0.20726828277111053 + 50.0 * 6.242885589599609
Epoch 1130, val loss: 1.1351337432861328
Epoch 1140, training loss: 312.2912902832031 = 0.2019759863615036 + 50.0 * 6.241786479949951
Epoch 1140, val loss: 1.1409245729446411
Epoch 1150, training loss: 312.25006103515625 = 0.19685488939285278 + 50.0 * 6.241063594818115
Epoch 1150, val loss: 1.1467770338058472
Epoch 1160, training loss: 312.2337646484375 = 0.1918836534023285 + 50.0 * 6.240837097167969
Epoch 1160, val loss: 1.1526062488555908
Epoch 1170, training loss: 312.634033203125 = 0.18698501586914062 + 50.0 * 6.248940944671631
Epoch 1170, val loss: 1.158536672592163
Epoch 1180, training loss: 312.24285888671875 = 0.18230126798152924 + 50.0 * 6.2412109375
Epoch 1180, val loss: 1.1642069816589355
Epoch 1190, training loss: 312.12615966796875 = 0.1776694655418396 + 50.0 * 6.238969802856445
Epoch 1190, val loss: 1.1701641082763672
Epoch 1200, training loss: 312.0868835449219 = 0.17323972284793854 + 50.0 * 6.238272666931152
Epoch 1200, val loss: 1.17628014087677
Epoch 1210, training loss: 312.3600769042969 = 0.16891469061374664 + 50.0 * 6.243823051452637
Epoch 1210, val loss: 1.18205988407135
Epoch 1220, training loss: 312.420166015625 = 0.16465263068675995 + 50.0 * 6.245110511779785
Epoch 1220, val loss: 1.1884623765945435
Epoch 1230, training loss: 312.0157775878906 = 0.16051572561264038 + 50.0 * 6.237105369567871
Epoch 1230, val loss: 1.1940912008285522
Epoch 1240, training loss: 312.00286865234375 = 0.15654121339321136 + 50.0 * 6.236926078796387
Epoch 1240, val loss: 1.20021390914917
Epoch 1250, training loss: 311.95562744140625 = 0.15270870923995972 + 50.0 * 6.236058235168457
Epoch 1250, val loss: 1.2064718008041382
Epoch 1260, training loss: 311.92572021484375 = 0.1489778757095337 + 50.0 * 6.23553466796875
Epoch 1260, val loss: 1.2127845287322998
Epoch 1270, training loss: 312.2836608886719 = 0.14534294605255127 + 50.0 * 6.2427659034729
Epoch 1270, val loss: 1.2191017866134644
Epoch 1280, training loss: 312.1148681640625 = 0.1417483538389206 + 50.0 * 6.239462375640869
Epoch 1280, val loss: 1.2247276306152344
Epoch 1290, training loss: 311.8873596191406 = 0.13828760385513306 + 50.0 * 6.234981536865234
Epoch 1290, val loss: 1.2311722040176392
Epoch 1300, training loss: 311.8468933105469 = 0.1349581927061081 + 50.0 * 6.234239101409912
Epoch 1300, val loss: 1.237589716911316
Epoch 1310, training loss: 311.94659423828125 = 0.1317487210035324 + 50.0 * 6.236297130584717
Epoch 1310, val loss: 1.2440217733383179
Epoch 1320, training loss: 311.8758239746094 = 0.12856097519397736 + 50.0 * 6.234944820404053
Epoch 1320, val loss: 1.2502533197402954
Epoch 1330, training loss: 311.8475646972656 = 0.12543779611587524 + 50.0 * 6.234442710876465
Epoch 1330, val loss: 1.2565593719482422
Epoch 1340, training loss: 311.7432861328125 = 0.12247105687856674 + 50.0 * 6.232416152954102
Epoch 1340, val loss: 1.263166069984436
Epoch 1350, training loss: 312.2590026855469 = 0.11956630647182465 + 50.0 * 6.242788791656494
Epoch 1350, val loss: 1.2697958946228027
Epoch 1360, training loss: 311.8633728027344 = 0.11668390035629272 + 50.0 * 6.234934329986572
Epoch 1360, val loss: 1.2756925821304321
Epoch 1370, training loss: 311.7469482421875 = 0.11390459537506104 + 50.0 * 6.232661247253418
Epoch 1370, val loss: 1.2821563482284546
Epoch 1380, training loss: 311.66632080078125 = 0.11122002452611923 + 50.0 * 6.231101989746094
Epoch 1380, val loss: 1.2888919115066528
Epoch 1390, training loss: 311.6221618652344 = 0.10861725360155106 + 50.0 * 6.230270862579346
Epoch 1390, val loss: 1.295411467552185
Epoch 1400, training loss: 311.8170471191406 = 0.10608595609664917 + 50.0 * 6.234219074249268
Epoch 1400, val loss: 1.3018438816070557
Epoch 1410, training loss: 311.6045227050781 = 0.1035989448428154 + 50.0 * 6.230018615722656
Epoch 1410, val loss: 1.3083583116531372
Epoch 1420, training loss: 311.7325134277344 = 0.10118310153484344 + 50.0 * 6.232626438140869
Epoch 1420, val loss: 1.3147516250610352
Epoch 1430, training loss: 311.6141052246094 = 0.09883090108633041 + 50.0 * 6.2303056716918945
Epoch 1430, val loss: 1.3213253021240234
Epoch 1440, training loss: 311.55780029296875 = 0.09655851870775223 + 50.0 * 6.229224681854248
Epoch 1440, val loss: 1.3278437852859497
Epoch 1450, training loss: 311.58056640625 = 0.09437213838100433 + 50.0 * 6.229724407196045
Epoch 1450, val loss: 1.3344507217407227
Epoch 1460, training loss: 311.6763916015625 = 0.09221905469894409 + 50.0 * 6.231683254241943
Epoch 1460, val loss: 1.3409467935562134
Epoch 1470, training loss: 311.5550231933594 = 0.09008565545082092 + 50.0 * 6.2292985916137695
Epoch 1470, val loss: 1.347320795059204
Epoch 1480, training loss: 311.4910583496094 = 0.08802653104066849 + 50.0 * 6.228061199188232
Epoch 1480, val loss: 1.3538382053375244
Epoch 1490, training loss: 311.499755859375 = 0.08604589104652405 + 50.0 * 6.228273868560791
Epoch 1490, val loss: 1.3603705167770386
Epoch 1500, training loss: 311.6551818847656 = 0.08410151302814484 + 50.0 * 6.23142147064209
Epoch 1500, val loss: 1.3667887449264526
Epoch 1510, training loss: 311.4688415527344 = 0.0821976512670517 + 50.0 * 6.227733135223389
Epoch 1510, val loss: 1.3732727766036987
Epoch 1520, training loss: 311.4635314941406 = 0.08034757524728775 + 50.0 * 6.227663516998291
Epoch 1520, val loss: 1.379721999168396
Epoch 1530, training loss: 311.38604736328125 = 0.07856417447328568 + 50.0 * 6.226150035858154
Epoch 1530, val loss: 1.3861619234085083
Epoch 1540, training loss: 311.34051513671875 = 0.07684574276208878 + 50.0 * 6.225273609161377
Epoch 1540, val loss: 1.3924976587295532
Epoch 1550, training loss: 311.4491882324219 = 0.07518800348043442 + 50.0 * 6.227479934692383
Epoch 1550, val loss: 1.3988367319107056
Epoch 1560, training loss: 311.40704345703125 = 0.07353035360574722 + 50.0 * 6.226670742034912
Epoch 1560, val loss: 1.4047750234603882
Epoch 1570, training loss: 311.3683166503906 = 0.07191882282495499 + 50.0 * 6.225927829742432
Epoch 1570, val loss: 1.4107228517532349
Epoch 1580, training loss: 311.3236389160156 = 0.07035142183303833 + 50.0 * 6.2250657081604
Epoch 1580, val loss: 1.4168107509613037
Epoch 1590, training loss: 311.2586364746094 = 0.06883153319358826 + 50.0 * 6.223796367645264
Epoch 1590, val loss: 1.423091173171997
Epoch 1600, training loss: 311.2530212402344 = 0.06736413389444351 + 50.0 * 6.223713397979736
Epoch 1600, val loss: 1.4291541576385498
Epoch 1610, training loss: 311.82196044921875 = 0.06592697650194168 + 50.0 * 6.23512077331543
Epoch 1610, val loss: 1.4350550174713135
Epoch 1620, training loss: 311.39837646484375 = 0.06449587643146515 + 50.0 * 6.226677894592285
Epoch 1620, val loss: 1.4412506818771362
Epoch 1630, training loss: 311.30157470703125 = 0.06311244517564774 + 50.0 * 6.224769592285156
Epoch 1630, val loss: 1.4470984935760498
Epoch 1640, training loss: 311.1976013183594 = 0.06177980452775955 + 50.0 * 6.222716808319092
Epoch 1640, val loss: 1.4532057046890259
Epoch 1650, training loss: 311.1519470214844 = 0.06049489974975586 + 50.0 * 6.221828937530518
Epoch 1650, val loss: 1.459293246269226
Epoch 1660, training loss: 311.22088623046875 = 0.05923899635672569 + 50.0 * 6.223233222961426
Epoch 1660, val loss: 1.4652774333953857
Epoch 1670, training loss: 311.505859375 = 0.05798426643013954 + 50.0 * 6.228957653045654
Epoch 1670, val loss: 1.4710378646850586
Epoch 1680, training loss: 311.15576171875 = 0.05679207295179367 + 50.0 * 6.22197961807251
Epoch 1680, val loss: 1.4768131971359253
Epoch 1690, training loss: 311.1084289550781 = 0.05560483783483505 + 50.0 * 6.2210564613342285
Epoch 1690, val loss: 1.4827805757522583
Epoch 1700, training loss: 311.0811767578125 = 0.05446704477071762 + 50.0 * 6.220534324645996
Epoch 1700, val loss: 1.4888029098510742
Epoch 1710, training loss: 311.0616149902344 = 0.05336696282029152 + 50.0 * 6.220164775848389
Epoch 1710, val loss: 1.4947009086608887
Epoch 1720, training loss: 311.53668212890625 = 0.05229811370372772 + 50.0 * 6.229687690734863
Epoch 1720, val loss: 1.5002130270004272
Epoch 1730, training loss: 311.2865295410156 = 0.05121292918920517 + 50.0 * 6.224706649780273
Epoch 1730, val loss: 1.5060930252075195
Epoch 1740, training loss: 311.06671142578125 = 0.050168197602033615 + 50.0 * 6.220330715179443
Epoch 1740, val loss: 1.5117384195327759
Epoch 1750, training loss: 311.06280517578125 = 0.049164917320013046 + 50.0 * 6.220273017883301
Epoch 1750, val loss: 1.517471194267273
Epoch 1760, training loss: 311.2057189941406 = 0.04818566516041756 + 50.0 * 6.223150730133057
Epoch 1760, val loss: 1.523192286491394
Epoch 1770, training loss: 311.02020263671875 = 0.04722709208726883 + 50.0 * 6.219459056854248
Epoch 1770, val loss: 1.5288136005401611
Epoch 1780, training loss: 311.1751403808594 = 0.046299804002046585 + 50.0 * 6.22257661819458
Epoch 1780, val loss: 1.5345548391342163
Epoch 1790, training loss: 311.08245849609375 = 0.045393913984298706 + 50.0 * 6.2207417488098145
Epoch 1790, val loss: 1.5400153398513794
Epoch 1800, training loss: 310.9498596191406 = 0.04450807347893715 + 50.0 * 6.218107223510742
Epoch 1800, val loss: 1.5455505847930908
Epoch 1810, training loss: 310.93267822265625 = 0.04365087300539017 + 50.0 * 6.217781066894531
Epoch 1810, val loss: 1.551194429397583
Epoch 1820, training loss: 311.1434631347656 = 0.04281769320368767 + 50.0 * 6.222012996673584
Epoch 1820, val loss: 1.5565615892410278
Epoch 1830, training loss: 311.1914978027344 = 0.04197392240166664 + 50.0 * 6.222990989685059
Epoch 1830, val loss: 1.561941385269165
Epoch 1840, training loss: 311.0052795410156 = 0.04118216037750244 + 50.0 * 6.219282150268555
Epoch 1840, val loss: 1.567211627960205
Epoch 1850, training loss: 310.9142761230469 = 0.04038650542497635 + 50.0 * 6.217478275299072
Epoch 1850, val loss: 1.572762370109558
Epoch 1860, training loss: 310.859375 = 0.039636075496673584 + 50.0 * 6.216394424438477
Epoch 1860, val loss: 1.5782232284545898
Epoch 1870, training loss: 310.84075927734375 = 0.038904428482055664 + 50.0 * 6.216036796569824
Epoch 1870, val loss: 1.5836083889007568
Epoch 1880, training loss: 310.9762878417969 = 0.03819699212908745 + 50.0 * 6.218761444091797
Epoch 1880, val loss: 1.5887261629104614
Epoch 1890, training loss: 311.01715087890625 = 0.03747355937957764 + 50.0 * 6.219593524932861
Epoch 1890, val loss: 1.5938690900802612
Epoch 1900, training loss: 310.85968017578125 = 0.036777254194021225 + 50.0 * 6.216458320617676
Epoch 1900, val loss: 1.5990480184555054
Epoch 1910, training loss: 310.8695983886719 = 0.03609909489750862 + 50.0 * 6.216670036315918
Epoch 1910, val loss: 1.6043541431427002
Epoch 1920, training loss: 310.98663330078125 = 0.0354582704603672 + 50.0 * 6.21902322769165
Epoch 1920, val loss: 1.6096125841140747
Epoch 1930, training loss: 310.857177734375 = 0.034806352108716965 + 50.0 * 6.216446876525879
Epoch 1930, val loss: 1.614832878112793
Epoch 1940, training loss: 310.815673828125 = 0.034171100705862045 + 50.0 * 6.215629577636719
Epoch 1940, val loss: 1.6199274063110352
Epoch 1950, training loss: 310.7579345703125 = 0.03356412425637245 + 50.0 * 6.214487552642822
Epoch 1950, val loss: 1.6250466108322144
Epoch 1960, training loss: 310.71612548828125 = 0.03297210484743118 + 50.0 * 6.213663578033447
Epoch 1960, val loss: 1.6300960779190063
Epoch 1970, training loss: 310.93121337890625 = 0.03240049257874489 + 50.0 * 6.217976093292236
Epoch 1970, val loss: 1.634939193725586
Epoch 1980, training loss: 311.02581787109375 = 0.03181431442499161 + 50.0 * 6.2198805809021
Epoch 1980, val loss: 1.6400442123413086
Epoch 1990, training loss: 310.80706787109375 = 0.03125586733222008 + 50.0 * 6.215516567230225
Epoch 1990, val loss: 1.6446336507797241
Epoch 2000, training loss: 310.7075500488281 = 0.03070857748389244 + 50.0 * 6.213537216186523
Epoch 2000, val loss: 1.6498998403549194
Epoch 2010, training loss: 310.6628112792969 = 0.030180245637893677 + 50.0 * 6.212652683258057
Epoch 2010, val loss: 1.6549508571624756
Epoch 2020, training loss: 310.6588134765625 = 0.02967154048383236 + 50.0 * 6.212583065032959
Epoch 2020, val loss: 1.6599032878875732
Epoch 2030, training loss: 310.86199951171875 = 0.029175199568271637 + 50.0 * 6.216656684875488
Epoch 2030, val loss: 1.6648799180984497
Epoch 2040, training loss: 310.93963623046875 = 0.02868633158504963 + 50.0 * 6.218218803405762
Epoch 2040, val loss: 1.6694036722183228
Epoch 2050, training loss: 310.6747131347656 = 0.02817622572183609 + 50.0 * 6.212930679321289
Epoch 2050, val loss: 1.6739307641983032
Epoch 2060, training loss: 310.6529846191406 = 0.027704160660505295 + 50.0 * 6.212505340576172
Epoch 2060, val loss: 1.6787440776824951
Epoch 2070, training loss: 310.6148681640625 = 0.027248751372098923 + 50.0 * 6.211752414703369
Epoch 2070, val loss: 1.6838033199310303
Epoch 2080, training loss: 310.5948791503906 = 0.02680729515850544 + 50.0 * 6.211361408233643
Epoch 2080, val loss: 1.6884574890136719
Epoch 2090, training loss: 310.74505615234375 = 0.02637825347483158 + 50.0 * 6.21437406539917
Epoch 2090, val loss: 1.693049430847168
Epoch 2100, training loss: 310.7383117675781 = 0.025935668498277664 + 50.0 * 6.214247703552246
Epoch 2100, val loss: 1.697234869003296
Epoch 2110, training loss: 310.59149169921875 = 0.025501055642962456 + 50.0 * 6.211319446563721
Epoch 2110, val loss: 1.7018117904663086
Epoch 2120, training loss: 310.5843200683594 = 0.025092830881476402 + 50.0 * 6.211184978485107
Epoch 2120, val loss: 1.7064450979232788
Epoch 2130, training loss: 310.6050720214844 = 0.024693353101611137 + 50.0 * 6.211607933044434
Epoch 2130, val loss: 1.7110204696655273
Epoch 2140, training loss: 310.8291015625 = 0.02429872378706932 + 50.0 * 6.216095924377441
Epoch 2140, val loss: 1.715410828590393
Epoch 2150, training loss: 310.6148681640625 = 0.023914827033877373 + 50.0 * 6.211818695068359
Epoch 2150, val loss: 1.719780445098877
Epoch 2160, training loss: 310.53387451171875 = 0.023539744317531586 + 50.0 * 6.210206985473633
Epoch 2160, val loss: 1.7242634296417236
Epoch 2170, training loss: 310.5068054199219 = 0.023177525028586388 + 50.0 * 6.209672927856445
Epoch 2170, val loss: 1.7288267612457275
Epoch 2180, training loss: 310.6854553222656 = 0.022829683497548103 + 50.0 * 6.213252544403076
Epoch 2180, val loss: 1.733258605003357
Epoch 2190, training loss: 310.5464782714844 = 0.022473042830824852 + 50.0 * 6.210480213165283
Epoch 2190, val loss: 1.7371904850006104
Epoch 2200, training loss: 310.52947998046875 = 0.022124135866761208 + 50.0 * 6.210147380828857
Epoch 2200, val loss: 1.74147367477417
Epoch 2210, training loss: 310.5007019042969 = 0.02178802154958248 + 50.0 * 6.209578037261963
Epoch 2210, val loss: 1.74579656124115
Epoch 2220, training loss: 310.57373046875 = 0.021466068923473358 + 50.0 * 6.211045742034912
Epoch 2220, val loss: 1.7499264478683472
Epoch 2230, training loss: 310.5941162109375 = 0.0211441982537508 + 50.0 * 6.211459636688232
Epoch 2230, val loss: 1.7541838884353638
Epoch 2240, training loss: 310.5565185546875 = 0.020817440003156662 + 50.0 * 6.210713863372803
Epoch 2240, val loss: 1.758270502090454
Epoch 2250, training loss: 310.4187316894531 = 0.020513338968157768 + 50.0 * 6.2079644203186035
Epoch 2250, val loss: 1.7623138427734375
Epoch 2260, training loss: 310.408447265625 = 0.020214471966028214 + 50.0 * 6.207764625549316
Epoch 2260, val loss: 1.7665226459503174
Epoch 2270, training loss: 310.8081359863281 = 0.019934818148612976 + 50.0 * 6.215764045715332
Epoch 2270, val loss: 1.7706212997436523
Epoch 2280, training loss: 310.6009826660156 = 0.019631855189800262 + 50.0 * 6.211627006530762
Epoch 2280, val loss: 1.7740552425384521
Epoch 2290, training loss: 310.4788818359375 = 0.019334878772497177 + 50.0 * 6.209190368652344
Epoch 2290, val loss: 1.7780802249908447
Epoch 2300, training loss: 310.3725891113281 = 0.01905788481235504 + 50.0 * 6.207070827484131
Epoch 2300, val loss: 1.7821804285049438
Epoch 2310, training loss: 310.3397521972656 = 0.018789982423186302 + 50.0 * 6.206418991088867
Epoch 2310, val loss: 1.78623366355896
Epoch 2320, training loss: 310.37200927734375 = 0.01853088289499283 + 50.0 * 6.2070698738098145
Epoch 2320, val loss: 1.7901743650436401
Epoch 2330, training loss: 310.6816711425781 = 0.018271373584866524 + 50.0 * 6.213267803192139
Epoch 2330, val loss: 1.7939496040344238
Epoch 2340, training loss: 310.5088806152344 = 0.018015647307038307 + 50.0 * 6.209817409515381
Epoch 2340, val loss: 1.7975552082061768
Epoch 2350, training loss: 310.43682861328125 = 0.017758803442120552 + 50.0 * 6.208381175994873
Epoch 2350, val loss: 1.8014299869537354
Epoch 2360, training loss: 310.4006042480469 = 0.017517585307359695 + 50.0 * 6.2076616287231445
Epoch 2360, val loss: 1.8052575588226318
Epoch 2370, training loss: 310.4949951171875 = 0.01728111319243908 + 50.0 * 6.209554672241211
Epoch 2370, val loss: 1.8091986179351807
Epoch 2380, training loss: 310.4008483886719 = 0.017043733969330788 + 50.0 * 6.207676410675049
Epoch 2380, val loss: 1.8128013610839844
Epoch 2390, training loss: 310.4056091308594 = 0.016816699877381325 + 50.0 * 6.207776069641113
Epoch 2390, val loss: 1.8164787292480469
Epoch 2400, training loss: 310.3699645996094 = 0.01659017987549305 + 50.0 * 6.207067012786865
Epoch 2400, val loss: 1.8202149868011475
Epoch 2410, training loss: 310.27899169921875 = 0.016364412382245064 + 50.0 * 6.205252647399902
Epoch 2410, val loss: 1.8239959478378296
Epoch 2420, training loss: 310.3226318359375 = 0.01615232788026333 + 50.0 * 6.206129550933838
Epoch 2420, val loss: 1.8276665210723877
Epoch 2430, training loss: 310.42352294921875 = 0.01594495214521885 + 50.0 * 6.208151817321777
Epoch 2430, val loss: 1.8311867713928223
Epoch 2440, training loss: 310.552978515625 = 0.01573758013546467 + 50.0 * 6.210744857788086
Epoch 2440, val loss: 1.8346377611160278
Epoch 2450, training loss: 310.2929382324219 = 0.015522419475018978 + 50.0 * 6.205548286437988
Epoch 2450, val loss: 1.8381255865097046
Epoch 2460, training loss: 310.42071533203125 = 0.015322760678827763 + 50.0 * 6.208107948303223
Epoch 2460, val loss: 1.8418047428131104
Epoch 2470, training loss: 310.276611328125 = 0.015125810168683529 + 50.0 * 6.20522928237915
Epoch 2470, val loss: 1.845117449760437
Epoch 2480, training loss: 310.2293701171875 = 0.014935165643692017 + 50.0 * 6.204288959503174
Epoch 2480, val loss: 1.8487333059310913
Epoch 2490, training loss: 310.23699951171875 = 0.01474857795983553 + 50.0 * 6.2044453620910645
Epoch 2490, val loss: 1.8522461652755737
Epoch 2500, training loss: 310.419677734375 = 0.014569458551704884 + 50.0 * 6.208102703094482
Epoch 2500, val loss: 1.8557002544403076
Epoch 2510, training loss: 310.2723693847656 = 0.014379492960870266 + 50.0 * 6.205159664154053
Epoch 2510, val loss: 1.858741044998169
Epoch 2520, training loss: 310.2502136230469 = 0.014198961667716503 + 50.0 * 6.204720497131348
Epoch 2520, val loss: 1.862122893333435
Epoch 2530, training loss: 310.3414001464844 = 0.014022334478795528 + 50.0 * 6.206547737121582
Epoch 2530, val loss: 1.8655552864074707
Epoch 2540, training loss: 310.2445983886719 = 0.013853976503014565 + 50.0 * 6.204615116119385
Epoch 2540, val loss: 1.8686212301254272
Epoch 2550, training loss: 310.26983642578125 = 0.013682960532605648 + 50.0 * 6.205123424530029
Epoch 2550, val loss: 1.872119665145874
Epoch 2560, training loss: 310.3254699707031 = 0.013515543192625046 + 50.0 * 6.206239223480225
Epoch 2560, val loss: 1.8753210306167603
Epoch 2570, training loss: 310.182861328125 = 0.013355094008147717 + 50.0 * 6.203390121459961
Epoch 2570, val loss: 1.8786054849624634
Epoch 2580, training loss: 310.146484375 = 0.013197413645684719 + 50.0 * 6.202666282653809
Epoch 2580, val loss: 1.8818422555923462
Epoch 2590, training loss: 310.1727294921875 = 0.013045546598732471 + 50.0 * 6.2031941413879395
Epoch 2590, val loss: 1.885094404220581
Epoch 2600, training loss: 310.25579833984375 = 0.01289467141032219 + 50.0 * 6.20485782623291
Epoch 2600, val loss: 1.8882668018341064
Epoch 2610, training loss: 310.21783447265625 = 0.012740208767354488 + 50.0 * 6.204102039337158
Epoch 2610, val loss: 1.8913347721099854
Epoch 2620, training loss: 310.2186584472656 = 0.012591316364705563 + 50.0 * 6.204121112823486
Epoch 2620, val loss: 1.8942904472351074
Epoch 2630, training loss: 310.09912109375 = 0.012445162050426006 + 50.0 * 6.201733112335205
Epoch 2630, val loss: 1.8974283933639526
Epoch 2640, training loss: 310.20220947265625 = 0.012305773794651031 + 50.0 * 6.203798294067383
Epoch 2640, val loss: 1.9006162881851196
Epoch 2650, training loss: 310.2537841796875 = 0.012164318934082985 + 50.0 * 6.204832077026367
Epoch 2650, val loss: 1.903452754020691
Epoch 2660, training loss: 310.13812255859375 = 0.012025601230561733 + 50.0 * 6.202521800994873
Epoch 2660, val loss: 1.9064764976501465
Epoch 2670, training loss: 310.159423828125 = 0.011890909634530544 + 50.0 * 6.202950477600098
Epoch 2670, val loss: 1.9092886447906494
Epoch 2680, training loss: 310.21746826171875 = 0.011757883243262768 + 50.0 * 6.2041144371032715
Epoch 2680, val loss: 1.912271499633789
Epoch 2690, training loss: 310.4022521972656 = 0.011624777689576149 + 50.0 * 6.207812309265137
Epoch 2690, val loss: 1.9154902696609497
Epoch 2700, training loss: 310.0585021972656 = 0.011496617458760738 + 50.0 * 6.200939655303955
Epoch 2700, val loss: 1.917985200881958
Epoch 2710, training loss: 310.0426330566406 = 0.011368915438652039 + 50.0 * 6.200624942779541
Epoch 2710, val loss: 1.9212530851364136
Epoch 2720, training loss: 310.0079650878906 = 0.011246883310377598 + 50.0 * 6.199934005737305
Epoch 2720, val loss: 1.9240754842758179
Epoch 2730, training loss: 310.0111999511719 = 0.011129447259008884 + 50.0 * 6.200001239776611
Epoch 2730, val loss: 1.9270087480545044
Epoch 2740, training loss: 310.2210998535156 = 0.011016017757356167 + 50.0 * 6.204201698303223
Epoch 2740, val loss: 1.9299371242523193
Epoch 2750, training loss: 310.1008605957031 = 0.010896941646933556 + 50.0 * 6.201799392700195
Epoch 2750, val loss: 1.9325369596481323
Epoch 2760, training loss: 310.04052734375 = 0.010777093470096588 + 50.0 * 6.200594902038574
Epoch 2760, val loss: 1.935241937637329
Epoch 2770, training loss: 309.9883117675781 = 0.010661769658327103 + 50.0 * 6.199553489685059
Epoch 2770, val loss: 1.9380266666412354
Epoch 2780, training loss: 309.9726257324219 = 0.010552598163485527 + 50.0 * 6.1992411613464355
Epoch 2780, val loss: 1.9408533573150635
Epoch 2790, training loss: 310.0772705078125 = 0.010445721447467804 + 50.0 * 6.201336860656738
Epoch 2790, val loss: 1.9435750246047974
Epoch 2800, training loss: 310.1738586425781 = 0.010337494313716888 + 50.0 * 6.203270435333252
Epoch 2800, val loss: 1.9460123777389526
Epoch 2810, training loss: 310.2239685058594 = 0.01023118942975998 + 50.0 * 6.204274654388428
Epoch 2810, val loss: 1.9485594034194946
Epoch 2820, training loss: 310.0136413574219 = 0.010123060084879398 + 50.0 * 6.200069904327393
Epoch 2820, val loss: 1.9514198303222656
Epoch 2830, training loss: 309.9532775878906 = 0.0100242393091321 + 50.0 * 6.1988654136657715
Epoch 2830, val loss: 1.9540742635726929
Epoch 2840, training loss: 309.9610900878906 = 0.009925718419253826 + 50.0 * 6.199023723602295
Epoch 2840, val loss: 1.956827163696289
Epoch 2850, training loss: 310.13055419921875 = 0.009832754731178284 + 50.0 * 6.202414512634277
Epoch 2850, val loss: 1.9595307111740112
Epoch 2860, training loss: 310.07763671875 = 0.009730597957968712 + 50.0 * 6.201358318328857
Epoch 2860, val loss: 1.9617838859558105
Epoch 2870, training loss: 309.9681396484375 = 0.009629507549107075 + 50.0 * 6.199169635772705
Epoch 2870, val loss: 1.9642053842544556
Epoch 2880, training loss: 310.0161437988281 = 0.009534232318401337 + 50.0 * 6.200132369995117
Epoch 2880, val loss: 1.9668223857879639
Epoch 2890, training loss: 310.10955810546875 = 0.009441077709197998 + 50.0 * 6.20200252532959
Epoch 2890, val loss: 1.9694269895553589
Epoch 2900, training loss: 310.0096740722656 = 0.009350120089948177 + 50.0 * 6.20000696182251
Epoch 2900, val loss: 1.9718436002731323
Epoch 2910, training loss: 309.9892883300781 = 0.009261456318199635 + 50.0 * 6.199600696563721
Epoch 2910, val loss: 1.9742850065231323
Epoch 2920, training loss: 309.87139892578125 = 0.00917037669569254 + 50.0 * 6.197244644165039
Epoch 2920, val loss: 1.9766830205917358
Epoch 2930, training loss: 309.881591796875 = 0.009084976278245449 + 50.0 * 6.197450160980225
Epoch 2930, val loss: 1.979170560836792
Epoch 2940, training loss: 310.00146484375 = 0.00900124479085207 + 50.0 * 6.1998491287231445
Epoch 2940, val loss: 1.981516718864441
Epoch 2950, training loss: 310.19964599609375 = 0.008916025049984455 + 50.0 * 6.203814506530762
Epoch 2950, val loss: 1.98391592502594
Epoch 2960, training loss: 309.9600830078125 = 0.008832314051687717 + 50.0 * 6.1990251541137695
Epoch 2960, val loss: 1.9860903024673462
Epoch 2970, training loss: 309.8851623535156 = 0.008746566250920296 + 50.0 * 6.197527885437012
Epoch 2970, val loss: 1.9883806705474854
Epoch 2980, training loss: 309.8636779785156 = 0.0086680818349123 + 50.0 * 6.1971001625061035
Epoch 2980, val loss: 1.9910002946853638
Epoch 2990, training loss: 310.0163879394531 = 0.008590402081608772 + 50.0 * 6.200156211853027
Epoch 2990, val loss: 1.99351966381073
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8371112282551397
=== training gcn model ===
Epoch 0, training loss: 431.776611328125 = 1.934149980545044 + 50.0 * 8.59684944152832
Epoch 0, val loss: 1.929504632949829
Epoch 10, training loss: 431.72906494140625 = 1.9256079196929932 + 50.0 * 8.5960693359375
Epoch 10, val loss: 1.9211488962173462
Epoch 20, training loss: 431.4414367675781 = 1.9149280786514282 + 50.0 * 8.590530395507812
Epoch 20, val loss: 1.910279393196106
Epoch 30, training loss: 429.5029602050781 = 1.901260495185852 + 50.0 * 8.552033424377441
Epoch 30, val loss: 1.8961126804351807
Epoch 40, training loss: 416.6828308105469 = 1.8846861124038696 + 50.0 * 8.295963287353516
Epoch 40, val loss: 1.8794692754745483
Epoch 50, training loss: 376.77789306640625 = 1.8659179210662842 + 50.0 * 7.498239994049072
Epoch 50, val loss: 1.8608523607254028
Epoch 60, training loss: 362.84967041015625 = 1.8524844646453857 + 50.0 * 7.219944000244141
Epoch 60, val loss: 1.8484910726547241
Epoch 70, training loss: 355.1785888671875 = 1.8419772386550903 + 50.0 * 7.066731929779053
Epoch 70, val loss: 1.8381390571594238
Epoch 80, training loss: 349.59674072265625 = 1.831096887588501 + 50.0 * 6.955313205718994
Epoch 80, val loss: 1.8278303146362305
Epoch 90, training loss: 345.2196044921875 = 1.8218740224838257 + 50.0 * 6.867954730987549
Epoch 90, val loss: 1.8194024562835693
Epoch 100, training loss: 341.07586669921875 = 1.8139511346817017 + 50.0 * 6.785238265991211
Epoch 100, val loss: 1.8124399185180664
Epoch 110, training loss: 337.14764404296875 = 1.8070847988128662 + 50.0 * 6.70681095123291
Epoch 110, val loss: 1.8062717914581299
Epoch 120, training loss: 334.4862365722656 = 1.8003824949264526 + 50.0 * 6.653717041015625
Epoch 120, val loss: 1.8000370264053345
Epoch 130, training loss: 332.58819580078125 = 1.7931047677993774 + 50.0 * 6.615901947021484
Epoch 130, val loss: 1.7931641340255737
Epoch 140, training loss: 331.1340637207031 = 1.7853816747665405 + 50.0 * 6.586973667144775
Epoch 140, val loss: 1.7859132289886475
Epoch 150, training loss: 329.845947265625 = 1.7771278619766235 + 50.0 * 6.561376094818115
Epoch 150, val loss: 1.778291940689087
Epoch 160, training loss: 328.744140625 = 1.7684190273284912 + 50.0 * 6.539514064788818
Epoch 160, val loss: 1.7701916694641113
Epoch 170, training loss: 327.6695861816406 = 1.7591569423675537 + 50.0 * 6.5182085037231445
Epoch 170, val loss: 1.7616482973098755
Epoch 180, training loss: 326.78033447265625 = 1.7491179704666138 + 50.0 * 6.500624656677246
Epoch 180, val loss: 1.7524088621139526
Epoch 190, training loss: 325.9894104003906 = 1.7382062673568726 + 50.0 * 6.4850239753723145
Epoch 190, val loss: 1.7424036264419556
Epoch 200, training loss: 325.148193359375 = 1.7262879610061646 + 50.0 * 6.468438148498535
Epoch 200, val loss: 1.731585144996643
Epoch 210, training loss: 324.4623107910156 = 1.7134610414505005 + 50.0 * 6.454977035522461
Epoch 210, val loss: 1.719819188117981
Epoch 220, training loss: 323.9493408203125 = 1.6993893384933472 + 50.0 * 6.444998741149902
Epoch 220, val loss: 1.706978440284729
Epoch 230, training loss: 323.38470458984375 = 1.6839704513549805 + 50.0 * 6.434014797210693
Epoch 230, val loss: 1.6930230855941772
Epoch 240, training loss: 322.9333190917969 = 1.667310118675232 + 50.0 * 6.425320148468018
Epoch 240, val loss: 1.6779476404190063
Epoch 250, training loss: 322.7460632324219 = 1.649284839630127 + 50.0 * 6.421935558319092
Epoch 250, val loss: 1.661830186843872
Epoch 260, training loss: 322.2164611816406 = 1.6299262046813965 + 50.0 * 6.411731243133545
Epoch 260, val loss: 1.644470453262329
Epoch 270, training loss: 321.7911071777344 = 1.6093271970748901 + 50.0 * 6.4036359786987305
Epoch 270, val loss: 1.6260014772415161
Epoch 280, training loss: 321.4123229980469 = 1.5875345468521118 + 50.0 * 6.396495819091797
Epoch 280, val loss: 1.6065946817398071
Epoch 290, training loss: 321.1426696777344 = 1.5645371675491333 + 50.0 * 6.391562461853027
Epoch 290, val loss: 1.5862232446670532
Epoch 300, training loss: 320.9283752441406 = 1.5402872562408447 + 50.0 * 6.38776159286499
Epoch 300, val loss: 1.5648858547210693
Epoch 310, training loss: 320.4679260253906 = 1.5148191452026367 + 50.0 * 6.379061698913574
Epoch 310, val loss: 1.5426061153411865
Epoch 320, training loss: 320.1435852050781 = 1.488625407218933 + 50.0 * 6.373099327087402
Epoch 320, val loss: 1.519749402999878
Epoch 330, training loss: 319.89141845703125 = 1.4615671634674072 + 50.0 * 6.36859655380249
Epoch 330, val loss: 1.4962975978851318
Epoch 340, training loss: 319.5677795410156 = 1.433468222618103 + 50.0 * 6.3626861572265625
Epoch 340, val loss: 1.472151279449463
Epoch 350, training loss: 319.4872131347656 = 1.4050397872924805 + 50.0 * 6.3616437911987305
Epoch 350, val loss: 1.4477574825286865
Epoch 360, training loss: 319.10565185546875 = 1.3756877183914185 + 50.0 * 6.3545989990234375
Epoch 360, val loss: 1.4230083227157593
Epoch 370, training loss: 318.8265686035156 = 1.346282720565796 + 50.0 * 6.349605560302734
Epoch 370, val loss: 1.398163080215454
Epoch 380, training loss: 318.572509765625 = 1.3166276216506958 + 50.0 * 6.345117568969727
Epoch 380, val loss: 1.3734172582626343
Epoch 390, training loss: 318.3944091796875 = 1.2868483066558838 + 50.0 * 6.342151641845703
Epoch 390, val loss: 1.348753809928894
Epoch 400, training loss: 318.3162841796875 = 1.2569115161895752 + 50.0 * 6.341187953948975
Epoch 400, val loss: 1.3239225149154663
Epoch 410, training loss: 318.0437927246094 = 1.2267978191375732 + 50.0 * 6.336339950561523
Epoch 410, val loss: 1.2994096279144287
Epoch 420, training loss: 317.8088073730469 = 1.197028398513794 + 50.0 * 6.332235813140869
Epoch 420, val loss: 1.275333285331726
Epoch 430, training loss: 317.7396545410156 = 1.1675480604171753 + 50.0 * 6.331442356109619
Epoch 430, val loss: 1.2517905235290527
Epoch 440, training loss: 317.56085205078125 = 1.1384567022323608 + 50.0 * 6.3284478187561035
Epoch 440, val loss: 1.2285146713256836
Epoch 450, training loss: 317.28839111328125 = 1.1096243858337402 + 50.0 * 6.323575496673584
Epoch 450, val loss: 1.2059394121170044
Epoch 460, training loss: 317.1253356933594 = 1.0813707113265991 + 50.0 * 6.3208794593811035
Epoch 460, val loss: 1.184065580368042
Epoch 470, training loss: 317.21282958984375 = 1.053528904914856 + 50.0 * 6.323185920715332
Epoch 470, val loss: 1.1627546548843384
Epoch 480, training loss: 316.8949890136719 = 1.026335597038269 + 50.0 * 6.317372798919678
Epoch 480, val loss: 1.141933560371399
Epoch 490, training loss: 316.7523498535156 = 0.9996976256370544 + 50.0 * 6.3150529861450195
Epoch 490, val loss: 1.1221950054168701
Epoch 500, training loss: 316.5772705078125 = 0.9735661149024963 + 50.0 * 6.312074184417725
Epoch 500, val loss: 1.1027811765670776
Epoch 510, training loss: 316.4252014160156 = 0.9481385350227356 + 50.0 * 6.30954122543335
Epoch 510, val loss: 1.0843815803527832
Epoch 520, training loss: 316.50079345703125 = 0.9232519865036011 + 50.0 * 6.311550617218018
Epoch 520, val loss: 1.066514492034912
Epoch 530, training loss: 316.2264099121094 = 0.8992272615432739 + 50.0 * 6.306543827056885
Epoch 530, val loss: 1.0495481491088867
Epoch 540, training loss: 316.0387878417969 = 0.8755155801773071 + 50.0 * 6.303265571594238
Epoch 540, val loss: 1.0331653356552124
Epoch 550, training loss: 315.92889404296875 = 0.8525941967964172 + 50.0 * 6.301525592803955
Epoch 550, val loss: 1.0177100896835327
Epoch 560, training loss: 315.9672546386719 = 0.8302543759346008 + 50.0 * 6.302740097045898
Epoch 560, val loss: 1.002655029296875
Epoch 570, training loss: 315.76397705078125 = 0.8084316849708557 + 50.0 * 6.2991108894348145
Epoch 570, val loss: 0.9885198473930359
Epoch 580, training loss: 315.5935363769531 = 0.7871224880218506 + 50.0 * 6.296128273010254
Epoch 580, val loss: 0.9748744368553162
Epoch 590, training loss: 315.6257629394531 = 0.7665938138961792 + 50.0 * 6.297183513641357
Epoch 590, val loss: 0.9621280431747437
Epoch 600, training loss: 315.4371643066406 = 0.7463303208351135 + 50.0 * 6.293816566467285
Epoch 600, val loss: 0.9495174884796143
Epoch 610, training loss: 315.30157470703125 = 0.7267839908599854 + 50.0 * 6.2914958000183105
Epoch 610, val loss: 0.9380267262458801
Epoch 620, training loss: 315.1734619140625 = 0.7077853083610535 + 50.0 * 6.289313316345215
Epoch 620, val loss: 0.9269627928733826
Epoch 630, training loss: 315.09307861328125 = 0.6893526315689087 + 50.0 * 6.288074493408203
Epoch 630, val loss: 0.9167142510414124
Epoch 640, training loss: 315.0371398925781 = 0.671289324760437 + 50.0 * 6.287316799163818
Epoch 640, val loss: 0.906833291053772
Epoch 650, training loss: 314.87445068359375 = 0.653701901435852 + 50.0 * 6.284414768218994
Epoch 650, val loss: 0.8975217938423157
Epoch 660, training loss: 314.7969055175781 = 0.6366833448410034 + 50.0 * 6.283204555511475
Epoch 660, val loss: 0.8887826800346375
Epoch 670, training loss: 315.1884460449219 = 0.6203485727310181 + 50.0 * 6.2913618087768555
Epoch 670, val loss: 0.8806915283203125
Epoch 680, training loss: 314.7781677246094 = 0.6037799715995789 + 50.0 * 6.283487796783447
Epoch 680, val loss: 0.8727553486824036
Epoch 690, training loss: 314.5608825683594 = 0.5881363153457642 + 50.0 * 6.279455184936523
Epoch 690, val loss: 0.8654652237892151
Epoch 700, training loss: 314.42755126953125 = 0.5729507803916931 + 50.0 * 6.277092456817627
Epoch 700, val loss: 0.8588123321533203
Epoch 710, training loss: 314.45977783203125 = 0.5582007169723511 + 50.0 * 6.278031826019287
Epoch 710, val loss: 0.8524539470672607
Epoch 720, training loss: 314.4087219238281 = 0.5435287356376648 + 50.0 * 6.277304172515869
Epoch 720, val loss: 0.8467322587966919
Epoch 730, training loss: 314.232421875 = 0.5294577479362488 + 50.0 * 6.274059295654297
Epoch 730, val loss: 0.8410009741783142
Epoch 740, training loss: 314.1397399902344 = 0.5157781839370728 + 50.0 * 6.27247953414917
Epoch 740, val loss: 0.8359196186065674
Epoch 750, training loss: 314.1257629394531 = 0.5024442672729492 + 50.0 * 6.272466659545898
Epoch 750, val loss: 0.8313751816749573
Epoch 760, training loss: 314.0067138671875 = 0.4893247187137604 + 50.0 * 6.270348072052002
Epoch 760, val loss: 0.8270252346992493
Epoch 770, training loss: 313.9857482910156 = 0.4765995144844055 + 50.0 * 6.270183086395264
Epoch 770, val loss: 0.8229859471321106
Epoch 780, training loss: 313.9729919433594 = 0.4643266499042511 + 50.0 * 6.270173072814941
Epoch 780, val loss: 0.8194270133972168
Epoch 790, training loss: 313.8194885253906 = 0.452253520488739 + 50.0 * 6.267344951629639
Epoch 790, val loss: 0.816066324710846
Epoch 800, training loss: 313.781494140625 = 0.44052743911743164 + 50.0 * 6.266819477081299
Epoch 800, val loss: 0.8130787014961243
Epoch 810, training loss: 313.6868896484375 = 0.4292650818824768 + 50.0 * 6.265152454376221
Epoch 810, val loss: 0.8105313777923584
Epoch 820, training loss: 313.60565185546875 = 0.41824811697006226 + 50.0 * 6.2637481689453125
Epoch 820, val loss: 0.8083215951919556
Epoch 830, training loss: 313.6346435546875 = 0.40757307410240173 + 50.0 * 6.2645416259765625
Epoch 830, val loss: 0.8063254356384277
Epoch 840, training loss: 313.593505859375 = 0.39703112840652466 + 50.0 * 6.26392936706543
Epoch 840, val loss: 0.8043771982192993
Epoch 850, training loss: 313.5509338378906 = 0.3868083357810974 + 50.0 * 6.263282775878906
Epoch 850, val loss: 0.8028066158294678
Epoch 860, training loss: 313.44842529296875 = 0.37692537903785706 + 50.0 * 6.261430263519287
Epoch 860, val loss: 0.8015168309211731
Epoch 870, training loss: 313.4839172363281 = 0.36727768182754517 + 50.0 * 6.262332916259766
Epoch 870, val loss: 0.8004834055900574
Epoch 880, training loss: 313.3053894042969 = 0.3577737808227539 + 50.0 * 6.2589521408081055
Epoch 880, val loss: 0.799640417098999
Epoch 890, training loss: 313.30029296875 = 0.3486292064189911 + 50.0 * 6.259033203125
Epoch 890, val loss: 0.7989957332611084
Epoch 900, training loss: 313.2281494140625 = 0.33974629640579224 + 50.0 * 6.257768154144287
Epoch 900, val loss: 0.7985619306564331
Epoch 910, training loss: 313.5482177734375 = 0.3310548663139343 + 50.0 * 6.26434326171875
Epoch 910, val loss: 0.7982978820800781
Epoch 920, training loss: 313.1278991699219 = 0.32259559631347656 + 50.0 * 6.256105899810791
Epoch 920, val loss: 0.7981322407722473
Epoch 930, training loss: 313.0972900390625 = 0.3144286572933197 + 50.0 * 6.255657196044922
Epoch 930, val loss: 0.7982950210571289
Epoch 940, training loss: 313.12554931640625 = 0.3064928650856018 + 50.0 * 6.256381511688232
Epoch 940, val loss: 0.7985211610794067
Epoch 950, training loss: 312.98944091796875 = 0.2987772822380066 + 50.0 * 6.253813743591309
Epoch 950, val loss: 0.7991700172424316
Epoch 960, training loss: 313.1761779785156 = 0.2913256883621216 + 50.0 * 6.257697105407715
Epoch 960, val loss: 0.799881100654602
Epoch 970, training loss: 313.05914306640625 = 0.28376272320747375 + 50.0 * 6.255507946014404
Epoch 970, val loss: 0.8003048300743103
Epoch 980, training loss: 312.875732421875 = 0.2766166925430298 + 50.0 * 6.251982688903809
Epoch 980, val loss: 0.8010771870613098
Epoch 990, training loss: 312.8323974609375 = 0.26971298456192017 + 50.0 * 6.251253604888916
Epoch 990, val loss: 0.8023938536643982
Epoch 1000, training loss: 312.7626037597656 = 0.2630418837070465 + 50.0 * 6.249991416931152
Epoch 1000, val loss: 0.8036395907402039
Epoch 1010, training loss: 312.75494384765625 = 0.2565470337867737 + 50.0 * 6.249967575073242
Epoch 1010, val loss: 0.8051007986068726
Epoch 1020, training loss: 313.000732421875 = 0.2501406669616699 + 50.0 * 6.255011558532715
Epoch 1020, val loss: 0.8064880967140198
Epoch 1030, training loss: 312.7325439453125 = 0.24391402304172516 + 50.0 * 6.249772071838379
Epoch 1030, val loss: 0.8079971075057983
Epoch 1040, training loss: 312.7452697753906 = 0.23784929513931274 + 50.0 * 6.250148296356201
Epoch 1040, val loss: 0.8097884654998779
Epoch 1050, training loss: 312.6249084472656 = 0.23201723396778107 + 50.0 * 6.247857570648193
Epoch 1050, val loss: 0.8116053938865662
Epoch 1060, training loss: 312.54437255859375 = 0.22636090219020844 + 50.0 * 6.2463603019714355
Epoch 1060, val loss: 0.8137214183807373
Epoch 1070, training loss: 312.51165771484375 = 0.2208947092294693 + 50.0 * 6.245815277099609
Epoch 1070, val loss: 0.8158185482025146
Epoch 1080, training loss: 313.2581481933594 = 0.21562227606773376 + 50.0 * 6.260850429534912
Epoch 1080, val loss: 0.8180522322654724
Epoch 1090, training loss: 312.51605224609375 = 0.21010887622833252 + 50.0 * 6.246119022369385
Epoch 1090, val loss: 0.819931149482727
Epoch 1100, training loss: 312.4892272949219 = 0.20499154925346375 + 50.0 * 6.245684623718262
Epoch 1100, val loss: 0.8224340081214905
Epoch 1110, training loss: 312.3671875 = 0.20009976625442505 + 50.0 * 6.243341445922852
Epoch 1110, val loss: 0.8249272108078003
Epoch 1120, training loss: 312.3643798828125 = 0.19535492360591888 + 50.0 * 6.243380546569824
Epoch 1120, val loss: 0.8275304436683655
Epoch 1130, training loss: 312.65948486328125 = 0.19069118797779083 + 50.0 * 6.249375820159912
Epoch 1130, val loss: 0.8300974369049072
Epoch 1140, training loss: 312.2921447753906 = 0.1860874444246292 + 50.0 * 6.24212121963501
Epoch 1140, val loss: 0.8329417705535889
Epoch 1150, training loss: 312.27325439453125 = 0.18167424201965332 + 50.0 * 6.2418317794799805
Epoch 1150, val loss: 0.8359060287475586
Epoch 1160, training loss: 312.5140075683594 = 0.1774415522813797 + 50.0 * 6.246731281280518
Epoch 1160, val loss: 0.8389661312103271
Epoch 1170, training loss: 312.376220703125 = 0.1731017827987671 + 50.0 * 6.244062423706055
Epoch 1170, val loss: 0.8415317535400391
Epoch 1180, training loss: 312.2557067871094 = 0.16902245581150055 + 50.0 * 6.241733551025391
Epoch 1180, val loss: 0.844714343547821
Epoch 1190, training loss: 312.1355895996094 = 0.16506552696228027 + 50.0 * 6.239410400390625
Epoch 1190, val loss: 0.8479335308074951
Epoch 1200, training loss: 312.1117858886719 = 0.16126292943954468 + 50.0 * 6.239010334014893
Epoch 1200, val loss: 0.8512532114982605
Epoch 1210, training loss: 312.4330139160156 = 0.15753521025180817 + 50.0 * 6.245509624481201
Epoch 1210, val loss: 0.8546054363250732
Epoch 1220, training loss: 312.2071838378906 = 0.1538793295621872 + 50.0 * 6.2410664558410645
Epoch 1220, val loss: 0.8578580617904663
Epoch 1230, training loss: 312.068603515625 = 0.15029601752758026 + 50.0 * 6.23836612701416
Epoch 1230, val loss: 0.8613128066062927
Epoch 1240, training loss: 312.0577392578125 = 0.14686547219753265 + 50.0 * 6.238217830657959
Epoch 1240, val loss: 0.8649174571037292
Epoch 1250, training loss: 312.1181335449219 = 0.14350613951683044 + 50.0 * 6.239492893218994
Epoch 1250, val loss: 0.8685402870178223
Epoch 1260, training loss: 311.9273986816406 = 0.1402440220117569 + 50.0 * 6.235742568969727
Epoch 1260, val loss: 0.8722002506256104
Epoch 1270, training loss: 311.9905090332031 = 0.137115016579628 + 50.0 * 6.237068176269531
Epoch 1270, val loss: 0.8760285973548889
Epoch 1280, training loss: 312.0172119140625 = 0.13403058052062988 + 50.0 * 6.237663745880127
Epoch 1280, val loss: 0.8799146413803101
Epoch 1290, training loss: 312.06414794921875 = 0.13098233938217163 + 50.0 * 6.238663196563721
Epoch 1290, val loss: 0.8837206363677979
Epoch 1300, training loss: 312.0624694824219 = 0.12799520790576935 + 50.0 * 6.238689422607422
Epoch 1300, val loss: 0.8876162171363831
Epoch 1310, training loss: 311.8895263671875 = 0.12512779235839844 + 50.0 * 6.235288143157959
Epoch 1310, val loss: 0.8913530111312866
Epoch 1320, training loss: 311.80133056640625 = 0.12236709892749786 + 50.0 * 6.233579158782959
Epoch 1320, val loss: 0.8956699967384338
Epoch 1330, training loss: 311.76177978515625 = 0.11970485001802444 + 50.0 * 6.232841491699219
Epoch 1330, val loss: 0.899882435798645
Epoch 1340, training loss: 311.9671630859375 = 0.11710643023252487 + 50.0 * 6.237001419067383
Epoch 1340, val loss: 0.9041590690612793
Epoch 1350, training loss: 311.8520202636719 = 0.11448037624359131 + 50.0 * 6.234751224517822
Epoch 1350, val loss: 0.9080137014389038
Epoch 1360, training loss: 311.8074645996094 = 0.11194293200969696 + 50.0 * 6.23391056060791
Epoch 1360, val loss: 0.912243127822876
Epoch 1370, training loss: 311.65618896484375 = 0.10952868312597275 + 50.0 * 6.23093318939209
Epoch 1370, val loss: 0.9166253209114075
Epoch 1380, training loss: 311.6434020996094 = 0.1072213426232338 + 50.0 * 6.230723857879639
Epoch 1380, val loss: 0.9211454391479492
Epoch 1390, training loss: 311.6466369628906 = 0.10496587306261063 + 50.0 * 6.230833053588867
Epoch 1390, val loss: 0.9256762266159058
Epoch 1400, training loss: 311.9458923339844 = 0.10273822396993637 + 50.0 * 6.236862659454346
Epoch 1400, val loss: 0.930026650428772
Epoch 1410, training loss: 311.7722473144531 = 0.10052478313446045 + 50.0 * 6.233434200286865
Epoch 1410, val loss: 0.934476912021637
Epoch 1420, training loss: 311.59625244140625 = 0.09841061383485794 + 50.0 * 6.22995662689209
Epoch 1420, val loss: 0.9390047192573547
Epoch 1430, training loss: 311.67633056640625 = 0.09636920690536499 + 50.0 * 6.231598854064941
Epoch 1430, val loss: 0.9435266852378845
Epoch 1440, training loss: 311.6345520019531 = 0.09437379240989685 + 50.0 * 6.230803966522217
Epoch 1440, val loss: 0.9482459425926208
Epoch 1450, training loss: 311.52862548828125 = 0.09241120517253876 + 50.0 * 6.228724479675293
Epoch 1450, val loss: 0.9528911709785461
Epoch 1460, training loss: 311.5486145019531 = 0.09052703529596329 + 50.0 * 6.229162216186523
Epoch 1460, val loss: 0.9576892852783203
Epoch 1470, training loss: 311.605712890625 = 0.08868817985057831 + 50.0 * 6.230340480804443
Epoch 1470, val loss: 0.9623968005180359
Epoch 1480, training loss: 311.51153564453125 = 0.08685740828514099 + 50.0 * 6.228493690490723
Epoch 1480, val loss: 0.9669899344444275
Epoch 1490, training loss: 311.5582580566406 = 0.08511198312044144 + 50.0 * 6.22946310043335
Epoch 1490, val loss: 0.9717819690704346
Epoch 1500, training loss: 311.41156005859375 = 0.08336983621120453 + 50.0 * 6.226563930511475
Epoch 1500, val loss: 0.9765982031822205
Epoch 1510, training loss: 311.36541748046875 = 0.08171285688877106 + 50.0 * 6.225673675537109
Epoch 1510, val loss: 0.9815000295639038
Epoch 1520, training loss: 311.37481689453125 = 0.08011656999588013 + 50.0 * 6.225894451141357
Epoch 1520, val loss: 0.9865952134132385
Epoch 1530, training loss: 311.79296875 = 0.07857093214988708 + 50.0 * 6.234287738800049
Epoch 1530, val loss: 0.9914112091064453
Epoch 1540, training loss: 311.4918518066406 = 0.07691642642021179 + 50.0 * 6.228299140930176
Epoch 1540, val loss: 0.9958455562591553
Epoch 1550, training loss: 311.3626403808594 = 0.07536111027002335 + 50.0 * 6.225746154785156
Epoch 1550, val loss: 1.0006977319717407
Epoch 1560, training loss: 311.2834167480469 = 0.0739196315407753 + 50.0 * 6.2241902351379395
Epoch 1560, val loss: 1.0057555437088013
Epoch 1570, training loss: 311.2588195800781 = 0.07251303642988205 + 50.0 * 6.223726272583008
Epoch 1570, val loss: 1.0108311176300049
Epoch 1580, training loss: 311.2502746582031 = 0.07114211469888687 + 50.0 * 6.223582744598389
Epoch 1580, val loss: 1.0159056186676025
Epoch 1590, training loss: 311.63238525390625 = 0.06982492655515671 + 50.0 * 6.231251239776611
Epoch 1590, val loss: 1.0209028720855713
Epoch 1600, training loss: 311.3292236328125 = 0.06840863823890686 + 50.0 * 6.225215911865234
Epoch 1600, val loss: 1.0255582332611084
Epoch 1610, training loss: 311.2517395019531 = 0.06710410863161087 + 50.0 * 6.223692417144775
Epoch 1610, val loss: 1.030584692955017
Epoch 1620, training loss: 311.5270080566406 = 0.06581737846136093 + 50.0 * 6.229223728179932
Epoch 1620, val loss: 1.0354809761047363
Epoch 1630, training loss: 311.2782287597656 = 0.06458280980587006 + 50.0 * 6.224273204803467
Epoch 1630, val loss: 1.0407832860946655
Epoch 1640, training loss: 311.18035888671875 = 0.0633573904633522 + 50.0 * 6.222340106964111
Epoch 1640, val loss: 1.0457262992858887
Epoch 1650, training loss: 311.2080383300781 = 0.06219696253538132 + 50.0 * 6.222917079925537
Epoch 1650, val loss: 1.050933599472046
Epoch 1660, training loss: 311.42889404296875 = 0.061023008078336716 + 50.0 * 6.227357387542725
Epoch 1660, val loss: 1.0558539628982544
Epoch 1670, training loss: 311.2192687988281 = 0.05989393591880798 + 50.0 * 6.223187446594238
Epoch 1670, val loss: 1.0608786344528198
Epoch 1680, training loss: 311.1108703613281 = 0.05877237021923065 + 50.0 * 6.221042156219482
Epoch 1680, val loss: 1.066023826599121
Epoch 1690, training loss: 311.13739013671875 = 0.05772225186228752 + 50.0 * 6.221593856811523
Epoch 1690, val loss: 1.071379542350769
Epoch 1700, training loss: 311.20654296875 = 0.05667167529463768 + 50.0 * 6.222997188568115
Epoch 1700, val loss: 1.0763612985610962
Epoch 1710, training loss: 311.0852966308594 = 0.05562285706400871 + 50.0 * 6.220593452453613
Epoch 1710, val loss: 1.0812907218933105
Epoch 1720, training loss: 311.0199890136719 = 0.05460486188530922 + 50.0 * 6.219307899475098
Epoch 1720, val loss: 1.0862951278686523
Epoch 1730, training loss: 311.1004943847656 = 0.05363935977220535 + 50.0 * 6.2209367752075195
Epoch 1730, val loss: 1.0914230346679688
Epoch 1740, training loss: 311.1073913574219 = 0.05266870930790901 + 50.0 * 6.221094131469727
Epoch 1740, val loss: 1.0964596271514893
Epoch 1750, training loss: 311.0339050292969 = 0.05173245817422867 + 50.0 * 6.219643592834473
Epoch 1750, val loss: 1.1016514301300049
Epoch 1760, training loss: 311.05926513671875 = 0.05080205947160721 + 50.0 * 6.2201690673828125
Epoch 1760, val loss: 1.1066739559173584
Epoch 1770, training loss: 311.1196594238281 = 0.04989947751164436 + 50.0 * 6.221395492553711
Epoch 1770, val loss: 1.1117241382598877
Epoch 1780, training loss: 311.00653076171875 = 0.049035198986530304 + 50.0 * 6.219149589538574
Epoch 1780, val loss: 1.116856575012207
Epoch 1790, training loss: 310.9118347167969 = 0.04817496985197067 + 50.0 * 6.217272758483887
Epoch 1790, val loss: 1.1220027208328247
Epoch 1800, training loss: 310.9788818359375 = 0.047352664172649384 + 50.0 * 6.218630313873291
Epoch 1800, val loss: 1.1271085739135742
Epoch 1810, training loss: 311.04241943359375 = 0.04654494300484657 + 50.0 * 6.2199177742004395
Epoch 1810, val loss: 1.1322444677352905
Epoch 1820, training loss: 311.03045654296875 = 0.045721638947725296 + 50.0 * 6.2196946144104
Epoch 1820, val loss: 1.1372052431106567
Epoch 1830, training loss: 310.9333190917969 = 0.04491624981164932 + 50.0 * 6.21776819229126
Epoch 1830, val loss: 1.1422967910766602
Epoch 1840, training loss: 310.8826904296875 = 0.04415503144264221 + 50.0 * 6.216770648956299
Epoch 1840, val loss: 1.147350788116455
Epoch 1850, training loss: 311.043701171875 = 0.04341624304652214 + 50.0 * 6.220005512237549
Epoch 1850, val loss: 1.1524474620819092
Epoch 1860, training loss: 310.9534912109375 = 0.042690664529800415 + 50.0 * 6.2182159423828125
Epoch 1860, val loss: 1.1577227115631104
Epoch 1870, training loss: 310.92327880859375 = 0.041957952082157135 + 50.0 * 6.217626094818115
Epoch 1870, val loss: 1.1626310348510742
Epoch 1880, training loss: 310.81707763671875 = 0.04124952107667923 + 50.0 * 6.215516567230225
Epoch 1880, val loss: 1.1678351163864136
Epoch 1890, training loss: 310.77142333984375 = 0.04056865721940994 + 50.0 * 6.2146172523498535
Epoch 1890, val loss: 1.1729347705841064
Epoch 1900, training loss: 311.0035400390625 = 0.03991464897990227 + 50.0 * 6.219272613525391
Epoch 1900, val loss: 1.177927851676941
Epoch 1910, training loss: 310.9207763671875 = 0.03923950716853142 + 50.0 * 6.217630386352539
Epoch 1910, val loss: 1.1827861070632935
Epoch 1920, training loss: 310.8497009277344 = 0.038562145084142685 + 50.0 * 6.216222763061523
Epoch 1920, val loss: 1.1876163482666016
Epoch 1930, training loss: 310.72088623046875 = 0.03793540969491005 + 50.0 * 6.213659286499023
Epoch 1930, val loss: 1.1926872730255127
Epoch 1940, training loss: 310.73358154296875 = 0.0373355858027935 + 50.0 * 6.213925361633301
Epoch 1940, val loss: 1.1978291273117065
Epoch 1950, training loss: 311.2464599609375 = 0.03675316274166107 + 50.0 * 6.224194526672363
Epoch 1950, val loss: 1.2029186487197876
Epoch 1960, training loss: 311.04693603515625 = 0.0361354798078537 + 50.0 * 6.220216274261475
Epoch 1960, val loss: 1.2073434591293335
Epoch 1970, training loss: 310.7294616699219 = 0.03552515059709549 + 50.0 * 6.213878631591797
Epoch 1970, val loss: 1.2123256921768188
Epoch 1980, training loss: 310.68023681640625 = 0.03496032953262329 + 50.0 * 6.212905406951904
Epoch 1980, val loss: 1.2173408269882202
Epoch 1990, training loss: 310.74090576171875 = 0.03441835939884186 + 50.0 * 6.214129447937012
Epoch 1990, val loss: 1.2222797870635986
Epoch 2000, training loss: 310.839111328125 = 0.03387518972158432 + 50.0 * 6.216104984283447
Epoch 2000, val loss: 1.2271558046340942
Epoch 2010, training loss: 310.6691589355469 = 0.03334469720721245 + 50.0 * 6.212716102600098
Epoch 2010, val loss: 1.231991171836853
Epoch 2020, training loss: 310.7535095214844 = 0.03282937780022621 + 50.0 * 6.214413166046143
Epoch 2020, val loss: 1.236832857131958
Epoch 2030, training loss: 310.6708068847656 = 0.032313231378793716 + 50.0 * 6.212769508361816
Epoch 2030, val loss: 1.2418092489242554
Epoch 2040, training loss: 310.7464904785156 = 0.03181447833776474 + 50.0 * 6.214293956756592
Epoch 2040, val loss: 1.2466025352478027
Epoch 2050, training loss: 310.9071350097656 = 0.031318847090005875 + 50.0 * 6.2175164222717285
Epoch 2050, val loss: 1.2510602474212646
Epoch 2060, training loss: 310.7362365722656 = 0.030824029818177223 + 50.0 * 6.214107990264893
Epoch 2060, val loss: 1.2562335729599
Epoch 2070, training loss: 310.6183166503906 = 0.030339673161506653 + 50.0 * 6.211759567260742
Epoch 2070, val loss: 1.2608578205108643
Epoch 2080, training loss: 310.55914306640625 = 0.02988981455564499 + 50.0 * 6.210585594177246
Epoch 2080, val loss: 1.2657891511917114
Epoch 2090, training loss: 310.5513610839844 = 0.029454167932271957 + 50.0 * 6.210438251495361
Epoch 2090, val loss: 1.2706568241119385
Epoch 2100, training loss: 310.5858154296875 = 0.02902926132082939 + 50.0 * 6.2111358642578125
Epoch 2100, val loss: 1.2754839658737183
Epoch 2110, training loss: 310.9635009765625 = 0.028607383370399475 + 50.0 * 6.218698024749756
Epoch 2110, val loss: 1.2801592350006104
Epoch 2120, training loss: 310.73089599609375 = 0.028159551322460175 + 50.0 * 6.214054584503174
Epoch 2120, val loss: 1.2849087715148926
Epoch 2130, training loss: 310.60498046875 = 0.02773544192314148 + 50.0 * 6.211544513702393
Epoch 2130, val loss: 1.2893177270889282
Epoch 2140, training loss: 310.5297546386719 = 0.027331063523888588 + 50.0 * 6.210048198699951
Epoch 2140, val loss: 1.294273018836975
Epoch 2150, training loss: 310.52191162109375 = 0.026943305507302284 + 50.0 * 6.209899425506592
Epoch 2150, val loss: 1.2990394830703735
Epoch 2160, training loss: 310.8272399902344 = 0.026571497321128845 + 50.0 * 6.216013431549072
Epoch 2160, val loss: 1.3039605617523193
Epoch 2170, training loss: 310.7242431640625 = 0.026166951283812523 + 50.0 * 6.213961601257324
Epoch 2170, val loss: 1.3081223964691162
Epoch 2180, training loss: 310.6139221191406 = 0.025776594877243042 + 50.0 * 6.211762428283691
Epoch 2180, val loss: 1.3129216432571411
Epoch 2190, training loss: 310.51422119140625 = 0.02540368214249611 + 50.0 * 6.209776401519775
Epoch 2190, val loss: 1.3174372911453247
Epoch 2200, training loss: 310.5007629394531 = 0.025050928816199303 + 50.0 * 6.209514141082764
Epoch 2200, val loss: 1.3222910165786743
Epoch 2210, training loss: 310.6753845214844 = 0.02470550872385502 + 50.0 * 6.213013172149658
Epoch 2210, val loss: 1.3269258737564087
Epoch 2220, training loss: 310.5902404785156 = 0.02435280755162239 + 50.0 * 6.211318016052246
Epoch 2220, val loss: 1.3315160274505615
Epoch 2230, training loss: 310.4655456542969 = 0.024001073092222214 + 50.0 * 6.208831310272217
Epoch 2230, val loss: 1.3360143899917603
Epoch 2240, training loss: 310.5599365234375 = 0.023672541603446007 + 50.0 * 6.2107253074646
Epoch 2240, val loss: 1.340423583984375
Epoch 2250, training loss: 310.44732666015625 = 0.023341353982686996 + 50.0 * 6.208479404449463
Epoch 2250, val loss: 1.345157504081726
Epoch 2260, training loss: 310.4678039550781 = 0.023022238165140152 + 50.0 * 6.208896160125732
Epoch 2260, val loss: 1.3497862815856934
Epoch 2270, training loss: 310.5815734863281 = 0.02271469682455063 + 50.0 * 6.211177349090576
Epoch 2270, val loss: 1.354146122932434
Epoch 2280, training loss: 310.4625244140625 = 0.02239362522959709 + 50.0 * 6.208802700042725
Epoch 2280, val loss: 1.3585364818572998
Epoch 2290, training loss: 310.4710998535156 = 0.02208588272333145 + 50.0 * 6.208980083465576
Epoch 2290, val loss: 1.362810730934143
Epoch 2300, training loss: 310.4798889160156 = 0.021789023652672768 + 50.0 * 6.209161758422852
Epoch 2300, val loss: 1.3672291040420532
Epoch 2310, training loss: 310.4516906738281 = 0.02150661312043667 + 50.0 * 6.208603382110596
Epoch 2310, val loss: 1.371799111366272
Epoch 2320, training loss: 310.4731140136719 = 0.021218817681074142 + 50.0 * 6.209037780761719
Epoch 2320, val loss: 1.3760548830032349
Epoch 2330, training loss: 310.34552001953125 = 0.020934859290719032 + 50.0 * 6.206491470336914
Epoch 2330, val loss: 1.3804320096969604
Epoch 2340, training loss: 310.4067077636719 = 0.02066589705646038 + 50.0 * 6.20772123336792
Epoch 2340, val loss: 1.3848295211791992
Epoch 2350, training loss: 310.51904296875 = 0.020403418689966202 + 50.0 * 6.209972381591797
Epoch 2350, val loss: 1.3891732692718506
Epoch 2360, training loss: 310.66986083984375 = 0.02013932354748249 + 50.0 * 6.212994575500488
Epoch 2360, val loss: 1.393717885017395
Epoch 2370, training loss: 310.43682861328125 = 0.01984844170510769 + 50.0 * 6.208339691162109
Epoch 2370, val loss: 1.3972946405410767
Epoch 2380, training loss: 310.32867431640625 = 0.01960393413901329 + 50.0 * 6.206181526184082
Epoch 2380, val loss: 1.4018522500991821
Epoch 2390, training loss: 310.290771484375 = 0.01935751549899578 + 50.0 * 6.205428600311279
Epoch 2390, val loss: 1.4060043096542358
Epoch 2400, training loss: 310.326171875 = 0.019122185185551643 + 50.0 * 6.206140995025635
Epoch 2400, val loss: 1.4101860523223877
Epoch 2410, training loss: 310.5642395019531 = 0.018883533775806427 + 50.0 * 6.210906982421875
Epoch 2410, val loss: 1.41408109664917
Epoch 2420, training loss: 310.459716796875 = 0.01864132285118103 + 50.0 * 6.2088212966918945
Epoch 2420, val loss: 1.4185019731521606
Epoch 2430, training loss: 310.3428649902344 = 0.01839262619614601 + 50.0 * 6.206489562988281
Epoch 2430, val loss: 1.4223480224609375
Epoch 2440, training loss: 310.3330993652344 = 0.01816224493086338 + 50.0 * 6.206298828125
Epoch 2440, val loss: 1.4265421628952026
Epoch 2450, training loss: 310.3052062988281 = 0.01794210635125637 + 50.0 * 6.205745220184326
Epoch 2450, val loss: 1.4307337999343872
Epoch 2460, training loss: 310.3539733886719 = 0.017726564779877663 + 50.0 * 6.206724643707275
Epoch 2460, val loss: 1.4347879886627197
Epoch 2470, training loss: 310.32391357421875 = 0.01750917173922062 + 50.0 * 6.206128120422363
Epoch 2470, val loss: 1.4388734102249146
Epoch 2480, training loss: 310.2237854003906 = 0.01729925535619259 + 50.0 * 6.204129695892334
Epoch 2480, val loss: 1.4431538581848145
Epoch 2490, training loss: 310.2406311035156 = 0.017099451273679733 + 50.0 * 6.204470634460449
Epoch 2490, val loss: 1.4472419023513794
Epoch 2500, training loss: 310.5428466796875 = 0.016908399760723114 + 50.0 * 6.210518836975098
Epoch 2500, val loss: 1.4512979984283447
Epoch 2510, training loss: 310.3728332519531 = 0.016686396673321724 + 50.0 * 6.207122802734375
Epoch 2510, val loss: 1.4550213813781738
Epoch 2520, training loss: 310.229248046875 = 0.01648012548685074 + 50.0 * 6.2042555809021
Epoch 2520, val loss: 1.4588130712509155
Epoch 2530, training loss: 310.1846008300781 = 0.01628357730805874 + 50.0 * 6.203365802764893
Epoch 2530, val loss: 1.462990641593933
Epoch 2540, training loss: 310.2194519042969 = 0.01609824225306511 + 50.0 * 6.204066753387451
Epoch 2540, val loss: 1.466795563697815
Epoch 2550, training loss: 310.34173583984375 = 0.015914788469672203 + 50.0 * 6.206516742706299
Epoch 2550, val loss: 1.4707107543945312
Epoch 2560, training loss: 310.2541198730469 = 0.015732644125819206 + 50.0 * 6.204767227172852
Epoch 2560, val loss: 1.4747533798217773
Epoch 2570, training loss: 310.2057189941406 = 0.015548836439847946 + 50.0 * 6.203803062438965
Epoch 2570, val loss: 1.478553295135498
Epoch 2580, training loss: 310.45147705078125 = 0.015378417447209358 + 50.0 * 6.20872163772583
Epoch 2580, val loss: 1.4827191829681396
Epoch 2590, training loss: 310.17645263671875 = 0.015189948491752148 + 50.0 * 6.203225135803223
Epoch 2590, val loss: 1.485991358757019
Epoch 2600, training loss: 310.13043212890625 = 0.01501601655036211 + 50.0 * 6.202308654785156
Epoch 2600, val loss: 1.4899966716766357
Epoch 2610, training loss: 310.2004699707031 = 0.0148540697991848 + 50.0 * 6.203712463378906
Epoch 2610, val loss: 1.4937900304794312
Epoch 2620, training loss: 310.3873596191406 = 0.014684096910059452 + 50.0 * 6.207453727722168
Epoch 2620, val loss: 1.4972604513168335
Epoch 2630, training loss: 310.181396484375 = 0.014518428593873978 + 50.0 * 6.203337669372559
Epoch 2630, val loss: 1.5013549327850342
Epoch 2640, training loss: 310.1513671875 = 0.014354798942804337 + 50.0 * 6.202740669250488
Epoch 2640, val loss: 1.5049467086791992
Epoch 2650, training loss: 310.31634521484375 = 0.014198708347976208 + 50.0 * 6.206043243408203
Epoch 2650, val loss: 1.5086075067520142
Epoch 2660, training loss: 310.1093444824219 = 0.014040485955774784 + 50.0 * 6.201906204223633
Epoch 2660, val loss: 1.5123640298843384
Epoch 2670, training loss: 310.13995361328125 = 0.013890990056097507 + 50.0 * 6.202521324157715
Epoch 2670, val loss: 1.5161625146865845
Epoch 2680, training loss: 310.0816955566406 = 0.013741843402385712 + 50.0 * 6.201359272003174
Epoch 2680, val loss: 1.5198415517807007
Epoch 2690, training loss: 310.2427062988281 = 0.01359950564801693 + 50.0 * 6.2045817375183105
Epoch 2690, val loss: 1.523441195487976
Epoch 2700, training loss: 310.1130065917969 = 0.013446549884974957 + 50.0 * 6.201991081237793
Epoch 2700, val loss: 1.5270838737487793
Epoch 2710, training loss: 310.13983154296875 = 0.01330912858247757 + 50.0 * 6.202530384063721
Epoch 2710, val loss: 1.5309829711914062
Epoch 2720, training loss: 310.127685546875 = 0.013165431097149849 + 50.0 * 6.2022905349731445
Epoch 2720, val loss: 1.534439206123352
Epoch 2730, training loss: 310.3095703125 = 0.01303513441234827 + 50.0 * 6.205930709838867
Epoch 2730, val loss: 1.5381888151168823
Epoch 2740, training loss: 310.1454772949219 = 0.012884356081485748 + 50.0 * 6.2026519775390625
Epoch 2740, val loss: 1.5410174131393433
Epoch 2750, training loss: 310.0669250488281 = 0.012750832363963127 + 50.0 * 6.201083660125732
Epoch 2750, val loss: 1.5450013875961304
Epoch 2760, training loss: 310.03985595703125 = 0.012622429989278316 + 50.0 * 6.200544357299805
Epoch 2760, val loss: 1.5486565828323364
Epoch 2770, training loss: 310.1871032714844 = 0.012495530769228935 + 50.0 * 6.203492641448975
Epoch 2770, val loss: 1.5519241094589233
Epoch 2780, training loss: 310.1169738769531 = 0.012366286478936672 + 50.0 * 6.202092170715332
Epoch 2780, val loss: 1.5555323362350464
Epoch 2790, training loss: 310.29931640625 = 0.012228972278535366 + 50.0 * 6.2057414054870605
Epoch 2790, val loss: 1.5583710670471191
Epoch 2800, training loss: 310.08013916015625 = 0.012105699628591537 + 50.0 * 6.201360702514648
Epoch 2800, val loss: 1.5623763799667358
Epoch 2810, training loss: 309.9947814941406 = 0.011981292627751827 + 50.0 * 6.199655532836914
Epoch 2810, val loss: 1.5655847787857056
Epoch 2820, training loss: 309.9800720214844 = 0.011869107373058796 + 50.0 * 6.199364185333252
Epoch 2820, val loss: 1.5694164037704468
Epoch 2830, training loss: 310.1034240722656 = 0.011760308407247066 + 50.0 * 6.201833724975586
Epoch 2830, val loss: 1.5729146003723145
Epoch 2840, training loss: 310.0947265625 = 0.01164120901376009 + 50.0 * 6.201662063598633
Epoch 2840, val loss: 1.5761497020721436
Epoch 2850, training loss: 310.0815124511719 = 0.011520050466060638 + 50.0 * 6.201400279998779
Epoch 2850, val loss: 1.5794341564178467
Epoch 2860, training loss: 309.9975280761719 = 0.011405614204704762 + 50.0 * 6.1997222900390625
Epoch 2860, val loss: 1.5829088687896729
Epoch 2870, training loss: 310.02032470703125 = 0.011299623176455498 + 50.0 * 6.200180530548096
Epoch 2870, val loss: 1.5863831043243408
Epoch 2880, training loss: 310.0489807128906 = 0.011191143654286861 + 50.0 * 6.200755596160889
Epoch 2880, val loss: 1.5895884037017822
Epoch 2890, training loss: 310.0380859375 = 0.011082884855568409 + 50.0 * 6.200540065765381
Epoch 2890, val loss: 1.5927464962005615
Epoch 2900, training loss: 310.0865173339844 = 0.010973981581628323 + 50.0 * 6.201510906219482
Epoch 2900, val loss: 1.5958727598190308
Epoch 2910, training loss: 310.180908203125 = 0.010865239426493645 + 50.0 * 6.2034010887146
Epoch 2910, val loss: 1.5991859436035156
Epoch 2920, training loss: 310.1188659667969 = 0.010756385512650013 + 50.0 * 6.202162742614746
Epoch 2920, val loss: 1.6019474267959595
Epoch 2930, training loss: 309.9541320800781 = 0.010656069032847881 + 50.0 * 6.198869705200195
Epoch 2930, val loss: 1.6058598756790161
Epoch 2940, training loss: 309.92218017578125 = 0.010560093447566032 + 50.0 * 6.198232173919678
Epoch 2940, val loss: 1.6090341806411743
Epoch 2950, training loss: 309.8880310058594 = 0.010466890409588814 + 50.0 * 6.197551250457764
Epoch 2950, val loss: 1.6124728918075562
Epoch 2960, training loss: 309.9666748046875 = 0.010378576815128326 + 50.0 * 6.19912576675415
Epoch 2960, val loss: 1.6158174276351929
Epoch 2970, training loss: 310.15972900390625 = 0.010285056196153164 + 50.0 * 6.202988624572754
Epoch 2970, val loss: 1.618945837020874
Epoch 2980, training loss: 310.02703857421875 = 0.010177558287978172 + 50.0 * 6.2003374099731445
Epoch 2980, val loss: 1.62150239944458
Epoch 2990, training loss: 309.9272155761719 = 0.010081952437758446 + 50.0 * 6.198342800140381
Epoch 2990, val loss: 1.6246591806411743
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7296296296296296
0.8392198207696363
The final CL Acc:0.72593, 0.01090, The final GNN Acc:0.83641, 0.00263
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11656])
remove edge: torch.Size([2, 9542])
updated graph: torch.Size([2, 10642])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7752380371094 = 1.9323399066925049 + 50.0 * 8.596858024597168
Epoch 0, val loss: 1.928735375404358
Epoch 10, training loss: 431.7340393066406 = 1.9240269660949707 + 50.0 * 8.596199989318848
Epoch 10, val loss: 1.9207773208618164
Epoch 20, training loss: 431.5025329589844 = 1.913520097732544 + 50.0 * 8.591780662536621
Epoch 20, val loss: 1.9105322360992432
Epoch 30, training loss: 430.0068054199219 = 1.899694561958313 + 50.0 * 8.562142372131348
Epoch 30, val loss: 1.8969647884368896
Epoch 40, training loss: 421.03521728515625 = 1.8821454048156738 + 50.0 * 8.383061408996582
Epoch 40, val loss: 1.8805948495864868
Epoch 50, training loss: 382.0124206542969 = 1.8635057210922241 + 50.0 * 7.602978706359863
Epoch 50, val loss: 1.8636606931686401
Epoch 60, training loss: 364.8620300292969 = 1.851180911064148 + 50.0 * 7.260217189788818
Epoch 60, val loss: 1.8527969121932983
Epoch 70, training loss: 355.9420471191406 = 1.8417452573776245 + 50.0 * 7.082005977630615
Epoch 70, val loss: 1.8434324264526367
Epoch 80, training loss: 349.2502136230469 = 1.832070231437683 + 50.0 * 6.948363304138184
Epoch 80, val loss: 1.833870768547058
Epoch 90, training loss: 344.07574462890625 = 1.8244976997375488 + 50.0 * 6.845025062561035
Epoch 90, val loss: 1.8265711069107056
Epoch 100, training loss: 340.034912109375 = 1.8176002502441406 + 50.0 * 6.764346122741699
Epoch 100, val loss: 1.8198078870773315
Epoch 110, training loss: 336.747802734375 = 1.8114938735961914 + 50.0 * 6.698726177215576
Epoch 110, val loss: 1.8136687278747559
Epoch 120, training loss: 334.7005310058594 = 1.8056104183197021 + 50.0 * 6.657898426055908
Epoch 120, val loss: 1.8075916767120361
Epoch 130, training loss: 333.0150451660156 = 1.7995535135269165 + 50.0 * 6.624309539794922
Epoch 130, val loss: 1.8012763261795044
Epoch 140, training loss: 331.4912109375 = 1.7933778762817383 + 50.0 * 6.59395694732666
Epoch 140, val loss: 1.7950716018676758
Epoch 150, training loss: 330.1606750488281 = 1.787004828453064 + 50.0 * 6.567473411560059
Epoch 150, val loss: 1.788797378540039
Epoch 160, training loss: 328.82763671875 = 1.780537486076355 + 50.0 * 6.5409417152404785
Epoch 160, val loss: 1.7825404405593872
Epoch 170, training loss: 327.8018493652344 = 1.773873209953308 + 50.0 * 6.520559787750244
Epoch 170, val loss: 1.776124119758606
Epoch 180, training loss: 326.6447448730469 = 1.766658067703247 + 50.0 * 6.497561931610107
Epoch 180, val loss: 1.7692736387252808
Epoch 190, training loss: 325.6808166503906 = 1.7588036060333252 + 50.0 * 6.478440284729004
Epoch 190, val loss: 1.7620071172714233
Epoch 200, training loss: 324.95916748046875 = 1.7501803636550903 + 50.0 * 6.464179992675781
Epoch 200, val loss: 1.7541207075119019
Epoch 210, training loss: 324.2336730957031 = 1.7407277822494507 + 50.0 * 6.449859142303467
Epoch 210, val loss: 1.7454471588134766
Epoch 220, training loss: 323.6375732421875 = 1.7303951978683472 + 50.0 * 6.438143253326416
Epoch 220, val loss: 1.7361044883728027
Epoch 230, training loss: 323.0833740234375 = 1.7191951274871826 + 50.0 * 6.42728328704834
Epoch 230, val loss: 1.7260756492614746
Epoch 240, training loss: 322.88092041015625 = 1.7069112062454224 + 50.0 * 6.42348051071167
Epoch 240, val loss: 1.7151987552642822
Epoch 250, training loss: 322.16644287109375 = 1.6935935020446777 + 50.0 * 6.409456729888916
Epoch 250, val loss: 1.7033880949020386
Epoch 260, training loss: 321.76495361328125 = 1.6792829036712646 + 50.0 * 6.4017133712768555
Epoch 260, val loss: 1.6908535957336426
Epoch 270, training loss: 321.3758850097656 = 1.6639400720596313 + 50.0 * 6.3942389488220215
Epoch 270, val loss: 1.677516222000122
Epoch 280, training loss: 321.5261535644531 = 1.6474981307983398 + 50.0 * 6.397573471069336
Epoch 280, val loss: 1.6634252071380615
Epoch 290, training loss: 320.7218322753906 = 1.6298770904541016 + 50.0 * 6.381839275360107
Epoch 290, val loss: 1.64823579788208
Epoch 300, training loss: 320.44091796875 = 1.6114228963851929 + 50.0 * 6.376590251922607
Epoch 300, val loss: 1.632495403289795
Epoch 310, training loss: 320.10784912109375 = 1.5920517444610596 + 50.0 * 6.370316028594971
Epoch 310, val loss: 1.6162309646606445
Epoch 320, training loss: 320.3607177734375 = 1.5717960596084595 + 50.0 * 6.3757781982421875
Epoch 320, val loss: 1.5994170904159546
Epoch 330, training loss: 319.7153625488281 = 1.550727128982544 + 50.0 * 6.363292694091797
Epoch 330, val loss: 1.5819538831710815
Epoch 340, training loss: 319.3992004394531 = 1.5292696952819824 + 50.0 * 6.357398509979248
Epoch 340, val loss: 1.5645365715026855
Epoch 350, training loss: 319.1489562988281 = 1.5073432922363281 + 50.0 * 6.352832317352295
Epoch 350, val loss: 1.546856164932251
Epoch 360, training loss: 318.92669677734375 = 1.485120415687561 + 50.0 * 6.348831653594971
Epoch 360, val loss: 1.5292346477508545
Epoch 370, training loss: 318.7333984375 = 1.4626888036727905 + 50.0 * 6.345414638519287
Epoch 370, val loss: 1.5116256475448608
Epoch 380, training loss: 318.5421447753906 = 1.4400001764297485 + 50.0 * 6.342042922973633
Epoch 380, val loss: 1.4940617084503174
Epoch 390, training loss: 318.381103515625 = 1.4172722101211548 + 50.0 * 6.3392767906188965
Epoch 390, val loss: 1.476576805114746
Epoch 400, training loss: 318.2184753417969 = 1.3945814371109009 + 50.0 * 6.336477756500244
Epoch 400, val loss: 1.4592787027359009
Epoch 410, training loss: 318.0542297363281 = 1.3719571828842163 + 50.0 * 6.333645343780518
Epoch 410, val loss: 1.4422458410263062
Epoch 420, training loss: 317.8418884277344 = 1.3495010137557983 + 50.0 * 6.329847812652588
Epoch 420, val loss: 1.425507664680481
Epoch 430, training loss: 317.7568664550781 = 1.3271068334579468 + 50.0 * 6.328595161437988
Epoch 430, val loss: 1.4090667963027954
Epoch 440, training loss: 317.7259826660156 = 1.3047252893447876 + 50.0 * 6.328425407409668
Epoch 440, val loss: 1.3928496837615967
Epoch 450, training loss: 317.40411376953125 = 1.2826416492462158 + 50.0 * 6.322429180145264
Epoch 450, val loss: 1.3767954111099243
Epoch 460, training loss: 317.2257385253906 = 1.2607591152191162 + 50.0 * 6.319299221038818
Epoch 460, val loss: 1.3611429929733276
Epoch 470, training loss: 317.0985107421875 = 1.2392481565475464 + 50.0 * 6.317184925079346
Epoch 470, val loss: 1.3460314273834229
Epoch 480, training loss: 317.185791015625 = 1.2179765701293945 + 50.0 * 6.3193559646606445
Epoch 480, val loss: 1.3313454389572144
Epoch 490, training loss: 316.8929138183594 = 1.1969072818756104 + 50.0 * 6.313920497894287
Epoch 490, val loss: 1.31690514087677
Epoch 500, training loss: 316.676025390625 = 1.1762793064117432 + 50.0 * 6.309995174407959
Epoch 500, val loss: 1.303031086921692
Epoch 510, training loss: 316.5396423339844 = 1.1560823917388916 + 50.0 * 6.307671070098877
Epoch 510, val loss: 1.2896031141281128
Epoch 520, training loss: 317.2003173828125 = 1.1361421346664429 + 50.0 * 6.32128381729126
Epoch 520, val loss: 1.276631474494934
Epoch 530, training loss: 316.571533203125 = 1.1165086030960083 + 50.0 * 6.309100151062012
Epoch 530, val loss: 1.263719081878662
Epoch 540, training loss: 316.1981506347656 = 1.0974323749542236 + 50.0 * 6.302014350891113
Epoch 540, val loss: 1.2517526149749756
Epoch 550, training loss: 316.0745849609375 = 1.0789231061935425 + 50.0 * 6.29991340637207
Epoch 550, val loss: 1.2403631210327148
Epoch 560, training loss: 315.96502685546875 = 1.0608460903167725 + 50.0 * 6.298083782196045
Epoch 560, val loss: 1.229448676109314
Epoch 570, training loss: 315.8966979980469 = 1.0431947708129883 + 50.0 * 6.297070503234863
Epoch 570, val loss: 1.2189662456512451
Epoch 580, training loss: 315.78753662109375 = 1.0258193016052246 + 50.0 * 6.295234680175781
Epoch 580, val loss: 1.209102749824524
Epoch 590, training loss: 315.802734375 = 1.008839726448059 + 50.0 * 6.295877933502197
Epoch 590, val loss: 1.199373722076416
Epoch 600, training loss: 315.6707763671875 = 0.9922856688499451 + 50.0 * 6.293570041656494
Epoch 600, val loss: 1.1904220581054688
Epoch 610, training loss: 315.49432373046875 = 0.9761589169502258 + 50.0 * 6.290363311767578
Epoch 610, val loss: 1.1821156740188599
Epoch 620, training loss: 315.41619873046875 = 0.960443913936615 + 50.0 * 6.289114952087402
Epoch 620, val loss: 1.174137830734253
Epoch 630, training loss: 315.5332946777344 = 0.9450010061264038 + 50.0 * 6.2917656898498535
Epoch 630, val loss: 1.1664018630981445
Epoch 640, training loss: 315.24249267578125 = 0.9298340678215027 + 50.0 * 6.286252975463867
Epoch 640, val loss: 1.1591522693634033
Epoch 650, training loss: 315.1717529296875 = 0.914974570274353 + 50.0 * 6.285135746002197
Epoch 650, val loss: 1.1521334648132324
Epoch 660, training loss: 315.1378173828125 = 0.9005481004714966 + 50.0 * 6.284745216369629
Epoch 660, val loss: 1.1457476615905762
Epoch 670, training loss: 315.1318664550781 = 0.8862736225128174 + 50.0 * 6.284912109375
Epoch 670, val loss: 1.139530897140503
Epoch 680, training loss: 314.9418029785156 = 0.8722668886184692 + 50.0 * 6.281391143798828
Epoch 680, val loss: 1.1335697174072266
Epoch 690, training loss: 314.849853515625 = 0.8585900664329529 + 50.0 * 6.279825210571289
Epoch 690, val loss: 1.1282238960266113
Epoch 700, training loss: 314.8323669433594 = 0.8451435565948486 + 50.0 * 6.2797441482543945
Epoch 700, val loss: 1.1231263875961304
Epoch 710, training loss: 314.7041931152344 = 0.8317950963973999 + 50.0 * 6.2774481773376465
Epoch 710, val loss: 1.117579698562622
Epoch 720, training loss: 314.7049560546875 = 0.8187671899795532 + 50.0 * 6.277724266052246
Epoch 720, val loss: 1.1132420301437378
Epoch 730, training loss: 314.7459716796875 = 0.8058850765228271 + 50.0 * 6.278801918029785
Epoch 730, val loss: 1.1085056066513062
Epoch 740, training loss: 314.5526428222656 = 0.7931849360466003 + 50.0 * 6.275189399719238
Epoch 740, val loss: 1.1040832996368408
Epoch 750, training loss: 314.5270690917969 = 0.7808170318603516 + 50.0 * 6.2749247550964355
Epoch 750, val loss: 1.1003047227859497
Epoch 760, training loss: 314.369384765625 = 0.7685001492500305 + 50.0 * 6.272017955780029
Epoch 760, val loss: 1.0964820384979248
Epoch 770, training loss: 314.3287048339844 = 0.7563897967338562 + 50.0 * 6.271446704864502
Epoch 770, val loss: 1.0929282903671265
Epoch 780, training loss: 314.2597961425781 = 0.7445182800292969 + 50.0 * 6.270305633544922
Epoch 780, val loss: 1.08961021900177
Epoch 790, training loss: 314.48309326171875 = 0.7328072786331177 + 50.0 * 6.275005340576172
Epoch 790, val loss: 1.0863282680511475
Epoch 800, training loss: 314.16802978515625 = 0.7211450934410095 + 50.0 * 6.268938064575195
Epoch 800, val loss: 1.083543300628662
Epoch 810, training loss: 314.13909912109375 = 0.7097053527832031 + 50.0 * 6.268587589263916
Epoch 810, val loss: 1.0808378458023071
Epoch 820, training loss: 314.28961181640625 = 0.6984610557556152 + 50.0 * 6.271823406219482
Epoch 820, val loss: 1.0785064697265625
Epoch 830, training loss: 314.0296630859375 = 0.6872572898864746 + 50.0 * 6.266848087310791
Epoch 830, val loss: 1.0760774612426758
Epoch 840, training loss: 313.91668701171875 = 0.6762934923171997 + 50.0 * 6.26480770111084
Epoch 840, val loss: 1.073809027671814
Epoch 850, training loss: 313.8333435058594 = 0.6655530333518982 + 50.0 * 6.263355731964111
Epoch 850, val loss: 1.072339653968811
Epoch 860, training loss: 313.8429870605469 = 0.6549364924430847 + 50.0 * 6.263761043548584
Epoch 860, val loss: 1.0706268548965454
Epoch 870, training loss: 313.8980712890625 = 0.6443652510643005 + 50.0 * 6.265073776245117
Epoch 870, val loss: 1.0689245462417603
Epoch 880, training loss: 313.6944274902344 = 0.6338836550712585 + 50.0 * 6.261210918426514
Epoch 880, val loss: 1.0676510334014893
Epoch 890, training loss: 313.6827087402344 = 0.6236918568611145 + 50.0 * 6.261180400848389
Epoch 890, val loss: 1.0667054653167725
Epoch 900, training loss: 313.7818298339844 = 0.6136168837547302 + 50.0 * 6.263364315032959
Epoch 900, val loss: 1.0656079053878784
Epoch 910, training loss: 313.6661682128906 = 0.6036097407341003 + 50.0 * 6.261251449584961
Epoch 910, val loss: 1.0649093389511108
Epoch 920, training loss: 313.53472900390625 = 0.5937662124633789 + 50.0 * 6.258819103240967
Epoch 920, val loss: 1.0641576051712036
Epoch 930, training loss: 313.6541748046875 = 0.5841442942619324 + 50.0 * 6.2614006996154785
Epoch 930, val loss: 1.064147710800171
Epoch 940, training loss: 313.4461364746094 = 0.5745458602905273 + 50.0 * 6.257431507110596
Epoch 940, val loss: 1.0633732080459595
Epoch 950, training loss: 313.5186767578125 = 0.5651370286941528 + 50.0 * 6.259070873260498
Epoch 950, val loss: 1.0631955862045288
Epoch 960, training loss: 313.3480224609375 = 0.5558162927627563 + 50.0 * 6.2558441162109375
Epoch 960, val loss: 1.0633504390716553
Epoch 970, training loss: 313.3153076171875 = 0.5466630458831787 + 50.0 * 6.255373001098633
Epoch 970, val loss: 1.0636216402053833
Epoch 980, training loss: 313.3420715332031 = 0.5376359224319458 + 50.0 * 6.256088733673096
Epoch 980, val loss: 1.0636121034622192
Epoch 990, training loss: 313.19573974609375 = 0.5287854671478271 + 50.0 * 6.2533392906188965
Epoch 990, val loss: 1.0642222166061401
Epoch 1000, training loss: 313.3126525878906 = 0.520058274269104 + 50.0 * 6.255852222442627
Epoch 1000, val loss: 1.0645742416381836
Epoch 1010, training loss: 313.19073486328125 = 0.5113804936408997 + 50.0 * 6.253586769104004
Epoch 1010, val loss: 1.0650379657745361
Epoch 1020, training loss: 313.0905456542969 = 0.5028773546218872 + 50.0 * 6.251753807067871
Epoch 1020, val loss: 1.0659010410308838
Epoch 1030, training loss: 313.0607604980469 = 0.4945329427719116 + 50.0 * 6.25132417678833
Epoch 1030, val loss: 1.0669001340866089
Epoch 1040, training loss: 313.2093505859375 = 0.4863102436065674 + 50.0 * 6.254461288452148
Epoch 1040, val loss: 1.0679439306259155
Epoch 1050, training loss: 313.0365295410156 = 0.478068470954895 + 50.0 * 6.251169681549072
Epoch 1050, val loss: 1.0683047771453857
Epoch 1060, training loss: 312.9183349609375 = 0.4701167941093445 + 50.0 * 6.248964309692383
Epoch 1060, val loss: 1.069758415222168
Epoch 1070, training loss: 313.0491027832031 = 0.4622686505317688 + 50.0 * 6.251736640930176
Epoch 1070, val loss: 1.0710699558258057
Epoch 1080, training loss: 312.92010498046875 = 0.4544830620288849 + 50.0 * 6.249311923980713
Epoch 1080, val loss: 1.0722689628601074
Epoch 1090, training loss: 312.8247375488281 = 0.44682127237319946 + 50.0 * 6.24755859375
Epoch 1090, val loss: 1.0737054347991943
Epoch 1100, training loss: 312.77581787109375 = 0.4393344521522522 + 50.0 * 6.246729850769043
Epoch 1100, val loss: 1.0754475593566895
Epoch 1110, training loss: 312.8045654296875 = 0.43198466300964355 + 50.0 * 6.2474517822265625
Epoch 1110, val loss: 1.0770843029022217
Epoch 1120, training loss: 312.80364990234375 = 0.4246741831302643 + 50.0 * 6.247579574584961
Epoch 1120, val loss: 1.0789200067520142
Epoch 1130, training loss: 312.6761169433594 = 0.4174097776412964 + 50.0 * 6.245173931121826
Epoch 1130, val loss: 1.0803600549697876
Epoch 1140, training loss: 312.7859802246094 = 0.41033220291137695 + 50.0 * 6.2475128173828125
Epoch 1140, val loss: 1.0823419094085693
Epoch 1150, training loss: 312.6068115234375 = 0.40332096815109253 + 50.0 * 6.244070053100586
Epoch 1150, val loss: 1.084596872329712
Epoch 1160, training loss: 312.5518798828125 = 0.3964977264404297 + 50.0 * 6.243107795715332
Epoch 1160, val loss: 1.08675217628479
Epoch 1170, training loss: 312.5132751464844 = 0.389777272939682 + 50.0 * 6.2424702644348145
Epoch 1170, val loss: 1.0891022682189941
Epoch 1180, training loss: 312.73431396484375 = 0.38318341970443726 + 50.0 * 6.24702262878418
Epoch 1180, val loss: 1.0917524099349976
Epoch 1190, training loss: 312.5992736816406 = 0.3764992356300354 + 50.0 * 6.244455814361572
Epoch 1190, val loss: 1.093600869178772
Epoch 1200, training loss: 312.4780578613281 = 0.36999523639678955 + 50.0 * 6.242161273956299
Epoch 1200, val loss: 1.0962415933609009
Epoch 1210, training loss: 312.39404296875 = 0.36363503336906433 + 50.0 * 6.240607738494873
Epoch 1210, val loss: 1.0990544557571411
Epoch 1220, training loss: 312.48089599609375 = 0.35739681124687195 + 50.0 * 6.2424702644348145
Epoch 1220, val loss: 1.1016298532485962
Epoch 1230, training loss: 312.343017578125 = 0.3511868119239807 + 50.0 * 6.2398362159729
Epoch 1230, val loss: 1.104864478111267
Epoch 1240, training loss: 312.37298583984375 = 0.3450548052787781 + 50.0 * 6.240558624267578
Epoch 1240, val loss: 1.1074655055999756
Epoch 1250, training loss: 312.2951354980469 = 0.33904483914375305 + 50.0 * 6.239121913909912
Epoch 1250, val loss: 1.1106914281845093
Epoch 1260, training loss: 312.26611328125 = 0.3331242799758911 + 50.0 * 6.238659858703613
Epoch 1260, val loss: 1.1134414672851562
Epoch 1270, training loss: 312.2572937011719 = 0.3273017108440399 + 50.0 * 6.23859977722168
Epoch 1270, val loss: 1.1166975498199463
Epoch 1280, training loss: 312.3916015625 = 0.3215503692626953 + 50.0 * 6.241400718688965
Epoch 1280, val loss: 1.1198614835739136
Epoch 1290, training loss: 312.2347106933594 = 0.315827876329422 + 50.0 * 6.238377571105957
Epoch 1290, val loss: 1.1233127117156982
Epoch 1300, training loss: 312.1572570800781 = 0.3101964592933655 + 50.0 * 6.236940860748291
Epoch 1300, val loss: 1.126663327217102
Epoch 1310, training loss: 312.2342224121094 = 0.30474552512168884 + 50.0 * 6.238589286804199
Epoch 1310, val loss: 1.1305246353149414
Epoch 1320, training loss: 312.065673828125 = 0.29924142360687256 + 50.0 * 6.235328197479248
Epoch 1320, val loss: 1.1331863403320312
Epoch 1330, training loss: 312.06866455078125 = 0.2939084470272064 + 50.0 * 6.235495090484619
Epoch 1330, val loss: 1.1370548009872437
Epoch 1340, training loss: 312.1324157714844 = 0.28863221406936646 + 50.0 * 6.236875534057617
Epoch 1340, val loss: 1.1403472423553467
Epoch 1350, training loss: 312.078369140625 = 0.2833974361419678 + 50.0 * 6.235899448394775
Epoch 1350, val loss: 1.1441203355789185
Epoch 1360, training loss: 312.09588623046875 = 0.2782154381275177 + 50.0 * 6.236353397369385
Epoch 1360, val loss: 1.1476340293884277
Epoch 1370, training loss: 311.9554443359375 = 0.27312833070755005 + 50.0 * 6.233646392822266
Epoch 1370, val loss: 1.1515735387802124
Epoch 1380, training loss: 311.9145202636719 = 0.26814377307891846 + 50.0 * 6.232927322387695
Epoch 1380, val loss: 1.1552636623382568
Epoch 1390, training loss: 311.9363098144531 = 0.2632867991924286 + 50.0 * 6.233460426330566
Epoch 1390, val loss: 1.1594775915145874
Epoch 1400, training loss: 312.04827880859375 = 0.25844356417655945 + 50.0 * 6.2357964515686035
Epoch 1400, val loss: 1.1629571914672852
Epoch 1410, training loss: 311.9473571777344 = 0.2536408007144928 + 50.0 * 6.233874320983887
Epoch 1410, val loss: 1.1668753623962402
Epoch 1420, training loss: 311.89276123046875 = 0.24893811345100403 + 50.0 * 6.232876300811768
Epoch 1420, val loss: 1.1710102558135986
Epoch 1430, training loss: 311.8835754394531 = 0.2443155199289322 + 50.0 * 6.232785224914551
Epoch 1430, val loss: 1.175112247467041
Epoch 1440, training loss: 311.8822021484375 = 0.23976950347423553 + 50.0 * 6.232848644256592
Epoch 1440, val loss: 1.1794875860214233
Epoch 1450, training loss: 311.81591796875 = 0.23530299961566925 + 50.0 * 6.231612682342529
Epoch 1450, val loss: 1.1837058067321777
Epoch 1460, training loss: 311.738037109375 = 0.23090240359306335 + 50.0 * 6.230142593383789
Epoch 1460, val loss: 1.188059687614441
Epoch 1470, training loss: 311.78717041015625 = 0.2266114056110382 + 50.0 * 6.231210708618164
Epoch 1470, val loss: 1.1922003030776978
Epoch 1480, training loss: 311.8672180175781 = 0.22235222160816193 + 50.0 * 6.2328972816467285
Epoch 1480, val loss: 1.1965259313583374
Epoch 1490, training loss: 311.8255310058594 = 0.2180928736925125 + 50.0 * 6.232149124145508
Epoch 1490, val loss: 1.2010300159454346
Epoch 1500, training loss: 311.6846923828125 = 0.21392841637134552 + 50.0 * 6.229415416717529
Epoch 1500, val loss: 1.2054377794265747
Epoch 1510, training loss: 311.6268005371094 = 0.20987799763679504 + 50.0 * 6.228338718414307
Epoch 1510, val loss: 1.2097985744476318
Epoch 1520, training loss: 311.6107177734375 = 0.20593924820423126 + 50.0 * 6.228095531463623
Epoch 1520, val loss: 1.2144691944122314
Epoch 1530, training loss: 311.7122802734375 = 0.20206119120121002 + 50.0 * 6.2302045822143555
Epoch 1530, val loss: 1.2191046476364136
Epoch 1540, training loss: 311.7124938964844 = 0.198210209608078 + 50.0 * 6.23028564453125
Epoch 1540, val loss: 1.22431480884552
Epoch 1550, training loss: 311.5694885253906 = 0.1944146454334259 + 50.0 * 6.227501392364502
Epoch 1550, val loss: 1.2290812730789185
Epoch 1560, training loss: 311.5803527832031 = 0.19069217145442963 + 50.0 * 6.227792739868164
Epoch 1560, val loss: 1.2338449954986572
Epoch 1570, training loss: 311.59600830078125 = 0.1870797872543335 + 50.0 * 6.22817850112915
Epoch 1570, val loss: 1.2393853664398193
Epoch 1580, training loss: 311.50823974609375 = 0.1835055947303772 + 50.0 * 6.226494789123535
Epoch 1580, val loss: 1.2438082695007324
Epoch 1590, training loss: 311.55401611328125 = 0.18004077672958374 + 50.0 * 6.227478981018066
Epoch 1590, val loss: 1.2495449781417847
Epoch 1600, training loss: 311.6051940917969 = 0.1765880435705185 + 50.0 * 6.228572368621826
Epoch 1600, val loss: 1.254019856452942
Epoch 1610, training loss: 311.5190734863281 = 0.17321044206619263 + 50.0 * 6.226917266845703
Epoch 1610, val loss: 1.2601618766784668
Epoch 1620, training loss: 311.50848388671875 = 0.16986997425556183 + 50.0 * 6.226772308349609
Epoch 1620, val loss: 1.2649450302124023
Epoch 1630, training loss: 311.404052734375 = 0.16661478579044342 + 50.0 * 6.224748611450195
Epoch 1630, val loss: 1.2704111337661743
Epoch 1640, training loss: 311.5060119628906 = 0.163455069065094 + 50.0 * 6.226851463317871
Epoch 1640, val loss: 1.2759568691253662
Epoch 1650, training loss: 311.425537109375 = 0.16029506921768188 + 50.0 * 6.22530460357666
Epoch 1650, val loss: 1.281477689743042
Epoch 1660, training loss: 311.34710693359375 = 0.15721416473388672 + 50.0 * 6.22379732131958
Epoch 1660, val loss: 1.2871363162994385
Epoch 1670, training loss: 311.3338623046875 = 0.15423189103603363 + 50.0 * 6.223592281341553
Epoch 1670, val loss: 1.2929073572158813
Epoch 1680, training loss: 311.5689697265625 = 0.15132994949817657 + 50.0 * 6.2283525466918945
Epoch 1680, val loss: 1.2983238697052002
Epoch 1690, training loss: 311.3486328125 = 0.14840255677700043 + 50.0 * 6.224004745483398
Epoch 1690, val loss: 1.3042353391647339
Epoch 1700, training loss: 311.3278503417969 = 0.14557930827140808 + 50.0 * 6.2236456871032715
Epoch 1700, val loss: 1.3096940517425537
Epoch 1710, training loss: 311.4255065917969 = 0.14283159375190735 + 50.0 * 6.225653648376465
Epoch 1710, val loss: 1.3154438734054565
Epoch 1720, training loss: 311.3055114746094 = 0.14010435342788696 + 50.0 * 6.223308086395264
Epoch 1720, val loss: 1.3211262226104736
Epoch 1730, training loss: 311.260009765625 = 0.137456476688385 + 50.0 * 6.222451210021973
Epoch 1730, val loss: 1.3277889490127563
Epoch 1740, training loss: 311.2336120605469 = 0.13487893342971802 + 50.0 * 6.2219743728637695
Epoch 1740, val loss: 1.3335871696472168
Epoch 1750, training loss: 311.4047546386719 = 0.13238154351711273 + 50.0 * 6.225447177886963
Epoch 1750, val loss: 1.3399279117584229
Epoch 1760, training loss: 311.29638671875 = 0.1298561841249466 + 50.0 * 6.223330497741699
Epoch 1760, val loss: 1.3448736667633057
Epoch 1770, training loss: 311.1937255859375 = 0.12741412222385406 + 50.0 * 6.2213263511657715
Epoch 1770, val loss: 1.3516523838043213
Epoch 1780, training loss: 311.22100830078125 = 0.12504570186138153 + 50.0 * 6.221919536590576
Epoch 1780, val loss: 1.3577685356140137
Epoch 1790, training loss: 311.30950927734375 = 0.1227399930357933 + 50.0 * 6.223735332489014
Epoch 1790, val loss: 1.364194393157959
Epoch 1800, training loss: 311.14111328125 = 0.12044131755828857 + 50.0 * 6.2204132080078125
Epoch 1800, val loss: 1.369960904121399
Epoch 1810, training loss: 311.1346130371094 = 0.11823724210262299 + 50.0 * 6.220327854156494
Epoch 1810, val loss: 1.375988483428955
Epoch 1820, training loss: 311.1341552734375 = 0.11609338223934174 + 50.0 * 6.220361232757568
Epoch 1820, val loss: 1.382445216178894
Epoch 1830, training loss: 311.42620849609375 = 0.11398232728242874 + 50.0 * 6.2262444496154785
Epoch 1830, val loss: 1.3884419202804565
Epoch 1840, training loss: 311.2359924316406 = 0.11185893416404724 + 50.0 * 6.222483158111572
Epoch 1840, val loss: 1.3945220708847046
Epoch 1850, training loss: 311.11767578125 = 0.10981545597314835 + 50.0 * 6.220157623291016
Epoch 1850, val loss: 1.400705337524414
Epoch 1860, training loss: 311.0875549316406 = 0.10783127695322037 + 50.0 * 6.219594955444336
Epoch 1860, val loss: 1.4069358110427856
Epoch 1870, training loss: 311.1217956542969 = 0.10589585453271866 + 50.0 * 6.220317840576172
Epoch 1870, val loss: 1.4134293794631958
Epoch 1880, training loss: 311.091796875 = 0.10399462282657623 + 50.0 * 6.219756126403809
Epoch 1880, val loss: 1.4194244146347046
Epoch 1890, training loss: 311.1767272949219 = 0.10212897509336472 + 50.0 * 6.221491813659668
Epoch 1890, val loss: 1.4255999326705933
Epoch 1900, training loss: 311.10455322265625 = 0.10029862076044083 + 50.0 * 6.2200846672058105
Epoch 1900, val loss: 1.4323222637176514
Epoch 1910, training loss: 311.0185546875 = 0.09851448237895966 + 50.0 * 6.218400955200195
Epoch 1910, val loss: 1.438683032989502
Epoch 1920, training loss: 310.9716796875 = 0.09678119421005249 + 50.0 * 6.217498302459717
Epoch 1920, val loss: 1.4448457956314087
Epoch 1930, training loss: 310.98773193359375 = 0.09509510546922684 + 50.0 * 6.217852592468262
Epoch 1930, val loss: 1.4508278369903564
Epoch 1940, training loss: 311.1931457519531 = 0.09344212710857391 + 50.0 * 6.221993923187256
Epoch 1940, val loss: 1.4572969675064087
Epoch 1950, training loss: 311.0926208496094 = 0.09178953617811203 + 50.0 * 6.2200164794921875
Epoch 1950, val loss: 1.4639229774475098
Epoch 1960, training loss: 310.9563903808594 = 0.09015405923128128 + 50.0 * 6.217324733734131
Epoch 1960, val loss: 1.4695069789886475
Epoch 1970, training loss: 310.9912414550781 = 0.08861237019300461 + 50.0 * 6.218052387237549
Epoch 1970, val loss: 1.4759248495101929
Epoch 1980, training loss: 311.0530090332031 = 0.08708325028419495 + 50.0 * 6.219318389892578
Epoch 1980, val loss: 1.4824700355529785
Epoch 1990, training loss: 311.0440673828125 = 0.08557603508234024 + 50.0 * 6.219169616699219
Epoch 1990, val loss: 1.4888465404510498
Epoch 2000, training loss: 310.9085998535156 = 0.08408890664577484 + 50.0 * 6.216490745544434
Epoch 2000, val loss: 1.494468092918396
Epoch 2010, training loss: 310.8584289550781 = 0.08267270773649216 + 50.0 * 6.21551513671875
Epoch 2010, val loss: 1.5011863708496094
Epoch 2020, training loss: 310.8797912597656 = 0.08128854632377625 + 50.0 * 6.215969562530518
Epoch 2020, val loss: 1.5069453716278076
Epoch 2030, training loss: 311.15948486328125 = 0.07992760092020035 + 50.0 * 6.221591472625732
Epoch 2030, val loss: 1.5130194425582886
Epoch 2040, training loss: 310.97955322265625 = 0.07855820655822754 + 50.0 * 6.218019962310791
Epoch 2040, val loss: 1.519688367843628
Epoch 2050, training loss: 310.9239196777344 = 0.07722868770360947 + 50.0 * 6.216933727264404
Epoch 2050, val loss: 1.5256685018539429
Epoch 2060, training loss: 310.8595886230469 = 0.07593291252851486 + 50.0 * 6.215672969818115
Epoch 2060, val loss: 1.5321117639541626
Epoch 2070, training loss: 310.814697265625 = 0.07467345893383026 + 50.0 * 6.2148003578186035
Epoch 2070, val loss: 1.5379645824432373
Epoch 2080, training loss: 310.8341064453125 = 0.07344995439052582 + 50.0 * 6.215212821960449
Epoch 2080, val loss: 1.54447603225708
Epoch 2090, training loss: 311.0474548339844 = 0.07224059104919434 + 50.0 * 6.219504356384277
Epoch 2090, val loss: 1.550328016281128
Epoch 2100, training loss: 310.86456298828125 = 0.07102417945861816 + 50.0 * 6.2158708572387695
Epoch 2100, val loss: 1.556073784828186
Epoch 2110, training loss: 310.8263854980469 = 0.06985046714544296 + 50.0 * 6.215130805969238
Epoch 2110, val loss: 1.562338948249817
Epoch 2120, training loss: 310.8388671875 = 0.06871838867664337 + 50.0 * 6.215402603149414
Epoch 2120, val loss: 1.568794846534729
Epoch 2130, training loss: 310.89703369140625 = 0.06760136783123016 + 50.0 * 6.216588497161865
Epoch 2130, val loss: 1.574113368988037
Epoch 2140, training loss: 310.7920837402344 = 0.06648977100849152 + 50.0 * 6.214511871337891
Epoch 2140, val loss: 1.5801641941070557
Epoch 2150, training loss: 310.7232971191406 = 0.06541721522808075 + 50.0 * 6.2131571769714355
Epoch 2150, val loss: 1.586414098739624
Epoch 2160, training loss: 310.7400817871094 = 0.0643901377916336 + 50.0 * 6.2135138511657715
Epoch 2160, val loss: 1.5927568674087524
Epoch 2170, training loss: 310.8344421386719 = 0.06337540596723557 + 50.0 * 6.215421199798584
Epoch 2170, val loss: 1.599058747291565
Epoch 2180, training loss: 310.76361083984375 = 0.06234471872448921 + 50.0 * 6.214025020599365
Epoch 2180, val loss: 1.6034594774246216
Epoch 2190, training loss: 310.6994934082031 = 0.061358384788036346 + 50.0 * 6.212762355804443
Epoch 2190, val loss: 1.6096745729446411
Epoch 2200, training loss: 310.8077697753906 = 0.060397446155548096 + 50.0 * 6.214947700500488
Epoch 2200, val loss: 1.6152551174163818
Epoch 2210, training loss: 310.8403625488281 = 0.059443265199661255 + 50.0 * 6.215618133544922
Epoch 2210, val loss: 1.6218775510787964
Epoch 2220, training loss: 310.6385803222656 = 0.05849353224039078 + 50.0 * 6.211601734161377
Epoch 2220, val loss: 1.6268787384033203
Epoch 2230, training loss: 310.61480712890625 = 0.05758996307849884 + 50.0 * 6.21114444732666
Epoch 2230, val loss: 1.6326830387115479
Epoch 2240, training loss: 310.818603515625 = 0.05672268569469452 + 50.0 * 6.215237617492676
Epoch 2240, val loss: 1.6387380361557007
Epoch 2250, training loss: 310.63348388671875 = 0.055823829025030136 + 50.0 * 6.21155309677124
Epoch 2250, val loss: 1.6441761255264282
Epoch 2260, training loss: 310.5914306640625 = 0.0549643374979496 + 50.0 * 6.210729122161865
Epoch 2260, val loss: 1.6496176719665527
Epoch 2270, training loss: 310.5826110839844 = 0.05413633584976196 + 50.0 * 6.210569381713867
Epoch 2270, val loss: 1.6551425457000732
Epoch 2280, training loss: 310.7342224121094 = 0.053340837359428406 + 50.0 * 6.213617324829102
Epoch 2280, val loss: 1.661482334136963
Epoch 2290, training loss: 310.6443176269531 = 0.05250723659992218 + 50.0 * 6.211835861206055
Epoch 2290, val loss: 1.6662455797195435
Epoch 2300, training loss: 310.6559753417969 = 0.05169731751084328 + 50.0 * 6.212085723876953
Epoch 2300, val loss: 1.6718682050704956
Epoch 2310, training loss: 310.6217041015625 = 0.0509091317653656 + 50.0 * 6.211415767669678
Epoch 2310, val loss: 1.6768653392791748
Epoch 2320, training loss: 310.6979675292969 = 0.050160251557826996 + 50.0 * 6.212955951690674
Epoch 2320, val loss: 1.683101773262024
Epoch 2330, training loss: 310.58087158203125 = 0.049413640052080154 + 50.0 * 6.210629463195801
Epoch 2330, val loss: 1.6890814304351807
Epoch 2340, training loss: 310.6129455566406 = 0.04868423566222191 + 50.0 * 6.211285591125488
Epoch 2340, val loss: 1.6938139200210571
Epoch 2350, training loss: 310.5189514160156 = 0.04796162247657776 + 50.0 * 6.2094197273254395
Epoch 2350, val loss: 1.6996393203735352
Epoch 2360, training loss: 310.498291015625 = 0.0472683310508728 + 50.0 * 6.209020137786865
Epoch 2360, val loss: 1.7046750783920288
Epoch 2370, training loss: 310.68743896484375 = 0.04660201445221901 + 50.0 * 6.2128167152404785
Epoch 2370, val loss: 1.710534691810608
Epoch 2380, training loss: 310.5283203125 = 0.045907944440841675 + 50.0 * 6.209648609161377
Epoch 2380, val loss: 1.7155852317810059
Epoch 2390, training loss: 310.4969787597656 = 0.04523593559861183 + 50.0 * 6.2090349197387695
Epoch 2390, val loss: 1.7211718559265137
Epoch 2400, training loss: 310.69195556640625 = 0.04459703341126442 + 50.0 * 6.212947368621826
Epoch 2400, val loss: 1.726714849472046
Epoch 2410, training loss: 310.582275390625 = 0.04393959045410156 + 50.0 * 6.210766315460205
Epoch 2410, val loss: 1.7313647270202637
Epoch 2420, training loss: 310.48065185546875 = 0.04329987242817879 + 50.0 * 6.208746910095215
Epoch 2420, val loss: 1.736396074295044
Epoch 2430, training loss: 310.43585205078125 = 0.04270137846469879 + 50.0 * 6.2078633308410645
Epoch 2430, val loss: 1.742679238319397
Epoch 2440, training loss: 310.47137451171875 = 0.04210963100194931 + 50.0 * 6.208585262298584
Epoch 2440, val loss: 1.7475897073745728
Epoch 2450, training loss: 310.7049865722656 = 0.041529424488544464 + 50.0 * 6.213269233703613
Epoch 2450, val loss: 1.7531859874725342
Epoch 2460, training loss: 310.56854248046875 = 0.04092559590935707 + 50.0 * 6.210552215576172
Epoch 2460, val loss: 1.757204294204712
Epoch 2470, training loss: 310.4508056640625 = 0.04035171493887901 + 50.0 * 6.208209037780762
Epoch 2470, val loss: 1.7632982730865479
Epoch 2480, training loss: 310.4703369140625 = 0.03980047628283501 + 50.0 * 6.208610534667969
Epoch 2480, val loss: 1.7683897018432617
Epoch 2490, training loss: 310.5482177734375 = 0.03925500437617302 + 50.0 * 6.210179328918457
Epoch 2490, val loss: 1.7726497650146484
Epoch 2500, training loss: 310.5663146972656 = 0.03870837390422821 + 50.0 * 6.210552215576172
Epoch 2500, val loss: 1.7768774032592773
Epoch 2510, training loss: 310.40325927734375 = 0.03817155957221985 + 50.0 * 6.207301616668701
Epoch 2510, val loss: 1.7830455303192139
Epoch 2520, training loss: 310.38140869140625 = 0.03765760734677315 + 50.0 * 6.206874847412109
Epoch 2520, val loss: 1.7880762815475464
Epoch 2530, training loss: 310.4349365234375 = 0.037151824682950974 + 50.0 * 6.207955837249756
Epoch 2530, val loss: 1.7925516366958618
Epoch 2540, training loss: 310.491455078125 = 0.0366620235145092 + 50.0 * 6.2090959548950195
Epoch 2540, val loss: 1.7985447645187378
Epoch 2550, training loss: 310.4522705078125 = 0.036161914467811584 + 50.0 * 6.208322048187256
Epoch 2550, val loss: 1.802477240562439
Epoch 2560, training loss: 310.33990478515625 = 0.035680294036865234 + 50.0 * 6.206084728240967
Epoch 2560, val loss: 1.8078471422195435
Epoch 2570, training loss: 310.3680725097656 = 0.03521418944001198 + 50.0 * 6.2066569328308105
Epoch 2570, val loss: 1.8125863075256348
Epoch 2580, training loss: 310.55560302734375 = 0.03476467728614807 + 50.0 * 6.210416793823242
Epoch 2580, val loss: 1.8180978298187256
Epoch 2590, training loss: 310.391845703125 = 0.03428658843040466 + 50.0 * 6.207150936126709
Epoch 2590, val loss: 1.8215218782424927
Epoch 2600, training loss: 310.2720947265625 = 0.03383620083332062 + 50.0 * 6.2047648429870605
Epoch 2600, val loss: 1.826784372329712
Epoch 2610, training loss: 310.2893981933594 = 0.03341095522046089 + 50.0 * 6.205119609832764
Epoch 2610, val loss: 1.8319437503814697
Epoch 2620, training loss: 310.5087585449219 = 0.03299621120095253 + 50.0 * 6.20951509475708
Epoch 2620, val loss: 1.8359078168869019
Epoch 2630, training loss: 310.4200439453125 = 0.03256183862686157 + 50.0 * 6.207749366760254
Epoch 2630, val loss: 1.8416409492492676
Epoch 2640, training loss: 310.3869323730469 = 0.03212009742856026 + 50.0 * 6.207096099853516
Epoch 2640, val loss: 1.8450181484222412
Epoch 2650, training loss: 310.28033447265625 = 0.03171073645353317 + 50.0 * 6.204972743988037
Epoch 2650, val loss: 1.8508119583129883
Epoch 2660, training loss: 310.23089599609375 = 0.031316570937633514 + 50.0 * 6.203991889953613
Epoch 2660, val loss: 1.8553193807601929
Epoch 2670, training loss: 310.26300048828125 = 0.030938664451241493 + 50.0 * 6.204640865325928
Epoch 2670, val loss: 1.8601634502410889
Epoch 2680, training loss: 310.42919921875 = 0.030561111867427826 + 50.0 * 6.207973003387451
Epoch 2680, val loss: 1.8646628856658936
Epoch 2690, training loss: 310.4501953125 = 0.03017428331077099 + 50.0 * 6.208400249481201
Epoch 2690, val loss: 1.868866205215454
Epoch 2700, training loss: 310.4727478027344 = 0.0297947209328413 + 50.0 * 6.208859443664551
Epoch 2700, val loss: 1.8741023540496826
Epoch 2710, training loss: 310.27740478515625 = 0.029406439512968063 + 50.0 * 6.204959869384766
Epoch 2710, val loss: 1.8779362440109253
Epoch 2720, training loss: 310.2038879394531 = 0.029045943170785904 + 50.0 * 6.20349645614624
Epoch 2720, val loss: 1.8823130130767822
Epoch 2730, training loss: 310.18896484375 = 0.02870327979326248 + 50.0 * 6.203205108642578
Epoch 2730, val loss: 1.8871979713439941
Epoch 2740, training loss: 310.1895751953125 = 0.028368273749947548 + 50.0 * 6.203224182128906
Epoch 2740, val loss: 1.8917739391326904
Epoch 2750, training loss: 310.6024475097656 = 0.02805018611252308 + 50.0 * 6.211487770080566
Epoch 2750, val loss: 1.8965575695037842
Epoch 2760, training loss: 310.3584899902344 = 0.02768183872103691 + 50.0 * 6.206615924835205
Epoch 2760, val loss: 1.8993186950683594
Epoch 2770, training loss: 310.3156433105469 = 0.02735261619091034 + 50.0 * 6.205766201019287
Epoch 2770, val loss: 1.9048794507980347
Epoch 2780, training loss: 310.2951965332031 = 0.027015406638383865 + 50.0 * 6.205363750457764
Epoch 2780, val loss: 1.9080333709716797
Epoch 2790, training loss: 310.1408386230469 = 0.026692694053053856 + 50.0 * 6.202282428741455
Epoch 2790, val loss: 1.9126579761505127
Epoch 2800, training loss: 310.1846618652344 = 0.026384815573692322 + 50.0 * 6.203165054321289
Epoch 2800, val loss: 1.9169241189956665
Epoch 2810, training loss: 310.2979431152344 = 0.02608245238661766 + 50.0 * 6.205437183380127
Epoch 2810, val loss: 1.920633316040039
Epoch 2820, training loss: 310.1981201171875 = 0.0257797222584486 + 50.0 * 6.203446388244629
Epoch 2820, val loss: 1.9257776737213135
Epoch 2830, training loss: 310.26702880859375 = 0.025480851531028748 + 50.0 * 6.204831123352051
Epoch 2830, val loss: 1.9302467107772827
Epoch 2840, training loss: 310.2364196777344 = 0.025178492069244385 + 50.0 * 6.204224586486816
Epoch 2840, val loss: 1.93332839012146
Epoch 2850, training loss: 310.18212890625 = 0.024889633059501648 + 50.0 * 6.203144550323486
Epoch 2850, val loss: 1.9380987882614136
Epoch 2860, training loss: 310.25274658203125 = 0.02461007609963417 + 50.0 * 6.204563140869141
Epoch 2860, val loss: 1.9418995380401611
Epoch 2870, training loss: 310.12841796875 = 0.024327164515852928 + 50.0 * 6.20208215713501
Epoch 2870, val loss: 1.9461089372634888
Epoch 2880, training loss: 310.1126403808594 = 0.02405387908220291 + 50.0 * 6.2017717361450195
Epoch 2880, val loss: 1.950312614440918
Epoch 2890, training loss: 310.1424560546875 = 0.02378692291676998 + 50.0 * 6.202373504638672
Epoch 2890, val loss: 1.9536969661712646
Epoch 2900, training loss: 310.2371826171875 = 0.023526858538389206 + 50.0 * 6.204273223876953
Epoch 2900, val loss: 1.9580504894256592
Epoch 2910, training loss: 310.2344665527344 = 0.023261783644557 + 50.0 * 6.204224109649658
Epoch 2910, val loss: 1.961690902709961
Epoch 2920, training loss: 310.1711120605469 = 0.022995425388216972 + 50.0 * 6.2029619216918945
Epoch 2920, val loss: 1.9651981592178345
Epoch 2930, training loss: 310.1475524902344 = 0.022741949185729027 + 50.0 * 6.20249605178833
Epoch 2930, val loss: 1.9705299139022827
Epoch 2940, training loss: 310.07244873046875 = 0.022489478811621666 + 50.0 * 6.2009992599487305
Epoch 2940, val loss: 1.9734824895858765
Epoch 2950, training loss: 310.1050720214844 = 0.02225356735289097 + 50.0 * 6.201656341552734
Epoch 2950, val loss: 1.9783546924591064
Epoch 2960, training loss: 310.11663818359375 = 0.022013012319803238 + 50.0 * 6.201892852783203
Epoch 2960, val loss: 1.9818705320358276
Epoch 2970, training loss: 310.1151428222656 = 0.02177499793469906 + 50.0 * 6.20186710357666
Epoch 2970, val loss: 1.9853180646896362
Epoch 2980, training loss: 310.2170104980469 = 0.021542763337492943 + 50.0 * 6.203909397125244
Epoch 2980, val loss: 1.9896290302276611
Epoch 2990, training loss: 310.0826721191406 = 0.021300090476870537 + 50.0 * 6.201227188110352
Epoch 2990, val loss: 1.9934362173080444
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.662962962962963
0.7986294148655773
=== training gcn model ===
Epoch 0, training loss: 431.7964172363281 = 1.9535589218139648 + 50.0 * 8.596857070922852
Epoch 0, val loss: 1.9603074789047241
Epoch 10, training loss: 431.7552795410156 = 1.9441685676574707 + 50.0 * 8.596221923828125
Epoch 10, val loss: 1.9506843090057373
Epoch 20, training loss: 431.5351257324219 = 1.9323737621307373 + 50.0 * 8.592055320739746
Epoch 20, val loss: 1.9386759996414185
Epoch 30, training loss: 430.0600280761719 = 1.917294979095459 + 50.0 * 8.562854766845703
Epoch 30, val loss: 1.9235236644744873
Epoch 40, training loss: 419.9889831542969 = 1.899509310722351 + 50.0 * 8.36178970336914
Epoch 40, val loss: 1.906233549118042
Epoch 50, training loss: 384.87994384765625 = 1.8793553113937378 + 50.0 * 7.6600117683410645
Epoch 50, val loss: 1.8862416744232178
Epoch 60, training loss: 376.15338134765625 = 1.8613080978393555 + 50.0 * 7.485841751098633
Epoch 60, val loss: 1.8695383071899414
Epoch 70, training loss: 364.4114074707031 = 1.848477840423584 + 50.0 * 7.251258373260498
Epoch 70, val loss: 1.8575541973114014
Epoch 80, training loss: 355.25946044921875 = 1.8376116752624512 + 50.0 * 7.068437099456787
Epoch 80, val loss: 1.846934199333191
Epoch 90, training loss: 349.2734680175781 = 1.8291425704956055 + 50.0 * 6.948886871337891
Epoch 90, val loss: 1.8383879661560059
Epoch 100, training loss: 344.4468994140625 = 1.821031093597412 + 50.0 * 6.852517127990723
Epoch 100, val loss: 1.8303351402282715
Epoch 110, training loss: 339.697021484375 = 1.8141037225723267 + 50.0 * 6.757658004760742
Epoch 110, val loss: 1.8235232830047607
Epoch 120, training loss: 336.5741882324219 = 1.8079936504364014 + 50.0 * 6.695323944091797
Epoch 120, val loss: 1.8171440362930298
Epoch 130, training loss: 334.2440185546875 = 1.8011001348495483 + 50.0 * 6.648858547210693
Epoch 130, val loss: 1.8099645376205444
Epoch 140, training loss: 332.5813903808594 = 1.7934117317199707 + 50.0 * 6.615759372711182
Epoch 140, val loss: 1.8022030591964722
Epoch 150, training loss: 331.21502685546875 = 1.7855793237686157 + 50.0 * 6.588588714599609
Epoch 150, val loss: 1.7944915294647217
Epoch 160, training loss: 329.9924621582031 = 1.7780512571334839 + 50.0 * 6.56428861618042
Epoch 160, val loss: 1.7870208024978638
Epoch 170, training loss: 328.7564697265625 = 1.7703734636306763 + 50.0 * 6.539721488952637
Epoch 170, val loss: 1.7796093225479126
Epoch 180, training loss: 327.9637451171875 = 1.7622227668762207 + 50.0 * 6.5240302085876465
Epoch 180, val loss: 1.7719848155975342
Epoch 190, training loss: 326.8789978027344 = 1.7531553506851196 + 50.0 * 6.502517223358154
Epoch 190, val loss: 1.7637521028518677
Epoch 200, training loss: 326.0906677246094 = 1.7433664798736572 + 50.0 * 6.486946105957031
Epoch 200, val loss: 1.7550386190414429
Epoch 210, training loss: 325.4110412597656 = 1.732783317565918 + 50.0 * 6.473565101623535
Epoch 210, val loss: 1.7456756830215454
Epoch 220, training loss: 325.0553283691406 = 1.7209668159484863 + 50.0 * 6.466687202453613
Epoch 220, val loss: 1.7354683876037598
Epoch 230, training loss: 324.35919189453125 = 1.7082576751708984 + 50.0 * 6.453018665313721
Epoch 230, val loss: 1.724397897720337
Epoch 240, training loss: 323.81475830078125 = 1.6944636106491089 + 50.0 * 6.442405700683594
Epoch 240, val loss: 1.7125691175460815
Epoch 250, training loss: 323.3148498535156 = 1.6796791553497314 + 50.0 * 6.432703495025635
Epoch 250, val loss: 1.700015664100647
Epoch 260, training loss: 323.1842346191406 = 1.663804054260254 + 50.0 * 6.430408477783203
Epoch 260, val loss: 1.6866966485977173
Epoch 270, training loss: 322.4845275878906 = 1.6469638347625732 + 50.0 * 6.416751384735107
Epoch 270, val loss: 1.6725879907608032
Epoch 280, training loss: 322.12506103515625 = 1.6291987895965576 + 50.0 * 6.409916877746582
Epoch 280, val loss: 1.6578861474990845
Epoch 290, training loss: 321.8504943847656 = 1.61051344871521 + 50.0 * 6.404799938201904
Epoch 290, val loss: 1.6426173448562622
Epoch 300, training loss: 321.4141540527344 = 1.5909873247146606 + 50.0 * 6.396463394165039
Epoch 300, val loss: 1.6267515420913696
Epoch 310, training loss: 321.0788879394531 = 1.5708366632461548 + 50.0 * 6.390161037445068
Epoch 310, val loss: 1.6105848550796509
Epoch 320, training loss: 320.90167236328125 = 1.5498684644699097 + 50.0 * 6.387035846710205
Epoch 320, val loss: 1.5940426588058472
Epoch 330, training loss: 320.4978942871094 = 1.5285663604736328 + 50.0 * 6.3793864250183105
Epoch 330, val loss: 1.5773721933364868
Epoch 340, training loss: 320.2072448730469 = 1.506807565689087 + 50.0 * 6.374008655548096
Epoch 340, val loss: 1.5606008768081665
Epoch 350, training loss: 319.9307861328125 = 1.4847614765167236 + 50.0 * 6.36892032623291
Epoch 350, val loss: 1.5439138412475586
Epoch 360, training loss: 320.1512145996094 = 1.4623907804489136 + 50.0 * 6.373775959014893
Epoch 360, val loss: 1.52711021900177
Epoch 370, training loss: 319.5752258300781 = 1.4397315979003906 + 50.0 * 6.362709999084473
Epoch 370, val loss: 1.510454773902893
Epoch 380, training loss: 319.2327575683594 = 1.4170849323272705 + 50.0 * 6.356313705444336
Epoch 380, val loss: 1.4940059185028076
Epoch 390, training loss: 319.02911376953125 = 1.3944169282913208 + 50.0 * 6.352694034576416
Epoch 390, val loss: 1.477723240852356
Epoch 400, training loss: 318.8677062988281 = 1.3715273141860962 + 50.0 * 6.349923610687256
Epoch 400, val loss: 1.461389183998108
Epoch 410, training loss: 318.6968078613281 = 1.3486151695251465 + 50.0 * 6.346963882446289
Epoch 410, val loss: 1.4453548192977905
Epoch 420, training loss: 318.4592590332031 = 1.325728178024292 + 50.0 * 6.342670917510986
Epoch 420, val loss: 1.4296818971633911
Epoch 430, training loss: 318.27825927734375 = 1.3029975891113281 + 50.0 * 6.339504718780518
Epoch 430, val loss: 1.4142637252807617
Epoch 440, training loss: 318.1353759765625 = 1.2801525592803955 + 50.0 * 6.337104320526123
Epoch 440, val loss: 1.3990954160690308
Epoch 450, training loss: 318.01806640625 = 1.257484793663025 + 50.0 * 6.335211753845215
Epoch 450, val loss: 1.3841302394866943
Epoch 460, training loss: 317.805908203125 = 1.2350752353668213 + 50.0 * 6.331416606903076
Epoch 460, val loss: 1.369832992553711
Epoch 470, training loss: 317.7313232421875 = 1.2129100561141968 + 50.0 * 6.3303680419921875
Epoch 470, val loss: 1.3559105396270752
Epoch 480, training loss: 317.4954528808594 = 1.1908504962921143 + 50.0 * 6.326091766357422
Epoch 480, val loss: 1.3421269655227661
Epoch 490, training loss: 317.47760009765625 = 1.1691455841064453 + 50.0 * 6.326169013977051
Epoch 490, val loss: 1.3291420936584473
Epoch 500, training loss: 317.38525390625 = 1.1476380825042725 + 50.0 * 6.324752330780029
Epoch 500, val loss: 1.3166476488113403
Epoch 510, training loss: 317.0771179199219 = 1.1267147064208984 + 50.0 * 6.3190083503723145
Epoch 510, val loss: 1.304488182067871
Epoch 520, training loss: 316.9322204589844 = 1.1061772108078003 + 50.0 * 6.316520690917969
Epoch 520, val loss: 1.293043613433838
Epoch 530, training loss: 317.1326904296875 = 1.0859689712524414 + 50.0 * 6.320934295654297
Epoch 530, val loss: 1.2819675207138062
Epoch 540, training loss: 316.7253723144531 = 1.0663965940475464 + 50.0 * 6.3131794929504395
Epoch 540, val loss: 1.2721656560897827
Epoch 550, training loss: 316.5921936035156 = 1.0472495555877686 + 50.0 * 6.310898780822754
Epoch 550, val loss: 1.2626475095748901
Epoch 560, training loss: 316.7481384277344 = 1.0286202430725098 + 50.0 * 6.314390182495117
Epoch 560, val loss: 1.253482460975647
Epoch 570, training loss: 316.36895751953125 = 1.0103620290756226 + 50.0 * 6.30717134475708
Epoch 570, val loss: 1.245326280593872
Epoch 580, training loss: 316.2032165527344 = 0.9927955865859985 + 50.0 * 6.304208278656006
Epoch 580, val loss: 1.2377523183822632
Epoch 590, training loss: 316.2217712402344 = 0.9756969809532166 + 50.0 * 6.3049211502075195
Epoch 590, val loss: 1.230559229850769
Epoch 600, training loss: 316.05755615234375 = 0.9589686989784241 + 50.0 * 6.301971912384033
Epoch 600, val loss: 1.2241414785385132
Epoch 610, training loss: 315.88336181640625 = 0.9428513646125793 + 50.0 * 6.2988104820251465
Epoch 610, val loss: 1.218068242073059
Epoch 620, training loss: 316.23333740234375 = 0.9271854758262634 + 50.0 * 6.306122779846191
Epoch 620, val loss: 1.2125715017318726
Epoch 630, training loss: 315.662109375 = 0.9117253422737122 + 50.0 * 6.295008182525635
Epoch 630, val loss: 1.207515835762024
Epoch 640, training loss: 315.5736389160156 = 0.896834671497345 + 50.0 * 6.293536186218262
Epoch 640, val loss: 1.203132152557373
Epoch 650, training loss: 315.47808837890625 = 0.8824336528778076 + 50.0 * 6.29191255569458
Epoch 650, val loss: 1.1993043422698975
Epoch 660, training loss: 315.6721496582031 = 0.8683428764343262 + 50.0 * 6.29607629776001
Epoch 660, val loss: 1.1954940557479858
Epoch 670, training loss: 315.31170654296875 = 0.8544982075691223 + 50.0 * 6.289144515991211
Epoch 670, val loss: 1.192471981048584
Epoch 680, training loss: 315.28839111328125 = 0.8409649729728699 + 50.0 * 6.2889485359191895
Epoch 680, val loss: 1.1893877983093262
Epoch 690, training loss: 315.0785217285156 = 0.827784538269043 + 50.0 * 6.285014629364014
Epoch 690, val loss: 1.18690025806427
Epoch 700, training loss: 315.1422424316406 = 0.8149688839912415 + 50.0 * 6.286545276641846
Epoch 700, val loss: 1.1849807500839233
Epoch 710, training loss: 314.9573669433594 = 0.8023834824562073 + 50.0 * 6.283099174499512
Epoch 710, val loss: 1.1829198598861694
Epoch 720, training loss: 314.9969787597656 = 0.7899578809738159 + 50.0 * 6.284140586853027
Epoch 720, val loss: 1.1814531087875366
Epoch 730, training loss: 314.8637390136719 = 0.7778725028038025 + 50.0 * 6.281717777252197
Epoch 730, val loss: 1.1800659894943237
Epoch 740, training loss: 314.9349365234375 = 0.7659843564033508 + 50.0 * 6.283379077911377
Epoch 740, val loss: 1.179315447807312
Epoch 750, training loss: 314.70062255859375 = 0.7541376352310181 + 50.0 * 6.278929710388184
Epoch 750, val loss: 1.1775699853897095
Epoch 760, training loss: 314.58685302734375 = 0.7427236437797546 + 50.0 * 6.276882171630859
Epoch 760, val loss: 1.1775180101394653
Epoch 770, training loss: 314.5946350097656 = 0.7314083576202393 + 50.0 * 6.27726411819458
Epoch 770, val loss: 1.176788568496704
Epoch 780, training loss: 314.45599365234375 = 0.7202176451683044 + 50.0 * 6.274715423583984
Epoch 780, val loss: 1.1767380237579346
Epoch 790, training loss: 314.427490234375 = 0.7091770172119141 + 50.0 * 6.27436637878418
Epoch 790, val loss: 1.1765216588974
Epoch 800, training loss: 314.2923889160156 = 0.6983457803726196 + 50.0 * 6.271881103515625
Epoch 800, val loss: 1.1766629219055176
Epoch 810, training loss: 314.2583312988281 = 0.687747597694397 + 50.0 * 6.271411895751953
Epoch 810, val loss: 1.1769551038742065
Epoch 820, training loss: 314.4057922363281 = 0.6771817207336426 + 50.0 * 6.274572372436523
Epoch 820, val loss: 1.1774214506149292
Epoch 830, training loss: 314.2606506347656 = 0.6664509177207947 + 50.0 * 6.271883964538574
Epoch 830, val loss: 1.1771972179412842
Epoch 840, training loss: 314.0343322753906 = 0.6560778021812439 + 50.0 * 6.2675652503967285
Epoch 840, val loss: 1.1778749227523804
Epoch 850, training loss: 313.9869384765625 = 0.6459357142448425 + 50.0 * 6.266819953918457
Epoch 850, val loss: 1.1786901950836182
Epoch 860, training loss: 314.1777038574219 = 0.6359604597091675 + 50.0 * 6.270834922790527
Epoch 860, val loss: 1.1794872283935547
Epoch 870, training loss: 314.01220703125 = 0.6258192658424377 + 50.0 * 6.267727851867676
Epoch 870, val loss: 1.1803977489471436
Epoch 880, training loss: 313.85992431640625 = 0.6159110069274902 + 50.0 * 6.264880657196045
Epoch 880, val loss: 1.1813451051712036
Epoch 890, training loss: 313.7483825683594 = 0.6061407923698425 + 50.0 * 6.262845039367676
Epoch 890, val loss: 1.1824615001678467
Epoch 900, training loss: 313.7627258300781 = 0.5965171456336975 + 50.0 * 6.26332426071167
Epoch 900, val loss: 1.1837347745895386
Epoch 910, training loss: 313.68414306640625 = 0.5868302583694458 + 50.0 * 6.261946678161621
Epoch 910, val loss: 1.184883952140808
Epoch 920, training loss: 313.6931457519531 = 0.5771532654762268 + 50.0 * 6.262320041656494
Epoch 920, val loss: 1.18612802028656
Epoch 930, training loss: 313.6137390136719 = 0.567674458026886 + 50.0 * 6.260921001434326
Epoch 930, val loss: 1.1874514818191528
Epoch 940, training loss: 313.624755859375 = 0.5582833290100098 + 50.0 * 6.261329174041748
Epoch 940, val loss: 1.188960313796997
Epoch 950, training loss: 313.4526672363281 = 0.5488584637641907 + 50.0 * 6.258076190948486
Epoch 950, val loss: 1.1900643110275269
Epoch 960, training loss: 313.58770751953125 = 0.5395605564117432 + 50.0 * 6.260963439941406
Epoch 960, val loss: 1.1911563873291016
Epoch 970, training loss: 313.57489013671875 = 0.5302160978317261 + 50.0 * 6.26089334487915
Epoch 970, val loss: 1.1933598518371582
Epoch 980, training loss: 313.4049072265625 = 0.5208660364151001 + 50.0 * 6.257680892944336
Epoch 980, val loss: 1.1941745281219482
Epoch 990, training loss: 313.3795471191406 = 0.5116525292396545 + 50.0 * 6.257358074188232
Epoch 990, val loss: 1.1961498260498047
Epoch 1000, training loss: 313.25201416015625 = 0.5025047063827515 + 50.0 * 6.254990100860596
Epoch 1000, val loss: 1.197495698928833
Epoch 1010, training loss: 313.2068176269531 = 0.4934707581996918 + 50.0 * 6.254266738891602
Epoch 1010, val loss: 1.1991806030273438
Epoch 1020, training loss: 313.4552307128906 = 0.48447152972221375 + 50.0 * 6.259415149688721
Epoch 1020, val loss: 1.2003705501556396
Epoch 1030, training loss: 313.3685607910156 = 0.4753841459751129 + 50.0 * 6.257863521575928
Epoch 1030, val loss: 1.2027456760406494
Epoch 1040, training loss: 313.13385009765625 = 0.466493159532547 + 50.0 * 6.253347396850586
Epoch 1040, val loss: 1.2040958404541016
Epoch 1050, training loss: 313.0881652832031 = 0.45769497752189636 + 50.0 * 6.2526092529296875
Epoch 1050, val loss: 1.2062022686004639
Epoch 1060, training loss: 313.029052734375 = 0.44892820715904236 + 50.0 * 6.251602649688721
Epoch 1060, val loss: 1.2079131603240967
Epoch 1070, training loss: 313.07403564453125 = 0.44023701548576355 + 50.0 * 6.252675533294678
Epoch 1070, val loss: 1.2096762657165527
Epoch 1080, training loss: 312.96710205078125 = 0.4316483736038208 + 50.0 * 6.250709056854248
Epoch 1080, val loss: 1.2118752002716064
Epoch 1090, training loss: 312.9314880371094 = 0.4231148958206177 + 50.0 * 6.250167369842529
Epoch 1090, val loss: 1.2139480113983154
Epoch 1100, training loss: 312.908203125 = 0.41457507014274597 + 50.0 * 6.249872207641602
Epoch 1100, val loss: 1.215603232383728
Epoch 1110, training loss: 312.8495178222656 = 0.40608835220336914 + 50.0 * 6.248868465423584
Epoch 1110, val loss: 1.2179468870162964
Epoch 1120, training loss: 312.764404296875 = 0.3978632092475891 + 50.0 * 6.247330665588379
Epoch 1120, val loss: 1.2199763059616089
Epoch 1130, training loss: 312.69818115234375 = 0.38976991176605225 + 50.0 * 6.24616813659668
Epoch 1130, val loss: 1.2223483324050903
Epoch 1140, training loss: 312.85186767578125 = 0.38179299235343933 + 50.0 * 6.249401092529297
Epoch 1140, val loss: 1.2250044345855713
Epoch 1150, training loss: 312.9173889160156 = 0.37375912070274353 + 50.0 * 6.250872611999512
Epoch 1150, val loss: 1.22706139087677
Epoch 1160, training loss: 312.6385192871094 = 0.36574333906173706 + 50.0 * 6.245455265045166
Epoch 1160, val loss: 1.229365587234497
Epoch 1170, training loss: 312.60009765625 = 0.3580649793148041 + 50.0 * 6.244840621948242
Epoch 1170, val loss: 1.2321200370788574
Epoch 1180, training loss: 312.5199279785156 = 0.3505658805370331 + 50.0 * 6.243387699127197
Epoch 1180, val loss: 1.2350635528564453
Epoch 1190, training loss: 312.9314880371094 = 0.3431680500507355 + 50.0 * 6.251766204833984
Epoch 1190, val loss: 1.2377341985702515
Epoch 1200, training loss: 312.6893005371094 = 0.3357904255390167 + 50.0 * 6.2470703125
Epoch 1200, val loss: 1.2403875589370728
Epoch 1210, training loss: 312.4645690917969 = 0.328511118888855 + 50.0 * 6.242721080780029
Epoch 1210, val loss: 1.2434824705123901
Epoch 1220, training loss: 312.3892517089844 = 0.32152554392814636 + 50.0 * 6.241354465484619
Epoch 1220, val loss: 1.2468427419662476
Epoch 1230, training loss: 312.4819641113281 = 0.3146668076515198 + 50.0 * 6.243346214294434
Epoch 1230, val loss: 1.2498393058776855
Epoch 1240, training loss: 312.373779296875 = 0.30787843465805054 + 50.0 * 6.2413177490234375
Epoch 1240, val loss: 1.2532395124435425
Epoch 1250, training loss: 312.4877014160156 = 0.30122074484825134 + 50.0 * 6.243729591369629
Epoch 1250, val loss: 1.256597876548767
Epoch 1260, training loss: 312.3027648925781 = 0.2947099208831787 + 50.0 * 6.240161418914795
Epoch 1260, val loss: 1.260306715965271
Epoch 1270, training loss: 312.3484802246094 = 0.2883740961551666 + 50.0 * 6.241202354431152
Epoch 1270, val loss: 1.2638262510299683
Epoch 1280, training loss: 312.3356628417969 = 0.2822198271751404 + 50.0 * 6.2410688400268555
Epoch 1280, val loss: 1.267647624015808
Epoch 1290, training loss: 312.2255554199219 = 0.2761622667312622 + 50.0 * 6.238987922668457
Epoch 1290, val loss: 1.2718642950057983
Epoch 1300, training loss: 312.2481689453125 = 0.27028152346611023 + 50.0 * 6.23955774307251
Epoch 1300, val loss: 1.2760475873947144
Epoch 1310, training loss: 312.3039855957031 = 0.2645144760608673 + 50.0 * 6.240789413452148
Epoch 1310, val loss: 1.279728889465332
Epoch 1320, training loss: 312.16070556640625 = 0.25883859395980835 + 50.0 * 6.238037109375
Epoch 1320, val loss: 1.2835016250610352
Epoch 1330, training loss: 312.1489562988281 = 0.2532961368560791 + 50.0 * 6.237913131713867
Epoch 1330, val loss: 1.2878317832946777
Epoch 1340, training loss: 312.2684020996094 = 0.24794231355190277 + 50.0 * 6.240408897399902
Epoch 1340, val loss: 1.2918519973754883
Epoch 1350, training loss: 312.2308044433594 = 0.2425895631313324 + 50.0 * 6.239764213562012
Epoch 1350, val loss: 1.295668363571167
Epoch 1360, training loss: 312.05401611328125 = 0.2374533861875534 + 50.0 * 6.236331462860107
Epoch 1360, val loss: 1.3005549907684326
Epoch 1370, training loss: 311.99371337890625 = 0.23244641721248627 + 50.0 * 6.235225200653076
Epoch 1370, val loss: 1.3047987222671509
Epoch 1380, training loss: 311.9973449707031 = 0.22762033343315125 + 50.0 * 6.235394477844238
Epoch 1380, val loss: 1.309583067893982
Epoch 1390, training loss: 312.1895751953125 = 0.22286073863506317 + 50.0 * 6.2393341064453125
Epoch 1390, val loss: 1.3141930103302002
Epoch 1400, training loss: 312.22344970703125 = 0.21806904673576355 + 50.0 * 6.240107536315918
Epoch 1400, val loss: 1.3182557821273804
Epoch 1410, training loss: 311.9718017578125 = 0.21345384418964386 + 50.0 * 6.235167026519775
Epoch 1410, val loss: 1.3223588466644287
Epoch 1420, training loss: 311.8702392578125 = 0.20898781716823578 + 50.0 * 6.233225345611572
Epoch 1420, val loss: 1.3275901079177856
Epoch 1430, training loss: 311.9152526855469 = 0.20465824007987976 + 50.0 * 6.2342119216918945
Epoch 1430, val loss: 1.3323280811309814
Epoch 1440, training loss: 311.9091796875 = 0.20040182769298553 + 50.0 * 6.234175682067871
Epoch 1440, val loss: 1.3370444774627686
Epoch 1450, training loss: 311.88909912109375 = 0.1962345391511917 + 50.0 * 6.23385763168335
Epoch 1450, val loss: 1.3418490886688232
Epoch 1460, training loss: 311.83612060546875 = 0.1921730786561966 + 50.0 * 6.232879161834717
Epoch 1460, val loss: 1.3469055891036987
Epoch 1470, training loss: 312.0075988769531 = 0.18819522857666016 + 50.0 * 6.236387729644775
Epoch 1470, val loss: 1.3519480228424072
Epoch 1480, training loss: 311.75262451171875 = 0.18420982360839844 + 50.0 * 6.231368541717529
Epoch 1480, val loss: 1.3568707704544067
Epoch 1490, training loss: 311.7149963378906 = 0.18044984340667725 + 50.0 * 6.230690956115723
Epoch 1490, val loss: 1.3621132373809814
Epoch 1500, training loss: 311.6831970214844 = 0.17677661776542664 + 50.0 * 6.230128288269043
Epoch 1500, val loss: 1.3673776388168335
Epoch 1510, training loss: 311.9117431640625 = 0.17320501804351807 + 50.0 * 6.23477029800415
Epoch 1510, val loss: 1.37226140499115
Epoch 1520, training loss: 311.7483215332031 = 0.16963611543178558 + 50.0 * 6.231573581695557
Epoch 1520, val loss: 1.3774409294128418
Epoch 1530, training loss: 311.9253845214844 = 0.1661185324192047 + 50.0 * 6.235185623168945
Epoch 1530, val loss: 1.3822627067565918
Epoch 1540, training loss: 311.6963195800781 = 0.1627291589975357 + 50.0 * 6.2306718826293945
Epoch 1540, val loss: 1.3885443210601807
Epoch 1550, training loss: 311.6253356933594 = 0.15940439701080322 + 50.0 * 6.229319095611572
Epoch 1550, val loss: 1.3933037519454956
Epoch 1560, training loss: 311.7994689941406 = 0.1562473177909851 + 50.0 * 6.2328643798828125
Epoch 1560, val loss: 1.3990923166275024
Epoch 1570, training loss: 311.5832824707031 = 0.15305261313915253 + 50.0 * 6.228604793548584
Epoch 1570, val loss: 1.404401421546936
Epoch 1580, training loss: 311.5337219238281 = 0.14998847246170044 + 50.0 * 6.22767448425293
Epoch 1580, val loss: 1.4098174571990967
Epoch 1590, training loss: 311.654296875 = 0.14702002704143524 + 50.0 * 6.230145454406738
Epoch 1590, val loss: 1.4149606227874756
Epoch 1600, training loss: 311.5304870605469 = 0.14405158162117004 + 50.0 * 6.227728843688965
Epoch 1600, val loss: 1.4207209348678589
Epoch 1610, training loss: 311.5566711425781 = 0.1411985456943512 + 50.0 * 6.228309154510498
Epoch 1610, val loss: 1.426729440689087
Epoch 1620, training loss: 311.6247863769531 = 0.13841700553894043 + 50.0 * 6.229727268218994
Epoch 1620, val loss: 1.4320958852767944
Epoch 1630, training loss: 311.4461975097656 = 0.13563837110996246 + 50.0 * 6.226211071014404
Epoch 1630, val loss: 1.4370968341827393
Epoch 1640, training loss: 311.45263671875 = 0.13297998905181885 + 50.0 * 6.226393699645996
Epoch 1640, val loss: 1.442812204360962
Epoch 1650, training loss: 311.6558837890625 = 0.13037051260471344 + 50.0 * 6.230510234832764
Epoch 1650, val loss: 1.448384404182434
Epoch 1660, training loss: 311.5686950683594 = 0.12779328227043152 + 50.0 * 6.228817462921143
Epoch 1660, val loss: 1.4544142484664917
Epoch 1670, training loss: 311.4405517578125 = 0.12527787685394287 + 50.0 * 6.2263054847717285
Epoch 1670, val loss: 1.4597561359405518
Epoch 1680, training loss: 311.40032958984375 = 0.12285088002681732 + 50.0 * 6.225549221038818
Epoch 1680, val loss: 1.4657272100448608
Epoch 1690, training loss: 311.4797668457031 = 0.12049774080514908 + 50.0 * 6.2271857261657715
Epoch 1690, val loss: 1.4715425968170166
Epoch 1700, training loss: 311.5028381347656 = 0.11813724040985107 + 50.0 * 6.227694034576416
Epoch 1700, val loss: 1.477428674697876
Epoch 1710, training loss: 311.3316650390625 = 0.11583742499351501 + 50.0 * 6.224316596984863
Epoch 1710, val loss: 1.4825507402420044
Epoch 1720, training loss: 311.3902282714844 = 0.1136055439710617 + 50.0 * 6.225532531738281
Epoch 1720, val loss: 1.4881583452224731
Epoch 1730, training loss: 311.3799133300781 = 0.11143643409013748 + 50.0 * 6.225369453430176
Epoch 1730, val loss: 1.494224190711975
Epoch 1740, training loss: 311.3936767578125 = 0.10932096093893051 + 50.0 * 6.225687503814697
Epoch 1740, val loss: 1.5005357265472412
Epoch 1750, training loss: 311.2777404785156 = 0.10721352696418762 + 50.0 * 6.223410606384277
Epoch 1750, val loss: 1.505685806274414
Epoch 1760, training loss: 311.29974365234375 = 0.10518791526556015 + 50.0 * 6.223890781402588
Epoch 1760, val loss: 1.5112277269363403
Epoch 1770, training loss: 311.3435363769531 = 0.10320231318473816 + 50.0 * 6.224806308746338
Epoch 1770, val loss: 1.5174037218093872
Epoch 1780, training loss: 311.34002685546875 = 0.10125605016946793 + 50.0 * 6.224775314331055
Epoch 1780, val loss: 1.523627519607544
Epoch 1790, training loss: 311.363037109375 = 0.09932583570480347 + 50.0 * 6.225274085998535
Epoch 1790, val loss: 1.5286420583724976
Epoch 1800, training loss: 311.23095703125 = 0.09744811058044434 + 50.0 * 6.222670555114746
Epoch 1800, val loss: 1.534806251525879
Epoch 1810, training loss: 311.1731262207031 = 0.09564606845378876 + 50.0 * 6.2215495109558105
Epoch 1810, val loss: 1.5405603647232056
Epoch 1820, training loss: 311.1554870605469 = 0.09387700259685516 + 50.0 * 6.2212324142456055
Epoch 1820, val loss: 1.5463963747024536
Epoch 1830, training loss: 311.2009582519531 = 0.09218035638332367 + 50.0 * 6.222175598144531
Epoch 1830, val loss: 1.5521892309188843
Epoch 1840, training loss: 311.2341613769531 = 0.09046535193920135 + 50.0 * 6.222874164581299
Epoch 1840, val loss: 1.5579733848571777
Epoch 1850, training loss: 311.16943359375 = 0.08875501155853271 + 50.0 * 6.221613883972168
Epoch 1850, val loss: 1.5635974407196045
Epoch 1860, training loss: 311.1380615234375 = 0.08712590485811234 + 50.0 * 6.2210187911987305
Epoch 1860, val loss: 1.5689446926116943
Epoch 1870, training loss: 311.1822814941406 = 0.08554647117853165 + 50.0 * 6.221934795379639
Epoch 1870, val loss: 1.575239658355713
Epoch 1880, training loss: 311.2581481933594 = 0.08398956805467606 + 50.0 * 6.223483562469482
Epoch 1880, val loss: 1.581063985824585
Epoch 1890, training loss: 311.09521484375 = 0.08242239058017731 + 50.0 * 6.2202558517456055
Epoch 1890, val loss: 1.5866292715072632
Epoch 1900, training loss: 311.0284423828125 = 0.0809408500790596 + 50.0 * 6.218950271606445
Epoch 1900, val loss: 1.5921486616134644
Epoch 1910, training loss: 311.0159912109375 = 0.07952070981264114 + 50.0 * 6.218729019165039
Epoch 1910, val loss: 1.5981842279434204
Epoch 1920, training loss: 311.24371337890625 = 0.07814984023571014 + 50.0 * 6.223310947418213
Epoch 1920, val loss: 1.6043660640716553
Epoch 1930, training loss: 311.0636901855469 = 0.07670798152685165 + 50.0 * 6.2197394371032715
Epoch 1930, val loss: 1.6089097261428833
Epoch 1940, training loss: 311.10162353515625 = 0.07532867044210434 + 50.0 * 6.220526218414307
Epoch 1940, val loss: 1.615486741065979
Epoch 1950, training loss: 311.032470703125 = 0.07398578524589539 + 50.0 * 6.219169616699219
Epoch 1950, val loss: 1.6203317642211914
Epoch 1960, training loss: 310.9747619628906 = 0.07269904762506485 + 50.0 * 6.21804141998291
Epoch 1960, val loss: 1.6264798641204834
Epoch 1970, training loss: 311.0858459472656 = 0.07144632935523987 + 50.0 * 6.220288276672363
Epoch 1970, val loss: 1.6315160989761353
Epoch 1980, training loss: 311.068115234375 = 0.07017761468887329 + 50.0 * 6.219958782196045
Epoch 1980, val loss: 1.6375852823257446
Epoch 1990, training loss: 310.9947509765625 = 0.06894921511411667 + 50.0 * 6.2185163497924805
Epoch 1990, val loss: 1.64289391040802
Epoch 2000, training loss: 310.9393310546875 = 0.06775937229394913 + 50.0 * 6.217431545257568
Epoch 2000, val loss: 1.648959994316101
Epoch 2010, training loss: 310.9496765136719 = 0.06660503894090652 + 50.0 * 6.217660903930664
Epoch 2010, val loss: 1.6543476581573486
Epoch 2020, training loss: 311.05108642578125 = 0.06548510491847992 + 50.0 * 6.219711780548096
Epoch 2020, val loss: 1.6602002382278442
Epoch 2030, training loss: 310.92669677734375 = 0.06434608995914459 + 50.0 * 6.217247009277344
Epoch 2030, val loss: 1.6656367778778076
Epoch 2040, training loss: 311.0836486816406 = 0.06326799094676971 + 50.0 * 6.220407485961914
Epoch 2040, val loss: 1.6706022024154663
Epoch 2050, training loss: 310.8563537597656 = 0.062159858644008636 + 50.0 * 6.215883731842041
Epoch 2050, val loss: 1.6763607263565063
Epoch 2060, training loss: 310.87347412109375 = 0.06112482026219368 + 50.0 * 6.216247081756592
Epoch 2060, val loss: 1.6818395853042603
Epoch 2070, training loss: 311.253662109375 = 0.06011223793029785 + 50.0 * 6.223870754241943
Epoch 2070, val loss: 1.6869813203811646
Epoch 2080, training loss: 310.9039306640625 = 0.059065595269203186 + 50.0 * 6.216897487640381
Epoch 2080, val loss: 1.6926158666610718
Epoch 2090, training loss: 310.8180847167969 = 0.0580853708088398 + 50.0 * 6.215199947357178
Epoch 2090, val loss: 1.697996735572815
Epoch 2100, training loss: 310.7936706542969 = 0.05714486539363861 + 50.0 * 6.214730262756348
Epoch 2100, val loss: 1.7038493156433105
Epoch 2110, training loss: 310.8320007324219 = 0.056227974593639374 + 50.0 * 6.215515613555908
Epoch 2110, val loss: 1.708968162536621
Epoch 2120, training loss: 311.160400390625 = 0.055303484201431274 + 50.0 * 6.222102165222168
Epoch 2120, val loss: 1.7139463424682617
Epoch 2130, training loss: 310.8125305175781 = 0.05435560271143913 + 50.0 * 6.215163707733154
Epoch 2130, val loss: 1.719495177268982
Epoch 2140, training loss: 310.82122802734375 = 0.053461648523807526 + 50.0 * 6.215354919433594
Epoch 2140, val loss: 1.7251371145248413
Epoch 2150, training loss: 310.75885009765625 = 0.05260534584522247 + 50.0 * 6.21412467956543
Epoch 2150, val loss: 1.7301764488220215
Epoch 2160, training loss: 310.7664794921875 = 0.051789794117212296 + 50.0 * 6.214293956756592
Epoch 2160, val loss: 1.7359386682510376
Epoch 2170, training loss: 311.04400634765625 = 0.05099181458353996 + 50.0 * 6.219860076904297
Epoch 2170, val loss: 1.741072654724121
Epoch 2180, training loss: 310.83306884765625 = 0.05012520030140877 + 50.0 * 6.215658664703369
Epoch 2180, val loss: 1.7459684610366821
Epoch 2190, training loss: 310.80694580078125 = 0.049341198056936264 + 50.0 * 6.215152263641357
Epoch 2190, val loss: 1.7513108253479004
Epoch 2200, training loss: 310.9105224609375 = 0.0485474094748497 + 50.0 * 6.2172393798828125
Epoch 2200, val loss: 1.7562532424926758
Epoch 2210, training loss: 310.8370666503906 = 0.04778752103447914 + 50.0 * 6.215785503387451
Epoch 2210, val loss: 1.762419581413269
Epoch 2220, training loss: 310.81689453125 = 0.04702456668019295 + 50.0 * 6.215397357940674
Epoch 2220, val loss: 1.7668139934539795
Epoch 2230, training loss: 310.6810607910156 = 0.04629509896039963 + 50.0 * 6.212695598602295
Epoch 2230, val loss: 1.77228844165802
Epoch 2240, training loss: 310.76025390625 = 0.04559410363435745 + 50.0 * 6.214293479919434
Epoch 2240, val loss: 1.7773419618606567
Epoch 2250, training loss: 310.8593444824219 = 0.04490239545702934 + 50.0 * 6.216289043426514
Epoch 2250, val loss: 1.7828125953674316
Epoch 2260, training loss: 310.719482421875 = 0.04418560862541199 + 50.0 * 6.213505744934082
Epoch 2260, val loss: 1.7874517440795898
Epoch 2270, training loss: 310.66900634765625 = 0.04351022467017174 + 50.0 * 6.212509632110596
Epoch 2270, val loss: 1.7921308279037476
Epoch 2280, training loss: 310.7087097167969 = 0.042860474437475204 + 50.0 * 6.213317394256592
Epoch 2280, val loss: 1.797546148300171
Epoch 2290, training loss: 310.7403259277344 = 0.04222278669476509 + 50.0 * 6.213962078094482
Epoch 2290, val loss: 1.8023875951766968
Epoch 2300, training loss: 310.7120666503906 = 0.04157698526978493 + 50.0 * 6.213409900665283
Epoch 2300, val loss: 1.8075370788574219
Epoch 2310, training loss: 310.6776123046875 = 0.0409441813826561 + 50.0 * 6.212733268737793
Epoch 2310, val loss: 1.8127659559249878
Epoch 2320, training loss: 310.7247009277344 = 0.0403473898768425 + 50.0 * 6.213687419891357
Epoch 2320, val loss: 1.8170350790023804
Epoch 2330, training loss: 310.7481689453125 = 0.03973815590143204 + 50.0 * 6.214168548583984
Epoch 2330, val loss: 1.822396159172058
Epoch 2340, training loss: 310.683837890625 = 0.03914545476436615 + 50.0 * 6.212893962860107
Epoch 2340, val loss: 1.8274015188217163
Epoch 2350, training loss: 310.72869873046875 = 0.0385771319270134 + 50.0 * 6.213802337646484
Epoch 2350, val loss: 1.8323739767074585
Epoch 2360, training loss: 310.64544677734375 = 0.03801173344254494 + 50.0 * 6.212148189544678
Epoch 2360, val loss: 1.8371644020080566
Epoch 2370, training loss: 310.64471435546875 = 0.037453874945640564 + 50.0 * 6.2121453285217285
Epoch 2370, val loss: 1.8418033123016357
Epoch 2380, training loss: 310.5707702636719 = 0.03691292181611061 + 50.0 * 6.210677146911621
Epoch 2380, val loss: 1.8464921712875366
Epoch 2390, training loss: 310.59393310546875 = 0.036392614245414734 + 50.0 * 6.211150646209717
Epoch 2390, val loss: 1.8512126207351685
Epoch 2400, training loss: 310.5335693359375 = 0.03587299957871437 + 50.0 * 6.209954261779785
Epoch 2400, val loss: 1.856040358543396
Epoch 2410, training loss: 310.6312255859375 = 0.035370904952287674 + 50.0 * 6.211916923522949
Epoch 2410, val loss: 1.8605941534042358
Epoch 2420, training loss: 310.63311767578125 = 0.034856557846069336 + 50.0 * 6.211965084075928
Epoch 2420, val loss: 1.8653064966201782
Epoch 2430, training loss: 310.52862548828125 = 0.03435212001204491 + 50.0 * 6.209885120391846
Epoch 2430, val loss: 1.8698368072509766
Epoch 2440, training loss: 310.68438720703125 = 0.03387770429253578 + 50.0 * 6.213010311126709
Epoch 2440, val loss: 1.8750406503677368
Epoch 2450, training loss: 310.5660400390625 = 0.03338630869984627 + 50.0 * 6.210653305053711
Epoch 2450, val loss: 1.8791362047195435
Epoch 2460, training loss: 310.506591796875 = 0.032920464873313904 + 50.0 * 6.209473133087158
Epoch 2460, val loss: 1.8839402198791504
Epoch 2470, training loss: 310.5156555175781 = 0.032483190298080444 + 50.0 * 6.2096638679504395
Epoch 2470, val loss: 1.8886600732803345
Epoch 2480, training loss: 310.87066650390625 = 0.03205903619527817 + 50.0 * 6.216771602630615
Epoch 2480, val loss: 1.8933405876159668
Epoch 2490, training loss: 310.5476379394531 = 0.03158143162727356 + 50.0 * 6.210320949554443
Epoch 2490, val loss: 1.8973073959350586
Epoch 2500, training loss: 310.4589538574219 = 0.03115188702940941 + 50.0 * 6.208555698394775
Epoch 2500, val loss: 1.9022988080978394
Epoch 2510, training loss: 310.4162902832031 = 0.030741721391677856 + 50.0 * 6.207711219787598
Epoch 2510, val loss: 1.9067448377609253
Epoch 2520, training loss: 310.433837890625 = 0.030343180522322655 + 50.0 * 6.208069801330566
Epoch 2520, val loss: 1.9113126993179321
Epoch 2530, training loss: 310.9693603515625 = 0.029952960088849068 + 50.0 * 6.2187886238098145
Epoch 2530, val loss: 1.916093111038208
Epoch 2540, training loss: 310.6366271972656 = 0.029533326625823975 + 50.0 * 6.212141990661621
Epoch 2540, val loss: 1.920091152191162
Epoch 2550, training loss: 310.5195617675781 = 0.0291239432990551 + 50.0 * 6.209808826446533
Epoch 2550, val loss: 1.9245874881744385
Epoch 2560, training loss: 310.4540710449219 = 0.028737032786011696 + 50.0 * 6.2085065841674805
Epoch 2560, val loss: 1.9292168617248535
Epoch 2570, training loss: 310.44256591796875 = 0.02837524749338627 + 50.0 * 6.208283424377441
Epoch 2570, val loss: 1.9334466457366943
Epoch 2580, training loss: 310.64300537109375 = 0.028015853837132454 + 50.0 * 6.212299823760986
Epoch 2580, val loss: 1.9380782842636108
Epoch 2590, training loss: 310.4889831542969 = 0.02763397805392742 + 50.0 * 6.209226608276367
Epoch 2590, val loss: 1.9418590068817139
Epoch 2600, training loss: 310.3689270019531 = 0.027280235663056374 + 50.0 * 6.2068328857421875
Epoch 2600, val loss: 1.9461448192596436
Epoch 2610, training loss: 310.3505859375 = 0.026937345042824745 + 50.0 * 6.206472873687744
Epoch 2610, val loss: 1.950669765472412
Epoch 2620, training loss: 310.5590515136719 = 0.02661089599132538 + 50.0 * 6.210649013519287
Epoch 2620, val loss: 1.9552470445632935
Epoch 2630, training loss: 310.5403137207031 = 0.02626887708902359 + 50.0 * 6.210280895233154
Epoch 2630, val loss: 1.959306001663208
Epoch 2640, training loss: 310.39117431640625 = 0.025903860107064247 + 50.0 * 6.207305431365967
Epoch 2640, val loss: 1.9628427028656006
Epoch 2650, training loss: 310.3770751953125 = 0.025583546608686447 + 50.0 * 6.207029819488525
Epoch 2650, val loss: 1.9673432111740112
Epoch 2660, training loss: 310.4340515136719 = 0.02527090348303318 + 50.0 * 6.2081756591796875
Epoch 2660, val loss: 1.971262812614441
Epoch 2670, training loss: 310.3905944824219 = 0.024961400777101517 + 50.0 * 6.20731258392334
Epoch 2670, val loss: 1.9757319688796997
Epoch 2680, training loss: 310.3175048828125 = 0.024656083434820175 + 50.0 * 6.205856800079346
Epoch 2680, val loss: 1.9796757698059082
Epoch 2690, training loss: 310.4295654296875 = 0.024366440251469612 + 50.0 * 6.208104133605957
Epoch 2690, val loss: 1.983755350112915
Epoch 2700, training loss: 310.4320068359375 = 0.024061385542154312 + 50.0 * 6.20815896987915
Epoch 2700, val loss: 1.987701654434204
Epoch 2710, training loss: 310.3145751953125 = 0.023758843541145325 + 50.0 * 6.20581579208374
Epoch 2710, val loss: 1.9918371438980103
Epoch 2720, training loss: 310.3814697265625 = 0.023477092385292053 + 50.0 * 6.207159996032715
Epoch 2720, val loss: 1.9955966472625732
Epoch 2730, training loss: 310.44903564453125 = 0.02319193072617054 + 50.0 * 6.208517074584961
Epoch 2730, val loss: 1.9994103908538818
Epoch 2740, training loss: 310.3570861816406 = 0.022917982190847397 + 50.0 * 6.20668363571167
Epoch 2740, val loss: 2.004558563232422
Epoch 2750, training loss: 310.5316467285156 = 0.02264883555471897 + 50.0 * 6.210180282592773
Epoch 2750, val loss: 2.007749319076538
Epoch 2760, training loss: 310.32525634765625 = 0.022362152114510536 + 50.0 * 6.206057548522949
Epoch 2760, val loss: 2.0113143920898438
Epoch 2770, training loss: 310.24945068359375 = 0.022101908922195435 + 50.0 * 6.204546928405762
Epoch 2770, val loss: 2.015787363052368
Epoch 2780, training loss: 310.2530212402344 = 0.021854842081665993 + 50.0 * 6.204623699188232
Epoch 2780, val loss: 2.019589900970459
Epoch 2790, training loss: 310.46331787109375 = 0.021615145727992058 + 50.0 * 6.208834171295166
Epoch 2790, val loss: 2.0232136249542236
Epoch 2800, training loss: 310.2379455566406 = 0.021350547671318054 + 50.0 * 6.204331398010254
Epoch 2800, val loss: 2.0270495414733887
Epoch 2810, training loss: 310.329345703125 = 0.021105460822582245 + 50.0 * 6.206164360046387
Epoch 2810, val loss: 2.030869483947754
Epoch 2820, training loss: 310.3097229003906 = 0.020857607945799828 + 50.0 * 6.205777645111084
Epoch 2820, val loss: 2.0342307090759277
Epoch 2830, training loss: 310.28387451171875 = 0.020612942054867744 + 50.0 * 6.205265522003174
Epoch 2830, val loss: 2.0382275581359863
Epoch 2840, training loss: 310.22967529296875 = 0.020381344482302666 + 50.0 * 6.204185962677002
Epoch 2840, val loss: 2.042198657989502
Epoch 2850, training loss: 310.2528991699219 = 0.0201541967689991 + 50.0 * 6.204655170440674
Epoch 2850, val loss: 2.0455284118652344
Epoch 2860, training loss: 310.3261413574219 = 0.019932128489017487 + 50.0 * 6.206124305725098
Epoch 2860, val loss: 2.048795223236084
Epoch 2870, training loss: 310.2424621582031 = 0.01971336454153061 + 50.0 * 6.2044548988342285
Epoch 2870, val loss: 2.0530974864959717
Epoch 2880, training loss: 310.21978759765625 = 0.019496504217386246 + 50.0 * 6.204005718231201
Epoch 2880, val loss: 2.0564727783203125
Epoch 2890, training loss: 310.47149658203125 = 0.01928221620619297 + 50.0 * 6.209043979644775
Epoch 2890, val loss: 2.0600621700286865
Epoch 2900, training loss: 310.4564208984375 = 0.019060779362916946 + 50.0 * 6.208746910095215
Epoch 2900, val loss: 2.0640509128570557
Epoch 2910, training loss: 310.22589111328125 = 0.01883847452700138 + 50.0 * 6.204141139984131
Epoch 2910, val loss: 2.066786050796509
Epoch 2920, training loss: 310.1595153808594 = 0.018641291186213493 + 50.0 * 6.202817440032959
Epoch 2920, val loss: 2.070787191390991
Epoch 2930, training loss: 310.2520446777344 = 0.0184487272053957 + 50.0 * 6.204671382904053
Epoch 2930, val loss: 2.074164867401123
Epoch 2940, training loss: 310.2129211425781 = 0.018248803913593292 + 50.0 * 6.203893661499023
Epoch 2940, val loss: 2.077861785888672
Epoch 2950, training loss: 310.1667785644531 = 0.018046533688902855 + 50.0 * 6.202974796295166
Epoch 2950, val loss: 2.081418752670288
Epoch 2960, training loss: 310.1701354980469 = 0.017859475687146187 + 50.0 * 6.20304536819458
Epoch 2960, val loss: 2.084467887878418
Epoch 2970, training loss: 310.46710205078125 = 0.01767432503402233 + 50.0 * 6.208988666534424
Epoch 2970, val loss: 2.087754011154175
Epoch 2980, training loss: 310.2249450683594 = 0.017486097291111946 + 50.0 * 6.20414924621582
Epoch 2980, val loss: 2.091805934906006
Epoch 2990, training loss: 310.125244140625 = 0.017294537276029587 + 50.0 * 6.2021589279174805
Epoch 2990, val loss: 2.0948452949523926
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6148148148148148
0.8065366367949395
=== training gcn model ===
Epoch 0, training loss: 431.802001953125 = 1.9612478017807007 + 50.0 * 8.59681510925293
Epoch 0, val loss: 1.9755979776382446
Epoch 10, training loss: 431.74566650390625 = 1.9514379501342773 + 50.0 * 8.595884323120117
Epoch 10, val loss: 1.9649943113327026
Epoch 20, training loss: 431.424560546875 = 1.9389749765396118 + 50.0 * 8.589712142944336
Epoch 20, val loss: 1.9514257907867432
Epoch 30, training loss: 429.5404357910156 = 1.9226094484329224 + 50.0 * 8.552356719970703
Epoch 30, val loss: 1.933613896369934
Epoch 40, training loss: 419.72491455078125 = 1.903499960899353 + 50.0 * 8.356428146362305
Epoch 40, val loss: 1.913666009902954
Epoch 50, training loss: 384.16217041015625 = 1.8823250532150269 + 50.0 * 7.645596981048584
Epoch 50, val loss: 1.8915239572525024
Epoch 60, training loss: 367.2852783203125 = 1.863184928894043 + 50.0 * 7.308441638946533
Epoch 60, val loss: 1.8734859228134155
Epoch 70, training loss: 356.786376953125 = 1.8522958755493164 + 50.0 * 7.098681926727295
Epoch 70, val loss: 1.8631956577301025
Epoch 80, training loss: 350.728759765625 = 1.8407667875289917 + 50.0 * 6.977760314941406
Epoch 80, val loss: 1.8512321710586548
Epoch 90, training loss: 345.6884765625 = 1.83074152469635 + 50.0 * 6.87715482711792
Epoch 90, val loss: 1.841163158416748
Epoch 100, training loss: 342.2969665527344 = 1.8217723369598389 + 50.0 * 6.80950403213501
Epoch 100, val loss: 1.831449031829834
Epoch 110, training loss: 338.8440856933594 = 1.8137032985687256 + 50.0 * 6.740607738494873
Epoch 110, val loss: 1.822500467300415
Epoch 120, training loss: 336.7073669433594 = 1.8066314458847046 + 50.0 * 6.698014736175537
Epoch 120, val loss: 1.8144621849060059
Epoch 130, training loss: 334.7965393066406 = 1.7992050647735596 + 50.0 * 6.659946918487549
Epoch 130, val loss: 1.8062397241592407
Epoch 140, training loss: 333.4667663574219 = 1.7913168668746948 + 50.0 * 6.633509159088135
Epoch 140, val loss: 1.7978477478027344
Epoch 150, training loss: 332.23388671875 = 1.7835038900375366 + 50.0 * 6.609007835388184
Epoch 150, val loss: 1.789729356765747
Epoch 160, training loss: 331.0428466796875 = 1.77572500705719 + 50.0 * 6.5853424072265625
Epoch 160, val loss: 1.781923770904541
Epoch 170, training loss: 329.71624755859375 = 1.767824649810791 + 50.0 * 6.558968544006348
Epoch 170, val loss: 1.7741808891296387
Epoch 180, training loss: 328.4879455566406 = 1.7596449851989746 + 50.0 * 6.5345659255981445
Epoch 180, val loss: 1.7663195133209229
Epoch 190, training loss: 327.7010192871094 = 1.7507514953613281 + 50.0 * 6.519004821777344
Epoch 190, val loss: 1.7577590942382812
Epoch 200, training loss: 326.6957702636719 = 1.7406166791915894 + 50.0 * 6.499102592468262
Epoch 200, val loss: 1.7482898235321045
Epoch 210, training loss: 325.879638671875 = 1.7297061681747437 + 50.0 * 6.482998847961426
Epoch 210, val loss: 1.7381752729415894
Epoch 220, training loss: 325.15155029296875 = 1.717802882194519 + 50.0 * 6.468674659729004
Epoch 220, val loss: 1.727189302444458
Epoch 230, training loss: 324.492431640625 = 1.704927682876587 + 50.0 * 6.455749988555908
Epoch 230, val loss: 1.7153538465499878
Epoch 240, training loss: 324.04345703125 = 1.6907919645309448 + 50.0 * 6.4470534324646
Epoch 240, val loss: 1.7025245428085327
Epoch 250, training loss: 323.36859130859375 = 1.6755093336105347 + 50.0 * 6.43386173248291
Epoch 250, val loss: 1.6887283325195312
Epoch 260, training loss: 322.844482421875 = 1.659014105796814 + 50.0 * 6.423708915710449
Epoch 260, val loss: 1.673797607421875
Epoch 270, training loss: 322.44207763671875 = 1.6411434412002563 + 50.0 * 6.416018486022949
Epoch 270, val loss: 1.6577086448669434
Epoch 280, training loss: 321.93524169921875 = 1.6220142841339111 + 50.0 * 6.406264781951904
Epoch 280, val loss: 1.6407651901245117
Epoch 290, training loss: 321.5425109863281 = 1.601711392402649 + 50.0 * 6.398816108703613
Epoch 290, val loss: 1.6228375434875488
Epoch 300, training loss: 321.2471618652344 = 1.5801805257797241 + 50.0 * 6.39333963394165
Epoch 300, val loss: 1.6039198637008667
Epoch 310, training loss: 320.84771728515625 = 1.5575436353683472 + 50.0 * 6.38580322265625
Epoch 310, val loss: 1.584220051765442
Epoch 320, training loss: 320.4936828613281 = 1.5340481996536255 + 50.0 * 6.379192352294922
Epoch 320, val loss: 1.5639020204544067
Epoch 330, training loss: 320.25616455078125 = 1.509854793548584 + 50.0 * 6.3749260902404785
Epoch 330, val loss: 1.5431395769119263
Epoch 340, training loss: 320.09149169921875 = 1.4848723411560059 + 50.0 * 6.372132301330566
Epoch 340, val loss: 1.5217373371124268
Epoch 350, training loss: 319.648193359375 = 1.4593634605407715 + 50.0 * 6.363777160644531
Epoch 350, val loss: 1.5004135370254517
Epoch 360, training loss: 319.3359680175781 = 1.4336707592010498 + 50.0 * 6.35804557800293
Epoch 360, val loss: 1.4789538383483887
Epoch 370, training loss: 319.1187438964844 = 1.4079432487487793 + 50.0 * 6.354216575622559
Epoch 370, val loss: 1.457579493522644
Epoch 380, training loss: 318.85272216796875 = 1.382006049156189 + 50.0 * 6.349413871765137
Epoch 380, val loss: 1.4363971948623657
Epoch 390, training loss: 318.6581726074219 = 1.3562077283859253 + 50.0 * 6.346039295196533
Epoch 390, val loss: 1.4153962135314941
Epoch 400, training loss: 318.4032287597656 = 1.330584168434143 + 50.0 * 6.341452598571777
Epoch 400, val loss: 1.394933819770813
Epoch 410, training loss: 318.26617431640625 = 1.3052140474319458 + 50.0 * 6.339219093322754
Epoch 410, val loss: 1.375036597251892
Epoch 420, training loss: 318.28955078125 = 1.2802879810333252 + 50.0 * 6.340185642242432
Epoch 420, val loss: 1.3554859161376953
Epoch 430, training loss: 317.87371826171875 = 1.2553898096084595 + 50.0 * 6.332366466522217
Epoch 430, val loss: 1.3362939357757568
Epoch 440, training loss: 317.6446838378906 = 1.2312854528427124 + 50.0 * 6.328267574310303
Epoch 440, val loss: 1.3181941509246826
Epoch 450, training loss: 317.4462890625 = 1.2077304124832153 + 50.0 * 6.324770927429199
Epoch 450, val loss: 1.300740361213684
Epoch 460, training loss: 317.4656982421875 = 1.1846973896026611 + 50.0 * 6.325620174407959
Epoch 460, val loss: 1.2840545177459717
Epoch 470, training loss: 317.3313293457031 = 1.1619646549224854 + 50.0 * 6.323387145996094
Epoch 470, val loss: 1.2676661014556885
Epoch 480, training loss: 317.0079040527344 = 1.1399168968200684 + 50.0 * 6.317359447479248
Epoch 480, val loss: 1.2524287700653076
Epoch 490, training loss: 316.86627197265625 = 1.1185336112976074 + 50.0 * 6.31495475769043
Epoch 490, val loss: 1.2378660440444946
Epoch 500, training loss: 316.68536376953125 = 1.0978177785873413 + 50.0 * 6.311751365661621
Epoch 500, val loss: 1.2239551544189453
Epoch 510, training loss: 316.54376220703125 = 1.077662467956543 + 50.0 * 6.309321880340576
Epoch 510, val loss: 1.2108176946640015
Epoch 520, training loss: 316.8904113769531 = 1.058040976524353 + 50.0 * 6.316647052764893
Epoch 520, val loss: 1.1981916427612305
Epoch 530, training loss: 316.3659362792969 = 1.0385390520095825 + 50.0 * 6.306548118591309
Epoch 530, val loss: 1.1860517263412476
Epoch 540, training loss: 316.18304443359375 = 1.0197629928588867 + 50.0 * 6.30326509475708
Epoch 540, val loss: 1.1745131015777588
Epoch 550, training loss: 316.1744079589844 = 1.0015770196914673 + 50.0 * 6.3034563064575195
Epoch 550, val loss: 1.1637566089630127
Epoch 560, training loss: 315.98431396484375 = 0.9837083220481873 + 50.0 * 6.300012111663818
Epoch 560, val loss: 1.1530050039291382
Epoch 570, training loss: 315.9414978027344 = 0.9663761854171753 + 50.0 * 6.299502849578857
Epoch 570, val loss: 1.1434088945388794
Epoch 580, training loss: 315.75494384765625 = 0.9493780732154846 + 50.0 * 6.296111583709717
Epoch 580, val loss: 1.133621335029602
Epoch 590, training loss: 315.6403503417969 = 0.9328597187995911 + 50.0 * 6.294149875640869
Epoch 590, val loss: 1.1247694492340088
Epoch 600, training loss: 315.9334411621094 = 0.916606605052948 + 50.0 * 6.300336837768555
Epoch 600, val loss: 1.1160472631454468
Epoch 610, training loss: 315.47930908203125 = 0.9006470441818237 + 50.0 * 6.291573524475098
Epoch 610, val loss: 1.1077024936676025
Epoch 620, training loss: 315.38043212890625 = 0.8850569128990173 + 50.0 * 6.289907455444336
Epoch 620, val loss: 1.0995876789093018
Epoch 630, training loss: 315.2591857910156 = 0.8698768019676208 + 50.0 * 6.28778600692749
Epoch 630, val loss: 1.0922338962554932
Epoch 640, training loss: 315.1446228027344 = 0.8549821972846985 + 50.0 * 6.285792827606201
Epoch 640, val loss: 1.0850920677185059
Epoch 650, training loss: 315.3111877441406 = 0.8403379917144775 + 50.0 * 6.289417266845703
Epoch 650, val loss: 1.0780465602874756
Epoch 660, training loss: 315.1843566894531 = 0.8257611393928528 + 50.0 * 6.287171363830566
Epoch 660, val loss: 1.071397066116333
Epoch 670, training loss: 314.97869873046875 = 0.8114811182022095 + 50.0 * 6.283344268798828
Epoch 670, val loss: 1.0651202201843262
Epoch 680, training loss: 314.84600830078125 = 0.7975709438323975 + 50.0 * 6.28096866607666
Epoch 680, val loss: 1.0592877864837646
Epoch 690, training loss: 314.76947021484375 = 0.7839508056640625 + 50.0 * 6.279709815979004
Epoch 690, val loss: 1.05368971824646
Epoch 700, training loss: 314.947021484375 = 0.7704607248306274 + 50.0 * 6.283531665802002
Epoch 700, val loss: 1.048169493675232
Epoch 710, training loss: 314.6344299316406 = 0.7571253776550293 + 50.0 * 6.277546405792236
Epoch 710, val loss: 1.0431503057479858
Epoch 720, training loss: 314.5561828613281 = 0.7440104484558105 + 50.0 * 6.276243209838867
Epoch 720, val loss: 1.0387248992919922
Epoch 730, training loss: 314.4580078125 = 0.7311658263206482 + 50.0 * 6.274536609649658
Epoch 730, val loss: 1.0340605974197388
Epoch 740, training loss: 314.501953125 = 0.7184913158416748 + 50.0 * 6.275669097900391
Epoch 740, val loss: 1.0296175479888916
Epoch 750, training loss: 314.73284912109375 = 0.7058147192001343 + 50.0 * 6.280540943145752
Epoch 750, val loss: 1.0262254476547241
Epoch 760, training loss: 314.4237976074219 = 0.6931894421577454 + 50.0 * 6.2746124267578125
Epoch 760, val loss: 1.022261381149292
Epoch 770, training loss: 314.2581787109375 = 0.6809496283531189 + 50.0 * 6.271544933319092
Epoch 770, val loss: 1.018316626548767
Epoch 780, training loss: 314.14935302734375 = 0.668988823890686 + 50.0 * 6.2696075439453125
Epoch 780, val loss: 1.0151517391204834
Epoch 790, training loss: 314.070556640625 = 0.6571796536445618 + 50.0 * 6.268267631530762
Epoch 790, val loss: 1.0122296810150146
Epoch 800, training loss: 314.0057678222656 = 0.6454871892929077 + 50.0 * 6.267205715179443
Epoch 800, val loss: 1.0094681978225708
Epoch 810, training loss: 314.3092041015625 = 0.6339560151100159 + 50.0 * 6.273505210876465
Epoch 810, val loss: 1.00713050365448
Epoch 820, training loss: 314.053955078125 = 0.622264564037323 + 50.0 * 6.268633842468262
Epoch 820, val loss: 1.0038903951644897
Epoch 830, training loss: 313.90740966796875 = 0.6108773946762085 + 50.0 * 6.265930652618408
Epoch 830, val loss: 1.001722812652588
Epoch 840, training loss: 313.86151123046875 = 0.5997390151023865 + 50.0 * 6.265235424041748
Epoch 840, val loss: 1.000123143196106
Epoch 850, training loss: 313.778076171875 = 0.5887097120285034 + 50.0 * 6.263787269592285
Epoch 850, val loss: 0.998403012752533
Epoch 860, training loss: 313.6961975097656 = 0.577814519405365 + 50.0 * 6.2623677253723145
Epoch 860, val loss: 0.9966095089912415
Epoch 870, training loss: 313.8323059082031 = 0.5671176910400391 + 50.0 * 6.265304088592529
Epoch 870, val loss: 0.9952467083930969
Epoch 880, training loss: 313.6226501464844 = 0.5563915967941284 + 50.0 * 6.261324882507324
Epoch 880, val loss: 0.9936103224754333
Epoch 890, training loss: 313.5687561035156 = 0.5458753108978271 + 50.0 * 6.260457992553711
Epoch 890, val loss: 0.9926859736442566
Epoch 900, training loss: 313.49041748046875 = 0.5355601906776428 + 50.0 * 6.259097099304199
Epoch 900, val loss: 0.991912841796875
Epoch 910, training loss: 313.4217834472656 = 0.5254054069519043 + 50.0 * 6.257927894592285
Epoch 910, val loss: 0.9912709593772888
Epoch 920, training loss: 313.7772216796875 = 0.515448272228241 + 50.0 * 6.265235900878906
Epoch 920, val loss: 0.990897536277771
Epoch 930, training loss: 313.52606201171875 = 0.505235493183136 + 50.0 * 6.260416030883789
Epoch 930, val loss: 0.9901386499404907
Epoch 940, training loss: 313.3562927246094 = 0.49534785747528076 + 50.0 * 6.257218837738037
Epoch 940, val loss: 0.989870548248291
Epoch 950, training loss: 313.2234191894531 = 0.48570942878723145 + 50.0 * 6.254754066467285
Epoch 950, val loss: 0.9899839162826538
Epoch 960, training loss: 313.2579650878906 = 0.47623008489608765 + 50.0 * 6.255634784698486
Epoch 960, val loss: 0.9899742007255554
Epoch 970, training loss: 313.1907958984375 = 0.46676722168922424 + 50.0 * 6.254480361938477
Epoch 970, val loss: 0.9904617667198181
Epoch 980, training loss: 313.2671813964844 = 0.45739126205444336 + 50.0 * 6.256195545196533
Epoch 980, val loss: 0.9908103942871094
Epoch 990, training loss: 313.1142883300781 = 0.448120653629303 + 50.0 * 6.253323554992676
Epoch 990, val loss: 0.9913902878761292
Epoch 1000, training loss: 312.9997863769531 = 0.43905943632125854 + 50.0 * 6.251214504241943
Epoch 1000, val loss: 0.9921147227287292
Epoch 1010, training loss: 313.012451171875 = 0.43017029762268066 + 50.0 * 6.251645565032959
Epoch 1010, val loss: 0.993080735206604
Epoch 1020, training loss: 313.1083679199219 = 0.4212990999221802 + 50.0 * 6.25374174118042
Epoch 1020, val loss: 0.993626594543457
Epoch 1030, training loss: 312.925048828125 = 0.41249504685401917 + 50.0 * 6.250250816345215
Epoch 1030, val loss: 0.9948574304580688
Epoch 1040, training loss: 313.0042724609375 = 0.4038569927215576 + 50.0 * 6.252007961273193
Epoch 1040, val loss: 0.9954143166542053
Epoch 1050, training loss: 312.80328369140625 = 0.39530232548713684 + 50.0 * 6.248159885406494
Epoch 1050, val loss: 0.9967234134674072
Epoch 1060, training loss: 312.77557373046875 = 0.38689544796943665 + 50.0 * 6.247773170471191
Epoch 1060, val loss: 0.9978423118591309
Epoch 1070, training loss: 312.7314147949219 = 0.3786245882511139 + 50.0 * 6.247055530548096
Epoch 1070, val loss: 0.998975396156311
Epoch 1080, training loss: 312.8816223144531 = 0.37046003341674805 + 50.0 * 6.250223636627197
Epoch 1080, val loss: 1.0001754760742188
Epoch 1090, training loss: 312.7638244628906 = 0.36231693625450134 + 50.0 * 6.248030185699463
Epoch 1090, val loss: 1.001402735710144
Epoch 1100, training loss: 312.6922607421875 = 0.35426485538482666 + 50.0 * 6.246759414672852
Epoch 1100, val loss: 1.0024359226226807
Epoch 1110, training loss: 312.6083984375 = 0.34635818004608154 + 50.0 * 6.245241165161133
Epoch 1110, val loss: 1.0035136938095093
Epoch 1120, training loss: 312.558349609375 = 0.33860793709754944 + 50.0 * 6.244394302368164
Epoch 1120, val loss: 1.0049731731414795
Epoch 1130, training loss: 313.003173828125 = 0.3310294449329376 + 50.0 * 6.253443241119385
Epoch 1130, val loss: 1.0066401958465576
Epoch 1140, training loss: 312.6701965332031 = 0.32326847314834595 + 50.0 * 6.246938705444336
Epoch 1140, val loss: 1.0068464279174805
Epoch 1150, training loss: 312.5009460449219 = 0.31578704714775085 + 50.0 * 6.2437028884887695
Epoch 1150, val loss: 1.008169412612915
Epoch 1160, training loss: 312.3953857421875 = 0.3085034191608429 + 50.0 * 6.2417378425598145
Epoch 1160, val loss: 1.0095831155776978
Epoch 1170, training loss: 312.37847900390625 = 0.3013981580734253 + 50.0 * 6.241541862487793
Epoch 1170, val loss: 1.0111242532730103
Epoch 1180, training loss: 312.6449279785156 = 0.294413685798645 + 50.0 * 6.247010231018066
Epoch 1180, val loss: 1.0125304460525513
Epoch 1190, training loss: 312.41705322265625 = 0.2874008119106293 + 50.0 * 6.242592811584473
Epoch 1190, val loss: 1.0131311416625977
Epoch 1200, training loss: 312.684814453125 = 0.28061166405677795 + 50.0 * 6.24808406829834
Epoch 1200, val loss: 1.014441967010498
Epoch 1210, training loss: 312.36920166015625 = 0.2738879323005676 + 50.0 * 6.24190616607666
Epoch 1210, val loss: 1.0159060955047607
Epoch 1220, training loss: 312.2813415527344 = 0.2673982083797455 + 50.0 * 6.240279197692871
Epoch 1220, val loss: 1.0170550346374512
Epoch 1230, training loss: 312.197021484375 = 0.26111865043640137 + 50.0 * 6.238718032836914
Epoch 1230, val loss: 1.0188277959823608
Epoch 1240, training loss: 312.19732666015625 = 0.2549889385700226 + 50.0 * 6.238846778869629
Epoch 1240, val loss: 1.0201776027679443
Epoch 1250, training loss: 312.33673095703125 = 0.24892815947532654 + 50.0 * 6.241755962371826
Epoch 1250, val loss: 1.021602988243103
Epoch 1260, training loss: 312.14312744140625 = 0.24295473098754883 + 50.0 * 6.238003253936768
Epoch 1260, val loss: 1.02278470993042
Epoch 1270, training loss: 312.09710693359375 = 0.23718751966953278 + 50.0 * 6.237198352813721
Epoch 1270, val loss: 1.0247421264648438
Epoch 1280, training loss: 312.0478820800781 = 0.23162229359149933 + 50.0 * 6.236325263977051
Epoch 1280, val loss: 1.026415467262268
Epoch 1290, training loss: 312.063232421875 = 0.22620119154453278 + 50.0 * 6.236740589141846
Epoch 1290, val loss: 1.0284830331802368
Epoch 1300, training loss: 312.16595458984375 = 0.22087721526622772 + 50.0 * 6.238901615142822
Epoch 1300, val loss: 1.030212163925171
Epoch 1310, training loss: 312.0730895996094 = 0.215586319565773 + 50.0 * 6.237150192260742
Epoch 1310, val loss: 1.0319781303405762
Epoch 1320, training loss: 312.0548400878906 = 0.21046645939350128 + 50.0 * 6.236887454986572
Epoch 1320, val loss: 1.0335675477981567
Epoch 1330, training loss: 311.9154357910156 = 0.20555220544338226 + 50.0 * 6.234197616577148
Epoch 1330, val loss: 1.035858392715454
Epoch 1340, training loss: 311.9201354980469 = 0.2007875293493271 + 50.0 * 6.234386920928955
Epoch 1340, val loss: 1.0380208492279053
Epoch 1350, training loss: 312.067626953125 = 0.1961284726858139 + 50.0 * 6.237430095672607
Epoch 1350, val loss: 1.040116786956787
Epoch 1360, training loss: 311.88604736328125 = 0.1915566325187683 + 50.0 * 6.233889579772949
Epoch 1360, val loss: 1.0428968667984009
Epoch 1370, training loss: 311.8231506347656 = 0.18712161481380463 + 50.0 * 6.232720375061035
Epoch 1370, val loss: 1.045068621635437
Epoch 1380, training loss: 311.92828369140625 = 0.1828473061323166 + 50.0 * 6.234908580780029
Epoch 1380, val loss: 1.047386646270752
Epoch 1390, training loss: 311.8499755859375 = 0.17860911786556244 + 50.0 * 6.233427047729492
Epoch 1390, val loss: 1.0505187511444092
Epoch 1400, training loss: 311.8707580566406 = 0.174491748213768 + 50.0 * 6.2339253425598145
Epoch 1400, val loss: 1.0524601936340332
Epoch 1410, training loss: 311.7810974121094 = 0.17052388191223145 + 50.0 * 6.232211112976074
Epoch 1410, val loss: 1.0561541318893433
Epoch 1420, training loss: 311.6935119628906 = 0.16667614877223969 + 50.0 * 6.230536460876465
Epoch 1420, val loss: 1.0589044094085693
Epoch 1430, training loss: 311.7970275878906 = 0.16295838356018066 + 50.0 * 6.2326812744140625
Epoch 1430, val loss: 1.0619335174560547
Epoch 1440, training loss: 311.8070373535156 = 0.15926553308963776 + 50.0 * 6.232955455780029
Epoch 1440, val loss: 1.0652847290039062
Epoch 1450, training loss: 311.6646423339844 = 0.15565280616283417 + 50.0 * 6.230180263519287
Epoch 1450, val loss: 1.068124532699585
Epoch 1460, training loss: 311.6292724609375 = 0.1521870195865631 + 50.0 * 6.229541778564453
Epoch 1460, val loss: 1.0718328952789307
Epoch 1470, training loss: 311.6532287597656 = 0.14884530007839203 + 50.0 * 6.230087757110596
Epoch 1470, val loss: 1.0751774311065674
Epoch 1480, training loss: 311.8252258300781 = 0.14556171000003815 + 50.0 * 6.233592987060547
Epoch 1480, val loss: 1.078710913658142
Epoch 1490, training loss: 311.67950439453125 = 0.14234033226966858 + 50.0 * 6.230743408203125
Epoch 1490, val loss: 1.082519292831421
Epoch 1500, training loss: 311.7414855957031 = 0.13920797407627106 + 50.0 * 6.2320451736450195
Epoch 1500, val loss: 1.0861464738845825
Epoch 1510, training loss: 311.52850341796875 = 0.13614462316036224 + 50.0 * 6.227847576141357
Epoch 1510, val loss: 1.0897830724716187
Epoch 1520, training loss: 311.5293273925781 = 0.1331954002380371 + 50.0 * 6.227922439575195
Epoch 1520, val loss: 1.0934547185897827
Epoch 1530, training loss: 311.4856872558594 = 0.13033533096313477 + 50.0 * 6.227107048034668
Epoch 1530, val loss: 1.0975356101989746
Epoch 1540, training loss: 311.5805358886719 = 0.12756186723709106 + 50.0 * 6.229059219360352
Epoch 1540, val loss: 1.1012369394302368
Epoch 1550, training loss: 311.54437255859375 = 0.12478407472372055 + 50.0 * 6.228391647338867
Epoch 1550, val loss: 1.1055234670639038
Epoch 1560, training loss: 311.5240478515625 = 0.12208583950996399 + 50.0 * 6.228038787841797
Epoch 1560, val loss: 1.1093937158584595
Epoch 1570, training loss: 311.4285583496094 = 0.11947847902774811 + 50.0 * 6.226181507110596
Epoch 1570, val loss: 1.1138908863067627
Epoch 1580, training loss: 311.3797607421875 = 0.11698134243488312 + 50.0 * 6.225255489349365
Epoch 1580, val loss: 1.1183395385742188
Epoch 1590, training loss: 311.4029541015625 = 0.11455448716878891 + 50.0 * 6.225767612457275
Epoch 1590, val loss: 1.122655987739563
Epoch 1600, training loss: 311.94915771484375 = 0.1121756061911583 + 50.0 * 6.236739635467529
Epoch 1600, val loss: 1.127625823020935
Epoch 1610, training loss: 311.4341735839844 = 0.10974358022212982 + 50.0 * 6.2264885902404785
Epoch 1610, val loss: 1.131136178970337
Epoch 1620, training loss: 311.31640625 = 0.10745640099048615 + 50.0 * 6.224179267883301
Epoch 1620, val loss: 1.135933756828308
Epoch 1630, training loss: 311.34881591796875 = 0.10526537895202637 + 50.0 * 6.2248711585998535
Epoch 1630, val loss: 1.1405906677246094
Epoch 1640, training loss: 311.5801696777344 = 0.10313083231449127 + 50.0 * 6.229541301727295
Epoch 1640, val loss: 1.1453943252563477
Epoch 1650, training loss: 311.3775939941406 = 0.10098500549793243 + 50.0 * 6.225532531738281
Epoch 1650, val loss: 1.1499241590499878
Epoch 1660, training loss: 311.3642272949219 = 0.0989329069852829 + 50.0 * 6.225306034088135
Epoch 1660, val loss: 1.1548497676849365
Epoch 1670, training loss: 311.2732849121094 = 0.09691979736089706 + 50.0 * 6.223527431488037
Epoch 1670, val loss: 1.1594816446304321
Epoch 1680, training loss: 311.22906494140625 = 0.09497683495283127 + 50.0 * 6.222681999206543
Epoch 1680, val loss: 1.1641244888305664
Epoch 1690, training loss: 311.2879943847656 = 0.09309413284063339 + 50.0 * 6.223897933959961
Epoch 1690, val loss: 1.1692553758621216
Epoch 1700, training loss: 311.3458251953125 = 0.0912233218550682 + 50.0 * 6.22509241104126
Epoch 1700, val loss: 1.1742547750473022
Epoch 1710, training loss: 311.3624267578125 = 0.08939097821712494 + 50.0 * 6.225460529327393
Epoch 1710, val loss: 1.1788357496261597
Epoch 1720, training loss: 311.3065490722656 = 0.08760394901037216 + 50.0 * 6.22437858581543
Epoch 1720, val loss: 1.1831722259521484
Epoch 1730, training loss: 311.2276306152344 = 0.08585891127586365 + 50.0 * 6.222835540771484
Epoch 1730, val loss: 1.1888327598571777
Epoch 1740, training loss: 311.2043762207031 = 0.08420519530773163 + 50.0 * 6.222403526306152
Epoch 1740, val loss: 1.1941373348236084
Epoch 1750, training loss: 311.26458740234375 = 0.0825662836432457 + 50.0 * 6.223639965057373
Epoch 1750, val loss: 1.1992820501327515
Epoch 1760, training loss: 311.1836853027344 = 0.08095263689756393 + 50.0 * 6.222054481506348
Epoch 1760, val loss: 1.203209638595581
Epoch 1770, training loss: 311.2761535644531 = 0.07938984036445618 + 50.0 * 6.223935604095459
Epoch 1770, val loss: 1.2087987661361694
Epoch 1780, training loss: 311.093017578125 = 0.07784600555896759 + 50.0 * 6.220303535461426
Epoch 1780, val loss: 1.213948369026184
Epoch 1790, training loss: 311.1642761230469 = 0.07636022567749023 + 50.0 * 6.2217583656311035
Epoch 1790, val loss: 1.2191323041915894
Epoch 1800, training loss: 311.16192626953125 = 0.07490278780460358 + 50.0 * 6.22174072265625
Epoch 1800, val loss: 1.2237712144851685
Epoch 1810, training loss: 311.05426025390625 = 0.0734783485531807 + 50.0 * 6.219615459442139
Epoch 1810, val loss: 1.2290701866149902
Epoch 1820, training loss: 311.0872802734375 = 0.07210376113653183 + 50.0 * 6.220303535461426
Epoch 1820, val loss: 1.2346135377883911
Epoch 1830, training loss: 311.18133544921875 = 0.07074993848800659 + 50.0 * 6.222211837768555
Epoch 1830, val loss: 1.2389373779296875
Epoch 1840, training loss: 311.1311340332031 = 0.06941249966621399 + 50.0 * 6.221234321594238
Epoch 1840, val loss: 1.2447388172149658
Epoch 1850, training loss: 311.1119384765625 = 0.06811129301786423 + 50.0 * 6.220876216888428
Epoch 1850, val loss: 1.2493112087249756
Epoch 1860, training loss: 311.0851745605469 = 0.06683055311441422 + 50.0 * 6.220366954803467
Epoch 1860, val loss: 1.2542320489883423
Epoch 1870, training loss: 311.027587890625 = 0.06559734046459198 + 50.0 * 6.219240188598633
Epoch 1870, val loss: 1.2593408823013306
Epoch 1880, training loss: 311.0146484375 = 0.06439784914255142 + 50.0 * 6.219005107879639
Epoch 1880, val loss: 1.2649493217468262
Epoch 1890, training loss: 311.06646728515625 = 0.0632261410355568 + 50.0 * 6.220064640045166
Epoch 1890, val loss: 1.2700706720352173
Epoch 1900, training loss: 311.2443542480469 = 0.062060028314590454 + 50.0 * 6.2236456871032715
Epoch 1900, val loss: 1.274935245513916
Epoch 1910, training loss: 310.9915466308594 = 0.06091094762086868 + 50.0 * 6.2186126708984375
Epoch 1910, val loss: 1.2799593210220337
Epoch 1920, training loss: 310.915283203125 = 0.059814658015966415 + 50.0 * 6.217109203338623
Epoch 1920, val loss: 1.2852742671966553
Epoch 1930, training loss: 310.88763427734375 = 0.058747563511133194 + 50.0 * 6.216577529907227
Epoch 1930, val loss: 1.2905113697052002
Epoch 1940, training loss: 311.0818176269531 = 0.057731933891773224 + 50.0 * 6.2204813957214355
Epoch 1940, val loss: 1.2962901592254639
Epoch 1950, training loss: 310.8780517578125 = 0.0566590391099453 + 50.0 * 6.216427326202393
Epoch 1950, val loss: 1.3002854585647583
Epoch 1960, training loss: 310.88555908203125 = 0.05563813075423241 + 50.0 * 6.2165985107421875
Epoch 1960, val loss: 1.3055340051651
Epoch 1970, training loss: 310.9682312011719 = 0.054658662527799606 + 50.0 * 6.218271732330322
Epoch 1970, val loss: 1.3104138374328613
Epoch 1980, training loss: 310.91485595703125 = 0.05369289219379425 + 50.0 * 6.217223644256592
Epoch 1980, val loss: 1.3157743215560913
Epoch 1990, training loss: 310.8394775390625 = 0.0527404323220253 + 50.0 * 6.215734958648682
Epoch 1990, val loss: 1.3210053443908691
Epoch 2000, training loss: 310.80670166015625 = 0.05182831361889839 + 50.0 * 6.215097427368164
Epoch 2000, val loss: 1.326067328453064
Epoch 2010, training loss: 310.8202819824219 = 0.05094532296061516 + 50.0 * 6.215386867523193
Epoch 2010, val loss: 1.331568717956543
Epoch 2020, training loss: 311.1412658691406 = 0.050085827708244324 + 50.0 * 6.221823692321777
Epoch 2020, val loss: 1.3365520238876343
Epoch 2030, training loss: 310.93585205078125 = 0.04918067157268524 + 50.0 * 6.217732906341553
Epoch 2030, val loss: 1.340484619140625
Epoch 2040, training loss: 310.860595703125 = 0.04832420498132706 + 50.0 * 6.216245651245117
Epoch 2040, val loss: 1.3459688425064087
Epoch 2050, training loss: 310.7464599609375 = 0.047496531158685684 + 50.0 * 6.213979721069336
Epoch 2050, val loss: 1.351210355758667
Epoch 2060, training loss: 310.7996520996094 = 0.04670749977231026 + 50.0 * 6.215058326721191
Epoch 2060, val loss: 1.3564238548278809
Epoch 2070, training loss: 310.82843017578125 = 0.04592043533921242 + 50.0 * 6.2156500816345215
Epoch 2070, val loss: 1.3615498542785645
Epoch 2080, training loss: 310.877685546875 = 0.04514146223664284 + 50.0 * 6.21665096282959
Epoch 2080, val loss: 1.3656584024429321
Epoch 2090, training loss: 310.7569580078125 = 0.04437335580587387 + 50.0 * 6.21425199508667
Epoch 2090, val loss: 1.3710123300552368
Epoch 2100, training loss: 310.7305908203125 = 0.04363851621747017 + 50.0 * 6.213738918304443
Epoch 2100, val loss: 1.375753402709961
Epoch 2110, training loss: 310.797607421875 = 0.04292118921875954 + 50.0 * 6.215094089508057
Epoch 2110, val loss: 1.3807064294815063
Epoch 2120, training loss: 310.85260009765625 = 0.042214516550302505 + 50.0 * 6.216207981109619
Epoch 2120, val loss: 1.3859713077545166
Epoch 2130, training loss: 310.7297668457031 = 0.04150526225566864 + 50.0 * 6.2137651443481445
Epoch 2130, val loss: 1.390491008758545
Epoch 2140, training loss: 310.7228088378906 = 0.04082025587558746 + 50.0 * 6.213639736175537
Epoch 2140, val loss: 1.3950964212417603
Epoch 2150, training loss: 310.6717834472656 = 0.04015353322029114 + 50.0 * 6.212632179260254
Epoch 2150, val loss: 1.3999252319335938
Epoch 2160, training loss: 310.70965576171875 = 0.03951059281826019 + 50.0 * 6.21340274810791
Epoch 2160, val loss: 1.4048781394958496
Epoch 2170, training loss: 310.7839660644531 = 0.03887766972184181 + 50.0 * 6.214901447296143
Epoch 2170, val loss: 1.4090487957000732
Epoch 2180, training loss: 310.69403076171875 = 0.03824291378259659 + 50.0 * 6.213115692138672
Epoch 2180, val loss: 1.4145028591156006
Epoch 2190, training loss: 310.6893615722656 = 0.03763680160045624 + 50.0 * 6.213034629821777
Epoch 2190, val loss: 1.4196277856826782
Epoch 2200, training loss: 310.6763916015625 = 0.03703521937131882 + 50.0 * 6.21278715133667
Epoch 2200, val loss: 1.4243557453155518
Epoch 2210, training loss: 310.795166015625 = 0.036459989845752716 + 50.0 * 6.215173721313477
Epoch 2210, val loss: 1.428852915763855
Epoch 2220, training loss: 310.6671142578125 = 0.03586214780807495 + 50.0 * 6.212625026702881
Epoch 2220, val loss: 1.433667778968811
Epoch 2230, training loss: 310.6728210449219 = 0.03529474139213562 + 50.0 * 6.21274995803833
Epoch 2230, val loss: 1.437700867652893
Epoch 2240, training loss: 310.6158142089844 = 0.034745875746011734 + 50.0 * 6.211621284484863
Epoch 2240, val loss: 1.443282127380371
Epoch 2250, training loss: 310.5649108886719 = 0.034211739897727966 + 50.0 * 6.210614204406738
Epoch 2250, val loss: 1.447522759437561
Epoch 2260, training loss: 310.7196960449219 = 0.03369799256324768 + 50.0 * 6.213719844818115
Epoch 2260, val loss: 1.4529647827148438
Epoch 2270, training loss: 310.5414123535156 = 0.03316088020801544 + 50.0 * 6.210165023803711
Epoch 2270, val loss: 1.4559876918792725
Epoch 2280, training loss: 310.5440368652344 = 0.03264448791742325 + 50.0 * 6.2102274894714355
Epoch 2280, val loss: 1.4608397483825684
Epoch 2290, training loss: 310.5489501953125 = 0.03216162696480751 + 50.0 * 6.210335731506348
Epoch 2290, val loss: 1.4654568433761597
Epoch 2300, training loss: 310.72052001953125 = 0.03168204799294472 + 50.0 * 6.2137770652771
Epoch 2300, val loss: 1.4697604179382324
Epoch 2310, training loss: 310.56536865234375 = 0.03119104914367199 + 50.0 * 6.210683345794678
Epoch 2310, val loss: 1.474242091178894
Epoch 2320, training loss: 310.5420227050781 = 0.0307180006057024 + 50.0 * 6.210226058959961
Epoch 2320, val loss: 1.4783624410629272
Epoch 2330, training loss: 310.5255126953125 = 0.030269235372543335 + 50.0 * 6.209904670715332
Epoch 2330, val loss: 1.4833165407180786
Epoch 2340, training loss: 310.6717834472656 = 0.029823532328009605 + 50.0 * 6.212839126586914
Epoch 2340, val loss: 1.487663984298706
Epoch 2350, training loss: 310.4598083496094 = 0.02937142923474312 + 50.0 * 6.208609104156494
Epoch 2350, val loss: 1.4913159608840942
Epoch 2360, training loss: 310.4680480957031 = 0.028943834826350212 + 50.0 * 6.208782196044922
Epoch 2360, val loss: 1.4960376024246216
Epoch 2370, training loss: 310.5342712402344 = 0.02853057160973549 + 50.0 * 6.2101149559021
Epoch 2370, val loss: 1.5001280307769775
Epoch 2380, training loss: 310.5367431640625 = 0.028115946799516678 + 50.0 * 6.210172653198242
Epoch 2380, val loss: 1.504494547843933
Epoch 2390, training loss: 310.5025329589844 = 0.02769809402525425 + 50.0 * 6.20949649810791
Epoch 2390, val loss: 1.5084936618804932
Epoch 2400, training loss: 310.4090881347656 = 0.027290698140859604 + 50.0 * 6.20763635635376
Epoch 2400, val loss: 1.5127514600753784
Epoch 2410, training loss: 310.4024963378906 = 0.026910612359642982 + 50.0 * 6.2075114250183105
Epoch 2410, val loss: 1.5172377824783325
Epoch 2420, training loss: 310.7349853515625 = 0.026550229638814926 + 50.0 * 6.214168548583984
Epoch 2420, val loss: 1.5222280025482178
Epoch 2430, training loss: 310.5811767578125 = 0.026149047538638115 + 50.0 * 6.2111005783081055
Epoch 2430, val loss: 1.52486252784729
Epoch 2440, training loss: 310.4715270996094 = 0.025770746171474457 + 50.0 * 6.2089152336120605
Epoch 2440, val loss: 1.529219150543213
Epoch 2450, training loss: 310.39093017578125 = 0.025409672409296036 + 50.0 * 6.207310199737549
Epoch 2450, val loss: 1.53361976146698
Epoch 2460, training loss: 310.36053466796875 = 0.025070078670978546 + 50.0 * 6.206709384918213
Epoch 2460, val loss: 1.5380868911743164
Epoch 2470, training loss: 310.556640625 = 0.02474198304116726 + 50.0 * 6.21063756942749
Epoch 2470, val loss: 1.5426887273788452
Epoch 2480, training loss: 310.3842468261719 = 0.02438397705554962 + 50.0 * 6.207197189331055
Epoch 2480, val loss: 1.5453274250030518
Epoch 2490, training loss: 310.36749267578125 = 0.024044016376137733 + 50.0 * 6.206868648529053
Epoch 2490, val loss: 1.5496644973754883
Epoch 2500, training loss: 310.4039001464844 = 0.023723283782601357 + 50.0 * 6.207603931427002
Epoch 2500, val loss: 1.5538727045059204
Epoch 2510, training loss: 310.4861755371094 = 0.023407669737935066 + 50.0 * 6.209255218505859
Epoch 2510, val loss: 1.558090329170227
Epoch 2520, training loss: 310.3568115234375 = 0.02308708056807518 + 50.0 * 6.206674098968506
Epoch 2520, val loss: 1.5618213415145874
Epoch 2530, training loss: 310.4227600097656 = 0.022782640531659126 + 50.0 * 6.207999229431152
Epoch 2530, val loss: 1.5661507844924927
Epoch 2540, training loss: 310.4749450683594 = 0.022481082007288933 + 50.0 * 6.209049224853516
Epoch 2540, val loss: 1.569594144821167
Epoch 2550, training loss: 310.427490234375 = 0.02217085473239422 + 50.0 * 6.208106994628906
Epoch 2550, val loss: 1.5729072093963623
Epoch 2560, training loss: 310.3301086425781 = 0.021880988031625748 + 50.0 * 6.206164360046387
Epoch 2560, val loss: 1.5773345232009888
Epoch 2570, training loss: 310.2802429199219 = 0.02159755304455757 + 50.0 * 6.205172538757324
Epoch 2570, val loss: 1.5805743932724
Epoch 2580, training loss: 310.2594299316406 = 0.021327517926692963 + 50.0 * 6.204761981964111
Epoch 2580, val loss: 1.5846742391586304
Epoch 2590, training loss: 310.3323059082031 = 0.021063655614852905 + 50.0 * 6.2062249183654785
Epoch 2590, val loss: 1.588117241859436
Epoch 2600, training loss: 310.68914794921875 = 0.020793696865439415 + 50.0 * 6.213367462158203
Epoch 2600, val loss: 1.591506004333496
Epoch 2610, training loss: 310.302978515625 = 0.02050049602985382 + 50.0 * 6.205649375915527
Epoch 2610, val loss: 1.5954657793045044
Epoch 2620, training loss: 310.2455139160156 = 0.020237289369106293 + 50.0 * 6.204505443572998
Epoch 2620, val loss: 1.599104881286621
Epoch 2630, training loss: 310.2447509765625 = 0.01999124512076378 + 50.0 * 6.204495429992676
Epoch 2630, val loss: 1.6029937267303467
Epoch 2640, training loss: 310.5767517089844 = 0.01975664682686329 + 50.0 * 6.211140155792236
Epoch 2640, val loss: 1.6068031787872314
Epoch 2650, training loss: 310.2803039550781 = 0.01949099823832512 + 50.0 * 6.205215930938721
Epoch 2650, val loss: 1.6098567247390747
Epoch 2660, training loss: 310.2367858886719 = 0.019249627366662025 + 50.0 * 6.204350471496582
Epoch 2660, val loss: 1.6135543584823608
Epoch 2670, training loss: 310.22418212890625 = 0.019018154591321945 + 50.0 * 6.204103469848633
Epoch 2670, val loss: 1.6175472736358643
Epoch 2680, training loss: 310.6307067871094 = 0.01879568211734295 + 50.0 * 6.212238311767578
Epoch 2680, val loss: 1.6210839748382568
Epoch 2690, training loss: 310.36737060546875 = 0.018555326387286186 + 50.0 * 6.206976413726807
Epoch 2690, val loss: 1.624255657196045
Epoch 2700, training loss: 310.22955322265625 = 0.01832010969519615 + 50.0 * 6.204224586486816
Epoch 2700, val loss: 1.6276222467422485
Epoch 2710, training loss: 310.1759948730469 = 0.018105123192071915 + 50.0 * 6.203157901763916
Epoch 2710, val loss: 1.6310343742370605
Epoch 2720, training loss: 310.1898498535156 = 0.017894500866532326 + 50.0 * 6.203439235687256
Epoch 2720, val loss: 1.634469747543335
Epoch 2730, training loss: 310.5351257324219 = 0.017691412940621376 + 50.0 * 6.210348606109619
Epoch 2730, val loss: 1.6374083757400513
Epoch 2740, training loss: 310.3018798828125 = 0.017473135143518448 + 50.0 * 6.205687999725342
Epoch 2740, val loss: 1.6416385173797607
Epoch 2750, training loss: 310.25640869140625 = 0.017258554697036743 + 50.0 * 6.204782962799072
Epoch 2750, val loss: 1.6442558765411377
Epoch 2760, training loss: 310.23626708984375 = 0.01706051081418991 + 50.0 * 6.2043843269348145
Epoch 2760, val loss: 1.647747278213501
Epoch 2770, training loss: 310.3214111328125 = 0.01686166785657406 + 50.0 * 6.206090450286865
Epoch 2770, val loss: 1.6511527299880981
Epoch 2780, training loss: 310.1600036621094 = 0.016664685681462288 + 50.0 * 6.202866554260254
Epoch 2780, val loss: 1.6548298597335815
Epoch 2790, training loss: 310.13861083984375 = 0.016472363844513893 + 50.0 * 6.202442646026611
Epoch 2790, val loss: 1.6579780578613281
Epoch 2800, training loss: 310.1761474609375 = 0.016290277242660522 + 50.0 * 6.203197002410889
Epoch 2800, val loss: 1.6616771221160889
Epoch 2810, training loss: 310.30072021484375 = 0.016114411875605583 + 50.0 * 6.205692291259766
Epoch 2810, val loss: 1.6651811599731445
Epoch 2820, training loss: 310.19927978515625 = 0.015922438353300095 + 50.0 * 6.203667163848877
Epoch 2820, val loss: 1.6671428680419922
Epoch 2830, training loss: 310.2059631347656 = 0.01573951728641987 + 50.0 * 6.2038044929504395
Epoch 2830, val loss: 1.6701527833938599
Epoch 2840, training loss: 310.18402099609375 = 0.015567309223115444 + 50.0 * 6.203369140625
Epoch 2840, val loss: 1.673618197441101
Epoch 2850, training loss: 310.20849609375 = 0.015392852015793324 + 50.0 * 6.203862190246582
Epoch 2850, val loss: 1.6764843463897705
Epoch 2860, training loss: 310.1986999511719 = 0.015222371555864811 + 50.0 * 6.203669548034668
Epoch 2860, val loss: 1.6801356077194214
Epoch 2870, training loss: 310.2678527832031 = 0.015047857537865639 + 50.0 * 6.205056190490723
Epoch 2870, val loss: 1.6826707124710083
Epoch 2880, training loss: 310.1098937988281 = 0.014881758019328117 + 50.0 * 6.201900005340576
Epoch 2880, val loss: 1.6863054037094116
Epoch 2890, training loss: 310.09429931640625 = 0.014723903499543667 + 50.0 * 6.201591491699219
Epoch 2890, val loss: 1.6898181438446045
Epoch 2900, training loss: 310.2334899902344 = 0.014571796171367168 + 50.0 * 6.204378604888916
Epoch 2900, val loss: 1.6926672458648682
Epoch 2910, training loss: 310.13751220703125 = 0.014406741596758366 + 50.0 * 6.202462196350098
Epoch 2910, val loss: 1.6953316926956177
Epoch 2920, training loss: 310.0598449707031 = 0.014249750413000584 + 50.0 * 6.200911998748779
Epoch 2920, val loss: 1.6985118389129639
Epoch 2930, training loss: 310.0693664550781 = 0.0141010582447052 + 50.0 * 6.20110559463501
Epoch 2930, val loss: 1.7019767761230469
Epoch 2940, training loss: 310.108642578125 = 0.013958058319985867 + 50.0 * 6.2018938064575195
Epoch 2940, val loss: 1.7048492431640625
Epoch 2950, training loss: 310.14581298828125 = 0.01381162740290165 + 50.0 * 6.202640056610107
Epoch 2950, val loss: 1.7076267004013062
Epoch 2960, training loss: 310.0807189941406 = 0.013664421625435352 + 50.0 * 6.201340675354004
Epoch 2960, val loss: 1.7100361585617065
Epoch 2970, training loss: 310.0732727050781 = 0.013524304144084454 + 50.0 * 6.201194763183594
Epoch 2970, val loss: 1.7129168510437012
Epoch 2980, training loss: 310.3939514160156 = 0.013387496583163738 + 50.0 * 6.207611083984375
Epoch 2980, val loss: 1.715326189994812
Epoch 2990, training loss: 310.07208251953125 = 0.013242614455521107 + 50.0 * 6.201176643371582
Epoch 2990, val loss: 1.718950629234314
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6444444444444445
0.8039008961518187
The final CL Acc:0.64074, 0.01983, The final GNN Acc:0.80302, 0.00329
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13230])
remove edge: torch.Size([2, 7878])
updated graph: torch.Size([2, 10552])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7937927246094 = 1.951819658279419 + 50.0 * 8.596839904785156
Epoch 0, val loss: 1.9496175050735474
Epoch 10, training loss: 431.7454833984375 = 1.9424464702606201 + 50.0 * 8.596060752868652
Epoch 10, val loss: 1.9406245946884155
Epoch 20, training loss: 431.44921875 = 1.9308104515075684 + 50.0 * 8.590368270874023
Epoch 20, val loss: 1.929052710533142
Epoch 30, training loss: 429.52178955078125 = 1.915863275527954 + 50.0 * 8.552118301391602
Epoch 30, val loss: 1.9140428304672241
Epoch 40, training loss: 418.0551452636719 = 1.898041009902954 + 50.0 * 8.323142051696777
Epoch 40, val loss: 1.8965682983398438
Epoch 50, training loss: 381.4871826171875 = 1.8779560327529907 + 50.0 * 7.592184543609619
Epoch 50, val loss: 1.8773311376571655
Epoch 60, training loss: 373.12579345703125 = 1.8613673448562622 + 50.0 * 7.425288677215576
Epoch 60, val loss: 1.8622522354125977
Epoch 70, training loss: 363.3251037597656 = 1.8466202020645142 + 50.0 * 7.229569911956787
Epoch 70, val loss: 1.8479887247085571
Epoch 80, training loss: 355.8411560058594 = 1.8332797288894653 + 50.0 * 7.080157279968262
Epoch 80, val loss: 1.8349405527114868
Epoch 90, training loss: 350.376708984375 = 1.8226983547210693 + 50.0 * 6.971080303192139
Epoch 90, val loss: 1.8243790864944458
Epoch 100, training loss: 344.3573913574219 = 1.8128935098648071 + 50.0 * 6.850890159606934
Epoch 100, val loss: 1.8147716522216797
Epoch 110, training loss: 340.02410888671875 = 1.804272174835205 + 50.0 * 6.764397144317627
Epoch 110, val loss: 1.8063358068466187
Epoch 120, training loss: 336.7059326171875 = 1.796167016029358 + 50.0 * 6.698194980621338
Epoch 120, val loss: 1.79771089553833
Epoch 130, training loss: 334.69287109375 = 1.7873492240905762 + 50.0 * 6.65811014175415
Epoch 130, val loss: 1.788522481918335
Epoch 140, training loss: 332.85302734375 = 1.7788838148117065 + 50.0 * 6.621482849121094
Epoch 140, val loss: 1.7801235914230347
Epoch 150, training loss: 331.23193359375 = 1.7711237668991089 + 50.0 * 6.589216232299805
Epoch 150, val loss: 1.7725862264633179
Epoch 160, training loss: 329.8310241699219 = 1.7632713317871094 + 50.0 * 6.561355113983154
Epoch 160, val loss: 1.7648720741271973
Epoch 170, training loss: 328.83160400390625 = 1.7547917366027832 + 50.0 * 6.541536331176758
Epoch 170, val loss: 1.7566454410552979
Epoch 180, training loss: 327.56182861328125 = 1.7456657886505127 + 50.0 * 6.516323089599609
Epoch 180, val loss: 1.747887134552002
Epoch 190, training loss: 326.6693115234375 = 1.735952615737915 + 50.0 * 6.498667240142822
Epoch 190, val loss: 1.7386977672576904
Epoch 200, training loss: 325.8091735839844 = 1.7252979278564453 + 50.0 * 6.481677532196045
Epoch 200, val loss: 1.728678822517395
Epoch 210, training loss: 325.0434875488281 = 1.7139067649841309 + 50.0 * 6.466591835021973
Epoch 210, val loss: 1.7180373668670654
Epoch 220, training loss: 324.4351806640625 = 1.701507806777954 + 50.0 * 6.4546732902526855
Epoch 220, val loss: 1.70657217502594
Epoch 230, training loss: 323.6590881347656 = 1.6883189678192139 + 50.0 * 6.439415454864502
Epoch 230, val loss: 1.694366693496704
Epoch 240, training loss: 323.051513671875 = 1.674180507659912 + 50.0 * 6.427546501159668
Epoch 240, val loss: 1.6813300848007202
Epoch 250, training loss: 322.62359619140625 = 1.6590542793273926 + 50.0 * 6.419290542602539
Epoch 250, val loss: 1.6674976348876953
Epoch 260, training loss: 322.1090087890625 = 1.6427597999572754 + 50.0 * 6.409325122833252
Epoch 260, val loss: 1.6526206731796265
Epoch 270, training loss: 321.5452880859375 = 1.6255922317504883 + 50.0 * 6.3983941078186035
Epoch 270, val loss: 1.6371906995773315
Epoch 280, training loss: 321.3009948730469 = 1.6074823141098022 + 50.0 * 6.3938703536987305
Epoch 280, val loss: 1.6210064888000488
Epoch 290, training loss: 320.7623291015625 = 1.5886114835739136 + 50.0 * 6.383473873138428
Epoch 290, val loss: 1.6042392253875732
Epoch 300, training loss: 320.375244140625 = 1.5688416957855225 + 50.0 * 6.37612771987915
Epoch 300, val loss: 1.586986780166626
Epoch 310, training loss: 320.04815673828125 = 1.5484241247177124 + 50.0 * 6.369994640350342
Epoch 310, val loss: 1.5692018270492554
Epoch 320, training loss: 319.68585205078125 = 1.5271769762039185 + 50.0 * 6.363173007965088
Epoch 320, val loss: 1.550855040550232
Epoch 330, training loss: 319.4051208496094 = 1.5055946111679077 + 50.0 * 6.357990741729736
Epoch 330, val loss: 1.5324881076812744
Epoch 340, training loss: 319.13470458984375 = 1.4836862087249756 + 50.0 * 6.353020668029785
Epoch 340, val loss: 1.514119029045105
Epoch 350, training loss: 319.42950439453125 = 1.461399793624878 + 50.0 * 6.3593621253967285
Epoch 350, val loss: 1.4958112239837646
Epoch 360, training loss: 318.8153076171875 = 1.438960075378418 + 50.0 * 6.347527027130127
Epoch 360, val loss: 1.477304220199585
Epoch 370, training loss: 318.4486083984375 = 1.416534662246704 + 50.0 * 6.340641498565674
Epoch 370, val loss: 1.459230661392212
Epoch 380, training loss: 318.2243957519531 = 1.3941760063171387 + 50.0 * 6.336604595184326
Epoch 380, val loss: 1.4414749145507812
Epoch 390, training loss: 318.03851318359375 = 1.3718689680099487 + 50.0 * 6.3333330154418945
Epoch 390, val loss: 1.4240449666976929
Epoch 400, training loss: 317.8869934082031 = 1.3493618965148926 + 50.0 * 6.330752849578857
Epoch 400, val loss: 1.4066020250320435
Epoch 410, training loss: 317.7227783203125 = 1.3271561861038208 + 50.0 * 6.3279128074646
Epoch 410, val loss: 1.3896737098693848
Epoch 420, training loss: 317.52337646484375 = 1.3050280809402466 + 50.0 * 6.324367046356201
Epoch 420, val loss: 1.373205542564392
Epoch 430, training loss: 317.3336181640625 = 1.2831799983978271 + 50.0 * 6.321009159088135
Epoch 430, val loss: 1.357094407081604
Epoch 440, training loss: 317.5241394042969 = 1.2613950967788696 + 50.0 * 6.325254917144775
Epoch 440, val loss: 1.3410547971725464
Epoch 450, training loss: 317.1067810058594 = 1.2396057844161987 + 50.0 * 6.317343711853027
Epoch 450, val loss: 1.3253523111343384
Epoch 460, training loss: 316.88873291015625 = 1.2180545330047607 + 50.0 * 6.313413619995117
Epoch 460, val loss: 1.3099567890167236
Epoch 470, training loss: 317.0721435546875 = 1.1966632604599 + 50.0 * 6.317509651184082
Epoch 470, val loss: 1.2948459386825562
Epoch 480, training loss: 316.602294921875 = 1.1755789518356323 + 50.0 * 6.308534622192383
Epoch 480, val loss: 1.2797620296478271
Epoch 490, training loss: 316.4415283203125 = 1.1546648740768433 + 50.0 * 6.305737018585205
Epoch 490, val loss: 1.2652174234390259
Epoch 500, training loss: 316.3030700683594 = 1.1340258121490479 + 50.0 * 6.303380966186523
Epoch 500, val loss: 1.250931978225708
Epoch 510, training loss: 316.3964538574219 = 1.113579511642456 + 50.0 * 6.305657386779785
Epoch 510, val loss: 1.2371011972427368
Epoch 520, training loss: 316.2117614746094 = 1.093244194984436 + 50.0 * 6.302370548248291
Epoch 520, val loss: 1.2227048873901367
Epoch 530, training loss: 315.96673583984375 = 1.0732433795928955 + 50.0 * 6.29787015914917
Epoch 530, val loss: 1.2090888023376465
Epoch 540, training loss: 315.8458557128906 = 1.053626298904419 + 50.0 * 6.295845031738281
Epoch 540, val loss: 1.196059226989746
Epoch 550, training loss: 316.098388671875 = 1.034192681312561 + 50.0 * 6.301284313201904
Epoch 550, val loss: 1.1829910278320312
Epoch 560, training loss: 315.7807312011719 = 1.0149739980697632 + 50.0 * 6.295314788818359
Epoch 560, val loss: 1.170318603515625
Epoch 570, training loss: 315.53387451171875 = 0.9961479902267456 + 50.0 * 6.290754318237305
Epoch 570, val loss: 1.1581186056137085
Epoch 580, training loss: 315.4171447753906 = 0.9778172969818115 + 50.0 * 6.288786888122559
Epoch 580, val loss: 1.1464390754699707
Epoch 590, training loss: 315.4130859375 = 0.9598780274391174 + 50.0 * 6.289064407348633
Epoch 590, val loss: 1.1350089311599731
Epoch 600, training loss: 315.3255615234375 = 0.9420723915100098 + 50.0 * 6.287669658660889
Epoch 600, val loss: 1.1237119436264038
Epoch 610, training loss: 315.1744689941406 = 0.9247043132781982 + 50.0 * 6.284995079040527
Epoch 610, val loss: 1.1130967140197754
Epoch 620, training loss: 315.0594482421875 = 0.9078142642974854 + 50.0 * 6.2830328941345215
Epoch 620, val loss: 1.1030077934265137
Epoch 630, training loss: 315.2429504394531 = 0.8912765383720398 + 50.0 * 6.287033557891846
Epoch 630, val loss: 1.0930753946304321
Epoch 640, training loss: 314.93780517578125 = 0.8749362230300903 + 50.0 * 6.281257629394531
Epoch 640, val loss: 1.0837116241455078
Epoch 650, training loss: 314.7878112792969 = 0.859130322933197 + 50.0 * 6.278573513031006
Epoch 650, val loss: 1.0749785900115967
Epoch 660, training loss: 314.7593078613281 = 0.8437300324440002 + 50.0 * 6.278311729431152
Epoch 660, val loss: 1.0666348934173584
Epoch 670, training loss: 314.9671630859375 = 0.828511655330658 + 50.0 * 6.282773017883301
Epoch 670, val loss: 1.0582916736602783
Epoch 680, training loss: 314.6815490722656 = 0.8136687874794006 + 50.0 * 6.277358055114746
Epoch 680, val loss: 1.0505322217941284
Epoch 690, training loss: 314.4931640625 = 0.7992261648178101 + 50.0 * 6.273878574371338
Epoch 690, val loss: 1.0434095859527588
Epoch 700, training loss: 314.3758850097656 = 0.7853227257728577 + 50.0 * 6.271811485290527
Epoch 700, val loss: 1.0368337631225586
Epoch 710, training loss: 314.302490234375 = 0.7717679738998413 + 50.0 * 6.2706146240234375
Epoch 710, val loss: 1.0306782722473145
Epoch 720, training loss: 314.63787841796875 = 0.758527398109436 + 50.0 * 6.277587413787842
Epoch 720, val loss: 1.0247383117675781
Epoch 730, training loss: 314.3123779296875 = 0.7452344298362732 + 50.0 * 6.271342754364014
Epoch 730, val loss: 1.0191975831985474
Epoch 740, training loss: 314.11041259765625 = 0.7324543595314026 + 50.0 * 6.267559051513672
Epoch 740, val loss: 1.0140167474746704
Epoch 750, training loss: 314.0154113769531 = 0.7200348377227783 + 50.0 * 6.2659077644348145
Epoch 750, val loss: 1.0093649625778198
Epoch 760, training loss: 314.2301330566406 = 0.7078621983528137 + 50.0 * 6.270445823669434
Epoch 760, val loss: 1.0050461292266846
Epoch 770, training loss: 314.0599670410156 = 0.6957788467407227 + 50.0 * 6.267283916473389
Epoch 770, val loss: 1.0006780624389648
Epoch 780, training loss: 313.9532775878906 = 0.6839395761489868 + 50.0 * 6.265387058258057
Epoch 780, val loss: 0.9968145489692688
Epoch 790, training loss: 313.80938720703125 = 0.672489583492279 + 50.0 * 6.262738227844238
Epoch 790, val loss: 0.9932447671890259
Epoch 800, training loss: 313.8163146972656 = 0.6612870097160339 + 50.0 * 6.263100624084473
Epoch 800, val loss: 0.9901956915855408
Epoch 810, training loss: 313.87835693359375 = 0.6501332521438599 + 50.0 * 6.264564514160156
Epoch 810, val loss: 0.9869222044944763
Epoch 820, training loss: 313.6703796386719 = 0.6392579078674316 + 50.0 * 6.260622501373291
Epoch 820, val loss: 0.98418128490448
Epoch 830, training loss: 313.547119140625 = 0.6286067366600037 + 50.0 * 6.258370399475098
Epoch 830, val loss: 0.9815950989723206
Epoch 840, training loss: 313.52337646484375 = 0.618187665939331 + 50.0 * 6.258103847503662
Epoch 840, val loss: 0.9794493317604065
Epoch 850, training loss: 313.63995361328125 = 0.6078938245773315 + 50.0 * 6.260641574859619
Epoch 850, val loss: 0.9771773815155029
Epoch 860, training loss: 313.42132568359375 = 0.597514271736145 + 50.0 * 6.256476402282715
Epoch 860, val loss: 0.9751285314559937
Epoch 870, training loss: 313.345947265625 = 0.5874655842781067 + 50.0 * 6.255169868469238
Epoch 870, val loss: 0.9733359813690186
Epoch 880, training loss: 313.2823791503906 = 0.5776426196098328 + 50.0 * 6.254095077514648
Epoch 880, val loss: 0.9717543125152588
Epoch 890, training loss: 313.3287353515625 = 0.5679375529289246 + 50.0 * 6.255216121673584
Epoch 890, val loss: 0.9705336093902588
Epoch 900, training loss: 313.3149719238281 = 0.5581663846969604 + 50.0 * 6.255136013031006
Epoch 900, val loss: 0.9686774611473083
Epoch 910, training loss: 313.2784118652344 = 0.5485267043113708 + 50.0 * 6.2545976638793945
Epoch 910, val loss: 0.9673008918762207
Epoch 920, training loss: 313.2124328613281 = 0.5390850305557251 + 50.0 * 6.253467082977295
Epoch 920, val loss: 0.9661034345626831
Epoch 930, training loss: 313.0196533203125 = 0.5297310948371887 + 50.0 * 6.249798774719238
Epoch 930, val loss: 0.9649191498756409
Epoch 940, training loss: 312.99212646484375 = 0.5205894112586975 + 50.0 * 6.2494306564331055
Epoch 940, val loss: 0.9641141295433044
Epoch 950, training loss: 313.25689697265625 = 0.5114777684211731 + 50.0 * 6.254908561706543
Epoch 950, val loss: 0.9633623957633972
Epoch 960, training loss: 312.9436340332031 = 0.502490758895874 + 50.0 * 6.2488226890563965
Epoch 960, val loss: 0.962450385093689
Epoch 970, training loss: 312.8299865722656 = 0.4935547411441803 + 50.0 * 6.246728420257568
Epoch 970, val loss: 0.9618515372276306
Epoch 980, training loss: 312.98736572265625 = 0.4847879111766815 + 50.0 * 6.250051021575928
Epoch 980, val loss: 0.961422860622406
Epoch 990, training loss: 312.79925537109375 = 0.47598886489868164 + 50.0 * 6.24646520614624
Epoch 990, val loss: 0.9606339931488037
Epoch 1000, training loss: 312.74859619140625 = 0.4673427641391754 + 50.0 * 6.245625019073486
Epoch 1000, val loss: 0.9601580500602722
Epoch 1010, training loss: 312.7093200683594 = 0.4588478207588196 + 50.0 * 6.245008945465088
Epoch 1010, val loss: 0.9596813917160034
Epoch 1020, training loss: 312.823486328125 = 0.45028582215309143 + 50.0 * 6.247464179992676
Epoch 1020, val loss: 0.9593563675880432
Epoch 1030, training loss: 312.6056213378906 = 0.4418123960494995 + 50.0 * 6.243276596069336
Epoch 1030, val loss: 0.9588125944137573
Epoch 1040, training loss: 312.66790771484375 = 0.43348604440689087 + 50.0 * 6.244688510894775
Epoch 1040, val loss: 0.9587167501449585
Epoch 1050, training loss: 312.626708984375 = 0.4251697361469269 + 50.0 * 6.244030952453613
Epoch 1050, val loss: 0.9584178924560547
Epoch 1060, training loss: 312.54925537109375 = 0.4170561134815216 + 50.0 * 6.2426438331604
Epoch 1060, val loss: 0.9582307934761047
Epoch 1070, training loss: 312.4443664550781 = 0.40895509719848633 + 50.0 * 6.240707874298096
Epoch 1070, val loss: 0.9582965970039368
Epoch 1080, training loss: 312.484619140625 = 0.40098458528518677 + 50.0 * 6.241672992706299
Epoch 1080, val loss: 0.9583598971366882
Epoch 1090, training loss: 312.6125793457031 = 0.3930608928203583 + 50.0 * 6.24439001083374
Epoch 1090, val loss: 0.9581193923950195
Epoch 1100, training loss: 312.352783203125 = 0.38500329852104187 + 50.0 * 6.239355564117432
Epoch 1100, val loss: 0.958249568939209
Epoch 1110, training loss: 312.3684997558594 = 0.3771628141403198 + 50.0 * 6.239826679229736
Epoch 1110, val loss: 0.958428144454956
Epoch 1120, training loss: 312.3170471191406 = 0.36937281489372253 + 50.0 * 6.238953113555908
Epoch 1120, val loss: 0.9583083391189575
Epoch 1130, training loss: 312.262451171875 = 0.36166566610336304 + 50.0 * 6.238015651702881
Epoch 1130, val loss: 0.9582923054695129
Epoch 1140, training loss: 312.19610595703125 = 0.3540785014629364 + 50.0 * 6.236840724945068
Epoch 1140, val loss: 0.9586147665977478
Epoch 1150, training loss: 312.2279968261719 = 0.3465985357761383 + 50.0 * 6.23762845993042
Epoch 1150, val loss: 0.9589477777481079
Epoch 1160, training loss: 312.3475646972656 = 0.33906465768814087 + 50.0 * 6.240170001983643
Epoch 1160, val loss: 0.9588949680328369
Epoch 1170, training loss: 312.12774658203125 = 0.33159056305885315 + 50.0 * 6.235922813415527
Epoch 1170, val loss: 0.9593928456306458
Epoch 1180, training loss: 312.09002685546875 = 0.3243539333343506 + 50.0 * 6.235313892364502
Epoch 1180, val loss: 0.9595463275909424
Epoch 1190, training loss: 312.046630859375 = 0.3172593414783478 + 50.0 * 6.234587669372559
Epoch 1190, val loss: 0.9601837992668152
Epoch 1200, training loss: 312.1308288574219 = 0.3103298842906952 + 50.0 * 6.236410140991211
Epoch 1200, val loss: 0.9605544209480286
Epoch 1210, training loss: 312.0838928222656 = 0.3033894896507263 + 50.0 * 6.235610485076904
Epoch 1210, val loss: 0.961463212966919
Epoch 1220, training loss: 312.00335693359375 = 0.2965216338634491 + 50.0 * 6.234137058258057
Epoch 1220, val loss: 0.9617083668708801
Epoch 1230, training loss: 311.9729919433594 = 0.2898198962211609 + 50.0 * 6.233663558959961
Epoch 1230, val loss: 0.962975263595581
Epoch 1240, training loss: 312.0515441894531 = 0.28326424956321716 + 50.0 * 6.235365867614746
Epoch 1240, val loss: 0.9637190103530884
Epoch 1250, training loss: 311.91790771484375 = 0.27683401107788086 + 50.0 * 6.232821464538574
Epoch 1250, val loss: 0.9646161198616028
Epoch 1260, training loss: 311.8599548339844 = 0.2705879807472229 + 50.0 * 6.231787204742432
Epoch 1260, val loss: 0.9660255908966064
Epoch 1270, training loss: 312.24603271484375 = 0.2644255757331848 + 50.0 * 6.2396321296691895
Epoch 1270, val loss: 0.9673742055892944
Epoch 1280, training loss: 311.9059143066406 = 0.2583938241004944 + 50.0 * 6.232950210571289
Epoch 1280, val loss: 0.9685022234916687
Epoch 1290, training loss: 311.7977294921875 = 0.25245240330696106 + 50.0 * 6.230905532836914
Epoch 1290, val loss: 0.9702450037002563
Epoch 1300, training loss: 311.7274169921875 = 0.24681754410266876 + 50.0 * 6.229611873626709
Epoch 1300, val loss: 0.9720670580863953
Epoch 1310, training loss: 311.6874694824219 = 0.2412736862897873 + 50.0 * 6.228923797607422
Epoch 1310, val loss: 0.9740325808525085
Epoch 1320, training loss: 312.0340881347656 = 0.23588275909423828 + 50.0 * 6.235964298248291
Epoch 1320, val loss: 0.9758549332618713
Epoch 1330, training loss: 311.8922424316406 = 0.23044587671756744 + 50.0 * 6.233235836029053
Epoch 1330, val loss: 0.9778344035148621
Epoch 1340, training loss: 311.65753173828125 = 0.22518810629844666 + 50.0 * 6.228646755218506
Epoch 1340, val loss: 0.9799799919128418
Epoch 1350, training loss: 311.6003723144531 = 0.22019025683403015 + 50.0 * 6.227603912353516
Epoch 1350, val loss: 0.9823997020721436
Epoch 1360, training loss: 311.629150390625 = 0.21534380316734314 + 50.0 * 6.228276252746582
Epoch 1360, val loss: 0.9848921895027161
Epoch 1370, training loss: 311.8717041015625 = 0.21055680513381958 + 50.0 * 6.233222484588623
Epoch 1370, val loss: 0.9872897863388062
Epoch 1380, training loss: 311.621337890625 = 0.2057618796825409 + 50.0 * 6.228312015533447
Epoch 1380, val loss: 0.9899812936782837
Epoch 1390, training loss: 311.4980163574219 = 0.2012060135602951 + 50.0 * 6.225936412811279
Epoch 1390, val loss: 0.9925259351730347
Epoch 1400, training loss: 311.4710693359375 = 0.19683118164539337 + 50.0 * 6.225484848022461
Epoch 1400, val loss: 0.9955306649208069
Epoch 1410, training loss: 311.6707458496094 = 0.1925944685935974 + 50.0 * 6.229562759399414
Epoch 1410, val loss: 0.9986521601676941
Epoch 1420, training loss: 311.6515197753906 = 0.18834605813026428 + 50.0 * 6.2292633056640625
Epoch 1420, val loss: 1.001417636871338
Epoch 1430, training loss: 311.515625 = 0.1841602474451065 + 50.0 * 6.22662878036499
Epoch 1430, val loss: 1.0046175718307495
Epoch 1440, training loss: 311.4140930175781 = 0.18017655611038208 + 50.0 * 6.2246785163879395
Epoch 1440, val loss: 1.0078394412994385
Epoch 1450, training loss: 311.3981018066406 = 0.17636753618717194 + 50.0 * 6.224434852600098
Epoch 1450, val loss: 1.0113580226898193
Epoch 1460, training loss: 311.56646728515625 = 0.17263014614582062 + 50.0 * 6.227876663208008
Epoch 1460, val loss: 1.0148825645446777
Epoch 1470, training loss: 311.4177551269531 = 0.16892319917678833 + 50.0 * 6.224977016448975
Epoch 1470, val loss: 1.0183179378509521
Epoch 1480, training loss: 311.3905944824219 = 0.16536884009838104 + 50.0 * 6.224504470825195
Epoch 1480, val loss: 1.0218981504440308
Epoch 1490, training loss: 311.3893127441406 = 0.16186149418354034 + 50.0 * 6.224548816680908
Epoch 1490, val loss: 1.025599718093872
Epoch 1500, training loss: 311.2751159667969 = 0.15842603147029877 + 50.0 * 6.222333908081055
Epoch 1500, val loss: 1.0294829607009888
Epoch 1510, training loss: 311.2775573730469 = 0.15514978766441345 + 50.0 * 6.222448348999023
Epoch 1510, val loss: 1.03337824344635
Epoch 1520, training loss: 311.2848205566406 = 0.1519460529088974 + 50.0 * 6.222657680511475
Epoch 1520, val loss: 1.0373494625091553
Epoch 1530, training loss: 311.5318908691406 = 0.14879773557186127 + 50.0 * 6.227661609649658
Epoch 1530, val loss: 1.040990948677063
Epoch 1540, training loss: 311.3621520996094 = 0.1456875205039978 + 50.0 * 6.224329471588135
Epoch 1540, val loss: 1.0451384782791138
Epoch 1550, training loss: 311.247314453125 = 0.14262819290161133 + 50.0 * 6.22209358215332
Epoch 1550, val loss: 1.0491101741790771
Epoch 1560, training loss: 311.2161560058594 = 0.139735609292984 + 50.0 * 6.221528053283691
Epoch 1560, val loss: 1.0535249710083008
Epoch 1570, training loss: 311.3365478515625 = 0.13690409064292908 + 50.0 * 6.223992824554443
Epoch 1570, val loss: 1.0575733184814453
Epoch 1580, training loss: 311.1996154785156 = 0.1340966522693634 + 50.0 * 6.221310138702393
Epoch 1580, val loss: 1.0620362758636475
Epoch 1590, training loss: 311.24847412109375 = 0.13139809668064117 + 50.0 * 6.222341060638428
Epoch 1590, val loss: 1.066168189048767
Epoch 1600, training loss: 311.1208190917969 = 0.12871301174163818 + 50.0 * 6.219841957092285
Epoch 1600, val loss: 1.0709489583969116
Epoch 1610, training loss: 311.06866455078125 = 0.12614184617996216 + 50.0 * 6.218850612640381
Epoch 1610, val loss: 1.075394868850708
Epoch 1620, training loss: 311.16558837890625 = 0.12365569174289703 + 50.0 * 6.22083854675293
Epoch 1620, val loss: 1.0799862146377563
Epoch 1630, training loss: 311.1114501953125 = 0.12116238474845886 + 50.0 * 6.219805717468262
Epoch 1630, val loss: 1.0843768119812012
Epoch 1640, training loss: 311.1033020019531 = 0.11872151494026184 + 50.0 * 6.219691753387451
Epoch 1640, val loss: 1.0887658596038818
Epoch 1650, training loss: 311.229248046875 = 0.11634127795696259 + 50.0 * 6.222258567810059
Epoch 1650, val loss: 1.093364953994751
Epoch 1660, training loss: 311.08856201171875 = 0.11398342996835709 + 50.0 * 6.219491481781006
Epoch 1660, val loss: 1.0981252193450928
Epoch 1670, training loss: 310.9915771484375 = 0.11177556216716766 + 50.0 * 6.217596054077148
Epoch 1670, val loss: 1.1029198169708252
Epoch 1680, training loss: 310.9523620605469 = 0.10961748659610748 + 50.0 * 6.216854572296143
Epoch 1680, val loss: 1.1077628135681152
Epoch 1690, training loss: 311.32000732421875 = 0.10751006007194519 + 50.0 * 6.224249839782715
Epoch 1690, val loss: 1.1126091480255127
Epoch 1700, training loss: 311.1336975097656 = 0.10534992814064026 + 50.0 * 6.220567226409912
Epoch 1700, val loss: 1.1171337366104126
Epoch 1710, training loss: 311.0008850097656 = 0.10326562076807022 + 50.0 * 6.217952251434326
Epoch 1710, val loss: 1.122012734413147
Epoch 1720, training loss: 310.9184265136719 = 0.10129407793283463 + 50.0 * 6.216342926025391
Epoch 1720, val loss: 1.1268163919448853
Epoch 1730, training loss: 311.10980224609375 = 0.09939096868038177 + 50.0 * 6.220208168029785
Epoch 1730, val loss: 1.13173508644104
Epoch 1740, training loss: 310.92279052734375 = 0.0974309965968132 + 50.0 * 6.2165069580078125
Epoch 1740, val loss: 1.1367498636245728
Epoch 1750, training loss: 310.8564758300781 = 0.09556020051240921 + 50.0 * 6.215218544006348
Epoch 1750, val loss: 1.141727328300476
Epoch 1760, training loss: 310.94384765625 = 0.09376973658800125 + 50.0 * 6.217001438140869
Epoch 1760, val loss: 1.146890640258789
Epoch 1770, training loss: 310.9839782714844 = 0.09197138994932175 + 50.0 * 6.21783971786499
Epoch 1770, val loss: 1.1519427299499512
Epoch 1780, training loss: 310.8646240234375 = 0.09021811932325363 + 50.0 * 6.215488433837891
Epoch 1780, val loss: 1.1564379930496216
Epoch 1790, training loss: 310.90924072265625 = 0.08853301405906677 + 50.0 * 6.216414451599121
Epoch 1790, val loss: 1.1618051528930664
Epoch 1800, training loss: 310.9178771972656 = 0.08686964958906174 + 50.0 * 6.216619968414307
Epoch 1800, val loss: 1.166793942451477
Epoch 1810, training loss: 310.7946472167969 = 0.08521126210689545 + 50.0 * 6.214188575744629
Epoch 1810, val loss: 1.171876072883606
Epoch 1820, training loss: 310.77679443359375 = 0.08363694697618484 + 50.0 * 6.213862895965576
Epoch 1820, val loss: 1.1771295070648193
Epoch 1830, training loss: 310.8011474609375 = 0.08211282640695572 + 50.0 * 6.214380741119385
Epoch 1830, val loss: 1.1821962594985962
Epoch 1840, training loss: 310.9951477050781 = 0.08059677481651306 + 50.0 * 6.218291282653809
Epoch 1840, val loss: 1.1875510215759277
Epoch 1850, training loss: 310.7906188964844 = 0.07908708602190018 + 50.0 * 6.214230537414551
Epoch 1850, val loss: 1.1922467947006226
Epoch 1860, training loss: 310.7485656738281 = 0.07763981074094772 + 50.0 * 6.213418483734131
Epoch 1860, val loss: 1.1973891258239746
Epoch 1870, training loss: 310.73187255859375 = 0.07622848451137543 + 50.0 * 6.213112831115723
Epoch 1870, val loss: 1.202796220779419
Epoch 1880, training loss: 310.8347473144531 = 0.07488324493169785 + 50.0 * 6.2151970863342285
Epoch 1880, val loss: 1.20799720287323
Epoch 1890, training loss: 310.7225646972656 = 0.07350756973028183 + 50.0 * 6.2129807472229
Epoch 1890, val loss: 1.2130024433135986
Epoch 1900, training loss: 310.7624206542969 = 0.072177954018116 + 50.0 * 6.213805198669434
Epoch 1900, val loss: 1.2178900241851807
Epoch 1910, training loss: 310.67547607421875 = 0.07087884843349457 + 50.0 * 6.212091445922852
Epoch 1910, val loss: 1.223205327987671
Epoch 1920, training loss: 310.7320556640625 = 0.06963140517473221 + 50.0 * 6.213248252868652
Epoch 1920, val loss: 1.2282391786575317
Epoch 1930, training loss: 310.87335205078125 = 0.0683940201997757 + 50.0 * 6.216099262237549
Epoch 1930, val loss: 1.2332673072814941
Epoch 1940, training loss: 310.7022705078125 = 0.0671454593539238 + 50.0 * 6.212702751159668
Epoch 1940, val loss: 1.2384926080703735
Epoch 1950, training loss: 310.66815185546875 = 0.06597649306058884 + 50.0 * 6.212043762207031
Epoch 1950, val loss: 1.2436754703521729
Epoch 1960, training loss: 310.8960876464844 = 0.06481824070215225 + 50.0 * 6.216625213623047
Epoch 1960, val loss: 1.2487220764160156
Epoch 1970, training loss: 310.6097106933594 = 0.06367430835962296 + 50.0 * 6.210921287536621
Epoch 1970, val loss: 1.2537150382995605
Epoch 1980, training loss: 310.5630798339844 = 0.06257843226194382 + 50.0 * 6.210010051727295
Epoch 1980, val loss: 1.2587448358535767
Epoch 1990, training loss: 310.7535400390625 = 0.06152646988630295 + 50.0 * 6.213840484619141
Epoch 1990, val loss: 1.2637085914611816
Epoch 2000, training loss: 310.61767578125 = 0.06045330688357353 + 50.0 * 6.21114444732666
Epoch 2000, val loss: 1.2692004442214966
Epoch 2010, training loss: 310.69964599609375 = 0.05940144881606102 + 50.0 * 6.212805271148682
Epoch 2010, val loss: 1.2743819952011108
Epoch 2020, training loss: 310.5093994140625 = 0.058376070111989975 + 50.0 * 6.209020137786865
Epoch 2020, val loss: 1.2792834043502808
Epoch 2030, training loss: 310.5143127441406 = 0.057401321828365326 + 50.0 * 6.2091383934021
Epoch 2030, val loss: 1.2844308614730835
Epoch 2040, training loss: 310.7354736328125 = 0.05645673722028732 + 50.0 * 6.21358060836792
Epoch 2040, val loss: 1.2896265983581543
Epoch 2050, training loss: 310.5422668457031 = 0.05548855662345886 + 50.0 * 6.209735870361328
Epoch 2050, val loss: 1.2944412231445312
Epoch 2060, training loss: 310.5820617675781 = 0.05455269291996956 + 50.0 * 6.210549831390381
Epoch 2060, val loss: 1.29953134059906
Epoch 2070, training loss: 310.6642761230469 = 0.053640689700841904 + 50.0 * 6.212212562561035
Epoch 2070, val loss: 1.3043235540390015
Epoch 2080, training loss: 310.4940185546875 = 0.052726950496435165 + 50.0 * 6.208825588226318
Epoch 2080, val loss: 1.3096981048583984
Epoch 2090, training loss: 310.55206298828125 = 0.05186722055077553 + 50.0 * 6.210003852844238
Epoch 2090, val loss: 1.3147419691085815
Epoch 2100, training loss: 310.5620422363281 = 0.051016468554735184 + 50.0 * 6.2102203369140625
Epoch 2100, val loss: 1.3195748329162598
Epoch 2110, training loss: 310.4657287597656 = 0.05019472539424896 + 50.0 * 6.208310604095459
Epoch 2110, val loss: 1.3246891498565674
Epoch 2120, training loss: 310.4349060058594 = 0.04938758909702301 + 50.0 * 6.2077107429504395
Epoch 2120, val loss: 1.3296759128570557
Epoch 2130, training loss: 310.6502380371094 = 0.048602230846881866 + 50.0 * 6.212032318115234
Epoch 2130, val loss: 1.3346251249313354
Epoch 2140, training loss: 310.5616149902344 = 0.047797344624996185 + 50.0 * 6.2102766036987305
Epoch 2140, val loss: 1.3396775722503662
Epoch 2150, training loss: 310.4425354003906 = 0.04699859023094177 + 50.0 * 6.207911014556885
Epoch 2150, val loss: 1.3445264101028442
Epoch 2160, training loss: 310.4256896972656 = 0.046248503029346466 + 50.0 * 6.207589149475098
Epoch 2160, val loss: 1.3494315147399902
Epoch 2170, training loss: 310.6369934082031 = 0.04551415517926216 + 50.0 * 6.211830139160156
Epoch 2170, val loss: 1.3546319007873535
Epoch 2180, training loss: 310.37799072265625 = 0.04478183388710022 + 50.0 * 6.206664562225342
Epoch 2180, val loss: 1.3590409755706787
Epoch 2190, training loss: 310.3697204589844 = 0.044087983667850494 + 50.0 * 6.206512451171875
Epoch 2190, val loss: 1.3639668226242065
Epoch 2200, training loss: 310.3804016113281 = 0.04340982437133789 + 50.0 * 6.206740379333496
Epoch 2200, val loss: 1.3689113855361938
Epoch 2210, training loss: 310.64337158203125 = 0.04273861646652222 + 50.0 * 6.212013244628906
Epoch 2210, val loss: 1.37362539768219
Epoch 2220, training loss: 310.4338684082031 = 0.04205035790801048 + 50.0 * 6.207836151123047
Epoch 2220, val loss: 1.378552794456482
Epoch 2230, training loss: 310.38031005859375 = 0.041393864899873734 + 50.0 * 6.206778526306152
Epoch 2230, val loss: 1.3834561109542847
Epoch 2240, training loss: 310.4525146484375 = 0.04076366499066353 + 50.0 * 6.208234786987305
Epoch 2240, val loss: 1.3881137371063232
Epoch 2250, training loss: 310.3128356933594 = 0.04013820365071297 + 50.0 * 6.205454349517822
Epoch 2250, val loss: 1.3929903507232666
Epoch 2260, training loss: 310.4156188964844 = 0.039538100361824036 + 50.0 * 6.207521438598633
Epoch 2260, val loss: 1.3978012800216675
Epoch 2270, training loss: 310.39599609375 = 0.03892863914370537 + 50.0 * 6.207140922546387
Epoch 2270, val loss: 1.4024184942245483
Epoch 2280, training loss: 310.3031921386719 = 0.038333989679813385 + 50.0 * 6.205297470092773
Epoch 2280, val loss: 1.4071379899978638
Epoch 2290, training loss: 310.2895812988281 = 0.0377730093896389 + 50.0 * 6.205036163330078
Epoch 2290, val loss: 1.4117519855499268
Epoch 2300, training loss: 310.3600769042969 = 0.03722051531076431 + 50.0 * 6.206457138061523
Epoch 2300, val loss: 1.4165170192718506
Epoch 2310, training loss: 310.4485168457031 = 0.03665199875831604 + 50.0 * 6.208237171173096
Epoch 2310, val loss: 1.4208062887191772
Epoch 2320, training loss: 310.2752380371094 = 0.03610009700059891 + 50.0 * 6.204782962799072
Epoch 2320, val loss: 1.4253116846084595
Epoch 2330, training loss: 310.24786376953125 = 0.035576850175857544 + 50.0 * 6.204245567321777
Epoch 2330, val loss: 1.429983377456665
Epoch 2340, training loss: 310.30792236328125 = 0.0350697822868824 + 50.0 * 6.2054572105407715
Epoch 2340, val loss: 1.4343394041061401
Epoch 2350, training loss: 310.25738525390625 = 0.03456548601388931 + 50.0 * 6.204456329345703
Epoch 2350, val loss: 1.438937783241272
Epoch 2360, training loss: 310.36187744140625 = 0.03406573832035065 + 50.0 * 6.20655632019043
Epoch 2360, val loss: 1.4434020519256592
Epoch 2370, training loss: 310.36199951171875 = 0.03357387334108353 + 50.0 * 6.206568717956543
Epoch 2370, val loss: 1.4479173421859741
Epoch 2380, training loss: 310.28863525390625 = 0.03308292478322983 + 50.0 * 6.205111026763916
Epoch 2380, val loss: 1.4529482126235962
Epoch 2390, training loss: 310.2017822265625 = 0.0326097346842289 + 50.0 * 6.203382968902588
Epoch 2390, val loss: 1.456763505935669
Epoch 2400, training loss: 310.2161560058594 = 0.032162848860025406 + 50.0 * 6.203680038452148
Epoch 2400, val loss: 1.4617422819137573
Epoch 2410, training loss: 310.39825439453125 = 0.03172186762094498 + 50.0 * 6.20733118057251
Epoch 2410, val loss: 1.4660860300064087
Epoch 2420, training loss: 310.1972961425781 = 0.031257014721632004 + 50.0 * 6.2033209800720215
Epoch 2420, val loss: 1.4702397584915161
Epoch 2430, training loss: 310.24725341796875 = 0.030830489471554756 + 50.0 * 6.204328536987305
Epoch 2430, val loss: 1.4747381210327148
Epoch 2440, training loss: 310.2065734863281 = 0.030403634533286095 + 50.0 * 6.203523635864258
Epoch 2440, val loss: 1.4788607358932495
Epoch 2450, training loss: 310.1374206542969 = 0.029983608052134514 + 50.0 * 6.2021484375
Epoch 2450, val loss: 1.4833413362503052
Epoch 2460, training loss: 310.2884521484375 = 0.029587261378765106 + 50.0 * 6.205177307128906
Epoch 2460, val loss: 1.4875365495681763
Epoch 2470, training loss: 310.1864318847656 = 0.029170794412493706 + 50.0 * 6.2031450271606445
Epoch 2470, val loss: 1.4921488761901855
Epoch 2480, training loss: 310.2295837402344 = 0.02877137064933777 + 50.0 * 6.204016208648682
Epoch 2480, val loss: 1.4965393543243408
Epoch 2490, training loss: 310.1782531738281 = 0.028377261012792587 + 50.0 * 6.202997207641602
Epoch 2490, val loss: 1.500589370727539
Epoch 2500, training loss: 310.1904602050781 = 0.02800912596285343 + 50.0 * 6.203248977661133
Epoch 2500, val loss: 1.5046770572662354
Epoch 2510, training loss: 310.2402038574219 = 0.02764253132045269 + 50.0 * 6.204251289367676
Epoch 2510, val loss: 1.5092281103134155
Epoch 2520, training loss: 310.17596435546875 = 0.02725447714328766 + 50.0 * 6.202974319458008
Epoch 2520, val loss: 1.5128800868988037
Epoch 2530, training loss: 310.1297912597656 = 0.02689417079091072 + 50.0 * 6.2020583152771
Epoch 2530, val loss: 1.517232060432434
Epoch 2540, training loss: 310.03997802734375 = 0.026547757908701897 + 50.0 * 6.200268745422363
Epoch 2540, val loss: 1.5216329097747803
Epoch 2550, training loss: 310.1208190917969 = 0.0262154508382082 + 50.0 * 6.201892375946045
Epoch 2550, val loss: 1.5259182453155518
Epoch 2560, training loss: 310.4222106933594 = 0.025868656113743782 + 50.0 * 6.2079267501831055
Epoch 2560, val loss: 1.5297895669937134
Epoch 2570, training loss: 310.1429443359375 = 0.02551310695707798 + 50.0 * 6.202348709106445
Epoch 2570, val loss: 1.5335490703582764
Epoch 2580, training loss: 310.0619812011719 = 0.025186654180288315 + 50.0 * 6.200736045837402
Epoch 2580, val loss: 1.5376695394515991
Epoch 2590, training loss: 310.0245666503906 = 0.024870285764336586 + 50.0 * 6.199994087219238
Epoch 2590, val loss: 1.541527271270752
Epoch 2600, training loss: 310.02197265625 = 0.024571433663368225 + 50.0 * 6.199948310852051
Epoch 2600, val loss: 1.5457533597946167
Epoch 2610, training loss: 310.3961181640625 = 0.024273507297039032 + 50.0 * 6.207437038421631
Epoch 2610, val loss: 1.5494989156723022
Epoch 2620, training loss: 310.3013916015625 = 0.02394554391503334 + 50.0 * 6.2055487632751465
Epoch 2620, val loss: 1.553977131843567
Epoch 2630, training loss: 310.10003662109375 = 0.023632127791643143 + 50.0 * 6.201528549194336
Epoch 2630, val loss: 1.556875467300415
Epoch 2640, training loss: 310.00701904296875 = 0.023334750905632973 + 50.0 * 6.199673175811768
Epoch 2640, val loss: 1.5612215995788574
Epoch 2650, training loss: 310.036865234375 = 0.023056484758853912 + 50.0 * 6.200275897979736
Epoch 2650, val loss: 1.565075397491455
Epoch 2660, training loss: 310.09698486328125 = 0.02277378737926483 + 50.0 * 6.201484203338623
Epoch 2660, val loss: 1.5688424110412598
Epoch 2670, training loss: 310.00067138671875 = 0.0225013867020607 + 50.0 * 6.199563503265381
Epoch 2670, val loss: 1.5729511976242065
Epoch 2680, training loss: 310.0531311035156 = 0.022234978154301643 + 50.0 * 6.200617790222168
Epoch 2680, val loss: 1.5765653848648071
Epoch 2690, training loss: 310.2265319824219 = 0.0219553392380476 + 50.0 * 6.204092025756836
Epoch 2690, val loss: 1.5805474519729614
Epoch 2700, training loss: 309.9678955078125 = 0.021660853177309036 + 50.0 * 6.198924541473389
Epoch 2700, val loss: 1.5837960243225098
Epoch 2710, training loss: 309.9141845703125 = 0.021408651024103165 + 50.0 * 6.197855472564697
Epoch 2710, val loss: 1.588162899017334
Epoch 2720, training loss: 309.9145812988281 = 0.021165909245610237 + 50.0 * 6.1978678703308105
Epoch 2720, val loss: 1.591876745223999
Epoch 2730, training loss: 309.9405212402344 = 0.020929742604494095 + 50.0 * 6.198391914367676
Epoch 2730, val loss: 1.5959285497665405
Epoch 2740, training loss: 310.3273010253906 = 0.020689159631729126 + 50.0 * 6.206132411956787
Epoch 2740, val loss: 1.5996209383010864
Epoch 2750, training loss: 310.0518493652344 = 0.020427130162715912 + 50.0 * 6.200628757476807
Epoch 2750, val loss: 1.6023228168487549
Epoch 2760, training loss: 309.948486328125 = 0.02018723636865616 + 50.0 * 6.198566436767578
Epoch 2760, val loss: 1.6066975593566895
Epoch 2770, training loss: 309.9653625488281 = 0.019961578771471977 + 50.0 * 6.19890832901001
Epoch 2770, val loss: 1.610025405883789
Epoch 2780, training loss: 310.11993408203125 = 0.019732849672436714 + 50.0 * 6.2020039558410645
Epoch 2780, val loss: 1.613817572593689
Epoch 2790, training loss: 309.9603271484375 = 0.0195059385150671 + 50.0 * 6.198816299438477
Epoch 2790, val loss: 1.6169248819351196
Epoch 2800, training loss: 309.9634094238281 = 0.01928715966641903 + 50.0 * 6.198882579803467
Epoch 2800, val loss: 1.6206382513046265
Epoch 2810, training loss: 310.1324462890625 = 0.01906561106443405 + 50.0 * 6.202267169952393
Epoch 2810, val loss: 1.6242716312408447
Epoch 2820, training loss: 309.9263610839844 = 0.018830863758921623 + 50.0 * 6.198150634765625
Epoch 2820, val loss: 1.6274980306625366
Epoch 2830, training loss: 309.865234375 = 0.018621550872921944 + 50.0 * 6.196932315826416
Epoch 2830, val loss: 1.6312363147735596
Epoch 2840, training loss: 309.85986328125 = 0.01842384599149227 + 50.0 * 6.196828365325928
Epoch 2840, val loss: 1.6349016427993774
Epoch 2850, training loss: 309.9347839355469 = 0.018226174637675285 + 50.0 * 6.198331356048584
Epoch 2850, val loss: 1.6384482383728027
Epoch 2860, training loss: 310.0511474609375 = 0.018020523712038994 + 50.0 * 6.200663089752197
Epoch 2860, val loss: 1.6415913105010986
Epoch 2870, training loss: 309.9889221191406 = 0.017827508971095085 + 50.0 * 6.1994218826293945
Epoch 2870, val loss: 1.6448407173156738
Epoch 2880, training loss: 309.867431640625 = 0.017619088292121887 + 50.0 * 6.196996688842773
Epoch 2880, val loss: 1.6483983993530273
Epoch 2890, training loss: 309.8144226074219 = 0.017426563426852226 + 50.0 * 6.195940017700195
Epoch 2890, val loss: 1.6519962549209595
Epoch 2900, training loss: 309.8526611328125 = 0.01725122518837452 + 50.0 * 6.1967082023620605
Epoch 2900, val loss: 1.6553922891616821
Epoch 2910, training loss: 310.01416015625 = 0.017071910202503204 + 50.0 * 6.199942111968994
Epoch 2910, val loss: 1.6589365005493164
Epoch 2920, training loss: 309.8500061035156 = 0.016877595335245132 + 50.0 * 6.196662425994873
Epoch 2920, val loss: 1.661791443824768
Epoch 2930, training loss: 309.8384704589844 = 0.016699939966201782 + 50.0 * 6.196435451507568
Epoch 2930, val loss: 1.6651854515075684
Epoch 2940, training loss: 309.8980712890625 = 0.01652931421995163 + 50.0 * 6.197630405426025
Epoch 2940, val loss: 1.6687480211257935
Epoch 2950, training loss: 310.01947021484375 = 0.016352619975805283 + 50.0 * 6.200062274932861
Epoch 2950, val loss: 1.6716853380203247
Epoch 2960, training loss: 309.9501647949219 = 0.016165021806955338 + 50.0 * 6.1986799240112305
Epoch 2960, val loss: 1.6745549440383911
Epoch 2970, training loss: 309.8706970214844 = 0.015997212380170822 + 50.0 * 6.197093963623047
Epoch 2970, val loss: 1.677897334098816
Epoch 2980, training loss: 309.83612060546875 = 0.01582898572087288 + 50.0 * 6.19640588760376
Epoch 2980, val loss: 1.6810307502746582
Epoch 2990, training loss: 309.8687438964844 = 0.015668975189328194 + 50.0 * 6.197062015533447
Epoch 2990, val loss: 1.6844274997711182
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8392198207696363
=== training gcn model ===
Epoch 0, training loss: 431.7795104980469 = 1.9375371932983398 + 50.0 * 8.596839904785156
Epoch 0, val loss: 1.9336585998535156
Epoch 10, training loss: 431.7347412109375 = 1.9288380146026611 + 50.0 * 8.596117973327637
Epoch 10, val loss: 1.9249539375305176
Epoch 20, training loss: 431.4731140136719 = 1.9183015823364258 + 50.0 * 8.591095924377441
Epoch 20, val loss: 1.914250135421753
Epoch 30, training loss: 429.7685546875 = 1.9051662683486938 + 50.0 * 8.557268142700195
Epoch 30, val loss: 1.9006067514419556
Epoch 40, training loss: 420.17645263671875 = 1.8897042274475098 + 50.0 * 8.365735054016113
Epoch 40, val loss: 1.8849352598190308
Epoch 50, training loss: 383.51556396484375 = 1.8733776807785034 + 50.0 * 7.632843494415283
Epoch 50, val loss: 1.868404507637024
Epoch 60, training loss: 370.2086181640625 = 1.8592369556427002 + 50.0 * 7.366988182067871
Epoch 60, val loss: 1.854926586151123
Epoch 70, training loss: 359.92694091796875 = 1.8461570739746094 + 50.0 * 7.16161584854126
Epoch 70, val loss: 1.8423137664794922
Epoch 80, training loss: 354.061767578125 = 1.8342316150665283 + 50.0 * 7.044550895690918
Epoch 80, val loss: 1.8303914070129395
Epoch 90, training loss: 348.6014709472656 = 1.8237543106079102 + 50.0 * 6.935554027557373
Epoch 90, val loss: 1.819967269897461
Epoch 100, training loss: 343.70001220703125 = 1.8141096830368042 + 50.0 * 6.8377180099487305
Epoch 100, val loss: 1.8104199171066284
Epoch 110, training loss: 338.90545654296875 = 1.8058433532714844 + 50.0 * 6.741991996765137
Epoch 110, val loss: 1.8021595478057861
Epoch 120, training loss: 334.9690856933594 = 1.7983794212341309 + 50.0 * 6.663414478302002
Epoch 120, val loss: 1.794618844985962
Epoch 130, training loss: 332.5187072753906 = 1.7905725240707397 + 50.0 * 6.61456298828125
Epoch 130, val loss: 1.7870067358016968
Epoch 140, training loss: 330.66009521484375 = 1.7823569774627686 + 50.0 * 6.577555179595947
Epoch 140, val loss: 1.7788612842559814
Epoch 150, training loss: 329.22808837890625 = 1.773862361907959 + 50.0 * 6.549084186553955
Epoch 150, val loss: 1.7701606750488281
Epoch 160, training loss: 327.88134765625 = 1.7648135423660278 + 50.0 * 6.5223307609558105
Epoch 160, val loss: 1.7610368728637695
Epoch 170, training loss: 326.7427062988281 = 1.7553437948226929 + 50.0 * 6.499747276306152
Epoch 170, val loss: 1.7515685558319092
Epoch 180, training loss: 325.84228515625 = 1.7452443838119507 + 50.0 * 6.481940746307373
Epoch 180, val loss: 1.7415145635604858
Epoch 190, training loss: 325.1919250488281 = 1.7341481447219849 + 50.0 * 6.469155311584473
Epoch 190, val loss: 1.7304521799087524
Epoch 200, training loss: 324.38653564453125 = 1.7219855785369873 + 50.0 * 6.453290939331055
Epoch 200, val loss: 1.718554139137268
Epoch 210, training loss: 323.82891845703125 = 1.708943247795105 + 50.0 * 6.442399501800537
Epoch 210, val loss: 1.7058254480361938
Epoch 220, training loss: 323.3250427246094 = 1.6948236227035522 + 50.0 * 6.4326043128967285
Epoch 220, val loss: 1.6920735836029053
Epoch 230, training loss: 322.8500671386719 = 1.679581880569458 + 50.0 * 6.423409461975098
Epoch 230, val loss: 1.6773662567138672
Epoch 240, training loss: 322.44281005859375 = 1.6631718873977661 + 50.0 * 6.415592670440674
Epoch 240, val loss: 1.6616054773330688
Epoch 250, training loss: 322.01739501953125 = 1.6456997394561768 + 50.0 * 6.407433986663818
Epoch 250, val loss: 1.644877552986145
Epoch 260, training loss: 321.7367248535156 = 1.626879096031189 + 50.0 * 6.402196407318115
Epoch 260, val loss: 1.6272432804107666
Epoch 270, training loss: 321.2657470703125 = 1.607136607170105 + 50.0 * 6.393172264099121
Epoch 270, val loss: 1.6087003946304321
Epoch 280, training loss: 320.8866271972656 = 1.5862809419631958 + 50.0 * 6.386007308959961
Epoch 280, val loss: 1.5894334316253662
Epoch 290, training loss: 320.5550842285156 = 1.5644569396972656 + 50.0 * 6.379812717437744
Epoch 290, val loss: 1.5694928169250488
Epoch 300, training loss: 320.2403259277344 = 1.5416964292526245 + 50.0 * 6.3739728927612305
Epoch 300, val loss: 1.5489436388015747
Epoch 310, training loss: 320.0406188964844 = 1.5180609226226807 + 50.0 * 6.370450973510742
Epoch 310, val loss: 1.527815818786621
Epoch 320, training loss: 319.6859130859375 = 1.4935150146484375 + 50.0 * 6.3638482093811035
Epoch 320, val loss: 1.5063291788101196
Epoch 330, training loss: 319.43963623046875 = 1.4685263633728027 + 50.0 * 6.359421730041504
Epoch 330, val loss: 1.4846214056015015
Epoch 340, training loss: 319.1764221191406 = 1.443088173866272 + 50.0 * 6.354666709899902
Epoch 340, val loss: 1.4629236459732056
Epoch 350, training loss: 319.07159423828125 = 1.4171087741851807 + 50.0 * 6.353089332580566
Epoch 350, val loss: 1.4411120414733887
Epoch 360, training loss: 318.7073059082031 = 1.3909164667129517 + 50.0 * 6.346327781677246
Epoch 360, val loss: 1.4192299842834473
Epoch 370, training loss: 318.4555358886719 = 1.3644708395004272 + 50.0 * 6.341821193695068
Epoch 370, val loss: 1.3976314067840576
Epoch 380, training loss: 318.3730163574219 = 1.3379124402999878 + 50.0 * 6.340702056884766
Epoch 380, val loss: 1.3762872219085693
Epoch 390, training loss: 318.1287536621094 = 1.3114116191864014 + 50.0 * 6.3363471031188965
Epoch 390, val loss: 1.3549790382385254
Epoch 400, training loss: 317.97650146484375 = 1.2847280502319336 + 50.0 * 6.333835601806641
Epoch 400, val loss: 1.3341925144195557
Epoch 410, training loss: 317.6859130859375 = 1.258334755897522 + 50.0 * 6.328551769256592
Epoch 410, val loss: 1.3138574361801147
Epoch 420, training loss: 317.47833251953125 = 1.23220694065094 + 50.0 * 6.324922561645508
Epoch 420, val loss: 1.2939099073410034
Epoch 430, training loss: 317.3905944824219 = 1.20628821849823 + 50.0 * 6.323685646057129
Epoch 430, val loss: 1.2745102643966675
Epoch 440, training loss: 317.2252502441406 = 1.1803706884384155 + 50.0 * 6.320898056030273
Epoch 440, val loss: 1.2554658651351929
Epoch 450, training loss: 317.0182800292969 = 1.1549733877182007 + 50.0 * 6.317266464233398
Epoch 450, val loss: 1.236981987953186
Epoch 460, training loss: 316.8195495605469 = 1.12995445728302 + 50.0 * 6.3137922286987305
Epoch 460, val loss: 1.2191463708877563
Epoch 470, training loss: 316.8000183105469 = 1.1053169965744019 + 50.0 * 6.313894271850586
Epoch 470, val loss: 1.201905369758606
Epoch 480, training loss: 316.60333251953125 = 1.0811219215393066 + 50.0 * 6.310444355010986
Epoch 480, val loss: 1.1850979328155518
Epoch 490, training loss: 316.3725891113281 = 1.0574429035186768 + 50.0 * 6.306303024291992
Epoch 490, val loss: 1.1689749956130981
Epoch 500, training loss: 316.2610168457031 = 1.0343008041381836 + 50.0 * 6.304534435272217
Epoch 500, val loss: 1.153437852859497
Epoch 510, training loss: 316.22271728515625 = 1.01161789894104 + 50.0 * 6.3042216300964355
Epoch 510, val loss: 1.1385828256607056
Epoch 520, training loss: 315.9920959472656 = 0.9892959594726562 + 50.0 * 6.300056457519531
Epoch 520, val loss: 1.1241644620895386
Epoch 530, training loss: 316.12335205078125 = 0.967560887336731 + 50.0 * 6.3031158447265625
Epoch 530, val loss: 1.1103931665420532
Epoch 540, training loss: 315.7506408691406 = 0.9464200735092163 + 50.0 * 6.296084403991699
Epoch 540, val loss: 1.097182273864746
Epoch 550, training loss: 315.6009521484375 = 0.9256963729858398 + 50.0 * 6.2935051918029785
Epoch 550, val loss: 1.0845354795455933
Epoch 560, training loss: 315.4700012207031 = 0.9056310057640076 + 50.0 * 6.291287422180176
Epoch 560, val loss: 1.0724339485168457
Epoch 570, training loss: 315.77642822265625 = 0.8858186602592468 + 50.0 * 6.297811985015869
Epoch 570, val loss: 1.0607198476791382
Epoch 580, training loss: 315.2715148925781 = 0.8665198683738708 + 50.0 * 6.288100242614746
Epoch 580, val loss: 1.049347162246704
Epoch 590, training loss: 315.1686096191406 = 0.8476889133453369 + 50.0 * 6.286418437957764
Epoch 590, val loss: 1.0385218858718872
Epoch 600, training loss: 315.1217346191406 = 0.8293870687484741 + 50.0 * 6.285847187042236
Epoch 600, val loss: 1.0281833410263062
Epoch 610, training loss: 314.94989013671875 = 0.8112426996231079 + 50.0 * 6.282773017883301
Epoch 610, val loss: 1.0181273221969604
Epoch 620, training loss: 314.9040832519531 = 0.7936258912086487 + 50.0 * 6.2822089195251465
Epoch 620, val loss: 1.0085382461547852
Epoch 630, training loss: 314.7767639160156 = 0.7763931155204773 + 50.0 * 6.280007362365723
Epoch 630, val loss: 0.9994671940803528
Epoch 640, training loss: 314.8262023925781 = 0.7596839070320129 + 50.0 * 6.281330585479736
Epoch 640, val loss: 0.9905866980552673
Epoch 650, training loss: 314.8672790527344 = 0.7429174780845642 + 50.0 * 6.282486915588379
Epoch 650, val loss: 0.9820691347122192
Epoch 660, training loss: 314.5767822265625 = 0.7268868684768677 + 50.0 * 6.2769975662231445
Epoch 660, val loss: 0.9739375114440918
Epoch 670, training loss: 314.4231262207031 = 0.711224377155304 + 50.0 * 6.274238109588623
Epoch 670, val loss: 0.966255784034729
Epoch 680, training loss: 314.328125 = 0.6959804892539978 + 50.0 * 6.272642612457275
Epoch 680, val loss: 0.9589218497276306
Epoch 690, training loss: 314.7461853027344 = 0.6811109781265259 + 50.0 * 6.281301021575928
Epoch 690, val loss: 0.9518756866455078
Epoch 700, training loss: 314.2271423339844 = 0.6662792563438416 + 50.0 * 6.271217346191406
Epoch 700, val loss: 0.9449871778488159
Epoch 710, training loss: 314.11077880859375 = 0.6520192623138428 + 50.0 * 6.2691755294799805
Epoch 710, val loss: 0.9385974407196045
Epoch 720, training loss: 314.0257263183594 = 0.6382049322128296 + 50.0 * 6.267750263214111
Epoch 720, val loss: 0.9326600432395935
Epoch 730, training loss: 314.02618408203125 = 0.6247589588165283 + 50.0 * 6.268028259277344
Epoch 730, val loss: 0.9271007180213928
Epoch 740, training loss: 314.03033447265625 = 0.6114690899848938 + 50.0 * 6.268377304077148
Epoch 740, val loss: 0.9215964078903198
Epoch 750, training loss: 313.8906555175781 = 0.5986724495887756 + 50.0 * 6.265839576721191
Epoch 750, val loss: 0.9165605306625366
Epoch 760, training loss: 313.8017883300781 = 0.5862038135528564 + 50.0 * 6.26431131362915
Epoch 760, val loss: 0.9119579792022705
Epoch 770, training loss: 313.8584289550781 = 0.5740647912025452 + 50.0 * 6.265686988830566
Epoch 770, val loss: 0.9075355529785156
Epoch 780, training loss: 313.77972412109375 = 0.5622243881225586 + 50.0 * 6.264349937438965
Epoch 780, val loss: 0.903446614742279
Epoch 790, training loss: 313.6535339355469 = 0.5506966710090637 + 50.0 * 6.262056827545166
Epoch 790, val loss: 0.899640679359436
Epoch 800, training loss: 313.5466003417969 = 0.5395941734313965 + 50.0 * 6.260140419006348
Epoch 800, val loss: 0.896178662776947
Epoch 810, training loss: 313.5677185058594 = 0.5287967920303345 + 50.0 * 6.260777950286865
Epoch 810, val loss: 0.8930624723434448
Epoch 820, training loss: 313.4871826171875 = 0.5181482434272766 + 50.0 * 6.259380340576172
Epoch 820, val loss: 0.8899734020233154
Epoch 830, training loss: 313.45843505859375 = 0.507768452167511 + 50.0 * 6.2590131759643555
Epoch 830, val loss: 0.8873416185379028
Epoch 840, training loss: 313.3806457519531 = 0.49777892231941223 + 50.0 * 6.257657527923584
Epoch 840, val loss: 0.8848575353622437
Epoch 850, training loss: 313.281982421875 = 0.487981379032135 + 50.0 * 6.255880355834961
Epoch 850, val loss: 0.8825120329856873
Epoch 860, training loss: 313.3722839355469 = 0.4784536361694336 + 50.0 * 6.257876396179199
Epoch 860, val loss: 0.8804749846458435
Epoch 870, training loss: 313.3059387207031 = 0.46910732984542847 + 50.0 * 6.2567362785339355
Epoch 870, val loss: 0.878570556640625
Epoch 880, training loss: 313.10577392578125 = 0.46005740761756897 + 50.0 * 6.2529144287109375
Epoch 880, val loss: 0.8769310116767883
Epoch 890, training loss: 313.1038818359375 = 0.45126327872276306 + 50.0 * 6.253052234649658
Epoch 890, val loss: 0.875438392162323
Epoch 900, training loss: 313.0028076171875 = 0.44265878200531006 + 50.0 * 6.2512030601501465
Epoch 900, val loss: 0.8742733001708984
Epoch 910, training loss: 313.035400390625 = 0.43426331877708435 + 50.0 * 6.252022743225098
Epoch 910, val loss: 0.8732529282569885
Epoch 920, training loss: 313.05096435546875 = 0.4259951114654541 + 50.0 * 6.252499103546143
Epoch 920, val loss: 0.8723379969596863
Epoch 930, training loss: 312.9288635253906 = 0.4179059863090515 + 50.0 * 6.250219345092773
Epoch 930, val loss: 0.8714510202407837
Epoch 940, training loss: 312.8130187988281 = 0.4101414978504181 + 50.0 * 6.2480573654174805
Epoch 940, val loss: 0.870949923992157
Epoch 950, training loss: 312.774169921875 = 0.4025668501853943 + 50.0 * 6.247432231903076
Epoch 950, val loss: 0.8706091046333313
Epoch 960, training loss: 313.0419006347656 = 0.39513635635375977 + 50.0 * 6.252935409545898
Epoch 960, val loss: 0.870320737361908
Epoch 970, training loss: 312.73028564453125 = 0.38773274421691895 + 50.0 * 6.246850967407227
Epoch 970, val loss: 0.870137631893158
Epoch 980, training loss: 312.6633605957031 = 0.3806110620498657 + 50.0 * 6.245655059814453
Epoch 980, val loss: 0.8701795339584351
Epoch 990, training loss: 312.6632080078125 = 0.3736864924430847 + 50.0 * 6.245790481567383
Epoch 990, val loss: 0.870337724685669
Epoch 1000, training loss: 312.62738037109375 = 0.366883248090744 + 50.0 * 6.24521017074585
Epoch 1000, val loss: 0.870610237121582
Epoch 1010, training loss: 312.610107421875 = 0.3602081537246704 + 50.0 * 6.244997978210449
Epoch 1010, val loss: 0.8710576891899109
Epoch 1020, training loss: 312.61395263671875 = 0.3536252975463867 + 50.0 * 6.245206356048584
Epoch 1020, val loss: 0.8715884685516357
Epoch 1030, training loss: 312.5226745605469 = 0.34724241495132446 + 50.0 * 6.243508815765381
Epoch 1030, val loss: 0.8722186088562012
Epoch 1040, training loss: 312.4523010253906 = 0.34093526005744934 + 50.0 * 6.242227077484131
Epoch 1040, val loss: 0.8729414939880371
Epoch 1050, training loss: 312.3905334472656 = 0.3348376452922821 + 50.0 * 6.241113662719727
Epoch 1050, val loss: 0.8738293647766113
Epoch 1060, training loss: 312.46893310546875 = 0.3288743197917938 + 50.0 * 6.242801189422607
Epoch 1060, val loss: 0.8748218417167664
Epoch 1070, training loss: 312.407958984375 = 0.3229619264602661 + 50.0 * 6.241699695587158
Epoch 1070, val loss: 0.8757830858230591
Epoch 1080, training loss: 312.3798828125 = 0.31712183356285095 + 50.0 * 6.241255760192871
Epoch 1080, val loss: 0.8768708109855652
Epoch 1090, training loss: 312.3058166503906 = 0.3114016056060791 + 50.0 * 6.2398881912231445
Epoch 1090, val loss: 0.8779420256614685
Epoch 1100, training loss: 312.19622802734375 = 0.3059312403202057 + 50.0 * 6.2378058433532715
Epoch 1100, val loss: 0.8792821764945984
Epoch 1110, training loss: 312.1689453125 = 0.3005502223968506 + 50.0 * 6.237368106842041
Epoch 1110, val loss: 0.8807505965232849
Epoch 1120, training loss: 312.5121765136719 = 0.2952805161476135 + 50.0 * 6.244337558746338
Epoch 1120, val loss: 0.882037341594696
Epoch 1130, training loss: 312.3990478515625 = 0.2899508476257324 + 50.0 * 6.24218225479126
Epoch 1130, val loss: 0.8836541175842285
Epoch 1140, training loss: 312.0967712402344 = 0.28479352593421936 + 50.0 * 6.236239433288574
Epoch 1140, val loss: 0.8852306008338928
Epoch 1150, training loss: 312.05853271484375 = 0.27978184819221497 + 50.0 * 6.235575199127197
Epoch 1150, val loss: 0.8869045376777649
Epoch 1160, training loss: 312.0152893066406 = 0.27490803599357605 + 50.0 * 6.23480749130249
Epoch 1160, val loss: 0.888656497001648
Epoch 1170, training loss: 312.1343078613281 = 0.27010881900787354 + 50.0 * 6.237284183502197
Epoch 1170, val loss: 0.8904790282249451
Epoch 1180, training loss: 312.23907470703125 = 0.26529034972190857 + 50.0 * 6.239475727081299
Epoch 1180, val loss: 0.8922301530838013
Epoch 1190, training loss: 312.0201416015625 = 0.26055708527565 + 50.0 * 6.235191345214844
Epoch 1190, val loss: 0.8939948081970215
Epoch 1200, training loss: 311.9248962402344 = 0.25601649284362793 + 50.0 * 6.233377933502197
Epoch 1200, val loss: 0.8959917426109314
Epoch 1210, training loss: 311.8840637207031 = 0.25158679485321045 + 50.0 * 6.232649803161621
Epoch 1210, val loss: 0.8980687856674194
Epoch 1220, training loss: 311.84765625 = 0.24725155532360077 + 50.0 * 6.23200798034668
Epoch 1220, val loss: 0.9002956748008728
Epoch 1230, training loss: 311.87786865234375 = 0.24298474192619324 + 50.0 * 6.232697486877441
Epoch 1230, val loss: 0.9025130271911621
Epoch 1240, training loss: 311.86083984375 = 0.2387010157108307 + 50.0 * 6.232442855834961
Epoch 1240, val loss: 0.9046653509140015
Epoch 1250, training loss: 311.8299255371094 = 0.23449194431304932 + 50.0 * 6.231908321380615
Epoch 1250, val loss: 0.9069910049438477
Epoch 1260, training loss: 311.8451843261719 = 0.2304278463125229 + 50.0 * 6.232295036315918
Epoch 1260, val loss: 0.9093303680419922
Epoch 1270, training loss: 311.842529296875 = 0.22642502188682556 + 50.0 * 6.2323222160339355
Epoch 1270, val loss: 0.9116093516349792
Epoch 1280, training loss: 311.7780456542969 = 0.22251419723033905 + 50.0 * 6.231110572814941
Epoch 1280, val loss: 0.9140430092811584
Epoch 1290, training loss: 311.7944030761719 = 0.21868197619915009 + 50.0 * 6.2315144538879395
Epoch 1290, val loss: 0.9167593717575073
Epoch 1300, training loss: 311.7296142578125 = 0.2148905247449875 + 50.0 * 6.230294227600098
Epoch 1300, val loss: 0.9193074107170105
Epoch 1310, training loss: 311.6780090332031 = 0.21118023991584778 + 50.0 * 6.229336261749268
Epoch 1310, val loss: 0.9219680428504944
Epoch 1320, training loss: 311.65789794921875 = 0.2075578272342682 + 50.0 * 6.229007244110107
Epoch 1320, val loss: 0.9247355461120605
Epoch 1330, training loss: 311.6708068847656 = 0.2039967179298401 + 50.0 * 6.229335784912109
Epoch 1330, val loss: 0.9273945093154907
Epoch 1340, training loss: 311.7774658203125 = 0.20047664642333984 + 50.0 * 6.231540203094482
Epoch 1340, val loss: 0.930027961730957
Epoch 1350, training loss: 311.6908264160156 = 0.19699952006340027 + 50.0 * 6.22987699508667
Epoch 1350, val loss: 0.9327852725982666
Epoch 1360, training loss: 311.5837707519531 = 0.19357487559318542 + 50.0 * 6.227804183959961
Epoch 1360, val loss: 0.9355713725090027
Epoch 1370, training loss: 311.5360107421875 = 0.19028113782405853 + 50.0 * 6.226914405822754
Epoch 1370, val loss: 0.9386492967605591
Epoch 1380, training loss: 311.50469970703125 = 0.18705351650714874 + 50.0 * 6.226353168487549
Epoch 1380, val loss: 0.9414808750152588
Epoch 1390, training loss: 311.9072570800781 = 0.18389402329921722 + 50.0 * 6.234467029571533
Epoch 1390, val loss: 0.944506824016571
Epoch 1400, training loss: 311.60333251953125 = 0.18069526553153992 + 50.0 * 6.228452682495117
Epoch 1400, val loss: 0.9471757411956787
Epoch 1410, training loss: 311.4548645019531 = 0.1776050627231598 + 50.0 * 6.2255449295043945
Epoch 1410, val loss: 0.950169563293457
Epoch 1420, training loss: 311.4683532714844 = 0.1746205985546112 + 50.0 * 6.225874423980713
Epoch 1420, val loss: 0.9532459378242493
Epoch 1430, training loss: 311.6008605957031 = 0.171689972281456 + 50.0 * 6.228583335876465
Epoch 1430, val loss: 0.9562788009643555
Epoch 1440, training loss: 311.5431213378906 = 0.16873522102832794 + 50.0 * 6.227487564086914
Epoch 1440, val loss: 0.9595942497253418
Epoch 1450, training loss: 311.39923095703125 = 0.16587530076503754 + 50.0 * 6.224667072296143
Epoch 1450, val loss: 0.9626430869102478
Epoch 1460, training loss: 311.3443298339844 = 0.16309237480163574 + 50.0 * 6.2236247062683105
Epoch 1460, val loss: 0.9660160541534424
Epoch 1470, training loss: 311.37322998046875 = 0.16037604212760925 + 50.0 * 6.224257469177246
Epoch 1470, val loss: 0.9692520499229431
Epoch 1480, training loss: 311.4886474609375 = 0.1576845645904541 + 50.0 * 6.226619243621826
Epoch 1480, val loss: 0.972613513469696
Epoch 1490, training loss: 311.4383850097656 = 0.15504756569862366 + 50.0 * 6.225666522979736
Epoch 1490, val loss: 0.9757907390594482
Epoch 1500, training loss: 311.49346923828125 = 0.15241996943950653 + 50.0 * 6.226821422576904
Epoch 1500, val loss: 0.9792618155479431
Epoch 1510, training loss: 311.3017883300781 = 0.14985930919647217 + 50.0 * 6.223038196563721
Epoch 1510, val loss: 0.9822357296943665
Epoch 1520, training loss: 311.2505187988281 = 0.14736711978912354 + 50.0 * 6.222063064575195
Epoch 1520, val loss: 0.9859910607337952
Epoch 1530, training loss: 311.2210388183594 = 0.14496295154094696 + 50.0 * 6.221521377563477
Epoch 1530, val loss: 0.9893912076950073
Epoch 1540, training loss: 311.3016662597656 = 0.1426030546426773 + 50.0 * 6.223181247711182
Epoch 1540, val loss: 0.9929344058036804
Epoch 1550, training loss: 311.2104187011719 = 0.140220507979393 + 50.0 * 6.221404075622559
Epoch 1550, val loss: 0.9963090419769287
Epoch 1560, training loss: 311.2960205078125 = 0.13789348304271698 + 50.0 * 6.22316312789917
Epoch 1560, val loss: 0.9999811053276062
Epoch 1570, training loss: 311.1663513183594 = 0.1356164664030075 + 50.0 * 6.220614910125732
Epoch 1570, val loss: 1.0034103393554688
Epoch 1580, training loss: 311.3171081542969 = 0.13341936469078064 + 50.0 * 6.2236738204956055
Epoch 1580, val loss: 1.007187008857727
Epoch 1590, training loss: 311.1473388671875 = 0.13119670748710632 + 50.0 * 6.220323085784912
Epoch 1590, val loss: 1.0105493068695068
Epoch 1600, training loss: 311.1173095703125 = 0.129071906208992 + 50.0 * 6.2197651863098145
Epoch 1600, val loss: 1.0144188404083252
Epoch 1610, training loss: 311.2597351074219 = 0.12699586153030396 + 50.0 * 6.222654819488525
Epoch 1610, val loss: 1.0180436372756958
Epoch 1620, training loss: 311.2102355957031 = 0.12491694837808609 + 50.0 * 6.221706390380859
Epoch 1620, val loss: 1.0216401815414429
Epoch 1630, training loss: 311.1249084472656 = 0.1228751391172409 + 50.0 * 6.220040798187256
Epoch 1630, val loss: 1.0254782438278198
Epoch 1640, training loss: 311.1218566894531 = 0.12089464068412781 + 50.0 * 6.220019340515137
Epoch 1640, val loss: 1.0290727615356445
Epoch 1650, training loss: 311.0960693359375 = 0.118961401283741 + 50.0 * 6.219542026519775
Epoch 1650, val loss: 1.0329716205596924
Epoch 1660, training loss: 311.0160217285156 = 0.1170443594455719 + 50.0 * 6.217979431152344
Epoch 1660, val loss: 1.036817193031311
Epoch 1670, training loss: 311.3326110839844 = 0.11519681662321091 + 50.0 * 6.224348068237305
Epoch 1670, val loss: 1.0407755374908447
Epoch 1680, training loss: 311.0721130371094 = 0.11331314593553543 + 50.0 * 6.219176292419434
Epoch 1680, val loss: 1.0443215370178223
Epoch 1690, training loss: 310.9815368652344 = 0.11149517446756363 + 50.0 * 6.217401027679443
Epoch 1690, val loss: 1.048317551612854
Epoch 1700, training loss: 310.970703125 = 0.10974179953336716 + 50.0 * 6.217219352722168
Epoch 1700, val loss: 1.0523134469985962
Epoch 1710, training loss: 311.2120666503906 = 0.1080213263630867 + 50.0 * 6.222080707550049
Epoch 1710, val loss: 1.0559391975402832
Epoch 1720, training loss: 311.0278015136719 = 0.1062721535563469 + 50.0 * 6.218430995941162
Epoch 1720, val loss: 1.059820294380188
Epoch 1730, training loss: 310.93682861328125 = 0.10457008332014084 + 50.0 * 6.216644763946533
Epoch 1730, val loss: 1.063660979270935
Epoch 1740, training loss: 310.89971923828125 = 0.10293219983577728 + 50.0 * 6.215935707092285
Epoch 1740, val loss: 1.0676610469818115
Epoch 1750, training loss: 311.05902099609375 = 0.10132700949907303 + 50.0 * 6.219153881072998
Epoch 1750, val loss: 1.0715522766113281
Epoch 1760, training loss: 311.1049499511719 = 0.09973423928022385 + 50.0 * 6.220104217529297
Epoch 1760, val loss: 1.0755478143692017
Epoch 1770, training loss: 310.9091796875 = 0.09812662750482559 + 50.0 * 6.216221332550049
Epoch 1770, val loss: 1.0792995691299438
Epoch 1780, training loss: 310.8525695800781 = 0.09658858180046082 + 50.0 * 6.2151198387146
Epoch 1780, val loss: 1.0833706855773926
Epoch 1790, training loss: 310.8089294433594 = 0.09508728235960007 + 50.0 * 6.2142767906188965
Epoch 1790, val loss: 1.0873191356658936
Epoch 1800, training loss: 310.8655090332031 = 0.09362930804491043 + 50.0 * 6.215437889099121
Epoch 1800, val loss: 1.0914385318756104
Epoch 1810, training loss: 310.9321594238281 = 0.0921606495976448 + 50.0 * 6.216799736022949
Epoch 1810, val loss: 1.0953980684280396
Epoch 1820, training loss: 310.79510498046875 = 0.0906948521733284 + 50.0 * 6.214087963104248
Epoch 1820, val loss: 1.0991154909133911
Epoch 1830, training loss: 310.7597961425781 = 0.08929314464330673 + 50.0 * 6.213409900665283
Epoch 1830, val loss: 1.1032519340515137
Epoch 1840, training loss: 310.8982849121094 = 0.0879238098859787 + 50.0 * 6.216207504272461
Epoch 1840, val loss: 1.1072790622711182
Epoch 1850, training loss: 310.8570556640625 = 0.08651714026927948 + 50.0 * 6.2154107093811035
Epoch 1850, val loss: 1.1110460758209229
Epoch 1860, training loss: 310.8289794921875 = 0.08514462411403656 + 50.0 * 6.214876651763916
Epoch 1860, val loss: 1.114951491355896
Epoch 1870, training loss: 310.71380615234375 = 0.08382675051689148 + 50.0 * 6.212599277496338
Epoch 1870, val loss: 1.1191844940185547
Epoch 1880, training loss: 310.6796569824219 = 0.08255623281002045 + 50.0 * 6.211942195892334
Epoch 1880, val loss: 1.123282790184021
Epoch 1890, training loss: 310.6763610839844 = 0.08131315559148788 + 50.0 * 6.2119011878967285
Epoch 1890, val loss: 1.1272931098937988
Epoch 1900, training loss: 310.9017028808594 = 0.08009453117847443 + 50.0 * 6.216431617736816
Epoch 1900, val loss: 1.1312527656555176
Epoch 1910, training loss: 310.646240234375 = 0.07883626967668533 + 50.0 * 6.211348533630371
Epoch 1910, val loss: 1.1352217197418213
Epoch 1920, training loss: 310.6806640625 = 0.07763324677944183 + 50.0 * 6.212060451507568
Epoch 1920, val loss: 1.139243245124817
Epoch 1930, training loss: 310.8177185058594 = 0.07644400745630264 + 50.0 * 6.214825630187988
Epoch 1930, val loss: 1.1433202028274536
Epoch 1940, training loss: 310.66986083984375 = 0.07526682317256927 + 50.0 * 6.211892127990723
Epoch 1940, val loss: 1.1472854614257812
Epoch 1950, training loss: 310.757568359375 = 0.07413888722658157 + 50.0 * 6.2136688232421875
Epoch 1950, val loss: 1.1513009071350098
Epoch 1960, training loss: 310.6161804199219 = 0.07298783957958221 + 50.0 * 6.2108635902404785
Epoch 1960, val loss: 1.155385136604309
Epoch 1970, training loss: 310.58795166015625 = 0.07187066227197647 + 50.0 * 6.210321426391602
Epoch 1970, val loss: 1.1594640016555786
Epoch 1980, training loss: 310.57305908203125 = 0.070794977247715 + 50.0 * 6.210045337677002
Epoch 1980, val loss: 1.163487434387207
Epoch 1990, training loss: 310.5693664550781 = 0.06973802298307419 + 50.0 * 6.209992408752441
Epoch 1990, val loss: 1.1676267385482788
Epoch 2000, training loss: 310.9765930175781 = 0.06870241463184357 + 50.0 * 6.21815824508667
Epoch 2000, val loss: 1.1713677644729614
Epoch 2010, training loss: 310.6920471191406 = 0.06763800233602524 + 50.0 * 6.212488174438477
Epoch 2010, val loss: 1.1755545139312744
Epoch 2020, training loss: 310.53558349609375 = 0.0665983185172081 + 50.0 * 6.20937967300415
Epoch 2020, val loss: 1.179535984992981
Epoch 2030, training loss: 310.5254821777344 = 0.06560681015253067 + 50.0 * 6.209197521209717
Epoch 2030, val loss: 1.1836624145507812
Epoch 2040, training loss: 310.58013916015625 = 0.06464311480522156 + 50.0 * 6.210309982299805
Epoch 2040, val loss: 1.1877628564834595
Epoch 2050, training loss: 310.6651916503906 = 0.06367746740579605 + 50.0 * 6.212029933929443
Epoch 2050, val loss: 1.1915627717971802
Epoch 2060, training loss: 310.61663818359375 = 0.06270299851894379 + 50.0 * 6.211078643798828
Epoch 2060, val loss: 1.1954095363616943
Epoch 2070, training loss: 310.57373046875 = 0.06175932660698891 + 50.0 * 6.210239410400391
Epoch 2070, val loss: 1.1996065378189087
Epoch 2080, training loss: 310.4732360839844 = 0.0608377642929554 + 50.0 * 6.208247661590576
Epoch 2080, val loss: 1.20344877243042
Epoch 2090, training loss: 310.4439392089844 = 0.05994736775755882 + 50.0 * 6.2076802253723145
Epoch 2090, val loss: 1.2076499462127686
Epoch 2100, training loss: 310.4743347167969 = 0.05907262861728668 + 50.0 * 6.2083048820495605
Epoch 2100, val loss: 1.211462140083313
Epoch 2110, training loss: 310.7653503417969 = 0.05820981413125992 + 50.0 * 6.214142799377441
Epoch 2110, val loss: 1.215654730796814
Epoch 2120, training loss: 310.61419677734375 = 0.057318489998579025 + 50.0 * 6.211137771606445
Epoch 2120, val loss: 1.2191641330718994
Epoch 2130, training loss: 310.42474365234375 = 0.056447867304086685 + 50.0 * 6.207365989685059
Epoch 2130, val loss: 1.2234814167022705
Epoch 2140, training loss: 310.4140319824219 = 0.0556209422647953 + 50.0 * 6.207168102264404
Epoch 2140, val loss: 1.2274104356765747
Epoch 2150, training loss: 310.4035949707031 = 0.05482301115989685 + 50.0 * 6.20697546005249
Epoch 2150, val loss: 1.2315759658813477
Epoch 2160, training loss: 310.5165100097656 = 0.0540410652756691 + 50.0 * 6.209249019622803
Epoch 2160, val loss: 1.235459327697754
Epoch 2170, training loss: 310.4666748046875 = 0.05323837324976921 + 50.0 * 6.208268642425537
Epoch 2170, val loss: 1.2393765449523926
Epoch 2180, training loss: 310.5668029785156 = 0.0524463877081871 + 50.0 * 6.210286617279053
Epoch 2180, val loss: 1.2431395053863525
Epoch 2190, training loss: 310.36419677734375 = 0.05166713148355484 + 50.0 * 6.2062506675720215
Epoch 2190, val loss: 1.2473112344741821
Epoch 2200, training loss: 310.3288269042969 = 0.050919655710458755 + 50.0 * 6.205557823181152
Epoch 2200, val loss: 1.2513048648834229
Epoch 2210, training loss: 310.3796081542969 = 0.05020216852426529 + 50.0 * 6.206588268280029
Epoch 2210, val loss: 1.2551121711730957
Epoch 2220, training loss: 310.61602783203125 = 0.04948044568300247 + 50.0 * 6.211331367492676
Epoch 2220, val loss: 1.2590057849884033
Epoch 2230, training loss: 310.322265625 = 0.04873628169298172 + 50.0 * 6.205470561981201
Epoch 2230, val loss: 1.2628211975097656
Epoch 2240, training loss: 310.29254150390625 = 0.04803769290447235 + 50.0 * 6.204890251159668
Epoch 2240, val loss: 1.2669007778167725
Epoch 2250, training loss: 310.28326416015625 = 0.047364652156829834 + 50.0 * 6.204718112945557
Epoch 2250, val loss: 1.2707465887069702
Epoch 2260, training loss: 310.2791748046875 = 0.04670319706201553 + 50.0 * 6.204649448394775
Epoch 2260, val loss: 1.2748141288757324
Epoch 2270, training loss: 310.5732727050781 = 0.0460677333176136 + 50.0 * 6.210543632507324
Epoch 2270, val loss: 1.2786368131637573
Epoch 2280, training loss: 310.3273620605469 = 0.045386746525764465 + 50.0 * 6.205639362335205
Epoch 2280, val loss: 1.282329797744751
Epoch 2290, training loss: 310.45379638671875 = 0.04473504051566124 + 50.0 * 6.208180904388428
Epoch 2290, val loss: 1.2861168384552002
Epoch 2300, training loss: 310.27996826171875 = 0.04409470036625862 + 50.0 * 6.20471715927124
Epoch 2300, val loss: 1.290008544921875
Epoch 2310, training loss: 310.2724304199219 = 0.0434817336499691 + 50.0 * 6.204578876495361
Epoch 2310, val loss: 1.293990969657898
Epoch 2320, training loss: 310.27667236328125 = 0.042884111404418945 + 50.0 * 6.204675674438477
Epoch 2320, val loss: 1.2977442741394043
Epoch 2330, training loss: 310.31610107421875 = 0.042291171848773956 + 50.0 * 6.2054762840271
Epoch 2330, val loss: 1.3015254735946655
Epoch 2340, training loss: 310.350830078125 = 0.04169784113764763 + 50.0 * 6.206182956695557
Epoch 2340, val loss: 1.305378794670105
Epoch 2350, training loss: 310.3663635253906 = 0.04110858961939812 + 50.0 * 6.206504821777344
Epoch 2350, val loss: 1.3092041015625
Epoch 2360, training loss: 310.3009948730469 = 0.04052823781967163 + 50.0 * 6.205209255218506
Epoch 2360, val loss: 1.3129557371139526
Epoch 2370, training loss: 310.2944641113281 = 0.0399710088968277 + 50.0 * 6.205089569091797
Epoch 2370, val loss: 1.3168728351593018
Epoch 2380, training loss: 310.2477111816406 = 0.03941161185503006 + 50.0 * 6.204166412353516
Epoch 2380, val loss: 1.3203972578048706
Epoch 2390, training loss: 310.232421875 = 0.03886266052722931 + 50.0 * 6.203871250152588
Epoch 2390, val loss: 1.3241435289382935
Epoch 2400, training loss: 310.18896484375 = 0.03833388909697533 + 50.0 * 6.203012466430664
Epoch 2400, val loss: 1.3279770612716675
Epoch 2410, training loss: 310.2015380859375 = 0.03781342878937721 + 50.0 * 6.203274726867676
Epoch 2410, val loss: 1.3316341638565063
Epoch 2420, training loss: 310.35784912109375 = 0.037302415817976 + 50.0 * 6.206410884857178
Epoch 2420, val loss: 1.3353511095046997
Epoch 2430, training loss: 310.226318359375 = 0.036784302443265915 + 50.0 * 6.20379114151001
Epoch 2430, val loss: 1.338823914527893
Epoch 2440, training loss: 310.16302490234375 = 0.0362745001912117 + 50.0 * 6.2025346755981445
Epoch 2440, val loss: 1.3427282571792603
Epoch 2450, training loss: 310.2038879394531 = 0.035787031054496765 + 50.0 * 6.203361511230469
Epoch 2450, val loss: 1.3463367223739624
Epoch 2460, training loss: 310.2605285644531 = 0.035306353121995926 + 50.0 * 6.204504489898682
Epoch 2460, val loss: 1.3498876094818115
Epoch 2470, training loss: 310.25250244140625 = 0.034819960594177246 + 50.0 * 6.2043538093566895
Epoch 2470, val loss: 1.3532023429870605
Epoch 2480, training loss: 310.1517028808594 = 0.03434242680668831 + 50.0 * 6.202347278594971
Epoch 2480, val loss: 1.357067346572876
Epoch 2490, training loss: 310.1037902832031 = 0.033885061740875244 + 50.0 * 6.2013983726501465
Epoch 2490, val loss: 1.3606702089309692
Epoch 2500, training loss: 310.1803894042969 = 0.0334472693502903 + 50.0 * 6.202938556671143
Epoch 2500, val loss: 1.3643094301223755
Epoch 2510, training loss: 310.31005859375 = 0.03299780935049057 + 50.0 * 6.205541133880615
Epoch 2510, val loss: 1.3678969144821167
Epoch 2520, training loss: 310.15081787109375 = 0.03254718706011772 + 50.0 * 6.202364921569824
Epoch 2520, val loss: 1.3710808753967285
Epoch 2530, training loss: 310.0913391113281 = 0.032116033136844635 + 50.0 * 6.2011847496032715
Epoch 2530, val loss: 1.3748246431350708
Epoch 2540, training loss: 310.19781494140625 = 0.03170149028301239 + 50.0 * 6.203322410583496
Epoch 2540, val loss: 1.3784210681915283
Epoch 2550, training loss: 310.1061706542969 = 0.03127758949995041 + 50.0 * 6.201497554779053
Epoch 2550, val loss: 1.3816128969192505
Epoch 2560, training loss: 310.19293212890625 = 0.03086603432893753 + 50.0 * 6.20324182510376
Epoch 2560, val loss: 1.3849513530731201
Epoch 2570, training loss: 310.2313232421875 = 0.030461490154266357 + 50.0 * 6.204017639160156
Epoch 2570, val loss: 1.3885045051574707
Epoch 2580, training loss: 310.08612060546875 = 0.030056584626436234 + 50.0 * 6.2011213302612305
Epoch 2580, val loss: 1.3918163776397705
Epoch 2590, training loss: 310.00927734375 = 0.029667990282177925 + 50.0 * 6.199592113494873
Epoch 2590, val loss: 1.395349383354187
Epoch 2600, training loss: 310.03106689453125 = 0.02929459512233734 + 50.0 * 6.200035095214844
Epoch 2600, val loss: 1.3988524675369263
Epoch 2610, training loss: 310.2682800292969 = 0.028932027518749237 + 50.0 * 6.204787254333496
Epoch 2610, val loss: 1.4021356105804443
Epoch 2620, training loss: 310.0687561035156 = 0.028547853231430054 + 50.0 * 6.200804710388184
Epoch 2620, val loss: 1.405382513999939
Epoch 2630, training loss: 309.9958190917969 = 0.028171353042125702 + 50.0 * 6.199352741241455
Epoch 2630, val loss: 1.4084018468856812
Epoch 2640, training loss: 310.00506591796875 = 0.02781907096505165 + 50.0 * 6.199544429779053
Epoch 2640, val loss: 1.4119220972061157
Epoch 2650, training loss: 310.25634765625 = 0.027476835995912552 + 50.0 * 6.204577445983887
Epoch 2650, val loss: 1.4148235321044922
Epoch 2660, training loss: 310.0448303222656 = 0.027112727984786034 + 50.0 * 6.20035457611084
Epoch 2660, val loss: 1.4186655282974243
Epoch 2670, training loss: 310.0068054199219 = 0.026767514646053314 + 50.0 * 6.199600696563721
Epoch 2670, val loss: 1.4214342832565308
Epoch 2680, training loss: 309.9955139160156 = 0.026434600353240967 + 50.0 * 6.1993818283081055
Epoch 2680, val loss: 1.4249378442764282
Epoch 2690, training loss: 310.07354736328125 = 0.02611086331307888 + 50.0 * 6.200948238372803
Epoch 2690, val loss: 1.4279394149780273
Epoch 2700, training loss: 310.0570373535156 = 0.025783129036426544 + 50.0 * 6.200624942779541
Epoch 2700, val loss: 1.4307348728179932
Epoch 2710, training loss: 310.0543212890625 = 0.025460200384259224 + 50.0 * 6.200577259063721
Epoch 2710, val loss: 1.4339638948440552
Epoch 2720, training loss: 309.99505615234375 = 0.025144586339592934 + 50.0 * 6.199398040771484
Epoch 2720, val loss: 1.4375221729278564
Epoch 2730, training loss: 310.0578308105469 = 0.02483893744647503 + 50.0 * 6.20065975189209
Epoch 2730, val loss: 1.4403408765792847
Epoch 2740, training loss: 310.0213928222656 = 0.02452780492603779 + 50.0 * 6.199936866760254
Epoch 2740, val loss: 1.4435272216796875
Epoch 2750, training loss: 310.1013488769531 = 0.0242244154214859 + 50.0 * 6.201542377471924
Epoch 2750, val loss: 1.4466062784194946
Epoch 2760, training loss: 309.96160888671875 = 0.023922082036733627 + 50.0 * 6.198753356933594
Epoch 2760, val loss: 1.4494434595108032
Epoch 2770, training loss: 309.9284973144531 = 0.023633688688278198 + 50.0 * 6.1980977058410645
Epoch 2770, val loss: 1.45268976688385
Epoch 2780, training loss: 309.9306640625 = 0.02335328608751297 + 50.0 * 6.198146343231201
Epoch 2780, val loss: 1.4556821584701538
Epoch 2790, training loss: 310.09130859375 = 0.023078005760908127 + 50.0 * 6.201364994049072
Epoch 2790, val loss: 1.4587206840515137
Epoch 2800, training loss: 310.0028991699219 = 0.02279607579112053 + 50.0 * 6.199602127075195
Epoch 2800, val loss: 1.4615979194641113
Epoch 2810, training loss: 309.8992614746094 = 0.022518401965498924 + 50.0 * 6.197535037994385
Epoch 2810, val loss: 1.4645435810089111
Epoch 2820, training loss: 309.98883056640625 = 0.022254979237914085 + 50.0 * 6.199331760406494
Epoch 2820, val loss: 1.4677941799163818
Epoch 2830, training loss: 309.98248291015625 = 0.021985454484820366 + 50.0 * 6.199210166931152
Epoch 2830, val loss: 1.4703510999679565
Epoch 2840, training loss: 309.96484375 = 0.021718954667448997 + 50.0 * 6.198862552642822
Epoch 2840, val loss: 1.4732218980789185
Epoch 2850, training loss: 309.94757080078125 = 0.02146085724234581 + 50.0 * 6.198522090911865
Epoch 2850, val loss: 1.4761154651641846
Epoch 2860, training loss: 309.8324279785156 = 0.02120337262749672 + 50.0 * 6.196224212646484
Epoch 2860, val loss: 1.4791702032089233
Epoch 2870, training loss: 309.8667907714844 = 0.020961973816156387 + 50.0 * 6.196916580200195
Epoch 2870, val loss: 1.4821081161499023
Epoch 2880, training loss: 309.9059143066406 = 0.020724527537822723 + 50.0 * 6.197703838348389
Epoch 2880, val loss: 1.4850108623504639
Epoch 2890, training loss: 309.9365539550781 = 0.020487211644649506 + 50.0 * 6.198321342468262
Epoch 2890, val loss: 1.487756609916687
Epoch 2900, training loss: 310.1456298828125 = 0.020249323919415474 + 50.0 * 6.202507495880127
Epoch 2900, val loss: 1.490513563156128
Epoch 2910, training loss: 309.9096374511719 = 0.02000468224287033 + 50.0 * 6.1977925300598145
Epoch 2910, val loss: 1.4933637380599976
Epoch 2920, training loss: 309.7988586425781 = 0.019767900928854942 + 50.0 * 6.195581912994385
Epoch 2920, val loss: 1.4959378242492676
Epoch 2930, training loss: 309.80059814453125 = 0.019546816125512123 + 50.0 * 6.195621013641357
Epoch 2930, val loss: 1.498978614807129
Epoch 2940, training loss: 309.865966796875 = 0.019334232434630394 + 50.0 * 6.196932315826416
Epoch 2940, val loss: 1.5016624927520752
Epoch 2950, training loss: 310.1056213378906 = 0.019123872742056847 + 50.0 * 6.201729774475098
Epoch 2950, val loss: 1.5041035413742065
Epoch 2960, training loss: 309.8822326660156 = 0.018887123093008995 + 50.0 * 6.197267055511475
Epoch 2960, val loss: 1.5070048570632935
Epoch 2970, training loss: 309.78741455078125 = 0.01867169514298439 + 50.0 * 6.195374965667725
Epoch 2970, val loss: 1.509436845779419
Epoch 2980, training loss: 309.77874755859375 = 0.01846913993358612 + 50.0 * 6.1952056884765625
Epoch 2980, val loss: 1.5125690698623657
Epoch 2990, training loss: 309.8065185546875 = 0.01827174797654152 + 50.0 * 6.195764541625977
Epoch 2990, val loss: 1.5150980949401855
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8386926726410122
=== training gcn model ===
Epoch 0, training loss: 431.79730224609375 = 1.9542200565338135 + 50.0 * 8.596861839294434
Epoch 0, val loss: 1.9592913389205933
Epoch 10, training loss: 431.75677490234375 = 1.9448232650756836 + 50.0 * 8.59623908996582
Epoch 10, val loss: 1.9490711688995361
Epoch 20, training loss: 431.5379943847656 = 1.932701826095581 + 50.0 * 8.592105865478516
Epoch 20, val loss: 1.9358181953430176
Epoch 30, training loss: 430.12823486328125 = 1.9172658920288086 + 50.0 * 8.56421947479248
Epoch 30, val loss: 1.9191949367523193
Epoch 40, training loss: 422.0670166015625 = 1.89913809299469 + 50.0 * 8.40335750579834
Epoch 40, val loss: 1.9005773067474365
Epoch 50, training loss: 388.7890319824219 = 1.8789781332015991 + 50.0 * 7.738201141357422
Epoch 50, val loss: 1.8803156614303589
Epoch 60, training loss: 375.87939453125 = 1.860520362854004 + 50.0 * 7.480377197265625
Epoch 60, val loss: 1.8627678155899048
Epoch 70, training loss: 366.1176452636719 = 1.8487787246704102 + 50.0 * 7.285377025604248
Epoch 70, val loss: 1.8515037298202515
Epoch 80, training loss: 354.9839782714844 = 1.838455080986023 + 50.0 * 7.062911033630371
Epoch 80, val loss: 1.8416423797607422
Epoch 90, training loss: 346.5390319824219 = 1.829061508178711 + 50.0 * 6.894199371337891
Epoch 90, val loss: 1.832641839981079
Epoch 100, training loss: 342.32513427734375 = 1.8205235004425049 + 50.0 * 6.810092449188232
Epoch 100, val loss: 1.8245136737823486
Epoch 110, training loss: 339.39971923828125 = 1.8113150596618652 + 50.0 * 6.751768112182617
Epoch 110, val loss: 1.816008448600769
Epoch 120, training loss: 337.0663146972656 = 1.8024064302444458 + 50.0 * 6.705278396606445
Epoch 120, val loss: 1.8077175617218018
Epoch 130, training loss: 335.11383056640625 = 1.794360876083374 + 50.0 * 6.666388988494873
Epoch 130, val loss: 1.8001385927200317
Epoch 140, training loss: 333.2062683105469 = 1.7868455648422241 + 50.0 * 6.628388404846191
Epoch 140, val loss: 1.7931023836135864
Epoch 150, training loss: 331.3274841308594 = 1.7794716358184814 + 50.0 * 6.5909600257873535
Epoch 150, val loss: 1.7862175703048706
Epoch 160, training loss: 329.8047180175781 = 1.7718955278396606 + 50.0 * 6.560656547546387
Epoch 160, val loss: 1.7793550491333008
Epoch 170, training loss: 328.2970275878906 = 1.7635929584503174 + 50.0 * 6.53066873550415
Epoch 170, val loss: 1.7718473672866821
Epoch 180, training loss: 327.1179504394531 = 1.7545994520187378 + 50.0 * 6.507266998291016
Epoch 180, val loss: 1.7638863325119019
Epoch 190, training loss: 326.2662048339844 = 1.7446436882019043 + 50.0 * 6.490431785583496
Epoch 190, val loss: 1.7550476789474487
Epoch 200, training loss: 325.40570068359375 = 1.7334778308868408 + 50.0 * 6.47344446182251
Epoch 200, val loss: 1.7454888820648193
Epoch 210, training loss: 324.7445983886719 = 1.7214725017547607 + 50.0 * 6.46046257019043
Epoch 210, val loss: 1.735192060470581
Epoch 220, training loss: 324.2742919921875 = 1.7084887027740479 + 50.0 * 6.451315879821777
Epoch 220, val loss: 1.7241368293762207
Epoch 230, training loss: 323.6671142578125 = 1.6946150064468384 + 50.0 * 6.439450263977051
Epoch 230, val loss: 1.7122924327850342
Epoch 240, training loss: 323.11578369140625 = 1.6797876358032227 + 50.0 * 6.428719997406006
Epoch 240, val loss: 1.6998023986816406
Epoch 250, training loss: 323.0274658203125 = 1.6640368700027466 + 50.0 * 6.4272685050964355
Epoch 250, val loss: 1.6865496635437012
Epoch 260, training loss: 322.2729187011719 = 1.6473524570465088 + 50.0 * 6.412511825561523
Epoch 260, val loss: 1.6724812984466553
Epoch 270, training loss: 321.87933349609375 = 1.6298375129699707 + 50.0 * 6.404989719390869
Epoch 270, val loss: 1.6578947305679321
Epoch 280, training loss: 321.4832763671875 = 1.6114858388900757 + 50.0 * 6.397436141967773
Epoch 280, val loss: 1.6425843238830566
Epoch 290, training loss: 321.5709228515625 = 1.5925348997116089 + 50.0 * 6.399567604064941
Epoch 290, val loss: 1.6266545057296753
Epoch 300, training loss: 320.93304443359375 = 1.572624921798706 + 50.0 * 6.387208461761475
Epoch 300, val loss: 1.610310673713684
Epoch 310, training loss: 320.51806640625 = 1.5523335933685303 + 50.0 * 6.379314422607422
Epoch 310, val loss: 1.5937086343765259
Epoch 320, training loss: 320.2354736328125 = 1.5317753553390503 + 50.0 * 6.3740739822387695
Epoch 320, val loss: 1.576941728591919
Epoch 330, training loss: 320.0567626953125 = 1.510861873626709 + 50.0 * 6.370918273925781
Epoch 330, val loss: 1.5599546432495117
Epoch 340, training loss: 319.79547119140625 = 1.489685297012329 + 50.0 * 6.366115570068359
Epoch 340, val loss: 1.5428498983383179
Epoch 350, training loss: 319.4903869628906 = 1.4685120582580566 + 50.0 * 6.360437870025635
Epoch 350, val loss: 1.525925874710083
Epoch 360, training loss: 319.22747802734375 = 1.4472883939743042 + 50.0 * 6.3556036949157715
Epoch 360, val loss: 1.5090566873550415
Epoch 370, training loss: 319.27532958984375 = 1.425912857055664 + 50.0 * 6.356988430023193
Epoch 370, val loss: 1.4923995733261108
Epoch 380, training loss: 318.8464050292969 = 1.4044939279556274 + 50.0 * 6.3488383293151855
Epoch 380, val loss: 1.4755330085754395
Epoch 390, training loss: 318.8182373046875 = 1.382956624031067 + 50.0 * 6.348705291748047
Epoch 390, val loss: 1.458674430847168
Epoch 400, training loss: 318.4076843261719 = 1.3611657619476318 + 50.0 * 6.340930461883545
Epoch 400, val loss: 1.4420948028564453
Epoch 410, training loss: 318.236083984375 = 1.3393638134002686 + 50.0 * 6.337934494018555
Epoch 410, val loss: 1.4254662990570068
Epoch 420, training loss: 318.1204833984375 = 1.317421793937683 + 50.0 * 6.336061477661133
Epoch 420, val loss: 1.4089992046356201
Epoch 430, training loss: 317.8995361328125 = 1.2952855825424194 + 50.0 * 6.332084655761719
Epoch 430, val loss: 1.3923990726470947
Epoch 440, training loss: 317.83856201171875 = 1.272796630859375 + 50.0 * 6.331315517425537
Epoch 440, val loss: 1.3758898973464966
Epoch 450, training loss: 317.6016540527344 = 1.250373125076294 + 50.0 * 6.327025890350342
Epoch 450, val loss: 1.359217643737793
Epoch 460, training loss: 317.3506774902344 = 1.2277398109436035 + 50.0 * 6.322458744049072
Epoch 460, val loss: 1.3426543474197388
Epoch 470, training loss: 317.2706604003906 = 1.20505690574646 + 50.0 * 6.321311950683594
Epoch 470, val loss: 1.326323390007019
Epoch 480, training loss: 317.0441589355469 = 1.1821874380111694 + 50.0 * 6.317239284515381
Epoch 480, val loss: 1.3098076581954956
Epoch 490, training loss: 317.1117858886719 = 1.1592090129852295 + 50.0 * 6.319051742553711
Epoch 490, val loss: 1.2937291860580444
Epoch 500, training loss: 316.9752502441406 = 1.1363418102264404 + 50.0 * 6.316778182983398
Epoch 500, val loss: 1.2772817611694336
Epoch 510, training loss: 316.68743896484375 = 1.1135833263397217 + 50.0 * 6.311477184295654
Epoch 510, val loss: 1.2611948251724243
Epoch 520, training loss: 316.49920654296875 = 1.0910491943359375 + 50.0 * 6.308163166046143
Epoch 520, val loss: 1.2456687688827515
Epoch 530, training loss: 316.35223388671875 = 1.0688652992248535 + 50.0 * 6.305666923522949
Epoch 530, val loss: 1.2305265665054321
Epoch 540, training loss: 316.65435791015625 = 1.0468825101852417 + 50.0 * 6.312149524688721
Epoch 540, val loss: 1.2159733772277832
Epoch 550, training loss: 316.1298828125 = 1.0251622200012207 + 50.0 * 6.302093982696533
Epoch 550, val loss: 1.201169490814209
Epoch 560, training loss: 316.03369140625 = 1.0038548707962036 + 50.0 * 6.300597190856934
Epoch 560, val loss: 1.1869781017303467
Epoch 570, training loss: 315.8876647949219 = 0.9831080436706543 + 50.0 * 6.298091411590576
Epoch 570, val loss: 1.1734447479248047
Epoch 580, training loss: 315.940673828125 = 0.9628074765205383 + 50.0 * 6.299557209014893
Epoch 580, val loss: 1.1605658531188965
Epoch 590, training loss: 315.738525390625 = 0.9429605007171631 + 50.0 * 6.2959113121032715
Epoch 590, val loss: 1.1482536792755127
Epoch 600, training loss: 315.9381103515625 = 0.9235950708389282 + 50.0 * 6.300290584564209
Epoch 600, val loss: 1.1362982988357544
Epoch 610, training loss: 315.5478210449219 = 0.9044547080993652 + 50.0 * 6.292867660522461
Epoch 610, val loss: 1.125123143196106
Epoch 620, training loss: 315.49609375 = 0.8859559893608093 + 50.0 * 6.292202472686768
Epoch 620, val loss: 1.1144484281539917
Epoch 630, training loss: 315.2965087890625 = 0.8677812218666077 + 50.0 * 6.288574695587158
Epoch 630, val loss: 1.1044470071792603
Epoch 640, training loss: 315.22222900390625 = 0.8501569628715515 + 50.0 * 6.287441253662109
Epoch 640, val loss: 1.0948257446289062
Epoch 650, training loss: 315.13604736328125 = 0.832789421081543 + 50.0 * 6.286065101623535
Epoch 650, val loss: 1.0857059955596924
Epoch 660, training loss: 315.0107116699219 = 0.8157969117164612 + 50.0 * 6.28389835357666
Epoch 660, val loss: 1.077143907546997
Epoch 670, training loss: 315.0426330566406 = 0.7992767095565796 + 50.0 * 6.284866809844971
Epoch 670, val loss: 1.0688339471817017
Epoch 680, training loss: 314.8992614746094 = 0.7829892635345459 + 50.0 * 6.282325744628906
Epoch 680, val loss: 1.0612869262695312
Epoch 690, training loss: 314.8746032714844 = 0.7671290636062622 + 50.0 * 6.282149791717529
Epoch 690, val loss: 1.053972601890564
Epoch 700, training loss: 314.7185363769531 = 0.7514684796333313 + 50.0 * 6.279341697692871
Epoch 700, val loss: 1.0470751523971558
Epoch 710, training loss: 314.67095947265625 = 0.736208438873291 + 50.0 * 6.278695106506348
Epoch 710, val loss: 1.0407763719558716
Epoch 720, training loss: 314.55792236328125 = 0.7211780548095703 + 50.0 * 6.276734828948975
Epoch 720, val loss: 1.034393548965454
Epoch 730, training loss: 314.5271301269531 = 0.7063974142074585 + 50.0 * 6.27641487121582
Epoch 730, val loss: 1.0286839008331299
Epoch 740, training loss: 314.57025146484375 = 0.691795825958252 + 50.0 * 6.277568817138672
Epoch 740, val loss: 1.0229616165161133
Epoch 750, training loss: 314.32513427734375 = 0.6773693561553955 + 50.0 * 6.272955417633057
Epoch 750, val loss: 1.0173453092575073
Epoch 760, training loss: 314.24468994140625 = 0.6632660627365112 + 50.0 * 6.271628379821777
Epoch 760, val loss: 1.0123623609542847
Epoch 770, training loss: 314.1747741699219 = 0.6494377255439758 + 50.0 * 6.270506381988525
Epoch 770, val loss: 1.0073660612106323
Epoch 780, training loss: 314.37615966796875 = 0.6358426809310913 + 50.0 * 6.274806499481201
Epoch 780, val loss: 1.0027247667312622
Epoch 790, training loss: 314.27593994140625 = 0.6222562193870544 + 50.0 * 6.273073673248291
Epoch 790, val loss: 0.9981389045715332
Epoch 800, training loss: 314.01336669921875 = 0.6089087724685669 + 50.0 * 6.2680888175964355
Epoch 800, val loss: 0.9939005374908447
Epoch 810, training loss: 313.9345703125 = 0.5959877371788025 + 50.0 * 6.2667717933654785
Epoch 810, val loss: 0.9896899461746216
Epoch 820, training loss: 313.8490905761719 = 0.5832816362380981 + 50.0 * 6.265316009521484
Epoch 820, val loss: 0.9860445261001587
Epoch 830, training loss: 314.05316162109375 = 0.5708034634590149 + 50.0 * 6.269647121429443
Epoch 830, val loss: 0.9823052287101746
Epoch 840, training loss: 313.8831481933594 = 0.5584691762924194 + 50.0 * 6.266493320465088
Epoch 840, val loss: 0.9790748953819275
Epoch 850, training loss: 313.7281799316406 = 0.5463135838508606 + 50.0 * 6.263637065887451
Epoch 850, val loss: 0.9757993221282959
Epoch 860, training loss: 313.64410400390625 = 0.5345145463943481 + 50.0 * 6.2621917724609375
Epoch 860, val loss: 0.9726450443267822
Epoch 870, training loss: 313.58551025390625 = 0.5229573845863342 + 50.0 * 6.261251449584961
Epoch 870, val loss: 0.9700491428375244
Epoch 880, training loss: 313.6687927246094 = 0.5116137266159058 + 50.0 * 6.263143062591553
Epoch 880, val loss: 0.967361569404602
Epoch 890, training loss: 313.59033203125 = 0.5005102753639221 + 50.0 * 6.261796474456787
Epoch 890, val loss: 0.9645599126815796
Epoch 900, training loss: 313.4539794921875 = 0.4895922541618347 + 50.0 * 6.2592878341674805
Epoch 900, val loss: 0.962570309638977
Epoch 910, training loss: 313.4711608886719 = 0.4790607690811157 + 50.0 * 6.2598419189453125
Epoch 910, val loss: 0.9603714346885681
Epoch 920, training loss: 313.4180603027344 = 0.4686650335788727 + 50.0 * 6.258987903594971
Epoch 920, val loss: 0.9586375951766968
Epoch 930, training loss: 313.2637939453125 = 0.458523154258728 + 50.0 * 6.256105422973633
Epoch 930, val loss: 0.9570632576942444
Epoch 940, training loss: 313.22271728515625 = 0.4487161338329315 + 50.0 * 6.25547981262207
Epoch 940, val loss: 0.9558244347572327
Epoch 950, training loss: 313.2521057128906 = 0.4391348659992218 + 50.0 * 6.256259441375732
Epoch 950, val loss: 0.9547401666641235
Epoch 960, training loss: 313.2337646484375 = 0.4297628700733185 + 50.0 * 6.256080150604248
Epoch 960, val loss: 0.9533266425132751
Epoch 970, training loss: 313.2121887207031 = 0.4204791784286499 + 50.0 * 6.255834579467773
Epoch 970, val loss: 0.9524140357971191
Epoch 980, training loss: 313.1055908203125 = 0.4115261137485504 + 50.0 * 6.253881454467773
Epoch 980, val loss: 0.9512325525283813
Epoch 990, training loss: 312.99395751953125 = 0.402784526348114 + 50.0 * 6.251823902130127
Epoch 990, val loss: 0.950657844543457
Epoch 1000, training loss: 313.02423095703125 = 0.39433184266090393 + 50.0 * 6.252598285675049
Epoch 1000, val loss: 0.9501833319664001
Epoch 1010, training loss: 313.0225830078125 = 0.3860100209712982 + 50.0 * 6.2527313232421875
Epoch 1010, val loss: 0.9496630430221558
Epoch 1020, training loss: 312.9561462402344 = 0.3779072165489197 + 50.0 * 6.251564979553223
Epoch 1020, val loss: 0.949379563331604
Epoch 1030, training loss: 312.92242431640625 = 0.36994466185569763 + 50.0 * 6.251049995422363
Epoch 1030, val loss: 0.9493119120597839
Epoch 1040, training loss: 312.8681945800781 = 0.36220744252204895 + 50.0 * 6.250119686126709
Epoch 1040, val loss: 0.9493296146392822
Epoch 1050, training loss: 312.9622497558594 = 0.3546375334262848 + 50.0 * 6.252151966094971
Epoch 1050, val loss: 0.9494471549987793
Epoch 1060, training loss: 312.7649841308594 = 0.3472307026386261 + 50.0 * 6.248355388641357
Epoch 1060, val loss: 0.9495260715484619
Epoch 1070, training loss: 312.7182922363281 = 0.3400469124317169 + 50.0 * 6.247564792633057
Epoch 1070, val loss: 0.9498852491378784
Epoch 1080, training loss: 312.93792724609375 = 0.33300143480300903 + 50.0 * 6.252098560333252
Epoch 1080, val loss: 0.9503024816513062
Epoch 1090, training loss: 312.8006896972656 = 0.3260765075683594 + 50.0 * 6.249492168426514
Epoch 1090, val loss: 0.9507849812507629
Epoch 1100, training loss: 312.6778564453125 = 0.31933102011680603 + 50.0 * 6.247170448303223
Epoch 1100, val loss: 0.9511713981628418
Epoch 1110, training loss: 312.5575256347656 = 0.3127579689025879 + 50.0 * 6.2448954582214355
Epoch 1110, val loss: 0.9522368311882019
Epoch 1120, training loss: 312.5152893066406 = 0.30639320611953735 + 50.0 * 6.24417781829834
Epoch 1120, val loss: 0.9531203508377075
Epoch 1130, training loss: 312.7076721191406 = 0.30015188455581665 + 50.0 * 6.248150825500488
Epoch 1130, val loss: 0.9543665051460266
Epoch 1140, training loss: 312.51031494140625 = 0.29397687315940857 + 50.0 * 6.244326591491699
Epoch 1140, val loss: 0.9551041126251221
Epoch 1150, training loss: 312.5328063964844 = 0.2879885137081146 + 50.0 * 6.244896411895752
Epoch 1150, val loss: 0.9563024640083313
Epoch 1160, training loss: 312.4172058105469 = 0.282077819108963 + 50.0 * 6.242702484130859
Epoch 1160, val loss: 0.9577232003211975
Epoch 1170, training loss: 312.40228271484375 = 0.27640071511268616 + 50.0 * 6.242517471313477
Epoch 1170, val loss: 0.9592171907424927
Epoch 1180, training loss: 312.4493408203125 = 0.27080419659614563 + 50.0 * 6.243570804595947
Epoch 1180, val loss: 0.9607096314430237
Epoch 1190, training loss: 312.4264831542969 = 0.2652701735496521 + 50.0 * 6.243224620819092
Epoch 1190, val loss: 0.9621071219444275
Epoch 1200, training loss: 312.3853454589844 = 0.25985831022262573 + 50.0 * 6.242509841918945
Epoch 1200, val loss: 0.9638206362724304
Epoch 1210, training loss: 312.39788818359375 = 0.2545420229434967 + 50.0 * 6.2428669929504395
Epoch 1210, val loss: 0.965421199798584
Epoch 1220, training loss: 312.2045593261719 = 0.2493981122970581 + 50.0 * 6.239103317260742
Epoch 1220, val loss: 0.9672569036483765
Epoch 1230, training loss: 312.16082763671875 = 0.24435971677303314 + 50.0 * 6.2383294105529785
Epoch 1230, val loss: 0.9692734479904175
Epoch 1240, training loss: 312.1165771484375 = 0.23945260047912598 + 50.0 * 6.237542629241943
Epoch 1240, val loss: 0.9713007211685181
Epoch 1250, training loss: 312.26123046875 = 0.23466180264949799 + 50.0 * 6.2405314445495605
Epoch 1250, val loss: 0.973360538482666
Epoch 1260, training loss: 312.28009033203125 = 0.22993984818458557 + 50.0 * 6.241003036499023
Epoch 1260, val loss: 0.9752742052078247
Epoch 1270, training loss: 312.1457214355469 = 0.22523561120033264 + 50.0 * 6.238409519195557
Epoch 1270, val loss: 0.9778518080711365
Epoch 1280, training loss: 312.10223388671875 = 0.22067566215991974 + 50.0 * 6.237631320953369
Epoch 1280, val loss: 0.9799286723136902
Epoch 1290, training loss: 312.1249694824219 = 0.21625110507011414 + 50.0 * 6.2381744384765625
Epoch 1290, val loss: 0.9826183319091797
Epoch 1300, training loss: 312.0477294921875 = 0.21188408136367798 + 50.0 * 6.2367167472839355
Epoch 1300, val loss: 0.985168993473053
Epoch 1310, training loss: 312.01690673828125 = 0.20764736831188202 + 50.0 * 6.236185073852539
Epoch 1310, val loss: 0.9878754615783691
Epoch 1320, training loss: 311.98394775390625 = 0.20345866680145264 + 50.0 * 6.235609531402588
Epoch 1320, val loss: 0.990626335144043
Epoch 1330, training loss: 311.9076843261719 = 0.1993875354528427 + 50.0 * 6.234165668487549
Epoch 1330, val loss: 0.9933918118476868
Epoch 1340, training loss: 312.04766845703125 = 0.195425346493721 + 50.0 * 6.237044811248779
Epoch 1340, val loss: 0.9964916110038757
Epoch 1350, training loss: 311.91558837890625 = 0.19148661196231842 + 50.0 * 6.2344818115234375
Epoch 1350, val loss: 0.9989510774612427
Epoch 1360, training loss: 311.8780822753906 = 0.18768310546875 + 50.0 * 6.2338080406188965
Epoch 1360, val loss: 1.002449870109558
Epoch 1370, training loss: 311.98583984375 = 0.18394413590431213 + 50.0 * 6.2360382080078125
Epoch 1370, val loss: 1.005501389503479
Epoch 1380, training loss: 311.8358459472656 = 0.180258646607399 + 50.0 * 6.233111381530762
Epoch 1380, val loss: 1.008499264717102
Epoch 1390, training loss: 311.79241943359375 = 0.17667147517204285 + 50.0 * 6.2323150634765625
Epoch 1390, val loss: 1.0116416215896606
Epoch 1400, training loss: 311.7473449707031 = 0.17316767573356628 + 50.0 * 6.2314839363098145
Epoch 1400, val loss: 1.0148499011993408
Epoch 1410, training loss: 311.98095703125 = 0.1697506159543991 + 50.0 * 6.236224174499512
Epoch 1410, val loss: 1.017743706703186
Epoch 1420, training loss: 311.7922058105469 = 0.16632340848445892 + 50.0 * 6.232517719268799
Epoch 1420, val loss: 1.0218822956085205
Epoch 1430, training loss: 311.8564147949219 = 0.16300375759601593 + 50.0 * 6.233868598937988
Epoch 1430, val loss: 1.025095820426941
Epoch 1440, training loss: 311.6178894042969 = 0.15973402559757233 + 50.0 * 6.22916316986084
Epoch 1440, val loss: 1.0288124084472656
Epoch 1450, training loss: 311.62445068359375 = 0.1565675437450409 + 50.0 * 6.229357719421387
Epoch 1450, val loss: 1.03266179561615
Epoch 1460, training loss: 311.6787109375 = 0.15348584949970245 + 50.0 * 6.230504512786865
Epoch 1460, val loss: 1.0365387201309204
Epoch 1470, training loss: 311.6466979980469 = 0.15039943158626556 + 50.0 * 6.229926109313965
Epoch 1470, val loss: 1.040342926979065
Epoch 1480, training loss: 311.5892028808594 = 0.1473834365606308 + 50.0 * 6.228836536407471
Epoch 1480, val loss: 1.0441621541976929
Epoch 1490, training loss: 311.58953857421875 = 0.1444481909275055 + 50.0 * 6.2289018630981445
Epoch 1490, val loss: 1.0479955673217773
Epoch 1500, training loss: 311.5341796875 = 0.14160719513893127 + 50.0 * 6.227851390838623
Epoch 1500, val loss: 1.0521347522735596
Epoch 1510, training loss: 311.6722106933594 = 0.1388208419084549 + 50.0 * 6.230667591094971
Epoch 1510, val loss: 1.0559660196304321
Epoch 1520, training loss: 311.5274353027344 = 0.1360480785369873 + 50.0 * 6.227827548980713
Epoch 1520, val loss: 1.0600346326828003
Epoch 1530, training loss: 311.6024169921875 = 0.13337354362010956 + 50.0 * 6.229381084442139
Epoch 1530, val loss: 1.063907504081726
Epoch 1540, training loss: 311.4996032714844 = 0.1307077556848526 + 50.0 * 6.227377891540527
Epoch 1540, val loss: 1.068241834640503
Epoch 1550, training loss: 311.4920349121094 = 0.12813995778560638 + 50.0 * 6.227277755737305
Epoch 1550, val loss: 1.072361707687378
Epoch 1560, training loss: 311.4306640625 = 0.12560640275478363 + 50.0 * 6.226100921630859
Epoch 1560, val loss: 1.07692551612854
Epoch 1570, training loss: 311.4906005859375 = 0.12315455824136734 + 50.0 * 6.227348327636719
Epoch 1570, val loss: 1.0809812545776367
Epoch 1580, training loss: 311.4098815917969 = 0.12072308361530304 + 50.0 * 6.225783348083496
Epoch 1580, val loss: 1.0853828191757202
Epoch 1590, training loss: 311.4268493652344 = 0.11836117506027222 + 50.0 * 6.226170063018799
Epoch 1590, val loss: 1.0895684957504272
Epoch 1600, training loss: 311.55535888671875 = 0.11603618413209915 + 50.0 * 6.228786468505859
Epoch 1600, val loss: 1.0940053462982178
Epoch 1610, training loss: 311.4668884277344 = 0.11373421549797058 + 50.0 * 6.227062702178955
Epoch 1610, val loss: 1.0983299016952515
Epoch 1620, training loss: 311.32415771484375 = 0.11148204654455185 + 50.0 * 6.2242536544799805
Epoch 1620, val loss: 1.1021250486373901
Epoch 1630, training loss: 311.2762756347656 = 0.10932327061891556 + 50.0 * 6.223339080810547
Epoch 1630, val loss: 1.1071279048919678
Epoch 1640, training loss: 311.2598571777344 = 0.10721245408058167 + 50.0 * 6.223052978515625
Epoch 1640, val loss: 1.111291527748108
Epoch 1650, training loss: 311.4783935546875 = 0.10516144335269928 + 50.0 * 6.22746467590332
Epoch 1650, val loss: 1.115798830986023
Epoch 1660, training loss: 311.40802001953125 = 0.10305413603782654 + 50.0 * 6.226099491119385
Epoch 1660, val loss: 1.1199339628219604
Epoch 1670, training loss: 311.3652648925781 = 0.10107120126485825 + 50.0 * 6.225283622741699
Epoch 1670, val loss: 1.1247175931930542
Epoch 1680, training loss: 311.2925720214844 = 0.0990775004029274 + 50.0 * 6.223869323730469
Epoch 1680, val loss: 1.129229187965393
Epoch 1690, training loss: 311.20025634765625 = 0.09718213230371475 + 50.0 * 6.222061634063721
Epoch 1690, val loss: 1.1335511207580566
Epoch 1700, training loss: 311.38055419921875 = 0.09532420337200165 + 50.0 * 6.225704193115234
Epoch 1700, val loss: 1.1378792524337769
Epoch 1710, training loss: 311.2073669433594 = 0.0934709832072258 + 50.0 * 6.222278118133545
Epoch 1710, val loss: 1.1421912908554077
Epoch 1720, training loss: 311.1321716308594 = 0.09167084842920303 + 50.0 * 6.2208099365234375
Epoch 1720, val loss: 1.1468873023986816
Epoch 1730, training loss: 311.1051940917969 = 0.0899292379617691 + 50.0 * 6.2203049659729
Epoch 1730, val loss: 1.1515427827835083
Epoch 1740, training loss: 311.1404113769531 = 0.08823420107364655 + 50.0 * 6.221043586730957
Epoch 1740, val loss: 1.1561758518218994
Epoch 1750, training loss: 311.33154296875 = 0.08656679838895798 + 50.0 * 6.2248992919921875
Epoch 1750, val loss: 1.1609512567520142
Epoch 1760, training loss: 311.1796875 = 0.08487998694181442 + 50.0 * 6.221896171569824
Epoch 1760, val loss: 1.1646349430084229
Epoch 1770, training loss: 311.0545349121094 = 0.08323828876018524 + 50.0 * 6.219425678253174
Epoch 1770, val loss: 1.1694424152374268
Epoch 1780, training loss: 311.05072021484375 = 0.08167537301778793 + 50.0 * 6.219380855560303
Epoch 1780, val loss: 1.1738907098770142
Epoch 1790, training loss: 311.193115234375 = 0.08014428615570068 + 50.0 * 6.222259521484375
Epoch 1790, val loss: 1.178714394569397
Epoch 1800, training loss: 311.02532958984375 = 0.07862469553947449 + 50.0 * 6.218934535980225
Epoch 1800, val loss: 1.1830476522445679
Epoch 1810, training loss: 311.0591125488281 = 0.07714516669511795 + 50.0 * 6.219639301300049
Epoch 1810, val loss: 1.1876884698867798
Epoch 1820, training loss: 311.1378479003906 = 0.07570349425077438 + 50.0 * 6.221242427825928
Epoch 1820, val loss: 1.1922389268875122
Epoch 1830, training loss: 310.99322509765625 = 0.07427377998828888 + 50.0 * 6.218379020690918
Epoch 1830, val loss: 1.1966431140899658
Epoch 1840, training loss: 310.9863586425781 = 0.07289392501115799 + 50.0 * 6.218269348144531
Epoch 1840, val loss: 1.2014211416244507
Epoch 1850, training loss: 311.46014404296875 = 0.07155245542526245 + 50.0 * 6.227771759033203
Epoch 1850, val loss: 1.2051290273666382
Epoch 1860, training loss: 310.9767761230469 = 0.07017256319522858 + 50.0 * 6.218132495880127
Epoch 1860, val loss: 1.2101982831954956
Epoch 1870, training loss: 310.9101867675781 = 0.06886415928602219 + 50.0 * 6.216826438903809
Epoch 1870, val loss: 1.2144683599472046
Epoch 1880, training loss: 310.8986511230469 = 0.06760575622320175 + 50.0 * 6.216620922088623
Epoch 1880, val loss: 1.2193565368652344
Epoch 1890, training loss: 310.8939514160156 = 0.06638187170028687 + 50.0 * 6.216551303863525
Epoch 1890, val loss: 1.2241225242614746
Epoch 1900, training loss: 311.34722900390625 = 0.06518549472093582 + 50.0 * 6.225640773773193
Epoch 1900, val loss: 1.2288686037063599
Epoch 1910, training loss: 311.01849365234375 = 0.06397520005702972 + 50.0 * 6.219090461730957
Epoch 1910, val loss: 1.2324731349945068
Epoch 1920, training loss: 311.0040283203125 = 0.06280957162380219 + 50.0 * 6.21882438659668
Epoch 1920, val loss: 1.2376424074172974
Epoch 1930, training loss: 310.8681945800781 = 0.06166253611445427 + 50.0 * 6.216130256652832
Epoch 1930, val loss: 1.2420144081115723
Epoch 1940, training loss: 310.8370056152344 = 0.06054440885782242 + 50.0 * 6.215528964996338
Epoch 1940, val loss: 1.246132493019104
Epoch 1950, training loss: 311.0619812011719 = 0.05947161838412285 + 50.0 * 6.22005033493042
Epoch 1950, val loss: 1.2508409023284912
Epoch 1960, training loss: 310.91107177734375 = 0.05840301886200905 + 50.0 * 6.217053413391113
Epoch 1960, val loss: 1.2553024291992188
Epoch 1970, training loss: 310.836669921875 = 0.05733923241496086 + 50.0 * 6.2155866622924805
Epoch 1970, val loss: 1.2595932483673096
Epoch 1980, training loss: 310.8031005859375 = 0.0563356950879097 + 50.0 * 6.214935302734375
Epoch 1980, val loss: 1.2643364667892456
Epoch 1990, training loss: 311.1009216308594 = 0.05535617470741272 + 50.0 * 6.220911502838135
Epoch 1990, val loss: 1.268294334411621
Epoch 2000, training loss: 310.7923278808594 = 0.05435289442539215 + 50.0 * 6.214759826660156
Epoch 2000, val loss: 1.273645281791687
Epoch 2010, training loss: 310.7418518066406 = 0.05340137705206871 + 50.0 * 6.21376895904541
Epoch 2010, val loss: 1.2777107954025269
Epoch 2020, training loss: 310.8326110839844 = 0.05248237028717995 + 50.0 * 6.215602397918701
Epoch 2020, val loss: 1.2827531099319458
Epoch 2030, training loss: 310.81488037109375 = 0.05156049132347107 + 50.0 * 6.215266227722168
Epoch 2030, val loss: 1.2869685888290405
Epoch 2040, training loss: 310.8192443847656 = 0.05065753683447838 + 50.0 * 6.215371608734131
Epoch 2040, val loss: 1.2914986610412598
Epoch 2050, training loss: 310.86395263671875 = 0.04978203773498535 + 50.0 * 6.216283321380615
Epoch 2050, val loss: 1.2960796356201172
Epoch 2060, training loss: 310.7593688964844 = 0.048929363489151 + 50.0 * 6.214209079742432
Epoch 2060, val loss: 1.3005644083023071
Epoch 2070, training loss: 310.7721252441406 = 0.048086948692798615 + 50.0 * 6.214480876922607
Epoch 2070, val loss: 1.305509328842163
Epoch 2080, training loss: 310.7088317871094 = 0.047270435839891434 + 50.0 * 6.213231086730957
Epoch 2080, val loss: 1.3095026016235352
Epoch 2090, training loss: 310.7220153808594 = 0.046466123312711716 + 50.0 * 6.213510990142822
Epoch 2090, val loss: 1.3139795064926147
Epoch 2100, training loss: 310.806396484375 = 0.04568325728178024 + 50.0 * 6.215214252471924
Epoch 2100, val loss: 1.3182357549667358
Epoch 2110, training loss: 310.7272033691406 = 0.04491213336586952 + 50.0 * 6.2136454582214355
Epoch 2110, val loss: 1.3233652114868164
Epoch 2120, training loss: 310.696533203125 = 0.044153790920972824 + 50.0 * 6.213047504425049
Epoch 2120, val loss: 1.3274556398391724
Epoch 2130, training loss: 310.84619140625 = 0.04341927543282509 + 50.0 * 6.216055393218994
Epoch 2130, val loss: 1.332294225692749
Epoch 2140, training loss: 310.65667724609375 = 0.04269374534487724 + 50.0 * 6.212279796600342
Epoch 2140, val loss: 1.3355259895324707
Epoch 2150, training loss: 310.6367492675781 = 0.041983675211668015 + 50.0 * 6.211894989013672
Epoch 2150, val loss: 1.3410080671310425
Epoch 2160, training loss: 310.75421142578125 = 0.04129854589700699 + 50.0 * 6.214258670806885
Epoch 2160, val loss: 1.3451240062713623
Epoch 2170, training loss: 310.6861572265625 = 0.04061875119805336 + 50.0 * 6.2129106521606445
Epoch 2170, val loss: 1.3492804765701294
Epoch 2180, training loss: 310.5873107910156 = 0.03994562849402428 + 50.0 * 6.210947513580322
Epoch 2180, val loss: 1.353730320930481
Epoch 2190, training loss: 310.5500793457031 = 0.03929833695292473 + 50.0 * 6.2102155685424805
Epoch 2190, val loss: 1.3582065105438232
Epoch 2200, training loss: 310.61285400390625 = 0.038674090057611465 + 50.0 * 6.211483478546143
Epoch 2200, val loss: 1.3620997667312622
Epoch 2210, training loss: 310.6755676269531 = 0.03804650157690048 + 50.0 * 6.212750434875488
Epoch 2210, val loss: 1.366434097290039
Epoch 2220, training loss: 310.70294189453125 = 0.03742685541510582 + 50.0 * 6.213310241699219
Epoch 2220, val loss: 1.37113618850708
Epoch 2230, training loss: 310.564697265625 = 0.03683291748166084 + 50.0 * 6.210557460784912
Epoch 2230, val loss: 1.3749550580978394
Epoch 2240, training loss: 310.6526184082031 = 0.03624966740608215 + 50.0 * 6.212327003479004
Epoch 2240, val loss: 1.3798425197601318
Epoch 2250, training loss: 310.706298828125 = 0.035667721182107925 + 50.0 * 6.213412761688232
Epoch 2250, val loss: 1.383164882659912
Epoch 2260, training loss: 310.57373046875 = 0.035091858357191086 + 50.0 * 6.21077299118042
Epoch 2260, val loss: 1.3882057666778564
Epoch 2270, training loss: 310.4784851074219 = 0.034533437341451645 + 50.0 * 6.208878993988037
Epoch 2270, val loss: 1.3922195434570312
Epoch 2280, training loss: 310.4599609375 = 0.03399915248155594 + 50.0 * 6.208518981933594
Epoch 2280, val loss: 1.3967397212982178
Epoch 2290, training loss: 310.47137451171875 = 0.03348429873585701 + 50.0 * 6.2087578773498535
Epoch 2290, val loss: 1.4011560678482056
Epoch 2300, training loss: 310.6589660644531 = 0.032974377274513245 + 50.0 * 6.212519645690918
Epoch 2300, val loss: 1.4056267738342285
Epoch 2310, training loss: 310.4888916015625 = 0.03245643526315689 + 50.0 * 6.209128379821777
Epoch 2310, val loss: 1.4090831279754639
Epoch 2320, training loss: 310.4788513183594 = 0.031959060579538345 + 50.0 * 6.208938121795654
Epoch 2320, val loss: 1.4139888286590576
Epoch 2330, training loss: 310.43829345703125 = 0.03147371858358383 + 50.0 * 6.208136558532715
Epoch 2330, val loss: 1.4180233478546143
Epoch 2340, training loss: 310.6139831542969 = 0.03100418858230114 + 50.0 * 6.2116594314575195
Epoch 2340, val loss: 1.422456979751587
Epoch 2350, training loss: 310.42205810546875 = 0.03051990084350109 + 50.0 * 6.207830905914307
Epoch 2350, val loss: 1.4261070489883423
Epoch 2360, training loss: 310.4058532714844 = 0.03006354719400406 + 50.0 * 6.207515716552734
Epoch 2360, val loss: 1.430107593536377
Epoch 2370, training loss: 310.6523742675781 = 0.029627928510308266 + 50.0 * 6.212454795837402
Epoch 2370, val loss: 1.434736967086792
Epoch 2380, training loss: 310.4407653808594 = 0.02916480228304863 + 50.0 * 6.2082319259643555
Epoch 2380, val loss: 1.4380089044570923
Epoch 2390, training loss: 310.4197082519531 = 0.028721831738948822 + 50.0 * 6.207819938659668
Epoch 2390, val loss: 1.442718744277954
Epoch 2400, training loss: 310.3671569824219 = 0.02829941362142563 + 50.0 * 6.206777095794678
Epoch 2400, val loss: 1.4466458559036255
Epoch 2410, training loss: 310.33428955078125 = 0.027888529002666473 + 50.0 * 6.206127643585205
Epoch 2410, val loss: 1.4512487649917603
Epoch 2420, training loss: 310.536865234375 = 0.027490878477692604 + 50.0 * 6.2101874351501465
Epoch 2420, val loss: 1.4555572271347046
Epoch 2430, training loss: 310.4051818847656 = 0.02708057314157486 + 50.0 * 6.207562446594238
Epoch 2430, val loss: 1.459071397781372
Epoch 2440, training loss: 310.351318359375 = 0.02668178081512451 + 50.0 * 6.206492900848389
Epoch 2440, val loss: 1.462735652923584
Epoch 2450, training loss: 310.3241271972656 = 0.026298802345991135 + 50.0 * 6.20595645904541
Epoch 2450, val loss: 1.4669240713119507
Epoch 2460, training loss: 310.40142822265625 = 0.025927264243364334 + 50.0 * 6.207509994506836
Epoch 2460, val loss: 1.4713994264602661
Epoch 2470, training loss: 310.4013366699219 = 0.025553742423653603 + 50.0 * 6.207515716552734
Epoch 2470, val loss: 1.4751135110855103
Epoch 2480, training loss: 310.3088684082031 = 0.025179877877235413 + 50.0 * 6.205673694610596
Epoch 2480, val loss: 1.4787962436676025
Epoch 2490, training loss: 310.2775573730469 = 0.0248293224722147 + 50.0 * 6.20505428314209
Epoch 2490, val loss: 1.4829391241073608
Epoch 2500, training loss: 310.3415832519531 = 0.024488935247063637 + 50.0 * 6.2063422203063965
Epoch 2500, val loss: 1.486951470375061
Epoch 2510, training loss: 310.4434814453125 = 0.0241483673453331 + 50.0 * 6.2083868980407715
Epoch 2510, val loss: 1.4907156229019165
Epoch 2520, training loss: 310.3266906738281 = 0.023800019174814224 + 50.0 * 6.206057548522949
Epoch 2520, val loss: 1.4946215152740479
Epoch 2530, training loss: 310.32135009765625 = 0.023480506613850594 + 50.0 * 6.205957889556885
Epoch 2530, val loss: 1.4991439580917358
Epoch 2540, training loss: 310.42132568359375 = 0.023160116747021675 + 50.0 * 6.207962989807129
Epoch 2540, val loss: 1.502631664276123
Epoch 2550, training loss: 310.33917236328125 = 0.022832533344626427 + 50.0 * 6.206326961517334
Epoch 2550, val loss: 1.5057862997055054
Epoch 2560, training loss: 310.4059753417969 = 0.022523872554302216 + 50.0 * 6.207668781280518
Epoch 2560, val loss: 1.5104408264160156
Epoch 2570, training loss: 310.2962341308594 = 0.02220930904150009 + 50.0 * 6.205480575561523
Epoch 2570, val loss: 1.5134073495864868
Epoch 2580, training loss: 310.2326965332031 = 0.02191598154604435 + 50.0 * 6.2042155265808105
Epoch 2580, val loss: 1.5175766944885254
Epoch 2590, training loss: 310.2913818359375 = 0.02162333019077778 + 50.0 * 6.205394744873047
Epoch 2590, val loss: 1.5211527347564697
Epoch 2600, training loss: 310.1907653808594 = 0.021332254633307457 + 50.0 * 6.203388690948486
Epoch 2600, val loss: 1.5252829790115356
Epoch 2610, training loss: 310.2768249511719 = 0.02105456404387951 + 50.0 * 6.20511531829834
Epoch 2610, val loss: 1.5295299291610718
Epoch 2620, training loss: 310.4039306640625 = 0.02077445574104786 + 50.0 * 6.207663059234619
Epoch 2620, val loss: 1.533051609992981
Epoch 2630, training loss: 310.2432556152344 = 0.020492350682616234 + 50.0 * 6.204455375671387
Epoch 2630, val loss: 1.5363761186599731
Epoch 2640, training loss: 310.1900634765625 = 0.020224783569574356 + 50.0 * 6.203396797180176
Epoch 2640, val loss: 1.5402147769927979
Epoch 2650, training loss: 310.24139404296875 = 0.019966857507824898 + 50.0 * 6.204428672790527
Epoch 2650, val loss: 1.5439832210540771
Epoch 2660, training loss: 310.26416015625 = 0.01970674842596054 + 50.0 * 6.204888820648193
Epoch 2660, val loss: 1.5477374792099
Epoch 2670, training loss: 310.20526123046875 = 0.019451742991805077 + 50.0 * 6.203716278076172
Epoch 2670, val loss: 1.5509605407714844
Epoch 2680, training loss: 310.1141662597656 = 0.019202405586838722 + 50.0 * 6.201899528503418
Epoch 2680, val loss: 1.5547449588775635
Epoch 2690, training loss: 310.263916015625 = 0.018966039642691612 + 50.0 * 6.204899311065674
Epoch 2690, val loss: 1.5587279796600342
Epoch 2700, training loss: 310.19989013671875 = 0.018719837069511414 + 50.0 * 6.2036237716674805
Epoch 2700, val loss: 1.5621684789657593
Epoch 2710, training loss: 310.2882385253906 = 0.018474649637937546 + 50.0 * 6.205395221710205
Epoch 2710, val loss: 1.5654726028442383
Epoch 2720, training loss: 310.1679992675781 = 0.01824122481048107 + 50.0 * 6.2029948234558105
Epoch 2720, val loss: 1.5688058137893677
Epoch 2730, training loss: 310.1212158203125 = 0.018011676147580147 + 50.0 * 6.202064037322998
Epoch 2730, val loss: 1.5721083879470825
Epoch 2740, training loss: 310.1098327636719 = 0.01779000647366047 + 50.0 * 6.201840877532959
Epoch 2740, val loss: 1.5761123895645142
Epoch 2750, training loss: 310.1351623535156 = 0.01757311448454857 + 50.0 * 6.2023515701293945
Epoch 2750, val loss: 1.5790525674819946
Epoch 2760, training loss: 310.2548828125 = 0.017362048849463463 + 50.0 * 6.2047505378723145
Epoch 2760, val loss: 1.582704782485962
Epoch 2770, training loss: 310.1735534667969 = 0.017146389931440353 + 50.0 * 6.203128337860107
Epoch 2770, val loss: 1.586266279220581
Epoch 2780, training loss: 310.26959228515625 = 0.016942864283919334 + 50.0 * 6.205053329467773
Epoch 2780, val loss: 1.5897053480148315
Epoch 2790, training loss: 310.22381591796875 = 0.016728147864341736 + 50.0 * 6.204141616821289
Epoch 2790, val loss: 1.5932365655899048
Epoch 2800, training loss: 310.10614013671875 = 0.016519401222467422 + 50.0 * 6.2017927169799805
Epoch 2800, val loss: 1.5963428020477295
Epoch 2810, training loss: 310.0427551269531 = 0.016321346163749695 + 50.0 * 6.200528621673584
Epoch 2810, val loss: 1.5997737646102905
Epoch 2820, training loss: 310.0219421386719 = 0.016132241114974022 + 50.0 * 6.20011568069458
Epoch 2820, val loss: 1.603209376335144
Epoch 2830, training loss: 310.0531311035156 = 0.015948334708809853 + 50.0 * 6.200743198394775
Epoch 2830, val loss: 1.6067767143249512
Epoch 2840, training loss: 310.54486083984375 = 0.015764540061354637 + 50.0 * 6.2105817794799805
Epoch 2840, val loss: 1.6105800867080688
Epoch 2850, training loss: 310.1968994140625 = 0.015568546019494534 + 50.0 * 6.20362663269043
Epoch 2850, val loss: 1.6124258041381836
Epoch 2860, training loss: 310.0514221191406 = 0.015384030528366566 + 50.0 * 6.20072078704834
Epoch 2860, val loss: 1.6162487268447876
Epoch 2870, training loss: 310.00604248046875 = 0.015208963304758072 + 50.0 * 6.199817180633545
Epoch 2870, val loss: 1.6195564270019531
Epoch 2880, training loss: 310.0994567871094 = 0.015040609054267406 + 50.0 * 6.201688289642334
Epoch 2880, val loss: 1.623265266418457
Epoch 2890, training loss: 310.09295654296875 = 0.01486445963382721 + 50.0 * 6.20156192779541
Epoch 2890, val loss: 1.6255611181259155
Epoch 2900, training loss: 310.0565490722656 = 0.014691256918013096 + 50.0 * 6.200837135314941
Epoch 2900, val loss: 1.6294069290161133
Epoch 2910, training loss: 310.010986328125 = 0.014525293372571468 + 50.0 * 6.199929237365723
Epoch 2910, val loss: 1.6324567794799805
Epoch 2920, training loss: 310.0272521972656 = 0.014364881440997124 + 50.0 * 6.200257301330566
Epoch 2920, val loss: 1.636258840560913
Epoch 2930, training loss: 310.21630859375 = 0.014208003878593445 + 50.0 * 6.204041481018066
Epoch 2930, val loss: 1.6398251056671143
Epoch 2940, training loss: 310.07379150390625 = 0.014041871763765812 + 50.0 * 6.201194763183594
Epoch 2940, val loss: 1.6408401727676392
Epoch 2950, training loss: 309.97540283203125 = 0.013882177881896496 + 50.0 * 6.199230194091797
Epoch 2950, val loss: 1.6451631784439087
Epoch 2960, training loss: 309.9339294433594 = 0.013730110600590706 + 50.0 * 6.198403835296631
Epoch 2960, val loss: 1.6482429504394531
Epoch 2970, training loss: 310.05950927734375 = 0.013583113439381123 + 50.0 * 6.200918674468994
Epoch 2970, val loss: 1.651493787765503
Epoch 2980, training loss: 309.98480224609375 = 0.013434109278023243 + 50.0 * 6.199427604675293
Epoch 2980, val loss: 1.6546992063522339
Epoch 2990, training loss: 309.97235107421875 = 0.01329193264245987 + 50.0 * 6.199180603027344
Epoch 2990, val loss: 1.6570974588394165
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8355297838692674
The final CL Acc:0.74074, 0.00000, The final GNN Acc:0.83781, 0.00163
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0 dfr1:0.4 dfr2:0.5
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.5, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11612])
remove edge: torch.Size([2, 9516])
updated graph: torch.Size([2, 10572])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7878723144531 = 1.9435793161392212 + 50.0 * 8.596885681152344
Epoch 0, val loss: 1.9421707391738892
Epoch 10, training loss: 431.75543212890625 = 1.9350244998931885 + 50.0 * 8.596407890319824
Epoch 10, val loss: 1.9336000680923462
Epoch 20, training loss: 431.5654602050781 = 1.9242331981658936 + 50.0 * 8.592824935913086
Epoch 20, val loss: 1.922353982925415
Epoch 30, training loss: 430.2272644042969 = 1.909865140914917 + 50.0 * 8.5663480758667
Epoch 30, val loss: 1.9073700904846191
Epoch 40, training loss: 422.6797180175781 = 1.8914737701416016 + 50.0 * 8.415764808654785
Epoch 40, val loss: 1.889150857925415
Epoch 50, training loss: 396.02325439453125 = 1.8714162111282349 + 50.0 * 7.8830366134643555
Epoch 50, val loss: 1.8700530529022217
Epoch 60, training loss: 375.5426330566406 = 1.8572196960449219 + 50.0 * 7.473708629608154
Epoch 60, val loss: 1.8572816848754883
Epoch 70, training loss: 361.8099670410156 = 1.8472211360931396 + 50.0 * 7.199254512786865
Epoch 70, val loss: 1.8478673696517944
Epoch 80, training loss: 354.738525390625 = 1.8370023965835571 + 50.0 * 7.058030128479004
Epoch 80, val loss: 1.8384265899658203
Epoch 90, training loss: 347.7053527832031 = 1.8272511959075928 + 50.0 * 6.917562484741211
Epoch 90, val loss: 1.8300360441207886
Epoch 100, training loss: 342.5968933105469 = 1.8187055587768555 + 50.0 * 6.815563678741455
Epoch 100, val loss: 1.822585105895996
Epoch 110, training loss: 338.8842468261719 = 1.810585618019104 + 50.0 * 6.741473197937012
Epoch 110, val loss: 1.815483570098877
Epoch 120, training loss: 336.31207275390625 = 1.8025641441345215 + 50.0 * 6.690190315246582
Epoch 120, val loss: 1.8084605932235718
Epoch 130, training loss: 334.3410339355469 = 1.7942928075790405 + 50.0 * 6.650935173034668
Epoch 130, val loss: 1.8013197183609009
Epoch 140, training loss: 332.77349853515625 = 1.7858301401138306 + 50.0 * 6.619753360748291
Epoch 140, val loss: 1.7939770221710205
Epoch 150, training loss: 331.3074645996094 = 1.7769885063171387 + 50.0 * 6.590609550476074
Epoch 150, val loss: 1.7863298654556274
Epoch 160, training loss: 329.9878234863281 = 1.7676931619644165 + 50.0 * 6.5644025802612305
Epoch 160, val loss: 1.7783194780349731
Epoch 170, training loss: 328.9474182128906 = 1.7576125860214233 + 50.0 * 6.543796539306641
Epoch 170, val loss: 1.7697325944900513
Epoch 180, training loss: 327.9022521972656 = 1.7466096878051758 + 50.0 * 6.523112773895264
Epoch 180, val loss: 1.7604598999023438
Epoch 190, training loss: 327.0694885253906 = 1.7345905303955078 + 50.0 * 6.506698131561279
Epoch 190, val loss: 1.7503554821014404
Epoch 200, training loss: 326.2810974121094 = 1.721441388130188 + 50.0 * 6.4911932945251465
Epoch 200, val loss: 1.7394243478775024
Epoch 210, training loss: 325.8684387207031 = 1.7070105075836182 + 50.0 * 6.48322868347168
Epoch 210, val loss: 1.727482557296753
Epoch 220, training loss: 325.1433410644531 = 1.6913962364196777 + 50.0 * 6.469038963317871
Epoch 220, val loss: 1.7144367694854736
Epoch 230, training loss: 324.5075378417969 = 1.674444317817688 + 50.0 * 6.456661701202393
Epoch 230, val loss: 1.7003576755523682
Epoch 240, training loss: 323.9945373535156 = 1.6563664674758911 + 50.0 * 6.446763515472412
Epoch 240, val loss: 1.6853313446044922
Epoch 250, training loss: 323.5369567871094 = 1.6370590925216675 + 50.0 * 6.437998294830322
Epoch 250, val loss: 1.6693257093429565
Epoch 260, training loss: 323.420654296875 = 1.6164790391921997 + 50.0 * 6.4360833168029785
Epoch 260, val loss: 1.6523317098617554
Epoch 270, training loss: 322.7588195800781 = 1.5947610139846802 + 50.0 * 6.423281192779541
Epoch 270, val loss: 1.634542465209961
Epoch 280, training loss: 322.414306640625 = 1.5721466541290283 + 50.0 * 6.416843414306641
Epoch 280, val loss: 1.616129994392395
Epoch 290, training loss: 322.13421630859375 = 1.5488120317459106 + 50.0 * 6.411708354949951
Epoch 290, val loss: 1.5973442792892456
Epoch 300, training loss: 322.0164489746094 = 1.524749755859375 + 50.0 * 6.409833908081055
Epoch 300, val loss: 1.5781018733978271
Epoch 310, training loss: 321.5345458984375 = 1.5004589557647705 + 50.0 * 6.400681972503662
Epoch 310, val loss: 1.5588972568511963
Epoch 320, training loss: 321.186279296875 = 1.4760241508483887 + 50.0 * 6.394205093383789
Epoch 320, val loss: 1.5398621559143066
Epoch 330, training loss: 320.9369201660156 = 1.451550841331482 + 50.0 * 6.389707565307617
Epoch 330, val loss: 1.5211310386657715
Epoch 340, training loss: 320.7190246582031 = 1.4271284341812134 + 50.0 * 6.385838031768799
Epoch 340, val loss: 1.5026099681854248
Epoch 350, training loss: 320.4020690917969 = 1.4028942584991455 + 50.0 * 6.379983425140381
Epoch 350, val loss: 1.4845629930496216
Epoch 360, training loss: 320.12744140625 = 1.3788940906524658 + 50.0 * 6.37497091293335
Epoch 360, val loss: 1.4668827056884766
Epoch 370, training loss: 320.2592468261719 = 1.3551844358444214 + 50.0 * 6.37808084487915
Epoch 370, val loss: 1.4496817588806152
Epoch 380, training loss: 319.7488708496094 = 1.3313663005828857 + 50.0 * 6.368350505828857
Epoch 380, val loss: 1.4324491024017334
Epoch 390, training loss: 319.5127258300781 = 1.3079365491867065 + 50.0 * 6.364096164703369
Epoch 390, val loss: 1.4156887531280518
Epoch 400, training loss: 319.26361083984375 = 1.2846250534057617 + 50.0 * 6.359579563140869
Epoch 400, val loss: 1.3992096185684204
Epoch 410, training loss: 319.0462341308594 = 1.2614113092422485 + 50.0 * 6.355696678161621
Epoch 410, val loss: 1.382980465888977
Epoch 420, training loss: 319.3309020996094 = 1.238189458847046 + 50.0 * 6.361854553222656
Epoch 420, val loss: 1.3668370246887207
Epoch 430, training loss: 318.8946228027344 = 1.2148759365081787 + 50.0 * 6.353594779968262
Epoch 430, val loss: 1.3507235050201416
Epoch 440, training loss: 318.5676574707031 = 1.1916496753692627 + 50.0 * 6.347520351409912
Epoch 440, val loss: 1.3348784446716309
Epoch 450, training loss: 318.3542785644531 = 1.168351650238037 + 50.0 * 6.3437180519104
Epoch 450, val loss: 1.319016695022583
Epoch 460, training loss: 318.3539733886719 = 1.145041584968567 + 50.0 * 6.344178199768066
Epoch 460, val loss: 1.3032618761062622
Epoch 470, training loss: 318.0627136230469 = 1.1217902898788452 + 50.0 * 6.338818073272705
Epoch 470, val loss: 1.2877752780914307
Epoch 480, training loss: 317.9095458984375 = 1.0984387397766113 + 50.0 * 6.336221694946289
Epoch 480, val loss: 1.2720867395401
Epoch 490, training loss: 317.71868896484375 = 1.075240969657898 + 50.0 * 6.332869052886963
Epoch 490, val loss: 1.2568424940109253
Epoch 500, training loss: 317.6888732910156 = 1.0521314144134521 + 50.0 * 6.332735061645508
Epoch 500, val loss: 1.2416142225265503
Epoch 510, training loss: 317.5010986328125 = 1.0292127132415771 + 50.0 * 6.329437732696533
Epoch 510, val loss: 1.2265653610229492
Epoch 520, training loss: 317.3855285644531 = 1.0064605474472046 + 50.0 * 6.32758092880249
Epoch 520, val loss: 1.211948037147522
Epoch 530, training loss: 317.2435302734375 = 0.9841404557228088 + 50.0 * 6.325188159942627
Epoch 530, val loss: 1.197756052017212
Epoch 540, training loss: 317.1351318359375 = 0.9620277881622314 + 50.0 * 6.323462009429932
Epoch 540, val loss: 1.1838730573654175
Epoch 550, training loss: 317.15130615234375 = 0.9402086138725281 + 50.0 * 6.324222087860107
Epoch 550, val loss: 1.1703095436096191
Epoch 560, training loss: 316.8924865722656 = 0.9189406633377075 + 50.0 * 6.3194708824157715
Epoch 560, val loss: 1.1574803590774536
Epoch 570, training loss: 316.73162841796875 = 0.8979756832122803 + 50.0 * 6.3166728019714355
Epoch 570, val loss: 1.1448986530303955
Epoch 580, training loss: 316.6351013183594 = 0.8775250315666199 + 50.0 * 6.315151214599609
Epoch 580, val loss: 1.1330512762069702
Epoch 590, training loss: 316.7850646972656 = 0.8574453592300415 + 50.0 * 6.318552494049072
Epoch 590, val loss: 1.121630072593689
Epoch 600, training loss: 316.4784240722656 = 0.8378501534461975 + 50.0 * 6.312811374664307
Epoch 600, val loss: 1.1107693910598755
Epoch 610, training loss: 316.3121032714844 = 0.8186830282211304 + 50.0 * 6.309867858886719
Epoch 610, val loss: 1.1002209186553955
Epoch 620, training loss: 316.1981201171875 = 0.8000174164772034 + 50.0 * 6.307961940765381
Epoch 620, val loss: 1.0904031991958618
Epoch 630, training loss: 316.40728759765625 = 0.7817981243133545 + 50.0 * 6.312510013580322
Epoch 630, val loss: 1.0808113813400269
Epoch 640, training loss: 316.3086242675781 = 0.7639656066894531 + 50.0 * 6.3108930587768555
Epoch 640, val loss: 1.0722113847732544
Epoch 650, training loss: 315.9306640625 = 0.7463685274124146 + 50.0 * 6.303686141967773
Epoch 650, val loss: 1.0633604526519775
Epoch 660, training loss: 315.8674621582031 = 0.7292767763137817 + 50.0 * 6.302763938903809
Epoch 660, val loss: 1.054966926574707
Epoch 670, training loss: 315.9091491699219 = 0.7126587629318237 + 50.0 * 6.303930282592773
Epoch 670, val loss: 1.0472242832183838
Epoch 680, training loss: 315.74273681640625 = 0.69632488489151 + 50.0 * 6.300928115844727
Epoch 680, val loss: 1.0398766994476318
Epoch 690, training loss: 315.6580505371094 = 0.6803814768791199 + 50.0 * 6.299553394317627
Epoch 690, val loss: 1.0330439805984497
Epoch 700, training loss: 315.8197937011719 = 0.6647678017616272 + 50.0 * 6.3031005859375
Epoch 700, val loss: 1.026272177696228
Epoch 710, training loss: 315.4860534667969 = 0.6493128538131714 + 50.0 * 6.296734809875488
Epoch 710, val loss: 1.019591212272644
Epoch 720, training loss: 315.3619079589844 = 0.6344166398048401 + 50.0 * 6.294549465179443
Epoch 720, val loss: 1.0137062072753906
Epoch 730, training loss: 315.2965087890625 = 0.6197822093963623 + 50.0 * 6.293534278869629
Epoch 730, val loss: 1.0079376697540283
Epoch 740, training loss: 315.34814453125 = 0.6054084897041321 + 50.0 * 6.294854640960693
Epoch 740, val loss: 1.0024983882904053
Epoch 750, training loss: 315.1932678222656 = 0.5912081003189087 + 50.0 * 6.292041301727295
Epoch 750, val loss: 0.9971357583999634
Epoch 760, training loss: 315.1412048339844 = 0.577395498752594 + 50.0 * 6.291276454925537
Epoch 760, val loss: 0.9923105835914612
Epoch 770, training loss: 315.0361633300781 = 0.5639104247093201 + 50.0 * 6.289445400238037
Epoch 770, val loss: 0.9877592921257019
Epoch 780, training loss: 314.9364318847656 = 0.550720751285553 + 50.0 * 6.28771448135376
Epoch 780, val loss: 0.9834589958190918
Epoch 790, training loss: 314.90142822265625 = 0.5378237962722778 + 50.0 * 6.2872724533081055
Epoch 790, val loss: 0.9794226884841919
Epoch 800, training loss: 315.25433349609375 = 0.5251620411872864 + 50.0 * 6.294582843780518
Epoch 800, val loss: 0.9755164384841919
Epoch 810, training loss: 314.8711853027344 = 0.5128284692764282 + 50.0 * 6.287167072296143
Epoch 810, val loss: 0.9720264673233032
Epoch 820, training loss: 314.6517333984375 = 0.5007949471473694 + 50.0 * 6.283019065856934
Epoch 820, val loss: 0.9689143300056458
Epoch 830, training loss: 314.5786437988281 = 0.4891056716442108 + 50.0 * 6.281790733337402
Epoch 830, val loss: 0.9661117196083069
Epoch 840, training loss: 314.6307678222656 = 0.47776082158088684 + 50.0 * 6.283060073852539
Epoch 840, val loss: 0.9637467265129089
Epoch 850, training loss: 314.5382995605469 = 0.46659478545188904 + 50.0 * 6.281434535980225
Epoch 850, val loss: 0.9613024592399597
Epoch 860, training loss: 314.4549560546875 = 0.45569539070129395 + 50.0 * 6.279985427856445
Epoch 860, val loss: 0.9594097137451172
Epoch 870, training loss: 314.50274658203125 = 0.4451088011264801 + 50.0 * 6.281153202056885
Epoch 870, val loss: 0.9578114748001099
Epoch 880, training loss: 314.3007507324219 = 0.43480184674263 + 50.0 * 6.277318954467773
Epoch 880, val loss: 0.9562669396400452
Epoch 890, training loss: 314.3533935546875 = 0.4247686564922333 + 50.0 * 6.2785725593566895
Epoch 890, val loss: 0.9552558064460754
Epoch 900, training loss: 314.27984619140625 = 0.4149680733680725 + 50.0 * 6.277297496795654
Epoch 900, val loss: 0.9545769691467285
Epoch 910, training loss: 314.29705810546875 = 0.40541818737983704 + 50.0 * 6.277832508087158
Epoch 910, val loss: 0.9538342356681824
Epoch 920, training loss: 314.1681213378906 = 0.3961372971534729 + 50.0 * 6.275439739227295
Epoch 920, val loss: 0.9540025591850281
Epoch 930, training loss: 314.1103515625 = 0.3870750963687897 + 50.0 * 6.274465084075928
Epoch 930, val loss: 0.953917920589447
Epoch 940, training loss: 313.987548828125 = 0.3782750368118286 + 50.0 * 6.272185802459717
Epoch 940, val loss: 0.9545639753341675
Epoch 950, training loss: 314.0374755859375 = 0.3696780800819397 + 50.0 * 6.273355960845947
Epoch 950, val loss: 0.9551910161972046
Epoch 960, training loss: 313.9646301269531 = 0.3612637519836426 + 50.0 * 6.272067546844482
Epoch 960, val loss: 0.9561696648597717
Epoch 970, training loss: 313.92041015625 = 0.35305482149124146 + 50.0 * 6.2713470458984375
Epoch 970, val loss: 0.9573221802711487
Epoch 980, training loss: 313.8740234375 = 0.345104843378067 + 50.0 * 6.270578384399414
Epoch 980, val loss: 0.9589681029319763
Epoch 990, training loss: 313.7567443847656 = 0.33730295300483704 + 50.0 * 6.268388748168945
Epoch 990, val loss: 0.9606231451034546
Epoch 1000, training loss: 313.8451232910156 = 0.32971665263175964 + 50.0 * 6.270308494567871
Epoch 1000, val loss: 0.962647020816803
Epoch 1010, training loss: 313.6860656738281 = 0.32227686047554016 + 50.0 * 6.267276287078857
Epoch 1010, val loss: 0.9648760557174683
Epoch 1020, training loss: 313.6316223144531 = 0.31502407789230347 + 50.0 * 6.266331672668457
Epoch 1020, val loss: 0.9672443270683289
Epoch 1030, training loss: 313.6247863769531 = 0.30791667103767395 + 50.0 * 6.2663373947143555
Epoch 1030, val loss: 0.9697278141975403
Epoch 1040, training loss: 313.5619201660156 = 0.3009880483150482 + 50.0 * 6.265218734741211
Epoch 1040, val loss: 0.9727686047554016
Epoch 1050, training loss: 313.6434326171875 = 0.29419559240341187 + 50.0 * 6.266984939575195
Epoch 1050, val loss: 0.9759697914123535
Epoch 1060, training loss: 313.4672546386719 = 0.28751567006111145 + 50.0 * 6.263595104217529
Epoch 1060, val loss: 0.9789721369743347
Epoch 1070, training loss: 313.4528503417969 = 0.2810169458389282 + 50.0 * 6.263436794281006
Epoch 1070, val loss: 0.9826491475105286
Epoch 1080, training loss: 313.6165771484375 = 0.27465951442718506 + 50.0 * 6.266838550567627
Epoch 1080, val loss: 0.9860360622406006
Epoch 1090, training loss: 313.4034729003906 = 0.2684095501899719 + 50.0 * 6.262701511383057
Epoch 1090, val loss: 0.989604115486145
Epoch 1100, training loss: 313.33447265625 = 0.2623429596424103 + 50.0 * 6.261443138122559
Epoch 1100, val loss: 0.99373859167099
Epoch 1110, training loss: 313.3460998535156 = 0.2563946545124054 + 50.0 * 6.261793613433838
Epoch 1110, val loss: 0.9973882436752319
Epoch 1120, training loss: 313.50421142578125 = 0.2505922019481659 + 50.0 * 6.265072345733643
Epoch 1120, val loss: 1.0018889904022217
Epoch 1130, training loss: 313.237548828125 = 0.24486246705055237 + 50.0 * 6.259853363037109
Epoch 1130, val loss: 1.0058211088180542
Epoch 1140, training loss: 313.1360778808594 = 0.23930931091308594 + 50.0 * 6.257935047149658
Epoch 1140, val loss: 1.01043701171875
Epoch 1150, training loss: 313.0671691894531 = 0.23389999568462372 + 50.0 * 6.2566657066345215
Epoch 1150, val loss: 1.0149154663085938
Epoch 1160, training loss: 313.092529296875 = 0.22861696779727936 + 50.0 * 6.2572784423828125
Epoch 1160, val loss: 1.0196473598480225
Epoch 1170, training loss: 313.3360900878906 = 0.22338280081748962 + 50.0 * 6.262253761291504
Epoch 1170, val loss: 1.0242668390274048
Epoch 1180, training loss: 313.0022277832031 = 0.21825246512889862 + 50.0 * 6.255679607391357
Epoch 1180, val loss: 1.0289353132247925
Epoch 1190, training loss: 312.95147705078125 = 0.21326866745948792 + 50.0 * 6.254764556884766
Epoch 1190, val loss: 1.03396737575531
Epoch 1200, training loss: 312.9112854003906 = 0.20839665830135345 + 50.0 * 6.25405740737915
Epoch 1200, val loss: 1.0389416217803955
Epoch 1210, training loss: 313.0567932128906 = 0.20367135107517242 + 50.0 * 6.2570624351501465
Epoch 1210, val loss: 1.0441102981567383
Epoch 1220, training loss: 312.9479064941406 = 0.19894497096538544 + 50.0 * 6.254979133605957
Epoch 1220, val loss: 1.0492476224899292
Epoch 1230, training loss: 312.9493103027344 = 0.1943298578262329 + 50.0 * 6.255099296569824
Epoch 1230, val loss: 1.0541108846664429
Epoch 1240, training loss: 312.7840881347656 = 0.18986178934574127 + 50.0 * 6.251884460449219
Epoch 1240, val loss: 1.0594934225082397
Epoch 1250, training loss: 312.7265319824219 = 0.18552491068840027 + 50.0 * 6.250820159912109
Epoch 1250, val loss: 1.064996361732483
Epoch 1260, training loss: 313.1288757324219 = 0.18132227659225464 + 50.0 * 6.258950710296631
Epoch 1260, val loss: 1.070497751235962
Epoch 1270, training loss: 312.8097229003906 = 0.17702683806419373 + 50.0 * 6.252654075622559
Epoch 1270, val loss: 1.075483798980713
Epoch 1280, training loss: 312.7054748535156 = 0.1729539930820465 + 50.0 * 6.250650405883789
Epoch 1280, val loss: 1.081323266029358
Epoch 1290, training loss: 312.63531494140625 = 0.16895794868469238 + 50.0 * 6.249327182769775
Epoch 1290, val loss: 1.0867642164230347
Epoch 1300, training loss: 312.6558837890625 = 0.1651032716035843 + 50.0 * 6.249815464019775
Epoch 1300, val loss: 1.0926448106765747
Epoch 1310, training loss: 312.70843505859375 = 0.16128294169902802 + 50.0 * 6.250943183898926
Epoch 1310, val loss: 1.0980653762817383
Epoch 1320, training loss: 312.6621398925781 = 0.15753567218780518 + 50.0 * 6.250092029571533
Epoch 1320, val loss: 1.1033275127410889
Epoch 1330, training loss: 313.0191650390625 = 0.15389315783977509 + 50.0 * 6.257305145263672
Epoch 1330, val loss: 1.109383463859558
Epoch 1340, training loss: 312.5309143066406 = 0.15024518966674805 + 50.0 * 6.247613430023193
Epoch 1340, val loss: 1.114731788635254
Epoch 1350, training loss: 312.4970703125 = 0.14675673842430115 + 50.0 * 6.247005939483643
Epoch 1350, val loss: 1.120213270187378
Epoch 1360, training loss: 312.4046325683594 = 0.143381729722023 + 50.0 * 6.245225429534912
Epoch 1360, val loss: 1.1259783506393433
Epoch 1370, training loss: 312.382568359375 = 0.1400870829820633 + 50.0 * 6.244850158691406
Epoch 1370, val loss: 1.1317776441574097
Epoch 1380, training loss: 312.70062255859375 = 0.13686370849609375 + 50.0 * 6.251275062561035
Epoch 1380, val loss: 1.1373064517974854
Epoch 1390, training loss: 312.7933044433594 = 0.13364756107330322 + 50.0 * 6.253193378448486
Epoch 1390, val loss: 1.1427063941955566
Epoch 1400, training loss: 312.42999267578125 = 0.13049975037574768 + 50.0 * 6.245989799499512
Epoch 1400, val loss: 1.148238182067871
Epoch 1410, training loss: 312.2879943847656 = 0.12747123837471008 + 50.0 * 6.243210315704346
Epoch 1410, val loss: 1.1538950204849243
Epoch 1420, training loss: 312.2734375 = 0.12454032152891159 + 50.0 * 6.242977619171143
Epoch 1420, val loss: 1.1596291065216064
Epoch 1430, training loss: 312.2428283691406 = 0.12167492508888245 + 50.0 * 6.242423057556152
Epoch 1430, val loss: 1.165303349494934
Epoch 1440, training loss: 312.5639343261719 = 0.11888083070516586 + 50.0 * 6.2489013671875
Epoch 1440, val loss: 1.1710079908370972
Epoch 1450, training loss: 312.34136962890625 = 0.11608640849590302 + 50.0 * 6.244505405426025
Epoch 1450, val loss: 1.1763668060302734
Epoch 1460, training loss: 312.3908996582031 = 0.11338026821613312 + 50.0 * 6.245550632476807
Epoch 1460, val loss: 1.1819502115249634
Epoch 1470, training loss: 312.2179870605469 = 0.11071932315826416 + 50.0 * 6.242145538330078
Epoch 1470, val loss: 1.1875214576721191
Epoch 1480, training loss: 312.17742919921875 = 0.10813993215560913 + 50.0 * 6.241385459899902
Epoch 1480, val loss: 1.1929222345352173
Epoch 1490, training loss: 312.096435546875 = 0.1056504100561142 + 50.0 * 6.239815711975098
Epoch 1490, val loss: 1.198769211769104
Epoch 1500, training loss: 312.1659240722656 = 0.10323170572519302 + 50.0 * 6.24125337600708
Epoch 1500, val loss: 1.2042967081069946
Epoch 1510, training loss: 312.20831298828125 = 0.10082019120454788 + 50.0 * 6.242149829864502
Epoch 1510, val loss: 1.2097821235656738
Epoch 1520, training loss: 312.0692443847656 = 0.09846430271863937 + 50.0 * 6.239416122436523
Epoch 1520, val loss: 1.2153831720352173
Epoch 1530, training loss: 312.1627197265625 = 0.09618202596902847 + 50.0 * 6.241330623626709
Epoch 1530, val loss: 1.2212600708007812
Epoch 1540, training loss: 312.0949401855469 = 0.09393949806690216 + 50.0 * 6.240020275115967
Epoch 1540, val loss: 1.2264659404754639
Epoch 1550, training loss: 312.0426940917969 = 0.09177694469690323 + 50.0 * 6.239018440246582
Epoch 1550, val loss: 1.2319914102554321
Epoch 1560, training loss: 311.9611511230469 = 0.08966735005378723 + 50.0 * 6.237430095672607
Epoch 1560, val loss: 1.2376385927200317
Epoch 1570, training loss: 312.1175231933594 = 0.08764296770095825 + 50.0 * 6.240597724914551
Epoch 1570, val loss: 1.2435836791992188
Epoch 1580, training loss: 312.0931091308594 = 0.08558917045593262 + 50.0 * 6.240150451660156
Epoch 1580, val loss: 1.248033881187439
Epoch 1590, training loss: 312.00848388671875 = 0.08359021693468094 + 50.0 * 6.238497734069824
Epoch 1590, val loss: 1.2539478540420532
Epoch 1600, training loss: 311.927490234375 = 0.0816715732216835 + 50.0 * 6.236916542053223
Epoch 1600, val loss: 1.2590198516845703
Epoch 1610, training loss: 311.9213562011719 = 0.0798296257853508 + 50.0 * 6.236830711364746
Epoch 1610, val loss: 1.2647165060043335
Epoch 1620, training loss: 312.21142578125 = 0.07801945507526398 + 50.0 * 6.2426676750183105
Epoch 1620, val loss: 1.2701098918914795
Epoch 1630, training loss: 311.9390869140625 = 0.07623569667339325 + 50.0 * 6.23725700378418
Epoch 1630, val loss: 1.275095820426941
Epoch 1640, training loss: 311.8257751464844 = 0.07451201975345612 + 50.0 * 6.235024929046631
Epoch 1640, val loss: 1.2806214094161987
Epoch 1650, training loss: 312.09521484375 = 0.07285391539335251 + 50.0 * 6.240447521209717
Epoch 1650, val loss: 1.2856093645095825
Epoch 1660, training loss: 311.8182373046875 = 0.07118590921163559 + 50.0 * 6.234941005706787
Epoch 1660, val loss: 1.291249394416809
Epoch 1670, training loss: 311.7887878417969 = 0.06958646327257156 + 50.0 * 6.234384059906006
Epoch 1670, val loss: 1.2961995601654053
Epoch 1680, training loss: 311.8376159667969 = 0.06804948300123215 + 50.0 * 6.235391139984131
Epoch 1680, val loss: 1.3015720844268799
Epoch 1690, training loss: 312.0391845703125 = 0.06654687225818634 + 50.0 * 6.239452362060547
Epoch 1690, val loss: 1.3066871166229248
Epoch 1700, training loss: 311.89337158203125 = 0.06504669040441513 + 50.0 * 6.236566543579102
Epoch 1700, val loss: 1.3116756677627563
Epoch 1710, training loss: 311.75738525390625 = 0.06359530985355377 + 50.0 * 6.233875751495361
Epoch 1710, val loss: 1.3166908025741577
Epoch 1720, training loss: 311.6860656738281 = 0.06221392750740051 + 50.0 * 6.232476711273193
Epoch 1720, val loss: 1.3218517303466797
Epoch 1730, training loss: 311.71533203125 = 0.060872070491313934 + 50.0 * 6.233088970184326
Epoch 1730, val loss: 1.3270623683929443
Epoch 1740, training loss: 311.9329528808594 = 0.05955106392502785 + 50.0 * 6.237468242645264
Epoch 1740, val loss: 1.331932783126831
Epoch 1750, training loss: 311.7659606933594 = 0.058251019567251205 + 50.0 * 6.234154224395752
Epoch 1750, val loss: 1.3365375995635986
Epoch 1760, training loss: 311.6701965332031 = 0.0569954514503479 + 50.0 * 6.232263565063477
Epoch 1760, val loss: 1.3416868448257446
Epoch 1770, training loss: 311.71661376953125 = 0.05578998103737831 + 50.0 * 6.233216762542725
Epoch 1770, val loss: 1.3466328382492065
Epoch 1780, training loss: 311.8435363769531 = 0.054614365100860596 + 50.0 * 6.235778331756592
Epoch 1780, val loss: 1.3511857986450195
Epoch 1790, training loss: 311.6211853027344 = 0.053428683429956436 + 50.0 * 6.2313551902771
Epoch 1790, val loss: 1.3559844493865967
Epoch 1800, training loss: 311.56939697265625 = 0.05229969695210457 + 50.0 * 6.230341911315918
Epoch 1800, val loss: 1.3606092929840088
Epoch 1810, training loss: 311.5654296875 = 0.05122053995728493 + 50.0 * 6.230284214019775
Epoch 1810, val loss: 1.3653216361999512
Epoch 1820, training loss: 311.5974426269531 = 0.05016844719648361 + 50.0 * 6.230945587158203
Epoch 1820, val loss: 1.3699959516525269
Epoch 1830, training loss: 311.80792236328125 = 0.04913317784667015 + 50.0 * 6.235175609588623
Epoch 1830, val loss: 1.374509334564209
Epoch 1840, training loss: 311.6756286621094 = 0.04812585562467575 + 50.0 * 6.232550144195557
Epoch 1840, val loss: 1.3791799545288086
Epoch 1850, training loss: 311.7099609375 = 0.04713758826255798 + 50.0 * 6.2332563400268555
Epoch 1850, val loss: 1.3836839199066162
Epoch 1860, training loss: 311.52691650390625 = 0.046177588403224945 + 50.0 * 6.229614734649658
Epoch 1860, val loss: 1.3884543180465698
Epoch 1870, training loss: 311.47344970703125 = 0.04524720087647438 + 50.0 * 6.2285637855529785
Epoch 1870, val loss: 1.3928086757659912
Epoch 1880, training loss: 311.4786682128906 = 0.0443609245121479 + 50.0 * 6.2286858558654785
Epoch 1880, val loss: 1.3972634077072144
Epoch 1890, training loss: 311.7546691894531 = 0.043497759848833084 + 50.0 * 6.234223365783691
Epoch 1890, val loss: 1.4014383554458618
Epoch 1900, training loss: 311.619384765625 = 0.04261254146695137 + 50.0 * 6.2315354347229
Epoch 1900, val loss: 1.4058587551116943
Epoch 1910, training loss: 311.5304870605469 = 0.04177049547433853 + 50.0 * 6.229773998260498
Epoch 1910, val loss: 1.4101063013076782
Epoch 1920, training loss: 311.59027099609375 = 0.04095353186130524 + 50.0 * 6.230986595153809
Epoch 1920, val loss: 1.4144212007522583
Epoch 1930, training loss: 311.3851013183594 = 0.040168099105358124 + 50.0 * 6.226898670196533
Epoch 1930, val loss: 1.4184324741363525
Epoch 1940, training loss: 311.4134521484375 = 0.039412591606378555 + 50.0 * 6.227481365203857
Epoch 1940, val loss: 1.4226263761520386
Epoch 1950, training loss: 311.5201110839844 = 0.03867417201399803 + 50.0 * 6.229629039764404
Epoch 1950, val loss: 1.42672598361969
Epoch 1960, training loss: 311.4532165527344 = 0.03793508931994438 + 50.0 * 6.228305816650391
Epoch 1960, val loss: 1.4310263395309448
Epoch 1970, training loss: 311.5133361816406 = 0.03722207248210907 + 50.0 * 6.229522228240967
Epoch 1970, val loss: 1.4349185228347778
Epoch 1980, training loss: 311.47637939453125 = 0.03652169927954674 + 50.0 * 6.22879695892334
Epoch 1980, val loss: 1.4387117624282837
Epoch 1990, training loss: 311.3464660644531 = 0.03584534674882889 + 50.0 * 6.226212024688721
Epoch 1990, val loss: 1.443178653717041
Epoch 2000, training loss: 311.31951904296875 = 0.03519541397690773 + 50.0 * 6.225686550140381
Epoch 2000, val loss: 1.4469329118728638
Epoch 2010, training loss: 311.3001708984375 = 0.034567855298519135 + 50.0 * 6.225311756134033
Epoch 2010, val loss: 1.4510267972946167
Epoch 2020, training loss: 311.6709899902344 = 0.033968087285757065 + 50.0 * 6.23274040222168
Epoch 2020, val loss: 1.4546781778335571
Epoch 2030, training loss: 311.4275207519531 = 0.03332595154643059 + 50.0 * 6.227883815765381
Epoch 2030, val loss: 1.45834481716156
Epoch 2040, training loss: 311.3560791015625 = 0.03272973746061325 + 50.0 * 6.226467132568359
Epoch 2040, val loss: 1.4621647596359253
Epoch 2050, training loss: 311.2561340332031 = 0.032142940908670425 + 50.0 * 6.224480152130127
Epoch 2050, val loss: 1.4658417701721191
Epoch 2060, training loss: 311.51202392578125 = 0.03158947825431824 + 50.0 * 6.22960901260376
Epoch 2060, val loss: 1.4695135354995728
Epoch 2070, training loss: 311.31536865234375 = 0.031028296798467636 + 50.0 * 6.225686550140381
Epoch 2070, val loss: 1.473250150680542
Epoch 2080, training loss: 311.2618713378906 = 0.030488383024930954 + 50.0 * 6.22462797164917
Epoch 2080, val loss: 1.4768080711364746
Epoch 2090, training loss: 311.1955871582031 = 0.029964499175548553 + 50.0 * 6.2233123779296875
Epoch 2090, val loss: 1.4806482791900635
Epoch 2100, training loss: 311.2286376953125 = 0.029464753344655037 + 50.0 * 6.223983287811279
Epoch 2100, val loss: 1.4842808246612549
Epoch 2110, training loss: 311.4168701171875 = 0.028975842520594597 + 50.0 * 6.227758407592773
Epoch 2110, val loss: 1.48768949508667
Epoch 2120, training loss: 311.28997802734375 = 0.028482623398303986 + 50.0 * 6.2252302169799805
Epoch 2120, val loss: 1.490784764289856
Epoch 2130, training loss: 311.3877258300781 = 0.028005477041006088 + 50.0 * 6.227194309234619
Epoch 2130, val loss: 1.4942682981491089
Epoch 2140, training loss: 311.5165710449219 = 0.027531586587429047 + 50.0 * 6.229780673980713
Epoch 2140, val loss: 1.4981377124786377
Epoch 2150, training loss: 311.2308349609375 = 0.027063826099038124 + 50.0 * 6.2240753173828125
Epoch 2150, val loss: 1.501299500465393
Epoch 2160, training loss: 311.13250732421875 = 0.026622610166668892 + 50.0 * 6.222117900848389
Epoch 2160, val loss: 1.5044994354248047
Epoch 2170, training loss: 311.1120300292969 = 0.026194525882601738 + 50.0 * 6.22171688079834
Epoch 2170, val loss: 1.507808804512024
Epoch 2180, training loss: 311.09942626953125 = 0.02578447200357914 + 50.0 * 6.22147274017334
Epoch 2180, val loss: 1.511032223701477
Epoch 2190, training loss: 311.3282775878906 = 0.025385839864611626 + 50.0 * 6.226057529449463
Epoch 2190, val loss: 1.513930082321167
Epoch 2200, training loss: 311.19610595703125 = 0.024981606751680374 + 50.0 * 6.223422050476074
Epoch 2200, val loss: 1.5176441669464111
Epoch 2210, training loss: 311.2158203125 = 0.024586545303463936 + 50.0 * 6.223824501037598
Epoch 2210, val loss: 1.5207093954086304
Epoch 2220, training loss: 311.1130065917969 = 0.024197803810238838 + 50.0 * 6.221776008605957
Epoch 2220, val loss: 1.5240410566329956
Epoch 2230, training loss: 311.1282653808594 = 0.023827699944376945 + 50.0 * 6.22208833694458
Epoch 2230, val loss: 1.5267560482025146
Epoch 2240, training loss: 311.1916809082031 = 0.02346913330256939 + 50.0 * 6.223364353179932
Epoch 2240, val loss: 1.5299686193466187
Epoch 2250, training loss: 311.1319580078125 = 0.023106394335627556 + 50.0 * 6.222177028656006
Epoch 2250, val loss: 1.5330886840820312
Epoch 2260, training loss: 311.1136779785156 = 0.022758550941944122 + 50.0 * 6.221817970275879
Epoch 2260, val loss: 1.5360441207885742
Epoch 2270, training loss: 311.04534912109375 = 0.022414464503526688 + 50.0 * 6.220458984375
Epoch 2270, val loss: 1.538974642753601
Epoch 2280, training loss: 311.156982421875 = 0.02208760753273964 + 50.0 * 6.222697734832764
Epoch 2280, val loss: 1.5423604249954224
Epoch 2290, training loss: 311.0872802734375 = 0.02175983227789402 + 50.0 * 6.221310138702393
Epoch 2290, val loss: 1.5448840856552124
Epoch 2300, training loss: 311.1986083984375 = 0.02143946662545204 + 50.0 * 6.223543167114258
Epoch 2300, val loss: 1.5475513935089111
Epoch 2310, training loss: 311.0163269042969 = 0.021115006878972054 + 50.0 * 6.219903945922852
Epoch 2310, val loss: 1.5501973628997803
Epoch 2320, training loss: 310.9755554199219 = 0.020805174484848976 + 50.0 * 6.219094753265381
Epoch 2320, val loss: 1.5531244277954102
Epoch 2330, training loss: 311.1072998046875 = 0.020513202995061874 + 50.0 * 6.221735954284668
Epoch 2330, val loss: 1.5561304092407227
Epoch 2340, training loss: 311.0652160644531 = 0.020216450095176697 + 50.0 * 6.220900535583496
Epoch 2340, val loss: 1.558542013168335
Epoch 2350, training loss: 311.010009765625 = 0.01993556134402752 + 50.0 * 6.219801425933838
Epoch 2350, val loss: 1.5609734058380127
Epoch 2360, training loss: 310.92779541015625 = 0.01965508423745632 + 50.0 * 6.218163013458252
Epoch 2360, val loss: 1.5641098022460938
Epoch 2370, training loss: 310.9772033691406 = 0.019389761611819267 + 50.0 * 6.219156742095947
Epoch 2370, val loss: 1.5667754411697388
Epoch 2380, training loss: 311.2018127441406 = 0.019128713756799698 + 50.0 * 6.223653793334961
Epoch 2380, val loss: 1.568967342376709
Epoch 2390, training loss: 311.0709533691406 = 0.01885458268225193 + 50.0 * 6.221042156219482
Epoch 2390, val loss: 1.571325659751892
Epoch 2400, training loss: 311.1611633300781 = 0.01859297603368759 + 50.0 * 6.222851753234863
Epoch 2400, val loss: 1.5740680694580078
Epoch 2410, training loss: 310.9310302734375 = 0.018342023715376854 + 50.0 * 6.2182536125183105
Epoch 2410, val loss: 1.5766485929489136
Epoch 2420, training loss: 310.9325866699219 = 0.01810183934867382 + 50.0 * 6.218289852142334
Epoch 2420, val loss: 1.579217553138733
Epoch 2430, training loss: 310.9486999511719 = 0.01786244660615921 + 50.0 * 6.218616962432861
Epoch 2430, val loss: 1.5815459489822388
Epoch 2440, training loss: 310.9758605957031 = 0.017627928406000137 + 50.0 * 6.2191643714904785
Epoch 2440, val loss: 1.5839333534240723
Epoch 2450, training loss: 310.95721435546875 = 0.01739448308944702 + 50.0 * 6.218796253204346
Epoch 2450, val loss: 1.5861352682113647
Epoch 2460, training loss: 310.9762268066406 = 0.017169734463095665 + 50.0 * 6.219181060791016
Epoch 2460, val loss: 1.5886874198913574
Epoch 2470, training loss: 310.9996337890625 = 0.016949281096458435 + 50.0 * 6.219654083251953
Epoch 2470, val loss: 1.591404676437378
Epoch 2480, training loss: 310.971923828125 = 0.016733018681406975 + 50.0 * 6.219103813171387
Epoch 2480, val loss: 1.5930753946304321
Epoch 2490, training loss: 310.8424377441406 = 0.01651594042778015 + 50.0 * 6.216518402099609
Epoch 2490, val loss: 1.5954432487487793
Epoch 2500, training loss: 311.1467590332031 = 0.016317931935191154 + 50.0 * 6.22260856628418
Epoch 2500, val loss: 1.597305178642273
Epoch 2510, training loss: 310.9919738769531 = 0.016103126108646393 + 50.0 * 6.219517230987549
Epoch 2510, val loss: 1.600138545036316
Epoch 2520, training loss: 310.8122863769531 = 0.01589447259902954 + 50.0 * 6.215927600860596
Epoch 2520, val loss: 1.6017698049545288
Epoch 2530, training loss: 310.78289794921875 = 0.015698732808232307 + 50.0 * 6.215343952178955
Epoch 2530, val loss: 1.6043972969055176
Epoch 2540, training loss: 310.8270263671875 = 0.015512815676629543 + 50.0 * 6.216230392456055
Epoch 2540, val loss: 1.606629729270935
Epoch 2550, training loss: 311.1575012207031 = 0.015328137204051018 + 50.0 * 6.222843647003174
Epoch 2550, val loss: 1.608465552330017
Epoch 2560, training loss: 310.94793701171875 = 0.015140061266720295 + 50.0 * 6.218656063079834
Epoch 2560, val loss: 1.610336184501648
Epoch 2570, training loss: 310.79327392578125 = 0.014952652156352997 + 50.0 * 6.215566158294678
Epoch 2570, val loss: 1.6127567291259766
Epoch 2580, training loss: 310.7347412109375 = 0.01477628480643034 + 50.0 * 6.214399337768555
Epoch 2580, val loss: 1.6147862672805786
Epoch 2590, training loss: 310.7513122558594 = 0.014607363380491734 + 50.0 * 6.214734077453613
Epoch 2590, val loss: 1.6168092489242554
Epoch 2600, training loss: 310.9324035644531 = 0.014443703927099705 + 50.0 * 6.218359470367432
Epoch 2600, val loss: 1.6188117265701294
Epoch 2610, training loss: 311.15863037109375 = 0.014275485649704933 + 50.0 * 6.22288703918457
Epoch 2610, val loss: 1.6212122440338135
Epoch 2620, training loss: 310.8398742675781 = 0.014095364138484001 + 50.0 * 6.21651554107666
Epoch 2620, val loss: 1.622300624847412
Epoch 2630, training loss: 310.73052978515625 = 0.01393335871398449 + 50.0 * 6.21433162689209
Epoch 2630, val loss: 1.624603033065796
Epoch 2640, training loss: 310.75079345703125 = 0.013779810629785061 + 50.0 * 6.214739799499512
Epoch 2640, val loss: 1.6266913414001465
Epoch 2650, training loss: 310.97845458984375 = 0.01363302394747734 + 50.0 * 6.219295978546143
Epoch 2650, val loss: 1.6285455226898193
Epoch 2660, training loss: 310.84423828125 = 0.01347311306744814 + 50.0 * 6.216615676879883
Epoch 2660, val loss: 1.6300708055496216
Epoch 2670, training loss: 310.67669677734375 = 0.013316010124981403 + 50.0 * 6.213267803192139
Epoch 2670, val loss: 1.6320476531982422
Epoch 2680, training loss: 310.6955871582031 = 0.013172011822462082 + 50.0 * 6.213647842407227
Epoch 2680, val loss: 1.6338149309158325
Epoch 2690, training loss: 311.1525573730469 = 0.013035072945058346 + 50.0 * 6.222790241241455
Epoch 2690, val loss: 1.6354179382324219
Epoch 2700, training loss: 310.79681396484375 = 0.01288696564733982 + 50.0 * 6.215678691864014
Epoch 2700, val loss: 1.6372511386871338
Epoch 2710, training loss: 310.7030944824219 = 0.01274396013468504 + 50.0 * 6.213806629180908
Epoch 2710, val loss: 1.6390093564987183
Epoch 2720, training loss: 310.7472839355469 = 0.012613954022526741 + 50.0 * 6.214693546295166
Epoch 2720, val loss: 1.6405645608901978
Epoch 2730, training loss: 310.8077392578125 = 0.012481085024774075 + 50.0 * 6.21590518951416
Epoch 2730, val loss: 1.6421509981155396
Epoch 2740, training loss: 310.7066345214844 = 0.012345871888101101 + 50.0 * 6.21388578414917
Epoch 2740, val loss: 1.6446207761764526
Epoch 2750, training loss: 310.9266052246094 = 0.01222322043031454 + 50.0 * 6.218287467956543
Epoch 2750, val loss: 1.6458719968795776
Epoch 2760, training loss: 310.62908935546875 = 0.012082977220416069 + 50.0 * 6.212340354919434
Epoch 2760, val loss: 1.6472517251968384
Epoch 2770, training loss: 310.6673278808594 = 0.011957000941038132 + 50.0 * 6.213107109069824
Epoch 2770, val loss: 1.6488984823226929
Epoch 2780, training loss: 310.6639099121094 = 0.011840587481856346 + 50.0 * 6.213041305541992
Epoch 2780, val loss: 1.6505792140960693
Epoch 2790, training loss: 310.7053527832031 = 0.011722653172910213 + 50.0 * 6.213872909545898
Epoch 2790, val loss: 1.6522762775421143
Epoch 2800, training loss: 310.665771484375 = 0.011602021753787994 + 50.0 * 6.213083267211914
Epoch 2800, val loss: 1.653696894645691
Epoch 2810, training loss: 310.7279968261719 = 0.011487103067338467 + 50.0 * 6.214330673217773
Epoch 2810, val loss: 1.6551328897476196
Epoch 2820, training loss: 310.67901611328125 = 0.011373724788427353 + 50.0 * 6.213352680206299
Epoch 2820, val loss: 1.656945824623108
Epoch 2830, training loss: 310.75128173828125 = 0.01126327458769083 + 50.0 * 6.2148003578186035
Epoch 2830, val loss: 1.6582642793655396
Epoch 2840, training loss: 310.71295166015625 = 0.011148559860885143 + 50.0 * 6.214035987854004
Epoch 2840, val loss: 1.6594507694244385
Epoch 2850, training loss: 310.63067626953125 = 0.011037091724574566 + 50.0 * 6.212392807006836
Epoch 2850, val loss: 1.6608495712280273
Epoch 2860, training loss: 310.6016540527344 = 0.010928865522146225 + 50.0 * 6.2118144035339355
Epoch 2860, val loss: 1.6623910665512085
Epoch 2870, training loss: 310.642333984375 = 0.010827619582414627 + 50.0 * 6.212630271911621
Epoch 2870, val loss: 1.6640417575836182
Epoch 2880, training loss: 310.7825927734375 = 0.010724556632339954 + 50.0 * 6.215437889099121
Epoch 2880, val loss: 1.665482997894287
Epoch 2890, training loss: 310.7045593261719 = 0.010620228946208954 + 50.0 * 6.213878631591797
Epoch 2890, val loss: 1.6663241386413574
Epoch 2900, training loss: 310.61041259765625 = 0.010517758317291737 + 50.0 * 6.2119975090026855
Epoch 2900, val loss: 1.6678211688995361
Epoch 2910, training loss: 310.58642578125 = 0.010421168990433216 + 50.0 * 6.211520671844482
Epoch 2910, val loss: 1.6692179441452026
Epoch 2920, training loss: 310.72589111328125 = 0.010332419537007809 + 50.0 * 6.214311122894287
Epoch 2920, val loss: 1.6706695556640625
Epoch 2930, training loss: 310.73089599609375 = 0.010233494453132153 + 50.0 * 6.214413642883301
Epoch 2930, val loss: 1.671504020690918
Epoch 2940, training loss: 310.6278076171875 = 0.010129579342901707 + 50.0 * 6.212353706359863
Epoch 2940, val loss: 1.6725131273269653
Epoch 2950, training loss: 310.6072082519531 = 0.010039395652711391 + 50.0 * 6.211943626403809
Epoch 2950, val loss: 1.67403244972229
Epoch 2960, training loss: 310.5307922363281 = 0.009949258528649807 + 50.0 * 6.210416793823242
Epoch 2960, val loss: 1.6750810146331787
Epoch 2970, training loss: 310.5050354003906 = 0.009862481616437435 + 50.0 * 6.209903717041016
Epoch 2970, val loss: 1.6766421794891357
Epoch 2980, training loss: 310.5673828125 = 0.00977947749197483 + 50.0 * 6.211152076721191
Epoch 2980, val loss: 1.6777112483978271
Epoch 2990, training loss: 310.6305236816406 = 0.00969516672194004 + 50.0 * 6.212416648864746
Epoch 2990, val loss: 1.6786479949951172
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6888888888888889
0.823405376910912
=== training gcn model ===
Epoch 0, training loss: 431.7738952636719 = 1.9309908151626587 + 50.0 * 8.596858024597168
Epoch 0, val loss: 1.9274739027023315
Epoch 10, training loss: 431.7320861816406 = 1.9221895933151245 + 50.0 * 8.596198081970215
Epoch 10, val loss: 1.9192631244659424
Epoch 20, training loss: 431.48406982421875 = 1.9111838340759277 + 50.0 * 8.59145736694336
Epoch 20, val loss: 1.9086724519729614
Epoch 30, training loss: 429.9284973144531 = 1.8970555067062378 + 50.0 * 8.560628890991211
Epoch 30, val loss: 1.8949414491653442
Epoch 40, training loss: 421.7601318359375 = 1.8805649280548096 + 50.0 * 8.397591590881348
Epoch 40, val loss: 1.8795088529586792
Epoch 50, training loss: 391.548095703125 = 1.8640302419662476 + 50.0 * 7.7936811447143555
Epoch 50, val loss: 1.8642396926879883
Epoch 60, training loss: 376.0715026855469 = 1.8519706726074219 + 50.0 * 7.484390735626221
Epoch 60, val loss: 1.8524810075759888
Epoch 70, training loss: 363.67083740234375 = 1.842048168182373 + 50.0 * 7.236576080322266
Epoch 70, val loss: 1.8416883945465088
Epoch 80, training loss: 352.8423156738281 = 1.8316386938095093 + 50.0 * 7.020213603973389
Epoch 80, val loss: 1.8307658433914185
Epoch 90, training loss: 345.95025634765625 = 1.8228602409362793 + 50.0 * 6.8825483322143555
Epoch 90, val loss: 1.8213517665863037
Epoch 100, training loss: 342.2164306640625 = 1.813897967338562 + 50.0 * 6.808050632476807
Epoch 100, val loss: 1.8121843338012695
Epoch 110, training loss: 339.53387451171875 = 1.8046542406082153 + 50.0 * 6.754584312438965
Epoch 110, val loss: 1.8029669523239136
Epoch 120, training loss: 337.1657409667969 = 1.7958152294158936 + 50.0 * 6.707398891448975
Epoch 120, val loss: 1.7941489219665527
Epoch 130, training loss: 335.1634216308594 = 1.7874016761779785 + 50.0 * 6.667520046234131
Epoch 130, val loss: 1.7857699394226074
Epoch 140, training loss: 333.5355529785156 = 1.7789064645767212 + 50.0 * 6.635133266448975
Epoch 140, val loss: 1.777480125427246
Epoch 150, training loss: 332.1342468261719 = 1.7699036598205566 + 50.0 * 6.6072869300842285
Epoch 150, val loss: 1.7687350511550903
Epoch 160, training loss: 331.075927734375 = 1.7597572803497314 + 50.0 * 6.586323261260986
Epoch 160, val loss: 1.7591371536254883
Epoch 170, training loss: 329.9666442871094 = 1.748740315437317 + 50.0 * 6.564357757568359
Epoch 170, val loss: 1.7484668493270874
Epoch 180, training loss: 328.91314697265625 = 1.7366511821746826 + 50.0 * 6.543529510498047
Epoch 180, val loss: 1.737004041671753
Epoch 190, training loss: 328.0777587890625 = 1.7236353158950806 + 50.0 * 6.527082443237305
Epoch 190, val loss: 1.7246475219726562
Epoch 200, training loss: 327.1246032714844 = 1.7092633247375488 + 50.0 * 6.508306980133057
Epoch 200, val loss: 1.7110451459884644
Epoch 210, training loss: 326.2422790527344 = 1.6936894655227661 + 50.0 * 6.490971565246582
Epoch 210, val loss: 1.6963739395141602
Epoch 220, training loss: 325.5162353515625 = 1.676721215248108 + 50.0 * 6.476790428161621
Epoch 220, val loss: 1.6803157329559326
Epoch 230, training loss: 325.13226318359375 = 1.6581642627716064 + 50.0 * 6.469481945037842
Epoch 230, val loss: 1.6627616882324219
Epoch 240, training loss: 324.3833923339844 = 1.6378990411758423 + 50.0 * 6.454909801483154
Epoch 240, val loss: 1.6437088251113892
Epoch 250, training loss: 323.8265380859375 = 1.6162538528442383 + 50.0 * 6.4442057609558105
Epoch 250, val loss: 1.623274326324463
Epoch 260, training loss: 323.3433837890625 = 1.5932118892669678 + 50.0 * 6.435003757476807
Epoch 260, val loss: 1.601625919342041
Epoch 270, training loss: 323.1947326660156 = 1.5686297416687012 + 50.0 * 6.432521820068359
Epoch 270, val loss: 1.5787750482559204
Epoch 280, training loss: 322.5108337402344 = 1.5430943965911865 + 50.0 * 6.4193549156188965
Epoch 280, val loss: 1.5547270774841309
Epoch 290, training loss: 322.1558532714844 = 1.5166096687316895 + 50.0 * 6.412785053253174
Epoch 290, val loss: 1.5300897359848022
Epoch 300, training loss: 321.7752685546875 = 1.4893969297409058 + 50.0 * 6.405716896057129
Epoch 300, val loss: 1.5048738718032837
Epoch 310, training loss: 321.8260498046875 = 1.461596131324768 + 50.0 * 6.407288551330566
Epoch 310, val loss: 1.4791069030761719
Epoch 320, training loss: 321.25897216796875 = 1.4331883192062378 + 50.0 * 6.396515369415283
Epoch 320, val loss: 1.4528664350509644
Epoch 330, training loss: 320.87933349609375 = 1.4046767950057983 + 50.0 * 6.389493465423584
Epoch 330, val loss: 1.42652428150177
Epoch 340, training loss: 320.8025207519531 = 1.3763264417648315 + 50.0 * 6.388524055480957
Epoch 340, val loss: 1.4003269672393799
Epoch 350, training loss: 320.3705749511719 = 1.347720980644226 + 50.0 * 6.380457401275635
Epoch 350, val loss: 1.3743057250976562
Epoch 360, training loss: 320.1132507324219 = 1.319486379623413 + 50.0 * 6.375875473022461
Epoch 360, val loss: 1.348535180091858
Epoch 370, training loss: 319.8573913574219 = 1.2915881872177124 + 50.0 * 6.371315956115723
Epoch 370, val loss: 1.3231045007705688
Epoch 380, training loss: 319.6982116699219 = 1.2638249397277832 + 50.0 * 6.368687629699707
Epoch 380, val loss: 1.2978802919387817
Epoch 390, training loss: 319.4521484375 = 1.236454963684082 + 50.0 * 6.364314079284668
Epoch 390, val loss: 1.273390769958496
Epoch 400, training loss: 319.23333740234375 = 1.209497332572937 + 50.0 * 6.360476970672607
Epoch 400, val loss: 1.2495187520980835
Epoch 410, training loss: 319.0220947265625 = 1.1831477880477905 + 50.0 * 6.356779098510742
Epoch 410, val loss: 1.2262977361679077
Epoch 420, training loss: 319.0741271972656 = 1.1571624279022217 + 50.0 * 6.358339309692383
Epoch 420, val loss: 1.2037805318832397
Epoch 430, training loss: 318.8385314941406 = 1.131742238998413 + 50.0 * 6.354135990142822
Epoch 430, val loss: 1.181587815284729
Epoch 440, training loss: 318.5285339355469 = 1.1067874431610107 + 50.0 * 6.348434925079346
Epoch 440, val loss: 1.1604695320129395
Epoch 450, training loss: 318.31146240234375 = 1.0824719667434692 + 50.0 * 6.344580173492432
Epoch 450, val loss: 1.140135645866394
Epoch 460, training loss: 318.3409118652344 = 1.0587035417556763 + 50.0 * 6.345643997192383
Epoch 460, val loss: 1.120742678642273
Epoch 470, training loss: 318.09039306640625 = 1.0356371402740479 + 50.0 * 6.341094970703125
Epoch 470, val loss: 1.1017484664916992
Epoch 480, training loss: 317.8157653808594 = 1.0129543542861938 + 50.0 * 6.336056232452393
Epoch 480, val loss: 1.0838394165039062
Epoch 490, training loss: 317.64801025390625 = 0.9910027980804443 + 50.0 * 6.3331403732299805
Epoch 490, val loss: 1.0667606592178345
Epoch 500, training loss: 317.5386047363281 = 0.9696618318557739 + 50.0 * 6.331378936767578
Epoch 500, val loss: 1.0505303144454956
Epoch 510, training loss: 317.3443908691406 = 0.9487752318382263 + 50.0 * 6.3279128074646
Epoch 510, val loss: 1.0351085662841797
Epoch 520, training loss: 317.2773132324219 = 0.928463876247406 + 50.0 * 6.326976776123047
Epoch 520, val loss: 1.0204545259475708
Epoch 530, training loss: 317.08551025390625 = 0.9086330533027649 + 50.0 * 6.323537826538086
Epoch 530, val loss: 1.0066699981689453
Epoch 540, training loss: 317.1609191894531 = 0.8891676664352417 + 50.0 * 6.325435161590576
Epoch 540, val loss: 0.9937036633491516
Epoch 550, training loss: 316.8851623535156 = 0.8702730536460876 + 50.0 * 6.320297718048096
Epoch 550, val loss: 0.9811055660247803
Epoch 560, training loss: 316.7063293457031 = 0.8518127202987671 + 50.0 * 6.3170905113220215
Epoch 560, val loss: 0.9694646596908569
Epoch 570, training loss: 316.56707763671875 = 0.8338325023651123 + 50.0 * 6.314664840698242
Epoch 570, val loss: 0.9585437774658203
Epoch 580, training loss: 316.8468017578125 = 0.8162623047828674 + 50.0 * 6.320611000061035
Epoch 580, val loss: 0.9482918977737427
Epoch 590, training loss: 316.3868713378906 = 0.7989071607589722 + 50.0 * 6.311758995056152
Epoch 590, val loss: 0.9386111497879028
Epoch 600, training loss: 316.3046569824219 = 0.7819587588310242 + 50.0 * 6.310454368591309
Epoch 600, val loss: 0.9294888973236084
Epoch 610, training loss: 316.15625 = 0.7653806805610657 + 50.0 * 6.307817459106445
Epoch 610, val loss: 0.9210945963859558
Epoch 620, training loss: 316.2137451171875 = 0.74923175573349 + 50.0 * 6.309290409088135
Epoch 620, val loss: 0.913192093372345
Epoch 630, training loss: 316.1343078613281 = 0.7332306504249573 + 50.0 * 6.308021068572998
Epoch 630, val loss: 0.9055328965187073
Epoch 640, training loss: 315.8819580078125 = 0.7174957990646362 + 50.0 * 6.30328893661499
Epoch 640, val loss: 0.8987082839012146
Epoch 650, training loss: 315.76336669921875 = 0.7021678686141968 + 50.0 * 6.3012237548828125
Epoch 650, val loss: 0.8922317028045654
Epoch 660, training loss: 315.874267578125 = 0.687133252620697 + 50.0 * 6.303742408752441
Epoch 660, val loss: 0.8861264586448669
Epoch 670, training loss: 315.63409423828125 = 0.6723754405975342 + 50.0 * 6.299234867095947
Epoch 670, val loss: 0.8806478381156921
Epoch 680, training loss: 315.5304870605469 = 0.6577968597412109 + 50.0 * 6.2974534034729
Epoch 680, val loss: 0.875417947769165
Epoch 690, training loss: 315.5081787109375 = 0.6436576247215271 + 50.0 * 6.297290802001953
Epoch 690, val loss: 0.8706806898117065
Epoch 700, training loss: 315.3299255371094 = 0.6296442747116089 + 50.0 * 6.294005870819092
Epoch 700, val loss: 0.8664325475692749
Epoch 710, training loss: 315.2518615722656 = 0.6158912777900696 + 50.0 * 6.292718887329102
Epoch 710, val loss: 0.8624792098999023
Epoch 720, training loss: 315.27374267578125 = 0.6025074124336243 + 50.0 * 6.293424606323242
Epoch 720, val loss: 0.8589472770690918
Epoch 730, training loss: 315.1372375488281 = 0.5892149806022644 + 50.0 * 6.290960788726807
Epoch 730, val loss: 0.8556591868400574
Epoch 740, training loss: 315.0201416015625 = 0.5762357115745544 + 50.0 * 6.288877964019775
Epoch 740, val loss: 0.8527703881263733
Epoch 750, training loss: 315.5346374511719 = 0.5635272264480591 + 50.0 * 6.299421787261963
Epoch 750, val loss: 0.8500629663467407
Epoch 760, training loss: 314.92095947265625 = 0.5508466362953186 + 50.0 * 6.287402629852295
Epoch 760, val loss: 0.8477967977523804
Epoch 770, training loss: 314.85089111328125 = 0.53849196434021 + 50.0 * 6.286248207092285
Epoch 770, val loss: 0.8458747267723083
Epoch 780, training loss: 314.7469482421875 = 0.5264371037483215 + 50.0 * 6.28441047668457
Epoch 780, val loss: 0.8441997170448303
Epoch 790, training loss: 314.7112731933594 = 0.5146016478538513 + 50.0 * 6.283933639526367
Epoch 790, val loss: 0.8429040312767029
Epoch 800, training loss: 314.9119873046875 = 0.5028187036514282 + 50.0 * 6.288183689117432
Epoch 800, val loss: 0.8421215415000916
Epoch 810, training loss: 314.7247619628906 = 0.4913448095321655 + 50.0 * 6.284668445587158
Epoch 810, val loss: 0.8409019708633423
Epoch 820, training loss: 314.5447998046875 = 0.47999265789985657 + 50.0 * 6.281296253204346
Epoch 820, val loss: 0.8404533863067627
Epoch 830, training loss: 314.4359130859375 = 0.46898138523101807 + 50.0 * 6.279338359832764
Epoch 830, val loss: 0.8402726054191589
Epoch 840, training loss: 314.7242736816406 = 0.45811697840690613 + 50.0 * 6.285322666168213
Epoch 840, val loss: 0.8404045701026917
Epoch 850, training loss: 314.38336181640625 = 0.44730648398399353 + 50.0 * 6.278721332550049
Epoch 850, val loss: 0.8406112194061279
Epoch 860, training loss: 314.34490966796875 = 0.43669626116752625 + 50.0 * 6.278163909912109
Epoch 860, val loss: 0.8409022688865662
Epoch 870, training loss: 314.21502685546875 = 0.42631569504737854 + 50.0 * 6.275774002075195
Epoch 870, val loss: 0.8414455652236938
Epoch 880, training loss: 314.2353515625 = 0.4161571264266968 + 50.0 * 6.276383876800537
Epoch 880, val loss: 0.8422166109085083
Epoch 890, training loss: 314.3906555175781 = 0.4061363935470581 + 50.0 * 6.279690742492676
Epoch 890, val loss: 0.8433180451393127
Epoch 900, training loss: 314.1136169433594 = 0.3960462808609009 + 50.0 * 6.274351119995117
Epoch 900, val loss: 0.8443571925163269
Epoch 910, training loss: 314.0255432128906 = 0.38629406690597534 + 50.0 * 6.272785186767578
Epoch 910, val loss: 0.8459607362747192
Epoch 920, training loss: 313.9367370605469 = 0.3767450749874115 + 50.0 * 6.271200180053711
Epoch 920, val loss: 0.8475884199142456
Epoch 930, training loss: 314.0673828125 = 0.3673136532306671 + 50.0 * 6.274001598358154
Epoch 930, val loss: 0.8496065139770508
Epoch 940, training loss: 313.93621826171875 = 0.3580635190010071 + 50.0 * 6.2715630531311035
Epoch 940, val loss: 0.851508378982544
Epoch 950, training loss: 313.8431091308594 = 0.34882035851478577 + 50.0 * 6.269886016845703
Epoch 950, val loss: 0.8539031147956848
Epoch 960, training loss: 313.8262939453125 = 0.33986398577690125 + 50.0 * 6.269728660583496
Epoch 960, val loss: 0.8562293648719788
Epoch 970, training loss: 313.7010498046875 = 0.33103135228157043 + 50.0 * 6.267400741577148
Epoch 970, val loss: 0.8589740991592407
Epoch 980, training loss: 313.6576232910156 = 0.32236969470977783 + 50.0 * 6.266705513000488
Epoch 980, val loss: 0.8619402647018433
Epoch 990, training loss: 313.7616882324219 = 0.3138550817966461 + 50.0 * 6.268957138061523
Epoch 990, val loss: 0.8649067878723145
Epoch 1000, training loss: 313.57427978515625 = 0.30552560091018677 + 50.0 * 6.265375137329102
Epoch 1000, val loss: 0.8680973052978516
Epoch 1010, training loss: 313.5332336425781 = 0.29736676812171936 + 50.0 * 6.2647175788879395
Epoch 1010, val loss: 0.8713181018829346
Epoch 1020, training loss: 313.65411376953125 = 0.28934380412101746 + 50.0 * 6.2672953605651855
Epoch 1020, val loss: 0.8748918175697327
Epoch 1030, training loss: 313.46417236328125 = 0.2813909947872162 + 50.0 * 6.263655662536621
Epoch 1030, val loss: 0.878784716129303
Epoch 1040, training loss: 313.34637451171875 = 0.27370327711105347 + 50.0 * 6.261453151702881
Epoch 1040, val loss: 0.8825420141220093
Epoch 1050, training loss: 313.3571472167969 = 0.2661745250225067 + 50.0 * 6.261819362640381
Epoch 1050, val loss: 0.8864131569862366
Epoch 1060, training loss: 313.4629211425781 = 0.2588045299053192 + 50.0 * 6.264081954956055
Epoch 1060, val loss: 0.8906924724578857
Epoch 1070, training loss: 313.2882385253906 = 0.251507431268692 + 50.0 * 6.260735034942627
Epoch 1070, val loss: 0.8946102857589722
Epoch 1080, training loss: 313.1974182128906 = 0.24445798993110657 + 50.0 * 6.259059429168701
Epoch 1080, val loss: 0.8990699648857117
Epoch 1090, training loss: 313.1391906738281 = 0.2376195639371872 + 50.0 * 6.258031845092773
Epoch 1090, val loss: 0.9036940932273865
Epoch 1100, training loss: 313.6839294433594 = 0.23104748129844666 + 50.0 * 6.269057750701904
Epoch 1100, val loss: 0.9081620573997498
Epoch 1110, training loss: 313.1505126953125 = 0.2242765575647354 + 50.0 * 6.2585248947143555
Epoch 1110, val loss: 0.9128203988075256
Epoch 1120, training loss: 313.0186767578125 = 0.21786251664161682 + 50.0 * 6.256016254425049
Epoch 1120, val loss: 0.917629599571228
Epoch 1130, training loss: 312.9942321777344 = 0.21168220043182373 + 50.0 * 6.255651473999023
Epoch 1130, val loss: 0.9223921895027161
Epoch 1140, training loss: 313.3934326171875 = 0.2056615948677063 + 50.0 * 6.2637553215026855
Epoch 1140, val loss: 0.9271928071975708
Epoch 1150, training loss: 313.1107177734375 = 0.19961406290531158 + 50.0 * 6.2582221031188965
Epoch 1150, val loss: 0.932282030582428
Epoch 1160, training loss: 312.9405517578125 = 0.19386446475982666 + 50.0 * 6.2549333572387695
Epoch 1160, val loss: 0.9372608661651611
Epoch 1170, training loss: 312.87017822265625 = 0.18824024498462677 + 50.0 * 6.253639221191406
Epoch 1170, val loss: 0.9425008893013
Epoch 1180, training loss: 312.967529296875 = 0.1828189641237259 + 50.0 * 6.2556939125061035
Epoch 1180, val loss: 0.9476938843727112
Epoch 1190, training loss: 313.0858459472656 = 0.1775580197572708 + 50.0 * 6.2581658363342285
Epoch 1190, val loss: 0.9525600671768188
Epoch 1200, training loss: 312.8641052246094 = 0.17230771481990814 + 50.0 * 6.253836154937744
Epoch 1200, val loss: 0.9579389691352844
Epoch 1210, training loss: 312.7239990234375 = 0.16731515526771545 + 50.0 * 6.251133441925049
Epoch 1210, val loss: 0.9631357789039612
Epoch 1220, training loss: 312.66009521484375 = 0.16245868802070618 + 50.0 * 6.249952793121338
Epoch 1220, val loss: 0.9685526490211487
Epoch 1230, training loss: 312.85943603515625 = 0.15776287019252777 + 50.0 * 6.254033088684082
Epoch 1230, val loss: 0.9739425182342529
Epoch 1240, training loss: 312.6628723144531 = 0.15318267047405243 + 50.0 * 6.250194072723389
Epoch 1240, val loss: 0.9791995286941528
Epoch 1250, training loss: 312.60723876953125 = 0.14871741831302643 + 50.0 * 6.249170303344727
Epoch 1250, val loss: 0.9844821691513062
Epoch 1260, training loss: 312.5592956542969 = 0.14445048570632935 + 50.0 * 6.248297214508057
Epoch 1260, val loss: 0.9899130463600159
Epoch 1270, training loss: 312.5755920410156 = 0.14031489193439484 + 50.0 * 6.2487053871154785
Epoch 1270, val loss: 0.9952434301376343
Epoch 1280, training loss: 312.6573486328125 = 0.1363128423690796 + 50.0 * 6.250420570373535
Epoch 1280, val loss: 1.0006240606307983
Epoch 1290, training loss: 312.6654052734375 = 0.1323225498199463 + 50.0 * 6.250661849975586
Epoch 1290, val loss: 1.006064534187317
Epoch 1300, training loss: 312.53668212890625 = 0.12851668894290924 + 50.0 * 6.24816370010376
Epoch 1300, val loss: 1.0115777254104614
Epoch 1310, training loss: 312.4530334472656 = 0.12484360486268997 + 50.0 * 6.246563911437988
Epoch 1310, val loss: 1.017085075378418
Epoch 1320, training loss: 312.3993225097656 = 0.12133190780878067 + 50.0 * 6.2455596923828125
Epoch 1320, val loss: 1.0226167440414429
Epoch 1330, training loss: 312.4068603515625 = 0.1179075837135315 + 50.0 * 6.245779037475586
Epoch 1330, val loss: 1.0280996561050415
Epoch 1340, training loss: 312.46917724609375 = 0.11458836495876312 + 50.0 * 6.247091770172119
Epoch 1340, val loss: 1.0335835218429565
Epoch 1350, training loss: 312.5155944824219 = 0.11133597791194916 + 50.0 * 6.2480854988098145
Epoch 1350, val loss: 1.0395184755325317
Epoch 1360, training loss: 312.54833984375 = 0.10821983218193054 + 50.0 * 6.248802661895752
Epoch 1360, val loss: 1.0445940494537354
Epoch 1370, training loss: 312.341064453125 = 0.10520335286855698 + 50.0 * 6.244717597961426
Epoch 1370, val loss: 1.050097107887268
Epoch 1380, training loss: 312.25 = 0.10227571427822113 + 50.0 * 6.242954730987549
Epoch 1380, val loss: 1.0556174516677856
Epoch 1390, training loss: 312.18267822265625 = 0.09949005395174026 + 50.0 * 6.241663932800293
Epoch 1390, val loss: 1.0611884593963623
Epoch 1400, training loss: 312.16375732421875 = 0.09680097550153732 + 50.0 * 6.241339206695557
Epoch 1400, val loss: 1.0666841268539429
Epoch 1410, training loss: 312.45855712890625 = 0.09420783072710037 + 50.0 * 6.247286796569824
Epoch 1410, val loss: 1.0721824169158936
Epoch 1420, training loss: 312.2365417480469 = 0.09163262695074081 + 50.0 * 6.242897987365723
Epoch 1420, val loss: 1.0774110555648804
Epoch 1430, training loss: 312.26446533203125 = 0.0892055481672287 + 50.0 * 6.243505001068115
Epoch 1430, val loss: 1.0828999280929565
Epoch 1440, training loss: 312.2098083496094 = 0.08683665096759796 + 50.0 * 6.242459774017334
Epoch 1440, val loss: 1.088057279586792
Epoch 1450, training loss: 312.06048583984375 = 0.08451706916093826 + 50.0 * 6.2395195960998535
Epoch 1450, val loss: 1.093520998954773
Epoch 1460, training loss: 312.2400817871094 = 0.08230963349342346 + 50.0 * 6.243155479431152
Epoch 1460, val loss: 1.0987415313720703
Epoch 1470, training loss: 312.0694274902344 = 0.08019427210092545 + 50.0 * 6.2397847175598145
Epoch 1470, val loss: 1.1041290760040283
Epoch 1480, training loss: 311.9915466308594 = 0.07811614125967026 + 50.0 * 6.2382683753967285
Epoch 1480, val loss: 1.10928213596344
Epoch 1490, training loss: 312.11822509765625 = 0.07613169401884079 + 50.0 * 6.240841388702393
Epoch 1490, val loss: 1.1145600080490112
Epoch 1500, training loss: 311.99407958984375 = 0.07417959719896317 + 50.0 * 6.23839807510376
Epoch 1500, val loss: 1.119603157043457
Epoch 1510, training loss: 312.1099853515625 = 0.07228074222803116 + 50.0 * 6.240754127502441
Epoch 1510, val loss: 1.125054955482483
Epoch 1520, training loss: 312.0133361816406 = 0.07047928124666214 + 50.0 * 6.238857269287109
Epoch 1520, val loss: 1.129949688911438
Epoch 1530, training loss: 311.91851806640625 = 0.068716861307621 + 50.0 * 6.236995697021484
Epoch 1530, val loss: 1.135154128074646
Epoch 1540, training loss: 311.8944091796875 = 0.06703978031873703 + 50.0 * 6.236547470092773
Epoch 1540, val loss: 1.1401727199554443
Epoch 1550, training loss: 312.1017150878906 = 0.06542150676250458 + 50.0 * 6.240725994110107
Epoch 1550, val loss: 1.1452717781066895
Epoch 1560, training loss: 311.88043212890625 = 0.06381496787071228 + 50.0 * 6.236332416534424
Epoch 1560, val loss: 1.1503372192382812
Epoch 1570, training loss: 311.8174743652344 = 0.062268227338790894 + 50.0 * 6.235104084014893
Epoch 1570, val loss: 1.1552395820617676
Epoch 1580, training loss: 312.0885009765625 = 0.0607851967215538 + 50.0 * 6.240554332733154
Epoch 1580, val loss: 1.1602504253387451
Epoch 1590, training loss: 312.0101623535156 = 0.05936392396688461 + 50.0 * 6.239016056060791
Epoch 1590, val loss: 1.1646565198898315
Epoch 1600, training loss: 311.8036193847656 = 0.05792050436139107 + 50.0 * 6.2349138259887695
Epoch 1600, val loss: 1.1698341369628906
Epoch 1610, training loss: 311.73699951171875 = 0.056589022278785706 + 50.0 * 6.233608245849609
Epoch 1610, val loss: 1.1746333837509155
Epoch 1620, training loss: 311.7063903808594 = 0.05529157444834709 + 50.0 * 6.2330217361450195
Epoch 1620, val loss: 1.1795649528503418
Epoch 1630, training loss: 312.02545166015625 = 0.054056379944086075 + 50.0 * 6.2394280433654785
Epoch 1630, val loss: 1.1843206882476807
Epoch 1640, training loss: 311.77081298828125 = 0.05278492346405983 + 50.0 * 6.234360218048096
Epoch 1640, val loss: 1.188905119895935
Epoch 1650, training loss: 311.84344482421875 = 0.05158848688006401 + 50.0 * 6.235837459564209
Epoch 1650, val loss: 1.1935687065124512
Epoch 1660, training loss: 311.6855163574219 = 0.05042548105120659 + 50.0 * 6.232701778411865
Epoch 1660, val loss: 1.198256015777588
Epoch 1670, training loss: 311.6737976074219 = 0.049325816333293915 + 50.0 * 6.232489585876465
Epoch 1670, val loss: 1.203150749206543
Epoch 1680, training loss: 311.65802001953125 = 0.048238374292850494 + 50.0 * 6.2321953773498535
Epoch 1680, val loss: 1.2076690196990967
Epoch 1690, training loss: 311.9367370605469 = 0.04719698056578636 + 50.0 * 6.237790584564209
Epoch 1690, val loss: 1.2120742797851562
Epoch 1700, training loss: 311.8070068359375 = 0.04616634175181389 + 50.0 * 6.2352166175842285
Epoch 1700, val loss: 1.216638445854187
Epoch 1710, training loss: 311.602783203125 = 0.04515140503644943 + 50.0 * 6.231152534484863
Epoch 1710, val loss: 1.2211072444915771
Epoch 1720, training loss: 311.5345764160156 = 0.0441904254257679 + 50.0 * 6.2298078536987305
Epoch 1720, val loss: 1.2257282733917236
Epoch 1730, training loss: 311.5364074707031 = 0.04327578470110893 + 50.0 * 6.229862689971924
Epoch 1730, val loss: 1.2303247451782227
Epoch 1740, training loss: 311.94989013671875 = 0.042408786714076996 + 50.0 * 6.238149166107178
Epoch 1740, val loss: 1.2345762252807617
Epoch 1750, training loss: 311.6407775878906 = 0.04146156832575798 + 50.0 * 6.231986045837402
Epoch 1750, val loss: 1.2387053966522217
Epoch 1760, training loss: 311.5437316894531 = 0.04062493517994881 + 50.0 * 6.230062007904053
Epoch 1760, val loss: 1.2431927919387817
Epoch 1770, training loss: 311.5302734375 = 0.03979054093360901 + 50.0 * 6.229809284210205
Epoch 1770, val loss: 1.2475123405456543
Epoch 1780, training loss: 311.56365966796875 = 0.03898197039961815 + 50.0 * 6.230494022369385
Epoch 1780, val loss: 1.2519376277923584
Epoch 1790, training loss: 311.4803161621094 = 0.03818671777844429 + 50.0 * 6.228842735290527
Epoch 1790, val loss: 1.2561777830123901
Epoch 1800, training loss: 311.6002502441406 = 0.037423186004161835 + 50.0 * 6.23125696182251
Epoch 1800, val loss: 1.260365605354309
Epoch 1810, training loss: 311.4449768066406 = 0.03668344020843506 + 50.0 * 6.228165626525879
Epoch 1810, val loss: 1.2647374868392944
Epoch 1820, training loss: 311.40728759765625 = 0.035952530801296234 + 50.0 * 6.227427005767822
Epoch 1820, val loss: 1.2690765857696533
Epoch 1830, training loss: 311.4836120605469 = 0.03525620326399803 + 50.0 * 6.228967189788818
Epoch 1830, val loss: 1.273437738418579
Epoch 1840, training loss: 311.46051025390625 = 0.034567080438137054 + 50.0 * 6.228518486022949
Epoch 1840, val loss: 1.2774046659469604
Epoch 1850, training loss: 311.4200744628906 = 0.03391046077013016 + 50.0 * 6.227723598480225
Epoch 1850, val loss: 1.281087040901184
Epoch 1860, training loss: 311.46392822265625 = 0.033260881900787354 + 50.0 * 6.228613376617432
Epoch 1860, val loss: 1.2855123281478882
Epoch 1870, training loss: 311.4181213378906 = 0.032629694789648056 + 50.0 * 6.227709770202637
Epoch 1870, val loss: 1.289081335067749
Epoch 1880, training loss: 311.403564453125 = 0.03200782462954521 + 50.0 * 6.227430820465088
Epoch 1880, val loss: 1.2931907176971436
Epoch 1890, training loss: 311.4845275878906 = 0.03143126517534256 + 50.0 * 6.229062080383301
Epoch 1890, val loss: 1.2972036600112915
Epoch 1900, training loss: 311.5165710449219 = 0.030845684930682182 + 50.0 * 6.229714393615723
Epoch 1900, val loss: 1.3011001348495483
Epoch 1910, training loss: 311.3138427734375 = 0.030254146084189415 + 50.0 * 6.225671768188477
Epoch 1910, val loss: 1.3048803806304932
Epoch 1920, training loss: 311.2663879394531 = 0.02971196547150612 + 50.0 * 6.224733352661133
Epoch 1920, val loss: 1.3086998462677002
Epoch 1930, training loss: 311.30157470703125 = 0.02918691374361515 + 50.0 * 6.225448131561279
Epoch 1930, val loss: 1.3127936124801636
Epoch 1940, training loss: 311.4630126953125 = 0.028669876977801323 + 50.0 * 6.228687286376953
Epoch 1940, val loss: 1.3163182735443115
Epoch 1950, training loss: 311.30548095703125 = 0.028141168877482414 + 50.0 * 6.225546836853027
Epoch 1950, val loss: 1.3201552629470825
Epoch 1960, training loss: 311.4162292480469 = 0.02765880525112152 + 50.0 * 6.227771759033203
Epoch 1960, val loss: 1.3239308595657349
Epoch 1970, training loss: 311.19818115234375 = 0.027156973257660866 + 50.0 * 6.223420143127441
Epoch 1970, val loss: 1.3275973796844482
Epoch 1980, training loss: 311.1925048828125 = 0.02668618969619274 + 50.0 * 6.223316669464111
Epoch 1980, val loss: 1.3315058946609497
Epoch 1990, training loss: 311.22991943359375 = 0.026237696409225464 + 50.0 * 6.22407341003418
Epoch 1990, val loss: 1.3353512287139893
Epoch 2000, training loss: 311.3574523925781 = 0.025798024609684944 + 50.0 * 6.226633071899414
Epoch 2000, val loss: 1.3385919332504272
Epoch 2010, training loss: 311.1688537597656 = 0.0253425445407629 + 50.0 * 6.222870349884033
Epoch 2010, val loss: 1.3420945405960083
Epoch 2020, training loss: 311.16619873046875 = 0.024918267503380775 + 50.0 * 6.222825527191162
Epoch 2020, val loss: 1.3458013534545898
Epoch 2030, training loss: 311.3990783691406 = 0.0245195422321558 + 50.0 * 6.22749137878418
Epoch 2030, val loss: 1.3491674661636353
Epoch 2040, training loss: 311.19940185546875 = 0.024103986099362373 + 50.0 * 6.223505973815918
Epoch 2040, val loss: 1.3529331684112549
Epoch 2050, training loss: 311.17108154296875 = 0.023701393976807594 + 50.0 * 6.222947597503662
Epoch 2050, val loss: 1.3562300205230713
Epoch 2060, training loss: 311.0931091308594 = 0.023318665102124214 + 50.0 * 6.221395969390869
Epoch 2060, val loss: 1.3598403930664062
Epoch 2070, training loss: 311.264892578125 = 0.022950777783989906 + 50.0 * 6.224838733673096
Epoch 2070, val loss: 1.363059639930725
Epoch 2080, training loss: 311.0644226074219 = 0.02256319299340248 + 50.0 * 6.220837116241455
Epoch 2080, val loss: 1.3664289712905884
Epoch 2090, training loss: 311.0460510253906 = 0.022202424705028534 + 50.0 * 6.2204766273498535
Epoch 2090, val loss: 1.3698786497116089
Epoch 2100, training loss: 311.157470703125 = 0.021862342953681946 + 50.0 * 6.222712516784668
Epoch 2100, val loss: 1.373247742652893
Epoch 2110, training loss: 311.0860900878906 = 0.02152031660079956 + 50.0 * 6.221291542053223
Epoch 2110, val loss: 1.3766118288040161
Epoch 2120, training loss: 311.0666198730469 = 0.021187476813793182 + 50.0 * 6.2209086418151855
Epoch 2120, val loss: 1.3798131942749023
Epoch 2130, training loss: 311.1289978027344 = 0.020864561200141907 + 50.0 * 6.22216272354126
Epoch 2130, val loss: 1.383354902267456
Epoch 2140, training loss: 311.0631103515625 = 0.02053341269493103 + 50.0 * 6.220851421356201
Epoch 2140, val loss: 1.386365294456482
Epoch 2150, training loss: 311.17327880859375 = 0.020217394456267357 + 50.0 * 6.223061561584473
Epoch 2150, val loss: 1.3896557092666626
Epoch 2160, training loss: 311.002685546875 = 0.019921235740184784 + 50.0 * 6.2196550369262695
Epoch 2160, val loss: 1.3927193880081177
Epoch 2170, training loss: 310.9975891113281 = 0.019619591534137726 + 50.0 * 6.219559192657471
Epoch 2170, val loss: 1.395975947380066
Epoch 2180, training loss: 311.0636291503906 = 0.0193305853754282 + 50.0 * 6.22088623046875
Epoch 2180, val loss: 1.399272084236145
Epoch 2190, training loss: 311.0081787109375 = 0.01904677227139473 + 50.0 * 6.219782829284668
Epoch 2190, val loss: 1.402161717414856
Epoch 2200, training loss: 310.9492492675781 = 0.01876804232597351 + 50.0 * 6.21860933303833
Epoch 2200, val loss: 1.405207872390747
Epoch 2210, training loss: 310.9875793457031 = 0.018499111756682396 + 50.0 * 6.219381809234619
Epoch 2210, val loss: 1.408147931098938
Epoch 2220, training loss: 311.2946472167969 = 0.0182485431432724 + 50.0 * 6.225528240203857
Epoch 2220, val loss: 1.4110362529754639
Epoch 2230, training loss: 311.02484130859375 = 0.01796799525618553 + 50.0 * 6.220137596130371
Epoch 2230, val loss: 1.4142905473709106
Epoch 2240, training loss: 310.9506530761719 = 0.01770772598683834 + 50.0 * 6.218658924102783
Epoch 2240, val loss: 1.416800618171692
Epoch 2250, training loss: 311.08154296875 = 0.017458349466323853 + 50.0 * 6.2212815284729
Epoch 2250, val loss: 1.4200397729873657
Epoch 2260, training loss: 310.89984130859375 = 0.017216484993696213 + 50.0 * 6.217652320861816
Epoch 2260, val loss: 1.4226717948913574
Epoch 2270, training loss: 311.0131530761719 = 0.016982752829790115 + 50.0 * 6.219923496246338
Epoch 2270, val loss: 1.4255772829055786
Epoch 2280, training loss: 310.84661865234375 = 0.016739532351493835 + 50.0 * 6.216597080230713
Epoch 2280, val loss: 1.4287304878234863
Epoch 2290, training loss: 310.8567810058594 = 0.01651577651500702 + 50.0 * 6.216805458068848
Epoch 2290, val loss: 1.4316678047180176
Epoch 2300, training loss: 311.1338806152344 = 0.016300873830914497 + 50.0 * 6.222351551055908
Epoch 2300, val loss: 1.4344302415847778
Epoch 2310, training loss: 310.92840576171875 = 0.01607828587293625 + 50.0 * 6.2182464599609375
Epoch 2310, val loss: 1.4367761611938477
Epoch 2320, training loss: 310.85052490234375 = 0.01585894636809826 + 50.0 * 6.21669340133667
Epoch 2320, val loss: 1.4399045705795288
Epoch 2330, training loss: 310.96258544921875 = 0.01565869152545929 + 50.0 * 6.21893835067749
Epoch 2330, val loss: 1.4425692558288574
Epoch 2340, training loss: 310.83319091796875 = 0.015443452633917332 + 50.0 * 6.216354846954346
Epoch 2340, val loss: 1.4453977346420288
Epoch 2350, training loss: 310.8778076171875 = 0.015243327245116234 + 50.0 * 6.217251300811768
Epoch 2350, val loss: 1.448006272315979
Epoch 2360, training loss: 310.85675048828125 = 0.015047094784677029 + 50.0 * 6.21683406829834
Epoch 2360, val loss: 1.4504361152648926
Epoch 2370, training loss: 310.8887023925781 = 0.014855194836854935 + 50.0 * 6.217476844787598
Epoch 2370, val loss: 1.4530586004257202
Epoch 2380, training loss: 310.9422912597656 = 0.014670037664473057 + 50.0 * 6.218552112579346
Epoch 2380, val loss: 1.4558745622634888
Epoch 2390, training loss: 310.86749267578125 = 0.014475292526185513 + 50.0 * 6.217060565948486
Epoch 2390, val loss: 1.4584637880325317
Epoch 2400, training loss: 310.75958251953125 = 0.01428543496876955 + 50.0 * 6.214905738830566
Epoch 2400, val loss: 1.4610477685928345
Epoch 2410, training loss: 310.7685852050781 = 0.014111493714153767 + 50.0 * 6.215089797973633
Epoch 2410, val loss: 1.4634822607040405
Epoch 2420, training loss: 310.80560302734375 = 0.013940654695034027 + 50.0 * 6.2158331871032715
Epoch 2420, val loss: 1.466246247291565
Epoch 2430, training loss: 310.783203125 = 0.013770650140941143 + 50.0 * 6.215388774871826
Epoch 2430, val loss: 1.4688035249710083
Epoch 2440, training loss: 310.74005126953125 = 0.013600815087556839 + 50.0 * 6.214529037475586
Epoch 2440, val loss: 1.4712493419647217
Epoch 2450, training loss: 310.89117431640625 = 0.013444793410599232 + 50.0 * 6.217554092407227
Epoch 2450, val loss: 1.473315715789795
Epoch 2460, training loss: 310.77001953125 = 0.013275825418531895 + 50.0 * 6.215135097503662
Epoch 2460, val loss: 1.475825309753418
Epoch 2470, training loss: 310.71234130859375 = 0.013109510764479637 + 50.0 * 6.213984489440918
Epoch 2470, val loss: 1.4784501791000366
Epoch 2480, training loss: 310.7173767089844 = 0.012964676134288311 + 50.0 * 6.214088439941406
Epoch 2480, val loss: 1.4807868003845215
Epoch 2490, training loss: 310.8918151855469 = 0.01281401701271534 + 50.0 * 6.2175798416137695
Epoch 2490, val loss: 1.4832090139389038
Epoch 2500, training loss: 310.71734619140625 = 0.012651152908802032 + 50.0 * 6.2140936851501465
Epoch 2500, val loss: 1.4853194952011108
Epoch 2510, training loss: 310.738037109375 = 0.012510010041296482 + 50.0 * 6.214510440826416
Epoch 2510, val loss: 1.4878045320510864
Epoch 2520, training loss: 310.7132873535156 = 0.012363051064312458 + 50.0 * 6.21401834487915
Epoch 2520, val loss: 1.4899004697799683
Epoch 2530, training loss: 310.92218017578125 = 0.012222631834447384 + 50.0 * 6.218198776245117
Epoch 2530, val loss: 1.4923237562179565
Epoch 2540, training loss: 310.7019958496094 = 0.012078353203833103 + 50.0 * 6.2137980461120605
Epoch 2540, val loss: 1.4945074319839478
Epoch 2550, training loss: 310.5929260253906 = 0.01193613838404417 + 50.0 * 6.211619853973389
Epoch 2550, val loss: 1.4969335794448853
Epoch 2560, training loss: 310.5821838378906 = 0.011806425638496876 + 50.0 * 6.211407661437988
Epoch 2560, val loss: 1.499132513999939
Epoch 2570, training loss: 310.7463073730469 = 0.011682202108204365 + 50.0 * 6.214692115783691
Epoch 2570, val loss: 1.5015792846679688
Epoch 2580, training loss: 310.6690979003906 = 0.011550852097570896 + 50.0 * 6.213150978088379
Epoch 2580, val loss: 1.5034979581832886
Epoch 2590, training loss: 310.6177673339844 = 0.011415943503379822 + 50.0 * 6.212127208709717
Epoch 2590, val loss: 1.5056393146514893
Epoch 2600, training loss: 310.60791015625 = 0.011291413567960262 + 50.0 * 6.21193265914917
Epoch 2600, val loss: 1.5077482461929321
Epoch 2610, training loss: 310.5838317871094 = 0.011168924160301685 + 50.0 * 6.211452960968018
Epoch 2610, val loss: 1.510032296180725
Epoch 2620, training loss: 310.6786193847656 = 0.011052599176764488 + 50.0 * 6.213351249694824
Epoch 2620, val loss: 1.512002944946289
Epoch 2630, training loss: 310.55743408203125 = 0.010935514234006405 + 50.0 * 6.210930347442627
Epoch 2630, val loss: 1.513971209526062
Epoch 2640, training loss: 310.7197570800781 = 0.010826497338712215 + 50.0 * 6.214178562164307
Epoch 2640, val loss: 1.5158741474151611
Epoch 2650, training loss: 310.6399230957031 = 0.010704870335757732 + 50.0 * 6.212584495544434
Epoch 2650, val loss: 1.5179346799850464
Epoch 2660, training loss: 310.5905456542969 = 0.01059176679700613 + 50.0 * 6.211599349975586
Epoch 2660, val loss: 1.5200196504592896
Epoch 2670, training loss: 310.6100769042969 = 0.010484796017408371 + 50.0 * 6.211991310119629
Epoch 2670, val loss: 1.5222595930099487
Epoch 2680, training loss: 310.55938720703125 = 0.010372043587267399 + 50.0 * 6.21097993850708
Epoch 2680, val loss: 1.524152398109436
Epoch 2690, training loss: 310.5014953613281 = 0.010261811316013336 + 50.0 * 6.209825038909912
Epoch 2690, val loss: 1.5260728597640991
Epoch 2700, training loss: 310.53277587890625 = 0.010161668993532658 + 50.0 * 6.210452556610107
Epoch 2700, val loss: 1.5282351970672607
Epoch 2710, training loss: 310.63916015625 = 0.01005867775529623 + 50.0 * 6.212581634521484
Epoch 2710, val loss: 1.5302999019622803
Epoch 2720, training loss: 310.4833984375 = 0.009956421330571175 + 50.0 * 6.209468841552734
Epoch 2720, val loss: 1.5319592952728271
Epoch 2730, training loss: 310.57464599609375 = 0.009861511178314686 + 50.0 * 6.2112956047058105
Epoch 2730, val loss: 1.53378164768219
Epoch 2740, training loss: 310.7039794921875 = 0.009761307388544083 + 50.0 * 6.213884353637695
Epoch 2740, val loss: 1.535667061805725
Epoch 2750, training loss: 310.4839172363281 = 0.009663360193371773 + 50.0 * 6.209484577178955
Epoch 2750, val loss: 1.5372493267059326
Epoch 2760, training loss: 310.45098876953125 = 0.009566150605678558 + 50.0 * 6.208828449249268
Epoch 2760, val loss: 1.539334774017334
Epoch 2770, training loss: 310.654541015625 = 0.00947742722928524 + 50.0 * 6.2129011154174805
Epoch 2770, val loss: 1.5413752794265747
Epoch 2780, training loss: 310.474365234375 = 0.009383633732795715 + 50.0 * 6.2093000411987305
Epoch 2780, val loss: 1.5426294803619385
Epoch 2790, training loss: 310.45257568359375 = 0.00929186213761568 + 50.0 * 6.208866119384766
Epoch 2790, val loss: 1.5446237325668335
Epoch 2800, training loss: 310.4613342285156 = 0.009202365763485432 + 50.0 * 6.209042072296143
Epoch 2800, val loss: 1.546621561050415
Epoch 2810, training loss: 310.6319274902344 = 0.009116078726947308 + 50.0 * 6.212456226348877
Epoch 2810, val loss: 1.548048973083496
Epoch 2820, training loss: 310.4344787597656 = 0.00903161708265543 + 50.0 * 6.2085089683532715
Epoch 2820, val loss: 1.5498920679092407
Epoch 2830, training loss: 310.3883361816406 = 0.008947353810071945 + 50.0 * 6.207587718963623
Epoch 2830, val loss: 1.5516269207000732
Epoch 2840, training loss: 310.4358825683594 = 0.008865946903824806 + 50.0 * 6.208539962768555
Epoch 2840, val loss: 1.5533089637756348
Epoch 2850, training loss: 310.5519104003906 = 0.00878954492509365 + 50.0 * 6.210862159729004
Epoch 2850, val loss: 1.5551427602767944
Epoch 2860, training loss: 310.54461669921875 = 0.008706621825695038 + 50.0 * 6.210718154907227
Epoch 2860, val loss: 1.5568612813949585
Epoch 2870, training loss: 310.35400390625 = 0.008621680550277233 + 50.0 * 6.206907272338867
Epoch 2870, val loss: 1.5581423044204712
Epoch 2880, training loss: 310.3694152832031 = 0.008543779142200947 + 50.0 * 6.207217216491699
Epoch 2880, val loss: 1.5599682331085205
Epoch 2890, training loss: 310.43035888671875 = 0.008469651453197002 + 50.0 * 6.208437442779541
Epoch 2890, val loss: 1.5616382360458374
Epoch 2900, training loss: 310.4832763671875 = 0.008395424112677574 + 50.0 * 6.209497928619385
Epoch 2900, val loss: 1.5629266500473022
Epoch 2910, training loss: 310.51385498046875 = 0.008325236849486828 + 50.0 * 6.210110664367676
Epoch 2910, val loss: 1.5643751621246338
Epoch 2920, training loss: 310.3258972167969 = 0.008246118202805519 + 50.0 * 6.206353187561035
Epoch 2920, val loss: 1.5662963390350342
Epoch 2930, training loss: 310.3424987792969 = 0.0081782266497612 + 50.0 * 6.206686496734619
Epoch 2930, val loss: 1.5678142309188843
Epoch 2940, training loss: 310.61492919921875 = 0.008113912306725979 + 50.0 * 6.212136268615723
Epoch 2940, val loss: 1.569116473197937
Epoch 2950, training loss: 310.41632080078125 = 0.008037611842155457 + 50.0 * 6.208166122436523
Epoch 2950, val loss: 1.5709141492843628
Epoch 2960, training loss: 310.3671569824219 = 0.007962973788380623 + 50.0 * 6.207183837890625
Epoch 2960, val loss: 1.5721776485443115
Epoch 2970, training loss: 310.28399658203125 = 0.007898096926510334 + 50.0 * 6.205522060394287
Epoch 2970, val loss: 1.5738732814788818
Epoch 2980, training loss: 310.27716064453125 = 0.007833450101315975 + 50.0 * 6.205386638641357
Epoch 2980, val loss: 1.5754034519195557
Epoch 2990, training loss: 310.7027282714844 = 0.007775258272886276 + 50.0 * 6.213898658752441
Epoch 2990, val loss: 1.576604962348938
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6777777777777778
0.819715340010543
=== training gcn model ===
Epoch 0, training loss: 431.8001708984375 = 1.9572051763534546 + 50.0 * 8.596858978271484
Epoch 0, val loss: 1.960855484008789
Epoch 10, training loss: 431.7580261230469 = 1.9476103782653809 + 50.0 * 8.596208572387695
Epoch 10, val loss: 1.951585054397583
Epoch 20, training loss: 431.4916076660156 = 1.935577154159546 + 50.0 * 8.591120719909668
Epoch 20, val loss: 1.9396061897277832
Epoch 30, training loss: 429.4850769042969 = 1.9195044040679932 + 50.0 * 8.551311492919922
Epoch 30, val loss: 1.9231750965118408
Epoch 40, training loss: 415.49224853515625 = 1.9002439975738525 + 50.0 * 8.27184009552002
Epoch 40, val loss: 1.904188871383667
Epoch 50, training loss: 383.213623046875 = 1.8806719779968262 + 50.0 * 7.626658916473389
Epoch 50, val loss: 1.8856626749038696
Epoch 60, training loss: 363.3517150878906 = 1.8673570156097412 + 50.0 * 7.229686737060547
Epoch 60, val loss: 1.8721274137496948
Epoch 70, training loss: 353.594482421875 = 1.8553919792175293 + 50.0 * 7.0347819328308105
Epoch 70, val loss: 1.860095739364624
Epoch 80, training loss: 346.3786926269531 = 1.8449121713638306 + 50.0 * 6.8906755447387695
Epoch 80, val loss: 1.850051760673523
Epoch 90, training loss: 340.9883117675781 = 1.8353197574615479 + 50.0 * 6.783059597015381
Epoch 90, val loss: 1.8409240245819092
Epoch 100, training loss: 337.3132019042969 = 1.8259758949279785 + 50.0 * 6.709744453430176
Epoch 100, val loss: 1.8318352699279785
Epoch 110, training loss: 334.4002685546875 = 1.8169821500778198 + 50.0 * 6.651665687561035
Epoch 110, val loss: 1.8231233358383179
Epoch 120, training loss: 332.37689208984375 = 1.8081616163253784 + 50.0 * 6.611374378204346
Epoch 120, val loss: 1.8144569396972656
Epoch 130, training loss: 330.6883850097656 = 1.7994060516357422 + 50.0 * 6.577779293060303
Epoch 130, val loss: 1.8059008121490479
Epoch 140, training loss: 329.4281921386719 = 1.7906047105789185 + 50.0 * 6.552751541137695
Epoch 140, val loss: 1.7973214387893677
Epoch 150, training loss: 328.4141845703125 = 1.7815120220184326 + 50.0 * 6.532653331756592
Epoch 150, val loss: 1.7885465621948242
Epoch 160, training loss: 327.5967712402344 = 1.771973729133606 + 50.0 * 6.516496181488037
Epoch 160, val loss: 1.7794363498687744
Epoch 170, training loss: 326.89215087890625 = 1.7619520425796509 + 50.0 * 6.502603530883789
Epoch 170, val loss: 1.7700122594833374
Epoch 180, training loss: 326.43804931640625 = 1.7512439489364624 + 50.0 * 6.4937357902526855
Epoch 180, val loss: 1.7600481510162354
Epoch 190, training loss: 325.6356201171875 = 1.7397534847259521 + 50.0 * 6.477917671203613
Epoch 190, val loss: 1.7494348287582397
Epoch 200, training loss: 325.138671875 = 1.7274123430252075 + 50.0 * 6.468225002288818
Epoch 200, val loss: 1.7381664514541626
Epoch 210, training loss: 324.62841796875 = 1.7140302658081055 + 50.0 * 6.458288192749023
Epoch 210, val loss: 1.726116418838501
Epoch 220, training loss: 324.0054931640625 = 1.6996171474456787 + 50.0 * 6.446117401123047
Epoch 220, val loss: 1.7131308317184448
Epoch 230, training loss: 323.5479431152344 = 1.6840544939041138 + 50.0 * 6.437277793884277
Epoch 230, val loss: 1.6991897821426392
Epoch 240, training loss: 323.33135986328125 = 1.6672194004058838 + 50.0 * 6.433282852172852
Epoch 240, val loss: 1.6842694282531738
Epoch 250, training loss: 322.7736511230469 = 1.6489733457565308 + 50.0 * 6.4224934577941895
Epoch 250, val loss: 1.6681561470031738
Epoch 260, training loss: 322.28375244140625 = 1.6296216249465942 + 50.0 * 6.413083076477051
Epoch 260, val loss: 1.6512118577957153
Epoch 270, training loss: 321.899169921875 = 1.6090346574783325 + 50.0 * 6.4058027267456055
Epoch 270, val loss: 1.6333420276641846
Epoch 280, training loss: 321.5590515136719 = 1.5872145891189575 + 50.0 * 6.3994364738464355
Epoch 280, val loss: 1.6146107912063599
Epoch 290, training loss: 321.5898132324219 = 1.5641422271728516 + 50.0 * 6.400513172149658
Epoch 290, val loss: 1.594915509223938
Epoch 300, training loss: 320.9562683105469 = 1.5399678945541382 + 50.0 * 6.3883256912231445
Epoch 300, val loss: 1.574556589126587
Epoch 310, training loss: 320.68927001953125 = 1.5149763822555542 + 50.0 * 6.383485794067383
Epoch 310, val loss: 1.5538203716278076
Epoch 320, training loss: 320.3721923828125 = 1.4893616437911987 + 50.0 * 6.377656936645508
Epoch 320, val loss: 1.5327801704406738
Epoch 330, training loss: 320.24365234375 = 1.4630100727081299 + 50.0 * 6.375612735748291
Epoch 330, val loss: 1.5114426612854004
Epoch 340, training loss: 319.915283203125 = 1.436267375946045 + 50.0 * 6.369580268859863
Epoch 340, val loss: 1.4899561405181885
Epoch 350, training loss: 319.63763427734375 = 1.4092501401901245 + 50.0 * 6.364567756652832
Epoch 350, val loss: 1.468609094619751
Epoch 360, training loss: 319.594482421875 = 1.3821702003479004 + 50.0 * 6.364246368408203
Epoch 360, val loss: 1.4475046396255493
Epoch 370, training loss: 319.2854919433594 = 1.3548293113708496 + 50.0 * 6.358613014221191
Epoch 370, val loss: 1.4264013767242432
Epoch 380, training loss: 319.11700439453125 = 1.3276842832565308 + 50.0 * 6.355785846710205
Epoch 380, val loss: 1.4056403636932373
Epoch 390, training loss: 318.78515625 = 1.3006925582885742 + 50.0 * 6.349689483642578
Epoch 390, val loss: 1.3853306770324707
Epoch 400, training loss: 318.6683044433594 = 1.274009108543396 + 50.0 * 6.347885608673096
Epoch 400, val loss: 1.365468978881836
Epoch 410, training loss: 318.5963134765625 = 1.2475730180740356 + 50.0 * 6.346975326538086
Epoch 410, val loss: 1.3458467721939087
Epoch 420, training loss: 318.3028869628906 = 1.221477746963501 + 50.0 * 6.341628551483154
Epoch 420, val loss: 1.326963186264038
Epoch 430, training loss: 318.0711364746094 = 1.1958781480789185 + 50.0 * 6.337504863739014
Epoch 430, val loss: 1.3085763454437256
Epoch 440, training loss: 317.9715270996094 = 1.170827031135559 + 50.0 * 6.3360137939453125
Epoch 440, val loss: 1.2906866073608398
Epoch 450, training loss: 318.0732421875 = 1.1462152004241943 + 50.0 * 6.338540554046631
Epoch 450, val loss: 1.2732561826705933
Epoch 460, training loss: 317.66607666015625 = 1.1219167709350586 + 50.0 * 6.330883026123047
Epoch 460, val loss: 1.2563823461532593
Epoch 470, training loss: 317.46307373046875 = 1.0983986854553223 + 50.0 * 6.327293872833252
Epoch 470, val loss: 1.240217685699463
Epoch 480, training loss: 317.2694091796875 = 1.0755659341812134 + 50.0 * 6.323876857757568
Epoch 480, val loss: 1.2247618436813354
Epoch 490, training loss: 317.47393798828125 = 1.0533251762390137 + 50.0 * 6.3284125328063965
Epoch 490, val loss: 1.2099616527557373
Epoch 500, training loss: 317.1773376464844 = 1.0314526557922363 + 50.0 * 6.322917461395264
Epoch 500, val loss: 1.19553804397583
Epoch 510, training loss: 316.9267578125 = 1.010144591331482 + 50.0 * 6.318332672119141
Epoch 510, val loss: 1.1818249225616455
Epoch 520, training loss: 316.7751159667969 = 0.9894983172416687 + 50.0 * 6.3157124519348145
Epoch 520, val loss: 1.1687802076339722
Epoch 530, training loss: 316.6351013183594 = 0.9695155024528503 + 50.0 * 6.31331205368042
Epoch 530, val loss: 1.156558632850647
Epoch 540, training loss: 316.47894287109375 = 0.9500905871391296 + 50.0 * 6.310576915740967
Epoch 540, val loss: 1.1449737548828125
Epoch 550, training loss: 316.5623779296875 = 0.9311294555664062 + 50.0 * 6.312625408172607
Epoch 550, val loss: 1.1339380741119385
Epoch 560, training loss: 316.6371765136719 = 0.9125118851661682 + 50.0 * 6.314493179321289
Epoch 560, val loss: 1.1232811212539673
Epoch 570, training loss: 316.37225341796875 = 0.8943172097206116 + 50.0 * 6.309558868408203
Epoch 570, val loss: 1.1131491661071777
Epoch 580, training loss: 316.0965270996094 = 0.8766428828239441 + 50.0 * 6.3043975830078125
Epoch 580, val loss: 1.1037665605545044
Epoch 590, training loss: 315.9465637207031 = 0.8594331741333008 + 50.0 * 6.3017425537109375
Epoch 590, val loss: 1.095036268234253
Epoch 600, training loss: 316.02789306640625 = 0.8426097631454468 + 50.0 * 6.30370569229126
Epoch 600, val loss: 1.0867470502853394
Epoch 610, training loss: 315.8371276855469 = 0.8260450959205627 + 50.0 * 6.3002214431762695
Epoch 610, val loss: 1.0788487195968628
Epoch 620, training loss: 315.6570129394531 = 0.8097689747810364 + 50.0 * 6.296944618225098
Epoch 620, val loss: 1.0713181495666504
Epoch 630, training loss: 315.715576171875 = 0.7938686013221741 + 50.0 * 6.298434734344482
Epoch 630, val loss: 1.0643755197525024
Epoch 640, training loss: 315.5643310546875 = 0.7781324982643127 + 50.0 * 6.295723915100098
Epoch 640, val loss: 1.0576587915420532
Epoch 650, training loss: 315.5914306640625 = 0.7626688480377197 + 50.0 * 6.29657506942749
Epoch 650, val loss: 1.0514250993728638
Epoch 660, training loss: 315.27325439453125 = 0.7474258542060852 + 50.0 * 6.290516376495361
Epoch 660, val loss: 1.0454503297805786
Epoch 670, training loss: 315.2420654296875 = 0.7325398921966553 + 50.0 * 6.29019021987915
Epoch 670, val loss: 1.040160059928894
Epoch 680, training loss: 315.1297912597656 = 0.7178945541381836 + 50.0 * 6.288238048553467
Epoch 680, val loss: 1.035128116607666
Epoch 690, training loss: 315.3748474121094 = 0.7034677863121033 + 50.0 * 6.29342794418335
Epoch 690, val loss: 1.030458927154541
Epoch 700, training loss: 315.0290222167969 = 0.6890973448753357 + 50.0 * 6.28679895401001
Epoch 700, val loss: 1.0260255336761475
Epoch 710, training loss: 314.9725646972656 = 0.6750231385231018 + 50.0 * 6.285951137542725
Epoch 710, val loss: 1.021899700164795
Epoch 720, training loss: 314.8650207519531 = 0.6612177491188049 + 50.0 * 6.284075736999512
Epoch 720, val loss: 1.0181928873062134
Epoch 730, training loss: 314.9234313964844 = 0.6476535201072693 + 50.0 * 6.285515785217285
Epoch 730, val loss: 1.0148391723632812
Epoch 740, training loss: 314.74188232421875 = 0.6342615485191345 + 50.0 * 6.2821526527404785
Epoch 740, val loss: 1.0118907690048218
Epoch 750, training loss: 314.6221008300781 = 0.6210904717445374 + 50.0 * 6.280020236968994
Epoch 750, val loss: 1.0092445611953735
Epoch 760, training loss: 314.6524658203125 = 0.6081632375717163 + 50.0 * 6.280886173248291
Epoch 760, val loss: 1.006917953491211
Epoch 770, training loss: 314.50762939453125 = 0.5953838229179382 + 50.0 * 6.278244495391846
Epoch 770, val loss: 1.0047639608383179
Epoch 780, training loss: 314.4006652832031 = 0.5828644037246704 + 50.0 * 6.276356220245361
Epoch 780, val loss: 1.0030332803726196
Epoch 790, training loss: 314.5764465332031 = 0.5706424117088318 + 50.0 * 6.280116081237793
Epoch 790, val loss: 1.001770257949829
Epoch 800, training loss: 314.3367919921875 = 0.558474600315094 + 50.0 * 6.275566577911377
Epoch 800, val loss: 1.0004937648773193
Epoch 810, training loss: 314.263427734375 = 0.546623170375824 + 50.0 * 6.274335861206055
Epoch 810, val loss: 0.9996609091758728
Epoch 820, training loss: 314.1905822753906 = 0.5350889563560486 + 50.0 * 6.2731099128723145
Epoch 820, val loss: 0.9992032647132874
Epoch 830, training loss: 314.173828125 = 0.5237418413162231 + 50.0 * 6.273001670837402
Epoch 830, val loss: 0.9989621639251709
Epoch 840, training loss: 314.1239318847656 = 0.512640118598938 + 50.0 * 6.272225856781006
Epoch 840, val loss: 0.9990454912185669
Epoch 850, training loss: 314.1902160644531 = 0.5016878247261047 + 50.0 * 6.273770809173584
Epoch 850, val loss: 0.9991394281387329
Epoch 860, training loss: 314.02587890625 = 0.4909619987010956 + 50.0 * 6.270698070526123
Epoch 860, val loss: 0.9998277425765991
Epoch 870, training loss: 313.8147277832031 = 0.48044151067733765 + 50.0 * 6.266685962677002
Epoch 870, val loss: 1.0005455017089844
Epoch 880, training loss: 313.7892761230469 = 0.4702368378639221 + 50.0 * 6.266380786895752
Epoch 880, val loss: 1.0014687776565552
Epoch 890, training loss: 314.1523132324219 = 0.4602205157279968 + 50.0 * 6.273841857910156
Epoch 890, val loss: 1.0029022693634033
Epoch 900, training loss: 313.8360900878906 = 0.4503597915172577 + 50.0 * 6.267714977264404
Epoch 900, val loss: 1.0043165683746338
Epoch 910, training loss: 313.7138366699219 = 0.4406534433364868 + 50.0 * 6.265463829040527
Epoch 910, val loss: 1.0059303045272827
Epoch 920, training loss: 313.7352294921875 = 0.4312579333782196 + 50.0 * 6.266079902648926
Epoch 920, val loss: 1.0079710483551025
Epoch 930, training loss: 313.7765808105469 = 0.42193037271499634 + 50.0 * 6.267093181610107
Epoch 930, val loss: 1.0097652673721313
Epoch 940, training loss: 313.5478210449219 = 0.41282060742378235 + 50.0 * 6.262700080871582
Epoch 940, val loss: 1.0123140811920166
Epoch 950, training loss: 313.42816162109375 = 0.4038904011249542 + 50.0 * 6.2604851722717285
Epoch 950, val loss: 1.014655351638794
Epoch 960, training loss: 313.3835144042969 = 0.3952314257621765 + 50.0 * 6.259765625
Epoch 960, val loss: 1.017412781715393
Epoch 970, training loss: 313.73004150390625 = 0.3867324888706207 + 50.0 * 6.266866207122803
Epoch 970, val loss: 1.0203361511230469
Epoch 980, training loss: 313.4633483886719 = 0.3782396912574768 + 50.0 * 6.261702537536621
Epoch 980, val loss: 1.0229555368423462
Epoch 990, training loss: 313.4664306640625 = 0.3700001537799835 + 50.0 * 6.261928558349609
Epoch 990, val loss: 1.0259069204330444
Epoch 1000, training loss: 313.25634765625 = 0.3619203269481659 + 50.0 * 6.2578887939453125
Epoch 1000, val loss: 1.0293059349060059
Epoch 1010, training loss: 313.50390625 = 0.35406720638275146 + 50.0 * 6.262996673583984
Epoch 1010, val loss: 1.0325819253921509
Epoch 1020, training loss: 313.221435546875 = 0.3462808430194855 + 50.0 * 6.257503032684326
Epoch 1020, val loss: 1.0359632968902588
Epoch 1030, training loss: 313.12298583984375 = 0.33870387077331543 + 50.0 * 6.255685329437256
Epoch 1030, val loss: 1.0396422147750854
Epoch 1040, training loss: 313.14501953125 = 0.33134832978248596 + 50.0 * 6.25627326965332
Epoch 1040, val loss: 1.0434843301773071
Epoch 1050, training loss: 313.2231140136719 = 0.3240474760532379 + 50.0 * 6.257981300354004
Epoch 1050, val loss: 1.0469788312911987
Epoch 1060, training loss: 313.0402526855469 = 0.31687435507774353 + 50.0 * 6.254467487335205
Epoch 1060, val loss: 1.0507328510284424
Epoch 1070, training loss: 313.0350341796875 = 0.3099387288093567 + 50.0 * 6.254501819610596
Epoch 1070, val loss: 1.0549774169921875
Epoch 1080, training loss: 313.06524658203125 = 0.3030838370323181 + 50.0 * 6.255243301391602
Epoch 1080, val loss: 1.0589407682418823
Epoch 1090, training loss: 312.8978576660156 = 0.29633086919784546 + 50.0 * 6.252030849456787
Epoch 1090, val loss: 1.0629884004592896
Epoch 1100, training loss: 312.8429260253906 = 0.28980475664138794 + 50.0 * 6.251062870025635
Epoch 1100, val loss: 1.0673918724060059
Epoch 1110, training loss: 313.09881591796875 = 0.28341642022132874 + 50.0 * 6.256308078765869
Epoch 1110, val loss: 1.071770191192627
Epoch 1120, training loss: 312.870849609375 = 0.27704957127571106 + 50.0 * 6.251876354217529
Epoch 1120, val loss: 1.0761942863464355
Epoch 1130, training loss: 312.78680419921875 = 0.2708691358566284 + 50.0 * 6.25031852722168
Epoch 1130, val loss: 1.0808988809585571
Epoch 1140, training loss: 312.8496398925781 = 0.2648349702358246 + 50.0 * 6.2516961097717285
Epoch 1140, val loss: 1.0854878425598145
Epoch 1150, training loss: 312.8934631347656 = 0.258883535861969 + 50.0 * 6.252691745758057
Epoch 1150, val loss: 1.090245008468628
Epoch 1160, training loss: 312.720947265625 = 0.2529585659503937 + 50.0 * 6.249359607696533
Epoch 1160, val loss: 1.0944828987121582
Epoch 1170, training loss: 312.6471862792969 = 0.2472764104604721 + 50.0 * 6.247997760772705
Epoch 1170, val loss: 1.099475622177124
Epoch 1180, training loss: 312.5750427246094 = 0.24172835052013397 + 50.0 * 6.246666431427002
Epoch 1180, val loss: 1.1045005321502686
Epoch 1190, training loss: 312.56634521484375 = 0.23632195591926575 + 50.0 * 6.24660062789917
Epoch 1190, val loss: 1.1094502210617065
Epoch 1200, training loss: 312.735595703125 = 0.2310028374195099 + 50.0 * 6.250091552734375
Epoch 1200, val loss: 1.1145737171173096
Epoch 1210, training loss: 312.72601318359375 = 0.22570446133613586 + 50.0 * 6.250006198883057
Epoch 1210, val loss: 1.1194349527359009
Epoch 1220, training loss: 312.6421813964844 = 0.2205132246017456 + 50.0 * 6.2484331130981445
Epoch 1220, val loss: 1.1243922710418701
Epoch 1230, training loss: 312.50494384765625 = 0.21545575559139252 + 50.0 * 6.245790004730225
Epoch 1230, val loss: 1.129332423210144
Epoch 1240, training loss: 312.540771484375 = 0.2105543613433838 + 50.0 * 6.2466044425964355
Epoch 1240, val loss: 1.1345250606536865
Epoch 1250, training loss: 312.57391357421875 = 0.20574675500392914 + 50.0 * 6.247363090515137
Epoch 1250, val loss: 1.1399258375167847
Epoch 1260, training loss: 312.3827819824219 = 0.2009756714105606 + 50.0 * 6.243635654449463
Epoch 1260, val loss: 1.144777536392212
Epoch 1270, training loss: 312.36334228515625 = 0.1963662952184677 + 50.0 * 6.2433390617370605
Epoch 1270, val loss: 1.150128722190857
Epoch 1280, training loss: 312.2849426269531 = 0.1918819397687912 + 50.0 * 6.241860866546631
Epoch 1280, val loss: 1.1555466651916504
Epoch 1290, training loss: 312.5876770019531 = 0.18749184906482697 + 50.0 * 6.2480034828186035
Epoch 1290, val loss: 1.1606926918029785
Epoch 1300, training loss: 312.3816223144531 = 0.18311139941215515 + 50.0 * 6.2439703941345215
Epoch 1300, val loss: 1.1658111810684204
Epoch 1310, training loss: 312.33306884765625 = 0.17885157465934753 + 50.0 * 6.243083953857422
Epoch 1310, val loss: 1.1712391376495361
Epoch 1320, training loss: 312.3770751953125 = 0.17469492554664612 + 50.0 * 6.24404764175415
Epoch 1320, val loss: 1.1762957572937012
Epoch 1330, training loss: 312.3370666503906 = 0.17063181102275848 + 50.0 * 6.24332857131958
Epoch 1330, val loss: 1.181924819946289
Epoch 1340, training loss: 312.2718200683594 = 0.16667009890079498 + 50.0 * 6.242103576660156
Epoch 1340, val loss: 1.187211513519287
Epoch 1350, training loss: 312.1435241699219 = 0.16276459395885468 + 50.0 * 6.239615440368652
Epoch 1350, val loss: 1.1925592422485352
Epoch 1360, training loss: 312.1390075683594 = 0.15898849070072174 + 50.0 * 6.23960018157959
Epoch 1360, val loss: 1.1979501247406006
Epoch 1370, training loss: 312.1009216308594 = 0.15530094504356384 + 50.0 * 6.238912105560303
Epoch 1370, val loss: 1.203412413597107
Epoch 1380, training loss: 312.3088073730469 = 0.1516975611448288 + 50.0 * 6.243142127990723
Epoch 1380, val loss: 1.2086987495422363
Epoch 1390, training loss: 312.2926330566406 = 0.14811941981315613 + 50.0 * 6.242889881134033
Epoch 1390, val loss: 1.2140060663223267
Epoch 1400, training loss: 312.15338134765625 = 0.14458537101745605 + 50.0 * 6.240175724029541
Epoch 1400, val loss: 1.218937635421753
Epoch 1410, training loss: 312.1778259277344 = 0.14117443561553955 + 50.0 * 6.2407331466674805
Epoch 1410, val loss: 1.2244300842285156
Epoch 1420, training loss: 312.0023498535156 = 0.13785423338413239 + 50.0 * 6.237289905548096
Epoch 1420, val loss: 1.2296406030654907
Epoch 1430, training loss: 311.9312744140625 = 0.1346459835767746 + 50.0 * 6.23593282699585
Epoch 1430, val loss: 1.2353620529174805
Epoch 1440, training loss: 311.8938293457031 = 0.13152310252189636 + 50.0 * 6.235246181488037
Epoch 1440, val loss: 1.2407066822052002
Epoch 1450, training loss: 312.1701965332031 = 0.12850207090377808 + 50.0 * 6.240833759307861
Epoch 1450, val loss: 1.2463185787200928
Epoch 1460, training loss: 312.4247131347656 = 0.1254275143146515 + 50.0 * 6.245985984802246
Epoch 1460, val loss: 1.2509363889694214
Epoch 1470, training loss: 312.0381164550781 = 0.12241660803556442 + 50.0 * 6.238314151763916
Epoch 1470, val loss: 1.2562565803527832
Epoch 1480, training loss: 311.830810546875 = 0.11952780932188034 + 50.0 * 6.234225749969482
Epoch 1480, val loss: 1.261845588684082
Epoch 1490, training loss: 311.8108215332031 = 0.1167583093047142 + 50.0 * 6.23388147354126
Epoch 1490, val loss: 1.267366647720337
Epoch 1500, training loss: 311.78436279296875 = 0.11406727135181427 + 50.0 * 6.233406066894531
Epoch 1500, val loss: 1.2729908227920532
Epoch 1510, training loss: 312.2841491699219 = 0.11144782602787018 + 50.0 * 6.2434539794921875
Epoch 1510, val loss: 1.2782659530639648
Epoch 1520, training loss: 312.11083984375 = 0.10878024250268936 + 50.0 * 6.2400407791137695
Epoch 1520, val loss: 1.2830663919448853
Epoch 1530, training loss: 311.84375 = 0.10619179159402847 + 50.0 * 6.234751224517822
Epoch 1530, val loss: 1.2888208627700806
Epoch 1540, training loss: 311.7027893066406 = 0.10370205342769623 + 50.0 * 6.2319817543029785
Epoch 1540, val loss: 1.2938945293426514
Epoch 1550, training loss: 311.7818298339844 = 0.10131300985813141 + 50.0 * 6.233610153198242
Epoch 1550, val loss: 1.299503207206726
Epoch 1560, training loss: 311.94140625 = 0.09897195547819138 + 50.0 * 6.236848831176758
Epoch 1560, val loss: 1.3046106100082397
Epoch 1570, training loss: 311.86578369140625 = 0.09665725380182266 + 50.0 * 6.235382556915283
Epoch 1570, val loss: 1.310179591178894
Epoch 1580, training loss: 311.68048095703125 = 0.09440886974334717 + 50.0 * 6.2317214012146
Epoch 1580, val loss: 1.315400242805481
Epoch 1590, training loss: 311.7060852050781 = 0.09223590046167374 + 50.0 * 6.2322773933410645
Epoch 1590, val loss: 1.3205788135528564
Epoch 1600, training loss: 311.8836975097656 = 0.09010229259729385 + 50.0 * 6.235872268676758
Epoch 1600, val loss: 1.3257758617401123
Epoch 1610, training loss: 311.71551513671875 = 0.08801242709159851 + 50.0 * 6.232550144195557
Epoch 1610, val loss: 1.331308364868164
Epoch 1620, training loss: 311.6461181640625 = 0.08599338680505753 + 50.0 * 6.231202125549316
Epoch 1620, val loss: 1.3365087509155273
Epoch 1630, training loss: 311.6352233886719 = 0.08403324335813522 + 50.0 * 6.23102331161499
Epoch 1630, val loss: 1.3419047594070435
Epoch 1640, training loss: 311.7402038574219 = 0.08212696015834808 + 50.0 * 6.233161926269531
Epoch 1640, val loss: 1.3474429845809937
Epoch 1650, training loss: 311.734619140625 = 0.0802433118224144 + 50.0 * 6.23308801651001
Epoch 1650, val loss: 1.3524965047836304
Epoch 1660, training loss: 311.57061767578125 = 0.07839713990688324 + 50.0 * 6.229844093322754
Epoch 1660, val loss: 1.357721209526062
Epoch 1670, training loss: 311.5160217285156 = 0.07663891464471817 + 50.0 * 6.228787899017334
Epoch 1670, val loss: 1.3628389835357666
Epoch 1680, training loss: 311.6068115234375 = 0.07493584603071213 + 50.0 * 6.230637550354004
Epoch 1680, val loss: 1.368257761001587
Epoch 1690, training loss: 311.69537353515625 = 0.07324755191802979 + 50.0 * 6.232442855834961
Epoch 1690, val loss: 1.373552918434143
Epoch 1700, training loss: 311.5602111816406 = 0.07158035784959793 + 50.0 * 6.229772567749023
Epoch 1700, val loss: 1.3783702850341797
Epoch 1710, training loss: 311.5108947753906 = 0.06998226791620255 + 50.0 * 6.228818416595459
Epoch 1710, val loss: 1.3838952779769897
Epoch 1720, training loss: 311.42529296875 = 0.06843782216310501 + 50.0 * 6.227136611938477
Epoch 1720, val loss: 1.3891634941101074
Epoch 1730, training loss: 311.4886474609375 = 0.0669563040137291 + 50.0 * 6.228434085845947
Epoch 1730, val loss: 1.3945131301879883
Epoch 1740, training loss: 311.70037841796875 = 0.06550117582082748 + 50.0 * 6.232697486877441
Epoch 1740, val loss: 1.3998148441314697
Epoch 1750, training loss: 311.4820251464844 = 0.06403930485248566 + 50.0 * 6.228359699249268
Epoch 1750, val loss: 1.4044408798217773
Epoch 1760, training loss: 311.51287841796875 = 0.06265834718942642 + 50.0 * 6.229004383087158
Epoch 1760, val loss: 1.4099979400634766
Epoch 1770, training loss: 311.4222106933594 = 0.061292242258787155 + 50.0 * 6.2272186279296875
Epoch 1770, val loss: 1.414576530456543
Epoch 1780, training loss: 311.3675842285156 = 0.0599827878177166 + 50.0 * 6.226151466369629
Epoch 1780, val loss: 1.4198981523513794
Epoch 1790, training loss: 311.82708740234375 = 0.0587284192442894 + 50.0 * 6.235367298126221
Epoch 1790, val loss: 1.425060510635376
Epoch 1800, training loss: 311.4886169433594 = 0.05744672939181328 + 50.0 * 6.228623867034912
Epoch 1800, val loss: 1.4296982288360596
Epoch 1810, training loss: 311.3649597167969 = 0.05623449385166168 + 50.0 * 6.226174354553223
Epoch 1810, val loss: 1.4348089694976807
Epoch 1820, training loss: 311.3645324707031 = 0.05507343262434006 + 50.0 * 6.226188659667969
Epoch 1820, val loss: 1.4399398565292358
Epoch 1830, training loss: 311.482666015625 = 0.05393551290035248 + 50.0 * 6.228574752807617
Epoch 1830, val loss: 1.4447845220565796
Epoch 1840, training loss: 311.3170166015625 = 0.0528220571577549 + 50.0 * 6.225284099578857
Epoch 1840, val loss: 1.45000422000885
Epoch 1850, training loss: 311.3055114746094 = 0.051747921854257584 + 50.0 * 6.2250752449035645
Epoch 1850, val loss: 1.4550963640213013
Epoch 1860, training loss: 311.7666320800781 = 0.05070088058710098 + 50.0 * 6.234318733215332
Epoch 1860, val loss: 1.459721326828003
Epoch 1870, training loss: 311.3211364746094 = 0.049624234437942505 + 50.0 * 6.225430011749268
Epoch 1870, val loss: 1.4643065929412842
Epoch 1880, training loss: 311.1921691894531 = 0.04861205816268921 + 50.0 * 6.222870826721191
Epoch 1880, val loss: 1.4693318605422974
Epoch 1890, training loss: 311.18377685546875 = 0.04765132814645767 + 50.0 * 6.22272253036499
Epoch 1890, val loss: 1.47445547580719
Epoch 1900, training loss: 311.5899963378906 = 0.046723395586013794 + 50.0 * 6.230865478515625
Epoch 1900, val loss: 1.479318618774414
Epoch 1910, training loss: 311.3102111816406 = 0.04576383903622627 + 50.0 * 6.2252888679504395
Epoch 1910, val loss: 1.4833773374557495
Epoch 1920, training loss: 311.2145690917969 = 0.044862642884254456 + 50.0 * 6.22339391708374
Epoch 1920, val loss: 1.4886976480484009
Epoch 1930, training loss: 311.2024230957031 = 0.043992117047309875 + 50.0 * 6.22316837310791
Epoch 1930, val loss: 1.4935132265090942
Epoch 1940, training loss: 311.2593994140625 = 0.04314884915947914 + 50.0 * 6.224325180053711
Epoch 1940, val loss: 1.4982514381408691
Epoch 1950, training loss: 311.1341857910156 = 0.04230748489499092 + 50.0 * 6.221837997436523
Epoch 1950, val loss: 1.5027127265930176
Epoch 1960, training loss: 311.3384094238281 = 0.04150783643126488 + 50.0 * 6.225937843322754
Epoch 1960, val loss: 1.5072166919708252
Epoch 1970, training loss: 311.14697265625 = 0.04070260375738144 + 50.0 * 6.222125053405762
Epoch 1970, val loss: 1.5118921995162964
Epoch 1980, training loss: 311.1772766113281 = 0.03993631526827812 + 50.0 * 6.222746849060059
Epoch 1980, val loss: 1.5161370038986206
Epoch 1990, training loss: 311.3797912597656 = 0.039183951914310455 + 50.0 * 6.22681188583374
Epoch 1990, val loss: 1.5209120512008667
Epoch 2000, training loss: 311.20703125 = 0.03843148425221443 + 50.0 * 6.223372459411621
Epoch 2000, val loss: 1.5246319770812988
Epoch 2010, training loss: 311.2776794433594 = 0.037714459002017975 + 50.0 * 6.224799156188965
Epoch 2010, val loss: 1.5293574333190918
Epoch 2020, training loss: 311.0550842285156 = 0.0370202474296093 + 50.0 * 6.220361232757568
Epoch 2020, val loss: 1.5338702201843262
Epoch 2030, training loss: 311.0628356933594 = 0.03636079654097557 + 50.0 * 6.220530033111572
Epoch 2030, val loss: 1.5385942459106445
Epoch 2040, training loss: 311.13397216796875 = 0.03571847826242447 + 50.0 * 6.221965312957764
Epoch 2040, val loss: 1.5427087545394897
Epoch 2050, training loss: 311.1809997558594 = 0.03507939726114273 + 50.0 * 6.222918510437012
Epoch 2050, val loss: 1.5470106601715088
Epoch 2060, training loss: 311.0717468261719 = 0.03444256633520126 + 50.0 * 6.22074556350708
Epoch 2060, val loss: 1.55094575881958
Epoch 2070, training loss: 311.1080627441406 = 0.03383767232298851 + 50.0 * 6.221484184265137
Epoch 2070, val loss: 1.5547120571136475
Epoch 2080, training loss: 311.3284606933594 = 0.03324361518025398 + 50.0 * 6.22590446472168
Epoch 2080, val loss: 1.5593178272247314
Epoch 2090, training loss: 311.02020263671875 = 0.032646387815475464 + 50.0 * 6.219750881195068
Epoch 2090, val loss: 1.563096523284912
Epoch 2100, training loss: 310.950439453125 = 0.03208264335989952 + 50.0 * 6.218367099761963
Epoch 2100, val loss: 1.5674686431884766
Epoch 2110, training loss: 310.9502258300781 = 0.03154529258608818 + 50.0 * 6.2183732986450195
Epoch 2110, val loss: 1.5718328952789307
Epoch 2120, training loss: 311.15478515625 = 0.031025635078549385 + 50.0 * 6.222475051879883
Epoch 2120, val loss: 1.5757864713668823
Epoch 2130, training loss: 311.0027770996094 = 0.030484529212117195 + 50.0 * 6.219445705413818
Epoch 2130, val loss: 1.5797195434570312
Epoch 2140, training loss: 310.9947814941406 = 0.029960449784994125 + 50.0 * 6.219296455383301
Epoch 2140, val loss: 1.5829957723617554
Epoch 2150, training loss: 310.97479248046875 = 0.029462361708283424 + 50.0 * 6.218906879425049
Epoch 2150, val loss: 1.587664246559143
Epoch 2160, training loss: 310.9536437988281 = 0.02897426299750805 + 50.0 * 6.218493938446045
Epoch 2160, val loss: 1.5911582708358765
Epoch 2170, training loss: 311.2480773925781 = 0.028507458046078682 + 50.0 * 6.224391460418701
Epoch 2170, val loss: 1.5952112674713135
Epoch 2180, training loss: 310.9844970703125 = 0.028030909597873688 + 50.0 * 6.2191290855407715
Epoch 2180, val loss: 1.5990307331085205
Epoch 2190, training loss: 311.2072448730469 = 0.02757084183394909 + 50.0 * 6.223593711853027
Epoch 2190, val loss: 1.6027427911758423
Epoch 2200, training loss: 310.8888244628906 = 0.02711494453251362 + 50.0 * 6.217233657836914
Epoch 2200, val loss: 1.6065126657485962
Epoch 2210, training loss: 310.84344482421875 = 0.026679158210754395 + 50.0 * 6.216335296630859
Epoch 2210, val loss: 1.6100454330444336
Epoch 2220, training loss: 310.8345947265625 = 0.0262672808021307 + 50.0 * 6.2161664962768555
Epoch 2220, val loss: 1.6142570972442627
Epoch 2230, training loss: 310.9892272949219 = 0.02586701139807701 + 50.0 * 6.219266891479492
Epoch 2230, val loss: 1.6179277896881104
Epoch 2240, training loss: 310.98681640625 = 0.025451846420764923 + 50.0 * 6.219227313995361
Epoch 2240, val loss: 1.6207996606826782
Epoch 2250, training loss: 310.8891296386719 = 0.02505102939903736 + 50.0 * 6.217281341552734
Epoch 2250, val loss: 1.6246355772018433
Epoch 2260, training loss: 310.991943359375 = 0.02466532774269581 + 50.0 * 6.219345569610596
Epoch 2260, val loss: 1.6281661987304688
Epoch 2270, training loss: 310.90386962890625 = 0.024281708523631096 + 50.0 * 6.217591762542725
Epoch 2270, val loss: 1.631499171257019
Epoch 2280, training loss: 310.7944030761719 = 0.023911098018288612 + 50.0 * 6.215409755706787
Epoch 2280, val loss: 1.6352157592773438
Epoch 2290, training loss: 310.8507385253906 = 0.023556742817163467 + 50.0 * 6.216543674468994
Epoch 2290, val loss: 1.6385451555252075
Epoch 2300, training loss: 310.9690246582031 = 0.023210816085338593 + 50.0 * 6.218915939331055
Epoch 2300, val loss: 1.6420437097549438
Epoch 2310, training loss: 310.8959045410156 = 0.02286202274262905 + 50.0 * 6.217461109161377
Epoch 2310, val loss: 1.645344614982605
Epoch 2320, training loss: 310.8728942871094 = 0.02251533418893814 + 50.0 * 6.217007160186768
Epoch 2320, val loss: 1.6485379934310913
Epoch 2330, training loss: 310.8310852050781 = 0.022184094414114952 + 50.0 * 6.216177940368652
Epoch 2330, val loss: 1.65195894241333
Epoch 2340, training loss: 310.8421325683594 = 0.021862002089619637 + 50.0 * 6.216405868530273
Epoch 2340, val loss: 1.6551182270050049
Epoch 2350, training loss: 310.80340576171875 = 0.021542420610785484 + 50.0 * 6.21563720703125
Epoch 2350, val loss: 1.6583714485168457
Epoch 2360, training loss: 311.016357421875 = 0.021233826875686646 + 50.0 * 6.219902515411377
Epoch 2360, val loss: 1.661730408668518
Epoch 2370, training loss: 310.822021484375 = 0.02092592418193817 + 50.0 * 6.21602201461792
Epoch 2370, val loss: 1.664767861366272
Epoch 2380, training loss: 310.8021240234375 = 0.020628156140446663 + 50.0 * 6.215629577636719
Epoch 2380, val loss: 1.6681255102157593
Epoch 2390, training loss: 310.8992004394531 = 0.020341405645012856 + 50.0 * 6.21757698059082
Epoch 2390, val loss: 1.6713695526123047
Epoch 2400, training loss: 310.6960144042969 = 0.020049620419740677 + 50.0 * 6.21351957321167
Epoch 2400, val loss: 1.674434781074524
Epoch 2410, training loss: 310.7047119140625 = 0.019772296771407127 + 50.0 * 6.213698863983154
Epoch 2410, val loss: 1.6773681640625
Epoch 2420, training loss: 310.89324951171875 = 0.019511165097355843 + 50.0 * 6.217474937438965
Epoch 2420, val loss: 1.6805615425109863
Epoch 2430, training loss: 310.6973876953125 = 0.019237272441387177 + 50.0 * 6.213563442230225
Epoch 2430, val loss: 1.6836106777191162
Epoch 2440, training loss: 310.7647705078125 = 0.018975602462887764 + 50.0 * 6.214915752410889
Epoch 2440, val loss: 1.6866579055786133
Epoch 2450, training loss: 311.0046081542969 = 0.018718302249908447 + 50.0 * 6.219717979431152
Epoch 2450, val loss: 1.6893634796142578
Epoch 2460, training loss: 310.8028869628906 = 0.018455227836966515 + 50.0 * 6.215688228607178
Epoch 2460, val loss: 1.6922632455825806
Epoch 2470, training loss: 310.6629638671875 = 0.018207306042313576 + 50.0 * 6.212894916534424
Epoch 2470, val loss: 1.6952394247055054
Epoch 2480, training loss: 310.6462707519531 = 0.017972277477383614 + 50.0 * 6.212565898895264
Epoch 2480, val loss: 1.6985390186309814
Epoch 2490, training loss: 310.866455078125 = 0.017742788419127464 + 50.0 * 6.21697473526001
Epoch 2490, val loss: 1.7008659839630127
Epoch 2500, training loss: 310.66278076171875 = 0.017507404088974 + 50.0 * 6.212905406951904
Epoch 2500, val loss: 1.7043051719665527
Epoch 2510, training loss: 310.7528076171875 = 0.017279572784900665 + 50.0 * 6.214710712432861
Epoch 2510, val loss: 1.7070367336273193
Epoch 2520, training loss: 310.6311340332031 = 0.017054280266165733 + 50.0 * 6.212281703948975
Epoch 2520, val loss: 1.709737777709961
Epoch 2530, training loss: 310.83172607421875 = 0.016849134117364883 + 50.0 * 6.216297626495361
Epoch 2530, val loss: 1.7127234935760498
Epoch 2540, training loss: 310.8360595703125 = 0.0166210625320673 + 50.0 * 6.216388702392578
Epoch 2540, val loss: 1.714657187461853
Epoch 2550, training loss: 310.61883544921875 = 0.016407443210482597 + 50.0 * 6.212048053741455
Epoch 2550, val loss: 1.717785120010376
Epoch 2560, training loss: 310.546630859375 = 0.01619991846382618 + 50.0 * 6.21060848236084
Epoch 2560, val loss: 1.7204136848449707
Epoch 2570, training loss: 310.5428771972656 = 0.01600492186844349 + 50.0 * 6.210537910461426
Epoch 2570, val loss: 1.7235112190246582
Epoch 2580, training loss: 310.6373291015625 = 0.01581631600856781 + 50.0 * 6.212430477142334
Epoch 2580, val loss: 1.7261714935302734
Epoch 2590, training loss: 310.8448486328125 = 0.015619727782905102 + 50.0 * 6.2165846824646
Epoch 2590, val loss: 1.7284237146377563
Epoch 2600, training loss: 310.7154541015625 = 0.015422523021697998 + 50.0 * 6.214000701904297
Epoch 2600, val loss: 1.7305930852890015
Epoch 2610, training loss: 310.62939453125 = 0.015230532735586166 + 50.0 * 6.212283134460449
Epoch 2610, val loss: 1.7338489294052124
Epoch 2620, training loss: 310.66558837890625 = 0.01505131833255291 + 50.0 * 6.213010787963867
Epoch 2620, val loss: 1.7361875772476196
Epoch 2630, training loss: 310.6975402832031 = 0.014873324893414974 + 50.0 * 6.213653564453125
Epoch 2630, val loss: 1.738618016242981
Epoch 2640, training loss: 310.5591735839844 = 0.01469237171113491 + 50.0 * 6.21088981628418
Epoch 2640, val loss: 1.7411082983016968
Epoch 2650, training loss: 310.5168151855469 = 0.014524736441671848 + 50.0 * 6.21004581451416
Epoch 2650, val loss: 1.7438400983810425
Epoch 2660, training loss: 310.5787353515625 = 0.014360315166413784 + 50.0 * 6.211287021636963
Epoch 2660, val loss: 1.7464393377304077
Epoch 2670, training loss: 310.662109375 = 0.014193752780556679 + 50.0 * 6.212958335876465
Epoch 2670, val loss: 1.748518705368042
Epoch 2680, training loss: 310.6606140136719 = 0.014027313329279423 + 50.0 * 6.2129316329956055
Epoch 2680, val loss: 1.7507487535476685
Epoch 2690, training loss: 310.58270263671875 = 0.01386766042560339 + 50.0 * 6.211377143859863
Epoch 2690, val loss: 1.753404140472412
Epoch 2700, training loss: 310.6133728027344 = 0.013708009384572506 + 50.0 * 6.211993217468262
Epoch 2700, val loss: 1.755433201789856
Epoch 2710, training loss: 310.55059814453125 = 0.013552471064031124 + 50.0 * 6.21074104309082
Epoch 2710, val loss: 1.7580097913742065
Epoch 2720, training loss: 310.5205993652344 = 0.01339638140052557 + 50.0 * 6.21014404296875
Epoch 2720, val loss: 1.759889006614685
Epoch 2730, training loss: 310.7207336425781 = 0.01325105782598257 + 50.0 * 6.2141499519348145
Epoch 2730, val loss: 1.7624139785766602
Epoch 2740, training loss: 310.42205810546875 = 0.013101479038596153 + 50.0 * 6.208179473876953
Epoch 2740, val loss: 1.764495849609375
Epoch 2750, training loss: 310.4392395019531 = 0.01295881625264883 + 50.0 * 6.208525657653809
Epoch 2750, val loss: 1.7669087648391724
Epoch 2760, training loss: 310.4701843261719 = 0.01282335165888071 + 50.0 * 6.2091474533081055
Epoch 2760, val loss: 1.7692601680755615
Epoch 2770, training loss: 310.73773193359375 = 0.01269061304628849 + 50.0 * 6.214500427246094
Epoch 2770, val loss: 1.7712349891662598
Epoch 2780, training loss: 310.5635986328125 = 0.012546975165605545 + 50.0 * 6.2110209465026855
Epoch 2780, val loss: 1.7733691930770874
Epoch 2790, training loss: 310.4793701171875 = 0.012410922907292843 + 50.0 * 6.209339141845703
Epoch 2790, val loss: 1.7753400802612305
Epoch 2800, training loss: 310.64794921875 = 0.01228730846196413 + 50.0 * 6.212713241577148
Epoch 2800, val loss: 1.7777446508407593
Epoch 2810, training loss: 310.614990234375 = 0.012152260169386864 + 50.0 * 6.212056636810303
Epoch 2810, val loss: 1.7794053554534912
Epoch 2820, training loss: 310.3934326171875 = 0.012018359266221523 + 50.0 * 6.20762825012207
Epoch 2820, val loss: 1.7815682888031006
Epoch 2830, training loss: 310.39599609375 = 0.011892852373421192 + 50.0 * 6.207681655883789
Epoch 2830, val loss: 1.783569574356079
Epoch 2840, training loss: 310.4206848144531 = 0.011776892468333244 + 50.0 * 6.2081780433654785
Epoch 2840, val loss: 1.786228895187378
Epoch 2850, training loss: 310.75103759765625 = 0.011664848774671555 + 50.0 * 6.214787483215332
Epoch 2850, val loss: 1.7879769802093506
Epoch 2860, training loss: 310.4852600097656 = 0.011533786542713642 + 50.0 * 6.209474563598633
Epoch 2860, val loss: 1.7892019748687744
Epoch 2870, training loss: 310.4380798339844 = 0.011420432478189468 + 50.0 * 6.20853328704834
Epoch 2870, val loss: 1.791517734527588
Epoch 2880, training loss: 310.52642822265625 = 0.01130622811615467 + 50.0 * 6.210302829742432
Epoch 2880, val loss: 1.7935458421707153
Epoch 2890, training loss: 310.4060363769531 = 0.011190803721547127 + 50.0 * 6.207896709442139
Epoch 2890, val loss: 1.795622706413269
Epoch 2900, training loss: 310.49224853515625 = 0.011083923280239105 + 50.0 * 6.209623336791992
Epoch 2900, val loss: 1.7974860668182373
Epoch 2910, training loss: 310.6323547363281 = 0.01097080484032631 + 50.0 * 6.212428092956543
Epoch 2910, val loss: 1.7985002994537354
Epoch 2920, training loss: 310.43597412109375 = 0.010859144851565361 + 50.0 * 6.208502292633057
Epoch 2920, val loss: 1.8010218143463135
Epoch 2930, training loss: 310.34210205078125 = 0.01075359433889389 + 50.0 * 6.206627368927002
Epoch 2930, val loss: 1.8025739192962646
Epoch 2940, training loss: 310.3179626464844 = 0.010654089041054249 + 50.0 * 6.206146240234375
Epoch 2940, val loss: 1.8048884868621826
Epoch 2950, training loss: 310.3770446777344 = 0.010556231252849102 + 50.0 * 6.207329750061035
Epoch 2950, val loss: 1.8066133260726929
Epoch 2960, training loss: 310.6295471191406 = 0.010459307581186295 + 50.0 * 6.212381362915039
Epoch 2960, val loss: 1.808133840560913
Epoch 2970, training loss: 310.4923095703125 = 0.010354054160416126 + 50.0 * 6.209639072418213
Epoch 2970, val loss: 1.8097723722457886
Epoch 2980, training loss: 310.4310607910156 = 0.010252333246171474 + 50.0 * 6.208415985107422
Epoch 2980, val loss: 1.8111296892166138
Epoch 2990, training loss: 310.3846740722656 = 0.010154456831514835 + 50.0 * 6.207489967346191
Epoch 2990, val loss: 1.813047170639038
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6925925925925926
0.8207696362677913
The final CL Acc:0.68642, 0.00630, The final GNN Acc:0.82130, 0.00155
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13132])
remove edge: torch.Size([2, 7890])
updated graph: torch.Size([2, 10466])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.8082275390625 = 1.9645057916641235 + 50.0 * 8.596874237060547
Epoch 0, val loss: 1.963179111480713
Epoch 10, training loss: 431.76800537109375 = 1.95481276512146 + 50.0 * 8.596263885498047
Epoch 10, val loss: 1.9534647464752197
Epoch 20, training loss: 431.51593017578125 = 1.9425114393234253 + 50.0 * 8.591468811035156
Epoch 20, val loss: 1.9410253763198853
Epoch 30, training loss: 429.5741271972656 = 1.9265626668930054 + 50.0 * 8.552950859069824
Epoch 30, val loss: 1.9249082803726196
Epoch 40, training loss: 416.7762145996094 = 1.9055726528167725 + 50.0 * 8.297412872314453
Epoch 40, val loss: 1.9038424491882324
Epoch 50, training loss: 391.71710205078125 = 1.879538655281067 + 50.0 * 7.796751022338867
Epoch 50, val loss: 1.8778995275497437
Epoch 60, training loss: 377.54974365234375 = 1.8608689308166504 + 50.0 * 7.513777256011963
Epoch 60, val loss: 1.8595848083496094
Epoch 70, training loss: 364.3143310546875 = 1.8500146865844727 + 50.0 * 7.249286651611328
Epoch 70, val loss: 1.8486214876174927
Epoch 80, training loss: 353.841064453125 = 1.8398655652999878 + 50.0 * 7.0400238037109375
Epoch 80, val loss: 1.8390579223632812
Epoch 90, training loss: 347.47412109375 = 1.82798433303833 + 50.0 * 6.9129228591918945
Epoch 90, val loss: 1.828408122062683
Epoch 100, training loss: 342.5965881347656 = 1.8164371252059937 + 50.0 * 6.815602779388428
Epoch 100, val loss: 1.8175092935562134
Epoch 110, training loss: 338.4083251953125 = 1.8060295581817627 + 50.0 * 6.732046127319336
Epoch 110, val loss: 1.80703604221344
Epoch 120, training loss: 335.125244140625 = 1.7952113151550293 + 50.0 * 6.666601181030273
Epoch 120, val loss: 1.7965335845947266
Epoch 130, training loss: 332.8703918457031 = 1.7838984727859497 + 50.0 * 6.621729850769043
Epoch 130, val loss: 1.785811185836792
Epoch 140, training loss: 331.0032043457031 = 1.7724560499191284 + 50.0 * 6.5846147537231445
Epoch 140, val loss: 1.7749791145324707
Epoch 150, training loss: 329.7056579589844 = 1.7606956958770752 + 50.0 * 6.558899402618408
Epoch 150, val loss: 1.7638050317764282
Epoch 160, training loss: 328.3508605957031 = 1.7480669021606445 + 50.0 * 6.532055377960205
Epoch 160, val loss: 1.7519500255584717
Epoch 170, training loss: 327.3072204589844 = 1.734757900238037 + 50.0 * 6.511448860168457
Epoch 170, val loss: 1.739461898803711
Epoch 180, training loss: 326.482177734375 = 1.7206611633300781 + 50.0 * 6.495230197906494
Epoch 180, val loss: 1.7263423204421997
Epoch 190, training loss: 325.68927001953125 = 1.7055604457855225 + 50.0 * 6.479674339294434
Epoch 190, val loss: 1.7121708393096924
Epoch 200, training loss: 324.9647521972656 = 1.689365029335022 + 50.0 * 6.465507984161377
Epoch 200, val loss: 1.6971741914749146
Epoch 210, training loss: 324.3656921386719 = 1.672084093093872 + 50.0 * 6.453872203826904
Epoch 210, val loss: 1.6811622381210327
Epoch 220, training loss: 324.24700927734375 = 1.6534823179244995 + 50.0 * 6.451870441436768
Epoch 220, val loss: 1.6641401052474976
Epoch 230, training loss: 323.4688415527344 = 1.6336870193481445 + 50.0 * 6.436702728271484
Epoch 230, val loss: 1.6460728645324707
Epoch 240, training loss: 322.9615783691406 = 1.6129497289657593 + 50.0 * 6.426972389221191
Epoch 240, val loss: 1.6272592544555664
Epoch 250, training loss: 322.55816650390625 = 1.5911293029785156 + 50.0 * 6.41934061050415
Epoch 250, val loss: 1.6076148748397827
Epoch 260, training loss: 322.24462890625 = 1.5684114694595337 + 50.0 * 6.413524150848389
Epoch 260, val loss: 1.5873087644577026
Epoch 270, training loss: 321.9810485839844 = 1.544761061668396 + 50.0 * 6.408725738525391
Epoch 270, val loss: 1.5663450956344604
Epoch 280, training loss: 321.58477783203125 = 1.5204919576644897 + 50.0 * 6.4012861251831055
Epoch 280, val loss: 1.5450797080993652
Epoch 290, training loss: 321.2908935546875 = 1.4958229064941406 + 50.0 * 6.395901679992676
Epoch 290, val loss: 1.523736834526062
Epoch 300, training loss: 321.0777587890625 = 1.4707971811294556 + 50.0 * 6.392139434814453
Epoch 300, val loss: 1.5023975372314453
Epoch 310, training loss: 320.7572021484375 = 1.445361614227295 + 50.0 * 6.386236667633057
Epoch 310, val loss: 1.4811384677886963
Epoch 320, training loss: 320.47161865234375 = 1.4199329614639282 + 50.0 * 6.381033897399902
Epoch 320, val loss: 1.4601808786392212
Epoch 330, training loss: 320.22137451171875 = 1.394405484199524 + 50.0 * 6.37653923034668
Epoch 330, val loss: 1.4394596815109253
Epoch 340, training loss: 320.0601806640625 = 1.3688896894454956 + 50.0 * 6.373825550079346
Epoch 340, val loss: 1.4191069602966309
Epoch 350, training loss: 319.7593994140625 = 1.3433821201324463 + 50.0 * 6.368320465087891
Epoch 350, val loss: 1.3990185260772705
Epoch 360, training loss: 319.4701232910156 = 1.3180299997329712 + 50.0 * 6.363041877746582
Epoch 360, val loss: 1.3794130086898804
Epoch 370, training loss: 319.4030456542969 = 1.2927969694137573 + 50.0 * 6.3622050285339355
Epoch 370, val loss: 1.3600976467132568
Epoch 380, training loss: 319.18560791015625 = 1.2672803401947021 + 50.0 * 6.3583664894104
Epoch 380, val loss: 1.3409346342086792
Epoch 390, training loss: 318.85662841796875 = 1.2420284748077393 + 50.0 * 6.352292060852051
Epoch 390, val loss: 1.3220981359481812
Epoch 400, training loss: 318.7376403808594 = 1.216815710067749 + 50.0 * 6.35041618347168
Epoch 400, val loss: 1.3035157918930054
Epoch 410, training loss: 318.5636901855469 = 1.1915684938430786 + 50.0 * 6.347442626953125
Epoch 410, val loss: 1.2851566076278687
Epoch 420, training loss: 318.2467956542969 = 1.1662691831588745 + 50.0 * 6.341610431671143
Epoch 420, val loss: 1.2669123411178589
Epoch 430, training loss: 318.0878601074219 = 1.1411646604537964 + 50.0 * 6.33893346786499
Epoch 430, val loss: 1.2489678859710693
Epoch 440, training loss: 317.92327880859375 = 1.1158827543258667 + 50.0 * 6.336147785186768
Epoch 440, val loss: 1.2309168577194214
Epoch 450, training loss: 317.74609375 = 1.0908244848251343 + 50.0 * 6.333105564117432
Epoch 450, val loss: 1.2133570909500122
Epoch 460, training loss: 317.5730285644531 = 1.0658855438232422 + 50.0 * 6.330142974853516
Epoch 460, val loss: 1.196054220199585
Epoch 470, training loss: 317.78070068359375 = 1.0411235094070435 + 50.0 * 6.33479118347168
Epoch 470, val loss: 1.17887544631958
Epoch 480, training loss: 317.3454284667969 = 1.0166469812393188 + 50.0 * 6.326575756072998
Epoch 480, val loss: 1.162084937095642
Epoch 490, training loss: 317.1172790527344 = 0.9924249649047852 + 50.0 * 6.3224968910217285
Epoch 490, val loss: 1.145696997642517
Epoch 500, training loss: 317.0891418457031 = 0.9686558842658997 + 50.0 * 6.322409629821777
Epoch 500, val loss: 1.1296918392181396
Epoch 510, training loss: 316.8880310058594 = 0.945193350315094 + 50.0 * 6.318856716156006
Epoch 510, val loss: 1.1141473054885864
Epoch 520, training loss: 316.76776123046875 = 0.9222204685211182 + 50.0 * 6.316911220550537
Epoch 520, val loss: 1.0991801023483276
Epoch 530, training loss: 316.5788879394531 = 0.899651050567627 + 50.0 * 6.313584804534912
Epoch 530, val loss: 1.0845991373062134
Epoch 540, training loss: 316.59759521484375 = 0.8776856064796448 + 50.0 * 6.314398288726807
Epoch 540, val loss: 1.0707025527954102
Epoch 550, training loss: 316.39947509765625 = 0.856124997138977 + 50.0 * 6.310866832733154
Epoch 550, val loss: 1.0572079420089722
Epoch 560, training loss: 316.1708984375 = 0.8350386619567871 + 50.0 * 6.3067169189453125
Epoch 560, val loss: 1.044297456741333
Epoch 570, training loss: 316.0695495605469 = 0.8144881725311279 + 50.0 * 6.30510139465332
Epoch 570, val loss: 1.0319056510925293
Epoch 580, training loss: 316.1002502441406 = 0.7944174408912659 + 50.0 * 6.306117057800293
Epoch 580, val loss: 1.0202014446258545
Epoch 590, training loss: 315.8715515136719 = 0.7748585939407349 + 50.0 * 6.301933765411377
Epoch 590, val loss: 1.0087474584579468
Epoch 600, training loss: 315.9317932128906 = 0.7557135224342346 + 50.0 * 6.303521633148193
Epoch 600, val loss: 0.9979820251464844
Epoch 610, training loss: 315.7194519042969 = 0.7370445728302002 + 50.0 * 6.299648284912109
Epoch 610, val loss: 0.9878218173980713
Epoch 620, training loss: 315.57073974609375 = 0.7188391089439392 + 50.0 * 6.2970380783081055
Epoch 620, val loss: 0.9781297445297241
Epoch 630, training loss: 315.58709716796875 = 0.7010992765426636 + 50.0 * 6.297719955444336
Epoch 630, val loss: 0.9689159393310547
Epoch 640, training loss: 315.36016845703125 = 0.6837886571884155 + 50.0 * 6.293528079986572
Epoch 640, val loss: 0.9604816436767578
Epoch 650, training loss: 315.2885437011719 = 0.6668609976768494 + 50.0 * 6.292433261871338
Epoch 650, val loss: 0.9521417617797852
Epoch 660, training loss: 315.3794860839844 = 0.6502304673194885 + 50.0 * 6.29458475112915
Epoch 660, val loss: 0.944483757019043
Epoch 670, training loss: 315.1482238769531 = 0.6340020298957825 + 50.0 * 6.290284156799316
Epoch 670, val loss: 0.9371299743652344
Epoch 680, training loss: 314.99969482421875 = 0.6181793212890625 + 50.0 * 6.287630558013916
Epoch 680, val loss: 0.9303265810012817
Epoch 690, training loss: 314.93218994140625 = 0.6027395725250244 + 50.0 * 6.286588668823242
Epoch 690, val loss: 0.9239609837532043
Epoch 700, training loss: 315.1286926269531 = 0.587624728679657 + 50.0 * 6.290821552276611
Epoch 700, val loss: 0.9180901646614075
Epoch 710, training loss: 314.86419677734375 = 0.5726629495620728 + 50.0 * 6.285830974578857
Epoch 710, val loss: 0.9123541712760925
Epoch 720, training loss: 314.7491149902344 = 0.5581170916557312 + 50.0 * 6.283820152282715
Epoch 720, val loss: 0.9072693586349487
Epoch 730, training loss: 314.7866516113281 = 0.5439940094947815 + 50.0 * 6.284852981567383
Epoch 730, val loss: 0.9029496908187866
Epoch 740, training loss: 314.6055908203125 = 0.5300741195678711 + 50.0 * 6.281510829925537
Epoch 740, val loss: 0.8985995650291443
Epoch 750, training loss: 314.5494079589844 = 0.5165707468986511 + 50.0 * 6.280656814575195
Epoch 750, val loss: 0.8951442241668701
Epoch 760, training loss: 314.49725341796875 = 0.5033456087112427 + 50.0 * 6.27987813949585
Epoch 760, val loss: 0.8918647766113281
Epoch 770, training loss: 314.5200500488281 = 0.4904809892177582 + 50.0 * 6.2805914878845215
Epoch 770, val loss: 0.8891138434410095
Epoch 780, training loss: 314.3476257324219 = 0.4778967499732971 + 50.0 * 6.2773942947387695
Epoch 780, val loss: 0.8868263959884644
Epoch 790, training loss: 314.3600769042969 = 0.4656817317008972 + 50.0 * 6.277888298034668
Epoch 790, val loss: 0.8849459290504456
Epoch 800, training loss: 314.2557373046875 = 0.4538189768791199 + 50.0 * 6.27603816986084
Epoch 800, val loss: 0.8836249709129333
Epoch 810, training loss: 314.1739501953125 = 0.4422776997089386 + 50.0 * 6.274633407592773
Epoch 810, val loss: 0.8826377987861633
Epoch 820, training loss: 314.3902893066406 = 0.4310647249221802 + 50.0 * 6.279184341430664
Epoch 820, val loss: 0.8820638656616211
Epoch 830, training loss: 314.1383056640625 = 0.42004072666168213 + 50.0 * 6.274365425109863
Epoch 830, val loss: 0.8813934922218323
Epoch 840, training loss: 313.98443603515625 = 0.4094165563583374 + 50.0 * 6.271500110626221
Epoch 840, val loss: 0.8817448019981384
Epoch 850, training loss: 313.919189453125 = 0.39909759163856506 + 50.0 * 6.270401477813721
Epoch 850, val loss: 0.8820561766624451
Epoch 860, training loss: 314.07672119140625 = 0.3890945017337799 + 50.0 * 6.273752689361572
Epoch 860, val loss: 0.8828989863395691
Epoch 870, training loss: 313.92718505859375 = 0.3793209195137024 + 50.0 * 6.270956993103027
Epoch 870, val loss: 0.8840740919113159
Epoch 880, training loss: 313.92095947265625 = 0.3698149025440216 + 50.0 * 6.271022796630859
Epoch 880, val loss: 0.8853209018707275
Epoch 890, training loss: 313.8098449707031 = 0.3605663478374481 + 50.0 * 6.268985748291016
Epoch 890, val loss: 0.8871297240257263
Epoch 900, training loss: 313.6432800292969 = 0.351617693901062 + 50.0 * 6.265833377838135
Epoch 900, val loss: 0.8889767527580261
Epoch 910, training loss: 313.6297302246094 = 0.34297728538513184 + 50.0 * 6.265734672546387
Epoch 910, val loss: 0.8912457227706909
Epoch 920, training loss: 313.6918640136719 = 0.33452388644218445 + 50.0 * 6.267146587371826
Epoch 920, val loss: 0.8936942219734192
Epoch 930, training loss: 313.5774230957031 = 0.32626819610595703 + 50.0 * 6.265023231506348
Epoch 930, val loss: 0.8962750434875488
Epoch 940, training loss: 313.9947814941406 = 0.3182907998561859 + 50.0 * 6.273529529571533
Epoch 940, val loss: 0.8991149663925171
Epoch 950, training loss: 313.5408020019531 = 0.3105141818523407 + 50.0 * 6.264605522155762
Epoch 950, val loss: 0.9022919535636902
Epoch 960, training loss: 313.3569030761719 = 0.3029375672340393 + 50.0 * 6.261078834533691
Epoch 960, val loss: 0.9055110216140747
Epoch 970, training loss: 313.343017578125 = 0.29567521810531616 + 50.0 * 6.260946750640869
Epoch 970, val loss: 0.9090766310691833
Epoch 980, training loss: 313.57208251953125 = 0.2885902523994446 + 50.0 * 6.265669822692871
Epoch 980, val loss: 0.9128479957580566
Epoch 990, training loss: 313.31268310546875 = 0.281603068113327 + 50.0 * 6.260621070861816
Epoch 990, val loss: 0.9162874221801758
Epoch 1000, training loss: 313.2480163574219 = 0.2749062776565552 + 50.0 * 6.259462356567383
Epoch 1000, val loss: 0.9203312993049622
Epoch 1010, training loss: 313.3410339355469 = 0.2683447301387787 + 50.0 * 6.261454105377197
Epoch 1010, val loss: 0.9243733882904053
Epoch 1020, training loss: 313.2907409667969 = 0.2620025873184204 + 50.0 * 6.260574817657471
Epoch 1020, val loss: 0.9286479353904724
Epoch 1030, training loss: 313.10809326171875 = 0.2557796835899353 + 50.0 * 6.257046222686768
Epoch 1030, val loss: 0.9328917860984802
Epoch 1040, training loss: 313.1674499511719 = 0.24978519976139069 + 50.0 * 6.258353233337402
Epoch 1040, val loss: 0.937506914138794
Epoch 1050, training loss: 313.0469055175781 = 0.24386931955814362 + 50.0 * 6.256060600280762
Epoch 1050, val loss: 0.941899299621582
Epoch 1060, training loss: 312.99139404296875 = 0.23815472424030304 + 50.0 * 6.255064487457275
Epoch 1060, val loss: 0.9467158913612366
Epoch 1070, training loss: 312.942626953125 = 0.2325868010520935 + 50.0 * 6.2542009353637695
Epoch 1070, val loss: 0.9514914155006409
Epoch 1080, training loss: 313.4071960449219 = 0.22722607851028442 + 50.0 * 6.263599395751953
Epoch 1080, val loss: 0.9565072655677795
Epoch 1090, training loss: 313.1286926269531 = 0.22187894582748413 + 50.0 * 6.25813627243042
Epoch 1090, val loss: 0.9613929986953735
Epoch 1100, training loss: 312.8371887207031 = 0.21668848395347595 + 50.0 * 6.252410411834717
Epoch 1100, val loss: 0.9664207100868225
Epoch 1110, training loss: 312.80596923828125 = 0.21169252693653107 + 50.0 * 6.251885414123535
Epoch 1110, val loss: 0.9716463685035706
Epoch 1120, training loss: 312.80584716796875 = 0.20682547986507416 + 50.0 * 6.251980304718018
Epoch 1120, val loss: 0.9769054055213928
Epoch 1130, training loss: 312.9114685058594 = 0.20205368101596832 + 50.0 * 6.254188060760498
Epoch 1130, val loss: 0.9821857810020447
Epoch 1140, training loss: 313.0361328125 = 0.19737011194229126 + 50.0 * 6.256775379180908
Epoch 1140, val loss: 0.9873687624931335
Epoch 1150, training loss: 312.7210998535156 = 0.19281961023807526 + 50.0 * 6.250565528869629
Epoch 1150, val loss: 0.9928854703903198
Epoch 1160, training loss: 312.61846923828125 = 0.18838752806186676 + 50.0 * 6.24860143661499
Epoch 1160, val loss: 0.9982675909996033
Epoch 1170, training loss: 312.60345458984375 = 0.1840866357088089 + 50.0 * 6.248387336730957
Epoch 1170, val loss: 1.003730297088623
Epoch 1180, training loss: 312.86944580078125 = 0.17991074919700623 + 50.0 * 6.253790855407715
Epoch 1180, val loss: 1.0093159675598145
Epoch 1190, training loss: 312.6894836425781 = 0.17568901181221008 + 50.0 * 6.250275611877441
Epoch 1190, val loss: 1.0146119594573975
Epoch 1200, training loss: 312.5635070800781 = 0.17162950336933136 + 50.0 * 6.247837543487549
Epoch 1200, val loss: 1.0201513767242432
Epoch 1210, training loss: 312.6089782714844 = 0.1676861196756363 + 50.0 * 6.248825550079346
Epoch 1210, val loss: 1.025823712348938
Epoch 1220, training loss: 312.4508972167969 = 0.1638137400150299 + 50.0 * 6.245741844177246
Epoch 1220, val loss: 1.0313447713851929
Epoch 1230, training loss: 312.498779296875 = 0.16004060208797455 + 50.0 * 6.246774673461914
Epoch 1230, val loss: 1.0369302034378052
Epoch 1240, training loss: 312.5052490234375 = 0.15636391937732697 + 50.0 * 6.24697732925415
Epoch 1240, val loss: 1.0426640510559082
Epoch 1250, training loss: 312.6715087890625 = 0.15272045135498047 + 50.0 * 6.250375747680664
Epoch 1250, val loss: 1.0483636856079102
Epoch 1260, training loss: 312.47552490234375 = 0.14912444353103638 + 50.0 * 6.246527671813965
Epoch 1260, val loss: 1.0538005828857422
Epoch 1270, training loss: 312.3421630859375 = 0.14566396176815033 + 50.0 * 6.243929862976074
Epoch 1270, val loss: 1.0594899654388428
Epoch 1280, training loss: 312.2787780761719 = 0.1423102766275406 + 50.0 * 6.242729663848877
Epoch 1280, val loss: 1.0652135610580444
Epoch 1290, training loss: 312.22528076171875 = 0.13905350863933563 + 50.0 * 6.241724491119385
Epoch 1290, val loss: 1.070928931236267
Epoch 1300, training loss: 312.20648193359375 = 0.13587725162506104 + 50.0 * 6.241412162780762
Epoch 1300, val loss: 1.0766657590866089
Epoch 1310, training loss: 312.7738037109375 = 0.13278433680534363 + 50.0 * 6.2528204917907715
Epoch 1310, val loss: 1.082391619682312
Epoch 1320, training loss: 312.3630676269531 = 0.12964880466461182 + 50.0 * 6.244668483734131
Epoch 1320, val loss: 1.0877108573913574
Epoch 1330, training loss: 312.17779541015625 = 0.1266583502292633 + 50.0 * 6.241023063659668
Epoch 1330, val loss: 1.0934258699417114
Epoch 1340, training loss: 312.24853515625 = 0.12373663485050201 + 50.0 * 6.242496013641357
Epoch 1340, val loss: 1.099015235900879
Epoch 1350, training loss: 312.1318664550781 = 0.12088759988546371 + 50.0 * 6.240219593048096
Epoch 1350, val loss: 1.1046555042266846
Epoch 1360, training loss: 312.407958984375 = 0.11810682713985443 + 50.0 * 6.245797157287598
Epoch 1360, val loss: 1.1104028224945068
Epoch 1370, training loss: 312.1956481933594 = 0.1153363287448883 + 50.0 * 6.241606712341309
Epoch 1370, val loss: 1.1157612800598145
Epoch 1380, training loss: 312.17083740234375 = 0.11266914010047913 + 50.0 * 6.24116325378418
Epoch 1380, val loss: 1.1214368343353271
Epoch 1390, training loss: 312.1304016113281 = 0.11001529544591904 + 50.0 * 6.240407466888428
Epoch 1390, val loss: 1.1268268823623657
Epoch 1400, training loss: 312.05548095703125 = 0.10746300220489502 + 50.0 * 6.2389607429504395
Epoch 1400, val loss: 1.1322780847549438
Epoch 1410, training loss: 311.9864807128906 = 0.10492893308401108 + 50.0 * 6.237631320953369
Epoch 1410, val loss: 1.1378031969070435
Epoch 1420, training loss: 312.0205383300781 = 0.10245507955551147 + 50.0 * 6.238361835479736
Epoch 1420, val loss: 1.1432963609695435
Epoch 1430, training loss: 312.06781005859375 = 0.10002924501895905 + 50.0 * 6.239355564117432
Epoch 1430, val loss: 1.1487410068511963
Epoch 1440, training loss: 311.97296142578125 = 0.09765670448541641 + 50.0 * 6.237505912780762
Epoch 1440, val loss: 1.1541626453399658
Epoch 1450, training loss: 312.00640869140625 = 0.09533770382404327 + 50.0 * 6.238221168518066
Epoch 1450, val loss: 1.159685492515564
Epoch 1460, training loss: 312.08941650390625 = 0.09307271987199783 + 50.0 * 6.239926815032959
Epoch 1460, val loss: 1.1653492450714111
Epoch 1470, training loss: 311.87371826171875 = 0.09083979576826096 + 50.0 * 6.235657215118408
Epoch 1470, val loss: 1.1706678867340088
Epoch 1480, training loss: 311.8594055175781 = 0.0887017771601677 + 50.0 * 6.235413551330566
Epoch 1480, val loss: 1.1762866973876953
Epoch 1490, training loss: 312.00274658203125 = 0.08662958443164825 + 50.0 * 6.2383222579956055
Epoch 1490, val loss: 1.1816737651824951
Epoch 1500, training loss: 311.8289489746094 = 0.08458815515041351 + 50.0 * 6.23488712310791
Epoch 1500, val loss: 1.18705153465271
Epoch 1510, training loss: 311.8414001464844 = 0.08261284232139587 + 50.0 * 6.235175609588623
Epoch 1510, val loss: 1.1924724578857422
Epoch 1520, training loss: 311.972412109375 = 0.08071232587099075 + 50.0 * 6.2378339767456055
Epoch 1520, val loss: 1.1979647874832153
Epoch 1530, training loss: 311.814697265625 = 0.07880434393882751 + 50.0 * 6.234718322753906
Epoch 1530, val loss: 1.202992558479309
Epoch 1540, training loss: 311.704833984375 = 0.07696101814508438 + 50.0 * 6.23255729675293
Epoch 1540, val loss: 1.2083531618118286
Epoch 1550, training loss: 311.6798400878906 = 0.0751807913184166 + 50.0 * 6.23209285736084
Epoch 1550, val loss: 1.2136859893798828
Epoch 1560, training loss: 312.04290771484375 = 0.07346039265394211 + 50.0 * 6.239388942718506
Epoch 1560, val loss: 1.218848705291748
Epoch 1570, training loss: 311.7652587890625 = 0.07174725830554962 + 50.0 * 6.233870506286621
Epoch 1570, val loss: 1.2239067554473877
Epoch 1580, training loss: 311.7015380859375 = 0.0700894147157669 + 50.0 * 6.23262882232666
Epoch 1580, val loss: 1.229061484336853
Epoch 1590, training loss: 311.6576232910156 = 0.06848997622728348 + 50.0 * 6.231782913208008
Epoch 1590, val loss: 1.2342246770858765
Epoch 1600, training loss: 311.64666748046875 = 0.0669361874461174 + 50.0 * 6.231595039367676
Epoch 1600, val loss: 1.2392158508300781
Epoch 1610, training loss: 311.72210693359375 = 0.06542501598596573 + 50.0 * 6.233133792877197
Epoch 1610, val loss: 1.2442477941513062
Epoch 1620, training loss: 311.90887451171875 = 0.06395857781171799 + 50.0 * 6.236897945404053
Epoch 1620, val loss: 1.2492430210113525
Epoch 1630, training loss: 311.6498718261719 = 0.06247565150260925 + 50.0 * 6.231748104095459
Epoch 1630, val loss: 1.254136562347412
Epoch 1640, training loss: 311.5434875488281 = 0.06108460575342178 + 50.0 * 6.229647636413574
Epoch 1640, val loss: 1.2591588497161865
Epoch 1650, training loss: 311.5439758300781 = 0.059729818254709244 + 50.0 * 6.229685306549072
Epoch 1650, val loss: 1.2641644477844238
Epoch 1660, training loss: 311.76043701171875 = 0.058405544608831406 + 50.0 * 6.2340407371521
Epoch 1660, val loss: 1.268946886062622
Epoch 1670, training loss: 311.6039123535156 = 0.05708670988678932 + 50.0 * 6.230936050415039
Epoch 1670, val loss: 1.2736122608184814
Epoch 1680, training loss: 311.5091247558594 = 0.05582018941640854 + 50.0 * 6.229065895080566
Epoch 1680, val loss: 1.2786004543304443
Epoch 1690, training loss: 311.4885559082031 = 0.05460085719823837 + 50.0 * 6.228679180145264
Epoch 1690, val loss: 1.2834824323654175
Epoch 1700, training loss: 311.57159423828125 = 0.053411368280649185 + 50.0 * 6.230363845825195
Epoch 1700, val loss: 1.288329839706421
Epoch 1710, training loss: 311.4832458496094 = 0.05224915221333504 + 50.0 * 6.2286200523376465
Epoch 1710, val loss: 1.2929524183273315
Epoch 1720, training loss: 311.6185302734375 = 0.05111438408493996 + 50.0 * 6.231348514556885
Epoch 1720, val loss: 1.2977218627929688
Epoch 1730, training loss: 311.62493896484375 = 0.04999038577079773 + 50.0 * 6.231499195098877
Epoch 1730, val loss: 1.3022103309631348
Epoch 1740, training loss: 311.4289855957031 = 0.048913583159446716 + 50.0 * 6.227601528167725
Epoch 1740, val loss: 1.3067495822906494
Epoch 1750, training loss: 311.3791809082031 = 0.04786885157227516 + 50.0 * 6.226625919342041
Epoch 1750, val loss: 1.3114701509475708
Epoch 1760, training loss: 311.5044250488281 = 0.046866439282894135 + 50.0 * 6.229150772094727
Epoch 1760, val loss: 1.3160206079483032
Epoch 1770, training loss: 311.454833984375 = 0.04586188122630119 + 50.0 * 6.228179454803467
Epoch 1770, val loss: 1.3204858303070068
Epoch 1780, training loss: 311.3424987792969 = 0.044880907982587814 + 50.0 * 6.2259521484375
Epoch 1780, val loss: 1.3249658346176147
Epoch 1790, training loss: 311.30157470703125 = 0.04394574090838432 + 50.0 * 6.225152492523193
Epoch 1790, val loss: 1.3295644521713257
Epoch 1800, training loss: 311.3582763671875 = 0.043046753853559494 + 50.0 * 6.226304054260254
Epoch 1800, val loss: 1.3340327739715576
Epoch 1810, training loss: 311.52532958984375 = 0.042158953845500946 + 50.0 * 6.229663848876953
Epoch 1810, val loss: 1.338435411453247
Epoch 1820, training loss: 311.4132080078125 = 0.04128432646393776 + 50.0 * 6.227438449859619
Epoch 1820, val loss: 1.3425285816192627
Epoch 1830, training loss: 311.27899169921875 = 0.04043327644467354 + 50.0 * 6.224771022796631
Epoch 1830, val loss: 1.3469561338424683
Epoch 1840, training loss: 311.3263854980469 = 0.03961881995201111 + 50.0 * 6.225735664367676
Epoch 1840, val loss: 1.3512743711471558
Epoch 1850, training loss: 311.3392333984375 = 0.038813184946775436 + 50.0 * 6.226008415222168
Epoch 1850, val loss: 1.3556126356124878
Epoch 1860, training loss: 311.325439453125 = 0.03803091496229172 + 50.0 * 6.225748062133789
Epoch 1860, val loss: 1.35973060131073
Epoch 1870, training loss: 311.19183349609375 = 0.03726501762866974 + 50.0 * 6.2230916023254395
Epoch 1870, val loss: 1.3639389276504517
Epoch 1880, training loss: 311.17626953125 = 0.03652874007821083 + 50.0 * 6.222794532775879
Epoch 1880, val loss: 1.3680610656738281
Epoch 1890, training loss: 311.33917236328125 = 0.03582606837153435 + 50.0 * 6.226067066192627
Epoch 1890, val loss: 1.3722480535507202
Epoch 1900, training loss: 311.27301025390625 = 0.03512297570705414 + 50.0 * 6.224757671356201
Epoch 1900, val loss: 1.3762414455413818
Epoch 1910, training loss: 311.20587158203125 = 0.034429699182510376 + 50.0 * 6.223429203033447
Epoch 1910, val loss: 1.380190372467041
Epoch 1920, training loss: 311.14398193359375 = 0.03375917673110962 + 50.0 * 6.222204685211182
Epoch 1920, val loss: 1.3842421770095825
Epoch 1930, training loss: 311.3298034667969 = 0.03311730921268463 + 50.0 * 6.22593355178833
Epoch 1930, val loss: 1.3881750106811523
Epoch 1940, training loss: 311.19744873046875 = 0.032485317438840866 + 50.0 * 6.223299503326416
Epoch 1940, val loss: 1.3918806314468384
Epoch 1950, training loss: 311.090087890625 = 0.0318632498383522 + 50.0 * 6.221164703369141
Epoch 1950, val loss: 1.3959797620773315
Epoch 1960, training loss: 311.09332275390625 = 0.03126554936170578 + 50.0 * 6.221240997314453
Epoch 1960, val loss: 1.399812936782837
Epoch 1970, training loss: 311.1327819824219 = 0.030693689361214638 + 50.0 * 6.222041606903076
Epoch 1970, val loss: 1.403767466545105
Epoch 1980, training loss: 311.2800598144531 = 0.03012993559241295 + 50.0 * 6.224998950958252
Epoch 1980, val loss: 1.4075638055801392
Epoch 1990, training loss: 311.311767578125 = 0.029572298750281334 + 50.0 * 6.225643634796143
Epoch 1990, val loss: 1.4113247394561768
Epoch 2000, training loss: 311.1399841308594 = 0.02901492267847061 + 50.0 * 6.222218990325928
Epoch 2000, val loss: 1.4151259660720825
Epoch 2010, training loss: 311.09576416015625 = 0.028496865183115005 + 50.0 * 6.221344947814941
Epoch 2010, val loss: 1.4189428091049194
Epoch 2020, training loss: 311.13427734375 = 0.027985358610749245 + 50.0 * 6.222126007080078
Epoch 2020, val loss: 1.4226282835006714
Epoch 2030, training loss: 311.25775146484375 = 0.027482932433485985 + 50.0 * 6.224605083465576
Epoch 2030, val loss: 1.4260035753250122
Epoch 2040, training loss: 311.014892578125 = 0.026982009410858154 + 50.0 * 6.219758033752441
Epoch 2040, val loss: 1.429657220840454
Epoch 2050, training loss: 310.9905700683594 = 0.026504836976528168 + 50.0 * 6.219281196594238
Epoch 2050, val loss: 1.4332389831542969
Epoch 2060, training loss: 310.9464416503906 = 0.026046473532915115 + 50.0 * 6.218408107757568
Epoch 2060, val loss: 1.4368008375167847
Epoch 2070, training loss: 311.00140380859375 = 0.025600675493478775 + 50.0 * 6.219515800476074
Epoch 2070, val loss: 1.4404072761535645
Epoch 2080, training loss: 311.2750244140625 = 0.02516324445605278 + 50.0 * 6.224997043609619
Epoch 2080, val loss: 1.4436992406845093
Epoch 2090, training loss: 311.03448486328125 = 0.02471258118748665 + 50.0 * 6.220195293426514
Epoch 2090, val loss: 1.4473298788070679
Epoch 2100, training loss: 310.94232177734375 = 0.024284768849611282 + 50.0 * 6.218360424041748
Epoch 2100, val loss: 1.4507696628570557
Epoch 2110, training loss: 310.9209899902344 = 0.023874536156654358 + 50.0 * 6.217942714691162
Epoch 2110, val loss: 1.4544004201889038
Epoch 2120, training loss: 311.0699462890625 = 0.023477347567677498 + 50.0 * 6.2209296226501465
Epoch 2120, val loss: 1.457790493965149
Epoch 2130, training loss: 310.95880126953125 = 0.023086488246917725 + 50.0 * 6.218714714050293
Epoch 2130, val loss: 1.4609776735305786
Epoch 2140, training loss: 311.268798828125 = 0.022698651999235153 + 50.0 * 6.224922180175781
Epoch 2140, val loss: 1.4643139839172363
Epoch 2150, training loss: 310.916259765625 = 0.02231200784444809 + 50.0 * 6.217878818511963
Epoch 2150, val loss: 1.4678152799606323
Epoch 2160, training loss: 310.8517150878906 = 0.021942855790257454 + 50.0 * 6.216595649719238
Epoch 2160, val loss: 1.4711554050445557
Epoch 2170, training loss: 310.8229675292969 = 0.021591106429696083 + 50.0 * 6.216027736663818
Epoch 2170, val loss: 1.4744991064071655
Epoch 2180, training loss: 310.82635498046875 = 0.021249108016490936 + 50.0 * 6.216102123260498
Epoch 2180, val loss: 1.4779062271118164
Epoch 2190, training loss: 311.0936584472656 = 0.020922619849443436 + 50.0 * 6.221454620361328
Epoch 2190, val loss: 1.4811724424362183
Epoch 2200, training loss: 310.93292236328125 = 0.020577948540449142 + 50.0 * 6.218246936798096
Epoch 2200, val loss: 1.4841558933258057
Epoch 2210, training loss: 310.90057373046875 = 0.020244596526026726 + 50.0 * 6.217606544494629
Epoch 2210, val loss: 1.4876487255096436
Epoch 2220, training loss: 310.8819580078125 = 0.019924618303775787 + 50.0 * 6.217240810394287
Epoch 2220, val loss: 1.4904450178146362
Epoch 2230, training loss: 310.8618469238281 = 0.019613487645983696 + 50.0 * 6.21684455871582
Epoch 2230, val loss: 1.4936498403549194
Epoch 2240, training loss: 311.06597900390625 = 0.01932128518819809 + 50.0 * 6.220932960510254
Epoch 2240, val loss: 1.4966968297958374
Epoch 2250, training loss: 310.7867431640625 = 0.019012335687875748 + 50.0 * 6.2153544425964355
Epoch 2250, val loss: 1.4999912977218628
Epoch 2260, training loss: 310.78497314453125 = 0.01872347854077816 + 50.0 * 6.215324878692627
Epoch 2260, val loss: 1.5029802322387695
Epoch 2270, training loss: 311.01068115234375 = 0.018447237089276314 + 50.0 * 6.219844818115234
Epoch 2270, val loss: 1.5059292316436768
Epoch 2280, training loss: 310.8012390136719 = 0.018158050253987312 + 50.0 * 6.215661525726318
Epoch 2280, val loss: 1.5088765621185303
Epoch 2290, training loss: 310.7801208496094 = 0.017886800691485405 + 50.0 * 6.215244770050049
Epoch 2290, val loss: 1.5118708610534668
Epoch 2300, training loss: 310.7512512207031 = 0.01762600988149643 + 50.0 * 6.214672088623047
Epoch 2300, val loss: 1.5149961709976196
Epoch 2310, training loss: 310.883056640625 = 0.017369665205478668 + 50.0 * 6.217313766479492
Epoch 2310, val loss: 1.5177700519561768
Epoch 2320, training loss: 310.9517517089844 = 0.017117269337177277 + 50.0 * 6.218692779541016
Epoch 2320, val loss: 1.5209474563598633
Epoch 2330, training loss: 310.7503967285156 = 0.016862234100699425 + 50.0 * 6.214670658111572
Epoch 2330, val loss: 1.5239038467407227
Epoch 2340, training loss: 310.71942138671875 = 0.016617363318800926 + 50.0 * 6.214056015014648
Epoch 2340, val loss: 1.5266562700271606
Epoch 2350, training loss: 310.6852111816406 = 0.016386080533266068 + 50.0 * 6.213376522064209
Epoch 2350, val loss: 1.5297768115997314
Epoch 2360, training loss: 310.84619140625 = 0.01616005040705204 + 50.0 * 6.21660041809082
Epoch 2360, val loss: 1.532719612121582
Epoch 2370, training loss: 310.7960510253906 = 0.015930216759443283 + 50.0 * 6.215602397918701
Epoch 2370, val loss: 1.5351643562316895
Epoch 2380, training loss: 310.7550964355469 = 0.015707887709140778 + 50.0 * 6.214787483215332
Epoch 2380, val loss: 1.5382200479507446
Epoch 2390, training loss: 310.6778869628906 = 0.015485481359064579 + 50.0 * 6.213248252868652
Epoch 2390, val loss: 1.5408233404159546
Epoch 2400, training loss: 310.7156982421875 = 0.015276064164936543 + 50.0 * 6.214008331298828
Epoch 2400, val loss: 1.543543815612793
Epoch 2410, training loss: 310.8206787109375 = 0.01506733987480402 + 50.0 * 6.21611213684082
Epoch 2410, val loss: 1.5463718175888062
Epoch 2420, training loss: 310.79296875 = 0.014859999530017376 + 50.0 * 6.215561866760254
Epoch 2420, val loss: 1.5490914583206177
Epoch 2430, training loss: 310.7780456542969 = 0.01465766690671444 + 50.0 * 6.215267658233643
Epoch 2430, val loss: 1.5514968633651733
Epoch 2440, training loss: 310.63946533203125 = 0.014461314305663109 + 50.0 * 6.212500095367432
Epoch 2440, val loss: 1.5545533895492554
Epoch 2450, training loss: 310.9398193359375 = 0.014274663291871548 + 50.0 * 6.218510627746582
Epoch 2450, val loss: 1.5572503805160522
Epoch 2460, training loss: 310.6589660644531 = 0.014081750996410847 + 50.0 * 6.212897777557373
Epoch 2460, val loss: 1.5597153902053833
Epoch 2470, training loss: 310.5743103027344 = 0.013899234123528004 + 50.0 * 6.211208343505859
Epoch 2470, val loss: 1.5623993873596191
Epoch 2480, training loss: 310.57080078125 = 0.013721851631999016 + 50.0 * 6.211141586303711
Epoch 2480, val loss: 1.5650873184204102
Epoch 2490, training loss: 310.6973876953125 = 0.013553936034440994 + 50.0 * 6.213676452636719
Epoch 2490, val loss: 1.5678335428237915
Epoch 2500, training loss: 310.73712158203125 = 0.013379434123635292 + 50.0 * 6.214475154876709
Epoch 2500, val loss: 1.570253849029541
Epoch 2510, training loss: 310.6395568847656 = 0.013200965709984303 + 50.0 * 6.212526798248291
Epoch 2510, val loss: 1.5727678537368774
Epoch 2520, training loss: 310.5712890625 = 0.013036897405982018 + 50.0 * 6.211165428161621
Epoch 2520, val loss: 1.574967622756958
Epoch 2530, training loss: 310.6587829589844 = 0.012876307591795921 + 50.0 * 6.212917804718018
Epoch 2530, val loss: 1.5777579545974731
Epoch 2540, training loss: 310.7362976074219 = 0.012716640718281269 + 50.0 * 6.214471340179443
Epoch 2540, val loss: 1.580368161201477
Epoch 2550, training loss: 310.6661682128906 = 0.012555614113807678 + 50.0 * 6.213072299957275
Epoch 2550, val loss: 1.5826507806777954
Epoch 2560, training loss: 310.53411865234375 = 0.012400566600263119 + 50.0 * 6.210434436798096
Epoch 2560, val loss: 1.5850363969802856
Epoch 2570, training loss: 310.5086669921875 = 0.012250395491719246 + 50.0 * 6.209928512573242
Epoch 2570, val loss: 1.587885856628418
Epoch 2580, training loss: 310.7701721191406 = 0.012108459137380123 + 50.0 * 6.215160846710205
Epoch 2580, val loss: 1.590317726135254
Epoch 2590, training loss: 310.47711181640625 = 0.011956391856074333 + 50.0 * 6.20930290222168
Epoch 2590, val loss: 1.5924814939498901
Epoch 2600, training loss: 310.5040283203125 = 0.011813800781965256 + 50.0 * 6.20984411239624
Epoch 2600, val loss: 1.594968318939209
Epoch 2610, training loss: 310.5367126464844 = 0.01167706586420536 + 50.0 * 6.210500240325928
Epoch 2610, val loss: 1.597567081451416
Epoch 2620, training loss: 310.76434326171875 = 0.011546839028596878 + 50.0 * 6.215056419372559
Epoch 2620, val loss: 1.59994375705719
Epoch 2630, training loss: 310.5271301269531 = 0.011404571123421192 + 50.0 * 6.2103142738342285
Epoch 2630, val loss: 1.6018868684768677
Epoch 2640, training loss: 310.6298522949219 = 0.011275499127805233 + 50.0 * 6.212371826171875
Epoch 2640, val loss: 1.6044564247131348
Epoch 2650, training loss: 310.4967041015625 = 0.011139624752104282 + 50.0 * 6.209711074829102
Epoch 2650, val loss: 1.6065661907196045
Epoch 2660, training loss: 310.537353515625 = 0.01101140771061182 + 50.0 * 6.210526466369629
Epoch 2660, val loss: 1.6089632511138916
Epoch 2670, training loss: 310.4762878417969 = 0.010889029130339622 + 50.0 * 6.20930814743042
Epoch 2670, val loss: 1.6115251779556274
Epoch 2680, training loss: 310.6759948730469 = 0.010770251043140888 + 50.0 * 6.21330451965332
Epoch 2680, val loss: 1.6138839721679688
Epoch 2690, training loss: 310.4828186035156 = 0.010647039860486984 + 50.0 * 6.20944356918335
Epoch 2690, val loss: 1.6157256364822388
Epoch 2700, training loss: 310.4459228515625 = 0.010530070401728153 + 50.0 * 6.208707809448242
Epoch 2700, val loss: 1.618040680885315
Epoch 2710, training loss: 310.4985656738281 = 0.010419226251542568 + 50.0 * 6.209763050079346
Epoch 2710, val loss: 1.6203023195266724
Epoch 2720, training loss: 310.54449462890625 = 0.010308035649359226 + 50.0 * 6.210683345794678
Epoch 2720, val loss: 1.6223220825195312
Epoch 2730, training loss: 310.5594482421875 = 0.010190379805862904 + 50.0 * 6.21098518371582
Epoch 2730, val loss: 1.6245806217193604
Epoch 2740, training loss: 310.4673767089844 = 0.010078931227326393 + 50.0 * 6.209146022796631
Epoch 2740, val loss: 1.6267253160476685
Epoch 2750, training loss: 310.5444030761719 = 0.00997284147888422 + 50.0 * 6.210688591003418
Epoch 2750, val loss: 1.6286146640777588
Epoch 2760, training loss: 310.4626770019531 = 0.009866088628768921 + 50.0 * 6.209056377410889
Epoch 2760, val loss: 1.6310831308364868
Epoch 2770, training loss: 310.4120788574219 = 0.009766274131834507 + 50.0 * 6.2080464363098145
Epoch 2770, val loss: 1.6332628726959229
Epoch 2780, training loss: 310.5168762207031 = 0.009668877348303795 + 50.0 * 6.21014404296875
Epoch 2780, val loss: 1.6353015899658203
Epoch 2790, training loss: 310.44183349609375 = 0.009563793428242207 + 50.0 * 6.208645820617676
Epoch 2790, val loss: 1.6373822689056396
Epoch 2800, training loss: 310.5007019042969 = 0.009467258118093014 + 50.0 * 6.209825038909912
Epoch 2800, val loss: 1.6394621133804321
Epoch 2810, training loss: 310.4051208496094 = 0.00937019381672144 + 50.0 * 6.20791482925415
Epoch 2810, val loss: 1.6416105031967163
Epoch 2820, training loss: 310.4459228515625 = 0.00927708763629198 + 50.0 * 6.208732604980469
Epoch 2820, val loss: 1.643376111984253
Epoch 2830, training loss: 310.5444641113281 = 0.009183600544929504 + 50.0 * 6.210705280303955
Epoch 2830, val loss: 1.6452618837356567
Epoch 2840, training loss: 310.43902587890625 = 0.00909026712179184 + 50.0 * 6.208598613739014
Epoch 2840, val loss: 1.6472394466400146
Epoch 2850, training loss: 310.396484375 = 0.00900022778660059 + 50.0 * 6.207749366760254
Epoch 2850, val loss: 1.6493031978607178
Epoch 2860, training loss: 310.34625244140625 = 0.008913624100387096 + 50.0 * 6.206747055053711
Epoch 2860, val loss: 1.6512864828109741
Epoch 2870, training loss: 310.346923828125 = 0.008829289115965366 + 50.0 * 6.206762313842773
Epoch 2870, val loss: 1.6532338857650757
Epoch 2880, training loss: 310.588134765625 = 0.008746925741434097 + 50.0 * 6.211587429046631
Epoch 2880, val loss: 1.6550047397613525
Epoch 2890, training loss: 310.38128662109375 = 0.008660458959639072 + 50.0 * 6.207452297210693
Epoch 2890, val loss: 1.6572227478027344
Epoch 2900, training loss: 310.401611328125 = 0.008576796390116215 + 50.0 * 6.207860469818115
Epoch 2900, val loss: 1.6590598821640015
Epoch 2910, training loss: 310.4209899902344 = 0.008493325673043728 + 50.0 * 6.208250045776367
Epoch 2910, val loss: 1.6609593629837036
Epoch 2920, training loss: 310.4396667480469 = 0.008414952084422112 + 50.0 * 6.208624839782715
Epoch 2920, val loss: 1.663004755973816
Epoch 2930, training loss: 310.3297424316406 = 0.008335773833096027 + 50.0 * 6.206428050994873
Epoch 2930, val loss: 1.6646925210952759
Epoch 2940, training loss: 310.30694580078125 = 0.008262716233730316 + 50.0 * 6.2059736251831055
Epoch 2940, val loss: 1.6665399074554443
Epoch 2950, training loss: 310.3265380859375 = 0.008189760148525238 + 50.0 * 6.206367492675781
Epoch 2950, val loss: 1.6685633659362793
Epoch 2960, training loss: 310.306640625 = 0.008116612210869789 + 50.0 * 6.205970764160156
Epoch 2960, val loss: 1.6705955266952515
Epoch 2970, training loss: 310.6258239746094 = 0.008047555573284626 + 50.0 * 6.212355136871338
Epoch 2970, val loss: 1.6725460290908813
Epoch 2980, training loss: 310.346435546875 = 0.007967907935380936 + 50.0 * 6.2067694664001465
Epoch 2980, val loss: 1.6734739542007446
Epoch 2990, training loss: 310.2857360839844 = 0.007896538823843002 + 50.0 * 6.205556869506836
Epoch 2990, val loss: 1.6759763956069946
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8376383763837639
=== training gcn model ===
Epoch 0, training loss: 431.79736328125 = 1.954393982887268 + 50.0 * 8.596858978271484
Epoch 0, val loss: 1.9560695886611938
Epoch 10, training loss: 431.75042724609375 = 1.9450891017913818 + 50.0 * 8.59610652923584
Epoch 10, val loss: 1.947056770324707
Epoch 20, training loss: 431.44384765625 = 1.9333430528640747 + 50.0 * 8.5902099609375
Epoch 20, val loss: 1.935318112373352
Epoch 30, training loss: 429.5174865722656 = 1.9177528619766235 + 50.0 * 8.551994323730469
Epoch 30, val loss: 1.919366717338562
Epoch 40, training loss: 419.9986877441406 = 1.8987836837768555 + 50.0 * 8.361998558044434
Epoch 40, val loss: 1.9007102251052856
Epoch 50, training loss: 398.869384765625 = 1.876863718032837 + 50.0 * 7.939850330352783
Epoch 50, val loss: 1.87873375415802
Epoch 60, training loss: 381.2530517578125 = 1.858156681060791 + 50.0 * 7.587898254394531
Epoch 60, val loss: 1.860952377319336
Epoch 70, training loss: 362.4179992675781 = 1.8434491157531738 + 50.0 * 7.211491107940674
Epoch 70, val loss: 1.84696364402771
Epoch 80, training loss: 352.9755859375 = 1.8320324420928955 + 50.0 * 7.022871017456055
Epoch 80, val loss: 1.835859775543213
Epoch 90, training loss: 347.3684997558594 = 1.8201590776443481 + 50.0 * 6.910966873168945
Epoch 90, val loss: 1.8241244554519653
Epoch 100, training loss: 343.348876953125 = 1.807832956314087 + 50.0 * 6.8308210372924805
Epoch 100, val loss: 1.812406063079834
Epoch 110, training loss: 340.5771789550781 = 1.7959131002426147 + 50.0 * 6.775625705718994
Epoch 110, val loss: 1.8008662462234497
Epoch 120, training loss: 337.8055725097656 = 1.7847682237625122 + 50.0 * 6.720416069030762
Epoch 120, val loss: 1.790252923965454
Epoch 130, training loss: 335.3471374511719 = 1.7752739191055298 + 50.0 * 6.6714372634887695
Epoch 130, val loss: 1.7809866666793823
Epoch 140, training loss: 333.3467102050781 = 1.7651880979537964 + 50.0 * 6.6316304206848145
Epoch 140, val loss: 1.7712550163269043
Epoch 150, training loss: 331.77862548828125 = 1.7543933391571045 + 50.0 * 6.600484848022461
Epoch 150, val loss: 1.7610305547714233
Epoch 160, training loss: 330.4570617675781 = 1.742908000946045 + 50.0 * 6.574282646179199
Epoch 160, val loss: 1.7504788637161255
Epoch 170, training loss: 329.6841125488281 = 1.730828046798706 + 50.0 * 6.559065341949463
Epoch 170, val loss: 1.7393790483474731
Epoch 180, training loss: 328.3142395019531 = 1.7175698280334473 + 50.0 * 6.531933784484863
Epoch 180, val loss: 1.7277584075927734
Epoch 190, training loss: 327.28521728515625 = 1.7039114236831665 + 50.0 * 6.511626243591309
Epoch 190, val loss: 1.7156221866607666
Epoch 200, training loss: 326.7199401855469 = 1.6892735958099365 + 50.0 * 6.500613689422607
Epoch 200, val loss: 1.7027722597122192
Epoch 210, training loss: 325.5953063964844 = 1.6736305952072144 + 50.0 * 6.478433132171631
Epoch 210, val loss: 1.688982367515564
Epoch 220, training loss: 324.8526611328125 = 1.6568957567214966 + 50.0 * 6.4639153480529785
Epoch 220, val loss: 1.6743830442428589
Epoch 230, training loss: 324.3758850097656 = 1.638909935951233 + 50.0 * 6.454739570617676
Epoch 230, val loss: 1.6587767601013184
Epoch 240, training loss: 323.64556884765625 = 1.619723916053772 + 50.0 * 6.440516948699951
Epoch 240, val loss: 1.6421986818313599
Epoch 250, training loss: 323.1905517578125 = 1.5995022058486938 + 50.0 * 6.431821346282959
Epoch 250, val loss: 1.6247633695602417
Epoch 260, training loss: 322.708984375 = 1.5779486894607544 + 50.0 * 6.42262077331543
Epoch 260, val loss: 1.6064939498901367
Epoch 270, training loss: 322.1711730957031 = 1.5557332038879395 + 50.0 * 6.412309169769287
Epoch 270, val loss: 1.5876054763793945
Epoch 280, training loss: 321.7640686035156 = 1.5327343940734863 + 50.0 * 6.404626369476318
Epoch 280, val loss: 1.5681086778640747
Epoch 290, training loss: 321.5533447265625 = 1.5088746547698975 + 50.0 * 6.4008893966674805
Epoch 290, val loss: 1.5478174686431885
Epoch 300, training loss: 321.06573486328125 = 1.4845006465911865 + 50.0 * 6.391624450683594
Epoch 300, val loss: 1.5276341438293457
Epoch 310, training loss: 320.6710510253906 = 1.4596971273422241 + 50.0 * 6.384227275848389
Epoch 310, val loss: 1.5071097612380981
Epoch 320, training loss: 320.3111267089844 = 1.4346303939819336 + 50.0 * 6.377530097961426
Epoch 320, val loss: 1.4863749742507935
Epoch 330, training loss: 320.3861083984375 = 1.409269094467163 + 50.0 * 6.3795366287231445
Epoch 330, val loss: 1.4655779600143433
Epoch 340, training loss: 319.8435974121094 = 1.383606195449829 + 50.0 * 6.369199752807617
Epoch 340, val loss: 1.4447447061538696
Epoch 350, training loss: 319.44647216796875 = 1.3579357862472534 + 50.0 * 6.3617706298828125
Epoch 350, val loss: 1.4240361452102661
Epoch 360, training loss: 319.2535095214844 = 1.332118272781372 + 50.0 * 6.358428001403809
Epoch 360, val loss: 1.4034185409545898
Epoch 370, training loss: 318.95587158203125 = 1.3064521551132202 + 50.0 * 6.352988243103027
Epoch 370, val loss: 1.3829596042633057
Epoch 380, training loss: 318.7005310058594 = 1.2808424234390259 + 50.0 * 6.348393440246582
Epoch 380, val loss: 1.3627104759216309
Epoch 390, training loss: 318.8006896972656 = 1.2552775144577026 + 50.0 * 6.350908279418945
Epoch 390, val loss: 1.342661738395691
Epoch 400, training loss: 318.3483581542969 = 1.2298040390014648 + 50.0 * 6.342370986938477
Epoch 400, val loss: 1.3230457305908203
Epoch 410, training loss: 318.0696105957031 = 1.2045838832855225 + 50.0 * 6.3373003005981445
Epoch 410, val loss: 1.3036859035491943
Epoch 420, training loss: 317.9380187988281 = 1.1796470880508423 + 50.0 * 6.335166931152344
Epoch 420, val loss: 1.2845879793167114
Epoch 430, training loss: 317.8553161621094 = 1.1550177335739136 + 50.0 * 6.334005832672119
Epoch 430, val loss: 1.2660447359085083
Epoch 440, training loss: 317.47784423828125 = 1.1308860778808594 + 50.0 * 6.326939105987549
Epoch 440, val loss: 1.2479376792907715
Epoch 450, training loss: 317.30072021484375 = 1.107171893119812 + 50.0 * 6.32387113571167
Epoch 450, val loss: 1.2303805351257324
Epoch 460, training loss: 317.1650695800781 = 1.084151268005371 + 50.0 * 6.321618556976318
Epoch 460, val loss: 1.2134772539138794
Epoch 470, training loss: 317.0137939453125 = 1.0613532066345215 + 50.0 * 6.319048881530762
Epoch 470, val loss: 1.1969579458236694
Epoch 480, training loss: 316.8583984375 = 1.0395355224609375 + 50.0 * 6.316376686096191
Epoch 480, val loss: 1.1815105676651
Epoch 490, training loss: 316.66192626953125 = 1.0182229280471802 + 50.0 * 6.3128743171691895
Epoch 490, val loss: 1.1666532754898071
Epoch 500, training loss: 316.5379638671875 = 0.9976772665977478 + 50.0 * 6.310805797576904
Epoch 500, val loss: 1.152588963508606
Epoch 510, training loss: 316.4117431640625 = 0.9778594970703125 + 50.0 * 6.308678150177002
Epoch 510, val loss: 1.1393131017684937
Epoch 520, training loss: 316.25006103515625 = 0.9586166143417358 + 50.0 * 6.30582857131958
Epoch 520, val loss: 1.1266448497772217
Epoch 530, training loss: 316.1626892089844 = 0.94001305103302 + 50.0 * 6.3044538497924805
Epoch 530, val loss: 1.1148234605789185
Epoch 540, training loss: 316.0796813964844 = 0.9218738675117493 + 50.0 * 6.30315637588501
Epoch 540, val loss: 1.1035194396972656
Epoch 550, training loss: 315.8930969238281 = 0.9044207334518433 + 50.0 * 6.299773693084717
Epoch 550, val loss: 1.0928844213485718
Epoch 560, training loss: 315.7466125488281 = 0.8876128792762756 + 50.0 * 6.29718017578125
Epoch 560, val loss: 1.083134651184082
Epoch 570, training loss: 315.70257568359375 = 0.8713959455490112 + 50.0 * 6.296623706817627
Epoch 570, val loss: 1.0738661289215088
Epoch 580, training loss: 315.7526550292969 = 0.8554715514183044 + 50.0 * 6.297943592071533
Epoch 580, val loss: 1.0648584365844727
Epoch 590, training loss: 315.42877197265625 = 0.8402814865112305 + 50.0 * 6.291769981384277
Epoch 590, val loss: 1.0567326545715332
Epoch 600, training loss: 315.3289794921875 = 0.8254096508026123 + 50.0 * 6.290071487426758
Epoch 600, val loss: 1.049173355102539
Epoch 610, training loss: 315.2424011230469 = 0.8110144138336182 + 50.0 * 6.288628101348877
Epoch 610, val loss: 1.0419893264770508
Epoch 620, training loss: 315.24713134765625 = 0.7969380617141724 + 50.0 * 6.289003849029541
Epoch 620, val loss: 1.0349680185317993
Epoch 630, training loss: 315.08343505859375 = 0.7830490469932556 + 50.0 * 6.286007881164551
Epoch 630, val loss: 1.0284667015075684
Epoch 640, training loss: 315.0379333496094 = 0.7695177793502808 + 50.0 * 6.285367965698242
Epoch 640, val loss: 1.0218513011932373
Epoch 650, training loss: 314.8720397949219 = 0.7562095522880554 + 50.0 * 6.2823166847229
Epoch 650, val loss: 1.015967607498169
Epoch 660, training loss: 314.7705993652344 = 0.7431196570396423 + 50.0 * 6.280549049377441
Epoch 660, val loss: 1.0103765726089478
Epoch 670, training loss: 314.81732177734375 = 0.7303118705749512 + 50.0 * 6.281740188598633
Epoch 670, val loss: 1.00519597530365
Epoch 680, training loss: 314.61834716796875 = 0.7174934148788452 + 50.0 * 6.278017044067383
Epoch 680, val loss: 0.9996554851531982
Epoch 690, training loss: 314.5772705078125 = 0.7049551010131836 + 50.0 * 6.277446269989014
Epoch 690, val loss: 0.9950320720672607
Epoch 700, training loss: 314.62591552734375 = 0.6925738453865051 + 50.0 * 6.278666973114014
Epoch 700, val loss: 0.9902796745300293
Epoch 710, training loss: 314.43597412109375 = 0.6802729964256287 + 50.0 * 6.275114059448242
Epoch 710, val loss: 0.9857771396636963
Epoch 720, training loss: 314.55841064453125 = 0.6680527329444885 + 50.0 * 6.277807235717773
Epoch 720, val loss: 0.981587827205658
Epoch 730, training loss: 314.27508544921875 = 0.6559283137321472 + 50.0 * 6.272383689880371
Epoch 730, val loss: 0.977394700050354
Epoch 740, training loss: 314.24310302734375 = 0.6438995599746704 + 50.0 * 6.271984100341797
Epoch 740, val loss: 0.9736983776092529
Epoch 750, training loss: 314.2042236328125 = 0.631967306137085 + 50.0 * 6.271445274353027
Epoch 750, val loss: 0.9699156880378723
Epoch 760, training loss: 314.09942626953125 = 0.6201823949813843 + 50.0 * 6.269585132598877
Epoch 760, val loss: 0.9667738080024719
Epoch 770, training loss: 314.119384765625 = 0.6084217429161072 + 50.0 * 6.270219326019287
Epoch 770, val loss: 0.9636015892028809
Epoch 780, training loss: 313.98419189453125 = 0.5967042446136475 + 50.0 * 6.267749786376953
Epoch 780, val loss: 0.9603765606880188
Epoch 790, training loss: 314.08026123046875 = 0.5850329995155334 + 50.0 * 6.269904613494873
Epoch 790, val loss: 0.9578731060028076
Epoch 800, training loss: 313.8715515136719 = 0.5732848048210144 + 50.0 * 6.265965461730957
Epoch 800, val loss: 0.9545952677726746
Epoch 810, training loss: 313.76959228515625 = 0.5617206692695618 + 50.0 * 6.264157772064209
Epoch 810, val loss: 0.9523036479949951
Epoch 820, training loss: 313.6943664550781 = 0.5502812266349792 + 50.0 * 6.262881278991699
Epoch 820, val loss: 0.9499485492706299
Epoch 830, training loss: 313.8171691894531 = 0.5389347076416016 + 50.0 * 6.265564441680908
Epoch 830, val loss: 0.9479743838310242
Epoch 840, training loss: 313.8283996582031 = 0.5273792743682861 + 50.0 * 6.26602029800415
Epoch 840, val loss: 0.9457457065582275
Epoch 850, training loss: 313.59442138671875 = 0.5160882472991943 + 50.0 * 6.261566638946533
Epoch 850, val loss: 0.9437249302864075
Epoch 860, training loss: 313.4826354980469 = 0.5049083232879639 + 50.0 * 6.259554386138916
Epoch 860, val loss: 0.94225013256073
Epoch 870, training loss: 313.462646484375 = 0.49382248520851135 + 50.0 * 6.259376049041748
Epoch 870, val loss: 0.9409365057945251
Epoch 880, training loss: 313.560791015625 = 0.4828364849090576 + 50.0 * 6.261558532714844
Epoch 880, val loss: 0.9394717216491699
Epoch 890, training loss: 313.3572082519531 = 0.4718630313873291 + 50.0 * 6.257707118988037
Epoch 890, val loss: 0.9380584359169006
Epoch 900, training loss: 313.3895263671875 = 0.4610380232334137 + 50.0 * 6.258569717407227
Epoch 900, val loss: 0.9367579817771912
Epoch 910, training loss: 313.2287292480469 = 0.45037660002708435 + 50.0 * 6.2555670738220215
Epoch 910, val loss: 0.9363293051719666
Epoch 920, training loss: 313.216064453125 = 0.43981748819351196 + 50.0 * 6.255524635314941
Epoch 920, val loss: 0.9354815483093262
Epoch 930, training loss: 313.3865051269531 = 0.4293578863143921 + 50.0 * 6.259142875671387
Epoch 930, val loss: 0.9350163340568542
Epoch 940, training loss: 313.14520263671875 = 0.41908562183380127 + 50.0 * 6.25452184677124
Epoch 940, val loss: 0.9343406558036804
Epoch 950, training loss: 313.29925537109375 = 0.4090615510940552 + 50.0 * 6.257803916931152
Epoch 950, val loss: 0.9346408247947693
Epoch 960, training loss: 313.08270263671875 = 0.3989929258823395 + 50.0 * 6.253674030303955
Epoch 960, val loss: 0.9339149594306946
Epoch 970, training loss: 312.9928283691406 = 0.38933318853378296 + 50.0 * 6.25206995010376
Epoch 970, val loss: 0.9341146349906921
Epoch 980, training loss: 312.89300537109375 = 0.37981659173965454 + 50.0 * 6.250263690948486
Epoch 980, val loss: 0.934420108795166
Epoch 990, training loss: 312.9158630371094 = 0.37057897448539734 + 50.0 * 6.250905990600586
Epoch 990, val loss: 0.9348745346069336
Epoch 1000, training loss: 312.9151916503906 = 0.36140766739845276 + 50.0 * 6.251075744628906
Epoch 1000, val loss: 0.9351617693901062
Epoch 1010, training loss: 312.8833312988281 = 0.3523781895637512 + 50.0 * 6.250618934631348
Epoch 1010, val loss: 0.9356768727302551
Epoch 1020, training loss: 312.7316589355469 = 0.3435877561569214 + 50.0 * 6.247761249542236
Epoch 1020, val loss: 0.9361369013786316
Epoch 1030, training loss: 312.6774597167969 = 0.33506712317466736 + 50.0 * 6.246848106384277
Epoch 1030, val loss: 0.9368270635604858
Epoch 1040, training loss: 312.8720703125 = 0.32674163579940796 + 50.0 * 6.250906467437744
Epoch 1040, val loss: 0.9374794363975525
Epoch 1050, training loss: 312.77801513671875 = 0.3185465931892395 + 50.0 * 6.249189376831055
Epoch 1050, val loss: 0.9389353394508362
Epoch 1060, training loss: 312.6309509277344 = 0.3105160593986511 + 50.0 * 6.246408939361572
Epoch 1060, val loss: 0.939808189868927
Epoch 1070, training loss: 312.6156005859375 = 0.3027499318122864 + 50.0 * 6.2462568283081055
Epoch 1070, val loss: 0.94083172082901
Epoch 1080, training loss: 312.58197021484375 = 0.2951640188694 + 50.0 * 6.245736122131348
Epoch 1080, val loss: 0.9425500631332397
Epoch 1090, training loss: 312.5113525390625 = 0.28780388832092285 + 50.0 * 6.244470596313477
Epoch 1090, val loss: 0.944418728351593
Epoch 1100, training loss: 312.40081787109375 = 0.28060051798820496 + 50.0 * 6.242403984069824
Epoch 1100, val loss: 0.9456669092178345
Epoch 1110, training loss: 312.4739685058594 = 0.273640900850296 + 50.0 * 6.244006156921387
Epoch 1110, val loss: 0.9473949074745178
Epoch 1120, training loss: 312.7789306640625 = 0.2667617201805115 + 50.0 * 6.250243663787842
Epoch 1120, val loss: 0.949582040309906
Epoch 1130, training loss: 312.4033203125 = 0.2599513530731201 + 50.0 * 6.242867469787598
Epoch 1130, val loss: 0.9506538510322571
Epoch 1140, training loss: 312.3199462890625 = 0.25338682532310486 + 50.0 * 6.241331100463867
Epoch 1140, val loss: 0.9529129862785339
Epoch 1150, training loss: 312.2534484863281 = 0.24707406759262085 + 50.0 * 6.2401275634765625
Epoch 1150, val loss: 0.9549151659011841
Epoch 1160, training loss: 312.45758056640625 = 0.24089115858078003 + 50.0 * 6.244333267211914
Epoch 1160, val loss: 0.9566500186920166
Epoch 1170, training loss: 312.29827880859375 = 0.23483072221279144 + 50.0 * 6.241269111633301
Epoch 1170, val loss: 0.9593077301979065
Epoch 1180, training loss: 312.1546936035156 = 0.22893565893173218 + 50.0 * 6.2385149002075195
Epoch 1180, val loss: 0.9612541198730469
Epoch 1190, training loss: 312.0941162109375 = 0.223223477602005 + 50.0 * 6.237417697906494
Epoch 1190, val loss: 0.9638219475746155
Epoch 1200, training loss: 312.1441650390625 = 0.21764546632766724 + 50.0 * 6.238530158996582
Epoch 1200, val loss: 0.9660249948501587
Epoch 1210, training loss: 312.2261047363281 = 0.21215833723545074 + 50.0 * 6.240279197692871
Epoch 1210, val loss: 0.9684258103370667
Epoch 1220, training loss: 312.06866455078125 = 0.2068432718515396 + 50.0 * 6.237236022949219
Epoch 1220, val loss: 0.9711667895317078
Epoch 1230, training loss: 312.0780334472656 = 0.2016764134168625 + 50.0 * 6.237526893615723
Epoch 1230, val loss: 0.9738245010375977
Epoch 1240, training loss: 311.9836730957031 = 0.19661810994148254 + 50.0 * 6.235741138458252
Epoch 1240, val loss: 0.9766521453857422
Epoch 1250, training loss: 312.47662353515625 = 0.1917591691017151 + 50.0 * 6.245697021484375
Epoch 1250, val loss: 0.9798120856285095
Epoch 1260, training loss: 312.01495361328125 = 0.18691766262054443 + 50.0 * 6.236560821533203
Epoch 1260, val loss: 0.981995165348053
Epoch 1270, training loss: 311.8764953613281 = 0.1822190135717392 + 50.0 * 6.233885765075684
Epoch 1270, val loss: 0.9849041700363159
Epoch 1280, training loss: 311.8603820800781 = 0.1777140349149704 + 50.0 * 6.233653545379639
Epoch 1280, val loss: 0.9880797266960144
Epoch 1290, training loss: 312.0968017578125 = 0.1733076274394989 + 50.0 * 6.238470077514648
Epoch 1290, val loss: 0.9906827807426453
Epoch 1300, training loss: 312.0007629394531 = 0.1689387559890747 + 50.0 * 6.236636638641357
Epoch 1300, val loss: 0.9937039017677307
Epoch 1310, training loss: 311.874267578125 = 0.1647409200668335 + 50.0 * 6.234190940856934
Epoch 1310, val loss: 0.9971619844436646
Epoch 1320, training loss: 311.7842102050781 = 0.16063421964645386 + 50.0 * 6.232471466064453
Epoch 1320, val loss: 1.0004576444625854
Epoch 1330, training loss: 311.8297119140625 = 0.15666933357715607 + 50.0 * 6.233460426330566
Epoch 1330, val loss: 1.0036951303482056
Epoch 1340, training loss: 311.7550964355469 = 0.15276578068733215 + 50.0 * 6.232046604156494
Epoch 1340, val loss: 1.0065618753433228
Epoch 1350, training loss: 311.7257385253906 = 0.14896182715892792 + 50.0 * 6.231535911560059
Epoch 1350, val loss: 1.0098367929458618
Epoch 1360, training loss: 311.6610412597656 = 0.14526516199111938 + 50.0 * 6.230315685272217
Epoch 1360, val loss: 1.0131443738937378
Epoch 1370, training loss: 311.6463623046875 = 0.1416807621717453 + 50.0 * 6.230093479156494
Epoch 1370, val loss: 1.0165317058563232
Epoch 1380, training loss: 311.7880859375 = 0.13820260763168335 + 50.0 * 6.232997417449951
Epoch 1380, val loss: 1.0199146270751953
Epoch 1390, training loss: 311.7572326660156 = 0.13476921617984772 + 50.0 * 6.232449054718018
Epoch 1390, val loss: 1.022943377494812
Epoch 1400, training loss: 311.7522888183594 = 0.1314181536436081 + 50.0 * 6.232417583465576
Epoch 1400, val loss: 1.0260779857635498
Epoch 1410, training loss: 311.78594970703125 = 0.1281430423259735 + 50.0 * 6.233156204223633
Epoch 1410, val loss: 1.0294569730758667
Epoch 1420, training loss: 311.52459716796875 = 0.12499383836984634 + 50.0 * 6.227992057800293
Epoch 1420, val loss: 1.0335617065429688
Epoch 1430, training loss: 311.5027770996094 = 0.1219378337264061 + 50.0 * 6.227616786956787
Epoch 1430, val loss: 1.0372592210769653
Epoch 1440, training loss: 311.4648132324219 = 0.11896176636219025 + 50.0 * 6.226917266845703
Epoch 1440, val loss: 1.040650486946106
Epoch 1450, training loss: 311.45404052734375 = 0.11607225239276886 + 50.0 * 6.226759910583496
Epoch 1450, val loss: 1.0441365242004395
Epoch 1460, training loss: 311.68487548828125 = 0.11324965208768845 + 50.0 * 6.2314324378967285
Epoch 1460, val loss: 1.0476007461547852
Epoch 1470, training loss: 311.68377685546875 = 0.11049014329910278 + 50.0 * 6.2314653396606445
Epoch 1470, val loss: 1.0506870746612549
Epoch 1480, training loss: 311.4927978515625 = 0.10776140540838242 + 50.0 * 6.227700710296631
Epoch 1480, val loss: 1.0545132160186768
Epoch 1490, training loss: 311.3850402832031 = 0.1051354631781578 + 50.0 * 6.225597858428955
Epoch 1490, val loss: 1.0583646297454834
Epoch 1500, training loss: 311.35894775390625 = 0.10262799263000488 + 50.0 * 6.225126266479492
Epoch 1500, val loss: 1.0623793601989746
Epoch 1510, training loss: 311.31201171875 = 0.1001729816198349 + 50.0 * 6.224236965179443
Epoch 1510, val loss: 1.065867304801941
Epoch 1520, training loss: 311.37908935546875 = 0.09780208021402359 + 50.0 * 6.225625514984131
Epoch 1520, val loss: 1.069523811340332
Epoch 1530, training loss: 311.4212646484375 = 0.09546365588903427 + 50.0 * 6.226516246795654
Epoch 1530, val loss: 1.0733767747879028
Epoch 1540, training loss: 311.4614562988281 = 0.09315831959247589 + 50.0 * 6.227365970611572
Epoch 1540, val loss: 1.0769646167755127
Epoch 1550, training loss: 311.3482666015625 = 0.09091772139072418 + 50.0 * 6.225147247314453
Epoch 1550, val loss: 1.0808247327804565
Epoch 1560, training loss: 311.23504638671875 = 0.08876517415046692 + 50.0 * 6.222925186157227
Epoch 1560, val loss: 1.084839940071106
Epoch 1570, training loss: 311.22613525390625 = 0.08669067174196243 + 50.0 * 6.2227888107299805
Epoch 1570, val loss: 1.088869333267212
Epoch 1580, training loss: 311.41107177734375 = 0.08467429876327515 + 50.0 * 6.226527690887451
Epoch 1580, val loss: 1.0932397842407227
Epoch 1590, training loss: 311.2799072265625 = 0.08263865858316422 + 50.0 * 6.223945617675781
Epoch 1590, val loss: 1.0959964990615845
Epoch 1600, training loss: 311.2024230957031 = 0.08068166673183441 + 50.0 * 6.2224345207214355
Epoch 1600, val loss: 1.1001790761947632
Epoch 1610, training loss: 311.1436462402344 = 0.0788004994392395 + 50.0 * 6.221296787261963
Epoch 1610, val loss: 1.1039286851882935
Epoch 1620, training loss: 311.1641540527344 = 0.07698429375886917 + 50.0 * 6.221743106842041
Epoch 1620, val loss: 1.1078248023986816
Epoch 1630, training loss: 311.3304138183594 = 0.07521471381187439 + 50.0 * 6.225103855133057
Epoch 1630, val loss: 1.1115727424621582
Epoch 1640, training loss: 311.18792724609375 = 0.07344809919595718 + 50.0 * 6.222289562225342
Epoch 1640, val loss: 1.1153194904327393
Epoch 1650, training loss: 311.2040100097656 = 0.07177510857582092 + 50.0 * 6.222644805908203
Epoch 1650, val loss: 1.1198073625564575
Epoch 1660, training loss: 311.1918029785156 = 0.07010433822870255 + 50.0 * 6.222434043884277
Epoch 1660, val loss: 1.1235642433166504
Epoch 1670, training loss: 311.09698486328125 = 0.06847751140594482 + 50.0 * 6.220570087432861
Epoch 1670, val loss: 1.1276227235794067
Epoch 1680, training loss: 311.1449890136719 = 0.06692898273468018 + 50.0 * 6.221561431884766
Epoch 1680, val loss: 1.1316111087799072
Epoch 1690, training loss: 311.1164245605469 = 0.06539962440729141 + 50.0 * 6.221020221710205
Epoch 1690, val loss: 1.1356468200683594
Epoch 1700, training loss: 311.0206298828125 = 0.06391134858131409 + 50.0 * 6.21913480758667
Epoch 1700, val loss: 1.1394832134246826
Epoch 1710, training loss: 311.009521484375 = 0.062485143542289734 + 50.0 * 6.218940258026123
Epoch 1710, val loss: 1.1435071229934692
Epoch 1720, training loss: 311.23895263671875 = 0.06109831482172012 + 50.0 * 6.223556995391846
Epoch 1720, val loss: 1.1478633880615234
Epoch 1730, training loss: 311.08184814453125 = 0.05969882756471634 + 50.0 * 6.220443248748779
Epoch 1730, val loss: 1.1513900756835938
Epoch 1740, training loss: 310.9816589355469 = 0.05837251618504524 + 50.0 * 6.218465805053711
Epoch 1740, val loss: 1.1552786827087402
Epoch 1750, training loss: 311.0052490234375 = 0.05709386616945267 + 50.0 * 6.218963146209717
Epoch 1750, val loss: 1.1596850156784058
Epoch 1760, training loss: 311.1266174316406 = 0.05584315210580826 + 50.0 * 6.2214155197143555
Epoch 1760, val loss: 1.1635767221450806
Epoch 1770, training loss: 310.9498291015625 = 0.05458235740661621 + 50.0 * 6.217904567718506
Epoch 1770, val loss: 1.1670171022415161
Epoch 1780, training loss: 310.89605712890625 = 0.053405024111270905 + 50.0 * 6.216853141784668
Epoch 1780, val loss: 1.1713507175445557
Epoch 1790, training loss: 311.2135009765625 = 0.0522688589990139 + 50.0 * 6.223224639892578
Epoch 1790, val loss: 1.1751326322555542
Epoch 1800, training loss: 310.917724609375 = 0.051106639206409454 + 50.0 * 6.217331886291504
Epoch 1800, val loss: 1.1790165901184082
Epoch 1810, training loss: 310.86920166015625 = 0.05001218616962433 + 50.0 * 6.216383457183838
Epoch 1810, val loss: 1.183198094367981
Epoch 1820, training loss: 310.8927307128906 = 0.04894735664129257 + 50.0 * 6.2168755531311035
Epoch 1820, val loss: 1.1873867511749268
Epoch 1830, training loss: 311.0146179199219 = 0.047921113669872284 + 50.0 * 6.219334125518799
Epoch 1830, val loss: 1.1912256479263306
Epoch 1840, training loss: 310.8570251464844 = 0.04689125344157219 + 50.0 * 6.216202259063721
Epoch 1840, val loss: 1.1948553323745728
Epoch 1850, training loss: 310.8976135253906 = 0.045922305434942245 + 50.0 * 6.217033386230469
Epoch 1850, val loss: 1.1988637447357178
Epoch 1860, training loss: 310.8140869140625 = 0.04495903477072716 + 50.0 * 6.2153825759887695
Epoch 1860, val loss: 1.2029478549957275
Epoch 1870, training loss: 310.8236999511719 = 0.04402107000350952 + 50.0 * 6.2155938148498535
Epoch 1870, val loss: 1.207276701927185
Epoch 1880, training loss: 311.25372314453125 = 0.04313936084508896 + 50.0 * 6.2242112159729
Epoch 1880, val loss: 1.211695671081543
Epoch 1890, training loss: 310.9304504394531 = 0.042208343744277954 + 50.0 * 6.217764854431152
Epoch 1890, val loss: 1.213965654373169
Epoch 1900, training loss: 310.77447509765625 = 0.04135424643754959 + 50.0 * 6.214662551879883
Epoch 1900, val loss: 1.218839168548584
Epoch 1910, training loss: 310.7202453613281 = 0.040517017245292664 + 50.0 * 6.213594436645508
Epoch 1910, val loss: 1.2222588062286377
Epoch 1920, training loss: 310.74993896484375 = 0.039715059101581573 + 50.0 * 6.214204788208008
Epoch 1920, val loss: 1.2262606620788574
Epoch 1930, training loss: 311.0022277832031 = 0.03892429918050766 + 50.0 * 6.219266414642334
Epoch 1930, val loss: 1.2297805547714233
Epoch 1940, training loss: 310.87835693359375 = 0.03814094141125679 + 50.0 * 6.216804027557373
Epoch 1940, val loss: 1.2336851358413696
Epoch 1950, training loss: 310.71148681640625 = 0.037381190806627274 + 50.0 * 6.213481903076172
Epoch 1950, val loss: 1.2378332614898682
Epoch 1960, training loss: 310.68218994140625 = 0.036658596247434616 + 50.0 * 6.2129106521606445
Epoch 1960, val loss: 1.2417700290679932
Epoch 1970, training loss: 310.764892578125 = 0.03594907373189926 + 50.0 * 6.214579105377197
Epoch 1970, val loss: 1.2456037998199463
Epoch 1980, training loss: 310.7488708496094 = 0.035257238894701004 + 50.0 * 6.214272499084473
Epoch 1980, val loss: 1.2490761280059814
Epoch 1990, training loss: 310.9786682128906 = 0.03458375483751297 + 50.0 * 6.218882083892822
Epoch 1990, val loss: 1.252600073814392
Epoch 2000, training loss: 310.79815673828125 = 0.033909279853105545 + 50.0 * 6.215284824371338
Epoch 2000, val loss: 1.2569830417633057
Epoch 2010, training loss: 310.6846923828125 = 0.03327376767992973 + 50.0 * 6.213028430938721
Epoch 2010, val loss: 1.2608731985092163
Epoch 2020, training loss: 310.8105773925781 = 0.03265496343374252 + 50.0 * 6.2155585289001465
Epoch 2020, val loss: 1.2648061513900757
Epoch 2030, training loss: 310.5965270996094 = 0.032034192234277725 + 50.0 * 6.211289405822754
Epoch 2030, val loss: 1.2677664756774902
Epoch 2040, training loss: 310.6003723144531 = 0.03143765404820442 + 50.0 * 6.211378574371338
Epoch 2040, val loss: 1.271608591079712
Epoch 2050, training loss: 310.5867919921875 = 0.030869975686073303 + 50.0 * 6.211118221282959
Epoch 2050, val loss: 1.2751842737197876
Epoch 2060, training loss: 310.7530212402344 = 0.030308961868286133 + 50.0 * 6.214454174041748
Epoch 2060, val loss: 1.2783843278884888
Epoch 2070, training loss: 310.7093505859375 = 0.02975965291261673 + 50.0 * 6.213592052459717
Epoch 2070, val loss: 1.281978726387024
Epoch 2080, training loss: 310.5506591796875 = 0.029223259538412094 + 50.0 * 6.210428237915039
Epoch 2080, val loss: 1.2861976623535156
Epoch 2090, training loss: 310.5576171875 = 0.028702685609459877 + 50.0 * 6.210577964782715
Epoch 2090, val loss: 1.2896586656570435
Epoch 2100, training loss: 310.6767272949219 = 0.02820155955851078 + 50.0 * 6.212970733642578
Epoch 2100, val loss: 1.2929065227508545
Epoch 2110, training loss: 310.5581970214844 = 0.027702011168003082 + 50.0 * 6.2106099128723145
Epoch 2110, val loss: 1.2965900897979736
Epoch 2120, training loss: 310.6842346191406 = 0.027224430814385414 + 50.0 * 6.21314001083374
Epoch 2120, val loss: 1.2996563911437988
Epoch 2130, training loss: 310.7289123535156 = 0.026749003678560257 + 50.0 * 6.214043140411377
Epoch 2130, val loss: 1.3030738830566406
Epoch 2140, training loss: 310.5592956542969 = 0.02628154866397381 + 50.0 * 6.210660457611084
Epoch 2140, val loss: 1.306727409362793
Epoch 2150, training loss: 310.50433349609375 = 0.025829046964645386 + 50.0 * 6.209570407867432
Epoch 2150, val loss: 1.3104692697525024
Epoch 2160, training loss: 310.4788513183594 = 0.02539840340614319 + 50.0 * 6.20906925201416
Epoch 2160, val loss: 1.313844919204712
Epoch 2170, training loss: 310.6363220214844 = 0.02498677186667919 + 50.0 * 6.212226390838623
Epoch 2170, val loss: 1.3176814317703247
Epoch 2180, training loss: 310.4851989746094 = 0.024560660123825073 + 50.0 * 6.209212779998779
Epoch 2180, val loss: 1.3203567266464233
Epoch 2190, training loss: 310.4692077636719 = 0.02414284460246563 + 50.0 * 6.208901405334473
Epoch 2190, val loss: 1.3234634399414062
Epoch 2200, training loss: 310.4620056152344 = 0.02375652827322483 + 50.0 * 6.208765029907227
Epoch 2200, val loss: 1.3271905183792114
Epoch 2210, training loss: 310.63262939453125 = 0.02337373048067093 + 50.0 * 6.212184906005859
Epoch 2210, val loss: 1.3299585580825806
Epoch 2220, training loss: 310.4893493652344 = 0.022994976490736008 + 50.0 * 6.209327697753906
Epoch 2220, val loss: 1.3339414596557617
Epoch 2230, training loss: 310.5163879394531 = 0.02261863462626934 + 50.0 * 6.209875583648682
Epoch 2230, val loss: 1.3366504907608032
Epoch 2240, training loss: 310.4248352050781 = 0.02225627563893795 + 50.0 * 6.208051681518555
Epoch 2240, val loss: 1.3398314714431763
Epoch 2250, training loss: 310.41021728515625 = 0.021909041330218315 + 50.0 * 6.207766056060791
Epoch 2250, val loss: 1.3427766561508179
Epoch 2260, training loss: 310.5144348144531 = 0.02157190442085266 + 50.0 * 6.20985746383667
Epoch 2260, val loss: 1.346073031425476
Epoch 2270, training loss: 310.4467468261719 = 0.021232090890407562 + 50.0 * 6.208510398864746
Epoch 2270, val loss: 1.3491541147232056
Epoch 2280, training loss: 310.4519348144531 = 0.020901719108223915 + 50.0 * 6.208620548248291
Epoch 2280, val loss: 1.3523058891296387
Epoch 2290, training loss: 310.5045166015625 = 0.0205849502235651 + 50.0 * 6.2096781730651855
Epoch 2290, val loss: 1.355042576789856
Epoch 2300, training loss: 310.5185852050781 = 0.02027205564081669 + 50.0 * 6.209966659545898
Epoch 2300, val loss: 1.3579459190368652
Epoch 2310, training loss: 310.4194641113281 = 0.019965071231126785 + 50.0 * 6.2079901695251465
Epoch 2310, val loss: 1.3614765405654907
Epoch 2320, training loss: 310.36614990234375 = 0.019664352759718895 + 50.0 * 6.206930160522461
Epoch 2320, val loss: 1.3642758131027222
Epoch 2330, training loss: 310.41632080078125 = 0.01937166042625904 + 50.0 * 6.2079386711120605
Epoch 2330, val loss: 1.3669438362121582
Epoch 2340, training loss: 310.42205810546875 = 0.0190854724496603 + 50.0 * 6.208059787750244
Epoch 2340, val loss: 1.3698850870132446
Epoch 2350, training loss: 310.53515625 = 0.018808206543326378 + 50.0 * 6.2103271484375
Epoch 2350, val loss: 1.3732821941375732
Epoch 2360, training loss: 310.38739013671875 = 0.018534168601036072 + 50.0 * 6.2073774337768555
Epoch 2360, val loss: 1.376197099685669
Epoch 2370, training loss: 310.3348693847656 = 0.018265429884195328 + 50.0 * 6.206331729888916
Epoch 2370, val loss: 1.3792823553085327
Epoch 2380, training loss: 310.31817626953125 = 0.018006879836320877 + 50.0 * 6.206003189086914
Epoch 2380, val loss: 1.3821033239364624
Epoch 2390, training loss: 310.4139404296875 = 0.017756329849362373 + 50.0 * 6.207923889160156
Epoch 2390, val loss: 1.3847079277038574
Epoch 2400, training loss: 310.3066711425781 = 0.017502177506685257 + 50.0 * 6.205783367156982
Epoch 2400, val loss: 1.3876700401306152
Epoch 2410, training loss: 310.3822326660156 = 0.017259106040000916 + 50.0 * 6.20729923248291
Epoch 2410, val loss: 1.3910192251205444
Epoch 2420, training loss: 310.458984375 = 0.017025204375386238 + 50.0 * 6.208839416503906
Epoch 2420, val loss: 1.393757700920105
Epoch 2430, training loss: 310.4854431152344 = 0.016784099861979485 + 50.0 * 6.2093729972839355
Epoch 2430, val loss: 1.3965330123901367
Epoch 2440, training loss: 310.404296875 = 0.01654748246073723 + 50.0 * 6.207755088806152
Epoch 2440, val loss: 1.3990685939788818
Epoch 2450, training loss: 310.2489318847656 = 0.016317876055836678 + 50.0 * 6.204652309417725
Epoch 2450, val loss: 1.4016245603561401
Epoch 2460, training loss: 310.2396545410156 = 0.01609848439693451 + 50.0 * 6.204470634460449
Epoch 2460, val loss: 1.4040350914001465
Epoch 2470, training loss: 310.3329162597656 = 0.015887079760432243 + 50.0 * 6.206340312957764
Epoch 2470, val loss: 1.406302809715271
Epoch 2480, training loss: 310.3474426269531 = 0.015674354508519173 + 50.0 * 6.206634998321533
Epoch 2480, val loss: 1.4088380336761475
Epoch 2490, training loss: 310.3346862792969 = 0.01546687912195921 + 50.0 * 6.206384181976318
Epoch 2490, val loss: 1.4121798276901245
Epoch 2500, training loss: 310.3247985839844 = 0.015263483859598637 + 50.0 * 6.206191062927246
Epoch 2500, val loss: 1.4145151376724243
Epoch 2510, training loss: 310.30560302734375 = 0.01506300549954176 + 50.0 * 6.205810546875
Epoch 2510, val loss: 1.4173271656036377
Epoch 2520, training loss: 310.2531433105469 = 0.014872811734676361 + 50.0 * 6.204765796661377
Epoch 2520, val loss: 1.4201608896255493
Epoch 2530, training loss: 310.248046875 = 0.014680035412311554 + 50.0 * 6.204667091369629
Epoch 2530, val loss: 1.4224779605865479
Epoch 2540, training loss: 310.25244140625 = 0.014491629786789417 + 50.0 * 6.204758644104004
Epoch 2540, val loss: 1.4247556924819946
Epoch 2550, training loss: 310.22882080078125 = 0.014310207217931747 + 50.0 * 6.204290390014648
Epoch 2550, val loss: 1.4275988340377808
Epoch 2560, training loss: 310.3014221191406 = 0.014131338335573673 + 50.0 * 6.205745697021484
Epoch 2560, val loss: 1.4300111532211304
Epoch 2570, training loss: 310.2488708496094 = 0.013954670168459415 + 50.0 * 6.20469856262207
Epoch 2570, val loss: 1.432221531867981
Epoch 2580, training loss: 310.20172119140625 = 0.013780639506876469 + 50.0 * 6.203758239746094
Epoch 2580, val loss: 1.4346120357513428
Epoch 2590, training loss: 310.4751281738281 = 0.013608761131763458 + 50.0 * 6.209230422973633
Epoch 2590, val loss: 1.4362329244613647
Epoch 2600, training loss: 310.25347900390625 = 0.013438649475574493 + 50.0 * 6.204801082611084
Epoch 2600, val loss: 1.4395288228988647
Epoch 2610, training loss: 310.176513671875 = 0.013276041485369205 + 50.0 * 6.2032647132873535
Epoch 2610, val loss: 1.4420952796936035
Epoch 2620, training loss: 310.130615234375 = 0.013116326183080673 + 50.0 * 6.20235013961792
Epoch 2620, val loss: 1.4443477392196655
Epoch 2630, training loss: 310.1639404296875 = 0.01296231709420681 + 50.0 * 6.203019618988037
Epoch 2630, val loss: 1.4466056823730469
Epoch 2640, training loss: 310.36712646484375 = 0.012812638655304909 + 50.0 * 6.207086086273193
Epoch 2640, val loss: 1.4488259553909302
Epoch 2650, training loss: 310.1864318847656 = 0.0126610416918993 + 50.0 * 6.203475475311279
Epoch 2650, val loss: 1.4513744115829468
Epoch 2660, training loss: 310.15155029296875 = 0.01251414604485035 + 50.0 * 6.202780723571777
Epoch 2660, val loss: 1.453764796257019
Epoch 2670, training loss: 310.30755615234375 = 0.012378069572150707 + 50.0 * 6.205903053283691
Epoch 2670, val loss: 1.4559471607208252
Epoch 2680, training loss: 310.14801025390625 = 0.01222899928689003 + 50.0 * 6.2027153968811035
Epoch 2680, val loss: 1.4580415487289429
Epoch 2690, training loss: 310.1412353515625 = 0.012088320218026638 + 50.0 * 6.202583312988281
Epoch 2690, val loss: 1.4600238800048828
Epoch 2700, training loss: 310.12213134765625 = 0.011951573193073273 + 50.0 * 6.202203273773193
Epoch 2700, val loss: 1.4622573852539062
Epoch 2710, training loss: 310.2217712402344 = 0.011815485544502735 + 50.0 * 6.204199314117432
Epoch 2710, val loss: 1.4639939069747925
Epoch 2720, training loss: 310.25830078125 = 0.011677811853587627 + 50.0 * 6.20493221282959
Epoch 2720, val loss: 1.466120719909668
Epoch 2730, training loss: 310.0939636230469 = 0.011551630683243275 + 50.0 * 6.201647758483887
Epoch 2730, val loss: 1.4685561656951904
Epoch 2740, training loss: 310.0508728027344 = 0.011421707458794117 + 50.0 * 6.200789451599121
Epoch 2740, val loss: 1.4706302881240845
Epoch 2750, training loss: 310.02508544921875 = 0.011301673948764801 + 50.0 * 6.200275897979736
Epoch 2750, val loss: 1.4728931188583374
Epoch 2760, training loss: 310.12164306640625 = 0.011186113581061363 + 50.0 * 6.202208995819092
Epoch 2760, val loss: 1.475054383277893
Epoch 2770, training loss: 310.2891845703125 = 0.011068382300436497 + 50.0 * 6.205562114715576
Epoch 2770, val loss: 1.4772194623947144
Epoch 2780, training loss: 310.02020263671875 = 0.010936060920357704 + 50.0 * 6.200185775756836
Epoch 2780, val loss: 1.4784907102584839
Epoch 2790, training loss: 310.0279846191406 = 0.010816547088325024 + 50.0 * 6.200343608856201
Epoch 2790, val loss: 1.4806580543518066
Epoch 2800, training loss: 310.0037536621094 = 0.010706898756325245 + 50.0 * 6.1998610496521
Epoch 2800, val loss: 1.482822060585022
Epoch 2810, training loss: 310.0066833496094 = 0.010600405745208263 + 50.0 * 6.199921607971191
Epoch 2810, val loss: 1.4846562147140503
Epoch 2820, training loss: 310.5069885253906 = 0.010500558651983738 + 50.0 * 6.209929943084717
Epoch 2820, val loss: 1.4860187768936157
Epoch 2830, training loss: 310.2848205566406 = 0.010379270650446415 + 50.0 * 6.205489158630371
Epoch 2830, val loss: 1.4879597425460815
Epoch 2840, training loss: 310.0819091796875 = 0.01027212105691433 + 50.0 * 6.201432704925537
Epoch 2840, val loss: 1.4905365705490112
Epoch 2850, training loss: 309.9794006347656 = 0.010165216401219368 + 50.0 * 6.199384689331055
Epoch 2850, val loss: 1.4924399852752686
Epoch 2860, training loss: 309.97442626953125 = 0.010065360926091671 + 50.0 * 6.199286937713623
Epoch 2860, val loss: 1.494128704071045
Epoch 2870, training loss: 310.21136474609375 = 0.009969144128262997 + 50.0 * 6.2040276527404785
Epoch 2870, val loss: 1.4958988428115845
Epoch 2880, training loss: 310.0410461425781 = 0.009866131469607353 + 50.0 * 6.200623512268066
Epoch 2880, val loss: 1.497815728187561
Epoch 2890, training loss: 310.0186462402344 = 0.009768076241016388 + 50.0 * 6.2001776695251465
Epoch 2890, val loss: 1.499354362487793
Epoch 2900, training loss: 310.13702392578125 = 0.009670689702033997 + 50.0 * 6.202547073364258
Epoch 2900, val loss: 1.5007456541061401
Epoch 2910, training loss: 310.1180725097656 = 0.009573765099048615 + 50.0 * 6.202169895172119
Epoch 2910, val loss: 1.5029234886169434
Epoch 2920, training loss: 309.9598083496094 = 0.009482079185545444 + 50.0 * 6.1990065574646
Epoch 2920, val loss: 1.505232334136963
Epoch 2930, training loss: 309.94171142578125 = 0.009393204003572464 + 50.0 * 6.198646068572998
Epoch 2930, val loss: 1.5070048570632935
Epoch 2940, training loss: 309.91448974609375 = 0.009304523468017578 + 50.0 * 6.198103427886963
Epoch 2940, val loss: 1.508958339691162
Epoch 2950, training loss: 309.9715270996094 = 0.009221053682267666 + 50.0 * 6.199245929718018
Epoch 2950, val loss: 1.510715365409851
Epoch 2960, training loss: 310.18267822265625 = 0.009140903130173683 + 50.0 * 6.203470230102539
Epoch 2960, val loss: 1.5126194953918457
Epoch 2970, training loss: 310.1063537597656 = 0.00905068963766098 + 50.0 * 6.201945781707764
Epoch 2970, val loss: 1.514344334602356
Epoch 2980, training loss: 310.05291748046875 = 0.008962624706327915 + 50.0 * 6.200879096984863
Epoch 2980, val loss: 1.5158284902572632
Epoch 2990, training loss: 309.9720458984375 = 0.008878206834197044 + 50.0 * 6.199263572692871
Epoch 2990, val loss: 1.5170954465866089
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8365840801265156
=== training gcn model ===
Epoch 0, training loss: 431.7797546386719 = 1.9355971813201904 + 50.0 * 8.596882820129395
Epoch 0, val loss: 1.9315301179885864
Epoch 10, training loss: 431.7425842285156 = 1.9269094467163086 + 50.0 * 8.5963134765625
Epoch 10, val loss: 1.9232853651046753
Epoch 20, training loss: 431.50885009765625 = 1.916280746459961 + 50.0 * 8.591851234436035
Epoch 20, val loss: 1.9128230810165405
Epoch 30, training loss: 429.8135681152344 = 1.9027190208435059 + 50.0 * 8.55821704864502
Epoch 30, val loss: 1.8992054462432861
Epoch 40, training loss: 419.48406982421875 = 1.885936975479126 + 50.0 * 8.35196304321289
Epoch 40, val loss: 1.882728099822998
Epoch 50, training loss: 394.3027038574219 = 1.867616891860962 + 50.0 * 7.8487019538879395
Epoch 50, val loss: 1.8655598163604736
Epoch 60, training loss: 370.4732971191406 = 1.8545966148376465 + 50.0 * 7.372374534606934
Epoch 60, val loss: 1.8539844751358032
Epoch 70, training loss: 355.24957275390625 = 1.8441858291625977 + 50.0 * 7.068108081817627
Epoch 70, val loss: 1.8435240983963013
Epoch 80, training loss: 347.941162109375 = 1.8334680795669556 + 50.0 * 6.922153949737549
Epoch 80, val loss: 1.8329228162765503
Epoch 90, training loss: 342.2335510253906 = 1.8221224546432495 + 50.0 * 6.808228492736816
Epoch 90, val loss: 1.8221197128295898
Epoch 100, training loss: 338.2002258300781 = 1.811289668083191 + 50.0 * 6.727778911590576
Epoch 100, val loss: 1.8117886781692505
Epoch 110, training loss: 335.3731689453125 = 1.8008575439453125 + 50.0 * 6.671445846557617
Epoch 110, val loss: 1.8016626834869385
Epoch 120, training loss: 333.10357666015625 = 1.7904623746871948 + 50.0 * 6.626262187957764
Epoch 120, val loss: 1.7914355993270874
Epoch 130, training loss: 331.1797790527344 = 1.7801040410995483 + 50.0 * 6.587993621826172
Epoch 130, val loss: 1.7811031341552734
Epoch 140, training loss: 329.6817321777344 = 1.7693382501602173 + 50.0 * 6.5582475662231445
Epoch 140, val loss: 1.770507574081421
Epoch 150, training loss: 328.3963623046875 = 1.7575998306274414 + 50.0 * 6.532775402069092
Epoch 150, val loss: 1.759215235710144
Epoch 160, training loss: 327.3986511230469 = 1.7448607683181763 + 50.0 * 6.513075828552246
Epoch 160, val loss: 1.7471446990966797
Epoch 170, training loss: 326.540771484375 = 1.7309573888778687 + 50.0 * 6.496196269989014
Epoch 170, val loss: 1.7341058254241943
Epoch 180, training loss: 325.7741394042969 = 1.7158852815628052 + 50.0 * 6.481165409088135
Epoch 180, val loss: 1.7200225591659546
Epoch 190, training loss: 325.1661682128906 = 1.6994706392288208 + 50.0 * 6.469334125518799
Epoch 190, val loss: 1.7047638893127441
Epoch 200, training loss: 324.66851806640625 = 1.6815454959869385 + 50.0 * 6.4597392082214355
Epoch 200, val loss: 1.688144564628601
Epoch 210, training loss: 324.0621643066406 = 1.6622134447097778 + 50.0 * 6.447999000549316
Epoch 210, val loss: 1.6703730821609497
Epoch 220, training loss: 323.5769348144531 = 1.6414717435836792 + 50.0 * 6.438709259033203
Epoch 220, val loss: 1.6514320373535156
Epoch 230, training loss: 323.3122863769531 = 1.6192536354064941 + 50.0 * 6.4338603019714355
Epoch 230, val loss: 1.6313064098358154
Epoch 240, training loss: 322.80267333984375 = 1.595696210861206 + 50.0 * 6.424139499664307
Epoch 240, val loss: 1.6101276874542236
Epoch 250, training loss: 322.4591979980469 = 1.5709422826766968 + 50.0 * 6.417764663696289
Epoch 250, val loss: 1.5880156755447388
Epoch 260, training loss: 322.276611328125 = 1.5452046394348145 + 50.0 * 6.414628028869629
Epoch 260, val loss: 1.5652803182601929
Epoch 270, training loss: 321.8821105957031 = 1.5185531377792358 + 50.0 * 6.407271385192871
Epoch 270, val loss: 1.5420451164245605
Epoch 280, training loss: 321.5672912597656 = 1.491460919380188 + 50.0 * 6.401516914367676
Epoch 280, val loss: 1.5187504291534424
Epoch 290, training loss: 321.266357421875 = 1.4640867710113525 + 50.0 * 6.396045684814453
Epoch 290, val loss: 1.4955195188522339
Epoch 300, training loss: 321.00555419921875 = 1.4365084171295166 + 50.0 * 6.391380786895752
Epoch 300, val loss: 1.472562551498413
Epoch 310, training loss: 320.8279113769531 = 1.409006953239441 + 50.0 * 6.388378143310547
Epoch 310, val loss: 1.4499367475509644
Epoch 320, training loss: 320.525634765625 = 1.3816912174224854 + 50.0 * 6.382879257202148
Epoch 320, val loss: 1.4278299808502197
Epoch 330, training loss: 320.36883544921875 = 1.3546961545944214 + 50.0 * 6.380282402038574
Epoch 330, val loss: 1.4063770771026611
Epoch 340, training loss: 320.05291748046875 = 1.328104019165039 + 50.0 * 6.3744964599609375
Epoch 340, val loss: 1.3856103420257568
Epoch 350, training loss: 319.87481689453125 = 1.3018051385879517 + 50.0 * 6.3714599609375
Epoch 350, val loss: 1.3653924465179443
Epoch 360, training loss: 319.7710876464844 = 1.275800108909607 + 50.0 * 6.369905948638916
Epoch 360, val loss: 1.3457238674163818
Epoch 370, training loss: 319.4360046386719 = 1.2502527236938477 + 50.0 * 6.363715171813965
Epoch 370, val loss: 1.3266857862472534
Epoch 380, training loss: 319.2070007324219 = 1.2251209020614624 + 50.0 * 6.359637260437012
Epoch 380, val loss: 1.3082306385040283
Epoch 390, training loss: 319.0385437011719 = 1.200394630432129 + 50.0 * 6.356762409210205
Epoch 390, val loss: 1.290324330329895
Epoch 400, training loss: 318.9808044433594 = 1.1761361360549927 + 50.0 * 6.356093406677246
Epoch 400, val loss: 1.2730250358581543
Epoch 410, training loss: 318.72308349609375 = 1.152258038520813 + 50.0 * 6.35141658782959
Epoch 410, val loss: 1.2561914920806885
Epoch 420, training loss: 318.441162109375 = 1.1287637948989868 + 50.0 * 6.346248149871826
Epoch 420, val loss: 1.2398269176483154
Epoch 430, training loss: 318.36297607421875 = 1.105886697769165 + 50.0 * 6.345141887664795
Epoch 430, val loss: 1.2241592407226562
Epoch 440, training loss: 318.1730651855469 = 1.0834273099899292 + 50.0 * 6.341792583465576
Epoch 440, val loss: 1.2089295387268066
Epoch 450, training loss: 318.04888916015625 = 1.0614880323410034 + 50.0 * 6.339747905731201
Epoch 450, val loss: 1.1941664218902588
Epoch 460, training loss: 317.8110656738281 = 1.0399608612060547 + 50.0 * 6.335422515869141
Epoch 460, val loss: 1.1798722743988037
Epoch 470, training loss: 317.7334899902344 = 1.0189915895462036 + 50.0 * 6.334290027618408
Epoch 470, val loss: 1.1660786867141724
Epoch 480, training loss: 317.5672912597656 = 0.9981299042701721 + 50.0 * 6.331383228302002
Epoch 480, val loss: 1.1524336338043213
Epoch 490, training loss: 317.34295654296875 = 0.9778420925140381 + 50.0 * 6.3273024559021
Epoch 490, val loss: 1.139291524887085
Epoch 500, training loss: 317.2037658691406 = 0.9579904675483704 + 50.0 * 6.324915885925293
Epoch 500, val loss: 1.126637578010559
Epoch 510, training loss: 317.4465026855469 = 0.9384949803352356 + 50.0 * 6.330160140991211
Epoch 510, val loss: 1.1143139600753784
Epoch 520, training loss: 317.0014343261719 = 0.9192743897438049 + 50.0 * 6.321642875671387
Epoch 520, val loss: 1.1020475625991821
Epoch 530, training loss: 316.8041076660156 = 0.9004069566726685 + 50.0 * 6.318073749542236
Epoch 530, val loss: 1.090243935585022
Epoch 540, training loss: 317.10345458984375 = 0.8819339275360107 + 50.0 * 6.324430465698242
Epoch 540, val loss: 1.0786067247390747
Epoch 550, training loss: 316.60736083984375 = 0.8634786009788513 + 50.0 * 6.314877986907959
Epoch 550, val loss: 1.0673068761825562
Epoch 560, training loss: 316.48577880859375 = 0.845429003238678 + 50.0 * 6.312807083129883
Epoch 560, val loss: 1.0563164949417114
Epoch 570, training loss: 316.39862060546875 = 0.8277339935302734 + 50.0 * 6.311418056488037
Epoch 570, val loss: 1.0456539392471313
Epoch 580, training loss: 316.4543151855469 = 0.8101962804794312 + 50.0 * 6.312881946563721
Epoch 580, val loss: 1.0351170301437378
Epoch 590, training loss: 316.13616943359375 = 0.793024480342865 + 50.0 * 6.306862831115723
Epoch 590, val loss: 1.0246447324752808
Epoch 600, training loss: 316.03106689453125 = 0.7760682106018066 + 50.0 * 6.305099964141846
Epoch 600, val loss: 1.0145670175552368
Epoch 610, training loss: 316.27093505859375 = 0.7594053745269775 + 50.0 * 6.310230731964111
Epoch 610, val loss: 1.0050463676452637
Epoch 620, training loss: 315.92767333984375 = 0.7428469061851501 + 50.0 * 6.303696155548096
Epoch 620, val loss: 0.9954155683517456
Epoch 630, training loss: 315.7908630371094 = 0.7266895174980164 + 50.0 * 6.301283836364746
Epoch 630, val loss: 0.9860550761222839
Epoch 640, training loss: 315.6826477050781 = 0.7108238935470581 + 50.0 * 6.299436569213867
Epoch 640, val loss: 0.9771736264228821
Epoch 650, training loss: 315.72607421875 = 0.6951637864112854 + 50.0 * 6.3006181716918945
Epoch 650, val loss: 0.9685795307159424
Epoch 660, training loss: 315.4776916503906 = 0.6795487403869629 + 50.0 * 6.295963287353516
Epoch 660, val loss: 0.9600599408149719
Epoch 670, training loss: 315.37982177734375 = 0.6643319129943848 + 50.0 * 6.294309616088867
Epoch 670, val loss: 0.9519549608230591
Epoch 680, training loss: 315.7474670410156 = 0.6493756771087646 + 50.0 * 6.301961898803711
Epoch 680, val loss: 0.9440628886222839
Epoch 690, training loss: 315.23724365234375 = 0.6343637108802795 + 50.0 * 6.292057514190674
Epoch 690, val loss: 0.9363465309143066
Epoch 700, training loss: 315.1404113769531 = 0.6197355389595032 + 50.0 * 6.290413856506348
Epoch 700, val loss: 0.9289677143096924
Epoch 710, training loss: 315.0496520996094 = 0.6053089499473572 + 50.0 * 6.288886547088623
Epoch 710, val loss: 0.9218842387199402
Epoch 720, training loss: 315.18231201171875 = 0.5909309387207031 + 50.0 * 6.29182767868042
Epoch 720, val loss: 0.9150457978248596
Epoch 730, training loss: 315.1595153808594 = 0.5766384601593018 + 50.0 * 6.2916579246521
Epoch 730, val loss: 0.9082786440849304
Epoch 740, training loss: 314.86358642578125 = 0.5627020001411438 + 50.0 * 6.286017417907715
Epoch 740, val loss: 0.901771605014801
Epoch 750, training loss: 314.7388916015625 = 0.5489588975906372 + 50.0 * 6.283798694610596
Epoch 750, val loss: 0.8957960605621338
Epoch 760, training loss: 314.65753173828125 = 0.5354660153388977 + 50.0 * 6.282441139221191
Epoch 760, val loss: 0.8900081515312195
Epoch 770, training loss: 314.642333984375 = 0.5221145153045654 + 50.0 * 6.282403945922852
Epoch 770, val loss: 0.884396493434906
Epoch 780, training loss: 314.7205505371094 = 0.508884072303772 + 50.0 * 6.284233570098877
Epoch 780, val loss: 0.879059374332428
Epoch 790, training loss: 314.6058349609375 = 0.49561765789985657 + 50.0 * 6.282204627990723
Epoch 790, val loss: 0.8740019798278809
Epoch 800, training loss: 314.46417236328125 = 0.4827806353569031 + 50.0 * 6.279627799987793
Epoch 800, val loss: 0.8692406415939331
Epoch 810, training loss: 314.3458251953125 = 0.47018367052078247 + 50.0 * 6.277513027191162
Epoch 810, val loss: 0.8650648593902588
Epoch 820, training loss: 314.2443542480469 = 0.45790067315101624 + 50.0 * 6.275729656219482
Epoch 820, val loss: 0.8611653447151184
Epoch 830, training loss: 314.18603515625 = 0.4458662271499634 + 50.0 * 6.274803638458252
Epoch 830, val loss: 0.857483983039856
Epoch 840, training loss: 314.5361022949219 = 0.4341239333152771 + 50.0 * 6.282039642333984
Epoch 840, val loss: 0.8539745807647705
Epoch 850, training loss: 314.392578125 = 0.42234498262405396 + 50.0 * 6.279405117034912
Epoch 850, val loss: 0.8510271906852722
Epoch 860, training loss: 314.0281982421875 = 0.410987913608551 + 50.0 * 6.27234411239624
Epoch 860, val loss: 0.8483947515487671
Epoch 870, training loss: 313.99725341796875 = 0.3999861776828766 + 50.0 * 6.271944999694824
Epoch 870, val loss: 0.8461155891418457
Epoch 880, training loss: 313.9187316894531 = 0.3892990052700043 + 50.0 * 6.270588397979736
Epoch 880, val loss: 0.844231128692627
Epoch 890, training loss: 313.9971618652344 = 0.3788624107837677 + 50.0 * 6.272365570068359
Epoch 890, val loss: 0.8424522280693054
Epoch 900, training loss: 313.87811279296875 = 0.3686763644218445 + 50.0 * 6.270188808441162
Epoch 900, val loss: 0.8410927057266235
Epoch 910, training loss: 313.82861328125 = 0.35880324244499207 + 50.0 * 6.2693963050842285
Epoch 910, val loss: 0.8399474024772644
Epoch 920, training loss: 313.8245544433594 = 0.3492572605609894 + 50.0 * 6.269506454467773
Epoch 920, val loss: 0.8389897346496582
Epoch 930, training loss: 313.80377197265625 = 0.33998551964759827 + 50.0 * 6.269275665283203
Epoch 930, val loss: 0.8386552333831787
Epoch 940, training loss: 313.64788818359375 = 0.33086222410202026 + 50.0 * 6.266340255737305
Epoch 940, val loss: 0.8388185501098633
Epoch 950, training loss: 313.6306457519531 = 0.3221537470817566 + 50.0 * 6.266170024871826
Epoch 950, val loss: 0.839023768901825
Epoch 960, training loss: 313.603271484375 = 0.31362730264663696 + 50.0 * 6.2657928466796875
Epoch 960, val loss: 0.8393691182136536
Epoch 970, training loss: 313.58514404296875 = 0.305370569229126 + 50.0 * 6.26559591293335
Epoch 970, val loss: 0.8396061062812805
Epoch 980, training loss: 313.5010681152344 = 0.2973279356956482 + 50.0 * 6.264074802398682
Epoch 980, val loss: 0.8405569195747375
Epoch 990, training loss: 313.397216796875 = 0.28953075408935547 + 50.0 * 6.2621541023254395
Epoch 990, val loss: 0.8412482142448425
Epoch 1000, training loss: 313.3486328125 = 0.28198057413101196 + 50.0 * 6.261332988739014
Epoch 1000, val loss: 0.8424445390701294
Epoch 1010, training loss: 313.59051513671875 = 0.27462875843048096 + 50.0 * 6.266317844390869
Epoch 1010, val loss: 0.843993604183197
Epoch 1020, training loss: 313.3309326171875 = 0.2674829661846161 + 50.0 * 6.2612690925598145
Epoch 1020, val loss: 0.8446755409240723
Epoch 1030, training loss: 313.2143859863281 = 0.26050111651420593 + 50.0 * 6.259077548980713
Epoch 1030, val loss: 0.8463699221611023
Epoch 1040, training loss: 313.15057373046875 = 0.2537996470928192 + 50.0 * 6.257935047149658
Epoch 1040, val loss: 0.8480824828147888
Epoch 1050, training loss: 313.226318359375 = 0.24726617336273193 + 50.0 * 6.259581089019775
Epoch 1050, val loss: 0.8498973846435547
Epoch 1060, training loss: 313.2235107421875 = 0.2409009337425232 + 50.0 * 6.259652137756348
Epoch 1060, val loss: 0.8514338731765747
Epoch 1070, training loss: 313.23992919921875 = 0.2345706969499588 + 50.0 * 6.260107517242432
Epoch 1070, val loss: 0.8536062836647034
Epoch 1080, training loss: 313.0110778808594 = 0.2285187989473343 + 50.0 * 6.255651473999023
Epoch 1080, val loss: 0.8553171753883362
Epoch 1090, training loss: 312.97442626953125 = 0.22268645465373993 + 50.0 * 6.255034923553467
Epoch 1090, val loss: 0.8576467633247375
Epoch 1100, training loss: 312.91259765625 = 0.21700651943683624 + 50.0 * 6.253911972045898
Epoch 1100, val loss: 0.8599849343299866
Epoch 1110, training loss: 313.14703369140625 = 0.21150557696819305 + 50.0 * 6.2587103843688965
Epoch 1110, val loss: 0.8622035384178162
Epoch 1120, training loss: 312.9377136230469 = 0.20600248873233795 + 50.0 * 6.254634380340576
Epoch 1120, val loss: 0.8647212982177734
Epoch 1130, training loss: 312.8724670410156 = 0.20073369145393372 + 50.0 * 6.253434658050537
Epoch 1130, val loss: 0.8668513298034668
Epoch 1140, training loss: 312.9639892578125 = 0.1956164687871933 + 50.0 * 6.255367279052734
Epoch 1140, val loss: 0.8691504597663879
Epoch 1150, training loss: 312.8188171386719 = 0.19057214260101318 + 50.0 * 6.252564430236816
Epoch 1150, val loss: 0.8722812533378601
Epoch 1160, training loss: 312.8522033691406 = 0.185678169131279 + 50.0 * 6.253330707550049
Epoch 1160, val loss: 0.8747151494026184
Epoch 1170, training loss: 312.69818115234375 = 0.1809515655040741 + 50.0 * 6.250344753265381
Epoch 1170, val loss: 0.8777151703834534
Epoch 1180, training loss: 312.65777587890625 = 0.17638655006885529 + 50.0 * 6.249627590179443
Epoch 1180, val loss: 0.8804848194122314
Epoch 1190, training loss: 312.7279052734375 = 0.17194616794586182 + 50.0 * 6.251119136810303
Epoch 1190, val loss: 0.8834190964698792
Epoch 1200, training loss: 312.6621398925781 = 0.16755370795726776 + 50.0 * 6.249891757965088
Epoch 1200, val loss: 0.8865081667900085
Epoch 1210, training loss: 312.76580810546875 = 0.16328248381614685 + 50.0 * 6.252050876617432
Epoch 1210, val loss: 0.8895944952964783
Epoch 1220, training loss: 312.6038513183594 = 0.15917953848838806 + 50.0 * 6.2488932609558105
Epoch 1220, val loss: 0.8925910592079163
Epoch 1230, training loss: 313.0140075683594 = 0.15513089299201965 + 50.0 * 6.257177829742432
Epoch 1230, val loss: 0.8959446549415588
Epoch 1240, training loss: 312.5719909667969 = 0.15115821361541748 + 50.0 * 6.248416900634766
Epoch 1240, val loss: 0.8989426493644714
Epoch 1250, training loss: 312.43450927734375 = 0.14734826982021332 + 50.0 * 6.245743274688721
Epoch 1250, val loss: 0.9024057984352112
Epoch 1260, training loss: 312.401123046875 = 0.1436764895915985 + 50.0 * 6.245148658752441
Epoch 1260, val loss: 0.9059202671051025
Epoch 1270, training loss: 312.5658264160156 = 0.1401120126247406 + 50.0 * 6.248514175415039
Epoch 1270, val loss: 0.9097309112548828
Epoch 1280, training loss: 312.4130859375 = 0.13657619059085846 + 50.0 * 6.245530128479004
Epoch 1280, val loss: 0.9125038385391235
Epoch 1290, training loss: 312.4127197265625 = 0.13315659761428833 + 50.0 * 6.245591640472412
Epoch 1290, val loss: 0.9163621664047241
Epoch 1300, training loss: 312.3831481933594 = 0.1298556923866272 + 50.0 * 6.245065689086914
Epoch 1300, val loss: 0.9197083711624146
Epoch 1310, training loss: 312.52667236328125 = 0.12664581835269928 + 50.0 * 6.248000621795654
Epoch 1310, val loss: 0.9235085844993591
Epoch 1320, training loss: 312.3460388183594 = 0.12351198494434357 + 50.0 * 6.244450569152832
Epoch 1320, val loss: 0.9278751611709595
Epoch 1330, training loss: 312.3685607910156 = 0.12046636641025543 + 50.0 * 6.244962215423584
Epoch 1330, val loss: 0.9315158128738403
Epoch 1340, training loss: 312.3466491699219 = 0.11748185008764267 + 50.0 * 6.2445831298828125
Epoch 1340, val loss: 0.9355682134628296
Epoch 1350, training loss: 312.22442626953125 = 0.1146264374256134 + 50.0 * 6.242196083068848
Epoch 1350, val loss: 0.9389995336532593
Epoch 1360, training loss: 312.15313720703125 = 0.11183643341064453 + 50.0 * 6.240825653076172
Epoch 1360, val loss: 0.9433351159095764
Epoch 1370, training loss: 312.2162170410156 = 0.10914772003889084 + 50.0 * 6.242141246795654
Epoch 1370, val loss: 0.9473353624343872
Epoch 1380, training loss: 312.4466247558594 = 0.10652966052293777 + 50.0 * 6.246801853179932
Epoch 1380, val loss: 0.9510243535041809
Epoch 1390, training loss: 312.2084045410156 = 0.10393594950437546 + 50.0 * 6.24208927154541
Epoch 1390, val loss: 0.9553136825561523
Epoch 1400, training loss: 312.19976806640625 = 0.10144113004207611 + 50.0 * 6.241966724395752
Epoch 1400, val loss: 0.9594020247459412
Epoch 1410, training loss: 312.1626892089844 = 0.09903080016374588 + 50.0 * 6.241272926330566
Epoch 1410, val loss: 0.9635269641876221
Epoch 1420, training loss: 312.0855407714844 = 0.09667400270700455 + 50.0 * 6.239777088165283
Epoch 1420, val loss: 0.9679954051971436
Epoch 1430, training loss: 312.1981201171875 = 0.09444723278284073 + 50.0 * 6.2420735359191895
Epoch 1430, val loss: 0.9722520709037781
Epoch 1440, training loss: 312.0166320800781 = 0.09217667579650879 + 50.0 * 6.238489627838135
Epoch 1440, val loss: 0.9764597415924072
Epoch 1450, training loss: 312.0001220703125 = 0.09002309292554855 + 50.0 * 6.23820161819458
Epoch 1450, val loss: 0.980667769908905
Epoch 1460, training loss: 312.18707275390625 = 0.0879393219947815 + 50.0 * 6.241982460021973
Epoch 1460, val loss: 0.9849876165390015
Epoch 1470, training loss: 311.94921875 = 0.08588580787181854 + 50.0 * 6.237266540527344
Epoch 1470, val loss: 0.989651620388031
Epoch 1480, training loss: 312.062744140625 = 0.08391322940587997 + 50.0 * 6.23957633972168
Epoch 1480, val loss: 0.9944541454315186
Epoch 1490, training loss: 311.9352722167969 = 0.08197475224733353 + 50.0 * 6.23706579208374
Epoch 1490, val loss: 0.9984654188156128
Epoch 1500, training loss: 311.876953125 = 0.08008594810962677 + 50.0 * 6.235937595367432
Epoch 1500, val loss: 1.0029765367507935
Epoch 1510, training loss: 311.8550720214844 = 0.0782955065369606 + 50.0 * 6.235535144805908
Epoch 1510, val loss: 1.0075881481170654
Epoch 1520, training loss: 311.91278076171875 = 0.07655002921819687 + 50.0 * 6.236724853515625
Epoch 1520, val loss: 1.0119434595108032
Epoch 1530, training loss: 311.9742736816406 = 0.07485289126634598 + 50.0 * 6.237988471984863
Epoch 1530, val loss: 1.016576886177063
Epoch 1540, training loss: 311.89727783203125 = 0.07317924499511719 + 50.0 * 6.2364821434021
Epoch 1540, val loss: 1.0213958024978638
Epoch 1550, training loss: 311.964111328125 = 0.07157157361507416 + 50.0 * 6.237851142883301
Epoch 1550, val loss: 1.0255913734436035
Epoch 1560, training loss: 311.9129943847656 = 0.06998088955879211 + 50.0 * 6.236860275268555
Epoch 1560, val loss: 1.030839204788208
Epoch 1570, training loss: 311.81939697265625 = 0.06843668222427368 + 50.0 * 6.235018730163574
Epoch 1570, val loss: 1.035030722618103
Epoch 1580, training loss: 311.7548828125 = 0.06696838140487671 + 50.0 * 6.233758449554443
Epoch 1580, val loss: 1.039872646331787
Epoch 1590, training loss: 311.8456726074219 = 0.06554693728685379 + 50.0 * 6.235602378845215
Epoch 1590, val loss: 1.044776439666748
Epoch 1600, training loss: 311.78778076171875 = 0.06413339078426361 + 50.0 * 6.234472751617432
Epoch 1600, val loss: 1.0494719743728638
Epoch 1610, training loss: 311.6960144042969 = 0.06275157630443573 + 50.0 * 6.232665538787842
Epoch 1610, val loss: 1.0537173748016357
Epoch 1620, training loss: 311.64544677734375 = 0.061440981924533844 + 50.0 * 6.231680393218994
Epoch 1620, val loss: 1.0588308572769165
Epoch 1630, training loss: 311.6553955078125 = 0.06017197668552399 + 50.0 * 6.231904029846191
Epoch 1630, val loss: 1.0634586811065674
Epoch 1640, training loss: 311.9540100097656 = 0.05893629416823387 + 50.0 * 6.23790168762207
Epoch 1640, val loss: 1.0680599212646484
Epoch 1650, training loss: 311.9695129394531 = 0.05767417326569557 + 50.0 * 6.238236427307129
Epoch 1650, val loss: 1.0724475383758545
Epoch 1660, training loss: 311.64013671875 = 0.056477706879377365 + 50.0 * 6.231673240661621
Epoch 1660, val loss: 1.0775822401046753
Epoch 1670, training loss: 311.5740661621094 = 0.055320076644420624 + 50.0 * 6.230374813079834
Epoch 1670, val loss: 1.0820873975753784
Epoch 1680, training loss: 311.6062927246094 = 0.05421721935272217 + 50.0 * 6.231041431427002
Epoch 1680, val loss: 1.086962342262268
Epoch 1690, training loss: 311.7990417480469 = 0.05314008146524429 + 50.0 * 6.234918117523193
Epoch 1690, val loss: 1.0914125442504883
Epoch 1700, training loss: 311.5887451171875 = 0.05205734074115753 + 50.0 * 6.230733394622803
Epoch 1700, val loss: 1.0968295335769653
Epoch 1710, training loss: 311.60894775390625 = 0.05103078484535217 + 50.0 * 6.231158256530762
Epoch 1710, val loss: 1.1009832620620728
Epoch 1720, training loss: 311.5667419433594 = 0.0500187948346138 + 50.0 * 6.230334281921387
Epoch 1720, val loss: 1.1060519218444824
Epoch 1730, training loss: 311.54449462890625 = 0.04903547838330269 + 50.0 * 6.2299089431762695
Epoch 1730, val loss: 1.1110765933990479
Epoch 1740, training loss: 311.7073059082031 = 0.04809503257274628 + 50.0 * 6.233184337615967
Epoch 1740, val loss: 1.1153432130813599
Epoch 1750, training loss: 311.4926452636719 = 0.047147028148174286 + 50.0 * 6.228909969329834
Epoch 1750, val loss: 1.1203428506851196
Epoch 1760, training loss: 311.514892578125 = 0.04623915255069733 + 50.0 * 6.229372978210449
Epoch 1760, val loss: 1.1246933937072754
Epoch 1770, training loss: 311.5349426269531 = 0.045364897698163986 + 50.0 * 6.229791164398193
Epoch 1770, val loss: 1.1296179294586182
Epoch 1780, training loss: 311.46075439453125 = 0.04450756311416626 + 50.0 * 6.228324890136719
Epoch 1780, val loss: 1.134092092514038
Epoch 1790, training loss: 311.5952453613281 = 0.04367426037788391 + 50.0 * 6.23103141784668
Epoch 1790, val loss: 1.1388206481933594
Epoch 1800, training loss: 311.51422119140625 = 0.042849231511354446 + 50.0 * 6.229427337646484
Epoch 1800, val loss: 1.1439841985702515
Epoch 1810, training loss: 311.39923095703125 = 0.042054567486047745 + 50.0 * 6.22714376449585
Epoch 1810, val loss: 1.1482162475585938
Epoch 1820, training loss: 311.3410339355469 = 0.041279636323451996 + 50.0 * 6.22599458694458
Epoch 1820, val loss: 1.153183102607727
Epoch 1830, training loss: 311.6845703125 = 0.04053870216012001 + 50.0 * 6.23288106918335
Epoch 1830, val loss: 1.1579484939575195
Epoch 1840, training loss: 311.4190979003906 = 0.03978300467133522 + 50.0 * 6.227585792541504
Epoch 1840, val loss: 1.161675214767456
Epoch 1850, training loss: 311.3763732910156 = 0.03904658183455467 + 50.0 * 6.226747035980225
Epoch 1850, val loss: 1.1664636135101318
Epoch 1860, training loss: 311.32379150390625 = 0.03835044056177139 + 50.0 * 6.225708484649658
Epoch 1860, val loss: 1.1709264516830444
Epoch 1870, training loss: 311.3796691894531 = 0.03767475113272667 + 50.0 * 6.226839542388916
Epoch 1870, val loss: 1.17535400390625
Epoch 1880, training loss: 311.3875732421875 = 0.03701026365160942 + 50.0 * 6.227011203765869
Epoch 1880, val loss: 1.1797969341278076
Epoch 1890, training loss: 311.3150329589844 = 0.03636620566248894 + 50.0 * 6.2255730628967285
Epoch 1890, val loss: 1.1851067543029785
Epoch 1900, training loss: 311.4085693359375 = 0.035737235099077225 + 50.0 * 6.227456569671631
Epoch 1900, val loss: 1.1890416145324707
Epoch 1910, training loss: 311.2630615234375 = 0.03509639576077461 + 50.0 * 6.224559307098389
Epoch 1910, val loss: 1.1935703754425049
Epoch 1920, training loss: 311.36944580078125 = 0.03449854254722595 + 50.0 * 6.226699352264404
Epoch 1920, val loss: 1.198319673538208
Epoch 1930, training loss: 311.3724060058594 = 0.033902574330568314 + 50.0 * 6.226769924163818
Epoch 1930, val loss: 1.2021074295043945
Epoch 1940, training loss: 311.2251892089844 = 0.03331708535552025 + 50.0 * 6.223837375640869
Epoch 1940, val loss: 1.2070881128311157
Epoch 1950, training loss: 311.20245361328125 = 0.032754212617874146 + 50.0 * 6.22339391708374
Epoch 1950, val loss: 1.2113667726516724
Epoch 1960, training loss: 311.2232360839844 = 0.03221295773983002 + 50.0 * 6.223820209503174
Epoch 1960, val loss: 1.2157093286514282
Epoch 1970, training loss: 311.3353271484375 = 0.03167697787284851 + 50.0 * 6.226073265075684
Epoch 1970, val loss: 1.2198361158370972
Epoch 1980, training loss: 311.3007507324219 = 0.031151864677667618 + 50.0 * 6.225391864776611
Epoch 1980, val loss: 1.224937915802002
Epoch 1990, training loss: 311.16436767578125 = 0.03062739036977291 + 50.0 * 6.22267484664917
Epoch 1990, val loss: 1.2286804914474487
Epoch 2000, training loss: 311.12261962890625 = 0.030132174491882324 + 50.0 * 6.2218499183654785
Epoch 2000, val loss: 1.2332297563552856
Epoch 2010, training loss: 311.2149658203125 = 0.029650408774614334 + 50.0 * 6.223706245422363
Epoch 2010, val loss: 1.2377151250839233
Epoch 2020, training loss: 311.300048828125 = 0.029168440029025078 + 50.0 * 6.225417613983154
Epoch 2020, val loss: 1.2419761419296265
Epoch 2030, training loss: 311.204345703125 = 0.028688570484519005 + 50.0 * 6.223513126373291
Epoch 2030, val loss: 1.2460774183273315
Epoch 2040, training loss: 311.4383544921875 = 0.0282305758446455 + 50.0 * 6.2282023429870605
Epoch 2040, val loss: 1.2501096725463867
Epoch 2050, training loss: 311.1155090332031 = 0.027771228924393654 + 50.0 * 6.221754550933838
Epoch 2050, val loss: 1.2540394067764282
Epoch 2060, training loss: 311.0517578125 = 0.027330540120601654 + 50.0 * 6.22048807144165
Epoch 2060, val loss: 1.258288025856018
Epoch 2070, training loss: 311.019287109375 = 0.026905957609415054 + 50.0 * 6.219847679138184
Epoch 2070, val loss: 1.262620210647583
Epoch 2080, training loss: 311.0155944824219 = 0.026494460180401802 + 50.0 * 6.219781875610352
Epoch 2080, val loss: 1.2668592929840088
Epoch 2090, training loss: 311.4629211425781 = 0.026099147275090218 + 50.0 * 6.228736877441406
Epoch 2090, val loss: 1.2708125114440918
Epoch 2100, training loss: 311.2169494628906 = 0.025680430233478546 + 50.0 * 6.223825931549072
Epoch 2100, val loss: 1.2747795581817627
Epoch 2110, training loss: 311.0810546875 = 0.02527034655213356 + 50.0 * 6.221115589141846
Epoch 2110, val loss: 1.2789486646652222
Epoch 2120, training loss: 310.98590087890625 = 0.024885181337594986 + 50.0 * 6.2192206382751465
Epoch 2120, val loss: 1.2827398777008057
Epoch 2130, training loss: 310.96136474609375 = 0.024512436240911484 + 50.0 * 6.2187371253967285
Epoch 2130, val loss: 1.2868090867996216
Epoch 2140, training loss: 311.5574035644531 = 0.02416113391518593 + 50.0 * 6.2306647300720215
Epoch 2140, val loss: 1.2900015115737915
Epoch 2150, training loss: 311.10345458984375 = 0.023777682334184647 + 50.0 * 6.221593856811523
Epoch 2150, val loss: 1.2953729629516602
Epoch 2160, training loss: 310.9716491699219 = 0.023419296368956566 + 50.0 * 6.218964576721191
Epoch 2160, val loss: 1.2984782457351685
Epoch 2170, training loss: 310.9151916503906 = 0.02307821996510029 + 50.0 * 6.2178425788879395
Epoch 2170, val loss: 1.3032032251358032
Epoch 2180, training loss: 311.08758544921875 = 0.022748850286006927 + 50.0 * 6.221297264099121
Epoch 2180, val loss: 1.306833267211914
Epoch 2190, training loss: 310.9186096191406 = 0.022407634183764458 + 50.0 * 6.217924118041992
Epoch 2190, val loss: 1.3106836080551147
Epoch 2200, training loss: 310.90570068359375 = 0.022077998146414757 + 50.0 * 6.217672824859619
Epoch 2200, val loss: 1.3138691186904907
Epoch 2210, training loss: 310.9178161621094 = 0.021760130301117897 + 50.0 * 6.217921257019043
Epoch 2210, val loss: 1.3182460069656372
Epoch 2220, training loss: 311.0201110839844 = 0.02145388536155224 + 50.0 * 6.219973087310791
Epoch 2220, val loss: 1.3217878341674805
Epoch 2230, training loss: 310.93743896484375 = 0.021150585263967514 + 50.0 * 6.218325614929199
Epoch 2230, val loss: 1.3263651132583618
Epoch 2240, training loss: 310.9744567871094 = 0.020855683833360672 + 50.0 * 6.219072341918945
Epoch 2240, val loss: 1.3296934366226196
Epoch 2250, training loss: 311.0340576171875 = 0.02055232971906662 + 50.0 * 6.220270156860352
Epoch 2250, val loss: 1.3329248428344727
Epoch 2260, training loss: 310.86102294921875 = 0.02025679312646389 + 50.0 * 6.216814994812012
Epoch 2260, val loss: 1.3374024629592896
Epoch 2270, training loss: 310.8326110839844 = 0.01997658982872963 + 50.0 * 6.216252326965332
Epoch 2270, val loss: 1.3411657810211182
Epoch 2280, training loss: 310.83258056640625 = 0.019706225022673607 + 50.0 * 6.216257095336914
Epoch 2280, val loss: 1.345139741897583
Epoch 2290, training loss: 311.20452880859375 = 0.019443059340119362 + 50.0 * 6.2237019538879395
Epoch 2290, val loss: 1.348530888557434
Epoch 2300, training loss: 311.0499572753906 = 0.01917695626616478 + 50.0 * 6.220615863800049
Epoch 2300, val loss: 1.3516480922698975
Epoch 2310, training loss: 310.8367919921875 = 0.01890227012336254 + 50.0 * 6.216358184814453
Epoch 2310, val loss: 1.3557465076446533
Epoch 2320, training loss: 310.8037414550781 = 0.018655763939023018 + 50.0 * 6.215702056884766
Epoch 2320, val loss: 1.3596309423446655
Epoch 2330, training loss: 310.9334716796875 = 0.018414318561553955 + 50.0 * 6.218301296234131
Epoch 2330, val loss: 1.3633005619049072
Epoch 2340, training loss: 310.9071960449219 = 0.018163984641432762 + 50.0 * 6.217780590057373
Epoch 2340, val loss: 1.3660768270492554
Epoch 2350, training loss: 310.8126525878906 = 0.01791907101869583 + 50.0 * 6.21589469909668
Epoch 2350, val loss: 1.3697590827941895
Epoch 2360, training loss: 310.742431640625 = 0.017684707418084145 + 50.0 * 6.2144951820373535
Epoch 2360, val loss: 1.3738561868667603
Epoch 2370, training loss: 310.72119140625 = 0.017456168308854103 + 50.0 * 6.214074611663818
Epoch 2370, val loss: 1.377179503440857
Epoch 2380, training loss: 310.89825439453125 = 0.01723894663155079 + 50.0 * 6.217620372772217
Epoch 2380, val loss: 1.3812235593795776
Epoch 2390, training loss: 310.8768615722656 = 0.017016088590025902 + 50.0 * 6.217196464538574
Epoch 2390, val loss: 1.3847765922546387
Epoch 2400, training loss: 310.80743408203125 = 0.016787651926279068 + 50.0 * 6.215813159942627
Epoch 2400, val loss: 1.387146234512329
Epoch 2410, training loss: 310.8054504394531 = 0.016574621200561523 + 50.0 * 6.215777397155762
Epoch 2410, val loss: 1.391058087348938
Epoch 2420, training loss: 310.7475891113281 = 0.01636732555925846 + 50.0 * 6.214624404907227
Epoch 2420, val loss: 1.3944813013076782
Epoch 2430, training loss: 310.7675476074219 = 0.016163859516382217 + 50.0 * 6.215027332305908
Epoch 2430, val loss: 1.397826075553894
Epoch 2440, training loss: 310.9173889160156 = 0.015969282016158104 + 50.0 * 6.218028545379639
Epoch 2440, val loss: 1.4015321731567383
Epoch 2450, training loss: 310.7160949707031 = 0.015761148184537888 + 50.0 * 6.2140069007873535
Epoch 2450, val loss: 1.404628038406372
Epoch 2460, training loss: 310.63909912109375 = 0.015569481067359447 + 50.0 * 6.212470531463623
Epoch 2460, val loss: 1.408123254776001
Epoch 2470, training loss: 310.6705627441406 = 0.015383878722786903 + 50.0 * 6.213103771209717
Epoch 2470, val loss: 1.4114396572113037
Epoch 2480, training loss: 311.0357971191406 = 0.015205079689621925 + 50.0 * 6.220412254333496
Epoch 2480, val loss: 1.4149705171585083
Epoch 2490, training loss: 310.91485595703125 = 0.015016074292361736 + 50.0 * 6.217997074127197
Epoch 2490, val loss: 1.4186278581619263
Epoch 2500, training loss: 310.7463073730469 = 0.014826055616140366 + 50.0 * 6.214629650115967
Epoch 2500, val loss: 1.4212676286697388
Epoch 2510, training loss: 310.653564453125 = 0.014647151343524456 + 50.0 * 6.212778091430664
Epoch 2510, val loss: 1.4239859580993652
Epoch 2520, training loss: 310.6786193847656 = 0.01447447668761015 + 50.0 * 6.213283061981201
Epoch 2520, val loss: 1.4272571802139282
Epoch 2530, training loss: 310.79840087890625 = 0.014306803233921528 + 50.0 * 6.215681552886963
Epoch 2530, val loss: 1.4311233758926392
Epoch 2540, training loss: 310.6761169433594 = 0.01414149347692728 + 50.0 * 6.213239669799805
Epoch 2540, val loss: 1.4344894886016846
Epoch 2550, training loss: 310.6015625 = 0.013976529240608215 + 50.0 * 6.211751461029053
Epoch 2550, val loss: 1.437681794166565
Epoch 2560, training loss: 310.6302185058594 = 0.013817301020026207 + 50.0 * 6.21232795715332
Epoch 2560, val loss: 1.4410998821258545
Epoch 2570, training loss: 310.7112731933594 = 0.013662592507898808 + 50.0 * 6.21395206451416
Epoch 2570, val loss: 1.4444042444229126
Epoch 2580, training loss: 311.0359802246094 = 0.013505985029041767 + 50.0 * 6.220448970794678
Epoch 2580, val loss: 1.4471193552017212
Epoch 2590, training loss: 310.67730712890625 = 0.01334149856120348 + 50.0 * 6.2132792472839355
Epoch 2590, val loss: 1.4490578174591064
Epoch 2600, training loss: 310.5615234375 = 0.013191149570047855 + 50.0 * 6.210967063903809
Epoch 2600, val loss: 1.453184962272644
Epoch 2610, training loss: 310.536865234375 = 0.013042889535427094 + 50.0 * 6.210476398468018
Epoch 2610, val loss: 1.4558788537979126
Epoch 2620, training loss: 310.51348876953125 = 0.012904120609164238 + 50.0 * 6.2100114822387695
Epoch 2620, val loss: 1.459323525428772
Epoch 2630, training loss: 310.5597229003906 = 0.012767854146659374 + 50.0 * 6.210939407348633
Epoch 2630, val loss: 1.462218165397644
Epoch 2640, training loss: 310.9286193847656 = 0.012628894299268723 + 50.0 * 6.218319416046143
Epoch 2640, val loss: 1.4655050039291382
Epoch 2650, training loss: 310.6195068359375 = 0.012481951154768467 + 50.0 * 6.2121405601501465
Epoch 2650, val loss: 1.4682189226150513
Epoch 2660, training loss: 310.5316467285156 = 0.012344351969659328 + 50.0 * 6.210386276245117
Epoch 2660, val loss: 1.4710865020751953
Epoch 2670, training loss: 310.505615234375 = 0.0122136939316988 + 50.0 * 6.209868431091309
Epoch 2670, val loss: 1.4739595651626587
Epoch 2680, training loss: 310.5129089355469 = 0.012085149995982647 + 50.0 * 6.210016250610352
Epoch 2680, val loss: 1.4771990776062012
Epoch 2690, training loss: 310.81292724609375 = 0.011965479701757431 + 50.0 * 6.216019153594971
Epoch 2690, val loss: 1.4801568984985352
Epoch 2700, training loss: 310.58404541015625 = 0.011833414435386658 + 50.0 * 6.21144437789917
Epoch 2700, val loss: 1.4835482835769653
Epoch 2710, training loss: 310.6128234863281 = 0.011709736660122871 + 50.0 * 6.212022304534912
Epoch 2710, val loss: 1.486613154411316
Epoch 2720, training loss: 310.4944763183594 = 0.011583415791392326 + 50.0 * 6.209657669067383
Epoch 2720, val loss: 1.488316535949707
Epoch 2730, training loss: 310.6857604980469 = 0.01146752294152975 + 50.0 * 6.2134857177734375
Epoch 2730, val loss: 1.4916036128997803
Epoch 2740, training loss: 310.6352844238281 = 0.011344680562615395 + 50.0 * 6.2124786376953125
Epoch 2740, val loss: 1.494593620300293
Epoch 2750, training loss: 310.57958984375 = 0.011223290115594864 + 50.0 * 6.211367130279541
Epoch 2750, val loss: 1.4968681335449219
Epoch 2760, training loss: 310.4552001953125 = 0.011111151427030563 + 50.0 * 6.208881855010986
Epoch 2760, val loss: 1.5000979900360107
Epoch 2770, training loss: 310.43115234375 = 0.01100100763142109 + 50.0 * 6.20840311050415
Epoch 2770, val loss: 1.5030183792114258
Epoch 2780, training loss: 310.4775085449219 = 0.010894566774368286 + 50.0 * 6.20933198928833
Epoch 2780, val loss: 1.5052645206451416
Epoch 2790, training loss: 310.6623840332031 = 0.01079095620661974 + 50.0 * 6.213031768798828
Epoch 2790, val loss: 1.5081104040145874
Epoch 2800, training loss: 310.4642639160156 = 0.010679070837795734 + 50.0 * 6.209071636199951
Epoch 2800, val loss: 1.5114210844039917
Epoch 2810, training loss: 310.4190673828125 = 0.010574623942375183 + 50.0 * 6.208169460296631
Epoch 2810, val loss: 1.5139515399932861
Epoch 2820, training loss: 310.5747985839844 = 0.010479248128831387 + 50.0 * 6.211286544799805
Epoch 2820, val loss: 1.5165752172470093
Epoch 2830, training loss: 310.5757141113281 = 0.010370157659053802 + 50.0 * 6.211306571960449
Epoch 2830, val loss: 1.5192034244537354
Epoch 2840, training loss: 310.5808410644531 = 0.01026593055576086 + 50.0 * 6.211411952972412
Epoch 2840, val loss: 1.5211715698242188
Epoch 2850, training loss: 310.46630859375 = 0.010166744701564312 + 50.0 * 6.209122657775879
Epoch 2850, val loss: 1.5238677263259888
Epoch 2860, training loss: 310.3616943359375 = 0.010064818896353245 + 50.0 * 6.207032680511475
Epoch 2860, val loss: 1.5270487070083618
Epoch 2870, training loss: 310.3870849609375 = 0.00997505709528923 + 50.0 * 6.2075419425964355
Epoch 2870, val loss: 1.5299888849258423
Epoch 2880, training loss: 310.537353515625 = 0.009884966537356377 + 50.0 * 6.210549354553223
Epoch 2880, val loss: 1.5330325365066528
Epoch 2890, training loss: 310.4781188964844 = 0.009791341610252857 + 50.0 * 6.209366321563721
Epoch 2890, val loss: 1.5351707935333252
Epoch 2900, training loss: 310.6136474609375 = 0.009699927642941475 + 50.0 * 6.21207857131958
Epoch 2900, val loss: 1.5373752117156982
Epoch 2910, training loss: 310.4362487792969 = 0.009603348560631275 + 50.0 * 6.208532810211182
Epoch 2910, val loss: 1.5397512912750244
Epoch 2920, training loss: 310.3653259277344 = 0.009515629149973392 + 50.0 * 6.20711612701416
Epoch 2920, val loss: 1.542368769645691
Epoch 2930, training loss: 310.3350524902344 = 0.009429337456822395 + 50.0 * 6.206512451171875
Epoch 2930, val loss: 1.5454065799713135
Epoch 2940, training loss: 310.5284423828125 = 0.009349553845822811 + 50.0 * 6.210381984710693
Epoch 2940, val loss: 1.5476353168487549
Epoch 2950, training loss: 310.32568359375 = 0.009261185303330421 + 50.0 * 6.206328868865967
Epoch 2950, val loss: 1.550148606300354
Epoch 2960, training loss: 310.363525390625 = 0.009178225882351398 + 50.0 * 6.207086563110352
Epoch 2960, val loss: 1.5527937412261963
Epoch 2970, training loss: 310.5899353027344 = 0.009101192466914654 + 50.0 * 6.2116169929504395
Epoch 2970, val loss: 1.5554660558700562
Epoch 2980, training loss: 310.4851379394531 = 0.009018780663609505 + 50.0 * 6.209522247314453
Epoch 2980, val loss: 1.5583703517913818
Epoch 2990, training loss: 310.3612060546875 = 0.008933540433645248 + 50.0 * 6.207045555114746
Epoch 2990, val loss: 1.5601844787597656
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8355297838692674
The final CL Acc:0.74815, 0.02619, The final GNN Acc:0.83658, 0.00086
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11574])
remove edge: torch.Size([2, 9498])
updated graph: torch.Size([2, 10516])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.7693176269531 = 1.9266746044158936 + 50.0 * 8.596853256225586
Epoch 0, val loss: 1.9215517044067383
Epoch 10, training loss: 431.7251892089844 = 1.918744683265686 + 50.0 * 8.596129417419434
Epoch 10, val loss: 1.9137670993804932
Epoch 20, training loss: 431.468017578125 = 1.9089562892913818 + 50.0 * 8.591180801391602
Epoch 20, val loss: 1.9037861824035645
Epoch 30, training loss: 429.64947509765625 = 1.8966413736343384 + 50.0 * 8.55505657196045
Epoch 30, val loss: 1.8910597562789917
Epoch 40, training loss: 416.1738586425781 = 1.8818180561065674 + 50.0 * 8.28584098815918
Epoch 40, val loss: 1.8761823177337646
Epoch 50, training loss: 377.304931640625 = 1.8652024269104004 + 50.0 * 7.508794784545898
Epoch 50, val loss: 1.8600828647613525
Epoch 60, training loss: 364.5370178222656 = 1.8532143831253052 + 50.0 * 7.253675937652588
Epoch 60, val loss: 1.8486177921295166
Epoch 70, training loss: 354.4529113769531 = 1.842071294784546 + 50.0 * 7.05221700668335
Epoch 70, val loss: 1.838340163230896
Epoch 80, training loss: 347.67523193359375 = 1.8326553106307983 + 50.0 * 6.916851997375488
Epoch 80, val loss: 1.8299626111984253
Epoch 90, training loss: 343.7086181640625 = 1.8237695693969727 + 50.0 * 6.8376970291137695
Epoch 90, val loss: 1.8218828439712524
Epoch 100, training loss: 341.1004943847656 = 1.815163493156433 + 50.0 * 6.785706520080566
Epoch 100, val loss: 1.8139674663543701
Epoch 110, training loss: 338.5042724609375 = 1.806886076927185 + 50.0 * 6.73394775390625
Epoch 110, val loss: 1.8066295385360718
Epoch 120, training loss: 335.90924072265625 = 1.7989989519119263 + 50.0 * 6.682204723358154
Epoch 120, val loss: 1.7997190952301025
Epoch 130, training loss: 333.6805114746094 = 1.7914445400238037 + 50.0 * 6.637781143188477
Epoch 130, val loss: 1.793102741241455
Epoch 140, training loss: 331.6119384765625 = 1.7833704948425293 + 50.0 * 6.596571445465088
Epoch 140, val loss: 1.7861571311950684
Epoch 150, training loss: 330.0517578125 = 1.7744439840316772 + 50.0 * 6.56554651260376
Epoch 150, val loss: 1.7785158157348633
Epoch 160, training loss: 328.6732177734375 = 1.7645137310028076 + 50.0 * 6.538173675537109
Epoch 160, val loss: 1.7701250314712524
Epoch 170, training loss: 327.61029052734375 = 1.7534438371658325 + 50.0 * 6.517136573791504
Epoch 170, val loss: 1.7610281705856323
Epoch 180, training loss: 326.7161560058594 = 1.7412595748901367 + 50.0 * 6.499497890472412
Epoch 180, val loss: 1.7510954141616821
Epoch 190, training loss: 326.0630187988281 = 1.7278721332550049 + 50.0 * 6.486702919006348
Epoch 190, val loss: 1.7403435707092285
Epoch 200, training loss: 325.3390808105469 = 1.7132867574691772 + 50.0 * 6.47251558303833
Epoch 200, val loss: 1.7285865545272827
Epoch 210, training loss: 324.7129821777344 = 1.6973397731781006 + 50.0 * 6.460312843322754
Epoch 210, val loss: 1.7158876657485962
Epoch 220, training loss: 324.26751708984375 = 1.6801507472991943 + 50.0 * 6.451747417449951
Epoch 220, val loss: 1.7022286653518677
Epoch 230, training loss: 323.7816162109375 = 1.6615455150604248 + 50.0 * 6.44240140914917
Epoch 230, val loss: 1.6875970363616943
Epoch 240, training loss: 323.2666015625 = 1.6417162418365479 + 50.0 * 6.432497501373291
Epoch 240, val loss: 1.6720763444900513
Epoch 250, training loss: 322.8386535644531 = 1.6207512617111206 + 50.0 * 6.424357891082764
Epoch 250, val loss: 1.6557475328445435
Epoch 260, training loss: 322.7014465332031 = 1.5986090898513794 + 50.0 * 6.422057151794434
Epoch 260, val loss: 1.638598918914795
Epoch 270, training loss: 322.1101989746094 = 1.5756088495254517 + 50.0 * 6.410691261291504
Epoch 270, val loss: 1.620805263519287
Epoch 280, training loss: 321.79962158203125 = 1.551822543144226 + 50.0 * 6.404955863952637
Epoch 280, val loss: 1.6025806665420532
Epoch 290, training loss: 321.4299011230469 = 1.5274323225021362 + 50.0 * 6.398049354553223
Epoch 290, val loss: 1.5840377807617188
Epoch 300, training loss: 321.5234069824219 = 1.5025873184204102 + 50.0 * 6.400416374206543
Epoch 300, val loss: 1.5652430057525635
Epoch 310, training loss: 320.9283447265625 = 1.4772058725357056 + 50.0 * 6.3890228271484375
Epoch 310, val loss: 1.5461477041244507
Epoch 320, training loss: 320.5669860839844 = 1.4516860246658325 + 50.0 * 6.382306098937988
Epoch 320, val loss: 1.5271098613739014
Epoch 330, training loss: 320.4575500488281 = 1.4260698556900024 + 50.0 * 6.380630016326904
Epoch 330, val loss: 1.5080227851867676
Epoch 340, training loss: 320.0757141113281 = 1.4003671407699585 + 50.0 * 6.373507022857666
Epoch 340, val loss: 1.4892313480377197
Epoch 350, training loss: 319.8063049316406 = 1.3746684789657593 + 50.0 * 6.368632793426514
Epoch 350, val loss: 1.470560908317566
Epoch 360, training loss: 319.6580505371094 = 1.3491051197052002 + 50.0 * 6.3661789894104
Epoch 360, val loss: 1.4521019458770752
Epoch 370, training loss: 319.382568359375 = 1.3236662149429321 + 50.0 * 6.361177921295166
Epoch 370, val loss: 1.4338934421539307
Epoch 380, training loss: 319.1214599609375 = 1.2982913255691528 + 50.0 * 6.356463432312012
Epoch 380, val loss: 1.4159144163131714
Epoch 390, training loss: 318.9198303222656 = 1.2731019258499146 + 50.0 * 6.35293436050415
Epoch 390, val loss: 1.398306965827942
Epoch 400, training loss: 318.76898193359375 = 1.248171329498291 + 50.0 * 6.35041618347168
Epoch 400, val loss: 1.3810327053070068
Epoch 410, training loss: 318.6197814941406 = 1.2235033512115479 + 50.0 * 6.347925186157227
Epoch 410, val loss: 1.3639419078826904
Epoch 420, training loss: 318.5423278808594 = 1.1989028453826904 + 50.0 * 6.346868515014648
Epoch 420, val loss: 1.3473868370056152
Epoch 430, training loss: 318.1940002441406 = 1.1746195554733276 + 50.0 * 6.340387344360352
Epoch 430, val loss: 1.3310823440551758
Epoch 440, training loss: 317.9674072265625 = 1.150686502456665 + 50.0 * 6.336334228515625
Epoch 440, val loss: 1.315156102180481
Epoch 450, training loss: 317.8302001953125 = 1.1271076202392578 + 50.0 * 6.334062099456787
Epoch 450, val loss: 1.2995045185089111
Epoch 460, training loss: 317.7132263183594 = 1.1037455797195435 + 50.0 * 6.332189559936523
Epoch 460, val loss: 1.2841469049453735
Epoch 470, training loss: 317.5099792480469 = 1.0806573629379272 + 50.0 * 6.328586578369141
Epoch 470, val loss: 1.2691704034805298
Epoch 480, training loss: 317.3497619628906 = 1.0580579042434692 + 50.0 * 6.325834274291992
Epoch 480, val loss: 1.254554271697998
Epoch 490, training loss: 317.2778015136719 = 1.0358836650848389 + 50.0 * 6.324838161468506
Epoch 490, val loss: 1.2403814792633057
Epoch 500, training loss: 317.2654113769531 = 1.0139384269714355 + 50.0 * 6.325029373168945
Epoch 500, val loss: 1.226850986480713
Epoch 510, training loss: 316.95697021484375 = 0.9922845363616943 + 50.0 * 6.319293975830078
Epoch 510, val loss: 1.213160514831543
Epoch 520, training loss: 316.82989501953125 = 0.9711586833000183 + 50.0 * 6.317174911499023
Epoch 520, val loss: 1.200012445449829
Epoch 530, training loss: 316.72235107421875 = 0.9504369497299194 + 50.0 * 6.315438270568848
Epoch 530, val loss: 1.1873862743377686
Epoch 540, training loss: 316.8179016113281 = 0.9299766421318054 + 50.0 * 6.317759037017822
Epoch 540, val loss: 1.175093173980713
Epoch 550, training loss: 316.5910339355469 = 0.909861147403717 + 50.0 * 6.313623428344727
Epoch 550, val loss: 1.1631155014038086
Epoch 560, training loss: 316.3544616699219 = 0.8901686668395996 + 50.0 * 6.309286117553711
Epoch 560, val loss: 1.1515828371047974
Epoch 570, training loss: 316.2819519042969 = 0.8709301352500916 + 50.0 * 6.308219909667969
Epoch 570, val loss: 1.140679955482483
Epoch 580, training loss: 316.266357421875 = 0.8519043922424316 + 50.0 * 6.308289051055908
Epoch 580, val loss: 1.1297813653945923
Epoch 590, training loss: 316.11395263671875 = 0.8332869410514832 + 50.0 * 6.3056135177612305
Epoch 590, val loss: 1.1195188760757446
Epoch 600, training loss: 315.945068359375 = 0.8150924444198608 + 50.0 * 6.3025994300842285
Epoch 600, val loss: 1.109631061553955
Epoch 610, training loss: 316.0030822753906 = 0.7973371148109436 + 50.0 * 6.304114818572998
Epoch 610, val loss: 1.1003575325012207
Epoch 620, training loss: 315.8333435058594 = 0.77963787317276 + 50.0 * 6.301074028015137
Epoch 620, val loss: 1.0908567905426025
Epoch 630, training loss: 315.7323913574219 = 0.7624267339706421 + 50.0 * 6.299399375915527
Epoch 630, val loss: 1.0823107957839966
Epoch 640, training loss: 315.5959167480469 = 0.7455577850341797 + 50.0 * 6.2970075607299805
Epoch 640, val loss: 1.0738229751586914
Epoch 650, training loss: 315.87408447265625 = 0.7290465235710144 + 50.0 * 6.302900791168213
Epoch 650, val loss: 1.0657466650009155
Epoch 660, training loss: 315.4800109863281 = 0.712804913520813 + 50.0 * 6.295344352722168
Epoch 660, val loss: 1.0583277940750122
Epoch 670, training loss: 315.45379638671875 = 0.6969346404075623 + 50.0 * 6.295137405395508
Epoch 670, val loss: 1.0510661602020264
Epoch 680, training loss: 315.28643798828125 = 0.6814177632331848 + 50.0 * 6.292100429534912
Epoch 680, val loss: 1.04434072971344
Epoch 690, training loss: 315.2625732421875 = 0.6662102341651917 + 50.0 * 6.291927337646484
Epoch 690, val loss: 1.0377885103225708
Epoch 700, training loss: 315.0859680175781 = 0.6512351036071777 + 50.0 * 6.288694381713867
Epoch 700, val loss: 1.0317225456237793
Epoch 710, training loss: 315.1018371582031 = 0.6367360949516296 + 50.0 * 6.289301872253418
Epoch 710, val loss: 1.0260941982269287
Epoch 720, training loss: 315.0692443847656 = 0.6224452257156372 + 50.0 * 6.288936138153076
Epoch 720, val loss: 1.020581841468811
Epoch 730, training loss: 314.9646911621094 = 0.6082937121391296 + 50.0 * 6.287127494812012
Epoch 730, val loss: 1.015411138534546
Epoch 740, training loss: 314.7917785644531 = 0.5946739315986633 + 50.0 * 6.283942222595215
Epoch 740, val loss: 1.010691523551941
Epoch 750, training loss: 314.7178649902344 = 0.5813872218132019 + 50.0 * 6.282729625701904
Epoch 750, val loss: 1.0063990354537964
Epoch 760, training loss: 315.0354309082031 = 0.5682978630065918 + 50.0 * 6.289342403411865
Epoch 760, val loss: 1.0022212266921997
Epoch 770, training loss: 314.6004333496094 = 0.5554555058479309 + 50.0 * 6.280899524688721
Epoch 770, val loss: 0.9990129470825195
Epoch 780, training loss: 314.5559997558594 = 0.5429587960243225 + 50.0 * 6.2802605628967285
Epoch 780, val loss: 0.9956955313682556
Epoch 790, training loss: 314.4376525878906 = 0.5308033227920532 + 50.0 * 6.27813720703125
Epoch 790, val loss: 0.9929567575454712
Epoch 800, training loss: 314.7701416015625 = 0.5189695358276367 + 50.0 * 6.285023212432861
Epoch 800, val loss: 0.9905032515525818
Epoch 810, training loss: 314.47967529296875 = 0.5071356892585754 + 50.0 * 6.2794508934021
Epoch 810, val loss: 0.9883772134780884
Epoch 820, training loss: 314.2753601074219 = 0.49563068151474 + 50.0 * 6.275594711303711
Epoch 820, val loss: 0.9863253831863403
Epoch 830, training loss: 314.21026611328125 = 0.4845275282859802 + 50.0 * 6.274514675140381
Epoch 830, val loss: 0.9847674369812012
Epoch 840, training loss: 314.4207458496094 = 0.473624587059021 + 50.0 * 6.278942108154297
Epoch 840, val loss: 0.9832971692085266
Epoch 850, training loss: 314.32952880859375 = 0.4629853367805481 + 50.0 * 6.2773308753967285
Epoch 850, val loss: 0.9827561974525452
Epoch 860, training loss: 314.087646484375 = 0.45247095823287964 + 50.0 * 6.272703170776367
Epoch 860, val loss: 0.9818281531333923
Epoch 870, training loss: 313.9726867675781 = 0.44234800338745117 + 50.0 * 6.270606994628906
Epoch 870, val loss: 0.9817968010902405
Epoch 880, training loss: 313.8966064453125 = 0.43252137303352356 + 50.0 * 6.269281387329102
Epoch 880, val loss: 0.9817922115325928
Epoch 890, training loss: 313.9466552734375 = 0.42293232679367065 + 50.0 * 6.270474433898926
Epoch 890, val loss: 0.9822420477867126
Epoch 900, training loss: 313.8714599609375 = 0.4133557677268982 + 50.0 * 6.269161701202393
Epoch 900, val loss: 0.9822542667388916
Epoch 910, training loss: 313.8704528808594 = 0.4040798246860504 + 50.0 * 6.269327163696289
Epoch 910, val loss: 0.9831617474555969
Epoch 920, training loss: 313.7278747558594 = 0.3950618505477905 + 50.0 * 6.266656398773193
Epoch 920, val loss: 0.9839690327644348
Epoch 930, training loss: 313.6986083984375 = 0.3863390386104584 + 50.0 * 6.266245365142822
Epoch 930, val loss: 0.9853313565254211
Epoch 940, training loss: 313.76422119140625 = 0.377765417098999 + 50.0 * 6.267728805541992
Epoch 940, val loss: 0.9866960048675537
Epoch 950, training loss: 313.8450927734375 = 0.36926698684692383 + 50.0 * 6.269516468048096
Epoch 950, val loss: 0.9876475930213928
Epoch 960, training loss: 313.5653991699219 = 0.3610939383506775 + 50.0 * 6.2640862464904785
Epoch 960, val loss: 0.9896033406257629
Epoch 970, training loss: 313.46429443359375 = 0.35312193632125854 + 50.0 * 6.262223720550537
Epoch 970, val loss: 0.991709291934967
Epoch 980, training loss: 313.43341064453125 = 0.3453913629055023 + 50.0 * 6.261760234832764
Epoch 980, val loss: 0.9940857887268066
Epoch 990, training loss: 313.81005859375 = 0.3378252387046814 + 50.0 * 6.269444465637207
Epoch 990, val loss: 0.996544599533081
Epoch 1000, training loss: 313.36138916015625 = 0.3302680552005768 + 50.0 * 6.260622501373291
Epoch 1000, val loss: 0.9990110993385315
Epoch 1010, training loss: 313.3075866699219 = 0.323013037443161 + 50.0 * 6.2596917152404785
Epoch 1010, val loss: 1.001894474029541
Epoch 1020, training loss: 313.31390380859375 = 0.3159763813018799 + 50.0 * 6.259958267211914
Epoch 1020, val loss: 1.0049163103103638
Epoch 1030, training loss: 313.3342590332031 = 0.3090391755104065 + 50.0 * 6.260504245758057
Epoch 1030, val loss: 1.0081543922424316
Epoch 1040, training loss: 313.1517639160156 = 0.30227595567703247 + 50.0 * 6.2569899559021
Epoch 1040, val loss: 1.0117242336273193
Epoch 1050, training loss: 313.1089782714844 = 0.2957531213760376 + 50.0 * 6.256264686584473
Epoch 1050, val loss: 1.0156638622283936
Epoch 1060, training loss: 313.306884765625 = 0.2894105315208435 + 50.0 * 6.260349750518799
Epoch 1060, val loss: 1.0194814205169678
Epoch 1070, training loss: 313.0870056152344 = 0.28307026624679565 + 50.0 * 6.256078720092773
Epoch 1070, val loss: 1.0231835842132568
Epoch 1080, training loss: 313.092529296875 = 0.2769286334514618 + 50.0 * 6.256312370300293
Epoch 1080, val loss: 1.0274215936660767
Epoch 1090, training loss: 313.07061767578125 = 0.2709464132785797 + 50.0 * 6.255993366241455
Epoch 1090, val loss: 1.0318489074707031
Epoch 1100, training loss: 312.96795654296875 = 0.2650485038757324 + 50.0 * 6.254058361053467
Epoch 1100, val loss: 1.036242961883545
Epoch 1110, training loss: 312.9541320800781 = 0.2593892216682434 + 50.0 * 6.253894805908203
Epoch 1110, val loss: 1.0410815477371216
Epoch 1120, training loss: 313.0720520019531 = 0.2537533938884735 + 50.0 * 6.256365776062012
Epoch 1120, val loss: 1.0456191301345825
Epoch 1130, training loss: 312.9416198730469 = 0.2481859177350998 + 50.0 * 6.253868103027344
Epoch 1130, val loss: 1.050325632095337
Epoch 1140, training loss: 312.7828369140625 = 0.24280571937561035 + 50.0 * 6.250800609588623
Epoch 1140, val loss: 1.0551763772964478
Epoch 1150, training loss: 312.7498779296875 = 0.23760317265987396 + 50.0 * 6.250245094299316
Epoch 1150, val loss: 1.0603528022766113
Epoch 1160, training loss: 312.9344482421875 = 0.23250225186347961 + 50.0 * 6.2540388107299805
Epoch 1160, val loss: 1.065125823020935
Epoch 1170, training loss: 312.86236572265625 = 0.227402925491333 + 50.0 * 6.25269889831543
Epoch 1170, val loss: 1.0708298683166504
Epoch 1180, training loss: 312.7781982421875 = 0.2223801463842392 + 50.0 * 6.2511162757873535
Epoch 1180, val loss: 1.0754649639129639
Epoch 1190, training loss: 312.6041259765625 = 0.21760138869285583 + 50.0 * 6.247730731964111
Epoch 1190, val loss: 1.0810548067092896
Epoch 1200, training loss: 312.61273193359375 = 0.21294814348220825 + 50.0 * 6.247995376586914
Epoch 1200, val loss: 1.086782455444336
Epoch 1210, training loss: 313.05712890625 = 0.2083316445350647 + 50.0 * 6.2569756507873535
Epoch 1210, val loss: 1.0920462608337402
Epoch 1220, training loss: 312.5585632324219 = 0.20377932488918304 + 50.0 * 6.247096061706543
Epoch 1220, val loss: 1.0977368354797363
Epoch 1230, training loss: 312.49359130859375 = 0.19938230514526367 + 50.0 * 6.245884418487549
Epoch 1230, val loss: 1.1036362648010254
Epoch 1240, training loss: 312.461669921875 = 0.19509053230285645 + 50.0 * 6.245331287384033
Epoch 1240, val loss: 1.109381914138794
Epoch 1250, training loss: 312.863525390625 = 0.19094952940940857 + 50.0 * 6.253451824188232
Epoch 1250, val loss: 1.1152833700180054
Epoch 1260, training loss: 312.57147216796875 = 0.18672849237918854 + 50.0 * 6.247694969177246
Epoch 1260, val loss: 1.1209604740142822
Epoch 1270, training loss: 312.41436767578125 = 0.18267226219177246 + 50.0 * 6.244633674621582
Epoch 1270, val loss: 1.1269062757492065
Epoch 1280, training loss: 312.7042541503906 = 0.1788046509027481 + 50.0 * 6.250509262084961
Epoch 1280, val loss: 1.1332823038101196
Epoch 1290, training loss: 312.39892578125 = 0.17478911578655243 + 50.0 * 6.24448299407959
Epoch 1290, val loss: 1.1384506225585938
Epoch 1300, training loss: 312.30767822265625 = 0.17104125022888184 + 50.0 * 6.242732524871826
Epoch 1300, val loss: 1.1448287963867188
Epoch 1310, training loss: 312.2546691894531 = 0.16735170781612396 + 50.0 * 6.241745948791504
Epoch 1310, val loss: 1.1508424282073975
Epoch 1320, training loss: 312.53314208984375 = 0.1637643426656723 + 50.0 * 6.247387409210205
Epoch 1320, val loss: 1.1567121744155884
Epoch 1330, training loss: 312.443359375 = 0.1601748764514923 + 50.0 * 6.245663642883301
Epoch 1330, val loss: 1.1630432605743408
Epoch 1340, training loss: 312.31927490234375 = 0.15663228929042816 + 50.0 * 6.243252754211426
Epoch 1340, val loss: 1.1689486503601074
Epoch 1350, training loss: 312.1601257324219 = 0.15324480831623077 + 50.0 * 6.240137577056885
Epoch 1350, val loss: 1.1753541231155396
Epoch 1360, training loss: 312.17633056640625 = 0.14996960759162903 + 50.0 * 6.240527153015137
Epoch 1360, val loss: 1.1817944049835205
Epoch 1370, training loss: 312.5663146972656 = 0.146736741065979 + 50.0 * 6.248391628265381
Epoch 1370, val loss: 1.1880617141723633
Epoch 1380, training loss: 312.23480224609375 = 0.14345943927764893 + 50.0 * 6.24182653427124
Epoch 1380, val loss: 1.1937729120254517
Epoch 1390, training loss: 312.0943298339844 = 0.14033614099025726 + 50.0 * 6.239079475402832
Epoch 1390, val loss: 1.200425386428833
Epoch 1400, training loss: 312.0970764160156 = 0.1373000144958496 + 50.0 * 6.239195823669434
Epoch 1400, val loss: 1.2065987586975098
Epoch 1410, training loss: 312.30609130859375 = 0.13431212306022644 + 50.0 * 6.243435859680176
Epoch 1410, val loss: 1.2128269672393799
Epoch 1420, training loss: 312.14996337890625 = 0.13140088319778442 + 50.0 * 6.240371227264404
Epoch 1420, val loss: 1.2195457220077515
Epoch 1430, training loss: 312.0259094238281 = 0.12852329015731812 + 50.0 * 6.237947940826416
Epoch 1430, val loss: 1.2256532907485962
Epoch 1440, training loss: 312.1380920410156 = 0.12578538060188293 + 50.0 * 6.240245819091797
Epoch 1440, val loss: 1.2322529554367065
Epoch 1450, training loss: 311.95257568359375 = 0.12303182482719421 + 50.0 * 6.236590385437012
Epoch 1450, val loss: 1.2385088205337524
Epoch 1460, training loss: 311.99163818359375 = 0.12039916217327118 + 50.0 * 6.237424850463867
Epoch 1460, val loss: 1.2450231313705444
Epoch 1470, training loss: 312.1488342285156 = 0.11779119074344635 + 50.0 * 6.2406206130981445
Epoch 1470, val loss: 1.2511980533599854
Epoch 1480, training loss: 311.93670654296875 = 0.11516449600458145 + 50.0 * 6.236430644989014
Epoch 1480, val loss: 1.2573097944259644
Epoch 1490, training loss: 311.8724060058594 = 0.11270953714847565 + 50.0 * 6.235194206237793
Epoch 1490, val loss: 1.263816237449646
Epoch 1500, training loss: 312.181640625 = 0.11032131314277649 + 50.0 * 6.241426467895508
Epoch 1500, val loss: 1.2702611684799194
Epoch 1510, training loss: 311.8641357421875 = 0.10787279158830643 + 50.0 * 6.2351250648498535
Epoch 1510, val loss: 1.276154637336731
Epoch 1520, training loss: 311.8021545410156 = 0.10555935651063919 + 50.0 * 6.233932018280029
Epoch 1520, val loss: 1.2826776504516602
Epoch 1530, training loss: 311.8760986328125 = 0.10333084315061569 + 50.0 * 6.235455513000488
Epoch 1530, val loss: 1.2891607284545898
Epoch 1540, training loss: 311.8789978027344 = 0.10109734535217285 + 50.0 * 6.235557556152344
Epoch 1540, val loss: 1.2952743768692017
Epoch 1550, training loss: 311.9729309082031 = 0.09891324490308762 + 50.0 * 6.237480640411377
Epoch 1550, val loss: 1.3014596700668335
Epoch 1560, training loss: 311.77655029296875 = 0.09678507596254349 + 50.0 * 6.233595371246338
Epoch 1560, val loss: 1.3079496622085571
Epoch 1570, training loss: 311.7930908203125 = 0.09473752230405807 + 50.0 * 6.233967304229736
Epoch 1570, val loss: 1.3142187595367432
Epoch 1580, training loss: 311.8498229980469 = 0.09273803979158401 + 50.0 * 6.235141754150391
Epoch 1580, val loss: 1.3205549716949463
Epoch 1590, training loss: 311.7598571777344 = 0.09074895083904266 + 50.0 * 6.233382225036621
Epoch 1590, val loss: 1.3268183469772339
Epoch 1600, training loss: 311.8128967285156 = 0.08886568248271942 + 50.0 * 6.234480381011963
Epoch 1600, val loss: 1.3332282304763794
Epoch 1610, training loss: 311.6283874511719 = 0.08694015443325043 + 50.0 * 6.230828762054443
Epoch 1610, val loss: 1.339293360710144
Epoch 1620, training loss: 311.6806640625 = 0.08510859310626984 + 50.0 * 6.2319111824035645
Epoch 1620, val loss: 1.3454151153564453
Epoch 1630, training loss: 311.8264465332031 = 0.08334434777498245 + 50.0 * 6.234862327575684
Epoch 1630, val loss: 1.3516021966934204
Epoch 1640, training loss: 311.68499755859375 = 0.08158508688211441 + 50.0 * 6.2320685386657715
Epoch 1640, val loss: 1.3579670190811157
Epoch 1650, training loss: 311.6273193359375 = 0.07986865192651749 + 50.0 * 6.2309489250183105
Epoch 1650, val loss: 1.3641034364700317
Epoch 1660, training loss: 311.78253173828125 = 0.07822369784116745 + 50.0 * 6.234086513519287
Epoch 1660, val loss: 1.3702865839004517
Epoch 1670, training loss: 311.6280822753906 = 0.07657162845134735 + 50.0 * 6.231029987335205
Epoch 1670, val loss: 1.3760056495666504
Epoch 1680, training loss: 311.6328430175781 = 0.07499349117279053 + 50.0 * 6.231157302856445
Epoch 1680, val loss: 1.3823456764221191
Epoch 1690, training loss: 311.5686950683594 = 0.07344754785299301 + 50.0 * 6.229904651641846
Epoch 1690, val loss: 1.3883763551712036
Epoch 1700, training loss: 311.5006408691406 = 0.07196827232837677 + 50.0 * 6.228573322296143
Epoch 1700, val loss: 1.3946419954299927
Epoch 1710, training loss: 311.6169738769531 = 0.07051526010036469 + 50.0 * 6.230928897857666
Epoch 1710, val loss: 1.40060293674469
Epoch 1720, training loss: 311.60504150390625 = 0.0690716877579689 + 50.0 * 6.230719566345215
Epoch 1720, val loss: 1.4067720174789429
Epoch 1730, training loss: 311.5294189453125 = 0.06766273081302643 + 50.0 * 6.2292351722717285
Epoch 1730, val loss: 1.4125702381134033
Epoch 1740, training loss: 311.67108154296875 = 0.06629448384046555 + 50.0 * 6.232095718383789
Epoch 1740, val loss: 1.418360948562622
Epoch 1750, training loss: 311.5043640136719 = 0.06496520340442657 + 50.0 * 6.228787899017334
Epoch 1750, val loss: 1.4245496988296509
Epoch 1760, training loss: 311.42669677734375 = 0.06365683674812317 + 50.0 * 6.227260589599609
Epoch 1760, val loss: 1.4304569959640503
Epoch 1770, training loss: 311.3832092285156 = 0.062381427735090256 + 50.0 * 6.22641658782959
Epoch 1770, val loss: 1.436277151107788
Epoch 1780, training loss: 311.51898193359375 = 0.06117178872227669 + 50.0 * 6.229156494140625
Epoch 1780, val loss: 1.4423078298568726
Epoch 1790, training loss: 311.4256286621094 = 0.05996902287006378 + 50.0 * 6.227313041687012
Epoch 1790, val loss: 1.4479987621307373
Epoch 1800, training loss: 311.3739013671875 = 0.05878334119915962 + 50.0 * 6.226302623748779
Epoch 1800, val loss: 1.453974723815918
Epoch 1810, training loss: 311.5630187988281 = 0.057624515146017075 + 50.0 * 6.23010778427124
Epoch 1810, val loss: 1.4596525430679321
Epoch 1820, training loss: 311.3384094238281 = 0.056495003402233124 + 50.0 * 6.225638389587402
Epoch 1820, val loss: 1.4649302959442139
Epoch 1830, training loss: 311.3055725097656 = 0.055396996438503265 + 50.0 * 6.225003719329834
Epoch 1830, val loss: 1.4707614183425903
Epoch 1840, training loss: 311.33380126953125 = 0.054349590092897415 + 50.0 * 6.225589275360107
Epoch 1840, val loss: 1.476477861404419
Epoch 1850, training loss: 311.40313720703125 = 0.053334176540374756 + 50.0 * 6.226995944976807
Epoch 1850, val loss: 1.4821857213974
Epoch 1860, training loss: 311.4053649902344 = 0.05231774225831032 + 50.0 * 6.2270612716674805
Epoch 1860, val loss: 1.4881715774536133
Epoch 1870, training loss: 311.3134765625 = 0.05131960287690163 + 50.0 * 6.225242614746094
Epoch 1870, val loss: 1.4930955171585083
Epoch 1880, training loss: 311.4472351074219 = 0.0503501296043396 + 50.0 * 6.227937698364258
Epoch 1880, val loss: 1.4986414909362793
Epoch 1890, training loss: 311.224853515625 = 0.04939552769064903 + 50.0 * 6.223508834838867
Epoch 1890, val loss: 1.5041191577911377
Epoch 1900, training loss: 311.19805908203125 = 0.048477645963430405 + 50.0 * 6.222991466522217
Epoch 1900, val loss: 1.5097019672393799
Epoch 1910, training loss: 311.21026611328125 = 0.04759598150849342 + 50.0 * 6.22325325012207
Epoch 1910, val loss: 1.5151115655899048
Epoch 1920, training loss: 311.29193115234375 = 0.04673832282423973 + 50.0 * 6.224903583526611
Epoch 1920, val loss: 1.5206332206726074
Epoch 1930, training loss: 311.47265625 = 0.04591108858585358 + 50.0 * 6.228535175323486
Epoch 1930, val loss: 1.5261754989624023
Epoch 1940, training loss: 311.1714782714844 = 0.045002929866313934 + 50.0 * 6.222529411315918
Epoch 1940, val loss: 1.5307209491729736
Epoch 1950, training loss: 311.1383361816406 = 0.044185735285282135 + 50.0 * 6.2218828201293945
Epoch 1950, val loss: 1.5364826917648315
Epoch 1960, training loss: 311.1541442871094 = 0.04340488091111183 + 50.0 * 6.222214698791504
Epoch 1960, val loss: 1.541650652885437
Epoch 1970, training loss: 311.3502197265625 = 0.042645473033189774 + 50.0 * 6.226151466369629
Epoch 1970, val loss: 1.5471259355545044
Epoch 1980, training loss: 311.0986633300781 = 0.04187701642513275 + 50.0 * 6.22113561630249
Epoch 1980, val loss: 1.552003264427185
Epoch 1990, training loss: 311.14520263671875 = 0.04114503785967827 + 50.0 * 6.222081184387207
Epoch 1990, val loss: 1.5572782754898071
Epoch 2000, training loss: 311.2879333496094 = 0.04043439030647278 + 50.0 * 6.224949836730957
Epoch 2000, val loss: 1.5624688863754272
Epoch 2010, training loss: 311.1487731933594 = 0.039725612848997116 + 50.0 * 6.2221808433532715
Epoch 2010, val loss: 1.5675634145736694
Epoch 2020, training loss: 311.1164855957031 = 0.0390402227640152 + 50.0 * 6.221549034118652
Epoch 2020, val loss: 1.5725793838500977
Epoch 2030, training loss: 311.150390625 = 0.03838716447353363 + 50.0 * 6.222239971160889
Epoch 2030, val loss: 1.577683925628662
Epoch 2040, training loss: 311.1100158691406 = 0.03771688789129257 + 50.0 * 6.2214460372924805
Epoch 2040, val loss: 1.5828182697296143
Epoch 2050, training loss: 311.04107666015625 = 0.03706372156739235 + 50.0 * 6.2200798988342285
Epoch 2050, val loss: 1.5874468088150024
Epoch 2060, training loss: 311.1627502441406 = 0.03644644096493721 + 50.0 * 6.2225260734558105
Epoch 2060, val loss: 1.592106819152832
Epoch 2070, training loss: 311.2314453125 = 0.03585662320256233 + 50.0 * 6.223911762237549
Epoch 2070, val loss: 1.5973771810531616
Epoch 2080, training loss: 311.0281066894531 = 0.03521810844540596 + 50.0 * 6.219857692718506
Epoch 2080, val loss: 1.601747989654541
Epoch 2090, training loss: 310.9827575683594 = 0.034629181027412415 + 50.0 * 6.218962669372559
Epoch 2090, val loss: 1.6067973375320435
Epoch 2100, training loss: 311.0052490234375 = 0.03407313674688339 + 50.0 * 6.219423294067383
Epoch 2100, val loss: 1.6116002798080444
Epoch 2110, training loss: 311.303955078125 = 0.0335303470492363 + 50.0 * 6.225408554077148
Epoch 2110, val loss: 1.6161751747131348
Epoch 2120, training loss: 311.0585021972656 = 0.03296685218811035 + 50.0 * 6.220510959625244
Epoch 2120, val loss: 1.6210685968399048
Epoch 2130, training loss: 310.9618835449219 = 0.03243023529648781 + 50.0 * 6.218588829040527
Epoch 2130, val loss: 1.6257448196411133
Epoch 2140, training loss: 311.2131042480469 = 0.03193894401192665 + 50.0 * 6.223622798919678
Epoch 2140, val loss: 1.6305972337722778
Epoch 2150, training loss: 310.900634765625 = 0.03138580173254013 + 50.0 * 6.217385292053223
Epoch 2150, val loss: 1.6345818042755127
Epoch 2160, training loss: 310.8848571777344 = 0.030884698033332825 + 50.0 * 6.2170796394348145
Epoch 2160, val loss: 1.639498233795166
Epoch 2170, training loss: 310.89776611328125 = 0.03040900081396103 + 50.0 * 6.217347621917725
Epoch 2170, val loss: 1.6440658569335938
Epoch 2180, training loss: 311.1598815917969 = 0.029941309243440628 + 50.0 * 6.222599029541016
Epoch 2180, val loss: 1.6484614610671997
Epoch 2190, training loss: 310.8938903808594 = 0.029467521235346794 + 50.0 * 6.217288494110107
Epoch 2190, val loss: 1.6529338359832764
Epoch 2200, training loss: 310.9125061035156 = 0.02901490032672882 + 50.0 * 6.217669486999512
Epoch 2200, val loss: 1.6572927236557007
Epoch 2210, training loss: 310.9611511230469 = 0.028568685054779053 + 50.0 * 6.21865177154541
Epoch 2210, val loss: 1.6617956161499023
Epoch 2220, training loss: 310.9248962402344 = 0.02814517542719841 + 50.0 * 6.217935085296631
Epoch 2220, val loss: 1.6660583019256592
Epoch 2230, training loss: 310.94696044921875 = 0.027717692777514458 + 50.0 * 6.218384742736816
Epoch 2230, val loss: 1.6705799102783203
Epoch 2240, training loss: 310.86383056640625 = 0.02729606255888939 + 50.0 * 6.216731071472168
Epoch 2240, val loss: 1.6746469736099243
Epoch 2250, training loss: 310.834228515625 = 0.026882870122790337 + 50.0 * 6.216146945953369
Epoch 2250, val loss: 1.6791436672210693
Epoch 2260, training loss: 310.9497375488281 = 0.02649090439081192 + 50.0 * 6.2184648513793945
Epoch 2260, val loss: 1.6832857131958008
Epoch 2270, training loss: 310.9533386230469 = 0.026106208562850952 + 50.0 * 6.218544960021973
Epoch 2270, val loss: 1.6873295307159424
Epoch 2280, training loss: 310.8839416503906 = 0.025711124762892723 + 50.0 * 6.217164516448975
Epoch 2280, val loss: 1.6914509534835815
Epoch 2290, training loss: 310.8829040527344 = 0.025327863171696663 + 50.0 * 6.217151641845703
Epoch 2290, val loss: 1.6957247257232666
Epoch 2300, training loss: 310.88775634765625 = 0.024953695014119148 + 50.0 * 6.21725606918335
Epoch 2300, val loss: 1.6997647285461426
Epoch 2310, training loss: 310.796630859375 = 0.02459363453090191 + 50.0 * 6.21544075012207
Epoch 2310, val loss: 1.7034602165222168
Epoch 2320, training loss: 310.7810974121094 = 0.02424875646829605 + 50.0 * 6.215136528015137
Epoch 2320, val loss: 1.7079732418060303
Epoch 2330, training loss: 310.8334045410156 = 0.023918073624372482 + 50.0 * 6.216189384460449
Epoch 2330, val loss: 1.7121332883834839
Epoch 2340, training loss: 310.8812561035156 = 0.02358219586312771 + 50.0 * 6.217153072357178
Epoch 2340, val loss: 1.7156808376312256
Epoch 2350, training loss: 310.9795227050781 = 0.023238804191350937 + 50.0 * 6.219125747680664
Epoch 2350, val loss: 1.719237208366394
Epoch 2360, training loss: 310.7947692871094 = 0.02289748378098011 + 50.0 * 6.215437889099121
Epoch 2360, val loss: 1.7231841087341309
Epoch 2370, training loss: 310.7055358886719 = 0.022576676681637764 + 50.0 * 6.213659286499023
Epoch 2370, val loss: 1.726928949356079
Epoch 2380, training loss: 310.6766052246094 = 0.022277062758803368 + 50.0 * 6.2130866050720215
Epoch 2380, val loss: 1.7313801050186157
Epoch 2390, training loss: 310.7762451171875 = 0.021993212401866913 + 50.0 * 6.215084552764893
Epoch 2390, val loss: 1.735293984413147
Epoch 2400, training loss: 310.88055419921875 = 0.021695416420698166 + 50.0 * 6.217176914215088
Epoch 2400, val loss: 1.7385929822921753
Epoch 2410, training loss: 310.80706787109375 = 0.021380867809057236 + 50.0 * 6.2157135009765625
Epoch 2410, val loss: 1.7421233654022217
Epoch 2420, training loss: 310.71551513671875 = 0.021102795377373695 + 50.0 * 6.213888645172119
Epoch 2420, val loss: 1.7463085651397705
Epoch 2430, training loss: 311.0171813964844 = 0.020837612450122833 + 50.0 * 6.219926834106445
Epoch 2430, val loss: 1.7495492696762085
Epoch 2440, training loss: 310.7445983886719 = 0.020528294146060944 + 50.0 * 6.214481353759766
Epoch 2440, val loss: 1.7532999515533447
Epoch 2450, training loss: 310.6817321777344 = 0.02026241458952427 + 50.0 * 6.213229656219482
Epoch 2450, val loss: 1.7568695545196533
Epoch 2460, training loss: 310.80780029296875 = 0.020009011030197144 + 50.0 * 6.215755462646484
Epoch 2460, val loss: 1.7609705924987793
Epoch 2470, training loss: 310.6499328613281 = 0.019740359857678413 + 50.0 * 6.212603569030762
Epoch 2470, val loss: 1.763806700706482
Epoch 2480, training loss: 310.5996398925781 = 0.01947343908250332 + 50.0 * 6.211603164672852
Epoch 2480, val loss: 1.7675923109054565
Epoch 2490, training loss: 310.58642578125 = 0.019232667982578278 + 50.0 * 6.211344242095947
Epoch 2490, val loss: 1.7712947130203247
Epoch 2500, training loss: 310.6439514160156 = 0.018996506929397583 + 50.0 * 6.212499618530273
Epoch 2500, val loss: 1.7749441862106323
Epoch 2510, training loss: 310.8958435058594 = 0.018758904188871384 + 50.0 * 6.217541694641113
Epoch 2510, val loss: 1.7781034708023071
Epoch 2520, training loss: 310.7232666015625 = 0.0185234434902668 + 50.0 * 6.214095115661621
Epoch 2520, val loss: 1.7813562154769897
Epoch 2530, training loss: 310.91632080078125 = 0.0182923786342144 + 50.0 * 6.217960834503174
Epoch 2530, val loss: 1.7851474285125732
Epoch 2540, training loss: 310.6182861328125 = 0.018056293949484825 + 50.0 * 6.212004661560059
Epoch 2540, val loss: 1.787873387336731
Epoch 2550, training loss: 310.60931396484375 = 0.017828477546572685 + 50.0 * 6.211830139160156
Epoch 2550, val loss: 1.7912997007369995
Epoch 2560, training loss: 310.7393798828125 = 0.017619462683796883 + 50.0 * 6.214435577392578
Epoch 2560, val loss: 1.794524073600769
Epoch 2570, training loss: 310.5206604003906 = 0.017400206997990608 + 50.0 * 6.2100653648376465
Epoch 2570, val loss: 1.7980200052261353
Epoch 2580, training loss: 310.5527038574219 = 0.01719891093671322 + 50.0 * 6.210709571838379
Epoch 2580, val loss: 1.8014799356460571
Epoch 2590, training loss: 310.6181335449219 = 0.01700914092361927 + 50.0 * 6.21202278137207
Epoch 2590, val loss: 1.80485999584198
Epoch 2600, training loss: 310.7794494628906 = 0.016804233193397522 + 50.0 * 6.21525239944458
Epoch 2600, val loss: 1.80787992477417
Epoch 2610, training loss: 310.6522521972656 = 0.016579413786530495 + 50.0 * 6.212713718414307
Epoch 2610, val loss: 1.8103642463684082
Epoch 2620, training loss: 310.5866394042969 = 0.01639748550951481 + 50.0 * 6.211404800415039
Epoch 2620, val loss: 1.814098834991455
Epoch 2630, training loss: 310.6200866699219 = 0.016202323138713837 + 50.0 * 6.212077617645264
Epoch 2630, val loss: 1.8170437812805176
Epoch 2640, training loss: 310.519775390625 = 0.016017161309719086 + 50.0 * 6.2100749015808105
Epoch 2640, val loss: 1.8201345205307007
Epoch 2650, training loss: 310.7472229003906 = 0.015840617939829826 + 50.0 * 6.214627742767334
Epoch 2650, val loss: 1.8229397535324097
Epoch 2660, training loss: 310.5122375488281 = 0.01564687304198742 + 50.0 * 6.209931373596191
Epoch 2660, val loss: 1.8261760473251343
Epoch 2670, training loss: 310.5009460449219 = 0.015468346886336803 + 50.0 * 6.209709644317627
Epoch 2670, val loss: 1.829094409942627
Epoch 2680, training loss: 310.6025390625 = 0.015294106677174568 + 50.0 * 6.211745262145996
Epoch 2680, val loss: 1.831910490989685
Epoch 2690, training loss: 310.5218505859375 = 0.015121995471417904 + 50.0 * 6.210134029388428
Epoch 2690, val loss: 1.8352670669555664
Epoch 2700, training loss: 310.6596374511719 = 0.014959998428821564 + 50.0 * 6.212893486022949
Epoch 2700, val loss: 1.838111162185669
Epoch 2710, training loss: 310.5489501953125 = 0.01478321012109518 + 50.0 * 6.210683345794678
Epoch 2710, val loss: 1.840504765510559
Epoch 2720, training loss: 310.4737854003906 = 0.014614677056670189 + 50.0 * 6.209183216094971
Epoch 2720, val loss: 1.8436193466186523
Epoch 2730, training loss: 310.4372253417969 = 0.014461314305663109 + 50.0 * 6.2084550857543945
Epoch 2730, val loss: 1.84686279296875
Epoch 2740, training loss: 310.4259033203125 = 0.014312784187495708 + 50.0 * 6.2082319259643555
Epoch 2740, val loss: 1.849669337272644
Epoch 2750, training loss: 310.81182861328125 = 0.014183830469846725 + 50.0 * 6.2159528732299805
Epoch 2750, val loss: 1.8524051904678345
Epoch 2760, training loss: 310.6233825683594 = 0.013998966664075851 + 50.0 * 6.21218729019165
Epoch 2760, val loss: 1.8548057079315186
Epoch 2770, training loss: 310.49346923828125 = 0.013841226696968079 + 50.0 * 6.209592342376709
Epoch 2770, val loss: 1.857454538345337
Epoch 2780, training loss: 310.39727783203125 = 0.013696531765162945 + 50.0 * 6.207671642303467
Epoch 2780, val loss: 1.8605985641479492
Epoch 2790, training loss: 310.3658142089844 = 0.01356226671487093 + 50.0 * 6.207045555114746
Epoch 2790, val loss: 1.8633701801300049
Epoch 2800, training loss: 310.5522155761719 = 0.013432497158646584 + 50.0 * 6.210775852203369
Epoch 2800, val loss: 1.8662670850753784
Epoch 2810, training loss: 310.39666748046875 = 0.01328106690198183 + 50.0 * 6.207667827606201
Epoch 2810, val loss: 1.8678491115570068
Epoch 2820, training loss: 310.38934326171875 = 0.013135679997503757 + 50.0 * 6.207524299621582
Epoch 2820, val loss: 1.87082839012146
Epoch 2830, training loss: 310.3941650390625 = 0.013002251274883747 + 50.0 * 6.207623481750488
Epoch 2830, val loss: 1.8735015392303467
Epoch 2840, training loss: 310.64154052734375 = 0.012877292931079865 + 50.0 * 6.212573051452637
Epoch 2840, val loss: 1.87604820728302
Epoch 2850, training loss: 310.396240234375 = 0.012740973383188248 + 50.0 * 6.207670211791992
Epoch 2850, val loss: 1.8787163496017456
Epoch 2860, training loss: 310.4355163574219 = 0.012612215243279934 + 50.0 * 6.208457946777344
Epoch 2860, val loss: 1.8810043334960938
Epoch 2870, training loss: 310.4534606933594 = 0.012482134625315666 + 50.0 * 6.20881986618042
Epoch 2870, val loss: 1.8834079504013062
Epoch 2880, training loss: 310.3504943847656 = 0.012361074797809124 + 50.0 * 6.206762790679932
Epoch 2880, val loss: 1.8864359855651855
Epoch 2890, training loss: 310.40924072265625 = 0.012243736535310745 + 50.0 * 6.207940101623535
Epoch 2890, val loss: 1.8889788389205933
Epoch 2900, training loss: 310.45953369140625 = 0.012125026434659958 + 50.0 * 6.208948135375977
Epoch 2900, val loss: 1.8910027742385864
Epoch 2910, training loss: 310.3700866699219 = 0.012007399462163448 + 50.0 * 6.207161903381348
Epoch 2910, val loss: 1.8935869932174683
Epoch 2920, training loss: 310.4792175292969 = 0.011895593255758286 + 50.0 * 6.209346294403076
Epoch 2920, val loss: 1.896315097808838
Epoch 2930, training loss: 310.37890625 = 0.011774555779993534 + 50.0 * 6.207342624664307
Epoch 2930, val loss: 1.8984642028808594
Epoch 2940, training loss: 310.358642578125 = 0.011661026626825333 + 50.0 * 6.206939697265625
Epoch 2940, val loss: 1.9004855155944824
Epoch 2950, training loss: 310.5141296386719 = 0.011560349725186825 + 50.0 * 6.210051536560059
Epoch 2950, val loss: 1.9028905630111694
Epoch 2960, training loss: 310.334716796875 = 0.011442110873758793 + 50.0 * 6.206465721130371
Epoch 2960, val loss: 1.9052149057388306
Epoch 2970, training loss: 310.38934326171875 = 0.011333110742270947 + 50.0 * 6.2075605392456055
Epoch 2970, val loss: 1.9075539112091064
Epoch 2980, training loss: 310.35443115234375 = 0.011230453848838806 + 50.0 * 6.206863880157471
Epoch 2980, val loss: 1.9099774360656738
Epoch 2990, training loss: 310.2731018066406 = 0.011121407151222229 + 50.0 * 6.205239772796631
Epoch 2990, val loss: 1.9120001792907715
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6814814814814815
0.8091723774380601
=== training gcn model ===
Epoch 0, training loss: 431.7882385253906 = 1.9454737901687622 + 50.0 * 8.596855163574219
Epoch 0, val loss: 1.9392616748809814
Epoch 10, training loss: 431.7471923828125 = 1.9358081817626953 + 50.0 * 8.596227645874023
Epoch 10, val loss: 1.9295936822891235
Epoch 20, training loss: 431.53216552734375 = 1.9236904382705688 + 50.0 * 8.592169761657715
Epoch 20, val loss: 1.9169386625289917
Epoch 30, training loss: 430.1515197753906 = 1.907922625541687 + 50.0 * 8.564871788024902
Epoch 30, val loss: 1.9001914262771606
Epoch 40, training loss: 421.5489807128906 = 1.8885087966918945 + 50.0 * 8.393209457397461
Epoch 40, val loss: 1.8806110620498657
Epoch 50, training loss: 392.9671630859375 = 1.8674710988998413 + 50.0 * 7.821994304656982
Epoch 50, val loss: 1.8596112728118896
Epoch 60, training loss: 376.5770263671875 = 1.8509345054626465 + 50.0 * 7.4945220947265625
Epoch 60, val loss: 1.8441119194030762
Epoch 70, training loss: 361.3906555175781 = 1.8407748937606812 + 50.0 * 7.190997123718262
Epoch 70, val loss: 1.834687352180481
Epoch 80, training loss: 352.46783447265625 = 1.8302758932113647 + 50.0 * 7.012751579284668
Epoch 80, val loss: 1.8250902891159058
Epoch 90, training loss: 346.8289489746094 = 1.8201663494110107 + 50.0 * 6.900176048278809
Epoch 90, val loss: 1.8162966966629028
Epoch 100, training loss: 342.1332702636719 = 1.8102490901947021 + 50.0 * 6.806460380554199
Epoch 100, val loss: 1.80742609500885
Epoch 110, training loss: 339.1277770996094 = 1.8010478019714355 + 50.0 * 6.74653434753418
Epoch 110, val loss: 1.7994083166122437
Epoch 120, training loss: 336.6734313964844 = 1.7922049760818481 + 50.0 * 6.697624683380127
Epoch 120, val loss: 1.791683554649353
Epoch 130, training loss: 334.7179870605469 = 1.7835392951965332 + 50.0 * 6.658689022064209
Epoch 130, val loss: 1.7841300964355469
Epoch 140, training loss: 333.02264404296875 = 1.7743914127349854 + 50.0 * 6.624965190887451
Epoch 140, val loss: 1.7764019966125488
Epoch 150, training loss: 331.7460632324219 = 1.7646169662475586 + 50.0 * 6.599628925323486
Epoch 150, val loss: 1.7679884433746338
Epoch 160, training loss: 330.6585388183594 = 1.7541180849075317 + 50.0 * 6.578088283538818
Epoch 160, val loss: 1.7588529586791992
Epoch 170, training loss: 329.820068359375 = 1.7427624464035034 + 50.0 * 6.5615458488464355
Epoch 170, val loss: 1.74899423122406
Epoch 180, training loss: 328.8675231933594 = 1.7306305170059204 + 50.0 * 6.54273796081543
Epoch 180, val loss: 1.738349437713623
Epoch 190, training loss: 328.0179748535156 = 1.7175766229629517 + 50.0 * 6.526007652282715
Epoch 190, val loss: 1.7269566059112549
Epoch 200, training loss: 327.2726745605469 = 1.7035608291625977 + 50.0 * 6.511382579803467
Epoch 200, val loss: 1.7145508527755737
Epoch 210, training loss: 326.5367736816406 = 1.6882926225662231 + 50.0 * 6.496969699859619
Epoch 210, val loss: 1.7012205123901367
Epoch 220, training loss: 325.81085205078125 = 1.6719236373901367 + 50.0 * 6.482778549194336
Epoch 220, val loss: 1.6868854761123657
Epoch 230, training loss: 325.15277099609375 = 1.6543842554092407 + 50.0 * 6.469967365264893
Epoch 230, val loss: 1.671489953994751
Epoch 240, training loss: 324.72100830078125 = 1.6356168985366821 + 50.0 * 6.461707592010498
Epoch 240, val loss: 1.6550650596618652
Epoch 250, training loss: 324.0829772949219 = 1.6154061555862427 + 50.0 * 6.4493513107299805
Epoch 250, val loss: 1.6376029253005981
Epoch 260, training loss: 323.5690002441406 = 1.5939542055130005 + 50.0 * 6.43950080871582
Epoch 260, val loss: 1.619309425354004
Epoch 270, training loss: 323.07635498046875 = 1.571439504623413 + 50.0 * 6.430098533630371
Epoch 270, val loss: 1.5999287366867065
Epoch 280, training loss: 322.8590393066406 = 1.5478553771972656 + 50.0 * 6.4262237548828125
Epoch 280, val loss: 1.5799702405929565
Epoch 290, training loss: 322.3084716796875 = 1.5234274864196777 + 50.0 * 6.415700435638428
Epoch 290, val loss: 1.5592378377914429
Epoch 300, training loss: 321.8835144042969 = 1.498199701309204 + 50.0 * 6.407706260681152
Epoch 300, val loss: 1.5383013486862183
Epoch 310, training loss: 321.7188415527344 = 1.472424864768982 + 50.0 * 6.404928684234619
Epoch 310, val loss: 1.5170414447784424
Epoch 320, training loss: 321.2667236328125 = 1.44612717628479 + 50.0 * 6.396411895751953
Epoch 320, val loss: 1.4957542419433594
Epoch 330, training loss: 320.9132385253906 = 1.4195337295532227 + 50.0 * 6.38987398147583
Epoch 330, val loss: 1.4744412899017334
Epoch 340, training loss: 320.91693115234375 = 1.392840027809143 + 50.0 * 6.390481472015381
Epoch 340, val loss: 1.4530112743377686
Epoch 350, training loss: 320.3544006347656 = 1.365860104560852 + 50.0 * 6.379770278930664
Epoch 350, val loss: 1.431969165802002
Epoch 360, training loss: 320.1162109375 = 1.3389934301376343 + 50.0 * 6.375544548034668
Epoch 360, val loss: 1.4112536907196045
Epoch 370, training loss: 320.0270690917969 = 1.3122910261154175 + 50.0 * 6.374295711517334
Epoch 370, val loss: 1.3906702995300293
Epoch 380, training loss: 319.6647033691406 = 1.2857054471969604 + 50.0 * 6.367579936981201
Epoch 380, val loss: 1.3705966472625732
Epoch 390, training loss: 319.4432678222656 = 1.259287714958191 + 50.0 * 6.363679885864258
Epoch 390, val loss: 1.3510416746139526
Epoch 400, training loss: 319.2103271484375 = 1.2331947088241577 + 50.0 * 6.3595428466796875
Epoch 400, val loss: 1.3315558433532715
Epoch 410, training loss: 319.04998779296875 = 1.20741605758667 + 50.0 * 6.356851100921631
Epoch 410, val loss: 1.312629222869873
Epoch 420, training loss: 318.8304138183594 = 1.1820521354675293 + 50.0 * 6.352967262268066
Epoch 420, val loss: 1.2942235469818115
Epoch 430, training loss: 318.7685241699219 = 1.157109022140503 + 50.0 * 6.35222864151001
Epoch 430, val loss: 1.2765387296676636
Epoch 440, training loss: 318.4881896972656 = 1.1327102184295654 + 50.0 * 6.347109317779541
Epoch 440, val loss: 1.2591243982315063
Epoch 450, training loss: 318.2911376953125 = 1.1087052822113037 + 50.0 * 6.343648910522461
Epoch 450, val loss: 1.2423919439315796
Epoch 460, training loss: 318.1163635253906 = 1.0852129459381104 + 50.0 * 6.340623378753662
Epoch 460, val loss: 1.2264903783798218
Epoch 470, training loss: 318.2991638183594 = 1.0621484518051147 + 50.0 * 6.344740867614746
Epoch 470, val loss: 1.210862159729004
Epoch 480, training loss: 317.826904296875 = 1.0396571159362793 + 50.0 * 6.335745334625244
Epoch 480, val loss: 1.1956430673599243
Epoch 490, training loss: 317.6381530761719 = 1.0176953077316284 + 50.0 * 6.332408905029297
Epoch 490, val loss: 1.1810336112976074
Epoch 500, training loss: 317.51800537109375 = 0.9962787628173828 + 50.0 * 6.330434322357178
Epoch 500, val loss: 1.1673775911331177
Epoch 510, training loss: 317.403564453125 = 0.9752844572067261 + 50.0 * 6.32856559753418
Epoch 510, val loss: 1.1541475057601929
Epoch 520, training loss: 317.2228698730469 = 0.9547092318534851 + 50.0 * 6.3253631591796875
Epoch 520, val loss: 1.1414177417755127
Epoch 530, training loss: 317.0875549316406 = 0.9347379207611084 + 50.0 * 6.323056697845459
Epoch 530, val loss: 1.1293411254882812
Epoch 540, training loss: 316.9350891113281 = 0.9152563214302063 + 50.0 * 6.320396900177002
Epoch 540, val loss: 1.117859125137329
Epoch 550, training loss: 317.4237060546875 = 0.8962231278419495 + 50.0 * 6.330549716949463
Epoch 550, val loss: 1.1066954135894775
Epoch 560, training loss: 316.776611328125 = 0.8773267865180969 + 50.0 * 6.317985534667969
Epoch 560, val loss: 1.0961377620697021
Epoch 570, training loss: 316.6031188964844 = 0.8589830994606018 + 50.0 * 6.314882755279541
Epoch 570, val loss: 1.0862013101577759
Epoch 580, training loss: 316.5260314941406 = 0.8411048650741577 + 50.0 * 6.313698768615723
Epoch 580, val loss: 1.076629638671875
Epoch 590, training loss: 316.54443359375 = 0.8234434723854065 + 50.0 * 6.314419746398926
Epoch 590, val loss: 1.067110538482666
Epoch 600, training loss: 316.29595947265625 = 0.8061230778694153 + 50.0 * 6.3097968101501465
Epoch 600, val loss: 1.058655023574829
Epoch 610, training loss: 316.18621826171875 = 0.789288341999054 + 50.0 * 6.307938575744629
Epoch 610, val loss: 1.0505231618881226
Epoch 620, training loss: 316.0621337890625 = 0.772804319858551 + 50.0 * 6.305786609649658
Epoch 620, val loss: 1.042896032333374
Epoch 630, training loss: 316.06451416015625 = 0.7566721439361572 + 50.0 * 6.306156635284424
Epoch 630, val loss: 1.0359373092651367
Epoch 640, training loss: 315.92724609375 = 0.7408455014228821 + 50.0 * 6.303728103637695
Epoch 640, val loss: 1.028594970703125
Epoch 650, training loss: 315.8611145019531 = 0.7252106070518494 + 50.0 * 6.302718162536621
Epoch 650, val loss: 1.0225697755813599
Epoch 660, training loss: 315.7234802246094 = 0.7100750207901001 + 50.0 * 6.300268173217773
Epoch 660, val loss: 1.016525149345398
Epoch 670, training loss: 315.59625244140625 = 0.6952356100082397 + 50.0 * 6.298020362854004
Epoch 670, val loss: 1.0110483169555664
Epoch 680, training loss: 315.6225891113281 = 0.6807632446289062 + 50.0 * 6.298836708068848
Epoch 680, val loss: 1.0061051845550537
Epoch 690, training loss: 315.628173828125 = 0.6664553284645081 + 50.0 * 6.299234867095947
Epoch 690, val loss: 1.0013784170150757
Epoch 700, training loss: 315.37261962890625 = 0.6524089574813843 + 50.0 * 6.29440450668335
Epoch 700, val loss: 0.9969651103019714
Epoch 710, training loss: 315.3209228515625 = 0.638816773891449 + 50.0 * 6.293642044067383
Epoch 710, val loss: 0.9934685826301575
Epoch 720, training loss: 315.3878479003906 = 0.6255542039871216 + 50.0 * 6.295246124267578
Epoch 720, val loss: 0.990139901638031
Epoch 730, training loss: 315.1133728027344 = 0.6124634146690369 + 50.0 * 6.290018081665039
Epoch 730, val loss: 0.9870530962944031
Epoch 740, training loss: 315.0500183105469 = 0.5997924208641052 + 50.0 * 6.289004802703857
Epoch 740, val loss: 0.9846456050872803
Epoch 750, training loss: 315.1622314453125 = 0.5874419808387756 + 50.0 * 6.2914958000183105
Epoch 750, val loss: 0.9824448227882385
Epoch 760, training loss: 315.0655517578125 = 0.5752794146537781 + 50.0 * 6.2898054122924805
Epoch 760, val loss: 0.9799081087112427
Epoch 770, training loss: 314.85626220703125 = 0.5633760690689087 + 50.0 * 6.285857677459717
Epoch 770, val loss: 0.9786052703857422
Epoch 780, training loss: 314.78094482421875 = 0.5519038438796997 + 50.0 * 6.284580707550049
Epoch 780, val loss: 0.9771431088447571
Epoch 790, training loss: 314.7265319824219 = 0.5407015085220337 + 50.0 * 6.283716678619385
Epoch 790, val loss: 0.9762844443321228
Epoch 800, training loss: 314.718994140625 = 0.5296906232833862 + 50.0 * 6.283786296844482
Epoch 800, val loss: 0.9750906229019165
Epoch 810, training loss: 314.7967224121094 = 0.5188468098640442 + 50.0 * 6.285557270050049
Epoch 810, val loss: 0.9748629927635193
Epoch 820, training loss: 314.54864501953125 = 0.5083096027374268 + 50.0 * 6.280807018280029
Epoch 820, val loss: 0.9747661352157593
Epoch 830, training loss: 314.4654846191406 = 0.4981020390987396 + 50.0 * 6.2793474197387695
Epoch 830, val loss: 0.9746806025505066
Epoch 840, training loss: 314.4758605957031 = 0.48817163705825806 + 50.0 * 6.279754161834717
Epoch 840, val loss: 0.9751008152961731
Epoch 850, training loss: 314.35736083984375 = 0.4783284068107605 + 50.0 * 6.277580261230469
Epoch 850, val loss: 0.9757883548736572
Epoch 860, training loss: 314.4316101074219 = 0.46872010827064514 + 50.0 * 6.279257774353027
Epoch 860, val loss: 0.976387083530426
Epoch 870, training loss: 314.1882019042969 = 0.4593304693698883 + 50.0 * 6.274577617645264
Epoch 870, val loss: 0.9777361750602722
Epoch 880, training loss: 314.18951416015625 = 0.45022451877593994 + 50.0 * 6.274785995483398
Epoch 880, val loss: 0.9792097210884094
Epoch 890, training loss: 314.32647705078125 = 0.44126036763191223 + 50.0 * 6.277704238891602
Epoch 890, val loss: 0.9805880784988403
Epoch 900, training loss: 314.1529541015625 = 0.4324583113193512 + 50.0 * 6.274409770965576
Epoch 900, val loss: 0.9822064638137817
Epoch 910, training loss: 314.00726318359375 = 0.42381688952445984 + 50.0 * 6.271668910980225
Epoch 910, val loss: 0.9838640093803406
Epoch 920, training loss: 313.9479675292969 = 0.41542887687683105 + 50.0 * 6.270650386810303
Epoch 920, val loss: 0.9855698347091675
Epoch 930, training loss: 314.0745544433594 = 0.4072135090827942 + 50.0 * 6.273346424102783
Epoch 930, val loss: 0.987490177154541
Epoch 940, training loss: 313.879638671875 = 0.39905136823654175 + 50.0 * 6.269611835479736
Epoch 940, val loss: 0.989959716796875
Epoch 950, training loss: 313.8155517578125 = 0.39106130599975586 + 50.0 * 6.268489837646484
Epoch 950, val loss: 0.9922987222671509
Epoch 960, training loss: 313.84246826171875 = 0.38321518898010254 + 50.0 * 6.2691850662231445
Epoch 960, val loss: 0.9947676062583923
Epoch 970, training loss: 313.7629089355469 = 0.37548673152923584 + 50.0 * 6.2677483558654785
Epoch 970, val loss: 0.9967105388641357
Epoch 980, training loss: 313.7666015625 = 0.3679054379463196 + 50.0 * 6.26797342300415
Epoch 980, val loss: 0.9992848634719849
Epoch 990, training loss: 313.65985107421875 = 0.36039599776268005 + 50.0 * 6.265989303588867
Epoch 990, val loss: 1.0019587278366089
Epoch 1000, training loss: 313.5378723144531 = 0.3530557155609131 + 50.0 * 6.263696193695068
Epoch 1000, val loss: 1.0048834085464478
Epoch 1010, training loss: 313.5408935546875 = 0.34589096903800964 + 50.0 * 6.263900279998779
Epoch 1010, val loss: 1.0075970888137817
Epoch 1020, training loss: 313.5901794433594 = 0.33881184458732605 + 50.0 * 6.2650275230407715
Epoch 1020, val loss: 1.0104820728302002
Epoch 1030, training loss: 313.80853271484375 = 0.33177071809768677 + 50.0 * 6.269535541534424
Epoch 1030, val loss: 1.0136773586273193
Epoch 1040, training loss: 313.4678649902344 = 0.3247625529766083 + 50.0 * 6.262861728668213
Epoch 1040, val loss: 1.0169075727462769
Epoch 1050, training loss: 313.34735107421875 = 0.3179968595504761 + 50.0 * 6.260587215423584
Epoch 1050, val loss: 1.0199726819992065
Epoch 1060, training loss: 313.2887878417969 = 0.31134915351867676 + 50.0 * 6.259549140930176
Epoch 1060, val loss: 1.023482084274292
Epoch 1070, training loss: 313.5492858886719 = 0.30481386184692383 + 50.0 * 6.264889717102051
Epoch 1070, val loss: 1.0270105600357056
Epoch 1080, training loss: 313.29498291015625 = 0.29825592041015625 + 50.0 * 6.259934902191162
Epoch 1080, val loss: 1.0302016735076904
Epoch 1090, training loss: 313.2339782714844 = 0.29184573888778687 + 50.0 * 6.258842945098877
Epoch 1090, val loss: 1.0339438915252686
Epoch 1100, training loss: 313.3585510253906 = 0.2855335474014282 + 50.0 * 6.261460304260254
Epoch 1100, val loss: 1.0374869108200073
Epoch 1110, training loss: 313.1341552734375 = 0.279326856136322 + 50.0 * 6.257096767425537
Epoch 1110, val loss: 1.0406187772750854
Epoch 1120, training loss: 313.06805419921875 = 0.2732298672199249 + 50.0 * 6.25589656829834
Epoch 1120, val loss: 1.0444893836975098
Epoch 1130, training loss: 313.4166259765625 = 0.2672068774700165 + 50.0 * 6.262988090515137
Epoch 1130, val loss: 1.0486350059509277
Epoch 1140, training loss: 313.03228759765625 = 0.26127180457115173 + 50.0 * 6.255420684814453
Epoch 1140, val loss: 1.0515797138214111
Epoch 1150, training loss: 312.9783630371094 = 0.25543397665023804 + 50.0 * 6.254458427429199
Epoch 1150, val loss: 1.0555753707885742
Epoch 1160, training loss: 313.1202392578125 = 0.2497137486934662 + 50.0 * 6.257410526275635
Epoch 1160, val loss: 1.0595953464508057
Epoch 1170, training loss: 312.9442138671875 = 0.24411140382289886 + 50.0 * 6.254002094268799
Epoch 1170, val loss: 1.0636976957321167
Epoch 1180, training loss: 312.88763427734375 = 0.23861722648143768 + 50.0 * 6.2529802322387695
Epoch 1180, val loss: 1.0676112174987793
Epoch 1190, training loss: 313.07293701171875 = 0.23325090110301971 + 50.0 * 6.256793975830078
Epoch 1190, val loss: 1.07206392288208
Epoch 1200, training loss: 312.8612365722656 = 0.2279515117406845 + 50.0 * 6.2526655197143555
Epoch 1200, val loss: 1.0760096311569214
Epoch 1210, training loss: 312.756591796875 = 0.2227504402399063 + 50.0 * 6.25067663192749
Epoch 1210, val loss: 1.0801764726638794
Epoch 1220, training loss: 313.0687255859375 = 0.21772539615631104 + 50.0 * 6.257020473480225
Epoch 1220, val loss: 1.0851354598999023
Epoch 1230, training loss: 312.948486328125 = 0.2127114087343216 + 50.0 * 6.254715442657471
Epoch 1230, val loss: 1.0881292819976807
Epoch 1240, training loss: 312.6453552246094 = 0.20775842666625977 + 50.0 * 6.248751640319824
Epoch 1240, val loss: 1.0930054187774658
Epoch 1250, training loss: 312.62347412109375 = 0.20303069055080414 + 50.0 * 6.248408794403076
Epoch 1250, val loss: 1.09776771068573
Epoch 1260, training loss: 312.57733154296875 = 0.1984204202890396 + 50.0 * 6.247578144073486
Epoch 1260, val loss: 1.102146029472351
Epoch 1270, training loss: 313.22161865234375 = 0.19389741122722626 + 50.0 * 6.260554313659668
Epoch 1270, val loss: 1.1072014570236206
Epoch 1280, training loss: 312.6725769042969 = 0.18939676880836487 + 50.0 * 6.249663829803467
Epoch 1280, val loss: 1.110823154449463
Epoch 1290, training loss: 312.474365234375 = 0.1850123107433319 + 50.0 * 6.245787620544434
Epoch 1290, val loss: 1.1157710552215576
Epoch 1300, training loss: 312.45452880859375 = 0.1807933896780014 + 50.0 * 6.245474815368652
Epoch 1300, val loss: 1.1205205917358398
Epoch 1310, training loss: 312.50799560546875 = 0.17668767273426056 + 50.0 * 6.246625900268555
Epoch 1310, val loss: 1.1249544620513916
Epoch 1320, training loss: 312.45013427734375 = 0.172623410820961 + 50.0 * 6.24554967880249
Epoch 1320, val loss: 1.129927158355713
Epoch 1330, training loss: 312.4588928222656 = 0.1686471402645111 + 50.0 * 6.245805263519287
Epoch 1330, val loss: 1.1344733238220215
Epoch 1340, training loss: 312.4713439941406 = 0.1647910624742508 + 50.0 * 6.24613094329834
Epoch 1340, val loss: 1.1398138999938965
Epoch 1350, training loss: 312.3451843261719 = 0.1610143631696701 + 50.0 * 6.243683338165283
Epoch 1350, val loss: 1.1447627544403076
Epoch 1360, training loss: 312.3025817871094 = 0.1573682278394699 + 50.0 * 6.242904186248779
Epoch 1360, val loss: 1.1493512392044067
Epoch 1370, training loss: 312.57421875 = 0.15384700894355774 + 50.0 * 6.248407363891602
Epoch 1370, val loss: 1.154520034790039
Epoch 1380, training loss: 312.3371887207031 = 0.15026190876960754 + 50.0 * 6.243738651275635
Epoch 1380, val loss: 1.1595453023910522
Epoch 1390, training loss: 312.2790832519531 = 0.14685967564582825 + 50.0 * 6.242644786834717
Epoch 1390, val loss: 1.1649340391159058
Epoch 1400, training loss: 312.2560729980469 = 0.14353738725185394 + 50.0 * 6.242250919342041
Epoch 1400, val loss: 1.1704139709472656
Epoch 1410, training loss: 312.3394775390625 = 0.14031435549259186 + 50.0 * 6.243983268737793
Epoch 1410, val loss: 1.175742506980896
Epoch 1420, training loss: 312.1479187011719 = 0.13715149462223053 + 50.0 * 6.240215301513672
Epoch 1420, val loss: 1.1805734634399414
Epoch 1430, training loss: 312.3149719238281 = 0.13410061597824097 + 50.0 * 6.243617534637451
Epoch 1430, val loss: 1.186463475227356
Epoch 1440, training loss: 312.20050048828125 = 0.1310654729604721 + 50.0 * 6.241388320922852
Epoch 1440, val loss: 1.1912161111831665
Epoch 1450, training loss: 312.11322021484375 = 0.12813079357147217 + 50.0 * 6.239701747894287
Epoch 1450, val loss: 1.1965982913970947
Epoch 1460, training loss: 312.0704345703125 = 0.1252851039171219 + 50.0 * 6.238903045654297
Epoch 1460, val loss: 1.202146291732788
Epoch 1470, training loss: 312.12786865234375 = 0.12253343313932419 + 50.0 * 6.240106582641602
Epoch 1470, val loss: 1.2078216075897217
Epoch 1480, training loss: 312.11395263671875 = 0.11982935667037964 + 50.0 * 6.239882469177246
Epoch 1480, val loss: 1.213431477546692
Epoch 1490, training loss: 312.2176513671875 = 0.11717008054256439 + 50.0 * 6.242010116577148
Epoch 1490, val loss: 1.219346046447754
Epoch 1500, training loss: 312.01910400390625 = 0.11459220200777054 + 50.0 * 6.2380900382995605
Epoch 1500, val loss: 1.2237870693206787
Epoch 1510, training loss: 311.9800109863281 = 0.11209405958652496 + 50.0 * 6.237358570098877
Epoch 1510, val loss: 1.2295310497283936
Epoch 1520, training loss: 311.922607421875 = 0.10966420918703079 + 50.0 * 6.2362589836120605
Epoch 1520, val loss: 1.235111951828003
Epoch 1530, training loss: 311.99176025390625 = 0.10730695724487305 + 50.0 * 6.23768949508667
Epoch 1530, val loss: 1.2407042980194092
Epoch 1540, training loss: 312.0447998046875 = 0.10498545318841934 + 50.0 * 6.238796234130859
Epoch 1540, val loss: 1.2462775707244873
Epoch 1550, training loss: 311.9670715332031 = 0.10271275788545609 + 50.0 * 6.2372870445251465
Epoch 1550, val loss: 1.251785159111023
Epoch 1560, training loss: 311.8708801269531 = 0.1005079373717308 + 50.0 * 6.235407829284668
Epoch 1560, val loss: 1.2573994398117065
Epoch 1570, training loss: 312.04144287109375 = 0.09839165210723877 + 50.0 * 6.238861083984375
Epoch 1570, val loss: 1.263452410697937
Epoch 1580, training loss: 311.9281921386719 = 0.09628628939390182 + 50.0 * 6.236638069152832
Epoch 1580, val loss: 1.268491506576538
Epoch 1590, training loss: 311.8319396972656 = 0.09423045814037323 + 50.0 * 6.2347540855407715
Epoch 1590, val loss: 1.2744147777557373
Epoch 1600, training loss: 311.7957763671875 = 0.0922599732875824 + 50.0 * 6.234070301055908
Epoch 1600, val loss: 1.2794129848480225
Epoch 1610, training loss: 311.8323974609375 = 0.09034883975982666 + 50.0 * 6.2348408699035645
Epoch 1610, val loss: 1.2851624488830566
Epoch 1620, training loss: 311.9161376953125 = 0.088474802672863 + 50.0 * 6.236553192138672
Epoch 1620, val loss: 1.2905628681182861
Epoch 1630, training loss: 312.025390625 = 0.08660590648651123 + 50.0 * 6.238775730133057
Epoch 1630, val loss: 1.2976937294006348
Epoch 1640, training loss: 311.76361083984375 = 0.08480910211801529 + 50.0 * 6.233575820922852
Epoch 1640, val loss: 1.3017096519470215
Epoch 1650, training loss: 311.7065124511719 = 0.08305167406797409 + 50.0 * 6.23246955871582
Epoch 1650, val loss: 1.3082088232040405
Epoch 1660, training loss: 311.6717224121094 = 0.08136409521102905 + 50.0 * 6.231807231903076
Epoch 1660, val loss: 1.3139501810073853
Epoch 1670, training loss: 311.81146240234375 = 0.0797278955578804 + 50.0 * 6.2346343994140625
Epoch 1670, val loss: 1.3195608854293823
Epoch 1680, training loss: 311.7648620605469 = 0.07810656726360321 + 50.0 * 6.233735084533691
Epoch 1680, val loss: 1.3247028589248657
Epoch 1690, training loss: 311.6904602050781 = 0.07651138305664062 + 50.0 * 6.232278823852539
Epoch 1690, val loss: 1.3310377597808838
Epoch 1700, training loss: 311.6802673339844 = 0.07498033344745636 + 50.0 * 6.232105731964111
Epoch 1700, val loss: 1.3367154598236084
Epoch 1710, training loss: 311.7229309082031 = 0.0734909400343895 + 50.0 * 6.2329888343811035
Epoch 1710, val loss: 1.34256112575531
Epoch 1720, training loss: 311.7610778808594 = 0.07203146815299988 + 50.0 * 6.233781337738037
Epoch 1720, val loss: 1.3475874662399292
Epoch 1730, training loss: 311.6372375488281 = 0.07058459520339966 + 50.0 * 6.231332778930664
Epoch 1730, val loss: 1.354519248008728
Epoch 1740, training loss: 311.582763671875 = 0.0691976472735405 + 50.0 * 6.230271816253662
Epoch 1740, val loss: 1.3599555492401123
Epoch 1750, training loss: 311.5412902832031 = 0.06785137951374054 + 50.0 * 6.229468822479248
Epoch 1750, val loss: 1.3651275634765625
Epoch 1760, training loss: 311.58917236328125 = 0.06654419749975204 + 50.0 * 6.230452537536621
Epoch 1760, val loss: 1.370996356010437
Epoch 1770, training loss: 311.74163818359375 = 0.06524813920259476 + 50.0 * 6.233528137207031
Epoch 1770, val loss: 1.3769471645355225
Epoch 1780, training loss: 311.6187438964844 = 0.06398354470729828 + 50.0 * 6.231094837188721
Epoch 1780, val loss: 1.381704568862915
Epoch 1790, training loss: 311.6287536621094 = 0.06275048106908798 + 50.0 * 6.231319904327393
Epoch 1790, val loss: 1.387648344039917
Epoch 1800, training loss: 311.48126220703125 = 0.06153363361954689 + 50.0 * 6.228394508361816
Epoch 1800, val loss: 1.3930392265319824
Epoch 1810, training loss: 311.56890869140625 = 0.06037655472755432 + 50.0 * 6.230170726776123
Epoch 1810, val loss: 1.3986002206802368
Epoch 1820, training loss: 311.803955078125 = 0.05923350527882576 + 50.0 * 6.234894275665283
Epoch 1820, val loss: 1.4042142629623413
Epoch 1830, training loss: 311.4772644042969 = 0.05807472765445709 + 50.0 * 6.228384017944336
Epoch 1830, val loss: 1.409868836402893
Epoch 1840, training loss: 311.4007263183594 = 0.05698106810450554 + 50.0 * 6.226875305175781
Epoch 1840, val loss: 1.4152225255966187
Epoch 1850, training loss: 311.3642578125 = 0.055929940193891525 + 50.0 * 6.226166248321533
Epoch 1850, val loss: 1.420793890953064
Epoch 1860, training loss: 311.38690185546875 = 0.0549108162522316 + 50.0 * 6.226640224456787
Epoch 1860, val loss: 1.4264185428619385
Epoch 1870, training loss: 311.70831298828125 = 0.053916048258543015 + 50.0 * 6.23308801651001
Epoch 1870, val loss: 1.4314932823181152
Epoch 1880, training loss: 311.4495849609375 = 0.05287150666117668 + 50.0 * 6.22793436050415
Epoch 1880, val loss: 1.4372879266738892
Epoch 1890, training loss: 311.4078674316406 = 0.05191104859113693 + 50.0 * 6.227118968963623
Epoch 1890, val loss: 1.4419759511947632
Epoch 1900, training loss: 311.65679931640625 = 0.05096517503261566 + 50.0 * 6.23211669921875
Epoch 1900, val loss: 1.4470465183258057
Epoch 1910, training loss: 311.3555908203125 = 0.05001084506511688 + 50.0 * 6.22611141204834
Epoch 1910, val loss: 1.4533026218414307
Epoch 1920, training loss: 311.2952880859375 = 0.04908375069499016 + 50.0 * 6.224924564361572
Epoch 1920, val loss: 1.4585628509521484
Epoch 1930, training loss: 311.26153564453125 = 0.04820728674530983 + 50.0 * 6.224266052246094
Epoch 1930, val loss: 1.463757038116455
Epoch 1940, training loss: 311.3793640136719 = 0.04736059531569481 + 50.0 * 6.226640224456787
Epoch 1940, val loss: 1.469942331314087
Epoch 1950, training loss: 311.31280517578125 = 0.046502262353897095 + 50.0 * 6.225326061248779
Epoch 1950, val loss: 1.4743753671646118
Epoch 1960, training loss: 311.2218017578125 = 0.04566687345504761 + 50.0 * 6.223523139953613
Epoch 1960, val loss: 1.4796662330627441
Epoch 1970, training loss: 311.2044677734375 = 0.04485594108700752 + 50.0 * 6.22319221496582
Epoch 1970, val loss: 1.4847019910812378
Epoch 1980, training loss: 311.22662353515625 = 0.04408124089241028 + 50.0 * 6.22365140914917
Epoch 1980, val loss: 1.4900152683258057
Epoch 1990, training loss: 311.49169921875 = 0.04332384094595909 + 50.0 * 6.228967189788818
Epoch 1990, val loss: 1.4945480823516846
Epoch 2000, training loss: 311.26806640625 = 0.042554572224617004 + 50.0 * 6.224510669708252
Epoch 2000, val loss: 1.499904751777649
Epoch 2010, training loss: 311.3253173828125 = 0.04180508106946945 + 50.0 * 6.225670337677002
Epoch 2010, val loss: 1.5055558681488037
Epoch 2020, training loss: 311.1728515625 = 0.04108123853802681 + 50.0 * 6.222635746002197
Epoch 2020, val loss: 1.509880542755127
Epoch 2030, training loss: 311.1700439453125 = 0.04037969186902046 + 50.0 * 6.222593307495117
Epoch 2030, val loss: 1.5151770114898682
Epoch 2040, training loss: 311.47686767578125 = 0.03971225768327713 + 50.0 * 6.228743553161621
Epoch 2040, val loss: 1.5195136070251465
Epoch 2050, training loss: 311.1894836425781 = 0.03901541233062744 + 50.0 * 6.2230095863342285
Epoch 2050, val loss: 1.5246546268463135
Epoch 2060, training loss: 311.18572998046875 = 0.038360364735126495 + 50.0 * 6.222947597503662
Epoch 2060, val loss: 1.5290398597717285
Epoch 2070, training loss: 311.1601257324219 = 0.0377151295542717 + 50.0 * 6.222448348999023
Epoch 2070, val loss: 1.5341421365737915
Epoch 2080, training loss: 311.05572509765625 = 0.037077199667692184 + 50.0 * 6.220372676849365
Epoch 2080, val loss: 1.5394361019134521
Epoch 2090, training loss: 311.2088928222656 = 0.036476101726293564 + 50.0 * 6.223448753356934
Epoch 2090, val loss: 1.5443538427352905
Epoch 2100, training loss: 311.2495422363281 = 0.03587112948298454 + 50.0 * 6.224273681640625
Epoch 2100, val loss: 1.548096776008606
Epoch 2110, training loss: 311.083740234375 = 0.03524681553244591 + 50.0 * 6.2209696769714355
Epoch 2110, val loss: 1.5537359714508057
Epoch 2120, training loss: 311.06341552734375 = 0.03467293456196785 + 50.0 * 6.220574855804443
Epoch 2120, val loss: 1.558233618736267
Epoch 2130, training loss: 311.0317077636719 = 0.0341191366314888 + 50.0 * 6.219951629638672
Epoch 2130, val loss: 1.5632785558700562
Epoch 2140, training loss: 311.2770080566406 = 0.03358321264386177 + 50.0 * 6.2248687744140625
Epoch 2140, val loss: 1.5675389766693115
Epoch 2150, training loss: 311.0872497558594 = 0.03302562236785889 + 50.0 * 6.2210845947265625
Epoch 2150, val loss: 1.5720412731170654
Epoch 2160, training loss: 311.0654296875 = 0.032496124505996704 + 50.0 * 6.220658302307129
Epoch 2160, val loss: 1.5768142938613892
Epoch 2170, training loss: 311.15045166015625 = 0.03197568655014038 + 50.0 * 6.22236967086792
Epoch 2170, val loss: 1.5809128284454346
Epoch 2180, training loss: 310.9967041015625 = 0.0314631350338459 + 50.0 * 6.219305038452148
Epoch 2180, val loss: 1.5860507488250732
Epoch 2190, training loss: 310.9793701171875 = 0.03097856044769287 + 50.0 * 6.218967914581299
Epoch 2190, val loss: 1.5905686616897583
Epoch 2200, training loss: 311.4178771972656 = 0.030521061271429062 + 50.0 * 6.227747440338135
Epoch 2200, val loss: 1.5936143398284912
Epoch 2210, training loss: 311.0797119140625 = 0.030009081587195396 + 50.0 * 6.220994472503662
Epoch 2210, val loss: 1.599897861480713
Epoch 2220, training loss: 310.9761962890625 = 0.02953844889998436 + 50.0 * 6.21893310546875
Epoch 2220, val loss: 1.6037707328796387
Epoch 2230, training loss: 310.9199523925781 = 0.02908431924879551 + 50.0 * 6.217817306518555
Epoch 2230, val loss: 1.6089425086975098
Epoch 2240, training loss: 310.91510009765625 = 0.02865433320403099 + 50.0 * 6.217728614807129
Epoch 2240, val loss: 1.6129305362701416
Epoch 2250, training loss: 311.5321350097656 = 0.02824767492711544 + 50.0 * 6.230077743530273
Epoch 2250, val loss: 1.6161844730377197
Epoch 2260, training loss: 311.12408447265625 = 0.02777734585106373 + 50.0 * 6.221926212310791
Epoch 2260, val loss: 1.6223676204681396
Epoch 2270, training loss: 310.93426513671875 = 0.027358317747712135 + 50.0 * 6.218138217926025
Epoch 2270, val loss: 1.625630259513855
Epoch 2280, training loss: 310.9376525878906 = 0.02694815769791603 + 50.0 * 6.21821403503418
Epoch 2280, val loss: 1.6303902864456177
Epoch 2290, training loss: 311.031982421875 = 0.026552200317382812 + 50.0 * 6.220108509063721
Epoch 2290, val loss: 1.634547233581543
Epoch 2300, training loss: 310.9996337890625 = 0.02615972049534321 + 50.0 * 6.2194695472717285
Epoch 2300, val loss: 1.6381670236587524
Epoch 2310, training loss: 310.9285888671875 = 0.025774171575903893 + 50.0 * 6.2180562019348145
Epoch 2310, val loss: 1.6427385807037354
Epoch 2320, training loss: 310.8952331542969 = 0.025400618091225624 + 50.0 * 6.2173967361450195
Epoch 2320, val loss: 1.6468888521194458
Epoch 2330, training loss: 310.8213195800781 = 0.02502978965640068 + 50.0 * 6.215925693511963
Epoch 2330, val loss: 1.651615858078003
Epoch 2340, training loss: 310.80902099609375 = 0.024674704298377037 + 50.0 * 6.215686798095703
Epoch 2340, val loss: 1.6553171873092651
Epoch 2350, training loss: 311.035888671875 = 0.02433551289141178 + 50.0 * 6.220231533050537
Epoch 2350, val loss: 1.6591242551803589
Epoch 2360, training loss: 310.96099853515625 = 0.023982398211956024 + 50.0 * 6.218740463256836
Epoch 2360, val loss: 1.6629120111465454
Epoch 2370, training loss: 310.85748291015625 = 0.023621197789907455 + 50.0 * 6.216677188873291
Epoch 2370, val loss: 1.666852593421936
Epoch 2380, training loss: 310.9210510253906 = 0.023296914994716644 + 50.0 * 6.217955112457275
Epoch 2380, val loss: 1.6709365844726562
Epoch 2390, training loss: 310.8946533203125 = 0.02296680398285389 + 50.0 * 6.217433452606201
Epoch 2390, val loss: 1.674818515777588
Epoch 2400, training loss: 310.87847900390625 = 0.022648155689239502 + 50.0 * 6.217116832733154
Epoch 2400, val loss: 1.6799356937408447
Epoch 2410, training loss: 310.7591247558594 = 0.0223315991461277 + 50.0 * 6.214735507965088
Epoch 2410, val loss: 1.6829028129577637
Epoch 2420, training loss: 310.80047607421875 = 0.022034823894500732 + 50.0 * 6.215569019317627
Epoch 2420, val loss: 1.686559796333313
Epoch 2430, training loss: 310.90008544921875 = 0.021739020943641663 + 50.0 * 6.217566967010498
Epoch 2430, val loss: 1.6906533241271973
Epoch 2440, training loss: 310.8123779296875 = 0.021437985822558403 + 50.0 * 6.215818881988525
Epoch 2440, val loss: 1.6945616006851196
Epoch 2450, training loss: 310.8433837890625 = 0.021144669502973557 + 50.0 * 6.216444969177246
Epoch 2450, val loss: 1.697763204574585
Epoch 2460, training loss: 310.88629150390625 = 0.020866114646196365 + 50.0 * 6.217308044433594
Epoch 2460, val loss: 1.70123291015625
Epoch 2470, training loss: 310.79266357421875 = 0.020578593015670776 + 50.0 * 6.215442180633545
Epoch 2470, val loss: 1.7050970792770386
Epoch 2480, training loss: 310.74517822265625 = 0.020298730581998825 + 50.0 * 6.2144975662231445
Epoch 2480, val loss: 1.7097704410552979
Epoch 2490, training loss: 310.8787841796875 = 0.020042752847075462 + 50.0 * 6.217174530029297
Epoch 2490, val loss: 1.7124590873718262
Epoch 2500, training loss: 310.78631591796875 = 0.019775021821260452 + 50.0 * 6.215331077575684
Epoch 2500, val loss: 1.7161200046539307
Epoch 2510, training loss: 310.7102966308594 = 0.01951172761619091 + 50.0 * 6.213815689086914
Epoch 2510, val loss: 1.7203751802444458
Epoch 2520, training loss: 310.73309326171875 = 0.019259605556726456 + 50.0 * 6.2142767906188965
Epoch 2520, val loss: 1.7246451377868652
Epoch 2530, training loss: 310.8993225097656 = 0.019018135964870453 + 50.0 * 6.217606067657471
Epoch 2530, val loss: 1.727906346321106
Epoch 2540, training loss: 310.7119140625 = 0.01877024583518505 + 50.0 * 6.213862895965576
Epoch 2540, val loss: 1.729405164718628
Epoch 2550, training loss: 310.6696472167969 = 0.018527088686823845 + 50.0 * 6.213022708892822
Epoch 2550, val loss: 1.7336350679397583
Epoch 2560, training loss: 310.7145690917969 = 0.01829761639237404 + 50.0 * 6.213924884796143
Epoch 2560, val loss: 1.7377052307128906
Epoch 2570, training loss: 310.7959899902344 = 0.018071629106998444 + 50.0 * 6.2155585289001465
Epoch 2570, val loss: 1.7405269145965576
Epoch 2580, training loss: 310.82818603515625 = 0.017841270193457603 + 50.0 * 6.2162065505981445
Epoch 2580, val loss: 1.7443357706069946
Epoch 2590, training loss: 310.74835205078125 = 0.01761380396783352 + 50.0 * 6.2146148681640625
Epoch 2590, val loss: 1.7462000846862793
Epoch 2600, training loss: 310.6220397949219 = 0.01738663762807846 + 50.0 * 6.212092876434326
Epoch 2600, val loss: 1.750954031944275
Epoch 2610, training loss: 310.56842041015625 = 0.01717567816376686 + 50.0 * 6.211024761199951
Epoch 2610, val loss: 1.7544516324996948
Epoch 2620, training loss: 310.6222229003906 = 0.016971593722701073 + 50.0 * 6.2121052742004395
Epoch 2620, val loss: 1.757966160774231
Epoch 2630, training loss: 310.8341979980469 = 0.016769494861364365 + 50.0 * 6.216348171234131
Epoch 2630, val loss: 1.7617030143737793
Epoch 2640, training loss: 310.6805419921875 = 0.016564790159463882 + 50.0 * 6.2132792472839355
Epoch 2640, val loss: 1.7635704278945923
Epoch 2650, training loss: 310.6445617675781 = 0.01636083796620369 + 50.0 * 6.212563991546631
Epoch 2650, val loss: 1.767276644706726
Epoch 2660, training loss: 310.6170654296875 = 0.01616758108139038 + 50.0 * 6.212018013000488
Epoch 2660, val loss: 1.7694259881973267
Epoch 2670, training loss: 310.7269592285156 = 0.015981953591108322 + 50.0 * 6.214219570159912
Epoch 2670, val loss: 1.7731833457946777
Epoch 2680, training loss: 310.6202697753906 = 0.015784617513418198 + 50.0 * 6.212090015411377
Epoch 2680, val loss: 1.7762236595153809
Epoch 2690, training loss: 310.5523681640625 = 0.015602698549628258 + 50.0 * 6.210735321044922
Epoch 2690, val loss: 1.7796143293380737
Epoch 2700, training loss: 310.5601806640625 = 0.015422248281538486 + 50.0 * 6.210895538330078
Epoch 2700, val loss: 1.7826108932495117
Epoch 2710, training loss: 310.7290344238281 = 0.015250262804329395 + 50.0 * 6.214275360107422
Epoch 2710, val loss: 1.7857378721237183
Epoch 2720, training loss: 310.6213684082031 = 0.015068350359797478 + 50.0 * 6.212125778198242
Epoch 2720, val loss: 1.787984848022461
Epoch 2730, training loss: 310.5185546875 = 0.014892909675836563 + 50.0 * 6.210073471069336
Epoch 2730, val loss: 1.7914015054702759
Epoch 2740, training loss: 310.5655517578125 = 0.014724238775670528 + 50.0 * 6.211016654968262
Epoch 2740, val loss: 1.7952648401260376
Epoch 2750, training loss: 310.6650085449219 = 0.014564264565706253 + 50.0 * 6.213008880615234
Epoch 2750, val loss: 1.7980612516403198
Epoch 2760, training loss: 310.5355529785156 = 0.014395184814929962 + 50.0 * 6.210422992706299
Epoch 2760, val loss: 1.799411654472351
Epoch 2770, training loss: 310.4622802734375 = 0.014238281175494194 + 50.0 * 6.20896053314209
Epoch 2770, val loss: 1.8026750087738037
Epoch 2780, training loss: 310.71258544921875 = 0.014093826524913311 + 50.0 * 6.213969707489014
Epoch 2780, val loss: 1.8053228855133057
Epoch 2790, training loss: 310.74859619140625 = 0.013926382176578045 + 50.0 * 6.214693546295166
Epoch 2790, val loss: 1.8084337711334229
Epoch 2800, training loss: 310.5440673828125 = 0.013768843375146389 + 50.0 * 6.210606098175049
Epoch 2800, val loss: 1.8107291460037231
Epoch 2810, training loss: 310.46380615234375 = 0.013616499491035938 + 50.0 * 6.209003925323486
Epoch 2810, val loss: 1.8136996030807495
Epoch 2820, training loss: 310.4360046386719 = 0.013477365486323833 + 50.0 * 6.2084503173828125
Epoch 2820, val loss: 1.8167791366577148
Epoch 2830, training loss: 310.6025390625 = 0.01334222313016653 + 50.0 * 6.2117838859558105
Epoch 2830, val loss: 1.8188254833221436
Epoch 2840, training loss: 310.61956787109375 = 0.013199223205447197 + 50.0 * 6.212127208709717
Epoch 2840, val loss: 1.8197691440582275
Epoch 2850, training loss: 310.45855712890625 = 0.013042982667684555 + 50.0 * 6.2089104652404785
Epoch 2850, val loss: 1.8248248100280762
Epoch 2860, training loss: 310.4606018066406 = 0.012905442155897617 + 50.0 * 6.208953857421875
Epoch 2860, val loss: 1.8266398906707764
Epoch 2870, training loss: 310.4896240234375 = 0.012776434421539307 + 50.0 * 6.209536552429199
Epoch 2870, val loss: 1.829741358757019
Epoch 2880, training loss: 310.5077819824219 = 0.012644810602068901 + 50.0 * 6.209903240203857
Epoch 2880, val loss: 1.8322821855545044
Epoch 2890, training loss: 310.43475341796875 = 0.012513292953372002 + 50.0 * 6.208444595336914
Epoch 2890, val loss: 1.8350554704666138
Epoch 2900, training loss: 310.6432189941406 = 0.012389781884849072 + 50.0 * 6.212616443634033
Epoch 2900, val loss: 1.8379169702529907
Epoch 2910, training loss: 310.4455261230469 = 0.01225909125059843 + 50.0 * 6.208664894104004
Epoch 2910, val loss: 1.8392279148101807
Epoch 2920, training loss: 310.36212158203125 = 0.012134872376918793 + 50.0 * 6.2069993019104
Epoch 2920, val loss: 1.8417646884918213
Epoch 2930, training loss: 310.4416809082031 = 0.012019253335893154 + 50.0 * 6.208593368530273
Epoch 2930, val loss: 1.8443243503570557
Epoch 2940, training loss: 310.610595703125 = 0.01190304197371006 + 50.0 * 6.211973667144775
Epoch 2940, val loss: 1.8464282751083374
Epoch 2950, training loss: 310.5556335449219 = 0.011777828447520733 + 50.0 * 6.210876941680908
Epoch 2950, val loss: 1.8504079580307007
Epoch 2960, training loss: 310.4067077636719 = 0.01165713556110859 + 50.0 * 6.2079010009765625
Epoch 2960, val loss: 1.8505115509033203
Epoch 2970, training loss: 310.59375 = 0.011547534726560116 + 50.0 * 6.211644172668457
Epoch 2970, val loss: 1.8533620834350586
Epoch 2980, training loss: 310.35321044921875 = 0.011425856500864029 + 50.0 * 6.206836223602295
Epoch 2980, val loss: 1.8559033870697021
Epoch 2990, training loss: 310.34271240234375 = 0.011316283605992794 + 50.0 * 6.20662784576416
Epoch 2990, val loss: 1.8592748641967773
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.674074074074074
0.8070637849235636
=== training gcn model ===
Epoch 0, training loss: 431.7994384765625 = 1.957656979560852 + 50.0 * 8.596835136413574
Epoch 0, val loss: 1.9611766338348389
Epoch 10, training loss: 431.751708984375 = 1.9482568502426147 + 50.0 * 8.5960693359375
Epoch 10, val loss: 1.951674222946167
Epoch 20, training loss: 431.48016357421875 = 1.9361637830734253 + 50.0 * 8.590880393981934
Epoch 20, val loss: 1.9393154382705688
Epoch 30, training loss: 429.8183898925781 = 1.9198081493377686 + 50.0 * 8.557971954345703
Epoch 30, val loss: 1.922480583190918
Epoch 40, training loss: 420.83026123046875 = 1.9005801677703857 + 50.0 * 8.378593444824219
Epoch 40, val loss: 1.9034584760665894
Epoch 50, training loss: 381.646484375 = 1.8797991275787354 + 50.0 * 7.595333576202393
Epoch 50, val loss: 1.8824505805969238
Epoch 60, training loss: 369.78619384765625 = 1.8634964227676392 + 50.0 * 7.358453750610352
Epoch 60, val loss: 1.867150068283081
Epoch 70, training loss: 362.62908935546875 = 1.8500899076461792 + 50.0 * 7.215579986572266
Epoch 70, val loss: 1.8538655042648315
Epoch 80, training loss: 358.2530212402344 = 1.8364992141723633 + 50.0 * 7.128330707550049
Epoch 80, val loss: 1.8406670093536377
Epoch 90, training loss: 353.84136962890625 = 1.8246322870254517 + 50.0 * 7.040334701538086
Epoch 90, val loss: 1.8297752141952515
Epoch 100, training loss: 349.2635192871094 = 1.8144854307174683 + 50.0 * 6.948980808258057
Epoch 100, val loss: 1.8203381299972534
Epoch 110, training loss: 345.7874450683594 = 1.8059353828430176 + 50.0 * 6.879630088806152
Epoch 110, val loss: 1.8122044801712036
Epoch 120, training loss: 342.8104553222656 = 1.79752516746521 + 50.0 * 6.820259094238281
Epoch 120, val loss: 1.8042795658111572
Epoch 130, training loss: 339.3490295410156 = 1.7894880771636963 + 50.0 * 6.751190662384033
Epoch 130, val loss: 1.7964990139007568
Epoch 140, training loss: 336.5671691894531 = 1.7818570137023926 + 50.0 * 6.695706367492676
Epoch 140, val loss: 1.789308786392212
Epoch 150, training loss: 334.396728515625 = 1.7739514112472534 + 50.0 * 6.6524553298950195
Epoch 150, val loss: 1.7817243337631226
Epoch 160, training loss: 332.697509765625 = 1.7654448747634888 + 50.0 * 6.618641376495361
Epoch 160, val loss: 1.7737711668014526
Epoch 170, training loss: 331.19793701171875 = 1.7561204433441162 + 50.0 * 6.588836193084717
Epoch 170, val loss: 1.7651116847991943
Epoch 180, training loss: 329.91461181640625 = 1.7459601163864136 + 50.0 * 6.563372611999512
Epoch 180, val loss: 1.7559504508972168
Epoch 190, training loss: 328.7768859863281 = 1.7349896430969238 + 50.0 * 6.540838241577148
Epoch 190, val loss: 1.7461649179458618
Epoch 200, training loss: 328.1811828613281 = 1.72323739528656 + 50.0 * 6.529159069061279
Epoch 200, val loss: 1.7356865406036377
Epoch 210, training loss: 326.9610290527344 = 1.7101203203201294 + 50.0 * 6.50501823425293
Epoch 210, val loss: 1.7241935729980469
Epoch 220, training loss: 326.1957092285156 = 1.6960151195526123 + 50.0 * 6.489994049072266
Epoch 220, val loss: 1.7117350101470947
Epoch 230, training loss: 325.4934997558594 = 1.6808440685272217 + 50.0 * 6.476253032684326
Epoch 230, val loss: 1.6984182596206665
Epoch 240, training loss: 325.125 = 1.6644381284713745 + 50.0 * 6.469211578369141
Epoch 240, val loss: 1.684125542640686
Epoch 250, training loss: 324.3673400878906 = 1.6468431949615479 + 50.0 * 6.454410076141357
Epoch 250, val loss: 1.6688752174377441
Epoch 260, training loss: 323.7644348144531 = 1.6281819343566895 + 50.0 * 6.44272518157959
Epoch 260, val loss: 1.6528865098953247
Epoch 270, training loss: 323.2552490234375 = 1.6084692478179932 + 50.0 * 6.43293571472168
Epoch 270, val loss: 1.6362249851226807
Epoch 280, training loss: 323.1390686035156 = 1.5879266262054443 + 50.0 * 6.431023120880127
Epoch 280, val loss: 1.618916392326355
Epoch 290, training loss: 322.42236328125 = 1.566255807876587 + 50.0 * 6.4171223640441895
Epoch 290, val loss: 1.6012953519821167
Epoch 300, training loss: 322.0765380859375 = 1.5440136194229126 + 50.0 * 6.410650730133057
Epoch 300, val loss: 1.5834057331085205
Epoch 310, training loss: 321.6551818847656 = 1.5213985443115234 + 50.0 * 6.402675628662109
Epoch 310, val loss: 1.565527319908142
Epoch 320, training loss: 321.4228210449219 = 1.4985233545303345 + 50.0 * 6.3984856605529785
Epoch 320, val loss: 1.54779851436615
Epoch 330, training loss: 321.0550537109375 = 1.4753978252410889 + 50.0 * 6.391592979431152
Epoch 330, val loss: 1.5300811529159546
Epoch 340, training loss: 320.9076232910156 = 1.4521936178207397 + 50.0 * 6.389108657836914
Epoch 340, val loss: 1.5128278732299805
Epoch 350, training loss: 320.5563659667969 = 1.4291924238204956 + 50.0 * 6.382543563842773
Epoch 350, val loss: 1.4960238933563232
Epoch 360, training loss: 320.21923828125 = 1.4065461158752441 + 50.0 * 6.376253604888916
Epoch 360, val loss: 1.4799031019210815
Epoch 370, training loss: 319.9588623046875 = 1.384276270866394 + 50.0 * 6.3714919090271
Epoch 370, val loss: 1.4642997980117798
Epoch 380, training loss: 319.72406005859375 = 1.3622372150421143 + 50.0 * 6.367236614227295
Epoch 380, val loss: 1.4491503238677979
Epoch 390, training loss: 319.8013916015625 = 1.3404697179794312 + 50.0 * 6.369217872619629
Epoch 390, val loss: 1.4347374439239502
Epoch 400, training loss: 319.3269958496094 = 1.318808674812317 + 50.0 * 6.360163688659668
Epoch 400, val loss: 1.4203218221664429
Epoch 410, training loss: 319.1536865234375 = 1.297465443611145 + 50.0 * 6.3571248054504395
Epoch 410, val loss: 1.4063001871109009
Epoch 420, training loss: 318.9424133300781 = 1.2765268087387085 + 50.0 * 6.353317737579346
Epoch 420, val loss: 1.3928333520889282
Epoch 430, training loss: 318.7444763183594 = 1.2556605339050293 + 50.0 * 6.349776744842529
Epoch 430, val loss: 1.3795596361160278
Epoch 440, training loss: 318.5584411621094 = 1.2349361181259155 + 50.0 * 6.346470355987549
Epoch 440, val loss: 1.3666083812713623
Epoch 450, training loss: 318.3908386230469 = 1.214499592781067 + 50.0 * 6.343526363372803
Epoch 450, val loss: 1.3540369272232056
Epoch 460, training loss: 318.2167053222656 = 1.1943174600601196 + 50.0 * 6.340447902679443
Epoch 460, val loss: 1.341759443283081
Epoch 470, training loss: 318.0589599609375 = 1.1743205785751343 + 50.0 * 6.337692737579346
Epoch 470, val loss: 1.3297951221466064
Epoch 480, training loss: 318.20263671875 = 1.154473066329956 + 50.0 * 6.340962886810303
Epoch 480, val loss: 1.3181190490722656
Epoch 490, training loss: 317.974853515625 = 1.1344822645187378 + 50.0 * 6.3368072509765625
Epoch 490, val loss: 1.3065681457519531
Epoch 500, training loss: 317.6681213378906 = 1.1147395372390747 + 50.0 * 6.3310675621032715
Epoch 500, val loss: 1.2951085567474365
Epoch 510, training loss: 317.5006103515625 = 1.0953936576843262 + 50.0 * 6.328104496002197
Epoch 510, val loss: 1.284220576286316
Epoch 520, training loss: 317.4388122558594 = 1.076266884803772 + 50.0 * 6.327250957489014
Epoch 520, val loss: 1.273751139640808
Epoch 530, training loss: 317.34295654296875 = 1.0573067665100098 + 50.0 * 6.325713157653809
Epoch 530, val loss: 1.2638580799102783
Epoch 540, training loss: 317.1417541503906 = 1.0385591983795166 + 50.0 * 6.322063446044922
Epoch 540, val loss: 1.2538292407989502
Epoch 550, training loss: 316.9915466308594 = 1.0201679468154907 + 50.0 * 6.319427490234375
Epoch 550, val loss: 1.2448610067367554
Epoch 560, training loss: 317.06011962890625 = 1.0020381212234497 + 50.0 * 6.321161270141602
Epoch 560, val loss: 1.2357721328735352
Epoch 570, training loss: 316.8165588378906 = 0.9842050075531006 + 50.0 * 6.316647529602051
Epoch 570, val loss: 1.2275183200836182
Epoch 580, training loss: 316.88836669921875 = 0.9667463302612305 + 50.0 * 6.318432807922363
Epoch 580, val loss: 1.2196805477142334
Epoch 590, training loss: 316.6345520019531 = 0.9493088722229004 + 50.0 * 6.313704967498779
Epoch 590, val loss: 1.2120085954666138
Epoch 600, training loss: 316.439208984375 = 0.9325450658798218 + 50.0 * 6.31013298034668
Epoch 600, val loss: 1.2052433490753174
Epoch 610, training loss: 316.3562927246094 = 0.916009247303009 + 50.0 * 6.308805465698242
Epoch 610, val loss: 1.198622703552246
Epoch 620, training loss: 316.2091979980469 = 0.8997756838798523 + 50.0 * 6.306188106536865
Epoch 620, val loss: 1.1931219100952148
Epoch 630, training loss: 316.2781982421875 = 0.8838150501251221 + 50.0 * 6.307887554168701
Epoch 630, val loss: 1.1870601177215576
Epoch 640, training loss: 316.0353698730469 = 0.8681625723838806 + 50.0 * 6.303344249725342
Epoch 640, val loss: 1.1821496486663818
Epoch 650, training loss: 316.0192565917969 = 0.8529364466667175 + 50.0 * 6.303326606750488
Epoch 650, val loss: 1.177430272102356
Epoch 660, training loss: 315.89410400390625 = 0.8379994034767151 + 50.0 * 6.301121711730957
Epoch 660, val loss: 1.173546314239502
Epoch 670, training loss: 315.7963562011719 = 0.8234169483184814 + 50.0 * 6.2994585037231445
Epoch 670, val loss: 1.1695449352264404
Epoch 680, training loss: 315.7383728027344 = 0.8090919256210327 + 50.0 * 6.298585891723633
Epoch 680, val loss: 1.1661665439605713
Epoch 690, training loss: 315.7027893066406 = 0.7949392795562744 + 50.0 * 6.29815673828125
Epoch 690, val loss: 1.162657618522644
Epoch 700, training loss: 315.515869140625 = 0.7810418009757996 + 50.0 * 6.294696807861328
Epoch 700, val loss: 1.1599247455596924
Epoch 710, training loss: 315.5279541015625 = 0.7674272656440735 + 50.0 * 6.295210361480713
Epoch 710, val loss: 1.157004475593567
Epoch 720, training loss: 315.49072265625 = 0.7540801763534546 + 50.0 * 6.294732570648193
Epoch 720, val loss: 1.1549782752990723
Epoch 730, training loss: 315.31298828125 = 0.7408678531646729 + 50.0 * 6.291442394256592
Epoch 730, val loss: 1.152292013168335
Epoch 740, training loss: 315.22772216796875 = 0.7279436588287354 + 50.0 * 6.2899956703186035
Epoch 740, val loss: 1.1505945920944214
Epoch 750, training loss: 315.12384033203125 = 0.7152949571609497 + 50.0 * 6.28817081451416
Epoch 750, val loss: 1.1488360166549683
Epoch 760, training loss: 315.063232421875 = 0.7028918862342834 + 50.0 * 6.287207126617432
Epoch 760, val loss: 1.1472985744476318
Epoch 770, training loss: 315.2289733886719 = 0.6905558109283447 + 50.0 * 6.290768146514893
Epoch 770, val loss: 1.1457819938659668
Epoch 780, training loss: 314.9752502441406 = 0.6783190369606018 + 50.0 * 6.285938739776611
Epoch 780, val loss: 1.1447197198867798
Epoch 790, training loss: 314.8751220703125 = 0.666361391544342 + 50.0 * 6.284175395965576
Epoch 790, val loss: 1.1438621282577515
Epoch 800, training loss: 314.7965087890625 = 0.6546198725700378 + 50.0 * 6.282837390899658
Epoch 800, val loss: 1.1431186199188232
Epoch 810, training loss: 315.02960205078125 = 0.6429669857025146 + 50.0 * 6.2877326011657715
Epoch 810, val loss: 1.1422185897827148
Epoch 820, training loss: 314.7802734375 = 0.6314408183097839 + 50.0 * 6.2829766273498535
Epoch 820, val loss: 1.141555666923523
Epoch 830, training loss: 314.5990295410156 = 0.6201305389404297 + 50.0 * 6.27957820892334
Epoch 830, val loss: 1.1411058902740479
Epoch 840, training loss: 314.5675354003906 = 0.6090766787528992 + 50.0 * 6.279169082641602
Epoch 840, val loss: 1.1408590078353882
Epoch 850, training loss: 314.6186218261719 = 0.598151445388794 + 50.0 * 6.280409812927246
Epoch 850, val loss: 1.1409233808517456
Epoch 860, training loss: 314.6328125 = 0.5871825218200684 + 50.0 * 6.280912399291992
Epoch 860, val loss: 1.1404846906661987
Epoch 870, training loss: 314.4293212890625 = 0.5763669013977051 + 50.0 * 6.277059555053711
Epoch 870, val loss: 1.1402721405029297
Epoch 880, training loss: 314.3262023925781 = 0.5657762885093689 + 50.0 * 6.275208950042725
Epoch 880, val loss: 1.140392780303955
Epoch 890, training loss: 314.2442321777344 = 0.5553769469261169 + 50.0 * 6.273777008056641
Epoch 890, val loss: 1.1407787799835205
Epoch 900, training loss: 314.6878662109375 = 0.545109748840332 + 50.0 * 6.28285551071167
Epoch 900, val loss: 1.141636848449707
Epoch 910, training loss: 314.1524963378906 = 0.5346248149871826 + 50.0 * 6.272356986999512
Epoch 910, val loss: 1.1411776542663574
Epoch 920, training loss: 314.174072265625 = 0.5244622230529785 + 50.0 * 6.27299165725708
Epoch 920, val loss: 1.1411885023117065
Epoch 930, training loss: 314.1584777832031 = 0.5144312381744385 + 50.0 * 6.272880554199219
Epoch 930, val loss: 1.1418253183364868
Epoch 940, training loss: 314.0001525878906 = 0.5045813322067261 + 50.0 * 6.269911766052246
Epoch 940, val loss: 1.1427451372146606
Epoch 950, training loss: 313.9421691894531 = 0.4947972297668457 + 50.0 * 6.268947124481201
Epoch 950, val loss: 1.1434522867202759
Epoch 960, training loss: 313.9004821777344 = 0.4852181077003479 + 50.0 * 6.268304824829102
Epoch 960, val loss: 1.1443551778793335
Epoch 970, training loss: 313.96966552734375 = 0.4756324887275696 + 50.0 * 6.269880294799805
Epoch 970, val loss: 1.1449509859085083
Epoch 980, training loss: 313.89837646484375 = 0.4661348760128021 + 50.0 * 6.2686448097229
Epoch 980, val loss: 1.1465216875076294
Epoch 990, training loss: 313.7582092285156 = 0.45665496587753296 + 50.0 * 6.266030788421631
Epoch 990, val loss: 1.1471245288848877
Epoch 1000, training loss: 313.703125 = 0.44738325476646423 + 50.0 * 6.265114784240723
Epoch 1000, val loss: 1.1482350826263428
Epoch 1010, training loss: 313.6159362792969 = 0.43832892179489136 + 50.0 * 6.263552188873291
Epoch 1010, val loss: 1.1495180130004883
Epoch 1020, training loss: 313.58135986328125 = 0.42943140864372253 + 50.0 * 6.263038635253906
Epoch 1020, val loss: 1.1511098146438599
Epoch 1030, training loss: 314.22308349609375 = 0.4206816554069519 + 50.0 * 6.276047706604004
Epoch 1030, val loss: 1.1525541543960571
Epoch 1040, training loss: 313.5470886230469 = 0.4116191565990448 + 50.0 * 6.262709617614746
Epoch 1040, val loss: 1.1537333726882935
Epoch 1050, training loss: 313.48529052734375 = 0.4029836058616638 + 50.0 * 6.261646270751953
Epoch 1050, val loss: 1.1554429531097412
Epoch 1060, training loss: 313.4105224609375 = 0.3945751190185547 + 50.0 * 6.260319232940674
Epoch 1060, val loss: 1.1573245525360107
Epoch 1070, training loss: 313.60369873046875 = 0.3862643837928772 + 50.0 * 6.264348983764648
Epoch 1070, val loss: 1.1586931943893433
Epoch 1080, training loss: 313.4737243652344 = 0.3779432773590088 + 50.0 * 6.261915683746338
Epoch 1080, val loss: 1.1619244813919067
Epoch 1090, training loss: 313.3090515136719 = 0.36980876326560974 + 50.0 * 6.258784770965576
Epoch 1090, val loss: 1.163428544998169
Epoch 1100, training loss: 313.25189208984375 = 0.3619030714035034 + 50.0 * 6.2577996253967285
Epoch 1100, val loss: 1.1658903360366821
Epoch 1110, training loss: 313.703125 = 0.3541456460952759 + 50.0 * 6.266979217529297
Epoch 1110, val loss: 1.1688839197158813
Epoch 1120, training loss: 313.31256103515625 = 0.3463214933872223 + 50.0 * 6.25932502746582
Epoch 1120, val loss: 1.17050039768219
Epoch 1130, training loss: 313.14105224609375 = 0.3387773334980011 + 50.0 * 6.256045341491699
Epoch 1130, val loss: 1.1732416152954102
Epoch 1140, training loss: 313.0863952636719 = 0.3314078152179718 + 50.0 * 6.255099773406982
Epoch 1140, val loss: 1.1761412620544434
Epoch 1150, training loss: 313.50482177734375 = 0.32416361570358276 + 50.0 * 6.263613224029541
Epoch 1150, val loss: 1.179036021232605
Epoch 1160, training loss: 313.2250671386719 = 0.31697118282318115 + 50.0 * 6.258161544799805
Epoch 1160, val loss: 1.181366205215454
Epoch 1170, training loss: 312.9900817871094 = 0.30993232131004333 + 50.0 * 6.253602981567383
Epoch 1170, val loss: 1.1846562623977661
Epoch 1180, training loss: 313.0015563964844 = 0.3031269609928131 + 50.0 * 6.253968715667725
Epoch 1180, val loss: 1.1880635023117065
Epoch 1190, training loss: 313.0411682128906 = 0.2964457869529724 + 50.0 * 6.254894256591797
Epoch 1190, val loss: 1.1912215948104858
Epoch 1200, training loss: 312.9682312011719 = 0.2898699939250946 + 50.0 * 6.253567218780518
Epoch 1200, val loss: 1.194912314414978
Epoch 1210, training loss: 312.9973449707031 = 0.28339558839797974 + 50.0 * 6.254279136657715
Epoch 1210, val loss: 1.198273777961731
Epoch 1220, training loss: 312.8546447753906 = 0.27710413932800293 + 50.0 * 6.251551151275635
Epoch 1220, val loss: 1.2024035453796387
Epoch 1230, training loss: 312.7795104980469 = 0.2709465026855469 + 50.0 * 6.250171661376953
Epoch 1230, val loss: 1.2061604261398315
Epoch 1240, training loss: 312.76953125 = 0.26496586203575134 + 50.0 * 6.250091552734375
Epoch 1240, val loss: 1.2104707956314087
Epoch 1250, training loss: 313.183837890625 = 0.2591629922389984 + 50.0 * 6.258493900299072
Epoch 1250, val loss: 1.2149333953857422
Epoch 1260, training loss: 312.8851013183594 = 0.253253310918808 + 50.0 * 6.252636909484863
Epoch 1260, val loss: 1.2183135747909546
Epoch 1270, training loss: 312.69769287109375 = 0.24756209552288055 + 50.0 * 6.249002456665039
Epoch 1270, val loss: 1.223252296447754
Epoch 1280, training loss: 312.7041015625 = 0.24210169911384583 + 50.0 * 6.249239921569824
Epoch 1280, val loss: 1.2279229164123535
Epoch 1290, training loss: 312.81103515625 = 0.23675604164600372 + 50.0 * 6.251485824584961
Epoch 1290, val loss: 1.2327154874801636
Epoch 1300, training loss: 312.75018310546875 = 0.2314549833536148 + 50.0 * 6.250374794006348
Epoch 1300, val loss: 1.2371751070022583
Epoch 1310, training loss: 312.600830078125 = 0.22629348933696747 + 50.0 * 6.247490882873535
Epoch 1310, val loss: 1.2425110340118408
Epoch 1320, training loss: 312.5024719238281 = 0.22129356861114502 + 50.0 * 6.245623588562012
Epoch 1320, val loss: 1.2475109100341797
Epoch 1330, training loss: 312.5106506347656 = 0.21643789112567902 + 50.0 * 6.245884418487549
Epoch 1330, val loss: 1.253010630607605
Epoch 1340, training loss: 312.9175109863281 = 0.21166133880615234 + 50.0 * 6.254116535186768
Epoch 1340, val loss: 1.2586438655853271
Epoch 1350, training loss: 312.7373962402344 = 0.2068956196308136 + 50.0 * 6.250609874725342
Epoch 1350, val loss: 1.2634069919586182
Epoch 1360, training loss: 312.4200439453125 = 0.20226334035396576 + 50.0 * 6.244355201721191
Epoch 1360, val loss: 1.2690129280090332
Epoch 1370, training loss: 312.3980407714844 = 0.19781675934791565 + 50.0 * 6.244004249572754
Epoch 1370, val loss: 1.275171160697937
Epoch 1380, training loss: 312.5375061035156 = 0.19350898265838623 + 50.0 * 6.246879577636719
Epoch 1380, val loss: 1.2808334827423096
Epoch 1390, training loss: 312.32720947265625 = 0.18921516835689545 + 50.0 * 6.242760181427002
Epoch 1390, val loss: 1.2873350381851196
Epoch 1400, training loss: 312.2955017089844 = 0.1850539594888687 + 50.0 * 6.242208957672119
Epoch 1400, val loss: 1.2933828830718994
Epoch 1410, training loss: 312.30059814453125 = 0.18102973699569702 + 50.0 * 6.242391586303711
Epoch 1410, val loss: 1.2996230125427246
Epoch 1420, training loss: 312.686279296875 = 0.17712698876857758 + 50.0 * 6.25018310546875
Epoch 1420, val loss: 1.3061714172363281
Epoch 1430, training loss: 312.38519287109375 = 0.17310930788516998 + 50.0 * 6.244242191314697
Epoch 1430, val loss: 1.3119146823883057
Epoch 1440, training loss: 312.34088134765625 = 0.16933147609233856 + 50.0 * 6.2434306144714355
Epoch 1440, val loss: 1.3184617757797241
Epoch 1450, training loss: 312.32171630859375 = 0.16560716927051544 + 50.0 * 6.243122100830078
Epoch 1450, val loss: 1.3248974084854126
Epoch 1460, training loss: 312.2309875488281 = 0.16198138892650604 + 50.0 * 6.241379737854004
Epoch 1460, val loss: 1.332321047782898
Epoch 1470, training loss: 312.1875305175781 = 0.15842929482460022 + 50.0 * 6.24058198928833
Epoch 1470, val loss: 1.3392057418823242
Epoch 1480, training loss: 312.46246337890625 = 0.15496604144573212 + 50.0 * 6.246150016784668
Epoch 1480, val loss: 1.3456403017044067
Epoch 1490, training loss: 312.2000732421875 = 0.1515381634235382 + 50.0 * 6.240970611572266
Epoch 1490, val loss: 1.3522796630859375
Epoch 1500, training loss: 312.1122741699219 = 0.14820951223373413 + 50.0 * 6.239280700683594
Epoch 1500, val loss: 1.3592147827148438
Epoch 1510, training loss: 312.0647888183594 = 0.14499042928218842 + 50.0 * 6.238395690917969
Epoch 1510, val loss: 1.3663610219955444
Epoch 1520, training loss: 312.36712646484375 = 0.14187082648277283 + 50.0 * 6.244504928588867
Epoch 1520, val loss: 1.3735055923461914
Epoch 1530, training loss: 312.1809997558594 = 0.1386941522359848 + 50.0 * 6.240845680236816
Epoch 1530, val loss: 1.3795418739318848
Epoch 1540, training loss: 312.10528564453125 = 0.13561251759529114 + 50.0 * 6.23939323425293
Epoch 1540, val loss: 1.3867074251174927
Epoch 1550, training loss: 312.0527648925781 = 0.13264846801757812 + 50.0 * 6.238402366638184
Epoch 1550, val loss: 1.393655776977539
Epoch 1560, training loss: 312.0897521972656 = 0.12973445653915405 + 50.0 * 6.239200592041016
Epoch 1560, val loss: 1.4010238647460938
Epoch 1570, training loss: 312.07574462890625 = 0.1268695890903473 + 50.0 * 6.238977909088135
Epoch 1570, val loss: 1.4083256721496582
Epoch 1580, training loss: 312.03253173828125 = 0.12408747524023056 + 50.0 * 6.238168716430664
Epoch 1580, val loss: 1.4150460958480835
Epoch 1590, training loss: 311.90740966796875 = 0.12135899066925049 + 50.0 * 6.235720634460449
Epoch 1590, val loss: 1.422239065170288
Epoch 1600, training loss: 312.00543212890625 = 0.11871938407421112 + 50.0 * 6.237734317779541
Epoch 1600, val loss: 1.4293376207351685
Epoch 1610, training loss: 311.9350891113281 = 0.11610537767410278 + 50.0 * 6.236379146575928
Epoch 1610, val loss: 1.436651349067688
Epoch 1620, training loss: 311.9471435546875 = 0.11354769766330719 + 50.0 * 6.2366719245910645
Epoch 1620, val loss: 1.4446969032287598
Epoch 1630, training loss: 312.0798645019531 = 0.11103271692991257 + 50.0 * 6.239376544952393
Epoch 1630, val loss: 1.4516960382461548
Epoch 1640, training loss: 311.9533386230469 = 0.1085737943649292 + 50.0 * 6.2368950843811035
Epoch 1640, val loss: 1.4585875272750854
Epoch 1650, training loss: 311.80438232421875 = 0.10616715252399445 + 50.0 * 6.233964443206787
Epoch 1650, val loss: 1.465880274772644
Epoch 1660, training loss: 311.7769470214844 = 0.10384242981672287 + 50.0 * 6.233461856842041
Epoch 1660, val loss: 1.4734703302383423
Epoch 1670, training loss: 311.8791809082031 = 0.10159255564212799 + 50.0 * 6.235551834106445
Epoch 1670, val loss: 1.480652928352356
Epoch 1680, training loss: 311.8502197265625 = 0.09936223179101944 + 50.0 * 6.235016822814941
Epoch 1680, val loss: 1.4885649681091309
Epoch 1690, training loss: 311.7593688964844 = 0.09716794639825821 + 50.0 * 6.233243942260742
Epoch 1690, val loss: 1.4956305027008057
Epoch 1700, training loss: 311.7286376953125 = 0.0950646921992302 + 50.0 * 6.23267126083374
Epoch 1700, val loss: 1.5032235383987427
Epoch 1710, training loss: 311.997314453125 = 0.09302718937397003 + 50.0 * 6.238086223602295
Epoch 1710, val loss: 1.510603427886963
Epoch 1720, training loss: 311.7476501464844 = 0.09099327772855759 + 50.0 * 6.233132839202881
Epoch 1720, val loss: 1.518432378768921
Epoch 1730, training loss: 311.75262451171875 = 0.08903737366199493 + 50.0 * 6.233271598815918
Epoch 1730, val loss: 1.5255199670791626
Epoch 1740, training loss: 311.8266296386719 = 0.08713117241859436 + 50.0 * 6.234790325164795
Epoch 1740, val loss: 1.53315269947052
Epoch 1750, training loss: 311.7264099121094 = 0.0852370336651802 + 50.0 * 6.232823371887207
Epoch 1750, val loss: 1.5411547422409058
Epoch 1760, training loss: 311.6388244628906 = 0.08342030644416809 + 50.0 * 6.231107711791992
Epoch 1760, val loss: 1.547893762588501
Epoch 1770, training loss: 311.6494445800781 = 0.08166275173425674 + 50.0 * 6.231355667114258
Epoch 1770, val loss: 1.5554357767105103
Epoch 1780, training loss: 311.6915283203125 = 0.07995089888572693 + 50.0 * 6.232231140136719
Epoch 1780, val loss: 1.5632190704345703
Epoch 1790, training loss: 311.7854919433594 = 0.0782695785164833 + 50.0 * 6.23414421081543
Epoch 1790, val loss: 1.5706803798675537
Epoch 1800, training loss: 311.7076110839844 = 0.07657632231712341 + 50.0 * 6.232620716094971
Epoch 1800, val loss: 1.5772072076797485
Epoch 1810, training loss: 311.6329345703125 = 0.07496320456266403 + 50.0 * 6.231159687042236
Epoch 1810, val loss: 1.5850367546081543
Epoch 1820, training loss: 311.5482482910156 = 0.07340971380472183 + 50.0 * 6.229496955871582
Epoch 1820, val loss: 1.5918136835098267
Epoch 1830, training loss: 311.5497131347656 = 0.07191736996173859 + 50.0 * 6.229555606842041
Epoch 1830, val loss: 1.599636197090149
Epoch 1840, training loss: 311.6166076660156 = 0.07044639438390732 + 50.0 * 6.230923175811768
Epoch 1840, val loss: 1.6065880060195923
Epoch 1850, training loss: 311.5357971191406 = 0.06899946182966232 + 50.0 * 6.229335784912109
Epoch 1850, val loss: 1.6136404275894165
Epoch 1860, training loss: 311.5311279296875 = 0.06759045273065567 + 50.0 * 6.2292704582214355
Epoch 1860, val loss: 1.6208767890930176
Epoch 1870, training loss: 311.7355651855469 = 0.06623227149248123 + 50.0 * 6.233386993408203
Epoch 1870, val loss: 1.6279234886169434
Epoch 1880, training loss: 311.6224060058594 = 0.06485598534345627 + 50.0 * 6.231151103973389
Epoch 1880, val loss: 1.63434898853302
Epoch 1890, training loss: 311.4308166503906 = 0.06349240243434906 + 50.0 * 6.227346420288086
Epoch 1890, val loss: 1.6416633129119873
Epoch 1900, training loss: 311.4096374511719 = 0.06223125010728836 + 50.0 * 6.226948261260986
Epoch 1900, val loss: 1.648809790611267
Epoch 1910, training loss: 311.3822021484375 = 0.06102868914604187 + 50.0 * 6.226423263549805
Epoch 1910, val loss: 1.6556963920593262
Epoch 1920, training loss: 311.37506103515625 = 0.05984411761164665 + 50.0 * 6.226304054260254
Epoch 1920, val loss: 1.6625328063964844
Epoch 1930, training loss: 311.6976013183594 = 0.05869520083069801 + 50.0 * 6.232778072357178
Epoch 1930, val loss: 1.668639898300171
Epoch 1940, training loss: 311.5578918457031 = 0.05751249939203262 + 50.0 * 6.230007171630859
Epoch 1940, val loss: 1.6770473718643188
Epoch 1950, training loss: 311.46844482421875 = 0.056363314390182495 + 50.0 * 6.228241443634033
Epoch 1950, val loss: 1.6824374198913574
Epoch 1960, training loss: 311.3568420410156 = 0.05527500435709953 + 50.0 * 6.226031303405762
Epoch 1960, val loss: 1.6896305084228516
Epoch 1970, training loss: 311.2952880859375 = 0.05422799661755562 + 50.0 * 6.224821090698242
Epoch 1970, val loss: 1.6965261697769165
Epoch 1980, training loss: 311.3551025390625 = 0.05321361497044563 + 50.0 * 6.226037502288818
Epoch 1980, val loss: 1.703323245048523
Epoch 1990, training loss: 311.63018798828125 = 0.05220355838537216 + 50.0 * 6.2315592765808105
Epoch 1990, val loss: 1.7095627784729004
Epoch 2000, training loss: 311.3624267578125 = 0.05118677392601967 + 50.0 * 6.226224899291992
Epoch 2000, val loss: 1.715387225151062
Epoch 2010, training loss: 311.27410888671875 = 0.05022018402814865 + 50.0 * 6.224477767944336
Epoch 2010, val loss: 1.7224347591400146
Epoch 2020, training loss: 311.247802734375 = 0.049296412616968155 + 50.0 * 6.223970413208008
Epoch 2020, val loss: 1.728813648223877
Epoch 2030, training loss: 311.3826599121094 = 0.04840310290455818 + 50.0 * 6.226685047149658
Epoch 2030, val loss: 1.7357101440429688
Epoch 2040, training loss: 311.24737548828125 = 0.04749882593750954 + 50.0 * 6.223997592926025
Epoch 2040, val loss: 1.7413601875305176
Epoch 2050, training loss: 311.25433349609375 = 0.04662580043077469 + 50.0 * 6.224153995513916
Epoch 2050, val loss: 1.7477729320526123
Epoch 2060, training loss: 311.4170227050781 = 0.045773863792419434 + 50.0 * 6.2274250984191895
Epoch 2060, val loss: 1.75411057472229
Epoch 2070, training loss: 311.24114990234375 = 0.04493309184908867 + 50.0 * 6.22392463684082
Epoch 2070, val loss: 1.75990891456604
Epoch 2080, training loss: 311.2446594238281 = 0.04412142187356949 + 50.0 * 6.224010467529297
Epoch 2080, val loss: 1.7660622596740723
Epoch 2090, training loss: 311.2943115234375 = 0.043338604271411896 + 50.0 * 6.225019454956055
Epoch 2090, val loss: 1.7718582153320312
Epoch 2100, training loss: 311.3102722167969 = 0.04256419464945793 + 50.0 * 6.225354194641113
Epoch 2100, val loss: 1.7786742448806763
Epoch 2110, training loss: 311.205810546875 = 0.04181130602955818 + 50.0 * 6.22327995300293
Epoch 2110, val loss: 1.7839778661727905
Epoch 2120, training loss: 311.1325378417969 = 0.04107833281159401 + 50.0 * 6.221828937530518
Epoch 2120, val loss: 1.790088176727295
Epoch 2130, training loss: 311.127685546875 = 0.040382374078035355 + 50.0 * 6.221746444702148
Epoch 2130, val loss: 1.7959299087524414
Epoch 2140, training loss: 311.4497985839844 = 0.03970718011260033 + 50.0 * 6.228201866149902
Epoch 2140, val loss: 1.801497220993042
Epoch 2150, training loss: 311.27764892578125 = 0.03899247571825981 + 50.0 * 6.224772930145264
Epoch 2150, val loss: 1.8074355125427246
Epoch 2160, training loss: 311.2978210449219 = 0.03831695765256882 + 50.0 * 6.225189685821533
Epoch 2160, val loss: 1.813454270362854
Epoch 2170, training loss: 311.1868896484375 = 0.0376499779522419 + 50.0 * 6.222984313964844
Epoch 2170, val loss: 1.818966269493103
Epoch 2180, training loss: 311.1007385253906 = 0.037024181336164474 + 50.0 * 6.221274375915527
Epoch 2180, val loss: 1.8243508338928223
Epoch 2190, training loss: 311.0404052734375 = 0.03641188144683838 + 50.0 * 6.2200798988342285
Epoch 2190, val loss: 1.830433964729309
Epoch 2200, training loss: 311.13018798828125 = 0.035820551216602325 + 50.0 * 6.221887111663818
Epoch 2200, val loss: 1.8356789350509644
Epoch 2210, training loss: 311.1658020019531 = 0.03521718084812164 + 50.0 * 6.222611427307129
Epoch 2210, val loss: 1.8409732580184937
Epoch 2220, training loss: 311.0738220214844 = 0.03462626039981842 + 50.0 * 6.220783710479736
Epoch 2220, val loss: 1.846725583076477
Epoch 2230, training loss: 311.0517272949219 = 0.03407013788819313 + 50.0 * 6.220353126525879
Epoch 2230, val loss: 1.8525323867797852
Epoch 2240, training loss: 311.32989501953125 = 0.033531494438648224 + 50.0 * 6.225926876068115
Epoch 2240, val loss: 1.8582648038864136
Epoch 2250, training loss: 311.0719299316406 = 0.03296922519803047 + 50.0 * 6.2207794189453125
Epoch 2250, val loss: 1.862565279006958
Epoch 2260, training loss: 311.0068054199219 = 0.03244439885020256 + 50.0 * 6.219487190246582
Epoch 2260, val loss: 1.8684368133544922
Epoch 2270, training loss: 311.12603759765625 = 0.03193584829568863 + 50.0 * 6.221882343292236
Epoch 2270, val loss: 1.8735426664352417
Epoch 2280, training loss: 311.1684875488281 = 0.031420763581991196 + 50.0 * 6.22274112701416
Epoch 2280, val loss: 1.8785563707351685
Epoch 2290, training loss: 310.9598388671875 = 0.030917394906282425 + 50.0 * 6.218578338623047
Epoch 2290, val loss: 1.8834288120269775
Epoch 2300, training loss: 310.9309387207031 = 0.030441034585237503 + 50.0 * 6.218010425567627
Epoch 2300, val loss: 1.8889379501342773
Epoch 2310, training loss: 310.92767333984375 = 0.02998221293091774 + 50.0 * 6.217954158782959
Epoch 2310, val loss: 1.893989086151123
Epoch 2320, training loss: 311.0427551269531 = 0.029539033770561218 + 50.0 * 6.220264434814453
Epoch 2320, val loss: 1.8989156484603882
Epoch 2330, training loss: 311.06170654296875 = 0.029084237292408943 + 50.0 * 6.2206525802612305
Epoch 2330, val loss: 1.9035263061523438
Epoch 2340, training loss: 310.99908447265625 = 0.028618838638067245 + 50.0 * 6.219408988952637
Epoch 2340, val loss: 1.9082762002944946
Epoch 2350, training loss: 310.9982604980469 = 0.028188124299049377 + 50.0 * 6.2194013595581055
Epoch 2350, val loss: 1.9136816263198853
Epoch 2360, training loss: 310.921630859375 = 0.027768386527895927 + 50.0 * 6.217877388000488
Epoch 2360, val loss: 1.918626308441162
Epoch 2370, training loss: 310.87713623046875 = 0.027363641187548637 + 50.0 * 6.2169952392578125
Epoch 2370, val loss: 1.9231622219085693
Epoch 2380, training loss: 310.9482421875 = 0.026970678940415382 + 50.0 * 6.218425273895264
Epoch 2380, val loss: 1.9279038906097412
Epoch 2390, training loss: 310.9281311035156 = 0.026575013995170593 + 50.0 * 6.21803092956543
Epoch 2390, val loss: 1.932454228401184
Epoch 2400, training loss: 311.1623840332031 = 0.026187341660261154 + 50.0 * 6.222723960876465
Epoch 2400, val loss: 1.9377371072769165
Epoch 2410, training loss: 311.03057861328125 = 0.02579725719988346 + 50.0 * 6.220095634460449
Epoch 2410, val loss: 1.942362904548645
Epoch 2420, training loss: 310.93963623046875 = 0.025416895747184753 + 50.0 * 6.2182841300964355
Epoch 2420, val loss: 1.946390151977539
Epoch 2430, training loss: 310.8928527832031 = 0.025053679943084717 + 50.0 * 6.217356204986572
Epoch 2430, val loss: 1.9510159492492676
Epoch 2440, training loss: 310.91778564453125 = 0.024702847003936768 + 50.0 * 6.217862129211426
Epoch 2440, val loss: 1.9552675485610962
Epoch 2450, training loss: 311.0146179199219 = 0.024353699758648872 + 50.0 * 6.2198052406311035
Epoch 2450, val loss: 1.9598948955535889
Epoch 2460, training loss: 310.8802490234375 = 0.023996610194444656 + 50.0 * 6.217125415802002
Epoch 2460, val loss: 1.9646269083023071
Epoch 2470, training loss: 310.8212585449219 = 0.02366385981440544 + 50.0 * 6.215952396392822
Epoch 2470, val loss: 1.9687772989273071
Epoch 2480, training loss: 310.7711486816406 = 0.02334381453692913 + 50.0 * 6.214956283569336
Epoch 2480, val loss: 1.9735442399978638
Epoch 2490, training loss: 310.8265380859375 = 0.02303226850926876 + 50.0 * 6.21606969833374
Epoch 2490, val loss: 1.9773519039154053
Epoch 2500, training loss: 311.0005798339844 = 0.02271992340683937 + 50.0 * 6.219557762145996
Epoch 2500, val loss: 1.9815698862075806
Epoch 2510, training loss: 310.87353515625 = 0.022400591522455215 + 50.0 * 6.21702241897583
Epoch 2510, val loss: 1.9863715171813965
Epoch 2520, training loss: 310.8165588378906 = 0.02209915779531002 + 50.0 * 6.2158894538879395
Epoch 2520, val loss: 1.99037766456604
Epoch 2530, training loss: 310.9187316894531 = 0.02181166037917137 + 50.0 * 6.21793794631958
Epoch 2530, val loss: 1.9943872690200806
Epoch 2540, training loss: 310.8418884277344 = 0.02150784060359001 + 50.0 * 6.216407775878906
Epoch 2540, val loss: 1.9985240697860718
Epoch 2550, training loss: 310.74688720703125 = 0.021218715235590935 + 50.0 * 6.214513301849365
Epoch 2550, val loss: 2.0024914741516113
Epoch 2560, training loss: 310.7353515625 = 0.02094394527375698 + 50.0 * 6.214288234710693
Epoch 2560, val loss: 2.0069162845611572
Epoch 2570, training loss: 310.7111511230469 = 0.020677141845226288 + 50.0 * 6.213809490203857
Epoch 2570, val loss: 2.0106759071350098
Epoch 2580, training loss: 310.84149169921875 = 0.020418472588062286 + 50.0 * 6.216421604156494
Epoch 2580, val loss: 2.014164686203003
Epoch 2590, training loss: 310.7759094238281 = 0.020155109465122223 + 50.0 * 6.215115547180176
Epoch 2590, val loss: 2.0189361572265625
Epoch 2600, training loss: 310.9984436035156 = 0.01989983394742012 + 50.0 * 6.219570636749268
Epoch 2600, val loss: 2.0221757888793945
Epoch 2610, training loss: 310.74725341796875 = 0.019620155915617943 + 50.0 * 6.214552402496338
Epoch 2610, val loss: 2.026885747909546
Epoch 2620, training loss: 310.67138671875 = 0.01938009262084961 + 50.0 * 6.213040351867676
Epoch 2620, val loss: 2.030090808868408
Epoch 2630, training loss: 310.80816650390625 = 0.019147122278809547 + 50.0 * 6.215780735015869
Epoch 2630, val loss: 2.033817768096924
Epoch 2640, training loss: 310.7402038574219 = 0.018901756033301353 + 50.0 * 6.214426040649414
Epoch 2640, val loss: 2.038198709487915
Epoch 2650, training loss: 310.671875 = 0.018662258982658386 + 50.0 * 6.213064193725586
Epoch 2650, val loss: 2.041628122329712
Epoch 2660, training loss: 310.66290283203125 = 0.018443088978528976 + 50.0 * 6.212889194488525
Epoch 2660, val loss: 2.045398235321045
Epoch 2670, training loss: 310.82464599609375 = 0.01822730153799057 + 50.0 * 6.216128826141357
Epoch 2670, val loss: 2.0495972633361816
Epoch 2680, training loss: 310.6351318359375 = 0.01799495704472065 + 50.0 * 6.212342262268066
Epoch 2680, val loss: 2.052570104598999
Epoch 2690, training loss: 310.7052917480469 = 0.01777590438723564 + 50.0 * 6.213750839233398
Epoch 2690, val loss: 2.055825710296631
Epoch 2700, training loss: 310.7210998535156 = 0.017566263675689697 + 50.0 * 6.2140703201293945
Epoch 2700, val loss: 2.0593459606170654
Epoch 2710, training loss: 310.7288818359375 = 0.01736091449856758 + 50.0 * 6.214230537414551
Epoch 2710, val loss: 2.0630087852478027
Epoch 2720, training loss: 310.61029052734375 = 0.017147665843367577 + 50.0 * 6.211862564086914
Epoch 2720, val loss: 2.066650390625
Epoch 2730, training loss: 310.6825866699219 = 0.016950787976384163 + 50.0 * 6.213313102722168
Epoch 2730, val loss: 2.070456027984619
Epoch 2740, training loss: 310.6874694824219 = 0.01675570383667946 + 50.0 * 6.213414192199707
Epoch 2740, val loss: 2.0730931758880615
Epoch 2750, training loss: 310.6118469238281 = 0.01656288467347622 + 50.0 * 6.211905479431152
Epoch 2750, val loss: 2.0765092372894287
Epoch 2760, training loss: 310.6093444824219 = 0.016377806663513184 + 50.0 * 6.211859226226807
Epoch 2760, val loss: 2.080392599105835
Epoch 2770, training loss: 310.6671447753906 = 0.016197601333260536 + 50.0 * 6.213018894195557
Epoch 2770, val loss: 2.0837950706481934
Epoch 2780, training loss: 310.6695556640625 = 0.01600942201912403 + 50.0 * 6.213070869445801
Epoch 2780, val loss: 2.0869576930999756
Epoch 2790, training loss: 310.6170959472656 = 0.01582394912838936 + 50.0 * 6.212025165557861
Epoch 2790, val loss: 2.089108943939209
Epoch 2800, training loss: 310.8673400878906 = 0.01565016619861126 + 50.0 * 6.217033863067627
Epoch 2800, val loss: 2.092771053314209
Epoch 2810, training loss: 310.5594177246094 = 0.01545962505042553 + 50.0 * 6.210878849029541
Epoch 2810, val loss: 2.096113681793213
Epoch 2820, training loss: 310.5324401855469 = 0.015291492454707623 + 50.0 * 6.210342884063721
Epoch 2820, val loss: 2.099682092666626
Epoch 2830, training loss: 310.7064514160156 = 0.015134651213884354 + 50.0 * 6.2138261795043945
Epoch 2830, val loss: 2.1027817726135254
Epoch 2840, training loss: 310.5934143066406 = 0.014962397515773773 + 50.0 * 6.211569309234619
Epoch 2840, val loss: 2.105804443359375
Epoch 2850, training loss: 310.55450439453125 = 0.014792718924582005 + 50.0 * 6.210793972015381
Epoch 2850, val loss: 2.108144998550415
Epoch 2860, training loss: 310.5009460449219 = 0.014639584347605705 + 50.0 * 6.209725856781006
Epoch 2860, val loss: 2.1118533611297607
Epoch 2870, training loss: 310.48773193359375 = 0.014489986002445221 + 50.0 * 6.2094645500183105
Epoch 2870, val loss: 2.1150271892547607
Epoch 2880, training loss: 310.7027587890625 = 0.014350018464028835 + 50.0 * 6.213768482208252
Epoch 2880, val loss: 2.117518186569214
Epoch 2890, training loss: 310.5876159667969 = 0.014186112210154533 + 50.0 * 6.21146821975708
Epoch 2890, val loss: 2.1202592849731445
Epoch 2900, training loss: 310.6048278808594 = 0.014033774845302105 + 50.0 * 6.21181583404541
Epoch 2900, val loss: 2.1231613159179688
Epoch 2910, training loss: 310.53662109375 = 0.013879294507205486 + 50.0 * 6.210454940795898
Epoch 2910, val loss: 2.1268558502197266
Epoch 2920, training loss: 310.5401916503906 = 0.013738105073571205 + 50.0 * 6.210529327392578
Epoch 2920, val loss: 2.1291940212249756
Epoch 2930, training loss: 310.49139404296875 = 0.013597873970866203 + 50.0 * 6.209555625915527
Epoch 2930, val loss: 2.132328987121582
Epoch 2940, training loss: 310.5906066894531 = 0.013465967029333115 + 50.0 * 6.211543083190918
Epoch 2940, val loss: 2.135380983352661
Epoch 2950, training loss: 310.6297302246094 = 0.013321788050234318 + 50.0 * 6.21232795715332
Epoch 2950, val loss: 2.1373698711395264
Epoch 2960, training loss: 310.5157470703125 = 0.013178564608097076 + 50.0 * 6.210051536560059
Epoch 2960, val loss: 2.1400060653686523
Epoch 2970, training loss: 310.4504089355469 = 0.0130438432097435 + 50.0 * 6.208747863769531
Epoch 2970, val loss: 2.143346071243286
Epoch 2980, training loss: 310.4914855957031 = 0.012917466461658478 + 50.0 * 6.209571361541748
Epoch 2980, val loss: 2.145909309387207
Epoch 2990, training loss: 310.5292053222656 = 0.012792573310434818 + 50.0 * 6.210328578948975
Epoch 2990, val loss: 2.148538589477539
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6296296296296297
0.8065366367949395
The final CL Acc:0.66173, 0.02290, The final GNN Acc:0.80759, 0.00114
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13200])
remove edge: torch.Size([2, 8002])
updated graph: torch.Size([2, 10646])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 431.7881164550781 = 1.944559097290039 + 50.0 * 8.596871376037598
Epoch 0, val loss: 1.9455008506774902
Epoch 10, training loss: 431.7531433105469 = 1.935572624206543 + 50.0 * 8.596351623535156
Epoch 10, val loss: 1.9367492198944092
Epoch 20, training loss: 431.5647277832031 = 1.9246110916137695 + 50.0 * 8.592802047729492
Epoch 20, val loss: 1.9255751371383667
Epoch 30, training loss: 430.3085632324219 = 1.910963535308838 + 50.0 * 8.567952156066895
Epoch 30, val loss: 1.9111138582229614
Epoch 40, training loss: 422.7384948730469 = 1.893729329109192 + 50.0 * 8.416894912719727
Epoch 40, val loss: 1.8930120468139648
Epoch 50, training loss: 395.5976257324219 = 1.8757755756378174 + 50.0 * 7.87443733215332
Epoch 50, val loss: 1.8740849494934082
Epoch 60, training loss: 375.1931457519531 = 1.8605414628982544 + 50.0 * 7.4666523933410645
Epoch 60, val loss: 1.8587937355041504
Epoch 70, training loss: 357.65380859375 = 1.8479477167129517 + 50.0 * 7.116117000579834
Epoch 70, val loss: 1.8461110591888428
Epoch 80, training loss: 349.02142333984375 = 1.8369699716567993 + 50.0 * 6.943688869476318
Epoch 80, val loss: 1.8350895643234253
Epoch 90, training loss: 343.5339660644531 = 1.826231598854065 + 50.0 * 6.8341546058654785
Epoch 90, val loss: 1.8239681720733643
Epoch 100, training loss: 339.6678466796875 = 1.8158330917358398 + 50.0 * 6.757040500640869
Epoch 100, val loss: 1.8133317232131958
Epoch 110, training loss: 336.50738525390625 = 1.8055084943771362 + 50.0 * 6.694037437438965
Epoch 110, val loss: 1.8032022714614868
Epoch 120, training loss: 334.1427307128906 = 1.795663595199585 + 50.0 * 6.646941661834717
Epoch 120, val loss: 1.7934852838516235
Epoch 130, training loss: 332.17608642578125 = 1.7857584953308105 + 50.0 * 6.60780668258667
Epoch 130, val loss: 1.78365957736969
Epoch 140, training loss: 330.6514892578125 = 1.7754948139190674 + 50.0 * 6.57751989364624
Epoch 140, val loss: 1.7734277248382568
Epoch 150, training loss: 329.4850158691406 = 1.7644612789154053 + 50.0 * 6.554410934448242
Epoch 150, val loss: 1.762636423110962
Epoch 160, training loss: 328.3338317871094 = 1.752619743347168 + 50.0 * 6.5316243171691895
Epoch 160, val loss: 1.7513750791549683
Epoch 170, training loss: 327.39013671875 = 1.7400696277618408 + 50.0 * 6.513000965118408
Epoch 170, val loss: 1.7394280433654785
Epoch 180, training loss: 326.6985168457031 = 1.7265690565109253 + 50.0 * 6.499439239501953
Epoch 180, val loss: 1.7268339395523071
Epoch 190, training loss: 325.974609375 = 1.711887001991272 + 50.0 * 6.485254287719727
Epoch 190, val loss: 1.713208794593811
Epoch 200, training loss: 325.25384521484375 = 1.6961209774017334 + 50.0 * 6.471154689788818
Epoch 200, val loss: 1.698718786239624
Epoch 210, training loss: 324.65087890625 = 1.6790279150009155 + 50.0 * 6.459437370300293
Epoch 210, val loss: 1.6831203699111938
Epoch 220, training loss: 324.162109375 = 1.660583257675171 + 50.0 * 6.45003080368042
Epoch 220, val loss: 1.6664470434188843
Epoch 230, training loss: 323.63421630859375 = 1.6406540870666504 + 50.0 * 6.439871311187744
Epoch 230, val loss: 1.6482374668121338
Epoch 240, training loss: 323.21514892578125 = 1.6192268133163452 + 50.0 * 6.431918144226074
Epoch 240, val loss: 1.628883719444275
Epoch 250, training loss: 322.74822998046875 = 1.5964086055755615 + 50.0 * 6.423036575317383
Epoch 250, val loss: 1.6082391738891602
Epoch 260, training loss: 322.4519348144531 = 1.5721861124038696 + 50.0 * 6.417594909667969
Epoch 260, val loss: 1.5863630771636963
Epoch 270, training loss: 322.0563049316406 = 1.5463593006134033 + 50.0 * 6.41019868850708
Epoch 270, val loss: 1.563240647315979
Epoch 280, training loss: 321.72216796875 = 1.5195600986480713 + 50.0 * 6.404052257537842
Epoch 280, val loss: 1.53915536403656
Epoch 290, training loss: 321.3313293457031 = 1.4916961193084717 + 50.0 * 6.396792411804199
Epoch 290, val loss: 1.5141099691390991
Epoch 300, training loss: 321.0025634765625 = 1.4630221128463745 + 50.0 * 6.390790939331055
Epoch 300, val loss: 1.4885016679763794
Epoch 310, training loss: 320.75238037109375 = 1.4337332248687744 + 50.0 * 6.3863725662231445
Epoch 310, val loss: 1.462490439414978
Epoch 320, training loss: 320.6090087890625 = 1.404057502746582 + 50.0 * 6.384099006652832
Epoch 320, val loss: 1.4358314275741577
Epoch 330, training loss: 320.20135498046875 = 1.37411367893219 + 50.0 * 6.376544952392578
Epoch 330, val loss: 1.4093867540359497
Epoch 340, training loss: 320.0531921386719 = 1.3442375659942627 + 50.0 * 6.374178886413574
Epoch 340, val loss: 1.382928729057312
Epoch 350, training loss: 319.721435546875 = 1.3142619132995605 + 50.0 * 6.368143081665039
Epoch 350, val loss: 1.3568040132522583
Epoch 360, training loss: 319.3833312988281 = 1.2848197221755981 + 50.0 * 6.3619704246521
Epoch 360, val loss: 1.3309845924377441
Epoch 370, training loss: 319.4798278808594 = 1.255764365196228 + 50.0 * 6.364481449127197
Epoch 370, val loss: 1.3055974245071411
Epoch 380, training loss: 318.9369812011719 = 1.2268801927566528 + 50.0 * 6.3542022705078125
Epoch 380, val loss: 1.280776023864746
Epoch 390, training loss: 318.7794189453125 = 1.1987169981002808 + 50.0 * 6.351613521575928
Epoch 390, val loss: 1.2565906047821045
Epoch 400, training loss: 318.56658935546875 = 1.1710211038589478 + 50.0 * 6.347911357879639
Epoch 400, val loss: 1.2330784797668457
Epoch 410, training loss: 318.3636474609375 = 1.1439924240112305 + 50.0 * 6.344393253326416
Epoch 410, val loss: 1.210276484489441
Epoch 420, training loss: 318.3179626464844 = 1.11753511428833 + 50.0 * 6.344008922576904
Epoch 420, val loss: 1.188171148300171
Epoch 430, training loss: 318.0335998535156 = 1.0914984941482544 + 50.0 * 6.338841915130615
Epoch 430, val loss: 1.1668192148208618
Epoch 440, training loss: 317.946044921875 = 1.0662027597427368 + 50.0 * 6.337596893310547
Epoch 440, val loss: 1.1461035013198853
Epoch 450, training loss: 317.7292175292969 = 1.0414761304855347 + 50.0 * 6.333755016326904
Epoch 450, val loss: 1.1260979175567627
Epoch 460, training loss: 317.5172424316406 = 1.017249345779419 + 50.0 * 6.329999923706055
Epoch 460, val loss: 1.1067783832550049
Epoch 470, training loss: 317.4588928222656 = 0.9937394261360168 + 50.0 * 6.329302787780762
Epoch 470, val loss: 1.0881623029708862
Epoch 480, training loss: 317.2686767578125 = 0.9704826474189758 + 50.0 * 6.325963973999023
Epoch 480, val loss: 1.070325255393982
Epoch 490, training loss: 317.0987548828125 = 0.9479788541793823 + 50.0 * 6.3230156898498535
Epoch 490, val loss: 1.0530941486358643
Epoch 500, training loss: 316.99322509765625 = 0.9259357452392578 + 50.0 * 6.321345806121826
Epoch 500, val loss: 1.036563754081726
Epoch 510, training loss: 316.77935791015625 = 0.9045106768608093 + 50.0 * 6.3174967765808105
Epoch 510, val loss: 1.0206177234649658
Epoch 520, training loss: 316.64520263671875 = 0.8834717869758606 + 50.0 * 6.315234661102295
Epoch 520, val loss: 1.0054271221160889
Epoch 530, training loss: 316.735595703125 = 0.8629036545753479 + 50.0 * 6.317453384399414
Epoch 530, val loss: 0.9908233880996704
Epoch 540, training loss: 316.6131896972656 = 0.8428499102592468 + 50.0 * 6.315406322479248
Epoch 540, val loss: 0.976753830909729
Epoch 550, training loss: 316.341552734375 = 0.8229033350944519 + 50.0 * 6.310372829437256
Epoch 550, val loss: 0.9633256793022156
Epoch 560, training loss: 316.2093505859375 = 0.803556501865387 + 50.0 * 6.3081159591674805
Epoch 560, val loss: 0.9503924250602722
Epoch 570, training loss: 316.0674133300781 = 0.7846491932868958 + 50.0 * 6.305655479431152
Epoch 570, val loss: 0.938063383102417
Epoch 580, training loss: 316.05340576171875 = 0.7661113739013672 + 50.0 * 6.305746078491211
Epoch 580, val loss: 0.9263447523117065
Epoch 590, training loss: 315.8218994140625 = 0.7477099299430847 + 50.0 * 6.301483631134033
Epoch 590, val loss: 0.9150863885879517
Epoch 600, training loss: 315.7127380371094 = 0.7297348380088806 + 50.0 * 6.2996602058410645
Epoch 600, val loss: 0.9043469429016113
Epoch 610, training loss: 316.0429992675781 = 0.7121481895446777 + 50.0 * 6.30661678314209
Epoch 610, val loss: 0.8940605521202087
Epoch 620, training loss: 315.6217346191406 = 0.6948118805885315 + 50.0 * 6.2985382080078125
Epoch 620, val loss: 0.8842892646789551
Epoch 630, training loss: 315.42620849609375 = 0.6778022646903992 + 50.0 * 6.294968128204346
Epoch 630, val loss: 0.8750765919685364
Epoch 640, training loss: 315.343017578125 = 0.6612027883529663 + 50.0 * 6.293636322021484
Epoch 640, val loss: 0.8664666414260864
Epoch 650, training loss: 315.5837097167969 = 0.644832968711853 + 50.0 * 6.2987775802612305
Epoch 650, val loss: 0.858353853225708
Epoch 660, training loss: 315.3104248046875 = 0.6289029121398926 + 50.0 * 6.293630123138428
Epoch 660, val loss: 0.8508225083351135
Epoch 670, training loss: 315.1150817871094 = 0.6133511662483215 + 50.0 * 6.290034770965576
Epoch 670, val loss: 0.8435858488082886
Epoch 680, training loss: 315.0096435546875 = 0.5981492400169373 + 50.0 * 6.288229942321777
Epoch 680, val loss: 0.8371773958206177
Epoch 690, training loss: 314.9126892089844 = 0.5833274126052856 + 50.0 * 6.286587238311768
Epoch 690, val loss: 0.8312978148460388
Epoch 700, training loss: 314.9052734375 = 0.5688904523849487 + 50.0 * 6.2867279052734375
Epoch 700, val loss: 0.8259466290473938
Epoch 710, training loss: 314.8766784667969 = 0.5548351407051086 + 50.0 * 6.286437034606934
Epoch 710, val loss: 0.8210235834121704
Epoch 720, training loss: 314.81475830078125 = 0.5410021543502808 + 50.0 * 6.28547477722168
Epoch 720, val loss: 0.8165837526321411
Epoch 730, training loss: 314.6153259277344 = 0.5276703834533691 + 50.0 * 6.281753063201904
Epoch 730, val loss: 0.8127837777137756
Epoch 740, training loss: 314.58380126953125 = 0.5147152543067932 + 50.0 * 6.281382083892822
Epoch 740, val loss: 0.8094797730445862
Epoch 750, training loss: 314.6441955566406 = 0.5021136999130249 + 50.0 * 6.282841682434082
Epoch 750, val loss: 0.8065819144248962
Epoch 760, training loss: 314.4634094238281 = 0.4896930158138275 + 50.0 * 6.27947473526001
Epoch 760, val loss: 0.8041127920150757
Epoch 770, training loss: 314.3829345703125 = 0.4777567982673645 + 50.0 * 6.278103828430176
Epoch 770, val loss: 0.8022271990776062
Epoch 780, training loss: 314.3748474121094 = 0.46613046526908875 + 50.0 * 6.27817440032959
Epoch 780, val loss: 0.8007533550262451
Epoch 790, training loss: 314.2023010253906 = 0.45481911301612854 + 50.0 * 6.274949073791504
Epoch 790, val loss: 0.7996249198913574
Epoch 800, training loss: 314.44329833984375 = 0.44386669993400574 + 50.0 * 6.2799882888793945
Epoch 800, val loss: 0.7988710403442383
Epoch 810, training loss: 314.2596435546875 = 0.4330335557460785 + 50.0 * 6.27653169631958
Epoch 810, val loss: 0.7983194589614868
Epoch 820, training loss: 314.0426025390625 = 0.42252179980278015 + 50.0 * 6.272401809692383
Epoch 820, val loss: 0.798248291015625
Epoch 830, training loss: 313.9899597167969 = 0.41235849261283875 + 50.0 * 6.271552085876465
Epoch 830, val loss: 0.7985875606536865
Epoch 840, training loss: 314.0513610839844 = 0.4024922847747803 + 50.0 * 6.272977352142334
Epoch 840, val loss: 0.7991846203804016
Epoch 850, training loss: 313.90948486328125 = 0.39287853240966797 + 50.0 * 6.270331859588623
Epoch 850, val loss: 0.7998796105384827
Epoch 860, training loss: 313.92608642578125 = 0.38344883918762207 + 50.0 * 6.270852565765381
Epoch 860, val loss: 0.8009886145591736
Epoch 870, training loss: 313.8170166015625 = 0.3743112087249756 + 50.0 * 6.26885461807251
Epoch 870, val loss: 0.8023238182067871
Epoch 880, training loss: 313.6992492675781 = 0.3653591275215149 + 50.0 * 6.2666778564453125
Epoch 880, val loss: 0.803924024105072
Epoch 890, training loss: 313.90045166015625 = 0.3566984236240387 + 50.0 * 6.270875453948975
Epoch 890, val loss: 0.8057606816291809
Epoch 900, training loss: 313.7324523925781 = 0.3481954336166382 + 50.0 * 6.2676849365234375
Epoch 900, val loss: 0.807645857334137
Epoch 910, training loss: 313.75640869140625 = 0.3399030864238739 + 50.0 * 6.268330097198486
Epoch 910, val loss: 0.8099057078361511
Epoch 920, training loss: 313.5536804199219 = 0.3318672478199005 + 50.0 * 6.2644362449646
Epoch 920, val loss: 0.8121917247772217
Epoch 930, training loss: 313.5211486816406 = 0.3239934742450714 + 50.0 * 6.263942718505859
Epoch 930, val loss: 0.8147674202919006
Epoch 940, training loss: 313.4687805175781 = 0.31634190678596497 + 50.0 * 6.2630486488342285
Epoch 940, val loss: 0.8175379037857056
Epoch 950, training loss: 313.45587158203125 = 0.30891573429107666 + 50.0 * 6.262938976287842
Epoch 950, val loss: 0.8203609585762024
Epoch 960, training loss: 313.3585510253906 = 0.30165448784828186 + 50.0 * 6.26113748550415
Epoch 960, val loss: 0.8234779834747314
Epoch 970, training loss: 313.29327392578125 = 0.2946063280105591 + 50.0 * 6.259973049163818
Epoch 970, val loss: 0.8266738653182983
Epoch 980, training loss: 313.621337890625 = 0.2877257466316223 + 50.0 * 6.266672134399414
Epoch 980, val loss: 0.8299500346183777
Epoch 990, training loss: 313.25213623046875 = 0.2808702886104584 + 50.0 * 6.259425163269043
Epoch 990, val loss: 0.8332638740539551
Epoch 1000, training loss: 313.227294921875 = 0.2742261290550232 + 50.0 * 6.259061336517334
Epoch 1000, val loss: 0.8367769122123718
Epoch 1010, training loss: 313.2388000488281 = 0.26783138513565063 + 50.0 * 6.2594194412231445
Epoch 1010, val loss: 0.8405147790908813
Epoch 1020, training loss: 313.0851745605469 = 0.26158788800239563 + 50.0 * 6.256471633911133
Epoch 1020, val loss: 0.8443334102630615
Epoch 1030, training loss: 313.1418151855469 = 0.25546765327453613 + 50.0 * 6.257727146148682
Epoch 1030, val loss: 0.8482505679130554
Epoch 1040, training loss: 313.1314392089844 = 0.24948136508464813 + 50.0 * 6.257639408111572
Epoch 1040, val loss: 0.8522219657897949
Epoch 1050, training loss: 312.9767150878906 = 0.24368254840373993 + 50.0 * 6.254660606384277
Epoch 1050, val loss: 0.8563401699066162
Epoch 1060, training loss: 312.9214782714844 = 0.23800362646579742 + 50.0 * 6.253669738769531
Epoch 1060, val loss: 0.8606234192848206
Epoch 1070, training loss: 313.046142578125 = 0.23249691724777222 + 50.0 * 6.25627326965332
Epoch 1070, val loss: 0.865009605884552
Epoch 1080, training loss: 312.8642272949219 = 0.22709183394908905 + 50.0 * 6.252742767333984
Epoch 1080, val loss: 0.8692301511764526
Epoch 1090, training loss: 312.8282775878906 = 0.2218276709318161 + 50.0 * 6.252129077911377
Epoch 1090, val loss: 0.8736421465873718
Epoch 1100, training loss: 312.81689453125 = 0.2166948765516281 + 50.0 * 6.2520036697387695
Epoch 1100, val loss: 0.8781000971794128
Epoch 1110, training loss: 313.0291442871094 = 0.21165885031223297 + 50.0 * 6.256349563598633
Epoch 1110, val loss: 0.8826874494552612
Epoch 1120, training loss: 312.85308837890625 = 0.2067418247461319 + 50.0 * 6.252926826477051
Epoch 1120, val loss: 0.8870112895965576
Epoch 1130, training loss: 312.73907470703125 = 0.20192064344882965 + 50.0 * 6.2507429122924805
Epoch 1130, val loss: 0.8916940689086914
Epoch 1140, training loss: 312.99188232421875 = 0.1973390281200409 + 50.0 * 6.255890846252441
Epoch 1140, val loss: 0.8962245583534241
Epoch 1150, training loss: 312.7552795410156 = 0.19264836609363556 + 50.0 * 6.251252174377441
Epoch 1150, val loss: 0.900837779045105
Epoch 1160, training loss: 312.6528625488281 = 0.18821942806243896 + 50.0 * 6.249292850494385
Epoch 1160, val loss: 0.9055214524269104
Epoch 1170, training loss: 312.6631774902344 = 0.1838395595550537 + 50.0 * 6.249586582183838
Epoch 1170, val loss: 0.9103240966796875
Epoch 1180, training loss: 312.6204528808594 = 0.17958877980709076 + 50.0 * 6.248816967010498
Epoch 1180, val loss: 0.9148992300033569
Epoch 1190, training loss: 312.5484313964844 = 0.17541031539440155 + 50.0 * 6.24746036529541
Epoch 1190, val loss: 0.9195255041122437
Epoch 1200, training loss: 312.54083251953125 = 0.1713847666978836 + 50.0 * 6.24738883972168
Epoch 1200, val loss: 0.9243397116661072
Epoch 1210, training loss: 312.4771728515625 = 0.16745245456695557 + 50.0 * 6.246194362640381
Epoch 1210, val loss: 0.9290429949760437
Epoch 1220, training loss: 312.4879150390625 = 0.16361045837402344 + 50.0 * 6.246486186981201
Epoch 1220, val loss: 0.933718204498291
Epoch 1230, training loss: 312.5155029296875 = 0.15985481441020966 + 50.0 * 6.247113227844238
Epoch 1230, val loss: 0.9383987784385681
Epoch 1240, training loss: 312.7566833496094 = 0.15613040328025818 + 50.0 * 6.252010822296143
Epoch 1240, val loss: 0.9432543516159058
Epoch 1250, training loss: 312.4073486328125 = 0.15257714688777924 + 50.0 * 6.245095252990723
Epoch 1250, val loss: 0.9477400779724121
Epoch 1260, training loss: 312.3049621582031 = 0.14905035495758057 + 50.0 * 6.2431182861328125
Epoch 1260, val loss: 0.952852725982666
Epoch 1270, training loss: 312.2885437011719 = 0.14566729962825775 + 50.0 * 6.242857933044434
Epoch 1270, val loss: 0.9577597379684448
Epoch 1280, training loss: 312.6423034667969 = 0.14237138628959656 + 50.0 * 6.249999046325684
Epoch 1280, val loss: 0.9625266790390015
Epoch 1290, training loss: 312.485595703125 = 0.1391061544418335 + 50.0 * 6.246930122375488
Epoch 1290, val loss: 0.9674464464187622
Epoch 1300, training loss: 312.26953125 = 0.13590431213378906 + 50.0 * 6.242672920227051
Epoch 1300, val loss: 0.9724311232566833
Epoch 1310, training loss: 312.18560791015625 = 0.1328439861536026 + 50.0 * 6.241055011749268
Epoch 1310, val loss: 0.9774573445320129
Epoch 1320, training loss: 312.23480224609375 = 0.12986165285110474 + 50.0 * 6.242099285125732
Epoch 1320, val loss: 0.9825197458267212
Epoch 1330, training loss: 312.2706298828125 = 0.1269281655550003 + 50.0 * 6.2428741455078125
Epoch 1330, val loss: 0.9872751832008362
Epoch 1340, training loss: 312.1956481933594 = 0.12405838072299957 + 50.0 * 6.241431713104248
Epoch 1340, val loss: 0.9921761751174927
Epoch 1350, training loss: 312.1534729003906 = 0.12127169221639633 + 50.0 * 6.2406439781188965
Epoch 1350, val loss: 0.9971168637275696
Epoch 1360, training loss: 312.3850402832031 = 0.11860084533691406 + 50.0 * 6.245328903198242
Epoch 1360, val loss: 1.002264380455017
Epoch 1370, training loss: 312.1012268066406 = 0.11586944013834 + 50.0 * 6.239706993103027
Epoch 1370, val loss: 1.0068120956420898
Epoch 1380, training loss: 312.02239990234375 = 0.11330480128526688 + 50.0 * 6.2381815910339355
Epoch 1380, val loss: 1.012177586555481
Epoch 1390, training loss: 312.04315185546875 = 0.11082866042852402 + 50.0 * 6.238646030426025
Epoch 1390, val loss: 1.017225742340088
Epoch 1400, training loss: 312.15850830078125 = 0.10838104784488678 + 50.0 * 6.241003036499023
Epoch 1400, val loss: 1.0224277973175049
Epoch 1410, training loss: 312.15032958984375 = 0.1059529110789299 + 50.0 * 6.240887641906738
Epoch 1410, val loss: 1.0269460678100586
Epoch 1420, training loss: 312.0325927734375 = 0.10364460200071335 + 50.0 * 6.238578796386719
Epoch 1420, val loss: 1.0322858095169067
Epoch 1430, training loss: 312.0430603027344 = 0.10136139392852783 + 50.0 * 6.238834381103516
Epoch 1430, val loss: 1.03715980052948
Epoch 1440, training loss: 312.0375061035156 = 0.09916572272777557 + 50.0 * 6.238767147064209
Epoch 1440, val loss: 1.0420783758163452
Epoch 1450, training loss: 312.0760192871094 = 0.09701884537935257 + 50.0 * 6.239580154418945
Epoch 1450, val loss: 1.0471140146255493
Epoch 1460, training loss: 311.9261779785156 = 0.09490252286195755 + 50.0 * 6.2366251945495605
Epoch 1460, val loss: 1.0521552562713623
Epoch 1470, training loss: 311.93243408203125 = 0.09287462383508682 + 50.0 * 6.236791133880615
Epoch 1470, val loss: 1.057090401649475
Epoch 1480, training loss: 311.8967590332031 = 0.0908762589097023 + 50.0 * 6.236117362976074
Epoch 1480, val loss: 1.062201976776123
Epoch 1490, training loss: 311.84490966796875 = 0.08892855048179626 + 50.0 * 6.235119342803955
Epoch 1490, val loss: 1.067061185836792
Epoch 1500, training loss: 312.08929443359375 = 0.08703231811523438 + 50.0 * 6.240045070648193
Epoch 1500, val loss: 1.071867823600769
Epoch 1510, training loss: 311.8837585449219 = 0.08518164604902267 + 50.0 * 6.235971927642822
Epoch 1510, val loss: 1.0771867036819458
Epoch 1520, training loss: 311.80731201171875 = 0.08337540924549103 + 50.0 * 6.234478950500488
Epoch 1520, val loss: 1.0817385911941528
Epoch 1530, training loss: 311.7605285644531 = 0.08162561058998108 + 50.0 * 6.233577728271484
Epoch 1530, val loss: 1.087227463722229
Epoch 1540, training loss: 311.862060546875 = 0.07993610948324203 + 50.0 * 6.235642910003662
Epoch 1540, val loss: 1.092003345489502
Epoch 1550, training loss: 311.772216796875 = 0.07826672494411469 + 50.0 * 6.2338786125183105
Epoch 1550, val loss: 1.0972025394439697
Epoch 1560, training loss: 311.8912353515625 = 0.07663515955209732 + 50.0 * 6.236291885375977
Epoch 1560, val loss: 1.1018158197402954
Epoch 1570, training loss: 311.69244384765625 = 0.07504168897867203 + 50.0 * 6.2323479652404785
Epoch 1570, val loss: 1.1068735122680664
Epoch 1580, training loss: 311.65625 = 0.0734972134232521 + 50.0 * 6.231655120849609
Epoch 1580, val loss: 1.1117792129516602
Epoch 1590, training loss: 311.691162109375 = 0.07203304022550583 + 50.0 * 6.232382774353027
Epoch 1590, val loss: 1.1168068647384644
Epoch 1600, training loss: 311.8447265625 = 0.07057961821556091 + 50.0 * 6.235482692718506
Epoch 1600, val loss: 1.1218854188919067
Epoch 1610, training loss: 311.6900329589844 = 0.06909852474927902 + 50.0 * 6.232419013977051
Epoch 1610, val loss: 1.1265159845352173
Epoch 1620, training loss: 311.6039123535156 = 0.06771577894687653 + 50.0 * 6.230723857879639
Epoch 1620, val loss: 1.131460428237915
Epoch 1630, training loss: 311.6275939941406 = 0.06636207550764084 + 50.0 * 6.231224536895752
Epoch 1630, val loss: 1.1362804174423218
Epoch 1640, training loss: 311.7992248535156 = 0.06505896896123886 + 50.0 * 6.234683036804199
Epoch 1640, val loss: 1.1412404775619507
Epoch 1650, training loss: 311.7139587402344 = 0.06373463571071625 + 50.0 * 6.233004570007324
Epoch 1650, val loss: 1.1459959745407104
Epoch 1660, training loss: 311.591064453125 = 0.062482837587594986 + 50.0 * 6.230571746826172
Epoch 1660, val loss: 1.150835394859314
Epoch 1670, training loss: 311.6109313964844 = 0.061251938343048096 + 50.0 * 6.230993747711182
Epoch 1670, val loss: 1.1556679010391235
Epoch 1680, training loss: 311.4848937988281 = 0.06004064157605171 + 50.0 * 6.228497505187988
Epoch 1680, val loss: 1.1603776216506958
Epoch 1690, training loss: 311.512939453125 = 0.05887757986783981 + 50.0 * 6.229081153869629
Epoch 1690, val loss: 1.165050983428955
Epoch 1700, training loss: 311.66217041015625 = 0.05774442106485367 + 50.0 * 6.232088565826416
Epoch 1700, val loss: 1.169613242149353
Epoch 1710, training loss: 311.6250305175781 = 0.05661911144852638 + 50.0 * 6.231368541717529
Epoch 1710, val loss: 1.174717903137207
Epoch 1720, training loss: 311.5767517089844 = 0.05552872270345688 + 50.0 * 6.230424404144287
Epoch 1720, val loss: 1.1788464784622192
Epoch 1730, training loss: 311.4290771484375 = 0.05445924401283264 + 50.0 * 6.227491855621338
Epoch 1730, val loss: 1.1841086149215698
Epoch 1740, training loss: 311.41351318359375 = 0.05344059690833092 + 50.0 * 6.227201461791992
Epoch 1740, val loss: 1.188685655593872
Epoch 1750, training loss: 311.55181884765625 = 0.05244094878435135 + 50.0 * 6.229988098144531
Epoch 1750, val loss: 1.1933974027633667
Epoch 1760, training loss: 311.3482360839844 = 0.05144154652953148 + 50.0 * 6.225935459136963
Epoch 1760, val loss: 1.1977723836898804
Epoch 1770, training loss: 311.59490966796875 = 0.05047222226858139 + 50.0 * 6.230888366699219
Epoch 1770, val loss: 1.2021647691726685
Epoch 1780, training loss: 311.42041015625 = 0.04951222985982895 + 50.0 * 6.227418422698975
Epoch 1780, val loss: 1.2067906856536865
Epoch 1790, training loss: 311.30145263671875 = 0.04859375208616257 + 50.0 * 6.225057125091553
Epoch 1790, val loss: 1.2112623453140259
Epoch 1800, training loss: 311.2973327636719 = 0.04770397022366524 + 50.0 * 6.224992752075195
Epoch 1800, val loss: 1.2158976793289185
Epoch 1810, training loss: 311.4954833984375 = 0.046856850385665894 + 50.0 * 6.228972911834717
Epoch 1810, val loss: 1.2204867601394653
Epoch 1820, training loss: 311.3866882324219 = 0.04597568139433861 + 50.0 * 6.226814270019531
Epoch 1820, val loss: 1.2249484062194824
Epoch 1830, training loss: 311.2906799316406 = 0.04513296112418175 + 50.0 * 6.224910736083984
Epoch 1830, val loss: 1.2292441129684448
Epoch 1840, training loss: 311.257568359375 = 0.04432366043329239 + 50.0 * 6.224265098571777
Epoch 1840, val loss: 1.2338566780090332
Epoch 1850, training loss: 311.2784423828125 = 0.043539613485336304 + 50.0 * 6.224698066711426
Epoch 1850, val loss: 1.238315463066101
Epoch 1860, training loss: 311.4732360839844 = 0.04278185963630676 + 50.0 * 6.228609085083008
Epoch 1860, val loss: 1.2424315214157104
Epoch 1870, training loss: 311.7153625488281 = 0.04198892414569855 + 50.0 * 6.2334675788879395
Epoch 1870, val loss: 1.2466477155685425
Epoch 1880, training loss: 311.3650817871094 = 0.04124215617775917 + 50.0 * 6.226477146148682
Epoch 1880, val loss: 1.2513716220855713
Epoch 1890, training loss: 311.2211608886719 = 0.040502212941646576 + 50.0 * 6.223613262176514
Epoch 1890, val loss: 1.2555311918258667
Epoch 1900, training loss: 311.1650695800781 = 0.039811667054891586 + 50.0 * 6.222504615783691
Epoch 1900, val loss: 1.2600127458572388
Epoch 1910, training loss: 311.185791015625 = 0.039130810648202896 + 50.0 * 6.222933292388916
Epoch 1910, val loss: 1.2644108533859253
Epoch 1920, training loss: 311.5396423339844 = 0.03848477080464363 + 50.0 * 6.230023384094238
Epoch 1920, val loss: 1.2683560848236084
Epoch 1930, training loss: 311.3754577636719 = 0.03778998181223869 + 50.0 * 6.2267537117004395
Epoch 1930, val loss: 1.2725011110305786
Epoch 1940, training loss: 311.1973571777344 = 0.03713562339544296 + 50.0 * 6.223204135894775
Epoch 1940, val loss: 1.2767977714538574
Epoch 1950, training loss: 311.11553955078125 = 0.03650147095322609 + 50.0 * 6.221580982208252
Epoch 1950, val loss: 1.2810181379318237
Epoch 1960, training loss: 311.1026306152344 = 0.03589281439781189 + 50.0 * 6.221334934234619
Epoch 1960, val loss: 1.2850176095962524
Epoch 1970, training loss: 311.4214782714844 = 0.03532297909259796 + 50.0 * 6.227723598480225
Epoch 1970, val loss: 1.2893662452697754
Epoch 1980, training loss: 311.2129211425781 = 0.034696295857429504 + 50.0 * 6.223564147949219
Epoch 1980, val loss: 1.2933424711227417
Epoch 1990, training loss: 311.1439514160156 = 0.0341140553355217 + 50.0 * 6.222196578979492
Epoch 1990, val loss: 1.296959400177002
Epoch 2000, training loss: 311.07244873046875 = 0.033545322716236115 + 50.0 * 6.220777988433838
Epoch 2000, val loss: 1.3015512228012085
Epoch 2010, training loss: 311.0757751464844 = 0.03300460800528526 + 50.0 * 6.220855712890625
Epoch 2010, val loss: 1.3053345680236816
Epoch 2020, training loss: 311.45648193359375 = 0.032466113567352295 + 50.0 * 6.228480339050293
Epoch 2020, val loss: 1.3091741800308228
Epoch 2030, training loss: 311.17974853515625 = 0.03193758428096771 + 50.0 * 6.22295618057251
Epoch 2030, val loss: 1.313449501991272
Epoch 2040, training loss: 311.2174377441406 = 0.031414762139320374 + 50.0 * 6.223720550537109
Epoch 2040, val loss: 1.3172627687454224
Epoch 2050, training loss: 310.9923095703125 = 0.030898336321115494 + 50.0 * 6.219228744506836
Epoch 2050, val loss: 1.3212049007415771
Epoch 2060, training loss: 311.05389404296875 = 0.03040381148457527 + 50.0 * 6.220469951629639
Epoch 2060, val loss: 1.3251739740371704
Epoch 2070, training loss: 311.1894226074219 = 0.0299263596534729 + 50.0 * 6.223189830780029
Epoch 2070, val loss: 1.3288719654083252
Epoch 2080, training loss: 310.99896240234375 = 0.02944800816476345 + 50.0 * 6.219390392303467
Epoch 2080, val loss: 1.3329799175262451
Epoch 2090, training loss: 310.9492492675781 = 0.02898586168885231 + 50.0 * 6.218405246734619
Epoch 2090, val loss: 1.336848497390747
Epoch 2100, training loss: 311.0414733886719 = 0.028546299785375595 + 50.0 * 6.220258712768555
Epoch 2100, val loss: 1.3407319784164429
Epoch 2110, training loss: 311.12457275390625 = 0.028109991922974586 + 50.0 * 6.22192907333374
Epoch 2110, val loss: 1.3444311618804932
Epoch 2120, training loss: 311.0010986328125 = 0.027669718489050865 + 50.0 * 6.219468116760254
Epoch 2120, val loss: 1.3478220701217651
Epoch 2130, training loss: 311.04046630859375 = 0.027243513613939285 + 50.0 * 6.220264434814453
Epoch 2130, val loss: 1.3520982265472412
Epoch 2140, training loss: 310.9681396484375 = 0.026817679405212402 + 50.0 * 6.2188262939453125
Epoch 2140, val loss: 1.3554983139038086
Epoch 2150, training loss: 310.9091491699219 = 0.026407547295093536 + 50.0 * 6.217655181884766
Epoch 2150, val loss: 1.3591866493225098
Epoch 2160, training loss: 310.9609680175781 = 0.026014847680926323 + 50.0 * 6.2186994552612305
Epoch 2160, val loss: 1.363043189048767
Epoch 2170, training loss: 311.02978515625 = 0.025625769048929214 + 50.0 * 6.220082759857178
Epoch 2170, val loss: 1.3664289712905884
Epoch 2180, training loss: 310.92584228515625 = 0.02525070123374462 + 50.0 * 6.218011856079102
Epoch 2180, val loss: 1.3699963092803955
Epoch 2190, training loss: 311.1083679199219 = 0.024871040135622025 + 50.0 * 6.221669673919678
Epoch 2190, val loss: 1.3738176822662354
Epoch 2200, training loss: 310.9504089355469 = 0.024503139778971672 + 50.0 * 6.218517780303955
Epoch 2200, val loss: 1.3767348527908325
Epoch 2210, training loss: 310.88006591796875 = 0.0241364985704422 + 50.0 * 6.217118263244629
Epoch 2210, val loss: 1.3810077905654907
Epoch 2220, training loss: 310.83709716796875 = 0.023790333420038223 + 50.0 * 6.216265678405762
Epoch 2220, val loss: 1.384347915649414
Epoch 2230, training loss: 310.98126220703125 = 0.02344997599720955 + 50.0 * 6.219156742095947
Epoch 2230, val loss: 1.3879735469818115
Epoch 2240, training loss: 310.9923095703125 = 0.023117491975426674 + 50.0 * 6.219383716583252
Epoch 2240, val loss: 1.390975832939148
Epoch 2250, training loss: 310.8201904296875 = 0.02277352660894394 + 50.0 * 6.215948581695557
Epoch 2250, val loss: 1.394872784614563
Epoch 2260, training loss: 310.7790832519531 = 0.022449616342782974 + 50.0 * 6.215132236480713
Epoch 2260, val loss: 1.3983350992202759
Epoch 2270, training loss: 310.7585754394531 = 0.022140635177493095 + 50.0 * 6.214728355407715
Epoch 2270, val loss: 1.402020812034607
Epoch 2280, training loss: 310.820556640625 = 0.02184252254664898 + 50.0 * 6.2159743309021
Epoch 2280, val loss: 1.4054205417633057
Epoch 2290, training loss: 310.9517822265625 = 0.021546058356761932 + 50.0 * 6.218605041503906
Epoch 2290, val loss: 1.4089574813842773
Epoch 2300, training loss: 310.9695739746094 = 0.02123447135090828 + 50.0 * 6.218966960906982
Epoch 2300, val loss: 1.4120523929595947
Epoch 2310, training loss: 310.8203125 = 0.02093936875462532 + 50.0 * 6.215987682342529
Epoch 2310, val loss: 1.415429711341858
Epoch 2320, training loss: 310.9173278808594 = 0.020651867613196373 + 50.0 * 6.217933654785156
Epoch 2320, val loss: 1.4186269044876099
Epoch 2330, training loss: 310.8576354980469 = 0.020373454317450523 + 50.0 * 6.216744899749756
Epoch 2330, val loss: 1.4217628240585327
Epoch 2340, training loss: 310.77447509765625 = 0.02009032852947712 + 50.0 * 6.215087890625
Epoch 2340, val loss: 1.424896478652954
Epoch 2350, training loss: 310.7237548828125 = 0.01981789991259575 + 50.0 * 6.214078903198242
Epoch 2350, val loss: 1.4283615350723267
Epoch 2360, training loss: 310.75128173828125 = 0.019557476043701172 + 50.0 * 6.214634418487549
Epoch 2360, val loss: 1.4317092895507812
Epoch 2370, training loss: 310.8282470703125 = 0.019300350919365883 + 50.0 * 6.216179370880127
Epoch 2370, val loss: 1.4346764087677002
Epoch 2380, training loss: 310.7401428222656 = 0.019047020003199577 + 50.0 * 6.214422225952148
Epoch 2380, val loss: 1.4378457069396973
Epoch 2390, training loss: 310.83599853515625 = 0.018802573904395103 + 50.0 * 6.216343879699707
Epoch 2390, val loss: 1.4412336349487305
Epoch 2400, training loss: 310.75860595703125 = 0.0185528751462698 + 50.0 * 6.214800834655762
Epoch 2400, val loss: 1.444216251373291
Epoch 2410, training loss: 310.7998352050781 = 0.018312010914087296 + 50.0 * 6.215630531311035
Epoch 2410, val loss: 1.4476810693740845
Epoch 2420, training loss: 310.71759033203125 = 0.018071649596095085 + 50.0 * 6.213990211486816
Epoch 2420, val loss: 1.450425624847412
Epoch 2430, training loss: 310.9412841796875 = 0.017844820395112038 + 50.0 * 6.21846866607666
Epoch 2430, val loss: 1.4534653425216675
Epoch 2440, training loss: 310.68658447265625 = 0.01761629804968834 + 50.0 * 6.213379383087158
Epoch 2440, val loss: 1.4569059610366821
Epoch 2450, training loss: 310.6469421386719 = 0.01739460974931717 + 50.0 * 6.21259069442749
Epoch 2450, val loss: 1.4598356485366821
Epoch 2460, training loss: 310.8006896972656 = 0.01718939282000065 + 50.0 * 6.215670108795166
Epoch 2460, val loss: 1.462962031364441
Epoch 2470, training loss: 310.7223815917969 = 0.01696423813700676 + 50.0 * 6.214108467102051
Epoch 2470, val loss: 1.4660412073135376
Epoch 2480, training loss: 310.66265869140625 = 0.016744835302233696 + 50.0 * 6.212917804718018
Epoch 2480, val loss: 1.4686399698257446
Epoch 2490, training loss: 310.64544677734375 = 0.016546625643968582 + 50.0 * 6.212578296661377
Epoch 2490, val loss: 1.4718225002288818
Epoch 2500, training loss: 310.6025695800781 = 0.01634206622838974 + 50.0 * 6.211724758148193
Epoch 2500, val loss: 1.4747923612594604
Epoch 2510, training loss: 310.8945617675781 = 0.016153139993548393 + 50.0 * 6.217568397521973
Epoch 2510, val loss: 1.4777964353561401
Epoch 2520, training loss: 310.7370910644531 = 0.01594299077987671 + 50.0 * 6.214423179626465
Epoch 2520, val loss: 1.4804431200027466
Epoch 2530, training loss: 310.5679626464844 = 0.015746675431728363 + 50.0 * 6.2110443115234375
Epoch 2530, val loss: 1.4835914373397827
Epoch 2540, training loss: 310.5556335449219 = 0.015562626533210278 + 50.0 * 6.210801124572754
Epoch 2540, val loss: 1.486318826675415
Epoch 2550, training loss: 310.69561767578125 = 0.01538085751235485 + 50.0 * 6.21360445022583
Epoch 2550, val loss: 1.489354133605957
Epoch 2560, training loss: 310.5671081542969 = 0.015195666812360287 + 50.0 * 6.211038112640381
Epoch 2560, val loss: 1.4920122623443604
Epoch 2570, training loss: 310.5848388671875 = 0.01501724123954773 + 50.0 * 6.211396217346191
Epoch 2570, val loss: 1.4949100017547607
Epoch 2580, training loss: 310.5699462890625 = 0.014844467863440514 + 50.0 * 6.211102485656738
Epoch 2580, val loss: 1.497962236404419
Epoch 2590, training loss: 310.6401672363281 = 0.014673784375190735 + 50.0 * 6.212509632110596
Epoch 2590, val loss: 1.5006736516952515
Epoch 2600, training loss: 310.65411376953125 = 0.0145038440823555 + 50.0 * 6.21279239654541
Epoch 2600, val loss: 1.503822684288025
Epoch 2610, training loss: 310.5563049316406 = 0.014336644671857357 + 50.0 * 6.21083927154541
Epoch 2610, val loss: 1.5059466361999512
Epoch 2620, training loss: 310.55584716796875 = 0.014172086492180824 + 50.0 * 6.210833549499512
Epoch 2620, val loss: 1.509321689605713
Epoch 2630, training loss: 310.53619384765625 = 0.014010312967002392 + 50.0 * 6.21044397354126
Epoch 2630, val loss: 1.511531949043274
Epoch 2640, training loss: 310.6193542480469 = 0.013855436816811562 + 50.0 * 6.2121100425720215
Epoch 2640, val loss: 1.5143448114395142
Epoch 2650, training loss: 310.4526062011719 = 0.013702181167900562 + 50.0 * 6.208777904510498
Epoch 2650, val loss: 1.5170661211013794
Epoch 2660, training loss: 310.4781188964844 = 0.013554799370467663 + 50.0 * 6.209291458129883
Epoch 2660, val loss: 1.520160436630249
Epoch 2670, training loss: 310.5827941894531 = 0.013412022963166237 + 50.0 * 6.211387634277344
Epoch 2670, val loss: 1.5225210189819336
Epoch 2680, training loss: 310.4685974121094 = 0.013259682804346085 + 50.0 * 6.209106922149658
Epoch 2680, val loss: 1.525056004524231
Epoch 2690, training loss: 310.6246032714844 = 0.013114765286445618 + 50.0 * 6.2122297286987305
Epoch 2690, val loss: 1.527644157409668
Epoch 2700, training loss: 310.58160400390625 = 0.012971058487892151 + 50.0 * 6.2113728523254395
Epoch 2700, val loss: 1.5303159952163696
Epoch 2710, training loss: 310.4686584472656 = 0.01282916497439146 + 50.0 * 6.2091169357299805
Epoch 2710, val loss: 1.5332163572311401
Epoch 2720, training loss: 310.50555419921875 = 0.012697895057499409 + 50.0 * 6.20985746383667
Epoch 2720, val loss: 1.5355126857757568
Epoch 2730, training loss: 310.55194091796875 = 0.012558035552501678 + 50.0 * 6.210787296295166
Epoch 2730, val loss: 1.5382853746414185
Epoch 2740, training loss: 310.4967956542969 = 0.012422866187989712 + 50.0 * 6.209687232971191
Epoch 2740, val loss: 1.5405088663101196
Epoch 2750, training loss: 310.3868713378906 = 0.012292680330574512 + 50.0 * 6.207491397857666
Epoch 2750, val loss: 1.5435693264007568
Epoch 2760, training loss: 310.3894958496094 = 0.012168603017926216 + 50.0 * 6.207546234130859
Epoch 2760, val loss: 1.545892357826233
Epoch 2770, training loss: 310.4581604003906 = 0.012046118266880512 + 50.0 * 6.208922386169434
Epoch 2770, val loss: 1.5486831665039062
Epoch 2780, training loss: 310.4799499511719 = 0.011920122429728508 + 50.0 * 6.209360599517822
Epoch 2780, val loss: 1.5510215759277344
Epoch 2790, training loss: 310.4742126464844 = 0.01179592963308096 + 50.0 * 6.209248065948486
Epoch 2790, val loss: 1.553024172782898
Epoch 2800, training loss: 310.57257080078125 = 0.011675803922116756 + 50.0 * 6.211217880249023
Epoch 2800, val loss: 1.5557987689971924
Epoch 2810, training loss: 310.3966369628906 = 0.01155642606317997 + 50.0 * 6.207701683044434
Epoch 2810, val loss: 1.5585044622421265
Epoch 2820, training loss: 310.349853515625 = 0.011442967690527439 + 50.0 * 6.206768035888672
Epoch 2820, val loss: 1.5606199502944946
Epoch 2830, training loss: 310.50390625 = 0.0113316485658288 + 50.0 * 6.2098517417907715
Epoch 2830, val loss: 1.5634207725524902
Epoch 2840, training loss: 310.3075866699219 = 0.011215835809707642 + 50.0 * 6.205926895141602
Epoch 2840, val loss: 1.565542221069336
Epoch 2850, training loss: 310.32147216796875 = 0.01110371109098196 + 50.0 * 6.206207275390625
Epoch 2850, val loss: 1.5679547786712646
Epoch 2860, training loss: 310.3993225097656 = 0.011000723578035831 + 50.0 * 6.207767009735107
Epoch 2860, val loss: 1.5703833103179932
Epoch 2870, training loss: 310.46630859375 = 0.010896984487771988 + 50.0 * 6.209108352661133
Epoch 2870, val loss: 1.572474718093872
Epoch 2880, training loss: 310.366943359375 = 0.010784871876239777 + 50.0 * 6.207123279571533
Epoch 2880, val loss: 1.5750614404678345
Epoch 2890, training loss: 310.4525451660156 = 0.010689560323953629 + 50.0 * 6.208837032318115
Epoch 2890, val loss: 1.577238917350769
Epoch 2900, training loss: 310.3399658203125 = 0.010580455884337425 + 50.0 * 6.206587791442871
Epoch 2900, val loss: 1.5796136856079102
Epoch 2910, training loss: 310.3202209472656 = 0.010476835072040558 + 50.0 * 6.20619535446167
Epoch 2910, val loss: 1.5815930366516113
Epoch 2920, training loss: 310.32073974609375 = 0.010381066240370274 + 50.0 * 6.206207275390625
Epoch 2920, val loss: 1.5839086771011353
Epoch 2930, training loss: 310.3682556152344 = 0.010286225005984306 + 50.0 * 6.207159519195557
Epoch 2930, val loss: 1.586408019065857
Epoch 2940, training loss: 310.3934326171875 = 0.010190742090344429 + 50.0 * 6.207664966583252
Epoch 2940, val loss: 1.5884610414505005
Epoch 2950, training loss: 310.3148498535156 = 0.010093429125845432 + 50.0 * 6.206094741821289
Epoch 2950, val loss: 1.590711236000061
Epoch 2960, training loss: 310.24725341796875 = 0.010000059381127357 + 50.0 * 6.204744815826416
Epoch 2960, val loss: 1.5928813219070435
Epoch 2970, training loss: 310.2582092285156 = 0.009911528788506985 + 50.0 * 6.204965591430664
Epoch 2970, val loss: 1.59516179561615
Epoch 2980, training loss: 310.31793212890625 = 0.0098239341750741 + 50.0 * 6.206161975860596
Epoch 2980, val loss: 1.5972530841827393
Epoch 2990, training loss: 310.2644348144531 = 0.009735858999192715 + 50.0 * 6.205093860626221
Epoch 2990, val loss: 1.5993094444274902
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7148148148148148
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 431.79345703125 = 1.9527817964553833 + 50.0 * 8.596813201904297
Epoch 0, val loss: 1.9520939588546753
Epoch 10, training loss: 431.73321533203125 = 1.9443122148513794 + 50.0 * 8.595778465270996
Epoch 10, val loss: 1.9438012838363647
Epoch 20, training loss: 431.3887634277344 = 1.933875560760498 + 50.0 * 8.58909797668457
Epoch 20, val loss: 1.9334495067596436
Epoch 30, training loss: 429.0790100097656 = 1.9208258390426636 + 50.0 * 8.543163299560547
Epoch 30, val loss: 1.9205509424209595
Epoch 40, training loss: 412.827880859375 = 1.9052608013153076 + 50.0 * 8.218452453613281
Epoch 40, val loss: 1.9055871963500977
Epoch 50, training loss: 370.9230041503906 = 1.886502742767334 + 50.0 * 7.380730152130127
Epoch 50, val loss: 1.888425588607788
Epoch 60, training loss: 361.861083984375 = 1.8704391717910767 + 50.0 * 7.199812889099121
Epoch 60, val loss: 1.8736872673034668
Epoch 70, training loss: 354.5726318359375 = 1.8552980422973633 + 50.0 * 7.054347038269043
Epoch 70, val loss: 1.8595231771469116
Epoch 80, training loss: 350.1697998046875 = 1.8410568237304688 + 50.0 * 6.966574668884277
Epoch 80, val loss: 1.8461847305297852
Epoch 90, training loss: 346.995361328125 = 1.8288670778274536 + 50.0 * 6.903330326080322
Epoch 90, val loss: 1.834425926208496
Epoch 100, training loss: 343.3125305175781 = 1.8177233934402466 + 50.0 * 6.829896450042725
Epoch 100, val loss: 1.8234691619873047
Epoch 110, training loss: 339.70245361328125 = 1.808197259902954 + 50.0 * 6.757884979248047
Epoch 110, val loss: 1.8138574361801147
Epoch 120, training loss: 337.53564453125 = 1.7993649244308472 + 50.0 * 6.714725494384766
Epoch 120, val loss: 1.804688572883606
Epoch 130, training loss: 335.592529296875 = 1.7897449731826782 + 50.0 * 6.676055908203125
Epoch 130, val loss: 1.7949973344802856
Epoch 140, training loss: 333.6453857421875 = 1.7801761627197266 + 50.0 * 6.637304306030273
Epoch 140, val loss: 1.7854433059692383
Epoch 150, training loss: 331.9021301269531 = 1.7710992097854614 + 50.0 * 6.602620601654053
Epoch 150, val loss: 1.7763890027999878
Epoch 160, training loss: 330.3979187011719 = 1.7618300914764404 + 50.0 * 6.572721481323242
Epoch 160, val loss: 1.7672313451766968
Epoch 170, training loss: 329.20562744140625 = 1.751490592956543 + 50.0 * 6.5490827560424805
Epoch 170, val loss: 1.7572213411331177
Epoch 180, training loss: 328.1126708984375 = 1.7399157285690308 + 50.0 * 6.527454853057861
Epoch 180, val loss: 1.7463741302490234
Epoch 190, training loss: 327.166015625 = 1.7277013063430786 + 50.0 * 6.5087666511535645
Epoch 190, val loss: 1.7348551750183105
Epoch 200, training loss: 326.316162109375 = 1.7147272825241089 + 50.0 * 6.492028713226318
Epoch 200, val loss: 1.7226182222366333
Epoch 210, training loss: 325.59991455078125 = 1.7006930112838745 + 50.0 * 6.477984428405762
Epoch 210, val loss: 1.709507942199707
Epoch 220, training loss: 324.9892272949219 = 1.6855095624923706 + 50.0 * 6.466073989868164
Epoch 220, val loss: 1.6954091787338257
Epoch 230, training loss: 324.39874267578125 = 1.6693192720413208 + 50.0 * 6.454588890075684
Epoch 230, val loss: 1.6805288791656494
Epoch 240, training loss: 323.9886474609375 = 1.6520178318023682 + 50.0 * 6.446732997894287
Epoch 240, val loss: 1.6647365093231201
Epoch 250, training loss: 323.35333251953125 = 1.633400321006775 + 50.0 * 6.434398651123047
Epoch 250, val loss: 1.6479754447937012
Epoch 260, training loss: 322.9660949707031 = 1.6137820482254028 + 50.0 * 6.427046298980713
Epoch 260, val loss: 1.6303201913833618
Epoch 270, training loss: 322.5279846191406 = 1.593158483505249 + 50.0 * 6.418696403503418
Epoch 270, val loss: 1.611638069152832
Epoch 280, training loss: 322.1343994140625 = 1.5713443756103516 + 50.0 * 6.411261081695557
Epoch 280, val loss: 1.592249870300293
Epoch 290, training loss: 321.9079895019531 = 1.5486325025558472 + 50.0 * 6.407186985015869
Epoch 290, val loss: 1.5719746351242065
Epoch 300, training loss: 321.4667663574219 = 1.5250455141067505 + 50.0 * 6.398834228515625
Epoch 300, val loss: 1.5510518550872803
Epoch 310, training loss: 321.18304443359375 = 1.5007327795028687 + 50.0 * 6.393646240234375
Epoch 310, val loss: 1.5296310186386108
Epoch 320, training loss: 320.8310241699219 = 1.4757496118545532 + 50.0 * 6.387105941772461
Epoch 320, val loss: 1.507761836051941
Epoch 330, training loss: 320.59454345703125 = 1.4503717422485352 + 50.0 * 6.382883071899414
Epoch 330, val loss: 1.4856194257736206
Epoch 340, training loss: 320.2651062011719 = 1.4243249893188477 + 50.0 * 6.3768157958984375
Epoch 340, val loss: 1.4632806777954102
Epoch 350, training loss: 319.97369384765625 = 1.3981685638427734 + 50.0 * 6.3715105056762695
Epoch 350, val loss: 1.4407685995101929
Epoch 360, training loss: 319.7123107910156 = 1.3717008829116821 + 50.0 * 6.366812229156494
Epoch 360, val loss: 1.4180744886398315
Epoch 370, training loss: 319.5125427246094 = 1.3450284004211426 + 50.0 * 6.3633503913879395
Epoch 370, val loss: 1.3954167366027832
Epoch 380, training loss: 319.15679931640625 = 1.318239450454712 + 50.0 * 6.356770992279053
Epoch 380, val loss: 1.3729737997055054
Epoch 390, training loss: 318.92694091796875 = 1.2914721965789795 + 50.0 * 6.3527092933654785
Epoch 390, val loss: 1.3506938219070435
Epoch 400, training loss: 318.7498779296875 = 1.2646939754486084 + 50.0 * 6.349704265594482
Epoch 400, val loss: 1.3283079862594604
Epoch 410, training loss: 318.506103515625 = 1.237970232963562 + 50.0 * 6.345362663269043
Epoch 410, val loss: 1.3063522577285767
Epoch 420, training loss: 318.26409912109375 = 1.211463451385498 + 50.0 * 6.341053009033203
Epoch 420, val loss: 1.2845380306243896
Epoch 430, training loss: 318.3553771972656 = 1.18528151512146 + 50.0 * 6.34340238571167
Epoch 430, val loss: 1.2631033658981323
Epoch 440, training loss: 317.9335021972656 = 1.1590640544891357 + 50.0 * 6.335488796234131
Epoch 440, val loss: 1.2421056032180786
Epoch 450, training loss: 317.71875 = 1.133348822593689 + 50.0 * 6.331707954406738
Epoch 450, val loss: 1.221504807472229
Epoch 460, training loss: 317.58380126953125 = 1.108157753944397 + 50.0 * 6.329513072967529
Epoch 460, val loss: 1.201521635055542
Epoch 470, training loss: 317.44451904296875 = 1.083235740661621 + 50.0 * 6.327226161956787
Epoch 470, val loss: 1.181867003440857
Epoch 480, training loss: 317.22698974609375 = 1.0588874816894531 + 50.0 * 6.323361873626709
Epoch 480, val loss: 1.163050889968872
Epoch 490, training loss: 317.0749206542969 = 1.0351488590240479 + 50.0 * 6.32079553604126
Epoch 490, val loss: 1.144797444343567
Epoch 500, training loss: 317.1558532714844 = 1.0119824409484863 + 50.0 * 6.322876930236816
Epoch 500, val loss: 1.127258539199829
Epoch 510, training loss: 316.84710693359375 = 0.9892751574516296 + 50.0 * 6.3171563148498535
Epoch 510, val loss: 1.1101428270339966
Epoch 520, training loss: 316.7463073730469 = 0.9673397541046143 + 50.0 * 6.315579414367676
Epoch 520, val loss: 1.093640685081482
Epoch 530, training loss: 316.5622863769531 = 0.9456547498703003 + 50.0 * 6.312332630157471
Epoch 530, val loss: 1.0781707763671875
Epoch 540, training loss: 316.41015625 = 0.9248564839363098 + 50.0 * 6.30970573425293
Epoch 540, val loss: 1.0628821849822998
Epoch 550, training loss: 316.2655334472656 = 0.9044377207756042 + 50.0 * 6.30722188949585
Epoch 550, val loss: 1.0485183000564575
Epoch 560, training loss: 316.3272705078125 = 0.8846734762191772 + 50.0 * 6.308851718902588
Epoch 560, val loss: 1.0345300436019897
Epoch 570, training loss: 316.23779296875 = 0.8651378154754639 + 50.0 * 6.307453155517578
Epoch 570, val loss: 1.021057367324829
Epoch 580, training loss: 316.00634765625 = 0.8459897637367249 + 50.0 * 6.3032073974609375
Epoch 580, val loss: 1.0080292224884033
Epoch 590, training loss: 315.8315734863281 = 0.8273263573646545 + 50.0 * 6.300084590911865
Epoch 590, val loss: 0.9955469369888306
Epoch 600, training loss: 315.7041320800781 = 0.809127151966095 + 50.0 * 6.297900676727295
Epoch 600, val loss: 0.9834423065185547
Epoch 610, training loss: 315.75201416015625 = 0.7912511825561523 + 50.0 * 6.299215316772461
Epoch 610, val loss: 0.9717305302619934
Epoch 620, training loss: 315.7351989746094 = 0.773262619972229 + 50.0 * 6.299239158630371
Epoch 620, val loss: 0.9602597951889038
Epoch 630, training loss: 315.4426574707031 = 0.7557355165481567 + 50.0 * 6.29373836517334
Epoch 630, val loss: 0.9491493701934814
Epoch 640, training loss: 315.34478759765625 = 0.7383387088775635 + 50.0 * 6.292129039764404
Epoch 640, val loss: 0.9384393095970154
Epoch 650, training loss: 315.39501953125 = 0.7210991382598877 + 50.0 * 6.293478488922119
Epoch 650, val loss: 0.9281197190284729
Epoch 660, training loss: 315.24456787109375 = 0.7040889263153076 + 50.0 * 6.290809154510498
Epoch 660, val loss: 0.9176244139671326
Epoch 670, training loss: 315.0618591308594 = 0.687013566493988 + 50.0 * 6.287497043609619
Epoch 670, val loss: 0.9078068733215332
Epoch 680, training loss: 314.98193359375 = 0.6702761650085449 + 50.0 * 6.286232948303223
Epoch 680, val loss: 0.8981797099113464
Epoch 690, training loss: 315.0023498535156 = 0.6535798907279968 + 50.0 * 6.286974906921387
Epoch 690, val loss: 0.8887994885444641
Epoch 700, training loss: 314.9466552734375 = 0.6368888020515442 + 50.0 * 6.286194801330566
Epoch 700, val loss: 0.8796902298927307
Epoch 710, training loss: 314.8354797363281 = 0.6205151081085205 + 50.0 * 6.284299373626709
Epoch 710, val loss: 0.8707283139228821
Epoch 720, training loss: 314.6405029296875 = 0.6040444374084473 + 50.0 * 6.280729293823242
Epoch 720, val loss: 0.86236572265625
Epoch 730, training loss: 314.5384216308594 = 0.5880258679389954 + 50.0 * 6.279008388519287
Epoch 730, val loss: 0.8542455434799194
Epoch 740, training loss: 314.68695068359375 = 0.5722201466560364 + 50.0 * 6.282294273376465
Epoch 740, val loss: 0.8464782238006592
Epoch 750, training loss: 314.72637939453125 = 0.5562366247177124 + 50.0 * 6.283402919769287
Epoch 750, val loss: 0.8391270637512207
Epoch 760, training loss: 314.4024963378906 = 0.5407212972640991 + 50.0 * 6.277235507965088
Epoch 760, val loss: 0.8320305347442627
Epoch 770, training loss: 314.2483825683594 = 0.5256178379058838 + 50.0 * 6.274455547332764
Epoch 770, val loss: 0.8254616856575012
Epoch 780, training loss: 314.1662902832031 = 0.510867178440094 + 50.0 * 6.27310848236084
Epoch 780, val loss: 0.8193901181221008
Epoch 790, training loss: 314.57696533203125 = 0.49641352891921997 + 50.0 * 6.281611442565918
Epoch 790, val loss: 0.8136059641838074
Epoch 800, training loss: 314.1835021972656 = 0.4819435775279999 + 50.0 * 6.274031639099121
Epoch 800, val loss: 0.8082297444343567
Epoch 810, training loss: 314.0345764160156 = 0.46806132793426514 + 50.0 * 6.271330833435059
Epoch 810, val loss: 0.8033482432365417
Epoch 820, training loss: 313.9070739746094 = 0.45457226037979126 + 50.0 * 6.269050121307373
Epoch 820, val loss: 0.7989267110824585
Epoch 830, training loss: 313.9339599609375 = 0.4414801001548767 + 50.0 * 6.26984977722168
Epoch 830, val loss: 0.7949118614196777
Epoch 840, training loss: 313.78656005859375 = 0.4286465048789978 + 50.0 * 6.267158031463623
Epoch 840, val loss: 0.7911139726638794
Epoch 850, training loss: 313.7295837402344 = 0.4161544740200043 + 50.0 * 6.266268253326416
Epoch 850, val loss: 0.7878206372261047
Epoch 860, training loss: 313.8309326171875 = 0.40402328968048096 + 50.0 * 6.268538475036621
Epoch 860, val loss: 0.7849266529083252
Epoch 870, training loss: 313.7948913574219 = 0.39226233959198 + 50.0 * 6.268052577972412
Epoch 870, val loss: 0.7821675539016724
Epoch 880, training loss: 313.6187438964844 = 0.38083842396736145 + 50.0 * 6.264758110046387
Epoch 880, val loss: 0.7797397375106812
Epoch 890, training loss: 313.49810791015625 = 0.369846373796463 + 50.0 * 6.2625651359558105
Epoch 890, val loss: 0.7778692841529846
Epoch 900, training loss: 313.4716796875 = 0.359239786863327 + 50.0 * 6.262248516082764
Epoch 900, val loss: 0.7761774063110352
Epoch 910, training loss: 313.61944580078125 = 0.34892788529396057 + 50.0 * 6.26540994644165
Epoch 910, val loss: 0.7747715711593628
Epoch 920, training loss: 313.3808898925781 = 0.33878347277641296 + 50.0 * 6.260842323303223
Epoch 920, val loss: 0.7736338973045349
Epoch 930, training loss: 313.2986755371094 = 0.3290814459323883 + 50.0 * 6.259391784667969
Epoch 930, val loss: 0.7728989124298096
Epoch 940, training loss: 313.2737731933594 = 0.3197309672832489 + 50.0 * 6.25908088684082
Epoch 940, val loss: 0.7724059224128723
Epoch 950, training loss: 313.36834716796875 = 0.310684472322464 + 50.0 * 6.261153221130371
Epoch 950, val loss: 0.772190511226654
Epoch 960, training loss: 313.3328857421875 = 0.30188801884651184 + 50.0 * 6.2606201171875
Epoch 960, val loss: 0.7718685865402222
Epoch 970, training loss: 313.3063659667969 = 0.2933824062347412 + 50.0 * 6.260259628295898
Epoch 970, val loss: 0.7719506025314331
Epoch 980, training loss: 313.0528564453125 = 0.28511422872543335 + 50.0 * 6.255354404449463
Epoch 980, val loss: 0.7724292278289795
Epoch 990, training loss: 313.0489196777344 = 0.27718183398246765 + 50.0 * 6.255434513092041
Epoch 990, val loss: 0.7730119228363037
Epoch 1000, training loss: 313.3393249511719 = 0.26947087049484253 + 50.0 * 6.261396884918213
Epoch 1000, val loss: 0.7737839221954346
Epoch 1010, training loss: 312.9449768066406 = 0.26213234663009644 + 50.0 * 6.253656387329102
Epoch 1010, val loss: 0.7746567726135254
Epoch 1020, training loss: 312.9109802246094 = 0.254992812871933 + 50.0 * 6.253119468688965
Epoch 1020, val loss: 0.7757107019424438
Epoch 1030, training loss: 312.9227294921875 = 0.24808497726917267 + 50.0 * 6.253492832183838
Epoch 1030, val loss: 0.7770761251449585
Epoch 1040, training loss: 312.8720397949219 = 0.24143460392951965 + 50.0 * 6.252612113952637
Epoch 1040, val loss: 0.7786310315132141
Epoch 1050, training loss: 312.8681945800781 = 0.2349763959646225 + 50.0 * 6.252664089202881
Epoch 1050, val loss: 0.7802760004997253
Epoch 1060, training loss: 312.7779846191406 = 0.2286994755268097 + 50.0 * 6.250985622406006
Epoch 1060, val loss: 0.7818813920021057
Epoch 1070, training loss: 312.72650146484375 = 0.22269663214683533 + 50.0 * 6.2500762939453125
Epoch 1070, val loss: 0.7837772369384766
Epoch 1080, training loss: 312.6627197265625 = 0.21688136458396912 + 50.0 * 6.2489166259765625
Epoch 1080, val loss: 0.785986065864563
Epoch 1090, training loss: 313.1053466796875 = 0.2112588733434677 + 50.0 * 6.2578816413879395
Epoch 1090, val loss: 0.7883256077766418
Epoch 1100, training loss: 312.69976806640625 = 0.20581237971782684 + 50.0 * 6.249878883361816
Epoch 1100, val loss: 0.7901004552841187
Epoch 1110, training loss: 312.59356689453125 = 0.20053526759147644 + 50.0 * 6.247860431671143
Epoch 1110, val loss: 0.7923135161399841
Epoch 1120, training loss: 312.5083923339844 = 0.19547386467456818 + 50.0 * 6.246258735656738
Epoch 1120, val loss: 0.7949054837226868
Epoch 1130, training loss: 312.54833984375 = 0.19061574339866638 + 50.0 * 6.247154712677002
Epoch 1130, val loss: 0.7974811792373657
Epoch 1140, training loss: 312.5436706542969 = 0.18586008250713348 + 50.0 * 6.247156620025635
Epoch 1140, val loss: 0.7999653816223145
Epoch 1150, training loss: 312.42755126953125 = 0.18120001256465912 + 50.0 * 6.244926452636719
Epoch 1150, val loss: 0.802642285823822
Epoch 1160, training loss: 312.401611328125 = 0.17675819993019104 + 50.0 * 6.244497299194336
Epoch 1160, val loss: 0.8053367733955383
Epoch 1170, training loss: 312.5396728515625 = 0.17249153554439545 + 50.0 * 6.24734354019165
Epoch 1170, val loss: 0.8079756498336792
Epoch 1180, training loss: 312.7698059082031 = 0.16819298267364502 + 50.0 * 6.252032279968262
Epoch 1180, val loss: 0.8107686042785645
Epoch 1190, training loss: 312.4050598144531 = 0.16418150067329407 + 50.0 * 6.244817733764648
Epoch 1190, val loss: 0.8138503432273865
Epoch 1200, training loss: 312.2593994140625 = 0.16021491587162018 + 50.0 * 6.241983890533447
Epoch 1200, val loss: 0.8168379664421082
Epoch 1210, training loss: 312.2315673828125 = 0.15641921758651733 + 50.0 * 6.24150276184082
Epoch 1210, val loss: 0.8199383020401001
Epoch 1220, training loss: 312.2345275878906 = 0.15277594327926636 + 50.0 * 6.241634845733643
Epoch 1220, val loss: 0.823101818561554
Epoch 1230, training loss: 312.3332214355469 = 0.14916680753231049 + 50.0 * 6.243680953979492
Epoch 1230, val loss: 0.8261123299598694
Epoch 1240, training loss: 312.18023681640625 = 0.14565034210681915 + 50.0 * 6.240691661834717
Epoch 1240, val loss: 0.8291720151901245
Epoch 1250, training loss: 312.165283203125 = 0.1422753930091858 + 50.0 * 6.24045991897583
Epoch 1250, val loss: 0.8325332999229431
Epoch 1260, training loss: 312.2293395996094 = 0.1390199363231659 + 50.0 * 6.241806507110596
Epoch 1260, val loss: 0.8359877467155457
Epoch 1270, training loss: 312.2425231933594 = 0.1358402669429779 + 50.0 * 6.242134094238281
Epoch 1270, val loss: 0.8392751216888428
Epoch 1280, training loss: 312.1044921875 = 0.1327284872531891 + 50.0 * 6.23943567276001
Epoch 1280, val loss: 0.842078447341919
Epoch 1290, training loss: 312.1045227050781 = 0.12973061203956604 + 50.0 * 6.239495754241943
Epoch 1290, val loss: 0.8458521366119385
Epoch 1300, training loss: 312.01690673828125 = 0.126797154545784 + 50.0 * 6.237802028656006
Epoch 1300, val loss: 0.8490904569625854
Epoch 1310, training loss: 312.12255859375 = 0.12397617846727371 + 50.0 * 6.23997163772583
Epoch 1310, val loss: 0.8524540662765503
Epoch 1320, training loss: 311.96893310546875 = 0.12120075523853302 + 50.0 * 6.236954212188721
Epoch 1320, val loss: 0.8558968901634216
Epoch 1330, training loss: 311.9634094238281 = 0.11849997937679291 + 50.0 * 6.236898422241211
Epoch 1330, val loss: 0.8592425584793091
Epoch 1340, training loss: 311.8870849609375 = 0.11590704321861267 + 50.0 * 6.235423564910889
Epoch 1340, val loss: 0.8628247380256653
Epoch 1350, training loss: 311.92279052734375 = 0.11339591443538666 + 50.0 * 6.236187934875488
Epoch 1350, val loss: 0.8663227558135986
Epoch 1360, training loss: 311.9330749511719 = 0.11090107262134552 + 50.0 * 6.236443519592285
Epoch 1360, val loss: 0.8698626756668091
Epoch 1370, training loss: 311.8574523925781 = 0.10846096277236938 + 50.0 * 6.23498010635376
Epoch 1370, val loss: 0.8731528520584106
Epoch 1380, training loss: 311.8103942871094 = 0.1061328649520874 + 50.0 * 6.2340850830078125
Epoch 1380, val loss: 0.8766738772392273
Epoch 1390, training loss: 311.779296875 = 0.10388678312301636 + 50.0 * 6.233508110046387
Epoch 1390, val loss: 0.8800565600395203
Epoch 1400, training loss: 312.0762023925781 = 0.10172625631093979 + 50.0 * 6.239490032196045
Epoch 1400, val loss: 0.8833328485488892
Epoch 1410, training loss: 311.8968811035156 = 0.09949978440999985 + 50.0 * 6.235948085784912
Epoch 1410, val loss: 0.8875060677528381
Epoch 1420, training loss: 311.9145202636719 = 0.09741194546222687 + 50.0 * 6.236342430114746
Epoch 1420, val loss: 0.890665590763092
Epoch 1430, training loss: 311.7197265625 = 0.09530512243509293 + 50.0 * 6.232488632202148
Epoch 1430, val loss: 0.8943008780479431
Epoch 1440, training loss: 311.6680603027344 = 0.09333215653896332 + 50.0 * 6.231494903564453
Epoch 1440, val loss: 0.8978165984153748
Epoch 1450, training loss: 311.61944580078125 = 0.09140747040510178 + 50.0 * 6.230560779571533
Epoch 1450, val loss: 0.901391327381134
Epoch 1460, training loss: 311.6548767089844 = 0.08953969180583954 + 50.0 * 6.231306552886963
Epoch 1460, val loss: 0.9050625562667847
Epoch 1470, training loss: 311.7150573730469 = 0.08769366145133972 + 50.0 * 6.232547283172607
Epoch 1470, val loss: 0.9086443185806274
Epoch 1480, training loss: 311.7423095703125 = 0.08587466180324554 + 50.0 * 6.233128547668457
Epoch 1480, val loss: 0.9122903943061829
Epoch 1490, training loss: 311.6349792480469 = 0.08407457917928696 + 50.0 * 6.23101806640625
Epoch 1490, val loss: 0.9156650304794312
Epoch 1500, training loss: 311.5472106933594 = 0.08237014710903168 + 50.0 * 6.229296684265137
Epoch 1500, val loss: 0.9194288849830627
Epoch 1510, training loss: 311.6180419921875 = 0.08071430027484894 + 50.0 * 6.230746269226074
Epoch 1510, val loss: 0.9229719042778015
Epoch 1520, training loss: 311.5989990234375 = 0.07908434420824051 + 50.0 * 6.230398654937744
Epoch 1520, val loss: 0.926493227481842
Epoch 1530, training loss: 311.53692626953125 = 0.07747423648834229 + 50.0 * 6.229188919067383
Epoch 1530, val loss: 0.929797887802124
Epoch 1540, training loss: 311.49554443359375 = 0.07590550184249878 + 50.0 * 6.228393077850342
Epoch 1540, val loss: 0.9334340691566467
Epoch 1550, training loss: 311.45916748046875 = 0.07440850138664246 + 50.0 * 6.227695465087891
Epoch 1550, val loss: 0.9370983242988586
Epoch 1560, training loss: 311.55419921875 = 0.0729571282863617 + 50.0 * 6.2296247482299805
Epoch 1560, val loss: 0.9407231211662292
Epoch 1570, training loss: 311.45318603515625 = 0.07150430232286453 + 50.0 * 6.227633953094482
Epoch 1570, val loss: 0.9442113637924194
Epoch 1580, training loss: 311.46484375 = 0.07008121907711029 + 50.0 * 6.227895259857178
Epoch 1580, val loss: 0.9476686120033264
Epoch 1590, training loss: 311.5126037597656 = 0.06872214376926422 + 50.0 * 6.228877544403076
Epoch 1590, val loss: 0.9512636661529541
Epoch 1600, training loss: 311.489990234375 = 0.06738020479679108 + 50.0 * 6.228452205657959
Epoch 1600, val loss: 0.9551953077316284
Epoch 1610, training loss: 311.41265869140625 = 0.06607615202665329 + 50.0 * 6.226931571960449
Epoch 1610, val loss: 0.9584519863128662
Epoch 1620, training loss: 311.33880615234375 = 0.0648072361946106 + 50.0 * 6.225479602813721
Epoch 1620, val loss: 0.9621222615242004
Epoch 1630, training loss: 311.33416748046875 = 0.06357559561729431 + 50.0 * 6.225411891937256
Epoch 1630, val loss: 0.9655364155769348
Epoch 1640, training loss: 311.471435546875 = 0.062376074492931366 + 50.0 * 6.228180885314941
Epoch 1640, val loss: 0.9689256548881531
Epoch 1650, training loss: 311.31915283203125 = 0.06114509329199791 + 50.0 * 6.225160121917725
Epoch 1650, val loss: 0.972710371017456
Epoch 1660, training loss: 311.2484130859375 = 0.06000887602567673 + 50.0 * 6.22376823425293
Epoch 1660, val loss: 0.9761970639228821
Epoch 1670, training loss: 311.3272705078125 = 0.05887635052204132 + 50.0 * 6.225368022918701
Epoch 1670, val loss: 0.9796662330627441
Epoch 1680, training loss: 311.34124755859375 = 0.05777997523546219 + 50.0 * 6.2256693840026855
Epoch 1680, val loss: 0.9831574559211731
Epoch 1690, training loss: 311.20257568359375 = 0.05668296292424202 + 50.0 * 6.2229180335998535
Epoch 1690, val loss: 0.9865567684173584
Epoch 1700, training loss: 311.1797790527344 = 0.055638838559389114 + 50.0 * 6.222483158111572
Epoch 1700, val loss: 0.9905850887298584
Epoch 1710, training loss: 311.1519470214844 = 0.054612137377262115 + 50.0 * 6.2219462394714355
Epoch 1710, val loss: 0.993945300579071
Epoch 1720, training loss: 311.4595031738281 = 0.05363406240940094 + 50.0 * 6.2281174659729
Epoch 1720, val loss: 0.9979321360588074
Epoch 1730, training loss: 311.3971862792969 = 0.05263838171958923 + 50.0 * 6.226891040802002
Epoch 1730, val loss: 1.0005830526351929
Epoch 1740, training loss: 311.1932067871094 = 0.05166541412472725 + 50.0 * 6.222830772399902
Epoch 1740, val loss: 1.0043989419937134
Epoch 1750, training loss: 311.0928649902344 = 0.05072776973247528 + 50.0 * 6.2208428382873535
Epoch 1750, val loss: 1.0080163478851318
Epoch 1760, training loss: 311.1476135253906 = 0.049831800162792206 + 50.0 * 6.221955299377441
Epoch 1760, val loss: 1.0113661289215088
Epoch 1770, training loss: 311.2093811035156 = 0.04894901439547539 + 50.0 * 6.223208427429199
Epoch 1770, val loss: 1.0149400234222412
Epoch 1780, training loss: 311.1955261230469 = 0.04805738851428032 + 50.0 * 6.222949504852295
Epoch 1780, val loss: 1.0183255672454834
Epoch 1790, training loss: 311.1492919921875 = 0.04720950126647949 + 50.0 * 6.222041606903076
Epoch 1790, val loss: 1.0219447612762451
Epoch 1800, training loss: 311.0284729003906 = 0.0463700033724308 + 50.0 * 6.219642639160156
Epoch 1800, val loss: 1.025552749633789
Epoch 1810, training loss: 311.0070495605469 = 0.04555952921509743 + 50.0 * 6.219229698181152
Epoch 1810, val loss: 1.0290638208389282
Epoch 1820, training loss: 311.06298828125 = 0.04478094354271889 + 50.0 * 6.220364570617676
Epoch 1820, val loss: 1.0323536396026611
Epoch 1830, training loss: 311.25274658203125 = 0.044000525027513504 + 50.0 * 6.224174499511719
Epoch 1830, val loss: 1.035737156867981
Epoch 1840, training loss: 311.1031799316406 = 0.043228235095739365 + 50.0 * 6.221198558807373
Epoch 1840, val loss: 1.0394984483718872
Epoch 1850, training loss: 310.9833068847656 = 0.04247743636369705 + 50.0 * 6.21881628036499
Epoch 1850, val loss: 1.042736530303955
Epoch 1860, training loss: 310.9476623535156 = 0.041757334023714066 + 50.0 * 6.218118190765381
Epoch 1860, val loss: 1.0464197397232056
Epoch 1870, training loss: 311.0775146484375 = 0.041069693863391876 + 50.0 * 6.220728874206543
Epoch 1870, val loss: 1.0497320890426636
Epoch 1880, training loss: 311.1558837890625 = 0.040371205657720566 + 50.0 * 6.2223100662231445
Epoch 1880, val loss: 1.0528193712234497
Epoch 1890, training loss: 310.980224609375 = 0.039663080126047134 + 50.0 * 6.21881103515625
Epoch 1890, val loss: 1.0565210580825806
Epoch 1900, training loss: 310.9268493652344 = 0.03900407999753952 + 50.0 * 6.217757225036621
Epoch 1900, val loss: 1.0599610805511475
Epoch 1910, training loss: 310.9407043457031 = 0.038365717977285385 + 50.0 * 6.21804666519165
Epoch 1910, val loss: 1.063589334487915
Epoch 1920, training loss: 311.0812072753906 = 0.03774061053991318 + 50.0 * 6.220869064331055
Epoch 1920, val loss: 1.0668325424194336
Epoch 1930, training loss: 310.92083740234375 = 0.037108372896909714 + 50.0 * 6.217674732208252
Epoch 1930, val loss: 1.0697736740112305
Epoch 1940, training loss: 310.8678894042969 = 0.036506667733192444 + 50.0 * 6.216628074645996
Epoch 1940, val loss: 1.0733399391174316
Epoch 1950, training loss: 311.0243835449219 = 0.03592834249138832 + 50.0 * 6.21976900100708
Epoch 1950, val loss: 1.0764309167861938
Epoch 1960, training loss: 310.88470458984375 = 0.03533921763300896 + 50.0 * 6.216987133026123
Epoch 1960, val loss: 1.0799199342727661
Epoch 1970, training loss: 310.8836364746094 = 0.034761738032102585 + 50.0 * 6.216977596282959
Epoch 1970, val loss: 1.0834277868270874
Epoch 1980, training loss: 310.8814697265625 = 0.0342060886323452 + 50.0 * 6.216945171356201
Epoch 1980, val loss: 1.0866286754608154
Epoch 1990, training loss: 310.909912109375 = 0.03365610912442207 + 50.0 * 6.217525005340576
Epoch 1990, val loss: 1.090201735496521
Epoch 2000, training loss: 310.8750305175781 = 0.03312202915549278 + 50.0 * 6.216838359832764
Epoch 2000, val loss: 1.0931291580200195
Epoch 2010, training loss: 310.8127746582031 = 0.03260648995637894 + 50.0 * 6.215603828430176
Epoch 2010, val loss: 1.096433162689209
Epoch 2020, training loss: 310.9088439941406 = 0.03210233896970749 + 50.0 * 6.21753454208374
Epoch 2020, val loss: 1.0997592210769653
Epoch 2030, training loss: 310.7855224609375 = 0.03158173710107803 + 50.0 * 6.215078830718994
Epoch 2030, val loss: 1.1032097339630127
Epoch 2040, training loss: 310.77581787109375 = 0.031102608889341354 + 50.0 * 6.2148942947387695
Epoch 2040, val loss: 1.1065267324447632
Epoch 2050, training loss: 310.8988342285156 = 0.030629601329565048 + 50.0 * 6.2173638343811035
Epoch 2050, val loss: 1.1098318099975586
Epoch 2060, training loss: 310.8670959472656 = 0.030144942924380302 + 50.0 * 6.216739177703857
Epoch 2060, val loss: 1.1125695705413818
Epoch 2070, training loss: 310.7666015625 = 0.029680710285902023 + 50.0 * 6.214738368988037
Epoch 2070, val loss: 1.1157103776931763
Epoch 2080, training loss: 310.7283630371094 = 0.02922792173922062 + 50.0 * 6.213982582092285
Epoch 2080, val loss: 1.1191649436950684
Epoch 2090, training loss: 310.7987365722656 = 0.028793804347515106 + 50.0 * 6.215398788452148
Epoch 2090, val loss: 1.122126579284668
Epoch 2100, training loss: 310.74615478515625 = 0.028356702998280525 + 50.0 * 6.214355945587158
Epoch 2100, val loss: 1.1253024339675903
Epoch 2110, training loss: 310.7324523925781 = 0.027935536578297615 + 50.0 * 6.214090824127197
Epoch 2110, val loss: 1.1285605430603027
Epoch 2120, training loss: 310.99908447265625 = 0.027520373463630676 + 50.0 * 6.219430923461914
Epoch 2120, val loss: 1.1314605474472046
Epoch 2130, training loss: 310.7518310546875 = 0.027104822918772697 + 50.0 * 6.214494705200195
Epoch 2130, val loss: 1.1349825859069824
Epoch 2140, training loss: 310.6806640625 = 0.02670508809387684 + 50.0 * 6.213079452514648
Epoch 2140, val loss: 1.1380351781845093
Epoch 2150, training loss: 310.8258972167969 = 0.02632969804108143 + 50.0 * 6.215991020202637
Epoch 2150, val loss: 1.1409879922866821
Epoch 2160, training loss: 310.66656494140625 = 0.025931360200047493 + 50.0 * 6.212812423706055
Epoch 2160, val loss: 1.1442782878875732
Epoch 2170, training loss: 310.6645202636719 = 0.025550546124577522 + 50.0 * 6.212779521942139
Epoch 2170, val loss: 1.1474674940109253
Epoch 2180, training loss: 310.616455078125 = 0.02518990822136402 + 50.0 * 6.211825847625732
Epoch 2180, val loss: 1.1504896879196167
Epoch 2190, training loss: 310.6493225097656 = 0.024832598865032196 + 50.0 * 6.212489604949951
Epoch 2190, val loss: 1.1538020372390747
Epoch 2200, training loss: 310.7972412109375 = 0.024476198479533195 + 50.0 * 6.215455055236816
Epoch 2200, val loss: 1.1566674709320068
Epoch 2210, training loss: 310.6643371582031 = 0.02413453906774521 + 50.0 * 6.212803840637207
Epoch 2210, val loss: 1.1597923040390015
Epoch 2220, training loss: 310.70709228515625 = 0.023794928565621376 + 50.0 * 6.213665962219238
Epoch 2220, val loss: 1.1630934476852417
Epoch 2230, training loss: 310.6650390625 = 0.023460768163204193 + 50.0 * 6.212831497192383
Epoch 2230, val loss: 1.165844440460205
Epoch 2240, training loss: 310.6127014160156 = 0.023121630772948265 + 50.0 * 6.211791515350342
Epoch 2240, val loss: 1.1683430671691895
Epoch 2250, training loss: 310.543701171875 = 0.02280607260763645 + 50.0 * 6.210418224334717
Epoch 2250, val loss: 1.172012209892273
Epoch 2260, training loss: 310.5171203613281 = 0.022497141733765602 + 50.0 * 6.209892272949219
Epoch 2260, val loss: 1.174783706665039
Epoch 2270, training loss: 310.6658935546875 = 0.022201303392648697 + 50.0 * 6.212874412536621
Epoch 2270, val loss: 1.1775456666946411
Epoch 2280, training loss: 310.6400451660156 = 0.02189779467880726 + 50.0 * 6.212362766265869
Epoch 2280, val loss: 1.1807833909988403
Epoch 2290, training loss: 310.525634765625 = 0.02158419042825699 + 50.0 * 6.210081100463867
Epoch 2290, val loss: 1.1833453178405762
Epoch 2300, training loss: 310.5227355957031 = 0.02129359543323517 + 50.0 * 6.210028648376465
Epoch 2300, val loss: 1.1866334676742554
Epoch 2310, training loss: 310.4923095703125 = 0.02101304940879345 + 50.0 * 6.209425449371338
Epoch 2310, val loss: 1.189501166343689
Epoch 2320, training loss: 310.8478088378906 = 0.02075273171067238 + 50.0 * 6.216541290283203
Epoch 2320, val loss: 1.192097544670105
Epoch 2330, training loss: 310.7481994628906 = 0.020461080595850945 + 50.0 * 6.214555263519287
Epoch 2330, val loss: 1.1954126358032227
Epoch 2340, training loss: 310.56732177734375 = 0.020184999331831932 + 50.0 * 6.210943222045898
Epoch 2340, val loss: 1.1980382204055786
Epoch 2350, training loss: 310.4554748535156 = 0.019915159791707993 + 50.0 * 6.208710670471191
Epoch 2350, val loss: 1.2009782791137695
Epoch 2360, training loss: 310.4446716308594 = 0.019661210477352142 + 50.0 * 6.208500385284424
Epoch 2360, val loss: 1.2039002180099487
Epoch 2370, training loss: 310.5014343261719 = 0.019414005801081657 + 50.0 * 6.2096405029296875
Epoch 2370, val loss: 1.2065671682357788
Epoch 2380, training loss: 310.6059265136719 = 0.019169950857758522 + 50.0 * 6.211735248565674
Epoch 2380, val loss: 1.2093790769577026
Epoch 2390, training loss: 310.60870361328125 = 0.01892700605094433 + 50.0 * 6.211795806884766
Epoch 2390, val loss: 1.2124260663986206
Epoch 2400, training loss: 310.6097412109375 = 0.01867550052702427 + 50.0 * 6.211821556091309
Epoch 2400, val loss: 1.2147557735443115
Epoch 2410, training loss: 310.49658203125 = 0.018434418365359306 + 50.0 * 6.2095627784729
Epoch 2410, val loss: 1.2180060148239136
Epoch 2420, training loss: 310.45404052734375 = 0.018201561644673347 + 50.0 * 6.208717346191406
Epoch 2420, val loss: 1.220636248588562
Epoch 2430, training loss: 310.40704345703125 = 0.01798090711236 + 50.0 * 6.2077813148498535
Epoch 2430, val loss: 1.223480463027954
Epoch 2440, training loss: 310.3915100097656 = 0.01776035875082016 + 50.0 * 6.207474708557129
Epoch 2440, val loss: 1.226306438446045
Epoch 2450, training loss: 310.6822814941406 = 0.017549507319927216 + 50.0 * 6.213294506072998
Epoch 2450, val loss: 1.228890299797058
Epoch 2460, training loss: 310.5396728515625 = 0.017330210655927658 + 50.0 * 6.210446834564209
Epoch 2460, val loss: 1.231608271598816
Epoch 2470, training loss: 310.4305419921875 = 0.017106758430600166 + 50.0 * 6.208268165588379
Epoch 2470, val loss: 1.2343168258666992
Epoch 2480, training loss: 310.4423522949219 = 0.01690191589295864 + 50.0 * 6.2085089683532715
Epoch 2480, val loss: 1.237045168876648
Epoch 2490, training loss: 310.46136474609375 = 0.016695577651262283 + 50.0 * 6.208893299102783
Epoch 2490, val loss: 1.2396527528762817
Epoch 2500, training loss: 310.367431640625 = 0.01649479754269123 + 50.0 * 6.2070183753967285
Epoch 2500, val loss: 1.242192029953003
Epoch 2510, training loss: 310.4164123535156 = 0.016306279227137566 + 50.0 * 6.208002090454102
Epoch 2510, val loss: 1.2443974018096924
Epoch 2520, training loss: 310.3427734375 = 0.016108740121126175 + 50.0 * 6.206533432006836
Epoch 2520, val loss: 1.2474957704544067
Epoch 2530, training loss: 310.38677978515625 = 0.01592457853257656 + 50.0 * 6.207417011260986
Epoch 2530, val loss: 1.249919056892395
Epoch 2540, training loss: 310.4373474121094 = 0.015738310292363167 + 50.0 * 6.208431720733643
Epoch 2540, val loss: 1.2527331113815308
Epoch 2550, training loss: 310.5367431640625 = 0.015547041781246662 + 50.0 * 6.210423946380615
Epoch 2550, val loss: 1.2555721998214722
Epoch 2560, training loss: 310.327880859375 = 0.015368050895631313 + 50.0 * 6.206250190734863
Epoch 2560, val loss: 1.2575829029083252
Epoch 2570, training loss: 310.281005859375 = 0.015187743119895458 + 50.0 * 6.205316066741943
Epoch 2570, val loss: 1.2603771686553955
Epoch 2580, training loss: 310.3169250488281 = 0.015017477795481682 + 50.0 * 6.206038475036621
Epoch 2580, val loss: 1.2630161046981812
Epoch 2590, training loss: 310.4622802734375 = 0.014855485409498215 + 50.0 * 6.208948135375977
Epoch 2590, val loss: 1.2656011581420898
Epoch 2600, training loss: 310.49273681640625 = 0.014684268273413181 + 50.0 * 6.209561347961426
Epoch 2600, val loss: 1.2680659294128418
Epoch 2610, training loss: 310.3247375488281 = 0.01451305951923132 + 50.0 * 6.206204414367676
Epoch 2610, val loss: 1.2700393199920654
Epoch 2620, training loss: 310.28558349609375 = 0.014350990764796734 + 50.0 * 6.205424785614014
Epoch 2620, val loss: 1.272628664970398
Epoch 2630, training loss: 310.2804260253906 = 0.014195568859577179 + 50.0 * 6.205324649810791
Epoch 2630, val loss: 1.2751119136810303
Epoch 2640, training loss: 310.44696044921875 = 0.014044090174138546 + 50.0 * 6.208658218383789
Epoch 2640, val loss: 1.2772653102874756
Epoch 2650, training loss: 310.24505615234375 = 0.013886453583836555 + 50.0 * 6.204623699188232
Epoch 2650, val loss: 1.2802001237869263
Epoch 2660, training loss: 310.3065490722656 = 0.013736680150032043 + 50.0 * 6.2058563232421875
Epoch 2660, val loss: 1.2825400829315186
Epoch 2670, training loss: 310.4015197753906 = 0.013587604276835918 + 50.0 * 6.207758903503418
Epoch 2670, val loss: 1.284926176071167
Epoch 2680, training loss: 310.2997741699219 = 0.013435372151434422 + 50.0 * 6.2057271003723145
Epoch 2680, val loss: 1.2876096963882446
Epoch 2690, training loss: 310.24078369140625 = 0.013294460251927376 + 50.0 * 6.204549312591553
Epoch 2690, val loss: 1.2898924350738525
Epoch 2700, training loss: 310.37451171875 = 0.013162442483007908 + 50.0 * 6.2072272300720215
Epoch 2700, val loss: 1.2924792766571045
Epoch 2710, training loss: 310.2657470703125 = 0.01301552727818489 + 50.0 * 6.205054759979248
Epoch 2710, val loss: 1.294351577758789
Epoch 2720, training loss: 310.23822021484375 = 0.012879342772066593 + 50.0 * 6.204506874084473
Epoch 2720, val loss: 1.296686053276062
Epoch 2730, training loss: 310.28228759765625 = 0.012746108695864677 + 50.0 * 6.205390453338623
Epoch 2730, val loss: 1.299499273300171
Epoch 2740, training loss: 310.230224609375 = 0.012610835023224354 + 50.0 * 6.204352378845215
Epoch 2740, val loss: 1.3016724586486816
Epoch 2750, training loss: 310.2806701660156 = 0.012481852434575558 + 50.0 * 6.205363750457764
Epoch 2750, val loss: 1.3041383028030396
Epoch 2760, training loss: 310.20855712890625 = 0.012352321296930313 + 50.0 * 6.203924179077148
Epoch 2760, val loss: 1.3063395023345947
Epoch 2770, training loss: 310.2630310058594 = 0.01222505047917366 + 50.0 * 6.205016136169434
Epoch 2770, val loss: 1.3084330558776855
Epoch 2780, training loss: 310.2470397949219 = 0.012102274224162102 + 50.0 * 6.20469856262207
Epoch 2780, val loss: 1.3106756210327148
Epoch 2790, training loss: 310.3638000488281 = 0.01198409404605627 + 50.0 * 6.207036018371582
Epoch 2790, val loss: 1.3128807544708252
Epoch 2800, training loss: 310.1866760253906 = 0.011854764074087143 + 50.0 * 6.203496932983398
Epoch 2800, val loss: 1.3151029348373413
Epoch 2810, training loss: 310.1432189941406 = 0.011737608350813389 + 50.0 * 6.202629566192627
Epoch 2810, val loss: 1.3173449039459229
Epoch 2820, training loss: 310.1245422363281 = 0.011623947881162167 + 50.0 * 6.202258110046387
Epoch 2820, val loss: 1.3196467161178589
Epoch 2830, training loss: 310.196044921875 = 0.011514916084706783 + 50.0 * 6.203690528869629
Epoch 2830, val loss: 1.3216118812561035
Epoch 2840, training loss: 310.3607482910156 = 0.011405032128095627 + 50.0 * 6.206986427307129
Epoch 2840, val loss: 1.323603630065918
Epoch 2850, training loss: 310.2013854980469 = 0.011283308267593384 + 50.0 * 6.20380163192749
Epoch 2850, val loss: 1.3261890411376953
Epoch 2860, training loss: 310.0963134765625 = 0.011170518584549427 + 50.0 * 6.201703071594238
Epoch 2860, val loss: 1.3281673192977905
Epoch 2870, training loss: 310.08941650390625 = 0.011062286794185638 + 50.0 * 6.20156717300415
Epoch 2870, val loss: 1.3306056261062622
Epoch 2880, training loss: 310.09600830078125 = 0.010961944237351418 + 50.0 * 6.2017011642456055
Epoch 2880, val loss: 1.3326855897903442
Epoch 2890, training loss: 310.5257263183594 = 0.010865996591746807 + 50.0 * 6.210297107696533
Epoch 2890, val loss: 1.334283709526062
Epoch 2900, training loss: 310.2751770019531 = 0.010751585476100445 + 50.0 * 6.205288410186768
Epoch 2900, val loss: 1.33729887008667
Epoch 2910, training loss: 310.1424255371094 = 0.010648952797055244 + 50.0 * 6.202635765075684
Epoch 2910, val loss: 1.338909387588501
Epoch 2920, training loss: 310.0657043457031 = 0.010548810474574566 + 50.0 * 6.2011027336120605
Epoch 2920, val loss: 1.3414219617843628
Epoch 2930, training loss: 310.05804443359375 = 0.010456115938723087 + 50.0 * 6.20095157623291
Epoch 2930, val loss: 1.3433319330215454
Epoch 2940, training loss: 310.3946838378906 = 0.01037001796066761 + 50.0 * 6.207685947418213
Epoch 2940, val loss: 1.3455408811569214
Epoch 2950, training loss: 310.20452880859375 = 0.010259203612804413 + 50.0 * 6.203885555267334
Epoch 2950, val loss: 1.3475347757339478
Epoch 2960, training loss: 310.0976867675781 = 0.010165129788219929 + 50.0 * 6.201750755310059
Epoch 2960, val loss: 1.3494746685028076
Epoch 2970, training loss: 310.0616455078125 = 0.01007181964814663 + 50.0 * 6.20103120803833
Epoch 2970, val loss: 1.3516180515289307
Epoch 2980, training loss: 310.53997802734375 = 0.009992338716983795 + 50.0 * 6.210599899291992
Epoch 2980, val loss: 1.3538010120391846
Epoch 2990, training loss: 310.191162109375 = 0.009885269217193127 + 50.0 * 6.203625202178955
Epoch 2990, val loss: 1.3555184602737427
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.838165524512388
=== training gcn model ===
Epoch 0, training loss: 431.779541015625 = 1.9373574256896973 + 50.0 * 8.596843719482422
Epoch 0, val loss: 1.935598373413086
Epoch 10, training loss: 431.73565673828125 = 1.9286209344863892 + 50.0 * 8.59614086151123
Epoch 10, val loss: 1.9275232553482056
Epoch 20, training loss: 431.47821044921875 = 1.917777419090271 + 50.0 * 8.591208457946777
Epoch 20, val loss: 1.9173351526260376
Epoch 30, training loss: 429.7362060546875 = 1.9035515785217285 + 50.0 * 8.556653022766113
Epoch 30, val loss: 1.9037120342254639
Epoch 40, training loss: 419.8559875488281 = 1.8848425149917603 + 50.0 * 8.35942268371582
Epoch 40, val loss: 1.8859210014343262
Epoch 50, training loss: 392.905517578125 = 1.8631919622421265 + 50.0 * 7.8208465576171875
Epoch 50, val loss: 1.8657608032226562
Epoch 60, training loss: 371.48919677734375 = 1.8466229438781738 + 50.0 * 7.39285135269165
Epoch 60, val loss: 1.8507096767425537
Epoch 70, training loss: 356.8768310546875 = 1.835951328277588 + 50.0 * 7.100818157196045
Epoch 70, val loss: 1.8401392698287964
Epoch 80, training loss: 348.9351501464844 = 1.8253952264785767 + 50.0 * 6.942194938659668
Epoch 80, val loss: 1.829728126525879
Epoch 90, training loss: 342.8437805175781 = 1.8136718273162842 + 50.0 * 6.8206024169921875
Epoch 90, val loss: 1.8183482885360718
Epoch 100, training loss: 338.4010925292969 = 1.8029942512512207 + 50.0 * 6.731961727142334
Epoch 100, val loss: 1.8077595233917236
Epoch 110, training loss: 335.62548828125 = 1.7931030988693237 + 50.0 * 6.676648139953613
Epoch 110, val loss: 1.797729253768921
Epoch 120, training loss: 333.7314453125 = 1.7828660011291504 + 50.0 * 6.63897180557251
Epoch 120, val loss: 1.7874090671539307
Epoch 130, training loss: 332.2278747558594 = 1.7717570066452026 + 50.0 * 6.609122276306152
Epoch 130, val loss: 1.7764862775802612
Epoch 140, training loss: 330.9372253417969 = 1.7602927684783936 + 50.0 * 6.58353853225708
Epoch 140, val loss: 1.7654061317443848
Epoch 150, training loss: 329.7926330566406 = 1.748275637626648 + 50.0 * 6.560887336730957
Epoch 150, val loss: 1.753839373588562
Epoch 160, training loss: 328.7718811035156 = 1.7354553937911987 + 50.0 * 6.54072904586792
Epoch 160, val loss: 1.7415989637374878
Epoch 170, training loss: 327.8055725097656 = 1.7216343879699707 + 50.0 * 6.521678447723389
Epoch 170, val loss: 1.7285692691802979
Epoch 180, training loss: 327.0848083496094 = 1.70663583278656 + 50.0 * 6.507563591003418
Epoch 180, val loss: 1.7146215438842773
Epoch 190, training loss: 326.30853271484375 = 1.6903661489486694 + 50.0 * 6.492363452911377
Epoch 190, val loss: 1.6996127367019653
Epoch 200, training loss: 325.5680236816406 = 1.6728166341781616 + 50.0 * 6.477903842926025
Epoch 200, val loss: 1.6835511922836304
Epoch 210, training loss: 325.0735168457031 = 1.6540050506591797 + 50.0 * 6.468390464782715
Epoch 210, val loss: 1.6665087938308716
Epoch 220, training loss: 324.3604736328125 = 1.6337031126022339 + 50.0 * 6.454535484313965
Epoch 220, val loss: 1.6483018398284912
Epoch 230, training loss: 323.80560302734375 = 1.6121761798858643 + 50.0 * 6.443868637084961
Epoch 230, val loss: 1.6291871070861816
Epoch 240, training loss: 323.2923583984375 = 1.5894399881362915 + 50.0 * 6.43405818939209
Epoch 240, val loss: 1.609247088432312
Epoch 250, training loss: 323.0875549316406 = 1.5655200481414795 + 50.0 * 6.430440425872803
Epoch 250, val loss: 1.5884590148925781
Epoch 260, training loss: 322.43121337890625 = 1.540431022644043 + 50.0 * 6.417815685272217
Epoch 260, val loss: 1.5668846368789673
Epoch 270, training loss: 322.0706787109375 = 1.5144709348678589 + 50.0 * 6.411124229431152
Epoch 270, val loss: 1.54481041431427
Epoch 280, training loss: 321.69036865234375 = 1.4877393245697021 + 50.0 * 6.404052734375
Epoch 280, val loss: 1.5223848819732666
Epoch 290, training loss: 321.3603210449219 = 1.4604747295379639 + 50.0 * 6.39799690246582
Epoch 290, val loss: 1.499814510345459
Epoch 300, training loss: 321.2128601074219 = 1.4327324628829956 + 50.0 * 6.395602703094482
Epoch 300, val loss: 1.4772530794143677
Epoch 310, training loss: 320.79144287109375 = 1.40476655960083 + 50.0 * 6.3877339363098145
Epoch 310, val loss: 1.4548120498657227
Epoch 320, training loss: 320.4855651855469 = 1.376723051071167 + 50.0 * 6.382176876068115
Epoch 320, val loss: 1.4325584173202515
Epoch 330, training loss: 320.19720458984375 = 1.3487204313278198 + 50.0 * 6.376969814300537
Epoch 330, val loss: 1.410637617111206
Epoch 340, training loss: 320.30743408203125 = 1.3208110332489014 + 50.0 * 6.379732608795166
Epoch 340, val loss: 1.3889843225479126
Epoch 350, training loss: 319.7545166015625 = 1.292961597442627 + 50.0 * 6.3692307472229
Epoch 350, val loss: 1.3676670789718628
Epoch 360, training loss: 319.53656005859375 = 1.2654621601104736 + 50.0 * 6.365421772003174
Epoch 360, val loss: 1.3467265367507935
Epoch 370, training loss: 319.2516784667969 = 1.2383005619049072 + 50.0 * 6.360267162322998
Epoch 370, val loss: 1.3261282444000244
Epoch 380, training loss: 319.23779296875 = 1.2113927602767944 + 50.0 * 6.360527992248535
Epoch 380, val loss: 1.3057281970977783
Epoch 390, training loss: 318.9058532714844 = 1.1847878694534302 + 50.0 * 6.354421138763428
Epoch 390, val loss: 1.2857877016067505
Epoch 400, training loss: 318.5937194824219 = 1.158470630645752 + 50.0 * 6.348704814910889
Epoch 400, val loss: 1.2660003900527954
Epoch 410, training loss: 318.75726318359375 = 1.132529377937317 + 50.0 * 6.352494716644287
Epoch 410, val loss: 1.246508002281189
Epoch 420, training loss: 318.2830810546875 = 1.106846809387207 + 50.0 * 6.343524932861328
Epoch 420, val loss: 1.2273069620132446
Epoch 430, training loss: 318.0223388671875 = 1.0816620588302612 + 50.0 * 6.338813781738281
Epoch 430, val loss: 1.2086055278778076
Epoch 440, training loss: 318.01385498046875 = 1.0569003820419312 + 50.0 * 6.339138984680176
Epoch 440, val loss: 1.1902037858963013
Epoch 450, training loss: 317.7922668457031 = 1.0324726104736328 + 50.0 * 6.335196018218994
Epoch 450, val loss: 1.1722080707550049
Epoch 460, training loss: 317.64599609375 = 1.0085585117340088 + 50.0 * 6.332748889923096
Epoch 460, val loss: 1.1546409130096436
Epoch 470, training loss: 317.4048156738281 = 0.9850119948387146 + 50.0 * 6.328395843505859
Epoch 470, val loss: 1.137183427810669
Epoch 480, training loss: 317.1843566894531 = 0.9621081352233887 + 50.0 * 6.3244452476501465
Epoch 480, val loss: 1.1206811666488647
Epoch 490, training loss: 317.15167236328125 = 0.9397372007369995 + 50.0 * 6.3242387771606445
Epoch 490, val loss: 1.1047157049179077
Epoch 500, training loss: 316.990234375 = 0.9178905487060547 + 50.0 * 6.321447372436523
Epoch 500, val loss: 1.0890493392944336
Epoch 510, training loss: 316.7842102050781 = 0.8966243267059326 + 50.0 * 6.317751407623291
Epoch 510, val loss: 1.074327826499939
Epoch 520, training loss: 316.88995361328125 = 0.8760057687759399 + 50.0 * 6.320279121398926
Epoch 520, val loss: 1.060004711151123
Epoch 530, training loss: 316.5404052734375 = 0.8559775948524475 + 50.0 * 6.313688278198242
Epoch 530, val loss: 1.0466872453689575
Epoch 540, training loss: 316.37811279296875 = 0.8366261124610901 + 50.0 * 6.3108296394348145
Epoch 540, val loss: 1.033950686454773
Epoch 550, training loss: 316.3398742675781 = 0.8179352879524231 + 50.0 * 6.310438632965088
Epoch 550, val loss: 1.0218932628631592
Epoch 560, training loss: 316.31915283203125 = 0.7997303605079651 + 50.0 * 6.310388088226318
Epoch 560, val loss: 1.0102876424789429
Epoch 570, training loss: 316.1090393066406 = 0.7820971012115479 + 50.0 * 6.3065385818481445
Epoch 570, val loss: 0.9994191527366638
Epoch 580, training loss: 315.9244079589844 = 0.7651548385620117 + 50.0 * 6.303184986114502
Epoch 580, val loss: 0.9892469644546509
Epoch 590, training loss: 315.8951721191406 = 0.7488046884536743 + 50.0 * 6.302927494049072
Epoch 590, val loss: 0.9795177578926086
Epoch 600, training loss: 315.7680358886719 = 0.7329331040382385 + 50.0 * 6.30070161819458
Epoch 600, val loss: 0.9704487919807434
Epoch 610, training loss: 315.6934814453125 = 0.7175864577293396 + 50.0 * 6.299517631530762
Epoch 610, val loss: 0.9618629813194275
Epoch 620, training loss: 315.5106506347656 = 0.7027748823165894 + 50.0 * 6.296157360076904
Epoch 620, val loss: 0.9538227319717407
Epoch 630, training loss: 315.6423645019531 = 0.6884756684303284 + 50.0 * 6.29907751083374
Epoch 630, val loss: 0.946190357208252
Epoch 640, training loss: 315.6787109375 = 0.6744053363800049 + 50.0 * 6.30008602142334
Epoch 640, val loss: 0.9388713240623474
Epoch 650, training loss: 315.3556823730469 = 0.6608006358146667 + 50.0 * 6.29389762878418
Epoch 650, val loss: 0.9320778250694275
Epoch 660, training loss: 315.2082214355469 = 0.6476547718048096 + 50.0 * 6.2912116050720215
Epoch 660, val loss: 0.9257045984268188
Epoch 670, training loss: 315.10284423828125 = 0.634861171245575 + 50.0 * 6.2893595695495605
Epoch 670, val loss: 0.9196261763572693
Epoch 680, training loss: 315.292236328125 = 0.622266411781311 + 50.0 * 6.293399810791016
Epoch 680, val loss: 0.9137686491012573
Epoch 690, training loss: 315.0545349121094 = 0.6099210381507874 + 50.0 * 6.2888922691345215
Epoch 690, val loss: 0.9080983996391296
Epoch 700, training loss: 314.84637451171875 = 0.5979107618331909 + 50.0 * 6.284969329833984
Epoch 700, val loss: 0.9029235243797302
Epoch 710, training loss: 314.8363342285156 = 0.5861665606498718 + 50.0 * 6.285003185272217
Epoch 710, val loss: 0.8979620337486267
Epoch 720, training loss: 314.7898864746094 = 0.5745017528533936 + 50.0 * 6.284307956695557
Epoch 720, val loss: 0.8930519819259644
Epoch 730, training loss: 314.79296875 = 0.5629922747612 + 50.0 * 6.284599304199219
Epoch 730, val loss: 0.8883000016212463
Epoch 740, training loss: 314.583984375 = 0.5516446828842163 + 50.0 * 6.280646800994873
Epoch 740, val loss: 0.8838103413581848
Epoch 750, training loss: 314.5343322753906 = 0.5404987335205078 + 50.0 * 6.279876708984375
Epoch 750, val loss: 0.8795123100280762
Epoch 760, training loss: 314.63079833984375 = 0.5294647216796875 + 50.0 * 6.282026767730713
Epoch 760, val loss: 0.8753572702407837
Epoch 770, training loss: 314.92401123046875 = 0.518450915813446 + 50.0 * 6.288111209869385
Epoch 770, val loss: 0.871144711971283
Epoch 780, training loss: 314.39361572265625 = 0.5074910521507263 + 50.0 * 6.2777228355407715
Epoch 780, val loss: 0.8671676516532898
Epoch 790, training loss: 314.309326171875 = 0.49674397706985474 + 50.0 * 6.276251792907715
Epoch 790, val loss: 0.8635684251785278
Epoch 800, training loss: 314.22943115234375 = 0.48612600564956665 + 50.0 * 6.274866104125977
Epoch 800, val loss: 0.8600675463676453
Epoch 810, training loss: 314.383056640625 = 0.47560805082321167 + 50.0 * 6.278148651123047
Epoch 810, val loss: 0.8567131161689758
Epoch 820, training loss: 314.1419677734375 = 0.46516117453575134 + 50.0 * 6.273536205291748
Epoch 820, val loss: 0.853653609752655
Epoch 830, training loss: 314.1392822265625 = 0.45485660433769226 + 50.0 * 6.273688316345215
Epoch 830, val loss: 0.8506389260292053
Epoch 840, training loss: 314.00360107421875 = 0.444631963968277 + 50.0 * 6.27117919921875
Epoch 840, val loss: 0.8479490876197815
Epoch 850, training loss: 313.95489501953125 = 0.434550017118454 + 50.0 * 6.270407199859619
Epoch 850, val loss: 0.845409631729126
Epoch 860, training loss: 313.8731994628906 = 0.4246198832988739 + 50.0 * 6.2689714431762695
Epoch 860, val loss: 0.8430705070495605
Epoch 870, training loss: 313.8656311035156 = 0.41481152176856995 + 50.0 * 6.269016265869141
Epoch 870, val loss: 0.8409447073936462
Epoch 880, training loss: 314.00933837890625 = 0.40510886907577515 + 50.0 * 6.2720842361450195
Epoch 880, val loss: 0.8388864994049072
Epoch 890, training loss: 313.87017822265625 = 0.39552122354507446 + 50.0 * 6.269493579864502
Epoch 890, val loss: 0.8371877074241638
Epoch 900, training loss: 313.65618896484375 = 0.38605207204818726 + 50.0 * 6.265402793884277
Epoch 900, val loss: 0.8353527188301086
Epoch 910, training loss: 313.70849609375 = 0.3767462372779846 + 50.0 * 6.266635417938232
Epoch 910, val loss: 0.8338757157325745
Epoch 920, training loss: 313.6229553222656 = 0.3675726652145386 + 50.0 * 6.265107154846191
Epoch 920, val loss: 0.8325332999229431
Epoch 930, training loss: 313.6014404296875 = 0.3584798574447632 + 50.0 * 6.264858722686768
Epoch 930, val loss: 0.8313289880752563
Epoch 940, training loss: 313.74212646484375 = 0.3494928777217865 + 50.0 * 6.267852783203125
Epoch 940, val loss: 0.8300067186355591
Epoch 950, training loss: 313.5013732910156 = 0.34060484170913696 + 50.0 * 6.263215065002441
Epoch 950, val loss: 0.8290188908576965
Epoch 960, training loss: 313.38507080078125 = 0.3318422734737396 + 50.0 * 6.261064529418945
Epoch 960, val loss: 0.8280800580978394
Epoch 970, training loss: 313.32891845703125 = 0.32325178384780884 + 50.0 * 6.26011323928833
Epoch 970, val loss: 0.8273126482963562
Epoch 980, training loss: 313.4565124511719 = 0.3147583603858948 + 50.0 * 6.2628350257873535
Epoch 980, val loss: 0.8267560601234436
Epoch 990, training loss: 313.7842712402344 = 0.3063286542892456 + 50.0 * 6.269558429718018
Epoch 990, val loss: 0.8262014389038086
Epoch 1000, training loss: 313.2983703613281 = 0.2979219853878021 + 50.0 * 6.260009288787842
Epoch 1000, val loss: 0.8255191445350647
Epoch 1010, training loss: 313.1953430175781 = 0.28974848985671997 + 50.0 * 6.25811243057251
Epoch 1010, val loss: 0.8253933191299438
Epoch 1020, training loss: 313.111328125 = 0.2817496061325073 + 50.0 * 6.256591796875
Epoch 1020, val loss: 0.8254375457763672
Epoch 1030, training loss: 313.2915344238281 = 0.2739052474498749 + 50.0 * 6.260353088378906
Epoch 1030, val loss: 0.8255237340927124
Epoch 1040, training loss: 313.1200866699219 = 0.2661154866218567 + 50.0 * 6.257079601287842
Epoch 1040, val loss: 0.8255217671394348
Epoch 1050, training loss: 313.23223876953125 = 0.2585286796092987 + 50.0 * 6.259474277496338
Epoch 1050, val loss: 0.8260547518730164
Epoch 1060, training loss: 312.9916687011719 = 0.251068651676178 + 50.0 * 6.254812240600586
Epoch 1060, val loss: 0.8264340758323669
Epoch 1070, training loss: 312.9154052734375 = 0.2438390851020813 + 50.0 * 6.25343132019043
Epoch 1070, val loss: 0.8271061778068542
Epoch 1080, training loss: 312.9085388183594 = 0.23682531714439392 + 50.0 * 6.253434658050537
Epoch 1080, val loss: 0.8280849456787109
Epoch 1090, training loss: 313.0295104980469 = 0.2299777865409851 + 50.0 * 6.255990505218506
Epoch 1090, val loss: 0.8289890289306641
Epoch 1100, training loss: 312.9533386230469 = 0.22329255938529968 + 50.0 * 6.254600524902344
Epoch 1100, val loss: 0.8302294611930847
Epoch 1110, training loss: 312.84356689453125 = 0.2167794108390808 + 50.0 * 6.252535820007324
Epoch 1110, val loss: 0.8314595818519592
Epoch 1120, training loss: 312.7555236816406 = 0.21049277484416962 + 50.0 * 6.250900745391846
Epoch 1120, val loss: 0.8328961730003357
Epoch 1130, training loss: 312.7344055175781 = 0.20440766215324402 + 50.0 * 6.2505998611450195
Epoch 1130, val loss: 0.8344560265541077
Epoch 1140, training loss: 312.92584228515625 = 0.19851014018058777 + 50.0 * 6.254546642303467
Epoch 1140, val loss: 0.8363268375396729
Epoch 1150, training loss: 312.940673828125 = 0.1927761435508728 + 50.0 * 6.254957675933838
Epoch 1150, val loss: 0.8381222486495972
Epoch 1160, training loss: 312.7071838378906 = 0.18719518184661865 + 50.0 * 6.250399589538574
Epoch 1160, val loss: 0.8403412699699402
Epoch 1170, training loss: 312.5817565917969 = 0.18184317648410797 + 50.0 * 6.247997760772705
Epoch 1170, val loss: 0.8425546288490295
Epoch 1180, training loss: 312.5503234863281 = 0.17671401798725128 + 50.0 * 6.247471809387207
Epoch 1180, val loss: 0.8449835777282715
Epoch 1190, training loss: 312.73687744140625 = 0.17176216840744019 + 50.0 * 6.251302242279053
Epoch 1190, val loss: 0.8474845886230469
Epoch 1200, training loss: 312.59967041015625 = 0.16692441701889038 + 50.0 * 6.248654842376709
Epoch 1200, val loss: 0.8501898646354675
Epoch 1210, training loss: 312.48828125 = 0.16227155923843384 + 50.0 * 6.246520519256592
Epoch 1210, val loss: 0.8528487682342529
Epoch 1220, training loss: 312.4134521484375 = 0.1578032374382019 + 50.0 * 6.245112895965576
Epoch 1220, val loss: 0.8557764291763306
Epoch 1230, training loss: 312.57098388671875 = 0.15350687503814697 + 50.0 * 6.248349666595459
Epoch 1230, val loss: 0.858750581741333
Epoch 1240, training loss: 312.4737548828125 = 0.14928436279296875 + 50.0 * 6.24648904800415
Epoch 1240, val loss: 0.8619430661201477
Epoch 1250, training loss: 312.6168212890625 = 0.14520998299121857 + 50.0 * 6.24943208694458
Epoch 1250, val loss: 0.8647302985191345
Epoch 1260, training loss: 312.35711669921875 = 0.1412896066904068 + 50.0 * 6.244316577911377
Epoch 1260, val loss: 0.8681287169456482
Epoch 1270, training loss: 312.27410888671875 = 0.13753391802310944 + 50.0 * 6.242731094360352
Epoch 1270, val loss: 0.8718199133872986
Epoch 1280, training loss: 312.3868408203125 = 0.13392652571201324 + 50.0 * 6.245058059692383
Epoch 1280, val loss: 0.8752381205558777
Epoch 1290, training loss: 312.4414367675781 = 0.13040320575237274 + 50.0 * 6.246220588684082
Epoch 1290, val loss: 0.8784235715866089
Epoch 1300, training loss: 312.2328186035156 = 0.1269758641719818 + 50.0 * 6.242116928100586
Epoch 1300, val loss: 0.8820017576217651
Epoch 1310, training loss: 312.1882629394531 = 0.12369619309902191 + 50.0 * 6.241291522979736
Epoch 1310, val loss: 0.885651171207428
Epoch 1320, training loss: 312.3254089355469 = 0.12053477019071579 + 50.0 * 6.2440972328186035
Epoch 1320, val loss: 0.8893934488296509
Epoch 1330, training loss: 312.14501953125 = 0.11746560782194138 + 50.0 * 6.240550994873047
Epoch 1330, val loss: 0.8931389451026917
Epoch 1340, training loss: 312.28466796875 = 0.11451824754476547 + 50.0 * 6.243402481079102
Epoch 1340, val loss: 0.896758496761322
Epoch 1350, training loss: 312.25360107421875 = 0.11162150651216507 + 50.0 * 6.242839336395264
Epoch 1350, val loss: 0.9008767604827881
Epoch 1360, training loss: 312.16693115234375 = 0.10882048308849335 + 50.0 * 6.241162300109863
Epoch 1360, val loss: 0.9045946002006531
Epoch 1370, training loss: 312.0647277832031 = 0.10613195598125458 + 50.0 * 6.239171981811523
Epoch 1370, val loss: 0.9088612794876099
Epoch 1380, training loss: 312.0345458984375 = 0.10354461520910263 + 50.0 * 6.238620281219482
Epoch 1380, val loss: 0.9128476977348328
Epoch 1390, training loss: 312.3247985839844 = 0.1010361835360527 + 50.0 * 6.244475364685059
Epoch 1390, val loss: 0.917191207408905
Epoch 1400, training loss: 312.25421142578125 = 0.09857386350631714 + 50.0 * 6.243112564086914
Epoch 1400, val loss: 0.9209734797477722
Epoch 1410, training loss: 312.04302978515625 = 0.0961696207523346 + 50.0 * 6.2389373779296875
Epoch 1410, val loss: 0.9251317381858826
Epoch 1420, training loss: 311.9903259277344 = 0.09387696534395218 + 50.0 * 6.237929344177246
Epoch 1420, val loss: 0.9296793341636658
Epoch 1430, training loss: 312.028076171875 = 0.09165717661380768 + 50.0 * 6.2387285232543945
Epoch 1430, val loss: 0.9338907599449158
Epoch 1440, training loss: 311.9403076171875 = 0.08949331194162369 + 50.0 * 6.237016201019287
Epoch 1440, val loss: 0.9381676316261292
Epoch 1450, training loss: 311.8771057128906 = 0.08741316199302673 + 50.0 * 6.2357940673828125
Epoch 1450, val loss: 0.9425670504570007
Epoch 1460, training loss: 312.0090026855469 = 0.08540132641792297 + 50.0 * 6.2384724617004395
Epoch 1460, val loss: 0.9468836188316345
Epoch 1470, training loss: 311.9866027832031 = 0.08342580497264862 + 50.0 * 6.238063335418701
Epoch 1470, val loss: 0.9512399435043335
Epoch 1480, training loss: 311.9036560058594 = 0.08149898052215576 + 50.0 * 6.236442565917969
Epoch 1480, val loss: 0.9558950066566467
Epoch 1490, training loss: 311.8849182128906 = 0.07963242381811142 + 50.0 * 6.236105918884277
Epoch 1490, val loss: 0.9600290656089783
Epoch 1500, training loss: 311.77978515625 = 0.07782763987779617 + 50.0 * 6.234039306640625
Epoch 1500, val loss: 0.9646796584129333
Epoch 1510, training loss: 311.7476501464844 = 0.07608909159898758 + 50.0 * 6.233431339263916
Epoch 1510, val loss: 0.9690715670585632
Epoch 1520, training loss: 312.0000305175781 = 0.07440689206123352 + 50.0 * 6.2385125160217285
Epoch 1520, val loss: 0.9733543395996094
Epoch 1530, training loss: 311.81524658203125 = 0.07273638248443604 + 50.0 * 6.2348504066467285
Epoch 1530, val loss: 0.97809898853302
Epoch 1540, training loss: 311.7200927734375 = 0.07111820578575134 + 50.0 * 6.232979774475098
Epoch 1540, val loss: 0.9823582172393799
Epoch 1550, training loss: 311.7017822265625 = 0.0695686861872673 + 50.0 * 6.232644081115723
Epoch 1550, val loss: 0.9872746467590332
Epoch 1560, training loss: 311.94805908203125 = 0.06806699931621552 + 50.0 * 6.237600326538086
Epoch 1560, val loss: 0.9916790127754211
Epoch 1570, training loss: 311.8399353027344 = 0.06657969206571579 + 50.0 * 6.235466957092285
Epoch 1570, val loss: 0.9961265325546265
Epoch 1580, training loss: 311.6893310546875 = 0.06513582915067673 + 50.0 * 6.232483863830566
Epoch 1580, val loss: 1.0006357431411743
Epoch 1590, training loss: 311.7920227050781 = 0.06375253945589066 + 50.0 * 6.234565258026123
Epoch 1590, val loss: 1.0053074359893799
Epoch 1600, training loss: 311.6665954589844 = 0.06238288804888725 + 50.0 * 6.232084274291992
Epoch 1600, val loss: 1.0100386142730713
Epoch 1610, training loss: 311.7325439453125 = 0.06106695160269737 + 50.0 * 6.233429431915283
Epoch 1610, val loss: 1.0148472785949707
Epoch 1620, training loss: 311.722900390625 = 0.05978383123874664 + 50.0 * 6.233262538909912
Epoch 1620, val loss: 1.0192861557006836
Epoch 1630, training loss: 311.57220458984375 = 0.05852557718753815 + 50.0 * 6.230273246765137
Epoch 1630, val loss: 1.0239442586898804
Epoch 1640, training loss: 311.57342529296875 = 0.05731263756752014 + 50.0 * 6.230322360992432
Epoch 1640, val loss: 1.0286545753479004
Epoch 1650, training loss: 311.7511291503906 = 0.05613582953810692 + 50.0 * 6.23390007019043
Epoch 1650, val loss: 1.0333188772201538
Epoch 1660, training loss: 311.66094970703125 = 0.05497104302048683 + 50.0 * 6.232120037078857
Epoch 1660, val loss: 1.0380268096923828
Epoch 1670, training loss: 311.6386413574219 = 0.05383644253015518 + 50.0 * 6.231696128845215
Epoch 1670, val loss: 1.0420092344284058
Epoch 1680, training loss: 311.5830078125 = 0.05273548141121864 + 50.0 * 6.230605602264404
Epoch 1680, val loss: 1.047256350517273
Epoch 1690, training loss: 311.4682312011719 = 0.05167004466056824 + 50.0 * 6.228331089019775
Epoch 1690, val loss: 1.051667332649231
Epoch 1700, training loss: 311.5997009277344 = 0.05064121633768082 + 50.0 * 6.230981349945068
Epoch 1700, val loss: 1.0564500093460083
Epoch 1710, training loss: 311.52923583984375 = 0.04961991682648659 + 50.0 * 6.229592323303223
Epoch 1710, val loss: 1.0608056783676147
Epoch 1720, training loss: 311.4150390625 = 0.0486246682703495 + 50.0 * 6.227328300476074
Epoch 1720, val loss: 1.065435767173767
Epoch 1730, training loss: 311.4166259765625 = 0.04766695201396942 + 50.0 * 6.227378845214844
Epoch 1730, val loss: 1.0701762437820435
Epoch 1740, training loss: 311.651123046875 = 0.046743761748075485 + 50.0 * 6.2320876121521
Epoch 1740, val loss: 1.0751442909240723
Epoch 1750, training loss: 311.5010681152344 = 0.045818161219358444 + 50.0 * 6.229105472564697
Epoch 1750, val loss: 1.0790585279464722
Epoch 1760, training loss: 311.6155090332031 = 0.0449121855199337 + 50.0 * 6.231411933898926
Epoch 1760, val loss: 1.083759069442749
Epoch 1770, training loss: 311.4282531738281 = 0.04402688890695572 + 50.0 * 6.227684497833252
Epoch 1770, val loss: 1.0881327390670776
Epoch 1780, training loss: 311.3387451171875 = 0.04318719357252121 + 50.0 * 6.2259111404418945
Epoch 1780, val loss: 1.0928996801376343
Epoch 1790, training loss: 311.2926025390625 = 0.04236992821097374 + 50.0 * 6.22500467300415
Epoch 1790, val loss: 1.0975505113601685
Epoch 1800, training loss: 311.4522399902344 = 0.04157821089029312 + 50.0 * 6.228213787078857
Epoch 1800, val loss: 1.1020766496658325
Epoch 1810, training loss: 311.30291748046875 = 0.040782470256090164 + 50.0 * 6.225243091583252
Epoch 1810, val loss: 1.1064214706420898
Epoch 1820, training loss: 311.41766357421875 = 0.04001429304480553 + 50.0 * 6.227553367614746
Epoch 1820, val loss: 1.1109992265701294
Epoch 1830, training loss: 311.3699035644531 = 0.039253029972314835 + 50.0 * 6.2266130447387695
Epoch 1830, val loss: 1.1155418157577515
Epoch 1840, training loss: 311.2742004394531 = 0.03851887211203575 + 50.0 * 6.2247138023376465
Epoch 1840, val loss: 1.1198920011520386
Epoch 1850, training loss: 311.27862548828125 = 0.037812527269124985 + 50.0 * 6.22481632232666
Epoch 1850, val loss: 1.1244689226150513
Epoch 1860, training loss: 311.4745178222656 = 0.037123698741197586 + 50.0 * 6.228748321533203
Epoch 1860, val loss: 1.1288552284240723
Epoch 1870, training loss: 311.2596435546875 = 0.03643135726451874 + 50.0 * 6.224464416503906
Epoch 1870, val loss: 1.1330848932266235
Epoch 1880, training loss: 311.2167663574219 = 0.03577443212270737 + 50.0 * 6.2236199378967285
Epoch 1880, val loss: 1.1377537250518799
Epoch 1890, training loss: 311.4402160644531 = 0.0351378433406353 + 50.0 * 6.22810173034668
Epoch 1890, val loss: 1.1422688961029053
Epoch 1900, training loss: 311.2119140625 = 0.03449351713061333 + 50.0 * 6.223548889160156
Epoch 1900, val loss: 1.1460630893707275
Epoch 1910, training loss: 311.1471252441406 = 0.03387599438428879 + 50.0 * 6.222265243530273
Epoch 1910, val loss: 1.1506587266921997
Epoch 1920, training loss: 311.2554626464844 = 0.03328542038798332 + 50.0 * 6.224443435668945
Epoch 1920, val loss: 1.1550531387329102
Epoch 1930, training loss: 311.2229919433594 = 0.03269195556640625 + 50.0 * 6.223806381225586
Epoch 1930, val loss: 1.1591497659683228
Epoch 1940, training loss: 311.3863525390625 = 0.03210930898785591 + 50.0 * 6.227085113525391
Epoch 1940, val loss: 1.1632405519485474
Epoch 1950, training loss: 311.130615234375 = 0.03154195472598076 + 50.0 * 6.221981048583984
Epoch 1950, val loss: 1.1677199602127075
Epoch 1960, training loss: 311.1260986328125 = 0.030996469780802727 + 50.0 * 6.221901893615723
Epoch 1960, val loss: 1.1718430519104004
Epoch 1970, training loss: 311.1487121582031 = 0.030468134209513664 + 50.0 * 6.222364902496338
Epoch 1970, val loss: 1.1760073900222778
Epoch 1980, training loss: 311.3382873535156 = 0.029949212446808815 + 50.0 * 6.22616720199585
Epoch 1980, val loss: 1.179921269416809
Epoch 1990, training loss: 311.18798828125 = 0.029428405687212944 + 50.0 * 6.223171234130859
Epoch 1990, val loss: 1.184570074081421
Epoch 2000, training loss: 311.1636047363281 = 0.028930502012372017 + 50.0 * 6.22269344329834
Epoch 2000, val loss: 1.1886463165283203
Epoch 2010, training loss: 311.1391906738281 = 0.028444357216358185 + 50.0 * 6.222214698791504
Epoch 2010, val loss: 1.192417860031128
Epoch 2020, training loss: 311.0491027832031 = 0.027969149872660637 + 50.0 * 6.220423221588135
Epoch 2020, val loss: 1.1966466903686523
Epoch 2030, training loss: 311.21759033203125 = 0.027511510998010635 + 50.0 * 6.223802089691162
Epoch 2030, val loss: 1.2007277011871338
Epoch 2040, training loss: 311.2087707519531 = 0.02704745903611183 + 50.0 * 6.223634719848633
Epoch 2040, val loss: 1.2049094438552856
Epoch 2050, training loss: 311.0751953125 = 0.02659442648291588 + 50.0 * 6.220972537994385
Epoch 2050, val loss: 1.208638310432434
Epoch 2060, training loss: 310.9835510253906 = 0.026157032698392868 + 50.0 * 6.2191481590271
Epoch 2060, val loss: 1.212911605834961
Epoch 2070, training loss: 310.97259521484375 = 0.025737527757883072 + 50.0 * 6.218937397003174
Epoch 2070, val loss: 1.2167412042617798
Epoch 2080, training loss: 311.3494567871094 = 0.02533295378088951 + 50.0 * 6.226482391357422
Epoch 2080, val loss: 1.2208350896835327
Epoch 2090, training loss: 311.0723571777344 = 0.02491074427962303 + 50.0 * 6.220949172973633
Epoch 2090, val loss: 1.224381923675537
Epoch 2100, training loss: 310.9899597167969 = 0.02451281063258648 + 50.0 * 6.219309329986572
Epoch 2100, val loss: 1.2282943725585938
Epoch 2110, training loss: 311.12310791015625 = 0.02412842959165573 + 50.0 * 6.22197961807251
Epoch 2110, val loss: 1.232140064239502
Epoch 2120, training loss: 311.0234680175781 = 0.023742619901895523 + 50.0 * 6.21999454498291
Epoch 2120, val loss: 1.2360639572143555
Epoch 2130, training loss: 311.00048828125 = 0.023367127403616905 + 50.0 * 6.219542026519775
Epoch 2130, val loss: 1.2400038242340088
Epoch 2140, training loss: 310.9468688964844 = 0.023006122559309006 + 50.0 * 6.218477249145508
Epoch 2140, val loss: 1.2437543869018555
Epoch 2150, training loss: 311.03472900390625 = 0.022654833272099495 + 50.0 * 6.220241546630859
Epoch 2150, val loss: 1.2473900318145752
Epoch 2160, training loss: 310.9363098144531 = 0.022301085293293 + 50.0 * 6.218279838562012
Epoch 2160, val loss: 1.2511662244796753
Epoch 2170, training loss: 311.1055603027344 = 0.0219633299857378 + 50.0 * 6.2216715812683105
Epoch 2170, val loss: 1.2543858289718628
Epoch 2180, training loss: 310.9245910644531 = 0.021621178835630417 + 50.0 * 6.218059539794922
Epoch 2180, val loss: 1.2585903406143188
Epoch 2190, training loss: 310.9991149902344 = 0.0212919469922781 + 50.0 * 6.2195563316345215
Epoch 2190, val loss: 1.2624592781066895
Epoch 2200, training loss: 310.9542541503906 = 0.02097039297223091 + 50.0 * 6.218666076660156
Epoch 2200, val loss: 1.265816569328308
Epoch 2210, training loss: 310.8656921386719 = 0.020656900480389595 + 50.0 * 6.216900825500488
Epoch 2210, val loss: 1.2695585489273071
Epoch 2220, training loss: 310.82867431640625 = 0.020352866500616074 + 50.0 * 6.2161664962768555
Epoch 2220, val loss: 1.2733293771743774
Epoch 2230, training loss: 310.8223876953125 = 0.02005641721189022 + 50.0 * 6.2160468101501465
Epoch 2230, val loss: 1.2770838737487793
Epoch 2240, training loss: 311.17962646484375 = 0.019770147278904915 + 50.0 * 6.223196983337402
Epoch 2240, val loss: 1.281001091003418
Epoch 2250, training loss: 310.9403381347656 = 0.019470471888780594 + 50.0 * 6.218417167663574
Epoch 2250, val loss: 1.2836252450942993
Epoch 2260, training loss: 310.883056640625 = 0.0191840548068285 + 50.0 * 6.2172770500183105
Epoch 2260, val loss: 1.2874349355697632
Epoch 2270, training loss: 310.8686828613281 = 0.018908074125647545 + 50.0 * 6.2169952392578125
Epoch 2270, val loss: 1.290936827659607
Epoch 2280, training loss: 310.8187561035156 = 0.018640995025634766 + 50.0 * 6.216001987457275
Epoch 2280, val loss: 1.2946122884750366
Epoch 2290, training loss: 311.0975036621094 = 0.018381420522928238 + 50.0 * 6.221582412719727
Epoch 2290, val loss: 1.298078179359436
Epoch 2300, training loss: 310.7804870605469 = 0.01811140961945057 + 50.0 * 6.215248107910156
Epoch 2300, val loss: 1.3016084432601929
Epoch 2310, training loss: 310.7200012207031 = 0.017855791375041008 + 50.0 * 6.214043140411377
Epoch 2310, val loss: 1.304803729057312
Epoch 2320, training loss: 310.76318359375 = 0.017612867057323456 + 50.0 * 6.214911460876465
Epoch 2320, val loss: 1.3083631992340088
Epoch 2330, training loss: 311.293701171875 = 0.01737518608570099 + 50.0 * 6.225526809692383
Epoch 2330, val loss: 1.3117467164993286
Epoch 2340, training loss: 310.8759765625 = 0.017124397680163383 + 50.0 * 6.217177391052246
Epoch 2340, val loss: 1.3148428201675415
Epoch 2350, training loss: 310.7391052246094 = 0.016888782382011414 + 50.0 * 6.214444637298584
Epoch 2350, val loss: 1.3184361457824707
Epoch 2360, training loss: 310.6795654296875 = 0.016662633046507835 + 50.0 * 6.213257789611816
Epoch 2360, val loss: 1.3218591213226318
Epoch 2370, training loss: 310.7572937011719 = 0.016445374116301537 + 50.0 * 6.214817047119141
Epoch 2370, val loss: 1.325090765953064
Epoch 2380, training loss: 310.80120849609375 = 0.016222283244132996 + 50.0 * 6.215699672698975
Epoch 2380, val loss: 1.328161597251892
Epoch 2390, training loss: 310.7066955566406 = 0.015997493639588356 + 50.0 * 6.2138142585754395
Epoch 2390, val loss: 1.3310930728912354
Epoch 2400, training loss: 310.7102966308594 = 0.01578589156270027 + 50.0 * 6.213890075683594
Epoch 2400, val loss: 1.3346810340881348
Epoch 2410, training loss: 310.8638000488281 = 0.015582544729113579 + 50.0 * 6.216964244842529
Epoch 2410, val loss: 1.337611436843872
Epoch 2420, training loss: 310.6659240722656 = 0.01537470892071724 + 50.0 * 6.213010787963867
Epoch 2420, val loss: 1.3411723375320435
Epoch 2430, training loss: 310.74456787109375 = 0.015177750959992409 + 50.0 * 6.214588165283203
Epoch 2430, val loss: 1.3443505764007568
Epoch 2440, training loss: 310.7331237792969 = 0.014983613044023514 + 50.0 * 6.214362621307373
Epoch 2440, val loss: 1.3473281860351562
Epoch 2450, training loss: 310.7279968261719 = 0.014791559427976608 + 50.0 * 6.214263916015625
Epoch 2450, val loss: 1.350777506828308
Epoch 2460, training loss: 310.70758056640625 = 0.014601309783756733 + 50.0 * 6.213860034942627
Epoch 2460, val loss: 1.3538259267807007
Epoch 2470, training loss: 310.72198486328125 = 0.014416320249438286 + 50.0 * 6.214151382446289
Epoch 2470, val loss: 1.3565008640289307
Epoch 2480, training loss: 310.69342041015625 = 0.014236834831535816 + 50.0 * 6.213583469390869
Epoch 2480, val loss: 1.3597822189331055
Epoch 2490, training loss: 310.8058776855469 = 0.0140603082254529 + 50.0 * 6.215836048126221
Epoch 2490, val loss: 1.3623900413513184
Epoch 2500, training loss: 310.6737060546875 = 0.01387994084507227 + 50.0 * 6.213196277618408
Epoch 2500, val loss: 1.3657736778259277
Epoch 2510, training loss: 310.6253662109375 = 0.01370979379862547 + 50.0 * 6.212233543395996
Epoch 2510, val loss: 1.3688077926635742
Epoch 2520, training loss: 310.7468566894531 = 0.01354637648910284 + 50.0 * 6.214666366577148
Epoch 2520, val loss: 1.3719589710235596
Epoch 2530, training loss: 310.7079772949219 = 0.013379102572798729 + 50.0 * 6.213892459869385
Epoch 2530, val loss: 1.3746784925460815
Epoch 2540, training loss: 310.5704040527344 = 0.01321383472532034 + 50.0 * 6.211143493652344
Epoch 2540, val loss: 1.3774925470352173
Epoch 2550, training loss: 310.6927185058594 = 0.01305743120610714 + 50.0 * 6.213593006134033
Epoch 2550, val loss: 1.3805519342422485
Epoch 2560, training loss: 310.711669921875 = 0.012901149690151215 + 50.0 * 6.213975429534912
Epoch 2560, val loss: 1.383296012878418
Epoch 2570, training loss: 310.59088134765625 = 0.012746930122375488 + 50.0 * 6.211562633514404
Epoch 2570, val loss: 1.385758876800537
Epoch 2580, training loss: 310.5408630371094 = 0.012596099637448788 + 50.0 * 6.210565090179443
Epoch 2580, val loss: 1.3889319896697998
Epoch 2590, training loss: 310.7278747558594 = 0.012452549301087856 + 50.0 * 6.214308261871338
Epoch 2590, val loss: 1.391808032989502
Epoch 2600, training loss: 310.5591735839844 = 0.01230369508266449 + 50.0 * 6.2109375
Epoch 2600, val loss: 1.3944168090820312
Epoch 2610, training loss: 310.5238952636719 = 0.012160181067883968 + 50.0 * 6.210235118865967
Epoch 2610, val loss: 1.3974424600601196
Epoch 2620, training loss: 310.5351867675781 = 0.012022438459098339 + 50.0 * 6.210463523864746
Epoch 2620, val loss: 1.4001880884170532
Epoch 2630, training loss: 310.9110107421875 = 0.011891450732946396 + 50.0 * 6.217982292175293
Epoch 2630, val loss: 1.402748942375183
Epoch 2640, training loss: 310.69647216796875 = 0.011747526004910469 + 50.0 * 6.2136945724487305
Epoch 2640, val loss: 1.4051676988601685
Epoch 2650, training loss: 310.5985412597656 = 0.01161365956068039 + 50.0 * 6.211738586425781
Epoch 2650, val loss: 1.4079622030258179
Epoch 2660, training loss: 310.58929443359375 = 0.011480729095637798 + 50.0 * 6.211556434631348
Epoch 2660, val loss: 1.4106345176696777
Epoch 2670, training loss: 310.46038818359375 = 0.011353905312716961 + 50.0 * 6.208980560302734
Epoch 2670, val loss: 1.4132517576217651
Epoch 2680, training loss: 310.463623046875 = 0.011231689713895321 + 50.0 * 6.209047794342041
Epoch 2680, val loss: 1.4159537553787231
Epoch 2690, training loss: 310.5607604980469 = 0.011110843159258366 + 50.0 * 6.210992813110352
Epoch 2690, val loss: 1.4182924032211304
Epoch 2700, training loss: 310.6552429199219 = 0.01098660659044981 + 50.0 * 6.21288537979126
Epoch 2700, val loss: 1.4210065603256226
Epoch 2710, training loss: 310.4771423339844 = 0.010861446149647236 + 50.0 * 6.209325313568115
Epoch 2710, val loss: 1.4232580661773682
Epoch 2720, training loss: 310.5447692871094 = 0.010745358653366566 + 50.0 * 6.2106804847717285
Epoch 2720, val loss: 1.4260470867156982
Epoch 2730, training loss: 310.45977783203125 = 0.010629231110215187 + 50.0 * 6.208983421325684
Epoch 2730, val loss: 1.4286096096038818
Epoch 2740, training loss: 310.4021301269531 = 0.010517025366425514 + 50.0 * 6.207832336425781
Epoch 2740, val loss: 1.4315522909164429
Epoch 2750, training loss: 310.5719299316406 = 0.010409656912088394 + 50.0 * 6.211230278015137
Epoch 2750, val loss: 1.4342397451400757
Epoch 2760, training loss: 310.5621032714844 = 0.010297464206814766 + 50.0 * 6.211036682128906
Epoch 2760, val loss: 1.4360898733139038
Epoch 2770, training loss: 310.38555908203125 = 0.010184739716351032 + 50.0 * 6.207507133483887
Epoch 2770, val loss: 1.4388507604599
Epoch 2780, training loss: 310.3802185058594 = 0.010079366154968739 + 50.0 * 6.207403182983398
Epoch 2780, val loss: 1.4413174390792847
Epoch 2790, training loss: 310.3871765136719 = 0.009978730231523514 + 50.0 * 6.207543849945068
Epoch 2790, val loss: 1.443755030632019
Epoch 2800, training loss: 310.765869140625 = 0.009881239384412766 + 50.0 * 6.215119361877441
Epoch 2800, val loss: 1.4465872049331665
Epoch 2810, training loss: 310.53033447265625 = 0.009774979203939438 + 50.0 * 6.210411071777344
Epoch 2810, val loss: 1.4487576484680176
Epoch 2820, training loss: 310.4873352050781 = 0.009673143737018108 + 50.0 * 6.209553241729736
Epoch 2820, val loss: 1.4508112668991089
Epoch 2830, training loss: 310.4177551269531 = 0.009576520882546902 + 50.0 * 6.208163261413574
Epoch 2830, val loss: 1.4533767700195312
Epoch 2840, training loss: 310.47076416015625 = 0.009481578134000301 + 50.0 * 6.209225177764893
Epoch 2840, val loss: 1.4557833671569824
Epoch 2850, training loss: 310.4817810058594 = 0.009384911507368088 + 50.0 * 6.209447383880615
Epoch 2850, val loss: 1.4580576419830322
Epoch 2860, training loss: 310.4134521484375 = 0.009292840026319027 + 50.0 * 6.208082675933838
Epoch 2860, val loss: 1.4602687358856201
Epoch 2870, training loss: 310.3403625488281 = 0.00920165702700615 + 50.0 * 6.206623077392578
Epoch 2870, val loss: 1.4631097316741943
Epoch 2880, training loss: 310.4366149902344 = 0.009114761836826801 + 50.0 * 6.208549976348877
Epoch 2880, val loss: 1.4653103351593018
Epoch 2890, training loss: 310.4520568847656 = 0.00902539398521185 + 50.0 * 6.208860397338867
Epoch 2890, val loss: 1.467247486114502
Epoch 2900, training loss: 310.492919921875 = 0.008937213569879532 + 50.0 * 6.20967960357666
Epoch 2900, val loss: 1.4690972566604614
Epoch 2910, training loss: 310.5110778808594 = 0.008850031532347202 + 50.0 * 6.2100443840026855
Epoch 2910, val loss: 1.4715772867202759
Epoch 2920, training loss: 310.3619079589844 = 0.008764158934354782 + 50.0 * 6.207062721252441
Epoch 2920, val loss: 1.4742521047592163
Epoch 2930, training loss: 310.34637451171875 = 0.008683012798428535 + 50.0 * 6.206753253936768
Epoch 2930, val loss: 1.4760949611663818
Epoch 2940, training loss: 310.4617614746094 = 0.008602428250014782 + 50.0 * 6.2090630531311035
Epoch 2940, val loss: 1.4783257246017456
Epoch 2950, training loss: 310.35784912109375 = 0.008520410396158695 + 50.0 * 6.206986427307129
Epoch 2950, val loss: 1.480515480041504
Epoch 2960, training loss: 310.3792419433594 = 0.008441745303571224 + 50.0 * 6.207415580749512
Epoch 2960, val loss: 1.4827650785446167
Epoch 2970, training loss: 310.3319396972656 = 0.008362406864762306 + 50.0 * 6.2064714431762695
Epoch 2970, val loss: 1.4848098754882812
Epoch 2980, training loss: 310.44219970703125 = 0.008287908509373665 + 50.0 * 6.208677768707275
Epoch 2980, val loss: 1.4875223636627197
Epoch 2990, training loss: 310.3578186035156 = 0.00820905715227127 + 50.0 * 6.206992149353027
Epoch 2990, val loss: 1.4892489910125732
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7666666666666667
0.8423827095413812
The final CL Acc:0.73086, 0.02536, The final GNN Acc:0.83957, 0.00199
Begin epxeriment: cont_weight: 50 epoch:3000 der1:0.4 der2:0.2 dfr1:0.4 dfr2:0.3
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0002, cl_num_epochs=3000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=50.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.4, drop_edge_rate_2=0.2, drop_feat_rate_1=0.4, drop_feat_rate_2=0.3, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11610])
remove edge: torch.Size([2, 9536])
updated graph: torch.Size([2, 10590])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:226: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 431.8025817871094 = 1.9614654779434204 + 50.0 * 8.596822738647461
Epoch 0, val loss: 1.9577094316482544
Epoch 10, training loss: 431.75439453125 = 1.951788306236267 + 50.0 * 8.596052169799805
Epoch 10, val loss: 1.9476083517074585
Epoch 20, training loss: 431.49530029296875 = 1.9396259784698486 + 50.0 * 8.591113090515137
Epoch 20, val loss: 1.9349758625030518
Epoch 30, training loss: 429.96917724609375 = 1.9236291646957397 + 50.0 * 8.560911178588867
Epoch 30, val loss: 1.9182178974151611
Epoch 40, training loss: 422.4552307128906 = 1.9035131931304932 + 50.0 * 8.41103458404541
Epoch 40, val loss: 1.8978153467178345
Epoch 50, training loss: 402.5472106933594 = 1.8800580501556396 + 50.0 * 8.01334285736084
Epoch 50, val loss: 1.8741039037704468
Epoch 60, training loss: 387.0094909667969 = 1.8597317934036255 + 50.0 * 7.7029948234558105
Epoch 60, val loss: 1.8545666933059692
Epoch 70, training loss: 366.7505798339844 = 1.848647117614746 + 50.0 * 7.298038959503174
Epoch 70, val loss: 1.8443827629089355
Epoch 80, training loss: 353.9584655761719 = 1.843313217163086 + 50.0 * 7.042303085327148
Epoch 80, val loss: 1.8385975360870361
Epoch 90, training loss: 347.2803955078125 = 1.8363127708435059 + 50.0 * 6.908881664276123
Epoch 90, val loss: 1.8312828540802002
Epoch 100, training loss: 342.1657409667969 = 1.8270052671432495 + 50.0 * 6.806774616241455
Epoch 100, val loss: 1.8224533796310425
Epoch 110, training loss: 338.1634826660156 = 1.818855881690979 + 50.0 * 6.726892471313477
Epoch 110, val loss: 1.8148061037063599
Epoch 120, training loss: 335.1936950683594 = 1.812724232673645 + 50.0 * 6.667619705200195
Epoch 120, val loss: 1.8088531494140625
Epoch 130, training loss: 333.0510559082031 = 1.8072267770767212 + 50.0 * 6.624876499176025
Epoch 130, val loss: 1.8034104108810425
Epoch 140, training loss: 331.2434387207031 = 1.8017467260360718 + 50.0 * 6.588833808898926
Epoch 140, val loss: 1.7980762720108032
Epoch 150, training loss: 329.8768005371094 = 1.7963197231292725 + 50.0 * 6.561609745025635
Epoch 150, val loss: 1.7929292917251587
Epoch 160, training loss: 328.6778564453125 = 1.7906386852264404 + 50.0 * 6.537744045257568
Epoch 160, val loss: 1.7875726222991943
Epoch 170, training loss: 327.6578369140625 = 1.7846956253051758 + 50.0 * 6.517462730407715
Epoch 170, val loss: 1.7820411920547485
Epoch 180, training loss: 326.7428894042969 = 1.7784792184829712 + 50.0 * 6.499288558959961
Epoch 180, val loss: 1.7763030529022217
Epoch 190, training loss: 326.262451171875 = 1.7717126607894897 + 50.0 * 6.4898152351379395
Epoch 190, val loss: 1.7701951265335083
Epoch 200, training loss: 325.4313659667969 = 1.764483094215393 + 50.0 * 6.473337650299072
Epoch 200, val loss: 1.7635350227355957
Epoch 210, training loss: 324.786865234375 = 1.7567026615142822 + 50.0 * 6.4606032371521
Epoch 210, val loss: 1.7565401792526245
Epoch 220, training loss: 324.2554931640625 = 1.748408555984497 + 50.0 * 6.450141906738281
Epoch 220, val loss: 1.7490934133529663
Epoch 230, training loss: 323.8025207519531 = 1.7394413948059082 + 50.0 * 6.4412617683410645
Epoch 230, val loss: 1.7410154342651367
Epoch 240, training loss: 323.3695373535156 = 1.7297919988632202 + 50.0 * 6.43279504776001
Epoch 240, val loss: 1.732430338859558
Epoch 250, training loss: 322.9668884277344 = 1.7194370031356812 + 50.0 * 6.424948692321777
Epoch 250, val loss: 1.7233097553253174
Epoch 260, training loss: 322.60418701171875 = 1.708341121673584 + 50.0 * 6.417916774749756
Epoch 260, val loss: 1.7135003805160522
Epoch 270, training loss: 322.21173095703125 = 1.696354627609253 + 50.0 * 6.41030740737915
Epoch 270, val loss: 1.7030184268951416
Epoch 280, training loss: 321.90142822265625 = 1.6835509538650513 + 50.0 * 6.404357433319092
Epoch 280, val loss: 1.6918253898620605
Epoch 290, training loss: 321.6510009765625 = 1.6699880361557007 + 50.0 * 6.399620056152344
Epoch 290, val loss: 1.679947853088379
Epoch 300, training loss: 321.2682800292969 = 1.6555571556091309 + 50.0 * 6.39225435256958
Epoch 300, val loss: 1.667433738708496
Epoch 310, training loss: 320.9617004394531 = 1.6403074264526367 + 50.0 * 6.386427879333496
Epoch 310, val loss: 1.6543182134628296
Epoch 320, training loss: 320.70660400390625 = 1.624202847480774 + 50.0 * 6.381648063659668
Epoch 320, val loss: 1.640524983406067
Epoch 330, training loss: 320.40740966796875 = 1.607400894165039 + 50.0 * 6.37600040435791
Epoch 330, val loss: 1.6261810064315796
Epoch 340, training loss: 320.1484680175781 = 1.5898964405059814 + 50.0 * 6.371171474456787
Epoch 340, val loss: 1.611283540725708
Epoch 350, training loss: 319.9476623535156 = 1.5716335773468018 + 50.0 * 6.367520809173584
Epoch 350, val loss: 1.595655083656311
Epoch 360, training loss: 319.6523132324219 = 1.552677035331726 + 50.0 * 6.361992835998535
Epoch 360, val loss: 1.5797460079193115
Epoch 370, training loss: 319.4148254394531 = 1.5333079099655151 + 50.0 * 6.357630729675293
Epoch 370, val loss: 1.5635732412338257
Epoch 380, training loss: 319.4742126464844 = 1.5134552717208862 + 50.0 * 6.359215259552002
Epoch 380, val loss: 1.5472118854522705
Epoch 390, training loss: 319.0386047363281 = 1.4929839372634888 + 50.0 * 6.350912570953369
Epoch 390, val loss: 1.5300602912902832
Epoch 400, training loss: 318.7789611816406 = 1.4722636938095093 + 50.0 * 6.346134185791016
Epoch 400, val loss: 1.5129293203353882
Epoch 410, training loss: 318.5726623535156 = 1.4513705968856812 + 50.0 * 6.34242582321167
Epoch 410, val loss: 1.4957505464553833
Epoch 420, training loss: 318.63092041015625 = 1.4302831888198853 + 50.0 * 6.344012260437012
Epoch 420, val loss: 1.4784836769104004
Epoch 430, training loss: 318.2736511230469 = 1.4088072776794434 + 50.0 * 6.337296962738037
Epoch 430, val loss: 1.460920810699463
Epoch 440, training loss: 318.1379089355469 = 1.3873238563537598 + 50.0 * 6.3350114822387695
Epoch 440, val loss: 1.4435038566589355
Epoch 450, training loss: 318.01605224609375 = 1.3657995462417603 + 50.0 * 6.333004951477051
Epoch 450, val loss: 1.4261671304702759
Epoch 460, training loss: 317.7672119140625 = 1.3440378904342651 + 50.0 * 6.328464031219482
Epoch 460, val loss: 1.4089514017105103
Epoch 470, training loss: 317.5788879394531 = 1.3224483728408813 + 50.0 * 6.32512903213501
Epoch 470, val loss: 1.3917862176895142
Epoch 480, training loss: 317.4728698730469 = 1.3008053302764893 + 50.0 * 6.323441028594971
Epoch 480, val loss: 1.3748135566711426
Epoch 490, training loss: 317.3215026855469 = 1.279325246810913 + 50.0 * 6.320843696594238
Epoch 490, val loss: 1.3580046892166138
Epoch 500, training loss: 317.31866455078125 = 1.2577509880065918 + 50.0 * 6.321218490600586
Epoch 500, val loss: 1.3412963151931763
Epoch 510, training loss: 317.03204345703125 = 1.2364933490753174 + 50.0 * 6.315911293029785
Epoch 510, val loss: 1.3245352506637573
Epoch 520, training loss: 316.9839172363281 = 1.2154061794281006 + 50.0 * 6.315370559692383
Epoch 520, val loss: 1.308661937713623
Epoch 530, training loss: 316.8146667480469 = 1.1945856809616089 + 50.0 * 6.31240177154541
Epoch 530, val loss: 1.2932195663452148
Epoch 540, training loss: 316.59869384765625 = 1.1739957332611084 + 50.0 * 6.3084940910339355
Epoch 540, val loss: 1.277936577796936
Epoch 550, training loss: 316.4764404296875 = 1.1538050174713135 + 50.0 * 6.306452751159668
Epoch 550, val loss: 1.2629740238189697
Epoch 560, training loss: 316.53594970703125 = 1.133773684501648 + 50.0 * 6.308043956756592
Epoch 560, val loss: 1.2485109567642212
Epoch 570, training loss: 316.3329162597656 = 1.1140632629394531 + 50.0 * 6.30437707901001
Epoch 570, val loss: 1.234676718711853
Epoch 580, training loss: 316.1879577636719 = 1.094785213470459 + 50.0 * 6.301863193511963
Epoch 580, val loss: 1.221517562866211
Epoch 590, training loss: 316.04034423828125 = 1.0759763717651367 + 50.0 * 6.299286842346191
Epoch 590, val loss: 1.2086408138275146
Epoch 600, training loss: 315.9345703125 = 1.0575298070907593 + 50.0 * 6.297540664672852
Epoch 600, val loss: 1.1965250968933105
Epoch 610, training loss: 316.1485290527344 = 1.0395166873931885 + 50.0 * 6.302180290222168
Epoch 610, val loss: 1.1845673322677612
Epoch 620, training loss: 315.7244873046875 = 1.0214344263076782 + 50.0 * 6.294061183929443
Epoch 620, val loss: 1.1733993291854858
Epoch 630, training loss: 315.69232177734375 = 1.003967523574829 + 50.0 * 6.293766975402832
Epoch 630, val loss: 1.1625311374664307
Epoch 640, training loss: 315.567626953125 = 0.986803412437439 + 50.0 * 6.291616439819336
Epoch 640, val loss: 1.1521540880203247
Epoch 650, training loss: 315.49462890625 = 0.9699291586875916 + 50.0 * 6.290493488311768
Epoch 650, val loss: 1.142024040222168
Epoch 660, training loss: 315.4136962890625 = 0.9533193707466125 + 50.0 * 6.289207935333252
Epoch 660, val loss: 1.132633090019226
Epoch 670, training loss: 315.3086853027344 = 0.9370108842849731 + 50.0 * 6.287433624267578
Epoch 670, val loss: 1.1233826875686646
Epoch 680, training loss: 315.18548583984375 = 0.9209544062614441 + 50.0 * 6.285290241241455
Epoch 680, val loss: 1.1149177551269531
Epoch 690, training loss: 315.2051696777344 = 0.9051109552383423 + 50.0 * 6.286001205444336
Epoch 690, val loss: 1.1064069271087646
Epoch 700, training loss: 315.1468505859375 = 0.889323890209198 + 50.0 * 6.285150527954102
Epoch 700, val loss: 1.0983867645263672
Epoch 710, training loss: 314.97967529296875 = 0.8738823533058167 + 50.0 * 6.282115936279297
Epoch 710, val loss: 1.0906784534454346
Epoch 720, training loss: 314.870361328125 = 0.8586555123329163 + 50.0 * 6.280234336853027
Epoch 720, val loss: 1.0828429460525513
Epoch 730, training loss: 314.876953125 = 0.8436630964279175 + 50.0 * 6.280665874481201
Epoch 730, val loss: 1.0756278038024902
Epoch 740, training loss: 314.8736572265625 = 0.8285881280899048 + 50.0 * 6.2809014320373535
Epoch 740, val loss: 1.0685328245162964
Epoch 750, training loss: 314.7428894042969 = 0.8136844038963318 + 50.0 * 6.278584003448486
Epoch 750, val loss: 1.0615339279174805
Epoch 760, training loss: 314.612548828125 = 0.7990468740463257 + 50.0 * 6.276270389556885
Epoch 760, val loss: 1.0549596548080444
Epoch 770, training loss: 314.49383544921875 = 0.7846118807792664 + 50.0 * 6.274184703826904
Epoch 770, val loss: 1.0488331317901611
Epoch 780, training loss: 314.57720947265625 = 0.7703684568405151 + 50.0 * 6.276136875152588
Epoch 780, val loss: 1.0432698726654053
Epoch 790, training loss: 314.6102600097656 = 0.7561288475990295 + 50.0 * 6.277082443237305
Epoch 790, val loss: 1.037069320678711
Epoch 800, training loss: 314.3144836425781 = 0.7420011162757874 + 50.0 * 6.271449565887451
Epoch 800, val loss: 1.0313434600830078
Epoch 810, training loss: 314.2661437988281 = 0.7282793521881104 + 50.0 * 6.27075719833374
Epoch 810, val loss: 1.0262750387191772
Epoch 820, training loss: 314.18878173828125 = 0.7148001194000244 + 50.0 * 6.269479274749756
Epoch 820, val loss: 1.0213698148727417
Epoch 830, training loss: 314.5047607421875 = 0.7015190124511719 + 50.0 * 6.276064872741699
Epoch 830, val loss: 1.0172158479690552
Epoch 840, training loss: 314.1936950683594 = 0.6881958842277527 + 50.0 * 6.270110130310059
Epoch 840, val loss: 1.0126745700836182
Epoch 850, training loss: 314.1568908691406 = 0.6751335263252258 + 50.0 * 6.269635200500488
Epoch 850, val loss: 1.0080621242523193
Epoch 860, training loss: 313.9847106933594 = 0.6624062657356262 + 50.0 * 6.266445636749268
Epoch 860, val loss: 1.0042140483856201
Epoch 870, training loss: 313.9234924316406 = 0.6499641537666321 + 50.0 * 6.265470504760742
Epoch 870, val loss: 1.0012950897216797
Epoch 880, training loss: 314.1340026855469 = 0.6377303600311279 + 50.0 * 6.269925594329834
Epoch 880, val loss: 0.9979813098907471
Epoch 890, training loss: 313.8919982910156 = 0.6257222890853882 + 50.0 * 6.26532506942749
Epoch 890, val loss: 0.9952120184898376
Epoch 900, training loss: 313.7475280761719 = 0.6138962507247925 + 50.0 * 6.2626729011535645
Epoch 900, val loss: 0.9924783110618591
Epoch 910, training loss: 313.9000549316406 = 0.602432370185852 + 50.0 * 6.265952110290527
Epoch 910, val loss: 0.9898570775985718
Epoch 920, training loss: 313.92169189453125 = 0.5909833312034607 + 50.0 * 6.2666144371032715
Epoch 920, val loss: 0.989145815372467
Epoch 930, training loss: 313.5685119628906 = 0.5797556042671204 + 50.0 * 6.259775638580322
Epoch 930, val loss: 0.9868136048316956
Epoch 940, training loss: 313.5384521484375 = 0.5689855813980103 + 50.0 * 6.2593889236450195
Epoch 940, val loss: 0.9853259325027466
Epoch 950, training loss: 313.4747009277344 = 0.5584709644317627 + 50.0 * 6.25832462310791
Epoch 950, val loss: 0.9843675494194031
Epoch 960, training loss: 313.7479248046875 = 0.548139750957489 + 50.0 * 6.263996124267578
Epoch 960, val loss: 0.9836898446083069
Epoch 970, training loss: 313.54541015625 = 0.5379089117050171 + 50.0 * 6.260149955749512
Epoch 970, val loss: 0.9831823110580444
Epoch 980, training loss: 313.43316650390625 = 0.5278379321098328 + 50.0 * 6.258106708526611
Epoch 980, val loss: 0.9827471971511841
Epoch 990, training loss: 313.3338623046875 = 0.5180886387825012 + 50.0 * 6.256315231323242
Epoch 990, val loss: 0.9826444387435913
Epoch 1000, training loss: 313.264892578125 = 0.5085693597793579 + 50.0 * 6.255126476287842
Epoch 1000, val loss: 0.98322594165802
Epoch 1010, training loss: 313.2098693847656 = 0.4992709159851074 + 50.0 * 6.254211902618408
Epoch 1010, val loss: 0.9835805296897888
Epoch 1020, training loss: 313.26080322265625 = 0.49016469717025757 + 50.0 * 6.255412578582764
Epoch 1020, val loss: 0.9843807816505432
Epoch 1030, training loss: 313.13616943359375 = 0.4811641573905945 + 50.0 * 6.2530999183654785
Epoch 1030, val loss: 0.9849887490272522
Epoch 1040, training loss: 313.098388671875 = 0.4723235070705414 + 50.0 * 6.252521514892578
Epoch 1040, val loss: 0.9860005974769592
Epoch 1050, training loss: 313.0455322265625 = 0.4636844992637634 + 50.0 * 6.251636981964111
Epoch 1050, val loss: 0.9874144196510315
Epoch 1060, training loss: 313.30810546875 = 0.4552026391029358 + 50.0 * 6.257058143615723
Epoch 1060, val loss: 0.9884911179542542
Epoch 1070, training loss: 313.0513610839844 = 0.4467224180698395 + 50.0 * 6.2520928382873535
Epoch 1070, val loss: 0.9903351068496704
Epoch 1080, training loss: 312.955322265625 = 0.43855759501457214 + 50.0 * 6.250335216522217
Epoch 1080, val loss: 0.9921495318412781
Epoch 1090, training loss: 312.863037109375 = 0.4306238293647766 + 50.0 * 6.248648166656494
Epoch 1090, val loss: 0.9943557381629944
Epoch 1100, training loss: 312.8493347167969 = 0.422848641872406 + 50.0 * 6.24852991104126
Epoch 1100, val loss: 0.9966106414794922
Epoch 1110, training loss: 313.15960693359375 = 0.415183961391449 + 50.0 * 6.254888534545898
Epoch 1110, val loss: 0.9991026520729065
Epoch 1120, training loss: 312.9199523925781 = 0.40746307373046875 + 50.0 * 6.25024938583374
Epoch 1120, val loss: 1.0012012720108032
Epoch 1130, training loss: 312.7324523925781 = 0.39997002482414246 + 50.0 * 6.246649742126465
Epoch 1130, val loss: 1.0039432048797607
Epoch 1140, training loss: 312.7032470703125 = 0.3927048146724701 + 50.0 * 6.246211051940918
Epoch 1140, val loss: 1.0069708824157715
Epoch 1150, training loss: 312.93121337890625 = 0.3855869770050049 + 50.0 * 6.250912189483643
Epoch 1150, val loss: 1.0100828409194946
Epoch 1160, training loss: 312.7276611328125 = 0.37836748361587524 + 50.0 * 6.246986389160156
Epoch 1160, val loss: 1.0124247074127197
Epoch 1170, training loss: 312.767333984375 = 0.37144362926483154 + 50.0 * 6.247918128967285
Epoch 1170, val loss: 1.0157437324523926
Epoch 1180, training loss: 312.5767517089844 = 0.36452242732048035 + 50.0 * 6.244244575500488
Epoch 1180, val loss: 1.0194201469421387
Epoch 1190, training loss: 312.5742492675781 = 0.35784900188446045 + 50.0 * 6.244328022003174
Epoch 1190, val loss: 1.023011565208435
Epoch 1200, training loss: 312.78265380859375 = 0.3512488603591919 + 50.0 * 6.24862813949585
Epoch 1200, val loss: 1.0262244939804077
Epoch 1210, training loss: 312.5650634765625 = 0.34463852643966675 + 50.0 * 6.24440860748291
Epoch 1210, val loss: 1.0299311876296997
Epoch 1220, training loss: 312.4640197753906 = 0.3382294178009033 + 50.0 * 6.242516040802002
Epoch 1220, val loss: 1.03389573097229
Epoch 1230, training loss: 312.6446228027344 = 0.33196866512298584 + 50.0 * 6.24625301361084
Epoch 1230, val loss: 1.0381698608398438
Epoch 1240, training loss: 312.42236328125 = 0.3256766200065613 + 50.0 * 6.241933345794678
Epoch 1240, val loss: 1.040927767753601
Epoch 1250, training loss: 312.4158935546875 = 0.3195582330226898 + 50.0 * 6.241927146911621
Epoch 1250, val loss: 1.0455442667007446
Epoch 1260, training loss: 312.5043640136719 = 0.3135165870189667 + 50.0 * 6.243817329406738
Epoch 1260, val loss: 1.0489845275878906
Epoch 1270, training loss: 312.41033935546875 = 0.30754417181015015 + 50.0 * 6.242055416107178
Epoch 1270, val loss: 1.0531798601150513
Epoch 1280, training loss: 312.3087463378906 = 0.30172571539878845 + 50.0 * 6.240140438079834
Epoch 1280, val loss: 1.0573911666870117
Epoch 1290, training loss: 312.2725830078125 = 0.2959941029548645 + 50.0 * 6.239531993865967
Epoch 1290, val loss: 1.0615004301071167
Epoch 1300, training loss: 312.3238525390625 = 0.2903735041618347 + 50.0 * 6.2406697273254395
Epoch 1300, val loss: 1.065991759300232
Epoch 1310, training loss: 312.2651062011719 = 0.28480952978134155 + 50.0 * 6.239605903625488
Epoch 1310, val loss: 1.070559024810791
Epoch 1320, training loss: 312.1541442871094 = 0.27932167053222656 + 50.0 * 6.237496376037598
Epoch 1320, val loss: 1.0750633478164673
Epoch 1330, training loss: 312.24859619140625 = 0.27397507429122925 + 50.0 * 6.239491939544678
Epoch 1330, val loss: 1.0795620679855347
Epoch 1340, training loss: 312.1227722167969 = 0.26864486932754517 + 50.0 * 6.237082481384277
Epoch 1340, val loss: 1.0837244987487793
Epoch 1350, training loss: 312.2046203613281 = 0.26345518231391907 + 50.0 * 6.238822937011719
Epoch 1350, val loss: 1.0879075527191162
Epoch 1360, training loss: 312.13916015625 = 0.2583145201206207 + 50.0 * 6.237617015838623
Epoch 1360, val loss: 1.0929795503616333
Epoch 1370, training loss: 312.0449523925781 = 0.2532384395599365 + 50.0 * 6.23583459854126
Epoch 1370, val loss: 1.0973255634307861
Epoch 1380, training loss: 312.0594787597656 = 0.2483242154121399 + 50.0 * 6.236223220825195
Epoch 1380, val loss: 1.1022875308990479
Epoch 1390, training loss: 312.1380920410156 = 0.24344348907470703 + 50.0 * 6.237893104553223
Epoch 1390, val loss: 1.1069639921188354
Epoch 1400, training loss: 312.0082702636719 = 0.23864246904850006 + 50.0 * 6.2353925704956055
Epoch 1400, val loss: 1.1113089323043823
Epoch 1410, training loss: 311.9994201660156 = 0.23396801948547363 + 50.0 * 6.235308647155762
Epoch 1410, val loss: 1.1164201498031616
Epoch 1420, training loss: 312.0110778808594 = 0.22935271263122559 + 50.0 * 6.235634803771973
Epoch 1420, val loss: 1.1213033199310303
Epoch 1430, training loss: 311.9057312011719 = 0.22480915486812592 + 50.0 * 6.233618259429932
Epoch 1430, val loss: 1.1260265111923218
Epoch 1440, training loss: 311.930908203125 = 0.22037677466869354 + 50.0 * 6.234210968017578
Epoch 1440, val loss: 1.1311894655227661
Epoch 1450, training loss: 311.9350891113281 = 0.2159615457057953 + 50.0 * 6.234382152557373
Epoch 1450, val loss: 1.1356276273727417
Epoch 1460, training loss: 312.1973571777344 = 0.21161654591560364 + 50.0 * 6.239715099334717
Epoch 1460, val loss: 1.1402039527893066
Epoch 1470, training loss: 311.9053649902344 = 0.20729564130306244 + 50.0 * 6.23396110534668
Epoch 1470, val loss: 1.1458964347839355
Epoch 1480, training loss: 311.76641845703125 = 0.2031051218509674 + 50.0 * 6.231266498565674
Epoch 1480, val loss: 1.1504685878753662
Epoch 1490, training loss: 311.7183532714844 = 0.19907277822494507 + 50.0 * 6.230385780334473
Epoch 1490, val loss: 1.1561756134033203
Epoch 1500, training loss: 311.7046813964844 = 0.19509756565093994 + 50.0 * 6.230191707611084
Epoch 1500, val loss: 1.1613978147506714
Epoch 1510, training loss: 312.212158203125 = 0.19120575487613678 + 50.0 * 6.240419387817383
Epoch 1510, val loss: 1.1669714450836182
Epoch 1520, training loss: 311.9422912597656 = 0.18713800609111786 + 50.0 * 6.235103130340576
Epoch 1520, val loss: 1.1705247163772583
Epoch 1530, training loss: 311.6503601074219 = 0.18326810002326965 + 50.0 * 6.229341983795166
Epoch 1530, val loss: 1.1759108304977417
Epoch 1540, training loss: 311.6544189453125 = 0.17954109609127045 + 50.0 * 6.229497909545898
Epoch 1540, val loss: 1.1813669204711914
Epoch 1550, training loss: 311.6288757324219 = 0.1758999526500702 + 50.0 * 6.229059219360352
Epoch 1550, val loss: 1.186368465423584
Epoch 1560, training loss: 311.7792663574219 = 0.17230895161628723 + 50.0 * 6.2321391105651855
Epoch 1560, val loss: 1.1913442611694336
Epoch 1570, training loss: 311.74847412109375 = 0.16872644424438477 + 50.0 * 6.231594562530518
Epoch 1570, val loss: 1.1967146396636963
Epoch 1580, training loss: 311.6302490234375 = 0.16516190767288208 + 50.0 * 6.229301452636719
Epoch 1580, val loss: 1.2016878128051758
Epoch 1590, training loss: 311.5836486816406 = 0.16172443330287933 + 50.0 * 6.228438854217529
Epoch 1590, val loss: 1.2070162296295166
Epoch 1600, training loss: 311.5243225097656 = 0.15840184688568115 + 50.0 * 6.227318286895752
Epoch 1600, val loss: 1.2123266458511353
Epoch 1610, training loss: 311.7191467285156 = 0.155142679810524 + 50.0 * 6.2312798500061035
Epoch 1610, val loss: 1.217448353767395
Epoch 1620, training loss: 311.54400634765625 = 0.151840940117836 + 50.0 * 6.227843284606934
Epoch 1620, val loss: 1.2224680185317993
Epoch 1630, training loss: 311.49725341796875 = 0.14865273237228394 + 50.0 * 6.2269721031188965
Epoch 1630, val loss: 1.2275968790054321
Epoch 1640, training loss: 311.4886779785156 = 0.14556245505809784 + 50.0 * 6.22686243057251
Epoch 1640, val loss: 1.2326748371124268
Epoch 1650, training loss: 311.6707458496094 = 0.14251664280891418 + 50.0 * 6.230564594268799
Epoch 1650, val loss: 1.2374019622802734
Epoch 1660, training loss: 311.52630615234375 = 0.13948968052864075 + 50.0 * 6.227736473083496
Epoch 1660, val loss: 1.2431875467300415
Epoch 1670, training loss: 311.4091796875 = 0.13655054569244385 + 50.0 * 6.225452899932861
Epoch 1670, val loss: 1.248479962348938
Epoch 1680, training loss: 311.3479309082031 = 0.1337045431137085 + 50.0 * 6.224284648895264
Epoch 1680, val loss: 1.253832459449768
Epoch 1690, training loss: 311.4086608886719 = 0.13094422221183777 + 50.0 * 6.2255539894104
Epoch 1690, val loss: 1.2592476606369019
Epoch 1700, training loss: 311.5577392578125 = 0.12817880511283875 + 50.0 * 6.228591442108154
Epoch 1700, val loss: 1.264084815979004
Epoch 1710, training loss: 311.33135986328125 = 0.12541937828063965 + 50.0 * 6.224118709564209
Epoch 1710, val loss: 1.269461750984192
Epoch 1720, training loss: 311.3095703125 = 0.12280211597681046 + 50.0 * 6.223735332489014
Epoch 1720, val loss: 1.2745765447616577
Epoch 1730, training loss: 311.7307434082031 = 0.12025655806064606 + 50.0 * 6.2322096824646
Epoch 1730, val loss: 1.2801481485366821
Epoch 1740, training loss: 311.3625793457031 = 0.11768212169408798 + 50.0 * 6.224897861480713
Epoch 1740, val loss: 1.28446364402771
Epoch 1750, training loss: 311.23480224609375 = 0.11521580070257187 + 50.0 * 6.2223920822143555
Epoch 1750, val loss: 1.289920687675476
Epoch 1760, training loss: 311.2219543457031 = 0.11285790801048279 + 50.0 * 6.222182273864746
Epoch 1760, val loss: 1.2955204248428345
Epoch 1770, training loss: 311.2654113769531 = 0.11054495722055435 + 50.0 * 6.223097324371338
Epoch 1770, val loss: 1.300731897354126
Epoch 1780, training loss: 311.4086608886719 = 0.10824516415596008 + 50.0 * 6.226008415222168
Epoch 1780, val loss: 1.3057011365890503
Epoch 1790, training loss: 311.4625244140625 = 0.10598733276128769 + 50.0 * 6.227130889892578
Epoch 1790, val loss: 1.3097314834594727
Epoch 1800, training loss: 311.26104736328125 = 0.10374428331851959 + 50.0 * 6.223146438598633
Epoch 1800, val loss: 1.3155468702316284
Epoch 1810, training loss: 311.20001220703125 = 0.10160595923662186 + 50.0 * 6.221968173980713
Epoch 1810, val loss: 1.3207223415374756
Epoch 1820, training loss: 311.1479797363281 = 0.09952595084905624 + 50.0 * 6.220969200134277
Epoch 1820, val loss: 1.3260358572006226
Epoch 1830, training loss: 311.1217956542969 = 0.09752266854047775 + 50.0 * 6.220485210418701
Epoch 1830, val loss: 1.3317104578018188
Epoch 1840, training loss: 311.49127197265625 = 0.09557951986789703 + 50.0 * 6.227913856506348
Epoch 1840, val loss: 1.3374513387680054
Epoch 1850, training loss: 311.2376708984375 = 0.09355808049440384 + 50.0 * 6.22288179397583
Epoch 1850, val loss: 1.341065764427185
Epoch 1860, training loss: 311.3080139160156 = 0.09165587276220322 + 50.0 * 6.224327087402344
Epoch 1860, val loss: 1.3466969728469849
Epoch 1870, training loss: 311.11163330078125 = 0.0897851437330246 + 50.0 * 6.220437049865723
Epoch 1870, val loss: 1.3517664670944214
Epoch 1880, training loss: 311.1924743652344 = 0.08797983825206757 + 50.0 * 6.222089767456055
Epoch 1880, val loss: 1.3564152717590332
Epoch 1890, training loss: 311.0757141113281 = 0.0861949622631073 + 50.0 * 6.219790458679199
Epoch 1890, val loss: 1.362208604812622
Epoch 1900, training loss: 311.0695495605469 = 0.08448697626590729 + 50.0 * 6.219701766967773
Epoch 1900, val loss: 1.3673315048217773
Epoch 1910, training loss: 311.0419616699219 = 0.08281891793012619 + 50.0 * 6.21918249130249
Epoch 1910, val loss: 1.3725379705429077
Epoch 1920, training loss: 311.29443359375 = 0.08121661841869354 + 50.0 * 6.224264621734619
Epoch 1920, val loss: 1.378184199333191
Epoch 1930, training loss: 311.34515380859375 = 0.07957111299037933 + 50.0 * 6.225311756134033
Epoch 1930, val loss: 1.3829607963562012
Epoch 1940, training loss: 311.09130859375 = 0.07793218642473221 + 50.0 * 6.220267295837402
Epoch 1940, val loss: 1.3867899179458618
Epoch 1950, training loss: 310.9696044921875 = 0.07641131430864334 + 50.0 * 6.2178635597229
Epoch 1950, val loss: 1.392584204673767
Epoch 1960, training loss: 310.9642028808594 = 0.07494794577360153 + 50.0 * 6.217784881591797
Epoch 1960, val loss: 1.3978506326675415
Epoch 1970, training loss: 311.18109130859375 = 0.07350759208202362 + 50.0 * 6.222151756286621
Epoch 1970, val loss: 1.4029468297958374
Epoch 1980, training loss: 310.94622802734375 = 0.07204177230596542 + 50.0 * 6.2174835205078125
Epoch 1980, val loss: 1.4077900648117065
Epoch 1990, training loss: 311.01910400390625 = 0.07065250724554062 + 50.0 * 6.218969345092773
Epoch 1990, val loss: 1.4127660989761353
Epoch 2000, training loss: 310.9327087402344 = 0.06928536295890808 + 50.0 * 6.217268943786621
Epoch 2000, val loss: 1.4179898500442505
Epoch 2010, training loss: 310.91473388671875 = 0.06797239929437637 + 50.0 * 6.216935157775879
Epoch 2010, val loss: 1.4226000308990479
Epoch 2020, training loss: 311.12664794921875 = 0.06670121848583221 + 50.0 * 6.221199035644531
Epoch 2020, val loss: 1.4277691841125488
Epoch 2030, training loss: 311.01898193359375 = 0.06540965288877487 + 50.0 * 6.219071388244629
Epoch 2030, val loss: 1.4324593544006348
Epoch 2040, training loss: 310.92449951171875 = 0.06416536122560501 + 50.0 * 6.2172064781188965
Epoch 2040, val loss: 1.437074065208435
Epoch 2050, training loss: 311.0067138671875 = 0.06297247111797333 + 50.0 * 6.218875408172607
Epoch 2050, val loss: 1.4417369365692139
Epoch 2060, training loss: 310.8952941894531 = 0.06178031116724014 + 50.0 * 6.216670513153076
Epoch 2060, val loss: 1.4468612670898438
Epoch 2070, training loss: 310.86749267578125 = 0.060633689165115356 + 50.0 * 6.216136932373047
Epoch 2070, val loss: 1.4524798393249512
Epoch 2080, training loss: 310.84619140625 = 0.059515681117773056 + 50.0 * 6.215733528137207
Epoch 2080, val loss: 1.4567511081695557
Epoch 2090, training loss: 311.2067565917969 = 0.05843723937869072 + 50.0 * 6.222966194152832
Epoch 2090, val loss: 1.4615353345870972
Epoch 2100, training loss: 310.91119384765625 = 0.05733416602015495 + 50.0 * 6.217076778411865
Epoch 2100, val loss: 1.466928482055664
Epoch 2110, training loss: 310.8252868652344 = 0.05627213045954704 + 50.0 * 6.2153801918029785
Epoch 2110, val loss: 1.4711889028549194
Epoch 2120, training loss: 310.78564453125 = 0.055277757346630096 + 50.0 * 6.2146077156066895
Epoch 2120, val loss: 1.476597785949707
Epoch 2130, training loss: 310.9570007324219 = 0.054309457540512085 + 50.0 * 6.218053817749023
Epoch 2130, val loss: 1.4814690351486206
Epoch 2140, training loss: 310.7609558105469 = 0.05329672247171402 + 50.0 * 6.214153289794922
Epoch 2140, val loss: 1.4856716394424438
Epoch 2150, training loss: 310.8265380859375 = 0.05234551802277565 + 50.0 * 6.215484142303467
Epoch 2150, val loss: 1.4897665977478027
Epoch 2160, training loss: 310.93597412109375 = 0.05141570419073105 + 50.0 * 6.217690944671631
Epoch 2160, val loss: 1.4944052696228027
Epoch 2170, training loss: 310.87823486328125 = 0.05049263685941696 + 50.0 * 6.216554641723633
Epoch 2170, val loss: 1.499574065208435
Epoch 2180, training loss: 310.73748779296875 = 0.04959765449166298 + 50.0 * 6.2137579917907715
Epoch 2180, val loss: 1.5043010711669922
Epoch 2190, training loss: 310.7055969238281 = 0.048745278269052505 + 50.0 * 6.213137149810791
Epoch 2190, val loss: 1.5090755224227905
Epoch 2200, training loss: 310.8593444824219 = 0.04792023077607155 + 50.0 * 6.216228485107422
Epoch 2200, val loss: 1.5136477947235107
Epoch 2210, training loss: 310.7932434082031 = 0.047074299305677414 + 50.0 * 6.214922904968262
Epoch 2210, val loss: 1.517209768295288
Epoch 2220, training loss: 310.66510009765625 = 0.04623270034790039 + 50.0 * 6.212377071380615
Epoch 2220, val loss: 1.522093415260315
Epoch 2230, training loss: 310.74383544921875 = 0.04544953629374504 + 50.0 * 6.213967800140381
Epoch 2230, val loss: 1.5263913869857788
Epoch 2240, training loss: 310.7958984375 = 0.04468115419149399 + 50.0 * 6.215024471282959
Epoch 2240, val loss: 1.5306406021118164
Epoch 2250, training loss: 310.7187194824219 = 0.043924495577812195 + 50.0 * 6.213496208190918
Epoch 2250, val loss: 1.5364184379577637
Epoch 2260, training loss: 310.6897277832031 = 0.04318844527006149 + 50.0 * 6.212930679321289
Epoch 2260, val loss: 1.5406405925750732
Epoch 2270, training loss: 310.8622131347656 = 0.04247669875621796 + 50.0 * 6.216394424438477
Epoch 2270, val loss: 1.5446397066116333
Epoch 2280, training loss: 310.66278076171875 = 0.041742969304323196 + 50.0 * 6.21242094039917
Epoch 2280, val loss: 1.5491230487823486
Epoch 2290, training loss: 310.6233825683594 = 0.04104633629322052 + 50.0 * 6.211646556854248
Epoch 2290, val loss: 1.5526375770568848
Epoch 2300, training loss: 310.73760986328125 = 0.04038628563284874 + 50.0 * 6.213944911956787
Epoch 2300, val loss: 1.557053565979004
Epoch 2310, training loss: 310.68768310546875 = 0.039714787155389786 + 50.0 * 6.2129597663879395
Epoch 2310, val loss: 1.5611015558242798
Epoch 2320, training loss: 310.66693115234375 = 0.039064064621925354 + 50.0 * 6.212557315826416
Epoch 2320, val loss: 1.565831184387207
Epoch 2330, training loss: 310.6067199707031 = 0.03842741623520851 + 50.0 * 6.211365699768066
Epoch 2330, val loss: 1.5700674057006836
Epoch 2340, training loss: 310.5652770996094 = 0.037816766649484634 + 50.0 * 6.210549354553223
Epoch 2340, val loss: 1.5743663311004639
Epoch 2350, training loss: 310.6184997558594 = 0.03722476214170456 + 50.0 * 6.211625576019287
Epoch 2350, val loss: 1.5778225660324097
Epoch 2360, training loss: 310.68841552734375 = 0.03663289546966553 + 50.0 * 6.213035583496094
Epoch 2360, val loss: 1.5827704668045044
Epoch 2370, training loss: 310.6758117675781 = 0.03604273125529289 + 50.0 * 6.212795257568359
Epoch 2370, val loss: 1.5869197845458984
Epoch 2380, training loss: 310.557861328125 = 0.03545668348670006 + 50.0 * 6.210448265075684
Epoch 2380, val loss: 1.5909507274627686
Epoch 2390, training loss: 310.52423095703125 = 0.03491334244608879 + 50.0 * 6.209786415100098
Epoch 2390, val loss: 1.595157504081726
Epoch 2400, training loss: 310.85382080078125 = 0.03439660370349884 + 50.0 * 6.216388702392578
Epoch 2400, val loss: 1.6003755331039429
Epoch 2410, training loss: 310.5963134765625 = 0.033816464245319366 + 50.0 * 6.211250305175781
Epoch 2410, val loss: 1.6022591590881348
Epoch 2420, training loss: 310.4982604980469 = 0.03329578787088394 + 50.0 * 6.209299564361572
Epoch 2420, val loss: 1.60737144947052
Epoch 2430, training loss: 310.45367431640625 = 0.03279167413711548 + 50.0 * 6.2084174156188965
Epoch 2430, val loss: 1.6108884811401367
Epoch 2440, training loss: 310.46002197265625 = 0.03230832889676094 + 50.0 * 6.208554267883301
Epoch 2440, val loss: 1.6149687767028809
Epoch 2450, training loss: 310.8209533691406 = 0.03184150159358978 + 50.0 * 6.215782642364502
Epoch 2450, val loss: 1.6194180250167847
Epoch 2460, training loss: 310.694580078125 = 0.03132907673716545 + 50.0 * 6.2132649421691895
Epoch 2460, val loss: 1.6218398809432983
Epoch 2470, training loss: 310.572509765625 = 0.030836690217256546 + 50.0 * 6.210833549499512
Epoch 2470, val loss: 1.626634955406189
Epoch 2480, training loss: 310.4115295410156 = 0.030372975394129753 + 50.0 * 6.207623481750488
Epoch 2480, val loss: 1.6300513744354248
Epoch 2490, training loss: 310.44329833984375 = 0.029941203072667122 + 50.0 * 6.2082672119140625
Epoch 2490, val loss: 1.633501410484314
Epoch 2500, training loss: 310.841796875 = 0.0295226089656353 + 50.0 * 6.216245651245117
Epoch 2500, val loss: 1.6369074583053589
Epoch 2510, training loss: 310.5745544433594 = 0.029067017138004303 + 50.0 * 6.210909843444824
Epoch 2510, val loss: 1.6420843601226807
Epoch 2520, training loss: 310.4480285644531 = 0.02863018400967121 + 50.0 * 6.208388328552246
Epoch 2520, val loss: 1.6443166732788086
Epoch 2530, training loss: 310.4251708984375 = 0.02822537161409855 + 50.0 * 6.2079386711120605
Epoch 2530, val loss: 1.6492074728012085
Epoch 2540, training loss: 310.576171875 = 0.02783370018005371 + 50.0 * 6.210967063903809
Epoch 2540, val loss: 1.6520330905914307
Epoch 2550, training loss: 310.3864440917969 = 0.02741694450378418 + 50.0 * 6.207180500030518
Epoch 2550, val loss: 1.6560975313186646
Epoch 2560, training loss: 310.39044189453125 = 0.02702781744301319 + 50.0 * 6.207268238067627
Epoch 2560, val loss: 1.6595295667648315
Epoch 2570, training loss: 310.5379638671875 = 0.026664815843105316 + 50.0 * 6.210226058959961
Epoch 2570, val loss: 1.6634445190429688
Epoch 2580, training loss: 310.4426574707031 = 0.02626877836883068 + 50.0 * 6.208327770233154
Epoch 2580, val loss: 1.6663601398468018
Epoch 2590, training loss: 310.36553955078125 = 0.02588401362299919 + 50.0 * 6.206793308258057
Epoch 2590, val loss: 1.6696003675460815
Epoch 2600, training loss: 310.3768310546875 = 0.025531260296702385 + 50.0 * 6.207025527954102
Epoch 2600, val loss: 1.6730139255523682
Epoch 2610, training loss: 310.4678039550781 = 0.025183474645018578 + 50.0 * 6.208852291107178
Epoch 2610, val loss: 1.6761707067489624
Epoch 2620, training loss: 310.3748779296875 = 0.024833079427480698 + 50.0 * 6.207000732421875
Epoch 2620, val loss: 1.6798925399780273
Epoch 2630, training loss: 310.3793029785156 = 0.024495994672179222 + 50.0 * 6.207096099853516
Epoch 2630, val loss: 1.6837421655654907
Epoch 2640, training loss: 310.3897399902344 = 0.02416064962744713 + 50.0 * 6.207311630249023
Epoch 2640, val loss: 1.6871178150177002
Epoch 2650, training loss: 310.33905029296875 = 0.023834601044654846 + 50.0 * 6.20630407333374
Epoch 2650, val loss: 1.6904929876327515
Epoch 2660, training loss: 310.3216247558594 = 0.023516489192843437 + 50.0 * 6.205962181091309
Epoch 2660, val loss: 1.693747639656067
Epoch 2670, training loss: 310.5035095214844 = 0.023206358775496483 + 50.0 * 6.209606170654297
Epoch 2670, val loss: 1.6964712142944336
Epoch 2680, training loss: 310.3594665527344 = 0.022883877158164978 + 50.0 * 6.206731796264648
Epoch 2680, val loss: 1.6993213891983032
Epoch 2690, training loss: 310.4053649902344 = 0.022579019889235497 + 50.0 * 6.207655906677246
Epoch 2690, val loss: 1.702501893043518
Epoch 2700, training loss: 310.3335876464844 = 0.02227270044386387 + 50.0 * 6.206226348876953
Epoch 2700, val loss: 1.705878496170044
Epoch 2710, training loss: 310.25091552734375 = 0.021978408098220825 + 50.0 * 6.204578876495361
Epoch 2710, val loss: 1.7100071907043457
Epoch 2720, training loss: 310.36920166015625 = 0.021702194586396217 + 50.0 * 6.2069501876831055
Epoch 2720, val loss: 1.7127492427825928
Epoch 2730, training loss: 310.36737060546875 = 0.021412691101431847 + 50.0 * 6.206918716430664
Epoch 2730, val loss: 1.7148131132125854
Epoch 2740, training loss: 310.21685791015625 = 0.021118029952049255 + 50.0 * 6.203914642333984
Epoch 2740, val loss: 1.7189220190048218
Epoch 2750, training loss: 310.2069396972656 = 0.020853102207183838 + 50.0 * 6.20372200012207
Epoch 2750, val loss: 1.7224342823028564
Epoch 2760, training loss: 310.19573974609375 = 0.020595824345946312 + 50.0 * 6.203502655029297
Epoch 2760, val loss: 1.7252687215805054
Epoch 2770, training loss: 310.2519836425781 = 0.02034565806388855 + 50.0 * 6.20463228225708
Epoch 2770, val loss: 1.7285783290863037
Epoch 2780, training loss: 310.4731750488281 = 0.020085541531443596 + 50.0 * 6.209062099456787
Epoch 2780, val loss: 1.7311222553253174
Epoch 2790, training loss: 310.2761535644531 = 0.01982029713690281 + 50.0 * 6.205127239227295
Epoch 2790, val loss: 1.7337181568145752
Epoch 2800, training loss: 310.25286865234375 = 0.019572392106056213 + 50.0 * 6.2046661376953125
Epoch 2800, val loss: 1.7363640069961548
Epoch 2810, training loss: 310.3589172363281 = 0.0193314366042614 + 50.0 * 6.206791877746582
Epoch 2810, val loss: 1.738976001739502
Epoch 2820, training loss: 310.1992492675781 = 0.01908782869577408 + 50.0 * 6.203603744506836
Epoch 2820, val loss: 1.743513584136963
Epoch 2830, training loss: 310.16754150390625 = 0.018856817856431007 + 50.0 * 6.202973365783691
Epoch 2830, val loss: 1.7463538646697998
Epoch 2840, training loss: 310.17144775390625 = 0.018632778897881508 + 50.0 * 6.2030558586120605
Epoch 2840, val loss: 1.7490124702453613
Epoch 2850, training loss: 310.42437744140625 = 0.01841798424720764 + 50.0 * 6.208118915557861
Epoch 2850, val loss: 1.7512941360473633
Epoch 2860, training loss: 310.37213134765625 = 0.01817469671368599 + 50.0 * 6.20707893371582
Epoch 2860, val loss: 1.7538301944732666
Epoch 2870, training loss: 310.205078125 = 0.017946042120456696 + 50.0 * 6.203742980957031
Epoch 2870, val loss: 1.7573890686035156
Epoch 2880, training loss: 310.1311340332031 = 0.017736010253429413 + 50.0 * 6.202268123626709
Epoch 2880, val loss: 1.7607368230819702
Epoch 2890, training loss: 310.1242370605469 = 0.017530886456370354 + 50.0 * 6.202134609222412
Epoch 2890, val loss: 1.764078140258789
Epoch 2900, training loss: 310.3846740722656 = 0.017338264733552933 + 50.0 * 6.2073469161987305
Epoch 2900, val loss: 1.7670438289642334
Epoch 2910, training loss: 310.1361083984375 = 0.017110535874962807 + 50.0 * 6.2023797035217285
Epoch 2910, val loss: 1.7678539752960205
Epoch 2920, training loss: 310.1292724609375 = 0.01690753363072872 + 50.0 * 6.202247142791748
Epoch 2920, val loss: 1.7714581489562988
Epoch 2930, training loss: 310.1809997558594 = 0.01671474426984787 + 50.0 * 6.2032856941223145
Epoch 2930, val loss: 1.774134635925293
Epoch 2940, training loss: 310.26092529296875 = 0.016522401943802834 + 50.0 * 6.204888343811035
Epoch 2940, val loss: 1.7763622999191284
Epoch 2950, training loss: 310.23455810546875 = 0.01632818579673767 + 50.0 * 6.204364776611328
Epoch 2950, val loss: 1.7792844772338867
Epoch 2960, training loss: 310.1461486816406 = 0.01613832451403141 + 50.0 * 6.202600002288818
Epoch 2960, val loss: 1.7824273109436035
Epoch 2970, training loss: 310.0939025878906 = 0.015958884730935097 + 50.0 * 6.201559066772461
Epoch 2970, val loss: 1.7853749990463257
Epoch 2980, training loss: 310.1709289550781 = 0.01578611694276333 + 50.0 * 6.203103065490723
Epoch 2980, val loss: 1.7878830432891846
Epoch 2990, training loss: 310.14306640625 = 0.015605825930833817 + 50.0 * 6.202549457550049
Epoch 2990, val loss: 1.7907878160476685
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6592592592592593
0.8170795993674222
=== training gcn model ===
Epoch 0, training loss: 431.7911376953125 = 1.9504836797714233 + 50.0 * 8.596813201904297
Epoch 0, val loss: 1.9491857290267944
Epoch 10, training loss: 431.7342834472656 = 1.9409174919128418 + 50.0 * 8.595867156982422
Epoch 10, val loss: 1.939299464225769
Epoch 20, training loss: 431.3931884765625 = 1.9289014339447021 + 50.0 * 8.589285850524902
Epoch 20, val loss: 1.926690697669983
Epoch 30, training loss: 429.1176452636719 = 1.913848876953125 + 50.0 * 8.544075965881348
Epoch 30, val loss: 1.910827875137329
Epoch 40, training loss: 414.9388427734375 = 1.8967881202697754 + 50.0 * 8.260841369628906
Epoch 40, val loss: 1.89344322681427
Epoch 50, training loss: 380.8758239746094 = 1.8785796165466309 + 50.0 * 7.579945087432861
Epoch 50, val loss: 1.8754487037658691
Epoch 60, training loss: 367.08648681640625 = 1.8652305603027344 + 50.0 * 7.30442476272583
Epoch 60, val loss: 1.862425684928894
Epoch 70, training loss: 355.9820251464844 = 1.853575587272644 + 50.0 * 7.082569122314453
Epoch 70, val loss: 1.850961685180664
Epoch 80, training loss: 348.1636657714844 = 1.8431328535079956 + 50.0 * 6.926410675048828
Epoch 80, val loss: 1.8411049842834473
Epoch 90, training loss: 343.3046875 = 1.8337560892105103 + 50.0 * 6.829418182373047
Epoch 90, val loss: 1.8320069313049316
Epoch 100, training loss: 339.48175048828125 = 1.8246898651123047 + 50.0 * 6.753141403198242
Epoch 100, val loss: 1.8234100341796875
Epoch 110, training loss: 336.4822082519531 = 1.816455602645874 + 50.0 * 6.693315029144287
Epoch 110, val loss: 1.8156708478927612
Epoch 120, training loss: 334.17120361328125 = 1.808908224105835 + 50.0 * 6.647246360778809
Epoch 120, val loss: 1.8085575103759766
Epoch 130, training loss: 332.1445007324219 = 1.801249623298645 + 50.0 * 6.606864929199219
Epoch 130, val loss: 1.801444411277771
Epoch 140, training loss: 330.6632385253906 = 1.7934414148330688 + 50.0 * 6.577396392822266
Epoch 140, val loss: 1.7940738201141357
Epoch 150, training loss: 329.45001220703125 = 1.7852460145950317 + 50.0 * 6.553295135498047
Epoch 150, val loss: 1.7863292694091797
Epoch 160, training loss: 328.4283752441406 = 1.7767386436462402 + 50.0 * 6.5330328941345215
Epoch 160, val loss: 1.7783044576644897
Epoch 170, training loss: 327.5950012207031 = 1.7678289413452148 + 50.0 * 6.516543865203857
Epoch 170, val loss: 1.7698376178741455
Epoch 180, training loss: 326.7782287597656 = 1.7583224773406982 + 50.0 * 6.5003981590271
Epoch 180, val loss: 1.7609045505523682
Epoch 190, training loss: 326.0437927246094 = 1.748166561126709 + 50.0 * 6.485912322998047
Epoch 190, val loss: 1.7513593435287476
Epoch 200, training loss: 325.5745849609375 = 1.7372665405273438 + 50.0 * 6.476746082305908
Epoch 200, val loss: 1.7411749362945557
Epoch 210, training loss: 324.8531494140625 = 1.7255641222000122 + 50.0 * 6.462551593780518
Epoch 210, val loss: 1.7301028966903687
Epoch 220, training loss: 324.19171142578125 = 1.7129496335983276 + 50.0 * 6.449574947357178
Epoch 220, val loss: 1.718320369720459
Epoch 230, training loss: 323.70501708984375 = 1.6994012594223022 + 50.0 * 6.440112113952637
Epoch 230, val loss: 1.7056586742401123
Epoch 240, training loss: 323.2563171386719 = 1.6848126649856567 + 50.0 * 6.431429862976074
Epoch 240, val loss: 1.6920725107192993
Epoch 250, training loss: 322.796630859375 = 1.669158697128296 + 50.0 * 6.422549724578857
Epoch 250, val loss: 1.6776008605957031
Epoch 260, training loss: 322.3779602050781 = 1.6524808406829834 + 50.0 * 6.4145097732543945
Epoch 260, val loss: 1.662224292755127
Epoch 270, training loss: 322.45953369140625 = 1.6347386837005615 + 50.0 * 6.4164958000183105
Epoch 270, val loss: 1.646058440208435
Epoch 280, training loss: 321.7362976074219 = 1.6160110235214233 + 50.0 * 6.402405738830566
Epoch 280, val loss: 1.6289763450622559
Epoch 290, training loss: 321.3229064941406 = 1.596487045288086 + 50.0 * 6.394528388977051
Epoch 290, val loss: 1.6112945079803467
Epoch 300, training loss: 321.0114440917969 = 1.5761998891830444 + 50.0 * 6.388704776763916
Epoch 300, val loss: 1.5931466817855835
Epoch 310, training loss: 320.6947937011719 = 1.5551098585128784 + 50.0 * 6.382793426513672
Epoch 310, val loss: 1.5744848251342773
Epoch 320, training loss: 320.4417419433594 = 1.5334107875823975 + 50.0 * 6.378166675567627
Epoch 320, val loss: 1.555410385131836
Epoch 330, training loss: 320.1844482421875 = 1.5112416744232178 + 50.0 * 6.373464107513428
Epoch 330, val loss: 1.536121129989624
Epoch 340, training loss: 320.0353698730469 = 1.4884181022644043 + 50.0 * 6.370939254760742
Epoch 340, val loss: 1.5166077613830566
Epoch 350, training loss: 319.6248779296875 = 1.4655252695083618 + 50.0 * 6.363187313079834
Epoch 350, val loss: 1.4971164464950562
Epoch 360, training loss: 319.3746643066406 = 1.4424794912338257 + 50.0 * 6.358643531799316
Epoch 360, val loss: 1.4777946472167969
Epoch 370, training loss: 319.15423583984375 = 1.4193323850631714 + 50.0 * 6.3546977043151855
Epoch 370, val loss: 1.4587119817733765
Epoch 380, training loss: 319.23260498046875 = 1.395997166633606 + 50.0 * 6.356732368469238
Epoch 380, val loss: 1.439634084701538
Epoch 390, training loss: 318.8179931640625 = 1.3728086948394775 + 50.0 * 6.348903656005859
Epoch 390, val loss: 1.4208158254623413
Epoch 400, training loss: 318.5927429199219 = 1.3497774600982666 + 50.0 * 6.3448591232299805
Epoch 400, val loss: 1.402435302734375
Epoch 410, training loss: 318.37762451171875 = 1.3269561529159546 + 50.0 * 6.341013431549072
Epoch 410, val loss: 1.384428858757019
Epoch 420, training loss: 318.1861572265625 = 1.3043097257614136 + 50.0 * 6.337636470794678
Epoch 420, val loss: 1.366852879524231
Epoch 430, training loss: 318.5327453613281 = 1.2819299697875977 + 50.0 * 6.3450164794921875
Epoch 430, val loss: 1.3496835231781006
Epoch 440, training loss: 318.0262756347656 = 1.2594842910766602 + 50.0 * 6.335335731506348
Epoch 440, val loss: 1.3326201438903809
Epoch 450, training loss: 317.7622985839844 = 1.2375202178955078 + 50.0 * 6.330495834350586
Epoch 450, val loss: 1.3160605430603027
Epoch 460, training loss: 317.5697326660156 = 1.2160279750823975 + 50.0 * 6.32707405090332
Epoch 460, val loss: 1.3001357316970825
Epoch 470, training loss: 317.4837341308594 = 1.1948221921920776 + 50.0 * 6.325778007507324
Epoch 470, val loss: 1.2846388816833496
Epoch 480, training loss: 317.3026123046875 = 1.1739134788513184 + 50.0 * 6.322573661804199
Epoch 480, val loss: 1.2697389125823975
Epoch 490, training loss: 317.14788818359375 = 1.153414011001587 + 50.0 * 6.319889545440674
Epoch 490, val loss: 1.2553414106369019
Epoch 500, training loss: 317.12322998046875 = 1.133437991142273 + 50.0 * 6.319796085357666
Epoch 500, val loss: 1.2414277791976929
Epoch 510, training loss: 316.94403076171875 = 1.113759994506836 + 50.0 * 6.316605091094971
Epoch 510, val loss: 1.2279990911483765
Epoch 520, training loss: 316.7669982910156 = 1.0943553447723389 + 50.0 * 6.31345272064209
Epoch 520, val loss: 1.215014934539795
Epoch 530, training loss: 316.7956237792969 = 1.0755516290664673 + 50.0 * 6.314401149749756
Epoch 530, val loss: 1.2025113105773926
Epoch 540, training loss: 316.62646484375 = 1.0566338300704956 + 50.0 * 6.311396598815918
Epoch 540, val loss: 1.1905885934829712
Epoch 550, training loss: 316.40948486328125 = 1.0384594202041626 + 50.0 * 6.30742073059082
Epoch 550, val loss: 1.1789432764053345
Epoch 560, training loss: 316.57275390625 = 1.0205167531967163 + 50.0 * 6.311045169830322
Epoch 560, val loss: 1.167733073234558
Epoch 570, training loss: 316.2030944824219 = 1.0029102563858032 + 50.0 * 6.304003715515137
Epoch 570, val loss: 1.1569976806640625
Epoch 580, training loss: 316.0791931152344 = 0.9856860637664795 + 50.0 * 6.301870346069336
Epoch 580, val loss: 1.1468137502670288
Epoch 590, training loss: 316.0578918457031 = 0.9689586162567139 + 50.0 * 6.301778793334961
Epoch 590, val loss: 1.1370515823364258
Epoch 600, training loss: 315.90338134765625 = 0.9523265957832336 + 50.0 * 6.299020767211914
Epoch 600, val loss: 1.1275079250335693
Epoch 610, training loss: 315.82537841796875 = 0.9361516833305359 + 50.0 * 6.297784328460693
Epoch 610, val loss: 1.118349552154541
Epoch 620, training loss: 315.6784362792969 = 0.920299768447876 + 50.0 * 6.295162677764893
Epoch 620, val loss: 1.109785795211792
Epoch 630, training loss: 315.9025573730469 = 0.9047843217849731 + 50.0 * 6.299955368041992
Epoch 630, val loss: 1.1015920639038086
Epoch 640, training loss: 315.62646484375 = 0.8895393013954163 + 50.0 * 6.29473876953125
Epoch 640, val loss: 1.0934712886810303
Epoch 650, training loss: 315.411376953125 = 0.8746154308319092 + 50.0 * 6.290735721588135
Epoch 650, val loss: 1.085979700088501
Epoch 660, training loss: 315.45477294921875 = 0.8600878119468689 + 50.0 * 6.29189395904541
Epoch 660, val loss: 1.078784704208374
Epoch 670, training loss: 315.26068115234375 = 0.8457508683204651 + 50.0 * 6.288298606872559
Epoch 670, val loss: 1.0720385313034058
Epoch 680, training loss: 315.22210693359375 = 0.8317845463752747 + 50.0 * 6.287806510925293
Epoch 680, val loss: 1.065382480621338
Epoch 690, training loss: 315.27325439453125 = 0.8180015087127686 + 50.0 * 6.289105415344238
Epoch 690, val loss: 1.0593308210372925
Epoch 700, training loss: 315.02752685546875 = 0.8045913577079773 + 50.0 * 6.284458637237549
Epoch 700, val loss: 1.0534244775772095
Epoch 710, training loss: 314.9511413574219 = 0.7915701866149902 + 50.0 * 6.283191680908203
Epoch 710, val loss: 1.0481457710266113
Epoch 720, training loss: 314.8814392089844 = 0.7787640690803528 + 50.0 * 6.282053470611572
Epoch 720, val loss: 1.0431770086288452
Epoch 730, training loss: 315.1189270019531 = 0.7662124037742615 + 50.0 * 6.287054538726807
Epoch 730, val loss: 1.0384621620178223
Epoch 740, training loss: 314.7789001464844 = 0.7536726593971252 + 50.0 * 6.2805047035217285
Epoch 740, val loss: 1.0339853763580322
Epoch 750, training loss: 314.7675476074219 = 0.7415618300437927 + 50.0 * 6.280519962310791
Epoch 750, val loss: 1.0298771858215332
Epoch 760, training loss: 314.60595703125 = 0.7296612858772278 + 50.0 * 6.277525424957275
Epoch 760, val loss: 1.0261050462722778
Epoch 770, training loss: 314.6294250488281 = 0.7180517315864563 + 50.0 * 6.27822732925415
Epoch 770, val loss: 1.022645115852356
Epoch 780, training loss: 314.6124572753906 = 0.7065196633338928 + 50.0 * 6.278119087219238
Epoch 780, val loss: 1.0193710327148438
Epoch 790, training loss: 314.4367370605469 = 0.6950815916061401 + 50.0 * 6.2748332023620605
Epoch 790, val loss: 1.0161936283111572
Epoch 800, training loss: 314.341796875 = 0.6839174032211304 + 50.0 * 6.273157596588135
Epoch 800, val loss: 1.013443112373352
Epoch 810, training loss: 314.2797546386719 = 0.672977089881897 + 50.0 * 6.2721357345581055
Epoch 810, val loss: 1.0108379125595093
Epoch 820, training loss: 314.5340270996094 = 0.6621719002723694 + 50.0 * 6.277437210083008
Epoch 820, val loss: 1.0083650350570679
Epoch 830, training loss: 314.2680969238281 = 0.6512369513511658 + 50.0 * 6.272336959838867
Epoch 830, val loss: 1.0061578750610352
Epoch 840, training loss: 314.5667419433594 = 0.6405707597732544 + 50.0 * 6.2785234451293945
Epoch 840, val loss: 1.0039137601852417
Epoch 850, training loss: 314.2120361328125 = 0.6300291419029236 + 50.0 * 6.271639823913574
Epoch 850, val loss: 1.0018647909164429
Epoch 860, training loss: 314.0154113769531 = 0.6194800138473511 + 50.0 * 6.267918586730957
Epoch 860, val loss: 1.0001364946365356
Epoch 870, training loss: 313.93096923828125 = 0.6092216968536377 + 50.0 * 6.266434669494629
Epoch 870, val loss: 0.9985498785972595
Epoch 880, training loss: 313.88763427734375 = 0.5990311503410339 + 50.0 * 6.265771865844727
Epoch 880, val loss: 0.9970256686210632
Epoch 890, training loss: 314.2796325683594 = 0.5888373255729675 + 50.0 * 6.273816108703613
Epoch 890, val loss: 0.9955580234527588
Epoch 900, training loss: 313.94244384765625 = 0.5785332322120667 + 50.0 * 6.267278671264648
Epoch 900, val loss: 0.9937546253204346
Epoch 910, training loss: 313.7635803222656 = 0.568362295627594 + 50.0 * 6.263904571533203
Epoch 910, val loss: 0.992419421672821
Epoch 920, training loss: 313.7498474121094 = 0.5583640933036804 + 50.0 * 6.263829708099365
Epoch 920, val loss: 0.9911070466041565
Epoch 930, training loss: 313.7226867675781 = 0.548315703868866 + 50.0 * 6.263487339019775
Epoch 930, val loss: 0.989791750907898
Epoch 940, training loss: 313.7203063964844 = 0.5382248163223267 + 50.0 * 6.263641357421875
Epoch 940, val loss: 0.9884455800056458
Epoch 950, training loss: 313.5882873535156 = 0.5281826257705688 + 50.0 * 6.261202335357666
Epoch 950, val loss: 0.9870407581329346
Epoch 960, training loss: 313.5523986816406 = 0.5182548761367798 + 50.0 * 6.260683059692383
Epoch 960, val loss: 0.9859007596969604
Epoch 970, training loss: 313.59881591796875 = 0.5083580613136292 + 50.0 * 6.261809349060059
Epoch 970, val loss: 0.9846488833427429
Epoch 980, training loss: 313.5035705566406 = 0.4983271062374115 + 50.0 * 6.260105133056641
Epoch 980, val loss: 0.983214795589447
Epoch 990, training loss: 313.4140319824219 = 0.4883486032485962 + 50.0 * 6.258513927459717
Epoch 990, val loss: 0.9819788932800293
Epoch 1000, training loss: 313.355224609375 = 0.47851234674453735 + 50.0 * 6.257534027099609
Epoch 1000, val loss: 0.9807557463645935
Epoch 1010, training loss: 313.35546875 = 0.4687110185623169 + 50.0 * 6.257735252380371
Epoch 1010, val loss: 0.9796398878097534
Epoch 1020, training loss: 313.3635559082031 = 0.4588608741760254 + 50.0 * 6.25809383392334
Epoch 1020, val loss: 0.9784204363822937
Epoch 1030, training loss: 313.4428405761719 = 0.44900476932525635 + 50.0 * 6.259876728057861
Epoch 1030, val loss: 0.9773305058479309
Epoch 1040, training loss: 313.2406311035156 = 0.4392976462841034 + 50.0 * 6.256026744842529
Epoch 1040, val loss: 0.9760593175888062
Epoch 1050, training loss: 313.2499084472656 = 0.429611474275589 + 50.0 * 6.256405830383301
Epoch 1050, val loss: 0.9750492572784424
Epoch 1060, training loss: 313.1020812988281 = 0.4199850559234619 + 50.0 * 6.2536420822143555
Epoch 1060, val loss: 0.9740990400314331
Epoch 1070, training loss: 313.1334228515625 = 0.41049644351005554 + 50.0 * 6.254458427429199
Epoch 1070, val loss: 0.9732966423034668
Epoch 1080, training loss: 313.14312744140625 = 0.40111568570137024 + 50.0 * 6.254839897155762
Epoch 1080, val loss: 0.9724518060684204
Epoch 1090, training loss: 313.1427307128906 = 0.3918778896331787 + 50.0 * 6.255016803741455
Epoch 1090, val loss: 0.9717835783958435
Epoch 1100, training loss: 313.0124206542969 = 0.3827124536037445 + 50.0 * 6.252593994140625
Epoch 1100, val loss: 0.9713076949119568
Epoch 1110, training loss: 312.90594482421875 = 0.37383386492729187 + 50.0 * 6.250641822814941
Epoch 1110, val loss: 0.9708961844444275
Epoch 1120, training loss: 312.8794860839844 = 0.3651365637779236 + 50.0 * 6.250287055969238
Epoch 1120, val loss: 0.9709562063217163
Epoch 1130, training loss: 313.1161804199219 = 0.3566332161426544 + 50.0 * 6.255190849304199
Epoch 1130, val loss: 0.9709434509277344
Epoch 1140, training loss: 312.8656005859375 = 0.3481297791004181 + 50.0 * 6.250349044799805
Epoch 1140, val loss: 0.970924437046051
Epoch 1150, training loss: 312.82208251953125 = 0.33997905254364014 + 50.0 * 6.249642372131348
Epoch 1150, val loss: 0.9714213013648987
Epoch 1160, training loss: 312.9198303222656 = 0.3320266008377075 + 50.0 * 6.251756191253662
Epoch 1160, val loss: 0.9719479084014893
Epoch 1170, training loss: 312.7342834472656 = 0.32424217462539673 + 50.0 * 6.2482008934021
Epoch 1170, val loss: 0.9727436304092407
Epoch 1180, training loss: 312.78582763671875 = 0.3167434334754944 + 50.0 * 6.2493815422058105
Epoch 1180, val loss: 0.9737274050712585
Epoch 1190, training loss: 312.70654296875 = 0.30937960743904114 + 50.0 * 6.247942924499512
Epoch 1190, val loss: 0.9749684929847717
Epoch 1200, training loss: 312.60577392578125 = 0.3022816777229309 + 50.0 * 6.24606990814209
Epoch 1200, val loss: 0.9763701558113098
Epoch 1210, training loss: 312.73895263671875 = 0.29546016454696655 + 50.0 * 6.248869895935059
Epoch 1210, val loss: 0.9781356453895569
Epoch 1220, training loss: 312.6850891113281 = 0.2886751890182495 + 50.0 * 6.247928619384766
Epoch 1220, val loss: 0.9796131253242493
Epoch 1230, training loss: 312.53314208984375 = 0.2821296751499176 + 50.0 * 6.245020389556885
Epoch 1230, val loss: 0.981812059879303
Epoch 1240, training loss: 312.4402160644531 = 0.27588793635368347 + 50.0 * 6.243286609649658
Epoch 1240, val loss: 0.984169602394104
Epoch 1250, training loss: 312.5640563964844 = 0.2698274552822113 + 50.0 * 6.245884418487549
Epoch 1250, val loss: 0.9866105914115906
Epoch 1260, training loss: 312.4282531738281 = 0.2639131247997284 + 50.0 * 6.243286609649658
Epoch 1260, val loss: 0.9890993237495422
Epoch 1270, training loss: 312.416015625 = 0.258083701133728 + 50.0 * 6.243158340454102
Epoch 1270, val loss: 0.9917892813682556
Epoch 1280, training loss: 312.4404296875 = 0.25260043144226074 + 50.0 * 6.2437567710876465
Epoch 1280, val loss: 0.9948306679725647
Epoch 1290, training loss: 312.4129638671875 = 0.24712753295898438 + 50.0 * 6.243316650390625
Epoch 1290, val loss: 0.9977508783340454
Epoch 1300, training loss: 312.3419494628906 = 0.2418743371963501 + 50.0 * 6.242001056671143
Epoch 1300, val loss: 1.0010539293289185
Epoch 1310, training loss: 312.3044738769531 = 0.23679018020629883 + 50.0 * 6.241353511810303
Epoch 1310, val loss: 1.00452721118927
Epoch 1320, training loss: 312.4085388183594 = 0.23187309503555298 + 50.0 * 6.243533134460449
Epoch 1320, val loss: 1.0080500841140747
Epoch 1330, training loss: 312.2278747558594 = 0.22700877487659454 + 50.0 * 6.240016937255859
Epoch 1330, val loss: 1.0117982625961304
Epoch 1340, training loss: 312.3546142578125 = 0.22235126793384552 + 50.0 * 6.242645263671875
Epoch 1340, val loss: 1.015569806098938
Epoch 1350, training loss: 312.1827392578125 = 0.21771028637886047 + 50.0 * 6.239300727844238
Epoch 1350, val loss: 1.0194061994552612
Epoch 1360, training loss: 312.12554931640625 = 0.2132626175880432 + 50.0 * 6.238245964050293
Epoch 1360, val loss: 1.023395299911499
Epoch 1370, training loss: 312.09906005859375 = 0.20896226167678833 + 50.0 * 6.237802028656006
Epoch 1370, val loss: 1.0276226997375488
Epoch 1380, training loss: 312.20086669921875 = 0.20479510724544525 + 50.0 * 6.2399210929870605
Epoch 1380, val loss: 1.0318422317504883
Epoch 1390, training loss: 312.1002197265625 = 0.20065288245677948 + 50.0 * 6.2379913330078125
Epoch 1390, val loss: 1.035959243774414
Epoch 1400, training loss: 312.0874328613281 = 0.1965607851743698 + 50.0 * 6.237817287445068
Epoch 1400, val loss: 1.0403472185134888
Epoch 1410, training loss: 312.2071228027344 = 0.19267167150974274 + 50.0 * 6.240289211273193
Epoch 1410, val loss: 1.044895887374878
Epoch 1420, training loss: 312.22760009765625 = 0.18878528475761414 + 50.0 * 6.240776538848877
Epoch 1420, val loss: 1.0492404699325562
Epoch 1430, training loss: 312.011474609375 = 0.18500593304634094 + 50.0 * 6.236529350280762
Epoch 1430, val loss: 1.0538493394851685
Epoch 1440, training loss: 311.9354248046875 = 0.1814187914133072 + 50.0 * 6.235079765319824
Epoch 1440, val loss: 1.058716058731079
Epoch 1450, training loss: 311.89202880859375 = 0.17790576815605164 + 50.0 * 6.23428201675415
Epoch 1450, val loss: 1.0635689496994019
Epoch 1460, training loss: 311.8817443847656 = 0.17447692155838013 + 50.0 * 6.234145641326904
Epoch 1460, val loss: 1.0684376955032349
Epoch 1470, training loss: 312.45550537109375 = 0.17112813889980316 + 50.0 * 6.245687007904053
Epoch 1470, val loss: 1.0733493566513062
Epoch 1480, training loss: 311.8810729980469 = 0.16762909293174744 + 50.0 * 6.234268665313721
Epoch 1480, val loss: 1.0776066780090332
Epoch 1490, training loss: 311.870849609375 = 0.16436906158924103 + 50.0 * 6.234129428863525
Epoch 1490, val loss: 1.0828025341033936
Epoch 1500, training loss: 311.8110656738281 = 0.1612415313720703 + 50.0 * 6.232995986938477
Epoch 1500, val loss: 1.0879231691360474
Epoch 1510, training loss: 312.05877685546875 = 0.15818722546100616 + 50.0 * 6.238012313842773
Epoch 1510, val loss: 1.0930070877075195
Epoch 1520, training loss: 311.8805847167969 = 0.15512198209762573 + 50.0 * 6.234508991241455
Epoch 1520, val loss: 1.0979770421981812
Epoch 1530, training loss: 311.9155578613281 = 0.15211635828018188 + 50.0 * 6.235268592834473
Epoch 1530, val loss: 1.1029119491577148
Epoch 1540, training loss: 311.78765869140625 = 0.14917631447315216 + 50.0 * 6.23276948928833
Epoch 1540, val loss: 1.1079477071762085
Epoch 1550, training loss: 311.69281005859375 = 0.1463736742734909 + 50.0 * 6.230928897857666
Epoch 1550, val loss: 1.1132858991622925
Epoch 1560, training loss: 311.6737060546875 = 0.14365248382091522 + 50.0 * 6.2306013107299805
Epoch 1560, val loss: 1.1186493635177612
Epoch 1570, training loss: 311.98687744140625 = 0.14099612832069397 + 50.0 * 6.236917972564697
Epoch 1570, val loss: 1.1239855289459229
Epoch 1580, training loss: 311.8678283691406 = 0.13824312388896942 + 50.0 * 6.234591484069824
Epoch 1580, val loss: 1.1287572383880615
Epoch 1590, training loss: 311.70819091796875 = 0.13557329773902893 + 50.0 * 6.231452465057373
Epoch 1590, val loss: 1.134123682975769
Epoch 1600, training loss: 311.59619140625 = 0.13304956257343292 + 50.0 * 6.229262828826904
Epoch 1600, val loss: 1.1395539045333862
Epoch 1610, training loss: 311.5809020996094 = 0.13061241805553436 + 50.0 * 6.229005813598633
Epoch 1610, val loss: 1.1449886560440063
Epoch 1620, training loss: 312.2330017089844 = 0.12825904786586761 + 50.0 * 6.242094993591309
Epoch 1620, val loss: 1.1505155563354492
Epoch 1630, training loss: 311.7711486816406 = 0.1257021725177765 + 50.0 * 6.232909202575684
Epoch 1630, val loss: 1.1551978588104248
Epoch 1640, training loss: 311.5517883300781 = 0.1233840063214302 + 50.0 * 6.228568077087402
Epoch 1640, val loss: 1.1608277559280396
Epoch 1650, training loss: 311.4981384277344 = 0.1211240142583847 + 50.0 * 6.227540493011475
Epoch 1650, val loss: 1.1662477254867554
Epoch 1660, training loss: 311.73248291015625 = 0.11896664649248123 + 50.0 * 6.232270240783691
Epoch 1660, val loss: 1.1716850996017456
Epoch 1670, training loss: 311.48681640625 = 0.11668208986520767 + 50.0 * 6.227403163909912
Epoch 1670, val loss: 1.1767925024032593
Epoch 1680, training loss: 311.5107421875 = 0.11451537162065506 + 50.0 * 6.227924823760986
Epoch 1680, val loss: 1.1822760105133057
Epoch 1690, training loss: 311.6368713378906 = 0.11242015659809113 + 50.0 * 6.2304887771606445
Epoch 1690, val loss: 1.1876708269119263
Epoch 1700, training loss: 311.4707946777344 = 0.11035723239183426 + 50.0 * 6.227208614349365
Epoch 1700, val loss: 1.1932204961776733
Epoch 1710, training loss: 311.4789733886719 = 0.1083531305193901 + 50.0 * 6.227412223815918
Epoch 1710, val loss: 1.1986839771270752
Epoch 1720, training loss: 311.422607421875 = 0.10639750957489014 + 50.0 * 6.226324558258057
Epoch 1720, val loss: 1.2041622400283813
Epoch 1730, training loss: 311.4751892089844 = 0.10448974370956421 + 50.0 * 6.227413654327393
Epoch 1730, val loss: 1.2096725702285767
Epoch 1740, training loss: 311.5027160644531 = 0.1026015505194664 + 50.0 * 6.228002071380615
Epoch 1740, val loss: 1.2151378393173218
Epoch 1750, training loss: 311.5287170410156 = 0.10072886198759079 + 50.0 * 6.228559494018555
Epoch 1750, val loss: 1.2206780910491943
Epoch 1760, training loss: 311.45166015625 = 0.09887883067131042 + 50.0 * 6.227055549621582
Epoch 1760, val loss: 1.225911259651184
Epoch 1770, training loss: 311.3388977050781 = 0.09709127992391586 + 50.0 * 6.224836349487305
Epoch 1770, val loss: 1.231553554534912
Epoch 1780, training loss: 311.31988525390625 = 0.09538795053958893 + 50.0 * 6.224489688873291
Epoch 1780, val loss: 1.2372227907180786
Epoch 1790, training loss: 311.4090270996094 = 0.09372792392969131 + 50.0 * 6.226306438446045
Epoch 1790, val loss: 1.2429455518722534
Epoch 1800, training loss: 311.36761474609375 = 0.09203177690505981 + 50.0 * 6.22551155090332
Epoch 1800, val loss: 1.2481694221496582
Epoch 1810, training loss: 311.44927978515625 = 0.09035107493400574 + 50.0 * 6.22717809677124
Epoch 1810, val loss: 1.2533235549926758
Epoch 1820, training loss: 311.2891845703125 = 0.08874807506799698 + 50.0 * 6.224009037017822
Epoch 1820, val loss: 1.2589852809906006
Epoch 1830, training loss: 311.2251281738281 = 0.08717513829469681 + 50.0 * 6.222758769989014
Epoch 1830, val loss: 1.2645732164382935
Epoch 1840, training loss: 311.24884033203125 = 0.085664764046669 + 50.0 * 6.223263263702393
Epoch 1840, val loss: 1.2701308727264404
Epoch 1850, training loss: 311.47698974609375 = 0.08417715132236481 + 50.0 * 6.227856159210205
Epoch 1850, val loss: 1.275550365447998
Epoch 1860, training loss: 311.39166259765625 = 0.08266626298427582 + 50.0 * 6.226179599761963
Epoch 1860, val loss: 1.2809250354766846
Epoch 1870, training loss: 311.2867431640625 = 0.08117008209228516 + 50.0 * 6.224111557006836
Epoch 1870, val loss: 1.2862604856491089
Epoch 1880, training loss: 311.1826477050781 = 0.07976381480693817 + 50.0 * 6.222057342529297
Epoch 1880, val loss: 1.291838526725769
Epoch 1890, training loss: 311.188232421875 = 0.07839622348546982 + 50.0 * 6.222196578979492
Epoch 1890, val loss: 1.2973148822784424
Epoch 1900, training loss: 311.4002380371094 = 0.07705704122781754 + 50.0 * 6.226463794708252
Epoch 1900, val loss: 1.302647590637207
Epoch 1910, training loss: 311.2062072753906 = 0.07571589201688766 + 50.0 * 6.222609996795654
Epoch 1910, val loss: 1.3082447052001953
Epoch 1920, training loss: 311.13421630859375 = 0.07440977543592453 + 50.0 * 6.221196174621582
Epoch 1920, val loss: 1.3136903047561646
Epoch 1930, training loss: 311.16778564453125 = 0.07313688844442368 + 50.0 * 6.221892833709717
Epoch 1930, val loss: 1.3191691637039185
Epoch 1940, training loss: 311.2123107910156 = 0.07189895212650299 + 50.0 * 6.222808361053467
Epoch 1940, val loss: 1.3245717287063599
Epoch 1950, training loss: 311.2071228027344 = 0.07064907997846603 + 50.0 * 6.222729682922363
Epoch 1950, val loss: 1.3297547101974487
Epoch 1960, training loss: 311.24346923828125 = 0.06942135095596313 + 50.0 * 6.223480701446533
Epoch 1960, val loss: 1.334916353225708
Epoch 1970, training loss: 311.1134033203125 = 0.06826024502515793 + 50.0 * 6.220902919769287
Epoch 1970, val loss: 1.3407840728759766
Epoch 1980, training loss: 311.1423034667969 = 0.06710848212242126 + 50.0 * 6.221503734588623
Epoch 1980, val loss: 1.3460971117019653
Epoch 1990, training loss: 311.1253662109375 = 0.06597159802913666 + 50.0 * 6.221187591552734
Epoch 1990, val loss: 1.3515013456344604
Epoch 2000, training loss: 311.1873779296875 = 0.06484587490558624 + 50.0 * 6.2224507331848145
Epoch 2000, val loss: 1.356695294380188
Epoch 2010, training loss: 311.1136169433594 = 0.06375469267368317 + 50.0 * 6.220997333526611
Epoch 2010, val loss: 1.3621786832809448
Epoch 2020, training loss: 311.0430908203125 = 0.06267575919628143 + 50.0 * 6.219608306884766
Epoch 2020, val loss: 1.3674490451812744
Epoch 2030, training loss: 310.9913635253906 = 0.06164656579494476 + 50.0 * 6.218594074249268
Epoch 2030, val loss: 1.372917890548706
Epoch 2040, training loss: 311.0511474609375 = 0.06064791977405548 + 50.0 * 6.2198100090026855
Epoch 2040, val loss: 1.3784321546554565
Epoch 2050, training loss: 311.2113952636719 = 0.059640973806381226 + 50.0 * 6.2230353355407715
Epoch 2050, val loss: 1.383660912513733
Epoch 2060, training loss: 310.9794921875 = 0.0585881769657135 + 50.0 * 6.218418121337891
Epoch 2060, val loss: 1.3885138034820557
Epoch 2070, training loss: 310.953125 = 0.057627610862255096 + 50.0 * 6.217910289764404
Epoch 2070, val loss: 1.3939597606658936
Epoch 2080, training loss: 311.023193359375 = 0.056703753769397736 + 50.0 * 6.219329833984375
Epoch 2080, val loss: 1.3994606733322144
Epoch 2090, training loss: 310.9922180175781 = 0.0557684563100338 + 50.0 * 6.218729496002197
Epoch 2090, val loss: 1.4045764207839966
Epoch 2100, training loss: 310.96343994140625 = 0.05487095192074776 + 50.0 * 6.2181715965271
Epoch 2100, val loss: 1.4100033044815063
Epoch 2110, training loss: 311.0785217285156 = 0.05398071929812431 + 50.0 * 6.2204909324646
Epoch 2110, val loss: 1.415099024772644
Epoch 2120, training loss: 310.960693359375 = 0.053084421902894974 + 50.0 * 6.2181525230407715
Epoch 2120, val loss: 1.4201745986938477
Epoch 2130, training loss: 310.86431884765625 = 0.0522334910929203 + 50.0 * 6.216241359710693
Epoch 2130, val loss: 1.4256776571273804
Epoch 2140, training loss: 310.97259521484375 = 0.051410794258117676 + 50.0 * 6.218423366546631
Epoch 2140, val loss: 1.4307358264923096
Epoch 2150, training loss: 311.0162353515625 = 0.050578244030475616 + 50.0 * 6.219313144683838
Epoch 2150, val loss: 1.4357163906097412
Epoch 2160, training loss: 310.859130859375 = 0.04975735396146774 + 50.0 * 6.216187953948975
Epoch 2160, val loss: 1.4411205053329468
Epoch 2170, training loss: 310.7945556640625 = 0.04896525666117668 + 50.0 * 6.214911937713623
Epoch 2170, val loss: 1.4461902379989624
Epoch 2180, training loss: 310.8669128417969 = 0.0482071191072464 + 50.0 * 6.216373920440674
Epoch 2180, val loss: 1.4513380527496338
Epoch 2190, training loss: 311.0490417480469 = 0.047449078410863876 + 50.0 * 6.22003173828125
Epoch 2190, val loss: 1.4563968181610107
Epoch 2200, training loss: 310.99615478515625 = 0.046697624027729034 + 50.0 * 6.218989372253418
Epoch 2200, val loss: 1.4614044427871704
Epoch 2210, training loss: 310.8348388671875 = 0.045943260192871094 + 50.0 * 6.215778350830078
Epoch 2210, val loss: 1.4665242433547974
Epoch 2220, training loss: 310.893798828125 = 0.04524139314889908 + 50.0 * 6.216971397399902
Epoch 2220, val loss: 1.4718431234359741
Epoch 2230, training loss: 310.8105773925781 = 0.044537004083395004 + 50.0 * 6.215321063995361
Epoch 2230, val loss: 1.4767869710922241
Epoch 2240, training loss: 310.7946472167969 = 0.043859053403139114 + 50.0 * 6.215015888214111
Epoch 2240, val loss: 1.4818413257598877
Epoch 2250, training loss: 310.77691650390625 = 0.04319613054394722 + 50.0 * 6.214674949645996
Epoch 2250, val loss: 1.4869099855422974
Epoch 2260, training loss: 310.9502258300781 = 0.04254124313592911 + 50.0 * 6.218153476715088
Epoch 2260, val loss: 1.4918413162231445
Epoch 2270, training loss: 310.93603515625 = 0.041857652366161346 + 50.0 * 6.217883110046387
Epoch 2270, val loss: 1.4963407516479492
Epoch 2280, training loss: 310.7681579589844 = 0.041211098432540894 + 50.0 * 6.214539051055908
Epoch 2280, val loss: 1.5014300346374512
Epoch 2290, training loss: 310.7675476074219 = 0.040576834231615067 + 50.0 * 6.214539051055908
Epoch 2290, val loss: 1.5062711238861084
Epoch 2300, training loss: 310.7835693359375 = 0.0399814248085022 + 50.0 * 6.214871883392334
Epoch 2300, val loss: 1.5112853050231934
Epoch 2310, training loss: 310.73876953125 = 0.03937864303588867 + 50.0 * 6.213988304138184
Epoch 2310, val loss: 1.5161386728286743
Epoch 2320, training loss: 310.8353576660156 = 0.03879885748028755 + 50.0 * 6.215931415557861
Epoch 2320, val loss: 1.5208520889282227
Epoch 2330, training loss: 310.7325439453125 = 0.03819985315203667 + 50.0 * 6.213886737823486
Epoch 2330, val loss: 1.5253251791000366
Epoch 2340, training loss: 310.679443359375 = 0.0376381054520607 + 50.0 * 6.212836265563965
Epoch 2340, val loss: 1.5302878618240356
Epoch 2350, training loss: 310.67193603515625 = 0.037096843123435974 + 50.0 * 6.212696552276611
Epoch 2350, val loss: 1.5352658033370972
Epoch 2360, training loss: 310.7037353515625 = 0.03655390813946724 + 50.0 * 6.213343620300293
Epoch 2360, val loss: 1.5397595167160034
Epoch 2370, training loss: 310.8614501953125 = 0.03601660951972008 + 50.0 * 6.216508865356445
Epoch 2370, val loss: 1.5443753004074097
Epoch 2380, training loss: 310.7155456542969 = 0.03549528494477272 + 50.0 * 6.213601112365723
Epoch 2380, val loss: 1.5494093894958496
Epoch 2390, training loss: 310.84521484375 = 0.03496938198804855 + 50.0 * 6.21620512008667
Epoch 2390, val loss: 1.5537878274917603
Epoch 2400, training loss: 310.5913391113281 = 0.03445529192686081 + 50.0 * 6.211137771606445
Epoch 2400, val loss: 1.5585718154907227
Epoch 2410, training loss: 310.6033630371094 = 0.03395847976207733 + 50.0 * 6.211387634277344
Epoch 2410, val loss: 1.5631622076034546
Epoch 2420, training loss: 310.7483825683594 = 0.033488936722278595 + 50.0 * 6.214298248291016
Epoch 2420, val loss: 1.567941427230835
Epoch 2430, training loss: 310.6965637207031 = 0.03301007300615311 + 50.0 * 6.213271141052246
Epoch 2430, val loss: 1.5721325874328613
Epoch 2440, training loss: 310.59271240234375 = 0.03251756355166435 + 50.0 * 6.211203575134277
Epoch 2440, val loss: 1.5764867067337036
Epoch 2450, training loss: 310.5457458496094 = 0.03206835687160492 + 50.0 * 6.210273742675781
Epoch 2450, val loss: 1.5815054178237915
Epoch 2460, training loss: 310.541259765625 = 0.03163165971636772 + 50.0 * 6.2101922035217285
Epoch 2460, val loss: 1.585956335067749
Epoch 2470, training loss: 310.6977233886719 = 0.031209832057356834 + 50.0 * 6.213330268859863
Epoch 2470, val loss: 1.5906648635864258
Epoch 2480, training loss: 310.661376953125 = 0.030765395611524582 + 50.0 * 6.212612152099609
Epoch 2480, val loss: 1.594780683517456
Epoch 2490, training loss: 310.5234375 = 0.030304381623864174 + 50.0 * 6.20986270904541
Epoch 2490, val loss: 1.5991747379302979
Epoch 2500, training loss: 310.5438232421875 = 0.02989482693374157 + 50.0 * 6.210278034210205
Epoch 2500, val loss: 1.6035208702087402
Epoch 2510, training loss: 310.84393310546875 = 0.029488058760762215 + 50.0 * 6.216289043426514
Epoch 2510, val loss: 1.6078554391860962
Epoch 2520, training loss: 310.60894775390625 = 0.0290682353079319 + 50.0 * 6.211597442626953
Epoch 2520, val loss: 1.6120893955230713
Epoch 2530, training loss: 310.4869689941406 = 0.02867082878947258 + 50.0 * 6.209166526794434
Epoch 2530, val loss: 1.6165997982025146
Epoch 2540, training loss: 310.4857482910156 = 0.0282962154597044 + 50.0 * 6.20914888381958
Epoch 2540, val loss: 1.6210874319076538
Epoch 2550, training loss: 310.81396484375 = 0.0279267318546772 + 50.0 * 6.2157206535339355
Epoch 2550, val loss: 1.6253188848495483
Epoch 2560, training loss: 310.59637451171875 = 0.027534399181604385 + 50.0 * 6.211377143859863
Epoch 2560, val loss: 1.629231572151184
Epoch 2570, training loss: 310.6152038574219 = 0.02715885266661644 + 50.0 * 6.211760997772217
Epoch 2570, val loss: 1.63339364528656
Epoch 2580, training loss: 310.48712158203125 = 0.026794595643877983 + 50.0 * 6.209206581115723
Epoch 2580, val loss: 1.6377438306808472
Epoch 2590, training loss: 310.45684814453125 = 0.026446085423231125 + 50.0 * 6.2086076736450195
Epoch 2590, val loss: 1.6420698165893555
Epoch 2600, training loss: 310.5994873046875 = 0.026115600019693375 + 50.0 * 6.211467266082764
Epoch 2600, val loss: 1.646461844444275
Epoch 2610, training loss: 310.6427917480469 = 0.025767777115106583 + 50.0 * 6.212340831756592
Epoch 2610, val loss: 1.6503535509109497
Epoch 2620, training loss: 310.492431640625 = 0.025404170155525208 + 50.0 * 6.209340572357178
Epoch 2620, val loss: 1.6542781591415405
Epoch 2630, training loss: 310.41363525390625 = 0.025061244145035744 + 50.0 * 6.2077717781066895
Epoch 2630, val loss: 1.6581932306289673
Epoch 2640, training loss: 310.3800354003906 = 0.024747446179389954 + 50.0 * 6.20710563659668
Epoch 2640, val loss: 1.66262686252594
Epoch 2650, training loss: 310.4000244140625 = 0.02444140985608101 + 50.0 * 6.2075114250183105
Epoch 2650, val loss: 1.6667624711990356
Epoch 2660, training loss: 310.7764587402344 = 0.02413983829319477 + 50.0 * 6.215046405792236
Epoch 2660, val loss: 1.6706609725952148
Epoch 2670, training loss: 310.5282897949219 = 0.02381843887269497 + 50.0 * 6.210089683532715
Epoch 2670, val loss: 1.6746071577072144
Epoch 2680, training loss: 310.40216064453125 = 0.023507939651608467 + 50.0 * 6.207572937011719
Epoch 2680, val loss: 1.678723931312561
Epoch 2690, training loss: 310.4661865234375 = 0.023227032274007797 + 50.0 * 6.208858966827393
Epoch 2690, val loss: 1.682942509651184
Epoch 2700, training loss: 310.45001220703125 = 0.022936642169952393 + 50.0 * 6.208541393280029
Epoch 2700, val loss: 1.6866477727890015
Epoch 2710, training loss: 310.40185546875 = 0.022635988891124725 + 50.0 * 6.207584381103516
Epoch 2710, val loss: 1.6902421712875366
Epoch 2720, training loss: 310.3958435058594 = 0.022361744195222855 + 50.0 * 6.207469463348389
Epoch 2720, val loss: 1.694442629814148
Epoch 2730, training loss: 310.5032653808594 = 0.02209422178566456 + 50.0 * 6.209623336791992
Epoch 2730, val loss: 1.6984045505523682
Epoch 2740, training loss: 310.4736328125 = 0.021805478259921074 + 50.0 * 6.209036350250244
Epoch 2740, val loss: 1.7019559144973755
Epoch 2750, training loss: 310.364990234375 = 0.021533804014325142 + 50.0 * 6.206868648529053
Epoch 2750, val loss: 1.705798625946045
Epoch 2760, training loss: 310.5472717285156 = 0.0212776567786932 + 50.0 * 6.210519790649414
Epoch 2760, val loss: 1.7097899913787842
Epoch 2770, training loss: 310.3202819824219 = 0.021002311259508133 + 50.0 * 6.2059855461120605
Epoch 2770, val loss: 1.7131576538085938
Epoch 2780, training loss: 310.3101806640625 = 0.020757384598255157 + 50.0 * 6.205788612365723
Epoch 2780, val loss: 1.7174075841903687
Epoch 2790, training loss: 310.27886962890625 = 0.020512014627456665 + 50.0 * 6.205167293548584
Epoch 2790, val loss: 1.7211887836456299
Epoch 2800, training loss: 310.35223388671875 = 0.02028132975101471 + 50.0 * 6.206638813018799
Epoch 2800, val loss: 1.725079894065857
Epoch 2810, training loss: 310.6595153808594 = 0.020031346008181572 + 50.0 * 6.212790012359619
Epoch 2810, val loss: 1.7283138036727905
Epoch 2820, training loss: 310.3515930175781 = 0.01976967602968216 + 50.0 * 6.206636428833008
Epoch 2820, val loss: 1.7319813966751099
Epoch 2830, training loss: 310.2859191894531 = 0.019527247175574303 + 50.0 * 6.20532751083374
Epoch 2830, val loss: 1.7355164289474487
Epoch 2840, training loss: 310.260986328125 = 0.0193099994212389 + 50.0 * 6.204833507537842
Epoch 2840, val loss: 1.7396074533462524
Epoch 2850, training loss: 310.2607727050781 = 0.01909138821065426 + 50.0 * 6.204833507537842
Epoch 2850, val loss: 1.7431915998458862
Epoch 2860, training loss: 310.62445068359375 = 0.018885916098952293 + 50.0 * 6.212110996246338
Epoch 2860, val loss: 1.7469172477722168
Epoch 2870, training loss: 310.3162841796875 = 0.018648067489266396 + 50.0 * 6.2059526443481445
Epoch 2870, val loss: 1.7501776218414307
Epoch 2880, training loss: 310.2120056152344 = 0.018428349867463112 + 50.0 * 6.203871250152588
Epoch 2880, val loss: 1.7537882328033447
Epoch 2890, training loss: 310.3313293457031 = 0.018224501982331276 + 50.0 * 6.206262111663818
Epoch 2890, val loss: 1.7572506666183472
Epoch 2900, training loss: 310.2961120605469 = 0.018009399995207787 + 50.0 * 6.205562114715576
Epoch 2900, val loss: 1.7605853080749512
Epoch 2910, training loss: 310.2228088378906 = 0.017795123159885406 + 50.0 * 6.204100608825684
Epoch 2910, val loss: 1.7641657590866089
Epoch 2920, training loss: 310.2131652832031 = 0.01759863831102848 + 50.0 * 6.203910827636719
Epoch 2920, val loss: 1.7678782939910889
Epoch 2930, training loss: 310.4965515136719 = 0.017418086528778076 + 50.0 * 6.209582328796387
Epoch 2930, val loss: 1.771631121635437
Epoch 2940, training loss: 310.29132080078125 = 0.017210157588124275 + 50.0 * 6.205482006072998
Epoch 2940, val loss: 1.774515986442566
Epoch 2950, training loss: 310.1850891113281 = 0.017008492723107338 + 50.0 * 6.203361511230469
Epoch 2950, val loss: 1.7780910730361938
Epoch 2960, training loss: 310.2088317871094 = 0.016827497631311417 + 50.0 * 6.203840255737305
Epoch 2960, val loss: 1.7813864946365356
Epoch 2970, training loss: 310.3072509765625 = 0.01664636842906475 + 50.0 * 6.205812454223633
Epoch 2970, val loss: 1.7848901748657227
Epoch 2980, training loss: 310.1949157714844 = 0.016466105356812477 + 50.0 * 6.203568458557129
Epoch 2980, val loss: 1.7883238792419434
Epoch 2990, training loss: 310.4287414550781 = 0.016297675669193268 + 50.0 * 6.208248615264893
Epoch 2990, val loss: 1.7916709184646606
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6666666666666667
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 431.7785949707031 = 1.9363224506378174 + 50.0 * 8.596845626831055
Epoch 0, val loss: 1.9341132640838623
Epoch 10, training loss: 431.7373046875 = 1.9274866580963135 + 50.0 * 8.596196174621582
Epoch 10, val loss: 1.924377679824829
Epoch 20, training loss: 431.5288391113281 = 1.9164565801620483 + 50.0 * 8.59224796295166
Epoch 20, val loss: 1.9120054244995117
Epoch 30, training loss: 430.27606201171875 = 1.902353048324585 + 50.0 * 8.567474365234375
Epoch 30, val loss: 1.8961869478225708
Epoch 40, training loss: 423.60821533203125 = 1.8861653804779053 + 50.0 * 8.434440612792969
Epoch 40, val loss: 1.8790870904922485
Epoch 50, training loss: 395.40362548828125 = 1.8692699670791626 + 50.0 * 7.870687484741211
Epoch 50, val loss: 1.8608866930007935
Epoch 60, training loss: 373.1330871582031 = 1.8539396524429321 + 50.0 * 7.4255828857421875
Epoch 60, val loss: 1.8455702066421509
Epoch 70, training loss: 357.1394958496094 = 1.8443142175674438 + 50.0 * 7.1059041023254395
Epoch 70, val loss: 1.836017370223999
Epoch 80, training loss: 347.53594970703125 = 1.8350640535354614 + 50.0 * 6.914018154144287
Epoch 80, val loss: 1.8269578218460083
Epoch 90, training loss: 342.0629577636719 = 1.8272160291671753 + 50.0 * 6.804714679718018
Epoch 90, val loss: 1.8192137479782104
Epoch 100, training loss: 338.7326354980469 = 1.8187427520751953 + 50.0 * 6.738277435302734
Epoch 100, val loss: 1.8108197450637817
Epoch 110, training loss: 336.3636779785156 = 1.810758352279663 + 50.0 * 6.69105863571167
Epoch 110, val loss: 1.8029890060424805
Epoch 120, training loss: 334.4239501953125 = 1.8035887479782104 + 50.0 * 6.652407169342041
Epoch 120, val loss: 1.7961393594741821
Epoch 130, training loss: 332.6792907714844 = 1.796898365020752 + 50.0 * 6.617647647857666
Epoch 130, val loss: 1.7897727489471436
Epoch 140, training loss: 331.22528076171875 = 1.7901570796966553 + 50.0 * 6.58870267868042
Epoch 140, val loss: 1.7833387851715088
Epoch 150, training loss: 330.0471496582031 = 1.7829904556274414 + 50.0 * 6.565283298492432
Epoch 150, val loss: 1.7766671180725098
Epoch 160, training loss: 329.05938720703125 = 1.7753512859344482 + 50.0 * 6.545680522918701
Epoch 160, val loss: 1.7696541547775269
Epoch 170, training loss: 328.1714782714844 = 1.7671140432357788 + 50.0 * 6.528087139129639
Epoch 170, val loss: 1.7622512578964233
Epoch 180, training loss: 327.4012451171875 = 1.7582542896270752 + 50.0 * 6.51285982131958
Epoch 180, val loss: 1.7544046640396118
Epoch 190, training loss: 326.6685791015625 = 1.748755931854248 + 50.0 * 6.498396396636963
Epoch 190, val loss: 1.7460933923721313
Epoch 200, training loss: 326.12298583984375 = 1.7384294271469116 + 50.0 * 6.4876909255981445
Epoch 200, val loss: 1.7370455265045166
Epoch 210, training loss: 325.4183349609375 = 1.7270534038543701 + 50.0 * 6.473825931549072
Epoch 210, val loss: 1.7273186445236206
Epoch 220, training loss: 324.8290100097656 = 1.7147719860076904 + 50.0 * 6.462284564971924
Epoch 220, val loss: 1.7166686058044434
Epoch 230, training loss: 324.4068603515625 = 1.701403260231018 + 50.0 * 6.454109191894531
Epoch 230, val loss: 1.7051620483398438
Epoch 240, training loss: 323.8628234863281 = 1.6867222785949707 + 50.0 * 6.443521499633789
Epoch 240, val loss: 1.6926621198654175
Epoch 250, training loss: 323.4429016113281 = 1.6708942651748657 + 50.0 * 6.4354400634765625
Epoch 250, val loss: 1.6791151762008667
Epoch 260, training loss: 323.1068115234375 = 1.6538690328598022 + 50.0 * 6.42905855178833
Epoch 260, val loss: 1.6646406650543213
Epoch 270, training loss: 322.69708251953125 = 1.6356141567230225 + 50.0 * 6.421229362487793
Epoch 270, val loss: 1.6492819786071777
Epoch 280, training loss: 322.4656677246094 = 1.6163376569747925 + 50.0 * 6.41698694229126
Epoch 280, val loss: 1.633044719696045
Epoch 290, training loss: 321.9766845703125 = 1.5959290266036987 + 50.0 * 6.4076151847839355
Epoch 290, val loss: 1.6161866188049316
Epoch 300, training loss: 321.6341857910156 = 1.5747156143188477 + 50.0 * 6.401189804077148
Epoch 300, val loss: 1.598824143409729
Epoch 310, training loss: 321.49456787109375 = 1.55279541015625 + 50.0 * 6.3988356590271
Epoch 310, val loss: 1.580937385559082
Epoch 320, training loss: 321.0457763671875 = 1.5300580263137817 + 50.0 * 6.39031457901001
Epoch 320, val loss: 1.5626648664474487
Epoch 330, training loss: 320.6933288574219 = 1.5068423748016357 + 50.0 * 6.383729934692383
Epoch 330, val loss: 1.5442777872085571
Epoch 340, training loss: 320.4113464355469 = 1.4833259582519531 + 50.0 * 6.3785600662231445
Epoch 340, val loss: 1.5259721279144287
Epoch 350, training loss: 320.29449462890625 = 1.459546446800232 + 50.0 * 6.376698970794678
Epoch 350, val loss: 1.5075445175170898
Epoch 360, training loss: 319.9526062011719 = 1.4355347156524658 + 50.0 * 6.3703413009643555
Epoch 360, val loss: 1.489203691482544
Epoch 370, training loss: 319.6718444824219 = 1.4116032123565674 + 50.0 * 6.36520528793335
Epoch 370, val loss: 1.4712560176849365
Epoch 380, training loss: 319.4462890625 = 1.3878737688064575 + 50.0 * 6.361167907714844
Epoch 380, val loss: 1.4537222385406494
Epoch 390, training loss: 319.4103088378906 = 1.3642164468765259 + 50.0 * 6.360921382904053
Epoch 390, val loss: 1.436336874961853
Epoch 400, training loss: 319.0693664550781 = 1.3407883644104004 + 50.0 * 6.354571342468262
Epoch 400, val loss: 1.4194495677947998
Epoch 410, training loss: 318.86944580078125 = 1.3177106380462646 + 50.0 * 6.351034641265869
Epoch 410, val loss: 1.4030972719192505
Epoch 420, training loss: 318.68121337890625 = 1.295059323310852 + 50.0 * 6.34772253036499
Epoch 420, val loss: 1.3873769044876099
Epoch 430, training loss: 318.5711364746094 = 1.2727487087249756 + 50.0 * 6.345967769622803
Epoch 430, val loss: 1.37211012840271
Epoch 440, training loss: 318.54754638671875 = 1.2506635189056396 + 50.0 * 6.345937252044678
Epoch 440, val loss: 1.3571704626083374
Epoch 450, training loss: 318.2485656738281 = 1.2289551496505737 + 50.0 * 6.340392589569092
Epoch 450, val loss: 1.3426167964935303
Epoch 460, training loss: 318.04632568359375 = 1.2076932191848755 + 50.0 * 6.336772441864014
Epoch 460, val loss: 1.3286792039871216
Epoch 470, training loss: 317.894287109375 = 1.186934471130371 + 50.0 * 6.3341474533081055
Epoch 470, val loss: 1.3152769804000854
Epoch 480, training loss: 318.0547790527344 = 1.16650390625 + 50.0 * 6.337765216827393
Epoch 480, val loss: 1.3021143674850464
Epoch 490, training loss: 317.6813049316406 = 1.146083950996399 + 50.0 * 6.330704212188721
Epoch 490, val loss: 1.2891900539398193
Epoch 500, training loss: 317.5382080078125 = 1.1262168884277344 + 50.0 * 6.328239440917969
Epoch 500, val loss: 1.276803970336914
Epoch 510, training loss: 317.3785095214844 = 1.1068476438522339 + 50.0 * 6.325433254241943
Epoch 510, val loss: 1.2651320695877075
Epoch 520, training loss: 317.2284240722656 = 1.087887167930603 + 50.0 * 6.322810649871826
Epoch 520, val loss: 1.2538872957229614
Epoch 530, training loss: 317.1112365722656 = 1.069299340248108 + 50.0 * 6.320838928222656
Epoch 530, val loss: 1.2429783344268799
Epoch 540, training loss: 317.1566467285156 = 1.050910234451294 + 50.0 * 6.322114944458008
Epoch 540, val loss: 1.2321311235427856
Epoch 550, training loss: 316.97259521484375 = 1.0327863693237305 + 50.0 * 6.318796157836914
Epoch 550, val loss: 1.221780776977539
Epoch 560, training loss: 316.80389404296875 = 1.0151770114898682 + 50.0 * 6.315774440765381
Epoch 560, val loss: 1.211889624595642
Epoch 570, training loss: 316.7477111816406 = 0.998009204864502 + 50.0 * 6.314993858337402
Epoch 570, val loss: 1.2025010585784912
Epoch 580, training loss: 316.59442138671875 = 0.9810170531272888 + 50.0 * 6.312268257141113
Epoch 580, val loss: 1.1932456493377686
Epoch 590, training loss: 316.53912353515625 = 0.964418888092041 + 50.0 * 6.31149435043335
Epoch 590, val loss: 1.1842639446258545
Epoch 600, training loss: 316.4815673828125 = 0.948197066783905 + 50.0 * 6.310667037963867
Epoch 600, val loss: 1.1757102012634277
Epoch 610, training loss: 316.3023376464844 = 0.9323242902755737 + 50.0 * 6.307400703430176
Epoch 610, val loss: 1.1674976348876953
Epoch 620, training loss: 316.295654296875 = 0.916829526424408 + 50.0 * 6.3075761795043945
Epoch 620, val loss: 1.1596393585205078
Epoch 630, training loss: 316.1127014160156 = 0.90153568983078 + 50.0 * 6.30422306060791
Epoch 630, val loss: 1.1521700620651245
Epoch 640, training loss: 316.00732421875 = 0.8866413831710815 + 50.0 * 6.3024139404296875
Epoch 640, val loss: 1.1451020240783691
Epoch 650, training loss: 315.9039611816406 = 0.8720765709877014 + 50.0 * 6.300637722015381
Epoch 650, val loss: 1.1381522417068481
Epoch 660, training loss: 315.9783020019531 = 0.8578492999076843 + 50.0 * 6.3024091720581055
Epoch 660, val loss: 1.1315661668777466
Epoch 670, training loss: 315.9858093261719 = 0.8435976505279541 + 50.0 * 6.302844524383545
Epoch 670, val loss: 1.1253716945648193
Epoch 680, training loss: 315.67059326171875 = 0.8296526074409485 + 50.0 * 6.296818733215332
Epoch 680, val loss: 1.1191109418869019
Epoch 690, training loss: 315.5355529785156 = 0.8161725997924805 + 50.0 * 6.2943878173828125
Epoch 690, val loss: 1.1133657693862915
Epoch 700, training loss: 315.5694885253906 = 0.8029584288597107 + 50.0 * 6.29533052444458
Epoch 700, val loss: 1.1078226566314697
Epoch 710, training loss: 315.6783752441406 = 0.7896563410758972 + 50.0 * 6.297774791717529
Epoch 710, val loss: 1.1020451784133911
Epoch 720, training loss: 315.3908996582031 = 0.7765722274780273 + 50.0 * 6.292286396026611
Epoch 720, val loss: 1.0967974662780762
Epoch 730, training loss: 315.2566833496094 = 0.7639344334602356 + 50.0 * 6.289854526519775
Epoch 730, val loss: 1.0919831991195679
Epoch 740, training loss: 315.16180419921875 = 0.7515864372253418 + 50.0 * 6.288204193115234
Epoch 740, val loss: 1.0875357389450073
Epoch 750, training loss: 315.0952453613281 = 0.7394043207168579 + 50.0 * 6.287116527557373
Epoch 750, val loss: 1.08334481716156
Epoch 760, training loss: 315.2169189453125 = 0.727249026298523 + 50.0 * 6.289793491363525
Epoch 760, val loss: 1.078985571861267
Epoch 770, training loss: 315.0679016113281 = 0.7151418328285217 + 50.0 * 6.287055015563965
Epoch 770, val loss: 1.0748252868652344
Epoch 780, training loss: 314.8831481933594 = 0.7034156918525696 + 50.0 * 6.283594131469727
Epoch 780, val loss: 1.0710768699645996
Epoch 790, training loss: 314.815185546875 = 0.691961407661438 + 50.0 * 6.282464504241943
Epoch 790, val loss: 1.0676835775375366
Epoch 800, training loss: 315.1910705566406 = 0.6806688904762268 + 50.0 * 6.290207862854004
Epoch 800, val loss: 1.0643993616104126
Epoch 810, training loss: 314.7200012207031 = 0.6693381071090698 + 50.0 * 6.281013488769531
Epoch 810, val loss: 1.0610178709030151
Epoch 820, training loss: 314.6549377441406 = 0.6583031415939331 + 50.0 * 6.279932975769043
Epoch 820, val loss: 1.0580281019210815
Epoch 830, training loss: 314.6340637207031 = 0.6474825143814087 + 50.0 * 6.279731750488281
Epoch 830, val loss: 1.05531907081604
Epoch 840, training loss: 314.5334777832031 = 0.6366726756095886 + 50.0 * 6.2779364585876465
Epoch 840, val loss: 1.0525920391082764
Epoch 850, training loss: 314.4936828613281 = 0.6260238289833069 + 50.0 * 6.277352809906006
Epoch 850, val loss: 1.0502302646636963
Epoch 860, training loss: 314.384521484375 = 0.6156406402587891 + 50.0 * 6.2753777503967285
Epoch 860, val loss: 1.0481692552566528
Epoch 870, training loss: 314.4571533203125 = 0.6053959727287292 + 50.0 * 6.277034759521484
Epoch 870, val loss: 1.0461143255233765
Epoch 880, training loss: 314.330322265625 = 0.5951301455497742 + 50.0 * 6.2747039794921875
Epoch 880, val loss: 1.0443052053451538
Epoch 890, training loss: 314.2556457519531 = 0.5850173234939575 + 50.0 * 6.273412227630615
Epoch 890, val loss: 1.042378306388855
Epoch 900, training loss: 314.1778564453125 = 0.5751276612281799 + 50.0 * 6.272054672241211
Epoch 900, val loss: 1.0412148237228394
Epoch 910, training loss: 314.2309265136719 = 0.5653608441352844 + 50.0 * 6.273311138153076
Epoch 910, val loss: 1.0397489070892334
Epoch 920, training loss: 314.0861511230469 = 0.5555818676948547 + 50.0 * 6.270611763000488
Epoch 920, val loss: 1.0387706756591797
Epoch 930, training loss: 314.4067687988281 = 0.5458928942680359 + 50.0 * 6.277217388153076
Epoch 930, val loss: 1.037876844406128
Epoch 940, training loss: 314.01446533203125 = 0.5362312197685242 + 50.0 * 6.269565105438232
Epoch 940, val loss: 1.0365004539489746
Epoch 950, training loss: 313.9187316894531 = 0.5268809795379639 + 50.0 * 6.267837047576904
Epoch 950, val loss: 1.036189317703247
Epoch 960, training loss: 313.83380126953125 = 0.5177068114280701 + 50.0 * 6.266322135925293
Epoch 960, val loss: 1.036065936088562
Epoch 970, training loss: 313.79644775390625 = 0.5086455941200256 + 50.0 * 6.265756130218506
Epoch 970, val loss: 1.0358936786651611
Epoch 980, training loss: 314.10906982421875 = 0.4996720850467682 + 50.0 * 6.272188186645508
Epoch 980, val loss: 1.0359678268432617
Epoch 990, training loss: 313.9468994140625 = 0.4905882179737091 + 50.0 * 6.269125938415527
Epoch 990, val loss: 1.0359303951263428
Epoch 1000, training loss: 313.65472412109375 = 0.48165199160575867 + 50.0 * 6.263461112976074
Epoch 1000, val loss: 1.0359792709350586
Epoch 1010, training loss: 313.7368469238281 = 0.47299790382385254 + 50.0 * 6.26527738571167
Epoch 1010, val loss: 1.0367852449417114
Epoch 1020, training loss: 313.6468200683594 = 0.46432384848594666 + 50.0 * 6.263649940490723
Epoch 1020, val loss: 1.0372968912124634
Epoch 1030, training loss: 313.5709228515625 = 0.45573458075523376 + 50.0 * 6.262303352355957
Epoch 1030, val loss: 1.037861943244934
Epoch 1040, training loss: 313.4979553222656 = 0.4473804235458374 + 50.0 * 6.261011123657227
Epoch 1040, val loss: 1.0390491485595703
Epoch 1050, training loss: 313.44927978515625 = 0.43915674090385437 + 50.0 * 6.260202407836914
Epoch 1050, val loss: 1.0403550863265991
Epoch 1060, training loss: 313.86395263671875 = 0.4310276210308075 + 50.0 * 6.268658638000488
Epoch 1060, val loss: 1.0413105487823486
Epoch 1070, training loss: 313.52691650390625 = 0.4227541983127594 + 50.0 * 6.262083053588867
Epoch 1070, val loss: 1.0430786609649658
Epoch 1080, training loss: 313.3768310546875 = 0.4147365689277649 + 50.0 * 6.259242057800293
Epoch 1080, val loss: 1.0446783304214478
Epoch 1090, training loss: 313.3566589355469 = 0.40689826011657715 + 50.0 * 6.258995056152344
Epoch 1090, val loss: 1.0461840629577637
Epoch 1100, training loss: 313.28143310546875 = 0.3991548418998718 + 50.0 * 6.257645130157471
Epoch 1100, val loss: 1.0483380556106567
Epoch 1110, training loss: 313.26434326171875 = 0.39155513048171997 + 50.0 * 6.257456302642822
Epoch 1110, val loss: 1.0504088401794434
Epoch 1120, training loss: 313.18017578125 = 0.3840163052082062 + 50.0 * 6.255923271179199
Epoch 1120, val loss: 1.0524067878723145
Epoch 1130, training loss: 313.35003662109375 = 0.37662607431411743 + 50.0 * 6.2594685554504395
Epoch 1130, val loss: 1.0547035932540894
Epoch 1140, training loss: 313.3046875 = 0.369189977645874 + 50.0 * 6.25870943069458
Epoch 1140, val loss: 1.0565816164016724
Epoch 1150, training loss: 313.1018981933594 = 0.3618919253349304 + 50.0 * 6.254800319671631
Epoch 1150, val loss: 1.0590219497680664
Epoch 1160, training loss: 313.0116882324219 = 0.35483866930007935 + 50.0 * 6.253137111663818
Epoch 1160, val loss: 1.061655044555664
Epoch 1170, training loss: 313.0317687988281 = 0.34793269634246826 + 50.0 * 6.253676891326904
Epoch 1170, val loss: 1.0646179914474487
Epoch 1180, training loss: 313.1562805175781 = 0.3410453796386719 + 50.0 * 6.256304740905762
Epoch 1180, val loss: 1.066817283630371
Epoch 1190, training loss: 312.95843505859375 = 0.3341643214225769 + 50.0 * 6.252485275268555
Epoch 1190, val loss: 1.0696853399276733
Epoch 1200, training loss: 312.8772888183594 = 0.3275376260280609 + 50.0 * 6.250994682312012
Epoch 1200, val loss: 1.072472095489502
Epoch 1210, training loss: 312.8547668457031 = 0.32105907797813416 + 50.0 * 6.250674724578857
Epoch 1210, val loss: 1.0758851766586304
Epoch 1220, training loss: 313.1478271484375 = 0.3146962821483612 + 50.0 * 6.256662845611572
Epoch 1220, val loss: 1.0789803266525269
Epoch 1230, training loss: 312.957763671875 = 0.30824849009513855 + 50.0 * 6.252990245819092
Epoch 1230, val loss: 1.0813934803009033
Epoch 1240, training loss: 312.8997802734375 = 0.3019844889640808 + 50.0 * 6.251956462860107
Epoch 1240, val loss: 1.0847373008728027
Epoch 1250, training loss: 312.73974609375 = 0.2958638370037079 + 50.0 * 6.24887752532959
Epoch 1250, val loss: 1.0882972478866577
Epoch 1260, training loss: 312.7254638671875 = 0.2899276316165924 + 50.0 * 6.248711109161377
Epoch 1260, val loss: 1.091750979423523
Epoch 1270, training loss: 312.6858215332031 = 0.28412380814552307 + 50.0 * 6.2480340003967285
Epoch 1270, val loss: 1.0954774618148804
Epoch 1280, training loss: 312.9143371582031 = 0.27837973833084106 + 50.0 * 6.252718925476074
Epoch 1280, val loss: 1.0989872217178345
Epoch 1290, training loss: 312.7289733886719 = 0.27260690927505493 + 50.0 * 6.249127388000488
Epoch 1290, val loss: 1.1020196676254272
Epoch 1300, training loss: 312.6147766113281 = 0.2670416533946991 + 50.0 * 6.246954917907715
Epoch 1300, val loss: 1.1058099269866943
Epoch 1310, training loss: 312.6256103515625 = 0.2616479992866516 + 50.0 * 6.247279167175293
Epoch 1310, val loss: 1.1097288131713867
Epoch 1320, training loss: 312.5918273925781 = 0.256290465593338 + 50.0 * 6.246710777282715
Epoch 1320, val loss: 1.1131739616394043
Epoch 1330, training loss: 312.57110595703125 = 0.2509952783584595 + 50.0 * 6.246402263641357
Epoch 1330, val loss: 1.1171677112579346
Epoch 1340, training loss: 312.5374450683594 = 0.24586546421051025 + 50.0 * 6.24583101272583
Epoch 1340, val loss: 1.1210700273513794
Epoch 1350, training loss: 312.5109558105469 = 0.2408672571182251 + 50.0 * 6.245401382446289
Epoch 1350, val loss: 1.1249828338623047
Epoch 1360, training loss: 312.4248352050781 = 0.23598578572273254 + 50.0 * 6.243776798248291
Epoch 1360, val loss: 1.129306435585022
Epoch 1370, training loss: 312.4818115234375 = 0.23121732473373413 + 50.0 * 6.245011806488037
Epoch 1370, val loss: 1.1332484483718872
Epoch 1380, training loss: 312.54168701171875 = 0.2265041619539261 + 50.0 * 6.246303558349609
Epoch 1380, val loss: 1.1372599601745605
Epoch 1390, training loss: 312.42718505859375 = 0.22182869911193848 + 50.0 * 6.244107246398926
Epoch 1390, val loss: 1.1413542032241821
Epoch 1400, training loss: 312.45819091796875 = 0.21731801331043243 + 50.0 * 6.244817733764648
Epoch 1400, val loss: 1.145174503326416
Epoch 1410, training loss: 312.32366943359375 = 0.21286433935165405 + 50.0 * 6.242216110229492
Epoch 1410, val loss: 1.1496464014053345
Epoch 1420, training loss: 312.3019104003906 = 0.20856691896915436 + 50.0 * 6.2418670654296875
Epoch 1420, val loss: 1.1536705493927002
Epoch 1430, training loss: 312.3270568847656 = 0.20438511669635773 + 50.0 * 6.242453575134277
Epoch 1430, val loss: 1.1581650972366333
Epoch 1440, training loss: 312.3046569824219 = 0.20024210214614868 + 50.0 * 6.2420878410339355
Epoch 1440, val loss: 1.1620590686798096
Epoch 1450, training loss: 312.2871398925781 = 0.19618605077266693 + 50.0 * 6.241818904876709
Epoch 1450, val loss: 1.1661510467529297
Epoch 1460, training loss: 312.3488464355469 = 0.19223305583000183 + 50.0 * 6.2431321144104
Epoch 1460, val loss: 1.170570969581604
Epoch 1470, training loss: 312.1853942871094 = 0.18831951916217804 + 50.0 * 6.239941596984863
Epoch 1470, val loss: 1.1748602390289307
Epoch 1480, training loss: 312.1363830566406 = 0.1845756322145462 + 50.0 * 6.2390360832214355
Epoch 1480, val loss: 1.1793228387832642
Epoch 1490, training loss: 312.1640319824219 = 0.1809297353029251 + 50.0 * 6.239662170410156
Epoch 1490, val loss: 1.1835498809814453
Epoch 1500, training loss: 312.2309875488281 = 0.17733225226402283 + 50.0 * 6.241073131561279
Epoch 1500, val loss: 1.1882498264312744
Epoch 1510, training loss: 312.1221008300781 = 0.17378143966197968 + 50.0 * 6.238966464996338
Epoch 1510, val loss: 1.1924948692321777
Epoch 1520, training loss: 312.1520690917969 = 0.17037807404994965 + 50.0 * 6.239634037017822
Epoch 1520, val loss: 1.1971380710601807
Epoch 1530, training loss: 312.147216796875 = 0.16699738800525665 + 50.0 * 6.239604473114014
Epoch 1530, val loss: 1.201292872428894
Epoch 1540, training loss: 311.98309326171875 = 0.16366122663021088 + 50.0 * 6.236388683319092
Epoch 1540, val loss: 1.2054615020751953
Epoch 1550, training loss: 311.9601745605469 = 0.16048555076122284 + 50.0 * 6.235993385314941
Epoch 1550, val loss: 1.2101389169692993
Epoch 1560, training loss: 312.0554504394531 = 0.15739883482456207 + 50.0 * 6.2379608154296875
Epoch 1560, val loss: 1.2148579359054565
Epoch 1570, training loss: 311.9532775878906 = 0.15431784093379974 + 50.0 * 6.235979080200195
Epoch 1570, val loss: 1.2191543579101562
Epoch 1580, training loss: 311.9422607421875 = 0.15130926668643951 + 50.0 * 6.235819339752197
Epoch 1580, val loss: 1.2238274812698364
Epoch 1590, training loss: 311.8863830566406 = 0.14840777218341827 + 50.0 * 6.23475980758667
Epoch 1590, val loss: 1.2281233072280884
Epoch 1600, training loss: 311.9472351074219 = 0.14558759331703186 + 50.0 * 6.236032485961914
Epoch 1600, val loss: 1.2329744100570679
Epoch 1610, training loss: 312.17535400390625 = 0.1427658647298813 + 50.0 * 6.240652084350586
Epoch 1610, val loss: 1.2372997999191284
Epoch 1620, training loss: 311.880126953125 = 0.13994036614894867 + 50.0 * 6.234803199768066
Epoch 1620, val loss: 1.241283655166626
Epoch 1630, training loss: 311.8279724121094 = 0.13729268312454224 + 50.0 * 6.233813762664795
Epoch 1630, val loss: 1.246293067932129
Epoch 1640, training loss: 311.76275634765625 = 0.1347479671239853 + 50.0 * 6.232560157775879
Epoch 1640, val loss: 1.2513606548309326
Epoch 1650, training loss: 311.7458190917969 = 0.1322636604309082 + 50.0 * 6.232271194458008
Epoch 1650, val loss: 1.2558432817459106
