Begin epxeriment: cont_weight: 2 epoch:1000 der1:0.3 der2:0 dfr1:0.4 dfr2:0.4
./run_pre_noisy.py:18: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
Namespace(add_edge_rate_1=0.0, add_edge_rate_2=0.0, cl_activation='relu', cl_base_model='GCNConv', cl_lr=0.0001, cl_num_epochs=1000, cl_num_hidden=128, cl_num_layers=2, cl_num_proj_hidden=128, cl_weight_decay=1e-05, config='config.yaml', cont_batch_size=0, cont_weight=2.0, cuda=True, dataset='Cora', debug=True, defense_mode='none', device_id=1, dis_weight=1, drop_edge_rate_1=0.3, drop_edge_rate_2=0.0, drop_feat_rate_1=0.4, drop_feat_rate_2=0.4, dropout=0.5, epochs=200, evaluate_mode='1by1', gpu_id=0, hidden=128, homo_boost_thrd=0.5, homo_loss_weight=100.0, inner=1, lr=0.01, model='GCN', no_cuda=False, proj_hidden=128, prune_thr=0.2, seed=10, selection_method='cluster_degree', target_class=0, target_loss_weight=1, tau=0.4, temperature=0.5, test_model='GCN', thrd=0.5, train_lr=0.01, trigger_size=3, trojan_epochs=200, use_vs_number=True, vs_number=5, vs_ratio=0, weight_decay=0.0005)
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 11590])
remove edge: torch.Size([2, 9562])
updated graph: torch.Size([2, 10596])
=== Noisy graph ===
/home/mfl5681/project-contrastive/RobustCL/model.py:224: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.labels = torch.tensor(labels, dtype=torch.long)
=== training gcn model ===
Epoch 0, training loss: 19.147708892822266 = 1.9539997577667236 + 2.0 * 8.596854209899902
Epoch 0, val loss: 1.9589495658874512
Epoch 10, training loss: 19.14235496520996 = 1.948805570602417 + 2.0 * 8.59677505493164
Epoch 10, val loss: 1.9535439014434814
Epoch 20, training loss: 19.13614845275879 = 1.943095326423645 + 2.0 * 8.596526145935059
Epoch 20, val loss: 1.9475783109664917
Epoch 30, training loss: 19.1275634765625 = 1.9363796710968018 + 2.0 * 8.59559154510498
Epoch 30, val loss: 1.9405282735824585
Epoch 40, training loss: 19.111562728881836 = 1.9280667304992676 + 2.0 * 8.591748237609863
Epoch 40, val loss: 1.9317231178283691
Epoch 50, training loss: 19.072397232055664 = 1.9175775051116943 + 2.0 * 8.577409744262695
Epoch 50, val loss: 1.9206300973892212
Epoch 60, training loss: 18.970687866210938 = 1.905003309249878 + 2.0 * 8.532842636108398
Epoch 60, val loss: 1.907612681388855
Epoch 70, training loss: 18.68638038635254 = 1.891837239265442 + 2.0 * 8.397271156311035
Epoch 70, val loss: 1.8944954872131348
Epoch 80, training loss: 18.034299850463867 = 1.8780001401901245 + 2.0 * 8.078149795532227
Epoch 80, val loss: 1.8808635473251343
Epoch 90, training loss: 17.729755401611328 = 1.8653240203857422 + 2.0 * 7.932215213775635
Epoch 90, val loss: 1.8688606023788452
Epoch 100, training loss: 17.137144088745117 = 1.8563631772994995 + 2.0 * 7.640390396118164
Epoch 100, val loss: 1.8606082201004028
Epoch 110, training loss: 16.457536697387695 = 1.8495690822601318 + 2.0 * 7.30398416519165
Epoch 110, val loss: 1.8542358875274658
Epoch 120, training loss: 15.96820068359375 = 1.8446581363677979 + 2.0 * 7.061771392822266
Epoch 120, val loss: 1.8496465682983398
Epoch 130, training loss: 15.683032989501953 = 1.8399882316589355 + 2.0 * 6.92152214050293
Epoch 130, val loss: 1.8452297449111938
Epoch 140, training loss: 15.488754272460938 = 1.8343781232833862 + 2.0 * 6.827188014984131
Epoch 140, val loss: 1.8399585485458374
Epoch 150, training loss: 15.35438346862793 = 1.8279788494110107 + 2.0 * 6.76320219039917
Epoch 150, val loss: 1.8339173793792725
Epoch 160, training loss: 15.248432159423828 = 1.8213183879852295 + 2.0 * 6.71355676651001
Epoch 160, val loss: 1.8277055025100708
Epoch 170, training loss: 15.144630432128906 = 1.8150020837783813 + 2.0 * 6.664813995361328
Epoch 170, val loss: 1.8217097520828247
Epoch 180, training loss: 15.048349380493164 = 1.8090262413024902 + 2.0 * 6.619661331176758
Epoch 180, val loss: 1.8161126375198364
Epoch 190, training loss: 14.96760082244873 = 1.8032560348510742 + 2.0 * 6.582172393798828
Epoch 190, val loss: 1.8106361627578735
Epoch 200, training loss: 14.907135009765625 = 1.7974299192428589 + 2.0 * 6.554852485656738
Epoch 200, val loss: 1.8051061630249023
Epoch 210, training loss: 14.853891372680664 = 1.7914023399353027 + 2.0 * 6.531244277954102
Epoch 210, val loss: 1.7995550632476807
Epoch 220, training loss: 14.809231758117676 = 1.785279393196106 + 2.0 * 6.51197624206543
Epoch 220, val loss: 1.7939683198928833
Epoch 230, training loss: 14.770349502563477 = 1.7790731191635132 + 2.0 * 6.495638370513916
Epoch 230, val loss: 1.7883297204971313
Epoch 240, training loss: 14.737613677978516 = 1.772709608078003 + 2.0 * 6.482451915740967
Epoch 240, val loss: 1.78260338306427
Epoch 250, training loss: 14.704389572143555 = 1.7661257982254028 + 2.0 * 6.469131946563721
Epoch 250, val loss: 1.7767356634140015
Epoch 260, training loss: 14.675371170043945 = 1.7592114210128784 + 2.0 * 6.458079814910889
Epoch 260, val loss: 1.7706573009490967
Epoch 270, training loss: 14.650142669677734 = 1.7518998384475708 + 2.0 * 6.449121475219727
Epoch 270, val loss: 1.7642929553985596
Epoch 280, training loss: 14.624080657958984 = 1.744153380393982 + 2.0 * 6.4399638175964355
Epoch 280, val loss: 1.7576206922531128
Epoch 290, training loss: 14.598257064819336 = 1.735943078994751 + 2.0 * 6.431157112121582
Epoch 290, val loss: 1.7505621910095215
Epoch 300, training loss: 14.574474334716797 = 1.7271521091461182 + 2.0 * 6.423661231994629
Epoch 300, val loss: 1.7430570125579834
Epoch 310, training loss: 14.552680969238281 = 1.71773362159729 + 2.0 * 6.417473793029785
Epoch 310, val loss: 1.7350502014160156
Epoch 320, training loss: 14.528043746948242 = 1.7076486349105835 + 2.0 * 6.410197734832764
Epoch 320, val loss: 1.7265262603759766
Epoch 330, training loss: 14.504292488098145 = 1.6968237161636353 + 2.0 * 6.40373420715332
Epoch 330, val loss: 1.7173856496810913
Epoch 340, training loss: 14.482467651367188 = 1.6851922273635864 + 2.0 * 6.398637771606445
Epoch 340, val loss: 1.7075824737548828
Epoch 350, training loss: 14.461943626403809 = 1.6727596521377563 + 2.0 * 6.394591808319092
Epoch 350, val loss: 1.6971129179000854
Epoch 360, training loss: 14.435563087463379 = 1.6595828533172607 + 2.0 * 6.3879899978637695
Epoch 360, val loss: 1.6859749555587769
Epoch 370, training loss: 14.411643981933594 = 1.6455378532409668 + 2.0 * 6.383053302764893
Epoch 370, val loss: 1.6741021871566772
Epoch 380, training loss: 14.387574195861816 = 1.6305805444717407 + 2.0 * 6.3784966468811035
Epoch 380, val loss: 1.6614387035369873
Epoch 390, training loss: 14.363937377929688 = 1.614661693572998 + 2.0 * 6.374637603759766
Epoch 390, val loss: 1.6479114294052124
Epoch 400, training loss: 14.340265274047852 = 1.5978412628173828 + 2.0 * 6.371212005615234
Epoch 400, val loss: 1.6336169242858887
Epoch 410, training loss: 14.316081047058105 = 1.5803180932998657 + 2.0 * 6.3678812980651855
Epoch 410, val loss: 1.6187273263931274
Epoch 420, training loss: 14.290231704711914 = 1.5620756149291992 + 2.0 * 6.364078044891357
Epoch 420, val loss: 1.6032685041427612
Epoch 430, training loss: 14.262535095214844 = 1.5431911945343018 + 2.0 * 6.3596720695495605
Epoch 430, val loss: 1.5872048139572144
Epoch 440, training loss: 14.237838745117188 = 1.5236451625823975 + 2.0 * 6.3570966720581055
Epoch 440, val loss: 1.5706313848495483
Epoch 450, training loss: 14.212615966796875 = 1.503675937652588 + 2.0 * 6.354470252990723
Epoch 450, val loss: 1.5537832975387573
Epoch 460, training loss: 14.184098243713379 = 1.4833544492721558 + 2.0 * 6.350371837615967
Epoch 460, val loss: 1.536584496498108
Epoch 470, training loss: 14.157404899597168 = 1.4626721143722534 + 2.0 * 6.3473663330078125
Epoch 470, val loss: 1.5191115140914917
Epoch 480, training loss: 14.130964279174805 = 1.4416817426681519 + 2.0 * 6.344641208648682
Epoch 480, val loss: 1.5014506578445435
Epoch 490, training loss: 14.104822158813477 = 1.4205780029296875 + 2.0 * 6.3421220779418945
Epoch 490, val loss: 1.4837745428085327
Epoch 500, training loss: 14.079099655151367 = 1.3995721340179443 + 2.0 * 6.339763641357422
Epoch 500, val loss: 1.4663270711898804
Epoch 510, training loss: 14.052143096923828 = 1.3786555528640747 + 2.0 * 6.3367438316345215
Epoch 510, val loss: 1.449050784111023
Epoch 520, training loss: 14.034911155700684 = 1.3578808307647705 + 2.0 * 6.338515281677246
Epoch 520, val loss: 1.4320183992385864
Epoch 530, training loss: 14.002158164978027 = 1.337501883506775 + 2.0 * 6.3323283195495605
Epoch 530, val loss: 1.4154630899429321
Epoch 540, training loss: 13.977007865905762 = 1.3173894882202148 + 2.0 * 6.329809188842773
Epoch 540, val loss: 1.399254560470581
Epoch 550, training loss: 13.95240306854248 = 1.2974977493286133 + 2.0 * 6.327452659606934
Epoch 550, val loss: 1.3833540678024292
Epoch 560, training loss: 13.930662155151367 = 1.2778167724609375 + 2.0 * 6.326422691345215
Epoch 560, val loss: 1.3678113222122192
Epoch 570, training loss: 13.912217140197754 = 1.2585983276367188 + 2.0 * 6.326809406280518
Epoch 570, val loss: 1.3528167009353638
Epoch 580, training loss: 13.884506225585938 = 1.2397819757461548 + 2.0 * 6.322361946105957
Epoch 580, val loss: 1.3383135795593262
Epoch 590, training loss: 13.860620498657227 = 1.2212388515472412 + 2.0 * 6.319690704345703
Epoch 590, val loss: 1.3241952657699585
Epoch 600, training loss: 13.838587760925293 = 1.2029236555099487 + 2.0 * 6.317831993103027
Epoch 600, val loss: 1.3103474378585815
Epoch 610, training loss: 13.817399978637695 = 1.1848152875900269 + 2.0 * 6.3162922859191895
Epoch 610, val loss: 1.2967573404312134
Epoch 620, training loss: 13.804206848144531 = 1.1669553518295288 + 2.0 * 6.3186259269714355
Epoch 620, val loss: 1.2835474014282227
Epoch 630, training loss: 13.775725364685059 = 1.149444341659546 + 2.0 * 6.313140392303467
Epoch 630, val loss: 1.2707704305648804
Epoch 640, training loss: 13.755937576293945 = 1.132163643836975 + 2.0 * 6.311886787414551
Epoch 640, val loss: 1.2582283020019531
Epoch 650, training loss: 13.735095977783203 = 1.1150339841842651 + 2.0 * 6.310030937194824
Epoch 650, val loss: 1.2458497285842896
Epoch 660, training loss: 13.720218658447266 = 1.0980868339538574 + 2.0 * 6.311066150665283
Epoch 660, val loss: 1.2336972951889038
Epoch 670, training loss: 13.69696044921875 = 1.0813944339752197 + 2.0 * 6.307783126831055
Epoch 670, val loss: 1.2218681573867798
Epoch 680, training loss: 13.676976203918457 = 1.064850926399231 + 2.0 * 6.306062698364258
Epoch 680, val loss: 1.2102574110031128
Epoch 690, training loss: 13.657904624938965 = 1.0484243631362915 + 2.0 * 6.304739952087402
Epoch 690, val loss: 1.1987559795379639
Epoch 700, training loss: 13.64366340637207 = 1.0321269035339355 + 2.0 * 6.305768013000488
Epoch 700, val loss: 1.1873894929885864
Epoch 710, training loss: 13.621113777160645 = 1.016026496887207 + 2.0 * 6.302543640136719
Epoch 710, val loss: 1.1762216091156006
Epoch 720, training loss: 13.602346420288086 = 1.0000413656234741 + 2.0 * 6.30115270614624
Epoch 720, val loss: 1.1651941537857056
Epoch 730, training loss: 13.583871841430664 = 0.9841372966766357 + 2.0 * 6.299867153167725
Epoch 730, val loss: 1.1542555093765259
Epoch 740, training loss: 13.57451057434082 = 0.9683321118354797 + 2.0 * 6.303089141845703
Epoch 740, val loss: 1.143432855606079
Epoch 750, training loss: 13.548421859741211 = 0.9527382850646973 + 2.0 * 6.297841548919678
Epoch 750, val loss: 1.1328339576721191
Epoch 760, training loss: 13.530564308166504 = 0.9372503757476807 + 2.0 * 6.296657085418701
Epoch 760, val loss: 1.1223362684249878
Epoch 770, training loss: 13.512609481811523 = 0.9218284487724304 + 2.0 * 6.295390605926514
Epoch 770, val loss: 1.1119040250778198
Epoch 780, training loss: 13.502928733825684 = 0.906485378742218 + 2.0 * 6.298221588134766
Epoch 780, val loss: 1.10158109664917
Epoch 790, training loss: 13.481605529785156 = 0.8913445472717285 + 2.0 * 6.295130729675293
Epoch 790, val loss: 1.0914192199707031
Epoch 800, training loss: 13.461393356323242 = 0.8763394355773926 + 2.0 * 6.292527198791504
Epoch 800, val loss: 1.0813994407653809
Epoch 810, training loss: 13.44441032409668 = 0.8614362478256226 + 2.0 * 6.291487216949463
Epoch 810, val loss: 1.0714844465255737
Epoch 820, training loss: 13.428181648254395 = 0.8466177582740784 + 2.0 * 6.2907819747924805
Epoch 820, val loss: 1.0616638660430908
Epoch 830, training loss: 13.415182113647461 = 0.8319335579872131 + 2.0 * 6.291624069213867
Epoch 830, val loss: 1.0519628524780273
Epoch 840, training loss: 13.395954132080078 = 0.8174408674240112 + 2.0 * 6.289256572723389
Epoch 840, val loss: 1.042462944984436
Epoch 850, training loss: 13.378889083862305 = 0.8030853867530823 + 2.0 * 6.287901878356934
Epoch 850, val loss: 1.033127784729004
Epoch 860, training loss: 13.368958473205566 = 0.7888656854629517 + 2.0 * 6.290046215057373
Epoch 860, val loss: 1.0239008665084839
Epoch 870, training loss: 13.350301742553711 = 0.7748544812202454 + 2.0 * 6.287723541259766
Epoch 870, val loss: 1.014836072921753
Epoch 880, training loss: 13.332216262817383 = 0.7610509991645813 + 2.0 * 6.285582542419434
Epoch 880, val loss: 1.0060677528381348
Epoch 890, training loss: 13.316139221191406 = 0.7474099397659302 + 2.0 * 6.284364700317383
Epoch 890, val loss: 0.9974619746208191
Epoch 900, training loss: 13.300986289978027 = 0.7339339852333069 + 2.0 * 6.2835259437561035
Epoch 900, val loss: 0.9890410304069519
Epoch 910, training loss: 13.296065330505371 = 0.7206212282180786 + 2.0 * 6.287722110748291
Epoch 910, val loss: 0.9808427691459656
Epoch 920, training loss: 13.274039268493652 = 0.7076350450515747 + 2.0 * 6.283202171325684
Epoch 920, val loss: 0.9728910326957703
Epoch 930, training loss: 13.258997917175293 = 0.6949028372764587 + 2.0 * 6.282047748565674
Epoch 930, val loss: 0.9652754068374634
Epoch 940, training loss: 13.244221687316895 = 0.6823912858963013 + 2.0 * 6.280915260314941
Epoch 940, val loss: 0.9578999876976013
Epoch 950, training loss: 13.229783058166504 = 0.6700878143310547 + 2.0 * 6.279847621917725
Epoch 950, val loss: 0.9507849812507629
Epoch 960, training loss: 13.217113494873047 = 0.6579928994178772 + 2.0 * 6.279560089111328
Epoch 960, val loss: 0.9439281225204468
Epoch 970, training loss: 13.209912300109863 = 0.6461517810821533 + 2.0 * 6.2818803787231445
Epoch 970, val loss: 0.9373746514320374
Epoch 980, training loss: 13.194527626037598 = 0.6346560120582581 + 2.0 * 6.279935836791992
Epoch 980, val loss: 0.9311091899871826
Epoch 990, training loss: 13.178582191467285 = 0.623421311378479 + 2.0 * 6.277580261230469
Epoch 990, val loss: 0.9252005219459534
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7111111111111111
0.8176067474960464
=== training gcn model ===
Epoch 0, training loss: 19.14802360534668 = 1.9543484449386597 + 2.0 * 8.596837997436523
Epoch 0, val loss: 1.9409172534942627
Epoch 10, training loss: 19.142871856689453 = 1.949394702911377 + 2.0 * 8.596738815307617
Epoch 10, val loss: 1.9358552694320679
Epoch 20, training loss: 19.13690757751465 = 1.9440473318099976 + 2.0 * 8.596429824829102
Epoch 20, val loss: 1.9303752183914185
Epoch 30, training loss: 19.128482818603516 = 1.9378985166549683 + 2.0 * 8.595292091369629
Epoch 30, val loss: 1.9241018295288086
Epoch 40, training loss: 19.111684799194336 = 1.9305146932601929 + 2.0 * 8.590584754943848
Epoch 40, val loss: 1.9165563583374023
Epoch 50, training loss: 19.067869186401367 = 1.9215080738067627 + 2.0 * 8.573180198669434
Epoch 50, val loss: 1.9073748588562012
Epoch 60, training loss: 18.949813842773438 = 1.9108588695526123 + 2.0 * 8.519477844238281
Epoch 60, val loss: 1.896780014038086
Epoch 70, training loss: 18.60964012145996 = 1.899590253829956 + 2.0 * 8.355025291442871
Epoch 70, val loss: 1.8860602378845215
Epoch 80, training loss: 17.94732093811035 = 1.886954665184021 + 2.0 * 8.030182838439941
Epoch 80, val loss: 1.8742988109588623
Epoch 90, training loss: 17.554859161376953 = 1.8753762245178223 + 2.0 * 7.839741230010986
Epoch 90, val loss: 1.8637549877166748
Epoch 100, training loss: 16.94192123413086 = 1.8665627241134644 + 2.0 * 7.537679195404053
Epoch 100, val loss: 1.8556947708129883
Epoch 110, training loss: 16.262834548950195 = 1.859912633895874 + 2.0 * 7.201460838317871
Epoch 110, val loss: 1.8496325016021729
Epoch 120, training loss: 15.814718246459961 = 1.8547009229660034 + 2.0 * 6.980008602142334
Epoch 120, val loss: 1.844838261604309
Epoch 130, training loss: 15.622429847717285 = 1.8495399951934814 + 2.0 * 6.886445045471191
Epoch 130, val loss: 1.8399721384048462
Epoch 140, training loss: 15.494149208068848 = 1.8430290222167969 + 2.0 * 6.825560092926025
Epoch 140, val loss: 1.833976149559021
Epoch 150, training loss: 15.3792724609375 = 1.8360015153884888 + 2.0 * 6.77163553237915
Epoch 150, val loss: 1.827840805053711
Epoch 160, training loss: 15.289321899414062 = 1.8293278217315674 + 2.0 * 6.729997158050537
Epoch 160, val loss: 1.8221827745437622
Epoch 170, training loss: 15.203518867492676 = 1.822982907295227 + 2.0 * 6.690268039703369
Epoch 170, val loss: 1.8168174028396606
Epoch 180, training loss: 15.120504379272461 = 1.8170150518417358 + 2.0 * 6.651744842529297
Epoch 180, val loss: 1.811858057975769
Epoch 190, training loss: 15.050073623657227 = 1.811291217803955 + 2.0 * 6.619391441345215
Epoch 190, val loss: 1.807078242301941
Epoch 200, training loss: 14.99087905883789 = 1.8055981397628784 + 2.0 * 6.592640399932861
Epoch 200, val loss: 1.802446722984314
Epoch 210, training loss: 14.941296577453613 = 1.7998971939086914 + 2.0 * 6.570699691772461
Epoch 210, val loss: 1.7979227304458618
Epoch 220, training loss: 14.894403457641602 = 1.7942959070205688 + 2.0 * 6.550053596496582
Epoch 220, val loss: 1.793424129486084
Epoch 230, training loss: 14.852465629577637 = 1.7887321710586548 + 2.0 * 6.531866550445557
Epoch 230, val loss: 1.788993239402771
Epoch 240, training loss: 14.812138557434082 = 1.7831932306289673 + 2.0 * 6.514472484588623
Epoch 240, val loss: 1.7845863103866577
Epoch 250, training loss: 14.775257110595703 = 1.7775683403015137 + 2.0 * 6.498844623565674
Epoch 250, val loss: 1.7801192998886108
Epoch 260, training loss: 14.740939140319824 = 1.7718400955200195 + 2.0 * 6.484549522399902
Epoch 260, val loss: 1.775538682937622
Epoch 270, training loss: 14.709961891174316 = 1.7658889293670654 + 2.0 * 6.472036361694336
Epoch 270, val loss: 1.7707444429397583
Epoch 280, training loss: 14.682260513305664 = 1.7595831155776978 + 2.0 * 6.461338520050049
Epoch 280, val loss: 1.765669822692871
Epoch 290, training loss: 14.654212951660156 = 1.753008484840393 + 2.0 * 6.450602054595947
Epoch 290, val loss: 1.7603025436401367
Epoch 300, training loss: 14.628494262695312 = 1.7460598945617676 + 2.0 * 6.441216945648193
Epoch 300, val loss: 1.7545924186706543
Epoch 310, training loss: 14.604156494140625 = 1.738656759262085 + 2.0 * 6.4327497482299805
Epoch 310, val loss: 1.74848473072052
Epoch 320, training loss: 14.583419799804688 = 1.730787754058838 + 2.0 * 6.426315784454346
Epoch 320, val loss: 1.7419483661651611
Epoch 330, training loss: 14.558022499084473 = 1.722408652305603 + 2.0 * 6.417807102203369
Epoch 330, val loss: 1.734984278678894
Epoch 340, training loss: 14.535971641540527 = 1.7134771347045898 + 2.0 * 6.411247253417969
Epoch 340, val loss: 1.7275646924972534
Epoch 350, training loss: 14.514841079711914 = 1.7039779424667358 + 2.0 * 6.405431747436523
Epoch 350, val loss: 1.7196011543273926
Epoch 360, training loss: 14.490845680236816 = 1.6938291788101196 + 2.0 * 6.398508071899414
Epoch 360, val loss: 1.7110910415649414
Epoch 370, training loss: 14.467607498168945 = 1.6829689741134644 + 2.0 * 6.392319202423096
Epoch 370, val loss: 1.7019431591033936
Epoch 380, training loss: 14.444992065429688 = 1.6712833642959595 + 2.0 * 6.38685417175293
Epoch 380, val loss: 1.6920686960220337
Epoch 390, training loss: 14.426369667053223 = 1.6587705612182617 + 2.0 * 6.3837995529174805
Epoch 390, val loss: 1.6814578771591187
Epoch 400, training loss: 14.400409698486328 = 1.6454060077667236 + 2.0 * 6.377501964569092
Epoch 400, val loss: 1.6701239347457886
Epoch 410, training loss: 14.376712799072266 = 1.6311835050582886 + 2.0 * 6.372764587402344
Epoch 410, val loss: 1.6580485105514526
Epoch 420, training loss: 14.352703094482422 = 1.6160756349563599 + 2.0 * 6.368313789367676
Epoch 420, val loss: 1.6452052593231201
Epoch 430, training loss: 14.329035758972168 = 1.5999194383621216 + 2.0 * 6.364558219909668
Epoch 430, val loss: 1.6313955783843994
Epoch 440, training loss: 14.309008598327637 = 1.5828502178192139 + 2.0 * 6.363079071044922
Epoch 440, val loss: 1.6167974472045898
Epoch 450, training loss: 14.279925346374512 = 1.565041422843933 + 2.0 * 6.3574419021606445
Epoch 450, val loss: 1.6015565395355225
Epoch 460, training loss: 14.253643035888672 = 1.5464273691177368 + 2.0 * 6.353607654571533
Epoch 460, val loss: 1.5855920314788818
Epoch 470, training loss: 14.2277250289917 = 1.5268726348876953 + 2.0 * 6.350426197052002
Epoch 470, val loss: 1.5687981843948364
Epoch 480, training loss: 14.209515571594238 = 1.5065466165542603 + 2.0 * 6.351484298706055
Epoch 480, val loss: 1.5513951778411865
Epoch 490, training loss: 14.17483139038086 = 1.485597848892212 + 2.0 * 6.344616889953613
Epoch 490, val loss: 1.5334901809692383
Epoch 500, training loss: 14.148338317871094 = 1.4640882015228271 + 2.0 * 6.342124938964844
Epoch 500, val loss: 1.5151088237762451
Epoch 510, training loss: 14.120683670043945 = 1.4420084953308105 + 2.0 * 6.3393378257751465
Epoch 510, val loss: 1.4962965250015259
Epoch 520, training loss: 14.092728614807129 = 1.4194179773330688 + 2.0 * 6.336655139923096
Epoch 520, val loss: 1.4771209955215454
Epoch 530, training loss: 14.064746856689453 = 1.3963466882705688 + 2.0 * 6.334199905395508
Epoch 530, val loss: 1.4575960636138916
Epoch 540, training loss: 14.038588523864746 = 1.372859239578247 + 2.0 * 6.332864761352539
Epoch 540, val loss: 1.4377894401550293
Epoch 550, training loss: 14.011801719665527 = 1.3493458032608032 + 2.0 * 6.331227779388428
Epoch 550, val loss: 1.4180529117584229
Epoch 560, training loss: 13.982025146484375 = 1.3258459568023682 + 2.0 * 6.328089714050293
Epoch 560, val loss: 1.3985599279403687
Epoch 570, training loss: 13.954683303833008 = 1.3023338317871094 + 2.0 * 6.326174736022949
Epoch 570, val loss: 1.3792061805725098
Epoch 580, training loss: 13.926814079284668 = 1.2790366411209106 + 2.0 * 6.323888778686523
Epoch 580, val loss: 1.360129714012146
Epoch 590, training loss: 13.899947166442871 = 1.2560480833053589 + 2.0 * 6.321949481964111
Epoch 590, val loss: 1.3414579629898071
Epoch 600, training loss: 13.873564720153809 = 1.2333052158355713 + 2.0 * 6.320129871368408
Epoch 600, val loss: 1.323133945465088
Epoch 610, training loss: 13.846955299377441 = 1.2107477188110352 + 2.0 * 6.318103790283203
Epoch 610, val loss: 1.3051488399505615
Epoch 620, training loss: 13.820928573608398 = 1.1883853673934937 + 2.0 * 6.316271781921387
Epoch 620, val loss: 1.2874935865402222
Epoch 630, training loss: 13.795373916625977 = 1.1662423610687256 + 2.0 * 6.314565658569336
Epoch 630, val loss: 1.2701828479766846
Epoch 640, training loss: 13.77452564239502 = 1.144413948059082 + 2.0 * 6.315055847167969
Epoch 640, val loss: 1.253264307975769
Epoch 650, training loss: 13.749463081359863 = 1.1229944229125977 + 2.0 * 6.313234329223633
Epoch 650, val loss: 1.2368751764297485
Epoch 660, training loss: 13.726150512695312 = 1.102005958557129 + 2.0 * 6.312072277069092
Epoch 660, val loss: 1.2210040092468262
Epoch 670, training loss: 13.699369430541992 = 1.0815197229385376 + 2.0 * 6.308924674987793
Epoch 670, val loss: 1.2055997848510742
Epoch 680, training loss: 13.675143241882324 = 1.06134033203125 + 2.0 * 6.306901454925537
Epoch 680, val loss: 1.1906187534332275
Epoch 690, training loss: 13.652189254760742 = 1.04148530960083 + 2.0 * 6.305351734161377
Epoch 690, val loss: 1.176010012626648
Epoch 700, training loss: 13.642705917358398 = 1.0219990015029907 + 2.0 * 6.3103532791137695
Epoch 700, val loss: 1.1618324518203735
Epoch 710, training loss: 13.610358238220215 = 1.0029999017715454 + 2.0 * 6.3036789894104
Epoch 710, val loss: 1.1482205390930176
Epoch 720, training loss: 13.587533950805664 = 0.9844250679016113 + 2.0 * 6.301554203033447
Epoch 720, val loss: 1.135002613067627
Epoch 730, training loss: 13.566052436828613 = 0.9661375880241394 + 2.0 * 6.299957275390625
Epoch 730, val loss: 1.1221095323562622
Epoch 740, training loss: 13.545268058776855 = 0.9480953216552734 + 2.0 * 6.298586368560791
Epoch 740, val loss: 1.1095337867736816
Epoch 750, training loss: 13.529648780822754 = 0.9303136467933655 + 2.0 * 6.2996673583984375
Epoch 750, val loss: 1.0972671508789062
Epoch 760, training loss: 13.508995056152344 = 0.9129155874252319 + 2.0 * 6.29803991317749
Epoch 760, val loss: 1.0854334831237793
Epoch 770, training loss: 13.487271308898926 = 0.8959298729896545 + 2.0 * 6.295670509338379
Epoch 770, val loss: 1.0739669799804688
Epoch 780, training loss: 13.467447280883789 = 0.8792157173156738 + 2.0 * 6.2941155433654785
Epoch 780, val loss: 1.0628092288970947
Epoch 790, training loss: 13.449065208435059 = 0.8627532720565796 + 2.0 * 6.293156147003174
Epoch 790, val loss: 1.051941156387329
Epoch 800, training loss: 13.435855865478516 = 0.8465862274169922 + 2.0 * 6.294634819030762
Epoch 800, val loss: 1.0414092540740967
Epoch 810, training loss: 13.413427352905273 = 0.8307034969329834 + 2.0 * 6.2913618087768555
Epoch 810, val loss: 1.0312416553497314
Epoch 820, training loss: 13.394803047180176 = 0.8151147365570068 + 2.0 * 6.289844036102295
Epoch 820, val loss: 1.0213836431503296
Epoch 830, training loss: 13.377108573913574 = 0.7997673749923706 + 2.0 * 6.288670539855957
Epoch 830, val loss: 1.0117980241775513
Epoch 840, training loss: 13.36080551147461 = 0.7846470475196838 + 2.0 * 6.288079261779785
Epoch 840, val loss: 1.0024694204330444
Epoch 850, training loss: 13.345277786254883 = 0.769773006439209 + 2.0 * 6.287752628326416
Epoch 850, val loss: 0.9934457540512085
Epoch 860, training loss: 13.327997207641602 = 0.7551994919776917 + 2.0 * 6.286398887634277
Epoch 860, val loss: 0.984748125076294
Epoch 870, training loss: 13.31082534790039 = 0.7408445477485657 + 2.0 * 6.284990310668945
Epoch 870, val loss: 0.9763450622558594
Epoch 880, training loss: 13.295602798461914 = 0.7267228364944458 + 2.0 * 6.284440040588379
Epoch 880, val loss: 0.9681840538978577
Epoch 890, training loss: 13.279698371887207 = 0.7128984928131104 + 2.0 * 6.283400058746338
Epoch 890, val loss: 0.9603480696678162
Epoch 900, training loss: 13.264986038208008 = 0.6994802355766296 + 2.0 * 6.282752990722656
Epoch 900, val loss: 0.9528956413269043
Epoch 910, training loss: 13.249664306640625 = 0.6863303184509277 + 2.0 * 6.2816667556762695
Epoch 910, val loss: 0.9457677602767944
Epoch 920, training loss: 13.234941482543945 = 0.6733728647232056 + 2.0 * 6.2807841300964355
Epoch 920, val loss: 0.9388989210128784
Epoch 930, training loss: 13.22048568725586 = 0.6605973243713379 + 2.0 * 6.27994441986084
Epoch 930, val loss: 0.932293713092804
Epoch 940, training loss: 13.206361770629883 = 0.6480106711387634 + 2.0 * 6.279175758361816
Epoch 940, val loss: 0.9259482622146606
Epoch 950, training loss: 13.192403793334961 = 0.6356000900268555 + 2.0 * 6.278401851654053
Epoch 950, val loss: 0.9198621511459351
Epoch 960, training loss: 13.185619354248047 = 0.6233642101287842 + 2.0 * 6.281127452850342
Epoch 960, val loss: 0.9140328168869019
Epoch 970, training loss: 13.169581413269043 = 0.6113718152046204 + 2.0 * 6.279104709625244
Epoch 970, val loss: 0.9085062146186829
Epoch 980, training loss: 13.152566909790039 = 0.599682629108429 + 2.0 * 6.276442050933838
Epoch 980, val loss: 0.9032931327819824
Epoch 990, training loss: 13.139678955078125 = 0.5881683230400085 + 2.0 * 6.275755405426025
Epoch 990, val loss: 0.8983551263809204
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.6851851851851852
0.8133895624670533
=== training gcn model ===
Epoch 0, training loss: 19.152597427368164 = 1.9588593244552612 + 2.0 * 8.596869468688965
Epoch 0, val loss: 1.9612646102905273
Epoch 10, training loss: 19.14726448059082 = 1.953643798828125 + 2.0 * 8.596810340881348
Epoch 10, val loss: 1.9559518098831177
Epoch 20, training loss: 19.141319274902344 = 1.94808828830719 + 2.0 * 8.5966157913208
Epoch 20, val loss: 1.950215220451355
Epoch 30, training loss: 19.13357925415039 = 1.9416933059692383 + 2.0 * 8.595943450927734
Epoch 30, val loss: 1.9435914754867554
Epoch 40, training loss: 19.120521545410156 = 1.9339667558670044 + 2.0 * 8.593276977539062
Epoch 40, val loss: 1.9355554580688477
Epoch 50, training loss: 19.089462280273438 = 1.9244078397750854 + 2.0 * 8.582527160644531
Epoch 50, val loss: 1.92561936378479
Epoch 60, training loss: 18.99787712097168 = 1.912894368171692 + 2.0 * 8.54249095916748
Epoch 60, val loss: 1.9136927127838135
Epoch 70, training loss: 18.68975830078125 = 1.9002326726913452 + 2.0 * 8.394762992858887
Epoch 70, val loss: 1.9008259773254395
Epoch 80, training loss: 17.789400100708008 = 1.8864914178848267 + 2.0 * 7.9514546394348145
Epoch 80, val loss: 1.8869770765304565
Epoch 90, training loss: 17.07146453857422 = 1.874848484992981 + 2.0 * 7.5983076095581055
Epoch 90, val loss: 1.875983476638794
Epoch 100, training loss: 16.574581146240234 = 1.8668005466461182 + 2.0 * 7.353890419006348
Epoch 100, val loss: 1.8685517311096191
Epoch 110, training loss: 16.227794647216797 = 1.8592449426651 + 2.0 * 7.184274673461914
Epoch 110, val loss: 1.8612267971038818
Epoch 120, training loss: 15.963089942932129 = 1.851791262626648 + 2.0 * 7.055649280548096
Epoch 120, val loss: 1.8539623022079468
Epoch 130, training loss: 15.775800704956055 = 1.8443905115127563 + 2.0 * 6.965704917907715
Epoch 130, val loss: 1.8469845056533813
Epoch 140, training loss: 15.601762771606445 = 1.8371307849884033 + 2.0 * 6.8823161125183105
Epoch 140, val loss: 1.8402066230773926
Epoch 150, training loss: 15.457319259643555 = 1.8301987648010254 + 2.0 * 6.8135600090026855
Epoch 150, val loss: 1.8337708711624146
Epoch 160, training loss: 15.34532642364502 = 1.8232338428497314 + 2.0 * 6.761046409606934
Epoch 160, val loss: 1.827300786972046
Epoch 170, training loss: 15.248202323913574 = 1.8162453174591064 + 2.0 * 6.715978622436523
Epoch 170, val loss: 1.8210194110870361
Epoch 180, training loss: 15.164102554321289 = 1.8094853162765503 + 2.0 * 6.677308559417725
Epoch 180, val loss: 1.814922571182251
Epoch 190, training loss: 15.089556694030762 = 1.8029510974884033 + 2.0 * 6.643302917480469
Epoch 190, val loss: 1.809044599533081
Epoch 200, training loss: 15.025867462158203 = 1.796546459197998 + 2.0 * 6.614660263061523
Epoch 200, val loss: 1.8032288551330566
Epoch 210, training loss: 14.968621253967285 = 1.7901793718338013 + 2.0 * 6.589221000671387
Epoch 210, val loss: 1.7974339723587036
Epoch 220, training loss: 14.91779613494873 = 1.7837764024734497 + 2.0 * 6.567009925842285
Epoch 220, val loss: 1.7916438579559326
Epoch 230, training loss: 14.874662399291992 = 1.7773101329803467 + 2.0 * 6.548676013946533
Epoch 230, val loss: 1.7858244180679321
Epoch 240, training loss: 14.832221984863281 = 1.770708680152893 + 2.0 * 6.53075647354126
Epoch 240, val loss: 1.7799193859100342
Epoch 250, training loss: 14.793944358825684 = 1.7638986110687256 + 2.0 * 6.5150227546691895
Epoch 250, val loss: 1.7738591432571411
Epoch 260, training loss: 14.760054588317871 = 1.7567691802978516 + 2.0 * 6.50164270401001
Epoch 260, val loss: 1.7675533294677734
Epoch 270, training loss: 14.728025436401367 = 1.7492479085922241 + 2.0 * 6.489388942718506
Epoch 270, val loss: 1.7609508037567139
Epoch 280, training loss: 14.697257995605469 = 1.7412896156311035 + 2.0 * 6.4779839515686035
Epoch 280, val loss: 1.7540016174316406
Epoch 290, training loss: 14.667888641357422 = 1.732811450958252 + 2.0 * 6.467538833618164
Epoch 290, val loss: 1.7466360330581665
Epoch 300, training loss: 14.639963150024414 = 1.723742961883545 + 2.0 * 6.458110332489014
Epoch 300, val loss: 1.738793969154358
Epoch 310, training loss: 14.613506317138672 = 1.7140880823135376 + 2.0 * 6.449708938598633
Epoch 310, val loss: 1.7304785251617432
Epoch 320, training loss: 14.587650299072266 = 1.7037864923477173 + 2.0 * 6.44193172454834
Epoch 320, val loss: 1.7216572761535645
Epoch 330, training loss: 14.561426162719727 = 1.6927850246429443 + 2.0 * 6.434320449829102
Epoch 330, val loss: 1.7122496366500854
Epoch 340, training loss: 14.535943984985352 = 1.6809977293014526 + 2.0 * 6.427473068237305
Epoch 340, val loss: 1.702202320098877
Epoch 350, training loss: 14.512603759765625 = 1.6683390140533447 + 2.0 * 6.42213249206543
Epoch 350, val loss: 1.691444993019104
Epoch 360, training loss: 14.487354278564453 = 1.6548638343811035 + 2.0 * 6.416244983673096
Epoch 360, val loss: 1.6800413131713867
Epoch 370, training loss: 14.461902618408203 = 1.640621304512024 + 2.0 * 6.410640716552734
Epoch 370, val loss: 1.6680119037628174
Epoch 380, training loss: 14.435918807983398 = 1.625509262084961 + 2.0 * 6.405204772949219
Epoch 380, val loss: 1.6552917957305908
Epoch 390, training loss: 14.41024112701416 = 1.6094766855239868 + 2.0 * 6.400382041931152
Epoch 390, val loss: 1.6417999267578125
Epoch 400, training loss: 14.384403228759766 = 1.592475175857544 + 2.0 * 6.3959641456604
Epoch 400, val loss: 1.6275010108947754
Epoch 410, training loss: 14.360565185546875 = 1.574615478515625 + 2.0 * 6.392974853515625
Epoch 410, val loss: 1.612572193145752
Epoch 420, training loss: 14.331700325012207 = 1.556090235710144 + 2.0 * 6.387804985046387
Epoch 420, val loss: 1.5970990657806396
Epoch 430, training loss: 14.30476188659668 = 1.5368285179138184 + 2.0 * 6.383966445922852
Epoch 430, val loss: 1.581082820892334
Epoch 440, training loss: 14.277069091796875 = 1.5168508291244507 + 2.0 * 6.3801093101501465
Epoch 440, val loss: 1.564558744430542
Epoch 450, training loss: 14.24901008605957 = 1.4961843490600586 + 2.0 * 6.376412868499756
Epoch 450, val loss: 1.547537922859192
Epoch 460, training loss: 14.227899551391602 = 1.4749997854232788 + 2.0 * 6.376450061798096
Epoch 460, val loss: 1.5302433967590332
Epoch 470, training loss: 14.195572853088379 = 1.4536771774291992 + 2.0 * 6.37094783782959
Epoch 470, val loss: 1.5129828453063965
Epoch 480, training loss: 14.165884017944336 = 1.4322373867034912 + 2.0 * 6.366823196411133
Epoch 480, val loss: 1.4957257509231567
Epoch 490, training loss: 14.137069702148438 = 1.4106472730636597 + 2.0 * 6.363211154937744
Epoch 490, val loss: 1.478449821472168
Epoch 500, training loss: 14.109323501586914 = 1.3889577388763428 + 2.0 * 6.360182762145996
Epoch 500, val loss: 1.461202621459961
Epoch 510, training loss: 14.083430290222168 = 1.367288589477539 + 2.0 * 6.3580708503723145
Epoch 510, val loss: 1.4441304206848145
Epoch 520, training loss: 14.055448532104492 = 1.3458467721939087 + 2.0 * 6.354800701141357
Epoch 520, val loss: 1.427329659461975
Epoch 530, training loss: 14.028848648071289 = 1.3246334791183472 + 2.0 * 6.352107524871826
Epoch 530, val loss: 1.4108483791351318
Epoch 540, training loss: 14.001567840576172 = 1.3036270141601562 + 2.0 * 6.348970413208008
Epoch 540, val loss: 1.3946703672409058
Epoch 550, training loss: 13.976768493652344 = 1.2828607559204102 + 2.0 * 6.346953868865967
Epoch 550, val loss: 1.3787840604782104
Epoch 560, training loss: 13.952798843383789 = 1.2624144554138184 + 2.0 * 6.3451924324035645
Epoch 560, val loss: 1.3632395267486572
Epoch 570, training loss: 13.92575740814209 = 1.242258906364441 + 2.0 * 6.34174919128418
Epoch 570, val loss: 1.3480600118637085
Epoch 580, training loss: 13.900077819824219 = 1.2224076986312866 + 2.0 * 6.3388352394104
Epoch 580, val loss: 1.3331819772720337
Epoch 590, training loss: 13.877222061157227 = 1.2027744054794312 + 2.0 * 6.337224006652832
Epoch 590, val loss: 1.3185479640960693
Epoch 600, training loss: 13.85302734375 = 1.183400273323059 + 2.0 * 6.334813594818115
Epoch 600, val loss: 1.3041692972183228
Epoch 610, training loss: 13.832695007324219 = 1.164320468902588 + 2.0 * 6.3341875076293945
Epoch 610, val loss: 1.2901138067245483
Epoch 620, training loss: 13.808770179748535 = 1.1455966234207153 + 2.0 * 6.331586837768555
Epoch 620, val loss: 1.2763900756835938
Epoch 630, training loss: 13.783766746520996 = 1.1271276473999023 + 2.0 * 6.328319549560547
Epoch 630, val loss: 1.2628893852233887
Epoch 640, training loss: 13.760998725891113 = 1.1088312864303589 + 2.0 * 6.326083660125732
Epoch 640, val loss: 1.2495921850204468
Epoch 650, training loss: 13.739127159118652 = 1.090692162513733 + 2.0 * 6.324217319488525
Epoch 650, val loss: 1.236406922340393
Epoch 660, training loss: 13.722818374633789 = 1.0727084875106812 + 2.0 * 6.325055122375488
Epoch 660, val loss: 1.2233808040618896
Epoch 670, training loss: 13.696619033813477 = 1.054909586906433 + 2.0 * 6.320854663848877
Epoch 670, val loss: 1.210513710975647
Epoch 680, training loss: 13.675865173339844 = 1.0372384786605835 + 2.0 * 6.3193135261535645
Epoch 680, val loss: 1.197774887084961
Epoch 690, training loss: 13.656874656677246 = 1.0197468996047974 + 2.0 * 6.318563938140869
Epoch 690, val loss: 1.1852624416351318
Epoch 700, training loss: 13.633914947509766 = 1.002480387687683 + 2.0 * 6.3157172203063965
Epoch 700, val loss: 1.1729710102081299
Epoch 710, training loss: 13.61413860321045 = 0.9853439927101135 + 2.0 * 6.31439733505249
Epoch 710, val loss: 1.1608091592788696
Epoch 720, training loss: 13.594667434692383 = 0.9683578610420227 + 2.0 * 6.313154697418213
Epoch 720, val loss: 1.14882230758667
Epoch 730, training loss: 13.574275970458984 = 0.9515209197998047 + 2.0 * 6.31137752532959
Epoch 730, val loss: 1.1370271444320679
Epoch 740, training loss: 13.554317474365234 = 0.9348297119140625 + 2.0 * 6.309743881225586
Epoch 740, val loss: 1.1253517866134644
Epoch 750, training loss: 13.538031578063965 = 0.9183218479156494 + 2.0 * 6.309854984283447
Epoch 750, val loss: 1.1138650178909302
Epoch 760, training loss: 13.517584800720215 = 0.9021027684211731 + 2.0 * 6.307741165161133
Epoch 760, val loss: 1.1026606559753418
Epoch 770, training loss: 13.497358322143555 = 0.8861110210418701 + 2.0 * 6.305623531341553
Epoch 770, val loss: 1.0917260646820068
Epoch 780, training loss: 13.478960037231445 = 0.8702915906906128 + 2.0 * 6.3043341636657715
Epoch 780, val loss: 1.0810132026672363
Epoch 790, training loss: 13.464216232299805 = 0.8546688556671143 + 2.0 * 6.304773807525635
Epoch 790, val loss: 1.0705466270446777
Epoch 800, training loss: 13.445646286010742 = 0.8392199873924255 + 2.0 * 6.303213119506836
Epoch 800, val loss: 1.060320496559143
Epoch 810, training loss: 13.4260835647583 = 0.8240851759910583 + 2.0 * 6.300999164581299
Epoch 810, val loss: 1.050418734550476
Epoch 820, training loss: 13.40849781036377 = 0.8091861009597778 + 2.0 * 6.299655914306641
Epoch 820, val loss: 1.040773630142212
Epoch 830, training loss: 13.391308784484863 = 0.7945355772972107 + 2.0 * 6.298386573791504
Epoch 830, val loss: 1.0314337015151978
Epoch 840, training loss: 13.37626838684082 = 0.780122697353363 + 2.0 * 6.298072814941406
Epoch 840, val loss: 1.0224119424819946
Epoch 850, training loss: 13.358458518981934 = 0.7659580707550049 + 2.0 * 6.296250343322754
Epoch 850, val loss: 1.013746976852417
Epoch 860, training loss: 13.345118522644043 = 0.752081036567688 + 2.0 * 6.296518802642822
Epoch 860, val loss: 1.0054006576538086
Epoch 870, training loss: 13.326326370239258 = 0.7384964227676392 + 2.0 * 6.293914794921875
Epoch 870, val loss: 0.9973935484886169
Epoch 880, training loss: 13.311870574951172 = 0.7251718640327454 + 2.0 * 6.293349266052246
Epoch 880, val loss: 0.9897118210792542
Epoch 890, training loss: 13.298578262329102 = 0.7120975852012634 + 2.0 * 6.293240547180176
Epoch 890, val loss: 0.9823443293571472
Epoch 900, training loss: 13.281621932983398 = 0.6992977857589722 + 2.0 * 6.291162014007568
Epoch 900, val loss: 0.9753041863441467
Epoch 910, training loss: 13.268020629882812 = 0.6867968440055847 + 2.0 * 6.290611743927002
Epoch 910, val loss: 0.9686367511749268
Epoch 920, training loss: 13.256278991699219 = 0.674553394317627 + 2.0 * 6.290862560272217
Epoch 920, val loss: 0.9622792601585388
Epoch 930, training loss: 13.240341186523438 = 0.662564754486084 + 2.0 * 6.288887977600098
Epoch 930, val loss: 0.9562612175941467
Epoch 940, training loss: 13.22913932800293 = 0.6508625745773315 + 2.0 * 6.289138317108154
Epoch 940, val loss: 0.95058673620224
Epoch 950, training loss: 13.214481353759766 = 0.6393803954124451 + 2.0 * 6.287550449371338
Epoch 950, val loss: 0.9452089667320251
Epoch 960, training loss: 13.203473091125488 = 0.6281669735908508 + 2.0 * 6.287652969360352
Epoch 960, val loss: 0.9401769638061523
Epoch 970, training loss: 13.187640190124512 = 0.6171814799308777 + 2.0 * 6.285229206085205
Epoch 970, val loss: 0.9354425072669983
Epoch 980, training loss: 13.174966812133789 = 0.6064244508743286 + 2.0 * 6.284271240234375
Epoch 980, val loss: 0.9309739470481873
Epoch 990, training loss: 13.163004875183105 = 0.5958501100540161 + 2.0 * 6.2835774421691895
Epoch 990, val loss: 0.9268073439598083
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7074074074074075
0.8176067474960464
The final CL Acc:0.70123, 0.01145, The final GNN Acc:0.81620, 0.00199
raw graph: torch.Size([2, 10556])
add edge: torch.Size([2, 13272])
remove edge: torch.Size([2, 7852])
updated graph: torch.Size([2, 10568])
=== Raw graph ===
=== training gcn model ===
Epoch 0, training loss: 19.133901596069336 = 1.9402092695236206 + 2.0 * 8.596846580505371
Epoch 0, val loss: 1.9388924837112427
Epoch 10, training loss: 19.128931045532227 = 1.9353801012039185 + 2.0 * 8.59677505493164
Epoch 10, val loss: 1.9340949058532715
Epoch 20, training loss: 19.1232967376709 = 1.9302910566329956 + 2.0 * 8.596503257751465
Epoch 20, val loss: 1.9290673732757568
Epoch 30, training loss: 19.115217208862305 = 1.9244028329849243 + 2.0 * 8.595407485961914
Epoch 30, val loss: 1.9232889413833618
Epoch 40, training loss: 19.098237991333008 = 1.9172534942626953 + 2.0 * 8.590492248535156
Epoch 40, val loss: 1.9163047075271606
Epoch 50, training loss: 19.048463821411133 = 1.9083914756774902 + 2.0 * 8.570035934448242
Epoch 50, val loss: 1.9076915979385376
Epoch 60, training loss: 18.887380599975586 = 1.8977652788162231 + 2.0 * 8.494807243347168
Epoch 60, val loss: 1.8974392414093018
Epoch 70, training loss: 18.415761947631836 = 1.885964274406433 + 2.0 * 8.264899253845215
Epoch 70, val loss: 1.8862236738204956
Epoch 80, training loss: 17.774681091308594 = 1.8731672763824463 + 2.0 * 7.950757026672363
Epoch 80, val loss: 1.874234676361084
Epoch 90, training loss: 17.318883895874023 = 1.8627994060516357 + 2.0 * 7.728042125701904
Epoch 90, val loss: 1.8644452095031738
Epoch 100, training loss: 16.677040100097656 = 1.855851173400879 + 2.0 * 7.410594463348389
Epoch 100, val loss: 1.8576297760009766
Epoch 110, training loss: 16.130672454833984 = 1.8515586853027344 + 2.0 * 7.139557361602783
Epoch 110, val loss: 1.8529434204101562
Epoch 120, training loss: 15.785523414611816 = 1.8478931188583374 + 2.0 * 6.968815326690674
Epoch 120, val loss: 1.8488714694976807
Epoch 130, training loss: 15.541297912597656 = 1.842673659324646 + 2.0 * 6.8493123054504395
Epoch 130, val loss: 1.8435920476913452
Epoch 140, training loss: 15.362454414367676 = 1.836586833000183 + 2.0 * 6.762933731079102
Epoch 140, val loss: 1.8377771377563477
Epoch 150, training loss: 15.253913879394531 = 1.830357313156128 + 2.0 * 6.711778163909912
Epoch 150, val loss: 1.8319023847579956
Epoch 160, training loss: 15.154309272766113 = 1.8243342638015747 + 2.0 * 6.664987564086914
Epoch 160, val loss: 1.826098084449768
Epoch 170, training loss: 15.06758975982666 = 1.818548560142517 + 2.0 * 6.624520778656006
Epoch 170, val loss: 1.8205287456512451
Epoch 180, training loss: 14.995088577270508 = 1.8129551410675049 + 2.0 * 6.591066837310791
Epoch 180, val loss: 1.8151801824569702
Epoch 190, training loss: 14.934638977050781 = 1.8073450326919556 + 2.0 * 6.5636467933654785
Epoch 190, val loss: 1.8098347187042236
Epoch 200, training loss: 14.88287353515625 = 1.8016905784606934 + 2.0 * 6.540591239929199
Epoch 200, val loss: 1.8044970035552979
Epoch 210, training loss: 14.838166236877441 = 1.7960777282714844 + 2.0 * 6.5210442543029785
Epoch 210, val loss: 1.7992949485778809
Epoch 220, training loss: 14.794568061828613 = 1.7905579805374146 + 2.0 * 6.502005100250244
Epoch 220, val loss: 1.794160008430481
Epoch 230, training loss: 14.756681442260742 = 1.784985065460205 + 2.0 * 6.485848426818848
Epoch 230, val loss: 1.7890605926513672
Epoch 240, training loss: 14.722643852233887 = 1.779348611831665 + 2.0 * 6.4716477394104
Epoch 240, val loss: 1.7838797569274902
Epoch 250, training loss: 14.688604354858398 = 1.773520588874817 + 2.0 * 6.4575419425964355
Epoch 250, val loss: 1.778648853302002
Epoch 260, training loss: 14.659744262695312 = 1.767505407333374 + 2.0 * 6.44611930847168
Epoch 260, val loss: 1.7732696533203125
Epoch 270, training loss: 14.63414478302002 = 1.7612402439117432 + 2.0 * 6.436452388763428
Epoch 270, val loss: 1.7676979303359985
Epoch 280, training loss: 14.608043670654297 = 1.7546696662902832 + 2.0 * 6.426686763763428
Epoch 280, val loss: 1.761944055557251
Epoch 290, training loss: 14.583727836608887 = 1.7477848529815674 + 2.0 * 6.417971611022949
Epoch 290, val loss: 1.755950689315796
Epoch 300, training loss: 14.559835433959961 = 1.740513563156128 + 2.0 * 6.409660816192627
Epoch 300, val loss: 1.7496733665466309
Epoch 310, training loss: 14.53808879852295 = 1.7327700853347778 + 2.0 * 6.4026594161987305
Epoch 310, val loss: 1.7430388927459717
Epoch 320, training loss: 14.519667625427246 = 1.7245699167251587 + 2.0 * 6.397548675537109
Epoch 320, val loss: 1.735996961593628
Epoch 330, training loss: 14.494935035705566 = 1.715857744216919 + 2.0 * 6.389538764953613
Epoch 330, val loss: 1.7286044359207153
Epoch 340, training loss: 14.472445487976074 = 1.7065956592559814 + 2.0 * 6.382925033569336
Epoch 340, val loss: 1.7207622528076172
Epoch 350, training loss: 14.45055103302002 = 1.6966753005981445 + 2.0 * 6.3769378662109375
Epoch 350, val loss: 1.7123905420303345
Epoch 360, training loss: 14.43096923828125 = 1.6860382556915283 + 2.0 * 6.37246561050415
Epoch 360, val loss: 1.7034225463867188
Epoch 370, training loss: 14.40818977355957 = 1.674710988998413 + 2.0 * 6.366739273071289
Epoch 370, val loss: 1.693902611732483
Epoch 380, training loss: 14.387771606445312 = 1.6627345085144043 + 2.0 * 6.362518787384033
Epoch 380, val loss: 1.6838234663009644
Epoch 390, training loss: 14.364982604980469 = 1.6499773263931274 + 2.0 * 6.357502460479736
Epoch 390, val loss: 1.6730825901031494
Epoch 400, training loss: 14.343076705932617 = 1.6363738775253296 + 2.0 * 6.353351593017578
Epoch 400, val loss: 1.661618947982788
Epoch 410, training loss: 14.323058128356934 = 1.6219154596328735 + 2.0 * 6.350571155548096
Epoch 410, val loss: 1.6493947505950928
Epoch 420, training loss: 14.304830551147461 = 1.606579303741455 + 2.0 * 6.349125862121582
Epoch 420, val loss: 1.6365382671356201
Epoch 430, training loss: 14.276459693908691 = 1.5907286405563354 + 2.0 * 6.342865467071533
Epoch 430, val loss: 1.6231403350830078
Epoch 440, training loss: 14.254039764404297 = 1.5741066932678223 + 2.0 * 6.339966297149658
Epoch 440, val loss: 1.6090985536575317
Epoch 450, training loss: 14.229830741882324 = 1.5566434860229492 + 2.0 * 6.3365936279296875
Epoch 450, val loss: 1.594383955001831
Epoch 460, training loss: 14.206010818481445 = 1.5383914709091187 + 2.0 * 6.333809852600098
Epoch 460, val loss: 1.5790185928344727
Epoch 470, training loss: 14.181625366210938 = 1.5193544626235962 + 2.0 * 6.331135272979736
Epoch 470, val loss: 1.563014268875122
Epoch 480, training loss: 14.156773567199707 = 1.4995715618133545 + 2.0 * 6.328600883483887
Epoch 480, val loss: 1.5464075803756714
Epoch 490, training loss: 14.13453483581543 = 1.4790722131729126 + 2.0 * 6.327731132507324
Epoch 490, val loss: 1.5292564630508423
Epoch 500, training loss: 14.107712745666504 = 1.458135962486267 + 2.0 * 6.324788570404053
Epoch 500, val loss: 1.5117089748382568
Epoch 510, training loss: 14.083213806152344 = 1.4367847442626953 + 2.0 * 6.323214530944824
Epoch 510, val loss: 1.4939261674880981
Epoch 520, training loss: 14.05567455291748 = 1.4150296449661255 + 2.0 * 6.320322513580322
Epoch 520, val loss: 1.4759440422058105
Epoch 530, training loss: 14.028827667236328 = 1.392938256263733 + 2.0 * 6.317944526672363
Epoch 530, val loss: 1.4576727151870728
Epoch 540, training loss: 14.003849029541016 = 1.3705037832260132 + 2.0 * 6.3166728019714355
Epoch 540, val loss: 1.4392151832580566
Epoch 550, training loss: 13.977341651916504 = 1.3478106260299683 + 2.0 * 6.314765453338623
Epoch 550, val loss: 1.420714259147644
Epoch 560, training loss: 13.950133323669434 = 1.3249753713607788 + 2.0 * 6.312579154968262
Epoch 560, val loss: 1.4022564888000488
Epoch 570, training loss: 13.92419719696045 = 1.301971435546875 + 2.0 * 6.311112880706787
Epoch 570, val loss: 1.383789300918579
Epoch 580, training loss: 13.898977279663086 = 1.278794288635254 + 2.0 * 6.310091495513916
Epoch 580, val loss: 1.3653292655944824
Epoch 590, training loss: 13.871138572692871 = 1.2554975748062134 + 2.0 * 6.3078203201293945
Epoch 590, val loss: 1.346980094909668
Epoch 600, training loss: 13.845390319824219 = 1.2320609092712402 + 2.0 * 6.306664943695068
Epoch 600, val loss: 1.3286603689193726
Epoch 610, training loss: 13.821057319641113 = 1.2084598541259766 + 2.0 * 6.306298732757568
Epoch 610, val loss: 1.3103363513946533
Epoch 620, training loss: 13.792160034179688 = 1.1848480701446533 + 2.0 * 6.303656101226807
Epoch 620, val loss: 1.2920750379562378
Epoch 630, training loss: 13.766439437866211 = 1.161118745803833 + 2.0 * 6.3026604652404785
Epoch 630, val loss: 1.2738534212112427
Epoch 640, training loss: 13.740562438964844 = 1.137215256690979 + 2.0 * 6.301673412322998
Epoch 640, val loss: 1.25563645362854
Epoch 650, training loss: 13.712993621826172 = 1.1132261753082275 + 2.0 * 6.299883842468262
Epoch 650, val loss: 1.2374889850616455
Epoch 660, training loss: 13.689064025878906 = 1.0891900062561035 + 2.0 * 6.299936771392822
Epoch 660, val loss: 1.2193974256515503
Epoch 670, training loss: 13.66053581237793 = 1.0651332139968872 + 2.0 * 6.297701358795166
Epoch 670, val loss: 1.2014057636260986
Epoch 680, training loss: 13.639122009277344 = 1.041114091873169 + 2.0 * 6.299004077911377
Epoch 680, val loss: 1.183514952659607
Epoch 690, training loss: 13.608451843261719 = 1.0173996686935425 + 2.0 * 6.295526027679443
Epoch 690, val loss: 1.1658042669296265
Epoch 700, training loss: 13.580894470214844 = 0.9937241077423096 + 2.0 * 6.293585300445557
Epoch 700, val loss: 1.1482723951339722
Epoch 710, training loss: 13.554638862609863 = 0.9702364802360535 + 2.0 * 6.292201042175293
Epoch 710, val loss: 1.1308233737945557
Epoch 720, training loss: 13.530378341674805 = 0.9469221830368042 + 2.0 * 6.2917280197143555
Epoch 720, val loss: 1.113565444946289
Epoch 730, training loss: 13.507431983947754 = 0.9239534139633179 + 2.0 * 6.291739463806152
Epoch 730, val loss: 1.0966033935546875
Epoch 740, training loss: 13.480731010437012 = 0.9015105962753296 + 2.0 * 6.289610385894775
Epoch 740, val loss: 1.0800914764404297
Epoch 750, training loss: 13.455361366271973 = 0.8796617984771729 + 2.0 * 6.2878499031066895
Epoch 750, val loss: 1.064019799232483
Epoch 760, training loss: 13.432868003845215 = 0.8582577109336853 + 2.0 * 6.2873053550720215
Epoch 760, val loss: 1.0483291149139404
Epoch 770, training loss: 13.41385269165039 = 0.837332010269165 + 2.0 * 6.288260459899902
Epoch 770, val loss: 1.0330815315246582
Epoch 780, training loss: 13.388113975524902 = 0.8172100782394409 + 2.0 * 6.285451889038086
Epoch 780, val loss: 1.0183995962142944
Epoch 790, training loss: 13.36625862121582 = 0.7977119088172913 + 2.0 * 6.284273147583008
Epoch 790, val loss: 1.0043272972106934
Epoch 800, training loss: 13.344754219055176 = 0.7788400650024414 + 2.0 * 6.282957077026367
Epoch 800, val loss: 0.9907666444778442
Epoch 810, training loss: 13.32522964477539 = 0.7605543732643127 + 2.0 * 6.282337665557861
Epoch 810, val loss: 0.9777172207832336
Epoch 820, training loss: 13.309252738952637 = 0.7428404092788696 + 2.0 * 6.283205986022949
Epoch 820, val loss: 0.9651865363121033
Epoch 830, training loss: 13.288265228271484 = 0.7258509397506714 + 2.0 * 6.281207084655762
Epoch 830, val loss: 0.9532759189605713
Epoch 840, training loss: 13.269765853881836 = 0.7096118927001953 + 2.0 * 6.28007698059082
Epoch 840, val loss: 0.9419894218444824
Epoch 850, training loss: 13.251099586486816 = 0.6939998865127563 + 2.0 * 6.278549671173096
Epoch 850, val loss: 0.9312434792518616
Epoch 860, training loss: 13.234195709228516 = 0.6788941621780396 + 2.0 * 6.277650833129883
Epoch 860, val loss: 0.9210139513015747
Epoch 870, training loss: 13.217916488647461 = 0.6643153429031372 + 2.0 * 6.276800632476807
Epoch 870, val loss: 0.9112482070922852
Epoch 880, training loss: 13.215509414672852 = 0.6502558588981628 + 2.0 * 6.282626628875732
Epoch 880, val loss: 0.9019725322723389
Epoch 890, training loss: 13.188497543334961 = 0.6367567777633667 + 2.0 * 6.275870323181152
Epoch 890, val loss: 0.8932342529296875
Epoch 900, training loss: 13.173824310302734 = 0.6238389611244202 + 2.0 * 6.2749924659729
Epoch 900, val loss: 0.8849471807479858
Epoch 910, training loss: 13.158479690551758 = 0.6113229990005493 + 2.0 * 6.27357816696167
Epoch 910, val loss: 0.8770884871482849
Epoch 920, training loss: 13.15150260925293 = 0.59920734167099 + 2.0 * 6.276147842407227
Epoch 920, val loss: 0.8696381449699402
Epoch 930, training loss: 13.134743690490723 = 0.5875951647758484 + 2.0 * 6.273574352264404
Epoch 930, val loss: 0.8625619411468506
Epoch 940, training loss: 13.120211601257324 = 0.5763806700706482 + 2.0 * 6.271915435791016
Epoch 940, val loss: 0.855877161026001
Epoch 950, training loss: 13.106362342834473 = 0.5654876828193665 + 2.0 * 6.270437240600586
Epoch 950, val loss: 0.8495404124259949
Epoch 960, training loss: 13.094690322875977 = 0.5548949837684631 + 2.0 * 6.2698974609375
Epoch 960, val loss: 0.8434935212135315
Epoch 970, training loss: 13.103841781616211 = 0.544645369052887 + 2.0 * 6.279598236083984
Epoch 970, val loss: 0.8377302885055542
Epoch 980, training loss: 13.073073387145996 = 0.5347064733505249 + 2.0 * 6.26918363571167
Epoch 980, val loss: 0.832304060459137
Epoch 990, training loss: 13.06068229675293 = 0.5251165628433228 + 2.0 * 6.267782688140869
Epoch 990, val loss: 0.8271701335906982
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7444444444444445
0.8408012651555088
=== training gcn model ===
Epoch 0, training loss: 19.13596534729004 = 1.9422932863235474 + 2.0 * 8.59683609008789
Epoch 0, val loss: 1.9462467432022095
Epoch 10, training loss: 19.130990982055664 = 1.9375029802322388 + 2.0 * 8.5967435836792
Epoch 10, val loss: 1.9412816762924194
Epoch 20, training loss: 19.125194549560547 = 1.932340145111084 + 2.0 * 8.596426963806152
Epoch 20, val loss: 1.9358936548233032
Epoch 30, training loss: 19.116683959960938 = 1.9263521432876587 + 2.0 * 8.595166206359863
Epoch 30, val loss: 1.929652452468872
Epoch 40, training loss: 19.098583221435547 = 1.9191083908081055 + 2.0 * 8.589736938476562
Epoch 40, val loss: 1.9220714569091797
Epoch 50, training loss: 19.050029754638672 = 1.9101648330688477 + 2.0 * 8.569931983947754
Epoch 50, val loss: 1.9126960039138794
Epoch 60, training loss: 18.92005157470703 = 1.8995078802108765 + 2.0 * 8.510272026062012
Epoch 60, val loss: 1.9016597270965576
Epoch 70, training loss: 18.585744857788086 = 1.8876572847366333 + 2.0 * 8.349043846130371
Epoch 70, val loss: 1.8897825479507446
Epoch 80, training loss: 18.021684646606445 = 1.8741695880889893 + 2.0 * 8.07375717163086
Epoch 80, val loss: 1.876624584197998
Epoch 90, training loss: 17.721376419067383 = 1.8609172105789185 + 2.0 * 7.930229187011719
Epoch 90, val loss: 1.8637969493865967
Epoch 100, training loss: 17.31161880493164 = 1.8498362302780151 + 2.0 * 7.730891227722168
Epoch 100, val loss: 1.8528368473052979
Epoch 110, training loss: 16.732892990112305 = 1.8412266969680786 + 2.0 * 7.445833206176758
Epoch 110, val loss: 1.8442515134811401
Epoch 120, training loss: 16.198484420776367 = 1.8352277278900146 + 2.0 * 7.181628227233887
Epoch 120, val loss: 1.8380094766616821
Epoch 130, training loss: 15.802345275878906 = 1.8300615549087524 + 2.0 * 6.986141681671143
Epoch 130, val loss: 1.8324419260025024
Epoch 140, training loss: 15.581243515014648 = 1.8245105743408203 + 2.0 * 6.878366470336914
Epoch 140, val loss: 1.8264447450637817
Epoch 150, training loss: 15.441917419433594 = 1.8177447319030762 + 2.0 * 6.81208610534668
Epoch 150, val loss: 1.8195284605026245
Epoch 160, training loss: 15.338432312011719 = 1.8102210760116577 + 2.0 * 6.764105796813965
Epoch 160, val loss: 1.8121418952941895
Epoch 170, training loss: 15.254589080810547 = 1.8025615215301514 + 2.0 * 6.726013660430908
Epoch 170, val loss: 1.8047748804092407
Epoch 180, training loss: 15.181720733642578 = 1.7952207326889038 + 2.0 * 6.6932501792907715
Epoch 180, val loss: 1.7977712154388428
Epoch 190, training loss: 15.119577407836914 = 1.788230299949646 + 2.0 * 6.665673732757568
Epoch 190, val loss: 1.791074275970459
Epoch 200, training loss: 15.061847686767578 = 1.781395673751831 + 2.0 * 6.640225887298584
Epoch 200, val loss: 1.7844657897949219
Epoch 210, training loss: 15.003047943115234 = 1.7746623754501343 + 2.0 * 6.614192962646484
Epoch 210, val loss: 1.7779260873794556
Epoch 220, training loss: 14.950990676879883 = 1.7679500579833984 + 2.0 * 6.591520309448242
Epoch 220, val loss: 1.771461009979248
Epoch 230, training loss: 14.895147323608398 = 1.761270523071289 + 2.0 * 6.566938400268555
Epoch 230, val loss: 1.7649636268615723
Epoch 240, training loss: 14.845955848693848 = 1.7543264627456665 + 2.0 * 6.545814514160156
Epoch 240, val loss: 1.758330225944519
Epoch 250, training loss: 14.79976749420166 = 1.7470306158065796 + 2.0 * 6.526368618011475
Epoch 250, val loss: 1.751379370689392
Epoch 260, training loss: 14.756047248840332 = 1.7392762899398804 + 2.0 * 6.50838565826416
Epoch 260, val loss: 1.7440043687820435
Epoch 270, training loss: 14.717239379882812 = 1.7309664487838745 + 2.0 * 6.493136405944824
Epoch 270, val loss: 1.7361414432525635
Epoch 280, training loss: 14.680929183959961 = 1.722076177597046 + 2.0 * 6.479426383972168
Epoch 280, val loss: 1.727777361869812
Epoch 290, training loss: 14.646627426147461 = 1.7125780582427979 + 2.0 * 6.467024803161621
Epoch 290, val loss: 1.7188920974731445
Epoch 300, training loss: 14.614246368408203 = 1.702380657196045 + 2.0 * 6.4559326171875
Epoch 300, val loss: 1.7094159126281738
Epoch 310, training loss: 14.589584350585938 = 1.6914199590682983 + 2.0 * 6.449082374572754
Epoch 310, val loss: 1.6992851495742798
Epoch 320, training loss: 14.555131912231445 = 1.6797066926956177 + 2.0 * 6.437712669372559
Epoch 320, val loss: 1.6885910034179688
Epoch 330, training loss: 14.526331901550293 = 1.667223334312439 + 2.0 * 6.429554462432861
Epoch 330, val loss: 1.6772500276565552
Epoch 340, training loss: 14.498485565185547 = 1.6538962125778198 + 2.0 * 6.422294616699219
Epoch 340, val loss: 1.6652189493179321
Epoch 350, training loss: 14.471185684204102 = 1.6396775245666504 + 2.0 * 6.4157538414001465
Epoch 350, val loss: 1.6524658203125
Epoch 360, training loss: 14.443904876708984 = 1.624583125114441 + 2.0 * 6.409660816192627
Epoch 360, val loss: 1.639020323753357
Epoch 370, training loss: 14.417383193969727 = 1.6087064743041992 + 2.0 * 6.404338359832764
Epoch 370, val loss: 1.6249843835830688
Epoch 380, training loss: 14.389303207397461 = 1.5920109748840332 + 2.0 * 6.398646354675293
Epoch 380, val loss: 1.6103458404541016
Epoch 390, training loss: 14.361674308776855 = 1.5744993686676025 + 2.0 * 6.393587589263916
Epoch 390, val loss: 1.5951122045516968
Epoch 400, training loss: 14.333616256713867 = 1.5561833381652832 + 2.0 * 6.388716697692871
Epoch 400, val loss: 1.57932710647583
Epoch 410, training loss: 14.309330940246582 = 1.5371742248535156 + 2.0 * 6.386078357696533
Epoch 410, val loss: 1.5631103515625
Epoch 420, training loss: 14.278074264526367 = 1.517757534980774 + 2.0 * 6.380158424377441
Epoch 420, val loss: 1.5467852354049683
Epoch 430, training loss: 14.249646186828613 = 1.4979791641235352 + 2.0 * 6.375833511352539
Epoch 430, val loss: 1.5303025245666504
Epoch 440, training loss: 14.22073745727539 = 1.4778060913085938 + 2.0 * 6.371465682983398
Epoch 440, val loss: 1.5136675834655762
Epoch 450, training loss: 14.192174911499023 = 1.4573265314102173 + 2.0 * 6.367424011230469
Epoch 450, val loss: 1.4969871044158936
Epoch 460, training loss: 14.16891860961914 = 1.4365838766098022 + 2.0 * 6.3661675453186035
Epoch 460, val loss: 1.4803133010864258
Epoch 470, training loss: 14.136489868164062 = 1.415963053703308 + 2.0 * 6.360263347625732
Epoch 470, val loss: 1.4639673233032227
Epoch 480, training loss: 14.108414649963379 = 1.3954204320907593 + 2.0 * 6.356497287750244
Epoch 480, val loss: 1.4479312896728516
Epoch 490, training loss: 14.08144760131836 = 1.3749500513076782 + 2.0 * 6.353248596191406
Epoch 490, val loss: 1.432160496711731
Epoch 500, training loss: 14.054367065429688 = 1.3545479774475098 + 2.0 * 6.34990930557251
Epoch 500, val loss: 1.4166594743728638
Epoch 510, training loss: 14.02780532836914 = 1.334240436553955 + 2.0 * 6.346782207489014
Epoch 510, val loss: 1.4014251232147217
Epoch 520, training loss: 14.005059242248535 = 1.3140777349472046 + 2.0 * 6.3454909324646
Epoch 520, val loss: 1.3864812850952148
Epoch 530, training loss: 13.978394508361816 = 1.2942237854003906 + 2.0 * 6.342085361480713
Epoch 530, val loss: 1.3719338178634644
Epoch 540, training loss: 13.95199966430664 = 1.2746413946151733 + 2.0 * 6.338679313659668
Epoch 540, val loss: 1.357740044593811
Epoch 550, training loss: 13.926864624023438 = 1.255211353302002 + 2.0 * 6.335826396942139
Epoch 550, val loss: 1.3437787294387817
Epoch 560, training loss: 13.903887748718262 = 1.2358981370925903 + 2.0 * 6.3339948654174805
Epoch 560, val loss: 1.330001711845398
Epoch 570, training loss: 13.880447387695312 = 1.2168269157409668 + 2.0 * 6.331810474395752
Epoch 570, val loss: 1.3164793252944946
Epoch 580, training loss: 13.85621452331543 = 1.1979871988296509 + 2.0 * 6.329113483428955
Epoch 580, val loss: 1.3031564950942993
Epoch 590, training loss: 13.833087921142578 = 1.1792949438095093 + 2.0 * 6.326896667480469
Epoch 590, val loss: 1.28996741771698
Epoch 600, training loss: 13.810347557067871 = 1.1606718301773071 + 2.0 * 6.324837684631348
Epoch 600, val loss: 1.2768522500991821
Epoch 610, training loss: 13.794098854064941 = 1.1420916318893433 + 2.0 * 6.326003551483154
Epoch 610, val loss: 1.2637524604797363
Epoch 620, training loss: 13.76628303527832 = 1.1237049102783203 + 2.0 * 6.3212890625
Epoch 620, val loss: 1.25075101852417
Epoch 630, training loss: 13.744208335876465 = 1.1054071187973022 + 2.0 * 6.319400787353516
Epoch 630, val loss: 1.2378112077713013
Epoch 640, training loss: 13.722387313842773 = 1.087177038192749 + 2.0 * 6.317605018615723
Epoch 640, val loss: 1.224906086921692
Epoch 650, training loss: 13.70163631439209 = 1.0689802169799805 + 2.0 * 6.316328048706055
Epoch 650, val loss: 1.211989164352417
Epoch 660, training loss: 13.682633399963379 = 1.050864815711975 + 2.0 * 6.315884113311768
Epoch 660, val loss: 1.1991064548492432
Epoch 670, training loss: 13.65936279296875 = 1.0329629182815552 + 2.0 * 6.313199996948242
Epoch 670, val loss: 1.1863149404525757
Epoch 680, training loss: 13.637837409973145 = 1.0151478052139282 + 2.0 * 6.311344623565674
Epoch 680, val loss: 1.1735950708389282
Epoch 690, training loss: 13.616755485534668 = 0.9974454641342163 + 2.0 * 6.30965518951416
Epoch 690, val loss: 1.1609104871749878
Epoch 700, training loss: 13.597612380981445 = 0.9798676371574402 + 2.0 * 6.308872222900391
Epoch 700, val loss: 1.148258924484253
Epoch 710, training loss: 13.576350212097168 = 0.9624665379524231 + 2.0 * 6.306941986083984
Epoch 710, val loss: 1.1356922388076782
Epoch 720, training loss: 13.557854652404785 = 0.9453514218330383 + 2.0 * 6.306251525878906
Epoch 720, val loss: 1.1233270168304443
Epoch 730, training loss: 13.537148475646973 = 0.9284372925758362 + 2.0 * 6.304355621337891
Epoch 730, val loss: 1.1111177206039429
Epoch 740, training loss: 13.517745018005371 = 0.9117406606674194 + 2.0 * 6.30300235748291
Epoch 740, val loss: 1.099039912223816
Epoch 750, training loss: 13.503361701965332 = 0.8952612280845642 + 2.0 * 6.304050445556641
Epoch 750, val loss: 1.0871211290359497
Epoch 760, training loss: 13.480283737182617 = 0.8791313767433167 + 2.0 * 6.300576210021973
Epoch 760, val loss: 1.075410008430481
Epoch 770, training loss: 13.462152481079102 = 0.8632466197013855 + 2.0 * 6.299452781677246
Epoch 770, val loss: 1.0639220476150513
Epoch 780, training loss: 13.447842597961426 = 0.8476564884185791 + 2.0 * 6.300093173980713
Epoch 780, val loss: 1.0526413917541504
Epoch 790, training loss: 13.42845344543457 = 0.8323978185653687 + 2.0 * 6.298027992248535
Epoch 790, val loss: 1.0416131019592285
Epoch 800, training loss: 13.409319877624512 = 0.8174323439598083 + 2.0 * 6.295943737030029
Epoch 800, val loss: 1.0308500528335571
Epoch 810, training loss: 13.393232345581055 = 0.8027682900428772 + 2.0 * 6.295231819152832
Epoch 810, val loss: 1.0203136205673218
Epoch 820, training loss: 13.377609252929688 = 0.7883899807929993 + 2.0 * 6.294609546661377
Epoch 820, val loss: 1.0100229978561401
Epoch 830, training loss: 13.359579086303711 = 0.7743393778800964 + 2.0 * 6.292619705200195
Epoch 830, val loss: 1.0000014305114746
Epoch 840, training loss: 13.344918251037598 = 0.7605426907539368 + 2.0 * 6.292187690734863
Epoch 840, val loss: 0.9902080297470093
Epoch 850, training loss: 13.330894470214844 = 0.7470084428787231 + 2.0 * 6.291943073272705
Epoch 850, val loss: 0.9806548953056335
Epoch 860, training loss: 13.312981605529785 = 0.7337755560874939 + 2.0 * 6.289603233337402
Epoch 860, val loss: 0.9713945388793945
Epoch 870, training loss: 13.2978515625 = 0.7208287715911865 + 2.0 * 6.288511276245117
Epoch 870, val loss: 0.9623989462852478
Epoch 880, training loss: 13.284307479858398 = 0.7080987095832825 + 2.0 * 6.28810453414917
Epoch 880, val loss: 0.9535855054855347
Epoch 890, training loss: 13.269756317138672 = 0.6955881118774414 + 2.0 * 6.287084102630615
Epoch 890, val loss: 0.9449896216392517
Epoch 900, training loss: 13.25877857208252 = 0.6833218932151794 + 2.0 * 6.287728309631348
Epoch 900, val loss: 0.9366023540496826
Epoch 910, training loss: 13.240424156188965 = 0.6713211536407471 + 2.0 * 6.284551620483398
Epoch 910, val loss: 0.9284763932228088
Epoch 920, training loss: 13.226736068725586 = 0.6594963073730469 + 2.0 * 6.2836198806762695
Epoch 920, val loss: 0.9205676913261414
Epoch 930, training loss: 13.217808723449707 = 0.6478428244590759 + 2.0 * 6.284983158111572
Epoch 930, val loss: 0.9128131866455078
Epoch 940, training loss: 13.2022123336792 = 0.6363489031791687 + 2.0 * 6.282931804656982
Epoch 940, val loss: 0.905217170715332
Epoch 950, training loss: 13.188350677490234 = 0.625087559223175 + 2.0 * 6.2816314697265625
Epoch 950, val loss: 0.8978833556175232
Epoch 960, training loss: 13.173713684082031 = 0.6139187812805176 + 2.0 * 6.279897689819336
Epoch 960, val loss: 0.890708863735199
Epoch 970, training loss: 13.161295890808105 = 0.6028624773025513 + 2.0 * 6.279216766357422
Epoch 970, val loss: 0.8836683630943298
Epoch 980, training loss: 13.156527519226074 = 0.5919160842895508 + 2.0 * 6.282305717468262
Epoch 980, val loss: 0.8767666220664978
Epoch 990, training loss: 13.139339447021484 = 0.5810580253601074 + 2.0 * 6.279140472412109
Epoch 990, val loss: 0.8700224757194519
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7407407407407408
0.8355297838692674
=== training gcn model ===
Epoch 0, training loss: 19.138830184936523 = 1.945143222808838 + 2.0 * 8.596843719482422
Epoch 0, val loss: 1.9490747451782227
Epoch 10, training loss: 19.133638381958008 = 1.9401423931121826 + 2.0 * 8.596748352050781
Epoch 10, val loss: 1.9440032243728638
Epoch 20, training loss: 19.127565383911133 = 1.9346812963485718 + 2.0 * 8.596442222595215
Epoch 20, val loss: 1.9383885860443115
Epoch 30, training loss: 19.11893081665039 = 1.9282325506210327 + 2.0 * 8.595349311828613
Epoch 30, val loss: 1.931640863418579
Epoch 40, training loss: 19.102554321289062 = 1.920247197151184 + 2.0 * 8.591153144836426
Epoch 40, val loss: 1.9231117963790894
Epoch 50, training loss: 19.063982009887695 = 1.9102704524993896 + 2.0 * 8.576855659484863
Epoch 50, val loss: 1.9123625755310059
Epoch 60, training loss: 18.969518661499023 = 1.8983649015426636 + 2.0 * 8.535576820373535
Epoch 60, val loss: 1.8997081518173218
Epoch 70, training loss: 18.73444366455078 = 1.8854706287384033 + 2.0 * 8.42448616027832
Epoch 70, val loss: 1.8864853382110596
Epoch 80, training loss: 18.198219299316406 = 1.8720258474349976 + 2.0 * 8.16309642791748
Epoch 80, val loss: 1.8726431131362915
Epoch 90, training loss: 17.610742568969727 = 1.8570754528045654 + 2.0 * 7.876833438873291
Epoch 90, val loss: 1.8574645519256592
Epoch 100, training loss: 17.11815071105957 = 1.8447370529174805 + 2.0 * 7.636706829071045
Epoch 100, val loss: 1.8452575206756592
Epoch 110, training loss: 16.598133087158203 = 1.8353524208068848 + 2.0 * 7.38139009475708
Epoch 110, val loss: 1.836100459098816
Epoch 120, training loss: 16.241180419921875 = 1.8269833326339722 + 2.0 * 7.207098960876465
Epoch 120, val loss: 1.8280316591262817
Epoch 130, training loss: 15.976696014404297 = 1.8186755180358887 + 2.0 * 7.079010009765625
Epoch 130, val loss: 1.820030689239502
Epoch 140, training loss: 15.791728973388672 = 1.8105733394622803 + 2.0 * 6.990577697753906
Epoch 140, val loss: 1.8120630979537964
Epoch 150, training loss: 15.650727272033691 = 1.802422285079956 + 2.0 * 6.924152374267578
Epoch 150, val loss: 1.8038842678070068
Epoch 160, training loss: 15.530597686767578 = 1.7947365045547485 + 2.0 * 6.8679304122924805
Epoch 160, val loss: 1.7962089776992798
Epoch 170, training loss: 15.420984268188477 = 1.7876383066177368 + 2.0 * 6.8166728019714355
Epoch 170, val loss: 1.7894011735916138
Epoch 180, training loss: 15.305728912353516 = 1.780912160873413 + 2.0 * 6.762408256530762
Epoch 180, val loss: 1.7832432985305786
Epoch 190, training loss: 15.20789623260498 = 1.7745497226715088 + 2.0 * 6.716673374176025
Epoch 190, val loss: 1.7773696184158325
Epoch 200, training loss: 15.134056091308594 = 1.76793372631073 + 2.0 * 6.683061122894287
Epoch 200, val loss: 1.771270990371704
Epoch 210, training loss: 15.07280158996582 = 1.7608522176742554 + 2.0 * 6.655974864959717
Epoch 210, val loss: 1.7645390033721924
Epoch 220, training loss: 15.018962860107422 = 1.753139615058899 + 2.0 * 6.632911682128906
Epoch 220, val loss: 1.7573275566101074
Epoch 230, training loss: 14.967597007751465 = 1.7449066638946533 + 2.0 * 6.611345291137695
Epoch 230, val loss: 1.7498018741607666
Epoch 240, training loss: 14.91869068145752 = 1.7362443208694458 + 2.0 * 6.591223239898682
Epoch 240, val loss: 1.7419466972351074
Epoch 250, training loss: 14.872032165527344 = 1.7270567417144775 + 2.0 * 6.572487831115723
Epoch 250, val loss: 1.7336533069610596
Epoch 260, training loss: 14.826047897338867 = 1.7173361778259277 + 2.0 * 6.554355621337891
Epoch 260, val loss: 1.7248786687850952
Epoch 270, training loss: 14.780654907226562 = 1.7070727348327637 + 2.0 * 6.5367913246154785
Epoch 270, val loss: 1.7156134843826294
Epoch 280, training loss: 14.739153861999512 = 1.6961568593978882 + 2.0 * 6.521498680114746
Epoch 280, val loss: 1.7057998180389404
Epoch 290, training loss: 14.698277473449707 = 1.684490442276001 + 2.0 * 6.506893634796143
Epoch 290, val loss: 1.6952435970306396
Epoch 300, training loss: 14.660305976867676 = 1.6719177961349487 + 2.0 * 6.494194030761719
Epoch 300, val loss: 1.6839226484298706
Epoch 310, training loss: 14.622583389282227 = 1.6583925485610962 + 2.0 * 6.482095241546631
Epoch 310, val loss: 1.6717324256896973
Epoch 320, training loss: 14.594429969787598 = 1.6439530849456787 + 2.0 * 6.47523832321167
Epoch 320, val loss: 1.6589680910110474
Epoch 330, training loss: 14.552477836608887 = 1.6288946866989136 + 2.0 * 6.461791515350342
Epoch 330, val loss: 1.6455740928649902
Epoch 340, training loss: 14.518949508666992 = 1.6129953861236572 + 2.0 * 6.452977180480957
Epoch 340, val loss: 1.6315863132476807
Epoch 350, training loss: 14.485450744628906 = 1.596132755279541 + 2.0 * 6.4446587562561035
Epoch 350, val loss: 1.616822361946106
Epoch 360, training loss: 14.452276229858398 = 1.5782272815704346 + 2.0 * 6.4370245933532715
Epoch 360, val loss: 1.60123872756958
Epoch 370, training loss: 14.419303894042969 = 1.5592467784881592 + 2.0 * 6.430028438568115
Epoch 370, val loss: 1.584814190864563
Epoch 380, training loss: 14.38640022277832 = 1.5391631126403809 + 2.0 * 6.423618793487549
Epoch 380, val loss: 1.567547082901001
Epoch 390, training loss: 14.353767395019531 = 1.517974615097046 + 2.0 * 6.417896270751953
Epoch 390, val loss: 1.5494205951690674
Epoch 400, training loss: 14.321322441101074 = 1.4958316087722778 + 2.0 * 6.412745475769043
Epoch 400, val loss: 1.5305927991867065
Epoch 410, training loss: 14.289299964904785 = 1.4729691743850708 + 2.0 * 6.408165454864502
Epoch 410, val loss: 1.5112874507904053
Epoch 420, training loss: 14.256566047668457 = 1.4492555856704712 + 2.0 * 6.403655052185059
Epoch 420, val loss: 1.4913640022277832
Epoch 430, training loss: 14.223588943481445 = 1.4246283769607544 + 2.0 * 6.39948034286499
Epoch 430, val loss: 1.4706999063491821
Epoch 440, training loss: 14.190275192260742 = 1.3992639780044556 + 2.0 * 6.395505428314209
Epoch 440, val loss: 1.4495198726654053
Epoch 450, training loss: 14.162455558776855 = 1.3731783628463745 + 2.0 * 6.394638538360596
Epoch 450, val loss: 1.427893042564392
Epoch 460, training loss: 14.124192237854004 = 1.3469007015228271 + 2.0 * 6.388645648956299
Epoch 460, val loss: 1.4062200784683228
Epoch 470, training loss: 14.090475082397461 = 1.3203178644180298 + 2.0 * 6.385078430175781
Epoch 470, val loss: 1.3844484090805054
Epoch 480, training loss: 14.056452751159668 = 1.2934588193893433 + 2.0 * 6.381496906280518
Epoch 480, val loss: 1.3625754117965698
Epoch 490, training loss: 14.024374008178711 = 1.2663893699645996 + 2.0 * 6.378992557525635
Epoch 490, val loss: 1.3406902551651
Epoch 500, training loss: 13.989301681518555 = 1.2394963502883911 + 2.0 * 6.374902725219727
Epoch 500, val loss: 1.3190339803695679
Epoch 510, training loss: 13.955975532531738 = 1.2127045392990112 + 2.0 * 6.371635437011719
Epoch 510, val loss: 1.2975356578826904
Epoch 520, training loss: 13.923049926757812 = 1.1861590147018433 + 2.0 * 6.36844539642334
Epoch 520, val loss: 1.2763410806655884
Epoch 530, training loss: 13.889636993408203 = 1.1599808931350708 + 2.0 * 6.364828109741211
Epoch 530, val loss: 1.2555865049362183
Epoch 540, training loss: 13.858341217041016 = 1.1342612504959106 + 2.0 * 6.362040042877197
Epoch 540, val loss: 1.235229730606079
Epoch 550, training loss: 13.82595443725586 = 1.1090625524520874 + 2.0 * 6.35844612121582
Epoch 550, val loss: 1.2154302597045898
Epoch 560, training loss: 13.79656982421875 = 1.0845599174499512 + 2.0 * 6.3560051918029785
Epoch 560, val loss: 1.196219801902771
Epoch 570, training loss: 13.765880584716797 = 1.0605970621109009 + 2.0 * 6.352641582489014
Epoch 570, val loss: 1.1775388717651367
Epoch 580, training loss: 13.736712455749512 = 1.0372114181518555 + 2.0 * 6.349750518798828
Epoch 580, val loss: 1.159301996231079
Epoch 590, training loss: 13.70816421508789 = 1.0143636465072632 + 2.0 * 6.346900463104248
Epoch 590, val loss: 1.1415742635726929
Epoch 600, training loss: 13.686426162719727 = 0.9921514987945557 + 2.0 * 6.347137451171875
Epoch 600, val loss: 1.1243582963943481
Epoch 610, training loss: 13.655033111572266 = 0.9708058834075928 + 2.0 * 6.342113494873047
Epoch 610, val loss: 1.1079401969909668
Epoch 620, training loss: 13.629593849182129 = 0.9503343105316162 + 2.0 * 6.339629650115967
Epoch 620, val loss: 1.0922952890396118
Epoch 630, training loss: 13.603919982910156 = 0.9305395483970642 + 2.0 * 6.336690425872803
Epoch 630, val loss: 1.0772453546524048
Epoch 640, training loss: 13.580280303955078 = 0.9113315343856812 + 2.0 * 6.334474563598633
Epoch 640, val loss: 1.062734603881836
Epoch 650, training loss: 13.561728477478027 = 0.8927850127220154 + 2.0 * 6.334471702575684
Epoch 650, val loss: 1.0487604141235352
Epoch 660, training loss: 13.537315368652344 = 0.8750455379486084 + 2.0 * 6.331134796142578
Epoch 660, val loss: 1.035502552986145
Epoch 670, training loss: 13.51440715789795 = 0.8579617142677307 + 2.0 * 6.328222751617432
Epoch 670, val loss: 1.0228427648544312
Epoch 680, training loss: 13.493141174316406 = 0.8414158821105957 + 2.0 * 6.325862884521484
Epoch 680, val loss: 1.0106890201568604
Epoch 690, training loss: 13.472558975219727 = 0.8253703713417053 + 2.0 * 6.323594093322754
Epoch 690, val loss: 0.9989997148513794
Epoch 700, training loss: 13.46125602722168 = 0.8098037242889404 + 2.0 * 6.32572603225708
Epoch 700, val loss: 0.9877969026565552
Epoch 710, training loss: 13.437660217285156 = 0.7948952317237854 + 2.0 * 6.321382522583008
Epoch 710, val loss: 0.977099597454071
Epoch 720, training loss: 13.41643238067627 = 0.7804915308952332 + 2.0 * 6.317970275878906
Epoch 720, val loss: 0.9670049548149109
Epoch 730, training loss: 13.39896011352539 = 0.7665408849716187 + 2.0 * 6.31620979309082
Epoch 730, val loss: 0.9573745131492615
Epoch 740, training loss: 13.381621360778809 = 0.7529908418655396 + 2.0 * 6.314315319061279
Epoch 740, val loss: 0.9481341242790222
Epoch 750, training loss: 13.365962028503418 = 0.7397939562797546 + 2.0 * 6.313084125518799
Epoch 750, val loss: 0.9392758011817932
Epoch 760, training loss: 13.350979804992676 = 0.726995050907135 + 2.0 * 6.311992168426514
Epoch 760, val loss: 0.9308160543441772
Epoch 770, training loss: 13.334632873535156 = 0.7146411538124084 + 2.0 * 6.309995651245117
Epoch 770, val loss: 0.9227973222732544
Epoch 780, training loss: 13.31869888305664 = 0.7026249766349792 + 2.0 * 6.308036804199219
Epoch 780, val loss: 0.9151758551597595
Epoch 790, training loss: 13.308052062988281 = 0.6908849477767944 + 2.0 * 6.308583736419678
Epoch 790, val loss: 0.9078734517097473
Epoch 800, training loss: 13.292078018188477 = 0.6794795989990234 + 2.0 * 6.306299209594727
Epoch 800, val loss: 0.900816023349762
Epoch 810, training loss: 13.276542663574219 = 0.6683102250099182 + 2.0 * 6.304116249084473
Epoch 810, val loss: 0.8941338062286377
Epoch 820, training loss: 13.261595726013184 = 0.6573758721351624 + 2.0 * 6.302109718322754
Epoch 820, val loss: 0.8876729607582092
Epoch 830, training loss: 13.2478609085083 = 0.6465772390365601 + 2.0 * 6.300642013549805
Epoch 830, val loss: 0.8814685344696045
Epoch 840, training loss: 13.235746383666992 = 0.6359384059906006 + 2.0 * 6.299903869628906
Epoch 840, val loss: 0.8754509091377258
Epoch 850, training loss: 13.224823951721191 = 0.6254562139511108 + 2.0 * 6.299684047698975
Epoch 850, val loss: 0.869624674320221
Epoch 860, training loss: 13.209518432617188 = 0.615166187286377 + 2.0 * 6.297176361083984
Epoch 860, val loss: 0.8639780879020691
Epoch 870, training loss: 13.196643829345703 = 0.6049615144729614 + 2.0 * 6.295841217041016
Epoch 870, val loss: 0.8585571646690369
Epoch 880, training loss: 13.187859535217285 = 0.5948653817176819 + 2.0 * 6.296496868133545
Epoch 880, val loss: 0.8532758355140686
Epoch 890, training loss: 13.171042442321777 = 0.5848714709281921 + 2.0 * 6.29308557510376
Epoch 890, val loss: 0.8481330275535583
Epoch 900, training loss: 13.161380767822266 = 0.5749329328536987 + 2.0 * 6.293223857879639
Epoch 900, val loss: 0.8431517481803894
Epoch 910, training loss: 13.149648666381836 = 0.5651370882987976 + 2.0 * 6.292255878448486
Epoch 910, val loss: 0.8382298946380615
Epoch 920, training loss: 13.13612174987793 = 0.5553948283195496 + 2.0 * 6.290363311767578
Epoch 920, val loss: 0.8335267305374146
Epoch 930, training loss: 13.12326717376709 = 0.5457072854042053 + 2.0 * 6.2887797355651855
Epoch 930, val loss: 0.8289229869842529
Epoch 940, training loss: 13.110617637634277 = 0.5360185503959656 + 2.0 * 6.287299633026123
Epoch 940, val loss: 0.8244161009788513
Epoch 950, training loss: 13.099048614501953 = 0.526308536529541 + 2.0 * 6.286369800567627
Epoch 950, val loss: 0.8199781179428101
Epoch 960, training loss: 13.09510612487793 = 0.5165692567825317 + 2.0 * 6.289268493652344
Epoch 960, val loss: 0.8156391978263855
Epoch 970, training loss: 13.077436447143555 = 0.5068875551223755 + 2.0 * 6.285274505615234
Epoch 970, val loss: 0.8113536834716797
Epoch 980, training loss: 13.065844535827637 = 0.49721410870552063 + 2.0 * 6.28431510925293
Epoch 980, val loss: 0.8072613477706909
Epoch 990, training loss: 13.052700996398926 = 0.4875414967536926 + 2.0 * 6.2825798988342285
Epoch 990, val loss: 0.803244411945343
=== picking the best model according to the performance on validation ===
Accuracy of GNN+CL: 0.7555555555555555
0.8439641539272537
The final CL Acc:0.74691, 0.00630, The final GNN Acc:0.84010, 0.00348
